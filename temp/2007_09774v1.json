{
  "id": "http://arxiv.org/abs/2007.09774v1",
  "title": "An Overview of Natural Language State Representation for Reinforcement Learning",
  "authors": [
    "Brielen Madureira",
    "David Schlangen"
  ],
  "abstract": "A suitable state representation is a fundamental part of the learning process\nin Reinforcement Learning. In various tasks, the state can either be described\nby natural language or be natural language itself. This survey outlines the\nstrategies used in the literature to build natural language state\nrepresentations. We appeal for more linguistically interpretable and grounded\nrepresentations, careful justification of design decisions and evaluation of\nthe effectiveness of different approaches.",
  "text": "An Overview of Natural Language State Representation\nfor Reinforcement Learning\nBrielen Madureira 1 David Schlangen 1\nAbstract\nA suitable state representation is a fundamental\npart of the learning process in Reinforcement\nLearning. In various tasks, the state can either\nbe described by natural language or be natural\nlanguage itself. This survey outlines the strategies\nused in the literature to build natural language\nstate representations. We appeal for more linguis-\ntically interpretable and grounded representations,\ncareful justiﬁcation of design decisions and evalu-\nation of the effectiveness of different approaches.\n1. Introduction\nIn any typical Reinforcement Learning (RL) scenario, there\nis an agent in an environment following a policy to take ac-\ntions which make it transition through states and receive re-\nwards (Sutton & Barto, 1998). Each of these elements must\nbe modeled according to the purpose of the task and their\nrepresentation can inﬂuence the outcome of RL algorithms.\nNatural Language Understanding can be integrated into\nthese components, both in language-conditional RL (when\nnatural language is inherent to the task) and in language-\nassisted RL (when natural language is an additional tool),\nas discussed in Luketina et al. (2019).\nThe reward function is usually under the spotlight because\nthe agent’s goal is maximizing the expected long-term re-\nward. But the state representation is no less vital. Based on\nit, the agent perceives the environment and comes to deci-\nsions on how to act. While in robotics the agent observes a\nspatial environment and in arcade games the state may be\ncomposed of a sequence of images, natural language is as\nwell a common part of RL states, as illustrated in Figure 1.\nThe choice of state representation is a problem on its own. It\ndirectly affects the learning process (Jones & Canas, 2010),\nso if applications neglect this, the agent may be prevented\nfrom accessing key information for decision-making.\n1University of Potsdam, Potsdam, Germany. Correspondence\nto: Brielen Madureira <madureiralasota@uni-potsdam.de>.\nAccepted to the 1 st Workshop on Language in Reinforcement\nLearning, ICML 2020. Copyright 2020 by the author(s).\nTXT\nTXT\nFigure 1. Natural language can describe the agent’s state (e.g. text-\nbased games) or be part of it (e.g. dialogue or text summarization).\nThere is ongoing research on the use of natural language\nto model the reward and the actions (He et al., 2016; Feng\net al., 2018; Goyal et al., 2019b), but we are not aware of\nany recent systematic survey about the various possibilities\nof using natural language to model the state representation\nand how to do that effectively. We aim at ﬁlling this gap by\nproviding an overview of previous work in which the state\nis based on text, delineating the main approaches.\nWe dive into a wide range of NLP papers that apply RL\nmethods and whose state representations have to capture\nlinguistic features that inﬂuence decision-making. Findings\nabout state representations in this area may potentially be\nextrapolated to other language-informed RL tasks. We thus\nhope this overview aids those seeking the objective of hav-\ning RL agents capable of understanding natural language.\nSince we notice there is no consensus on how to design\nnatural language state representations, we conclude with\nsome concrete recommendations for future research.\n2. State Representation in RL\nThe construction of suitable state representations for RL has\nbeen assessed by several works, for example in the ﬁeld of\nrobotics and autonomous cars, where modeling the state is\nparticularly hard due to very high dimensional data com-\ning from multiple sensors (Jonschkowski & Brock, 2013;\nDe Bruin et al., 2018). Lesort et al. (2018), for instance,\npresented a review on State Representation Learning algo-\nrithms and evaluation methods for control scenarios.\nBy deﬁnition, the state in a Markov Decision Process, which\nunderlies the formalization of RL, needs to be Markovian;\ni.e. all that needs to be known about the past must be con-\ntained in the current state. Jonschkowski & Brock (2013)\nsummarized other desirable properties of state representa-\ntions proposed by many authors: It should provide good\nfeatures for learning the value function; be compact but\nstill allowing the original observations to be reconstructed;\narXiv:2007.09774v1  [cs.CL]  19 Jul 2020\nNatural Language State Representation\nchange slowly over time; be useful for predicting future\nobservations and rewards given future actions; and ideally\nbe shared by various similar tasks. Although their focus was\nrobotics, these properties can be extended to textual states.\nMany other aspects of state representation have been in-\nvestigated.1 In early works, states were handcrafted by\nthe system designer using features. Although it remains\na common practice, feature engineering has evident draw-\nbacks: it needs an experienced designer, is tedious and\ntime-consuming and does not generalize (B¨ohmer et al.,\n2015). The popularization of deep learning can to some\nextent spare us part of these shortcomings, as raw data can\npotentially be used as input to end-to-end models that build\nstate representations implicitly. De Bruin et al. (2018), for in-\nstance, analyzed methods for integrating state representation\nlearning into deep RL. Still, choosing which information,\ndata structure and model to use can be regarded as a mod-\nern form of feature engineering. Several options have been\napplied to textual data, as we describe in the next section.\n3. State Representation in NLP tasks\nWhen RL methods are adopted to solve NLP tasks, one\ncommon characteristic is that the state S is represented as\na function of texts T, i.e. S = f(T). As we will see, f\nmay take a variety of forms and additional inputs. We begin\nby reviewing the development of natural language state\nrepresentation in dialogue, instruction following, text-based\ngames and text summarization, NLP areas in which RL\nmethods have been used more often. With the consolidation\nof deep RL methods, more studies tried to solve several\nother tasks, which we aggregate in the last part.2\nDialogue is likely the NLP area with the most substantial\nnumber of papers that adopt RL, with research going on for\nmore than two decades, e.g. Levin et al. (1998). In dialogue,\nthe state representation is usually a mapping from text to\nan abstract or compact format, such as a template, a belief\nstate or an embedding vector, used by the agent to generate\nthe next utterance. A Natural Language Understanding\ncomponent between the text and the state representation is\ncommon, as illustrated, for example, in Lipton et al. (2018).\nDelineating the state space was at ﬁrst mostly a manual task,\nbecause considering the entire dialogue used to be impracti-\ncal (Singh et al., 2000a). Keeping the state space small was\na common concern in order to avoid the curse of dimension-\nality (Levin et al., 2000; Singh et al., 2000a). Therefore, sys-\n1See e.g. McCallum (1996); Kaelbling et al. (1998); Finney\net al. (2002); Van Otterlo (2002); Morales (2004); Roy et al. (2005);\nFrommberger (2008); Mahmud (2010); Maillard et al. (2011);\nOrtiz et al. (2018); Franc¸ois-Lavet et al. (2019).\n2LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Cho\net al., 2014) are references to models mentioned throughout.\ntem designers picked a set of variables or features based on\ntheir experience, usually resulting in application-dependent\nrepresentations that did not generalize (Walker, 2000; Levin\net al., 2000; Frampton & Lemon, 2005; English & Heeman,\n2005; Mitchell et al., 2013; Khouzaimi et al., 2015). The\nslots and values of the dialogue state were used to represent\nthe environment state (Papangelis, 2012) or the Information\nStates approach was incorporated (Georgila et al., 2005;\nHenderson et al., 2005; Heeman et al., 2012). It was soon\nclear that modeling the state space is a fundamental aspect\nof RL for dialogue, as it has direct impact on the dynamics\nof the system, and attention was drawn to the fact that the\nresearch community had neither established best practices\nfor modeling the state nor agreed upon domain-independent\nvariables (Paek, 2006).\nSome studies tried to examine which state representation\nwas more effective for the main task (Schefﬂer & Young,\n2002), constructing them with different feature combina-\ntions. There was also discussion on ensuring the Marko-\nvian property (Singh et al., 2000b), which means that the\n“representation must encode everything that the system ob-\nserved about everything that has happened in the dialogue\nso far” (Walker, 2000). Ablation studies and metrics to eval-\nuate the effect of each feature were proposed (Frampton &\nLemon, 2005; Tetreault & Litman, 2008; Heeman, 2009;\nMitchell et al., 2013).\nOnce neural network models started to be employed, new\nways to represent states and to integrate representation learn-\ning into the agent’s learning were enabled. States could be\nmore easily represented by a vector and fed directly into a\nparametrized value or policy function. A common approach\nwas building a belief state (a distribution over possible dia-\nlogue states, usually represented by a ﬁxed set of slot-value\npairs) when dialogue tasks are modeled as POMDPs (Su\net al., 2016; Fatemi et al., 2016; Wen et al., 2017b; Weisz\net al., 2018). In Wen et al. (2017b), a sequence of free form\ntext was mapped into a ﬁxed set of slot-value pairs by an\nRNN. Dhingra et al. (2017) compared a handcrafted and\na neural belief tracker that uses a GRU over turns. The\nbelief state was sometimes combined with other representa-\ntions. Wen et al. (2017a) modeled the dialogue state vector\nas a concatenation of user input (encoded by a BiLSTM), a\nbelief vector (probability distributions over domain speciﬁc\nslot-value pairs, extracted by RNN-CNN belief trackers)\nand the degree of matching in a knowledge base.\nNote that, even when the designer does not have to build\nthe representation manually, the selection of slots and value\nranges, the choice of input features and the NLU representa-\ntions still require human intervention. Features can still be\nused as input to deep learning models. Williams & Zweig\n(2016) represented the dialogue history with features (such\nas input and output entities) as input to an LSTM, which in-\nNatural Language State Representation\nferred the state representation and mapped the input directly\nto actions. The NLU component played a key role in the\nstate representation e.g. in Manuvinakurike et al. (2017).\nCuay´ahuitl et al. (2016) pointed out that it is typically un-\nclear what features to incorporate in a multi-domain dia-\nlogue and proposed applying deep RL directly to texts, so\nthat the agent learns feature representation and the policy\ntogether, bypassing NLU components. Similarly, Li et al.\n(2016) used the two previous dialogue turns, transformed\ninto a continuous vector representation by an LSTM en-\ncoder, to build representations directly from raw text.\nA common strategy has been to use the hidden vector of\nRNNs to represent and update the environment state (Zhao\n& Eskenazi, 2016; Williams et al., 2017; Liu & Lane, 2017).\nMultimodal state representation has also become frequent,\ncombining encoded text with images (Das et al., 2017),\nknowledge base queries (Li et al., 2017; Liu & Lane, 2017)\nor an embedding of a knowledge graph (Yang et al., 2020).\nIn Instruction Following, deciding the next action means\ndirectly interpreting a textual instruction. In this task, states\nwere ﬁrst designed as tuples of world and linguistic features:\nwords and documents (Branavan et al., 2009; 2010) or car-\ndinal directions and utterances (Vogel & Jurafsky, 2010).\nWe then observed the use of sentence embeddings (usually\nby an LSTM or GRU) combined with other sources such\nas image embeddings (Misra et al., 2017; Hermann et al.,\n2017; Kaplan et al., 2017; Fu et al., 2019) or instruction\nmemory (Oh et al., 2017). A state processing module was\nintroduced in Chaplot et al. (2018) to create a joint represen-\ntation of the image and the instruction. Janner et al. (2018)\nconverted the instruction text into a real-valued vector and\nused it both to build a global map-level representation and\nas a kernel in a convolution operation to obtain a local rep-\nresentation of the state.\nIn the related vision-language navigation task, the state was\nmodeled by combining visually grounded textual context\nand textually grounded visual context (Wang et al., 2019;\nWang et al., 2020) or by using an attention mechanism over\nthe representations of the instruction (Anderson et al., 2018).\nText-Based Games pose a related, but more general chal-\nlenge, as here the text describing the current game state must\nbe integrated into the agent state and the best action must\nbe inferred. The work of Narasimhan et al. (2015) was a\nreference for modeling the state representation in text-based\ngames, which is jointly learned together with action policies\nin their setting. They used an LSTM over textual data with\na mean pooling layer on top, in an attempt to capture the\nsemantics of the game states. Subsequent works followed\nthe same approach (Ansari et al., 2018; Yuan et al., 2018;\nJain et al., 2020) or used variants of continuous vectors built\nby neural networks (He et al., 2016; Foerster et al., 2016).\nOther elaborate ideas appeared subsequently. Narasimhan\net al. (2018) used a factorized state representation, concate-\nnating an object embedding with its textual speciﬁcation\nembedding (LSTM or bag-of-words).\nAmmanabrolu &\nRiedl (2019) combined the embedded text (by a sliding BiL-\nSTM) with an embedded knowledge graph built by the agent\nthroughout the game. Their concatenation served as input\nto an MLP that outputted state representations. Murugesan\net al. (2020) used embeddings of a local belief graph and a\nglobal common sense graph of entities. Zhong et al. (2020)\nproposed building representations that capture interactions\nbetween the goal, a document describing environment dy-\nnamics, and environment observations.\nText Summarization is another ﬂourishing area for RL.\nIn this task, an agent processes a text and either pick key\nsentences to compose a summary or use Natural Language\nGeneration to output its own words. Initial works applying\nRL used tuples of features to represent the state, composed\nof the summary at each time step, a history of actions and\na binary variable indicating the terminal state (Ryang &\nAbekawa, 2012; Rioux et al., 2014; Henß et al., 2015).\nRL was really consolidated for text summarization after\ndeep learning methods became available. In most cases, the\nstate representation was, as in other tasks, the hidden state\nof an RNN generating the summary (Ling & Rush, 2017;\nPasunuru & Bansal, 2018), also together with an encoding\nof a candidate sentence (Lee & Lee, 2017). Paulus et al.\n(2018), a reference for subsequent works (Kry´sci´nski et al.,\n2018), used context vectors with intra-temporal attention\nover the hidden states of the encoder (which process the\noriginal text) concatenated to the hidden state of the decoder\n(which generates the summary).\nHierarchical approaches are common. Wu & Hu (2018)\nbuilt a document encoding with a CNN operating at word\nlevel and a BiGRU at sentence level, whereas Narayan et al.\n(2018) and Chen & Bansal (2018) used a CNN at sentence\nlevel and an LSTM or BiLSTM at document level to capture\nglobal information. Likewise, Yao et al. (2018) employed\nan RNN or CNN sentence encoder, together with a repre-\nsentation of the current summary and the document content.\nOther NLP Tasks comprise each a currently smaller num-\nber of studies using RL, so we group them here into main\ngeneral choices of state design. Some tasks used ordered\nwords (Li et al., 2018, paraphrase generation), predicted\nwords (Grissom II et al., 2014, simultaneous machine trans-\nlation), a vocabulary set and a taxonomy (Mao et al., 2018,\ntaxonomy induction) or a bag of sentences (Zeng et al.,\n2018, relation extraction). Coreference resolution adopted\npartially formed coreference chains (Stoyanov & Eisner,\n2012) or word embeddings and features (Clark & Manning,\n2016). For syntactic or semantic parsing, the parser con-\nﬁguration was used as set of discrete variables describing\nNatural Language State Representation\nthe state of a parse structure (Zhang & Chan, 2009; Jiang\net al., 2012; Lˆe & Fokkens, 2017), a concatenation of their\nrepresentation built by an LSTM (Naseem et al., 2019) or a\nquery for a knowledge base (Liang et al., 2017).\nA usual strategy to represent the state was handcrafting\nvectors composed of selected variables or metrics, for ex-\nample, similarity scores and number of words (Ling et al.,\n2017, text-based clinical diagnosis), conﬁdence scores, td-\nidf and one-hot encoding of matches (Narasimhan et al.,\n2016; Taniguchi et al., 2018, information extraction), situ-\national and linguistic information (Dethlefs & Cuay´ahuitl,\n2011, NLG), entitites and relations (Godin et al., 2019, ques-\ntion answering), probability distribution over a set of ob-\njects (Hu et al., 2018, question selection) or values generated\nby a parser (Wang et al., 2018, math word problem).\nThere has been a myriad of creative efforts to build represen-\ntations using neural networks, especially RNNs, as one can\nseamlessly regard the RNN as an agent and its hidden state\nas the environment state. The internal state (hidden vector\nand/or cell vector in LSTMs) was set as the environment\nstate in language modeling (Ranzato et al., 2015), image\ncaptioning (Rennie et al., 2017), math word problem (Huang\net al., 2018) and NLG (Yasui et al., 2019), sometimes with\nattention over hidden states of a sequence in grammatical\nerror correction (Sakaguchi et al., 2017) or a concatenation\nof hidden states further encoded by another neural network\nin sentence representation (Yogatama et al., 2017). A Trans-\nformer’s hidden states (Vaswani et al., 2017) were also used\nin machine translation (Wu et al., 2018). The encoding\nprovided by the output layer was used for semantic pars-\ning (Guu et al., 2017) and also with location-based attention\nfor text anonymization (Mosallanezhad et al., 2019). Hier-\narchical network representations combining word level and\nsentence level encodings were proposed, as in text classi-\nﬁcation (Zhang et al., 2018). Both the hidden state of the\nencoder and the decoder comprised the state in sentence\nsimpliﬁcation (Zhang & Lapata, 2017). CNNs represen-\ntations combined with predictive marginals were used in\nactive learning for NER (Fang et al., 2017).\n4. Concluding Discussion\nThe proliﬁc literature combining NLP and RL reveals a\nnoticeable synergy between them. While RL methods have\nextended frontiers of NLP research, natural language can be\na valuable source of information for RL agents, in particular\nto model the state signal. An abundance of tailored natural\nlanguage state representations has been explored and there\nis a current trend to opt for end-to-end models, encoding or\ndecoding text with the aid of RNNs and for multimodal or\nhierarchical representations, as shown in Figure 2.\nMore insight is needed on how natural language state rep-\nDialogue\nInstruction Following\nText-Based Games\nfeatures         tuples        slot-value \ninformation state      belief state             \nNN hidden state          multimodal   \nutterance embeddings   end-to-end\nNN hidden states \nmultimodal\ntuples \nsentence embeddings \nmultimodal\ndiscrete variables                  words \nsentences     handcrafted vectors \nNN hidden states        embeddings \nhierarchical\nText Summarization\nOther NLP Tasks\ntuples \nNN hidden states \nhierarchical\nFigure 2. Key concepts of state representation in each area.\nresentations should be put together. Unfortunately, it is not\nuncommon to ﬁnd papers with unclear or missing deﬁni-\ntions of their state signal or without much justiﬁcation of\ntheir choices, which hinders comparison and reproducibility.\nWe thus encourage researchers to carefully describe the state\nrepresentation, provide justiﬁcation for state design model-\ning and to also report attempts that had negative impact on\nthe agent’s learning process.\nStudies to validate the effectiveness of NL state representa-\ntion used to be common when engineering features was a\ntrend. Deep learning methods may give us the illusion that\nalmost no human intervention or manual feature selection\ntakes place, but they still play a relevant role in state signal\ndesign in terms of choice of architecture, model and input\ndata. Adjusting evaluation and ablation methods to new ap-\nproaches is thus necessary. Goyal et al. (2019a), for instance,\nexperiment with three different natural language instruc-\ntions representations, to test the effect of language-based\nreward. Similar analyses can also be done with language-\nbased states, as in Narasimhan et al. (2018).\nThere is cutting-edge research being conducted about the\ninterpretability of neural NLP models and their linguistic\nrepresentational power (Belinkov & Glass, 2019). Natu-\nral language state representation would thus proﬁt from\nstudies about interpretability and also from diagnostic re-\nsearch (Hupkes et al., 2018) on their abilities of distilling,\ncomposing and retaining semantic information throughout\nthe agent’s steps, usually expressed by recurrence in the\nneural networks.\nIn addition, grounding the meaning of texts to the dynamics\nof the environment, as discussed in Narasimhan et al. (2018),\nis a promising area of future research. The authors delineate\ntwo needs that must be met in state design with language\ngrounding: the representation should fuse different modal-\nities and capture the compositional nature of language in\norder to map semantics to the agent’s world.\nFinally, Narasimhan et al. (2015; 2018) discuss the beneﬁts\nof representations that are effective across different games\nand that enable policy transfer. Therefore, another desirable\nproperty of natural language state representation is cross-\ndomain validity, so that their encoding of world knowledge\ncan be exploited in various RL scenarios.\nNatural Language State Representation\nAcknowledgments\nWe thank the two anonymous reviewers for their feedback\nand suggestions.\nReferences\nAmmanabrolu, P. and Riedl, M. Playing text-adventure\ngames with graph-based deep reinforcement learning. In\nProceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pp. 3557–3565, Minneapolis, Min-\nnesota, June 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/N19-1358. URL https:\n//www.aclweb.org/anthology/N19-1358.\nAnderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M.,\nS¨underhauf, N., Reid, I., Gould, S., and van den Hengel,\nA. Vision-and-language navigation: Interpreting visually-\ngrounded navigation instructions in real environments. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 3674–3683, 2018.\nAnsari, G. A., Chandar, S., Ravindran, B., et al.\nLan-\nguage expansion in text-based games. arXiv preprint\narXiv:1805.07274, 2018.\nBelinkov, Y. and Glass, J.\nAnalysis methods in neu-\nral language processing: A survey.\nTransactions of\nthe Association for Computational Linguistics, 7:49–72,\nMarch 2019. doi: 10.1162/tacl a 00254. URL https:\n//www.aclweb.org/anthology/Q19-1004.\nB¨ohmer, W., Springenberg, J. T., Boedecker, J., Riedmiller,\nM., and Obermayer, K. Autonomous learning of state\nrepresentations for control: An emerging ﬁeld aims to au-\ntonomously learn state representations for reinforcement\nlearning agents from their real-world sensor observations.\nKI-K¨unstliche Intelligenz, 29(4):353–362, 2015.\nBranavan, S., Chen, H., Zettlemoyer, L., and Barzilay, R.\nReinforcement learning for mapping instructions to ac-\ntions. In Proceedings of the Joint Conference of the 47th\nAnnual Meeting of the ACL and the 4th International\nJoint Conference on Natural Language Processing of the\nAFNLP, pp. 82–90, Suntec, Singapore, August 2009. As-\nsociation for Computational Linguistics. URL https:\n//www.aclweb.org/anthology/P09-1010.\nBranavan, S., Zettlemoyer, L., and Barzilay, R. Reading\nbetween the lines: Learning to map high-level instruc-\ntions to commands. In Proceedings of the 48th Annual\nMeeting of the Association for Computational Linguistics,\npp. 1268–1277, 2010.\nChaplot, D. S., Sathyendra, K. M., Pasumarthi, R. K., Ra-\njagopal, D., and Salakhutdinov, R. Gated-attention archi-\ntectures for task-oriented language grounding. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence, 2018.\nChen, Y.-C. and Bansal, M. Fast abstractive summarization\nwith reinforce-selected sentence rewriting. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp.\n675–686, 2018.\nCho, K., van Merri¨enboer, B., Gulcehre, C., Bahdanau, D.,\nBougares, F., Schwenk, H., and Bengio, Y. Learning\nphrase representations using RNN encoder–decoder for\nstatistical machine translation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 1724–1734, 2014.\nClark, K. and Manning, C. D. Deep reinforcement learning\nfor mention-ranking coreference models. In Proceed-\nings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, pp. 2256–2262, Austin,\nTexas, November 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/D16-1245. URL https:\n//www.aclweb.org/anthology/D16-1245.\nCuay´ahuitl, H., Yu, S., Williamson, A., and Carse, J. Deep\nreinforcement learning for multi-domain dialogue sys-\ntems. arXiv preprint arXiv:1611.08675, 2016.\nDas, A., Kottur, S., Moura, J. M. F., Lee, S., and Batra,\nD. Learning cooperative visual dialog agents with deep\nreinforcement learning. In The IEEE International Con-\nference on Computer Vision (ICCV), Oct 2017.\nDe Bruin, T., Kober, J., Tuyls, K., and Babuˇska, R. Integrat-\ning state representation learning into deep reinforcement\nlearning. IEEE Robotics and Automation Letters, 3(3):\n1394–1401, 2018.\nDethlefs, N. and Cuay´ahuitl, H. Hierarchical reinforcement\nlearning and hidden Markov models for task-oriented\nnatural language generation. In Proceedings of the 49th\nAnnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies, pp. 654–\n659, Portland, Oregon, USA, June 2011. Association\nfor Computational Linguistics. URL https://www.\naclweb.org/anthology/P11-2115.\nDhingra, B., Li, L., Li, X., Gao, J., Chen, Y.-N., Ahmad, F.,\nand Deng, L. Towards end-to-end reinforcement learning\nof dialogue agents for information access. In Proceed-\nings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp.\n484–495, 2017.\nNatural Language State Representation\nEnglish, M. and Heeman, P. Learning mixed initiative di-\nalog strategies by using reinforcement learning on both\nconversants. In Proceedings of Human Language Tech-\nnology Conference and Conference on Empirical Meth-\nods in Natural Language Processing, pp. 1011–1018,\nVancouver, British Columbia, Canada, October 2005. As-\nsociation for Computational Linguistics. URL https:\n//www.aclweb.org/anthology/H05-1127.\nFang, M., Li, Y., and Cohn, T. Learning how to active learn:\nA deep reinforcement learning approach. In Proceedings\nof the 2017 Conference on Empirical Methods in Natural\nLanguage Processing, Copenhagen, Denmark, September\n2017. Association for Computational Linguistics. doi: 10.\n18653/v1/D17-1063. URL https://www.aclweb.\norg/anthology/D17-1063.\nFatemi, M., El Asri, L., Schulz, H., He, J., and Suleman,\nK. Policy networks with two-stage training for dialogue\nsystems. In Proceedings of the 17th Annual Meeting of\nthe Special Interest Group on Discourse and Dialogue,\npp. 101–110, 2016.\nFeng, W., Zhuo, H. H., and Kambhampati, S. Extracting\naction sequences from texts based on deep reinforcement\nlearning. In 27th International Joint Conference on Arti-\nﬁcial Intelligence, IJCAI 2018, pp. 4064–4070. Interna-\ntional Joint Conferences on Artiﬁcial Intelligence, 2018.\nFinney, S., Gardiol, N. H., Kaelbling, L. P., and Oates, T.\nThe thing that we tried didn’t work very well: Deictic\nrepresentation in reinforcement learning. In Proceedings\nof the Eighteenth conference on Uncertainty in artiﬁcial\nintelligence, pp. 154–161, 2002.\nFoerster, J., Assael, I. A., de Freitas, N., and Whiteson,\nS. Learning to communicate with deep multi-agent re-\ninforcement learning.\nIn Lee, D. D., Sugiyama, M.,\nLuxburg, U. V., Guyon, I., and Garnett, R. (eds.), Ad-\nvances in Neural Information Processing Systems 29, pp.\n2137–2145. Curran Associates, Inc., 2016.\nFrampton, M. and Lemon, O. Reinforcement learning of\ndialogue strategies using the users last dialogue act. In IJ-\nCAI Workshop on Knowledge and Reasoning in Practical\nDialogue Systems, 2005.\nFranc¸ois-Lavet, V., Bengio, Y., Precup, D., and Pineau, J.\nCombined reinforcement learning via abstract representa-\ntions. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pp. 3582–3589, 2019.\nFrommberger, L. Learning to behave in space: A qual-\nitative spatial representation for robot navigation with\nreinforcement learning. International Journal on Artiﬁ-\ncial Intelligence Tools, 17(03):465–482, 2008.\nFu, J., Korattikara, A., Levine, S., and Guadarrama, S. From\nlanguage to goals: Inverse reinforcement learning for\nvision-based instruction following. In International Con-\nference on Learning Representations, 2019.\nGeorgila, K., Henderson, J., and Lemon, O. Learning user\nsimulations for information state update dialogue systems.\nIn Ninth European Conference on Speech Communication\nand Technology, 2005.\nGodin, F., Kumar, A., and Mittal, A.\nLearning when\nnot to answer: A ternary reward structure for rein-\nforcement learning based question answering. In Pro-\nceedings of the 2019 Conference of the North Ameri-\ncan Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 2 (In-\ndustry Papers), pp. 122–129, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/N19-2016.\nURL https://www.\naclweb.org/anthology/N19-2016.\nGoyal, P., Niekum, S., and Mooney, R. J. Using natural\nlanguage for reward shaping in reinforcement learning.\nIn Proceedings of the Twenty-Eighth International Joint\nConference on Artiﬁcial Intelligence, IJCAI-19, pp. 2385–\n2391. International Joint Conferences on Artiﬁcial Intelli-\ngence Organization, 7 2019a. doi: 10.24963/ijcai.2019/\n331. URL https://doi.org/10.24963/ijcai.\n2019/331.\nGoyal, P., Niekum, S., and Mooney, R. J. Using natural\nlanguage for reward shaping in reinforcement learning.\nIn Proceedings of the 28th International Joint Conference\non Artiﬁcial Intelligence, pp. 2385–2391. AAAI Press,\n2019b.\nGrissom II, A., He, H., Boyd-Graber, J., Morgan, J., and\nDaum´e III, H. Don’t until the ﬁnal verb wait: Rein-\nforcement learning for simultaneous machine transla-\ntion. In Proceedings of the 2014 Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP),\npp. 1342–1352, Doha, Qatar, October 2014. Associ-\nation for Computational Linguistics.\ndoi: 10.3115/\nv1/D14-1140. URL https://www.aclweb.org/\nanthology/D14-1140.\nGuu, K., Pasupat, P., Liu, E., and Liang, P.\nFrom lan-\nguage to programs: Bridging reinforcement learning\nand maximum marginal likelihood. In Proceedings of\nthe 55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pp.\n1051–1062, Vancouver, Canada, July 2017. Associa-\ntion for Computational Linguistics.\ndoi: 10.18653/\nv1/P17-1097.\nURL https://www.aclweb.org/\nanthology/P17-1097.\nNatural Language State Representation\nHe, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Os-\ntendorf, M. Deep reinforcement learning with a natural\nlanguage action space. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pp. 1621–1630, Berlin,\nGermany, August 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/P16-1153. URL https:\n//www.aclweb.org/anthology/P16-1153.\nHeeman, P. A. Representing the reinforcement learning\nstate in a negotiation dialogue. In 2009 IEEE Workshop\non Automatic Speech Recognition & Understanding, pp.\n450–455. IEEE, 2009.\nHeeman, P. A., Fryer, J., Lunsford, R., Rueckert, A., and\nSelfridge, E. Using reinforcement learning for dialogue\nmanagement policies: Towards understanding mdp viola-\ntions and convergence. In Thirteenth Annual Conference\nof the International Speech Communication Association,\n2012.\nHenderson, J., Lemon, O., and Georgila, K. Hybrid rein-\nforcement/supervised learning for dialogue policies from\ncommunicator data. In IJCAI workshop on knowledge\nand reasoning in practical dialogue systems, pp. 68–75,\n2005.\nHenß, S., Mieskes, M., and Gurevych, I.\nA reinforce-\nment learning approach for adaptive single-and multi-\ndocument summarization. In GSCL, pp. 3–12, 2015.\nHermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,\nSoyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,\nM., Teplyashin, D., et al. Grounded language learning in\na simulated 3d world. arXiv preprint arXiv:1706.06551,\n2017.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\nHu, H., Wu, X., Luo, B., Tao, C., Xu, C., Wu, W., and\nChen, Z. Playing 20 question game with policy-based\nreinforcement learning. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, pp. 3233–3242, Brussels, Belgium, October-\nNovember 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/D18-1361. URL https:\n//www.aclweb.org/anthology/D18-1361.\nHuang, D., Liu, J., Lin, C.-Y., and Yin, J. Neural math\nword problem solver with reinforcement learning. In\nProceedings of the 27th International Conference on\nComputational Linguistics, pp. 213–223, Santa Fe, New\nMexico, USA, August 2018. Association for Compu-\ntational Linguistics.\nURL https://www.aclweb.\norg/anthology/C18-1018.\nHupkes, D., Veldhoen, S., and Zuidema, W. Visualisation\nand ‘diagnostic classiﬁers’ reveal how recurrent and re-\ncursive neural networks process hierarchical structure.\nJournal of Artiﬁcial Intelligence Research, 61:907–926,\n2018.\nJain, V., Fedus, W., Larochelle, H., Precup, D., and Belle-\nmare, M. G. Algorithmic improvements for deep re-\ninforcement learning applied to interactive ﬁction. In\nThirty-Fourth AAAI Conference on Artiﬁcial Intelligence\n(forthcoming), 2020.\nJanner, M., Narasimhan, K., and Barzilay, R. Representation\nlearning for grounded spatial reasoning. Transactions of\nthe Association for Computational Linguistics, 6:49–61,\n2018.\nJiang, J., Teichert, A., Eisner, J., and Daume, H. Learned pri-\noritization for trading off accuracy and speed. In Pereira,\nF., Burges, C. J. C., Bottou, L., and Weinberger, K. Q.\n(eds.), Advances in Neural Information Processing Sys-\ntems 25, pp. 1331–1339. Curran Associates, Inc., 2012.\nJones, M. and Canas, F. Integrating reinforcement learning\nwith models of representation learning. In Proceedings\nof the Annual Meeting of the Cognitive Science Society,\nvolume 32, 2010.\nJonschkowski, R. and Brock, O. Learning task-speciﬁc\nstate representations by maximizing slowness and pre-\ndictability. In 6th international workshop on evolutionary\nand reinforcement learning for autonomous robot systems\n(ERLARS), 2013.\nKaelbling, L. P., Littman, M. L., and Cassandra, A. R. Plan-\nning and acting in partially observable stochastic domains.\nArtiﬁcial intelligence, 101(1-2):99–134, 1998.\nKaplan, R., Sauer, C., and Sosa, A. Beating Atari with\nnatural language guided reinforcement learning. arXiv\npreprint arXiv:1704.05539, 2017.\nKhouzaimi, H., Laroche, R., and Lef`evre, F.\nOptimis-\ning turn-taking strategies with reinforcement learning.\nIn Proceedings of the 16th Annual Meeting of the Spe-\ncial Interest Group on Discourse and Dialogue, pp. 315–\n324, Prague, Czech Republic, September 2015. Asso-\nciation for Computational Linguistics. doi: 10.18653/\nv1/W15-4643. URL https://www.aclweb.org/\nanthology/W15-4643.\nKry´sci´nski, W., Paulus, R., Xiong, C., and Socher, R. Im-\nproving abstraction in text summarization. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 1808–1817, 2018.\nNatural Language State Representation\nLˆe, M. and Fokkens, A. Tackling error propagation through\nreinforcement learning: A case of greedy dependency\nparsing.\nIn Proceedings of the 15th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers, pp. 677–\n687, Valencia, Spain, April 2017. Association for Com-\nputational Linguistics. URL https://www.aclweb.\norg/anthology/E17-1064.\nLee, G. H. and Lee, K. J. Automatic text summarization\nusing reinforcement learning with embedding features.\nIn Proceedings of the Eighth International Joint Confer-\nence on Natural Language Processing (Volume 2: Short\nPapers), pp. 193–197, 2017.\nLesort, T., D´ıaz-Rodr´ıguez, N., Goudou, J.-F., and Filliat,\nD. State representation learning for control: An overview.\nNeural Networks, 108:379–392, 2018.\nLevin, E., Pieraccini, R., and Eckert, W. Using Markov\ndecision process for learning dialogue strategies. In Pro-\nceedings of the 1998 IEEE International Conference on\nAcoustics, Speech and Signal Processing, ICASSP’98\n(Cat. No. 98CH36181), volume 1, pp. 201–204. IEEE,\n1998.\nLevin, E., Pieraccini, R., and Eckert, W. A stochastic model\nof human-machine interaction for learning dialog strate-\ngies. IEEE Transactions on speech and audio processing,\n8(1):11–23, 2000.\nLi, J., Monroe, W., Ritter, A., Jurafsky, D., Galley, M.,\nand Gao, J. Deep reinforcement learning for dialogue\ngeneration.\nIn Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Processing,\npp. 1192–1202, Austin, Texas, November 2016. Asso-\nciation for Computational Linguistics. doi: 10.18653/\nv1/D16-1127. URL https://www.aclweb.org/\nanthology/D16-1127.\nLi, X., Chen, Y.-N., Li, L., Gao, J., and Celikyilmaz, A.\nEnd-to-end task-completion neural dialogue systems. In\nProceedings of the Eighth International Joint Conference\non Natural Language Processing (Volume 1: Long Pa-\npers), pp. 733–743, 2017.\nLi, Z., Jiang, X., Shang, L., and Li, H. Paraphrase generation\nwith deep reinforcement learning.\nIn Proceedings of\nthe 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 3865–3878, Brussels, Belgium,\nOctober-November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1421. URL https:\n//www.aclweb.org/anthology/D18-1421.\nLiang, C., Berant, J., Le, Q., Forbus, K. D., and Lao,\nN.\nNeural symbolic machines:\nLearning semantic\nparsers on Freebase with weak supervision.\nIn Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Pa-\npers), pp. 23–33, Vancouver, Canada, July 2017. Asso-\nciation for Computational Linguistics. doi: 10.18653/\nv1/P17-1003.\nURL https://www.aclweb.org/\nanthology/P17-1003.\nLing, J. and Rush, A. M. Coarse-to-ﬁne attention mod-\nels for document summarization. In Proceedings of the\nWorkshop on New Frontiers in Summarization, pp. 33–42,\n2017.\nLing, Y., Hasan, S. A., Datla, V., Qadir, A., Lee, K., Liu, J.,\nand Farri, O. Learning to diagnose: Assimilating clinical\nnarratives using deep reinforcement learning. In Pro-\nceedings of the Eighth International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers),\npp. 895–905, Taipei, Taiwan, November 2017. Asian Fed-\neration of Natural Language Processing. URL https:\n//www.aclweb.org/anthology/I17-1090.\nLipton, Z., Li, X., Gao, J., Li, L., Ahmed, F., and Deng,\nL. BBQ-networks: Efﬁcient exploration in deep rein-\nforcement learning for task-oriented dialogue systems. In\nThirty-Second AAAI Conference on Artiﬁcial Intelligence,\n2018.\nLiu, B. and Lane, I. Iterative policy learning in end-to-end\ntrainable task-oriented neural dialog models. In 2017\nIEEE Automatic Speech Recognition and Understanding\nWorkshop (ASRU), pp. 482–489. IEEE, 2017.\nLuketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas,\nJ., Grefenstette, E., Whiteson, S., and Rockt¨aschel, T. A\nsurvey of reinforcement learning informed by natural lan-\nguage. In Proceedings of the 28th International Joint Con-\nference on Artiﬁcial Intelligence, pp. 6309–6317. AAAI\nPress, 2019.\nMahmud, M. Constructing states for reinforcement learning.\nIn Proceedings of the 27th International Conference on\nMachine Learning (ICML-10), pp. 727–734, 2010.\nMaillard, O.-A., Ryabko, D., and Munos, R. Selecting\nthe state-representation in reinforcement learning. In\nAdvances in Neural Information Processing Systems, pp.\n2627–2635, 2011.\nManuvinakurike, R., DeVault, D., and Georgila, K. Us-\ning reinforcement learning to model incrementality in a\nfast-paced dialogue game. In Proceedings of the 18th\nAnnual SIGdial Meeting on Discourse and Dialogue, pp.\n331–341, Saarbr¨ucken, Germany, August 2017. Asso-\nciation for Computational Linguistics. doi: 10.18653/\nv1/W17-5539. URL https://www.aclweb.org/\nanthology/W17-5539.\nNatural Language State Representation\nMao, Y., Ren, X., Shen, J., Gu, X., and Han, J. End-to-\nend reinforcement learning for automatic taxonomy in-\nduction. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pp. 2462–2472, Melbourne, Aus-\ntralia, July 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/P18-1229. URL https:\n//www.aclweb.org/anthology/P18-1229.\nMcCallum, R. A. Hidden state and reinforcement learning\nwith instance-based state identiﬁcation. IEEE Transac-\ntions on Systems, Man, and Cybernetics, Part B (Cyber-\nnetics), 26(3):464–473, 1996.\nMisra, D., Langford, J., and Artzi, Y. Mapping instructions\nand visual observations to actions with reinforcement\nlearning.\nIn Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing, pp.\n1004–1015, 2017.\nMitchell, C., Boyer, K., and Lester, J.\nEvaluating\nstate representations for reinforcement learning of turn-\ntaking policies in tutorial dialogue.\nIn Proceedings\nof the SIGDIAL 2013 Conference, pp. 339–343, Metz,\nFrance, August 2013. Association for Computational\nLinguistics.\nURL https://www.aclweb.org/\nanthology/W13-4052.\nMorales, E. F. Relational state abstractions for reinforce-\nment learning. In Proceedings of the ICML04 workshop\non Relational Reinforcement Learning, 2004.\nMosallanezhad, A., Beigi, G., and Liu, H. Deep reinforce-\nment learning-based text anonymization against private-\nattribute inference. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pp.\n2360–2369, Hong Kong, China, November 2019. As-\nsociation for Computational Linguistics. doi: 10.18653/\nv1/D19-1240. URL https://www.aclweb.org/\nanthology/D19-1240.\nMurugesan, K., Atzeni, M., Shukla, P., Sachan, M., Kapa-\nnipathi, P., and Talamadupula, K. Enhancing text-based\nreinforcement learning agents with commonsense knowl-\nedge. arXiv preprint arXiv:2005.00811, 2020.\nNarasimhan, K., Kulkarni, T., and Barzilay, R. Language\nunderstanding for text-based games using deep reinforce-\nment learning. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing,\npp. 1–11, Lisbon, Portugal, September 2015. Associ-\nation for Computational Linguistics.\ndoi: 10.18653/\nv1/D15-1001. URL https://www.aclweb.org/\nanthology/D15-1001.\nNarasimhan, K., Yala, A., and Barzilay, R.\nImproving\ninformation extraction by acquiring external evidence\nwith reinforcement learning. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language\nProcessing, pp. 2355–2365, Austin, Texas, November\n2016. Association for Computational Linguistics. doi: 10.\n18653/v1/D16-1261. URL https://www.aclweb.\norg/anthology/D16-1261.\nNarasimhan, K., Barzilay, R., and Jaakkola, T. Ground-\ning language for transfer in deep reinforcement learning.\nJournal of Artiﬁcial Intelligence Research, 63:849–874,\n2018.\nNarayan, S., Cohen, S. B., and Lapata, M. Ranking sen-\ntences for extractive summarization with reinforcement\nlearning. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pp. 1747–1759, 2018.\nNaseem, T., Shah, A., Wan, H., Florian, R., Roukos, S.,\nand Ballesteros, M.\nRewarding Smatch: Transition-\nbased AMR parsing with reinforcement learning.\nIn\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pp. 4586–4592,\nFlorence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1451. URL https:\n//www.aclweb.org/anthology/P19-1451.\nOh, J., Singh, S., Lee, H., and Kohli, P. Zero-shot task gen-\neralization with multi-task deep reinforcement learning.\nIn Proceedings of the 34th International Conference on\nMachine Learning - Volume 70, ICML17, pp. 26612670.\nJMLR.org, 2017.\nOrtiz, J., Balazinska, M., Gehrke, J., and Keerthi, S. S.\nLearning state representations for query optimization\nwith deep reinforcement learning. In Proceedings of the\nSecond Workshop on Data Management for End-To-End\nMachine Learning, pp. 1–4, 2018.\nPaek, T. Reinforcement learning for spoken dialogue sys-\ntems: Comparing strengths and weaknesses for practical\ndeployment. In Proc. Dialog-on-Dialog Workshop, Inter-\nspeech. Citeseer, 2006.\nPapangelis, A.\nA comparative study of reinforcement\nlearning techniques on dialogue management. In Pro-\nceedings of the Student Research Workshop at the 13th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pp. 22–31, Avi-\ngnon, France, April 2012. Association for Computa-\ntional Linguistics.\nURL https://www.aclweb.\norg/anthology/E12-3003.\nNatural Language State Representation\nPasunuru, R. and Bansal, M. Multi-reward reinforced sum-\nmarization with saliency and entailment. In Proceedings\nof the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 2 (Short Papers),\npp. 646–653, 2018.\nPaulus, R., Xiong, C., and Socher, R.\nA deep rein-\nforced model for abstractive summarization.\nIn In-\nternational Conference on Learning Representations,\n2018. URL https://openreview.net/forum?\nid=HkAClQgA-.\nRanzato, M., Chopra, S., Auli, M., and Zaremba, W. Se-\nquence level training with recurrent neural networks.\narXiv preprint arXiv:1511.06732, 2015.\nRennie, S. J., Marcheret, E., Mroueh, Y., Ross, J., and Goel,\nV. Self-critical sequence training for image captioning.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), July 2017.\nRioux, C., Hasan, S. A., and Chali, Y. Fear the reaper:\nA system for automatic multi-document summarization\nwith reinforcement learning. In Proceedings of the 2014\nconference on empirical methods in natural language\nprocessing (EMNLP), pp. 681–690, 2014.\nRoy, N., Gordon, G., and Thrun, S. Finding approximate\nPOMDP solutions through belief compression. Journal\nof artiﬁcial intelligence research, 23:1–40, 2005.\nRyang, S. and Abekawa, T. Framework of automatic text\nsummarization using reinforcement learning. In Proceed-\nings of the 2012 Joint Conference on Empirical Methods\nin Natural Language Processing and Computational Nat-\nural Language Learning, pp. 256–265. Association for\nComputational Linguistics, 2012.\nSakaguchi, K., Post, M., and Van Durme, B. Grammatical er-\nror correction with neural reinforcement learning. In Pro-\nceedings of the Eighth International Joint Conference on\nNatural Language Processing (Volume 2: Short Papers),\npp. 366–372, Taipei, Taiwan, November 2017. Asian Fed-\neration of Natural Language Processing. URL https:\n//www.aclweb.org/anthology/I17-2062.\nSchefﬂer, K. and Young, S. Automatic learning of dia-\nlogue strategy using dialogue simulation and reinforce-\nment learning. In Proceedings of the second international\nconference on Human Language Technology Research,\npp. 12–19. Citeseer, 2002.\nSingh, S., Kearns, M., Litman, D. J., Walker, M. A., et al.\nEmpirical evaluation of a reinforcement learning spoken\ndialogue system. In AAAI/IAAI, pp. 645–651, 2000a.\nSingh, S. P., Kearns, M. J., Litman, D. J., and Walker, M. A.\nReinforcement learning for spoken dialogue systems. In\nAdvances in Neural Information Processing Systems, pp.\n956–962, 2000b.\nStoyanov, V. and Eisner, J. Easy-ﬁrst coreference resolu-\ntion. In Proceedings of COLING 2012, pp. 2519–2534,\nMumbai, India, December 2012. The COLING 2012 Or-\nganizing Committee. URL https://www.aclweb.\norg/anthology/C12-1154.\nSu, P.-H., Gasic, M., Mrksic, N., Rojas-Barahona, L., Ultes,\nS., Vandyke, D., Wen, T.-H., and Young, S. Continuously\nlearning neural dialogue management. arXiv preprint\narXiv:1606.02689, 2016.\nSutton, R. and Barto, A.\nReinforcement Learning: An\nIntroduction. A Bradford book. MIT Press, 1998. ISBN\n9780262193986.\nTaniguchi, M., Miura, Y., and Ohkuma, T.\nJoint mod-\neling for query expansion and information extraction\nwith reinforcement learning. In Proceedings of the First\nWorkshop on Fact Extraction and VERiﬁcation (FEVER),\npp. 34–39, Brussels, Belgium, November 2018. Asso-\nciation for Computational Linguistics. doi: 10.18653/\nv1/W18-5506. URL https://www.aclweb.org/\nanthology/W18-5506.\nTetreault, J. R. and Litman, D. J. A reinforcement learning\napproach to evaluating state representations in spoken\ndialogue systems. Speech Communication, 50(8-9):683–\n696, 2008.\nVan Otterlo, M. Relational representations in reinforcement\nlearning: Review and open problems. In Proceedings of\nthe ICML, volume 2, 2002.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nVogel, A. and Jurafsky, D. Learning to follow navigational\ndirections. In Proceedings of the 48th Annual Meeting of\nthe Association for Computational Linguistics, pp. 806–\n814. Association for Computational Linguistics, 2010.\nWalker, M. A. An application of reinforcement learning to\ndialogue strategy selection in a spoken dialogue system\nfor email. Journal of Artiﬁcial Intelligence Research, 12:\n387–416, 2000.\nWang, L., Zhang, D., Gao, L., Song, J., Guo, L., and Shen,\nH. T. MathDQN: Solving arithmetic word problems via\ndeep reinforcement learning.\nIn Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\nNatural Language State Representation\nWang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D.,\nWang, Y.-F., Wang, W. Y., and Zhang, L. Reinforced\ncross-modal matching and self-supervised imitation learn-\ning for vision-language navigation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pp. 6629–6638, 2019.\nWang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D.,\nWang, Y., Wang, W., and Zhang, L. Vision-language navi-\ngation policy learning and adaptation. IEEE Transactions\non Pattern Analysis and Machine Intelligence, pp. 1–1,\n2020.\nWeisz, G., Budzianowski, P., Su, P.-H., and Gaˇsi´c, M. Sam-\nple efﬁcient deep reinforcement learning for dialogue\nsystems with large action spaces. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing, 26\n(11):2083–2097, 2018.\nWen, T.-H., Miao, Y., Blunsom, P., and Young, S. Latent\nintention dialogue models. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume\n70, pp. 3732–3741. JMLR. org, 2017a.\nWen, T.-H., Vandyke, D., Mrkˇsi´c, N., Gasic, M., Barahona,\nL. M. R., Su, P.-H., Ultes, S., and Young, S. A network-\nbased end-to-end trainable task-oriented dialogue system.\nIn Proceedings of the 15th Conference of the European\nChapter of the Association for Computational Linguistics:\nVolume 1, Long Papers, pp. 438–449, 2017b.\nWilliams, J. D. and Zweig, G. End-to-end LSTM-based dia-\nlog control optimized with supervised and reinforcement\nlearning. arXiv preprint arXiv:1606.01269, 2016.\nWilliams, J. D., Atui, K. A., and Zweig, G. Hybrid code net-\nworks: Practical and efﬁcient end-to-end dialog control\nwith supervised and reinforcement learning. In Proceed-\nings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp.\n665–677, 2017.\nWu, L., Tian, F., Qin, T., Lai, J., and Liu, T.-Y. A study\nof reinforcement learning for neural machine transla-\ntion. In Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pp. 3612–\n3621, Brussels, Belgium, October-November 2018. As-\nsociation for Computational Linguistics. doi: 10.18653/\nv1/D18-1397. URL https://www.aclweb.org/\nanthology/D18-1397.\nWu, Y. and Hu, B. Learning to extract coherent summary\nvia deep reinforcement learning. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\nYang, K., Kong, X., Wang, Y., Zhang, J., and De Melo, G.\nReinforcement learning over knowledge graphs for ex-\nplainable dialogue intent mining. IEEE Access, 8:85348–\n85358, 2020.\nYao, K., Zhang, L., Luo, T., and Wu, Y. Deep reinforce-\nment learning for extractive document summarization.\nNeurocomputing, 284:52–62, 2018.\nYasui, G., Tsuruoka, Y., and Nagata, M. Using seman-\ntic similarity as reward for reinforcement learning in\nsentence generation.\nIn Proceedings of the 57th An-\nnual Meeting of the Association for Computational Lin-\nguistics: Student Research Workshop, pp. 400–406, Flo-\nrence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-2056. URL https:\n//www.aclweb.org/anthology/P19-2056.\nYogatama, D., Blunsom, P., Dyer, C., Grefenstette, E., and\nLing, W. Learning to compose words into sentences with\nreinforcement learning. In 5th International Conference\non Learning Representations (ICLR 2017). International\nConference on Learning Representations, 2017.\nYuan, X., Cˆot´e, M.-A., Sordoni, A., Laroche, R., Combes,\nR. T. d., Hausknecht, M., and Trischler, A. Counting\nto explore and generalize in text-based games. arXiv\npreprint arXiv:1806.11525, 2018.\nZeng, X., He, S., Liu, K., and Zhao, J. Large scaled relation\nextraction with reinforcement learning. In Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, 2018.\nZhang, L. and Chan, K. P. Dependency parsing with energy-\nbased reinforcement learning.\nIn Proceedings of the\n11th International Conference on Parsing Technologies\n(IWPT’09), pp. 234–237, Paris, France, October 2009. As-\nsociation for Computational Linguistics. URL https:\n//www.aclweb.org/anthology/W09-3838.\nZhang, T., Huang, M., and Zhao, L. Learning structured\nrepresentation for text classiﬁcation via reinforcement\nlearning. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, 2018.\nZhang, X. and Lapata, M. Sentence simpliﬁcation with\ndeep reinforcement learning. In Proceedings of the 2017\nConference on Empirical Methods in Natural Language\nProcessing, pp. 584–594, 2017.\nZhao, T. and Eskenazi, M. Towards end-to-end learning for\ndialog state tracking and management using deep rein-\nforcement learning. In Proceedings of the 17th Annual\nMeeting of the Special Interest Group on Discourse and\nDialogue, pp. 1–10, 2016.\nNatural Language State Representation\nZhong, V., Rockt¨aschel, T., and Grefenstette, E. RTFM:\nGeneralising to new environment dynamics via reading.\nIn International Conference on Learning Representations,\n2020.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-07-19",
  "updated": "2020-07-19"
}