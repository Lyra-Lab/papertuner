{
  "id": "http://arxiv.org/abs/2007.10040v1",
  "title": "Knowledge Graph Extraction from Videos",
  "authors": [
    "Louis Mahon",
    "Eleonora Giunchiglia",
    "Bowen Li",
    "Thomas Lukasiewicz"
  ],
  "abstract": "Nearly all existing techniques for automated video annotation (or captioning)\ndescribe videos using natural language sentences. However, this has several\nshortcomings: (i) it is very hard to then further use the generated natural\nlanguage annotations in automated data processing, (ii) generating natural\nlanguage annotations requires to solve the hard subtask of generating\nsemantically precise and syntactically correct natural language sentences,\nwhich is actually unrelated to the task of video annotation, (iii) it is\ndifficult to quantitatively measure performance, as standard metrics (e.g.,\naccuracy and F1-score) are inapplicable, and (iv) annotations are\nlanguage-specific. In this paper, we propose the new task of knowledge graph\nextraction from videos, i.e., producing a description in the form of a\nknowledge graph of the contents of a given video. Since no datasets exist for\nthis task, we also include a method to automatically generate them, starting\nfrom datasets where videos are annotated with natural language. We then\ndescribe an initial deep-learning model for knowledge graph extraction from\nvideos, and report results on MSVD* and MSR-VTT*, two datasets obtained from\nMSVD and MSR-VTT using our method.",
  "text": "Knowledge Graph Extraction from Videos\nLouis Mahon1, Eleonora Giunchiglia1, Bowen Li1, and Thomas Lukasiewicz1\n1Computer Science Department, Oxford University\nAbstract—Nearly all existing techniques for automated video\nannotation (or captioning) describe videos using natural language\nsentences. However, this has several shortcomings: (i) it is\nvery hard to then further use the generated natural language\nannotations in automated data processing, (ii) generating natural\nlanguage annotations requires to solve the hard subtask of\ngenerating semantically precise and syntactically correct natural\nlanguage sentences, which is actually unrelated to the task of\nvideo annotation, (iii) it is difﬁcult to quantitatively measure\nperformance, as standard metrics (e.g., accuracy and F1-score)\nare inapplicable, and (iv) annotations are language-speciﬁc.\nIn this paper, we propose the new task of knowledge graph\nextraction from videos, i.e., producing a description in the form\nof a knowledge graph of the contents of a given video. Since\nno datasets exist for this task, we also include a method to\nautomatically generate them, starting from datasets where videos\nare annotated with natural language. We then describe an initial\ndeep-learning model for knowledge graph extraction from videos,\nand report results on MSVD* and MSR-VTT*, two datasets\nobtained from MSVD and MSR-VTT using our method.\nI. INTRODUCTION\nRecent progress in deep learning shows exciting poten-\ntial for visual understanding. Promising results have been\nproduced in the tasks of video annotation (or captioning)\nusing natural language (see, e.g., [1]–[3]) and video question\nanswering (see, e.g., [4]–[6]).\nHowever, annotations written in natural language (NL) are\nnot ideal, for several reasons. Firstly, it is difﬁcult to use\nNL sentences in subsequent automated data processing tasks.\nOne example of such data processing is database search. If\nwe have a database of videos annotated with NL and want\nto search for all videos depicting a man, we cannot simply\nperform string matching on the annotations. This would, e.g.,\nproduce a false positive for the annotation “sailors man the\nship”, and a false negative for “a ﬁreﬁghter puts on his\ncoat”. Another example of automated data processing is logical\ninference. For example, if an NL annotation model correctly\nidentiﬁes that there is a man throwing a ball in an image,\nit cannot then conclude that there is a male throwing a ball\nin the image. That is, a logical inference rule of the form\n∀x man(x) ⇒male(x) cannot easily be applied. Automated\ninference in NL is known to be a difﬁcult problem [8], [9].\nAgain, string matching would not provide a solution, as the\nabove examples attest. A second drawback of NL annotations\nis that they require more work than just understanding the\ncontents to be annotated. Traditionally, text generation has\nbeen divided into a semantic stage of determining what to\nsay, and then a realization stage of determining how to say\nit [10], [11]. The former corresponds to understanding the\ncontents being annotated and is the task that we are interested\nin, but deep learning NL annotation models must, at the same\ntime, learn to perform the latter, which means learning the\nlanguage’s complex meaning and grammatical structure. This\nis irrelevant to the goal of annotation and so needlessly makes\nthe task more difﬁcult. Thirdly, it is not easy to interpret and\nquantitatively measure the performance of NL annotations.\nThe agreement between a ground-truth and the predicted\nsentence cannot be measured by standard metrics, such as\naccuracy and F1-score. Instead, they require specially designed\nones (e.g., BLEU [12], METEOR [13], and LEPOR [14]).\nRecognizing the imperfection of each of these, results typically\nreport scores on multiple metrics, none of which have a simple\nand intuitive interpretation. This makes it difﬁcult to evaluate\nhow well a model is performing. A fourth problem is that\nNL annotations are speciﬁc to a single language. A model\ntrained to produce annotations in English cannot produce\nannotations in German, and it is non-trivial to translate the\nEnglish annotations into German ones. This is a particular\ndrawback for low-resource languages.\nReplacing NL annotations with knowledge graphs avoids\nall the above problems. A knowledge graph (KG) speciﬁes\nthe individuals (i.e., the objects) present in the video and the\nfacts that hold true of these individuals. A fact expresses a\nrelation between two individuals (e.g., fold(person, paper))\nor an attribute of an individual (e.g., white(paper)). A visual\nrepresentation of a KG is given in Fig. 1, where nodes\nrepresenting individuals are depicted in red, nodes represent-\ning attributes are depicted in blue, and nodes representing\nrelations are depicted in green. KGs are machine-readable,\nso enable automated data processing, such as searching for\nall videos depicting a man or applying the inference rule\n∀x man(x)\n⇒\nmale(x). They are determined only by\nthe semantics of the video contents, and do not involve\na semantically and syntactically complex natural language\nstructure. They can be evaluated using standard machine-\nlearning metrics, such as accuracy and F1-score, which makes\nit easy to interpret performance. For each data point, the\nground truth and the prediction are sets of facts, so, e.g., the\nF1-score can be computed by counting the facts that appear\nin both as the true positives, those that appear in the former\nonly as false negatives, etc. Finally, each fact, and hence each\nKG, can be trivially translated by translating one component\nat a time. For example, we can translate white(paper) to\nGerman by translating white to weiss and paper to Papier,\ngiving weiss(Papier). (As discussed in Sections II and V, the\ncomponents of each fact denote particular senses of words, so\narXiv:2007.10040v1  [cs.CL]  20 Jul 2020\n(1) MSR-VTT ground truth:\n(7) Visual representation of the\na person folds a piece of white paper\npredicted knowledge graph:\n(2) Predicted caption from [7]:\nperson\nman\npaper\norigami\nfold\nwhite\ndemonstrate\nfold\nattributes\nobjects\nrelations\nLegend:\na person is folding a piece of paper\n(3) MSR-VTT* ground truth:\nfold(person, piece)\n(4) Predicted individuals:\nperson, man, paper, origami\n(5) Predicted facts:\ndemonstrate(person), fold(person, paper) white(paper), fold(man, origami)\n(6) Some inferred facts:\nchange(person, paper), fold(male, origami)\nFig. 1. The ﬁrst frame from a video in MSR-VTT*, with (1) the ground-truth natural language annotation in MSR-VTT, (2) the natural language annotation\nproduced by [7], (3) the ground-truth set of facts in MSR-VTT*, (4) the individuals predicted by our system, (5) the facts predicted by our system, and\n(6) some further inferences that can be made on top of these facts.\nword-sense disambiguation is not an issue.) For these reasons,\nwe propose to produce annotations using KGs rather than NL.\nThese advantages have begun to be recognized in neigh-\nbouring areas. The entire task of open information extraction\nis motivated by use cases that can only be met by a structured\nrepresentation of information, and not by the same information\nexpressed in natural language. In computer vision, researchers\nhave started to argue for the importance of basic visual\nreasoning (see, e.g., [15]) and the ability to leverage external\nknowledge (see, e.g., [16]). A number of works aim to extract a\nstructured description from an input image to reason about the\nidentiﬁed objects (see, e.g., [17]). In the video understanding\nﬁeld, there have been only preliminary attempts [18], though,\nrecently, [19] has sketched some planned future work to\nmanually create a dataset of videos annotated with facts.\nIn this paper, we describe a method for automatically gener-\nating datasets of videos and corresponding KGs. The method\nuses a rule-based semantic parser, based on the Stanford parser\n[20]. The generated datasets are sufﬁcient for the development\nof KG extraction models. Additionally, we introduce one such\nmodel, trained and tested on two generated datasets, which\nachieves a superior performance to [18], the only existing work\nto attempt the same task (F1-score of 14.0 vs. 6.1). We also\ncompare to an artiﬁcial baseline, conduct ablation experiments\nthat demonstrate the efﬁcacy of each of the model’s compo-\nnents, and present qualitative results. This model enjoys all\nthe advantages described above: it facilitates automated data\nprocessing, it does not require modelling complex NL syntax\nand semantics, its performance is easy to evaluate, and the\nresulting annotations are easy to translate.\nExample 1: Fig. 1 shows an example where the input is\na video of someone folding paper. Given this video, our\nmodel detects the contained individuals, namely, person, man,\npaper, and origami. Then, it predicts which facts are true of\nthese individuals: demonstrate(person), fold(person, paper),\nwhite(paper), and fold(man, origami). As an example of\nautomated data processing, it may then exploit an ontology to\npredict more facts that are true for the video, e.g., the rule ∀x, y\n(fold(x, y) ⇒change(x, y)) to infer the fact change(person,\npaper). The corresponding KG is shown in (7).\n◁\nThe main contributions of this paper are brieﬂy as follows.\n• We propose the task of extracting KGs from videos. We\nprovide arguments for the superiority of this approach to\nNL annotations, and support these arguments empirically.\n• We describe a method for automatically generating\ndatasets of videos and corresponding KGs, and describe\nits application to generate two datasets.\n• We introduce a model for KG extraction, trained and\ntested on the above two datasets. This outperforms the\nexisting work to attempt the same task. We also compare\nto an artiﬁcial baseline and conduct ablation studies on\nthe main components of the model.\nThe rest of this paper is organized as follows. In Section II,\nwe present our method for automatically generating datasets.\nIn Section III, we describe our KG extraction model, and then\nreport experimental results in Section IV. Section V provides\na further discussion, and Section VI summarizes our main\ncontributions and gives an outlook on future work.\nII. DATASET GENERATION\nIn the absence of appropriate datasets, we devise an auto-\nmated method to generate them. There are several NL-anno-\ntated datasets, which we refer to as video-captioning datasets.\nOur method begins with these datasets and converts the NL\nannotations into sets of facts (which are equivalent to KGs).\nWe now describe this method with reference to its application\nto two well-known video-captioning datasets: MSVD [21] and\nMSR-VTT [22]. We denote the generated datasets MSVD*\nand MSR-VTT*, respectively, and Section IV reports the\nresults of our proposed model on these datasets.\nIn both MSVD and MSR-VTT, the training examples are\ncomposed of a video and a NL sentence describing the\ncontents of that video. We parse these sentences using a rule-\nbased parser based on the Stanford NLP syntactic parser [20].\n2\nThis produces a dependency parse of the sentence, which\nidentiﬁes the part of speech for each word, and the syntactic\nrelations that hold between them. We then apply a sequence\nof rules to extract the semantic information from the sentence,\nand use it to form a KG, which is composed of a set of\nfacts. Each fact contains a predicate and the corresponding\narguments: ⟨subject, predicate, object⟩triples in the case of\nbinary predicates, and ⟨subject, predicate⟩pairs in the case\nof unary predicates (see Figs. 1 and 3). A full description of\nthis parser can be found in the appendix.\nThe next step is to link all predicates, and individuals to\nentities in an ontology. The ontology that we use here is\nWordNet [23]. The linking method is context-sensitive, i.e.,\nfor a given word, it ﬁnds the most suitable WordNet synset\nusing the surrounding words in the NL annotation. This is done\nby comparing the word vectors for the surrounding words to\nthe word vectors for the synset deﬁnition. Again, full details\nare given in the appendix. Linking to an ontology formalizes\nthe vocabulary and allows the application of inference rules to\naugment the information produced directly by our annotation\nsystem. It also means that the components of our KGs corre-\nspond to particular senses of words, rather than the words\nthemselves. For example, “bank” referring to the ﬁnancial\ninstitution, and “bank” referring to the side of a river would\nbe linked to different WordNet synsets, and so would appear\nas different items in a KG.\nWords that appear only a small number of times across the\ndataset would be difﬁcult to reach a good performance on.\nFor this reason, we exclude all words that appear fewer than\n50 times and also some common semantically weak verbs,\nas detailed in the appendix.\nThe method described so far produces only non-negated\nfacts, from here on referred to as T. This means a model\ncould learn to simply predict every potential fact to be true.\nTo prevent this, we use the local closed-world assumption [24]\nto create a set of negated facts, from here on referred to as F,\nwhich the model must also learn to predict. For each fact in\nthe description, we obtain a corrupted version by replacing\nthe predicate with a different predicate from the vocabulary.\nFor example, the fact fold(person, paper), which appears in\nFig. 1, could be corrupted to ¬throw(person, paper). This\nnegated fact is then added to F.\nAs a ﬁnal step, we merge all KGs for each video by taking\nthe union of the facts of their parses. In MSVD and MSR-VTT,\neach video appears with multiple captions, and each produces\na separate training example. In MSVD* and MSR-VTT*, each\nvideo appears in only one training example. After exclusion\nand merging, the numbers of distinct individuals, distinct\npredicates, and data points with at least one fact or negated\nfact are as reported in Table I.\nIII. OUR MODEL\nIn this section, we describe a deep learning model (illus-\ntrated in Fig. 2) for extracting KGs from videos. We ﬁrst\ndescribe a general method for extracting a KG from an\narbitrary input, and then detail how this method applies to\nthe case where the inputs are videos.\nA. Knowledge Graph Extraction Model\nGiven a set of possible inputs X (videos in our case)\nand a knowledge base KB (of ontological domain knowledge\nabout X), let our vocabulary consist of the set P of all\npredicates that appear in KB and the set A of all individuals\nthat appear in KB. Then, the neural architecture for KG\nextraction consists of the following four components:\n1) an encoder f : X →Z,\n2) a multi-classiﬁer g: Z →(0, 1)|A|,\n3) a set of predicate multilayer perceptrons (MLPs): {mp |\np ∈P},\n4) a set of trainable individual vectors: {va | a ∈A},\nwhere Z is the space of extracted feature vectors.\nThe data for training the above neural architecture consists\nof 4-tuples (x, c, T, F), where x ∈X is the input to be\nannotated (a 4D video tensor in our case), c ⊆A is the set of\nindividuals that are present in the input, and T (resp., F) is a\nset of facts (resp., negated facts) containing these individuals.\nExample 2: Consider again the video in Fig. 1. The training\ntuple associated with the video is\n(x, {person, piece}, {fold(person, piece)}, F) ,\nwhere x represents the video itself, and F is a set of negated\nfacts not containing ¬fold(person, piece). An example of a\nnegated fact that belongs to F is ¬throw(person, piece).\n◁\nDuring training, we ﬁrst compute the feature vector\nf(x) = e ∈Z. Then, we feed this to the multi-classiﬁer to\npredict the individuals present in x, ˆc = g(e), and compute\na binary cross-entropy loss (BCE) for each individual in the\nvocabulary. This gives\nLc = P|A|\nj=0 Lbin(ˆcj, cj) ,\nwhere cj ∈{0, 1} and ˆcj ∈(0, 1) are the ground truth and\nprediction as to whether the jth individual in the vocabulary\nis present, respectively, and Lbin represents BCE. We also\nproduce predictions for the facts and negated facts in T and\nF by selecting the individual vectors corresponding to the\nindividuals in the ground truth, and feeding these vectors\n(along with the video encoding) to the predicate MLPs cor-\nresponding to the predicates in the ground truth. That is, for\neach fact t ∈T with t = p(a, b), we compute a prediction\nˆt = mp(e, va, vb), and similarly for negated facts. A naive\nbrute-force method would be to run every predicate MLP on\nevery individual vector. This would require |P| × |A| forward\npasses for every video. Selecting only the present individuals\nreduces this to |P| × n, where n is the number of individuals\nin the input video, which is typically less than three.\nThe vectors va and vb are initialized to the corresponding\nword2vec word vectors [25]. For example, vman is initialized\nto the word2vec vector for the word “man”. These vectors are\nupdated during training along with the network weights.\n3\nTABLE I\nNUMBERS OF DISTINCT PREDICATES AND INDIVIDUALS THAT WERE INCLUDED IN OUR FINAL DATASETS. PREDICATES AND\nINDIVIDUALS WERE INCLUDED IF THEY APPEARED AT LEAST 50 TIMES. THE FINAL COLUMN SHOWS THE NUMBER OF DATA POINTS\nTHAT REMAINED WITH NON-EMPTY CAPTIONS EVEN AFTER INFREQUENTLY OCCURRING WORDS WERE EXCLUDED.\nNum Training Examples\nNum Individuals\nNum Attributes\nNum Relations\nNum Facts\nNum Non-empty Training Examples\nMSVD*\n1970\n122\n48\n69\n117\n1800\nMSR-VTT*\n10000\n372\n235\n113\n348\n9802\nThe training data contains many more negated facts than\nfacts. To counteract this imbalance, we weigh the loss for\nfacts and negated facts inversely to how many there are. The\nprediction loss is again calculated using BCE, this time applied\nto each fact (resp., negated fact). Formally,\nLp = −\n1\n2|T |\nP\nt∈T log(˜t) −\n1\n2|F |\nP\nf∈F log(1 −˜f) .\nThe total backpropagated loss is then a simple summation:\nL = Lc + Lp .\nAt inference time, given an input x, we ﬁrst compute the\nfeature vector (step (i)), then threshold the outputs of the multi-\nclassiﬁer to obtain the set I of predicted individuals (step (ii)):\nˆI = {a ∈A | g(f(x))a > 0.5} .\nAs we no longer have access to the ground truth set of individ-\nuals, we instead select the predicted individuals and pass them\nto the predicate-MLPs (step (iii)). This again avoids having\nto run all predicate-MLPs on all individuals. Speciﬁcally, to\npredict the unary relations that hold of these individuals, we\nrun all predicate-MLPs in our vocabulary on all individual\nvectors corresponding to individuals in ˆI, and to predict the\nbinary relations, we do the same on all pairs of individuals\nin ˆI. The total set ˆT of predicted facts is the union of the two:\nU = {p(a) | p ∈P, a ∈ˆI, mp(a) > 0.5},\nB = {p(a, b) | p ∈P, a, b ∈ˆI, mp(a, b) > 0.5},\nˆT = U ∪B.\nNow, as an example of automated information processing, we\ncan apply the additional knowledge from KB to the predicted\nfacts ˆT to obtain an additional set of inferred facts T ′:\nU ′ = {p(a) | p ∈P, a ∈ˆI, KB ∪ˆT |= p(a)},\nB′ = {p(a, b) | p ∈P, a, b ∈ˆI, KB ∪ˆT |= p(a, b)},\nT ′ = U ′ ∪B′.\nThe annotation is then updated with these inferences (step (iv)):\nˆT ←ˆT ∪T ′.\nExample 3: Consider again the video in Fig. 1, and the\nschema of the model in Fig. 2. Following the steps in Fig. 2,\nthe KG for the video in Fig. 1 is produced in four steps.\n(i) The input video x is mapped into a ﬁxed-length feature\nvector e by the encoder f.\n(ii) The encoding e is fed into a multi-classiﬁer MLP g,\nwhich returns the probability that each individual in the\nvocabulary is present in x. The probabilities are then\nthresholded at 0.5, and we obtain the set\nI = {person, man, paper, origami}.\n(iii) Consider the individual paper. The encoding e, together\nwith the vector vpaper is passed through all the MLPs,\nand a fact is returned only when the prediction of the\nMLP is greater than 0.5, such as white(paper). The\nsame procedure is then repeated for each predicted in-\ndividual, and each ordered pair of predicted individuals,\nproducing\nˆT = {demonstrating(man), white(paper),\nfold(person, paper), fold(man, origami)} .\n(iv) All facts that can be inferred from KB and the pre-\ndicted facts are used to augment the KG extracted\nfrom the video. Examples of such inferred facts are\nchange(person, paper) and fold(male, origami).\n◁\nB. KG Extraction from Videos\nThe above method can be used to extract a KG from any\nunstructured data X. For example, X may contain sequences\nof word embeddings representing text, spectrograms represent-\ning audio, or image tensors representing still images. In each\ncase, we only have to choose an encoder f, so that f(x) is a\nﬁxed-length feature vector for each x ∈X. We now describe\nhow this method is applied when the input domain is videos.\nOur encoder, f, is composed of a convolutional neural\nnetwork, followed by a recurrent neural network. Given a\nvideo x = x1, x2, . . . , xn, we ﬁrst process each video frame\nxi using a pre-trained frozen copy of VGG19 [26] and take\nas ith frame encoding, ζi, the output of the 3rd last layer.\n(This is a standard approach taken by others in the ﬁeld; e.g.,\n[27]–[29].) The sequence of frame encodings ζ1, . . . , ζn is then\npassed through a gated recurrent unit (GRU) [30] to produce\na sequence ζ1, . . . , ζn. As a second stream, we also compute\nthe feature vector from a frozen copy of the I3D network [31].\nThe ﬁnal output of the encoder is the concatenation of this I3D\nfeature vector and a weighted sum of the ζi’s, weighted by\na learnable n-dimensional vector w. Using a vector notation,\nwith the VGG network applied along the ﬁrst dimension, our\nencoder f is deﬁned by:\nf(x) = w.GRU(V GG(x)) · I3D(x) .\n(1)\nThe MLPs for each predicate have one hidden layer, as does\nthe multi-classiﬁer. The input size of the multi-classiﬁer is the\n4\n(iii)\n(iv)\n(ii)                                      \n(i)                     \nVideo\n0.17 \n0.03 \n0.04 \n0.21\n0.91\n0.45\n0.84\nIndividual\nVector\nwoman\n(1.646 -1.285  0.35  0.353 ...  1.250)\nperson\n(-0.257 -0.424 0.041 -0.063 ... 1.030)\ntable\n(1.266 -0.556 0.438 -0.727 ... 0.8905)\ncarrot\n(-0.041 -0.418 0.634 -1.029 ... -1.001)\ndog\n(-2.231 -0.348 0.298 -3.125  ... 0.833)\npaper\n(1.855 -0.065 -0.764  1.159  ... 1.030)\ncar\n(-1.187 -0.424 0.128  0.128  ... 1.0300)\nbicycle\n(1.471 -0.407 -0.080 -1.399 ... -1.686)\nhat\n(0.9066 -1.7541 -1.6476  1.3443  ... 0.4825)\ncar\n(0.0496  1.6049 -0.5890 -0.1140  ... 0.4000)\n-0.257 \n-0.424 \n0.041 \n-0.063 \n... \n1.030\n1.855 \n-0.065 \n-0.764  \n1.159  \n... \n1.030\n1.855 \n-0.065 \n-0.764  \n1.159  \n... \n1.030\n0.96\n1.855 \n-0.065 \n-0.764  \n1.159  \n... \n1.030\n0.11\n-0.257 \n-0.424 \n0.041 \n-0.063 \n... \n1.030\n1.855 \n-0.065 \n-0.764  \n1.159  \n... \n1.030\n-0.257 \n-0.424 \n0.041 \n-0.063 \n... \n1.030\npaper\nperson\npaper\nFrom step (iii): \nwhite(paper), \nfold(man,origami) \nfold(person,paper), \ndemonstrate(person) \nInference rules:  \nman(x)       male(x) \nfold(x,y)       change(x,y)\nInferences: \nfold(male,origami), \nchange(person,paper)  \nFinal annotation: \nwhite(paper), \nfold(person,paper), \nfold(male,origami), \nchange(person,paper), \nchange(man,origami), \ndemonstrate(person)\nVideo Encoder\nMulti- \nclassifier\npaper\nperson\nperson\npaper\n0.01\n0.34\n0.05\n0.82\nEncoding\nFig. 2. Depiction of how the neural architecture processes a video of a man folding a piece of paper (as in Fig. 1). The four stages are: (i) encode the video,\n(ii) predict individuals, (iii) predict facts, and (iv) add facts inferred from KB.\nsize of the video encoding, denoted dim(f(x)). The predicate-\nMLPs have input size dim(f(x)) + D in the case of unary\nrelations and dim(f(x)) + 2D in the case of binary relations,\nwhere D is the size of the individuals’ vectors (300 in our\ncase). Using the encoder deﬁned in (1), dim(f(x)) = 4096 +\n1024 = 5120. We restrict our attention to unary and binary\npredicates, but the framework can naturally be extended to\ninclude higher-order predicates.\nIV. EXPERIMENTAL RESULTS\nIn this section, we provide quantitative experimental results\nfor the two datasets MSVD* and MSR-VTT* and compare\nthese results. We also report on ablation studies for our neural\narchitecture and give some further qualitative results.\nA. Quantitative Results\nTables II and III display our performance on the two gener-\nated datasets MSVD* and MSR-VTT*, respectively, reporting\nboth overall accuracy and F1-score. For each training example,\nthese datasets contain many more negated facts than facts (see\nSection II for details). This is the reason for such a large\ndifference between accuracy and F1-score: the predictions are\ndominated by true negatives, which increase overall accuracy\nbut have no effect on the F1-score. Accuracy and F1-score are\nharsh metrics, as they pertain to whole facts. An annotation\nwould achieve 0 accuracy if it contained correct individuals\nand correct predicates, but related them in the wrong way, e.g.,\nfold(paper, person) or carry(person, paper) in Fig. 1. All\nresults are taken from a held-out test set, using the train/val/test\nsplits deﬁned in the original MSVD and MSR-VTT datasets.\nThere are limited existing baselines (only [18]) for the task\nof KG extraction from videos. To benchmark our performance,\nwe also compare to an artiﬁcial baseline composed of the\nTABLE II\nRESULTS ON THE MSVD* VIDEO AND DERIVED KGS, AS\nDESCRIBED IN SECTION II. THE BEST RESULTS ARE IN BOLD.\nF1-score\nPositive\nAccuracy\nNegative\nAccuracy\nTotal\nAccuracy\nOur Model\n13.99\n12.65\n99.20\n22.16\nBaseline\n13.5\n7.53\n99.96\n25.55\nV&L 2018\n6.11\n3.36\n-\n-\nTABLE III\nRESULTS ON THE MSR-VTT* VIDEO AND DERIVED KGS, AS\nDESCRIBED IN SECTION II. THE BEST RESULTS ARE IN BOLD.\nF1-score\nPositive\nAccuracy\nNegative\nAccuracy\nTotal\nAccuracy\nOur Model\n8.90\n7.33\n99.48\n59.19\nBaseline\n11.83\n6.76\n99.96\n83.01\nvideo-captioning network from [7], and the semantic parser\nused to create our datasets (described brieﬂy in Section II,\nand more in detail in the appendix).\nOur model signiﬁcantly outperforms the previous work by\n[18], and performs on par with the video-captioning baseline.\nImportantly, it shows the best positive accuracy, which is the\nmost difﬁcult metric to score highly on.\nThe metrics, both for the baseline and for our network, are\nlower on MSR-VTT*. This is consistent with other reported\nresults in the literature. For example [7], [29], [32]–[34] all\nreport lower video captioning performance on MSR-VTT than\non MSVD. A likely reason is the greater vocabulary size\nof MSR-VTT: 29,316 vs. 13,010 for MSVD (ﬁgures taken\nfrom [22]). Although we exclude all but the most common\n5\nTABLE IV\nABLATION RESULTS FOR MSVD*. THE BEST RESULTS ARE IN\nBOLD.\nF1\nPositive\nAccuracy\nNegative\nAccuracy\nTotal\nAccuracy\nOur Model\n13.99\n12.65\n99.20\n22.16\nSingle MLP decoder\n13.75\n12.5\n98.45\n19.04\nSingle individual\nvector decoder\n10.72\n9.91\n99.32\n22.50\nWithout encoder\n8.87\n9.77\n97.49\n15.60\nindividuals, a greater vocabulary size in the original NL\ncaptions would still make our task more difﬁcult. It means\nthat more facts will be excluded, and so there will be more\nannotations with missing information, which do not, therefore,\nfully describe the input video.\nB. Ablation Studies\nTo investigate the contribution of each part of our neural\narchitecture, we perform ablation studies on the encoder f,\npredicate MLPs and the individual vector ({mp | p ∈P} and\n{Va | a ∈A} in the terminology of Section III).\nThe results for MSVD* and MSR-VTT* are shown in\nTables IV and V, respectively. In the “without encoder” setting,\nthe video encoding feature vector is replaced with a randomly\ngenerated vector. This tests whether the system is actually\nusing information from the video or is just predicting common\nindividuals and predicates, e.g., talk(man). That the results in\nthis setting are worse shows that information from the video\nis indeed being used. In the “single MLP decoder” setting,\none MLP is used for all predicates, in contrast to the main\nsystem in which each predicate has its own MLP. This is to\ntest whether predicate-speciﬁc information is being used, e.g.,\nwhether the system makes meaningfully different predictions\nfor throws(woman, ball) and sees(woman, ball). Similarly,\nin the single-individual decoder setting, one vector is used for\nall individuals, in contrast to the main system in which each\nindividual has its own vector. This tests if individual-speciﬁc\ninformation has been learned by the individual vectors. Both\ndecoder ablation settings produce worse results, which shows\nthat the predicate-speciﬁc MLP and individual-speciﬁc vectors\nhave learned some meaningful semantics of their respective\npredicates and individuals. Note that the results suffer more\nin these ablation settings on MSR-VTT* than on MSVD*.\nThis is to be expected, as MSR-VTT* contains a larger\nvocabulary, which allows for more semantic differentiation\nbetween different predicates and between different individuals.\nC. Qualitative Results\nTo further evaluate the quality of our KG extractions, we\npresent some manual inspections of images and the predicted\nfacts. Fig. 3 shows the ﬁrst frame from an MSVD* video with\nour predicted facts, and the same for a video from MSR-VTT*.\n(Fig. 1 shows another from MSR-VTT*.)\nWe can now infer additional facts from KB and the facts\ngenerated by the video annotation network, which is one of\nTABLE V\nABLATION RESULTS FOR MSR-VTT*. THE BEST RESULTS ARE IN\nBOLD.\nF1\nPositive\nAccuracy\nNegative\nAccuracy\nTotal\nAccuracy\nOur Model\n8.90\n7.33\n99.48\n59.19\nSingle MLP decoder\n6.16\n6.75\n99.15\n49.09\nSingle individual\nvector decoder\n3.84\n3.94\n99.51\n58.72\nWithout encoder\n5.00\n5.36\n99.19\n49.64\nthe advantages of our approach of using KGs. The class\nman is a subclass of person and male in WordNet, and\nwe can apply this inference to all facts mentioning a man,\nproducing stand(male) and stand(person) in Fig. 3. If we\nwanted to determine how many videos in a database depicted\nat least one person, or how many depict males, or how many\ndepict males sitting vs. standing, this would not be possible\nby merely annotating the videos with NL sentences. It is,\non the other hand, possible with our system, because of the\ninferences that can be made on top of a KG. Finally, the\nqualitative examples show the limitations imposed by the\nsmaller vocabulary size in MSVD*. As discussed in Section II,\nwe exclude individuals and relations that appear fewer than 50\ntimes across the dataset. This means that there is sometimes\ninsufﬁcient material to fully describe a video. For example,\nthe man in the video shown in Fig. 3 is eating a banana, and\none of the MSVD captions expressed this. However, banana\nappears fewer than 50 times, so it is excluded from our training\ndata, and we cannot predict eat(man, banana). (Recall from\nSection II that there are multiple annotations for each video,\nand we merge all of them in our dataset. The annotation shown\nin Fig. 3 is one that was not excluded.)\nV. FURTHER DISCUSSION\nIn this section, we further discuss the advantages of gener-\nating KGs compared to NL annotations with reference to the\noutput of our model. We also examine our model as compared\nto the video-captioning baseline.\nA. Value of KGs\nOne advantage of KGs is that they enable the application of\nautomated data processing. An example is the application of\ninference rules to infer additional facts, which is illustrated by\nthe results shown in Fig. 3. The facts predicted by our model\n(3) allow the inference of further information (6), whereas\nthe NL annotation produced by [7] does not, even when it\nis exactly correct, as in the case of the MSR-VTT video.\nAs well as producing additional information about the video\ncontents, these inferences are especially useful for producing\nabstract classes and relations, such as male in Fig. 3 or change\nFig. 1. Other examples of automated data processing are query\nanswering and search. If we are interested in videos that depict\na male changing some object in some way, as happens in the\nvideo shown in Fig. 1, then using the extracted KGs (including\nfurther inferences), we can simply return the videos where,\n6\n(1) MSVD ground truth:\n(1) MSR-VTT ground truth:\na man is standing at the bus stop\na man is driving a car\n(2) Predicted caption from [7]:\n(2) Predicted caption from [7]:\na man is talking on the phone\na man is driving a car\n(3) MSVD* ground truth:\n(3) MSR-VTT* ground truth:\nstand(man)\ndrive(man, car)\n(4) Predicted individuals:\n(4) Predicted individuals:\nman\nman, car, person\n(5) Predicted facts:\n(5) Predicted facts:\nstand(man), sit(man)\ndrive(man, car), drive(person, car)\n(6) Some inferred facts:\n(6) Some inferred facts:\nstand(person), stand(male)\ndrive(man, vehicle)\nFig. 3.\nLeft: the ﬁrst frame from a video in the MSVD* test set, with the ground-truth caption, the caption produced by [7], the facts predicted by our\nsystem, and some further inferences made using KB and these facts. Compared with a video captioning system, our output can be used to infer additional\ninformation about the video, such as the fact that it contains a male. Right: the second frame (ﬁrst is very unclear) from a video in the MSR-VTT* test set,\nwith the ground-truth caption, the caption produced by [7], the facts predicted by our system, and some further inferred facts. Although the video captioning\nmodel produces a sentence similar to the ground truth, it cannot be used for inference and so misses the additional inferred information.\nfor some individual x, the fact change(male, x) has been\npredicted. In contrast, NL annotations do not enable us to\nperform such a search. We cannot simply replace all occur-\nrences of man, boy, guy etc. with male, and all occurrences\nof fold, open, close, etc. with change and then count the\nannotations that contain both male and change. This would\nproduce false positives with annotations like “a man is handed\na folded sheet of paper”. In such a sentence, although male and\nchange would both be present, they are not connected in the\nway that we are interested in. The sentence does not describe\na scenario in which a male is changing something, even\nthough it contains the words man and fold. It is non-trivial\nto represent the information encoded in a NL sentence as a\nset of individuals and relations that hold between members of\nthis set. To apply such a translation, we would need to employ\na semantic parser, i.e., we would be using the method of the\nvideo-captioning baseline, and the demerits of this approach\nare discussed below. String matching in NL is not even able\nto detect the individuals that are present. The sentence “a\nperson folds a boy’s school uniform” could describe a video in\nwhich no boy is present. Our annotations represent information\nin the semantically structured format of a KG, from which\ninformation such as the above can be easily read off. Finally,\nwe can see that the set of facts (equivalent to a KG) shown\nin Fig. 3 could be easily represented in a language other\nthan English. The individuals and predicates in the vocabulary\ncorrespond to particular senses of particular words, rather\nthan entire words, so disambiguating polysemous words is\nnot required. To translate, e.g., the fact drive(man, car) to\nGerman, we could simply look up the translation for the\nsenses of each of these three words: drive →fahren, man\n→Mann, and car →Auto. Translating one item at a time,\nwe then get fahren(Mann, Auto). In contrast, translating\nthe NL sentence would require both the disambiguation of\npolysemous words, and the extraction of English syntactic\nrelations and grammatical inﬂections, and then the expression\nof these relations and inﬂections in German.\nB. Comparison with Video-Captioning Baseline\nOne possible method of producing KGs, and hence the ad-\nvantages discussed above, is to ﬁrst produce an NL annotation\nand then semantically parse this NL sentence to convert it\nto a set of facts. This is the method employed in the video-\ncaptioning baseline, to which we compare our performance in\nSection IV. The quantitative comparison, across the multiple\nmetrics and datasets, shows that it performs about equally well\nto our model. However, there are three reasons to believe that\nthe latter is more promising for future progress. Firstly, the\nvideo-captioning baseline must solve two problems to produce\nan annotation: the problem of annotating videos with NL, and\nthe problem of semantically parsing NL. This means it has\ntwo potential sources of error. However, errors in the second\nstage (that of semantic parsing) are not properly penalized\nin the results in Tables II and III, because the baseline\nuses the same semantic parser as was used to create the\ndataset. Therefore, if the video-captioning network correctly\npredicts the original NL caption, the result, after parsing,\nwill automatically be correct according to our dataset. This\nwill happen even if there was a mistake in the semantic\nparse. For example, in Fig. 1, the NL annotation predicted\nby [7] is “a person is folding a piece of paper”, which the\nparser then, incorrectly, parses as fold(person, piece) (instead\nof fold(person, paper)). However, this same incorrect parse\nwas made when creating the dataset, so the ground truth in\nMSR-VTT* is also fold(person, piece), and the baseline is\nmarked correct in this prediction. If there existed a dataset of\nvideos with human-made KGs, the baseline would still require\na semantic parser, and its accuracy would be limited by the\nparsing accuracy, whereas our model would not.\nA second advantage of our model is that we can restrict\nthe vocabulary of our annotations when desired. We may, on\noccasion, only be interested in a certain set of individuals, and\nwe could easily modify our network to only output annotations\nconcerning these individuals. If, for example, we were only in-\nterested in determining whether a video contained any animals\nand what those animals were doing, we could remove all facts\n7\nfrom our dataset other than those pertaining to animals and\nthen train our model on the modiﬁed dataset. This would be\na simpler task and be expected to give a higher accuracy. The\nvideo-captioning baseline, however, would still output an NL\nsentence as an intermediary step, and it would not be simple\nto apply the same restriction to this space of NL sentences.\nOne may think that the vocabulary of the output language\nmodel in the video-captioning network could be restricted\nto only a certain, animal-related vocabulary. However, many\nwords that have one meaning denoting an animal have also\nother meanings, e.g., “ﬂy” or “bear”. Again, this highlights\nthe complexities of working with NL and so the undesirability\nof using it as an intermediary step to produce a KG.\nThirdly, the captioning and parsing approach is only pos-\nsible where there already exist networks mapping the input\ndomain to NL. This is true for videos, but need not be true\nin general. Our method, on the other hand, can be applied to\nany input domain, as long as a suitable encoder is used.\nVI. CONCLUSION\nThis paper has proposed the task of KG extraction from\nvideos, where a KG is composed of a set of facts that describes\nrelations holding between individuals. We have provided ar-\nguments and empirical support for the advantages of KGs\nover NL annotations. No datasets currently exist of videos and\ncorresponding KGs, so we have proposed a method to generate\nthem from existing datasets of videos and NL annotations.\nFurther, we have introduced a deep learning model for KG\nextraction, and evaluated its performance both qualitatively\nand quantitatively on two generated datasets.\nFuture work includes exploring KG extraction from other\ninput domains. Of particular interest is the application to text,\nwhere our model would perform a task akin to open informa-\ntion extraction. Another extension is to manually construct a\ndataset designed speciﬁcally for the purpose of KG extraction,\nrather than using those generated by our automated method.\nREFERENCES\n[1] J. Zhang and Y. Peng, “Hierarchical vision-language alignment for video\ncaptioning,” in MultiMedia Modeling.\nSpringer, 2019, pp. 42–54.\n[2] L. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, “Video captioning\nwith attention-based LSTM and semantic consistency,” IEEE T. Multi-\nmedia, vol. 19, no. 9, pp. 2045–2055, 2017.\n[3] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, “End-to-end\ndense video captioning with masked transformer,” in Proc. CVPR, 2018,\npp. 8739–8748.\n[4] Z. Zhao, Q. Yang, D. Cai, X. He, and Y. Zhuang, “Video question\nanswering via hierarchical spatio-temporal attention networks,” in Proc.\nIJCAI, 2017, pp. 3518–3524.\n[5] K. Zeng, T. Chen, C. Chuang, Y. Liao, J. C. Niebles, and M. Sun,\n“Leveraging video descriptions to learn video question answering,” in\nProc. AAAI, 2017, pp. 4334–4340.\n[6] J. Lei, L. Yu, M. Bansal, and T. L. Berg, “TVQA: localized, compo-\nsitional video question answering,” in Proc. EMNLP, 2018, pp. 1369–\n1379.\n[7] S. Olivastri, G. Singh, and F. Cuzzolin, “An end-to-end baseline for\nvideo captioning,” arXiv preprint arXiv:1904.02628, 2019.\n[8] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large\nannotated corpus for learning natural language inference,” arXiv preprint\narXiv:1508.05326, 2015.\n[9] Y. Belinkov, A. Poliak, S. M. Shieber, B. V. Durme, and A. M. Rush,\n“Don’t take the premise for granted: Mitigating artifacts in natural\nlanguage inference,” in Proc. ACL, 2019, pp. 877–891.\n[10] K. R. McKeown, “Discourse strategies for generating natural-language\ntext,” Artiﬁcial intelligence, vol. 27, no. 1, pp. 1–41, 1985.\n[11] W. J. Levelt, Speaking: From intention to articulation.\nMIT press,\n1993, vol. 1.\n[12] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: A method for\nautomatic evaluation of machine translation,” in Proc. ACL, 2002, pp.\n311–318.\n[13] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT\nevaluation with improved correlation with human judgments,” in Proc.\nof the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures\nfor Machine Translation and/or Summarization, 2005, pp. 65–72.\n[14] A. L. Han, D. F. Wong, and L. S. Chao, “Lepor: A robust evaluation\nmetric for machine translation with augmented factors,” in Proc. COL-\nING, Posters, 2012, pp. 441–450.\n[15] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick,\nand R. B. Girshick, “CLEVR: A diagnostic dataset for compositional\nlanguage and elementary visual reasoning,” in Proc. CVPR, 2016, pp.\n1988–1997.\n[16] P. Wang, Q. Wu, C. Shen, A. R. Dick, and A. van den Hengel, “FVQA:\nFact-based visual question answering,” IEEE TPAMI, vol. 40, pp. 2413–\n2427, 2016.\n[17] P. Wang, Q. Wu, C. Shen, A. Dick, and A. Van Den Henge, “Explicit\nknowledge-based reasoning for visual question answering,” in Proc.\nIJCAI, 2017, pp. 1290–1296.\n[18] D. Vasile and T. Lukasiewicz, “Learning structured video descriptions:\nAutomated video knowledge extraction for video understanding tasks,”\nin On the Move to Meaningful Internet Systems. OTM 2018 Conferences,\n2018, pp. 315–332.\n[19] K. Curtis, G. Awad, S. Rajput, and I. Soboroff, “HLVU: A new challenge\nto test deep understanding of movies the way humans do,” in Proc.\nICMR, 2020, pp. 355–361.\n[20] P. Qi, T. Dozat, Y. Zhang, and C. D. Manning, “Universal dependency\nparsing from scratch,” in Proc. CoNLL, 2018, pp. 160–170.\n[21] D. L. Chen and W. B. Dolan, “Collecting highly parallel data for\nparaphrase evaluation,” in Proc. HTM, 2011, pp. 190–200.\n[22] J. Xu, T. Mei, T. Yao, and Y. Rui, “MSR-VTT: A large video description\ndataset for bridging video and language,” in Proc. CVPR, 2016, pp.\n5288–5296.\n[23] G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller,\n“Introduction to WordNet: An on-line lexical database,” International\nJournal of Lexicography, vol. 3, no. 4, pp. 235–244, 1990.\n[24] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy,\nT. Strohmann, S. Sun, and W. Zhang, “Knowledge Vault: A web-scale\napproach to probabilistic knowledge fusion,” in Proc. ACM SIGKDD,\n2014, pp. 601–610.\n[25] Q. Le and T. Mikolov, “Distributed representations of sentences and\ndocuments,” in Proc. ICML, 2014, pp. 1188–1196.\n[26] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[27] Y. Pan, T. Yao, H. Li, and T. Mei, “Video captioning with transferred\nsemantic attributes,” in Proc. CVPR, 2017, pp. 6504–6512.\n[28] Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui, “Jointly modeling embedding\nand translation to bridge video and language,” in Proc. CVPR, 2016, pp.\n4594–4602.\n[29] X. Zhang, K. Gao, Y. Zhang, D. Zhang, J. Li, and Q. Tian, “Task-\ndriven dynamic fusion: Reducing ambiguity in video description,” in\nProc. CVPR, 2017, pp. 3713–3721.\n[30] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, “On the\nproperties of neural machine translation: Encoder-decoder approaches,”\nin Proc. SSST@EMNLP, 2014, pp. 103–111.\n[31] J. Carreira and A. Zisserman, “Quo vadis, action recognition? A new\nmodel and the kinetics dataset,” in Proc. CVPR, 2017, pp. 6299–6308.\n[32] J. Xu, T. Yao, Y. Zhang, and T. Mei, “Learning multimodal attention\nLSTM networks for video captioning,” in Proc. ACM MM, 2017, pp.\n537–545.\n[33] Y. Chen, S. Wang, W. Zhang, and Q. Huang, “Less is more: Picking\ninformative frames for video captioning,” in Proc. ECCV, 2018, pp.\n358–373.\n[34] B. Wang, L. Ma, W. Zhang, and W. Liu, “Reconstruction network for\nvideo captioning,” in Proc. CVPR, 2018, pp. 7622–7631.\n8\n[35] K. Droganova and D. Zeman, “Towards deep universal dependencies,”\nin Proceedings of the Fifth International Conference on Dependency\nLinguistics (Depling, SyntaxFest 2019), 2019, pp. 144–152.\n[36] A. Rademaker and F. Tyers, Proceedings of the 3rd Workshop on\nUniversal Dependencies (UDW, SyntaxFest 2019), 2019.\n9\nVII. APPENDIX\nTo create the datasets MSVD* and MSR-VTT*, as de-\nscribed in Section II, we use a semantic parser to convert the\nnatural language annotations into logical annotations. There\nare four steps to this conversion: parsing individual sentences,\nlinking entities to the ontology (in our case, WordNet [23]),\nmerging the multiple annotations of each video and ﬁltering\nout uncommon entities. They are described, respectively, in\nthe 3 following subsections.\nA. Parsing Natural Language Sentences\nFirst, the input natural language sentence is parsed using\nthe Stanford parser. This identiﬁes the part of speech of each\nword and constructs a grammatical dependency tree. In a\ngrammatical dependency tree, a single word is identiﬁed as\nthe root, and all other words are marked depending on their\nsyntactic relation to their parent in the tree. The information\nproduced by the Stanford Parser, for our purposes, can be\nthought of as marking each word in the input sentence\nwith (i) a part of speech, (ii) another word in the sentence\ndesignated as the parent (unless this word is the root), and\n(iii) a grammatical dependency relation to the parent. An\nexample of such a relation is ‘nsubj’, which means this word\noccupies the subject position of its parent, its parent being\na verb. Further discussion, including a list of all possible\nsuch dependencies, can be found in [35], [36] and online at\nhttps://universaldependencies.org/introduction.html. From this\nPP VBD    DT     NN       IN        RB        JJ                 NN\nFig. 4. An example dependency parse of the sentence ‘I saw the ship with\nvery strong binoculars’. The root is ‘saw’, and all other words in the tree are\nmarked for their syntactic relation to their parent. Parts of speech are shown in\nblue, using the Penn Treebank tagset, which is the tagset used by the Stanford\nparser.\nsyntactic parse, atoms are formed by Algorithm 2.\nB. Linking to an Ontology\nTo apply inference rules from our ontology, we need to\nidentify, for each predicate and object in our logical annota-\ntion, the corresponding entity from the ontology. This amounts\nto word-sense disambiguation. In WordNet, words are broken\nup, by sense, into different synsets. E.g., the word ‘bank’\nhas one synset corresponding to the ﬁnancial institution and\nanother corresponding to the bank of a river. To identify the\ncorrect synset for each word w in our logical annotations,\nwe compute the doc2vec [25] representation of the original\nnatural language sentence from which w was taken, and the\ndoc2vec representation of the deﬁnition of each WordNet\nAlgorithm 1 ExtractRootAtom\nInputs: grammatical dependence tree of a natural language\nsentence\nOutput: a single atom if one can be found, else None\ns ←the input natural language sentence\nroot ←the word designated as root\nif there is a word w in s with dependency marking ‘nsubj’\nto root then\nsubj ←w\nelse\nReturn None\nend if\nif root is a verb then\nif there is a word w′ in s with dependency marking ‘obj’\nto root then\nreturn root(subj, w′)\nelse\nreturn root(subj)\nend if\nend if\nif root is an adjective then\nif there is a word c in s with dependency marking ‘cop’\nto root then\nreturn root(subj)\nelse\nreturn null\nend if\nend if\nif root is an noun then\nif there is a word c in s with dependency marking ‘cop’\nto root and a word p with dependency marking ‘case’ to\nroot and with part of speech ‘ADP’ then\nreturn p(arg1, root)\nend if\nend if\nsynset containing the word w, and we pick the synset which\nhas the most similar vector representation. Formally,\nsyn =\nargmax\n{syn′|w∈syn′}\ndoc2vec(def(syn′))doc2vec(s),\nwhere def(syn′) denotes the deﬁnition of synset syn′, and\ns is the original natural language annotation sentence from\nwhich w was taken.\nC. Merging Logical Annotations\nA logical annotation is a set of atoms. We can merge all the\nlogical annotations for each video by simply taking the union\nof the constituent atoms.\nD. Filtering the Vocabulary\nAs described in Section II, we only consider ontology\nelements (i.e., objects and predicates) that appear a sufﬁcient\nnumber of times in our dataset. Speciﬁcally, we remove all\nelements that appear in fewer than 50 data points, after the\nmerging of captions in the previous step. Additionally, we\n10\nAlgorithm 2 ExtractAllAtoms\nInputs: a natural language sentence\nOutput: a list of atoms\ns ←input natural language sentence\nextractedAtoms ←[]\nroot ←the word in sentence with dependency marking\n‘root’\nif there is a word w with dependency marking ‘obj’ to root\nthen\nappend ExtractRootAtom(s, root, w) to extractedAtoms\nelse\nreturn null\nend if\nfor each word conjWord in s with dependency relation\n‘cc’ do\nsubClauseRoot ←parent of conjWord\nsubClauseSubj ←parent of subClauseRoot\nsubClauseAtom ←ExtractRootAtom(s,\nsubClauseRoot,subClauseSubj)\nappend subClauseAtom to extractedAtoms\nend for\nfor each word modWord in s with dependency relation\n‘amod’ to to some other word targetWord do\nappend modWord(targetWord) to extractedAtoms\nend for\nfor each word compWord in sentence with dependency\nrelation ‘compound’ to to some other word targetWord\ndo\nif compWord appears immediately before targetWord\nin s then\nreplace all instances of targetWord in\nextractedAtoms with the concatenation\ncompWord · targetWord\nelse\nif\ncompWord\nappears\nimmediately\nbefore\ntargetWord in s then\nreplace all instances of targetWord in\nextractedAtoms with the concatenation\ntargetWord · compWord\nend if\nend for\nreturn extractedAtoms\nexclude the verbs “take”, “do”, “be”, and “have”. These are\nsemantically weak verbs, which normally function as copulas\nor other syntactic operators, and do not convey relevant\ninformation about the video. For example, the parse of the\nsentence “a woman is standing at the top of the stairs” should\nnot include the atom be(woman).\n11\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2020-07-20",
  "updated": "2020-07-20"
}