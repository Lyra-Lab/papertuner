{
  "id": "http://arxiv.org/abs/2007.10445v2",
  "title": "PanRep: Graph neural networks for extracting universal node embeddings in heterogeneous graphs",
  "authors": [
    "Vassilis N. Ioannidis",
    "Da Zheng",
    "George Karypis"
  ],
  "abstract": "Learning unsupervised node embeddings facilitates several downstream tasks\nsuch as node classification and link prediction. A node embedding is universal\nif it is designed to be used by and benefit various downstream tasks. This work\nintroduces PanRep, a graph neural network (GNN) model, for unsupervised\nlearning of universal node representations for heterogenous graphs. PanRep\nconsists of a GNN encoder that obtains node embeddings and four decoders, each\ncapturing different topological and node feature properties. Abiding to these\nproperties the novel unsupervised framework learns universal embeddings\napplicable to different downstream tasks. PanRep can be furthered fine-tuned to\naccount for possible limited labels. In this operational setting PanRep is\nconsidered as a pretrained model for extracting node embeddings of heterogenous\ngraph data. PanRep outperforms all unsupervised and certain supervised methods\nin node classification and link prediction, especially when the labeled data\nfor the supervised methods is small. PanRep-FT (with fine-tuning) outperforms\nall other supervised approaches, which corroborates the merits of pretraining\nmodels. Finally, we apply PanRep-FT for discovering novel drugs for Covid-19.\nWe showcase the advantage of universal embeddings in drug repurposing and\nidentify several drugs used in clinical trials as possible drug candidates.",
  "text": "1\nPanRep: Graph neural networks for extracting\nuniversal node embeddings in heterogeneous graphs.\nVassilis N. Ioannidis, Da Zheng, George Karypis\nAbstract—Learning unsupervised node embeddings facilitates\nseveral downstream tasks such as node classiﬁcation and link\nprediction. A node embedding is universal if it is designed\nto be used by and beneﬁt various downstream tasks. This\nwork introduces PanRep, a graph neural network (GNN) model,\nfor unsupervised learning of universal node representations for\nheterogenous graphs. PanRep consists of a GNN encoder that\nobtains node embeddings and four decoders, each capturing\ndifferent topological and node feature properties. Abiding to these\nproperties the novel unsupervised framework learns universal\nembeddings applicable to different downstream tasks. PanRep\ncan be furthered ﬁne-tuned to account for possible limited labels.\nIn this operational setting PanRep is considered as a pretrained\nmodel for extracting node embeddings of heterogenous graph\ndata. PanRep outperforms all unsupervised and certain semi-\nsupervised methods in node classiﬁcation and link prediction,\nespecially when the labeled data for the semi-supervised methods\nis small. PanRep-FT (with ﬁne-tuning) outperforms all other\nsemi-supervised approaches, which corroborates the merits of\npretraining models. Finally, we apply PanRep-FT for discovering\nnovel drugs for Covid-19. We showcase the advantage of universal\nembeddings in drug repurposing and identify several drugs used\nin clinical trials as possible drug candidates.\nI. INTRODUCTION\nLearning node representations from heterogeneous graph\ndata powers the success of many downstream machine learning\ntasks such as node classiﬁcation [1] and link prediction [2].\nGraph neural networks (GNNs) learn node embeddings by\napplying a sequence of nonlinear operations parametrized\nby the graph adjacency matrix and achieve state-of-the-art\nperformance in the aforementioned downstream tasks. The\nera of big data provides an opportunity for machine learning\nmethods to harness large datasets [3]. Nevertheless, typically\nthe labels in these datasets are scarce due to either lack of\ninformation or increased labeling costs [4]. The lack of labeled\ndata points hinders the performance of supervised algorithms,\nwhich may not generalize well to unseen data and motivates\nunsupervised learning.\nUnsupervised node embeddings may be used for downstream\nlearning tasks, while the speciﬁc tasks are typically not known\na priori. For example, node representations of the Amazon\nbook graph can be employed for recommending new books\nas well as classifying a book’s genre. Unsupervised learning\ncan also be employed when limited labels of the downstream\ntask are available in two ways. First, the node embeddings\nlearned in the unsupervised setting can be used as ﬁxed\nfeatures in a supervised learning method that will train with\nThe work in this paper has been supported by Amazon Web Services (AWS).\nV. N. Ioannidis, D. Zheng and G. Karypis are with the AWS Artiﬁcial\nIntelligence.\nthe limited labels. Such a two-stage approach will reduce\nthe risk of overﬁtting since the node embeddings are not\ntrained with the labels. Second, reﬁning the unsupervised\nnode embeddings with these labels could further increase the\nrepresentation power of the embeddings. This can be achieved\nby ﬁne-tuning the unsupervised model. In this setting, the GNN\nmodel is initialized with the trained model weights obtained by\nunsupervised learning. Next the model is extended by an output\nlayer and trained for a few epochs with the available labels in\nan end-to-end fashion. Natural language processing methods\nhave achieved state-of-the-art performance by applying such\na ﬁne-tuning framework [5]. Fine-tuning pretrained models is\nbeneﬁcial compared to end-to-end supervised learning since\nthe former typically generalizes better especially when labeled\ndata are limited and decreases the inference time since typically\njust a few ﬁne-tuning iterations typically sufﬁce for the model\nto converge [6]. In this work we will study both applications\nof unsupervised learning with limited label information.\nThis work aspires to provide universal node embeddings,\nwhich will be applied in multiple downstream tasks and achieve\ncomparable performance to their supervised counterparts. The\npaper puts forth a novel framework for unsupervised learning\nof universal node representations on heterogenous graphs\ntermed PanRep1. It consists of a GNN encoder that maps\nthe heterogenous graph data to node embeddings and four\ndecoders, each capturing different topological and node feature\nproperties. The cluster and recover (CR) decoder exploits a\nclustering prior of the node attributes. The motif (Mot) decoder\ncaptures structural node properties that are encoded in the\nnetwork motifs. The meta-path random walk (MRW) decoder\npromotes embedding similarity among nodes participating in a\nMRW and hence captures intermediate neighborhood structure.\nFinally, the heterogeneous information maximization (HIM)\ndecoder aims at maximizing the mutual information among\nnode local and the global representations per node type. These\ndecoders model general properties of the graph data related\nto node homophily [7], [8] or node structural similarity [9],\n[10]. PanRep is solely supervised by the decoders and has no\nknowledge of the labels of the downstream task.\nPanRep may utilize limited labels in two ways. First, the\nuniversal embeddings learned by PanRep are employed as\nfeatures by models such as SVM [11] or DistMult [12] to be\ntrained for the downstream tasks. Such an approach will reduce\nthe risk of overﬁtting since the universal embeddings encode\ngeneral properties of the graph data. Second, this paper also\nconsiders a ﬁne-tuning PanRep (PanRep-FT) that inherits the\n1Pan: Pangkosmios (Greek for universal) and Rep: Representation\narXiv:2007.10445v2  [cs.LG]  4 Mar 2021\n2\nweights of the PanRep model and employs an output layer\nfor the label prediction. By selecting an appropriate output\nlayer PanRep-FT accommodates arbitrary downstream tasks.\nIn this operational setting, PanRep-FT is optimized adhering to\na task-speciﬁc loss. PanRep can be considered as a pretrained\nmodel for extracting node embeddings of heterogenous graph\ndata. Figure 1 illustrates the two novel models.\nThe contribution of this work is threefold.\nC1. We introduce a novel problem formulation of universal\nunsupervised learning and design a tailored learning\nframework termed PanRep. We identify the following\ngeneral properties of the heterogenous graph data: (i) the\nclustering of local node features, (ii) structural similarity\namong nodes, (iii) the local and intermediate neighborhood\nstructure, (iv) and the mutual information among same-\ntype nodes. We develop four tasks speciﬁc decoders to\nmodel the aforementioned properties.\nC2. We extend the unsupervised universal learning framework\nto directly account for labels of the downstream task.\nBy appropriately extending PanRep’s architecture we\ncan model arbitrary downstream learning tasks like node\nclassiﬁcation or link prediction. PanRep-FT reﬁnes the uni-\nversal embeddings and increases the model generalization\ncapability.\nC3. We compare the proposed models to state-of-the-art\nsemi-supervised and unsupervised methods for node\nclassiﬁcation and link prediction. PanRep outperforms\nall unsupervised and certain semi-supervised methods\nin node classiﬁcation, especially when the labeled data\nfor the semi-supervised methods is small. PanRep-FT\noutperforms even semi-supervised approaches in node\nclassiﬁcation and link prediction, which corroborates the\nmerits of pretraining models. Finally, we apply our method\non the drug-repurposing knowledge graph (DRKG) for\ndiscovering drugs for Covid-19 and identify several drugs\nused in clinical trials as possible drug candidates.\nThe rest of the paper is structured as follows. Sec. II\nintroduces some related works to PanRep. Sec. III reviews some\npreliminaries to introduce the main paper. Sec. IV introduces\nthe proposed PanRep model. The experimental setup is detailed\nin Sec. V. Finally, results and conclusions are presented in\nSecs. VI and VII, respectively.\nII. RELATED WORK\nDeep learning architectures typically process the input\ninformation using a succession of L hidden layers. Each\nof the layers comprises a conveniently parametrized linear\ntransformation, a scalar nonlinear transformation, and possibly\na dimensionality reduction (pooling) operator. By successively\ncombining (non)linearly local features, the aim at a high level\nis to progressively extract useful information for learning [13],\n[14]. GNNs tailor these operations to the graph that supports the\ndata [15], including the linear [16], and nonlinear [16], [17],\n[18], [19], [20], [21] operators. PanRep is an unsupervised\nGNN model that operates even in the absence of labeled data,\nwhereas PanRep-FT provides a ﬁnetuned version of PanRep\nfor supervised learning. The following sections introduce three\nresearch areas that this paper also explores with introducing\nrelevant related work.\nA. Unsupervised learning\nRepresentation learning amounts to mapping nodes in an\nembedding space where the graph topological information and\nstructure is preserved [22]. Typically, representation learning\nmethods follow the encoder-decoder framework advocated\nby PanRep. Nevertheless, the decoder is typically attuned to\na single task based on e.g., matrix factorization [23], [24],\n[25], [26], random walks [27], [28], or kernels on graphs [29].\nRecently, methods relying on GNNs are increasingly popular\nfor representation learning tasks [30]. GNNs typically rely on\nrandom walk-based objectives [27], [22] or on maximizing the\nmutual information among node representations [31]. Relational\nGNNs methods extend representation learning to heterogeneous\ngraphs [32], [33], [34]. Relative to these contemporary works\nPanRep introduces multiple decoders to learn universal embed-\ndings for heterogeneous graph data capturing the clustering\nof local node features, structural similarity among nodes, the\nlocal and intermediate neighborhood structure, and the mutual\ninformation among same-type nodes.\nB. Supervised learning\nNode classiﬁcation is typically formulated as a semi-\nsupervised learning (SSL) task over graphs, where the labels\nfor a subset of nodes are available for training [35]. Graph-\nbased SSL methods typically assume that the true labels are\nsmooth over the graph, which naturally motivates leveraging the\nnetwork topology to propagate the labels and increase learning\nperformance. Graph-induced smoothness can be captured by\ngraph kernels [36], [29], [37]; Gaussian random ﬁelds [38]; or\nlow-rank parametric models [39], [40], [41], [42], [43]. GNNs\nachieve state-of-the-art performance in SSL by utilizing regular\ngraph convolution [1] or graph attention [44], while these\nmodels have been extended in heterogeneous graphs [45], [46],\n[47].\nSimilarly, another prominent supervised downstream learning\ntask is link prediction with numerous applications in recommen-\ndation systems [2] and drug discovery [48], [49]. Knowledge-\ngraph (KG) embedding models rely on mapping the nodes\nand edges of the KG to a vector space by maximizing a\nscore function for existing KG edges [2], [12], [50]. RGCN\nmodels [45] have been successful in link prediction and contrary\nto KG embedding models can further utilize node features. The\nuniversal embeddings extracted from PanRep without labeled\nsupervision offer a strong competitive to these supervised\napproaches for both node classiﬁcation and link prediction\ntasks.\nC. Pretraining\nPretraining models provides a signiﬁcant performance boost\ncompared to traditional approaches in natural language pro-\ncessing [5] and computer vision [51], [52] . Pretraining offers\nincreased generalization capability especially when the labeled\ndata is scarce and increased inference speed relative to end-to-\nend training [5]. Parallel to our work [53], [54] proposed\n3\nFig. 1. Illustration of the PanRep (left) and PanRep-FT (right) models. The GNN encoder processes the node features X to obtain the embeddings H. The\ndecoders facilitate unsupervised learning of H.\npretraining models for GNNs. These contemporary works\nsupervise the models only using attribute or edge generation\nschemes without accounting for higher order structural similar-\nity or maximizing the mutual information of the embeddings.\nFurther, the work in [53] focuses solely on ranking objectives\nfor evaluation. Recently, [55] introduced a framework for\npretraining GNNs for graph classiﬁcation. Different than [55]\nthat focuses on graph representations, PanRep aims at node\nprediction tasks and obtains node representations via capturing\nproperties related to node homophily [7] or node structural\nsimilarity [9]. PanRep is a novel pretrained model for node\nclassiﬁcation and link prediction that requires signiﬁcantly less\nlabeled points to reach the performance of its fully supervised\ncounterparts.\nIII. DEFINITIONS AND NOTATION\nScalars are denoted by lowercase, column vectors by bold\nlowercase, and matrices by bold uppercase letters. Superscript\n·⊤denotes the transpose of a matrix; and diag(x) corresponds\nto a diagonal matrix with the entries of x on its diagonal.\nA heterogeneous graph with T node types and R relation\ntypes is deﬁned as G := {{Vt}T\nt=1, {Er}R\nr=1}2. The node types\nrepresent the different entities and the relation types represent\nhow these entities are semantically associated to each other.\nFor example, in the IMDB network, the node types correspond\nto actors, directors, movies, etc., whereas the relation types\ncorrespond to directed-by and played-in relations. The number\nof nodes of type t is denoted by Nt and its associated nodal\nset by Vt := {nt}Nt\nn=1. The total number of nodes in G is\nN := Pt\nt=1 Nt. The rth relation type, Er := {(nt, r, n′\nt′) ∈\nVt ×Vt′}, holds all interactions of a certain type among Vt and\nVt′ and may represent that a movie is directed-by a director.\nHeterogenous graphs are typically used to represent KGs [2].\nEach node nt is also associated with an F ×1 feature vector\nxnt. This feature may be a natural language embedding of the\ntitle of a movie. The nodal features are collected in a N × F\nmatrix X. Note that certain node types may not have features\nand for these we use an embedding layer to represent their\nfeatures.\n2This paper considers unweighted graphs that is the weight in the edge is\neither 0 or 1.\nIV. PANREP\nGiven G and X, our goal is to learn a function g such\nthat H := g(X, G), where H ∈RN×D represents the node\nembeddings and D is the size of the embedding space. Note\nthat in estimating g, no labeled information is available.\nOur work aims at universal representations H that perform\nwell on different downstream tasks. Different node classiﬁcation\nand link prediction tasks may arise by considering different\nnumber of training nodes and links and different label types,\ne.g., occupation label or education level label. Consider I\ndownstream task, for the universal representations H it holds\nthat\nL(i)(f (i)(H), T (i)) ≤ϵ, i = 1, . . . , I,\n(1)\nwhere L(i), f (i), and T (i) represent the loss function, learned\nclassiﬁer, and training set (node labels or links) for task i,\nrespectively and ϵ is the largest error for all tasks. The goal of\nunsupervised universal representation learning is to learn H\nsuch that ϵ is small. While learning H, PanRep does not have\nknowledge of {L(i), f (i), T (i)}i. Nevertheless, by utilizing the\nnovel decoder scheme PanRep achieves superior performance\neven compared to supervised approaches across tasks.\nOur universal representation learning framework aims at\nembedding nodes in a low-dimensional space such that the\nrepresentations are discriminative for node classiﬁcation and\nlink prediction. PanRep utilizes an GNN encoder that maps\nthe node features and graph structure to node embeddings\nH that is detailed in Sec. IV-A. The node embedding matrix\nH is the input to four decoders each capturing unique graph\nproperties; see Sec. IV-B. The combined loss of the decoders\ntrains PanRep in an end-to-end fashion. A task speciﬁc loss\nfunction for node classiﬁcation and link prediction extends\nPanRep to supervised learning settings and gives rise to PanRep-\nFT in Sec. IV-C. Figure 1 provides an illustration of the\noverall PanRep architecture. Different than existing approaches,\nPanRep combines multiple decoders in a multi-task learning\nscheme to learn node embedding that perform well in a variety\nof tasks.\nA. PanRep Encoder\nThe abundance of graph-abiding data calls for advanced\nlearning techniques that complement nicely standard machine\n4\nlearning tools when the latter cannot be directly employed,\ne.g. due to irregular data inter-dependencies. Permeating the\nbeneﬁts of deep learning to graph data, graph neural networks\n(GNNs) offer a versatile and powerful framework to learn from\ncomplex graph data [15].\nThe GNN layer operates per node n in the graph and\nperforms 3 steps: 1) aggregates the input embeddings cor-\nresponding to the neighbors of n, 2) Projects the combined\nembedding to a new vector via a parametrized projection matrix\nW, and 3) Nonlinearly transforms the projected embedding.\nDifferent GNN models employ variations of these steps.\nAlthough the PanRep framework can utilize any GNN model\nas an encoder [30], in this paper PanRep uses a relational graph\nconvolutional network (RGCN) encoder [45]. RGCNs extend\nthe graph convolution operation [1] to heterogenous graphs.\nThe lth RGCN layer computes the nth node representation\nh(l+1)\nn\nas follows\nh(l+1)\nn\n:= σ\n\n\nR\nX\nr=1\nX\nn′∈N r\nn\nh(l)\nn′ W(l)\nr\n\n,\n(2)\nwhere N r\nn is the neighborhood of node n under relation r, σ the\nrectiﬁed linear unit non linear function, and W(l)\nr\nis a learnable\nmatrix associated with the rth relation. Essentially, the output\nof the RGCN layer for node n is a nonlinear combination\nof the hidden representations of neighboring nodes weighted\nbased on the relation type. The node features are the input of\nthe ﬁrst layer in the model i.e., h(0)\nn\n= xn, where xn is the\nnode feature for node n. The matrix H in this paper represents\nthe embedding extracted in the ﬁnal layer.\nRecent studies [56], [57] manage to unify different GNN\nvariants into the message passing paradigm. The RGCN layer\nin 2 can be interpreted under this framework where each node n\ncollects the messages send by his neighbors h(l)\nn′ and transforms\nthem. By implementing the message passing paradigm the deep\ngraph learning (DGL)3 library provides highly optimized GNN\nmodels [58].\nB. Universal supervision decoders\nMethods for learning over graphs typically rely on modeling\nhomophily of nodes that postulates neighboring vertices to\nhave similar attributes [29], [59] or structural similarity among\nnodes [9], where vertices involved in similar graph structural\npatterns possess related attributes [10].\nMotivated by these methods we identify related properties\nencoded in the graph data. Clustering nodes based on their\nattributes provides a strong signal for node homophily [60].\nNetwork motifs reveal the local structure information for\nnodes in the graph [61]. Metapaths encode the heterogeneous\ngraph neighborhood and indicate the local connectivity [32].\nFinally, maximizing the mutual information among embeddings\ndeclusters node representations and provides further discrimi-\nnative information [31]. Towards capturing the aforementioned\nproperties, we develop four novel neural network based\ndecoders. PanRep’s encoder computes the embedding matrix\n3https://www.dgl.ai/\n2-node motif\n3-node motifs\n4-node motifs\nFig. 2. Connected motifs up to size 4 nodes. The motif frequency µn shows\nhow many times node n exits in the corresponding motifs.\nH that is fed to the decoders, which supervise the learning by\npromoting graph properties in H.\n1) Cluster and recover supervision: Node attributes may\nreveal interesting properties of nodes, such as clusters of\ncustomers based on their buying power and age. This is\nimportant in recommendation systems, where traditional matrix\nfactorization approaches [60] rely on revealing clusters of\nsimilar buyers. To capitalize such information we propose\nto supervise the universal embeddings by such cluster rep-\nresentations. Speciﬁcally, we cluster the node attributes via\nK-means [62] and then design a model that decodes H to\nrecover the original clusters. The CR-decoder amounts to a\nsingle layer multilayer perceptron (MLP) as follows\nˆC := σ(HWCR),\n(3)\nwhere ˆC is a N × K matrix representing the output of the\ndecoder and WCR is a learnable matrix. The task speciﬁc loss\nfunction for this decoder is\nLCR := −\nN\nX\nn=1\nK\nX\nk=1\nCnk ln ˆCnk,\n(4)\nwhere the cluster assignment Cnk is 1 if node n belongs in\nclass k and the predicted cluster assignment ˆCnk is the output\nof the CR-decoder. We showcase in the experiments that this\ndecoder is superior to the attribute generation scheme used\nin [53]. Such a supervision decoder will enrich the universal\nembeddings H with information based on the clustering of\nlocal node features.\n2) Motif supervision: Network motifs are sub-graphs where\nthe nodes have speciﬁc connectivity patterns. Typical size-\n3 motifs for example, are the triangle and the star motifs.\nEach of these sub-graphs conforms to a particular pattern of\ninteractions among nodes, and reveals important properties\nfor the participating nodes. In gene regulatory networks for\nexample, motifs relate to certain biological properties [63].\nThe work in [61] develops efﬁcient parallel implementations\nfor extracting network motifs. We aspire to capture structural\nsimilarity among nodes by predicting their motif information.\nThe motivation is that nodes which might be distant in the\ngraph may have similar structural properties as described by\ntheir motifs.\nUsing the method in [61] we extract a M × 1 frequency\nvector µn per node that shows how many times the node n\nparticipates in motifs of size up to 4 nodes. Fig. 2 shows\nthe different motifs up to size 4. This information reveals the\n5\nstructural role of nodes such as star-center, star-edge nodes,\nor bridge nodes. The motif decoder predicts this vector for\nall nodes using the universal representation H. This allows\nfor information sharing among nodes which are far away in\nthe graph but have similar motif frequency vectors. The motif\ndecoder utilizes a single layer MLP as follows\nˆµn := σ(hnWMOT),\nn = 1, . . . , N,\n(5)\nwhere ˆµn is a M × 1 matrix representing the output of the\ndecoder and WMOT is a D × M learnable matrix. The motif\ndecoder optimizes the following loss function\nLMOT :=\nN\nX\nn=1\n∥µn −ˆµn∥2\n2,\n(6)\nwhere ˆµn is the output of the Mot-decoder for the nth\nnode. Using the Mot-decoder PanRep enhances the universal\nembeddings by structural information encoded in the node\nmotifs.\n3) Metapath RW supervision: Metapaths are sequences of\nedges of possibly different type that connect nodes in a KG.\nA metapath random walk (MRW) is a specialized RW that\nfollows different edge-types [32].\nWe aspire to capture local connectivity patterns by promoting\nnodes participating in a MRW to have similar embeddings.\nConsider all node pairs for nodes (nt, n′t′) participating in a\nMRW, the following criterion maximizes the similarity among\nthese nodes as follows\nLMRW := log(1 + exp(−y × h⊤\nntdiag(rt,t′)hn′t′)),\n(7)\nwhere hnt and hn′t′ are the universal embeddings for nodes\nnt and n′t′, respectively, rt,t′ is an embedding parametrized\non the pair of node-types and y is 1 if nt and n′t′ co-occur in\nthe MRW and −1 otherwise. Negative examples are generated\nby randomly selecting tail nodes for a ﬁxed head node with\nratio 5 negatives per positive example.\nMetapaths convey more information than regular links since\nthe former can be deﬁned to promote certain prior knowledge.\nFor example, in predicting the movie genre in IMDB the\nmetapath conﬁgured by the edge types (played by, played\nin) among node types (movie, actor, movie) will potentially\nconnect movies with same genre, which will allow the PanRep\nto map such movies to similar embeddings and hence it is\ndesirable. The embedding per node-type pair rt,t′ allows the\nMRW-decoder to weight the similarity among node embeddings\nfrom different node types accordingly. The length of the MRW\ncontrols the radius of the graph neighborhood in equation (7)\nand it can vary from local to intermediate. Note that link\nprediction is a special case of MRW supervision that considers\nMRWs of length 1.\n4) Heterogenous information maximization: The aforemen-\ntioned supervision decoders capture clustering afﬁnity, struc-\ntural similarity and local and intermediate neighborhood of the\nnodes. Nevertheless, further information can be extracted by the\nrepresentations by maximizing the mutual information among\nnode representations. Such an approach for homogeneous\ngraphs is detailed in [31], where the mutual information\nbetween node representations and the global graph summaries\nis maximized [64].\nTowards further reﬁning the universal embeddings, we\npropose a generalization of [31] for heterogeneous graphs. We\nconsider a global summary vector per t as st := PNt\nnt=1 hnt\nthat captures the average tth node representation. We aspire\nto maximize the mutual information among st and the corre-\nsponding nodes in Vt. The proposed HIM decoder optimizes\nthe following contrastive loss function\nLHIM :=\nT\nX\nt=1\n\u0012\nNt\nX\nnt=1\nlog\n\u0000σ(h⊤\nntWst)\n\u0001\n+ log\n\u00001 −σ(˜h⊤\nntWst)\n\u0001\u0013\n,\n(8)\nwhere the bilinear scoring function σ(˜h⊤\nntWst) captures how\nclose is hnt to the global summary, W is a learnable matrix\nand ˜hnt represents the negative example used to facilitate\ntraining. Designing negative examples is a cornerstone property\nfor training contrastive models [31]. We generate the negative\nexamples in (8) by shufﬂing node attributes among nodes of the\nsame type. The HIM decoder maximizes the mutual information\nacross nodes and complements the former decoders.\n5) The overall loss function: RGCN extracts the embedding\nmatrix H that is the input to the decoders presented in IV-B.\nPanRep’s overall loss function is the linear combination of (4),\n(6), (7), and (8)\nL := LCR + LMOT + LMRW + LHIM.\n(9)\nA backpropagation algorithm [65] minimizes (9). The objective\nalso relates to the framework of deep multitask learning [66],\nsince the GNN encoder is shared across the multiple super-\nvision tasks and PanRep makes multiple inferences in one\nforward pass. Such networks are not only scalable, but the\nshared features within these networks can induce more robust\nregularization and possibly boost performance [67]. Introducing\nadaptive weights per decoder to control its learning rate is a\nfuture direction of PanRep.\nC. PanRep-FT\nIn certain cases a subset of labels may be known a priori for\nthe downstream task and it is beneﬁcial to ﬁne-tune PanRep’s\nmodel to obtain reﬁned node representations. In this context,\nPanRep is a pretrained model and a downstream task speciﬁc\nloss can further supervise PanRep. First, we train PanRep for\nsome epochs using the unsupervised decoders in Sec. IV-B and\nthen we train PanRep’s encoder for some epochs solely on the\ndownstream task speciﬁc loss. Fig. 1 illustrates on the right side\nthe PanRep-FT framework. Next, we present the supervised\ntasks along with their corresponding problem formulations and\ntraining losses.\n1) Node classiﬁcation: Node classiﬁcation has numerous\napplications and gives rise a SSL task over graphs, where\ngiven the features X, the graph structure G, and the labels for\na subset of nodes the goal is to predict the labels across all\nnodes [35].\nEach node n has a label yn ∈{0, . . . , P −1}, which in the\nIMDB network may represent the genre of a movie. In SSL, we\n6\nknow labels only for a subset of nodes {yn}n∈M, with M ⊂V.\nThis partial availability may be attributed to privacy concerns\n(medical data); energy considerations (sensor networks); or\nunrated items (recommender systems). The N × P matrix Y\nis the one-hot representation of the true nodal labels; that is,\nif yn = p then Yn,p = 1 and Yn,p′ = 0, ∀p′ ̸= p.\nGiven the universal embeddings H PanRep-FT employs a\nsingle layer MLP to obtain the predicted label matrix ˆY as\nfollows\nˆY := HWNC,\n(10)\nwhere the projection matrix WNC is a learnable parameter, ˆY\nis an N × K matrix, and ˆYn,k represents the probability that\nyn = k. PanRep-FT loss function for this node classiﬁcation\n(nc) task is a cross-entropy loss\nLNC := −\nX\nn∈M\nP\nX\np=1\nYnp ln ˆYnp,\n(11)\nwhere the error is averaged over the nodes with labels in M.\nThe loss function in (11) will force PanRep’s encoder to learn\nsuitable parameters to minimize the classiﬁcation error. After\ntraining the matrix ˆY from the output function in (10) predicts\nthe class of the unlabeled nodes.\n2) Link prediction: Consider the heterogeneous graph G in\nSec. III. Given the sets of links {Er}R\nr=1, and the node features\nX the goal of link prediction is to predict whether a set of\nlinks different than the one used for training might exist or not\nin the graph.\nTypically, link prediction models utilize a contrastive loss\nfunction that requires the model to distinguish among positive\nand negative examples [50]. In this context, positive examples\nare the set of existing links in the graph. The negative examples,\nwhich are links that the model should classify as nonexistent,\nare typically sampled from the missing links in the graph. For\neach positive triplet q = (nt, r, n′t′) a number of negative links\nis generated by corrupting the head and tail entities at random\n(nt, r, ν′t′) and (νt, r, n′t′). This paper considers 5 negative\nexamples per one positive for link prediction.\nPanRep-FT employs a DistMult model [12] for link predic-\ntion. The loss function is\nmin\nX\n(nt,r,n′t′)∈D+∪D−\nlog(1 + exp(−y × h⊤\nntdiag(hr)hn′t′)),\n(12)\nwhere hnt and hn′t′ are the embedding of the head and\ntail entity nt, n′t′ obtained by PanRep’s encoder and hr\nis a trainable parameter corresponding to relation r that is\ndirectly from (12). The scalar represented by h⊤\nntdiag(hr)hn′t′\ndenotes the score of triplet (nt, r, n′t′) as given by the DistMult\nmodel [12]. Finally, D+ and D−are the positive and negative\nsets of triplets and y = 1 if the triplet corresponds to a positive\nexample and −1 otherwise. By minimizing (12) PanRep-FT\nwill learn node embeddings H, which will be informative for\nlink prediction. PanRep-FT predicts new links by calculating\ntheir DistMult score; see also Sec. V-C.\nPretraining PanRep before applying the task speciﬁc loss\nfunctions may increase the model performance. BERT models\nin natural language processing have reported state of the art re-\nsults by considering such a pretrain and ﬁne-tune framework [5].\nPanRep-FT is a counterpart of BERT for extracting information\nfrom heterogenous graph data. PanRep-FT combines the beneﬁt\nof universal unsupervised learning and task speciﬁc information\nand achieves greater generalization capacity especially when\nlabeled data are scarce [6].\nAlternative encoders. Several works consider designing\npossibly more general GNN encoders that utilize attention\nmechanism [47], [44] or graph isomorphism networks [68].\nThis paper proposes novel supervision decoders for unsuper-\nvised learning that capture general properties of the graph data.\nDesigning a universal encoder based on these contemporary\nGNN models is a future direction of PanRep.\nV. EXPERIMENTAL SETUP\nWe implement PanRep in the efﬁcient deep graph learning\n(DGL)4 library [58]. PanRep experiments run on an AWS\nP3.8xlarge instances with 8 GPUs each having 16GB of\nmemory5.\nA. Methods\nOur universal represention learning techniques compares\nagainst with state-of-the-art methods. For node classiﬁcation\nconsider the following contemporary methods RGCN [45],\nHAN [47], MAGNN [46], node2vec [27], meta2vec [32] and\nan adaptations of the work in [31] for heterogenous graphs\ntermed HIM. The competing methods RGCN, MAGNN and\nHAN also use the DGL. For link prediction the baseline models\nis RGCN [45] with DistMult supervision [12] that uses the\nsame encoder as PanRep.\nThe parameters for all methods considered optimize the\nperformance on the validation set. For PanRep the Mot-decoder\nand RC-decoder employ a 1-layer MLP. For the MRW-decoder\nwe use length-2 MRWs. For the majority of the experiments\nPanRep uses a hidden dimension of 300, 1 hidden layer, 800\nepochs of model training, 100 epochs for ﬁnetuning, and\nan ADAM optimzer [69] with a learning rate of 0.001. For\nlink prediction ﬁnetuning PanRep uses a DistMult model [12]\nwhereas for node classiﬁcation it uses a logistic loss.\nB. Datasets\nWe consider a subset of the IMDB dataset [70] containing\n11,616 nodes belonging to three node-types and 17,106 edges\nbelonging to six edge-types. Each movie is associated with\na label representing its genre and with a feature vector\ncorresponding to its keywords. We also use a subset of the\nOAG dataset [71] with 23,696 nodes belonging to four node-\ntypes (authors, afﬁliations, papers, venues) and 90,183 edges\nbelonging to 14 edge-types. In OAG we did not use MOT\nsupervision since the graph does not have a rich motif structure.\nEach paper is associated with a label denoting the scientiﬁc area\nand with an embedding of the papers’ text. Table I provides\nadditional information about these two datasets.\n4https://www.dgl.ai/\n5https://aws.amazon.com/ec2/instance-types/p3/\n7\nTABLE I\nDATASET STATISTICS. THE REVERSE EDGES ARE ALSO PRESENT BUT DO\nNOT APPEAR IN THE TABLE.\nDataset\nNode type\nNodes\nEdge type\nEdges\nIMDB\nMovie (M)\n4,278\nM-directed by-D\n4,278\nDirector (D)\n2,081\nM-played by-A\n12,828\nActor (A)\n5,25\nOAG\nAuthor (A)\n13,720\nA-writes as last-P\n4,522\nPaper (P)\n7,326\nP-in journal-V\n3,941\nAfﬁliation (Af)\n2,290\nP-conference-V\n3,368\nVenue (V)\n782\nP-cites-P\n3,327\nA-writes as other-P\n7,769\nA-writes as ﬁrst-P\n4,795\nA-afﬁliated with-Af\n17,035\nFig. 3. The schema of the Drug Repurposing Knowledge Graph (DRKG).\nThe number next to an edge indicates the number of relation-types among the\ncorresponding entity types in the DRKG.\nIn addition, we use a third dataset corresponding to the drug\nrepurposing knowledge graph (DRKG) constructed in [49].\nDRKG, whose schema is shown in Figure 3, contains 5,874,261\nbiological interactions belonging to 107 edge-types and 97,238\nbiological entities from 13 entity-types. DRKG is used for\nevaluating link-prediction tasks.\nC. Evaluation metrics\nFor evaluating node classiﬁcation methods we use macro\nand micro F1 score that is advantageous when the classes\nare unbalanced [72]. The F1 score is the harmonic mean of\nthe precision and recall and reaches its best value at 1 and\nworst value at 0. The Mirco-F1 score calculates the metric\nacross labels by counting the total true positives, false negatives\nand false positives. The Macro-F1 calculate metrics for each\nlabel, and ﬁnd their unweighted mean. This does not take label\nimbalance into account.\nLink prediction evaluation utilizes two standard criteria the\nHit-10 and Mean Reciprocal Rank (MRR) metrics [73], [74],\n[50]. Both metrics are derived by comparing how the score\nof the positive triplet relates to the scores of its associated\nnegative instances.\nFor each positive triplet q = (nt, r, n′t′) we generate all\npossible triplets of the form (nt, r, ν′t′) and (νt, r, n′t′) by\ncorrupting the head and tail entities. We then removed from\nthem any triplets that already exist in the dataset. The set of\ntriplets that remained form the negative triplets associated with\nthe initial positive triplet. The positive and negative triplets are\nthen scored by the DistMult [12] model in (12).\nWe construct a list of triples Sq containing q and its\nassociated negative triplets that order them in a non-increasing\nscore fashion, and let rankq be qth’s position in Sq. Given\nthat, Hit-10 is the average number of times the positive\ntriplet is among the 10 highest ranked triplets, whereas\nMRR is the average reciprocal rank of the positive instances.\nMathematically, the metrics are as follows\nHit-10 := 1\nQ\nQ\nX\nq=1\n1rankq≤10,\nMRR := 1\nQ\nQ\nX\nq=1\n1\nrankq\n,\nwhere Q is the total number of positive triplets and 1rankq≤10\nis 1 if rankq ≤10, otherwise it is 0. Note that Hit-10 and MRR\nare between 0 and 1, with values closer to 1 corresponding to\nbetter predictions.\nD. Scalability\nPanRep is implemented using DGL’s mini-batch training,\nwhich is scalable w.r.t. time and space. During mini-batch\ntraining a subset of seed nodes is sampled along with the\nK-hop neighbors of the seed nodes, where K is the depth of\nthe GNN. Furthermore, the number of edges at each level is\nbounded by using neighbor sampling. This allows training on\ngraphs with billions of nodes/edges since only a fraction of\nthe nodes/edges will be loaded on the GPU per mini-batch.\nPanRep’s complexity is controlled by the RGCN encoder whose\ncomputation complexity of a mini-batch is O(BF K) for sparse\ngraphs, where B is the batch size, K is the number of layers\nand F is the maximum number of neighbors per node. By\nusing neighbor sampling, F is usually less than 10–20 and\nin our experiments we found that K=2 often works well. The\ndecoders’ complexity is signiﬁcantly lower and do not add\nbut a constant factor to the overal complexity and hence the\noverall per-epoch runtime is similar to that of RGCN.\nVI. RESULTS\nA. Node classiﬁcation\nIn this experiment we compare semi-supervised and unsu-\npervised methods for classiﬁcation. First, the labeled nodes are\nsplit in 10% training, 5% validation, and 85% testing sets. The\nsemi-supervised methods utilize the training labels whereas\nthe unsupervised methods do not use them in calculating the\nnode embeddings. Then the embeddings corresponding to the\n85% testing nodes are further split to training and testing sets\nand a linear support vector machine (SVM) is trained on the\ntraining set for evaluation following the setup in [46]. The\nreason that we selected a simple linear classiﬁer as the SVM\nover a potentially more powerful non-linear one, e.g., an multi-\nlayer perception (MLP) classiﬁer, is because an SVM classiﬁer\nallows us to directly compare the representation power of the\ndifferent approaches, whereas an MLP classiﬁer that employs\na neural network will enhance the representation power of\ncertain methods and result to unfair comparisons.\n8\nTABLE II\nNODE CLASSIFICATION RESULTS.\nTrain %\nUnsupervised\nSemi-supervised\nnode2vec\nmeta2vec\nHIM\nPanRep\nHAN\nMAGNN\nRGCN\nPanRep-FT\nIMDB\nMacro-F1\n40%\n50.63\n47.57\n55.21\n56.04\n56.15\n60.27\n58.48\n59.49\n60%\n51.65\n48.17\n57.66\n58.51\n57.29\n60.66\n58.42\n59.86\n80%\n51.49\n49.99\n57.89\n60.23\n58.51\n61.44\n58.76\n61.49\nMicro-F1\n40%\n51.77\n48.17\n55.11\n55.92\n57.32\n60.50\n58.64\n59.67\n60%\n52.79\n49.87\n56.57\n58.41\n58.42\n60.88\n58.55\n59.75\n80%\n52.72\n50.50\n57.79\n60.14\n59.24\n61.53\n58.89\n61.59\nOAG\nMacro-F1\n40%\n56.37\n65.75\n50.54\n57.76\n63.99\n63.31\n64.68\n64.72\n60%\n57.01\n66.09\n51.98\n59.72\n64.25\n63.42\n65.96\n66.99\n80%\n58.05\n65.75\n53.25\n63.03\n64.37\n63.89\n67.67\n67.90\nMicro-F1\n40%\n70.17\n74.54\n71.91\n75.50\n73.95\n72.74\n81.92\n80.36\n60%\n70.95\n74.96\n73.89\n77.39\n75.32\n72.75\n81.39\n81.78\n80%\n72.24\n74.73\n75.31\n79.76\n75.24\n73.43\n82.38\n83.17\nTABLE III\nNODE CLASSIFICATION RESULTS FOR DIFFERENT LABELED SUPERVISION SPLITS.\nDatasets\nMetrics\nTrain embeddings %\n5%\n10%\n20%\nTrain%\nPanRep\nRGCN\nPanRep-FT\nRGCN\nPanRep-FT\nRGCN\nPanRep-FT\nIMDB\nMacro-F1\n40%\n56.04\n55.12\n56.85\n58.48\n59.49\n61.30\n63.14\n60%\n58.51\n55.20\n59.39\n58.42\n59.86\n60.98\n62.91\n80%\n60.23\n55.55\n61.27\n58.76\n61.49\n61.10\n62.72\nMicro-F1\n40%\n55.92\n55.27\n56.92\n58.64\n59.67\n61.49\n63.17\n60%\n58.41\n55.39\n59.45\n58.55\n59.75\n61.17\n62.89\n80%\n60.14\n55.62\n61.32\n58.89\n61.39\n61.30\n62.75\nOAG\nMacro-F1\n40%\n57.76\n55.51\n64.99\n64.68\n64.72\n67.07\n65.31\n60%\n59.72\n55.99\n66.62\n65.96\n66.99\n67.58\n66.25\n80%\n63.03\n56.36\n68.94\n66.10\n68.60\n67.67\n67.90\nMicro-F1\n40%\n75.50\n78.00\n80.19\n81.92\n80.36\n82.57\n81.17\n60%\n77.39\n78.07\n81.36\n81.39\n81.78\n81.74\n81.34\n80%\n79.76\n78.44\n82.52\n82.38\n83.17\n82.20\n82.31\nWe report the Macro and Micro F1 accuracy for different\ntraining percentages of the 85% nodes fed to the SVM classiﬁer\nin Table II. First, PanRep outperforms other unsupervised\napproaches as well as some semi-supervised approaches. In\nthe 80% splits, PanRep outperforms even its semi-supervised\ncounterpart RGCN that uses node labels for supervision.\nMetapath2vec [32] reports competitive performance for OAG in\nMacro-F1 score but underperforms in Micro-F1. Nevertheless,\nin this experiment Metapath2vec only uses the best performing\nmetapath that is paper-venue-paper, which is considerably\nbetter than that of most other metapaths. This way of selecting\nthe metapath, gives Meta2vec an (unfair) advantage over\nPanRep, which uses all metapaths of length 2 as it computes\na universal representation (along with the other supervision\ndecoders). PanRep-FT outperforms RGCN that uses the same\nencoder, which is a testament to the power of pretraining\nmodels. Finally, PanRep-FT matches and outperforms in certain\nsplits the state-of-the-art MAGNN that uses a more expressive\nencoder. PanRep’s universal decoders enhance the embeddings\nwith additional discriminative power that results to improved\nperformance in the downstream tasks.\nTable III reports the accuracy of the PanRep-FT and\nthe encoder RGCN, which is trained directly for the semi-\nsupervised learning task to obtain the embeddings. PanRep-\nFT consistently outperforms RGCN across most SVM splits,\nwhereas PanRep-FT’s advantage over RGCN decreases as more\ntraining data for the embeddings are available; see e.g., the\nlast column with 20% training, which is justiﬁable since more\nlabels diminish the advantage of pretraining.6 Moreover even\nwithout labeled supervision the unsupervised embeddings of\nPanRep outperform the semi-supervised RGCN embeddings\nfor 5% training labels. This demonstrates the importance of\nusing PanRep as a pretraining method. Finally, RGCN reports\nsimilar performance across SVM training splits for all columns,\nwhereas PanRep-FT increases with more supervision. These\nresults suggest that PanRep-FT’s embeddings have higher\ngeneralization capacity.\nB. Link prediction\nOur universal embedding framework is further evaluated\nfor link prediction using the IMDB and OAG datasets. The\nMRW decoder is used to evaluate the performance of PanRep\nin link prediction. Speciﬁcally, the MRW score calculated\nby the scaled inner product h⊤\nntdiag(rt,t′)hn′t′ in (7) is used\nto predict whether there exists a link between the nodes nt\nand n′′t; see also Sec.V-C for details on the link prediction\nevaluation. In this section, we utilize a percentage x% of all\nthe links in the graph to train the methods and the rest 95−x%\nto test the approaches, while holding 5% for validation.\nFigures 4 and 5 report the MRR and Hit-10 scores of\nthe baseline methods along with the PanRep and PanRep-\nFT methods. We report the performance of the methods for\ndifferent percentages of links used for training. Observe that\nPanRep-FT consistently outperforms the competing methods\n6PanRep-FT performs only 100 ﬁnetuning epochs with labeled supervision,\nwhich is signiﬁcantly less to the 800 epochs of labeled supervision by RGCN.\n9\nTABLE IV\nABLATION STUDY FOR DIFFERENT SUPERVISION DECODERS.\nDatasets\nMetrics\nTrain %\nHIM\nMRW\nHIM+MRW\nMOT\nCR\nPanRep\nIMDB\nMacro-F1\n40%\n55.21\n54.54\n55.32\n42.28\n32.21\n56.04\n60%\n57.66\n56.12\n57.24\n43.41\n35.16\n58.51\n80%\n57.89\n56.64\n57.74\n44.31\n35.66\n60.23\nMicro-F1\n40%\n55.11\n54.36\n55.53\n43.66\n39.13\n55.92\n60%\n56.57\n55.91\n56.25\n44.89\n40.03\n58.41\n80%\n57.79\n56.49\n58.42\n45.65\n40.66\n60.14\nOAG\nMacro-F1\n40%\n50.54\n55.92\n56.11\n-\n15.46\n57.76\n60%\n51.98\n58.40\n58.91\n-\n15.48\n59.72\n80%\n53.25\n60.61\n61.74\n-\n15.54\n63.03\nMicro-F1\n40%\n71.91\n74.39\n74.65\n-\n63.06\n75.50\n60%\n73.89\n76.76\n76.33\n-\n63.14\n77.39\n80%\n75.31\n78.90\n78.71\n-\n63.59\n79.76\n60\n70\n80\n90\n75\n80\n85\n90\n95\nTraining link percentage\nMRR\nIMDB\nPanRep-FT\nPanRep\nRGCN\n60\n70\n80\n90\n90\n95\n100\nTraining link percentage\nHit-10\nIMDB\nPanRep-FT\nPanRep\nRGCN\nFig. 4. MRR and Hit-10 for link prediction across different percentages of testing links for IMDB.\n60\n70\n80\n90\n50\n60\n70\nTraining link percentage\nMRR\nOAG\nPanRep-FT\nPanRep\nRGCN\n60\n70\n80\n90\n80\n85\n90\nTraining link percentage\nHit-10\nOAG\nPanRep-FT\nPanRep\nRGCN\nFig. 5. MRR and Hit-10 for link prediction across different percentages of testing links for OAG.\nand the performance gain increases as the percentage of training\nlinks decreases. This corroborates the advantage of pretraining\nGNNs for link prediction. Note that PanRep that is trained\non MRW decoder which resembles the link prediction task\nreports similar performance with RGCN that is trained solely\nin link prediction. For few training data PanRep outperforms\nRGCN whereas as the training data increases RGCN reduces\nthe gap in performance and eventually outperforms PanRep.\nThis competitive performance of PanRep highlights the use of\nthe other decoders that regularize learning and corroborate to\nthe success of the universal embeddings in link prediction.\n1) Drug repurposing: Drug-repurposing aims at discovering\nthe most effective existing drugs to treat a certain disease.\nWe formulate drug-repurposing as a link prediction task,\nwhere we are trying to recover the appropriate drug nodes\nto link to a disease node. The Drug Repurposing Knowledge\nGraph (DRKG) [49] collects interactions from a collection of\nbiological databases such as Drugbank [75], STRING [76],\nIntAct [77] and DGIdb [78].\nWe put forth drug-repurposing as predicting direct links in\nthe DRKG. Here, we attempt to predict whether a drug inhibits\na certain gene, which is related to the target disease. We identify\n442 genes that relate to the Covid-19 disease. We select 8,104\nFDA-approved drugs in the DRKG as candidates. Our work\nfocused entirely on evaluating the ranked list of candidate drugs\nagainst a set of 32 drugs that, at the time of submission, were\n10\nTABLE V\nDRUG INHIBITS GENE SCORES FOR COVID-19.\nPanRep-FT\nRGCN\nDrug name\n# hits\nDrug name\n# hits\nLosartan\n232\nChloroquine\n69\nChloroquine\n198\nColchicine\n41\nDeferoxamine\n104\nTetrandrine\n40\nRibavirin\n101\nOseltamivir\n37\nMethylprednisolone\n44\nAzithromycin\n36\nThalidomide\n41\nTofacitinib\n33\nHydroxychloroquine\n19\nRibavirin\n32\nTetrandrine\n13\nMethylprednisolone\n30\nEculizumab\n10\nDeferoxamine\n30\nTocilizumab\n9\nThalidomide\n25\nDexamethasone\n7\nDexamethasone\n24\nAzithromycin\n6\nBevacizumab\n21\nNivolumab\n5\nHydroxychloroquine\n19\nPiclidenoson\n5\nLosartan\n19\nOseltamivir\n5\nRuxolitinib\n13\nEculizumab\n12\nSarilumab\n8\nBaricitinib\n6\nin Covid-19 clinical trials [79], [48]. Speciﬁcally, given a gene\nwhose associated protein is a potential drug target, we rank the\n8104 FDA approved drugs and select the 100 highest ranked\ndrugs. We evaluated the performance of a method by looking\nat the intersection between these 100 highest ranked drugs and\nthe 32 drugs in Covid-19 trials. In our experiments, we used\n442 genes that researchers have identiﬁed as potential targets\nfor Covid-19 and computed 442 ranked lists and associated\nintersections. We aggregated the results by counting the number\nof times each one of the 32 drugs were in these intersections.\nUsing the DRKG, we compare the drug repurposing results in\nCovid-19 among PanRep-FT that is ﬁnetuned in link prediction\nand the baseline RGCN. We employ L = 1 hidden layer with\nD = 600 and train for 800 epochs both networks. For each\ngene node we calculate with RGCN and PanRep-FT an inhibit\nlink score associated with every drug. Based on this score,\nwe rank all ‘drug-inhibits-gene’ triples per target gene. We\nobtain in this way 442 ranked lists of drugs, one per gene\nnode. Finally, to assess whether our prediction is in par with\nthe drugs used for treatment, we check the overlap among the\ntop 100 predicted drugs and the drugs used in clinical trials\nper gene.\nTable V lists the clinical drugs included in the top-100\npredicted drugs across all the genes with their corresponding\nnumber of hits for the RGCN and PanRep-FT. Several of the\nwidely used drugs in clinical trials appear high on the predicted\nlist in both prediction. Furthermore, PanRep-FT reports a higher\nhit rate than RGCN, which corroborates the beneﬁt of using\nthe universal pretraining decoders. The universal representation\nendows PanRep with increased generalization power that allows\nfor accurate link prediction performance when training data\nare extremely scarce as is the case of Covid-19. While this\nstudy, does not recommend speciﬁc drugs, it demonstrates\na powerful deep learning methodology to prioritize existing\ndrugs for further investigation, which holds the potential of\naccelerating therapeutic development for Covid-19.\nC. Ablation study\nTable IV reports an ablation study by using different decoder\nsubsets. The OAG graph does not have a rich motif structure\nand thus we did not use the motif supervision there and it is\nexcluded from the ablation study. PanRep that uses all decoders\nobtains the best performance. The HIM and MRW decoders and\ntheir combination exhibit the second best performance.\nNext, we compare the cluster and recover supervision\ndecoder to the attribute generation supervision that is employed\nby several GNN pretraining methods [53]. In this experiment\nwe employ the setting of Sec. VI-A for node classiﬁcation.\nThe attribute generation supervision employs a 2-layer MLP\nnetwork that attempts to reconstruct the original nodal attributes\ngiven the node embeddings. In Table VI we report the node\nclassiﬁcation accuracy for different number of clusters K\nconsidered in the CR supervision. The CR methods outperform\nthe attribute generation method. One reason for this is that the\nGNN encoder already consideres the attributes in calculating\nthe embeddings, and this supervision does not increase the\npredictive capacity of the model. On the other hand the\nclustering supervision allows the GNN to discover clusters\nof nodes with similar attributes and hence reﬁnes the learned\nembeddings.\nVII. CONCLUSION\nThis paper develops a novel framework for unsupervised\nlearning of universal node representations on heterogenous\ngraphs termed. PanRep supervises the GNN encoder by\ndecoders attuned to model the clustering of local node features,\nstructural similarity among nodes, the local and intermediate\nneighborhood structure, and the mutual information among\nsame-type nodes. To further facilitate cases where limited labels\nare available we implement PanRep-FT. Experiments in node\nclassiﬁcation and link prediction corroborate the competitive\nperformance of the learned universal node representations\ncompared to unsupervised and semi-supervised methods. Exper-\niments on the DRKG showcase the advantage of the universal\nembeddings in drug repurposing.\nREFERENCES\n[1] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\nconvolutional networks,” in Proc. Int. Conf. on Learn. Represantions,\nToulon, France, Apr. 2017.\n[2] Q. Wang, Z. Mao, B. Wang, and L. Guo, “Knowledge graph embedding: A\nsurvey of approaches and applications,” IEEE Transactions on Knowledge\nand Data Engineering, vol. 29, no. 12, pp. 2724–2743, 2017.\n[3] X. Wu, X. Zhu, G.-Q. Wu, and W. Ding, “Data mining with big data,”\nIEEE transactions on knowledge and data engineering, vol. 26, no. 1,\npp. 97–107, 2013.\n[4] Y. Bengio, A. C. Courville, and P. Vincent, “Unsupervised feature learning\nand deep learning: A review and new perspectives,” CoRR, abs/1206.5538,\nvol. 1, p. 2012, 2012.\n[5] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[6] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and\nS. Bengio, “Why does unsupervised pre-training help deep learning?”\nJournal of Machine Learning Research, vol. 11, no. Feb, pp. 625–660,\n2010.\n[7] D. F. Gleich, “Pagerank beyond the web,” SIAM Review, vol. 57, no. 3,\npp. 321–363, 2015.\n11\nTABLE VI\nATTRIBUTE GENERATION SUPERVISION AGAINST CLUSTERING AND RECOVER SUPERVISION FOR DIFFERENT NUMBER OF CLUSTERS K.\nDatasets\nMetrics\nTrain %\nATTRIBUTE GENERATION\nCR K = 2\nCR K = 4\nCR K = 5\nCR K = 10\nIMDB\nMacro-F1\n40%\n17.52\n29.52\n31.94\n32.21\n31.22\n60%\n17.53\n30.75\n34.50\n35.16\n34.19\n80%\n17.30\n3145\n35.02\n35.66\n34.82\nMicro-F1\n40%\n35.66\n38.51\n39.40\n39.13\n38.38\n60%\n35.70\n38.83\n41.02\n41.13\n40.03\n80%\n35.07\n39.54\n41.16\n41.39\n40.66\n[8] K. Kloster and D. F. Gleich, “Heat kernel based community detection,”\nin Proc. Intl. Conf. on Knowledge Disc. and Data Mining (KDD), 2014,\npp. 1386–1395.\n[9] R. A. Rossi and N. K. Ahmed, “Role discovery in networks,” IEEE\nTransactions on Knowledge and Data Engineering, vol. 27, no. 4, pp.\n1112–1131, 2014.\n[10] C. Donnat, M. Zitnik, D. Hallac, and J. Leskovec, “Learning structural\nnode embeddings via diffusion wavelets,” in Proc. Intl. Conf. on\nKnowledge Disc. and Data Mining (KDD), 2018.\n[11] J. A. Suykens and J. Vandewalle, “Least squares support vector machine\nclassiﬁers,” Neural processing letters, vol. 9, no. 3, pp. 293–300, 1999.\n[12] B. Yang, W.-t. Yih, X. He, J. Gao, and L. Deng, “Embedding entities and\nrelations for learning and inference in knowledge bases,” arXiv preprint\narXiv:1412.6575, 2014.\n[13] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep Learning.\nMIT press Cambridge, 2016, vol. 1.\n[14] F. Gama, E. Isuﬁ, G. Leus, and A. Ribeiro, “Graphs, convolutions, and\nneural networks,” arXiv preprint arXiv:2003.03777, 2020.\n[15] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst,\n“Geometric deep learning: going beyond euclidean data,” IEEE Sig.\nProcess. Mag., vol. 34, no. 4, pp. 18–42, 2017.\n[16] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural\nnetworks on graphs with fast localized spectral ﬁltering,” in Proc.\nAdvances Neural Inf. Process. Syst., Barcelona, Spain, Dec. 2016, pp.\n3844–3852.\n[17] F. Gama, A. G. Marques, G. Leus, and A. Ribeiro, “Convolutional neural\nnetworks architectures for signals supported on graphs,” arXiv preprint\narXiv:1805.00165, 2018.\n[18] V. N. Ioannidis, A. G. Marques, and G. B. Giannakis, “Tensor graph\nconvolutional networks for multi-relational and robust learning,” IEEE\nTransactions on Signal Processing, vol. 68, pp. 6535–6546, 2020.\n[19] V. N. Ioannidis, A. G. Marques, and G. B. Giannakis, “A recursive\nmulti-layer graph neural network architecture for processing network\ndata,” in Proc. IEEE Int. Conf. Acoust., Speech, Sig. Process., London,\nEngland, May 2019.\n[20] L. Ruiz, F. Gama, A. G. Marques, and A. Ribeiro, “Median activation\nfunctions for graph neural networks,” arXiv preprint arXiv:1810.12165,\n2018.\n[21] V. N. Ioannidis, S. Chen, and G. B. Giannakis, “Efﬁcient and stable\ngraph scattering transforms via pruning,” IEEE Trans. Pattern Anal.\nMach. Intel., 2020.\n[22] W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on\ngraphs: Methods and applications,” arXiv preprint arXiv:1709.05584,\n2017.\n[23] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line:\nLarge-scale information network embedding,” in Proceedings of the 24th\ninternational conference on world wide web, 2015, pp. 1067–1077.\n[24] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and\nA. J. Smola, “Distributed large-scale natural graph factorization,” in\nProceedings of the 22nd international conference on World Wide Web,\n2013, pp. 37–48.\n[25] S. Cao, W. Lu, and Q. Xu, “Grarep: Learning graph representations\nwith global structural information,” in Proceedings of the 24th ACM\ninternational on conference on information and knowledge management,\n2015, pp. 891–900.\n[26] M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity\npreserving graph embedding,” in Proceedings of the 22nd ACM SIGKDD\ninternational conference on Knowledge discovery and data mining, 2016,\npp. 1105–1114.\n[27] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for\nnetworks,” in Proceedings of the 22nd ACM SIGKDD international\nconference on Knowledge discovery and data mining, 2016, pp. 855–\n864.\n[28] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning\nof social representations,” in Proceedings of the 20th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining, 2014,\npp. 701–710.\n[29] A. J. Smola and R. I. Kondor, “Kernels and regularization on graphs,” in\nLearning Theory and Kernel Machines.\nSpringer, 2003, pp. 144–158.\n[30] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A\ncomprehensive survey on graph neural networks,” IEEE Transactions on\nNeural Networks and Learning Systems, 2020.\n[31] P. Veliˇckovi´c, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D.\nHjelm, “Deep graph infomax,” arXiv preprint arXiv:1809.10341, 2018.\n[32] Y. Dong, N. V. Chawla, and A. Swami, “metapath2vec: Scalable\nrepresentation learning for heterogeneous networks,” in Proceedings of\nthe 23rd ACM SIGKDD international conference on knowledge discovery\nand data mining, 2017, pp. 135–144.\n[33] C. Shi, B. Hu, W. X. Zhao, and S. Y. Philip, “Heterogeneous informa-\ntion network embedding for recommendation,” IEEE Transactions on\nKnowledge and Data Engineering, vol. 31, no. 2, pp. 357–370, 2018.\n[34] J. Shang, M. Qu, J. Liu, L. M. Kaplan, J. Han, and J. Peng, “Meta-path\nguided embedding for similarity search in large-scale heterogeneous\ninformation networks,” arXiv preprint arXiv:1610.09769, 2016.\n[35] M. Belkin, I. Matveeva, and P. Niyogi, “Regularization and semi-\nsupervised learning on large graphs,” in Proc. Annual Conf. Learning\nTheory, vol. 3120.\nBanff, Canada: Springer, Jul. 2004, pp. 624–638.\n[36] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization: A\ngeometric framework for learning from labeled and unlabeled examples,”\nJ. Mach. Learn. Res., vol. 7, pp. 2399–2434, 2006.\n[37] V. N. Ioannidis, M. Ma, A. Nikolakopoulos, G. B. Giannakis, and\nD. Romero, “Kernel-based inference of functions on graphs,” in Adaptive\nLearning Methods for Nonlinear System Modeling, D. Comminiello and\nJ. Principe, Eds.\nElsevier, 2018.\n[38] X. Zhu, Z. Ghahramani, and J. D. Lafferty, “Semi-supervised learning\nusing gaussian ﬁelds and harmonic functions,” in Proc. Int. Conf. Mach.\nLearn., Washington, USA, Jun. 2003, pp. 912–919.\n[39] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,\n“The emerging ﬁeld of signal processing on graphs: Extending high-\ndimensional data analysis to networks and other irregular domains,”\nIEEE Sig. Process. Mag., vol. 30, no. 3, pp. 83–98, May 2013.\n[40] E. Ceci, Y. Shen, G. B. Giannakis, and S. Barbarossa, “Graph-based\nlearning under perturbations via total least-squares,” IEEE Transactions\non Signal Processing, vol. 68, pp. 2870–2882, 2020.\n[41] E. Ceci and S. Barbarossa, “Graph signal processing in the presence of\ntopology uncertainties,” IEEE Transactions on Signal Processing, vol. 68,\npp. 1558–1573, 2020.\n[42] S. Segarra, A. G. Marques, G. Leus, and A. Ribeiro, “Reconstruction of\ngraph signals through percolation from seeding nodes,” arXiv preprint\narXiv:1507.08364, 2015.\n[43] V. N. Ioannidis, A. S. Zamzam, G. B. Giannakis, and N. D. Sidiropoulos,\n“Coupled graphs and tensor factorization for imputation and community\ndetection,” IEEE Trans. on Knowledge and Data Engineering (Submitted\nSep.), 2018.\n[44] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and\nY. Bengio, “Graph attention networks,” in Proc. Int. Conf. on Learn.\nRepresantions, 2018.\n[45] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and\nM. Welling, “Modeling relational data with graph convolutional networks,”\nin European Semantic Web Conference.\nSpringer, 2018, pp. 593–607.\n[46] X. Fu, J. Zhang, Z. Meng, and I. King, “Magnn: Metapath aggregated\ngraph neural network for heterogeneous graph embedding,” in Proceed-\nings of The Web Conference 2020, 2020, pp. 2331–2341.\n[47] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu,\n“Heterogeneous graph attention network,” in The World Wide Web\nConference, 2019, pp. 2022–2032.\n12\n[48] Y. Zhou, Y. Hou, J. Shen, Y. Huang, W. Martin, and F. Cheng, “Network-\nbased drug repurposing for novel coronavirus 2019-ncov/sars-cov-2,”\nCell discovery, vol. 6, no. 1, pp. 1–18, 2020.\n[49] V. N. Ioannidis, X. Song, S. Manchanda, M. Li, X. Pan, D. Zheng,\nX. Ning, X. Zeng, and G. Karypis, “Drkg - drug repurposing knowledge\ngraph for covid-19,” https://github.com/gnn4dr/DRKG/, 2020.\n[50] D. Zheng, X. Song, C. Ma, Z. Tan, Z. Ye, J. Dong, H. Xiong, Z. Zhang,\nand G. Karypis, “Dgl-ke: Training knowledge graph embeddings at scale,”\narXiv preprint arXiv:2004.08532, 2020.\n[51] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and\nT. Darrell, “Decaf: A deep convolutional activation feature for generic\nvisual recognition,” in International conference on machine learning,\n2014, pp. 647–655.\n[52] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies\nfor accurate object detection and semantic segmentation,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition,\n2014, pp. 580–587.\n[53] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun, “Gpt-gnn: Generative\npre-training of graph neural networks,” in arxiv, 2020.\n[54] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang,\nand J. Tang, “Gcc: Graph contrastive coding for graph neural network\npre-training,” in arxiv, 2020.\n[55] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec,\n“Strategies for pre-training graph neural networks,” in International\nConference on Learning Representations, 2019.\n[56] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,\n“Neural message passing for quantum chemistry,” in International\nConference on Machine Learning, 2017.\n[57] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zam-\nbaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner\net al., “Relational inductive biases, deep learning, and graph networks,”\narXiv preprint arXiv:1806.01261, 2018.\n[58] M. Wang, L. Yu, D. Zheng, Q. Gan, Y. Gai, Z. Ye, M. Li, J. Zhou,\nQ. Huang, C. Ma et al., “Deep graph library: Towards efﬁcient and\nscalable deep learning on graphs,” arXiv preprint arXiv:1909.01315,\n2019.\n[59] G. Yuan, P. K. Murukannaiah, Z. Zhang, and M. P. Singh, “Exploiting\nsentiment homophily for link prediction,” in Proceedings of the 8th ACM\nConference on Recommender systems, 2014, pp. 17–24.\n[60] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques for\nrecommender systems,” Computer, vol. 42, no. 8, pp. 30–37, 2009.\n[61] N. K. Ahmed, J. Neville, R. A. Rossi, N. G. Dufﬁeld, and T. L. Willke,\n“Graphlet decomposition: Framework, algorithms, and applications,”\nKnowledge and Information Systems, vol. 50, no. 3, pp. 689–722, 2017.\n[62] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman,\nand A. Y. Wu, “An efﬁcient k-means clustering algorithm: Analysis and\nimplementation,” IEEE transactions on pattern analysis and machine\nintelligence, vol. 24, no. 7, pp. 881–892, 2002.\n[63] M. M. Babu, N. M. Luscombe, L. Aravind, M. Gerstein, and S. A. Te-\nichmann, “Structure and evolution of transcriptional regulatory networks,”\nCurrent opinion in structural biology, vol. 14, no. 3, pp. 283–291, 2004.\n[64] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bach-\nman, A. Trischler, and Y. Bengio, “Learning deep representations\nby mutual information estimation and maximization,” arXiv preprint\narXiv:1808.06670, 2018.\n[65] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning represen-\ntations by back-propagating errors,” Nature, vol. 323, no. 6088, p. 533,\n1986.\n[66] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection by\ndeep multi-task learning,” in European conference on computer vision.\nSpringer, 2014, pp. 94–108.\n[67] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich, “Gradnorm:\nGradient normalization for adaptive loss balancing in deep multitask\nnetworks,” arXiv preprint arXiv:1711.02257, 2017.\n[68] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph\nneural networks?” in Proc. Int. Conf. on Learn. Represantions, 2019.\n[69] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimization,”\nin Proc. Int. Conf. on Learn. Represantions, San Diego, CA, USA, May\n2015.\n[70] www.imdb.com, 2020.\n[71] www.openacademic.ai/oag/, 2020.\n[72] D. M. Powers, “Evaluation: from precision, recall and f-measure to roc,\ninformedness, markedness and correlation,” Journal of Machine Learning\nTechnologies, vol. 2, p. 37–63, 2011.\n[73] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko,\n“Translating embeddings for modeling multi-relational data,” in Neural\nInformation Processing Systems (NIPS), 2013, pp. 1–9.\n[74] D. Poole, “Simple embedding for link prediction in knowledge graphs,”\nin NeurIPS, 2018.\n[75] W. DS, F. YD, G. AC, L. EJ, M. A, G. JR, S. T, J. D, L. C, S. Z, A. N,\nI. I, L. Y, M. A, G. N, W. A, C. L, C. R, L. D, P. A, K. C, and W. M,\n“Drugbank 5.0: a major update to the drugbank database for 2018 nucleic\nacids res,” PubMed, 2017.\n[76] S. D, G. AL, L. D, J. A, W. S, H.-C. J, S. M, D. NT, M. JH, B. P, J. LJ,\nand von Mering C., “String v11: protein-protein association networks\nwith increased coverage, supporting functional discovery in genome-wide\nexperimental datasets.” PubMed, 2019.\n[77] S. Orchard, M. Ammari, B. Aranda, L. Breuza, L. Briganti, F. Broackes-\nCarter, N. H. Campbell, G. Chavali, C. Chen, N. Del-Toro et al., “The\nmintact project—intact as a common curation platform for 11 molecular\ninteraction databases,” Nucleic acids research, vol. 42, no. D1, pp. D358–\nD363, 2014.\n[78] K. C. Cotto, A. H. Wagner, Y.-Y. Feng, S. Kiwala, A. C. Coffman,\nG. Spies, A. Wollam, N. C. Spies, O. L. Grifﬁth, and M. Grifﬁth,\n“DGIdb 3.0: a redesign and expansion of the drug–gene interaction\ndatabase,” Nucleic Acids Research, vol. 46, no. D1, pp. D1068–D1073,\n11 2017. [Online]. Available: https://doi.org/10.1093/nar/gkx1143\n[79] D. E. Gordon, G. M. Jang, M. Bouhaddou, J. Xu, K. Obernier, M. J.\nO’meara, J. Z. Guo, D. L. Swaney, T. A. Tummino, R. Huttenhain et al.,\n“A sars-cov-2-human protein-protein interaction map reveals drug targets\nand potential drug-repurposing,” Nature, 2020.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-07-20",
  "updated": "2021-03-04"
}