{
  "id": "http://arxiv.org/abs/2007.10854v1",
  "title": "Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification",
  "authors": [
    "Jianing Li",
    "Shiliang Zhang"
  ],
  "abstract": "Unsupervised domain adaptive person Re-IDentification (ReID) is challenging\nbecause of the large domain gap between source and target domains, as well as\nthe lackage of labeled data on the target domain. This paper tackles this\nchallenge through jointly enforcing visual and temporal consistency in the\ncombination of a local one-hot classification and a global multi-class\nclassification. The local one-hot classification assigns images in a training\nbatch with different person IDs, then adopts a Self-Adaptive Classification\n(SAC) model to classify them. The global multi-class classification is achieved\nby predicting labels on the entire unlabeled training set with the Memory-based\nTemporal-guided Cluster (MTC). MTC predicts multi-class labels by considering\nboth visual similarity and temporal consistency to ensure the quality of label\nprediction. The two classification models are combined in a unified framework,\nwhich effectively leverages the unlabeled data for discriminative feature\nlearning. Experimental results on three large-scale ReID datasets demonstrate\nthe superiority of proposed method in both unsupervised and unsupervised domain\nadaptive ReID tasks. For example, under unsupervised setting, our method\noutperforms recent unsupervised domain adaptive methods, which leverage more\nlabels for training.",
  "text": "Joint Visual and Temporal Consistency for\nUnsupervised Domain Adaptive Person\nRe-Identiﬁcation\nJianing Li, and Shiliang Zhang\nDepartment of Computer Science, School of EE&CS, Peking University,\nBeijing 100871, China\n{ljn-vmc,slzhang.jdl}@pku.edu.cn\nAbstract. Unsupervised domain adaptive person Re-IDentiﬁcation (ReID)\nis challenging because of the large domain gap between source and tar-\nget domains, as well as the lackage of labeled data on the target domain.\nThis paper tackles this challenge through jointly enforcing visual and\ntemporal consistency in the combination of a local one-hot classiﬁcation\nand a global multi-class classiﬁcation. The local one-hot classiﬁcation\nassigns images in a training batch with diﬀerent person IDs, then adopts\na Self-Adaptive Classiﬁcation (SAC) model to classify them. The global\nmulti-class classiﬁcation is achieved by predicting labels on the entire\nunlabeled training set with the Memory-based Temporal-guided Cluster\n(MTC). MTC predicts multi-class labels by considering both visual sim-\nilarity and temporal consistency to ensure the quality of label prediction.\nThe two classiﬁcation models are combined in a uniﬁed framework, which\neﬀectively leverages the unlabeled data for discriminative feature learn-\ning. Experimental results on three large-scale ReID datasets demonstrate\nthe superiority of proposed method in both unsupervised and unsuper-\nvised domain adaptive ReID tasks. For example, under unsupervised\nsetting, our method outperforms recent unsupervised domain adaptive\nmethods, which leverage more labels for training.\nKeywords: Domain Adaption, Person Re-Identiﬁcation, Convolution\nNeural Networks\n1\nIntroduction\nPerson Re-Identiﬁcation (ReID) aims to identify a probe person in a camera\nnetwork by matching his/her images or video sequences and has many promis-\ning applications like smart surveillance and criminal investigation. Recent years\nhave witnessed the signiﬁcant progresses on supervised person ReID in discrimi-\nnative feature learning from labeled person images [27,32,14,17,38,23] and videos\n[11,12,13]. However, supervised person ReID methods rely on a large amount of\nlabeled data which is expensive to annotate. Deep models trained on the source\ndomain suﬀer substantial performance drop when transferred to a diﬀerent tar-\nget domain. Those issues make it hard to deploy supervised ReID models in real\napplications.\narXiv:2007.10854v1  [cs.CV]  21 Jul 2020\n2\nJ. Li, and S. Zhang\nTo tackle this problem, researchers focus on unsupervised learning [39,5,29],\nwhich could take advantage of abundant unlabeled data for training. Compared\nwith supervised learning, unsupervised learning relieves the requirement for ex-\npensive data annotation, hence shows better potential to push person ReID\ntowards real applications. Recent works deﬁne unsupervised person ReID as a\ntransfer learning task, which leverages labeled data on other domains. Related\nworks can be summarized into two categories, e.g., (1) using Generative Adver-\nsarial Network (GAN) to transfer the image style from labeled source domain to\nunlabeled target domain while preserving identity labels for training [31,41,39],\nor (2) pre-training a deep model on source domain, then clustering unlabeled\ndata in target domain to estimate pseudo labels for training\n[5,34]. The sec-\nond category has signiﬁcantly boosted the performance of unsupervised person\nReID. However, there is still a considerable performance gap between supervised\nand unsupervised person ReID. The reason may be because many persons share\nsimilar appearance and the same person could exhibit diﬀerent appearances,\nleading to unreliable label estimation. Therefore, more eﬀective ways to utilize\nthe unlabeled data should still be investigated.\nThis work targets to learn discriminative features for unlabeled target domain\nthrough generating more reliable label predictions. Speciﬁcally, reliable labels\ncan be predicted from two aspects. First, since each training batch samples a\nsmall number of images from the training set, it is likely that those images are\nsampled from diﬀerent persons. We thus could label each image with a distinct\nperson ID and separate them from each other with a classiﬁcation model. Second,\nit is not reliable to estimate labels on the entire training set with only visual\nsimilarity. We thus consider both visual similarity and temporal consistency\nfor multi-class label prediction, which is hence utilized to optimize the inter and\nintra class distances. Compared with previous methods, which only utilize visual\nsimilarity to cluster unlabeled images [5,34], our method has potential to exhibit\nbetter robustness to visual variance. Our temporal consistency is inferred based\non the video frame number, which can be easily acquired without requiring extra\nannotations or manual alignments.\nThe above intuitions lead to two classiﬁcation tasks for feature learning. The\nlocal classiﬁcation in each training batch is conducted by a Self-Adaptive Classiﬁ-\ncation (SAC) model. Specially, in each training batch, we generate a self-adaptive\nclassiﬁer from image features and apply one-hot label to separate images from\neach other. The feature optimization in the entire training set is formulated as a\nmulti-label classiﬁcation task for global optimization. We propose the Memory-\nbased Temporal-Guided Cluster (MTC) to predict multi-class labels based on\nboth visual similarity and temporal consistency. In other words, two images are\nassigned with the same label if they a) share large visual similarity and b) share\nenough temporal consistency.\nInspired by [30], we compute the temporal consistency based on the distri-\nbution of time interval between two cameras, i.e., interval of frame numbers of\ntwo images. For example, when we observe a person appears in camera i at time\nt, according to the estimated distribution, he/she would have high possibility to\nJVTC for Unsupervised Domain Adaptive Person ReID\n3\nbe recorded by camera j at time t + ∆t, and has low possibility will be recorded\nby another camera k. This cue would eﬀectively ﬁlter hard negative samples\nwith similar visual appearance, as well as could be applied in ReID to reduce\nthe search space. To further ensure the accuracy of clustering result, MTC uti-\nlizes image features stored in the memory bank. Memory bank is updated with\naugmented features after each training iteration to improve feature robustness.\nThe two classiﬁcation models are aggregated in a uniﬁed framework for\ndiscriminative feature learning. Experiments on three large-scale person ReID\ndatasets show that, our method exhibits substantial superiority to existing un-\nsupervised and domain adaptive ReID methods. For example, we achieve rank1\naccuracy of 79.5% on Market-1501 with unsupervised training, and achieve 86.8%\nafter unsupervised domain transfer, respectively.\nOur promising performance is achieved with the following novel components.\n1) The SAC model eﬃciently performs feature optimization in each local training\nbatch by assigning images with diﬀerent labels. 2) The MTC method performs\nfeature optimization in the global training set by predicting labels with visual\nsimilarity and temporal consistency. 3) Our temporal consistency does not re-\nquire any extra annotations or manual alignments, and could be utilized in both\nmodel training and ReID similarity computation. To the best of our knowledge,\nthis is an early unsupervised person ReID work utilizing temporal consistency\nfor label prediction and model training.\n2\nRelated Work\nThis work is closely related to unsupervised domain adaptation and unsuper-\nvised domain adaptive person ReID. This section brieﬂy summarizes those two\ncategories of works.\nUnsupervised Domain Adaptation (UDA) has been extensively studied in\nimage classiﬁcation. The aim of UDA is to align the domain distribution between\nsource and target domains. A common solution of UDA is to deﬁne and minimize\nthe domain discrepancy between source and target domain. Gretton et.al. [9]\nproject data samples into a reproducing kernel Hilbert space and compute the\ndiﬀerence of sample means to reduce the Maximum Mean Discrepancy (MMD).\nSun et.al. [28] propose to learn a transformation to align the mean and covariance\nbetween two domains in the feature space. Pan et.al. [24] propose to align each\nclass in source and target domain through Prototypical Networks. Adversarial\nlearning is also widely used to minimize domain shift. Ganin et.al. [6] propose a\nGradient Reversal Layer (GRL) to confuse the feature learning model and make\nit can’t distinguish the features from source and target domains. DRCN [7] takes\na similar approach but also performs multi-task learning to reconstruct target\ndomain images. Diﬀerent from domain adaption in person ReID, traditional\nUDA mostly assumes that the source domain and target domain share same\nclasses. However, in person ReID, diﬀerent domain commonly deals with diﬀerent\npersons, thus have diﬀerent classes.\n4\nJ. Li, and S. Zhang\nUnsupervised Domain Adaptive Person ReID: Early methods design hand\ncraft features for person ReID [8,20]. Those methods can be directly adapted to\nunlabeled dataset, but show unsatisfactory performance. Recent works propose\nto train deep models on labeled source domain and then transfer to unlabeled\ntarget domain. Yu et.al. [34] use the labeled source dataset as a reference to learn\nsoft labels. Fu et.al. [5] cluster the global and local features to estimate pseudo\nlabels, respectively. Generative Adversarial Network (GAN) is also applied to\nbridge the gap across cameras or domains. Wei et.al. [31] transfer images from the\nsource domain to target domain while reserving the identity labels for training.\nZhong et.al. [40] apply CycleGAN [42] to generate images under diﬀerent camera\nstyles for data augmentation. Zhong et.al. [39] introduce the memory bank [33]\nto minimize the gap between source and target domains.\nMost existing methods only consider visual similarity for feature learning on\nunlabeled data, thus are easily inﬂuenced by the large visual variation and do-\nmain bias. Diﬀerent from those works, we consider visual similarity and temporal\nconsistency for feature learning. Compared with existing unsupervised domain\nadaptive person ReID methods, our method exhibits stronger robustness and\nbetter performance. As shown in our experiments, our approach outperforms re-\ncent ReID methods under both unsupervised and unsupervised domain adaptive\nsettings. To the best of our knowledge, this is an early attempt to jointly consider\nvisual similarity and temporal consistency in unsupervised domain adaptive per-\nson ReID. Another person ReID work [30] also uses temporal cues. Diﬀerent with\nour work, it focuses on supervised training and only uses temporal cues in the\nReID stage for re-ranking.\n3\nProposed Method\n3.1\nFormulation\nFor any query person image q, the person ReID model is expected to produce a\nfeature vector to retrieve the image g containing the same person from a gallery\nset. In other words, the ReID model should guarantee q share more similar fea-\nture with g than with other images. Therefore, learning a discriminative feature\nextractor is critical for person ReID.\nIn unsupervised domain adaptive person ReID, we have an unlabeled target\ndomain T = {ti}NT\ni=1 containing NT person images. Additionally, a labeled source\ndomain S = {si, yi}NS\ni=1 containing NS labeled person images is provided as an\nauxiliary training set, where yi is the identity label associated with the person\nimage si. The goal of domain adaptive person ReID is to learn a discriminative\nfeature extractor f(·) for T, using both S and T.\nThe training of f(·) can be conducted by minimizing the training loss on\nboth source and target domains. With person ID labels, the training on S can\nbe considered as a classiﬁcation task by minimizing the cross-entropy loss, i.e.,\nLsrc = −1\nNS\nNS\nX\ni=1\nlog P(yi|si),\n(1)\nJVTC for Unsupervised Domain Adaptive Person ReID\n5\nwhere P(yi|si) is the predicted probability of sample si belonging to class yi.\nThis supervised learning ensures the performance of f(·) on source domain.\nTo gain discriminative power of f(·) to the target domain, we further com-\npute training loss with predicted labels on T. First, because each training batch\nsamples nT , nT ≪NT images from T, it is likely that nT images are sampled\nfrom diﬀerent persons. We thus simply label each image ti in the mini-batch with\na distinct person ID label, i.e., an one-hot vector li with li[j] = 1 only if i = j.\nA Self-Adaptive Classiﬁcation (SAC) model is adopted to separate images of\ndiﬀerent persons in the training batch. The objective of SAC can be formulated\nas minimizing the classiﬁcation loss, i.e.,\nLlocal = 1\nnT\nnT\nX\ni=1\nL(V × f(ti), li),\n(2)\nwhere nT denotes the number of images in a training batch. f(·) produces a d-\ndim feature vector. V stores nT d-dim vectors as the classiﬁer. V ×f(ti) computes\nthe classiﬁcation score, and L(·) computes the loss by comparing classiﬁcation\nscores and one-hot labels. Details of classiﬁer V will be given in Sec. 3.2.\nBesides the local optimization in each training batch, we further predict\nlabels on the entire T and perform a global optimization. Since each person\nmay have multiple images in T, we propose the Memory-based Temporal-guide\nCluster (MTC) to predict a multi-class label for each image. For an image ti,\nMTC predicts its multi-class label mi, where mi[j] = 1 only if ti and tj are\nregarded as containing the same person.\nPredicted label mi allows for a multi-label classiﬁcation on T. We introduce a\nmemory bank K ∈RNT ×d to store NT image features as a NT -class classiﬁer [39].\nThe multi-label classiﬁcation loss is computed by classifying image feature f(ti)\nwith the memory bank K, then comparing the classiﬁcation scores with multi-\nclass label mi. The multi-label classiﬁcation loss on T can be represented as\nLglobal =\n1\nNT\nNT\nX\ni=1\nL(K × f(ti), mi),\n(3)\nwhere K×f(ti) produces the classiﬁcation score. The memory bank K is updated\nafter each training iteration as\nK[i]t = (1 −α)K[i]t−1 + α f(ti),\n(4)\nwhere the superscript t denotes the training epoch, α is the updating rate. De-\ntailed of MTC and mi computation will be presented in Sec. 3.3.\nBy combining the above losses computed on S and T, the overall training\nloss of our method can be formulated as,\nL = Lsrc + w1Llocal + w2Lglobal,\n(5)\nwhere w1 and w2 are loss weights.\n6\nJ. Li, and S. Zhang\nMemory-based \nTemporal-guided \nCluster (MTC)\nunlabeled target domain\naugmented images\nlabeled source domain\nmemory bank\nSelf-Adaptive\nClassification \n(SAC)\nCNN\ntraining batch\nsrc\nlocal\nglobal\nFig. 1. Overview of the proposed framework for unsupervised domain adaptive ReID\nmodel training. Lsrc is computed on the source domain. SAC computes Llocal in each\ntraining batch. MTC computes Lglobal on the entire target domain. SAC and MTC\npredict one-hot label and multi-class label for each image, respectively. Without Lsrc,\nour framework works as unsupervised training.\nThe accuracy of predicted labels, i.e., l and m is critical for the training on\nT. The accuracy of l can be guaranteed by setting batch size nT ≪NT , and\nusing careful sampling strategies. To ensure the accuracy of m, MTC considers\nboth visual similarity and temporal consistency for label prediction.\nWe illustrate our training framework in Fig. 1, where Llocal can be eﬃciently\ncomputed within each training batch by classifying a few images. Lglobal is a more\npowerful supervision by considering the entire training set T. The combination of\nLlocal and Lglobal utilizes both temporal and visual consistency among unlabeled\ndata and guarantees strong robustness of the learned feature extractor f(·). The\nfollowing parts proceed to introduces the computation of Llocal in SAC, and\nLlocal in MTC, respectively.\n3.2\nSelf-Adaptive Classiﬁcation\nSAC classiﬁes unlabeled data in each training batch. As shown in Eq. (2), the\nkey of SAC is the classiﬁer V. For a batch consisting of nT images, the classiﬁer\nV is deﬁned as a nT × d sized tensor, where the i-th d-dim vector represents the\nclassiﬁers for the i-th image. To enhance its robustness, V is calculated based on\nfeatures of original images and their augmented duplicates.\nSpeciﬁcally, for an image ti in training batch, we generate k images t(j)\ni\n(j =\n1, 2, ..., k) with image argumentation. This enlarges the training batch to nT ×\nJVTC for Unsupervised Domain Adaptive Person ReID\n7\n(k + 1) images belonging to nT categories. The classiﬁer V is computed as,\nV = [v1, v2, ...vnT ] ∈RnT ×d, vi =\n1\nk + 1(f(ti) +\nk\nX\nj=1\nf(t(j)\ni )),\n(6)\nwhere vi is the averaged feature of ti and its augmented images. It can be inferred\nthat, the robustness of V enhances as f(·) gains more discriminative power. We\nthus call V as a self-adapted classiﬁer.\nData augmentation is critical to ensure the robustness of V to visual varia-\ntions. We consider each camera as a style domain and adopt CycleGAN [42] to\ntrain camera style transfer models [40]. For each image under a speciﬁc camera,\nwe totally generate C −1 images with diﬀerent styles, where C is the camera\nnumber in the target domain. We set k < C −1. Therefore, each training batch\nrandomly selects k augmented images for training.\nBased on classiﬁer V and the one-hot label l, the Llocal of SAC can be for-\nmulated as the cross-entropy loss, i.e.,\nLlocal = −\n1\nnT × (k + 1)\nnT\nX\ni=1\n(log(P(i|ti) +\nk\nX\nj=1\nlog(P(i|t(j)\ni )),\n(7)\nwhere P(i|ti) is the probability of image ti being classiﬁed to label i, i.e.,\nP(i|ti) =\nexp(vT\ni · f(ti)/β1)\nPnT\nn=1 exp(vTn · f(ti)/β1)\n(8)\nwhere β1 is a temperature factor to balance the feature distribution.\nLlocal can be eﬃciently computed on nT images. Minimizing Llocal enlarges\nthe feature distance of images in the same training batch, meanwhile decreases\nthe feature distance of augmented images in the same category. It thus boosts\nthe discriminative power of f(·) on T.\n3.3\nMemory-based Temporal-guided Cluster\nMTC predicts the multi-class label mi for image ti through clustering images\nin T, i.e., images inside the same cluster are assigned with the same label. The\nclustering is conducted based on the pair-wise similarity considering both visual\nsimilarity and temporal consistency of two images.\nVisual similarity can be directly computed using the feature extractor f(·)\nor the features stored in the memory bank K. Using f(·) requires to extract fea-\ntures for each image in T, which introduces extra time consumption. Meanwhile,\nthe features in K can be enhanced by diﬀerent image argumentation strategies,\nmaking them more robust. We hence use features in K to compute the visual\nsimilarity between two images ti and tj, i.e.,\nvs(ti, tj) = cosine(K[i], K[j]),\n(9)\n8\nJ. Li, and S. Zhang\nquery\nquery\nquery\nquery\nFig. 2. Illustration of person ReID results on DukeMTMC-reID dataset. Each example\nshows the top-5 retrieved images by visual similarity (ﬁrst tow) and joint similarity\ncomputed in Eq. (12) (second row). The true match is annotated by the green bounding\nbox and false match is annotated by the red bounding box.\nwhere vs(·) computes the visual similarity with cosine distance.\nTemporal consistency is independent to visual features and is related to the\ncamera id and frame id of each person image. Suppose we have two images ti\nfrom camera a and tj from camera b with frame IDs fidi and fidj, respectively.\nThe temporal consistency between ti and tj can be computed as,\nts(ti, tj) = H(a,b)(fidi −fidj),\n(10)\nwhere H(a,b)(·) is a function for camera pair (a, b). It estimates the temporal\nconsistency based on frame id interval of ti and tj, which reﬂects the time interval\nwhen they are recorded by cameras a and b.\nH(a,b)(·) can be estimated based on a histogram ¯H(a,b)(int), which shows the\nprobability of appearing identical person at camera a and b for frame id interval\nint. ¯H(a,b)(int) can be easily computed on datasets with person ID labels. To\nestimate it on unlabeled T, we ﬁrst cluster images in T with visual similarity in\nEq. (9) to acquire pseudo person ID labels. Suppose n(a,b) is the total number of\nimage pairs containing identical person in camera a and b. The value of int-th\nbin in histogram, i.e., ¯H(a,b)(int) is computed as,\n¯H(a,b)(int) = nint\n(a,b)/n(a,b),\n(11)\nwhere nint\n(a,b) is the number of image pairs containing identical person in camera\na and b with frame id intervals int.\nFor a dataset with C cameras, C(C −1)/2 histograms will be computed. We\nﬁnally use Gaussian function to smooth the histogram and take the smoothed\nhistogram as H(a,b)(·) for temporal consistency computation.\nJVTC for Unsupervised Domain Adaptive Person ReID\n9\nOur ﬁnal pair-wise similarity is computed based on vs(·) and ts(·). Because\nthose two similarities have diﬀerent value ranges, we ﬁrst normalize them, then\nperform the fusion. This leads to the joint similarity function J(·), i.e.,\nJ(ti, tj) = 1/(1 + λ0e−γ0 vs(ti,tj)) × 1/(1 + λ1e−γ1 ts(ti,tj)),\n(12)\nwhere λ0 and λ1 are smoothing factors, γ0 and γ1 are shrinking factors.\nEq. (12) computes more reliable similarities between images than either\nEq. (9) or Eq. (10). J(·) can also be used in person ReID for query-gallery\nsimilarity computation. Fig. 2 compares some ReID results achieved by visual\nsimilarity and joint similarity, respectively. It can be observed that, the joint\nsimilarity is more discriminative than the visual similarity.\nWe hence cluster images in target domain T based on J(·) and assign the\nmulti-class label for each image. For an image ti, its multi-class label mi[j] = 1\nonly if ti and tj are in the same cluster. Based on m, the Lglobal on T can be\ncomputed as,\nLglobal = −1\nNT\nNT\nX\ni=1\nNT\nX\nj=1\nmi[j] × log ¯P(j|ti)/|mi|1,\n(13)\nwhere | · |1 computes the L-1 norm. ¯P(j|ti) denotes the probability of image ti\nbeing classiﬁed to the j-th class in multi-label classiﬁcation, i.e.,\n¯P(j|ti) =\nexp(K[j]T · f(ti)/β2)\nPNT\nn=1 exp(K[n]T · f(ti)/β2)\n,\n(14)\nwhere β2 is the temperature factor. The following section proceeds to discuss\nthe eﬀects of parameters and conduct comparisons with recent works.\n4\nExperiment\n4.1\nDataset\nWe evaluate our methods on three widely used person ReID datasets, e.g., Mar-\nket1501 [36], DukeMTMC-ReID [37,26], and MSMT17 [31], respectively.\nMarket1501 consists of 32,668 images of 1,501 identities under 6 cameras.\nThe dataset is divided into training and test sets, which contains 12,936 images\nof 751 identities and 19,732 images of 750 identities, respectively.\nDukeMTMC-ReID is composed of 1,812 identities and 36,411 images un-\nder 8 cameras. 16,522 images of 702 pedestrians are used for training. The other\nidentities and images are included in the testing set.\nMSMT17 is currently the largest image person ReID dataset. MSMT17\ncontains 126,441 images of 4,101 identities under 15 cameras. The training set\nof MSMT17 contains 32,621 bounding boxes of 1,041 identities, and the testing\nset contains 93,820 bounding boxes of 3,060 identities.\nWe follow the standard settings in previous works [5,39] for training in do-\nmain adaptive person ReID and unsupervised person ReID, respectively. Per-\nformance is evaluated by the Cumulative Matching Characteristic (CMC) and\nmean Average Precision (mAP). We use JVTC to denote our method.\n10\nJ. Li, and S. Zhang\nTable 1. Evaluation of individual components of JVTC.\nDataset\nDukeMTMC →Market1501\nMarket1501 →DukeMTMC\nMethod\nmAP\nr1\nr5\nr10\nr20\nmAP\nr1\nr5\nr10\nr20\nSupervised\n69.7\n86.3\n94.3\n96.5\n97.6\n61.0\n80.2\n89.1\n91.9\n94.2\nDirect transfer\n18.2\n42.1\n60.7\n67.9\n74.8\n16.6\n31.8\n48.4\n55.0\n61.7\nBaseline\n46.6\n77.4\n89.5\n93.0\n95.1\n43.6\n66.1\n77.7\n81.7\n84.8\nSAC\n41.8\n64.5\n76.0\n79.6\n92.3\n37.5\n59.4\n74.1\n78.3\n81.4\nMTC\n56.4\n79.8\n91.0\n93.9\n95.9\n51.1\n71.3\n81.1\n84.3\n86.3\nJVTC\n61.1\n83.8\n93.0\n95.2\n96.9\n56.2\n75.0\n85.1\n88.2\n90.4\nJVTC+\n67.2\n86.8 95.2 97.1 98.1\n66.5\n80.4 89.9 92.2 93.7\n4.2\nImplementation Details\nWe adopt ResNet50 [10] as the backbone and add a 512-dim embedding layer\nfor feature extraction. We initialize the backbone with the model pre-trained\non ImageNet [2]. All models are trained and ﬁnetuned with PyTorch. Stochastic\nGradient Descent (SGD) is used to optimize our model. Input images are resized\nto 256 × 128. The mean value is subtracted from each (B, G, and R) channel.\nThe batch size is set as 128 for both source and target domains. Each training\nbatch in the target domain contains 32 original images and each image has 3\naugmented duplicates, i.e., we set k=3.\nThe temperature factor β1 is set as 0.1 and β2 is set as 0.05. The smoothing\nfactors and shrinking factors λ0, λ1, γ0 and γ1 in Eq. (12) are set as 1, 2, 5 and 5,\nrespectively. The initial learning rate is set as 0.01, and is reduced by ten times\nafter 40 epoches. The multi-class label m are updated every 5 epochs based on\nvisual similarity initially, and the joint similarity is introduced at 30-th epoch.\nOnly local loss Llocal is applied at the initial epoch. The Lglobal is applied at the\n10-th epoch. The training is ﬁnished after 100 epoches. The memory updating\nrate α starts from 0 and grows linearly to 1. The loss weights w1 and w2 are set\nas 1 and 0.2, respectively. DBSCAN [4] is applied for clustering.\n4.3\nAblation Study\nEvaluation of Individual Components: This section investigates the ef-\nfectiveness of each component in our framework, e.g., the SAC and MTC. We\nsummarize the experimental results in Table 1. In the table, “Supervised” de-\nnotes training deep models with labeled data on the target domain, and testing\non the testing set. “Direct transfer” denotes directly using the model trained\non source domain for testing. “Baseline” uses memory bank for multi-label clas-\nsiﬁcation, but predicts multi-class label only based on visual similarity. “SAC”\nis implemented based on “Direct transfer” by applying SAC model for one-hot\nclassiﬁcation. “MTC” utilizes both visual similarity and temporal consistency\nfor multi-class label prediction. “JVTC” combines SAC and MTC. “JVTC+”\ndenotes using the joint similarity for person ReID.\nJVTC for Unsupervised Domain Adaptive Person ReID\n11\nMarket1501\nDukeMTMC\nMarket1501\nDukeMTMC\n60\n65\n70\n75\n80\n85\n0 0.1 0.2 0.5 1\n2\n5\n60\n65\n70\n75\n80\n85\n0 0.1 0.2 0.5 1\n2\n5\nRank1 accuracy(%)\nRank1 accuracy (%)\n(b) Effects of loss weights\n50\n60\n70\n80\n90\n0.01 0.05 0.1 0.2 0.5\n1\n45\n55\n65\n75\n85\n0.01 0.05 0.1 0.2 0.5\n1\nRank1 accuracy (%)\nRank1 accuracy (%)\nβ1\nβ2\n(a) Effects of temperature factors\nFig. 3. Inﬂuences of temperature factors β1 and β2 in (a), and loss weights w1, w2 in\n(b). Experiments are conducted on Market1501 and DukeMTMC-reID.\nTable 1 shows that, supervised learning on the target domain achieves promis-\ning performance. However, directly transferring the supervised model to diﬀerent\ndomains leads to substantial performance drop, e.g., the rank1 accuracy drops\nto 44.2% on Market1501 and 48.4% on DukeMTMC-reID after direct transfer.\nThe performance drop is mainly caused by the domain bias between datasets.\nIt is also clear that, SAC consistently outperforms direct transfer by large\nmargins. For instance, SAC improves the rank1 accuracy from 42.1% to 64.5%\nand 31.8% to 59.4% on Market-1501 and DukeMTMC-reID, respectively. This\nshows that, although SAC is eﬃcient to compute, it eﬀectively boosts the ReID\nperformance on target domain. Compared with the baseline, MTC uses joint\nsimilarity for label prediction. Table 1 shows that, MTC performs better than\nthe baseline, e.g., outperforms baseline by 9.8% and 5.2% in mAP on Market1501\nand DukeMTMC-reID, respectively. This performance gain clearly indicates the\nrobustness of our joint similarity.\nAfter combining SAC and MTC, JVTC achieves more substantial perfor-\nmance gains on two datasets. For instance, JVTC achieves mAP of 61.1% on\nMarket1501, much better than the 46.6% of baseline. “JVTC+” further uses\njoint similarity to compute the query-gallery similarity. It achieves the best per-\nformance, and outperforms the supervised training on target domain. We hence\ncould conclude that, each component in our method is important for performance\nboost, and their combination achieves the best performance.\nHyper-parameter Analysis: This section investigates some important hyper-\nparameters in our method, including the temperature factors β1, β2, and the\nloss weights w1 and w2, respectively. To make the evaluation possible, each\nexperiment varies the value of one hyper-parameter while keeping others ﬁxed.\nAll experiments are conducted with unsupervised domain adaptive ReID setting\non both Market-1501 and DukeMTMC-reID.\nFig. (3)(a) shows the eﬀects of temperature factors β1 and β2 in Eq. (8) and\nEq.(14). We can see that, a small temperature factor usually leads to better ReID\nperformance. This is because that smaller temperature factor leads to a smaller\nentropy in the classiﬁcation score, which is commonly beneﬁcial for classiﬁcation\n12\nJ. Li, and S. Zhang\nTable 2. Comparison with unsupervised, domain adaptive, and semi-supervised ReID\nmethods on Market1501 and DukeMTMC-reID.\nDataset\nMarket1501\nDukeMTMC\nMethod\nSource\nmAP\nr1\nr5\nr10\nr20\nSource\nmAP\nr1\nr5\nr10\nr20\nSupervised\nMarket\n69.7 86.3 94.3 96.5 97.6\nDuke\n61.0 80.2 89.1 91.9 94.2\nDirect transfer\nDuke\n18.2 42.1 60.7 67.9 74.8\nMarket\n16.6 31.8 48.4 55.0 61.7\nLOMO [20]\nNone\n8.0\n27.2 41.6 49.1\n-\nNone\n4.8\n12.3 21.3 26.6\n-\nBOW [36]\nNone\n14.8 35.8 52.4 60.3\n-\nNone\n8.3\n17.1 28.8 34.9\n-\nBUC [21]\nNone\n38.3 66.2 79.6 84.5\n-\nNone\n27.5 47.4 62.6 68.4\n-\nDBC [3]\nNone\n41.3 69.2 83.0 87.8\n-\nNone\n30.0 51.5 64.6 70.1\n-\nJVTC\nNone\n41.8 72.9 84.2 88.7 92.0\nNone\n42.2 67.6 78.0 81.6 84.5\nJVTC+\nNone\n47.5 79.5 89.2 91.9 94.0\nNone\n50.7 74.6 82.9 85.3 87.2\nPTGAN [31]\nDuke\n-\n38.6\n-\n66.1\n-\nMarket\n-\n27.4\n-\n50.7\n-\nCamStyle [41]\nDuke\n27.4 58.8 78.2 84.3 88.8\nMarket\n25.1 48.4 62.5 68.9 74.4\nT-Fusion [22]\nCUHK01\n-\n60.8 74.4 79.3\n-\n-\n-\n-\n-\n-\n-\nARN [19]\nDuke\n39.4 70.3 80.4 86.3 93.6\nMarket\n33.4 60.2 73.9 79.5 82.5\nMAR [34]\nMSMT17 40.0 67.7 81.9 87.3\n-\nMSMT17 48.0 67.1 79.8 84.2\n-\nECN [39]\nDuke\n43.0 75.1 87.6 91.6\n-\nMarket\n40.4 63.3 75.8 80.4\n-\nPDA-Net [18]\nDuke\n47.6 75.2 86.3 90.2\n-\nMarket\n45.1 63.2 77.0 82.5\n-\nPAST [35]\nDuke\n54.6 78.4\n-\n-\n-\nMarket\n54.3 72.4\n-\n-\n-\nCAL-CCE [25]\nDuke\n49.6 73.7\n-\n-\n-\nMarket\n45.6 64.0\n-\n-\n-\nCR-GAN [1]\nDuke\n54.0 77.7 89.7 92.7\n-\nMarket\n48.6 68.9 80.2 84.7\n-\nSSG [5]\nDuke\n58.3 80.0 90.0 92.4\n-\nMarket\n53.4 73.0 80.6 83.2\n-\nTAUDL [15]\nTracklet\n41.2 63.7\n-\n-\n-\nTracklet\n43.5 61.7\n-\n-\n-\nUTAL [16]\nTracklet\n46.2 69.2\n-\n-\n-\nTracklet\n43.5 62.3\n-\n-\n-\nSSG+ [5]\nDuke\n62.5 81.4 91.6 93.8\n-\nMarket\n56.7 74.2 83.5 86.7\n-\nSSG++ [5]\nDuke\n68.7 86.2 94.6 96.5\n-\nMarket\n60.3 76.0 85.8 89.3\n-\nJVTC\nDuke\n61.1 83.8 93.0 95.2 96.9\nMarket\n56.2 75.0 85.1 88.2 90.4\nJVTC+\nDuke\n67.2 86.8 95.2 97.1 98.1\nMarket\n66.5 80.4 89.9 92.2 93.7\nloss computation. However, too small temperature factor makes the training hard\nto converge. According to Fig. 3(a), we set β1 = 0.1, β2 = 0.05.\nFig. 3(b) shows eﬀects of loss weight w1 and w2 in network training. We vary\nthe loss weight w1 and w2 from 0 to 5. w1(w2) = 0 means we don’t consider\nthe corresponding loss during training. It is clear that, a positive loss weight\nis beneﬁcial for the ReID performance on both datasets. As we increase the\nloss weights, the ReID performance starts to increase. The best performance is\nachieved with w1 = 1 and w2 = 0.2 on two datasets. Further increasing the loss\nweights substantially drops the the ReID performance. This is because increasing\nw1 and w2 decreases the weight of Lsrc, which is still important. Based on this\nobservation, we set w1 = 1 and w2 = 0.2 in following experiments.\n4.4\nComparison with State-of-the-art Methods\nThis section compares our method against state-of-the-art unsupervised, un-\nsupervised domain adaptive, and semi-supervised methods on three datasets.\nJVTC for Unsupervised Domain Adaptive Person ReID\n13\nComparisons on Market1501 and DukeMTMC-reID are summarized in Table 2.\nComparisons on MSMT17 are summarized in Table 3. In those tables, “Source”\nrefers to the labeled source dataset, which is used for training in unsupervised\ndomain adaptive ReID. “None” denotes unsupervised ReID.\nComparison on Market1501 and DukeMTMC-reID: We ﬁrst compare\nour method with unsupervised learning methods. Compared methods include\nhand-crafted features LOMO [20] and BOW [36], and deep learning methods\nDBC [3] and BUC [21]. It can be observed from Table 2 that, hand-crafted\nfeatures LOMO and BOW show unsatisfactory performance, even worse than\ndirectly transfer. Using unlabeled training dataset for training, deep learning\nbased methods outperform hand-crafted features. BUC and DBC ﬁrst treat each\nimage as a single cluster, then merge clusters to seek pseudo labels for training.\nOur method outperforms them by large margins, e.g., our rank1 accuracy on\nMarket1501 achieves 72.9% vs. their 66.2% and 69.2%, respectively. The rea-\nsons could be because our method considers both visual similarity and temporal\nconsistency to predict labels. Moreover, our method further computes classiﬁ-\ncation loss in each training batch with SAC. By further considering temporal\nconsistency during testing, JVTC+ gets further performance promotions on both\ndatasets, even outperforms several unsupervised domain adaptive methods.\nWe further compare our method with unsupervised domain adaptive meth-\nods including PTGAN [31], CamStyle [41], T-Fusion [22], ARN [19], MAR [34],\nECN [39], PDA-Net [18], PAST [35], CAL-CCE [25], CR-GAN [1] and SSG [5],\nand semi-supervised methods including TAUDL [15], UTAL [16], SSG+ [5],\nand SSG++ [5]. Under the unsupervised domain adaptive training setting, our\nmethod achieves the best performance on both Market1501 and DukeMTMC-\nreID in Table 2. For example, our method achieves 83.8% rank1 accuracy on\nMarket1501 and gets 75.0% rank1 accuracy on DukeMTMC-reID. T-Fusion [22]\nalso use temporal cues for unsupervised ReID, but achieves unsatisfactory per-\nformance, e.g., 60.8% rank1 accuracy on Market1501 dataset. The reason may\nbecause that T-Fusion directly multiplies the visual and temporal probabilities,\nwhile our method fuses the visual and temporal similarities through more rea-\nsonable smooth fusion to boost the robustness. Our method also consistently\noutperforms the recent SSG [5] on those two datasets. SSG clusters multiple\nvisual features and needs to train 2100 epoches before convergence. Diﬀerently,\nour method only uses global feature and could be well-trained in 100 epoches.\nWe hence could conclude that, our method is also more eﬃcient than SSG. By\nfurther considering temporal consistency during testing, JVTC+ outperforms\nsemi-supervised method SSG++ [5] and supervised training on target domain.\nComparison on MSMT17: MSMT17 is more challenging than Market1501\nand DukeMTMC-reID because of more complex lighting and scene variations.\nSome works have reported performance on MSMT17, including unsupervised do-\nmain adaptive methods PTGAN [31], ECN [39] and SSG [5], and semi-supervised\nmethod SSG++ [5], respectively. The comparison on MSMT17 are summarized\nin Table 3. As shown in the table, our method outperforms existing methods by\nlarge margins. For example, our method achieves 45.4% rank1 accuracy when\n14\nJ. Li, and S. Zhang\nTable 3. Comparison with unsupervised and domain adaptive methods on MSMT17.\nMethod\nSource\nmAP\nr1\nr5\nr10\nr20\nSupervised\nMSMT17\n35.9 63.3 77.7 82.4 85.9\nJVTC\nNone\n15.1 39.0 50.9 56.8 61.9\nJVTC+\nNone\n17.3 43.1 53.8 59.4 64.7\nPTGAN [31]\nMarket1501\n2.9\n10.2 24.4\n-\nECN [39]\n8.5\n25.3 36.3 42.1\n-\nSSG [5]\n13.2 31.6 49.6\n-\n-\nSSG++ [5]\n16.6 37.6 57.2\n-\n-\nJVTC\n19.0 42.1 53.4 58.9 64.3\nJVTC+\n25.1 48.6 65.3 68.2 75.2\nPTGAN [31]\nDukeMTMC\n3.3\n11.8 27.4\n-\n-\nECN [39]\n10.2 30.2 41.5 46.8\n-\nSSG [5]\n13.3 32.2 51.2\n-\n-\nSSG++ [5]\n18.3 41.6 62.2\n-\n-\nJVTC\n20.3 45.4 58.4 64.3 69.7\nJVTC+\n27.5 52.9 70.5 75.9 81.2\nusing DukeMTMC-reID as the source dataset, which outperforms the unsuper-\nvised domain adaptive method SSG [5] and semi-supervised method SSG++ [5]\nby 13.2% and 3.8%, respectively. We further achieves 52.9% rank1 accuracy after\napplying the joint similarity during ReID. This outperforms the semi-supervised\nmethod SSG++ [5] by 11.3%. The above experiments on three datasets demon-\nstrate the promising performance of our JVTC.\n5\nConclusion\nThis paper tackles unsupervised domain adaptive person ReID through jointly\nenforcing visual and temporal consistency in the combination of local one-hot\nclassiﬁcation and global multi-class classiﬁcation. Those two classiﬁcation tasks\nare implemented by SAC and MTC, respectively. SAC assigns images in the\ntraining batch with distinct person ID labels, then adopts a self-adaptive classier\nto classify them. MTC predicts multi-class labels by considering both visual sim-\nilarity and temporal consistency to ensure the quality of label prediction. The\ntwo classiﬁcation models are combined in a uniﬁed framework for discrimina-\ntive feature learning on target domain. Experimental results on three datasets\ndemonstrate the superiority of the proposed method over state-of-the-art unsu-\npervised and domain adaptive ReID methods.\nAcknowledgments This work is supported in part by Peng Cheng Laboratory, The\nNational Key Research and Development Program of China under Grant No. 2018YFE0118400,\nin part by Beijing Natural Science Foundation under Grant No. JQ18012, in part\nJVTC for Unsupervised Domain Adaptive Person ReID\n15\nby Natural Science Foundation of China under Grant No. 61936011, 61620106009,\n61425025, 61572050, 91538111.\n16\nJ. Li, and S. Zhang\nReferences\n1. Chen, Y., Zhu, X., Gong, S.: Instance-guided context rendering for cross-domain\nperson re-identiﬁcation. In: ICCV (2019)\n2. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\nhierarchical image database. In: CVPR (2009)\n3. Ding12, G., Khan, S., Yin12, Q., Tang12, Z.: Dispersion based clustering for un-\nsupervised person re-identiﬁcation. In: BMVC (2019)\n4. Ester, M., Kriegel, H.P., Sander, J., Xu, X.: Density-based spatial clustering of\napplications with noise. In: KDD (1996)\n5. Fu, Y., Wei, Y., Wang, G., Zhou, Y., Shi, H., Huang, T.S.: Self-similarity group-\ning: A simple unsupervised cross domain adaptation approach for person re-\nidentiﬁcation. In: ICCV (2019)\n6. Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.\narXiv preprint arXiv:1409.7495 (2014)\n7. Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., Li, W.: Deep reconstruction-\nclassiﬁcation networks for unsupervised domain adaptation. In: ECCV (2016)\n8. Gray, D., Tao, H.: Viewpoint invariant pedestrian recognition with an ensemble of\nlocalized features. In: ECCV (2008)\n9. Gretton, A., Borgwardt, K., Rasch, M., Sch¨olkopf, B., Smola, A.J.: A kernel\nmethod for the two-sample-problem. In: NeurIPS (2007)\n10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016)\n11. Li, J., Wang, J., Tian, Q., Gao, W., Zhang, S.: Global-local temporal representa-\ntions for video person re-identiﬁcation. In: ICCV (2019)\n12. Li, J., Zhang, S., Huang, T.: Multi-scale 3d convolution network for video based\nperson re-identiﬁcation. In: AAAI (2019)\n13. Li, J., Zhang, S., Huang, T.: Multi-scale temporal cues learning for video person\nre-identiﬁcation. IEEE Trans. on Image Processing 29, 4461–4473 (2020)\n14. Li, J., Zhang, S., Tian, Q., Wang, M., Gao, W.: Pose-guided representation learn-\ning for person re-identiﬁcation. IEEE Trans. on Pattern Analysis and Machine\nIntelligence (2019)\n15. Li, M., Zhu, X., Gong, S.: Unsupervised person re-identiﬁcation by deep learning\ntracklet association. In: ECCV (2018)\n16. Li, M., Zhu, X., Gong, S.: Unsupervised tracklet person re-identiﬁcation. IEEE\nTrans. on Pattern Analysis and Machine Intelligence (2019)\n17. Li, W., Zhu, X., Gong, S.: Harmonious attention network for person re-\nidentiﬁcation. In: CVPR (2018)\n18. Li,\nY.J.,\nLin,\nC.S.,\nLin,\nY.B.,\nWang,\nY.C.F.:\nCross-dataset\nperson\nre-\nidentiﬁcation via unsupervised pose disentanglement and adaptation. arXiv\npreprint arXiv:1909.09675 (2019)\n19. Li, Y.J., Yang, F.E., Liu, Y.C., Yeh, Y.Y., Du, X., Frank Wang, Y.C.: Adaptation\nand re-identiﬁcation network: An unsupervised deep transfer learning approach to\nperson re-identiﬁcation. In: CVPR Workshops (2018)\n20. Liao, S., Hu, Y., Zhu, X., Li, S.Z.: Person re-identiﬁcation by local maximal oc-\ncurrence representation and metric learning. In: CVPR (2015)\n21. Lin, Y., Dong, X., Zheng, L., Yan, Y., Yang, Y.: A bottom-up clustering approach\nto unsupervised person re-identiﬁcation. In: AAAI (2019)\n22. Lv, J., Chen, W., Li, Q., Yang, C.: Unsupervised cross-dataset person re-\nidentiﬁcation by transfer learning of spatial-temporal patterns. In: CVPR (2018)\nJVTC for Unsupervised Domain Adaptive Person ReID\n17\n23. Mao, S., Zhang, S., Yang, M.: Resolution-invariant person re-identiﬁcation. In:\nIJCAI (2019)\n24. Pan, Y., Yao, T., Li, Y., Wang, Y., Ngo, C.W., Mei, T.: Transferrable prototypical\nnetworks for unsupervised domain adaptation. In: CVPR (2019)\n25. Qi, L., Wang, L., Huo, J., Zhou, L., Shi, Y., Gao, Y.: A novel unsupervised camera-\naware domain adaptation framework for person re-identiﬁcation. arXiv preprint\narXiv:1904.03425 (2019)\n26. Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C.: Performance measures\nand a data set for multi-target, multi-camera tracking. In: ECCV (2016)\n27. Su, C., Li, J., Zhang, S., Xing, J., Gao, W., Tian, Q.: Pose-driven deep convolu-\ntional model for person re-identiﬁcation. In: ICCV (2017)\n28. Sun, B., Feng, J., Saenko, K.: Return of frustratingly easy domain adaptation. In:\nAAAI (2016)\n29. Wang, D., Zhang, S.: Unsupervised person re-identiﬁcation via multi-label classi-\nﬁcation. In: CVPR (2020)\n30. Wang, G., Lai, J., Huang, P., Xie, X.: Spatial-temporal person re-identiﬁcation.\nIn: AAAI (2019)\n31. Wei, L., Zhang, S., Gao, W., Tian, Q.: Person transfer gan to bridge domain gap\nfor person re-identiﬁcation. In: CVPR (2018)\n32. Wei, L., Zhang, S., Yao, H., Gao, W., Tian, Q.: Glad: Global-local-alignment de-\nscriptor for pedestrian retrieval. In: ACM MM (2017)\n33. Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via non-\nparametric instance discrimination. In: CVPR (2018)\n34. Yu, H.X., Zheng, W.S., Wu, A., Guo, X., Gong, S., Lai, J.H.: Unsupervised person\nre-identiﬁcation by soft multilabel learning. In: CVPR (2019)\n35. Zhang, X., Cao, J., Shen, C., You, M.: Self-training with progressive augmentation\nfor unsupervised cross-domain person re-identiﬁcation. In: ICCV (2019)\n36. Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person re-\nidentiﬁcation: A benchmark. In: ICCV (2015)\n37. Zheng, Z., Zheng, L., Yang, Y.: Unlabeled samples generated by gan improve the\nperson re-identiﬁcation baseline in vitro. In: ICCV (2017)\n38. Zhong, Y., Wang, X., Zhang, S.: Robust partial matching for person search in the\nwild. In: CVPR (2020)\n39. Zhong, Z., Zheng, L., Luo, Z., Li, S., Yang, Y.: Invariance matters: Exemplar\nmemory for domain adaptive person re-identiﬁcation. In: CVPR (2019)\n40. Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: Camera style adaptation for\nperson re-identiﬁcation. In: CVPR (2018)\n41. Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: Camstyle: A novel data aug-\nmentation method for person re-identiﬁcation. IEEE Trans. on Image Processing\n28(3), 1176–1190 (2018)\n42. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In: ICCV (2017)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-07-21",
  "updated": "2020-07-21"
}