{
  "id": "http://arxiv.org/abs/2007.11984v1",
  "title": "Unsupervised Deep Representation Learning for Real-Time Tracking",
  "authors": [
    "Ning Wang",
    "Wengang Zhou",
    "Yibing Song",
    "Chao Ma",
    "Wei Liu",
    "Houqiang Li"
  ],
  "abstract": "The advancement of visual tracking has continuously been brought by deep\nlearning models. Typically, supervised learning is employed to train these\nmodels with expensive labeled data. In order to reduce the workload of manual\nannotations and learn to track arbitrary objects, we propose an unsupervised\nlearning method for visual tracking. The motivation of our unsupervised\nlearning is that a robust tracker should be effective in bidirectional\ntracking. Specifically, the tracker is able to forward localize a target object\nin successive frames and backtrace to its initial position in the first frame.\nBased on such a motivation, in the training process, we measure the consistency\nbetween forward and backward trajectories to learn a robust tracker from\nscratch merely using unlabeled videos. We build our framework on a Siamese\ncorrelation filter network, and propose a multi-frame validation scheme and a\ncost-sensitive loss to facilitate unsupervised learning. Without bells and\nwhistles, the proposed unsupervised tracker achieves the baseline accuracy as\nclassic fully supervised trackers while achieving a real-time speed.\nFurthermore, our unsupervised framework exhibits a potential in leveraging more\nunlabeled or weakly labeled data to further improve the tracking accuracy.",
  "text": "International Journal of Computer Vision (IJCV) manuscript No.\n(will be inserted by the editor)\nUnsupervised Deep Representation Learning for Real-Time Tracking\nNing Wang · Wengang Zhou · Yibing Song · Chao Ma · Wei Liu · Houqiang Li\nReceived: date / Accepted: date\nAbstract The advancement of visual tracking has continu-\nously been brought by deep learning models. Typically, su-\npervised learning is employed to train these models with\nexpensive labeled data. In order to reduce the workload of\nmanual annotations and learn to track arbitrary objects, we\npropose an unsupervised learning method for visual track-\ning. The motivation of our unsupervised learning is that a\nrobust tracker should be effective in bidirectional tracking.\nSpeciﬁcally, the tracker is able to forward localize a target\nobject in successive frames and backtrace to its initial po-\nsition in the ﬁrst frame. Based on such a motivation, in the\nNing Wang\nThe CAS Key Laboratory of GIPAS, University of Science and Tech-\nnology of China, Hefei, China.\nE-mail: wn6149@mail.ustc.edu.cn\nWengang Zhou\nThe CAS Key Laboratory of GIPAS, University of Science and Tech-\nnology of China, Hefei, China.\nInstitute of Artiﬁcial Intelligence, Hefei Comprehensive National Sci-\nence Center, Hefei, China.\nE-mail: zhwg@ustc.edu.cn\nYibing Song\nTencent AI Lab, Shenzhen, China.\nE-mail: yibingsong.cv@gmail.com\nChao Ma\nThe MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao\nTong University, Shanghai, China.\nE-mail: chaoma@sjtu.edu.cn\nWei Liu\nTencent AI Lab, Shenzhen, China.\nE-mail: wl2223@columbia.edu\nHouqiang Li\nThe CAS Key Laboratory of GIPAS, University of Science and Tech-\nnology of China, Hefei, China.\nInstitute of Artiﬁcial Intelligence, Hefei Comprehensive National Sci-\nence Center, Hefei, China.\nE-mail: lihq@ustc.edu.cn\nCorresponding Authors: Wengang Zhou and Houqiang Li\ntraining process, we measure the consistency between for-\nward and backward trajectories to learn a robust tracker from\nscratch merely using unlabeled videos. We build our frame-\nwork on a Siamese correlation ﬁlter network, and propose\na multi-frame validation scheme and a cost-sensitive loss to\nfacilitate unsupervised learning. Without bells and whistles,\nthe proposed unsupervised tracker achieves the baseline ac-\ncuracy as classic fully supervised trackers while achieving a\nreal-time speed. Furthermore, our unsupervised framework\nexhibits a potential in leveraging more unlabeled or weakly\nlabeled data to further improve the tracking accuracy.\nKeywords Visual tracking · Unsupervised learning ·\nCorrelation ﬁlter · Siamese network\n1 Introduction\nVisual object tracking is a fundamental task in computer\nvision with numerous applications including video surveil-\nlance, autonomous driving, augmented reality, and human-\ncomputer interactions. It aims to localize a moving object\nannotated at the initial frame with a bounding box. Re-\ncently, deep models have improved the tracking accuracies\nby strengthening the feature representations [41,13,9] or op-\ntimizing networks end-to-end [1,32,46,58]. These models\nare ofﬂine pretrained with full supervision, which requires\na large number of annotated ground-truth labels during the\ntraining stage. Manual annotations are always expensive\nand time-consuming, whereas a huge number of unlabeled\nvideos are readily available on the Internet. On the other\nhand, visual tracking differs from other recognition tasks\n(e.g., object detection, image classiﬁcation) in the sense that\nobject labels vary according to target initializations on the\nﬁrst frame. The extensive and uncertain labeling process for\nsupervised learning raises our interest to develop an alterna-\narXiv:2007.11984v1  [cs.CV]  22 Jul 2020\n2\nNing Wang et al.\nSupervised Training:\nAnnotated sequences      \nForward tracking       \nUnsupervised Training:\nUnlabeled sequences      \nForward and Backward tracking      \nFig. 1 Visual tracking via supervised and unsupervised learnings. Su-\npervised learning requires ground-truth labels for individual frames in\nthe training videos, while our proposed unsupervised learning is free\nof any labels by measuring the trajectory consistency between forward\nand backward tracking.\ntive learning scheme by using unlabeled video sequences in\nthe wild.\nIn this paper, we propose an unsupervised learning ap-\nproach for visual tracking. Instead of using off-the-shelf\ndeep models, we train the visual tracking network from\nscratch. The intuition of unsupervised learning resides on\nthe bidirectional motion analysis in video sequences. Track-\ning an object can be executed in both the forward and back-\nward ways. Initially, given the bounding box annotation of\na target object in the ﬁrst frame, we can track the target ob-\nject forward in the subsequent frames. When tracking back-\nward, we use the predicted location in the last frame as the\ninitial target bounding box, and track it backward towards\nthe ﬁrst frame. Ideally, the estimated bounding box location\nin the ﬁrst frame is identical with the given one in the for-\nward pass. In this work, we measure the difference between\nthe forward and backward target trajectories and formulate\nit as a loss function. We use the computed loss to train our\nnetwork in a self-supervised manner1, as shown in Fig. 1.\nBy repeatedly tracking forward and backward, our model\nlearns to locate target objects in consecutive frames without\nany supervision.\nThe proposed unsupervised training aims to learn a\ngeneric feature representation instead of strictly focusing\non tracking a complete object. In the ﬁrst frame, we initial-\nize a bounding box that covers the informative local region\nwith high image entropy. The bounding box may contain ar-\nbitrary image content and may not cover an entire object.\nThen, our tracking network learns to track the bounding box\nregion in the training video sequences. Our unsupervised an-\nnotation shares similarity with the part-based [36] and edge-\nbased [34] tracking methods that track the subregions of a\ntarget object. We expect our tracker not only to concentrate\non the shape of a complete object, but also to track any part\n1 In this paper, we do not distinguish between the terms unsuper-\nvised and self-supervised, as both refer to learning without ground-\ntruth annotations.\nof it. The bounding box initialization by image entropy gets\nrid of the manual annotation on the ﬁrst frame and thus en-\nsures the whole learning process unsupervised.\nWe employ unsupervised learning under the Siamese\ncorrelation ﬁlter framework. The training steps consist of\nforward tracking and backward veriﬁcation. A limitation of\nthe forward and backward consistency measurement is that\nthe target trajectory in the forward pass may coincide with\nthat in the backward pass although the tracker loses the tar-\nget. The consistency loss function fails to penalize this situ-\nation because the predicted target region can still backtrace\nto the initial position on the ﬁrst frame regardless of losing\nthe target. In addition, challenges such as heavy occlusion or\nout-of-view in training videos will degrade the CNN feature\nrepresentation capability. To tackle these issues, we intro-\nduce a multi-frame validation scheme and a cost-sensitive\nloss to facilitate unsupervised training. If the tracker loses\nthe target, the trajectories predicted from the forward and\nbackward directions are unlikely to be consistent when more\nframes are used in the training stage. Besides, we propose a\nnew cost-sensitive loss to alleviate the impact of the noisy\nsamples during unsupervised learning. The training samples\ncontaining background texture will be excluded by the im-\nage entropy measurement. Based on the multi-frame vali-\ndation and sample selection strategies discussed above, our\nnetwork training is stabilized.\nWe evaluate our method on the challenging benchmark\ndatasets including OTB-2013 [69], OTB-2015 [70], Temple-\nColor [35], VOT2016 [25], VOT2017/2018 [24], LaSOT\n[16], and TrackingNet [45]. Extensive experimental results\nindicate that without bells and whistles, the proposed unsu-\npervised tracker is even comparable with the baseline con-\nﬁguration of fully supervised trackers [1,58,64]. When inte-\ngrated with an adaptive online model update [11,9], the pro-\nposed tracker shows state-of-the-art performance. It is worth\nmentioning that our tracker trained via unsupervised learn-\ning achieves comparable performance with that via super-\nvised learning when only limited or noisy labels are avail-\nable. In addition, we demonstrate the potential of our tracker\nto further boost the accuracy by using more unlabeled data.\nA complete analysis of various training conﬁgurations is\ngiven in Section 4.2.\nIn summary, the contributions of this work are three-\nfold:\n– We propose an unsupervised learning method on the\nSiamese correlation ﬁlter network. The unsupervised\nlearning consists of forward and backward trackings to\nmeasure the trajectory consistency for network training.\n– We propose a multi-frame validation scheme to enlarge\nthe trajectory inconsistency when the tracker loses the\ntarget. In addition, we propose a cost-sensitive loss and\nan entropy selection metric to reduce the contributions\nfrom easy samples in the training process.\nUnsupervised Deep Representation Learning for Real-Time Tracking\n3\n– The extensive experiments carried out on seven standard\nbenchmarks show the favorable performance of the pro-\nposed tracker. We provide an in-depth analysis of our\nunsupervised representation and reveal the potential of\nunsupervised learning in visual tracking.\nIn the remainder of this paper, we describe the related\nwork in Section 2, the proposed method in Section 3, and\nthe experiments in Section 4. Finally, we conclude the paper\nin Section 5.\n2 Related Work\nIn this section, we perform a literature review on deep track-\ning methods, forward-backward motion analysis, and unsu-\npervised representation learning.\n2.1 Deep Visual Tracking\nDeep models have inﬂuenced visual tracking mainly from\ntwo perspectives. The ﬁrst one is to provide a discrimina-\ntive CNN feature representation by using off-the-shelf back-\nbones (e.g., VGG [52,3]), while the second one is to formu-\nlate a complete tracking network for end-to-end training and\npredictions. The discriminative correlation ﬁlters (DCFs) [2,\n19,10,21,42,55,38] handle the visual tracking task by solv-\ning a ridge regression task using densely sampled candi-\ndates. While being integrated with discriminative CNN fea-\ntures, the remaining operations (e.g., regression solver, on-\nline update) are kept still in the DCF trackers [13,33,63,\n9]. On the other hand, the end-to-end learning network can\nbe categorized as classiﬁcation and regression based net-\nworks. The classiﬁcation networks [46,54,22,48] incremen-\ntally train a binary classiﬁer to differentiate the target and\nbackground distractors. The regression networks [53,37,40]\nuse CNN layers to regress CNN features of the search re-\ngion to a response map for accurate localization. These end-\nto-end learning networks need online update and inevitably\nincrease the computational burden.\nRecently, the Siamese network has received huge in-\nvestigations because of its efﬁciency in online prediction.\nThe SiamFC tracker [1] uses a cross-correlation layer to\nmeasure feature similarity between the template patch and\nsearch patches. The fully convolutional nature of SiamFC\nefﬁciently predicts the target response without redundancy.\nBy incorporating the region proposal network (RPN) [50],\nthe SiamRPN methods [32,80] achieve state-of-the-art per-\nformance while running at 160 FPS. Other improvements\nbased on Siamese networks include ensemble learning [18],\ndynamic memory [71], attention modulation [65], capacity\nincrements [76], and reinforcement learning [20,15]. By in-\ntegrating the correlation ﬁlter, the Siamese correlation ﬁlter\nnetwork [58,64] achieves favorable performance even with\nan extremely lightweight model. Different from the above\ndeep trackers that train a CNN model in a supervised man-\nner or directly use off-the-shelf deep models, we propose\nto learn a tracking network from scratch using unlabeled\nvideos via unsupervised learning.\n2.2 Forward-Backward Analysis\nThe forward and backward strategy has been investigated\nin motion analysis scenarios. Meister et al. [43] combined\nthe forward-backward consistency estimation and pixel con-\nstruction to learn optical ﬂows. Wang et al. [67] leveraged\nthe cycle-consistency across multiple steps temporally to\nlearn feature representations for different tasks. The differ-\nentiable tracker in [67] is deliberately designed to be weak\nfor feature representation learning. In contrast, aiming at\nrobust visual tracking, we adopt a strong tracking base-\nline (Siamese correlation ﬁlter network), which is not fully-\ndifferentiable in the trajectory loop due to the pseudo label-\ning. However, by repeating forward tracking and backward\nveriﬁcation, we incrementally promote the tracking net-\nwork by pseudo-labeling based self-training. The forward-\nbackward consistency check is also applied in image align-\nment [79,78] and depth estimation [73,77]. In the visual\ntracking community, the forward-backward consistency is\nmainly used for the output reliability or uncertainty mea-\nsurement. The tracking-learning-detection (TLD) [23] uses\nthe Kanade-Lucas-Tomasi (KLT) tracker [56] to perform\nforward-backward matching to detect tracking failures. Lee\net al. [30] proposed to select the reliable base tracker by\ncomparing the geometric similarity, cyclic weight, and ap-\npearance consistency between a pair of forward-backward\ntrajectories. However, these methods rely on empirical met-\nrics to identify the target trajectories. In addition, repeat-\nedly performing forward and backward trackings brings in\na heavy computational cost for online tracking and largely\nhurts the real-time performance. Differently, in TrackingNet\n[45], forward-backward analysis is used for evaluating the\ntracking performance and annotating a sparsely labeled\ndataset such as Youtube-BoundingBox [49] to obtain the\nper-frame object bounding box labels. In this work, we\ntarget at visual tracking but revisit the forward-backward\nscheme from a different view, i.e., training a deep visual\ntracker in under unsupervised manner.\n2.3 Unsupervised Representation Learning\nOur tracking framework relates to unsupervised represen-\ntation learning. Learning feature representations from raw\nvideos under an unsupervised manner has gained increas-\ning attention in recent years. These approaches typically\n4\nNing Wang et al.\n(b)  Unsupervised Learning Pipeline using a Siamese Network\nCNN\nSearch\nSearch\nTemplate\nTemplate\nPseudo Label\nForward  Tracking\nBackward  Tracking\nResponse\nResponse\nInitial Label\nCNN\nCNN\nCNN\nCorrelation \nFilter\nCorrelation \nFilter\n(a)  Unsupervised Learning Motivation\nTemplate Patch Search Patch\nSearch Patch Template Patch\nForward Tracking\nusing \nInitial Label\nBackward Tracking\nusing \nPseudo Label\nConsistency Loss \nComputation\n#1\n#2\n#1\n#2\n#1\n#2\n#1\n#2\n#2\n#1\nConsistency \nLoss\nFeature\nweight \nsharing\nT\nY\nT\nW\nS\nW\nS\nR\nT\nR\nFig. 2 An overview of unsupervised learning in deep tracking. We show our motivation in (a) that we track forward and backward to compute the\nconsistency loss for network training. The detailed training procedure is shown in (b), where unsupervised learning is integrated into a Siamese\ncorrelation ﬁlter network. In the testing stage, we only track forward to predict the target location.\ndesign ingenious techniques to explore and utilize the free\nsupervision in images or videos. In [31], the feature repre-\nsentation is learned by shufﬂing the video frames and then\nsorting them again to achieve self-supervised training. The\nmulti-layer autoencoder on large-scale unlabeled data has\nbeen explored in [29]. Vondrick et al. [59] proposed to an-\nticipate the visual representation of frames in the future. In\n[60], Vondrick et al. colorized gray-scale videos by copying\ncolors from a reference frame to learn a CNN model. Wang\nand Gupta [66] used the KCF tracker [19] to pre-process the\nraw videos, and then selected a pair of tracked images to-\ngether with another random patch for learning CNNs using\na ranking loss. Our method differs from [66] signiﬁcantly in\ntwo aspects. First, we integrate the tracking algorithm into\nunsupervised training instead of merely utilizing an off-the-\nshelf tracker as the data pre-processing tool. Second, our un-\nsupervised framework is coupled with a tracking objective\nfunction, so the learned feature representation is effective in\ncharacterizing the generic target objects.\nIn the visual tracking community, unsupervised learn-\ning has rarely been touched. According to our knowl-\nedge, the only related but different approach is the auto-\nencoder based method [62]. However, the encoder-decoder\nis a general unsupervised framework [47], whereas our un-\nsupervised method is specially designed for the tracking\ntask. Since the visual objects or scenes in videos typically\nchange smoothly, the motion information of the objects in\na forward-backward trajectory loop provides free yet in-\nformative self-supervision signals for unsupervised learn-\ning, which is naturally suitable for the motion-related visual\ntracking task.\n3 Proposed Method\nThe motivation of our unsupervised learning is shown in\nFig. 2(a). We ﬁrst select a content-rich local region as the\ntarget object. Given this initialized bounding box label,\nwe track forward to predict its location in the subsequent\nframes. Then, we reverse the sequence and take the pre-\ndicted bounding box in the last frame as the pseudo label for\nbackward veriﬁcation. The predicted bounding box in the\nﬁrst frame via backward tracking is ideally identical to the\noriginal bounding box. We measure the difference between\nthe forward and backward trajectories using the consistency\nloss to train the network. Fig. 2(b) shows an overview of our\nunsupervised Siamese correlation ﬁlter network.\nIn the following, we ﬁrst revisit the correlation ﬁlter as\nwell as the Siamese network. In Section 3.2, we present our\nunsupervised learning prototype for an intuitive understand-\ning. In Section 3.3, we improve our prototype to facilitate\nunsupervised training. Finally, training details and online\ntracking are elaborated in Sections 3.4 and 3.5, respectively.\n3.1 Revisiting Correlation Tracking\nThe Discriminative Correlation Filters (DCFs) [2,19]\nregress the circularly shifted versions of the input features\nof a search patch to a soft target response map for target lo-\ncalization. When training a DCF, we select a template patch\nX with the corresponding ground-truth label Y, which is\nGaussian-shaped with the peak localized at the target posi-\ntion. The size of the template patch is usually larger than\nthat of the target. Fig. 2 shows an example of the template\npatch, where there are both target and background contents.\nUnsupervised Deep Representation Learning for Real-Time Tracking\n5\nThe ﬁlter W can be learned by solving the following ridge\nregression problem:\nmin\nW ∥W ∗X −Y∥2\n2 + λ∥W∥2\n2,\n(1)\nwhere λ is a regularization parameter and ∗denotes the cir-\ncular convolution. Eq. 1 can be efﬁciently calculated in the\nFourier domain [2,10,19] and the DCF can be computed by\nW = F −1\n\u0012\nF(X) ⊙F ⋆(Y)\nF ⋆(X) ⊙F(X) + λ\n\u0013\n,\n(2)\nwhere ⊙is the element-wise product, F(·) is the Discrete\nFourier Transform (DFT), F −1(·) is the inverse DFT, and\n⋆denotes the complex-conjugate operation. In each subse-\nquent frame, given a search patch Z, its corresponding re-\nsponse map R can be computed in the Fourier domain:\nR = W ∗Z = F −1 (F ⋆(W) ⊙F(Z)) .\n(3)\nThe above DCF framework starts from learning the tar-\nget template’s correlation ﬁlter (i.e., W) using the template\npatch and then convolves it with a search patch Z to gen-\nerate the response. Recently, the Siamese correlation ﬁlter\nnetwork [58,64] embeds the DCF in the Siamese framework\nand constructs two shared-weight branches to extract fea-\nture representations, as shown in Fig. 2(b). The ﬁrst one is\nthe template branch which takes a template patch X as input\nand extracts its features to further generate a target template\nﬁlter via DCF. The second one is the search branch which\ntakes a search patch Z as input for feature extraction. The\ntemplate ﬁlter is then convolved with the CNN features of\nthe search patch to generate the response map. The advan-\ntage of the Siamese DCF network is that both the feature\nextraction CNN and correlation ﬁlter are formulated into an\nend-to-end framework, so the learned features are more re-\nlated to the visual tracking scenarios.\n3.2 Unsupervised Learning Prototype\nGiven two consecutive frames P1 and P2, we crop the tem-\nplate and search patches from them, respectively. By con-\nducting forward tracking and backward veriﬁcation, the pro-\nposed framework does not require additional supervision.\nThe location difference between the initial bounding box\nand the predicted bounding box in P1 will formulate a con-\nsistency loss. We utilize this loss to train the network without\nground-truth annotations.\n3.2.1 Forward Tracking\nFollowing the previous approaches [58,64], we build a\nSiamese correlation ﬁlter network to track the initialized\nbounding box region in frame P1. After generating the tem-\nplate patch T from the ﬁrst frame P1, we compute the cor-\nresponding template ﬁlter WT as follows:\nWT = F −1\n\u0012\nF(ϕθ(T)) ⊙F ⋆(YT)\nF ⋆(ϕθ(T)) ⊙F(ϕθ(T)) + λ\n\u0013\n,\n(4)\nwhere ϕθ(·) denotes the CNN feature extraction operation\nwith trainable network parameters θ, and YT is the label of\nthe template patch T. This label is a Gaussian response cen-\ntered at the initialized bounding box center. Once we obtain\nthe learned template ﬁlter WT, the response map of a search\npatch S from frame P2 can be computed by\nRS = F −1(F ⋆(WT) ⊙F(ϕθ(S))).\n(5)\nIf the ground-truth Gaussian label of patch S is available,\nthe network ϕθ(·) can be trained by computing the L2 dis-\ntance between RS and the ground-truth label. Different from\nthe supervised framework, in the following, we present how\nto train the network without requiring labels by exploiting\nbackward trajectory veriﬁcation.\n3.2.2 Backward Tracking\nAfter generating the response map RS for frame P2, we cre-\nate a pseudo Gaussian label centered at its maximum value,\nwhich is denoted by YS. In backward tracking, we switch\nthe role between the search patch and the template patch.\nBy treating S as the template patch, we generate a template\nﬁlter WS using the pseudo label YS. The template ﬁlter\nWS can be learned using Eq. 4 by replacing T with S and\nreplacing YT with YS, as follows:\nWS = F −1\n\u0012\nF(ϕθ(S)) ⊙F ⋆(YS)\nF ⋆(ϕθ(S)) ⊙F(ϕθ(S)) + λ\n\u0013\n.\n(6)\nThen, we generate the response map RT of the template\npatch through Eq. 5 by replacing WT with WS and replac-\ning S with T, as shown in Eq. 7.\nRT = F −1(F ⋆(WS) ⊙F(ϕθ(T))).\n(7)\nNote that we only use one Siamese correlation ﬁlter network\nfor executing forward and backward trackings. The network\nparameters θ are ﬁxed during the tracking steps.\n3.2.3 Consistency Loss Computation\nAfter forward and backward tracking, we obtain the re-\nsponse map RT. Ideally, RT should be a Gaussian label\nwith the peak located at the initialized target position. In\nother words, RT should be as similar as the originally given\nlabel YT. Therefore, the representation network ϕθ(·) can\nbe trained under an unsupervised manner by minimizing the\nreconstruction error as follows:\nLun = ∥RT −YT∥2\n2.\n(8)\n6\nNing Wang et al.\nTracker\n#1\n#2\nTracker\n    Label \nPrediction\n    Model \n  Training\nForward Stage: Data labeling using the tracking model\nBackward Stage: Tracking model update using labeled data\n#1\n#2\nInitial          Predicted\nA Training Pair\nFig. 3 The intuition of pseudo-labeling based self-training. We use the\nsame network for both forward and backward predictions. The for-\nward stage generates a pseudo label for the search patch. The back-\nward stage updates the tracking network using training pairs via loss\nback-propagation. During training iterations, the response map of the\ntemplate gradually approaches the initial label via self supervision.\nOur unsupervised learning can be viewed as an incre-\nmental self-training process that iteratively predicts labels\nand updates the model to steadily improve the tracking capa-\nbility. Fig. 3 shows the intuition, where we use the same net-\nwork for both forward and backward predictions. In the for-\nward tracking, we generate a pseudo label YS for the search\npatch S. Then we treat generated YS as the label of S and\ncreate a corresponding sample. Using these labeled training\npairs (i.e., with initial or pseudo labels), we can update the\nSiamese correlation ﬁlter network in a similar way to su-\npervised learning. During loss back-propagation, we follow\nthe Siamese correlation ﬁlter methods [64,74] to update the\nnetwork:\n∂Lun\n∂ϕθ(T) = F −1\n\u0012\n∂Lun\n∂(F (ϕθ(T)))⋆+\n\u0012\n∂Lun\n∂(F (ϕθ(T)))\n\u0013⋆\u0013\n,\n∂Lun\n∂ϕθ(S) = F −1\n\u0012\n∂Lun\n∂(F (ϕθ(S)))⋆\n\u0013\n.\n(9)\nThe above unsupervised training process is based on the\nforward-backward consistency between two frames, which\nis summarized by Algorithm 1. In the next section, we ex-\ntend this prototype framework to consider multiple frames\nfor better network training.\n3.3 Enhancement for Unsupervised Learning\nThe proposed unsupervised learning method constructs the\nobjective function based on the consistency between RT\nand YT. In practice, the tracker may deviate from the tar-\nget in the forward tracking but still return to the original po-\nsition during the backward process. However, the proposed\nAlgorithm 1: Unsupervised training prototype\nInput: Unlabeled videos.\nOutput: Pretrained tracking network ϕθ(·).\n1 Crop the patches (i.e., T and S) from the raw videos;\n2 Initialize the CNN model ϕθ(·) with random weights θ;\n3 for each training epoch do\n4\nfor each training pair do\n5\nObtain ϕθ(T) and ϕθ(S);\n6\n// Forward Trajectory\n7\nConstruct WT using ϕθ(T) and YT (Eq. 4);\n8\nCompute RS using WT (Eq. 5) and obtain the\npseudo label of S;\n9\n// Backward Trajectory\n10\nConstruct WS using ϕθ(S) and YS (Eq. 6);\n11\nCompute the response map RT of T (Eq. 7);\n12\n// Calculate Consistency Loss\n13\nCompute the consistency loss of YT and RT (Eq. 8);\n14\nend\n15\nUpdate network ϕθ(·) using the computed loss;\n16 end\nloss function does not penalize this deviation because of the\nconsistent trajectories. Meanwhile, the raw videos may con-\ntain textureless or occluded training samples that deteriorate\nthe unsupervised learning process. In this section, we pro-\npose a multi-frame validation scheme and a cost-sensitive\nloss to tackle these two limitations.\n3.3.1 Multi-frame Validation\nWe propose a multi-frame validation approach to enlarge the\ntrajectory inconsistency when the tracker loses the target.\nOur intuition is to incorporate more frames during training\nto reduce the limitation that the erroneous localization in the\nsubsequent frame successfully backtraces to the initial posi-\ntion in the ﬁrst frame. In this way, the reconstruction error\nin Eq. 8 will effectively capture the inconsistent trajectory.\nAs shown in Fig. 3, adding more frames in the forward stage\nfurther challenges the model tracking capability.\nOur unsupervised learning prototype can be easily ex-\ntended to multiple frames. To build a trajectory cycle us-\ning three frames, we can involve another frame P3 which is\nthe subsequent frame after P2. We crop a search patch S1\nfrom P2 and another search patch S2 from P3. If the gener-\nated response map RS1 is different from its corresponding\nground-truth response, the difference tends to become larger\nin the next frame P3. As a result, the inconsistency is more\nlikely to appear in backward tracking, and the generated re-\nsponse map RT is more likely to differ from YT, as shown\nin Fig. 4. By involving more search patches during forward\nand backward trackings, the proposed consistency loss will\nbe more effective to penalize the inaccurate localizations.\nWe can further extend the number of frames utilized for\nmulti-frame validation. The length of trajectory will increase\nas shown in Fig. 5. The limitation of consistent trajectory\nUnsupervised Deep Representation Learning for Real-Time Tracking\n7\n#1\n#2\nSearch Patch\nTemplate Patch\n Search\nPatch #1\n#1\n#2\n#3\n Search\nPatch #2\nTemplate Patch \nCoincidental Success\nError Accumulation\nFig. 4 Single frame validation and multi-frame validation. The inac-\ncurate localization in single frame validation may not be captured as\nshown on the left. By involving more frames as shown on the right, we\naccumulate the localization errors to break the prediction consistency\nduring forward and backward trackings.\nwhen losing the target is more unlikely to affect the training\nprocess. Let R(Sk→T) denote the response map of the tem-\nplate T, which is generated (or tracked) by the DCF trained\nusing the k-th search patch Sk. The corresponding consis-\ntency loss function can be computed as follows:\nLk =\n\r\rR(Sk→T) −YT\n\r\r2\n2 .\n(10)\nConsidering different trajectory cycles, the multi-frame con-\nsistency loss can be computed by\nLun =\nM\nX\nk=1\nLk,\n(11)\nwhere k is the index of the search parch. Taking Fig. 5(c)\nas an example, the ﬁnal consistency objective contains three\nlosses (i.e., M = 3 in Eq. 11), which are denoted by the\nblue, green, and red cycles in Fig. 5(c), respectively.\n3.3.2 Cost-sensitive Loss\nWe initialize a bounding box region as a training sample\nin the ﬁrst frame during unsupervised training. The image\ncontent within this bounding box region may contain arbi-\ntrary or partial objects. Fig. 6 shows an overview of these re-\ngions. To alleviate the background interference, we propose\na cost-sensitive loss to effectively exclude noisy samples for\nnetwork training. For simplicity, we use three consecutive\nframes as an example to illustrate sample selection, which\ncan be naturally extended to more frames. The pipeline of\nusing three frames is shown in Fig. 5(b).\nDuring unsupervised learning, we construct multiple\ntraining triples from video sequences. For a trajectory con-\ntaining three frames, each training triple consists of one\ninitialized template patch T in frame P1 and two search\npatches S1 and S2 in the subsequent frames P2 and P3, re-\nspectively. We use several triples to form a training batch\nfor Siamese network learning. In practice, we ﬁnd that some\ntraining triples with extremely high losses prevent network\ntraining from convergence. To reduce these outlier effects in\nT\nS\n1\nS\nT\n2\nS\n①\n②\n③\nT\n1\nS\n2\nS\n3\nS\n(a)\n(b)\n(c)\nFig. 5 An overview of multi-frame trajectory consistency. We denote\nT as a template and S as a search patch, respectively. Our unsuper-\nvised training prototype is shown in (a), where only two frames are\ninvolved. Using more frames as shown in (b) and (c), we can gradually\nimprove the training performance to overcome consistent trajectories\nwhen losing the target.\npseudo-labeling based self-training, we exclude 10% of the\nwhole training triples which contain the highest loss values.\nTheir losses can be computed using Eq. 10. To this end, we\nassign a binary weight Ai\ndrop to each training triple. All these\nweights constitute a vector Adrop, where 10% of its elements\nare 0 and the others are 1.\nIn addition to the outlier training pairs, the raw videos\ninclude meaningless image patches, where there are texture-\nless backgrounds or still objects. In these patches, the ob-\njects (e.g., sky, grass, or tree) do not contain big movements.\nWe assign a motion weight vector Amotion to all the training\npairs to increase the large motion effect for network learn-\ning. Each element Ai\nmotion within this vector can be com-\nputed by\nAi\nmotion =\n\r\rRi\nS1 −Yi\nT\n\r\r2\n2 +\n\r\rRi\nS2 −Yi\nS1\n\r\r2\n2 ,\n(12)\nwhere Ri\nS1 and Ri\nS2 are the response maps in the i-th train-\ning pair, and Yi\nT and Yi\nS1 are the corresponding initial (or\npseudo) labels. Eq. 12 calculates the target motion differ-\nence from frame P1 to P2 and P2 to P3. When the value of\nAi\nmotion is large, the target object undergoes fast motion in\nthis trajectory. On the other hand, the large value of Ai\nmotion\nrepresents the hard training pair which the network should\npay more attention to. We normalize the motion weight and\nthe binary weight as follows:\nAi\nnorm =\nAi\ndrop · Ai\nmotion\nPN\ni=1 Ai\ndrop · Ai\nmotion\n,\n(13)\nwhere N is number of the training pairs in a mini-batch.\nThe sample weight Ai\nnorm serves as a scalar that reweighs\nthe training data without gradient back-propagation.\nThe ﬁnal unsupervised loss for the case of Fig. 5(b) in a\nmini-batch is computed as:\nL3-frame = 1\nN\nN\nX\ni=1\nAi\nnorm ·\n\r\r\rRi\n(S2→T) −Yi\nT\n\r\r\r\n2\n2 .\n(14)\n8\nNing Wang et al.\nWe can naturally extend Eq. 14 to the following by using\nmore frames to construct trajectories of different lengths, as\nillustrated by the toy example of Fig. 5(c). Combining with\nEq. 11, we compute the ﬁnal unsupervised loss function us-\ning M subsequent frames as:\nLﬁnal = 1\nN\nM\nX\nk=1\nN\nX\ni=1\nAi\nnorm · Li\nk,\n(15)\nwhere Li\nk =\n\r\r\rRi\n(Sk→T) −Yi\nT\n\r\r\r\n2\n2 is similar to that in Eq. 10\nbut with the index i for differentiating different samples in a\nmini-batch.\n3.4 Unsupervised Training Details\nNetwork Structure. We follow the DCFNet [64] to use a\nshallow Siamese network consisting of two convolutional\nlayers for tracking. This shallow structure is demonstrated\neffective in CFNet [58] to integrate the DCF formulation.\nThe ﬁlter sizes of these convolutional layers are 3×3×3×32\nand 3 × 3 × 32 × 32, respectively. Besides, a local response\nnormalization (LRN) layer is employed at the end of con-\nvolutional layers following [64]. This lightweight structure\nenables efﬁcient forward inferences for online tracking.\nTraining Data. We choose ILSVRC 2015 [51] as our train-\ning data, which is the same dataset employed by existing\nsupervised trackers. In the data pre-processing step, super-\nvised approaches [1,58,64] require per-frame labels. Be-\nsides, the frames will be removed, where the target object\nis occluded, partially out-of-view, or in an irregular shape\n(e.g., snake). The data pre-precessing for the supervised ap-\nproaches is time-consuming with human labor. In contrast,\nour method does not rely on manually annotated labels for\ndata pre-processing.\nIn our approach, for the ﬁrst frame in a raw video, we\ncrop overlapped small patches (5 × 5 in total) by sliding\nwindows as shown in Fig. 7. Then, we compute the image\nentropy of each image patch. Image entropy effectively mea-\nsures the content variance of an image patch. When an image\npatch only contains the unitary texture (e.g., the sky), the\nentropy of this patch approaches 0. When an image patch\ncontains textured content, the entropy will become higher.\nWe select the cropped image patch containing the highest\nimage entropy. This image patch initializes the KCF [19]\ntracker for localization in the subsequent frames. Then, we\ncrop a larger image patch with a padding of 2 times of the\ntarget size following DCFNet [64], which is further resized\nto 125×125 as the input of our network. Fig. 6 exhibits some\nexamples of the cropped patches. We randomly choose 4\ncropped patches from the continuous 10 frames in a video to\nform a training trajectory, and one of them is deﬁned as the\ntemplate and the rest as search patches. This is based on the\nFig. 6 Examples of the cropped image patches from ILSVRC 2015\n[51]. Most of these samples contain meaningful objects, while some\nsamples are less meaningful (e.g., last row).\nEntropy: 7.26\nEntropy: 7.43 Entropy: 7.42\nEntropy: 6.91 Entropy: 6.54 Entropy: 6.74\n...\n...\n...\nFig. 7 The illustration of training samples generation. The proposed\nmethod crops 5 × 5 patch candidates in the center region of the initial\nframe. Then we select the image patch with the highest image entropy.\nAs shown in the right ﬁgure, the background patches (e.g., labeled by\ngreen and red boxes) have a small image entropy.\nassumption that the center located target objects are unlikely\nto move out of the cropped region in a short span of time. We\ntrack the content in the image patch regardless of speciﬁc\nobject categories. Although this entropy-based method may\nnot accurately select a target region and the KCF tracker is\nnot robust enough to track the cropped region, this method\ncan well alleviate the meaningless background regions.\n3.5 Online Object Tracking\nAfter ofﬂine unsupervised learning, we perform online\ntracking in the way of forward tracking as illustrated in Sec-\ntion 3.2. We online update DCF to adapt to the target appear-\nance changes. The DCF update follows a moving average\noperation shown as follows:\nWt = (1 −αt)Wt−1 + αtW,\n(16)\nwhere αt ∈[0, 1] is the linear interpolation coefﬁcient. The\ntarget scale is estimated through a patch pyramid with scale\nfactors {as|a = 1.015, s = {−1, 0, 1}} [12]. We name our\nTracker as LUDT (i.e., Learning Unsupervised Deep Track-\ning). Besides, we update our model adaptively via αt and\nfollow the superior DCF formulation as that in ECO [9]. We\nname the improved tracker as LUDT+.\nUnsupervised Deep Representation Learning for Real-Time Tracking\n9\nWe keep the notation of our preliminary tracker UDT\nand UDT+ [61] in the following experiment section. Our\nprevious UDT uses a 3-frame cycle (Fig. 5(b)) and simply\ncrops the center patch in raw videos. LUDT improves UDT\nin two aspects: (1) LUDT combines different trajectory cy-\ncles as shown in Fig. 5(c), and (2) LUDT utilizes image en-\ntropy to select the informative image patches instead of the\ncenter crop. LUDT+ and UDT+ improve LUDT and UDT\nby adopting some online tracking techniques (e.g., adaptive\nupdate) proposed in [9], respectively.\n4 Experiments\nIn this section, we ﬁrst analyze the effectiveness of our unsu-\npervised training framework and discuss our network poten-\ntials. Then, we compare our tracker LUDT against state-of-\nthe-art trackers on both standard and recently released large-\nscale benchmarks including OTB-2013 [69], OTB-2015\n[70], Temple-Color [35], VOT2016 [25], VOT2017/2018\n[24], LaSOT [16], and TrackingNet [45].\n4.1 Experimental Details\nIn our experiments, we use the stochastic gradient descent\n(SGD) with a momentum of 0.9 and a weight decay of 0.005\nto train our model. Our unsupervised network is trained for\n50 epochs with a learning rate exponentially decaying from\n10−2 to 10−5 and a mini-batch size of 32. We set the tra-\njectory length as 4. All the experiments are executed on a\nPC with 4.00GHz Intel Core I7-4790K and NVIDIA GTX\n1080Ti GPU. On a single GPU, our LUDT and LUDT+ ex-\nhibit about 70 FPS and 55 FPS, respectively2.\nThe proposed method is evaluated on seven benchmarks.\nOn the OTB-2013/2015, TempleColor, LaSOT, and Track-\ningNet datasets, we use one-pass evaluation (OPE) with dis-\ntance and overlap precision metrics. The distance precision\nthreshold is set as 20 pixels. The overlap success plot uses\nthresholds ranging from 0 to 1, and the area-under-curve\n(AUC) is computed to evaluate the overall performance. On\nthe VOT2016 and VOT2017/2018 datasets, we measure the\nperformance using Expected Average Overlap (EAO).\n4.2 Ablation Experiments and Discussions\n4.2.1 Improvements upon UDT\nOur preliminary tracker UDT [61] adopts a three-frame val-\nidation (i.e., Fig. 5(b)) and the center crop for sample gen-\neration. The improvement upon UDT is that we construct\n2 The source code is provided at https://github.com/\n594422814/UDT\nTable 1 Ablation study of Trajectory Enlargement (TE) and RoI Se-\nlection (RS). We denote UDT as our preliminary tracker [61]. We inte-\ngrate TE and RS into UDT during training and report the performance\nimprovement. The evaluation metrics are DP and AUC scores on the\nOTB-2015 and Temple-Color datasets.\nOTB-2015\nTemple-Color\nDP / AUC (%)\nDP / AUC (%)\nUDT [61]\n76.0 / 59.4\n65.8 / 50.7\nUDT + TE\n76.5 / 59.8\n66.7 / 51.2\nUDT + RS\n76.5 / 60.0\n66.9 / 51.3\nUDT + TE + RS\n76.9 / 60.2\n67.1 / 51.5\nTable 2 Comparison results of the DCFNet tracking framework with\ndifferent feature extractors. Random: the randomly initialized feature\nextractor without pre-training. HOG: adopting HOG [8] without deep\nfeatures. ED: the backbone network trained via encoder-decoder [62].\nThe evaluation metrics are DP and AUC scores on OTB-2015.\nRandom\nHOG\nED\nLUDT (Ours)\nDP (%)\n59.1\n69.2\n71.6\n76.9\nAUC (%)\n46.9\n52.1\n54.5\n60.2\na multi-supervision consistency loss function using more\nframes. We denote this strategy as Trajectory Enlargement\n(TE) in Table 1. Meanwhile, we select the RoI (region of\ninterest) from raw videos using the image entropy and KCF\ntracker, while only the center region is utilized in UDT. We\ndenote RoI Selection as RS in the table. Note that the perfor-\nmance of UDT has been close to that of its supervised con-\nﬁguration and exceeded several supervised trackers. More-\nover, under the same training conﬁguration, LUDT steadily\nimproves UDT by using TE and RS during training. LUDT\nachieves 60.2% and 51.5% in AUC on the OTB-2015 and\nTemple-Color benchmarks, respectively.\n4.2.2 Baseline Performance\nTo verify the effectiveness of the proposed unsupervised\nframework, we evaluate our tracker using different feature\nextractors. As shown in Table 2, without pre-training, the\nmodel still exhibits a weak tracking capability, which can\nbe attributed to the discriminating power of the correlation\nﬁlter. By adopting the empirical HOG representations, the\nperformance is still signiﬁcantly lower than ours. Further-\nmore, we leverage the auto-encoder framework [62] to train\nthe backbone network under an unsupervised manner us-\ning the same training data. From Table 2, we can observe\nthat our approach is superior to the encoder-decoder in this\ntracking scenario since our forward-backward based unsu-\npervised training is tightly related to object tracking.\n4.2.3 Training Data\nWe evaluate the tracking performance using different data\npre-processing strategies. The results are shown in Table 3.\nOur unsupervised LUDT method uses the last RoI selection\n10\nNing Wang et al.\nTable 3 Evaluation results of our network trained using different data\npre-processing strategies. Our LUDT tracker uses RoI selection via im-\nage entropy for unsupervised training. The evaluation metrics are DP\nand AUC scores on the OTB-2015 dataset.\nGroundtruth\nGroundtruth label\nCenter\nRoI Selection\nlabel\nwith deviations\ncropping\nvia entropy\nFull supervision\nWeak supervision\nUnsupervision\nUnsupervision\nDP (%)\n80.6\n78.9\n76.0\n76.5\nAUC (%)\n62.6\n61.4\n59.4\n60.0\nstrategy. During the evaluation, we keep the remaining mod-\nules ﬁxed in LUDT.\nComparison with Full Supervision. Using the same videos\n(i.e., ILSVRC 2015 [51]), we conduct the supervised train-\ning of our network. The supervised learning with ground-\ntruth annotations can be regarded as the upper bound of our\nunsupervised learning. We observe that the performance gap\nis small (i.e., 2.6% AUC) between the trackers trained using\nunsupervised learning (60.0% AUC) and fully supervised\nlearning (62.6% AUC).\nComparison with Weak Supervision. On ILSVRC 2015,\nwe add deviations to the ground-truth boxes to crop the\ntraining samples. The deviations range from -20 pixels to\n20 pixels randomly. The reason for setting sample devia-\ntions from the ground-truth bounding boxes is that we aim\nto simulate the inaccurate object localizations on in-the-wild\nvideos using existing object detection or optical ﬂow ap-\nproaches. We assume that these deviated samples are pre-\ndicted by existing methods and then utilized to train our un-\nsupervised network. In Table 3, we observe that our tracker\nlearned by these weakly labeled samples is comparable with\nthe supervised results (61.4% vs. 62.6% AUC). Note that\n20 pixels deviations can be achieved with many object lo-\ncalization methods. The comparable performance indicates\nthat our method can be applied to raw videos with weakly\nor sparsely labeled annotations (e.g., the dataset Youtube-\nBB [49]). On the other hand, existing object detectors and\nmodels are mostly trained by supervised learning. To ensure\nour method to be fully unsupervised, we use two unsuper-\nvised data pre-processing methods: center cropping and RoI\nselection based on entropy.\nCenter Cropping. In center cropping, we crop the center re-\ngion of the video frame. Although we crop a ﬁxed region of\nthe image, the image content appears randomly in this region\nand we denote this operation as center cropping. There may\nbe meaningless content (e.g., textureless sky and ocean) in\nthis region to disturb our unsupervised learning. The tracker\nlearned by center cropping achieves an AUC score of 59.4%.\nRoI Selection. We use the entropy-based image patch se-\nlection as illustrated in Section 3.4. Compared to the center\ncropping, image-entropy based selection can suppress the\nmeaningless background samples such as sky and grass, and\nthe KCF tracker is able to capture the selected informative\nregion in the subsequent frames. The RoI selection achieves\nTable 4 Evaluation results of our unsupervised model trained using\ndifferent trajectory lengths. Note that the 4 and 5 frames validations\nconduct multiple self-supervisions as illustrated in Fig. 5. The evalua-\ntion metrics are DP and AUC scores on the OTB-2015 dataset. Com-\npared with 3 frames validation, using more frames further improves the\ntracking accuracy.\nFrame Number\n2 frames\n3 frames\n4 frames\n5 frames\nFig. 5(a)\nFig. 5(b)\nFig. 5(c)\nakin to Fig. 5(c)\nDP (%)\n73.2\n76.0\n76.8\n76.8\nAUC (%)\n57.4\n59.4\n59.8\n59.7\nTable 5 A performance study by adding additional training data.\nAdding more unlabeled data steadily improves the tracking results. The\nevaluation metrics are DP and AUC scores on the OTB-2015 dataset.\nLUDT\nFew-shot ﬁne-tune\nMore data\nMore data\nOTB-2015\nOxUvA\nLaSOT\nDP (%)\n76.9\n78.1\n77.6\n78.2\nAUC (%)\n60.2\n61.5\n61.4\n62.0\nbetter performance than center cropping with an AUC score\nof 60.0%.\n4.2.4 Trajectory Length\nAs discussed in Section 3.3.1, trajectory enlargement helps\nmeasure the consistency loss when the tracker loses RoI. In\nTable 4, we show the performance with different trajectory\nlengths on the OTB-2015 dataset. We use center cropping\nto generate training samples following UDT for compari-\nson. The prototype of our unsupervised learning is denoted\nas 2 frames validation. By incorporating the third frame,\nthe learned tracker achieves improvement (i.e., 2.8% DP\nand 2.0% AUC). The 4 frames validation proposed in this\nwork not only extends the trajectory length but also com-\nbines multiple self-supervision constraints, which further\nimproves the accuracy. However, the 5 frames validation\nseems to be less effective. It may be because the validation\nwith 4 frames already contains adequate self-supervision\nand effectively measures the consistency loss.\n4.2.5 Cost-sensitive Loss\nOn the OTB-2015 dataset, without hard sample reweigh-\ning (i.e., Amotion in Eq. 13), the performance of our LUDT\ntracker drops about 1.5% DP and 1% AUC scores. We did\nnot conduct the ablation study of the sample dropout be-\ncause we observe that the unsupervised training cannot well\nconverge without Adrop illustrated in Eq. 14.\n4.2.6 Unlabled Data Augmentation\nFew-shot Domain Adaptation. To better ﬁt a new domain\nsuch as OTB, we construct a small training set by collect-\ning the ﬁrst several frames (e.g., 5 frames in our experi-\nUnsupervised Deep Representation Learning for Real-Time Tracking\n11\nTable 6 A performance potential of our unsupervised tracker. When\nusing more data (LaSOT) for network training, the performance is fur-\nther improved. By incorporating empirical features (HOG), our unsu-\npervised tracker achieves superior results. The performance is evalu-\nated on the OTB-2015 dataset using DP and AUC metrics.\nECOhc\nLUDT+\nLUDT+\nLUDT+\nonly ILSVRC\nmore data\nmore data + HOG\nDP (%)\n85.4\n84.3\n85.5\n85.8\nAUC (%)\n64.1\n63.9\n64.9\n65.7\nSpeed (FPS)\n60\n55\n55\n42\nment) from the videos in OTB-2015 with only the ground-\ntruth bounding box in the ﬁrst frame available. Using these\nlimited samples, we ﬁne-tune our network by 100 iterations\nusing the forward-backward pipeline, which takes about 6\nminutes. As our learning method is unsupervised, we can\nutilize the frames from test sequences to adapt our tracker.\nTable 5 shows that performance is further improved by using\nthis strategy. Our ofﬂine unsupervised training learns gen-\neral feature representation, which can be transferred to an\ninterested domain (e.g., OTB videos) using few-shot domain\nadaptation. This domain adaptation is similar to that in MD-\nNet [46], while our network parameters are initially ofﬂine\nlearned in an unsupervised manner.\nAdditional Internet Videos. We also utilize more unlabeled\nvideos to train our network. These videos are from the Ox-\nUvA dataset [57], where there are 337 videos in total. The\nOxUvA dataset is a subset of Youtube-BB [49] collected\non YouTube. By adding these videos during training, our\ntracker improves the original one by 0.7% DP and 1.2%\nAUC as shown in Table 5. By leveraging another large-\nscale LaSOT dataset [16] where there are 1200 videos col-\nlected on the Internet, the tracking performance is further\nimproved. It indicates that unlabeled data advances the unsu-\npervised training. As our framework is fully unsupervised, it\nhas the potential to take advantage of the in-the-wild videos\non the Internet to boost the performance.\n4.2.7 Empirical Features Embedding\nAs shown in Table 6, we train unsupervised LUDT+ us-\ning more unlabeled video sequences (both ILSVRC and\nLaSOT), which outperforms ECOhc [9] leveraging hand-\ncrafted features including HOG [8] and ColorName [68].\nIn addition, we can combine the learned CNN features and\nempirical features to generate a more discriminative repre-\nsentation. We add the HOG feature to LUDT+ during track-\ning and evaluate its performance. Table 6 shows that this\ncombination achieves a 65.7% AUC on OTB-2015. More-\nover, embedding the HOG feature helps LUDT+ to outper-\nform most state-of-the-art real-time trackers as shown in Ta-\nble 7. Besides feature embedding and adaptive model up-\ndate, there are still many improvements from [65,17,44,39]\navailable to beneﬁt our tracker. However, adding more addi-\nUnsupervised\nSupervised\nBird1\nBlurCar1\nCoke\nFaceOcc1\nSkiing\nSoccer\nFrom left to right: Conv1 to Conv5 of VGG-19\nInput\nFig. 8 Visualization of feature representations. First column: input im-\nage patches. Second and third columns: feature representations of our\nunsupervised LUDT and fully-supervised LUDT. The rest columns:\nfeature maps from VGG-19 (from left to right: Conv1-2, Conv2-2,\nConv3-4, Conv4-4, and Conv5-4). The feature map is visualized by\naveraging all the channels. Best viewed in color and zoom in.\ntional mechanisms is out of the scope of this work. Follow-\ning SiamFC and DCFNet, we currently use LUDT/LUDT+\ntrackers which are only trained on the ILSVRC dataset for\nfair comparison in the following evaluations.\n4.3 Visualization of Unsupervised Representation\nAfter learning the unsupervised Siamese tracking network,\nwe visualize the network response to see how it differs\nfrom the same network trained using supervised learning.\nFig. 8 shows the visualization performance. The ﬁrst col-\numn shows the input frames. The network responses from\nunsupervised learning and supervised learning are shown in\nthe second and third columns, respectively. The remaining\ncolumns show the feature responses from the off-the-shelf\ndeep model VGG-19 [52]. We observe that the responses\nfrom the unsupervised learning and supervised learning are\nsimilar with minor differences. Speciﬁcally, the boundary\nresponses from supervised learning are higher than those of\nunsupervised learning. This is because of the strong super-\nvisions brought by the ground-truth labels. The network has\nlearned to differentiate the target and background distrac-\ntors according to labels, which increases the network atten-\ntion around the object boundaries. In comparison, our un-\nsupervised learning does not employ this process for atten-\ntion enhancement, while still focusing on the center region\nof the object responses. From the viewpoint of Siamese net-\nwork, both unsupervised and supervised feature representa-\ntions focus on the target appearances, which facilitate the\ntemplate matching through the correlation operation. Com-\npared with the empirical features (e.g., HOG), we will show\nin the following that our unsupervised feature representa-\n12\nNing Wang et al.\nTable 7 Evaluations with fully-supervised baseline (left) and state-of-the-art trackers (right) on the popular OTB-2015 benchmark [70]. The\nevaluation metrics are DP and AUC scores. Our unsupervised LUDT tracker performs favorably against popular baseline methods (left), while our\nLUDT+ tracker achieves comparable results with the recent state-of-the-art supervised trackers (right).\nTrackers\nSiamFC\nDCFNet\nCFNet\nLUDT\nEAST\nHP\nSA-Siam\nSiamRPN\nRASNet\nSACF\nSiam-tri\nRT-MDNet\nMemTrack\nStructSiam\nLUDT+\nDP (%)\n77.1\n-\n74.8\n76.9\n-\n79.6\n86.5\n85.1\n-\n83.9\n78.1\n88.5\n82.0\n85.1\n84.3\nAUC (%)\n58.2\n58.0\n56.8\n60.2\n62.9\n60.1\n65.7\n63.7\n64.2\n63.3\n59.2\n65.0\n62.6\n62.1\n63.9\nFPS\n86\n70\n65\n70\n25\n159\n69\n160\n83\n23\n86\n50\n50\n45\n55\ntions achieve higher accuracy compared with hand-crafted\nfeatures.\nOur unsupervised representation is compared with the\noff-the-shelf deep model VGG-19. Note that the VGG\nmodel is trained on an image classiﬁcation task with super-\nvised learning. We show the feature maps from different lay-\ners (i.e., Conv1-2, Conv2-2, Conv3-4, Conv4-4, and Conv5-\n4) of the VGG-19. From Fig. 8, we observe that our unsu-\npervised feature representations share similarities with the\nlow-level features (i.e., the ﬁrst two layers) of VGG, which\ntypically represents spatial details. It has been well studied\nin HCF [41] and C-COT [13] that merely using the ﬁrst or\nsecond layer of the VGG model for DCF tracking contains\nlimitations. However, our unsupervised representation bet-\nter suits the tracking scenario since we jointly combine the\nfeature representation learning with the DCF formulation in\nan end-to-end fashion. In the deeper layers of VGG-19 such\nas Conv4-4 and Conv5-4, the feature representation gradu-\nally loses spatial details but increases semantics, which can\nbe combined with the low-level features to further boost the\ntracking performance [41,13]. The semantic representation\ncapability is obtained by distinguishing different object cat-\negories (i.e., image classiﬁcation), while our unsupervised\nlearning process lacks such image labels. In the future, we\nwill investigate how to learn rich multiple-level representa-\ntions for visual tracking under an unsupervised manner.\n4.4 Comparison with State-of-the-art Methods\nOTB-2013 Dataset. The OTB-2013 dataset [69] contains 50\nchallenging videos. On the OTB-2013 dataset, we evaluate\nour LUDT and LUDT+ trackers with state-of-the-art real-\ntime trackers including ACT [4], ACFN [7], CFNet [58],\nSiamFC [1], SCT [6], CSR-DCF [39], DSST [10], and KCF\n[19] using precision and success plots.\nAs illustrated in Fig. 9, our unsupervised LUDT tracker\noutperforms CFNet and SiamFC in both distance preci-\nsion and AUC score. It is worth mentioning that both\nLUDT and CFNet have similar network capabilities (net-\nwork depth), leverage the same training data, and are not\nequipped with additional online improvements. Even though\nour approach is free of ground-truth supervision, it still\nachieves very competitive tracking accuracy. Our improved\nversion, LUDT+, performs favorably against recent state-\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nLocation error threshold (pixels)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision rate\nPrecision plots of OPE\nACT [87.5]\nLUDT+ [87.4]\nACFN [84.3]\nSCT [84.0]\nLUDT [81.9]\nSiamFC [79.7]\nCFNet [79.6]\nCSR-DCF [79.0]\nKCF [74.3]\nDSST [73.2]\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess rate\nSuccess plots of OPE\nLUDT+ [64.7]\nACT [64.6]\nLUDT [64.1]\nCFNet [61.3]\nSiamFC [60.9]\nACFN [60.8]\nSCT [60.3]\nCSR-DCF [58.9]\nDSST [55.9]\nKCF [52.2]\nFig. 9 Precision and success plots on the OTB-2013 dataset [69] for\nrecent real-time trackers. The legend in each tracker shows the preci-\nsion at 20 pixels of precision plot and AUC of success plot.\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nLocation error threshold (pixels)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision rate\nPrecision plots of OPE\nLUDT+ [84.3]\nACT [84.2]\nACFN [79.4]\nSiamFC [77.1]\nCSR-DCF [77.0]\nLUDT [76.9]\nSCT [76.4]\nCFNet [74.8]\nKCF [69.6]\nDSST [68.9]\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess rate\nSuccess plots of OPE\nLUDT+ [63.9]\nACT [62.5]\nLUDT [60.2]\nSiamFC [58.2]\nCSR-DCF [58.1]\nACFN [57.0]\nCFNet [56.8]\nSCT [53.7]\nDSST [51.8]\nKCF [48.5]\nFig. 10 Precision and success plots on the OTB-2015 dataset [70] for\nrecent real-time trackers. The legend in each tracker shows the preci-\nsion at 20 pixels of precision plot and AUC of success plot.\nof-the-art real-time trackers such as ACT and ACFN. Be-\nsides, our LUDT and LUDT+ trackers also exhibit a real-\ntime speed of about 70 FPS and 55 FPS, respectively.\nOTB-2015 Dataset. The OTB-2015 dataset [70] contains\n100 challenging videos. On the OTB-2015 dataset [70],\nwe evaluate LUDT and LUDT+ trackers with state-of-the-\nart real-time algorithms as that in OTB-2013. In Table 7,\nwe further compare our methods with more state-of-the-art\nreal-time trackers such as StructSiam [75], MemTrack [71],\nRT-MDNet [22], Siam-tri [14], SACF [74], RASNet [65],\nSiamRPN [32], SA-Siam [18], HP [15], and EAST [20].\nFrom Fig. 10 and Table 7, we observe that our unsu-\npervised LUDT tracker is comparable with supervised base-\nline methods (e.g., SiamFC and CFNet). On the OTB-2015\ndataset, SiamFC achieves 77.1% DP and 58.2% AUC, while\nLUDT exhibits 76.9% DP and 60.2% AUC. Compared with\nCFNet, LUDT outperforms by 2.1% DP and 3.4% AUC.\nThe DSST algorithm is a traditional DCF based tracker\nwith an accurate target scale estimation. LUDT signiﬁcantly\noutperforms it by 8.0% DP and 8.4% AUC, which illus-\ntrates that our unsupervised feature representation is more\nUnsupervised Deep Representation Learning for Real-Time Tracking\n13\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nLocation error threshold (pixels)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision rate\nPrecision plots of OPE\nLUDT+ [72.5]\nSiamFC [68.8]\nLUDT [67.1]\nCSR-DCF [64.7]\nSCT [62.7]\nCFNet [60.7]\nKCF [54.9]\nDSST [53.4]\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nSuccess rate\nSuccess plots of OPE\nLUDT+ [55.2]\nLUDT [51.5]\nSiamFC [50.3]\nCSR-DCF [47.7]\nSCT [46.6]\nCFNet [45.6]\nDSST [40.5]\nKCF [38.7]\nFig. 11 Precision and success plots on the Temple-Color dataset [35]\nfor recent real-time trackers. The legend in each tracker shows the pre-\ncision at 20 pixels of precision plot and AUC of success plot.\nrobust than empirical features (e.g., HOG). With a better\nDCF formulation and more advanced online update strate-\ngies [9], our LUDT+ tracker achieves comparable perfor-\nmance with the recent ACFN and ACT trackers. In Fig. 10\nand Table 7, we do not compare with some remarkable non-\nrealtime trackers. For example, MDNet [46] and ECO [9]\ncan yield 67.8% and 69.4% AUC on the OTB-2015, but they\nare far from achieving a real-time speed.\nTable 7 compares more recent supervised trackers.\nThese latest approaches are mainly based on the Siamese\nnetwork, which improve the baseline SiamFC method us-\ning various sophisticated techniques. Most trackers in Ta-\nble 7 are trained using ILSVRC including LUDT+. How-\never, it is worth mentioning that some algorithms (e.g., SA-\nSiam and RT-MDNet) adopt pre-trained CNN models (e.g.,\nAlexNet [28] and VGG-M [3]) for network initialization.\nSiamRPN additionally uses more labeled training videos\nfrom the Youtube-BB dataset [49]. Compared with them,\nLUDT+ does not require data labels or off-the-shelf deep\nmodels, while still achieving comparable performance and\nefﬁciency.\nTemple-Color Dataset. Temple-Color [35] is a more chal-\nlenging benchmark with 128 color videos. In this dataset,\nwe compare our trackers with some baselines and state-of-\nthe-art trackers as on the OTB-2015 benchmark. Compared\nwith the DCF trackers with empirical features (e.g., HOG\nfeature), our tracker with unsupervised deep features ex-\nhibits a signiﬁcant performance improvement as shown in\nFig. 11. Speciﬁcally, SiamFC which is learned with full su-\npervision achieves an AUC score of 50.3%, while LUDT\nexhibits a 51.3% AUC score. Compared with another repre-\nsentative supervised method CFNet, LUDT exceeds its per-\nformance by 6.4% DP and 4.7% AUC. Furthermore, our\nLUDT+ tracker performs favorably against existing state-\nof-the-art trackers.\nVOT2016 Dataset. We report the evaluation results on the\nVOT2016 benchmark [25], which contains 60 videos se-\nlected from more than 300 videos. Different from the OTB\ndataset, the VOT toolkit will reinitialize when the tracker\nfails. The expected average overlap (EAO) is the ﬁnal metric\n0\n0.2\n0.4\n0.6\n0.8\n1\nRobustness (S = 100.00)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAccuracy\nAR plot for experiment baseline (mean)\nLUDT+\nLUDT\n0\n0.2\n0.4\n0.6\n0.8\n1\nRobustness (S = 100.00)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAccuracy\nAR plot for experiment baseline (pooled)\nLUDT+\nLUDT\n1\n5\n9\n13\n17\n21\n25\n29\n33\n37\nOrder\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nAverage expected overlap\nExpected overlap scores for baseline\nLUDT+\nLUDT\nLUDTplus\nLUDT\nCCOT\nTCNN\nSSAT\nMLDF\nStaple\nDDC\nEBT\nSRBT\nDNT\nSSKCF\nSiamFCR\nDeepSRDCF\nMDNet\nSRDCF\nDPT\nSiamFCA\nDeepMKCF\nNSAMF\nColorKCF\nCCCT\nSODLT\nHCF\nGCF\nDAT\nASMS\nBST\nKCF\nSCT4\nSAMF\nSWCF\nDSST\nACT\nMIL\nDFST\nFoT\nStruck\nIVT\nSTC\nFig. 12 Top: Accuracy-Robustness (AR) ranking plots generated by\nsequence mean (left) and sequence pooling (right) on the VOT2016\ndataset [25]. Trackers closer to the upper right corner perform better.\nBottom: Expected Average Overlap (EAO) graph with trackers ranked\nfrom right to left evaluated on VOT2016.\nTable 8 Comparison with state-of-the-art and baseline trackers on the\nVOT2016 benchmark [25]. The evaluation metrics include Accuracy,\nFailures (over 60 sequences), and Expected Average Overlap (EAO).\nThe up arrows indicate that higher values are better for the correspond-\ning metric and vice versa.\nTrackers\nAccuracy (↑) Failures (↓) EAO (↑)\nFPS (↑)\nECO\n0.54\n-\n0.374\n6\nVITAL\n-\n-\n0.323\n1\nDSLT\n-\n-\n0.332\n6\nRTINet\n0.57\n-\n0.298\n9\nC-COT\n0.52\n51\n0.331\n0.3\npyMDNet\n-\n-\n0.304\n2\nHCF\n0.45\n85\n0.220\n12\nACT\n-\n-\n0.275\n30\nSA-Siam\n0.53\n-\n0.291\n50\nSiamRPN\n0.56\n-\n0.344\n160\nSACF\n-\n-\n0.275\n23\nStructSiam\n-\n-\n0.264\n45\nMemTrack\n0.53\n-\n0.273\n50\nSiamFC\n0.53\n99\n0.235\n86\nSCT4\n0.48\n117\n0.188\n40\nDSST\n0.53\n151\n0.181\n25\nKCF\n0.49\n122\n0.192\n170\nLUDT (Ours)\n0.54\n100\n0.231\n70\nLUDT+ (Ours)\n0.54\n62\n0.309\n55\n14\nNing Wang et al.\nTable 9 Comparison with state-of-the-art and baseline trackers on the\nVOT2017/2018 benchmark [25]. The evaluation metrics include Ac-\ncuracy, Failures (over 60 sequences), and Expected Average Overlap\n(EAO). The up arrows indicate that higher values are better for the cor-\nresponding metric and vice versa.\nTrackers\nAccuracy (↑) Failures (↓) EAO (↑)\nFPS (↑)\nECO\n0.48\n59\n0.280\n6\nC-COT\n0.49\n68\n0.267\n0.3\nSA-Siam\n0.50\n-\n0.236\n50\nSiamRPN\n-\n-\n0.243\n160\nSiamFC\n0.50\n125\n0.188\n86\nStaple\n0.53\n147\n0.169\n70\nTRACA\n0.42\n183\n0.137\n100\nSRDCF\n0.49\n208\n0.119\n5\nDSST\n0.40\n310\n0.079\n25\nKCF\n0.45\n165\n0.135\n170\nLUDT (Ours)\n0.46\n149\n0.154\n70\nLUDT+ (Ours)\n0.49\n88\n0.230\n55\nfor tracker ranking [27]. In Fig. 12, we show the accuracy-\nrobustness (AR) plot and EAO ranking plot on VOT2016\nwith some participant trackers. The VOT2016 champion C-\nCOT uses the pre-trained VGG-M model for feature extrac-\ntion while not achieving real-time performance. The pro-\nposed LUDT+ method performs slightly worse than C-COT\nbut runs much faster. It is worth mentioning that our real-\ntime LUDT+ tracker even performs favorably against re-\nmarkable non-realtime deep trackers such as MDNet. Our\nLUDT tracker, without bells and whistles, surpasses clas-\nsic DCF trackers such as DSST and KCF by a considerable\nmargin and is comparable with some DCF methods with an\noff-the-shelf deep model (e.g., DeepMKCF and HCF).\nIn Table 8, we include more state-of-the-art trackers in-\ncluding VITAL [54], DSLT [37], RTINet [72], ACT [4],\nSA-Siam [18], SiamRPN [32], SACF [74], StructSiam [75],\nand MemTrack [71] on the VOT2016 benchmark. Com-\npared with the baseline SiamFC, our LUDT tracker yields\nfavorable results. Compared with fully-supervised trackers,\nLUDT+ overall exhibits competitive performance as well as\nefﬁciency.\nVOT2017/2018 Dataset. The VOT2017 [26] and VOT2018\n[24] are the same benchmark with more challenging videos\ncompared with those in the VOT2016 dataset. In Table 9,\nwe present the Accuracy, Failures, and EAO of the state-\nof-the-art trackers on VOT2017/VOT2018. The proposed\nLUDT tracker is still superior to the standard DCF track-\ners using hand-crafted features such as DSST and KCF. Our\nLUDT+ yields an EAO score of 0.230, which is comparable\nwith the advanced Siamese trackers such as SA-Siam and\nSiamRPN that take advantage of additional backbone net-\nworks or training data.\nLaSOT Dataset. We further evaluate our unsupervised ap-\nproach on the large-scale LaSOT testing dataset [16] with\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSuccess rate\nSuccess plots of OPE on LaSOT Testing Set\n[0.397] MDNet\n[0.390] VITAL\n[0.336] SiamFC\n[0.335] StructSiam\n[0.333] DSiam\n[0.324] ECO\n[0.314] SINT\n[0.308] STRCF\n[0.305] LUDT+ (Ours)\n[0.304] ECO_HC\n[0.275] CFNet\n[0.262] LUDT (Ours)\n[0.259] BACF\n[0.257] TRACA\n[0.257] MEEM\n[0.250] HCFT\n[0.250] PTAV\n[0.245] SRDCF\n[0.244] CSRDCF\n[0.243] Staple\n[0.238] Staple_CA\n[0.233] SAMF\n[0.221] LCT\n[0.212] Struck\n[0.210] TLD\n[0.207] DSST\n[0.203] fDSST\n[0.194] ASLA\n[0.191] SCT4\n[0.178] KCF\nFig. 13 Success plots on the LaSOT testing set [16]. The legend in\neach tracker shows the AUC of the success plot. Best viewed in color\nand zoom in.\nTable 10 Comparison with state-of-the-art and baseline trackers on\nthe TrackingNet benchmark [45]. The evaluation metrics include Pre-\ncision, Normalized Precision, and Success (AUC score).\nTrackers\nPrecision\nNorm.Prec\nSuccess\nMDNet\n0.565\n0.705\n0.606\nCFNet\n0.533\n0.654\n0.578\nSiamFC\n0.533\n0.663\n0.571\nECO\n0.492\n0.618\n0.554\nECOhc\n0.476\n0.608\n0.541\nCSRDCF\n0.480\n0.622\n0.534\nStaple CA\n0.468\n0.605\n0.529\nStaple\n0.470\n0.603\n0.528\nBACF\n0.461\n0.580\n0.523\nSRDCF\n0.455\n0.573\n0.521\nSAMF\n0.477\n0.598\n0.504\nASLA\n0.406\n0.536\n0.478\nSAMF AT\n0.447\n0.560\n0.472\nDLSSVM\n0.418\n0.562\n0.470\nDSST\n0.460\n0.588\n0.464\nMEEM\n0.386\n0.545\n0.460\nStruck\n0.402\n0.539\n0.456\nDCF\n0.419\n0.548\n0.448\nKCF\n0.419\n0.546\n0.447\nCSK\n0.368\n0.503\n0.429\nTLD\n0.336\n0.460\n0.417\nTLD\n0.292\n0.438\n0.400\nMOSSE\n0.326\n0.442\n0.388\nLUDT (Ours)\n0.469\n0.593\n0.543\nLUDT+ (Ours)\n0.495\n0.633\n0.563\n280 videos. The videos in LaSOT are more challenging\nwith an average length of about 2500 frames. As shown\nin Fig. 13, our LUDT tracker still outperforms hand-crafted\nfeature based DCF trackers such as BACF [17], CSR-DCF\n[38], DSST [10], and SCT4 [6]. Furthermore, the proposed\nLUDT+ approach achieves an AUC score of 30.5%, which is\neven comparable with some state-of-the-art deep DCF track-\nUnsupervised Deep Representation Learning for Real-Time Tracking\n15\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nOverall\nBC\nDEF\nFM\nIPR\nIV\nLR\nMB\nOCC\nOPR\nOV\nSV\nLUDT-full         LUDT      SiamFC        CFNet        DSST\n0\n10\n20\n30\n40\n50\n60\n70\nOverall\nBC\nDEF\nFM\nIPR\nIV\nLR\nMB\nOCC\nOPR\nOV\nSV\nLUDT-full\nLUDT      SiamFC         CFNet        DSST\nDP Score (%) \nAUC Score (%) \nFig. 14 Attribute-based evaluation on the OTB-2015 dataset [70]. The 11 attributes are background clutter (BC), deformation (DEF), fast motion\n(FM), in-plane rotation (IPR), illumination variation (IV), low resolution (LR), motion blur (MB), occlusion (OCC), out-of-plane rotation (OPR),\nout-of-view (OV), and scale varition (SV), respectively.\nLUDT\nSiamFC\nCFNet\nACFN\nACT\nTRACA\nDSST\nFig. 15 Qualitative evaluation of our proposed LUDT and other trackers including SiamFC [1], CFNet [58], ACFN [7], ACT [4], TRACA [5], and\nDSST [10] on 8 challenging videos from OTB-2015. From left to right and top to down are Basketball, Board, Matrix, CarScale, Diving, BlurOwl,\nBolt, and Tiger1, respectively. Best viewed in color.\ners including ECO (32.4%) [9], STRCF (30.8%) [33], and\nTRACA (25.7%) [5] that leverage off-the-shelf deep mod-\nels as feature extractors.\nTrackingNet Dataset. The recently released large-scale\nTrackingNet dataset [45] contains more than 30K videos\nwith more than 14 million dense bounding box annotations.\nThe videos are collected on the Internet (YouTube), provid-\ning large-scale high-quality data for assessing trackers in the\nwild. We test our LUDT and LUDT+ on the testing set with\n511 videos. Following [45], we adopt three metrics includ-\ning Precision, Normalized Precision, and Success (AUC) for\nperformance evaluation. In Table 10, we exhibit the results\nof our methods and all the evaluated trackers on this bench-\nmark. On this dataset, our LUDT achieves an AUC score\nof 54.3%, which obviously outperforms other hand-crafted\nfeature based DCF trackers such as ECOhc, CSR-DCF, and\nBACF by 0.2%, 0.9%, and 2.0%, respectively. Note that the\nabove DCF trackers are improved versions with additional\nregularization terms, while ours merely utilizes a standard\nDCF formulation. Our superior performance illustrates the\nrepresentational power of our unsupervised features. Be-\nsides, it is worth mentioning that our LUDT, on this large-\nscale benchmark, is even comparable with the state-of-the-\nart ECO, which leverages both hand-crafted and off-the-\nshelf deep features. Without labeled data for model train-\ning, our improved LUDT+ achieves better performance and\nslightly outperforms ECO by 0.9% in terms of AUC.\n16\nNing Wang et al.\nAttribute Analysis. The videos on OTB-2015 [70] are an-\nnotated with 11 different attributes, namely: background\nclutter (BC), deformation (DEF), out-of-plane rotation\n(OPR), scale variation (SV), occlusion (OCC), illumination\nvariation (IV), motion blur (MB), in-plane rotation (IPR),\nout-of-view (OV), fast motion (FM), and low resolution\n(LR). On the OTB-2015 benchmark, we further analyze the\nperformances over different challenges in Fig. 14. On the\nmajority of challenging scenes, our LUDT tracker outper-\nforms the popular SiamFC [1] and CFNet [58] trackers.\nHowever, our performance advantage in the DP metric is\nless obvious than that in the AUC metric. Compared with\nthe fully-supervised LUDT tracker, the main performance\ngaps are from illumination variation (IV), occlusion (OCC),\nand fast motion (FM) attributes. Unsupervised learning can\nbe further improved on these attributes.\nQualitative Evaluation. We evaluate LUDT with super-\nvised trackers (e.g., ACT, ACFN, SiamFC, TRACA, and\nCFNet) and a baseline DCF tracker (DSST) on eight chal-\nlenging videos, as shown in Fig. 15. On Matrix and Tiger1\nvideos, the targets undergo partial occlusion and background\nclutter, while on BlurOwl, the target is extremely blurry due\nto the drastic camera shaking. In these videos, DSST based\non empirical features fails to cope with the challenging fac-\ntors while LUDT is able to handle. This illustrates the ro-\nbustness of our unsupervised feature representation, which\nachieves favorable performance compared to the empirical\nfeatures. The SiamFC and CFNet trackers tend to drift when\nthe target and distractors are similar (e.g., Bolt and Basket-\nball sequences) while LUDT is able to handle these chal-\nlenging scenes because of the discriminating capability of\nDCF and its online model update mechanism. Without on-\nline improvements, LUDT is still able to track the target\naccurately, especially on the challenging Board and Diving\nvideos. It is worth mentioning that such a robust tracker is\nlearned from raw videos under an unsupervised manner.\n4.5 Limitations\nFig. 16 shows the limitations of our unsupervised learn-\ning. First, compared with the fully supervised learning, our\ntracker trained via unsupervised learning tends to drift when\nan occlusion or a drastic appearance change occurs (e.g., the\ntargets in Skiing and Soccer sequences). The semantic rep-\nresentations brought by ground-truth annotations are miss-\ning. Second, our unsupervised learning involves both for-\nward and backward trackings. The computational load dur-\ning the training phase is a potential drawback although the\nlearning process is ofﬂine.\nLUDT\nFully-supervised LUDT\nFig. 16 Failure cases of our LUDT tracker. The top and bottom videos\nare Skiing and Soccer, respectively. Compared to its fully-supervised\nversion, our unsupervised method is not robust enough when the target\nundergoes drastic appearance change and occlusion.\n5 Concluding Remarks\nIn this paper, we present how to train a visual tracker using\nunlabeled videos in the wild, which is rarely investigated in\nvisual tracking. By designing an unsupervised Siamese cor-\nrelation ﬁlter network, we verify the feasibility and effec-\ntiveness of our forward-backward based unsupervised train-\ning pipeline. To further facilitate the unsupervised training,\nwe extend our framework to consider multiple frames and\nemploy a cost-sensitive loss. Extensive experiments exhibit\nthat the proposed unsupervised tracker, without bells and\nwhistles, performs as a solid baseline and achieves com-\nparable results with the classic fully-supervised trackers.\nEquipped with additional online improvements such as a so-\nphisticated update scheme, our LUDT+ tracker performs fa-\nvorably against the state-of-the-art tracking algorithms. Fur-\nthermore, we provide a deep analysis of our unsupervised\nrepresentation by feature visualization and extensive abla-\ntion studies. Our unsupervised framework shows a promis-\ning potential in visual tracking, such as utilizing more un-\nlabeled data or weakly labeled data to further improve the\ntracking accuracy.\nAcknowledgements This work was supportedin part to Dr. Houqiang\nLi byNSFC under contract No.61836011, andin part to Dr. Wengang\nZhou byNSFC under contract No.61822208&61632019andYouth In-\nnovation Promotion Association CAS (No. 2018497). Dr. Chao Ma\nwas supported by NSFC under contract No. 60906119 and Shanghai\nPujiang Program.\nReferences\n1. Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr,\nP.H.: Fully-convolutional siamese networks for object tracking.\nIn: Proceedings of the European Conference on Computer Vision\nWorkshops (ECCV Workshop) (2016)\n2. Bolme, D.S., Beveridge, J.R., Draper, B.A., Lui, Y.M.: Visual ob-\nject tracking using adaptive correlation ﬁlters. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) (2010)\nUnsupervised Deep Representation Learning for Real-Time Tracking\n17\n3. Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return\nof the devil in the details: Delving deep into convolutional nets.\nIn: British Machine Vision Conference (BMVC) (2014)\n4. Chen, B., Wang, D., Li, P., Wang, S., Lu, H.: Real-time’actor-\ncritic’tracking. In: Proceedings of the European Conference on\nComputer Vision (ECCV) (2018)\n5. Choi, J., Jin Chang, H., Fischer, T., Yun, S., Lee, K., Jeong, J.,\nDemiris, Y., Young Choi, J.: Context-aware deep feature compres-\nsion for high-speed visual tracking. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR)\n(2018)\n6. Choi, J., Jin Chang, H., Jeong, J., Demiris, Y., Young Choi, J.:\nVisual tracking using attention-modulated disintegration and in-\ntegration. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) (2016)\n7. Choi, J., Jin Chang, H., Yun, S., Fischer, T., Demiris, Y.,\nYoung Choi, J.: Attentional correlation ﬁlter network for adap-\ntive visual tracking. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) (2017)\n8. Dalal, N., Triggs, B.: Histograms of oriented gradients for human\ndetection. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) (2005)\n9. Danelljan, M., Bhat, G., Shahbaz Khan, F., Felsberg, M.: Eco: Ef-\nﬁcient convolution operators for tracking. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (2017)\n10. Danelljan, M., H¨ager, G., Khan, F., Felsberg, M.: Accurate scale\nestimation for robust visual tracking. In: British Machine Vision\nConference (BMVC) (2014)\n11. Danelljan, M., H¨ager, G., Khan, F.S., Felsberg, M.: Adaptive de-\ncontamination of the training set: A uniﬁed formulation for dis-\ncriminative visual tracking. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2016)\n12. Danelljan, M., Hager, G., Shahbaz Khan, F., Felsberg, M.: Learn-\ning spatially regularized correlation ﬁlters for visual tracking. In:\nProceedings of the IEEE International Conference on Computer\nVision (ICCV) (2015)\n13. Danelljan, M., Robinson, A., Khan, F.S., Felsberg, M.: Beyond\ncorrelation ﬁlters: Learning continuous convolution operators for\nvisual tracking. In: Proceedings of the European Conference on\nComputer Vision (ECCV) (2016)\n14. Dong, X., Shen, J.: Triplet loss in siamese network for object\ntracking. In: Proceedings of the European Conference on Com-\nputer Vision (ECCV) (2018)\n15. Dong, X., Shen, J., Wang, W., Liu, Y., Shao, L., Porikli, F.: Hy-\nperparameter optimization for tracking with continuous deep q-\nlearning. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) (2018)\n16. Fan, H., Lin, L., Yang, F., Chu, P., Deng, G., Yu, S., Bai, H., Xu,\nY., Liao, C., Ling, H.: Lasot: A high-quality benchmark for large-\nscale single object tracking. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2019)\n17. Galoogahi, H.K., Fagg, A., Lucey, S.: Learning background-aware\ncorrelation ﬁlters for visual tracking. In: Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV) (2017)\n18. He, A., Luo, C., Tian, X., Zeng, W.: A twofold siamese network\nfor real-time object tracking. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2018)\n19. Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: High-speed\ntracking with kernelized correlation ﬁlters. IEEE Transactions on\nPattern Analysis and Machine Intelligence (TPAMI) 37(3), 583–\n596 (2015)\n20. Huang, C., Lucey, S., Ramanan, D.: Learning policies for adaptive\ntracking with deep feature cascades. In: Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV) (2017)\n21. Huang, D., Luo, L., Chen, Z., Wen, M., Zhang, C.: Applying\ndetection proposals to visual tracking for scale and aspect ratio\nadaptability.\nInternational Journal of Computer Vision (IJCV)\n122(3), 524–541 (2017)\n22. Jung, I., Son, J., Baek, M., Han, B.: Real-time mdnet. In: Proceed-\nings of the European Conference on Computer Vision (ECCV)\n(2018)\n23. Kalal, Z., Mikolajczyk, K., Matas, J.: Tracking-learning-detection.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n(TPAMI) 34(7), 1409–1422 (2012)\n24. Kristan, M., Leonardis, A., Matas, J., Felsberg, M., Pﬂugfelder, R.,\nCehovin Zajc, L., et al.: The sixth visual object tracking vot2018\nchallenge results. In: Proceedings of the European Conference on\nComputer Vision Workshops (ECCV Workshop) (2018)\n25. Kristan, M., Matas, J., Leonardis, A., Felsberg, M., Cehovin, L.,\nFern´andez, G., Vojir, T., Hager, et al.: The visual object track-\ning vot2016 challenge results. In: Proceedings of the European\nConference on Computer Vision Workshops (ECCV Workshop)\n(2016)\n26. Kristan, M., Matas, J., Leonardis, A., Felsberg, M., Cehovin, L.,\nFern´andez, G., Vojir, T., Hager, et al.: The visual object tracking\nvot2017 challenge results. In: Proceedings of the IEEE Interna-\ntional Conference on Computer Vision Workshops (ICCV Work-\nshop) (2017)\n27. Kristan, M., Matas, J., Leonardis, A., Voj´ıˇr, T., Pﬂugfelder, R.,\nFernandez, G., Nebehay, G., Porikli, F., ˇCehovin, L.: A novel\nperformance evaluation methodology for single-target trackers.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n(TPAMI) 38(11), 2137–2155 (2016)\n28. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁca-\ntion with deep convolutional neural networks. In: Advances in\nNeural Information Processing Systems (NeurIPS) (2012)\n29. Le, Q.V., Ranzato, M., Monga, R., Devin, M., Chen, K., Cor-\nrado, G.S., Dean, J., Ng, A.Y.: Building high-level features using\nlarge scale unsupervised learning. arXiv preprint arXiv:1112.6209\n(2011)\n30. Lee, D.Y., Sim, J.Y., Kim, C.S.: Multihypothesis trajectory analy-\nsis for robust visual tracking. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2015)\n31. Lee, H.Y., Huang, J.B., Singh, M., Yang, M.H.: Unsupervised\nrepresentation learning by sorting sequences. In: Proceedings of\nthe IEEE International Conference on Computer Vision (ICCV)\n(2017)\n32. Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X.: High performance visual\ntracking with siamese region proposal network. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (2018)\n33. Li, F., Tian, C., Zuo, W., Zhang, L., Yang, M.H.: Learning spatial-\ntemporal regularized correlation ﬁlters for visual tracking. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2018)\n34. Li, F., Yao, Y., Li, P., Zhang, D., Zuo, W., Yang, M.H.: Integrat-\ning boundary and center correlation ﬁlters for visual tracking with\naspect ratio variation. In: Proceedings of the IEEE International\nConference on Computer Vision Workshops (ICCV Workshop)\n(2017)\n35. Liang, P., Blasch, E., Ling, H.: Encoding color information for\nvisual tracking: algorithms and benchmark. IEEE Transactions on\nImage Processing (TIP) 24(12), 5630–5644 (2015)\n36. Liu, S., Zhang, T., Cao, X., Xu, C.: Structural correlation ﬁlter for\nrobust visual tracking. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) (2016)\n37. Lu, X., Ma, C., Ni, B., Yang, X., Reid, I., Yang, M.H.: Deep re-\ngression tracking with shrinkage loss. In: Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV) (2018)\n38. Luke´zI˘a´z, A., Voj´ıˇr, T., ˇCehovin Zajc, L., Matas, J., Kristan, M.:\nDiscriminative correlation ﬁlter tracker with channel and spa-\ntial reliability. International Journal of Computer Vision (IJCV)\n126(7), 671–688 (2018)\n18\nNing Wang et al.\n39. Lukezic, A., Vojir, T., Cehovin Zajc, L., Matas, J., Kristan, M.:\nDiscriminative correlation ﬁlter with channel and spatial reliabil-\nity. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) (2017)\n40. Luo, W., Sun, P., Zhong, F., Liu, W., Zhang, T., Wang, Y.: End-\nto-end active object tracking and its real-world deployment via\nreinforcement learning. IEEE transactions on pattern analysis and\nmachine intelligence (TPAMI) 42(6), 1317–1332 (2019)\n41. Ma, C., Huang, J.B., Yang, X., Yang, M.H.: Hierarchical convo-\nlutional features for visual tracking. In: Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV) (2015)\n42. Ma, C., Huang, J.B., Yang, X., Yang, M.H.: Adaptive correlation\nﬁlters with long-term and short-term memory for object tracking.\nInternational Journal of Computer Vision (IJCV) 126(8), 771–796\n(2018)\n43. Meister, S., Hur, J., Roth, S.: Unﬂow: Unsupervised learning of\noptical ﬂow with a bidirectional census loss. In: AAAI Conference\non Artiﬁcial Intelligence (AAAI) (2018)\n44. Mueller, M., Smith, N., Ghanem, B.: Context-aware correlation\nﬁlter tracking. In: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) (2017)\n45. M¨uller, M., Bibi, A., Giancola, S., Al-Subaihi, S., Ghanem, B.:\nTrackingnet: A large-scale dataset and benchmark for object track-\ning in the wild. In: Proceedings of the European Conference on\nComputer Vision (ECCV) (2018)\n46. Nam, H., Han, B.: Learning multi-domain convolutional neural\nnetworks for visual tracking. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2016)\n47. Olshausen, B.A., Field, D.J.: Sparse coding with an overcomplete\nbasis set: A strategy employed by v1? Vision research 37(23),\n3311–3325 (1997)\n48. Pu, S., Song, Y., Ma, C., Zhang, H., Yang, M.H.: Deep attentive\ntracking via reciprocative learning. In: Advances in Neural Infor-\nmation Processing Systems (NeurIPS) (2018)\n49. Real, E., Shlens, J., Mazzocchi, S., Pan, X., Vanhoucke, V.:\nYoutube-boundingboxes: A large high-precision human-annotated\ndata set for object detection in video. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR)\n(2017)\n50. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-\ntime object detection with region proposal networks. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence (TPAMI)\n39(6), 1137–1149 (2016)\n51. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma,\nS., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Ima-\ngenet large scale visual recognition challenge. International Jour-\nnal of Computer Vision (IJCV) 115(3), 211–252 (2015)\n52. Simonyan, K., Zisserman, A.: Very deep convolutional networks\nfor large-scale image recognition. arXiv preprint arXiv:1409.1556\n(2014)\n53. Song, Y., Ma, C., Gong, L., Zhang, J., Lau, R., Yang, M.H.: Crest:\nConvolutional residual learning for visual tracking. In: Proceed-\nings of the IEEE International Conference on Computer Vision\n(ICCV) (2017)\n54. Song, Y., Ma, C., Wu, X., Gong, L., Bao, L., Zuo, W., Shen,\nC., Lau, R.W., Yang, M.H.: Vital: Visual tracking via adversarial\nlearning. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) (2018)\n55. Sui, Y., Zhang, Z., Wang, G., Tang, Y., Zhang, L.: Exploiting the\nanisotropy of correlation ﬁlter learning for visual tracking. Inter-\nnational Journal of Computer Vision (IJCV) pp. 1–22 (2019)\n56. Tomasi, C., Kanade, T.: Detection and tracking of point features\n(1991)\n57. Valmadre, J., Bertinetto, L., Henriques, J.F., Tao, R., Vedaldi, A.,\nSmeulders, A., Torr, P., Gavves, E.: Long-term tracking in the\nwild: A benchmark. In: Proceedings of the European Conference\non Computer Vision (ECCV) (2018)\n58. Valmadre, J., Bertinetto, L., Henriques, J.F., Vedaldi, A., Torr,\nP.H.: End-to-end representation learning for correlation ﬁlter\nbased tracking. In: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) (2017)\n59. Vondrick, C., Pirsiavash, H., Torralba, A.: Anticipating visual rep-\nresentations from unlabeled video. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR)\n(2016)\n60. Vondrick, C., Shrivastava, A., Fathi, A., Guadarrama, S., Murphy,\nK.: Tracking emerges by colorizing videos. In: Proceedings of the\nEuropean Conference on Computer Vision (ECCV) (2018)\n61. Wang, N., Song, Y., Ma, C., Zhou, W., Liu, W., Li, H.: Unsuper-\nvised deep tracking. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) (2019)\n62. Wang, N., Yeung, D.Y.: Learning a deep compact image repre-\nsentation for visual tracking. In: Advances in Neural Information\nProcessing Systems (NeurIPS) (2013)\n63. Wang, N., Zhou, W., Tian, Q., Hong, R., Wang, M., Li, H.: Multi-\ncue correlation ﬁlters for robust visual tracking. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (2018)\n64. Wang, Q., Gao, J., Xing, J., Zhang, M., Hu, W.: Dcfnet: Discrimi-\nnant correlation ﬁlters network for visual tracking. arXiv preprint\narXiv:1704.04057 (2017)\n65. Wang, Q., Teng, Z., Xing, J., Gao, J., Hu, W., Maybank, S.: Learn-\ning attentions: Residual attentional siamese network for high per-\nformance online visual tracking.\nIn: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR)\n(2018)\n66. Wang, X., Gupta, A.: Unsupervised learning of visual represen-\ntations using videos. In: Proceedings of the IEEE International\nConference on Computer Vision (ICCV) (2015)\n67. Wang, X., Jabri, A., Efros, A.A.: Learning correspondence from\nthe cycle-consistency of time. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2019)\n68. Weijer, J.V.D., Schmid, C., Verbeek, J., Larlus, D.: Learning color\nnames for real-world applications. IEEE Transactions on Image\nProcessing (TIP) 18(7), 1512–1523 (2009)\n69. Wu, Y., Lim, J., Yang, M.H.: Online object tracking: A bench-\nmark. In: Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) (2013)\n70. Wu, Y., Lim, J., Yang, M.H.: Object tracking benchmark.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n(TPAMI) 37(9), 1834–1848 (2015)\n71. Yang, T., Chan, A.B.: Learning dynamic memory networks for\nobject tracking. In: Proceedings of the European Conference on\nComputer Vision (ECCV) (2018)\n72. Yao, Y., Wu, X., Zhang, L., Shan, S., Zuo, W.: Joint representation\nand truncated inference learning for correlation ﬁlter based track-\ning. In: Proceedings of the European Conference on Computer\nVision (ECCV) (2018)\n73. Yin, Z., Shi, J.: Geonet: Unsupervised learning of dense depth, op-\ntical ﬂow and camera pose. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2018)\n74. Zhang, M., Wang, Q., Xing, J., Gao, J., Peng, P., Hu, W., Maybank,\nS.: Visual tracking via spatially aligned correlation ﬁlters network.\nIn: Proceedings of the European Conference on Computer Vision\n(ECCV) (2018)\n75. Zhang, Y., Wang, L., Qi, J., Wang, D., Feng, M., Lu, H.: Structured\nsiamese network for real-time visual tracking. In: Proceedings of\nthe European Conference on Computer Vision (ECCV) (2018)\n76. Zhipeng, Z., Houwen, P., Qiang, W.: Deeper and wider siamese\nnetworks for real-time visual tracking.\nIn: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (2019)\n77. Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised\nlearning of depth and ego-motion from video. In: Proceedings\nUnsupervised Deep Representation Learning for Real-Time Tracking\n19\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR) (2017)\n78. Zhou, T., Krahenbuhl, P., Aubry, M., Huang, Q., Efros, A.A.:\nLearning dense correspondence via 3d-guided cycle consistency.\nIn: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2016)\n79. Zhou, X., Zhu, M., Daniilidis, K.: Multi-image matching via fast\nalternating minimization. In: Proceedings of the IEEE Interna-\ntional Conference on Computer Vision (ICCV) (2015)\n80. Zhu, Z., Wang, Q., Li, B., Wu, W., Yan, J., Hu, W.: Distractor-\naware siamese networks for visual object tracking. In: Proceedings\nof the European Conference on Computer Vision (ECCV) (2018)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-07-22",
  "updated": "2020-07-22"
}