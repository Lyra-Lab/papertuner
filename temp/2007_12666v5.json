{
  "id": "http://arxiv.org/abs/2007.12666v5",
  "title": "Safe Model-Based Reinforcement Learning for Systems with Parametric Uncertainties",
  "authors": [
    "S M Nahid Mahmud",
    "Scott A Nivison",
    "Zachary I. Bell",
    "Rushikesh Kamalapurkar"
  ],
  "abstract": "Reinforcement learning has been established over the past decade as an\neffective tool to find optimal control policies for dynamical systems, with\nrecent focus on approaches that guarantee safety during the learning and/or\nexecution phases. In general, safety guarantees are critical in reinforcement\nlearning when the system is safety-critical and/or task restarts are not\npractically feasible. In optimal control theory, safety requirements are often\nexpressed in terms of state and/or control constraints. In recent years,\nreinforcement learning approaches that rely on persistent excitation have been\ncombined with a barrier transformation to learn the optimal control policies\nunder state constraints. To soften the excitation requirements, model-based\nreinforcement learning methods that rely on exact model knowledge have also\nbeen integrated with the barrier transformation framework. The objective of\nthis paper is to develop safe reinforcement learning method for deterministic\nnonlinear systems, with parametric uncertainties in the model, to learn\napproximate constrained optimal policies without relying on stringent\nexcitation conditions. To that end, a model-based reinforcement learning\ntechnique that utilizes a novel filtered concurrent learning method, along with\na barrier transformation, is developed in this paper to realize simultaneous\nlearning of unknown model parameters and approximate optimal state-constrained\ncontrol policies for safety-critical systems.",
  "text": "1\nSafe Model-Based Reinforcement Learning for Systems with\nParametric Uncertainties\nS M Nahid Mahmud1 Scott A Nivison2 Zachary I. Bell2 Rushikesh Kamalapurkar1\nAbstract\nReinforcement learning has been established over the past decade as an effective tool to ﬁnd optimal\ncontrol policies for dynamical systems, with recent focus on approaches that guarantee safety during\nthe learning and/or execution phases. In general, safety guarantees are critical in reinforcement learning\nwhen the system is safety-critical and/or task restarts are not practically feasible. In optimal control\ntheory, safety requirements are often expressed in terms of state and/or control constraints. In recent\nyears, reinforcement learning approaches that rely on persistent excitation have been combined with a\nbarrier transformation to learn the optimal control policies under state constraints. To soften the excitation\nrequirements, model-based reinforcement learning methods that rely on exact model knowledge have\nalso been integrated with the barrier transformation framework. The objective of this paper is to develop\nsafe reinforcement learning method for deterministic nonlinear systems, with parametric uncertainties\nin the model, to learn approximate constrained optimal policies without relying on stringent excitation\nconditions. To that end, a model-based reinforcement learning technique that utilizes a novel ﬁltered\nconcurrent learning method, along with a barrier transformation, is developed in this paper to realize\nsimultaneous learning of unknown model parameters and approximate optimal state-constrained control\npolicies for safety-critical systems.\nI. INTRODUCTION\nDue to advantages such as repeatability, accuracy, and lack of physical fatigue, autonomous\nsystems have been increasingly utilized to perform tasks that are dull, dirty, or dangerous.\nAutonomy in safety-critical applications such as autonomous driving and unmanned ﬂight relies\n*This research was supported, in part, by the Air Force Research Laboratories under award number FA8651-19-2-0009. Any\nopinions, ﬁndings, or recommendations in this article are those of the author(s), and do not necessarily reﬂect the views of the\nsponsoring agencies.\n1School\nof\nMechanical\nand\nAerospace\nEngineering,\nOklahoma\nState\nUniversity,\nemail:\n{nahid.mahmud,\nabudia@okstate.edu, rushikesh.kamalapurkar} @okstate.edu.\n2 Air Force Research Laboratories, Florida, USA, email: {scott.nivison, zachary.bell.10} @us.af.mil.\nOctober 6, 2021\nDRAFT\narXiv:2007.12666v5  [eess.SY]  5 Oct 2021\n2\non the ability to synthesize safe controllers. To improve robustness to parametric uncertainties\nand changing objectives and models, autonomous systems also need the ability to simultaneously\nsynthesize and execute control policies online and in real time. This paper concerns reinforcement\nlearning (RL), which has been established as an effective tool for safe policy synthesis for both\nknown and uncertain dynamical systems with ﬁnite state and action spaces (see, e.g., [1], [2]).\nRL typically requires a large number of iterations due to sample inefﬁciency (see, e.g., [1]).\nSample efﬁciency in RL can be improved using model-based reinforcement learning (MBRL);\nhowever, MBRL methods are prone to failure due to inaccurate models (see, e.g., [3]–[5]).\nOnline MBRL methods that handle modeling uncertainties are motivated by complex tasks that\nrequire systems to operate in dynamic environments with changing objectives and system models,\nwhere accurate models of the system and environment are generally not available in due to\nsparsity of data. In the past, MBRL techniques under the umbrella of approximate dynamic\nprogramming (ADP) have been successfully utilized to solve reinforcement learning problems\nonline with model uncertainty (see, e.g., [6]–[8]). ADP utilizes parametric methods such as\nneural networks (NNs) to approximate the value function, and the system model online. By\nobtaining an approximation of both the value function and the system model, a stable closed\nloop adaptive control policy can be developed (see, e.g., [9]–[13]).\nReal-world optimal control applications typically include constraints on states and/or inputs\nthat are critical for safety (see, e.g., [14]). ADP was successfully extended to address input\nconstrained control problems in [6] and [15]. The state-constrained ADP problem was studied\nin the context of obstacle avoidance in [16] and [17], where an additional term that penalizes\nproximity to obstacles was added to the cost function. Since the added proximity penalty in [16]\nwas ﬁnite, the ADP feedback could not guarantee obstacle avoidance, and an auxiliary controller\nwas needed. In [17], a barrier-like function was used to ensure unbounded growth of the proximity\npenalty near the obstacle boundary. While this approach results in avoidance guarantees, it relies\non the relatively strong assumptions that the value function is continuously differentiable over\na compact set that contains the obstacles in spite of penalty-induced discontinuities in the cost\nfunction.\nControl Barrier Function (CBF) is another approach to guarantee safety in safety-critical\nsystems (see e.g., [18]), with recent applications to the safe reinforcement learning problems\n(see e.g., [19]–[21]). [19] have addressed the issue of model uncertainty in safety-critical control\nwith an RL-based data-driven approach. A drawback of this approach is that it requires a\nOctober 6, 2021\nDRAFT\n3\nnominal controller that keeps the system stable during the learning phase, which may not be\nalways possible to design. In [21], the authors proposes a safe off-policy RL scheme which\ntrades-off between safety and performance. In [20] the authors proposes a safe RL scheme in\nwhich the proximity penalty approach from [17] is cast into the framework of CBFs. While the\ncontrol barrier function results in safety guarantees, the existence of a smooth value function, in\nspite of a nonsmooth cost function, needs to be assumed. Furthermore, to facilitate parametric\napproximation of the value function, the existence of a forward invariant compact set in the\ninterior of the safe set needs to be established. Since the invariant set needs to be in the interior\nof the safe set, the penalty becomes superﬂuous, and safety can be achieved through conventional\nLyapunov methods.\nThis paper is inspired by a safe reinforcement learning technique, recently developed in [22],\nbased on the idea of transforming a state and input constrained nonlinear optimal control problem\ninto an unconstrained one with a type of saturation function, introduced in [23], [24]. In [22],\nthe state constrained optimal control problem is transformed using a barrier transformation (BT),\ninto an equivalent, unconstrained optimal control problem. Later, a learning technique is used\nto synthesize the feedback control policy for this unconstrained optimal control problem. The\ncontroller for the original system is then derived from the unconstrained approximate optimal\npolicy by inverting the barrier transformation. In [25], the restrictive persistence of excitation\nrequirement in [22] is softened using model-based reinforcement learning (MBRL), where exact\nknowledge of the system dynamics is utilized in the barrier transformation.\nOne of the primary contributions of this paper is a detailed analysis of the connection between\nthe transformed dynamics and the original dynamics, which is missing from results such as [22],\n[25], and [26]. While the stability of the transformed dynamics under the designed controllers\nis established in results such as [22], [25], and [26], the implications of the behavior of the\ntransformed system on the original system are not examined. In this paper, it is shown that the\ntrajectories of the original system are related to the trajectories of the transformed system via\nthe barrier transformation as long as the trajectories of the transformed system remain bounded.\nWhile the transformation in [22] and [25] results in veriﬁable safe controllers, it requires exact\nknowledge of the system model, which is often difﬁcult to obtain. Another primary contribution\nof this paper is the development of a novel ﬁltered concurrent learning technique for online\nmodel learning and its integration with the barrier transformation method to yield a novel MBRL\nsolution to the online state-constrained optimal control problem under parametric uncertainty.\nOctober 6, 2021\nDRAFT\n4\nThe developed MBRL method learns an approximate optimal control policy in the presence of\nparametric uncertainties for safety critical systems while maintaining stability and safety during\nthe learning phase. The inclusion of ﬁltered concurrent learning makes the controller robust to\nmodeling errors and guarantees local stability under a ﬁnite (as opposed to persistent) excitation\ncondition.\nIn the following, the problem is formulated in Section II and the BT is described and\nanalyzed in Section III. A novel parameter estimation technique is detailed in Section IV and\na model-based reinforcement learning technique for synthesizing feedback control policy in the\ntransformed coordinates is developed in Section V. In Section VI, a Lypaunov-based analysis\nis utilized to establish practical stability of the closed-loop system resulting from the developed\nMBRL technique in the transformed coordinates, which guarantees that the safety requirements\nare satisﬁed in the original coordinates. Simulation results in Section VII demonstrate the\nperformance of the developed method and analyze its sensitivity to various design parameters,\nfollowed by a comparison of the performance of the developed MBRL approach to an ofﬂine\npseudospectral optimal control method. Strengths and limitations of the developed method are\ndiscussed in Section VIII, along with possible extensions.\nII. PROBLEM FORMULATION\nA. Control objective\nConsider a continuous-time afﬁne nonlinear dynamical system\n˙x = f(x)θ + g(x)u,\n(1)\nwhere x = [x1; . . . ; xn] ∈Rn is the system state, θ ∈Rp are the unknown parameters, u ∈Rq\nis the control input, and the functions f : Rn →Rn×p and g : Rn →Rn×q are known, locally\nLipschitz functions.In the following, [a; b] denotes the vector [a b]T and (v)i denotes the ith\ncomponent of the vector v.\nThe objective is to design a controller u for the system in (1) such that starting from a given fea-\nsible initial condition x0, the trajectories x(·) decay to the origin and satisfy xi(t) ∈(ai, Ai), ∀t ≥\n0, where i = 1, 2, . . . , n and ai < 0 < Ai. While MBRL methods such as those detailed in [5]\nguarantee stability of the closed-loop with state constraints are typically difﬁcult to establish\nwithout extensive trial and error. In the following, a BT is used to guarantee state constraints.\nOctober 6, 2021\nDRAFT\n5\nIII. BARRIER TRANSFORMATION\nA. Design\nLet the function b : R →R, referred to as the barrier function (BF), be deﬁned as\nb(ai,Ai)(y) := log Ai(ai −y)\nai(Ai −y),\n∀i = 1, 2, . . . , n.\n(2)\nDeﬁne b(a,A) : Rn →Rn as b(a,A)(x) := [b(a1,A1)((x)1); . . . ; b(an,An)((x)n)] with a = [a1; . . . ; an]\nand A = [A1; . . . ; An]. Moreover, the inverse of (2) on the interval (ai, Ai), is given by\nb−1\n(ai,Ai)(y) = aiAi\ney −1\naiey −Ai\n.\n(3)\nDeﬁne b−1\n(a,A) : Rn →Rn as b−1\n(a,A)(s) := [b−1\n(a1,A1)((s)1); . . . ; b−1\n(an,An)((s)n)]. Taking the derivative\nof (3) with respect to y yields\ndb−1\n(ai,Ai)(y)\ndy\n=\n1\nBi(y),\nwhere\nBi(y) := a2\ni ey −2aiAi + A2\ni e−y\nAia2\ni −aiA2\ni\n.\n(4)\nConsider the BF based state transformation\nsi := b(ai,Ai)(xi),\nxi = b−1\n(ai,Ai)(si),\n(5)\nwhere s := [s1, · · · , sn] denotes the transformed state. In the following derivation, whenever\nclear from the context, the subscripts ai and Ai of the BF and its inverse are suppressed for\nbrevity. The time derivative of the transformed state can be computed using the chain rule as\n˙si = Bi(si) ˙xi which yields the transformed dynamics\n˙si = Bi(si) (f(x)θ + g(x)u)i .\n(6)\nThe dynamics of the transformed state can then be expressed as\n˙s = F(s) + G(s)u,\n(7)\nwhere F(s) := y(s)θ, (y(s))i := Bi(si) (f (b−1(s)))i ∈R1×p, and (G(s))i := Bi(si) (g (b−1(s)))i ∈\nR1×q.\nContinuous differentiability of b−1 implies that F and G are locally Lipschitz continuous.\nFurthermore, f(0) = 0 along with the fact that b−1(0) = 0 implies that F(0) = 0. As a result,\nfor all compact sets Ω⊂Rn containing the origin, G is bounded on Ωand there exists a positive\nconstant Ly such that ∀s ∈Ω, ∥y(s)∥≤Ly∥s∥. The following section relates the solutions of\nthe original system to the solutions of the transformed system.\nOctober 6, 2021\nDRAFT\n6\nB. Analysis\nIn the following lemma, the trajectories of the original system and the transformed system are\nshown to be related by the barrier transformation provided the trajectories of the transformed\nsystem are complete (see, e.g., page 33 of [27]). The completeness condition is not vacuous,\nit is not difﬁcult to construct a system where the transformed trajectories escape to inﬁnity in\nﬁnite time, while the original trajectories are complete. For example, consider the system ˙x =\nx + x2u with x ∈R and u ∈R. All nonzero solutions of the corresponding transformed system\n˙s = B1(s)b−1\n(−0.5,0.5)(s) + B1(s)\n\u0010\nb−1\n(−0.5,0.5)(s)\n\u00112\nu under the feedback ζ(s, t) = −b−1\n(−0.5,0.5)(s)\nescape in ﬁnite time. However all nonzero solutions of the original system under the feedback\nξ(x, t) = ζ(b(−0.5,0.5)(x), t) = −x converge to either −1 or 1.\nLemma 1. If t 7→Φ\n\u0000t, b(x0), ζ\n\u0001\nis a complete Carath´eodory solution to (7), starting from the\ninitial condition b(x0), under the feedback policy (s, t) 7→ζ(s, t) and t 7→Λ(t, x0, ξ) is a\nCarath´eodory solution to (1), starting from the initial condition x0, under the feedback policy\n(x, t) 7→ξ(x, t), deﬁned as ξ(x, t) = ζ(b(x), t), then Λ(·, x0, ξ) is complete and Λ(t, x0, ξ) =\nb−1 (Φ(t, b(x0), ζ)) for all t ∈R≥0.\nProof. See Lemma 1 in the Appendix.\nNote that the feedback ξ is well-deﬁned at x only if b(x) is well-deﬁned, which is the case\nwhenever x is inside the barrier. As such, the main conclusion of the lemma also implies that\nΛ(·, x0, ξ) remains inside the barrier. It is thus inferred from Lemma 1 that if the trajectories of (7)\nare bounded and decay to a neighborhood of the origin under a feedback policy (s, t) 7→ζ(s, t),\nthen the feedback policy (x, t) 7→ζ\n\u0000b(x), t\n\u0001\n, when applied to the original system in (1), achieves\nthe control objective stated in section (II-A).\nTo achieve BT MBRL in the presense of parametric uncertainties, the following section\ndevelops a novel parameter estimator.\nIV. PARAMETER ESTIMATION\nThe following parameter estimator design is motivated by the subsequent Lyapunov analysis,\nand is inspired by the ﬁnite-time estimator in [28] and the ﬁltered concurrent learning (FCL)\nOctober 6, 2021\nDRAFT\n7\nmethod in [29]. Estimates of the unknown parameters, ˆθ ∈Rp, are generated using the ﬁlter\n˙Y =\n\n\n\n\n\ny(s),\n∥Yf∥≤Yf,\n0,\notherwise,\nY (0) = 0,\n(8)\n˙Yf =\n\n\n\n\n\nY TY,\n∥Yf∥≤Yf,\n0,\notherwise,\nYf(0) = 0,\n(9)\n˙Gf =\n\n\n\n\n\nG(s)u,\n∥Yf∥≤Yf,\n0,\notherwise,\n,\nGf(0) = 0,\n(10)\n˙Xf =\n\n\n\n\n\nY T(s −s0 −Gf),\n∥Yf∥≤Yf,\n0,\notherwise,\nXf(0) = 0,\n(11)\nwhere s0 = [b (x0\n1) ; . . . ; b (x0\nn)], and the update law\n˙ˆθ = β1Y T\nf (Xf −Yf ˆθ),\nˆθ(0) = θ0,\n(12)\nwhere β1 is a symmetric positive deﬁnite gain matrix and Yf is a tunable upper bound on the\nﬁltered regressor Yf.\nEquations (7) - (12) constitute a nonsmooth system of differential equations\n˙z = h(z, u) =\n\n\n\n\n\nh1(z, u),\n∥Yf∥≤Yf,\nh2(z, u),\notherwise,\n(13)\nwhere z = [s; vec(Y ); vec(Yf); Gf; Xf; ˆθ], h1(z, u) = [F(s) + G(s)u; vec(y(s)); vec(Y TY );\nG(s)u; Y T(s−so−Gf); β1Y T\nf (Xf−Yf ˆθ)], and h2(z, u) = [F(s)+G(s)u; 0; 0; 0; 0; β1Y T\nf (Xf−\nYf ˆθ)]. Since ∥Yf∥is non-decreasing in time, it can be shown that (13) admits Carath´eodory\nsolutions.\nLemma 2. If ∥Yf∥is non-decreasing in time then (13) admits Carath´eodory solutions.\nProof. see Lemma 2 in Appendix.\nNote that (9), expressed in the integral form\nYf(t) =\nZ t3\n0\nY T(τ)Y (τ)dτ,\n(14)\nOctober 6, 2021\nDRAFT\n8\nwhere t3 := inf\nt {t ≥0\n|\n∥Yf(t)∥≤Yf}, along with (11), expressed in the integral form\nXf(t) =\nZ t3\n0\nY T(τ)\n\u0000s(τ) −s0 −Gf(τ)\n\u0001\ndτ,\n(15)\nand the fact that s(τ) −s0 −Gf(τ) = Y (τ)θ, can be used to conclude that Xf(t) = Yf(t)θ, for\nall t ≥0. As a result, a measure for the parameter estimation error can be obtained using known\nquantities as Yf ˜θ = Xf −Yf ˆθ, where ˜θ := θ −ˆθ. The dynamics of the parameter estimation\nerror can then be expressed as\n˙˜θ = −β1Y T\nf Yf ˜θ.\n(16)\nThe ﬁlter design is thus motivated by the fact that if the matrix Y T\nf Yf is positive deﬁnite, uni-\nformly in t, then the Lyapunov function V1(˜θ) = 1\n2 ˜θTβ−1\n1 ˜θ can be used to establish convergence\nof the parameter estimation error to the origin. Initially, Y T\nf Yf is a matrix of zeros. To ensure\nthat there exists some ﬁnite time T such that Y T\nf (t)Yf(t) is positive deﬁnite, uniformly in t for\nall t ≥T, the following ﬁnite excitation condition is imposed.\nAssumption 1. There exists a time instance T > 0 such that Yf(T) is full rank.\nNote that the minimum eigenvalue of Yf is trivially non-decreasing for t ≥t3 since Yf(t)\nis constant ∀t ≥t3. For t4 ≤t5 ≤t3, Yf(t5) = Yf(t4) +\nR t5\nt4 Y T(τ)Y (τ)dτ. Since Yf(t4) is\npositive semideﬁnite, and so is the integral\nR t5\nt4 Y T(τ)Y (τ)dτ, we conclude that λmin(Yf(t5)) ≥\nλmin(Yf(t4)), As a result, t 7→λmin(Yf(t)) is non-decreasing. Therefore, if Assumption 1 is\nsatisﬁed at t = T, then Yf(t) is also full rank for all t ≥T. Similar to other MBRL methods\nthat rely on system identiﬁcation (see e.g., chapter 4 of [5]) the following assumption is needed\nto ensure boundedness of the state trajectories over the interval [0, T].\nAssumption 2. A fallback controller ψ : Rn ×R≥0 →Rq that keeps the trajectories of (7) inside\na known bounded set over the interval [0, T), without requiring the knowledge of θ, is available.\nIf a fallback controller that satisﬁes Assumption 2 is not available, then, under the additional\nassumption that the trajectories of (7) are exciting over the interval [0, T), such a controller can\nbe learned, online while maintaining system stability, using model-free reinforcement learning\ntechniques such as [30], [31] and [32].\nRemark 1. While the analysis of the developed technique dictates that a different stabilizing\ncontroller should be used over the time interval [0, T), typically, similar to the examples in\nOctober 6, 2021\nDRAFT\n9\nEnvironment\nTransformed \nEnvironment\nBT\nParameter \nEstimator\nActor\nCritic\nAction\nReward\nBE\nAction\nfallback controller\nYf(T) is \nfull rank\nNo\nTS\nTS\nAction\nSimulated \nTS - action - derivative\ntriplets\nAction\nAction\nYes\nTS\nFig. 1.\nThe developed BT MBRL framework. The control system consists of a model-based barrier-actor-critic-estimator\narchitecture. In addition to the transformed state-action measurements, the critic also utilizes states, actions, and the corresponding\nstate derivatives, evaluated at arbitrarily selected points in the state space, to learn the value function. In the ﬁgure, BT: Barrier\nTransformation; TS: Transformed State; BE: Bellman Error.\nSections VII-A and VII-B, the transient response of the developed controller provides sufﬁcient\nexcitation so that T is small (in the examples provided in Sections VII-A and VII-B, T is the\norder of 10−5 and 10−6, respectively), and a different stabilizing controller is not needed in\npractice.\nV. MODEL-BASED REINFORCEMENT LEARNING\nLemma 1 implies that if a feedback controller that practically stabilizes the transformed system\nin (7) is designed, then the same feedback controller, applied to the original system by inverting\nthe BT, also achieves the control objective stated in Section II-A. In the following, a controller\nthat practically stabilizes (7) is designed as an estimate of the controller that minimizes the\ninﬁnite horizon cost.\nJ(u(·)) :=\nZ ∞\n0\nr(φ(τ, s0, u(·)), u(τ))dτ,\n(17)\nover the set U of piecewise continuous functions t 7→u(t), subject to (7), where φ(τ, s0, u(·))\ndenotes the trajectory of (7), evaluated at time τ, starting from the state s0 and under the controller\nOctober 6, 2021\nDRAFT\n10\nu(·), r(s, u) := sTQs + uTRu, and Q ∈Rn×n and R ∈Rq×q are symmetric positive deﬁnite\n(PD) matrices1.\nAssuming that an optimal controller exists, let the optimal value function, denoted by V ∗:\nRn × Rq →R, be deﬁned as\nV ∗(s) :=\nmin\nu(·)∈U[t,∞)\nZ ∞\nt\nr(φ(τ, s, u[t,τ)(·)), u(·))dτ,\n(18)\nwhere uI and UI are obtained by restricting the domains of u and functions in UI to the interval\nI ⊆R, respectively. Assuming that the optimal value function is continuously differentiable, it\ncan be shown to be the unique positive deﬁnite solution of the Hamilton-Jacobi-Bellman (HJB)\nequation (see, e.g., [33])\nmin\nu∈Rq\n\u0000∇sV (s) (F(s) + G(s)u) + sTQs + uTRu\n\u0001\n= 0,\n(19)\nwhere ∇(·) :=\n∂\n∂(·). Furthermore, the optimal controller is given by the feedback policy u(t) =\nu∗(φ(t, s, u[0,t))) where u∗: Rn →Rq deﬁned as\nu∗(s) := −1\n2R−1G(s)T(∇sV ∗(s))T.\n(20)\nRemark 2. In the developed method, the cost function is selected to be quadratic in the trans-\nformed coordinates. However, a physically meaningful cost function is more likely to be available\nin the original coordinates. If such a cost function is available, it can be transformed from the\noriginal coordinates to the barrier coordinates using the inverse barrier function, to yield a cost\nfunction that is not quadratic in the state. While the analysis in this paper addresses the quadratic\ncase, it can be extended to address the non-quadratic case with minimal modiﬁcations as long\nas s 7→r(s, u) is positive deﬁnite for all u ∈Rq.\nA. Value function approximation\nSince computation of analytical solutions of the HJB equation is generally infeasible, especially\nfor systems with uncertainty, parametric approximation methods are used to approximate the\nvalue function V ∗and the optimal policy u∗. The optimal value function is expressed as\nV ∗(s) = W Tσ (s) + ϵ (s) ,\n(21)\n1For ease of exposition, a state penalty of the form sT Qs has been considered in this paper. However, the analysis extends\nin a straightforward manner to general positive deﬁnite state penalty functions s 7→Q(s). As such, a state penalty function\nx 7→P(x), given in the original coordinates, can easily be transformed into an equivalent state penalty Q(s) = P(b−1(s)).\nSince the barrier function is monotonic and b(0) = 0, if P is positive deﬁnite, then so is Q. Furthermore, for applications with\nbounded control inputs, a non-quadratic penalty function similar to Eq. 17 of [26] can be incorporated in (17).\nOctober 6, 2021\nDRAFT\n11\nwhere W ∈RL is an unknown vector of bounded weights, σ : Rn →RL is a vector of\ncontinuously differentiable nonlinear activation functions such that σ (0) = 0 and ∇sσ (0) = 0,\nL ∈N is the number of basis functions, and ϵ : Rn →R is the reconstruction error.\nThe basis functions are selected such that the approximation of the functions and their\nderivatives is uniform over the compact set χ ⊂Rn so that given a positive constant ϵ ∈R, there\nexists L ∈N and known positive constants ¯W and σ such that ∥W∥≤¯W, sups∈χ ∥ϵ (s)∥≤ϵ,\nsups∈χ ∥∇sϵ (s)∥≤ϵ, sups∈χ ∥σ (s)∥≤σ, and sups∈χ ∥∇sσ (s)∥≤σ (see, e.g., [34]). Using\n(19), a representation of the optimal controller using the same basis as the optimal value function\nis derived as\nu∗(s) = −1\n2R−1GT (s)\n\u0000∇sσT (s) W + ∇sϵT (s)\n\u0001\n.\n(22)\nSince the ideal weights, W, are unknown, an actor-critic approach is used in the following to\nestimate W. To that end, let the NN estimates ˆV : Rn × RL →R and ˆu : Rn × RL →Rq be\ndeﬁned as\nˆV\n\u0010\ns, ˆWc\n\u0011\n:= ˆW T\nc σ (s) ,\n(23)\nˆu\n\u0010\ns, ˆWa\n\u0011\n:= −1\n2R−1GT (s) ∇sσT (s) ˆWa,\n(24)\nwhere the critic weights, ˆWc ∈RL and actor weights, ˆWa ∈RL are estimates of the ideal\nweights, W.\nB. Bellman Error\nSubstituting (23) and (24) into (19) results in a residual term, ˆδ : Rn × RL × RL × Rp →R,\nreferred to as Bellman Error (BE), deﬁned as\nˆδ(s, ˆWc, ˆWa, ˆθ) := ∇s ˆV (s, ˆWc)\n\u0010\ny(s)ˆθ + G(s)ˆu(s, ˆWa)\n\u0011\n+ ˆu(s, ˆWa)TRˆu(s, ˆWa) + sTQs. (25)\nTraditionally, online RL methods require a persistence of excitation (PE) condition to be able\nlearn the approximate control policy (see, e.g., [3], [6], [7]). Guaranteeing PE a priori and\nverifying PE online are both typically impossible. However, using virtual excitation facilitated\nby model-based BE extrapolation, stability and convergence of online RL can established under\na PE-like condition that, while impossible to guarantee a priori, can be veriﬁed online (by\nmonitoring the minimum eigenvalue of a matrix in the subsequent Assumption 3 (see, e.g., [4]).\nUsing the system model, the BE can be evaluated at any arbitrary point in the state space.\nOctober 6, 2021\nDRAFT\n12\nVirtual excitation can then be implemented by selecting a set of states {sk | k = 1, · · · , N} and\nevaluating the BE at this set of states to yield\nˆδk(sk, ˆWc, ˆWa, ˆθ) := ∇sk ˆV (sk, ˆWc)\n\u0000ykˆθ+Gkˆu(sk, ˆWa)\n\u0001\n+ˆu(sk, ˆWa)TRˆu(sk, ˆWa)+sT\nk Qsk, (26)\nwhere, ∇sk :=\n∂\n∂sk , yk := y(sk) and Gk := G (sk). Deﬁning the actor and critic weight estimation\nerrors as ˜Wc := W −ˆWc and ˜Wa := W −ˆWa and substituting the estimates (21) and (22) into\n(19), and subtracting from (25) yields the analytical BE that can be expressed in terms of the\nweight estimation errors as\nˆδ = −ωT ˜Wc + 1\n4\n˜W T\na Gσ ˜Wa −W T∇sσy˜θ + ∆,\n(27)\nwhere ∆:= 1\n2W T∇sσGR∇sϵT + 1\n4Gϵ−∇sϵF, GR := GR−1GT ∈Rn×n, Gϵ := ∇sϵGR∇sϵT ∈R,\nGσ := ∇sσGR−1GT∇sσT ∈RL×L, and ω := ∇sσ\n\u0010\nyˆθ + Gˆu(s, ˆWa)\n\u0011\n∈RL. In (27) and the\nrest of the manuscript, the dependence of various functions on the state, s, is omitted for brevity\nwhenever it is clear from the context. Similarly, (26) implies that\nˆδk = −ωT\nk ˜Wc + 1\n4\n˜W T\na Gσk ˜Wa −W T∇skσkyk˜θ + ∆k,\n(28)\nwhere, Fk := F(sk), ϵk := ϵ(sk), σk := σ(sk), ∆k := 1\n2W T∇skσkGRk∇skϵT\nk + 1\n4Gϵk −∇skϵkFk,\nGϵk := ∇skϵkGRk∇skϵT\nk , ωk := ∇skσk\n\u0010\nykˆθ + Gkˆu(sk, ˆWa)\n\u0011\n∈RL, GRk := GkR−1GT\nk ∈Rn×n\nand Gσk := ∇skσkGkR−1GT\nk ∇skσT\nk ∈RL×L. Note that sups∈χ ∥∆∥≤dϵ and if sk ∈χ then\n∥∆k∥≤dϵk, for some constant d > 0. While the extrapolation states sk are assumed to be\nconstant in this analysis for ease of exposition, the analysis extends in a straightforward manner\nto time-varying extrapolation states that are conﬁned to a compact neighborhood of the origin.\nC. Update laws for Actor and Critic weights\nThe actor and the critic weights are held at their initial values over the interval [0, T) and\nstarting at t = T, using the instantaneous BE ˆδ from (25) and extrapolated BEs ˆδk from (26),\nthe weights are updated according to\n˙ˆWc = −kc1Γω\nρ\nˆδ −kc2\nN Γ\nN\nX\nk=1\nωk\nρk\nˆδk,\n(29)\n˙Γ = βΓ −kc1ΓωωT\nρ2 Γ −kc2\nN Γ\nN\nX\nk=1\nωkωT\nk\nρ2\nk\nΓ,\n(30)\n˙ˆWa = −ka1\n\u0010\nˆWa −ˆWc\n\u0011\n−ka2 ˆWa + kc1GT\nσ ˆWaωT\n4ρ\nˆWc +\nN\nX\nk=1\nkc2GT\nσk ˆWaωT\nk\n4Nρk\nˆWc,\n(31)\nOctober 6, 2021\nDRAFT\n13\nwith Γ (t0) = Γ0, where Γ : R≥t0 →RL×L is a time-varying least-squares gain matrix, ρ (t) :=\n1 + γ1ωT (t) ω (t), ρk (t) := 1 + γ1ωT\nk (t) ωk (t), β > 0 ∈R is a constant forgetting factor, and\nkc1, kc2, ka1, ka2 > 0 ∈R are constant adaptation gains. The control commands sent to the system\nare then computed using the actor weights as\nu(t) =\n\n\n\n\n\nψ(s(t), t),\n0 < t < T,\nˆu\n\u0010\ns(t), ˆWa(t)\n\u0011\n,\nt ≥T,\n(32)\nwhere the controller ψ was introduced in Assumption 1. The following veriﬁable PE-like rank\ncondition is then utilized in the stability analysis.\nAssumption 3. There exists a constant c3 > 0 such that the set of points {sk ∈Rn | k = 1, . . . , N}\nsatisﬁes\nc3IL ≤inf\nt∈R≥T\n \n1\nN\nN\nX\nk=1\nωk (t) ωT\nk (t)\nρ2\nk (t)\n!\n.\n(33)\nSince ωk is a function of the weight estimates ˆθ and ˆWa, Assumption 3 cannot be guar-\nanteed a priori. However, unlike the PE condition, Assumption 3 does not impose excitation\nrequirements on the system trajectory, the excitation requirements are imposed on a user-selected\nset of points in the state space. Furthermore, Assumption 3 can be veriﬁed online. Since\nλmin\n\u0010PN\nk=1\nωk(t)ωT\nk (t)\nρ2\nk(t)\n\u0011\nis non-decreasing in the number of samples, N, Assumption 3 can be\nmet, heuristically, by increasing the number of samples.\nVI. STABILITY ANALYSIS\nIn the following theorem, the stability of the trajectories of the transformed system, and the\nestimation errors ˜Wc, ˜Wa, and ˜θ are shown.\nTheorem 1. Provided Assumptions 1, 2, and 3 hold, the gains are selected large enough based on\n(46) - (49), and the weights ˆθ, ˆWc, Γ, and ˆWa are updated according to (12), (29), (30), and (31),\nrespectively, then the estimation errors ˜Wc, ˜Wa, and ˜θ and the trajectories of the transformed\nsystem in (7) under the controller in (32) are locally uniformly ultimately bounded.\nProof. See Theorem 1 in Appendix.\nUsing Lemma 1, it can then be concluded that the feedback control law\nu(t) =\n\n\n\n\n\nψ\n\u0000b(a,A)(x(t)), t\n\u0001\n,\n0 < t < T,\nˆu\n\u0010\nb(a,A)(x(t)), ˆWa(t)\n\u0011\n,\nt ≥T,\n(34)\nOctober 6, 2021\nDRAFT\n14\napplied to the original system in (1), achieves the control objective stated in section (II-A).\nVII. SIMULATION\nTo demonstrate the performance of the developed method for a nonlinear system with an\nunknown value function, two simulation results, one for a two-state dynamical system (35), and\none for a four-state dynamical system (37) corresponding to a two-link planar robot manipulator,\nare provided.\nA. Two state dynamical system\nThe dynamical system is given by\n˙x = f(x)θ + g(x)u\n(35)\nwhere\nf(x) =\n\nx2\n0\n0\n0\n0\nx1\nx2\nx2(cos(2x1) + 2)2\n\n,\n(36)\nθ = [θ1; θ2; θ3; θ4] and g(x) = [0; cos(2x1) + 2]. The BT version of the system can be expressed\nin the form (7) with G(s) = [0; G21] and y(s) =\n\nF11\n0\n0\n0\n0\nF22\nF23\nF24\n\n, where\nF11 = B1(s1)x2,\nF22 = B2(s2)x1, F23 = B2(s2)x2,\nF24 = B2(s2)x2(cos(2x1) + 2)2,\nG21 = B2(s2) cos(2x1) + 2.\nThe state x = [x1\nx2]T needs to satisfy the constraints x1 ∈(−7, 5) and x2 ∈(−5, 7). The\nobjective for the controller is to minimize the inﬁnite horizon cost function in (17), with Q =\ndiag(10, 10) and R = 0.1. The basis functions for value function approximation are selected as\nσ(s) = [s2\n1; s1s2; s2\n2]. The initial conditions for the system and the initial guesses for the weights\nand parameters are selected as x(0) = [−6.5; 6.5], ˆθ(0) = [0; 0; 0; 0], Γ(0) = diag(1, 1, 1), and\nˆWa(0) = ˆWc(0) = [1/2; 1/2; 1/2]. The ideal values of the unknown parameters in the system model\nare θ1 = 1, θ2 = −1, θ3 = −0.5, θ4 = 0.5, and the ideal values of the actor and the critic weights\nare unknown. The simulation uses 100 ﬁxed Bellman error extrapolation points in a 4x4 square\naround the origin of the s−coordinate system.\nOctober 6, 2021\nDRAFT\n15\nTABLE I\nCOMPARISON OF COSTS FOR A SINGLE BARRIER TRANSFORMED TRAJECTORY OF (35), OBTAINED USING THE OPTIMAL\nFEEDBACK CONTROLLER GENERATED VIA THE DEVELOPED METHOD, AND OBTAINED USING PSEUDOSPECTRAL\nNUMERICAL OPTIMAL CONTROL SOFTWARE\nMethod\nCost\nBT MBRL with FCL\n71.8422\nGPOPS II ( [35])\n72.9005\n1) Results for the two state system: As seen from Fig. 2, the system state x stays within\nthe user-speciﬁed safe set while converging to the origin. The results in Fig. 3 indicate that the\nunknown weights for both the actor and critic NNs converge to similar values. As demonstrated\nin Fig. 4 the parameter estimation errors also converge to the zero.\nSince the ideal actor and critic weights are unknown, the estimates cannot be directly compared\nagainst the ideal weights. To gauge the quality of the estimates, the trajectory generated by the\ncontroller u(t) = ˆu\n\u0010\ns(t), ˆW ∗\nc\n\u0011\n, where ˆW ∗\nc is the ﬁnal value of the critic weights obtained in\nFig. 3, starting from a speciﬁc initial condition, is compared against the trajectory obtained\nusing an ofﬂine numerical solution computed using the GPOPS II optimization software (see,\ne.g., [35]). The total cost, generated by numerically integrating (17), is used as the metric for\ncomparison. The costs are computed over a ﬁnite horizon, selected to be roughly 5 times the time\nconstant of the optimal trajectories. The results in Table I indicate that while the two solution\ntechniques generate slightly different trajectories in the phase space (see Fig. 5) the total cost\nof the trajectories is similar.\n2) Sensitivity Analysis for the two state system: To study the sensitivity of the developed tech-\nnique to changes in various tuning parameters, a one-at-a-time sensitivity analysis is performed.\nThe parameters kc1, kc2, ka1, ka2, β, and v are selected for the sensitivity analysis. The costs\nof the trajectories, under the optimal feedback controller obtained using the developed method,\nare presented in Table II for 5 different values of each parameter. The parameters are varied\nin a neighborhood of the nominal values (selected through trial and error) kc1 = 0.3, kc2 = 5,\nka1 = 180, ka2 = 0.0001, β = .03, and v = 0.5. The value of β1 is set to be diag(50, 50, 50, 50).\nThe results in Table II indicate that the developed method is robust to small changes in the\nlearning gains.\nOctober 6, 2021\nDRAFT\n16\n-8\n-6\n-4\n-2\n0\n2\n4\n6\n-6\n-4\n-2\n0\n2\n4\n6\n8\nFig. 2. Phase portrait for the two-state dynamical system using MBRL with FCL in the original coordinates. The boxed area\nrepresents the user-selected safe set.\n0\n2\n4\n6\n8\n0\n2\n4\n6\n8\n10\nFig. 3. Estimates of the actor and the critic weights under nominal gains for the two-state dynamical system.\n0\n1\n2\n3\n4\n-1\n-0.5\n0\n0.5\n1\nFig. 4. Estimates of the unknown parameters in the system under the nominal gains for the two-state dynamical system. The\ndash lines in the ﬁgure indicates the ideal values of the parameters.\nOctober 6, 2021\nDRAFT\n17\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nFig. 5.\nComparison of the optimal trajectories obtained using GPOPS II and using BT MBRL with FCL and ﬁxed optimal\nweights for the two-state dynamical system.\nTABLE II\nSENSITIVITY ANALYSIS FOR THE TWO STATE SYSTEM\nkc1=\n0.01\n0.05\n0.1\n0.2\n0.3\nCost\n72.7174\n72.6919\n72.5378\n72.3019\n72.1559\nkc2=\n2\n3\n5\n10\n15\nCost\n71.7476\n72.3198\n72.1559\n71.8344\n71.7293\nka1=\n175\n180\n250\n500\n1000\nCost\n72.1568\n72.1559\n72.1384\n72.1085\n72.0901\nka2=\n0.0001\n0.0009\n0.001\n0.005\n0.01\nCost\n72.1559\n72.1559\n72.1559\n72.1559\n72.1559\nβ=\n0.001\n0.005\n0.01\n0.03\n0.04\nCost\n72.2141\n72.1559\n72.1958\n72.1559\n72.1352\nv=\n0.5\n1\n10\n50\n100\nCost\n72.1559\n72.4054\n72.6582\n79.1540\n81.32\nB. Four state dynamical system\nThe four-state dynamical system corresponding to a two-link planar robot manipulator is given\nby\n˙x = f1(x) + f2(x)θ + g(x)u\n(37)\nOctober 6, 2021\nDRAFT\n18\nwhere\nf1(x) =\n\n\nx3\nx4\n−M −1Vm\n\nx3\nx4\n\n\n\n\n,\nf2(x) =\n\n\n0, 0, 0, 0\n0, 0, 0, 0\n−[M −1, M −1]D\n\n,\nθ =\n\n\nfd1\nfd2\nfs1\nfs1\n\n\n,\n(38)\ng(x) =\n\n\n0, 0\n0, 0\n(M −1)T\n\n,\nD := diag\nh\nx3, x4, tanh(x3), tanh(x4)\ni\n,\n(39)\nM :=\n\np1 + 2p3c2\np2 + p3c2\np2 + p3c2\np2\n\n∈R2×2,\nVM :=\n\n−p3s2x4\n−p3s2(x3 + x4)\np3s2x3\n0\n\n∈R2×2, (40)\nwith s2 = sin(x2), c2 = cos(x2), p1 = 3.473, p2 = 0.196, p3 = 0.242. The positive con-\nstants fd1, fd2, fs1, fs1 ∈R are the unknown parameters. The parameters are selected as fd1 =\n5.3, fd2 = 1.1, fs1 = 8.45, fs1 = 2.35. The state x = [x1\nx2\nx3\nx4]T that corresponds to\nangular positions and the angular velocities of the two links needs to satisfy the constraints,\nx1 ∈(−7, 5), x2 ∈(−7, 5), x3 ∈(−5, 7) and x4 ∈(−5, 7). The objective for the con-\ntroller is to minimize the inﬁnite horizon cost function in (17), with Q = diag(1, 1, 1, 1) and\nR = diag(1, 1) while identifying the unknown parameters θ ∈R4 that correspond to static and\ndynamic friction coefﬁcients in the two links. The ideal values of the the unknown parameters\nare θ1 = 5.3, θ2 = 1.1, θ3 = 8.45, and θ4 = 2.35. The basis functions for value function\napproximation are selected as σ(s) = [s1s3; s2s4; s3s2; s4s1; s1s2; s4s3; s2\n1; s2\n2; s2\n3; s2\n4]. The initial\nconditions for the system and the initial guesses for the weights and parameters are selected\nas x(0) = [−5; −5; 5; 5], ˆθ(0) = [5; 5; 5; 5], Γ(0) = diag(10, 10, 10, 10, 10, 10, 10, 10, 10, 10), and\nˆWa(0) = ˆWc(0) = [60; 2; 2; 2; 2; 2; 40; 2; 2; 2]. The ideal values of the actor and the critic weights\nare unknown. The simulation uses 100 ﬁxed Bellman error extrapolation points in a 4x4 square\naround the origin of the s−coordinate system.\n1) Results for the four state system: As seen from Fig. 6, the system state x stays within the\nuser-speciﬁed safe set while converging to the origin. As demonstrated in Fig. 8, the parameter\nestimations converge to the true values. A comparison with ofﬂine numerical optimal control,\nsimilar to the procedure used for the two-state, yields the results in Table III indicate that the\ntwo solution techniques generate slightly different trajectories in the state space (see Fig. 9) and\nthe total cost of the trajectories is different. We hypothesize that the difference in costs is due\nOctober 6, 2021\nDRAFT\n19\n0\n10\n20\n30\n40\n50\n60\n-5\n0\n5\nFig. 6.\nState trajectories for the four-state dynamical system using MBRL with FCL in the original coordinates. The dash lines\nrepresent the user-selected safe set.\n0\n2\n4\n6\n8\n10\n12\n-10\n0\n10\n20\n30\n40\n50\n60\nFig. 7. Estimates of the critic weights under nominal gains for the four-state dynamical system.\nto the basis for value function approximation being unknown. In summary, the newly developed\nmethod can achieve online optimal control thorough a BT MBRL approach while estimating the\nvalue of the unknown parameters in the system dynamics and ensuring safety guarantees in the\noriginal coordinates during the learning phase.\nThe following section details a one-at-a-time\nsensitivity analysis and study the sensitivity of the developed technique to changes in various\ntuning parameters.\n2) Sensitivity Analysis for the four state system: The parameters kc1, kc2, ka1, ka2, β, and v\nare selected for the sensitivity analysis. The costs of the trajectories, under the optimal feedback\ncontroller obtained using the developed method, are presented in Table IV for 5 different values\nof each parameter. The parameters are varied in a neighborhood of the nominal values (selected\nOctober 6, 2021\nDRAFT\n20\n0\n5\n10\n15\n0\n2\n4\n6\n8\n10\nFig. 8. Estimates of the unknown parameters in the system under the nominal gains for the four-state dynamical system. The\ndash lines in the ﬁgure indicates the ideal values of the parameters.\nTABLE III\nCOSTS FOR A SINGLE BARRIER TRANSFORMED TRAJECTORY OF (37), OBTAINED USING THE DEVELOPED METHOD, AND\nUSING PSEUDOSPECTRAL NUMERICAL OPTIMAL CONTROL SOFTWARE\nMethod\nCost\nBT MBRL with FCL\n95.1490\nGPOPS II\n57.8740\nTABLE IV\nSENSITIVITY ANALYSIS FOR THE FOUR STATE SYSTEM\nkc1=\n0.01\n0.05\n0.1\n0.5\n1\nCost\n95.91\n95.4185\n95.1490\n94.1607\n93.5487\nkc2=\n1\n5\n10\n20\n30\nCost\n304.4\n101.0786\n95.1490\n92.7148\n93.729\nka1=\n5\n10\n20\n30\n50\nCost\n94.9464\n95.1224\n95.1490\n95.1736\n95.1974\nka2=\n0.05\n0.1\n0.2\n0.5\n1\nCost\n95.2750\n95.2480\n95.1490\n94.9580\n94.6756\nβ=\n0.1\n0.5\n0.8\n0.9\n0.95\nCost\n125.33\n109.7721\n95.1490\n92.91\n93.7231\nv=\n50\n70\n100\n125\n150\nCost\n92.2836\n93.34\n95.1490\n96.1926\n97.9870\nOctober 6, 2021\nDRAFT\n21\n0\n50\n100\n150\n-2\n-1\n0\n1\n0\n5\n10\n15\n20\n-1\n0\n1\n2\nFig. 9. Comparison of the optimal angular position (top) and angular velocity (bottom) trajectories obtained using GPOPS II\nand BT MBRL with ﬁxed optimal weights for the four-state dynamical system.\nthrough trial and error) kc1 = 0.1, kc2 = 10, ka1 = 20, ka2 = 0.2, β = 0.8, and v = 100.\nThe value of β1 is set to be diag(100, 100, 100, 100). The results in Table IV indicate that the\ndeveloped method is not sensitive to small changes in the learning gains.\nThe results in Tables 2 and 4 indicate that the developed method is not sensitive to small\nchanges in the learning gains. While reduced sensitivity to gains simpliﬁes gain selection, as\nindicated by the local stability result, the developed method is sensitive to selection of basis\nfunction and initial guesses of the unknown weights. Due to high dimensionality of the vector\nof unknown weights, a complete characterization of the region of attraction is computationally\ndifﬁcult. As such, the basis functions and the initial guess were selected via trial and error.\nVIII. CONCLUSION\nThis paper develops a novel online safe control synthesis technique which relies on a non-\nlinear coordinate transformation that transforms a constrained optimal control problem into an\nunconstrained optimal control problem. A model of the system in the transformed coordinates\nis simultaneously learned and utilized to simulate experience. Simulated experience is used to\nrealize convergent RL under relaxed excitation requirements. Safety of the closed-loop system,\nexpressed in terms of box constraint, regulation of the system states to a neighborhood of the\norigin, and convergence of the estimated policy to a neighborhood of the optimal policy in\ntransformed coordinates is established using a Lyapunov-based stability analysis.\nOctober 6, 2021\nDRAFT\n22\nWhile the main result of the paper states that the state is uniformly ultimately bounded, the\nsimulation results hint towards asymptotic convergence of the part of the state that corresponds\nto the system trajectories, x(·). Proving such a result is a part of future research.\nLimitations and possible extensions of the ideas presented in this paper revolve around the\ntwo key issues: (a) safety, and (b) online learning and optimization. The barrier function used\nin the BT to address safety can only ensure a ﬁxed box constraint. A more generic and adaptive\nbarrier function, constructed, perhaps, using sensor data is a subject for future research.\nFor optimal learning, parametric approximation techniques are used to approximate the value\nfunctions in this paper. Parametric approximation of the value function requires selection of\nappropriate basis functions which may be hard to ﬁnd for the barrier-transformed dynamics.\nDeveloping techniques to systematically determine a set of basis functions for real-world systems\nis a subject for research.\nThe barrier transformation method to ensure safety relies on knowledge of the dynamics of the\nsystem. While this paper addresses parametric uncertainties, the BE method could potentially\nresult in a safety violation due to unmodeled dynamics. In particular, the safety guarantees\ndeveloped in this paper rely on the relationship (Lemma 1) between trajectories of the original\ndynamics and the transformed system, which holds in the presence of parametric uncertainty, but\nfails if a part of the dynamics is not included in the original model. Further research is needed\nto establish safety guarantees that are robust to unmodeled dynamics (for a differential games\napproach to robust safety, see [26]).\nOctober 6, 2021\nDRAFT\n23\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, Reinforcement learning: an introduction.\nCambridge, MA, USA: MIT Press, 1998.\n[2] K. Doya, “Reinforcement learning in continuous time and space,” Neural Comput., vol. 12, no. 1, pp. 219–245, 2000.\n[3] R. Kamalapurkar, J. A. Rosenfeld, and W. E. Dixon, “Efﬁcient model-based reinforcement learning for approximate\nonline optimal control,” Automatica, vol. 74, pp. 247–258, Dec. 2016.\n[4] R. Kamalapurkar, P. Walters, and W. E. Dixon, “Model-based reinforcement learning for approximate optimal regulation,”\nAutomatica, vol. 64, pp. 94–104, Feb. 2016.\n[5] R. Kamalapurkar, P. Walters, J. A. Rosenfeld, and W. E. Dixon, Reinforcement learning for optimal feedback control: A\nLyapunov-based approach, ser. Communications and Control Engineering.\nSpringer International Publishing, 2018.\n[6] H. Modares, F. L. Lewis, and M.-B. Naghibi-Sistani, “Adaptive optimal control of unknown constrained-input systems\nusing policy iteration and neural networks,” IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 10, pp. 1513–1525, 2013.\n[7] B. Kiumarsi, F. L. Lewis, H. Modares, A. Karimpour, and M.-B. Naghibi-Sistani, “Reinforcement Q-learning for optimal\ntracking control of linear discrete-time systems with unknown dynamics,” Automatica, vol. 50, no. 4, pp. 1167–1175, Apr.\n2014.\n[8] C. Qin, H. Zhang, and Y. Luo, “Online optimal tracking control of continuous-time linear systems with unknown dynamics\nby using adaptive dynamic programming,” Int. J. Control, vol. 87, no. 5, pp. 1000–1009, 2014.\n[9] K. Vamvoudakis, D. Vrabie, and F. L. Lewis, “Online policy iteration based algorithms to solve the continuous-time inﬁnite\nhorizon optimal control problem,” in IEEE Symp. Adapt. Dyn. Program. Reinf. Learn., 2009, pp. 36–41.\n[10] F. L. Lewis and D. Vrabie, “Reinforcement learning and adaptive dynamic programming for feedback control,” IEEE\nCircuits Syst. Mag., vol. 9, no. 3, pp. 32–50, 2009.\n[11] D. P. Bertsekas, “Dynamic programming and optimal control 3rd edition, volume II,” Belmont, MA: Athena Scientiﬁc,\n2011.\n[12] S. Bhasin, R. Kamalapurkar, M. Johnson, K. G. Vamvoudakis, F. L. Lewis, and W. E. Dixon, “An actor-critic-identiﬁer\narchitecture for adaptive approximate optimal control,” in Reinforcement Learning and Approximate Dynamic\nProgramming for Feedback Control, ser. IEEE Press Series on Computational Intelligence, F. L. Lewis and D. Liu, Eds.\nWiley and IEEE Press, Feb. 2012, pp. 258–278.\n[13] D. Liu and Q. Wei, “Policy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems,” IEEE\nTrans. Neural Netw. Learn. Syst., vol. 25, no. 3, pp. 621–634, Mar. 2014.\n[14] W. He, Z. Li, and C. L. P. Chen, “A survey of human-centered intelligent robots: issues and challenges,” IEEE/CAA J.\nAutom. Sin., vol. 4, no. 4, pp. 602–609, 2017.\n[15] K. G. Vamvoudakis, M. F. Miranda, and J. P. Hespanha, “Asymptotically stable adaptive-optimal control algorithm with\nsaturating actuators and relaxed persistence of excitation,” IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 11, pp.\n2386–2398, 2016.\n[16] P. Walters, R. Kamalapurkar, and W. E. Dixon, “Approximate optimal online continuous-time path-planner with static\nobstacle avoidance,” in Proc. IEEE Conf. Decis. Control, Osaka, Japan, Dec. 2015, pp. 650–655.\n[17] P. Deptula, H. Chen, R. A. Licitra, J. A. Rosenfeld, and W. E. Dixon, “Approximate optimal motion planning to avoid\nunknown moving avoidance regions,” IEEE Transactions on Robotics, vol. 36, no. 2, pp. 414–430, 2020.\n[18] A. D. Ames, X. Xu, J. W. Grizzle, and P. Tabuada, “Control barrier function based quadratic programs for safety critical\nsystems,” IEEE Trans. Autom. Control, vol. 62, no. 8, pp. 3861–3876, Aug. 2017.\nOctober 6, 2021\nDRAFT\n24\n[19] J. Choi, F. Casta˜neda, C. Tomlin, and K. Sreenath, “Reinforcement Learning for Safety-Critical Control under Model\nUncertainty, using Control Lyapunov Functions and Control Barrier Functions,” in Proc. Robot. Sci. Syst., Corvalis, Oregon,\nUSA, July 2020.\n[20] M. H. Cohen and C. Belta, “Approximate optimal control for safety-critical systems with control barrier functions,” in\nProc. IEEE Conf. Decis. Control, 2020, pp. 2062–2067.\n[21] Z. Marvi and B. Kiumarsi, “Safe reinforcement learning: A control barrier function optimization approach,” Int. J. Robust\nNonlinear Control, vol. 31, no. 6, pp. 1923–1940, 2021.\n[22] Y. Yang, K. G. Vamvoudakis, H. Modares, W. He, Y.-X. Yin, and D. Wunsch, “Safety-aware reinforcement learning\nframework with an actor-critic-barrier structure,” in Proc. Am. Control Conf., 2019, pp. 2352–2358.\n[23] K. Graichen and N. Petit, “Incorporating a class of constraints into the dynamics of optimal control problems,” Optimal\nControl Applications and Methods, vol. 30, no. 6, pp. 537–561, 2009.\n[24] C. P. Bechlioulis and G. A. Rovithakis, “Adaptive control with guaranteed transient and steady state tracking error bounds\nfor strict feedback systems,” Automatica, vol. 45, no. 2, pp. 532 – 538, 2009.\n[25] M. L. Greene, P. Deptula, S. Nivison, and W. E. Dixon, “Sparse learning-based approximate dynamic programming with\nbarrier constraints,” IEEE Control Syst. Lett., vol. 4, no. 3, pp. 743–748, 2020.\n[26] Y. Yang, D.-W. Ding, H. Xiong, Y. Yin, and D. C. Wunsch, “Online barrier-actor-critic learning for h∞control with\nfull-state constraints and input saturation,” J. Franklin Inst., vol. 357, no. 6, pp. 3316 – 3344, 2020.\n[27] R. G. Sanfelice, Hybrid Feedback Control.\nPrinceton University Press, 2021.\n[28] V. Adetola and M. Guay, “Finite-time parameter estimation in adaptive control of nonlinear systems,” IEEE Trans. Autom.\nControl, vol. 53, no. 3, pp. 807–811, 2008.\n[29] S. B. Roy, S. Bhasin, and I. N. Kar, “Parameter convergence via a novel PI-like composite adaptive controller for uncertain\nEuler-Lagrange systems,” in Proc. IEEE Conf. Decis. Control, Dec. 2016, pp. 1261–1266.\n[30] S. Bhasin, R. Kamalapurkar, M. Johnson, K. G. Vamvoudakis, F. L. Lewis, and W. E. Dixon, “A novel actor-critic-identiﬁer\narchitecture for approximate optimal control of uncertain nonlinear systems,” Automatica, vol. 49, no. 1, pp. 89–92, Jan.\n2013.\n[31] D. Vrabie and F. Lewis, “Online adaptive optimal control based on reinforcement learning,” in Optimization and Optimal\nControl: Theory and Applications, A. Chinchuluun, P. M. Pardalos, R. Enkhbat, and I. Tseveendorj, Eds.\nNew York,\nNY: Springer New York, 2010, pp. 309–323.\n[32] H. Modares, F. L. Lewis, and M.-B. Naghibi-Sistani, “Integral reinforcement learning and experience replay for adaptive\noptimal control of partially-unknown constrained-input continuous-time systems,” Automatica, vol. 50, no. 1, pp. 193–202,\n2014.\n[33] M. L. Greene, P. Deptula, R. Kamalapurkar, and W. E. Dixon, “Mixed density methods for approximate dynamic\nprogramming,” in Handbook of Reinforcement Learning and Control, ser. Studies in Systems, Decision and Control,\nK. G. Vamvoudakis, Y. Wan, F. Lewis, and D. Cansever, Eds.\nCham: Springer International Publishing, 2021, vol. 325,\nch. 5, pp. 139–172.\n[34] F. Sauvigny, Partial Differential Equations 1.\nSpringer, 2012.\n[35] M. A. Patterson and A. V. Rao, “GPOPS-II: A MATLAB software for solving multiple-phase optimal control problems\nusing hp-adaptive gaussian quadrature collocation methods and sparse nonlinear programming,” ACM Trans. Math. Softw.,\nvol. 41, no. 1, Oct. 2014.\n[36] H. K. Khalil, Nonlinear systems, 3rd ed.\nUpper Saddle River, NJ: Prentice Hall, 2002.\nOctober 6, 2021\nDRAFT\n25\nAPPENDIX\nLemma 1. If t 7→Φ\n\u0000t, b(x0), ζ\n\u0001\nis a complete Carath´eodory solution to (7), starting from the\ninitial condition b(x0), under the feedback policy (s, t) 7→ζ(s, t) and t 7→Λ(t, x0, ξ) is a\nCarath´eodory solution to (1), starting from the initial condition x0, under the feedback policy\n(x, t) 7→ξ(x, t), deﬁned as ξ(x, t) = ζ(b(x), t), then Λ(·, x0, ξ) is complete and Λ(t, x0, ξ) =\nb−1 (Φ(t, b(x0), ζ)) for all t ∈R≥0.\nProof. Note that since t 7→Φ\n\u0000t, b(x0), ζ\n\u0001\nis a complete Carath´eodory solution to ˙s = F(s) +\nG(s)ζ(s, t), it is differentiable at almost all t ∈R≥0. Since b−1 is smooth, t 7→b−1 (Φ (t, b(x0), ζ))\nis also differentiable at almost all t ∈R≥0. That is,\nd(b−1 ◦Φi)\ndt\n\u0000t, b(x0), ζ\n\u0001\n=\ndb−1\n(ai,Ai)(y)\ndy\n|y=Φi(t,b(x0),ζ)\ndΦi\ndt\n\u0000t, b(x0), ζ\n\u0001\n,\nfor almost all t ∈R≥0 and all i = 1, · · · , n, where Φi denotes the ith component of Φ. As a\nresult,\nd(b−1 ◦Φi)\ndt\n\u0000t, b(x0), ζ\n\u0001\n= (F (Φ (t, b(x0), ζ)))i\nBi (Φi (t, b(x0), ζ)) + (G (Φ (t, b(x0), ζ)))i ζ (Φ (t, b(x0), ζ) , t)\nBi (Φi (t, b(x0), ζ))\n,\nfor almost all t ∈R≥0 and all i = 1, . . . , n. By the construction of F, G, and ξ,\nd(b−1 ◦Φ)\ndt\n\u0000t, b(x0), ζ\n\u0001\n= f\n\u0000b−1 ◦Φ\n\u0000t, b(x0), ζ\n\u0001\u0001\nθ\n+ g\n\u0000b−1 ◦Φ\n\u0000t, b(x0), ζ\n\u0001\u0001\nξ\n\u0000b−1 ◦Φ\n\u0000t, b(x0), ζ\n\u0001\n, t\n\u0001\n,\nfor almost all t ∈R≥0. Clearly t 7→b−1◦Φ (t, b(x0), ζ) is a Carath´eodory solution of (1) on R≥0,\nstarting from the initial condition b−1\u0000b(x0)\n\u0001\n= x0 under the feedback policy (x, t) 7→ξ(x, t). By\nuniqueness of solutions ˙x = f(x)θ + g(x)ξ(x, t) (which follows from local Lipschitz continuity\nof f, g, and b inside the barrier), Λ(·, x0, ξ) is complete and Λ(t, x0, ξ) = b−1 (Φ (t, b(x0), ζ))\nfor all t ∈R≥0.\nLemma 2. If ∥Yf∥is non-decreasing in time then (13) admits Carath´eodory solutions.\nProof. Since ∥Yf(0)∥= 0, given any piecewise continuous control signal t 7→u(t) and initial\nconditions s0 and θ0, the Cauchy problem ˙z = h1(z, u), z(0) = z0 = [s0; 0; 0; 0; 0; θ0] admits\na unique Carath´eodory solution t 7→z1(0, z0) over [0, t∗), with t∗= min(t1, t2), where t1 =\ninf{t ∈R≥0 | ∥Yf1(t, z0∥= Yf} and t2 = inf{t ∈R≥0| limτ7→t ∥z1(τ, z0)∥= ∞}, where Yf1\ndenotes the Yf component of z1.\nOctober 6, 2021\nDRAFT\n26\nGiven any (t′, z′) ∈R≥0×R2n+2p+p2+np, the Cauchy problem ˙z = h2(z, u), z(t′) = z′, also ad-\nmits a unique Carath´eodory solution t 7→z2(t; t′, z′) over [t′, t∗∗) where t∗∗= min\n\u0010\n∞,\n\u0000inf{t ∈\nR≥t′| limτ7→t ∥z2(τ, b′, z′)∥= ∞}\n\u0001\u0011\n.\nIf t∗= t2 then t 7→z1(t, z0) is also a unique Carath´eodory solution to the Cauchy problem\n˙z = h(z, u), z(0) = z0. If not, then\nt 7→z∗(t, z0) =\n\n\n\n\n\nz1(t, z0),\nt < t1,\nz2\n\u0000t, t1, limτ↑t1 z1(τ, z0)\n\u0001\n,\nt ≥t1,\nis a unique Carath´eodory solution to the Cauchy problem ˙z = h(z, u), z(0) = z0.\nTheorem 1. Provided Assumptions 1, 2, and 3 hold, the gains are selected large enough based\non (46) - (49), and the weights ˆθ, ˆWc, Γ, and ˆWa are updated according to (12), (29), (30),\nand (31), respectively, then the estimation errors ˜Wc, ˜Wa, and ˜θ and the trajectories of the\ntransformed system in (7) under the controller in (32) are locally uniformly ultimately bounded.\nProof. Under Assumption 1, the state trajectories are bounded over the interval [0, T). Over the\ninterval [T, ∞), let Br ⊂Rn+2L+p denote a closed ball with radius r centered at the origin.\nLet χ denote the projection of Br onto Rn. For any continuous function h : Rn →Rm,\nlet the notation ∥(·)∥be deﬁned as ∥h∥:= supso∈χ ∥h (so)∥. To facilitate the analysis, let\n{ϖj ∈R>0 | j = 1, · · · , 7} be constants such that ϖ1+ϖ2+ϖ3 = 1, and ϖ4+ϖ5+ϖ6+ϖ7 = 1.\nLet c ∈R>0 be a constant deﬁned as\nc :=\nβ\n2Γkc2\n+ c3\n2 ,\n(41)\nk5 be a positive constant deﬁned as k5 := ¯WKc1∥∇sσ∥Ly. and let ι ∈R be a positive constant\ndeﬁned as\nι ≜(kc1 + kc2)2 ∥∆∥\n2\n4kc2cϖ3\n+ 1\n4∥Gϵ∥+\n1\n4 (ka1 + ka2) ϖ6\n\u00121\n2W∥Gσ∥+ 1\n2∥∇sϵGT∇sσT∥\n\u0013\n+\n1\n4 (ka1 + ka2) ϖ6\n\u0012\nka2W + 1\n4 (kc1 + kc2) W\n2∥Gσ∥\n\u00132\n.\n(42)\nTo facilitate the stability analysis, let VL : Rn+2L+p×R≥0 →R≥0 be a continuously differentiable\ncandidate Lyapunov function deﬁned as\nVL (Z, t) := V ∗(s) + 1\n2\n˜W T\nc Γ−1(t) ˜Wc + 1\n2\n˜W T\na ˜Wa + V1(˜θ),\n(43)\nOctober 6, 2021\nDRAFT\n27\nwhere V ∗is the optimal value function, V1 was introduced in section IV and Z ≜\nh\ns; ˜Wc; ˜Wa; ˜θ\ni\n.\nThe update law in (29) ensures that the adaptation gain matrix is bounded such that\nΓ ≤∥Γ(t)∥≤Γ, ∀t ∈R≥T.\n(44)\nUsing the fact that V ∗and V1 are positive deﬁnite, Lemma 4.3 from [36] yields\nvl (∥Z∥) ≤VL (Z, t) ≤vl (∥Z∥) ,\n(45)\nfor all t ∈R≥T and for all Z ∈Rn+2L+p, where vl, vl : R≥0 →R≥0 are class K functions. Let vl :\nR≥0 →R≥0 be a function deﬁned as vl (∥Z∥) := λmin{Q}∥s∥2\n2\n+ kc2cϖ1\n2\n\r\r\r ˜Wc\n\r\r\r\n2\n+ (ka1+ka2)ϖ4\n2\n\r\r\r ˜Wa\n\r\r\r\n2\n+\n∥˜θ∥\n2\n2 .\nThe sufﬁcient conditions for ultimate boundedness of Z are derived based on the subsequent\nstability analysis as\n\u0012\nkc2cϖ2 −k5rϵ\n2\n\u0013\n(ka1 + ka2)ϖ5 ≥\n\u0012\nka1 + 1\n4 (kc1 + kc2) W∥Gσ∥\n\u0013\n,\n(46)\n(ka1 + ka2) ϖ7 ≥1\n4 (kc1 + kc2) W∥Gσ∥,\n(47)\nλmin{Yf(T)} ≥k5r\n2ϵ + 1,\n(48)\nv−1\nl (ι) < vl\n−1(vl(r)).\n(49)\nThe bound on the function F and the NN function approximation errors depend on the underlying\ncompact set; hence, ι is a function of r. Even though, in general, ι increases with increasing\nr, the sufﬁcient condition in (49) can be satisﬁed provided the points for BE extrapolation are\nselected such that the constant c, introduced in (41) is large enough and that the basis for value\nfunction approximation are selected such that ∥ϵ∥and ∥∇ϵ∥are small enough.\nThe differential equation (7), under the controller in (32), along with (12), (29), and (31),\nconstitute the closed-loop system ˙Z = h(Z, t) to be analyzed. Let ˙VL denote the orbital derivative\nof (43) along the trajectories of the closed-loop system, i.e., ˙VL := ∇tVL + ∇ZVL(Z, t)h(Z, t).\nThen,\n˙VL = ∇sV ∗F + ∇sV ∗Gˆu + ˜W T\nc Γ−1 ˙˜Wc + 1\n2\n˜W T\nc ˙Γ−1 ˜Wc + ˜W T\na ˙˜Wa + ˙V1.\n(50)\nSubstituting (29) - (31) in (50) yields\n˙VL ≤∇sV ∗(F + Gu∗) −∇sV ∗Gu∗+ ∇sV ∗Gˆu −˜W T\nc Γ−1\n\u0012\n−kc1Γω\nρ\nˆδ −1\nN Γ\nN\nX\nk=1\nkc2ωi\nρk\nˆδk\n\u0013\nOctober 6, 2021\nDRAFT\n28\n−1\n2\n˜W T\nc Γ−1\n\u0012\nβΓ −kc1(ΓωωT\nρ2 Γ) −kc2\nN Γ\nN\nX\nk=1\nωkωT\nk\nρ2\nk\nΓ\n\u0013\nΓ−1 ˜Wc\n−˜W T\na\n\u0012\n−ka1( ˆWa −ˆWc) −ka2 ˆWa +\n\u0000(kc1ω\n4ρ\nˆW T\na Gσ +\nN\nX\nk=1\nkc2ωk\n4Nρk\nˆW T\na Gσk)T ˆWc\n\u0001\u0013\n+ ˙V1.\n(51)\nThe Lyapunov derivative can be rewritten as\n˙VL ≤−sTQs −1\n4W TGσW + 1\n2W TGσ ˜Wa + 1\n4Gϵ + 1\n2\n˜W T\na ∇sσGR∇sϵTr\n−˜W T\nc Γ−1\n\u0012\n−kc1Γω\nρ (−ωT ˜Wc+ 1\n4\n˜W T\na Gσ ˜Wa−W T∇sσy˜θ+ 1\n2W T∇sσGR∇sϵT + 1\n4Gϵ−∇sϵF)\n\u0013\n+ ˜W T\nc Γ−1\n\u0012 1\nN Γ\nN\nX\nk=1\nkc2ωk\nρk\n\u0000−ωT\nk ˜Wc + 1\n4\n˜W T\na Gσk ˜Wa −(W T∇sσkyk˜θ) + ∆k\n\u0001\u0013\n−β\n2\n˜W T\nc Γ−1 ˜Wc + 1\n2kc1 ˜W T\nc\nωωT\nρ2\n˜Wc\n+ 1\n2kc2 ˜W T\nc\n1\nN\nN\nX\nk=1\nωkωT\nk\nρ2\nk\n˜Wc + ka1 ˜W T\na ˜Wc −(ka1 + ka2) ˜W T\na ˜Wa + ka2 ˜W T\na W\n−˜W T\na\n \u0012kc1ω\n4ρ\nˆW T\na Gσ +\nN\nX\nk=1\nkc2ωk\n4Nρk\nˆW T\na Gσk\n\u0013T\nˆWc\n!\n−λmin{Yf}∥˜θ∥2.\n(52)\nUsing Young’s inequality, Cauchy-Schwarz inequality, and completion of squares, (52) can be\nbounded as\n˙VL ≤−sTQs −kc2c (ϖ1 + ϖ2 + ϖ3)\n\r\r\r ˜Wc\n\r\r\r\n2\n−(ka1 + ka2) (ϖ4 + ϖ5 + ϖ6 + ϖ7)\n\r\r\r ˜Wa\n\r\r\r\n2\n+\n\u00121\n2W∥Gσ∥+ 1\n2\n\r\r∇sϵGT∇sσT\r\r + ka2W + 1\n4 (kc1 + kc2) W\n2∥Gσ∥\n\u0013 \r\r\r ˜Wa\n\r\r\r\n+\n\r\r\r ˜Wc\n\r\r\r\n\u0010\n(kc1 + kc2)\n\r\r\rˆδ\n\r\r\r\n\u0011\n+\n\u0012\nka1 + 1\n4 (kc1 + kc2) W∥Gσ∥\n\u0013 \r\r\r ˜Wa\n\r\r\r\n\r\r\r ˜Wc\n\r\r\r + 1\n4∥Gϵ∥\n+ 1\n4 (kc1 + kc2) W∥Gσ∥\n\r\r\r ˜Wa\n\r\r\r\n2\n−λmin{Yf}∥˜θ∥2 + (k5r)\n \n∥˜θ∥2\n2ϵ\n+ ϵ∥˜Wc∥2\n2\n!\n.\n(53)\nProvided the gains are selected based on the sufﬁcient conditions in (46), (47), (48) and (49),\nthe orbital derivative can be upper-bounded as\n˙VL ≤−vl (∥Z∥) ,\n∀∥Z∥≥v−1\nl\n(ι) ,\n(54)\nfor all t ≥T and ∀Z ∈Br. Using (45), (49), and (54), Theorem 4.18 in [36] can then be invoked\nto conclude that Z is locally uniformly ultimately bounded in the sense that all trajectories\nstarting from initial conditions bounded by ∥Z(T)∥≤vl−1(vl(r)), satisfy lim supt→∞∥Z (t)∥≤\nOctober 6, 2021\nDRAFT\n29\nvl−1 \u0000vl\n\u0000v−1\nl\n(ι)\n\u0001\u0001\n. Furthermore, the concatenated state trajectories are bounded such that ∥Z (t)∥∈\nBr for all t ∈R≥T. Since the estimates ˆWa approximate the ideal weights W, the policy ˆu\napproximates the optimal policy u∗.\nOctober 6, 2021\nDRAFT\n",
  "categories": [
    "eess.SY",
    "cs.SY",
    "math.OC"
  ],
  "published": "2020-07-24",
  "updated": "2021-10-05"
}