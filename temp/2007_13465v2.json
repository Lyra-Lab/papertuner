{
  "id": "http://arxiv.org/abs/2007.13465v2",
  "title": "Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation",
  "authors": [
    "Felix Kreuk",
    "Joseph Keshet",
    "Yossi Adi"
  ],
  "abstract": "We propose a self-supervised representation learning model for the task of\nunsupervised phoneme boundary detection. The model is a convolutional neural\nnetwork that operates directly on the raw waveform. It is optimized to identify\nspectral changes in the signal using the Noise-Contrastive Estimation\nprinciple. At test time, a peak detection algorithm is applied over the model\noutputs to produce the final boundaries. As such, the proposed model is trained\nin a fully unsupervised manner with no manual annotations in the form of target\nboundaries nor phonetic transcriptions. We compare the proposed approach to\nseveral unsupervised baselines using both TIMIT and Buckeye corpora. Results\nsuggest that our approach surpasses the baseline models and reaches\nstate-of-the-art performance on both data sets. Furthermore, we experimented\nwith expanding the training set with additional examples from the Librispeech\ncorpus. We evaluated the resulting model on distributions and languages that\nwere not seen during the training phase (English, Hebrew and German) and showed\nthat utilizing additional untranscribed data is beneficial for model\nperformance.",
  "text": "Self-Supervised Contrastive Learning for Unsupervised Phoneme\nSegmentation\nFelix Kreuk1, Joseph Keshet1, Yossi Adi2\n1Bar-Ilan University\n2Facebook AI Research\nfelix.kreuk@gmail.com\nAbstract\nWe propose a self-supervised representation learning model\nfor the task of unsupervised phoneme boundary detection. The\nmodel is a convolutional neural network that operates directly\non the raw waveform.\nIt is optimized to identify spectral\nchanges in the signal using the Noise-Contrastive Estimation\nprinciple. At test time, a peak detection algorithm is applied\nover the model outputs to produce the ﬁnal boundaries.\nAs\nsuch, the proposed model is trained in a fully unsupervised\nmanner with no manual annotations in the form of target\nboundaries nor phonetic transcriptions. We compare the pro-\nposed approach to several unsupervised baselines using both\nTIMIT and Buckeye corpora. Results suggest that our approach\nsurpasses the baseline models and reaches state-of-the-art\nperformance on both data sets. Furthermore, we experimented\nwith expanding the training set with additional examples from\nthe Librispeech corpus.\nWe evaluated the resulting model\non distributions and languages that were not seen during the\ntraining phase (English, Hebrew and German) and showed\nthat utilizing additional untranscribed data is beneﬁcial for\nmodel performance.\nOur implementation is available at:\nhttps://github.com/felixkreuk/UnsupSeg.\nIndex Terms:\nUnsupervised Phoneme Segmentation, Self-\nSupervised Learning, Contrastive Noise Estimation\n1. Introduction\nPhoneme Segmentation or Phoneme Boundary Detection is an\nimportant precursor task for many speech and audio applica-\ntions such as Automatic Speech Recognition (ASR) [1, 2, 3],\nspeaker diarization [4], keyword spotting [5], and speech sci-\nence [6, 7].\nThe task of phoneme boundary detection has been explored\nunder both supervised and unsupervised settings [8, 9, 10, 11].\nUnder the supervised setting two schemes have been consid-\nered: text-independent speech segmentation and phoneme-to-\nspeech alignment also known as forced alignment, which is a\ntext-dependent task. In the former setup, the model is provided\nwith target boundaries, while in the latter setup, the model is\nprovided with additional information in the form of a set of pro-\nnounced or presumed phonemes. In both schemes, the goal is\nto learn a function that maps the speech utterance to the target\nboundaries as accurately as possible.\nHowever, creating annotated data of phoneme boundaries\nis a strenuous process, often requiring domain expertise, espe-\ncially in low-resource languages [12]. As a consequence, un-\nsupervised methods and Self-Supervised Learning (SSL) meth-\nods, in particular, are highly desirable and even essential.\nIn unsupervised phoneme boundary detection, also called\nFigure 1: An illustration of our model and SSL training scheme.\nThe solid line represents a reference frame z1, the dashed line\nrepresents its positive pair z2, and the dotted lines represent\nnegative distractor frames randomly sampled from the signal.\nblind-segmentation [13, 10], the model is trained to ﬁnd\nphoneme boundaries using the audio signal only. In the self-\nsupervised setting, the unlabeled input is used to deﬁne an aux-\niliary task that can generate labeled pseudo training data. This\ncan then be used to train the model using supervised techniques.\nSSL has been proven to be effective in natural language pro-\ncessing [14, 15], vision [16], and recently has been shown to\ngenerate a useful representation for speech processing [17, 18].\nMost of the SSL work in the domain of speech processing\nand recognition has been focused on extracting acoustic repre-\nsentations for the task of ASR [17, 18]. However, it remains\nunclear how effective SSL methods are when applied to other\nspeech processing applications.\nIn this work, we explore the use of SSL for phoneme bound-\nary detection. Speciﬁcally, we suggest learning a feature repre-\nsentation from the raw waveform to identify spectral changes\nand detect phoneme boundaries accurately. We optimize a Con-\nvolutional Neural Network (CNN) using the Noise Contrastive\nEstimation principle [19] to distinguish between pairs of ad-\njacent frames and pairs of random distractor pairs. The pro-\nposed model is depicted in Figure 1. During inference, a peak-\ndetection algorithm is applied over the model outputs to produce\nthe ﬁnal segment boundaries.\nWe evaluate our method on the TIMIT [20] and Buck-\neye [21] datasets. Results suggest that the proposed approach\nis more accurate than other state-of-the-art unsupervised seg-\nmentation methods. We conducted further experiments with\nlarger amount of untranscribed data that was taken from the Lib-\nrispeech corpus. Such an approach proved to be beneﬁcial for\nbetter overall performance on unseen languages.\narXiv:2007.13465v2  [eess.AS]  6 Aug 2020\nOur contributions:\n• We demonstrated the efﬁciency of SSL, in terms of\nmodel performance, for learning effective representa-\ntions for unsupervised phoneme boundary detection.\n• We provide SOTA results in the task of unsupervised\nphoneme segmentation on several datasets.\n• We provide empirical evidence that leveraging more un-\nlabeled data leads to better overall performance on un-\nseen languages.\nThe paper is organized as follows: In Section 3 we formally set\nthe notation and deﬁnitions used throughout the paper as well\nas the proposed model. Section 3 provides empirical results and\nanalysis. In Section 2 we refer to the relevant prior work. We\nconclude the paper with a discussion in Section 5.\n2. Related work\nThe task of phoneme boundary detection was explored in var-\nious settings. Under the supervised setting, the most common\napproach is the forced alignment. In this setup, previous work\nmainly involved hidden Markov models (HMMs) or structured\nprediction algorithms on handcrafted input features [22, 23].\nIn the text independent setup, most previous work reduced the\ntask of phoneme segmentation to a binary classiﬁcation at each\ntime-step [24, 9]. More recently, [8] suggested using an RNN-\ncoupled with structured loss parameters.\nUnder the unsupervised setting, the speech utterance is pro-\nvided by itself with no boundaries as supervision. Traditionally,\nsignal processing methods were used to detect spectral changes\nover time [25, 26, 27, 13], such areas of change were presumed\nto be the boundary of a speech unit. Recently, Michel et al. [10]\nsuggested training a next-frame prediction model using HMM\nor RNN. Regions of high prediction error were identiﬁed us-\ning peak detection and ﬂagged as phoneme boundaries. More\nrecently, Wang et al. [28] suggested training an RNN autoen-\ncoder and tracking the norm of various intermediate gate val-\nues (forget-gate for LSTM and update-gate for GRU). To ﬁnd\nphoneme boundaries, similar peak detection techniques were\nused on the gate norm over time.\nIn the ﬁeld of self-supervised learning, Van Den Oord et al.\n[17] and Schneider et al. [18] suggested to train a Convolutional\nneural network to distinguish true future samples from random\ndistractor samples using a probabilistic contrastive loss. Also\ncalled Noise Contrastive Estimation, this approach exploits un-\nlabeled data to learn a representation in an unsupervised man-\nner. The resulting representation proved to be useful for a va-\nriety of downstream supervised speech tasks such as ASR and\nspeaker identiﬁcation.\n3. Model\nFollowing the recent success of contrastive self-supervised\nlearning [16, 17, 18], we propose a training scheme for learn-\ning useful representations for unsupervised phoneme boundary\ndetection. We denote the domain of audio samples by X ⊂R.\nThe representation for a raw speech signal is therefore a se-\nquence of samples x = (x1, . . . , xT ), where xt ∈X for all\n1 ≤t ≤T. The length of the input signal varies for different\ninputs, thus the number of input samples in the sequence, T, is\nnot ﬁxed. We denote by X ∗the set of all ﬁnite-length sequences\nover X.\nDenote by z = (z1, . . . , zL) a sequence of spectral rep-\nresentations sampled at a low frequency. Each element in the\nsequence is an N-dimensional real vector, zi ∈Z ⊆RN for\n1 ≤i ≤L. Every element zi corresponds to a 10 ms frame\nof audio with a processing window of 30 ms. Let Z∗denote all\nﬁnite-length sequences over Z.\nWe learn an encoding function f : X ∗→Z∗, from the\ndomain of audio sequences to the domain of spectral represen-\ntations.\nThe function f is optimized to distinguish between\npairs of adjacent frames in the sequence z and pairs of ran-\ndomly sampled distractor frames from z. Denote by D(zi) the\nset non-adjacent frames to zi,\nD(zi) = {zj : |i −j| > 1 , zj ∈z}.\n(1)\nPractically we use K randomly selected frames from D(zi),\nand denote it by DK(zi) ⊂D(zi). The loss for frame zi is\ndeﬁned as,\nˆL(zi, DK(zi)) = −log\nesim(zi,zi+1)\nP\nzj∈{zi+1}∪DK(zi) esim(zi,zj) ,\n(2)\nwhere sim(u, v) = u⊤v/||u|| ||v|| denotes the cosine simi-\nlarly between two vectors u and v. Overall, given a training set\nof m examples S = {xi}m\ni=1, we would like to minimize the\nfollowing objective function,\nL =\nX\nx∈S\nX\nzi∈f(x)\nˆL(zi, DK(zi))\n(3)\nDuring inference, we receive a new utterance x. We then\napply the encoding function to get z = f(x). We set the score\nfor a boundary at time i to be the dissimilarity between the i-th\nframe and the i + 1-th frame for i = 1, . . . , L −1. That is\nscore(zi) = −sim(zi, zi+1) .\n(4)\nIntuitively, score(zi) can be interpreted as the model’s conﬁ-\ndence that the next frame zi+1 belongs to a different segment\nthan that of the current frame zi. Thus, times with high dis-\nsimilarity values are associated with segment changes, and are\nconsidered as candidates for segment boundaries. We apply a\npeak detection algorithm over the dissimilarity values, score(z)\nto get the ﬁnal segmentation. The frames for which the score ex-\nceeds a peak prominence of δ are predicted as boundaries. The\noptimal value of δ is tuned in a cross-validation procedure.\nFigure 2 presents an example utterance from TIMIT. The\npower spectrum of the utterance is presented in (a), the score\nfunction is presented in (b) and the corresponding learned rep-\nresentation z in (c).\n4. Experiments\nIn this section, we provide a detailed description of the experi-\nments. We start by presenting the experimental setup. Then we\noutline the evaluation method. We conclude this section with\nexperimental results and analysis.\n4.1. Experimental setup\nThe function f was implemented as a convolutional neural net-\nwork, constructed of 5 blocks of 1-D strided convolution, fol-\nlowed by Batch-Normalization and a Leaky ReLU [30] non-\nlinear activation function. The network f has kernel sizes of\n(10, 8, 4, 4, 4), strides of (5, 4, 2, 2, 2) and 256 channels per\nlayer.\nFinally, the output was linearly projected by a fully\nconnected-layer. Overall the model was similar to the one pro-\nposed by [17, 18]. However, unlike the aforementioned prior\nTable 1: Comparison of phoneme segmentation models using TIMIT and Buckeye data sets. Precision and recall are calculated with\ntolerance value of 20 ms. Results marked with * are reported using our own optimization.\nTIMIT\nBuckeye\nSetting\nModel\nPrecision\nRecall\nF1\nR-val\nPrecision\nRecall\nF1\nR-val\nUnsupervised\nHoang et al. [29]\n-\n-\n78.20\n81.10\n-\n-\n-\n-\nMichel et al. [10]\n74.80\n81.90\n78.20\n80.10\n69.34∗\n65.14∗\n67.18∗\n72.13∗\nWang et al. [28]\n-\n-\n-\n83.16\n69.61∗\n72.55∗\n71.03∗\n74.83∗\nOurs\n83.89\n83.55\n83.71\n86.02\n75.78\n76.86\n76.31\n79.69\nSupervised\nKing et al.[24]\n87.00\n84.80\n85.90\n87.80\n-\n-\n-\n-\nFranke et al.[9]\n91.10\n88.10\n89.6\n90.80\n87.80\n83.30\n85.50\n87.17\nKreuk et al.[8]\n94.03\n90.46\n92.22\n92.79\n85.40\n89.12\n87.23\n88.76\n(a)\n(b)\n(c)\nFigure 2: An illustration of the prediction produced by our\nmodel: (a) the original spectrogram; (b) our model’s output\nat each time step, red dashed lines represent the ground truth\nsegmentation; (c) the learned representation z.\nwork, the proposed model does not utilize a context network.\nOur experiments with such a network led to inferior perfor-\nmance, and therefore this component was omitted from the ﬁnal\nmodel architecture.\nWe optimized the model using a batch size of 8 exam-\nples and a learning-rate of 1e-4 for 50 epochs. We follow an\nearly-stopping criterion computed over the validation set. All\nreported results are averaged over a set of 3 runs using cross-\nvalidation with different random seed values. To get DK we\nexperimented K ∈{1, 3, 5, 7, 10}, but did not observe signiﬁ-\ncant differences in performance.\nWe evaluated our model on both TIMIT and Buckeye cor-\npora. For the TIMIT corpus, we used the standard train/test\nsplit, where we randomly sampled 10% of the training set for\nvalidation. For Buckeye, we split the corpus at the speaker level\ninto training, validation, and test sets with a ratio of 80/10/10.\nSimilarly to [8], we split long sequences into smaller ones by\ncutting during noises, silences, and un-transcribed segments.\nOverall, each sequence started and ended with a maximum of\n20 ms of non-speech1.\n4.2. Evaluation method\nFollowing previous work on phoneme boundary detection [10,\n28], we evaluated the performance of the proposed models and\nbaseline models using precision (P), recall (R) and F1-score\nwith a tolerance level of 20 ms.\n1All experiments were conducted at Bar-Ilan university.\nA drawback of the F1-score for boundary detection is its\nsensitivity to over-segmentation. A naive segmentation model\nthat outputs a boundary every 40 ms may yield a high F1-score\nby achieving high recall at the cost of low precision. The au-\nthors in [31] proposed a more robust complementary metric de-\nnoted as R-value:\nR-value = 1 −|r1| + |r2|\n2\nr1 =\np\n(1 −R)2 + (OS)2, r2 = −OS + R −1\n√\n2\n(5)\nwhere OS is an over-segmentation measure, deﬁned as OS =\nR/P −1. Overall the performance is presented in terms of\nPrecision, Recall, F1-score and R-value.\n4.3. Results\nIn Table 1 we compared the proposed model against several un-\nsupervised phoneme segmentation baselines: Hoang et al. [29],\nMichel et al. [10], and Wang et al. [28]. We also report re-\nsults for SOTA supervised algorithms in order to gauge the gap\nbetween the unsupervised and supervised methods. As the un-\nsupervised baselines did not report results for the Buckeye data\nset, and there are no pre-trained models available, we optimized\nthese models locally. For a fair comparison we veriﬁed that the\nperformance of the reproduced models is comparable to the one\noriginally reported on TIMIT. These results are marked with *.\nResults suggest that the proposed model is superior to the\nbaseline models over all metrics on both corpora. Notice, for the\nTIMIT benchmark, the proposed model achieves comparable\nresults to a supervised method based on a Kernel-SVM [24].\nAdditionally, as opposed to the reported unsupervised baselines\nwhich are built using Recurrent Neural Networks, our model\nis mainly composed of convolutional operations, hence can be\nparallelized over the temporal axis.\n4.4. The effect of more training data\nBy not relying on manual annotations, SSL methods allow\nleveraging large unlabeled corpora for additional training data.\nIn this sub-section we explored the effect of expanding the\ntraining set with additional examples from the Librispeech cor-\npus [32]. We evaluated the model under the following schemes:\n(i) training distribution and test distribution match; (ii) test dis-\ntribution is different from the training set distribution, but both\nare from the same language; and (iii) test and training distri-\nbutions are from different languages. In the following exper-\nPrecision\nRecall\nF1\nR-val\n50\n60\n70\n80\nR-value\nTIMIT\nTIMIT + Libri\n(a) Hebrew\nPrecision\nRecall\nF1\nR-val\n50\n60\n70\n80\nR-value\nTIMIT\nTIMIT + Libri\n(b) German\nPrecision\nRecall\nF1\nR-val\n50\n60\n70\n80\nR-value\nBuckeye\nBuckeye + Libri\n(c) Hebrew\nPrecision\nRecall\nF1\nR-val\n50\n60\n70\n80\nR-value\nBuckeye\nBuckeye + Libri\n(d) German\nFigure 3: Precision, Recall, F1, and R-value as a function of data added from Librispeech. All models were trained on English training\ndata (sub ﬁgures (a) and (b) on TIMIT while sub ﬁgures (c) and (d) on Buckeye) and evaluated on both Hebrew and German data sets.\nTable 2: Analysis of model performance on the TIMIT and\nBuckeye test sets before and after augmenting them with exam-\nples from Librispeech.\nTraining set\nTest set\nP\nR\nF1\nR-val\nTIMIT\nTIMIT\n83.89\n83.55\n83.71\n86.02\nTIMIT+\nTIMIT\n84.11\n84.17\n84.13\n86.40\nBuckeye\nBuckeye\n75.78\n76.86\n76.31\n79.69\nBuckeye+\nBuckeye\n74.92\n79.41\n77.09\n79.82\nTable 3: Analysis of approach when evaluating the model on a\ntest set that originates from a different distribution than that of\nthe training set.\nTraining set\nTest set\nP\nR\nF1\nR-val\nTIMIT\nBuckeye\n67.48\n73.71\n70.41\n73.10\nTIMIT+\nBuckeye\n71.17\n81.66\n76.05\n76.53\nBuckeye\nTIMIT\n86.26\n79.63\n82.80\n84.61\nBuckeye+\nTIMIT\n86.19\n80.10\n83.03\n84.90\niments, we denote by TIMIT+ and Buckeye+ the augmented\nversions of TIMIT and Buckeye, respectively. To better match\nrecording conditions we chose different partition from Lib-\nrispeech to augment TIMIT and Buckeye.\nFor TIMIT+ we\nused the “train-clean-100” partition from Librispeech, while\nfor Buckeye+ we used the “train-other-500” partition from Lib-\nrispeech.\nIn-domain test set\nResults are summarized in Table 2. Sur-\nprisingly, the models trained on the augmented training sets\nshowed minor improvements over the original models trained\non the TIMIT and Buckeye data sets. In order to better under-\nstand the effect of more training data on model performance,\nwe explore the use of out-of-domain test sets in the following\nparagraphs.\nOut-of-domain test set\nWe repeated the experiment from the\nprevious paragraph, however this time with a cross dataset eval-\nuation. In other words, we optimized a model on TIMIT and\ntested it on Buckeye and vice-versa. Results are summarized\nin Table 3. It can be seen that in cases where the training set\nand the test set originate from the same distribution (Table 2),\nadding more data leads to minor improvements in model per-\nformance. However, when these are coming from mismatched\ndistributions as seen in Table 3, adding more data leads to an\nimprovement in performance. For the TIMIT data set, the R-\nvalue for the model trained on TIMIT+ was improved by 3.43\npoints. For the Buckeye data set, we observed a smaller increase\nin performance.\nMulti-lingual evaluation\nFinally, we analyzed the effect of\nmore training data in the multi-lingual setup. To that end, we\nevaluated the proposed models, trained on TIMIT, TIMIT+,\nBuckeye, and Buckeye+ (English data), using two data sets\nfrom unseen languages. Speciﬁcally, we used a Hebrew data\nset [33] and the PHONDAT German data set [34] as test sets.\nFigure 3 presents the Precision, Recall, F1, and R-value for\nboth data sets with and without additional training data from\nLibrispeech.\nResults suggest that utilizing additional unlabeled data\nyields an increase in performance on unseen languages. For\nexample, when evaluated on the German data set PHONDAT,\nthe TIMIT+ model improved from an R-value of 55.34 to an\nR-value of 75.58, while on the Hebrew data set the Buckeye+\nmodel improved from an R-value of 79.25 to an R-value of\n82.63.\nNotice, the improvement using TIMIT+ is larger by\none order of magnitude comparing to the Buckeye+ improve-\nment. One possible explanation for that is TIMIT being signif-\nicantly smaller comparing to Buckeye, hence beneﬁting more\nfrom additional data. These results highlight the importance of\nadditional diverse data sets in cases where there is a mismatch\nbetween training set and test set languages. Moreover, this sug-\ngests that the representations obtained by the suggested model\nare not tightly coupled with language-speciﬁc features.\n5. Discussion and future work\nIn this work we empirically demonstrated the efﬁciency of self-\nsupervised methods in terms of model performance for the\ntask of unsupervised phoneme boundary detection. Our model\nreached SOTA results on both TIMIT and Buckeye data sets\nunder the unsupervised setting, as well as showed promising re-\nsults in terms of closing the gap between unsupervised and su-\npervised methods. Moreover, we empirically demonstrated that\nusing diverse datasets and leveraging more training data pro-\nduced models with better overall performance on out-of-domain\ndata coming from Hebrew and German.\nFor future work, we will explore the semi-supervised set-\nting, where we are provided with a limited amount of manually\nannotated data. Additionally, we will explore the use of the\nproposed method on low-resource languages and under “in-the-\nwild” conditions. Lastly, we would like to explore the viability\nof such unsupervised segmentation methods in an unsupervised\nASR pipeline.\n6. References\n[1] F. Kubala, T. Anastasakos, H. Jin, L. Nguyen, and R. Schwartz,\n“Transcribing radio news,” in Proceeding of Fourth International\nConference on Spoken Language Processing. ICSLP’96, vol. 2.\nIEEE, 1996, pp. 598–601.\n[2] D. Rybach, C. Gollan, R. Schluter, and H. Ney, “Audio segmenta-\ntion for speech recognition using segment features,” in 2009 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing.\nIEEE, 2009, pp. 4197–4200.\n[3] C.-K. Yeh, J. Chen, C. Yu, and D. Yu, “Unsupervised speech\nrecognition via segmental empirical output distribution match-\ning,” arXiv preprint arXiv:1812.09323, 2018.\n[4] M. H. Moattar and M. M. Homayounpour, “A review on speaker\ndiarization systems and approaches,” Speech Communication,\nvol. 54, no. 10, pp. 1065–1103, 2012.\n[5] J. Keshet, D. Grangier, and S. Bengio, “Discriminative keyword\nspotting,” Speech Communication, vol. 51, no. 4, pp. 317–329,\n2009.\n[6] Y. Adi, J. Keshet, E. Cibelli, E. Gustafson, C. Clopper, and\nM. Goldrick, “Automatic measurement of vowel duration via\nstructured prediction,” The Journal of the Acoustical Society of\nAmerica, vol. 140, no. 6, pp. 4517–4527, 2016.\n[7] Y. Adi, J. Keshet, and M. Goldrick, “Vowel duration measurement\nusing deep neural networks,” in 2015 IEEE 25th International\nWorkshop on Machine Learning for Signal Processing (MLSP).\nIEEE, 2015, pp. 1–6.\n[8] F. Kreuk, Y. Sheena, J. Keshet, and Y. Adi, “Phoneme bound-\nary detection using learnable segmental features,” arXiv preprint\narXiv:2002.04992, 2020.\n[9] J. Franke, M. Mueller, F. Hamlaoui, S. Stueker, and A. Waibel,\n“Phoneme boundary detection using deep bidirectional lstms,” in\nSpeech Communication; 12. ITG Symposium.\nVDE, 2016, pp.\n1–5.\n[10] P. Michel, O. R¨as¨anen, R. Thiolliere, and E. Dupoux, “Blind\nphoneme segmentation with temporal prediction errors,” arXiv\npreprint arXiv:1608.00508, 2016.\n[11] O. Rasanen, “Basic cuts revisited: Temporal segmentation of\nspeech into phone-like units with statistical learning at a pre-\nlinguistic level,” in Proceedings of the Annual Meeting of the Cog-\nnitive Science Society, vol. 36, no. 36, 2014.\n[12] M. Goldrick, J. Keshet, E. Gustafson, J. Heller, and J. Needle,\n“Automatic analysis of slips of the tongue: Insights into the cog-\nnitive architecture of speech production,” Cognition, vol. 149, pp.\n31–39, 2016.\n[13] O. R¨as¨anen, U. K. Laine, and T. Altosaar, “Blind segmentation of\nspeech using non-linear ﬁltering methods,” Speech Technologies,\npp. 105–124, 2011.\n[14] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[15] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta:\nA\nrobustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[16] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A sim-\nple framework for contrastive learning of visual representations,”\narXiv preprint arXiv:2002.05709, 2020.\n[17] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748,\n2018.\n[18] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec:\nUnsupervised pre-training for speech recognition,” arXiv preprint\narXiv:1904.05862, 2019.\n[19] M. Gutmann and A. Hyv¨arinen, “Noise-contrastive estimation: A\nnew estimation principle for unnormalized statistical models,” in\nProceedings of the Thirteenth International Conference on Artiﬁ-\ncial Intelligence and Statistics, 2010, pp. 297–304.\n[20] J. S. Garofolo, “Timit acoustic phonetic continuous speech cor-\npus,” Linguistic Data Consortium, 1993, 1993.\n[21] M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Raymond,\nE. Hume, and E. Fosler-Lussier, “Buckeye corpus of conversa-\ntional speech (2nd release),” Columbus, OH: Department of Psy-\nchology, Ohio State University, 2007.\n[22] J. Keshet, S. Shalev-Shwartz, Y. Singer, and D. Chazan,\n“Phoneme alignment based on discriminative learning,” in Ninth\nEuropean Conference on Speech Communication and Technology,\n2005.\n[23] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Son-\nderegger, “Montreal forced aligner: Trainable text-speech align-\nment using kaldi.” in Interspeech, 2017, pp. 498–502.\n[24] S. King and M. Hasegawa-Johnson, “Accurate speech segmen-\ntation by mimicking human auditory processing,” in 2013 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing.\nIEEE, 2013, pp. 8096–8100.\n[25] S. Dusan and L. Rabiner, “On the relation between maximum\nspectral transition positions and phone boundaries,” in Ninth In-\nternational Conference on Spoken Language Processing, 2006.\n[26] Y. P. Estevan, V. Wan, and O. Scharenborg, “Finding maximum\nmargin segments in speech,” in 2007 IEEE International Con-\nference on Acoustics, Speech and Signal Processing-ICASSP’07,\nvol. 4.\nIEEE, 2007, pp. IV–937.\n[27] G. Almpanidis and C. Kotropoulos, “Phonemic segmentation us-\ning the generalised gamma distribution and small sample bayesian\ninformation criterion,” Speech Communication, vol. 50, no. 1, pp.\n38–55, 2008.\n[28] Y.-H. Wang, C.-T. Chung, and H.-y. Lee, “Gate activation signal\nanalysis for gated recurrent neural networks and its correlation\nwith phoneme boundaries,” arXiv preprint arXiv:1703.07588,\n2017.\n[29] D.-T. Hoang and H.-C. Wang, “Blind phone segmentation based\non spectral change detection using legendre polynomial approx-\nimation,” The Journal of the Acoustical Society of America, vol.\n137, no. 2, pp. 797–805, 2015.\n[30] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectiﬁer nonlinearities\nimprove neural network acoustic models,” in Proc. icml, vol. 30,\nno. 1, 2013, p. 3.\n[31] O. J. R¨as¨anen, U. K. Laine, and T. Altosaar, “An improved speech\nsegmentation quality measure: the r-value,” in Tenth Annual Con-\nference of the International Speech Communication Association,\n2009.\n[32] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: an asr corpus based on public domain audio books,”\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2015, pp. 5206–5210.\n[33] A. Ben-Shalom, J. Keshet, D. Modan, and A. Laufer, “Automatic\ntools for analyzing spoken hebrew.”\n[34] H. G. Tillmann and B. Pompino-Marschall, “Theoretical princi-\nples concerning segmentation, labelling strategies and levels of\ncategorical annotation for spoken language database systems,” in\nThird European Conference on Speech Communication and Tech-\nnology, 1993.\n",
  "categories": [
    "eess.AS",
    "cs.LG",
    "cs.SD",
    "stat.ML"
  ],
  "published": "2020-07-27",
  "updated": "2020-08-06"
}