{
  "id": "http://arxiv.org/abs/2007.13959v1",
  "title": "On Deep Unsupervised Active Learning",
  "authors": [
    "Changsheng Li",
    "Handong Ma",
    "Zhao Kang",
    "Ye Yuan",
    "Xiao-Yu Zhang",
    "Guoren Wang"
  ],
  "abstract": "Unsupervised active learning has attracted increasing attention in recent\nyears, where its goal is to select representative samples in an unsupervised\nsetting for human annotating. Most existing works are based on shallow linear\nmodels by assuming that each sample can be well approximated by the span (i.e.,\nthe set of all linear combinations) of certain selected samples, and then take\nthese selected samples as representative ones to label. However, in practice,\nthe data do not necessarily conform to linear models, and how to model\nnonlinearity of data often becomes the key point to success. In this paper, we\npresent a novel Deep neural network framework for Unsupervised Active Learning,\ncalled DUAL. DUAL can explicitly learn a nonlinear embedding to map each input\ninto a latent space through an encoder-decoder architecture, and introduce a\nselection block to select representative samples in the the learnt latent\nspace. In the selection block, DUAL considers to simultaneously preserve the\nwhole input patterns as well as the cluster structure of data. Extensive\nexperiments are performed on six publicly available datasets, and experimental\nresults clearly demonstrate the efficacy of our method, compared with\nstate-of-the-arts.",
  "text": "On Deep Unsupervised Active Learning\nChangsheng Li1 , Handong Ma2 , Zhao Kang2 , Ye Yuan1 , Xiao-Yu Zhang3 and Guoren Wang1∗\n1School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China\n2SCSE, University of Electronic Science and Technology of China, Chengdu, China\n3Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\n{lcs, yuan-ye}@bit.edu.cn, 201921080133@std.uestc.edu.cn, zkang@uestc.edu.cn,\nzhangxiaoyu@iie.ac.cn, wanggrbit@126.com\nAbstract\nUnsupervised active learning has attracted increas-\ning attention in recent years, where its goal is to se-\nlect representative samples in an unsupervised set-\nting for human annotating. Most existing works\nare based on shallow linear models by assuming\nthat each sample can be well approximated by the\nspan (i.e., the set of all linear combinations) of cer-\ntain selected samples, and then take these selected\nsamples as representative ones to label. However,\nin practice, the data do not necessarily conform to\nlinear models, and how to model nonlinearity of\ndata often becomes the key point to success. In\nthis paper, we present a novel Deep neural net-\nwork framework for Unsupervised Active Learn-\ning, called DUAL. DUAL can explicitly learn a\nnonlinear embedding to map each input into a latent\nspace through an encoder-decoder architecture, and\nintroduce a selection block to select representative\nsamples in the the learnt latent space. In the selec-\ntion block, DUAL considers to simultaneously pre-\nserve the whole input patterns as well as the cluster\nstructure of data. Extensive experiments are per-\nformed on six publicly available datasets, and ex-\nperimental results clearly demonstrate the efﬁcacy\nof our method, compared with state-of-the-arts.\n1\nIntroduction\nIn many real-world applications, there are lots of available\nunlabeled data whereas labeled data are often difﬁcult to get.\nIt is expensive and time-consuming to manually annotate the\ndata, especially when domain experts must be involved. In\nthis situation, active learning provides a promising way to re-\nduce the cost by automatically selecting the most informa-\ntive or representative samples from an unlabeled data pool\nfor human labeling. In other words, these selected samples\ncan improve the performance of the model (e.g., classiﬁer)\nthe most if they are labeled and used as training data. Due to\nits huge potential, active learning has been successfully ap-\nplied to various tasks such as image classiﬁcation [Joshi et\n∗Guoren Wang is the corresponding author.\nal., 2009], object detection [Vijayanarasimhan and Grauman,\n2014], video recommendation [Cai et al., 2019], etc.\nCurrently, research works on active learning follow two\nlines according to whether supervised information is involved\n[Li et al., 2019]. The ﬁrst line concentrates on how to lever-\nage data structures to select representative samples in an un-\nsupervised manner. Typical algorithms include transductive\nexperimental design (TED) [Yu et al., 2006], locally linear\nreconstruction [Zhang et al., 2011], robust representation and\nstructured sparsity [Nie et al., 2013], joint active learning and\nfeature selection (ALFS) [Li et al., 2019]. The other line con-\nsiders the problems of querying informative samples. Such\nmethods basically need a pre-trained classiﬁer to select sam-\nples, which means that they need some initially labeled data\nfor training. In this line, many approaches have been pro-\nposed in the past decades [Freund et al., 1997; Kapoor et al.,\n2007; Jain and Kapoor, 2009; Huang et al., 2010; Elham-\nifar et al., 2013; Zheng and Ye, 2015; Zhang et al., 2017;\nHaussmann et al., 2019]. Due to the limitation of space, we\nrefer the reader to [Aggarwal et al., 2014] and [Settles, 2009]\nfor more details. In this paper, we focus on unsupervised ac-\ntive learning, since it is a challenging problem because of the\nlack of supervised information.\nMost existing works on unsupervised active learning [Yu\net al., 2006; Nie et al., 2013; Hu et al., 2013; FY et al., 2015;\nShi and Shen, 2016; Li et al., 2017; Li et al., 2019] assume\nthat each data point can be reconstructed by the span, i.e., the\nset of all linear combinations, of a selected sample subset,\nand resort to shallow linear models to minimize the recon-\nstruction error. Such methods often suffer from the follow-\ning limitations: ﬁrst, they attempt to reconstruct the whole\ndataset, but ignore the cluster structure of data. For example,\nlet us consider an extreme case: assume that there are 100\nsamples, among which 99 samples are positive and one sam-\nple is negative. Thus, the negative sample is very important,\nand it should be selected as one of the representative sam-\nples. However, if we only minimize the total reconstruction\nloss of all samples by a selected sample subset, then it is very\nlikely that the negative sample is not selected, because of it\nbeing far from the positive samples. The second limitation\nof these methods is that they use shallow and linear mapping\nfunctions to reveal the intrinsic structure of data, thus may\nfail in handling data with complex (often nonlinear) struc-\ntures. To address this issue, the manifold adaptive experimen-\narXiv:2007.13959v1  [cs.LG]  28 Jul 2020\ntal design (MAED) algorithm [Cai and He, 2011] attempts to\nlearn the representation in a manifold adaptive kernel space\nobtained by incorporating the manifold structure into the re-\nproducing kernel Hilbert space (RKHS). As we know, kernel\nbased methods heavily depend on the choice of kernel func-\ntions, while it is difﬁcult to ﬁnd an optimal or even suitable\nkernel function in real-world applications.\nTo overcome the above limitations, in this paper, we pro-\npose a novel unsupervised active learning framework based\non a deep learning model, called Deep Unsupervised Active\nLearning (DUAL). DUAL takes advantage of an encoder-\ndecoder model to explicitly learn a nonlinear latent space,\nwhere DUAL can perform sample selection by introducing\na selection block at the junction between the encoder and the\ndecoder. In the selection block, we attempt to simultaneously\nreconstruct the whole dataset and the cluster centroids of the\ndata. Since the selection block is differentiable, DUAL is an\nend-to-end trainable framework. To the best of our knowl-\nedge, our approach constitutes the ﬁrst attempt to select rep-\nresentative samples based on a deep neural network in an\nunsupervised setting. Compared with existing unsupervised\nactive learning approaches, our method signiﬁcantly differs\nfrom them in the following aspects:\n• DUAL directly learns a nonlinear representation by a\nmulti-layer neural network, which offers stronger abil-\nity to discover nonlinear structures of data.\n• In contrast to kernel-based approaches, DUAL can\nprovide explicit transformations, avoiding subjectively\nchoosing the kernel function in advance.\n• Not only can DUAL model the whole input patterns by\nthe selected representative samples, but also it can pre-\nserve cluster structures of data well.\nWe extensively evaluate our method on six publicly avail-\nable datasets. The experimental results show our DUAL out-\nperforms the related state-of-the-art methods.\n2\nRelated Work\nAs mentioned above, we focus on the studies on unsupervised\nactive learning. In this section, we brieﬂy review some al-\ngorithms devoted to unsupervised active learning and some\nrelated works to this topic.\n2.1\nUnsupervised Active Learning\nAmong existing unsupervised active learning methods, the\nmost representative one is the transductive experimental de-\nsign (TED) [Yu et al., 2006], where its goal is to select sam-\nples that can best represent the dataset using a linear repre-\nsentation. Thus, TED proposes an objective function as:\nmin\nZ,S\nn\nX\ni=1\n(||xi −Zsi||2\n2 + α||si||2\n2)\ns.t. S =[s1, ..., sn] ∈Rm×n, Z = [z1, ..., zm] ⊂X,\n(1)\nwhere X = [x1, ..., xn] ∈Rd×n denotes the data matrix,\nwhere d is the dimension of samples and n is the number of\nsamples. Z ⊂X denotes the selected sample subset, and S is\nthe reconstruction coefﬁcient matrix. α is a tradeoff param-\neter to control the amount of shrinkage. || · ||2 denotes the\nl2-norm of a vector.\nFollowing TED, more unsupervised active learning meth-\nods have been proposed in recent years.\nInspired by the\nidea of Locally Linear Embedding (LLE) [Roweis and Saul,\n2000], [Zhang et al., 2011] propose to represent each sam-\nple by a linear combination of its neighbors, with the purpose\nof preserving the intrinsic local structure of data. Similarly,\nALNR also incorporates the neighborhood relation into the\nsample selection process, where the nearest neighbors are ex-\npected to have stronger effect on the reconstruction of sam-\nples [Hu et al., 2013].\n[Nie et al., 2013] extend TED to\na convex formulation by introducing a structured sparsity-\ninducing norm, and take advantage of a robust sparse rep-\nresentation loss function to suppress outliers. [Shi and Shen,\n2016] extend convex TED to a diversity version for select-\ning complementary samples. More recently, ALFS [Li et al.,\n2019] study the coupling effect on unsupervised active learn-\ning and feature selection, and perform them jointly via the\nCUR matrix decomposition [Mahoney and Drineas, 2009].\nThe above methods are linear models, which cannot model\nnonlinear structures of data in many read-world scenarios.\nThus, [Cai and He, 2011] propose a kernel-based method\nto perform nonlinear sample selection in a manifold adap-\ntive kernel space. However, how to choose appropriate kernel\nfunctions for kernel-based methods is usually unclear in prac-\ntice.\nUnlike these approaches, our method explicitly learns a\nnonlinear embedding by a deep neural network architecture,\nso that the nonlinear structures of data can be discovered in\nthe latent space, and thus a better representative sample sub-\nset can be obtained.\n2.2\nMatrix Column Subset Selection\nUnsupervised active learning is related to one popular math-\nematical problem: matrix column subset selection (MCSS)\n[Chan, 1987]. The MCSS problem aims to select a subset of\ncolumns from an input matrix, such that the selected columns\ncan capture as much of the input as possible. More precisely,\nit attempts to select m columns of X to form a new matrix\nZ ∈Rd×m that minimizes the following residual:\nmin\nZ,C ||X −ZC||ε = ||X −ZZ†X||ε\nwhere Z† is the Moore-Penrose pseudoinverse of Z. ZZ†\ndenotes the projection onto the m-dimensional space spanned\nby the columns of Z. ε = 2 or F denotes the spectral norm\nor Frobenius norm.\nDifferent from our method, MCSS still attempts to recon-\nstruct the input based on a linear combination of the selected\ncolumns, which cannot model the nonlinearity of data.\n3\nDeep Unsupervised Active Learning\nIn this section, we will elaborate the details of the proposed\nDUAL model for unsupervised active learning. As shown\nin Figure 1, DUAL mainly consists of the following three\nblocks: an encoder block and a decoder block are used to\nlearn a nonlinear representation, and a selection block at the\n漕\n漕\n漕\nInput\nEncoder Block\n漕\n漕\n漕\nx1\nx2\nxn\nSelection Block\nFC\n漕\n漕\n漕\nFC\nxn-1\nLatent Representation \n漕\n漕\n漕\nDecoder Block\nFC\n漕\n漕\n漕\nFC\n漕\n漕\n漕\nOutput\nc1\ncK\nCentroid Matrix \n漕\n漕\n漕\nK-means\nK clusters\n(\n)\n1\nL\ng\n\u0002\n\u0003\n1\n\u0004 x\nReconstruction \n1\n( )\n[\n(\n),\n,\n(\n)]\nn\n\u0004\n\u0005 \u0004\n\u0004\nX\nx\nxn\n\u0002\n\u0003\n2\n\u0004 x\n\u0002\n\u0003\n1\nn\u0006\n\u0004 x\n\u0002\n\u0003\nn\n\u0004 x\nQ\nP\n( )\n\u0004 X Q\n1\n( )\n\u0004 X q\n2\n( )\n\u0004 X q\n1\n( )\nn\u0006\n\u0004 X q\n( )\nn\n\u0004 X q\n1\n[ ,\n,\n]\nK\n\u0005\nC\nc\ncK\n2\n2,1\n( )\nc\nF\nL\n\u0007\n\u0005\n\u0006 \u0004\n\b\nC\nX P\nP\n( )\n2\nL\ng\n( )\n1\nL\nn\u0006\ng\n( )\nL\nn\ng\nFigure 1: Illustration of the overall architecture. DUAL consists of an encoder block, a selection block and a decoder block. In particular, the\nencoder and decoder blocks are used to learn a nonlinear representation. The selection block consists of two branches, where each is made\nup of one fully connected layer without bias and nonlinear activation functions. The top branch attempts to capture as much of the latent\nrepresentation Φ(X) as possible, while the bottom one aims to approximate the K cluster centroids well. The dash line in the fully connected\nlayer of the selection block denotes that the sample in the left side has no contribution to the reconstruction of the corresponding sample in\nthe right side, i.e., its weight in Q or P is equal to zero.\njunction between the encoder and the decoder used for se-\nlecting samples. Before explaining how DUAL is speciﬁcally\ndesigned for these parts, we ﬁrst give our problem setting.\n3.1\nProblem Setting\nLet X = [x1, ..., xn] ∈Rd×n denote a data matrix, where\nd and n are the feature dimension and the number of training\ndata, respectively. Our goal is to learn a nonlinear transforma-\ntion Φ to map input X to a new latent representation Φ(X),\nand then select m samples expected to not only approximate\nthe latent representation Φ(X) well but also preserve the clus-\nter structure of the training data. This problem is quite chal-\nlenging, since solving it exactly is a hard combinatorial opti-\nmization problem (NP-hard). After obtaining a sample sub-\nset based on our method DUAL, a prediction model (e.g., a\nSVM classiﬁer for classiﬁcation ) will be trained by labeling\nthe selected m samples and using Φ(·) as their new feature\nrepresentations.\n3.2\nEncoder Block\nIn order to learn the nonlinear mapping Φ, we utilize a deep\nneural network to map each input to a latent space. Since\nwe have no access to the labeled data, we adopt an encoder-\ndecoder architecture because of its effectiveness for unsuper-\nvised learning. Our encoder block consists of L + 1 layers\nfor performing L nonlinear transformations as the desired Φ\n(The detail of the decoder block is introduced in Section 3.4).\nFor easy of presentation, we ﬁrst provide the deﬁnition of the\noutput of each layer in the encoder block:\nh(l)\ni\n= σ(W(l)\ne h(l−1)\ni\n+ b(l)\ne ), l = 1, . . . , L,\n(2)\nwhere h(0)\ni\n= xi, i = 1, · · · , n, denotes the original training\ndata X as the input of the encoder block. W(l)\ne\nand b(l)\ne\nare\nthe weights and bias associated with the l-th hidden layer,\nrespectively. σ(·) is a nonlinear activation function. Then,\nwe can deﬁne our latent representation Φ(X) as:\nΦ(X) = H(L) = [h(L)\n1\n, · · · , h(L)\nn ] ∈Rd′×n\n(3)\nwhere d′ denotes the dimension of our latent representation.\n3.3\nSelection Block\nAs discussed above, we aim to seek a sample subset which\ncan better capture the whole input patterns and simultane-\nously preserve the cluster structure of data. To this end, we\nintroduce a selection block at the junction between the en-\ncoder and the decoder, as shown in Figure 1. In Figure 1,\nthe selection block consists of two branches, of which each is\ncomposed of one fully connected layer but without bias and\nnonlinear activation functions. The top branch is used to se-\nlect a sample subset to approximate all samples in the latent\nspace. The bottom one aims to reconstruct the cluster cen-\ntroids, such that the cluster structure can be well preserved.\nNext, we will introduce the two branches in detail.\nTop branch: In order to best approximate all samples, we\npresent to minimize the following loss function:\nLa =\nn\nX\ni=1\n(||Φ(xi) −Φ(Y)qi||2\nℓ+ γ||qi||1)\n(4)\ns.t. Φ(Y) = [Φ(y1), . . . , Φ(ym)] ∈Rd′×m\nwhere Φ(Y) ⊂Φ(X) denotes the selected m samples, and\nqi ∈Rm is the reconstruction coefﬁcients for sample Φ(xi).\n||·||1 denotes the l1-norm of a vector. ||·||ℓdenotes the ℓ-norm\nof a vector, indicating certain loss measuring strategy. In this\npaper, we use a common norm, the l2-norm, for simplicity.\nγ ≥0 is a tradeoff parameter.\nThe ﬁrst term in Eq. (4) aims to pick out m samples to\nreconstruct the whole dataset in the latent space, while the\nsecond term is a regularization term to enforce the coefﬁcient\nsparse. Unfortunately, there is not an easy solution to (4), as it\nis a combinatorial optimization. Inspired by [Li et al., 2019],\nwe relax (4) to a convex optimization problem, and write it\ninto a matrix format as\nmin\nQ∈Rn×n La = ||Φ(X) −Φ(X)Q||2\nF + γ||Q||2,1\ns.t. ||diag(Q)||1 = 0\n(5)\nwhere || · ||2,1 denotes the l2,1-norm of a matrix, deﬁned as\nsum of the l2-norms of row vectors. The constraint condition\nensures the diagonal elements of Q equal to zeros, avoiding\nthe degenerated solution. To minimize (5), we utilize the fact\nthat, Q = [q1, . . . , qn] can be thought of the parameters of a\nfully connected layer without bias and nonlinear activations,\nsuch that Φ and Q can be solved jointly through a standard\nbackpropagation procedure.\nBottom branch: In the top branch, the selected samples\ncan approximate the whole dataset well, while it might fail to\npreserve the cluster structure of data. To solve this problem,\nwe ﬁrst utilize a clustering algorithm to cluster the data into\nsome clusters in the latent space, and then select a sample\nsubset to best approximate the obtained cluster centroids.\nSpeciﬁcally, given the latent representation Φ(X), we can\nobtain K clusters based on a clustering algorithm, K-means\nused in this paper. Then, we denote the cluster centroid ma-\ntrix C as\nC = [c1, c2, . . . , cK] ∈Rd′×K\n(6)\nwhere ck denotes the k-th cluster centroid, and K is the num-\nber of clusters.\nIn order to preserve the cluster structure, we minimize the\nfollowing loss function:\nLc = ||C −Φ(X)P||2\nF + η||P||2,1\n(7)\nwhere P is the coefﬁcient matrix for reconstructing C. η ≥0\nis a tradeoff parameter. Similarly, P can also be thought of\nthe parameters of a fully connected layer without bias and\nnonlinear activation functions, and thus can be optimized\njointly with Φ.\n3.4\nDecoder Block\nAs aforementioned, since we have no access to the label infor-\nmation, we attempt to recover each input by a decoder block,\nto guide the learning of the latent representation. In other\nwords, each input actually plays the role of a supervisor. Sim-\nilar to the encoder block, our decoder block also consists of\nL+1 layers for performing L nonlinear transformations. The\noutput of each layer in the decoder block can be deﬁned as:\ng(l)\ni\n= σ(W(l)\nd g(l−1)\ni\n+ b(l)\nd ), l = 1, . . . , L,\n(8)\nwhere g(0)\ni\n= Φ(X)qi means that the output of the selection\nblock is used as the input of the decoder block. W(l)\nd and b(l)\nd\nare the weights and bias associated with the l-th hidden layer\nin the decoder block, respectively.\nThen, the reconstruction loss function is deﬁned as:\nLr =\nn\nX\ni=1\n||xi −g(L)\ni\n||2\n2 = ||X −G(L)||2\nF ,\n(9)\nwhere g(L)\ni\ndenotes the output of the decoder to the input xi,\nand G(L) is expressed by G(L) = [g(L)\n1\n, g(L)\n2\n, . . . , g(L)\nn ].\n3.5\nOverall Model and Training\nAfter introducing all the building blocks of this work, we now\ngive the ﬁnal training objective and explain how to jointly\noptimize it. Based on the Eq. (5), (7), and (9), the ﬁnal loss\nfunction is deﬁned as:\nmin L = Lr + αLa + βLc\n(10)\nwhere α and β are two positive tradeoff parameters.\nIn (10), there are three reconstruction loss terms. The ﬁrst\nterm denotes the reconstruction loss of the encoder-decoder\nmodel in Eq. (9). The second term corresponds to input pat-\nterns reconstruction loss in Eq. (5). The last term is the clus-\nter centroids reconstruction loss as shown in Eq. (7).\nTo solve (10), we present a three-stage training strategy\nwhich is an end-to-end trainable fashion. Firstly, we pre-train\nthe encoder and decoder block in the beginning without con-\nsidering the selection block. After that, we utilize the output\nof the encoder block as the latent representation to perform\nK-means, and regard the obtained K cluster centroids as the\ncentroid matrix C for subsequent sample selection. Lastly,\nwe use the pre-trained parameters to initialize the encoder and\ndecoder blocks, and load all data into a batch to optimize the\nwhole network, i.e., minimizing the loss (10). Throughout\nthe experiment, we use three fully connected layers in the en-\ncoder and decoder blocks, respectively. The rectiﬁed linear\nunit (ReLU) is used as the non-linear activation function. In\naddition, we use Adam [Kingma and Ba, 2014] as the opti-\nmizer, where the learning rate is set to 1.0 × 10−4.\nOnce the model is trained, we can obtain two reconstruc-\ntion coefﬁcient matrices Q and P. We then use a simple strat-\negy to select the most representative samples based on Q and\nP. Speciﬁcally, we calculate the l2-norm of the rows of Q\nand P respectively, and obtain two corresponding vectors ˆq\nand ˆp. After that, we normalize ˆq and ˆp to the range of [0,1].\nFinally, we can sort all the samples by adding normalized ˆq\nto normalized ˆp in descending order, and select the top m\nsamples as the most representative ones.\n4\nExperiments\nTo verify the effectiveness of our method DUAL, we perform\nthe experiments on six publicly available datasets which are\nwidely used for active learning [Baram et al., 2004]. The\ndetails of these datasets are summarized in Table 1 1.\n1 These datasets are downloaded from the UCI Machine Learn-\ning Repository\n0\n10\n20\n30\n40\n50\n60\n70\n80\n#Query\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAccuracy\nDCS\nKMeans\nRRSS\nTED\nMAED\nALNR\nALFS\nDUAL\n(a) Urban\n0\n100\n200\n300\n400\n500\n#Query\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nAccuracy\nDCS\nKMeans\nRRSS\nTED\nMAED\nALNR\nALFS\nDUAL\n(b) MASS\n0\n100\n200\n300\n400\n500\n600\n700\n800\n#Query\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nDCS\nKMeans\nRRSS\nTED\nMAED\nALNR\nALFS\nDUAL\n(c) Plant Species Leaves\n0\n200\n400\n600\n800\n1000\n#Query\n0.15\n0.2\n0.25\n0.3\n0.35\nAccuracy\nDCS\nKMeans\nRRSS\nTED\nMAED\nALNR\nALFS\nDUAL\n(d) sEMG\n0\n500\n1000\n1500\n2000\n2500\n#Query\n0.68\n0.7\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\nAccuracy\nDCS\nKMeans\nTED\nMAED\nALNR\nALFS\nDUAL\n(e) Waveform\n0\n1000\n2000\n3000\n4000\n5000\n#Query\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\nAccuracy\nDCS\nKMeans\nTED\nMAED\nALNR\nALFS\nDUAL\n(f) Gesture Phase Segment\nFigure 2: Comparisons of different active learning methods in terms of accuracy on six benchmark datasets.\nDataset\nSize\nDimension\nClass\nUrban Land Cover\n168\n148\n9\nWaveform\n5000\n40\n3\nsEMG\n1800\n3000\n6\nPlant Species Leaves\n1600\n64\n100\nGesture Phase Segment\n9873\n50\n5\nMammographic Mass\n961\n6\n2\nTable 1: Summary of Experimental Datasets.\n4.1\nExperimental Setting\nCompared methods2: We compare DUAL with several typ-\nical unsupervised active learning algorithms, including TED\n[Yu et al., 2006], RRSS [Nie et al., 2013], ALNR [Hu et al.,\n2013], MAED [Cai and He, 2011], ALFS [Li et al., 2019].\nWe also compare Deterministic Column Sampling(DCS) [Pa-\npailiopoulos et al., 2014] in our experiments. In addition, we\nalso take K-means as a baseline.\nExperimental protocol: Following [Li et al., 2019], for\neach dataset, we randomly select 50% of the samples as can-\ndidates for sample selection, and use the rest as the testing\ndata. To evaluate the effectiveness of sample selection, we\ntrain a SVM classiﬁer with a linear kernel and C = 100 by\nusing these selected samples as the training data. We set the\nparameters γ = η for simplicity, and search all tradeoff pa-\nrameters in our algorithm from {0.01, 0.1, 1, 10}. The num-\n2All source codes are obtained from the authors of the corre-\nsponding papers, except K-means and ALNR.\nber of clusters K is searched from {5, 10, 20, 50}. We use\naccuracy and AUC to measure the performance. We repeat\nevery test case ﬁve times, and report the average result.\n4.2\nExperimental Result\nGeneral Performance: Figure 2 and 3 show the scores in\nterms of different numbers of queries. Our method outper-\nforms other algorithms under most of cases, especially on the\nlarger datasets. This illustrates that by preserving input pat-\nterns and cluster structures, DUAL can select representative\nsamples. In addition, we observe that MAED and DCS have\ngood results on some datasets. This may be because MAED\nis a nonlinear method that can handle complex data, while\nDCS selects a subset with largest leverage scores, resulting in\na good low-rank matrix surrogate.\nAblation Study: We study the effectiveness of the com-\nponents in our selection block on the Urban and Waveform\ndatasets. The experimental setting is as follows: we only con-\nsider to preserve the whole input patterns, i.e., setting β = 0\nin (10). We call it DUALw/o. The number of queries is set\nto half of the candidates. Figure 4 shows the results. DUAL\nachieves better results than DUALw/o, which indicates that\npreserving cluster structures is helpful for unsupervised ac-\ntive learning. On the Waveform dataset, the improvement of\nDUAL over DUALw/o is a little bit light. This is because\nthe numbers of samples among different classes are balanced\nin this dataset, and thus selecting samples by only modeling\ninput patterns may preserve the cluster structure well.\nVisualization: In this subsection, we apply our method\nDUAL on the Waveform dataset to give an intuitive result.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n#Query\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nAUC\nDCS\nKMeans\nRRSS\nTED\nMAED\nALNR\nALFS\nDUAL\n(a) Urban\n0\n100\n200\n300\n400\n500\n#Query\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nAUC\nDCS\nKMeans\nRRSS\nTED\nMAED\nALNR\nALFS\nDUAL\n(b) MASS\n0\n100\n200\n300\n400\n500\n600\n700\n800\n#Query\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nAUC\nDCS\nKMeans\nRRSS\nTED\nMAED\nALNR\nALFS\nDUAL\n(c) Plant Species Leaves\n0\n200\n400\n600\n800\n1000\n#Query\n0.48\n0.5\n0.52\n0.54\n0.56\n0.58\n0.6\nAUC\nDCS\nKMeans\nRRSS\nTED\nMAED\nALNR\nALFS\nDUAL\n(d) sEMG\n0\n500\n1000\n1500\n2000\n2500\n#Query\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\nAUC\nDCS\nKMeans\nTED\nMAED\nALNR\nALFS\nDUAL\n(e) Waveform\n0\n1000\n2000\n3000\n4000\n5000\n#Query\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nAUC\nDCS\nKMeans\nTED\nMAED\nALNR\nALFS\nDUAL\n(f) Gesture Phase Segment\nFigure 3: Comparisons of different active learning methods in terms of AUC on six benchmark datasets.\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nAUC\nDUALw/o DUAL\nDUAL\nDUALw/o\n(a) Urban\n0.845\n0.855\n0.865\n0.875\n0.885\n0.895\nDUALw/o DUAL\nDUALw/o DUAL\nAccuracy\nAUC\n(b) Waveform\nFigure 4: The effectiveness veriﬁcation of the components in the\nselection block.\nFigure 5: The visualization by t-SNE. The red circles denote the se-\nlected samples, and other color solid circles denote different classes.\n(a) \u0002\n0.01\n0.1\n1\n10\nAccuracy\n0\n0.5\n1\n(b) \u0003\n0.01\n0.1\n1\n10\nAccuracy\n0\n0.5\n1\n(c) \u0004\n0.01\n0.1\n1\n10\nAccuracy\n0\n0.5\n1\n(d) K\n5\n10\n20\n50\nAccuracy\n0\n0.5\n1\nFigure 6: Parameter Study on the Urban dataset.\nWe use the t-SNE [Maaten and Hinton, 2008] to visualize the\nsamples selected by our method, as shown in Figure 5. The\nsamples selected by DUAL can better represent the dataset.\nThis is because DUAL can better capture the nonlinear struc-\nture of data by performing active learning in the latent space.\nParameter Study: We study the sensitivity of our algo-\nrithm about the tradeoff parameters α, β, γ(= η), and the\nnumber of clusters K on the Urban dataset. We ﬁx the num-\nber of queries to half of the candidates, and report their accu-\nracies. The results are shown in Figure 6. Our method is not\nsensitive to the parameters with a relatively wide range.\n5\nConclusion\nIn this paper, we proposed a deep learning based framework\nfor unsupervised active learning, called DUAL. DUAL can\nmodel the nonlinear structure of data, and select represen-\ntative samples to preserve the input pattern and the clus-\nter structure of data. Extensive experimental results on six\ndatasets demonstrate the effectiveness of our method.\nAcknowledgements\nThis work was supported by the NSFC Grant No. 61806044,\n61932004, N181605012, and 61732003.\nReferences\n[Aggarwal et al., 2014] Charu C Aggarwal, Xiangnan Kong,\nQuanquan Gu, Jiawei Han, and S Yu Philip. Active learn-\ning: A survey.\nIn Data Classiﬁcation, pages 599–634.\nChapman and Hall/CRC, 2014.\n[Baram et al., 2004] Yoram Baram, Ran El Yaniv, and Kobi\nLuz. Online choice of active learning algorithms. JMLR,\n5(3):255–291, 2004.\n[Cai and He, 2011] Deng Cai and Xiaofei He.\nMani-\nfold adaptive experimental design for text categorization.\nTKDE, 24(4):707–719, 2011.\n[Cai et al., 2019] Jia-Jia Cai, Jun Tang, Qing-Guo Chen, Yao\nHu, Xiaobo Wang, and Sheng-Jun Huang. Multi-view ac-\ntive learning for video recommendation. In IJCAI, pages\n2053–2059. AAAI Press, 2019.\n[Chan, 1987] Tony F Chan. Rank revealing qr factorizations.\nLinear algebra and its applications, 88:67–82, 1987.\n[Elhamifar et al., 2013] Ehsan Elhamifar, Guillermo Sapiro,\nAllen Yang, and S. Shankar Sasrty. A convex optimization\nframework for active learning. In ICCV, pages 209–216,\n2013.\n[Freund et al., 1997] Yoav Freund, H. Sebastian Seung, Eli\nShamir, and Naftali Tishby. Selective sampling using the\nquery by committee algorithm. Machine Learning, 28(2-\n3):133–168, 1997.\n[FY et al., 2015] Zhu FY, Xingliang Zhu, Xiang SM, Pan\nCH, et al. 10,000+ times accelerated robust subset selec-\ntion (arss). In AAAI, pages 3217–3223, 2015.\n[Haussmann et al., 2019] Manuel Haussmann, Fred Ham-\nprecht, and Melih Kandemir. Deep active learning with\nadaptive acquisition. In IJCAI, pages 2470–2476, 2019.\n[Hu et al., 2013] Yao Hu, Debing Zhang, Zhongming Jin,\nDeng Cai, and Xiaofei He. Active learning via neighbor-\nhood reconstruction. In IJCAI, pages 1415–1421, 2013.\n[Huang et al., 2010] ShengJun Huang, Rong Jin, and Zhi-\nHua Zhou. Active learning by querying informative and\nrepresentative examples. In NIPS, pages 892–900, 2010.\n[Jain and Kapoor, 2009] Prateek Jain and Ashish Kapoor.\nActive learning for large multi-class problems. In CVPR,\npages 762–769, 2009.\n[Joshi et al., 2009] Ajay J Joshi, Fatih Porikli, and Nikolaos\nPapanikolopoulos. Multi-class active learning for image\nclassiﬁcation. In CVPR, pages 2372–2379, 2009.\n[Kapoor et al., 2007] Ashish Kapoor,\nEric Horvitz,\nand\nSumit Basu. Selective supervision: Guiding supervised\nlearning with decision-theoretic active learning. In IJCAI,\nvolume 7, pages 877–882, 2007.\n[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba.\nAdam:\nA method for stochastic optimization.\narXiv\npreprint arXiv:1412.6980, 2014.\n[Li et al., 2017] Qin Li,\nXiaoshuang Shi,\nLinfei Zhou,\nZhifeng Bao, and Zhenhua Guo. Active learning via lo-\ncal structure reconstruction. PRL, 92:81–88, 2017.\n[Li et al., 2019] Changsheng Li, Xiangfeng Wang, Weishan\nDong, Junchi Yan, Qingshan Liu, and Hongyuan Zha.\nJoint active learning with feature selection via cur matrix\ndecomposition. TPAMI, 41(6):1382–1396, 2019.\n[Maaten and Hinton, 2008] Laurens van der Maaten and Ge-\noffrey Hinton.\nVisualizing data using tsne.\nJMLR,\n9(11):2579–2605, 2008.\n[Mahoney and Drineas, 2009] Michael W Mahoney and Pet-\nros Drineas. Cur matrix decompositions for improved data\nanalysis. PNAS, 106(3):697–702, 2009.\n[Nie et al., 2013] Feiping Nie, Wang Hua, Heng Huang, and\nChris Ding. Early active learning via robust representation\nand structured sparsity. In IJCAI, pages 1572–1578, 2013.\n[Papailiopoulos et al., 2014] Dimitris Papailiopoulos, Anas-\ntasios Kyrillidis, and Christos Boutsidis. Provable deter-\nministic leverage score sampling.\nIn KDD, pages 997–\n1006, 2014.\n[Roweis and Saul, 2000] Sam T Roweis and Lawrence K\nSaul. Nonlinear dimensionality reduction by locally lin-\near embedding. science, 290(5500):2323–2326, 2000.\n[Settles, 2009] Burr Settles. Active learning literature sur-\nvey. Technical report, University of Wisconsin-Madison\nDepartment of Computer Sciences, 2009.\n[Shi and Shen, 2016] Lei Shi and Yi-Dong Shen. Diversi-\nfying convex transductive experimental design for active\nlearning. In IJCAI, pages 1997–2003. AAAI Press, 2016.\n[Vijayanarasimhan and Grauman, 2014] Sudheendra\nVi-\njayanarasimhan and Kristen Grauman.\nLarge-scale live\nactive learning: Training object detectors with crawled\ndata and crowds. IJCV, 108(1-2):97–114, 2014.\n[Yu et al., 2006] Kai Yu, Jinbo Bi, and Volker Tresp. Active\nlearning via transductive experimental design. In ICML,\npages 1081–1088, 2006.\n[Zhang et al., 2011] Lijun Zhang, Chun Chen, Jiajun Bu,\nDeng Cai, Xiaofei He, and Thomas S Huang.\nActive\nlearning based on locally linear reconstruction. TPAMI,\n33(10):2026–2038, 2011.\n[Zhang et al., 2017] XiaoYu Zhang, Shupeng Wang, and Xi-\naochun Yun.\nBidirectional active learning: A two-way\nexploration into unlabeled and labeled data set. TNNLS,\n26(12):3034–3044, 2017.\n[Zheng and Ye, 2015] Wang Zheng and Jieping Ye. Query-\ning discriminative and representative samples for batch\nmode active learning. ACM Transactions on Knowledge\nDiscovery from Data, 9(3):1–23, 2015.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-07-28",
  "updated": "2020-07-28"
}