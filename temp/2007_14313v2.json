{
  "id": "http://arxiv.org/abs/2007.14313v2",
  "title": "Deep frequency principle towards understanding why deeper learning is faster",
  "authors": [
    "Zhi-Qin John Xu",
    "Hanxu Zhou"
  ],
  "abstract": "Understanding the effect of depth in deep learning is a critical problem. In\nthis work, we utilize the Fourier analysis to empirically provide a promising\nmechanism to understand why feedforward deeper learning is faster. To this end,\nwe separate a deep neural network, trained by normal stochastic gradient\ndescent, into two parts during analysis, i.e., a pre-condition component and a\nlearning component, in which the output of the pre-condition one is the input\nof the learning one. We use a filtering method to characterize the frequency\ndistribution of a high-dimensional function. Based on experiments of deep\nnetworks and real dataset, we propose a deep frequency principle, that is, the\neffective target function for a deeper hidden layer biases towards lower\nfrequency during the training. Therefore, the learning component effectively\nlearns a lower frequency function if the pre-condition component has more\nlayers. Due to the well-studied frequency principle, i.e., deep neural networks\nlearn lower frequency functions faster, the deep frequency principle provides a\nreasonable explanation to why deeper learning is faster. We believe these\nempirical studies would be valuable for future theoretical studies of the\neffect of depth in deep learning.",
  "text": "Deep frequency principle towards understanding why deeper\nlearning is faster\nZhi-Qin John Xua* and Hanxu Zhoua\naSchool of Mathematical Sciences, MOE-LSC and Institute of Natural Sciences, Shanghai Jiao Tong\nUniversity, Shanghai, 200240, P.R. China\nDecember 22, 2020\nAbstract\nUnderstanding the eﬀect of depth in deep learning is a critical problem. In this work, we\nutilize the Fourier analysis to empirically provide a promising mechanism to understand why\nfeedforward deeper learning is faster. To this end, we separate a deep neural network, trained\nby normal stochastic gradient descent, into two parts during analysis, i.e., a pre-condition\ncomponent and a learning component, in which the output of the pre-condition one is the\ninput of the learning one. We use a ﬁltering method to characterize the frequency distribution\nof a high-dimensional function. Based on experiments of deep networks and real dataset, we\npropose a deep frequency principle, that is, the eﬀective target function for a deeper hidden\nlayer biases towards lower frequency during the training. Therefore, the learning component\neﬀectively learns a lower frequency function if the pre-condition component has more layers.\nDue to the well-studied frequency principle, i.e., deep neural networks learn lower frequency\nfunctions faster, the deep frequency principle provides a reasonable explanation to why deeper\nlearning is faster. We believe these empirical studies would be valuable for future theoretical\nstudies of the eﬀect of depth in deep learning.\n1\nIntroduction\nDeep neural networks have achieved tremendous success in many applications, such as computer\nvision, speech recognition, speech translation, and natural language processing etc. The depth in\nneural networks plays an important role in the applications. Understanding the eﬀect of depth is\na central problem to reveal the “black box” of deep learning. For example, empirical studies show\nthat a deeper network can learn faster and generalize better in both real data and synthetic data\n[13, 1]. Diﬀerent network structures have diﬀerent computation costs in each training epoch. In\nthis work, we deﬁne that the learning of a deep neural network is faster if the loss of the deep neural\nnetwork decreases to a designated error with fewer training epochs. For example, as shown in Fig. 1\n(a), when learning data sampled from a target function cos(3x) + cos(5x), a deep neural network\nwith more hidden layers achieves the designated training loss with fewer training epochs. Although\nempirical studies suggest deeper neural networks may learn faster, there is few understanding of\nthe mechanism.\n* xuzhiqin@sjtu.edu.cn. To Appear in AAAI-2021.\n1\narXiv:2007.14313v2  [cs.LG]  20 Dec 2020\n(a) diﬀerent networks\n(b) diﬀerent target functions\nFigure 1: Training epochs (indicated by ordinate axis) of diﬀerent deep neural networks when they\nachieve a ﬁxed error. (a) Using networks with diﬀerent number of hidden layers with the same size\nto learn data sampled from a target function cos(3x) + cos(5x). (b) Using a ﬁxed network to learn\ndata sampled from diﬀerent target functions.\nIn this work, we would empirically explore an underlying mechanism that may explain why\ndeeper neural network (note: in this work, we only study feedforward networks) can learn faster\nfrom the perspective of Fourier analysis.\nWe start from a universal phenomenon of frequency\nprinciple [31, 24, 32, 20, 9], that is, deep neural networks often ﬁt target functions from low to\nhigh frequencies during the training. Recent works show that frequency principle may provide an\nunderstanding to the success and failure of deep learning [32, 35, 9, 22]. We use an ideal example to\nillustrate the frequency principle, i.e., using a deep neural network to ﬁt diﬀerent target functions.\nAs the frequency of the target function decreases, the deep neural network achieves a designated\nerror with fewer training epochs, which is similar to the phenomenon when using a deeper network\nto learn a ﬁxed target function.\nInspired by the above analysis, we propose a mechanism to understand why a deeper network,\nfθ(x), faster learns a set of training data, S = {(xi, yi)}n\ni=1 sampled from a target function f ∗(x),\nillustrated as follows. Networks are trained as usual while we separate a deep neural network into two\nparts in the analysis, as shown in Fig. 2, one is a pre-condition component and the other is a learning\ncomponent, in which the output of the pre-condition one, denoted as f [l−1]\nθ\n(x) (ﬁrst l −1 layers\nare classiﬁed as the pre-condition component), is the input of the learning one. For the learning\ncomponent, the eﬀective training data at each training epoch is S[l−1] = {(f [l−1]\nθ\n(xi), yi)}n\ni=1. We\nthen perform experiments based on the variants of Resnet18 structure [13] and CIFAR10 dataset.\nWe ﬁx the learning component (fully-connected layers). When increasing the number of the pre-\ncondition layer (convolution layers), we ﬁnd that S[l−1] has a stronger bias towards low frequency\nduring the training. By frequency principle, the learning of a lower frequency function is faster,\ntherefore, the learning component is faster to learn S[l−1] when the pre-condition component has\nmore layers. The analysis among diﬀerent network structures is often much more diﬃcult than the\nanalysis of one single structure. For providing hints for future theoretical study, we study a ﬁxed\nfully-connected deep neural network by classifying diﬀerent number of layers into the pre-condition\n2\ncomponent, i.e., varying l for a network in the analysis. As l increases, we similarly ﬁnd that S[l]\ncontains more low frequency and less high frequency during the training. Therefore, we propose\nthe following principle:\nDeep frequency principle: The eﬀective target function for a deeper hidden layer biases to-\nwards lower frequency during the training.\nWith the well-studied frequency principle, the deep frequency principle shows a promising mech-\nanism for understanding why a deeper network learns faster.\nFigure 2: General deep neural network.\n1.1\nRelated work\nFrom the perspective of approximation, the expressive power of a deep neural network increases\nwith the depth [28, 11, 10]. However, the approximation theory renders no implication on the\noptimization of deep neural networks.\nWith residual connection, [13] successfully train very deep networks and ﬁnd that deeper net-\nworks can achieve better generalization error. In addition, [13] also show that the training of deeper\nnetwork is faster. [1] show that the acceleration eﬀect of depth also exists in deep linear neural\nnetwork and provide a viewpoint for understanding the eﬀect of depth, that is, increasing depth\n3\ncan be seen as an acceleration procedure that combines momentum with adaptive learning rates.\nThere are also many works studying the eﬀect of depth for deep linear networks [26, 17, 12, 27]. In\nthis work, we study the optimization eﬀect of depth in non-linear deep networks.\nVarious studies suggest that the function learned by the deep neural networks increases its\ncomplexity as the training goes [3, 29, 23, 16, 34]. This increasing complexity is also found in deep\nlinear network [12]. The high-dimensional experiments in [32] show that the low-frequency part\nis converged ﬁrst, i.e., frequency principle. Therefore, the ratio of the power of the low-frequency\ncomponent of the deep neural network output experiences a increasing stage at the beginning (due\nto the convergence of low-frequency part), followed by a decreasing stage (due to the convergence of\nhigh-frequency part). As more high-frequency involved, the complexity of the deep neural network\noutput increases. Therefore, the ratio of the low-frequency component used in this paper validates\nthe complexity increasing during the training, which is consistent with other studies.\nFrequency principle is examined in extensive datasets and deep neural networks [31, 24, 32].\nTheoretical studies subsequently shows that frequency principle holds in general setting with inﬁnite\nsamples [20] and in the regime of wide neural networks (Neural Tangent Kernel (NTK) regime [14])\nwith ﬁnite samples [35] or suﬃcient many samples [8, 34, 25, 6]. [9] show that the integral equation\nwould naturally leads to the frequency principle. With the theoretical understanding, the frequency\nprinciple inspires the design of deep neural networks to fast learn a function with high frequency\n[19, 30, 15, 7, 5, 18].\n2\nPreliminary\n2.1\nLow Frequency Ratio (LFR)\nTo compare two 1-d functions in the frequency domain, we can display their spectrum. However, this\ndoes not apply for high-dimensional functions, because the computation cost of high-dimensional\nFourier transform suﬀers from the curse of dimensionality. To overcome this, we use a low-frequency\nﬁlter to derive a low-frequency component of the interested function and then use a Low Frequency\nRatio (LFR) to characterize the power ratio of the low-frequency component over the whole spec-\ntrum.\nThe LFR is deﬁned as follows. We ﬁrst split the frequency domain into two parts, i.e., a low-\nfrequency part with frequency |k| ≤k0 and a high-frequency part with |k| > k0, where | · | is the\nlength of a vector. Consider a dataset {(xi, yi)}n\ni=1, xi ∈Rd, and yi ∈Rdo. For example, d = 784\nand do = 10 for MNIST and d = 3072 and do = 10 for CIFAR10. The LFR is deﬁned as\nLFR(k0) =\nP\nk 1|k|≤k0|ˆy(k)|2\nP\nk |ˆy(k)|2\n,\n(1)\nwhere ˆ· indicates Fourier transform, 1k≤k0 is an indicator function, i.e.,\n1|k|≤k0 =\n(\n1,\n|k| ≤k0,\n0,\n|k| > k0.\nHowever, it is almost impossible to compute above quantities numerically due to high computa-\ntional cost of high-dimensional Fourier transform. Similarly as previous study [32], We alternatively\nuse the Fourier transform of a Gaussian function ˆGδ(k), where δ is the variance of the Gaussian\n4\nfunction G, to approximate 1|k|>k0. Note that 1/δ can be interpreted as the variance of ˆG. The\napproximation is reasonable due to the following two reasons. First, the Fourier transform of a\nGaussian is still a Gaussian, i.e., ˆGδ(k) decays exponentially as |k| increases, therefore, it can ap-\nproximate 1|k|≤k0 by ˆGδ(k) with a proper δ(k0). Second, the computation of LFR contains the\nmultiplication of Fourier transforms in the frequency domain, which is equivalent to the Fourier\ntransform of a convolution in the spatial domain. We can equivalently perform the computation in\nthe spatial domain so as to avoid the almost impossible high-dimensional Fourier transform. The\nlow frequency part can be derived by\nylow,δ(k0)\ni\n≜(y ∗Gδ(k0))i,\n(2)\nwhere ∗indicates convolution operator. Then, we can compute the LFR by\nLFR(k0) =\nP\ni |ylow,δ(k0)\ni\n|2\nP\ni |yi|2\n.\n(3)\nThe low frequency part can be derived on the discrete data points by\nylow,δ\ni\n= 1\nCi\nn−1\nX\nj=0\nyjGδ(xi −xj),\n(4)\nwhere Ci = Pn−1\nj=0 Gδ(xi −xj) is a normalization factor and\nGδ(xi −xj) = exp\n\u0000−|xi −xj|2/2δ\n\u0001\n.\n(5)\n1/δ is the variance of ˆG, therefore, it can be interpreted as the frequency width outside which is\nﬁltered out by convolution.\n2.2\nRatio Density Function (RDF)\nLFR(k0) characterizes the power ratio of frequencies within a sphere of radius k0. To characterize\neach frequency in the radius direction, similarly to probability, we deﬁne the ratio density function\n(RDF) as\nRDF(k0) = ∂LFR(k0)\n∂k0\n.\n(6)\nIn practical computation, we use 1/δ for k0 and use the linear slope between two consecutive points\nfor the derivative. For illustration, we show the LFR and RDF for sin(kπx) in Fig. 3. As shown\nin Fig. 3(a), the LFR of low-frequency function faster approaches one when the ﬁlter width in\nthe frequency domain is small, i.e., small 1/δ.\nThe RDF in Fig.\n3(b) shows that as k in the\ntarget function increases, the peak of RDF moves towards wider ﬁlter width, i.e., higher frequency.\nTherefore, it is more intuitive that the RDF eﬀectively reﬂects where the power of the function\nconcentrates in the frequency domain. In the following, we will use RDF to study the frequency\ndistribution of eﬀective target functions for hidden layers.\n5\n0\n500\n1000\n1500\n2000\n2500\n1/\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLFR\nk=1\nk=5\nk=9\nk=13\n(a)\n0\n500\n1000\n1500\n2000\n2500\n1/\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRDF\nk=1\nk=5\nk=9\nk=13\n(b)\nFigure 3: LFR and RDF for sin(kπx) vs. 1/δ. Note that we normalize RDF in (b) by the maximal\nvalue of each curve for visualization.\n2.3\nGeneral deep neural network\nWe adopt the suggested standard notation in [4]. An L-layer neural network is deﬁned recursively,\nf [0]\nθ (x) = x,\n(7)\nf [l]\nθ (x) = σ ◦(W [l−1]f [l−1]\nθ\n(x) + b[l−1])\n1 ≤l ≤L −1,\n(8)\nfθ(x) = f [L]\nθ (x) = W [L−1]f [L−1]\nθ\n(x) + b[L−1],\n(9)\nwhere W [l] ∈Rml+1×ml, b[l] = Rml+1, m0 = din = d, mL = do, σ is a scalar function and “◦” means\nentry-wise operation. We denote the set of parameters by θ. For simplicity, we also denote\nf [−l]\nθ\n(x) = f [L−l+1]\nθ\n(x).\n(10)\nFor example, the output layer is layer “−1”, i.e., f [−1]\nθ\n(x) for a given input x, and the last hidden\nlayer is layer “−2”, i.e., f [−2]\nθ\n(x) for a given input x, illustrated in Fig. 2.\nThe eﬀective target function for the learning component, consisting from layer “l” to the output\nlayer, is\nS[l−1] = {(f [l−1]\nθ\n(xi), yi)}n\ni=1.\n(11)\n2.4\nTraining details\nWe list training details for experiments as follows.\nFor the experiments of the variants of Resnet18 on CIFAR10, the network structures are shown\nin Fig. 4. The output layer is equipped with softmax and the network is trained by Adam optimizer\nwith cross-entropy loss and batch size 256. The learning rate is changed as the training proceeds,\nthat is, 10−3 for epoch 1-40 , 10−4 for epoch 41-60, and 10−5 for epoch 61-80. We use 40000 samples\nof CIFAR10 as the training set and 10000 examples as the validation set. The training accuracy\nand the validation accuracy are shown in Fig. 5. The RDF of the eﬀective target function of the\nlast hidden layer for each variant is shown in Fig. 7.\n6\nFigure 4: Variants of Resnet18.\nFor the experiment of fully-connected network on MNIST, we choose the activation function of\ntanh and size 784 −500 −500 −500 −500 −500 −10. The output layer of the network does not\nequip any activation function. The network is trained by Adam optimizer with mean squared loss,\nbatch size 256 and learning rate 10−5. The training is stopped when the loss is smaller than 10−2.\nWe use 30000 samples of the MNIST as training set. The RDF of the eﬀective target functions of\ndiﬀerent hidden layers are shown in Fig. 8.\nNote that ranges of diﬀerent dimensions in the input are diﬀerent, which would result in that\nfor the same δ, diﬀerent dimensions keep diﬀerent frequency ranges when convolving with the\nGaussian function. Therefore, we normalized each dimension by its maximum amplitude, thus,\neach dimension lies in [−1, 1]. Without doing such normalization, we still obtain similar results of\ndeep frequency principle.\nAll codes are written by Python and Tensorﬂow, and run on Linux system with Nvidia GTX\n2080Ti or Tesla V100 cards. Codes can be found at github.com.\n7\n3\nResults\nBased on the experiments of deep networks and real datasets, we would show a deep frequency\nprinciple, a promising mechanism, to understand why deeper neural networks learn faster, that is,\nthe eﬀective target function for a deeper hidden layer biases towards lower frequency during the\ntraining. To derive the eﬀective function, we decompose the target function into a pre-condition\ncomponent, consisting of layers before the considered hidden layer, and a learning component,\nconsisting from the considered hidden layer to the output layer, as shown in Fig.\n2.\nAs the\nconsidered hidden layer gets deeper, the learning component eﬀectively learns a lower frequency\nfunction. Due to the frequency principle, i.e., deep neural networks learn low frequency faster, a\ndeeper neural network can learn the target function faster. The key to validate deep frequency\nprinciple is to show the frequency distribution of the eﬀective target function for each considered\nhidden layer.\n(a) Training\n(b) Validation\nFigure 5: Training accuracy and validation accuracy vs. epoch for variants of Resnet18.\nFirst, we study a practical and common situation, that is, networks with more hidden layers\nlearn faster. Then, we examine the deep frequency principle on a ﬁxed deep neural network but\nconsider the eﬀective target function for diﬀerent hidden layers.\n3.1\nDeep frequency principle on variants of Resnet18\nIn this subsection, we would utilize variants of Resnet18 and CIFAR10 dataset to validate deep\nfrequency principle. The structures of four variants are illustrated as follows. As shown in Fig. 4,\nall structures have several convolution parts, followed by two same fully-connected layers. Compared\nwith Resnet18-i, Resnet18-(i + 1) drops out a convolution part and keep other parts the same.\nAs shown in Fig. 5, a deeper net attains a ﬁxed training accuracy with fewer training epochs\nand achieves a better generalization after training.\nFrom the layer “-2” to the ﬁnal output, it can be regarded as a two-layer neural network, which\nis widely studied. Next, we examine the RDF for layer “-2”. The eﬀective target function is\nS[−3] =\nn\u0010\nf [−3]\nθ\n(xi), yi\n\u0011on\ni=1 .\n(12)\n8\n(a) epoch 0\n(b) epoch 1\n(c) epoch 2\n(d) epoch 3\n(e) epoch 15\n(f) epoch 80\nFigure 6: RDF of\nn\u0010\nf [−3]\nθ\n(xi), f [−1]\nθ\n(xi)\n\u0011on\ni=1 vs. 1/δ at diﬀerent epochs for variants of Resnet18.\n(a) epoch 0\n(b) epoch 1\n(c) epoch 2\n(d) epoch 3\n(e) epoch 15\n(f) epoch 80\nFigure 7: RDF of S[−3] (eﬀective target function of layer “-2”) vs. 1/δ at diﬀerent epochs for\nvariants of Resnet18.\n9\nAs shown in Fig.\n7(a), at initialization, the RDFs for deeper networks concentrate at higher\nfrequencies. However, as training proceeds, the concentration of RDFs of deeper networks moves\ntowards lower frequency faster. Therefore, for the two-layer neural network with a deeper pre-\ncondition component, learning can be accelerated due to the fast convergence of low frequency in\nneural network dynamics, i.e., frequency principle.\nFor the two-layer neural network embedded as the learning component of the full network, the\neﬀective target function is S[−3]. As the pre-condition component has more layers, layer “-2” is a\ndeeper hidden layer in the full network. Therefore, Fig. 7 validates that the eﬀective target function\nfor a deeper hidden layer biases towards lower frequency during the training, i.e., deep frequency\nprinciple. One may curious about how is the frequency distribution of the eﬀective function of\nthe learning component, i.e., {f [−3]\nθ\n(x), f [−1]\nθ\n(x)}.\nWe consider RDF for the eﬀective function\nevaluated on training points, that is,\nn\u0010\nf [−3]\nθ\n(xi), f [−1]\nθ\n(xi)\n\u0011on\ni=1. This is similar as the eﬀective\ntarget function, that is, those in deeper networks bias towards more low frequency function, as\nshown in Fig. 6.\nWe have also performed many other experiments and validate the deep frequency principle,\nsuch as networks with diﬀerent activation functions, fully-connected networks without residual\nconnection, and diﬀerent loss functions. An experiment for discussing the residual connection is\npresented in the discussion part.\nThe comparison in experiments above crosses diﬀerent networks, which would be diﬃcult for\nfuture analysis. Alternatively, we can study how RDFs of S[l] of diﬀerent l in a ﬁxed deep network\nevolves during the training process. As expected, as l increases, S[l] would be dominated by more\nlower frequency during the training process.\n3.2\nRDF of diﬀerent hidden layers in a fully-connected deep neural net-\nwork\nAs analyzed above, an examination of the deep frequency principle in a deep neural network would\nprovide valuable insights for future theoretical study. A key problem is that diﬀerent hidden layers\noften have diﬀerent sizes, i.e., S[l]’s have diﬀerent input dimensions over diﬀerent l’s. LFR is similar\nto a volume ratio, thus, it depends on the dimension. To control the dimension variable, we consider\na fully-connected deep neural network with the same size for diﬀerent hidden layers to learn MNIST\ndataset.\nAs shown in Fig. 8, at initialization, the peak of RDF for a deeper hidden layer locates at a\nhigher frequency. As the training goes, the peak of RDF of a deeper hidden layer moves towards low\nfrequency faster. At the end of the training, the frequency of the RDF peak monotonically decreases\nas the hidden layer gets deeper. This indicates that the eﬀective target function for a deeper hidden\nlayer evolves faster towards a low frequency function, i.e., the deep frequency principle in a deep\nneural network.\n4\nDiscussion\nIn this work, we empirically show a deep frequency principle that provides a promising mechanism\nfor understanding the eﬀect of depth in deep learning, that is, the eﬀective target function for a\ndeeper hidden layer biases towards lower frequency during the training. Speciﬁcally, based on the\nwell-studied frequency principle, the deep frequency principle well explains why deeper learning can\n10\n(a) epoch 0\n(b) epoch 100\n(c) epoch 200\n(d) epoch 300\n(e) epoch 400\n(f) epoch 600\n(g) epoch 800\n(h) epoch 900\nFigure 8: RDF of diﬀerent hidden layers vs. 1/δ at diﬀerent epochs for a fully-connected deep neural\nnetwork when learning MNIST. The ﬁve colored curves are for ﬁve hidden layers, respectively. The\ncurve with legend “layer i” is the RDF of S[l].\nbe faster. We believe that the study of deep frequency principle would provide valuable insight for\nfurther theoretical understanding of deep neural networks. Next, we will discuss the relation of this\nwork to other studies and some implications.\n4.1\nKernel methods\nKernel methods, such as support vector machine and random feature model, are powerful at ﬁtting\nnon-linearly separable data. An intuitive explanation is that when data are projected into a much\nhigher dimensional space, they are closer to be linearly separable. From the perspective of Fourier\nanalysis, we quantify this intuition through the low-frequency ratio. After projecting data to higher\ndimensional space through the hidden layers, neural networks transform the high-dimensional target\nfunction into a lower frequency eﬀective function. The deeper neural networks project data more\nthan once into high dimensional space, which is equivalent to the combination of multiple kernel\nmethods. In addition, neural networks not only learn the weights of kernels but also are able to\nlearn the kernels, showing a much more capability compared with kernel methods.\n4.2\nGeneralization\nFrequency principle reveals a low-frequency bias of deep neural networks [31, 32], which provides\nqualitative understandings [33, 35] for the good generalization of neural networks in problems dom-\ninated by low frequencies, such as natural image classiﬁcation tasks, and for the poor generalization\nin problems dominated by high frequencies, such as predicting parity function. Generalization in\nreal world problems [13] is often better as the network goes deeper. How to characterize the better\ngeneralization of deeper network is also a critical problem in deep learning. This work, validating\n11\na deep frequency principle, may provide more understanding to this problem in future work. As\nthe network goes deeper, the eﬀective target function for the last hidden layer is more dominated\nby low frequency. This deep frequency principle phenomenon is widely observed, even in ﬁtting\nhigh-frequency function, such as parity function in our experiments. This suggest that deeper net-\nwork may have more bias towards low frequency. However, it is diﬃcult to examine the frequency\ndistribution of the learned function on the whole Fourier domain due to the high dimensionality\nof data. In addition, since the generalization increment of a deeper network is more subtle, we\nare exploring a more precise characterization of the frequency distribution of a high-dimensional\nfunction.\n4.3\nHow deep is enough?\nThe eﬀect of depth can be intuitively understood as a pre-condition that transforms the target\nfunction to a lower frequency function. Qualitatively, it requires more layers to ﬁt a higher frequency\nfunction. However, the eﬀect of depth can be saturated. For example, the eﬀective target functions\nfor very deep layers can be very similar in the Fourier domain (dominated by very low frequency\ncomponents) when the layer number is large enough, as an example shown in Fig. 8(g, h). A too\ndeep network would cause extra waste of computation cost. A further study of the deep frequency\nprinciple may also provide a guidance for design the depth of the network structure.\n4.4\nVanishing gradient problem\nWhen the network is too deep, vanishing gradient problem often arises, slows down the training, and\ndeteriorates the generalization [13]. As an example, we use very deep fully connected networks, i.e.,\n20, 60 and 80 layers, to learn MNIST dataset. As shown in Fig. 9(a) (solid lines), deeper networks\nlearn slower in such very deep situations. The frequency distribution of eﬀective target function\nalso violates deep frequency principle in this case. For example, at epoch 267 as shown in Fig.\n9(b) (solid lines), the RDFs of S[−3] of diﬀerent networks show that the eﬀective target function\nof a deeper hidden layer has more power on high-frequency components. Residual connection is\nproposed to save deep neural network from vanishing gradient problem and utilize the advantage\nof depth [13], in which deep frequency principle is satisﬁed as shown in the previous example in\nFig. 7. To verify the eﬀectiveness of residual connection, we add residual connection to the very\ndeep fully connected networks to learn MNIST dataset. As shown in Fig. 9(a) (dashed lines), the\nlearning processes for diﬀerent networks with residual connections are almost at the same speed.\nThe RDFs in Fig. 9(b) (dashed lines) show that with residual connections, the depth only incurs\nalmost a negligible eﬀect of deep frequency principle, i.e., a saturation phenomenon. The detailed\nstudy of the relation of the depth and the diﬃculty of the task is left for further research.\nTaken together, the deep frequency principle proposed in this work may have fruitful implica-\ntion for future study of deep learning. A detailed study of deep frequency principle may require\nanalyze diﬀerent dynamical regimes of neural networks. As an example, a recent work [21] draws a\nphase diagram for two-layer ReLU neural networks at inﬁnite width limit by three regimes, linear,\ncritical and condensed regimes. Such study could inspire the study of phase diagram of deep neural\nnetworks. The linear regime is well studied [14, 35, 36, 2], which may be a good starting point and\nshed lights on the study of other regimes.\n12\n(a)\n(b)\nFigure 9: (a) Training loss vs. epoch. (b) RDF of S[−3] vs. 1/δ at epoch 267. Legend is the number\nof layers of the fully-connected network with (“with resnet”) or without (“without resnet”) residual\nconnection trained to ﬁt MNIST.\n5\nAcknowledgements\nZ.X. is supported by National Key R&D Program of China (2019YFA0709503), Shanghai Sailing\nProgram, Natural Science Foundation of Shanghai (20ZR1429000), NSFC 62002221 and partially\nsupported by HPC of School of Mathematical Sciences and Student Innovation Center at Shanghai\nJiao Tong University.\nReferences\n[1] Sanjeev Arora, N Cohen, and Elad Hazan. On the optimization of deep networks: Implicit\nacceleration by overparameterization. In 35th International Conference on Machine Learning,\n2018.\n[2] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.\nOn exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955,\n2019.\n[3] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,\nMaxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.\nA closer look at memorization in deep networks. International Conference on Machine Learn-\ning, 2017.\n[4] BAAI.\nSuggested\nnotation\nfor\nmachine\nlearning.\nhttps: // github. com/ Mayuyu/\nsuggested-notation-for-machine-learning , 2020.\n13\n[5] Simon Biland, Vinicius C Azevedo, Byungsoo Kim, and Barbara Solenthaler. Frequency-aware\nreconstruction of ﬂuid simulations with generative networks. arXiv preprint arXiv:1912.08776,\n2019.\n[6] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.\nSpectrum dependent learning\ncurves in kernel regression and wide neural networks. arXiv preprint arXiv:2002.02561, 2020.\n[7] Wei Cai, Xiaoguang Li, and Lizuo Liu. A phase shift deep neural network for high frequency\napproximation and wave problems. Accepted by SISC, arXiv:1909.11759, 2019.\n[8] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understand-\ning the spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.\n[9] Weinan E, Chao Ma, and Lei Wu. Machine learning from a continuous viewpoint, i. Science\nChina Mathematics, pages 1–34, 2020.\n[10] Weinan E and Wang Qingcan. Exponential convergence of the deep neural network approxi-\nmationfor analytic functions. SCIENCE CHINA Mathematics, 61(10):1733, 2018.\n[11] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In\nConference on learning theory, pages 907–940, 2016.\n[12] Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely.\nThe implicit bias of depth: How\nincremental learning drives generalization. In International Conference on Learning Represen-\ntations, 2019.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[14] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. In Advances in neural information processing systems, pages\n8571–8580, 2018.\n[15] Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Adaptive activation functions\naccelerate convergence in deep and physics-informed neural networks. Journal of Computa-\ntional Physics, 404:109136, 2020.\n[16] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz\nBarak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity.\nIn Advances in Neural Information Processing Systems, pages 3491–3501, 2019.\n[17] Kenji Kawaguchi, Jiaoyang Huang, and Leslie Pack Kaelbling. Eﬀect of depth and width on\nlocal minima in deep learning. Neural computation, 31(7):1462–1498, 2019.\n[18] Xi-An Li, Zhi-Qin John Xu, and Lei Zhang. A multi-scale dnn algorithm for nonlinear elliptic\nequations with multiple scales. Communications in Computational Physics, 28(5):1886–1906,\n2020.\n[19] Ziqi Liu, Wei Cai, and Zhi-Qin John Xu. Multi-scale deep neural network (mscalednn) for\nsolving poisson-boltzmann equation in complex domains. Communications in Computational\nPhysics, 28(5):1970–2001, 2020.\n14\n[20] Tao Luo, Zheng Ma, Zhi-Qin John Xu, and Yaoyu Zhang. Theory of the frequency principle\nfor general deep neural networks. arXiv preprint arXiv:1906.09235, 2019.\n[21] Tao Luo, Zhi-Qin John Xu, Zheng Ma, and Yaoyu Zhang. Phase diagram for two-layer relu\nneural networks at inﬁnite-width limit. arXiv preprint arXiv:2007.07497, 2020.\n[22] Chao Ma, Lei Wu, and Weinan E. The slow deterioration of the generalization error of the\nrandom feature model. In Proceedings of Machine Learning Research, 2020.\n[23] Chris Mingard, Joar Skalse, Guillermo Valle-P´erez, David Mart´ınez-Rubio, Vladimir Mikulik,\nand Ard A Louis. Neural networks are a priori biased towards boolean functions with low\nentropy. arXiv preprint arXiv:1909.11522, 2019.\n[24] Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht,\nYoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. Interna-\ntional Conference on Machine Learning, 2019.\n[25] Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural\nnetworks for learned functions of diﬀerent frequencies. In Advances in Neural Information\nProcessing Systems, pages 4763–4772, 2019.\n[26] Andrew M Saxe, James L Mcclelland, and Surya Ganguli. Exact solutions to the nonlinear\ndynamics of learning in deep linear neural network. In In International Conference on Learning\nRepresentations. Citeseer, 2014.\n[27] Yeonjong Shin. Eﬀects of depth, width, and initialization: A convergence analysis of layer-wise\ntraining for deep linear neural networks. arXiv preprint arXiv:1910.05874, 2019.\n[28] Matus Telgarsky. beneﬁts of depth in neural networks. In Conference on Learning Theory,\npages 1517–1539, 2016.\n[29] Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis.\nDeep learning generalizes\nbecause the parameter-function map is biased towards simple functions.\narXiv preprint\narXiv:1805.08522, 2018.\n[30] Feng Wang, Alberto Eljarrat, Johannes M¨uller, Trond R Henninen, Rolf Erni, and Christoph T\nKoch. Multi-resolution convolutional neural networks for inverse problems. Scientiﬁc reports,\n10(1):1–11, 2020.\n[31] Zhi-Qin J Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in\nfrequency domain. International Conference on Neural Information Processing, pages 264–274,\n2019.\n[32] Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency princi-\nple: Fourier analysis sheds light on deep neural networks. Communications in Computational\nPhysics, 28(5):1746–1767, 2020.\n[33] Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis.\narXiv preprint arXiv:1808.04295, 2018.\n[34] Greg Yang and Hadi Salman. A ﬁne-grained spectral perspective on neural networks. arXiv\npreprint arXiv:1907.10599, 2019.\n15\n[35] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. Explicitizing an implicit bias of the\nfrequency principle in two-layer neural networks. arXiv preprint arXiv:1905.10264, 2019.\n[36] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma.\nA type of generalization error\ninduced by initialization in deep neural networks. In Mathematical and Scientiﬁc Machine\nLearning, pages 144–164. PMLR, 2020.\n16\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-07-28",
  "updated": "2020-12-20"
}