{
  "id": "http://arxiv.org/abs/2007.15580v1",
  "title": "Deep-Learning based Inverse Modeling Approaches: A Subsurface Flow Example",
  "authors": [
    "Nanzhe Wang",
    "Haibin Chang",
    "Dongxiao Zhang"
  ],
  "abstract": "Deep-learning has achieved good performance and shown great potential for\nsolving forward and inverse problems. In this work, two categories of\ninnovative deep-learning based inverse modeling methods are proposed and\ncompared. The first category is deep-learning surrogate-based inversion\nmethods, in which the Theory-guided Neural Network (TgNN) is constructed as a\ndeep-learning surrogate for problems with uncertain model parameters. By\nincorporating physical laws and other constraints, the TgNN surrogate can be\nconstructed with limited simulation runs and accelerate the inversion process\nsignificantly. Three TgNN surrogate-based inversion methods are proposed,\nincluding the gradient method, the iterative ensemble smoother (IES), and the\ntraining method. The second category is direct-deep-learning-inversion methods,\nin which TgNN constrained with geostatistical information, named TgNN-geo, is\nproposed for direct inverse modeling. In TgNN-geo, two neural networks are\nintroduced to approximate the respective random model parameters and the\nsolution. Since the prior geostatistical information can be incorporated, the\ndirect-inversion method based on TgNN-geo works well, even in cases with sparse\nspatial measurements or imprecise prior statistics. Although the proposed\ndeep-learning based inverse modeling methods are general in nature, and thus\napplicable to a wide variety of problems, they are tested with several\nsubsurface flow problems. It is found that satisfactory results are obtained\nwith a high efficiency. Moreover, both the advantages and disadvantages are\nfurther analyzed for the proposed two categories of deep-learning based\ninversion methods.",
  "text": " \n1 \n \nDeep-Learning based Inverse Modeling Approaches: \nA Subsurface Flow Example \nNanzhe Wanga, Haibin Changa,*, and Dongxiao Zhangb,* \n \na BIC-ESAT, ERE, and SKLTCS, College of Engineering, Peking University, Beijing 100871, \nP. R. China \nb School of Environmental Science and Engineering, Southern University of Science and \nTechnology, Shenzhen 518055, P. R. China \n \n*Corresponding author: E-mail address: changhaibin@pku.edu.cn (Haibin Chang); \nzhangdx@sustech.edu.cn (Dongxiao Zhang) \n \nKey Points: \n Two categories of innovative deep-learning based inverse modeling methods are \nproposed and compared.  \n The deep-learning surrogate-based inversion methods can accelerate the inversion \nprocess significantly. \n The direct-deep-learning-inversion method works well in cases with sparse spatial \nmeasurements or imprecise prior statistics. \nAbstract \nDeep-learning has achieved good performance and shown great potential for solving \nforward and inverse problems. In this work, two categories of innovative deep-learning based \ninverse modeling methods are proposed and compared. The first category is deep-learning \nsurrogate-based inversion methods, in which the Theory-guided Neural Network (TgNN) is \nconstructed as a deep-learning surrogate for problems with uncertain model parameters. By \nincorporating physical laws and other constraints, the TgNN surrogate can be constructed with \nlimited simulation runs and accelerate the inversion process significantly. Three TgNN \nsurrogate-based inversion methods are proposed, including the gradient method, the iterative \nensemble smoother (IES), and the training method. The second category is direct-deep-\n \n2 \n \nlearning-inversion methods, in which TgNN constrained with geostatistical information, \nnamed TgNN-geo, is proposed for direct inverse modeling. In TgNN-geo, two neural networks \nare introduced to approximate the respective random model parameters and the solution. Since \nthe prior geostatistical information can be incorporated, the direct-inversion method based on \nTgNN-geo works well, even in cases with sparse spatial measurements or imprecise prior \nstatistics. Although the proposed deep-learning based inverse modeling methods are general in \nnature, and thus applicable to a wide variety of problems, they are tested with several \nsubsurface flow problems. It is found that satisfactory results are obtained with a high \nefficiency. Moreover, both the advantages and disadvantages are further analyzed for the \nproposed two categories of deep-learning based inversion methods. \n \nKeywords: Deep-learning; Inverse modeling; Theory-guided Neural Network; Subsurface \nflow. \n1 Introduction  \nInverse modeling aims to infer uncertain parameters of a system with noisy observations \nof the system response, which has been widely utilized in various scientific and engineering \npractices, such as seismic inversion (Bunks et al., 1995), petroleum reservoir history matching \n(Oliver et al., 2008), aquifer parameter estimation in hydrology (Carrera & Neuman, 1986a, b, \nc), and medical imaging (Arridge, 1999). Under certain conditions, the inverse problems can \nbe viewed as optimization problems, in which the model parameters are modified, such that \nthe predictions from forward models can match the measurements. In addition, prior \nknowledge can be used to regularize the objective function and to obtain the maximum a \nposteriori (MAP) estimate of the model parameters. \nThe gradient-based method is a straightforward and common way to perform inverse \nmodeling tasks. Anterion et al. (1989) presented a rigorous analytical method to calculate the \ngradients of observations with respect to reservoir characteristics, which can then be used to \nassist the process of parameter adjustment for history matching. For calculating the required \n \n3 \n \ngradients in inverse modeling, the adjoint method is a frequently utilized technique (Chavent \net al., 1975; Chen et al., 1974; Wasserman et al., 1975; Yang & Watson, 1988). Carrera and \nNeuman (1986a, b, c) estimated the aquifer model parameters with maximum likelihood \nestimation theory, in which the adjoint method was adopted to calculate the gradients of \nminimization criterions with respect to the model parameters. Wu et al. (1999) used the adjoint \nmethod to calculate the sensitivity coefficients of wellbore pressures and water-oil-ratios to \nmodel parameters, and then the Gauss-Newton method was employed to optimize the objective \nfunction of history matching for two-phase problems. Li et al. (2003) extended the adjoint \nmethod to calculate the sensitivity coefficients for history matching of three-dimensional, \nthree-phase flow problems. Although the adjoint method achieves high efficiency, which only \nrequires one forward simulation to calculate the sensitivity, it is tedious to construct adjoint \nequations, especially for complex problems.  \nDifferent from gradient-based methods, ensemble-based inverse modeling methods have \nattained numerous successes due to their ease of implementation and capability of dealing with \nlarge-scale problems. The ensemble Kalman filter (EnKF), proposed by Evensen (1994), is an \neffective ensemble-based method for inverse modeling, and has been extensively used in \nvarious fields, such as meteorology (Houtekamer & Mitchell, 2001; Houtekamer et al., 2005), \npetroleum engineering (Chang et al., 2010; Gu & Oliver, 2005; Nævdal et al., 2005), and \nhydrology (Chen & Zhang, 2006; Reichle et al., 2002). Nonlinearity and non-Gaussianity \nconstitute the major challenges for EnKF, and many variants have been proposed for solving \nthese complex situations. Gu and Oliver (2007) developed an iterative ensemble Kalman filter, \nnamed the ensemble randomized maximum likelihood filter (EnRML), for solving nonlinear \nmultiphase fluid flow. Zhou et al. (2011) proposed the normal-score ensemble Kalman filter \n(NS-EnKF), which can handle non-Gaussian model parameters and state variables by \ntransforming the original state vector into a new Gaussian vector.  \nThe ensemble smoother (ES), proposed by Van Leeuwen and Evensen (1996), is another \nensemble-based method for inverse modeling, which performs one global update using data \nfrom all time steps simultaneously rather than sequential updates as that in EnKF. Skjervheim \n \n4 \n \nand Evensen (2011) used the ES for history matching of reservoir simulation, and compared \nthe performance of ES with EnKF. In their work, they concluded that, by avoiding the \nsimulation restarts associated with the sequential updates in EnKF, ES is more efficient and \nsimpler compared with EnKF, and it can provide identical results to EnKF for linear dynamic \nmodels. Performing only a single update, however, ES is improper for nonlinear dynamic \nproblems. For solving this problem, the iterative ensemble smoother (IES) is developed to \nachieve better performance for nonlinear problems (Chen & Oliver, 2012, 2013). An IES \nmethod based on a modified Levenberg–Marquardt method is proposed by Chen and Oliver \n(2013), in which the explicit computation of the sensitivity matrix is avoided by modifying the \nHessian matrix. Moreover, Chang et al. (2017) put forward a surrogate model based IES for \nparameter inversion, in which the polynomial chaos expansion (PCE) surrogate and the \ninterpolation surrogate are employed to augment the efficiency of the inversion process. \nSimilarly, Ju et al. (2018) presented an adaptive Gaussian process (GP)-based iterative \nensemble smoother (GPIES) for heterogeneous conductivity estimation of subsurface flow, in \nwhich the GP surrogate is constructed and adaptively refined by adding a few new points \nchosen from the updated parameter ensemble. \nRecently, deep-learning based approaches have been adopted for solving inverse problems, \nand achieved good performance in numerous fields. Jin et al. (2017) proposed the FBPConvNet \nfor solving image reconstruction problems, which combined the Filtered Back Projection (FBP) \nand convolutional neural network (CNN). In their work, the FBP is first performed, and \nsubsequently used as input for CNN to regress the FBP results to the ground truth images. The \nframework is adopted for X-ray computed tomography (CT) reconstruction, and good results \nare obtained. Adler and Öktem (2017) developed a partially learned gradient descent scheme \nfor solving ill-posed inverse problems, in which the gradient-like iterative scheme is used for \noptimizing the objective function, and gradients are learned with a CNN from the training data. \nAntholzer et al. (2019) adopted a deep-learning framework for image reconstruction in \nphotoacoustic tomography, in which a CNN is trained with training data and used for image \nreconstruction from sparse data. The CNN structure has also been utilized for seismic inversion \n \n5 \n \nin geophysics. Wu and Lin (2020) proposed the InversionNet for full waveform inversion, \nwhich employed an encoder-decoder structure of CNN. Li et al. (2020) proposed an end-to-\nend seismic inversion network (SeisInvNet), which takes advantage of all seismic data for \nreconstruction of velocity models. In hydrology, Mo et al. (2019) developed a deep \nautoregressive neural network-based surrogate as a forward groundwater contaminant transport \nmodel, and the iterative local updating ensemble smoother is adopted for groundwater \ncontaminant source identification. Deep-learning techniques have also been used in \nparameterization of geological media, such as Variational Autoencoder (VAE) (Laloy et al., \n2017) and Generative Adversarial Network (GAN) (Laloy et al., 2018), which constitutes an \nimportant step for geological media property inversion. \nInverse modeling based on the Physics Informed Neural Network (PINN) (Raissi et al., \n2019) has also been investigated. The PINN, proposed by Raissi et al. (2019), can incorporate \nthe residual of partial differential equations (PDEs) into the loss function of a neural network, \nand can be used to identify coefficients in the PDEs with available data. For space-dependent \ncoefficients, such as hydraulic conductivity, Tartakovsky et al. (2020) used neural networks to \napproximate both the hydraulic conductivity and hydraulic head, which are trained with \nDarcy’s law, in addition to available measurements. A similar strategy has also been applied to \nmultiphysics data assimilation for subsurface transport (He et al., 2020). However, PINN-based \ninverse modeling methods do not honor geostatistical information of model parameters, and \nthus require a large number of measurements, which is unrealistic for engineering practice.  \nIn this work, two categories of inverse modeling approaches are proposed and compared. \nThe first category is deep-learning surrogate-based methods, including the Theory-guided \nneural network (TgNN) surrogate-based gradient method, iterative ensemble smoother (IES), \nand training method. The TgNN, developed by Wang et al. (2020b), can incorporate not only \nphysical principles, but also practical engineering theories, and can be further developed as a \nsurrogate for problems with uncertain model parameters (Wang et al., 2020a). Since the TgNN \nsurrogate can be constructed with limited simulation data, and even in a label-free manner, \nfewer forward simulations are required. In addition, model predictions from the TgNN \n \n6 \n \nsurrogate necessitate little computation effort, which can improve efficiency for parameter \ninversion. Moreover, solving inverse problems with the gradient method and the training \nmethod becomes feasible since the gradient can be easily obtained from the TgNN surrogate. \nThe second category is the direct-deep-learning inversion method, in which TgNN with \ngeostatistical constraint, named TgNN-geo, is proposed as the deep-learning framework for \ninverse modeling. In TgNN-geo, two neural networks are introduced to approximate the \nrandom model parameter and solution, respectively. In order to honor prior knowledge of \ngeostatistical information of the random model parameter, the neural network for \napproximating the random model parameter is trained by using the realizations generated from \nKarhunen-Loeve expansion (KLE). By minimizing the loss function of TgNN-geo, estimation \nof the model parameter and approximation of the model solution can be simultaneously \nobtained. Indeed, since prior geostatistical information can be incorporated, the direct inversion \nmethod based on TgNN-geo can work well, even in cases with sparse spatial measurements. \nThe two categories of methods are tested with several subsurface flow problems, and the \nadvantages and disadvantages of the two categories of methods are discussed.  \n2 Inverse Problem \nTo introduce the inverse problem, let us consider a mathematical model with the following \nrelationship: \n(\n)=\ng m\nd                              (1) \nwhere m   denotes the model parameters; \n( )\ng   denotes the theoretical forward model or \nsimulator; and d  denotes the predicted outputs from the model, given the model parameters \nm . Predicting the model response with known model parameters by running a simulator can \nbe termed the forward problem. However, in many science and engineering practices, the \nmodel parameters are not specifically known, and only some measurements are available. As a \nconsequence, one usually needs to infer the model parameters that characterize the system with \ngiven measurements to make more accurate predictions of the model response, which can be \ntermed the inverse problem. The inverse problem requires solving a group of equations (Oliver \n \n7 \n \net al., 2008): \n(\n)+\nobs\ng\n\n\nd\nm\n                          (2) \nwhere \nobs\nd\n denotes the measurement data, including direct and indirect measurements; and \n denotes the observation errors. The solution of the inverse problem is usually non-unique, \nand one may need to characterize the posterior conditional probability density function (PDF) \nof model parameters m  given measurement data \nobs\nd\n, which is \n(\n|\n)\nobs\np m d\n. To solve the \ninverse problem, one may maximize the posterior PDF of model parameters from a \nprobabilistic perspective. If the prior PDF of model parameters, modeling and measurement \nerrors all follow a Gaussian distribution, maximizing the posterior PDF is equivalent to \nminimizing the objective function (Oliver et al., 2008): \n\n\n\n\n\n\n\n\n1\n1\n1\n(\n)\n(\n)\n(\n)\n2\n1\n           \n2\nT\nD\nT\npr\np\nobs\no\nr\nbs\nM\nO\ng\nC\ng\nC\n\n\n\n\n\n\n\n\nd\nd\nm\nm\nm\nm\nm\nm\nm\n               (3) \nwhere \nD\nC  denotes the covariance matrix of modeling and measurement errors; \nM\nC\n denotes \nthe prior covariance matrix of the model parameters; and \npr\nm\n denotes the prior estimate of \nthe model parameters. Therefore, under certain conditions, the inverse modeling problem can \nbe understood as an optimization problem.  \n3 Optimization Methods \n3.1 Optimization based on gradient method \nTo solve the optimization problem shown in Eq. (3), the gradient method is a \nstraightforward option. Among different types of gradient methods, the Levenberg–Marquardt \nalgorithm has achieved good performance for optimization, and the iterative update of model \nparameters can be formulated as (Chang et al., 2017; Chen & Oliver, 2013): \n\n\n\n\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\nT\npr\nT\nobs\nl\nl\nl\nM\nl\nD\nl\nM\nl\nl\nD\nl\nC\nG C G\nC\nG C\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm\nm\nm\nm\nm\nd\n    (4) \nwhere l  denotes the iteration number index; \nl\nG  denotes the sensitivity matrix of data with \n \n8 \n \nrespect to model parameters at th\nl\niteration; and \nl denotes a multiplier, which modified the \nHessian to reduce the influence of the large data mismatch in early iterations (Chen & Oliver, \n2013). Eq. (4) can be further rewritten as (Chang et al., 2017): \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n          \n1\nT\nT\npr\nl\nl\nM\nM\nl\nl\nD\nl\nM\nl\nl\nM\nM\nl\nl\nT\nT\nobs\nM\nl\nl\nD\nl\nM\nl\nl\nC\nC G\nC\nG C G\nG C\nC\nC G\nC\nG C G\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm\nm\nm\nm\nm\nd\n (5) \nIn the iteration process of the gradient method, the sensitivity matrix of observed data with \nrespect to model parameters should be calculated in each iteration step. For the numerical \nsimulator, calculation of sensitivity coefficients is tedious with the adjoint method or the \nperturbation method. Therefore, an efficient approach for sensitivity calculation is needed. \nMoreover, updating one realization of model parameters is not adequate for characterizing \nposterior PDF. \n3.2 Optimization based on iterative ensemble smoother (IES) \nIn order to characterize the posterior PDF, the ensemble-based method can be adopted. \nThe sensitivity matrix can also be approximated with the ensemble statistics. Specifically, in \nthe IES, the Hessian matrix is modified and approximated, in which the explicit computation \nof the sensitivity matrix is avoided (Chen & Oliver, 2013). \nIn the ensemble-based method, a group of realizations of model parameters should be \nupdated with Eq. (5) as follows: \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1,\n,\n,\n,\n,\n,\n,\n1\n,\n,\n,\n,\n1\n1\n1\n            \n1\n,\n1,\n,\nT\nT\npr\nl\nj\nl j\nM\nM\nl j\nl\nD\nl j\nM\nl j\nl j\nM\nM\nl j\nj\nl\nT\nT\nobs\nM\nl j\nl\nD\nl j\nM\nl j\nl j\nj\ne\nC\nC G\nC\nG C G\nG C\nC\nC G\nC\nG C G\ng\nj\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm\nm\nm\nm\nm\nd\n(6) \nwhere j  denotes the realization index; \n,l j\nG\n denotes the sensitivity matrix, taking a value at \n,l j\nm\n; and \ne\nN  denotes the total number of realizations in the ensemble. \nThe updating formula is then further modified in the following ways: first, the prior \ncovariance matrix \nM\nC   in the Hessian matrix is replaced by the covariance matrix of the \n \n9 \n \nupdated model parameters \nl\nM\nC\n at iteration step l  (Chang et al., 2017; Chen & Oliver, 2013); \nand second, the sensitivity matrix \n,l j\nG\n is replaced by the averaged sensitivity matrix \nl\nG  at \niteration step l  (Chang et al., 2017; Gu & Oliver, 2007; Le et al., 2016). The updating formula \nthen becomes: \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1,\n,\n,\n1\n,\n1\n1\n1\n            \n1\n,\n1,\n,\nl\nl\nl\nl\nl\nl\nT\nT\npr\nl\nj\nl j\nM\nM\nl\nl\nD\nl\nM\nl\nl\nM\nM\nl j\nj\nl\nT\nT\nobs\nM\nl\nl\nD\nl\nM\nl\nl j\nj\ne\nC\nC G\nC\nG C G\nG C\nC\nC G\nC\nG C G\ng\nj\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm\nm\nm\nm\nm\nd\n(7) \nThen, the following approximations are adopted (Chang et al., 2017; Zhang, 2001): \nl\nl\nl\nT\nM\nl\nM D\nC G\nC\n\n                         (8) \nl\nl\nl\nT\nl\nM\nl\nD D\nC\nG C G \n                       (9) \nwhere \nl\nl\nM D\nC\n  denotes the cross covariance between the updated model parameters and the \npredicted data at iteration step l ; and \nl\nl\nD D\nC\n denotes the covariance of the predicted data at \niteration step l . By substituting Eq. (8) and Eq. (9) into Eq. (7), one has the following: \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1,\n,\n,\n1\n,\n1\n1\n1\n            \n1\n,\n1,\n,\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\npr\nl\nj\nl j\nM\nM D\nl\nD\nD M\nM\nl j\nj\nl\nobs\nM D\nl\nD D\nD D\nD\nl j\nj\ne\nC\nC\nC\nC\nC\nC\nC\nC\nC\ng\nj\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm\nm\nm\nm\nm\nd\n  (10) \nIn the IES, the uncertainty can be quantified with the updated realizations, and it is simple \nto combine with the simulator since explicit computation of sensitivity is not required. When \nperforming IES, the forward simulation should be run repeatedly during the iteration because \na group of realizations should be updated, which may incur large computational effort. \nConstructing a surrogate model for forward evaluation is an effective way to improve the \nefficiency of IES (Chang et al., 2017). Due to the curse of dimensionality, however, most \nexisting surrogate models may not work well for problems with large dimensionality. In the \nnext section, a deep-learning surrogate is introduced, which shows superiority for problems \nwith large dimensionality. \n \n10 \n \n4 Deep-learning Surrogate-based Inversion Methods \nIn this section, several deep-learning surrogate-based inversion methods will be presented. \nHere, we will first introduce the investigated physical problem in this work. \nConsider a dynamic physical problem with the following stochastic partial differential \nequations (SPDEs) as the governing equation: \n( ( , ; );\n( ; ))\n0,       \n, \n, \n0,\n          ( ( , ; ))\n( ),\n, \n0,\n          ( ( ,0; ))\n(\n[\n]\n    \n[\n]\n),\n   \n, \n \n0\nh\nt\nK\nD\nt\nT\nh\nt\nb\nt\nT\nh\ni\nD t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\nx\nx\nx\nx\nx\nx\nx\nx\nN\nB\nI\n            (11) \nwhere N  denotes a general nonlinear differential operator;  denotes the random variables \nin the probability space  ; D   denotes the physical domain; T   denotes the time span; \n( ; )\nK\n\nx\n  denotes the space-dependent coefficients, which can be seen as a random field; \n( , ; )\nh\nt \nx\n  denotes the quantity of interest or the solution of the problem; B   denotes the \nboundary condition operator defined on the boundary domain ; and I  denotes the initial \ncondition operator defined at initial time. \n4.1 Parameterization of random field \nAs introduced previously, the space-dependent uncertain model parameter \n( ; )\nK\n\nx\n can \nbe regarded as a random field or random process. In order to efficiently represent and operate \nthe model parameter, some techniques can be adopted to parameterize the random field, such \nas Singular Value Decomposition (SVD) (Tavakoli et al., 2011), Variational Autoencoder (VAE) \n(Canchumuni et al., 2019; Laloy et al., 2017), and Generative Adversarial Network (GAN) \n(Chan & Elsheikh, 2019; Laloy et al., 2018). In this work, Karhunen-Loeve expansion (KLE) \nis utilized for parameterization, which can honor second-order moments. \nIn this work, we assume that \nln ( ; )\nK\n\nx\n  is a Gaussian random field. Let\n( ; )=ln ( ; )\nY\nK\n\n\nx\nx\n, and using KLE, \n( ; )\nY\n\nx\n can be expressed as (Ghanem & Spanos, 2003; \nZhang & Lu, 2004): \n \n11 \n \n1\n( ; )\n( )\n( ) ( )\ni\ni\ni\ni\nY\nY\nf\n\n\n\n\n\n\n\nx\nx\nx\n                  (12) \nwhere \n( )\nY x  denotes the mean of the random field; \ni and \n( )\nif x  denote the eigenvalue and \neigenfunction of the covariance, respectively; and \n( )\ni denotes the independent orthogonal \nGaussian random variables with zero mean and unit variance. The infinite terms in Eq. (12) \ncan be truncated with a finite number of terms (n) for retaining a certain percentage of energy \n(\n1\n1\n/\nn\ni\ni\ni\ni\n\n\n\n\n\n\n\n). The random field \n( ; )\nY\n\nx\n can then be expressed as follows: \n1\n( ; )\n( )\n( ) ( )\nn\ni\ni\ni\ni\nY\nY\nf\n\n\n\n\n\n\nx\nx\nx\n                (13) \nTherefore, the model parameter \n( ; )\nK\n\nx\n  can be parameterized by a vector composed of \nindependent random variables: \n\n\n1\n2\n( ),\n( ),\n,\n( )\nn\n\n\n\n\n\n\nξ\n                   (14) \n4.2 TgNN as deep-learning surrogate \nIn this subsection, the Theory-guided Neural Network (TgNN) surrogate (Wang et al., \n2020a) is constructed for the problem of Eq. (11). In the TgNN surrogate, physical/engineering \nconstraints and other domain knowledge are incorporated as prior knowledge into the neural \nnetwork training process. Considering the governing equation in Eq. (11), in order to \napproximate the quantity of interest \n( , ; )\nh\nt \nx\n, a Deep Neural Network (DNN) is defined as \nfollows (with \n( ; )\nK\n\nx\n parameterized by ξ ): \nˆ( , , ;\n( ,\n )\n( , , ;\n; )\n )\nh\nt\nh\nt\nt\nNN\n\n\n\n\n\nx\nξ\nx\nξ\nx\n               (15) \nwhere \n denotes the parameters of the network, including weights and bias. Therefore, the \nlocation, time, and stochastic parameters comprise the inputs of the neural network, i.e., \n( , , )\nt\nx\nξ , as shown in Figure 1. \nSeveral forward model simulations should be performed to provide training data for \n\n \n12 \n \nsurrogate modeling, and the labeled data points can be represented as \n\n\n\n1\n, ,\n,\nN\ni\ni\ni\ni\ni\nt\nh\n\nx\nξ\n, where \n denotes the total number of labeled training data. Then, the loss function of data mismatch \nbetween network prediction and ground truth can be formulated as mean squared error: \n2\n1\n1\nˆ\n( )\n( ,\n,\n; )\nN\nDATA\ni\ni\ni\ni\ni\ni\nMSE\nh t\nh\nN\n\n\n\n\n\n\nx ξ\n                   (16) \nIn order to achieve theory-guided training, the DNN can then be substituted into the \ngoverning equation and boundary/initial conditions in Eq. (11), and the loss functions for \nphysics-violation can be expressed as follows: \n2\n1\n1\n1\nˆ\n( )\n(\n, ,\n; )\n( )\n( )\n;\nc\nN\nn\nPDE\ni\ni\ni\ni\ni\ni\ni\ni\ni\nc\nMSE\nh\nt\nY\nf\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\nξ\nx\nx\nN\n     (17) \n2\n1\n(\n)\n(\n1\nˆ\n( )\n(\n,\n,\n)\n)\n; \nB\nN\nb\nb\nb\nb\nB\ni\ni\ni\ni\ni\ni\nB\nMSE\nh\nb\nt\nN\n\n\n\n\n\n\nx\nξ\nx\nB\n             (18) \n2\n0\n0\n0\n1\n1\nˆ\n( )\n(\n,0,\n; )\n(\n)\n(\n)\nI\nN\nI\ni\ni\ni\ni\ni\nI\nMSE\nh\ni\nN\n\n\n\n\n\n\nx\nξ\nx\nI\n              (19) \nwhere \n\n1\n, ,\nc\nN\ni\ni\ni\ni\nt\n\nx\nξ\n denote the collocation points to enforce the physical constraints (Raissi et \nal., 2019); \n\n1\n,\n,\nB\nN\nb\nb\nb\ni\ni\ni\ni\nt\n\nx\nξ\n  denote the collocation points to enforce the boundary conditions; \n\n\n0\n0\n1\n,0,\nI\nN\ni\ni\ni\nx\nξ\n denote the collocation points at the initial time to enforce the initial conditions; \nand \nc\nN , \nB\nN , and \nI\nN  denote the total number of collocation points for governing equation, \nboundary conditions, and initial conditions, respectively.  \n \nN\n \n13 \n \n \nFigure 1. Structure of the TgNN surrogate. \n \nConsequently, in order to train the TgNN surrogate, those loss functions can be optimized \nsimultaneously by minimizing a total loss function, which is defined as: \n( )\n( )\n( )\n           \n( )\n( )\nDATA\nDATA\nPDE\nPDE\nB\nB\nI\nI\nL\nMSE\nMSE\nMSE\nMSE\n\n\n\n\n\n\n\n\n\n\n\n\n\n            (20) \nwhere \n, \n, \nDATA\nPDE\nB\n\n\n, and \nI\n control the importance of each term in the total loss function. \nThe optimization can be performed via various algorithms, such as Stochastic Gradient Descent \n(SGD) (Bottou, 2010), Adaptive Moment Estimation (Adam) (Kingma & Ba, 2015), etc. The \nframework of the TgNN surrogate is presented in Figure 1. By incorporating physical laws, \nthe TgNN surrogate can be constructed with limited labeled data, or even in a label-free manner. \nFurthermore, the effect of the number of labeled data and collocation points has been studied \nin Wang et al. (2020a). \n4.3 TgNN surrogate-based inversion methods \nUsing the TgNN surrogate, the model predictions for different model parameter \nrealizations can be easily obtained. Therefore, the inversion tasks may be performed efficiently \nusing the TgNN surrogate. In this work, three different ways to combine the inversion method \nwith the TgNN surrogate are proposed, which will be discussed in the following. \n \n14 \n \n4.3.1 TgNN surrogate-based gradient method \nIn this work, by using KLE, the independent random variables ξ  constitute the model \nparameter m. In addition, forward model prediction can be obtained from the constructed \nTgNN surrogate, i.e., \n\nsurr\nl\ng\nξ\n. Therefore, the gradient method for parameter inversion with \nEq. (5) can be rewritten as follows: \n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n1\nsurr\n1\n1\n1\n          \n1\nT\nT\npr\nl\nl\nl\nl\nD\nl\nl\nl\nl\nl\nT\nT\nobs\nl\nl\nD\nl\nl\nl\nC\nC G\nC\nG C G\nG C\nC\nC G\nC\nG C G\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nξ\nξ\nξ\nξ\nξ\nξ\nξ\nξ\nξ\nξ\nξ\nξ\nd\n  (21) \nSince the TgNN surrogate is constructed with the neural network, the sensitivity matrix \nl\nG  can be calculated through automatic differentiation, which is more efficient than the adjoint \nmethod or the perturbation method for sensitivity calculation. Using the gradient method, we \nmay only update one realization of model parameter, and thus this method may have high \nefficiency. However, the posterior PDF cannot be characterized with only one updated \nrealization. On the other hand, the performance of the gradient method is easily influenced by \nthe initial guess of model parameters. For solving these problems, a group of realizations can \nbe updated with Eq. (21), and the sensitivity matrix \nl\nG   may be replaced by the average \nsensitivity \nl\nG . Then, Eq. (21) can be rewritten as:  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1,\n,\n,\n1\nsurr\n,\n1\n1\n1\n          \n1\n,\n1,\n,\nT\nT\npr\nl\nj\nl j\nl\nl\nD\nl\nl\nl\nl j\nl\nT\nT\nobs\nl\nl\nD\nl\nl\nl j\nj\ne\nC\nC G\nC\nG C G\nG C\nC\nC G\nC\nG C G\ng\nj\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nξ\nξ\nξ\nξ\nξ\nd\n  (22) \nThe uncertainty of the estimated posterior can then be quantified with updated realizations, and \nthe stability of the algorithm can be improved. \nDuring the iteration, the data mismatch between the measurements and predicted data \nfrom the TgNN surrogate is utilized to assess the accuracy of the updated model parameters, \nwhich is defined as follows: \n\n\n2\nsurr\n,\n1\n1\n1\n1\n( )\n(\n)\ne\nd\nN\nN\nobs\nj\nj i\ni\nj\ni\ne\nd\nMIS\ng\nN N\n\n\n\n\n\nξ\nξ\nd\n                (23) \n \n15 \n \nwhere \nd\nN  denotes the total number of measurement data. The terminating criteria for iteration \nare defined as: \n(1) \n1\n;1\n1,\n,\n1\ni\ni\nj Ne\ni n\nl\nj\nl j\nMAX\n\n\n\n\n\n\n\n\n; \n(2) \n1\n2\n(\n)\n( )\nmax(1,\n( ) )\nl\nl\nl\nMIS\nMIS\nMIS\n\n\n\n\nξ\nξ\nξ\n; \n(3) Iteration reaches the pre-given maximum iteration number \nmax\nI\n. \nwhere \n1 and \n2\n are the predefined limits of error; and n  denotes the dimension of the \nmodel parameter.  \n4.3.2 TgNN surrogate-based IES \nUsing the TgNN surrogate, Eq. (10) of IES can be reformulated as:  \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nsurr \nsurr \nsurr \n1\n1,\n,\n,\n1\nsurr \nsurr \nsurr \nobs \n,\n1\n1\n1\n        \n1\n,\n1,\n,\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\npr\nl\nj\nl j\nD\nl\nD\nD D\nD\nl j\nj\nl\nD\nl\nD\nD\nl j\nj\ne\nD\nC\nC\nC\nC\nC\nC\nC\nC\nC\ng\nj\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd\n   (24) \nwhere \nsurr\nl\nl\nD\nC\n and \nsurr \nl\nl\nD D\nC\n can be approximated with: \n\n\n\n\n\n\nsurr\nsurr\nsurr\n,\n,\n,\n,\n1\n1\n1\ne\nl\nl\nN\nT\nD\nl j\nl j\nl j\nl j\nj\ne\nC\ng\ng\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nξ\nξ\n      (25) \n\n\n\n\n\n\n\n\n\n\nsurr \nsurr \nsurr \n,\n,\n1\nsurr \nsurr \n,\n,\n1\n1\n            \ne\nl\nl\nN\nD D\nl j\nl j\nj\ne\nT\nl j\nl j\nC\ng\ng\nN\ng\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nξ\nξ\nξ\nξ\n             (26) \nSince the independent random variables in ξ  are Gaussian random variables with zero \nmean and unit variance, the initial covariance \n0\nC in the Hessian matrix and the C out of \nthe Hessian matrix should be identity matrixes. Therefore, \n1\nC\n can be neglected in Eq. (24). \nMoreover, the termination criteria for iteration can be defined in a similar way, as shown in \nsection 4.3.1. In the IES, since a group of realizations are updated simultaneously, the influence \nof initial guesses is weak. The explicit computation of sensitivity is avoided by using the \n \n16 \n \ncovariance. Since the covariance can be calculated from the ensemble, the implementation of \nIES is simple. Some errors, however, will be incurred from the approximations shown in Eq. \n(8) and Eq. (9). Using the TgNN surrogate, the computational cost of forward model prediction \nduring the iteration can be significantly reduced, and the efficiency of the inversion process \ncan be improved.  \n4.3.3 TgNN surrogate-based training method \nThe above inversion methods are designed by following the optimization framework, \nwhich has similar procedures to that in deep-learning. In the deep-learning framework, a loss \nfunction is designed and minimized during the training process. Here, following the deep-\nlearning framework, another inversion method is proposed, i.e., the training method. With the \nconstructed TgNN surrogate ˆ( , , ; )\nh\nt\n\nx\nξ\n, the objective function Eq. (3) for inverse modeling \ncan be optimized directly as follows: \n\n\n\n\n\n\n\n\nsurr\n1\nsurr\n1\n1\n( )\n( )\n( )\n2\n1\n           \n2\nT\nD\nT\np\nobs\no\nr\npr\nbs\nM\nO\ng\nC\ng\nC\n\n\n\n\n\n\n\n\nξ\nd\nd\nξ\nξ\nξ\nξ\nξ\nξ\n               (27) \nwhere \nsurr( )\ng\nξ  denotes the prediction from the TgNN surrogate. While in the training process, \nthe parameters  of the neural network are fixed, including weights and bias, and the model \nparameters ξ , i.e., the input of the TgNN surrogate, are updated to optimize the objective \nfunction. Various optimization algorithms can be utilized to minimize the objective function, \nand the algorithm Adam (Kingma & Ba, 2015) is adopted here, which can be implemented \neasily with existing deep-learning frameworks, such as Pytorch (Paszke et al., 2017) and \ntensorflow (Abadi et al., 2016). This procedure is similar to the neural network training process, \nand the difference is that the tunable parameters are the inputted random variables, and not the \nweights of the network. Compared with the gradient method or the IES method introduced \npreviously, neither the Gauss-Newton nor the Levenberg–Marquardt algorithm is adopted to \noptimize the objective function, and some built-in algorithms of Pytorch are utilized instead. \nTherefore, the parameter inversion tasks can be simply implemented with many optimization \n \n17 \n \nalgorithms embedded in the existing deep-learning frameworks, and one only needs to set some \nparameters, such as the training iterations and the learning rate. It is worth noting that the \ntraining method can be applied benefiting from the constructed TgNN surrogate, which is \nderivable and efficient to evaluate.  \n5 Direct-deep-learning-inversion Methods \nIn this section, inversion methods directly via the deep-learning framework are introduced. \nDifferent from the deep-learning surrogate-based inversion methods, in this category of \nmethods, it is not necessary to construct the surrogate, and both the measurements of SPDE \nsolutions \n( , ; )\nh\nt \nx\n  and the measurements of model parameters \n( ; )\nK\n\nx\n  are usually \nrequired.  \n5.1 Direct-deep-learning-inversion method without incorporating geostatistics \nInversion methods directly via deep-learning framework have previously been \ninvestigated. Raissi et al. (2019) recently proposed the Physics Informed Neural Network \n(PINN) framework, which is used to infer constant model parameters in the PDEs. In their \nwork, a residual of the governing equation is incorporated into the loss function, and the neural \nnetwork for approximating the solution, as well as the model parameters, are learned together \nduring the training process. Tartakovsky et al. (2020) improved the PINN framework to infer \nheterogeneous parameters, in which both the model parameter field and the SPDE solution are \napproximated with neural networks and substituted into the governing equation to constitute \nthe physics-constrained loss. In this subsection, we will briefly introduce the direct PINN \ninversion method, additional details of which can be found in Raissi et al. (2019) and \nTartakovsky et al. (2020). \nConsider the problem with governing equation Eq. (11), assuming \nk\nN  measurements of \nthe model parameter field and \nh\nN  measurements of the SPDE solution are collected, and two \nfully-connected feed forward neural networks are defined to approximate the model parameter \nand solution, respectively: \n \n18 \n \nˆ ( ; \n)\n( ; \n)\nk\nk\nk\nK\nNN\n\n\n\nx\nx\n                        (28) \nˆ( , ; \n)\n( , ; \n)\nh\nh\nh\nh\nt\nNN\nt\n\n\n\nx\nx\n                        (29) \nThen, the two neural networks can be trained simultaneously by minimizing the loss function: \n\n\n2\n1\n2\n1\n2\n0\n0\n1\n2\n1\n;\n(\n)\n(\n)\n1\nˆ\nˆ\n(\n,\n)\n(\n, ; \n)\n(\n; \n)\n1\nˆ\n              \n(\n,\n; \n)\n1\nˆ\n              \n(\n,0; \n)\n1\n1\nˆ\nˆ\n              \n(\n; \n)\n(\n,\n(\n)\n(\n)\nf\nb\ni\nk\nN\nk\nh\ni\ni\ni\nh\ni\ni\nk\ni\nf\nN\nb\nb\nb\ni\ni\ni\nh\ni\ni\nb\nN\ni\ni\nh\ni\ni\ni\nN\ni\ni\nk\ni\ni\ni\ni\nk\nh\nL\nh\nt\nK\nN\nh\nt\nN\nh\nN\nK\nK\nh\nN\nN\nb\ni\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\nx\nx\nx\nx\nx\nx\nx\nN\nB\nI\n2\n1\n; \n)\nh\nN\ni\nh\ni\ni\nt\nh\n\n\n\n\n(30) \nwhere \nf\nN  ,\nb\nN  , and \ni\nN   denote the number of collocation points for PDE, boundary \nconditions, and initial conditions, respectively. The first three terms in the loss function \n(\n,\n)\nk\nh\nL \n constitute the physics-based loss, and the last two terms represent the data-driven \nloss or the data mismatch (Tartakovsky et al., 2020).  \nAlthough the loss function is similar to that for TgNN surrogate construction, i.e., Eq. \n(20), the following two differences exist: first, the PINN model is constructed for a specific \nconductivity field, and thus no random variables exist in the neural network inputs; and second, \ntwo different neural networks are defined in the PINN model with two sets of network \nparameters (\n,\nk\nh\n ) needing to be trained simultaneously. The two neural networks can be \ntrained with optimization algorithms, such as Adam (Kingma & Ba, 2015). Once trained, the \nnetwork ˆ ( ; \n)\n( ; \n)\nk\nk\nk\nK\nNN\n\n\n\nx\nx\n can provide the estimation of the model parameter field, and \nˆ( , ; \n)\n( , ; \n)\nh\nh\nh\nh\nt\nNN\nt\n\n\n\nx\nx\n can approximate the solution. \nThe PINN method has attracted much attention for forward modeling, and has also been \nused for multiphysics data assimilation in subsurface transport (He et al., 2020). However, two \ndrawbacks exist for this method: first, geostatistical information cannot be honored when \napproximating the model parameter field with a neural network; and second, a relatively large \n \n19 \n \nnumber of direct measurements of model parameter are required to obtain satisfactory accuracy, \nwhich is not realistic in engineering practice. It is worth noting that the TgNN surrogate-based \nmethods described in subsection 4.3 work well even without direct model parameter \nmeasurements. \n5.2 Direct-deep-learning-inversion method constrained with geostatistics \nFor solving the problems with PINN, in this subsection, a direct TgNN inversion method \nis proposed, which can incorporate geostatistical information of the model parameter field and \ncan reduce dependence on the number of direct measurement data. The proposed method \nutilizes physical/engineering principles and domain knowledge, as well as geostatistical \ninformation, and it is named TgNN-geo in this work. For differentiation, the PINN described \nin section 5.1 is named PINN-no-geo in this work.  \nRegarding geostatistical information, KLE provides an effective method for honoring \nsecond-order moments. Therefore, in order to incorporate geostatistical information in TgNN, \na fully-connected neural network can be defined to approximate the relationship between the \nrandom variables ξ  and the random parameter field \n( )\nK x , with the realizations generated \nfrom KLE serving as training data. Here, it is necessary to mention that, if only some training \nimages or realizations are available serving as geostatistical information without knowing the \ntheoretical geostatistical model, certain techniques, such as Variational Autoencoder (VAE) \n(Laloy et al., 2017) and Generative Adversarial Network (GAN) (Laloy et al., 2018), can be \nadopted to construct a mapping from training images/realizations \n( )\nK x  to the latent variables \nξ . For approximating random model parameter fields, the neural network can then be defined \nas: \nˆ ( , ; \n)\n( , ; \n)\npara\npara\npara\nK\nNN\n\n\n\nx ξ\nx ξ\n                   (31) \nTherefore, the loss function can be expressed as: \n2\n1\n1\nˆ\n(\n)\n( , ; \n)\n( ,\n)\npara\nN\npara\npara\npara\ni\ni\ni\npara\nMSE\nK\nK\nN\n\n\n\n\n\n\nx ξ\nx ξ\n          (32) \n \n20 \n \nwhere \n(\n,\n)\ni\ni\nK x ξ\n  denotes data points sampled in the generated realizations of model \nparameters from KLE; and \npara\nN\n  denotes the total number of sampled data points. \nSubsequently, after the minimization of Eq. \n(32), the trained neural network \n( , ; \n)\npara\npara\nNN\n\nx ξ\n can learn the information of spatial correlation from the generated KLE \nrealizations.  \n   The loss function of the direct TgNN inversion method can be written as follows: \n\n\n2\n1\n2\n1\n2\n0\n0\n1\n2\n1\n1\nˆ\nˆ\n(\n,\n)\n(\n, ; \n)\n(\n; \n)\n1\nˆ\n              \n(\n,\n; \n)\n1\nˆ\n              \n(\n,0; \n)\n1\n1\nˆ\nˆ\n              \n(\n;\n;\n(\n)\n(\n)\n(\n)\n(\n \n)\n(\n,\n)\nf\nb\ni\nk\nN\nk\nh\ni\ni\ni\nh\ni\ni\nk\ni\nf\nN\nb\nb\nb\ni\ni\ni\nh\ni\ni\nb\nN\ni\ni\nh\ni\ni\ni\nN\ni\ni\nk\ni\ni\ni\ni\nk\nh\nL\nh\nt\nK\nN\nh\nt\nN\nb\ni\nh\nN\nK\nK\nh\nN\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nξ\nx\nx\nξ\nx\nx\nx\nx\nx\nξ\nx\nN\nB\nI\n2\n1\n; \n)\nh\nN\ni\nh\ni\ni\nt\nh\n\n\n\n\n    (33) \nIt is worth noting that while training the loss function \n(\n,\n)\nk\nh\nL\n\nξ\n , the parameters \npara\n\n  of \n( , ; \n)\npara\npara\nNN\n\nx ξ\n are fixed, and the input random variables \nkξ  are tunable. After finishing \nthe training process, the estimated model parameter field can be obtained from \n( , ; \n)\npara\npara\nNN\n\nx ξ\n by inputting the trained \nkξ . There are two reasons that we use a neural \nnetwork to approximate the random model parameter in Eq. (30) rather than using KLE directly: \nfirst, the differential operator can be easily implemented working with a neural network; and \nsecond, analytical expressions of KLE are not always available for given correlation structures. \nCompared with PINN-no-geo, in TgNN-geo, the network for approximating the model \nparameter is first trained by the realizations generated with KLE or other random field \ngenerators, which contain spatial correlation information. Moreover, during the inversion \nprocess (training the two networks simultaneously), only the inputted random variables \nkξ  \nneed to be updated for the \n( , ; \n)\npara\npara\nNN\n\nx ξ\n network, and thus the parameters of networks \nneeded to be trained are significantly reduced.  \n \n21 \n \n6 Cases Studies \nAlthough the proposed deep-learning based inverse modeling methods are applicable to a \nbroad range of problems, we would like to illustrate the performance of these inversion \nmethods with subsurface flow problems. In this section, several subsurface flow examples are \ndesigned to test the performance of the proposed two categories of deep-learning based \ninversion methods.  \n6.1 Subsurface flow problem \nIn this work, transient saturated subsurface flow problems are considered, which have a \ngeneral form of governing equation: \n( , )\n(\n( )\n( , ))\n0\ns\nh\nt\nS\nK\nh\nt\nt\n\n\n\n\n\nx\nx\nx\n                 (34) \nwhere \nsS   denotes the specific storage; \n( , )\nh\nt\nx\n  denotes the hydraulic head; and \n( )\nK x  \ndenotes the hydraulic conductivity. Due to the spatial variation and limited direct measurements \nof \n( )\nK x , large uncertainty usually exists about \n( )\nK x , and thus in the solution \n( , )\nh\nt\nx\n. As the \nmodel parameters can be treated as random variables/fields, the governing equation becomes a \nstochastic partial differential equation (SPDE). Furthermore, inverse modeling using direct and \nindirect measurements is necessary for accurately inferring \n( )\nK x  and predicting \n( , )\nh\nt\nx\n. \n6.2 TgNN surrogate-based IES \nConsider a two-dimensional dynamic subsurface flow problem, which satisfies the \ngoverning equation of Eq. (34), and is subjected to the following boundary and initial \nconditions: \n0\n|\n202[ ]\nx x\nh\nL\n\n,\n0\n|\n200[ ]\nx\nx x\nL\nh\nL\n\n\n                     (35) \n0\n0\n or \n0\ny\ny y\ny\nL\nh\ny\n\n\n\n\n\n                               (36) \n0\n0,\n|\n200[ ]\nt\nx x\nh\nL\n\n\n                             (37) \nwhere \n0\n0\n(\n,\n)\nx\ny\n  denotes the starting position of the domain; and \nxL   and \ny\nL   denote the \n \n22 \n \nlength of the domain in the respective directions. The domain is a square, and the length in both \ndirections takes a value of \n1020 [ ]\nx\ny\nL\nL\nL\n\n\n (where [ ]\nL  denotes any consistent length unit). \nThe specific storage is assumed to be a constant, taking a value of \n1\n0.0001[\n]\ns\nS\nL\n\n. The log \nhydraulic conductivity ln K   is assumed to be a stationary Gaussian random field with the \nmean and variance given as \nln\n0\nK   and \n2\nln\n1.0\nK\n\n\n , respectively. A separable exponential \ncovariance function is defined for ln K : \n\n\n2\n2\n2\nln\n1\n2\nl\n1\n2\n1\n2\nn\n,\nexp\nK\nK\nx\ny\nx\nx\ny\ny\nC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx x\n             (38) \nwhere \n1\n1\n1\n( ,\n)\nx y\n\nx\n denotes the coordinate of a point in the domain. This particular covariance \nfunction is used, which admits an analytical expression for the KLE (Zhang & Lu, 2004). \nHowever, the framework proposed is applicable to any covariance function, although \nnumerical evaluation of the KLE may be needed. The correlation length in both directions is \nset as \n0.4\n408[ ]\nx\ny\nxL\nL\n\n\n\n\n\n. The ln K  field is parameterized via KLE, and 20 terms are \nretained in the expansion, resulting in 80% energy maintained. The ln K  field can then be \nrepresented by 20 standard Gaussian random variables, \n\n\n1\n2\n20\n( ),\n( ),\n,\n( )\n\n\n\n\nξ\n . A \nrandomly generated ln K   field is chosen as the reference field for the inversion target, as \nshown in Figure 2(a).  \nWith the given reference ln K  field, the flow problem is solved by using MODFLOW \nsoftware. For numerical simulation, the domain is uniformly discretized into 51×51 grids, and \nthe total time span is chosen as 10[ ]\nT  (where [ ]\nT  denotes any consistent time unit), and is \nevenly discretized into 50 time-steps with each time step being 0.2[ ]\nT . Five observation points \nare set in the domain, as shown in Figure 2(b), where the hydraulic head measurements are \ncontinuously collected for the 50 time-steps. Noises are added to the simulation data as \nobservation errors. The noises are assumed to be uncorrelated, and thus the covariance of \nobservation errors constitutes a diagonal matrix. The mean and standard deviation of noises are \n \n23 \n \nset to be 0 and 0.01, respectively.  \n \n \n                        (a)                    (b) \nFigure 2. Reference field (a) and location of observation points (b). \n6.2.1 TgNN surrogate construction \nThe TgNN surrogate is first constructed. A seven-hidden-layer fully-connected neural \nnetwork with 50 neurons per layer is chosen for TgNN surrogate construction. 30 hydraulic \nconductivity fields are generated with KLE, and numerical simulations are then performed to \nobtain training data. Furthermore, 106 collocation points, at which the physical constraints \nare  \n \n \n \n \n24 \n \n \nFigure 3. Reference and prediction from the TgNN surrogate at time-step 25 for three \ndifferent hydraulic conductivity fields. \nimposed, are randomly sampled from high-dimensional parameter space: \n0\n0\n0\n0\n0\n1\n20\n( ,\n)\n(\n,\n)\n(\n,\n)\n(0,1)\n       \n(0,1)\ni\nend\ni\nx\ni\ny\ni\ni\nt\nU t t\nx\nU x\nx\nL\ny\nU y\ny\nL\nN\nN\n\n\n\n\n                          (39) \nwhere U   and N   denote uniform distribution and normal distribution, respectively. The \nAdam algorithm (Kingma & Ba, 2015) is utilized to train the neural network with a constant \nlearning rate of 0.001 for 2,000 epochs. The training process is performed on an NVIDIA \nGeForce RTX 2080 Ti Graphics Processing Unit (GPU) card, which takes approximately 2.2 h \n(7,857.762 s). The constructed TgNN surrogate can predict the hydraulic head distribution for \ndifferent conductivity fields with a relatively high accuracy, three of which are shown in Figure \n3. \n6.2.2 Inversion results with TgNN surrogate-based IES \nThe TgNN surrogate-based IES can then be implemented for inverse modeling. We set \n=100\ne\nN\n, \n1=0.01\n\n, \n2=0.0001\n\n, and \nmax\n10\nI\n\n. The update terminates at the fifth iteration. \nThe mean of the initial, as well as the final updated ln K  realizations are presented in Figure \n4. It can be seen that the estimated ln K  is similar to the reference field, and the major features \nof the reference field have been captured. In order to analyze the uncertainty of the posterior \nln K , the standard deviation of the ensemble at the initial and the final step are also shown in \nFigure 4. It can be seen that the standard deviation of the initial ensemble is relatively large \nsince the realizations are randomly generated with KLE. In addition, the standard deviation \n \n25 \n \nlargely decreases to a low level at the final updating step, which means that the final estimation \nhas low uncertainty. It is worth noting that, since the areas near the boundaries are far from the \nobservation points, higher uncertainty exists in these regions.  \n \n \nFigure 4. Mean and standard deviation of log hydraulic conductivity at initial and final \nstep. \n6.2.3 Effect of ensemble size \nThe sensitivity of the proposed algorithm for ensemble size is investigated in this \nsubsection. TgNN surrogate-based IES is implemented with different ensemble sizes using the \nprevious case. In order to evaluate the accuracy of the estimation quantitatively, the RMSE (root \nmean square error) is introduced as a criterion: \n2\n1\n1\n(ln\nln\n)\ncell\nN\nref\nest\ni\ncell\nRMSE\nK\nK\nN\n\n\n\n\n                 (40) \nwhere the superscripts ref  and est  denote the reference and estimation, respectively; and \ncell\nN\n denotes the total number of grid blocks. The RMSE  of the estimated ln K  from the \nTgNN surrogate-based IES with different \ne\nN  and the corresponding standard deviation for \n50 different parameter initializations are shown in Figure 5(a). It can be seen that RMSE  and \nthe corresponding standard deviation decrease as the size of the ensemble increases, which \nmeans that the results become increasingly accurate and stable as the ensemble size gets larger. \n \n26 \n \nThis can be explained by the fact that the approximation of covariance becomes more accurate \nas the ensemble size increases.  \n \n \n                      (a)                         (b) \nFigure 5. Results for ensemble size effect: (a) mean and standard deviation of RMSE for \ndifferent numbers of realizations; (b) computation time for different numbers of realizations \nwith the TgNN surrogate and simulator. \n \nTo explore the efficiency of the TgNN surrogate-based IES, the inversion consumed time \nfor different ensemble sizes is shown in Figure 5(b). For comparison, the computational cost \nwith MODFLOW for different ensemble sizes is also provided. It can be seen that the \ncomputation time increases as the size of the ensemble increases. However, the elapsed time \nfor inversion with the TgNN surrogate-based IES increases more slowly compared with \nrunning the simulator directly, even for just one iteration, which demonstrates the efficiency of \nthe proposed algorithm. Indeed, the TgNN surrogate construction is time-consuming, but once \ntrained, it can be used for parameter inversion of different cases. \n6.3 TgNN surrogate-based gradient method \nConsider the case introduced in subsection 6.2, and the TgNN surrogate-based gradient \nmethod is implemented. In this method, both the forward evaluation and sensitivity coefficients \ncalculation are implemented with the TgNN surrogate. We set \n1=0.01\n\n , \n2=0.0001\n\n , and \nmax\n10\nI\n\n. The update process terminates at the third iteration, taking only 0.1234 s. The initial \nand final updated hydraulic conductivity field are shown in Figure 6. It can be seen that the \n \n27 \n \nfinal updated ln K  exhibits a similar pattern to the reference field with \n=0.628\nRMSE\n, which \nshows that the estimation achieves satisfactory accuracy. However, unlike the ensemble-based \nmethod, only one realization is updated in the gradient method, and thus uncertainty \nquantification of the posterior model parameter cannot be performed. Additional discussion of \nthis issue will be provided in the next subsection. \n \n \nFigure 6. Initial and final log hydraulic conductivity from the gradient method. \n \n6.3.1 Effect of initialization \nThe stability of the gradient method regarding different initializations of model parameters \nis investigated in this subsection. Inversion tasks are performed with different initial guesses \nof hydraulic conductivity, the results of which are presented in Figure 7 and Table 1. It can be \nseen that the gradient method exhibits relatively poor stability, and performance is affected by \nthe initialization of the hydraulic conductivity. For solving this problem, a strategy is adopted, \nas introduced in subsection 4.3.1, which is similar to ensemble-based method. A group of \nrealizations (\n100\ne\nN \n ) are updated with the gradient method, and the average sensitivity \nmatrix \nl\nG  is utilized when updating the model parameters. Five groups of realizations with \ndifferent initial guesses are updated, and the results are shown in Table 2. It can be seen that \nthe RMSE for different groups are relatively stable. The mean and standard deviation of the \ninitial and final updated realizations in Group 1 are presented in Figure 8, from which one can \nsee that the uncertainty of posterior can be quantified with this method. The consumed time for \nthe inversion process, however, increases significantly compared with updating only one \n \n28 \n \nrealization. Moreover, the mean and standard deviation of RMSE for 50 different group \ninitializations with different realization numbers are shown in Figure 9. It can be seen that the \nresults become increasingly accurate and stable as the number of realizations increases, which \nis similar to the results of the TgNN surrogate-based IES.  \n \n \n \n \n \n \n \n29 \n \nFigure 7. Initial and final updated log hydraulic conductivity with different parameter \ninitializations. \n \nTable 1. Inversion results for different initializations of model parameter. \n \nRMSE \nIteration \nTime (s) \nInitialization 1 \n0.710 \n4 \n0.1185 \nInitialization 2 \n0.758 \n7 \n0.2108 \nInitialization 3 \n0.699 \n4 \n0.1427 \nInitialization 4 \n0.625 \n5 \n0.1523 \nInitialization 5 \n0.892 \n3 \n0.1075 \n \n \n \nFigure 8. Mean and standard deviation of the initial and final updated realizations in \nGroup 1. \n \nTable 2. Inversion results for different group initializations of model parameter. \nInitialization \nRealization Number RMSE Iteration Time (s) \nGroup Initialization 1 \n100 \n0.575 \n3 \n5.5757 \nGroup Initialization 2 \n100 \n0.567 \n3 \n5.5680 \nGroup Initialization 3 \n100 \n0.569 \n3 \n5.5725 \nGroup Initialization 4 \n100 \n0.562 \n3 \n5.4860 \nGroup Initialization 5 \n100 \n0.558 \n3 \n5.6855 \n \n \n30 \n \n \nFigure 9. Mean and standard deviation of RMSE for 50 different group initializations \nwith different numbers of realizations. \n \n6.4 TgNN surrogate-based training method \nIn this subsection, parameter inversion is performed with the TgNN surrogate-based \ntraining method for the same case discussed in subsections 6.2 and 6.3. The ln K   field is \ninitialized with the mean field, as shown in Figure 10(a). The model parameters ξ  are tuned \nin the training process. The Adam algorithm is adopted to minimize the objective function with \nlearning rate of 0.1 for 300 iterations. Similar to the TgNN surrogate-based gradient method in \nsubsection 6.3, only one realization is updated to fit the training data. The RMSE  of estimation \nis 0.662, and the training process only takes 7.0109 s. The estimated ln K  field is shown in \nFigure 10(b), which demonstrates the satisfactory performance of this method. In addition, the \nchange of objective function value during the training process is shown in Figure 10(c). It is \nworth noting that this method can also be implemented in an ensemble manner, as introduced \nin subsection 6.3.1, to quantify the posterior uncertainty and stabilize the method. This case \nvalidates the feasibility of the proposed TgNN surrogate-based training method.  \n \n \n31 \n \n \n \n                 (a)                   (b)                   (c) \nFigure 10. The initial log hydraulic conductivity (a), the final log hydraulic conductivity \nwith training method (b), and the objective function in the training process (c). \n6.5 Data matching and prediction  \nIn this subsection, it is assumed that only the hydraulic heads of the first 30 time-steps are \navailable as observation data, and the hydraulic heads of the last 20 time-steps are utilized for \ntesting the prediction from the estimated model parameters. Consider the case in subsection \n6.2, and the inversion results of three surrogate-based methods are shown in Figure 11 and \nTable 3. It can be seen that the estimated ln K   field can capture the main patterns of the \nreference. However, the estimation is less accurate compared with the results in subsections \n6.2-6.4 because less measurements are available. Indeed, high efficiency can be achieved with \nthe proposed methods, which can be seen from Table 3.  \n \n \n \n(a)                   (b)                  (c) \nFigure 11. Inversion results of three surrogate-based methods: (a) IES;  \n(b) gradient method; (c) and training method. \n \nTable 3. Inversion results of three surrogate-based methods. \n \nRealization Number \nRMSE \nTime (s) \n \n32 \n \nIES \n100 \n0.636 \n4.7468 \nGradient \n1 \n0.776 \n0.1454 \nTraining \n1 \n0.719 \n6.8519 \n \nThe data matching and prediction results of points 1 and 2 are shown in Figure 12, in \nwhich the blue dashed lines indicate the beginning of prediction. The data matching and \nprediction results of remaining points are shown in Appendix A. It can be seen that the \nhydraulic heads from the updated model parameters can match the reference well, even in the \nprediction period. The standard deviation at the initial and final steps for the IES method are \npresented in Figure 13. It can be seen that the final standard deviation of the ensemble increases \nalong the x direction due to the fact that the flow moves from left to right, and the hydraulic \nheads at points 2 and 4 indicated in Figure 12 and Appendix A, respectively, change slowly \nfor the first 30 time-steps. Therefore, less information can be obtained from hydraulic head \nmeasurements from points 2 and 4, which leads to greater uncertainty. \n \n \n   \n \n33 \n \n \nFigure 12. Data matching and prediction results of points 1 and 2 with different \nmethods: IES method (first row); gradient method (second row); and training method (third \nrow). \n \n \nFigure 13. Standard deviation at initial and final steps with the IES method. \n \n6.6 Inversion for high-resolution field \nIn this case, a high-resolution conductivity field is used as a reference. When generating \nthe reference field with KLE, 90% of energy is retained to maintain more specific information \nof the conductivity field, which leads to 60 terms in the expansion. The high-resolution \nreference field is shown in Figure 14(a). The TgNN surrogate previously constructed for the \nlow-resolution conductivity field is employed, in which 20 independent random variables are \nused to parameterize the random field. Here, we aim to test the performance of the proposed \nmethods for estimating the high-resolution field when the TgNN surrogate is constructed for \nthe low-resolution field.  \nThree TgNN surrogate-based methods are implemented for this case, with \n=\ne\nN\n100, 1, \nand 1, respectively. The estimation results of high-resolution ln K  with different methods are \nshown in Figure 14 and Table 4. From the figure, it can be seen that, although the estimation \nfor detailed features is not sufficiently accurate, the major pattern of the high-resolution \n \n34 \n \nreference ln K   field has been captured. Therefore, the proposed methods can be used to \nroughly estimate the property fields of the complicated subsurface formation. In fact, compared \nwith the reference field, the underlying retained percentage of the correlation structure \ninformation is intentionally reduced in this case to test the robustness of the proposed surrogate-\nbased methods. The influence of other prior statistical parameters will be explored further in \nsection 6.9. \n \n \n(a)              (b)               (c)              (d) \nFigure 14. The high-resolution reference field (a) and the estimation results of high-\nresolution field with different methods: (b) IES; (c) gradient method; and (d) training method. \nTable 4. Inversion results for high-resolution reference with three surrogate-based \nmethods. \n \nRealization Number \nRMSE \nTime (s) \nIES \n100 \n0.727 \n4.9113 \nGradient \n1 \n0.639 \n0.1713 \nTraining \n1 \n0.772 \n3.5333 \n6.7 Direct TgNN-geo inversion method \nIn this subsection, the performance of the proposed direct TgNN-geo inversion method is \ntested. The case introduced in subsection 6.2 is considered here and the difference is that, apart \nfrom the measurements of hydraulic heads, the direct measurements of hydraulic conductivity \nare assumed to be available at the observation points. Requiring a number of hydraulic \nconductivity measurements constitutes a disadvantage of the PINN-no-geo method for inverse \nmodeling. Therefore, the proposed TgNN-geo method attempts to alleviate the problem by \nincorporating geostatistical information of the hydraulic conductivity field. \n \n \n35 \n \n \n \n \nFigure 15. The testing results for three randomly generated realizations. \n \n300 hydraulic conductivity fields are generated with KLE, which honor the first two \nmoments of the random field, as introduced in subsection 6.2. The generated hydraulic \nconductivity fields constitute a training dataset, and are used to train a neural network, \nˆ ( , ; \n)\n( , ; \n)\npara\npara\npara\nK\nNN\n\n\n\nx ξ\nx ξ\n . The training process takes approximately 22.95 min \n(1377.17 s) for 2000 epochs with learning rate of 0.001. The testing results of trained \n( , ; \n)\npara\npara\nNN\n\nx ξ\n for three randomly generated realizations are shown in Figure 15. It can \nbe seen that the performance of the trained neural network is satisfactory, which has learned \nthe geostatistical information from KLE. Then, \n( , ; \n)\npara\npara\nNN\n\nx ξ\n can be used to couple with \nthe network ˆ( , ; \n)\n( , ; \n)\nh\nh\nh\nh\nt\nNN\nt\n\n\n\nx\nx\n for inverse modeling via the deep-learning framework. \nThe parameters \npara\n\n of neural network \n( , ; \n)\npara\npara\nNN\n\nx ξ\n are fixed, and the inputs ξ  are \n \n36 \n \ntuned during the inversion process. Consequently, the number of parameters to be trained is \nreduced significantly compared with the PINN-no-geo method.  \nThe measurements of hydraulic head and conductivity at the five observation points \nindicated in Figure 2(b) are used to constitute the data mismatch loss, as illustrated in the last \ntwo terms of Eq. (33). The physics-based loss is also incorporated, as shown in the first three \nterms of Eq. (33). 10,000 collocation points are employed to enforce the physics constraints. \n2,000 epochs of training are performed with learning rate of 0.001. The final estimated result \nwith TgNN-geo, as well as the corresponding error, are presented in Figure 16 and Table 5, \nand the estimated result with PINN-no-geo is also shown for comparison. It is obvious that the \nestimation from TgNN-geo is superior to that from PINN-no-geo given the same number of \nmeasurements. It can also be seen that the PINN-no-geo has a higher degree of freedom without \nconstraining with any geostatistical information to capture the spatial correlation during the \ntraining process, and produces unsatisfactory estimation with sparse measurements. Moreover, \nthe consumed time for direct-deep-learning-inversion is significantly less than that for the \ndeep-learning surrogate construction, as shown in Table 5. This is because the TgNN surrogate \nis constructed for uncertain model parameters, which can predict SPDE solutions for different \nrandom fields, while the direct TgNN-geo method is used for a specific target random field and \na particular problem setup.  \n \n  \n  \n \n \n37 \n \n  \n \nFigure 16. Estimation results and errors with TgNN-geo and PINN-no-geo method. \n \nTable 5. Inversion results with TgNN-geo and PINN-no-geo methods when hydraulic \nconductivity measurements are available. \n \nObservation \nPoints \nCollocation \nPoints \nEpochs Training Time (s) RMSE \nTgNN-geo \n5 \n10000 \n2000 \n290.566 \n0.423 \nPINN-no-geo \n5 \n10000 \n2000 \n305.597 \n0.897 \n \n6.8 Methods comparison \nThe performances of the proposed methods are compared in this subsection. In order to \ncompare the two categories of methods for inverse modeling under the same conditions, two \ntypes of scenarios are considered. The first one considers the case in subsection 6.7, in which \nboth indirect (i.e., hydraulic head) and direct (i.e., hydraulic conductivity) measurements are \navailable at the observation points. In the second case, the direct deep-learning inversion \nmethods, as well as deep-learning surrogate-based methods, are implemented with only the \nhydraulic head measurements. Furthermore, in order to investigate the effect of the number of \nobservation points, different observation location settings are studied for all of the five \nalgorithms discussed in this work.  \n \n \n38 \n \n \n(a) TgNN surrogate-based IES \n \n \n(b) TgNN surrogate-based gradient method \n \n \n(c) TgNN surrogate-based training method \n \n \n39 \n \n \n(d) Direct TgNN-geo inversion method \n \n \n(e) Direct PINN-no-geo inversion method \nFigure 17. Estimation results of the five methods with different numbers of both \nhydraulic head and conductivity measurements. The red dots in the second panel in each case \nindicate the measurement locations. \n \nThe results of the five methods with different numbers of available hydraulic head and \nconductivity measurements are shown in Figure 17, Figure 18(a), and Table 6. The results \nwithout hydraulic conductivity measurements are presented in Figure 18(b). From these \nfigures, it can be seen that the results with both hydraulic head and conductivity measurements \nare more accurate than those without hydraulic conductivity measurements. In addition, the \nperformance gets better as the number of observation points increases. Furthermore, when the \nhydraulic conductivity measurements are not available, both the TgNN-geo and PINN-no-geo \nperform poorly (Figure 18(b)), especially the PINN-no-geo method. \nCompared with TgNN-geo, the TgNN surrogate-based IES method, the gradient method, \nand the training method can provide equivalent accuracy given the same number of hydraulic \nhead and conductivity measurements, as shown in Figure 17 and Figure 18(a). Considering \n \n40 \n \nthe time consumed to construct the TgNN surrogate (7,857.762 s), however, the direct TgNN-\ngeo inversion method is more efficient, which doesn’t require a pre-constructed surrogate. \nHowever, once the TgNN surrogate is constructed, the inversion processes takes a very small \namount of time, as shown in Table 6, and the trained surrogate can be directly used for cases \nwith different setups (e.g., variance, correlation scale), or different types and amount of \nmeasurement data. Furthermore, the direct TgNN-geo inversion method only works well when \ndirect hydraulic conductivity measurements are available, while the TgNN surrogate-based \nmethods do not require this. It can also be seen that both the proposed TgNN surrogate-based \nmethods and the direct TgNN-geo inversion method perform much better than the PINN-no-\ngeo method under sparse measurement data. \n \n \n(a)                                (b) \nFigure 18. RMSE of five methods for different numbers of observations: (a) when both \nhydraulic head and conductivity measurements are available; (b) when only hydraulic head \nmeasurements are available. \n \nTable 6. RMSE and inversion consumed time of the five methods with different \nnumbers of both hydraulic head and conductivity measurements. \n \nTime (s) \nRMSE \nTime (s) \nRMSE \nTime (s) \nRMSE \nTime (s) \nRMSE \nObservation \nPoints \n5 \n9 \n12 \n16 \nIES \n2.4230 \n0.359 \n4.7845 \n0.380 \n6.9057 \n0.325 \n11.3170 \n0.127 \nGradient \n0.0796 \n0.366 \n0.0817 \n0.265 \n0.0866 \n0.213 \n0.0966 \n0.201 \n \n41 \n \nTraining \n11.3061 \n0.470 \n11.7067 \n0.317 \n11.7149 \n0.276 \n11.5279 \n0.214 \nTgNN-geo \n290.5658 \n0.423 \n326.8532 \n0.378 \n314.5828 \n0.298 \n347.8243 \n0.287 \nPINN-no-geo \n305.5972 \n0.897 \n308.5481 \n0.710 \n322.5508 \n0.684 \n328.9147 \n0.574 \n \n6.9 Influence of prior statistics \nConsidering that the prior statistics of the model parameter field may not be accurately \nknown in engineering practice, in this subsection, the influence of the prior statistics of the \nmodel parameter field, such as variance and correlation length, are investigated. Three \nhydraulic conductivity fields with \n2\nln\n0.5\nK\n\n\n, \n2\nln\n2.0\nK\n\n\n, and \n204[ ]\nx\ny\nL\n\n\n\n\n, respectively, are \nchosen as reference fields for three new cases, as shown in Figure 19(a). The color bars are set \nto be the same to highlight the different spatial variability of the reference fields. Other \nstatistical information remains the same as the case in subsection 6.2. 16 observation points are \nselected, where the hydraulic head and/or conductivity measurements are collected, as shown \nin Figure 19(b). \n \n \n                     (a)                                  (b) \nFigure 19.Reference fields with different variance and correlation length (a) and the \nlocation of observation points (b). \n \n6.9.1 Influence of prior statistics for TgNN surrogate-based methods \nThe three new cases are investigated with TgNN surrogate methods in this subsection. \nThe TgNN surrogate is still constructed with \n2\nln\n1.0\nK\n\n\n  and \n408[ ]\nx\ny\nL\n\n\n\n\n  as previously. \nFirstly, the three cases are implemented with only the hydraulic head measurements, and the \n \n42 \n \nresults of the cases with the three TgNN surrogate-based methods are shown in Figure 20 and \nTable 7. It can be seen that the imprecise variance of the field can be easily corrected during \nthe inversion process, and the degree of spatial variability is captured, as shown in Figure 20. \nThe imprecise correlation length has a more obvious impact on the performance, since the \nTgNN surrogate is constructed with preset limited KLE terms (20 terms), and it is difficult to \ntotally recover the high-resolution field, as discussed in subsection 6.6. However, the results \nare still satisfactory, and present similar general patterns to the reference field. From these cases, \nit can be seen that the TgNN surrogate-based methods are effective with imprecise variance \nand correlation length of random model parameter field in a certain range. \nMoreover, the three cases are implemented with both the hydraulic head and conductivity \nmeasurements. The results are shown in Figure 21 and Table 7. It is obvious that the results \nare much better than the cases with only hydraulic head measurements. In addition, when both \nthe hydraulic head and conductivity measurements are available, a similar conclusion can be \ndrawn that the imprecise variance of the field can be more readily corrected than the correlation \nlength. \n \n \n \n43 \n \n \nFigure 20. Results for the three new cases (\n2\nln\n0.5\nK\n\n\n, \n2\nln\n2.0\nK\n\n\n, \n204[ ]\nx\ny\nL\n\n\n\n\n) with \nTgNN surrogate-based methods when only hydraulic head measurements are available. \n \n \n \n \nFigure 21. Results for the three new cases (\n2\nln\n0.5\nK\n\n\n, \n2\nln\n2.0\nK\n\n\n, \n204[ ]\nx\ny\nL\n\n\n\n\n) with \nTgNN surrogate-based methods when both hydraulic head and conductivity measurements \nare available. \n \nTable 7. RMSE of the five methods with imprecise prior statistics. \n \n44 \n \n \n2\nln\n0.5\nK\n\n\n \n2\nln\n2.0\nK\n\n\n \n204[ ]\nx\ny\nL\n\n\n\n\n \nIES (with only h measurements) \n0.434 \n0.538 \n0.815 \nGradient (with only h \nmeasurements) \n0.488 \n0.802 \n0.763 \nTraining (with only h \nmeasurements) \n0.436 \n0.705 \n0.838 \nIES (with both h and lnK \nmeasurements) \n0.090 \n0.231 \n0.666 \nGradient (with both h and lnK \nmeasurements) \n0.138 \n0.576 \n0.624 \nTraining (with both h and lnK \nmeasurements) \n0.209 \n0.486 \n0.747 \nTgNN-geo (with both h and lnK \nmeasurements) \n0.220 \n0.432 \n0.620 \nPINN-no-geo (with both h and \nlnK measurements) \n0.401 \n0.644 \n0.860 \n \n6.9.2 Influence of prior statistics for direct-deep-learning-inversion methods \nThe influence of prior statistics for TgNN-geo and PINN-no-geo is tested here with the \nthree new cases. In this subsection, not only the hydraulic head, but also the hydraulic \nconductivity measurements at the observation points, are assumed to be available to improve \nthe performance of the two direct-deep-leaning-inversion methods, as indicated by the results \nin subsection 6.8. The results of the three new cases with the two direct-deep-leaning-inversion \nmethods are shown in Figure 22 and Table 7. Here, it is important to note that the PINN-no-\ngeo is not affected by imprecise geostatistical information because this information is not \nconsidered in the PINN-no-geo method; whereas, the TgNN-geo is affected by imprecise \ngeostatistical information, which is incorporated when approximating the hydraulic \nconductivity. Similar to the results in subsection 6.9.1, the imprecise variance of the random \nfield is easily corrected, while the imprecise correlation length is more difficult to amend since \nthe random dimension is already settled when constructing the neural network \nˆ ( , ; \n)\n( , ; \n)\npara\npara\npara\nK\nNN\n\n\n\nx ξ\nx ξ\n . However, compared with PINN-no-geo, TgNN-geo still \nperforms better in these cases, as shown in Figure 22 and Table 7, which means that the \n \n45 \n \navailable imprecise geostatistical information is still beneficial. Moreover, when the \ngeostatistical information is largely biased, it may be difficult to achieve satisfactory results \nusing TgNN-geo. Compared with the results of TgNN surrogate-based methods when both the \nhydraulic head and conductivity measurements are available, one can see that TgNN-geo \nachieves equivalent performance without any surrogate. In addition, both the TgNN surrogate \nmethods and TgNN-geo outperform the PINN-no-geo method with imprecise prior statistics.  \n \n \nFigure 22. Results for the three new cases (\n2\nln\n0.5\nK\n\n\n, \n2\nln\n2.0\nK\n\n\n, \n204[ ]\nx\ny\nL\n\n\n\n\n) with \nTgNN-geo and PINN-no-geo methods. \n7 Discussions and conclusions  \nIn this work, two categories of inversion methods are introduced and compared. The first \ncategory is deep-learning surrogate-based methods, including TgNN surrogate-based IES, \nTgNN surrogate-based gradient method, and TgNN surrogate-based training method. The latter \ntwo methods take advantage of the differentiable property of neural networks. The second \ncategory is the direct-deep-learning-inversion method, in which the TgNN-geo is proposed, \nwhich can incorporate geostatistical information. Several two-dimensional subsurface flow \nproblems are designed to test the performance of the proposed methods.  \nFor the TgNN surrogate-based methods, the TgNN surrogate is trained by matching the \navailable simulation data and honoring physical/engineering principles at selected collocation \n \n46 \n \npoints. The TgNN surrogate can be constructed with limited labeled data, or even in a label-\nfree manner, which is an advantage of the TgNN surrogate as studied in Wang et al. (2020a). \nAlthough the training process of the TgNN surrogate may require some computational cost, \nthe trained surrogate accelerates the inversion procedure significantly and can be used for \nsolving new cases, such as varying prior statistics, and different spatial and temporal \nobservations. Furthermore, the differentiability of the TgNN surrogate makes it possible to \nefficiently use the gradient method and training method for inversion tasks.  \nThe performance of the three TgNN surrogate-based methods are first tested with a two-\ndimensional dynamic subsurface flow case, and then more complicated tasks, such as \nextrapolation to future times, high-resolution field estimation, and inversion under imprecise \ngeostatistical information situations, are investigated. Satisfactory results demonstrate the \nrobustness of the proposed methods.  \nThe gradient method needs a sensitivity matrix, which can be obtained from the TgNN \nsurrogate. Updating only one realization is feasible using this method, which is much more \nefficient compared with ensemble-based methods. However, the performance of the gradient \nmethod is easily influenced by the initial guess of model parameter, and the uncertainty of the \nposterior cannot by quantified with only one realization. Nonetheless, the gradient method can \nbe implemented in an ensemble manner, which can assist to improve stability and quantify \nestimation uncertainty. The IES method does not need explicit computation of the sensitivity \nmatrix, and instead the covariance calculated from the ensemble is utilized. In this method, a \ngroup of realizations need to be updated simultaneously, and thus the uncertainty of posterior \ncan be quantified, and the initialization of model parameter has little effect on performance. \nMoreover, the iterative update of realizations in the ensemble can be implemented efficiently \nwith the TgNN surrogate. The training method optimizes the objective function of inverse \nproblems directly with embedded algorithms of deep-learning frameworks, such as Adam \nembedded in Pytorch. Therefore, the training method can be operated easily. Similar to the \ngradient method, only one realization is needed to update, and its feasibility benefits from the \ndifferentiable property of the TgNN surrogate. \n \n47 \n \nThe PINN-no-geo method has been utilized for inverse modeling in the past. However, \nthe requirement of a large number of direct model parameter measurements (e.g., hydraulic \nconductivity) and lack of geostatistical information constraint constitute the major drawbacks \nof this method. The proposed TgNN-geo method deals with this problem by incorporating the \ngeostatistical information of the random field. In TgNN-geo, two neural networks are \nintroduced to approximate the random model parameter and the solution, respectively. In order \nto honor prior geostatistical information of the random model parameter, the neural network \nfor approximating the random model parameter is trained by using the realizations generated \nfrom KLE. By minimizing the loss function of TgNN-geo, estimation of the model parameter \nand approximation of the model solution can be simultaneously obtained. By learning the \ngeostatistical information from KLE, the TgNN-geo works well, even under conditions of \nsparse observation points. Although some geostatistical information may be imprecise, the \navailable partial information is still beneficial. The hydraulic conductivity measurements play \nan important role for TgNN-geo, and without this kind of measurement, performance is \nunsatisfactory. \nThe TgNN surrogate-based methods can also be implemented with hydraulic conductivity \nmeasurements for comparison with the direct TgNN-geo inversion method, and equivalent \nperformance can be achieved. The advantage of the direct TgNN-geo inversion method is that \nno surrogates are needed when solving inverse problems, which can save some computational \ncost. Regarding the requirement of the direct measurements of the model parameters being \ninferred (e.g., hydraulic conductivity), the TgNN surrogate-based methods can work well \nwithout this kind of measurement, which constitutes an advantage of this category of methods.  \n \nAcknowledgements \nThis work is partially funded by the National Natural Science Foundation of China (Grant No. \n51520105005) and the National Science and Technology Major Project of China (Grant No. \n2017ZX05009-005 and 2017ZX05049-003). All of the data of results shown in this paper are \nmade available for download from a public repository of research data through the following \n \n48 \n \nlink: \nhttps://figshare.com/articles/dataset/Data_for_Deep_Learning_based_Inverse_Modeling_App\nroaches/12587048 (DOI: 10.6084/m9.figshare.12587048). \n \nReferences \nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., et al. (2016). Tensorflow: A \nsystem for large-scale machine learning. Paper presented at the 12th Symposium on \nOperating Systems Design and Implementation. \nAdler, J., & Öktem, O. (2017). Solving ill-posed inverse problems using iterative deep neural \nnetworks. Inverse Problems, 33(12), 124007. https://doi.org/10.1088/1361-6420/aa9581 \nAnterion, F., Eymard, R., & Karcher, B. (1989). Use of Parameter Gradients for Reservoir \nHistory Matching. Paper presented at the SPE Symposium on Reservoir Simulation, \nHouston, Texas. https://doi.org/10.2118/18433-MS \nAntholzer, S., Haltmeier, M., & Schwab, J. (2019). Deep learning for photoacoustic \ntomography from sparse data. Inverse Problems in Science and Engineering, 27(7), 987-\n1005. https://doi.org/10.1080/17415977.2018.1518444 \nArridge, S. R. (1999). Optical tomography in medical imaging. Inverse Problems, 15(2), R41-\nR93. http://dx.doi.org/10.1088/0266-5611/15/2/022 \nBottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In \nProceedings of COMPSTAT'2010 (pp. 177-186): Springer. \nBunks, C., Saleck, F. M., Zaleski, S., & Chavent, G. (1995). Multiscale seismic waveform \ninversion. Geophysics, 60(5), 1457-1473. https://doi.org/10.1190/1.1443880 \nCanchumuni, S. W. A., Emerick, A. A., & Pacheco, M. A. C. (2019). Towards a robust \nparameterization for conditioning facies models using deep variational autoencoders and \nensemble \nsmoother. \nComputers \n& \nGeosciences, \n128, \n87-102. \nhttps://doi.org/10.1016/j.cageo.2019.04.006 \nCarrera, J., & Neuman, S. P. (1986a). Estimation of Aquifer Parameters Under Transient and \nSteady State Conditions: 1. Maximum Likelihood Method Incorporating Prior \nInformation. \nWater \nResources \nResearch, \n22(2), \n199-210. \nhttps://doi.org/10.1029/WR022i002p00199 \nCarrera, J., & Neuman, S. P. (1986b). Estimation of Aquifer Parameters Under Transient and \nSteady State Conditions: 2. Uniqueness, Stability, and Solution Algorithms. Water \nResources Research, 22(2), 211-227. https://doi.org/10.1029/WR022i002p00211 \nCarrera, J., & Neuman, S. P. (1986c). Estimation of Aquifer Parameters Under Transient and \nSteady State Conditions: 3. Application to Synthetic and Field Data. Water Resources \nResearch, 22(2), 228-242. https://doi.org/10.1029/WR022i002p00228 \nChan, S., & Elsheikh, A. H. (2019). Parametric generation of conditional geological \nrealizations using generative neural networks. Computational Geosciences, 23(5), 925-\n952. https://doi.org/10.1007/s10596-019-09850-7 \n \n49 \n \nChang, H., Chen, Y., & Zhang, D. (2010). Data assimilation of coupled fluid flow and \ngeomechanics using the ensemble kalman filter. SPE Journal, 15(2), 382-394. \nhttps://doi.org/10.2118/118963-PA \nChang, H., Liao, Q., & Zhang, D. (2017). Surrogate model based iterative ensemble smoother \nfor subsurface flow data assimilation. Advances in Water Resources, 100, 96-108. \nhttp://dx.doi.org/10.1016/j.advwatres.2016.12.001 \nChavent, G., Dupuy, M., & Lemmonier, P. (1975). History Matching by Use of Optimal Theory. \nSPE Journal, 15(01), 74-86. https://doi.org/10.2118/4627-PA \nChen, W. H., Gavalas, G. R., Seinfeld, J. H., & Wasserman, M. L. (1974). A New Algorithm \nfor \nAutomatic \nHistory \nMatching. \nSPE \nJournal, \n14(06), \n593-608. \nhttps://doi.org/10.2118/4545-PA \nChen, Y., & Oliver, D. S. (2012). Ensemble Randomized Maximum Likelihood Method as an \nIterative \nEnsemble \nSmoother. \nMathematical \nGeosciences, \n44(1), \n1-26. \nhttps://doi.org/10.1007/s11004-011-9376-z \nChen, Y., & Oliver, D. S. (2013). Levenberg–Marquardt forms of the iterative ensemble \nsmoother for efficient history matching and uncertainty quantification. Computational \nGeosciences, 17(4), 689-703. https://doi.org/10.1007/s10596-013-9351-5 \nChen, Y., & Zhang, D. (2006). Data assimilation for transient flow in geologic formations via \nensemble Kalman filter. Advances in \nWater Resources, 29(8), 1107-1122. \nhttps://doi.org/10.1016/j.advwatres.2005.09.007 \nEvensen, G. (1994). Sequential data assimilation with a nonlinear quasi‐geostrophic model \nusing Monte Carlo methods to forecast error statistics. Journal of Geophysical Research: \nOceans, 99(C5), 10143-10162. https://doi.org/10.1029/94JC00572 \nGhanem, R. G., & Spanos, P. D. (2003). Stochastic finite elements: a spectral approach. New \nYork: Dover Publications. \nGu, Y., & Oliver, D. S. (2005). History Matching of the PUNQ-S3 Reservoir Model Using the \nEnsemble Kalman Filter. SPE Journal, 10(02), 217-224. https://doi.org/10.2118/89942-\nPA \nGu, Y., & Oliver, D. S. (2007). An Iterative Ensemble Kalman Filter for Multiphase Fluid Flow \nData Assimilation. SPE Journal, 12(04), 438-446. https://doi.org/10.2118/108438-PA \nHe, Q., Barajas-Solano, D., Tartakovsky, G., & Tartakovsky, A. M. (2020). Physics-informed \nneural networks for multiphysics data assimilation with application to subsurface \ntransport. \nAdvances \nin \nWater \nResources, \n141, \n103610. \nhttps://doi.org/10.1016/j.advwatres.2020.103610 \nHoutekamer, P. L., & Mitchell, H. L. (2001). A sequential ensemble Kalman filter for \natmospheric \ndata \nassimilation. \nMonthly \nWeather \nReview, \n129(1), \n123-137. \nhttps://doi.org/10.1175/1520-0493(2001)129<0123:ASEKFF>2.0.CO;2 \nHoutekamer, P. L., Mitchell, H. L., Pellerin, G., Buehner, M., Charron, M., Spacek, L., & \nHansen, B. (2005). Atmospheric data assimilation with an ensemble Kalman filter: Results \nwith \nreal \nobservations. \nMonthly \nWeather \nReview, \n133(3), \n604-620. \nhttps://doi.org/10.1175/MWR-2864.1 \nJin, K. H., McCann, M. T., Froustey, E., & Unser, M. (2017). Deep Convolutional Neural \n \n50 \n \nNetwork for Inverse Problems in Imaging. IEEE Transactions on Image Processing, 26(9), \n4509-4522. https://doi.org/10.1109/TIP.2017.2713099 \nJu, L., Zhang, J., Meng, L., Wu, L., & Zeng, L. (2018). An adaptive Gaussian process-based \niterative ensemble smoother for data assimilation. Advances in Water Resources, 115, 125-\n135. https://doi.org/10.1016/j.advwatres.2018.03.010 \nKingma, D. P., & Ba, J. L. (2015). Adam: A Method for Stochastic Optimization. Paper \npresented at the International conference on learning representations.  \nLaloy, E., Hérault, R., Jacques, D., & Linde, N. (2018). Training‐Image Based Geostatistical \nInversion Using a Spatial Generative Adversarial Neural Network. Water Resources \nResearch, 54(1), 381-406. https://doi.org/10.1002/2017WR022148 \nLaloy, E., Hérault, R., Lee, J., Jacques, D., & Linde, N. (2017). Inversion using a new low-\ndimensional representation of complex binary geological media based on a deep neural \nnetwork. \nAdvances \nin \nWater \nResources, \n110, \n387-405. \nhttps://doi.org/10.1016/j.advwatres.2017.09.029 \nLe, D. H., Emerick, A. A., & Reynolds, A. C. (2016). An Adaptive Ensemble Smoother With \nMultiple Data Assimilation for Assisted History Matching. SPE Journal, 21(06), 2195-\n2207. https://doi.org/10.2118/173214-PA \nLi, R., Reynolds, A. C., & Oliver, D. S. (2003). Sensitivity Coefficients for Three-Phase Flow \nHistory Matching. Journal of Canadian Petroleum Technology, 42(04), 70-77. \nhttps://doi.org/10.2118/03-04-04 \nLi, S., Liu, B., Ren, Y., Chen, Y., Yang, S., Wang, Y., & Jiang, P. (2020). Deep-Learning \nInversion of Seismic Data. IEEE Transactions on Geoscience and Remote Sensing, 58(3), \n2135-2149. https://doi.org/10.1109/TGRS.2019.2953473 \nMo, S., Zabaras, N., Shi, X., & Wu, J. (2019). Deep Autoregressive Neural Networks for \nHigh‐Dimensional Inverse Problems in Groundwater Contaminant Source Identification. \nWater Resources Research, 55(5), 3856-3881. https://doi.org/10.1029/2018WR024638 \nNævdal, G., Johnsen, L. M., Aanonsen, S. I., & Vefring, E. H. (2005). Reservoir monitoring \nand continuous model updating using ensemble Kalman filter. SPE Journal, 10(1), 66-74. \nhttps://doi.org/10.2118/84372-MS \nOliver, D. S., Reynolds, A. C., & Liu, N. (2008). Inverse Theory for Petroleum Reservoir \nCharacterization and History Matching. New York: Cambridge University Press. \nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., et al. (2017). Automatic \ndifferentiation in pytorch. Paper presented at the 31st Conference on Neural Information \nProcessing Systems (NIPS 2017), Long Beach, CA, USA.  \nRaissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks: A \ndeep learning framework for solving forward and inverse problems involving nonlinear \npartial differential equations. Journal of Computational Physics, 378, 686-707. \nhttps://doi.org/10.1016/j.jcp.2018.10.045 \nReichle, R. H., McLaughlin, D. B., & Entekhabi, D. (2002). Hydrologic data assimilation with \nthe \nensemble \nKalman \nfilter. \nMonthly \nWeather \nReview, \n130(1), \n103-114. \nhttps://doi.org/10.1175/1520-0493(2002)130<0103:HDAWTE>2.0.CO;2 \nSkjervheim, J.-a., & Evensen, G. (2011). An Ensemble Smoother for Assisted History Matching. \n \n51 \n \nPaper presented at the SPE Reservoir Simulation Symposium, The Woodlands, Texas, \nUSA. https://doi.org/10.2118/141929-MS \nTartakovsky, A. M., Marrero, C. O., Perdikaris, P., Tartakovsky, G. D., & Barajas‐Solano, D. \n(2020). Physics ‐ Informed Deep Neural Networks for Learning Parameters and \nConstitutive Relationships in Subsurface Flow Problems. Water Resources Research, \n56(5). https://doi.org/10.1029/2019WR026731 \nTavakoli, R., Tavakoli, R., Reynolds, A. C., & Reynolds, A. C. (2011). Monte Carlo simulation \nof permeability fields and reservoir performance predictions with SVD parameterization \nin RML compared with EnKF. Computational Geosciences, 15(1), 99-116. \nhttps://doi.org/10.1007/s10596-010-9200-8 \nVan Leeuwen, P. J., & Evensen, G. (1996). Data assimilation and inverse methods in terms of \na \nprobabilistic \nformulation. \nMonthly \nWeather \nReview, \n124(12), \n2898-2913. \nhttps://doi.org/10.1175/1520-0493(1996)124<2898:DAAIMI>2.0.CO;2 \nWang, N., Chang, H., & Zhang, D. (2020a). Efficient Uncertainty Quantification for Dynamic \nSubsurface Flow with Surrogate by Theory-guided Neural Network. arXiv preprint \narXiv:.13560.  \nWang, N., Zhang, D., Chang, H., & Li, H. (2020b). Deep learning of subsurface flow via \ntheory-guided \nneural \nnetwork. \nJournal \nof \nHydrology, \n584, \n124700. \nhttps://doi.org/10.1016/j.jhydrol.2020.124700 \nWasserman, M. L., Emanuel, A. S., & Seinfeld, J. H. (1975). Practical Applications of Optimal-\nControl Theory to History-Matching Multiphase Simulator Models. SPE Journal, 15(04), \n347-355. https://doi.org/10.2118/5020-PA \nWu, Y., & Lin, Y. (2020). InversionNet: An Efficient and Accurate Data-Driven Full Waveform \nInversion. \nIEEE \nTransactions \non \nComputational \nImaging, \n6, \n419-433. \nhttps://doi.org/10.1109/TCI.2019.2956866 \nWu, Z., Reynolds, A. C., & Oliver, D. S. (1999). Conditioning geostatistical models to two-\nphase production data. SPE Journal, 4(2), 142-155. https://doi.org/10.2118/56855-PA \nYang, P. H., & Watson, A. T. (1988). Automatic History Matching With Variable-Metric \nMethods. SPE Reservoir Engineering, 3(03), 995-1001. https://doi.org/10.2118/16977-PA \nZhang, D. (2001). Stochastic methods for flow in porous media: coping with uncertainties: \nElsevier. \nZhang, D., & Lu, Z. (2004). An efficient, high-order perturbation approach for flow in random \nporous media via Karhunen–Loeve and polynomial expansions. Journal of Computational \nPhysics, 194(2), 773-794. https://doi.org/10.1016/j.jcp.2003.09.015 \nZhou, H., Gómez-Hernández, J. J., Hendricks Franssen, H.-J., & Li, L. (2011). An approach to \nhandling non-Gaussianity of parameters and state variables in ensemble Kalman filtering. \nAdvances \nin \nWater \nResources, \n34(7), \n844-864. \nhttps://doi.org/10.1016/j.advwatres.2011.04.014 \n \n \n \n \n52 \n \nAppendix A \nThis appendix provides the data matching and prediction results for the observation point \n3, 4, and 5 as introduced in subsection 6.5. In this case, only the hydraulic heads of the first 30 \ntime-steps are available, as observation data and the hydraulic heads of the last 20 time-steps \nare utilized for testing the prediction from the estimated model parameters. The blue dashed \nlines in the figures indicate the beginning of prediction. It can be seen that the hydraulic heads \nfrom the updated model parameters can match the reference well in the prediction period. \n \n \n \n \n(a) Data matching and prediction results with the IES method \n \n \n \n(b) Data matching and prediction results with the gradient method \n \n \n53 \n \n \n \n \n(c) Data matching and prediction results with the training method \nFigure A. 1. Data matching and prediction results of points 3, 4, and 5 with different \nmethods: IES method (a); gradient method (b); training method (c). \n \n",
  "categories": [
    "eess.SP",
    "cs.LG",
    "math.OC",
    "physics.comp-ph",
    "stat.ML"
  ],
  "published": "2020-07-28",
  "updated": "2020-07-28"
}