{
  "id": "http://arxiv.org/abs/2007.15778v1",
  "title": "Weakly supervised one-stage vision and language disease detection using large scale pneumonia and pneumothorax studies",
  "authors": [
    "Leo K. Tam",
    "Xiaosong Wang",
    "Evrim Turkbey",
    "Kevin Lu",
    "Yuhong Wen",
    "Daguang Xu"
  ],
  "abstract": "Detecting clinically relevant objects in medical images is a challenge\ndespite large datasets due to the lack of detailed labels. To address the label\nissue, we utilize the scene-level labels with a detection architecture that\nincorporates natural language information. We present a challenging new set of\nradiologist paired bounding box and natural language annotations on the\npublicly available MIMIC-CXR dataset especially focussed on pneumonia and\npneumothorax. Along with the dataset, we present a joint vision language weakly\nsupervised transformer layer-selected one-stage dual head detection\narchitecture (LITERATI) alongside strong baseline comparisons with class\nactivation mapping (CAM), gradient CAM, and relevant implementations on the NIH\nChestXray-14 and MIMIC-CXR dataset. Borrowing from advances in vision language\narchitectures, the LITERATI method demonstrates joint image and referring\nexpression (objects localized in the image using natural language) input for\ndetection that scales in a purely weakly supervised fashion. The architectural\nmodifications address three obstacles -- implementing a supervised vision and\nlanguage detection method in a weakly supervised fashion, incorporating\nclinical referring expression natural language information, and generating high\nfidelity detections with map probabilities. Nevertheless, the challenging\nclinical nature of the radiologist annotations including subtle references,\nmulti-instance specifications, and relatively verbose underlying medical\nreports, ensures the vision language detection task at scale remains\nstimulating for future investigation.",
  "text": "Weakly supervised one-stage vision and language\ndisease detection using large scale pneumonia\nand pneumothorax studies\nLeo K. Tam1, Xiaosong Wang1, Evrim Turkbey2, Kevin Lu1, Yuhong Wen1,\nand Daguang Xu1\n1 NVIDIA, 2788 San Tomas Expy, Santa Clara, CA, 95051\n2 National Institute of Health Clinical Center, 10 Center Dr., Bethesda, MD 20814\nAbstract. Detecting clinically relevant objects in medical images is a\nchallenge despite large datasets due to the lack of detailed labels. To\naddress the label issue, we utilize the scene-level labels with a detection\narchitecture that incorporates natural language information. We present\na challenging new set of radiologist paired bounding box and natural lan-\nguage annotations on the publicly available MIMIC-CXR dataset espe-\ncially focussed on pneumonia and pneumothorax. Along with the dataset,\nwe present a joint vision language weakly supervised transformer layer-\nselected one-stage dual head detection architecture (LITERATI) along-\nside strong baseline comparisons with class activation mapping (CAM),\ngradient CAM, and relevant implementations on the NIH ChestXray-14\nand MIMIC-CXR dataset. Borrowing from advances in vision language\narchitectures, the LITERATI method demonstrates joint image and re-\nferring expression (objects localized in the image using natural language)\ninput for detection that scales in a purely weakly supervised fashion. The\narchitectural modiﬁcations address three obstacles – implementing a su-\npervised vision and language detection method in a weakly supervised\nfashion, incorporating clinical referring expression natural language in-\nformation, and generating high ﬁdelity detections with map probabilities.\nNevertheless, the challenging clinical nature of the radiologist annota-\ntions including subtle references, multi-instance speciﬁcations, and rel-\natively verbose underlying medical reports, ensures the vision language\ndetection task at scale remains stimulating for future investigation.\nKeywords: deep learning · weak supervision · natural language pro-\ncessing · vision language · chest x-ray · electronic health record\n1\nIntroduction\nRecently, the release of large scale datasets concerning chest x-rays has enabled\nmethods that scale with such datasets [7,18,25]. Whereas image classiﬁcation\nimplementations may reach adequate performance using scene-level labels, sig-\nniﬁcantly more eﬀort is required for annotation of bounding boxes around nu-\nmerous visual features of interest. Yet there is detailed information present in\narXiv:2007.15778v1  [cs.CV]  31 Jul 2020\n2\nTam, L.K., Wang, X, et. al.\nreleased clinical reports that could inform natural language (NL) methods. The\nproposed method brings together advances in object detection [5], language [3],\nand their usage together [26,28].\nTypically object detection algorithms are either multi-stage with a region\nproposal stage or single stage with proposed regions scored to a certain object\nclass when proposed [20,4]. The single stage detectors have the beneﬁt of fast\ninference time at often nearly the same performance in accuracy [20,21]. The\nobject detection networks beneﬁt from using the same classiﬁcation network\narchitecture as their image classiﬁcation cousins, where the visual features carry\nsigniﬁcance and shared modularity of networks holds. The modularity of network\narchitectures is further realized with recent vision language architectures.\nVision language networks seek to encode the symbolic and information dense\ncontent in NL with visual features to solve applications such as visual question\nand answering, high ﬁdelity image captioning, and other multi-modal tasks, some\nof which have seen application in medical imaging [26,27,31,16]. Recent advances\nin NLP incorporate the transformer unit architecture, a computational building\nblock that allows the attention of every word to learn an attentional weighting\nwith regards to every other word in a sequence, given standard NLP tasks such as\ncloze, next sentence prediction, etc [24]. Furthermore, deep transformer networks\nof a dozen or more layers trained for the language modeling task (next word\nprediction) were found to be adaptable to a variety of tasks, in part due to\ntheir ability to learn the components of a traditional NLP processing pipeline\n[23]. The combination of NLP for the vision task of object detection centers\naround the issue of visual grounding, namely given a referring phrase, how the\nphrase places an object in the image. The computational generation of referring\nphrases in a natural fashion is a nontrivial problem centered on photographs of\ncluttered scenes, where prevailing methods are based on probabilistically mapped\npotentials for attribute categories [10]. Related detection methods on cluttered\nscenes emphasize a single stage and end-to-end training [10,8].\nIn particular, our method builds on a supervised single stage visual grounding\nmethod that incorporates ﬁxed transformer embeddings [29]. The original one-\nstage methods fuses the referring expression in the form of a transformer embed-\nding to augment the spatial features in the YOLOv3 detector. Our method alters\nto a weakly supervised implementation, taking care to ensure adequate training\nsignal can be propagated to the object detection backbone through the tech-\nnique adapted for directly allowing training signal through a ﬁxed global average\npooling layer [12]. The fast and memory eﬃcient backbone of the DarkNet-53\narchitecture combines with ﬁxed bidirectionally encoded features [3] to visually\nground radiologist-annotated phrases. The ﬁxed transformer embeddings are al-\nlowed increased ﬂexibility through a learned selection layer as corroborated by\nthe concurrent work [14], though here our explicit reasoning is to boost the\nNL information (veriﬁed by ablation study) of a more sophisticated phrasing\nthen encountered in the generic visual grounding setting. To narrow our focus\nfor object detection, we consider two datasets – the ChestXray-14 dataset [25],\nwhich was released with 984 bounding box annotations spread across 8 labels,\nLITERATI: weakly-supervised one-stage detection\n3\nand the MIMIC-CXR dataset [7], for which we collected over 400 board-certiﬁed\nradiologist bounding box annotations. Our weakly supervised transformer layer-\nselected one-stage dual head detection architecture (LITERATI) forms a strong\nbaseline on a challenging set of annotations with scant information provided by\nthe disease label.\n2\nMethods\nThe architecture of our method is presented in Fig. 1 with salient improvements\nnumbered. The inputs of the model are anteroposterior view chest x-ray images\nand a referring expression parsed from the associated clinical report for the study.\nThe output of the model is a probability map for each disease class (pneumonia\nand pneumothorax) as well as a classiﬁcation for the image to generate the\nscene level loss. Intersection over union (IOU) is calculated for the input and\nground truth annotation as A∩B\nA∪B where A is the input and B is the ground truth\nannotation.\nFig. 1. The LITERATI network architecture is a vision language one-stage detector\nadapted from the supervised method of [28] for the weakly supervised case of medical\nimaging visually grounded object detection. The changes are highlighted by numbers\nand described in Sec. 2.2.\n2.1\nPreprocessing\nThe MIMIC-CXR dataset second stage release [7] included a reference label\nﬁle built on the CheXpert labeler [6], which we used for our ﬁltering and data\nselection. The CheXpert labels are used to isolate pneumonia and pneumothorax\nimages and the corresponding chest x-ray reports retrieved by subject and study\nnumber. The images are converted from the full resolution (typically 2544x3056)\n4\nTam, L.K., Wang, X, et. al.\nto 416x416 to match the preferred one-stage resolution. For the ChestXray-14\ndataset, the 1024x1024 PNG ﬁles are converted in PNG format.\nFor the MIMIC-CXR dataset, the radiologist reports are parsed to search\nfor referring expressions, i.e. an object of interest is identiﬁed and located in the\nimage [10]. The referring expression is created using tools adapted from [10].\nNamely, the tooling uses the Stanford CoreNLP [15] parser and the NLTK tok-\nenizer [13] to separate sentences into the R1-R7 attributes and reformed where\nthere is an object in the image as either subject or direct object. Speciﬁcally, the\nreferring phrase consists of the R7 attributes (generics), R1 attributes (entry-\nlevel name), R5 attributes (relative location), and ﬁnally R6 attributes (relative\nobject) [10]. Sample referring phrases in the reports are “conﬂuent opacity at\nbases”, “left apical pneumothorax”, and “multifocal bilateral airspace consoli-\ndation”. As occasionally, the referring phrase does not capture the disease fo-\ncus, the reports are additionally processed to excerpt phrases with “pneumonia”\nand “pneumothorax” to create a disease emphasis dataset split. For example, a\nphrase that does not qualify as a canonical referring expression but is present\nfor the disease is, “vague right mid lung opacity, which is of uncertain etiology,\nalthough could represent an early pneumonia” which is a positive mention, stand-\ning in contrast to, “no complications, no pneumothorax” as a negative mention.\nTo include the presence of normal and negative example images, data negative\nfor pneumothorax and pneumonia was mixed in at an equal ratio to positive\ndata for either category. The experiments on the disease emphasis phrases are\npresented as an interrogative data distribution ablation of the NL function. To\nfurther capture the language function, the scene level label can be submitted\nfor the language embedding itself. For the ChestXray-14 dataset, the scene level\nlabel is the only textual input publicly available.\nDuring the NL ablation, experiments are performed on the MIMIC-CXR\ndataset with three diﬀerent levels of referring phrases provided during training.\nThe tersest level of phrasing is simply the scene level label, which include the\ncases pneumonia, pneumothorax, pneumonia and pneumothorax, or the negative\nphrase ’no pneumo’. The second level and third level are the phrasing described\npreviously. At test time, the full annotation provided by the radiologist is used.\nOnce the referring expressions are in place, the ingredients are set for board-\ncertiﬁed radiologist clinical annotations. We pair the images and highlight rel-\nevant phrases in the clinical report by building functionality on the publicly\navailable MS COCO annotator [2]. The radiologist is given free rein to se-\nlect phrases that are clinically relevant in the report to highlight. The radi-\nologist has annotated 455 clinically relevant phrases with bounding boxes on the\nMIMIC-CXR dataset, which we release at https://github.com/leotam/MIMIC-\nCXR-annotations. As of writing, the annotations constitute the largest disease\nfocussed bounding box labels with referring expressions publicly released, and\nwe hope is a valuable contribution to the clinical visual grounding milieu.\nLITERATI: weakly-supervised one-stage detection\n5\n2.2\nNetwork architecture\nThere are six modiﬁcations to [28] noted in Fig. 1 beyond the parsing discussed.\nTo adapt the network from supervised to weakly supervised, the classiﬁcation\nlayer must be not be trained to reduce partitioning of the data information. To\nachieve that purpose, a global average pooling layer [12] was combined with a\ncross-entropy loss on the scene labels, Fig. 1 (5, 6), to replace the convolution-\nbatch norm-convolution (CBC) layer that generates the bounding box anchors\nin the original implementation. To replace the CBC layer, a deconvolution layer\nnorm layer was prepended to the pooling layer Fig. 1 (3), which additionally\nallowed grounding on an image scale probability map Fig. 1 (7), instead of the\nanchor shifts typically dictated by the YOLOv3 implementation [21].\nFor the NL implementation, the ability of transformer architectures to imple-\nment aspects of the NLP pipeline [23] suggests a trained layer may be successful\nin increasing the expressivity of the language information for the task. Nor-\nmally, a fully connected layer is appended to a transformer model to ﬁne-tune\nfor a given task for various classiﬁcation tasks [19]. The architecture includes a\nconvolutional 1D layer in the language mapping module Fig. 1 (2) that allows\naccess to all the transformer layers instead of the linear layers on the averaged\nlast four transformer layers output in the original implementation [28]. Such a\nmodiﬁcation echos the custom attention on layers mechanism from a concurrent\nstudy on the automatic labeler work [14].\nFor the bounding box generation, we move to the threshold detection method\nsimilar to [25], which diﬀers from the threshold detection method in [28]. The\ncurrent generation method uses the tractable probability map output after the\ndeconvolution-layer norm stage close to the weak supervision signal to select\nregions of high conﬁdence given a neighborhood size. Speciﬁcally, a maximal\nﬁlter is applied to the map probabilities as follows:\nM =\n\u001a\neci\nP\ni eci | ci ∈C\n\u001b\n(1)\nS = {m | max(m ∈M) ∀m | ∥m −x0∥< d}\n(2)\nx0 ≡\nN\nP(x −x0)\nN\n= d | x ∈S.\n(3)\nFirst the M map probabilities are generated from the convolutional outputs\nC via softmax, followed by maximal ﬁltering (with the threshold d as a hyperpa-\nrameter set by tree-structured parzen estimator [1], as are all hyperparameters\nacross methods) to generate the regions S, and then x0 center of masses collected\nas bounding box centers. The original method [21] used the conﬁdence scores to\nassign a probability to a superimposed anchor grid. As the LITERATI output\nis at parity resolution with the image, the deconvolution and maximal ﬁltering\nobviates the anchor grid and oﬀset prediction mechanism.\n6\nTam, L.K., Wang, X, et. al.\n2.3\nTraining\nThe implementation in PyTorch [17] allows for straight-forward data parallelism\nvia the nn.DataParallel class, which we found was key to performance in a timely\nfashion. The LITERATI network was trained for 100 epochs (as per the original\nimplementation) on a NVIDIA DGX-2 (16x 32 GB V100) with 16-way data\nparallelism using a batch size of 416 for approximately 12 hours. The dataset for\nthe weakly supervised case was split in an 80/10/10 ratio for training, test, and\nvalidation respectively, or 44627, 5577, and 5777 images respectively. The test set\nwas checked infrequently for divergence from the validation set. The epoch with\nthe best validation performance was selected for the visual grounding task and\nwas observed to always perform better than the overﬁtted last epoch. For the\nsupervised implementation of the original, there were not enough annotations\nand instead the default parameters from [28] were used.\n3\nResults and discussion\nIn Tab. 1, experiments are compared across the ChestXray-14 dataset. The meth-\nods show the gradient CAM implementation derived from [22,9] outperforms the\ntraditional CAM implementation [30], likely due to the higher resolution local-\nization map. Meanwhile the LITERATI with scene label method improves on the\ngrad CAM method most signiﬁcantly at IOU = 0.2, though lagging at IOU=0.4\nand higher. Above both methods is the jointly trained supervised and unsu-\npervised method from [11]. The LITERATI method and the one-stage method\nwere designed for visual grounding, but the performance for detection on the\nsupervised one-stage method with minimal modiﬁcation is not strong at the\ndataset size here, 215 ChestXray-14 annotations for speciﬁcally pneumonia and\npneumothorax as that is the supervision available.\nTable 1. ChestXray-14 detection accuracy\nIOU\n0.1\n0.2\n0.3\n0.4\n0.5\nCAM [30] WS\n0.505 0.290 0.150 0.075 0.030\nMulti-stage S + WS [11] 0.615 0.505 0.415 0.275 0.180\nGradient CAM WS\n0.565 0.298 0.175 0.097 0.049\nLITERATI SWS\n0.593 0.417 0.204 0.088 0.046\nOne-stage S\n0.115 0.083 0.073 0.021 0.003\nSupervised (S), weakly supervised (WS), and scene-level NLP (SWS) methods\nWhile we include two relevant prior art baselines without language input,\nthe present visual grounding task is more speciﬁcally contained by the referring\nexpression label and at times further removed from the scene level label due to\nan annotation’s clinical importance determined by the radiologist. The data in\nTab. 2 shows detection accuracy on the visual grounding task. The LITERATI\nLITERATI: weakly-supervised one-stage detection\n7\nmethod improves on the supervised method at IOU=0.1 and on gradient CAM\nat all IOUs. We present qualitative results in ﬁg. 2 that cover a range of anno-\nTable 2. MIMIC-CXR detection accuracy\nIOU\n0.1\n0.2\n0.3\n0.4\n0.5\nGradient CAM WS 0.316 0.104 0.049 0.005 0.001\nLITERATI NWS\n0.349 0.125 0.060 0.024 0.007\nOne-stage S\n0.209 0.125 0.125 0.125 0.031\nSupervised (S), weakly supervised (WS), and NLP (NWS) methods\ntations from disease focussed (“large left pneumonia”) to more subtle features\n(“patchy right infrahilar opacity”). Of interest are the multi-instance annota-\ntions (e.g. “multifocal pneumonia” or “bibasilar consolidations”) which would\ntypically fall out of scope for referring expressions, but were included at the radi-\nologist’s discretion in approximately 46 of the annotations. Considering the case\nof “bibasilar consolidations”, the ground truth annotation indicates two sym-\nmetrical boxes on each lung lobe. Such annotations are especially challenging\nfor the one-stage method as it does not consider the multi-instance problem.\nFig. 2. Qualitative detection results using the LITERATI method. The referring ex-\npressions (A-J) are “large left pneumothorax”, “pneumothorax”, “left lower lobe pneu-\nmonia”, “severe right lung consolidation”, “left apical pneumothorax”, “superimposed\nlower lobe pneumonia”, “patchy right infrahilar opacity”, “focal opacities in the lateral\nright mid lung and medial right lower lung”, “hazy retrocardial opacity”, and “mul-\ntifocal pneumonia”. In the top row (A-E), mainly disease focussed referring phrases\nare localized. In the bottom row (F-J), examples of diﬃcult annotations where the\nreferenced location may be vague or spatially ambiguous are presented.\n8\nTam, L.K., Wang, X, et. al.\nThe ablation study on diﬀerent levels of NL information is given in Tab. 3.\nNote, the table retains the same architecture and training procedure from the\nLITERATI description in sec. 2.2. The improvements using detail ranging from\nTable 3. Ablation with diﬀering NL information supplied during training\nIOU\n0.1\n0.2\n0.3\n0.4\n0.5\nScene label\n0.337 0.123 0.048 0.012 0.000\nReferring expression\n0.349 0.125 0.051 0.014 0.002\nReferring disease emphasis 0.349 0.125 0.060 0.024 0.007\nWeakly supervised experiments with varying language input, i.e. scene label\n(“pneumonia”), referring expression (“patchy right infrahilar opacity”), or referring\nexpression with scene label disease (“large left pneumonia”).\nthe scene level label to a full disease related sentence show performance gain at\nhigh IOUs. Although the training NL phrasing diﬀers in the ablation, at test\ntime the phrasing is the same across methods. Since a pretrained ﬁxed BERT\nencoder is used, the ablation probes the adaptability from the NL portion of the\narchitecture to the task. Since the pretrained encoder is trained on a large generic\ncorpora, it likely retains the NLP pipeline (named entity recognition, corefer-\nence disambiguation, etc.) necessary for the visual grounding task. In limited\nexperiments (not shown), ﬁne-tuning on corpora appears to depend granularly\non matching the domain speciﬁc corpora with the task.\n4\nConclusion\nWe present a weakly supervised vision language method and associated clinical\nreferring expressions dataset on pneumonia and pneumothorax chest x-ray im-\nages at scale. The clinical reports generate expressions that isolate discriminative\nobjects inside the images. As parsing into referring expressions is accurate and\nmostly independently of vocabulary (i.e. it’s tractable to identify a direct object\nwithout knowing exactly the meaning of the object) [10], the referring phrases\nrepresent a valuable source of information during the learning process.\nThough not necessarily motivated by learning processes in nature, algorithms\nincluding NL bend towards explainable mechanisms, i.e. the localized image and\nlanguage pairs form clear concepts. The explainable nature of visually grounded\nreferring expressions in a clinical setting, while cogent here, merits further in-\nvestigation on the lines of workﬂow performance. For pure NLP tasks, training\non a data distribution closely matching the testing distribution has encountered\nsuccess. An appropriately matching referring expressions dataset may draw from\nan ontology [25,27] or from didactic literature.\nThe study suggests that vision language approaches may be valuable for\naccessing information within clinical reports.\nLITERATI: weakly-supervised one-stage detection\n9\nReferences\n1. Bergstra,\nJ.,\nBardenet,\nR.,\nBengio,\nY.,\nK´egl,\nB.:\nAlgorithms\nfor\nhyper-\nparameter\noptimization.\nIn:\nShawe-Taylor,\nJ.,\nZemel,\nR.S.,\nBartlett,\nP.L.,\nPereira,\nF.C.N.,\nWeinberger,\nK.Q.\n(eds.)\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n24:\n25th\nAnnual\nConference\non\nNeural\nInformation\nProcessing Systems 2011. Proceedings of a meeting held 12-14 December\n2011, Granada, Spain. pp. 2546–2554 (2011), http://papers.nips.cc/paper/\n4443-algorithms-for-hyper-parameter-optimization\n2. Brooks, J.: Coco annotator. https://github.com/jsbroks/coco-annotator/\n(2019)\n3. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec-\ntional transformers for language understanding. CoRR abs/1810.04805 (2018),\nhttp://arxiv.org/abs/1810.04805\n4. Girshick, R.B.: Fast R-CNN. In: 2015 IEEE International Conference on Computer\nVision, ICCV 2015, Santiago, Chile, December 7-13, 2015. pp. 1440–1448. IEEE\nComputer Society (2015). https://doi.org/10.1109/ICCV.2015.169, https://doi.\norg/10.1109/ICCV.2015.169\n5. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I.,\nWojna, Z., Song, Y., Guadarrama, S., Murphy, K.: Speed/accuracy trade-oﬀs for\nmodern convolutional object detectors. CoRR abs/1611.10012 (2016), http://\narxiv.org/abs/1611.10012\n6. Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund,\nH., Haghgoo, B., Ball, R.L., Shpanskaya, K.S., Seekins, J., Mong, D.A., Halabi,\nS.S., Sandberg, J.K., Jones, R., Larson, D.B., Langlotz, C.P., Patel, B.N., Lungren,\nM.P., Ng, A.Y.: Chexpert: A large chest radiograph dataset with uncertainty labels\nand expert comparison. CoRR abs/1901.07031 (2019), http://arxiv.org/abs/\n1901.07031\n7. Johnson, A.E.W., Pollard, T.J., Berkowitz, S.J., Greenbaum, N.R., Lungren, M.P.,\nDeng, C., Mark, R.G., Horng, S.: MIMIC-CXR: A large publicly available database\nof labeled chest radiographs. CoRR abs/1901.07042 (2019), http://arxiv.org/\nabs/1901.07042\n8. Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: Fully convolutional localization\nnetworks for dense captioning. In: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) (June 2016)\n9. Kao,\nH.:\nGradcam\non\nchexnet\n(Mar\n2020),\nhttps://github.com/thtang/\nCheXNet-with-localization\n10. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.L.: Referitgame: Referring to\nobjects in photographs of natural scenes. In: Moschitti, A., Pang, B., Daelemans,\nW. (eds.) Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meet-\ning of SIGDAT, a Special Interest Group of the ACL. pp. 787–798. ACL (2014).\nhttps://doi.org/10.3115/v1/d14-1086, https://doi.org/10.3115/v1/d14-1086\n11. Li, Z., Wang, C., Han, M., Xue, Y., Wei, W., Li, L., Li, F.: Thoracic disease\nidentiﬁcation and localization with limited supervision. CoRR abs/1711.06373\n(2017), http://arxiv.org/abs/1711.06373\n12. Lin, M., Chen, Q., Yan, S.: Network in network. In: Bengio, Y., LeCun, Y. (eds.)\n2nd International Conference on Learning Representations, ICLR 2014, Banﬀ, AB,\nCanada, April 14-16, 2014, Conference Track Proceedings (2014), http://arxiv.\norg/abs/1312.4400\n10\nTam, L.K., Wang, X, et. al.\n13. Loper, E., Bird, S.: NLTK: the natural language toolkit. CoRR cs.CL/0205028\n(2002), https://arxiv.org/abs/cs/0205028\n14. Lyubinets, V., Boiko, T., Nicholas, D.: Automated labeling of bugs and tick-\nets using attention-based mechanisms in recurrent neural networks. CoRR\nabs/1807.02892 (2018), http://arxiv.org/abs/1807.02892\n15. Manning, C.D., Surdeanu, M., Bauer, J., Finkel, J.R., Bethard, S., McClosky,\nD.: The stanford corenlp natural language processing toolkit. In: Proceedings of\nthe 52nd Annual Meeting of the Association for Computational Linguistics, ACL\n2014, June 22-27, 2014, Baltimore, MD, USA, System Demonstrations. pp. 55–60.\nThe Association for Computer Linguistics (2014). https://doi.org/10.3115/v1/p14-\n5010, https://doi.org/10.3115/v1/p14-5010\n16. Moradi, M., Madani, A., Gur, Y., Guo, Y., Syeda-Mahmood, T.: Bimodal network\narchitectures for automatic generation of image annotation from text. In: Frangi,\nA.F., Schnabel, J.A., Davatzikos, C., Alberola-L´opez, C., Fichtinger, G. (eds.)\nMedical Image Computing and Computer Assisted Intervention – MICCAI 2018.\npp. 449–456. Springer International Publishing, Cham (2018)\n17. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,\nLin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K¨opf, A., Yang, E., DeVito, Z.,\nRaison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala,\nS.: Pytorch: An imperative style, high-performance deep learning library. CoRR\nabs/1912.01703 (2019), http://arxiv.org/abs/1912.01703\n18. Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D.Y.,\nBagul, A., Langlotz, C., Shpanskaya, K.S., Lungren, M.P., Ng, A.Y.: Chexnet:\nRadiologist-level pneumonia detection on chest x-rays with deep learning. CoRR\nabs/1711.05225 (2017), http://arxiv.org/abs/1711.05225\n19. Rajpurkar, P., Jia, R., Liang, P.: Know what you don’t know: Unanswerable ques-\ntions for squad. CoRR abs/1806.03822 (2018), http://arxiv.org/abs/1806.\n03822\n20. Redmon, J., Divvala, S.K., Girshick, R.B., Farhadi, A.: You only look once: Uniﬁed,\nreal-time object detection. CoRR abs/1506.02640 (2015), http://arxiv.org/\nabs/1506.02640\n21. Redmon,\nJ.,\nFarhadi,\nA.:\nYolov3:\nAn\nincremental\nimprovement.\nCoRR\nabs/1804.02767 (2018), http://arxiv.org/abs/1804.02767\n22. Selvaraju, R.R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., Batra, D.: Grad-\ncam: Why did you say that? visual explanations from deep networks via gradient-\nbased localization. CoRR abs/1610.02391 (2016), http://arxiv.org/abs/1610.\n02391\n23. Tenney, I., Das, D., Pavlick, E.: BERT rediscovers the classical NLP pipeline.\nCoRR abs/1905.05950 (2019), http://arxiv.org/abs/1905.05950\n24. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. CoRR abs/1706.03762 (2017), http:\n//arxiv.org/abs/1706.03762\n25. Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8:\nHospital-scale chest x-ray database and benchmarks on weakly-supervised clas-\nsiﬁcation and localization of common thorax diseases. CoRR abs/1705.02315\n(2017), http://arxiv.org/abs/1705.02315\n26. Wang, X., Peng, Y., Lu, L., Lu, Z., Summers, R.M.: Tienet: Text-image embedding\nnetwork for common thorax disease classiﬁcation and reporting in chest x-rays.\nCoRR abs/1801.04334 (2018), http://arxiv.org/abs/1801.04334\nLITERATI: weakly-supervised one-stage detection\n11\n27. Yan, K., Wang, X., Lu, L., Summers, R.M.: Deeplesion: Automated deep mining,\ncategorization and detection of signiﬁcant radiology image ﬁndings using large-\nscale clinical lesion annotations. CoRR abs/1710.01766 (2017), http://arxiv.\norg/abs/1710.01766\n28. Yang, Z., Gong, B., Wang, L., Huang, W., Yu, D., Luo, J.: A fast and accurate one-\nstage approach to visual grounding. In: 2019 IEEE/CVF International Conference\non Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019. pp. 4682–4692. IEEE (2019). https://doi.org/10.1109/ICCV.2019.00478,\nhttps://doi.org/10.1109/ICCV.2019.00478\n29. Yang, Z., Gong, B., Wang, L., Huang, W., Yu, D., Luo, J.: A fast and accurate\none-stage approach to visual grounding. CoRR abs/1908.06354 (2019), http:\n//arxiv.org/abs/1908.06354\n30. Zhou, B., Khosla, A., Lapedriza, `A., Oliva, A., Torralba, A.: Learning deep\nfeatures for discriminative localization. CoRR abs/1512.04150 (2015), http:\n//arxiv.org/abs/1512.04150\n31. Zhu, W., Vang, Y.S., Huang, Y., Xie, X.: Deepem: Deep 3d convnets with em for\nweakly supervised pulmonary nodule detection. In: Frangi, A.F., Schnabel, J.A.,\nDavatzikos, C., Alberola-L´opez, C., Fichtinger, G. (eds.) Medical Image Comput-\ning and Computer Assisted Intervention – MICCAI 2018. pp. 812–820. Springer\nInternational Publishing, Cham (2018)\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "published": "2020-07-31",
  "updated": "2020-07-31"
}