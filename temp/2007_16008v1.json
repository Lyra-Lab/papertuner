{
  "id": "http://arxiv.org/abs/2007.16008v1",
  "title": "Multi-task learning for natural language processing in the 2020s: where are we going?",
  "authors": [
    "Joseph Worsham",
    "Jugal Kalita"
  ],
  "abstract": "Multi-task learning (MTL) significantly pre-dates the deep learning era, and\nit has seen a resurgence in the past few years as researchers have been\napplying MTL to deep learning solutions for natural language tasks. While\nsteady MTL research has always been present, there is a growing interest driven\nby the impressive successes published in the related fields of transfer\nlearning and pre-training, such as BERT, and the release of new challenge\nproblems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place\nmore focus on how weights are shared across networks, evaluate the re-usability\nof network components and identify use cases where MTL can significantly\noutperform single-task solutions. This paper strives to provide a comprehensive\nsurvey of the numerous recent MTL contributions to the field of natural\nlanguage processing and provide a forum to focus efforts on the hardest\nunsolved problems in the next decade. While novel models that improve\nperformance on NLP benchmarks are continually produced, lasting MTL challenges\nremain unsolved which could hold the key to better language understanding,\nknowledge discovery and natural language interfaces.",
  "text": "Multi-task learning for natural language processing in the 2020s:\nwhere are we going?\nJoseph Worsham\nUniversity of Colorado Colorado Springs\nJugal Kalita\nUniversity of Colorado Colorado Springs\nAbstract\nMulti-task learning (MTL) signiﬁcantly pre-dates the\ndeep learning era, and it has seen a resurgence in\nthe past few years as researchers have been apply-\ning MTL to deep learning solutions for natural lan-\nguage tasks. While steady MTL research has always\nbeen present, there is a growing interest driven by\nthe impressive successes published in the related ﬁelds\nof transfer learning and pre-training, such as BERT,\nand the release of new challenge problems, such as\nGLUE and the NLP Decathlon (decaNLP). These\neﬀorts place more focus on how weights are shared\nacross networks, evaluate the re-usability of network\ncomponents and identify use cases where MTL can\nsigniﬁcantly outperform single-task solutions.\nThis\npaper strives to provide a comprehensive survey of\nthe numerous recent MTL contributions to the ﬁeld\nof natural language processing and provide a forum\nto focus eﬀorts on the hardest unsolved problems in\nthe next decade. While novel models that improve\nperformance on NLP benchmarks are continually pro-\nduced, lasting MTL challenges remain unsolved which\ncould hold the key to better language understanding,\nknowledge discovery and natural language interfaces.\n1\nIntroduction\nMulti-task learning (MTL) is a collection of tech-\nniques intended to improve generalization, strengthen\nlatent representations and enable domain adapta-\ntion within the ﬁeld of machine learning (Caruana,\n1997).\nIt has been applied to feed-forward neural\nnetworks (Caruana, 1997), decision trees (Caruana,\n1997), random forests (Wang et al., 2008), Gaussian\nProcesses (Bonilla et al., 2008), support-vector ma-\nchines (Jebara, 2004) and, most recently, deep neural\nnetworks (Ruder, 2017) across a broad range of do-\nmains. This includes speciﬁc deep learning architec-\ntures such as MTL seq2seq models (Velay and Daniel,\n2018) and MTL transformers (Liu et al., 2019a). It\nhas been shown that under certain circumstances, and\nwith well-crafted tasks, MTL can help models achieve\nstate-of-the-art performance on a range of diﬀerent\ntasks (Standley et al., 2019). It has also been shown,\nhowever, that MTL can be extremely fragile and sen-\nsitive to both the selected tasks and the training pro-\ncess which leads to models that signiﬁcantly under-\nperform when compared to the best single-task mod-\nels (Alonso and Plank, 2017). While MTL has been\na subject of research for multiple decades (Ruder,\n2017), there still exist a number of unsolved prob-\nlems, unexplored questions and shortcomings in pro-\nduction systems which are addressed within.\nThis\nsurvey will present a condensed summary of the large\nlibrary of current MTL research applied to natural\nlanguage processing (NLP) and present a set of goals\nintended to help highlight the MTL problems that we\nshould strive to solve in the next decade.\n2\nCharacterizing\nMulti-Task\nLearning\nMTL introduces additional training objectives to a\nlearning system to bias the learner with a broader un-\nderstanding through solving related tasks. The end-\ngoal is to improve performance on a set of primary\ntasks through the inductive bias introduced by the\nadditional tasks (Caruana, 1997). The set of primary\ntasks are referred to as the target task set, and addi-\ntional tasks, which are used to improve performance\non the target set, belong to the auxiliary task set.\nWhile this is the standard approach (Ruder, 2017),\nc⃝2020. This manuscript version is made available under the CC-BY-NC-ND 4.0 license\nhttp://creativecommons.org/licenses/by-nc-nd/4.0/\n1\narXiv:2007.16008v1  [cs.CL]  22 Jul 2020\nothers have also designed MTL models with no aux-\niliary focusing on competitively solving all the tasks\njointly (McCann et al., 2018).\nIn practice, MTL is closely related to Transfer\nLearning (TL) (Baxter, 1998), as the goal of each is\nto improve the performance of a target task or do-\nmain through the use of related tasks and domains.\nA task is deﬁned as a speciﬁc operational capability,\nsuch as Part of Speech Tagging or Question Answer-\ning. Tasks traditionally do not share the same output\nfeatures or solution space. A domain is a certain fea-\nture space and underlying data generating process.\nWhen working with TL and MTL, commonly, diﬀer-\nent domains share the same feature space, but have\ndiﬀerent data generating processes.\nWhile there is\nno limit on how much variance can exist in diﬀerent\nbut related domains, a common example in NLP is to\ntreat diﬀerent languages as diﬀerent domains (Zoph\net al., 2016).\nBoth TL and MTL can make use of\ndiﬀering domains and diﬀering tasks.\nTransfer Learning is broken down into three dif-\nferent categories based on what diﬀers between the\nsource and the target (Redko et al., 2019).\nIf the\nsource and target share the same task with diﬀerent\ndomains, this is called Transductive Transfer Learn-\ning, commonly known as Domain Adaptation. If the\nsource and target share the same domain with dif-\nferent tasks this is Inductive Transfer Learning which\nlearns to transfer via inductive bias. If the source and\ntarget have diﬀerent domains and diﬀerent tasks this\nis a form of unsupervised Transfer Learning which\nlearns common representations despite having no di-\nrect similarities between the source and the target.\nWith this TL taxonomy, we formulate a related\nbreakdown for MTL. While MTL terminology is tra-\nditionally focused on varying tasks it is also pos-\nsible to train jointly on diﬀerent domains.\nIf the\nsource task and auxiliary tasks are the same with\ndiﬀerent domains, we label this Transductive Multi-\nTask Learning, or Domain Regularization. When the\nsource task and auxiliary tasks are diﬀerent but share\nthe same domain this is the standard form of MTL\nwhich we formally identify as Inductive Bias MTL.\nFinally, if the source and auxiliary tasks are diﬀer-\nent and do not share the same domain we call it\nMulti-Task Feature Learning, originally introduced\nby Romera-Paredes et al. (2012) Table 1 shows this\nbreakdown for both TL and MTL.\nTable 1: TL / MTL Task and Domain Categories\nSame Tasks\nDiﬀerent Tasks\nSame Domains\nStandard Learning Setting\nInductive\nTransfer\nLearning\nInductive Bias\n(standard) MTL\nDiﬀerent Domains\nTransductive\nTransfer Learning\n(Domain Adapation)\nTransductive MTL\n(Domain Reuglarization)\nUnsupervised\nTransfer Learning\nMulti-Task\nFeature Learning\nA relational representation of these concepts is pro-\nvided in Figure 1.\nThis shows that TL and MTL\nshare some overlap, indicating that these techniques\ncan be used together. While Standley et al. (2019)\nshow that there are signiﬁcant diﬀerences in the types\nof tasks proven to be useful in MTL vs. TL, Bingel\nand Søgaard (2017) argue that they produce similar\nobserved beneﬁts. Liu et al. (2019a) show that TL\nand MTL are complementary to one another when\nused in combination to train a complex model, which\nis in contradiction to earlier work that showed, in dif-\nferent circumstances, this combination yielded no sig-\nniﬁcant improvement (Mou et al., 2016). These tech-\nniques also overlap with standard single-task learning\nthrough a method called zero-shot learning.\nZero-\nshot learners, or generalist agents, are capable of\njointly understanding many tasks or concepts with no\nﬁne-tuning on speciﬁc tasks (McCann et al., 2018).\n3\nWhen\nto\nUse\nMulti-Task\nLearning\nOne of the biggest needs for a successful machine\nlearning system is access to extremely large amounts\nof labeled data.\nMTL is proposed as a technique\nto help overcome data sparsity by learning to jointly\nsolve related or similar problems to produce a more\ngeneralized internal representation. Regardless of the\nnumber of target tasks to solve, MTL can only be\nconsidered useful when at least one target task is im-\nproved upon when compared to a collection of single-\ntask models (Standley et al., 2019).\nAlong with enabling zero-shot learning (McCann\net al., 2018), MTL is commonly presented as a regu-\nlarization technique to aid in the generalization of a\ntask to unseen examples (Caruana, 1997; Luong et al.,\n2015; Liu et al., 2019a; Radford et al., 2019). This be-\nlongs to the Transductive MTL class in Table 1.\nAdditionally, MTL has desirable traits when it\n2\nFigure 1: Relationship of Machine Learning Concepts with a Focus on Transfer Learning and Multi-Task\nLearning\ncomes to eﬃciency. Stickland and Murray (2019) de-\nscribe a well designed MTL model to be computa-\ntionally less complex, to have fewer parameters lim-\niting the burden on memory and storage and to ul-\ntimately require less power consumption at inference\ntime. Standley et al. (2019) also point out the de-\nsirable trait of quicker inferencing depending on the\narchitecture of the MTL model. These characteris-\ntics pertain to neural network MTL implementations\nwhich are less expensive than deploying a complete\nmodel for each class individually.\n4\nMTL Implications and Dis-\ncoveries\nResearchers have been studying the implications and\nnuances of MTL when compared to traditional single-\ntask training since its introduction. Given the human\nintuition of how MTL can help improve model perfor-\nmance, practitioners are often surprised at how del-\nicate and sensitive these algorithms can be (Alonso\nand Plank, 2017; Standley et al., 2019). This section\nwill discuss MTL discoveries in this regard through\nthe topics of task relationship, dataset diversity,\nmodel design considerations and training curriculum.\nTechniques identiﬁed in this section are shown rela-\ntionally in Figure 2.\n4.1\nTask Selection\nThe similarities between a set of tasks are commonly\ncited as one of the most inﬂuential design factors in\nbuilding MTL systems. Through a series of exper-\niments, Caruana (1997) showed that the beneﬁt of\nMTL is due to the direct knowledge learned from the\nauxiliary tasks. He further showed that some induc-\ntive bias can actually harm performance. A host of\nother researchers have gone on to argue that task re-\nlatedness plays a key role in determining how knowl-\nedge information is shared (Mou et al., 2016; Ben-\nDavid and Borbely, 2008) and when an auxiliary task\nwill help and when it will hurt (Standley et al., 2019;\nKim et al., 2019).\nStandley et al. (2019) continued to explore this con-\ncept and showed that tasks which seem related can of-\nten have underlying complex and competing dynam-\nics. They present a table showing how every factor-\nized task pair performed in relation to a single-task\nmodel, and, via the hypothesis that related tasks im-\nprove performance, show which tasks they believe to\nbe related. While this work was not performed on a\nset of NLP tasks, it showed the importance of task\nrelationship and provided a novel way to measure re-\nlatedness.\nAnother unique observation published by Standley\net al. (2019) shows that tasks which are beneﬁcial in\na TL environment appear to perform poorly as auxil-\n3\niary tasks in an MTL setting. This raises a question\non whether or not dissimilar tasks could be used in-\ntentionally to help regularize a system or even protect\na system from adversarial attack. While this observa-\ntion seems to disagree with the conventional wisdom\nthat only similar or related tasks can lead to improved\nmodel performance (Ben-David and Borbely, 2008),\nthey are not alone. Romera-Paredes et al. (2012) have\nalso shown that unrelated tasks can still be beneﬁcial.\nThis poses a unique opportunity to further explore\ntask relationships and usefulness on the most recent\nMTL benchmarks.\n4.2\nMTL Dataset Considerations\nThe datasets used for target tasks and auxiliary tasks\nplay an important role in building successful MTL\nsystems. The ﬁrst topic addressed considers how the\nsize and diversity of the datasets impact the learning\nof a model. Luong et al. (2015) perform a set of ex-\nperiments to determine how the size of the datasets\nfor the target and auxiliary tasks impact the overall\nresults of the model on the target set.\nThey show\nthat the size ratio between a target task dataset and\nan auxiliary task dataset does have an impact on per-\nformance. They argue that when the target dataset\nis large, the best MTL performance is achieved with\na small auxiliary dataset with a size ratio between\n0.01 and 0.1. When this mixing ratio gets too high it\nis shown that the model will overﬁt to the auxiliary\ntask at the cost of performance on the target task.\nOther researchers agree that the best performance is\nachieved with a small number of auxiliary task up-\ndates compared to target task updates (Alonso and\nPlank, 2017; Kim et al., 2019) and that adding more\ndata to a poorly selected auxiliary task can signiﬁ-\ncantly harm the model (Kim et al., 2019).\nResearchers have also considered the underlying\nproperties and statistics to determine how they im-\npact MTL performance. A theoretical deﬁnition of\nMTL and task relatedness is presented by Ben-David\nand Borbely (2008). The goal of this work is to de-\nvelop a formulated approach for determining when\nMTL is advantageous and to what degree. They seek\na theoretical justiﬁcation for task relatedness based\non measurable similarities found within the under-\nlying data generating processes of each task. While\ntheir deﬁnition of task more closely relates to the def-\ninition of a domain within this survey, they establish\nformal error bounds to measure and learn task relat-\nedness.\nRecent work has gone on to argue that size is not\na useful metric for determining MTL gain (Bingel\nand Søgaard, 2017). Research has shown that sim-\nple tasks, requiring few training iterations, and diﬃ-\ncult tasks, which struggle to converge on a solution,\ndo not lead to the development of useful MTL rep-\nresentations (Caruana, 1997; McCann et al., 2018).\nAlonso and Plank (2017) argue that MTL task se-\nlection should be addressed via data properties, not\nintuition on what a human performer may consider\neasy. They perform a set of studies that measure sta-\ntistical distributions of supervised labels in auxiliary\ntask datasets and ﬁnd that the best performance is\nachieved when the auxiliary tasks have compact mid-\nentropy distributions. That is to say, the best aux-\niliary tasks are neither too easy to predict nor too\ndiﬃcult to learn.\nAnother perspective on underlying properties of the\nauxiliary datasets is to consider the loss produced by\neach task while learning. The magnitude of the task\nloss can be considered a task similarity metric. Stand-\nley et al. (2019) show that imbalanced tasks in this re-\ngard produce largely varied gradients which can con-\nfuse model training. Oftentimes, task selection is not\nsomething that can be changed, but Standley et al.\n(2019) recommend using a task weighting coeﬃcient\nto help normalize the gradient magnitudes across the\ntasks. Similar to task loss, the learning curve, show-\ning how loss decreases over training, is also proposed\nas a metric for task similarity.\nIt was found that\nMTL gains are more likely when a target task’s learn-\ning curve plateaus early in training and the auxiliary\ntasks do not plateau (Bingel and Søgaard, 2017).\nIt is also worth noting that Data Augmentation\nis a proven technique to help overcome data spar-\nsity and improve task performance in TL and MTL\nsettings.\nAnaby-Tavor et al. (2019) propose LAM-\nBADA, a language generator model ﬁne-tuned on\na small task-speciﬁc dataset, which generates semi-\nsupervised training data to augment the data avail-\nable to a task speciﬁc language classiﬁcation task.\n4.3\nModel Selection and Design\nThere is a large body of research considering how\nMTL inﬂuences model selection and design. While\nthe authors acknowledge the importance of other ma-\n4\nFigure 2: Relationships of Transfer Learning and Multi-Task Learning Techniques for Deep Learning\nchine learning models, this section will focus solely on\nneural networks due to recent deep learning trends.\nFigure 2 shows the primary MTL techniques for Deep\nLearning and their relationships. There are two pri-\nmary families of MTL neural network approaches:\nhard parameter sharing and soft parameter sharing\n(Ruder, 2017).\nHard parameter sharing is the ap-\nproach most closely related to traditional machine\nlearning techniques and the same mechanism used by\nmany transfer learning solutions. In hard parameter\nsharing, full network layers and their parameters are\nshared directly between tasks. Soft parameter sharing\nis more specialized and instead creates separate layer\nparameters for each task. These task-speciﬁc layers\nare then regularized during training to reduce the dif-\nferences between shared layers. This encourages lay-\ners to have similar weights but allows each task to spe-\ncialize speciﬁc components. Diagrams showing these\nnetwork concepts can be found in the MTL survey\nby Ruder (2017). Knowledge Distillation (KD) is a\nproven soft parameter technique which trains a stu-\ndent network to imitate the full output of a trained\nteacher network (Hinton et al., 2015).\nAn early study in MTL showed that auxiliary tasks\nwhich help increase performance on a target task pre-\nfer to share hidden layers and weights with the tar-\nget task, while unhelpful auxiliary tasks prefer to use\nweights not used by the target task (Caruana, 1997).\nThis intuition has laid the groundwork for deep learn-\ning models which focus on building an enhanced inter-\nnal representation of a problem space through shared\nhidden layers. It has been shown that pre-deﬁning\nwhich layers to share can improve the performance of\na deep learning MTL model when the tasks are gen-\nerally beneﬁcial, but this can break down if the wrong\ntask pairs are selected (Ruder, 2017). The study goes\non to argue for the beneﬁt of learning task hierarchies\ninternal to the model during training to help overcome\nthis problem. Research has also shown that the depth\nof a layer and the beneﬁt of sharing the layer between\ntwo tasks can be considered a measure of similarity\nof the two tasks (Mou et al., 2016). They argue that\nlow-level layers, such as word embeddings, are gener-\nally useful for all NLP tasks, while higher level layers\nbecome more speciﬁc and can only be shared among\nmore similar tasks. This suggests that model archi-\ntectures can be built oﬀthis metric when combined\nwith other evaluations of task relatedness.\nAnother model consideration when building MTL\nsystems is the capacity of the network.\nRadford\net al. (2019) prove that the capacity of a language\nmodel is essential to good performance and that in-\ncreasing capacity produces a log-linear improvement.\nThis follows conventional neural network wisdom and\nagrees with other research, such as BERT (Devlin\net al., 2019), whose performance appears to scale with\nthe model size, and T5 (Raﬀel et al., 2019), which\nachieved state-of-the-art results when pushed to 11\n5\nbillion parameters.\nRuder et al. (2017) show that hard-parameter shar-\ning, task-speciﬁc network layers, hierarchical NLP\nlayers and a regularizer to encourage tasks to share\nonly what is useful, called block-sparse regulariza-\ntion, can be combined to create a powerful MTL net-\nwork called a Sluice network.\nThe Sluice network\nconsistently performed better than single-task multi-\nlayer perceptrons (MLPs) on all evaluation tasks and\noutperformed traditional hard parameter sharing ap-\nproaches (Caruana, 1997) on most NLP tasks.\nAn additional question that must be addressed is\nhow a task is represented within the model. It is com-\nmon with Inductive Bias MTL that each task has a\nspeciﬁc set of output layers that can be queried to\nreturn task speciﬁc results (Ruder, 2017). However,\nMcCann et al. (2018) present a novel idea in which the\ntask itself in included as input to the network, identi-\nﬁed within this survey as Context Tasking. While the\nimplementation may diﬀer across domains and tasks,\nContext Tasking was implemented here by represent-\ning each task as a natural language question with a\nnatural language answer.\nThis avoids the need for\nany task-specialized components and naturally sup-\nports zero-shot learning and open-set classiﬁcation\n(Scheirer et al., 2014). Aralikatte et al. (2019) present\nanother interesting approach to Context Tasking by\ncasting the NLP tasks of ellipsis resolution and coref-\nerence resolution as reading comprehension problems\nand produced new state-of-the-art results using In-\nductive Bias MTL.\n4.4\nTraining Curriculum\nA ﬁnal topic of MTL training implications is the de-\nsign of a training curriculum.\nGiven the research\nabove regarding mixing ratios, task weighting, shared\nrepresentations and sensitivities to task selection, it\nseems natural that MTL should be addressed with\nan intelligent training curriculum. The standard cur-\nriculum for MTL tasks is to build mini-batches con-\ntaining examples for a single task and then alternate\nbetween tasks during training. The ratio of task mini-\nbatches can be identical for all tasks or varied based\non task performance or dataset size (Caruana, 1997;\nLuong et al., 2015; Ruder, 2017; Wang et al., 2018).\nMcCann et al. (2018) refer to this as a ﬁxed-order\nround robin curriculum and prove that it works well\non tasks that require few iterations, but it struggles\nwith more complex tasks. They furthermore consider\nhand-crafted curricula and show that beginning with\nmore diﬃcult tasks and slowly introducing additional\ntasks performs the best. Other work has considered\nincluding task-speciﬁc stopping conditions for TL and\nMTL (Mou et al., 2016), and more recent research has\nproposed a teacher-based annealing solution to dy-\nnamically control the auxiliary task impact with KD\n(Clark et al., 2019). Other research has shown that\nround robin training is most impactful towards the\nend of the curriculum (Stickland and Murray, 2019).\nThey propose a technique called annealed sampling\nin which batch sampling is originally based on the\nratio of dataset sizes and slowly anneals to an even\ndistribution across all tasks as the current epoch num-\nber increases. These discoveries, when combined with\ncurriculum research emerging from the ﬁeld of rein-\nforcement learning (Svetlik et al., 2017), lead to a\nwealth of new research opportunities towards the de-\nsign of MTL curricula.\n5\nLearning Task Relationships\nBeyond the research into measuring properties of\ntasks and datasets to determine similarities, there\nhave been multiple eﬀorts to intrinsically learn task\nrelatedness through a learning process.\nCaruana\n(1997) showed that neural networks trained in an\nMTL setting exhibited a behavior where related tasks\nwould share hidden nodes and unrelated tasks would\nnot. This discovery implies that neural networks are\nable to determine what information is useful for shar-\ning between tasks without an explicit signal convey-\ning the task relationship. It is therefore reasonable\nto believe that neural networks are able to learn, and\neven describe, task relationship explicitly through the\nMTL training process. Research has since explored\ndiﬀerent clustering techniques built on this discov-\nery which attempt to cluster network weights and pa-\nrameters leading to a latent task relationship embed-\nded in the task clusters (Ruder, 2017). Not only do\nthese techniques inherently learn task relationships,\nthey also help to train neural networks by penalizing\nthem from diverging too much from a common set\nof knowledge shared by similar tasks. Ruder (2017)\nalso presents the Deep Relationship Network and the\nCross-Stitch Network which are hard and soft param-\neter models, respectively, able to identify task rela-\n6\ntionship through training.\nAn approach called task2vec has been proposed\nwhich learns an embedding vector for an entire task\nthat is agnostic to the size of the dataset (Achille\net al., 2019). The embedding attempts to capture se-\nmantic similarities between tasks by training a model\nto solve a task, and then probing the network to ap-\nproximate the amount of information carried by the\nweights. The proximities between two task embed-\nding vectors are theorized to represent task related-\nness while the magnitude of the embedding vector is\nthought to correlate to the complexity of the task.\nAn alternative approach to learning directly from\nthe hidden nodes and gradients is to eﬃciently search\nthrough task pairs to determine task similarities. De-\npending on the number of tasks an exhaustive search\nvery quickly becomes impossible, however, heuristic\nbased searches have been found to act as a good\nstand-in to estimate when tasks may be related (Stan-\ndley et al., 2019).\nThey show that there is a high\ncorrelation between the validation loss of a network\ntrained on 20% of the data and the fully trained val-\nidation loss of a network. Based on this claim, they\nuse the loss at 20% as a heuristic to lightly train\nmulti-task permutations for ﬁnding optimally per-\nforming task sets.\nThey go on to show that given\nthree tasks, the average loss of every two-pair combi-\nnation is an eﬀective approximation of the loss when\nall three tasks are trained jointly. This acts as a good\nsearch heuristic for ﬁnding optimized task sets. While\nthis work has focused on small task sets and relatively\nsmall combinations, others have shown the beneﬁt of\nhaving many auxiliary tasks to boost MTL perfor-\nmance (Ruder, 2017; Ratner et al., 2018; Liu et al.,\n2019a). More research into the implications of these\nﬁndings is important to understanding the eﬀect of\nthe number of tasks present in an auxiliary task set.\n6\nMTL\nBenchmarks\nand\nLeaderboards\nWhile there are many research eﬀorts that evaluate\nMTL model performance on custom task sets, there\nexist several gold standard benchmarks which enable\ncomparative evaluation. The ﬁrst of these is the NLP\nDecathlon (McCann et al., 2018), decaNLP, which\ncombines ten common NLP tasks/datasets:\nQues-\ntion Answering, Machine Translation, Summariza-\ntion, Natural Language Inference, Sentiment Anal-\nysis, Semantic Role Labeling, Zero-Shot Relation Ex-\ntraction, Goal-Oriented Dialog, Semantic Parsing and\nPronoun Resolution. Each task is assigned a scoring\nmetric between 0 and 100. An overall decaScore is\ncomputed as the sum of all the task scores with the\nhighest possible being 1,000. Using the Context Task-\ning technique, every task is represented as a natural\nlanguage question, a context and an answer. The de-\ncaNLP leaderboard presents an opportunity for MTL\nresearchers to assess model performance.\nOne of the most popular evaluation benchmarks\nused for TL and MTL alike is the General Language\nUnderstanding Evaluation (GLUE) (Wang et al.,\n2018). GLUE challenges models to solve the following\n9 NLP tasks: Grammatical Acceptance Prediction,\nSentiment Analysis, Paraphrasing, Semantic Equiva-\nlence, Semantic Similarity, Question Answering, Pro-\nnoun Resolution and two diﬀerent Textual Entailment\ntasks. Each task has a deﬁned scoring metric to eval-\nuate task-speciﬁc performance; F1 score is commonly\nused among the tasks. GLUE does not require that all\ntasks be solved by the same model, and, as such, many\ntop solutions have a ﬁne-tuned model per task. An\noverall GLUE score, the macro-average of all tasks, is\ncomputed for each model.\nIn order to keep challenging researchers to push\nthe state-of-the-art, an additional NLP benchmark,\ncalled SuperGLUE, is presented which is designed to\nbe signiﬁcantly more diﬃcult (Wang et al., 2019).\nThe following 7 tasks are included in the Super-\nGLUE: Binary Question Answering, Imbalanced 3-\nClass Textual Entailment, Logical Causal Relation-\nship, Textual Entailment, Binary Word-Sense Dis-\nambiguation, Pronoun Resolution and two diﬀerent\nMultiple-Choice Question Answering tasks. Textual\nEntailment and Pronoun Resolution are the only two\ntasks from the original GLUE benchmark retained in\nSuperGLUE. These tasks were kept because they still\nshowed room for improvement and proved to be two\nof the hardest tasks in GLUE.\n7\nMTL Solutions for NLP\nThere is a rich library of research presenting techni-\ncal implementations and use cases for MTL models\nand architectures. This section provides an overview\n7\nTable 2: MTL Model Comparison\nModel\nParams\nGLUE\nSuperGLUE\ndecaScore\nMQAN\n29M\n-\n-\n609.0\nMT-DNN\n350M\n87.6\n-\n-\nBERTBase\n110M\n78.3\n-\n-\nBERTLarge\n340M\n80.5\n69.0\n-\nBERT with PALs\n125M\n-\n-\n-\nBERT+BAM\n335M\n82.3\n-\n-\nRoBERTa\n375M\n88.5\n84.6\n-\nALBERTxxl Ensemble\n235M\n89.4\n-\n-\nGPT-2\n1,542M\n-\n-\n-\nXLNet-Large\n340M\n88.4\n-\n-\nT5-11B\n11B\n89.7\n89.3\n-\nof recent state-of-the-art approaches. Table 2 shows\na comparison of model sizes and scores on common\nbenchmarks.\n7.1\nMulti-task\nQuestion\nAnswering\nNetwork\nThe\nMulti-task\nQuestion\nAnswering\nNetwork\n(MQAN), (McCann et al., 2018), is a natural lan-\nguage Context Tasking network designed to jointly\nlearn over all tasks with no task speciﬁc weights or\nparameters in the network.\nAll inputs and tasks\nare modeled as natural language questions and\noutputs in the form of a natural language answer.\nThis enables the network to learn to solve tasks\nwhich traditionally have diﬀerent input and output\nstructures, such as machine translation and relation\nextraction. The authors show that MQAN is able to\nachieve performance comparable to ten single-task\nnetworks with no ﬁne-tuning or task speciﬁc layers.\nDue to the common contextualized input design,\nMQAN is able to do zero-shot training and can even\nadapt to unseen classes in classiﬁcation.\n7.2\nBERT and Related Models\nArguably one of the most important models re-\ncently proposed is BERT: Bidirectional Encoder Rep-\nresentations from Transformers (Devlin et al., 2019).\nBERT pre-trains a transformer model (Vaswani et al.,\n2017) with an unsupervised multi-task objective.\nThis pre-training objective trains the network to pre-\ndict a random mask of hidden words in a text docu-\nment and to predict if a shown sentence is the logical\nnext sentence in the document via a binary classiﬁer.\nAlong with the novel pre-training objective, BERT\nalso presents a mechanism for contextualizing on both\nthe left and right text directions while other popular\nmodels, such as GPT (Radford et al., 2018), are unidi-\nrectional. BERT scored competitively on the GLUE\nleaderboard and provided a base for researchers to\nbuild upon.\nSince the release of BERT, there have been a num-\nber of modiﬁcations which have surpassed the base-\nline score on GLUE (Phang et al., 2018; Joshi et al.,\n2019; Lan et al., 2019). BERT and PALs train a sin-\ngle BERT model to be used for all tasks jointly, as\nopposed to building a ﬁne-tuned model for each task\n(Stickland and Murray, 2019).\nClark et al. (2019)\napproach MTL with BERT from a diﬀerent angle by\ndoing multi-task ﬁne-tuning through knowledge dis-\ntillation and a multi-teacher paradigm, called BAM.\nThe model is trained with a teacher annealing curricu-\nlum that gradually transfers the target learner from\ndistillation through the teachers to a supervised MTL\nsignal. RoBERTa is an optimized take on BERT that\nﬁnds techniques which signiﬁcantly improve perfor-\nmance (Liu et al., 2019b).\nALBERT replaces the\nnext sentence prediction task, proven ineﬀective by\nYang et al. (2019), with a sentence-order prediction\npre-training task (Lan et al., 2019).\nOne notable extension of BERT with true multi-\ntask learning across all 9 GLUE tasks is the Multi-\nTask Deep Neural Network (MT-DNN) (Liu et al.,\n2019a). The authors argue that MT-DNN has better\ndomain transfer across tasks than standard BERT.\nThe process begins with the regular BERT pre-\ntraining, followed by multi-task training with hard\nparameter sharing and a random round robin curricu-\nlum and ﬁnally ends with task-speciﬁc ﬁne-tuning.\n7.3\nGPT/GPT-2\nBERT is not the only type of language model that has\nsuccessfully performed in MTL environments. GPT\n(Radford et al., 2018) is based on a multi-layer trans-\nformer network and GPT-2 (Radford et al., 2019)\nextends this model with an unsupervised multi-task\npre-training objective. In inference settings GPT-2 is\nﬁrst task-conditioned to solve the desired task. This\nzero-shot type of learning can outperform the current\nstate-of-the-art on a majority of NLP tasks. GPT-\n2 is also shown to perform competitively when used\nin a traditional pre-training and ﬁne-tuning process.\nThe authors have indicated that in future work they\nplan to assess GPT-2 performance on decaNLP and\n8\nGLUE benchmarks.\n7.4\nXLNet\nXLNet is proposed as a next-generation model which\nis intended to leverage the best features found in\nBERT and GPT while overcoming their intrinsic\nshortcomings (Yang et al., 2019). The authors claim\nthat BERT suﬀers from a pre-train/ﬁne-tune discrep-\nancy due to the masked words introduced in pre-\ntraining.\nWhile the masked words are helpful for\nbuilding a latent understanding of language, masked\nwords are never seen in practice and thus there is a\ndistinct diﬀerence in the training data and real-world\ninputs. While this simpliﬁcation has worked well for\nBERT, Yang et al. (2019) attempt to improve per-\nformance by estimating the joint probability of the\nwords seen in a piece of text. The authors also em-\npirically show that BERT’s next sentence prediction\npre-training objective did not improve model perfor-\nmance and, hence, was dropped from the XLNet pre-\ntraining regimen.\n7.5\nT5\nT5 (Raﬀel et al., 2019) (Text-to-Text Transfer Trans-\nformer) is a reﬁnement to the traditional transformer\nwhich boasts an unsupervised pre-training corpus of\nroughly 750 GB and uses natural language Context\nTasking. The highest performing model designed by\nthe authors contains 11 billion parameters, far more\nthan what any other model has considered, and has\nbeaten all other models addressed above on the GLUE\nand SuperGLUE leaderboards.\nThis work provides\nconvincing evidence regarding the claim that model\ncapacity is an important factor in transfer learning\nand MTL in NLP.\n8\nCurrent Challenges and Op-\nportunities\nMost challenges that are still faced today in MTL are\nthe same challenges that have existed for the past two\ndecades. Caruana (1997) proved that some inductive\nbias can hurt, and while it is still generally believed\nthat task relatedness leads to good bias, there is no\nstrong general notion of measuring this (Ben-David\nand Borbely, 2008; Ruder, 2017).\nStandley et al.\n(2019) begin to address this by confronting the un-\nderlying challenge of crosstalk, in which MTL suf-\nfers from complex and competing objectives. Addi-\ntional studies have researched task relationship and\nperformance on earlier model generations, such as bi-\nLSTMs (Bingel and Søgaard, 2017; Luong et al., 2015;\nAlonso and Plank, 2017). Studies applying similar in-\ndepth analysis to the most recent multi-task bench-\nmarks with the latest transformer-based models are\nprime research opportunities to understand better the\ntasks to solve and the implications of the selected\nmodels.\nStandley et al. (2019) present several interesting\nclaims which are worth exploring and applying to\nknown MTL benchmarks. The ﬁrst is that it could be\nbetter to train dissimilar tasks as opposed to semanti-\ncally similar tasks. Additionally they argue that MTL\nperformance estimates can be made by averaging the\nresults of lesser-order task pairs. Both claims present\nresearch opportunities that could lead to better un-\nderstanding of the impact of auxiliary task selection.\nThe new set of MTL deep learning models should also\nbe explored through probing in a manner similar to\nthat of Kim et al. (2019) to understand better the\nimpact of NLP task selection. There still is a need\nfor deeper and more general techniques for task selec-\ntion and task assessment. As research dives deeper\ninto the implications of MTL it is important to con-\ntinue strengthening the current understanding of task\nrelationship and selection.\nCurriculum learning is continuing to gain popu-\nlarity and will likely become of larger interest with\nthe introduction of standardized MTL benchmarks.\nCurriculum learning has not been explored much in\nNLP or MTL, however, it has a rich history in re-\ninforcement learning (RL) where curriculum is used\nto guide trained agents to more complex and real-\nistic behaviors (Svetlik et al., 2017; Florensa et al.,\n2017). The curriculum is often generated in RL set-\ntings and it would be interesting to expand on these\ncapabilities for MTL curriculum generation.\nThese\ngenerations could leverage some form of relatedness\n(Liu et al., 2019a) or be driven by unsupervised or la-\ntent signals (Achille et al., 2019). Other research into\nlifelong learning and continuous learning (Mitchell\net al., 2018; Parisi et al., 2019) present new ideas and\nparadigms which are related to MTL and can be uti-\nlized to help solve the MTL tasks mentioned in this\n9\nsurvey.\nAlthough many unsupervised natural language un-\nderstanding tasks have recently been used in a pre-\ntraining setting, Luong et al. (2015) pose the ques-\ntion of how unsupervised objectives may impact MTL\nperformance as auxiliary tasks. Building oﬀthe TL\nprocess there are open questions on how an MTL\nmodel can leverage the same unsupervised datasets.\nThey argue that an auxiliary task must be compatible\nwith the target task, and both intrinsic (perplexity)\nand extrinsic (accuracy) metrics must be improved on\nthe target task when trained with the auxiliary task.\nAlonso and Plank (2017) pose an additional question:\nmost auxiliary tasks are classiﬁcation tasks, how do\nregression tasks fare as auxiliary tasks?\nGonz´alez-\nGarduno and Søgaard (2018) provide an example of\nthis with text readability prediction and an auxiliary\ngaze prediction task.\nThey showed that they only\nneeded small samples from the auxiliary task, the\nselection of the auxiliary task was robust to small\nchanges in the domain and the shared feature rep-\nresentation provably enhanced model performance.\nThis work shows that further research into regression\nauxiliary tasks could help to advance MTL state-of-\nthe-art. Finally, Liu et al. (2019a) present a unique\nopportunity to study how MTL architectures perform\nagainst adversarial tasks which could potentially lead\nto a new set of hardened auxiliary tasks.\nWe hy-\npothesize that Domain Regularization or Multi-Task\nFeature Learning could help machine learning models\nbetter withstand adversarial attacks.\nMost recent advancements in TL and MTL are\nbased oﬀhard parameter sharing. How do model ar-\nchitectures, such as the transformer, perform when\nregularized with an MTL-based soft parameter shar-\ning?\nHow would this compare to standard models\nsuch as BERT and GPT and what other techniques\ncan be borrowed from Ruder (2017) for the latest gen-\neration of deep learning models?\nLastly, the biggest challenge faced in current MTL\nresearch is that ﬁne-tuned single-task models consis-\ntently outperform non-ﬁne-tuned MTL models that\nshare layers (McCann et al., 2018; Clark et al.,\n2019). MTL pre-training followed by single-task ﬁne-\ntuning is able to leverage the rich knowledge ac-\nquired through inductive bias, but the impact of the\nstrong supervised signal creates narrow experts which\nare able to outperform the generalized experts pro-\nduced by MTL. While this is ﬁne for narrow systems\ndesigned to solve problems with expansive training\ndatasets, this gap needs to be closed to improve per-\nformance on data sparse tasks and domains. A long-\nterm goal that will continue to persist is to develop\ngeneral experts which can compete with their single-\ntask counterparts (Clune, 2019).\nUltimately we ﬁnd this ambitious task before us.\nTo ﬁnd ways to build robust and capable MTL mod-\nels and help to enable the next generation of general\nArtiﬁcial Intelligence.\nReferences\nAchille, A., Lam, M., Tewari, R., Ravichandran,\nA., Maji, S., Fowlkes, C., Soatto, S., Perona, P.,\n2019. Task2vec: Task embedding for meta-learning.\narXiv preprint arXiv:1902.03545 .\nAlonso, H.M., Plank, B., 2017. When is multitask\nlearning eﬀective?\nsemantic sequence prediction\nunder varying data conditions, in: EACL Vol. 1,\npp. 44–53.\nAnaby-Tavor, A., Carmeli, B., Goldbraich, E., Kan-\ntor, A., Kour, G., Shlomov, S., Tepper, N., Zw-\nerdling, N., 2019. Not enough data? deep learning\nto the rescue! arXiv preprint arXiv:1911.03118 .\nAralikatte, R., Lamm, M., Hardt, D., Søgaard, A.,\n2019. Ellipsis and coreference resolution as question\nanswering. arXiv preprint arXiv:1908.11141 .\nBaxter, J., 1998. Theoretical models of learning to\nlearn, in: Learning to learn. Springer, pp. 71–94.\nBen-David, S., Borbely, R.S., 2008. A notion of task\nrelatedness yielding provable multiple-task learning\nguarantees. Machine learning 73, 273–287.\nBingel, J., Søgaard, A., 2017. Identifying beneﬁcial\ntask relations for multi-task learning in deep neural\nnetworks, in: EACL Vol. 2, pp. 164–169.\nBonilla, E.V., Chai, K.M., Williams, C., 2008. Multi-\ntask gaussian process prediction, in:\nNIPS, pp.\n153–160.\nCaruana, R., 1997.\nMultitask learning.\nMachine\nlearning 28, 41–75.\n10\nClark, K., Luong, M.T., Khandelwal, U., Manning,\nC.D., Le, Q., 2019. Bam! born-again multi-task\nnetworks for natural language understanding, in:\nACL, pp. 5931–5937.\nClune, J., 2019. Ai-gas: Ai-generating algorithms, an\nalternate paradigm for producing general artiﬁcial\nintelligence. arXiv preprint arXiv:1905.10985 .\nDevlin,\nJ.,\nChang,\nM.W.,\nLee,\nK.,\nToutanova,\nK., 2019.\nBert:\nPre-training of deep bidirec-\ntional transformers for language understanding, in:\nNAACL-HLT Vol 1, pp. 4171–4186.\nFlorensa, C., Held, D., Wulfmeier, M., Zhang, M.,\nAbbeel, P., 2017.\nReverse curriculum genera-\ntion for reinforcement learning.\narXiv preprint\narXiv:1707.05300 .\nGonz´alez-Garduno, A.V., Søgaard, A., 2018. Learn-\ning to predict readability using eye-movement data\nfrom natives and learners, in: Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nHinton, G., Vinyals, O., Dean, J., 2015.\nDistilling\nthe knowledge in a neural network. arXiv preprint\narXiv:1503.02531 .\nJebara, T., 2004. Multi-task feature and kernel selec-\ntion for svms, in: ICML, p. 55.\nJoshi, M., Chen, D., Liu, Y., Weld, D.S., Zettle-\nmoyer, L., Levy, O., 2019. Spanbert: Improving\npre-training by representing and predicting spans.\narXiv preprint arXiv:1907.10529 .\nKim, N., Patel, R., Poliak, A., Xia, P., Wang,\nA., McCoy, T., Tenney, I., Ross, A., Linzen, T.,\nVan Durme, B., et al., 2019. Probing what diﬀer-\nent nlp tasks teach machines about function word\ncomprehension, in: SEM, pp. 235–249.\nLan, Z., Chen, M., Goodman, S., Gimpel, K.,\nSharma, P., Soricut, R., 2019. Albert: A lite bert\nfor self-supervised learning of language representa-\ntions. arXiv preprint arXiv:1909.11942 .\nLiu, X., He, P., Chen, W., Gao, J., 2019a. Multi-task\ndeep neural networks for natural language under-\nstanding. arXiv preprint arXiv:1901.11504 .\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M.,\nChen, D., Levy, O., Lewis, M., Zettlemoyer, L.,\nStoyanov, V., 2019b.\nRoberta: A robustly opti-\nmized bert pretraining approach.\narXiv preprint\narXiv:1907.11692 .\nLuong, M.T., Le, Q.V., Sutskever, I., Vinyals, O.,\nKaiser, L., 2015. Multi-task sequence to sequence\nlearning. arXiv preprint arXiv:1511.06114 .\nMcCann, B., Keskar, N.S., Xiong, C., Socher, R.,\n2018.\nThe natural language decathlon:\nMulti-\ntask learning as question answering. arXiv preprint\narXiv:1806.08730 .\nMitchell, T., Cohen, W., Hruschka, E., Talukdar, P.,\nYang, B., Betteridge, J., Carlson, A., Dalvi, B.,\nGardner, M., Kisiel, B., et al., 2018. Never-ending\nlearning.\nCommunications of the ACM 61, 103–\n115.\nMou, L., Meng, Z., Yan, R., Li, G., Xu, Y., Zhang, L.,\nJin, Z., 2016. How transferable are neural networks\nin nlp applications?, in: EMNLP, pp. 479–489.\nParisi, G.I., Kemker, R., Part, J.L., Kanan, C.,\nWermter, S., 2019. Continual lifelong learning with\nneural networks: A review. Neural Networks .\nPhang, J., F´evry, T., Bowman, S.R., 2018.\nSen-\ntence encoders on stilts: Supplementary training\non intermediate labeled-data tasks. arXiv preprint\narXiv:1811.01088 .\nRadford,\nA.,\nNarasimhan,\nK.,\nSalimans,\nT.,\nSutskever, I., 2018.\nImproving language under-\nstanding by generative pre-training .\nRadford, A., Wu, J., Child, R., Luan, D., Amodei,\nD., Sutskever, I., 2019. Language models are unsu-\npervised multitask learners .\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang,\nS., Matena, M., Zhou, Y., Li, W., Liu, P.J., 2019.\nExploring the limits of transfer learning with a\nuniﬁed text-to-text transformer.\narXiv preprint\narXiv:1910.10683 .\nRatner, A., Hancock, B., Dunnmon, J., Goldman, R.,\nR´e, C., 2018. Snorkel metal: Weak supervision for\nmulti-task learning, in: Workshop on Data Man-\nagement for End-To-End Machine Learning, ACM.\np. 3.\n11\nRedko, I., Morvant, E., Habrard, A., Sebban, M.,\nBennani, Y., 2019. Advances in Domain Adapta-\ntion Theory. Elsevier.\nRomera-Paredes, B., Argyriou, A., Berthouze, N.,\nPontil, M., 2012.\nExploiting unrelated tasks in\nmulti-task learning, in: International conference on\nartiﬁcial intelligence and statistics, pp. 951–959.\nRuder, S., 2017.\nAn overview of multi-task learn-\ning in deep neural networks.\narXiv preprint\narXiv:1706.05098 .\nRuder, S., Bingel, J., Augenstein, I., Søgaard, A.,\n2017. Sluice networks: Learning what to share be-\ntween loosely related tasks. stat 1050, 23.\nScheirer, W.J., Jain, L.P., Boult, T.E., 2014. Proba-\nbility models for open set recognition. IEEE trans-\nactions on pattern analysis and machine intelli-\ngence 36, 2317–2324.\nStandley, T., Zamir, A.R., Chen, D., Guibas, L., Ma-\nlik, J., Savarese, S., 2019. Which tasks should be\nlearned together in multi-task learning?\narXiv\npreprint arXiv:1905.07553 .\nStickland, A.C., Murray, I., 2019. Bert and pals: Pro-\njected attention layers for eﬃcient adaptation in\nmulti-task learning, in: ICML, pp. 5986–5995.\nSvetlik, M., Leonetti, M., Sinapov, J., Shah, R.,\nWalker, N., Stone, P., 2017. Automatic curriculum\ngraph generation for reinforcement learning agents,\nin: AAAI.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A.N., Kaiser,  L., Polosukhin, I.,\n2017. Attention is all you need, in: NIPS, pp. 5998–\n6008.\nVelay, M., Daniel, F., 2018.\nSeq2seq and multi-\ntask learning for joint intent and content extrac-\ntion for domain speciﬁc interpreters. arXiv preprint\narXiv:1808.00423 .\nWang, A., Pruksachatkun, Y., Nangia, N., Singh,\nA., Michael, J., Hill, F., Levy, O., Bowman, S.R.,\n2019. Superglue: A stickier benchmark for general-\npurpose language understanding systems.\narXiv\npreprint arXiv:1905.00537 .\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O.,\nBowman, S., 2018. Glue: A multi-task benchmark\nand analysis platform for natural language under-\nstanding, in:\nEMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pp. 353–355.\nWang, Q., Zhang, L., Chi, M., Guo, J., 2008. Mtfor-\nest: Ensemble decision trees based on multi-task\nlearning., in: ECAI, pp. 122–126.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhut-\ndinov, R., Le, Q.V., 2019. Xlnet: Generalized au-\ntoregressive pretraining for language understand-\ning. arXiv preprint arXiv:1906.08237 .\nZoph, B., Yuret, D., May, J., Knight, K., 2016. Trans-\nfer learning for low-resource neural machine trans-\nlation. arXiv preprint arXiv:1604.02201 .\n12\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "I.2.6; I.2.7"
  ],
  "published": "2020-07-22",
  "updated": "2020-07-22"
}