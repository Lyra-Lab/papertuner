{
  "id": "http://arxiv.org/abs/2008.00766v1",
  "title": "Tracking the Race Between Deep Reinforcement Learning and Imitation Learning -- Extended Version",
  "authors": [
    "Timo P. Gros",
    "Daniel Höller",
    "Jörg Hoffmann",
    "Verena Wolf"
  ],
  "abstract": "Learning-based approaches for solving large sequential decision making\nproblems have become popular in recent years. The resulting agents perform\ndifferently and their characteristics depend on those of the underlying\nlearning approach. Here, we consider a benchmark planning problem from the\nreinforcement learning domain, the Racetrack, to investigate the properties of\nagents derived from different deep (reinforcement) learning approaches. We\ncompare the performance of deep supervised learning, in particular imitation\nlearning, to reinforcement learning for the Racetrack model. We find that\nimitation learning yields agents that follow more risky paths. In contrast, the\ndecisions of deep reinforcement learning are more foresighted, i.e., avoid\nstates in which fatal decisions are more likely. Our evaluations show that for\nthis sequential decision making problem, deep reinforcement learning performs\nbest in many aspects even though for imitation learning optimal decisions are\nconsidered.",
  "text": "Tracking the Race Between Deep Reinforcement\nLearning and Imitation Learning –\nExtended Version\nTimo P. Gros, Daniel H¨oller, J¨org Hoﬀmann, and Verena Wolf\nSaarland University, Saarland Informatics Campus, 66123 Saarbr¨ucken, Germany\n{timopgros, hoeller, hoffmann, wolf}@cs.uni-saarland.de\nhttps://mosi.uni-saarland.de, http://fai.cs.uni-saarland.de\nAbstract. Learning-based approaches for solving large sequential deci-\nsion making problems have become popular in recent years. The resulting\nagents perform diﬀerently and their characteristics depend on those of\nthe underlying learning approach.\nHere, we consider a benchmark planning problem from the reinforce-\nment learning domain, the Racetrack, to investigate the properties of\nagents derived from diﬀerent deep (reinforcement) learning approaches.\nWe compare the performance of deep supervised learning, in particular\nimitation learning, to reinforcement learning for the Racetrack model.\nWe ﬁnd that imitation learning yields agents that follow more risky\npaths. In contrast, the decisions of deep reinforcement learning are more\nforesighted, i.e., avoid states in which fatal decisions are more likely. Our\nevaluations show that for this sequential decision making problem, deep\nreinforcement learning performs best in many aspects even though for\nimitation learning optimal decisions are considered.\nKeywords: Deep Reinforcement Learning · Imitation Learning · Race-\ntrack.\n1\nIntroduction\nIn recent years, deep learning (DL) and especially deep reinforcement learning\n(DRL) have been applied with great successes to the task of learning near-\noptimal policies for sequential decision making problems. DRL has been applied\nto various applications such as Atari games [11, 12], Go and Chess [16–18], or Ru-\nbic’s cube [1]. It relies on a feedback loop between self-play and the improvement\nof the current strategy by reinforcing decisions that lead to good performance.\nPassive imitation learning (PIL) is another well-known approach to solve se-\nquential decision making problems, where a policy is learned based on training\ndata that is labeled by an expert [15]. An extension of this approach is ac-\ntive imitation learning (AIL), where after an initial phase of passive learning,\nadditional data is iteratively generated by exploring the state space based on\nthe current strategy and subsequent expert labeling [7, 14]. AIL has successfully\narXiv:2008.00766v1  [cs.LG]  3 Aug 2020\n2\nT. Gros et al.\nbeen applied to common reinforcement learning benchmarks such as cart-pole\nor bicycle-balancing [7].\nSequential decision making problems are typically described by Markov de-\ncision processes (MDPs). During the simulation of an MDP, the set of those\nstates that will be visited in the future depend on current decisions. In PIL,\nthe agent, which represents a policy, is trained by iterating over the given ex-\npert data set, whose distribution does not generally resemble this dependence.\nAIL extends the data with sequentially generated experiences. Hence, the data\nis more biased towards sequentially taken decisions. In contrast, DRL does not\nrely on expert data at all, but simply alternates between exploitation of former\nexperiences and exploration. It is a priori not obvious which method achieves\nthe best result for a particular sequential decision making problem.\nHere we aim at an in-depth study of empirical learning agent behavior for\na range of diﬀerent learning frameworks. Speciﬁcally we are interested in diﬀer-\nences due to the sequential nature of action decisions, inherent in reinforcement\nlearning and active imitation learning but not in passive imitation learning. To\nbe able to study and understand algorithm behavior in detail, we conduct our\ninvestigation in a simple benchmark problem, namely Racetrack.\nRacetrack is originally a pen and paper game, adopted as a benchmark in\nAI sequential decision making for the evaluation of MDP solution algorithms [2,\n3, 13, 19]. A map with obstacles is given, and a policy for reaching a goal region\nfrom an initial position has to be found. Decisions for two-dimensional accelera-\ntions are taken sequentially, which requires foresighted planning. Ignoring traﬃc,\nchanging weather conditions, fuel consumption, and technical details, Racetrack\ncan be considered a simpliﬁed model of autonomous driving control [4]. Race-\ntrack is ideally suited for a comparison of diﬀerent learning approaches, because\nnot only the performance of diﬀerent agents but also their “driving characteris-\ntics” can be analyzed. Moreover, for small maps, expert data describing optimal\npolicies can be obtained.\nWe train diﬀerent agents for Racetrack using DRL, PIL, and AIL and study\ntheir characteristics. We ﬁrst apply PIL and train agents represented by linear\nfunctions and artiﬁcial neural networks. As expert labeling, we apply the A∗\nalgorithm to ﬁnd optimal actions for states in Racetrack. We suggest diﬀerent\nvariants of data generation to obtained more appropriate sample distributions.\nFor AIL, we use the DAGGER approach [14] to train agents represented by\nneural networks. We use the same network architecture when we apply deep\nreinforcement learning. More speciﬁcally, we train deep Q-networks [12] to solve\nthe Racetrack benchmark. We compare the resulting agents considering three\ndiﬀerent aspects: the success rate, the quality of the resulting action sequences,\nand the relative number of optimal and fatal decisions.\nAmongst other things, we ﬁnd that, even though it is based on optimal train-\ning data, imitation learning leads to unsafe policies, much more risky than those\nfound by RL. Upon closer inspection, it turns out that this apparent contra-\ndiction actually has an intuitive explanation in terms of the nature of the ap-\nplication and the diﬀerent learning methods: to minimize time to goal, optimal\nTracking the Race Between Deep RL and Imitation Learning\n3\ndecisions navigate very closely to dangerous states. This works well when taking\noptimal decisions throughout – but is brittle to (and thus fatal in the presence\nof) even small divergences as are to be expected from a learned policy. We be-\nlieve that this characteristic might carry over to many other applications beyond\nRacetrack.\nThe outline of our paper is the following: We ﬁrst introduce the Racetrack\ndomain (Section 2). Then we introduce the DAGGER framework and deep Q-\nlearning (Section 3), before we describe our application to the Racetrack domain\n(Section 4). In Section 5, we present our experiments and ﬁndings. We ﬁnally\ndraw a conclusion and present future work in Section 6.\nThis report is an extended version of the conference paper by Gros et al. [6].\n2\nRacetrack\nRacetrack has been used as a benchmark in the context of planning [3, 13] and\nreinforcement learning [2, 19]. It can be played on diﬀerent maps. The example\nused throughout the paper is displayed in Figure 1.\n2.1\nThe Racetrack Game\nAt the beginning of the game, a car is placed randomly at one of the discrete\npositions on the start line (in purple) with zero velocity. In every step it can\nspeed up, hold the velocity or slow down in x and/or y dimension. Then, the\ncar moves in a straight line with the new velocity from the old position to a\nnew one, where we discretize the maps into cells. The game is lost when the\ncar crashes, which is the case when either (1) the new position itself is a wall\nposition or outside the map, or (2) the straight line between the old and new\nposition intersects with a wall, i.e. the car drives through a wall on its way to the\nnew position. The game is won when the car either stops at or drives through\nthe goal line (in green).\nFig. 1. Example of a Racetrack map: goal line is green, start line is purple.\n4\nT. Gros et al.\n2.2\nMarkov Decision Process\nGiven a Racetrack map, the game can be modeled as a Markov decision process.\nStates. The current state is uniquely deﬁned by the position p = (x, y) and the\nvelocity v = (vx, vy).\nActions. Actions represent the acceleration a. As the car can be accelerated\nwith values {−1, 0, 1} in the x and in the y dimension, there are exactly 32 = 9\ndiﬀerent actions available in every state.\nTransitions. We assume a wet road, so with a chance of 0.1, the acceleration\ncannot be applied, i.e. a = (0, 0). Otherwise, with probability 0.9, the accelera-\ntion is as selected by the action. The new velocity v′ = (vx ′, vy ′) is given by the\nsum of the acceleration a = (ax, ay) and the current velocity. The new position\np′ = (x′, y′) is given by adding v′ to the current position, i.e.\nvx\n′ = vx + ax,\nx′ = x + vx,\nvy\n′ = vy + ay,\ny′ = y + vy.\nTo deﬁne several properties, we use a discretization of transitions of the MDP\nsimilar to the one of Bonet & Geﬀner [3]. The corresponding driving trajectory\nis a sequence of visited positions\nT = ⟨(x0, y0) , (x1, y1) , . . . , (xn−1, yn−1) , (xn, yn)⟩,\nsuch that\nT =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n⟨(x, y)⟩\nif vx ′ = 0 and vy ′ = 0 (1)\n⟨(x, y) , (x + σx, y) , (x + 2 · σx, y) . . . , (x′, y′)⟩\nif vx ′ ̸= 0 and vy ′ = 0 (2)\n⟨(x, y) , (x, y + σy) , (x, y + 2 · σy) . . . , (x′, y′)⟩\nif vx ′ = 0 and vy ′ ̸= 0 (3)\n⟨(x, y) , (x + σx, ⌊y + my⌉) , (x + 2 · σx, ⌊y + 2 · my⌉) . . . , (x′, y′)⟩\nif vx ′ ̸= 0 and vy ′ ̸= 0\nand |vx ′| ≥|vy ′|\n(4)\n⟨(x, y) , (⌊x + mx⌉, y + σy) , (⌊x + 2 · mx⌉, y + 2 · σy) . . . , (x′, y′)⟩\nif vx ′ ̸= 0 and vy ′ ̸= 0\nand |vx ′| < |vy ′|\n(5),\nwhere σx = sgn(vx ′), σy = sgn(vy ′) and mx =\nvx\n′\n|vy ′|, my =\nvy\n′\n|vx ′| [5].\nIf either the vertical or horizontal speed is 0, exactly all grid coordinates\nbetween p = (x, y) and p′ = (x′, y′) are contained in the trajectory. Otherwise,\nwe consider n equidistant points on the linear interpolation between the two\npositions and for each one round to the closest position on the map. While in the\noriginal discretization n = |vx ′| [3], in this model it is given by max (|vx| , |vy|).\nThe former is problematic when having a velocity which moves less into the\nx than into the y direction, as then only few points will be contained in the\ntrajectory and counterintuitive results may be produced.\nWe consider a transition to be valid, if and only if it does not crash, i.e.\nno position p ∈T is either a wall or outside of the map. A transition is said\nTracking the Race Between Deep RL and Imitation Learning\n5\nto reach the goal, if and only if one of the positions p ∈T is on the goal line.\nAdditionally, a transition cannot be invalid and reach the goal. If a transition\nfulﬁlls the conditions for both, only the one that was fulﬁlled ﬁrst holds. In\nwords: if a car has already reached the goal, it cannot crash anymore and vice\nversa.\nA transition leads to a state with new position p′ if it is valid and does not\nreach the goal. If it is invalid, it leads to a bottom state ⊥that has no further\ntransitions. Otherwise, i.e. it is reaching the goal, it leads to the goal state ⊤.\nRewards/Costs. As we consider both, planning and learning approaches, we\ndeﬁne the following two cost functions: For planning we consider a uniform\ncost function, such that an optimal planner will ﬁnd the shortest path to reach\nthe goal line. For reinforcement learning, we consider a reward function that is\npositive if the step reaches the goal, negative if the step is invalid and 0 otherwise.\nMore concretely, we chose\nR\n\u0012\ns\n(ax ,ay)\n−−−−→s′\n\u0013\n=\n\n\n\n\n\n100\nif s′ = ⊤\n−50\nif s′ = ⊥\n0\notherwise\nfor the reward of a transition.\nAs reinforcement learning makes use of discounting, both functions motivate\nto reach the goal as fast as possible.\n2.3\nSimulation\nFor a given map, we consider several variants of the simulation.\n1. Normal start (NS) versus random start (RS): Usually a game starts on the\nstart line, but we also consider the slightly more diﬃcult task of starting on\na random (valid) position on the map.\n2. Zero velocity (ZV) versus random velocity (RV): Usually a game starts with\nvelocity (0, 0), but we further use a variant starting with a random velocity\n(vx, vy) between 0 and a given upper bound.\n3. Noisy (N) versus deterministic (D): Usually the chosen acceleration is applied\nwith the rules given above. When the deterministic option is set, the chosen\nacceleration is always applied without assuming a wet road, i.e. without a\nchance to ignore the acceleration and keep the velocity unchanged.\n3\nLearning – Approaches\nWe consider two diﬀerent learning approaches that are based on diﬀerent princi-\nples. Imitation learning is based on labeled training data, while deep reinforce-\nment learning is based on self-play without prior knowledge.\n6\nT. Gros et al.\n3.1\nImitation Learning\nWe consider both passive and active imitation learning. For passive imitation\nlearning, we use (1) logistic regression (LR) and linear discriminant analysis\n(LDA) to train linear functions, and (2) stochastic gradient descent to train\nneural networks. To represent the class of active imitation learning algorithms,\nwe consider DAGGER [14].\nDAGGER. Dataset Aggregation (DAGGER) is a meta-algorithm for active im-\nitation learning. The main idea of DAGGER is to mitigate the problem related\nto the disruption of the independently identical distributed (i.i.d.) assumption\nin passive imitation learning for sequential decision making. The trained agent is\nthen used to iteratively sample more labeled data to train a new neural network.\nThe algorithm starts with a pre-trained neural network using the following steps:\n(i) It follows the current action policy to explore the state space.\n(ii) For every visited state, it uses an expert to ﬁnd the action that shall be\nimitated.\n(iii) It adds the pairs of state and action to the training set, and\n(iv) trains a new policy on the enlarged data set.\nStep (i) can be varied via a hyper-parameter β ∈[0, 1] that sets the ratio of\nfollowing the current policy or the expert for exploration. With β = 0 it follows\nthe current policy only. Step (iii) can be done with any thinkable expert and\nstep (iv) can be done with any training procedure.\n3.2\nDeep Reinforcement Learning\nWhile there are many diﬀerent approaches of deep reinforcement learning, e.g.\npolicy-based methods [10] or methods based on Monte Carlo tree search [16, 18],\nwe here focus on the value-based approach of deep Q-learning [12].\nDeep Q-learning. Given an MDP, we train an agent which represents a policy\nsuch that the expected cumulative reward of the MDP’s episodes is maximized.\nAs (potentially) a race can last forever, the task is a continuing one [19] and the\naccumulated future reward, the so-called return, of step t is therefore given by\nGt = P∞\ni=t γi · Ri+1, where γ is a discount factor with γ ∈[0, 1] and we assume\nthat Ri+1 is the reward obtained during the transition from the state Si to state\nSi+1 for i ∈{0, 1, . . .} [19].\nFor a ﬁxed state s, an action a, and a policy π, the action-value qπ(s, a) gives\nthe expected return that is achieved by taking action a in state s and following\nthe policy π afterwards, i.e.\nqπ(s, a) = Eπ [Gt | St = s, At = a] = Eπ\n\u0002P∞\nk=0 γkRt+k+1\n\f\f St = s, At = a\n\u0003\n.\nWe write q∗(s, a) for the optimal action-value function that maximizes the\nexpected return. The idea of value-based reinforcement learning methods is to\nTracking the Race Between Deep RL and Imitation Learning\n7\nﬁnd an estimate Q(s, a) of the optimal action-value function. Artiﬁcial neural\nnetworks can express complex non-linear relationships and are able to generalize.\nHence, they have become popular for function approximation. We estimate the\nQ-value function using a neural network with weights θ, a so-called deep Q-\nnetwork (DQN) [11]. We denote the DQN by Q(s, a; θ) and optimize it w.r.t. the\ntarget\ny(s, a; θ) = E [Rt+1 + γ · maxa′ Q(St+1, a′; θ) | St = s, At = a] .\n(1)\nThus, in iteration i the corresponding loss function is\nL(θi) = E\nh\n(y(St, At; θ−) −Q(St, At; θi))2i\n,\n(2)\nwhere θ−refers to the parameters from some previous iteration, with the so-\ncalled ﬁxed target [12] y(St, At; θ−). We optimize the loss function by stochastic\ngradient descent using an approximation of ∇L(θi) [12].\nFurthermore, we apply the idea of experience replay [12]. Instead of directly\nlearning from observations, we store all experience tuples in a data set and\nsample uniformly from that set.\nWe generate our experience tuples by exploring the state space epsilon-\ngreedily, that is, with a chance of 1 −ϵ during the Monte Carlo simulation we\nfollow the policy that is implied by the current network weights and otherwise\nuniformly choose a random action [12].\nIn the following, we will use the terms reinforcement learning (RL) and deep\nreinforcement learning (DRL) interchangeably.\n4\nTraining Racetrack Agents\nIn this section we describe the training process of agents based on active and\npassive imitation learning as well as deep reinforcement learning.\nState Encoding. Although a state in the Racetrack problem is uniquely given by\nthe car’s position and velocity, we provide several other features that can be used\nas state encoding to improve the learning procedure. Instead of giving a complete\nencoding of the grid to the agent, the following features will be provided. These\nfeatures correspond well to the idea of Racetrack being a model of autonomous\ndriving control.\n– d1, . . . , d8: linear distance to a wall in all directions. These eight distances\nare distributed equally around the car position and are given analogously to\nthe acceleration, i.e. −1, 0 or 1 in both dimensions.\n– dgx, dgy: distance to the nearest goal ﬁeld in x and y dimension, respectively.\n– dg: total goal distance, i.e. |dgx| +\n\f\fdgy\n\f\f.\nTogether with the position and the velocity, this gives us a total of 15 features\nper state. We use these features for all considered learning approaches.\n8\nT. Gros et al.\nObjective Function. The learning methods that we consider rely on two diﬀerent\nobjective functions: DRL uses the reward function and imitation learning uses\ndata sets that were optimized w.r.t. the number of steps until the goal is reached.\nAs DRL makes use of discounting (see Section 3.2), the accumulated reward is\nhigher if less steps are taken. Thus, both objective functions serve the same\npurpose, even though they are not completely equivalent. Note that a direct\nmapping from the costs used in the planning procedure to the reward structure\nwas not possible. We tested diﬀerent reward structures for DRL and found that a\nnegative reward for each single step combined with a positive reward for the goal\nand a negative reward for invalid states led to very poor convergence properties of\nthe training procedure. No well-performing alternative was found to the reward\nstructure deﬁned in Section 2.2 up to scaling.\n4.1\nImitation Learning\nWe want to train agents for all simulation scenarios including those where the\ncar starts at an arbitrary position on the map and visits future positions on\nthe map with diﬀerent velocities. Usually, all learning methods are based on\nthe assumption that the data is i.i.d.. Data that is generated via simulation\nof the Racetrack greatly disrupts this assumption. Thus, we propose diﬀerent\napproaches for data generation to encounter this problem.\nData Sets. In the base case, we uniformly sample states and velocities for the\nsimulation scenarios described in Section 2.3. The samples are then labeled by an\nexpert. This expert basically is a Racetrack-tailored version of the A∗algorithm\nto ﬁnd an optimal action (there might be more than one), i.e. acceleration, from\nthe current state.\nWe further use additional options that can be set when sampling data to\naddress the problem of decisions depending on each other:\n– Complete trajectory (T): If this option is set, all states on the way to the\ngoal are added to the data set instead of only the current state.\n– Exhaustive (E): If the exhaustive option is set, all optimal solutions for the\nspeciﬁed state are added to the data set.\n– Unique (U): Only states having a unique optimal acceleration are added to\nthe data set.\nOption E excludes option T due to runtime constraints as the number of optimal\ntrajectories increases exponentially with the trajectory’s length.\nThis leads to a total of 6 diﬀerent combinations as displayed in Table 1.\nThe ﬁrst data set contains uniformly sampled (valid) positions and velocities\nand combines them with a single optimal action. This explores the whole state\nspace equally. The data sets (2) and (3) diﬀer in their starting points. For (2),\nthe car is positioned on the start line, for (3) it might be anywhere on the map.\nBoth sets contain not only the optimal acceleration for this starting state, but\nfor every one visited on the trajectory from there on to the goal. To do both,\nTracking the Race Between Deep RL and Imitation Learning\n9\nTable 1. Racetrack conﬁgurations used to create our data sets.\nNo\nID\nDescription\nRS RV T E U\n(1)\nRS-RV\nUniform sample from all positions on map and all\npossible velocities.\n\u0013\n\u0013 \u0017 \u0017 \u0017\n(2) NS-ZV-T Uniform sample from all positions on the start line;\ncombined with zero velocity. All states that were vis-\nited on the optimal trajectory to the goal line are\nincluded in the data set.\n\u0017\n\u0017 \u0013 \u0017 \u0017\n(3) RS-ZV-T Uniform sample from all positions on the map; com-\nbined with zero velocity. All states that were visited\non the optimal trajectory to the goal line are included\nin the data set.\n\u0013\n\u0017 \u0013 \u0017 \u0017\n(4) RS-RV-T Uniform sample from all positions on the map and all\npossible velocities. All states visited on the optimal\ntrajectory to the goal line are included in the data\nset.\n\u0013\n\u0013 \u0013 \u0017 \u0017\n(5) RS-RV-E Uniform sample from all positions on the map and all\npossible velocities. All optimal actions for that state\nare included in the data set.\n\u0013\n\u0013 \u0017 \u0013 \u0017\n(6) RS-RV-U Uniform sample from all positions on the map and\nall possible velocities. Only such states that have a\nunique optimal action are included in the data set.\n\u0013\n\u0013 \u0017 \u0017 \u0013\nuniformly sample through the state space and take into account the trajectories,\n(4) starts with a random position and a random velocity but still collects the\nwhole trace. The data set (5) includes all optimal solutions instead of just one.\nApart from that, (5) is similar to set (1). (6) only includes entries that have a\nunique optimal next action.\nFor each learning method, we train several instances; at least one on each\ndata set. Each data set consists of approximately 105 entries.\nPassive Imitation Learning\nLinear Predictors. While deep learning clearly is more powerful than linear\nlearning, linear classiﬁers have the advantage that their decisions are more trans-\nparent.\nWe use the package sklearn to apply both Linear Discriminant Analysis\n(LDA) and Logistic Regression (LR). Together with the six combinations of\ndata sets, this gives 12 diﬀerent agents.\nNeural Networks. We use the PyTorch package to train neural networks [8].\nWe repeatedly iterate over the labeled training data. We use the MSE as loss\nfunction. As neural networks tend to overﬁt when the training iterates over\n10\nT. Gros et al.\nthe training data too often, we store the neural network after every iteration.\nWe experimentally found that a maximum iteration number of 20 is more than\nsuﬃcient. As we again use every 6 data sets, this gives us a total of 120 agents.\nAs explained in Section 2, a state is represented by 15 features, which gives\nus the input size of the network. There are 9 possible actions. As we do not\nprocess the game via an image but through predeﬁned features, we only use\nfully connected layers. More sophisticated network structures are only needed\nfor complex inputs such as images. For a fair comparison, we use the same\nnetwork size for all methods. We use two hidden layers of size 64, resulting in a\nnetwork structure of 15 × 64 × 64 × 9.\nActive Imitation Learning\nDAGGER. In the case of active imitation learning, we applied DAGGER using\nβ = 0 for all iterations, i.e. after the pre-training we followed the trained agent\nwithout listening to the expert for exploration. To have a fair comparison, DAG-\nGER has the same number of samples as PIL, i.e. 105. Still, the pre-training is\nimportant for sampling within the ﬁrst iteration of the algorithm, but the main\nidea is to generate further entries that are more important for the training of\nthe agent. Thus, we pre-trained the agent on each of our data sets and then\nadditionally allowed DAGGER to add 105 samples. We split these 105 samples\ninto 20 iterations. The neural network was trained by performing eight itera-\ntions over the data set. Our experiments with the networks showed that this is\nthe best trade-oﬀbetween over- and under-ﬁtting. Again, we store the trained\nagents after every iteration, giving us a total of 120 agents for the DAGGER\nmethod.\n4.2\nDeep Reinforcement Learning\nDeep Q-learning. In contrast to imitation learning, reinforcement learning is\nnot based on data sets and thus is not applied to any of the data sets given\nin Table 1. Training is done by self-play only; the Racetrack agent chooses its\nactions using a neural network and applies them to the environment. After every\nmove, the next state (given by the features), a reward as deﬁned in Section 2\nthat was achieved for the move, as well as the information whether the episode\nis terminated are returned to the agent. The agent then uses the loss, deﬁned\nagain by the MSE function, between the accumulated reward and the expected\nreturn to correct the weight of the network.\nAll imitation learning agents were trained with 105 (new) samples using the\nsame network structure. Therefore, we here restrict the agents to (1) 105 entries\nin the replay buﬀer, i.e. the maximal number of entries an agent can learn from\nat the same time, and (2) 105 episodes that the agent can play at all. The neural\nnetwork is not pre-trained but initiated randomly.\nTo have a trade-oﬀbetween exploration and exploitation, our agent acts ϵ-\ngreedy, i.e. with a probability of ϵ it chooses a random acceleration instead of\nTracking the Race Between Deep RL and Imitation Learning\n11\nthe best acceleration to explore the state space. As our DRL agent is initiated\nrandomly – and thus starts without any experience about what good actions\nare – in the beginning of the training phase the focus lies on exploration. We\ntherefore begin our training with ϵ = 1, i.e. always choosing a random action.\nAfter every episode i, we decrease ϵ exponentially with a factor λ = 0.999 to\nshift the focus from exploration to exploitation during training, until a threshold\nof ϵend = 0.0001 is reached, i.e. ϵi+1 = max (ϵi · λ, ϵend).\nTo train the agents to not just reach the goal but to minimize the number of\nsteps, we make use of a discount factor γ = 0.99.\nTable 2. Racetrack conﬁgurations used to train Racetrack agents with deep reinforce-\nment learning.\nNo\nID\nDescription\nRS D\n1 NS-D Starting on a random position on the start line using\nthe deterministic simulation.\n\u0017 \u0013\n2 NS-N Starting on a random position on the start line using\nthe noisy simulation.\n\u0017 \u0017\n3 RS-D Starting on a random position on the map using the\ndeterministic simulation.\n\u0013 \u0013\n4 RS-N Starting on a random position on the map using the\nnoisy simulation\n\u0013 \u0017\nBesides the given options of either starting on the start line (NS) or anywhere\non the map (RS), DRL can beneﬁt from learning while the noisy (N) version of\nRacetrack is simulated instead of the deterministic (D) one. This gives us four\ndiﬀerent training modes listed in Table 2.\nTo determine the best network weights, the average return over the last\n100 episodes during the training process is used. We save the network weights\nthat achieve the best result. The training progress is displayed in Figure 2.\nAdditionally, we save the network weights after running all training episodes,\nindependent from the received average return. This results in total to 8 diﬀerent\nDRL agents.\n5\nResults\nFor evaluation, we consider all possible combinations given by the simulation\nparameters as described in Section 2.3. In total, it results in 6 diﬀerent simulation\nsettings on which we compare the trained agents. These settings are given in\nTable 3. The combinations with NS and RV are not considered, as they include\nmore starting states where a crash is inevitable than solvable ones.\nIn the sequel, for each learning method we present the best-performing pa-\nrameter combination of all those that we tested. We investigate three aspects of\n12\nT. Gros et al.\nFig. 2. Training progress of the RL agent. The left graph shows the RS-N mode, the\nright one displays NS-D. The right plot further displays a temporarily decrease of the\nreturn, which is not uncommon during training.\nthe behavior of the resulting agents: the success rate, the quality of the resulting\naction sequences, and the relative number of optimal and fatal decisions.\nTable 3. Conﬁgurations on which we evaluate the agents.\nNo\nID\nDescription\nRS RV D\n1 NS-ZV-D Starting on a random position on the start line with zero\nvelocity using the deterministic simulation\n\u0017\n\u0017 \u0013\n2 NS-ZV-N Starting on a random position on the start line with zero\nvelocity using the noisy simulation\n\u0017\n\u0017\n\u0017\n3 RS-ZV-D Starting on a random position on the map with zero ve-\nlocity using the deterministic simulation\n\u0013\n\u0017 \u0013\n4 RS-ZV-N Starting on a random position on the map with zero ve-\nlocity using the noisy simulation\n\u0013\n\u0017\n\u0017\n5 RS-RV-D Starting on a random position on the map with a random\nvelocity using the deterministic simulation\n\u0013\n\u0013 \u0013\n6 RS-RV-N Starting on a random position on the map with a random\nvelocity using the noisy simulation\n\u0013\n\u0013 \u0017\n5.1\nSuccess Rate\nWe ﬁrst compare how often the agents win a game, i.e. reach the goal, or loose,\ni.e. crash into a wall. We limit the game to 1000 steps. If an agent then neither\nTracking the Race Between Deep RL and Imitation Learning\n13\n0\n2000\n4000\n6000\n8000\n10000\n# Number of Episodes\nRL\nDAGGER\nNN\nLDA\nLR\n99.27%\n64.24%\n47.61%\n40.16%\n51.22%\n0.73%\n35.76%\n52.39%\n59.84%\n48.78%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\nNS_ZV_N\nWin\nLoose\nTimeout\n0\n2000\n4000\n6000\n8000\n10000\n# Number of Episodes\nRL\nDAGGER\nNN\nLDA\nLR\n100.0%\n92.37%\n57.1%\n24.32%\n49.51%\n0.0%\n7.63%\n42.9%\n75.68%\n50.49%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\nRS_ZV_D\n0\n2000\n4000\n6000\n8000\n10000\n# Number of Episodes\nRL\nDAGGER\nNN\nLDA\nLR\n99.28%\n76.67%\n47.79%\n22.86%\n46.04%\n0.72%\n23.33%\n52.21%\n77.14%\n53.96%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\nRS_ZV_N\n0\n2000\n4000\n6000\n8000\n10000\n# Number of Episodes\nRL\nDAGGER\nNN\nLDA\nLR\n65.05%\n60.04%\n42.95%\n16.02%\n38.02%\n34.95%\n39.96%\n57.05%\n83.94%\n61.98%\n0.0%\n0.0%\n0.0%\n0.04%\n0.0%\nRS_RV_D\n0\n2000\n4000\n6000\n8000\n10000\n# Number of Episodes\nRL\nDAGGER\nNN\nLDA\nLR\n60.09%\n44.96%\n33.81%\n15.62%\n31.47%\n39.91%\n55.04%\n66.19%\n84.38%\n68.53%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\nRS_RV_N\nFig. 3. Success rate results for all classes of examined agents.\n14\nT. Gros et al.\nsucceeded nor failed we count the episode as timed out. We compare the agents\non 104 simulation runs. For each single run of the simulation, all agents start in\nthe same initial state. The results can be found in Figure 3.\nWe omitted the plot for NS-ZV-D, as all of the agents had 100% winning\nrate. The linear agents perform worst. Especially with random starting points\nand velocities, they fail to reach the goal. DAGGER outperforms the passive\nimitation learning agents. This is not surprising, as it has been designed to cope\nwith sequential decision making.\nThroughout all settings, the DRL agents perform best. They clearly out-\nperform DAGGER, reaching the goal more than 1.5 times more often in the\nNS-ZV-N setting.\n5.2\nQuality of Action Sequences\nWe illustrate results for the quality of the chosen action sequences in Figure 4.\nThe left plot gives the cumulative reward reached by the agents averaged over\nRS_ZV_N\nRS_RV_N\nRS_RV_D\nRS_ZV_D\nNS_ZV_D\nNS_ZV_N\nSimulation Setting\n20\n0\n20\n40\n60\n80\nReturn\nRS_ZV_N\nRS_RV_N\nRS_RV_D\nRS_ZV_D\nNS_ZV_D\nNS_ZV_N\nSimulation Setting\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nSteps\nRL\nLDA\nLR\nNN\nDAGGER\nFig. 4. Average reward (left) and average number of needed steps (right) for all classes\nof agents.\nall runs (also over those that are not successful). DRL clearly achieves the high-\nest cumulative reward. We remark that the optimal policies computed via A∗\ngive higher cumulative rewards as the goal is reached faster. However, imitation\nlearning achieves lower results on average as it fails more often.\nThe right of Figure 4 shows results for the number of steps needed. When a\ncar crashes, we are not interested in the number of steps taken. Therefore – in\nthis speciﬁc analysis – we only report on successful runs. They show that – while\nreinforcement learning has the most wins and is the best agent considering the\nTracking the Race Between Deep RL and Imitation Learning\n15\n0\n500\n1000\n1500\n2000\n# Number of Episodes\nRL\nDAGGER\nNN\nLDA\nLR\n77.9%\n92.6%\n98.85%\n95.15%\n96.2%\n3.55%\n5.3%\n0.1%\n3.7%\n1.65%\n18.55%\n2.1%\n1.05%\n1.15%\n2.15%\nNS_ZV_D\nOptimal\nSecure\nFatal\n0\n500\n1000\n1500\n2000\n# Number of Episodes\nRL\nDAGGER\nNN\nLDA\nLR\n70.6%\n84.9%\n95.65%\n88.7%\n89.85%\n1.35%\n5.85%\n0.65%\n3.7%\n4.65%\n28.05%\n9.25%\n3.7%\n7.6%\n5.5%\nRS_ZV_D\n0\n500\n1000\n1500\n2000\n# Number of Episodes\nRL\nDAGGER\nNN\nLDA\nLR\n67.85%\n84.95%\n95.3%\n89.7%\n91.0%\n5.95%\n8.1%\n1.4%\n4.7%\n4.65%\n26.2%\n6.95%\n3.3%\n5.6%\n4.35%\nRS_RV_D\nFig. 5. Quality of selected actions.\nreward objective – it is consuming the highest number of steps when reaching\nthe goal. It even takes more steps than linear classiﬁers.\n5.3\nQuality of Single Action Choices\nNext we examine whether the agents choose the optimal acceleration, i.e. the\nacceleration that does not crash and leads to the goal with as few steps as pos-\nsible, for diﬀerent positions and velocities. We distinguish between (1) optimal\nactions, (2) fatal actions that unavoidably lead to a crash, and (3) secure actions\nthat are neither of the former. We use the same settings as before, except for the\nones with noise, which does not make sense when considering optimal actions,\ni.e. NS-ZV, RS-ZV and RS-RV.\n16\nT. Gros et al.\nThe results are given in Figure 5. Especially when we start from a random\nposition on the map, we see that (independent from the setting) passive imitation\nlearning with neural networks selects optimal actions more often than active\nimitation learning or deep reinforcement learning. Interestingly, DAGGER and\nRL select both secure and fatal choices more often than PIL.\n5.4\nDiscussion\nWe found that passive imitation learning agents perform poorly (see Figure 3)\neven though they select optimal actions most often. One reason for this is that the\ndata sets from which they learn contain samples that have not been generated by\niteratively improving the current policy. Hence, it is not biased towards sequences\nof dependent decisions leading to good performance. We have observed that\nDAGGER and in particular DRL sometimes do not select optimal actions, but\nthose with lower risk of hitting a wall. As a result, they need more steps than\nother approaches before reaching the goal, but the trajectories they use are more\nsecure and they crash less often. This is an interesting insight, as all approaches\n(including PIL) try to optimize the same objective: reach the goal as soon as\npossible without hitting a wall.\nThe fact that both, DAGGER and RL have a relatively high number of fatal\nactions, but not an increased number of losses, leads us to the assumption that\nthese agents avoid states where they might make fatal decisions, even though\nthese states could help reaching the goal faster.\nFigure 6 illustrates the paths taken by the diﬀerent agents for the easiest\ncase (NS-ZV-D) where all policies reach their goal. DRL diﬀers the most from\nthe optimal (black) trajectory, which describes one of the shortest paths to the\ngoal and obtains the maximum cumulative reward. For the harder setting where\na starting point is chosen randomly (RS-ZV-D), only DAGGER and DRL make\nit to the goal, with DRL using signiﬁcantly more steps than the optimal agent.\nIn summary, DRL performs surprisingly well. In some aspects, it performs\neven better than active imitation learning, which is not only considered a state\nof the art for sequential decision making [7], but – in contrast to DRL – even\nhas the chance to beneﬁt from expert knowledge.\n6\nConclusion\nWe have presented an extensive comparison between diﬀerent learning approaches\nto solve the Racetrack benchmark. Even though we provided optimal decisions\nduring imitation learning, the agents based on deep reinforcement learning out-\nperform those of imitation learning in many aspects.\nWe believe that our observations carry over to other applications, in particu-\nlar to more complex autonomous vehicle control algorithms. We plan to consider\nextensions of the Racetrack problem, which include further real-world charac-\nteristics of autonomous driving. We believe that, to address the diﬃculties we\nTracking the Race Between Deep RL and Imitation Learning\n17\nFig. 6. Traces of diﬀerent Racetrack agents. The black trajectories are optimal. The\nother colors are chosen as in Figure 4. The upper plot is a NS-ZV-D simulation, while\nthe lower shows RS-ZV-D.\nobserved with imitation learning, further investigations into the combination of\nexpert data sets and reinforcement learning agents are necessary.\nAdditionally, other methods of guiding the agents to more promising solutions\nduring training will be examined, such as reward shaping [9], and their inﬂuence\non the characteristics of the ﬁnal agent. Another interesting question for future\nwork is whether multi-objective reinforcement learning can be used to adjust the\nagents’ behavior in a ﬁne-grained manner.\nAcknowledgements\nThis work has been partially funded by DFG grant 389792660 as part of TRR 248\n(see https://perspicuous-computing.science)\nReferences\n1. Agostinelli, F., McAleer, S., Shmakov, A., Baldi, P.: Solving the Rubik’s Cube with\ndeep reinforcement learning and search. Nature Machine Intelligence 1(8), 356–363\n(2019)\n2. Barto, A.G., Bradtke, S.J., Singh, S.P.: Learning to act using real-time dynamic\nprogramming. Artiﬁcial Intelligence 72(1-2), 81–138 (1995)\n18\nT. Gros et al.\n3. Bonet, B., Geﬀner, H.: GPT: A tool for planning with uncertainty and partial\ninformation. In: Proceedings of the IJCAI Workshop on Planning with Uncertainty\nand Incomplete Information. pp. 82–87 (2001)\n4. Gros, T.P., Hermanns, H., Hoﬀmann, J., Klauck, M., Steinmetz, M.: Deep sta-\ntistical model checking. In: Proceedings of the 40th International Conference on\nFormal Techniques for Distributed Objects, Components, and Systems (FORTE).\npp. 96–114. Springer (2020)\n5. Gros,\nT.P.,\nHermanns,\nH.,\nHoﬀmann,\nJ.,\nKlauck,\nM.,\nSteinmetz,\nM.:\nModels\nand\nInfrastructure\nused\nin\n”Deep\nStatistical\nModel\nChecking”.\nhttp://doi.org/10.5281/zenodo.3760098 (2020)\n6. Gros, T.P., H¨oller, D., Hoﬀmann, J., Wolf, V.: Tracking the race between deep\nreinforcement learning and imitation learning. In: Proceedings of the 17th Interna-\ntional Conference on Quantitative Evaluation of SysTems (QEST). Springer (2020)\n7. Judah, K., Fern, A.P., Dietterich, T.G., Tadepalli, P.: Active imitation learning:\nFormal and practical reductions to i.i.d. learning. Journal of Machine Learning\nResearch 15(120), 4105–4143 (2014)\n8. Ketkar, N.: Introduction to pytorch. In: Deep learning with Python, pp. 195–208.\nSpringer (2017)\n9. Laud, A.D.: Theory and application of reward shaping in reinforcement learning.\nTech. rep. (2004)\n10. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver,\nD., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning. In:\nInternational conference on machine learning. pp. 1928–1937 (2016)\n11. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,\nRiedmiller, M.: Playing Atari with deep reinforcement learning. In: NIPS Deep\nLearning Workshop (2013)\n12. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,\nGraves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Beattie, C.,\nSadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis,\nD.: Human-level control through deep reinforcement learning. Nature 518(7540),\n529–533 (2015)\n13. Pineda, L.E., Zilberstein, S.: Planning under uncertainty using reduced models:\nRevisiting determinization. In: Proceedings of the 24th International Conference\non Automated Planning and Scheduling (ICAPS). pp. 217–225. AAAI Press (2014)\n14. Ross, S., Gordon, G.J., Bagnell, D.: A reduction of imitation learning and struc-\ntured prediction to no-regret online learning. In: Proceedings of the 14th Inter-\nnational Conference on Artiﬁcial Intelligence and Statistics (AISTATS). JMLR\nProceedings, vol. 15, pp. 627–635. JMLR.org (2011)\n15. Schaal, S.: Is imitation learning the route to humanoid robots? Trends in cognitive\nsciences 3(6), 233–242 (1999)\n16. Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G.,\nSchrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S.,\nGrewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M.,\nKavukcuoglu, K., Graepel, T., Hassabis, D.: Mastering the game of Go with deep\nneural networks and tree search. Nature 529, 484–503 (2016)\n17. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot,\nM., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., Hassabis, D.:\nA general reinforcement learning algorithm that masters Chess, Shogi, and Go\nthrough self-play. Science 362(6419), 1140–1144 (2018)\nTracking the Race Between Deep RL and Imitation Learning\n19\n18. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A.,\nHubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre,\nL., van den Driessche, G., Graepel, T., Hassabis, D.: Mastering the game of Go\nwithout human knowledge. Nature 550, 354–359 (2017)\n19. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. Adaptive\ncomputation and machine learning, The MIT Press, second edn. (2018)\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-08-03",
  "updated": "2020-08-03"
}