{
  "id": "http://arxiv.org/abs/2008.01171v1",
  "title": "Deep Reinforcement Learning using Cyclical Learning Rates",
  "authors": [
    "Ralf Gulde",
    "Marc Tuscher",
    "Akos Csiszar",
    "Oliver Riedel",
    "Alexander Verl"
  ],
  "abstract": "Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning\nof hyperparameters to successfully resolve problems. One of the most\ninfluential parameters in optimization procedures based on stochastic gradient\ndescent (SGD) is the learning rate. We investigate cyclical learning and\npropose a method for defining a general cyclical learning rate for various DRL\nproblems. In this paper we present a method for cyclical learning applied to\ncomplex DRL problems. Our experiments show that, utilizing cyclical learning\nachieves similar or even better results than highly tuned fixed learning rates.\nThis paper presents the first application of cyclical learning rates in DRL\nsettings and is a step towards overcoming manual hyperparameter tuning.",
  "text": "Deep Reinforcement Learning using Cyclical Learning Rates\nRalf Gulde*, Marc Tuscher*, Akos Csiszar, Oliver Riedel and Alexander Verl\nAbstract— Deep Reinforcement Learning (DRL) methods\noften rely on the meticulous tuning of hyperparameters to\nsuccessfully resolve problems. One of the most inﬂuential\nparameters in optimization procedures based on stochastic\ngradient descent (SGD) is the learning rate. We investigate\ncyclical learning and propose a method for deﬁning a general\ncyclical learning rate for various DRL problems. In this\npaper we present a method for cyclical learning applied to\ncomplex DRL problems. Our experiments show that, utilizing\ncyclical learning achieves similar or even better results than\nhighly tuned ﬁxed learning rates. This paper presents the ﬁrst\napplication of cyclical learning rates in DRL settings and is a\nstep towards overcoming manual hyperparameter tuning.\nI. INTRODUCTION\nDriven by the rapid increase of the amount of available\ndata and computational resources, models and algorithms for\ndeep neural networks have undergone remarkable develop-\nments and are state of the art in addressing fundamental tasks\nranging from computer vision problems like image classiﬁ-\ncation [1], scene segmentation [2], face recognition [3] to\nnatural language processing.\nIn Machine Learning (ML) hyperparameter tuning is the\nproblem of selecting a set of hyperparameters for an optimal\nlearning strategy. A hyperparameter is a parameter value that\nis used to control the learning process. The learning speed\nη is an essential hyperparameter and controls the rate of\nupdating the model. In particular, η controls the amount of\nerror assigned to update the weights of the model. The use\nof deep neural networks has brought signiﬁcant progress in\nsolving challenging problems in various ﬁelds using Deep\nReinforcement Learning (DRL). Reproducing existing work\nand accurately evaluating improvements offered by novel\nmethods is vital to maintain this progress.\nDRL problems vary from supervised deep learning prob-\nlems in one important aspect: the distribution from which\nthe data is taken is non-stationary. Transferring learning\nrate techniques from deep learning to deep reinforcement\nlearning is therefore not a trivial task. A common method\nfor determining the learning rate in supervised deep learning\napproaches is the learning rate decay. However, due to the\nnon-stationarity of the RL problem, training with linearly\ndecreasing learning rates is inferior, and training should be\ndone at different learning rates [4].\nThe main contributions of this work are:\nR. Gulde, M. Tuscher, A. Csiszar, O. Riedel and A. Verl are with the\nInstitute of Control Engineering of Machine Tools and Manufacturing Units,\nUniversity of Stuttgart, 70174 Stuttgart, Germany (phone: 0049-711-685-\n82505; fax: 0049-711-685-72505; e-mail: ralf.gulde@isw.uni-stuttgart.de).\nThe Research is supported by the Graduate School of Excellence advanced\nManufacturing Engineering and by the German Research Foundation (DFG).\n* These authors contributed equally.\n• We apply learning rate cycling to DRL problems by\ntraining an agent with the PP02 [5] algorithm.\n• We reduce the necessity of a manual learning rate\ntuning for particular RL environments by applying our\ngeneral cyclical learning rate for a variety of complex\nRL problems.\n• Our experiments show that cyclical learning rates\nachieve similar or even better performance in various\nenvironments compared to ﬁxed learning rates.\nThe paper is organized as follows: In section II we provide\na brief overview of the related work on the learning rate\ncycling, followed by a brief recapitulation of the theoret-\nical foundations of our method in section III. Section IV\nintroduces the proposed method, section V describes our\nexperiments, and section VI illustrates the critical parameters\nof our method. Concluding remarks are given in sectionVII.\nII. RELATED WORK\nVanilla gradient descent methods can be made more reli-\nable via line search [6]. Line search relies on computing the\nfull loss on the dataset to ﬁnd a good learning rate. However,\ncomputing the full loss and therefore the full ﬁrst derivative is\noften computationally expensive in settings where stochastic\ngradient descent (SGD) methods are used.\nSchaul et. al [4] propose an algorithm to compute optimal\nlearning rates for SGD on non-stationary problems. This\nmethod relies on expectations of the gradient and the square\nnorm of the gradient.\nSmith proposes cyclical learning rates for supervised deep\nlearning in [7]. Step sizes are estimated based on the size\nof an epoch. The base and maximum learning rate are\ndetermined using a method for learning rate identiﬁcation\nin which a model is trained for an epoch with linearly\nincreasing learning rates. Experiments show that cyclical\nlearning rates lead to signiﬁcant performance improvements.\nLearning rate cycling is also combined with a momentum\nthat increases and decreases counteractingly to the learning\nrate [8]. This improves the stability of the training process\nand increases the training speed, which is further improved\nby setting a very high maximum learning rate. However,\nsince high learning rates impose regularization, the number\nof other regularization techniques must be reduced [9].\nTo the best of the authors’ knowledge, there is no known\napproach that employs cyclical learning rates for DRL prob-\nlems. The introduction of a general cyclical learning rate\nresults in the acknowledgement of the non-stationarity of the\nDRL problem and the overcoming of manual hyperparameter\ntuning.\narXiv:2008.01171v1  [cs.LG]  31 Jul 2020\nIII. BACKGROUND\nThis section covers the theoretical foundations for our\nexperiments.\nA. Reinforcement Learning and Policy Optimization\nWe deﬁne the problem of reinforcement learning (RL) and\nintroduce the notation we use throughout the paper. In this\npaper we consider discounted Markov decision processes\n(MDP) with a ﬁnite horizon. At each time step t the RL\nagent observes the current state st ∈S, performs an action\nat ∈A and then receives a reward rt+1 ∈R. After that\nthe resulting state st+1 will be observed, determined by the\nunknown dynamics of the environment p(st+1|at, st). An\nepisode has a pre-deﬁned length T time steps. The goal\nof the agent is to ﬁnd a parameter θ of a policy πθ(a|s)\nthat maximizes the expected cummulated reward J over a\ntrajectory\nJ(πθ) = Eτ∼πθ\n\" T\nX\nt=0\nπ(at|st)\nT\nX\nk=t\nγk−trk+1\n#\n,\n(1)\nwhere γ ∈[0, 1] is the discount factor.\nRL methods solve an MDP by interacting with the system\nand accumulating the reward achieved. We consider several\nmodel-free policy gradient algorithms with open source\nimplementations that are common in the literature, e.g.,\nSoft Actor Critic [10], Deep Deterministic Policy Gradient\n(DDPG) [11], and Proximal Policy Optimization (PPO) [5].\nGenerally, PPO maximizes (1) using a robust version of the\npolicy gradient theorem\n∇θJ(πθ) = Eτ∼πθ\n\u0014\nT\nX\nt=0\n∇θ log π(at|st)\nT\nX\nk=t\nγk−trk+1\n\u0015\nby performing gradient ascent steps\nθk+1 = θk + α∇θJ(πθ).\n(2)\nB. Non-Stationarity of RL Problems\nDeep reinforcement learning and supervised deep learning\ndiffer in an important aspect: while the general optimization\nmethods and models can be quite similar, the data in deep\nRL methods is sampled from a non-stationary distribution.\nThat is, the data is sampled from an environment according\nto a continuously changing policy πθ. Every time we update\nour parameters θ, the policy πθ changes, and since every\ntrajectory τ is sampled while following πθ\nP(τ|πθ) = ρ0(s0)\nT −1\nY\nt=0\nP(st+1|st, at)πθ(at|st)\nour data distribution changes with a change in policy. Off-\npolicy RL methods often save trajectories τ in a replay buffer,\nwhile on-policy methods use the sampled trajectories only for\na single policy update.\nmaximum bound (             )\nminimum bound (             )\nstepsize\nlearning\nrate (    )\nFig. 1: Cyclical learning rate with maximum and minimum\nbound and stepsize [7].\nIV. CYCLICAL LEARNING RATES FOR DRL\nWe apply two kinds of cyclical learning rate policies\nto the DRL problem, the triangular- and the exp range-\npolicy. Herein, we comply to the terminology used in the\noriginal paper introducing cyclical learning rates [7]. The\ntriangular policy linearly increases the learning rate for\nstepsize number of iterations and then linearly decreases for\nstepsize iterations. Increments and decrements occur between\nthe maximum bound (ηmax) and the minimum bound (ηmin).\nThis approach is depicted in ﬁgure 1. The exp range policy\ninherits the properties of the triangular policy, while also\ndecaying the maximum ηmax and the minimum learning rate\nηmin over time. Before each cycle, we compute new learning\nrate bounds ηmin and ηmax by\nηmin = ηmin,0 · λkcycle\nηmax = ηmax,0 · λkcycle,\nwhere λ is an exponential decay factor and kcycle is the\ncurrent cycle count. The learning rate over timestep using\nthis method is depicted in ﬁgure 2, starting with ηmax,0 =\n0.001,\nηmin,0 = 0.0001 and an exponential decay factor\nλ = 0.99.\nSince no notion of training epochs is present in deep RL,\nepochs cannot be used to provide a good estimate for the step\nsize. We use a ﬁxed step size for all training runs. Further,\nwe use ﬁxed ηmax and ηmin, since determining reasonable\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nstep\n×106\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\nlearning rate\nexp range\nFig. 2: Exponential decay of ηmax and ηmin using the\nexp range method.\n(a) CartPole-v0\n(b) BipedalWalker-v3\n(c) Swimmer-v2\n(d) LunarLander-v3\nFig. 3: RL benchmarking environments used in our experiments.\nlearning rates for a non-stationary problem is non-trivial, as\ndepicted in section VI.\nTo increase the stability of the training process at high\nlearning rates, we increase and decrease the momentum of\nthe optimizer anti-proportional to the learning rate. That is,\nwhen the learning rate increases, the momentum decreases\nand vice versa.\nV. EXPERIMENTS\nThe Experiments are performed on various reinforcement\nlearning environments contained in openai/gym. In each\nexperiment, three reinforcement learning agents are trained\nusing the PPO2 algorithm (on the same environment) with\nthe same random seed. The optimal hyperparameters for\neach environment are obtained from rl-zoo [12]. Two agents\nare trained using the triangular and the exp range policy,\nrespectively, the remaining agent is trained with the optimal\nﬁxed learning rate for this environment. We compare the\nepisode reward over time steps of all agents.\nIn a variety of environments, the triangular learning rate\npolicy performs similar or better than the optimal ﬁxed\nlearning rate for the particular environment. Notably, we do\nnot tune ηmin, ηmax nor the step size s for any of our\nenvironments, but perform all experiments with a general\nsetting for learning rate cycling. These settings are ηmax =\n0.01,\nηmin = 0.0001,\ns = 2000. For the exp range\npolicy additionally use a decay factor λ = 0.99. Further,\nas described in section IV we cycle the momentum of\nthe optimizer between 1.0 and 0.8, anti-proportional to the\nlearning rate.\nResults of the experiments are shown in ﬁgure 4. Each plot\ncontains the episode reward of the training process using\nthe triangular policy (labelled as triangular), the training\nprocess using the exp range policy (labelled as exp range)\nand the training process using the optimal ﬁxed learning rate\n(labelled with η = 0.001).\nFigure 4a shows results of experiments on the CartPole-\nv0 environment. Learning rate cycling introduces a longer\nperiod of exploration in the beginning of the training process\nbut once converged, the episode reward does not drop in\na later stage of training. The exp range policy reduces\nthis effect substantially. Note, that for experiments in the\nBipedalWalker-v3 environment (Fig. 4b) our method applies\nlearning rates to the optimization process that signiﬁcant\nhigher (are two orders of magnitude) than the optimal learn-\ning rate of η = 0.00025. This experiment also shows a longer\nperiod of exploration using learning rate cycling, which is\nconsistent to the notion, that training at higher learning\nrates introduces regularization to the training process, e.g. no\noverﬁtting to a speciﬁc policy in the RL setting. In particular,\nwhen using learning rate cycling, the agent is capable of\nachieving an overall higher reward compared to the opti-\nmal learning rate for that particular environment. Again,\nthe effects are reduced when applying the exp range pol-\nicy. Experiments on the Swimmer-v2 environment (Fig. 4c)\ndemonstrate that training with cyclical learning rates can\nachieve higher rewards than training with a ﬁxed learning\nrate. The triangular policy results in a higher exploration\nwhile the exprange policy produces a more robust training\nprocess. Figure 4d shows experiments on the LunarLander-\nv3 environment wherein all training processes achieve similar\nresults. However, the triangular policy again imposes more\nexploration and the exp range policy is capable of achieving\nthe best results with the fastest convergence. In this setting,\nthe exponential decay is able to stabilize the learning process.\nVI. ABLATION STUDY\nSince general statements on methods in DRL and DL\noften do not hold when varying parameters, we perform an\nablation study to identify critical parameters of our method.\nThis helps to identify the most inﬂuential and essential\ncomponents for the success or failure of training.\nVery High LR. Very high learning rates lead to diver-\ngence. Deﬁning ηmax = 0.1 or higher leads to divergence\nin each environment of the experiments. This is due to the\ninstability that high learning rates impose on the optimization\nprocess.\nLearning Rate Finding. Learning rate ﬁnding is the\nprocedure of training a model with a few batches, each\ntraining step with a different learning rate. The optimal\nlearning rate is the one at which the loss decreases the\nmost. In the supervised setting, appropriate learning rates per\nepoch can be identiﬁed by evaluating varying learning rates\napplied to the training process. However, due to the non-\nstationarity of the RL problem, this technique is not trivially\ntransferable. This is especially the case when using on-policy\nRL, where each update is performed using a new batch from\nthe previous rollout. Transferring this learning rate ﬁnding\ntechnique and applying it to our training procedure leads to\ndivergence for each experiment in each environment.\nPendulum-v0. Training an agent at cyclical learning rates\nin the Pendulum-v0 environment leads to divergences in each\nenvironment. We attribute this to a poor combination of\nhyperparameters for this environment.\n0\n25000\n50000\n75000 100000 125000 150000 175000 200000\nstep\n0\n100\n200\n300\n400\n500\nepisode reward\nPPO2: CartPole-v0\ntriangular\nη = 0.001\nexp range\n(a)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nstep\n×106\n−40\n−30\n−20\n−10\n0\n10\n20\n30\nepisode reward\nPPO2: BipedalWalker-v3\ntriangular\nη = 0.00025\nexp range\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nstep\n×107\n−25\n0\n25\n50\n75\n100\n125\n150\n175\nepisode reward\nPPO2: Swimmer-v2\ntriangular\nη = 0.00025\nexp range\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nstep\n×107\n−600\n−400\n−200\n0\n200\nepisode reward\nPPO2: LunarLander-v3\ntriangular\nη = 0.00025\nexp range\n(d)\nFig. 4: Results of training DRL agents using the PPO2 algorithm in various RL environments. Each subﬁgure illustrates the\nepisode reward over time steps for three training processes on the same environment: applying the triangular policy, the\nexp range policy or using a tuned ﬁxed learning rate.\nVII. CONCLUSION\nWe apply learning rate cycling, ﬁrst introduced in [7],\nto DRL by training agents on various environments using\nthe PPO2 algorithm with cyclical learning. Experiments\nshow that, training with cyclical learning rates is capable of\ndeveloping strategies to achieve similar or better results than\ntraining with a ﬁxed learning rate. Most notably, our method\nis capable of achieving these results without manually tuning\nthe learning rate bounds for speciﬁc environments. Hence,\nwe take a step towards reducing the amount of hyper-\nparameters to be manually tuned in Deep Reinforcement\nLearning (DRL) training processes. Whether this technique\ncan successfully be applied to off-policy deep RL remains\nan open question. This seems especially appealing since off-\npolicy DRL training is known to be prone to divergence [13].\nREFERENCES\n[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2016, pp. 770–778.\n[2] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in\nProceedings of the IEEE international conference on computer vision,\n2017, pp. 2961–2969.\n[3] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed\nembedding for face recognition and clustering,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2015,\npp. 815–823.\n[4] T. Schaul, S. Zhang, and Y. LeCun, “No More Pesky Learning Rates,”\narXiv:1206.1106 [cs, stat], Feb. 2013.\n[5] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[6] J. E. Dennis Jr and R. B. Schnabel, Numerical methods for uncon-\nstrained optimization and nonlinear equations.\nSIAM, 1996.\n[7] L. N. Smith, “Cyclical Learning Rates for Training Neural Networks,”\narXiv:1506.01186 [cs], Apr. 2017.\n[8] ——, “A disciplined approach to neural network hyperparameters,”\narXiv preprint arXiv:1803.09820, 2018.\n[9] L. N. Smith and N. Topin, “Super-convergence: Very fast training of\nneural networks using large learning rates.”\n[10] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” arXiv preprint arXiv:1801.01290, 2018.\n[11] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Ried-\nmiller, “Deterministic policy gradient algorithms,” in ICML, 2014.\n[12] A.\nRafﬁn,\n“Rl\nbaselines\nzoo,”\nhttps://github.com/arafﬁn/\nrl-baselines-zoo, 2018.\n[13] R. S. Sutton, A. G. Barto et al., Introduction to reinforcement learning.\nMIT press Cambridge, 1998, vol. 135.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-07-31",
  "updated": "2020-07-31"
}