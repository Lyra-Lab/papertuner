{
  "id": "http://arxiv.org/abs/2008.01188v3",
  "title": "Learning to Play Two-Player Perfect-Information Games without Knowledge",
  "authors": [
    "Quentin Cohen-Solal"
  ],
  "abstract": "In this paper, several techniques for learning game state evaluation\nfunctions by reinforcement are proposed. The first is a generalization of tree\nbootstrapping (tree learning): it is adapted to the context of reinforcement\nlearning without knowledge based on non-linear functions. With this technique,\nno information is lost during the reinforcement learning process. The second is\na modification of minimax with unbounded depth extending the best sequences of\nactions to the terminal states. This modified search is intended to be used\nduring the learning process. The third is to replace the classic gain of a game\n(+1 / -1) with a reinforcement heuristic. We study particular reinforcement\nheuristics such as: quick wins and slow defeats ; scoring ; mobility or\npresence. The four is another variant of unbounded minimax, which plays the\nsafest action instead of playing the best action. This modified search is\nintended to be used after the learning process. The five is a new action\nselection distribution. The conducted experiments suggest that these techniques\nimprove the level of play. Finally, we apply these different techniques to\ndesign program-players to the game of Hex (size 11 and 13) surpassing the level\nof Mohex 3HNN with reinforcement learning from self-play without knowledge.",
  "text": "Learning to Play Two-Player Perfect-Information Games\nwithout Knowledge\nQuentin Cohen-Solala\naCRIL, Univ. Artois and CNRS, F-62300 Lens, France\nLAMSADE, Universit´e Paris-Dauphine, PSL, CNRS, France\nAbstract\nIn this paper, several techniques for learning game state evaluation functions by rein-\nforcement are proposed. The ﬁrst is a generalization of tree bootstrapping (tree learn-\ning): it is adapted to the context of reinforcement learning without knowledge based on\nnon-linear functions. With this technique, no information is lost during the reinforce-\nment learning process. The second is a modiﬁcation of minimax with unbounded depth\nextending the best sequences of actions to the terminal states. This modiﬁed search is\nintended to be used during the learning process. The third is to replace the classic gain\nof a game (+1 / −1) with a reinforcement heuristic. We study particular reinforcement\nheuristics such as: quick wins and slow defeats ; scoring ; mobility or presence. The\nfourth is another variant of unbounded minimax, which plays the safest action instead\nof playing the best action. This modiﬁed search is intended to be used after the learning\nprocess. The ﬁfth is a new action selection distribution. The conducted experiments\nsuggest that these techniques improve the level of play. We apply these different tech-\nniques to design program-players to the game of Hex (size 11 and 13) surpassing the\nlevel of Mohex 3HNN with reinforcement learning from self-play without knowledge.\nKeywords: Machine Learning, Tree Search, Games, Game Tree Learning, Minimax,\nHeuristic, Action Distribution, Unbound Minimax, Hex.\nURL: cohen-solal@cril.fr (Quentin Cohen-Solal)\nPreprint submitted to Journal of Artiﬁcial Intelligence\nOctober 13, 2021\narXiv:2008.01188v3  [cs.AI]  12 Oct 2021\n1. Introduction\nOne of the most difﬁcult tasks in artiﬁcial intelligence is the sequential decision\nmaking problem [1], whose applications include robotics and games. As for games, the\nsuccesses are numerous. Machine surpasses man for several games, such as backgam-\nmon, checkers, chess, and go [2]. A major class of games is the set of two-player\ngames in which players play in turn, without any chance or hidden information. This\nclass is sometimes called two-player perfect information1 games Mycielski [3] or also\ntwo-player combinatorial games. There are still many challenges for these games. For\nexample, for the game of Hex, computers have only been able to beat strong humans\nsince 2020 [4]. For general game playing [5] (even restricted to games with perfect\ninformation): man is always superior to machine on an unknown complex game (when\nman and machine have a relatively short learning time to master the rules of the game).\nIn this article, we focus on two-player zero-sum games with perfect information, al-\nthough most of the contributions in this article should be applicable or easily adaptable\nto a more general framework2.\nThe ﬁrst approaches used to design a game-playing program are based on a game\ntree search algorithm, such as minimax, combined with a handcrafted game state eval-\nuation function based on expert knowledge. A notable use of this technique is the\nDeep Blue chess program [8]. However, the success of Deep Blue is largely due to the\nraw power of the computer, which could analyze two hundred million game states per\n1With some deﬁnitions, perfect information games include games with chance. It depends on whether\none includes information about the future in perfect knowledge.\n2All the proposed techniques are directly applicable to the framework of perfect information single-agent\nsequential decision-making problems and in particular to solitaire games with perfect information. Tree\nlearning, ordinal distribution, and reinforcement heuristics are applicable in the generalized cases (in case\nof m agents, reinforcement heuristics can be valued in real m-space). Minimax variants are applicable in\nmulti-player perfect-information games as variants of the paranoid algorithm [6]. Descent can be applied\nto learn value functions for stochastic perfect-information games by applying it to a large set of determinist\ngames corresponding to the stochastic game where the chance was ﬁxed and known before the start of the\ngame. To use the corresponding learned heuristic, either a minimax at depth 1 or an algorithm such as\nexpectiminimax [7] must be used. However, for stochastic games and multiplayer games generalizations of\nthe minimax variants should give better results in general.\n2\nsecond. In addition, this approach is limited by having to design an evaluation func-\ntion manually (at least partially). This design is a very complex task, which must, in\naddition, be carried out for each different game. Several works have thus focused on\nthe automatic learning of evaluation functions [9]. One of the ﬁrst successes of learn-\ning evaluation functions is on the Backgammon game [10]. However, for many games,\nsuch as Hex or Go, minimax-based approaches, with or without machine learning, have\nfailed to overcome human. Two causes have been identiﬁed [11]. Firstly, the very large\nnumber of possible actions at each game state prevents an exhaustive search at a sig-\nniﬁcant depth (the game can only be anticipated a few turns in advance). Secondly, for\nthese games, no sufﬁciently powerful evaluation function could be identiﬁed. An alter-\nnative approach to solve these two problems has been proposed, giving notably good\nresults to Hex and Go, called Monte Carlo Tree Search and denoted MCTS [12, 13].\nThis algorithm explores the game tree non-uniformly, which is a solution to the prob-\nlem of the very large number of actions. In addition, it evaluates the game states from\nvictory statistics of a large number of random end-game simulations. It does not need\nan evaluation function. This was not enough, however, to go beyond the level of human\nplayers. Several variants of Monte Carlo tree search were then proposed, using in par-\nticular knowledge to guide the exploration of the game tree and/or random end-game\nsimulations [13]. Recent improvements in Monte Carlo tree research have focused\non the automatic learning of MCTS knowledge and their uses. This knowledge was\nﬁrst generated by supervised learning [14, 15, 16, 17, 18] then by supervised learning\nfollowed by reinforcement learning [19], and ﬁnally by only reinforcement learning\n[2, 20, 21]. This allowed programs to reach and surpass the level of world champion at\nthe game of Go with the latest versions of the program AlphaGo [19, 2]. In particular,\nAlphaGo zero [2], which only uses reinforcement learning, did not need any knowl-\nedge to reach its level of play. This last success, however, required 29 million games of\nself-play (with 1,600 state evaluations per move). This approach has also been applied\nto chess [22]. The resulting program broke the best chess program (which is based\non minimax). AlphaZero was subsequently reimplemented in Polygames [4], an open-\nsource and multi-game program, which was notably applied to Hex and Havannah. In\nparticular, at the 2020 Computer Olympiad, the gold medals for these games were won\n3\nby Polygames.\nIt is therefore questionable whether minimax is totally out of date or whether the\nspectacular successes of recent programs are more based on reinforcement learning\nthan Monte Carlo tree search. In particular, it is interesting to ask whether reinforce-\nment learning would enhance minimax enough to make it competitive with Monte\nCarlo tree search on games where it dominates minimax so far, such as Go or Hex.\nIn this article3, we therefore focus on reinforcement learning within the minimax\nframework. We propose and asses new techniques for reinforcement learning of eval-\nuation functions. Then, we apply them to design new program-players to the game\nof Hex (without using other knowledge than the rules of the game). We compare this\nprogram-player to Mohex 3HNN [16], the best Hex program, champion at Hex (size\n11 and 13) of the 2018 Computer Olympiad [24].\nIn the next section, we brieﬂy present the game algorithms and in particular min-\nimax with unbounded depth on which we base several of our experiments. We also\npresent reinforcement learning in games, the game of Hex, the state of the art of game\nprograms on this game, as well as other games on which experiments are performed.\nIn the following sections, we propose different techniques aimed at improving learn-\ning performances and we expose the experiments carried out using these techniques.\nIn particular, in Section 3, we extends the tree bootstrapping (tree learning) technique\nto the context of reinforcement learning without knowledge based on non-linear func-\ntions. In Section 4, we present a new search algorithm, a variant of unbounded mini-\nmax called descent, intended to be used during the learning process. In Section 5, we\nintroduce reinforcement heuristics. Their usage is a simple way to use general or dedi-\ncated knowledge in reinforcement learning processes. We study several reinforcement\nheuristics in the context of different games. In Section 6, we propose another variant of\nunbounded minimax, which plays the safest action instead of playing the best action.\nThis modiﬁed search is intended to be used after the learning process. In Section 7,\nwe introduce a new action selection distribution and we apply it with all the previous\ntechniques to design program-players to the game of Hex (size 11 and 13) and com-\n3Note that this paper is an extended, improved, and english version of [23].\n4\npare them to Mohex 3HNN. Finally, in the last section, we conclude and expose the\ndifferent research perspectives.\n2. Background and Related Work\nIn this section, we brieﬂy present game tree search algorithms, reinforcement learn-\ning in the context of games and their applications to Hex and Chess (for more details\nabout game algorithms, see [25]).\nGames can be represented by their game tree (a node corresponds to a game state\nand the children of a node are the states that can be reached by an action). From this\nrepresentation, we can determine the action to play using a game tree search algorithm.\nIn order to win, each player tries to maximize his score (i.e. the value of the game\nstate for this player at the end of the game). As we place ourselves in the context of\ntwo-player zero-sum games, to maximize the score of a player is to minimize the score\nof his opponent (the score of a player is the negation of the score of his opponent).\n2.1. Game Tree Search Algorithms\nThe central algorithm is minimax which recursively determines the value of a node\nfrom the value of its children and the functions min and max, up to a limit recursion\ndepth. With this algorithm, the game tree is uniformly explored. A better implemen-\ntation of minimax uses alpha-beta pruning [26, 25] which makes it possible not to\nexplore the sections of the game tree which are less interesting given the values of the\nnodes already met and the properties of min and max. Many variants and improve-\nments of minimax have been proposed [27]. For instance, iterative deepening [28, 29]\nallows one to use minimax with a time limit. It sequentially performs increasing depth\nalpha-beta searches as long as there is time. It is generally combined with the move\nordering technique [30], which consists of extending the best move from the previous\nsearch ﬁrst, which accelerates the new search. Some variants perform a search with\nunbounded depth (that is, the depth of their search is not ﬁxed) [31, 32, 33]. Unlike\nminimax with or without alpha-beta pruning, the exploration of these algorithms is\nnon-uniform. One of these algorithms is the best-ﬁrst minimax search [34]. To avoid\n5\nany confusion with some best-ﬁrst approaches at ﬁxed depth, we call this algorithm\nUnbound Best-First Minimax, or more succinctly UBFM. UBFM iteratively extends\nthe game tree by adding the children of one of the leaves of the game tree having the\nsame value as that of the root (minimax value). These leaves are the states obtained af-\nter having played one of the best sequences of possible actions given the current partial\nknowledge of the game tree. Thus, this algorithm iteratively extends the a priori best\nsequences of actions. These best sequences usually change at each extension. Thus,\nthe game tree is non-uniformly explored by focusing on the a priori most interesting\nactions without exploring just one sequence of actions. In this article, we use the any-\ntime version of UBFM [34], i.e. we leave a ﬁxed search time for UBFM to decide the\naction to play. We also use transposition tables [35, 27] with UBFM, which makes it\npossible not to explicitly build the game tree and to merge the nodes corresponding to\nthe same state. Algorithm 1 is the used implementation of UBFM in this paper4.\n2.2. Learning of Evaluation Functions\nReinforcement learning of evaluation functions can be done by different techniques\n[9, 2, 20, 36]. The general idea of reinforcement learning of state evaluation functions\nis to use a game tree search algorithm and an adaptive evaluation function fθ, of pa-\nrameter θ, (for example a neural network) to play a sequence of games (for example\nagainst oneself, which is the case in this article). Each game will generate pairs (s, v)\nwhere s is a state and v the value of s calculated by the chosen search algorithm using\nthe evaluation function fθ. The states generated during one game can be the states of\nthe sequence of states of the game [10, 37]. For example, in the case of root bootstrap-\nping (technique that we call root learning in this article), the set of pairs used during\nthe learning phase is D = {(s, v) | s ∈R} with R the set of states of the sequence of\nthe game. In the case of the tree bootstrapping (tree learning) technique [37], the gen-\n4This implementation is a slight variant of Korf and Chickering algorithm. Their algorithm is very slightly\nmore efﬁcient but it offers less freedom: our algorithm behaves slightly differently depending on how we\ndecide between two states having the same value. The exploration of states is identical between their algo-\nrithm and ours when with our algorithm equality is decided in deepest ﬁrst. Our variant has been discovered\nindependently of Korf and Chickering work.\n6\nSymbols\nDeﬁnition\nactions (s)\naction set of the state s for the current player\nﬁrst player (s)\ntrue if the current player of the state s is the ﬁrst player\nterminal (s)\ntrue if s is an end-game state\na(s)\nstate obtained after playing the action a in the state s\ntime ()\ncurrent time in seconds\nS\nkeys of the transposition table T\nT\ntransposition table (contains functions as v and/or v′ ; depends on the used search algorithm)\nτ\nsearch time per action\nt\ntime elapsed since the start of the reinforcement learning process\ntmax\nchosen total duration of the learning process\nn(s, a)\nnumber of times the action a is selected in state s (initially, n(s, a) = 0 for all s and a\nv(s)\nvalue of state s in the game tree according to the last tree search\nv′(s, a)\nvalue obtained after playing action a in state s\nc(s)\ncompletion value of state s (0 by default)\nr(s)\nresolution value of state s (0 by default)\nf(s)\nthe used evaluation function (ﬁrst player point of view)\nfθ(s)\nadaptive evaluation function (of non-terminal game tree leaves ; ﬁrst player point of view)\nft(s)\nevaluation of terminal states, e.g. gain game (ﬁrst player point of view)\nGain function bt(s)\n0 if s is a draw, 1 if s is winning for the ﬁrst player, −1 if s is losing for the ﬁrst player\nsearch(s, S, T, fθ, ft)\na seach algorithm (it extends the game tree from s, by adding new states in S\nand labeling its states, in particular, by a value v(s), stored in T,\nusing fθ as evaluation of the non-terminal leaves and ft as evaluation of terminal states\naction selection(s, S, T)\ndecides the action to play in the state s depending on the partial game tree, i.e. on S and T\nprocessing(D)\nvarious optional data processing: data augmentation (symmetry), experience replay, ...\nupdate(fθ, D)\nupdates the parameter θ of fθ in order for fθ(s) is closer to v for each (s, v) ∈D\nTable 1: Index of symbols\n7\nAlgorithm 1 UBFM (Unbounded Best-First Minimax) algorithm : it computes the\nbest action to play in the generated non-uniform partial game tree (see Table 1 for the\ndeﬁnitions of symbols ; at any time T = {v′(s, a) | s ∈S, a ∈actions(s)}).\nFunction UBFM iteration(s, S, T)\nif terminal(s) then\nreturn f(s)\nelse\nif s /∈S then\nS ←S ∪{s}\nforeach a ∈actions(s) do\nv′(s, a) ←f (a(s))\nelse\nab ←best action(s)\nv′(s, ab) ←UBFM iteration(ab(s), S, T)\nab ←best action(s)\nreturn v′(s, ab)\nFunction best action(s, S, T)\nif first player(s) then\nreturn\narg max\na∈actions(s)\nv′ (s, a)\nelse\nreturn\narg min\na∈actions(s)\nv′ (s, a)\nFunction UBFM(s, τ)\nt = time() while time()−t < τ do UBFM iteration(s, S, T)\nreturn best action(s, S, T)\n8\nerated states are the states of the game tree built to decide which actions to play (which\nincludes the states of the sequence of states of the game): D = {(s, v) | s ∈T} with T\nthe set of states of the partial game tree of the game. Thus, contrary to root bootstrap-\nping, tree bootstrapping does not discard most of the information used to decide the\nactions to play. The values of the generated states can be their minimax values in the\npartial game tree built to decide which actions to play [37, 10]. Work on tree bootstrap-\nping has been limited to reinforcement learning of linear functions of state features.\nIt has not been formulated or studied in the context of reinforcement learning without\nknowledge and based on non-linear functions. Note that, in the case of AlphaGo Zero,\nthe value of each generated state, the states of the sequence of the game, is the value of\nthe terminal state of the game [2]. We call this technique terminal learning.\nGenerally between two consecutive games (between match phases), a learning\nphase occurs, using the pairs of the last game. Each learning phase consists in modi-\nfying fθ so that for all pairs (s, v) ∈D, fθ(s) sufﬁciently approaches v to constitute\na good approximation. Note that, in the context of a variant, learning phases can use\nthe pairs of several games. This technique is called experience replay [38]. Note that,\nadaptive evaluation functions fθ only serve to evaluate non-terminal states since we\nknow the true value of terminal states.\nRemark 1. There is another reinforcement learning technique for games: the Temporal\nDifferences differences TD(λ) [10]. We can see the temporal differences as a kind of\ninterpolation between root learning of a depth 1 minimax and the terminal learning.\nA variant has been proposed [39], called TDLeaf(λ), where the temporal difference\ntechnique is applied not to learn the value of the root but the value of the leaf of the\nprincipal variation of a minimax search (at any depth). A comparison between these\ntechniques was made [37]. Finally, [40] describe a method for automatically tuning\nsearch-extension parameters, to decide which branches of the game tree must be ex-\nplored during the search.\n2.3. Action Selection Distribution\nOne of the problems related to reinforcement learning is the exploration-exploitation\ndilemma [9]. It consists of choosing between exploring new states to learn new knowl-\n9\nedge and exploiting the acquired knowledge. Many techniques have been proposed to\ndeal with this dilemma [41]. However, most of these techniques do not scale because\ntheir application requires memorizing all the encountered states. For this reason, in\nthe context of games with large numbers of states, some approaches use probabilistic\nexploration [36, 2, 9, 42]. With this approach, to exploit is to play the best action and\nto explore is to play uniformly at random. More precisely, a parametric probability\ndistribution is used to associate with each action its probability of being played. The\nparameter associated with the distribution corresponds to the exploration rate (between\n0 and 1), which we denote ϵ (the exploitation rate is therefore 1 −ϵ, which we denote\nϵ′). The rate is often experimentally ﬁxed. Simulated annealing [43] can, however, be\napplied to avoid choosing a value for this parameter. In this case, at the beginning of re-\ninforcement learning, the parameter is 1 (we are just exploring). It gradually decreases\nuntil reaching 0 at the end of learning. The simplest action selection distribution is ϵ-\ngreedy [36] (of parameter ϵ). With this distribution, the action is chosen uniformly with\nprobability ϵ and the best action is chosen with probability 1−ϵ (see also Algorithm 2).\nAlgorithm 2 ϵ-greedy algorithm with simulated annealing used in the experiments of\nthis article (see Table 1 for the deﬁnitions of symbols).\nFunction ϵ greedy(s, v′)\nif probability\nt\ntmax then\nif first player(s) then\nreturn arg maxa∈actions(s) v′ (s, a)\nelse\nreturn arg mina∈actions(s) v′ (s, a)\nelse\nreturn a ∈actions(s) uniformly chosen.\nThe ϵ-greedy distribution has the disadvantage of not differentiating the actions\n(except the best action) in terms of probabilities. Another distribution is often used,\ncorrecting this disadvantage. This is the softmax distribution [42, 9]. It is deﬁned by\nP (ai) =\ne\nv′(s,ai)/τ\nPn\nj=1 ev′(s,aj )/τ with n the number of children of the current state s, P (ai)\nthe probability of playing the action ai , v′(s, ai) the value of the state obtained after\n10\nFigure 1: A Hex end game of size 11 (white wins)\nplaying ai in s, i ∈{0, . . . , n −1}, and τ ∈]0, +∞[ a parameter called temperature\n(τ ≃0 : exploitation, τ ≃+∞: exploration).\n2.4. Game of Hex\nThe game of Hex [44] is a two-player combinatorial strategy game. It is played on\nan empty n×n hexagonal board. We say that a n×n board is of size n. The board can\nbe of any size, although the classic sizes are 11, 13 and 19. In turn, each player places\na stone of his color on an empty cell (each stone is identical). The goal of the game is\nto be the ﬁrst to connect the two opposite sides of the board corresponding to its color.\nFigure 1 illustrates an end game. Although these rules are simplistic, Hex tactics and\nstrategies are complex. The number of states and the number of actions per state are\nvery large, similar to the game of Go. From the board size 11, the number of states is,\nfor example, higher than that of chess (Table 6 of [45]). For any board size, the ﬁrst\nplayer has a winning strategy [46] which is unknown, except for board sizes smaller\nthan or equal to 10 [47] (the game is weaky solved up to the size 10). In fact, resolving\na particular state is PSPACE-complete [48, 49]. There is a variant of Hex using a swap\nrule. With this variant, the second player can play in ﬁrst action a special action, called\nswap, which swaps the color of the two players (i.e. they swap their pieces and their\nsides). This rule reduces the imbalance between the two players (without the swap rule,\nthe ﬁrst player has a very strong advantage). It is generally used in competitions.\n11\n2.5. Hex Programs\nMany Hex player programs have been developed. For example, Mohex 1.0 [50] is\na program based on Monte Carlo tree search. It also uses many techniques dedicated to\nHex, based on speciﬁc theoretical results. In particular, it is able to quickly determine\na winning strategy for some states (without expanding the search tree) and to prune at\neach state many actions that it knows to be inferior. It also uses ad hoc knowledge to\nbias simulations of Monte Carlo tree search.\nMohex 2.0 [50] is an improvement of Mohex 1.0 that uses learned knowledge\nthrough supervised learning (namely correlations between victory and board patterns)\nto guide both tree exploration and simulations.\nOther work then focused on predicting best actions, through supervised learning of\na database of games, using a neural network [51, 52, 53]. The neural network is used\nto learn a policy, i.e. a prior probability distribution on the actions to play. These prior\nprobabilities are used to guide the exploration of Monte Carlo tree search. First, there is\nMohex-CNN [15] which is an improvement of Mohex 2.0 using a convolutional neural\nnetwork [54]. A new version of Mohex was then proposed: Mohex-3HNN [16]. Unlike\nMohex-CNN, it is based on a residual neural network [55]. It calculates, in addition to\nthe policy, a value for states and actions. The value of states replaces the evaluation\nof states based on simulations of Monte Carlo tree search. Adding a value to actions\nallows Mohex-HNN to reduce the number of calls of the neural network, improving\nperformance. Mohex-3HNN is the best Hex program. It wons Hex size 11 and 13\ntournaments at 2018 Computer Olympiad [24].\nPrograms which learn the evaluation function by reinforcement have also been de-\nsigned. These programs are NeuroHex [36], EZO-CNN [56], DeepEzo [57] and ExIt\n[20]. They learn from self-play. Unlike the other three programs, NeuroHex performs\nsupervised learning (of a common Hex heuristic) followed by reinforcement learning.\nNeuroHex also starts its games with a state from a database of games. EZO-CNN and\nDeepEzo use knowledge to learn winning strategies in some states. DeepEzo also uses\nknowledge during confrontations. ExIt learns a policy in addition to the value of states\nand it is based on MCTS. It is the only program to have learned to play Hex without\nusing knowledge. This result is, however, limited to the board size 9. A comparison of\n12\nPrograms\nSize\nSearch\nLearning\nNetwork\nUse\nMohex-CNN\n13\nMCTS\nsupervised\nconvolutional\npolicy\nMohex-3HNN\n13\nMCTS\nsupervised\nresidual\npolicy, state, action\nNeuroHex\n13\nnone\nsupervised, reinforcement\nconvolutional\nstate\nEZO-CNN\n7, 9, 11\nMinimax\nreinforcement\nconvolutional\nstate\nDeepEZO\n13\nMinimax\nreinforcement\nconvolutional\npolicy, state\nExIt\n9\nMCTS\nreinforcement\nconvolutional\npolicy, state\nTable 2: Comparison of the main features of the latest Hex programs. These characteristics are respectively\nthe board sizes on which learning is based, the used tree search algorithm, the type of learning, the type of\nneural network and its use (to approximate the values of states, actions and/or policy.\nthe main characteristics of these different programs is presented in Table 2.\n2.6. Games of Paper Experiments\nWe now brieﬂy present the other games on which experiments are performed in\nthis article, namely: Surakarta, Othello, Outer Open Gomoku, Clobber, Breakthrough,\nAmazons, Lines of Action, Santorini. They are all board games. All of these games\n(except Santorini so far) are present and recurring at the Computer Olympiads, the\nworldwide multi-games event in which computer programs compete against each other.\nMoreover, all these games (and their rules) are included (and available for free) in\nLudii [58], a general game system.\n2.6.1. Surakarta\nSurakarta is a move and capture game (like the checkers game), the object of the\ngame being to take all the opposing pieces. In his turn, a player can either move a\npiece to an empty square at a distance of 1 or move a piece to a square occupied by\nan opponent’s piece under certain conditions and according to a mechanism speciﬁc\nto Surakarta (based on a movement circuit dedicated only to capture), allowing \"long\ndistance\" capture.\n13\n2.6.2. Othello\nOthello (also called Reversi) is a linear territory and encirclement game whose goal\nis to have more pieces than your opponent. In his turn, a player places a piece of his\ncolor on the board (only if he can make an encirclement, otherwise he pass his turn).\nThere is an encirclement if an opponent’s line of pieces has at its two ends the piece\nthat has just been placed and another piece of the player performing the encirclement.\nAs a result of this encirclement, the encircled pieces are replaced by pieces from that\nplayer.\n2.6.3. Outer Open Gomoku\nOuter Open Gomoku is an alignment game. The object of the game is to line up\nat least 5 pieces of its color. On his turn, a player places a piece of his color. In the\nﬁrst turn, the ﬁrst player can only place a piece at a distance of 2 from the sides of the\nboard.\n2.6.4. Clobber\nClobber is a move and capture game. The goal is to be the last player to have played.\nA player can play if he can orthogonally move one of his pieces onto a neighboring\nsquare on which there is an opponent’s piece. This movement is always a capture (the\nopponent’s piece is removed from the board).\n2.6.5. Breakthrough\nBreakthrough is a move and capture game. The object of the game is to be the ﬁrst\nto make one of his pieces reach the other side of the board. A piece can only move\nby moving forward one square (straight or diagonal). A capture can only be done\ndiagonally.\n2.6.6. Amazons\nAmazons is a move and blocking game. In turn, a player moves one of his pieces\nin a straight line in any direction (like the queen of the game of Chess). Then he places\na neutral piece (blocking movements like the players’ pieces) in any direction starting\n14\nfrom the new position of the piece just moved (in the manner of the queen of the game\nof Chess). The goal of the game is to be the last to play.\n2.6.7. Lines of Action\nLines of Action is a game of movement and regrouping. On his turn, a player can\nmove one of his pieces in one direction as many squares as there are pieces in that\ndirection. A piece cannot move if there is an opponent’s piece in its path, unless it is\nthe square to arrive (in which case a capture is made). The goal is to have all of its\npieces connected (at the same time).\n2.6.8. Santorini\nSantorini is a three-dimensional building and moving game. The goal of the game\nis to reach the 3rd ﬂoor of a building. In his turn, a player moves one of his pieces\nby one square then places the ﬁrst ﬂoor on an adjacent empty square or increases a\npre-existing construction by one ﬂoor (on which no player’s piece is located). A piece\ncannot move to a square having strictly more than one ﬂoor more than the square where\nit is located (a piece only go up one ﬂoor at a time and can descend as many ﬂoors as\nwanted). A move cannot be made to a square with 4 ﬂoors. A construction cannot be\ndone on a square of 4 ﬂoors. A player who cannot play loses. Advanced mode (i.e. the\nuse of power cards) is not used in the experiments in this article.\n3. Data Use in Game Learning\nIn this section, we adapt and study tree learning (see Section 2.2) in the context\nof reinforcement learning and the use of non-linear adaptive evaluation functions. For\nthis, we compare it to root learning and terminal learning in this context. We start\nby adapting tree learning, root learning, and terminal learning. Next, we describe the\nexperiment protocol common to several sections of this article. Finally, we expose the\ncomparison of tree learning with root learning and terminal learning.\n3.1. Tree Learning\nAs we saw in Section 2.2, tree learning consists in learning the value of the states\nof the partial game tree obtained at the end of the game. Root learning consists in\n15\nlearning the values of the states of the sequence of states of the game (the value of\neach state is its value in the search tree). Terminal learning consists in learning the\nvalues of the sequence of states of the game but the value of each state is the value\nof the terminal state of the game (i.e. the gain of the game). Data to learn after each\ngame, can be modiﬁed by some optional data processing methods, such as experience\nreplay (see Section 2.2). The learning phase uses a particular update method so that the\nadaptive evaluation function ﬁt the chosen data. The adaptation of tree learning, root\nlearning, and terminal learning are given respectively in Algorithm 3, Algorithm 4,\nand Algorithm 5. In this article, we use experience replay as data processing method\n(see Algorithm 6 ; its parameter are the memory size µ and the sampling rate σ). In\naddition, we use a stochastic gradient descent as update method (see Algorithm 7 ;\nits parameter is B the batch size). Formally, in Algorithm 3, Algorithm 4, and Algo-\nrithm 5, we have: processing(D) is experience replay(D, µ, σ) and update(fθ, D) is\nstochastic gradient descent(fθ, D, B). Finally, we use ϵ-greedy as default action se-\nlection method (i.e. action selection(s, S, T) is ϵ-greedy(s, T.v′) (T stores the children\nvalue function v′ ; see Algorithm 2)).\n3.2. Common Experiment Protocol\nThe experiments of several sections share the same protocol. It is presented in this\nsection. The protocol is used to compare different variants of reinforcement learning\nalgorithms. A variant corresponds to a certain combination of elementary algorithms.\nMore speciﬁcally, a combination consists of the association of a search algorithm (it-\nerative deepening alpha-beta (with move ordering), MCTS (UCT with c = 0.4 as\nexploration constant), UBFM, ...), of an action selection method (ϵ-greedy distribution\n(used by default), softmax distribution, ...), a terminal evaluation function ft (the clas-\nsic game gain (used by default), ...), and a procedure for selecting the data to be learned\n(root learning, tree learning, or terminal learning). The protocol consists in carrying out\na reinforcement learning of 48 hours for each variant. At several stages of the learning\nprocess, matches are performed using the adaptive evaluation functions obtained by the\ndifferent variants. Each variant is then characterized by a winning percentage at each\nstage of the reinforcement learning process. More formally, we denote by f c\nθh the eval-\n16\nuation generated by the combination c at the hour h. Each combination is evaluated\nevery hour by a winning percentage. The winning percentage of a combination c at a\nhour h ≤48 (i.e. of f c\nθh) is computed from matches against each combination c′ at\nﬁnal time h = 48, i.e. against each f c′\nθ48 (there is one match in ﬁrst player and another\nin second player per pair of combination). The matches are made by using alpha-beta\nat depth 1.\nThis protocol is repeated several times for each experiment in order to reduce the\nstatistical noise in the winning percentages obtained for each variant (the obtained per-\ncentage is the average of the percentages of repetitions). The winning percentages are\nthen represented in a graph showing the evolution of the winning percentages during\ntraining.\nIn addition to the curve, the different variants are also compared in relation to their\nﬁnal winning percentage, i.e. at the end of the learning process. Unlike the experiment\nof the evolution of winning percentages, in the comparison of the different variants\nAlgorithm 3 Tree learning (tree bootstrapping) algorithm (see Table 1 for the deﬁ-\nnitions of symbols). In this context, S is the set of states which are non-leaves or\nterminal.\nFunction tree learning(tmax)\nt0 ←time()\nwhile time()−t0 < tmax do\ns ←initial game state()\nS ←∅\nT ←{}\nwhile ¬terminal(s) do\nS, T ←search(s, S, T, fθ, ft)\na ←action selection(s, S, T)\ns ←a(s)\nD ←{(s, v(s)) | s ∈S}\nD ←processing(D)\nupdate(fθ, D)\n17\nat the ﬁnal stage, each evaluation f c\nθ48 confronts each other evaluation f c′\nθ48 of all the\nrepetitions. In other words, this experiment consists of performing an all-play-all tour-\nnament with all the evaluation functions generated during the different repetitions. The\npresented winning percentage of a combination is still the average over the repetitions.\nThe matches are also made by using alpha-beta at depth 1. These percentages are\nshown in tables.\nRemark 2. The used version of MCTS does not performed random simulations for\nevaluating the leaves. Instead, leaves are evaluated by a neural network. No policies\nare used (unless it is explicitly speciﬁed).\nRemark 3. All the experiments involving MCTS were also performed with c =\n√\n2 as\nexploration constant. The results are similar.\nAlgorithm 4 Root learning (root bootstrapping) algorithm (see Table 1 for the deﬁni-\ntions of symbols).\nFunction root learning(tmax)\nt0 ←time()\nwhile time()−t0 < tmax do\ns ←initial game state()\nS ←∅\nT ←{}\nD ←∅\nwhile ¬terminal(s) do\nS, T ←search(s, S, T, fθ, ft)\na ←action selection(s, S, T)\nD ←D ∪{(s, v(s))}\ns ←a(s)\nD ←D ∪{(s, v(s))}\nD ←processing(D)\nupdate(fθ, D)\n18\n3.2.1. Technical Details\nThe used parameters are: search time per action τ = 2s, batch size B = 128,\nmemory size µ = 106, sampling rate σ = 4% (see Section 3.1). Moreover, the used\nAlgorithm 5 Terminal learning algorithm (see Table 1 for the deﬁnitions of symbols).\nFunction terminal learning(tmax)\nt0 ←time()\nwhile time()−t0 < tmax do\ns ←initial game state()\nS ←∅\nT ←{}\nG ←{s}\nwhile ¬terminal(s) do\nS, T ←search(s, S, T, fθ, ft)\na ←action selection(s, S, T)\ns ←a(s)\nG ←G ∪{s}\nD ←{(s′, ft(s)) | s′ ∈G}\nD ←processing(D)\nupdate(fθ, D)\nAlgorithm 6 Experience replay (replay buffer) algorithm used in the experiments of\nthis article. µ is the memory size and σ is the sampling rate. M is the memory buffer\n(global variable initialized by an empty queue). If the number of data is less than σ · µ,\nthen it returns all data (no sampling). Otherwise, it returns σ · µ random elements.\nFunction experience replay(D, µ, σ)\nadd the elements of D in M\nif |M| > µ then\nremove the oldest items of M to have |M| = µ\nif |M| ≤σ · µ then\nreturn M\nreturn a list of random items of M whose size is σ · µ\n19\nadaptive evaluation function for each combination is a convolutional neural network\n[54] having three convolution layers5 followed by a fully connected hidden layer. For\neach convolutional layer, the kernel size is 3×3 and the ﬁlter number is 64. The number\nof neurons in the fully connected layer is 100. The margin of each layer is zero. After\neach layer except the last one, the ReLU activation function [61] is used. The output\nlayer contains a neuron. When the classical terminal evaluation is used, tanh is the\noutput activation function. Otherwise, there is no activation function for the output.\nRemark 4. In this paper, ﬁlter numbers and numbers of neurons are chosen in order to\nthere are about the same number of variables in the convolution layers and in the dense\nlayers.\n3.3. Comparison of Learning Data Selection Algorithms\nWe now compare tree learning, root learning and terminal learning, using the pro-\ntocol of Section 3.2. Each combination uses either tree learning or root learning or\nterminal learning. Moreover, each combination uses either iterative deepening alpha-\nbeta (denoted by ID) or MCTS. Furthermore, each combination uses ϵ-greedy as action\nselection method (see Section 3.1) and the classical terminal evaluation (1 if the ﬁrst\nplayer wins, −1 if the ﬁrst player loses, 0 in case of a draw). There are a total of 6\n5There is an exception: for the game Surkarta, there is only two convolution layers.\nAlgorithm 7 Stochastic gradient descent algorithm used in the experiments of this ar-\nticle. It is based on Adam optimization (1 epoch per update) [59] and L2 regularization\n(with λ = 0.001 as parameter) [60] and implemented with tensorﬂow. B is the batch\nsize (see Table 1 for the deﬁnitions of the other symbols)\nFunction stochastic gradient descent(fθ, D, B)\nSplit D in m disjoint sets, denoted {Di}m\ni=1, such that D = Sm\ni=1 Di and |Di| = B\nfor each i ∈{1, . . . , m −1}\nforeach i ∈{1, . . . , m} do\nminimize P\n(s,v)∈Di (fθ(s) −v)2 by using Adam and L2 regularization\n20\ncombinations. The experiment was repeated 32 times. The winning percentage of a\ncombination for each game and for each evaluation step (i.e. each hour) is therefore\ncalculated from 192 matches. The winning percentage curves are shown in Figure 2.\nThe ﬁnal winning percentages are shown in Table 3. Each percentage of the table has\nrequired 6144 matches. ID is ﬁrst on all games except in Outer Open Gomoku where\nit is second (MCTS root learning is ﬁrst) and in Surakarta (MCTS with tree learning is\nﬁrst). MCTS with root learning is better than MCTS with tree learning except in Break-\nthrough and Surakarta. At Hex and Amazons, MCTS with root learning gives better\nresults throughout the learning process but ends up being caught up by ID with terminal\nlearning. Terminal learning performs worse everywhere, except in a few cases where\nit is very slightly better. On average, ID with tree learning is better (71% win), then\nMCTS with root learning is second (9% lower win percentage), followed by MCTS\nwith tree learning (18% lower to ID).\nIn conclusion, tree learning with ID performs much better than other combinations,\nalthough the results are very tight at Amazons, Hex, and Outer Gomoku with MCTS\nwith root learning.\n4. Tree Search Algorithms for Game Learning\nIn this section, we introduce a new tree search algorithm, that we call descent min-\nimax or more succinctly descent, dedicated to be used during the learning process. It\nrequires tree learning (combining it with root learning or terminal learning is of no\ninterest6). After presenting descent, we compare it to MCTS with root learning and\nwith tree learning, to iterative deepening alpha-beta with root learning and with tree\nlearning and to UBFM with tree learning.\n21\nFigure 2: Evolutions of the winning percentages of the combinations of the experiment of Section 3.3, i.e.\nMCTS (dotted line) or iterative deepening alpha-beta (continuous line) with tree learning (blue line) or root\nlearning (red line) or terminal learning (green line). The display uses a simple moving average of 6 data.\n22\ntree learning\nroot learning\nterminal learning\nMCTS\nID\nMCTS\nID\nMCTS\nID\nOthello\n48.8%\n55.4%\n51.0%\n31.9%\n52.9%\n43.7%\nHex\n45.1%\n79.8%\n79.8%\n44.1%\n29.8%\n27.8%\nClobber\n41.5%\n62.5%\n50.0%\n45.5%\n45.7%\n49.8%\nOuter Open Gomoku\n40.3%\n80.0%\n87.3%\n48.8%\n21.6%\n23.1%\nAmazons\n46.2%\n58.8%\n56.1%\n44.5%\n39.4%\n46.0%\nBreakthrough\n78.1%\n79.2%\n45.5%\n50.6%\n22.3%\n14.3%\nSantorini\n50.2%\n73.8%\n60.5%\n51.3%\n29.4%\n36.5%\nSurakarta\n69.4%\n65.2%\n56.2%\n20.8%\n28.9%\n23.6%\nLines of Action\n58.8%\n81.7%\n67.7%\n9.6%\n6.9%\n2.7%\nmean\n53.1%\n70.7%\n61.6%\n38.5%\n30.8%\n29.7%\nTable 3: Final winning percentages of the combinations of the experiment of Section 3.3 (ID: iterative deep-\nening alpha-beta). Reminder: the percentage is the average over the repetitions, of the winning percentage of\na combination against each other combination, in ﬁrst and second player (see 3.2 ; 95% conﬁdence intervals:\nmax ±0.85%).\n23\nAlgorithm 8 Descent tree search algorithm (see Table 1 for the deﬁnitions of symbols\n; note: S is the set of states which are non-leaves or terminal and T = (v, v′)).\nFunction descent iteration(s, S, T, fθ, ft)\nif terminal(s) then\nS ←S ∪{s}\nv(s) ←ft(s)\nelse\nif s /∈S then\nS ←S ∪{s}\nforeach a ∈actions(s) do\nif terminal(a(s)) then\nS ←S ∪{a(s)}\nv′(s, a) ←ft (a(s))\nv(a(s)) ←v′(s, a)\nelse\nv′(s, a) ←fθ (a(s))\nab ←best action(s)\nv′(s, ab) ←descent iteration(ab(s))\nab ←best action(s)\nv(s) ←v′(s, ab)\nreturn v(s)\nFunction best action(s)\nif first player(s) then\nreturn\narg max\na∈actions(s)\nv′ (s, a)\nelse\nreturn\narg min\na∈actions(s)\nv′ (s, a)\nFunction descent(s, S, T, fθ, ft, τ)\nt = time()\nwhile time()−t < τ do descent iteration(s, S, T, fθ, ft)\nreturn S, T\n24\n4.1. Descent: Generate Better Data\nThus, we present descent. It is a modiﬁcation of UBFM which builds a different,\ndeeper, game tree, to be combined with tree learning. The idea of descent is to combine\nUBFM with deterministic end-game simulations providing interesting values from the\npoint of view of learning. The algorithm descent (Algorithm 8) recursively selects the\nbest child of the current node, which becomes the new current node. It adds the children\nof the current node if they are not in the tree. It performs this recursion from the root\n(the current state of the game) until reaching a terminal node (an end game). It then\nupdates the value of the selected nodes (minimax value). The algorithm descent repeats\nthis recursive operation starting from the root as long as there is some search time left.\nDescent is almost identical to UBFM. The only difference is that descent performs\nan iteration until reaching a terminal state while UBFM performs this iteration until\nreaching a leaf of the tree (UBFM stops the iteration much earlier). In other words,\nduring an iteration, UBFM just extends one of the leaves of the game tree while descent\nrecursively extends the best child from this leaf until reaching the end of the game.\nThe algorithm descent has the advantage of UBFM, i.e. to perform a longer search to\ndetermine a better action to play. By learning the values of the game tree (by using for\nexample tree learning), it also has the advantage of a minimax search at depth 1, i.e. to\nraise the values of the terminal nodes to the other nodes more quickly. In addition, the\nstates thus generated are closer to the terminal states. Their values are therefore better\napproximations.\nRemark 5. In the experiments of this article, when there is a value tie in best action(s),\nthe tie is broken at random. An alternative could be to choose the subtree whose prin-\ncipal variation is the least deep.\n6Because, with root and terminal learning, the extra computations of descent compared to UBFM with\nthe same number of iterations, do not change, in general, the values to learn and the move to play (but the\ntime to perform the iterations is much longer).\n25\n4.2. Comparison of Search Algorithms for Game Learning\nWe now compare descent with tree learning to MCTS with root learning and with\ntree learning, to iterative deepening alpha-beta with root learning and with tree learn-\ning, and to UBFM with tree learning, using the protocol of Section 3.2. Each combi-\nnation uses one of these tree search algorithms combined with tree/root learning. There\nare a total of 6 combinations. The experiment was repeated 32 times. The winning per-\ncentage of a combination for each game and for each evaluation step (i.e. each hour)\nis therefore calculated from 192 matches. The winning percentage curves are shown\nin Figure 3. The ﬁnal winning percentages are shown in Table 4. Each percentage\nof the table has required 6144 matches. It is descent which gets the best curves on\nall games. For two games (Surakarta and Outer Open Gomoku), the difference with\nUBFM is very narrow but the results remain better than the classic approaches (MCTS\nand alpha-beta). On each game, descent obtains a ﬁnal percentage higher than all the\nother combinations (except in Santorini where it is 2% lower than UBFM, the best\nalgorithm at this game). On average over all games, descent has 82% win and is above\nUBFM, the second best combination, by 18% and ID with tree learning, the third best\ncombination, by 34%.\nIn conclusion, descent (with tree learning) is undoubtedly the\nbest combination. UBFM (with tree learning) is the second best combination, some-\ntimes very close to descent performances and sometimes very far, but always superior\nto other combinations (slightly or largely depending on the game).\n5. Reinforcement Heuristic to Improve Learning Performance\nIn this section, we propose the technique of reinforcement heuristic, which consists\nto replace the classical terminal evaluation function – that we denote by bt, which\nreturns 1 if the ﬁrst player wins, −1 if the second player wins, and 0 in case of a draw\n[36, 2, 16] – by another heuristic to evaluate terminal states during the learning process.\nBy using this technique, non-terminal states are therefore evaluated differently, partial\ngame trees and thus matches during the learning process are different, which can impact\nthe learning performances. We start by deﬁning what we call a reinforcement heuristic\nand we offering several reinforcement heuristics. Then, we propose a complementary\n26\nFigure 3: Evolutions of the winning percentages of the combinations of the experiment of Section 4.2, i.e.\nof descent (dashed line), UBFM (dotted dashed line), MCTS (dotted line), iterative deepening alpha-beta\n(continuous line) with tree learning (blue line) or root learning (red line). The display uses a simple moving\naverage of 6 data.\n27\ntree learning\nroot learning\ndescent\nUBFM\nMCTS\nID\nMCTS\nID\nOthello\n89.4%\n47.2%\n37.7%\n42.7%\n44.9%\n22.1%\nHex\n94.9%\n71.7%\n20.5%\n50.7%\n50.2%\n20.5%\nClobber\n83.0%\n56.7%\n32.9%\n48.9%\n42.0%\n35.5%\nOuter Open Gomoku\n77.6%\n63.9%\n18.0%\n51.6%\n64.7%\n18.2%\nAmazons\n84.3%\n55.3%\n32.5%\n46.3%\n43.7%\n31.7%\nBreakthrough\n86.5%\n72.5%\n45.5%\n47.6%\n19.2%\n21.0%\nSantorini\n69.9%\n71.8%\n31.9%\n47.6%\n37.7%\n40.2%\nSurakarta\n82.8%\n69.7%\n41.2%\n42.1%\n29.4%\n14.5%\nLines of Action\n73.4%\n66.9%\n36.1%\n57.4%\n39.4%\n3.7%\nmean\n82.4%\n64.0%\n32.9%\n48.3%\n41.2%\n23.1%\nTable 4: Final winning percentages of the combinations of the experiment of Section 4.2 (ID: iterative\ndeepening alpha-beta ; see 3.2 ; 95% conﬁdence intervals: max ±0.85%)\ntechnique, that we call completion, which corrects state evaluation functions taking\ninto account the resolution of states. Finally, we compare the reinforcement heuristics\nthat we propose to the classical terminal evaluation function.\n5.1. Some Reinforcement Heuristics\nA reinforcement heuristic is a terminal evaluation function that is more expressive\nthan the classical terminal function, i.e. the game gain.\nDeﬁnition 6. Let bt the game gain function of a game (i.e. bt returns 1 if the ﬁrst\nplayer wins, −1 if the second player wins, and 0 in case of a draw).\nA reinforcement heuristic hr is a function that preserves the order of the game\ngain function: for any two terminal states of the game s, s′, bt (s) < bt (s′) implies\nhr (s) < hr (s′).\nIn the following subsections, we propose different reinforcement heuristics.\n28\n5.1.1. Scoring\nSome games have a natural reinforcement heuristic: the game score. For example,\nin the case of the game Othello (and in the case of the game Surakarta), the game score\nis the number of its pieces minus the number of pieces of his opponent (the goal of the\ngame is to have more pieces than its opponent at the end of the game). The scoring\nheuristic used as a reinforcement heuristic consists of evaluating the terminal states by\nthe ﬁnal score of the game. With this reinforcement heuristic, the adaptive evaluation\nfunction will seek to learn the score of states. In the context of an algorithm based on\nminimax, the score of a non-terminal state is the minimax value of the subtree starting\nfrom this state whose terminal leaves are evaluated by their scores. After training, the\nadaptive evaluation function then contains more information than just an approximation\nof the result of the game, it contains an approximation of the score of the game. If the\ngame score is intuitive, this should improve learning performances.\nRemark 7. In the context of the game of the Amazons, the score is the size of the\nterritory of the winning player, i.e. the squares which can be reached by a piece of the\nwinning player. This is approximately the number of empty squares.\n5.1.2. Additive and Multiplicative Depth Heuristics\nNow we offer the following reinforcement heuristic: the depth heuristic. It consists\nin giving a better value to the winning states close to the start of the game than to the\nwinning states far from the start. Reinforcement learning with the depth heuristic, it is\nlearning the duration of matches in addition to their results. This learned information\nis then used to try to win as quickly as possible and try to lose as late as possible. The\nhypothesis of this heuristic is that a state close to the end of the game has a more precise\nvalue than a state more distant and that the duration of the game is easily learned. Under\nthis assumption, with this heuristic, we will take less risk to try to win as quickly\nas possible and to lose as late as possible. In addition, with a long game, a player\nin difﬁculty will have more opportunities to regain the upper hand. We propose two\nrealizations of the depth heuristic: the additive depth heuristic, that we denote by pt,\nand the multiplicative depth heuristic, that we denote by pt′. The evaluation function\npt returns the value l if the ﬁrst player wins, the value −l if the second player wins,\n29\nand 0 in case of a draw, with l = P −p + 1 where P is the maximum number of\nplayable actions in a game and p is the number of actions played since the beginning\nof the game. For the game of Hex, l is the number of empty cells on the board plus 1.\nFor the games where P is very large or difﬁcult to compute, we can instead use l =\nmax\n\u0010\n1, ˜P −p\n\u0011\nwith ˜P a constant approximating P (close to the empirical average\nlength of matches).The evaluation function pt′ is identical except that l satisﬁes l =\n˜\nP\np .\nRemark 8. Note that the idea of fast victory and slow defeat has already been proposed\nbut not used in a learning process [62].\n5.1.3. Cummulative Mobility\nThe next reinforcement heuristic that we propose is cummulative mobility. It con-\nsists in favoring the games where the player has more possibility of action and where\nhis opponent has less. The implementation used in this article is as following. The\nvalue of a terminal state is M1\nM2 if the ﬁrst player wins, −M2\nM1 if the second player wins,\nand 0 in case of a draw, where M1 is the mean of the number of available actions in each\nturn of the ﬁrst player since the start of the game and M2 is the mean of the number of\navailable actions in each turn of the second player since the start of the game.\n5.1.4. Piece Counting: Presence\nFinally, we propose as reinforcement heuristic: the presence heuristic. It consists in\ntaking into account the number of pieces of each player and starts from the assumption\nthat the more a player has pieces the more this one has an advantage. There are several\nimplementations for this heuristic, we use in this article the following implementation:\nthe heuristic value is max(n1 −n2, 1) if the ﬁrst player wins, min(n1 −n2, −1) if the\nsecond player wins, and 0 in case of a draw, where n1 is the number of pieces of the\nﬁrst player and n2 is the number of pieces of the second player. Note that in the games\nSurakarta and Othello, the score corresponds to a presence heuristic.\n5.2. Completion\nRelying solely on the value of states calculated from the terminal evaluation func-\ntion and the adaptive evaluation function can sometimes lead to certain aberrant be-\nhaviors. More precisely, if we only seek to maximize the value of states, we will then\n30\nFigure 4: The left graph is a game tree where maximizing does not lead to the best decision ; the right graph\nis the left game tree with completion (nodes are labeled by a pair of values) and thus maximizing leads to\nthe best decision (square node: ﬁrst player node (max node), circle node: second player node (min node),\noctagon: terminal node).\nchoose to play a state s rather than another state s′ if s is of greater value than s′ even\nif s′ is a winning resolved state (a state is resolved if we know the result of the game\nstarting from this state in which the two players play optimally). A search algorithm\ncan resolve a state. This happens when all the leaves of the subtree starting from this\nstate are terminal. Choosing s rather than s′, a winning resolved state, is an error7 when\ns is not resolved (or when s is resolved and is not winning). By choosing s, guarantee\nof winning is lost. The left graph of Figure 4 illustrates such a scenario. It is therefore\nnecessary to take into account both the value of states and the resolution of states. The\ncompletion technique, which we propose in this section, is one way of doing it. It con-\nsists, on the one hand, in associating with each state s a completion value c(s) and a\nresolution value r(s). The completion value c(s) of a leaf state s is 0 if the state s is not\n7There is perhaps, in certain circumstances, an interest in making this error from the point of view of\nlearning.\n31\nterminal or if it is a draw, 1 if it is a winning terminal state and −1 if the state is a losing\nterminal state. The value c(s) of a non-leaf state s is computed as the minimax value\nof the subtree of the partial game tree starting from s where the leaves are evaluated by\ntheir completion value. The resolution value r(s) of a leaf state s is 0 if the state s is\nnot terminal and 1 if it is terminal. The resolution value r(s) of a non-leaf state s is 1 if\n|c (s)| = 1. Otherwise, r(s) is the minimum of the resolution values of the childen of\ns. The completion technique consists, on the other hand, in using c (·) to compute v (·).\nFor this, states are compared from pairs (c (·) , v (·)), by using the lexicographic order\n(instead of just compare states from v (·)). More precisely, the value v (s) of a state\ns is computed in calculating (c(s), v(s)) as the minimax value of the subtree of the\npartial game tree starting from s where the leaves l are evaluated by (c(l), v(l)). Thus,\nthe value v (s) of a winning state s is always the value of the corresponding winning\nterminal leaf. Finally, with the completion technique, to decide which action to play\nduring the search, we choose the best unresolved action if it exists and otherwise the\nbest resolved action (i.e. we choose the action which maximize (−r (s) , c (s) , v (s))\nin a max state and which minimize (r (s) , c (s) , v (s)) in a min state). The right graph\nof Figure 4 illustrates the use of completion. The use of the resolution of states also\nmakes it possible to stop the search in the resolved subtrees and thus to save comput-\ning time. Descent algorithm modiﬁed to use the completion and the resolution stop is\ndescribed in Algorithm 9. With completion, descent always chooses an action leading\nto a winning resolved state and never chooses, if possible, an action leading to a losing\nresolved state.\nRemark 9. For a game where there is no draw, the computation of the resolution value\nis not necessary (all necessary information is in the completion value).\nRemark 10. A variant consists in performing a strong resolution, i.e. to compute r (·)\nfor non-leaf states in the following way: r (s) is (in all cases) the minimum of the res-\nolution values of the childen of the state s. With this variant, unnecessary calculations\nare made to determine the best action to play. However, the additional values may be\nuseful for learning. It is not clear which variant is the best.\nAnother variant is possible, it consists in not using the completition c (·) to compute\n32\nv (·). With this version, v (s) is the minimax value of the subtree starting from s where\nthe leaf nodes l are evaluated by v (l). With this variant, for certain resolved states,\ninstead of learning a lower bound (in absolute value), which is an exact value, we learn\nan estimated value, which could be a potentially an overestimated value. It is not clear\nwhich variant is the best.\nWe also propose to use the resolution of states with action selections, to reduce the\nduration of games and therefore a priori the duration of the learning process: always\nplay an action leading to a winning resolved state if it exists and never play an action\nleading to a losing resolved state if possible. Thus, if among the available actions we\nknow that one of the actions is winning, we play it. If there is none, we play according\nto the chosen action selection method among the actions not leading to a resolved state\n(if possible). If it is not possible, we play the best action leading to a state resolved as\na draw and if there is none, we play the best action leading to a losing resolved state.\nWe call it completed action selection.\nRemark 11. It is not clear, however, that this improves performance as it prunes a\nportion of the game tree in the exploration whose values could be useful for learning\n(but could also negatively impact the learning performance).\n5.3. Comparison of Reinforcement Heuristics\nWe now compare the different heuristics that we have proposed to the classical ter-\nminal evaluation function bt on different games, using the protocol of Section 3.2. Each\ncombination uses descent with completion (Algorithm 9) and completed ϵ-greedy (see\nAlgorithm 2 and Section 5.2). Each combination uses a different terminal evaluation\nfunction. These terminal evaluations are the classical (“binary”) evaluation function\nbt, the additive depth heuristic, the multiplicative depth heuristic, the scoring heuristic,\nthe cummulative mobility, and the presence heuristic. Other parameters are the same\nas Section 3.3. There are, at most, a total of 6 combinations per game (on some games,\nsome heuristics are not evaluated because they are trivially of no interest or equivalent\nto another heuristic). The experiment was repeated 48 times. The winning percentage\nof a combination for each game and for each evaluation step (i.e. each hour) is there-\nfore calculated from 96 to 192 matches. The ﬁnal winning percentages are shown in\n33\nTable 5. Each percentage of the table has required between 4608 and 9216 matches.\nOn average and in 7 of the 9 games, the classic terminal heuristic has the worst per-\ncentage (exception are Othello and Lines of Action). In scoring games, scoring is the\nbest heuristic, as we might expect. Leaving aside the score heuristic, with the excep-\ntion of Surakarta, Othello and Clobber, it is one of the two depth heuristics that has\nthe best winning percentage. In Surakarta and Clobber, mobility is just ahead of the\ndepth heuristics. On average, using the additive depth heuristic instead of using the\nclassic evaluation increases the winning percentage by 15%, and using the best depth\nheuristic increases the winning percentage by 19%. The winning percentage curves are\nshown in Figure 5. The ﬁnal percentages summarize the curves quite well. Note how-\never, on the one hand, that the clear impact compared to the other heuristics (except\nscore) of the additive depth heuristic to Breakthrough, Amazons, Hex, and Santorini\nand of the multiplicative depth heuristic to Clobber, Hex, and Open Outer Gomoku.\nIn conclusion, the use of generic reinforcement heuristics has signiﬁcantly improved\nperformances and the depth heuristics are prime candidates as a powerful generic rein-\nforcement heuristic.\n6. Search Algorithms for Game Playing\nIn this section, we propose another variant of UBFM, dedicated to be used in\ncompetition mode. Then, we compare it with other tree search algorithms.\n6.1. Unbound Best-First Minimax with Safe Decision: UBFMs\nThus, we propose a modiﬁcation of UBFM, denoted UBFMs. It aims to provide\na safer game. The action UBFM chooses to play is the one that leads to the state of\nbest value. In some cases, the (a priori) best action can lead to a state that has not\nbeen sufﬁciently visited (such as a non-terminal leaf). Choosing this action is therefore\na risky decision. We propose, to avoid this problem, a different decision that aims to\nplay the safest action, in the same way as MCTS (max child selection [13]). If no\naction leads to a winning resolved state, the action chosen by UBFMs is the one that\nhas been the most selected (since the current state of the game) during the exploration\n34\nFigure 5: Evolutions of the winning percentages of the combinations of the experiment of Section 5.3,\ni.e. the use of the following heuristics: classic (black line), score (purple line), additive depth (blue line),\nmultiplicative depth (turquoise line), cumulative mobility (green line), and presence (red line). The display\nuses a simple moving average of 6 data.\n35\ndepth\nclassic\nscore\nadditive\nmultiplicative\nmobility\npresence\nOthello\n49.8%\n70.6%\n50.1%\n48.9%\n18.5%\nscore\nHex\n33.3%\nX\n66.1%\n60.4%\nX\nX\nClobber\n43.7%\nX\n47.0%\n49.8%\n53.5%\nX\nOuter Open Gomoku\n33.0%\nX\n41.4%\n74.4%\nX\nX\nAmazons\n36.8%\n67.9%\n60.0%\n50.7%\n49.0%\nX\nBreakthrough\n39.0%\nX\n69.5%\n40.4%\n43.9%\n48.5%\nSantorini\n42.7%\nX\n59.7%\n46.6%\n43.3%\nX\nSurakarta\n33.5%\n68.9%\n43.1%\n35.3%\n55.6%\nscore\nLines of Action\n50.9%\nX\n57.1%\n46.8%\n53.7%\n44.0%\nmean\n40.3%\n69.1%\n54.9%\n50.4%\n45.4%\n46.3%\nTable 5: Final winning percentages of the combinations of the experiment of Section 5.3 (X: heuristic without\ninterest in this context ; presence coincides with score in Surakarta and Othello ; see 3.2 ; 95% conﬁdence\nintervals: max ±1.04%)\nof the game tree. In case of a tie, UBFMs decides by choosing the one that leads to\nthe state of best value. This decision is safer because the number of times an action is\nselected is the number of times that this action is more interesting than the others.\nExample 12. The current player has the choice between two actions a1 and a2. The\naction a1 leads to a state of value 5 and was selected 7 times (from the current state\nand from the beginning of the game). The action a2 leads to a state of value 2 and was\nselected 30 times. UBFM chooses the action a1 while UBFMs chooses the action a2.\nThe algorithm of UBFMs with completion is described in Algorithm 12 (which\nuses Algorithm 11 ; resolution stop is not used for the succinctness of the presentation).\nRemark 13. In some cases, we will prefer to ensure the draw rather than trying to\nwin. Algorithm 12 must then be adapted: to decide the action to play, the ﬁrst player\nmust then maximize (c(a(s)), r (a(s)) , n(s, a), v′ (s, a)) and the second player must\nminimize (c(a(s)), −r (a(s)) , n(s, a), v′ (s, a)).\n36\n6.2. Comparison of Search Algorithms for Game Playing\nWe now compare the winning percentages of different game algorithms, by using\nevaluation functions learned by starting again the experiment of Section 4.2 with 16 as\nrepetition number (the used evaluations are those corresponding to the end of the learn-\ning process, i.e. h = 48). We compare UBFMs with UBFM and iterative deepening\nalpha-beta with move ordering, denoted ID (each of these algorithms uses completion).\nFor each game, each combination (A, fθ48, r) confronts the combinations (A′, fθ48, r),\nwith a search time of 1s per action, where A ∈{UBFMs, UBFM, ID}, A′ is minimax\nat depth 1, fθ48 is one of the ﬁnal evaluation functions of Section 4.2, and r is the\nnumber of one of the 16 repetitions (r ∈{1, . . . , 16}). The winning percentage of A is\nthe average of the winning percentage of (A, fθ48, r) over the functions fθ48 and over\nthe repetitions r ∈{1, . . . , 16}. The winning percentages are described in Table 6.\nFor each game, the winning percentage of a search algorithm is calculated from 18432\nmatches. On all games except Clobber, UBFMs gets the best winning percentage (on\ntwo games, Hex and Outer Open Gomoku, UBFMs and ID have the same percentage).\nOn Clobber, it is UBFM which obtains the best percentage, but only 1% more than\nUBFMs . On average, across all games, UBFMs is 3.6% better than UBFM and 5%\nbetter than ID.\nThen a variation of this experiment was performed. For each game, each combi-\nnation (A, fθ48, r) confronts all the others, but the used evaluation functions fθ48 are\nrestricted to those generated from the learning algorithm descent and the search time\nis 10s per action. The corresponding winning percentages are described in Table 7.\nFor each game, the winning percentage of a search algorithm is calculated from 1536\nmatches. In all games, except Clobber and Santorini, it is again UBFMs which obtains\nthe best winning percentage. At Clobber and Santorini, it is UBFM which obtains the\nbest percentage. On average across all games, UBFMs is 5% better than UBFM and\n19% better than ID. In conclusion, in the context of these experiments, UBFMs is the\nbest search algorithm.\nRemark 14. After the realization of this study, a study comparing UBFM and UBFMs\nto MCTS and ID was been performed at Hex 11 and Hex 13 using evaluation functions\n37\nOuter Open Gomoku\nClobber\nBreakthrough\nSantorini\nHex\nUBFMs\n45%\n84%\n51%\n62%\n51%\nUBFM\n38%\n85%\n45%\n58%\n41%\nID\n45%\n82%\n45%\n55%\n51%\nLines of Action\nOthello\nAmazons\nSurakarta\nmean\nUBFMs\n48%\n62%\n62%\n43%\n56.6%\nUBFM\n41%\n61%\n61%\n38%\n52.0%\nID\n39%\n55%\n51%\n42%\n51.6%\nTable 6: Average winning percentages of UBFMs, UBFM, and ID over the evaluation functions fθ48 of\nSection 4.2, for different games, of the ﬁrst experiment of Section 6.2 (search time: 1 second per action ;\n95% conﬁdence intervals: ±0.1%).\nOuter Open Gomoku\nClobber\nBreakthrough\nSantorini\nHex\nUBFMs\n64%\n49%\n63%\n53%\n65%\nUBFM\n45%\n53%\n51%\n56%\n60%\nID\n41%\n50%\n37%\n41%\n27%\nLines of Action\nOthello\nAmazons\nSurakarta\nmean\nUBFMs\n56%\n58%\n53%\n49%\n57%\nUBFM\n53%\n56%\n52%\n42%\n52%\nID\n24%\n33%\n51%\n38%\n38%\nTable 7: Average winning percentages of UBFMs, UBFM, and ID over the evaluation functions fθ48\n(generated from descent in Section 4.2), for different games, of the second experiment of Section 6.2 (search\ntime: 10 seconds per action ; 95% conﬁdence intervals: ±0.8%).\n38\ngenerated from long training runs (more than 30 days) [? ]. For each of the performed\nexperiments, UBFMs is the best search algorithm, followed by UBFM.\nRemark 15. When UBFM is used, to decide which action to play, the equality of values\nis determined by the number of visits (the most chosen action is preferred).\n7. Ordinal Distribution and Application to Hex\nIn this section, we propose the last technique, a new action selection distribution,\nand we apply it with all the previous techniques to design program-players to the game\nof Hex.\n7.1. Ordinal Action Distribution\nThus, we propose an alternative probability distribution (see Section 2.3), that we\ncall ordinal distribution. This distribution does not depend on the value of states. How-\never, it depends on the order of their values. Its formula is:\nP (ci) =\n\u0012\nϵ′ + 1 −ϵ′\nn −i\n\u0013\n·\n\n1 −\nj<i\nX\nj=0\nP (cj)\n\n\nwith n the number of children of the root, i ∈{0, . . . , n −1}, ci the i-th best child\nof the root, P (ci) the probability of playing the action leading to the child ci and ϵ′\nthe exploitation parameter (ϵ′ = 1 −ϵ). Algorithm 13 describes the action selection\nmethod resulting from the use of the ordinal distribution with an optimized calculation.\nRemark 16. In an experiment using the common protocol, not presented in this ar-\nticle, ordinal distribution has been mostly better than softmax distribution, but lower\nthan ϵ-greedy. However, during long-time learning processes at Hex (similar to the\nexperiments of the following sections), ordinal distribution has performed best.\n7.2. A Long Training for Hex 11\nWe now apply all the techniques that we have proposed to carry out a long self-play\nreinforcement learning on Hex size 11 with swap. More precisely, we use completed\n39\ndescent (Algorithm 9) with tree learning (Algorithm 3), completed ordinal distribution\n(see Section 5.2 and Algorithm 13), and the additive depth heuristic (see Section 5.1.2).\n7.2.1. Technical details\nIn addition, we use a classical data augmentation: the adding of symmetrical states.\nSymmetrical states are added in D, the set of pairs (s, v) of the game tree (see Sec-\ntion 3), after the end of each game and before the application of experience replay. For-\nmally, D ←D ∪{(r180° (s) , v) | (s, v) ∈D} where r180° (s) is s rotated by 180◦. In\nother words, the processing(D) method of Algorithm 3 is experience replay(symmetry(D),\nµ, σ)) where symmetry(D) adds symetrical states in D as described above and re-\nturns D. The used learning parameters are: search time per action τ = 2s, batch size\nB = 3000. The experience replay parameters are: games memory size µ = 1008,\nsampling rate σ = 5%. We use the following neural network as adaptative evaluation\nfunction: a residual network with 4 residual blocks (2 convolutions per block with 83\nﬁlters each) followed by a fully connected hidden layers (with 74 neurons). The ac-\ntivation function used is the ReLU. The input of the neural network is a game board\nextended by one line at the top, bottom, right and left (in the manner of [36, 20]). More\nprecisely, each of these lines is completely ﬁlled with the stones of the player of the\nside where it is located. This extension is simply used to explicitly represent the sides\nof the board and their membership. Moreover, when the children of a state are evalu-\nated by the neural network, they are batched and thus evaluated in parallel (on the only\nGPU). The evaluation function has been pre-initialized by learning the values of ran-\ndom terminal states (their number is 15, 168, 000). Other parameters are the same as\nSection 3.2.1. Resolved states are kept in memory (the memory of the resolved states\nis emptied every two learning day).\nThe reinforcement learning process lasted 34, 321 matches. Note that the number\nof data used during the learning process is of the order of 59·107, the number of neural\nnetwork evaluations is of the order of 196 · 106, and the number of state evaluations is\n8A variant of replay experience was applied, it is here to memorize the pairs of the last 100 games and\nnot the last 100 pairs.\n40\nsearch time\nMohex 2nd\nMohex 1st\nmean\n95% conﬁdence interval\ntotal matchs\n2.5s\n98%\n84%\n91%\n2%\n1000\n10s\n91%\n86%\n88%\n2%\n1600\nTable 8: Winning percentages against Mohex 3HNN of UBFMs using the learned evaluation function of\nSection 7.2 (the search time per action is the same for each player ; default settings are used for Mohex ; as\nmany matches in ﬁrst as in second player).\nof the order of 15 · 109.\n7.2.2. Results\nThe winning percentages against Mohex of UBFMs using the learned evaluation\nfunction are described in Table 8. Note that the proposed techniques have made it\npossible to exceed the level of Mohex 3HNN on Hex size 11 (with swap), and without\nthe use of knowledge (which had not been done until then).\n7.3. A Long Training for Hex 13\nWe carry out the same experiment as the previous section, but on Hex size 13\n(always with swap).\n7.3.1. Technical differences\nThe architecture of the neural network is 8 residual blocks (2 convolutions per block\nwith 186 ﬁlters each) followed by 2 fully connected hidden layers (with 220 neurons\neach). The activation function used is the ReLU. The network was not initialized by\nrandom end state values. The experience replay parameters are: games memory size\nµ = 2509, sampling rate σ = 2%. The search time per action τ is 5s. The reinforce-\nment learning process lasted 5, 288 matches. Note that the number of data used during\nthe learning process is of the order of 16 · 107, the number of neural network evalua-\ntions is of the order of 64 · 106, and the number of state evaluations is of the order of\n7 · 109.\n9A variant of replay experience was applied, it is here to memorize the pairs of the last 250 games and\nnot the last 250 pairs.\n41\nsearch\nMohex 2nd\nMohex 1st\nmean\n95% conﬁdence interval\ntotal matchs\n2.5s\n100%\n100%\n100%\n0%\n1200\n10s\n100%\n100%\n100%\n0%\n800\nTable 9: Winning percentages against Mohex 3HNN of UBFMs using the learned evaluation function of\nSection 7.3 (the search time per action is the same for each player ; default settings are used for Mohex ; as\nmany matches in ﬁrst as in second player).\n7.3.2. Results\nThe winning percentages against Mohex 3HNN of UBFMs using the learned eval-\nuation function are described in Table 9. The techniques that we have proposed have\nalso allowed for Hex size 13 (with swap) to exceed, the level of Mohex 3HNN, without\nthe use of knowledge (which had also not been done until then).\n8. Conclusion\nWe have proposed several new techniques for reinforcement learning state evalua-\ntion functions.\nFirstly, we have generalized tree bootstrapping (tree learning) and shown that learn-\ning the values of the game tree instead of just learning the value of the root signiﬁ-\ncantly improves learning performances with (iterative deepening) alpha-beta, and that\nthis combination is more efﬁcient than MCTS with tree learning, root learning or ter-\nminal learning, in the context of reinforcement learning without knowledge based on\nnon-linear functions.\nSecondly, we have introduced the descent algorithm which explores in the man-\nner of unbounded best-ﬁrst minimax, intended to be used during the learning process.\nUnlike the latter, descent iteratively explores the sequences of best actions up to the ter-\nminal states. Its objective is to improve the quality of the data used during the learning\nphases, while keeping the advantages of unbounded best-ﬁrst minimax. In the context\nof our experiments, the use of descent gives signiﬁcantly better performances than the\nuse of alpha-beta, unbound minimax, or MCTS.\nThirdly, we have suggested to replace the classic gain of a game (+1/ −1) by\n42\na different terminal evaluation function. We have propose different general terminal\nevaluations, such as the depth heuristic, which takes into account the duration of games\nin order to favor quick wins and slow defeats. Our experiments have shown that the\nuse of a reinforcement heuristic improves performances and that the depth heuristic is\na useful general heuristic.\nFourthly, we have proposed another variant of best-ﬁrst minimax, which plays the\nsafest action instead of playing the best action. It is intended to be used, in particular,\nafter the learning process. In the experiments carried out, it is this algorithm which\nobtains the best winning percentages on average and in 8 of the 9 tested games. We\nhave also proposed a new action selection distribution which does not take into account\nthe value of states but only their order.\nIn the context of this experiments, the techniques that we propose are the best\napproach of reinforcement learning of state values. We apply all these different tech-\nniques to design program-players to Hex with swap (size 11 and 13) surpassing, for\nthe ﬁrst time, the level of Mohex 3HNN (the best Hex program based on supervised\nlearning and dedicated Hex algorithms and knowledge) by reinforcement learning from\nself-play without knowledge (or Monte Carlo techniques).\nOverall, the results show that unbounded minimax, which has obtained good re-\nsults in the experiments of this article, has been under-studied in the literature and that\nunbounded minimax and its variants, that we offer, seems to constitute particularly\nefﬁcient alternatives to alpha-beta and MCTS, in the era of reinforcement learning.\nA promising research perspective is the parallelization of UBFM and descent when\nlearning: UBFM determines the next action to play and descent determines the pairs\nto learn. The other research perspectives include the application of our contributions\nto the game of Go and to General Game Playing (at ﬁrst with perfect information).\nThey also include the application and adaptation of our contributions to the contexts\nof hidden information, stochastic and multiplayer games. Moreover, they include the\napplication of our contributions to optimization problems, such that the RNA Inverse\nFolding problem [63].\nRemark 17. The realized programs (based on the descent framework) are programmed\n43\nin Python. Switching to a faster performing language should reduce the learning time\nby a factor between two and ﬁve.\nNote to conclude that, on the one hand, the descent framework with the unbounded\nminimax faced Polygames during the computer olympiad of 2020 and beat it at Othello\n8x8, Othello 10x10 and Breakthrough. In fact, 5 gold medals were won by the pro-\nposed algorithms for the following games: Othello 10x10, Breakthrough, Surakarta,\nAmazons, and Clobber. At Clobber and at Surakarta, one of the participating programs\nwas also based on AlphaZero. That of Clobber used in addition the combinatorial game\ntheory to solve states. Moreover, a silver medal were won by the proposed algorithms\nat Othello 8x8. It is the ﬁrst time at the Computer Olympiad that the same program\nwins 5 gold medals.\nOn the other hand, the descent framework has also competed in the 2021 Computer\nOlympiad. This time it won 11 gold medals (Hex 11 × 11, Hex 13 × 13, Hex 19 × 19,\nHavannah 8×8, Havannah 10×10, Othello 8×8, Surakarta, Amazons, Breakthrough,\nBrazilian Draughts, Canadian Draughts; there was no competition at Othello 10x10\nand Clobber). Descent notably beat Polygames at games where they met (Hex 13×13,\nHex 19 × 19, Havannah 8 × 8, Havannah 10 × 10).\n9. Acknowledgements\nThis work was granted access to the HPC resources of IDRIS under the alloca-\ntion 2020-AD011011461 made by GENCI. Quentin Cohen-Solal is supported by the\nPRAIRIE institute. I thank GREYC for giving me access to its computation server,\nwhich allowed me to perform several experiments that inspired those in this article.\nReferences\n[1] M. L. Littman, Algorithms for sequential decision making, Ph.D. thesis, 1996.\n[2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,\nT. Hubert, L. Baker, M. Lai, A. Bolton, et al., Mastering the game of go without\nhuman knowledge, Nature 550 (2017) 354.\n44\n[3] J. Mycielski, Games with perfect information, Handbook of game theory with\neconomic applications 1 (1992) 41–70.\n[4] T. Cazenave, Y.-C. Chen, G.-W. Chen, S.-Y. Chen, X.-D. Chiu, J. Dehos, M. Elsa,\nQ. Gong, H. Hu, V. Khalidov, et al., Polygames: Improved zero learning, arXiv\npreprint arXiv:2001.09832 (2020).\n[5] M. Genesereth, N. Love, B. Pell, General game playing: Overview of the aaai\ncompetition, AI magazine 26 (2005) 62.\n[6] N. R. Sturtevant, R. E. Korf,\nOn pruning techniques for multi-player games,\nAAAI/IAAI 49 (2000) 201–207.\n[7] D. Michie, Game-playing and game-learning automata, in: Advances in pro-\ngramming and non-numerical computation, Elsevier, 1966, pp. 183–200.\n[8] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artiﬁcial Intelligence 134\n(2002) 57–83.\n[9] J. Mandziuk, Knowledge-free and learning-based methods in intelligent game\nplaying, volume 276, Springer, 2010.\n[10] G. Tesauro, Temporal difference learning and td-gammon, Communications of\nthe ACM 38 (1995) 58–68.\n[11] H. Baier, M. H. Winands, Mcts-minimax hybrids with state evaluations, Journal\nof Artiﬁcial Intelligence Research 62 (2018) 193–231.\n[12] R. Coulom, Efﬁcient selectivity and backup operators in monte-carlo tree search,\nin: Computers and Games, 5th International Conference, CG 2006, Turin, Italy,\nMay 29-31, 2006. Revised Papers, 2006, pp. 72–83.\n[13] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlf-\nshagen, S. Tavener, D. Perez, S. Samothrakis, S. Colton, A survey of monte carlo\ntree search methods, Transactions on Computational Intelligence and AI in games\n4 (2012) 1–43.\n45\n[14] C. Clark, A. Storkey, Training deep convolutional neural networks to play go, in:\nInternational Conference on Machine Learning, 2015, pp. 1766–1774.\n[15] C. Gao, R. B. Hayward, M. M¨uller, Move prediction using deep convolutional\nneural networks in hex, Transactions on Games (2017).\n[16] C. Gao, M. M¨uller, R. Hayward,\nThree-head neural network architecture for\nmonte carlo tree search., in: International Joint Conference on Artiﬁcial Intelli-\ngence, 2018, pp. 3762–3768.\n[17] T. Cazenave, Residual networks for computer go, Transactions on Games 10\n(2018) 107–110.\n[18] Y. Tian, Y. Zhu, Better computer go player with neural network and long-term\nprediction, arXiv preprint arXiv:1511.06410 (2015).\n[19] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,\nJ. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., Mastering\nthe game of go with deep neural networks and tree search, Nature 529 (2016)\n484.\n[20] T. Anthony, Z. Tian, D. Barber, Thinking fast and slow with deep learning and\ntree search, in: Advances in Neural Information Processing Systems, 2017, pp.\n5360–5370.\n[21] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanc-\ntot, L. Sifre, D. Kumaran, T. Graepel, et al., A general reinforcement learning\nalgorithm that masters chess, shogi, and go through self-play, Science 362 (2018)\n1140–1144.\n[22] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanc-\ntot, L. Sifre, D. Kumaran, T. Graepel, et al.,\nMastering chess and shogi\nby self-play with a general reinforcement learning algorithm,\narXiv preprint\narXiv:1712.01815 (2017).\n46\n[23] Q. Cohen-Solal, Apprendre `a jouer aux jeux `a deux joueurs `a information parfaite\nsans connaissance, 2019.\n[24] C. Gao, K. Takada, R. Hayward, Hex 2018: Mohex3hnn over deepezo., J. Int.\nComput. Games Assoc. 41 (2019) 39–42.\n[25] G. N. Yannakakis, J. Togelius, Artiﬁcial Intelligence and Games, Springer, 2018.\n[26] D. E. Knuth, R. W. Moore, An analysis of alpha-beta pruning, Artiﬁcial Intelli-\ngence 6 (1975) 293–326.\n[27] I. Millington, J. Funge, Artiﬁcial intelligence for games, CRC Press, 2009.\n[28] D. J. Slate, L. R. Atkin, Chess 4.5¨ı¿œthe northwestern university chess program,\nin: Chess skill in Man and Machine, Springer, 1983, pp. 82–118.\n[29] R. E. Korf, Depth-ﬁrst iterative-deepening: An optimal admissible tree search,\nArtiﬁcial Intelligence 27 (1985) 97–109.\n[30] W. Fink, An enhancement to the iterative, alpha-beta, minimax search procedure,\nICGA Journal 5 (1982) 34–35.\n[31] H. J. Van Den Herik, M. H. Winands, Proof-number search and its variants, in:\nOppositional Concepts in Computational Intelligence, Springer, 2008, pp. 91–\n118.\n[32] J. Schaeffer, Conspiracy numbers, Artiﬁcial Intelligence 43 (1990) 67–84.\n[33] H. Berliner,\nThe b* tree search algorithm: A best-ﬁrst proof procedure,\nin:\nReadings in Artiﬁcial Intelligence, Elsevier, 1981, pp. 79–87.\n[34] R. E. Korf, D. M. Chickering, Best-ﬁrst minimax search, Artiﬁcial intelligence\n84 (1996) 299–337.\n[35] R. D. Greenblatt, D. E. Eastlake, S. D. Crocker, The greenblatt chess program,\nin: Computer chess compendium, Springer, 1988, pp. 56–66.\n[36] K. Young, G. Vasan, R. Hayward, Neurohex: A deep q-learning hex agent, in:\nComputer Games, Springer, 2016, pp. 3–18.\n47\n[37] J. Veness, D. Silver, A. Blair, W. Uther, Bootstrapping from game tree search, in:\nAdvances in neural information processing systems, 2009, pp. 1937–1945.\n[38] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,\nA. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Human-level\ncontrol through deep reinforcement learning, Nature 518 (2015) 529.\n[39] J. Baxter, A. Tridgell, L. Weaver, Learning to play chess using temporal differ-\nences, Machine Learning 40 (2000) 243–263.\n[40] Y. Bj¨ornsson, T. A. Marsland, Learning extension parameters in game-tree search,\nInformation Sciences 154 (2003) 95–118.\n[41] J. Mellor, Decision Making Using Thompson Sampling, Ph.D. thesis, 2014.\n[42] N. N. Schraudolph, P. Dayan, T. J. Sejnowski,\nLearning to evaluate go posi-\ntions via temporal difference methods, in: Computational Intelligence in Games,\nSpringer, 2001, pp. 77–98.\n[43] S. Kirkpatrick, C. D. Gelatt, M. P. Vecchi, Optimization by simulated annealing,\nScience 220 (1983) 671–680.\n[44] C. Browne, Hex Strategy: Making the right connections, AK Peters Natick, MA,\n2000.\n[45] H. J. Van Den Herik, J. W. Uiterwijk, J. Van Rijswijck, Games solved: Now and\nin the future, Artiﬁcial Intelligence 134 (2002) 277–311.\n[46] E. R. Berlekamp, J. H. Conway, R. K. Guy, Winning Ways for Your Mathematical\nPlays, Volume 3, AK Peters/CRC Press, 2003.\n[47] J. Pawlewicz, R. B. Hayward, Scalable parallel dfpn search, in: International\nConference on Computers and Games, Springer, 2013, pp. 138–150.\n[48] S. Reisch, Hex ist pspace-vollst¨andig, Acta Informatica 15 (1981) 167–191.\n[49] ´E. Bonnet, F. Jamain, A. Safﬁdine,\nOn the complexity of connection games,\nTheoretical Computer Science 644 (2016) 2–28.\n48\n[50] S.-C. Huang, B. Arneson, R. B. Hayward, M. M¨uller, J. Pawlewicz, Mohex 2.0:\na pattern-based mcts hex player, in: International Conference on Computers and\nGames, Springer, 2013, pp. 60–71.\n[51] R. S. Michalski, J. G. Carbonell, T. M. Mitchell, Machine learning: An artiﬁcial\nintelligence approach, Springer Science & Business Media, 2013.\n[52] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015) 436.\n[53] I. Goodfellow, Y. Bengio, A. Courville, Y. Bengio, Deep learning, volume 1, MIT\npress Cambridge, 2016.\n[54] A. Krizhevsky, I. Sutskever, G. E. Hinton,\nImagenet classiﬁcation with deep\nconvolutional neural networks, in: Advances in Neural Information Processing\nSystems, 2012, pp. 1097–1105.\n[55] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,\nin: Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.\n[56] K. Takada, H. Iizuka, M. Yamamoto, Reinforcement learning for creating eval-\nuation function using convolutional neural network in hex, in: 2017 Conference\non Technologies and Applications of Artiﬁcial Intelligence, IEEE, 2017, pp. 196–\n201.\n[57] K. Takada, H. Iizuka, M. Yamamoto, Reinforcement learning to create value and\npolicy functions using minimax tree search in hex, IEEE Transactions on Games\n(2019).\n[58] ´E. Piette, D. J. N. J. Soemers, M. Stephenson, C. F. Sironi, M. H. M. Winands,\nC. Browne,\nLudii – the ludemic general game system,\nin: G. D. Giacomo,\nA. Catala, B. Dilkina, M. Milano, S. Barro, A. Bugar ˜An, J. Lang (Eds.), Proceed-\nings of the 24th European Conference on Artiﬁcial Intelligence (ECAI 2020),\nvolume 325 of Frontiers in Artiﬁcial Intelligence and Applications, IOS Press,\n2020, pp. 411–418.\n49\n[59] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint\narXiv:1412.6980 (2014).\n[60] A. Y. Ng, Feature selection, l 1 vs. l 2 regularization, and rotational invariance,\nin: The twenty-ﬁrst international conference on Machine learning, ACM, 2004,\np. 78.\n[61] X. Glorot, A. Bordes, Y. Bengio,\nDeep sparse rectiﬁer neural networks,\nin:\nThe Fourteenth International Conference on Artiﬁcial Intelligence and Statistics,\n2011, pp. 315–323.\n[62] T. Cazenave, A. Safﬁdine, M. J. Schoﬁeld, M. Thielscher, Nested monte carlo\nsearch for two-player games, in: The Thirtieth AAAI Conference on Artiﬁcial\nIntelligence, February 12-17, 2016, Phoenix, Arizona, USA., 2016, pp. 687–693.\n[63] T. Cazenave, T. Fournier,\nMonte carlo inverse folding,\narXiv preprint\narXiv:2005.09961 (2020).\n50\nAlgorithm 9 Descent tree search algorithm with completion and resolution stop (see\nTable 1 for the deﬁnitions of symbols and Algorithm 10 for the deﬁnitions of com-\npleted best action(s) and backup resolution(s)). Note: T = (v, v′, c, r) and S is the\nset of states of the partial game tree which are non-leaves or terminal.\nFunction completed descent iteration(s, S, T, fθ, ft)\nif terminal(s) then\nS ←S ∪{s}\nr (s) , c (s) , v (s) ←1, bt(s), ft(s)\nelse\nif s /∈S then\nS ←S ∪{s}\nforeach a ∈actions(s) do\nif terminal(a(s)) then\nS ←S ∪{a(s)}\nv′(s, a) ←ft (a(s))\nr(a(s)), c(a(s)), v(a(s)) ←1, bt(a(s)), v′(s, a)\nelse\nv′(s, a) ←fθ (a(s))\nab ←completed best action(s, actions(s))\nc(s), v(s) ←c (ab(s)) , v′ (s, ab)\nr (s) ←backup resolution(s)\nif r (s) = 0 then\nA ←{a ∈actions(s) | r (a (s)) = 0}\nab ←completed best action dual(s, A)\nn(s, ab) ←n(s, ab) + 1\nv′ (s, ab) ←completed descent iteration(ab(s))\nab ←completed best action(s, actions(s))\nc(s), v(s) ←c (ab(s)) , v′ (s, ab)\nr (s) ←backup resolution(s)\nreturn v (s)\nFunction completed descent(s, S, T, fθ, ft, τ)\nt = time()\nwhile time()−t < τ ∧r (s) = 0 do completed descent iteration(s,\nS, T, fθ, ft)\nreturn S, T\n51\nAlgorithm 10 Deﬁnition of the algorithms completed best action(s, A), which com-\nputes the a priori best action by using completion, and backup resolution(s), which\nupdates the resolution of s from its child states.\nFunction completed best action(s, A)\nif first player(s) then\nreturn arg max\na∈A\n(c (a (s)) , v′ (s, a) , n (s, s′))\nelse\nreturn arg min\na∈A\n(c (a (s)) , v′ (s, a) , −n (s, s′))\nFunction completed best action dual(s, A)\nif first player(s) then\nreturn arg max\na∈A\n(c (a (s)) , v′ (s, a) , −n (s, s′))\nelse\nreturn arg min\na∈A\n(c (a (s)) , v′ (s, a) , n (s, s′))\nFunction backup resolution(s)\nif |c (s)| = 1 then\nreturn 1\nelse\nreturn min a∈actions(s)r (a (s))\n52\nAlgorithm 11 UBFMs tree search algorithm with completion and resolution stop (see\nTable 1 for the deﬁnitions of symbols and Algorithm 10 for the deﬁnitions of com-\npleted best action(s) and backup resolution(s)). Note: T = (v, v′, c, r, n).\nFunction ubfms iteration(s, S, T, fθ, ft)\nif terminal(s) then\nS ←S ∪{s}\nr(s), c(s), v(s) ←1, bt(s), ft(s)\nelse\nif r (s) = 0 then\nif s /∈S then\nS ←S ∪{s}\nforeach a ∈actions(s) do\nif terminal(a(s)) then\nS ←S ∪{a(s)}\nv′(s, a) ←ft (a(s))\nr(a(s)), c(a(s)), v(a(s)) ←1, bt(a(s)), v′(s, a)\nelse\nv′(s, a) ←fθ (a(s))\nelse\nA ←{a ∈actions(s) | r (a (s)) = 0}\nab ←completed best action dual(s, A)\nn(s, ab) ←n(s, ab) + 1\nv′ (s, ab) ←ubfms iteration(ab(s))\nab ←completed best action(s, actions(s))\nc(s), v(s) ←c (ab(s)) , v′ (s, ab)\nr (s) ←backup resolution(s)\nreturn v(s)\nFunction ubfms tree search(s, S, T, fθ, ft, τ)\nt = time()\nwhile time()−t < τ ∧r (s) = 0 do ubfms iteration(s, S, T, fθ, ft)\nreturn S, T\n53\nAlgorithm 12 UBFMs action decision algorithm with completion (see Table 1 for the\ndeﬁnitions of symbols). Note: T = (v, v′, c, r, n) and S is the set of states of the game\ntree which are non-leaves or terminal.\nFunction safest action(s, T)\nif first player(s) then\nreturn\narg max\na∈actions(s)\n(c(a(s)), n(s, a), v′ (s, a))\nelse\nreturn\narg min\na∈actions(s)\n(c(a(s)), −n(s, a), v′ (s, a))\nFunction ubfms(s, S, T, fθ, ft, τ)\nS, T ←ubfms tree search(s, S, T, fθ, ft)\nreturn safest action(s, T)\nAlgorithm 13 Ordinal action distribution algorithm with simulated annealing used in\nthe experiments of this article (see Table 1 for the deﬁnitions of symbols).\nFunction ordinal(s, v′)\nif first player(s) then\nA ←actions(s) sorted in descending order by a 7→v′(s, a)\nelse\nA ←actions(s) sorted in ascending order by a 7→v′(s, a)\nj ←0\nn ←|A|\nfor a ∈A do\nif probability\n\u0000 t\nT · (n −j −1) + 1\n\u0001\n/ (n −j) then\nreturn a\nj ←j + 1\n54\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2020-08-03",
  "updated": "2021-10-12"
}