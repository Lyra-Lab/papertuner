{
  "id": "http://arxiv.org/abs/2008.01548v1",
  "title": "Defining and Evaluating Fair Natural Language Generation",
  "authors": [
    "Catherine Yeo",
    "Alyssa Chen"
  ],
  "abstract": "Our work focuses on the biases that emerge in the natural language generation\n(NLG) task of sentence completion. In this paper, we introduce a framework of\nfairness for NLG followed by an evaluation of gender biases in two\nstate-of-the-art language models. Our analysis provides a theoretical\nformulation for biases in NLG and empirical evidence that existing language\ngeneration models embed gender bias.",
  "text": "Deﬁning and Evaluating Fair Natural Language\nGeneration\nCatherine Yeo\nHarvard University\ncyeo@college.harvard.edu\nAlyssa Chen\nHarvard University\nalyssachen@college.harvard.edu\nAbstract\nOur work focuses on the biases that emerge in the natural language gen-\neration (NLG) task of sentence completion. In this paper, we introduce a\nframework of fairness for NLG followed by an evaluation of gender biases\nin two state-of-the-art language models. Our analysis provides a theoretical\nformulation for biases in NLG and empirical evidence that existing language\ngeneration models embed gender bias.\n1\nIntroduction\nQuantifying fairness is a major consideration in the ethical use of natural language processing\n(NLP) tasks such as machine translation and word embedding. Standard machine learning\nmodels succeed through training on corpora of text usually written by humans. However,\nthis training process results in human-like semantic biases in language models (Caliskan et\nal., 2017). Language models can not only embed bias, but have been shown in some NLP\ntasks to amplify existing bias (Zhao et al., 2017), a result which further underscores the\nimportance of detecting biases and requiring fairness.\nState-of-the-art natural language generation models exhibit such human-like biases. Sheng\net al. (2019) found that when given the prompts of “The man worked” and “The woman\nworked”, OpenAI’s GPT-2 (Radford et al., 2019) generated the sentences “The man worked\nas a car salesman at the local Wal-Mart” and “The woman worked as a prostitute under the\nname of Hariya.”\nDespite evidence of biases in NLG, less work exists on identifying fair NLG compared\nto on exploring fairness in other tasks such as word embedding. Font et al. (2019) used\nword embedding debiasing methods (Bolukbasi et al., 2016) to equalize gender bias in the\ntask of translation, which indicates that such a process can be similarly applied to language\ngeneration as well. However, gender biases can be recovered from debiased models, implying\nthat debiasing is not a ﬁx-all solution (Gonen et al., 2019).\nThese works suggest a need for a clearer framework of fairness which is ﬂexible toward\ndifferent notions of bias and which potentially allows for a way to evaluate and build fair\nNLG. Thus, in Section 2, we propose using the notion of individual fairness in order to\nbuild such a framework for deﬁning a fair language generation model. In Section 3, we use\nmethods from Bolukbasi et al. (2016) in conjunction with our proposed fairness deﬁnition to\ndemonstrate existing gender bias in GPT-2 as well as in Google’s XLNet (Yang et al., 2019).\narXiv:2008.01548v1  [cs.CL]  28 Jul 2020\n2\nTheoretical Framework\nThe goal of individual fairness under a classiﬁcation task is posed in Dwork et al. (2011) as\nachieving a classiﬁer which maps similar individuals to similar distributions of outcomes in\nclassiﬁcation, i.e. a mapping which satisﬁes the Lipschitz property. In particular:\nDeﬁnition 2.1. (Individual Fairness) A randomized classiﬁer C : U →∆(O) is individually\nfair with respect to D : ∆(O)×∆(O) →[0, 1] and d : U ×U →[0, 1] if for every u, v ∈U,\nD(C(u), C(v)) ≤d(u, v).\nHere, U is the universe of individuals being classiﬁed, O is the space of outcomes (which is\nsimply {0, 1} in binary classiﬁcation tasks), D is a measure of similarity between distribu-\ntions (eg. DTV ), and d is a given similarity metric between individuals.\nWe propose, as stated, incorporating this notion of individual fairness from Dwork et al.\n(2011) into the evaluation of fairness of language models. In particular, we posit that a fair\nlanguage generation system should output similar results given similar individual inputs; i.e.,\na fair sentence completer should return similarly biased sentences given similar prompts.\nDeﬁnition 2.2. (Fair Language Generation System) Given a measure of bias b : V →[0, 1]1,\na language generation system C : U →∆(V ) is fair with respect to d : U × U →[0, 1] if\nfor every u, v ∈U,\n|E [b(C(u))] −E [b(C(v))] | ≤d(u, v).\nHere, C is a language generation system which takes in a prompt and gives a distribution\nover outputs, U is the universe of prompts, V is the universe of outputs, and d is a given\nsimilarity metric between individual prompts (i.e. inputs to C). Note that b(C(u)) and\nb(C(v)) are distributions on [0, 1], so E [b(C(u))] captures the expected bias of C toward\ninput u (and input v in the analogous expression).\nRemarks. This deﬁnition of fairness in NLG is stated generally, but can be reﬁned towards\nformalizing fairness in the task of sentence completion. In other NLG tasks, such as\ngenerating text from data, the universe of prompts U is likely not the same as the output\nspace V . In sentence completion, however, it is possible to take U = V , where U represents\nthe space of possible texts which C can be prompted with and can output. Note furthermore\nthat in practice, there are several possible representations of U; for example, in the bag-\nof-words model, U = N|D|, where D is a dictionary of words and a sentence or phrase is\nrepresented by frequencies of words. (Note that in our evaluations of language models in\nSection 3, we still treat U and V separately.)\nFurthermore, this deﬁnition can be adapted to accommodate multiple dimensions of bias;\nfor example, if given b1 : V →[0, 1] and b2 : V →[0, 1] which capture different types of\nbiases (gender, race, regard, etc.), we can require a fair language generation system to be\nfair with respect to both bias metrics. In other words, we can write that under fair language\ngeneration, for every u, v ∈U,\n|E [b1(C(u))] −E [b1(C(v))] | ≤d(u, v);\n|E [b2(C(u))] −E [b2(C(v))] | ≤d(u, v).\nOne ﬁnal initial remark is on the utility of deﬁning fairness in language models as in\nDeﬁnition 2.2. Importantly, while this deﬁnition is useful for determining the fairness of\na language generation system, it can also be used in the process of building and training a\nfair language generation system, as individual fairness in Dwork et al. (2011) was used to\nspecify an algorithm for building a classiﬁer which involves maximizing utility subject to\nthe fairness constraint.\n1A bias b : V →[−1, 1], for example, can be normalized so its output lies in [0, 1].\n2\n2.1\nMeasuring Distance in the Input Space\nAs in Dwork et al. (2011), we must also consider how the similarity metric d should be\nconstructed. It is difﬁcult to give a general statement for what it means for two prompts to\nbe similar to each other. However, we can imagine, for example, that prompts which are\nidentical apart from a change of demographics of the subject should be considered as similar.\nThen, “He works as” and “She works as” are similar prompts and should result in similar\nsentences under a fair language generation model. As we will see in Section 3, even this\nsimple deﬁnition of similar sentences can be useful for evaluation.\nTo create a more comprehensive similarity metric d would be a complicated task, but previous\nworks on measuring distances in discrete text spaces suggest multiple options for analyzing\ntext similarity.\nBasic similarity measures involve matching words or phrases in the surface representation of\na text; in other words, these measures are purely lexical (Metzler et al., 2007). For example,\none possible measure is the Jaccard similarity coefﬁcient, which can be deﬁned in the context\nof text similarity by representing a text as an unordered set of its words (Achananuparp et\nal., 2008).\nHowever, purely lexical measures of similarity often fall short of capturing the text similarity,\ni.e. similarity in meaning, in prompts (Metzler et al., 2007). For example, while “woman”\nand “man” have lexical distance 1, so does “woman” and “potato”, but we know that “woman”\nand “man” are much more associated in meaning than “woman” and “potato”. Thus, an ideal\ndistance metric in the input space would also take into account semantic similarity. Measures\nof semantic similarity include structure-based measures, information content measures, and\nfeature-based measures (Slimani, 2013).\nIndeed, the similarity of the prompt pairs in Section 3 depend both on the semantic similarity\nof gender pairs like “woman” and “man,” as well as on the lexical similarity of prompts\nwhich differ by only one word. One direction for future work is to explore different measures\nof semantic similarity as distance metrics for our input space.\n2.2\nMeasuring the Bias of a Sentence\nCrucially, our framework of fairness also depends on a bias function b : V →[0, 1], which\ncalculates the bias of a completed sentence or, more generally, any output of C. What “bias”\nis in this context exactly is left intentionally unclear, as there are several interpretations of\nwhat the bias of a sentence should look like.\nFor example, Sheng et al. (2019) attempted to deﬁne bias through the concept of regard,\nwhere a sentence’s bias can be captured through the attitude it projects onto different\ndemographics.2 The idea that similar prompts should result in sentences with similar regard\nmakes sense; examples such as “The woman worked as a prostitute” and “The white man\nworked as a police ofﬁcer” show how differences in regard can capture the bias of a language\nmodel.\nHowever, Sheng et al. (2019) deﬁned regard to be only positive, neutral, or negative. For\nexample, the two sentences “XYZ was thought of as the most powerful man on Earth” and\n“XYZ was known for his love of books, and his love of writing” were both classiﬁed as\nhaving positive regard for XYZ, despite the ﬁrst sentence clearly projecting a more positive\nregard toward XYZ. Furthermore, “XYZ worked as a nurse” and “XYZ worked as a doctor”\nboth indicate positive regard for XYZ. Then, this notion of regard fails to capture the classic\nexample of gender bias where “She worked as a nurse” and “He worked as a doctor” are\ncommon completed sentences.\n2For example, a language generator generating “XYZ was a pimp” indicates that the language model\nhas negative regard for XYZ.\n3\nBolukbasi et al. (2016) introduced a method of identifying bias, with a focus on gender bias,\nin word embeddings. Speciﬁcally, in word embedding models such as Word2vec, a gender\nsubspace g can be identiﬁed through principal component analysis using gendered pairs such\nas he-she or boy-girl. Then, to evaluate gender biases in selected language models, we can\ndeﬁne the bias to be b(v) = ⃗w · g, where ⃗w is the word embedding of the profession in the\ncompleted sentence, and g is the gender subspace of a word embedding model as identiﬁed\nin Bolukbasi et al. (2016).\n3\nEvaluation and Results\n3.1\nResults with Original Word Embeddings\nIn evaluating the bias in language models, we focused on OpenAI’s GPT-2 (Radford et al.,\n2019) and Google’s XLNet (Yang et al., 2019). We constructed 8 unique preﬁx templates that\nwould generate sentences related to professions when completed with a gender demographic,\nfor example, “{She, He, The man, The woman} has a job as”. Then, we used GPT-2 and\nXLNet to generate 25 sample sentences per completed preﬁx template. From each sample,\nwe parsed the profession keyword. For example, given the preﬁx “She has a job as...”, we\nparse as follows:\nShe has a job as a lawyer and has two kids →lawyer\nThen, we measured the gender bias of the profession as described in Section 2, using the\ngender subspace from Bolukbasi et al. (2016). As discussed in Section 2.1, we can reason\nthat each of the four pairs of prompts (eg. “The woman works as...” and “The man works\nas...”) are similar, since they differ only in the demographic of the subject. Thus, under a fair\nlanguage model, we expect the biases of outputs across each pair to be similar.\nBias in Female Prompts\nBias in Male Prompts\nPreﬁx Template\nGPT-2\nXLNet\nPreﬁx Template\nGPT-2\nXLNet\nThe woman works as...\n0.0927\n0.1833\nThe man works as...\n-0.0059\n-0.0474\nShe works as...\n0.0834\n0.0430\nHe works as...\n-0.0055\n0.0152\nThe woman has a job as...\n0.1311\n0.0822\nThe man has a job as...\n0.0061\n-0.0142\nShe has a job as...\n0.0754\n0.0864\nHe has a job as...\n0.0423\n0.0259\nAverage\n0.0957\n0.0987\nAverage\n0.0092\n-0.0051\nTable 1: Bias measurements averaged over the 25 samples per preﬁx template.\nWe ﬁrst conducted this experiment using the original word2vec (Mikolov et al., 2013) word\nembedding model. On average, for both language models, the magnitudes of bias toward\nfemale prompts were much larger than the magnitudes of bias toward male prompts, as\nseen in Table 1 and Figure 1. This difference in bias toward similar prompts quantiﬁes the\nunfairness of the language generation model, under which male prompts generate more\ngender-neutral professions while female prompts generate more female-biased professions\nsuch as “housekeeper” and “prostitute”.\n3.2\nResults with Debiased Word Embeddings\nSo far, what we have described as the “gender bias” of a profession is effectively a measure\nof how gender-related the profession is. However, as in Bolukbasi et al. (2016), a distinction\ncan be drawn between, for example, stereotypical female-related professions, such as “maid”,\nand professions which are female-related by deﬁnition, such as “congresswoman”. Bolukbasi\net al. (2016) showed that word embeddings such as word2vec (Mikolov et al., 2013) exhibit\nbiases, meaning that the evaluation in Section 3.1 incorporates the biases found in word\nembeddings, eg. stereotypical biases, in its deﬁnition of gender bias.\n4\nFigure 1: Bias comparison for GPT-2 and XLNet for each preﬁx template.\nThen, a second possibility is to debias the word embedding, following the debiasing proce-\ndure presented in Bolukbasi et al. (2016), in an attempt to instead treat the bias measure b as\na gender score which maps profession words to how gender-related they are on a deﬁnitional,\nor ground truth, level.\nTable 2 shows the results of conducting the same experimental analysis on the debiased\nword2vec word embedding model.\nGPT-2\nXLNet\nFemale Prompts\n0.0401\n0.0426\nMale Prompts\n0.0183\n0.0180\nTable 2: Bias measurements averaged over the 25 samples per preﬁx template as grouped\nby female versus male prompts.\nFigure 2: Bias comparison for GPT-2 and XLNet with original and debiased word embed-\ndings for each preﬁx template.\nThese results are consistent with those in Section 3.1. As seen in Figure 2, biases toward\nfemale prompts exceed biases toward male prompts, though the differences in bias have\nsmaller magnitudes after debiasing the word embeddings.\n5\n4\nDiscussion and Future Work\nWe have constructed a theoretical framework for fairness that has allowed us to incorporate\nprevious work on gender bias in word embeddings to demonstrate bias in language generation\nmodels. Namely, we used the principle from individual fairness in classiﬁcation in Dwork et\nal. (2011) to propose a notion of fairness in NLG which requires similarly biased completed\nsentences from similar prompts. Then, we evaluated gender bias in OpenAI’s GPT-2\n(Radford et al., 2019) and Google’s XLNet (Yang et al., 2019) by building semantically\nsimilar prompts and by quantifying the gender bias of the completed sentences using the\ngender subspace as identiﬁed in Bolukbasi et al. (2016).\nThere are several directions for future work. One challenge would be to use this fairness\ndeﬁnition as a tool for building fair and useful language models. Importantly, a fair language\nmodel is not necessarily useful: for example, a sentence completer which generates the same\nsentence regardless of the prompt is fair by our deﬁnition. As in Dwork et al. (2011), an\nalgorithm to create fair NLG might then be posed as the process of optimizing utility under\na fairness constraint.\nAnother important direction of future work is to further quantify distance in the prompt input\nspace through consideration of different measures of semantic similarity, as well as to explore\nother ways to deﬁne the bias measure beyond the gender bias we explored. This area of\nwork might involve not only further exploration and testing of the utility of various similarity\nmeasures, but also a sociologically-motivated study of ethical NLG usage, especially for\ndeﬁning a bias metric.\nThis paper focused on contrasting gendered prompts (i.e. “woman”/“man”, “she”/“he”).\nHowever, both GPT-2 and XLNet treated “they,” which is often used as a singular gender-\nneutral pronoun, as a plural pronoun and hence did not generate singular professions when\ngiven prompts such as “They worked as”. Thus, we could not evaluate results as we did\nwith gendered prompts. An extension of our work in gender bias in NLG might explore and\naddress fairness for prompts involving gender-neutral and non-binary demographics.\nAcknowledgements\nWe thank Cynthia Dwork and Yonatan Belinkov for their support and helpful discussions.\nReferences\nPalakorn Achananuparp, Xiaohua Hu, and Xiajiong Shen. 2008. The Evaluation of Sentence Simi-\nlarity Measures. Data Warehousing and Knowledge Discovery: 10th International Conference,\nDaWaK 2008 Turin, Italy, September 2-5, 2008 Proceedings, 305–316.\nTolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016. Man is\nto Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. Advances\nin Neural Information Processing Systems, 4349–4357.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases. Science, 356(6334):183–186.\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel. 2011. Fairness\nThrough Awareness. ITCS ’12: Proceedings of the 3rd Innovations in Theoretical Computer\nScience Conference, 214–226.\nJoel Escudé Font and Marta R. Costa-jussà. 2019. Equalizing Gender Biases in Neural Machine\nTranslation with Word Embeddings Techniques. ArXiv, abs/1901.03116.\n6\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic\ngender biases in word embeddings but do not remove them. CoRR, abs/1903.03862.\nDonald Metzler, Susan Dumais, and Christopher Meek. 2007. Similarity Measures for Short\nSegments of Text. Proceedings of the 29th European Conference on IR Research, 16–27.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient Estimation of Word\nRepresentations in Vector Space. International Conference on Learning Representations.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask learners. OpenAI Blog.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The Woman Worked\nas a Babysitter: On Biases in Language Generation. Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing, 3407–3412.\nThabet Slimani. 2013. Description and Evaluation of Semantic similarity Measures Approaches.\nInternational Journal of Computer Applications, 80(10):25–33.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le.\n2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. Advances in\nNeural Information Processing Systems, 5753–5763.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men Also Like\nShopping: Reducing Gender Bias Ampliﬁcation using Corpus-level Constraints. Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Language Processing.\n7\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-07-28",
  "updated": "2020-07-28"
}