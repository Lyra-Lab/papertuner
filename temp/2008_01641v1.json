{
  "id": "http://arxiv.org/abs/2008.01641v1",
  "title": "Exploring Variational Deep Q Networks",
  "authors": [
    "A. H. Bell-Thomas"
  ],
  "abstract": "This study provides both analysis and a refined, research-ready\nimplementation of Tang and Kucukelbir's Variational Deep Q Network, a novel\napproach to maximising the efficiency of exploration in complex learning\nenvironments using Variational Bayesian Inference. Alongside reference\nimplementations of both Traditional and Double Deep Q Networks, a small novel\ncontribution is presented - the Double Variational Deep Q Network, which\nincorporates improvements to increase the stability and robustness of\ninference-based learning. Finally, an evaluation and discussion of the\neffectiveness of these approaches is discussed in the wider context of Bayesian\nDeep Learning.",
  "text": "EXPLORING VARIATIONAL DEEP Q NETWORKS\nA.H. Bell-Thomas\nComputer Laboratory\nUniversity of Cambridge\nAlexander.Bell-Thomas@cl.cam.ac.uk\nABSTRACT\nThis study provides a research-ready implementation of Tang and Kucukelbir’s Variational Deep Q Net-\nwork, a novel approach to maximising the efﬁciency of exploration in complex learning environments\nusing Variational Bayesian Inference, using the Edward PPL. Alongside reference implementations\nof both Traditional and Double Deep Q Networks, a small novel contribution is presented — the\nDouble Variational Deep Q Network, which incorporates improvements to increase the stability and\nrobustness of inference-based learning. Finally, an evaluation and discussion of the effectiveness of\nthese approaches is discussed in the wider context of Bayesian Deep Learning.\nIntroduction\nThe balance between state-space exploration and exploiting known effective actions is one of the funda-\nmental problems in learning against complex environments. A promising approach to tackling this is by\nbuilding an approximation of the potential value of exploration — this can be achieved by using Bayesian\nmethods on agents’ uncertainty about the optimality of the current policy. This concept was ﬁrst introduced\nover 20 years ago by Dearden et. al., [1] and has been explored in numerous works since. The focus of this\nstudy shall be the application of Variational Bayesian Inference [2] in this domain, primarily inspired by the\nwork of Tang and Kucukelbir. [3]\nThis paper accompanies an open-source reference implementation1 of all four algorithms discussed, one of\nwhich is a novel contribution. Further, it extends the evaluation and discussion contributed by the original\nauthors, including commentary on concepts discussed in works published since.\n1\nBackground\nBased on Markov Decision Processes (MDPs), Reinforcement Learning (RL) is one of the most fundamental\nmodern machine learning techniques. RL is, at its core, a stochastic optimisation approach for unknown\nMDPs — it observes the state of the environment and takes action according to the current policy it holds.\nThe goal is simple: maximise the reward gained, as deﬁned by the model’s reward function, by choosing the\nbest action from any given state.\nQ(s, a) : S × A →R\n(1)\n1https://github.com/HarriBellThomas/VDQN\narXiv:2008.01641v1  [cs.LG]  4 Aug 2020\nAn optimal policy (2) is deﬁned as a policy maximising the discounted cumulative reward from any state.\nπ∗= arg max\nπ\nEσ∼π(·|s0)\nh\n∑\nt∈σ\nQπ(st, at) · γt i\n(2)\nWhere γ is the discount constant, 0 ≤γ ≤1, and σ is the sequence of actions and states chosen by the\npolicy, π; this deﬁnes a single episode.\nQ Learning\nQ Learning is a model-free RL approach introduced by Watkins [4]. In additional to learning π∗, it concur-\nrently iteratively learns an optimal reward function, Q∗, for the problem domain (3). α is the learning rate,\n0 < α ≤1.\nQnext(st, at) ←(1 −α) ∗Q(st, at) + α ∗(rt + γ ∗arg max\na\nQ(st+1, a))\n(3)\nAn error function is required to guide both the policy and reward functions to optimal forms — for this\nBellman’s equations are used. [5] The Bellman error is a measure of the decay in expectation of future\nadvantage as an agent executes an action under policy π (4). Conceptually, J(π) measures the expected loss\nin reward earning-potential as the agent progresses through an episode. It follows that a pure optimal policy,\nπ∗, will conserve earning potential (have zero Bellman error) across agents’ actions (5).\nJ(π) = Eat∼π(·|st)\nh\u0000Qπ(st, at) −arg max\na\nE[rt + γ · Qπ(st+1, a)]\n\u00012i\n(4)\nQ∗(st, at) = arg max\na\nE\nh\nrt + γ · Q∗(st+1, a)\ni\n(5)\nDeep Q Networks (DQN)\nDeep RL extends traditional RL methods by leveraging a neural network (NN) to approximate the policy\nπ. This neural policy, πθ, where θ represents the parameter vector of the policy NN, can be updated directly\nusing differentiation (6).\nθ ←θ + α · ∇θ · J(θ)\n(6)\nIn practicality, the loss function is discretised into N samples. Additionally, as introduced by Mnih et. al. in\nthe seminal DQN paper, [6] it is common to maintain two networks concurrently — and active network, Qθ,\nand a target network, Qθ−. Both networks have identical architectures, but the target is updated more slowly\nthan the active one to minimise perturbations in performance; this is effectively a debouncing method. This\ngives (7), the approximate Bellman error for a DQN.\nJ(θ) ≈1\nN\nN\n∑\nj=1\n(Qθ(sj, aj) −arg max\na′\n(rt + γ · Qθ−(s′\nj, a′)))2\n(7)\n2\nVariational Deep Q Networks (VDQN)\nIntroduced by Tang and Kucukelbir, [3] the Variational DQN exploits an equivalence between a surrogate\nobjective (11) and variational inference loss.2 Taking interaction with the environment as a generative model,\nVDQNs construct prior and posterior distributions on θ, the network’s parameters, with respect to recorded\nobservations, D.\nPrior →p(θ)\n(8)\nPosterior →p(θ|D) = p(θ, D)\np(D)\n(9)\nAs explained in [7], calculating the posterior is generally extremely challenging. The core concept un-\nderpinning variational methods is the selection of a family of distributions over the latent variables with\nits own variational parameters; here qφ(θ) is used to approximate the posterior. This has now become an\noptimisation problem over a class of tractable distributions, q, parameterised by φ, in order to ﬁnd the one\nmost similar to p(θ|D). This is iteratively solved using gradient descent on the Kullback-Leibler divergence\nbetween approximate and true posterior distributions (10), and is suitable for use as a surrogate posterior\ndistribution. [2, 8, 9]\nφ ←min\nφ\nKL(qφ(θ) ∥p(θ|D))\n(10)\nA valuable insight contributed by Tang and Kucukelbir is that in order to encourage efﬁcient exploration\nqφ(θ) must have a sufﬁciently large entropy. Thus the goal of a VDQN is to ﬁnd φ that minimises Bellman\nloss across a highly dispersed set of candidate policies. With λ > 0, a regularisation constant, a VDQN’s\nobjective function is expressed as follows.\nEθ∼qφ(θ)\nh\u0000Qθ(sj, aj) −max\na′\nE[rj + γQθ(s′\nj, a′)]\n\u00012i\n−λH(qφ(θ))\n(11)\nThe debouncing method from DQNs is implemented in a similar way here: the active network is parame-\nterised by θ ∼qφ(θ) and the target network by θ−∼qφ−(θ−).\nDouble Q Learning\nQ-Learning occasionally performs poorly in some stochastic environments; this is ascribed to large overes-\ntimation of action rewards in training. To remedy this, van Hasselt proposed Double Q Learning: [10, 11]\nthere are two different approaches (one per paper) — this study uses the latter. [11] The concept is relatively\nsimple — to discourage action overestimation incorporate the target network as an infrequently changing\nframe of reference. To achieve this the Q function update operation (previously deﬁned in (3)) is changed as\nfollows. Note that the arg max function is over the target network instead of the active one.\nQnext(st, at) ←(1 −α) ∗Q(st, at) + α ∗(rt + γ ∗arg max\na\nQ′(st+1, a))\n(12)\n2David Blei has published an excellent introduction brieﬁng on the underlying mathematical concepts of variational\ninference. [7]\n3\nFigure 1: MountainCar-v0 — VDQN VI loss during training.\n2\nPrototype\nThe open-source implementation that accompanies this report implements four Q learning implementations;\nDQN, Double DQN, VDQN, and Double VDQN. Double VDQN (DVDQN) is a novel extension of the original\nVDQN design, adapting Mnih et. al.’s stabilisation techniques to improve observed volatility of the variational\ninference approach.\nDouble Variational Deep Q Networks\nAs the Evaluation section will show in detail, the results seen from the VDQN are highly promising. They\nclearly show that the new approach does appear to provide highly efﬁcient state-space exploration, as claimed.\nFigure 1 is a plot of observed VI loss3 whilst training the model in the OpenAI Gym’s MountainCar-v0\nenvironment; the VDQN’s perturbations are caused by the inference model ’jumping’ to correct itself. This\nbehaviour is reminiscent of the erraticism DDQN aimed to tackle.\nThe hypothesis held when this study commenced was that the root cause of this erratic behaviour is the\nintroduction of the −λH(qφ(θ)) bias term. It, just as seen with the vanilla DQN’s arg max term, is a guiding\napproximation that could be susceptible to over-approximating its true value. Thus an approach close to\nthat employed by DDQNs is used — DVDQNs manage the speed of updates to φ, the variational parameter,\nto indirectly have the same effect as (12); importantly, both the entropy of the variational family and the\nqualitative behaviours of the sampled set of θ parameters (and therefore the policies πθ) are inﬂuenced. This\ntechnique is implemented using a modiﬁed form of (12) to update the posterior distribution of the inference\nmodel.\nImplementation\nAll four algorithms are implemented using the same technologies in a uniﬁed Python framework; using\nthe framework and reproducing the experiments described in this report will be discussed in Appendix A.\nThe framework uses Tensorﬂow, Chainer, and the Edward probabilistic programming library. All models\n3VI loss is the delta between the prior and approximate posterior distributions after training; lower is better.\n4\n...\n......\n...\n...\nI1\nI2\nI3\nIx\nH1,1\nH1,n\nH2,1\nH2,n\nO1\nOy\nFigure 2: The dual hidden layer design used by all four algorithms.\nuse the same network structure (Figure 2) — two equally-sized fully-connected hidden layers.4 The frame-\nwork currently only supports a subset of OpenAI Gym’s environments due to varying state/action space\nrepresentations — Box(x,) state spaces and Discrete(y) action spaces are supported.5\nDQN / DDQN Implementation\nThe reference non-variational algorithms were implemented primarily\nusing the Chainer library.6 A rectiﬁed network design is used (the activation function is build from ReLU\ncalls), and both use the traditional ϵ-greedy technique (13) to facilitate early episode exploration [12] (with a\nconﬁgurable linear decay (1.0 →0.1) period of 30 episodes).\nπ(s) =\n(\nmaxaQ(s, a),\nwith probability 1 −ϵ\nrandom action,\notherwise\n(13)\nVDQN / DVDQN Implementation\nThe neural network structures for the variational models are con-\nstructed manually using a combination of TensorFlow and Edward to conform to their respective designs.\nAgain, the neural networks used are rectiﬁed. The generative model built to represent the environment uses\na Gaussian distribution centered on a greedily chosen action from the current Qθ function ((14), σ = 0.01\nby default). As suggested in the original paper, an improper uniform prior7 is used. The structure for the\nBayesian neural network was inspired by the Edward whitepaper, [13] which describes how to initialise all of\nthe required components, and the original VDQN paper.\ndi ∼N(Qθ(s, a), σ2)\n(14)\nA note on the Edward library: the original plan for this project had been to use the new Edward2 library8\nwith TensorFlow v2 but there were a number of blocking problems. At the time of writing Edward2 has no\nstable/release version and currently lacks fully-featured inference functionality. The upgrade guide suggests\nbuilding a trace update workﬂow manually, but given the complexity of the inference model required by\nthis project this was deemed to be too far out of scope to be viable. Instead, Edward (v1.3.5)9 and the most\nrecent version of TensorFlow it supports (v1.4.1) are used — this version of TensorFlow is also used for the\nnon-variational implementations to ensure fairness.\n4The experiments performed for this report used 100 nodes per layer.\n5More information about this can be found at: https://github.com/openai/gym/wiki/Table-of-environments.\n6https://github.com/chainer/chainer\n7A uniform distribution with an integral not necessarily equal to 1.\n8https://github.com/google/edward2\n9http://edwardlib.org/\n5\n3\nEvaluation\nA set of experiments from OpenAI’s Gym10 were used to assess the performances of the different algorithms.\nThe original VDQN paper gave a very brief look at the performance in 4 control problems; Figure 3 recreates\nthis, and Figures 4 and 5 extends it. For each algorithm in each experiment the highest performing parameter\nset has been plotted. All graphs have been placed at the end of the section for clarity.\n3.1\nDiscussion of Results\nOn simpler tasks (Figure 3), such as the CartPole experiments, the variational approaches make progress\nmuch faster, but, as clear in CartPole-v1, they take longer to converge on the optimal policy. As Tang and\nKucukelbir point out, this is likely caused by the hindrance of the entropy bonus term as it encourages\nexploration even when a suspected optimal policy has been found.\nIn these simple control tasks, DVDQN performs just as well or better than VDQN; improvements are made\nfaster, and in some cases the ﬁnal policy found is more optimal. Figure 5 tells an even more interesting story\n— these types of visualisation was omitted from the original VDQN paper. The most crucial improvement\nDVDQN appears to bring is a more stable and improved performance in VI training loss. VI loss can be\nconceptually thought of as the delta between the prior and approximate posterior distributions after training;\ndecreasing VI loss implies convergence. DVDQN appears to both converge at a faster speed and exhibit a\nmuch lower variance; VDQN’s peaks, representing sudden, unexpected divergence, are signiﬁcantly reduced\n— this was the primary goal of DVDQN.\nTo complement the four simple control tasks in Figure 3, all four algorithms were trained against two of\nOpenAI Gym’s Atari game environments — SpaceInvaders-v0 and Pong-v0. Only 100 episodes were run\nfor each to try to capture how effective each algorithm’s exploration strategy is in the environment.\n1. SpaceInvaders-v0 — in almost every episode DVDQN is able to accrue the greatest reward out of\nthe four algorithms, with VDQN easily able to match the performance of the two non-variational\nalgorithms.\n2. Pong-v0 — the variational algorithms appear to fail to convert exploration to any meaningful reward,\nin contrast to non-variational ones which, although not close to being optimal, occasionally win\none point. The VI loss curves (not presented here) appear to neither converge nor diverge; minimal\nprogress appear to be made, which may explain the poor performance.\nRandomised Prior Initialisation\nOsband et. al. [14] make an incredibly interesting argument about the\nunsuitability of randomised prior functions. Lemma 2 in their paper points out the following:\nLet Y ∼N(µY, σY).\nIf we train X ∼N(µ, σ) according to mean squared error\nµ∗, σ∗∈arg min\nµ,σ\nE\n\u0002(X −Y)2\u0003\nfor X, Y independent\nthen the solution µ∗= µY, σ∗= 0 propagates no uncertainty from Y to X.\nThis, unfortunately, does appear to encompass the prior function used by VDQNs — the randomised\nuniform prior initialisation is independent to the (assumed) Gaussian distribution of the variational family.\nThe consequences of this aren’t clear, as as far as was expected the variational algorithms appear to work well.\nFurther investigation is undoubtedly required, but this could explain the occasional failure of the inference\nalgorithm to converge.\n10https://gym.openai.com/\n6\nRelative performance\nDQN\n1.000 ± 0.000\nDDQN\n0.859 ± 0.252\nVDQN\n0.133 ± 0.029\nDVDQN\n0.110 ± 0.031\nTable 1: Relative performance of each algorithm against DQN. The error values are ±s, the sample std. dev.\nComparisons are made using the number of iterations per second the implementation is able to achieve in\neach environment.\n3.2\nEdward\nOne of the questions this study aimed to investigate was how impactful the use of Edward in an RL context\nwould be. Table 1 presents the relative training times of each algorithm, using the number of iterations (steps\ntaken by an actor) per second as an indicator. The training time required for these implementations of the\nvariational algorithms are ∼8x those for the non-variational. This isn’t entirely surprising — DQN/DDQN\nare implemented using a fairly standard Chainer design, and as such should be expected to be more\nheavily optimised than a manually wired Bayesian neural network. Although this may not seem entirely\nfair, it is important to note that beyond optimising the Edward library itself there is not much scope for\nimprovements; a large portion of the additional running time is used to run the inference engine. Translating\nthe VDQN/DVDQN implementations to use Pyro11 instead of Edward would help isolate bottlenecks, and\nprovide a benchmark for evaluating both in the context of Deep RL. To ensure fairness, every experiment ran\nusing 2 threads and 3 GB RAM each.\n3.3\nAvenues for Future Work\nThe question raised by Osband et. al. about the validity of the initial prior distribution used will be\nimportant to resolve. They present a number of interesting ideas in their paper [14] which could be used\nin lieu, particularly the introduction of γ-discounted temporal difference loss (15) — this could be a viable\nreplacement for Bellman error. The target and active Q functions are generated from a combination of the\nvariational families and a new prior function, p; a full explanation can be found in their paper.\nL(qφ, qφ−, p, D) = ∑\nt∈D\n \nrt + γ arg max\na′\n(qφ−+ p)(s′\nt, a′) −(qφ + p)(st, at)\n!2\n(15)\nThere are a number of improvements to that framework and additional experiments to develop understand-\ning about VDQNs and DVDQNs further. For example, adapting the framework to accept all OpenAI Gym\nwill open up a much wider range of evaluation contexts, such as the more advanced Atari games or Robotics\nsimulators. This study explores a handful of the models’ parameters, but a number of the VDQN/DVDQN\nparameters could be tuned to improve performance further.\n1. τ — this implementation supports Polyak averaging [16] for model updates, though this study only\nused τ = 1 (hard-copy updating).\n2. σ — the standard deviation of the Gaussian created for selecting the next greedy action to take.\n3. γ — the learning rate. Although a number were tested in this study’s experiments, the behaviours of\nthe variational and non-variational variants were very different.\n4. The number of hidden layers — the effect of changing the size of the neural network structure is\ncurrently completely unknown. n = 100 layers were used here.\n11https://pyro.ai, [15]\n7\n(a) CartPole-v0\n(b) CartPole-v1\n(c) Acrobot-v1\n(d) MountainCar-v0\nFigure 3: Rewards observed during training for all four algorithms.\nError bounds are ±s, the sample std. dev.\n8\n(a) SpaceInvaders-v0\n(b) Pong-v0\nFigure 4: Training curves observed for 2 complex OpenAI Gym Atari game environments.\nError bounds are ±s, the sample std. dev.\n9\n(a) Acrobot-v1 — VI Loss\n(b) Acrobot-v1 — Bellman Error\n(c) MountainCar-v0 — VI Loss\nFigure 5: Variational training losses observed during training on selected tasks.\nError bounds are ±s, the sample std. dev.\n10\n4\nConclusion\nVariational Bayesian interpretations of Deep Q Learning algorithms have proved to be promising, appearing\nto deliver efﬁcient state-space exploration of complex environments. This study has provided a research-ready\nframework for using these approaches, as well as DVDQN, an effective optimisation technique and small\nnovel contribution to the ﬁeld. Although there are a number of interesting scenarios and promising parameter\nconﬁgurations left unexplored, a number of key, previously unseen, observations have been shown. These\nwill, hopefully, provide inspiration and motivation for further exploration in this area.\nReferences\n1R. Dearden, N. Friedman, and S. Russell, “Bayesian Q-Learning”, in Proceedings of the Fifteenth Na-\ntional/Tenth Conference on Artiﬁcial Intelligence/Innovative Applications of Artiﬁcial Intelligence, AAAI\n’98/IAAI ’98 (1998), pp. 761–768.\n2D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational Inference: A Review for Statisticians”, Journal\nof the American Statistical Association 112, 859–877 (2017).\n3Y. Tang and A. Kucukelbir, Variational Deep Q Network, 2017.\n4C. Watkins and P. Dayan, “Technical Note: Q-Learning”, Machine Learning 8, 279–292 (1992).\n5R. Bellman, “The theory of dynamic programming”, Bull. Amer. Math. Soc. 60, 503–515 (1954).\n6V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, Playing Atari\nwith Deep Reinforcement Learning, 2013.\n7D. Blei, “Variational Inference”, cs.princeton.edu (2011).\n8A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M. Blei, Automatic Differentiation Variational\nInference, 2016.\n9R. Ranganath, S. Gerrish, and D. M. Blei, Black Box Variational Inference, 2013.\n10H. V. Hasselt, “Double Q-learning”, in Advances in Neural Information Processing Systems 23, edited by J. D.\nLafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (Curran Associates, Inc., 2010),\npp. 2613–2621.\n11H. van Hasselt, A. Guez, and D. Silver, Deep Reinforcement Learning with Double Q-learning, 2015.\n12R. S. Sutton and A. G. Barto, “Reinforcement learning: An introduction”, (2011).\n13D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei, Edward: A library for probabilistic\nmodeling, inference, and criticism, 2016.\n14I. Osband, J. Aslanides, and A. Cassirer, Randomized Prior Functions for Deep Reinforcement Learning, 2018.\n15E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer, N. Pradhan, T. Karaletsos, R. Singh, P. Szerlip, P.\nHorsfall, and N. D. Goodman, Pyro: Deep Universal Probabilistic Programming, 2018.\n16B. Polyak and A. Juditsky, “Acceleration of Stochastic Approximation by Averaging”, SIAM Journal on\nControl and Optimization 30, 838–855 (1992).\n11\nA\nReproducing the Experiments\nThe evaluation framework developed alongside this report is readily available on GitHub, and has been\nfully tested on both macOS and Linux.\nhttps://github.com/HarriBellThomas/VDQN\nThe next few steps assume a fresh Ubuntu installation is being used. The process may deviate slightly if a\ndifferent system is used. A helper script is included for installing the required dependencies on Linux; the\nprocess is near-identical for macOS.\ngit clone https://github.com/HarriBellThomas/VDQN.git\ncd VDQN\n./init.sh\nsource env/bin/activate\nAll Python dependencies are installed and managed inside a Python virtual environment; this helps keep\nthe system clean, as we will be using some slightly older versions of standard libraries.\nThere are four main ﬁles to be aware of:\n1. run.py — this is the main entry point for running a single instance of one of the four algorithms. It\naccepts a number of CLI arguments for conﬁguring the parameters it used.\n2. DQN.py — this is the source ﬁle containing the implementations for both DQN and DDQN.\n3. VDQN.py — this is the source ﬁle containing the implementations for both VDQN and DVDQN.\n4. drive.py — this script is the driver used for running experiments at scale. It constructs a collection\nof 80 experiments, and iteratively loops through them.\nThe run.py script can be used as follows:\npython3 run.py --algorithm DQN|DDQN|VDQN|DVDQN \\\n--environment CartPole-v0 \\\n--episodes 200 \\\n--timesteps 200 \\\n--lossrate 1e-2\n12\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-08-04",
  "updated": "2020-08-04"
}