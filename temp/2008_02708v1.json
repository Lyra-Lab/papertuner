{
  "id": "http://arxiv.org/abs/2008.02708v1",
  "title": "Deep reinforcement learning to detect brain lesions on MRI: a proof-of-concept application of reinforcement learning to medical images",
  "authors": [
    "Joseph Stember",
    "Hrithwik Shalu"
  ],
  "abstract": "Purpose: AI in radiology is hindered chiefly by: 1) Requiring large annotated\ndata sets. 2) Non-generalizability that limits deployment to new scanners /\ninstitutions. And 3) Inadequate explainability and interpretability. We believe\nthat reinforcement learning can address all three shortcomings, with robust and\nintuitive algorithms trainable on small datasets. To the best of our knowledge,\nreinforcement learning has not been directly applied to computer vision tasks\nfor radiological images. In this proof-of-principle work, we train a deep\nreinforcement learning network to predict brain tumor location.\n  Materials and Methods: Using the BraTS brain tumor imaging database, we\ntrained a deep Q network on 70 post-contrast T1-weighted 2D image slices. We\ndid so in concert with image exploration, with rewards and punishments designed\nto localize lesions. To compare with supervised deep learning, we trained a\nkeypoint detection convolutional neural network on the same 70 images. We\napplied both approaches to a separate 30 image testing set.\n  Results: Reinforcement learning predictions consistently improved during\ntraining, whereas those of supervised deep learning quickly diverged.\nReinforcement learning predicted testing set lesion locations with 85%\naccuracy, compared to roughly 7% accuracy for the supervised deep network.\n  Conclusion: Reinforcement learning predicted lesions with high accuracy,\nwhich is unprecedented for such a small training set. We believe that\nreinforcement learning can propel radiology AI well past the inherent\nlimitations of supervised deep learning, with more clinician-driven research\nand finally toward true clinical applicability.",
  "text": " \nTitle:  \nDeep reinforcement learning to detect brain lesions on MRI: a proof-of-concept application of \nreinforcement learning to medical images \n \nAuthors: \nStember JN1*, Shalu H2 \n \n*Corresponding author \n \n1. Memorial Sloan Kettering Cancer Center \nDepartment of Radiology \n1275 York Avenue \nNY, NY 10065 \n \n2. Indian Institute of Technology Madras \nDepartment of Aerospace Engineering \nIIT P.O., Chennai 600 036 \nINDIA \n \nAbstract \n \nPurpose \n \nArtificial intelligence (AI) in radiology is hindered chiefly by: 1) Requiring large annotated data \nsets. 2) Non-generalizability that limits deployment to new scanners / institutions. And 3) \nInadequate explainability and interpretability, these being critical to foster the trust needed for \nwider clinical adoption.  \n \nWe believe that Reinforcement Learning can address all three shortcomings, with robust and \nintuitive algorithms trainable on small datasets. We in fact feel that reinforcement learning will \nhelp to usher radiology AI beyond its infancy and into true clinical applicability, while \nbroadening its research horizons. To the best of our knowledge, reinforcement learning has not \nbeen directly applied to computer vision tasks for radiological images. In this proof-of-principle \nwork, we train a deep reinforcement learning network to predict brain tumor location. \n \nMaterial and Methods \n \nUsing the BraTS brain tumor imaging database, we trained a deep Q network (DQN) on 70 post-\ncontrast T1-weighted 2D image slices. We did so in concert with image exploration, with \nrewards and punishments designed to localize lesions.  \n \nTo compare with supervised deep learning, we trained a keypoint detection convolutional \nneural network on the same 70 images. We applied both approaches to a separate 30 image \ntesting set.  \n \nResults \n \nReinforcement learning predictions consistently improved during training, whereas those of \nsupervised deep learning quickly diverged. Reinforcement learning predicted testing set lesion \nlocations with 85% accuracy, compared to roughly 7% accuracy for the supervised deep \nnetwork.  \n \nConclusion \n \nReinforcement learning predicted lesions with high accuracy, which is unprecedented for such a \nsmall training set. We have thus illustrated some its tremendous potential for radiology. Most \nof the current radiology AI literature focuses on incremental improvements in supervised deep \nlearning. We believe that reinforcement learning can propel radiology AI well past this \ninherently limited approach, with more clinician-driven research and finally toward true clinical \napplicability.   \n \n \n \nIntroduction \n \nOver the past few years, artificial intelligence (AI) and deep learning have gained traction and \nubiquity in radiology research. Indeed, the volume of research in this field has grown \nexponentially (1–3).  \n \nHowever, significant hurdles remain, and the key to overcoming them may lie in areas of AI \nhitherto unexplored in radiology. A 20,000-foot view of deep learning is in order: most \nradiology AI up until now has been in the realm of supervised deep learning (SDL). In SDL, large \nnumbers of explicitly labeled data are used to train convolutional neural networks (CNNs). \nThese trained networks can then make predictions on new unlabeled data, often classification, \nsemantic segmentation or localization tasks.  \n \nSDL is complemented by unsupervised deep learning, which seeks to cluster data without pre-\nlabeling by an imaging expert. The goal in unsupervised deep learning is to uncover \ncommonalities and differences in data, without overt data labeling or annotation. Unsupervised \ndeep learning is more of an exploratory procedure; typically, the researcher does not initially \nknow what s/he is looking for specifically, and no goal can be provided to the algorithm. \nUnsupervised learning is a small but growing portion of the landscape (4,5). However, SDL \ncontinues to constitute the vast majority of current radiology deep learning research. \n \nSDL suffers from the following three major limitations:  \n1. It requires tremendous volumes of expertly hand annotated data, which is time-consuming \nand tedious.  \n2. It is unstable in the sense of being exquisitely sensitive to even subtle differences in image \nquality. Goodfellow et al. showed that adding just a small amount of imperceptible image noise \ncan throw off the predictions of SDL (6). As such, a network trained on a given patient \npopulation with certain CT or MRI scanner settings at a particular institution in general fails to \nperform acceptably well when deployed on a new scanner and/or different institution with \ndifferent patient population.  \n3. SDL suffers from a notorious “black box” phenomenon (7,8). Gaining the trust of people, \nparticularly in a field as critical as health care, requires that the algorithms helping to make \ndecisions do so in a transparent manner. Understanding at least part of the rationale for why AI \nalgorithms make certain determinations fulfills this basic need of both patients and clinicians \nthat is required before widescale adoption is possible (9,10).  \n \nIn our search to overcome these hurdles, we note that a third major field of AI research has \nalready produced astounding results in applications as varied as board games, automated video \ngame playing and robotics. This AI field is called reinforcement learning (RL), and it is the focus \nof the present work. RL lies somewhere in between supervised and unsupervised deep learning, \nyet is in important ways distinct from both (5). Notably, RL was a key contributor to AlphaGo’s \nvictory over the European champion in the game of Go, a major breakthrough given the game’s \ninherent complexity (11). Additional advances that would have been unfeasible with supervised \nor unsupervised approaches include world champion level autonomous video game playing \n(12–14) and in the field of robotics (15,16).  \n \nRL can address the three aforementioned drawbacks of SDL:  \n1. The way RL learns about the environment / image includes much of what is not the desired \nlabel or structure of interest. As such, many fewer labeled images are needed for RL to learn \nenough to apply to new images.  \n2. Because of this learning of the “non-answer,” as opposed to SDL’s reliance on labeled correct \nanswers, RL is robust to inevitable noise and variations in image acquisition techniques and \npatient populations. This has to do with how RL works; goals are not provided explicitly as in \nSDL, but rather implicitly through a system of rewards. This process turns out to provide a more \nrobust and generalized kind of learning during training.  \n3. The reward system also provides vital intuition that is lacking in SDL. Reward structures \nprovide the rationale for why an algorithm makes certain predictions. It also opens \nopportunities for imaging medical specialists such as radiologists and pathologists to exploit \ntheir domain knowledge to help craft algorithms.  \n \nDespite the promise, RL has not as of yet, to the best of our knowledge, been applied directly to \ncomputer vision in radiological / medical images. Coming closest, in the field of computer vision \nmore generally, Wang and Sarcar were able to accurately segment non-radiological images \nusing RL with simulated pen drawings (17).  \n \nWe seek in this work to demonstrate proof-of-principle application of RL to radiology. Our \napplication is to detect brain tumors from 2D MRI images. This will lead to more sophisticated \nimplementations for image classification, object detection and segmentation. We believe that \nRL will ultimately far exceed the accomplishments of SDL. We anticipate further that as RL in \nradiology advances in subsequent work, it will finally begin to demonstrate, in a paradigm-\nshifting manner, the truly enormous potential of AI to fundamentally improve the efficiency \nand success of clinical image interpretation.  \n \nBefore describing the application of RL to our detection problem, we very briefly introduce the \napproach, starting with its history. RL gradually came into being through two disparate threads \nof research: behavioral psychology and engineering control theory. The notion of rewards and \npunishments engendering learning has a long history in psychology, originally formulated by \nThorndike in 1911 based on animal studies (18) as the “Law of Effect.” Minsky (19) and Bellman \n(20–22) provided the foundational control theory formulations in the 1950s. These two threads \ncoalesced during a revival of the field in the 1980s–1990s. It was in this era that most of the \nmodern RL concepts were developed by Barto and Sutton (23–27). More recently, the \nintroduction of deep convolutional neural networks has produced deep reinforcement learning \n(DRL). The associated deep Q-networks (DQNs) provide functional approximations that can \nultimately allow for optimal actions to be taken. For example, an early success for DRL was \nchampion-level performance on Atari games (12) because it allowed for the autonomous game-\nplayer to select the optimal action at every step of the game based purely in pixelwise input. \nWe employ a DRL formulation in the present work.  \n \nMethods \n \nConcepts and terms \n \nIn order to describe the method, we need to very briefly introduce some key concepts from \nRL/DRL. We will do so in the context of our particular system and approach. For the interested \nreader, much more detailed and didactic treatments of RL and DRL are available elsewhere, for \nexample the textbook by Barto and Sutto (28).  \n \nRL focuses on the process of learning from an environment (28,29). This type of learning is \nguided by the experiences of an agent, which is able to take certain actions within the \nenvironment. Depending on the action and particular state, the agent receives certain rewards. \nOnce one has specified the environment, states and possible actions, a careful selection of the \nrewards can guide learning to fulfill a desired task. The goal of the algorithm, or agent, is to \nachieve the maximum cumulative reward. This can be at the cost of short-term gains. \n \nMore formally defining our terms: \n-Environment: This is the physical world in which the agent operates and with which it \ninteracts. In our case, the environment consists mostly of the 2D slice of a post-contrast T1-\nweighted brain MRI containing a glioblastoma multiforme (GBM) lesion. In order to make the \nproblem of finding this lesion more tractable, we add to our environment the set of (x,y) \npositions looked at by a radiologist at an earlier time during simulated image interpretation, as \nrecorded by eye tracking hardware and software (30–34). This set of (x, y) coordinates within \nthe image is referred to as the gaze plot. Hence the full environment here is the 2D image with \noverlaid gaze plot for that image.  \n-Agent: entity that takes actions in an environment and receives rewards. In our case the agent \nis a moving point sitting on a pixel that we ultimately hope will land within the tumor, thereby \npredicting its position. \n-State: the state conveys the agent’s current situation. In our case the state is where in the \nimage our point resides, i.e. on which pixel it is currently sitting. In order to decrease the state \nspace, or possible ways the point can move as it seeks to find the lesion, we restrict our agent \nto move only along the gaze plot, i.e. among pixels that were looked at by the radiologist. The \nassumption is that during the prior simulated image interpretation, the radiologist (JNS, with \ntwo years of experience in neuroradiology) looked at the lesion at least once. Hence, by moving \nalong gaze points, which constitutes a one-dimensional space, the agent is certain to intersect \nthe lesion. How the agent can move between states is illustrated in Figure 1.  \n-Action: a change in the state. In our case, action is our point moving between pixels in the \nimage, more specifically between points on the gaze plot. \n-Policy: the prescription for which action to take in a given state. The goal of RL/DRL training is \nto produce an optimal policy. In our case, the optimal policy is for the agent to move toward \nthe tumor as quickly as possible and stay there to mark / predict that lesion. \n-Value: the cumulative future reward that the agent receives by taking a given action in a \nparticular state. In our system, moving toward the lesion has high value, whereas moving away \nfrom it has low value. \n \nIt should be noted that our environment satisfies the property of being a Markov Decision \nProcess. Markov Decision Processes are environments in which essentially all RL problems can \nbe formulated. Their chief attribute is lack of prior knowledge of environment dynamics.  \n \nRewards \n \nWe build our reward system so as to align the agent’s goal of maximizing cumulative reward \nwith our objective to detect brain lesions. We wish to incentivize reaching the pixels in the \ntumor and then staying within the tumor, and de-incentivize staying still outside the tumor or \nmoving away from the tumor. To this end, the reward 𝑅 is defined by: \n \n \n𝑅=\n⎩\n⎪\n⎨\n⎪\n⎧\n+2, if agent is within lesion and staying still \n−4, if agent is outside lesion and staying still \n       +0.5, if agent is within lesion and moving backward\n          −1.5, if agent is outside lesion and moving backward\n  +0.5, if agent is within lesion and moving forward\n    −0.5, if agent is outside lesion and moving forward\n \n \n       (1) \n \nAs stated above, we restrict our agent’s possible positions to being along the 1D gaze plot. We \ndo so in order to decrease the state and action space and simplify learning calculations. We \ndefine the anterograde direction as being toward the final point in the gaze plot, which is the \nlast point at which the radiologist had looked for that 2D image. Retrograde is toward the initial \npoint, looked at first by the radiologist.  \n \nActions \n \nFrom the current state / gaze point, we define 3 possible actions that the agent can take:  \n1) Moving anterograde (if not at the last gaze point, in which case it stays still). \n2) Not moving  \n3) Moving retrograde (if not currently on the first gaze point, in which case it would not move).  \nIn other words, the action vector 𝐴≡{→, ↺, ←}, where → denotes moving anterograde along \nthe gaze plot, ↺ is staying still in the same pixel and ← is moving retrograde. The agent is only \nallowed to move by one gaze point at a time. \n \nStates and policy \n \nOur state space is defined by where along the gaze plot we are, i.e. on which point our agent is \nlocated. We denote the gaze plot for the 𝑗th image 𝐼O consisting of 𝑁gaze\n(O)  points, by T𝑔V\n(O)W\nVXY\nZgaze\n([)\n. \nOur agent begins all training episodes at the first point 𝑔Y\n(O). This state is displayed in Figure 2. \nThe state consists of the 2D brain MRI slice overlaid with gaze plot points in red, then which \ngaze point the agent is currently located on as a blue square of size 11 x 11 square pixels \ncentered on the point. In order to allow the algorithm to “see through” the gaze points and \nsquare representing agent position, both were set to partial transparency. However, the figures \nhere show them with no transparency for the sake of display.  \n \nIn order to learn the optimal policy 𝜋, we define a state-action value or quality function \n𝑄^(𝑠, 𝑎), as the expected / average total reward when taking action 𝑎 in state 𝑠 and and then \nall actions according to policy 𝜋 thereafter: \n \n \n𝑄^(𝑠, 𝑎) = 𝔼[𝑅|𝑠, 𝑎, 𝜋], \n \n       (2) \n \nWhere 𝔼 is the expectation value. Now if we could learn 𝑄 for all possible state-action pairs and \nfind a policy that maximizes 𝑄, we could follow that policy by picking best actions to arrive at \nthe lesion quickly and reliably. In order to do so, 𝑄-learning can be employed, wherein 𝑄 values \nfor each possible station-action combination are calculated in an iterative fashion, eventually \nconverging to true values. The problem with 𝑄 learning is that it is unfeasible for all except \nsmall systems with few possible states and actions. Instead, for most applications, including \nours, it is preferable to learn a function approximation of 𝑄, as a function of state and action. \nThat allows us to calculate 𝑄 values for states and actions not yet seen in iterative exploration, \nbut interpolated from nearby values that were sampled.  \n \nDeep Q Network \n \nAs is known from SDL, CNNs provide excellent function approximation. We can similarly employ \nthem to learn 𝑄-functions, and call them deep 𝑄-networks (DQNs). The architecture of our \nDQN is shown in Figure 2. Taking the state as input, we used 3 x 3 kernels with stride 2 and \npadding so as to maintain the size of the resulting filters. We produced 32 filters at each \nconvolution operation. The network consisted of four such convolutional filters in sequence, \nusing exponential linear unit (elu) activation. The last convolution layer was followed by a fully \nconnected 512-node layer, fully connected to a 3-node output layer, representing 3 𝑄 values, \none corresponding to each possible action. Our loss is the difference between the 𝑄 values \nresulting from a forward pass of the network, which we shall denote as 𝑄CNN and the “target” 𝑄 \nvalue computed by the Bellman equation, which updates by sampling from the environment \nand experiencing rewards.  \n \nBellman Equation \n \nThe Bellman update equation is given by: \n \n \n𝑄(𝑠g, 𝑎g) ⟵𝑄(𝑠g, 𝑎g) + 𝛼[𝑟g + 𝜆𝑚𝑎𝑥n𝑄(𝑠goY, 𝑎g) −𝑄(𝑠g, 𝑎g)], \n \n       (3) \n \nwhere ∝ is the learning rate and 𝜆 is the discount factor, which reflects the present value of \nfuture rewards, the latter factored more as 𝜆 is increased toward one. 𝑄(𝑠g, 𝑎g) is the current 𝑄 \nvalue being updated and 𝑚𝑎𝑥n𝑄(𝑠goY, 𝑎g) is the estimated reward from our next action given \non-policy behavior, i.e. taking actions that maximize cumulative future reward.  \n \nAs discussed more below, using a form of equation (3), we update 𝑄 values, and record these \nalong with the state, action and reward values, calling the result tuple a transition, Τg =\nr𝑠g,𝑎g, 𝑟g, 𝑠goYs. \n \nWorkflow \n \nOur workflow proceeds as follows:  \nStarting with the first gaze point, 𝑔Y\n(O), we create a square matrix around that pixel which is 11 x \n11 pixelsv. This size is chosen to stand out as a state more than just the single point, and thus \nwe expect the DQN to “see it” better. Nevertheless, the square represents the single center \npoint and we will interconvert between the two.  \n \nWe next pursue an interleaved process of sampling and learning from our environment using \nthe reward scheme from Equation 1, sampling the state-action space via the Bellman Equation \nand training our DQN. Again, doing the first part ensures that we will obtain a 𝑄 estimation that \napproaches 𝑄*, the optimal policy. The DQN training assures that we have a function \napproximation for 𝑄 that also approaches 𝑄*. Hence, in the end we have a function taking \nstates as input and calculating 𝑄 values of the three possible actions. We can then select the \noptimal action to take as simply the action giving the largest 𝑄 value, i.e. the argmax over \nactions. Then we can optimally move the agent point around the image so as to ensure that at \nthe end of a testing set episode the point is inside the lesion. Thus, we will have localized the \ntumor.  \n \nWe use a training set of 70 images + gaze plots. We select each image at random and compute \nthe square centered at the first gaze point. We overlay the corresponding pixels in blue onto \nthe 2D grayscale image slice already overlaid with gaze points in red. We plot red and blue \nrepresenting gaze points and agent locations, respectively, with partial transparency. This \nimage serves as the initial state, 𝑠Y (Figure 2, no transparency for purposes of display). Next we \nselect an action 𝑎Y to perform on this state based on the 𝜖-greedy algorithm: if a random \nnumber between zero and one is larger than 𝜖, then an on-policy action is selected. This action \nis computed by acting on 𝑠Y with a forward pass of the DQN, denoted in functional form as \nℱCNN. The forward pass ℱCNN outputs three nodes, one corresponding to the 𝑄 value of each \npossible action (Figure 2). Then picking the argmax action corresponding to the largest 𝑄 value, \nwe choose the optimal / on-policy action for this step. If, on the other hand, the random \nnumber is less than 𝜖, then the action is chosen at random from among the three possible \nactions. \n \nOf note, initially the policy is purely random because we do not know from the outset what the \npolicy actually is. This is manifested as DQN’s weights being initialized according to a Glorot \nrandom distribution. Our algorithm learns the optimal policy through repeated iterations of \nexploring its environment.  \n \nUpon taking the selected action 𝑎Y, our agent arrives at new state 𝑠v, receiving an award 𝑟Y as \nper Equation 1. In this case of the initial state, we record the transition ΤY = r𝑠Y,𝑎Y, 𝑟Y, 𝑠vs. In \ngeneral, we record  Τg = r𝑠g,𝑎g, 𝑟g, 𝑠goYs. As we continue running the process, we keep adding \nrows Τg to an initially growing transition matrix 𝕋, up to a maximum number of 12,000 \ntransitions stored as 12,000 rows in 𝕋. This number of rows / transitions is specified by the \nmemory size 𝑁memory. Once we have added enough transitions Τg to bring 𝕋 to its maximum \nsize, 𝑁memory × 4 = 12,000 × 4, we begin discarding the earliest transitions as new ones are \nadded, keeping the number of rows fixed at 𝑁memory. For example, once we add the 12,001st \ntransition ΤYv,{{Y, we have to remove the first row ΤY. Then when adding ΤYv,{{v to 𝕋, we make \nroom by removing Τv. \n \nWe note that 𝑁memory is an adjustable hyperparameter. The tradeoff in selecting the best \nmemory size value: larger values of 𝑁memory yield better DQNs because more transition samples \nare used for its training, learning more about the environment. However, larger 𝑁memory slows \nthe calculation and at a certain point will overwhelm the CPU’s random access memory (RAM).  \n \nGoing back to the start of our procedure, having just calculated ΤY, we next compute a target 𝑄 \nvalue, 𝑄target\n(Y)\n by the Bellman Equation as 𝑟Y + 𝜆𝑚𝑎𝑥n𝑄(𝑠v, 𝑎), where 𝑚𝑎𝑥n𝑄(𝑠v, 𝑎) is the \naction of the maximum 𝑄 output from the forward feed of the CNN at 𝑠v, i.e. ℱCNN(𝑠v). In \ngeneral, with the following modified Bellman equation, we calculate \n \n \n𝑄target\n(g)\n= 𝑟g + 𝜆𝑚𝑎𝑥n𝑄(𝑠goY, 𝑎). \n       (4) \n \nWe also calculate 𝑄CNN\n(g) = ℱCNN(𝑠g), where ℱCNN again is the function / operator of a forward \npass of the DQN. Then we have a vector / set of 𝑁memory =12,000 target values 𝑄target = \nT𝑄target\n(g)\nW\ngXY\nZmemory, and 12,000 DQN-predicted values, 𝑄CNN = T𝑄CNN\n(g) W\ngXY\nZmemory. In each step \nbackpropagating our DQN, we randomly select a batch size of 𝑁batch = 64 transitions and then \nbackpropagate to minimize the loss ℒbatch of that batch, \n \n \nℒbatch =\n1\n𝑁batch\n~ •𝑄target\n(V)\n−𝑄CNN\n(V) •\nZbatch\nVX{\n. \n \n \n(5) \n \nHaving backpropagated and re-adjusted DQN weights, we then take our next action and update \n𝕋, then re-compute 𝑄target and 𝑄CNN. We randomly select another batch, recompute ℒbatch and \nrun another backpropagation, again updating the DQN weights. We continue in this manner for \nall the steps in each successive episode. By this process, 𝑄target approaches the optimal value 𝑄∗ \nas we continue to sample our environment, and 𝑄CNN approaches 𝑄target as our DQN learns to \nminimize the loss. Hence, 𝑄CNN approaches 𝑄∗, and we ultimately reach the optimal policy.  \n \nThe images we used for training and testing came from the BraTS high grade glioma database \n(35). From that database’s T1-weighted post-contrast 3D image volumes, we randomly selected \n100 2D image slices. We employed a 70/30 training-testing split, using the first 70 of these \nimages for training. We trained for 300 episodes, where an episode was defined as running \n𝑁gaze\n(O)  steps of simulation on image 𝐼O. We note that the analogue of number of episodes in DRL \nis number of epochs for SDL.  \n \nOur DRL agent had input parameters as follows: \n𝛾= 0.99, to reflect the fact that we wanted our agent to count future rewards significantly. \n𝜖= 0.5, with a rate of decay of 1 × 10ƒ„, so that 𝜖 would be slowly decreased by this amount \neach episode until reaching what we defined as a minimal value of 𝜖min = 1 × 10ƒ„. This way, a \nlot of exploration could take place early in the training. This would give way to a steadily \ndecreasing amount of random-move exploration as the algorithm learned the details of the \nimages and 𝑄 converged on the true optimal 𝑄*. In other words, as the optimal policy was \nimplicitly learned, the agent acted more and more according to that policy, while always leaving \na little room to explore possible new solutions. We used a learning rate for our DQN of \n1 × 10ƒ„, a standard-range value in SDL CNNs. \n \n \n \n \nResults \n \nFigure 3 displays a value for the agent’s score during training. The score is the mean reward the \nagent receives during a given episode 𝑖, which for image 𝐼O is defined to last for a duration of \n𝑁gaze\n(O)  steps: \n \n \nscoreV =\n1\n𝑁gaze\n(O) × ~ 𝑅†\nZgaze\n([)\n†XY\n. \n \n       (6) \n \nFigure 3 shows overall consistent improvement during training, although the inherent noisiness \nof the training process is apparent, with prominent jumps between episodes.  \n \nWe trained for a total of 300 episodes, after which convergence was manifest, as can be seen in \nFigures 4 and 5. The training time was roughly 7 minutes and 31 seconds. Figures 4 and 5 show \nthe training and testing set accuracies, respectively during the training process. We define a \ntrue positive training set state or testing set prediction as the agent’s location being within the \nhand annotated lesion mask. We computed both training and testing values every 10 episodes \nsimply as the proportion of number of true positives (TP) over those episodes:  \n \n \naccuracy =\n‡ˆ\nY{. \n \n       (7) \n \nFigure 4 also shows ongoing improvement in training despite some noisiness. Importantly, \nFigure 5 tells us that we are not overfitting the training data, since we see a concurrent \nimproving accuracy in localizing testing set lesions, on which no training was performed. The \nmean of the last 10 predicted testing set accuracies is 85%. \n \nIn order to compare DQN to SDL, we trained a keypoint detection CNN on the training set for \n300 epochs using a network architecture that was identical to that of the DQN except for the \nlast layer. For the keypoint detection network, the latter consisted of two nodes, followed by \nsigmoid activation, to represent the predicted x and y coordinates. In order to keep the \ncomparison as close as possible, we trained the SDL CNN on the same training set of 70 images \nused for the DQN. We computed a loss of the testing set (sometimes alternatively in the \ncontext of an SDL CNN referred to as a validation set), shown along with the training set loss in \nFigure 6. The loss here is defined as negative mean absolute error from the center of the hand \nannotated mask’s bounding box. Importantly, Figure 6 spotlights how the SDL CNN is already \noverfitting the training set by the 10th epoch, manifested by divergence of training and testing \nset losses.  \n \nWe made DRL predictions on the testing set by applying our DQN to each of the 30 images and \ncomputing the network’s accuracy. We act now according to the policy our DQN has implicitly \nlearned, this approaching the optimal policy at later stages of training. At each step of training, \nfor each testing set image 𝐼O with 𝑗∈[71,100], we apply ℱCNN and on-policy action selection \nfor 𝑁gaze\n(O)  steps, starting with 𝑠Y. In general, at time step / iteration 𝑡, we get the three possible \n𝑄 values, corresponding to the three possible actions Œ𝑎V,g•VXY\nŽ\n, by applying a forward pass of \nthe DQN: \n \n \n \nℱCNN(𝑠g) = Œ𝑄n•,••\nVXY\nŽ\n= ‘\n𝑄n’,•\n𝑄n“,•\n𝑄n”,•\n•. \n \n \n       (8) \n \nThen we take the softmax 𝜎 of the predictions, producing a probability distribution that sums to \none: \n \n \n \n𝜎rℱCNN(𝑠g)s = 𝜎—Œ𝑄n•,••\nVXY\nŽ\n˜ =\n⎝\n⎜\n⎜\n⎜\n⎜\n⎛\n𝑒•ž’,•\n∑\n𝑒•ž ,•\nŽ\n†XY\n𝑒•ž“,•\n∑\n𝑒•ž ,•\nŽ\n†XY\n𝑒•ž”,•\n∑\n𝑒•ž ,•\nŽ\n†XY\n⎠\n⎟\n⎟\n⎟\n⎟\n⎞\n. \n \n       (9) \n \nAgain, we are now acting exclusively on-policy, and compute the next action 𝑎g =\nargmax ¤𝜎—Œ𝑄n•,••\nVXY\nŽ\n˜¥. Then taking action 𝑎g brings us to state 𝑠goY. We continue in this \nmanner until eventually reaching the last state 𝑠Zgaze\n([) after the 𝑁gaze\n(O) th\n application of ℱCNN and \noptimal action selection. If the agent’s position in the final state 𝑠Zgaze\n([) lies within the lesion \nmask, the true positive count is increased by one. After computing the sum of true positives, \n𝑇𝑃, the accuracy is calculated as 𝑇𝑃30\n©\n.  \n \nFor direct comparison between RL and SDL, we computed the mean of the last 10 predictions of \nboth approaches as in Equation 7 (epochs or episodes 200, 210, …, 300 for SDL or RL, \nrespectively). Doing so, the mean accuracy for RL was 0.85 (between 25 and 26 out of 30 \nlesions correctly predicted), while that for SDL was roughly 0.07 (2 out of 30 lesions). The \ndifference, not surprisingly was statistically significant, with a 𝑝-value of 1.2 × 10ƒvv. The \ncomparison of method predictions is shown in Figure 7.  \n \nDiscussion \n \nAlthough DRL is widely used in many cutting edge applications, such as robotics and game \nplaying (36), to our knowledge it has not been applied directly to radiological computer vision. \nThis work represents an initial proof-of-principle application of DRL/RL to MRI brain images. We \nhave applied the approach to localize glioblastoma multiforme brain tumors from the BraTS \npublic image database.  \n \nIn so doing, we are able to glimpse the enormous potential of the DRL for radiology computer \nvision. We believe that this approach will in fact ultimately prove to be a seismic shift forward \nin artificial intelligence, one that will herald in the age of true clinical applicability, meaningfully \nchanging the practice of radiology.  \n \nTraining with DRL on only 70 training images, the algorithm was able to predict lesion location \nwith 85% accuracy on a separate testing set. By comparison, this represents the general range \nof success for SDL when trained on hundreds or thousands of training set images, often with \ndata augmentation. When trained on the same training set, the SDL network here quickly began \nto overfit the training data. It ultimately predicted testing set lesion location with accuracy of \nless than 10%. This last result is not surprising, given the widely known requirement of SDL for \nlarge amounts of annotated data to perform reasonably well.  \n \nWe feel that we can confidently proclaim that DRL will change AI in radiology for two main \nreasons. It can:  \n1. Produce accurate results even when trained on very small data sets. \n2. Provide and benefit from user intuition about the images being studied. \n \nRegarding the first benefit, we note that 85% accuracy is a truly remarkable result for a training \nset of 70 images. Of note, we did not even employ data augmentation, a standard way to \nincrease training set size used for SDL. Along with the obvious benefits of accelerating AI \nresearch by now allowing for small data sets to power effective AI, this ability provides two \nother key advantages.  \n \nFirstly, it allows for AI to accelerate and automate processing and analysis of the types of small \ndatasets often encountered in academic settings. One may imagine for instance a particularly \nrare disease entity that only a handful of patients around the world have. The academic \ninstitution where we could imagine the foremost clinician treating perhaps 100 such patients \nwould now be able to unleash the power of AI with even the correspondingly small collection of \nimages.  \n \nSecondly, a decisive barrier to routine clinical application of current SDL-based algorithms is the \ndrop-off in predictive power when a CNN trained on one data set is deployed for example at a \nnew hospital, with the subtle variations secondary to different scanners with discrepant \nsettings as well as the different patient populations. With its ability to train effectively on very \nsmall data sets, we expect that RL can be relatively easily and effectively retrained for new \nscanners / institutions.  \n \nThe intuition for why RL is able to train effectively and in a generalizable fashion on such small \ndata sets lies in the fact that it combs so extensively through the images. Whereas SDL learns \nwhat to do (e.g. locate or segment the lesion), RL learns what not to do (do so for non-lesion \npixels). By learning all of the non-lesion parts of the image, the algorithm gains a more \nextensive sense of the image. It is thus less sensitive to the random noise that is introduced \nwhen encountering images from a new scanner. \n \nAs mentioned above, RL also provides intuition. This can alleviate the dreaded “black box” \nproblem of SDL, in which limited-to-no information for why a network may be working or not, \nand what it is uncovering or what features it is geared for are hidden in the depth of the huge \nnetwork, with its often millions of weights. By contrast, as illustrated in our example \napplication, RL lets the user guide training by specifying appropriate rewards for certain kinds of \nactions in certain states. This offers understanding as to why a particular RL network is working \nor how it may be adjusted to work better. It also empowers radiologists and clinicians to bring \ntheir domain knowledge to bear on AI imaging training and applications.  \n \nOur system provides a nice illustration for this sort of intuition. Here, we want the agent not to \n“stand still” if it is not already in the lesion. Especially toward the onset near the initial gaze \npoint, we want our agent to get moving and explore the state space so that it can find the \ntumor. Hence, we penalize the action of not moving while outside the tumor by a relatively high \nnegative reward. Since we know that the goal is to localize tumors, we can tell our agent and \nhence DQN to usually stay still in the lesion once inside, by providing a generous reward to do \nso in training. Yet we still want it to explore a bit and find other points both within the lesion \nand “downhill” from it, so we give positive values for continuing to move within the lesion, \nthough of lower magnitude than staying still in the tumor.  \n \nFuture work will extend the approach to images without using eye tracking points, hence going \nfrom a one-dimensional state and action space to two dimensions and ultimately three-\ndimensions for full volumetric image stacks. Though this will increase the size and complexity of \nthe state and action space, we will employ more sophisticated DRL techniques appropriate to \nthese environments, such as the actor-critic method. \n \n \n \n \n \n \n \n \n \nFigure 1: illustration of the reward structure. Figure 1A shows the state in which the agent is \nlocated at the second gaze point. The reward of -4 for staying still while not within the lesion \npenalizes this possible action. Moving forward is rewarded, while moving backward is \npenalized, but not by as much as staying still. Figure 1B shows a state in which the agent is \nwithin the lesion, and is now incentivized by a +2 reward to stay in the same position, so that \nresiding within lesions is favored. However, there are small positive rewards for anterograde \nand retrograde movement, less than staying still but not a large penalty, so that other states \npossibly in the lesion can be explored. \n \n \nFigure 2: architecture of the deep q neural network.  \n \n \nFigure 3: Scores normalized by number of gaze points during the training process of the deep Q \nnetwork. Although noisy by the nature of the DQN approach, the overall trend is toward \nincreasing accuracy.  \n \n \nFigure 4: Accuracy of the DQN lesion location prediction on the training set during the course of \ntraining, sampled / computed every 10th episode. Although the accuracy is noisy, the overall \ntrend is one of increasing accuracy, as seen by the best fit line with positive slope.   \n \n \nFigure 5: Accuracy of the DQN lesion location prediction on the testing set during the course of \ntraining, sampled / computed every 10th episode. Of note is that the results toward the end of \ntraining are clustered around 80% accuracy. Additionally, the overall trend is one of improving \naccuracy, as evidenced by the positive-slope best fit regression line.    \n \n \n \n \nFigure 6: For comparison, we trained a supervised deep learning convolutional neural network \n(CNN) for 50 epochs using a keypoint detection CNN with overall similar architecture to the \nnetwork used in our DQN. As the figure shows, before the 10th epoch, the validation set has \nalready diverged and the network is beginning to increasingly overfit the training data. \n \n \nFigure 7: Comparison of the trained reinforcement learning deep Q network predictions on \ntesting set versus those of supervised deep learning. The box heights are average values. Error \nbars represent standard deviation.  \n \n \n \n \n \nReferences \n \n1.  \nSoffer S, Ben-Cohen A, Shimon O, Amitai MM, Greenspan H, Klang E. Convolutional \nneural networks for radiologic images: a radiologist’s guide. Radiology. Radiological \nSociety of North America; 2019;290(3):590–606. \n2.  \nSaba L, Biswas M, Kuppili V, et al. The present and future of deep learning in radiology. \nEur J Radiol. Elsevier; 2019;114:14–24. \n3.  \nMazurowski MA, Buda M, Saha A, Bashir MR. Deep learning in radiology: An overview of \nthe concepts and a survey of the state of the art with focus on MRI. J Magn Reson \nimaging. Wiley Online Library; 2019;49(4):939–954. \n4.  \nHosny A, Parmar C, Quackenbush J, Schwartz LH, Aerts HJWL. Artificial intelligence in \nradiology. Nat Rev Cancer. NIH Public Access; 2018;18(8):500–510. \n5.  \nChoy G, Khalilzadeh O, Michalski M, et al. Current applications and future impact of \nmachine learning in radiology. Radiology. Radiological Society of North America; \n2018;288(2):318–328. \n6.  \nGoodfellow IJ, Shlens J, Szegedy C. Explaining and harnessing adversarial examples. arXiv \nPrepr arXiv14126572. 2014; \n7.  \nBuhrmester V, Münch D, Arens M. Analysis of explainers of black box deep neural \nnetworks for computer vision: A survey. arXiv Prepr arXiv191112116. 2019; \n8.  \nLiu X, Faes L, Kale AU, et al. A comparison of deep learning performance against health-\ncare professionals in detecting diseases from medical imaging: a systematic review and \nmeta-analysis. lancet Digit Heal. Elsevier; 2019;1(6):e271–e297. \n9.  \n(ESR ES of R. What the radiologist should know about artificial intelligence–an ESR white \npaper. Insights Imaging. Springer; 2019;10(1):44. \n10.  \nHolzinger A, Langs G, Denk H, Zatloukal K, Müller H. Causability and explainability of \nartificial intelligence in medicine. Wiley Interdiscip Rev Data Min Knowl Discov. Wiley \nOnline Library; 2019;9(4):e1312. \n11.  \nSilver D, Huang A, Maddison CJ, et al. Mastering the game of Go with deep neural \nnetworks and tree search. Nature. Nature Publishing Group; 2016;529(7587):484–489. \n12.  \nMnih V, Kavukcuoglu K, Silver D, et al. Playing Atari with Deep Reinforcement Learning. \n2013; \n13.  \nMnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement \nlearning. Nature. Nature Publishing Group; 2015;518(7540):529–533. \n14.  \nVan Hasselt H, Guez A, Silver D. Deep reinforcement learning with double q-learning. \nThirtieth AAAI Conf Artif Intell. 2016. \n15.  \nDankwa S, Zheng W. Twin-Delayed DDPG: A Deep Reinforcement Learning Technique to \nModel a Continuous Movement of an Intelligent Robot Agent. Proc 3rd Int Conf Vision, \nImage Signal Process. 2019. p. 1–5. \n16.  \nGu S, Holly E, Lillicrap T, Levine S. Deep reinforcement learning for robotic manipulation \nwith asynchronous off-policy updates. 2017 IEEE Int Conf Robot Autom. IEEE; 2017. p. \n3389–3396. \n17.  \nWang Z, Sarcar S, Liu J, Zheng Y, Ren X. Outline objects using deep reinforcement \nlearning. arXiv Prepr arXiv180404603. 2018; \n18.  \nThorndike EL. Animal intelligence: Experimental studies. Transaction Publishers; 1970. \n19.  \nMinsky ML. Neural Nets and the Brain Model Problem (Ph. D. dissertation). Princet Univ. \n1954; \n20.  \nBellman R. A problem in the sequential design of experiments. Sankhyā Indian J Stat. \nJSTOR; 1956;16(3/4):221–229. \n21.  \nBellman R. Dynamic programming princeton university press princeton. New Jersey \nGoogle Sch. 1957; \n22.  \nBellman RE. A markov decision process. journal of Mathematical Mechanics. 1957; \n23.  \nBarto AG, Sutton RS. Landmark learning: An illustration of associative search. Biol Cybern. \nSpringer; 1981;42(1):1–8. \n24.  \nBarto AG, Sutton RS, Brouwer PS. Associative search network: A reinforcement learning \nassociative memory. Biol Cybern. Springer; 1981;40(3):201–211. \n25.  \nSutton RS, Barto AG. A temporal-difference model of classical conditioning. Proc ninth \nAnnu Conf Cogn Sci Soc. Seattle, WA; 1987. p. 355–378. \n26.  \nBarto A, Duff M. Monte Carlo matrix inversion and reinforcement learning. Adv Neural \nInf Process Syst. 1994. p. 687–694. \n27.  \nBarto AG, Bradtke SJ, Singh SP. Learning to act using real-time dynamic programming. \nArtif Intell. Elsevier; 1995;72(1–2):81–138. \n28.  \nSutton RS, Barto AG. Reinforcement learning: An introduction. MIT press; 2018. \n29.  \nArulkumaran K, Deisenroth MP, Brundage M, Bharath AA. A brief survey of deep \nreinforcement learning. arXiv Prepr arXiv170805866. 2017; \n30.  \nStember JN, Celik H, Krupinski E, et al. Eye Tracking for Deep Learning Segmentation \nUsing Convolutional Neural Networks. J Digit Imaging. Springer International Publishing; \n2019;32(4):597–604. \n31.  \nKhosravan N, Celik H, Turkbey B, et al. Gaze2Segment: A Pilot Study for Integrating Eye-\nTracking Technology into Medical Image Segmentation. . \n32.  \nKhosravan N, Celik H, Turkbey B, Jones EC, Wood B, Bagci U. A collaborative computer \naided diagnosis (C-CAD) system with eye-tracking, sparse attentional model, and deep \nlearning. Med Image Anal. Elsevier; 2019;51:101–115. \n33.  \nTourassi G, Voisin S, Paquit V, Krupinski E. Investigating the link between radiologists’ \ngaze, diagnostic decision, and image content. . \n34.  \nNodine CF, Kundel HL, Toto LC, Krupinski EA. Recording and analyzing eye-position data \nusing a microcomputer workstation. Behav. Res. Methods. Instruments. Comput. 1992. \n35.  \nMenze BH, Jakab A, Bauer S, et al. The Multimodal Brain Tumor Image Segmentation \nBenchmark (BRATS). IEEE Trans Med Imaging. NIH Public Access; 2015;34(10):1993–\n2024. \n36.  \nLi Y. Deep reinforcement learning: An overview. arXiv Prepr arXiv170107274. 2017; \n \n",
  "categories": [
    "cs.AI"
  ],
  "published": "2020-08-06",
  "updated": "2020-08-06"
}