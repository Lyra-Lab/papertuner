{
  "id": "http://arxiv.org/abs/2008.04378v1",
  "title": "Unsupervised Deep Metric Learning with Transformed Attention Consistency and Contrastive Clustering Loss",
  "authors": [
    "Yang Li",
    "Shichao Kan",
    "Zhihai He"
  ],
  "abstract": "Existing approaches for unsupervised metric learning focus on exploring\nself-supervision information within the input image itself. We observe that,\nwhen analyzing images, human eyes often compare images against each other\ninstead of examining images individually. In addition, they often pay attention\nto certain keypoints, image regions, or objects which are discriminative\nbetween image classes but highly consistent within classes. Even if the image\nis being transformed, the attention pattern will be consistent. Motivated by\nthis observation, we develop a new approach to unsupervised deep metric\nlearning where the network is learned based on self-supervision information\nacross images instead of within one single image. To characterize the\nconsistent pattern of human attention during image comparisons, we introduce\nthe idea of transformed attention consistency. It assumes that visually similar\nimages, even undergoing different image transforms, should share the same\nconsistent visual attention map. This consistency leads to a pairwise\nself-supervision loss, allowing us to learn a Siamese deep neural network to\nencode and compare images against their transformed or matched pairs. To\nfurther enhance the inter-class discriminative power of the feature generated\nby this network, we adapt the concept of triplet loss from supervised metric\nlearning to our unsupervised case and introduce the contrastive clustering\nloss. Our extensive experimental results on benchmark datasets demonstrate that\nour proposed method outperforms current state-of-the-art methods for\nunsupervised metric learning by a large margin.",
  "text": "Unsupervised Deep Metric Learning with\nTransformed Attention Consistency and\nContrastive Clustering Loss\nYang Li1, Shichao Kan2, and Zhihai He1\n1 University of Missouri, Columbia, MO, USA\nyltb5@mail.missouri.edu, hezhi@missouri.edu\n2 Beijing Jiaotong University, Beijing, China\n16112062@bjtu.edu.cn\nAbstract. Existing approaches for unsupervised metric learning focus\non exploring self-supervision information within the input image itself.\nWe observe that, when analyzing images, human eyes often compare\nimages against each other instead of examining images individually. In\naddition, they often pay attention to certain keypoints, image regions,\nor objects which are discriminative between image classes but highly\nconsistent within classes. Even if the image is being transformed, the\nattention pattern will be consistent. Motivated by this observation, we\ndevelop a new approach to unsupervised deep metric learning where the\nnetwork is learned based on self-supervision information across images\ninstead of within one single image. To characterize the consistent pat-\ntern of human attention during image comparisons, we introduce the\nidea of transformed attention consistency. It assumes that visually sim-\nilar images, even undergoing diﬀerent image transforms, should share\nthe same consistent visual attention map. This consistency leads to a\npairwise self-supervision loss, allowing us to learn a Siamese deep neu-\nral network to encode and compare images against their transformed or\nmatched pairs. To further enhance the inter-class discriminative power\nof the feature generated by this network, we adapt the concept of triplet\nloss from supervised metric learning to our unsupervised case and intro-\nduce the contrastive clustering loss. Our extensive experimental results\non benchmark datasets demonstrate that our proposed method outper-\nforms current state-of-the-art methods for unsupervised metric learning\nby a large margin.\nKeywords: Unsupervised deep metric learning, attention map, consis-\ntency loss, contrastive loss.\n1\nIntroduction\nDeep metric learning aims to learn discriminative features that can aggregate\nvisually similar images into compact clusters in the high-dimensional feature\nspace while separating images of diﬀerent classes from each other. It has many\narXiv:2008.04378v1  [cs.CV]  10 Aug 2020\n2\nY. Li, S. Kan, and Z. He\nimportant applications, including image retrieval [36,14,11], face recognition [35],\nvisual tracking [32] and person re-identiﬁcation [41,15]. In supervised deep metric\nlearning, we assume that the labels for training data are available. In this paper,\nwe consider unsupervised deep metric learning where the image labels are not\navailable. Learning directly and automatically from images in an unsupervised\nmanner without human supervision represents a very important yet challenging\ntask in computer vision and machine learning.\nClustering is one of the earliest methods developed for unsupervised learning.\nRecently, motivated by the remarkable success of deep learning, researchers have\nstarted to develop unsupervised learning methods using deep neural networks\n[2]. Auto-encoder trains an encoder deep neural network to output feature rep-\nresentations with suﬃcient information to reconstruct input images by a paired\ndecoder [42]. As we know, during deep neural network training, the network\nmodel is updated and learned in an iterative and progressive manner so that\nthe network output can match the target. In other works, deep neural networks\nneed human supervision to provide ground-truth labels. However, in unsuper-\nvised learning, there are no labels available. To address this issue, researchers\nhave exploited the unique characteristics of images and videos to create various\nself-supervised labels, objective functions, or loss functions, which essentially\nconvert the unsupervised learning into a supervised one so that the deep neural\nnetworks can be successfully trained. For example, in DeepCluster [2], clustering\nis used to generate pseudo labels for images. Various supervised learning meth-\nods have been developed to train networks to predict the relative position of two\nrandomly sampled patches [6], solve Jigsaw image puzzles [25], predict pixel val-\nues of missing image patches [5], classify image rotations of four discrete angles\n[10], reconstruct image transforms [42], etc. Once successfully trained by these\npretext tasks, the baseline network should be able to generate discriminative\nfeatures for subsequent tasks, such as image retrieval, classiﬁcation, matching,\netc [13].\nFig. 1. Consistency of visual attention across images under transforms.\nIn this work, we propose to explore a new approach to unsupervised deep\nmetric learning. We observe that existing methods for unsupervised metric learn-\ning focus on learning a network to analyze the input image itself. As we know,\nUnsupervised Deep Metric Learning with TAC and CCL\n3\nwhen examining and classifying images, human eyes compare images back and\nforth in order to identify discriminative features [9]. In other words, compari-\nson plays an important role in human visual learning. When comparing images,\nthey often pay attention to certain keypoints, image regions, or objects which are\ndiscriminative between image classes but highly consistent across image within\nclasses. Even when the image is being transformed, the attention areas will be\nconsistent. To further illustrate this, we provide three examples in Fig. 1. In (a),\nhuman eyes can easily tell the top image A of the ﬁrst column and the bottom\nimage B are the same bird since they have the same visual characteristics. The\nattention will be on the feather texture and head shape. In the pixel domain, A\nand B are up to a spatial transform, speciﬁcally, cropping plus resizing. When\nthe human eyes moves from image A to its transformed version B, the attention\nwill be also transformed so that it can be still focused on the head and feather.\nIf we represent this attention using the attention map in deep neural networks,\nthe attention map M(A) for image A and the attention map M(B) for image\nB should also follow the same transform, as shown in the second column of Fig.\n1(a). We can also see this consistency of attention across image under diﬀerent\ntransforms in other examples in Figs. 1 (b) and (c).\nThis lead to our idea of transformed attention consistency. Based on this\nidea, we develop a new approach to unsupervised deep metric learning based\non image comparison. Speciﬁcally, using this consistency, we can deﬁne a pair-\nwise self-supervision loss, allowing us to learn a Siamese deep neural network to\nencode and compare images against their transformed or matched pairs. To fur-\nther enhance the inter-class discriminative power of the feature generated by this\nnetwork, we adapt the concept of triplet loss from supervised metric learning to\nour unsupervised case and introduce the contrastive clustering loss. Our exten-\nsive experimental results on benchmark datasets demonstrate that our proposed\nmethod outperforms current state-of-the-art methods by a large margin.\n2\nRelated Work and Major Contributions\nThis work is related to deep metric learning, self-supervised representation learn-\ning, unsupervised metric learning, and attention mechanisms.\n(1) Deep metric learning. The main objective of deep metric learning is to\nlearn a non-linear transformation of an input image by deep neural networks. In a\ncommon practice [33,40], the backbone in deep metric learning can be pre-trained\non 1000 classes ImageNet [28] classiﬁcation, and is then jointly trained on the\nmetric learning task with an additional linear embedding layer. Many recent deep\nmetric learning methods are built on pair-based [4,12,26] and triplet relationships\n[21,29,38]. Triplet loss [29] deﬁnes a positive pair and a negative pair based on\nthe same anchor point. It encourages the embedding distance of positive pair to\nbe smaller than the distance of negative pair by a given margin. Multi-similarity\nloss [33] considers multiple similarities and provides a more powerful approach\nfor mining and weighting informative pairs by considering multiple similarities.\nThe ability of mining informative pairs in existing methods is limited by the size\n4\nY. Li, S. Kan, and Z. He\nof mini-batch. Cross-batch memory (XBM) [34] provides a memory bank for the\nfeature embeddings of past iterations. In this way, the informative pairs can be\nidentiﬁed across the dataset instead of a mini-batch.\n(2) Self-supervised representation learning. Self-supervised representa-\ntion learning directly derives information from unlabeled data itself by formulat-\ning predictive tasks to learn informative feature representations. DeepCluster [2]\nuses k-means clustering to assign pseudo-labels to the features generated by the\ndeep neural network and introduces a discriminative loss to train the network.\nGidaris et al. [10] explore the geometric transformation and propose to predict\nthe angle (0◦, 90◦, 180◦, and 270◦) of image rotation as a four-way classiﬁcation.\nZhang et al. [42] propose to predict the randomly sampled transformation from\nthe encoded features by Auto-encoding transformation (AET). The encoder is\nforced to extract the features with visual structure information, which are in-\nformative enough for the decoder to decode the transformation. Self-supervision\nhas been widely used to initialize and pre-train backbone on unlabeled data, and\nis then ﬁne-tuned on a labeled training data for evaluating diﬀerent tasks.\n(3) Unsupervised metric learning. Unsupervised metric learning is a\nrelatively new research topic. It is a more challenging task since the training\nclasses have no labels and it does not overlap with the testing classes. Iscen et al.\n[17] propose an unsupervised method to mine hard positive and negative samples\nbased on manifold-aware sampling. The feature embedding can be trained with\nstandard contrastive and triplet loss. Ye et al. [40] propose to utilize the instance-\nwise relationship instead of class information in the learning process. It optimizes\nthe instance feature embedding directly based on the positive augmentation\ninvariant and negative separated properties.\n(4) Attention mechanism. The goal of the attention mechanism is to\ncapture the most informative feature in the image. It explores important parts of\nfeatures and suppress unnecessary parts [1,22,18]. Convolutional block attention\nmodule (CBAM) [37] is an eﬀective attention method with channel and spatial\nattention module which can be integrated into existing convolutional neural\nnetwork architectures. Fu et al. [8] propose to produce the attention proposals\nand train the attention module and embedding module in an iterative two-stage\nmanner. Chen et al. [3] propose the hybrid-attention system by random walk\ngraph propagation for object attention and the adversary constraint for channel\nattention.\nCompared to existing methods, the unique contributions of this paper can\nbe summarized as follows. (1) Unlike existing methods which focus on informa-\ntion analysis of the input image only, we explore a new approach for unsupervised\ndeep metric learning based on image comparison and cross-image consistency.\n(2) Motivated by the human visual experience, we introduce the new approach\nof transformed attention consistency to eﬀectively learn a deep neural network\nwhich can focus on discriminative features. (3) We extend the existing triplet\nloss developed for supervised metric learning to unsupervised learning using k-\nmean clustering to assign pseudo labels and memory bank to allow its access\nto all training samples, instead of samples in the current mini-batch. (4) Our\nUnsupervised Deep Metric Learning with TAC and CCL\n5\nexperimental results demonstrate that our proposed approach has improved the\nstate-of-the-art performance by a large margin.\nFig. 2. Overview of the proposed approach for unsupervised deep metric learning with\ntransformed attention consistency and contrastive clustering loss.\n3\nMethod\n3.1\nOverview\nSuppose that we have a set of unlabeled images X = {x1, x2, ..., xN}. Our goal is\nto learn a deep neural network to extract their features G(xn) ∈Rd, where d is\nthe feature dimension. Fig. 2 shows the overall design of our proposed method for\nunsupervised deep metric learning based on transformed attention consistency\nand contrastive clustering loss (TAC-CCL). Given an input image xn, we apply\na transform T, which is randomly sampled from a set of image transforms T , to\nxn, to obtain its transformed version x′\nn = T(xn). In our experiments, we mainly\nconsider spatial transforms, including cropping (sub-image), rotation, zooming,\nand perspective transform. Each transform is controlled by a set of transform\nparameters. For example, the cropping is controlled by its bounding box. The\nperspective transform is controlled by its 6 parameters. Image pairs (xn, x′\nn) are\ninputs to the Siamese deep neural network. These two identical networks will\nbe trained to extract features Fn and F ′\nn for these two images. As illustrated\nin Fig. 2, each network is equipped with an attention network to learn the\nattention map which will modulate the output feature map. The attention map\ncan enforce the network to focus on discriminative local features for the speciﬁc\nlearning tasks or datasets. Let Mn and M ′\nn be the attention maps for images\nxn and x′\nn, respectively. According to the transformed attention consistency, we\nshall have\nM ′\nn = T(Mn).\n(1)\nBased on this constraint, we introduce the transformed attention consistency loss\nLT AC to train the feature embedding network G, which will be further explained\nin Section 3.3. Besides this attention consistency, we also require that the output\n6\nY. Li, S. Kan, and Z. He\nfeatures Fn and F\n′\nn should be similar to each other since the corresponding input\nimages xn and x\n′\nn are visually the same. To enforce this constraint, we introduce\nthe feature similarity loss LF = ||Fn −F\n′\nn||2 which is the L2-normal between\nthese two features. To ensure that image features from the same class aggregate\ninto compact clusters while image features from diﬀerent classes are pushed\naway from each other in the high-dimensional feature space, we introduce the\ncontrastive clustering loss LCC, which will be further explained in the Section\n3.3.\n3.2\nBaseline System\nIn this work, we ﬁrst design a baseline system. Recently, a method called multi-\nsimilarity (MS) loss [33] has been developed for supervised deep metric learning.\nIn this work, we adapt this method from supervised metric learning to unsu-\npervised metric learning using k-means clustering to assign pseudo labels. Also,\nthe original MS method computes the similarity scores between image samples\nin the current mini-batch. In this work, we extend this similarity analysis to the\nwhole training set using the approach of memory bank [34]. The features of all\ntraining samples generated by the network are stored in the memory bank by the\nenqueue method. When the memory bank is full, the features and corresponding\nlabels of the oldest mini-batch are removed by the dequeue method. Using this\napproach, the current mini-batch has access to the whole training set. We can\nthen compute the similarity scores between all samples in the mini-batch and\nall samples in the training set. Our experiments demonstrate that this enhanced\nsimilarity matrix results in signiﬁcantly improved performance in unsupervised\nmetric learning. In this work, we use this network as the baseline system, denoted\nby TAC-CLL (baseline).\n3.3\nLoss Functions\nTo further improve the performance of the baseline system, we introduce the\nideas of transformed attention consistency and contrastive clustering loss, which\nare explained in the following.\nThe transformed attention consistency aims to enforce the feature embed-\nding network G to focus visually important features instead of other back-\nground noise. Let Mn(u, v) and M\n′\nn(u, v) be the attention maps for input im-\nage pair xn and x\n′\nn, where (u, v) represents a point location in the attention\nmap. Under the transform T, this point is mapped to a new location denoted by\n(Tu(u, v), Tv(u, v)) According to (1), if we transform the attention map Mn(u, v)\nfor the original image xn by T, it should match the attention map M ′\nn(u, v) for\nthe transformed image x′\nn = T(xn). Based on this, the proposed transformed\nattention consistency loss LT AC is deﬁned as follows\nLT AC =\nX\n(u,v)\n|Mn(u, v) −M ′\nn(Tu(u, v), Tv(u, v))|2,\n(2)\nUnsupervised Deep Metric Learning with TAC and CCL\n7\n(a) Baseline without TAC-CLL\n(b) Baseline with TAC-CCL\nFig. 3. The Barnes-Hut t-SNE visualizations of the CUB test dataset with and without\nthe TAC-CCL method.\nwhere u′ = Tu(u, v) and v′ = Tv(u, v) are the mapped location of (u, v) in image\nx′\nn.\nThe constrastive clustering loss extends the triplet loss [29] developed in\nsupervised deep metric learning, where an anchor sample x is associated with a\npositive sample x+ and a negative sample x−. The triplet loss aims to maximize\nthe ratio S(x, x+)/S(x, x−), where S(·, ·) represents the cosine similarity between\ntwo features. It should be noted that this triplet loss requires the knowledge of\nimage labels, which however are not available in our unsupervised case. To extend\nthis triplet loss to unsupervised metric learning, we propose to cluster the image\nfeatures into K clusters. In the high-dimensional feature space, we wish these\nclusters are compact and are well separated from each other by large margins. Let\n{Ck}, 1 ≤k ≤K, be the cluster centers. Let C+(Fn) be the nearest center which\nhas the minimum distance to the input image feature Fn and the corresponding\ndistance is denoted by d+(Fn) = ||Fn −C+(Fn)||2. Let C−(Fn) be the cluster\ncenter which has the second minimum distance to Fn and the corresponding\ndistance is denoted by d−(Fn) = ||Fn −C−(Fn)||2. If the contrastive ratio of\nd+(Fn)/d+(Fn) is small, then this feature has more discriminative power. We\ndeﬁne the following contrastive clustering loss\nLCC = EFn\n\u001a||Fn −C+(Fn)||2\n||Fn −C−(Fn)||2\n\u001b\n,\n(3)\nwhich is the average contrastive ratio of all input image features. During the\ntraining process, the network G, as well as the feature for each input, is progres-\nsively updated. For example, the clustering is performed and the cluster centers\nare updated for every 20 epochs.\nFig. 3 visualizes the images of the CUB dataset in the feature space with\nfeatures extracted by the baseline system with and without the TAC-CCL ap-\nproach. We can see that using the TAC-CCL, the obtained feature clusters are\nmore compact within each class and better separated from each other between\nclasses.\n8\nY. Li, S. Kan, and Z. He\n3.4\nTransformed Attention Consistency with Cross-Images\nSupervision\nNote that, in our proposed approach, we transform or augment the input image\nxn to create its pair x\n′\nn. These two are from the same image source. We also no-\ntice that most of existing self-supervision methods, such as predicting locations\nof image patches and classifying the rotation of an image [10], and reconstructing\nthe transform of the image [42], all focus on self-supervision information within\nthe image itself. The reason behind this is that image patches from the same\nimage will automatically have the same class label. This provides an important\nself-supervision constraint to train the network. However, this one-image ap-\nproach will limit the learning capability of the network since the network is not\nable to compare multiple images. As we know, when human eyes are examining\nimages to determine which features are discriminative, they need to compare\nmultiple images to determine which set of features are consistent across images\nand which set of features are background noise [9]. Therefore, in unsupervised\nlearning, it is highly desirable to utilize the information across images.\nFig. 4. Sub-image matching for cross-image supervision.\nFigs. 4(a)-(c) show image samples from the Cars and SOP benchmark datasets.\nWe can see that images from the same class exhibit strong similarity between\nimages, especially in the object regions. The question is how to utilize these\nunlabeled images to create reliable self-supervision information for unsupervised\nlearning? In this work, we propose to perform keypoint or sub-image match-\ning across images. Speciﬁcally, as illustrated in Fig. 4(d) and (e), for a given\nimage sample In, in the pre-processing stage, we perform aﬃne-SIFT [23] key-\npoint matching between In and other images in the dataset and ﬁnd the top\nmatches with conﬁdence scores about a very high threshold. We then crop out\nthe sub-images containing high-conﬁdence keypoints as xn and x′\nn which are\nrelated by a transform T. This high-conﬁdence constraint aims to ensure that\nxn and x′\nn are having the same object class or semantic label. In this way, for\neach image in the k-means cluster, we can ﬁnd multiple high-conﬁdence matched\nsub-images. This will signiﬁcantly augment the training set, establish cross-image\nself-supervision, and provide signiﬁcantly enhanced visual diversity for the net-\nwork to learn more robust and discriminative features. In this work, we com-\nUnsupervised Deep Metric Learning with TAC and CCL\n9\nbine this cross-image supervision with the transformed attention consistency. Let\n{(ui, vi)} and {(u′\ni, v′\ni)}, 1 ≤i ≤N, be the set of matched keypoints in xn and\nx′\nn. We wish that, within the small neighborhoods of these matched keypoints,\nthe attention maps Mn and M ′\nn are consistent. To deﬁne a small neighborhood\naround a keypoint (ui, vi), we use the following 2-D Gaussian kernel,\nφ(u −ui, v −vi) = e\n−(u−ui)2\n2σ2u\n−(v−vi)2\n2σ2v\n.\n(4)\nLet\nΓ(u, v) =\nM\nX\ni=1\nφ(u −ui, v −vi),\nΓ ′(u, v) =\nM\nX\ni=1\nφ(u −u′\ni, v −v′\ni)\n(5)\nwhich deﬁne two masks to indicate the neighborhood areas around these matched\nkeypoints in these two attention maps. The extended transformed attention con-\nsistency becomes\nLT AC =\nX\n(u,v)\n|Mn(u, v) · Γ(u, v) −M ′\nn(u, v) · Γ ′(u, v)|2,\n(6)\nwhich compares the diﬀerence between these two attention maps around these\nmatched keypoints. Compared to the label propagation method developed for\nsemi-supervised learning [43,44], our cross-image supervision method is unique\nin the following aspects: (1) it discovers sub-images of the same label (with very\nhigh probability) from unlabeled images. (2) It establishes the transform between\nthese two sub-images and combines with the transformed attention consistency\nto achieve eﬃcient unsupervised deep metric learning.\n4\nExperimental Results\nIn this section, we conduct extensive experiments on benchmark datasets in\nimage retrieval settings to evaluate the proposed TAC-CCL method for unsu-\npervised deep metric learning.\n4.1\nDatasets\nWe follow existing papers on unsupervised deep metric learning [40] to evaluate\nour proposed methods on the following three benchmark datasets. (1) CUB-\n200-2011 (CUB) is composed of 11,788 images of birds from 200 classes. The\nﬁrst 100 classes (5864 images) are used for training, with the rest 100 classes\n(5924 images) for testing. (2) Cars-196 (Cars) contains 16,185 images of 196\nclasses of car models. We use the ﬁrst 98 classes with 8054 images for train-\ning, and remaining 98 classes (8131 images) for testing. (3) Stanford Online\nProducts (SOP) has 22,634 classes (120,053 images) of online products. We\nuse the ﬁrst 11,318 products (59,551 images) for training and the remaining\n11,316 products (60,502 images) for testing. The training classes are separated\nfrom the test classes. We use the standard image retrieval performance metric\n(Recall@k), for performance evaluations and comparisons.\n10\nY. Li, S. Kan, and Z. He\n(a) CUB\n(b) Cars\n(c) SOP\nFig. 5. Retrieval results of some example queries on CUB, Cars, and SOP datasets.\nThe query images and the negative retrieved images are highlighted with blue and red.\n4.2\nImplementation Details\nWe implement our proposed method by PyTorch and follow the standard exper-\nimental settings in existing papers [26,33,40] for performance comparison. We\nuse the same GoogLeNet [31] pre-trained on ImageNet as the backbone network\n[24,26,30] and a CBAM [37] attention module is placed after the inception 5b\nlayer. A fully connected layer is then added on the top of the network as the\nembedding layer. The default dimension of embedding is set as 512. For the\nclustering, we set the number of clusters K to be 100 for the CUB and Cars\ndatasets, and K = 10000 for the SOP dataset. For each batch, we follow the\ndata sampling strategy in multi-similarity loss [33] to sample 5 images per class.\nFor data augmentation, images in the training set are randomly cropped at size\n227 × 227 with random horizontal ﬂipping, while the images in testing set is\ncenter cropped. Adam optimizer [20] is used in all experiments and the weigh\ndecay is set as 5e−4.\n4.3\nPerformance Comparisons with State-of-the-Art Methods\nWe compare the performance of our proposed methods with the state-of-the-\nart unsupervised methods on image retrieval tasks. The mining on manifolds\n(MOM) [17] and the invariant and spreading instance feature method (denoted\nUnsupervised Deep Metric Learning with TAC and CCL\n11\nTable 1. Recall@K (%) performance on CUB dataset in comparison with other meth-\nods\nMethods\nBackbone\nCUB\nR@1\nR@2 R@4 R@8\nSupervised Methods\nABIER [27]\nGoogLeNet\n57.5\n68.7 78.3 86.2\nABE [19]\nGoogLeNet\n60.6\n71.5 79.8 87.4\nMulti-Similarity [33]\nBN-Inception\n65.7\n77.0 86.3 91.2\nUnsupervised Methods\nExamplar [7]\nGoogLeNet\n38.2\n50.3 62.8 75.0\nNCE [39]\nGoogLeNet\n39.2\n51.4 63.7 75.8\nDeepCluster [2]\nGoogLeNet\n42.9\n54.1 65.6 76.2\nMOM [17]\nGoogLeNet\n45.3\n57.8 68.6 78.4\nInstance [40]\nGoogLeNet\n46.2\n59.0 70.1 80.2\nTAC-CCL (baseline) GoogLeNet\n53.9\n66.2 76.9 85.8\nTAC-CCL\nGoogLeNet\n57.5\n68.8 78.8 87.2\nGain\n+11.3 +9.8 +8.7 +7.0\nby Instance) [40] are current state-of-the-art methods for unsupervised metric\nlearning. They both use the GoogLeNet [31] as the backbone encoder. In the\nInstance paper [40], the authors have also implemented three other state-of-\nthe-art methods originally developed for supervised deep metric learning and\nadapted them to unsupervised metric learning tasks: Examplar [7], NCE (Noise-\nContrastive Estimation) [39], and DeepCluster [2]. We include the results of\nthese methods for comparisons. We have also included the performance of re-\ncent supervised deep metric learning methods for comparison so that we can\nsee the performance diﬀerence between unsupervised metric learning and super-\nvised one. These methods include: ABIER [27], and ABE [19], and MS (Multi-\nSimilarity) [33]. Both ABIER and ABE methods are using the GoogleNet as the\nbackbone encoder. The MS method is using the BN-Inception network [16] as\nthe backbone encoder.\nThe results for the CUB, Cars, and SOP datasets are summarized in Tables 1,\n2, and 3, respectively. We can see that our proposed TAC-CCL method achieves\nnew state-of-the-art performance in unsupervised metric learning on both ﬁne-\ngrained CUB and Cars datasets and the large-scale SOP dataset. On the CUB\ndataset, our TAC-CCL improves the Recall@1 by 11.3% and is even competitive\nto some supervised metric learning methods, e.g., ABIER [27]. On the Cars\ndataset, our TAC-CCL outperforms the current state-of-the-art Instance method\n[40] by 4.8%. On SOP, our method achieves 63.9% and outperforms existing\nmethods by a large margin of 15%. For other Recall@K rates with large values\nof k, the amount of improvement is also very signiﬁcant. Note that our baseline\nsystem achieves a large improvement over existing methods. The proposed TAC-\nCCL approach further improves upon this baseline system by another 1.4-3.6%.\n12\nY. Li, S. Kan, and Z. He\nTable 2. Recall@K (%) performance on Cars dataset in comparison with other meth-\nods\nMethods\nBackbone\nCars\nR@1 R@2 R@4 R@8\nSupervised Methods\nABIER [27]\nGoogLeNet\n82.0 89.0 93.2 96.1\nABE [19]\nGoogLeNet\n85.2 90.5 94.0 96.1\nMulti-Similarity [33]\nBN-Inception 84.1 90.4 94.0 96.5\nUnsupervised Methods\nExamplar [7]\nGoogLeNet\n36.5 48.1 59.2 71.0\nNCE [39]\nGoogLeNet\n37.5 48.7 59.8 71.5\nDeepCluster [2]\nGoogLeNet\n32.6 43.8 57.0 69.5\nMOM [17]\nGoogLeNet\n35.5 48.2 60.6 72.4\nInstance [40]\nGoogLeNet\n41.3 52.3 63.6 74.9\nTAC-CCL (baseline) GoogLeNet\n43.0 53.8 65.3 76.0\nTAC-CCL\nGoogLeNet\n46.1 56.9 67.5 76.7\nGain\n+4.8 +4.6 +3.9 +1.8\nFig. 5 shows examples of retrieval results from the CUB, Cars, and SOP\ndatasets. In each row, the ﬁrst image highlighted with a blue box is the query\nimage. The rest images are the top 15 retrieval results. Images highlighted with\nred boxes are from diﬀerent classes. It should be noted that some classes have\nvery small number of samples. We can see that our TAC-CCL can learn discrim-\ninitive features to achieve satisfying retrieval results, even for these challenging\ntasks. For example, at the ﬁrst row of the SOP dataset, our model is able to\nlearn the glass decoration feature under the lampshade, which is a unique feature\nof the query images. In addition, the negative retrieved results are also visually\ncloser to the query images.\n4.4\nAblation Studies\nIn this section, we conduct ablation studies to perform in-depth analysis of our\nproposed method and its diﬀerent components.\n(1) Impact of the number of clusters. The proposed contrastive cluster-\ning loss is based on clustering in the feature space. The number of clusters K is\na critical parameter for the proposed method since it determines the number of\npseudo labels. We conduct the following ablation study experiment on the CUB\ndata to study the impact of K. The ﬁrst plot in Fig. 6 shows the Recall@1 results\nwith diﬀerent values of K: 50, 100, 200, 500, and 1000. The other three plots\nshow the results for Recall@2, 4, and 8. We can see that, on this dataset, the\nbest value of K is 100, which is the number of test classes in the CUB dataset.\nThe performance drops when K increases. This study suggests that the best\nvalue of K is close to the truth number test classes of the dataset.\nUnsupervised Deep Metric Learning with TAC and CCL\n13\nTable 3. Recall@K (%) performance on SOP dataset in comparison with other meth-\nods\nMethods\nBackbone\nSOP\nR@1\nR@10 R@100\nSupervised Methods\nABIER [27]\nGoogLeNet\n74.2\n86.9\n94.0\nABE [19]\nGoogLeNet\n76.3\n88.4\n94.8\nMulti-Similarity [33]\nBN-Inception\n78.2\n90.5\n96.0\nUnsupervised Methods\nExamplar [7]\nGoogLeNet\n45.0\n60.3\n75.2\nNCE [39]\nGoogLeNet\n46.6\n62.3\n76.8\nDeepCluster [2]\nGoogLeNet\n34.6\n52.6\n66.8\nMOM [17]\nGoogLeNet\n43.3\n57.2\n73.2\nInstance [40]\nGoogLeNet\n48.9\n64.0\n78.0\nTAC-CCL (baseline) GoogLeNet\n62.5\n76.5\n87.2\nTAC-CCL\nGoogLeNet\n63.9\n77.6\n87.8\nGain\n+15.0 +13.6 +9.8\nFig. 6. Recall@K (%) performance on CUB dataset in comparison with diﬀerent num-\nber of clusters\n(2) Impact of diﬀerent embedding sizes. In this ablation study, we\nfollow existing supervised metric learning methods [33,27] to study the impact\nof diﬀerent embedding sizes, or the size of the embedded feature. For example,\nthe feature size ranges from 64, 128, 256, 512, to 1024. The ﬁrst plot of Fig. 7\nshows the Recall@1 results for diﬀerent embedding size. The results for Recall@2,\n4, 8 are shown in the other three plots. We can see that unsupervised metric\nlearning performance increases with the embedding size since it contains more\nfeature information with enhanced discriminative power.\n(3) Performance contributions of diﬀerent algorithm components.\nOur proposed system has three major components: the baseline system for un-\nsupervised deep metric learning, transformed attention consistency (TAC), and\ncontrastive clustering loss (CCL). In this ablation study, we aim to identify the\ncontribution of each algorithm component on diﬀerent datasets. Table 4 summa-\nrizes the performance results on the CUB, Cars, and SOP datasets using three\ndiﬀerent method conﬁgurations: (1) the baseline system, (2) baseline with CCL,\n14\nY. Li, S. Kan, and Z. He\nFig. 7. Recall@K (%) performance on CUB dataset in comparison with diﬀerent em-\nbedding sizes\nand (3) baseline with CCL + TAC. We can see that both the CCL and TAC\napproaches signiﬁcantly improve the performance.\nIn our Supplemental materials, we will provide additional method implemen-\ntation details, experimental results, and ablation studies.\nTable 4. The performance of diﬀerent components from our TAC-CCL method on\nCUB, Cars, and SOP datasets.\nCUB\nCars\nSOP\nR@1 R@2 R@4 R@8 R@1 R@2 R@4 R@8 R@1 R@10 R@100\nBaseline 53.9 66.2 76.9 85.8\n43.0 53.8 65.3 76.0\n62.5\n76.5\n87.2\n+CCL\n55.7 67.8 77.5 86.2\n44.7 55.6 65.9 75.7\n63.0\n76.8\n87.2\n+TAC 57.5 68.8 78.8 87.2 46.1 56.9 67.5 76.7 63.9 77.6\n87.8\n5\nConclusions\nIn this work, we have developed a new approach to unsupervised deep metric\nlearning based on image comparisons, transformed attention consistency, and\nconstrastive clustering loss. This transformed attention consistency leads to a\npairwise self-supervision loss, allowing us to learn a Siamese deep neural network\nto encode and compare images against their transformed or matched pairs. To\nfurther enhance the inter-class discriminative power of the feature generated by\nthis network, we have adapted the concept of triplet loss from supervised metric\nlearning to our unsupervised case and introduce the contrastive clustering loss.\nOur extensive experimental results on benchmark datasets demonstrate that\nour proposed method outperforms current state-of-the-art methods by a large\nmargin.\nUnsupervised Deep Metric Learning with TAC and CCL\n15\nReferences\n1. Ba, J., Mnih, V., Kavukcuoglu, K.: Multiple object recognition with visual atten-\ntion. arXiv preprint arXiv:1412.7755 (2014)\n2. Caron, M., Bojanowski, P., Joulin, A., Douze, M.: Deep clustering for unsuper-\nvised learning of visual features. In: Proceedings of the European Conference on\nComputer Vision (ECCV). pp. 132–149 (2018)\n3. Chen, B., Deng, W.: Hybrid-attention based decoupled metric learning for zero-\nshot image retrieval. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. pp. 2750–2759 (2019)\n4. Chopra, S., Hadsell, R., LeCun, Y.: Learning a similarity metric discriminatively,\nwith application to face veriﬁcation. In: 2005 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR’05). vol. 1, pp. 539–546.\nIEEE (2005)\n5. DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net-\nworks with cutout. arXiv preprint arXiv:1708.04552 (2017)\n6. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning\nby context prediction. In: Proceedings of the IEEE International Conference on\nComputer Vision. pp. 1422–1430 (2015)\n7. Dosovitskiy, A., Fischer, P., Springenberg, J.T., Riedmiller, M., Brox, T.: Discrimi-\nnative unsupervised feature learning with exemplar convolutional neural networks.\nIEEE transactions on pattern analysis and machine intelligence 38(9), 1734–1747\n(2015)\n8. Fu, J., Zheng, H., Mei, T.: Look closer to see better: Recurrent attention convo-\nlutional neural network for ﬁne-grained image recognition. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition. pp. 4438–4446 (2017)\n9. Gazzaniga, M.S.: The cognitive neurosciences. MIT Press (2009)\n10. Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018)\n11. Grabner, A., Roth, P.M., Lepetit, V.: 3d pose estimation and 3d model retrieval for\nobjects in the wild. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. pp. 3022–3031 (2018)\n12. Hadsell, R., Chopra, S., LeCun, Y.: Dimensionality reduction by learning an invari-\nant mapping. In: 2006 IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR’06). vol. 2, pp. 1735–1742. IEEE (2006)\n13. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\nvisual representation learning. arXiv preprint arXiv:1911.05722 (2019)\n14. He, X., Zhou, Y., Zhou, Z., Bai, S., Bai, X.: Triplet-center loss for multi-view 3d\nobject retrieval. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. pp. 1945–1954 (2018)\n15. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-\nidentiﬁcation. arXiv preprint arXiv:1703.07737 (2017)\n16. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)\n17. Iscen, A., Tolias, G., Avrithis, Y., Chum, O.: Mining on manifolds: Metric learning\nwithout labels. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. pp. 7642–7651 (2018)\n18. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks.\nIn: Advances in neural information processing systems. pp. 2017–2025 (2015)\n16\nY. Li, S. Kan, and Z. He\n19. Kim, W., Goyal, B., Chawla, K., Lee, J., Kwon, K.: Attention-based ensemble for\ndeep metric learning. In: Proceedings of the European Conference on Computer\nVision (ECCV). pp. 736–751 (2018)\n20. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n21. Kumar, V., Harwood, B., Carneiro, G., Reid, I., Drummond, T.: Smart mining for\ndeep metric learning. arXiv preprint arXiv:1704.01285 2 (2017)\n22. Mnih, V., Heess, N., Graves, A., et al.: Recurrent models of visual attention. In:\nAdvances in neural information processing systems. pp. 2204–2212 (2014)\n23. Morel, J.M., Yu, G.: Asift: A new framework for fully aﬃne invariant image com-\nparison. SIAM journal on imaging sciences 2(2), 438–469 (2009)\n24. Movshovitz-Attias, Y., Toshev, A., Leung, T.K., Ioﬀe, S., Singh, S.: No fuss dis-\ntance metric learning using proxies. In: Proceedings of the IEEE International\nConference on Computer Vision. pp. 360–368 (2017)\n25. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving\njigsaw puzzles. In: European Conference on Computer Vision. pp. 69–84. Springer\n(2016)\n26. Oh Song, H., Xiang, Y., Jegelka, S., Savarese, S.: Deep metric learning via lifted\nstructured feature embedding. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition. pp. 4004–4012 (2016)\n27. Opitz, M., Waltner, G., Possegger, H., Bischof, H.: Deep metric learning with bier:\nBoosting independent embeddings robustly. IEEE transactions on pattern analysis\nand machine intelligence (2018)\n28. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-\nnition challenge. International journal of computer vision 115(3), 211–252 (2015)\n29. Schroﬀ, F., Kalenichenko, D., Philbin, J.: Facenet: A uniﬁed embedding for face\nrecognition and clustering. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition. pp. 815–823 (2015)\n30. Sohn, K.: Improved deep metric learning with multi-class n-pair loss objective. In:\nAdvances in neural information processing systems. pp. 1857–1865 (2016)\n31. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\nVanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings\nof the IEEE conference on computer vision and pattern recognition. pp. 1–9 (2015)\n32. Tao, R., Gavves, E., Smeulders, A.W.: Siamese instance search for tracking. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 1420–1429 (2016)\n33. Wang, X., Han, X., Huang, W., Dong, D., Scott, M.R.: Multi-similarity loss with\ngeneral pair weighting for deep metric learning. In: Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition. pp. 5022–5030 (2019)\n34. Wang, X., Zhang, H., Huang, W., Scott, M.R.: Cross-batch memory for embedding\nlearning. arXiv preprint arXiv:1912.06798 (2019)\n35. Wen, Y., Zhang, K., Li, Z., Qiao, Y.: A discriminative feature learning approach for\ndeep face recognition. In: European conference on computer vision. pp. 499–515.\nSpringer (2016)\n36. Wohlhart, P., Lepetit, V.: Learning descriptors for object recognition and 3d pose\nestimation. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition. pp. 3109–3118 (2015)\n37. Woo, S., Park, J., Lee, J.Y., So Kweon, I.: Cbam: Convolutional block attention\nmodule. In: Proceedings of the European Conference on Computer Vision (ECCV).\npp. 3–19 (2018)\nUnsupervised Deep Metric Learning with TAC and CCL\n17\n38. Wu, C.Y., Manmatha, R., Smola, A.J., Krahenbuhl, P.: Sampling matters in deep\nembedding learning. In: Proceedings of the IEEE International Conference on Com-\nputer Vision. pp. 2840–2848 (2017)\n39. Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via non-\nparametric instance discrimination. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. pp. 3733–3742 (2018)\n40. Ye, M., Zhang, X., Yuen, P.C., Chang, S.F.: Unsupervised embedding learning via\ninvariant and spreading instance feature. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition. pp. 6210–6219 (2019)\n41. Yu, R., Dou, Z., Bai, S., Zhang, Z., Xu, Y., Bai, X.: Hard-aware point-to-set deep\nmetric for person re-identiﬁcation. In: Proceedings of the European Conference on\nComputer Vision (ECCV). pp. 188–204 (2018)\n42. Zhang, L., Qi, G.J., Wang, L., Luo, J.: Aet vs. aed: Unsupervised representation\nlearning by auto-encoding transformations rather than data. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition. pp. 2547–\n2555 (2019)\n43. Zhu, X., Ghahramani, Z.: Learning from labeled and unlabeled data with label\npropagation. Tech. rep., Citeseer (2002)\n44. Zhu, X.J.: Semi-supervised learning literature survey. Tech. rep., University of\nWisconsin-Madison Department of Computer Sciences (2005)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-08-10",
  "updated": "2020-08-10"
}