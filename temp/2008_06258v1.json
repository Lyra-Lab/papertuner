{
  "id": "http://arxiv.org/abs/2008.06258v1",
  "title": "Unsupervised vs. transfer learning for multimodal one-shot matching of speech and images",
  "authors": [
    "Leanne Nortje",
    "Herman Kamper"
  ],
  "abstract": "We consider the task of multimodal one-shot speech-image matching. An agent\nis shown a picture along with a spoken word describing the object in the\npicture, e.g. cookie, broccoli and ice-cream. After observing one paired\nspeech-image example per class, it is shown a new set of unseen pictures, and\nasked to pick the \"ice-cream\". Previous work attempted to tackle this problem\nusing transfer learning: supervised models are trained on labelled background\ndata not containing any of the one-shot classes. Here we compare transfer\nlearning to unsupervised models trained on unlabelled in-domain data. On a\ndataset of paired isolated spoken and visual digits, we specifically compare\nunsupervised autoencoder-like models to supervised classifier and Siamese\nneural networks. In both unimodal and multimodal few-shot matching experiments,\nwe find that transfer learning outperforms unsupervised training. We also\npresent experiments towards combining the two methodologies, but find that\ntransfer learning still performs best (despite idealised experiments showing\nthe benefits of unsupervised learning).",
  "text": "Unsupervised vs. transfer learning for multimodal one-shot\nmatching of speech and images\nLeanne Nortje\nHerman Kamper\nE&E Engineering, Stellenbosch University, South Africa\nnortjeleanne@gmail.com, kamperh@sun.ac.za\nAbstract\nWe consider the task of multimodal one-shot speech-image\nmatching. An agent is shown a picture along with a spoken\nword describing the object in the picture, e.g. cookie, broccoli\nand ice-cream. After observing one paired speech-image exam-\nple per class, it is shown a new set of unseen pictures, and asked\nto pick the “ice-cream”. Previous work attempted to tackle this\nproblem using transfer learning: supervised models are trained\non labelled background data not containing any of the one-shot\nclasses. Here we compare transfer learning to unsupervised\nmodels trained on unlabelled in-domain data. On a dataset of\npaired isolated spoken and visual digits, we speciﬁcally compare\nunsupervised autoencoder-like models to supervised classiﬁer\nand Siamese neural networks. In both unimodal and multimodal\nfew-shot matching experiments, we ﬁnd that transfer learning\noutperforms unsupervised training. We also present experiments\ntowards combining the two methodologies, but ﬁnd that trans-\nfer learning still performs best (despite idealised experiments\nshowing the beneﬁts of unsupervised learning).\nIndex Terms: one-shot learning, multimodal modelling, unsu-\npervised models, transfer learning, word acquisition\n1. Introduction\nYoung children are able to learn new objects and words from\nonly a few examples [1–4]. In contrast, most conventional vision\nor speech processing systems require large amounts of labelled\ndata. This has motivated studies into one-shot learning [5–11]:\nto learn a new concept from one or a few labelled examples.\nOne-shot learning studies have mainly focused on learning new\nconcepts in a single modality. But recently, multimodal one-shot\nlearning has also been considered [12]. Instead of observing\nan item together with a class label, the model observes a pair\nof items coming from different modalities but representing the\nsame concept. As an example, imagine a household robot is\nshown examples of milk, eggs, butter and a mug, each visual\ninstance being paired with a spoken tag. At test time, the agent is\nthen presented with a spoken query such as “butter”, and asked\nto identify the corresponding visual object.\nIn [12], this was investigated on a dataset of isolated spoken\ndigits paired with images. To perform multimodal matching at\ntest-time, separate speech-speech and image-image comparisons\nwere combined: a spoken query is compared to all the speech\nitems in a so-called support set, the image corresponding to the\nclosest item in the support set is determined, and this image is\nthen compared to all the items in the matching set to predict\nthe test image best matching the input speech query. To learn a\ndistance metric within each modality, transfer learning was used\nby training supervised vision and speech models on background\ntraining data not containing any of the one-shot test classes. As\nin other unimodal one-shot studies in gesture recognition [13,\n14], video [15] and robotics [16,17], this can be motivated by\nthe observation that humans can call on prior knowledge when\nlearning new concepts.\nExcept for existing knowledge, it is also conceivable that,\nbefore being shown paired examples, an agent such as the house-\nhold robot would be exposed to a large amount of unlabelled\nspeech and visual data from its environment. Some of these\nunlabelled examples could correspond to the classes of interest.\nMotivated by this observation, we ask how unsupervised models\ntrained on unlabelled in-domain data compares to transfer learn-\ning from background data for multimodal one-shot matching.\nTo learn feature representations for within-modality com-\nparisons, we speciﬁcally consider two unsupervised learning\nstrategies. An autoencoder (AE) attempts to reproduce its input\nat its output through a bottleneck feature layer. The correspon-\ndence autoencoder (CAE) tries to reproduce another instance of\nthe input at its output [18]. Since we only have unlabelled data,\nthe CAE samples nearest neighbours to obtain its output targets.\nWe compare these unsupervised models to supervised classiﬁer\nand Siamese neural networks trained on background data [12].\nEach of the models are trained separately on vision and speech\ndata and then used to estimate within-modality similarity.\nOn the same isolated digit speech-image multimodal one-\nshot matching task as in [12], we show that transfer learning out-\nperforms unsupervised modelling. We also consider approaches\nfor combining transfer and unsupervised learning. Although this\nyields improvements over a purely unsupervised model, the best\noverall performance is still achieved through transfer learning.1\n2. Multimodal one-shot matching\nWe ﬁrst describe unimodal one-shot matching and then extend it\nto the multimodal case. As an example, we consider one-shot\nspeech classiﬁcation, illustrated on the left in Figure 1(a). The\nmodel is shown a support set S, containing one isolated spoken\nword with a text label for each of the L word classes. From\nthis set, the model must learn a classiﬁer CS that can make\npredictions on an unseen test query x∗\na. approach is to simply\ncompare the query with each item in the support set and then\npredict the label of the closest item, as illustrated on the right in\nFigure 1(a).\nFigure 1(b) illustrates multimodal one-shot speech-image\nmatching. Instead of labelled examples, the multimodal sup-\nport set S = {(x(i)\na , x(i)\nv )}L\ni=1 consists of pairs, where each\nisolated spoken word x(i)\na\nhas a corresponding image x(i)\nv .\nOne pair is given for each of the L classes. At test time, the\nmodel is presented with an unseen spoken query x∗\na and asked\nto determine the matching image in a test (or matching) set\nMv = {(x(i)\nv )}N\ni=1 of unseen images, as illustrated on the left\nin Figure 1(b). Neither the query x∗\na nor the matching set items\n1We release source code at: https://github.com/\nLeanneNortje/multimodal_speech-image_matching.\narXiv:2008.06258v1  [cs.CL]  14 Aug 2020\nQuery 𝐱∗𝑎\nSupport set \n= {(\n,\n)\n𝐱(𝑖)\n𝑎\ny(𝑖)\n𝑎}𝐿\n𝑖=1\nTest question:\nHow?\nTest question:\nQuery: 𝐱∗𝑎\nHow?\n?\n(b)\n(a)\nSupport set \n \n= {(\n,\n)\n𝐱(𝑖)\n𝑎\n𝐱(𝑖)\n𝑣}𝐿\n𝑖=1\nMatching set \n= {(\n)\n𝑣\n𝐱(𝑖)\n𝑣}𝑁\n𝑖=1\n→\n(\n)\n𝐶𝐱𝑎\n→\n(\n,\n)\n𝐷𝐱𝑎𝐱𝑣\nSupport set \n= {(\n,\n)\n𝐱(𝑖)\n𝑎\ny(𝑖)\n𝑎}𝐿\n𝑖=1\nSupport set \n \n= {(\n,\n)\n𝐱(𝑖)\n𝑎\n𝐱(𝑖)\n𝑣}𝐿\n𝑖=1\nMatching set \n= {(\n)\n𝑣\n𝐱(𝑖)\n𝑣}𝑁\n𝑖=1\n=  “four”\nŷ 𝑎\n“zero”\n“two”\n“five”\n“eight”\n“three”\n“zero”\n“two”\n“five”\n“eight”\n“three”\n“eight”\n“eight”\nQuery: 𝐱∗𝑎\n=?\nŷ 𝑎\n(“four”)\n“oh”\n“four”\n“zero”\n“two”\n“one”\n“oh”\n“four”\n“zero”\n“two”\n“one”\nQuery: 𝐱∗𝑎\n“four”\nFigure 1: (a) Unimodal one-shot speech classiﬁcation and (b) multimodal one-shot speech-image matching. In both cases, the left side\nillustrates the question shown at test time, and the right side illustrates how the model makes its prediction.\nMv occur exactly in the support set S. To perform this task,\nwe need to use S to construct a distance metric DS(xa, xv)\nbetween audio queries and test images.\nThe approach we use (originally proposed in [12]) is to\nreduce the task to two unimodal comparisons, as shown on the\nright in Figure 1(b). First, we compare the query x∗\na to each x(i)\na\nin S to ﬁnd the query’s closest spoken neighbour in the support\nset. This closest neighbour’s paired image is then compared to\neach image x(i)\nv\nin the matching set Mv. This closest matching-\nset image is then selected as the model’s prediction. In the ﬁgure,\nthis is the image of the rightmost eight.\nWe can also extend one-shot learning to K-shot learning. In\nunimodal L-way K-shot classiﬁcation, the support set S con-\ntains L classes and K labelled examples per class. In multimodal\nL-way K-shot matching, S = {(x(i)\na , x(i)\nv )}L×K\ni=1\nconsists of K\nspeech-image pairs for each of the L classes.\n3. Feature representations\nIn the description above we implicitly assume that we have a\nmethod or model that can measure similarity within a modality.\nThe aim of this paper is to consider different feature represen-\ntations for these similarity comparisons, speciﬁcally comparing\ntransfer learning (used in [12]) to unsupervised feature learning.\nTo compare the different features, we use the same framework\nas in [12] where multimodal one-shot learning is performed via\ntwo unimodal comparisons (as outlined above, Figure 1(b)-right).\nNote that this is not an end-to-end approach; future work will\nexplore learning direct cross-modal matching networks.\nAs a baseline, we use raw speech and image features di-\nrectly (§3.1). We then consider different neural networks to\nlearn feature representations (§3.2 and §3.3). We use separate\nnetworks for learning speech and image features. For both the\nspeech and vision models, we consider two settings: training\non unlabelled in-domain data (§3.2) and training on labelled\nbackground data (§3.3).\n3.1. Raw feature matching\nAs a nearest neighbour baseline, we use cosine distance over\nimage pixels for image-to-image comparisons, and dynamic time\nwarping (DTW) over MFCCs for speech-to-speech comparisons.\n3.2. Unsupervised models on unlabelled in-domain data\nWe consider two unsupervised models trained on unlabelled in-\ndomain speech and vision data—data which includes unlabelled\ninstances of classes that we will see during one-shot testing.\nAn autoencoder (AE) is an unsupervised neural network\nwhich aims to reconstruct its input through a lower dimensional\nlatent representation that acts as an information bottleneck [19].\nAs shown in Figure 2, the AE’s encoder fθ(x(i)) encodes the in-\nput x(i) to the feature representation z(i). The decoder fφ(z(i))\ndecodes z(i) to produce the output ˆy(i). We use a squared loss\nbetween the network’s output ˆy(i) and the desired output y(i),\ni.e., ℓ= ||y(i) −ˆy(i)||2\n2, with the target set to y(i) = x(i).\nThe correspondence autoencoder (CAE) is identical to the\nAE but instead of reproducing the input x(i), it aims to reproduce\nanother instance x(i)\npair of the same class as the input [18], i.e. we\nset the target y(i) = x(i)\npair in the loss ℓ. The intuition is that the\nCAE will produce features that are invariant to properties not\ncommon to two inputs while capturing aspects that are, such\nas the class. We consider two variants of the CAE: one trained\nfrom scratch and another pretrained as an AE before switching\nto the CAE loss (denoted as AE-CAE). To train the CAE, we\nneed pairs of items of the same class. Since our in-domain data\nis unlabelled, we use cosine distance over pixels to ﬁnd image\npairs that are most alike, and DTW to ﬁnd spoken word pairs\npredicted to be of the same type. Speaker information is used to\nensure that speech pairs are from different speakers.\nUsing unlabelled in-domain image data, we train unsuper-\nvised vision networks with the AE, CAE and AE-CAE losses;\nwe use the architecture shown in Figure 2(a), with a convo-\nlutional neural network (CNN) encoder producing the latent\nfeature vector, and a decoder with transposed convolutions. Sim-\nilarly, we use unlabelled in-domain speech data to train unsuper-\nvised speech networks using the AE, CAE and AE-CAE losses;\nwe use an encoder recurrent neural network (RNN) producing\nthe latent feature vector which is then used to condition a de-\ncoder RNN, as shown in Figure 2(b). These speech RNNs are\nsimilar to the acoustic embedding models of [20–23], since they\ngive a ﬁxed-sized embedding for variable duration input.\n3.3. Transfer learning from labelled background data\nWe next consider training supervised models on labelled back-\nground data. These datasets do not contain any instances of the\ntarget one-shot classes. The idea is that features learned by such\nmodels would still be useful for determining similarity on unseen\nclasses [10]. This is a form of transfer learning [24,25].\nWe speciﬁcally consider supervised classiﬁer and Siamese\nneural networks, as in [12]. We use identical architectures to the\nencoder parts of the networks in Figure 2. For the classiﬁers, we\nadd a softmax layer after the feature embedding layer z(i) and\ntrain the networks with the multiclass log loss.\nA Siamese network does not classify an input, but mea-\nsures similarity between inputs [26]. The network consists of\n𝐱(𝑖)\n𝑣\n𝐳(𝑖)\n𝑣\n𝐲̂ (𝑖)\n𝑣\n(\n)\n𝑓𝜃𝐱(𝑖)\n𝑣\n(\n)\n𝑓𝜙𝐳(𝑖)\n𝑣\n𝐲̂ (𝑖)\n𝑎\n(\n)\n𝑓𝜃𝐱(𝑖)\n𝑎\n(\n)\n𝑓𝜙𝐳(𝑖)\n𝑎\n𝐳(𝑖)\n𝑎\nPre-processing of\nspoken word (audio)\ninto MFCC\n(a)\n(b)\n=  “nine”\n𝐱(𝑖)\n𝑎\nFigure 2: (a) Convolutional neural networks (CNNs) are used to learn feature representations for image data and (b) recurrent neural\nnetworks (RNNs) are used to learn feature representations for speech data.\nidentical sub networks with shared parameters; each network\nmaps its input to an embedding. Ideally, inputs of the same\nclass should have similar embeddings and inputs of different\nclasses should have different embeddings. Say we have in-\nputs x, xpair and xneg, where x and xpair are from the same\nclass and x and xneg are from different classes.\nWe want\nthe distance between the embeddings of x and xpair to be\nsmaller than those of x and xneg. We use the triplet hinge loss\nl(x, xpair, xneg) = max{0, m+d(x, xpair)−d(x, xneg)}, where\nd(x1, x2) =\n\r\rz1 −z2\n\r\r2\n2 is the squared Euclidean distance be-\ntween the embeddings z1 and z2 of x1 and x2, respectively, and\nm is a margin parameter [27, 28]. To sample negative items,\nwe use the online semi-hard mining scheme, where for each\npositive pair (x, xpair), the most difﬁcult negative pair (x, xneg)\nis sampled (meeting some constraints) [29–31].\nAgain, separate classiﬁer and Siamese vision CNNs and\nspeech RNNs are trained on labelled background data. We\nalso consider supervised variants of the CAE and AE-CAE ap-\nproaches, where instead of ﬁnding input-output training pairs\nbased on their nearest neighbours (§3.2), we train on ground\ntruth pairs from the background data (these were not considered\nin [12]). For all of the models, we use the embedding z(i) as\nrepresentation for unseen input x(i).\n4. Experimental setup\n4.1. Data\nWe follow the same setup as [12], using a dataset of paired iso-\nlated spoken digits and handwritten digit images [32]. Speech\ndata are parametrised as Mel-frequency cepstral coefﬁcients\n(MFCCs). Image pixels are normalised to [0, 1]. We use the\nTIDigits corpus as our in-domain speech data; the corpus con-\nsists of spoken digit sequences from 326 speakers [33]. We split\nthese sequences into isolated digits using forced alignments. As\nour in-domain image data, we use the MNIST corpus which\ncontains 28 × 28 grayscale handwritten digit images [34]. Al-\nthough the TIDigits and MNIST datasets are labelled, note that\nwe use it as unlabelled in-domain data for the models in §3.2;\nwe speciﬁcally train these unsupervised models on unlabelled\nisolated examples from the training subsets of these datasets.\nAll one-shot evaluation experiments are then performed on the\nMNIST and TIDigits test subsets.\nFor background speech data, we use the Buckeye corpus of\nEnglish speech from 40 speakers [35]. We use forced alignments\nto extract a set of labelled isolated words from this set. For\nbackground image data, we use Omniglot [36], containing 1623\ntypes of handwritten characters which we invert and downsample\nto 28 × 28. We ensure that there are no instances of the target\ndigit classes in either the Buckeye or Omniglot background data.\n4.2. Models\nNeural networks are implemented in TensorFlow and trained us-\ning Adam optimisation [37] with a learning rate of 10−3. Model\nhyperparameters were tuned using unimodal one-shot classiﬁca-\ntion on test subsets of the background data, while early stopping\nwas performed on validation subsets—neither of these back-\nground sets have item or class overlap with the ﬁnal evaluation\ndata. We use a feature embedding dimensionality of 130 in all\nmodels to make results comparable. All speech RNNs take static\nMFCCs as input, but ﬁrst and second order derivatives are used\nin the DTW baseline where it is beneﬁcial.\nUnsupervised speech RNNs are trained on unlabelled iso-\nlated digits from the TIDigits training set using the AE, CAE\nand AE-CAE losses (§3.2). In all cases, the encoder and decoder\neach consists of three 400-unit RNN layers. Unsupervised vision\nCNNs are trained with the AE, CAE and AE-CAE losses on\nunlabelled images from the MNIST training set. The encoder\nconsists of three convolutional layers with 3×3 kernels and 32,\n64 and 128 units; the decoder has the inverse architecture.\nFor transfer learning (§3.3), we train supervised classiﬁer\nand Siamese speech RNNs on labelled isolated words from the\nBuckeye training set. Similarly, we train supervised classiﬁer\nand Siamese vision CNNs on Omniglot. All these supervised\nmodels share the same structure as the encoder components\nfrom their unsupervised counterparts. We also train supervised\nvariants of the CAE and AE-CAE speech and vision models on\nthe labelled background data.\n4.3. Evaluation\nWe evaluate models averaged over 400 “episodes” [10]. To\nconstruct the support set, each multimodal episode randomly\nsamples a spoken digit and paired image for each of the L = 11\nclasses (“one” to “nine”, as well as “zero” and “oh”). A matching\nset is then sampled for testing, containing ten digit images not\nin the support set. Finally, a spoken query is sampled, also not\nin the support set. The speech query then needs to be matched\nto the correct image in the matching set. The matching set only\ncontains ten digit images since there are only ten unique hand-\nwritten digit classes (both “zero” and “oh” are counted as correct\nif the image is that of a 0). Within an episode, ten different\nTable 1: Unimodal one- and ﬁve-shot speech classiﬁcation.\nModel\n11-way accuracy (%)\none-shot\nﬁve-shot\nBaseline\nDTW\n65.90\n89.45\nTransfer learning\nmodels\nClassiﬁer RNN\n86.87 ± 0.83\n95.40 ± 0.50\nSiamese RNN\n83.52 ± 2.56\n94.34 ± 0.86\nCAE RNN\n79.89 ± 1.32\n92.16 ± 0.90\nAE-CAE RNN\n80.02 ± 1.04\n93.91 ± 0.25\nUnsupervised\nmodels\nAE RNN\n53.82 ± 1.70\n75.58 ± 1.54\nCAE RNN\n75.80 ± 1.76\n95.14 ± 0.80\nAE-CAE RNN\n77.01 ± 1.29\n93.30 ± 0.56\nquery instances are also sampled while keeping the support and\nmatching sets ﬁxed. We report unimodal and multimodal one-\nand ﬁve-shot matching accuracies with 95% conﬁdence intervals\naveraged over ﬁve models trained with different seeds.\n5. Experimental Results\n5.1. K-shot unimodal speech and image classiﬁcation\nWe ﬁrst consider unimodal results in isolation. Table 1 shows\none- and ﬁve-shot speech classiﬁcation results. All models ex-\ncept the AE RNN outperform the baseline. The classiﬁer RNN\nachieves the highest accuracies, followed by the Siamese RNN.\nIn all cases, transfer learning models outperform their unsuper-\nvised counterparts, except for the ﬁve-shot CAE RNN.\nFor unimodal image classiﬁcation (not shown here), the\ntrends are very similar, with the classiﬁer and Siamese CNNs\nachieving accuracies of around 64% and 84% for the one- and\nﬁve-shot cases, respectively. Again, these transfer learning mod-\nels outperform all the unimodal unsupervised image models.\n5.2. K-shot multimodal speech and image matching\nTable 2 shows multimodal one- and ﬁve-shot results.2 In each\ncase, the same model type is used to obtain speech and image\nfeatures, e.g. the Classiﬁer row uses a CNN vision classiﬁer to\nget image features with an RNN speech classiﬁer for speech\nfeatures. In both one- and ﬁve-shot multimodal matching, the\nclassiﬁer performs best followed closely by the Siamese model.\nNone of the unsupervised models perform as well as these mod-\nels obtained using transfer learning. For the CAE and AE-CAE\nlosses, the models trained using labelled background data also\noutperform the unsupervised variants.\n5.3. Towards combined transfer and unsupervised learning\nIt is evident that the transfer learning approach originally fol-\nlowed in [12] outperforms the unsupervised approach developed\nhere. However, the two methodologies might be complementary:\ntransfer learning from background data could capture general\nproperties within a particular modality, while unsupervised learn-\ning on unlabelled in-domain data could provide a way to tailor\nrepresentations to a speciﬁc test setting.\nAs an initial investigation, we propose two combined mod-\nels here, with results given in Table 3. The CAE with cosine\npairs (row 3) is repeated from Table 2. Instead of ﬁnding near-\n2Note that the results here are not directly comparable to that of [12].\nWe found a small bug in the validation setup of [12]; the scores across\nmodels in [12] are comparable, but lower scores are achieved when using\nthe proper validation setup used in this paper. We reran the code of [12]\nto conﬁrm the scores reported here.\nTable 2: Multimodal one- and ﬁve-shot speech-image matching.\nModel\n11-way accuracy (%)\none-shot\nﬁve-shot\nBaseline\nDTW + Pixels\n31.80\n41.88\nTransfer learning\nmodels\nClassiﬁer [12]\n56.80 ± 1.19\n59.67 ± 1.73\nSiamese [12]\n54.83 ± 1.80\n59.25 ± 0.79\nCAE\n46.60 ± 0.69\n53.82 ± 1.07\nAE-CAE\n48.15 ± 1.21\n56.81 ± 1.21\nUnsupervised\nmodels\nAE\n28.99 ± 0.84\n38.68 ± 1.51\nCAE\n42.75 ± 0.62\n52.15 ± 0.69\nAE-CAE\n42.81 ± 1.01\n50.28 ± 0.29\nTable 3: Multimodal one- and ﬁve-shot speech-image matching\nusing models that combine transfer and unsupervised learning.\nModel\n11-way accuracy (%)\none-shot\nﬁve-shot\nBaseline: DTW + Pixels\n31.80\n41.88\nTransfer learning: Classiﬁer [12]\n56.80 ± 1.19\n59.67 ± 1.73\nCAE with cosine pairs\n42.75 ± 0.62\n52.15 ± 0.69\nCAE with classiﬁer pairs\n48.66 ± 1.14\n55.59 ± 0.71\nTransfer learning + CAE ﬁne-tuning\n54.32 ± 2.19\n59.37 ± 1.80\nCAE with oracle pairs\n89.19 ± 0.69\n92.81 ± 0.47\nest neighbours using cosine distance, we use representations\nfrom the classiﬁer (trained on background data) to ﬁnd pairs\nin the unlabelled in-domain data for training a CAE (as with\nthe standard CAE, speaker information is still used to ensure\nthat pairs are from different speakers). We see that this CAE\nwith classiﬁer pairs (row 4) gives a small improvement over the\nstandard CAE. By additionally initialising the CAE by training\nit on the labelled background data and then ﬁne-tuning it on the\nin-domain data, we get a further improvement (Transfer learning\n+ CAE ﬁne-tuning, row 5). Neither of these approaches, however,\noutperform the transfer learned classiﬁer (row 2).\nIn order to see if it is at all possible to achieve better perfor-\nmance with the CAE by using more accurate training pairs, we\nalso give the performance of a CAE trained only using correct\npairs in the last row of Table 3. We see that this oracle model\noutperforms all other approaches, indicating that, if we were\nable to improve the CAE’s training pairs, we might be able to\ntake advantage of an unsupervised learning scheme.\n6. Conclusion\nWe have compared existing and new models for few-shot mul-\ntimodal speech-image matching. Transfer learning from back-\nground data consistently outperformed unsupervised modelling\non unlabelled in-domain data on a multimodal one-shot match-\ning benchmark. We also proposed two approaches for combining\ntransfer and unsupervised learning. Although neither improved\nthe best transfer learning approach, performance improved over\nthe standard unsupervised approach. We will therefore also con-\nsider other approaches for combining the methodologies in future\nwork. Building on models which directly maps images and unla-\nbelled speech into a joint space [38–41], we will also consider\nend-to-end solutions for multimodal one-shot learning.\nThis work is supported in part by the National Research Foundation\nof South Africa (grant number: 120409), a Google Faculty Award for\nHK, a DST CSIR scholarship for LN, and funding from Saigen.\n7. References\n[1] I. Biederman, “Recognition-by-components: A theory of human\nimage understanding.” Psych. Review, vol. 94, 1987.\n[2] G. A. Miller and P. M. Gildea, “How children learn words,” SciAM,\nvol. 257, 1987.\n[3] R. L. G´omez and L. Gerken, “Infant artiﬁcial language learning\nand language acquisition,” TiCS, vol. 4, 2000.\n[4] O. R¨as¨anen and H. Rasilo, “A joint model of word segmentation\nand meaning acquisition through cross-situational learning.” Psych.\nReview, vol. 122, 2015.\n[5] L. Fei-Fei, Fergus, and Perona, “A Bayesian approach to unsu-\npervised one-shot learning of object categories,” in Proc. ICCV,\n2003.\n[6] L. Fei-Fei, R. Fergus, and P. Perona, “One-shot learning of object\ncategories,” IEEE Trans. PAMI, vol. 28, 2006.\n[7] B. M. Lake, R. Salakhutdinov, J. Gross, and J. B. Tenenbaum,\n“One shot learning of simple visual concepts,” CogSci, vol. 33,\n2011.\n[8] B. M. Lake, C.-y. Lee, J. R. Glass, and J. B. Tenenbaum, “One-shot\nlearning of generative speech concepts,” CogSci, vol. 36, 2014.\n[9] G. Koch, “Siamese neural networks for one-shot image recogni-\ntion,” in Proc. ICML, 2015.\n[10] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wier-\nstra, “Matching networks for one shot learning,” in Proc. NIPS,\n2016.\n[11] P. Shyam, S. Gupta, and A. Dukkipati, “Attentive recurrent com-\nparators,” in Proc. ICML, 2017.\n[12] R. Eloff, H. A. Engelbrecht, and H. Kamper, “Multimodal one-shot\nlearning of speech and images,” in Proc. ICCASP, 2019.\n[13] W. Thomason and R. A. Knepper, “Recognizing unfamiliar ges-\ntures for human-robot interaction through zero-shot learning,” in\nProc. ISER, 2017.\n[14] D. Wu, F. Zhu, and L. Shao, “One shot learning gesture recognition\nfrom RGBD images,” in Proc IEEE Comput. Soc. Conf. Comput.\nVis. Pattern Recognit., 2012.\n[15] T. Stafylakis and G. Tzimiropoulos, “Zero-Shot keyword spotting\nfor visual speech recognition in-the-wild,” in Proc. ECCV, 2018.\n[16] M. R. Walter, Y. Friedman, M. Antone, and S. Teller, “One-shot\nvisual appearance learning for mobile manipulation,” IJRR, vol. 31,\n2012.\n[17] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, “One-shot\nvisual imitation learning via meta-learning,” arXiv:1709.04905,\n2017.\n[18] H. Kamper, M. Elsner, A. Jansen, and S. Goldwater, “Unsuper-\nvised neural network based feature extraction using weak top-down\nconstraints,” in Proc. ICCASP, 2015.\n[19] D. Chicco, P. Sadowski, and P. Baldi, “Deep autoencoder neural\nnetworks for gene ontology annotation predictions,” in Proc. ACM,\n2014.\n[20] Y.-A. Chung, C.-C. Wu, C.-H. Shen, and H.-y. Lee, “Unsuper-\nvised learning of audio segment representations using sequence-to-\nsequence recurrent neural networks,” in Proc. Interspeech, 2016.\n[21] Y.-H. Wang, H.-y. Lee, and L.-s. Lee, “Segmental audio Word2Vec:\nRepresenting utterances as sequences of vectors with applications\nin spoken term detection,” in Proc. ICCASP, 2018.\n[22] N. Holzenberger, M. Du, J. Karadayi, R. Riad, and E. Dupoux,\n“Learning word embeddings: Unsupervised methods for ﬁxed-\nsize representations of variable-length speech segments,” in Proc.\nInterspeech, 2018.\n[23] H. Kamper, “Truly unsupervised acoustic word embeddings using\nweak top-down constraints in encoder-decoder models,” in Proc.\nICCASP, 2019.\n[24] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans.\nKnowl. Data Eng., vol. 22, 2009.\n[25] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng,\nand T. Darrell, “DeCAF: A deep convolutional activation feature\nfor generic visual recognition,” in Proc. ICML, 2014.\n[26] J. Bromley, I. Guyon, Y. LeCun, E. S¨ackinger, and R. Shah, “Sig-\nnature veriﬁcation using a “Siamese” time delay neural network,”\nin Proc. NIPS, 1994.\n[27] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,\nB. Chen, and Y. Wu, “Learning ﬁne-grained image similarity with\ndeep ranking,” in Proc. CVPR, 2014.\n[28] K. M. Hermann and P. Blunsom, “Multilingual distributed repre-\nsentations without word alignment,” in Proc. ICLR, 2014.\n[29] F. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet: A uniﬁed\nembedding for face recognition and clustering,” in Proc. CVPR,\n2015.\n[30] E. Hoffer and N. Ailon, “Deep metric learning using triplet net-\nwork,” in Proc. SIMBAD, 2015.\n[31] A. Hermans, L. Beyer, and B. Leibe, “In defense of the triplet loss\nfor person re-identiﬁcation,” arXiv:1703.07737, 2017.\n[32] K. Kashyap, “Learning digits via joint audio-visual representa-\ntions,” Thesis, Massachusetts Institute of Technology, 2017.\n[33] R. G. Leonard and G. R. Doddington, “TIDIGITS LDC93S10,”\nPhiladelphia: Linguistic Data Consortium, 1993.\n[34] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based\nlearning applied to document recognition,” in Proc. IEEE, 1998.\n[35] M. A. Pitt, K. Johnson, E. Hume, S. Kiesling, and W. Raymond,\n“The Buckeye corpus of conversational speech:labeling conventions\nand a test of transcriber reliability,” Speech Commun., vol. 45,\n2005.\n[36] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, “Human-\nlevel concept learning through probabilistic program induction,”\nScience, vol. 350, 2015.\n[37] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” in Proc. ICLR, 2015.\n[38] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,\n“Multimodal deep learning,” in Proc. ICML, 2011.\n[39] D. Harwath, A. Torralba, and J. Glass, “Unsupervised learning of\nspoken language with visual context,” in Proc. NIPS, 2016.\n[40] K. Leidal, D. Harwath, and J. Glass, “Learning modality-invariant\nrepresentations for speech and images,” in Proc. ASRU, 2017.\n[41] D. Harwath, A. Recasens, D. Suris, G. Chuang, A. Torralba, and\nJ. Glass, “Jointly discovering visual objects and spoken words from\nraw sensory input,” in Proc. ECCV, 2018.\n",
  "categories": [
    "cs.CL",
    "cs.CV",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2020-08-14",
  "updated": "2020-08-14"
}