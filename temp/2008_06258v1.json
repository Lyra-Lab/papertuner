{
  "id": "http://arxiv.org/abs/2008.06258v1",
  "title": "Unsupervised vs. transfer learning for multimodal one-shot matching of speech and images",
  "authors": [
    "Leanne Nortje",
    "Herman Kamper"
  ],
  "abstract": "We consider the task of multimodal one-shot speech-image matching. An agent\nis shown a picture along with a spoken word describing the object in the\npicture, e.g. cookie, broccoli and ice-cream. After observing one paired\nspeech-image example per class, it is shown a new set of unseen pictures, and\nasked to pick the \"ice-cream\". Previous work attempted to tackle this problem\nusing transfer learning: supervised models are trained on labelled background\ndata not containing any of the one-shot classes. Here we compare transfer\nlearning to unsupervised models trained on unlabelled in-domain data. On a\ndataset of paired isolated spoken and visual digits, we specifically compare\nunsupervised autoencoder-like models to supervised classifier and Siamese\nneural networks. In both unimodal and multimodal few-shot matching experiments,\nwe find that transfer learning outperforms unsupervised training. We also\npresent experiments towards combining the two methodologies, but find that\ntransfer learning still performs best (despite idealised experiments showing\nthe benefits of unsupervised learning).",
  "text": "Unsupervised vs. transfer learning for multimodal one-shot\nmatching of speech and images\nLeanne Nortje\nHerman Kamper\nE&E Engineering, Stellenbosch University, South Africa\nnortjeleanne@gmail.com, kamperh@sun.ac.za\nAbstract\nWe consider the task of multimodal one-shot speech-image\nmatching. An agent is shown a picture along with a spoken\nword describing the object in the picture, e.g. cookie, broccoli\nand ice-cream. After observing one paired speech-image exam-\nple per class, it is shown a new set of unseen pictures, and asked\nto pick the â€œice-creamâ€. Previous work attempted to tackle this\nproblem using transfer learning: supervised models are trained\non labelled background data not containing any of the one-shot\nclasses. Here we compare transfer learning to unsupervised\nmodels trained on unlabelled in-domain data. On a dataset of\npaired isolated spoken and visual digits, we speciï¬cally compare\nunsupervised autoencoder-like models to supervised classiï¬er\nand Siamese neural networks. In both unimodal and multimodal\nfew-shot matching experiments, we ï¬nd that transfer learning\noutperforms unsupervised training. We also present experiments\ntowards combining the two methodologies, but ï¬nd that trans-\nfer learning still performs best (despite idealised experiments\nshowing the beneï¬ts of unsupervised learning).\nIndex Terms: one-shot learning, multimodal modelling, unsu-\npervised models, transfer learning, word acquisition\n1. Introduction\nYoung children are able to learn new objects and words from\nonly a few examples [1â€“4]. In contrast, most conventional vision\nor speech processing systems require large amounts of labelled\ndata. This has motivated studies into one-shot learning [5â€“11]:\nto learn a new concept from one or a few labelled examples.\nOne-shot learning studies have mainly focused on learning new\nconcepts in a single modality. But recently, multimodal one-shot\nlearning has also been considered [12]. Instead of observing\nan item together with a class label, the model observes a pair\nof items coming from different modalities but representing the\nsame concept. As an example, imagine a household robot is\nshown examples of milk, eggs, butter and a mug, each visual\ninstance being paired with a spoken tag. At test time, the agent is\nthen presented with a spoken query such as â€œbutterâ€, and asked\nto identify the corresponding visual object.\nIn [12], this was investigated on a dataset of isolated spoken\ndigits paired with images. To perform multimodal matching at\ntest-time, separate speech-speech and image-image comparisons\nwere combined: a spoken query is compared to all the speech\nitems in a so-called support set, the image corresponding to the\nclosest item in the support set is determined, and this image is\nthen compared to all the items in the matching set to predict\nthe test image best matching the input speech query. To learn a\ndistance metric within each modality, transfer learning was used\nby training supervised vision and speech models on background\ntraining data not containing any of the one-shot test classes. As\nin other unimodal one-shot studies in gesture recognition [13,\n14], video [15] and robotics [16,17], this can be motivated by\nthe observation that humans can call on prior knowledge when\nlearning new concepts.\nExcept for existing knowledge, it is also conceivable that,\nbefore being shown paired examples, an agent such as the house-\nhold robot would be exposed to a large amount of unlabelled\nspeech and visual data from its environment. Some of these\nunlabelled examples could correspond to the classes of interest.\nMotivated by this observation, we ask how unsupervised models\ntrained on unlabelled in-domain data compares to transfer learn-\ning from background data for multimodal one-shot matching.\nTo learn feature representations for within-modality com-\nparisons, we speciï¬cally consider two unsupervised learning\nstrategies. An autoencoder (AE) attempts to reproduce its input\nat its output through a bottleneck feature layer. The correspon-\ndence autoencoder (CAE) tries to reproduce another instance of\nthe input at its output [18]. Since we only have unlabelled data,\nthe CAE samples nearest neighbours to obtain its output targets.\nWe compare these unsupervised models to supervised classiï¬er\nand Siamese neural networks trained on background data [12].\nEach of the models are trained separately on vision and speech\ndata and then used to estimate within-modality similarity.\nOn the same isolated digit speech-image multimodal one-\nshot matching task as in [12], we show that transfer learning out-\nperforms unsupervised modelling. We also consider approaches\nfor combining transfer and unsupervised learning. Although this\nyields improvements over a purely unsupervised model, the best\noverall performance is still achieved through transfer learning.1\n2. Multimodal one-shot matching\nWe ï¬rst describe unimodal one-shot matching and then extend it\nto the multimodal case. As an example, we consider one-shot\nspeech classiï¬cation, illustrated on the left in Figure 1(a). The\nmodel is shown a support set S, containing one isolated spoken\nword with a text label for each of the L word classes. From\nthis set, the model must learn a classiï¬er CS that can make\npredictions on an unseen test query xâˆ—\na. approach is to simply\ncompare the query with each item in the support set and then\npredict the label of the closest item, as illustrated on the right in\nFigure 1(a).\nFigure 1(b) illustrates multimodal one-shot speech-image\nmatching. Instead of labelled examples, the multimodal sup-\nport set S = {(x(i)\na , x(i)\nv )}L\ni=1 consists of pairs, where each\nisolated spoken word x(i)\na\nhas a corresponding image x(i)\nv .\nOne pair is given for each of the L classes. At test time, the\nmodel is presented with an unseen spoken query xâˆ—\na and asked\nto determine the matching image in a test (or matching) set\nMv = {(x(i)\nv )}N\ni=1 of unseen images, as illustrated on the left\nin Figure 1(b). Neither the query xâˆ—\na nor the matching set items\n1We release source code at: https://github.com/\nLeanneNortje/multimodal_speech-image_matching.\narXiv:2008.06258v1  [cs.CL]  14 Aug 2020\nQuery ğ±âˆ—ğ‘\nSupport set \nîˆ¿= {(\n,\n)\nğ±(ğ‘–)\nğ‘\ny(ğ‘–)\nğ‘}ğ¿\nğ‘–=1\nTest question:\nHow?\nTest question:\nQuery: ğ±âˆ—ğ‘\nHow?\n?\n(b)\n(a)\nSupport set \n \nîˆ¿= {(\n,\n)\nğ±(ğ‘–)\nğ‘\nğ±(ğ‘–)\nğ‘£}ğ¿\nğ‘–=1\nMatching set \n= {(\n)\nîˆ¹ğ‘£\nğ±(ğ‘–)\nğ‘£}ğ‘\nğ‘–=1\nîˆ¿â†’\n(\n)\nğ¶îˆ¿ğ±ğ‘\nîˆ¿â†’\n(\n,\n)\nğ·îˆ¿ğ±ğ‘ğ±ğ‘£\nSupport set \nîˆ¿= {(\n,\n)\nğ±(ğ‘–)\nğ‘\ny(ğ‘–)\nğ‘}ğ¿\nğ‘–=1\nSupport set \n \nîˆ¿= {(\n,\n)\nğ±(ğ‘–)\nğ‘\nğ±(ğ‘–)\nğ‘£}ğ¿\nğ‘–=1\nMatching set \n= {(\n)\nîˆ¹ğ‘£\nğ±(ğ‘–)\nğ‘£}ğ‘\nğ‘–=1\n= Â â€œfourâ€\nyÌ‚Â ğ‘\nâ€œzeroâ€\nâ€œtwoâ€\nâ€œfiveâ€\nâ€œeightâ€\nâ€œthreeâ€\nâ€œzeroâ€\nâ€œtwoâ€\nâ€œfiveâ€\nâ€œeightâ€\nâ€œthreeâ€\nâ€œeightâ€\nâ€œeightâ€\nQuery: ğ±âˆ—ğ‘\n=?\nyÌ‚Â ğ‘\n(â€œfourâ€)\nâ€œohâ€\nâ€œfourâ€\nâ€œzeroâ€\nâ€œtwoâ€\nâ€œoneâ€\nâ€œohâ€\nâ€œfourâ€\nâ€œzeroâ€\nâ€œtwoâ€\nâ€œoneâ€\nQuery: ğ±âˆ—ğ‘\nâ€œfourâ€\nFigure 1: (a) Unimodal one-shot speech classiï¬cation and (b) multimodal one-shot speech-image matching. In both cases, the left side\nillustrates the question shown at test time, and the right side illustrates how the model makes its prediction.\nMv occur exactly in the support set S. To perform this task,\nwe need to use S to construct a distance metric DS(xa, xv)\nbetween audio queries and test images.\nThe approach we use (originally proposed in [12]) is to\nreduce the task to two unimodal comparisons, as shown on the\nright in Figure 1(b). First, we compare the query xâˆ—\na to each x(i)\na\nin S to ï¬nd the queryâ€™s closest spoken neighbour in the support\nset. This closest neighbourâ€™s paired image is then compared to\neach image x(i)\nv\nin the matching set Mv. This closest matching-\nset image is then selected as the modelâ€™s prediction. In the ï¬gure,\nthis is the image of the rightmost eight.\nWe can also extend one-shot learning to K-shot learning. In\nunimodal L-way K-shot classiï¬cation, the support set S con-\ntains L classes and K labelled examples per class. In multimodal\nL-way K-shot matching, S = {(x(i)\na , x(i)\nv )}LÃ—K\ni=1\nconsists of K\nspeech-image pairs for each of the L classes.\n3. Feature representations\nIn the description above we implicitly assume that we have a\nmethod or model that can measure similarity within a modality.\nThe aim of this paper is to consider different feature represen-\ntations for these similarity comparisons, speciï¬cally comparing\ntransfer learning (used in [12]) to unsupervised feature learning.\nTo compare the different features, we use the same framework\nas in [12] where multimodal one-shot learning is performed via\ntwo unimodal comparisons (as outlined above, Figure 1(b)-right).\nNote that this is not an end-to-end approach; future work will\nexplore learning direct cross-modal matching networks.\nAs a baseline, we use raw speech and image features di-\nrectly (Â§3.1). We then consider different neural networks to\nlearn feature representations (Â§3.2 and Â§3.3). We use separate\nnetworks for learning speech and image features. For both the\nspeech and vision models, we consider two settings: training\non unlabelled in-domain data (Â§3.2) and training on labelled\nbackground data (Â§3.3).\n3.1. Raw feature matching\nAs a nearest neighbour baseline, we use cosine distance over\nimage pixels for image-to-image comparisons, and dynamic time\nwarping (DTW) over MFCCs for speech-to-speech comparisons.\n3.2. Unsupervised models on unlabelled in-domain data\nWe consider two unsupervised models trained on unlabelled in-\ndomain speech and vision dataâ€”data which includes unlabelled\ninstances of classes that we will see during one-shot testing.\nAn autoencoder (AE) is an unsupervised neural network\nwhich aims to reconstruct its input through a lower dimensional\nlatent representation that acts as an information bottleneck [19].\nAs shown in Figure 2, the AEâ€™s encoder fÎ¸(x(i)) encodes the in-\nput x(i) to the feature representation z(i). The decoder fÏ†(z(i))\ndecodes z(i) to produce the output Ë†y(i). We use a squared loss\nbetween the networkâ€™s output Ë†y(i) and the desired output y(i),\ni.e., â„“= ||y(i) âˆ’Ë†y(i)||2\n2, with the target set to y(i) = x(i).\nThe correspondence autoencoder (CAE) is identical to the\nAE but instead of reproducing the input x(i), it aims to reproduce\nanother instance x(i)\npair of the same class as the input [18], i.e. we\nset the target y(i) = x(i)\npair in the loss â„“. The intuition is that the\nCAE will produce features that are invariant to properties not\ncommon to two inputs while capturing aspects that are, such\nas the class. We consider two variants of the CAE: one trained\nfrom scratch and another pretrained as an AE before switching\nto the CAE loss (denoted as AE-CAE). To train the CAE, we\nneed pairs of items of the same class. Since our in-domain data\nis unlabelled, we use cosine distance over pixels to ï¬nd image\npairs that are most alike, and DTW to ï¬nd spoken word pairs\npredicted to be of the same type. Speaker information is used to\nensure that speech pairs are from different speakers.\nUsing unlabelled in-domain image data, we train unsuper-\nvised vision networks with the AE, CAE and AE-CAE losses;\nwe use the architecture shown in Figure 2(a), with a convo-\nlutional neural network (CNN) encoder producing the latent\nfeature vector, and a decoder with transposed convolutions. Sim-\nilarly, we use unlabelled in-domain speech data to train unsuper-\nvised speech networks using the AE, CAE and AE-CAE losses;\nwe use an encoder recurrent neural network (RNN) producing\nthe latent feature vector which is then used to condition a de-\ncoder RNN, as shown in Figure 2(b). These speech RNNs are\nsimilar to the acoustic embedding models of [20â€“23], since they\ngive a ï¬xed-sized embedding for variable duration input.\n3.3. Transfer learning from labelled background data\nWe next consider training supervised models on labelled back-\nground data. These datasets do not contain any instances of the\ntarget one-shot classes. The idea is that features learned by such\nmodels would still be useful for determining similarity on unseen\nclasses [10]. This is a form of transfer learning [24,25].\nWe speciï¬cally consider supervised classiï¬er and Siamese\nneural networks, as in [12]. We use identical architectures to the\nencoder parts of the networks in Figure 2. For the classiï¬ers, we\nadd a softmax layer after the feature embedding layer z(i) and\ntrain the networks with the multiclass log loss.\nA Siamese network does not classify an input, but mea-\nsures similarity between inputs [26]. The network consists of\nğ±(ğ‘–)\nğ‘£\nğ³(ğ‘–)\nğ‘£\nğ²Ì‚Â (ğ‘–)\nğ‘£\n(\n)\nğ‘“ğœƒğ±(ğ‘–)\nğ‘£\n(\n)\nğ‘“ğœ™ğ³(ğ‘–)\nğ‘£\nğ²Ì‚Â (ğ‘–)\nğ‘\n(\n)\nğ‘“ğœƒğ±(ğ‘–)\nğ‘\n(\n)\nğ‘“ğœ™ğ³(ğ‘–)\nğ‘\nğ³(ğ‘–)\nğ‘\nPre-processing of\nspoken word (audio)\ninto MFCC\n(a)\n(b)\n= Â â€œnineâ€\nğ±(ğ‘–)\nğ‘\nFigure 2: (a) Convolutional neural networks (CNNs) are used to learn feature representations for image data and (b) recurrent neural\nnetworks (RNNs) are used to learn feature representations for speech data.\nidentical sub networks with shared parameters; each network\nmaps its input to an embedding. Ideally, inputs of the same\nclass should have similar embeddings and inputs of different\nclasses should have different embeddings. Say we have in-\nputs x, xpair and xneg, where x and xpair are from the same\nclass and x and xneg are from different classes.\nWe want\nthe distance between the embeddings of x and xpair to be\nsmaller than those of x and xneg. We use the triplet hinge loss\nl(x, xpair, xneg) = max{0, m+d(x, xpair)âˆ’d(x, xneg)}, where\nd(x1, x2) =\n\r\rz1 âˆ’z2\n\r\r2\n2 is the squared Euclidean distance be-\ntween the embeddings z1 and z2 of x1 and x2, respectively, and\nm is a margin parameter [27, 28]. To sample negative items,\nwe use the online semi-hard mining scheme, where for each\npositive pair (x, xpair), the most difï¬cult negative pair (x, xneg)\nis sampled (meeting some constraints) [29â€“31].\nAgain, separate classiï¬er and Siamese vision CNNs and\nspeech RNNs are trained on labelled background data. We\nalso consider supervised variants of the CAE and AE-CAE ap-\nproaches, where instead of ï¬nding input-output training pairs\nbased on their nearest neighbours (Â§3.2), we train on ground\ntruth pairs from the background data (these were not considered\nin [12]). For all of the models, we use the embedding z(i) as\nrepresentation for unseen input x(i).\n4. Experimental setup\n4.1. Data\nWe follow the same setup as [12], using a dataset of paired iso-\nlated spoken digits and handwritten digit images [32]. Speech\ndata are parametrised as Mel-frequency cepstral coefï¬cients\n(MFCCs). Image pixels are normalised to [0, 1]. We use the\nTIDigits corpus as our in-domain speech data; the corpus con-\nsists of spoken digit sequences from 326 speakers [33]. We split\nthese sequences into isolated digits using forced alignments. As\nour in-domain image data, we use the MNIST corpus which\ncontains 28 Ã— 28 grayscale handwritten digit images [34]. Al-\nthough the TIDigits and MNIST datasets are labelled, note that\nwe use it as unlabelled in-domain data for the models in Â§3.2;\nwe speciï¬cally train these unsupervised models on unlabelled\nisolated examples from the training subsets of these datasets.\nAll one-shot evaluation experiments are then performed on the\nMNIST and TIDigits test subsets.\nFor background speech data, we use the Buckeye corpus of\nEnglish speech from 40 speakers [35]. We use forced alignments\nto extract a set of labelled isolated words from this set. For\nbackground image data, we use Omniglot [36], containing 1623\ntypes of handwritten characters which we invert and downsample\nto 28 Ã— 28. We ensure that there are no instances of the target\ndigit classes in either the Buckeye or Omniglot background data.\n4.2. Models\nNeural networks are implemented in TensorFlow and trained us-\ning Adam optimisation [37] with a learning rate of 10âˆ’3. Model\nhyperparameters were tuned using unimodal one-shot classiï¬ca-\ntion on test subsets of the background data, while early stopping\nwas performed on validation subsetsâ€”neither of these back-\nground sets have item or class overlap with the ï¬nal evaluation\ndata. We use a feature embedding dimensionality of 130 in all\nmodels to make results comparable. All speech RNNs take static\nMFCCs as input, but ï¬rst and second order derivatives are used\nin the DTW baseline where it is beneï¬cial.\nUnsupervised speech RNNs are trained on unlabelled iso-\nlated digits from the TIDigits training set using the AE, CAE\nand AE-CAE losses (Â§3.2). In all cases, the encoder and decoder\neach consists of three 400-unit RNN layers. Unsupervised vision\nCNNs are trained with the AE, CAE and AE-CAE losses on\nunlabelled images from the MNIST training set. The encoder\nconsists of three convolutional layers with 3Ã—3 kernels and 32,\n64 and 128 units; the decoder has the inverse architecture.\nFor transfer learning (Â§3.3), we train supervised classiï¬er\nand Siamese speech RNNs on labelled isolated words from the\nBuckeye training set. Similarly, we train supervised classiï¬er\nand Siamese vision CNNs on Omniglot. All these supervised\nmodels share the same structure as the encoder components\nfrom their unsupervised counterparts. We also train supervised\nvariants of the CAE and AE-CAE speech and vision models on\nthe labelled background data.\n4.3. Evaluation\nWe evaluate models averaged over 400 â€œepisodesâ€ [10]. To\nconstruct the support set, each multimodal episode randomly\nsamples a spoken digit and paired image for each of the L = 11\nclasses (â€œoneâ€ to â€œnineâ€, as well as â€œzeroâ€ and â€œohâ€). A matching\nset is then sampled for testing, containing ten digit images not\nin the support set. Finally, a spoken query is sampled, also not\nin the support set. The speech query then needs to be matched\nto the correct image in the matching set. The matching set only\ncontains ten digit images since there are only ten unique hand-\nwritten digit classes (both â€œzeroâ€ and â€œohâ€ are counted as correct\nif the image is that of a 0). Within an episode, ten different\nTable 1: Unimodal one- and ï¬ve-shot speech classiï¬cation.\nModel\n11-way accuracy (%)\none-shot\nï¬ve-shot\nBaseline\nDTW\n65.90\n89.45\nTransfer learning\nmodels\nClassiï¬er RNN\n86.87 Â± 0.83\n95.40 Â± 0.50\nSiamese RNN\n83.52 Â± 2.56\n94.34 Â± 0.86\nCAE RNN\n79.89 Â± 1.32\n92.16 Â± 0.90\nAE-CAE RNN\n80.02 Â± 1.04\n93.91 Â± 0.25\nUnsupervised\nmodels\nAE RNN\n53.82 Â± 1.70\n75.58 Â± 1.54\nCAE RNN\n75.80 Â± 1.76\n95.14 Â± 0.80\nAE-CAE RNN\n77.01 Â± 1.29\n93.30 Â± 0.56\nquery instances are also sampled while keeping the support and\nmatching sets ï¬xed. We report unimodal and multimodal one-\nand ï¬ve-shot matching accuracies with 95% conï¬dence intervals\naveraged over ï¬ve models trained with different seeds.\n5. Experimental Results\n5.1. K-shot unimodal speech and image classiï¬cation\nWe ï¬rst consider unimodal results in isolation. Table 1 shows\none- and ï¬ve-shot speech classiï¬cation results. All models ex-\ncept the AE RNN outperform the baseline. The classiï¬er RNN\nachieves the highest accuracies, followed by the Siamese RNN.\nIn all cases, transfer learning models outperform their unsuper-\nvised counterparts, except for the ï¬ve-shot CAE RNN.\nFor unimodal image classiï¬cation (not shown here), the\ntrends are very similar, with the classiï¬er and Siamese CNNs\nachieving accuracies of around 64% and 84% for the one- and\nï¬ve-shot cases, respectively. Again, these transfer learning mod-\nels outperform all the unimodal unsupervised image models.\n5.2. K-shot multimodal speech and image matching\nTable 2 shows multimodal one- and ï¬ve-shot results.2 In each\ncase, the same model type is used to obtain speech and image\nfeatures, e.g. the Classiï¬er row uses a CNN vision classiï¬er to\nget image features with an RNN speech classiï¬er for speech\nfeatures. In both one- and ï¬ve-shot multimodal matching, the\nclassiï¬er performs best followed closely by the Siamese model.\nNone of the unsupervised models perform as well as these mod-\nels obtained using transfer learning. For the CAE and AE-CAE\nlosses, the models trained using labelled background data also\noutperform the unsupervised variants.\n5.3. Towards combined transfer and unsupervised learning\nIt is evident that the transfer learning approach originally fol-\nlowed in [12] outperforms the unsupervised approach developed\nhere. However, the two methodologies might be complementary:\ntransfer learning from background data could capture general\nproperties within a particular modality, while unsupervised learn-\ning on unlabelled in-domain data could provide a way to tailor\nrepresentations to a speciï¬c test setting.\nAs an initial investigation, we propose two combined mod-\nels here, with results given in Table 3. The CAE with cosine\npairs (row 3) is repeated from Table 2. Instead of ï¬nding near-\n2Note that the results here are not directly comparable to that of [12].\nWe found a small bug in the validation setup of [12]; the scores across\nmodels in [12] are comparable, but lower scores are achieved when using\nthe proper validation setup used in this paper. We reran the code of [12]\nto conï¬rm the scores reported here.\nTable 2: Multimodal one- and ï¬ve-shot speech-image matching.\nModel\n11-way accuracy (%)\none-shot\nï¬ve-shot\nBaseline\nDTW + Pixels\n31.80\n41.88\nTransfer learning\nmodels\nClassiï¬er [12]\n56.80 Â± 1.19\n59.67 Â± 1.73\nSiamese [12]\n54.83 Â± 1.80\n59.25 Â± 0.79\nCAE\n46.60 Â± 0.69\n53.82 Â± 1.07\nAE-CAE\n48.15 Â± 1.21\n56.81 Â± 1.21\nUnsupervised\nmodels\nAE\n28.99 Â± 0.84\n38.68 Â± 1.51\nCAE\n42.75 Â± 0.62\n52.15 Â± 0.69\nAE-CAE\n42.81 Â± 1.01\n50.28 Â± 0.29\nTable 3: Multimodal one- and ï¬ve-shot speech-image matching\nusing models that combine transfer and unsupervised learning.\nModel\n11-way accuracy (%)\none-shot\nï¬ve-shot\nBaseline: DTW + Pixels\n31.80\n41.88\nTransfer learning: Classiï¬er [12]\n56.80 Â± 1.19\n59.67 Â± 1.73\nCAE with cosine pairs\n42.75 Â± 0.62\n52.15 Â± 0.69\nCAE with classiï¬er pairs\n48.66 Â± 1.14\n55.59 Â± 0.71\nTransfer learning + CAE ï¬ne-tuning\n54.32 Â± 2.19\n59.37 Â± 1.80\nCAE with oracle pairs\n89.19 Â± 0.69\n92.81 Â± 0.47\nest neighbours using cosine distance, we use representations\nfrom the classiï¬er (trained on background data) to ï¬nd pairs\nin the unlabelled in-domain data for training a CAE (as with\nthe standard CAE, speaker information is still used to ensure\nthat pairs are from different speakers). We see that this CAE\nwith classiï¬er pairs (row 4) gives a small improvement over the\nstandard CAE. By additionally initialising the CAE by training\nit on the labelled background data and then ï¬ne-tuning it on the\nin-domain data, we get a further improvement (Transfer learning\n+ CAE ï¬ne-tuning, row 5). Neither of these approaches, however,\noutperform the transfer learned classiï¬er (row 2).\nIn order to see if it is at all possible to achieve better perfor-\nmance with the CAE by using more accurate training pairs, we\nalso give the performance of a CAE trained only using correct\npairs in the last row of Table 3. We see that this oracle model\noutperforms all other approaches, indicating that, if we were\nable to improve the CAEâ€™s training pairs, we might be able to\ntake advantage of an unsupervised learning scheme.\n6. Conclusion\nWe have compared existing and new models for few-shot mul-\ntimodal speech-image matching. Transfer learning from back-\nground data consistently outperformed unsupervised modelling\non unlabelled in-domain data on a multimodal one-shot match-\ning benchmark. We also proposed two approaches for combining\ntransfer and unsupervised learning. Although neither improved\nthe best transfer learning approach, performance improved over\nthe standard unsupervised approach. We will therefore also con-\nsider other approaches for combining the methodologies in future\nwork. Building on models which directly maps images and unla-\nbelled speech into a joint space [38â€“41], we will also consider\nend-to-end solutions for multimodal one-shot learning.\nThis work is supported in part by the National Research Foundation\nof South Africa (grant number: 120409), a Google Faculty Award for\nHK, a DST CSIR scholarship for LN, and funding from Saigen.\n7. References\n[1] I. Biederman, â€œRecognition-by-components: A theory of human\nimage understanding.â€ Psych. Review, vol. 94, 1987.\n[2] G. A. Miller and P. M. Gildea, â€œHow children learn words,â€ SciAM,\nvol. 257, 1987.\n[3] R. L. GÂ´omez and L. Gerken, â€œInfant artiï¬cial language learning\nand language acquisition,â€ TiCS, vol. 4, 2000.\n[4] O. RÂ¨asÂ¨anen and H. Rasilo, â€œA joint model of word segmentation\nand meaning acquisition through cross-situational learning.â€ Psych.\nReview, vol. 122, 2015.\n[5] L. Fei-Fei, Fergus, and Perona, â€œA Bayesian approach to unsu-\npervised one-shot learning of object categories,â€ in Proc. ICCV,\n2003.\n[6] L. Fei-Fei, R. Fergus, and P. Perona, â€œOne-shot learning of object\ncategories,â€ IEEE Trans. PAMI, vol. 28, 2006.\n[7] B. M. Lake, R. Salakhutdinov, J. Gross, and J. B. Tenenbaum,\nâ€œOne shot learning of simple visual concepts,â€ CogSci, vol. 33,\n2011.\n[8] B. M. Lake, C.-y. Lee, J. R. Glass, and J. B. Tenenbaum, â€œOne-shot\nlearning of generative speech concepts,â€ CogSci, vol. 36, 2014.\n[9] G. Koch, â€œSiamese neural networks for one-shot image recogni-\ntion,â€ in Proc. ICML, 2015.\n[10] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wier-\nstra, â€œMatching networks for one shot learning,â€ in Proc. NIPS,\n2016.\n[11] P. Shyam, S. Gupta, and A. Dukkipati, â€œAttentive recurrent com-\nparators,â€ in Proc. ICML, 2017.\n[12] R. Eloff, H. A. Engelbrecht, and H. Kamper, â€œMultimodal one-shot\nlearning of speech and images,â€ in Proc. ICCASP, 2019.\n[13] W. Thomason and R. A. Knepper, â€œRecognizing unfamiliar ges-\ntures for human-robot interaction through zero-shot learning,â€ in\nProc. ISER, 2017.\n[14] D. Wu, F. Zhu, and L. Shao, â€œOne shot learning gesture recognition\nfrom RGBD images,â€ in Proc IEEE Comput. Soc. Conf. Comput.\nVis. Pattern Recognit., 2012.\n[15] T. Stafylakis and G. Tzimiropoulos, â€œZero-Shot keyword spotting\nfor visual speech recognition in-the-wild,â€ in Proc. ECCV, 2018.\n[16] M. R. Walter, Y. Friedman, M. Antone, and S. Teller, â€œOne-shot\nvisual appearance learning for mobile manipulation,â€ IJRR, vol. 31,\n2012.\n[17] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, â€œOne-shot\nvisual imitation learning via meta-learning,â€ arXiv:1709.04905,\n2017.\n[18] H. Kamper, M. Elsner, A. Jansen, and S. Goldwater, â€œUnsuper-\nvised neural network based feature extraction using weak top-down\nconstraints,â€ in Proc. ICCASP, 2015.\n[19] D. Chicco, P. Sadowski, and P. Baldi, â€œDeep autoencoder neural\nnetworks for gene ontology annotation predictions,â€ in Proc. ACM,\n2014.\n[20] Y.-A. Chung, C.-C. Wu, C.-H. Shen, and H.-y. Lee, â€œUnsuper-\nvised learning of audio segment representations using sequence-to-\nsequence recurrent neural networks,â€ in Proc. Interspeech, 2016.\n[21] Y.-H. Wang, H.-y. Lee, and L.-s. Lee, â€œSegmental audio Word2Vec:\nRepresenting utterances as sequences of vectors with applications\nin spoken term detection,â€ in Proc. ICCASP, 2018.\n[22] N. Holzenberger, M. Du, J. Karadayi, R. Riad, and E. Dupoux,\nâ€œLearning word embeddings: Unsupervised methods for ï¬xed-\nsize representations of variable-length speech segments,â€ in Proc.\nInterspeech, 2018.\n[23] H. Kamper, â€œTruly unsupervised acoustic word embeddings using\nweak top-down constraints in encoder-decoder models,â€ in Proc.\nICCASP, 2019.\n[24] S. J. Pan and Q. Yang, â€œA survey on transfer learning,â€ IEEE Trans.\nKnowl. Data Eng., vol. 22, 2009.\n[25] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng,\nand T. Darrell, â€œDeCAF: A deep convolutional activation feature\nfor generic visual recognition,â€ in Proc. ICML, 2014.\n[26] J. Bromley, I. Guyon, Y. LeCun, E. SÂ¨ackinger, and R. Shah, â€œSig-\nnature veriï¬cation using a â€œSiameseâ€ time delay neural network,â€\nin Proc. NIPS, 1994.\n[27] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,\nB. Chen, and Y. Wu, â€œLearning ï¬ne-grained image similarity with\ndeep ranking,â€ in Proc. CVPR, 2014.\n[28] K. M. Hermann and P. Blunsom, â€œMultilingual distributed repre-\nsentations without word alignment,â€ in Proc. ICLR, 2014.\n[29] F. Schroff, D. Kalenichenko, and J. Philbin, â€œFaceNet: A uniï¬ed\nembedding for face recognition and clustering,â€ in Proc. CVPR,\n2015.\n[30] E. Hoffer and N. Ailon, â€œDeep metric learning using triplet net-\nwork,â€ in Proc. SIMBAD, 2015.\n[31] A. Hermans, L. Beyer, and B. Leibe, â€œIn defense of the triplet loss\nfor person re-identiï¬cation,â€ arXiv:1703.07737, 2017.\n[32] K. Kashyap, â€œLearning digits via joint audio-visual representa-\ntions,â€ Thesis, Massachusetts Institute of Technology, 2017.\n[33] R. G. Leonard and G. R. Doddington, â€œTIDIGITS LDC93S10,â€\nPhiladelphia: Linguistic Data Consortium, 1993.\n[34] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, â€œGradient-based\nlearning applied to document recognition,â€ in Proc. IEEE, 1998.\n[35] M. A. Pitt, K. Johnson, E. Hume, S. Kiesling, and W. Raymond,\nâ€œThe Buckeye corpus of conversational speech:labeling conventions\nand a test of transcriber reliability,â€ Speech Commun., vol. 45,\n2005.\n[36] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, â€œHuman-\nlevel concept learning through probabilistic program induction,â€\nScience, vol. 350, 2015.\n[37] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimiza-\ntion,â€ in Proc. ICLR, 2015.\n[38] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,\nâ€œMultimodal deep learning,â€ in Proc. ICML, 2011.\n[39] D. Harwath, A. Torralba, and J. Glass, â€œUnsupervised learning of\nspoken language with visual context,â€ in Proc. NIPS, 2016.\n[40] K. Leidal, D. Harwath, and J. Glass, â€œLearning modality-invariant\nrepresentations for speech and images,â€ in Proc. ASRU, 2017.\n[41] D. Harwath, A. Recasens, D. Suris, G. Chuang, A. Torralba, and\nJ. Glass, â€œJointly discovering visual objects and spoken words from\nraw sensory input,â€ in Proc. ECCV, 2018.\n",
  "categories": [
    "cs.CL",
    "cs.CV",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2020-08-14",
  "updated": "2020-08-14"
}