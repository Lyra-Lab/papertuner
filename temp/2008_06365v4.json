{
  "id": "http://arxiv.org/abs/2008.06365v4",
  "title": "An Overview of Deep Learning Architectures in Few-Shot Learning Domain",
  "authors": [
    "Shruti Jadon",
    "Aryan Jadon"
  ],
  "abstract": "Since 2012, Deep learning has revolutionized Artificial Intelligence and has\nachieved state-of-the-art outcomes in different domains, ranging from Image\nClassification to Speech Generation. Though it has many potentials, our current\narchitectures come with the pre-requisite of large amounts of data. Few-Shot\nLearning (also known as one-shot learning) is a sub-field of machine learning\nthat aims to create such models that can learn the desired objective with less\ndata, similar to how humans learn. In this paper, we have reviewed some of the\nwell-known deep learning-based approaches towards few-shot learning. We have\ndiscussed the recent achievements, challenges, and possibilities of improvement\nof few-shot learning based deep learning architectures. Our aim for this paper\nis threefold: (i) Give a brief introduction to deep learning architectures for\nfew-shot learning with pointers to core references. (ii) Indicate how deep\nlearning has been applied to the low-data regime, from data preparation to\nmodel training. and, (iii) Provide a starting point for people interested in\nexperimenting and perhaps contributing to the field of few-shot learning by\npointing out some useful resources and open-source code. Our code is available\nat Github: https://github.com/shruti-jadon/Hands-on-One-Shot-Learning.",
  "text": "An Overview of Deep Learning Architectures in\nFew-Shot Learning Domain\nShruti Jadon\nIEEE Member\nshrutijadon@ieee.org\nAryan Jadon\nSan Jose State University\naryan.jadon@sjsu.edu\nAbstract—Since 2012, Deep learning has revolutionized Arti-\nﬁcial Intelligence and has achieved state-of-the-art outcomes in\ndifferent domains, ranging from Image Classiﬁcation to Speech\nGeneration. Though it has many potentials, our current archi-\ntectures come with the pre-requisite of large amounts of data.\nFew-Shot Learning (also known as one-shot learning) is a sub-\nﬁeld of machine learning that aims to create such models that can\nlearn the desired objective with less data, similar to how humans\nlearn. In this paper, we have reviewed some of the well-known\ndeep learning-based approaches towards few-shot learning. We\nhave discussed the recent achievements, challenges, and possi-\nbilities of improvement of few-shot learning based deep learning\narchitectures. Our aim for this paper is threefold: (i) Give a brief\nintroduction to deep learning architectures for few-shot learning\nwith pointers to core references. (ii) Indicate how deep learning\nhas been applied to the low-data regime, from data preparation\nto model training. and, (iii) Provide a starting point for people\ninterested in experimenting and perhaps contributing to the\nﬁeld of few-shot learning by pointing out some useful resources\nand open-source code. Our code is available at Github:https:\n//github.com/shruti-jadon/Hands-on-One-Shot-Learning\nIndex Terms—Deep Learning, Neural Networks, Computer\nVision, Architectures, Few-Shot Learning, Siamese Networks,\nMatching Networks, Optimization, Meta Networks, Model Ag-\nnostic Meta Learning, LSTM Meta learner, Memory Augmented\nNeural Networks.\nI. INTRODUCTION\nHumans learn new things with a very small set of examples\ne.g. a child can generalize the concept of a ”Dog” from a single\npicture but a machine learning system needs a lot of examples\nto learn its features. In particular, when presented with stimuli,\npeople seem to be able to understand new concepts quickly and\nthen recognize variations on these concepts in future precepts.\nMachine learning as a ﬁeld has been highly successful at\na variety of tasks such as classiﬁcation, web search, image\nand speech recognition. Often times however, these models\ndo not do very well in the regime of low data. This is\nthe primary motivation behind Few Shot Learning; to train\na model with fewer examples but generalize to unfamiliar\ncategories without extensive retraining. Deep learning has\nplayed an important role in the advancement of machine\nlearning, but it also requires large data-sets. Different tech-\nniques such as regularization reduces over-ﬁtting in low data\nregimes, but do not solve the inherent problem that comes with\nThis work has been as part of writing ”Hands on one-shot Learning using\nPython” book.\nfewer training examples. Furthermore, the large size of data-\nsets leads to slow learning, requiring many weight updates\nusing stochastic gradient descent. This is mostly due to the\nparametric aspect of the model, in which training examples\nneed to be slowly learned by the model into its parameters.\nIn contrast, many known non-parametric models like nearest\nneighbors do not require any training but performance depends\non a sometimes arbitrarily chosen distance metric like the L2\ndistance.few-shot learning is an object categorization problem\nin computer vision. Whereas most machine learning based\nobject categorization algorithms require training on hundreds\nor thousands of images and very large data-sets, few-shot\nlearning aims to learn information about object categories from\none, or only a few, training images [1].\nFig. 1.\nSample images of widely used few-Shot learning data-sets:mini-\nImagenet and Omniglot data-set\nA. Data-sets for Few-shot learning\nMost commonly used data-sets for few shots learning are\nMini-ImageNet, and Omniglot data-set. Omniglot is a data-\nset is specially designed to compare and contrast the learning\nabilities of humans and machines. The data-set contains hand-\nwritten characters of 50 languages (alphabets) with 1623 total\ncharacters. The data-set is divided into a background set and\nan evaluation set. Background set contains 30 alphabets (964\ncharacters) and only this set should be used to perform all\narXiv:2008.06365v4  [cs.CV]  16 Apr 2023\nlearning (e.g. hyper-parameter inference or feature learning).\nThe remaining 20 alphabets are for pure evaluation purposes\nonly. Each character is a 105 x 105 greyscale image. There are\nonly 20 samples for each character, each drawn by a distinct\nindividual(Refer Figure 2).\nMini-Imagenet as the name suggests if the small version\nof Image-Net data-set, speciﬁcally designed for Few-Shots\nLearning Image Classiﬁcation Task, Refer Figure 1.\nII. TYPES OF FEW-SHOT LEARNING APPROACHES\nIn recent years, Deep Learning ﬁeld has advanced a lot, It\ngrew from a simple Artiﬁcial Neural Network classiﬁcation\nto LSTM Networks for Language Modeling and Convolu-\ntional Neural Networks for Image processing. Though Deep\nLearning has provided many solutions for various problems\nsuch as face recognition, speech recognition etc; but there are\nstill major industries, which faces the constraints of smaller\ndata-set are yet to experience all the advantages of Deep\nLearning, such as the medical and manufacturing industries. A\nlot of research has been done in ﬁguring out architectures to\nlearn from less data regime. In this paper we have explained\nsome widely used few-shot deep learning architectures and\nattempted to explain the intuition and logic behind these\narchitectures. In broader sense, deep learning based few-shot\nlearning approaches majorly can be divided in 4 categories:\n• Data Augmentation Methods.\n• Metrics Based Methods.\n• Models Based Methods.\n• Optimization Based Methods.\nIII. DATA AUGMENTATION METHODS\nData Augmentation is a technique to enhance the amount\nand enrich the quality of training data-sets. It has been ma-\njorly appreciated by vision deep learning community to build\nbetter models. Image Augmentation methods have evolved\nin recent years from simple geometric transformations, color\nspace augmentations to generative adversarial networks, neural\nstyle transfer methods. At present, a lot of research is being\nconducted in application of augmentation methods based on\nGANs for their practical use-case in medical ﬁeld community\n[2]. In addition to general augmentation techniques, use of\nData Augmentation can also improve the performance of\nmodels and expand limited data-sets to take advantage of the\ncapabilities of big data. Though Data Augmentation is a good\napproach to solve less data problem, but it also have some\nlimitations, for example, If your current data distribution is\nskewed, that will result in augmented data distribution to be\nskewed as well. There is also a high chance of over-ﬁtting\nwith data augmentation.\nIV. METRICS BASED METHODS\nMetrics Based Methods, as the name suggests is based upon\nmetrics. Metrics play a very important role in our models,\nas we know the underlying idea of all machine learning or\ndeep learning models is mathematics, we convey our data and\nobjective to machines through the mathematical representation\nFig. 2. Sample Siamese Network Architecture\nof data, but this representation [3] can have many forms\nespecially if we go in higher dimensions, or if we wish to\nlearn these representations based on our different objectives;\nfor example, to calculate similarity between 2 images, we\ncan calculate both Euclidean distance or cosine similarity.\nThe key question is how to learn which embeddings are\nbetter representation of our task, or which loss function is\ngood for our objective. In this section, we have summarized\ntwo well known metrics based approaches: Siamese Networks\nand Matching Networks, which attempts to improve the task\nrepresentative embedding through different architecture and\ntraining procedure.\nA. Siamese Networks\nA Siamese network [4], as the name suggests, is an archi-\ntecture with two parallel layers. In this architecture, instead\nof a model learning to classify its inputs using classiﬁcation\nloss functions, the model learns to differentiate between two\ngiven inputs. It compares two inputs based on a similarity\nmetric, and checks whether they are same or not. This network\nconsists of two identical neural networks, which share similar\nparameters, each head taking one input data point (see ﬁg.\n2). In the middle layer, we extract similar kinds of features,\nas weights and biases are the same. The last layers of these\nnetworks are fed to a loss function layer, which calculates the\nsimilarity between the two inputs. The whole idea of using\nSiamese architecture [5] [1] is not to classify between classes\nbut to learn to discriminate between inputs. So, it needed a\ndifferentiating form of loss function such as the contrastive\nloss function.\n1) Understanding the data processing for Siamese Net-\nworks: is really important. For training Siamese network, we\nneed a special kind of pre-processing the data-set. We have to\ncreate pairs of data points:\n• pair of similar images, and\n• pair of dissimilar images.\nFig. 3. In the following sample architecture for matching networks, input to g(θ) are randomly sampled data (support set) from already existing examples,\nand input to f(θ) is input test data point\nWe also need to create labels accordingly for similar(y = 1),\nand dissimilar data points(y = 0); then, each pair is fed to\nthe Siamese architecture. At the end of the layer, Siamese\nNetwork uses a differentiating form of loss function known as\nContrastive Loss Function.\n2) Understanding Contrastive loss function: As the whole\nidea of using Siamese architecture is not to classify between\nclasses but to learn to discriminate between inputs. So, it\nneeded a differentiating form of loss function known as\ncontrastive loss function. It is given as follows:\nLoss = (1 −Y )1\n2D2\nw + (Y )1\n2(max(0, m −Dw)2\nHere, Dw =\np\nGw(X1) −Gw(X2 and Gw is the output from\none of sister networks, X1, X2 are input data pair.\nIf we carefully look into the equation of Contrastive Loss\nFunction, we will observe that it consists of dual terms:\n• ﬁrst part to decrease the energy of like pairs and,\n• second increase the energy of unlike pairs.\nThe ﬁrst part of the loss function is simply Mean Squared Error\nmultiplied with their respective labels; whereas, the second\npart of loss function resembles Hinge Loss, with m as a\nthreshold. Here m is a margin value which is greater than\n0. It indicates that dissimilar pairs with Root Mean Squared\nValue greater than m, won’t contribute in the dissimilar pair\nloss function. This also holds up logically as you would only\nwant to optimize the network based on pairs that are actually\ndissimilar, which network thinks are similar.\nMany times a problem can be solved using various ap-\nproaches, for example, Face Detection in our phones, Sig-\nnature veriﬁcation, etc. Image Classiﬁcation is one of the\napproaches but that will need a lot of data points; whereas, if\nwe use the Siamese network architecture of One-shot learning,\nwe can achieve greater accuracy with only a few data points.\nSiamese Network Architecture became one of the popular\nOne-shot learning architecture, which industry has adopted\nand are using for various other applications, such as Face\nDetection, Handwriting Detection, Spam Detection, and so\non. But there is still a lot of scope of improvement, for\nexample, for Contrastive Loss Function itself. In other modﬁed\nversions of Siamese Networks Architecture, it was discussed\nthat contrastive loss functions, wasn’t able to learn through\ndecision boundaries very clearly, so a new loss function was\nsuggested known as Triplet Loss, which helped the architecture\nto advance.\nNote: Transfer learning using Siamese Network Approach\nis only valid for similar domain dataset [6]. If the domain\naren’t similar, we also needs to take care of domain adaption,\nthat is, try to ensure that our training and testing data-sets are\nclose in terms of data distribution. For example, if one wants\nto create a system to test whether two hand writings are of\nthe same person, what they can do is train Siamese Network\nArchitecture on MNIST data-set through which it will learn\nhandwriting speciﬁc features such as curves, strokes of any\ncharacter.\nB. Matching Networks\nMatching networks [7], in general, proposes a framework\nwhich learns a network that maps a small data-set and an\nunlabeled example to its label. When it comes to a small\ndata-set, we come across the problem of over-ﬁtting and\nunder-ﬁtting which can be alleviated using regularization and\ndata augmentation respectively; but still, it doesn’t solve the\nroot cause of the issue. The other problem is that learning\nof parameters is very slow, requiring various weight up-\ndates using stochastic gradient descent; whereas many non-\nparametric models, such as k-nearest neighbor, doesn’t even\nrequire any form of training; it stores data-set and makes a\ndecision based on a metric. Matching networks incorporate\nthe best characteristic of both parametric and non-parametric\nmodels, also famously known as Differential Nearest neighbor.\nMatching networks are designed to be two-fold:\n1) Modeling level: At Modeling level, they proposed\nMatching nets, which uses advances made in attention\nand memory that enable fast and efﬁcient learning.\n2) Training procedure: At Training Level, they have one\ncondition that distribution of training and test set must\nbe same. For example: show a few examples per class,\nswitching the task from mini-batch to mini-batch, similar\nto how it will be tested when presented with a few\nexamples of a new task.\nIn simpler terms, matching networks learn the proper em-\nbeddings representation and use cosine similarity measure to\nensure whether a test data point is something ever seen or not.\nMatching Networks is a neural network model that imple-\nments an end-to-end training procedure that combines feature\nextraction and differentiable k-NN with cosine similarity.\nMatching network’s architecture is majorly inspired by the\nattention model and memory-based networks. In all these\nmodels, a neural attention mechanism is deﬁned to access a\nmemory matrix, which stores useful information to solve the\ntask at hand. To begin with, ﬁrst we need to understand certain\nterminologies of Matching Network:\n1) Label Set: This is the sample set of all possible cate-\ngories. For example, if we have 1,000 categories and as\npart of sampling from the database we only use only 5\ncategories.\n2) Support Set: This is the sampled input data points (for\nexample, images) of label set categories.\n3) Batch: Similar to support set, batch is also a sampled set\nconsisting of input data points of Label Set Categories.\n4) N-way K-Shot Method: Here, N is the size of the\nsupport set. In simpler terms, the number of possible\ncategories in the training set. For example, in the ﬁgure\nthat follows, we have four different types of dog breeds,\nand we are planning to do 5-shot learning method, that\nis, have at least ﬁve examples of each category. This will\nmake Matching Network Architecture as 4 way 5 shot\nlearning.\nThe key idea of Matching networks is to map images\nto an embeddings space, which also encapsulates the label\ndistribution and then project test image in the same embedding\nspace using different architecture; later, use cosine similarity\nto measure the similarity metric.\n1) Understanding data processing for Matching Networks:\nFor Matching networks architecture, we need to sample N\nlabels from training dataset, and then according to those\nlabels, sample K examples for sample-set, and B examples\nfor batch-set. As part of pre-processing data, a support set\nS of k examples will be created as (xi, yi) as shown in ﬁg\n2. After obtaining Support Set, it passes through a standard\nfeature extraction layer (g), followed by bidirectional LSTM\narchitecture, it helps us learn the probabilistic distribution\nof labels present in support set, by switching from batch to\nbatch. These extracted embeddings are called Fully Contextual\nEmbeddings.\nSimilarly, for the query image ˆx also go through similar\npath and obtain Full Context Embeddings in same embedding\nspace as that of Support set. After obtaining the outcomes\nfrom both support set and query image, they get feeded into\na kernel followed by our cosine similarity function.\n2) Understanding Training Process of Matching Networks:\nTraining process of matching networks is a bit different from\ntraditional deep learning approaches. It tries to replicate data\ndistribution of test-data in training data as well. In simpler\nterms, as we have observed as part of data pre-processing step,\nMatching Networks actually create sample sets from training\nexamples too and try to re-create scenario similar to test-sets.\nMatching Networks learns its parameters through training-set\nby creating small subsets [8] as that of a test-sets; that is, it\nis actually training the model as a few-shot learning model.\nV. MODELS BASED METHODS\nModels Based Methods are majorly inspired from how\nhumans store prior information in memory units, and have\naccess while learning new objectives. Model-Based Meta-\nLearning models depend upon architecture design for rapid\ngeneralization of few-shot learning tasks. As Humans, we\nalways have access to a large amount of memory for any task,\nand a majority of the times, how humans learn a new task is\nthrough recalling some memory and adapt to the new task in\nshort span. In this chapter, we will be studying about such deep\nlearning architectures, which are able to learn parameters with\nonly a few training steps with the help of external memory,\nsuch for of architectures are known as Model-Based methods.\nFig. 4. Neural Turing Machine [9]\nFig. 5. Memory Augmented Neural Network Approach\nA. Neural Turing Machine\nThe early development stages of the ﬁeld of Artiﬁcial Intel-\nligence was heavily dominated with the symbolic approach\n- which attempted to explain how information processing\nsystems like human brain might work in terms of symbols,\nstructures and rules to manipulate those symbols and struc-\ntures. It wasn’t until about 1980s when the ﬁeld of AI took\na different approach - connectionism, neural networks being\nthe most promising modeling technique of connectionism. The\nadvent of neural networks met with two heavy criticisms. First\nneural networks typically accept ﬁxed sized inputs and won’t\nbe much useful as most of the input in real life problems\nis variable length (sequence of frames in video, text, speech).\nSecond, neural networks seem unable to bind values to speciﬁc\nlocations within data structures which is heavily employed\nby two information systems we know of - human brain and\ncomputers. The ﬁrst criticism about variable length input was\nanswered by Recurrent Neural Networks (RNNs) which have\nachieved state-of-the-art performance on many tasks. Neural\nTuring Machines [9], the topic of discussion in this section\ntries to answer second criticism by giving external memory\nto neural networks and a method to learn to use it. We will\ndiscuss the overall architecture of Neural Turing Machine\n(NTM) in this section which is foundational to understanding\nMemory-Augmented Neural Networks (MANN) [10] which\nmodiﬁes NMT architecture and adapt it for few-shot learning\ntask.\nModern computers have advanced a lot over past 50 years\nbut they are still composed of three systems - memory, control\nﬂow and arithmetic/logic operations. There’s also biological\nevidence that working memory is crucial in quick and mean-\ningful store and retrieval of information. The same is also es-\ntablished extensively by the computational neuroscience ﬁeld.\nTaking inspirations from this, A NTM [9] is fundamentally\ncomposed of a neural network - controller and a 2D matrix\ncalled the memory bank (or memory matrix). At each time\nstep, the neural network receives some input and generates\nsome output corresponding to that input. In the process of\ndoing so, it also accesses the internal memory bank and per-\nforms read and/or write operations on it. Drawing inspiration\nfrom traditional turing machines, NMT uses the term ’head’\nto specify the memory location(s). The overall architecture is\nshown in Figure 4. There’s one challenge in this architecture, If\nwe access the memory location by specifying row and column\nindex in the memory matrix, we can’t take gradient of that\nindex. This operation is not back-propagable and would restrict\nthe training of NMT using standard back-propagation and\ngradient descent based optimization techniques. To circumvent\nthis problem, the controller of the NTM [11] interacts with\nmemory using ’blurry’ read and write operations that interacts\nwith all elements of the memory to varying degrees. More\nprecisely the controller produces weights over all memory\nlocations in a differentiable manner which helps in training the\nnetwork end-to-end using standard gradient based optimization\nmethods.\n1) Modeling: The memory matrix at time step t(Mt) has\nR rows and C columns. There’s an attention mechanism\nwhich dictates where the head should read/write from.\nThis attention vector, generated by controller is a length-\nR vector called weight vector Wt where each entry of\nthis vector Wt(i) is the weight for the ith row of the\nmemory bank. The weight vector is normalized which\nmeans it satisﬁes following conditions:\n0 ≤wt(i) ≤1 and , P\ni=1 Rwt(i) = 1\n2) Reading: The read head will return a length- C vector\nrt that is a linear combination of the memory’s rows\nMt(i) scaled by the weight vector:\nrt ←P\ni=1 R(wt(i)Mt(i))\n3) Writing: Writing is a combination of two steps: erasing\nand adding. In order to erase old data the write head\nuses an additional length- C erase vector, et along with\nthe weight vector. The following equations deﬁnes the\nintermediate step of erasing the rows.\nM erased\nt\n(i) ←Mt−1(i)[1 −wt(i)et) Finally, the write\nhead uses length- C add vector at along with M erased\nfrom above and weight vector to update the rows of the\nmemory matrix. Mt(i) ←M erased\nt\n+ wt(i)at\n4) Addressing: The key to read and write operation is the\nweight vector which indicates which rows to read/write\nfrom/to. The controller produces this weight vector in\nfour stages. Each stage produces in an intermediate\nvector which gets passed to the next stage. The ﬁrst\nstage is content-based addressing, the goal of which is\nto generate a weight vector based on how similar each\nrow is to some key vector kt of length C. More precisely,\ncontroller emits vector kt which is compared to each row\nof Mt using cosine similarity measure, deﬁned below\nK(u, v) =\nu.v\n||u||.||v||\nThe content weight vector is not normalized yet, so its\nnormalized with the following operation.\nwc\nt(i) =\nexp(βtK(kt,Mt(i)))\nP\nj exp(βtK(kt,Mt(j)))\nThe next stage is the location based addressing which\nfocuses on read/write from speciﬁc memory locations as\nopposed to read/write speciﬁc location values done by\nstage 1. In the second stage, a scalar parameter gt ∈\n(0, 1) called the interpolation gate, blends the content\nweight vector wc\nt with the previous time step’s weight\nvector wt−1 to produce the gated weighting wg\nt . This\nallows the system learn when to use (or ignore) content-\nbased addressing.\nwg\nt ←gtwc\nt + (1 −gt)wt−1\nIn the third stage, after interpolation, the head emits a\nnormalized shift weighting st to perform shift modulo\nR operation (i.e. move rows upwards or downwards).\nThis is deﬁned by the operation below.\n˜wt(i) ←PR−1\nj=0 wg\nt (j)st(i −j)\nThe fourth and ﬁnal stage, sharpening, is used to prevent\nthe shifted weight ˜wt from blurring. This is done with\na scalar γ ≥1 and applying the operation.\nwt(i) ←\n˜\nwt(i)γt\nP\nj ˜\nwt(j)γt\nAll the operations, including read, write and four stages of\naddressing are differentiable and thus the entire NMT model\ncould be trained end-to-end with back-propagation and any\ngradient descent based optimizer. The controller is a neural\nnetwork which could be a feed-forward network or even a\nrecurrent neural network like LSTMs. Now that we understand\nthe architecture and the working of NTM, we can dive into\nMemory Augmented Neural Networks which is a modiﬁcation\nof NMT and has been modiﬁed to excel at few-shot learning.\nB. Memory Augmented Neural Networks\nThe goal of Memory Augmented Neural Network (MANN)\n[10] is to excel at few-shot learning task. The NMT controller,\nas we read earlier uses both content-based addressing and\nlocation-based addressing. On the other hand, the MANN\ncontroller uses only content-based addressing. There are two\nreasons to do this. One reason is that location-based addressing\nis not required for the few-shot learning task. In this task, for\na given input, there are only two actions a controller might\nneed to take and both actions are content dependent and not\nlocation dependent. One action is that the input is very similar\nto previously seen input in which case we might what to update\nwhatever is there in memory. The other action is that current\ninput is not similar to previously seen inputs in which case we\ndon’t want to overwrite the recent information but the least\nused memory location. The memory module, in this case, is\ncalled Least Recently Used Access (LRUA) module.\n1) Reading: The read operation of MANN is very similar\nto the read operation of NTMs, with a minor difference\nthat the weight vector here uses only content bases ad-\ndressing (stage -1 of NMT addressing). More precisely,\nthe controller uses normalized read weight vector, wr\nt\nis used along with the rows of the Mt to produce read\nvector rt. rt ←PR\ni (wr\nt (i)Mt(i) The read-weight vector\nwr\nt is produced by controller deﬁned by the operations\nbelow. wr\nt =\nexp(K(kt,Mt(i))\nP\nj exp((kt,Mt(j))) where, operation K()\nis the cosine similarity , similar to the one deﬁned for\nNMTs above.\n2) Writing: To write to the memory, the controller in-\nterpolates between writing to the most recently read\nmemory rows and writing to least recently read memory\nrows. ww\nt\n←σ(α)wr\nt−1 + (1 −σ(α))wlu\nt−1 Mt(i) ←\nMt−1(i) + ww\nt (i)kt wu\nt ←γwu\nt−1 + wr\nt + ww\nt\nC. Meta Networks\nMeta Networks [12] as the name suggests is a form of\nModel-based Meta-Learning Approach. Similar to how Meta-\nLearning approaches, In meta-networks also, we have a base\nlearner and meta-learner sharing parameters where a meta-\nlearner extracts common features of all tasks and a base learner\nlearns the targeted task. As we can see in ﬁgure 7, Meta\nNetworks is also equipped with external memory, similar to\narchitecture we just read above Memory Augmented Neural\nNetworks. The underlying key idea of Meta Networks is to\nlearn weights in a fast manner for rapid generalizations, and\nit has been done by processing higher order meta information.\nIn the next section, we will go through an understanding of\narchitecture, extraction of meta information, and it’s parameter\noptimization algorithm.\nNote: Previous work on Meta-learning has formulated the\nproblem as two-level learning: slow learning of a meta-level\nmodel performing across tasks and rapid learning of a base-\nlevel model acting within each task. The goal of a meta-level\nlearner is to acquire a general knowledge of different tasks.\nThe knowledge can then be transferred to the base-level learner\nto provide generalization in the context of a single task. The\nbase and meta-level models can be framed in a single learner.\nIn usual Deep Learning Methods, weights of neural net-\nworks are updated by stochastic gradient descent and many\ntimes it takes a lot of time to train, in other words, slow\nweights update. In Meta Networks, one solution has been\nsuggested to improve upon this method: To utilize one neural\nnetwork in parallel to original neural network, to predict\nthe parameters of another neural network and the generated\nweights are called fast weights.\nIn MetaNet [12], loss gradients are used as meta information\nto enable models that learn fast weights. Slow and fast weights\nare combined to make predictions in neural networks just as\nshown in the ﬁgure above. Here, sum means element-wise\nFig. 6. Architecture of Meta Networks\nsum. Meta Networks Learning occurs at two levels in separate\nspaces: meta space and task space. here, the base learner\nlearns in task-speciﬁc space, whereas the meta-learner learns\nmeta-knowledge across different tasks. After training of Meta\nlearner, the base learner ﬁrst analyzes the input task and then\nprovides the meta-learner with feedback in the form of higher\norder meta information(gradients) to explain its own status in\nthe current task space.\nThere are two important components of a MetaNet as shown\nin ﬁgure 6:\n1) Embedding function (fθ): As part of Meta Net, we\ntrain Embedding generator function, which is then used\nto compare features of two different data points, very\nsimilar to what we have in Siamese Networks.\n2) Base Learner Model gφ: It is parameterized by weights,\nand completes the actual learning task.\nAs we have two different types of networks trained in\nparallel: fast, and slow network, therefore we also need to\nget Embedding Function and a Base Learner Model for a fast\nnetwork:\n1) Embedding Function(Fw): an LSTM architecture, for\nlearning fast weights θ of the embedding function(fθ)\nof slow network.\n2) Base Learner Model (Gv): a neural network parameter-\nized by v learning fast weights φ for the base learner\ngφfrom its loss gradients.\nFor training, Meta Networks also follow a similar training\nprocedure as of Matching Networks. Here, also we obtain\ntraining data contains multiple pairs of datasets: support set\nS = (x′\ni, y′\ni) and training set U= (xi, yi) . So, now we\nhave four networks and four sets of model parameters to\nlearn, (θ, φ, w, v). [12] also proposed the meta learning based\nalgorithm to train networks as shown in ﬁgure 7\nVI. OPTIMIZATION BASED METHODS\nIf we look into the learning method of Neural Networks\narchitectures, it usually consists of a lot of parameters and is\nFig. 7. Algorithm for One-shot learning using Meta-Net in Supervised data\nsetting [12].\noptimized using Gradient descent algorithm, which takes many\niterative steps over many examples to perform well. Gradient\nDescent Algorithm though provides a decent performance of\nmodels, but fails drastically in scenarios when there is less\namount of data. There are mainly two reasons why gradient\ndescent algorithm fails to optimize a neural network with\nsmaller data-set:\n1) If we look into variants of gradient descent’s weight\nupdating step methods (Adagrad, Adam, RMS, and so\non), can’t perform well with less number of epochs.\nEspecially when used for non-convex optimization these\nalgorithms don’t have a strong guarantee of convergence.\n2) For each new task, Neural Network has to start from\na random initialization of its parameters, which results\nin late convergence. Transfer learning has been used to\nalleviate this problem, by using a pre-trained network,\nbut it is limited to be of same domain task.\nWhat can be really helpful is to learn some common initializa-\ntion, which can be used across all domains as a good point of\ninitialization. The key idea of a Gradient Descent Algorithm\nis based on the direction of the next step, which is chosen\non the basis of probabilistic distribution assumption. So, if we\nFig. 8. Here, θ is the model’s [13] parameters and the bold black line is the\nmeta-learning phase. Let’s assume that we have 3 different new tasks 1, 2\nand 3, a gradient step is taken for each task (the gray lines)\nFig. 9. Optimization Algorithm for Model Agnostic Meta Learning [13].\nsomehow are able to approximate that probabilistic distribution\ncompletely, we will be able to optimize the network, with\nonly a few steps. This is the basic idea of Optimization-based\nalgorithms: Model-Agnostic Meta-Learning [13] and LSTM-\nMeta Learner [14]\nA. Model Agnostic Meta Learning\nThe goal of Model Agnostic Meta-Learning [13] architec-\nture is to quickly learn a new task from a small amount\nof new data. The key idea of this approach is to train the\nmodels’ initial parameters using a different data-set, such that\nas the new task comes, the model gives maximum perfor-\nmance by using already initialized parameters, to ﬁne-tune\nthe architecture through one or more gradient steps. As we\nknow that Neural Networks use Gradient descent algorithm to\ntrain, so the method of training a model’s parameters such\nthat a few gradient steps can optimize loss function, and\ngive good results, can be viewed from a feature learning\nstandpoint as building an internal representation. As it is very\ngeneric, therefore can be used for various tasks. The primary\ncontribution of this work is a simple model and task-agnostic\nalgorithm that trains a model’s parameters such that a small\nnumber of gradient updates will lead to fast learning on the\nnew task. The Objective of Model-Agnostic Meta-Learning\n(MAML) [13] is to provide a good initialization of a model’s\nparameters to achieve optimal fast learning on a new task\nwith less number of gradient steps. It also attempts to avoid\noverﬁtting scenarios, which happens while training a Neural\nNetwork with less amount of data architecture.\nAs we can see in the ﬁgure 8 parameters θ are close to all\nthe 3 optimal parameters of task 1, 2, and 3 which makes θ the\nbest parameters initialization that can quickly adapt to different\nnew tasks. As a result, only a small change in the parameters θ\nwill lead to an optimal minimization of the loss function of any\ntask. This is the key idea of MAML, that somehow we learn\nθ through the original dataset, so while ﬁne-tuning on the real\ndataset, we just have to move a small step. For One/Few Shots\nLearning, ﬁrst, we prepare our data. For a particular task say,\nTi we have data samples which are forming task distribution\nof form P(T) (Probabilistic distribution across all tasks). Our\naim is to train a model to learn task Ti from K data points\nand feedback generated by Loss , L(Ti). During meta-training,\na task Ti is sampled from P(T) i.e; training data available,\nthen model is trained with K samples and feedback from the\ncorresponding loss L(Ti) from Ti , and then tested on new\nsamples from Ti . The model is then improved by considering\nhow the test error on new data changes with respect to the\nparameters. In effect, the test error on sampled tasks Ti serves\nas the training error of the meta-learning process. At the end\nof the training, new tasks are sampled from p(T), and meta-\nperformance is measured by the model’s performance after\nlearning from K samples.\nLTi(fφ) = P\nxj,yj∽Ti ||fφ(x(j)) −y(j)||2\n2\nwhere x(j), y(j) are sampled from task Ti. To learn more in\ndetail about steps followed in maml optimization Algorithms,\nplease take a look at the algorithm published in the paper [13].\nB. LSTM Meta Learner\nAs the name suggests, LSTM meta-learner [14] algorithm\nis speciﬁcally designed on LSTM Architecture because a cell-\nstate update in LSTM is similar to gradient-based update\nin backpropagation, and information about the history of\ngradients help in optimizing model better. This Optimization\nAlgorithm is trained to optimize a learner neural network\nclassiﬁer. It captures the knowledge of both the short term\nof a particular task and common long term.\nLSTM meta learner is heavily inspired from connection in\nlogic of Gradient descent and LSTM cell update. Ravi et al.,\n[14] explained that iff we look into update method of Gradient\nDescent, we will see an equation like this:\nθt = θt−1 −αt ▽Lt\nhere, θt is parameters at t time step, ▽Lt is gradient of\nLoss at t, and αt is learning rate at time t.\nOne observation that authors of LSTM meta learner made\nwas, that this update looks very similar to how cells get\nupdated in LSTMs.. So, The Key idea of Meta-Learner LSTM\nis to learn an update rule for training a neural network. Lets\nﬁrst dive in how a cell state is deﬁned in an LSTM.\nct = ft ⊙ct−1 + it ⊙¯ct\nFig. 10.\nThe architecture of LSTM Meta Networks [14] consists of 2\nparts: Learner, and Meta-Learner. Learner trains for T-steps and sends its\nupdated parameters to Meta-Learner, then Meta-Learner evaluates updates,\nand optimize its parameters. This whole process runs for D steps.\nIf we put ft = 1, ct−1 = θt−1, it = αt and ¯ct = ▽Lt ,\nwe will get a gradient descent update rule. Considering this,\nLogically, we want to learn it, as that is essentially similar\nto estimating the learning rate of gradient descent. so, LSTM\nMeta Learner deﬁne it as:\nit = σ(WI.[▽Lt, Lt, θt−1, it−1] + bI)\ni.e; a sigmoid function with a combination of current\ngradient, Loss, and previous learning rate it−1.\nFor ft, it should be 1, but to avoid problems of shrinking\ngradients, and exploding gradients. It has been chosen as:\nft = σ(WF .[▽Lt, Lt, θt−1, ft−1] + bF )\ni.e; sigmoid function with a combination of current gradient,\nLoss, and forget gate.\n1) Data Pre-processing: In Typical Machine Learning set-\nting, give a data-set D, we divide into 2 parts: training set, and\na test set. Whereas in Meta-Learning Setting, we have Meta-\nsets say D , consists of multiple sets of different task examples.\nFor each D ∈D consists of Dtrain and Dtest , so for K-Shot\nLearning each Dtrain consists of K × N examples, where N\nis the number of classes. In Meta-Learning, since beginning\ndata is divided into 3 parts: Dmeta−train, Dmeta−validation,\nand Dmeta−test Here the objective is to use Dmeta−train is\nfor training a learning procedure that can take as input one of\nits training sets Dtrain and produce a classiﬁer (learner) that\nachieves high average classiﬁcation performance.\n2) Model Training: Following the concept of Matching\nNetworks, This Optimization Algorithm also tried to match\ntraining conditions to test time conditions, as they have been\nproven to perform really well at a task. To make similar\nconditions, when considering each dataset D , it uses loss Ltest\nproduced by D’s test set Dtest , for step by step knowledge\nrefer to Algorithm mentioned in paper [14].\nSo, while iterating for T steps overall D’s training data,\nLSTM meta-learner receives some parameter updates from\nlearner. After T-steps, ﬁnal parameters are then used to evalu-\nate Test set, and make updates on meta-learner. To understand,\na pictorial representation of architecture, refer architecture in\n10\nFig. 11. Semi Supervised Learning\nVII. OTHER APPROACHES FOR FEW-SHOT LEARNING\nAs we know few-shot Learning is a sub-ﬁeld of Machine\nLearning, there are different relevant solutions which are very\nclose to few-shot Learning approach, but yet different in their\nsolution approach. Such problems can be solved by using few-\nshot Learning algorithms as well. Let’s go through each of\nsuch ﬁelds of Machine Learning, and observe how they are\nclose to few-shot Learning Problem:\n1) Semi-Supervised Learning\n2) Imbalanced Learning\n3) Transfer Learning\nA. Semi-Supervised Learning\nSemi-supervised learning [15] is a machine learning tech-\nnique that involves training a model on both labeled and\nunlabeled data. In traditional supervised learning, models are\ntrained only on labeled data, which is expensive and time-\nconsuming to obtain. However, with semi-supervised learn-\ning, models can leverage the vast amount of unlabeled data\navailable to improve performance on labeled data.\nSuppose we have 10,000 data points, out of which only\n20,000 are labeled, and the rest 80,000 are unlabeled. In such\ncases, we can take the help of semi-supervised learning to train\na more accurate model. Semi-supervised learning [15] goes\nthrough a pseudo-labeling technique to increase the training\nset. In this technique, we train a model using the 20,000\nlabeled dataset and use it on an equally sized test dataset\nto create pseudo-labels for the unlabeled data points. After\nobtaining pseudo-labels, we concatenate the real labels with\nthe pseudo-labels and real features with the pseudo-features.\nWe then train a new model on the concatenated dataset, which\nhas been shown to be more accurate than the initial model. We\nrepeat this process until optimal accuracy is achieved.\nSemi-supervised learning works well in scenarios where\nlabeled data is scarce, and there is a vast amount of unlabeled\ndata available. By leveraging the unlabeled data, models can\ngain a better understanding of the population structure in\ngeneral, leading to improved performance on the labeled data.\nSemi-supervised learning has been applied in various ﬁelds\nsuch as computer vision, natural language processing, and\nspeech recognition.\nHowever, it is important to note that semi-supervised learn-\ning is not a panacea, and it may not always lead to improved\nperformance. The success of semi-supervised learning depends\non various factors such as the quality of the unlabeled data,\nthe complexity of the task, and the type of model used. Addi-\ntionally, the pseudo-labeling technique may introduce errors if\nthe model makes incorrect predictions on the unlabeled data.\nTherefore, it is crucial to validate the accuracy of the pseudo-\nlabels before using them to train the model.\nB. Imbalanced Learning\nIn Imbalanced Learning [16] scenario, we have an imbal-\nanced data-set i.e; we have more samples from one class,\nwhereas very few examples of other categories, this is also\npopularly known as skewed distribution data-set. Let’s take\na look at some popular methods for dealing with a skewed\ndata-set.\n1) Choice of Metric: There are various forms of metrics\n[17] that can help in assessing the accuracy of a model,\nsuch as Confusion Matrix, Precision, Recall, and F1-\nScore.\n2) Choice of Machine Learning Algorithm: Parametric\nAlgorithms learn their parameters through the data-set,\nso if the data-set is biased, it is most likely that the\nparametric model will also be biased. Non-parametric\napproaches such as K-Nearest Neighbor, Decision Trees\netc. Ensembles such as Random Forest, Adaboost, XG-\nBoost are proven to be the best approaches when it\ncomes to a biased data-set.\n3) Choice of Data Sampling Methods: Data Sampling can\nalso be considered to ensure that data-set doesn’t remain\nskewed.\nThis is close to few-shot Learning, as the machine learning\nmodel we are expected to create should be able to learn\ndistribution from a few examples.\nC. Transfer Learning\nTransfer Learning [18] is a machine learning technique that\ninvolves the application of knowledge gained from one task to\nanother related task. It is based on the idea that the knowledge\nacquired by a model while solving one task can be leveraged\nto help improve the performance of a different task.\nThe primary motivation behind transfer learning is to reduce\nthe need for large amounts of labeled data for a new task. In\nmany real-world scenarios, it is expensive and time-consuming\nto collect large amounts of labeled data. However, with transfer\nlearning, the knowledge gained from solving a related task can\nbe used to train a model on the new task, reducing the amount\nof labeled data required.\nThere are three main types of transfer learning:\n1) Inductive Transfer Learning: In this approach, the\nmodel is ﬁrst trained on a large, diverse dataset and\nthen ﬁne-tuned on the new task. The idea behind this\napproach is that the model has already learned features\nthat are relevant to the new task, and ﬁne-tuning helps\nto reﬁne those features.\n2) Transductive Transfer Learning: This approach in-\nvolves transferring knowledge between related tasks,\nwhere the goal is to learn the mapping between two\ndifferent domains. For example, mapping between two\ndifferent languages, or between image and text domains.\n3) Unsupervised Transfer Learning: This approach in-\nvolves using the knowledge gained from unsupervised\nlearning to improve the performance of a supervised\nlearning task. In unsupervised learning, the model learns\nto represent data in a way that captures its underlying\nstructure. This representation can then be used to im-\nprove the performance of a supervised learning task that\nuses the same data.\nTransfer learning has been applied in various ﬁelds such\nas computer vision, natural language processing, and speech\nrecognition. For instance, in computer vision, transfer learning\nhas been used to improve the performance of object recogni-\ntion, object detection, and image classiﬁcation. Similarly, in\nnatural language processing [19], transfer learning has been\nused to improve the performance of tasks such as sentiment\nanalysis, named entity recognition, and machine translation.\nIn summary, Transfer Learning is a powerful technique that\nenables models to leverage knowledge gained from one task to\nimprove performance on another related task. It is particularly\nuseful in scenarios where labeled data is scarce, and it has\nbeen applied in various ﬁelds to improve the performance of\nmachine learning models.\nVIII. CONCLUSION\nIn order to develop a ﬂawless few-shot learning method, it\nis essential to have access to impeccable information embed-\ndings, efﬁcient memory storage, and advanced optimization\nalgorithms beyond the scope of gradient descent. Our research\ndelves into various algorithms, including Matching Networks\nand Meta Networks, to explore their efﬁcacy in achieving\nstate-of-the-art accuracy in classiﬁcation tasks. However, for\nmore complex tasks such as object detection and image\nsegmentation, our models are still facing difﬁculties.\nAlthough there are various theoretical applications for few-\nshot learning, it has only recently gained momentum in real-\nworld scenarios. Advancements in this ﬁeld have been applied\nto diverse domains, such as writing SQL codes, enhancing\ndeformed medical images, and verifying signatures. Although\nmany other domains are still under research, major companies\nsuch as OpenAI, Google, Microsoft, and Amazon are investing\nsigniﬁcantly in AI research.\nThe successful implementation of few-shot learning can\nrevolutionize the world by creating a human-like brain capable\nof detecting rare diseases and optimizing supply-chain models\nto tackle global food crises. The possibilities are limitless, and\nsolving the challenges associated with few-shot learning can\nbe a game-changer for humanity.\nREFERENCES\n[1] S. Jadon and A. Garg, Hands-On One-shot Learning with Python. Packt\nPublishing, 2020.\n[2] S. Jadon, O. P. Leary, I. Pan, T. J. Harder, D. W. Wright, L. H. Merck,\nand D. L. Merck, “A comparative study of 2d image segmentation\nalgorithms for traumatic brain lesions using ct data from the protectiii\nmulticenter clinical trial,” in Medical Imaging 2020: Imaging Informat-\nics for Healthcare, Research, and Applications, vol. 11318, p. 113180Q,\nInternational Society for Optics and Photonics, 2020.\n[3] S. Jadon, “Introduction to different activation functions for deep learn-\ning,” Mar 2018.\n[4] G. Koch, R. Zemel, and R. Salakhutdinov, “Siamese neural networks for\none-shot image recognition,” in ICML deep learning workshop, vol. 2,\n2015.\n[5] S. Jadon and A. A. Srinivasan, “Improving siamese networks for one\nshot learning using kernel based activation functions,” arXiv preprint\narXiv:1910.09798, 2019.\n[6] C. Vargas, Q. Zhang, and E. Izquierdo, “One shot logo recognition based\non siamese neural networks,” in Proceedings of the 2020 International\nConference on Multimedia Retrieval, pp. 321–325, 2020.\n[7] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., “Matching\nnetworks for one shot learning,” in Advances in neural information\nprocessing systems, pp. 3630–3638, 2016.\n[8] S. Jadon, “Ssm-net for plants disease identiﬁcation in lowdata regime,”\narXiv preprint arXiv:2005.13140, 2020.\n[9] A. Graves, G. Wayne, and I. Danihelka, “Neural turing machines,” arXiv\npreprint arXiv:1410.5401, 2014.\n[10] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap,\n“One-shot learning with memory-augmented neural networks,” arXiv\npreprint arXiv:1605.06065, 2016.\n[11] M. Collier and J. Beel, “Implementing neural turing machines,” in\nInternational Conference on Artiﬁcial Neural Networks, pp. 94–104,\nSpringer, 2018.\n[12] T. Munkhdalai and H. Yu, “Meta networks,” Proceedings of machine\nlearning research, vol. 70, p. 2554, 2017.\n[13] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for\nfast adaptation of deep networks,” arXiv preprint arXiv:1703.03400,\n2017.\n[14] S. Ravi and H. Larochelle, “Optimization as a model for few-shot\nlearning,” 2016.\n[15] X. Zhu and A. B. Goldberg, “Introduction to semi-supervised learning,”\nSynthesis lectures on artiﬁcial intelligence and machine learning, vol. 3,\nno. 1, pp. 1–130, 2009.\n[16] G. Haixiang, L. Yijing, J. Shang, G. Mingyun, H. Yuanyue, and\nG. Bing, “Learning from class-imbalanced data: Review of methods and\napplications,” Expert Systems with Applications, vol. 73, pp. 220–239,\n2017.\n[17] A. Jadon, A. Patil, and S. Jadon, “A comprehensive survey of regres-\nsion based loss functions for time series forecasting,” arXiv preprint\narXiv:2211.02989, 2022.\n[18] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans-\nactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–\n1359, 2009.\n[19] A. Patil and A. Jadon, “Auto-labelling of bug report using natural\nlanguage processing,” arXiv preprint arXiv:2212.06334, 2022.\n[20] G. Keren, M. Schmitt, T. Kehrenberg, and B. Schuller, “Weakly\nsupervised one-shot detection with attention siamese networks,” stat,\nvol. 1050, p. 12, 2018.\n[21] S. Mshir and M. Kaya, “Signature recognition using machine learning,”\nin 2020 8th International Symposium on Digital Forensics and Security\n(ISDFS), pp. 1–4, 2020.\n[22] J. Thathachar, T. Kornuta, and A. S. Ozcan, “Encoder-decoder memory-\naugmented neural network architectures,” Mar. 19 2020. US Patent App.\n16/135,990.\n[23] M. Collier and J. Beel, “Memory-augmented neural networks for ma-\nchine translation,” arXiv preprint arXiv:1909.08314, 2019.\n[24] F. Shen, S. Yan, and G. Zeng, “Meta networks for neural style transfer,”\narXiv preprint arXiv:1709.04111, 2017.\n[25] Z. Chen, Y. Fu, Y.-X. Wang, L. Ma, W. Liu, and M. Hebert, “Image\ndeformation meta-networks for one-shot learning,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition,\npp. 8680–8689, 2019.\n[26] Z. Liu, R. Zhang, Y. Song, and M. Zhang, “When does maml work\nthe best? an empirical study on model-agnostic meta-learning in nlp\napplications,” arXiv preprint arXiv:2005.11700, 2020.\n[27] H. S. Behl, A. G. Baydin, and P. H. Torr, “Alpha maml: Adaptive model-\nagnostic meta-learning,” arXiv preprint arXiv:1905.07435, 2019.\n[28] H. Li, D. Eigen, S. Dodge, M. Zeiler, and X. Wang, “Finding task-\nrelevant features for few-shot learning by category traversal,” in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 1–10, 2019.\n[29] S.-A. Rebufﬁ, S. Ehrhardt, K. Han, A. Vedaldi, and A. Zisserman,\n“Semi-supervised learning with scarce annotations,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops, pp. 762–763, 2020.\n[30] C. Huang, Y. Li, C. C. Loy, and X. Tang, “Learning deep representation\nfor imbalanced classiﬁcation,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 5375–5384, 2016.\n[31] A. Fern´andez, S. Garc´ıa, M. Galar, R. C. Prati, B. Krawczyk, and\nF. Herrera, Learning from imbalanced data sets. Springer, 2018.\n[32] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and\nQ. He, “A comprehensive survey on transfer learning,” Proceedings of\nthe IEEE, 2020.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2020-08-12",
  "updated": "2023-04-16"
}