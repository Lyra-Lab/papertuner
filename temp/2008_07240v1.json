{
  "id": "http://arxiv.org/abs/2008.07240v1",
  "title": "Model-Reference Reinforcement Learning for Collision-Free Tracking Control of Autonomous Surface Vehicles",
  "authors": [
    "Qingrui Zhang",
    "Wei Pan",
    "Vasso Reppa"
  ],
  "abstract": "This paper presents a novel model-reference reinforcement learning algorithm\nfor the intelligent tracking control of uncertain autonomous surface vehicles\nwith collision avoidance. The proposed control algorithm combines a\nconventional control method with reinforcement learning to enhance control\naccuracy and intelligence. In the proposed control design, a nominal system is\nconsidered for the design of a baseline tracking controller using a\nconventional control approach. The nominal system also defines the desired\nbehaviour of uncertain autonomous surface vehicles in an obstacle-free\nenvironment. Thanks to reinforcement learning, the overall tracking controller\nis capable of compensating for model uncertainties and achieving collision\navoidance at the same time in environments with obstacles. In comparison to\ntraditional deep reinforcement learning methods, our proposed learning-based\ncontrol can provide stability guarantees and better sample efficiency. We\ndemonstrate the performance of the new algorithm using an example of autonomous\nsurface vehicles.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n1\nModel-Reference Reinforcement Learning for Collision-Free\nTracking Control of Autonomous Surface Vehicles\nQingrui Zhang1,2, Wei Pan2, and Vasso Reppa1\nAbstract—This paper presents a novel model-reference rein-\nforcement learning algorithm for the intelligent tracking control\nof uncertain autonomous surface vehicles with collision avoidance.\nThe proposed control algorithm combines a conventional control\nmethod with reinforcement learning to enhance control accuracy\nand intelligence. In the proposed control design, a nominal system\nis considered for the design of a baseline tracking controller\nusing a conventional control approach. The nominal system also\ndeﬁnes the desired behaviour of uncertain autonomous surface\nvehicles in an obstacle-free environment. Thanks to reinforcement\nlearning, the overall tracking controller is capable of compensat-\ning for model uncertainties and achieving collision avoidance at\nthe same time in environments with obstacles. In comparison to\ntraditional deep reinforcement learning methods, our proposed\nlearning-based control can provide stability guarantees and better\nsample efﬁciency. We demonstrate the performance of the new\nalgorithm using an example of autonomous surface vehicles.\nIndex Terms—Reinforcement learning, collision avoidance,\ntracking control, autonomous surface vehicle.\nI. INTRODUCTION\nAutonomous surface vehicles (ASVs) have attracted ex-\ntensive research attention, due to their advantages in many\napplications, such as environmental monitoring [1], resource\nexploration [2], enhancing the efﬁciency and safety of water-\nborne transportation [3], [4], and many more [5], [6]. Successful\nlaunch of ASVs in real life requires avoiding collisions with\nobstacles [7] and accurate tracking along a desired trajectory\n[8]. Both collision avoidance and tracking control are the\nmajor research topics for ASVs in the maritime engineering\n[4], [9]–[15]. However, accurate tracking control for ASVs\nin the presence of obstacles is challenging, as ASVs are\nsubject to uncertain nonlinear hydrodynamics and unknown\nenvironmental disturbances [16]. Due to the complexity of the\nproblem, collision avoidance and tracking control are mostly\nstudied in a separate manner.\nCollision avoidance methods for ASVs are categorized into\npath/motion planning approaches [17]–[20] and optimization-\nbased algorithms [14], [21]–[24]. In the path/motion planning\napproaches, a collision-free reference trajectory or motion is\ngenerated based on either off-line or on-line methods, e.g.,\nA∗[17], RRT∗[18], [20], and potential ﬁeld methods [25],\netc. It is assumed that the generated collision-free reference\ntrajectory can be tracked with high accuracy by the ASV based\non a well-designed control module. Thus, collision avoidance\nfollowing the path/motion planning approaches may fail for\nuncertain systems that lack valid tracking controllers. Due\n1Department of Maritime and Transport Technology, Delft University of\nTechnology, Delft, the Netherlands Qingrui.Zhang@tudelft.nl;\nV.Reppa@tudelft.nl\n2Department of Cognitive Robotics, Delft University of Technology, Delft,\nthe Netherlands Wei.Pan@tudelft.nl\nto the two-module design feature, there always exists a time\ndelay for the ASV to apply collision avoidance actions, as\nthe inner-loop controller needs time to react to changes in the\nreference trajectories. Such a time delay will also downgrade\nthe performance of the path/motion planning approaches in\nenvironments with fast-moving obstacles.\nThe optimization-based algorithms can directly ﬁnd a control\nlaw with collision avoidance by optimizing a certain objective\nfunction, e.g. model predictive control (MPC) [14], [22] and\nreinforcement learning (RL) [23]. They potentially have a better\nperformance than the path/motion planning approaches in dy-\nnamic environments. However, collision avoidance algorithms\nbased on MPC suffer from high computational complexity and\nrely on accurate modeling of ASV systems [14], [22]. They will,\ntherefore, experience dramatic degradation in performances\nfor uncertain ASVs. In comparison to MPC, RL can learn an\nintelligent collision avoidance law from data samples [26], [27],\nwhich can signiﬁcantly reduce the dependence on modeling\nefforts and thus make RL very suitable for uncertain ASVs.\nTracking control algorithms for uncertain systems including\nASVs mainly lie in four categories: 1) robust control that is the\n“worst-case” design for bounded uncertainties and disturbances\n[28]; 2) adaptive control that estimates uncertainty parameters\n[8], [29], [30]; 3) disturbance observer (DO)-based control that\ncompensates uncertainties and disturbances in terms of the\nobservation technique [31]–[33]; and 4) reinforcement learning\n(RL) that learns a control law from data samples [13], [34]. In\nrobust control, uncertainties and disturbances are assumed to\nbe bounded with known boundaries [35]. As a consequence,\nrobust control will lead to conservative high-gain control laws\nthat might degrade the control performances (i.e., overshoot,\nsettling time, and stability margins) [36]. Adaptive control can\nhandle varying uncertainties with unknown boundaries, but\nsystem uncertainties are assumed to be linearly parameterized\nwith known structure and unknown parameters [37]–[39]. DO-\nbased control can adapt to both uncertainties and disturbances\nwith unknown structures [40], [41]. However, the frequency\ninformation of uncertainty and disturbance signals are necessary\nin the DO-based control for the choice of proper control gains,\notherwise, it is highly possible to end up with a high-gain\ncontrol law [41], [42]. In general, comprehensive modeling and\nanalysis of systems are essential for all model-based methods.\nIn comparison to existing model-based methods, RL is\ncapable of learning a complex tracking control law with\ncollision avoidance from data samples using much less model\ninformation [27]. It is, therefore, more promising in controlling\nsystems subject to massive uncertainties and disturbances as\nASVs [13], [34] and meanwhile achieving collision avoidance\n[23], given the sufﬁciency and good quality of collected data.\nNevertheless, it is challenging for model-free RL to ensure\narXiv:2008.07240v1  [eess.SY]  17 Aug 2020\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n2\nclosed-loop stability, though some research attempts have been\nmade [43], [44]. Model-based RL with stability guarantee has\nbeen investigated by introducing a Lyapunov constraint into\nthe objective function [45]. However, the model-based RL\nwith stability guarantees requires an admissible control law —\na control law that makes the original system asymptotically\nstable — for the initialization. Both the Lyapunov candidate\nfunction and complete system dynamics are assumed to be\nLipschitz continuous with known Lipschitz constants for the\nconstruction of the Lyapunov constraint. It is challenging to\nﬁnd the Lipschitz constant of an uncertain system. Therefore,\nthe introduced Lyapunov constraint function is restrictive, as\nit is established based on the worst-case consideration [45].\nWith the consideration of the merits and limitations of\nexisting RL methods, we propose a novel learning-based\ncontrol algorithm for uncertain ASVs with collision avoidance\nby combining a conventional control method with deep RL\nin this paper. The proposed learning-based control design,\ntherefore, consists of two components: a baseline control\nlaw that stabilizes a nominal ASV system and a deep RL\ncontrol law that compensates for system uncertainties and also\nachieves intelligent collision avoidance. Such a design structure\nhas several advantages over both conventional model-based\nmethods and pure deep RL methods. First of all, in relation to\nthe “model-free” feature of deep RL, we can learn from data\nsamples a control law that directly compensates for system\nuncertainties without exploiting their structures, boundaries,\nor frequencies [46]. Intelligent collision avoidance can also\nbe learned by the deep RL. Second, closed-loop stability is\nguaranteed by the overall learned control law for the tracking\ncontrol in obstacle-free environments, if the baseline control\nlaw is able to stabilize the ASV system at least locally. Without\nintroducing a restrictive Lyapunov constraint into the objective\nfunction of the policy improvement in RL as in [45], we can\navoid exploiting the Lipschitz constant of the overall system\nand potentially produce less conservative results. Lastly, the\nproposed design is potentially more sample efﬁcient than an RL\nalgorithm learning from scratch – that is, fewer data samples\nare needed for training. In RL, a system learns from mistakes,\ndemanding a lot of trials and errors. In our design, the baseline\ncontrol that can stabilize the overall system, can help to exclude\nunnecessary mistakes. Hence, it provides a good starting point\nfor the RL training. A similar idea is used in [47] for the control\nof quadrotors. The baseline control in [47] is constructed based\non the full accurate model of a quadrotor system, but stability\nanalysis is missing. The design in [47] is deployed as the inner-\nloop control to stabilize the attitude of quadrotors, but it is\nnot designed for tracking control with collision avoidance. The\noverall contributions of this paper are summarized as below.\n1) A new formulation method is presented for the learning-\nbased control of ASV systems. With the new formulation,\nwe can leverage the advantages of both model-based\ncontrol methods and data-driven methods such as RL.\n2) A model-reference RL algorithm is developed for the\ncollision-free tracking control of uncertain ASVs. The\nproposed model-reference RL algorithm doesn’t need the\nstructures, boundaries, or frequencies of uncertainties. It\nvp\nInertial \nframe\nBody frame\n(xp, yp)\n p\nup\nYI\nEast\nXI\nNorth\nXB\nYB\n⌘= [xp, yp,  p]T\n⌫= [up, vp, rp]T\nFig. 1: Coordinate systems of an autonomous surface vehicle\nis potentially more efﬁcient than a RL algorithm that\nlearns from scratch. Closed-loop stability is guaranteed\nfor the overall learning-based control law.\n3) The proposed model-reference RL algorithm is analyzed\nsystematically. Convergence analysis is provided. Closed-\nloop stability is analyzed for the tracking control at the\nobstacle-free environments.\nSome of the work in this paper has been accepted to be\npresented in the 59th IEEE Conference on Decision and\nControl (CDC) that will be hosted at December, 2020. The\nonline version of our CDC paper can be found in [48]. In our\nCDC paper, the collision avoidance problem is not addressed.\nMathematical proofs are not provided for the convergence\nanalysis. Rigorous the closed-loop stability proof is also missing\nin the CDC paper. Besides, in this paper, we present more\ndetails on the problem formulation and algorithm design,\nincluding the choices of the control policies in RL, discussions\nof reward functions, and descriptions of the deep neural\nnetworks, etc. More simulation results are given in this paper.\nThe rest of the paper is organized as follows. In Section II, we\npresent the ASV dynamics. The model-reference reinforcement\nlearning control is formulated at length in Section III, including\nthe problem formulation, basic concepts of reinforcement\nlearning, and choices of reward functions. In Section IV, the\nmodel-reference reinforcement Learning is developed based\ndeep neural networks. Section V presents the details on\nthe analysis of the proposed model-reference reinforcement\nlearning algorithm, including the convergence analysis and\nstability analysis. Section VI provides the simulation results\nof the application of the algorithm to an example of ASVs.\nConclusion remarks are given in Section VII.\nII. AUTONOMOUS SURFACE VEHICLE DYNAMICS\nThe full dynamics of autonomous surface vehicles (ASVs)\nhave six degrees of freedom (DOF), including three linear\nmotions and three rotational motions [16]. In most scenarios, we\nare interested in controlling the horizontal motions of (ASVs),\nignoring the vertical, rolling, and pitching motions [49].\nLet xp ∈R and yp ∈R be the horizontal position\ncoordinates of an ASV in the inertial frame and ψp ∈R the\nheading angle as shown in Figure 1. In the body frame, up ∈R\nand vp ∈R to represent the linear velocities in surge (x-axis)\nand sway (y-axis), respectively. The heading angular rate is\ndenoted by rp ∈R. The general 3-DOF nonlinear dynamics\nof an ASV are described by\n\u001a\n˙η\n=\nR (η) ν\nM ˙ν + (C (ν) + D (ν)) ν + G (ν)\n=\nτ\n(1)\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n3\nwhere η = [xp, yp, ψp]T ∈R3 is a generalized coordinate\nvector, ν = [up, vp, rp]T\n∈R3 is the speed vector, M\nis the inertia matrix, C (ν) denotes the matrix of Cori-\nolis and centripetal terms, D (ν) is the damping matrix,\nτ\n= [τu, τv, τr] ∈R3 represents the control forces and\nmoments, G (ν) = [g1 (ν) , g2 (ν) , g3 (ν)]T ∈R3 denotes\nunmodeled dynamics due to gravitational and buoyancy forces\nand moments [16], and R is a rotation matrix given by\nR (η) =\n\n\ncos ψ\n−sin ψ\n0\nsin ψ\ncos ψ\n0\n0\n0\n1\n\n\nThe inertia matrix M = M T > 0 is\nM = [Mij] =\n\n\nM11\n0\n0\n0\nM22\nM23\n0\nM32\nM33\n\n\n(2)\nwhere M11 = m −X ˙u, M22 = m −Y ˙v, M33 = Iz −N ˙r, and\nM32 = M23 = mxg −Y ˙r. The matrix C (ν) = −CT (ν) is\nC = [Cij] =\n\n\n0\n0\nC13 (ν)\n0\n0\nC23 (ν)\n−C13 (ν)\n−C23 (ν)\n0\n\n\n(3)\nwhere C13 (ν) = −M22v −M23r, C23 (ν) = M11u. The\ndamping matrix D (ν) is\nD (ν) = [Dij] =\n\n\nD11 (ν)\n0\n0\n0\nD22 (ν)\nD23 (ν)\n0\nD32 (ν)\nD33 (ν)\n\n\n(4)\nwhere D11 (ν) = −Xu −X|u|u|u| −Xuuuu2, D22 (ν) =\n−Yv −Y|v|v|v| −Y|r|v|r|, D23 (ν) = −Yr −Y|v|r|v| −Y|r|r|r|,\nD32 (ν) = −Nv −N|v|v|v| −N|r|v|r|, D33 (ν) = −Nr −\nN|v|r|v| −N|r|r|r|, and X(·), Y(·), and N(·) are hydrodynamic\ncoefﬁcients whose deﬁnitions can be found in [16]. Accurate\nnumerical models of the nonlinear dynamics (1) are rarely\navailable. Major uncertainty sources come from M, C (ν), and\nD (ν) due to hydrodynamics, and G (ν) due to gravitational\nand buoyancy forces and moments.\nIII. PROBLEM FORMULATION\nIn this section, we will formulate the model-reference control\nstructure, introduce the reinforcement learning theory, and\ndeﬁne reward functions for reinforcement learning.\nA. Model-reference control formulation\nLet x =\n\u0002\nηT , νT \u0003T and u = τ, so (1) can be rewritten as\n˙x =\n\u0014 0\nR (η)\n0\nA (ν)\n\u0015\nx +\n\u0014 0\nB\n\u0015\nu +\n\u0014\n0\nM −1G (ν)\n\u0015\n(5)\nwhere A (ν) = M −1 (C (ν) + D (ν)), and B = M −1.\nAssuming that an accurate model (5) is not available, it is\npossible to get a nominal model expressed as\n˙xm =\n\u0014 0\nR (η)\n0\nAm\n\u0015\nxm +\n\u0014\n0\nBm\n\u0015\num\n(6)\nwhere Am and Bm are the known system matrices, and the\nunmodelled dynamics G (ν) ignored. Note that Am and Bm\nAutonomous Surface \nVehicle\nNominal System\nub\nul\nxr\nx\nxm\nRL \nControl\nBaseline \nControl\nBaseline \nControl\nxm\nx\nx\num\n˙xm =\n\n0\nR (⌘)\n0\nAm\n\"\nxm +\n\n0\nBm\n\"\num\nFig. 2: Model-reference reinforcement learning control\nare different from A (ν) and B, respectively. In Am and Bm,\nwe will ignore all unknown nonlinear terms, and obtain a linear\nnominal model. Assume that there exists a control law um\nallowing the states of the nominal system (6) to converge to a\nreference signal xr, i.e., ∥xm −xr∥2 →0 as t →∞.\nThe objective of the work in this paper is to design a\ncontroller allowing the state x to track state trajectories of\nthe nominal model (6) and avoid collisions with obstacles\nhaving known states xoi, where i ∈{1, . . . , No} indicates\nthe i-th obstacle. As shown in Figure 2, the overall control\nstructure for the ASV system (5) is\nu = ub + ul\n(7)\nwhere ub is a baseline control designed based on (6), and ul\nis a control law from the deep RL module whose design is\nprovided in Section IV-B.\nRemark 1. The baseline control ub is employed to ensure\nthe basic tracking performance without obstacles, (i.e., local\nstability of the tracking control), while ul is introduced to\ncompensate for all system uncertainties and achieve collision\navoidance. The baseline control ub in (7) can be designed\nusing any existing method based on the nominal model (6).\nOne potential choice for the design of ub is the nonlinear\nbackstepping control [40]. Hence, we ignore the design process\nof ub, and focus on the development of ul with RL.\nB. Markov decision process\nFor the formulation of RL, the ASV dynamics (5) and (6)\nare characterized using another mathematical model called\nMarkov decision process that is denoted by a tuple MDP :=\n\nS, U, P, R, γ\n\u000b\n, where S is the state space, U speciﬁes the\naction/input space, P : S × U × S →R deﬁnes a transition\nprobability, R : S×U →R is a reward function, and γ ∈[0, 1)\nis a discount factor. Note that the state vector s ∈S contains\nall available signals affecting the learned control ul. In this\npaper, such signals include x, xm, xr, ub, and xoi, where\nxm represents the desired behaviour of the system (5) and\nub is a function of x and xr, and xoi are obstacle states in\nthe neighbourhood (e.g., the position and velocity of the i-th\nobstacle). Hence, s =\nn\nxm, x, ub, ∪No\ni\nxoi\no\n, where ∪No\ni\nxoi\ndenotes the states of No obstacles detected by the ASV. More\ndetails on obstacles will be discussed in Section III-D.\nSince RL learns the control policies using data samples, it is\nassumed that we can sample input and state data from system\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n4\n(5) at discrete time steps. The sample time step is ﬁxed and\ndenoted by δt. Without loss of generality, let xt, ub,t, and ul,t\nbe the ASV state, the baseline control action, and the control\naction from RL at the time step t, respectively. The union of\nobstacles detected by the ASV is characterized by ∪No\ni\nxoi,t.\nThe state signal s at the time step t is, therefore, denoted by\nst =\nn\nxm,t, xt, ub,t, ∪No\ni\nxoi,t\no\n.\nC. Reinforcement learning\nFor standard RL, the objective is to maximize an expected\naccumulated return described by a value function Vπ (st) with\nVπ (st) =\n∞\nX\nt\nX\nul,t\nπ (ul,t|st)\nX\nst+1\nPt+1|t\n\u0000Rt + γVπ(st+1)\n\u0001\n(8)\nwhere Pt+1|t = P (st+1 |st, ul,t ) is the transition probability\nof the ASV system, Rt = R(st, ul,t) is the reward function,\nγ ∈[0, 1) is a constant discount factor, and π (ul,t|st) is called\ncontrol policy in RL. A policy in RL, denoted by π (ul,t|st),\nis the probability of choosing an action ul,t ∈U at a state\nst ∈S [27]. In this paper, a Gaussian policy is used, which is\nπ (ul|s) = N (ul (s) , σ)\n(9)\nwhere N (·, ·) denotes a Gaussian distribution with ul (s) as\nthe mean value and σ as the covariance matrix. The covariance\nmatrix σ controls the exploration performance at the learning\nstage. For the algorithm design, we also introduce an action-\nvalue function (a.k.a., Q-function) deﬁned by\nQπ (st, ul,t) = Rt + γEst+1 [Vπ(st+1)]\n(10)\nwhere Est+1 [·] = P\nst+1 Pt+1|t [·] is an expectation operator\nover the distribution of st+1. Maximizing Qπ (st, ul,t) is\nequivalent to maximizing Vπ(st). In the sequel, we will focus\nthe maximization of Qπ (st, ul,t) instead of Vπ(st).\nIn this paper, the deep RL is resolved based on the soft actor-\ncritic (SAC) algorithm which provides both sample efﬁcient\nlearning and convergence [50]. In SAC, an entropy term is\nadded to regulate the exploration performance at the training\nstage, thus resulting in a modiﬁed Q-function in (11).\nQπ (st, ul,t) =Rt + γEst+1 [Vπ(st+1)\n+αH (π (ul,t+1|st+1))]\n(11)\nwhere H (π (ul,t|st)) = −P\nul,t π (ul,t|st) ln (π (ul,t|st)) =\n−Eπ [ln (π (ul,t|st))] is the entropy of the policy, and α is a\ntemperature parameter [50]. Hence, SAC aims to solve the\nfollowing optimization problem.\nπ∗= arg max\nπ∈Π\n\u0000Rt + γEst+1 [Vπ(st+1)\n+αH (π (ul,t+1|st+1))]\n\u0001\n(12)\nwhere Π denotes a policy set.\nRemark 2. Once the optimization problem (12) is resolved,\nwe will have π∗(ul|s) = N (u∗\nl (s) , σ∗) according to (9).\nThe variance σ∗will be close to 0.Thus, the stochastic policy\nwill converge to a deterministic one in the end. The mean\nvalue function u∗\nl (s) will be the learned optimal control law\nInertial frame\nNorth\nEast\nXI\nYI\nda\npa\npoi\nva\nvoi\ndoi\ndaoi\ndsi\ndsi > doi + da\nFig. 3: Variables for collision avoidance (da: size of the ASV;\ndoi: size of the i-th obstacle; dsi: radius of the safe region;\ndaoi: relative distance between the ASV and the i-th obstacle)\nthat is eventually used to compensate system uncertainties and\navoid collisions. Notably, the optimal control law u∗\nl (s) will\nbe learned instead of designed. In the real implementation,\nu∗\nl (s) are approximated using deep neural networks that will\nbe discussed in Section IV-A. The learning process is to ﬁnd\nthe optimal parameters of the deep neural networks used to\napproximate the optimal control law u∗\nl (s).\nTraining/learning process of SAC will repeatedly execute\npolicy evaluation and policy improvement. In the policy\nevaluation, the Q-value in (11) is computed by applying a\nBellman operation Qπ (st, ul,t) = T πQπ (st, ul,t) where\nT πQπ (st, ul,t) = Rt + γEst+1 {Eπ [Qπ (st+1, ul,t+1)\n−α ln (π (ul,t+1|st+1))]}\n(13)\nIn the policy improvement, the policy is updated by\nπnew = arg min\nπ′∈Π DKL\n\u0010\nπ′ (·|st)\n\r\r\rZπolde\n1\nα Qπold(st,·)\u0011\n(14)\nwhere πold denotes the policy from the last update, Qπold\nis the Q-value of πold, DKL denotes the Kullback-Leibler\n(KL) divergence, and Zπold is a normalization factor. Via\nmathematical manipulations, the objective is transformed into\nπ∗= arg min\nπ∈Π Eπ\nh\nα ln (π (ul,t|st)) −Q (st, ul,t)\ni\n(15)\nMore details on how (15) is obtained can be found in [50].\nD. Reward functions\nIn our design, two objectives are deﬁned for the ASV:\ntrajectory tracking and collision avoidance. For the trajectory\ntracking, we aim to allow system (5) to track the nominal\nsystem (6), so the tracking reward Rt,1 is deﬁned as\nRt,1 = −(xt −xm,t)T H1 (xt −xm,t) −uT\nl,tH2ul,t (16)\nwhere H1 > 0 and H2 > 0 are positive deﬁnite matrices.\nThe second objective is to avoid obstacles along the trajectory\nof the ASV. Figure 3 shows variables used for the deﬁnition\nof the reward function for collision avoidance. All obstacles\nare assumed to be inscribed in a circle. The maximum size of\nthe i-th obstacle is deﬁned by doi as shown in Figure 3. The\nsize of the ASV is denoted by da. We introduce a safe region\nwith a radius of dsi for the i-th agent, where dsi > doi + da.\nThe relative distance between the i-th obstacle and the ASV\nis deﬁned as daoi. If daoi ≤dd, the i-th obstacle is visible\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n5\nInertial frame\ndi,t\n(va −voi)T !\npoi −pa\n\"\n> 0\nva −voi\npoi −pa\npoi\nNorth\nEast\nXI\nYI\npa\nva\nvoi\ndsi\nInertial frame\ndi,t\n(va −voi)T !\npoi −pa\n\"\n< 0\nva −voi\npoi −pa\npoi\nNorth\nEast\nXI\nYI\npa\nva\nvoi\ndsi\nFig. 4: Illustration of di,t (Note that di,t is only useful when (va −voi)T (poi −pa) > 0, otherwise collision is avoided)\nFig. 5: The impact of ci on Rt,2 with one obstacle and qc = 10\nto the ASV, where dd is the radius of the detection region\nof the ASV. Note that the obstacles could be either static\nor moving, so the state vector the i-th obstacle is written\nas xoi =\n\u0002\npT\noi, vT\noi\n\u0003T , where poi is the position of the i-\nth obstacle, and voi is the velocity of the i-th obstacle. Let\npa = [xp, yp]T and va = [up, vp]T be the position and velocity\nof the ASV, respectively. For the i-th visible obstacle at the\ntime step t, deﬁne the following variable shown in Figure 4.\ndi,t = ∥(va −voi)× (poi −pa) ∥2\n∥va −voi∥2\n(17)\nwhere “×” denotes the cross product operation, di,t repre-\nsents the closest possible distance between the ASV and\nthe obstacle, if the ASV keeps its current moving direction\nrelative to the obstacle. Note that di,t is only meaningful, if\n(va −voi)T (poi −pa) > 0. If (va −voi)T (poi −pa) > 0,\nit implies that the ASV moves towards the obstacle, otherwise,\nthe ASV moves away from the obstacle. Therefore, the reward\nfunction for collision avoidance is deﬁned to be\nRt,2 =\n(\n−PNo\ni=1\nqc,i1oi(xoi,pa,va)\n1+exp(ci(di,t−dsi)),\ndaoi ≤dd\n0,\notherwise\n(18)\nwhere qc,i > 0 is the maximum possible cost for collisions,\nci > 0 is a design parameter, and 1oi (xoi, pa, va) is\n1oi (xoi, pa, va) =\n\u001a\n1,\n(va −voi)T (poi −pa) > 0\n0,\notherwise\nThe parameter ci > 0 adjusts the sensitivity of collision\navoidance of RL in related to the i-th obstacle. The inﬂuence\nof ci > 0 on Rt,2 is illustrated in Figure 5.\nThe overall reward function is, therefore, deﬁned to be\nRt = Rt,1 + Rt,2\n(19)\nIV. MODEL-REFERENCE DEEP REINFORCEMENT\nLEARNING DESIGN AND IMPLEMENTATION\nIn this section, we will present the design and practical\nimplementation of the model-reference deep RL control.\nA. Deep neural networks\nIn RL, the value function Qπ (st, ul,t), which contains future\ninformation as shown in (10), is not known in advance. Simi-\nlarly, the control policy π is unknown as well. Hence, a feasible\nsolution is to approximate both the value function Qπ (st, ul,t)\nand the control policy π using deep neural networks. In this\npaper, the deep neural networks used to approximate both\nthe value function Qπ (st, ul,t) and the policy π (ul,t|st) are\nchosen to be fully connected multiple layer perceptrons (MLP)\nwith rectiﬁed linear unit (ReLU) nonlinearities as the activation\nfunctions. The ReLU nonlinearities are deﬁned by\nrelu (z) = max {z, 0}\nThe ReLU activation function outperforms other activa-\ntion functions like sigmoid functions [51]. For a vector\nz\n=\n[z1, . . . , , zn]T\n∈\nRn, there exists relu (z)\n=\n[relu (z1) , . . . , relu (zn)]T . As an example, a MLP with\n“ReLU” as the activation functions and two hidden layers is\nMLP 2\nw (z) = w2\n\"\nrelu\n\u0012\nw1\nh\nrelu\n\u0000w0\n\u0002\nzT ,1\n\u0003\u0001T ,1\niT \u0013T\n,1\n#T\n(20)\nwhere\n\u0002\nzT , 1\n\u0003T is a vector composed of z and a bias 1, the\nsuperscript “2” denotes the total number of hidden layers, the\nsubscript “w” denotes the parameter set to be trained in a\nMLP with w = {w0, w1, w2}, and w0, w1, and w2 are weight\nmatrices with appropriate dimensions.\nInput layer\nHidden layers\nOutput \nlayer\nActor \nNeural \nNetwork\nState\nState\nInput layer\nHidden layers\nOutput \nlayer\nCritic \nNeural \nNetwork\nInput\nst\nul,t\nQ✓\nst\nul,φ\nFig. 6: Approximation of Qθ and ul,φ using MLP’s\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n6\nIf there is a set of inputs z = {z1, . . . , zL} for the MLP\nin (20) with z1, . . ., zL denoting vector signals, we have\nMLP 2\nw (z) = MLP 2\nw\n\u0010\u0002\nzT\n1 , . . . , zT\nL\n\u0003T \u0011\n(21)\nBesides, MLP 2\nw (z1, z2) = MLP 2\nw\n\u0010\u0002\nzT\n1 , zT\n2\n\u0003T \u0011\nfor two\nvector inputs z1 and z2. If z1 = {z11, . . . , z1L} is a set of\nvectors, MLP 2\nw (z1, z2) = MLP 2\nw\n\u0010\u0002\nzT\n11, . . . , zT\n1L, zT\n2\n\u0003T \u0011\n.\nLet Qθ (st, ul,t) be the approximated Q-function using a\nMLP with a set of parameters denoted by θ. Following (20)\nand (21), the Q-function approximation Qθ (st, ul,t) is\nQθ (st, ul,t) = MLP K1\nθ\n(st, ul,t)\n(22)\nwhere θ = {θ0, . . . , θK1} with θi ∈R(L)×(L+1) for 0 ≤i ≤\nK1 denoting the weight matrices with proper dimensions. The\ndeep neural network for Qθ is illustrated in Figure 6.\nThe control law ul is also approximated using a MLP. The\napproximated control law of ul with a parameter set φ is\nul,φ = MLP K2\nφ (st)\n(23)\nThe illustration of ul,φ is given in Figure 6. In SAC, there\nare two outputs for the MLP in (23). One is the control law\nul,φ, the other one is σφ that is the standard deviation of the\nexploration noise [50]. According to (9), the parameterized\npolicy πφ in our model-reference RL is\nπφ = N\n\u0000ul,φ (st) , σ2\nφ\n\u0001\n(24)\nThe deep neural network for Qθ is called critic, while the one\nfor πφ is called actor.\nB. Algorithm design and implementation\nThe algorithm training process is illustrated in Figure 7. The\nwhole training process will be ofﬂine. We repeatedly run the\nsystem (5) under a trajectory tracking task. At each time step\nt + 1, we collect data samples, such as an input from the last\ntime step ul,t, a state from the last time step st, a reward Rt,\nand a current state st+1. Those historical data will be stored as\na tuple (st, ul,t, Rt, st+1) at a replay memory D [52]. At each\npolicy evaluation or improvement step, we randomly sample\na batch of historical data, B, from the replay memory D for\nthe training of the parameters θ and φ. Starting the training,\nwe apply the baseline control policy ub to an ASV system to\ncollect the initial data D0 as shown in Algorithm 1. The initial\ndata set D0 is used for the initial ﬁtting of Q-value functions.\nWhen the initialization is over, we execute both ub and the\nlatest updated RL policy πφ (ul,t|st) to run the ASV system.\nAt the policy evaluation step, the parameters θ are trained\nto minimize the following Bellman residual.\nJQ (θ) = E(st,ul,t)∼D\n\u00141\n2 (Qθ (st, ul,t) −Ytarget)2\n\u0015\n(25)\nwhere (st, ul,t) ∼D implies that we randomly pick data\nsamples (st, ul,t) from a replay memory D, and\nYtarget = Rt + γEst+1\n\u0002\nEπ [Q¯θ (st+1, ul,t+1) −α ln (πφ)]\n\u0003\nAutonomous Surface \nVehicle\nActor neural \nnetwork\nCritic neural \nnetwork\nRun the ASV using the latest learned control law and collect data\nUpdate critic and actor neural networks using historical data\nReplay memory\nRandomly sample \na batch of data\n⇠\nExploration noise\ninput\nstate\nReward\nFig. 7: Ofﬂine training process of deep reinforcement learning\nAlgorithm 1 Model reference reinforcement learning control\n1: Initialize parameters θ1, θ2 for Qθ1 and Qθ2, respectively,\nand φ for the actor network (23).\n2: Assign values to the target parameters ¯θ1 ←θ1, ¯θ2 ←θ2,\nD ←∅, D0 ←∅,\n3: Get data set D0 by running ub on (5) with ul = 0\n4: Turn off the exploration and train initial critic parameters\nθ0\n1, θ0\n2 using D0 according to (25).\n5: Initialize the replay memory D ←D0\n6: Assign initial values to critic parameters θ1 ←θ0\n1, θ2 ←θ0\n2\nand their targets ¯θ1 ←θ0\n1, ¯θ2 ←θ0\n2\n7: repeat\n8:\nfor each data collection step do\n9:\nChoose an action ul,t according to πφ (ul,t|st)\n10:\nCollect st+1 = {xt+1, xm,t+1, ub,t+1} based on the\nnominal system (6) and the full system (5)\n11:\nD ←D S {st, ul,t, R (st, ul,t) , st+1}\n12:\nend for\n13:\nfor each gradient update step do\n14:\nSample a batch of data B from D\n15:\nθj ←θj −ιQ∇θJQ (θj), and j = 1, 2\n16:\nφ ←φ −ιπ∇φJπ (φ),\n17:\nα ←α −ια∇αJα (α)\n18:\n¯θj ←κθj + (1 −κ) ¯θj, and j = 1, 2\n19:\nend for\n20: until convergence (i.e. JQ (θ) < a small threshold)\n21: Output the optimal parameters φ∗and θ∗\nj , and j = 1, 2\nwhere ¯θ is the target parameter which will be updated slowly.\nApplying a stochastic gradient descent technique (ADAM [53]\nin this paper) to (25) on a data batch B with a ﬁxed size yields\n∇θJQ (θ) =\nX ∇θQθ\n|B|\n\u0010\nQθ (st, ul,t) −Ytarget\n\u0011\nwhere |B| is the batch size.\nAt the policy improvement step, the objective function\ndeﬁned in (15) is represented using data samples from the\nreplay memory D as given in (26).\nJπ (φ) = E(st,ul,t)∼D\n\u0010\nα ln(πφ) −Qθ (st, ul,t)\n\u0011\n(26)\nParameter φ is trained to minimize (26) using a stochastic\ngradient descent technique. Applying the policy gradient\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n7\ntechnique to (26), we can calculate the gradient of Jπ (φ)\nwith respect to φ in terms of the stochastic gradient method as\n∇φJπ =\nX α∇φ ln πφ + (α∇ul ln πφ −∇ulQθ) ∇φ ˆul,φ\n|B|\nThe temperature parameters α are updated by minimizing\nJα = Eπ\n\u0002\n−α ln π (ul,t|st) −α ¯H\n\u0003\n(27)\nwhere ¯H is a target entropy. In the ﬁnal implementation, two\ncritics are introduced to reduce the over-estimation issue in the\ntraining of critic neural networks [54]. Under the two-critic\nmechanism, the target value Ytarget is modiﬁed to be\nYtarget = Rt + γ min\nn\nQ¯θ1 (st+1, ul,t+1) ,\nQ¯θ2 (st+1, ul,t+1)\no\n−γα ln (πφ)\n(28)\nThe entire process is summarized in Algorithm 1, in which ιQ,\nιπ, ια > 0 are learning rates, and κ > 0 is a constant scalar.\nOnce the training process is over, Algorithm 1 will output\nthe optimal parameters for the deep neural networks in (22)\nand (23). Once the optimal parameters are obtained, the learned\ncontrol law ul is approximated by\nul ≃ul,φ∗\n(29)\nwhere φ∗is the optimal parameter set for the MLP in (23) and\nis obtained via Algorithm 1.\nV. ALGORITHM ANALYSIS\nIn this section, the performance of the proposed model-\nreference RL algorithm will be analyzed, including convergence\nand closed-loop stability for tracking control.\nA. Convergence analysis\nThe general structure of a deep RL algorithm is summarized\nin Algorithm 2. The learning process will recursively execute\nthe policy evaluation and policy improvement until convergence.\nAs we mentioned in Section III-A, the baseline control ub is\nassumed to stabilize the ASV without collision avoidance.\nTherefore, the following assumption is introduced for the\nconvergence analysis.\nAssumption 1. If no obstacles are considered, the trajectory\ntracking errors of the ASV are bounded using the baseline\ncontrol ub.\nAccording to (16) and (18), both Rt,1 and Rt,2 are non-\npositive. With Assumption 1, the reward function Rt,1 is\nensured to be bounded. Additionally, the reward function Rt,2\nis bounded by design as shown in Figure 5 for a ﬁnite number\nof obstacles. Hence, the overall reward Rt is bounded, namely\nRt ∈[Rmin, 0]\n(30)\nwhere Rmin is the lowest bound for the reward function under\nthe baseline control ub.\nIn terms of (30), we can present the following Lemma 1\nand Lemma 2 for the convergence analysis of the entropy-\nregularized SAC algorithm [50], [55].\nAlgorithm 2 Policy iteration technique\n1: Start from an initial control policy u0\n2: repeat\n3:\nfor Policy evaluation do\n4:\nUnder a ﬁxed policy ul, apply the Bellman backup\noperator T π to the Q value function, Q (st, ul,t) =\nT πQ (st, ul,t) given in (13)\n5:\nend for\n6:\nfor Policy improvement do\n7:\nUpdate policy π according to (15)\n8:\nend for\n9: until convergence\nLemma 1 (Policy evaluation). Let T π be the Bellman\nbackup operator under a ﬁxed policy π and Qk+1 (s, ul) =\nT πQk (s, ul). The sequence Qk+1 (s, ul) will converge to the\nsoft Q-function Qπ of the policy π as k →∞.\nProof. Proof details are given in Appendix A.\nLemma 2 (Policy improvement). Let πold be an old policy\nand πnew be a new policy obtained according to (14). There\nexists Qπnew (s, ul) ≥Qπold (s, ul) ∀s ∈S and ∀u ∈U.\nProof. Proof details are given in Appendix B.\nIn terms of (1) and (2), we are ready to present Theorem 1\nto show the convergence of the model-reference RL algorithm.\nIn the sequel, the superscript i denotes the i-th iteration of\nthe policy iteration algorithm or the i-th policy improvement,\nwhere i = 0, 1, . . ., ∞.\nTheorem 1 (Convergence). Suppose πi is the policy obtained\nat the i-th policy improvement with π0 denoting any initial\npolicy in Π, and i = 0, 1, . . ., ∞. If one repeatedly applies the\npolicy evaluation and policy improvement steps as elaborated\nin Algorithm 2, there exists πi →π∗as i →∞such that\nQπ∗(s, ul) ≥Qπi (s, ul) ∀πi ∈Π, ∀s ∈S, and ∀ul ∈U,\nwhere π∗∈Π denotes the optimal policy.\nProof. Proof details are given in Appendix C.\nB. Stability of the tracking control\nIn this subsection, we will show the closed-loop stability of\nthe overall control law (baseline control ub plus the learned\ncontrol ul) for the tracking control without obstacles. The\nclosed-loop stability is analyzed under the general tracking\nperformance without the consideration of collision avoidance,\nas the tracking control is the fundamental task. Before the\nclosed-loop stability is analyzed, the admissible control concept\nis introduced in Deﬁnition 1, which is similar to the admissible\ncontrol in adaptive dynamic programming [56]–[58].\nDeﬁnition 1. A control law ub is said admissible with respect\nto the system (5), if it can stabilize the system (5) and ensure\nthat the state of (5) is uniformly ultimately bounded under\nsystem uncertainties.\nNote that the admissible control in [56]–[58] needs to provide\nthe asymptotic stability for the system. However, the admissible\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n8\ncontrol in Deﬁnition 1 doesn’t necessarily ensure the system\n(5) to be asymptotically stable. Hence, the admissible control\nconcept in this paper is less conservative than that in [56]–[58].\nAssume that the baseline control ub developed using the\nnominal system (6) of the ASV (5) is an admissible control\nlaw for the uncertain system (5). Let ∆(t) be the overall\nuncertainties in (5). Without loss of generality, it is assumed\nthat ∆(t) is bounded, namely ∥∆(t) ∥L∞≤¯∆where ∥· ∥L∞\ndenotes the L∞norm. In this paper, the objective of the tracking\ncontrol in obstacle-free environment is to ensure that the ASV\nsystem (5) can track its desired behaviour deﬁned by its nominal\nsystem (6), namely ∥x −xm∥2 →0 as t →∞. Let et =\nxt −xm,t be the tracking error of the ASV at the time instant\nt. The following assumption is made for a admissible baseline\ncontrol ub according to Deﬁnition 1 and Theorem 4.18 in [59]\n(Chapter 4, Page 172).\nAssumption 2. The baseline control law ub is feasible\nwith respect to the uncertain system (5), and there exists a\ncontinuously differentiable function V (st) associate with ub\nsuch that\nµ1 (∥et∥2) ≤V (et) ≤µ2 (∥et∥2)\nV (et+1) −V (et) ≤−W1 (et) + µ3 (∥∆(et) ∥2)\nW1 (et) > µ3 (∥∆(t) ∥2) , ∀∥et∥2 > c∆\n(31)\nwhere µ1 (·), µ2 (·), and µ3 (·) are class K functions, W (et)\nis a continuous positive deﬁnite function, and c∆is a constant\nrelated to the upper bound of system uncertainty\nAssumption 2 is possible in real world. One can treat the\nnominal model (6) as a linearized model of the overall ASV\nsystem (5) around a certain equilibrium. Assumption 2 presents\nthe basic design requirements for the baseline control law. With\na baseline control law satisfying Assumption 1, we could obtain\ntwo advantages which makes the RL process more efﬁcient.\nFirstly, it can ensure that the reward function Rt is bounded,\nimplying that both Vπ(st+1) and Q (st, ul,t) are bounded.\nSecondly, it could provide a “warm” start for the RL process.\nWith a feasible baseline control, the uncertain ASV system\ncan be ensured to be stable during the entire learning process. In\nthe stability analysis, we ignore the entropy term H (π), as it\nwill converge to zero in the end and it is only introduced\nto regulate the exploration magnitude. Hence, exploration\nnoises will be set to be zero. Now, we present Theorem 2\nto demonstrate the closed-loop stability of the ASV system (5)\nunder the composite control law (7).\nTheorem 2 (Stability of tracking control). Suppose Assump-\ntion 2 holds. The overall control law ui = ub +ui\nl can always\nstabilize the ASV system (5), where ui\nl represents the RL control\nlaw from i-th iteration, and i = 0, 1, 2, ... ∞.\nProof. The details of proof can be found in Appendix D.\nVI. SIMULATION RESULTS\nIn this section, the proposed learning-based control algorithm\nis implemented to the trajectory tracking control of a supply\nship model presented in [49], [60]. The ASV has two actuators\nthat are a propeller and a rudder in the rear. Hence, we mainly\nconsider two control inputs in the design, which are τu for the\nsurge speed control and τr for the heading control, respectively.\nBy default, the sway speed is not controlled, which implies\nτv = 0 in the simulations. Model parameters are summarized\nin Table A1 in Appendix E. The unmodeled dynamics in the\nsimulations are given by g1 = 0.279uv2 + 0.342v2r, g2 =\n0.912u2v, and g3 = 0.156ur2 + 0.278urv3, respectively. The\nbased-line control law ub is designed based on a nominal\nmodel with the following simpliﬁed linear dynamics in terms\nof the backstepping control method [40], [59].\nMm ˙νm = τ −Dmνm\n(32)\nwhere\nMm\n=\ndiag {M11, M22, M33}.\nDm\n=\ndiag {−Xv, −Yv, −Nr}.\nIn the simulation, a motion planner is employed to generate\nthe reference trajectories. The motion planner is expressed as\n˙ηr = R (ηr) νr,\n˙νr = ar\n(33)\nwhere ηr = [xr, yr, ψr]T is the generalized reference position\nvector, νr = [ur, 0, rr]T is the generalized reference velocity\nvector, and ar = [ ˙ur, 0, ˙rr]T .\nFour simulations scenarios are performed in this section.\nIn the ﬁrst scenario, our algorithm is implemented to an\nobstacle-free environment to show the closed-loop stability\nof the tracking control. In the second scenario, some ﬁxed\nobstacles are added to the environment to demonstrate both\nthe closed-loop stability and the collision avoidance capability\nof our proposed algorithm. In the third scenario, the proposed\nalgorithm is applied to the environment with both still and mov-\ning obstacles. The efﬁciency of our algorithm is demonstrated\nvia the comparison with the RL without baseline control. In the\nlast scenario, the simulation was conducted at different values\nof the parameters c in the reward function R2,t to illustrate\nthe impact of c on the collision avoidance performance.\nA. Trajectory tracking control without obstacles\nIn the ﬁrst simulation, the initial position vector ηr (0) is\nchosen to be ηr (0) =\n\u0002\n0, 0, π\n4\n\u0003T , and we set ur (0) = 0.4\nm/s and rr (0) = 0 rad/s. The reference acceleration ˙ur and\nangular rates are chosen to be\n˙ur\n=\n\u001a 0.005\nm/s2\nif t < 20 s\n0\nm/s2\notherwise\n(34)\n˙rr\n=\n\u001a\nπ\n600\nrad/s2\nif 25 s ≤t < 50 s\n0\nrad/s2\notherwise\n(35)\nFig. 8: Learning curves of two RL algorithms at training (One\nepisode is a training trial, and 1000 time steps per episode)\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n9\n(a) Model reference RL control\n(b) Only deep RL\n(c) Only baseline control\nFig. 9: Trajectory tracking results of the three algorithms\nFig. 10: Position tracking errors, ex\nThe reference signals ηr and νr are calculated using the\nreference motion planner (33) based on the aforementioned\ninitial conditions and the reference acceleration and angular\nrates given in (34) and (35), respectively.\nAt the training stage, we uniformly randomly sample x (0)\nand y (0) from (−1.5, 1.5), ψ (0) from (0.1π, 0.4π) and u (0)\nfrom (0.2, 0.4), and we choose v (0) = 0 and r (0) = 0. The\nproposed control algorithm is compared to two benchmark\ndesigns: the baseline control u0 and the RL control without u0.\nConﬁgurations for the training and neural networks are found in\nTable A2 in Appendix E. The matrices H1 and H2 are chosen\nto be H1 = diag {0.025, 0.025, 0.0016, 0.005, 0.001, 0} and\nH2 = diag\n\b\n1.25e−3, 1.25e−3\t\n, respectively. During the\ntraining process, we repeat the training processes for 1000\ntimes (i.e., 1000 episodes). For each episode, the ASV system\nis run for 100 s. Figure 8 shows the learning curves of the\nproposed algorithm (red) and the RL algorithm without baseline\ncontrol (blue). The learning curves demonstrate that both of\nthe two algorithms will converge in terms of the long term\nreturns. However, our proposed algorithm results in a larger\nreturn (red) in comparison to the RL without baseline control\n(blue). Hence, the introduction of the baseline control helps\nto increase the sample efﬁciency signiﬁcantly, as the proposed\nalgorithm (blue) converges faster to a higher return value.\nFig. 11: Position tracking errors, ey\nFig. 12: Mean absolute distance errors,\nq\ne2x + e2y\nAt the evaluation stage, we run the ASV system for 200 s\nand change the reference trajectory to demonstrate whether the\ncontrol law can ensure stable trajectory tracking. At the second\nevaluation, the reference angular acceleration is changed to\n˙rr =\n\n\n\nπ\n600\nrad/s2\nif\n25 s ≤t < 50 s\n−π\n600\nrad/s2\nif 125 s ≤t < 150 s\n0\nrad/s2\notherwise\n(36)\nThe trajectory tracking performance of the three algorithms\n(our proposed algorithm, the baseline control u0, and only RL\ncontrol) is shown in Figure 9. As observed in Figure 9.(b),\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n10\nFig. 13: Control inputs, τu\nFig. 14: Control inputs, τr\nthe ASV trajectory by the control law learned merely using\ndeep RL tends to drift away from the designed trajectory. It\nimplies that only deep RL could not ensure the closed-loop\nstability. In addition, the baseline control itself fails to achieve\nacceptable tracking performance mainly due to the existence\nof system uncertainties. By combining the baseline control\nand deep RL, the trajectory tracking performance is improved\ndramatically, and the closed-loop stability is guaranteed. The\ntracking errors in the X- and Y - coordinates of the inertial\nframe are summarized in Figure 10 and 11, respectively. The\nASV reaches its steady state after 80 s as shown in Figures\n10 and 11. Hence, we present the absolute average distance\nerrors from 80 s to 200 s to compare the tracking accuracy\nof the three algorithms in Figure 12. The introduction of the\ndeep RL increases the tracking performance of the baseline\ncontrol law substantially. The control inputs are provided in\nFigures 13 and 14.\nB. Tracking control with ﬁxed obstacles\nIn the second simulation, the initial position vector ηr (0)\nis chosen to be the same as the case in Section VI-A. We\nset ur (0) = 0.7 m/s and rr (0) = 0 rad/s. The reference\nacceleration is set as ˙ur = 0 m2/s. The angular rate is\n˙rr =\n\u001a\nπ\n800\nrad/s2\nif 20 s ≤t < 50 s\n0\nrad/s2\notherwise\n(37)\nInitial states of the ASV are randomly generated as summarized\nin Section VI-A. Three ﬁxed obstacles are added to the\nsimulation environment as shown in Figure 16.a, which have a\nradius of 1.5 m, 1.8 m, and 2.0 m, respectively. The detection\nFig. 15: Learning curves of two RL algorithms at training (One\nepisode is a training trial, and 1000 time steps per episode)\n(a) Model-reference RL control\n(b) Only deep RL\nFig. 16: Tracking control with ﬁxed obstacles (Obstacle radii:\n1.5 m, 1.8 m, and 2.0 m (from the lower to the upper))\nradius for the ASV is dd = 7.5 m, and the radius of the ASV\nis da = 1 m. The deep neural network conﬁgurations and\ntraining set-up for the collision avoidance scenario is the same\nas shown in Table A2 in Appendix E. We choose qc,i = 1 and\nci = 25 for all obstacles in the simulation.\nAt the training stage, 1000 episodes of training are conducted.\nFor each episode, the ASV system is run for 100 s. Figure 15\nshows the learning curves of the proposed algorithm (red) and\nthe RL algorithm without baseline control (blue).\nAt the evaluation stage, we run the ASV system for 200 s to\ndemonstrate whether the control law can ensure stable trajectory\ntracking and collision avoidance. The proposed algorithm is\ncompared with the RL algorithm without baseline control. The\nsimulation results of both our algorithm and the RL without\nbaseline control are shown in Figure 16. Although the RL\nwithout baseline control will converge in returns as shown\nFigure 15, the learned control law fails to avoid collision\nwith some obstacle as demonstrated in Figure 16.b. However,\nour algorithm can ensure both the trajectory tracking and the\ncollision avoidance at the same time. The control inputs are\nshown in Figures 17 and 18.\nC. Tracking control with ﬁxed obstacles and moving obstacles\nIn the third simulation, we show the collision avoidance with\nmoving obstacles. The reference trajectory is the same as that\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n11\nFig. 17: Control inputs, τu\nFig. 18: Control inputs, τr\nin the second simulation in Section VI-B. In the simulation,\nthere are two ﬁxed obstacles and one moving obstacle (e.g.,\nanother ASV). The moving obstacle has a safe radius of 1 m,\nand moving with a constant speed with oi,v = [−0.4, 0.25]\nm/s in the simulation. The training setup is the same as the\ncase in Section VI-B. At the evaluation, the ASV system is\nrun for 200 s. The learning curves are shown in Figure 19.\nThe trajectory tracking performance of both our algorithm\nand the RL without baseline control is shown in Figure 20.\nAlthough both of the two algorithms can learn a control law\nwith collision avoidance, our algorithm apparently has better\ntracking performance than the RL without baseline control.\nThe control inputs are given in Figures 21 and 22.\nFig. 19: Learning curves of two RL algorithms at training (One\nepisode is a training trial, and 1000 time steps per episode)\nD. The impact of different choices of c\nIn this simulation, we train our model-reference reinforce-\nment learning-based control at the different choices of c for\nthe collision avoidance reward R2,t in (18). Three choices are\nconsidered for c, which are c = 0.25, c = 2.5, and c = 25,\nrespectively. Similar to the simulation environment in Section\nVI-B, three ﬁxed obstacles are considered. The trajectory\ntracking performance is summarized in Figure 23. When a\nsmaller c is chosen, the ASV will take more conservative\nactions to avoid collisions with obstacles as illustrated in Figure\n23. This is because a small c will make R2,t change slowly\nwith respect to the distance between the ASV and an obstacle.\nThe slow variation of R2,t will make the ASV take more\nconservative actions to avoid collisions.\nVII. CONCLUSIONS\nIn this paper, we presented a novel learning-based control\nalgorithm for ASV systems with collision avoidance. The\nproposed control algorithm combined a conventional control\nmethod with deep reinforcement learning to provide closed-loop\nstability guarantee, uncertainty compensation, and collision\navoidance. Convergence of the learning algorithm was analyzed.\nWe also presented the stability analysis of the tracking control.\nThe proposed control algorithm shows much better performance\nin both tracking control and collision avoidance than the RL\nwithout baseline control. In the future works, we will further\nanalyze the sample efﬁciency of the proposed algorithm, and\nextend the design to the scenario with extensive environment\ndisturbances.\nAPPENDIX\nA. Proof of Lemma 1\nProof. The following entropy-augmented reward function ˆRt\nis introduced.\nˆRt = Rt −γEst+1 {Eπ [α ln (π (ul,t+1|st+1))]}\n(A.1)\nHence, the Bellman backup operation can be rewritten as\nT πQπ (st, ul,t) = ˆRt + γEst+1,π [Qπ (st+1, ul,t+1)] (A.2)\n(a) Model-reference RL control\n(b) Only deep RL\nFig. 20: Tracking control with both ﬁxed and moving obstacles\n(Radii of ﬁxed obstacles: 1.5 m and 2.0 m (from the lower to\nthe upper))\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n12\nFig. 21: Control inputs, τu\nFig. 22: Control inputs, τr\nWith the choice of a ﬁnite coefﬁcient α, the second term in\n(A.1) is always bounded. According to (30), there are two\nconstants ˆRmin and ˆRmax such that ˆRt ∈\nh\nˆRmin, ˆRmax\ni\n,\nand furthermore, | ˆRt| ≤¯R with ¯R = max\nn\n| ˆRmin|, | ˆRmax|\no\n.\nIn terms of (8) and (10), one has Qπ (st, ul,t) =\nˆRt +\nγ P∞\nt+1\nP\nul,t+1 π (ul,t+1|st+1) P\nst+1 Pt+1|t ˆRt+1, thus\n∥Qπ (st, ul,t) ∥∞≤\n¯R\n1 −γ\n(A.3)\nwhere the ∞-norm in (A.3) is deﬁned to be ∥Qπ (s, ul) ∥∞=\nmaxs,ul |Qπ (s, ul) |. Hence, the Q-value Qπ is bounded in\n∞-norm based on the baseline control. For two distinct Q\nvalues Qπ and Q′\nπ, there exists\n∥T πQπ −T πQ′\nπ∥∞= ∥ˆRt + γEst+1,π [Qπ (st+1, ul,t+1)]\n−ˆRt −γEst+1,π [Q′\nπ (st+1, ul,t+1)] ∥∞\n≤γ∥Qπ −Q′\nπ∥∞\n(A.4)\nwhere Q′\nπ represents the Q-value function approximated at the\nlast iteration, and Qπ is the Q-value function approximated at\nthe current iteration. The Bellman backup operation (A.2) is\nγ-contraction with 0 ≤γ < 1. According to Banach’s ﬁxed-\npoint theorem, T π possesses a unique ﬁxed point. Hence, the\nsequence Qk+1 (s, ul) will converge to the soft Q-function\nQπ of the policy π as k →∞.\nB. Proof of Lemma 2\nProof. Based on (14), we can obtain\nEπnew\nh\nα ln (πnew (ul,t|st)) −Qπold (st, ul,t)\ni\n≥\nEπold\nh\nα ln (πold (ul,t|st)) −Qπold (st, ul,t)\ni\n(A.5)\nLet V π (st) = Eπ [Qπ (st, ul,t) −α ln (π (ul,t|st))]. Accord-\ning to (A.5) and (13), it yields\nQπold (st, ul,t) =Rt + γEst+1 [V πold (st)]\n≤Rt + γEst+1 [Eπnew [Qπold (st+1, ul,t+1)\n−α ln (πnew (ul,t+1|st+1))]]\n≤Rt + γEst+1\nh\nEπnew\nh\nRt+1\n+ γEst+2 [Eπnew [Qπold (st+2, ul,t+2)\n−α ln (πnew (ul,t+2|st+2))]]\n−α ln (πnew (ul,t+1|st+1))\nii\n...\n≤Qπnew (st, ul,t)\n(A.6)\nHence, Qπnew (st, ul,t) ≥Qπold (st, ul,t) ∀st ∈S and\n∀ul,t ∈U.\nC. Proof of Theorem 1\nProof. According to Lemma 2, one has Qπi (s, ul)\n≥\nQπi−1 (s, ul), so Qπi (s, ul) is monotonically non-decreasing\nwith respect to the policy iteration step i. In addition,\nQπi (s, ul) is upper bounded according to the deﬁnition of the\nreward given in (19), so Qπi (s, ul) will converge to an upper\nlimit Qπ∗(s, ul) with Qπ∗(s, ul) ≥Qπi (s, ul) ∀πi ∈Π,\n∀s ∈S, and ∀ul ∈U.\nD. Proof of Theorem 2\nProof. In our proposed algorithm, we start the training/learning\nusing the baseline control law ub. According to Lemma 1, we\nare able to obtain the corresponding Q value function for the\nbaseline control law ub. Let the Q value function be Q0 \u0000s, u0\nl\n\u0001\nat the beginning of the iteration where u0\nl is the initial RL-\nbased control function. According to the deﬁnitions of the\nreward function in (19) and Q value function in (10), we can\nchoose the Lyapunov function candidate as\nV0 (e) = −Q0 \u0000s, u0\nl\n\u0001\n(A.7)\nwhere Q0 \u0000s, u0\nl\n\u0001\nis the action value function of the initial\ncontrol law u0\nl . Note that the baseline control ub is implicitly\nincluded in the state vector s, as s consists of x, xm, and\nub in this paper as discussed in Section III. Hence, V (st)\nin Assumption 2 is a Lyapunov function for the closed-loop\nsystem of (5) with the baseline control ub.\nSince ASVs have deterministic dynamics and exploration\nnoises are not considered, we have Q0 (st, ul,t) = V 0 (st)\nand Q0 (st, ul,t) = R0\nt + γQ0 (st+1, ul,t+1) where R0\nt =\nR(st, u0\nl,t). With the consideration of V0 (e) = −Q0 (s, ul),\nthere exists V0 (et) = −R0\nt + γV0 (et+1).\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n13\n(a) c = 0.25\n(b) c = 2.5\n(c) c = 25\nFig. 23: Collision avoidance performance at different values of c\nIf Assumption 2 holds, there exists V0 (et+1) −V0 (et) ≤\n−W1 (et) + µ3 (∥∆(st) ∥2) and W1 (et) > µ3 (∥∆(t) ∥2),\n∀∥et∥2 > c∆. Hence,\n(1 −γ) V0 (et+1) + R0\nt ≤−W1 (et) + µ3 (∥∆(t) ∥2) (A.8)\nIn the policy improvement, the control law is updated by\nu1\nl = min\nπ\n\u0000−R0\nt + γV0 (et+1)\n\u0001\n(A.9)\nNote that ul is implicitly contained in both Rt and V0 (et+1)\naccording to (16) and (A.7). In the policy evaluation, the\nfollowing update is conducted.\nV1 (et) = −R1\nt + γV0 (et+1)\n(A.10)\nwhere R1\nt = R(st, u1\nl,t). Hence, for u1\nl , there exists\nV1 (et+1) −V1 (et) =V1 (et+1) + R1\nt −γV0 (et+1)\n=V1 (et+1) −V0 (et+1) + R1\nt\n−R0\nt + R0\nt + (1 −γ) V0 (et+1)\n≤−W1 (et) + µ3 (∥∆(t) ∥2) + R1\nt\n−R0\nt + V1 (et+1) −V0 (et+1)\nAccording to (A.9) and (A.10), one has V1 (et+1) ≤V0 (et+1)\nand R1\nt ≤R0\nt . Therefore, V0 (et+1) −V1 (et+1) + R0\nt −R1\nt +\nW1 (et) ≥W1 (et). As W1 (et) > µ3 (∥∆(t) ∥2), ∀∥et∥2 >\nc∆, there must exist a new constant c1\n∆≤c∆such that\nV0 (et+1)−V1 (et+1)+R0\nt −R1\nt +W1 (et) > µ3 (∥∆(t) ∥2),\n∀∥et∥2 > c1\n∆.\nThe new control law u1\nl can also ensure the closed-loop ASV\nsystem to be uniformally ultimately bounded. In the worst case,\nc1\n∆= c∆, which implies that u1\nl will have the same control\nperformance with u0\nt, namely guaranteeing the same ultimate\nboundaries for the tracking errors. If there exists c1\n∆> c∆, it\nimplies that u1\nl will result in smaller tracking errors than u0\nl .\nFollowing the same analysis, we can show that u2\nl also\nstabilizes the ASV system (5) in terms of V1 (st) and replacing\nu0\nl in (A.9) and (A.10) with u1\nl . Repeating (A.9) and (A.10)\nfor all i = 1, 2, . . ., we can prove that all ui\nl can stabilize the\nASV system (5), if Assumption 2 holds. It implies that the\nASV system (5) will be stabilized by the overall control law\nui = ub + ui\nl.\nE. Simulation conﬁgurations\nTABLE A1: Model parameters\nParameters\nValues\nParameters\nValues\nm\n23.8\nY ˙r\n−0.0\nIz\n1.76\nYr\n0.1079\nxg\n0.046\nY|v|r\n−0.845\nX ˙u\n−2.0\nY|r|r\n−3.45\nXu\n−0.7225\nNv\n−0.1052\nX|u|u\n−1.3274\nN|v|v\n5.0437\nXuuu\n−1.8664\nN|r|v\n−0.13\nY ˙v\n−10.0\nN ˙r\n−1.0\nYv\n−38.612\nNr\n−1.9\nY|v|v\n−36.2823\nN|v|r\n0.08\nY|r|v\n−0.805\nN|r|r\n−0.75\nTABLE A2: Reinforcement learning conﬁgurations\nParameters\nValues\nLearning rate ιQ\n0.001\nLearning rate ιπ\n0.0001\nLearning rate ια\n0.0001\nκ\n0.01\nactor neural network\nfully connected with two hidden layers\n(128 neurons per hidden layer)\ncritic neural networks\nfully connected with two hidden layers\n(128 neurons per hidden layer)\nReplay memory capacity\n1 × 106\nSample batch size\n128\nγ\n0.998\nTraining episodes\n1000\nSteps per episode\n1000\ntime step size δt\n0.1\nREFERENCES\n[1] D. O.B.Jones, A. R.Gates, V. A.I.Huvenne, A. B.Phillips, and B. J.Bett,\n“Autonomous marine environmental monitoring: Application in decom-\nmissioned oil ﬁelds,” Science of The Total Environment, vol. 668, no. 10,\npp. 835– 853, 2019.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n14\n[2] J. Majohr and T. Buch, Advances in Unmanned Marine Vehicles.\nInsti-\ntution of Engineering and Technology, 2006, ch. Modelling, simulation\nand control of an autonomous surface marine vehicle for surveying\napplications Measuring Dolphin MESSIN.\n[3] O. Levander, “Autonomous ships on the high seas,” IEEE Spectrum,\nvol. 54, no. 2, pp. 26 – 31, 2017.\n[4] E. Tu, G. Zhang, L. Rachmawati, E. Rajabally, and G.-B. Huang,\n“Exploiting ais data for intelligent maritime navigation: A comprehensive\nsurvey from data to methodology,” IEEE Transactions on Intelligent\nTransportation Systems, vol. 19, no. 5, pp. 1559 –1582, May 2018.\n[5] P. ˇSvec, A. Thakur, E. Raboin, B. C. Shah, and S. K. Gupta, “Target\nfollowing with motion prediction for unmanned surface vehicle operating\nin cluttered environments,” International Journal of Robotics Research,\nno. 36, pp. 383 – 405, Apr. 2014.\n[6] D. D. Bloisi, F. Previtali, A. Pennisi, D. Nardi, and M. Fiorini, “Enhancing\nautomatic maritime surveillance systems with visual information,” IEEE\nTransactions on Intelligent Transportation Systems, vol. 18, no. 4, pp.\n824 – 833, Apr. 2017.\n[7] S. Campbell, W. Naeem, and G. Irwin, “A review on improving the\nautonomy of unmanned surface vehicles through intelligent collision\navoidance manoeuvres,” Annual Reviews in Control, vol. 36, no. 2, pp.\n267 – 283, 2012.\n[8] K. Do and J. Pan, “Global robust adaptive path following of underactuated\nships,” Automatica, vol. 42, no. 10, pp. 1713 – 1722, Oct. 2006.\n[9] R. A. Soltan, H. Ashraﬁuon, and K. R. Muske, “State-dependent\ntrajectory planning and tracking control of unmanned surface vessels,”\nin Proceedings of 2009 American Control Conference.\nSt. Louis, MO,\nUSA: IEEE, Jun. 2009.\n[10] A. Ram ´On, J. Ruiz, and F. S. Granja, “A short-range ship navigation\nsystem based on ladar imaging and target tracking for improved safety\nand efﬁciency,” IEEE Transactions on Intelligent Transportation Systems,\nvol. 10, no. 1, pp. 186 – 197, Mar. 2012.\n[11] L. P. Perera, P. Oliveira, , and C. G. Soares, “Maritime trafﬁc monitoring\nbased on vessel detection, tracking, state estimation, and trajectory\nprediction,” IEEE Transactions on Intelligent Transportation Systems,\nvol. 13, no. 3, pp. 1188 – 1200, Sep. 2012.\n[12] N. E.Kahveci and P. A. Ioannou, “Adaptive steering control for uncertain\nship dynamics and stability analysis,” Automatica, vol. 49, no. 3, pp.\n685–697, May 2013.\n[13] J. Woo, C. Yu, and N. Kim, “Deep reinforcement learning-based controller\nfor path following of an unmanned surface vehicle,” Ocean Engineering,\nvol. 183, no. 1, pp. 155 – 166, Dec. 2019.\n[14] T. A. Johansen, T. Perez, and A. Cristofaro, “Ship collision avoidance\nand colregs compliance using simulation-based control behavior selection\nwith predictive hazard assessment,” IEEE Transactions on Intelligent\nTransportation Systems, vol. 17, no. 2, pp. 3407 – 3422, May 2016.\n[15] S. Li, J. Liu, and R. R. Negenborn, “Distributed coordination for collision\navoidance of multiple ships considering ship maneuverability,” Ocean\nEngineering, vol. 181, pp. 212 – 226, 2019.\n[16] T. I. Fossen, Handbook of Marine Craft Hydrodynamics and Motion\nControl.\nJohn Wiley & Sons, Inc., 2011.\n[17] M. Greytak and F. Hover, “Motion planning with an analytic risk cost\nfor holonomic vehicles,” in Proceedings of the 48th IEEE Conference on\nDecision and Control (CDC).\nShanghai, P.R. China: IEEE, Dec. 2009.\n[18] F. S. Hover, R. M. Eustice, A. Kim, B. Englot, H. Johannsson, M. Kaess,\nand J. J. Leonard, “Advanced perception, navigation and planning for\nautonomous in-water ship hull inspection,” The International Journal of\nRobotics Research, vol. 31, no. 12, pp. 1445 – 1464, 2012.\n[19] D. Gonz´alez, J. P´erez, V. Milan´es, and F. Nashashibi, “A review of\nmotion planning techniques for automated vehicles,” IEEE Transactions\non Intelligent Transportation Systems, vol. 17, no. 4, pp. 1135 – 1145,\n2016.\n[20] H.-T. L. Chiang and L. Tapia, “COLREG-RRT: An RRT-based\nCOLREGS-compliant motion planner for surface vehicle navigation,”\nIEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 2024 – 2031,\nJul. 2018.\n[21] A. J. H¨ausler, A. Saccon, A. P. Aguiar, J. Hauser, and A. M. Pascoal,\n“Cooperative motion planning for multiple autonomous marine vehicles,”\nin Proceedings of the IFAC Proceedings Volumes, Arenzano, 2012, pp.\n244 – 249.\n[22] M. Abdelaal, M. Fr¨anzle, and A. Hahn, “Nonlinear model predictive\ncontrol for trajectory tracking and collision avoidance of underactuated\nvessels with disturbances,” Ocean Engineering, vol. 160, no. 15, pp. 168\n–180, Jul. 2018.\n[23] E. Meyer, H. Robinson, A. Rasheed, and O. San, “Taming an autonomous\nsurface vehicle for path following and collision avoidance using deep\nreinforcement learning,” IEEE Access, vol. 8, pp. 41 466 – 41 481, 2020.\n[24] X. Zhang, A. Liniger, and F. Borrelli, “Optimization-based collision\navoidance,” IEEE Transactions on Control Systems Technology, 2020,\n(Early Access).\n[25] D. Panagou, “A distributed feedback motion planning protocol for\nmultiple unicycle agents of different classes,” IEEE Transactions on\nAutomatic Control, vol. 62, no. 3, pp. 1178 – 1193, Mar. 2017.\n[26] T. Fan, P. Long, W. Liu, and J. Pan, “Distributed multi-robot collision\navoidance via deep reinforcement learning for navigation in complex\nscenarios,” The International Journal of Robotics Research, vol. 39, no. 7,\np. 856?892, May 2020.\n[27] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introductions,\n2nd ed.\nThe MIT Press, 2018.\n[28] R. Yu, Q. Zhu, G. Xia, and Z. Liu, “Sliding mode tracking control of\nan underactuated surface vessel,” IET Control Theory & Applications,\nvol. 6, no. 3, pp. 461 – 466, 2012.\n[29] J. A. Farrell and M. M. Polycarpou, Adaptive Approximation Based\nControl: Unifying Neural, Fuzzy and Traditional Adaptive Approximation\nApproaches.\nNew York, NY: Wiley, 2006.\n[30] G. Chowdhary, T. Yucelen, M. M¨uhlegg, and E. N. Johnson, “Concurrent\nlearning adaptive control of linear systems with exponentially convergent\nbounds,” International Journal of Adaptive Control and Signal Processing,\nvol. 27, no. 4, pp. 280–301, May 2013.\n[31] Y. Guan and M. Saif, “A novel approach to the design of unknown input\nobservers,” IEEE Transactions on Automatic Control, vol. 36, no. 5, pp.\n632 – 635, May 1991.\n[32] H. Dahmani, O. Pag`es, A. E. Hajjaji, and N. Daraoui, “Observer-based\nrobust control of vehicle dynamics for rollover mitigation in critical\nsituations,” IEEE Transactions on Intelligent Transportation Systems,\nvol. 15, no. 1, pp. 274–284, Jan. 2014.\n[33] Y. Li, B. Yang, T. Zheng, Y. Li, M. Cui, and S. Peeta, “Extended-state-\nobserver-based double-loop integral sliding-mode control of electronic\nthrottle valve,” IEEE Transactions on Intelligent Transportation Systems,\nvol. 16, no. 5, pp. 2501–2510, Mar. 2015.\n[34] W. Shi, S. Song, C. Wu, and C. L. P. Chen, “Multi pseudo q-learning-\nbased deterministic policy gradient for tracking control of autonomous\nunderwater vehicles,” IEEE Transactions on Neural Networks and\nLearning Systems, vol. 30, no. 12, pp. 3534 – 3546, Dec. 2019.\n[35] T. Shen and K. Tamura, “Robust h∞control of uncertain nonlinear\nsystem via state feedback,” IEEE Transactions on Automatic Control,\nvol. 40, no. 4, pp. 766 – 768, Apr. 1995.\n[36] X. Liu, H. Su, B. Yao, and J. Chu, “Adaptive robust control of a class\nof uncertain nonlinear systems with unknown sinusoidal disturbances,”\nin Proceedings of 2008 47th IEEE Conference on Decision and Control.\nCancun, Mexico, USA: IEEE, Dec. 2008.\n[37] P. A. Ioannou and J. Sun, Robust Adaptive Control.\nPrentice-Hall, Inc.,\n1996.\n[38] W. M. Haddad and T. Hayakawa, “Direct adaptive control for non-linear\nuncertain systems with exogenous disturbances,” International Journal\nof Adaptive Control and Signal Processing, vol. 16, no. 2, pp. 151 – 172,\nFeb. 2002.\n[39] Q. Zhang and H. H. Liu, “Aerodynamic model-based robust adaptive\ncontrol for close formation ﬂight,” Aerospace Science and Technology,\nvol. 79, pp. 5 – 16, 2018.\n[40] ——, “UDE-based robust command ﬁltered backstepping control for\nclose formation ﬂight,” IEEE Transactions on Industrial Electronics,\nvol. 65, no. 11, pp. 8818–8827, Nov. 2018.\n[41] B. Zhu, Q. Zhang, and H. H. Liu, “Design and experimental evaluation\nof robust motion synchronization control for multivehicle system without\nvelocity measurements,” International Journal of Robust and Nonlinear\nControl, vol. 28, no. 7, pp. 5437 – 5463, 2018.\n[42] S. Mondal and C. Mahanta, “Chattering free adaptive multivariable sliding\nmode controller for systems with matched and mismatched uncertainty,”\nISA Transactions, vol. 52, pp. 335 – 341, 2013.\n[43] M. Han, Y. Tian, L. Zhang, J. Wang, and W. Pan, “h∞model-free\nreinforcement learning with robust stability guarantee,” in Proceedings of\nthe 33rd Conference on Neural Information Processing Systems (NeurIPS\n2019), Vancouver, Canada, Dec. 2019.\n[44] M. Han, L. Zhang, J. Wang, and W. Pan, “Actor-critic reinforcement learn-\ning for control with stability guarantee,” arXiv preprint arXiv:2004.14288,\n2020.\n[45] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, “Safe model-\nbased reinforcement learning with stability guarantees,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems (NIPS 2017), Long Beach, CA, USA, Dec. 2017.\n[46] R. Sutton, A. Barto, and R. Williams, “Reinforcement learning is direct\nadaptive optimal control,” IEEE Control Systems Magazine, vol. 12, no. 2,\npp. 19 – 22, Apr. 1992.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX XXXX\n15\n[47] J. Hwangbo, I. Sa, R. Siegwart, and M. Hutter, “Control of a quadrotor\nwith reinforcement learning,” IEEE Robotics and Automation Letters,\nvol. 2, no. 4, pp. 2096 – 2103, Oct. 2017.\n[48] Q. Zhang, W. Pan, and V. Reppa, “Model-reference reinforcement\nlearning control of autonomous surface vehicles with uncertainties,” arXiv\npreprint arXiv:2003.13839, 2020.\n[49] R. Skjetne, T. I. Fossen, and P. V. Kokotovi´c, “Adaptive maneuvering,\nwith experiments, for a model ship in a marine control laboratory,”\nMathematics of Operations Research, vol. 41, pp. 289 – 298, 2005.\n[50] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in Proceedings of the 35th International Conference on Machine\nLearning (ICML 2018), vol. 80, Stockholmsm¨assan, Stockholm Sweden,\nJul. 2018, pp. 1861–1870.\n[51] G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural\nnetworks for lvcsr using rectiﬁed linear units and dropout,” in Proceedings\nof 2013 IEEE International Conference on Acoustics, Speech and Signal\nProcessing, Vancouver, BC, Canada, May 2013.\n[52] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\nC. B. Stig Petersen, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis, “Human-level control through\ndeep reinforcement learning,” Nature, vol. 518, pp. 529–533, Feb. 2015.\n[53] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin Proceedings of the 3rd International Conference for Learning\nRepresentations (ICLR 2015), San Diego, USA, May 2015.\n[54] S. Fujimoto, H. van Hoof, and D. Meger, “Addressing function ap-\nproximation error in actor-critic methods,” vol. 80, Stockholmsm¨assan,\nStockholm Sweden, Jul. 2018, pp. 1587–1596.\n[55] T. Haarnoja, K. H. Aurick Zhou, G. Tucker, S. Ha, J. Tan, V. Kumar,\nH. Zhu, A. Gupta, P. Abbeel, and S. Levine, “Soft actor-critic algorithms\nand applications,” arXiv preprint arXiv:1812.05905, 2018.\n[56] R. W. Beard, G. N. Saridis, and J. T. Wen, “Galerkin approximations of\nthe generalized hamilton-jacobi-bellman equation,” Automatica, vol. 33,\nno. 12, pp. 2159 – 2177, 1997.\n[57] A. Al-Tamimi, F. L. Lewis, and M. Abu-Khalaf, “A short-range ship\nnavigation system based on ladar imaging and target tracking for\nimproved safety and efﬁciency,” IEEE Transactions on Systems, Man,\nand Cybernetics, Part B (Cybernetics), vol. 38, no. 4, pp. 943 – 949,\nAug. 2008.\n[58] Y. Jiang and Z.-P. Jiang, “Global adaptive dynamic programming for\ncontinuous-time nonlinear systems,” IEEE Transactions on Automatic\nControl, vol. 60, no. 11, pp. 2917 – 2929, Mar. 2015.\n[59] H. K. Khalil, Nonlinear Systems, 3rd ed.\nPrentice Hall, 2001.\n[60] Z. Peng, D. Wang, T. Li, and Z. Wu, “Leaderless and leader-follower\ncooperative control of multiple marine surface vehicles with unknown\ndynamics,” Nonlinear Dynamics, vol. 74, pp. 95 – 106, 2013.\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.RO",
    "cs.SY"
  ],
  "published": "2020-08-17",
  "updated": "2020-08-17"
}