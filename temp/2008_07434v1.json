{
  "id": "http://arxiv.org/abs/2008.07434v1",
  "title": "Integrating Deep Reinforcement Learning Networks with Health System Simulations",
  "authors": [
    "Michael Allen",
    "Thomas Monks"
  ],
  "abstract": "Background and motivation: Combining Deep Reinforcement Learning (Deep RL)\nand Health Systems Simulations has significant potential, for both research\ninto improving Deep RL performance and safety, and in operational practice.\nWhile individual toolkits exist for Deep RL and Health Systems Simulations, no\nframework to integrate the two has been established.\n  Aim: Provide a framework for integrating Deep RL Networks with Health System\nSimulations, and to ensure this framework is compatible with Deep RL agents\nthat have been developed and tested using OpenAI Gym.\n  Methods: We developed our framework based on the OpenAI Gym framework, and\ndemonstrate its use on a simple hospital bed capacity model. We built the Deep\nRL agents using PyTorch, and the Hospital Simulatation using SimPy.\n  Results: We demonstrate example models using a Double Deep Q Network or a\nDuelling Double Deep Q Network as the Deep RL agent.\n  Conclusion: SimPy may be used to create Health System Simulations that are\ncompatible with agents developed and tested on OpenAI Gym environments.\n  GitHub repository of code:\nhttps://github.com/MichaelAllen1966/learninghospital",
  "text": "Integrating Deep Reinforcement Learning Networks with Health\nSystem Simulations.\nA practical OpenAI Gym-like coding framework using PyTorch and SimPy.\nMichael Allen1 and Thomas Monks2\n1University of Exeter Medical School & NIHR South West Peninsula Applied Research Collaboration (ARC).\n2University of Exeter Institute of Data Science and Artiﬁcial Intelligence\nAugust 18, 2020\nAbstract\nBackground and motivation: Combining Deep Reinforcement Learning (Deep RL) and Health Systems\nSimulations has signiﬁcant potential, for both research into improving Deep RL performance and safety,\nand in operational practice. While individual toolkits exist for Deep RL and Health Systems Simulations,\nno framework to integrate the two has been established.\nAim: Provide a framework for integrating Deep RL Networks with Health System Simulations, and to\nensure this framework is compatible with Deep RL agents that have been developed and tested using\nOpenAI Gym.\nMethods: We developed our framework based on the OpenAI Gym framework, and demonstrate its use on\na simple hospital bed capacity model. We built the Deep RL agents using PyTorch, and the Hospital\nSimulation using SimPy.\nResults: We demonstrate example models using a Double Deep Q Network or a Duelling Double Deep Q\nNetwork as the Deep RL agent.\nConclusion: SimPy may be used to create Health System Simulations that are compatible with agents\ndeveloped and tested on OpenAI Gym environments.\nGitHub repository of code: https://github.com/MichaelAllen1966/learninghospital\n1\nIntroduction\nDeep Reinforcement Learning and Health System\nSimulations are two complementary and parallel\nmethods that have the potential to improve the\ndelivery of health systems.\nDeep Reinforcement Learning (Deep RL) is a\nrapidly developing area of research, ﬁnding applica-\ntion in areas as diverse as game playing, robotics,\nnatural language processing, computer vision, and\nsystems control1. Deep RL involves an agent that\ninteracts with an environment with the aim of de-\nveloping a policy that maximises long term return\nof rewards. Deep RL has a framework that allows\nfor generic problem solving that is not dependent\non pre-existing domain knowledge, making these\ntechniques applicable to a wide range of problems.\nHealth Systems Simulation seeks to mimic the be-\nhaviour of real systems.\nThese may be used to\noptimise services such as emergency departments2,\nhospital ward operation and capacity3 and commu-\nnity hospital capacity4. These examples of health\nservice simulations are used for oﬀ-line planning\nand optimization of service conﬁguration.\nHealth Systems simulations are usually used for\nplanning of service delivery changes. There is po-\ntential for these type of simulations to be used to\ntest, develop and train Deep RL agents. The moti-\nvation for this integration includes:\n• To perform research on the relative perfor-\nmance of diﬀerent Deep RL methods (e.g.\ncomparison of techniques such as Deep Q\nLearning and Actor-Critic methods).\n• To perform research on the eﬀect of diﬀering\n1\narXiv:2008.07434v1  [cs.LG]  21 Jul 2020\nreward structures on the performance of Deep\nRL agents, and enabling the development of\nreward structures that carefully balance av-\nerage performance with safety (avoiding rare\nbut catastrophic events).\n• Ultimately, to be able to pre-train Deep RL\nagents which would then be transferred to,\nand used in, real world settings.\nIn order to test, train, and develop Deep RL agents,\nwe need a standardised structure that we can use\nacross diﬀerent types of health systems. One such\nstandardised structure, used across many diﬀering\ndomains, already exists, and that is OpenAI Gym\n(gym.openai.com)5. Gym provides a common in-\nterface to a range of problems, from control systems\nthrough to video games. The common interface\nallows the easy transfer of agents from one problem-\nsolving environment to another. Gym is structured\non an episodic framework to learning. The agent is\nexposed to multiple iterations, where the environ-\nment is reset to a ﬁxed or random state, and the\nagent then interacts with the environment through\na series of steps until some terminal state is reached\nindicating the end of the episode. With each step\nthat agent passes an action to the environment. The\nenvironment returns an updated set of observations\nabout the environment state, a reward, whether\nthe terminal state has been reached, and any ex-\ntra information available. Agents are designed to\nmaximise the return of long term rewards.\nIn this paper we present a framework for coding\nHealth Systems simulations, using the commonly\nused Python discrete event simulation package,\nSimPy6 in a framework that allows interaction with\na Deep RL agent. The framework uses RL agent\nmethod calls with high compatibility with OpenAI\nGym, allowing easy transfer of agents developed\nwith/for OpenAI Gym environments. As a demon-\nstration, we use a simple hospital bed simulation\nmodel in SimPy, and show interaction with two\nDeep RL agents (written with PyTorch): a Double\nDeep Q Learning Network and a Duelling Deep Q\nNetwork. Our intention is not to provide a robust\nhospital bed simulation model, nor to provide an\noptimised Deep RL agent for such use, but to demon-\nstrate a framework for combining Gym-compatible\nDeep RL agents with Health Systems simulations\nin SimPy.\n2\nGitHub repository\nThe GitHub repository containing this code is:\nhttps://github.com/MichaelAllen1966/\nlearninghospital\nThe examples cited in this paper are from release\nversion 1.0.0 (DOI 10.5281/zenodo.3936515):\nhttps://github.com/MichaelAllen1966/\nlearninghospital/releases/tag/v0.0.1.\n3\nMethod\n3.1\nGeneric simulation properties\nAll simulations will share some common structure,\nmethods, and attributes.\n3.1.1\nGeneric structure\nAlgorithm 1 shows a high level structure of the code.\nThis will be common to all interactions of Deep RL\nagents and SimPy simulations with only RL-speciﬁc\nalterations (such as the use of target networks and\nmemory).\nAlgorithm 1: High level view of model using A\nDouble Deep Q Network (using policy net, target\nnet, and memory)\nSet up policy net;\nSet up target net;\nSet up memory;\nwhile Training episodes not complete do\nReset sim;\nwhile not in terminal state do\nGet action from policy net;\nPass action to sim;\nTake a time-step in sim;\nReceive (next state, reward, terminal, info)\nfrom sim;\nAdd (state, next state, reward, terminal) to\nmemory;\nRender environment (optional);\nUpdate policy net;\nend\nUpdate target net;\nend\nAssess performance of policy net;\n3.1.2\nGeneric simulation methods\nThe simulation is set up with three methods that\ninterface the Deep RL agent and the simulation:\n• reset: resets the sim to a starting state and\nreturns the ﬁrst set of state observations.\n• step: takes a step in the simulation. Passes an\naction to the simulation. Runs the simulation\nuntil the end of the next time step, and re-\nturns a tuple of next state, reward, terminal,\n2\ninfo. The step method uses the SimPy method\nenv.run(until=target-time), with target-time\nbeing incremented in the desired time steps.\nWhen the simulation time reaches the desired\nmaximum simulation duration, the simulation\nreturns terminal=True.\n• render: displays the current state of the sim-\nulation.\nOther internal methods in the simulation (not ac-\ncessed by the Deep RL agent) that will be common\nto all simulations are:\n• calculate reward:\ncalculates the reward to\npass back to the Deep RL agents.\n• get observations: creates a list of observations\nfrom the state.\n• islegal: checks whether an action from the\nDeep RL agent is legal. If the action is not\nlegal, this method will raise an exception.\n3.1.3\nGeneric simulation attributes\nAll simulations will contain the following attributes.\n• actions: A list of possible actions.\n• action size: The number of possible actions.\n• observation size: The number of features in\nthe observation.\n• state: An object containing the state of the\nsimulation. This may be a simple object, such\nas a list or dictionary, or may be a custom\nPython object.\n3.2\nHospital bed simulation\n3.2.1\nHospital bed simulation overview\nThe hospital bed simulation is a very simpliﬁed\nmodel of a real hospital. Patients arrive at a hospi-\ntal, stay for a given length-of-stay, and leave. The\ninter-arrival time of patients is sampled from an ex-\nponential distribution, the mean of which depends\non the day of week (with average arrival numbers\nbeing higher on weekdays than weekends). The\nlength-of-stay is also sampled from an exponential\ndistribution, the mean of which does not depend on\nday of week. The hospital has a certain number of\nbeds at any time. The Deep RL agent can request\na change to the number of staﬀed beds, but this\nchange is only enacted after 2 days. The simulation\nruns for 365 days by default, and the hospital is\nloaded initially with the expected average number\nof patients.\n3.2.2\nHospital bed simulation state\nThe state in the simulation is held by a dictionary.\nThis dictionary contains:\n• weekday: The current day of week (0-6).\n• beds: The total number of staﬀed beds in the\nhospital (free or occupied).\n• patients: The total number of patients in the\nhospital.\n• spare beds: The number of unoccupied beds.\nIf the number of patients exceeds the num-\nber of staﬀed beds then this number becomes\nnegative and indicated the number of patients\nwithout a bed.\n• pending bed change: The changes in staﬀed\nbed numbers requested by the Deep RL agent,\nbut which has not yet been actualised.\n3.2.3\nHospital bed simulation reward\nThe simulation has a target number of free staﬀed\nbeds. By default this is set at 5% the number of\npatients in the hospital at any given time. The re-\nward is always zero or negative and is the negative\ndiﬀerence between the number of spare beds and\nthe target number of spare beds (equation 1).\nreward = −abs(spare beds −target spare beds) (1)\n3.2.4\nHospital bed simulation methods\nMethods that are speciﬁc to the hospital bed simu-\nlation are:\n• adjust bed numbers: Adjusts the staﬀed bed\nnumbers after a delay (SimPy timeout). Prior\nto the delay, the adjust pending bed change\nmethod is called to track the requested\nchanges in staﬀed bed numbers. The delay\nis the simulation time between the Deep RL\nagent requesting a change to the number of\nstaﬀed beds, and the change being made. The\ndelay is stored in the simulation attribute\ndelay to change beds, and may be set when\ninitializing the simulation. When the num-\nber of staﬀed beds changes, the state dictio-\nnary items beds and pending bed change are\nadjusted accordingly.\n• adjust pending bed change: Adjusts the state\ndictionary item pending bed change when the\nDeep RL agent requests a change to the num-\nber of staﬀed beds.\n3\n• load patients: Loads new patients at the start\nof the simulation, such that the initial number\nof patients in the hospital equals the calcu-\nlated long term average (arrivals per day ∗\naverage length of stay). This method calls\nthe patient spell method. This method incre-\nments the number of patients and staﬀed beds\nby 1 for each patient loaded into the simula-\ntion.\n• new admission: A continuous loop of new pa-\ntients. This method/process is initiated on\nsimulation reset. A new patient arrival is initi-\nated by calling the patient spell method. The\nnumber of patients in the hospital is incre-\nmented by 1. There is then a delay (SimPy\ntimeout) before the next iteration of the loop.\nThe delay is the inter-arrival time of patients.\nThis is sampled from an exponential distribu-\ntion, the mean of which depends on both the\naverage arrival rate (set using arrivals per day\nattribute, which may be set when initializing\nthe simulation. Mean arrivals per day are in-\ncreased by 20% on weekdays (days 0-4), and\nreduced by 50% on weekends (days 5 & 6).\n• patient spell: The patient spell in the hospital.\nLength of stay is sampled from an exponential\ndistribution based on a mean length of stay.\nIf the patient is part of the initial load of the\nhospital, the length of stay is multiplied by a\nrandom number between 0-1 to account for\nthe fraction of the length of stay already com-\npleted. After the spell in hospital is complete\nthe number of patients is reduced by 1, and\nthe number of spare beds recalculated.\n3.2.5\nHospital bed simulation reset method\nThe actions in the simulation reset method (re-\nquired in all simulations for interaction with the\nDeep RL agents) are:\n1. Create new hospital simulation environment.\n2. Initialise simulation processes (new admission\nmethod).\n3. Set starting state values for state dictionary.\n4. Call load patients method.\n5. Get and return ﬁrst set of state observations.\n3.2.6\nHospital bed simulation step method\nThe actions in the simulation step method (required\nin all simulations for interaction with the Deep RL\nagents) are:\n1. Check requested action is legal.\n2. Adjust pending bed change.\n3. Call bed change process.\n4. Make a step in the simulation.\nUse: env.run(until=self.next time stop).\n5. Get new observations.\n6. Check whether terminal state reached (based\non simulation time).\n7. Get reward.\n8. Create an empty information dictionary (this\ndictionary is required to be compatible with\nOpenAI Gym step method).\n9. Render environment if requested.\n10. Return tuple of next state, reward, terminal,\ninfo.\n3.2.7\nHospital bed simulation attributes\nAttributes that are speciﬁc to the hospital bed sim-\nulation are:\n• arrivals per day: Average arrivals per day.\n• delay to change beds: Time between request-\ning change in beds, and change in beds hap-\npening (days).\n• los: Average patient length of stay (days).\n• sim duration:\nLength of simulation run\n(days).\n• target reserve: target free staﬀed beds as a\nproportion of the number of patients present.\n• time step: Time between action steps (day).\n3.3\nDeep\nReinforcement\nLearning\nAgents\nA range of standard Deep RL agents were imple-\nmented for this model (and are provided in separate\nnotebooks in the associated GitHub repository):\n1. Double Deep Q Network (D2QN): Standard\nDeep Q Network, with policy and target net-\nworks7.\n2. Duelling Double Deep Q Network (D3QN) 8:\nPolicy and target networks calculate Q from\nsum of *value* of state and *advantage* of\neach action (*advantage* represents the added\nvalue of an action compared to the mean value\nof all actions).\n3. Noisy D3QN 9: Networks have target layers\nthat add Gaussian noise to aid exploration.\n4\nFigure 1: Example model. A Bagging Duelling Double Deep Q Network agent trained to manage bed\ncapacity of the hospital bed simulation. Left: Exploration (epsilon, the probability of taking an action\npurely at random, green) and average reward across the training episode (red). Right: The number of\npatients (red) and staﬀed beds (green) in the last training run\n4. Prioritised replay D3QN 10: When training\nthe policy network, steps are sampled from\nthe memory using a method that prioritises\nsteps where the network had the greatest error\nin predicting Q.\n5. Bootstrapped D3QN 11: Multiple networks are\ntrained from diﬀerent bootstrap samples from\nthe memory.\n6. Combinations of the above\n4\nResults\nThe GitHub repository contains examples of the\nvarious Deep RL agents implemented.\nThe output of the Bagging D3QN is shown in ﬁgure\n1. It is not the intention of this paper to present a\nfully optimised Deep RL agent, but it can be seen\nthat the example network improves in performance\nover time (repeated model runs) and manages the\nmodelled bed stock appropriately.\n5\nDiscussion\nCombining Deep RL and Health Systems Simula-\ntions has signiﬁcant potential, for both research\ninto improving Deep RL performance and safety,\nand in operational practice. Our aim in this pa-\nper is not to present an optimised Deep RL model,\nor a detailed hospital simulation, but to provide a\nframework that is compatible with OpenAI Gym\nenvironments, enabling easy transfer of the many\nmethods that have been developed and tested in\nsuch environments.\nThe potential for combining Deep Learning and\nHealth Systems Simulations goes beyond the frame-\nwork provided here. For example, we have demon-\nstrated that a machine learning model can be used\nto simulate patient-level clinical decision making12\nas part of broader clinical pathway simulation study.\nThe combination of Deep Learning and Health Sys-\ntems Simulation is an area of research that will\nhopefully bear much fruit in the coming years.\n6\nReferences\nReferences\n[1] Y. Li, “Deep reinforcement learning:\nAn\noverview,” arXiv:1701.07274 [cs], Nov. 2018.\n5\narXiv: 1701.07274.\n[2] T. Monks and R. Meskarian, “Using simula-\ntion to help hospitals reduce emergency depart-\nment waiting times: Examples and impact,” in\n2017 Winter Simulation Conference (WSC),\npp. 2752–2763, Dec. 2017. ISSN: 1558-4305.\n[3] M. L. Penn, T. Monks, A. A. Kazmierska,\nand M. R. A. R. Alkoheji, “Towards generic\nmodelling of hospital wards:\nReuse and\nredevelopment of simple models,” Journal\nof Simulation, vol. 14, pp. 107–118, Apr.\n2020.\nPublisher: Taylor & Francis\neprint:\nhttps://doi.org/10.1080/17477778.2019.1664264.\n[4] T. Monks, D. Worthington, M. Allen, M. Pitt,\nK. Stein, and M. A. James, “A modelling tool\nfor capacity planning in acute and commu-\nnity stroke services,” BMC Health Services Re-\nsearch, vol. 16, p. 530, Sept. 2016.\n[5] G. Brockman, V. Cheung, L. Pettersson,\nJ. Schneider, J. Schulman, J. Tang, and\nW. Zaremba, “Openai gym,” arXiv:1606.01540\n[cs], June 2016. arXiv: 1606.01540.\n[6] SimPy,\n“Simpy.\ndiscrete\nevent\nsimulation\nfor\npython.,”\nhttps://simpy.readthedocs.io/en/latest/,\n2020.\n[7] H. van Hasselt, A. Guez, and D. Silver,\n“Deep Reinforcement Learning with Double Q-\nlearning,” arXiv:1509.06461 [cs], Dec. 2015.\narXiv: 1509.06461.\n[8] Z. Wang, T. Schaul, M. Hessel, H. van Has-\nselt, M. Lanctot, and N. de Freitas, “Dueling\nNetwork Architectures for Deep Reinforcement\nLearning,” arXiv:1511.06581 [cs], Apr. 2016.\narXiv: 1511.06581.\n[9] M. Fortunato, M. G. Azar, B. Piot, J. Menick,\nI. Osband, A. Graves, V. Mnih, R. Munos,\nD. Hassabis, O. Pietquin, C. Blundell, and\nS. Legg, “Noisy Networks for Exploration,”\narXiv:1706.10295 [cs, stat], July 2019. arXiv:\n1706.10295.\n[10] T. Schaul, J. Quan, I. Antonoglou, and\nD. Silver, “Prioritized Experience Replay,”\narXiv:1511.05952 [cs], Feb. 2016.\narXiv:\n1511.05952.\n[11] I. Osband,\nC. Blundell,\nA. Pritzel,\nand\nB. Van Roy, “Deep Exploration via Boot-\nstrapped DQN,” arXiv:1602.04621 [cs, stat],\nJuly 2016. arXiv: 1602.04621.\n[12] M. Allen, K. Pearn, T. Monks, B. D. Bray,\nR. Everson, A. Salmon, M. James, and K. Stein,\n“Can clinical audits be enhanced by pathway\nsimulation and machine learning? An example\nfrom the acute stroke pathway,” BMJ Open,\nvol. 9, p. e028296, Sept. 2019.\nFunding\nThis study was funded by the National Institute for Health Research (NIHR) Applied Research Col-\nlaboration (ARC) South West Peninsula. The views and opinions expressed in this paper are those of\nthe authors, and not necessarily those of the NHS, the National Institute for Health Research, or the\nDepartment of Health.\n6\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-07-21",
  "updated": "2020-07-21"
}