{
  "id": "http://arxiv.org/abs/2008.07524v3",
  "title": "Reinforcement Learning with Quantum Variational Circuits",
  "authors": [
    "Owen Lockwood",
    "Mei Si"
  ],
  "abstract": "The development of quantum computational techniques has advanced greatly in\nrecent years, parallel to the advancements in techniques for deep reinforcement\nlearning. This work explores the potential for quantum computing to facilitate\nreinforcement learning problems. Quantum computing approaches offer important\npotential improvements in time and space complexity over traditional algorithms\nbecause of its ability to exploit the quantum phenomena of superposition and\nentanglement. Specifically, we investigate the use of quantum variational\ncircuits, a form of quantum machine learning. We present our techniques for\nencoding classical data for a quantum variational circuit, we further explore\npure and hybrid quantum algorithms for DQN and Double DQN. Our results indicate\nboth hybrid and pure quantum variational circuit have the ability to solve\nreinforcement learning tasks with a smaller parameter space. These comparison\nare conducted with two OpenAI Gym environments: CartPole and Blackjack, The\nsuccess of this work is indicative of a strong future relationship between\nquantum machine learning and deep reinforcement learning.",
  "text": "*correspondence: lockwo@rpi.edu\nREINFORCEMENT LEARNING WITH QUANTUM VARIATIONAL\nCIRCUITS\nOwen Lockwood1∗and Mei Si2\n1Department of Computer Science, Rensselaer Polytechnic Institute\n2Department of Cognitive Science, Rensselaer Polytechnic Institute\nAbstract\nThe development of quantum computational techniques has advanced greatly in recent years, parallel to the advancements in techniques\nfor deep reinforcement learning. This work explores the potential for quantum computing to facilitate reinforcement learning problems.\nQuantum computing approaches offer important potential improvements in time and space complexity over traditional algorithms because\nof its ability to exploit the quantum phenomena of superposition and entanglement. Speciﬁcally, we investigate the use of quantum\nvariational circuits, a form of quantum machine learning. We present our techniques for encoding classical data for a quantum variational\ncircuit, we further explore pure and hybrid quantum algorithms for DQN and Double DQN. Our results indicate both hybrid and pure\nquantum variational circuit have the ability to solve reinforcement learning tasks with a smaller parameter space. These comparison\nare conducted with two OpenAI Gym environments: CartPole and Blackjack, The success of this work is indicative of a strong future\nrelationship between quantum machine learning and deep reinforcement learning.\n1\nIntroduction\nDeep reinforcement learning (RL) has accelerated at as-\ntounding speed in the last decade. Achieving superhuman\nperformance in massively complex games such as Chess, Go\n[27], StarCraft II [29], Dota 2 [5], and all 57 Atari games [2],\ndeep RL has become a critical tool for Game AI. Many RL\nalgorithms are benchmarked with games. The improvements\nin recent years in deep RL algorithms are often driven by\nthe desire to improve the objective score performance of an\nagent and/or to reduce the training time or model size.\nParallel to the impressive development of deep RL is the\nequally outstanding developments in quantum computing.\nEarly theoretical work demonstrated the massive potential\nof quantum computers, such as Grover’s algorithm, which\nenables searching an unsorted list in O(\n√\nN) time [15], and\nShor’s algorithm which can break cryptosystems like RSA\nin polynomial time [26]. Only recently has quantum compu-\ntation become more reality than spectre, with some claims\nof quantum supremacy, i.e. solving a problem that cannot be\ncalculated on a traditional computer in any feasible amount\nof time [1]. Quantum algorithms offer unique potentials be-\ncause of their exploitation of quantum mechanical properties,\nsuch as superposition and entanglement (see the Quantum\nComputing section for more details).\nUsing quantum computing to help with machine learning\ntasks has attracted a lot attention in recent years. Quantum\nmachine learning has signiﬁcant potential to improve the\nspeed of machine learning algorithms, with quantum percep-\ntrons and quantum RL having theoretical potential for O(\n√\nN)\nspeedups [6]. Already work has been done to develop quan-\ntum GANs [31] and quantum CNNs [11]. Recently, the quan-\ntum RL ﬁeld has been expanding with a variety of approaches\nsuch as using Grover Iterations [14] and CV photonic gates\n[16] to solve gridworld environments. Other work has been\ndone to envisage quantum computing as a RL problem [17].\nWe explore the potential of utilizing quantum computing\nto aid with reinforcement learning tasks. We take inspira-\ntion from and extend the work done in [10] to use Quan-\ntum Variational Circuits (QVC), quantum circuits with gates\nparametrized by learnable values, in reinforcement learning.\nIn [10], QVCs were used with Double DQN to solve the deter-\nministic 4x4 Frozen Lake OpenAI Gym environment. They\nreported that the parameter space complexity scales O(N) in\nQVCs which is a signiﬁcant improvement over the traditional\nneural network DQN which has parameter space complexity\nO(N2). However, their work only investigated QVCs with\none algorithm that operated on a simple deterministic environ-\nment with a single input value and the observation space and\noutput space we restricted to be the same number of quantum\nbits. We expand upon their work, evaluating more algorithms\nand multiple types of QVCs, creating new encoding schemes,\nand advancing to more complex environments.\nIn this work, we use a quantum simulator to explore the\npotential for using quantum computing to solve reinforce-\nment learning tasks. Expanding upon previous work, we\npresent algorithms and encoding techniques that improve\nupon previous results. We apply our techniques to OpenAI\nGym environments more complex than previous quantum\nRL work with largely positive results. Our results are indica-\ntive of the potential power of quantum computing in aiding\nreinforcement learning.\n2\nReinforcement Learning\nThe general formulation of reinforcement learning can be\ndeﬁned by an agent interacting with an environment attempt-\ning to maximize its reward function. This is often formulated\nas a Markov Decision Process (MDP). An MDP is charac-\nterized by the tuple ⟨S, A, Pa, R⟩, where S is the set of states,\nA the set of actions, Pa is the probability of state transition\nPa = P[st+1 = s′|st = s, at = a], and R is the reward given\nfor executing action at at state st. In this work, our envi-\narXiv:2008.07524v3  [quant-ph]  28 Aug 2020\nREINFORCEMENT LEARNING WITH QUANTUM VARIATIONAL CIRCUITS\n2\nronments lack stochasticity and thus Pa = 1. The goal is\nto design an agent that operates policy π, π(st) = at, such\nthat it maximizes the expected reward, E[P∞\nt=0 R(st, at)|π]. In\nlearning the policy, the future reward is often discounted by\na parameter γ.\nIn order to learn π, deep RL relies on neural networks\nparametrized by weights and biases θ. This paper relies on Q\nvalues estimation algorithms; the Q value being deﬁned by\nQ(st, at) = rt + maxat+1Q(st+1, at+1), where Q(st, at) is the Q\nvalue (or numerical estimation of reward) of taking action at\nat state st, rt is the reward and maxat+1Q(st+1, at+1) is the max-\nimum future Q value. This Q function usually represented by\na neural network. The general Q learning policy is deﬁned\nfor discrete actions spaces and can be formulated as such,\nπ(s; θ) = maxaQ(s; θ), i.e. the policy parametrized by θ is to\nchoose the action that has the maximum Q value. To learn\nthe policy utilizing neural networks a variation of the Bell-\nman equation can be employed to calculate the mean squared\nloss function and from there, the gradients needed for back-\npropagation, Lt(θ) = E[(rt + maxa′Q(s′, a′; θ) −Q(s, a; θ))2]\n[21]. This max operation can lead to over-estimations of\nthe Q value, leading to convergence problems. A number\nof improvements to the vanilla DQN algorithm have been\nsuggested. Double DQN [28], has a separate target network\nsolely for predicting the future Q value inside the max opera-\ntion, dueling DQN [30] has separate network heads predict\nthe advantage and value components of the Q value, distribu-\ntional DQN [3], and noisy nets [13], to name a few. In this\nwork the traditional DQN is used, to establish a base which\nvariations can be applied and Double DQN because it does\nnot require signiﬁcant restructuring of the internals of the Q\nestimation function.\n3\nQuantum Computing\n3.1\nQubits and Superposition\nThe ﬁrst important feature of quantum computing, criti-\ncal to its representational and computational power, is the\nconcept of superposition. In a classical computer, data is\nrepresented as a (binary) 0 or 1 and can be ﬂipped between\nthese states. The base unit of quantum computing is the quan-\ntum bit (qubit). Qubits rely on the quantum phenomenon of\nspin. The spin of a qubit is represented mathematically in\nthe wavefunction. A quantum mechanical wavefunction, Ψ,\nrepresents the state of a system and can be a linear combina-\ntion of components, e.g. Ψ = α|0⟩+ β|1⟩. These coefﬁcients\nrepresent the probability amplitude of the wavefunction, i.e.\nR ∞\n−∞|Ψ(x, t)|2dx = 1. Because a qubit can store information\nin this superposition, information representation scales with\nN qubits by O(2N) rather than O(N) as with traditional com-\nputers. A single qubit can be visualized via a Bloch Sphere\nrepresenting its wavefunction, see Figure 1 which is taken\nfrom [9]. Classical binary states (i.e. 0 or 1) can be repre-\nsented in the Z direction, |0⟩=\n\"\n1\n0\n#\nrepresents spin up and\n|1⟩=\n\"\n0\n1\n#\nrepresents spin down. However, as Figure 1 demon-\nstrates, the state of the qubit can be anywhere on the sphere.\nThus, this allows two states to be simultaneously represented\nin a ’superposition’ (i .e. linear combination). Because the\nsuperposition is one of a probabilistic nature, when the mea-\nsurement operator is applied the superposition collapses and\nonly one state is measured, i.e. only a 0 or 1 is measured.\nFigure 1: Bloch sphere representation of a qubit\n3.2\nGates\nIn order to manipulate qubits, unique quantum gates must\nbe used. There are many different quantum gates, but the ones\nmost relevant to this work is the controlled NOT (CNOT)\ngate, Pauli gates and rotation gates. CNOT is important for its\nability to induce entanglement. When qubits are entangled,\nthey can no longer be represented as truly separate wave-\nfunctions. Consider the two qubit entangled wavefunction\nΨ = |10⟩+|01⟩\n√\n2\n, i.e. an equal superposition of the states |10⟩and\n01⟩. This wavefunction cannot be represented by two distinct\nsingle qubit wavefunctions, Ψ = (p|0⟩+ q|1⟩)(r|0⟩+ s|1⟩),\nwhere p, q, r, s are the coefﬁcients that when squared yield the\nprobability of measuring the qubit in that state. This would\nrequire q ∗r = p ∗s = 1/\n√\n2 and p ∗r = q ∗s = 0, which\nwould require either p or r to be 0 making the ﬁrst equation\nimpossible. Thus, the qubits are called entangled because\nthey are no longer isolated systems and share a wavefunction.\nWhen entangled actions done on either qubit will result in\na change of the wavefunction, which will effect both qubits.\nEntanglement is important for quantum computing because\nit allows one operation to have an effect on multiple qubits\nin superposition, and doing the same on a classical computer\nwould require many operations. CNOT is a two qubit gate\nand when acting on a purely spin up/spin down pair converts\n|11⟩into |10⟩and |10⟩into |11⟩. However, when the qubits\nare not in pure spin up/down states, the wavefunction effects\ncannot be reﬂected in traditional computers (as changes in the\nwavefunction in superpositions are only allowed due to the\nquantum mechanical exploitation). The Pauli X, Y, Z gates\nﬂip the wavefunction about the speciﬁed axis. The rotation\ngates are a specialized version of the Pauli gates. he rotation\ngates are denoted: Rx, Ry, and Rz and these gates rotate the\nqubit about the speciﬁed axis by the given θ radians.\n3.3\nQuantum Variational Circuits\nQVCs are a collection of gates that operate on a set of\nqubits [4]. They have a deﬁned initial/input circuit, and a set\nREINFORCEMENT LEARNING WITH QUANTUM VARIATIONAL CIRCUITS\n3\nof qubits and a collection of gates, parametrized by θ, make\nup the body of the QVC. This collection of gates are denoted\nby U(θ). The parameters, θ, is what is being ’learned’. For a\ngiven input, the circuit is evaluated and a ’readout operator’ is\napplied to extract information from these gates as an output.\nThe readout operator we use is the Pauli Z gate. I.e. the Pauli\nZ gate is applied which results in a numerical measurement. It\nis possible to apply other Pauli gates here and readout in that\nbasis. We use the Pauli Z gate because Z, the ’computational\nbasis’ is common. External to the circuit a loss function and\ngradients are calculated in order to update the parameters. In\nthis work, the loss function is the mean squared error.\nCalculating the gradient for a QVC requires different tech-\nniques than a neural network because the mathematical oper-\nations are fundamentally different. In this work the parameter\nshift differentiator is used. Although this is implemented by\nTensorFlow-Quantum the gradient calculations are an im-\nportant difference between QVCs and deep neural networks,\nas such it is important to provide an adequate mathematical\noverview. In order to understand the gradient calculation for-\nmula we must understand both the individual gates and QVC\nas a function. The collection of gates U(θ) can be separated\ninto a collection of N layers operating on M qubits. For a\ngiven layer ℓ, it can be represented as a set of single qubit ro-\ntation gates operating in parallel, Uℓ(θℓ) =\nNM\ni=1 Uℓ\ni (θℓ\ni ) [8].\nWithin this layer, each gate can enact a rotation of the qubit.\nThis rotation of angles can be expressed similar to Euler’s\nform: Uℓ\ni (θℓ\ni ) = e−iaGθℓ\ni . Where G is a linear combination of\nPauli gates (called a generator). G can be represented as a\n2 × 2 matrix (like all Pauli gates) and has two eigenvalues\ne0, e1 [12]. The derivative of this is straightforward, due to the\nnature of exponentials,\n∂\n∂θU(θ) = −iaGe−iaGθ = −iaGU(θ).\nPrior to applying this differentiation we must present a big\npicture view of a QVC. A QVC is a function with param-\neters θ. The output of this function is what the result of\nthe Z gates. Thus, the QVC is a function that results in\nthe expectation values from the Z gates.\nPrior to these\nZ gates being applied, however, the parametrized gates\nare applied.\nThese parametrized gates change the start-\ning wavefunction prior to the Z gates (as they are the very\nlast gates used to generate output), or in quantum mechani-\ncal notation: f(θ) = ⟨Ψ0|U†(θ) ˆZU(θ)|Ψ0⟩. The parameter\nshift rule states that\n∂\n∂θ f(θ) = ⟨Ψ0|( ∂\n∂θU†(θ)) ˆZU(θ)|Ψ0⟩+\n⟨Ψ0|U†(θ) ˆZ( ∂\n∂θU(θ))|Ψ0⟩. We can combine this parameter\nshift rule with the derivative calculated above [25]. This\ncan then be reduced down to the ﬁnal differentiating rule:\n∂\n∂θ f(θ) = r[ f(θ + π\n4r) −f(θ −π\n4r)], where r = a\n2(e1 −e0) [12].\nThis last equation is the parameter shift technique for how\ngradients are calculated for a QVC as seen in algorithm 1.\n4\nApproach\nIn this work, we explore the potential for using QVCs\nin place of neural networks in RL algorithms. Substituting\nQVCs for neural networks requires almost no modiﬁcation\nto the traditional algorithm. We evaluate four different varia-\ntions, either being a pure QVC model or a hybrid QVC (i.e.\na QVC which has outputs then fed into a single dense layer)\nin place of a neural network in either DQN and Double DQN.\nWhile all QVC are ’hybrid’ in that they utilize traditional\ncomputers for loss calculations, we use the pure/hyrbid termi-\nnology to refer to whether or not the output of the QVC is fed\ninto a single dense neural network layer. Hybrid QVCs have\nseen limited use in quantum RL but their positive results are\nimportant as they allow differences in the qubit observation\nand action spaces. We also modify the pure QVC technique\nto allow for differences in observation and action space by\ncombining quantum pooling operations [11] with traditional\nQVCs. We also experimentally evaluate QVCs representa-\ntional power, as our results indicate that a QVC can perform\ncomparably to neural networks with total parameters at least\nan order of magnitude larger. We present two new encoding\nschemes for different types of input data, both suitable for\nenvironments with arrays as inputs, an important advance-\nment as many RL environments have more than single integer\nobservation spaces. This work was done on simulations on a\nclassical computer using the Noisy Intermediate Scale Quan-\ntum (NISQ) [23] simulator Cirq (from Google AI Quantum)\nand TensorFlow-Quantum (TFQ) [8].\n4.1\nQuantum Data Encoding\nSpecial techniques are needed to work with classical data\non a quantum device or simulator. Although one can convert\nall classical data to binary and represent that binary with the\nqubits, this is very inefﬁcient, since a single precision ﬂoat-\ning point would then take 32 qubits (a substantial amount).\nWhile, theoretical encoding schemes do exist, e.g. an encod-\ning scheme for arbitrary state preparation [20] and ﬂexible\nrepresentation of quantum images [19] these approaches are\nnot yet suited for the use in QVC RL. The techniques in [20]\nare cost efﬁcient, but require gates beyond TFQ’s simulating\npower. And the technique from [19] uses available gates, but\nit required hundreds of gates which we found to be too high,\nmaking it impractical for data intensive applications like RL.\nIn order to solve the problems mentioned above we present\ntwo approaches to data encoding. They are both fast and ef-\nfective, however, they are slightly below the theoretically\noptimal data representation. Although utilizing the same\ngates as [10], these are fundamentally different algorithms.\nThe technique presented in [10] converts a single integer into\nbinary then uses that as input into the gates. Our algorithms\ncan handle multiple inputs of both integers and ﬂoats (which\nare impractical to convert to binary). Our algorithms are also\nmore efﬁcient in terms of qubit usage, requiring O(N) qubits\n(N = size of input array), superior to converting all numbers to\nbinary as converting to binary would scale O(Nlogn) with in-\nput (N = number of input elements, n = size of input integer).\nThe ﬁrst encoding scheme, which we call Scaled Encoding, is\nfor environments that have input values with deﬁned ranges.\nThe process is simple, scale each input between 0 and 2π\nand rotate along Rx and Rz with the corresponding radians.\nThis is fast (requiring only 2 gates per qubit) and is shown to\nbe experimentally effective (see Blackjack section for more\ndetails). However, in some environments, e.g. CartPole, the\nREINFORCEMENT LEARNING WITH QUANTUM VARIATIONAL CIRCUITS\n4\nrange on some data points is between −∞to ∞and the data is\nskewed such that even inserting artiﬁcial range cutoffs would\nbe impractical. For environments such as this we use a differ-\nent scheme, called Directional Encoding, deﬁned by rotating\neach qubit Rx and Rz by either π or 0 radians determined by\nthe simple conditional: radians = π if datapoint > 0 else 0.\nI.e. if, and only if, the value is positive we rotate, speciﬁcally\nπ radians. This also only requires 2 gates per qubit and is\nshown to be experimentally viable. These encoding schemes\nscale O(N) with the size of the input array, worse than the\noptimal O(logN). These schemes represent an advancement\nover [10] because of their abilities of take more than 1 input\nvalue and improve on the representational complexity.\n4.2\nModel Architecture\nWe implement and compare two versions of QVC models.\nIn both cases, the body of the QVC is the same. It is com-\nposed of several ‘layers’ (they are called layers only because\nof visual aspects, the mathematical operations are not those\nof a neural network layer), seen in Figure 2 (Figure 2 and\nFigure 3 were generated with IBM Quantum Experience Cir-\ncuit Composer). In this work, the QVCs use three layers. I.e.\nthe block speciﬁed in Figure 2 is repeated 3 times. These are\ncomposed of Rx, Ry, and Rz gates parametrized by different\nθ. In addition, there is a collection of CNOT gates in front of\nthe rotations whose primary goal is to entangle the qubits.\nFigure 2: One ’layer’ of the QVC, composed of CNOT and\nparametrized rotation gates\nIn the hybrid model, the output is fed into a single dense\noutput layer, one that has the same number of nodes as the\naction space. Thus, we reduce (or expand) the output to ﬁt\nthe action space. In pure QVC, we rely on quantum pooling\ntechniques. The function of the quantum pooling operation\nis very similar to traditional pooling operations, it seeks to\ncombine and reduce the size of layer so that the observation\nspace can be reduced to the action space without losing in-\nformation. Using the quantum pooling operation from [11],\nwe can reduce the 2 qubits to 1 qubit, applying this operation\nas many times as needed. The pooling operation is deﬁned\nparametrized Rx, Ry, Rz gates, a CNOT gate, followed by the\nparametrized inverse rotation gates R−1\nx , R−1\ny , R−1\nz . Figure 3\nshows a diagram of a single pooling operation where the (1)\nand (-1) represent the power.\nFigure 3: Quantum pooling operation\n4.3\nQVC Versions of DQN and DDQN\nAlgorithms\nThe Double DQN and DQN algorithm are effectively the\nsame as in [28]. There are no algorithmic changes from the\nestablished DDQN algorithm. There are, however, necessary\nimplementation differences. Prior to feeding the data into the\nQVC, the data must be encoded. The replay buffer functions\nin the same way as in traditional approaches, keeping track\nof the ⟨s, a, r, s′⟩tuples just with a different, encoded, state\nrepresentation. We see this as one of the important facets\nof this work, the ease of implementation within existing\nalgorithms. One does not have to fundamentally or drastically\nchange an algorithm in order to apply the power of QVCs to\nit. The algorithm presented in Algorithm 1.\nAlgorithm 1: Q-DDQN\nInitialize replay buffer D\nInitialize action QVC θ, target QVC θt ←−θ\nfor episode = 0, N do\nencode s1 to quantum circuit ξ1 = ξ(s1)\nwhile game is not ﬁnished do\nselect action at via ϵ-greedy policy\nexecute action at and observe reward rt and next\nstate st+1\nencode st+1, ξt+1 = ξ(st+1)\nstore MDP tuple ⟨ξt, at, rt, ξt+1⟩in D\nselect random minibatch from D\nset yi = ri + γmaxa′Q(ξi+1, a′; θt)\ncalculate loss L according to L(θ) = (yi −Q(ξi, a; θ))2\nupdate parameters utilizing differentiator:\n∂\n∂θ f(θ) = r[ f(θ + π\n4r) −f(θ −π\n4r)]\nif episode mod C == 0 then\nθt ←−θ;\nelse\nθt = τθt + (1 −τ)θ\n5\nExperiments\nAll experiments were conducted with OpenAI Gym [7],\nspeciﬁcally the CartPole and Blackjack environments. We\nchose these environments because they represent an advance-\nment in complexity over previous research in quantum RL\nwhich has largely been dominated by gridworld environments,\nboth in terms of policy complexity and input complexity (i.e.\ntheir input arrays are larger and have more possible values).\nThey are also very different from each other, with different\nreward functions and strategies and utilize different encod-\ning schemes. CartPole works with the Directional Encoding\nREINFORCEMENT LEARNING WITH QUANTUM VARIATIONAL CIRCUITS\n5\nbecause the range of the input values is inﬁnite. Blackjack\nutilizes the Scaled encoding scheme and demonstrates that\nthis scheme is able to properly encode the magnitude of the\ndata (which is lost for CartPole). This magnitude of the input\nis critical as the optimal strategy for playing blackjack with\n1 point vs 20 points is substantially different (as if you have\nmore than 21 points you bust and lose). While neither are as\ncomplex environments as StarCraft II or Dota 2, they demon-\nstrate an notable advancement in complexity over previous\nwork and are used as benchmarks for RL algorithms [22]. Al-\ngorithm parameters were constant across experiments: initial\nϵ of 1.0, ϵ decay of 0.9, ϵ minimum of 0.01, and a reward\ndiscount, γ, of 0.95. The optimizer, ADAM [18], and associ-\nated hyperparameters were constant for all experiments. The\nwall-clock training time of these on a single NVIDIA GTX\n1070 GPU ranges from 5-30 minutes.\n5.1\nCartPole\nThe ﬁrst environment is CartPole, which we use to com-\npare QVC based DQN/DDQN with traditional deep neural\nnetwork DQN/DDQN. The Directional encoding scheme is\napplied to both the neural network and the QVC. Speciﬁcally,\nthis means that just as the qubits are encoded with 0s and\n1s, so too does the neural network receive a binary array.\nAll graphs begin at 50 iterations because the average reward\nis calculated by averaging over the previous 50 iterations.\nFigure 4 shows a comparison between the traditional neural\nnetwork DQN and the two types (pure, hybrid) of QVC used.\nAll shaded areas represent the 95% conﬁdence interval over\n6 runs. This ﬁgure demonstrates that both hybrid and pure\nQVC models achieve a better policy and arrive at this policy\nfaster than traditional neural networks. Figure 5 demonstrates\nthe same comparison, using the Double DQN algorithm. This\nexperiment demonstrates that the QVC models perform at\nleast as well, if not better, than the neural network based\nmodels.\nFigure 4 and Figure 5 include neural networks with dif-\nferent numbers of parameters to show how well the QVCs\nperform in terms of representational abilities too. These ﬁg-\nures show that QVCs have superior representational power\nover neural networks. In this work, the pure QVC has 48\ntrainable parameters in each ’network’. Each ﬁgure shows\na comparison between the pure and hybrid QVC with pa-\nrameter space of 101 and neural networks with parameters\non order 101, 102, 103, speciﬁcally 58, 226, and 1,282 train-\nable weights (with 1, 2, 3 intermediate layers). This experi-\nmentally demonstrates the encoding and potential represen-\ntational strength of QVCs, as they operate comparable to\nneural networks with orders of magnitude more parameters.\n5.2\nBlackjack\nBlackjack’s different reward approach stems from the fact\nthat it originates as a casino game, designed such that the\nhouse always wins, i.e. the optimal policy is right below\nthe 0 reward mark. This explains the discrepancy between\nthe results in Figures 6 and 7 and for CartPole. Because\nFigure 4: Comparison of NN and QVC DQN on CartPole\nFigure 5: Comparison of NN and QVC DDQN on CartPole\nthe optimal policy is much more limited, and thus easier to\nattain, both pure and hybrid QVC achieve similar results.\nFigures 6 and 7 show comparisons of the DQN and DDQN\nalgorithms, respectively. We compare the speed at which the\nmodel’s learn is a metric by which comparisons can be made.\nThe enlarged areas in Figures 6 and 7 demonstrate how the\nquantum approaches learn slightly faster than the same order\nparameter neural network approach (although slightly below\nthe higher parameter networks). In this example the exact\nnumber of parameters are 33 for the QVC, and for the neural\nnetworks 38, 194, and 1,250 (with 1, 2, 3 intermediate lay-\ners). A random agent is also included to establish a baseline\nperformance. This random agent is not included in CartPole\nas CartPole does not have as much inherent randomness that\ncan cause random agents to perform at all comparably to RL\nagents and would be almost about 0. This shows that the\nScaled encoding scheme can be effective.\nREINFORCEMENT LEARNING WITH QUANTUM VARIATIONAL CIRCUITS\n6\nFigure 6: Comparison of NN and QVC DQN on Blackjack\nFigure 7: Comparison of NN and QVC Double DQN on\nBlackjack\n6\nDiscussion\n6.1\nDifferences in Hybrid and Pure\nWhile in three out of the four tests, the hybrid model per-\nformed comparably to the pure model, the Double DQN\nCartPole is the exception. This is slightly different in that\nthe pure QVC model achieves a superior policy, whereas\nthe hybrid performs similar to the neural network. This is\nthe only case in which the discrepancy is signiﬁcant. We\nsuspect that hybrid models converge faster (for the early time\nsteps the hybrid model is scoring better than the pure) and\nit is possibly converging on a local optimum rather than the\nglobal optimum policy because of its higher convergence\nspeed. This faster convergence of the hybrid model becomes\napparent in the Blackjack experiment. Because the optimal\npolicy is less complex in this case, faster convergence be-\ncomes advantageous.\n6.2\nGeneralizability\nThis work suggests the potential for improved generaliz-\nability. Because the encoding schemes are more ﬂexible and\nthe observation and action spaces are no longer tied together,\nthis should expand the problem space that these algorithms\ncan be applied to. Naturally quantum data is also compatible\nwith this approach. The encoding schemes should make the\napproach more general by allowing the input to be arrays of\nﬂoating point values and integers. The Directional encoding\nscheme is designed to be applied to inputs that have an inﬁ-\nnite range in which the magnitude does not matter, and the\nScaled encoding scheme can be applied to inputs with well\ndeﬁned ranges that do not have a signiﬁcant skew. This work\nis also generalizable to future improvements to the DQN al-\ngorithm, as any improvements made to the DQN algorithm\ncan be utilized by this technique as well, e.g. prioritized\nexperience replay [24].\n6.3\nFuture Work\nAlthough this work suggests the potential for improved\ngeneralizability, further work is needed to verify this. To\nverify the generalizability, experiments should be conducted\nwith more variations on the hyperparameters of the Quantum\nVariational Circuits and different applications. Expanding\nthe applications of these algorithms to more complex envi-\nronments (e.g. Atari) is a natural next step. In addition, we\nare interested in investigating more encoding schemes, as\ndescribed in the Quantum Data Encoding Section. This is\npredicated upon the necessary gates becoming available in\nTensorFlow-Quantum.\n7\nConclusion\nThis work expands upon previous ideas and algorithms in\nthe ﬁeld of quantum computing and reinforcement learning\nto present Quantum Variational Circuit approaches to solve\nreinforcement learning tasks. We introduce and demonstrate\nthe potential of both hybrid and pure Quantum Variational\nCircuits in both Double DQN and DQN algorithm variations\nusing the CartPole and Blackjack OpenAI Gym environments.\nThis work also demonstrate the potential of our two new clas-\nsical to quantum data encoding schemes: Scaled encoding\nand Directional encoding. The success that these models\nachieved on both environments suggests that Quantum Varia-\ntional Circuits may have representational abilities superior to\ntraditional neural networks. This work is indicative of the po-\ntentially impactful relationship between quantum computing\nand reinforcement learning.\nReferences\n[1] Arute, F.; Arya, K.; Babbush, R.; Bacon, D.; Bardin,\nJ. C.; Barends, R.; Biswas, R.; Boixo, S.; Brandao, F. G.;\nREINFORCEMENT LEARNING WITH QUANTUM VARIATIONAL CIRCUITS\n7\nBuell, D. A.; et al.\n2019.\nQuantum supremacy us-\ning a programmable superconducting processor. Nature\n574(7779):505–510.\n[2] Badia, A. P.; Piot, B.; Kapturowski, S.; Sprechmann, P.;\nVitvitskyi, A.; Guo, D.; and Blundell, C. 2020. Agent57:\nOutperforming the atari human benchmark. arXiv preprint\narXiv:2003.13350.\n[3] Bellemare, M. G.; Dabney, W.; and Munos, R. 2017.\nA distributional perspective on reinforcement learning.\nIn Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, 449–458. JMLR. org.\n[4] Benedetti, M.; Lloyd, E.; Sack, S.; and Fiorentini, M.\n2019. Parameterized quantum circuits as machine learning\nmodels. Quantum Science and Technology 4(4):043001.\n[5] Berner, C.; Brockman, G.; Chan, B.; Cheung, V.; Debiak,\nP.; Dennison, C.; Farhi, D.; Fischer, Q.; Hashme, S.;\nHesse, C.; et al. 2019. Dota 2 with large scale deep\nreinforcement learning. arXiv preprint arXiv:1912.06680.\n[6] Biamonte, J.; Wittek, P.; Pancotti, N.; Rebentrost, P.;\nWiebe, N.; and Lloyd, S. 2017. Quantum machine learn-\ning. Nature 549(7671):195–202.\n[7] Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.;\nSchulman, J.; Tang, J.; and Zaremba, W. 2016. Openai\ngym. arXiv preprint arXiv:1606.01540.\n[8] Broughton, M.; Verdon, G.; McCourt, T.; Martinez, A. J.;\nYoo, J. H.; Isakov, S. V.; Massey, P.; Niu, M. Y.; Halavati,\nR.; Peters, E.; et al. 2020. Tensorﬂow quantum: A soft-\nware framework for quantum machine learning. arXiv\npreprint arXiv:2003.02989.\n[9] Cacciapuoti, A. S.; Calefﬁ, M.; Van Meter, R.; and\nHanzo, L. 2020. When entanglement meets classical\ncommunications: Quantum teleportation for the quantum\ninternet. IEEE Transactions on Communications.\n[10] Chen, S. Y.-C.; Yang, C.-H. H.; Qi, J.; Chen, P.-Y.; Ma,\nX.; and Goan, H.-S. 2020. Variational quantum circuits\nfor deep reinforcement learning. IEEE Access.\n[11] Cong, I.; Choi, S.; and Lukin, M. D. 2019. Quan-\ntum convolutional neural networks.\nNature Physics\n15(12):1273–1278.\n[12] Crooks, G. E. 2019. Gradients of parameterized quan-\ntum gates using the parameter-shift rule and gate decom-\nposition. arXiv preprint arXiv:1905.13311.\n[13] Fortunato, M.; Azar, M. G.; Piot, B.; Menick, J.; Hessel,\nM.; Osband, I.; Graves, A.; Mnih, V.; Munos, R.; Hassabis,\nD.; Pietquin, O.; Blundell, C.; and Legg, S. 2018. Noisy\nnetworks for exploration. In International Conference on\nLearning Representations.\n[14] Ganger, M., and Hu, W. 2019. Quantum multiple q-\nlearning. International Journal of Intelligence Science\n9(01):1.\n[15] Grover, L. K. 1996. A fast quantum mechanical algo-\nrithm for database search. In Proceedings of the twenty-\neighth annual ACM symposium on Theory of computing,\n212–219.\n[16] Hu, W., and Hu, J. 2019. Reinforcement learning\nwith deep quantum neural networks. Journal of Quantum\nInformation Science 9(1):1–14.\n[17] Khairy, S.; Shaydulin, R.; Cincio, L.; Alexeev, Y.; and\nBalaprakash, P. 2020. Learning to optimize variational\nquantum circuits to solve combinatorial problems. In\nAAAI, 2367–2375.\n[18] Kingma, D., and Ba, J. 2015. Adam: A method for\nstochastic optimization. In 3rd international conference\nfor learning representations, San Diego.\n[19] Le, P. Q.; Dong, F.; and Hirota, K. 2011. A ﬂexible rep-\nresentation of quantum images for polynomial preparation,\nimage compression, and processing operations. Quantum\nInformation Processing 10(1):63–84.\n[20] Long, G.-L., and Sun, Y. 2001. Efﬁcient scheme for ini-\ntializing a quantum register with an arbitrary superposed\nstate. Physical Review A 64(1):014303.\n[21] Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013.\nPlaying atari with deep reinforcement learning. arXiv\npreprint arXiv:1312.5602.\n[22] Nagendra, S.; Podila, N.; Ugarakhod, R.; and George,\nK. 2017. Comparison of reinforcement learning algo-\nrithms applied to the cart-pole problem. In 2017 Interna-\ntional Conference on Advances in Computing, Communi-\ncations and Informatics (ICACCI), 26–32. IEEE.\n[23] Preskill, J. 2018. Quantum computing in the nisq era\nand beyond. Quantum 2:79.\n[24] Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D.\n2016. Prioritized experience replay. In ICLR (Poster).\n[25] Schuld, M.; Bergholm, V.; Gogolin, C.; Izaac, J.; and\nKilloran, N. 2019. Evaluating analytic gradients on quan-\ntum hardware. Physical Review A 99(3):032331.\n[26] Shor, P. W. 1999. Polynomial-time algorithms for\nprime factorization and discrete logarithms on a quantum\ncomputer. SIAM review 41(2):303–332.\n[27] Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.;\nLai, M.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.;\nGraepel, T.; et al. 2018. A general reinforcement learning\nalgorithm that masters chess, shogi, and go through self-\nplay. Science 362(6419):1140–1144.\n[28] Van Hasselt, H.; Guez, A.; and Silver, D. 2016. Deep re-\ninforcement learning with double q-learning. In Thirtieth\nAAAI conference on artiﬁcial intelligence.\n[29] Vinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Math-\nieu, M.; Dudzik, A.; Chung, J.; Choi, D. H.; Powell, R.;\nEwalds, T.; Georgiev, P.; et al. 2019. Grandmaster level\nin starcraft ii using multi-agent reinforcement learning.\nNature 575(7782):350–354.\n[30] Wang, Z.; Schaul, T.; Hessel, M.; Hasselt, H.; Lanctot,\nM.; and Freitas, N. 2016. Dueling network architectures\nfor deep reinforcement learning. In International confer-\nence on machine learning, 1995–2003.\nREINFORCEMENT LEARNING WITH QUANTUM VARIATIONAL CIRCUITS\n8\n[31] Zoufal, C.; Lucchi, A.; and Woerner, S. 2019. Quantum\ngenerative adversarial networks for learning and loading\nrandom distributions. npj Quantum Information 5(1):1–9.\n",
  "categories": [
    "quant-ph",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-08-15",
  "updated": "2020-08-28"
}