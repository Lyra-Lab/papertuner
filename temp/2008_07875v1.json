{
  "id": "http://arxiv.org/abs/2008.07875v1",
  "title": "Towards Closing the Sim-to-Real Gap in Collaborative Multi-Robot Deep Reinforcement Learning",
  "authors": [
    "Wenshuai Zhao",
    "Jorge Peña Queralta",
    "Li Qingqing",
    "Tomi Westerlund"
  ],
  "abstract": "Current research directions in deep reinforcement learning include bridging\nthe simulation-reality gap, improving sample efficiency of experiences in\ndistributed multi-agent reinforcement learning, together with the development\nof robust methods against adversarial agents in distributed learning, among\nmany others. In this work, we are particularly interested in analyzing how\nmulti-agent reinforcement learning can bridge the gap to reality in distributed\nmulti-robot systems where the operation of the different robots is not\nnecessarily homogeneous. These variations can happen due to sensing mismatches,\ninherent errors in terms of calibration of the mechanical joints, or simple\ndifferences in accuracy. While our results are simulation-based, we introduce\nthe effect of sensing, calibration, and accuracy mismatches in distributed\nreinforcement learning with proximal policy optimization (PPO). We discuss on\nhow both the different types of perturbances and how the number of agents\nexperiencing those perturbances affect the collaborative learning effort. The\nsimulations are carried out using a Kuka arm model in the Bullet physics\nengine. This is, to the best of our knowledge, the first work exploring the\nlimitations of PPO in multi-robot systems when considering that different\nrobots might be exposed to different environments where their sensors or\nactuators have induced errors. With the conclusions of this work, we set the\ninitial point for future work on designing and developing methods to achieve\nrobust reinforcement learning on the presence of real-world perturbances that\nmight differ within a multi-robot system.",
  "text": "1\nTowards Closing the Sim-to-Real Gap in\nCollaborative Multi-Robot Deep Reinforcement Learning\nWenshuai Zhao1, Jorge Pe˜na Queralta1, Li Qingqing1, Tomi Westerlund1\n1 Turku Intelligent Embedded and Robotic Systems Lab, University of Turku, Finland\nEmails: 1{wezhao, jopequ, qingqli, tovewe}@utu.ﬁ\nAbstract—Current research directions in deep reinforcement\nlearning include bridging the simulation-reality gap, improv-\ning sample efﬁciency of experiences in distributed multi-agent\nreinforcement learning, together with the development of ro-\nbust methods against adversarial agents in distributed learning,\namong many others. In this work, we are particularly interested\nin analyzing how multi-agent reinforcement learning can bridge\nthe gap to reality in distributed multi-robot systems where the\noperation of the different robots is not necessarily homogeneous.\nThese variations can happen due to sensing mismatches, inherent\nerrors in terms of calibration of the mechanical joints, or simple\ndifferences in accuracy. While our results are simulation-based,\nwe introduce the effect of sensing, calibration, and accuracy\nmismatches in distributed reinforcement learning with proximal\npolicy optimization (PPO). We discuss on how both the different\ntypes of perturbances and how the number of agents experiencing\nthose perturbances affect the collaborative learning effort. The\nsimulations are carried out using a Kuka arm model in the\nBullet physics engine. This is, to the best of our knowledge,\nthe ﬁrst work exploring the limitations of PPO in multi-robot\nsystems when considering that different robots might be exposed\nto different environments where their sensors or actuators have\ninduced errors. With the conclusions of this work, we set the\ninitial point for future work on designing and developing methods\nto achieve robust reinforcement learning on the presence of real-\nworld perturbances that might differ within a multi-robot system.\nIndex Terms—Reinforcement Learning; Multi-Robot Systems;\nCollaborative Learning; Deep RL; Adversarial RL; Sim-to-Real;\nI. INTRODUCTION\nReinforcement learning (RL) algorithms for robotics and\ncyber-physical systems have seen an increasing adoption\nacross multiple domains over the past decade [1], [2]. Deep\nreinforcement learning (DRL) enables agents to be trained in\nrealistic environments without the need for large amounts of\ndata to be gathered and labeled a priori. Speciﬁcally, rein-\nforcement learning has enjoyed signiﬁcant success in robotic\ncontrol tasks involving manipulation [3], [4], [5]. Motivated\nby the way humans and animals learn, DRL algorithms work\non a trial and error basis, where an agent interacts with its\nenvironment and receives a reward based on its performance.\nWhen complex agents or environments are involved, the\nlearning process can be relatively slow. This has motivated\nthe design and development of multi-agent DRL algorithms.\nIn this paper, we are interested in exploring some of the\nchallenges remaining in multi-robot collaborative DRL.\nReinforcement learning applied to multi-agent systems has\ntwo dimensions: DRL algorithms that model policies for multi-\nagent control and interaction, and DRL approaches that rely on\nExperience Aggregation\nExperience Aggregation\nExperience Aggregation\nExperiences \nand \nRewards\nUpdated\nCommon \nModel\nPolicy \nUpdates\nPolicy \nUpdates\nPolicy Updates\nPPO2\nFig. 1: Conceptual view of the proposed scenario, where multiple\nrobotic agents are collaboratively learning the same task. While the\ntask is common, and the agents are a priori identical, we study how\ndifferent alterations in the agents or their environments affects the\nperformance of the collaborative learning process.\nmultiple agents to parallelize the learning process or explore a\nwider variety of experiences. Within the former category, we\ncan ﬁnd examples of DRL for formation control [6], obstacle\nand collision avoidance [7], [8], collaborative assembly [9], or\ncooperative multi-agent control in general [10]. In the latter\ncategory, most existing approaches refer to the utilization of\nmultiple agents to learn in parallel, but from the point of\nview of a multi-process or multi-threaded application [3]. We\nare interested in works exploring the possibilities of using\nmultiple robotic agents that collaborate on learning the same\ntask. This has been identiﬁed as one of the future paradigms\nwith 5G-and-beyond connectivity and edge computing [11],\n[12]. For instance, in [13] an asynchronous method for off-\npolicy updates between robots was presented. Other works\nalso consider network conditions and propose frameworks\nfor multi-agent collaborative DRL over imperfect network\nchannels [14]. This type of scenario is illustrated in Fig. 1,\nwhere three robotic arms are collaboratively learning the same\ntask and sharing their experiences to update a common policy.\nHereinafter, we refer to these types of scenarios as multi-agent\narXiv:2008.07875v1  [cs.LG]  18 Aug 2020\n2\nor multi-robot collaborative RL tasks, where multiple agents\ncollaborate to learn the same task but might be exposed to\ndifferent environments, or work under different conditions.\nAmong the multiple challenges in DRL, recent years have\nseen a growing research interest in closing the simulation-\nto-reality gap [5], [15], and on the design and development\nof robust algorithms with resilience against adversarial con-\nditions [16], [17], [18]. This latter topic is also of paramount\nrelevance in distributed or multi-agent DRL, where adversarial\nagents can hinder the collaborative learning process [19].\nWhen multiple agents are learning a collaborative or coordi-\nnated behavior, byzantine agents can signiﬁcantly reduce the\nperformance of the system as a whole.\nWe aim at studying how adversarial conditions can help\nto bridge the simulation-to-reality gap. In [5] and [15], the\nauthors analyze perturbances in the rewards towards the\napplicability of DRL in real-world applications. In [5], the\nfocus is on learning how to manipulate deformable objects,\nwith agents trained in a simulation environment but directly\ndeployable in the real-world. In [20], the authors present a\nmeta-learning approach for domain adaption in simulation-to-\nreality transfers. Our objective in this paper is not to design\na speciﬁc sim-to-real method for a given algorithm or task,\nbut instead to analyze the performance of collaborative multi-\nrobot DRL in the presence of disturbances in the environment\nas a step towards more effective sim-to-real transfers where\nreal noises, errors or perturbances are accounted for also\nin the simulation environment. This includes variability in\nthe operation of the robots, as robots might be operating in\nslightly different environments, or operate in different ways\nunder the same environment. In particular, we are interested\nin studying how exposing multiple collaborative robots to\ndifferent environments from the point of view of sensing and\nactuation can affect the joint learning effort.\nIn this paper, therefore, we focus on introducing pertur-\nbances inspired by real-world cases in a multi-agent DRL\nsimulation. We expose different subsets of agents to slightly\nmodiﬁed environments and study how different types of dis-\nturbances affect the collaborative learning process and the\nability of the multi-robot system to converge to a common\npolicy. The main contribution of this paper is the analysis of\nhow input and output disturbances affect a collaborative deep\nreinforcement learning process with multiple robot arms. In\nparticular, we simulate real-world perturbations that can occur\non robotic arms, from the sensing and actuation perspectives.\nThis is, to the best of our knowledge, the ﬁrst study to consider\nthe evaluation of both sensing and actuation disturbances in\na multi-robot collaborative learning scenario, with different\nrobots being exposed to different environments.\nThe remainder of this document is organized as follows.\nIn Section II we review the literature in distributed RL,\nadversarial RL, and robust multi-agent RL in the presence\nof byzantine agents. Then, Section III introduces the DRL\nalgorithm, and the methodology and simulation environment\nutilized in our experiments. The agent training methods and\nenvironment disturbances introduced to emulate real-world\noperational variability, together with the simulations results,\nare presented in Section IV. Section V concludes the work.\nII. RELATED WORKS\nIn this work, we study adversarial conditions in a simulation\nenvironment to emulate real-world conditions in terms of\nvariability of the environment across a set of multiple agents\ncollaborating in learning the same task. With most of the\nliterature in simulation-to-reality transfer aiming at speciﬁc\napplications or adaptation to different environments [15], [5],\n[20], in this section we focus instead on previous works\nanalyzing the effect of adversarial of byzantine effects in\nmulti-agent reinforcement learning, as well as considering\nother perturbations in the environment to better emulate real-\nworld conditions. The literature in adversarial conditions for\ncollaborative multi-agent learning is, nonetheless, sparse.\nAdversarial RL has been a topic of extensive study over\nthe past years. Multiple deep learning algorithms have been\nshown to be vulnerable when subject to adversarial input\nperturbations, being able to induce certain policies [16]. This\nis a general problem of reinforcement learning that affects\ndifferent types of algorithms and scenarios. In multi-agent\nenvironments, the ability of an attacker to create adversarial\nobservations increases signiﬁcantly [17]. A comprehensive\nsurvey on the main challenges and potential solutions for ad-\nversarial attacks on DRL is available in [21]. The authors clas-\nsify attacks in four categories: attacks targeting (i) rewards, (ii)\npolicies, (iii) observations, and (iv) the environment. Among\nthese, those targeting observations and the environment are\nthe most relevant within the scope of this survey. In most of\nthese cases, however, the literature only considers single-agent\nlearning (or multiple agents being affected in the same way).\nMoreover, previous works focus on malicious perturbations\naimed at decreasing the performance of the learning agent.\nIn this paper, nonetheless, we induce perturbations that are\ninspired by real-world issues including changes in accuracy\nor calibration errors.\nOther authors have explored the effects of having noisy\nrewards in RL. In this direction, Wang et al. presented an\nanalysis of perturbed rewards for different RL algorithms,\nincluding PPO but also DQN and DDPG, among others [18].\nCompared to their approach, we also consider perturbances\non the RL process but focus on those that model real-world\nnoises and errors. Moreover, we speciﬁcally put an emphasis\non multi-robot collaborative learning, and consider situations\nin which the perturbances that affect different robots are also\ndifferent. We also focus on the PPO algorithm as the state-\nof-the-art in three-dimensional locomotion. In fact, PPO has\nbeen identiﬁed as one of the most robust approaches against\nreward perturbances in [18]. Also within the study of noisy\nrewards, a method to improve performance in such scenarios\nis proposed in [22].\nIn general, we see a gap in the literature in the study of\nnoisy or perturbed environments that do not affect equally\nacross multiple agents collaborating towards learning the same\ntask. This paper thus tries to address this issue with an\ninitial assessment of how perturbations in the environment\ninﬂuencing a subset of agents affect a global common model\nwhere experiences from different agents are aggregated.\n3\nIII. METHODOLOGY\nIn this section, we deﬁne our problem of distributed rein-\nforcement learning with a subset of perturbed agents, as well\nas the simulation environment and modiﬁcations applied to it.\nA. Multi-agent RL\nIn multi-agent reinforcement learning, approaches can be\nroughly divided into two parallel modes, asynchronous and\nsynchronous. A3C (Asynchronous Advantage Actor-Critic)[3]\nis one of the most widely adopted methods for multi-agent re-\ninforcement learning, representing the asynchronous paradigm.\nA3C consists of multiple independent agents with their own\nnetworks. These agents interact with different copies of the en-\nvironment in parallel and update a global network periodically\nand asynchronously.After each update, the agents reset their\nown weights to those of the global network and then resume\ntheir independent exploration.Because some of the agents will\nbe exploring the environment with an older version of the\nnetwork weights, A3C results in relatively suboptimal use of\ncomputational resources as well as more noisy updates. An\nalternative is A2C (Advantage Actor-Critic), which utilizes\nsynchronous parallel mode. In this case, there are only two\nnetworks in the system. One is used by all agents equally to\ninteract with the environment in parallel, and outputs a batch\nof experiences. With this data, the second network is trained\nand updates the former network.\nIn this paper, we utilize a synchronous multi-agent rein-\nforcement learning algorithm: proximal policy optimization\n(PPO). PPO and has been adopted as the default method\nof OpenAI owing to its excellent performance. The PPO\nalgorithm takes advantage of the A2C ideas in terms of having\nmultiple workers, and gradient policy ideas from TRPO (Trust\nRegion Policy Optimization) to improve the actor performance\nby utilizing a trust region. PPO seeks to ﬁnd a balance between\nthe ease of implementation, sample complexity, and ease of\nadjustment, trying to update at each step to minimize the cost\nfunction while assuring that the new policies are not far from\nlast policies. The scheme follows these steps:\n1) Set the initial policy parameters θ0.\n2) In each iteration, use θk to interact with the environment,\ncollect experience data (a tuple of state and action {st, at}),\nand compute their advantage Aθk(st, at) [3].\n3) Find the optimal θ by optimizing JP P O(θ):\nJθk\nP P O(θ) = Jθk(θ) −βKL\n\u0000θ, θk\u0001\n(1)\nwhere β is a hyperparameter and will be adapted according to\nthe value of KL. Jθk(θ) is calculated by:\nJθk(θ) ≈\nX\n(st,at)\npθ(at|st)\npθk(at|st)Aθk (st, at)\n(2)\nwhere pθk (at|st) is the probability of (st, at) under θk.\nB. Simulation Environment\nOur simulation environment is built based on top one of\nthe Bullet physics simulators, speciﬁcally the PyBullet Kuka\narm for grasping [23]. In order to simplify the training of\nFig. 2: Kuka arm reaching environment based on Bullet simulator.\nour RL algorithm, we modify the original grasping task into a\nreaching task, which allows us to focus on observing the effect\nof adversarial agents in training distributed reinforcement\nlearning algorithms, rather on optimizing the training itself.\nThe simulation environment is shown in Figure 2. The\nrobotic arm in this environment attempts to reach the object\nin the bin. It takes the Cartesian coordinates of the gripper\nand the relative position of the object as input instead of the\non-shoulder camera observation. This input can be seen as a\nlist with nine elements:\nInput = [xg, yg, zg, yawg, pitg, rolg, xog, yog, rolog]\n(3)\nwhere xg, yg, zg denote the Cartesian coordinates of the center\nof the gripper, and yawg, pitg, rolg refers to its three Euler\nangles, while xog, yog, rolog indicate the relative x, y position\nand the roll angle of the object in the gripper space.\nOur RL algorithm receives the input and then gives a\nCartesian displacement:\nOutput = [dx, dy, dz, dφ]\n(4)\nin which φ is the rotation angle of the wrist around the z-axis.\nAn inverse kinematics method is then employed to calculate\nthe real motor control values of the joints. Note that all the\nunits used for the position are in meters, and the angles are\nin radians. This environment with our training code is now\nopen-source on Github1.\nC. Collaborative Learning under Real-World Perturbations\nIn real robots, some of the most characteristic sources of\nperturbations within a homogeneous multi-robot team come\nfrom the calibration of the robots in terms of sensing and\nactuation. In this paper, we thus study how these two types\nof input (sensing) and output (actuation) perturbations affect\na collaborative learning process:\nInput perturbations: we consider both ﬁxed and variable\nerrors in the input to the network regarding the position of the\nobject to be reached. This emulates the error that might result\nfrom identifying the position of the object from a camera or\nanother sensor on the robot arm. The ﬁxed noise represents, for\ninstance, installation or calibration errors on the position of the\ncamera, which might have an offset in position or orientation.\n1https://github.com/TIERS/NoisyKukaReacher\n4\nVariable errors, on the other hand, try to emulate the sensing\nerrors that come, for example, from the vibration of the arm or\nlocal odometry errors describing its orientation and position.\nOutput perturbations: we simulate both ﬁxed and variable\nperturbations in the actuation of the robotic arm, emulating\ncalibration errors (e.g., a constant offset in one direction), or\nchanges in accuracy or repeatability across different robots.\nThrough multiple simulations, we study how these types of\nperturbations affect the collaborative learning effort when they\nare not common across the entire set of agents.\nIV. EXPERIMENTATION AND RESULTS\nIn this section, we describe the training parameters uti-\nlized through our simulations, and the ways in which the\nenvironments have been modiﬁed to introduce disturbances\nin both sensors and actuators. We then present the results of\nmultiple simulations where different numbers of agents have\nbeen trained in different environments but treated equally from\nthe point of view of the collaborative learning process.\nA. Training Method\nThe maximal number of steps in one episode is set to 1200,\nand the maximum number of steps for the whole training\nprocess is set to 4 million. If the gripper contacts the object or\napproaches it at a very small distance (0.008 m), the episode\nwill be terminated. The ﬁnal score for this episode is thus\ncalculated by summing all the rewards obtained in all the steps\nuntil termination.\nThe initial reward is set as -1000 for each step if the distance\nis larger than 1 m. However, if the distance between the ﬁnger\non the gripper and the object is smaller than 1 m, the reward is\ncomputed as rewardraw = −10·distance. Moreover, we also\nadd the cost of each step, in order to encourage the gripper to\napproach the target as soon as possible. The cost of each step\nis set as 1. Therefore, the ﬁnal reward for this step is hence:\nrewardfinal = −10·distance−1, where the distance is given\nin meters. If the gripper ﬁnally contacts its target or approach\nit in a threshold, we give it a signiﬁcantly larger reward (1000)\nto help the model learn faster and clearly.\nIn total, in our simulations, we utilize 30 agents parallelized\non the GPU processes to produce experience data based on a\nvectorized environment. Therefore, these agents can represent\na multi-robot system learning a collaborative RL task. We\ngive different settings on individual environments to manually\nsimulate the possible perturbations that robots ﬁnd in real-\nworld scenarios.\nB. Calibration and Accuracy Noises\nTo emulate the practical noises and errors that robots could\nencounter when training an RL algorithm, we consider the\nfollowing four types of perturbations, for each of which we\ngenerate a different environment to expose a variable number\nof robots to: ﬁxed input errors on all the nine elements by\n0.005 m, uniformly distributed sensing errors in the interval\n[0.005 m, 0.01 m], ﬁxed output errors modifying the gripper\nactuators by an offset of 0.005 m on the x axis, and uniformly\ndistributed output errors in the interval [0.005 m, 0.01 m]. It\nshould be noted that the uniform distributed errors on input and\noutput could be different in each step, which can be regarded\nas inaccurate sensing errors, or reduced repeatability in the\nactuation of real robots.\nMoreover, in order to further analyze how more extreme\ncases affect the collaborative learning process, we also con-\nsider ﬁxed disturbances on larger magnitude (0.015 m on all\nthe values for the sensing error and 0.015 m on the x-axis\nfor the actuation error) as well as scenarios where the noise\nis different for each of the agents exposed to the modiﬁed\nenvironment (in the interval 0.005 m to 0.025 m for 25 agents).\nC. Simulation Results\nFigure 3 shows the results of our simulations. The notation\ndescribing each subﬁgure is as follows: {I,O}: representing\nthe input (sensing) and output (actuation) perturbances, {F,V}:\nrepresenting ﬁxed and variable perturbances, and {5, 15, 25}:\nrepresenting the number of agents exposed to the modiﬁed\nenvironment where the perturbances occur.\nComparing perturbations in the sensors versus perturbations\nin the actuators, we see an overall more robust performance\nagainst adversarial elements in the sensing part. In Figures 3b\nto 3d, we see that the network always converges and we only\nsee more unstable behaviour when there is a large fraction\nof agents suffering of variable sensing errors (50% and 83%).\nWhen we compare the effect of constant or ﬁxed perturbations\nagainst variable ones, we notice that variable perturbations\ninduce less stable convergence. This can be to some extent\nexplained by the fact that there are no large subsets of agents\nbeing exposed to a common environment.\nFor small ﬁxed perturbations affecting actuation (output dis-\nturbances), we have seen that the agents are able to converge\ntowards a working policy. In the cases where 5 or 25 of the\nagents are affected, this was expected as there is a majority\n(25) of agents, in both cases, that work in exactly the same\nway, and a small subset (5) that work in a slightly different\nway (but still the same within that subset). When this ﬁxed\nperturbation is introduced to half of the agents, then we have\ntwo subsets of the same size operating in different ways, but\nagain consistently across each of the subsets. In this case, we\nhave seen that for a small magnitude in the perturbation, the\nagents still converge on a policy that works for both subsets.\nAs the difference between the operation of the agents in these\ntwo subsets diverges, the performance of the system as a whole\ndrops signiﬁcantly. Nonetheless, we have observed that the\ncase were half of the agents have a common ﬁxed perturbation\nof small magnitude the system is able to converge even when\nthe initial conditions are disadvantageous.\nIn order to analyze the effect of perturbations with larger\nmagnitude as well as ﬁxed perturbations in both sensing and\nactuation that vary across the robots exposed to a modiﬁed\nenvironment, we have analyzed four more cases shown in\nFig. 4. In Figures 4a and 4b, we analyze how perturbations\nwith larger magnitude affect the learning process, with half of\nthe agents being affected as the worst-case scenario. We see\nthat the trend from the previous results is followed, with the\n5\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\nTraining\nEvaluation\n(a) Base scenario with common perturbation-free environments.\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(b) IF5\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(c) IF15\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(d) IF25\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(e) IV5\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(f) IV15\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(g) IV25\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(h) OF5\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(i) OF15\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(j) OF25\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(k) OV5\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(l) OV15\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\n(m) OV25\nFig. 3: Simulation results where we show the training in perturbance-free environment, and 12 cases where we analyze the effect of a\nmodiﬁed environment (ﬁxed and variable perturbances on both sensing and actuation) on 5, 15 and 25 agents. The total number of agents\nis 30 in all cases. The legend is common across all graphs and has been omitted in subﬁgures (b) through (m) to improve readability.\nnetwork being able to converge to a common policy when a\nconstant error is added to the sensors interface, but not when\nthe disturbances affect to the actuators. Finally, Figures 4c\nand 4d show that when there are no differentiated subsets of\nagents with a common behaviour and the perturbations are\ndifferent across a large number of agents, then the system is\nnot able to converge.\nV. CONCLUSION AND FUTURE WORK\nAdversarial agents and closing the simulation-to-reality gap\nare among the key challenges preventing wider adoption\nof reinforcement learning in real-world applications. In this\npaper, we have addressed the latter one from the perspective of\nthe former: by introducing adversarial conditions inspired by\nreal-world perturbances to a subset of agents in a multi-robot\nsystem during a collaborative reinforcement learning process,\nwe have been able to identify points where the robustness of\ndistributed multi-agent DRL algorithms needs to be improved.\nIn this paper, we have considered multiple robotic arms in\na simulation environment collaborating towards learning a\ncommon policy to reach an object. In order to emulate more\nrealistic conditions and understand how perturbances in the\nenvironment affect the learning process, we have considered\nvariability across the agents in terms of their ability to sense\nand actuate accurately. We have shown how different types of\ndisturbances in the model’s input (sensing) and output (actua-\ntion) affect the robustness and ability to converge towards an\neffective policy. We have seen how variable perturbances have\nthe most effect on the ability of the network to converge, while\ndisturbances in the ability of the robots to actuate properly\nhave had a comparatively worse effect than those in their\nability to sense the position of the object accurately.\nThe conclusions of this work serve as a starting point\ntowards the design and development of more robust methods\n6\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\nTraining\nEvaluation\n(a) IF15 - Large perturbations\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\nTraining\nEvaluation\n(b) OF15 - Large perturbations\n0\n1\n2\n3\n4\n-8\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\nTraining\nEvaluation\n(c) OF25 - Each agent exposed to a different environment\n0\n1\n2\n3\n4\n-6\n-4\n-2\nSimulation steps (millions)\nMean score (×10−4)\nTraining\nEvaluation\n(d) IF25 - Each agent exposed to a different environment\nFig. 4: Simulation results with an extra 4 scenarios analyzed: two of them where we consider perturbations of larger magnitude, and two\nmore where we consider that each of the agents in a modiﬁed environment is affected differently.\nable to identify and take into account these disturbances in\nthe environment that do not occur across all robots equally.\nThis will be the subject of our future work, as well as\nthe study of other types or combinations of disturbances in\nthe environment. We will also work towards modeling more\naccurately real-world errors for RL simulation environments.\nACKNOWLEDGEMENTS\nThis work was supported by the Academy of Finland’s\nAutoSOS project with grant number 328755.\nREFERENCES\n[1] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil An-\nthony Bharath. A brief survey of deep reinforcement learning. arXiv\npreprint arXiv:1708.05866, 2017.\n[2] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep\nreinforcement learning for multiagent systems: A review of challenges,\nsolutions, and applications. IEEE transactions on cybernetics, 2020.\n[3] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex\nGraves, Timothy Lillicrap, Tim Harley, David Silver, and Koray\nKavukcuoglu. Asynchronous methods for deep reinforcement learning.\nIn International conference on machine learning, 2016.\n[4] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani,\nJohn Schulman, Emanuel Todorov, and Sergey Levine.\nLearning\ncomplex dexterous manipulation with deep reinforcement learning and\ndemonstrations. arXiv preprint arXiv:1709.10087, 2017.\n[5] Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real rein-\nforcement learning for deformable object manipulation. arXiv preprint\narXiv:1806.07851, 2018.\n[6] Ronny Conde, Jos´e Ram´on Llata, and Carlos Torre-Ferrero.\nTime-\nvarying formation controllers for unmanned aerial vehicles using deep\nreinforcement learning. arXiv preprint arXiv:1706.01384, 2017.\n[7] Yu Fan Chen, Miao Liu, Michael Everett, and Jonathan P How. Decen-\ntralized non-communicating multiagent collision avoidance with deep\nreinforcement learning. In ICRA, pages 285–292. IEEE, 2017.\n[8] Pinxin Long, Tingxiang Fanl, Xinyi Liao, Wenxi Liu, Hao Zhang, and\nJia Pan. Towards optimally decentralized multi-robot collision avoidance\nvia deep reinforcement learning. In ICRA. IEEE, 2018.\n[9] Dorothea Schwung, Fabian Csaplar, Andreas Schwung, and Steven X\nDing. An application of reinforcement learning algorithms to industrial\nmulti-robot stations for cooperative handling operation. In 15th INDIN,\npages 194–199. IEEE, 2017.\n[10] Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative\nmulti-agent control using deep reinforcement learning. In AAMAS, pages\n66–83. Springer, 2017.\n[11] Jorge Pe˜na Queralta, Li Qingqing, Zhuo Zou, and Tomi Westerlund.\nEnhancing autonomy with blockchain and multi-acess edge computing\nin distributed robotic systems. In The Fifth International Conference on\nFog and Mobile Edge Computing (FMEC). IEEE, 2020.\n[12] Jorge Pe˜na Queralta and Tomi Westerlund. Blockchain-powered collab-\noration in heterogeneous swarms of robots. Frontiers in Robotics and\nAI, 2020.\n[13] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep\nreinforcement learning for robotic manipulation with asynchronous off-\npolicy updates. In ICRA, pages 3389–3396. IEEE, 2017.\n[14] Yiding Yu, Soung Chang Liew, and Taotao Wang. Multi-agent deep rein-\nforcement learning multiple access for heterogeneous wireless networks\nwith imperfect channels. arXiv preprint arXiv:2003.11210, 2020.\n[15] Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac,\nVineet Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend, et al.\nDeepracer: Educational autonomous racing platform for experimentation\nwith sim2real reinforcement learning. arXiv:1911.01562, 2019.\n[16] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement\nlearning to policy induction attacks. In MLDM. Springer, 2017.\n[17] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine,\nand Stuart Russell. Adversarial policies: Attacking deep reinforcement\nlearning. arXiv preprint arXiv:1905.10615, 2019.\n[18] Jingkang Wang, Yang Liu, and Bo Li.\nReinforcement learning with\nperturbed rewards. In AAAI, pages 6202–6209, 2020.\n[19] Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. Multi-\nagent generative adversarial imitation learning. In Advances in neural\ninformation processing systems, pages 7461–7472, 2018.\n[20] Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. Meta\nreinforcement learning for sim-to-real domain adaptation. arXiv preprint\narXiv:1909.12906, 2019.\n[21] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Jan-\njua, Ala Al-Fuqaha, Dinh Thai Hoang, and Dusit Niyato. Challenges and\ncountermeasures for adversarial attacks on deep reinforcement learning.\narXiv preprint arXiv:2001.09684, 2020.\n[22] Aashish Kumar et al. Enhancing performance of reinforcement learning\nmodels in the presence of noisy rewards. PhD thesis, 2019.\n7\n[23] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics\nsimulation for games, robotics and machine learning. 2016.\n",
  "categories": [
    "cs.LG",
    "cs.DC",
    "stat.ML"
  ],
  "published": "2020-08-18",
  "updated": "2020-08-18"
}