{
  "id": "http://arxiv.org/abs/2008.09450v1",
  "title": "Adversarial Imitation Learning via Random Search",
  "authors": [
    "MyungJae Shin",
    "Joongheon Kim"
  ],
  "abstract": "Developing agents that can perform challenging complex tasks is the goal of\nreinforcement learning. The model-free reinforcement learning has been\nconsidered as a feasible solution. However, the state of the art research has\nbeen to develop increasingly complicated techniques. This increasing complexity\nmakes the reconstruction difficult. Furthermore, the problem of reward\ndependency is still exists. As a result, research on imitation learning, which\nlearns policy from a demonstration of experts, has begun to attract attention.\nImitation learning directly learns policy based on data on the behavior of the\nexperts without the explicit reward signal provided by the environment.\nHowever, imitation learning tries to optimize policies based on deep\nreinforcement learning such as trust region policy optimization. As a result,\ndeep reinforcement learning based imitation learning also poses a crisis of\nreproducibility. The issue of complex model-free model has received\nconsiderable critical attention. A derivative-free optimization based\nreinforcement learning and the simplification on policies obtain competitive\nperformance on the dynamic complex tasks. The simplified policies and\nderivative free methods make algorithm be simple. The reconfiguration of\nresearch demo becomes easy. In this paper, we propose an imitation learning\nmethod that takes advantage of the derivative-free optimization with simple\nlinear policies. The proposed method performs simple random search in the\nparameter space of policies and shows computational efficiency. Experiments in\nthis paper show that the proposed model, without a direct reward signal from\nthe environment, obtains competitive performance on the MuJoCo locomotion\ntasks.",
  "text": "Adversarial Imitation Learning via Random Search\nMyungJae Shin\nSchool of Computer Science and Engineering\nChung-Ang University\nSeoul, Korea\nmjshin.cau@gmail.com\nJoongheon Kim\nSchool of Computer Science and Engineering\nChung-Ang University\nSeoul, Korea\njoongheon@gmail.com\nAbstract—Developing agents that can perform challenging\ncomplex tasks is the goal of reinforcement learning. The model-\nfree reinforcement learning has been considered as a feasible\nsolution. However, the state of the art research has been to\ndevelop increasingly complicated techniques. This increasing\ncomplexity makes the reconstruction difﬁcult. Furthermore, the\nproblem of reward dependency is still exists. As a result, research\non imitation learning, which learns policy from a demonstration\nof experts, has begun to attract attention. Imitation learning\ndirectly learns policy based on data on the behavior of the experts\nwithout the explicit reward signal provided by the environment.\nHowever, imitation learning tries to optimize policies based\non deep reinforcement learning such as trust region policy\noptimization. As a result, deep reinforcement learning based\nimitation learning also poses a crisis of reproducibility. The issue\nof complex model-free model has received considerable critical\nattention. A derivative-free optimization based reinforcement\nlearning and the simpliﬁcation on policies obtain competitive\nperformance on the dynamic complex tasks. The simpliﬁed\npolicies and derivative free methods make algorithm be simple.\nThe reconﬁguration of research demo becomes easy. In this paper,\nwe propose an imitation learning method that takes advantage\nof the derivative-free optimization with simple linear policies.\nThe proposed method performs simple random search in the\nparameter space of policies and shows computational efﬁciency.\nExperiments in this paper show that the proposed model, without\na direct reward signal from the environment, obtains competitive\nperformance on the MuJoCo locomotion tasks.\nI. INTRODUCTION\nIn 2013, the Deep Q-Learning showed how to combine clas-\nsical Q-Learning with convolution neural network to success-\nfully solve Atari games, reinvigorating reinforcement learning\n(RL) as one of the most remarkable research ﬁelds [1]–[5]. As\na result, much attention has been drawn to deep reinforcement\nlearning (DRL) that approximates value function through\nneural network. DRL has been studied as a feasible solution\nfor controlling dynamic tasks (i.e., autonomous driving, hu-\nmanoid robot, etc) without requiring models of the system\ndynamics [6]–[10].\nHowever, the main challenge faced by many researchers\nis the model-free RL requires too much data to achieve\nreasonable performance [3], [11]. To address this issue, the\nmodels become complicated; and the models lead to repro-\nducibility crisis. Furthermore, the models are sensitive to the\nimplementation structure of the same algorithm and rewards\nfrom environments. As a result, the reconstruction results do\nnot show reasonable performance, and stuck in sub-optimal.\nTherefore, the models have not yet been successfully deployed\nto control systems [12], [13].\nThe rewards sensitivity makes the optimization of model-\nfree RL to be difﬁcult. The reward signals are information\nabout how to improve the inner neural network to better\ncontrol. The optimization of the inner network depends on\nthe reward signals in determining whether to propagate the\neffects of network weights to optimization. In many dynamic\ntasks, the reward signals are extremely sparse or none at all.\nAs a result, when the models are stuck in sub-optimal, the\nproblems can not be handled appropriately.\nThe reward shaping which makes the signals to be more\ndense to lead to the reasonable performance in dynamic\nsystems has been studied. Several attempts have been made\nto manually design reward function by hand. However, it\nis difﬁcult to conﬁgure an appropriate reward function by\nhand. Therefore, imitation learning is proposed. Imitation\nlearning trains the models based on the desired behavior\ndemonstrations rather than conﬁguring the reward function\nthat would generate such behavior. Imitation learning shows\nimpressive performance when there is sub-optimal problems\narising from problems such as sparse reward. Imitation learn-\ning has performed remarkably well in areas such as robotics\nand autonomous navigation [14]–[16]. In imitation learning,\nsupervision through a set of expert demonstrations is to\nbe a guideline which learner can query when the models\nare trained. The simplest method of imitation learning is\nbehavioral cloning (BC). It works by collecting training data\nfrom the expert demonstrations, and then uses it to directly\nlearn a policy. BC shows high performance when we have\nabundant expert demonstrations, but agents tend to be fragile\nif the agents deviate from trajectories which trained in training\nprocedure. This is because supervised learning method tries to\nreduce the 1-step deviation error of training data, not to reduce\nthe error of entire trajectories. Recently, as the method that\nmakes the distribution of state-action trajectories of the agents\nto be matched the distribution of expert trajectories of the\nexperts, a model-free imitation learning called GAIL (Gen-\nerative Adversarial Imitation Learning) is proposed [17]. In\nGAIL, the discriminator of Generative Adversarial Networks\n(GAN) takes a role of the reward function. The reward signals\nfrom the discriminator means the probability that how much\nthe learner’s trajectories is similar to the trajectories of expert.\nBy using this reward, GAIL train the policy of agent based on\narXiv:2008.09450v1  [cs.LG]  21 Aug 2020\ntrust region policy optimization (TRPO). Through GAIL, the\nagent learns the policy which achieves the expected rewards of\nexpert trajectories in dynamic continuous tasks and sometimes\nbetter than the experts, because deep reinforcement learning\nis executed in inner loop and its not constrained to always be\nclose to the expert [17]. GAIL requires a lot of interaction\nwith the environment to get reasonable performance in the\ntraining procedure. Therefore, to stabilize the training, the\ncomplex DRL algorithms (i.e., TRPO and proximal policy\noptimization ) are used. As a result, GAIL also poses a\ncrisis of reproducibility. To address the crisis, the simplest\nmodel-free RL has been studied. Recently, dierent directions\nhave been proposed. Firstly, evolution strategies (ES) shows\na powerful derivative-free policy optimization method [18].\nSecondly, the simple linear policy based on the natural gradient\npolicy algorithm shows the competitive performance on the\nMuJoCo locomotion tasks [19]. Augmented random search\n(ARS) is proposed as a result of integrating these concepts.\nIn ARS, the policy is trained through random searches in the\nparameter space. ARS is a derivative-free simple linear policy\noptimization method [20]. The speciﬁc objective of this study\nis to propose highly reproducible imitation learning method. In\nthis work, we combine ideas from the work of [20] and [17].\nThe simple linear policies are used. By using the derivative-\nfree random search, the trained policies show stabilized rea-\nsonable performance. Furthermore, the discriminator is used to\nreplace the reward function of environments. The trained agent\nachieves the expected rewards of expert trajectories. Through\nthe experiments, we demonstrate that a simple random search\nbased imitation learning method can train linear policy efﬁ-\nciently on MuJoCo locomotion benchmarks. For more details,\nour contributions are as follows:\n1) The performance of our method on the benchmark\nMuJoCo locomotion tasks. Our method can successfully\nimitate expert demonstration; and static and linear poli-\ncies can achieve high rewards on all MuJoCo task.\n2) Since previous imitation learning methods is based on\nRL methods which has complicate conﬁguration to han-\ndle the complex tasks, it difﬁcult to choose what is the\nbest method for a speciﬁc task as well as to reconstruct\nthe result. Howver, our method is based on the derivate-\nfree simple random search algorithm with simple linear\npolicies; and thus it can solve a reproducibility crisis.\n3) Within the knowledge we know, the combination of\nadversarial network and simple random search is the\nstate-of-the-art; thus the proposed method will be a new\nguideline of imitation learning in the future.\nSec. II describes the background knowledge and Sec. III\nshows how to design the proposed algorithm. Sec. IV shows\nthe experiment results of the proposed imitation learning on\nexpert demonstration in the MuJoCo locomotion environment.\nSec. V concludes this paper and presents future work.\nII. BACKGROUND\nPreliminaries. A Markov Decision Process (MDP) is deﬁned\nas M = {S, A, T, r}, where S denotes the state space, A\ndenotes the set of possible actions, T denotes the transition\nmodel and r denotes the reward structure. Throughout the\npaper, we consider a ﬁnite state space S ∈Rn and a ﬁnite\naction space A ∈Rp. The goal of imitation learning is\nto train a policy πθ ∈Π : S × A →[0, 1]p which can\nimitate expert demonstration using the idea from generative\nadversarial network (GAN) Dφ(s, a) →[0, 1] where θ ∈Rn\nare the policy parameters and φ ∈Rn+p are the discriminator\nparameters [17].\nExpert demonstrations TE = {τ1, τ2, ..., τN} is available.\nEach demonstration τi is consist of a set of action state-action\npairs such that τi = {(s0, a0), (s1, a1), . . . , (sT , aT )} where\nN is the number of demonstration set and T is the length of\nepisode.\nBehavior Cloning (BC). Behavioral cloning learns a policy\nas a way of supervised learning over state-action pairs from\nexpert demonstration. Distribution of states which is visited\nby expert is deﬁned as PE = P(s|πE). The objective of BC\nis deﬁned as:\nargmin\nθ\nEs∼PE [L (aE, πθ (s))] = Es∼PE\nh\n(aE −πθ (s))2i\n(1)\nThough BC is appealingly simple (1), it only tends to trains\npolices successfully when we have large amounts of expert\ndata. BC tries to minimize 1-step deviation error along the\nexpert demonstration; it makes the trained polices to be fragile\nwhen distribution mismatch between training and testing. In\nthe some case of experiments, by initializing policy parameters\nwith BC, the learning speed of the proposed method is\nimproved; and thus BC is adapted to our evaluation.\nInverse Reinforcement Learning (IRL). Inverse reinforce-\nment learning is able to learns a policy in the case that a MDP\nspeciﬁcation is known but the reward r is unknown and expert\ndemonstrations TE is available. IRL uncovers the hidden\nreward function R∗which explains the expert demonstration.\nE\n\" ∞\nX\nt=0\nγtR∗(st)|πE\n#\n≥E\n\" ∞\nX\nt=0\nγtR∗(st)|πθ\n#\n(2)\nBased on the uncovered reward function R∗, the reinforcement\nlearning is carried out to train the policy πθ. The objective of\nIRL can be deﬁned as:\nargmax\nθ\nEs∼PE [R∗(s, πθ(s))]\n(3)\nIRL learns a reward function R∗that explains entire expert\ntrajectories. Therefore, a problem which makes the trained\npolicy to be fragile when there are mismatch between training\nand testing environment is not an issue. However, IRL is\nexpensive to run because it has to perform both reward\nfunction optimization (2) and policy optimization (3) at the\nsame time.\nGenerative Adversarial Imitation Learning (GAIL) [17].\nGenerative adversarial imitation learning (GAIL) learns a pol-\nicy that can imitate expert demonstration using the adversarial\nnetwork from generative adversarial network (GAN). The\nAlgorithm 1: Augmented Random Search V 2\nHyperparameters: α step size, N number of sampled\ndirections per iteration, δ a zero\nmean Gaussian vector, ν a positive\nreal number standard deviation of the\nexploration noise\nInitialize\n: θ0 = 0 ∈Rp×n, µ0 = 0 ∈Rn, and\nP\n0 = In ∈Rn×n\n1 while t ≤Max Iteration do\n2\nSample δt = {δ1, δ2, ..., δN; δi ∈Rp×n} with i.i.d.\n3\nCollect 2N rollouts and their corresponding rewards\nusing the 2N policies.\n4\nπt,i,+(x) = (θt + νδi)diag(P\nt)1/2(x −µt)\n5\nπt,i,−(x) = (θt −νδi)diag(P\nt)1/2(x −µt)\n6\nfor i ∈{1, 2, . . . , N}\n7\nUpdate Step:\nθt+1 = θt +\nα\nNσR\nPN\ni=1\n\u0002\nr(πt,(i),+) −r(πt,(i),−)\n\u0003\n8\nSet µt+1, P\nt+1 to be the mean and covariance of the\nstates encountered from the start of training.\n9\nt = t + 1\n10 end\nobjective of GAIL is deﬁned as:\nargmin\nθ\nargmax\nφ\nEπθ [log Dφ(s, a)] + EπE [log(1 −Dφ(s, a))]\n(4)\nwhere πθ, πE are a policy which is parameterized by θ\nand an expert policy. Dφ(s, a) →[0, 1] is an discriminator\nparameteriezd by φ ∈Rn+p [17]. The discriminator network\nand policy play an min-max game to train policy πθ by having\nthe policy πθ confuse a discriminator Dφ. Discriminator Dφ\nuses state-action pair τi from the expert demonstrations TE; it\ndistinguish between the expert trajectories and the trajectories\ndistribution of the trained policy. Dφ(s, a) is the probability\nthat state-action pairs (s, a) belongs to an expert demonstra-\ntion. During the policy optimization, the GAIL uses trust\nregion policy optimization (TRPO) to prevent perturbations of\npolicy. Let the objective loss Equation (4) as LG. The gradient\nof each component is as follows:\n▽φLG = Eπθ [▽φ log Dφ(s, a)] +EπE [▽φ log(1 −Dφ(s, a))]\n(5)\n▽θLG = Eπθ [▽θ log Dφ(s, a)]\n(6)\n= Eπθ [▽θ log πφ(a|s)Q(s, a)]\n(7)\nIn Equation (6), log Dφ(s, a) can not be differentiable with\nrespect to θ. Therefore, the form of the policy gradient Equa-\ntion (7) is used to compute the gradient. The discriminator Dφ\ntakes the role of a reward function; and thus it gives learning\nsignal to the policy [17], [21], [22].\nAugmented Random Search (ARS) [20]. Augmented ran-\ndom search (ARS) is a model-free reinforcement learning\nalgorithm based on random search in the parameter space of\npolicies [20], [23]. The objective of ARS is to learn the policy\nwhich maximize the expected rewards; it can be described:\nmax\nθ∈Rn E [r(πθ)]\n(8)\nwhere θ is parameter of the linear policy πθ : Rn →Rp.\nThe random search in parameter space makes the algorithm\nto be derivative-free optimization with noise [20], [23]. Ran-\ndom search algorithm which is the basic concept of ARS\nselects directions uniformly in parameter space and updates\nthe policies along the selected direction without using a line\nsearch. For updating the parameterized policy πθ, the update\ndirection is calculated as follow:\nr(πθ−νδ) + r(πθ+νδ)\nν\n,\n(9)\nfor δ a zero mean Gaussian vector and ν a positive real number\nstandard deviation of the exploration noise. When ν is small\nenough, Eδ [r(πθ+νδ)] can be the smoothed form of Equation\n(8). Therefore, an update increment is an unbiased gradient\nestimator with respect to θ of Eδ [r(πθ+νδ)]; and it makes the\nupdate step of the policies πθ to be unbiased update [20], [24].\nBased on this fact, Bandit Gradient Descent which is called\nBRS was proposed in [25]. Let the θt is the weight of policy at\nt-th training iteration. N denotes that the number of sampled\ndirections per iteration. In BRS, the update step is conﬁgured\nas follows:\nθt+1 = θt + α\nN\nN\nX\ni=1\n[r(πθ+νδi) −r(πθ−νδi)] δi\n(10)\nHowever, the problem of random search in the parameter\nspace of policies is large variations in terms of the rewards\nr(πθ ± νδ) which are observed during training procedure.\nThe variations makes the updated policies to be perturbed\nthrough the updates step (10). To address the large variation\nissue, the standard deviation σR of the rewards which are\ncollected at each iteration is used to adjust the size of the\nupdate step in ARS. Based on the adaptive step size, the ARS\nshows higher performance compared to the deep reinforcement\nlearning algorithms (i.e., PPO, TRPO, A3C, etc.) and BRS\neven if the simple linear policy is used. In this paper, for policy\noptimization of imitation learning, the ARS V2 algorithm\nis used as a baseline. The ARS algorithm is described as\nAlgorithm 1.\nThe update step of ARS means that if r(πt,(i),+)\n>\nr(πt,(i),−), the policy weights θt is updated in the direction of\nδi. However, if r(πt,(i),+) < r(πt,(i),−), the policy weights\nθt is updated in the direction of −δi. This update step\ndoes not need backpropagation procedure which is used to\noptimize DRL; and thus ARS is derivative-free optimization.\nFurthermore, ARS shows that simple linear policies can obtain\ncompetitive performance on the high dimensional complex\nproblems, showing that complicated neural network policies\nFig. 1: Structure of AILSRS.\nare not needed to solve these problems [19], [20].\nIII. RANDOM SEARCH BASED IMITATION LEARNING\nThe proposed simple random search based adversarial\nimitation learning (AILSRS) is based on the ARS-V2 and\ngenerative adversarial imitation learning (GAIL) [17], [20].\nThe main idea of AILSRS is to update the linear policy πθ\nto imitate expert trajectories using adversarial network. We\ndescribe the details of AILSRS in Section III and make a\nconnection between adversarial network and ARS.\nUpdating discriminator (Dφ). In GAIL, Equation (4) draws a\nconnection between adversarial network and imitation learning\nalgorithm. The policy πθ is trained to confuse a discriminator\nDφ. The Dφ tries to distinguish between the distribution\nof trajectories which is sampled by the policy πθ and the\nexpert trajectories TE. The trajectories are consist of state-\naction pair (s, a). The discriminator takes the role of a reward\nfunction in AILSRS shown in Fig. 1; and thus the result of\nthe discriminator is used to train the policy πθ. Therefore, the\nperformance of the discriminator is important in our method.\nHowever, since the policy πθ is updated every iteration,\nsampled trajectories which are used to train the discriminator\nare changed. The training of the discriminator is not stabilized;\nand thus it makes the inaccurate reward signal. As a result, the\npolicy is perturbated during update step [21], [26]. In AILSRS,\nthe loss function of least square GAN (LS-GAN) is used to\ntrain a discriminator Dφ [27]. The objective function of the\ndiscriminator is as follows:\nargmin\nφ\nLLS(D) = 1\n2EπE\n\u0002\n(Dφ(s, a) −b)2\u0003\n+\n1\n2Eπθ\n\u0002\n(Dφ(s, a) −a)2\u0003\n(11)\nwhere a and b are the target discriminator labels for the\nsampled trajectories from the policy πθ and the expert tra-\njectories. In Equation (4), sampled trajectories which are far\nfrom the expert trajectories but on the correct side of the\ndecision boundary are almost not penalized by sigmoid cross-\nentropy loss. In a contrast, the least-squares loss function\n(11) penalizes the sampled trajectories which are far from the\nexpert trajectories on either side of decision boundary [27].\nTherefore, the stability of training is improved; and it leads\nthe discriminator to give accurate reward signals to the update\nstep. In LS-GAN, a and b have relationship b −a = 2 for\nAlgorithm 2: Adversarial Imitation Learning through Sim-\nple Random Search (AILSRS)\nHyperparameters: α step size, N number of sampled\ndirections per iteration, δ a zero\nmean Gaussian vector, ν a positive\nreal number standard deviation of the\nexploration noise\nInitialize\n: θ0 = 0 ∈Rp×n, µ0 = 0 ∈Rn, and\nP\n0 = In ∈Rn×n\n1 while t ≤Max Iteration do\n2\nSample δt = {δ1, δ2, ..., δN; δi ∈Rp×n} with i.i.d.\n3\nCollect 2N rollouts and their corresponding rewards\nusing the 2N policies.\n4\nπt,i,+(s) = (θt + νδi)diag(P\nt)1/2(s −µt)\n5\nπt,i,−(s) = (θt −νδi)diag(P\nt)1/2(s −µt)\n6\nfor i ∈{1, 2, . . . , N}\n7\nUpdate discriminator parameter φt :\n8\n∇φtLLS = 1\n2EπE\n\u0002\n(∇φtDφt(s, a) −b)2\u0003\n9\n+ 1\n2Eπθ\n\u0002\n(∇φDφt(s, a) −a)2\u0003\n10\nUpdate the policy parameter θt :\n11\nθt+1 = θt+\nα\nNσR\nPN\ni=1\n\u0002\nr(πt,(i),+) −r(πt,(i),−)\n\u0003\nδ(i)\n12\nwhere trajectories T sampled from π(t,(i),±)\n13\nand r(πt,(i),±) = E(s,a)∼πt,(i),± [−log(1 −Dφt(T))]\n14\nSet µt+1, P\nt+1 to be the mean and covariance of the\nstates encountered from the start of training.\n15\nt = t + 1\n16 end\nEquation (11) to be Pearson X 2 divergence [27]. However,\nwe use a = 0 and b = 1 as the target discriminator labels.\nThe result of the discriminator Dφ in the range of 0 to 1.\nThese values are chosen by empirical results.\nUpdating policy (πθ). The discriminator in AILSRS is inter-\npreted as a reward function for which the policy optimizes.\nThe form of reward signal is as follows:\nrπθ(s, a) = −log(1 −Dφ(s, a))\n(12)\nThis means that if the trajectories sampled from the policy\nπθ is similar to expert trajectories, the policy πθ gets higher\nreward rπθ(s, a). The policy πθ is updated to maximize the\ndiscounted sum of rewards given by the discriminator rather\nthan the reward from the environment as shown in Fig. 1. The\nobjective of AILSRS can be described:\nargmax\nθ\nE(s,a)∼πθ [r(s, a)] = E(s,a)∼πθ [−log(1 −Dφ(s, a))]\n(13)\nThis Equation (13) is connection of adversarial imitation\nlearning and simple random search.\nAlgorithm. Foremetioned, AILSRS is based on ARS which\nis model-free reinforcement algorithm. Therefore, AILSRS\nuses simple linear policy and parameter space exploration for\nderivative-free policy optimization. The parameters of policy\nπθ is denoted θ and hence θ is a p × n matrix. The noises\nδ of parameter space for exploration are also p × n matrix.\nThe noises are sampled from a zero mean and ν standard\ndeviation Gaussian distribution. AILSRS algorithm is shown\nin Algorithm 2. For each iteration, the noises δ which mean\nsearch directions in parameter space of policy are chosen\nrandomly (line [2]). Each of the selected N noises make two\npolicies in the current policy πθ. We collect 2N rollouts and\nrewards from N noisy policies πt,i,± = θt ± νδi (line [3-6]).\nThe high dimensional complex problems have multiple state\ncomponents with various ranges; and thus it makes the policies\nto result in large changes in the actions when the same sized\nchanges is not equally inﬂuence state components. Therefore,\nthe state normalization is used in AILSRS (line [4-5,14]); and\nit allows linear policies πt,i,± to have equal inﬂuence for the\nchanges of state components when there are state components\nwith various ranges. [18], [20], [28]. The discriminator Dφ\ngives the reward signal to update step. However, since the\ntrajectories for the training of the discriminator can only be\nobtained from current policies πθt, a discriminator is trained\nwhenever the policy parameter θt is updated. The discriminator\nDφ ﬁnds the parameter φ which minimize the objective\nfunction (11) (line [7-9]). By using the reward signals from\nthe discriminator, the policy weight is updated in the direction\nof δ or −δ based on the result of r(πt,(i),+)−r(πt,(i),−) (line\n[10-13]). The state normalization is based on the information\nof the states encountered during the training procedure; and\nthus µ and P are updated (line [14]).\nIV. EXPERIMENTS\nImplementation Setting. In this paper, adversarial imita-\ntion learning through simple random search (AILSRS) is\nimplemented with Python/TensorFlow [29]. Multi-GPU plat-\nform (equipped with the 2 NVIDIA Titan XP GPUs using\n1405 MHz main clock and 12 GB memory) was used for\ntraining and evaluation the proposed method. The performance\nof AILSRS is evaluated on the MuJoCo locomotion tasks [30],\n[31]. The OpenAI Gym provides benchmark reward functions\nfor Gym environments; and it is used to evaluate the per-\nformance of AILSRS. Evaluation on three random seeds is\nwidely adopted in the researches. Therefore, We wanted to\nshow the performance of AILSRS in an equally competitive\nposition [17], [18], [20], [21]. The experiment is implemented\nwith 1, 3 and 5 random seeds of the MuJoCo tasks. The\nhyperparameters were summarized in Table I. Results show\nthat AILSRS achieves rewards from expert trajectories in\nvarious random seed evaluation. Each training curve was\nsmoothed through a Gaussian ﬁlter for the average of the\nexperimental results. The discriminator network for AILSRS\nis consist of two hidden layer of 100 units, with tanh non-\nlinearities in between layers. In the experiment, BC used the\nsame structure policy as AILSRS.\nTABLE I: Hyperparameters\nHyperparameters\nDescriptions\nα update step\n0.02\nN number of direction\n320\nν standard deviation of noise\n0.03\nMax iteration of each rollout\n1000\nMax Training iteration\n100000\nDiscriminator learning rate\n0.00025\nDiscriminator batch size\nepisode length of each rollout\nDiscriminator training iteration\n3\nSample Efﬁciency Experiments. The purpose of experiments\nof Fig. 2 was to show the sample efﬁciency of AILSRS. In\nFig. 2, the blue lines means that the performance of the trained\npolicies by AILSRS for the MuJoCo locomotion tasks. To\ncompare the difference between the performance of AILSRS\nand GAIL, the experiments is executed on the HalfCheetah-v2,\nSwimmer-v2, Hopper-v2, and Walker-v2. In this experiment,\nin order to assess performance variability, repeated-measures\nwere used based on three random seeds. Behavior cloning\n(BC) is used to accelerate the training policies for AILSRS and\nGAIL on HalfCheetah-v2 experiments. We evaluate AILSRS\nagainst two benchmars:\n• Behavior Cloning (BC) : The policy is trained with\nsupervised learning, using Adam optimizer. The policy\nparameter is trained to satisfy Equation (1).\n• Generative Adversarial Imitation Learning (GAIL) : The\nalgorithm of [17] using the objective function (4). The\nimplementation is based on OpenAI baseline with deter-\nministic/stochastic policy GAIL [32].\nFig. (2) presents that AILSRS achieve reasonable performance\non MuJoCO locomotion tasks. On average, the performance\nof AILSRS were shown to learn stable policies.\nIn the HalfCheetah-v2 experiment, we used expert trajecto-\nries with an average reward of 4632. At this time, BC has\nthe worst performances of 1000 ± 10.32 when 10 expert\ntrajectories are the least. However, as the number of expert\ntrajectories increases, performance is better, and ﬁnally, perfor-\nmance is better than 4120 ± 129.12 expert rewards. However,\nthe standard deviation of the performance is very large and\nshows that it does not reach the expert rewards.\nIn the case of HalfCheetah-v2, I used imitation learning\nusing GAIL after learning some policy through BC. As a\nresult, GAIL shows the ability to reach expert rewards in the\nlowest 10 expert trajectories in the experimental environment.\nAnd as expert trajectories increase, they perform consistently\nbetter than expert rewards. And, unlike BC, the standard\ndeviations of performance are very small, and we can see that\nstable policy has been learned.\nAs with GAIL, AILSRS pre-learned the policy through\nBC and imitation learning was done using AILSRS. In the\nHalfCheetah-v2 environment, AILSRS also reached 4632,\nthe expert rewards in lesser expert trajectories, and shows\nless standard deviation performance. In addition, unlike BC,\nyou can see that it performs better by interacting with the\nenvironment and learning.\n(a) HalfCheetah-v2\n(b) Hopper-v2\n(c) Walker-v2\n(d) Swimmer-v2\nFig. 2: The performance of trained policy according to the set number of expert trajectories\nWe used expert trajectories with an average reward of 3245\nin the Hopper-v2 experiment. At this time, BC has the worst\nperformances of 1200 ± 10.325 when 10 expert trajectories\nare the least. As the number of Expert trajectories increases,\nthe performance is similar to the expert rewards of 3224\n± 15.413, as in HalfCheetah-v2. In the case of Hopper-v2,\nBC is more stable than HalfCheetah-v2. In the Hopper-v2\nexperiment, both GAIL and AILSRS did not use BC. GAIL\nhas the lowest number of expert trajectories (10), and it shows\nstable performance comparable to expert rewards. AILSRS,\nlike GAIL, shows stable performance reaching expert rewards\nin fewer expert trajectories.\nExpert trajectories were used to show expected rewards of\n1021 for Walker, and data showing 362 expected rewards for\nSwimmer. Walker and Swimmer show that all three bench-\nmark algorithms perform well compared to HalfCheetah-v2\nand Hopper-v2. In the Walker-v2 experiment, BC showed\nperformance of 862.745 ± 30.62 for 10 expert rewards, and\n890.823 ± 31.23 for 30 expert trajectories on average. I give.\nIn all experiments, performance is somewhat less than the\nother algorithms, and the performance of the learned policy is\nalso less stable. In both GAIL and AILSRS, we show that we\nare effectively learning policy within the performance range\nof 1021 ± 51.05 for Expert rewards.\nIn the Swimmer-v2 experiment, all three benchmark algo-\nrithms, such as Walker-v2, show good performance. BC shows\nperformance of 352.95 + - 9.16 in 10 expert trjaectories, and\nis closer to the performance of the most expert than previous\nexperiments. The rest of the Swimmer-v2 experiment shows\nsimilar good performance.\nGAIL and AILSRS Like other experiments, it shows per-\nformance equivalent to expert rewards. At the same time, it\nshows a small standard deviation of about 4.51 and shows\nstable learning performance.\nOverall, these results indicate that the random search in the\nparameter space of policies can be used to imitation learning.\nThrough this section we demonstrate that the performance\nof the proposed AILSRS shows competitive performance\ncomparing with BC and GAIL. AILSRS showed the successful\nlearning of expert trajectories without direct reward of envi-\nronment on MuJoCo locomotion tasks. Together these results\nprovide important possibility into the imitation learning using\nrandom search with simple linear policies.\nTraining Curve. The this sub section of the experiments was\nconcerned with the training stability when we use multiple\nrandom seeds. Fig. 3 shows the training curve of Mujoco\nlocomotion tasks in AILSRS. Each experiment shows the\nresults of learning by increasing the number of random seeds.\nThe average of 5 randomly selected experimental out of 20\nexperimental results is smoothed through a Gaussian ﬁlter.\nGraph made.\nIn the case of HalfCheetah-v2, it shows that the trained\npolicies in 1, 3 and 5 random seed environments reaches the\nexpert’s reward. However, as the number of random seeds\nincreases, the learning process becomes unstable. In case\nof hopper-v2, it shows that reaching expert rewards when\n1 and 3 random seeds are reached, but it is slightly less\nthan 5 when it is 5. For Walker-v2 and Swimmer-v2, all\n(a) HalfCheetah-v2\n(b) Hopper-v2\n(c) Walker-v2\n(d) Swimmer-v2\nFig. 3: An evaluation of AILSRS on the OpenAI Gym and mujoco locomotion tasks. The training curves are averaged for\neach random seed experiments.\nexperimental results show that they reach expert rewards. This\ngraph shows that AILSRS can quickly and successfully learn\npolicies from expert trajectories without a direct reward signal\nof the environment, even if the number of random seeds\nincreases to one, three, and ﬁve. However, in the case of Ant-\nv2 and Humanoid-v2 which are difﬁcult problems in Mujoco\nlocomotion tasks, the learning speed is very slow, and the\nenvironment and analysis of the proposed algorithm are still\nnecessary for the environment.\nV. CONCLUDING REMARKS\nThe proposed simple random search based imitation learn-\ning method is not only a derivative free but aloso model-\nfree reinforcement learning algorithm. Furthermore, the simple\nlinear policies are used to control complex dynamic tasks. It\nshows competitive performance on MuJoCo locomotion tasks.\nThe simple update step makes the algorithm to be facile;\nand thus it makes the reconstruction results is able to get\nreasonable performance easily.\nBy comparing the performance of the proposed model with\ncomplex deep reinforcement learning based imitation learning,\nwe demonstrated that simple random search based imitation\nlearning could be used to train linear policies that achieve\nreasonable performance on the MuJoCo locomotion tasks.\nThis results can be a breakthrough to the common belief\nthat random searches in the parameter space of policy can\nnot be competitive in terms of performance. However, the\nproposed method was not able to get competitive performance\non the Ant-v2 and Humanoid-v2 within reasonable training\ntime. Therefore, since the proposed method is simple on-\npolicy algorithm, we can perform extensive research as future\nresearch directions.\nACKNOWLEDGMENT\nThis research was supported by the National Research Foun-\ndation of Korea (2016R1C1B1015406, 2017R1A4A1015675);\nand also by Institute for Information & Communications\nTechnology Promotion (IITP) grant funded by the Korea\ngovernment (MSIT) (No.2018-0-00170, Virtual Presence in\nMoving Objects through 5G). J. Kim is a corresponding author\nof this paper.\nREFERENCES\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller, “Playing atari with deep reinforcement learn-\ning,” arXiv preprint arXiv:1312.5602, 2013.\n[2] S. Levine and V. Koltun, “Guided policy search,” in International\nConference on Machine Learning, 2013, pp. 1–9.\n[3] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in International Conference on Machine\nLearning, 2015, pp. 1889–1897.\n[4] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv preprint arXiv:1509.02971, 2015.\n[5] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel,\nH. Van Hasselt, and D. Silver, “Distributed prioritized experience\nreplay,” arXiv preprint arXiv:1803.00933, 2018.\n[6] L. Busoniu, R. Babuska, and B. De Schutter, “A comprehensive survey\nof multiagent reinforcement learning,” IEEE Transactions on Systems,\nMan, And Cybernetics-Part C: Applications and Reviews, 38 (2), 2008,\n2008.\n[7] S. Lange, M. Riedmiller, and A. Voigtlander, “Autonomous reinforce-\nment learning on raw visual input data in a real world application,” in\nNeural Networks (IJCNN), The 2012 International Joint Conference on.\nIEEE, 2012, pp. 1–8.\n[8] J. Kober and J. Peters, “Reinforcement learning in robotics: A survey,”\nin Reinforcement Learning.\nSpringer, 2012, pp. 579–610.\n[9] H. Zhu, A. Gupta, A. Rajeswaran, S. Levine, and V. Kumar, “Dexterous\nmanipulation with deep reinforcement learning: Efﬁcient, general, and\nlow-cost,” arXiv preprint arXiv:1810.06045, 2018.\n[10] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learning\naffordance for direct perception in autonomous driving,” in 2015 IEEE\nInternational Conference on Computer Vision (ICCV). IEEE, 2015, pp.\n2722–2730.\n[11] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[12] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and\nD. Meger, “Deep reinforcement learning that matters,” arXiv preprint\narXiv:1709.06560, 2017.\n[13] R. Islam, P. Henderson, M. Gomrokchi, and D. Precup, “Reproducibil-\nity of benchmarked deep reinforcement learning tasks for continuous\ncontrol,” arXiv preprint arXiv:1708.04133, 2017.\n[14] A. Billard, S. Calinon, R. Dillmann, and S. Schaal, “Robot programming\nby demonstration,” in Springer handbook of robotics.\nSpringer, 2008,\npp. 1371–1394.\n[15] D. Pomerleau, “Rapidly adapting artiﬁcial neural networks for au-\ntonomous navigation,” in Advances in neural information processing\nsystems, 1991, pp. 429–435.\n[16] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural\nnetwork,” in Advances in neural information processing systems, 1989,\npp. 305–313.\n[17] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in\nAdvances in Neural Information Processing Systems, 2016, pp. 4565–\n4573.\n[18] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, “Evolution\nstrategies as a scalable alternative to reinforcement learning,” arXiv\npreprint arXiv:1703.03864, 2017.\n[19] A. Rajeswaran, K. Lowrey, E. V. Todorov, and S. M. Kakade, “Towards\ngeneralization and simplicity in continuous control,” in Advances in\nNeural Information Processing Systems, 2017, pp. 6550–6561.\n[20] H. Mania, A. Guy, and B. Recht, “Simple random search provides\na competitive approach to reinforcement learning,” arXiv preprint\narXiv:1803.07055, 2018.\n[21] Anonymous, “Generative adversarial self-imitation learning,” in Sub-\nmitted to International Conference on Learning Representations, 2019,\nunder review.\n[22] P. Henderson, W.-D. Chang, P.-L. Bacon, D. Meger, J. Pineau, and\nD. Precup, “Optiongan: Learning joint reward-policy options using\ngenerative adversarial inverse reinforcement learning,” arXiv preprint\narXiv:1709.06683, 2017.\n[23] J. Matyas, “Random optimization,” Automation and Remote control,\nvol. 26, no. 2, pp. 246–253, 1965.\n[24] Y. Nesterov and V. Spokoiny, “Random gradient-free minimization\nof convex functions,” Universit´e catholique de Louvain, Center for\nOperations Research and Econometrics (CORE), Tech. Rep., 2011.\n[25] A. D. Flaxman, A. T. Kalai, and H. B. McMahan, “Online convex\noptimization in the bandit setting: gradient descent without a gradient,”\nin Proceedings of the sixteenth annual ACM-SIAM symposium on\nDiscrete algorithms.\nSociety for Industrial and Applied Mathematics,\n2005, pp. 385–394.\n[26] J. Sorg, R. L. Lewis, and S. P. Singh, “Reward design via online gradient\nascent,” in Advances in Neural Information Processing Systems, 2010,\npp. 2190–2198.\n[27] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley, “Least\nsquares generative adversarial networks,” in Computer Vision (ICCV),\n2017 IEEE International Conference on.\nIEEE, 2017, pp. 2813–2821.\n[28] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine, “Neural network\ndynamics for model-based deep reinforcement learning with model-free\nﬁne-tuning,” in 2018 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2018, pp. 7559–7566.\n[29] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,\nA. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,\nM. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Murray,\nC. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,\nP. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals,\nP. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng,\n“TensorFlow: Large-scale machine learning on heterogeneous systems,”\n2015, software available from tensorﬂow.org. [Online]. Available:\nhttp://tensorﬂow.org/\n[30] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-\nbased control,” in Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ\nInternational Conference on.\nIEEE, 2012, pp. 5026–5033.\n[31] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” 2016.\n[32] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford,\nJ. Schulman, S. Sidor, Y. Wu, and P. Zhokhov, “Openai baselines,” https:\n//github.com/openai/baselines, 2017.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-08-21",
  "updated": "2020-08-21"
}