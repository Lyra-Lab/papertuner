{
  "id": "http://arxiv.org/abs/2008.09659v1",
  "title": "Efficient neural speech synthesis for low-resource languages through multilingual modeling",
  "authors": [
    "Marcel de Korte",
    "Jaebok Kim",
    "Esther Klabbers"
  ],
  "abstract": "Recent advances in neural TTS have led to models that can produce\nhigh-quality synthetic speech. However, these models typically require large\namounts of training data, which can make it costly to produce a new voice with\nthe desired quality. Although multi-speaker modeling can reduce the data\nrequirements necessary for a new voice, this approach is usually not viable for\nmany low-resource languages for which abundant multi-speaker data is not\navailable. In this paper, we therefore investigated to what extent multilingual\nmulti-speaker modeling can be an alternative to monolingual multi-speaker\nmodeling, and explored how data from foreign languages may best be combined\nwith low-resource language data. We found that multilingual modeling can\nincrease the naturalness of low-resource language speech, showed that\nmultilingual models can produce speech with a naturalness comparable to\nmonolingual multi-speaker models, and saw that the target language naturalness\nwas affected by the strategy used to add foreign language data.",
  "text": "Efﬁcient neural speech synthesis for low-resource languages through\nmultilingual modeling\nMarcel de Korte, Jaebok Kim, Esther Klabbers\nReadSpeaker\nHuis ter Heide, the Netherlands\n{marcel.korte,jaebok.kim,esther.judd}@readspeaker.com\nAbstract\nRecent advances in neural TTS have led to models that can\nproduce high-quality synthetic speech. However, these mod-\nels typically require large amounts of training data, which can\nmake it costly to produce a new voice with the desired qual-\nity. Although multi-speaker modeling can reduce the data re-\nquirements necessary for a new voice, this approach is usually\nnot viable for many low-resource languages for which abundant\nmulti-speaker data is not available. In this paper, we therefore\ninvestigated to what extent multilingual multi-speaker model-\ning can be an alternative to monolingual multi-speaker model-\ning, and explored how data from foreign languages may best be\ncombined with low-resource language data. We found that mul-\ntilingual modeling can increase the naturalness of low-resource\nlanguage speech, showed that multilingual models can produce\nspeech with a naturalness comparable to monolingual multi-\nspeaker models, and saw that the target language naturalness\nwas affected by the strategy used to add foreign language data.\nIndex Terms: neural TTS, sequence-to-sequence models, mul-\ntilingual synthesis, multi-speaker models, data reduction\n1. Introduction\nOver the past few years, developments in sequence-to-sequence\n(S2S) neural text-to-speech (TTS) research have led to synthetic\nspeech that sounds almost indistinguishable from human speech\n(e.g. [1, 2, 3]). However, large amounts of high-quality record-\nings are typically required from a professional voice talent to\ntrain models of such quality, which can make them prohibitively\nexpensive to produce. To counter this issue, investigations into\nhow S2S models can facilitate multi-speaker data has become a\npopular topic of research [4, 5, 6]. A study by [7], for example,\nshowed that multi-speaker models can perform as well or even\nbetter than single-speaker models when large amounts of target\nspeaker data are not available, and that single-speaker models\nonly perform better when substantial amounts of data are used.\nTheir research also showed that the amount of data necessary\nfor an additional speaker can be as little as 1250 or 2500 sen-\ntences without signiﬁcantly reducing naturalness. With regards\nto parametric synthesis, [8] investigated the effect of several\nmulti-speaker modeling strategies for class imbalanced data.\nTheir research found that for limited amounts of speech, multi-\nspeaker modeling and oversampling could improve speech nat-\nuralness compared to single speaker models, while undersam-\npling was found to generally have a harmful effect. They also\nshowed that ensemble methods can further improve naturalness,\nbut this strategy comes with a considerable computational cost\nthat is usually not feasible for S2S modeling.\nAlthough the above research shows that multi-speaker mod-\neling can be an effective strategy to reduce data requirements,\nit is not a suitable solution for many languages for which large\nquantities of high-quality multi-speaker data are not available.\nMultilingual multi-speaker synthesis aims to address this issue\nby training a multilingual model on the data of multiple lan-\nguages. Among the ﬁrst to propose a neural approach to multi-\nlingual modeling was [9]. Instead of modeling languages sepa-\nrately, they modeled language variation through cluster adap-\ntive training, where a mean tower as well as language basis\ntowers were trained. They found that multilingual modeling\ndid not harm naturalness for high-resource languages, while\nlow-resource languages beneﬁted from multilingual modeling.\nAnother study by [10] scaled up the number of unseen low-\nresource languages to twelve, and similarly found that multilin-\ngual models tend to outperform single speaker models.\nMore recently, multilingual modeling was also adopted in\nS2S architectures [11, 12, 13, 14, 15, 16], however mostly for\nthe purposes of code-mixing and cross-lingual synthesis. Lan-\nguage information was typically represented either with a lan-\nguage embedding [12, 15] or with a separate encoder for each\nlanguage [11], while [13] applied both approaches to code-\nmixing and accent conversion.\nWith regards to multilingual\nmodeling, [12] showed that multilingual models can attain a\nnaturalness and speaker similarity that is comparable to that of a\nsingle speaker model for high-resource target languages, while\nresearch from [16] obtained promising results with a crosslin-\ngual transfer learning approach.\nWhile research into S2S multilingual modeling is clearly\nvibrant, there appears to exist little systematic research into how\nS2S multilingual models could be used to increase speech natu-\nralness for low-resource languages. To ﬁll this void, this paper\ninvestigated to what extent results that are found in S2S mono-\nlingual multi-speaker modeling are transferable to multilingual\nmulti-speaker modeling, and if it is possible to attain higher nat-\nuralness on low-resource languages with multilingual models\nthan with single speaker models. Because multilingual model-\ning can beneﬁt from the inclusion of large amounts of non-target\nlanguage data, we also experimented with several data addition\nstrategies and evaluated to what extent these strategies are ef-\nfective to improve naturalness for low-resource languages. As\nthis research is primarily addressing the viability of different\napproaches with regards to low-resource languages, our focus\nis not so much on maximizing naturalness but rather on gain-\ning a better understanding of how different strategies work and\nwould potentially scale up using larger amounts of data.\nThe rest of this paper is organized as follows. In Section 2,\nwe describe the architecture used to conduct our experiments.\nIn Section 3, we describe the experimental design and give de-\ntails about training and evaluation. In Section 4, we provide the\nexperimental results. Finally, in Section 5, we discuss conclu-\nsions and directions for future research.\narXiv:2008.09659v1  [eess.AS]  20 Aug 2020\n2. System architecture\n2.1. S2S Acoustic model\nThe architecture that is used in this paper for acoustic model-\ning is based on VoiceLoop [17]. This architecture is appealing\nfor several reasons: the architecture is relatively small which\nmakes it more suitable to train with smaller amounts of data, the\nmodel takes relatively little time to train, and it is capable of dis-\nentangling speaker information well for seen speakers [18]. To\nmake the architecture suitable for multilingual modeling and in-\ncrease its naturalness and robustness, we made several changes\nto the architecture. First, we incorporated a separate encoder\nfor each language to disentangle language information, simi-\nlar to [11]. We empirically found that representing language\ninformation this way was more effective than using a language\nembedding. This language encoder is used to convert phonemes\nfrom a language-dependent phone set into 256-dimensional em-\nbeddings. Second, we added a 3-layer convolutional prenet Npr\nin the style of [1] to better model phonetic context. Third, we\nadded a two-layer LSTM recurrency Nr with 512 nodes to the\ndecoder to better retain long-term information. The model was\ntrained to produce 80-dimensional mel-spectrogram features in\na way similar to [1]. The resulting architecture is visualized in\nFigure 1.\nLanguage encoder 1\nLanguage encoder N\n... ...\nPrenet (N_pr)\nAttention network\n(N_a)\nUpdate network (N_u)\nOutput network (N_o)\nRecurrent network\n(N_r)\nF_u\nF_o\nSpeaker embedding\nPhoneme sequence\nlanguage 1\nPhoneme sequence\nlanguage N\nSpeaker input\n80-dim mel-\nspectrogram\nFigure 1: Overview of the acoustic model architecture used in\nthis paper\n2.1.1. Class weighted loss\nTraining a multilingual model on a mixture of high-resource and\nlow-resource languages can lead to class imbalances between\nlanguages, which can negatively affect naturalness for minority\nclasses. Although it is possible to address this issue through\nover- and undersampling as explored in [8], we instead decided\nto change the weighting of classes through our loss function,\nfollowing [19]. The purpose of the reweighting is to increase\nimportance of minority class samples, while reducing the im-\npact of majority classes. The advantage of this approach is that\nthe re-weighting operation has a low computational cost, and is\ntherefore more efﬁcient than oversampling or ensemble-based\nmethods. The class weights were computed as follows:\nαi =\nr\nc\nci × N\n(1)\nWhere αi denotes the class weight that is computed for\nclass i, N refers to the number of classes, c is the total num-\nber of samples, and ci is the number of samples for class i. It\nwas suggested by [19] that the model might become less ro-\nbust if the variation in the class weights becomes too large. To\ncounter this effect, we applied a square root operation to the\nweights and found that this led to better naturalness compared\nto both the unbalanced and the balanced weights. The weights\nwere then normalized to correct for the square root operation,\nwhere j is the index that iterates over the number of classes:\nnαi = αi ×\nc\nPN\nj cj × αj\n(2)\n3. Experimental setup\nIn this paper, we aimed to answer the following research ques-\ntions:\n1. To what extent does adding data from non-target lan-\nguage speakers increase the naturalness for various\namounts of data from a low-resource language?\n2. How does replacing monolingual multi-speaker mod-\nels with multilingual multi-speaker models affect speech\nnaturalness?\n3. In what way can additional non-target language data best\nbe added to improve the naturalness of low-resource tar-\nget language speech?\nTwo listening experiments were designed to answer these re-\nsearch questions.\n3.1. Experimental design\nThe ﬁrst experiment was designed to compare the naturalness of\nsingle speaker models with that of multilingual models for dif-\nferent amounts of data from the target speaker. For this purpose,\nwe trained three single speaker models using 2000, 4000, and\n8000 sentences (referred to as SING-2k, SING-4k, and SING-\n8k respectively) as target language data. We also trained three\nmultilingual models with the same amount of data for the target\nlanguage, and added an additional 16000 sentences from a for-\neign language speaker (referred to as MULT-2k+16k, MULT-\n4k+16k, MULT-8k+16k).\nWe hypothesize that the multilin-\ngual multi-speaker models will perform better than the single\nspeaker models when the data set of the target speaker is lim-\nited, as we expect that the addition of foreign language data will\nimprove the robustness of the model. We also hypothesize that\nthe effect will become smaller when more target language data\nis available. We used the data of an American English speaker\nfor the target language, and the data of a Dutch speaker as aux-\niliary language data. Other language pairs were tried internally\nto ensure that ﬁndings were reproducible. However, for the pur-\nposes of the listening test, American English was chosen as the\ntarget language to make subjective evaluation more straightfor-\nward, while Dutch was chosen to informally evaluate potential\nadverse effects in the auxiliary language.\nThe second experiment was designed to compare monolin-\ngual multi-speaker models to multilingual multi-speaker mod-\nels, with similar as well as larger amounts of non-target lan-\nguage data. To evaluate how the models would behave when\ngiven similar amounts of data, we created a monolingual model\n(MONO-2k+16k) with 2000 sentences of our target speaker and\n16000 sentences from another American English speaker. This\nmodel was compared to the MULT-2k+16k model that was also\nused in the previous experiment. We hypothesize that because\nof the effort to separate languages with language encoders, the\nmultilingual model should attain a naturalness close to or sim-\nilar to the naturalness of the monolingual model.\nAlthough\nthere is more overlap in terms of pronunciation and prosody for\nmonolingual speakers than for multilingual speakers, we expect\nthat its effect on the naturalness of the target speaker should be\nlimited because the rest of the model is trained jointly.\nBecause multilingual modeling makes it more straightfor-\nward to include language data from non-target languages, we\nalso used this experiment to analyze whether adding more for-\neign language data could improve naturalness, and in which\nway additional data can best be added. We designed three addi-\ntional models to evaluate this question. The ﬁrst model, MULT-\n2k+2x16k, was trained on the same data as the MULT-2k+16k\nmodel, but with an additional 16000 sentences from a second\nDutch speaker. If naturalness increases as a result of this addi-\ntional data, this could indicate that it is beneﬁcial for the model\nto have data from multiple speakers in the data set, for example\nto better separate speaker speciﬁc prosody and pronunciation\npatterns. The second model, MULT-2k+16k+16k, was trained\non the same data as the MULT-2k+16k model, but with an ad-\nditional 16000 sentences from a third language, in this case\nFrench. If naturalness increases signiﬁcantly as a result of this\nstrategy, it could be an indication that the model beneﬁts from\nthe ability to distinguish between large amounts of data, for\nexample to better handle differences in prosody or pronuncia-\ntion. The third model, MULT-2k+16x2k, was again trained on\n2000 sentences from the target speaker, and an additional 2000\nsentences from 16 speakers of 14 languages (13 European lan-\nguages as well as Arabic). If this approach increases naturalness\nsigniﬁcantly, this could be an indication that the model beneﬁts\nfrom language variety or from a lack of class imbalances.\nTo train the models, we used a proprietary text-to-speech\ndata set. The speech consisted of recordings from professional\nvoice talents who were asked to read aloud texts in a studio envi-\nronment. After recording, all speech was processed and down-\nsampled to 22 kHz. Foreign language recordings, for example\nEnglish recordings for non-English languages, were excluded\nto ensure that the results of this experiment were not impacted\nby such sentences.\nFor both experiments, we used a MUltiple Stimuli with\nHidden Reference and Anchor (MUSHRA) test to evaluate nat-\nuralness [20]. Speaker similarity was not subjectively evalu-\nated, because we found that the speaker characteristics of the\ntarget speaker were not harmed by the addition of data from\nother speakers. For the test, we recruited 30 participants with\na good command of English. For both experiments, we created\nthree separate test sets, each containing 10 stimulus panels with\naudio from unseen sentences. A participant was assigned one\nout of three test sets for both of the experiments, hence every\nparticipant evaluated 20 panels. This way, the time to com-\nplete the test was reduced whilst ensuring that results were not\nsigniﬁcantly impacted by a particular sentence. Following the\nMUSHRA guidelines, we included a resynthesized sample on\neach stimulus panel, both as a reference and as a hidden anchor.\nFor the design of the listening tests, we used the publicly\navailable WebAudioEvaluationTool [21]. Both the panels as\nwell as the samples within a panel were randomized. In ad-\ndition, the initial value of each slider in a panel was randomized\nto nudge participants to use the whole spectrum from 0 (com-\npletely unnatural) to 100 (completely natural). Participants had\nto listen to every sample and change the value of every slider\nbefore being allowed to proceed to the next panel. The exper-\niments were then analyzed with a Wilcoxon signed-rank test,\nwhere a Holm-Bonferroni correction [22] was used to reduce\nthe chance of Type 1 errors.\n3.2. Training procedure\nThe training of all acoustic models was done in two stages.\nEach model was ﬁrst pretrained on sentences of up to 800\nframes (≈9.3 seconds), and split into separate parts up to 200\nframes similar to [17] to aid learning.\nFor the pretraining,\nStochastic Gradient Descent was used, with a batch size of 32,\na learning rate of 0.1, and momentum of 0.75. After pretrain-\ning, the model was ﬁnetuned using the ADAM optimizer, with\na batch size of 64, a learning rate of 0.0001 and betas of 0.9 and\n0.98. For the monolingual multi-speaker models, class weight-\ning was applied to the loss function to correct for imbalances\nin the speaker distribution. For the multilingual multi-speaker\nmodels, class weighting was applied to counter both speaker\nand language imbalances. The input to the models consisted of\nphonemes from a separate phoneset per language which were\nthen converted into integers, while on the output side the mod-\nels were trained to produce unnormalized 80-dimensional mel-\nspectrogram features. The mel-spectrogram features were then\ndecoded by a WaveGlow vocoder [23], that was trained in uni-\nversal fashion [24] on a proprietary data set consisting of 5000\nsentences each from 3 female and 2 male speakers.\n4. Results\n4.1. Experiment 1: Single-speaker modeling vs multilin-\ngual modeling\nFor the ﬁrst experiment, 30 participants were invited to evaluate\n10 stimulus panels with 7 audio samples per panel. Of the 300\nresulting data points, we discarded 15 data points where one\nsingle sample was rated considerably higher than the resynthe-\nsized sample. If multiple samples were rated higher than the\nresynthesized sample, we did not consider them anomalies and\ndid not remove them. The rationale behind this approach is that\nif just a single sample was rated higher, it was more likely to be\nan outlier, and would also have a larger impact in the Wilcoxon\nrank testing than if multiple samples were rated higher.\nFigure 2: Boxplot showing the naturalness of single speaker\nmodels and multilingual models used in experiment 1. Red lines\nshow median values, green lines show mean values\nThe MUSHRA scores of the ﬁrst experiment are displayed\nin Figure 2. The results showed that the naturalness signiﬁ-\ncantly increased when more target language data was available,\nboth for single speaker and multilingual models. More interest-\ningly, adding foreign language data to the target language data\ngenerally had a positive effect on the naturalness of the target\nspeaker. When comparing the models with 2000 sentences of\ntarget language data, we found that the MULT-2k+16k model\noutperformed the SING-2k model signiﬁcantly, and was on par\nwith the SING-4k model (p ≈0.172). Although the SING-2k\nmodel generally produced stable attention, the naturalness rat-\nings for this model were negatively impacted by occasional mis-\npronunciations that almost never occurred in the speech of other\nmodels. For the models that were trained on 4000 sentences\nfrom the low-resource language, the MULT-4k+16k model still\nproduced signiﬁcantly more natural speech than the SING-4k\nmodel. When comparing the models for which 8000 target lan-\nguage sentences were available, the difference in naturalness\nbetween the single speaker and the multilingual model was no\nlonger signiﬁcant (p ≈0.506). All other system combinations\nwere signiﬁcantly different, and the resynthesized speech was\nrated signiﬁcantly higher than speech from all other systems.\nThe results obtained for multilingual modeling followed\nsimilar patterns to the results in the monolingual multi-speaker\nsettings in [7, 8]. Similar to [7], the addition of non-target lan-\nguage data helped to improve the robustness and naturalness\nof the model when data quantities for the target language were\nlimited, and similar to [8], the difference became insigniﬁcant\nwhen more target language data was available. The fact that\nthe same effects could be replicated in a multilingual setting as\nin a monolingual setting suggests that the model does not suf-\nfer from being trained on different language inputs. We suspect\nthat the effect is minimal because language information is well\nseparated by the language encoders, thus limiting pronunciation\noverlap, while beneﬁting from shared training in the decoder.\n4.2. Experiment 2:\nMonolingual vs multilingual multi-\nspeaker modeling\nOur second experiment was designed to better understand how\nvarious monolingual and multilingual model strategies may ef-\nfect naturalness. We again asked 30 participants to evaluate ten\ndifferent stimulus panels from one out of three test sets. Each\npanel consisted of a resynthesized sample as the reference and\nhidden anchor, and a sample from each of the ﬁve models. A\nsimilar procedure as in the ﬁrst experiment was applied to re-\nmove anomalies, discarding 11 out of 300 data points.\nTable 1: Subjective MUSHRA naturalness scores for systems in\nExperiment 2\nSystem identiﬁer\nMean\nMedian\nAverage rank\nMONO-2k+16k\n42.58\n45\n4.24\nMULT-2k+16k\n45.41\n47\n3.96\nMULT-2k+2x16k\n44.48\n47\n3.98\nMULT-2k+16k+16k\n45.51\n48\n3.91\nMULT-2k+16x2k\n47.24\n50\n3.72\nResynthesis\n88.00\n92\n1.20\nThe results of the second experiment are displayed in Ta-\nble 1.\nWhen comparing the monolingual and the multilin-\ngual model that have similar amounts of data, we found that\nthe multilingual MULT-2k+16k model performed on par with\nthe monolingual MONO-2k+16k model (p ≈0.054).A sig-\nniﬁcant difference between the monolingual model and multi-\nlingual models was found for some of the multilingual mod-\nels with additional data, with a signiﬁcant difference between\nthe MONO-2k+16k and the MULT-2k+16x2k model (p ≈\n0.0003), while the difference between the MONO-2k+16k and\nthe MULT-2k+16k+16k model was marginally signiﬁcant after\nHolm-Bonferroni correction (p ≈0.007). Similar to the ﬁrst\nexperiment, the resynthesized speech was rated signiﬁcantly\nbetter than the speech of all other systems. For the remaining\nsystem combinations, the differences were not signiﬁcant.\nThe results of this experiment showed that no signiﬁcant\ndifference in naturalness was found between the model with\nauxiliary target language data and the model with auxiliary non-\ntarget language data.\nWe suspect that the difference is lim-\nited because the task of mel-spectrogram prediction is relatively\nlanguage-indepedent. In fact, given that languages are sepa-\nrately modeled in the encoder, it might in some cases be beneﬁ-\ncial to have auxiliary non-target language data instead of target\nlanguage data because the architecture allows for better disen-\ntanglement of speaker-speciﬁc prosodic and pronunciation in-\nformation.\nWhen analyzing the multilingual models with additional\ndata, we found that the naturalness of a multilingual model\ncould even surpass that of a monolingual model, but that\nthis was dependent on the sort of data added.\nWhile the\nMULT-2k+16x2k and the MULT-2k+16k+16k approach af-\nfected the naturalness of the target language positively, the\nMULT-2k+2x16k approach did not lead to a signiﬁcant natu-\nralness increase. These results thus suggest that when adding\nmore data, a multilingual model beneﬁts most from language\nvariation and a reduction of class imbalances.\n5. Conclusions and Future Research\nThis paper aimed to investigate the effectiveness of multilin-\ngual modeling to improve speech naturalness of low-resource\nlanguage neural speech synthesis. Our results showed that the\naddition of auxiliary non-target language data can positively im-\npact the naturalness of low-resource language speech and can\nbe a viable alternative to auxiliary target language data when\nsuch data is not readily available. We furthermore found that\nwhen more target language data was available, the inclusion of\nthe auxiliary non-target language data did not negatively affect\nnaturalness. Although we did not compare multilingual mod-\nels with single speaker models for even larger amounts of target\nlanguage data in this research, we expect that results from mul-\ntilingual modeling will largely mimic the effects observed in\nstudies of monolingual multi-speaker modeling [7]. Finally, we\nexplored several strategies for including additional non-target\nlanguage data. We showed that not all data addition strategies\nare equally effective, and reported that language diversity and\nminimizing class imbalances appear to be the most important\nvariables to consider when adding data.\nBased on our conclusions, we identify several directions for\nfuture research. First of all, the current research didn’t con-\nsider the issue of language proximity on the effect of multi-\nlingual modeling. Although languages are modeled separately\nin the encoders, language proximity may positively affect nat-\nuralness.\nAdditionally, this research evaluated low-resource\nlanguage speech naturalness at a general level, while it may\nbe more interesting to focus on the naturalness of language-\nspeciﬁc characteristics such as language-speciﬁc phonemes or\nstress patterns. We furthermore note that the amount of auxil-\niary data used was relatively limited in our experiments. Fur-\nther analysis could be done to ﬁnd out whether our ﬁndings\nhold when scaled up with more data. Finally, we found that\nthe MULT-2k+16x2k model was most effective to improve nat-\nuralness of target language speech, but this result does not clar-\nify whether this effect can be attributed to the large variation\nin languages and speakers, or to the minimization of class im-\nbalances. It would be interesting to disentangle these variables\nby comparing this model to a monolingual multi-speaker model\nwith similar amounts of data per speaker.\n6. References\n[1] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan et al., “Natural\ntts synthesis by conditioning wavenet on mel spectrogram pre-\ndictions,” in 2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2018, pp. 4779–\n4783.\n[2] Z. Kons, S. Shechtman, A. Sorin, C. Rabinovitz, and R. Hoory,\n“High quality, lightweight and adaptable tts using lpcnet,” arXiv\npreprint arXiv:1905.00590, 2019.\n[3] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.\nLiu, “Fastspeech: Fast, robust and controllable text to speech,”\nin Advances in Neural Information Processing Systems, 2019, pp.\n3165–3174.\n[4] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen,\nR. Pang, I. L. Moreno, Y. Wu et al., “Transfer learning from\nspeaker veriﬁcation to multispeaker text-to-speech synthesis,” in\nAdvances in neural information processing systems, 2018, pp.\n4480–4490.\n[5] A. Gibiansky, S. Arik, G. Diamos, J. Miller, K. Peng, W. Ping,\nJ. Raiman, and Y. Zhou, “Deep voice 2: Multi-speaker neural text-\nto-speech,” in Advances in neural information processing systems,\n2017, pp. 2962–2970.\n[6] Y. Deng, L. He, and F. Soong, “Modeling multi-speaker latent\nspace to improve neural tts: Quick enrolling new speaker and en-\nhancing premium voice,” arXiv preprint arXiv:1812.05253, 2018.\n[7] J. Latorre, J. Lachowicz, J. Lorenzo-Trueba, T. Merritt, T. Drug-\nman, S. Ronanki, and V. Klimkov, “Effect of data reduction on\nsequence-to-sequence neural tts,” in ICASSP 2019-2019 IEEE In-\nternational Conference on Acoustics, Speech and Signal Process-\ning (ICASSP).\nIEEE, 2019, pp. 7075–7079.\n[8] H.-T. Luong, X. Wang, J. Yamagishi, and N. Nishizawa, “Train-\ning multi-speaker neural text-to-speech systems using speaker-\nimbalanced speech corpora,” arXiv preprint arXiv:1904.00771,\n2019.\n[9] B. Li and H. Zen, “Multi-language multi-speaker acoustic mod-\neling for lstm-rnn based statistical parametric speech synthesis,”\n2016.\n[10] A. Gutkin, “Uniform multilingual multi-speaker acoustic model\nfor statistical parametric speech synthesis of low-resourced lan-\nguages,” 2017.\n[11] E. Nachmani and L. Wolf, “Unsupervised polyglot text-to-\nspeech,” in ICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2019,\npp. 7055–7059.\n[12] Y. Zhang, R. J. Weiss, H. Zen, Y. Wu, Z. Chen, R. Skerry-Ryan,\nY. Jia, A. Rosenberg, and B. Ramabhadran, “Learning to speak\nﬂuently in a foreign language: Multilingual speech synthesis and\ncross-language voice cloning,” arXiv preprint arXiv:1907.04448,\n2019.\n[13] Y. Cao, X. Wu, S. Liu, J. Yu, X. Li, Z. Wu, X. Liu, and H. Meng,\n“End-to-end code-switched tts with mix of monolingual record-\nings,” in ICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2019,\npp. 6935–6939.\n[14] L. Xue, W. Song, G. Xu, L. Xie, and Z. Wu, “Building a mixed-\nlingual neural tts system with only monolingual data,” arXiv\npreprint arXiv:1904.06063, 2019.\n[15] Z. Liu and B. Mak, “Cross-lingual multi-speaker text-to-speech\nsynthesis for voice cloning without using parallel corpus for un-\nseen speakers,” arXiv preprint arXiv:1911.11601, 2019.\n[16] T. Tu, Y.-J. Chen, C.-c. Yeh, and H.-y. Lee, “End-to-end text-to-\nspeech for low-resource languages by cross-lingual transfer learn-\ning,” arXiv preprint arXiv:1904.06508, 2019.\n[17] Y. Taigman, L. Wolf, A. Polyak, and E. Nachmani, “Voiceloop:\nVoice ﬁtting and synthesis via a phonological loop,” arXiv\npreprint arXiv:1707.06588, 2017.\n[18] E. Nachmani, A. Polyak, Y. Taigman, and L. Wolf, “Fitting new\nspeakers based on a short untranscribed sample,” arXiv preprint\narXiv:1802.06984, 2018.\n[19] T. Alum¨ae, S. Tsakalidis, and R. M. Schwartz, “Improved multi-\nlingual training of stacked neural network acoustic models for low\nresource languages.” in Interspeech, 2016, pp. 3883–3887.\n[20] I. Recommendation, “1534-1,method for the subjective assess-\nment of intermediate sound quality (mushra),” International\nTelecommunications Union, Geneva, Switzerland, 2001.\n[21] N. Jillings, B. Man, D. Moffat, J. D. Reiss et al., “Web audio eval-\nuation tool: A browser-based listening test environment,” 2015.\n[22] S. Holm, “A simple sequentially rejective multiple test proce-\ndure,” Scandinavian journal of statistics, pp. 65–70, 1979.\n[23] R. Prenger, R. Valle, and B. Catanzaro, “Waveglow: A ﬂow-based\ngenerative network for speech synthesis,” in ICASSP 2019-2019\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2019, pp. 3617–3621.\n[24] R. Valle, J. Li, R. Prenger, and B. Catanzaro, “Mellotron: Mul-\ntispeaker expressive voice synthesis by conditioning on rhythm,\npitch and global style tokens,” arXiv preprint arXiv:1910.11997,\n2019.\n",
  "categories": [
    "eess.AS",
    "cs.CL",
    "cs.SD"
  ],
  "published": "2020-08-20",
  "updated": "2020-08-20"
}