{
  "id": "http://arxiv.org/abs/2008.10312v2",
  "title": "Self-Supervised Learning for Large-Scale Unsupervised Image Clustering",
  "authors": [
    "Evgenii Zheltonozhskii",
    "Chaim Baskin",
    "Alex M. Bronstein",
    "Avi Mendelson"
  ],
  "abstract": "Unsupervised learning has always been appealing to machine learning\nresearchers and practitioners, allowing them to avoid an expensive and\ncomplicated process of labeling the data. However, unsupervised learning of\ncomplex data is challenging, and even the best approaches show much weaker\nperformance than their supervised counterparts. Self-supervised deep learning\nhas become a strong instrument for representation learning in computer vision.\nHowever, those methods have not been evaluated in a fully unsupervised setting.\nIn this paper, we propose a simple scheme for unsupervised classification based\non self-supervised representations. We evaluate the proposed approach with\nseveral recent self-supervised methods showing that it achieves competitive\nresults for ImageNet classification (39% accuracy on ImageNet with 1000\nclusters and 46% with overclustering). We suggest adding the unsupervised\nevaluation to a set of standard benchmarks for self-supervised learning. The\ncode is available at https://github.com/Randl/kmeans_selfsuper",
  "text": "Self-Supervised Learning for Large-Scale\nUnsupervised Image Clustering\nEvgenii Zheltonozhskii1 [0000âˆ’0002âˆ’5400âˆ’9321], Chaim Baskin1, Alex M. Bronstein1,\nand Avi Mendelson1\nTechnion â€“ Israel Institute of Technology\nevgeniizh@campus.technion.ac.il; chaimbaskin@campus.technion.ac.il;\nbron@cs.technion.ac.il; avi.mendelson@cs.technion.ac.il\nAbstract. Unsupervised learning has always been appealing to machine\nlearning researchers and practitioners, allowing them to avoid an expensive\nand complicated process of labeling the data. However, unsupervised learning\nof complex data is challenging, and even the best approaches show much\nweaker performance than their supervised counterparts. Self-supervised\ndeep learning has become a strong instrument for representation learning\nin computer vision. However, those methods have not been evaluated in\na fully unsupervised setting. In this paper, we propose a simple scheme\nfor unsupervised classiï¬cation based on self-supervised representations. We\nevaluate the proposed approach with several recent self-supervised methods\nshowing that it achieves competitive results for ImageNet classiï¬cation (39%\naccuracy on ImageNet with 1000 clusters and 46% with overclustering). We\nsuggest adding the unsupervised evaluation to a set of standard benchmarks\nfor self-supervised learning. The code is available at https://github.com/\nRandl/kmeans_selfsuper.\nKeywords: Deep Neural Networks, Unsupervised Deep Learning, Represen-\ntation learning, Self-Supervised Learning\n1\nIntroduction\nDeep learning has become the primary tool in various computer vision tasks, being\nespecially successful in image classiï¬cation, detection, and segmentation. However,\nalong with massive computing resources required to train state-of-the-art neural\nnetworks (NNs), massive datasets with millions of labeled samples are a necessary\npart of its success. Since creating those datasets is a costly procedure, researchers\nhave recently started looking at the methods of training NNs without labeled data.\nThose methods commonly referred to as self-supervised learning recently has become\na powerful instrument for large-scale computer vision.\nA result of training a network in a self-supervised manner is usually a repre-\nsentation: a vector in a latent space. Evaluation of those representations proceeds\nmainly by two following approaches: ï¬ne-tuning the network as a feature extractor\nfor some task (common choices are segmentation tasks or ImageNert classiï¬cation\non a small amount of data, e.g., 1% of labels) or training a linear classiï¬er on the\narXiv:2008.10312v2  [cs.CV]  9 Nov 2020\n2\nE. Zheltonozhskii et al.\nextracted features. A variation of the latter is to train a ğ‘˜-nearest neighbor classiï¬er\ninstead of a linear classiï¬er. While linear classiï¬cation directly evaluates the learned\nrepresentation, it is not always capable of predicting performance on downstream\ntasks (Resnick et al., 2019). On the other hand, performance of a ï¬ne-tuned network\nstrongly depends on the training procedure which is hard to separate from the\nquality of the representation itself.\nIn computer vision, the main approaches to training a network in a self-\nsupervised manner are contrastive losses, pretext tasks, and generative models.\nContrastive methods (van den Oord et al., 2018; Ye et al., 2019; Ermolov et al.,\n2020) try to create diï¬€erent views of the same image and bring a representation\nof diï¬€erent views closer and representations of diï¬€erent images farther apart.\nAlternatively, it is possible to train the network to perform some label-free pretext\ntask, such as predicting context (Doersch et al., 2015), image rotation (Gidaris et al.,\n2018; Kolesnikov et al., 2019), colorization (Zhang et al., 2016), solving â€œjigsaw\npuzzleâ€ (Kim et al., 2018), etc. Generative-based representation learning uses the\nlatent vectors of a generative model, e.g., Boltzmann machines (Lee et al., 2009),\nautoencoders (Caron et al., 2018a) or GANs (Donahue et al., 2016; Donahue and\nSimonyan, 2019), as a representation.\nThe overview of recent methods of self-supervised and unsupervised approaches\nis given in Table 1.\nTable 1: Brief review of selected self-supervised methods, including links to code\nand linear-evaluation performance. Clustering from pretext (Van Gansbeke et al.,\n2020) is an unsupervised method. First plus in each row is a hyperlink.\nMethod\nCode Checkpoints ResNet-50 Best\nCPC (van den Oord et al., 2018)\nâ€“\nâ€“\nâ€“\n48.7%\nCPC v2 (HÃ©naï¬€et al., 2019)\nâ€“\nâ€“\n63.8%\n71.5%\nAMDIM (Bachman et al., 2019)\n+\n+\nâ€“\n68.1%\nCMC (Tian et al., 2019)\n+\n+\n66.2%\n70.6%\nBigBiGAN (Donahue and Simonyan, 2019)\nâ€“\n+\nâ€“\n61.3%\nMoCo (He et al., 2019)\n+\n+\n60.6%\n68.6%\nSelf-Label (Asano et al., 2019)\n+\n+\n71.1%\n71.1%\nSimCLR (Chen et al., 2020b)\n+\n+\n69.3%\n76.5%\nMoCo v2 (Chen et al., 2020d)\n+\n+\n71.1%\n71.1%\nInfoMin (Tian et al., 2020)\n+\n+\n73.0%\n75.2%\nBYOL (Grill et al., 2020)\n+\n+\n74.3%\n79.6%\nSwAV (Caron et al., 2020)\n+\n+\n75.3%\n78.5%\nSimCLRv2 (Chen et al., 2020c)\n+\n+\n71.7%\n79.8%\niGPT (Chen et al., 2020a)\n+\n+\nâˆ’\n72.0%\nClustering from pretext (Van Gansbeke et al., 2020)\n+\n+\nâ€“\nâ€“\nSelf-Supervised Learning for Large-Scale Unsupervised Image Clustering\n3\nContribution In this paper, we propose an additional way of evaluating self-\nsupervised learning: training a clustering algorithm on extracted features in an\nunsupervised manner. While this method suï¬€ers from similar disadvantages as\nlinear evaluation, it can provide additional insights and a benchmark for unsu-\npervised learning on large-scale datasets, such as ImageNet. We also show that\nself-supervised learning provides a strong baseline for unsupervised computer\nvision and mentions some possible direction for the current self-supervised methods\nperformance improvement.\nThanks to the increasing trend of publishing pre-trained models and code, we\nwere able to test the existing approaches on the proposed benchmark. In particular,\nwe show that the best-performing self-supervised algorithm achieves almost 40%\ntop-1 accuracy on ImageNet without any supervision. Those results are on par with\na specialized clustering approach by Van Gansbeke et al. (2020). We also evaluate\nObjectNet (Barbu et al., 2019), a dataset created for testing image classiï¬cation\nalgorithms in conditions closer to real-life, and conclude that it is hard to achieve\ngeneralization in unsupervised settings.\nThis benchmark provides a more challenging task for future self-supervised\nlearning approaches, allowing them to better track their progress.\n2\nMethod\n2.1\nMetrics\nThe evaluation of unsupervised learning methods is a complicated topic, and many\ndiï¬€erent metrics were developed. In this section, we brieï¬‚y review the metrics we\nutilized for clustering evaluation.\nAccuracy In the presence of ground truth labels, it is possible to evaluate the\nprediction accuracy by assigning classes to predicted clusters. Similarly to previous\nworks (Xie et al., 2015; Jiang et al., 2016; Van Gansbeke et al., 2020) we use\nlinear assignment (Kuhn, 1955; Crouse, 2016) for assignment of clusters to the\nclasses. In cases when the number of clusters is larger than the number of classes\n(overclustering), we assign one cluster to each class, while the rest is assigned\ngreedily to maximize accuracy.\nNormalized Mutual Information (V-measure) For a partition of the instances, ğ‘ˆ, we\ndeï¬ne entropy as\nğ»(ğ‘ˆ) = âˆ’\nğ‘…\nâˆ‘ï¸\nğ‘–=1\nğ‘ƒğ‘ˆ(ğ‘–) log(ğ‘ƒğ‘ˆ(ğ‘–)),\n(1)\nand mutual information between two partitions as\nMI(ğ‘ˆ,ğ‘‰) =\nğ‘…\nâˆ‘ï¸\nğ‘–=1\nğ¶\nâˆ‘ï¸\nğ‘—=1\nğ‘ƒğ‘ˆğ‘‰(ğ‘–, ğ‘—) log\n\u0012 ğ‘ƒğ‘ˆğ‘‰(ğ‘–, ğ‘—)\nğ‘ƒğ‘ˆ(ğ‘–)ğ‘ƒğ‘‰(ğ‘—)\n\u0013\n,\n(2)\n4\nE. Zheltonozhskii et al.\nwhere\nğ‘ƒğ‘ˆğ‘‰(ğ‘–, ğ‘—) = |ğ‘ˆğ‘–|\n\f\fğ‘‰ğ‘—\n\f\f\nğ‘\n(3)\nğ‘ƒğ‘ˆ(ğ‘–) = |ğ‘ˆğ‘–|\nğ‘.\n(4)\nTo be able to compare mutual information in diï¬€erent cases, it is usually normalized\n(Kvalseth, 1987):\nNMI(ğ‘ˆ,ğ‘‰) =\nMI(ğ‘ˆ,ğ‘‰)\navg(ğ»(ğ‘ˆ), ğ»(ğ‘‰)),\n(5)\nwhere avg is some function, in our case the arithmetic mean.\nAdjusted Mutual Information Since mutual information tends to have larger values\nwhen the number of clusters is large, mutual information should be adjusted for\nrandom chance (Vinh et al., 2010)\nAMI(ğ‘ˆ,ğ‘‰) =\nMI(ğ‘ˆ,ğ‘‰) âˆ’ğ”¼[MI(ğ‘ˆ,ğ‘‰)]\n[avg(ğ»(ğ‘ˆ), ğ»(ğ‘‰)) âˆ’ğ”¼[ğ‘€ğ¼(ğ‘ˆ,ğ‘‰)] .\n(6)\nAdjusted Rand Index\nRand index (Rand, 1971) is another measure of clustering\nquality. It can be viewed as an accuracy measure over pairs of instances: denoting\nthe number of pairs as ğ‘ğ‘= \u0000ğ‘\n2\n\u0001\n, the number of pairs of instances that belong to\nthe same set in both partitions as TP, and the number of pairs of instances that\nbelong to the diï¬€erent sets in both partitions as TN, we deï¬ne Rand index as\nRI = TP + TN\nğ‘ğ‘\n.\n(7)\nWe also adjust the index for chance in the usual manner (Hubert and Arabie, 1985):\nARI = RI(ğ‘ˆ,ğ‘‰) âˆ’ğ”¼[RI(ğ‘ˆ,ğ‘‰)]\n[1 âˆ’ğ”¼[RI(ğ‘ˆ,ğ‘‰)]\n,\n(8)\nwhere 1 is the maximal value of Rand index.\n2.2\nEvaluation\nTo train a clustering model, we extract features of both the training and the\nvalidation set with a pre-trained model. We do not apply any augmentations\nduring feature extraction. As opposed to the existing clustering approaches, e.g.,\nDeepCluster (Caron et al., 2018b), our method does not utilize a clustering objective\nas a part of feature extractor training, but merely uses a feature extractor pre-trained\nin a self-supervised manner.\nModern clustering approaches are usually based on some distance between\ndiï¬€erent samples. Unfortunately, if the dimension of space is high, the distance\nSelf-Supervised Learning for Large-Scale Unsupervised Image Clustering\n5\nbetween samples provides a little information. In our case, since most of the methods\nprovide at least 1000-dimensional embeddings, we apply dimensional reduction.\nIn particular, we train incremental PCA model with batch size max \u00004096, 2 Â· ğ‘›ğ‘“\n\u0001\n,\nwhere ğ‘›ğ‘“is the dimension of extracted features.\nAfter applying dimensional reduction, we train mini-batch variation of k-means\nwith the transformed features. Since the features are extracted only once, the\ntraining clustering model is relatively cheap. Depending on the model, it takes\na couple of hours on CPU. Using augmentation and training ï¬rst PCA and then\nclustering models can boost performance but is much more resource-demanding.\nWhile by default, we set the number of clusters to be 1000 (number of ImageNet\nclasses), we also experiment with overclustering, following Van Gansbeke et al.\n(2020).\n3\nExperimental Results\nWe evaluate recent state-of-the-art approaches with the proposed protocol: MoCo\nv2 (Chen et al., 2020d), InfoMin (Tian et al., 2020), SwAV (Caron et al., 2020),\nSimCLRv2 (Chen et al., 2020c), and BigBiGAN (Donahue and Simonyan, 2019). For\nevery paper, we evaluate ResNet-50 and best performing network. We also add\nresults for three models trained in supervised manner1: ResNet-152, Eï¬ƒcientNet-L2\n(Xie et al., 2019), and IG-ResNeXt-101 32Ã—48d (Mahajan et al., 2018).\nFor experiments, we utilize feature extracted from two diï¬€erent datasets:\nImageNet and ObjectNet (Barbu et al., 2019).\nWe trained k-means for 60 epochs, but even 1 epoch often gets decent results.\nImageNet Experimental results for ImageNet are shown in Table 2. During accuracy\ncalculation, we used training labels for cluster assignment. In addition, we visualize\ndiï¬€erent metrics in Fig. 1. We note a strong correlation between linear evaluation\naccuracy and k-means accuracy, except for SimCLRv2 (ResNet-152 3Ã—, SK) and\nSwAV. We note that SCAN (Van Gansbeke et al., 2020) gives signiï¬cantly larger ARI\nfor similar accuracy values. We also note that both supervised and self-supervised\nmethods with high-dimensional embeddings ( ResNet-152 3Ã— and Eï¬ƒcientNet-L2)\nshow weaker results than their counterparts.\nObjectNet To access the generalization of acquired clustering, in addition to\nImageNet, we evaluate the proposed method on ObjectNet. ObjectNet is a test set\nfor vision tasks, created to control the performance of vision algorithms in settings\nclose to real life.\nTable 3 shows results for k-means trained on ImageNet training set. In this case,\nwe evaluated only on intersecting classes between ImageNet and ObjectNet. We\nshow accuracy both for cluster assignment based on ImageNet training set (ACC-tr)\nand ObjectNet itself (ACC-val). Note that since some ObjectNet classes are mapped\nto two diï¬€erent ImageNet classes, assigning all images to a single class will result\n1 We employ evaluation code by Wightman (2020).\n6\nE. Zheltonozhskii et al.\nTable 2: Experimental results on ImageNet in form meanÂ±std of 5 runs. Overclus-\ntering denoted as â€œover.â€, supervised models denoted as â€œsuper.â€ Bold denotes\nhighest results among our experiments, and red denotes results within one standard\ndeviation of best results. Results for self-label are taken from the paperâ€™s oï¬ƒcial\nrepository.\nMethod\nLinear (super.)\nACC\nARI\nAMI\nNMI\nMoCo v2 (ResNet-50)\n71.1\n23.09 Â± 0.16\n11.99 Â± 0.13\n37.04 Â± 0.10\n63.22 Â± 0.05\nInfoMin (ResNet-50)\n73.0\n33.17 Â± 0.32\n14.71 Â± 0.38\n48.25 Â± 0.27\n68.80 Â± 0.17\nSwAV (ResNet-50)\n75.3\n15.04 Â± 0.77\n7.72 Â± 0.33\n32.33 Â± 0.16\n55.34 Â± 0.62\nSimCLRv2 (ResNet-50)\n71.7\n22.40 Â± 0.19\n10.97 Â± 0.20\n34.85 Â± 0.29\n61.52 Â± 0.18\nBigBiGAN (RevNet-50 4Ã—)\n61.3\n3.00 Â± 0.09\n1.01 Â± 0.04\n8.81 Â± 0.27\n35.99 Â± 0.69\nInfoMin (ResNeXt-152)\n75.2\n38.60 Â± 0.67\n22.15 Â± 0.52 52.56 Â± 0.11 72.17 Â± 0.13\nSimCLRv2 (ResNet-152, SK)\n77.2\n39.07 Â± 0.61 22.80 Â± 0.60 52.03 Â± 0.19\n71.83 Â± 0.13\nSimCLRv2 (ResNet-152 3Ã—, SK)\n79.8\n31.15 Â± 0.74\n13.84 Â± 0.84\n46.64 Â± 0.25\n65.79 Â± 0.58\nSimCLRv2 (ResNet-152, SK, 1.5Ã— over.)\n77.2\n46.03 Â± 0.21\n23.94 Â± 0.16\n50.77 Â± 0.25\n73.14 Â± 0.06\nSCAN (Van Gansbeke et al., 2020)\nâˆ’\n39.9\n27.5\n51.2\n72.0\nSelf-label (Asano et al., 2019)\n63.5\n30.5\n16.2\n42.0\n75.4\nSelf-label 3Ã— over. (Asano et al., 2019)\n68.8\n38.1\n27.6\n52.8\n75.7\nResNet-152 (super.)\n81.0\n65.60 Â± 0.93\n53.02 Â± 0.76\n74.02 Â± 0.22\n84.97 Â± 0.17\nIG-ResNeXt-101 32Ã—48d (super.)\n85.4\n72.39 Â± 0.52\n63.31 Â± 0.40\n81.17 Â± 0.08\n89.23 Â± 0.05\nEï¬ƒcientNet-L2 (super.)\n88.2\n59.08 Â± 0.67\n46.32 Â± 0.60\n69.35 Â± 0.26\n82.33 Â± 0.18\n0\n20\n40\n60\n80\n100\nLinear evaluation accuracy, %\n0\n25\n50\n75\n100\nk-means accuracy, %\nLinear ï¬t\n(a)\n0\n20\n40\n60\n80\n100\nUnsupervised accuracy, %\n0\n25\n50\n75\n100\nARI, %\nLinear ï¬t\nSelf-supervised methods\nSCAN\n(b)\nFig. 1: Visualization of diï¬€erent metrics: (a) unsupervised accuracy and linear\nevaluation accuracy; (b) ARI and unsupervised accuracy. Green points are outliers\n(SimCLRv2 (ResNet-152 3Ã—, SK) and SwAV).\nin âˆ¼1.77% accuracy. By manually inspecting the predictions of the k-means, we\nconclude that in many cases, assignment of a large part of instances to a single\nclass indeed happens.\nWhen the ImageNet cluster assignment is used, no network, including supervised\nones, show better-than-random performance. For ObjectNet assignment, only\nBigBiGAN (as well as supervised networks) is signiï¬cantly better than assigning\nall the instances to a single class. Moreover, ResNet-50 shows better results than\nlarger networks among diï¬€erent self-supervised networks. We advise that, as for\nnow, AMI should be used as a metric for tracking progress on this task.\nSelf-Supervised Learning for Large-Scale Unsupervised Image Clustering\n7\nTable 4 shows results for clustering trained directly on ObjectNet. For pre-trained\nmodels, the performance on classes that are part of ImageNet is much better: for\nexample, IG-ResNeXt-101 32Ã—48d, has 41.36% accuracy as compared to 15.81%\nfor classes not in ImageNet. For self-supervised, the diï¬€erence is much smaller: for\nInfoMin, performance on classes not included in ImageNet is the same (6.53%).\nTable 3: Experimental results on ObjectNet in form meanÂ±std of 5 runs, using\nclusters acquired from ImageNet training.\nMethod\nACC-tr\nACC-val\nARI\nAMI\nNMI\nMoCo v2 (ResNet-50)\n0.11 Â± 0.19\n1.76 Â± 0.20\n0.02 Â± 0.04\n0.42 Â± 0.49\n0.82 Â± 0.66\nInfoMin (ResNet-50)\n0.12 Â± 0.26\n2.18 Â± 0.25\n0.08 Â± 0.07 1.81 Â± 0.57\n2.34 Â± 0.56\nSwAV (ResNet-50)\n0.21 Â± 0.33\n1.85 Â± 0.12\n0.06 Â± 0.08\n0.65 Â± 0.34\n1.30 Â± 0.48\nSimCLRv2 (ResNet-50)\n0.00 Â± 0.00\n2.14 Â± 0.30\n0.06 Â± 0.06\n1.47 Â± 0.76\n2.43 Â± 0.75\nBigBiGAN (RevNet-50 4Ã—)\n0.10 Â± 0.01 4.92 Â± 0.20 0.10 Â± 0.01 1.00 Â± 0.06 15.98 Â± 0.69\nInfoMin (ResNeXt-152)\n0.67 Â± 0.39 1.96 Â± 0.39\n0.01 Â± 0.01\n0.70 Â± 0.37\n1.26 Â± 0.58\nSimCLRv2 (ResNet-152, SK)\n0.00 Â± 0.00\n1.69 Â± 0.28\n0.03 Â± 0.07\n0.55 Â± 0.82\n0.86 Â± 0.94\nSimCLRv2 (ResNet-152 3Ã—, SK)\n0.00 Â± 0.00\n1.72 Â± 0.19\n0.01 Â± 0.01\n0.44 Â± 0.42\n0.94 Â± 0.61\nSimCLRv2 (ResNet-152, SK, 1.5Ã— over.) 0.04 Â± 0.10\n1.75 Â± 0.19\n0.01 Â± 0.01\n0.45 Â± 0.38\n0.99 Â± 0.50\nResNet-152 (super.)\n0.36 Â± 0.48\n1.75 Â± 0.35\n0.03 Â± 0.07\n0.53 Â± 0.97\n0.76 Â± 1.19\nIG-ResNeXt-101 32Ã—48d (super.)\n0.04 Â± 0.08\n2.15 Â± 0.84\n0.14 Â± 0.21\n2.12 Â± 2.91\n2.51 Â± 3.31\nEï¬ƒcientNet-L2 (super.)\n0.36 Â± 0.44\n2.10 Â± 0, 41\n0.13 Â± 0.14\n1.95 Â± 1.37\n2.34 Â± 1.51\nTable 4: Experimental results on ObjectNet using clusters acquired by training\nk-means on ObjectNet itself.\nMethod\nACC\nARI\nAMI\nNMI\nMoCo v2 (ResNet-50)\n4.30 Â± 0.05\n0.77 Â± 0.02\n8.08 Â± 0.10\n20.57 Â± 0.39\nInfoMin (ResNet-50)\n4.96 Â± 0.08\n0.92 Â± 0.22\n8.85 Â± 0.08\n21.49 Â± 0.17\nSwAV (ResNet-50)\n3.44 Â± 0.11\n0.60 Â± 0.04\n6.77 Â± 0.11\n16.20 Â± 0.54\nSimCLRv2 (ResNet-50)\n3.67 Â± 0.22\n0.63 Â± 0.02\n6.72 Â± 0.09\n18.75 Â± 0.24\nBigBiGAN (RevNet-50 4Ã—)\n2.30 Â± 0.03 0.116 Â± 0.001\n1.75 Â± 0.03\n14.93 Â± 0.22\nInfoMin (ResNeXt-152)\n6.53 Â± 0.19 1.59 Â± 0.04 12.49 Â± 0.13 24.97 Â± 0.24\nSimCLRv2 (ResNet-152, SK)\n5.34 Â± 0.20\n1.15 Â± 0.07\n9.24 Â± 0.23\n22.08 Â± 0.17\nSimCLRv2 (ResNet-152 3Ã—, SK)\n4.20 Â± 0.19\n1.00 Â± 0.08\n8.62 Â± 0.16\n17.53 Â± 0.27\nSimCLRv2 (ResNet-152, SK, 1.5Ã— over.) 6.47 Â± 0.07\n1.32 Â± 0.05\n9.46 Â± 0.08\n23.62 Â± 0.28\nResNet-152 (super.)\n14.36 Â± 1.80\n6.09 Â± 1.20\n23.93 Â± 0.26\n32.60 Â± 2.55\nIG-ResNeXt-101 32Ã—48d (super.)\n25.25 Â± 0.46 14.03 Â± 0.18\n36.30 Â± 0.11\n44.72 Â± 0.24\nEï¬ƒcientNet-L2 (super.)\n7.70 Â± 0.57\n2.07 Â± 0.30\n17.78 Â± 0.44\n22.60 Â± 0.61\n8\nE. Zheltonozhskii et al.\n16\n64\n256\n1024\nPCA dimensions\n0\n10\n20\n30\n40\nTop-1 accuracy, %\n0\n10\n20\nAdjusted Rand index, %\n(a) Accuracy and ARI as a function of dimen-\nsions after dimensional reduction.\n1000\n2000\n3000\n4000\n5000\nNumber of clusters\n0\n20\n40\nTop-1 accuracy, %\n0\n10\n20\nAdjusted Rand index, %\n(b) Accuracy and ARI as a function of number\nof clusters.\nFig. 2: Ablation study for the best-performing model, SimCLRv2 (ResNet-152, SK).\n3.1\nAblation study\nDimensionality reduction Fig. 2a shows the eï¬€ect of the number of dimensions\nused for clustering. As expected, an increasing number of dimensions provide\ndiminishing returns and might harm the results for more than 1024 dimensions.\nOverclustering Since some classes in ImageNet may contain fairly diï¬€erent images,\nincreasing the number of clusters beyond 1000 improves not only accuracy (since\nthis metric uses real labels, its calculation inevitably involves passing information),\nbut also ARI, as shown in Fig. 2b. For that reason, we add 1.5Ã— overclustering\nversion of the best-performing model to comparison.\n4\nConclusion\nIn this paper, we study the applications of self-supervised learning for unsupervised\nclassiï¬cation. We establish competitive baselines by just applying PCA dimensional\nreduction and k-means clustering to features extracted by existing self-supervised\nmethods. Thanks to a practice of publishing both code and pre-trained models,\nwe were able to evaluate multiple state-of-the-art approaches and achieve as\nhigh as 39% accuracy on ImageNet in an unsupervised manner and 46% with\noverclustering.\nAlso, we propose an unsupervised clustering of extracted features as an ad-\nditional way to evaluate self-supervised training approaches, along with linear\nevaluation and transfer learning.\nFinally, we raise several issues and possible directions for future work. First, the\nquestion of whether severe underperformance of models with higher-dimensional\nfeature space, such as ResNet 3Ã—, remains open. Is it the weakness of the proposed\nclustering method or rather a property of the model? Can the reduction of the\ndimension of the embeddings improve performance on other tasks?\nSecond, the modelsâ€™ poor performance transferred to ObjectNet, even among\nsupervised models, with a prominent exception of BigBiGAN, is of great interest. Is it\npossible to achieve high performance on ObjectNet and ImageNet simultaneously, at\nSelf-Supervised Learning for Large-Scale Unsupervised Image Clustering\n9\nleast at the linear classiï¬cation level? What is the reason BigBiGAN is the only model\nshowing better-than-random results on ObjectNet? What is performance on other\nImageNet-related datasets such as ReaL labels (Beyer et al., 2020), ImageNetV2\n(Recht et al., 2019), ImageNet-R (Hendrycks et al., 2020), etc.?\nLast, can the approach itself be improved? Can we take into account the method\nduring the self-supervised training without signiï¬cant performance degradation in\nother tasks? What is the better approach for dimensional reduction and clustering\nitself? What is the eï¬€ect of augmentation in the clustering training phase?\nWe hope this paper will raise an interest in self-supervised approach to large-\nscale image clustering.\nBibliography\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi.\nSelf-labelling\nvia simultaneous clustering and representation learning.\narXiv preprint\narXiv:1911.05371, 2019. URL https://arxiv.org/abs/1911.05371. (cited on\npp. 2 and 6)\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations\nby maximizing mutual information across views. arXiv preprint arXiv:1906.00910,\n2019. URL https://arxiv.org/abs/1906.00910. (cited on p. 2)\nAndrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang,\nDan Gutfreund, Josh Tenenbaum, and Boris Katz. ObjectNet: A large-scale\nbias-controlled dataset for pushing the limits of object recognition models.\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 32, pages\n9453â€“9463. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/\n9142-objectnet-a-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-re\n(cited on pp. 3 and 5)\nLucas Beyer, Olivier J HÃ©naï¬€, Alexander Kolesnikov, Xiaohua Zhai, and AÃ¤ron\nvan den Oord. Are we done with ImageNet? arXiv preprint arXiv:2006.07159,\n2020. URL https://arxiv.org/abs/2006.07159. (cited on p. 9)\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep\nclustering for unsupervised learning of visual features. In The European Conference\non Computer Vision (ECCV), September 2018a. (cited on p. 2)\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.\nDeep clustering for unsupervised learning of visual features.\nIn Proceed-\nings of the European Conference on Computer Vision (ECCV), September\n2018b.\nURL https://openaccess.thecvf.com/content_ECCV_2018/html/\nMathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.html. (cited on\np. 4)\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and\nArmand Joulin. Unsupervised learning of visual features by contrasting cluster\nassignments. arXiv preprint arXiv:2006.09882, 2020. URL https://arxiv.org/\nabs/2006.09882. (cited on pp. 2 and 5)\nMark Chen, Alec Radford, Rewon Child, Jeï¬€rey Wu, Heewoo Jun, David Luan, and\nIlya Sutskever. Generative pretraining from pixels. In Proceedings of Machine\nLearning and Systems 2020, pages 10466â€“10478. 2020a. URL https://openai.\ncom/blog/image-gpt/. (cited on p. 2)\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoï¬€rey Hinton. A simple\nframework for contrastive learning of visual representations. arXiv preprint\narXiv:2002.05709, 2020b. URL https://arxiv.org/abs/2002.05709. (cited\non p. 2)\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoï¬€rey\nHinton. Big self-supervised models are strong semi-supervised learners. arXiv\nSelf-Supervised Learning for Large-Scale Unsupervised Image Clustering\n11\npreprint arXiv:2006.10029, 2020c. URL https://arxiv.org/abs/2006.10029.\n(cited on pp. 2 and 5)\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with\nmomentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020d. URL\nhttps://arxiv.org/abs/2003.04297. (cited on pp. 2 and 5)\nDavid F. Crouse. On implementing 2D rectangular assignment algorithms. IEEE\nTransactions on Aerospace and Electronic Systems, 52(4):1679â€“1696, 2016. URL\nhttps://ieeexplore.ieee.org/document/7738348. (cited on p. 3)\nCarl Doersch, Abhinav Gupta, and Alexei A. Efros.\nUnsupervised visual\nrepresentation learning by context prediction.\nIn Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), December 2015.\nURL\nhttps://openaccess.thecvf.com/content_iccv_2015/html/Doersch_\nUnsupervised_Visual_Representation_ICCV_2015_paper.html. (cited on\np. 2)\nJeï¬€Donahue and Karen Simonyan. Large scale adversarial representation learning.\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett,\neditors, Advances in Neural Information Processing Systems 32, pages 10542â€“\n10552. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/\n9240-large-scale-adversarial-representation-learning. (cited on pp.\n2 and 5)\nJeï¬€Donahue, Philipp KrÃ¤henbÃ¼hl, and Trevor Darrell. Adversarial feature learning.\narXiv preprint arXiv:1605.09782, 2016.\nURL https://arxiv.org/abs/1605.\n09782. (cited on p. 2)\nAleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening\nfor self-supervised representation learning. arXiv preprint arXiv:2007.06346, 2020.\nURL https://arxiv.org/abs/2007.06346. (cited on p. 2)\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation\nlearning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.\nURL https://arxiv.org/abs/1803.07728. (cited on p. 2)\nJean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre H\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-\nhan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu,\nRÃ©mi Munos, and Michal Valko.\nBootstrap your own latent: A new ap-\nproach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. URL\nhttps://arxiv.org/abs/2006.07733. (cited on p. 2)\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum\ncontrast for unsupervised visual representation learning, 2019. URL https:\n//arxiv.org/abs/1911.05722. (cited on p. 2)\nOlivier J. HÃ©naï¬€, Aravind Srinivas, Jeï¬€rey De Fauw, Ali Razavi, Carl Doersch,\nSM Eslami, and AÃ¤ron van den Oord. Data-eï¬ƒcient image recognition with\ncontrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019. URL https:\n//arxiv.org/abs/1905.09272. (cited on p. 2)\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan\nDorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob\nSteinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis\n12\nE. Zheltonozhskii et al.\nof out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020. URL\nhttps://arxiv.org/abs/2006.16241. (cited on p. 9)\nLawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classi-\nï¬cation, 2(1):193â€“218, 1985. URL https://link.springer.com/article/10.\n1007%2FBF01908075. (cited on p. 4)\nZhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou.\nVariational deep embedding: An unsupervised and generative approach to clus-\ntering. arXiv preprint arXiv:1611.05148, 2016. URL https://arxiv.org/abs/\n1611.05148. (cited on p. 3)\nDahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon. Learning image\nrepresentations by completing damaged jigsaw puzzles. In 2018 IEEE Winter\nConference on Applications of Computer Vision (WACV), pages 793â€“802. IEEE, 2018.\nURL https://ieeexplore.ieee.org/abstract/document/8354196.\n(cited\non p. 2)\nAlexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised\nvisual representation learning.\nIn Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR), June 2019.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2019/html/Kolesnikov_\nRevisiting_Self-Supervised_Visual_Representation_Learning_CVPR_\n2019_paper.html. (cited on p. 2)\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research\nlogistics quarterly, 2(1-2):83â€“97, 1955. (cited on p. 3)\nTarald O Kvalseth. Entropy and correlation: Some comments. IEEE Transactions on\nSystems, Man, and Cybernetics, 17(3):517â€“519, 1987. URL https://ieeexplore.\nieee.org/document/4309069. (cited on p. 4)\nHonglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Convolutional\ndeep belief networks for scalable unsupervised learning of hierarchical represen-\ntations. In Proceedings of the 26th annual international conference on machine\nlearning, pages 609â€“616, 2009. URL https://dl.acm.org/doi/abs/10.1145/\n1553374.1553453. (cited on p. 2)\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar\nPaluri,\nYixuan\nLi,\nAshwin\nBharambe,\nand\nLaurens\nvan\nder\nMaaten.\nExploring\nthe\nlimits\nof\nweakly\nsupervised\npretraining.\nIn\nProceed-\nings of the European Conference on Computer Vision (ECCV), September\n2018.\nURL https://openaccess.thecvf.com/content_ECCV_2018/html/\nDhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.html. (cited on\np. 5)\nWilliam M. Rand. Objective criteria for the evaluation of clustering methods. Journal\nof the American Statistical association, 66(336):846â€“850, 1971. URL https://www.\ntandfonline.com/doi/abs/10.1080/01621459.1971.10482356. (cited on p.\n4)\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do\nImageNet classiï¬ers generalize to ImageNet?\nIn Kamalika Chaudhuri and\nRuslan Salakhutdinov, editors, Proceedings of the 36th International Conference\non Machine Learning, volume 97 of Proceedings of Machine Learning Research,\nSelf-Supervised Learning for Large-Scale Unsupervised Image Clustering\n13\npages 5389â€“5400, Long Beach, California, USA, 09â€“15 Jun 2019. PMLR. URL\nhttp://proceedings.mlr.press/v97/recht19a.html. (cited on p. 9)\nCinjon Resnick, Zeping Zhan, and Joan Bruna. Probing the state of the art: A critical\nlook at visual representation evaluation. arXiv preprint arXiv:1912.00215, 2019.\nURL https://arxiv.org/abs/1912.00215. (cited on p. 2)\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv\npreprint arXiv:1906.05849, 2019. URL https://arxiv.org/abs/1906.05849.\n(cited on p. 2)\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip\nIsola.\nWhat makes for good views for contrastive learning.\narXiv preprint\narXiv:2005.10243, 2020. URL https://arxiv.org/abs/2005.10243. (cited on\npp. 2 and 5)\nAÃ¤ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. URL https:\n//arxiv.org/abs/1807.03748. (cited on p. 2)\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans,\nand Luc Van Gool. SCAN: Learning to classify images without labels. arXiv\npreprint arXiv:2005.12320, 2020. URL https://arxiv.org/abs/2005.12320.\n(cited on pp. 2, 3, 5, and 6)\nNguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures\nfor clusterings comparison: Variants, properties, normalization and correction\nfor chance. Journal of Machine Learning Research, 11(95):2837â€“2854, 2010. URL\nhttp://jmlr.org/papers/v11/vinh10a.html. (cited on p. 4)\nRoss Wightman.\nPyTorch Image Models, 2020.\nURL https://github.com/\nrwightman/pytorch-image-models. (cited on p. 5)\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for\nclustering analysis. arXiv preprint arXiv:1511.06335, 2015. URL https://arxiv.\norg/abs/1511.06335. (cited on p. 3)\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with\nnoisy student improves ImageNet classiï¬cation. arXiv preprint arXiv:1911.04252,\n2019. URL https://arxiv.org/abs/1911.04252. (cited on p. 5)\nMang Ye, Xu Zhang, Pong C. Yuen, and Shih-Fu Chang.\nUnsupervised em-\nbedding learning via invariant and spreading instance feature.\nIn The\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), June\n2019.\nURL https://openaccess.thecvf.com/content_CVPR_2019/html/\nYe_Unsupervised_Embedding_Learning_via_Invariant_and_Spreading_\nInstance_Feature_CVPR_2019_paper.html. (cited on p. 2)\nRichard Zhang, Phillip Isola, and Alexei A. Efros. Colorful image colorization.\nIn European Conference on Computer Vision, pages 649â€“666. Springer, 2016.\nURL https://link.springer.com/chapter/10.1007/978-3-319-46487-9_\n40. (cited on p. 2)\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2020-08-24",
  "updated": "2020-11-09"
}