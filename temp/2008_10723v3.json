{
  "id": "http://arxiv.org/abs/2008.10723v3",
  "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries",
  "authors": [
    "Arpit Narechania",
    "Arjun Srinivasan",
    "John Stasko"
  ],
  "abstract": "Natural language interfaces (NLIs) have shown great promise for visual data\nanalysis, allowing people to flexibly specify and interact with visualizations.\nHowever, developing visualization NLIs remains a challenging task, requiring\nlow-level implementation of natural language processing (NLP) techniques as\nwell as knowledge of visual analytic tasks and visualization design. We present\nNL4DV, a toolkit for natural language-driven data visualization. NL4DV is a\nPython package that takes as input a tabular dataset and a natural language\nquery about that dataset. In response, the toolkit returns an analytic\nspecification modeled as a JSON object containing data attributes, analytic\ntasks, and a list of Vega-Lite specifications relevant to the input query. In\ndoing so, NL4DV aids visualization developers who may not have a background in\nNLP, enabling them to create new visualization NLIs or incorporate natural\nlanguage input within their existing systems. We demonstrate NL4DV's usage and\ncapabilities through four examples: 1) rendering visualizations using natural\nlanguage in a Jupyter notebook, 2) developing a NLI to specify and edit\nVega-Lite charts, 3) recreating data ambiguity widgets from the DataTone\nsystem, and 4) incorporating speech input to create a multimodal visualization\nsystem.",
  "text": "© 2020 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The ﬁnal version of this record is available at: 10.1109/TVCG.2020.3030378\nNL4DV: A Toolkit for Generating Analytic Speciﬁcations\nfor Data Visualization from Natural Language Queries\nArpit Narechania*, Arjun Srinivasan*, and John Stasko\nCreate a histogram showing distribution of \nIMDB ratings\nShow average gross across genres for science \nfiction and fantasy movies\nVisualize rating and budget\na\nb\nNL4DV\nc\nAttribute(s)\nVisualization(s)\nTask(s)\nAttribute(s)\nVisualization(s)\nTask(s)\nAttribute(s)\nVisualization(s)\nTask(s)\nIMDB Rating\nDistribution\nHistogram\n- Derived Value (Attribute = Worldwide \nGross; Operation = AVG)\n- Filter (Attribute = Creative Type; Values \n= Science Fiction, Fantasy;)\nBar Chart\nIMDB Rating, Content Rating, Rotten Tomatoes \nRating, Production Budget\n- Correlation (Attributes = [IMDB Rating, Production \nBudget], [Rotten Tomatoes Rating, Production Budget])\n- Derived Value (Attributes = Production Budget; \nOperation = AVG)\nScatterplot, Bar Chart\nWorldwide Gross, Genre,\nCreative Type\nMovies\nFig. 1: Examples illustrating the ﬂexibility of natural language queries for specifying data visualizations. NL4DV processes\nall three query variations, inferring explicit , partially explicit or ambiguous , and implicit references to attributes, tasks, and\nvisualizations. The corresponding visualizations suggested by NL4DV in response to the individual queries are also shown.\nAbstract— Natural language interfaces (NLIs) have shown great promise for visual data analysis, allowing people to ﬂexibly specify\nand interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation\nof natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present\nNL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and\na natural language query about that dataset. In response, the toolkit returns an analytic speciﬁcation modeled as a JSON object\ncontaining data attributes, analytic tasks, and a list of Vega-Lite speciﬁcations relevant to the input query. In doing so, NL4DV aids\nvisualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural\nlanguage input within their existing systems. We demonstrate NL4DV’s usage and capabilities through four examples: 1) rendering\nvisualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data\nambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.\nIndex Terms—Natural Language Interfaces; Visualization Toolkits;\n1\nINTRODUCTION\nNatural language interfaces (NLIs) for visualization are becoming in-\ncreasingly popular in both academic research (e.g., [13,22,46,52,69])\nas well as commercial software [34,55]. At a high-level, visualization\nNLIs allow people to pose data-related queries and generate visual-\nizations in response to those queries. To generate visualizations from\nnatural language (NL) queries, NLIs ﬁrst model the input query in\nterms of data attributes and low-level analytic tasks [1,9] (e.g., ﬁlter,\ncorrelation, trend). Using this information, the systems then determine\nwhich visualizations are most suited as a response to the input query.\nWhile NLIs provide ﬂexibility in posing data-related questions, inherent\ncharacteristics of NL such as ambiguity and underspeciﬁcation make\nimplementing NLIs for data visualization a challenging task.\n• Arpit Narechania, Arjun Srinivasan, and John Stasko are from the Georgia\nInstitute of Technology, Atlanta, GA (USA). E-mail: {arpitnarechania,\narjun010}@gatech.edu, stasko@cc.gatech.edu.\n• *Authors contributed equally.\nManuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication\nxx xxx. 201x; date of current version xx xxx. 201x. For information on\nobtaining reprints of this article, please send e-mail to: reprints@ieee.org.\nDigital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx\nTo create visualization NLIs, besides implementing a graphical user\ninterface (GUI) and rendering views, visualization system developers\nmust also implement a natural language processing (NLP) module to\ninterpret queries. Although there exist tools to support GUI and visu-\nalization design (e.g., D3.js [6], Vega-Lite [43]), developers currently\nhave to implement custom modules for query interpretation. However,\nfor developers without experience with NLP techniques and toolkits\n(e.g., NLTK [31], spaCy [21]), implementing this pipeline is non-trivial,\nrequiring them to spend signiﬁcant time and effort in learning and\nimplementing different NLP techniques.\nConsider the spectrum of queries in Figure 1 issued to create vi-\nsualizations in the context of an IMDb movies dataset with different\nattributes including the Û Worldwide Gross, ~ Genre, and \u0011 Release\nYear, among others (for consistency, we use this movies dataset for\nexamples throughout this paper). The query “Create a histogram show-\ning distribution of IMDB ratings” (Figure 1a) explicitly refers to a data\nattribute (IMDB Rating), a low-level analytic task (Distribution), and\nrequests a speciﬁc visualization type (Histogram). This is an ideal inter-\npretation scenario from a system standpoint since the query explicitly\nlists all components required to generate a visualization.\nOn the other hand, the second query “Show average gross across\ngenres for science ﬁction and fantasy movies” (Figure 1b) does not\nexplicitly state the visualization type or the attribute Creative Type.\n1\narXiv:2008.10723v3  [cs.HC]  23 Nov 2020\nInstead, it explicitly references the attributes Worldwide Gross and\nGenre through ‘gross’ and ‘genres’, and implicitly refers to the Creative\nType through the values ‘science ﬁction’ and ‘fantasy’. Furthermore,\nby specifying data values for the Creative Type attribute and the word\n‘average,’ the query also mentions two intended analytic tasks: Filtering\nand computing Derived Values, respectively. This second query is\nmore challenging since it requires the system to implicitly infer one of\nthe attributes and then determine the visualization type based on the\nidentiﬁed attributes and tasks.\nFinally, the third query “Visualize rating and budget” (Figure 1c) is\neven more challenging to interpret since it neither explicitly states the\ndesired visualization type nor the intended analytic task. Furthermore,\nwhile it explicitly references one attribute (Production Budget through\n‘budget’), the reference to the second attribute is ambiguous (‘rating’\ncan map to IMDB Rating, Content Rating, or Rotten Tomatoes Rating).\nTo accommodate such query variations, visualization NLIs employ\nsophisticated NLP techniques (e.g., dependency parsing, semantic word\nmatching) to identify relevant information from the query and build\nupon visualization concepts (e.g., analytic tasks) and design principles\n(e.g., choosing graphical encodings based on attribute types) to generate\nappropriate visualizations. For instance, given the query in Figure 1b,\nafter detecting the data attributes and analytic tasks, a visualization\nNLI should select a visualization (e.g., bar chart) that is well-suited to\nsupport the task of displaying Derived Values (average) for Worldwide\nGross (a quantitative attribute) across different Genres (a nominal\nattribute). Similarly, in the scenario in Figure 1c, a NLI must ﬁrst detect\nambiguities in the input query attributes, determine the visualizations\nsuited to present those attribute combinations (e.g., scatterplot for two\nquantitative attributes), and ultimately infer the analytic tasks based\non those attributes and visualizations (e.g., a scatterplot may imply the\nuser is interested in ﬁnding correlations).\nTo support prototyping NLIs for data visualization, we contribute the\nNatural Language-Driven Data Visualization (NL4DV) toolkit. NL4DV\nis a Python package that developers can initialize with a tabular dataset.\nOnce initialized, NL4DV processes subsequent NL queries about the\ndataset, inferring data attributes and analytic tasks from those queries.\nAdditionally, using built-in mappings between attributes, tasks, and\nvisualizations, NL4DV also returns an ordered list of Vega-Lite spec-\niﬁcations relevant to those queries. By providing a high-level API to\ntranslate NL queries to visualizations, NL4DV abstracts out the core\ntask of interpreting NL queries and provides task-based visualization\nrecommendations as plug-and-play functionality. Using NL4DV, de-\nvelopers can create new visualization NLIs as well as incorporate NL\nquerying capabilities into their existing visualization systems.\nIn this paper, we discuss NL4DV’s design goals and describe how\nthe toolkit infers data attributes, analytic tasks, and visualization spec-\niﬁcations from NL queries. Furthermore, we formalize the inferred\ninformation into a JSON-based analytic speciﬁcation that can be pro-\ngrammatically parsed by visualization developers. Finally, through\nexample applications, we showcase how this formalization can help:\n1) implement visualization NLIs from scratch, 2) incorporate NL in-\nput into an existing visualization system, and 3) support visualization\nspeciﬁcation in data science programming environments.\nTo support development of future systems, we also provide NL4DV\nand the described applications as open-source software available at:\nhttps://nl4dv.github.io/nl4dv/\n2\nRELATED WORK\n2.1\nNatural Language Interfaces for Data Visualization\nIn 2001, Cox et al. [10] presented an initial prototype of a NLI that\nsupported using well-structured commands to specify visualizations.\nSince then, given the advent of NL understanding technology and NLIs\nfor databases (e.g., [4,17,20,29,39,40,60,71]), there has been a surge of\nNLIs for data visualization [13,22,24,25,27,46,47,49,50,52,54,69], es-\npecially in recent years. Srinivasan and Stasko [51] summarize a subset\nof these NLIs, characterizing systems based on their supported capa-\nbilities including visualization-focused capabilities (e.g., specifying or\ninteracting with visualizations), data-focused capabilities (e.g., com-\nputationally answering questions about a dataset), and system control-\nfocused capabilities (e.g., augmenting graphical user interface actions\nlike moving windows with NL). Along these lines, NL4DV’s current\nfocus is primarily to support visualization speciﬁcation. With this scope\nin mind, below we highlight systems that serve as the motivation for\nNL4DV’s development and are most relevant to our work.\nArticulate [54] is a visualization NLI that allows people to generate\nvisualizations by deriving mappings between tasks and data attributes in\nuser queries. DataTone [13] uses a combination of lexical, constituency,\nand dependency parsing to let people specify visualizations through NL.\nFurthermore, detecting ambiguities in the input query, DataTone lever-\nages mixed-initiative interaction to resolve these ambiguities through\nGUI widgets such as dropdown menus. FlowSense [69] uses seman-\ntic parsing techniques to support NL interaction within a dataﬂow\nsystem, allowing people to specify and connect components without\nlearning the intricacies of operating a dataﬂow system. Eviza [46]\nincorporates a probabilistic grammar-based approach and a ﬁnite state\nmachine to allow people to interact with a given visualization. Ex-\ntending Eviza’s capabilities and incorporating additional pragmatics\nconcepts, Evizeon [22] allows both specifying and interacting with\nvisualizations through standalone and follow-up utterances. The ideas\nin Eviza and Evizeon were also used to design the Ask Data feature in\nTableau [55]. Ask Data internally uses Arklang [47], an intermediate\nlanguage developed to describe NL queries in a structured format that\nTableau’s VizQL [53] can parse to generate visualizations.\nThe aforementioned systems all present different interfaces and ca-\npabilities, supporting NL interaction through grammar- and/or lexical-\nparsing techniques. A commonality in their underlying NLP pipeline,\nhowever, is the use of data attributes and analytic tasks (e.g., correlation,\ndistribution) to determine user intent for generating the system response.\nBuilding upon this central observation and prior system implementa-\ntions (e.g., string similarity metrics and thresholds [13,46,52], parsing\nrules [13,25,69]), NL4DV uses a combination of lexical and depen-\ndency parsing-based techniques to infer attributes and tasks from NL\nqueries. However, unlike previous systems that implement custom NLP\nengines and languages that translate NL queries into system actions,\nwe develop NL4DV as an interface-agnostic toolkit. In doing so, we\nformalize attributes and tasks inferred from a NL query into a structured\nJSON object that can be programmatically parsed by developers.\n2.2\nVisualization Toolkits and Grammars\nFundamentally, our research falls under the broad category of user inter-\nface toolkits [28,37,38]. As such, instead of presenting a single novel\ntechnique or interface, we place emphasis on reducing development\nviscosity, lowering development skill barriers, and enabling replication\nand creative exploration. Within visualization research, there exist a\nnumber of visualization toolkits with similar goals that particularly\nfocus on easing development effort for specifying and rendering visual-\nizations. Examples of such toolkits include Prefuse [19], Protovis [5],\nand D3 [6]. With the advent of visualizations on alternative platforms\nlike mobile devices and AR/VR, a new range of toolkits are also being\ncreated to assist visualization development on these contemporary plat-\nforms. For instance, EasyPZ.js [45] supports incorporating navigation\ntechniques (pan and zoom) in web-based visualizations across both\ndesktops and mobile devices. Toolkits like DXR [48] enable develop-\nment of expressive and interactive visualizations in Unity [57] that can\nbe deployed in AR/VR environments. NL4DV extends this line of work\non toolkits for new modalities and platforms by making it easier for\nvisualization system developers to interpret NL queries without having\nto learn or implement NLP techniques.\nBesides toolkits that aid programmatically creating visualizations,\nresearchers have also formulated visualization grammars that provide\na high-level abstraction for building visualizations to reduce software\nengineering know-how [18]. Along these lines, based on the Grammar\nof Graphics [62], more recently developed visualization grammars\nsuch as Vega [44] and Vega-Lite [43] support visualization design\nthrough declarative speciﬁcations, enabling rapid visualization design\nand prototyping. NL4DV’s primary goal is to return visualizations\nin response to NL queries. To enable this, in addition to a structured\nrepresentation of attributes and tasks inferred from a query, NL4DV\n2\n© 2020 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The ﬁnal version of this record is available at: 10.1109/TVCG.2020.3030378\nalso needs to return visualization speciﬁcations most relevant to an input\nquery. Given the conciseness of Vega-Lite and its growing usage in both\nweb-based visualization systems and Python-based visual data analysis,\nNL4DV uses Vega-Lite as its underlying visualization grammar.\n2.3\nNatural Language Processing Toolkits\nToolkits like NLTK [31], Stanford CoreNLP [33] and NER [11], and\nspaCy [21] help developers perform NLP tasks such as part-of-speech\n(POS) tagging, entity recognition, and dependency parsing, among\nothers. However, since these are general-purpose toolkits, to imple-\nment visualization NLIs, developers need to learn to use the toolkit\nand also understand the underlying NLP techniques/concepts (e.g.,\nknowing which dependency paths to traverse while parsing queries,\nunderstanding semantic similarity metrics). Furthermore, to implement\nvisualization systems, developers need to write additional code to con-\nvert the output from NLP toolkits into visualization-relevant concepts\n(e.g., attributes and values for applying data ﬁlters), which can be both\ncomplex and tedious. Addressing these challenges, NL4DV internally\nuses NLTK [31], Stanford CoreNLP [33], and spaCy [21] but provides\nan API that encapsulates and hides the underlying NLP implementation\ndetails. This allows visualization developers to focus more on front-end\ncode pertaining to the user interface and interactions while invoking\nhigh-level functions to interpret NL queries.\n3\nNL4DV OVERVIEW\nFigure 2 presents an overview of a typical pipeline for implementing\nNLIs that generate visualizations in response to NL queries. At a\nhigh-level, once an input query is collected through a User Interface,\na Query Processor infers relevant information such as data attributes\nand analytic tasks from the input query. This information is then\npassed to a Visualization Recommendation Engine which generates a\nlist of visualizations speciﬁcations relevant to the input query. These\nspeciﬁcations are ﬁnally rendered through a library (e.g., D3 [6]) of the\ndeveloper’s choice. In the context of this pipeline, NL4DV provides\na high-level API for processing NL queries and generating Vega-Lite\nspeciﬁcations relevant to the input query. Developers can choose to\ndirectly render the Vega-Lite speciﬁcations to create views (e.g., using\nVega-Embed [59]) or use the attributes and tasks inferred by NL4DV\nto make custom changes to their system’s interface.\n3.1\nDesign Goals\nFour key design goals drove the development of NL4DV. We compiled\nthese goals based on a review of design goals and system implemen-\ntations of prior visualization NLIs [13, 22, 46, 52, 54, 69] and recent\ntoolkits for supporting visualization development on new platforms and\nmodalities (e.g, [45,48]).\nDG1. Minimize NLP learning curve. NL4DV’s primary target users\nare developers without a background or experience in working with\nNLP techniques. Correspondingly, it was important to make the learn-\ning curve as ﬂat as possible. In other words, we wanted to enable\ndevelopers to use the output of NL4DV without having to spend time\nlearning about the mechanics of how information is extracted from\nNL queries. In terms of toolkit design, this consideration translated to\nproviding high-level functions for interpreting NL queries and design-\ning a response structure that was optimized for visualization system\ndevelopment by emphasizing visualization-related information such as\nanalytic tasks (e.g., ﬁlter, correlation) and data attributes and values.\nDG2. Generate modularized output and support integration with\nexisting system components. By default, NL4DV recommends Vega-\nLite speciﬁcations in response to NL queries. However, a developer\nmay prefer rendering visualizations using a different library such as\nD3 or may want to use a custom visualization recommendation engine\n(e.g., [30,36,65]), only leveraging NL4DV to identify attributes and/or\ntasks in the input query. Supporting this goal required us to ensure that\nNL4DV’s output was modularized (allowing developers to choose if\nthey wanted attributes, tasks, and/or visualizations) and that developers\ndo not have to signiﬁcantly modify their existing system architecture\nUser \nInterface\nRendering\nVisualization \nRecommendation\nQuery\nProcessing\nAttributes, \nTasks\nNL4DV\n• Data Attributes\n• Analytic Tasks\n• Vega-Lite Specifications\n+\n“Query”\nFig. 2: An overview of steps to generate visualizations based on NL\nqueries. NL4DV encapsulates the query processing and visualization\nrecommendation components, providing abstract functions to support\ntheir functionality. Once initialized with a dataset, NL4DV parses input\nNL queries and returns relevant information (in terms of data attributes\nand analytic tasks) and an ordered list of Vega-Lite speciﬁcations.\nto use NL4DV. In terms of toolkit design, in addition to using a stan-\ndardized grammar for visualizations (in our case, Vega-Lite), these\nconsiderations translated to devising a formalized representation of\ndata attributes and analytic tasks that developers can programmatically\nparse to link NL4DV’s output to other components in their system.\nDG3. Highlight inference type and ambiguity. NL is often under-\nspeciﬁed and ambiguous. In other words, input queries may only\ninclude partial references to data attributes or may implicitly refer to\nintended tasks and visualizations (e.g., Figure 1b, c) [56]. Besides\naddressing these challenges from an interpretation standpoint, it was\nalso important to make developers aware of the resulting uncertainty\nin NL4DV’s output so they can choose to use/discard the output and\nprovide appropriate visual cues (e.g., ambiguity widgets [13]) in their\nsystems’ interface. In terms of toolkit design, this translated to struc-\nturing NL4DV’s output so it indicates whether information is inferred\nthrough an explicit (e.g., query substring matches an attribute name)\nor implicit (e.g., query refers to a data attribute through the attribute’s\nvalues) reference, and highlights potential ambiguities in its response\n(e.g., two or more attributes map to the same word in an input query as\nin Figure 1c).\nDG4. Support adding aliases and overriding toolkit defaults. Vi-\nsualization systems are frequently used to analyze domain-speciﬁc\ndatasets (e.g., sales, medical records, sports). Given a domain, it is\ncommon for data attributes to have abbreviations or aliases (e.g., “GDP”\nfor Gross domestic product, “investment” for Capital), or values that\nare unique to the dataset (e.g., the letter “A” can refer to a value in\na course grade dataset, but would be considered as a stopword and\nignored by most NLP algorithms by default). In terms of toolkit design,\nthese dataset-speciﬁc considerations translated to providing develop-\ners with helper functions to specify aliases or special word lists that\nNL4DV should consider/exclude for a given dataset.\n4\nNL4DV DESIGN AND IMPLEMENTATION\nIn this section, we detail NL4DV’s design and implementation, high-\nlighting key functions and describing how the toolkit interprets NL\nqueries. We defer discussing example applications developed using\nNL4DV to the following section.\nListing 1 shows the basic Python code for using NL4DV. Given a\ndataset (as a CSV, TSV, or JSON) and a query string, with a single func-\ntion call\nanalyze query(query) , NL4DV infers attributes, tasks, and\nvisualizations, returning them as a JSON object (DG1). Speciﬁcally,\nNL4DV’s response object has an attributeMap composed of the in-\nferred dataset attributes, a taskMap composed of the inferred analytic\ntasks, and a visList, a list of visualization speciﬁcations relevant\nto the input query. By providing attributes, tasks, and visualizations\nas separate keys in the response object, NL4DV allows developers to\nselectively extract and use parts of its output (DG2).\n4.1\nData Interpretation\nOnce initialized with a dataset (Listing 1, line 2), NL4DV iterates\nthrough the underlying data item values to infer metadata including the\n3\n1\nfrom nl4dv import NL4DV\n2\nnl4dv_instance = NL4DV(data_url=\"movies.csv\")\n3\nresponse = nl4dv_instance.analyze_query(\"Show the\nrelationship between budget and rating for Action\nand Adventure movies that grossed over 100M\")\n,→\n,→\n4\nprint(response)\n{\n\"attributeMap\": { ... },\n\"taskMap\": { ... },\n\"visList\": [ ... ]\n}\nListing 1: Python code illustrating NL4DV’s basic usage involving\ninitializing NL4DV with a dataset (line 2) and analyzing a query string\n(line 3). The high-level structure of NL4DV’s response is also shown.\nattribute types (Û Quantitative, ~ Nominal, 9 Ordinal, \u0011 Temporal)\nalong with the range and domain of values for each attribute. This\nattribute metadata is used when interpreting queries to infer appropriate\nanalytic tasks and generate relevant visualization speciﬁcations.\nSince NL4DV uses data values to infer attribute types, it may make\nerroneous interpretations. For example, a dataset may have the attribute\nDay with values in the range [1,31]. Detecting a range of integer values,\nby default, NL4DV will infer Day as a quantitative attribute instead\nof temporal. This misinterpretation can lead to NL4DV making poor\ndesign choices when selecting visualizations based on the inferred at-\ntributes (e.g., a quantitative attribute may result in a histogram instead of\na line chart). To overcome such issues caused by data quality or dataset\nsemantics, NL4DV allows developers to verify the inferred metadata\nusing get metadata() . This function returns a hash map of attributes\nalong with their inferred metadata. If they notice errors, developers can\nuse other helper functions (e.g.,\nset attribute type(attribute,type) )\nto override the default interpretation (DG4).\n4.2\nQuery Interpretation\nTo generate visualizations in response to a query, visualization NLIs\nneed to identify informative phrases in the query that map to relevant\nconcepts like data attributes and values, analytic tasks, and visualization\ntypes, among others. Figure 3 shows the query in Listing 1 “Show\nthe relationship between budget and rating for Action and Adventure\nmovies that grossed over 100M” with such annotated phrases (we use\nthis query as a running example throughout this section to describe\nNL4DV’s query interpretation strategy). To identify relevant phrases\nand generate the attributeMap, taskMap, and visList, NL4DV\nperforms four steps: 1) query parsing, 2) attribute inference, 3) task\ninference, and 4) visualization speciﬁcation generation. Figure 4 gives\nan overview of NL4DV’s underlying architecture. Below we describe\nthe individual query interpretation steps (task inference is split into two\nsteps to aid explanation) and summarize the pipeline in Figure 5.\n4.2.1\nQuery Parsing\nThe query parser runs a series of NLP functions on the input string to\nextract details that can be used to detect relevant phrases. In this step,\nNL4DV ﬁrst preprocesses the query to convert any special symbols\nor characters into dataset-relevant values (e.g., converting 100M to\nthe number 100000000). Next, the toolkit identiﬁes the POS tags for\neach token (e.g., NN: Noun, JJ: Adjective, CC: Coordinating Con-\njunction) using Stanford’s CoreNLP [33]. Furthermore, to understand\nShow the relationship between budget and rating for Action\nand Adventure movies that grossed over 100M.\nTask Keyword\nAttributes\nData Values\nFilter Expression\nFig. 3: An illustration of query phrases that NL4DV identiﬁes while\ninterpreting NL queries.\nQuery\nParser\nAttribute\nIdentifier\nTask\nIdentifier\nVisualization Spec. \nGenerator\n“Input\nQuery”\nOutput\nJSON\nAliases\nParsing rules\n& Keywords\nFig. 4: NL4DV’s architecture. The arrows indicate the ﬂow of informa-\ntion between different modules.\nthe relationship between different phrases in the query, NL4DV uses\nCoreNLP’s dependency parser to create a dependency tree. Then, with\nthe exception of conjunctive/disjunctive terms (e.g., ‘and’, ‘or’) and\nsome prepositions (e.g., ‘between’, ‘over’) and adverbs (e.g., ‘except’,\n‘not’), NL4DV trims the input query by removing all stop words and\nperforms stemming (e.g., ‘grossed’ →‘gross’). Lastly, the toolkit\ngenerates all N-grams from the trimmed query string. The output from\nthe query parser (POS tags, dependency tree, N-grams) is shown in\nFigure 5a and is used internally by NL4DV during the remaining stages\nof query interpretation.\n4.2.2\nAttribute Inference\nAfter parsing the input query, NL4DV looks for data attributes that are\nmentioned both explicitly (e.g., through direct references to attribute\nnames) and implicitly (e.g., through references to an attribute’s values).\nDevelopers can also conﬁgure aliases (e.g., ‘Investment’ for Production\nBudget) to support dataset- and domain-speciﬁc attribute references\n(DG4). To do so, developers can provide a JSON object consisting of\nattributes (as keys) and lists of aliases (as values). This object can be\npassed through the optional parameters alias map or alias map url\nwhen initializing NL4DV (Listing 1, line 2) or using the helper function\nset alias map(alias map, url=\"\") .\nTo infer attributes, NL4DV iterates through the N-grams generated\nby the query parser, checking for both syntactic (e.g., misspelled words)\nand semantic (e.g., synonyms) similarity between N-grams and a lex-\nicon composed of data attributes, aliases, and values. To check for\nsyntactic similarity, NL4DV computes the cosine similarity Simcos(i, j)\nbetween a N-gram i and a tokenized lexical entity j. The possible values\nfor Simcos(i, j) range from [0,1] with 1 indicating that strings are equiv-\nalent. For semantic similarity, the toolkit checks for the Wu-Palmer\nsimilarity score [67] Simwup(i, j) between a N-gram i and a tokenized\nlexical entry j. This score returns the distance between stemmed ver-\nsions of p and a in the WordNet graph [35], and is a value in the range\n(0,1], with higher values implying greater similarity. If Simcos(i, j) or\nSimwup(i, j) ≥0.8, NL4DV maps the N-gram i to the attribute corre-\nsponding to j, also adding the attribute as a key in the attributeMap.\nAs shown in Figure 5b, the attributeMap is structured such that\nbesides the attributes themselves, for each attribute, developers can also\nidentify: (1) query substrings that led to an attribute being detected\n(queryPhrase) along with (2) the type of reference (inferenceType),\nand (3) ambiguous matches (ambiguity). For instance, given the query\nin Figure 3, NL4DV detects the attributes Production Budget (based\non ‘budget’), Content Rating, IMDB Rating, Rotten Tomatoes Rating\n(ambiguity caused by the word ‘rating’), Worldwide Gross (based on\n‘grossed’), and Genre (based on the values ‘Action’ and ‘Adventure’).\nFurthermore, since Genre is referenced by its values, it is marked as\nimplicit whereas the other attributes are marked as explicit (DG3).\n4.2.3\nExplicit Task Inference\nAfter detecting N-grams mapping to data attributes, NL4DV checks the\nremaining N-grams for references to analytic tasks. NL4DV currently\nidentiﬁes ﬁve low-level analytic tasks [1] including four base tasks: Cor-\nrelation, Distribution, Derived Value, Trend, and a ﬁfth Filter task. We\nseparate base tasks from ﬁlter since base tasks are used to determine ap-\npropriate visualizations (e.g., correlation maps to a scatterplot) whereas\nﬁlters are applied across different types of visualizations. We focus on\nthese ﬁve tasks as a starting set since they are commonly supported in\nprior NLIs [13,24,47,54,69] and are most relevant to NL4DV’s primary\n4\n© 2020 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The ﬁnal version of this record is available at: 10.1109/TVCG.2020.3030378\nQuery\nGoal:\nExtract sub-phrases from \nthe input query and \ndetermine relationships \nbetween those phrases.\nKey Challenges:\nDependency parsing\nPOS tagging\nUnit conversion\nStemming\n1.\nProcess the input query to \nconvert units into data values \n(e.g., 100M = 100000000).\n2.\nIdentify POS tags and create a \ndependency tree.\n3.\nRemove stopwords and perform \nstemming.\n4.\nGenerate a list of tokens and N-\ngrams.\nN-grams\nAliases\nDataset-specific\nwords to check\nor ignore\nGoal:\nIdentify data attributes \nreferenced in the query.\nKey Challenges:\nSemantic and\nsyntactic matching.\nDetecting ambiguous\nand implicit attribute\nreferences.\n5.\nCompare N-grams to attributes \n(+aliases) and data values to \nidentify explicit and implicit data \nattribute references.\n6.\nAdd identified attributes along \nwith reference type and \ncorresponding query N-gram to \nattributeMap.\n7.\nIf there is ambiguity (multiple \nattribute matches) associated \nwith an attribute in step 5, also \nlist the ambiguous references.\nTokens\nDependency\nTree & Parsing\nRules\nTask keywords\nVisualizations\ninferred from\nquery attributes\nVisualization x\nTask mappings\nGoal:\nIdentify intended analytic \ntasks.\nKey Challenges:\nPopulating task\nparameters through\nrelationships\nbetween query\nphrases.\nInferring implicitly\nstated analytic tasks.\n8. Compare tokens to task \nkeywords to detect explicitly \nmentioned base tasks.\n9. Parse the dependency tree to \npopulate the taskMap with both \nfilter tasks and base tasks from \nstep 8.\n10. If only filter tasks are detected in \nstep 8, go to step 12.\n11. Use inferred visualizations from \nstep 14 to implicitly infer tasks.\nN-grams\nAttributes\nTasks\nAttribute x\nTask x\nVisualization\nmappings\nGoal:\nGenerate query-relevant \nvisualization specifications.\nKey Challenge:\nImplicitly inferring\nand ranking\nvisualizations based\non data attributes\nand/or analytic tasks.\n12. Check N-grams for explicit \nvisualization requests. If true, go \nto step 13, else go to step 14.\n13. Generate Vega-Lite specifications \ncorresponding to the requested \nchart type using the attributes \ndetected in steps 4-6 and \npopulate the visList.\n14. If base tasks are detected in step \n8, use inferred attributes & tasks \nto determine visualizations and \npopulate the visList. Else, only \nuse attributes to infer relevant \nvisualizations and go to step 11.\n{\n“inferenceType”: “implicit”,\n“attributes”: [“IMDB Rating”,  \n“Production Budget”],\n“tasks”: [“Correlation”, \n“Filter”],\n“vlSpec”: {\n“mark” : “point”,\n“encoding” : {\n…\n{\n“inferenceType”: “implicit”,\n“attributes”: [“Production \nBudget”, “Rotten Tomatoes Rating”],\n“tasks”: [“Correlation”, \n“Filter”],\n“vlSpec”: {\n“mark” : “point”,\n“encoding” : {\n…\n{\n“inferenceType”: “implicit”,   \n“attributes”: [“Content\nRating”,  “Production Budget”],\n“tasks”: [“Correlation”,\n“Filter”],\n“vlSpec”: {\n“mark” : “point”,\n“encoding” : {\n…\nQuery Parsing\nInput\nGoal & Challenges\nNL4DV Steps\nOutput\nAttribute Inference\nTask Inference\nVisualization Generation\n{\n“Production Budget”:{\n“queryPhrase”: “budget”,\n“inferenceType” : “explicit”\n} ,\n“Content Rating”:{\n“queryPhrase”: “rating”,\n“isAmbiguous”: true,\n“ambiguity”: [“IMDB Rating”,\n“Rotten Tomatoes Rating”] ,\n“inferenceType” : “explicit”\n},\n“IMDB Rating”:{\n“queryPhrase”: “rating”,\n“isAmbiguous”: true,\n“ambiguity”: [“Content Rating”, \n“Rotten Tomatoes Rating”] ,\n“inferenceType” : “explicit”\n},\n“Rotten Tomatoes Rating”:{\n“queryPhrase”: “rating”,\n“isAmbiguous”: true,\n“ambiguity”: [“Content Rating”, \n“IMDB Rating”] ,\n“inferenceType” : “explicit”\n},\n“Worldwide Gross”:{\n“queryPhrase”: “grossed”,\n“inferenceType” : “explicit” ,\n“encode” : false\n},\n“Genre”:{\n“queryPhrase”: “Action and \nAdventure”,\n“inferenceType” : “implicit”,\n“encode” : false\n}\n}\n{\n“Correlation”: [\n{\n“attributes”: [“Production Budget”,\n“IMDB Rating”] ,\n“inferenceType”: “explicit”,\n},\n{\n“attributes”: [“Production Budget”, \n“Content Rating”] ,\n“inferenceType”: “explicit”,\n},\n{\n“attributes”: [“Production Budget”,\n“Rotten Tomatoes Rating”] ,\n“inferenceType”: “explicit”,\n}\n],\n“Filter”:[ \n{\n“attributes”: [“Major Genre”],\n“operator”: “IN”,\n“values”: [“Action”, “Adventure”],\n“inferenceType”: “explicit”,\n},\n{\n“attributes”: [“Worldwide Gross”],\n“operator”: “GT”,\n“values”: [100000000] ,\n“inferenceType”: “explicit”,\n}\n]\n}\nvisList\ntaskMap\nattributeMap\nShow the relationship between budget and rating for Action and Adventure movies that grossed over 100M.\nVB DT\nNN\nIN\nNN\nCC NN IN NNP CC\nNNP\nNNS WDT VBD IN CD\n(Show), (relationship), (budget), …, (Show relationship), (relationship budget), …,\n(Action and Adventure), (Adventure movies gross), …, (Show relationship budget and\nrating Action and Adventure movies gross over 100000000)\nnmod: between\nconj: and\nnmod: for\nconj: and\nnmod: \nover\nnmod: between\nPOS Tags + Dependency Parse Tree\nTokens and N-grams\nnsubj\n(Note: only a subset of the traversed paths are shown to aid readability.)\nVIS\n+NL\nVIS\n+NL\nVIS\n+NL\nVIS\nNLP\nNLP\n(a)\n(b)\n(c)\n(d)\nFig. 5: Summary of the query interpretation pipeline triggered by analyze query(query) . Information ﬂows sequentially across stages unless\nexplicitly indicated by bi-directional arrows. Input to different stages is g provided externally by developers, 2 generated by other stages of\nNL4DV’s pipleline, or / preconﬁgured into NL4DV. The ﬁgure also highlights the key goal and implementation challenges for each stage ( NLP :\ngeneral NLP challenge,\nVIS+NL : challenge speciﬁc to visualization NLIs,\nVIS : visualization design/recommendation challenge). NL4DV\ninternally tackles these challenges, providing visualization developers with a high-level API for query interpretation.\ngoal of supporting visualization speciﬁcation through NL (as opposed\nto interacting with a given chart [46,52] or question answering [25]).\nWhile ﬁlters may be detected via data values (e.g., ‘Action’, ‘Com-\nedy’), to detect base tasks, NL4DV compares the query tokens to a\npredeﬁned list of task keywords (e.g., ‘correlate’, ‘relationship’, etc.,\nfor the Correlation task, ‘range’, ‘spread’, etc., for the Distribution\ntask, ‘average’, ‘sum’, etc., for Derived Value). Merely detecting refer-\nences to attributes, values, and tasks is insufﬁcient to infer user intent,\nhowever. To model relationships between query phrases and popu-\nlate task details, NL4DV leverages the POS tags and the dependency\ntree generated by the query parser. Speciﬁcally, using the token type\nand dependency type (e.g., nmod, conj, nsubj) and distance, NL4DV\nidentiﬁes mappings between attributes, values, and tasks. These map-\npings are then used to model the taskMap. The task keywords and\ndependency parsing rules were deﬁned based on the query patterns and\nexamples from prior visualization NLIs [13,22,46,54,69] as well as\n∼200 questions collected by Amar et al. [1] when formulating their\nanalytic task taxonomy.\nThe taskMap contains analytic tasks as keys. Tasks are broken down\nas a list of objects that include an inferenceType ﬁeld to indicate\n5\nif a task was stated explicitly (e.g., through keywords) or derived\nimplicitly (e.g., if a query requests for a line chart, a trend task may\nbe implied) and parameters to apply when executing a task. These\ninclude the attributes a task maps to, the operator to be used (e.g.,\nGT, EQ, AVG, SUM), and values. If there are ambiguities in task\nparameters (e.g., the word ‘ﬁction’ may refer to the values ‘Science\nFiction,’ ‘Contemporary Fiction,’ ‘Historical Fiction’), NL4DV adds\nadditional ﬁelds (e.g., isValueAmbiguous=true) to highlight them\n(DG3). In addition to the tasks themselves, this structuring of the\ntaskMap allows developers to detect: (1) the parameters needed to\nexecute a task (attributes, operator, values), (2) operator- and\nvalue-level ambiguities (e.g., isValueAmbiguous), and (3) if the task\nwas stated explicitly or implicitly (inferenceType).\nConsider the taskMap (Figure 5c) for the query in Figure 3. Using\nthe dependency tree in Figure 5a, NL4DV infers that the word ‘relation-\nship’ maps to the Correlation task and links to the tokens ‘budget’ and\n‘rating’ which are in-turn linked by the conjunction term ‘and.’ Next,\nreferring back to the attributeMap, NL4DV maps the words ‘budget’\nand ‘rating’ to their respective data attributes, adding three objects\ncorresponding to correlations between the attributes [Production\nBudget, IMDB Rating], [Production Budget, Content Rating], and\n[Production Budget, Rotten Tomatoes Rating] to the correlation task.\nLeveraging the tokens ‘Action’ and ‘Adventure’, NL4DV also infers\nthat the query refers to a Filter task on the attribute Genre, where\nthe values are in the list (IN) [Action, Adventure]. Lastly, using the\ndependencies between tokens in the phrase ‘gross over 100M,’ NL4DV\nadds an object with the attribute Worldwide Gross, the greater than\n(GT) operator, and 100000000 in the values ﬁeld. While populating\nﬁlter tasks, NL4DV also updates the corresponding attributes in the\nattributeMap with the key encode=False (Figure 5b). This helps\ndevelopers detect that an attribute is used for ﬁltering and is not visually\nencoded in the recommended charts.\n4.2.4\nVisualization Generation\nNL4DV uses Vega-Lite as the underlying visualization grammar. The\ntoolkit currently supports the Vega-Lite marks: bar, tick, line, area,\npoint, arc, boxplot, text and encodings: x, y, color, size, column, row,\ntheta to visualize up to three attributes at a time. This combination\nof marks and encodings allows NL4DV to support a range of com-\nmon visualization types including histograms, strip plots, bar charts\n(including stacked and grouped bar charts), line and area charts, pie\ncharts, scatterplots, box plots, and heatmaps. To determine visualiza-\ntions relevant to the input query, NL4DV checks the query for explicit\nrequests for visualization types (e.g., Figure 1a) or implicitly infers\nvisualizations from attributes and tasks (e.g., Figures 1b, 1c, and 3).\nExplicit visualization requests are identiﬁed by comparing query\nN-grams to a predeﬁned list of visualization keywords (e.g., ‘scatter-\nplot’, ‘histogram’, ‘bar chart’). For instance, the query in Figure 1a\nspeciﬁes the visualization type through the token ‘histogram,’ leading\nto NL4DV setting bar as the mark type and binned IMDB Rating as\nthe x encoding in the underlying Vega-Lite speciﬁcation.\nTo implicitly determine visualizations, NL4DV uses a combination\nof the attributes and tasks inferred from the query. NL4DV starts\nby listing all possible visualizations using the detected attributes by\napplying well-known mappings between attributes and visualizations\n(Table 1). These mappings are preconﬁgured within NL4DV based on\nheuristics used in prior systems like Show Me [32] and Voyager [64,\n66]. As stated earlier, when generating visualizations from attributes,\nNL4DV does not visually encode the attributes used as ﬁlters. Instead,\nﬁlter attributes are added as a filter transform in Vega-Lite. Doing\nso helps avoid a combinatorial explosion of attributes when a query\nincludes multiple ﬁlters (e.g., including the ﬁlter attributes for the query\nin Figure 3 would require generating visualizations that encode four\nattributes instead of two).\nBesides attributes, if tasks are explicitly stated in the query, NL4DV\nuses them as an additional metric to modify, prune, and/or rank the\ngenerated visualizations. Consider the query in Figure 3. Similar to\nthe query in Figure 1c, if only attributes were used to determine the\ncharts, NL4DV would output two scatterplots (for QxQ) and one bar\nAttributes\n(x, y, color/size/row/column)\nVisualizations\nTask\nQ x Q x {N, O, Q, T}\nScatterplot\nCorrelation\nN, O x Q x {N, O, Q, T}\nBar Chart\nDerived Value\nQ, N, O x {N, O, Q, T} x {Q}\nStrip Plot, Histogram,\nBar Chart, Heatmap\nDistribution\nT x {Q} x {N, O}\nLine Chart\nTrend\nTable 1: Attribute (+encodings), visualization, and task mappings pre-\nconﬁgured in NL4DV. Attributes in curly brackets {are optional}. Note\nthat these defaults can be overridden via explicit queries. For instance,\n“Show average gross across genres as a scatterplot” will create a scat-\nterplot instead of a bar chart with Genre on the x- and AVG(Worldwide\nGross) on the y-axis. For unsupported attribute combinations and tasks,\nNL4DV resorts to a table-like view created using Vega-Lite’s text mark.\nchart (for NxQ). However, since the query contains the token ‘relation-\nship,’ which maps to a Correlation task, NL4DV enforces a scatterplot\nas the chart type, setting the mark in the Vega-Lite speciﬁcations to\npoint. Furthermore, because correlations are more apparent in QxQ\ncharts, NL4DV also ranks the two QxQ charts higher, returning the\nthree visualization speciﬁcations shown in Figure 5d. These Task x Vi-\nsualization mappings (Table 1) are conﬁgured within NL4DV based on\nprior visualization systems [8,14,36] and studies [26,42].\nNL4DV complies the inferred visualizations into a visList (Fig-\nure 5d). Each object in this list is composed of a vlSpec contain-\ning the Vega-Lite speciﬁcation for a chart, an inferenceType ﬁeld to\nhighlight if a visualization was requested explicitly or implicitly in-\nferred by NL4DV, and a list of attributes and tasks that a visualization\nmaps to. Developers can use the visList to directly render visual-\nizations in their systems (via the vlSpec). Alternatively, ignoring the\nvisList, developers can also extract only attributes and tasks using\nthe attributeMap and taskMap, and feed them as input to other\nvisualization recommendation engines (e.g., [30,65]) (DG2).\n4.2.5\nImplicit Task Inference\nWhen the input query lacks explicit keywords referring to analytic tasks,\nNL4DV ﬁrst checks if the query requests for a speciﬁc visualization\ntype. If so, the toolkit uses mappings between Visualizations x Tasks\nin Table 1 to infer tasks (e.g., distribution for a histogram, trend for a\nline chart, correlation for a scatterplot).\nAlternatively, if the query only mentions attributes, NL4DV ﬁrst lists\npossible visualizations based on those attributes. Then, using the in-\nferred visualizations, the toolkit implicitly infers tasks (again leveraging\nthe Visualization x Task mappings in Table 1). Consider the example\nin Figure 1c. In this case, the tasks Correlation and Derived Value are\ninferred based on the two scatterplots and one bar chart generated using\nthe attribute combinations QxQ and NxQ, respectively. In such cases\nwhere the tasks are implicitly inferred through visualizations, NL4DV\nalso sets their inferenceType in the taskMap to implicit.\n5\nEXAMPLE APPLICATIONS\n5.1\nUsing NL4DV in Jupyter Notebook\nSince NL4DV generates Vega-Lite speciﬁcations, in environments that\nsupport rendering Vega-Lite charts, the toolkit can be used to create\nvisualizations through NL in Python. Speciﬁcally, NL4DV provides a\nwrapper function render vis(query) that automatically renders the ﬁrst\nvisualization in the visList. By rendering visualizations in response\nto NL queries in environments like Jupyter Notebook, NL4DV en-\nables novice Python data scientists and programmers to conduct visual\nanalysis without needing to learn about visualization design or Python\nvisualization packages (e.g., Matplotlib, Plotly). Figure 6 shows an\ninstance of a Jupyter Notebook demonstrating the use of NL4DV to\ncreate visualizations for a cars dataset. For the ﬁrst query “Create a\nboxplot of acceleration,” detecting an explicit visualization request,\nNL4DV renders a box plot showing values for the attribute Accelera-\ntion. For the second query “Visualize horsepower mpg and cylinders”,\n6\n© 2020 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The ﬁnal version of this record is available at: 10.1109/TVCG.2020.3030378\nFig. 6: NL4DV being used to specify visualizations through NL in\nPython within a Jupyter Notebook.\nFig. 7: A Vega-Lite editor that supports NL-based chart speciﬁcation\nand presents design alternatives using the visList returned by NL4DV.\nHere, the query “Show debt and earnings for different types of colleges”\nissued in the context of a U.S. colleges dataset results in the system\nsuggesting a colored scatterplot and a colored + faceted scatterplot. The\nfaceted scatterplot is selected as the active chart by the user.\nNL4DV implicitly selects a scatterplot as the appropriate visualization\nusing the inferred attributes (Û Horsepower, Û MPG, Î Cylinders).\n5.2\nCreating Visualization Systems with NL4DV\nThe above example illustrates how the visList generated by NL4DV\nis used to create visualizations in Python-based data science environ-\nments. The following two examples showcase how NL4DV can assist\nthe development of web-based NLIs for data visualization.\n5.2.1\nNL-Driven Vega-Lite Editor\nAlthough the declarative nature of Vega-Lite makes it an intuitive way\nto specify visualizations, novices unaware of visualization terminology\nmay need to spend a signiﬁcant amount of time and effort to look at\nexamples and learn the speciﬁcation grammar. NL input presents itself\nas a promising solution in such scenarios to help onboard users for\nlearning Vega-Lite. With a NLI, users can load their data and express\ntheir intended chart through NL. The system in response can present\nboth the chart and the corresponding Vega-Lite speciﬁcation, allowing\nusers to learn the underlying grammar through charts of their interest.\nFigure 7 illustrates this idea implemented as an alternative version of\nthe Vega-Lite editor [58], supporting NL input. Users can enter queries\nthrough the text input box at the top of the page to specify charts and\nedit the speciﬁcation on the left to modify the resulting visualization.\nBesides the main visualization returned in response to an NL query,\nthe interface also presents a alternative visualizations using different\nencodings similar to the Voyager systems [64,66].\nThis example is developed following a classic client-server archi-\ntecture and is implemented as a Python Flask [12] application. From\na development standpoint, the client-side of this application is writ-\nten from scratch using HTML and JavaScript. On the Python server-\nside, a single call is made to NL4DV’s analyze query(query) function\n1\n$.post(\"/analyzeQuery\", {\"query\": query})\n2\n.done(function (responseString) {\n3\nlet nl4dvResponse = JSON.parse(responseString);\n4\nlet visList = nl4dvResponse['visList'];\n5\n// render visList[0]['vlSpec'] as default chart\n6\nfor(let visObj of visList){\n7\n// add visObj['vlSpec'] as a thumbnail in the\nbottom panel displaying all possible designs\n,→\n8\n}\n9\n});\nListing 2: JavaScript code to parse NL4DV’s output to create the Vega-\nLite editor application shown in Figure 7.\nFig. 8: A sample interface illustrating how NL4DV can be used to\nreplicate DataTone’s [13] ambiguity widgets.\nwhere the query is collected and passed via JavaScript. As shown\nearlier (Listing 1), this function returns a JSON object composed of\nthe attributeMap, taskMap, and visList. For this example, the\nvisList returned by NL4DV is parsed in JavaScript to render the main\nchart along with the alternative designs (Listing 2, lines 5-8). The\nvisualizations are rendered using Vega-Embed [59].\n5.2.2\nRecreating Ambiguity Widgets in DataTone\nConsider a second example where we use NL4DV to replicate features\nof the DataTone system [13]. Given a NL query, DataTone identiﬁes\nambiguities in the query and surfaces them via “ambiguity widgets”\n(dropdown menus) that users can interact with to clarify their intent.\nFigure 8 shows a DataTone-like interface implemented using NL4DV.\nThis system is also implemented as a Flask web-application using\nHTML and JavaSript on the client-side. The example in Figure 8\nillustrates the result of executing the query “Show me medals for hockey\nand skating by country” against an Olympics medal winners dataset\n(query reused from the DataTone paper [13]). Here, ‘medals’ is an\nambiguous reference to four data attributes—the three medal types\n(Bronze, Silver, Gold) and the Total Medals. Similarly, ‘hockey’ and\n‘skating’ are value-level ambiguities corresponding to the Sport attribute\n(e.g., ‘hockey’=[Ice Hockey, Hockey]).\nSimilar to the Vega-Lite editor application, the server-side code only\ninvolves initializing NL4DV with the active dataset and making a call to\nthe analyze query(query) function to process user queries. As detailed\nin Listing 3, on the client-side, to highlight attribute- and value-level\nambiguities in the query, we parse the attributeMap and taskMap\nreturned by NL4DV in JavaScript, checking the isAmbiguous ﬁelds.\nVega-Embed is once again used to render the vlSpecs returned as\npart of NL4DV’s visList. Note that we only focus on data ambigu-\nity widgets in this example, not displaying design ambiguity widgets\n(e.g., dropdown menus for switching between visualization types). To\ngenerate design ambiguity widgets, however, developers can parse\nthe visList, converting the Vega-Lite marks and encodings into\ndropdown menu options.\n5.3\nAdding NL Input to an Existing Visualization System\nNL4DV can also be used to augment existing visualization systems\nwith NL input. As an example, consider TOUCHPLOT (Figure 9-top), a\n7\n1\n$.post(\"/analyzeQuery\", {\"query\": query})\n2\n.done(function (responseString) {\n3\nlet nl4dvResponse = JSON.parse(responseString);\n4\nlet attributeMap = nl4dvResponse['attributeMap'],\n5\ntaskMap = nl4dvResponse['taskMap'];\n6\nfor(let attr in attributeMap){\n7\nif(attributeMap[attr]['isAmbiguous']){\n8\n// add attr and attributeMap[attr]['ambiguity']\nto attribute-level ambiguity widget\ncorresponding to the\nattributeMap[attr]['queryPhrase']\n,→\n,→\n,→\n9\n}\n10\n}\n11\n...\n12\n});\nListing 3: JavaScript code to parse NL4DV’s output and generate\nattribute-level ambiguity widgets (highlighted in Figure 8-left). A\nsimilar logic is used to iterate over the taskMap when creating value-\nlevel ambiguity widgets (highlighted in Figure 8-right) for ﬁltering.\ntouch-based scatterplot visualization system running on a tablet. We\nmodeled TOUCHPLOT after the interface and capabilities of the scatter-\nplot visualization system, Tangere [41]. Speciﬁcally, users can select\npoints and zoom/pan by interacting directly with the chart canvas, bind\nattributes to the position, color, and size encodings using dropdown\nmenus on axes and legends, or apply ﬁlters using a side panel. TOUCH-\nPLOT is implemented using HTML and JavaScript, and D3 is used for\ncreating the visualization.\nRecent work has shown that complementing touch interactions with\nspeech can support a more ﬂuid interaction experience during visual\nanalysis on tablets [49]. For example, while touch can support ﬁne-\ngrained interactions with marks, speech can allow specifying ﬁlters\nwithout having to open and interact with the side panel, saving screen\nspace and preserving the user workﬂow. To explore such ﬂuid interac-\ntions, we developed MMPLOT (Figure 9-bottom), a modiﬁed version\nof TOUCHPLOT that supports multimodal touch and speech input. In\naddition to touch interactions, MMPLOT allows issuing speech com-\nmands to specify charts (e.g., “Correlate age and salary by country”)\nand ﬁlter points (e.g., “Remove players over the age of 30”).\nTo support these interactions, we record speech input and con-\nvert it to a text string using the Web Speech API [61]. This query\nstring is then passed to the server, where we make a call to NL4DV’s\nanalyze query(query) . By parsing NL4DV’s response in JavaScript,\nTOUCHPLOT is modiﬁed to support the required speech interactions\n(Listing 4). In particular, we parse the taskMap to detect and apply\nany ﬁlters requested as part of the query (lines 6-12). Next, we check if\nthe input query speciﬁes a new scatterplot that can be rendered by the\nsystem and adjust the view mappings accordingly (lines 13-16). This\nsequential parsing of taskMap and visList allows using speech to\napply ﬁlters, specify new scatterplots, or do both with a single query\n(Figure 9). Unlike previous examples, since this application uses D3\n(as opposed to Vega-Lite) to create the visualization, when parsing\nNL4DV’s output, we perform an added step of invoking the D3 code\nrequired to update the view (line 17).\nImplementing the aforementioned examples (NL-based Vega-Lite edi-\ntor, DataTone’s ambiguity widgets, MMPLOT) would typically require\ndevelopers to write hundreds of lines of code (in addition to the front-\nend code) requiring both NLP and visualization design knowledge\n(Figure 2). As illustrated above, with NL4DV, developers can accom-\nplish the desired NLI capabilities with a single call to analyze query()\nand a few additional lines of code to parse NL4DV’s response, enabling\nthem to focus on the interface design and user experience.\n6\nDISCUSSION AND FUTURE WORK\n6.1\nEvaluation\nIn this paper, we illustrate NL4DV’s query interpretation capabilities\nthrough sample queries executed on different datasets1. As part of this\n1Additional queries available at: https://nl4dv.github.io/nl4dv/showcase.html\nFig. 9: (Top) TOUCHPLOT interface supporting interaction through\ntouch and control panels. (Bottom) MMPLOT interface supporting\nmultimodal interactions. Here, the user has speciﬁed a new scatterplot\nand applied a ﬁlter through a single query “Show a scatter plot of age\nand salary for players under the age of 30.”\n1\n$.post(\"/analyzeQuery\", {\"query\": query})\n2\n.done(function (responseString) {\n3\nlet nl4dvResponse = JSON.parse(responseString);\n4\nlet taskMap = nl4dvResponse['taskMap'],\n5\nvisList = nl4dvResponse['visList'];\n6\nif(\"filter\" in taskMap){ // query includes a filter\n7\nfor(let taskObj of taskMap['filter']){\n8\nfor(let attr of taskObj['attributes']){\n9\n// use the attribute type, 'operator', and\n'values' to apply requested filters\n,→\n10\n}\n11\n}\n12\n}\n13\nif(visList.length>0){ // query specifies a new chart\n14\nlet newVisSpec = visList[0]['vlSpec'];\n15\n// check if newVisSpec is a scatterplot\nconfiguration supported by the system and\nmodify the attribute-encoding mappings\n,→\n,→\n16\n}\n17\n// invoke the D3 code to update the view\n18\n});\nListing 4: JavaScript code to parse NL4DV’s output for supporting\nspeech and multimodal interactions in MMPLOT (Figure 9-bottom).\ninitial validation, we used NL4DV to query tabular datasets containing\n300-6000 rows and up to 27 attributes. The toolkit’s response time for\nthese queries ranged between 1-18 sec. (mean: 3 sec.)2. Besides sample\nqueries, we also present applications employing NL4DV to highlight\nhow the toolkit can reduce development viscosity and lower skill bar-\nriers (in terms of prior knowledge of NLP tools and techniques) [38].\nHowever, assessing the toolkit’s usability and utility likely involves a\nmore detailed evaluation in two ways. First, we need to formally bench-\nmark NL4DV’s performance by executing it against a large corpus of\nNL queries. To this end, an important area for future work is to collect\nlabeled data on utterances people use to specify visualizations and use\nit to benchmark NL4DV and other visualization NLIs. Second, we\nneed to conduct a longitudinal study incorporating feedback from both\nvisualization and NLP developers. Going forward, we hope that the\nopen-source nature of this research will help us conduct such a study\nin the wild, enabling us to assess NL4DV’s practical usability, identify\npotential issues, and understand the breadth of possible applications.\n6.2\nSupporting Follow-up Queries\nNL input presents the opportunity to support a richer visual analytic\ndialog through conversational interaction (as opposed to one-off utter-\n2Reported based on a MacBook Pro with a 6-core 2.9GHz processor and\n16GB RAM running MacOS Catalina version 10.15.5\n8\n© 2020 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The ﬁnal version of this record is available at: 10.1109/TVCG.2020.3030378\nances). For example, instead of a single query including both visual-\nization speciﬁcation and ﬁltering requests (e.g., “Show a scatterplot\nof gross and budget highlighting only Action and Adventure movies”),\none can issue a shorter query to ﬁrst gain an overview by specifying a\nvisualization (e.g., “Show a scatterplot of gross and budget”) and then\nissue a follow-up query to apply ﬁlters (e.g., “Now just show Action\nand Adventure movies”). However, supporting such a dialog through an\ninterface-agnostic toolkit is challenging as it requires the toolkit to have\ncontext of system state in which the query was issued. Furthermore, not\nall queries in a dialog may be follow-up queries. While current systems\nlike Evizeon [22] allow users to reset the canvas to reset the query\ncontext, explicitly specifying when context should be preserved/cleared\nwhile operating an interface-agnostic toolkit is impractical.\nNL4DV currently does not support follow-up queries. However,\nas a ﬁrst pass at addressing these challenges, we are experiment-\ning with an additional dialog parameter to\nanalyze query() and\nrender vis() to support follow-up queries involving ﬁltering and en-\ncoding changes. Speciﬁcally, setting dialog=true notiﬁes NL4DV\nto check for follow-up queries. NL4DV uses conversational centering\ntechniques [15,16] similar to prior visualization NLIs [22,50,52] to\nidentify missing attributes, tasks, or visualization details in a query\nbased on the toolkit’s previous response. Consider the example in\nFigure 10 showing queries issued in the context of a housing dataset.\nIn response to the ﬁrst query “Show average prices for different home\ntypes over the years,” NL4DV generates a line chart by detecting the\nattributes Price, House Type, and Year, and the task Derived Value (with\nthe operator AVG). Next, given the follow-up query “As a bar chart,”\nNL4DV infers the attributes and tasks from its previous response, up-\ndating the mark type and encoding channels in the Vega-Lite speciﬁ-\ncation to create a grouped bar chart. Lastly, with the third query, “Just\nshow condos and duplexes,” detecting ‘condos’ and ‘duplexes’ as data\nvalues, NL4DV modiﬁes the underlying taskMap and applies a ﬁlter on\nthe House Type attribute. Besides implementing additional parameters\nand functions to support conversational interaction in NL4DV, a general\nfuture research challenge is to investigate how interface context (e.g.,\nactive encodings, selections) can be modeled into a structured format\nthat can be interpreted by interface-agnostic toolkits like NL4DV.\n6.3\nImproving Query Interpretation and Enabling Addi-\ntional Query Types\nThrough our initial testing, we have already identiﬁed some areas for\nimprovement in NL4DV’s interpretation pipeline. One of these is better\ninference of attribute types upon initialization. To this end, we are\nlooking into how we can augment NL4DV’s data interpretation pipeline\nwith recent semantic data type detection models that use both attribute\nnames and values (e.g., [23, 70]). Another area for improvement is\ntask detection. NL4DV currently leverages a combination of lexicon-\nand dependency-based approach to infer tasks. Although this serves\nas a viable starting point, it is less reliable when there is uncertainty\nin the task keywords (e.g., the word “relationship” may not always\nmap to a Correlation task) or the keywords conﬂict with data attributes\n(e.g., the query “Show the average cost of schools by region” would\ncurrently apply a Derived Value task on the Average Cost attribute\neven though Average Cost already represents a derived value). As we\ncollect more user queries, we are exploring ways to complement the\ncurrent approach with semantic parsers (e.g., [2,3]) and contemporary\ndeep learning models (e.g., [7, 68]) that can infer tasks based on the\nquery phrasing and structure. Finally, a third area for improvement\nis to connect NL4DV to knowledge bases like WolframAlpha [63] to\nsemantically understand words in the input query. Incorporating such\nconnections will help resolve vague predicates for data values (e.g.,\n‘large’, ‘expensive’, ‘near’) [46,47] and may also reduce the need for\ndevelopers to manually conﬁgure attribute aliases.\nBesides improving query interpretation, another theme for future\nwork is to support additional query types. As stated earlier, NL4DV\nis currently primarily geared to support visualization speciﬁcation-\noriented queries (e.g., “What is the relationship between worldwide\ngross and content rating?,” “Create a bar chart showing average proﬁt\nby state”). To aid development of full-ﬂedged visualization systems,\n“Show average prices for different \nhome types over the years.”\n“As a bar chart.”\n“Just show condos and duplexes.”\nFig. 10: Example of NL4DV supporting follow-up queries using the\nexperimental dialog parameter. Here, the three visualizations are gen-\nerated through consecutive calls to render vis(query, dialog=true ).\nhowever, the toolkit needs to support a tighter coupling with an active vi-\nsualization and enable other tasks such as question answering [25] (e.g.,\n“How many movies grossed over 100M?,” “When was the difference be-\ntween the two stocks the highest?”) and formatting visualizations (e.g.,\n“Color SUVs green,” “Highlight labels for countries with a population\nof more than 200M”). Incorporating these new query types would\nentail making changes in terms of both the interpretation strategies (to\nidentify new task categories and their parameters) and the output format\n(to include the computed “answers” and changes to the view).\n6.4\nBalancing Simplicity and Customization\nNL4DV is currently targeted towards visualization developers who\nmay not have a strong NLP background (DG1). As such, NL4DV\nuses a number of default settings (e.g., preset rules for dependency\nparsing, empirically set thresholds for attribute detection) to minimize\nthe learning curve and facilitate ease-of-use. Developers can override\nsome of these defaults (e.g., replace CoreNLP with spaCy [21], ad-\njust similarity matching thresholds) and also conﬁgure dataset-speciﬁc\nsettings to improve query interpretation (e.g., attribute aliases, special\nwords referring to data values, additional stopwords to ignore) (DG4).\nFurthermore, by invoking analyze query() with the debug parameter\nset to true, developers can also get additional details such as why an\nattribute was detected (e.g., semantic vs. syntactic match along with\nthe match score) or how a chart was implicitly inferred (e.g., using\nattributes vs. attributes and tasks).\nNLP or visualization experts, however, may prefer using custom\nmodules for query processing or visualization recommendation. To this\nend, visualization developers can override the toolkit’s default recom-\nmendation engine by using the inferred attributes and tasks as input to\ntheir custom modules (i.e., ignoring the visList). However, NL4DV\ncurrently does not support using custom NLP models for attribute and\ntask inference (e.g., using word embedding techniques to detect syn-\nonyms or classiﬁcation models to identify tasks). Going forward, as we\ngather feedback on the types of customizations developers prefer, we\nhope to provide the option for developers to replace NL4DV’s heuristic\nmodules with contemporary ML models/techniques. Given the end goal\nof aiding prototyping of visualization NLIs, a challenge in supporting\nthis customization, however, is to ensure that the output from the cus-\ntom models can be compiled into NL4DV’s output speciﬁcation or to\nmodify NL4DV’s speciﬁcation to accommodate additional information\n(e.g., classiﬁcation accuracy) generated by the custom models.\n7\nCONCLUSION\nWe present NL4DV, a toolkit that supports prototyping visualization\nNLIs. Given a dataset and a NL query, NL4DV generates a JSON-based\nanalytic speciﬁcation composing of attributes, tasks, and visualizations\ninferred from the query. Through example applications, we show\nhow developers can use this JSON response to create visualizations\nin Jupyter notebooks through NL, develop web-based visualization\nNLIs, and augment existing visualization tools with NL interaction. We\nprovide NL4DV and the example applications as open-source software\n(https://nl4dv.github.io/nl4dv/) and hope these will serve as\nvaluable resources to advance research on NLIs for data visualization.\nACKNOWLEDGMENTS\nThis work was supported in part by a National Science Foundation\nGrant IIS-1717111.\n9\nREFERENCES\n[1] R. Amar, J. Eagan, and J. Stasko. Low-level components of analytic\nactivity in information visualization. In Proceedings of IEEE InfoVis,\npages 111–117, 2005.\n[2] J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on freebase\nfrom question-answer pairs. In Proceedings of the EMNLP, pages 1533–\n1544. ACL, 2013.\n[3] J. Berant and P. Liang. Semantic parsing via paraphrasing. In Proceedings\nof the 52nd Annual Meeting of the ACL (Volume 1: Long Papers), pages\n1415–1425, 2014.\n[4] L. Blunschi, C. Jossen, D. Kossmann, M. Mori, and K. Stockinger. Soda:\nGenerating sql for business users. Proceedings of the VLDB Endowment,\n5(10):932–943, 2012.\n[5] M. Bostock and J. Heer. Protovis: A graphical toolkit for visualization.\nIEEE Transactions on Visualization and Computer Graphics, 15(6):1121–\n1128, 2009.\n[6] M. Bostock, V. Ogievetsky, and J. Heer. D3: data-driven documents. IEEE\nTransactions on Visualization and Computer Graphics, 17(12):2301–2309,\n2011.\n[7] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models\nare few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[8] S. M. Casner. Task-analytic approach to the automated design of graphic\npresentations. ACM Transactions on Graphics (ToG), 10(2):111–151,\n1991.\n[9] Y. Chen, J. Yang, and W. Ribarsky. Toward effective insight management\nin visual analytics systems. In Proceedings of IEEE PaciﬁcVis, pages\n49–56, 2009.\n[10] K. Cox, R. E. Grinter, S. L. Hibino, L. J. Jagadeesan, and D. Mantilla.\nA multi-modal natural language interface to an information visualization\nenvironment. International Journal of Speech Technology, 4(3-4):297–314,\n2001.\n[11] J. R. Finkel, T. Grenager, and C. Manning. Incorporating non-local in-\nformation into information extraction systems by gibbs sampling. In\nProceedings of ACL, pages 363–370, 2005.\n[12] Flask. https://palletsprojects.com/p/flask/.\n[13] T. Gao, M. Dontcheva, E. Adar, Z. Liu, and K. G. Karahalios. DataTone:\nManaging ambiguity in natural language interfaces for data visualization.\nIn Proceedings of ACM UIST, pages 489–500, 2015.\n[14] D. Gotz and Z. Wen. Behavior-driven visualization recommendation. In\nProceedings of ACM IUI, pages 315–324, 2009.\n[15] B. J. Grosz and C. L. Sidner. Attention, intentions, and the structure of\ndiscourse. Computational linguistics, 12(3):175–204, 1986.\n[16] B. J. Grosz, S. Weinstein, and A. K. Joshi. Centering: A framework for\nmodeling the local coherence of discourse. Computational linguistics,\n21(2):203–225, 1995.\n[17] P. He, Y. Mao, K. Chakrabarti, and W. Chen. X-SQL: reinforce schema\nrepresentation with context. arXiv preprint arXiv:1908.08113, 2019.\n[18] J. Heer and M. Bostock. Declarative language design for interactive\nvisualization. IEEE Transactions on Visualization and Computer Graphics,\n16(6):1149–1156, 2010.\n[19] J. Heer, S. K. Card, and J. A. Landay. Prefuse: a toolkit for interactive\ninformation visualization. In Proceedings of ACM CHI, pages 421–430,\n2005.\n[20] J. Herzig, P. K. Nowak, T. M¨uller, F. Piccinno, and J. M. Eisenschlos.\nTAPAS: Weakly Supervised Table Parsing via Pre-training. arXiv preprint\narXiv:2004.02349, 2020.\n[21] M. Honnibal and I. Montani. spacy 2: Natural language understanding\nwith bloom embeddings. Convolutional Neural Networks and Incremental\nParsing, 2017.\n[22] E. Hoque, V. Setlur, M. Tory, and I. Dykeman. Applying pragmatics\nprinciples for interaction with visual analytics. IEEE Transactions on\nVisualization and Computer Graphics, 24(1):309–318, 2018.\n[23] M. Hulsebos, K. Hu, M. Bakker, E. Zgraggen, A. Satyanarayan, T. Kraska,\nc. Demiralp, and C. Hidalgo. Sherlock: A deep learning approach to\nsemantic data type detection. In Proceedings of SIGKDD. ACM, 2019.\n[24] J.-F. Kassel and M. Rohs. Valletto: A multimodal interface for ubiquitous\nvisual analytics. In ACM CHI ’18 Extended Abstracts, 2018.\n[25] D. H. Kim, E. Hoque, and M. Agrawala. Answering questions about charts\nand generating visual explanations. In Proceedings of ACM CHI, pages\n:1–:13, 2020.\n[26] Y. Kim and J. Heer. Assessing effects of task and data distribution on\nthe effectiveness of visual encodings. In Computer Graphics Forum,\nvolume 37, pages 157–167. Wiley Online Library, 2018.\n[27] A. Kumar, J. Aurisano, B. Di Eugenio, A. Johnson, A. Gonzalez, and\nJ. Leigh. Towards a dialogue system that supports rich visualizations of\ndata. In Proceedings of the SIGDIAL, pages 304–309, 2016.\n[28] D. Ledo, S. Houben, J. Vermeulen, N. Marquardt, L. Oehlberg, and\nS. Greenberg. Evaluation strategies for hci toolkit research. In Proceedings\nof ACM CHI, pages 1–17, 2018.\n[29] F. Li and H. V. Jagadish. NaLIR: an interactive natural language interface\nfor querying relational databases. In Proceedings of the ACM SIGMOD,\npages 709–712, 2014.\n[30] H. Lin, D. Moritz, and J. Heer. Dziban: Balancing agency & automation\nin visualization design via anchored recommendations. In Proceedings of\nACM CHI, pages 751:1–751:12, 2020.\n[31] E. Loper and S. Bird. NLTK: The natural language toolkit. In Proceedings\nof ACL Workshop on Effective Tools and Methodologies for Teaching\nNatural Language Processing and Computational Linguistics, pages 63–\n70, 2002.\n[32] J. Mackinlay, P. Hanrahan, and C. Stolte. Show Me: Automatic presenta-\ntion for visual analysis. IEEE Transactions on Visualization and Computer\nGraphics, 13(6):1137–1144, 2007.\n[33] C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard, and\nD. McClosky. The Stanford CoreNLP natural language processing toolkit.\nIn Proceedings of ACL: System Demonstrations, pages 55–60, 2014.\n[34] Microsoft Power BI. https://powerbi.microsoft.com/en-us.\n[35] G. A. Miller. WordNet: a lexical database for english. Communications of\nthe ACM, 38(11):39–41, 1995.\n[36] D. Moritz, C. Wang, G. L. Nelson, H. Lin, A. M. Smith, B. Howe, and\nJ. Heer. Formalizing visualization design knowledge as constraints: Ac-\ntionable and extensible models in draco. IEEE Transactions on Visualiza-\ntion and Computer Graphics, 25(1):438–448, 2018.\n[37] B. Myers, S. E. Hudson, and R. Pausch. Past, present, and future of\nuser interface software tools. ACM Transactions on Computer-Human\nInteraction, 7(1):3–28, 2000.\n[38] D. R. Olsen Jr. Evaluating user interface systems research. In Proceedings\nof ACM UIST, pages 251–258, 2007.\n[39] P. Pasupat and P. Liang.\nCompositional semantic parsing on semi-\nstructured tables. In Proceedings of IJCNLP, pages 1470–1480. ACL,\n2015.\n[40] A.-M. Popescu, O. Etzioni, and H. Kautz. Towards a theory of natural\nlanguage interfaces to databases. In Proceedings of IUI, pages 149–157.\nACM, 2003.\n[41] R. Sadana and J. Stasko. Designing and implementing an interactive\nscatterplot visualization for a tablet computer. In Proceedings of AVI,\npages 265–272, 2014.\n[42] B. Saket, A. Endert, and C¸ . Demiralp. Task-based effectiveness of basic vi-\nsualizations. IEEE Transactions on Visualization and Computer Graphics,\n25(7):2505–2512, 2018.\n[43] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer. Vega-Lite:\nA grammar of interactive graphics. IEEE Transactions on Visualization\nand Computer Graphics, 23(1):341–350, 2016.\n[44] A. Satyanarayan, R. Russell, J. Hoffswell, and J. Heer. Reactive vega: A\nstreaming dataﬂow architecture for declarative interactive visualization.\nIEEE Transactions on Visualization and Computer Graphics, 22(1):659–\n668, 2015.\n[45] M. Schwab, J. Tompkin, J. Huang, and M. A. Borkin. Easypz. js: Inter-\naction binding for pan and zoom visualizations. In Proceedings of IEEE\nVIS: Short Papers, pages 31–35, 2019.\n[46] V. Setlur, S. E. Battersby, M. Tory, R. Gossweiler, and A. X. Chang. Eviza:\nA natural language interface for visual analysis. In Proceedings of ACM\nUIST, pages 365–377, 2016.\n[47] V. Setlur, M. Tory, and A. Djalali. Inferencing underspeciﬁed natural\nlanguage utterances in visual analysis. In Proceedings of ACM IUI, pages\n40–51, 2019.\n[48] R. Sicat, J. Li, J. Choi, M. Cordeil, W.-K. Jeong, B. Bach, and H. Pﬁs-\nter. Dxr: A toolkit for building immersive data visualizations. IEEE\nTransactions on Visualization and Computer Graphics, 25(1):715–725,\n2018.\n[49] A. Srinivasan, B. Lee, N. H. Riche, S. M. Drucker, and K. Hinckley. InCho-\nrus: Designing consistent multimodal interactions for data visualization on\ntablet devices. In Proceedings of ACM CHI, pages 653:1–653:13, 2020.\n[50] A. Srinivasan, B. Lee, and J. T. Stasko. Interweaving multimodal in-\nteraction with ﬂexible unit visualizations for data exploration. IEEE\n10\n© 2020 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The ﬁnal version of this record is available at: 10.1109/TVCG.2020.3030378\nTransactions on Visualization and Computer Graphics, 2020.\n[51] A. Srinivasan and J. Stasko. Natural language interfaces for data anal-\nysis with visualization: Considering what has and could be asked. In\nProceedings of EuroVis: Short Papers, pages 55–59, 2017.\n[52] A. Srinivasan and J. Stasko. Orko: Facilitating multimodal interaction\nfor visual exploration and analysis of networks. IEEE Transactions on\nVisualization and Computer Graphics, 24(1):511–521, 2018.\n[53] C. Stolte, D. Tang, and P. Hanrahan. Polaris: A system for query, anal-\nysis, and visualization of multidimensional relational databases. IEEE\nTransactions on Visualization and Computer Graphics, 8(1):52–65, 2002.\n[54] Y. Sun, J. Leigh, A. Johnson, and S. Lee. Articulate: A semi-automated\nmodel for translating natural language queries into meaningful visualiza-\ntions. In Proceedings of the International Symposium on Smart Graphics,\npages 184–195, 2010.\n[55] Tableau Ask Data. https://www.tableau.com/about/blog/2018/\n10/announcing-20191-beta-96449.\n[56] M. Tory and V. Setlur. Do what i mean, not what i say! design consid-\nerations for supporting intent and context in analytical conversation. In\nProceedings of IEEE VAST, pages 93–103, 2019.\n[57] Unity. https://unity.com/.\n[58] Vega editor. https://vega.github.io/editor/.\n[59] Vega-embed. https://github.com/vega/vega-embed.\n[60] C. Wang, K. Tatwawadi, M. Brockschmidt, P.-S. Huang, Y. Mao, O. Polo-\nzov, and R. Singh. Robust Text-to-SQL generation with execution-guided\ndecoding. arXiv preprint arXiv:1807.03100, 2018.\n[61] https://developer.mozilla.org/en-US/docs/Web/API/Web_\nSpeech_API, 2019.\n[62] L. Wilkinson. The grammar of graphics. Springer Science & Business\nMedia, 2013.\n[63] WolframAlpha. https://www.wolframalpha.com/.\n[64] K. Wongsuphasawat, D. Moritz, A. Anand, J. Mackinlay, B. Howe, and\nJ. Heer. Voyager: Exploratory analysis via faceted browsing of visualiza-\ntion recommendations. IEEE transactions on visualization and computer\ngraphics, 22(1):649–658, 2015.\n[65] K. Wongsuphasawat, D. Moritz, A. Anand, J. Mackinlay, B. Howe, and\nJ. Heer. Towards a general-purpose query language for visualization\nrecommendation. In Proceedings of the HILDA Workshop, pages 1–6,\n2016.\n[66] K. Wongsuphasawat, Z. Qu, D. Moritz, R. Chang, F. Ouk, A. Anand,\nJ. Mackinlay, B. Howe, and J. Heer. Voyager 2: Augmenting visual\nanalysis with partial view speciﬁcations. In Proceedings of ACM CHI,\npages 2648–2659, 2017.\n[67] Z. Wu and M. Palmer. Verbs semantics and lexical selection. In Proceed-\nings of ACL, pages 133–138, 1994.\n[68] T. Young, D. Hazarika, S. Poria, and E. Cambria. Recent trends in deep\nlearning based natural language processing. IEEE Computational Intelli-\ngence magazine, 13(3):55–75, 2018.\n[69] B. Yu and C. T. Silva. FlowSense: A natural language interface for\nvisual data exploration within a dataﬂow system. IEEE Transactions on\nVisualization and Computer Graphics, 26(1):1–11, 2019.\n[70] D. Zhang, Y. Suhara, J. Li, M. Hulsebos, C¸ . Demiralp, and W.-C. Tan.\nSato: Contextual semantic type detection in tables.\narXiv preprint\narXiv:1911.06311, 2019.\n[71] V. Zhong, C. Xiong, and R. Socher. Seq2SQL: Generating Structured\nQueries from Natural Language using Reinforcement Learning. CoRR,\nabs/1709.00103, 2017.\n11\n",
  "categories": [
    "cs.HC"
  ],
  "published": "2020-08-24",
  "updated": "2020-11-23"
}