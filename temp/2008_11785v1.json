{
  "id": "http://arxiv.org/abs/2008.11785v1",
  "title": "Understanding scholarly Natural Language Processing system diagrams through application of the Richards-Engelhardt framework",
  "authors": [
    "Guy Clarke Marshall",
    "Caroline Jay",
    "André Freitas"
  ],
  "abstract": "We utilise Richards-Engelhardt framework as a tool for understanding Natural\nLanguage Processing systems diagrams. Through four examples from scholarly\nproceedings, we find that the application of the framework to this ecological\nand complex domain is effective for reflecting on these diagrams. We argue for\nvocabulary to describe multiple-codings, semiotic variability, and\ninconsistency or misuse of visual encoding principles in diagrams. Further, for\napplication to scholarly Natural Language Processing systems, and perhaps\nsystems diagrams more broadly, we propose the addition of \"Grouping by Object\"\nas a new visual encoding principle, and \"Emphasising\" as a new visual encoding\ntype.",
  "text": "Understanding scholarly Natural Language\nProcessing system diagrams through application\nof the Richards-Engelhardt framework\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\nUniversity of Manchester\nAbstract. We utilise Richards-Engelhardt framework as a tool for un-\nderstanding Natural Language Processing systems diagrams. Through\nfour examples from scholarly proceedings, we ﬁnd that the application\nof the framework to this ecological and complex domain is eﬀective for re-\nﬂecting on these diagrams. We argue for vocabulary to describe multiple-\ncodings, semiotic variability, and inconsistency or misuse of visual en-\ncoding principles in diagrams. Further, for application to scholarly Nat-\nural Language Processing systems, and perhaps systems diagrams more\nbroadly, we propose the addition of “Grouping by Object” as a new vi-\nsual encoding principle, and “Emphasising” as a new visual encoding\ntype.\nKeywords: Neural networks; Natural language processing; Visual en-\ncoding; Graphic language; Nature of diagrams; Systems diagrams\n1\nIntroduction\nThe framework proposed by Engelhardt and Richards [5] is designed to be ap-\nplicable to “all types of visualizations”. Their framework provides a method and\nvocabulary for analysing diagrams and diagram components, which they have\ntested on a large variety of visualization types. It appears the framework has not\nyet been applied to system diagrams. We aim to advance the understanding of\nthe heterogeneous diagrammatic representations of Natural Language Process-\ning (NLP) systems found in scholarly conference proceedings by applying this\nframework. The scholarly NLP systems diagrams domain has been chosen due\nto its unusual heterogeneity, and its importance and prevalence in communi-\ncating about NLP systems research. Further, we propose modest clariﬁcations\nor extensions to the framework in order to ﬁt this domain, which may beneﬁt\nthe wider domain of system diagrams. Whilst Engelhardt and Richards tested\ntheir framework on a wide variety of information visualisation resources they\ndid not examine (a) systems diagrams, or (b) “ecological” diagram examples\nnot conforming to an established standard or grammar. Our method is to:\n1. Manually select a variety of natural language processing system diagrams in\nrecent conference proceedings\narXiv:2008.11785v1  [cs.HC]  26 Aug 2020\n2\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\n2. Apply the Richards-Engelhardt framework in order to analyse these exam-\nples\n3. Discuss and make suggestions to improve the applicability of Richards-\nEngelhardt framework in this domain\n4. Discuss implications of this reﬂection for the scholarly NLP community\nIn the course of this analysis, we necessarily comment on some existing dia-\ngrams. Our goal is not to criticise the authors of these diagrams, but rather to\nsupport the community in creating eﬀective diagrams. Our contribution is to:\n• Conduct diagram analysis on scholarly NLP system diagrams\n• Qualitatively evaluate Richards-Engelhardt framework as a tool for reﬂecting\non scholarly NLP systems diagrams\n• Suggest further requirements for the framework in this domain, including\nemphasis, language, graphical object schematicity, and semiotic considera-\ntions\n2\nNLP Diagrams\n2.1\nNLP Systems\nNatural Language Processing is a discipline within Computer Science, and is\nconcerned with creating systems that solve tasks relating to Natural Language\ninterpretation. NLP systems take a text input, go through data manipulation\nsteps, and create an output that is usually a classiﬁcation or a prediction, such\nas what the next word in a sequence is likely to be. The state-of-the-art systems\nare technically complex, requiring application of mathematical and algorithmic\ntechniques. These NLP systems are often described through diagrams. We have\nchosen to examine scholarly NLP system diagrams found in conference proceed-\nings, as a sub-domain of NLP systems diagrams with a well-deﬁned scope.\n2.2\nNLP System Components\nModern NLP systems are often based on neural networks, and it is these sys-\ntems we focus on. A neural network takes an input (in NLP, text), and then pro-\ncesses this via a series of layers, to arrive at an output (classiﬁcation/prediction).\nWithin each layer are a number of nodes that hold information and transmit sig-\nnals to nodes in other layers. Speciﬁc mathematical functions or operations are\nalso used in these systems, such as sigmoid, concatenate, softmax, max pooling,\nand loss. The system architecture describes the way in which the components\nare arranged. Diﬀerent architectures are used for diﬀerent types of activities. For\nexample Convolutional Neural Networks (CNN), inspired by the human visual\nsystem, are commonly used for processing images. Long Short Term Memory\nnetworks (LSTM), a type of Recurrent Neural Network (RNN) which are de-\nsigned for processing sequences, are often used for text.\nUnderstanding scholarly NLP system diagrams\n3\nThese neural networks “learn” a function, but have to be trained to do so.\nTraining consists of providing inputs and expected outputs, allowing the sys-\ntem to develop an understanding of how an input should be interpreted. The\nsystem is then tested with unseen inputs, to see if it is able to handle these\ncorrectly. System diagrams almost always depict the training process. A more\ndetailed introduction to LSTM architectures, including schematics, is provided\nby Olah [12].\n2.3\nNLP scholarly documents\nThe representation of these systems in conference proceedings is done in nat-\nural language, in pseudocode, and in diagrams which often appear to describe\nthe system beyond the neural network itself, including inputs, outputs, and the\nrelationship between components. Code may be shared as a supporting artifact\nexternal to the formal proceedings. System outputs are often shared as tables,\ncharts, and metrics. In order to consider diagrams separately from the text, we\nfocus on self-contained diagrams which were deemed to meaningfully exist with\nassumed knowledge but without reference to other resources.\n2.4\nNLP scholarly diagrams\nOverview In this domain, representation can be challenging and complex. As\nnoted by Mahoney [8] in engineering, there are signiﬁcation issues associated\nwith representing how things work rather than what they are:\n“But to show what machines do or how they are assembled is one thing;\nto show how they work is quite another. However accurately and fully a\ncomplex mechanism may be portrayed, an understanding of its operation\nas a whole rests ultimately on familiarity with the operations of its basic\ncomponents. Treatises of the genre under discussion took that familiarity\nfor granted. Their authors could not do otherwise, given the nature of\ntheir medium. A picture of a windlass, or of a system of pulleys, cannot\nin and of itself set forth the laws that deﬁne the device’s mechanical\nadvantage. A drawing of a closed tube standing in a pool of water and\nhaving a piston with a valve that opens in one direction only will still\nnot explain a water pump until the readers know the laws (or at least\nthe rules of thumb) that link the reduction of air pressure to the rise\nin the head of a column of liquid. Readers must bring knowledge or\nexperience of such matters to the illustrations in order then to appreciate\nor proﬁt from the ingenuity with which the basic machines are combined\nor adapted to particular circumstances.” [8, p.201]\nDiagrammatic representation and usage in this context lacks scientiﬁc in-\nvestigation. Indeed, scholarly diagrams more broadly have been neglected by\nscholarly enquiry, despite their potential to enhance the communicative eﬀec-\ntiveness of research outputs. An exception to this is within automated computer\nvision, where there is increasing interest in classifying scholarly ﬁgures [17], and\nparticularly scholarly charts of experimental results [14,15].\n4\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\nAnalysing scholarly diagrams in Computer Science In these diagrams,\napart from context related aspects such as diagram usage, two aspects of research\ninterest are (a) what is represented and (b) how it is represented. For examples,\nsee Figs. 2, 3, 4 and 5.\nWe are not aware of research applying existing diagram analysis frameworks\nto scholarly communication. One such framework is “Physics of Notations”,\nwhich is highly cited for designing visual notations [10]. This could potentially\nbe re-purposed as an analytic lens to describe scholarly-NLP-domain-speciﬁc\nphenomena, as it has been for investigating the impact of the addition of colour\nin UML Activity Diagrams found in Software Engineering [6]. However, Physics\nof Notations is fairly abstract, with categories such as “include explicit mecha-\nnisms for dealing with complexity”. We choose to progress with the Richards-\nEngelhardt framework due to its concreteness, its task-independence, and its\npotential for conducting systematic analysis. We also consider further develop-\nment of the framework as part of the analysis process.\n3\nApplication of the Richards-Engelhardt framework to\nthe NLP systems diagram domain\n3.1\nMethodology\nThe Richards-Engelhardt framework consists of three modes, ﬁve types, and 15\nprinciples of visual encoding, and provides a systematic method for analysing\ndiagrams and visualisations. In its present form the framework focuses on the\nprinciples of visual encoding, which we also focus on in our analysis. Within the\nvisualisation design space, system diagrams in NLP could broadly be considered\nﬂow diagrams, though they do not conform to a consistent or standard form.\nExamining self-contained diagrams found at ACL 2019, a top NLP conference,\nwe found a visually and semantically heterogeneous set of diagrams across the\nproceedings. From this set, we selected four diagrams to investigate further.\nThey were chosen to be distinct in terms of mode of visual encoding, and the\ninformation they convey.\n3.2\nAspects of visual representation in NLP systems diagrams\nWe apply the Richards-Engelhardt framework in the NLP system diagram do-\nmain. Our application area falls within Engelhardt and Richard’s scope of 2D\nstatic representations, though this is a novel and more complex area of applica-\ntion, as we shall demonstrate. We proceed to consider the three representational\nmodes, followed by a general discussion of visual encoding types in NLP, and\ndiscuss in depth visual encoding principles of four examples from this domain.\nMode of correspondence In NLP systems diagrams, and indeed all diagrams of\nsoftware, all visual encoding is necessarily non-literal, in that the relationship\nis conceptual and metaphorical. This is not trivial, since the functions (or code)\nUnderstanding scholarly NLP system diagrams\n5\nthat the system comprises have speciﬁc virtual inputs and outputs. It is also the\ncase that software architecture diagrams often do not represent how the code is\nwritten, but rather how the system works, being more about signifying function\nthan about representing code in a “realistic” sense.\nMode of depiction It would be challenging to describe NLP system diagrams on\na realistic/precise to schematic dimension. To highlight this, Fig. 1 shows some\nof the diﬀerent graphical components used in visual representations of vectors\n(which in this context are indexable lists of elements), a speciﬁc and prevalent\naspect of NLP systems. Some are more “schematic” that others, in the sense\nthat they use less ink to signify the same thing (a vector of unknown dimension).\nThere can also be visual concreteness (such as four circles in a vector, seen left\nof Fig. 1), which does not necessarily signify precision or semantic relevance (in\nthe example, there may not be four elements). In this domain, perhaps realistic\nand precise are fundamentally diﬀerent: Often where we ﬁnd a precise depiction,\nit does not depict precision.\nFurther speciﬁc discussion of depiction could include whether graphical com-\nponents should include “mathematical symbols”. There are other standard re-\ncurrent symbols, such as ellipses or domain speciﬁc iconography, which could be\ncaptured by sub-categories of “depicting”.\nFig. 1. Nine diﬀerent graphical components representing vectors found at ACL 2019.\nLeft to right: Circles: [Zang et al, ACL 2019]. Vector: [Lee et al, ACL 2019]. Example:\n[Wang et al, ACL 2019]. x1 circle: [Ma et al ACL 2019]. w1 square: [Sarkar et al, ACL\n2019]. Stacked squares: [Mai et al, ACL 2019]. Horizontal squares: [Mihaylova and\nMartins, ACL 2019]. Rectangle: [Barezi and Fung, ACL 2019]. Curved corners, index\nplus example [Zhang et al, ACL 2019]\nMode of visual encoding For “depicting”, the NLP system diagram, as a technical\nillustration, is picturing (rather than mapping), and this must be the key visual\nencoding depiction principle. Again due to the lack of physical location, we\nare not in the “mapping” category. With a more metaphorical, less physically\ngrounded, but equally rational deﬁnition, perhaps an NLP system diagram could\nbe considered a map of the functionality (cf. [4,13]). In other domains, a “process\nmap” would not ﬁt naturally within the deﬁnition of either mapping or depicting\ncategories.\nThe above domain-level features are applicable throughout diagrams used in\nthis domain. We proceed to use the guidance in Fig. 1 of [5] to apply the “types\nof visual encoding” to our domain. A collection of diagrams can be found at\n6\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\naidiagrams.com/resources, on which this more general commentary is based.\nExamining this larger sample of over 150 scholarly NLP system diagrams allows\nfor an awareness of the heterogeneity of the domain.\nVisual encoding types\nScaling This is variable between and within diagrams. Within a diagram, size can\nbe used to represent dimensional diﬀerences, often indicating a binary “bigger\nor smaller than”, rather than precise scaling (see Fig. 3).\nOrdering Often these diagrams are (broadly) read linearly left-right, in chrono-\nlogical order of data processing at training-time. The diagrams are not usually\nof the system at run-time. Some information important for the creation and\noperation of the system, including chronological information such as “parame-\nter training process”, intervals, epochs, and updating of parameters are often\nomitted.\nGrouping Varied, often multiple diﬀerent encodings are used within each dia-\ngram. Often, multiple visual encodings are applied to perform the same grouping\n(such as nesting, colour, proximity, alignment, and boundary, plus a label and a\ncaption, as shown in Fig. 2).\nLinking This is perhaps the most important part of the NLP system diagrams,\nsince along with components themselves, the relationship between components\ndetermines the architecture. Arrows and nesting are often used.\nIt remains to consider visual encoding principles, which we will through anal-\nysis of four example diagrams extracted from scholarly conference proceedings.\n3.3\nExample diagram prioritising “layers”: Fig. 2\nThe diagram in Fig. 2 prioritises depiction of the layers of the neural network\narchitecture, including labelled FC-Layers (Fully Connected layers, used for clas-\nsiﬁcation).\nColour and boundary are consistently employed simultaneously for group-\ning, together with labels-attached-by-proximity for the red and blue sub-ﬁgures.\nAlignment groups the right section as the main components of the system (com-\nprised of the FC-Layers). The green component is grouped by proximity to the\nother “early-in-the-workﬂow” item of context embedding.\nThis diagram contains 23 words, and of those ﬁve (over 20%) are “embed-\nding”. That does not include the caption, which (quite unusually) reemphasises\nthe labels by describing the visual techniques of the diagram, and clarifying a\nsymbol which is used in an unconventional way (and does not match the same\nsymbol in the diagram, “+” is neither visually nor semantically equivalent to\n“L”). It would be possible to rearrange this diagram with less natural lan-\nguage and the same semantic content, for example by making use of the Nesting\nUnderstanding scholarly NLP system diagrams\n7\nFig. 2. A layer-centric example [Wang et al, ACL 2019], with labels from the framework\noverlaid\nvisual encoding principle. This is an example of how an awareness of Richards-\nEngelhardt framework has the potential to help diagram authors with novel\nrepresentations. Engelhardt and Richards have already assessed hundreds of di-\nagram types which work well with their framework, so we focus our narrative\non diagrams for which it does not work so well, such as where the author makes\nunconventional and internally inconsistent visual encoding choices, where com-\nmunicative language is mixed, or where a visual encoding principle is used for\nemphasis.\n3.4\nExample diagram prioritising data manipulation: Fig. 3\nFig. 3 shows a data centric depiction of a neural network system. The dominating\nfeature (in terms of both ink and semantic content) are the grids which represent\ndata ﬂows. Note that two visual techniques (gradient and scaling) are used but\ndo not mean what the framework suggests. Colour gradient is not ordering (OG),\nbut rather is descriptive of dilution. Scaling by proportion (SP) occurs in the\nconcatenate layer, but is not what is meant by the use of Scaling in the local\nmax-pooling or convolutional layer. This diagram also showcases inconsistent\ndual-coding, highlighting the arbitrary dimensions used throughout this domain:\nThe sizing is not consistent with the dimensional labeling within each layer,\nwhich implies they are diﬀerent widths (such as the convolutional layer, which\nhas variable dimensions implied by the mathematical notation gm−2 and gm−4).\nThe framework does not allow us to discuss author-assumed conventions (e.g.\n8\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\nFig. 3. A data-centric example [Dereli and Saraclar, ACL 2019], with labels from the\nframework overlaid\nellipsis, acronyms, subscript notation). Also in this diagram we have arrows for\nthe last two layers but not elsewhere, and the linking is semantically diﬀerent.\nWith an implicit axis, it is understandable that the authors did not include\nexplicit arrows as is usual for system ﬂow. In Fig. 3, we found the caption to be\nvery helpful in interpreting the overall diagram. This motivates the inclusion of\nthe caption as part of the scope of the diagram, though they do not have the\nspatial relation required of a diagram and were not discussed in Engelhardt and\nRichards’ examples.\n3.5\nExample diagram with picturing and sub-ﬁgures: Fig. 4\nWe chose Fig. 4 due to its visual appeal, range of visualisation choices, and clear\nsub-ﬁgures allowing for easy reference without needing to overlay additional in-\nformation. With the depiction of chessboards, this is depiction by mapping, as\nwell as the dominant depiction by picturing. Fig. 4 has many diﬀerent grouping\nby boundary mechanisms, where the visual objects used to establish the bound-\nary are also grouped by colour and shape (wide brush with rounded corners,\nthin brush with rounded corners, thick brush with “folded” corner). Grouping is\ndone by position and alignment, particularly within the labelled sub-ﬁgures (a),\n(b) and (c). Additionally, proximity is used to conceptually attach the vectors\n(collection of circles) to the chess boards, certainly in the leftmost instance. The\nother instances have a dotted yellow boundary (if you look very closely!). It is\nnot clear why there is this inconsistency.\nUnderstanding scholarly NLP system diagrams\n9\nFig. 4. An example with sub-ﬁgures [Zang et al, ACL 2019], similar in size to the\noriginal, with labels from the framework overlaid\nThere are some visual inconsistencies which do not appear to represent mean-\ning. Indeﬁnite continuity of the process, in this case, is represented by one ellipsis,\nthat is aligned only with the rightmost chessboard. Other lines simply stop. It\nwould seem better to have the ellipsis positioned further right. Ellipses are also\nused within sub-ﬁgure (b) to indicate arbitrary dimension, which is consistent\nwith the other use. However, in sub-ﬁgure (a), the 4 circles of vector “ES”, the\nimplied four overlaid squares of “F”, and the size and number of bars in “D”,\nas well as the stacked bar charts v, are precise and arbitrary. In the text it is\nmade clear that the number of feature planes “F” is 20. It is not clear why dif-\nferent representational choices have been made for the same concept. The yellow\narrows are double-barred, further distinguishing them from the other coloured\narrows. The rightmost two arrows in (a) have a lower weight than any other\narrow, presumably an oversight. The overall ordering of the diagram is slightly\nunclear, with no single ﬂow directed by linking or position. This is further compli-\ncated by the sub-ﬁgures. The framework does not encompass languages involved,\nwhich can be argued to be a speciﬁc visual encoding, or at very least a special\ntype of graphical component. With example usage from Fig. 4, we have: English\n(“Quality”), mathematical (“ ˆm(0)”), Smith chess notation (“g1e2”). This would\nbe useful, to facilitate comparison between diagrams, as a diagram with a dif-\nferent language has a diﬀerent set of implicit assumptions made about the prior\nknowledge of the reader.\nThe size of the chessboards and the iconic sub-ﬁgures used within the main\ndiagram (blue and purple) vary, presumably for pragmatic size-constraint rea-\n10\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\nsons. It is also potentially distracting for readers that the scaling of the (blue,\npurple and green) icons preserved the size of the arrowheads (only), and that\npurple introduces curvature to arrows.\nThe framework has allowed a discussion of this multifaceted diagram. Perhaps\na framework omission in describing Fig. 4 is that this is semiotically diﬀerent to\nFigs. 2 and 3, in that the diagram shows a speciﬁc example (of the processing\nof a single chess move) which is used to signify the entire system.\n3.6\nExample prioritising a schematic of the contribution: Fig. 5\nFig. 5 can be thoroughly described using the Richards-Engelhardt framework,\nwhich we will brieﬂy do here without going into minutiae sub-ﬁgure discussion.\nThere is grouping by alignment, by colour and by position. It uses two dif-\nFig. 5. An example of a contribution-oriented schematic [Lebanoﬀet al, ACL 2019],\nwith labels from the framework overlaid\nferent linking graphical components (dotted lines and arrows). The content is\nquite schematic, with omission of details not important to the core contribution,\nsuch as layer labels and operations, as seen in the previous diagrams. Moody\net al. [11] would describe this ﬁgure as demonstrating good “graphic econ-\nomy”, whilst in Gestalt theory this would be “Pr¨agnanz” [18], language that\nthe Richards-Engelhardt framework appears to lack. The framework does not\nfacilitate discussion of the schematic and relatively minimalist content style, as\nthe mode of depiction has a physical deﬁnition. Currently, the framework has the\nmode of depicting being schematic or realistic/precise, but this does not support\nthe examination of individual graphic objects which is necessary for comparing\nbetween diagrams (see Fig 1).\nUnderstanding scholarly NLP system diagrams\n11\nThere are some important features that the framework does not currently in-\nclude, such as the bracket object being used for grouping, which might be termed\nGrouping by Object. The visual encoding here diﬀers from other examples, in that\nthe set of graphic components is taken from a smaller pallet: This is a simpler\ndiagram by design. Unusual styling is present, in the graded colouring within the\nrectangles (it is not clear if this serves an encoding purpose). The rotated text\nalso is visually notable, and may require head-tilting which physically changes\nthe readers interaction with the diagram. The use of an icon to accompany “In-\nput Document(s)” is a dual-coding unexpected given the schematic nature of the\nrest of the diagram. Note that the system inputs are data rather than physical\npieces of paper, so the mode of correspondence remains non-literal. Moody [10]\nwould describe this dual-coding as “semiotically unclear”, as there is not a\n1:1 mapping between semantic constructs and graphical symbols. The precision\nof the diagram is also arbitrary; the numbers are indicative rather than mean-\ningful, as are the number of rectangles. The numbers allow inference that the\nhighest value is selected. The dotted lines suggest the network is Fully Con-\nnected, though there is one connection omitted top left to bottom right in the\nred network, perhaps accidentally. We ﬁnd Emphasis by Colour is used as well\nas Grouping by Colour. More generally, even within a simple example such as a\npie chart, colour, spatial separation, and orientation or layout techniques can be\nused for emphasis. Semiotically, the object being signiﬁed is variable within this\ndiagram, with sentence-speciﬁc numeric information alongside the system-level\n“document(s)”. Indeed more broadly this diagram appears to be representing\nthe “contribution” rather than the “system”.\nWhere the authors reference the diagram in the text, it is described as an\n“illustration” [7], perhaps reﬂecting that the mode of depiction is schematic\nrather than precise.\n4\nResults and discussion\nThe scholarly NLP system diagram domain is substantially more complex than\nthe examples initially considered by Engelhardt and Richards, making intelligi-\nble implementation of the visual methods of their Fig. 2 challenging. This visual\nmethod has a table of visual encoding principles which was used to point at the\ninstantiation of that principle. To clearly apply to complicated system diagrams\nfollowing this visual method would seem to warrant an interactive digital vi-\nsualisation rather than static diagram, due to the multiple applications of the\nsame principles. In particular, unlike our domain, the examples the framework\nwas tested on do not have extensive “subsets of graphical components” (sub-\nﬁgures), and often employ a fairly limited number of visual encoding principles.\n4.1\nStrengths of applying the framework\nWe found it extremely useful to have the vocabulary and breadth of consider-\nations provided by the framework, particularly around the graphical elements.\n12\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\nIt has successfully and usefully described how diﬀerent principles are used to\ngroup and link, within diagrams in this domain. In general, we found the visual\nencoding principles unambiguous and straightforward to identify.\nApplying this framework has also allowed us to discover internal inconsis-\ntency in some of the diagrams, and a number of errors in the creation of the\ndiagrams appear to have been found. Perhaps authors and reviewers would ben-\neﬁt from examining diagrams with a critical eye, using Engelhardt and Richards\nVisual Encoding Principles.\n4.2\nReﬁnements and extensions to make this a useful tool for\nreﬂection on (scholarly NLP) system diagrams\nWhilst, as the prior work of this paper attests, applying this framework was\nuseful, there are a number of items which the framework requires for utility in\nour domain. It would be helpful to have standard terminology for some additional\nattributes of diagrams as part of the mode of depiction.\nAuthor assumed knowledge and conventions Assumptions made by the author,\nsuch as use of acronyms, mathematical notation, or context-speciﬁc conventions,\nare not reﬂected here. This may also usefully be considered as part of visual en-\ncoding in a general domain. We believe that, even in Engelhardt and Richards’\nown example (their Fig. 2) the assumed knowledge of Dutch language, and politi-\ncal acronyms, are important parts of the visual encoding. This may be signiﬁcant\nenough to require calling out separately as part of “Depicting”.\nInternal consistency For complex diagrams, “consistency” may be important.\nThis does not only apply to how visual encoding principles are used, but also in\nnatural language, consistency of capitalisation or use of symbols (e.g. Fig. 2 uses\nellipsis sometimes for arbitrary dimension, some lower case ﬁrst letters, and even\nthe “+” symbol is inconsistent between the caption and the diagram). Regardless\nof whether internal inconsistency is cognitively problematic, it is necessary to\ncapture this in order to describe the diagram, and would provide an entry point\nto discussing diﬀerences in representational choice within the diagram.\nLanguage The choice of communication modes such as mathematical symbols,\ncode, abbreviations, “iconic” symbols, or natural language is signiﬁcant to the\nencoding. Whilst these can be considered as a subset of graphic components it\nperhaps does not do justice to the cognitive diﬀerence using these makes [16,2].\nThis could be structured, and consideration made of tone, language etc. This\nis a speciﬁc case of “Author assumed knowledge and conventions”, but also is\na diﬀerent modality with diﬀerent perceptual and semiotic considerations, and\ntherefore seems worthy of speciﬁc attention.\nSemiotics of examples vs systems As part of the future work, it would be good\nto include semiotic or representational considerations (keeping within the frame-\nwork’s scope of “meaning represented in a diagram”). For example, if a systems\nUnderstanding scholarly NLP system diagrams\n13\ndiagram includes an example, as in Fig. 4, this shifts the diagrammatic repre-\nsentamen to being a speciﬁc instantiation of the system.\nCommon domain-speciﬁc symbols Graphic components: within either “mode of\ndepicting” or “visual encoding: depicting”, it would be useful to have graphical\ncomponents categorised to describe common symbol choices when comparing di-\nagrams. Further, it is noted that for the visual encoding principles, quantitative,\nordinal and nominal attributes are covered. The “depicting: picturing” can be\nextended to include other properties of the object that are in a general sense\niconic, which would allow us to describe (for example) the diﬀerences in vector\nrepresentations shown in Fig. 1.\nSub-ﬁgures In our domain, sub-ﬁgures (such as the layers) sometimes contain\ndiﬀerent representational choices to the “macroscopic” diagram. Engelhardt and\nRichards consider this (p204), and consider them to be nested visual structures,\nwhich we have been abbreviating as sub-ﬁgures. This indicates the utility of this\nframework at diﬀerent levels of granularity. For discussion of complex diagrams,\nit might be suitable to add an external axes to indicate position of sub-ﬁgures and\nfeatures, rather than overlaying this information onto the diagram. We would\nwelcome discussion on other ideas for this.\nSchematics The framework does not capture the schematic (or otherwise) na-\nture of the diagram, making comparison or discussion on “How much content is\nincluded or omitted” diﬃcult. This applies not only in the mode of depiction,\nbut also as part of the mode of visual encoding, as demonstrated by the wide\nrange of graphical elements representing vectors in Fig. 1.\nPhysical emphasis For non-physical systems, dimensions of discussion are re-\nduced to being “non-literal” correspondence with “realistic/precise” depiction.\nIt would be nice to have more language to help describe the use of non-physical\nmetaphor and models. This could be part of the content work.\nQuantitative analysis Allowing for quantitative discussion of diagrams at scale\nis challenging, particularly for complicated diagrams. This capability would be\nuseful in order to do justice to the heterogeneity of visual encoding techniques\nemployed, and allow for quantitative comparison (say, between diﬀerent domains\nor media). This would seem to be a core beneﬁt of covering the whole design\nspace of visualisations, and as such an extension to the framework to support\nquantitative analysis of visual encoding principles would be helpful.\nGrouping by Object In encoding by visual appearance, we found graphical com-\nponents typiﬁed by “{”, and natural language labels, being used to group other\ngraphical components. Whilst it could be argued this is a grouping by proximity\nor boundary, this does not capture that the visual object “{” is used with spatial\ngrouping to visually encode “nesting” at diﬀerent levels of grouping. The right\nbrackets of Fig. 5 is one such example of “{” usage. In this example, it is to\n14\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\ngroup together individual rows by “Input Document(s)”, “1st Summ Sent” or\n“2nd Summ Sent”, where spatial grouping is used to link the coloured rectangu-\nlar objects with the descriptions. The “1st Summ Sent” grouping would not be\nclearly only applied to the last three coloured rectangles without the mediating\n“{” symbol. As such, we claim that Grouping by Object is as diﬀerent from other\ngrouping mechanisms as a sheepdog is from a fence. Note that the “{” symbol\nshould not be considered a “linking” symbol, as an arrow might be, as it applies\na common property to a set of other objects and is therefore about “category”,\nwhich Engelhardt and Richards have deﬁned to be the goal of grouping (p206).\n(This “type” of grouping is often applied only after other visual encoding prin-\nciples have been applied. It may be this is because it does not leverage Gestalt\nto the extent of the other Grouping principles.)\nEmphasis Principle Additionally we propose “Emphasis” as a worthwhile ex-\ntension to the visual encoding principles. It is most similar to the principle\n“Ordering”, but fulﬁls a diﬀerent function:\n• Emphasising is used to make visually salient particular aspects. Empha-\nsising answers questions such as What is most important? and What should\nthe reader look at ﬁrst?\n– Emphasis by colour is a visual encoding principle often utilising bright\nor high contrast colours.\n– Emphasis by position is done by placing visual primacy to certain\nelements, for example with a prominent position (extreme left, right\nor central), or providing lower spatial clustering with respect to other\ngraphic elements or sub-ﬁgures.\n– Emphasis by uniqueness is done by using a unique visual encoding\nprinciple for an element (or sub-ﬁgure). This could be a diﬀerent bound-\nary, colour, or graphical elements.\nAmbiguities and other observations\n• The scope of the framework, in terms of describing diagrams, was quite clear.\nWe found it easy and clear to apply the visual encoding principles. However,\nit was unclear whether the caption should be considered as “part” of the\ndiagram, and it would be good if that could be clariﬁed as part of the future\ncontent work.\n• We found that, even in our ecological setting, this worked well for stan-\ndardised diagrams, and novel-but-thoughtful diagrams. The framework lends\nitself to diagrams that are consistently assembled. However, it lacks the vo-\ncabulary to describe errors or misleading visual encodings, which would be\nuseful as part of validating this framework and providing feedback to dia-\ngram authors, a potential use case of this framework.\n• The vocabulary enables standard terminology to describe visual features of\ndiagrams, and has been useful in describing ﬁgures in our domain. However,\nit does not yet provide guidance for semantic-content-focused vocabulary,\nUnderstanding scholarly NLP system diagrams\n15\nand borderline topics such as graphic economy, and semiotics. The deﬁni-\ntion of “mode of depiction” also appears to need clariﬁcation in order to\ndisambiguate precise/realistic and describe schematics and conventions.\n• The framework has facilitated description and discussion of complex systems\ndiagrams, including sub-ﬁgures. However, the framework is not yet optimised\nfor sub-ﬁgures, or for describing the layout of the diagram. Sometimes we\nhave to infer the intention or meaning of the author, and clariﬁcation is only\npossible outside of the diagram (e.g. in text or speech).\n• For “positioning along an axis”, we have assumed this axis does not need\nto be explicitly drawn, though for the infographics domain perhaps this\ndistinction is useful. Either way, the framework allows for useful discussion\non this topic. More generally, we felt that more precise deﬁnitions would be\nhelpful for meaningful and unambiguous application of the framework.\n• In our domain, Authors sometimes seem to use visual encoding mechanisms\nin unconventional ways (cf. colour gradient in Fig. 3). This is perhaps not\nunusual in diagrams more broadly, and the framework would beneﬁt from\nterminology for this.\n• Additional research that could be linked to this framework includes visual\nontologies, which aim to completely describe objects and the relations be-\ntween them. Of particular relevance are those from image processing and\nmachine vision (such as [9]), which have the potential to automate the ap-\nplication of this framework.\n4.3\nDiscussion on scholarly NLP system diagrams\nThere is an overwhelming absence of relevant, established representations or\nvisual notations for scholarly systems diagrams: As background for this study,\nwe randomly sampled over 150 diagrams from top NLP conference proceedings.\nNone utilised standards of UML or Block diagrams, nor used a diagrammatic\noutput from the software. These established diagrammatic frameworks could, in\nprinciple, be used in the domain. We hypothesise that they are not due to their\ninability to express the new representational challenges of neural networks, such\nas the dimensional change, embedding, layers, etc.\nThere is an as yet un-examined semiotic diﬀerence between a system instan-\ntiated around an example, and one describing a general system. The former\ndescribes a single operation, rather than a system, though the meaning com-\nmunicated may or may not be similar. Most diagrams we found represent the\nsystem, but some represent data pre-processing, data itself, or an example.\nNLP systems can be created through visual programming languages, includ-\ning specialist interfaces such as TensorFlow [1] which has the TensorBoard visu-\nalisation tool for automatically creating a system diagram. We did not ﬁnd any\ncases of this being used in conference proceedings. We hypothesise this is because\na large proportion of research is done using other tools, and also because the\nlevel of granularity is too low. It may be that an additional, higher-level view on\nTensorBoard would facilitate system architecture diagrams that would be suit-\nable for immediate export into conference proceedings. Repeatability, the ability\n16\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\nto reproduce work, is a problem in Computer Science [3] and better diagrams\ncould be part of improving system transparency and lead to better repeatability\nand reproducibility.\nWe found that, even in very visually and semantically diﬀerent diagrams, al-\nmost every Richards-Engelhardt grouping and ordering technique was deployed.\nThis makes intuitive sense, as descriptions of software systems are principally\nabout grouping and ordering. The prevalence of “Repetition”, despite the use of\nvarying graphic components for vectors and indeed the indeterminate nature of\neach one of the repetition uses, was unexpected. In conjunction with repetition,\nsome diagrams used the ellipsis symbol “. . . ” to indicate continuity.\n4.4\nLimitations and future work\nUsing this framework, there are numerous avenues to investigate, including de-\nscribing and comparing diagrams. These observations are in addition to the\npotentials described by Engelhardt and Richards.\nOther system diagrams, such as engineering drawings or those deployed in the\ndiscipline of Systems Theory, may beneﬁt from examination with this framework.\nAdditionally, there is the potential to investigate other ecological or “organically\ncreated” diagrams, such as biology school textbook diagrams or diagrams used\nin journalism, and to examine temporal or evolutionary diﬀerences in diagrams\noccurring in diﬀerent domains.\nIt would be useful to investigate perceptual properties of diﬀerent visual\nencoding principles. Speciﬁcally for NLP system diagrams, a useful example to\nstudy would be repetition, including whether an ellipsis or a diﬀerent number\nof repeated symbols has an impact on interpretability. Designing an experiment\nthat respects the need to be distinct from any particular task would make this\nmore challenging. Another relevant topic for our domain would be the perceptual\nand cognitive impact of simultaneous application of multiple visual encodings\nfor grouping.\nFurther, contrasting the Richard-Engelhardt framework with domain spe-\nciﬁc tools, such as Physics of Notations for Software Engineering, might provide\njustiﬁcation or insight into potential changes to those domain-speciﬁc guidelines.\n5\nConclusion\nTo our knowledge, this is the ﬁrst work to consider scholarly diagrams, as well as\nNatural Language Processing system diagrams, and amongst the ﬁrst to apply\nthe Richards-Engelhardt framework. We have successfully applied the framework\nin order to describe and discuss Natural Language Processing systems diagrams,\nidentifying unconventional visual encoding choices and inconsistencies in the\ndiagrams.\nWe have identiﬁed modiﬁcations and additions that would make the appli-\ncation of this framework more useful in our domain, and perhaps for systems\ndiagrams more generally. We have also demonstrated that scholarly diagrams\nare an interesting avenue for investigation by the diagrammatic community.\nUnderstanding scholarly NLP system diagrams\n17\nReferences\n1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\nG.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,\nIrving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\nJ., Man´e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J.,\nSteiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\nVi´egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,\nX.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\nhttps://www.tensorflow.org/, software available from tensorﬂow.org\n2. Boon, M., Knuuttila, T.: Models as epistemic tools in engineering sciences. In:\nPhilosophy of technology and engineering sciences, pp. 693–726. Elsevier (2009)\n3. Collberg, C., Proebsting, T.A.: Repeatability in computer systems research. Com-\nmunications of the ACM 59(3), 62–69 (2016)\n4. Dodge, M., Kitchin, R.: Atlas of cyberspace, vol. 158. Addison-Wesley London\n(2001)\n5. Engelhardt, Y., Richards, C.: A framework for analyzing and designing diagrams\nand graphics. In: International Conference on Theory and Application of Diagrams.\npp. 201–209. Springer (2018)\n6. Gopalakrishnan, S., Krogstie, J., Sindre, G.: Adapting uml activity diagrams for\nmobile work process modelling: experimental comparison of two notation alterna-\ntives. In: IFIP Working Conference on The Practice of Enterprise Modeling. pp.\n145–161. Springer (2010)\n7. Lebanoﬀ, L., Song, K., Dernoncourt, F., Kim, D.S., Kim, S., Chang, W., Liu, F.:\nScoring sentence singletons and pairs for abstractive summarization. arXiv preprint\narXiv:1906.00077 (2019)\n8. Mahoney, M.S.: Diagrams and dynamics: mathematical perspectives on edgerton’s\nthesis. Science and the Arts in the Renaissance pp. 198–220 (1985)\n9. Maillot, N., Thonnat, M., Boucher, A.: Towards ontology-based cognitive vision.\nMachine Vision and Applications 16(1), 33–40 (2004)\n10. Moody, D.: The “physics” of notations: toward a scientiﬁc basis for constructing vi-\nsual notations in software engineering. IEEE Transactions on software engineering\n35(6), 756–779 (2009)\n11. Moody, D.L., Heymans, P., Matuleviˇcius, R.: Visual syntax does matter: improv-\ning the cognitive eﬀectiveness of the i* visual notation. Requirements Engineering\n15(2), 141–175 (2010)\n12. Olah, C.: Understanding lstm networks. https://colah.github.io/posts/2015-08-\nUnderstanding-LSTMs/ (2015), accessed: 2019-10-16\n13. Peterson, J.B.: Maps of meaning: The architecture of belief. Routledge (2002)\n14. Ray Choudhury, S., Giles, C.L.: An architecture for information extraction from\nﬁgures in digital libraries. In: Proceedings of the 24th International Conference on\nWorld Wide Web. pp. 667–672. ACM (2015)\n15. Savva, M., Kong, N., Chhajta, A., Fei-Fei, L., Agrawala, M., Heer, J.: Revision:\nAutomated classiﬁcation, analysis and redesign of chart images. In: Proceedings of\nthe 24th annual ACM symposium on User interface software and technology. pp.\n393–402. ACM (2011)\n16. Scanlan, D.A.: Structured ﬂowcharts outperform pseudocode: An experimental\ncomparison. IEEE software 6(5), 28–36 (1989)\n17. Siegel, N., Horvitz, Z., Levin, R., Divvala, S., Farhadi, A.: Figureseer: Parsing\nresult-ﬁgures in research papers. In: European Conference on Computer Vision.\npp. 664–680. Springer (2016)\n18\nGuy Clarke Marshall, Caroline Jay, and Andr´e Freitas\n18. Wertheimer, M.: Untersuchungen zur Lehre von der Gestalt. II. Psychologis-\nche Forschung 4(1), 301–350 (1923). https://doi.org/10.1007/BF00410640, http:\n//link.springer.com/10.1007/BF00410640\n",
  "categories": [
    "cs.HC",
    "cs.AI",
    "cs.CL"
  ],
  "published": "2020-08-26",
  "updated": "2020-08-26"
}