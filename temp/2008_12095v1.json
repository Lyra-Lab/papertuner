{
  "id": "http://arxiv.org/abs/2008.12095v1",
  "title": "Document-editing Assistants and Model-based Reinforcement Learning as a Path to Conversational AI",
  "authors": [
    "Katya Kudashkina",
    "Patrick M. Pilarski",
    "Richard S. Sutton"
  ],
  "abstract": "Intelligent assistants that follow commands or answer simple questions, such\nas Siri and Google search, are among the most economically important\napplications of AI. Future conversational AI assistants promise even greater\ncapabilities and a better user experience through a deeper understanding of the\ndomain, the user, or the user's purposes. But what domain and what methods are\nbest suited to researching and realizing this promise? In this article we argue\nfor the domain of voice document editing and for the methods of model-based\nreinforcement learning. The primary advantages of voice document editing are\nthat the domain is tightly scoped and that it provides something for the\nconversation to be about (the document) that is delimited and fully accessible\nto the intelligent assistant. The advantages of reinforcement learning in\ngeneral are that its methods are designed to learn from interaction without\nexplicit instruction and that it formalizes the purposes of the assistant.\nModel-based reinforcement learning is needed in order to genuinely understand\nthe domain of discourse and thereby work efficiently with the user to achieve\ntheir goals. Together, voice document editing and model-based reinforcement\nlearning comprise a promising research direction for achieving conversational\nAI.",
  "text": "Document-editing Assistants and Model-based Reinforcement Learning\nas a Path to Conversational AI\nKatya Kudashkina1,2,3,4, Patrick M. Pilarski3,4, Richard S. Sutton3,4\n1University of Guelph, 2Vector Institute for Artiﬁcial Intelligence,\n3University of Alberta,\n4Alberta Machine Intelligence Institute\nCurrently under review. Corresponding author: kudashki@ualberta.ca\nAbstract\nIntelligent assistants that follow commands or answer sim-\nple questions, such as Siri and Google search, are among\nthe most economically important applications of AI. Future\nconversational AI assistants promise even greater capabili-\nties and a better user experience through a deeper under-\nstanding of the domain, the user, or the user’s purposes. But\nwhat domain and what methods are best suited to research-\ning and realizing this promise? In this article we argue for\nthe domain of voice document editing and for the methods\nof model-based reinforcement learning. The primary advan-\ntages of voice document editing are that the domain is tightly\nscoped and that it provides something for the conversation to\nbe about (the document) that is delimited and fully accessi-\nble to the intelligent assistant. The advantages of reinforce-\nment learning in general are that its methods are designed\nto learn from interaction without explicit instruction and that\nit formalizes the purposes of the assistant. Model-based rein-\nforcement learning is needed in order to genuinely understand\nthe domain of discourse and thereby work efﬁciently with the\nuser to achieve their goals. Together, voice document editing\nand model-based reinforcement learning comprise a promis-\ning research direction for achieving conversational AI.\nKeywords: conversational AI, intelligent assistants, dialogue\nsystems, human-computer interaction, reinforcement learn-\ning, model-based reinforcement learning\nThe ambition of AI research is not solely to create in-\ntelligent artifacts that have the same capabilities as peo-\nple; we also seek to enhance our intelligence and, in par-\nticular, to build intelligent artifacts that assist in our intel-\nlectual activities. Intelligent assistants are a central compo-\nnent of a long history of using computation to improve hu-\nman activities, dating at least back to the pioneering work\nof Douglas Engelbart (1962). Early examples of intelli-\ngent assistants include sales assistants (McDermott 1982),\nscheduling assistants (Fox and Smith 1984), intelligent tu-\ntoring systems (Grignetti, Hausmann, and Gould,Anderson,\nBoyle, and Reiser 1975,1985), and intelligent assistants for\nsoftware development and maintenance (Winograd, Kaiser,\nFeiler, and Popovich 1973, 1988). More recent examples\nof intelligent assistants are e-commerce assistants (Lu and\nSmith 2007), meeting assistants (T¨ur et al. 2010), and sys-\ntems that offer the intelligent capabilities of modern search\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nengines (Fain and Pedersen, Thompson, Croft, Metzler, and\nStrohman 2006, 2006, 2010). Building intelligent assistants\nhas been positioned as one of the key areas of development\nin AI (Waters 1986).\nToday, there are already economically important con-\nsumer products based on intelligent assistants. Intelligent as-\nsistants built on voice interaction, such as Amazon Alexa,\nGoogle Personal Assistant, Microsoft Cortana, Apple Siri,\nFacebook Portal, are among the most important applications\nof artiﬁcial intelligence today. These voice personal assis-\ntants help people with many daily tasks, such as shopping,\nbooking appointments, setting timers, and ﬁltering emails.\nThe economic value of these systems is created by gener-\nating revenue from selling not only hardware devices, such\nas smart speakers, but also voice application features, in the\nform of sales of digital goods as a one-time purchase, sub-\nscriptions, or consumables. The revenue from voice personal\nassistants alone was forecast to grow from $1.6 billion in\n2015 to $15.8 billion in 2021 (Tractica 2016). It is hard to\nthink of another area in AI that has more immediate social\nand economic impact.\nConversational and Purposive Assistants\nToday’s voice assistants are fairly limited in their conversa-\ntional abilities and we look forward to their evolution toward\nincreasing capability. Smart speakers and voice applications\nare a result of the foundational research that has come to\nlife in today’s consumer products. These systems can com-\nplete simple tasks well: send and read text messages; an-\nswer basic informational queries; set timers and calendar\nentries; set reminders, make lists, and do basic math cal-\nculations; control Internet-of-Things-enabled devices such\nas thermostats, lights, alarms, and locks; and tell jokes and\nstories (Hoy 2018). Although voice assistants have greatly\nimproved in the last few years, when it comes to more com-\nplicated routines, such as re-scheduling appointments in a\ncalendar, changing a reservation at a restaurant, or having a\nconversation, we are still looking forward to a future where\nassistants are capable of completing these tasks. Are today’s\nvoice systems “conversational”? We say that intelligent as-\nsistants are conversational if they are able to recognize and\nrespond to input; to generate their own input; to deal with\nconversational functions, such as turn taking, feedback, and\nrepair mechanisms; and to give signals that indicate the state\nof the conversation (Cassell 2000). It is our ambition to\nachieve a conversational AI assistant that demonstrates these\nproperties, communicates in free-form language, and con-\ntinuously adapts to users’ changing needs, the contexts they\nencounter, and the dynamics of the surroundings. This con-\nversational AI assistant would understand the domain within\nwhich it is assisting and provide appropriate support, as well\nas a pleasant experience for its users. This kind of assis-\ntant has not been developed yet and requires more research.\nConversational AI is a primary goal along the path toward\ncreating intelligent assistants in general. Achieving conver-\nsational AI would lead to even better intelligent assistants—\nintelligent assistants that have a genuine, deeper understand-\ning of their domains and users, helping people to achieve\ntheir goals.\nKey to the effectiveness of an intelligent assistant is that\nit is able to understand the higher-level goals of a task when\nassisting its users. Hawkins (1968) deﬁned a goal or a pur-\npose as a future state that is brought about through instru-\nmental control of a choice of actions among alternatives.\nThese higher-level goals or purposes are the reasons for\ncompleting a task. These purposes motivate and inﬂuence\nsmaller intermediate goals. For example, if a primary goal\nis ﬂying to San-Francisco, then intermediate goals can be\npurchasing ﬂight tickets and packing the luggage. We use\nthe word “purpose” to refer to the combination of the high-\nlevel context of a task and the user’s goals and use the words\n“agent” and “assistant” interchangeably. A user interacting\nwith an intelligent assistant has purposes related to the user’s\ntask. An assistant that understands users’ purposes and has\nits own purposes is a purposive intelligent assistant. Every-\none who has worked on intelligent assistants has recognized\nthe importance of agents’ understanding of purposes, yet\nan assistant that carries this property has not been devel-\noped. Building learning systems that genuinely understand\npurposes has been talked about for decades (Lindgren, Sun,\nChen, and Rudnicky, Serban et al. 1968, 2016, 2017b) and\nis still ahead on the AI research road map in fulﬁlling the\npromise of assisting people.\nUnderstanding users’ purposes is key not only for intel-\nligent assistants in general but, in particular, for conversa-\ntional AI agents. Purpose understanding is important for\nvoice assistants in serving their users and adjusting to the\nusers’ unique preferences. Imagine a user who wants to\nhave a business meeting with a client and interacts with a\nmeeting-booking assistant. In this example, the purpose is\na successful business meeting. The user’s initial ask is to\nbook a lunch reservation at a French restaurant for the pur-\npose of the meeting, which triggers an intermediate goal:\nto make the reservation. A meeting-booking assistant that\nunderstands the purpose may suggest an Italian restaurant\ninstead. This is because the assistant has information that\nthe Italian restaurant is quieter and better suited for business\nmeetings, even though the assistant realizes that the sug-\ngested restaurant is not a requested French type. The user\nmay accept the intelligent assistant’s suggestion, believing\nthat it might be even better than their initial ask because the\nsuggestion supports the primary purpose, the meeting. The\nintelligent assistant’s awareness of the purpose is what mo-\ntivates and drives this scenario. Another example of such a\npurposive intelligent assistant is a robotic arm or other ma-\nnipulation device controlled by a human user (e.g., a pros-\nthesis or an industrial robot). Take the case of a human-\ncontrolled robotic assistant picking up an object on a chess\nboard or pushing it; both tasks would require different mo-\ntions or other situation-speciﬁc actions from the assistant.\nIf the agent understands the context when assisting with a\ntask, then it would know how to implement a small direct\nmove, like advancing a pawn on the chess board, or an indi-\nrect move, like deploying a knight, with minimal delays and\nmicromanagement on the user’s part. The assistant creates a\nsmoother user experience by making better choice of actions\nwhen being purposive.\nA purposive intelligent assistant improves its capabilities\nfurther when it develops goals or purposes of its own. This\nconcept is close to an idea where agent-based adjustable au-\ntonomy is not prescribed by the user (see Maheswaran et\nal., 2003). An example is an iRobot Roomba that cleans a\nhouse. Roomba could set a goal of not hurting itself. This in-\ntelligent assistant not only seeks and adjusts to the user and\ntheir preferences, but also develops and sets its own goals\nand purposes. It is important that the user should be aware of\nthe intelligent assistant’s goals. This awareness would help\nthe user to adjust their preferences, contributing to a more\nharmonious interaction. This adjustment between the assis-\ntant and the user increases the assistant’s capabilities result-\ning in a better user experience.\nThe Challenge of Conversation\nEfforts to build voice assistants that learn purposes are\npresent not only in modern dialogue systems but go back\nthrough four decades of incremental research and develop-\nment (Carbonell, Winograd, Simmons and Slocum, Power,\nBruce,Walker and Grosz,Cohen,Allen,Pollack, Hirschberg,\nand Webber, Grosz, Woods, Finin, Joshi, and Webber, Car-\nberry, Moore and Paris, Smith and Hipp, Kamm 1971,\n1971,1972,1974,1975,1978,1978,1979,1982–1984,1986,\n1989, 1989, 1994, 1995). Achieving this goal calls for large\namounts of computational power, a great deal of engi-\nneering effort to overcome architectural challenges, and\ncontinual human-machine interaction to produce the data.\nSHRDLU (Winograd 1971) was an early, well-known dia-\nlogue system that responded to instructions and moved ob-\njects in a simulated world. Naively, one could believe that\nSHRDLU demonstrated understanding of purposes; how-\never, today it is known that symbolic manipulation systems\nwould fail in the ambiguous situations humans encounter in\nreal-world settings.\nWe separate modern dialogue systems into three cate-\ngories to illustrate their closeness to purposive intelligent\nassistants. The ﬁrst category is entertainment systems that\nprovide open-domain conversations (see Huang, Zhu, and\nGao, 2020). These are chatbots and systems that bring a\nsense of companionship (e.g., Quarteroni and Manandhar,\n2009; Nonaka et al., 2012; Higashinaka et al., 2014; Li et\nal., 2016a; Smith et al., 2020; Zhou et al., 2020), mostly\nimplemented with sequence-to-sequence models (Sutskever,\n2\nVinyals, and Le 2014) or retrieval-based methods, which\nselect responses from an existing pre-deﬁned repository. It\nis more difﬁcult to develop purposive intelligent assistants\nin this category because the user’s purpose does not result\nin a concrete output, and thus is even less ﬁrm or clear\nthan in other categories. The second category is text-based\ninstruction systems that are primarily represented by text-\nbased games (e.g., He et al., 2015; Kaplan, Sauer, and Sosa,\n2017; Goyal, Niekum, and Mooney, 2019). These dialogue\nsystems are closer to learning purposes. An example of the\nwork advancing in this direction is the use of natural lan-\nguage instructions by Goyal, Niekum, and Mooney (2019).\nThe third category is task-oriented systems that help users\nwith particular tasks (see Liu, 2018), such as ﬁnding a prod-\nuct, booking a reservation, or call classiﬁcation (e.g., T¨ur,\nHakkani-T¨ur, and Schapire, 2005; Bapna et al., 2017). Task-\noriented systems category is the closest to our deﬁnition of a\npurposive intelligent assistant, yet the delivery of the task by\nthese systems still depends on pre-designed slots and tem-\nplates (He et al., Zhao and Eskenazi, Lipton et al., Liu et\nal.,Goyal, Niekum, and Mooney,Gupta et al.,Zhou, Arnold,\nand Yu 2015,2016,2018,2018,2019,2019,2019) or a hand-\ncrafted series of commands that come with if-else conditions\nand rules.\nModern dialogue systems have rapidly advanced in the\nlast few years, but remain limited in their ability to learn pur-\nposes. The introduction of deep learning techniques through\nthe sequence-to-sequence network by Sutskever, Vinyals,\nand Le (2014) combined with the massive amount of data\nand computational power produced amazing results with the\nrecent system GPT-2 (Radford et al. 2019) being a prime ex-\nample. But is GPT-2 a step toward purposive intelligent as-\nsistants? We make a simple distinction and clarify that it is a\nconﬁned language model that predicts the next word, given\nall the previous words within some text. GPT-2 is an amaz-\ning engineering effort that deserves special recognition. The\nlimitation is that the system’s answering capabilities rely on\nword-by-word prediction, and not on genuine understanding\nof a domain or users’ goals. Today’s dialogue systems have\ncome a long way yet continue to remain somewhat scripted,\noften using a limited number of pre-deﬁned slots and canned\nresponses.\nWhy is it so difﬁcult to get to the goal that would advance\ntoday’s research community’s answers to the challenges of\nconversational AI and to develop agents that learn purposes?\nGeneral methods, such as supervised learning, that scale\nwith increased computation, continue to build knowledge\ninto our agents. However, this built-in knowledge is not\nenough when it comes to conversational AI agents assist-\ning users, especially in complicated domains. One of the\nbiggest problems in conversational AI is the limitlessness\nof domains, which leads to high expectations of conversa-\ntional agents. In a general conversation, an intelligent assis-\ntant is expected to know everything that a human conversa-\ntional partner might know and, in some cases, also special-\nized user-related or world-related information. For example,\nthe agent is expected to know about relevant aspects or pat-\nterns in the wider environment and users’ lives, such as what\nit means to have a schedule. In other words, the agent is ex-\npected to know about the world of its user, and a user can\npotentially ask the agent anything. The research community\nhas an ultimate goal to have intelligent assistants that are\nable to provide support as effectively as expert human as-\nsistants can, but high expectations with respect to an agent’s\nknowledge about the world and the users are an obstacle on\nthe path to getting there.\nWe propose that what is really needed is that the domain is\ntightly scoped and fully accessible to the intelligent assistant,\nso that the assistant can fully understand it. This leads to the\nquestion: Is there a domain in which assistants can have fo-\ncused conversations with their users and be helpful without\nknowing everything about the rest of the world but knowing\neverything about what they have to assist with? We now pro-\npose such a domain that allows us to accelerate and advance\nthis ﬁeld without immediately solving the grand challenge\nof human-level AI.\nVoice Document Editing\nThe challenge of genuine understanding in conversational\nAI requires us to pick a domain that is small enough for\nan assistant to fully understand. Document editing is one of\nthe domains that allows an agent to focus a conversation.\nImagine an intelligent assistant that helps create and mod-\nify a document via a free-form language conversation with\na user. This conversation is focused on the document the as-\nsistant and the user are authoring. We call this domain voice\ndocument editing and propose it as particularly well-suited\nto develop conversational AI.\nThe voice document-editing domain ﬁts well into the idea\nwe described earlier: learning purposes enables intelligent\nassistants to become more helpful and powerful. Document\nediting assistants could provide better help if they could un-\nderstand users’ purposes and allow interactions in a free-\nform language, making the interaction process timely and\nefﬁcient. They could help create and edit text messages,\nemails, and other documents on-the-go. Voice document-\nediting assistants could perform actions such as deleting and\ninserting words; creating and editing itemized lists; chang-\ning the order of words, paragraphs, or sentences; convert-\ning one tense to another; or ﬁne-tuning the style. For ex-\nample, if a user asks an assistant “Please move the second\nparagraph above”, and then says “Delete the last word in\nthe ﬁrst sentence”, then the assistant would know that the\nﬁrst sentence the user is referring to is in that particular\nparagraph that was moved by the assistant’s previous ac-\ntion. Such assistants have been emerging for more than three\ndecades (Ades and Swinehart, Douglas, Lucas, Miko, and\nBennington 1986,1999,2004).\nWe separate today’s dictation systems into two types: the\nones that allow voice text modiﬁcations in addition to dic-\ntation and the ones that do not. Examples of the latter type\ninclude systems such as Dragon by Nuance, ListNote, and\nthe Speech Recogniser of iOS (Duffy 2018). Our focus is\nonly on the former type: in these systems, users can write\nby dictating while walking, cooking, or doing other things,\nand then edit the document by using pre-deﬁned commands.\nWe refer to them as voice editing-enabled systems. These\n3\nare dictation software systems such as SMARTedit (Lau et\nal. 2001), Apple Dictation (Gruber and Clark 2017), Dic-\ntion.io (Google AI Blog. Digital Inspiration 2020), Google\nDocs Voice Editor (Douglas,Google 1999,2020), and Win-\ndows Speech Recognition (Microsoft 2020). We now de-\nscribe how document editing is performed in these systems.\nDocument editing can be thought of as a manipulation\nof the manuscript in text blocks. Card, Moran, and Newell\n(1980) show that, from a cognitive perspective, document\nediting is structured into a sequence of almost indepen-\ndent unit tasks. The unit tasks are manipulations of se-\nlected text blocks, as suggested by a number of patents re-\nlated to users’ text editing (e.g., Greyson et al., 1997; Taka-\nhashi, 2001; Walker, 1998). In particular, a block of text\nis ﬁrst identiﬁed by a user. Next, the block may be moved\naround, modiﬁed, or formatted in place. A modiﬁcation op-\neration may include insertion of the new text, or the se-\nlected block may be removed completely. Today’s afore-\nmentioned voice editing-enabled systems (Lau et al., Gru-\nber and Clark, Google AI Blog. Digital Inspiration, Dou-\nglas, Google, Microsoft 1999, 2001, 2017, 2020, 2020, 2020)\nclassify document-modiﬁcation unit tasks into text-editing\nfunctions representing the types of text-editing operations\npeople do in their editor of choice.\nTo demonstrate the suitability of voice document edit-\ning for conversational AI, we further look into its advan-\ntages. One advantage of a voice document-editing domain\nis that it excludes the real-world complexities that many\nother assistive systems have. Consider an intelligent clean-\ning robot or some other assistive robotic system (e.g., Dario\net al., 1996; Salichs et al., 2019). These systems have many\nreal-world complexities, such as the effects of the electronic\nhardware, materials, sensory systems, surrounding objects,\nand the variability of sensors. Voice document editing does\nnot have these dependencies and its reduced complexity is\nfavorable for the agent’s learning process and for the re-\nsearchers’ experimentation.\nAn important advantage is that the voice document-\nediting domain serves as a micro world with a ﬁnite num-\nber of clearly deﬁned concepts for the agent to learn. We\nrefer to this property of the domain as being tightly scoped.\nThe world is represented by a manuscript being dictated and\nedited by the user. This world is smaller and more manage-\nable than in many other real-life applications of conversa-\ntional AI, such as open domain conversations for chatbots\n(e.g., Saleh et al., 2019). It is easier for the agent to learn\nin this smaller world because the agent only has to learn\nabout the state of the document and its modiﬁcations, both\nof which are fully accessible to the agent. At the same time,\nthe agent does not have to understand the content of the\ndocument. For example, it is not necessary for the agent to\nknow about history if a user is writing a historical article,\nor to know something about medicine if the user is writing\na health related article. The agent is not expected to know\ninformation about the user that extends beyond the docu-\nment editing context: what the user had for breakfast, their\nreligion, other aspects of the user’s life, or additional world-\nrelated information. The agent, however, knows about the\nstructure of the document and can learn about text blocks,\nsuch as paragraphs, sentences, and words. The agent can\nalso know grammatical structure and core organizational\ncomponents of the document, such as salutations and vale-\ndictions when composing an email. The voice document-\nediting domain allows the agent to center what a conver-\nsation is about—the document itself and its edits. The ﬁnite\namount of editing concepts in the fully-accessible document\ndeﬁnes ﬁxed bounds for the domain and makes it easier to\nevaluate the performance. As a result, relative to other real-\nlife applications of conversational AI, the agent has to learn\na smaller number of things, which is favorable for the agent\nand leads to reasonable expectations.\nOne way to evaluate our choice of voice document-editing\ndomain is to compare it to other domains that may carry\nsimilar properties of reduced real-world complexity and be-\ning tightly scoped. We compare our domain to voice image\nediting and task-oriented systems: both are the closest to our\ndeﬁnition of the purposive intelligent assistant. We show in\nwhich ways voice image editing and task-oriented systems\ndiffer from the domain of our choice, consequently making\nthese domains more challenging than voice document edit-\ning.\nA voice image editing assistant has to be able to recog-\nnize the content, which may require the assistant to learn an\nunlimited number of representations before becoming useful\nto its users. Consider a conversational image editing system\nproposed by Manuvinakurike et al. (2018) that is able to rec-\nognize voice commands such as “remove the tree”. The abil-\nity to execute such commands entails that the system should\nbe able not only to understand the command itself, but also\nto have a representation of a tree to be able to identify a\ntree in the image that is being edited. To recognize a tree\nin an arbitrary image, the agent would have to learn what\nall possible trees look like. Learning about all possible trees\nrequires the assistant to learn an unlimited number of tree\nrepresentations, which results in the agent having to acquire\ninﬁnite multi-domain knowledge. In contrast, in the voice\ndocument-editing domain, the agent does not need to under-\nstand the content of the document; it only needs to learn how\nto perform the edits. For example, the user dictated a phrase\n“There was a tree”. When the user asks to replace the word\n“tree” with the word “lake”, the voice editing assistant does\nnot need to know what the concept of a “tree” or a “lake” is.\nIt simply needs to transcribe the word from voice to text and\nto know the replacement operation, because it already knows\nthe location of words in the text. This example illustrates\nthat voice image editing is not tightly scoped compared to\nvoice document editing. While conversational image editing\nis a suitable domain for incremental dialogue processing, it\nis more difﬁcult for the agent to become useful to its users\nin this domain because of the large amount of information it\nhas to learn.\nTask-oriented systems also have unlimited concepts for\nthe assistant to learn despite the focus on a particular user\ngoal. Consider a restaurant booking system (e.g., Wen et al.,\n2017). To be a good assistant, the system has to know some-\nthing about the user’s schedule, transportation logistics, and\nmany additional concepts. For example, the assistant needs\nto understand that the reservation cannot be made during\n4\nthe time when the user is picking up their children. The as-\nsistant also needs to know how to select the best location\nof the restaurant when it comes to transportation logistics.\nScheduling and logistics are only a couple of concept exam-\nples that the agent is expected to know about in addition to\nknowing the reservation action. The complexity of the real\nworld leads to a possible unlimited number of concepts that\nthe agent is expected to know. This example demonstrates\nhow task-oriented systems may appear tightly scoped while\nhaving unlimited concepts that the assistant has to learn.\nIn contrast, our choice of voice document-editing domain\nmakes learning possible because the agent can learn a lim-\nited number of concepts and can still be useful to the user.\nNow that we have looked at the advantages of voice doc-\nument editing, we turn our attention to the current state of\nsuch systems. Despite enormous effort in this direction, to-\nday’s voice editing assistive systems remain rudimentary.\nWhen it comes to text modiﬁcations using voice, there are\nlimitations: users can format and edit by using only a few\npre-deﬁned commands such as “New line” in order to start a\nnew line or “Go to end of paragraph” when a user wants to\nmove the cursor. If the user says slightly modiﬁed versions\nof commands such as “Let’s go to a new line” or “Move the\ncursor to the end” instead of the aforementioned pre-deﬁned\ncommands, then the agent may not perform the right action.\nThese constraints limit beneﬁts of editing a document via\nvoice—at the end of the day, a user has to either perform\nmanual text manipulation using a keyboard or carefully re-\nmember all the commands to manipulate the manuscript via\nvoice. For example, one of the advanced editors, Google\nDocs Voice Editor (Google 2020), has over a hundred basic\ncommands that a user would need to memorize or to look up\nwhile editing. Communication to the voice editing assistant\nin a free-form language is yet to be developed.\nAn assistant that can communicate to the user in a free-\nform language and be helpful to the user is one of the main\nchallenges in conversational AI. Developing such assistant\ninvolves a number of complex elements, that are incredi-\nbly challenging in isolation: from natural language under-\nstanding, to acoustic prosody, to natural language genera-\ntion, to response generation, to knowledge acquisition (Eric\n2020). Voice document editing combines these elements in\none domain and thereby opens up research opportunities that\ncould beneﬁt the advancement of conversational AI. We now\npropose methods suitable for developing voice document-\nediting assistants.\nReinforcement Learning Assistants\nReinforcement learning has been pursued as a natural ap-\nproach to intelligent assistants (Kozierok and Maes, Pol-\nlack et al., Pineau et al. 1993, 2002, 2003). Reinforcement\nlearning starts with an agent that is interactive and goal-\nseeking. Formally, reinforcement learning is an approach\nfor solving optimal control problems in which a behavior\nis learned through repeated trial-and-error interactions be-\ntween a learning system and the world the system operates\nin. The learned behavior is called a policy, a learning sys-\ntem is an agent, and the world the agent operates in is called\nan environment. In each interaction with the world, the agent\ntakes an action and receives a reward which can be positive\nor negative. The agent aims to maximize the expected re-\nturn, which is the sum of the total rewards in the simplest\ncase (Sutton and Barto 2018). Reinforcement learning has\nnot been previously investigated speciﬁcally for voice doc-\nument editing and it is the approach that we are going to\nexplore in this article.\nThe ﬁrst advantage of reinforcement learning for intelli-\ngent assistants is that it provides an opportunity for an intu-\nitive human-computer interaction, in contrast to more com-\nmon machine learning formulations of supervised and unsu-\npervised learning. In particular, an interactive reinforcement\nlearning agent can directly learn things about its environ-\nment with every action and select future actions according\nto that knowledge. This means that agents are not learn-\ning from the input-output pairs that were provided ahead of\ntime, but from direct experience—online learning. Online\nlearning provides a natural opportunity for intuitive inter-\nactions, during which intelligent assistants adapt to users.\nImagine a voice assistant that recommends fun things to\ndo during travel, similar to the NJFun system (Litman et\nal., Singh et al. 2000, 2002a) that provided users with in-\nformation about fun things to do in New Jersey. The as-\nsistant can learn about the user’s preferences much faster\nand provide better recommendations by extracting a reward\nsignal after each suggestion made and adjusting its sugges-\ntions accordingly. When this assistant helps a traveler who\nis interested in music history, it can learn the user’s prefer-\nences quickly, based on how satisﬁed the user was with its\npreviously provided recommendations. The assistant can tai-\nlor the recommended places to music history museums, mu-\nsic history festivals, music exhibits, and other similar attrac-\ntions. This adjustment via online learning between the assis-\ntant and the user improves the assistant’s reasoning about its\nfuture actions. The importance of online learning and inter-\naction feedback have appeared in many studies (e.g., Long,\n1981; Pica, Doughty, and Young,Pica, 1986,1987; Gass and\nVaronis, 1994; Bassiri, 2011; Gaˇsi´c et al., 2011; Gaˇsi´c et al.,\n2013a; Ferreira and Lefvre, 2015; Li et al., 2017a; Liu et al.,\n2017).\nThe second advantage of reinforcement learning is that\nusers’ feedback can be formulated as a reward signal. Feed-\nback can be used as a goal-directed signal of users’ satisfac-\ntion or dissatisfaction with actions the assistant takes, which\nis one way to evaluate the assistant (see Jiang et al., 2015).\nIn the context of dialogue settings, it is expected that users\nhave a way of communicating their feedback to the assistant,\neither through voice interaction or physical interaction via a\nrobotic device. Examples of these interactions include: train-\ning assistants with animal-like robot clicker techniques (Ka-\nplan et al. 2002); using a combination of reward signals from\na human and an environment (Knox and Stone 2012); and\nusing a sparse human-delivered training signal, as in the case\nof adaptable, intelligent artiﬁcial limbs (Pilarski et al. 2011).\nAn intelligent assistant maximizing users’ satisfaction is a\nreinforcement learning agent maximizing the expected re-\nturn.\nThe third advantage of reinforcement learning is that it al-\n5\nlows changes in the agent’s knowledge. These changes are\nfundamental for learning, which is different from built-in\nknowledge. A process when the agent has to change what it\nknows, rather than knowing whether something is a fact, is\ndeﬁned as learning by Selfridge (1993) and is considered the\nmost important part of intelligence (Woodrow 1946). Know-\ning is simply factual: one learns a particular kind of knowl-\nedge and knows if it is the truth that applies to a particular\nsetting. Learning, however, leads to understanding. Under-\nstanding is more ﬂuid than drawing knowledge from built-in\nfacts: in understanding, we relate the facts to everything else\nand can reason about the consequences of our actions. Rea-\nsoning about the outcome results in a controlled choice of\nactions when presented with alternatives. As we argued ear-\nlier, a controlled choice of actions is a quality of a purposive\nintelligent assistant that continuously adapts to users chang-\ning needs. Thus, reasoning about the consequences of ac-\ntions leads to an assistant that learns how to learn, adapt, and\nimprove by interacting with users (e.g., Li et al., 2017b). The\nmost brilliantly-engineered slot-ﬁlling systems will not learn\npurposes because they cannot learn to avoid repetitive mis-\ntakes, nor can they learn to adapt to different users. In con-\ntrast to learning from static datasets, reinforcement learning\nis more general and allows for these adjustments. In partic-\nular, the structure and other properties of the world are not\nassumed in reinforcement learning.\nFor voice document-editing assistants in particular, rein-\nforcement learning is a natural ﬁt. In reinforcement learn-\ning terminology, an agent is a document-editing assistant.\nAn environment is both a document and the user that in-\nteracts with the agent. The document could be any text of\nany length, for example, an email, a manuscript, or a text\nmessage. The user could either dictate a new sequence of\nwords or ask the assistant to modify a block of text: to switch\nsome words or sentences, delete words, or highlight parts\nof the text, etc. The user’s communication would be trans-\nformed from voice into text that would then be observed by\nthe agent. The agent would react to this communication text\nand take an action: an edit to the document or a request for a\nclariﬁcation from the user. This action would affect the envi-\nronment, potentially resulting in changes to the document in\nthe case of editing actions. The user would respond in turn\nto the agent’s action by replying with the next request or\nby continuing to request edits. This response would create\na new observation for the agent and this interaction would\nrepeat until the user would be fully satisﬁed. The satisfac-\ntion of the user would be reﬂected in a reward signal that the\nagent receives after every selected action. We describe such\nvoice document-editing assistants further later in this arti-\ncle, and ﬁrst we address the work that has been already done\nin applying reinforcement learning methods to developing\nconversational AI.\nPrior Work Applying Reinforcement Learning\nto Conversational AI\nReinforcement learning has been applied to conversational\nAI in various ways since late 1970s. Some of earliest works\nwere Walker and Grosz, Biermann and Long, Levin, Pier-\naccini, and Eckert, Singh et al. (1978, 1996, 1997, 2000). In\nmore recent works, Gaˇsi´c et al. (2011, 2013a) use reinforce-\nment learning in online settings to directly learn from hu-\nman interactions using rewards provided by users and to\noptimize the agent’s behavior in reaction to the user. Dhin-\ngra et al. (2017) also explore online learning, but in simu-\nlated settings. They train their agent entirely from the feed-\nback that mimics the behavior of real users. Such simu-\nlated settings are not always available for a learning task and\nbuilding simulators for dialogue scenarios and tasks is chal-\nlenging (Cuay´ahuitl et al., Li et al. 2005, 2016b). To over-\ncome these challenges, Zhou et al. (2017) choose to opti-\nmize a policy in ofﬂine settings using the raw transcripts\nof the dialogues, while Liu and Lane (2017a) take an ap-\nproach of jointly optimizing the dialogue agent and the user\nsimulator. Xu, Wu, and Wu (2018) also apply joint model-\ning of dialogue act selection but use reinforcement learning\nonly to optimize response generation. A number of others\nalso use reinforcement learning for open-domain dialogue\ngeneration(e.g., Ranzato et al., 2015; Li et al., 2016a; Yu\net al., 2017; Budzianowski et al., 2017; Xu, Wu, and Wu,\n2018; Jaques et al., 2019).\nOne of the big trends in the last ﬁve years has been to\nuse deep reinforcement learning in conversational AI (He et\nal.,Cuay´ahuitl et al.,Shah, Hakkani-T¨ur, and Heck,Zhao and\nEskenazi, Bordes, Boureau, and Weston, Budzianowski et\nal.,Shen et al.,Liu et al.,Su et al.,Peng et al.,Williams, Asadi,\nand Zweig, Liu, Peng et al., Tang et al., Weisz et al., Zhang,\nZhao, and Yu, Mendez et al., Shin et al., Zhao et al. , 2015,\n2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017, 2017,\n2018,2018,2018,2018,2018,2019,2019), building on the re-\ncent success of deep reinforcement learning on games such\nas Atari, Go, chess and shogi (Mnih et al.,Silver et al.,Schrit-\ntwieser et al. 2013,2018,2019). There is a mix of approaches\nin this body of work. For example, Liu et al. (2017) use re-\ninforcement learning in combination with supervised learn-\ning, and then optimize the agent during the interactions with\nthe users. Shah et al. (2016,2018) contrast the interpretation\nof the human feedback as a reward value (Thomaz, Hoff-\nman, and Breazeal, Thomaz, Hoffman, and Breazeal, Knox\nand Stone,Loftin et al. 2005,2006,2012,2014) and propose\nan interactive reinforcement learning approach in which the\nuser feedback is treated as a label on the speciﬁc action taken\nby the agent similar to Grifﬁth et al. (2013). Reinforcement\nlearning is studied for policy adaptation between domains in\nmulti-domain settings (e.g., Gaˇsi´c et al., 2013b; Cuay´ahuitl\net al., 2017; Gaˇsi´c et al., 2017; Rastogi, Hakkani-T¨ur, and\nHeck, 2017; Chen et al., 2018; Liu and Lane, 2017b). Serban\net al. (2017a) apply reinforcement learning to select from a\nnumber of responses produced by an ensemble of so-called\nresponse models. Tang et al. (2018) use reinforcement learn-\ning to train multi-level policy that allows agents to accom-\nplish subgoals. Foerster et al. (2016), Sukhbaatar, Szlam,\nand Fergus (2016), Lazaridou, Pham, and Baroni (2016),\nMordatch and Abbeel (2018), and Papangelis et al. (2020)\napply reinforcement learning to teaching agents to commu-\nnicate with each other in multi-agent environments. These\nworks are a small fraction of the large body of research that\nimplements deep reinforcement learning approaches in dia-\n6\nlogue systems.\nModel-based Reinforcement Learning\nIn the dialogue systems that we have discussed so far, the\nreinforcement learning agent learns policies and value func-\ntions, but not a model of the environment that can be used\nfor planning. By models of the environment, or models of\nthe world, we mean any function that the agent can use to\npredict how the environment will respond to the agent’s ac-\ntions. We use the term planning to refer to any computa-\ntional process that takes a model of the environment as in-\nput and produces an improved policy for interacting with\nthe modeled environment. The idea of augmenting a rein-\nforcement learning agent with a world model that is used for\nplanning is known as model-based reinforcement learning\n(Sutton and Barto, Sutton and Pinette,Sutton, Chapman and\nKaelbling, Singh, Atkeson and Santamaria, Wiering, Salus-\ntowicz, and Schmidhuber,Abbeel et al.,Sutton et al.,Ha and\nSchmidhuber, Holland, Talvitie, and Bowling, Schrittwieser\net al. 1981, 1985, 1990–1992, 1997, 2001, 2007, 2008, 2018,\n2018,2019).\nModels and planning are helpful. One advantage of mod-\nels and planning is that they are useful when the agent faces\nunfamiliar or novel situations—when the agent may have to\nconsider actions that they have not experienced or seen be-\nfore. Planning can help the agent evaluate possible actions\nby rolling out hypothetical scenarios according to the model\nand then computing their expected future outcomes (Doll,\nSimon, and Daw, Ha and Schmidhuber 2012, 2018). These\noutcomes can be computed a few steps ahead and can be\nthought of as the agent reasoning about the long-term con-\nsequences of its actions, similar to how people evaluate the\nlong-term consequences of their decisions. The agent rea-\nsoning about the consequences of its actions and acting\nbased on the world model’s predictions is analogous to the\nway a person reasons and acts based on their understand-\ning of the world. Hypothetical scenarios allow the agent to\nsafely explore the possible consequences of actions. For ex-\nample, the agent can use hypothetical scenarios to explore\nactions that in real-life applications could lead to a costly\ncrash or other disaster (Berkenkamp et al. 2017). Another\nadvantage of world models and planning is that they help\naccelerate the learning of policies (Freeman, Ha, and Metz\n2019).\nThe advantages of world models and planning also arise\nwith intelligent assistants. World models and planning en-\nable intelligent assistants to leverage the interaction infor-\nmation as much as possible, with a minimal amount of prior\nknowledge and in the absence of external supervision. Thus,\nmodels and planning make it possible for the agent to ac-\nquire the full beneﬁts of reinforcement learning in conver-\nsational AI, which in turn would allow us to create a purpo-\nsive intelligent assistant that is efﬁcient and useful, can adapt\nto its users, reason about the consequences of its actions,\ncan control its choice of actions among alternatives, and can\nlearn how the real world works. Planning has been previ-\nously explored in dialogue systems outside of reinforcement\nlearning (e.g., Stent, Prasad, and Walker, 2004; Walker et\nal., 2007; Jiang et al., 2019).\nThere are a few existing works in conversational AI that\nalready apply model-based reinforcement learning (MBRL)\nmethods. They use a world model to mimic user responses\nand planning to generate hypothetical experiences that are\nthen used to improve the policy. Lewis et al. (2017) in-\ntroduced dialogue rollouts, in which planning proceeds by\nimagining many hypothetical completions of the conversa-\ntion. They interleaved reinforcement learning updates with\nsupervised updates. This work was followed by Yarats and\nLewis (2017) who improved the effectiveness of long-term\nplanning using rollouts. Peng et al. (2018) incorporated\nplanning into dialogue policy learning in their deep Dyna-\nQ framework, followed by Su et al. (2018) who proposed\nto control the quality of hypothetical experiences gener-\nated by the world model in the planning phase. Wu et al.\n(2019) extended the deep Dyna-Q framework by integrat-\ning a switcher that allows to differentiate between a real or\nhypothetical experience for policy learning (see also Gao et\nal., 2019; Zhang et al., 2020). Subsequently, Zhao et al. ()\nbuilt on the work of Peng et al. (2018), using similar world\nmodel designs, which is a model-based approach that is sim-\nilar to the Dyna architecture (Sutton 1990), where learn-\ning and planning are combined and the predictions are im-\nproved based on the planning (cf. Lison, 2013). All these\nprior works in conversational AI with MBRL are important\ndevelopments, yet, do not offer a scalable MBRL agent that\nis fully capable of assisting the user and combines learn-\ning and planning in all aspects. We have begun to explore\nthe combination of learning and planning (Kudashkina et al.\n2020). Below we develop the idea of MBRL and identify\nits major components, which will help to think further about\napplying MBRL to the voice document editing domain.\nThere are three primary components of MBRL: an agent\nstate and its construction; an environmental model; and a\npolicy, together with a function that estimates the outcome\nof the agent’s actions (Figure 1). The ﬁrst component of the\nMBRL agent is a representation of where it is at the current\ntime—an agent state. The agent state is an approximation of\na minimal environmental state that is the basis of the theory\nof Markov decision processes. The agent state is a compact\nsummary of all that has come before, which includes all the\nhistories. The agent state changes at every time step based on\nthe actions the agent takes and the observed signals from the\nenvironment, which we call observations. The agent state is\ncomputed incrementally as a function of the previous agent\nstate, the most recent observation, and the most recent ac-\ntion. This function is called a state update function. The state\nupdate function is similar to the recursive state update intro-\nduced by Littman and Sutton (2001). The agent state is an\nessential part of the MBRL architecture and an input that\nbecomes crucial in the intelligent assistant’s decisions.\nThe second component of MBRL is an environmental\nmodel. As discussed, environmental models are fundamen-\ntal in making long-term predictions and evaluating the con-\nsequences of actions based on those predictions. Nortmann\net al. (2015) suggest people’s internal models of the world\nare learned during the interaction with the world and con-\ntain the information that is perceived by their brains. Sim-\n7\nilarly, environmental models are learned by the agent dur-\ning human-computer interaction. Forrester (1971) describes\npeople’s models of the world as the image of the world that\nhumans carry in their head: the concepts and relationship\nbetween them that are used to represent a real system. Like-\nwise, environmental models capture the dynamics of the en-\nvironment and the user-agent interaction within it (Sutton\n2019). These dynamics are everything that the agent needs\nto know about the environment. More precisely, the knowl-\nedge of the environment dynamics enables the agent to pre-\ndict next states and rewards from the previous state and its\nactions. Thus, environmental models help the agent to make\ninformed action choices by enabling it to compute hypothet-\nical future outcomes of different actions with the modelled\nenvironment. Recall that we refer to this computational pro-\ncess as planning. Planning is particularly important because\nit allows the agent to learn not only from the past actions\nthat resulted in a reward, but also from hypothetical actions\nfor which the agent has not seen a reward. Ng et al. (2004)\npresent a successful application of reinforcement learning\nthat uses a model for autonomous helicopter ﬂight, saving\nhours of adjustments and ﬂight testing and preventing un-\nnecessary crashes. Their model is learned ofﬂine prior the\nagent training. Ng et al. (2004) start with a human pilot ﬂy-\ning the helicopter for several minutes, and then use the data\nto ﬁt a model of the helicopters dynamics. Similarly, Coates,\nAbbeel, and Ng (2017) provide an agent with the pre-trained\nmodel and the reward function.\nThe third component of MBRL includes a policy and a\nfunction that estimates the outcome of the agent’s actions.\nThese functions are dependent on states or state-action pairs\nand are called value functions. Policies and value functions\nare a part of core reinforcement learning methods that do\nnot include environmental models— model-free methods. In\nState Update \nFunction\nupdates\nPurposive \nIntelligent\nAssistant\nActions, \nRewards\nEnvironmental \nmodel\n&\nPlanner\nPolicy \n& \nValue Function\nPrevious \nAction, \nPrevious \nState\nState\nReward \nSignal\nAction\nObservation\nWorld\nFigure 1: A MBRL purposive intelligent assistant with pri-\nmary components: an agent state and a state update function;\nan environmental model; and the agent’s policy and value\nfunction. Adopted from Sutton and Barto (2018).\nsimpliﬁed terms, value functions compute a numeric value\nof a given state based on the observed rewards. These values\nindicate how effective it is for the agent to be in the given\nstate or how effective it is to perform a particular action\nfrom that state. Policies and value functions are often ap-\nproximated by artiﬁcial neural networks (Parr et al., Sutton\net al. 2008,2008).\nThe policy and the value function, together with the agent\nstate, the state update function, and the environmental model\ncomplete a full MBRL conceptual architecture. This archi-\ntecture is general and has no domain-speciﬁc components.\nThis generality could result in a scalable and potentially last-\ning impact. The state update function and the environmental\nmodel components are critical parts of this architecture and\ndependent on each other. The agent state serves as an input\nto and an output of the environmental model, in addition to\nbeing an input for the policy and the value function. Dis-\ncovering and learning models of environment dynamics and\nstate update functions are important steps toward learning\nusers’ purposes during interactions while performing assis-\ntive tasks.\nThere is a diversity of open research areas in MBRL. One\narea is in the direction of the choice of models (see Chua\net al., 2018), such as probabilistic transition models that\nuse Gaussian processes (e.g., Kocijan et al., 2004; Deisen-\nroth, Fox, and Rasmussen, 2013; Kamthe and Deisenroth,\n2018), nonlinear neural network models (e.g., Hernandaz\nand Arkun, 1990), generative models (e.g., Buesing et al.,\n2018), latent state-space models (e.g., Wahlstr¨om, Sch¨on,\nand Deisenroth, 2015; Watter et al., 2015), and policy search\nmethods (e.g., Bagnell and Schneider, 2001; Deisenroth and\nRasmussen, 2011; Levine and Abbeel, 2014; Levine et al.,\n2016). Another area of study is on asymptotic performance\nof model-based methods to match the asymptotic perfor-\nmance of model-free algorithms (e.g., Chua et al., 2018).\nThe effectiveness of planning methods and data sample ef-\nﬁciency is one more area of active research (e.g., Hafner et\nal., 2018; Kamthe and Deisenroth, 2018; Kaiser et al., 2019).\nSilver et al. (2017a) demonstrate the effectiveness of plan-\nning methods in applications such as AlphaGo, offering an\nalgorithm based solely on reinforcement learning, without\nhuman data, guidance, or domain knowledge beyond game\nrules. Nevertheless, cases of full model-based reinforcement\nlearning similar to Silver et al. (2017b), in which the envi-\nronment model is learned from online data and then used\nfor planning, are rare, especially in stochastic domains. Re-\nduction of errors in learned models that resemble the real\nenvironment, data efﬁciency, asymptotic performance, and\na choice of planning methods are some of the topics at the\nforefront of MBRL research.\nVoice Document Editing with MBRL\nWe are at the beginning of a wave of real-world applications\nof AI that use reinforcement learning methods. In particular,\nvoice document-editing assistants are one of the purposive\nintelligent assistants that can be realized using model-based\nreinforcement learning. One effort along these lines is what\nwe mentioned before—by Kudashkina et al. (2020). The re-\n8\nalization of voice document-editing assistants can be natu-\nrally mapped into MBRL components.\nThe ﬁrst component of MBRL, the agent state, encapsu-\nlates the representation of the document and the history of\nthe conversation and its edits. The agent’s state is the current\ndocument with its structure, such as paragraphs, sentences,\nwords; what was earlier dictated by the user; which actions\nwere taken by the agent; and how satisﬁed the user was in\nresponse to the actions taken. An observation is represented\nby a combination of the current document’s content and the\nuser’s speech. The observation can be thought of as some in-\nformation about what is going on in the environment—the\ndocument and the user interacting with the assistant. The in-\nformation contained in the observation is partial because it\ndoes not include things such as the user’s emotions or any-\nthing else outside of the document. Outside information like\nusers’ emotional reactions can be approximated from addi-\ntional data such as the time between users’ reactions, their\ntone of voice, or their facial expressions (e.g., via techniques\nsuch as face valuing by Veeriah, Pilarski, and Sutton, 2016,\nor other cues as suggested by Skantze, 2016).\nThe second component of MBRL, an environmental\nmodel, helps to develop agent’s understanding of what hap-\npens to the document if it takes an unseen action. The en-\nvironmental model captures the dynamics of the user-agent\ninteractions that the agent learns by observing the results\nof actions taken. Learning environmental models in voice\ndocument-editing domain is possible because of the domain\nbeing tightly scoped. Learning better models leads to im-\nproved planning processes that result in better predictions.\nEven if the assistant has never heard a command that the\nuser is saying, by using environmental models and planning,\nthe agent would recognize whether the command is a con-\ntinuation of a dictation or an edit that the user wants to per-\nform. When editing requests are recognized, then the agent\ncan plan which editorial action leads to a better outcome and\nhigher users’ satisfaction.\nIn the third component of MBRL, the policy in voice doc-\nument editing is the agent’s behavior in response to user’s\nrequests. This behavior allows the agent to make a choice\nbetween continuing to listen to the user when the user wishes\nto dictate more, and selecting an editing action to manipu-\nlate a text block when the user requests an edit. The value\nfunction in voice document editing represents a numerical\nvalue of user’s satisfaction.\nRemaining Questions\nThis article outlined a domain of voice document editing\nand MBRL methods as particularly well-suited for devel-\noping conversational AI. It is appropriate to discuss some\nquestions that may arise when implementing this proposal,\nyet implementation details are beyond the scope of this arti-\ncle. One such question is that of how ambitious the agent’s\nlearning can be. Recall that there are many complex ele-\nments in conversational AI, such as natural language un-\nderstanding, dialogue management, natural language gen-\neration, and response generation. The agent’s learning can\nfocus on only one (or on a few) of the elements, while the re-\nmaining elements can be treated as a black box; for example,\nthe user’s speech can be a black box and can be transcribed\nby existing tools for speech-to-text conversion (e.g., Bijl and\nHyde-Thomson, 2001; Rao, 2011). When not treated as a\nblack box, each of the elements has its own challenges; for\nexample, diversity and coherence in natural language gen-\neration element (e.g., Yarats and Lewis, 2017; Jang, Lee,\nand Kim, 2019; Shi et al., 2018; Gu et al., 2019; Wang et\nal., 2019; Zheng et al., 2019). The agent’s ambitiousness in\nlearning that can vary in its complexity with the choices for\nand within each element of conversational AI.\nThe agent’s learning within one element in conversational\nAI can be thought of as having three levels of complexity. As\nan example, consider natural language generation. The ﬁrst\nlevel of complexity is when the agent’s responses being en-\ntirely pre-deﬁned by a system designer; this means that no\nlanguage generation occurs. The agent’s learning then fo-\ncuses only on the selection of actions to satisfy the user, and\nthe agent’s only intelligence is in the learned policy that it\nfollows to select a response. The model of the environment\nwill learn the dynamics of the interactions and still help the\nagent with this action selection. Despite the absence of lan-\nguage generation, this agent can still be useful to its users.\nThe second level of complexity is when the response gen-\neration is treated as a black box and can be supplemented\nby one of the existing approaches (e.g., Serban et al., 2017a)\ninstead of using pre-deﬁned responses as in the ﬁrst level.\nThe third level of complexity is when the response genera-\ntion as an element that is fully learned and this learning be-\ncomes a part of the MBRL agent. The response generation\nis a demonstration of the agent’s ambitiousness in learning\nwithin one element of conversational AI.\nAn ambitious agent is the most desirable; the agent can\nbe fully responsible for learning all the elements of conver-\nsational AI, from generating responses to selecting them. An\nenvironmental model will be more complex for this sophis-\nticated agent because it will have to learn all about the inter-\naction dynamics between the user and the assistant. These\nkinds of implementations are often referred to as end-to-\nend training. Variations of such implementations can include\nmultiple agents with multiple models, where each of the\nagent-model pairs can be responsible for a particular con-\nversational AI element. Further, there are higher-level mod-\nels of the world based on extended ways of a temporally\nabstract behavior, which were introduced as options by Sut-\nton, Singh, and Precup (1999). Higher-level models provide\na way of obtaining higher-level planning and reasoning (Sut-\nton 2020).\nAnother implementation question is how to train voice\ndocument-editing assistants: should they learn online or of-\nﬂine? Recall that online learning is learning directly from\nexperience, such as from real human-computer interactions.\nOfﬂine learning is a traditional approach in supervised learn-\ning that relies on pre-constructed datasets. These datasets\ncan be constructed in a number of ways, including real user-\ncomputer interactions, and can be used then to build simula-\ntors.\nOne way an ofﬂine dataset can be created is by using the\nMechanical Turk service, a crowdsourcing web service that\n9\ncoordinates the supply and demand of tasks (see Paolacci,\nChandler, and Ipeirotis, 2010). Using Mechanical Turk, dic-\ntation and editing of the resulting documents can be recorded\nas a dataset, which is then used to create an environment\nfor voice editing simulations. As Dhingra et al. (2017)\npoint out, it is common in the dialogue community to use\nsimulated users for this purpose (e.g., Schatzmann et al.,\n2007; Cuay´ahuitl et al., 2005; Asri, He, and Suleman, 2016).\nAnother way to obtain ofﬂine datasets requires more cre-\native approaches. Consider the work of Feng et al. (2019) in\nwhich organizational business documents are used as inputs\nto generate a conversational ofﬂine dataset. In this conversa-\ntional dataset, the conversation is based on the organization’s\nworkﬂow. We encourage the reader to think about similar\ncreative ways of obtaining document-editing datasets. For\nexample, with some good engineering, the mechanical ma-\nnipulations of text blocks in manuscripts can be collected\nand converted into conversational data with the addition of\nvoice commands.\nOur preferred method of training is online learning from\nreal interaction data. Online learning allows the intelligent\nassistant to adjust to the users during the training process,\nand it creates an opportunity for the assistant and the user\nto build the communication resources developed together\nduring their ongoing interaction (see Pilarski et al., 2017).\nBuilding communication resources can effectively improve\ncollaboration and interaction between the user and the voice-\nediting assistant. As Mendez et al. (2019) point out, a\nwidespread adoption by users of such assistants is limited to\nthe assistants’ quality, which often requires the investment\nof vast amounts of data. Thus, even if we were to have ac-\ncess to systems that allow us to experiment with real online\ndata (e.g., Google, 2020), then the assistant that learns fully\nonline might be of lower quality in the beginning of train-\ning than one that was pre-trained with ofﬂine datasets. In the\npast, when a secretary was hired, they were expected to have\nstructural language knowledge. Similarly, it is reasonable to\nhave such expectations of voice editing assistants and pre-\ntrain them with some preliminary knowledge learned from\nofﬂine datasets before these assistants can be offered to users\nand continue to learn online.\nSummary and Implications\nIn this article, we have proposed that the domain of voice\ndocument editing is particularly well-suited for the devel-\nopment of intelligent assistants that can engage in a con-\nversation. To make progress in developing useful assistants\nfor conversational AI, these assistants should be purposive.\nA natural approach for developing purposive assistants is\nreinforcement learning, and, in particular, MBRL. This ap-\nproach is well-suited to assistants that learn and adapt within\ndocument editing and general conversational AI settings.\nMany aspects of using MBRL remain open areas in AI re-\nsearch, in particular, its use within voice document editing.\nFinding solutions for the voice document-editing domain\nwith MBRL and building these systems can provide us with\nlessons that move us closer to building other systems that\ngenuinely understand the user and learn their purposes. In\nthis way, a better voice document editing system will also\ncontribute to the development of other assistive systems,\nmoving the research toward the ultimate goal of assistive\nagents that fully and functionally understand the real world\naround them.\nThe realization of voice document-editing assistants not\nonly serves our objectives of creating a purposive assistant\nand achieving goals of conversational AI, but also results in\nan application that directly beneﬁts society: from improving\nproductivity to beneﬁting people with limited typing abili-\nties.\nFunding\nSupport for this work was provided in part by the Arrell\nFood Institute, the University of Alberta, and the Canadian\nInstitute for Advanced Research AI Catalyst Fund Project\n#CF-0110.\nAcknowledgments\nWe would like to thank Peter Wittek and Joseph Modayil for\ntheir useful discussions and feedback.\nReferences\nAbbeel, P.; Coates, A.; Quigley, M.; and Ng, A. Y. 2007. An\napplication of reinforcement learning to aerobatic helicopter\nﬂight. In Proceedings of the 21st Conference on Neural In-\nformation Processing Systems, pp. 1–8.\nAdes, S., and Swinehart, D. C.\n1986.\nVoice annotation\nand editing in a workstation environment. Technical Report\nCSL-86-3, XEROX Corporation, Palo Alto Research Center.\nAllen, J. F.\n1979.\nA plan-based approach to speech\nact recognition.\nTechnical Report 131/79, University of\nToronto, Department of Computer Science.\nAnderson, J. R.; Boyle, C. F.; and Reiser, B. J. 1985. Intel-\nligent tutoring systems. Science 228(4698):456–462.\nAsri, L. E.; He, J.; and Suleman, K. 2016. A Sequence-to-\nSequence Model for User Simulation in Spoken Dialogue\nSystems. In Interspeech. ISCA.\nAtkeson, C. G., and Santamaria, J. C. 1997. A compari-\nson of direct and model-based reinforcement learning. In\nProceedings of International Conference on Robotics and\nAutomation, volume 4, pp. 3557–3564.\nBagnell, J. A., and Schneider, J. G. 2001. Autonomous he-\nlicopter control using reinforcement learning policy search\nmethods.\nIn Proceedings 2001 ICRA. IEEE Interna-\ntional Conference on Robotics and Automation (Cat. No.\n01CH37164), volume 2, pp. 1615–1620. IEEE.\nBapna, A.; T¨ur, G.; Hakkani-T¨ur, D.; and Heck, L. 2017. To-\nwards zero-shot frame semantic parsing for domain scaling.\nArXiv:1707.02363.\nBassiri, M. A. 2011. Interactional Feedback and the Impact\nof Attitude and Motivation on Noticing L2 Form. English\nLanguage and Literature Studies 1(2):61.\nBerkenkamp, F.; Turchetta, M.; Schoellig, A.; and Krause,\nA.\n2017.\nSafe model-based reinforcement learning with\n10\nstability guarantees. In Proceedings of the 31st Conference\non Neural Information Processing Systems, pp. 908–918.\nBiermann, A. W., and Long, P. M. 1996. The composition\nof messages in speech-graphics interactive systems. In Pro-\nceedings of the 1996 International Symposium on Spoken\nDialogue, pp. 97–100.\nBijl, D., and Hyde-Thomson, H. 2001. Speech to text con-\nversion. US Patent 6,173,259. Google Patents.\nBordes, A.; Boureau, Y.-L.; and Weston, J. 2017. Learning\nend-to-end goal-oriented dialog. In Proceedings of the 5th\nInternational Conference on Learning Representations.\nBourbakis, N. G., and Kavraki, D.\n2001.\nAn intelligent\nassistant for navigation of visually impaired people.\nIn\nProceedings 2nd Annual IEEE International Symposium on\nBioinformatics and Bioengineering (BIBE 2001), pp. 230–\n235. IEEE.\nBruce, B. 1975. Belief systems and language understanding.\nTrends in Linguistics. Studies and Monographs 19:113–160.\nBudzianowski, P.; Ultes, S.; Su, P.-H.; Mrkˇsi´c, N.; Wen, T.-\nH.; Casanueva, I.; Rojas-Barahona, L.; and Gaˇsi´c, M. 2017.\nSub-domain modelling for dialogue management with hier-\narchical reinforcement learning. ArXiv:1706.06210.\nBuesing, L.; Weber, T.; Racaniere, S.; Eslami, S. M. A.;\nRezende, D.; Reichert, D. P.; Viola, F.; Besse, F.; Gregor,\nK.; Hassabis, D.; and Wierstra, D.\n2018.\nLearning and\nquerying fast generative models for reinforcement learning.\nArXiv:1802.03006.\nCarberry, S.\n1989.\nPlan recognition and its use in un-\nderstanding dialog.\nIn User Models in Dialog Systems.\nSpringer. pp. 133–162.\nCarbonell, J. R. 1971. Mixed-initiative man-computer in-\nstructional dialogues. Technical report, Cambridge, Bolt Be-\nranek and Newman.\nCard, S. K.; Moran, T. P.; and Newell, A. 1980. Computer\ntext-editing: An information-processing analysis of a routine\ncognitive skill. Cognitive psychology 12(1):32–74.\nCassell, J. 2000. More than just another pretty face: Embod-\nied conversational interface agents. Communications of the\nACM 43(4):70–78.\nChapman, D., and Kaelbling, L. P. 1991. Input Generaliza-\ntion in Delayed Reinforcement Learning: An Algorithm and\nPerformance Comparisons. In Proceedings of the 1991 In-\nternational Joint Conference on Artiﬁcial Intelligence, vol-\nume 91, pp. 726–731.\nChen, L.; Chang, C.; Chen, Z.; Tan, B.; Gaˇsi´c, M.; and Yu,\nK. 2018. Policy adaptation for deep reinforcement learning-\nbased dialogue management. In IEEE International Confer-\nence on Acoustics, Speech and Signal Processing, pp. 6074–\n6078. IEEE.\nChua, K.; Calandra, R.; McAllister, R.; and Levine, S. 2018.\nDeep reinforcement learning in a handful of trials using\nprobabilistic dynamics models. In Proceedings of the 31st\nConference on Neural Information Processing Systems, pp.\n4754–4765.\nCoates, A.; Abbeel, P.; and Ng, A. Y. 2017. Autonomous\nHelicopter Flight Using Reinforcement Learning. Boston,\nMA: Springer US. pp. 75–85.\nCohen, P. R. 1978. On Knowing What to Say: Planning\nSpeech Acts. Ph.D. Dissertation, University of Toronto, De-\npartment of Computer Science.\nCroft, W. B.; Metzler, D.; and Strohman, T. 2010. Search\nengines: Information retrieval in practice, volume 520.\nAddison-Wesley Reading.\nCuay´ahuitl, H.; Renals, S.; Lemon, O.; and Shimodaira, H.\n2005. Human-computer dialogue simulation using hidden\nMarkov models. In Workshop on Automatic Speech Recog-\nnition and Understanding, IEEE, pp. 290–295. IEEE.\nCuay´ahuitl, H.; Yu, S.; Williamson, A.; and Carse, J. 2016.\nDeep reinforcement learning for multi-domain dialogue sys-\ntems. In Workshop on Deep Reinforcement Learning, NIPS.\nCuay´ahuitl, H.; Yu, S.; Williamson, A.; and Carse, J. 2017.\nScaling up deep reinforcement learning for multi-domain di-\nalogue systems. In 2017 International Joint Conference on\nNeural Networks, pp. 3339–3346. IEEE.\nDario, P.; Guglielmelli, E.; Genovese, V.; and Toro, M. 1996.\nRobot assistants: Applications and evolution. Robotics and\nautonomous systems 18(1-2):225–234.\nDean, J. 2019. Looking Back at Google’s Research Efforts\nin 2018.\nDeisenroth, M., and Rasmussen, C. E. 2011. PILCO: A\nmodel-based and data-efﬁcient approach to policy search.\nIn Proceedings of the 28th International Conference on ma-\nchine learning, pp. 465–472. Omnipress.\nDeisenroth, M. P.; Fox, D.; and Rasmussen, C. E.\n2013.\nGaussian processes for data-efﬁcient learning in robotics\nand control. IEEE transactions on pattern analysis and ma-\nchine intelligence 37(2):pp. 408–423.\nDhingra, B.; Li, L.; Li, X.; Gao, J.; Chen, Y.-N.; Ahmed,\nF.; and Deng, L. 2017. Towards end-to-end reinforcement\nlearning of dialogue agents for information access. In Pro-\nceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics, pp. 484–495.\nDoll, B. B.; Simon, D. A.; and Daw, N. D. 2012. The ubiq-\nuity of model-based reinforcement learning. Current opin-\nion in neurobiology 22(6):1075–1081.\nDouglas, H. R. 1999. Method and apparatus for editing\ndocuments through voice recognition. US Patent 5,875,429,\nGoogle Patents.\nDuffy, J.\n2018.\nThe Best Dictation Software for\n2019.\nhttps://zapier.com/blog/best-text-dictation-software\n[Accessed July 27, 2020].\nEngelbart, D. C. 1962. Augmenting human intellect: A con-\nceptual framework. Technical Report AFOSR-3223, Stan-\nford Research Institute.\nEric,\nM.\n2020.\nNeurIPS\n2019\nConvAI\nWork-\nshop\nRecap.\nIEEE\nSignal\nProcessing\nSociety.\nhttps://openai.com/blog/better-language-models [Accessed\nJuly 27, 2020].\n11\nFain, D. C., and Pedersen, J. O. 2006. Sponsored search: A\nbrief history. Bulletin of the American Society for Informa-\ntion Science and Technology 32(2):12–13.\nFazel-Zarandi, M.; Li, S.-W.; Cao, J.; Casale, J.; Henderson,\nP.; Whitney, D.; and Geramifard, A. 2017. Learning robust\ndialog policies in noisy environments. In 1st Workshop on\nConversational AI, NIPS.\nFeng, S.; Fadni, K.; Liao, Q. V.; and Lastras, L. A.\n2019. Doc2Dial: A Framework for Dialogue Composition\nGrounded in Business Documents. In Workshop on Docu-\nment Intelligence, NeurIPS.\nFerreira, E., and Lefvre, F. 2015. Reinforcement-learning\nbased dialogue system for humanrobot interactions with\nsocially-inspired rewards. Computer Speech & Language\n34(1):256–274.\nFinin, T. W.; Joshi, A. K.; and Webber, B. L. 1986. Natural\nlanguage interactions with artiﬁcial experts. Proceedings of\nthe IEEE 74(7):921–938.\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic\nmeta-learning for fast adaptation of deep networks. In Pro-\nceedings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1126–1135. JMLR.org.\nFoerster, J.; Assael, I. A.; De Freitas, N.; and Whiteson, S.\n2016. Learning to communicate with deep multi-agent rein-\nforcement learning. In Proceedings of the 30th Conference\non Advances in Neural Information Processing Systems, pp.\n2137–2145.\nForrester, J. W. 1971. Counterintuitive behavior of social\nsystems. Theory and decision 2(2):pp. 109–140.\nFox, M. S., and Smith, S. F. 1984. Isisa knowledge-based\nsystem for factory scheduling. Expert systems 1(1):25–49.\nFreeman, D.; Ha, D.; and Metz, L. 2019. Learning to Pre-\ndict Without Looking Ahead: World Models Without For-\nward Prediction.\nIn Proceedings of the 33rd Conference\non Advances in Neural Information Processing Systems, pp.\n5380–5391.\nGao, J.; Galley, M.; Li, L.; et al. 2019. Neural Approaches\nto Conversational AI. In Foundations and Trends R⃝in Infor-\nmation Retrieval, volume 13, pp. 127–298. Now Publishers,\nInc.\nGaˇsi´c, M.; Jurˇc´ıˇcek, F.; Thomson, B.; Yu, K.; and Young,\nS. 2011. On-line policy optimisation of spoken dialogue\nsystems via live interaction with human subjects. In IEEE\nWorkshop on Automatic Speech Recognition & Understand-\ning, pp. 312–317. IEEE.\nGaˇsi´c, M.; Breslin, C.; Henderson, M.; Kim, D.; Szummer,\nM.; Thomson, B.; Tsiakoulis, P.; and Young, S. 2013a. On-\nline policy optimisation of bayesian spoken dialogue sys-\ntems via human interaction.\nIn 2013 IEEE International\nConference on Acoustics, Speech and Signal Processing, pp.\n8367–8371. IEEE.\nGaˇsi´c, M.; Breslin, C.; Henderson, M.; Kim, D.; Szummer,\nM.; Thomson, B.; Tsiakoulis, P.; and Young, S.\n2013b.\nPOMDP-based dialogue manager adaptation to extended\ndomains. In Proceedings of the SIGDIAL 2013 Conference,\npp. 214–222. Metz, France: Association for Computational\nLinguistics.\nGaˇsi´c, M.; Mrkˇsi´c, N.; Rojas-Barahona, L. M.; Su, P.-H.;\nUltes, S.; Vandyke, D.; Wen, T.-H.; and Young, S. 2017.\nDialogue manager domain adaptation using gaussian pro-\ncess reinforcement learning. Computer Speech & Language\n45:552–569.\nGass, S. M., and Varonis, E. M. 1994. Input, interaction, and\nsecond language production. Studies in Second Language\nAcquisition 16(3):283–302.\nGoogle AI Blog. Digital Inspiration. 2020. Dictation.io.\nhttp://dictation.io [Accessed July 27, 2020].\nGoogle.\n2020.\nType with your voice: Edit your doc-\nument.\nhttps://support.google.com/docs/answer/4492226\n[Accessed February 22, 2020].\nGoyal, P.; Niekum, S.; and Mooney, R. J. 2019. Using nat-\nural language for reward shaping in reinforcement learning.\nIn Kraus, S., ed., Proceedings of the Twenty-Eighth Interna-\ntional Joint Conference on Artiﬁcial Intelligence, pp. 2385–\n2391. Macao, China: International Joint Conferences on Ar-\ntiﬁcial Intelligence Organization.\nGreyson, A. M.; Hokit, J. D.; Kaptanoglu, M.; Wagner,\nA. M.; and Capps, S. P. 1997. Method and apparatus for\nthe manipulation of text on a computer display screen. US\nPatent 5,666,552. Google Patents.\nGrifﬁth, S.; Subramanian, K.; Scholz, J.; Isbell, C. L.; and\nThomaz, A. L. 2013. Policy shaping: Integrating human\nfeedback with reinforcement learning. In Proceedings of the\n27th Conference on Advances in Neural Information Pro-\ncessing Systems, pp. 2625–2633.\nGrignetti, M. C.; Hausmann, C.; and Gould, L. 1975. An\nintelligent on-line assistant and tutor: NLS-SCHOLAR. In\nProceedings of the May 19-22, 1975, National Computer\nConference and Exposition, pp. 775–781. ACM.\nGrosz, B. 1983. Team: A transportable natural language in-\nterface system. In Proceedings of the 1st Conference on Ap-\nplied Natural Language Processing, pp. 38–45. Santa Mon-\nica, California: Association for Computational Linguistics.\nGruber, T. R., and Clark, G. C. 2017. Dictation that allows\nediting. US Patent App. 15/268,215. Google Patents.\nGu, X.; Cho, K.; Ha, J.-W.; and Kim, S. 2019. DialogWAE:\nMultimodal response generation with conditional wasser-\nstein auto-encoder. In International Conference on Learning\nRepresentations.\nGupta, A.; Zhang, P.; Lalwani, G.; and Diab, M.\n2019.\nCASA-NLU: Context-Aware Self-Attentive Natural Lan-\nguage Understanding for Task-oriented Chatbots. In Pro-\nceedings of EMNLP-IJCNLP 2019.\nHa, D., and Schmidhuber, J. 2018. World models. In Pro-\nceedings of the Thirty-second Conference on Neural Infor-\nmation Processing Systems. Montr´eal, Canada: Curran As-\nsociates, Inc.\nHafner, D.; Lillicrap, T.; Fischer, I.; Villegas, R.; Ha, D.;\nLee, H.; and Davidson, J. 2018. Learning latent dynamics\nfor planning from pixels. In Proceedings of the 36th Interna-\n12\ntional Conference on Machine Learning. Long Beach, USA:\nPMLR.\nHawkins, D. 1968. The nature of purpose. In Purposive\nsystems: Proceedings of the ﬁrst annual symposium of the\nAmerican Society for Cybernetics, pp. 163–173.\nHe, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Osten-\ndorf, M. 2015. Deep reinforcement learning with a natural\nlanguage action space. ArXiv:1511.04636.\nHernandaz, E., and Arkun, Y. 1990. Neural network mod-\neling and an extended dmc algorithm to control nonlinear\nsystems. In 1990 American Control Conference, pp. 2454–\n2459. IEEE.\nHigashinaka, R.; Imamura, K.; Meguro, T.; Miyazaki, C.;\nKobayashi, N.; Sugiyama, H.; Hirano, T.; Makino, T.; and\nMatsuo, Y. 2014. Towards an open-domain conversational\nsystem fully based on natural language processing. In Pro-\nceedings of COLING 2014, the 25th International Confer-\nence on Computational Linguistics: Technical Papers, pp.\n928–939.\nHolland, G. Z.; Talvitie, E. J.; and Bowling, M. 2018. The\nEffect of Planning Shape on Dyna-style Planning in High-\ndimensional State Spaces. ArXiv:1806.01825.\nHoy, M. B. 2018. Alexa, Siri, Cortana, and more: an in-\ntroduction to voice assistants. Medical reference services\nquarterly 37(1):81–88.\nHuang, M.; Zhu, X.; and Gao, J. 2020. Challenges in build-\ning intelligent open-domain dialog systems. ACM Transac-\ntions on Information Systems (TOIS) 38(3):1–32.\nJang, Y.; Lee, J.; and Kim, K.-E. 2019. Bayes-Adaptive\nMonte-Carlo Planning and Learning for Goal-Oriented Dia-\nlogues. In 3rd Workshop on Conversational AI, NeurIPS.\nVancouver, Canada: Neural Information Processing Sys-\ntems.\nJaques, N.; Ghandeharioun, A.; Shen, J. H.; Ferguson, C.;\nLapedriza, A.; Jones, N.; Gu, S.; and Picard, R. 2019. Way\noff-policy batch deep reinforcement learning of implicit hu-\nman preferences in dialog. ArXiv:1907.00456.\nJiang, J.; Hassan Awadallah, A.; Jones, R.; Ozertem, U.; Zi-\ntouni, I.; Gurunath Kulkarni, R.; and Khan, O. Z.\n2015.\nAutomatic Online Evaluation of Intelligent Assistants. In\nProceedings of the 24th International Conference on World\nWide Web, pp. 506–516.\nJiang, Z.; Mao, X.-L.; Huang, Z.; Ma, J.; and Li, S. 2019.\nTowards End-to-End Learning for Efﬁcient Dialogue Agent\nby Modeling Looking-ahead Ability.\nProceedings of the\n20th Annual SIGdial Meeting on Discourse and Dialogue.\nKaiser, L.; Babaeizadeh, M.; Milos, P.; Osinski, B.; Camp-\nbell, R. H.; Czechowski, K.; Erhan, D.; Finn, C.; Koza-\nkowski, P.; Levine, S.; Mohiuddin, A.; Sepassi, R.; Tucker,\nG.; and Michalewski, H. 2019. Model-based reinforcement\nlearning for Atari. ArXiv:1903.00374.\nKaiser, G. E.; Feiler, P. H.; and Popovich, S. S. 1988. Intelli-\ngent assistance for software development and maintenance.\nIEEE software 5(3):40–49.\nKamm, C.\n1995.\nUser interfaces for voice applications.\nIn Proceedings of the National Academy of Sciences, vol-\nume 92, pp. 10031–10037. National Acad Sciences.\nKamthe, S., and Deisenroth, M. 2018. Data-efﬁcient rein-\nforcement learning with probabilistic model predictive con-\ntrol. In Storkey, A., and Perez-Cruz, F., eds., Proceedings of\nthe 21st International Conference on Artiﬁcial Intelligence\nand Statistics, volume 84 of Proceedings of Machine Learn-\ning Research, pp. 1701–1710. Lanzarote, Spain: PMLR.\nKaplan, F.; Oudeyer, P.-Y.; Kubinyi, E.; and Mikl´osi, A.\n2002. Robotic clicker training. Robotics and Autonomous\nSystems 38(3-4):197–206.\nKaplan, R.; Sauer, C.; and Sosa, A.\n2017.\nBeating\nAtari with natural language guided reinforcement learning.\nArXiv:1704.05539.\nKnox, W. B., and Stone, P. 2012. Reinforcement learning\nfrom simultaneous human and MDP reward. In Proceedings\nof the 11th International Conference on Autonomous Agents\nand Multiagent Systems-Volume 1, pp. 475–482. Interna-\ntional Foundation for Autonomous Agents and Multiagent\nSystems.\nKocijan, J.; Murray-Smith, R.; Rasmussen, C. E.; and Gi-\nrard, A. 2004. Gaussian process model based predictive\ncontrol. In Proceedings of the 2004 American control con-\nference, volume 3, pp. 2214–2219. IEEE.\nKozierok, R., and Maes, P. 1993. A learning interface agent\nfor scheduling meetings. In Proceedings of the 1st interna-\ntional conference on Intelligent user interfaces, pp. 81–88.\nACM.\nKudashkina, K.; Chockalingam, V.; Taylor, G. W.; and\nBowling, M. 2020. Sample-Efﬁcient Model-based Actor-\nCritic for an Interactive Dialogue Task. ArXiv:2004.13657.\nLau, T.; Wolfman, S. A.; Domingos, P.; and Weld, D. S.\n2001.\nLearning repetitive text-editing procedures with\nSMARTedit. In Your wish is my command. Elsevier. pp.\n209–XI.\nLazaridou, A.; Pham, N. T.; and Baroni, M.\n2016.\nTo-\nwards multi-agent communication-based language learning.\nArXiv:1605.07133.\nLevin, E.; Pieraccini, R.; and Eckert, W. 1997. Learning di-\nalogue strategies within the Markov decision process frame-\nwork. In IEEE Workshop on Automatic Speech Recognition\nand Understanding Proceedings, pp. 72–79. IEEE.\nLevine, S., and Abbeel, P. 2014. Learning neural network\npolicies with guided policy search under unknown dynam-\nics. In Advances in Neural Information Processing Systems,\npp. 1071–1079. Curran Associates, Inc.\nLevine, S.; Finn, C.; Darrell, T.; and Abbeel, P. 2016. End-\nto-end training of deep visuomotor policies. The Journal of\nMachine Learning Research 17(1):1334–1373.\nLewis, M.; Yarats, D.; Dauphin, Y.; Parikh, D.; and Batra,\nD. 2017. Deal or No Deal? End-to-End Learning of Nego-\ntiation Dialogues. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing, pp.\n2443–2453. Copenhagen, Denmark: Association for Com-\nputational Linguistics.\n13\nLi, J.; Monroe, W.; Ritter, A.; Galley, M.; Gao, J.; and Ju-\nrafsky, D. 2016a. Deep reinforcement learning for dialogue\ngeneration. ArXiv:1606.01541.\nLi, X.; Lipton, Z. C.; Dhingra, B.; Li, L.; Gao, J.; and Chen,\nY.-N.\n2016b.\nA user simulator for task-completion dia-\nlogues. ArXiv:1612.05688.\nLi, J.; Miller, A. H.; Chopra, S.; Ranzato, M.; and Weston, J.\n2017a. Dialogue learning with human-in-the-loop. In Pro-\nceedings of the 5th International Conference on Learning\nRepresentations. Toulon, France: OpenReview.net.\nLi, J.; Miller, A. H.; Chopra, S.; Ranzato, M.; and Weston,\nJ. 2017b. Learning through dialogue interactions by asking\nquestions. In Proceedings of the 5th International Confer-\nence on Learning Representations. Toulon, France: Open-\nReview.net.\nLi, X.; Chen, Y.-N.; Li, L.; Gao, J.; and Celikyilmaz, A.\n2017c.\nEnd-to-end task-completion neural dialogue sys-\ntems.\nIn Proceedings of the The 8th International Joint\nConference on Natural Language Processing, pp. 733–743.\nTaipei, Taiwan: AFNLP.\nLi, M.; Weston, J.; and Roller, S. 2019. ACUTE-EVAL:\nImproved Dialogue Evaluation with Optimized Questions\nand Multi-turn Comparisons. In 3rd Workshop on Conver-\nsational AI, NeurIPS.\nLindgren, N. 1968. Purposive systems: The edge of knowl-\nedge. IEEE spectrum 5(4):89–100.\nLipton, Z.; Li, X.; Gao, J.; Li, L.; Ahmed, F.; and Deng,\nL. 2018. BBQ-networks: Efﬁcient exploration in deep rein-\nforcement learning for task-oriented dialogue systems. In\nThirty-Second AAAI Conference on Artiﬁcial Intelligence,\npp. 5237–5244.\nLison, P. 2013. Model-based bayesian reinforcement learn-\ning for dialogue management. ArXiv:1304.1819.\nListNote.\n2020.\nListNote\nSpeech-to-Text\nNotes.\nhttps://zapier.com/blog/best-text-dictation-\nsoftware/#listnote.\nLitman, D.; Singh, S.; Kearns, M.; and Walker, M. 2000.\nNJFun: a reinforcement learning spoken dialogue system.\nIn ANLP-NAACL 2000 Workshop: Conversational Systems,\npp. 17–20.\nLittman, M. L., and Sutton, R. S. 2001. Predictive repre-\nsentations of state. In Proceedings of the 14th Conference\non Advances in Neural Information Processing Systems, pp.\n1555–1561. MIT Press.\nLiu, B., and Lane, I. 2017a. Iterative policy learning in end-\nto-end trainable task-oriented neural dialog models. In Au-\ntomatic Speech Recognition and Understanding Workshop,\nIEEE, pp. 482–489. IEEE.\nLiu, B., and Lane, I. 2017b. Multi-domain adversarial learn-\ning for slot ﬁlling in spoken language understanding. In 1st\nWorkshop on Conversational AI, NIPS.\nLiu,\nB.;\nT¨ur,\nG.;\nHakkani-T¨ur,\nD.;\nShah,\nP.;\nand\nHeck, L.\n2017.\nEnd-to-end optimization of task-\noriented dialogue model with deep reinforcement learning.\nArXiv:1711.10712.\nLiu, B.; T¨ur, G.; Hakkani-T¨ur, D.; Shah, P.; and Heck, L.\n2018. Dialogue learning with human teaching and feedback\nin end-to-end trainable task-oriented dialogue systems. In\nProceedings of NAACL-HLT, pp. 2060–2069. New Orleans,\nLouisiana: Association for Computational Linguistics.\nLiu, B. 2018. Learning Task-Oriented Dialog with Neu-\nral Network Methods.\nPh.D. Dissertation, Carnegie Mel-\nlon University, Department of Electrical and Computer En-\ngineering.\nLoftin, R. T.; MacGlashan, J.; Peng, B.; Taylor, M. E.;\nLittman, M. L.; Huang, J.; and Roberts, D. L.\n2014.\nA\nstrategy-aware technique for learning behaviors from dis-\ncrete human feedback. In Twenty-Eighth AAAI Conference\non Artiﬁcial Intelligence, pp. 937–943.\nLong, M. H. 1981. Input, interaction and second language\nacquisition. Annals of the New York Academy of Sciences\n379(1):259–278.\nLu, Y., and Smith, S.\n2007.\nAugmented Reality E-\nCommerce Assistant System: Trying While Shopping. In\nJacko, J. A., ed., Human-Computer Interaction. Interaction\nPlatforms and Techniques, pp. 643–652. Berlin, Heidelberg:\nSpringer Berlin Heidelberg.\nLucas, M.; Miko, S.; and Bennington, S. A. 2004. Method\nand apparatus for voice dictation and document production.\nUS Patent 6,834,264. Google Patents.\nMadotto, A.; Lin, Z.; Wu, C.-S.; Shin, J.; and Fung, P. 2019.\nAttention over Parameters for Dialogue Systems.\nIn 3rd\nWorkshop on Conversational AI, NeurIPS.\nMaheswaran, R. T.; Tambe, M.; Varakantham, P.; and My-\ners, K. 2003. Adjustable autonomy challenges in personal\nassistant agents: A position paper. In International Work-\nshop on Computational Autonomy, AAMAS, pp. 187–194.\nSpringer.\nManuvinakurike, R.; Bui, T.; Chang, W.; and Georgila, K.\n2018.\nConversational image editing: Incremental intent\nidentiﬁcation in a new dialogue task. In Proceedings of the\n19th Annual SIGdial Meeting on Discourse and Dialogue,\npp. 284–295.\nMathewson, K. W., and Pilarski, P. M. 2016. Simultane-\nous control and human feedback in the training of a robotic\nagent with actor-critic reinforcement learning. In Workshop\non Interactive Machine Learning, IJCAI.\nMcDermott, J. 1982. XSEL: A computer salesperson’s as-\nsistant. Machine intelligence 10(1):235–337.\nMendez, J. A.; Geramifard, A.; Ghavamzadeh, M.; and Liu,\nB. 2019. Reinforcement Learning of Multi-Domain Dia-\nlog Policies Via Action Embeddings. In 3rd Workshop on\nConversational AI, NeurIPS.\nMicrosoft. 2020. Windows Speech Recognition commands.\nhttps://support.microsoft.com/en-ca/help/12427/windows-\nspeech-recognition-commands.\nAccessed\nFebruary\n23,\n2020.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M.\n2013.\nPlaying Atari with Deep Reinforcement Learning. In Deep\nLearning Workshop, NIPS.\n14\nMoore, J. D., and Paris, C. L. 1989. Planning text for advi-\nsory dialogues. In Proceedings of the 27th Annual Meeting\non Association for Computational Linguistics, pp. 203–211.\nAssociation for Computational Linguistics.\nMoore, A. 2015. Artiﬁcial Intelligence: 10 Things To Know.\nMordatch, I., and Abbeel, P. 2018. Emergence of grounded\ncompositional language in multi-agent populations.\nIn\nThirty-Second AAAI Conference on Artiﬁcial Intelligence.\nAAAI.\nNg, A. Y.; Kim, H. J.; Jordan, M. I.; and Sastry, S. 2004.\nAutonomous helicopter ﬂight via reinforcement learning. In\nThrun, S.; Saul, L. K.; and Sch¨olkopf, B., eds., Proceedings\nof the 16th Conference on Advances in Neural Information\nProcessing Systems. MIT Press. pp. 799–806.\nNg, A. Y.; Coates, A.; Diel, M.; Ganapathi, V.; Schulte, J.;\nTse, B.; Berger, E.; and Liang, E. 2006. Autonomous in-\nverted helicopter ﬂight via reinforcement learning. In Ex-\nperimental robotics IX. Springer. pp. 363–372.\nNonaka, Y.; Sakai, Y.; Yasuda, K.; and Nakano, Y. 2012. To-\nwards assessing the communication responsiveness of peo-\nple with dementia. In International Conference on Intelli-\ngent Virtual Agents, pp. 496–498. Springer.\nNortmann, N.; Rekauzke, S.; Onat, S.; K¨onig, P.; and Jancke,\nD. 2015. Primary visual cortex represents the difference be-\ntween past and present. Cerebral Cortex 25(6):1427–1440.\nNuance Communications, Inc.\n2020.\nNuance Winscribe\nDictation.\nPaolacci, G.; Chandler, J.; and Ipeirotis, P. G. 2010. Running\nExperiments on Amazon Mechanical Turk. Judgment and\nDecision making 5(5):411–419.\nPapangelis, A.; Namazifar, M.; Khatri, C.; Wang, Y.-C.;\nMolino, P.; and T¨ur, G.\n2020.\nPlato Dialogue Sys-\ntem: A Flexible Conversational AI Research Platform.\nArXiv:2001.06463.\nParr, R.; Li, L.; Taylor, G.; Painter-Wakeﬁeld, C.; and\nLittman, M. L. 2008. An analysis of linear models, lin-\near value-function approximation, and feature selection for\nreinforcement learning. In Proceedings of the 25th interna-\ntional conference on Machine learning, pp. 752–759.\nPeng, B.; Li, X.; Li, L.; Gao, J.; Celikyilmaz, A.; Lee, S.;\nand Wong, K.-F.\n2017.\nComposite task-completion dia-\nlogue policy learning via hierarchical deep reinforcement\nlearning. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, pp. 2231–2240.\nCopenhagen, Denmark: Association for Computational Lin-\nguistics.\nPeng, B.; Li, X.; Gao, J.; Liu, J.; and Wong, K.-F. 2018.\nDeep Dyna-Q: Integrating Planning for Task-Completion\nDialogue Policy Learning. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers).\nAssociation for Computa-\ntional Linguistics.\nPica, T.; Doughty, C. J.; and Young, R. 1986. Making input\ncomprehensible: Do interactional modiﬁcations help? ITL-\nInternational Journal of Applied Linguistics 72(1):1–25.\nPica, T. 1987. Second-language acquisition, social interac-\ntion, and the classroom. Applied linguistics 8(1):3–21.\nPilarski, P. M., and Sutton, R. S. 2012. Between instruction\nand reward: human-prompted switching. Technical Report\nFS-12-07, AAAI 2012 Fall Symposium on Robots Learn-\ning Interactively from Human Teachers (RLIHT), Arlington,\nVA.\nPilarski, P. M.; Dawson, M. R.; Degris, T.; Fahimi, F.; Carey,\nJ. P.; and Sutton, R. S. 2011. Online human training of a my-\noelectric prosthesis controller via actor-critic reinforcement\nlearning.\nIn Proceedings of the IEEE International Con-\nference on Rehabilitation Robotics, pp. 134–140. Zurich,\nSwitzerland: IEEE.\nPilarski, P. M.; Sutton, R. S.; Mathewson, K. W.; Sherstan,\nC.; Parker, A. S.; and Edwards, A. L. 2017. Communicative\nCapital for Prosthetic Agents. ArXiv:1711.03676.\nPilarski, P. M.; Sutton, R. S.; and Mathewson, K. W. 2015.\nProsthetic devices as goal-seeking agents. In Second Work-\nshop on Present and Future of Non-Invasive Peripheral-\nNervous-System Machine Interfaces: Progress in Restoring\nthe Human Functions, PNS-MI.\nPineau, J.; Montemerlo, M.; Pollack, M.; Roy, N.; and\nThrun, S.\n2003.\nTowards robotic assistants in nursing\nhomes: Challenges and results. Robotics and autonomous\nsystems 42(3-4):271–281.\nPollack, M. E.; Brown, L.; Colbry, D.; Orosz, C.; Peintner,\nB.; Ramakrishnan, S.; Engberg, S.; Matthews, J. T.; Dunbar-\nJacob, J.; McCarthy, C. E.; et al. 2002. Pearl: A mobile\nrobotic assistant for the elderly. In Workshop on automation\nas eldercare, AAAI, pp. 85–91.\nPollack, M. E.; Hirschberg, J.; and Webber, B. 1982. User\nparticipation in the reasoning processes of expert systems. In\nProceedings First National Conference on Artiﬁcial Intelli-\ngence (AAAI-82), pp. 358–361. University of Pennsylvania.\nPoole, D. L., and Mackworth, A. K. 2010. Artiﬁcial Intel-\nligence: foundations of computational agents. Cambridge\nUniversity Press.\nPower, R. 1974. A computer model of conversation. Ph.D.\nDissertation, The University of Edinburg.\nQuarteroni, S., and Manandhar, S. 2009. Designing an in-\nteractive open-domain question answering system. Natural\nLanguage Engineering 15(1):73–95.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei,\nD.;\nand\nSutskever,\nI.\n2019.\nLanguage\nmodels\nare unsupervised multitask learners.\nOpenAI Blog.\nhttps://openai.com/blog/better-language-models. [Accessed\nJuly 23, 2020].\nRanzato, M.; Chopra, S.; Auli, M.; and Zaremba, W. 2015.\nSequence level training with recurrent neural networks.\nArXiv:1511.06732.\nRao, A. P. 2011. Predictive speech-to-text input. US Patent\n7,904,298. Google Patents.\nRastogi, A.; Hakkani-T¨ur, D.; and Heck, L. 2017. Scalable\nmulti-domain dialogue state tracking. In IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU),\npp. 561–568. IEEE.\n15\nRumelhart, D. E.; Smolensky, P.; McClelland, J. L.; and Hin-\nton, G. 1986. Sequential thought processes in PDP mod-\nels. Parallel Distributed Processing: Explorations in the Mi-\ncrostructures of Cognition 2:pp. 7–57. MIT Press.\nSaleh, A.; Jaques, N.; Ghandeharioun, A.; Shen, J. H.; and\nPicard, R. 2019. Hierarchical reinforcement learning for\nopen-domain dialog. In 3rd Workshop on Conversational\nAI, NeurIPS.\nSalichs, M. A.; Ge, S. S.; Barakova, E.; Cabibihan, J.-J.;\nWagner, A. R.; Gonzalez, A. C.; and He, H., eds.\n2019.\nPreface–Social Robotics: 11th International Conference,\nICSR. Springer.\nSchatzmann, J.; Thomson, B.; Weilhammer, K.; Ye, H.; and\nYoung, S. 2007. Agenda-based user simulation for boot-\nstrapping a POMDP dialogue system. In Human Language\nTechnologies 2007: The Conference of the North American\nChapter of the Association for Computational Linguistics;\nCompanion Volume, Short Papers, pp. 149–152. Associa-\ntion for Computational Linguistics.\nSchrittwieser, J.; Antonoglou, I.; Hubert, T.; Simonyan, K.;\nSifre, L.; Schmitt, S.; Guez, A.; Lockhart, E.; Hassabis, D.;\nGraepel, T.; Lillicrap, T.; and Silver, D. 2019. Mastering\nAtari, Go, chess and shogi by planning with a learned model.\nIn Proceedings of the Thirty-third Conference on Neural In-\nformation Processing Systems. Vancouver, Canada: Curran\nAssociates, Inc.\nSelfridge, O. G. 1993. The Gardens of Learning: A Vision\nfor AI. AI Magazine 14(2):36–48.\nSerban, I. V.; Sankar, C.; Germain, M.; Zhang, S.; Lin,\nZ.; Subramanian, S.; Kim, T.; Pieper, M.; Chandar, S.; Ke,\nN. R.; et al. 2017a. A deep reinforcement learning chatbot.\nArXiv:1709.02349.\nSerban, I. V.; Sordoni, A.; Lowe, R.; Charlin, L.; Pineau, J.;\nCourville, A.; and Bengio, Y. 2017b. A hierarchical latent\nvariable encoder-decoder model for generating dialogues. In\nProceedings of the Thirty-First AAAI Conference on Artiﬁ-\ncial Intelligence, pp. 3295–3301. AAAI.\nShah, P.; Hakkani-T¨ur, D.; T¨ur, G.; Rastogi, A.; Bapna, A.;\nNayak, N.; and Heck, L. 2018. Building a conversational\nagent overnight with dialogue self-play. ArXiv:1801.04871.\nShah, P.; Hakkani-T¨ur, D.; and Heck, L. 2016. Interactive\nreinforcement learning for task-oriented dialogue manage-\nment. In Workshop on Deep Learning for Action and Inter-\naction, NIPS.\nShen, Y.; Huang, P.-S.; Gao, J.; and Chen, W. 2017. Rea-\nsoNet: Learning to Stop Reading in Machine Comprehen-\nsion. In Proceedings of the 23rd ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data Min-\ning, pp. 1047–1055.\nShi, Z.; Chen, X.; Qiu, X.; and Huang, X. 2018. Toward\nDiverse Text Generation with Inverse Reinforcement Learn-\ning. Proceedings of the Twenty-Seventh International Joint\nConference on Artiﬁcial Intelligence.\nShin, J.; Xu, P.; Madotto, A.; and Fung, P. 2019. Happy-\nbot: Generating empathetic dialogue responses by improv-\ning user experience look-ahead. ArXiv:1906.08487.\nShu, L.; Molino, P.; Namazifar, M.; Liu, B.; Xu, H.; Zheng,\nH.; and T¨ur, G. 2018. Incorporating the structure of the\nbelief state in end-to-end task-oriented dialogue systems. In\n2nd Workshop on Conversational AI, NIPS, volume 32.\nSilver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.;\nHuang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton,\nA.; et al. 2017a. Mastering the game of Go without human\nknowledge. Nature 550(7676):354–359.\nSilver, D.; van Hasselt, H.; Hessel, M.; Schaul, T.; Guez,\nA.; Harley, T.; Dulac-Arnold, G.; Reichert, D.; Rabinowitz,\nN.; Barreto, A.; et al. 2017b. The Predictron: End-to-end\nlearning and planning.\nIn Proceedings of the 34th Inter-\nnational Conference on Machine Learning-Volume 70, pp.\n3191–3199. JMLR. org.\nSilver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,\nM.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,\nT.; Lillicrap, T.; Simonyan, K.; and Hassabis, D. 2018. A\ngeneral reinforcement learning algorithm that masters chess,\nshogi, and Go through self-play. Science 362(6419):1140–\n1144.\nSimmons, R., and Slocum, J.\n1972.\nGenerating English\ndiscourse from semantic networks. Communications of the\nACM 15(10):891–905.\nSingh, S. P.; Kearns, M. J.; Litman, D. J.; and Walker, M. A.\n2000. Reinforcement learning for spoken dialogue systems.\nIn Proceedings of the 13th Conference on Advances in Neu-\nral Information Processing Systems, pp. 956–962.\nSingh, S.; Litman, D.; Kearns, M.; and Walker, M. 2002a.\nOptimizing dialogue management with reinforcement learn-\ning: Experiments with the NJFun system. Journal of Artiﬁ-\ncial Intelligence Research 16:105–133.\nSingh, S.; Litman, D.; Kearns, M.; and Walker, M. 2002b.\nOptimizing dialogue management with reinforcement learn-\ning: Experiments with the NJFun system. Artiﬁcial Intelli-\ngence 16:pp. 105–133.\nSingh, S. P. 1992. Reinforcement learning with a hierarchy\nof abstract models. In Proceedings of the National Confer-\nence on Artiﬁcial Intelligence, number 10, p.202. Citeseer.\nSkantze, G. 2016. Real-time coordination in human-robot\ninteraction using face and voice. AI Magazine 37(4):19–31.\nSmith, R. W., and Hipp, D. R. 1994. Spoken natural lan-\nguage dialog systems: A practical approach. Oxford Uni-\nversity Press.\nSmith, E. M.; Williamson, M.; Shuster, K.; Weston, J.;\nand Boureau, Y.-L.\n2020.\nCan You Put it All Together:\nEvaluating Conversational Agents’ Ability to Blend Skills.\nArXiv:2004.08449.\nStent, A.; Prasad, R.; and Walker, M.\n2004.\nTrainable\nsentence planning for complex information presentation in\nspoken dialog systems. In Proceedings of the 42nd Annual\nMeeting on Association for Computational Linguistics, ACL\n04, pp. 79–87. Barcelona, Spain: Association for Computa-\ntional Linguistics.\nSu, P.-H.; Gaˇsi´c, M.; Mrkˇsic, N.; Rojas-Barahona, L.;\nUltes, S.; Vandyke, D.; Wen, T.-H.; and Young, S.\n16\n2016. Continuously learning neural dialogue management.\nArXiv:1606.02689.\nSu, P.-H.; Budzianowski, P.; Ultes, S.; Gaˇsi´c, M.; and Young,\nS. 2017. Sample-efﬁcient actor-critic reinforcement learning\nwith supervised data for dialogue management. In Proceed-\nings of the SIGDIAL 2017 Conference, pp. 147–157.\nSu, S.-Y.; Li, X.; Gao, J.; Liu, J.; and Chen, Y.-N. 2018.\nDiscriminative Deep Dyna-Q: Robust Planning for Dialogue\nPolicy Learning.\nIn Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, pp.\n3813–3823. Brussels, Belgium: Association for Computa-\ntional Linguistics.\nSukhbaatar, S.; Szlam, A.; and Fergus, R. 2016. Learning\nmultiagent communication with backpropagation. In Pro-\nceedings of the 30th Conference on Advances in Neural In-\nformation Processing Systems, pp. 2244–2252.\nSun, M.; Chen, Y.-N.; and Rudnicky, A. I. 2016. An intelli-\ngent assistant for high-level task understanding. In Proceed-\nings of the 21st International Conference on Intelligent User\nInterfaces, pp. 169–174. Sonoma, USA: ACM.\nSutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence\nto sequence learning with neural networks. In Proceedings\nof the 28th Conference on Advances in neural information\nprocessing systems, pp. 3104–3112.\nSutton, R. S., and Barto, A. G. 1981. An adaptive network\nthat constructs and uses and internal model of its world.\nCognition and Brain Theory 4(3):217–246.\nSutton, R. S., and Barto, A. G. 2018. Reinforcement learn-\ning: An introduction. MIT press.\nSutton, R. S., and Pinette, B. 1985. The learning of world\nmodels by connectionist networks. In Proceedings of the\nseventh annual conference of the cognitive science society,\npp. 54–64.\nSutton, R. S.; Szepesv´ari, C.; Geramifard, A.; and Bowling,\nM. P. 2008. Dyna-style planning with linear function ap-\nproximation and prioritized sweeping. In Proceedings of the\n24th Conference on Uncertainty in Artiﬁcial Intelligence,\npp. 528–536.\nSutton, R. S.; Singh, S.; and Precup, D. 1999. Between\nMDPs and semi-MDPs: Learning, planning, and represent-\ning knowledge at multiple temporal scales. Artiﬁcial Intelli-\ngence 112:118–211.\nSutton, R. S.\n1990.\nIntegrated architectures for learn-\ning, planning, and reacting based on approximating dynamic\nprogramming. In Proceedings of the Seventh International\nConference on Machine Learning, pp. 216–224.\nMorgan\nKaufmann.\nSutton, R. S. 2019. Toward a New Approach to Model-\nbased Reinforcement Learning. Technical report, University\nof Alberta.\nSutton, R. S. 2020. Experience and Intelligence: Toward a\nScalable AI-Agent Architecture. Vector Institute for Artiﬁ-\ncial Intelligence, Visitor Talk.\nTakahashi, F. 2001. Document editing system and method.\nUS Patent 6,202,073. Google Patents.\nTang, D.; Li, X.; Gao, J.; Wang, C.; Li, L.; and Jebara, T.\n2018.\nSubgoal discovery for hierarchical dialogue policy\nlearning. ArXiv:1804.07855.\nThomaz, A. L.; Hoffman, G.; and Breazeal, C. 2005. Real-\ntime interactive reinforcement learning for robots. In Work-\nshop on human comprehensible machine learning, AAAI.\nThomaz, A. L.; Hoffman, G.; and Breazeal, C. 2006. Re-\ninforcement Learning with Human Teachers: Understanding\nHow People Want to Teach Robots. In The 15th IEEE Inter-\nnational Symposium on Robot and Human Interactive Com-\nmunication, RO-MAN 2006, pp. 352–357. IEEE.\nThompson, C. 2006. Google’s China Problem. The Power of\nInformation. School of Journalism, Stony Brook University.\nTodorov, E.; Erez, T.; and Tassa, Y. 2012. Mujoco: A physics\nengine for model-based control. In IEEE/RSJ International\nConference on Intelligent Robots and Systems, pp. 5026–\n5033. IEEE.\nTractica. 2016. The Virtual Digital Assistant Market Will\nReach $15.8 Billion Worldwide by 2021. A market intel-\nligence\nﬁrm.\nhttps://www.tractica.com/newsroom/press-\nreleases/the-virtual-digital-assistant-market-will-reach-15-\n8-billion-worldwide-by-2021/ Accessed [July 23, 2020].\nT¨ur, G.; Stolcke, A.; Voss, L.; Peters, S.; Hakkani-T¨ur, D.;\nDowding, J.; Favre, B.; Fern´andez, R.; Frampton, M.; Frand-\nsen, M.; et al. 2010. The CALO Meeting Assistant System.\nIEEE Transactions on Audio, Speech, and Language Pro-\ncessing 18(6):1601–1611.\nT¨ur, G.; Hakkani-T¨ur, D.; and Schapire, R. E. 2005. Com-\nbining active and semi-supervised learning for spoken lan-\nguage understanding.\nSpeech Communication 45(2):171–\n186.\nVeeriah, V.; Pilarski, P. M.; and Sutton, R. S. 2016. Face\nvaluing: Training user interfaces with facial expressions and\nreinforcement learning. In Workshop on Interactive Machine\nLearning, IJCAI.\nWahlstr¨om, N.; Sch¨on, T. B.; and Deisenroth, M. P. 2015.\nFrom pixels to torques: Policy learning with deep dynamical\nmodels. ArXiv:1502.02251.\nWalker, D. E., and Grosz, B. J. 1978. Understanding spoken\nlanguage. Elsevier Science Inc.\nWalker, M. A.; Stent, A.; Mairesse, F.; and Prasad, R.\n2007. Individual and domain adaptation in sentence plan-\nning for dialogue. Journal of Artiﬁcial Intelligence Research\n30:413–456.\nWalker, R. C. 1998. Text processor. US Patent 5,802,533.\nGoogle Patents.\nWalker, M. A. 2000. An application of reinforcement learn-\ning to dialogue strategy selection in a spoken dialogue sys-\ntem for email. Journal of Artiﬁcial Intelligence Research\n12:pp. 387–416.\nWang, Z.; Schaul, T.; Hessel, M.; Van Hasselt, H.; Lanctot,\nM.; and De Freitas, N. 2016. Dueling network architectures\nfor deep reinforcement learning. In Proceedings of the 33 rd\nInternational Conference on Machine Learning, volume 48.\nNew York, USA: JMLR: W&CP.\n17\nWang, Y.; Si, P.; Lei, Z.; Xun, G.; and Yang, Y.\n2019.\nHSCJN: A Holistic Semantic Constraint Joint Network for\nDiverse Response Generation. In 3rd Workshop on Conver-\nsational AI, NeurIPS.\nWaters, R. C. 1986. KBEmacs: Where’s the AI? AI Maga-\nzine 7(1):47–47.\nWatter, M.; Springenberg, J.; Boedecker, J.; and Riedmiller,\nM.\n2015.\nEmbed to control: A locally linear latent dy-\nnamics model for control from raw images. In Proceedings\nof the 28th International Conference on Neural Information\nProcessing Systems, pp. 2746–2754. Cambridge,USA: MIT\nPress.\nWeisz, G.; Budzianowski, P.; Su, P.-H.; and Gaˇsi´c, M. 2018.\nSample efﬁcient deep reinforcement learning for dialogue\nsystems with large action spaces. IEEE/ACM Transactions\non Audio, Speech, and Language Processing 26(11):2083–\n2097.\nWen, T.-H.; Vandyke, D.; Mrkˇsic, N.; Gaˇsi´c, M.; Rojas Bara-\nhona, L. M.; Su, P.-H.; Ultes, S.; and Young, S.\n2017.\nA Network-based End-to-End Trainable Task-oriented Di-\nalogue System. In Proceedings of the 15th Conference of\nthe European Chapter of the Association for Computational\nLinguistics: Volume 1, Long Papers, pp. 437–449. Valencia,\nSpain: Association for Computational Linguistics.\nWiering, M.; Salustowicz, R.; and Schmidhuber, J. 2001.\nModel-based reinforcement learning for evolving soccer\nstrategies. In Computational intelligence in games, pp. 99–\n132. Springer.\nWilliams, J. D.; Asadi, K.; and Zweig, G. 2017. Hybrid code\nnetworks: practical and efﬁcient end-to-end dialog control\nwith supervised and reinforcement learning. In Proceedings\nof the 55th Annual Meeting of the Association for Computa-\ntional Linguistics. Association for Computational Linguis-\ntics.\nWinograd, T. 1971. Procedures as a representation for data\nin a computer program for understanding natural language.\nTechnical report, MIT.\nWinograd, T.\n1973.\nBreaking the complexity barrier\nagain. In ACM SIGIR Forum, volume 9, pp. 13–30. ACM.\nReprinted in David R. Barstow, Howard E. Shrobe, and Erik\nSandewall (Eds.), Interactive Programming Environments,\nMcGraw-Hill Book Co., New York, 1984.\nWoodrow, H. 1946. The ability to learn. Psychological\nReview 53(3):147–158.\nWoods, W. 1984. Natural language communication with\nmachines: An ongoing goal. Artiﬁcial intelligence applica-\ntions for business pp. 195–209.\nWu, Y.; Li, X.; Liu, J.; Gao, J.; and Yang, Y. 2019. Switch-\nbased active deep Dyna-Q: Efﬁcient adaptive planning for\ntask-completion dialogue policy learning. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, volume 33,\npp. 7289–7296.\nXu, C.; Wu, W.; and Wu, Y. 2018. Towards explainable and\ncontrollable open domain dialogue generation with dialogue\nacts. ArXiv:1807.07255.\nYarats, D., and Lewis, M. 2017. Hierarchical Text Gener-\nation and Planning for Strategic Dialogue. In Proceedings\nof the 35th International Conference on Machine Learning.\nStockholm, Sweden: PMRL 80.\nYoung, S.; Gaˇsi´c, M.; Thomson, B.; and Williams, J. D.\n2013. POMDP-based statistical spoken dialog systems: A\nreview. In Proceedings of the IEEE, volume 101, pp. 1160–\n1179. IEEE.\nYu, L.; Zhang, W.; Wang, J.; and Yu, Y. 2017. Seqgan:\nSequence generative adversarial nets with policy gradient.\nIn Thirty-First AAAI Conference on Artiﬁcial Intelligence.\nAAAI.\nZhang, Z.; Takanobu, R.; Huang, M.; and Zhu, X. 2020. Re-\ncent Advances and Challenges in Task-oriented Dialog Sys-\ntem. Science China Information Sciences. Under Review.\nZhang, J.; Zhao, T.; and Yu, Z. 2018. Multimodal hierar-\nchical reinforcement learning policy for task-oriented visual\ndialog. ArXiv:1805.03257.\nZhao, T., and Eskenazi, M. 2016. Towards end-to-end learn-\ning for dialog state tracking and management using deep re-\ninforcement learning. In Proceedings of the SIGDIAL 2016\nConference, 1–10. Los Angeles: Association for Computa-\ntional Linguistics.\nZhao, Y.; Wang, Z.; Yin, K.; Zhang, R.; Huang, Z.; and\nWang, P. Dynamic Reward-based Dueling Deep Dyna-Q:\nRobust Policy Learning in Noisy Environments. In Thirty-\nFourth AAAI Conference on Artiﬁcial Intelligence.\nZheng, Y.; Chen, G.; Huang, M.; Liu, S.; and Zhu, X. 2019.\nPersonalized dialogue generation with diversiﬁed traits. In\n3rd Workshop on Conversational AI, NeurIPS.\nZhou, M.; Arnold, J.; and Yu, Z.\n2019.\nBuilding task-\noriented visual dialog systems through alternative optimiza-\ntion between dialog policy and language generation. In Pro-\nceedings of EMNLP-IJCNLP 2019.\nZhou, L.; Small, K.; Rokhlenko, O.; and Elkan, C. 2017.\nEnd-to-end ofﬂine goal-oriented dialog policy learning via\npolicy gradient. In Workshop on Conversational AI, NIPS.\nZhou, L.; Gao, J.; Li, D.; and Shum, H.-Y. 2020. The De-\nsign and Implementation of XiaoIce, an Empathetic Social\nChatbot. Computational Linguistics 46(1):53–93.\n18\nKatya Kudashkina is pursuing her\nPhD at the Vector Institute for Arti-\nﬁcial Intelligence and the University\nof Guelph, working closely with RLAI\nLab at University of Alberta. In the\npast, she has founded two AI startups:\nUDIO AgTech, and Cultura. Prior to\nthat she spent over six years at the Canada Pension Plan\nInvestment Board and at IBM. She studied Engineering in\nRussia, and then moved to Canada where she completed a\ndegree in Computer Science and then received her MBA at\nthe University of Toronto.\nDr. Patrick M. Pilarski is a Canada\nResearch Chair in Machine Intelli-\ngence for Rehabilitation at the Univer-\nsity of Alberta, an Associate Profes-\nsor in the Department of Medicine, and\na Fellow of the Alberta Machine In-\ntelligence Institute. As part of his re-\nsearch, Dr. Pilarski explores new machine learning tech-\nniques for sensorimotor control and prediction, including re-\ninforcement learning methods for human-machine interac-\ntion, communication, and user-speciﬁc device optimization.\nDr. Pilarski is the author or co-author of more than 80 peer-\nreviewed articles, a Senior Member of the IEEE, and has\nbeen supported by provincial, national, and international re-\nsearch grants.\nRichard S. Sutton is a distinguished\nresearch scientist at DeepMind in Ed-\nmonton and a professor in the De-\npartment of Computing Science at the\nUniversity of Alberta. Prior to joining\nDeepMind in 2017 and the University\nof Alberta in 2003, he worked in indus-\ntry at AT&T and GTE Labs, and in academia at the Univer-\nsity of Massachusetts. He received a PhD in computer sci-\nence from the University of Massachusetts in 1984 and a\nBA in psychology from Stanford University in 1978. He is\nco-author of the textbook Reinforcement Learning: An In-\ntroduction from MIT Press. He is also a fellow of the Royal\nSociety of Canada, the Association for the Advancement of\nArtiﬁcial Intelligence, the Alberta Machine Intelligence In-\nstitute, and CIFAR.\n19\n",
  "categories": [
    "cs.AI",
    "cs.HC",
    "cs.LG"
  ],
  "published": "2020-08-27",
  "updated": "2020-08-27"
}