{
  "id": "http://arxiv.org/abs/2008.13044v1",
  "title": "Reinforcement Learning with Feedback-modulated TD-STDP",
  "authors": [
    "Stephen Chung",
    "Robert Kozma"
  ],
  "abstract": "Spiking neuron networks have been used successfully to solve simple\nreinforcement learning tasks with continuous action set applying learning rules\nbased on spike-timing-dependent plasticity (STDP). However, most of these\nmodels cannot be applied to reinforcement learning tasks with discrete action\nset since they assume that the selected action is a deterministic function of\nfiring rate of neurons, which is continuous. In this paper, we propose a new\nSTDP-based learning rule for spiking neuron networks which contains feedback\nmodulation. We show that the STDP-based learning rule can be used to solve\nreinforcement learning tasks with discrete action set at a speed similar to\nstandard reinforcement learning algorithms when applied to the CartPole and\nLunarLander tasks. Moreover, we demonstrate that the agent is unable to solve\nthese tasks if feedback modulation is omitted from the learning rule. We\nconclude that feedback modulation allows better credit assignment when only the\nunits contributing to the executed action and TD error participate in learning.",
  "text": "Reinforcement Learning with Feedback-modulated\nTD-STDP\nStephen Chung\nDepartment of Computer Science\nUniversity of Massachusetts Amherst\nAmherst, MA 01003\nminghaychung@umass.edu\nRobert Kozma\nDepartment of Computer Science\nUniversity of Massachusetts Amherst\nAmherst, MA 01003\nrkozma55@gmail.com\nAbstract\nSpiking neuron networks have been used successfully to solve simple reinforcement\nlearning tasks with continuous action set applying learning rules based on spike-\ntiming-dependent plasticity (STDP). However, most of these models cannot be\napplied to reinforcement learning tasks with discrete action set since they assume\nthat the selected action is a deterministic function of ﬁring rate of neurons, which\nis continuous. In this paper, we propose a new STDP-based learning rule for\nspiking neuron networks which contains feedback modulation. We show that the\nSTDP-based learning rule can be used to solve reinforcement learning tasks with\ndiscrete action set at a speed similar to standard reinforcement learning algorithms\nwhen applied to the CartPole and LunarLander tasks. Moreover, we demonstrate\nthat the agent is unable to solve these tasks if feedback modulation is omitted\nfrom the learning rule. We conclude that feedback modulation allows better credit\nassignment when only the units contributing to the executed action and TD error\nparticipate in learning.\n1\nIntroduction\nIn recent years, spiking neural networks (SNNs) gained popularity in solving machine learning tasks\n[1–4]. Still, recent advances in deep learning mostly focus on artiﬁcial neural networks (ANNs)\ninstead of SNNs, partly because ANN can be trained efﬁciently by back-propagation, while SNNs\nhave problems with back-propagation since their units communicate by binary spikes, therefore the\nnetwork is non-differentiable [5]. However, back-propagation is generally believed to be biologically\nimplausible [6]. However, spike-timing dependent plasticity (STDP) and reward-modulated STDP\n(R-STDP) have been experimentally observed in biological neural systems [7]. STDP can be\nderived theoretically as a result of maximizing mutual information between presynaptic neuron and\npostsynaptic neuron [8], while R-STDP can be derived theoretically as a result of maximizing a\nglobal reward signal [9, 10].\nPrevious works have shown some success in solving reinforcement learning tasks with SNN using\nSTDP-based learning rules. For tasks with continuous action set, [11, 10] showed that R-STDP or\nTD-error-modulated STDP (TD-STDP), a learning rule similar to R-STDP but with reward replaced\nwith temporal difference error (TD error), can be used to solve reinforcement learning tasks with\ncontinuous action set such as maze task and inverted CartPole. [12] showed success in solving a\nlane-keeping task using R-STDP.\nFor tasks with discrete action set, [13] used a population of SNNs to successfully learn tasks such\nas Mountain Car and CartPole, but the learning is much slower than that of standard reinforcement\nlearning algorithms. [14] solved the task of grid-world successfully, but their algorithm could only\nPreprint. Under review.\narXiv:2008.13044v1  [cs.LG]  29 Aug 2020\napply in tasks with ﬁnite and moderate number of states. [15] used R-STDP to solve a simpliﬁed\nversion of Pong.\nIn these SNN models (except [13]), the action chosen is a deterministic function of ﬁring rate of\nactor neurons, therefore they can only rely on the stochastic activity of spike train to encourage\nexploration. However, this way of encouraging exploration is inefﬁcient since it is difﬁcult for all\nneurons to have their independent random noise aligned. For instance, when using the average of a\ngroup of neurons’ ﬁring rate as strength of action, the random noise necessary for exploration will\nmostly cancel out after taking the average. This method of exploration works only if the action set\nis continuous and the reward function is continuous function of action, since a very small random\nnoise in action is sufﬁcient to estimate change in return, which is essentially equivalent to gradient\nestimation by numerical approximation. If the action set is discrete and a threshold function is used\nto convert ﬁring rate to action, the gradient will be zero almost everywhere and there will be no\nexploration if noise is too small.\nActor-critic algorithms are one of the earliest and most popular methods investigated in reinforcement\nlearning [16, 17]. The actor learning rule of actor-critic algorithm is similar to long-term potentiation\n(LTP) side of R-STDP, but with state replaced by traces of incoming spikes in SNN. It has also been\nhypothesized that actor and critic network may correspond to dorsal and ventral subdivision of the\nstriatum in the biological neural system [18], and TD error representing dopamine modulation in\nbiological neural systems [19].\nRecent neuroscience studies indicate that feedback connections may have roles in learning and\nattention. [20] summarized different evidences indicating the presence of feedback modulation in\nlearning synaptic weight, and stated a gating hypothesis to explain the evidences, which suggests\nthat “response selection elicits feedback signals that enable the plasticity of upstream synapses”. The\nfeedback-modulated TD-STDP learning rule we propose corresponds to this hypothesis. [21, 22] also\nproposed learning rule with feedback modulation for rate-based neurons. As the main goal of this\npaper is to solve reinforcement learning task with SNN, we refer readers to [20] for relevant works\non feedback modulation in neuroscience.\nIn this paper, we investigate how to solve a reinforcement learning task efﬁciently with discrete action\nset using SNN and STDP-based learning rules. We propose an actor-critic architecture that treats actor\nneurons’ output as the probability of choosing an action instead of the action itself. Since the action\nchosen is no longer a deterministic function of actor neurons’ output, we also propose a new learning\nrule, which we called feedback-modulated TD-STDP, to allow better credit assignment. We show that\nthe new feedback-modulated TD-STDP learning rule can be used to solve common reinforcement\nlearning tasks such as CartPole and LunarLander at a speed similar to standard reinforcement learning\nalgorithms. At the same time, TD-STDP and R-STDP are unable to learn the task without feedback\nmodulation.\nIt is important to point out that a recent work, developed independently from us, proposed a learning\nrule similar to ours, called e-prop [23]. It is based on back-propagation through time (BPTT) and the\npaper showed that it can solve some Atari games efﬁciently. Their proposed learning rule for solving\nreinforcement learning tasks with LIF neurons have several differences from our approach: (i) we\nuse LTD in STDP while they do not use such; (ii) they use different approach in the computation of\npre-synaptic traces; (iii) we use regularization which is needed for more complex tasks. Our paper\nwas submitted to a peer-reviewed conference before [23] was published and we were unaware of this\nstudy before submission of our paper. Apart from [23], we are not aware of any prior works that uses\nSNN with STDP-based learning rule to solve common reinforcement learning tasks with discrete\naction set at a speed similar to standard reinforcement learning algorithms.\nThe results introduced in this paper open the prospect of broader application of SNNs in combination\nwith reinforcement learning to solve machine learning problems efﬁciently.\n2\nBackground\n2.1\nMarkov Decision Process\nWe will consider a continuous-time Markov Decision Process (MDP); for detailed deﬁnition, see\n[24]. A continuous-time MDP is a tuple given by (S, A, q(j|i, a), r(i, a)), where:\n2\nFigure 1: Architecture of the proposed actor-critic model. The environment state is converted to\nspikes at the input neurons. The spikes are passed from input neurons to both critic and actor neurons.\nFiring rate of actor neurons will determine the ﬁring probability of action neurons, which encode the\naction directed to the environment.\nS is the set of possible states of the environment. The state at time t is denoted by St which is in S;\nA is the set of possible actions. The action at time t is denoted by At which is in A. We only consider\ndiscrete action set here;\nq(s′|s, a) is the transition rate function which describes dynamics of state;\nR(s, a) is the reward function, deﬁned to be E[Rt|St = s, At = a], where Rt is the reward at time t\nwhich is in R.\nWe are interested in ﬁnding a policy function π : (S, A) →R such that if action is sampled\nfrom policy function, that is, if At|St ∼π(St, ·), then the expected return E[Gt|π] is maximized,\nwhere return is deﬁned to be Gt :=\nR ∞\ns=t exp\n\u0010\n−(s−t)\nτ\n\u0011\nRsds. τ is the time constant for discount\nand exp\n\u0000 −t\nτ\n\u0001\nis the analog of discount factor γt in discrete case. Value function is deﬁned to be\nV π(s) := E[Gt|St = s, π].\n2.2\nSpiking Neural Network (SNN)\nFor the SNNs employed here, all neurons use standard leaky-integrate-and-ﬁre (LIF) model [5] with\nno refractory period. The membrane voltage Vj of neuron j is computed as:\nτ dVj(t)\ndt\n= Eres −Vj(t) + R\nX\ni\nwijXi(t)\n(1)\nwhere τ is the time constant of the neuron, Eres is the resting potential, R is the resistance, Xi(t) is\nthe spike train from presynaptic neuron i and wij is the synaptic weight from neuron i to neuron j.\nNeuron j ﬁres if Vj(t) > θ where θ is the threshold for ﬁring. Immediately after ﬁring, Vj(t) is reset\nto Eres. We use Eres = −65mV, θ = −52mV, τ = 100ms and R = 1Ωin our model.\n3\nModel\nThe proposed model architecture is shown on Fig 1, which is based on actor-critic architecture. We\nwill discuss each group of neurons in the following sections.\n3\n3.1\nCritic Neuron\nThe role of critic neurons is to estimate the value function of the state, V (St). The architecture of\nthe critic neurons largely follows that of [10] except a slight change in computation of TD error in\ndiscrete step, which is explained in the Appendix.\nTo learn value function, we can minimize TD error of the current value estimation. Denote ˆV (s) as\nthe estimated value function, then continuous version of TD error is given by (same as [10]; here\nˆV ′(St) denotes the derivative of ˆV (St) w.r.t. time t):\nδ(t) = ˆV ′(St) −1\nτ\nˆV (St) + R(t)\n(2)\nEstimation of value is based on ﬁring rate of critic neurons. We ﬁrst compute ﬁring rate of a single\nneuron using temporal average, given by:\nρj(t) = 1\nτn\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013\nXj(s)ds\n(3)\nwhere ρj(t) is the ﬁring rate of neuron j, Xj(s) is the spike train of neuron j and τn is the time\nconstant for the temporal ﬁlter.\nThen, assuming there are Ne excitatory critic neurons and Ni inhibitory critic neurons, the estimated\nˆV (St) is given by a linear function of average ﬁring rate of excitatory critic neurons subtracting the\naverage ﬁring rate of inhibitory critic neurons:\nˆV (St) = α\n\n1\nNe\nNe\nX\nj=1\nρc\ne,j(t) −1\nNi\nNi\nX\nj=1\nρc\ni,j(t)\n\n+ β\n(4)\nwhere ρc\ne,j(t) is the ﬁring rate of excitatory critic neurons j, ρc\ni,j(t) is the ﬁring rate of inhibitory\ncritic neurons j, α and β are scalar constant for linear transformation.\n3.2\nLearning Rule for Critic Neuron\nWhen the TD error is positive, it is expected that the excitatory critic neurons ﬁre more rapidly to\nreduce the TD error, and vice versa. Thus, we can treat TD error as reward for the excitatory critic\nneurons. Similarly, we can use negative of TD error as reward for the inhibitory critic neurons.\nTherefore, we can use R-STDP with eligibility trace1, which is shown theoretically that can maximize\nreward [9], to train critic neurons. In this case, however, the reward is replaced by TD error. The\nlearning rule of weight from input neuron i to critic neuron j is then:\ndwij(t)\ndt\n= ±ηδ(t)zij(t)\n(5)\ndzij(t)\ndt\n= −zij(t)\nτz\n+ A+Pi(t)Xj(t) −A−Pj(t)Xi(t)\n(6)\ndPi(t)\ndt\n= −Pi(t)\nτp\n+ Xi(t)\n(7)\ndPj(t)\ndt\n= −Pj(t)\nτp\n+ Xj(t)\n(8)\nwhere η is the learning rate, zij is the eligibility trace, Pi(t) and Pj(t) are the trace of presynaptic\nspike train and postsynaptic spike train respectively, A+ and A−are constants determining the\nstrength of LTP and LTD of STDP respectively, τp and τz are time constant, and δ(t) is the TD error\ncomputed using (2).\nIn (5), excitatory critic neurons are updated with positive sign while inhibitory critic neurons are\nupdated with negative sign. It can be shown that the above learning rule approximately follows a\ngradient descent on the squared TD error, which is the critic’s learning rule in actor-critic model [25].\nThis is stated in the follow:\n1R-STDP with eligibility trace discussed here is equivalent to MSTDPET from [9]\n4\nTheorem 1. Assume that A+ = 1, A−= 0, τp = τ and τz = τn. Then the learning rules given by\n(5) to (8) are approximated as follows:\ndwij(t)\ndt\n≈ˆηδt∇wij ˆV (St),\n(9)\nwhere ˆη is a constant, and the approximation is given by the straight-through estimator (STE) [26].\nThe detailed proof can be found in Appendix.\nComments:\n1. The approximation is due to use of straight-through estimator (STE) [26], since the step\nfunction in determining ﬁring of neurons has zero derivative almost everywhere.\n2. The update rule in (5) can be further combined with Adam optimizer [27], by treating the\nupdate as gradient input to Adam optimizer. We found that this stabilizes the learning\nprocess as the use of momentum smooths out temporal noise.\n3.3\nActor and Action Neuron\nAs a novel aspect of this approach, we treat the output of the actor neurons as the probability of\nchoosing an action instead of the action itself. To be speciﬁc, let assume that the action set has K\ndiscrete actions, and we have Ne excitatory actor neurons and Ni inhibitory actor neurons per discrete\naction, making a total of N = K(Ne + Ni) actor neurons. We compute the ﬁring rate of each actor\nneuron using the same formula in (3), and the average ﬁring rate of excitatory actor neurons and\ninhibitory actor neurons for action k, denoted by ρa\ne,k(t) and ρa\ni,k(t) respectively, is given by:\nρa\ne,k(t) = 1\nNe\nNe\nX\nj=1\nρa\ne,k,j(t)\nfor k ∈1, 2, ...K\n(10)\nρa\ni,k(t) = 1\nNi\nNi\nX\nj=1\nρa\ni,k,j(t)\nfor k ∈1, 2, ...K\n(11)\nwhere ρa\ne,k,j(t) and ρa\ni,k,j(t) is the ﬁring rate of excitatory actor neuron j and inhibitory actor neuron\nj for action k respectively. Then the ﬁnal action at time t, A(t), is chosen according to a softmax\nfunction of the average ﬁring rate of actor neurons:\ns(t) : = softmax(α(ρa\ne(t) −ρa\ni (t)))\n(12)\nP(A(t) = k) = sk(t)\n(13)\nwhere ρa\ne(t) = [ρa\ne,1(t), ρa\ne,2(t), ..., ρa\ne,K(t)]T , ρa\ni (t) = [ρa\ni,1(t), ρa\ni,2(t), ..., ρa\ni,K(t)]T , s(t) is the\nvector of action probability, and α is the temperature constant to control the degree of exploration. As\nα becomes larger, the model will degenerate to the deterministic case where there is no exploration.\nAs α approaches 0, the model will just degenerate to a uniform policy. This method thus allows\neffective coordination of exploration across a large number of action units. To prevent rapid oscillation\nof A(t), we can re-sample this every m time step instead of every time step.\nWe can also think of A(t) having a corresponding one-hot representation in a layer on top of actor\nneurons and we call it action neurons, which corresponds to the ﬁnal action chosen according to (13).\nThus, there will be a total of K actor neurons, one per action, whose ﬁring corresponds to the action\nbeing executed at the moment. That is, the spike train of action neurons are Ak(t) = I{A(t) = k}.\n3.4\nLearning Rule for Actor Neuron\nIn theory, the critic learning rule above can be directly applied to actor neuron and it still maximizes\nthe return as shown by [10]. However, this is very inefﬁcient in credit assignment if the action chosen\nis a stochastic function of ﬁring rate of actor neurons. Consider the case of only two actions, where\nthe two groups of actor neurons are ﬁring at the same rate, and one action is then randomly chosen. A\npositive TD error is received after the action. If we continue to use the critic learning rule, then both\ngroups will be rewarded, even though only the chosen action is causing the positive TD error. A more\n5\nefﬁcient credit assignment method is to assign credit only to the actor neurons causing the chosen\naction, that is, the learning rule has to be regulated by A(t), as in the three-factor learning rule in\nactor-critic model [25].\nIn light of this idea, we propose a new learning rule that is modulated by A(t), that is, there is a\nfeedback connection from action neurons that modulate the learning. Therefore, we call this learning\nrule feedback-modulated TD-STDP. The proposed learning rule for weight from input neuron i to\nactor neuron j (denote k as the action for this actor neuron) is as follows:\ndwij(t)\ndt\n= ±ηδ(t)qij(t)\n(14)\ndqij(t)\ndt\n= −qij(t)\nτq\n+ (Ak(t) −sk(t)) zij(t)\n(15)\ndzij(t)\ndt\n= −zij(t)\nτz\n+ A+Pi(t)Xj(t) −A−Pj(t)Xi(t)\n(16)\ndPi(t)\ndt\n= −Pi(t)\nτp\n+ Xi(t)\n(17)\ndPj(t)\ndt\n= −Pj(t)\nτp\n+ Xj(t)\n(18)\nwhere Ak(t) is the spike train for action neuron, qij is a feedback-gated trace and τq is the time\nconstant for the trace. Other notations are the same as the notations in learning rules of critic neurons.\nIn (14), excitatory actor neurons are updated with positive sign while inhibitory actor neurons are\nupdated with negative sign.\nAs compared it with the critic neuron learning rule, the only difference in the actor neurons is\nthe addition of (15), which gates the eligibility trace by feedback signal Ak(t) −sk(t). With this\nfeedback gate, if TD error is positive, only the actor neurons corresponding to the chosen action will\nbe rewarded while all other actor neurons will be punished (the case for negative TD error is similar).\nOne may also wonder why sk(t) is in the learning rule. This actually corresponds to derivative of log\nof chosen action’s probability:\n∇ρa\ne,k(t) log P(A(t)) = α(Ak(t) −sk(t))\n(19)\nIf we omitted sk(t), the agent is still learning in our experiment but the result is worse. The weight\nwill easily explode to a very large magnitude.\nIt can be shown that the above learning rule approximately equals to the actor’s learning rule in\nactor-critic model with eligibility trace [25]:\nTheorem 2. Assume A+ = 1, A−= 0, τp = τ, τz = τn and τq =\n1\n1−γλ. Then the learning rules\ngiven by (14) to (18) for excitatory actor neurons are approximated as follows:\ndˆqij(t)\ndt\n≈−(1 −γλ)ˆqij(t) + ∇wij log P(A(t)|S(t))\n(20)\ndwij(t)\ndt\n= ˆηδ(t)ˆqij(t)\n(21)\nwhere ˆη is a constant, and the approximation is given by the straight-through estimator (STE) [26].\nAgain, the approximation is due to use of straight-through estimator (STE) [26]. For inhibitory actor\nneurons, a negative sign has to be placed on right-hand-side of (20) and (21). The detailed proof can\nbe found in Appendix.\n3.5\nRegularization in SNN\n3.5.1\nEntropy Regularization\nSimilar to ANN, entropy regularization can be employed to encourage exploration by preventing\nsaturation of actions’ probability [28]. In our model, gradient of entropy w.r.t weight from input\n6\nneuron i to excitatory actor neuron j (denote k as the action for this actor neuron) can be computed\nas:\n−\n∂\n∂wij\nK\nX\nm=1\nP(A(t) = m) log P(A(t) = m)\n(22)\n= −\nK\nX\nm=1\nsm(t)(log sm(t) + 1)(I{m = k} −sk(t))\n∂\n∂wij\nα(ρa\ne,k(t) −ρa\ni,k(t)))\n(23)\n= −α\nK\nX\nm=1\nsm(t)(log sm(t) + 1)(I{m = k} −sk(t))\n∂\n∂wij\nρa\ne,k(t)\n(24)\n≈Rα\nNeτnτ (gk · ((Xj · (Xi ∗kτ)) ∗kτn))(t)\n(25)\nwhere gk(t) = −PK\nm=1 sm(t)(log sm(t) + 1)(I{m = k} −sk(t)) and (25) uses similar step in\nAppendix A.1. Absorbing\nRα\nNeτnτ into constant ce and assuming A−= 0, the learning rule in (14) is\nmodiﬁed to2:\ndwij(t)\ndt\n= η(±δ(t)qij(t) ± cegk(t)zij(t))\n(26)\nIn (26), excitatory neurons are updated with positive sign while inhibitory neurons are updated with\nnegative sign.\nIn our experiment, we found that entropy regularization is necessary to solve LunarLander. Without\nentropy regularization, action 0 (doing nothing) will have almost zero probability after the ﬁrst few\nhundreds of episodes, making the agent stuck in local optima policy.\n3.5.2\nWeight Decay\nWeight decay, or L2 weight regularization, is also beneﬁcial in training SNN. Since we do not employ\nany restrictions on weight’s norm in our model, we observe that some actor neurons are ﬁring on\nalmost every step. But the absolute level of ﬁring rate of actor neurons is unimportant. Only the\nrelative level of ﬁring rate of actor neurons determines the probability of choosing an action. By\nﬁring on every step, the above learning rule can no longer further increase the ﬁring rate of a neuron.\nTherefore, a form of regularization is required to keep the absolute level of ﬁring rate of actor neurons\nlow such that the above learning rules are effective in controlling ﬁring rate. We found that the use of\nweight decay can achieve such regularization. Denote cw as strength of weight decay, the learning\nrule in (14) is modiﬁed to:\ndwij(t)\ndt\n= η(±δ(t)qij(t) −1\n2cwwij(t))\n(27)\n3.5.3\nTarget Firing Rate\nAnother mechanism to control ﬁring rate of actor neurons is to do gradient descent on the squared\ndifference between average ﬁring rate of actor neurons ρa\ne(t) = 1\nK ρa\ne,k(t) and a target ﬁring rate ˆρa\ne.\nIn our model, gradient of (ρa\ne(t) −ˆρa\ne)2 w.r.t weight from input neuron i to excitatory actor neuron j\n(denote k as the action for this actor neuron) can be computed as:\n2For the case of A−> 0, one can use separate eligibility trace for LTP and LTD in (16), and use the eligibility\ntrace for LTP to compute (Xj · (Xi ∗kτ))(t).\n7\n−\n∂\n∂wij\n(ρa\ne(t) −ˆρa\ne)2\n(28)\n= −2\nK (ρa\ne(t) −ˆρa\ne)\n∂\n∂wij\nρa\ne,k\n(29)\n≈−2\nK (ρa\ne(t) −ˆρa\ne) · (Xj · (Xi ∗kτ)) ∗kτn)(t)\n(30)\nAgain, (30) uses similar step in Appendix A.1. Absorbing 2\nK into constant ct and assuming A−= 0,\nthe learning rule in (14) is modiﬁed to:\ndwij(t)\ndt\n= η(±δ(t)qij(t) −ct(ρa\ne(t) −ˆρa\ne)zij(t))\n(31)\nThe three methods of regularization proposed above can be combined as:\ndwij(t)\ndt\n= η(±δ(t)qij(t) ± cegk(t)zij(t) −1\n2cwwij(t) −ct(ρa\ne(t) −ˆρa\ne)zij(t))\n(32)\n4\nExperimental Results\nTo test our proposed learning rule, we apply it to the CartPole problem (also called pole balancing\nand inverted pendulum) and LunarLander. Next, we will discuss the experimental details of the\nimplementations. We used BindsNET [29] to simulate SNN in our experiment, with each step in\nSNN representing 1ms.\nFor CartPole, we ﬁrst perform Fourier transformation on input with a Fourier order of 2 [30], thus the\noutput dimension is 81 = (2 + 1)4, with each output oi in the range of [−1, 1]. Then we rescale the\noutput to [0, 1] by fi = oi+1\n2 , which is the feature we used in CartPole.\nFor LunarLander, we ﬁrst perform Fourier transformation on input with a Fourier order of 1 and\ndo not include any cross terms, thus the output dimension is 8, with each output oi in the range of\n[−1, 1]. Then we concatenate negative of these 8 outputs and a bias (a constant one) in the output\nvector, and ﬁnally apply ReLu on it to obtain the feature vector. The dimension of feature is thus 17,\nwith each fi in the range of [0, 1].\nThe ﬁring rate per ms of input neurons is then given by the value of corresponding feature. Since\nwe use a time step of 1ms in our stimulation of SNN, the ﬁring rate is converted to spike by\nXi(t) ∼Ber(fi(t)) as input to the network. There are 1 input neuron per feature in CartPole and 16\ninput neurons per feature in LunarLander. We use the same features for baseline models, without\nconversion to spike.\nWe added a warm-up period of 100ms before the start of each episode, where the initial state is\npresented to the agent without change for this period. No action has effects on state and no reward is\ngiven in this period. There is also no learning during the warm-up period. The reason for adding this\nperiod is to allow the ﬁring rate of the agent to pick up from zero initially.\nThe hyperparameters used in CartPole and Lunarlander experiment are shown on Table 1. We selected\nthese hyperparameters values based on our educated guess followed by manual tuning to optimize\nperformance.\nFor CartPole, we scaled all rewards by 0.02, so the reward for each 1ms before the end of episode is\n0.001. Time constant for discount rate is 1000ms. We have not used batch update, inhibitory neurons,\nAdam optimizer, and any regularization. We re-sampled new actions at every environment step.\nFor LunarLandar, we scaled all rewards by 0.012 and time constant for discount rate is 2000ms. We\nused Adam optimizer with β1 = 0.995 and β2 = 0.99995, and a batch size of 16. We re-sampled\nnew action at every two environment steps.\nSince one step in the environment represents multiple steps in SNN, we distribute the reward from\nthe environment evenly to the corresponding SNN’s time steps. We also set the target ˆV (St+∆t) to 0\nin the last 2ms of an episode when computing TD error.\n8\nTable 1: Hyperparameters used in CartPole and LunarLander.\nCartPole\nLunarLander\nCritic Network\nActor Network\nCritic Network\nActor Network\nNe\n40\n20\n128\n32\nNi\n0\n0\n128\n32\nη\n2.5e-3\n1e-2\n1.25e-4\n6.25e-5\nα\n2\n25\n4\n15\nβ\n-0.2\nn.a.\n-2\nn.a.\nτn, τp, τz\n20\n20\n20\n20\nτq\nn.a.\n40\nn.a.\n20\nce\nn.a.\n0\nn.a.\n1e-4\ncw\nn.a.\n0\nn.a.\n1e-8\nct\nn.a.\n0\nn.a.\n0\nA+\n1\n1\n1\n1\nA−\n0\n0\n0\n0\nTable 2: Episodes required for learning the task.\nCartPole\nLunarLander\nTf\nTs\nTf\nTs\nMean\nStd.\nMean\nStd.\nMean\nStd.\nMean\nStd.\nProposed\n57.00\n23.97\n169.50\n23.47\n383.80\n71.49\n2575.20\n666.51\nBaseline\n180.70\n23.85\n301.90\n28.97\n503.30\n76.03\n1295.20\n165.68\nModel in [13]\n1023.80\n77.65\nn.a.\nn.a.\n4.1\nCartPole\nFor CartPole, we used CartPole-v1 in OpenAI’s Gym [31] for our implementation of CartPole.\nEpisode ends if it lasts more than 500 steps (equivalent to 10s). The experimental results are shown\nin Fig 2, which displays the episode return achieved by the agent for 400 episodes, averaged over 10\nindependent runs. Let Tf denote the ﬁrst episode when the agent achieves a perfect score (maintaining\nthe pole for 10s). The average value over 10 runs is ¯Tf = 57.00, given in Table 2. In addition, let ¯Ts\ndenote the average number of episodes required for solving the task (deﬁned as maintaining the pole\nfor 10s over all of the last 100 episodes); ¯Ts = 169.50.\n(a) CartPole\n(b) LunarLander\nFigure 2: Learning curve in proposed model and baseline model. Results are averaged over 10 runs,\nand shaded area represents standard deviation over the runs.\n9\nFigure 3: Illustration of the model after learning. First graph: Firing rate of both actor and critic\nneurons for a sample interval in an episode. Second and third graph: spike trains of critic and actor\nneurons. Actor neuron 1 to 40 is for left action and actor neuron 41 to 80 is for right action. The\nanimation of this graph can be viewed online.\nNext, we consider a comparison with other models. For baseline, we used a standard actor-critic\nmodel (one-step actor-critic (episodic) from [25]) with a similar architecture: both critic network\nand actor network are a one-hidden layer neural network with hidden layer’s size of 40 using ReLu\nactivation. This can be seen as the counterpart of our SNN model in ANN. Although this is not a\nstate-of-the-art model, our goal is to show that SNN can be used to solve some common reinforcement\nlearning tasks at a speed similar (or better) to their ANN counterpart. The results are shown in Table\n2. It is seen that the proposed model is signiﬁcantly better than the baseline considered. The learning\nspeed obtained by feedback-modulated TD-STDP is on par with most standard reinforcement learning\nalgorithms.\nIn addition, we have tested the model proposed in [13]3. Since their model does not converge for 400\nepisodes, we run the model for 2000 episodes instead. We noted that the model cannot solve the task\n(according to the deﬁnition above) in all 10 runs and ¯Tf, the average ﬁrst episode agent achieves a\nperfect score, is 1023.80, signiﬁcantly larger than both the baseline and our proposed model.\nTo illustrate the strength of the trained network, we have tested its performance over longer time\nintervals, i.e., not terminating the task after 10s, which was used during training. Although the agent\nis trained for at most 10s in any single episode, it is able to continue balancing the pole stably for a\nvery long time beyond 10s. In fact, we have to terminate its operation manually after it shows no\nsigns of failing for more than 5 minutes. The video demonstrating this performance can be found on\nhttps://youtu.be/Ti5CznX4H9I.\nAs most hyperparameters are tuned to optimize performance instead of ﬁtting biologically observed\nvalue, we ﬁnd that the most neurons in above model are ﬁring at a high frequency of above 200Hz,\nwhich is biologically implausible. However, this issue could be solved by tuning the relevant\nhyperparameters and using the regularization method discussed in Section 3.5. For critic neurons, α\ncould be set to a high value such that a low ﬁring rate indicates a high value. For actor neurons, we can\nemploy target ﬁring rate discussed in Section 3.5.3. To be specifc, we used the same hyperparameters\nexcept for critic network: α = 20, β = −1, Ne = 80, and for actor network: Ne = 40, ct = 5e−6,\n¯ρa = 5e−3. After these changes, average ﬁring rate of actor neurons and critic neurons are around\n50Hz and 70Hz respectively, which are in the gamma frequency band, and ¯Tf and ¯Ts are 132.90 and\n251.00 respectively, worse than the original model but still better than the baseline. At this ﬁring\nrate, the dynamic of spiking neuron and rate-based neuron are also very different. The spike trains\n3In their paper, they used CartPole-v0 instead of CartPole-v1, which only differs in maximal length of\nepisode. Maximal episode length of CartPole-v1 is 500 steps (10s) while that of CartPole-v0 is 200 steps (4s).\nWe run their code published online on CartPole-v1 instead of CartPole-v0 in our experiment, using a population\nsize of 10.\n10\nobtained by model simulations are illustrated in Fig 3, and the animation of this could be found on\nhttps://youtu.be/X7I6FpF3MJQ.\nWe have tried to train the actor neuron with TD-STDP only without any feedback modulation. The\nagent fails to learn and episode length stays below 1s, showing that feedback modulation is necessary\nfor the agent to learn.\n4.2\nLunarLander\nWe also applied the proposed model to LunarLander-v2 in OpenAI’s Gym [31], a task which is much\nmore complex than CartPole. There are four available actions in the task and the goal is to land the\nlander to the landing pad, which will yield 200 scores or above. The implementation of the model is\nlargely the same as the one we used in CartPole, except: (i) we used 128 excitatory neurons and 128\ninhibitory neurons in both critic network and actor network, (ii) we used Adam optimizer in updating\nweight, (iii) we added entropy regularization in learning, and (iv) we used a batch size of 16.\nWe deﬁne perfect score as having a return of 200 or above, and solving the task as having an average\nreturn of 200 or above for the last 100 episodes. For each of the 10 independent runs, we run the\nmodel for 3000 or Ts episodes, whichever is greater. The baseline model is similar to the one we\nused in CartPole, but with 128 hidden units for both critic and actor network.\nThe experimental result is shown in Fig 2, which displays the episode return achieved by the agent\nfor 3000 episodes, averaged over 10 independent runs. ¯Tf, the average value of the ﬁrst episode\nthat agent achieves a perfect score, is 383.80, and ¯Ts, the average value of the episodes required for\nsolving the task, is 2575.20. A comparison with the baseline model is shown on Table 2. Though\nthe proposed model is able to solve the task, the learning is slower than that of the baseline model.\nThe performance of the feedback-modulated TD-STDP learning can be improved by optimizing its\nhyperparameters. Such optimization, however, has not been conducted extensively in this study. The\npresent work aims at demonstrating the feasibility of the proposed approach for a more complex\ncontrol problem.\nWe have made a video showing the trained model solving LunarLander, which could be found on\nhttps://youtu.be/UzysCKolMFg. All 10 episodes shown in the video achieve a score of over 200\n(as long as the center of the lander is within the two ﬂags, it is counted as successful landing).\n5\nConclusions\nThe goal of this paper is to to use SNNs to solve reinforcement learning tasks with discrete action set\nefﬁciently. In our approach, the actor-critic architecture treats actor neurons’ output as the probability\nof choosing an action instead of the action itself. We derive a novel learning rule for SNN with\nactor-critic architecture. Since the action chosen is no longer a deterministic function of actor neurons’\noutput, we propose a new learning rule, which we called feedback-modulated TD-STDP, to allow\nbetter credit assignment.\nWe show that our learning approach can be used to solve CartPole and LunarLander at a speed\ncomparable to standard reinforcement learning algorithms. The possibility of using SNN with\nbiologically inspired learning rules to solve reinforcement learning tasks efﬁciently can provide hints\non how learning may happen in biological neural systems. For example, we show that the feedback\nmodulation in our proposed learning rule is necessary for the agent to learn, which may point to\none of the possible roles of feedback connection in the biological neural system is structural credit\nassignment.\nA learning rule similar to ours has been developed recently, called e-prop [23]. It is based on back-\npropagation through time (BPTT) and it was shown that the learning rule can solve some Atari games\nefﬁciently. E-prop and our approach has several differences: (i) We include LTD side of STDP in\nour learning rule; (ii) In computation of pre-synaptic traces, they used Xj = max(0, 1 −| Vj(t)−θ\nθ\n|)\nwhile we use Xj = 1{Vj(t) ≥θ}; (iii) we propose methods for regularization that is necessary to\nsolve more complex tasks.\nBesides more sophisticated methods to encourage exploration, possible future work can include\nlearning hidden representations by the model, which can be applied to pixel-based games such as\nAtari. [32, 33] has shown that STDP can be used to train SNN to extract useful features for image\n11\nclassiﬁcation. These methods can be used to train hidden layers for both actor and critic networks in\nour proposed model, and point to the possibility of learning a multi-layer SNN to solve a complex\nreinforcement learning task without back-propagation.\nComparison between SNN and ANN in learning reinforcement learning tasks also deserves further\ninvestigation. Since neurons in biological neural systems communicate by spikes, SNNs are generally\nconsidered to be closer to the biological neural system than ANN. Are there any advantages of SNN\nover ANN in learning, besides low communication cost? Does SNN allow faster learning or more\nrobust policy learnt? We hope that this work can encourage more attention paid to SNN and can lead\nto further advance in using SNN to solve more complex reinforcement learning tasks.\n6\nAcknowledgment\nWe thank Andy Barto who motivated this research and provided valuable insight and comments.\nA\nTheoretical Derivation of TD-STDP and Feedback-modulated TD-STDP\nIn this section, we will derive the formulas for LTP side of both TD-STDP for critic actors and\nfeedback-modulated TD-STDP based on learning rule from actor-critic model in reinforcement\nlearning and straight-through estimator (STE) [26].\nWe denote ∗as symbol for convolution:\n(f ∗g)(t) :=\nZ ∞\n−∞\nf(s)g(t −s)ds\n(33)\nand deﬁne kernel kτ(t) := exp(−t\nτ ) for t ≥0 and 0 else. We also denote ˆδ(t) as the Dirac delta\nfunction. Then ﬁring rate of a single neuron in (3) can be written as:\nρj(t) = 1\nτn\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013\nXj(s)ds = 1\nτn\n(Xj ∗kτn)(t)\n(34)\nA.1\nProof of Theorem 1\nProof. In actor-critic model, TD error of critic network can be minimized by the learning rule [25]:\nw ←w + ηδt∇w ˆV (St)\n(35)\nwhere η is the learning rate. In continuous time step, it can be expressed as:\ndw(t)\ndt\n= ηδt∇w ˆV (St)\n(36)\nUsing (4) and (34), ∂ˆV (St)\nwij\nin our model can be computed as (assuming the weight is connected to\nexcitatory critic neuron j):\n∂ˆV (St)\n∂wij\n=\n∂\n∂wij\nα\n\n1\nNe\nNe\nX\nj=1\nρc\ne,j(t) −1\nNi\nNi\nX\nj=1\nρc\ni,j(t)\n\n+ β\n(37)\n=\n∂\n∂wij\nα\nNe\nρc\ne,j(t)\n(38)\n=\n∂\n∂wij\nα\nNeτn\n(Xj ∗kτn)(t)\n(39)\n=\nα\nNeτn\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013 ∂Xj(s)\n∂wij\nds\n(40)\n=\nα\nNeτn\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013\nˆδ(0)∂I{Vj(s) > θ}\n∂wij\nds\n(41)\n12\nWe note that ∂Xj(s)\n∂wij\nis zero almost everywhere. To go around the problem, we used straight-through\nestimator (STE) [26], which was ﬁrstly proposed by Hinton in his lecture [34]. The idea is to replace\nthe derivative of threshold function with a related surrogate. Theoretical justiﬁcations of STE and\ndiscussion on choices of surrogate can be found in [35]. Here we used the ReLu function as our\nchoice of surrogate. That is, we used ∂max(Vj(s)−θ,0)\n∂wij\nto replace ∂I{Vj(s)>θ}\n∂wij\n.\nAlso, we note that LIF model given by (1), combined with the resetting dynamic after ﬁring, can be\nwritten as:\nVj(t) = Eres + R\nτ\nZ t\n−∞\nexp\n\u0012\n−t −s\nτ\n\u0013 X\ni\nwijXi(s)ds\n−(θ −Eres)\nZ t\n−∞\nexp\n\u0012\n−t −s\nτ\n\u0013 X\nf\nˆδ(s −t(f)\nj\n)ds\n(42)\nwhere {t(f)\nj\n} = {t|Vj(t) > θ}, the set of ﬁring times of neuron j. Ignoring the effects of wij on t(f)\nj\n,\nwe have,\n∂Vj(s)\n∂wij\n≈R\nτ\nZ s\n−∞\nexp\n\u0012\n−s −q\nτ\n\u0013\nXi(q)dq\n(43)\n= R\nτ (Xi ∗kτ)(s)\n(44)\nThus combining STE and (44), we obtain:\n∂ˆV (St)\n∂wij\n=\nα\nNeτn\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013\nˆδ(0)∂I{Vj(s) > θ}\n∂wij\nds\n(45)\n≈\nα\nNeτn\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013\nˆδ(0)∂max(Vj(s) −θ, 0)\n∂wij\nds\n(46)\n=\nα\nNeτn\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013\nXj(s)∂Vj(s)\n∂wij\nds\n(47)\n≈\nRα\nNeτnτ\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013\nXj(s)(Xi ∗kτ)(s)ds\n(48)\n=\nRα\nNeτnτ ((Xj · (Xi ∗kτ)) ∗kτn)(t)\n(49)\nSubstituting back into (36) and absorbing\nRα\nNeτnτ into learning rate η, the learning rule is therefore\ngiven by:\ndwij(t)\ndt\n= ηδ(t)((Xj · (Xi ∗kτ)) ∗kτn)(t)\n(50)\nUsing the fact that the differential equation dx(t)\ndt\n= −x(t)\nτ\n+ m(t) has solution x(t) = (m ∗kτ)(t),\nthe learning rule for critic neurons given by (5) to (8) is equivalent to that in (50) with A+ = 1,\nA−= 0, τp = τ and τz = τn. The derivation for learning rule of inhibitory critic neurons is similar\nbut with an opposite sign.\nA.2\nProof of Theorem 2\nProof. In actor critic model with eligibility trace, actor network can be trained by the following\nlearning rule [25]:\nq ←γλq + ∇w log P(At|St)\n(51)\nw ←w + ηδtq\n(52)\n13\nwhere η is the learning rate, γ is the discount rate and λ is the trace discount rate. In continuous time\nstep, it can be expressed as:\ndq(t)\ndt\n= −(1 −γλ)q(t) + ∇w log P(A(t)|S(t))\n(53)\ndw(t)\ndt\n= ηδ(t)q(t)\n(54)\nDenote h(t) = ∇w log P(A(t)|S(t)), then q(t) can be expressed as (h ∗kτh)(t) where τh =\n1\n1−γλ.\nWe can compute ∂log P (A(t)|S(t))\n∂wij\nas follows, using formula (10) to (13) (assuming the weight is\nconnected to excitatory actor neuron j for action k):\n∂log P(A(t)|S(t))\n∂wij\n=\n∂\n∂wij\nlog[softmax(α(ρa\ne(t) −ρa\ni (t)))]A(t)\n(55)\n= α(Ak(t) −sk(t))\n∂\n∂wij\nρa\ne,k(t)\n(56)\n= α(Ak(t) −sk(t))\n∂\n∂wij\n1\nNe\nNe\nX\nj=1\nρa\ne,k,j(t)\n(57)\n= α\nNe\n(Ak(t) −sk(t))\n∂\n∂wij\nρa\ne,k,j(t)\n(58)\n=\nα\nNeτn\n(Ak(t) −sk(t)) ∂\nwij\n(Xj ∗kτn)(t)\n(59)\n=\nα\nNeτn\n(Ak(t) −sk(t))\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013 ∂Xj(s)\n∂wij\nds\n(60)\nAgain, ∂Xj(s)\n∂wij\nis almost zero everywhere and we use the same technique in derivation of TD-STDP\nto go around the problem. That is, we use ∂Xj(s)\n∂wij\n≈R\nτ Xj(s)(Xi ∗kτ)(s). Then, it follows:\n∂log P(A(t)|S(t))\n∂wij\n=\nα\nNeτn\n(Ak(t) −sk(t))\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013 ∂Xj(s)\n∂wij\nds\n(61)\n≈\nRα\nNeτnτ (Ak(t) −sk(t))\nZ t\n−∞\nexp\n\u0012\n−t −s\nτn\n\u0013\nXj(s)(Xi ∗kτ)(s)ds\n(62)\n=\nRα\nNeτnτ (Ak(t) −sk(t))((Xj · (Xi ∗kτ)) ∗kτn)(t)\n(63)\n=\nRα\nNeτnτ ((Ak −sk) · ((Xj · (Xi ∗kτ)) ∗kτn))(t)\n(64)\nSubstituting back into q(t), we thus obtain:\nq(t) = (h ∗kτh)(t)\n(65)\n=\nRα\nNeτnτ (((Ak −sk) · ((Xj · (Xi ∗kτ)) ∗kτn)) ∗kτh)(t)\n(66)\nSubstituting back into (54) and absorbing\nRα\nNeτnτ into learning rate η, the learning rule is therefore\ngiven by:\ndwij(t)\ndt\n= ηδ(t)(((Ak −sk) · ((Xj · (Xi ∗kτ)) ∗kτn)) ∗kτh)(t)\n(67)\nUsing the fact that the differential equation dx(t)\ndt\n= −x(t)\nτ\n+ m(t) has solution x(t) = (m ∗kτ)(t),\nthe learning rule for actor neurons given by (14) to (18) is equivalent to that in (67) with A+ = 1,\nA−= 0, τp = τ, τz = τn, and τq = τh. The derivation for learning rule of inhibitory actor neuron is\nsimilar but with an opposite sign.\n14\nB\nFormulas in Discrete Time Step\nThe computation of TD error in (2) and learning rules in Section 3 are all in differential form. Since\nwe simulate SNN using discrete time steps, we will list the corresponding formulas in discrete time\nstep here for completeness.\nFirstly, we can estimate the TD error from t to t + ∆t by integrating both side of (2). Using the fact\nthat exp(−−∆t\nτ ) ≈1 −∆t\nτ , we have:\nδt:t+∆t ≈exp\n\u0012−∆t\nτ\n\u0013\nˆV (St+∆t) + Rt∆t −ˆV (St)\n(68)\nHowever, this equation assumes Rt does not decay for the period ∆t when used to estimate the value\nof V (St). A better estimate is to adjust for this decay as well, by using ∆t\n2 as duration for discount:\nδt:t+∆t ≈exp\n\u0012−∆t\nτ\n\u0013\nˆV (St+∆t) + exp\n\u0012−∆t\n2τ\n\u0013\nRt∆t −ˆV (St)\n(69)\nThe equation in (69) is intuitive.\nThe discounted return from t to t + ∆t is approxi-\nmated by exp\n\u0000 −∆t\n2τ\n\u0001\nRt while the discounted return from t + ∆t onward is approximated by\nexp( −∆t\nτ ) ˆV (St+∆t). (69) will be used to compute TD error δ in (73) and (78). If the episode\nended already, then ˆV (St+∆t) in (69) is replaced with 0.\nFor learning rule, we denote ˆXi as the indicator function of whether neuron i is ﬁring, equivalent to\nre-scaling the Dirac delta functions in spike train Xi to 1. Then, the learning rule of weight from\ninput neuron i to critic neuron j in discrete time step is as follows (corresponds to (5) to (8)):\nPj ←exp\n\u0012−∆t\nτp\n\u0013\nPj + ˆXj\n(70)\nPi ←exp\n\u0012−∆t\nτp\n\u0013\nPi + ˆXi\n(71)\nzij ←exp\n\u0012−∆t\nτz\n\u0013\nzij + A+Pi ˆXj −A−Pj ˆXi\n(72)\nwij ←wij ± ηδzij\n(73)\nThe learning rule for weight from input neuron i to actor neuron j (denote k as the action for this\nactor neuron) in discrete time step is as follows (corresponds to (14) to (18)):\nPj ←exp\n\u0012−∆t\nτp\n\u0013\nPj + ˆXj\n(74)\nPi ←exp\n\u0012−∆t\nτp\n\u0013\nPi + ˆXi\n(75)\nzij ←exp\n\u0012−∆t\nτz\n\u0013\nzij + A+Pi ˆXj −A−Pj ˆXi\n(76)\nqij ←exp\n\u0012−∆t\nτq\n\u0013\nqij + (Ak −sk)zij\n(77)\nwij ←wij ± ηδqij\n(78)\nIn (73) and (78), excitatory neurons are updated with positive sign while inhibitory neurons are\nupdated with negative sign.\nReferences\n[1] Michael Pfeiffer and Thomas Pfeil. Deep learning with spiking neurons: opportunities and\nchallenges. Frontiers in neuroscience, 12:774, 2018.\n15\n[2] Dongsung Huh and Terrence J Sejnowski. Gradient descent for spiking neural networks. In\nAdvances in Neural Information Processing Systems, pages 1433–1443, 2018.\n[3] Wenrui Zhang and Peng Li. Spike-train level backpropagation for training deep recurrent spiking\nneural networks. In Advances in Neural Information Processing Systems, pages 7800–7811,\n2019.\n[4] Dina Obeid, Hugo Ramambason, and Cengiz Pehlevan. Structured and deep similarity matching\nvia structured and deep hebbian networks. In Advances in Neural Information Processing\nSystems, pages 15377–15386, 2019.\n[5] Peter Dayan and Laurence F Abbott. Theoretical neuroscience: computational and mathematical\nmodeling of neural systems. 2001.\n[6] Demis Hassabis, Dharshan Kumaran, Christopher Summerﬁeld, and Matthew Botvinick.\nNeuroscience-inspired artiﬁcial intelligence. Neuron, 95(2):245–258, 2017.\n[7] Wulfram Gerstner, Werner M Kistler, Richard Naud, and Liam Paninski. Neuronal dynamics:\nFrom single neurons to networks and models of cognition. Cambridge University Press, 2014.\n[8] Taro Toyoizumi, Jean-Pascal Pﬁster, Kazuyuki Aihara, and Wulfram Gerstner. Spike-timing\ndependent plasticity and mutual information maximization for a spiking neuron model. In\nAdvances in neural information processing systems, pages 1409–1416, 2005.\n[9] R˘azvan V Florian. Reinforcement learning through modulation of spike-timing-dependent\nsynaptic plasticity. Neural Computation, 19(6):1468–1502, 2007.\n[10] Nicolas Frémaux, Henning Sprekeler, and Wulfram Gerstner. Reinforcement learning using\na continuous time actor-critic framework with spiking neurons. PLoS computational biology,\n9(4), 2013.\n[11] Eleni Vasilaki, Nicolas Frémaux, Robert Urbanczik, Walter Senn, and Wulfram Gerstner.\nSpike-based reinforcement learning in continuous state and action space: when policy gradient\nmethods fail. PLoS Comput Biol, 5(12):e1000586, 2009.\n[12] Zhenshan Bing, Claus Meschede, Kai Huang, Guang Chen, Florian Rohrbein, Mahmoud Akl,\nand Alois Knoll. End to end learning of spiking neural network based on r-stdp for a lane\nkeeping vehicle. In 2018 IEEE International Conference on Robotics and Automation (ICRA),\npages 1–8. IEEE, 2018.\n[13] Sneha Aenugu, Abhishek Sharma, Sasikiran Yelamarthi, Hananel Hazan, Philip S. Thomas, and\nRobert Kozma. Reinforcement learning with a network of spiking agents, 2019.\n[14] Wiebke Potjans, Abigail Morrison, and Markus Diesmann. A spiking neural network model of\nan actor-critic learning agent. Neural computation, 21(2):301–339, 2009.\n[15] Timo Wunderlich, Akos F Kungl, Eric Müller, Andreas Hartel, Yannik Stradmann, Syed Ahmed\nAamir, Andreas Grübl, Arthur Heimbrecht, Korbinian Schreiber, David Stöckel, et al. Demon-\nstrating advantages of neuromorphic computation: a pilot study. Frontiers in neuroscience,\n13:260, 2019.\n[16] Ian H Witten. An adaptive optimal controller for discrete-time markov environments. Informa-\ntion and control, 34(4):286–295, 1977.\n[17] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements\nthat can solve difﬁcult learning control problems. IEEE transactions on systems, man, and\ncybernetics, (5):834–846, 1983.\n[18] Yuji Takahashi, Geoffrey Schoenbaum, and Yael Niv. Silencing the critics: understanding\nthe effects of cocaine sensitization on dorsolateral and ventral striatum in the context of an\nactor/critic model. Frontiers in neuroscience, 2:14, 2008.\n[19] Paul W Glimcher. Understanding dopamine and reinforcement learning: the dopamine reward\nprediction error hypothesis. Proceedings of the National Academy of Sciences, 108(Supplement\n3):15647–15654, 2011.\n16\n[20] Pieter R Roelfsema and Anthony Holtmaat. Control of synaptic plasticity in deep cortical\nnetworks. Nature Reviews Neuroscience, 19(3):166, 2018.\n[21] Pieter R Roelfsema and Arjen van Ooyen. Attention-gated reinforcement learning of internal\nrepresentations for classiﬁcation. Neural computation, 17(10):2176–2214, 2005.\n[22] Jaldert O Rombouts, Sander M Bohte, and Pieter R Roelfsema. How attention can create\nsynaptic tags for the learning of working memories in sequential tasks. PLoS Comput Biol,\n11(3):e1004060, 2015.\n[23] Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Leg-\nenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of\nspiking neurons. Nature communications, 11:3625, 2020.\n[24] Xianping Guo and Onésimo Hernández-Lerma. Continuous-time markov decision processes.\nIn Continuous-Time Markov Decision Processes, pages 9–18. Springer, 2009.\n[25] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[26] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n[27] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[28] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. In International conference on machine learning, pages 1928–1937,\n2016.\n[29] Hananel Hazan, Daniel J. Saunders, Hassaan Khan, Devdhar Patel, Darpan T. Sanghavi, Hava T.\nSiegelmann, and Robert Kozma. Bindsnet: A machine learning-oriented spiking neural networks\nlibrary in python. Frontiers in Neuroinformatics, 12:89, 2018.\n[30] George Konidaris, Sarah Osentoski, and Philip Thomas. Value function approximation in\nreinforcement learning using the fourier basis. In Twenty-ﬁfth AAAI conference on artiﬁcial\nintelligence, 2011.\n[31] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[32] Peter U Diehl and Matthew Cook. Unsupervised learning of digit recognition using spike-\ntiming-dependent plasticity. Frontiers in computational neuroscience, 9:99, 2015.\n[33] Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Simon J Thorpe, and Timothée Masquelier.\nStdp-based spiking deep convolutional neural networks for object recognition. Neural Networks,\n99:56–67, 2018.\n[34] G. Hinton. Neural networks for machine learning. coursera, video lectures. lecture 9c. https://\nwww.youtube.com/watch?v=LN0xtUuJsEI&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_\nSNhz9&index=41, 2012.\n[35] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin.\nUnderstanding straight-through estimator in training activation quantized neural nets. arXiv\npreprint arXiv:1903.05662, 2019.\n17\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML",
    "I.2.8"
  ],
  "published": "2020-08-29",
  "updated": "2020-08-29"
}