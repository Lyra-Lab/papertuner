{
  "id": "http://arxiv.org/abs/2008.13221v1",
  "title": "Human-in-the-Loop Methods for Data-Driven and Reinforcement Learning Systems",
  "authors": [
    "Vinicius G. Goecks"
  ],
  "abstract": "Recent successes combine reinforcement learning algorithms and deep neural\nnetworks, despite reinforcement learning not being widely applied to robotics\nand real world scenarios. This can be attributed to the fact that current\nstate-of-the-art, end-to-end reinforcement learning approaches still require\nthousands or millions of data samples to converge to a satisfactory policy and\nare subject to catastrophic failures during training. Conversely, in real world\nscenarios and after just a few data samples, humans are able to either provide\ndemonstrations of the task, intervene to prevent catastrophic actions, or\nsimply evaluate if the policy is performing correctly. This research\ninvestigates how to integrate these human interaction modalities to the\nreinforcement learning loop, increasing sample efficiency and enabling\nreal-time reinforcement learning in robotics and real world scenarios. This\nnovel theoretical foundation is called Cycle-of-Learning, a reference to how\ndifferent human interaction modalities, namely, task demonstration,\nintervention, and evaluation, are cycled and combined to reinforcement learning\nalgorithms. Results presented in this work show that the reward signal that is\nlearned based upon human interaction accelerates the rate of learning of\nreinforcement learning algorithms and that learning from a combination of human\ndemonstrations and interventions is faster and more sample efficient when\ncompared to traditional supervised learning algorithms. Finally,\nCycle-of-Learning develops an effective transition between policies learned\nusing human demonstrations and interventions to reinforcement learning. The\ntheoretical foundation developed by this research opens new research paths to\nhuman-agent teaming scenarios where autonomous agents are able to learn from\nhuman teammates and adapt to mission performance metrics in real-time and in\nreal world scenarios.",
  "text": "HUMAN-IN-THE-LOOP METHODS FOR DATA-DRIVEN AND REINFORCEMENT\nLEARNING SYSTEMS\nA Dissertation\nby\nVINICIUS GUIMARAES GOECKS\nSubmitted to the Ofﬁce of Graduate and Professional Studies of\nTexas A&M University\nin partial fulﬁllment of the requirements for the degree of\nDOCTOR OF PHILOSOPHY\nChair of Committee,\nJohn Valasek\nCommittee Members,\nGregory Chamitoff\nDaniel Selva\nDylan Shell\nHead of Department,\nRodney D. W. Bowersox\nMay 2020\nMajor Subject: Aerospace Engineering\nCopyright 2020 Vinicius Guimaraes Goecks\narXiv:2008.13221v1  [cs.LG]  30 Aug 2020\nABSTRACT\nRecent successes combine reinforcement learning algorithms and deep neural networks, despite\nreinforcement learning not being widely applied to robotics and real world scenarios. This can be\nattributed to the fact that current state-of-the-art, end-to-end reinforcement learning approaches\nstill requires thousands or millions of data samples to converge to a satisfactory policy and are\nsubject to catastrophic failures during training. Conversely, in real world scenarios and after just\na few data samples, humans are able to either provide demonstrations of the task, intervene to\nprevent catastrophic actions, or simply evaluate if the policy is performing correctly. This research\ninvestigates how to integrate these human interaction modalities to the reinforcement learning\nloop, increasing sample efﬁciency and enabling real-time reinforcement learning in robotics and\nreal world scenarios. The theoretical foundation of this research work builds upon the actor-\ncritic reinforcement learning architecture, the use of function approximation to represent action-\nand value-based functions, and the integration of different human interaction modalities; namely,\ntask demonstration, intervention, and evaluation, to these functions and to reward signals. This\nnovel theoretical foundation is called Cycle-of-Learning, a reference to how different human\ninteraction modalities are cycled and combined to reinforcement learning algorithms. This approach\nis validated on an Unmanned Air System (UAS) collision avoidance and landing scenario using a\nhigh-ﬁdelity simulated environment and several continuous control tasks standardized to benchmark\nreinforcement learning algorithms. Results presented in this work show that the reward signal\nthat is learned based upon human interaction accelerates the rate of learning of reinforcement\nlearning algorithms, when compared to traditional handcrafted or binary reward signals returned\nby the environment. Results also show that learning from a combination of human demonstrations\nand interventions is faster and more sample efﬁcient when compared to traditional supervised\nlearning algorithms. Finally, Cycle-of-Learning develops an effective transition between policies\nlearned using human demonstrations and interventions to reinforcement learning. It learns faster\nand uses fewer interactions with the environment when compared to state-of-the-art algorithms.\nii\nThe theoretical foundation developed by this research opens new research paths to human-agent\nteaming scenarios where autonomous agents are able to learn from human teammates and adapt to\nmission performance metrics in real-time and in real world scenarios.\niii\nDEDICATION\nTo my wife, family, and friends who have always supported me\nthroughout my academic journey.\niv\nACKNOWLEDGMENTS\nFirst of all, I would not be able to complete this doctorate degree without the support of my\nloving wife, Lucieni, and my parents, Elizabeth and Claudio, and brother, Thiago. My wife has been\nalways by my side when facing the challenges of graduate school and also sharing all the happy\nmoments that came with it. My parents, even though in a another country, were always supportive\nof me when pursuing this degree. I would not be able to complete this dissertation without their\nemotional support.\nI would like to acknowledge and thank who contributed to the development of this research\nand the successful completion of this dissertation. Dr. John Valasek, who has been an exceptional\nadvisor and mentor not only on academic matters, but also on leadership and management, which I\nsee as invaluable skills for my future career. My committee members, Dr. Gregory Chamitoff, Dr.\nDylan Shell, and Dr. Daniel Selva for their guidance when grounding the research fundamentals\nand support when deﬁning the research direction. I would like to specially thank Dr. Nicholas\nWaytowich, Dr. Gregory Gremillion, and Dr. Vernon Lawhern, from the U.S. Army Research\nLaboratory, for being an integral part of the development process of this research, for working\ntogether when deﬁning the research direction and research questions to be answered, and for all\nsupport when presenting and publishing the research results. This research would not be successful\nwithout their technical contributions, guidance, and support.\nFinally, I would like to also thank my graduate student peers and personal friends that directly\ncontributed to my formation and supported me during my doctorate journey: Humberto Ramos,\nNiladri Das, Hakjoo Kim, Jack Lu, Bochan Lee, Josh Harris, Robyn Woollands, Austin Probe,\nClark Moody, and more recently, Ritwik Bera and Morgan Wood. Thank you for everything you\ntaught me throughout this graduate school years and for being part of this journey.\nv\nCONTRIBUTORS AND FUNDING SOURCES\nContributors\nThis work was supported by a dissertation committee consisting of Professor John Valasek,\nGregory Chamitoff, and Daniel Selva of the Department of Aerospace Engineering and Professor\nDylan Shell of the Department of Computer Science.\nThe data analyses for chapters 7 through 10 were conducted in collaboration with Dr. Nicholas R.\nWaytowich, Dr. Vernon J. Lawhern, and Dr. Gregory M. Gremillion from the U.S. Army Research\nLaboratory and were published in 2018 and 2019 in articles listed in the Biographical Sketch.\nAll other work conducted for the dissertation was completed by the student independently.\nFunding Sources\nGraduate study was supported by a fellowship from Coordenação de Aperfeiçoamento de\nPessoal de Nível Superior, by a fellowship from the U.S. Army Research Laboratory through\nthe Oak Ridge Associated Universities, and by a fellowship from the Department of Aerospace\nEngineering at Texas A&M University.\nResearch was sponsored by the U.S. Army Research Laboratory and was accomplished under\nCooperative Agreement Number W911NF-18-2-0134 and W911NF-17-2-0078. The views and\nconclusions contained in this document are those of the authors and should not be interpreted as\nrepresenting the ofﬁcial policies, either expressed or implied, of the Army Research Laboratory or\nthe U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copyright notation herein.\nvi\nNOMENCLATURE\nA3C\nAsynchronous Advantage Actor-Critic\nAAAI\nAssociation for the Advancement of Artiﬁcial Intelligence\nA(s, a)\nAdvantage function\nat\nAction vector at time t\nAGL\nAltitude Above Ground Level\nAIAA\nAmerican Institute of Aeronautics and Astronautics\nANN\nArtiﬁcial Neural Network\nA-RLC\nAdaptive-Reinforcement Learning Control\nBCE\nBinary Cross-Entropy\nCE\nCross-Entropy\nCNN\nConvolutional Neural Network\nCoL\nCycle-of-Learning for Autonomous Systems\nBC\nBehavior Cloning\nCEM\nCross-Entropy Method\nCMA-ES\nCovariance-Matrix Adaptation Evolution Strategy\nCOACH\nConvergent Actor-Critic by Humans\nCPU\nCentral Processing Unit\nD\nDiscriminator network\nDAgger\nDataset Aggregation\nDAPG\nDemo-Augmented Policy Gradient\nDeep RL\nDeep Reinforcement Learning\nDDPG\nDeep Deterministic Policy Gradient\nvii\nDDPGfD\nDeep Deterministic Policy Gradient from Demonstrations\nDPG\nDeterministic Policy Gradient\nDL\nDeep Learning\nDQN\nDeep Q-Network\nDQfD\nDeep Q-learning from Demonstrations\nDOF\nDegree-of-Freedom\nELU\nExponential Linear Unit\nJ(·)\nObjective function\nHCRL\nHuman-Centered Reinforcement Learning\nHER\nHindsight Experience Replay\nHRI\nHuman-Robot Interaction\nI2A\nImagination-Augmented Agents\nIEEE\nInstitute of Electrical and Electronics Engineers\nIJCNN\nInternational Joint Conference on Neural Networks\nIL\nImitation Learning\niLQR\nIterative Linear Quadratic Regulator\nIMPALA\nImportance Weighted Actor-Learner Architecture\nIOC\nInverse Optimal Control\nIPG\nInterpolated Policy Gradient\nIRL\nInverse Reinforcement Learning\nIRM\nIntrinsic Reward Module\nFCN\nFully-Connected Network\nFCCN\nFully-Connected Convolutional Network\nG\nGenerator network\nGAE\nGeneralized Advantage Estimator\nviii\nGAIL\nGenerative Adversarial Imitaiton Learning\nGAN\nGenerative Adversarial Network\nGCL\nGuided Cost Learning\nGPS\nGlobal Positioning System\nGPU\nGraphics Processing Unit\nKL\nKullback-Leibler\nL\nLoss function\nLfD\nLearning from Demonstrations\nLfE\nLearning from Evaluations\nLfI\nLearning from Interventions\nLSTM\nLong Short-Term Memory\nLP\nLinear Programming\nM-PAC\nMulti-Preference Actor Critic\nMAE\nMean Absolute Error\nMADCAT\nMission\nAdaptive\nDigital\nComposite\nAerostructure\nTechnologies\nMDN\nMixture Density Network\nMDP\nMarkov Decision Process\nMLP\nMultilayer Perceptron\nMPC\nModel Predictive Control\nMSE\nMean Squared Error\nMVE\nModel Value Expansion\nNAF\nNormalized Advantage Function\not\nObservation vector at time t\nOML\nOuter Mold Line\nπ\nPolicy\nix\nπ0\nInitial Policy\nPDMS\nPolydimethylsiloxane\nPER\nPrioritized Experience Replay\nPG\nPolicy Gradient\nPOfD\nPolicy Optimization with Demonstrations\nPOMDP\nPartially Observable Markov Decision Process\nPPO\nProximal Policy Optimization\nQ(s, a)\nState-action value function\nR\nReplay Buffer\nR1\n1-step return\nRn\nn-step return\nrt\nReward value at time t\nReLU\nRectiﬁed Linear Unit\nRGB\nVisible spectrum color code\nRL\nReinforcement Learning\nRMSProp\nRoot Mean Square Propagation\nRNN\nRecurrent Neural Network\nst\nState vector at time t\nSAA\nSense-and-Avoid\nSAC\nSoft Actor-Critic\nSAMI\nStructured Adaptive Model Inversion\nSC2LE\nStarCraft II Learning Environment\nSMA\nShape-Memory Alloy\nsUAS\nSmall Unmanned Air System\nSVM\nSupport Vector Machine\nx\nt\nTime step\nT\nTotal time steps in one episode\nτ\nTrajectory\nTAMER\nTraining an Agent Manually via Evaluative Reinforcement\nTAMU\nTexas A&M University\nTanh\nHyperbolic Tangent\nTD\nTemporal Difference\nTD3\nTwin Delayed DDPG\nθπ\nParameters of policy π (actor)\nθQ\nParameters of the critic\nTNPG\nTruncated Natural Policy Gradient\nTRPO\nTrust Region Policy Optimization\nµ\nDeterministic policy\nUAS\nUnmanned Air System\nUAV\nUnmanned Air Vehicle\nUNREAL\nUnsupervised Reinforcement and Auxiliary Learning\nV (s)\nState value function\nVAE\nVariational Autoencoder\nVICE-RAQ\nVariational Inverse Control with Events - Reinforcement learn-\ning with Active Queries\nVPG\nVanilla Policy Gradient\nVSCL\nVehicle Systems & Control Laboratory\nw\nWeight vector of a model\nZ\nProbability normalization term\nxi\nTABLE OF CONTENTS\nPage\nABSTRACT .........................................................................................\nii\nDEDICATION.......................................................................................\niv\nACKNOWLEDGMENTS ..........................................................................\nv\nCONTRIBUTORS AND FUNDING SOURCES .................................................\nvi\nNOMENCLATURE ................................................................................. vii\nTABLE OF CONTENTS ........................................................................... xii\nLIST OF FIGURES ................................................................................. xvi\nLIST OF TABLES................................................................................... xxi\n1. INTRODUCTION...............................................................................\n1\n1.1\nResearch Problem Overview .............................................................\n1\n1.2\nLiterature Review .........................................................................\n2\n1.2.1\nLearning from Human Interaction ...............................................\n2\n1.2.1.1\nLearning from Demonstrations .......................................\n2\n1.2.1.2\nLearning from Interventions ..........................................\n4\n1.2.1.3\nLearning from Evaluations............................................\n5\n1.2.2\nReinforcement Learning .........................................................\n7\n1.2.2.1\nState-of-the-Art Algorithms ..........................................\n7\n1.2.2.2\nSample-Efﬁciency in Reinforcement Learning ...................... 12\n1.2.2.3\nHuman-in-the-loop Reinforcement Learning ........................ 14\n1.2.2.4\nReinforcement Learning in Robotics and in the Real World ........ 18\n1.3\nMotivation and Objective ................................................................. 20\n1.3.1\nMotivation ........................................................................ 20\n1.3.2\nObjective .......................................................................... 20\n1.4\nResearch Contributions ................................................................... 21\n1.5\nOrganization ............................................................................... 22\n2. MACHINE LEARNING AND CLONING BEHAVIORS .................................... 24\n2.1\nProblem Deﬁnition ........................................................................ 24\n2.2\nSupervised Learning ...................................................................... 25\n2.2.1\nModel Representation and Learning............................................. 26\nxii\n2.2.1.1\nNeural Networks ...................................................... 26\n2.2.1.2\nLearning in Discrete and Continuous Spaces ........................ 30\n2.3\nSequential Decision Making Problems................................................... 35\n2.4\nCloning Behaviors with Imitation Learning ............................................. 35\n3. FROM SHALLOW TO DEEP REINFORCEMENT LEARNING ........................... 37\n3.1\nPartially and Fully Observable Markov Decision Processes ............................ 37\n3.2\nThe Reinforcement Learning Problem ................................................... 37\n3.2.1\nProblem Deﬁnition ............................................................... 37\n3.2.2\nTypes of Reinforcement Learning Algorithms .................................. 41\n3.3\nFrom Shallow to Deep Representations.................................................. 43\n3.3.1\nInductive Bias..................................................................... 45\n3.4\nExpanding Q-learning to Continuous Space with Deep Reinforcement Learning ..... 45\n4. CASE STUDY: DEEP REINFORCEMENT LEARNING ON INTELLIGENT MOTION\nVIDEO GUIDANCE FOR UNMANNED AIR SYSTEM GROUND TARGET TRACKING 49\n4.1\nProblem Deﬁnition ........................................................................ 49\n4.2\nRelated Work .............................................................................. 51\n4.3\nLearning Tracking Policies with Reinforcement Learning.............................. 52\n4.4\nPolicy Gradient Deep Reinforcement Learning Controller ............................. 54\n4.5\nNumerical Results......................................................................... 58\n4.6\nSummary .................................................................................. 60\n5. CASE STUDY: CONTROL OF MORPHING WING SHAPES WITH DEEP REIN-\nFORCEMENT LEARNING .................................................................... 62\n5.1\nProblem Deﬁnition ........................................................................ 62\n5.2\nLearning Morphing Between Wing Shapes .............................................. 64\n5.3\nExperimental Setup ....................................................................... 66\n5.4\nNumerical Results......................................................................... 71\n5.4.1\nValidation of the Learning Algorithm ........................................... 71\n5.4.2\nThe Learning Algorithm in the Wind Tunnel ................................... 72\n5.4.3\nThe Learning Algorithm in the Simulation Model.............................. 73\n5.5\nSummary .................................................................................. 74\n6. CASE STUDY: INVERSE REINFORCEMENT LEARNING APPLIED TO GUIDANCE\nAND CONTROL OF SMALL UNMANNED AERIAL VEHICLES ........................ 76\n6.1\nProblem Deﬁnition ........................................................................ 76\n6.2\nNotation and Background................................................................. 77\n6.2.1\nNotation ........................................................................... 77\n6.2.2\nGuided Cost Learning ............................................................ 78\n6.2.3\nGenerative Adversarial Imitation Learning ..................................... 78\n6.3\nRelated Work .............................................................................. 79\n6.4\nApplication to Guidance and Control of Small Unmanned Aerial Vehicles ........... 80\nxiii\n6.5\nResults and Discussion.................................................................... 81\n6.5.1\nNumerical Results ................................................................ 81\n6.5.2\nUnmanned Aerial Vehicle Results ............................................... 83\n6.6\nSummary .................................................................................. 85\n6.6.1\nTraining Hyperparameters ....................................................... 86\n7. CYBER-HUMAN APPROACH FOR LEARNING HUMAN INTENTION AND SHAPE\nROBOTIC BEHAVIOR BASED ON TASK DEMONSTRATION........................... 88\n7.1\nProblem Deﬁnition ........................................................................ 89\n7.2\nBackground................................................................................ 90\n7.2.1\nPreliminaries...................................................................... 90\n7.2.2\nBehavior Cloning ................................................................. 91\n7.2.3\nDeep Deterministic Policy Gradient ............................................. 92\n7.3\nLearning Human Intention and Shaping Robotic Behavior ............................. 93\n7.3.1\nThe CyberSteer Mechanics ...................................................... 93\n7.3.2\nEnvironment and Task Modeling ................................................ 96\n7.3.2.1\nUnmanned Air System Simulation Environment .................... 96\n7.3.2.2\nCollision Avoidance Scenario ........................................ 97\n7.4\nNumerical Results......................................................................... 97\n7.5\nSummary ..................................................................................100\n8. CYCLE-OF-LEARNING FOR AUTONOMOUS SYSTEMS FROM HUMAN INTER-\nACTION .........................................................................................104\n8.1\nProblem Deﬁnition ........................................................................104\n8.2\nTypes of Learning from Human Interaction .............................................105\n8.2.1\nLearning from Human Demonstrations .........................................106\n8.2.2\nLearning from Human Interventions ............................................ 107\n8.2.3\nLearning from Human Evaluations .............................................. 107\n8.3\nThe Cycle-of-Learning Concept..........................................................108\n8.3.1\nIntegrating and Switching Between Human Interaction Modalities ...........110\n8.4\nSummary .................................................................................. 111\n9. EFFICIENTLY COMBINING HUMAN DEMONSTRATIONS AND INTERVENTIONS\nFOR SAFE TRAINING OF AUTONOMOUS SYSTEMS IN REAL-TIME................113\n9.1\nProblem Deﬁnition ........................................................................113\n9.2\nBackground and Related Work ...........................................................116\n9.2.1\nLearning from Demonstrations ..................................................116\n9.2.2\nLearning from Interventions ..................................................... 117\n9.2.3\nRelated Work .....................................................................118\n9.3\nCombining Learning from Human Demonstrations and Interventions .................119\n9.3.1\nData Efﬁciency ...................................................................120\n9.3.2\nSafe Learning .....................................................................120\n9.3.3\nReal-Time Interaction ............................................................ 121\nxiv\n9.4\nImplementation............................................................................ 121\n9.4.1\nEnvironment Modeling........................................................... 121\n9.4.2\nThe Cycle-of-Learning Algorithm...............................................123\n9.4.3\nExperimental Methodology ......................................................125\n9.5\nNumerical Results.........................................................................126\n9.6\nSummary ..................................................................................130\n9.6.1\nCurrent Limitations and Future Directions......................................132\n10. INTEGRATING BEHAVIOR CLONING AND REINFORCEMENT LEARNING FOR\nIMPROVED PERFORMANCE IN SPARSE AND DENSE REWARD ENVIRONMENTS134\n10.1 Problem Deﬁnition ........................................................................135\n10.2 Preliminaries .............................................................................. 137\n10.3 Related Work ..............................................................................138\n10.4 Integrating Behavior Cloning and Reinforcement Learning ............................139\n10.5 Numerical Results......................................................................... 141\n10.5.1 Experimental Setup............................................................... 141\n10.5.2 Experimental Results.............................................................145\n10.5.3 Component Analysis .............................................................146\n10.5.3.1 Effects of Pre-Training ................................................148\n10.5.3.2 Effects of Combined Loss ............................................150\n10.5.3.3 Effects of Human Experience Replay Sampling ..................... 151\n10.6 Summary ..................................................................................152\n11. CONCLUSIONS AND RECOMMENDATIONS .............................................155\nREFERENCES ......................................................................................158\nxv\nLIST OF FIGURES\nFIGURE\nPage\n1.1\nCycle-of-Learning for Autonomous Systems from Human Interaction: as the policy\ndevelops, the autonomy independence increases while the human interaction level\ndecreases. Adapted from [1]. ............................................................. 22\n2.1\nA neural network with D inputs, one hidden layer with M units, and K outputs.\nAdapted from [2]. ......................................................................... 27\n2.2\nLinear activation function y(x) and its derivative y′(x). ................................ 30\n2.3\nSigmoid activation function y(x) and its derivative y′(x). .............................. 30\n2.4\nHyperbolic Tangent (Tanh) activation function y(x) and its derivative y′(x). ......... 31\n2.5\nRectiﬁed Linear Unit (ReLU) activation function y(x) and its derivative y′(x)........ 31\n2.6\nLeaky Rectiﬁed Linear Unit (Leaky ReLU) activation function y(x) and its deriva-\ntive y′(x). .................................................................................. 32\n2.7\nExponential Linear Unit (ELU) activation function y(x) and its derivative y′(x)...... 32\n2.8\nSoftplus activation function y(x) and its derivative y′(x). .............................. 33\n2.9\nPartially-Observable Markov Decision Process diagram. Adapted from [3]. .......... 35\n3.1\nReinforcement learning problem modeled as agent and environment interactions..... 38\n3.2\nPolicy evaluation and optimization cycle. ............................................... 41\n3.3\nVisualization of each layer of a deep neural network use to classify different images.\nReprinted from [4]......................................................................... 44\n4.1\nIntelligent motion video guidance for unmanned air system ground target tracking\nmodeled as a reinforcement learning problem. Reprinted from [5]..................... 53\n4.2\nIllustration of AV-8B Harrier, whose linear model is used to validate the proposed\nPG Deep RL controller. Reprinted from [5]. ............................................ 54\n4.3\nPerformance evaluation with respect to reward values and maximum number of\nsteps achieved for different learning agents with different hyperparameters (see\nTable 4.3) during each training episode. Reprinted from [5]. ........................... 59\nxvi\n4.4\nPerformance evaluation with respect to mean discounted reward and its standard\ndeviation for different learning agents with different hyperparameters (see Table\n4.3) during each training episode. Reprinted from [5]................................... 60\n4.5\nConsistent tracking performance of PG Deep RL agent on a simulated environment.\nReprinted from [5]......................................................................... 61\n5.1\nDiagram of the learning algorithm and interface with the morphing wing model.\nReprinted from [6]......................................................................... 65\n5.2\nDeep neural network architecture that maps the current wing conﬁguration to control\ninputs. Reprinted from [6]. ............................................................... 66\n5.3\nExperimental setup for the learning algorithm: prototype in the wind tunnel.\nReprinted from [6]......................................................................... 67\n5.4\nExperimental setup for the learning algorithm: top view of the prototype. Reprinted\nfrom [6]. ................................................................................... 68\n5.5\nExperimental setup for the learning algorithm: bottom view of the prototype.\nReprinted from [6]......................................................................... 68\n5.6\nPreliminary results modeling the simple spring system showing (a) Displacement\nand temperature changes over time; and (b) Displacement as a function of tempera-\nture. Reprinted from [6]................................................................... 70\n5.7\nResults of the validation of the Deep Deterministic Policy Gradient algorithm solv-\ning the inverted pendulum upswing. Average of ﬁve runs and standard deviation\nof rewards per episode. (a) Unitary mass pendulum case; (b) Double mass pendu-\nlum case; (c) Double length pendulum case; and (d) Half gravity pendulum case.\nReprinted from [6]......................................................................... 72\n5.8\nTraining on the Wind Tunnel: Deep Reinforcement Learning controller performance\nfor training time consisting of: (a) 100 and (b) 200 episodes. Reprinted from [6]..... 73\n5.9\nTraining on the Modeled Airfoil: Deep Reinforcement Learning controller perfor-\nmance after 900 (a) and 1500 (b) episodes of training time, respectively. Reprinted\nfrom [6]. ................................................................................... 74\n6.1\nIllustration of the Pendulum environment. The agent controls the torque on the base\nof the pendulum in order to maintain it in the vertical position. ........................ 81\n6.2\nPerformance in terms of original task reward for different number of expert trajecto-\nries for Pendulum GCL. .................................................................. 82\n6.3\nIllustration of the LunarLanderContinuous environment. The agent controls the\nmain and side thrusts of the vehicle in order to land safely between he ﬂags. ......... 83\nxvii\n6.4\nPerformance in terms of original task reward for different number of expert trajecto-\nries for LunarLanderContinuous GAIL. ................................................. 84\n6.5\nPerformance in terms of original task reward for the best performing number of\nexpert trajectories for LunarLanderContinuous GAIL. ................................. 84\n6.6\nPerformance in terms of original task reward for different hyperparameters for\nAirSim GAIL. ............................................................................. 85\n6.7\nPerformance in terms of original task reward for the best performing hyperparame-\nters for AirSim GAIL. .................................................................... 86\n7.1\nOverall Diagram of the CyberSteer framework. Reprinted from [7]. .................. 93\n7.2\nCyberSteer #1: Computing estimated rewards based on the likelihood of the action\ntaken being similar to previous actions taken by the human supervisor. Reprinted\nfrom [7]. ................................................................................... 94\n7.3\nCyberSteer #2: Computing estimated rewards based on how similar the actions\ntaken by the agent are when compared to a behavior cloning network trained with\ndemonstration form the human supervisor. Reprinted from [7]......................... 96\n7.4\nWarehouse scenario created for the collision avoidance task using Unreal Engine\nfor visually realistic textures and objects. Reprinted from [7]. ......................... 98\n7.5\nComparison of the proposed solutions to achieve human-like performance on the\ntask with no feedback from the environment when compared to a baseline dependent\non environment reward signals. Reprinted from [7]. ....................................100\n7.6\nTask completion performance of the proposed solutions with no feedback from the\nenvironment when compared to the established baseline dependent on environment\nreward signals. Reprinted from [7]. ...................................................... 101\n7.7\nExtended plots of the comparison of the proposed solutions to achieve human-like\nperformance on the task with no feedback from the environment when compared to\na baseline dependent on environment reward signals. Reprinted from [7]. ............102\n7.8\nExtended plots of the task completion performance of the proposed solutions with\nno feedback from the environment when compared to the established baseline\ndependent on environment reward signals. Reprinted from [7]. ........................103\nxviii\n9.1\nCycle-of-Learning for Autonomous Systems from Human Interaction: a concept for\ncombining multiple forms of human interaction with reinforcement learning. As the\npolicy develops, the autonomy independence increases and the human interaction\nlevel decreases. This work focuses on the ﬁrst two components of the cycle (dashed\nbox): Learning from Demonstration and Learning from Intervention. Reprinted\nfrom [8]. ...................................................................................116\n9.2\nFlow diagram illustrating the learning from demonstration and intervention stages\nin the CoL for the quadrotor perching task. Reprinted from [8]. .......................122\n9.3\nScreenshot of AirSim environment and landing task. Inset image in lower right\ncorner: downward-facing camera view used for extracting the position and radius\nof the landing pad which is part of the observation-space that the agent learns from.\nReprinted from [8].........................................................................123\n9.4\nPerformance comparison in terms of task completion with Interventions (Int),\nDemonstrations (Demo) and the Cycle-of-Learning (CoL) framework for (A) 4\nhuman interactions, (B) 8 human interactions, (C) 12 human interactions and (D) 20\nhuman interactions, respectively. Here, an interaction equates to a single demonstra-\ntion or intervention and roughly corresponds to the number of episodes. Error bars\ndenote 1 standard error of the mean. We see that CoL outperforms Int and Demo\nacross nearly all human interaction levels. Reprinted from [8]. ........................ 127\n9.5\nComparison of the number of human samples used for training with Interventions\n(Int), Demonstrations (Demo) and the Cycle-of-Learning (CoL) framework for (A)\n4 human interactions, (B) 8 human interactions, (C) 12 human interactions and\n(D) 20 human interactions, respectively. Error bars denote 1 standard error of the\nmean. We see that CoL uses less data than the demonstration-only condition and\nonly slightly more data than the intervention-only condition. Reprinted from [8]. ....128\n9.6\nPerformance comparison between the Cycle-of-Learning (CoL) with four contin-\nuous actions and the Deep Reinforcement Learning algorithm Proximal Policy\nOptimization (PPO) trained for three different task complexities using 2, 3, and 4\ncontinuous actions. Reprinted from [8]. .................................................129\n10.1 Comparison of CoL, BC, DDPG, and DAPG for 3 random seeds (bold line repre-\nsenting the mean and shaded area the standard error) in the dense–reward Lunar-\nLanderContinuous-v2 environment. Reprinted from [9]. ...............................146\n10.2 Comparison of CoL, BC, DDPG, and DAPG for 3 random seeds (bold line repre-\nsenting the mean and shaded area the standard error) in the sparse-reward Lunar-\nLanderContinuous-v2 environment. Reprinted from [9]. ............................... 147\n10.3 Comparison of CoL, BC, DDPG, and DAPG for 3 random seeds (bold line repre-\nsenting the mean and shaded area the standard error) in the sparse-reward Microsoft\nAirSim quadrotor landing environment. Reprinted from [9]. ...........................148\nxix\n10.4 Effects of the pre-training phase in the Cycle-of-Learning. Results for 3 random\nseeds (bold line representing the mean and shaded area the standard error) show-\ning component analysis in LunarLanderContinuous-v2 environment comparing\npre-trained Cycle-of-Learning (CoL curve) against the Cycle-of-Learning without\nthe pre-training phase (CoL-PT curve) and the behavior cloning (BC) baseline.\nReprinted from [9].........................................................................149\n10.5 Effects of the combined loss in the Cycle-of-Learning. Results for 3 random seeds\n(bold line representing the mean and shaded area the standard error) showing com-\nponent analysis in LunarLanderContinuous-v2 environment comparing complete\nCycle-of-Learning (CoL), CoL without the expert behavior cloning loss (CoL-BC),\nand pre-training with BC followed by DDPG without combined loss (BC+DDPG).\nReprinted from [9]......................................................................... 151\n10.6 Effects of human experience replay sampling in the Cycle-of-Learning. Results\nfor 3 random seeds (bold line representing the mean and shaded area the standard\nerror) showing ablation study in LunarLanderContinuous-v2 environment, dense\n(D) and sparse (S) reward cases, comparing complete Cycle-of-Learning (CoL)\ntrained with ﬁxed ratio of expert and agent samples and complete Cycle-of-Learning\nusing Prioritized Experience Replay (CoL+PER) with a variable ratio of expert and\nagent samples ranked based on their temporal difference error. Reprinted from [9]...152\nxx\nLIST OF TABLES\nTABLE\nPage\n2.1\nSelect activation function equations and their derivatives. .............................. 28\n2.2\nSelect activation function advantages and disadvantages................................ 29\n4.1\nAircraft trim parameters. Reprinted from [5]. ........................................... 56\n4.2\nModeled camera speciﬁcations. Reprinted from [5]..................................... 57\n4.3\nHyperparameter values used for the policy gradient deep reinforcement learning\nalgorithm. Reprinted from [5]. ........................................................... 60\n5.1\nHyperparameter values used for DDPG algorithm. Reprinted from [6]. ............... 66\n6.1\nHyperparemeter search for LunarLanderContinuous GAIL ............................ 86\n6.2\nHyperparemeter search for AirSim GAIL ............................................... 87\n6.3\nTraining Statistics for AirSim GAIL ..................................................... 87\n10.1 Cycle-of-Learning hyperparemeters for each environment: (a) LunarLanderContinuous-\nv2 and (b) Microsoft AirSim. Reprinted from [9]. ......................................144\n10.2 Method Comparison on LunarLanderContinuous-v2 environment, dense-reward\ncase. Reprinted from [9]. ................................................................. 147\nxxi\n1.\nINTRODUCTION\n“We call ourselves Homo sapiens — man the wise — because our intelligence is so important\nto us. For thousand of years, we have tried to understand how we think; that is, how a mere\nhandful of matter can perceive, understand, predict, and manipulate a world far larger and more\ncomplicated than itself. The ﬁeld of artiﬁcial intelligence, or AI, goes further still: it attempts not\njust to understand but also build intelligent entities.”\n— Stuart Russell. “Artiﬁcial Intelligence: A Modern Approach”, 2003 [10].\n1.1\nResearch Problem Overview\nData-driven approaches and learning algorithms are well suited to solve high-level prediction\nand control problems in an information-rich world. Learning algorithms are able to learn directly\nfrom examples, to search for an underlying pattern of an apparently patternless data, and to improve\na decision-making process by continuously repeating it and observing the results.\nThe primary goal of learning methodologies is to imbue intelligent agents with the capability\nto autonomously and successfully perform complex tasks, when a priori design of the necessary\nbehaviors is intractable. Instead, given an objective function for the desired behavior, learning\ntechniques can be used to empirically discover the policy or controller required to satisfy it. Several\nclasses of these techniques have yielded promising results, including reinforcement learning,\nlearning from demonstrations, interventions, and evaluations.\nReinforcement learning has been shown to work on scenarios with well-designed reward\nfunctions and easily available interactions with the environment. However, in real-world robotic\napplications, explicit reward functions are non-existent, and interactions with the hardware are\nexpensive and susceptible to catastrophic failures. This motivates leveraging human interaction\nto supply this reward function and task knowledge, to reduce the amount of high-risk interactions\nwith the environment and to safely shape the behavior of robotic agents, thus, enabling Real-Time\nHuman-in-the-Loop Reinforcement Learning.\n1\n1.2\nLiterature Review\nThis research focuses on two main areas: learning from human interaction and reinforcement\nlearning.\n1.2.1\nLearning from Human Interaction\nThis section addresses the state-of-the-art and current challenges when learning from human\ninteraction, including learning from human demonstrations, interventions, and evaluations. Other\nhuman interaction modalities considered but not included on this work are natural language, eye\ngaze tracking, gestures, brain electrical activity, and domain knowledge that is applied only to a\nsingle task.\n1.2.1.1\nLearning from Demonstrations\nLearning from Demonstrations (LfD) can be used to provide a more directed path to these\nintended behaviors by utilizing examples of humans performing the task. This technique has the\nadvantage of quickly converging to more stable behaviors. However, given that it is typically\nperformed ofﬂine, it does not provide a mechanism for corrective or preventative inputs when the\nlearned behavior results in undesirable or catastrophic outcomes, potentially due to unseen states.\nLfD also inherently requires the maximal burden on the human, requiring them to perform the task\nmany times until the state space has been sufﬁciently explored, so as to generate a robust policy.\nAlso, it necessarily fails when the human is incapable of performing the task successfully at all.\nThere are many empirical successes of using imitation learning to train a policy or controller\nbased on human demonstrations [11]. Early research on learning from human demonstrations was\nprimarily focused on teaching higher-level commands, as for example “pick\", “move\", and “place\"\nwhen controlling a robotic arm [12, 13, 14], which later shifted to trajectory-level planning when\nthe term “Learning from Demonstrations\" became popular [15, 16, 17, 14].\nFor self-driving cars, the earlier Autonomous Land Vehicle In a Neural Network (ALVINN) [18]\nby Pomerleau learned from demonstrations to map from images to discrete actions using a single\nhidden-layer neural network. Most recent research also successfully used human demonstrations\n2\nto train a policy that mapped from front-facing camera images to steering wheel commands using\naround one hundred hours of human driving data [19]. Similar approaches have been taken to train\nsmall unmanned air system (sUAS) to navigate through cluttered environments while avoiding\nobstacles [20]. Nair et al. [21] combining self-supervised learning and behavior cloning used images\nas inputs to create a pixel-level inverse dynamics model of a robotic rope manipulation task. Related\nto navigation, there is related work that uses inverse optimal control on human demonstrations to\nlearn navigation skill in a real world robot, and detect failure states and learn recover policies using\nGaussian processes [22].\nRahmatizadeh et al. [23] demonstrated that humans can also aid robotic learning through\ndemonstration of the goal task. Long Short-Term Memory (LSTM) networks [24] can be used to\ngeneralize human demonstration from virtual environments and have the learned policy transferred\nto a physical robot. When human data are limited, Deisenroth et al. [25] used Gaussian Processes\nto extract more information about each interaction between human and robot to reduce the time\nrequired to learn the robotic tasks. These approaches currently limit the robot behavior to what has\nbeen demonstrated by the human expert.\nAnother example of work that attempts to augment learning from demonstrations with additional\nhuman interaction is the Dataset Aggregation (DAgger) algorithm [26]. DAgger is an iterative\nalgorithm that consists of two policies, a primary agent policy that is used for direct control of a\nsystem, and a reference policy that is used to generate additional labels to ﬁne-tune the primary\npolicy towards optimal behavior. Importantly, the reference policy’s actions are not taken, but are\ninstead aggregated and used as additional labels to re-train the primary policy for the next iteration.\nIn [27] DAgger was used to train a collision avoidance policy for an autonomous quadrotor using\nimitation learning on a set of human demonstrations to learn the primary policy and using the\nhuman observer as a reference policy. There are some drawbacks to this approach that are worth\ndiscussing. As noted by Ross et al. [27], because the human observer is never in direct control\nof the policy, safety is not guaranteed, since the agent has the potential to visit previously unseen\nstates, which could cause catastrophic failures. Additionally, the subsequent labeling by the human\n3\ncan be suboptimal both in the amount of data recorded (perhaps recording more data in suboptimal\nstates than is needed to learn an optimal policy) as well as in capturing the intended result of the\nhuman observer’s action (as in distinguishing a minor course correction from a sharp turn, or the\nappropriate combination of actions to perform a behavior). Another limitation of DAgger is that\nthe human feedback was provided ofﬂine after each run while viewing a slower replay of the video\nstream to improve the resulting label quality. This prevents the application to tasks where real-time\ninteraction between humans and agents are required.\nDemonstrations can also be used to infer the cost function used by the demonstrator while\nperforming the task, known as Inverse Optimal Control (IOC) [28, 29, 30, 31] or Inverse Reinforce-\nment Learning (IRL) [32, 33, 28] when it is inferred the reward function, which is the negative of\nthe cost function. Following this principle, related research uses the maximum entropy principle to\nmaximize the entropy of the model distribution subject to the feature constraints from demonstration\ndata [34, 35], evaluated using the difference between value function for the optimal policy obtained\nusing the learned reward model and using the ground truth reward, or expected value difference [35].\nLeveraging the principle of maximum entropy, previous approaches [36] learns the cost function\nand optimizes a policy for it at the same time, compares this policy to the demonstrated trajectories,\nperforms and evaluation step, and repeats the procedure until convergence or the desired perfor-\nmance is achieved. Alternatively to IRL, demonstrations can be integrated directly to Reinforcement\nLearning (RL) algorithms to increase their sample-efﬁciency during training.\n1.2.1.2\nLearning from Interventions\nLearning from interventions (LfI), where a human acts as an overseer while an agent is perform-\ning a task and periodically takes over control or intervenes when necessary [37, 38], can provide\na method to improve the agent policy while preventing or mitigating catastrophic behaviors [39].\nSimilar work by Hilleli and El-Yaniv [40] has proposed using human interaction to train a classiﬁer\nto detect unsafe states, which would then trigger the intervention by a safe policy previously trained\nbased on human demonstration of the task. This technique can also reduce the number of direct\ninteractions with the agent, when compared to learning from demonstration. However, this technique\n4\nsuffers from the disadvantage that desired behaviors must be discovered through more variable\nexploration, resulting in slower convergence and less stable behavior.\nRelated work includes slowing down the execution of the task controlled by RL agent during\ntraining so the human can intervene at any step to prevent catastrophic actions by replacing agent’s\nactions by safe human actions. On Saunders et al. [39], the task is paused and a model is trained\nto imitate human intervention decisions. The trained intervention model replaces human and the\ntraining continues. They show that this approach works well for simple cases when the human is able\nto prevent catastrophic actions but it does not scale well to more complex tasks due to the amount\nof human intervention required. Related work uses interventions to build upon policies trained with\nhuman demonstration for robot navigation in the real world [41] and kinesthetic teaching of a robot\nvia keyframes that can be connected to generate trajectories [38].\nSeveral cases include the combination of demonstrations and mixed initiative control for training\nrobotic polices [42] as well as combining imitation learning with interactive reward shaping in a\nsimulated racing game [40]. Previous approaches combined human actions with robot autonomy to\nachieve a common goal. On Javdani et al. [43], the robot did not know the goal a-priori and used\ninverse optimal control and the maximum entropy principle to estimate the distribution over the\nhuman’s goal based on previous inputs. They showed this approach enabled faster task completion\nusing less data samples when compared to the traditional approach where the robot ﬁrst predicts the\ngoal then assist for it. Other approaches [44] combined a pre-trained Q-learning policy with human\ninput and showed that the combination is better than the parts by themselves. The main limitation is\nthat it needs a Q-function trained already, which might not be realistic for real-world tasks.\n1.2.1.3\nLearning from Evaluations\nLearning from evaluation (LfE) is one such way to leverage human domain knowledge and\nintent to shape agent behavior through sparse interactions in the form of evaluative feedback,\npossibly allowing for the approximation of a reward function [45, 46, 47]. This technique has the\nadvantage of minimally tasking the human evaluator and can be used when training behaviors they\nthemselves cannot perform, only requiring an understanding of the task goal. An example would be\n5\nmaneuvering a robotic arm with multiple degrees of freedom in a constrained space. Due to the\nnumber of joints and obstacles, the human might not be able to provide complete demonstrations\nbut is able to evaluate if the maneuver was executed successfully or not. Additionally, if the\ntime-scale of the autonomous system is faster than human reaction time, then it can be challenging\nfor the autonomous system to attribute which actions correspond to the provided feedback (credit\nassignment problem).\nSimilar to LfI, it can be slow to converge as the agent can only identify desired or even stable\nbehaviors through more random exploration or indirect guidance from human negative reinforcement\nof unwanted actions, rather than through more explicit examples of desired behaviors. Another\ndisadvantage is that the human evaluation signals are generally non-stationary and policy-dependent,\nfor example, what was classiﬁed as a good action in the past may not be classiﬁed in the same way\nin the present depending on the human perception of the autonomous system’s policy.\nThomaz and Breazeal [48] addressed how humans want to reward machines guided by RL\nalgorithms, how to design machines that learn effectively from natural human interaction, and\nthe motivation to maintain safe exploration of new actions and states when using RL algorithms\nguided by humans. Knox et al. [45, 50, 49, 51] have developed methods for adapting human\ninputs to classic machine learning algorithms via the ”Training an Agent Manually via Evaluative\nReinforcement\" (TAMER) framework. The human trainer rewards the robot based on its past actions\nand the framework handles the distribution of these rewards along the state-action pairs to shape the\npolicy of the intelligent agent. This work was later extended to use deep neural networks as learning\nrepresentation to solve ATARI games using raw images as inputs, called Deep TAMER [47]. León\net al. [52] mixed human demonstration and direct natural language human feedback during the task\nexecution as an additional dynamic reward shaping mechanism. So far these methods have only\nbeen applied to low-dimensional RL problems.\nDifferently than using the human as a reward function generator, MacGlashan et al. presents the\nConvergent Actor-Critic by Humans (COACH) algorithm, which uses the human as the temporal\ndifference (TD) error. COACH is based on the insight that the advantage function is a good model\n6\nof human feedback and that actor-critic algorithms update a policy using the critic’s TD error, which\nis an unbiased estimate of the advantage function. This work was later extended by Arumugam et al.\n[53] to use deep neural networks, called Deep COACH, to learn policies mapping raw pixel inputs\nto actions in Minecraft.\nChristiano et al. [54] provided a framework to shape the behavior of an RL agent when the task\nis relatively complicated to be demonstrated by a human operator. Instead of performing the task,\nthe agent presents two different trajectories (sequence of states and actions) and queries the human\nthe preferred option. The resultant behavior is more natural when compared to the one achieved\nby human-handcrafted rewards. Similar to this approach, Singh et al. [55] presents VICE-RAQ\n(Variational Inverse Control with Events - Reinforcement learning with Active Queries) which\nconstructs a classiﬁer to function as a reward function based on example of successful outcomes\nprovide by humans instead of relying on a handcrafted environment reward. After that, the agent\ncan query the human for more labels when it encounters possible goal states. Ibarz et al. [56]\ncombines learning from expert demonstrations to learning from preferences to solve ATARI games\nwithout using the game score. They train an behavior cloning policy then learn a reward using\nhuman feedback and trajectory preferences and show that this combined approach outperforms\nthe use of only preferences or only demonstrations. Additionally, they also show that humans can\nprevent reward hacking due to the constant and online feedback.\n1.2.2\nReinforcement Learning\nThis section addresses the state-of-the-art and current challenges relevant to the Reinforcement\nLearning (RL) literature, including sample-efﬁciency when training RL algorithms and deploying\nthese algorithms in real-world and robotics applications.\n1.2.2.1\nState-of-the-Art Algorithms\nReinforcement Learning is a fast-moving ﬁeld and according to Schulman [57], there is still\nno consensus on which RL algorithm would perform better on different applications and robotic\nsystems: actor-only and actor-critic methods, policy gradient methods (e.g., score function, natural\n7\nor non-natural re-parameterization), value learning (e.g., Q-learning and its derived algorithms), or\nderivative-free optimization approaches (e.g., cross-entropy method). According to Grondman et al.\n[58], in (quasi-)stationary Markov Decision Processes (MDP), actor-critic methods should provide\npolicy gradients with lower variance than actor-only methods. However, actor-only methods are\nmore resilient to fast changing non-stationary environments due to the critic not being able to adapt\nas quick as the actor to the new environment and, consequently, providing poor information for\nactor updates.\nIt could be argued that policy gradient reinforcement learning started with Sutton et al. [59], work\nthat paved the way to modern policy gradient algorithms using function approximation. Another\nclassic work by Kakade and Langford [60] proposed an algorithm to estimate the lower bound\nof a policy performance, which can be used to constrain the policy updates and guarantee policy\nimprovement. Policy gradient is a sub-ﬁeld of policy search [61] where the gradient is used to guide\nthe search. Alternatively, Levine and Koltun [62] uses iterative linear quadratic regular (iLQR [63]),\ninitialized from demonstrations, for trajectory optimization and direct policy search. The iLQR\nsamples trajectories from high-reward regions, which are incorporated to the policy search by using\nregularized importance sampling.\nIn modern policy gradient methods for deep reinforcement learning (Deep RL), Trust Region\nPolicy Optimization (TRPO) and Truncated Natural Policy Gradient (TNPG) by Schulman et al.\n[64] iteratively optimize policies with guaranteed monotonic improvement using the natural policy\ngradient to optimize a given model in the policy-space, constrained by a policy trust region, instead\nof optimize it in the parameter-space performed by gradient descent. Similarly, Proximal Policy\nOptimization (PPO) [65] incorporates similar ideas to TRPO in terms of constraining policy updates\nin the policy-space instead of parameter-space by clipping the objective function and also penalizes\nthe KL divergence between new and old policy in the objective function. The bias-variance trade-off\nfor this methods can be controlled using Generalized Advantage Estimator (GAE) [66], leading\nto high-performance agents in complex controls benchmarks. Multiple Deep RL methods for\ncontinuous control are benchmarked by Duan et al. [67] showing the efﬁcacy of the natural policy\n8\ngradient methods.\nPolicy gradient methods can also be parallelized. Heess et al. [68] implements a distributed\nform of PPO where data collection and gradient calculation are distributed between parallel workers.\nThe authors show that agents trained in rich environments without an speciﬁcally handcrafted\nreward function can lead to the development of non-trivial locomotion skills that would be difﬁcult\nto be encoded in a reward function. Mnih et al. [69] introduces the Asynchronous Advantage\nActor-Critic (A3C) that substitutes the experience replay buffer by running multiple copies of the\nenvironment. A3C maintains a policy and a value function and uses a mix of n-step returns to\nupdate both. Policy is updated using the policy gradient based on advantage function, while the\nvalue function is updated based on the mean squared error with the n-step return. This work also\nuses target networks to stabilize training, shared layers between policy and value function, and\nan entropy regularization term with respect to the policy parameters. The IMPALA (Importance\nWeighted Actor-Learner Architecture) algorithm by Espeholt et al. [70] generates, in parallel,\nmultiple trajectories and communicate them to a central learner. Since the policy generating the\ntrajectories might be different to the one being updated, they propose the new V-trace algorithm\nfor off-policy correction using truncated importance sampling. This new approach achieves better\nperformance than previous agents with less data on controls and ATARI benchmarks.\nFor value learning in discrete action-spaces, for example, when the agent learns to predict based\non the temporal differences [71] or learns a Q-function [72] representing the expected discounted\nsum of rewards to be received at the end of an episode when the agent is a given state and performs\na given action, much of the recent progress came after the work by Mnih et al. [73, 74], which\nsuccessfully integrated deep neural networks to reinforcement learning in the Deep Q-Network\n(DQN) algorithm to solve multiple ATARI games learning directly from images (pixel inputs) — a\nsigniﬁcantly larger state-space when compared to learning from low-dimensional states. Key insight\nfrom the DQN work is the beneﬁt of having a replay buffer [75] to storage previous interactions\nbetween agent and environment, more speciﬁcally using Prioritized Experience Replay (PER)\n[76]. By using an experience replay and random sampling, the samples become closer to satisfy\n9\nthe i.i.d. (independent and identically distributed) assumptions used in stochastic gradient-based\nalgorithms and liberates online learning agents from processing transitions in the exact order they\nare experienced [76]. In addition to that and to replace random sampling, PER samples transition\nwith higher TD-error, corrected with importance sampling, and diversity alleviated using stochastic\nprioritization, liberating agents from considering transitions with the same frequency that they are\nexperienced [76]. Successful transitions can also be added to this buffer to aid the learning process\n[77].\nThe DQN work was extended by Hasselt et al. [79, 78] with the Double Deep Q-Networks\n(DDQN) algorithm, correcting the overestimation of the Q values typical of DQN approach due to\nthe maximization step over estimated action values. DDQN adds a second Q network (called target\nnetwork) that decouples action selection and action evaluation and leading to better performance\non the ATARI benchmark. Further extending DQN, Wang et al. [80] proposes a neural network\narchitecture that decouples the Q-function in values and advantages while sharing a common feature\nlearning module. This switches the focus to ﬁnding the more valuable states and removing the need\nto evaluate the Q-function for each action, improving DQN performance on tasks with larger action-\nspace. Instead of only learning the expected value for the value functions, Bellemare et al. [81] with\nDistributional Q-learning proposes learning complete value function distributions. Authors argue\nthat learning the complete distribution, when combined with function approximation, leads to a\nmore stable policy during training. Many of these advances and extensions to DQN were combined\nin a single algorithm, Rainbow DQN [82], to achieve state-of-the-art performance on ATARI games.\nRainbow DQN uses noisy networks for exploration [83], instead of the traditional ϵ-greedy approach\nused in Q-learning which adds noise to the action-space. In the work by Fortunato et al., the authors\nadd noise to the parameters of the policy (to the weights, if using a neural networks), which are\nalso learned with gradient descent. This approach lead to better performance when compared to\ntraditional RL approaches for exploration.\nFor value learning in continuous action-spaces, the Deep Deterministic Policy Gradient (DDPG)\n[84, 85] by Lillicrap et al. was one the ﬁrst adaptations of DQN for continuous action-spaces. DDPG\n10\nuses two deep neural networks to learn at the same time a deterministic policy and an Q-function,\nwhich is assumed to be differentiable with respect to the actions and used to guide the policy updates\nbased on its gradients, substituting the maxa Q(s, a) used in DQN. Alternatively to DDPG, Gu\net al. [86] proposes the Normalized Advantage Function (NAF) algorithm that parameterizes the\nadvantage term as a quadratic function so the actions are computed analytically and the agent only\nneeds to learn the Q-values. The authors also show that incorporating a model-based approach\nto their method to generate off-policy samples during the beginning of training leads to better\nperformance when compared to just using a model-free approach. Interestingly, the model-based\npart needs to be switched off a later stages of training because Q-learning performs better with\non-policy samples when the policy is already better developed. Similar to DQN, the main problem\nwith DDPG is overestimating the Q-function and having the policy to exploit this error, leading\nto policy breaking during training. Fujimoto et al. [87] presents the Twin Delayed DDPG (TD3)\nalgorithm that solves this issue by using two clipped Q-networks, learning the policy slower than\nthe critic, and adding noise and smoothing actions to prevent exploitation of overestimated Q-values.\nThis approach leads to better performance when compared to DDPG. As an alternative to the\n“deterministic policy\" of DDPG, Haarnoja et al. [88, 89] introduces the Soft Actor-Critic (SAC)\nalgorithm combining off-policy actor-critic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maximization objective. The entropy term on the\nobjective functions allow to control for the exploration-exploitation trade-off in RL while learning a\nmore robust policy to noisy observations.\nHybrid approaches as the Interpolated Policy Gradient (IPG) algorithm [90] and Q-Prop [91]\nintegrate the sample-efﬁciency of off-policy RL algorithms with the stability of the on-policy ones\nby combining off- and on-policy updates in the same loss function while satisfying performance\nbounds. These algorithm uniﬁes both techniques, has theoretical guarantees on the bias introduced\nby off-policy updates, and improves on the state-of-the-art model-free deep RL methods.\n11\n1.2.2.2\nSample-Efﬁciency in Reinforcement Learning\nCurrent state-of-the-art RL algorithms heavily rely on multiple graphics and central processing\nunits (GPUs and CPUs) to train intelligent agents on end-to-end approaches [73, 78, 76, 69]. End-\nto-end algorithms are initialized with no previous knowledge of the task nor the environment and\nand the action selection process (trial-and-error) develops almost randomly. This approach lead\nto learning algorithms that are able to generalize to multiple problems at the cost of high number\nof interactions with the environment. Agents learning simple continuous tasks (e.g., controlling\na simulated two-link robotic arm) require on average more than 2.5 million samples to achieve\nsatisfactory performance [85]. On more complex tasks (e.g., control of a simulated humanoid\nrobot or robots with multiple degrees of freedom) they require on average 25 million samples\n[67]. Discrete computer game tasks, such as learning how to play Atari games, may require tens\nof millions of interactions with the environment to achieve state-of-the-art results [73, 74]. Less\ncomplex algorithms, like REINFORCE [92] and Cross-entropy Methods (CEM) [93], achieve good\nperformance optimizing policies parametrized as deep neural networks, but are likely to converge\nprematurely to local optima [94]. There is a need for sample-efﬁcient learning architectures in\nwhich a learning agent would quickly learn meaningful behavior, requiring fewer interactions with\nthe environment, easing the transition to robotic systems performing real-world tasks.\nModel-based approaches are a common option to improve sample-efﬁciency of RL algorithms\nbut it becomes challenging to learn a complex dynamical system with a low number of samples\navailable. Feinberg et al. [95] uses model-based approaches to estimate value functions for short\nterm and model-free for long-term estimation of Q-values in reward dense environments. The\ndistinction between how many time steps comprises short- or long-term relies on model value\nexpansion (MVE) for value estimation error. Their approach can be generalized for any actor-critic\nalgorithm. Nagabandi et al. [96] proposes to learn the dynamics using state and action as input\nand the variation in the state as output of a deep neural networks based on data generated by a\nrandom policy. With this learned model, at every time step the agent plans the trajectory using\nmodel predictive control (MPC) [97, 98], performs one step of the trajectory and re-plans. Reward is\n12\nbased on how close the agent follows the trajectory commanded. This on-policy data is aggregated\nwith the initial data used to learn the model and the dynamics is retrained once in a while. On\ntheir experiments with a real-robot, the model-based approach performs worse than a model-free\nbenchmark. To solve this issues, the authors ﬁne-tune the MPC controller together with learned\ndynamics using model-free RL. First, the agent’s policy is trained to replicate the MPC controller\n(still mixes on- and off-policy data to re-train policy and dynamic model). At the ﬁnal stage this\npolicy is transferred to a actor-only algorithm, for example TRPO, and the training continues. This\napproach outperforms the model-based or model-free alone.\nAnother approach for leveraging simulated rollouts is proposed by Ha and Schmidhuber [99].\nOn this work, the agent learns on a unsupervised manner, from random rollouts, a compressed\nrepresentation of the environment using variational autoencoders (VAE) [100, 101], mixture density\nnetworks (MDN) [102, 2], and recurrent neural networks (RNN) [103, 104, 24], and use this\nlearned model to update its policy which is later performs in the real environment. This research is\ninspired in human factors research that suggests that human brains only remembers certain features\nand what humans perceive depends on brain’s internal model for future prediction. The models\nlearned are mostly imperfect but the authors uses a weighting parameter to quantify the amount of\nuncertainty. The policy is trained using Covariance-Matrix Adaptation Evolution Strategy (CMA-\nES) [105, 106] using multiple CPU cores to run multiple copies of the environment to parallelize\nand accelerate learning. Another model-based approach for imperfect models by Racanière et al.\n[107] called Imagination-Augmented Agents (I2A) does not rely exclusively on simulated returns.\nTheir approach still learns a forward model but converts trajectories to embeddings, abstracting\nmodel imperfections.\nDynamic models can also be use to control exploration of RL environments. Pathak et al. [108]\nintroduces an intrinsic reward module that rewards the agent based on how well it is able to predict\nthe environment states. The better the prediction, lower the reward. This forces the agent to explore\nunexplored areas in the state space. In their approach, the agent creates a forward model based on\nlatent-space and also learns an inverse model to predict which actions were used to change states.\n13\nSimilar model-based approach was earlier proposed by Schmidhuber [109] where the agent builds\nthe model by actively provoking situation where it expects to learn something about the dynamics\nof the environment.\nOther approaches to increase sample-efﬁciency of RL algorithms include combining them with\nclassical controllers [110], adding auxiliary task and rewards [111], and learning from episodes\nwhere the goal is not reached [112]. In the work by Johannink et al. [110] the authors add actions\nproduced by the RL policy to actions of a hand-tuned classic controller. The majority of the task is\nsolved by the classic controller and the rest is ﬁne-tuned with RL. In their paper they use the TD3\nalgorithm to solve a block placing task with handcrafted dense reward function using simulated and\nreal robots, also transferring policies learned in simulation to real world. The state-space for the RL\nagent includes the goal of the task and action-space is the position of the end-effector. The addition\nof auxiliary tasks in the work by Jaderberg et al. [111] introduces the UNsupervised REinforcement\nand Auxiliary Learning (UNREAL) agent, which translates to adding two additional reward signals\n(in addition to the one returned by the environment): maximize change in pixel intensity (generally\ncorrelated to important events in the screen) and correct reward prediction based on stacked input\nframes. It does increase the sample-efﬁciency because the agent is able to extract more information\nfrom the same amount of data plus converts any tasks with sparse reward signals to dense, aiding\nthe optimization process. The main insight to learn from episodes where the goal is not reached\ncomes from Andrychowicz et al. [112] where the Hindsight Experience Replay (HER) algorithm is\nproposed. HER adds a goal vector to standard observation input for RL algorithms. At the end of\neach episode, it interprets the ﬁnal state as a pseudo-goal so even when the agent fails to achieve the\ndesired goal, it at least learns how to achieve this alternative state. It solves to problem of sparse\nrewards tasks with narrow successful regions where positive rewards are difﬁcult to achieve.\n1.2.2.3\nHuman-in-the-loop Reinforcement Learning\nReinforcement learning has been proven to work on scenarios with well-designed reward\nfunctions and easily available interactions with the environment [8]. However, in real-world robotic\napplications, explicit reward functions are non-existent, and interactions with the hardware are\n14\nexpensive and susceptible to catastrophic failures [8]. This motivates leveraging human interaction\nto supply this reward function and task knowledge, to reduce the amount of high-risk interactions\nwith the environment, and to safely shape the behavior of robotic agents [8].\nA common approach in human-in-the-loop reinforcement learning is modify the reinforcement\nlearning loss function to leverage a human dataset of trajectories to solve the desired task. Hester\net al. [113] presents the iconic Deep Q-learning from Demonstrations (DQfD) algorithm where the\nagent is pre-trained and trained with four combined losses: 1-step double Q-learning loss, n-step\ndouble Q-learning loss, supervised large margin classiﬁcation loss, and L2 regularization on network\nweights and biases. Authors argue that the combination of all four losses during pre-training is\nessential to learn a uniﬁed representation that is not destroyed when the loss function changes from\nthe pre-training to the training phase. Even after pre-training, the agent must continue using the\nexpert data [113]. They also modiﬁed the replay buffer to combine expert demonstrations and\nself-generated data (never overwriting the expert data) sampling proportional amounts of each\ntype. The authors shows that DQfD lead to more sample-efﬁcient approach to solve the ATARI\nbenchmarks.\nSimilar to DQfD but without the pre-training phase and the supervised loss in the loss function\nVecerík et al. [114] present the Deep Deterministic Policy Gradient from Demonstration (DDPGfD)\nalgorithm. It loads all expert demonstrations in a prioritized experience replay buffer before training,\nwhich is kept throughout the training process, and performs more than one learning step when\nsampling data from the buffers. The main contribution of this algorithm is that it can be used in\ncontinuous action-spaces, while DQfD is restricted to discrete ones. Another extension to DQfD\nproposed by Pohlen et al. [115], the Ape-X DQfD algorithm differs from DQfD in three aspects:\nno pre-training phase using only expert transitions, ﬁxed ratio of actor and expert transitions, and\nthe supervised loss is only applied to the best expert episode instead of all episodes. This was\nthe ﬁrst deep RL algorithm to solve the ﬁrst level of Montezuma’s Revenge, the ATARI game\nin which DQN had poor performance [74]. A pre-training strategy is also used by de la Cruz\net al. [116] feature and policy learning in RL. The authors tackle the feature learning during the\n15\npre-training phase with a combined supervised classiﬁcation loss, an unsupervised reconstruction\nloss, and a value function loss. They also use a transformed Bellman operator to scale the whole\naction-value function without clipping or modifying the reward signal. Another contribution is\ntheir self-imitation learning approach encourages the agent to imitate past decisions only when the\nreturns are larger than the current estimated value. Their approach is tested on ATARI games and\nshows improved sample-efﬁciency when compared to deep RL algorithms.\nBuilding on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay,\nNair et al. [117] shows that when an actor is pre-trained with demonstration off-policy data and\ntransferred to RL for training, the actor weights are destroyed by the untrained critic. Also, training\nthe critic with only off-policy data it fails to correctly evaluate on-policy transitions, which motivates\nthe combined loss approaches and mxiture between on- and off-policy data. Similarly, the approach\nby Sun et al. [118] combines expert and agent data in a mix of on- and off-policy updates to reduce\ndistribution mismatch between training and testing dataset and argue that interactive approaches\nleads to better performance through a reduction to no-regret online learning. This algorithm matches\nexpert performance and even surpass it in case the expert is not optimal. Multiple critics can also be\nused to constrain the actor’s policy, as showed by Durugkar et al. [119] with the Multi-Preference\nActor Critic (M-PAC) framework.\nMixing directly expert trajectories with RL, Schoettler et al. [120] extends the concept of\nResidual RL, where actions selected by the agent are added to actions performed by an expert\ncontroller, to the setting of vision-based manipulation tasks. The authors show that this approach\ncan be used to train agents directly in the real-world. Peng et al. [121] uses expert trajectories as\nreference to compute a reward function that penalizes mismatch between reference and performed\ntrajectories. Constructed this reward function, the agent can be trained with any RL algorithm.\nThe authors show that multiple references can be integrate into the learning process to develop\nmulti-skilled agents capable of performing multiple and combined skills.\nHumans can also aid RL algorithms through natural language. Kaplan et al. [122] trains an\nagent that learns the meaning of English words, translates them to game states, and learns how\n16\nto execute these commands. Game images are used to train a policy and state value function\nusing an state vector augmented with language instructions. The authors generate training data by\ntemplate matching the frames with the instructions, which is later used to verify if instructions were\ncompleted, giving the agent an additional reward. They also generated training data by playing the\ngame and leaving the agent at speciﬁc positions or performing speciﬁc actions to cover gaps in the\ndemonstration dataset. Similarly, Waytowich et al. [123] proposed a model that learns a mutual\nembedding between natural language commands and goal-states that can be used as a form of reward\nshaping and improve RL performance on environment with sparse rewards. Their approach is tested\non the SC2LE (StarCraft II Learning Environment) [124]. For dialogue applications, Jaques et al.\n[125] train a generative model on known sequences of interaction data that is later used as reference\nduring the RL training phase, penalizing divergence from this prior generative model with different\nforms of KL-control. Authors argue that this is necessary to remove the overestimation bias when\nagents are pre-trained directly with expert data.\nHumans be queried to provide feedback, called active learning, and train a reward model for RL\nagents. Daniel et al. [126] propose to use humans to evaluate the agent’s actions (assign numerical\nvalues to observed trajectories), instead of having them to design the reward function. Since human\nevaluation is noisy and non-repeatable, the authors also propose to learn a probabilistic model of the\nreward function using Gaussian processes and a noise hyperparameter. The reward model is learned\nby sampling from a memory buffer the best performing trajectories plus the trajectories evaluated\nby the human expert. When to query the expert is based on the ratio of the predictive variance\nand observation noise, called lambda. If lambda is above a certain threshold, the agent queries the\nhuman and uses this evaluation to update the reward model. The proposed approach is evaluated\nin a simulated environment and in a real robot for grasping tasks. Su et al. [127] use RL to train a\ndialogue policy at the same time as a reward model is trained based on human feedback. They also\nuse Gaussian processes to quantify uncertainty to reduce number of queries to the user. This allow\nfor online learning in real-world dialogue systems without manually annotated data. Lopes et al.\n[128] introduces active reward learning for inverse reinforcement learning where the agent queries\n17\nthe user for demostrations at speciﬁc areas of the state-space. It does that by measuring a state-wise\nentropy in the distribution and queries the user for the most informative states. Including elements\nof active learning, Hilleli and El-Yaniv [129] initially use human to generate expert trajectories and\ntrain a policy to imitate them. The human later labels the states in order to train a reward model\nand uses DDQN deep RL algorithm to optimize the reward, which learns to perform better than the\nhuman demonstrator.\nMore recent meta-learning approaches, as proposed by Zhou et al. [130], learns initially from\nfew demonstration and later from binary feedback. This approach works with two separate policies:\ntrial (learn from demonstrations) and retrial (learn from experience) policy. Trial policy is trained in\na meta-imitation learning setup and is frozen to generate data to the retrial policy, which is trained on\nthis stationary data. Huang et al. [131] address the one-shot imitation learning problem, where the\ngoal is to execute a previously unseen task based on only one demonstration. This is made possible\nby formulating the one-shot imitation learning as a symbolic planning and symbol grounding\nproblem together with relaxation of the discrete symbolic planner to plan on the probabilistic\noutputs of the symbol grounding model.\n1.2.2.4\nReinforcement Learning in Robotics and in the Real World\nIn robotics, it is often challenging to successfully integrate learning algorithms and con-\ntrollers. Valasek et al., Valasek et al. [132, 133], developed a novel architecture called Adaptive-\nReinforcement Learning Control (A-RLC) for morphing air vehicles. It deﬁnes optimal aircraft\nshape based on mission objectives, learns how to morph to the desired format, and controls variations\non system’s dynamics due to the new shape.\nReinforcement learning methods often fail in dynamical systems due to failure in properly rep-\nresenting the problem, inaccurate function approximation, and not accounting for time dependency\nin the selection of actions, as pointed by Kirkpatrick and Valasek [134]. Controlling real continuous\nsystems requires computer-based control, so sampling of the continuous system is necessary [134].\nIf necessary to discretize these continuous systems, the size of the grid must be tuned to capture\nenvironment details and still achieve good convergence, as presented by Lampton et al. [135].\n18\nOften applied to simulated tasks and environments, RL algorithms need to overcome their\nextensive trial-and-error approach before they can be applied to real robotic systems. Kober et al.\n[136], in their extensive survey of reinforcement learning applied to robotics, highlighted several\nopen questions that need to be answered in order to have RL techniques reliably deployed to robotic\nsystems, for example: 1) How to choose the best system representation in terms of approximate\nstates, value functions, and policies, when dealing with continuous and multi-dimensional robotics\nproblems? 2) How to generate reward functions straight from data, reducing the need for manually-\nengineered solutions? 3) How to incorporate prior knowledge and how much is needed to make\nrobotics reinforcement learning problems tractable? 4) How to deal with parameter sensitivity\nand reduce the need for preprocessing sensory data, simplifying behavior programming? 5) How\nto approach model error and under-modeling during simulated training — when the simulated\nenvironment cannot perfectly model every aspect of the real environment, since the policies learned\nonly in simulation frequently cannot be transferred directly to the robot? 6) How to develop accurate\nsimulation training environments which will ease transfer of policies learned from simulation to\nreal-world hardware tasks?\nAmodei et al. [137] addressed concrete risks of poorly-design AI systems deployed to real-world\napplications. Speciﬁcally in RL, reward hacking and safe exploration are the main issues. Reward\nhacking refers to developing unintended behavior by exploiting a faulty reward function. For\nexample, a running robot that is rewarded according to its speed could learn to run in circles at\nmaximum speed instead of completing the running course. Safe exploration refers to developing\nunsafe behavior (e.g., behavior that would damage the agent or its environment) while exploring the\nobservation space. These problems are currently unsolved.\nThere is also a research gap on trust on autonomous systems. Prior studies [138] indicate that\nhumans have low levels of trust in semi and fully autonomous systems. This is directly correlated\nwith the unpredictability of the robot [139], their rate of success and reliability [140, 141, 142], and\nhow knowledgeable the user is about an autonomous system [143] and the task to be completed\n[144]. This highlights the need to develop a platform to illustrate what future actions are being\n19\nplanned by the intelligent agent or any other form of education to demystify the intelligent behavior.\nThis would improve human trust in these systems and allow humans to supervise future actions and\nconsequences.\n1.3\nMotivation and Objective\n1.3.1\nMotivation\nAs discussed on Sections 1.2.1 and 1.2.2, current state-of-the-art research on learning algorithms\nfocuses on end-to-end approaches. The learning agent is initialized with no previous knowledge\nof the task nor the environment and the action selection process (trial-and-error) develops almost\nrandomly. End-to-end approaches abstract intermediate learning milestones at the cost of high\nnumber of interactions with the environment. An important question is how RL agents could\nincorporate prior knowledge or learn directly from a trained policy. For example, incorporate human\nknowledge or learn from an existing sub-optimal controller. Humans are preferred because they\ndictate the task to be accomplished and have traits that are in the frontier of learning algorithms\nresearch, as for example long-term autonomy [145] and continual lifelong learning [146].\nCurrently, there is no deﬁnite approach to combining humans and RL agents. Machines that\ncan be taught by and seamlessly work in collaboration with humans will not only learn faster, but\nwill potentially expand the frontier of having machines augmenting human performance. The rapid\nincrease of automation and development of AI techniques demands a better understanding of how\nhumans and machines can better work in collaboration.\n1.3.2\nObjective\nThe objective of this research work is to develop the theoretical foundation to integrate multiple\nhuman input modalities (in the form of demonstration, intervention, and evaluation) to allow\nintelligent data-driven agents to learn in real-time, safely, and with fewer samples (increase sample-\nefﬁciency) by bootstrapping human interaction data and reinforcement learning theory.\n20\n1.4\nResearch Contributions\nThis dissertation investigates how to efﬁciently and safely train autonomous systems in real-time\nby extending supervised and reinforcement learning theories and human input modalities. Intuitively\nnamed the Cycle-of-Learning (CoL), this research investigates and develops training autonomous\nsystem policies through human interaction that is based on the evidence-based teaching methods\nof “teaching for mastery\" [147]. On teaching for mastery, or mastery learning [147, 148, 149],\nhuman interaction is provided systematically, from complete human control (and no autonomy)\nwhen learning from demonstrations to no human control (and full autonomy) at the reinforcement\nlearning stage. After learning from demonstration the policy is evaluated in real-time and given\nfeedback only on speciﬁc areas that needs to improve through interventions. After mastering an\ninitial concept, the role of the human is only to evaluate the performance of the policy while it\ndevelops by itself using reinforcement learning. This process is repeated as new concepts are\nintroduced and the policy masters previous concepts, as illustrated in Figure 1.1.\nWhile extensive research has been conducted into each of these stages separately in the context\nof machine learning and robotics, a complete theoretical integration of these concepts has yet to be\ndone. This theoretical integration will be important to ﬁelding adaptable autonomous systems that\ncan be trained in real-time to perform new behaviors depending on the task at hand, in a manner\nthat does not require expert programming.\nThe contributions of the research are:\n1. A novel theoretical contribution of incorporating multiple human input modalities to rein-\nforcement learning algorithms to perform, simultaneously, on-policy and off-policy learning,\nleveraging their strengths and mitigating their weaknesses.\n2. Reduction of the data-complexity and the number of samples (interactions with the environ-\nment) required by state-of-the-art reinforcement learning algorithms to achieve satisfactory\nperformance.\n3. Enabling reinforcement learning agents to be trained in real-time, with real-world data, safely.\n21\nFigure 1.1: Cycle-of-Learning for Autonomous Systems from Human Interaction: as the policy de-\nvelops, the autonomy independence increases while the human interaction level decreases. Adapted\nfrom [1].\nResults presented in this dissertation show that policies trained with human demonstrations\nand human interventions together outperform policies trained with just human demonstrations\nwhile simultaneously using fewer data samples. To the best of knowledge, this is the ﬁrst result\nshowing that training a policy with a speciﬁc sequence of human interactions (demonstrations, then\ninterventions) outperforms training a policy with just human demonstrations (controlling for the\ntotal amount of time of human interactions). One can obtain this performance with signiﬁcantly\nreduced data requirements, providing initial evidence that the role of the human should adapt during\nthe training of safe autonomous systems.\n1.5\nOrganization\nThis dissertation is organized as follows. Chapter 2 details Behavior Cloning and Imitation\nLearning in discrete or continuous spaces and how these algorithms can be used to replicate human\nbehavior. Chapter 3 formalizes the Reinforcement Learning problem as a Markov Decision Process\nand explains how this research ﬁeld transitioned from shallow to deep representations for policy\nand value functions when using function approximation. This chapter also includes an unusual\napplication example: Deep Reinforcement Learning on Intelligent Motion Video Guidance for\n22\nUnmanned Air System Ground Target Tracking. Chapters 4 and 5 presents to case studies of Deep\nReinforcement Learning applied to Aerospace problems: the tracking of a ground target using\nunmanned aerial systems and learning to control morphing wings. Chapter 6 initializes the human-\nin-the-loop work with a case study leveraging Inverse Reinforcement Learning methods to land a\nsimulated lander and simulated unmanned aerial vehicle by learning from human demonstrations of\nthe task. Chapter 7 addresses how to leverage initial human demonstrations to learn the intrinsic\nreward function used by the human to pursue the task goal while performing the demonstrations.\nThis reward signal is then used to feed reinforcement learning algorithms in a unmanned aerial\nsystem collision-avoidance scenario. Chapter 8 introduces the main theoretical contribution of\nthis dissertation: Cycle-of-Learning for Autonomous Systems from Human Interaction. It covers\nhow to combine multiple forms of human interaction and integrate them to reinforcement learning.\nChapter 9 explains how to efﬁciently combine human demonstrations and interventions on learning\nalgorithms to increase task performance while using fewer human samples. The main study case\nis learning from human interaction autonomous landing controllers for unmanned aerial systems.\nChapter 10 ﬁnalizes the Cycle-of-Learning by transitioning from policies learned from human\ninteraction to reinforcement learning. This approach is validated on the previous unmanned\naerial system landing scenario and standard reinforcement learning tasks for continuous control.\nConclusions from the various applications and recommendations for future work are summarized in\nChapter 11.\n23\n2.\nMACHINE LEARNING AND CLONING BEHAVIORS\nThis chapter introduces the notation and basics of machine learning, speciﬁcally supervised\nlearning, and how an algorithms can learn to clone behaviors by framing the learning problem as a\nsequential decision making problem. This includes important concepts in machine learning, the\nmathematical formulation of sequential decision making problem framed as a Markov Decision\nProcess, the relation between imitation learning and behavior cloning, success cases in the literature,\nand the limitations of this approach in isolation.\n2.1\nProblem Deﬁnition\nAccording to Barber [150], Machine Learning is the research ﬁeld concerned in automating\nlarge-scale data analysis, leveraging concepts of statistics with focus on mathematical modeling\nand prediction. The dataset that one uses to learn from to create this mathematical model, or train\nthe model, is called the training set [2]. Ideally, the model trained on this training set will be\nable to generalize to new unseen inputs, often called the test set [2]. The process of training the\nmodel depends on how this training set is structured, which further subdivides the ﬁeld of machine\nlearning in three groups: supervised learning, unsupervised learning, and reinforcement learning.\nIn supervised learning, the training set contains data inputs and desired outputs so the learning\nmodel changes its internal parameters during training in order to produce outputs closer to the\ndesired ones given the same input data. For example, a machine learning model for autonomous\nvehicles would be interested in identifying trafﬁc signs and lights from image data so the vehicle\ncan behave according to the trafﬁc rules. The training set would consist in multiple images as input\nand the location of all trafﬁc signs and lights in the image as output. In unsupervised learning, the\ntraining set consists only of input data and no desired output. The main goal is to discover groups\nin similar category (clustering), estimate the input distribution (density estimation), project data\nin different dimensions to aid visualization [2]. An example of unsupervised learning would be a\nmachine learning model that is trained to cluster customers of a bank based on their credit card\n24\ntransaction history. In Reinforcement Learning there is no training set and machine learning model\nis trained by trial-and-error while performing a desired task in order to maximize a performance\nmetric. A example of reinforcement learning would be a machine learning model that receives\nimages from a screen while playing a computer game and aims to maximize the game score by\nperforming different commands and observing its effect in the next game screens and score.\nSupervised learning is covered with more details on Section 2.4 and Reinforcement Learning on\nChapter 3 of this research work. Unsupervised learning is out of scope of this dissertation however\nintroductory material in the area can be found in Barber [150] and Bishop [2].\n2.2\nSupervised Learning\nBarber [150] deﬁnes supervised learning problem as, given a set of data pairs\nD = {(xn, yn), n = 1, . . . , N},\nlearning the relationship between the input x and output y such that, given an unseen input x′ not\npresent in D the predicted y′ output is accurate, assuming the pair (x′, y′) is generated by the same\nunknown process that generated the set D. In other words, it is desired to predict y′ conditioned\non know x′ and the set D, which can be represented by the conditional distribution p(y′|x, D). If\nthe output y is one of a discrete number representing a possible discrete outcome, the supervised\nlearning problem is called “classiﬁcation” (for example, given computed tomography (CT) x-ray\nimages of the lung, predicts if the person is likely or not to have cancer [151]). If the output y is a\ncontinuous variable, the supervised learning problem is called “regression” (for example, predicting\nfuture market price of a given stock based on ﬁnancial news articles [152]). This distinction is\nimportant to deﬁne the loss function used to compute the misﬁt between true and predicted model,\nwhich guides the optimization process used to learn the model, to be discussed in more detail\nthroughout the next sections.\n25\n2.2.1\nModel Representation and Learning\nThe word model has not been properly deﬁned in this work yet. The model is the mathematical\nrepresentation of the underlying function or distribution it is desired to approximate, from which\nthe training set is sample from, in order to generalize to unseen data samples and predict future\noutcomes. Models differ by the number and the mathematical relationship between its parameters,\nwhich should be chosen based on the underlying function the model is desired to approximate and\nassumptions made. Examples of these assumptions are if the underlying process that generated the\ntraining set is linear or nonlinear, noise processes that affect the measurements, and others.\nAs explained by Bishop [2], an example of model representation would be a polynomial function\nof the form\ny(x, w) = w0 + w1x + w2x2 + · · · + wMxM =\nM\nX\nj=0\nwjxj,\n(2.1)\nwhere M is the order of the polynomial and its coefﬁcients w0, . . . , wM are collectively denoted\nby the vector w. Even though there is a linear relation between the polynomial coefﬁcients w, the\nmodel y(x, w) is nonlinear and, consequently, is able to approximate nonlinear functions. The main\nlimitation of models comprised of linear combination of ﬁxed basis is that they do not scale well as\nthe number of inputs x increases, so called “curse of dimensionality” [153, 2]. For example, for the\npolynomial case of order M with N inputs, the number of coefﬁcients grow following the power\nlaw N M.\n2.2.1.1\nNeural Networks\nOne of the model representations that counters these limitations are neural networks [2, 154,\n155], also called feedforward neural networks, or multilayer perceptrons (MLPs) [155]. Neural\nnetworks are said to be universal approximators due to their approximation properties, which\nare able to approximate any function given enough network size [156, 157, 158, 159, 160, 161,\n162, 163, 164]. Originally inspired by biological neural networks [165, 166, 167, 168], neural\nnetworks are a series of functional transformations where a linear combination of parameters w are\n26\ntransformed for nonlinear activation functions h(·) in cascade along layers, which consists of many\nunits that act in parallel representing a vector-to-scalar function [155]. A neural network with D\ninputs, one hidden layer with M units, and K outputs, as seen in Figure 2.1, is be represented by\nthe equation\nyk(x, w) = h2\n M\nX\nj=1\nw(2)\nkj h1\n D\nX\ni=1\nw(1)\nji xi + w(1)\nj0\n!\n+ w(1)\nk0\n!\n.\n(2.2)\nFigure 2.1: A neural network with D inputs, one hidden layer with M units, and K outputs. Adapted\nfrom [2].\nThe activation function h(·) plays an important role in neural networks since it gives nonlinear\nproperties while propagating the inputs and also affects how the the gradients are back-propagated\nduring training. Example of the most common activation functions are: Linear (Figure 2.2), Sigmoid\n(Figure 2.3), Hyperbolic Tangent (Tanh, Figure 2.4), Rectiﬁed Linear Unit (ReLU, Figure 2.5),\nLeaky Rectiﬁed Linear Unit (Leaky ReLU, Figure 2.6), Exponential Linear Unit (Figure 2.7),\nand Softplus (Figure 2.8). Table 2.1 details the equations and their derivatives and Table 2.2\ndetails advantages and disadvantages for the select of activation functions commonly used in neural\nnetworks [169].\n27\nTable 2.1: Select activation function equations and their derivatives.\nName\nEquation\nDerivative\nLinear (Fig. 2.2)\nh(x) = x\nh′(x) = 1\nSigmoid (Fig. 2.3)\nh(x) =\n1\n1 + e−x\nh′(x) = h(x)(1 −h(x))\nTanh (Fig. 2.4)\nh(x) =\n2\n1 + e−2x −1\nh′(x) = 1 −h(x)2\nReLU (Fig. 2.5)\nh(x) =\n(\n0,\nfor x < 0.\nx,\nfor x ≥0.\nh′(x) =\n(\n0,\nfor x < 0.\n1,\nfor x ≥0.\nLeaky ReLU (Fig. 2.6)\nh(x) =\n(\nαx,\nfor x < 0.\nx,\nfor x ≥0.\nh′(x) =\n(\nα,\nfor x < 0.\n1,\nfor x ≥0.\nELU (Fig. 2.7)\nh(x) =\n(\nα(ex −1),\nfor x < 0.\nx,\nfor x ≥0.\nh′(x) =\n(\nαex,\nfor x < 0.\n1,\nfor x ≥0.\nSoftplus (Fig. 2.8)\nh(x) = ln(1 + ex)\nh′(x) =\n1\n1 + e−x\n28\nTable 2.2: Select activation function advantages and disadvantages.\nName\nAdvantages\nDisadvantages\nLinear\nWide range of outputs, useful\nDoes not add nonlinearities to inputs,\nwhen scaling is not desired.\nonly a linear transformation.\nConstant derivative, so gradient is not\ndependent on input when back-propagated.\nSigmoid\nBounded output [0,1].\nSaturates and kills gradients due to\nSmooth derivative.\nﬂat derivative towards the tail.\nOutputs not zero centered, changing\ndynamics of gradient descent.\nTanh\nBounded output [-1,1].\nSaturates and kills gradients due to\nSmooth derivative.\nﬂat derivative towards the tail.\nReLU\nSimple implementation and\nLarge gradients can shift weights\ninexpensive operation.\nin a way that disables neuron\nShown to accelerate learning\nunits permanently (never activate),\non vision-based tasks [170].\nthe “dead neuron” problem.\nLeaky ReLU\nAttempts to correct the ReLU\nLarge gradients can shift weights\n“dead neuron” problem.\nin a way that disables neuron\nSimple implementation and\nunits permanently (never activate).\ninexpensive operation.\nELU\nAttempts to correct the ReLU\nOutput not bounded for x > 0.\n“dead neuron” problem.\nCan output negative values.\nStrong alternative to ReLU.\nSoftplus\nSimilar to ReLU, but\nOperation not as\nwith smooth gradient.\ninexpensive as ReLU.\nnear zero.\n29\nFigure 2.2: Linear activation function y(x) and its derivative y′(x).\nFigure 2.3: Sigmoid activation function y(x) and its derivative y′(x).\n2.2.1.2\nLearning in Discrete and Continuous Spaces\nLearning in neural networks occurs by systematically changing the network weights of each\nlayer in order to drive the predicted output ˆyi closer to the target value or label yi for the same\ninput sample. This is measured by a cost function, generally represented by L(·), which is different\nfor discrete spaces (classiﬁcation) in continuous (regression) cases. Common cost function for\nclassiﬁcation are Cross-Entropy and Hinge loss while for regression we have Mean Squared Error\n30\nFigure 2.4: Hyperbolic Tangent (Tanh) activation function y(x) and its derivative y′(x).\nFigure 2.5: Rectiﬁed Linear Unit (ReLU) activation function y(x) and its derivative y′(x).\n(MSE), Mean Absolute Error (MAE), and Huber loss. The Cross-Entropy (CE) loss, written as\nLCE(yi, ˆyi) = −1\nN\nN\nX\ni\nC\nX\nj=1\nyij log(ˆyij),\nis used for classiﬁcation with C classes. Binary Cross-Entropy (BCE) loss, also known as Log loss,\nis the special case of the CE loss when the number of classes C is equal to 2 (binary classiﬁcation\n31\nFigure 2.6: Leaky Rectiﬁed Linear Unit (Leaky ReLU) activation function y(x) and its derivative\ny′(x).\nFigure 2.7: Exponential Linear Unit (ELU) activation function y(x) and its derivative y′(x).\nproblem) and is written as\nLBCE(yi, ˆyi) = −1\nN\nN\nX\ni\nC=2\nX\nj=1\nyij log(ˆyij) = −yi1 log(ˆyi1) −(1 −yi1) log(1 −ˆyi1).\n32\nFigure 2.8: Softplus activation function y(x) and its derivative y′(x).\nHinge loss, often used for maximum-margin classiﬁcation in Support Vector Machines (SVM)\n[150], is written as\nLHinge(yi, ˆyi) = 1\nN\nN\nX\ni\nmax(0, 1 −yiˆyi).\nFor regression problems, Mean Squared Error (MSE) loss computes the average of the square of\nthe errors between predicted ˆyi and true yi value, which results in an arithmetic mean-unbiased\nestimator:\nLMSE(yi, ˆyi) = 1\nN\nN\nX\ni\n(yi −ˆyi)2.\nThe Mean Absolute Error (MAE) loss, similar to MSE, computes the average of the absolute errors\nbetween predicted ˆyi and true yi value, which results in an median-unbiased estimator:\nLMAE(yi, ˆyi) = 1\nN\nN\nX\ni\n|yi −ˆyi|.\nHuber loss combines the nonlinearity of MSE and linearity of MAE in a single loss function,\ncontrolled by the hyperparameter δ, less sensitive to high magnitude error that would normally be\n33\nampliﬁed by MSE loss. The Huber loss is written as\nLHuber(y, ˆy) =\n\n\n\n\n\n\n\n1\n2(y −ˆy)2,\nfor |y −ˆy| ≤δ.\nδ|y −ˆy| −1\n2δ2,\notherwise.\nDeﬁned a cost function, the neural network weights are updated (also called “trained”) using the\nbackpropagation algorithm. In backpropagation each weight parameter is computed based on their\nproportional contribution to the cost for the given input x using the chain rule for partial derivatives\nand gradient descent updates. For example, in the case of the neural network presented in Figure 2.1\nin a regression task, the cost or loss would be given by\nL =\nK\nX\nj=1\n(a(L)\nj\n−yj)2,\nwhere L is the index of the hidden layer, K the number of output neurons, and aj the activation\noutput of the j neuron in the layer. Explicitly, for the case we have three neurons and one bias in the\nhidden layer, aj is represented as\na(L)\nj\n= h(w(L)\n10 a(L−1)\n1\n+ w(L)\n20 a(L−1)\n2\n+ w(L)\n30 a(L−1)\n3\n+ b(L)\nj ),\nwhere h(·) is a, ideally nonlinear, activation function. Using the chain rule, the loss contribution\nof each previous activation output can be written as\n∂L\n∂a(L−1)\nk\n=\nK\nX\nj=1\n∂a(L)\nj\n∂a(L−1)\nk\n∂L\n∂a(L)\nj\n.\nThis process is repeated for every previous layer until we reach the input layer and compute ∂L\n∂xi\nand for each weight component\n∂L\n∂wij along the connection path. Each weight component at time t,\ndenoted wt\nij, is then updated with gradient descent as\nwt+1\nij\n= wt\nij −α ∂L\n∂wt\nij\n,\n34\nwhere α is a step size hyperparemeter.\n2.3\nSequential Decision Making Problems\nMathematically and similarly to any sequential decision making process, in its most general\nform, the RL problem is formalized as a Partially Observable Markov Decision Process (POMDP)\n[171]. A POMDP M = {S, O, E, A, T , r} is characterize by its state-space S (where a vector\nof states s ∈S), an observation-space O (where a vector of observations o ∈O), an emission\nprobability E that controls the probability of observing o conditioned to the underlying states s,\nan action-space A (where a vector of actions a ∈A), a transition operator T (which deﬁnes the\nprobability distribution p(st+1|st)), and the reward function r : S × A →R (or r(s, a)), as showed\nin Figure 2.9 of the previous chapter. In Fully Observable Markov Decision Process, or simply a\nMarkov Decision Process (MDP), it is assumed full knowledge of the underlying states, removing\nthe dependency to the observations and simplifying the process to M = {S, A, T , r}.\nFigure 2.9: Partially-Observable Markov Decision Process diagram. Adapted from [3].\n2.4\nCloning Behaviors with Imitation Learning\nAs discussed in Section 1.2.1.1, Imitation Learning provides a directed path to learn demon-\nstrated behaviors by utilizing examples of a secondary policy performing the task, quickly converg-\ning to more stable behaviors. This secondary policy can be another agent trained with reinforcement\nlearning, a human, a classical controllers, or any other entity that is able to attempt to solve the\ndesired task.\nBehavior Cloning (BC) is the subset of Imitation Learning where the demonstration data is\ndirectly used to clone the demonstrated behavior by training a model to generate actions as similar\n35\nas possible to the demonstrated data, given similar inputs. In BC, a policy π is trained in order to\ngeneralize over a subset D of states and actions visited during a task demonstration over T time\nsteps:\nD = {a0, s0, a1, s1, ..., aT, sT} .\nThis demonstration can be performed by a human supervisor, optimal controller, or virtually\nany other pre-trained policy. In the case of human demonstrations, the human is implicitly trying to\nmaximize what may be represented as an internal reward function for a given task (Equation 2.3),\nwhere π∗(a∗\nt|st) represents the optimal policy that is not necessarily known, in which the optimal\naction a∗is taken at state s for every time step t.\nmax\na0,...,aT\nT\nX\nt=0\nrt(st, at) =\nT\nX\nt=0\nlog p(π∗(a∗\nt|st))\n(2.3)\nDeﬁning the policy of the supervisor as πsup and its estimate as ˆπsup, behavior cloning can be\nachieved through standard supervised learning, where the parameters θ of a policy πθ are trained in\norder to minimize a loss function, such as mean squared error, as shown in Equation 2.4. Behavior\ncloning can be seen as supervised learning applied to sequential decision making problems.\nˆπsup = argmin\nπθ\nT\nX\nt=0\n||πθ(st) −at||2\n(2.4)\n36\n3.\nFROM SHALLOW TO DEEP REINFORCEMENT LEARNING\nThis chapter introduces the notation and deﬁnes the Reinforcement Learning (RL) problem,\nincluding its mathematical formulation as a Markov Decision Process, types of Reinforcement\nLearning algorithms, and the transition from “Shallow\" to “Deep\" Reinforcement Learning. Re-\ninforcement Learning is very closely related to the theory of classical optimal control, dynamic\nprogramming, stochastic programming, and optimization [172]. However, while optimal control\nassumes perfect knowledge of the system’s model, RL operates based on performance metrics\nreturned as consequence of interactions with an unknown environment [136].\n3.1\nPartially and Fully Observable Markov Decision Processes\nMathematically and similarly to the supervised learnig problem presented on Chapter 2, in\nits most general form, the RL problem is formalized as a Partially Observable Markov Decision\nProcess (POMDP) [171]. A POMDP M = {S, O, E, A, T , r} is characterize by its state-space S\n(where a vector of states s ∈S), an observation-space O (where a vector of observations o ∈O),\nan emission probability E that controls the probability of observing o conditioned to the underlying\nstates s, an action-space A (where a vector of actions a ∈A), a transition operator T (which deﬁnes\nthe probability distribution p(st+1|st)), and the reward function r : S × A →R (or r(s, a)), as\nshowed in Figure 2.9 of Chapter 2. In Fully Observable Markov Decision Process, or simply a\nMarkov Decision Process (MDP), it is assumed full knowledge of the underlying states, removing\nthe dependency to the observations and simplifying the process to M = {S, A, T , r}.\n3.2\nThe Reinforcement Learning Problem\n3.2.1\nProblem Deﬁnition\nIn Reinforcement Learning (RL), it is desired to train an agent to learn the parameters θ of a\npolicy (or controller) πθ, in order to map the partially-observable environment’s observation vector\no (or state vector s in fully-observable environments) to agent actions a. The performance of the\nagent is measured by a scalar reward signal r returned by the environment (external rewards, re)\n37\nand/or returned by the agent itself (intrinsic rewards, ri). At each time step t the reward signal can\nbe computed as the sum of all the extrinsic and intrinsic rewards received rt = ret + rit. Figure 3.1\nillustrates this description as a diagram.\nFigure 3.1: Reinforcement learning problem modeled as agent and environment interactions.\nThis interaction between agent and environment occurs during an episode, which is deﬁned as\nthe time interval between the initial time step t0 and the maximum number of time steps allowed\nT or until a previously established metric is achieved. At each time step t of a ﬁnite time horizon\nT, the policy (or controller) π, parametrized by θt, maps the current environment’s states st to\nactions at: πθ(at|st). This action affects the current environment’s states st which evolves to st+1\nbased on the environment’s transition distribution (dynamics) p(st+1|st, at). During an episode, the\nsequence of states observed and actions taken over a number of time steps T can be represented by\na trajectory τ = {s0, a0, s1, a1, . . . , sT, aT}. The total reward per episode R is deﬁned as the sum of\nthe rewards received for each time step, as shown in Equation 7.2.\nR =\nT\nX\nt=0\nrt =\nT\nX\nt=0\n(ret + rit) .\n(3.1)\nSimilarly, the expected total reward per episode received by a policy πθ(at|st) can be deﬁned by\nEquation 7.3.\nRπθ =\nT\nX\nt=0\nE\nat∼πθ[rt(st, at)].\n(3.2)\n38\nTo simplify further derivation of the algorithm it is assumed a fully-observable environment,\nwhere the observation-space O = S (state-space) and, consequently, vector of observations o = s\n(states).\nAt each time step t of a ﬁnite time horizon T, the policy (or controller) π, parametrized by θt,\nmaps the current environment’s states st to actions at: πθ(at|st). This action affects the current\nenvironment’s states st which evolves to st+1 based on the environment’s transition distribution\n(dynamics) p(st+1|st, at). The environment also returns a scalar reward function r that evaluates the\naction taken at at the state st: r(st, at).\nThe probability of experiencing a given trajectory τ (sequence of state s and action a pairs) in a\nMarkov Decision process can be written as:\nπθ(τ) = pθ(s0, a0, s1, a1, . . . , sT, aT)\n(3.3)\n= p(s1)\nTY\nt=1\nπθ(at|st)p(st+1|st, at).\n(3.4)\nThe goal in RL is to ﬁnd the parameters θ∗that will maximize the objective J(θ)\nJ(θ) =\nE\nτ∼pθ(τ)\n\" T\nX\nt=1\nr(st, at)\n#\n(3.5)\n=\nE\nτ∼πθ(τ)[r(τ)] =\nZ\n(πθ(τ)r(τ)dτ),\n(3.6)\nwhich represents the expected total reward to be received by this policy πθ(τ):\nθ∗= argmax\nθ\nJ(θ)\n(3.7)\n= argmax\nθ\nE\nτ∼pθ(τ)\n\" T\nX\nt=1\nr(st, at)\n#\n.\n(3.8)\nTo maximize J(θ), it is possible to apply the gradient operator ∇θ and compute its gradient\n39\nwith respect to its parameters θ:\n∇θJ(θ) =\nZ\n(∇θπθ(τ)r(τ)dτ).\n(3.9)\nUsing the following identity\nπθ(τ)∇θ log πθ(τ) = πθ(τ)∇θπθ(τ)\nπθ(τ)\n= ∇θπθ(τ)\n(3.10)\non Equation 3.9 it is possible to simplify it to\n∇θJ(θ) =\nZ\n(πθ(τ)∇θ log πθ(τ)r(τ)dτ)\n(3.11)\n∇θJ(θ) =\nE\nτ∼πθ(τ)[∇θ log πθ(τ)r(τ)],\n(3.12)\nknown as the policy gradient, which, by substituting τ, can also be written in terms of states and\nactions over time steps t in a given time horizon T:\n∇θJ(θ) =\nE\nτ∼πθ(τ)\n\" T\nX\nt=1\n∇θ log πθ(at|st)\nT\nX\nt=1\nr(at, st)\n#\n.\n(3.13)\nSince the structure of the objective J(θ) and its gradient ∇θJ(θ) are unknown, in practice the\nonly way to evaluate them and approximate the expectation term is by sampling and averaging over\nN samples:\n∇θJ(θ) ≈1\nN\nN\nX\ni=0\n\" T\nX\nt=0\n∇θ log πθ(a(i)\nt |s(i)\nt )\n!  T\nX\nt=0\nr(a(i)\nt , s(i)\nt )\n!#\n.\n(3.14)\nThe policy is improved by gradient ascent according to a step size α, as shown in Equation 4.13\nand Figure 3.2, optimizing the agent’s policy during the training time in what is called the vanilla\npolicy gradient (VPG) in RL.\n40\nθ →θ + α∇θJ(θ)\n(3.15)\nFigure 3.2: Policy evaluation and optimization cycle.\n3.2.2\nTypes of Reinforcement Learning Algorithms\nAs shown in the previous section, policy gradient methods in RL directly differentiate the\nobjective function, which is to maximize the expected sum of rewards, and improve the policy by\nfollowing gradient ascent. Other types of RL algorithms directly estimate a value function for a\npolicy π of a particular state V π(s), representing the expected sum of rewards to be received if\nstarting at that state s, written as\nV π(s) = E\nπ[Rt|st = s],\nor the value function of a particular state-action pair Qπ(s, a), representing the expected sum of\nrewards to be received if starting at that state s and taking the action a [153], written as\nQπ(s, a) = E\nπ[Rt|st = s, at = a].\nOne can also deﬁne an advantage function A(s, a) as\nAπ(s, a) = Qπ(s, a) −V π(s)\n41\nto quantify the advantage of taking a particular action a at the state s when compared to the\naverage value of that state [173, 59]. Bellman [174, 175] showed that in Markovian Decision\nProcesses there is a relationship between the value of a state and the value of it successor states\n[153]. After averaging and weighting all probabilities, Bellman showed that “the value of the start\nstate must equal the (discounted) value of the expexted next state, plus the reward expected along\nthe way”[153]:\nV π(st) = E\nπ[rt + γV π(st+1)].\nWe can remove the dependency to the policy π and show that, under the optimal policy, the value of\nthe state V ∗(st) is equal the expected discount sum of rewards for the best action when the agent is\nin that state, know as the Bellman Optimality Equation [153]:\nV ∗(st) = max\na\nQπ∗(st, at)\n= max\na\nE[rt + γV ∗(st+1)].\nSimilarly, the Bellman Optimality Equation can be written for Q∗(st, at) as\nQ∗(st, at) = E[rt + γ max\na′\nQ∗(st+1, a′)],\nwhere a′ is the action to be taken at state st+1.\nBased on the Bellman equations, other RL algorithms can be derived. A classical example\nis Q-learning, by Watkins [176]. In Q-learning, by interacting with the environment, the agent\ncontinuously estimates the state-action value Q(st, at) as\nQ(st, at) ←Q(st, at) + α[rt + γ max\na′\nQ(st+1, at) −Q(st, at)],\nwhere α is a learning rate hyperparameter.\nAnother type of RL algorithms called actor-critic methods combine both policy gradient and\n42\nvalue function concepts. In Section 3.2.1 it was shown that, in policy gradient methods, the gradient\nof the objective can be written as\n∇θJ(θ) =\nE\nτ∼πθ(τ)\n\" T\nX\nt=1\n∇θ log πθ(at|st)\nT\nX\nt=1\nr(at, st)\n#\n.\n(3.16)\nNote that the term PT\nt=1 r(at, st) is unknown until the end of the episode or a given number of time\nsteps T, however it can be estimated by the value function ˆQπ\nt as\n∇θJ(θ) =\nE\nτ∼πθ(τ)\n\" T\nX\nt=1\n∇θ log πθ(at|st) ˆQπ\nt\n#\n(3.17)\nand work as a critic in order to evaluate the actions taken by policy π, the actor. Hence, actor-critic\nalgorithms incorporate value learning by updating the estimates of its value function and using it to\ncompute the gradient of the policy.\nPolicy gradient, value learning, and actor-critic are also classiﬁed as model-free algorithms\nbecause they assume the dynamic model of the environment is unknown to the agent. Other\nalgorithms, called model-based, leverage interaction data between agent and environment to learn\nthe dynamics of the environment (or are given the model), which could be use for planning [96],\nperform policy updates using simulated data [99], and others.\n3.3\nFrom Shallow to Deep Representations\nAs a side note, in Deep RL where deep neural networks are used as learning representation for\nthe policies, a typical policy is represented by a neural network with two fully-connected layers\nwith 64 neurons each (plus a bias term). If the dimension of the input is N and the dimension of the\noutput is M, this leads to (N + 1) ∗64 + (64 + 1) ∗64 + (64 + 1) ∗M parameters. For example,\na policy for a continuous dynamical system with twelve inertial states and three control outputs\nwould be represented by 5187 parameters.\nThe two key ideas that allow reinforcement learning to achieve the desired goal are: sampling\nthrough interactions to compactly represent the dynamics of an unknown stochastic environment,\n43\nand the use of function approximations to represent policies [177] that guides the algorithm during\nthe action selection. Recently, due to advances in computational power, deep neural networks, or\nDeep Learning, is being used for sensory processing and function approximation. Goodfellow et al.\n[155] deﬁne deep learning as the idea of training computers to gather knowledge by experience and\nto learn complex concepts by building them out of simpler ones. Feeding raw pixels of an image to a\ndeep neural network is possible to see how it combines simple concepts of edges to create concepts\nof corners and contours, and combine the concepts of corners and contours to represent the concept\nof an object, as seen in Figure 3.3. In RL, Deep Learning is used to represent polices and value\nfunctions (or the dynamics model, in model-based approaches) — so-called Deep Reinforcement\nLearning (Deep RL) [85, 73]. The higher complexity and plasticity of deep neural networks allows\nbetter quality on feature and data representations [155] and is directly related to the performance of\nreinforcement learning algorithms [73].\nFigure 3.3: Visualization of each layer of a deep neural network use to classify different images.\nReprinted from [4].\n44\n3.3.1\nInductive Bias\nA core concept of learning is to be able to generalize from past experiences (for example, a\ntraining dataset) to address unseen situations [178]. Inductive bias refers to “any basis for choosing\none generalization over another, other than strict consistency with the observed training instances.\"\n[178].\nThis research work assumes that the human brain and its underlying decision process is nonlin-\near and stochastic. Nonlinear due to the neural pathway arrangement and connections [179], and\nstochastic in a sense that different action outputs can be observed from the same observation inputs\ndepending on the current human emotional states [180]. To address this nonlinearity and stochas-\nticity, this research adopts deep neural networks with Gaussian outputs to represent the learning\npolicy. This representation also allows processing the high-dimensional continuous observation\nand action-space encountered in real-world robotic applications. By using neural networks it is\nalso assumed that the policy to be learned, and the function that approximates it, has a smooth\ndifferentiable gradient which can be learned through back-propagation.\n3.4\nExpanding Q-learning to Continuous Space with Deep Reinforcement Learning\nTabular Q-learning has been a popular algorithm in reinforcement learning due to its simplicity\nand convergence guarantees [176]. According to Lillicrap et al. [85], it is not possible to directly\napply Q-learning to continuous action spaces stating that the optimization is too slow to be prac-\ntical with large and unconstrained function approximators (required for continuous spaces). The\ntraditional tabular Q-learning is also unfeasible due to the “curse of dimensionality”, when the\ndiscretized action space grows exponentially with the number of degrees-of-freedom of the problem\n[181]. Due to these restrictions, Lillicrap et al. [85] presented a novel model-free, off-policy actor-\ncritic reinforcement learning algorithm using deep neural networks to learn policies in continuous\naction spaces based on Silver et al. [84] called the Discrete Policy Gradient (DPG) algorithm. It is\nmodel-free because it does not require model dynamics to learn the control policy. It is off-policy\nbecause it can follow a different control policy than the one that is being learned (can use different\n45\npolicies to explore the state space). Actor-critic because it has different structures to improve the\npolicy (actor, through the gradient of the policy’s performance with respect to its parameters) and\nevaluate it (critic, through function approximation to estimate expected future rewards).\nAccording to Silver et al. [84], the policy gradient is the gradient of the policy’s performance,\nwhich on DPG is represented by the actor µ(s|θµ), where θµ is the actor’s network parameters.\nA critic Q(s, a|θQ) is learned using the traditional Q-learning approach [176], where θQ is the\ncritic’s network parameters. The actor parameters are updated by applying the chain rule to the start\ndistribution J with respect to the actor parameters θµ [85], as showed in Eq. (3.18).\n∇θµJ ≈E[∇θµQ(s, a|θQ)|s=st,a=µ(st|θµ)]\n= E[∇aQ(s, a|θQ)|s=st,a=µ(st)∇θµµ(s|θµ)|s=st]\n(3.18)\nPrior to the work presented by Mnih et al. [73], it was thought that using large nonlinear\nfunction approximators to learn critic functions was difﬁcult and unstable [85]. However, using the\ntechniques of replay buffer and target Q networks introduced by Deep Q-Networks (DQN) was\npossible to train actor-critic methods using neural function approximators. The resulting algorithm,\nDeep DPG (DDPG), is able to learn competitive policies using only low-dimension observations\n(e.g. coordinates, velocities, angles, or any other form of sensor reading). The main advantage of\nDDPG is being able to do off-policy exploration, one of the major challenges when learning on\ncontinuous spaces. The exploration policy was constructed by adding process noise N to the actor\npolicy [85]:\nµ′(st) = µ(st|θµ\nt ) + N\n(3.19)\nThe Deep Deterministic Policy Gradient (DDPG) algorithm is outlined in Algorithm 1 below\nand works as follow: two neural networks are initialized - one to work as the actor µ(s|θµ), and\nthe second to work as the critic Q(s, a|θQ). Two copies of these networks are created (they will\nwork as the target actor µ′(s|θµ) and the target critic Q′(s, a|θQ) networks). An empty replay buffer\nR is created to store what the agent is experiencing by interacting with the environment (states s,\n46\nactions a, and rewards r). Each interaction between the agent and environment count as a time\nstep (represented by t). A collection of a given number of time steps T is called an episode. The\nDDPG algorithm loops for a desired number of episodes M. In the beginning of each episode, a\nrandom process N is initialized (for state exploration, explained below) and the agent is placed in a\ndifferent initial state s1 (from where it has a different initial observation). For each time step an\naction is sampled from the actor network and added with exploration noise (noisy actions allow the\nagent to explore the state space beyond its policy, which leads to higher future rewards). This action\nis executed in the environment, which returns the next state and the reward for the action. The most\nrecent experience (reward, action applied, and the current and next state) is stored in the replay\nbuffer. A batch of N experiences is sampled from the replay buffer to train the learning algorithm.\nThe training consists of the following steps: the critic Q evaluates the sampled states and actions,\nadds the returned reward, and computes the target value y; the critic network computes a gradient\nstep to update its parameters based on the target value; and the actor network µ is updated based on\nthe critic’s network gradient - the policy gradient. The parameters of the actor and critic networks\n(θµ and θQ, respectively) are copied to its respective targets (µ′(s|θµ) and Q′(s, a|θQ), respectively)\nbased on the target update rate hyperparameter. Following this process, the actor network tends to\nincrease the probability of suggesting actions that will be better evaluate by the critic, which will\nlead to higher rewards and higher controller performance.\n47\nAlgorithm 1 Deep Deterministic Policy Gradient (DDPG) [85]\n1: Randomly initialize critic Q(s, a|θQ) and actor µ(s|θµ) neural networks with weights θQ and\nθQµ\n2: Initialize target network Q′ and µ′ with weights θQ′ ←θQ, θµ′ ←θµ\n3: Initialize replay buffer R\n4: for episode = 1, M do\n5:\nInitialize a random process N for action exploration\n6:\nReceive initial observation state s1\n7:\nfor t = 1, T do\n8:\nSelect action at = µ(st|θu) + N according to the current policy and exploration noise\n9:\nExecute action at and observe reward rt and new state st=1\n10:\nStore transition (st, at, rt, st+1) in R\n11:\nSample a random minibatch of N transitions (st, at, rt, st+1) from R\n12:\nSet yi = ri + γQ′(si+1, µ′(si+1|θµ′)|θQ′)\n13:\nUpdate critic by minimizing the loss, Eq. (10.7)\nL = 1\nN\nN\nX\ni\n(yi −Q(si, ai|θQ))2\n(3.20)\n14:\nUpdate the actor policy using the sampled policy gradient, Eq. (3.21)\n∇θµJ ≈1\nN\nN\nX\ni\n∇aQ(s, a|θQ)|s=si,a=µ()si∇θµµ(s|θµ)|si\n(3.21)\n15:\nUpdate target networks, Eq. (3.22)\nθQ′ ←τθQ + (1 −τ)θQ′\nθµ′ ←τθµ + (1 −τ)θµ′\n(3.22)\n48\n4.\nCASE STUDY: DEEP REINFORCEMENT LEARNING ON INTELLIGENT MOTION\nVIDEO GUIDANCE FOR UNMANNED AIR SYSTEM GROUND TARGET TRACKING∗\nTracking motion of ground targets based on aerial images can beneﬁt commercial, civilian, and\nmilitary applications. On small ﬁxed-wing unmanned air systems that carry strapdown instead of\ngimbaled cameras, it is a challenging problem since the aircraft must maneuver to keep the ground\ntargets in the image frame of the camera. Previous approaches for strapdown cameras achieved\nsatisfactory tracking performance using standard reinforcement learning algorithms. However, these\nalgorithms assumed constant airspeed and constant altitude because the number of states and actions\nwas restricted. This paper presents an approach to solve the ground target tracking problem by\nproposing the Policy Gradient Deep Reinforcement Learning controller. The learning is based on\nthe continuous full-state aircraft states and uses multiple states and actions. Compared to previous\napproaches, the major advantage of this controller is the ability to handle the full-state ground target\ntracking case. Policies are trained for three different target cases: static, constant linear motion, and\nrandom motion. Results presented in the paper on a simulated environment show that the trained\nPolicy Gradient Deep Reinforcement Learning controller is able to consistently keep a randomly\nmaneuvering target in the camera image frame. Learning algorithm sensitivity to hyperparameters\nselection is investigated in the paper, since this can drastically impact the tracking performance.\n4.1\nProblem Deﬁnition\nFollowing breakthroughs in artiﬁcial intelligence and cost reduction of unmanned air system\n(UAS) platforms, novel applications combining UAS and computer vision for tracking ground\ntargets are on the rise. Examples of civilian and military applications that directly beneﬁt from\nautonomous UAS with ground target tracking capabilities are commercial package delivery and\ndisaster relief response equipped with sense-and-avoid (SAA) technology [182, 183], aerial ﬁlming\n∗Adapted with permission from ”Deep Reinforcement Learning on Intelligent Motion Video Guidance for Un-\nmanned Air System Ground Target Tracking”, by Vinicius G. Goecks and John Valasek, presented at the AIAA Scitech\n2019 Forum [5], Copyright 2019 by the American Institute of Aeronautics and Astronautics.\n49\nand photography [184], and military intelligence, surveillance, and reconnaissance (ISR) programs\n[185, 186].\nGround target tracking can be performed by using cameras ﬁxed to the UAS body (camera\nframe ﬁxed with respect to the aircraft frame) or gimbaled cameras (camera frame independent\nof the aircraft frame). The problem is simpliﬁed by using gimbaled cameras, in which the image\nframe can be controlled independently of the aircraft trajectory. Unfortunately, small ﬁxed-wing\nUAS have restricted payload capabilities, which restricts the use of onboard gimbaled cameras.\nThe immediate solution is to equip small ﬁxed-wing UAS with cameras ﬁxed to the fuselage. This\nrequires aircraft maneuvers to keep the target of interest in the image frame.\nCurrent ground target tracking approaches using ﬁxed cameras [187, 188] are able to achieve\nsatisfactory tracking performance using reinforcement learning algorithms to train a control policy\non a simulated environment, which is later transferred to the physical hardware. These approaches\nrely on training the control policy on simpliﬁed planar motion simulated environments, with constant\naltitude and airspeed, restricting the controller to a single-input to the aircraft due to limitations of\nthe learning algorithm applied.\nThis research builds upon the previously presented work [187, 189] by proposing the de-\nvelopment of a learning controller that handles the full-state continuous target tracking case by\nincorporating concepts of deep reinforcement learning (Deep RL) and policy gradient (PG) algo-\nrithms. The main contributions of this research are i) development of a learning controller that\nhandles the full-state continuous target tracking case by incorporating concepts of deep reinforce-\nment learning (Deep RL) and policy gradient (PG) algorithms; ii) investigation of different learning\ncontroller designs to achieve better performance on the tracking problem; and iii) comparison of the\ntracking performance between previous work that relied on discretization of the state-space and\ncurrent work the uses the full-state continuous state-space. The PG Deep RL controller uses the\n6-Degree-of-Freedom (DOF) linear aircraft simulation to extract target position on the image frame.\nThe aircraft states consist of linear velocities and position; roll, pitch, and yaw attitude angles and\nrates. The controls consist of body-axis pitch, roll and yaw rates; throttle and nozzle position.\n50\n4.2\nRelated Work\nValasek et al. [187] developed a machine learning algorithm approach for learning control\npolicies based on target position in the image frame (pixel position) and current aircraft roll angle.\nThey developed both Q-learning and Q-learning-with-eligibility-traces policies to solve the tracking\nproblem. Dunn and Valasek [189] developed a similar approach to localize atmospheric thermal\nlocations and then guide an UAS to soar from one to another. This research was inspired on how\nthey framed the target tracking problem and it also builds upon their camera and target dynamics.\nNoren et al. [188] detailed the initial ﬂight testing of a previously trained control policy [187]\nfor the autonomous tracking of ground targets. Their work showed that it is possible to train a\ncontroller using machine learning algorithms by using a simulated environment and later deploy\nthe learned policy to hardware. Even limited by a planar motion simulation and single-output\ncontroller, they were able to achieve satisfactory tracking performance on the tracking problem\nwhen ﬂying the algorithm trained on a simulated environment. The authors expected to follow the\nsame development route presented by [188] and deploy the presented Deep RL PG algorithm to\nhardware.\nThere were also alternative approaches besides machine learning algorithms to solve the target\ntracking problem with a ﬁxed strap-down camera on a small UAS. Beard and Egbert [190] derived\na explicit roll-angle and altitude-above-ground-level (AGL) constraints for road tracking that that\nguarantees the target will remain in the camera frame. Beard and Saunders [191] later extended\ntheir previous work proposing a non-linear guidance law using range-to-target and bearing-to-target\ninformation obtained from target motion in the image plane to plan trajectories for the aircraft. Both\napproaches are validated on simulation and ﬂight testing.\nAdvances in modern Deep RL algorithms greatly contributed to the presented research. Schul-\nman et al. presented a series of improvement to classic policy gradient algorithms in order to\nmake them tractable and suitable to optimize policies represented by deep neural networks. Trust\nRegion Policy Optimization (TRPO) [64] and Proximal Policy Optimization (PPO) [65] both rely\non restricting the gradients during the policy update, either by a trust region (deﬁned according to\n51\nthe Kullback-Leibler divergence between current and previous policy) or by a clipped objective\n(deﬁned by the log probability ratio of current and previous policy). Both approaches are able to\nstabilize learning of policies represented by deep neural networks, although PPO leads to faster\nresults in practice. Schulman et al. also reduced the variance of the policy gradient estimates\nby proposing a exponentially-weighted estimator, known as Generalized Advantage Estimation\n(GAE) [66]. These three insights were incorporated to the present research and greatly improved\nthe learning performance.\nThe main weakness of the presented approach is it increase computational complexity. Even\nif the control policy is not update in real-time during ﬂight, the policy is still represented by deep\nneural networks, which require more computational when compared to classical controllers. The\ncurrent approach also requires extensive validation and veriﬁcation, since there is no formal proof\nof convergence of the learning algorithm. Additionally, hard constraints should be added to aircraft\nstates and controls in order to guarantee operation within safety limits.\n4.3\nLearning Tracking Policies with Reinforcement Learning\nThe proposed PG Deep RL controller frames ground target tracking as a reinforcement learning\nproblem. In Reinforcement Learning (RL), it is desired to train an agent to learn the parameters θ\nof a policy (or controller) πθ, in order to map the partially-observable environment’s observation\nvector o (or state vector s in fully-observable environments) to agent actions a. The performance of\nthe agent is measured by a scalar reward signal r returned by the environment. Figure 3.1 illustrates\nthis description as a diagram and Figure 4.1 shows how it can be applied to the target tracking case.\nIn the target tracking case proposed by this work, the environment comprises the aircraft, camera,\nand ground target model, explained in more details in the following paragraphs. Environment states\nare represented by the aircraft states (linear position and velocity, roll, pitch, yaw angles and rates)\nand target pixel position in the image plane — see Equation 4.1. The agent is represented by the PG\nDeep RL controller, explained in more details in Section 4.4. The agent controls the aircraft through\nelevator, throttle, nozzle position, aileron, and rudder actions, δe, δT, δn, δa, δr respectively — see\nEquation 4.2. Rudder control was removed to reduce complexity of the model for the PG Deep RL\n52\nFigure 4.1: Intelligent motion video guidance for unmanned air system ground target tracking\nmodeled as a reinforcement learning problem. Reprinted from [5].\ncontroller. The agent’s actions are evaluated by the a scalar value, the reward signal. The reward\nsignal is computed based on the normalized Euclidean distance of the target to the center of the\nimage plane — see Equation 4.3.\nS = {XT, YT, u, w, q, θ, x, z, v, p, r, φ, ψ, v}\n(4.1)\nA = {δe, δT, δn, δa, δr}\n(4.2)\nr =\n \n1 −\np\nX2\nT + Y 2\nT\np\nX2\nT MAX + Y 2\nT MAX\n!2\n(4.3)\nThe PG Deep controller optimizes the action selection by trial-and-error process on a simulated\nenvironment. The simulation is broke down in episodes, starting with the aircraft on a random\nposition in the inertial space and ground target initially in the image frame. The episode resets if\nthe target leaves the image frame or if aircraft performs any maneuver outside the safety limits,\nmanually deﬁned by the controller design.\n53\n4.4\nPolicy Gradient Deep Reinforcement Learning Controller\nThis section details how each part is modeled and how the tracking policies are learned.\nOne of the main contributions of this research is the development of a learning controller that\nhandles the full-state continuous target tracking case, which includes the full-state continuous\nsimulation of the aircraft, camera, and target. The aircraft is represented by a linear model of the\nAV-8B Harrier — Figure 4.2 illustrates the aircraft, Equations 4.4 and 4.5 represent the longitudinal\nand lat/d model, respectively, and Table 4.1 its trim parameters. Camera speciﬁcations are shown in\nTable 4.2. The target is modeled as a point mass with planar motion on the XY plane. Policies are\ntrained for three different target cases: static, constant linear motions, and random motion.\nFigure 4.2: Illustration of AV-8B Harrier, whose linear model is used to validate the proposed PG\nDeep RL controller. Reprinted from [5].\n\n\n˙u\n˙w\n˙q\n˙θ\n˙x\n˙z\n\n\n= [ALON]\n\n\nu\nw\nq\nθ\nx\nz\n\n\n+ [BLON]\n\n\nδe\nδT\nδn\n\n\n,\n(4.4)\nwhere\n54\n[ALON] =\n\n\n−0.0693\n−0.0006\n−0.132\n−32.171\n0\n0\n0.0199\n0.0005\n10.1644\n−0.3936\n0\n0\n0\n0\n−0.0409\n0.0373\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nand\n[BLON] =\n\n\n3.816\n0.0237\n−32.667\n4.255\n−1.008\n−0.724\n−4.648\n−0.0094\n−0.0679\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n.\n\n\n˙v\n˙p\n˙r\n˙φ\n˙ψ\n˙y\n\n\n= [ALATD]\n\n\nv\np\nr\nφ\nψ\ny\n\n\n+ [BLATD]\n\n\nδa\nδr\n\n\n(4.5)\nwhere\n55\n[ALATD] =\n\n\n0\n0.131\n−10.252\n32.171\n0\n0\n−0.105\n0.0264\n0\n0\n0\n−0.0027\n−0.0489\n0\n0\n0\n1\n0.0122\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\nand\n[BLATD] =\n\n\n−1.0263\n4.972\n13.735\n0.454\n0.959\n−1.347\n0\n0\n0\n0\n0\n0\n\n\n.\nTable 4.1: Aircraft trim parameters. Reprinted from [5].\nTrim Parameter\nValue\nM1\n0.009\nU1 (ft/s)\n10\nH1 (ft)\n100\nδe (deg)\n1.82\nδn (deg)\n88\nδT (deg)\n89.5\nReinforcement Learning is concerned about learning in an unknown stochastic environment and\ncan be formalized as a Partially Observable Markov Decision Process (POMDP) [171]. A POMDP\nM can be characterize by its state-space S (where a vector of states s ∈S), an observation-space\nO (where a vector of observations o ∈O), an action-space A (where a vector of actions a ∈A), a\ntransition operator T (which deﬁnes the probability distribution p(ot+1|ot)), and the reward function\n56\nTable 4.2: Modeled camera speciﬁcations. Reprinted from [5].\nParameter\nValue\nResolution (pixels)\n1024x768\nAspect Ratio\n4:3\nHorizontal Field of View (deg)\n90\nVertical Field of View (deg)\n30\nPan Angle w.r.t. Aircraft Frame (deg)\n-90\nTilt Angle w.r.t. Aircraft Frame (deg)\n-20\nr : S × A →R (or r(s, a). This deﬁnition is illustrated by Equation 4.6 and Figure 2.9:\nM = {S, A, O, T , r}.\n(4.6)\nTo simplify further derivation of the algorithm it is assumed a fully-observable environment,\nwhere the observation-space O = S (state-space) and, consequently, vector of observations o = s\n(states).\nIn Reinforcement Learning, at each time step t of a ﬁnite time horizon T, a policy (or controller)\nπ, parametrized by θt, maps the current environment’s states st to actions at: πθ(at|st). This\naction affects the current environment’s states st which evolves to st+1 based on the environment’s\ntransition distribution (dynamics) p(st+1|st, at). The environment also returns a scalar reward\nfunction r that evaluates the action taken at at the state st: r(st, at).\nDuring an episode, the sequence of states observed and actions taken over a number of time\nsteps T can be represented by a trajectory τ:\nτ = {s0, a0, s1, a1, . . . , sT, aT}\n(4.7)\nThe probability of experiencing a given trajectory τ in a Markov Decision process can be written\n57\nas:\nπθ(τ) = pθ(s0, a0, s1, a1, . . . , sT, aT)\n(4.8)\n= p(s1)\nTY\nt=1\nπθ(at|st)p(st+1|st, at)\n(4.9)\nThe goal in RL is to ﬁnd the parameters θ∗that will maximize the objective J(θ), which\nrepresents the expected total reward to be received by this policy πθ(τ):\nθ∗= argmax\nθ\nJ(θ)\n(4.10)\n= argmax\nθ\nE\nτ∼pθ(τ)[\nT\nX\nt=1\nr(st, at)]\n(4.11)\nTo maximize J(θ), it is possible to compute its gradient with respect to its parameters θ. Since\nthe structure of the objective J(θ) and its gradient ∇θJ(θ) are unknown, in practice the only way to\nevaluate them and approximate the expectation term is by sampling and averaging over N samples:\n∇θJ(θ) ≈1\nN\nN\nX\ni=1\n[(\nT\nX\nt=1\n∇θ log πθ(a(i)\nt |s(i)\nt ))(\nT\nX\nt=1\nr(a(i)\nt , s(i)\nt ))]\n(4.12)\nThe policy is improved by gradient ascent according to a step size α, as shown in Equation 4.13\nand Figure 3.2, optimizing the agent’s policy during the training time.\nθ →θ + α∇θJ(θ)\n(4.13)\n4.5\nNumerical Results\nMachine learning algorithms, specially reinforcement learning using deep neural network\nas model representation, require thousands or even millions of training episodes to converge\nto a satisfactory policy. These learning algorithms are also sensitive to hyperparameter values,\nwhich tune the learning performance. Due to this reason, it is a good practice to performance a\n58\nhyperparameter search before running long-term experiments. During a hyperparameter search, the\nsame algorithm is tested for a short number of episodes with multiple hyperparameter values. Since,\nintuitively, more data leads to better performance in machine learning, it was decided to investigate\nthe impact of the batch size hyperparameter, which controls the number of episodes that are used to\ncompute the gradient ascent step during the policy improvement phase.\nFigure 4.3 shows the reward values and maximum number of steps achieved per episode for\ndifferent learning agents with different batch sizes, as summarized in Table 4.3. A moderate batch\nsize of 60, compare to 20 and 100, lead to longer tracking time as faster reward convergence. Counter-\nintuitively, larger batch size does not necessarily leads to faster convergence of the algorithm. Figure\n4.4 shows the mean discounted reward achieved during a training batch, averaged by the batch size.\nLarger batch sizes seems to lead to high variance performance.\n(a) Mean reward achieved during a training batch.\n(b) Maximum number of steps during a training batch.\nFigure 4.3: Performance evaluation with respect to reward values and maximum number of steps\nachieved for different learning agents with different hyperparameters (see Table 4.3) during each\ntraining episode. Reprinted from [5].\nAfter the hyperparameter search, it was desired to evaluate the tracking performance of the best\nperforming agent. Using the hyperparameters of Agent #1, the agent was retrained during 50000\nepisodes. The main goal was to keep the target in the image frame throughout the simulation period\n59\n(a) Mean discounted reward achieved during a training\nbatch.\n(b) Standard deviation of discounted reward achieved\nduring a training batch.\nFigure 4.4: Performance evaluation with respect to mean discounted reward and its standard\ndeviation for different learning agents with different hyperparameters (see Table 4.3) during each\ntraining episode. Reprinted from [5].\nTable 4.3: Hyperparameter values used for the policy gradient deep reinforcement learning algorithm.\nReprinted from [5].\nHyperparameter\nAgent #0\nAgent #1\nAgent #2\nNumber of Episodes per Training Batch\n20\n60\n100\nNumber of Episodes\n20000\n20000\n20000\nDiscount Factor γ\n0.995\n0.995\n0.995\nGeneralized Advantage Estimation λ\n0.98\n0.98\n0.98\nKL Divergence Target Value\n0.003\n0.003\n0.003\n(one minute). Figure 4.5 shows the the PG Deep RL controller is able to consistently keep the target\non the image frame throughout the maximum simulated time after 23,000 training episodes.\n4.6\nSummary\nThis paper presented an approach to solve the ground target tracking problem by proposing the\nPolicy Gradient Deep Reinforcement Learning controller. The major advantage of the proposed\ncontroller with respect to previous approaches is being able to handle the full-state ground target\ntracking case, learning based on the full-state continuous aircraft states and controlling multiple\noutputs. Results on a simulated environment show that, after trained, the controller is able to\n60\nFigure 4.5: Consistent tracking performance of PG Deep RL agent on a simulated environment.\nReprinted from [5].\nconsistently keep the target in the image frame.\nCurrent results shows that the performance of the controller is drastically affected by the choice\nof the agent’s hyperparameters, speciﬁcally the number of episodes per training batch — batch size.\nContrary to intuition, increasing batch size doesn’t lead to improve controller performance. Batch\nsize in the order of 60 episodes leads to faster learning and improved tracking time when compared\nto 20 and 100 episodes per batch.\nAfter selected the best performing hyperparameters, current results shows that after 23,000\nepisodes of training the controller is able to learn how to maneuver the aircraft and keep the target\nin the image frame during the whole simulation period (60 seconds).\n61\n5.\nCASE STUDY: CONTROL OF MORPHING WING SHAPES WITH DEEP\nREINFORCEMENT LEARNING∗\nTraditional model-based feedback control techniques are of limited utility for the control of\nmany shape changing systems due to the high reconﬁgurability, high dimensionality, and nonlinear\nproperties of the plant and actuators of these systems. Computational intelligence and learning\ntechniques offer the promise of effectively leveraging the use of both smart materials and controls for\napplication in aerospace systems such as morphing air vehicles. This paper addresses the challenge\nof controlling morphing air vehicles by developing a deep neural networks and reinforcement\nlearning technique as a control strategy for shape-memory alloy (SMA) actuators in the context of a\nmorphing wing. The control objective is to minimize the error between an objective and the actual\nairfoil trailing edge deﬂection. The proposed controller is evaluated on a simple inverted pendulum\nfor validation, on a 3D printed wing section that is actuated by a composite SMA actuator in a wind\ntunnel, and on a simulation based on wind tunnel data. Results show that the learning algorithm is\ncapable of learning how to morph the wing. It is also able to control shape changes from arbitrary\ninitial shapes to arbitrary goal shapes using the same trained learning algorithm. The results provide\na proof of concept for the use of learning algorithms to control more complex morphing aircraft\nwith continuous states and actions for the outer mold line conﬁguration.\n5.1\nProblem Deﬁnition\nAircraft design is typically a performance trade-off between different ﬂight conditions while\nsatisfying certain safety requirements [192]. Morphing aircraft have the potential to allow an aircraft\nto change its shape and optimize performance for different mission objectives [193]. Recent efforts\nin the development of shape-memory alloy (SMA) actuators [194] have shown that SMA actuators\nenable small-scale camber morphing that can increase performance and versatility for different\n∗Adapted with permission from “Control of Morphing Wing Shapes with Deep Reinforcement Learning”, by\nVinicius G. Goecks, Pedro B. Leal, Trent White, John Valasek, and Darren J. Hartl, presented at the 2018 AIAA\nInformation Systems-AIAA Infotech@ Aerospace [6], Copyright 2018 by the American Institute of Aeronautics and\nAstronautics.\n62\nmission objectives [195, 193]. However, these actuators have strong thermomechanical coupling,\nhysteretic dynamical behavior, and can change response characteristics over time [196]. Traditional\nmodel-based feedback control techniques are thus of limited utility especially when multi actuation\nis considered, however, computational intelligence and learning techniques offer the promise of\neffectively synthesizing robust and reliable controllers for systems with smart materials.\nAcademic interest in morphing structures has existed for decades [192], and federal agencies\nsuch as NASA have shown strong interest over the years. An example of this is the Mission\nAdaptive Digital Composite Aerostructure Technologies (MADCAT) [197]; a NASA program to\ndevelop a novel aerostructure concept that takes advantage of emerging digital composite materials\nand manufacturing methods. The objective is to build high stiffness-to-density ratio, ultra-light\nstructures that will facilitate the design of adaptive and aerodynamically efﬁcient air vehicles.\nAnother explored concept was the Spanwise Adaptive Wing Concept [198] that introduced a folding\nsurface at the outboard part of the wing. According to ﬂight conditions, the surface is folded to\nreduce drag, increase lift, and improve lateral-directional stability. This would allow an increase of\nperformance during taxiing, takeoff, cruise, and supersonic ﬂight.\nConsidering morphing wings, they not only present challenges from a material science and\nstructure standpoint, but also from a ﬂight control design standpoint due to changing dynamics\nand inherent uncertainties. A solution proposed by Valasek et al. [132, 133] is the Adaptive-\nReinforcement Learning Control technique (A-RLC) that learns near optimal shape changes from\narbitrary initial conﬁguration to other arbitrary conﬁguration that maximizes the performance of the\naircraft for a given ﬂight condition or maneuver. At the same time, it uses Structured Adaptive Model\nInversion (SAMI) as a trajectory tracking controller to handle the time-variant properties, parametric\nuncertainties, and disturbances. The control law learns how to shape the airfoil by changing two\ndiscrete degrees-of-freedom: thickness and camber. The goal of the learning algorithm is to meet\nnominal aerodynamic goals in terms of lift [199]. The work was later extended to handle four\ndegrees-of-freedom while morphing: wing tip chord, root chord, span, and leading edge sweep\nangle [200].\n63\nThis paper addresses the camber control of morphing wings using modern reinforcement\nlearning techniques and deep neural networks. The main advantage of this method is the use of\ncontinuous states and actions for the outer mold line (OML) conﬁguration and control inputs. The\nsame learning algorithm is validated in different cases of a pendulum upswing task (different mass,\ngravity, length) to validate that a nominal learning algorithm can learn a satisfactory policy even\nwhen the model changes, which will happen to a morphing airfoil. The validated algorithm is then\napplied to morph a scaled wing prototype with embedded SMA actuators tested in a wind tunnel\nenvironment, and on a simulated morphing wing model created based on the wind tunnel data.\nPerformance of the algorithm is evaluated by commanding different desired airfoil trailing edge\ndeﬂections from different initial states.\n5.2\nLearning Morphing Between Wing Shapes\nIn this paper, the conﬁguration of the wing is deﬁned by the deﬂection of its trailing edge. The\ngoal of the learning algorithm is to adapt the deep neural network parameters that represent the\ncontroller to successfully map the input deﬂections to output voltages necessary to move from any\narbitrary wing conﬁguration to another. The Deep Reinforcement Learning controller receives\nsensor readings that indicate the current deﬂection of the wing and a reward signal based off of an\nevaluation of the control applied. The output voltage is used to heat up the SMA wires via Joule\neffect leading to austenite transformation. Consequently, the transformation strain is recovered and\nthe wing morphs. Figure 5.1 shows how the learning algorithm interfaces with the morphing wing\nmodel, stores past experiences in a memory buffer, maps current states to actions (learns the policy),\nand computes the gradients based on the received rewards to update the network parameters, as\nexplained in Algorithm 1 and Subsection 3.4.\nAs mentioned in Section II.B, the network architecture consists of neurons organized as hidden\nlayers. The neural network herein implemented is depicted in Figure 5.2 which in addition shows\nhow these neurons are connected between layers, the speciﬁc activation functions used after each\nlayer output, and additional operations to transform the network output. The learning algorithm\nis evaluated based on the control effort required to morph, time to go from the initial to the ﬁnal\n64\nSensors\nSTART\nPolicy Gradient \nComputation\nREPEAT\nUpdate\nCritic\nUpdate\nActor\nTD Error \nand Loss\nSMA Actuator\nMorphing Airfoil\nMemory \nBuffer\ns't\nr't\ns't\ns't+1\na't\nTarget Q-values\nr't\nairfoil next states (st+1)\nst+1\nairfoil states (st)\nat\nactuator action (at)\nMaps states \nto actions\nEvaluates \nactions\ns't+1\nRead next \nmeasurements \nAveraged gradient \nof the critic and \nactor networks \n(DPG Theorem)\nUpdate based \non computed \ngradient\nUpdate Target \nnetworks\nGradient of Loss \nFunction\nActor \nNetwork\nActor-Target \nNetwork\nCritic \nNetwork\nCritic-Target \nNetwork\nSample Minibatch\n(s't, a't, r't, s't+1)\nFigure 5.1: Diagram of the learning algorithm and interface with the morphing wing model.\nReprinted from [6].\nconﬁguration, and number of iterations required to train the learning algorithm to achieve the best\nperformance.\nAnother important feature of the DDPG algorithm are the hyperparameters. These parameters\nare used for extra tuning of the learning algorithm and direct affect its learning performance. The\nhyperparameter values used in this work are depicted in Table 5.1. All the parameters and their\nfunction are herein described. The Target Update Rate deﬁnes how often (in episodes) the target\nnetwork parameters are copied from the original network. Target Update Factor is a relaxation factor\nfor the copied network parameters (θ). Actor and Critic Learning Rates deﬁne the step size of the\ngradient update when performing the optimization of the network parameters. Memory Buffer Size\ndeﬁnes the number of past experiences (rewards, states, and actions observed from the interaction\n65\nInput\n400 Units\n300 Units\nScale\nFully-Connected\nFully-Connected\nUnprocessed \nActor Output\n400 Units\n300 Units\nOutput\nFully-Connected\nFully-Connected\n Actor Output \nor Critic Input\nFigure 5.2: Deep neural network architecture that maps the current wing conﬁguration to control\ninputs. Reprinted from [6].\nbetween agent and environment) that are stored and mixed with current experiences. Minibatch Size\ncontrols the number of experiences used during each gradient update on the networks. Discount\nFactor, bounded between zero and one, deﬁnes how future expected rewards are accounted during\nthe learning phase.\nTable 5.1: Hyperparameter values used for DDPG algorithm. Reprinted from [6].\nHyperparameter\nValue\nTarget Update Rate (Episodes)\n100\nTarget Update Factor (τ)\n0.001\nActor Learning Rate\n0.0001\nCritic Learning Rate\n0.001\nMemory Buffer Size\n100,000\nMinibatch Size\n64\nDiscount Factor γ\n0.99\n5.3\nExperimental Setup\nThe main experimental setup comprised of wind tunnel testing a 3D-printed, avian-inspired,\nwing prototype actuated by a composite SMA actuator powered by an external power supply, as\n66\nseen in Figures 5.3, 5.4, and 5.5. The actuator consists of a PDMS matrix with embedded SMA\nwires. Trailing edge deﬂection of the wing, voltage applied to the SMA wire, and its temperature\nwere measured during the experiment.\nFigure 5.3: Experimental setup for the learning algorithm: prototype in the wind tunnel. Reprinted\nfrom [6].\nThe experiment consists in deﬁning a random setpoint and initial position for the trailing edge\nof the wing. The learning algorithm reads the current deﬂection and setpoint applied voltage to the\nSMA wire in order to morph the wing to the desired state in less than 200 measurement iterations\nthat comprise the learning episode. At the beginning of each episode, the goal and initial states are\nrandomized and the learning algorithm has to perform the task again. During each episode, the\nlearning algorithm is trying to maximize the objective function R (“total reward” of the episode)\ndependent on the magnitude of control applied and error between current and desired deﬂection\ngiven as:\nR = 1\nT\nT\nX\nt=0\ne(x∗−x)2−u/8,\n(5.1)\n67\nFigure 5.4: Experimental setup for the learning algorithm: top view of the prototype. Reprinted\nfrom [6].\nFigure 5.5: Experimental setup for the learning algorithm: bottom view of the prototype. Reprinted\nfrom [6].\nwhere T is the total number of time steps t, x is the current deﬂection, x∗the setpoint, and u is the\nvoltage applied being normalized by its maximum value of 8 volts.\nThe network used to learn the dynamics comprises of three fully connected layers (each input\nis connected to every parameter of that layer) with 220, 160, and 130 neurons, respectively. The\nfully connected layers are followed by dropout connections (randomly disconnects 20% of the\nunits and its input connections during the training phase for better generalization [201]). The\n68\nconnection between each layer is made through rectiﬁed linear units (ReLU) activation functions,\nwhich discards the input if it is negative and adds nonlinearities to the network when more hidden\nlayers are stacked. The deep neural network if optimized using the “Adam” (Adaptive Momentum\nEstimation) [202] optimizer based on a standard mean squared error loss function of the predicted\nand true temperature and displacement values. “Adam” is an algorithm for ﬁrst-order gradient-based\noptimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\nIt is computationally efﬁcient, little memory requirements, is invariant to diagonal rescaling of the\ngradients, and is well suited for problems that are large in terms of data and/or parameters. It stores\nexponentially decaying average of past squared gradients and past gradients [202].\nApplying data-driven learning algorithms directly to hardware is challenging. Deep Rein-\nforcement Learning algorithms are well known to heavily depend on multiple graphic and central\nprocessing units (GPUs and CPUs) [73, 78, 76, 69] and training time on the order of days [73, 74]\nto achieve meaningful results. Simple continuous tasks (e.g., controlling a two-link robotic arm),\nrequires on average more than 2.5 million steps [85] and more complex tasks (e.g., control of a\nhumanoid robot or robots with multiple degrees of freedom), requires on average 25 million steps\nusing different reinforcement learning algorithms [67]. Since it was infeasible to perform a wind\ntunnel test in the order of weeks, it was desired to create a high-ﬁdelity simulation model that would\nhave the same dynamics of the avian-inspired airfoil section actuated by a SMA wire under a wind\ntunnel test. To achieve this goal, a deep neural network was implemented using initial wind tunnel\ndata to learn to approximate the hardware dynamics. Figure 5.6 shows the learned model compared\nto the truth model in terms of temperature and displacement.\n69\n(a)\n(b)\nFigure 5.6: Preliminary results modeling the simple spring system showing (a) Displacement and\ntemperature changes over time; and (b) Displacement as a function of temperature. Reprinted from\n[6].\n70\n5.4\nNumerical Results\n5.4.1\nValidation of the Learning Algorithm\nBefore implementing the DDPG learning algorithm on the morphing wing experiment, it was\nthe implementation was tested on a simulated inverted pendulum upswing. In this system, the\nalgorithm must apply continuous positive or negative torque (u) to the pendulum shaft to keep it\nin the upright position. The only states available to the learning algorithm is the current angular\nposition and velocity (θ and ˙θ) of the pendulum. A reward signal was given to algorithm to represent\nits performance, as shown in Eq. (5.2), for every step. Reward results of more than -200 per episode\nare considered satisfactory because they numerically represent that the pendulum is held in vertical\nequilibrium after the swing. Figure 5.7a shows the resultant reward values per episode. It can be\nseen that the algorithm achieved the desired behavior of obtaining reward results of more than\n-200 on the ﬁnal learning episodes, converging to a constant result. This can be interpreted as the\nlearning algorithm, based on measured position and velocity and torque applied to the pendulum,\nbeing able to maximize the reward signal returned by the environment and complete the maneuver\nof an upswing in within 10 seconds.\nTo validate that the algorithm with same hyperparameters is able to achieve satisfactory behavior\neven with changes in the model, the same agent was trained on an inverted pendulum double\nthe original mass. The algorithm has no knowledge that the environment was changed. Figure\n5.7b shows the resultant reward values per episode for the double mass case. It can be observed\nthat the same algorithm and neural network were able to learn a policy robust to changes in the\nenvironments and still approach the deﬁned mark of -200 reward points that represent a complete\ninverted pendulum upswing maneuver. As an additional evaluation, Fig. 5.7c and 5.7d show the\nresultant reward values per episode for a double length pendulum case and for the half gravity case,\nrespectively.\nr = −(||θ2|| + .1( ˙θ2) + .001(u2))\n(5.2)\n71\n(a)\n(b)\n(c)\n(d)\nFigure 5.7: Results of the validation of the Deep Deterministic Policy Gradient algorithm solving\nthe inverted pendulum upswing. Average of ﬁve runs and standard deviation of rewards per episode.\n(a) Unitary mass pendulum case; (b) Double mass pendulum case; (c) Double length pendulum\ncase; and (d) Half gravity pendulum case. Reprinted from [6].\n5.4.2\nThe Learning Algorithm in the Wind Tunnel\nThe main hypothesis is that the same algorithm used to learn to control different cases of the\nupswing pendulum task can be used to learn an shape change policy for morphing airfoils. The\ndeep reinforcement learning algorithm was deployed to the hardware platform to test the hypothesis.\nThe algorithm ran in the wind tunnel for more than 300 episodes. Every 200 time steps, or about\n200 seconds, the commanded setpoint was randomly changed and the learning algorithm had to\n72\nadapt the applied voltage to match the commanded value. It was intended to perform the training\nsession for more than 300 episodes, but the wind tunnel operation was limited. Results achieved\nafter 100 and 200 episodes are shown in Fig. 5.8. As expected, 300 episodes were not enough to\ntrain a policy that would satisfactory command the current displacement to the desired reference\nvalue, but it is possible to observe an improvement after 100 and 200 training episodes.\n(a)\n(b)\nFigure 5.8: Training on the Wind Tunnel: Deep Reinforcement Learning controller performance for\ntraining time consisting of: (a) 100 and (b) 200 episodes. Reprinted from [6].\n5.4.3\nThe Learning Algorithm in the Simulation Model\nAs explained in Section 5.4.2, due to limited time at the wind tunnel, the learning algorithm\nwas also tested using a simulated wing model created based on data collected from the wind tunnel.\nThe algorithm ran for 1500 episodes. Every 200 time steps the commanded setpoint was randomly\nchanged and the learning algorithm had to adapt the applied voltage to match the commanded value.\nFigure 5.9 shows the controller performance after 900 and 1500 episodes of training. After 900\nepisodes there is still steady-state error between the current and commanded displacement. After\n1500 the deep reinforcement learning algorithm is able to complete the task multiple times in a row\n73\nfor different commanded morphing shapes.\n(a)\n(b)\nFigure 5.9: Training on the Modeled Airfoil: Deep Reinforcement Learning controller performance\nafter 900 (a) and 1500 (b) episodes of training time, respectively. Reprinted from [6].\n5.5\nSummary\nThis paper addressed the challenge of controlling morphing air vehicles by developing a deep\nneural network and reinforcement learning technique for the control of shape memory alloy actuators\nthat adapt the wing outer mold line. The objective was to investigate if the learning algorithm could\nalso be used to learn satisfactory control policies for morphing wing. These are characterized by a\nplant which has continuous states and actions and time-varying dynamics. Based upon the results\npresented in the paper, the following conclusions are drawn.\n1. Data collected from wind tunnel testing was shown to be effective in training a deep neural\nnetwork that accurately mimics the dynamic behavior of the wing section actuated by a shape\nmemory alloy. The same learning algorithm and hyper-parameters were also shown to control\nshape changes from arbitrary initial shapes to arbitrary goal shapes when trained with the\nhigh-ﬁdelity simulation model.\n74\n2. The deep reinforcement learning algorithm using the Deep Deterministic Policy Gradient\nlearned to compute torques and perform an upswing maneuver on four different versions of\nan inverted pendulum. This algorithm was also shown to control a 3D-printed, avian-inspired\nairfoil section actuated by a shape-memory alloy operating in a wind tunnel. However, a fully\nsatisfactory control policy was not learned due to time limitations on the testing platform.\n3. The results provided insights that deep neural networks can be successfully used to model the\ndynamics of complex morphing air vehicle systems, followed by deep reinforcement learning\nalgorithms to control these dynamic systems with continuous states and actions.\nFurther work on deep reinforcement learning algorithms are needed to reduce the extensive\nofﬂine training required, and reduce the number of interactions with the environment. This would\nease the transition to direct training on relevant hardware platforms.\n75\n6.\nCASE STUDY: INVERSE REINFORCEMENT LEARNING APPLIED TO GUIDANCE\nAND CONTROL OF SMALL UNMANNED AERIAL VEHICLES\nThis chapter was submitted under the name of “Inverse Reinforcement Learning Applied to\nGuidance and Control of Small Unmanned Aerial Vehicles” as ﬁnal project of the 2018 Rein-\nforcement Learning class at Texas A&M University, in Department of Electrical and Computer\nEngineering, taught by Dr. Dileep Kalathil, wrote by Vinicius G. Goecks, Akshay Sarvesh, Kishan\nP. Badrinath and is partially reproduced here.\nIn this chapter we investigate traditional Inverse Reinforcement Learning approaches and their\nextensions Guided Cost Learning and Generative Adversarial Imitation Learning to learn how\nto land a small Unmanned Aerial Vehicle based on human demonstration. This task is complex\nto traditional Inverse Reinforcement Learning algorithms due to the continuous nature of the\nobservation and action-space. Our approach is ﬁrst validated on less complex continuous tasks\navailable on OpenAI Gym [203] — the Pendulum-v0 and LunarLanderContinuous-v2 environments\n— and later transferred to a high-ﬁdelity UAV simulator, Microsoft AirSim [204]. Our results\nshow that Generative Adversarial Imitation Learning is able to learn how to land an unmanned\naerial vehicle and surpass human mean performance after 100 learning iterations. This approach\nlearns from 100 human demonstrations of the landing task, equivalent to about 30 minutes of\ndemonstration. The learning converges to the upper-bound of human performance after 400 learning\niterations.\n6.1\nProblem Deﬁnition\nIt is true that humans can no longer claim to be the experts in tasks like image classiﬁcation\nand recognition [205]. At the same time there are also a number of tasks where humans can easily\noutperform the machines, for now, in more complex tasks as, for example, driving and playing\nsports, mostly due to challenges in perception, motion control, and mechanical actuation.\nA traditional Reinforcement Learning (RL) problem focuses on an agent learning a process\n76\nof decisions (policies) to produce a output which maximizes a reward function return by the\nenvironment. In Inverse Reinforcement Learning (IRL), the goal is to extract a reward functions\nby observing the behavior of an policy or any set of demonstrations. In this project we investigate\ntraditional IRL approaches and their extensions Guided Cost Learning (GCL) and Generative\nAdversarial Imitation Learning (GAIL) to learn how to land a small Unmanned Aerial Vehicle\n(UAV) based on human demonstration. This task is complex for traditional IRL algorithms due\nto the continuous nature of the observation and action-space. Our approach is ﬁrst validated\non less complex continuous tasks available on OpenAI Gym [203] — the Pendulum-v0 and\nLunarLanderContinuous-v2 environments — and later transferred to a high-ﬁdelity UAV simulator,\nMicrosoft AirSim [204].\nSection 6.2 of this chapter introduces the notation used throughout this document and the\ntheoretical background of the algorithms used for this work. Section 6.3 details previous approaches\nfor the IRL problem, highlighting their strengths and limitations. Section 6.4 details the application\nof IRL to learn the UAV landing task based on human demonstration, followed by results and\ndiscussion on Section 7.4. A summary is described in Section 7.5.\n6.2\nNotation and Background\n6.2.1\nNotation\nA ﬁnite Markov Decision process (MDP) is a tuple (S, A, {Psa}, γ, R) where : S is a ﬁnite set\nof N states, A = {a1, a2, .., ak} actions, Psa are the transition probabilities for taking an action\na in state s, γ ϵ (0, 1] is the discount factor, R is the reinforcement function which is bounded by\nRmax.\nA policy π is deﬁned as a map from π : S →A and the value fn evaluated for any policy π is\ngiven by V π(s) = E[R(s1) + γR(S2)+] + γ2R(s3) + ... + γkR(sk)]. The Q fn can also be deﬁned\nas : Qπ(s, a) = R(s) + γE[V π(s)]. The optimal value fn is V ∗(s) = supπVπ(s) and the optimal Q\nfn is Q∗(s, a) = supπQπ(s, a).\nIn a standard RL problem we ﬁnd a policy π such that V pi(s) is maximized. In the IRL problem,\n77\nthe goal is to ﬁnd the best reward function for a set of observations.\n6.2.2\nGuided Cost Learning\nGuided Cost Learning (GCL) [206] introduces an iterative sample-based method for estimating\nthe probability normalization term Z in the Maximum entropy IRL formulation [34], and can\nscale to high-dimensional state and action spaces and non-linear reward structures. The algorithm\nestimates Z by training a new sampling distribution q(τ), with estimates of the expert trajectories\nas ˜p(τ), general reward function parameterized by θ, and using importance sampling:\nLreward(θ)\n=\nEτ∼p[−rθ(τ)] + log\n\u0012\nEτ∼q[\nexp(rθ(τ))\n1\n2 ˜p(τ) + 1\n2q(τ)]\n\u0013\n.\nGCL alternates between optimizing rθ using this estimate, and optimizing q(τ) to minimize the\nvariance of the importance sampling estimate.\nThe optimal importance sampling distribution for estimating the partition function Z is propor-\ntional to the exponential family of the reward function. During GCL, the sampling policy q(τ) is\nupdated to match this distribution by minimizing the Kullback-Leibler (KL or DKL) divergence\nbetween q(τ and 1\nZ exp(rθ(τ)), or equivalently maximizing the learned reward and entropy:\nLsampler(q) = Eτ∼q[−rθ(τ)] + Eτ∼q[log q(τ)].\n6.2.3\nGenerative Adversarial Imitation Learning\nGenerative Adversarial Imitation Learning (GAIL) [207] follows the generative modeling\nliterature, in particular, Generative Adversarial Networks (GANs). Here, two models, a generator\nG and a discriminator D, are trained simultaneously. The discriminator is tasked with classifying\nits inputs as either the output of the generator, or actual samples coming from an expert’s data\ndistribution p(τ); where τ is sample trajectories for GAIL. The goal of the generator is to produce\noutputs that are classiﬁed by the discriminator as coming from the underlying data distribution.\nTo elucidate, the generator is subsumed by the environment generating trajectories (τ ∼G)\n78\nfrom a random policy, while the discriminator takes as input a sample τ and outputs the probability\nD(τ) that the sample was from the data distribution. Since D and G are playing the same game but\nwith opposing goals, their loss functions can be written as the following respectively:\nL(D) = Eτ∼p[−log D(τ)]\n+\nEτ∼G[−log(1 −D(τ))]\nL(G) = Eτ∼G[−log D(τ)]\n+\nEτ∼G[log(1 −D(τ))].\n6.3\nRelated Work\nNg and Russell [32] deﬁned the problem of IRL almost two decades ago. They state that the\nreward function is a more robust deﬁnition of a task rather than the policies themselves. The authors\nformulate the problem of IRL as a optimization problem and demonstrate algorithms to solve\nthe Linear Programming (LP) problem on simple discrete, ﬁnite and continuous and inﬁnite state\nproblems.\nZiebart et al. [34] takes a different approach to matching feature counts which allow their\nformulation to avoid the ambiguity in getting many reward functions to derive policies; hence giving\na unique, but randomized, solution as opposed to works like Abbeel and Ng [208]. They employ\na tool, maximum entropy, from statistical mechanics developed by Jaynes [209], which prefers\nhigher reward functions at an exponential rate in probability. So, maximizing the sum, meaning\nall possible observed trajectories, of logarithm of these probabilities across the reward function\nparameters, yields us with a stochastic solution for the reward function. But the suggested gradient\ndescent algorithm has a drawback of recalculating the Markov Decision Process (MDP) in every\nstep. So, we move on to the next work which incorporates these ideas and perform better.\nWulfmeier et al. [35] states that the objective of maximum entropy inverse reinforcement learning\ndeﬁned by Ziebart et al. [34], maximizing the joint posterior distribution of observing the expert\ndemonstration under a given reward structure and model parameters, is fully differentiable with\nrespect to deep neural network weights. Wulfmeier et al. [35] uses fully-connected convolutional\nnetworks (FCCN) to learn the reward model and evaluate the algorithm using expected value\n79\ndifference: difference between value function for the optimal policy obtained using the learned\nreward model and using the ground truth reward. This approach achieves better results when\ncompared to traditional maximum entropy IRL [34].\nFinn et al. [206] extended MaxEntropy [34] formulation for solving high dimensional problems\nby making use of statistic’s tool of importance sampling to estimate the partition function, which\nwas the cause of hindrance in MaxEntropy problem. Ho and Ermon [207] presented a GAN-like\nalgorithm for imitation learning, where the goal is to recover the policy that matched the export\ntrajectories. Finn et al. [210] show that both GCL and GAIL are mathematically equivalent, that is,\nboth converge to the same policy which imitates the expert policy. However, Ho and Ermon [207]\nuse the typical unconstrained form of the discriminator and do not use the generator’s density, and\nthus the reward function remains implicit within the discriminator and cannot be recovered.\n6.4\nApplication to Guidance and Control of Small Unmanned Aerial Vehicles\nMore traditional IRL approaches as [32, 208, 34] requires the MDP to be solved at each learning\niteration in order to optimize the policy for the learned reward function. In our UAV scenario, since\nthe MDP is unknown, it is not possible to solve it at each iteration. Two recent algorithms, as\nexplained in Section 6.2, proposes to address these issues: GCL and GAIL.\nThe GCL and GAIL algorithms are ﬁrst validated on less complex continuous tasks available on\nOpenAI Gym [203] — the Pendulum-v0 and LunarLanderContinuous-v2 environments — and later\ntransferred to a high-ﬁdelity UAV simulator, Microsoft AirSim [204]. For the UAV scenario, the\ndata was collected using the Microsoft AirSim environment [204] modiﬁed to simulate an small\nUAV perching task (landing) on top of a static vehicle, as seen in Figure 9.3. For this task, a UAV\nequipped with a bottom-facing RGB camera starts on a random location ﬂying over the target\nvehicle and have to land in a certain amount of time. The states consists of inertial data of the UAV\n(estimated x, y, and z position, linear and angular velocities) and three additional features of the\nlanding pad extracted using a custom computer vision module running on the background (radius, x\nand y pixel position of the center of the landing pad) — total of 15 features.\n80\n6.5\nResults and Discussion\nThis section details the validation of the GCL and GAIL algorithms using the Pendulum-v0\nand LunarLanderContinuous-v2 OpenAI Gym environments and the transition to Microsoft Airsim\nsimulator to train an agent to perform the UAV landing task.\n6.5.1\nNumerical Results\nThis work started with simpler environments to evaluate and understand how the GCL and\nGAIL algorithms worked, what hyperparameters had the most impact in the performance, and how\nmany expert demonstrations were necessary to learn a satisfactory policy.\nThis evaluation started by using the GCL algorithm on the OpenAI Gym Pendulum-v0 environ-\nment. The Pendulum-v0 task consists of applying a torque on the pendulum joint (one-dimensional\ncontinuous action) in order to keep it in a inverted vertical position. The observartion-space com-\nprises three continuous features: sin(θ), cos(θ), and ˙θ, where θ is the pendulum angle measured\nfrom the inverted vertical position, as seen in Figure 6.1.\nFigure 6.1: Illustration of the Pendulum environment. The agent controls the torque on the base of\nthe pendulum in order to maintain it in the vertical position.\nIn order to evaluate the impact of the quantity and quality of the expert demonstrations to\ninitialize the GCL algorithm, we ran GCL on Pendulum-v0 initializing it with 5, 10, 15, and 20\n81\nclose-to-optimal demonstrations and a ﬁnal run with 200 demonstrations, in which half of them\nwere close-to-optimal and the other half was sub-optimal. Results in terms of average return can\nbe seen in Figure 6.2 when trained for 200 learning iterations. The algorithm performed the best\nwith 10 close-to-optimal demonstration, showing that more demonstrations does not necessarily\ntranslates to better task performance.\nFigure 6.2: Performance in terms of original task reward for different number of expert trajectories\nfor Pendulum GCL.\nThe GAIL algorithm was evaluated using the OpenAI Gym Lunar Lander Continuous envi-\nronment. The LunarLanderContinuous-v2 task consists of controlling the main and side thrusts\n(two-dimensional continuous action) in order to safely land a spacecraft between to ﬂags. The\nobservartion-space comprises eight continuous and discrete features related to the spacecraft’s\nposition and attitude and contact to the ground, as illustrated in Figure 6.3.\nIn order to evaluate the impact of the quantity and quality of the expert demonstrations to\ninitialize the GAIL algorithm, we ran GAIL on LunarLanderContinuous-v2 initializing it with 25,\n125, 1,250, 2,505, and 10,035 close-to-optimal demonstrations. Results in terms of average return\n82\nFigure 6.3: Illustration of the LunarLanderContinuous environment. The agent controls the main\nand side thrusts of the vehicle in order to land safely between he ﬂags.\ncan be seen in Figure 6.4 when trained for 500 learning iterations. The algorithm performed the\nbest with 25 close-to-optimal demonstration, showing one more time that more demonstrations\ndoes not necessarily translates to better task performance. Figure 6.5 shows the performance of this\nbetter performing hyperparameter when trained for 1,000 learning iterations. The complete list of\nhyperparameters can be see in Table 6.1.\n6.5.2\nUnmanned Aerial Vehicle Results\nAfter validation of the GAIL algorithm on a similar but less complex task, the Lunar Lander\nContinuous, we applied GAIL to solve the UAV landing task in Microsoft AirSim. A human pilot\nperformed the landing task for approximately 30 minutes (or 100 demonstrations). This data was\nsaved and used to initialize the GAIL algorithm. Results in terms of original task average return\nduring the hyperparameter tuning phase can be seen in Figure 6.6 when trained for 500 learning\niterations. Figure 6.7 details the performance of the better performing hyperparameter when trained\nfor 500 learning iterations. For the UAV case, GAIL was able to surpass human mean performance\nafter about 100 training iterations and converged to the human performance upper-bound after\nabout 400 learning iterations. We also compared GAIL to a pure RL approach using TRPO (Trust\n83\nFigure 6.4: Performance in terms of original task reward for different number of expert trajectories\nfor LunarLanderContinuous GAIL.\nFigure 6.5: Performance in terms of original task reward for the best performing number of expert\ntrajectories for LunarLanderContinuous GAIL.\n84\nRegion Policy Optimization), as also seen in Figure 6.7. TRPO only reached human performance\nby training iteration 500. The complete list of hyperparameters can be see in Table 6.2 and training\nstatistics in Table 6.3.\nFigure 6.6: Performance in terms of original task reward for different hyperparameters for AirSim\nGAIL.\n6.6\nSummary\nThis chapter presented the application of IRL, speciﬁcally GCL and GAIL, to learn from\nprevious task demonstration and solve continuous observation and action-space problems. Our\napproach using GCL and GAILwas initially validated in less complex environments, as the OpenAI\nGym Pendulum-v0 and LunarLanderContinuous-v2, where we were able to solve the tasks with\nless than 200 learning iterations using as little as 10 demonstrations. The same GAIL approach was\napplied to a UAV landing task, where we were able to surpass human mean performance after 100\nlearning iterations. The same performance using pure RL, speciﬁcally TRPO, takes 400 learning\niterations, or 400% more iterations. GAIL, as demonstrated for LunarLander (see Figure 6.4) and\n85\nFigure 6.7: Performance in terms of original task reward for the best performing hyperparameters\nfor AirSim GAIL.\nUAV (see Figures 6.6 and 6.7), is sample efﬁcient in terms of number of expert trajectories needed\nit to reach a policy which is closer to the expert policy.\n6.6.1\nTraining Hyperparameters\nTable 6.1: Hyperparemeter search for LunarLanderContinuous GAIL\nNo. Expert Traj.\n25\n125\n1250\nHyperparameter\nPolicy NN\n20, 20\n30, 30\n50, 50, 10\nBatch Size\n4000\n7000\n7000\nMax Path Length\n200\n200\n200\nDiscrim. Iters\n100\n100\n100\nDiscount Factor\n0.99\n0.99\n0.99\nPolicy NN\n100, 100, 20\n200, 200, 20\n20, 20\nBatch Size\n7000\n7000\n7000\nMax Path Length\n200\n200\n200\nDiscrim. Iters\n100\n100\n0\nDiscount Factor\n0.99\n0.99\n0.99\n86\nTable 6.2: Hyperparemeter search for AirSim GAIL\nHyperparameter\nv1\nv2\nv3\nPolicy NN\n50, 50, 10\n32, 32\n32, 32\nBatch Size\n60\n60\n1200\nMax Path Length\n60\n60\n60\nDiscrim. Iters\n100\n100\n100\nDiscount Factor\n0.99\n0.99\n0.99\nTable 6.3: Training Statistics for AirSim GAIL\nTraining Time\n48.12 hours\nTrajectories Generated\n10,429\nGAIL Iterations\n489\nTimer per GAIL Iterations\n∼6 minutes\nTime per Trajectory (average)\n16.6 seconds\nHuman Trajectories\n100\nHuman Time\n28 minutes\n87\n7.\nCYBER-HUMAN APPROACH FOR LEARNING HUMAN INTENTION AND SHAPE\nROBOTIC BEHAVIOR BASED ON TASK DEMONSTRATION∗\nRecent developments in artiﬁcial intelligence enabled training of autonomous robots without\nhuman supervision. Even without human supervision during training, current models have yet to be\nhuman-engineered and have neither guarantees to match human expectation nor perform within\nsafety bounds. This paper proposes Cycle-of-Learning to leverage human-robot interaction and\nalign goals between humans and robotic intelligent agents. Based on human demonstration of\nthe task, Cycle-of-Learning learns an intrinsic reward function used by the human demonstrator\nto pursue the goal of the task. The learned intrinsic human function shapes the robotic behavior\nduring training through deep reinforcement learning algorithms, removing the need for environment-\ndependent or hand-engineered reward signal. Two different hypotheses were tested, both using\nnon-expert human operators for initial demonstration of a given task or desired behavior: one\ntraining a deep neural network to classify human-like behavior and other training a behavior cloning\ndeep neural network to suggest actions. In this experiment, Cycle-of-Learning was tested in a\nhigh-ﬁdelity unmanned air system simulation environment, Microsoft AirSim. The simulated aerial\nrobot performed collision avoidance through a clustered forest environment using forward-looking\ndepth sensing. The performance of Cycle-of-Learning is compared to behavior cloning algorithms\nand reinforcement learning algorithms guided by handcrafted reward functions. Results show\nthat the human-learned intrinsic reward function can shape the behavior of robotic systems and\nhave better task performance guiding reinforcement learning algorithms compared to standard\nhuman-handcrafted reward functions.\n∗Adapted with permission from “Cyber-human approach for learning human intention and shape robotic behavior\nbased on task demonstration”, by Vinicius G. Goecks, Gregory M. Gremillion, Hannah C. Lehman, and William D.\nNothwang, presented at the 2018 International Joint Conference on Neural Networks (IJCNN) [7], Copyright 2018 by\nthe Institute of Electrical and Electronics Engineers.\n88\n7.1\nProblem Deﬁnition\nIntelligent robots have the potential to positively impact and augment human activities in various\ntasks and modalities. Part of this success will depend on how they are integrated and the underlying\nmotivation that drives the behavior of these intelligent robots. To comply with this need, it is\nbeneﬁcial to design a framework to train and shape the behavior of intelligent robotic agents to\ncomply with human intention.\nOne of the approaches to shape the behavior of robotic agents is to formulate the process as\na reinforcement learning problem. The robot (called agent) senses its surrounding environment\n(observation) through onboard sensors. The robot’s controller (policy) selects that adequate control\ninput (action) in order to maximize a given metric of performance (reward signal). The policy is\ntrained based on its interaction with the environment in order to maximize the reward signal, which\ncan be thought of as the goal of the intelligent agent.\nThis paper proposes the CyberSteer framework to better understand how to use human resources\nto train robotic agents in an environment without an explicit metric of performance (reward signal).\nThe main research problem addressed by this paper is how to leverage initial human demonstration\nof the task to learn an intrinsic reward function used by the human to pursue the goal of the task.\nThis learned intrinsic human function then shapes the robotic behavior during training through deep\nreinforcement learning (Deep RL) algorithms, aligning goals between humans and these intelligent\nagents.\nForming a reward signal is a challenge in real-world tasks. While games have clearly deﬁned\nrewards from the game score, rewards in real-worlds tasks are currently handcrafted and hard-coded\nfor each task based on some a priori knowledge of the possible state and clear goal. The challenge\nis in developing a method to translate human intention to a reward function in a framework that can\nbe applied to arbitrary tasks.\nCyberSteer provides a novel framework to integrate humans to robotic learning agents providing\ntask demonstration and operating as an intervention mechanism to provide safety during learning\nand exploration. CyberSteer acts as an Intrinsic Reward Module (IRM) which provides the reward\n89\nsignal for the Deep RL agent.\nTwo different hypotheses were tested, both using non-expert human operators for initial demon-\nstration of a given task or desired behavior. CyberSteer #1 collected these demonstrated trajectories\nand trained a deep neural network to classify human-like behavior. CyberSteer #2 trained a behavior\ncloning deep neural network that asynchronously ran in the background suggesting actions to the\nDeep RL module.\nThe human is not required to be an expert on the task, only to have limited knowledge of its\nhigh-level goal and be able to perform rudimentary elements of the task.\nThe framework is designed for real-world robotic applications where external rewards are not\npresent and safe environment exploration is a concern. CyberSteer is tested in a high-ﬁdelity\nunmanned air system (UAS) simulation environment, Microsoft AirSim. The simulated aerial robot\nperforms collision avoidance through a clustered forest environment using forward-looking depth\nsensing and roll, pitch, and yaw references angle commands to the ﬂight controller.\nThe proposed approach is compared to a direct behavior cloning deep neural network trained\nusing the human demonstration dataset and a deep reinforcement learning algorithm, Deep Deter-\nministic Policy Gradient, guided by a handcrafted reward signal. Evaluation is quantiﬁed by the\nalignment of the agents actions with human inputs and completion of the task.\n7.2\nBackground\n7.2.1\nPreliminaries\nIn reinforcement learning problems it is desired to train an agent to learn the parameters θ of\na policy πθ, in order to map the environment’s state vectors s (sampled from a distribution S) to\nagent actions a (sampled from a distribution A). The performance of the agent is measured by a\nreward signal returned by the environment (external rewards, re) and/or returned by the agent itself\n(intrinsic rewards, ri). At each time step t the reward signal can be computed as the sum of all the\nextrinsic and intrinsic rewards received (Equation 7.1).\n90\nrt = ret + rit\n(7.1)\nAn episode is deﬁned as the time interval between the initial time step t0 and the maximum\nnumber of time steps allowed T or until a previously established objective is achieved. The total\nreward per episode R is deﬁned as the sum of the rewards received for each time step, as shown in\nEquation 7.2.\nR =\nT\nX\nt=0\nrt =\nT\nX\nt=0\n(ret + rit)\n(7.2)\nThe expected total reward per episode received by a policy πθ(at|st) can be deﬁned by Equation\n7.3:\nRπθ =\nT\nX\nt=0\nE\nat∼πθ[rt(st, at)]\n(7.3)\n7.2.2\nBehavior Cloning\nBehavior Cloning, or Imitation Learning, is deﬁned by training a policy π in order to replicate\nan expert’s behavior given states and actions visited during an expert demonstration\nD = {a0, s0, a1, s1, ..., aT, sT} .\nThis demonstration can be performed by a human supervisor, optimal controller, or virtually any\nother pre-trained policy π.\nIn the case of human demonstrations, the human expert is implicitly trying to maximize the\nreward function of a given task, as shown in Equation 9.1, where π∗(a∗\nt|st) represents the optimal\npolicy (not necessarily known) in which the optimal action a∗is taken at state s for every time step\nt.\n91\nmax\na0,...,aT\nT\nX\nt=0\nrt(st, at) =\nT\nX\nt=0\nlog p(π∗(a∗\nt|st))\n(7.4)\nDeﬁning the policy of the expert supervisor as πsup and its estimated policy as ˆπsup, behavior\ncloning can be achieved through standard supervised learning (where the parameters θ of a policy πθ\nare ﬁt in order to minimize a loss function - such as mean squared error - as shown in Equation 9.2)\nor using more advanced methods, such as Dataset Aggregation [26] (DAgger, where data collected\nby the estimated policy is aggregated with data provided by the expert), Generative Adversarial\nImitation Learning [207] (where generative adversarial networks are used to ﬁt distributions of\nstates and actions deﬁned by expert behavior), or Guided Cost Learning [206] (where regularized\nneural networks and a cost learning algorithm based on policy optimization are used to learn the\nexpert’s cost function under unknown dynamics and high-dimensional continuous systems).\nˆπsup = argmin\nπθ\nT\nX\nt=0\n||πθ(st) −at||2\n(7.5)\n7.2.3\nDeep Deterministic Policy Gradient\nThe Deep Deterministic Policy Gradient (DDPG) algorithm [85] is a model-free, off-policy,\nactor-critic algorithm that combines the Deterministic Policy Gradient (DPG) algorithm [84] to\ncompute the policy’s gradient and the insights gained by the Deep Q-Network (DQN) algorithm\n[73, 74] to train the critic using Q-updates as, for example, the Experience Replay and Target\nNetworks. Experience replay and target networks has been proven [73, 74, 211] to improve and\nstabilize policies learned using reinforcement learning. Both actor and critic are represented by\ndeep neural networks.\nThe DDPG algorithm was selected for being able to process high-dimensional continuous state\nspace, characteristic of robotic environments, and command continuous actions, similar to the\nhuman supervisor. To improve the state exploration performance in continuous spaces a noise\ncomponent, sampled from a noisy process N, is added to the agent’s policy which affects the\nselected action.\n92\n7.3\nLearning Human Intention and Shaping Robotic Behavior\n7.3.1\nThe CyberSteer Mechanics\nA summarized diagram of the mechanics of CyberSteer can be seen in Figure 7.1, which will\nsupport the explanation below.\nGiven an autonomous robotic agent initialized with an untrained policy (which maps the current\nobservations to the next action), the human supervisor performs the desired task in order to collect\ndemonstration data (observations and actions used by the human) safely performing an initial\nenvironment exploration. From this point, two hypotheses are being tested to efﬁciently use this\nhuman data:\n1. CyberSteer #1 (Detailed in Algorithm 2) Trains a convolutional recurrent neural network\nbased on collected human data and random behavior to classify actions taken as being\nprovided by human or not (human-like actions), Figure 7.2. The estimated reward ˆr to be\nFigure 7.1: Overall Diagram of the CyberSteer framework. Reprinted from [7].\n93\nFigure 7.2: CyberSteer #1: Computing estimated rewards based on the likelihood of the action\ntaken being similar to previous actions taken by the human supervisor. Reprinted from [7].\nreturned to the Deep RL algorithm is calculated based on the likelihood that the action taken\nis similar to the action executed by a human p(a = ahuman|(s, a)), as shown in Equation 7.6.\nˆr = rmax(2 · p(a = ahuman|(s, a)) −1)\n(7.6)\nThe estimated reward is bounded between rmax (when p(a = ahuman|(s, a)) = 1) and −rmax\n(when p(a = ahuman|(s, a)) = 0). CyberSteer # 1 uses a convolutional network with two\nconvolutional layers (32 and 16 ﬁlters, 3x3 kernels, 1 pixel stride, no padding, using rectiﬁed\nlinear unit (ReLU) activation function and max pooling to reduced the size of the input in half\nafter each layer), 8 LSTM recurrent units, two fully-connected layers with 64 neurons each,\nfollowed by a ReLU activation function, and the fully connected output layer, followed by a\nsigmoid function. Adam algorithm [202] is used for optimization of a binary cross-entropy\nloss function.\n2. CyberSteer #2 (Detailed in Algorithm 3) Trains a behavior cloning neural network using\nconvolutional and recurrent layers to suggest actions to the Deep RL algorithm that would\nhave been taken by the human supervisor, Figure 7.3. The mean square error (aMSE) between\nthe action suggested by the behavior cloning (aBC) and taken by the Deep RL algorithm\n(aDRL) is used to compute an estimated reward (Equation 7.7, where aMSE = (aBC −aDRL)2)\nto guide the Deep RL algorithm.\n94\nAlgorithm 2 CyberSteer #1\n1: Human controls the robot and demonstrates the task to be executed. States (s) and actions\n(a) are recorded using onboard sensors and compiled as a human-controlled dataset DH =\n{a0, s0, a1, s1, ..., aT, sT}.\n2: Robot performed magnitude-controlled random actions. States (s) and actions (a) are recorded\nusing onboard sensors and compiled as a non-human-controlled dataset DNH.\n3: Datasets DH and DNH are reorganized in order to have three sequences of actions stacked as\none data point.\n4: Datasets DH and DNH are aggregated and shufﬂed.\n5: Convolutional recurrent neural network is trained using the aggregated dataset in order to\nclassify between human and non-human behavior - the CyberSteer #1 network.\n6: Controls are completely transferred to robot, and its Deep Reinforcement Learning (Deep RL)\nmodule is initialized. Robot is trained:\n7: while Desired behavior is not achieved do\n8:\nRobot’s Deep RL module selects next action.\n9:\nif Human judges robot’s behavior is inadequate then\n10:\nHuman intervention. Provides new demonstration.\n11:\nBatch update CyberSteer #1 network.\n12:\nelse\n13:\nCyberSteer #1 network evaluates from 0 to 100% how similar the action is to human\nbehavior.\n14:\nEstimated reward is computed based on Equation 7.6 by the Intrinsic Reward Module\n(IRM).\n15:\nUpdate Robot’s Deep RL module.\n95\nFigure 7.3: CyberSteer #2: Computing estimated rewards based on how similar the actions taken by\nthe agent are when compared to a behavior cloning network trained with demonstration form the\nhuman supervisor. Reprinted from [7].\nˆr = −rmax (1 −√aMSE)\n(7.7)\nThe estimated reward is bounded between rmax (when aMSE = 0) and −rmax (when aMSE =\n4). CyberSteer # 2 uses a convolutional network with two convolutional layers (32 and 16\nﬁlters, 3x3 kernels, 1 pixel stride, no padding, using rectiﬁed linear unit (ReLU) activation\nfunction and max pooling to reduce the size of the input in half after each layer), 8 LSTM\nrecurrent units, one fully-connected layers with 64 neurons followed by a ReLU activation\nfunction, and the fully connected output layer followed by a sigmoid function. Adam algorithm\n[202] is used for optimization of a binary cross-entropy loss function.\n7.3.2\nEnvironment and Task Modeling\n7.3.2.1\nUnmanned Air System Simulation Environment\nThe proposed approach was ﬁrst developed and tested using the Microsoft AirSim platform [204]\n- a high-ﬁdelity simulated environment for unmanned air systems (UAS) ﬂight and control. This\nplatform simulates the UAS ﬂight dynamics, onboard inertial sensors (GPS, barometer, gyroscope,\naccelerometer, and magnetometer), onboard camera data (monocular RGB, ground truth depth\nsensor, and random image segmentation), and collision and environment physics.\nThe authors refer the reader to the original paper [204] for more details about the architecture,\n96\nAlgorithm 3 CyberSteer #2\n1: Human controls the robot and demonstrates the task to be executed. States (s) and actions\n(a) are recorded using onboard sensors and compiled as a human-controlled dataset DH =\n{a0, s0, a1, s1, ..., aT, sT}.\n2: Dataset DH is reorganized in order to have three sequences of actions stacked as one data point\nand dataset is shufﬂed.\n3: Behavior Cloning network is trained is order to suggest actions based on past human demon-\nstration.\n4: Controls are completely transferred to robot, and its Deep Reinforcement Learning (Deep RL)\nmodule is initialized. Robot is trained:\n5: while Desired behavior is not achieved do\n6:\nRobot’s Deep RL module selects next action.\n7:\nif Human judges robot’s behavior is inadequate then\n8:\nHuman intervention. Provides new demonstration.\n9:\nBatch update CyberSteer #2 network (Behavior Cloning).\n10:\nelse\n11:\nCyberSteer #2 network (Behavior Cloning) suggests action based on current robot’s\nstates.\n12:\nEstimated reward is computed based on Equation 7.7 by the Intrinsic Reward Module\n(IRM).\n13:\nUpdate Robot’s Deep RL module.\nsimulation parameters, accuracy of the model, and ﬂight characteristics when compared to a real\nUAS ﬂying in real-world.\n7.3.2.2\nCollision Avoidance Scenario\nThe proposed algorithm was ﬁrst tested on a collision avoidance scenario where the agent had to\nmaneuver the unmanned air system using roll commands while automatic ﬂying forward at constant\naltitude. A warehouse environment was created using Unreal Engine for visually realistic textures\nand objects. A sample of the forest scenario used can be seen in Figure 7.4.\n7.4\nNumerical Results\nBuilding onto the behavior cloning concepts explained in Subsection 7.2.2, it was desired\nto validated that the autonomous agent would be able to learn, to some degree, to perform the\ndemonstrated collision avoidance maneuvers using the behavior cloning network. The behavior\ncloning network is a critical component of the proposed framework as it is subsequently used to\n97\nFigure 7.4: Warehouse scenario created for the collision avoidance task using Unreal Engine for\nvisually realistic textures and objects. Reprinted from [7].\nevaluate the actions proposed by the Deep RL algorithm for one of the hypotheses.\nA non-expert human piloted the unmanned air system performing collision avoidance maneuvers\nfor 12,000 time steps (approximately 20 minutes, performing actions at 20 Hz). The UAS was\nset to have constant altitude and constant forward velocity and images were collected from the\nonboard depth sensor and resized to 36 by 64 pixels. The human sent roll angle control inputs to\nsteer laterally and avoid incoming obstacles. These images and control inputs were compiled as\nhuman-controlled dataset DH.\nA convolutional network with three convolutional layers (32, 64, and 128 ﬁlters, 3x3 kernels,\n1 pixel stride, no padding, using rectiﬁed linear unit (ReLU) activation function and max pooling\nto reduced the size of the input in half after each layer) and two fully-connected layers connected\nby a 50% dropout layer during training was trained to imitate human performance. Three frames\nwere stacked together to extract velocity of the scene. For the ﬁrst experiment, the network was\ntrained ofﬂine and deployed to the agent for testing. Video of the learning performance can be seen\nat: https://www.youtube.com/watch?v=rEbNytWz3c8.\nThe behavior cloning experiment was followed by the evaluation of both hypotheses. The\nmain goal was to create an Intrinsic Reward Module (IRM) that would be able to guide a Deep RL\nalgorithm.\n98\nCyberSteer # 1 combined the human-controlled dataset DH (collected for the behavior cloning\nnetwork) to a dataset of images and control inputs generated at random by the robot performing\nthe task: the non-human-controlled dataset DNH. While the agent was performing random actions,\nthe human had total control authority to intervene and control the robot to prevent any damage\nto the platform. Both datasets, DH and DNH, were aggregated and shufﬂed and the CyberSteer\n# 1 network was trained to classify image-action pairs between human and non-human actions\nassociating a number between 0 and 1 based on how similar the image-action pairs were compared\nto the collected human behavior. CyberSteer # 1 network was deployed to the IRM in order to guide\na Deep RL algorithm (Deep Deterministic Policy Gradient: DDPG) by using this similarity metric\nas a reward function.\nCyberSteer # 2 uses the collected human-controlled dataset DH to train a behavior cloning\nnetwork. When the Deep RL agent is controlling the robot, CyberSteer # 2 IRM runs asynchronously\nin the background comparing the actions taken by the agent and what would have been taken by\nthe CyberSteer # 2 network (behavior cloning). Based on how similar the action taken is when\ncompared to the suggested one, a reward signal is generated and guides the Deep RL algorithm.\nA human reward baseline was established based on the maximum possible total reward for\nthe task. In the case of CyberSteer #1, all actions proposed by the Deep RL would be classiﬁed\nwith 100% conﬁdence by the IRM as being human-like actions. In the case of CyberSteer #2, all\nactions proposed by the Deep RL would 100% match the actions suggested by the IRM running a\nbehavior cloning network based on the initial demonstration of the task. As a baseline, the proposed\nsolution was compared to the same Deep RL algorithm being fed by a standard human-engineered\nreward function returned by the environment based on the task completion, i.e., the fraction of the\nenvironment’s total distance traversed.\nFigure 7.5 shows the result of this ﬁrst comparison for 500 episodes of the task being performed\nfor about 30 seconds. CyberSteer #2 performed seven orders of magnitude better than CyberSteer\n#1 and the baseline.\nFigure 7.6 shows the comparison in terms of task completion using the proposed solution and the\n99\nFigure 7.5: Comparison of the proposed solutions to achieve human-like performance on the task\nwith no feedback from the environment when compared to a baseline dependent on environment\nreward signals. Reprinted from [7].\nestablished baseline. Even though motivated by different reward signals both proposed approaches\nand the baseline performed similar in terms of task completion during training.\nSince we are using deep neural networks as learning representation, there is still no theoretical\nconvergence guarantee that the algorithm will lead to the optimal task behavior.\n7.5\nSummary\nThis paper proposed CyberSteer to leverage human-robot interaction to train autonomous robots\naligning their goals to comply with human intention. CyberSteer frames the training task as a\nreinforcement learning problem: an agent interacting with the environment to maximize a reward\nsignal that represents the goal of the task. The learned human intention is instantiated as an\nintrinsic reward signal to shape the robotic behavior during training when combined with Deep\nReinforcement Learning algorithms.\nTwo different hypotheses were tested to translate human intention as reward signal, both using\nnon-expert human operators for initial demonstration of a given task or desired behavior. CyberSteer\n100\nFigure 7.6: Task completion performance of the proposed solutions with no feedback from the\nenvironment when compared to the established baseline dependent on environment reward signals.\nReprinted from [7].\n#1 collected these demonstrated trajectories and trained a deep neural network to classify human-like\nbehavior. CyberSteer #2 trained a behavior cloning deep neural network that asynchronously ran in\nthe background suggesting actions to the Deep Reinforcement Learning module.\nIn this experiment, CyberSteer was tested in a high-ﬁdelity unmanned air system simulation\nenvironment, Microsoft AirSim. The simulated aerial robot performed collision avoidance through\na clustered forest environment using forward-looking depth sensing.\nCyberSteer #2 initially showed seven-fold improvement in performance when compared to\na human-engineered approach when training autonomous agents. The approach showed that\nmodern Deep Reinforcement Learning algorithms can be adapted to be guided by reward signals\nreconstructed from human demonstration, with no need to have a human expert to handcraft a\nreward function. This enables the same algorithm to be deployed in different real-world applications,\nas long as the task can be demonstrated beforehand.\nBoth CyberSteer approaches were still able to perform similarly well to standard approaches\ndependent on environment-returned and human-engineered reward signals when driven by modern\n101\nFigure 7.7: Extended plots of the comparison of the proposed solutions to achieve human-like\nperformance on the task with no feedback from the environment when compared to a baseline\ndependent on environment reward signals. Reprinted from [7].\nDeep Reinforcement Learning algorithms.\nCurrently, both approaches show decrease of performance after the ﬁrst 100 episodes, as seen in\nFigures 7.7 and 7.8. This will be further investigated when performing different tasks.\n102\nFigure 7.8: Extended plots of the task completion performance of the proposed solutions with\nno feedback from the environment when compared to the established baseline dependent on\nenvironment reward signals. Reprinted from [7].\n103\n8.\nCYCLE-OF-LEARNING FOR AUTONOMOUS SYSTEMS FROM HUMAN\nINTERACTION∗\nWe discuss different types of human-robot interaction paradigms in the context of training\nend-to-end reinforcement learning algorithms. We provide a taxonomy to categorize the types\nof human interaction and present our Cycle-of-Learning framework for autonomous systems that\ncombines different human-interaction modalities with reinforcement learning. Two key concepts\nprovided by our Cycle-of-Learning framework are how it handles the integration of the different\nhuman-interaction modalities (demonstration, intervention, and evaluation) and how to deﬁne the\nswitching criteria between them.\n8.1\nProblem Deﬁnition\nReinforcement learning (RL) has been successfully applied to solve challenging problems from\nplaying video games to robotics. In simple scenarios, a reward function or model of the environment\nis typically available to the RL algorithm, and standard RL techniques can be applied. In real-world\nphysical environments where reward functions are not available or are too intractable to design by\nhand, these standard RL methods often tend to fail. Using humans to train robotic systems is a\nnatural way to overcome these burdens.\nThere have been many examples in the ﬁeld of human-robot interaction where human interaction\nis used to train autonomous systems in the context of end-to-end reinforcement learning. For\nexample, imitation learning is an approach where demonstrations of a task provided from a human\nare used to initially train an autonomous system to learn a policy that “imitates” the actions of\nthe human. A recent approach termed Human Centered Reinforcement learning (HCRL) trains\nautonomous systems using positive and negative feedback provided from a human trainer, showing\npromising strides for learning policies in the absence of a reward function. Approaches that learn\n∗Adapted with permission from “Cycle-of-Learning for Autonomous Systems from Human Interaction”, by\nNicholas R. Waytowich, Vinicius G. Goecks, Vernon J. Lawhern, presented at the 2018 Artiﬁcial Intelligence and\nHuman-Interaction (AI-HRI) AAAI Fall Series Symposium [1], available under a Creative Commons license at the\nAI-HRI/2018/05 proceedings at https://arxiv.org/html/1809.06606.\n104\ncontrol policies jointly with humans and autonomous systems acting together in a shared autonomy\nframework have also been developed.\nEach of these human-interaction approaches, which have their own unique advantages and\ndisadvantages, have mostly been utilized in isolation without taking into consideration the varying\ndegrees of human involvement; the human is a valuable, but ﬁnite, source of information that can\ncommunicate information in many ways. A key question to consider is not only what information\nthe human should convey, but how this information should be conveyed. A method for combining\nthese different paradigms is needed to enable efﬁcient learning from human interaction.\nIn this paper, we present a new conceptual framework for using humans to train autonomous\nsystems called the Cycle-of-Learning for Autonomous Systems from Human Interaction. This\nframework fuses together different human-interaction modalities into a single learning paradigm\ninspired by how humans teach other humans new tasks. We believe this intuitive concept should\nbe employed whenever humans are interacting with autonomous systems. Our contributions in\nthis paper are as follows: we ﬁrst describe a taxonomy of learning from human interaction (with\ncorresponding literature) and list the advantages and disadvantages of each learning modality.\nWe then describe the Cycle-of-Learning (Figure 1.1), our conceptual framework for intuitively\ncombining these interaction modalities for efﬁcient learning for autonomous systems. We describe\nthe different stages of our framework in the context of the degree of human involvement (and,\nconversely, the amount of autonomy of the system). We conclude with potential avenues for future\nresearch.\n8.2\nTypes of Learning from Human Interaction\nThere are many different ways a human may be used to train an autonomous system to perform\na task, each of which can be broadly categorized by the modality of human-interaction. Here we\npresent a taxonomy that categorizes most types of human interactions into one of three method-\nologies: Learning from human demonstrations (LFD), learning from human interventions (LFI)\nand learning from human evaluations (LFE). This taxonomy is partitioned based on the amount\nof control that the human or Autonomous system has during the learning process. In LFD the\n105\nhuman is providing demonstrations and is in full control. In LFI, where the human occasionally\nintervenes, both the human and autonomous system share control. In LFE, where the human is\nproviding evaluative feedback as the autonomous system performs a task, the autonomous systems\nis in control. We describe rec ent research efforts for each of these categories and present their\nrelative advantages and disadvantages.\n8.2.1\nLearning from Human Demonstrations\nLearning from human Demonstrations (LFD) is from a broad class of techniques called Imitation\nLearning (IL) where the aim is to train autonomous systems to mimic human behavior in a given\ntask. In this interaction paradigm, the human takes the role of a demonstrator to provide examples of\nthe task in the terms of sequences of states and actions. Using these demonstrations, the autonomous\nsystem learns a policy (a mapping from states to actions) that mimics the human demonstrations.\nThere are many empirical successes of using imitation learning to train autonomous systems. For\nself-driving cars, Bojarski et al. successfully used IL to train a policy that mapped from front-facing\ncamera images to steering wheel commands using around one hundred hours of human driving\ndata [19]. A similar approach was taken to train a small unmanned air system (sUAS) to navigate\nthrough forest trails, but in this case data was collected by a human following the trail [20].\nThese results highlight both advantages and disadvantages of imitation learning approaches. IL\ncan be used end-to-end to learn a new task, overcoming initial exploration of randomly-initialized\nlearning algorithms while having better convergence properties because it trains on static datasets.\nHowever, the major drawback is that policy performance and generalization rely on the diversity and\nquality and size of the training dataset — often requiring large amounts of demonstrated behavior.\nInverse Reinforcement Learning (IRL), also known as apprenticeship learning, is another form of\nlearning from human demonstrations where a reward function is learned based on task demonstration.\nThe idea is that during the demonstration, the human is using a policy that is optimal with respect\nto some internal reward function. However, in IRL, the quality of the reward function learned is\nstill dependent on the demonstration performance. In addition, IRL is fundamentally underdeﬁned\n(degenerated), in a sense that different reward functions can lead to the same behavior [32, 206].\n106\nIRL focuses on learning the reward function, which is believed to be a more robust and transferable\ndeﬁnition of the task when compared to the policy learned [32].\n8.2.2\nLearning from Human Interventions\nLearning From human Interventions (LFI) is a much less explored method of human interaction\nfor training autonomous systems. Simply put, the human acts as an overseer and intervenes (i.e.\ntakes control) when the autonomous system is about to enter a catastrophic state. This is especially\nimportant for training embodied autonomous systems, such as quadrotors or ground robots, that\nare learning and interacting in a real environment where sub-optimal actions could lead to damage\nto the systems itself or the surrounding environment. Recently, this concept was formalized in a\nframework called learning from Human Interventions for safe Reinforcement Learning (HIRL)\n[39]. In HIRL, when the human observes the agent perform sub-optimal actions that could lead to a\nfailure state, the human “intervenes” by blocking the current action and providing an alternative\naction. Additionally, a negative reward signal is given to the AI system during the intervention to\nlearn from.\nBy using humans to perform an initial state exploration and prevent catastrophic actions,\nLFI is a promising approach to solve the exploitation-exploration dilemma and increase safety\nin RL. Additionally, there is evidence that shared autonomy scenarios can also improve human\nperformance [44]. However, LFI by itself does not scale to more complex environments where\nmillions of observations are required for successful learning, increasing the required amount of\nhuman supervision [39]).\n8.2.3\nLearning from Human Evaluations\nAnother human-interaction modality that we consider is Learning From human Evaluations\n(LFE). In LFE, the human acts as a supervisor and provides real-time evaluations (or critiques) to\ninteractively shape the behavior of the autonomous system. There are several different approaches\nfor how to provide the human feedback for LFE. One of the simplest approaches is ﬁtting a reward\nfunction based on binarized feedback; for example, “good” vs “bad” actions, indicating positive and\n107\nnegative reward, respectively. Existing frameworks that use this approach include TAMER [45, 47]\nand COACH [46]. Another approach asks the human to rank-order a given set of trajectories, or\nshort movie clips representing a sequence of state-action pairs, and learning a reward function in\na relational manner based on human preferences [54]. Both approaches have been shown to be\neffective across a variety of domains and illustrates the utility of using human-centered reward\nshaping for shaping the policy of autonomous systems.\nAn advantage of LFE techniques is that they do not require the human to be able to perform\ndemonstrations and only require an understanding of the task goal. However, if the time-scale\nof the autonomous system is faster than human reaction time, then it can be challenging for the\nautonomous system to attribute which actions correspond to the provided feedback. In addition,\nthe human reward signals are generally non-stationary and policy-dependent, i.e.: what was a good\naction in the past may not be a good action in the present depending on the humans perception of\nthe autonomous system’s policy.\n8.3\nThe Cycle-of-Learning Concept\nThe Cycle-of-Learning is a framework for training autonomous systems through human interac-\ntion that is based on the intuition on how a human would teach another human to perform a new task.\nFor example, teachers conveying new concepts to their students proceed ﬁrst by demonstrating the\nconcept, intervening as needed while students are learning the concept, then providing critique after\nstudents have started to gain mastery of the concept. This process is repeated as new concepts are\nintroduced. While extensive research has been conducted into each of these stages separately in the\ncontext of machine learning and robotics, to the best of our knowledge, a model incorporating each\nof these aspects into one learning framework has yet to be proposed. We believe such a framework\nwill be important to ﬁelding adaptable autonomous systems that can be trained on-the-ﬂy to perform\nnew behaviors depending on the task at hand, in a manner that does not require expert programming.\nUnder the proposed Cycle-of-Learning framework (Figure 1.1), we start with LFD where a\nhuman would be asked to provide several demonstrations of the task. This demonstration data\n(observations received and actions taken) constitute the initial human dataset DH. The dataset\n108\nAlgorithm 4 Cycle-of-Learning Framework\n1: procedure LEARNING FROM DEMONSTRATION\n2:\nwhile SwitchingFunctionLFD : do\n3:\nCollect human demonstration data DH\n4:\nTrain imitation learning policy πθDH\n5:\nLearn human reward function RH from DH\n6: procedure LEARNING FROM INTERVENTION\n7:\nwhile SwitchingFunctionLFI : do\n8:\nAutonomous system performs the task\n9:\nif Human intervenes then:\n10:\nCollect human intervention data DI\n11:\nAggregate DH ←DH ∪DI\n12:\nUpdate imitation learning policy πθDH\n13:\nUpdate human reward function RH\n14:\nCompute intervention reward RI\n15:\nTrain critic QH using RI and TD error\n16:\nUpdate policy πθDH using policy gradient\n17: procedure LEARNING FROM EVALUATION\n18:\nwhile SwitchingFunctionLFE : do\n19:\nCollect human evaluation reward rH\n20:\nUpdate critic QH using rH and TD error\n21:\nUpdate policy πθDH using policy gradient\n22:\nUpdate human reward function RH with rH\n23: procedure REINFORCEMENT LEARNING\n24:\nwhile SwitchingFunctionRL : do\n25:\nAutonomous system performs the task\n26:\nCompute rewards using RH\n27:\nUpdate critic QH using RH and TD error\n28:\nUpdate policy πθDH using policy gradient\nDH feeds an imitation learning algorithm to be trained via supervised learning, resulting in the\npolicy πθDH. In parallel to the policy training, the dataset DH is used by an Inverse Reinforcement\nLearning (IRL) algorithm to infer the reward function RH used by the human while demonstrating\nthe task (Algorithm 4, line 1).\nOn LFI (Algorithm 4, line 6) the autonomous system performs the task according to the\npolicy πθDH. During the task the human is able to intervene by taking over the control of the\nautonomous system, perhaps to avoid catastrophic failure, and provides more demonstrations during\n109\nthe intervention. This new intervention dataset DI is aggregated to the previous human dataset\nDH. Using this augmented dataset, the policy πθDH and the reward model RH are updated. An\nintervention reward RI is computed based on the degree of the intervention. The reward signal RI\nand the temporal-difference (TD) error associated with it are used to train a value function QH (the\ncritic) and evaluate the actions taken by the actor. At this point, the policy πθDH is updated using\nactor-critic policy gradient methods.\nAfter the human demonstration and intervention stages, the human assumes the role of a\nsupervisor who evaluates the autonomous system actions through a reward signal rH — Learning\nfrom Evaluation (LFE, Algorithm 4, line 17). Similarly to the LFI stage, the reward signal rI and\nthe TD error associated with it are used to update the critic QH and the policy πθDH. The reward\nmodel RH is also updated according to the signal rH plus the observations and actions associated\nwith it.\nThe ﬁnal stage is pure Reinforcement Learning (RL). The autonomous system performs the\ntask and its performance is evaluated using the learned reward model RH (Algorithm 4, line 23).\nSimilar to the LFI and LFE stages, the reward signal RH and the TD error associated with it are\nused to update the critic QH and the policy πθDH. This sequential process is repeated as new tasks\nare introduced.\n8.3.1\nIntegrating and Switching Between Human Interaction Modalities\nTwo key concepts of the Cycle-of-Learning framework are how to handle the integration of\nthe learned models from the different interaction modalities (demonstration, intervention, and\nevaluation) and how to deﬁne the criteria to switch between them. First, to integrate the different\ninteraction modalities, we propose using an actor-critic architecture [153]: initially training only the\nactor, and later adding the critic. Training the actor ﬁrst allows the framework to leverage the initial\ndemonstration and intervention provided by the human. The critic is then trained as the human\nassumes the role of supervisor. After enough human demonstration data has been collected we can\ninfer a reward function through IRL. At the end, the actor and critic are combined on a standard\nactor-critic reinforcement learning architecture driven by the learned reward model.\n110\nSecond, we propose different concepts to deﬁne a criteria to switch between interaction modali-\nties: performance metrics, data modality limitation, and advantage functions. Performance metrics:\nA pre-deﬁned performance metric can be used to indicate when to switch modalities once the policy\nreaches a certain level. Alternatively, the human interacting with the system could manually switch\nbetween different interaction modalities as s/he observes that the autonomous system performance\nis not increasing. Data modality limitations: Depending on the task, there can be a limited amount\nof demonstration, intervention, or evaluation that can be provided by humans. In this case, the\nframework switches between modalities according to data availability. Advantage functions: After\ntraining the reward model RH, advantages A(s, a) (the difference between the state-action value\nfunction Q(s, a) and the state value function V (s), which compares the expected return of a given\nstate-action pair to the expected return on that state) can be computed and used for expected return\ncomparison between human and autonomous systems actions. With this information, the framework\ncould switch interaction modalities whenever the advantage function of the autonomous system\nsurpasses the advantage function of the human. These, as well as other potential concepts for\nmodality switching, need to be further investigated and can be adapted to meet task requirements.\n8.4\nSummary\nThis paper presents the Cycle-of-Learning framework, envisioning the integration between\ndifferent human-interaction modalities and reinforcement learning algorithms in an efﬁcient manner.\nThe main contributions of this work are (1) the formalization of the underlying learning architecture\n— ﬁrst leveraging human demonstrations and interventions to train an actor policy and reward\nmodel, then gradually moving to training a critic and ﬁne-tuning the reward model based on the\nsame interventions and additional evaluations, to ﬁnally combining these different parts on an\nactor-critic architecture driven by the learned reward model and optimized by a reinforcement\nlearning algorithm — and (2) the switching between these human-interaction modalities based on\nperformance metrics, data modality limitations, and/or advantage functions.\nWe believe the proposed Cycle-of-Learning framework is most suitable for robotic applications,\nwhere both human and autonomous system resources are valuable and ﬁnite. As future work, it is\n111\nplanned to demonstrate these techniques on a human-sUAS (small unmanned air system) scenario.\n112\n9.\nEFFICIENTLY COMBINING HUMAN DEMONSTRATIONS AND INTERVENTIONS\nFOR SAFE TRAINING OF AUTONOMOUS SYSTEMS IN REAL-TIME∗\nThis paper investigates how to utilize different forms of human interaction to safely train\nautonomous systems in real-time by learning from both human demonstrations and interventions.\nWe implement two components of the Cycle-of-Learning for Autonomous Systems, which is our\nframework for combining multiple modalities of human interaction. The current effort employs\nhuman demonstrations to teach a desired behavior via imitation learning, then leverages intervention\ndata to correct for undesired behaviors produced by the imitation learner to teach novel tasks to\nan autonomous agent safely, after only minutes of training. We demonstrate this method in an\nautonomous perching task using a quadrotor with continuous roll, pitch, yaw, and throttle commands\nand imagery captured from a downward-facing camera in a high-ﬁdelity simulated environment.\nOur method improves task completion performance for the same amount of human interaction when\ncompared to learning from demonstrations alone, while also requiring on average 32% less data\nto achieve that performance. This provides evidence that combining multiple modes of human\ninteraction can increase both the training speed and overall performance of policies for autonomous\nsystems.\n9.1\nProblem Deﬁnition\nThe primary goal of learning methodologies is to imbue intelligent agents with the capability to\nautonomously and successfully perform complex tasks, when a priori design of the necessary\nbehaviors is intractable. Most tasks of interest, especially those with real-world applicability,\nquickly exceed the capability of designers to handcraft optimal or even successful policies. It can\neven be infeasible to construct appropriate objective or reward functions in many cases. Instead,\nlearning techniques can be used to empirically discover the underlying objective function for the\n∗Adapted with permission from “Efﬁciently combining human demonstrations and interventions for safe training\nof autonomous systems in real time”, by Vinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern, John Valasek,\nand Nicholas R. Waytowich, presented at the 2019 AAAI Conference on Artiﬁcial Intelligence [8], Copyright 2019 by\nthe Association for the Advancement of Artiﬁcial Intelligence.\n113\ntask and the policy required to satisfy it, typically utilizing state, action, or reward data. Several\nclasses of these techniques have yielded promising results, including learning from demonstration,\nlearning from evaluation, and reinforcement learning.\nReinforcement learning has been proven to work on scenarios with well-designed reward\nfunctions and easily available interactions with the environment [74]. However, in real-world\nrobotic applications, explicit reward functions are non-existent, and interactions with the hardware\nare expensive and susceptible to catastrophic failures. This motivates leveraging human interaction\nto supply this reward function and task knowledge, to reduce the amount of high-risk interactions\nwith the environment, and to safely shape the behavior of robotic agents.\nLearning from evaluation is one such way to leverage human domain knowledge and intent\nto shape agent behavior through sparse interactions in the form of evaluative feedback, possibly\nallowing for the approximation of a reward function [45, 46, 47]. This technique has the advantage\nof minimally tasking the human evaluator and can be used when training behaviors they themselves\ncannot perform. However, it can be slow to converge as the agent can only identify desired or\neven stable behaviors through more random exploration or indirect guidance from human negative\nreinforcement of unwanted actions, rather than through more explicit examples of desired behaviors.\nIn such a case, learning from demonstration can be used to provide a more directed path to these\nintended behaviors by utilizing examples of the humans performing the task. This technique has\nthe advantage of quickly converging to more stable behaviors. However, given that it is typically\nperformed ofﬂine, it does not provide a mechanism for corrective or preventative inputs when the\nlearned behavior results in undesirable or catastrophic outcomes, potentially due to unseen states.\nLearning from demonstration also inherently requires the maximal burden on the human, requiring\nthem to perform the task many times until the state space has been sufﬁciently explored, so as to\ngenerate a robust policy. Also, it necessarily fails when the human is incapable of performing the\ntask successfully at all.\nLearning from interventions, where a human acts as an overseer while an agent is performing\na task and periodically takes over control or intervenes when necessary, can provide a method to\n114\nimprove the agent policy while preventing or mitigating catastrophic behaviors [39]. This technique\ncan also reduce the amount of direct interactions with the agent, when compared to learning from\ndemonstration. Similar to learning from evaluation, this technique suffers from the disadvantage\nthat desired behaviors must be discovered through more variable exploration, resulting in slower\nconvergence and less stable behavior.\nMost of these human interaction methods have been studied separately, and there is very little\nwork combining multiple modalities to leverage strengths and mitigate weaknesses. In this paper, we\nwork towards our conceptual framework that combines multiple human-agent interaction modalities\ninto a single framework, called the Cycle-of-Learning for Autonomous Systems from Human\nInteraction [1]. Our goal is to unify different human-in-the-loop learning techniques in a single\nframework to overcome the drawbacks of training from different human interaction modalities in\nisolation, while also maintaining data-efﬁciency and safety.\nIn this paper, we present our initial work towards this goal with a method for combining learning\nfrom demonstrations and learning from interventions for safe and efﬁcient training of autonomous\nsystems. We seek to develop a real-time learning technique that combines demonstrations as well as\ninterventions provided from a human to outperform traditional imitation learning techniques while\nmaintaining agent safety and requiring less data. We validate our method with an aerial robotic\nperching task in a high-ﬁdelity simulator using a quadrotor that has continuous roll, pitch, yaw and\nthrottle commands and a downward facing camera. In particular, the contributions of our work are\ntwofold:\n1. We propose a method for efﬁciently and safely learning from human demonstrations and\ninterventions in real-time.\n2. We empirically investigate both the task performance and data efﬁciency associated with\ncombining human demonstrations and interventions.\nWe show that policies trained with human demonstrations and human interventions together outper-\nform policies trained with just human demonstrations while simultaneously using less data. To the\n115\nFigure 9.1: Cycle-of-Learning for Autonomous Systems from Human Interaction: a concept for\ncombining multiple forms of human interaction with reinforcement learning. As the policy develops,\nthe autonomy independence increases and the human interaction level decreases. This work focuses\non the ﬁrst two components of the cycle (dashed box): Learning from Demonstration and Learning\nfrom Intervention. Reprinted from [8].\nbest of our knowledge this is the ﬁrst result showing that training a policy with a speciﬁc sequence\nof human interactions (demonstrations, then interventions) outperforms training a policy with just\nhuman demonstrations (controlling for the total amount of human interactions), and that one can\nobtain this performance with signiﬁcantly reduced data requirements, providing initial evidence\nthat the role of the human should adapt during the training of safe autonomous systems.\n9.2\nBackground and Related Work\n9.2.1\nLearning from Demonstrations\nHere we provide a brief summary of Learning from Demonstrations (LfD); a more comprehen-\nsive review can be found in [11]. Learning from Demonstrations, sometimes referred to as Imitation\nLearning, is deﬁned by training a policy π in order to generalize over a subset D of states and\nactions visited during a task demonstration over T time steps:\nD = {a0, s0, a1, s1, ..., aT, sT} .\n116\nThis demonstration can be performed by a human supervisor, optimal controller, or virtually any\nother pre-trained policy.\nIn the case of human demonstrations, the human is implicitly trying to maximize what may be\nrepresented as an internal reward function for a given task (Equation 9.1), where π∗(a∗\nt|st) represents\nthe optimal policy that is not necessarily known, in which the optimal action a∗is taken at state s\nfor every time step t.\nmax\na0,...,aT\nT\nX\nt=0\nrt(st, at) =\nT\nX\nt=0\nlog p(π∗(a∗\nt|st))\n(9.1)\nDeﬁning the policy of the supervisor as πsup and its estimate as ˆπsup, imitation learning can be\nachieved through standard supervised learning, where the parameters θ of a policy πθ are trained in\norder to minimize a loss function, such as mean squared error, as shown in Equation 9.2.\nˆπsup = argmin\nπθ\nT\nX\nt=0\n||πθ(st) −at||2\n(9.2)\nThere are many empirical successes of using imitation learning to train autonomous systems.\nFor self-driving cars, Bojarski et al. successfully used human demonstrations to train a policy that\nmapped from front-facing camera images to steering wheel commands using around one hundred\nhours of human driving data [19]. Similar approaches have been taken to train small unmanned\nair system (sUAS) to navigate through cluttered environments while avoiding obstacles, where\ndemonstration data was collected by human oracles in simulated [7] and real-world environments\n[20].\n9.2.2\nLearning from Interventions\nIn Learning from Interventions (LfI) the human takes the role of a supervisor and watches\nthe agent performing the task and intervenes (i.e. overriding agent actions with human actions)\nwhen necessary, in order to avoid unsafe behaviors that may lead to catastrophic states. Recently,\nthis learning from human intervention concept was used for safe reinforcement learning (RL) that\ncould train model-free RL agents without a single catastrophe [39]. Similar work has proposed\n117\nusing human interaction to train a classiﬁer to detect unsafe states, which would then trigger the\nintervention by a safe policy previously trained based on human demonstration of the task [40].\nThis off-policy data generated by the safe policy is aggregated to the replay buffer of a value-based\nreinforcement learning algorithm (Double Deep Q-Network, or DDQN [78]). The main advantage\nof this method is being able to combine the off-policy data generated by the interventions to update\nthe current policy.\n9.2.3\nRelated Work\nSeveral existing works have studied, in isolation, the use of different human interaction modal-\nities to train policies for autonomous systems, either in the form of demonstrations [37], [11],\ninterventions [38], [39] or evaluations [45]. However, there has been relatively little work on how\nto effectively combine multiple human interaction modalities into a single learning framework.\nSeveral cases include the combination of demonstrations and mixed initiative control for training\nrobot polices [42] as well as the recent work by Hilleli and El-Yaniv where imitation learning was\ncombined with interactive reward shaping in a simulated racing game [40] and the recent work\n[121] where deviation from the expert demonstration is added to a reward function to be optimized\nwith reinforcement learning.\nAnother example of work that attempts to augment learning from demonstrations with additional\nhuman interaction is the Dataset Aggregation (DAgger) algorithm [26]. DAgger is an iterative\nalgorithm that consists of two policies, a primary agent policy that is used for direct control of a\nsystem, and a reference policy that is used to generate additional labels to ﬁne-tune the primary\npolicy towards optimal behavior. Importantly, the reference policy’s actions are not taken, but are\ninstead aggregated and used as additional labels to re-train the primary policy for the next iteration.\nIn [27] DAgger was used to train a collision avoidance policy for an autonomous quadrotor using\nimitation learning on a set of human demonstrations to learn the primary policy and using the human\nobserver as a reference policy. There are some drawbacks to this approach that are worth discussing.\nAs noted in [27], because the human observer is never in direct control of the policy, safety is not\nguaranteed, since the agent has the potential to visit previously unseen states, which could cause\n118\ncatastrophic failures. Additionally, the subsequent labeling by the human can be suboptimal both in\nthe amount of data recorded (perhaps recording more data in suboptimal states than is needed to\nlearn an optimal policy) as well as in capturing the intended result of the human observer’s action\n(as in distinguishing a minor course correction from a sharp turn, or the appropriate combination\nof actions to perform a behavior). Another limitation of DAgger is that the human feedback was\nprovided ofﬂine after each run while viewing a slower replay of the video stream to improve the\nresulting label quality. This prevents the application to tasks where real-time interaction between\nhumans and agents are required.\n9.3\nCombining Learning from Human Demonstrations and Interventions\nThis work demonstrates a technique for efﬁciently training an autonomous system safely and in\nreal-time by combining learning from demonstrations and interventions. It is the ﬁrst part of the\nCycle-of-Learning concept (Figure 9.1) which aims to combine multiple forms of human-agent\ninteraction for learning a policy that mimics the human trainer in a safe and efﬁcient manner.\nAlthough this paper focuses on the ﬁrst two parts of the Cycle-of-Learning, for brevity, we will\nrefer to the algorithm presented here as the Cycle-of-Learning (CoL).\nThe CoL starts by training an initial policy π0 from a set of task demonstrations provided by\nthe human trainer using a standard supervised learning technique (regression in this case since the\naction-space for our task is continuous). Next, the agent is given control and executes π0 while the\nhuman takes the role of overseer and supervises the agent’s actions. Using a joystick controller, the\nhuman intervenes whenever the agent exhibits unwanted behavior that diverges from the policy\nof the human trainer, and provides corrective actions to drive the agent back on course, and then\nreleases control back to the agent. The agent then learns from this intervention by augmenting\nthe original training dataset with the states and actions from the intervention, and then ﬁne-tuning\nπ0. The agent then executes the new policy πn while the human continues to oversee and provides\ninterventions as necessary. In practice, the human trainer can easily switch between providing\ndemonstrations and interventions by switching control between the human and the agent as shown\nin Figure 9.2. Combining demonstration and intervention data in this way should not only improve\n119\nthe policy over what learning from demonstration can do alone but also require less training data to\ndo so. The intuition is that the agent will inevitably end up in states previously unexplored with\nthe original demonstration data which will cause it’s policy to fail and that intervening from those\nfailure states allows the agent to quickly learn from those deﬁciencies or \"blind spots\" in its own\npolicy in a more targeted fashion than from demonstration data alone [212]. In this way, we learn\nonly from the critical states, which is more data efﬁcient, instead of using all states for training as is\ndone in DAgger [26].\n9.3.1\nData Efﬁciency\nA demonstration is deﬁned as a human-produced trajectory of state-action pairs for the entire\nepisode, while an intervention is deﬁned as a trajectory of state-action pairs for only the subset of\nthe episode where corrective action is deemed necessary by the human. Thus, the amount of data\nprovided via intervention is nearly always less than the amount provided via demonstration. Training\nroutines that incorporate more episodes utilizing learning from intervention rather than learning\nfrom demonstration will in general be more data sparse, assuming comparable task performance.\nTherefore, by utilizing components of the CoL to learn from both demonstration and intervention,\nwe can train with less data than if demonstrations had been used in isolation for an equivalent\nnumber of episodes, resulting in a more efﬁcient training framework. This concept generalizes to\nthe full CoL, as the agent naturally requires less input from the human as its policy develops, its\ntask proﬁciency increases, and it becomes more autonomous (indicated in Figure 9.1).\n9.3.2\nSafe Learning\nThe notion of safe learning here refers to the ability of a human oracle to intervene in cases\nwhere catastrophic failure may be imminent. Thus, the agent is able to explore higher risk regions of\nthe state space with a greater degree of safety. This approach leverages human domain knowledge\nand ability to forecast such boundary states, which the agent cannot do early in the training process\nwhen the state space is less explored. By allowing the policy to explore less seen regions and then\nprovide training data of how to correct from those states, human interventions provide a richer\n120\ndataset that improves the policy in those regimes. This is contrasted to a method based solely on\ndemonstration, which may only see states and observations along a nominal trajectory and have a\npolicy poorly ﬁt to data outside that envelope. The result is a policy that is more robust, through\ngreater data diversity, while not risking damage to the agent that is typical with methods that rely\non random exploration of the state space. This provides a method to safely train an autonomous\nsystem.\n9.3.3\nReal-Time Interaction\nThe utility of the demonstrated approach is partially linked to the ability of the agent to consume\ndata as it is provided by the human oracle and update its policy online. The current system\naccomplishes this by storing all subject state-action pairs in the training dataset, which is queried in\nreal-time to update the policy, and then ﬁne-tuning that policy whenever new samples are added to\nthe dataset. During intervention, this allows for interaction with an agent using a policy trained on\nthe most recent corrective actions provided by the human. The short time between novel human\nintervention data and behavioral roll-outs from the agent policy prevents signiﬁcant delay in this\nfeedback loop that might result from more infrequent, batch learning. As in closed loop systems,\nlarge temporal delays between feedback inputs and their resultant output behaviors can lead to\ninstability. In this context, that would manifest as unstable training as the human oracle would\nneed to correct for undesired actions for signiﬁcantly longer before seeing any effect on the agent\nbehavior. This shortcoming was exhibited in DAgger, where policy correction was a delayed, ofﬂine\nprocess.\n9.4\nImplementation\nThe next sections address the experimental methodology used to evaluate the proposed approach\nand the implementation of the learning algorithm (shown in Figures 9.2 and Algorithm 6).\n9.4.1\nEnvironment Modeling\nWe tested our CoL approach (Figure 9.2) in an autonomous quadrotor perching task using a\nhigh-ﬁdelity drone simulator based on the Unreal Engine called AirSim developed by Microsoft\n121\nFigure 9.2: Flow diagram illustrating the learning from demonstration and intervention stages in the\nCoL for the quadrotor perching task. Reprinted from [8].\n[204]. AirSim provides realistic emulation of quad-rotor vehicle dynamics while the Unreal Engine\nallows for the development of photo-realistic environments. In this paper, we are concerned with\ntraining a quadrotor to autonomously land on a small landing platform placed on top of a ground\nvehicle (see Figure 9.3).\nThe current observation-space consists of vehicle inertial and angular positions, linear and\n122\nFigure 9.3: Screenshot of AirSim environment and landing task. Inset image in lower right corner:\ndownward-facing camera view used for extracting the position and radius of the landing pad which\nis part of the observation-space that the agent learns from. Reprinted from [8].\nangular velocities, and pixel position and radius of the landing pad extracted using a hand-crafted\ncomputer vision module (15 dimensional continuous observation-space). The vehicle is equipped\nwith a downward-facing RGB camera that captures 320 × 240 pixel resolution images. The camera\nframerate and agent action frequency is 10.5 Hz, while the human observer views the video stream\nat approximately 35 Hz. The action-space comprises the four continuous joystick commands (roll,\npitch, yaw, and throttle), which are translated to reference velocity commands (lateral, longitudinal,\nheading rate, and heave) by the vehicle autopilot.\nFor the perching task, the goal is to land the quadrotor as close to the center of the landing pad\nas possible. We deﬁne a landing a success if the quadrotor lands within 0.5m radius of the center\nof the platform and a failure otherwise. At the beginning of each episode, the quadrotor starts in a\nrandom x,y location at a ﬁxed height above the landing pad and the episode ends when either the\nquadrotor reaches the ground (or landing pad) or after 500 time-steps have elapsed.\n9.4.2\nThe Cycle-of-Learning Algorithm\nAs shown in Algorithm 6 the main procedure starts by initializing the agent’s policy π, the\nhuman dataset DH, the Update Policy subroutine, and task performance threshold. The main loop\n123\nAlgorithm 5 Combining Human Demonstrations and Interventions (Cycle-of-Learning)\n1: procedure MAIN\n2:\nInitialize agent’s policy π\n3:\nInitialize human dataset DH\n4:\nInitialize Update Policy procedure\n5:\nDeﬁne performance threshold α\n6:\nwhile task performance < α do\n7:\nRead observation o\n8:\nSample action aπ ∼π\n9:\nif Human Interaction (aH) then:\n10:\nPerform human action aH\n11:\nAdd o and aH to DH\n12:\nelse\n13:\nPerform aπ\n14:\nif End of Episode then\n15:\nEvaluate task performance\n16: procedure UPDATE POLICY\n17:\nSpawn separate thread\n18:\nInitialize loss threshold lossTH\n19:\nwhile Main procedure running do\n20:\nLoad human dataset DH\n21:\nif New Samples then:\n22:\nwhile loss > lossTH or n < nmax do\n23:\nSample N samples o, a from DH\n24:\nSample ˆa ∼π\n25:\nCompute loss = 1\nN\nPN\ni (ˆai −ai)2\n26:\nPerform gradient descent update on π\nconsists of either executing actions provided by the agent or actions provided by the human. The\nagent reads an observation from the environment and an action is sampled based on the current\npolicy. At any moment the human supervisor is able to override the agent’s action by holding a\njoystick trigger. When this trigger is held, the actions performed by the human ah are sent to the\nvehicle to be executed and are added to the human dataset DH to update the policy according to the\nUpdate Policy subroutine.\nThe agent’s policy π is a fully-connected, three hidden-layer, neural network with 130, 72,\nand 40 neurons, respectively. The network is randomly initialized with weights sampled from a\nnormal distribution. The policy is optimized by minimizing the mean squared error loss using the\n124\nRoot Mean Square Propagation (RMSProp) optimizer with learning rate of 1e-4. Unless deﬁned\notherwise, the human dataset DH is initialized as an empty comma-separated value (CSV) ﬁle. Its\nmain goal is to store the observations and actions performed by the human. The procedure to update\nthe policy in real-time spawns a separate CPU thread to perform policy updates in real-time while\nthe human either demonstrates the task or intervenes. This separate thread continuously checks for\nnew demonstration or intervention data based on the size of the human dataset. If new samples are\nfound, this thread samples a minibatch of 64 samples of observations and actions from the human\ndataset and is used to perform policy updates based on the mean squared error loss until it reaches\nthe loss threshold of 0.005 or maximum number of epochs (in this case, 2000 epochs). This iterative\nupdate routine continues until the task performance threshold α is achieved, which can vary from\ntask to task depending on the desired performance. For this work, we set α to 1 and only stop\ntraining after a pre-speciﬁed number of episodes deﬁned in our experimentation methodology to\nempirically evaluate our method over a controlled number of human interactions, here deﬁned as\neither human demonstrations or human interventions.\n9.4.3\nExperimental Methodology\nUsing the AirSim landing task, we tested our proposed CoL framework against several baseline\nconditions where we compared against using only a single human interaction modality (i.e. only\ndemonstrations or only interventions) using equal amounts of human interaction time for each\ncondition. By controlling for the human interaction time, we can assess if our method of utilizing\nmultiple forms of human interaction provides an improvement over a single form of interaction\ngiven the same amount of human effort.\nEach human participant (n=4) followed the same experimental protocol: given an RGB video\nstream from the downward-facing camera, the participant controlled the continuous roll, pitch,\nyaw, and throttle of the vehicle using an Xbox One joystick to perform 4, 8, 12 and 20 complete\nepisodes of the perching task for three experimental conditions: demonstrations only, interventions\nonly, and demonstrations plus interventions with the CoL method, with each condition starting\nfrom a randomly initialized policy. For the CoL condition, participants performed an equal number\n125\nof demonstrations and interventions to match the total number of episodes for that condition.\nFor example, given 4 episodes of training, our CoL approach would train with learning from\ndemonstrations in the ﬁrst 2 episodes and then switch to learning from interventions for the last 2\nepisodes. We compared this to learning from demonstration for all 4 episodes as well as learning\nfrom interventions for all 4 episodes. This was repeated for 8, 12 and 20 episodes to study the effect\nof varying amounts of human interaction on task performance. Following the diagram in Figure 9.2\nand Update Policy procedure on Algorithm 6, the agent’s policy is trained on a separate thread in\nreal-time, and a model is saved for each complete episode together with the human-observed states\nand the actions they performed. These saved models are later evaluated to assess task performance\naccording to our evaluation procedure described in the next section.\nWe also compared our approach to a random agent as well as an agent trained using a state-of-\nthe-art reinforcement learning approach. The reinforcement learning agent used a publicly available\nimplementation of Proximal Policy Optimization (PPO) [65] with a four degree-of-freedom action\nspace (pitch, roll, throttle, yaw) and was trained for 1000 episodes, using only task completion\nas a binary sparse reward signal. To investigate the effect of action-space complexity on task\nperformance, we also implemented the PPO where only two actions (pitch and roll) and three\nactions (pitch, roll and throttle) were available; for both cases, all other actions were held to a\nconstant value. For the two actions condition (pitch and roll), the agent was given constant throttle\nand descended in altitude at a constant velocity. For both conditions yaw was set to 0. Training time\nof the reinforcement learning agent was limited to the simulated environment running in real-time.\n9.5\nNumerical Results\nWe evaluated our method in terms of task completion percentage, deﬁned as the number of times\nthe drone successfully landed on the landing pad over 100 evaluation runs, for each training method\nas well as for different amounts of human training data. Additionally, We compared the number of\nhuman data samples, i.e. observation-action pairs, used during training for each condition.\nFigure 9.4 compares the performance of the models trained using only interventions (Int), the\nmodels trained using only demonstrations (Demo), and the models trained using the Cycle-of-\n126\nFigure 9.4: Performance comparison in terms of task completion with Interventions (Int), Demon-\nstrations (Demo) and the Cycle-of-Learning (CoL) framework for (A) 4 human interactions, (B) 8\nhuman interactions, (C) 12 human interactions and (D) 20 human interactions, respectively. Here,\nan interaction equates to a single demonstration or intervention and roughly corresponds to the\nnumber of episodes. Error bars denote 1 standard error of the mean. We see that CoL outperforms\nInt and Demo across nearly all human interaction levels. Reprinted from [8].\nLearning approach (CoL). We show results for only these conditions as the random policy condition\nand the RL condition trained using PPO with the full four degree-of-freedom action space were not\nsuccessful given the small amount of training episodes, as explained later in this section. Barplots\nshow the task completion performance from each condition averaged over all participants with error\nbars representing 1 standard error. Subpanels show the performance for varying amounts of human\ninteraction: 4, 8, 12 and 20 episodes. For the 4 human interaction condition (Figure 9.4A), all\nmethods show similar task completion conditions. However, for the 8, 12 and 20 human interaction\nconditions, we see that the CoL approach achieves higher task completion percentages compared to\nthe demonstration-only and intervention-only conditions, with the intervention condition performing\nthe worst.\n127\nFigure 9.5: Comparison of the number of human samples used for training with Interventions\n(Int), Demonstrations (Demo) and the Cycle-of-Learning (CoL) framework for (A) 4 human\ninteractions, (B) 8 human interactions, (C) 12 human interactions and (D) 20 human interactions,\nrespectively. Error bars denote 1 standard error of the mean. We see that CoL uses less data than\nthe demonstration-only condition and only slightly more data than the intervention-only condition.\nReprinted from [8].\nFor the ﬁnal condition of 20 episodes our proposed approach achieves 90.25% (± 5.63% std.\nerror) task completion as compared to 76.25% (± 2.72% std. error) task completion using only\ndemonstrations. In comparison, for the 8 episodes condition, our proposed approach achieves\n74.75% (± 9.38% std. error) task completion in contrast to 54.00% (± 8.95% std. error) task\ncompletion when using only demonstrations.\nFigure 9.5 compares the number of human data samples used to train the models for the same\nconditions and datasets as in Figure 9.4. For the ﬁnal condition of 20 episodes our proposed\napproach used on average 1574.50 (± 54.22 std. error) human-provided samples, which is 37.79%\nfewer data samples when compared to using only demonstrations. Note that the policies generated\nfrom this sparser dataset were able to increase task completion by 14.00%. These results yield a\n128\nFigure 9.6: Performance comparison between the Cycle-of-Learning (CoL) with four continuous\nactions and the Deep Reinforcement Learning algorithm Proximal Policy Optimization (PPO)\ntrained for three different task complexities using 2, 3, and 4 continuous actions. Reprinted from\n[8].\nCoL agent that has 1.90 times the rate of task completion performance per sample when compared\nto learning from demonstrations alone. This value is computed by comparing the ratios of task\ncompletion rate to data samples utilized between the CoL agent and the demonstration-only agent,\nrespectively. Averaging the results over all presented conditions and datasets, the task completion\nincreased by 12.81% (± 3.61% std. error) using 32.05% (± 3.25% std. error) less human samples,\nwhich results in a CoL agent that overall has a task completion rate per sample 1.84 times higher\nthan its counterparts.\nFigure 9.6 shows a comparison between the performance of the CoL method as well as the PPO\nbaseline comparisons using a 2, 3 and 4 degree-of-freedom action space. For PPO with two actions,\nthe agent was able to achieve 100% task completion after 500 episodes, on average. However, when\nthe action space complexity increases to three actions, the PPO agent performance was signiﬁcantly\nreduced, now completing the task less than 5% of the time after training for 1000 episodes. As\n129\nexpected, the PPO agent with the full four degree-of-freedom action space fails to complete the task\nafter training for 1000 episodes. In contrast, the CoL method, with the same four degree-of-freedom\naction space, achieves about 90% task completion in only 20 episodes, representing signiﬁcant\ngains in sample efﬁciency compared to PPO.\n9.6\nSummary\nLearning from demonstrations in combination with learning from interventions yields a more\nproﬁcient policy based on less data, when compared to either approach in isolation. It is likely that\nthe superior performance of the CoL is due to combining the two methods in sequence so as to\nleverage their strengths while attenuating their deﬁciencies.\nHaving been initialized to a random policy, learning from interventions alone produced more\nrandom behaviors, making convergence to a baseline behavior much slower. Overall performance is\nthus slower to develop, resulting in lower percent completion for the same number of interaction\nepisodes. Conversely, learning from demonstrations alone was quicker to converge to stable\nbehavior, but it was consistently outperformed by the CoL across varying numbers of interactions,\nwhile having more training data to utilize. This seems initially counter-intuitive as more training\ndata should result in a more accurate and presumably more proﬁcient policy. However, as the\ndemonstrations typically follow stable trajectories, the agent is less likely to encounter regions of\nthe state space outside these trajectories. When enacting the policy at test time, any deviations from\nthese previously observed states is not captured well in the policy, resulting in poor generalization\nperformance. By allowing the agent to act under its current policy, in conjunction with adaptively\nupdating the policy with corrective human-provided actions needed to recover from potentially\ncatastrophic states, the dataset and subsequent policy is improved. Thus, the CoL allows for both\nrapid convergence to baseline stable behavior and then safe exploration of state space to make the\npolicy more reﬁned and robust.\nThe results shown in Figure 9.5 conﬁrm the expectation that the combination of learning\nfrom demonstrations and interventions requires less data than the condition of learning from\ndemonstrations alone, for the same number of episodes. This supports the notion that the CoL is\n130\na more data efﬁcient approach to training via human inputs. When additionally considering the\nsuperior performance exhibited in Figure 9.4, the data efﬁciency provided by this technique is\neven more signiﬁcant. This result further supports the notion that a combinatorial learning strategy\ninherently samples more data rich inputs from the human observer.\nIt should be emphasized here that rather than providing an incremental improvement to a speciﬁc\ndemonstration or intervention learning strategy, this work proposes an algorithmically agnostic\nmethodology for combining modes of human-based learning. The primary assertion of this work\nis that learning is made more robust, data efﬁcient, and safe through a ﬂuid and complementary\ncycling of these two modes, and would similarly be improved with the addition of the later stages\nof the CoL (i.e. learning from human evaluation and reinforcement learning).\nAs seen in Figure 9.6, the PPO baseline comparison method was tested across varying complexity\nwith different numbers of action dimensions. A striking result that can be seen is the signiﬁcant\ndrop-off in performance when going from two-actions, in which the drone had a constant downward\nthrottle and only controlled roll and pitch, to three and four actions, in which the drone also had to\ncontrol its own throttle. An obvious characteristic of a successful policy for the perching task is\nthat the drone needs to descend in a stable and smooth manner, which is already provided in the\ntwo-action condition, as the downward throttle was set apriori. This makes the task of solving for\nan optimal policy much simpler. In the three and four action condition, however, this behavior must\nbe learned from a sparse reward signal (success or failure to land), which is very difﬁcult given\nlimited episodes.\nWhen implementing the CoL in real world environments, catastrophic failures may be seriously\ndamaging to the autonomous agent, and thus unacceptable. Having a human observer capable of\nintervening provides a mechanism to prevent this inadmissible outcome. Further, techniques that\nmight be applied to enforce a similar level of safety automatically might limit the exploration of\nthe state space, yielding a less robust or less capable policy. Analogously to the shift of policy\ndesign from roboticists or domain experts to human users and laypersons, which is yielded by\nhuman-in-the-loop learning, the technique of learning from interventions shifts the implementation\n131\nof system fail safes away from developers toward users. This shift leverages human abilities\nto predict outcomes, adapt to dynamic circumstances, and synthesize contextual information in\ndecision making.\n9.6.1\nCurrent Limitations and Future Directions\nOur current implementation is limited to the ﬁrst two stages of the CoL: learning from demon-\nstrations and interventions. Our planned future work will include adding in more components of the\nCoL; for example, learning from human evaluative feedback as done in [45, 46, 47]. Additionally,\nwe aim to incorporate reinforcement learning techniques to further ﬁne-tune the learning perfor-\nmance after learning from human demonstrations, interventions and evaluations using an actor-critic\nstyle RL architecture [153].\nA second limitation of the current implementation is that it requires the human to supervise\nthe actions taken by the agent at all times. Future work aims to incorporate conﬁdence metrics in\nour learning models so that the autonomous system can potentially halt its own actions when it\ndetermines it has low conﬁdence and query the human directly for feedback in a mixed-initiative\nstyle framework [42], similar to active learning techniques. Furthermore, our results clearly indicate\nthat a two-stage process - with a primary stage with a large proportion of human-provided actions\nfollowed by a secondary stage with a smaller proportion of those actions - outperforms processes\nwith uniformly large or small amounts of human data throughout. This suggests there is perhaps\nan optimal point in the learning process at which to vary in the amount of human input from full\ndemonstrations to interventions. Figure 9.4 illustrates this notion across the varying number of\ninteractions shown in the subﬁgures, i.e. through the change in relative performance between\nthe three conditions. In future work we will examine if such an optimal mixture or sequencing\nof demonstrations and interventions exists, such that learning speed and stability are maximized,\nand if so, whether it is operator dependent. Rather than having a predetermined staging of the\ndemonstrations and interventions that is potentially suboptimal, a mixed initiative framework could\ndetermine this optimal transition point. This could further reduce the burden on the human observer,\nallow for faster training, and even provide a mechanism to generate more robust policies through\n132\nguided exploration of the state space.\nThis work demonstrates the ﬁrst two stages of the CoL in a simulation environment with the\ngoal of eventually transitioning to physical systems, such as an sUAS. The CoL framework was\nimplicitly designed for use in real world systems, where interactions are limited, and catastrophic\nactions are unacceptable. As can be seen in Figure 9.6, our method learns to perform the perching\ntask in several orders of magnitude less time than traditional RL approaches, potentially allowing\nfor feasible on-the-ﬂy training of real systems. Therefore, we expect that the application of the\nCoL to sUAS platforms, or other physical systems, should operate in effectively the same manner\nas demonstrated in this work. Future efforts will focus on transitioning this framework onto such\nphysical platforms to study its efﬁcacy in real world settings. One critical hurdle that must be\novercome, is the implementation of the learning architecture on embedded hardware, constrained\nby the limited payload of an sUAS.\nAdditionally, given that we are utilizing a relatively high ﬁdelity simulation environment, i.e.\nAirSim, it may be beneﬁcial to bootstrap a real world system with a policy learned in simulation.\nAlthough there are numerous challenges in transferring a policy learned in simulation into the real\nworld, the CoL itself should allow for signiﬁcantly smoother transfer due to its cyclic nature in\nwhich the user can revert to more direct and user intensive inputs at any point during the learning to\nallow for adaptation to previously unobserved states. This capability inherently provides a method\nof transfer learning in the case of disparities between simulated and real world properties of the\nvehicle, sensors, and environment. For example, if the perching behavior learned in simulation\nwas transferred to an actual sUAS, the vehicle dynamics may have unmodeled non-linearities, the\nimagery may have dynamic range limitations, or the environment may present exogenous gust\ndisturbances. In such cases, the baseline policy would be monitored and corrected via learning from\nintervention, if these discrepancies yielded undesirable or possibly catastrophic behaviors.\n133\n10.\nINTEGRATING BEHAVIOR CLONING AND REINFORCEMENT LEARNING FOR\nIMPROVED PERFORMANCE IN SPARSE AND DENSE REWARD ENVIRONMENTS∗\nThis paper investigates how to efﬁciently transition and update policies, trained initially with\ndemonstrations, using off-policy actor-critic reinforcement learning. It is well-known that techniques\nbased on Learning from Demonstrations, for example behavior cloning, can lead to proﬁcient\npolicies given limited data. However, it is currently unclear how to efﬁciently update that policy\nusing reinforcement learning as these approaches are inherently optimizing different objective\nfunctions. Previous works have used loss functions, which combine behavior cloning losses with\nreinforcement learning losses to enable this update. However, the components of these loss functions\nare often set anecdotally, and their individual contributions are not well understood. In this work,\nwe propose the Cycle-of-Learning (CoL) framework that uses an actor-critic architecture with\na loss function that combines behavior cloning and 1-step Q-learning losses with an off-policy\npre-training step from human demonstrations. This enables transition from behavior cloning to\nreinforcement learning without performance degradation and improves reinforcement learning in\nterms of overall performance and training time. Additionally, we carefully study the composition\nof these combined losses and their impact on overall policy learning. We show that our approach\noutperforms state-of-the-art techniques for combining behavior cloning and reinforcement learning\nfor both dense and sparse reward scenarios. Our results also suggest that directly including the\nbehavior cloning loss on demonstration data helps to ensure stable learning and ground future policy\nupdates.\n∗Adapted with permission from “Integrating Behavior Cloning and Reinforcement Learning for Improved Perfor-\nmance in Sparse and Dense Reward Environments”, by Vinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern,\nJohn Valasek, and Nicholas R. Waytowich, presented at the 2020 International Conference on Autonomous Agents\nand Multi-Agent Systems [9] and is partially reproduced here, Copyright 2020 by the International Foundation for\nAutonomous Agents and MultiAgent Systems.\n134\n10.1\nProblem Deﬁnition\nReinforcement Learning (RL) has yielded many recent successes in solving complex tasks that\nmeet and exceed the capabilities of human counterparts, demonstrated in video game environments\n[74], robotic manipulators [213], and various open-source simulated scenarios [85]. However, these\nRL approaches are sample inefﬁcient and slow to converge to this impressive behavior, limited\nsigniﬁcantly by the need to explore potential strategies through trial and error, which produces\ninitial performance signiﬁcantly worse than human counterparts. The resultant behavior that\nis initially random and slow to reach proﬁciency is poorly suited to various situations, such as\nphysically embodied ground and air vehicles or in scenarios where sufﬁcient capability must be\nachieved in short time spans. In such situations, the random exploration of the state space of\nan untrained agent can result in unsafe behaviors and catastrophic failure of a physical system,\npotentially resulting in unacceptable damage or downtime. Similarly, slow convergence of the\nagent’s performance requires exceedingly many interactions with the environment, which is often\nprohibitively difﬁcult or infeasible for physical systems that are subject to energy constraints,\ncomponent failures, and operation in dynamic or adverse environments. These sample efﬁciency\npitfalls of RL are exacerbated even further when trying to learn in the presence of sparse rewards,\noften leading to cases where RL can fail to learn entirely.\nOne approach for overcoming these limitations is to utilize demonstrations of desired behavior\nfrom a human data source (or potentially some other agent) to initialize the learning agent to a\nsigniﬁcantly higher level of performance than is yielded by a randomly initialized agent. This is\noften termed Learning from Demonstrations (LfD) [11], which is a subset of imitation learning that\nseeks to train a policy to imitate the desired behavior of another policy or agent. LfD leverages data\n(in the form of state-action tuples) collected from a demonstrator for supervised learning, and can\nbe used to produce an agent with qualitatively similar behavior in a relatively short training time and\nwith limited data. This type of LfD, called Behavior Cloning (BC), learns a mapping between the\nstate-action pairs contained in the set of demonstrations to mimic the behavior of the demonstrator.\nThough BC techniques do allow for the relatively rapid learning of behaviors that are comparable\n135\nto that of the demonstrator, they are limited by the quality and quantity of the demonstrations\nprovided and are only improved by providing additional, high-quality demonstrations. In addition,\nBC is plagued by the distributional drift problem in which a mismatch between the learned policy\ndistribution of states and the distribution of states in the training set can cause errors that propagate\nover time and lead to catastrophic failures. By combining BC with subsequent RL, it is possible\nto address the drawbacks of either approach, initializing a signiﬁcantly more capable and safer\nagent than with random initialization, while also allowing for further self-improvement without\nneeding to collect additional data from a human demonstrator. However, it is currently unclear how\nto effectively update a policy initially trained with BC using RL as these approaches are inherently\noptimizing different objective functions. Previous works have used loss functions that combine BC\nlosses with RL losses to enable this update, however, the components of these loss functions are\noften set anecdotally and their individual contributions are not well understood.\nIn this work, we propose the Cycle-of-Learning (CoL) framework, which uses an actor-critic\narchitecture with a loss function that combines behavior cloning and 1-step Q-learning losses\nwith an off-policy algorithm, and a pre-training step to learn from human demonstrations. Unlike\nprevious approaches to combine BC with RL, such as [214], our approach uses an actor-critic\narchitecture to learn both a policy and value function from the human demonstration data, which we\nshow, speeds up learning. Additionally, we perform a detailed component analysis of our method to\ninvestigate the individual contributions of pre-training, combined losses, and sampling methods of\nthe demonstration data and their effects on transferring from BC to RL. To summarize, the main\ncontribution of this work are:\n• We introduce an actor-critic based method, that combines pre-training as well as combined\nloss functions to learn both a policy and value function from demonstrations, to enable\ntransition from behavior cloning to reinforcement learning.\n• We show that our method can transfer from BC to RL without performance degradation while\nimproving upon existing state-of-the-art BC to RL algorithms in terms of overall performance\nand training time.\n136\n• We perform a detailed analysis to investigate the contributions of the individual components\nin our method.\nOur results show that our approach outperforms BC, Deep Deterministic Policy Gradients\n(DDPG), and Demonstration Augmented Policy Gradient (DAPG) in two different application\ndomains for both dense- and sparse-reward settings. Our results also suggest that directly including\nthe behavior cloning loss on demonstration data helps to ensure stable learning and ground future\npolicy updates, and that a pre-training step enables the policy to start at a performance level greater\nthan behavior cloning.\n10.2\nPreliminaries\nWe adopt the standard Markov Decision Process (MDP) formulation for sequential decision\nmaking [153], which is deﬁned as a tuple (S, A, R, P, γ), where S is the set of states, A is the set\nof actions, R(s, a) is the reward function, P(s′|s, a) is the transition probability function and γ is\na discount factor. At each state s ∈S, the agent takes an action a ∈A, receives a reward R(s, a)\nand arrives at state s′ as determined by P(s′|s, a). The goal is to learn a behavior policy π which\nmaximizes the expected discounted total reward. This is formalized by the Q-function, sometimes\nreferred to as the state-action value function:\nQπ(s, a) = Eat∼π\n\"+∞\nX\nt=0\nγtR(st, at)\n#\ntaking the expectation over trajectories obtained by executing the policy π starting at s0 = s and\na0 = a.\nHere we focus on actor-critic methods which seek to maximize\nJ(θ) = Es∼µ[Qπ(.|θ)(s, π(s|θ))]\nwith respect to parameters θ and an initial state distribution µ. The Deep Deterministic Policy\nGradient (DDPG) [85] is an off-policy actor-critic reinforcement learning algorithm for continuous\n137\naction spaces, which calculates the gradient of the Q-function with respect to the action to train\nthe policy. DDPG makes use of a replay buffer to store past state-action transitions and target\nnetworks to stabilize Q-learning [74]. Since DDPG is an off-policy algorithm, it allows for the use\nof arbitrary data, such as demonstrations from another source, to update the policy. A demonstration\ntrajectory is a tuple (s, a, r, s′) of state s, action a, the reward r = R(s, a) and the transition state\ns′ collected from a demonstrator’s policy. In most cases these demonstrations are from a human\nobserver, although in principle these demonstrations can come from any existing agent or policy.\n10.3\nRelated Work\nSeveral works have shown the efﬁcacy of combining behavior cloning with reinforcement learn-\ning across a variety of tasks. Recent work by [113], known as Deep Q-learning from Demonstrations\n(DQfD), combined behavior cloning with deep Q-learning [74] to learn policies for Atari games by\nleveraging a loss function that combines a large-margin supervised learning loss function, 1-step\nQ-learning loss, and an n-step Q-learning loss function that helps ensure the network satisﬁes\nthe Bellman equation. This work was extended to continuous action spaces by [114] with DDPG\nfrom Demonstrations (DDPGfD), who proposed an extension of DDPG [85] that uses human\ndemonstrations, and applied their approach to object manipulation tasks for both simulated and\nreal robotic environments. The loss functions for these methods include the n-step Q-learning loss,\nwhich is known to require on-policy data to accurately estimate. Similar work by [117] combined\nbehavior cloning-based demonstration learning, goal-based reinforcement learning, and DDPG for\nrobotic manipulation of objects in a simulated environment.\nA method that is very similar to ours is the Demonstration Augmented Policy Gradient (DAPG)\n[214], a policy-gradient method that uses behavior cloning as a pre-training step together with\nan augmented loss function with a heuristic weight function that interpolates between the policy\ngradient loss, computed using the Natural Policy Gradient [215], and behavior cloning loss. They\napply their approach across four different robotic manipulations tasks using a 24 Degree-of-Freedom\n(DoF) robotic hand in a simulator and show that DAPG outperforms DDPGfD [114] across all tasks.\nTheir work also showed that behavior cloning combined with Natural Policy Gradient performed\n138\nvery similarly to DAPG for three of the four tasks considered, showcasing the importance of using a\nbehavior cloning loss both in pre-training and policy training.\n10.4\nIntegrating Behavior Cloning and Reinforcement Learning\nThe Cycle-of-Learning (CoL) framework is a method for leveraging multiple modalities of\nhuman input to improve the training of RL agents. These modalities can include human demon-\nstrations, i.e. human-provided exemplar behaviors, human interventions, i.e. interdictions in agent\nbehavior with subsequent partial demonstrations, and human evaluations, i.e. sparse indications of\nthe quality of agent behavior. These individual mechanisms of human interaction have been previ-\nously shown to provide various beneﬁts in learning performance and efﬁciency [45, 46, 47, 8, 39].\nThe successful integration of these disparate techniques, which would leverage their complementary\ncharacteristics, requires a learning architecture that allows for optimization of common objective\nfunctions and consistent representations. An actor-critic framework with a combined loss function,\nas presented in this work, is such an architecture.\nIn this paper, we focus on extending the Cycle-of-Learning framework to tackle the known\nissue of transitioning BC policies to RL by utilizing an actor-critic architecture with a combined\nBC+RL loss function and pre-training phase for continuous state-action spaces, that can learn in\nboth dense- and sparse-reward environments. The main advantage of our method is the use of an\noff-policy, actor-critic architecture to pre-train both a policy and value function, as well as continued\nre-use of demonstration data during agent training, which reduces the amount of interactions needed\nbetween the agent and environment. This is an important aspect especially for robotic applications\nor real-world systems where interactions can be costly.\nThe combined loss function consists of the following components: an expert behavior cloning\nloss that drives the actor’s actions toward previous human trajectories, 1-step return Q-learning loss\nto propagate values of human trajectories to previous states, the actor loss, and a L2 regularization\nloss on the actor and critic to stabilize performance and prevent over-ﬁtting during training. The\nimplementation of each loss component and their combination are deﬁned as follows:\n139\n• Expert behavior cloning loss (LBC): Given expert demonstration subset DE of continuous\nstates and actions sE and aE visited by the expert during a task demonstration over T time\nsteps\nDE =\n\b\nsE\n0 , aE\n0 , sE\n1 , aE\n1 , ..., sE\nT , aE\nT\n\t\n,\n(10.1)\na behavior cloning loss (mean squared error) from demonstration data LBC can be written as\nLBC(θπ) = 1\n2\n\u0000π(st|θπ) −aE\nt )\n\u00012\n(10.2)\nin order to minimize the difference between the actions predicted by the actor network π(st),\nparametrized by θπ, and the expert actions aEt for a given state vector st.\n• 1-step return Q-learning loss (L1): The 1-step return R1 can be written in terms of the critic\nnetwork Q, parametrized by θQ, as\nR1 = rt + γQ(st+1, π(st+1|θπ)|θQ).\n(10.3)\nIn order to satisfy the Bellman equation, we minimize the difference between the predicted\nQ-value and the observed return from the 1-step roll-out for a batch of sampled states s:\nLQ1(θQ) = 1\n2 (R1 −Q(s, π(s|θπ)|θQ))2 .\n(10.4)\n• Actor Q-loss (LA): It is assumed that the critic function Q is differentiable with respect\nto the action. Since we want to maximize the Q-values for the current state, the actor loss\nbecame the negative of the Q-values predicted by the critic for a batch of sampled states s:\nLA(θπ) = −Q(s, π(s|θπ)|θQ).\n(10.5)\n140\nCombining the above loss functions for the Cycle-of-Learning becomes\nLCoL(θQ, θπ) = λBCLBC(θπ) + λALA(θπ)\n+ λQ1LQ1(θQ) + λL2QLL2(θQ) + λL2πLL2(θπ).\n(10.6)\nOur approach starts by collecting contiguous trajectories from expert policies and stores the cur-\nrent and subsequent state-actions pairs, reward received, and task completion signal in a permanent\nexpert memory buffer DE. During the pre-training phase, the agent samples a batch of trajectories\nfrom the expert memory buffer DE containing expert trajectories to perform updates on the actor\nand critic networks using the same combined loss function (Equations 10.6). This procedure shapes\nthe actor and critic initial distributions to be closer to the expert trajectories and eases the transition\nfrom policies learned through expert demonstration to reinforcement learning.\nAfter the pre-training phase, the policy is allowed to roll-out and collect its ﬁrst on-policy\nsamples, which are stored in a separate ﬁrst-in-ﬁrst-out memory buffer with only the agent’s samples.\nAfter collecting a given number of on-policy samples, the agent samples a batch of trajectories\ncomprising 25% of samples from the expert memory buffer and 75% from the agent’s memory\nbuffer. This ﬁxed ratio guarantees that each gradient update is grounded by expert trajectories. If a\nhuman demonstrator is used, they can intervene at any time the agent is executing their policy, and\nadd this new trajectories to the expert memory buffer.\nThe proposed method is shown in Algorithm 6.\n10.5\nNumerical Results\n10.5.1\nExperimental Setup\nAs described in the previous sections, in our approach, the Cycle-of-Learning (CoL), we collect\ncontiguous trajectories from expert policies and store them in a permanent memory buffer. The\npolicy is allowed to roll-out and is trained with a combined loss from a mix of demonstration\nand agent data, stored in a separate ﬁrst-in-ﬁrst-out buffer. We validate our approach in three\n141\nAlgorithm 6 Cycle-of-Learning (CoL): Transitioning from Demonstration to Reinforcement Learn-\ning\n1: Input:\nEnvironment env, number of training steps T, number of training steps per batch M, number\nof pre-training steps L, number of gradient updates K, and CoL hyperparameters λQ1,\nλBC, λA, λL2Q, λL2π, τ.\n2: Output:\nTrained actor π(s|θπ) and critic Q(s, π|θQ) networks.\n3: Randomly initialize:\nActor network π(s|θπ) and its target π′(s|θπ′).\nCritic network Q(s, π|θQ) and its target Q′(s, π′|θQ′).\n4: Initialize agent and expert replay buffers R and RE.\n5: Load R and RE with expert dataset DE.\n6: for pre-training steps = 1, ..., L do\n7:\nCall TrainUpdate() procedure.\n8: for training steps = 1, ..., T do\n9:\nReset env and receive initial state s0.\n10:\nfor batch steps = 1, ..., M do\n11:\nSelect action at = π(st|θπ) according to policy.\n12:\nPerform action at and observe reward rt and next state st+1.\n13:\nStore transition (st, at, rt, st+1) in R.\n14:\nfor update steps = 1, ..., K do\n15:\nCall TrainUpdate() procedure.\n16: procedure TRAINUPDATE()\n17:\nif Pre-training then\n18:\nRandomly sample N transitions (si, ai, ri, si+1) from the expert replay buffer RE.\n19:\nelse\n20:\nRandomly sample N ∗0.25 transitions (si, ai, ri, si+1) from the expert replay buffer RE\nand N ∗0.75 transitions from the agent replay buffer R.\n21:\nCompute LQ1(θQ), LBC(θπ), LA(θπ), LL2(θQ), LL2(θπ)\n22:\nUpdate actor and critic for K steps according to Equation 10.6.\n23:\nUpdate target networks:\nθπ′ ←τθπ + (1 −τ)θπ′,\nθQ′ ←τθQ + (1 −τ)θQ′.\nenvironments with continuous observation- and action-space: LunarLanderContinuous-v2 [203]\n(dense and sparse reward cases) and a custom quadrotor landing task [8] implemented using\n142\nMicrosoft AirSim [204]. The dense reward case of LunarLanderContinuous-v2 is the standard\nenvironment provided by OpenAI Gym library [203]: the state space consists of a eight-dimensional\ncontinuous vector with inertial states of the lander, the action space consists of a two-dimensional\ncontinuous vector controlling main and side thrusters, and the reward is given at every step based\non the relative motion of the lander with respect to the landing pad (bonus reward is given when\nthe landing is completed successfully). The sparse reward case is a custom modiﬁcation with the\nsame reward scheme and state-action space, however the reward is stored during the policy roll-out\nand is only given to the agent when the episode ends and is zero otherwise. The custom quadrotor\nlanding task is a modiﬁed version of the environment proposed by Goecks et al. [8], implemented\nusing Microsoft AirSim [204], which consists of landing a quadrotor on a static landing pad in a\nsimulated gusty environment, as seen in Figure 9.3. The state space consists of a ﬁfteen-dimensional\ncontinuous vector with inertial states of the quadrotor and visual features that represent the landing\npad image-frame position and radius as seen by a downward-facing camera. The action space is a\nfour-dimensional continuous vector that sends velocity commands for throttle, roll, pitch, and yaw.\nWind is modeled as noise applied directly to the actions commanded by the agent and follows a\ntemporal-based, instead of distance-based, discrete wind gust model [216] with 65% probability of\nencountering a wind gust at each time step. This was done to induce additional stochasticity in the\nenvironment. The gust duration is uniformly sampled to last between one to three real time seconds\nand can be imparted in any direction, with maximum velocity of half of what can be commanded by\nthe agent along each axis. This task has a sparse-reward scheme (reward R is given at the end of the\nepisode, and is zero otherwise) based on the relative distance rrel between the quadrotor and the\ncenter of the landing pad at the ﬁnal time step of the episode:\nR =\n1\n1 + r2\nrel\n.\nThe hyperparameters used in CoL for each environment are described in table 10.1.\nThe baselines that we compare our approach to are Deep Deterministic Policy Gradient (DDPG)\n143\nTable 10.1: Cycle-of-Learning hyperparemeters for each environment: (a) LunarLanderContinuous-\nv2 and (b) Microsoft AirSim. Reprinted from [9].\nEnvironments\nHyperparameter\n(a)\n(b)\nλQ1 factor\n1.0\n1.0\nλBC factor\n1.0\n1.0\nλA factor\n1.0\n1.0\nλL2Q factor\n1.0e−5\n1.0e−5\nλL2π factor\n1.0e−5\n1.0e−5\nBatch size\n512\n512\nActor learning rate\n1.0e−3\n1.0e−3\nCritic learning rate\n1.0e−4\n1.0e−4\nMemory size\n5.0e5\n5.0e5\nExpert trajectories\n20\n5\nPre-training steps\n2.0e4\n2.0e4\nTraining steps\n5.0e6\n5.0e5\nDiscount factor γ\n0.99\n0.99\nHidden layers\n3\n3\nNeurons per layer\n128\n128\nActivation function\nELU\nELU\n[85, 84], Demonstration Augmented Policy Gradient (DAPG) [214], and traditional behavior\ncloning (BC). For the DDPG baseline we used an open-source implementation by Stable Baselines\n[217]. The hyperparameters used concur with the original DDPG publication [85]: actor and critic\nnetworks with 2 hidden layers with 400 and 300 units respectively, optimized using Adam [202]\nwith learning rate of 10−4 for the actor and 10−3 for the critic, discount factor of γ = 0.99, trained\nwith minibatch size of 64, and replay buffer size of 106. Exploration noise was added to the action\nfollowing an Ornstein-Uhlenbeck process [218] with mean of 0.15 and standard deviation of 0.2.\nFor the DAPG baseline we used an ofﬁcial release of the DAPG codebase from the authors †.\nThe policy is represented by a deep neural network with three hidden layers of 128 units each,\npre-trained with behavior cloning for 100 epochs, with a batch size of 32 samples, and learning rate\nof 10−3, λ0 = 0.01, and λ1 = 0.99. The BC policies are trained by minimizing the mean squared\n†Code available at https://github.com/aravindr93/hand_dapg.\n144\nerror between the expert demonstrations and the output of the model. The policies consist of a\nfully-connected neural network with 3 hidden layers with 128 units each and exponential linear unit\n(ELU) activation function [219]. The BC policy was evaluated for 100 episodes which was used to\ncalculate the mean and standard error of the performance of the policy.\nAll baselines that rely on demonstrations, namely BC, DAPG, and CoL, use the same human\ntrajectories collected in the LunarLanderContinuous-v2 and custom Microsoft AirSim environment.\n10.5.2\nExperimental Results\nThe comparative performances of the CoL against the baseline methods (BC, DDPG and DAPG)\nfor the LunarLanderContinuous-v2 environment are presented via their training curves in Figure\n10.1, using the standard dense reward. The mean reward of the BC pre-trained from the human\ndemonstrations is also shown for reference, and its standard error is shown by the shaded band.\nThe CoL reward initializes to values at or above the BC and steadily improves throughout the\nreinforcement learning phase. Conversely, the DDPG RL baseline initially returns rewards lower\nthan the BC and slowly improves until its performance reaches similar levels to the CoL after\napproximately one million steps. However, this baseline never performs as consistently as the CoL\nand eventually begins to diverge, losing much of its performance gains after about four million\nsteps. The DAPG baseline initial performance, similar to the CoL, surpasses behavior cloning due\nto the pre-training phase and slowly converges to a high score, although slower than the CoL.\nWhen using sparse rewards, meaning the rewards generated by the LunarLanderContinuous-v2\nenvironment are provided only at the last time step of each episode, the performance improvement of\nthe CoL relative to the DDPG and DAPG baselines is even greater (Figure 10.2). The performance\nof the CoL is qualitatively similar during training to that of the dense case, with an initial reward\nroughly equal to or greater than that of the BC and a consistently increasing reward. Conversely,\nthe performance of the DDPG baseline is greatly diminished for the sparse reward case, yielding\neffectively no improvement throughout the whole training period. The training of the DAPG does\nnot deteriorate when compared to the dense reward case but still the performance does not match\nCoL for the speciﬁed training time.\n145\nThe results for the more realistic and challenging AirSim quadrotor landing environment (Figure\n10.3) illustrate a similar trend. The CoL initially returns rewards above the BC, DDPG, and DAPG\nbaselines and steadily increases its performance, with DAPG converging at end to a similar level of\nperformance. The DDPG baseline practically never succeeds and subsequently fails to learn a viable\npolicy, while displaying greater variance in performance when compared to CoL and DAPG. Noting\nthat successfully landing on the target would generate a sparse episode reward of approximately\n0.64, it is clear that these baseline algorithms, with exception of DAPG, rarely generate a satisfactory\ntrajectory for the duration of training.\nFigure 10.1: Comparison of CoL, BC, DDPG, and DAPG for 3 random seeds (bold line representing\nthe mean and shaded area the standard error) in the dense–reward LunarLanderContinuous-v2\nenvironment. Reprinted from [9].\n10.5.3\nComponent Analysis\nSeveral component analyses were performed to evaluate the impact of each of the critical\nelements of the CoL on learning. These respectively include the effects of pre-training, the\n146\nFigure 10.2: Comparison of CoL, BC, DDPG, and DAPG for 3 random seeds (bold line representing\nthe mean and shaded area the standard error) in the sparse-reward LunarLanderContinuous-v2\nenvironment. Reprinted from [9].\nTable 10.2: Method Comparison on LunarLanderContinuous-v2 environment, dense-reward case.\nReprinted from [9].\nMethod\nPre-Training Loss\nTraining Loss\nBuffer Type\nAverage Reward\nCoL\nLQ1 + LA + LBC\nLQ1 + LA + LBC\nFixed Ratio\n261.80 ± 22.53\nCoL-PT\nNone\nLQ1 + LA + LBC\nFixed Ratio\n253.24 ± 46.50\nCoL+PER\nLQ1 + LA + LBC\nLQ1 + LA + LBC\nPER\n245.24 ± 37.66\nDAPG\nLBC\nAugmented Policy Gradient\nNone\n127.99 ± 37.28\nDDPG\nNone\nLQ1 + LA\nUniform\n152.98 ± 69.45\nBC\nLBC\nNone\nNone\n-48.83 ± 27.68*\nBC+DDPG\nLBC\nLQ1 + LA\nUniform\n-57.38 ± 50.11\nCoL-BC\nLQ1 + LA\nLQ1 + LA\nFixed Ratio\n-105.65 ± 196.85\nSummary of learning methods. Enumerated for each method are all non-zero loss components\n(excluding regularization), buffer type, and average and standard error of the reward throughout\ntraining (after pre-training) across the three seeds, evaluated with dense reward in LunarLander-\nContinuous-v2 environment. ∗For BC, these values are computed from 100 evaluation trajectories\nof the ﬁnal pre-trained agent.\n147\nFigure 10.3: Comparison of CoL, BC, DDPG, and DAPG for 3 random seeds (bold line representing\nthe mean and shaded area the standard error) in the sparse-reward Microsoft AirSim quadrotor\nlanding environment. Reprinted from [9].\ncombined loss function, and the sample composition of the experience replay buffer. The results of\neach analysis are shown in Figures 10.4-10.6 and are summarized in Table 10.2.\n10.5.3.1\nEffects of Pre-Training\nTo determine the effects of pre-training on performance we compare the standard CoL against\nan implementation without this pre-training phase, where the number of pre-training steps L = 0,\ndenoted as CoL-PT. The complete combined loss, as seen in Equations 10.6 is used during the\nreinforcement learning phase. This condition assesses the impact on learning performance of not\npre-training the agent, while still using the combined loss in the RL phase. As seen in Figure 10.4,\nthis condition differs from the baseline CoL in its initial performance being worse, i.e. signiﬁcantly\nbelow the BC, but does reach similar rewards after several hundred thousand steps, exhibiting the\nsame consistent response during training thereafter. Effectively, this highlights that the beneﬁt\nof pre-training is improved initial response and signiﬁcant speed gain in reaching steady-state\nperformance level, without qualitatively impacting the long-term training behavior.\n148\nFigure 10.4: Effects of the pre-training phase in the Cycle-of-Learning. Results for 3 random seeds\n(bold line representing the mean and shaded area the standard error) showing component analysis in\nLunarLanderContinuous-v2 environment comparing pre-trained Cycle-of-Learning (CoL curve)\nagainst the Cycle-of-Learning without the pre-training phase (CoL-PT curve) and the behavior\ncloning (BC) baseline. Reprinted from [9].\n149\n10.5.3.2\nEffects of Combined Loss\nTo determine the effects of the combined loss function on performance we compare the standard\nCoL against two alternate learning implementations: 1) the CoL without the behavioral cloning\nexpert loss on the actor (λBC := 0) during both pre-training and RL phases, denoted as CoL-BC,\nand 2) standard BC followed by DDPG using standard loss functions, denoted as BC+DDPG.\nFor the implementation of the CoL without the behavior cloning loss (CoL-BC), the critic loss\nremains the same as in Equation 10.6 for both training phases. This condition assesses the impact\non learning performance of the behavior cloning loss component LBC, given otherwise consistent\nloss functions in both pre-training and RL phases. As seen in Figure 10.5, this condition (purple,\ndashed) improves upon the CoL-PT condition (Figure 10.4) in its initial reward return and similarly\nachieves comparable performance to the baseline CoL in the ﬁrst few hundred thousand steps, but\nthen steadily deteriorates as training continues, with several catastrophic losses in performance.\nThis result makes clear that the behavioral cloning loss is an essential component of the combined\nloss function toward maintaining performance throughout training, anchoring the learning to some\npreviously demonstrated behaviors that are sufﬁciently proﬁcient.\nThe second of these comparative implementations that illustrate the effects of the combined loss,\nbehavior cloning with subsequent DDPG (BC+DDPG), utilized standard loss functions (Equations\n10.2, 10.4, and 10.5) rather than the CoL combined loss in both phases (Equation 10.6). Pre-training\nof the actor with BC uses only the regression loss, as seen in Equation 10.2. DDPG utilizes standard\nloss functions for the actor and critic, as seen in Equation 10.7. This condition assesses the impact on\nlearning performance of standardized loss functions rather than our combined loss functions across\nboth training phases. This condition (Figure 10.5; red, dashed) produces initial rewards below the\nBC response and subsequently improves in performance only to an average level similar to that of\nthe BC and is much less stable in its response throughout training, as indicated by the wide standard\nerror band. This result indicates that simply sequencing standard BC and RL algorithms results in\nsigniﬁcantly worse performance and stability even after millions of training steps, emphasizing the\nvalue of a consistent combined loss function across all training phases.\n150\nFigure 10.5: Effects of the combined loss in the Cycle-of-Learning. Results for 3 random seeds\n(bold line representing the mean and shaded area the standard error) showing component analysis\nin LunarLanderContinuous-v2 environment comparing complete Cycle-of-Learning (CoL), CoL\nwithout the expert behavior cloning loss (CoL-BC), and pre-training with BC followed by DDPG\nwithout combined loss (BC+DDPG). Reprinted from [9].\nLDDPG(θQ, θπ) =λQ1LQ1(θQ) + λALA(θπ)\n+ λL2QLL2(θQ) + λL2πLL2(θπ).\n(10.7)\n10.5.3.3\nEffects of Human Experience Replay Sampling\nTo determine the effects of the experience replay buffer on performance we compare the\nstandard CoL, which utilizes a ﬁxed ratio buffer of samples comprising 25% expert data and 75%\nagent data, against an implementation with Prioritized Experience Replay (PER) [76], with a data\nbuffer prioritized by the magnitude of each transition’s temporal difference (TD) error, denoted as\nCoL+PER. The comparative performance of these implementations, for both the dense- (D) and\nsparse-reward (S) cases of the LunarLanderContinuous-v2 scenario, are shown in Figure 10.6. For\n151\nFigure 10.6: Effects of human experience replay sampling in the Cycle-of-Learning. Results for\n3 random seeds (bold line representing the mean and shaded area the standard error) showing\nablation study in LunarLanderContinuous-v2 environment, dense (D) and sparse (S) reward cases,\ncomparing complete Cycle-of-Learning (CoL) trained with ﬁxed ratio of expert and agent samples\nand complete Cycle-of-Learning using Prioritized Experience Replay (CoL+PER) with a variable\nratio of expert and agent samples ranked based on their temporal difference error. Reprinted from\n[9].\nthe dense-reward condition, there is no signiﬁcant difference in the learning performance between the\nﬁxed ratio and PER buffers. However, for the sparse-reward case of the CoL+PER implementation,\nthe learning breaks down after approximately 1.3 million training steps, resulting in a signiﬁcantly\ndecreased performance thereafter. This result illustrates that the ﬁxed sampling ratio for the replay\nbuffer in the standard CoL is a more robust mechanism of incorporating experience data, particularly\nin sparse-reward environments, likely because it grounds performance to demonstrated human\nbehavior throughout training.\n10.6\nSummary\nIn this work, we present a novel method for combining behavior cloning with reinforce-\nment learning using an actor-critic architecture that implements a combined loss function and\n152\na demonstration-based pre-training phase. We compare our approach against state-of-the-art base-\nlines, including BC, DDPG, and DAPG, and demonstrate the superiority of our method in terms of\nlearning speed, stability, and performance with respect to these baselines. This is shown in the Ope-\nnAI Gym LunarLanderContinuous-v2 and the high-ﬁdelity Microsoft AirSim quadrotor simulation\nenvironments in both dense and sparse reward settings. This result is especially noticeable in the\nAirSim landing task (Figure 10.3), an environment designed to exhibit a high degree of stochasticity.\nThe BC and DDPG baselines fail to converge to an effective and stable policy after ﬁve million\ntraining steps on the LunarLanderContinuous-v2 environment with dense reward and the modiﬁed\nversion with a sparse reward signal. DAPG, although successful in both LunarLanderContinuous-v2\nenvironments and the custom AirSim landing task, converges at a slower rate when compared to\nthe proposed method and starts the training at a lower performance value after pre-training with\ndemonstration data. Conversely, our method, CoL, is able to quickly achieve high performance\nwithout degradation, surpassing both behavior cloning and reinforcement learning algorithms alone,\nin both dense and sparse reward cases. Additionally, we demonstrate through separate analyses\nof several components of our architecture that pre-training, the use of a combined loss function,\nand a ﬁxed ratio of human-generated experience are critical to the performance improvements.\nThis component analysis also indicated that simply sequencing standard behavior cloning and\nreinforcement learning algorithms does not produce these gains and highlighted the importance of\ngrounding the training to the demonstrated data by using a ﬁxed ratio of expert and agent trajectories\nin the experience replay buffer.\nFuture work will investigate how to effectively integrate multiple forms of human feedback\ninto an efﬁcient human-in-the-loop RL system capable of rapidly adapting autonomous systems\nin dynamically changing environments. Actor-critic methods, such as the CoL method proposed\nin this paper, provide an interesting opportunity to integrate different human feedback modalities\nas additional learning signals at different stages of policy learning [1]. For example, existing\nworks have shown the utility of leveraging human interventions [8, 39], and speciﬁcally learning\na predictive model of what actions to ignore at every time step [220], which could be used to\n153\nimprove the quality of the actor’s policy. Deep reinforcement learning with human evaluative\nfeedback has also been shown to quickly train policies across a variety of domains [47, 221] and\ncan be a particularly useful approach when the human is unable to provide a demonstration of\ndesired behavior but can articulate when desired behavior is achieved. Feedback of this type can be\ninterpreted as a critique of an agent’s current behavior relative to the human’s expectation of desired\nbehavior [221], thus making it conceptually similar to an advantage function which can be used to\nimprove the quality of the critic. Further, the capability our approach provides, to transition from\na limited number of human demonstrations to a baseline behavior cloning agent and subsequent\nimprovement through reinforcement learning without signiﬁcant losses in performance, is largely\nmotivated by the goal of human-in-the-loop learning on physical systems. Thus our aim is to\nintegrate this method onto such systems and demonstrate rapid, safe, and stable learning from\nlimited human interaction.\n154\n11.\nCONCLUSIONS AND RECOMMENDATIONS\nThe following conclusions are made based on the results presented in this dissertation:\n1. The Cycle-of-Learning is able to quickly achieve high performance without degradation,\nsurpassing both behavior cloning and reinforcement learning algorithms alone, despite re-\nceiving only sparse rewards. Compared CoL to BC, DDPG, and DAPG, showing that\nCoL converges to better policy performance on the continuous environments studied. On\nLunarLanderContinuous-v2, this translates to 636% better when compared to BC, 71% better\ncompared to DDPG, and 104% better than DAPG. Not only converges to better performance,\nbut it starts with average performance higher than the baselines and, more important, behav-\nior cloning. On the Microsoft AirSim environment, this translates to 161% initial higher\nperformance when compared to DAPG, and 297% when compared to BC.\n2. Component Analysis showed that the pre-training phase, each loss component, and the\ndifferent forms of combining expert and agent samples in the experience replay are critical\nto the performance improvements. This component analysis also indicated that simply\nsequencing standard behavior cloning and reinforcement learning algorithms does not produce\nthese gains.\n3. Learning from demonstrations in combination with learning from interventions yields a more\nproﬁcient policy based on less data, when compared to either approach in isolation. Averaging\nthe results over all presented conditions and datasets, the task completion increased by 12.8%\n(± 3.6% std. error) using 32.1% (± 3.2% std. error) less human samples, which results in\na CoL agent that overall has a task completion rate per sample 1.84 times higher than its\ncounterparts. This supports the notion that the Cycle-of-Learning is a more data efﬁcient\napproach to training via human inputs and further supports the notion that a combinatorial\nlearning strategy inherently samples more data rich inputs from the human observer.\n155\n4. Human interaction modalities, or any expert prior knowledge or policy, should be exploited in\nall aspects of the Reinforcement Learning cycle for better results, and not only on the reward\nfunction.\n5. Further, the case studies presented the application of inverse reinforcement learning and\ngenerative models to learn from human demonstrations, showing that an agent could learn\nhow to perform a UAS landing task and surpass human mean performance after only 100\nlearning iterations.\nSeveral recommendations are made based on the research in this dissertation:\n1. Investigate the trade-offs between learning in a simulated environment and transferring to\nhardware compared to directly learning in hardware. High-ﬁdelity simulation environments\ntake great effort and time to be implemented and there are no theoretical guarantees that the\npolicy will be transferred successfully.\n• How: As shown in this research with the Cycle-of-Learning and similar algorithms,\nas DAPG, if expert demonstrations are available, it is possible to integrate them to\nthe reinforcement learning loop and learn directly in hardware. The time allocated\nto the development of the simulated environment can be shifted to collecting expert\ndemonstrations.\n2. Investigate the impact of neural network architecture size, mainly the number of hidden layers\nand neurons, on task performance when using machine learning algorithms, focusing on\nlearning direct on hardware.\n• How: Advances in the ﬁelds of Neural Architecture Search (NAS) and Neural Network\nCompression can be leverage to ﬁnd smaller architectures with similar accuracy that\ncan increase inference speed and allow complex models to be deployed on hardware.\n3. Develop new algorithms focusing on realistic, and consequently more complex, scenarios,\neven at the cost of reduced performance. Although simple environments are important to de-\n156\nvelop and benchmark new algorithms, there is no theoretical guarantees that the performance\nwill hold when they are applied to more realistic scenarios.\n• How: When working with simulated environments, consider adding sensor noise, time\nsynchronization, and high-ﬁdelity dynamic models. Machine learning models are\ngenerally not robust to changes in the inputs distribution.\n4. Consider the dynamic nature of human policies. This work assumed that the human policy\nremained ﬁxed while training the learning agent and the human demonstrators were given\npractice time before performing the task. In reality, human performance can increase during\ntraining while the human learns to become better at performing the task or can degrade due to\npsychological factors.\n• How: Add a conﬁdence metric to the behavior cloning loss based on expertise of the\nhuman expert or current psychological states.\n5. Investigate how to extend Cycle-of-Learning to teach a learning agent multiple tasks, as\nopposed to a single task as presented in this dissertation. This is essential to create learning\nagents that are able to fulﬁl additional roles in real-time tasks.\n• How: Currently being developed by another graduate student in VSCL, Ritwik Bera,\nis a learning model that segments given demonstrations into differents options, which\nrepresent sub-tasks, and learns a option-conditioned policy.\n157\nREFERENCES\n[1] Nicholas R. Waytowich, Vinicius G. Goecks, and Vernon J. Lawhern. Cycle-of-Learning for\nAutonomous Systems from Human Interaction. CoRR, abs/1808.09572, 2018.\n[2] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science\nand Statistics). Springer-Verlag, Berlin, Heidelberg, 2006. ISBN 0387310738.\n[3] Sergey Levine. UC Berkeley CS 294: Deep Reinforcement Learning, Lecture Notes, 2018.\nLast visited on 2019/09/01.\n[4] Matthew D. Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks.\nComputer Vision–ECCV 2014, 8689:818–833, 2014. ISSN 978-3-319-10589-5. doi: 10.\n1007/978-3-319-10590-1_53.\n[5] Vinicius G. Goecks and John Valasek. Deep Reinforcement Learning on Intelligent Motion\nVideo Guidance for Unmanned Air System Ground Target Tracking. In AIAA Scitech 2019\nForum. doi: 10.2514/6.2019-0137.\n[6] Vinicius G. Goecks, Pedro B. Leal, Trent White, John Valasek, and Darren J. Hartl. Control\nof Morphing Wing Shapes with Deep Reinforcement Learning. In 2018 AIAA Information\nSystems-AIAA Infotech @ Aerospace. doi: 10.2514/6.2018-2139.\n[7] V. G. Goecks, G. M. Gremillion, H. C. Lehman, and W. D. Nothwang. Cyber-Human\nApproach For Learning Human Intention And Shape Robotic Behavior Based On Task\nDemonstration. In 2018 International Joint Conference on Neural Networks (IJCNN), pages\n1–7, July 2018. doi: 10.1109/IJCNN.2018.8489595.\n[8] Vinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern, John Valasek, and\nNicholas R. Waytowich.\nEfﬁciently Combining Human Demonstrations and Interven-\ntions for Safe Training of Autonomous Systems in Real-Time.\nIn Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, pages 2462–2470. AAAI Press, 2019. doi:\n10.1609/aaai.v33i01.33012462.\n[9] Vinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern, John Valasek, and\n158\nNicholas R. Waytowich. Integrating Behavior Cloning and Reinforcement Learning for\nImproved Performance in Dense and Sparse Reward Environments. In Proceedings of the\n19th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’20,\npage 465–473, Richland, SC, 2020. International Foundation for Autonomous Agents and\nMultiagent Systems. ISBN 9781450375184. doi: 10.5555/3398761.3398819.\n[10] Stuart Russell and Peter Norvig. Artiﬁcial Intelligence: A Modern Approach. Prentice Hall\nPress, USA, 3rd edition, 2009. ISBN 0136042597.\n[11] Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A Survey of\nRobot Learning from Demonstration. Robot. Auton. Syst., 57(5):469–483, May 2009. ISSN\n0921-8890. doi: 10.1016/j.robot.2008.10.024.\n[12] Sing Bing Kang and K. Ikeuchi. Toward Automatic Robot Instruction from Perception-\nMapping Human Grasps to Manipulator Grasps. IEEE Transactions on Robotics and Au-\ntomation, 13(1):81–95, 1997.\n[13] Y. Kuniyoshi, M. Inaba, and H. Inoue. Learning by Watching: Extracting Reusable Task\nKnowledge from Visual Observation of Human Performance. IEEE Transactions on Robotics\nand Automation, 10(6):799–822, 1994.\n[14] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J. Andrew Bagnell, Pieter Abbeel, and Jan\nPeters. An Algorithmic Perspective on Imitation Learning. Foundations and Trends R⃝in\nRobotics, 7(1-2):1–179, 2018. ISSN 1935-8253. doi: 10.1561/2300000053.\n[15] Stefan Schaal. Learning from Demonstration. In Proceedings of the 9th International Con-\nference on Neural Information Processing Systems, NIPS’96, page 1040–1046, Cambridge,\nMA, USA, 1996. MIT Press.\n[16] Stefan Schaal. Is Imitation Learning the Route to Humanoid Robots? Trends in Cognitive Sci-\nences, 3(6):233 – 242, 1999. ISSN 1364-6613. doi: https://doi.org/10.1016/S1364-6613(99)\n01327-3.\n[17] Christopher G. Atkeson and Stefan Schaal. Robot Learning From Demonstration. In\nProceedings of the Fourteenth International Conference on Machine Learning, ICML ’97,\n159\npage 12–20, San Francisco, CA, USA, 1997. Morgan Kaufmann Publishers Inc. ISBN\n1558604863. doi: 10.5555/645526.657285.\n[18] Dean A. Pomerleau. ALVINN: An Autonomous Land Vehicle in a Neural Network. In\nAdvances in Neural Information Processing Systems 1, page 305–313, San Francisco, CA,\nUSA, 1989. Morgan Kaufmann Publishers Inc. ISBN 1558600159. doi: 10.5555/89851.\n89891.\n[19] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp,\nPrasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin\nZhang, Jake Zhao, and Karol Zieba. End to End Learning for Self-Driving Cars. CoRR,\nabs/1604.07316, 2016.\n[20] Alessandro Giusti, Jérôme Guzzi, Dan C Cire, Fang-lin He, Juan P Rodríguez, Flavio\nFontana, Matthias Faessler, Christian Forster, Jürgen Schmidhuber, Gianni Di Caro, Davide\nScaramuzza, and Luca M Gambardella. A Machine Learning Approach to Visual Perception\nof Forest Trails for Mobile Robots. 3766(c):1–7, 2015. doi: 10.1109/LRA.2015.2509024.\n[21] Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel, Jitendra Malik, and\nSergey Levine. Combining Self-Supervised Learning and Imitation for Vision-Based Rope\nManipulation. CoRR, abs/1703.02018, 2017.\n[22] F. D. Duchetto, A. Kucukyilmaz, L. Iocchi, and M. Hanheide. Do Not Make the Same\nMistakes Again and Again: Learning Local Recovery Policies for Navigation From Human\nDemonstrations. IEEE Robotics and Automation Letters, 3(4):4084–4091, Oct 2018. ISSN\n2377-3766. doi: 10.1109/LRA.2018.2861080.\n[23] Rouhollah Rahmatizadeh, Pooya Abolghasemi, and Ladislau Bölöni. Learning Manipulation\nTrajectories Using Recurrent Neural Networks. CoRR, abs/1603.03833, 2016.\n[24] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation,\n9(8):1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735.\n[25] M. P. Deisenroth, D. Fox, and C. E. Rasmussen. Gaussian Processes for Data-Efﬁcient\nLearning in Robotics and Control. IEEE Transactions on Pattern Analysis and Machine\n160\nIntelligence, 37(2):408–423, Feb 2015. ISSN 1939-3539. doi: 10.1109/TPAMI.2013.218.\n[26] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A Reduction of Imitation Learning and\nStructured Prediction to No-Regret Online Learning. volume 15 of Proceedings of Machine\nLearning Research, pages 627–635, Fort Lauderdale, FL, USA, 11–13 Apr 2011. JMLR\nWorkshop and Conference Proceedings.\n[27] S. Ross, N. Melik-Barkhudarov, K. S. Shankar, A. Wendel, D. Dey, J. A. Bagnell, and\nM. Hebert. Learning Monocular Reactive UAV Control in Cluttered Natural Environments.\nIn 2013 IEEE International Conference on Robotics and Automation, pages 1765–1772, May\n2013. doi: 10.1109/ICRA.2013.6630809.\n[28] Claude Sammut and Geoffrey I. Webb, editors. Encyclopedia of Machine Learning. Springer\nUS, 2010. doi: 10.1007/978-0-387-30164-8.\n[29] Sergey Levine and Vladlen Koltun. Continuous Inverse Optimal Control with Locally\nOptimal Examples. In Proceedings of the 29th International Coference on International\nConference on Machine Learning, ICML’12, page 475–482, Madison, WI, USA, 2012.\nOmnipress. ISBN 9781450312851.\n[30] M. Johnson, N. Aghasadeghi, and T. Bretl. Inverse Optimal Control for Deterministic\nContinuous-Time Nonlinear Systems. In 52nd IEEE Conference on Decision and Control,\npages 2906–2913, Dec 2013. doi: 10.1109/CDC.2013.6760325.\n[31] F. Ornelas, E. N. Sanchez, and A. G. Loukianov. Discrete-Time Inverse Optimal Control for\nNonlinear Systems Trajectory Tracking. In 49th IEEE Conference on Decision and Control\n(CDC), pages 4813–4818, Dec 2010. doi: 10.1109/CDC.2010.5716974.\n[32] Andrew Ng and Stuart Russell. Algorithms for Inverse Reinforcement Learning. Proceedings\nof the Seventeenth International Conference on Machine Learning, 0:663–670, 2000. ISSN\n00029645. doi: 10.2460/ajvr.67.2.323.\n[33] Deepak Ramachandran and Eyal Amir. Bayesian Inverse Reinforcement Learning. In\nProceedings of the 20th International Joint Conference on Artiﬁcal Intelligence, IJCAI’07,\npage 2586–2591, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc. doi:\n161\n10.5555/1625275.1625692.\n[34] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum En-\ntropy Inverse Reinforcement Learning. In Proceedings of the 23rd National Conference on\nArtiﬁcial Intelligence - Volume 3, AAAI’08, page 1433–1438. AAAI Press, 2008. ISBN\n9781577353683. doi: 10.5555/1620270.1620297.\n[35] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum Entropy Deep Inverse\nReinforcement Learning. arXiv e-prints, art. arXiv:1507.04888, July 2015.\n[36] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided Cost Learning: Deep Inverse\nOptimal Control via Policy Optimization. In Proceedings of the 33rd International Confer-\nence on International Conference on Machine Learning - Volume 48, ICML’16, page 49–58.\nJMLR.org, 2016. doi: 10.5555/3045390.3045397.\n[37] Baris Akgun, Maya Cakmak, Karl Jiang, and Andrea L. Thomaz. Keyframe-based Learning\nfrom Demonstration. International Journal of Social Robotics, 4(4):343–355, 2012. ISSN\n1875-4791. doi: 10.1007/s12369-012-0160-0.\n[38] B. Akgun, M. Cakmak, J. W. Yoo, and A. L. Thomaz. Trajectories and Keyframes for\nKinesthetic Teaching: A Human-Robot Interaction Perspective. In 2012 7th ACM/IEEE\nInternational Conference on Human-Robot Interaction (HRI), pages 391–398, March 2012.\ndoi: 10.1145/2157689.2157815.\n[39] William Saunders, Girish Sastry, Andreas Stuhlmüller, and Owain Evans. Trial without\nError: Towards Safe Reinforcement Learning via Human Intervention. In Proceedings of the\n17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18,\npage 2067–2069, Richland, SC, 2018. International Foundation for Autonomous Agents and\nMultiagent Systems. doi: 10.5555/3237383.3238074.\n[40] Bar Hilleli and Ran El-Yaniv. Toward Deep Reinforcement Learning Without a Simulator:\nAn Autonomous Steering Example. In Sheila A. McIlraith and Kilian Q. Weinberger, editors,\nProceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18),\npages 1471–1478. AAAI Press, 2018.\n162\n[41] Maggie Wigness and John G Rogers III. On-line Human Intervention for Robot Behavior\nAdaptation. 2020.\n[42] D. H. Grollman and O. C. Jenkins. Dogged Learning for Robots. In Proceedings 2007 IEEE\nInternational Conference on Robotics and Automation, pages 2483–2488, April 2007. doi:\n10.1109/ROBOT.2007.363692.\n[43] Shervin Javdani, Henny Admoni, Stefania Pellegrinelli, Siddhartha S. Srinivasa, and\nJ. Andrew Bagnell. Shared Autonomy via Hindsight Optimization for Teleoperation and\nTeaming. The International Journal of Robotics Research, 37(7):717–742, 2018. doi:\n10.1177/0278364918776060.\n[44] Siddharth Reddy, Anca Dragan, and Sergey Levine. Shared Autonomy via Deep Reinforce-\nment Learning. In Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania,\nJune 2018. doi: 10.15607/RSS.2018.XIV.005.\n[45] W. Bradley Knox and Peter Stone. Interactively Shaping Agents via Human Reinforcement:\nThe TAMER Framework. In Proceedings of the Fifth International Conference on Knowledge\nCapture, K-CAP ’09, page 9–16, New York, NY, USA, 2009. Association for Computing\nMachinery. ISBN 9781605586588. doi: 10.1145/1597735.1597738.\n[46] James MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L. Roberts,\nMatthew E. Taylor, and Michael L. Littman. Interactive Learning from Policy-Dependent\nHuman Feedback. In Proceedings of the 34th International Conference on Machine Learning\n- Volume 70, ICML’17, page 2285–2294. JMLR.org, 2017. doi: 10.5555/3305890.3305917.\n[47] Garrett Warnell, Nicholas R. Waytowich, Vernon Lawhern, and Peter Stone. Deep TAMER:\nInteractive Agent Shaping in High-Dimensional State Spaces. In Sheila A. McIlraith and Kil-\nian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, (AAAI-18), pages 1545–1554. AAAI Press, 2018.\n[48] Andrea L. Thomaz and Cynthia Breazeal. Reinforcement Learning with Human Teachers:\nEvidence of Feedback and Guidance with Implications for Learning Performance. In Proceed-\nings of the 21st National Conference on Artiﬁcial Intelligence - Volume 1, AAAI’06, page\n163\n1000–1005. AAAI Press, 2006. ISBN 9781577352815. doi: 10.5555/1597538.1597696.\n[49] W. Bradley Knox, Peter Stone, and Cynthia Breazeal. Training a Robot via Human Feedback:\nA Case Study. In Proceedings of the 5th International Conference on Social Robotics -\nVolume 8239, ICSR 2013, page 460–470, Berlin, Heidelberg, 2013. Springer-Verlag. ISBN\n9783319026749. doi: 10.1007/978-3-319-02675-6_46.\n[50] W. Bradley Knox and Peter Stone. Reinforcement Learning from Simultaneous Human\nand MDP Reward. In Proceedings of the 11th International Conference on Autonomous\nAgents and Multiagent Systems - Volume 1, AAMAS ’12, page 475–482, Richland, SC,\n2012. International Foundation for Autonomous Agents and Multiagent Systems. ISBN\n0981738117. doi: 10.5555/2343576.2343644.\n[51] W. Bradley Knox and Peter Stone. Framing Reinforcement Learning from Human Re-\nward: Reward Positivity, Temporal Discounting, Episodicity, and Performance. Artiﬁcial\nIntelligence, 225:24 – 50, 2015. ISSN 0004-3702. doi: 10.1016/j.artint.2015.03.009.\n[52] L Adrián León, Ana C Tenorio, and Eduardo F Morales. Human Interaction for Effective\nReinforcement Learning. In European Conf. Mach. Learning and Principles and Practice of\nKnowledge Discovery in Databases (ECMLPKDD 2013), volume 3, 2013.\n[53] Dilip Arumugam, Jun Ki Lee, Sophie Saskin, and Michael L. Littman. Deep Reinforcement\nLearning from Policy-Dependent Human Feedback. CoRR, abs/1902.04257, 2019.\n[54] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario\nAmodei.\nDeep Reinforcement Learning from Human Preferences.\nIn Proceedings of\nthe 31st International Conference on Neural Information Processing Systems, NIPS’17, page\n4302–4310, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. doi:\n10.5555/3294996.3295184.\n[55] Avi Singh, Larry Yang, Chelsea Finn, and Sergey Levine. End-To-End Robotic Reinforcement\nLearning without Reward Engineering. In Proceedings of Robotics: Science and Systems,\nFreiburgimBreisgau, Germany, June 2019. doi: 10.15607/RSS.2019.XV.073.\n[56] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei.\n164\nReward Learning from Human Preferences and Demonstrations in Atari. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 31, pages 8011–8023. Curran Associates, Inc.,\n2018.\n[57] John Schulman. The Machine Learning Summer School (MLSS). Technical report, 2016.\n[58] I. Grondman, L. Busoniu, G. A. D. Lopes, and R. Babuska. A Survey of Actor-Critic\nReinforcement Learning: Standard and Natural Policy Gradients. IEEE Transactions on\nSystems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291–1307, Nov\n2012. ISSN 1558-2442. doi: 10.1109/TSMCC.2012.2218595.\n[59] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy Gradient\nMethods for Reinforcement Learning with Function Approximation. In Proceedings of the\n12th International Conference on Neural Information Processing Systems, NIPS’99, page\n1057–1063, Cambridge, MA, USA, 1999. MIT Press. doi: 10.5555/3009657.3009806.\n[60] Sham Kakade and John Langford. Approximately Optimal Approximate Reinforcement\nLearning. In Proceedings of the Nineteenth International Conference on Machine Learning,\nICML ’02, page 267–274, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers\nInc. ISBN 1558608737. doi: 10.5555/645531.656005.\n[61] M. P. Deisenroth, G. Neumann, and J. Peters. A Survey on Policy Search for Robotics. now,\n2013. ISBN 9781601987037. doi: 10.1561/2300000021.\n[62] Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the 30th\nInternational Conference on International Conference on Machine Learning - Volume 28,\nICML’13, page III–1–III–9. JMLR.org, 2013. doi: 10.5555/3042817.3042937.\n[63] Weiwei Li and Emanuel Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear\nBiological Movement Systems. In Proceedings of the First International Conference on\nInformatics in Control, Automation and Robotics - Volume 1: ICINCO,, pages 222–229.\nINSTICC, SciTePress, 2004. ISBN 972-8865-12-0. doi: 10.5220/0001143902220229.\n[64] John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust\n165\nRegion Policy Optimization. In Proceedings of the 32nd International Conference on\nInternational Conference on Machine Learning - Volume 37, ICML’15, page 1889–1897.\nJMLR.org, 2015. doi: 10.5555/3045118.3045319.\n[65] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\nPolicy Optimization Algorithms. CoRR, abs/1707.06347, 2017.\n[66] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\nDimensional Continuous Control Using Generalized Advantage Estimation. In Proceedings\nof the International Conference on Learning Representations (ICLR), 2016.\n[67] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking Deep\nReinforcement Learning for Continuous Control. In Proceedings of the 33rd International\nConference on International Conference on Machine Learning - Volume 48, ICML’16, page\n1329–1338. JMLR.org, 2016. doi: 10.5555/3045390.3045531.\n[68] Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval\nTassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin A. Riedmiller, and David Silver.\nEmergence of Locomotion Behaviours in Rich Environments. CoRR, abs/1707.02286, 2017.\n[69] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley,\nTimothy P. Lillicrap, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for\nDeep Reinforcement Learning. In Proceedings of the 33rd International Conference on\nInternational Conference on Machine Learning - Volume 48, ICML’16, page 1928–1937.\nJMLR.org, 2016. doi: 10.5555/3045390.3045594.\n[70] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward,\nYotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu.\nIMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Archi-\ntectures. volume 80 of Proceedings of Machine Learning Research, pages 1407–1416,\nStockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR.\n[71] Richard S. Sutton. Learning to Predict by the Methods of Temporal Differences. Machine\nLearning, 3(1):9–44, August 1988. ISSN 0885-6125. doi: 10.1023/A:1022633531479.\n166\n[72] Christopher J. C. H. Watkins and Peter Dayan. Q-Learning. Machine Learning, 8(3):279–292,\nMay 1992. ISSN 1573-0565. doi: 10.1007/BF00992698.\n[73] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,\nDaan Wierstra, and Martin A. Riedmiller. Playing Atari with Deep Reinforcement Learning.\nCoRR, abs/1312.5602, 2013.\n[74] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig\nPetersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,\nDaan Wierstra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep\nReinforcement Learning. Nature, 518(7540):529–533, 2015. ISSN 14764687. doi: 10.1038/\nnature14236.\n[75] Long-Ji Lin. Self-Improving Reactive Agents Based on Reinforcement Learning, Planning\nand Teaching. Machine Learning, 8(3–4):293–321, May 1992. ISSN 0885-6125. doi:\n10.1007/BF00992699.\n[76] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experience\nReplay. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on\nLearning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference\nTrack Proceedings, 2016.\n[77] Zachary C. Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, and Li Deng. Efﬁcient\nExploration for Dialog Policy Learning with Deep BBQ Networks \\& Replay Buffer Spiking.\nCoRR, abs/1608.05081, 2016.\n[78] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double\nQ-Learning. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,\nAAAI’16, page 2094–2100. AAAI Press, 2016. doi: 10.5555/3016100.3016191.\n[79] Hado V. Hasselt. Double Q-learning. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor,\nR. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23,\npages 2613–2621. Curran Associates, Inc., 2010.\n167\n[80] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando\nDe Freitas. Dueling Network Architectures for Deep Reinforcement Learning. In Proceedings\nof the 33rd International Conference on International Conference on Machine Learning -\nVolume 48, ICML’16, page 1995–2003. JMLR.org, 2016. doi: 10.5555/3045390.3045601.\n[81] Marc G. Bellemare, Will Dabney, and Rémi Munos. A Distributional Perspective on Rein-\nforcement Learning. In Proceedings of the 34th International Conference on Machine Learn-\ning - Volume 70, ICML’17, page 449–458. JMLR.org, 2017. doi: 10.5555/3305381.3305428.\n[82] Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will\nDabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining\nImprovements in Deep Reinforcement Learning, 2018.\n[83] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband,\nAlexander Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blun-\ndell, and Shane Legg. Noisy Networks for Exploration. In Proceedings of the International\nConference on Representation Learning (ICLR 2018), Vancouver (Canada), 2018.\n[84] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried-\nmiller. Deterministic Policy Gradient Algorithms. volume 32 of Proceedings of Machine\nLearning Research, pages 387–395, Bejing, China, 22–24 Jun 2014. PMLR.\n[85] T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and Daan Wierstra.\nContinuous Control with Deep Reinforcement Learning. CoRR, abs/1509.02971, 2016.\n[86] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous Deep\nQ-Learning with Model-Based Acceleration. In Proceedings of the 33rd International\nConference on International Conference on Machine Learning - Volume 48, ICML’16, page\n2829–2838. JMLR.org, 2016. doi: 10.5555/3045390.3045688.\n[87] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation\nError in Actor-Critic Methods. volume 80 of Proceedings of Machine Learning Research,\npages 1587–1596, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR.\n[88] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-\n168\nPolicy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. volume 80\nof Proceedings of Machine Learning Research, pages 1861–1870, Stockholmsmässan, Stock-\nholm Sweden, 10–15 Jul 2018. PMLR.\n[89] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie\nTan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft\nActor-Critic Algorithms and Applications. CoRR, abs/1812.05905, 2018.\n[90] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, Bernhard Schölkopf,\nand Sergey Levine. Interpolated Policy Gradient: Merging on-Policy and off-Policy Gradient\nEstimation for Deep Reinforcement Learning. In Proceedings of the 31st International\nConference on Neural Information Processing Systems, NIPS’17, page 3849–3858, Red\nHook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. doi: 10.5555/3294996.\n3295141.\n[91] Shixiang Gu, Timothy P. Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine.\nQ-Prop: Sample-Efﬁcient Policy Gradient with An Off-Policy Critic. CoRR, abs/1611.02247,\n2016.\n[92] Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist\nReinforcement Learning. Mach. Learn., 8(3–4):229–256, May 1992. ISSN 0885-6125. doi:\n10.1007/BF00992696.\n[93] I. Szita and A. Lörincz. Learning Tetris Using the Noisy Cross-Entropy Method. Neural\nComputation, 18(12):2936–2941, Dec 2006. ISSN 0899-7667. doi: 10.1162/neco.2006.18.\n12.2936.\n[94] Jan Peters and Stefan Schaal. Reinforcement Learning by Reward-Weighted Regression for\nOperational Space Control. In Proceedings of the 24th International Conference on Machine\nLearning, ICML ’07, page 745–750, New York, NY, USA, 2007. Association for Computing\nMachinery. ISBN 9781595937933. doi: 10.1145/1273496.1273590.\n[95] Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey\nLevine. Model-Based Value Estimation for Efﬁcient Model-Free Reinforcement Learning.\n169\nCoRR, abs/1803.00101, 2018.\n[96] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural Network Dynamics for\nModel-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. In 2018 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 7559–7566, May 2018.\ndoi: 10.1109/ICRA.2018.8463189.\n[97] Carlos E. García, David M. Prett, and Manfred Morari. Model Predictive Control: Theory\nand Practice—A Survey. Automatica, 25(3):335 – 348, 1989. ISSN 0005-1098. doi:\n10.1016/0005-1098(89)90002-2.\n[98] Manfred Morari and Jay H. Lee. Model Predictive Control: Past, Present and Future.\nComputers & Chemical Engineering, 23(4):667 – 682, 1999. ISSN 0098-1354. doi: 10.1016/\nS0098-1354(98)00301-9.\n[99] David Ha and Jürgen Schmidhuber. Recurrent World Models Facilitate Policy Evolution.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,\neditors, Advances in Neural Information Processing Systems 31, pages 2450–2462. Curran\nAssociates, Inc., 2018.\n[100] Diederik P. Kingma and M. Welling.\nAuto-Encoding Variational Bayes.\nCoRR,\nabs/1312.6114, 2014.\n[101] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation\nand Approximate Inference in Deep Generative Models. In Proceedings of the 31st Interna-\ntional Conference on International Conference on Machine Learning - Volume 32, ICML’14,\npage II–1278–II–1286. JMLR.org, 2014. doi: 10.5555/3044805.3045035.\n[102] Christopher Bishop. Mixture Density Networks. Technical Report NCRG/94/004, January\n1994.\n[103] Barak A. Pearlmutter. Learning State Space Trajectories in Recurrent Neural Networks.\nNeural Comput., 1(2):263–269, June 1989. ISSN 0899-7667. doi: 10.1162/neco.1989.1.2.\n263.\n[104] Axel Cleeremans, David Servan-Schreiber, and James L. McClelland. Finite State Automata\n170\nand Simple Recurrent Networks. Neural Computation, 1(3):372–381, September 1989. ISSN\n0899-7667. doi: 10.1162/neco.1989.1.3.372.\n[105] N. Hansen and A. Ostermeier.\nAdapting Arbitrary Normal Mutation Distributions in\nEvolution Strategies: The Covariance Matrix Adaptation. In Proceedings of IEEE In-\nternational Conference on Evolutionary Computation, pages 312–317, May 1996. doi:\n10.1109/ICEC.1996.542381.\n[106] N. Hansen, S. D. Müller, and P. Koumoutsakos.\nReducing the Time Complexity of\nthe Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES).\nEvolutionary Computation, 11(1):1–18, March 2003. ISSN 1063-6560. doi: 10.1162/\n106365603321828970.\n[107] Sébastien Racanière, Théophane Weber, David P. Reichert, Lars Buesing, Arthur Guez,\nDanilo Rezende, Adria Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li,\nRazvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, and Daan Wierstra.\nImagination-Augmented Agents for Deep Reinforcement Learning. In Proceedings of\nthe 31st International Conference on Neural Information Processing Systems, NIPS’17, page\n5694–5705, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. doi:\n10.5555/3295222.3295320.\n[108] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-Driven\nExploration by Self-Supervised Prediction. In Proceedings of the 34th International Confer-\nence on Machine Learning - Volume 70, ICML’17, page 2778–2787. JMLR.org, 2017. doi:\n10.5555/3305890.3305968.\n[109] J. Schmidhuber. Curious Model-Building Control Systems. In 1991 IEEE International Joint\nConference on Neural Networks, pages 1458–1463 vol.2, Nov 1991. doi: 10.1109/IJCNN.\n1991.170605.\n[110] T. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar, M. Loskyll, J. A. Ojea, E. Solowjow,\nand S. Levine. Residual Reinforcement Learning for Robot Control. In 2019 International\nConference on Robotics and Automation (ICRA), pages 6023–6029, May 2019. doi: 10.1109/\n171\nICRA.2019.8794127.\n[111] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo,\nDavid Silver, and Koray Kavukcuoglu. Reinforcement Learning with Unsupervised Auxiliary\nTasks. CoRR, abs/1611.05397, 2016.\n[112] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welin-\nder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience\nReplay. In Proceedings of the 31st International Conference on Neural Information Process-\ning Systems, NIPS’17, page 5055–5065, Red Hook, NY, USA, 2017. Curran Associates Inc.\nISBN 9781510860964. doi: 10.5555/3295222.3295258.\n[113] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan\nHorgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep Q-Learning from Demon-\nstrations. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[114] Matej Vecerík, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot,\nNicolas Heess, Thomas Rothörl, Thomas Lampe, and Martin A. Riedmiller. Leveraging\nDemonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Re-\nwards. CoRR, abs/1707.08817, 2017.\n[115] Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David\nBudden, Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel Vecerík, Matteo Hessel,\nRémi Munos, and Olivier Pietquin. Observe and Look Further: Achieving Consistent\nPerformance on Atari. CoRR, abs/1805.11593, 2018.\n[116] Gabriel Victor de la Cruz, Yunshu Du, and Matthew E. Taylor. Jointly Pre-training with\nSupervised, Autoencoder, and Value Losses for Deep Reinforcement Learning. CoRR,\nabs/1904.02206, 2019.\n[117] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming Ex-\nploration in Reinforcement Learning with Demonstrations. In 2018 IEEE International\nConference on Robotics and Automation (ICRA), pages 6292–6299, May 2018.\ndoi:\n10.1109/ICRA.2018.8463162.\n172\n[118] Wen Sun, Arun Venkatraman, Geoffrey J. Gordon, Byron Boots, and J. Andrew Bagnell.\nDeeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction. volume 70\nof Proceedings of Machine Learning Research, pages 3309–3318, International Convention\nCentre, Sydney, Australia, 06–11 Aug 2017. PMLR.\n[119] Ishan Durugkar, Matthew J. Hausknecht, Adith Swaminathan, and Patrick MacAlpine. Multi-\nPreference Actor Critic. CoRR, abs/1904.03295, 2019.\n[120] Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan Aparicio Ojea, Eugen\nSolowjow, and Sergey Levine. Deep Reinforcement Learning for Industrial Insertion Tasks\nwith Visual Inputs and Natural Rewards. CoRR, abs/1906.05841, 2019.\n[121] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. DeepMimic:\nExample-Guided Deep Reinforcement Learning of Physics-Based Character Skills. ACM\nTrans. Graph., 37(4), July 2018. ISSN 0730-0301. doi: 10.1145/3197517.3201311.\n[122] Russell Kaplan, Christopher Sauer, and Alexander Sosa. Beating Atari with Natural Language\nGuided Reinforcement Learning. CoRR, abs/1704.05539, 2017.\n[123] Nicholas Waytowich, Sean L Barton, Vernon Lawhern, Ethan Stump, and Garrett Warnell.\nGrounding Natural Language Commands to StarCraft II Game States for Narration-Guided\nReinforcement Learning. In Artiﬁcial Intelligence and Machine Learning for Multi-Domain\nOperations Applications, volume 11006, page 110060S. International Society for Optics and\nPhotonics, 2019.\n[124] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\nMichelle Yeo, Alireza Makhzani, Heinrich Küttler, John P. Agapiou, Julian Schrittwieser,\nJohn Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt,\nDavid Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David\nLawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. StarCraft II: A New Challenge\nfor Reinforcement Learning. CoRR, abs/1708.04782, 2017.\n[125] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Àgata Lapedriza,\nNoah Jones, Shixiang Gu, and Rosalind W. Picard. Way Off-Policy Batch Deep Reinforce-\n173\nment Learning of Implicit Human Preferences in Dialog. CoRR, abs/1907.00456, 2019.\n[126] Christian Daniel, Malte Viering, Jan Metz, Oliver Kroemer, and Jan Peters. Active Reward\nLearning. Robotics: Science and Systems (RSS), 10, 2014. doi: 10.15607/RSS.2014.X.031.\n[127] Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Maria Rojas-Barahona, Stefan Ultes, David\nVandyke, Tsung-Hsien Wen, and Steve J. Young. On-line Active Reward Learning for Policy\nOptimisation in Spoken Dialogue Systems. CoRR, abs/1605.07669, 2016.\n[128] Manuel Lopes, Francisco Melo, and Luis Montesano. Active Learning for Reward Estimation\nin Inverse Reinforcement Learning. In Wray Buntine, Marko Grobelnik, Dunja Mladeni´c,\nand John Shawe-Taylor, editors, Machine Learning and Knowledge Discovery in Databases,\npages 31–46, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. ISBN 978-3-642-04174-\n7. doi: 10.1007/978-3-642-04174-7_3.\n[129] Bar Hilleli and Ran El-Yaniv. Deep Learning of Robotic Tasks using Strong and Weak\nHuman Supervision. CoRR, abs/1612.01086, 2016.\n[130] Allan Zhou, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei\nBai, Mrinal Kalakrishnan, Sergey Levine, and Chelsea Finn. Watch, Try, Learn: Meta-\nLearning from Demonstrations and Rewards. In International Conference on Learning\nRepresentations, 2020.\n[131] D. Huang, D. Xu, Y. Zhu, A. Garg, S. Savarese, L. Fei-Fei, and J. C. Niebles. Continuous\nRelaxation of Symbolic Planner for One-Shot Imitation Learning. In 2019 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems (IROS), pages 2635–2642, Nov 2019.\ndoi: 10.1109/IROS40897.2019.8967761.\n[132] John Valasek, Monish D. Tandale, and Jie Rong. A Reinforcement Learning - Adaptive\nControl Architecture for Morphing. Journal of Aerospace Computing, Information, and\nCommunication, 2(April):174–195, 2005. ISSN 1542-9423. doi: 10.2514/1.11388.\n[133] John Valasek, James Doebbler, Monish D. Tandale, and Andrew J. Meade. Improved\nAdaptive-Reinforcement Learning Control for Morphing Unmanned Air Vehicles. IEEE\nTransactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 38(4):1014–1020,\n174\n2008. ISSN 10834419. doi: 10.1109/TSMCB.2008.922018.\n[134] Kenton Kirkpatrick and John Valasek. Approximation of Agent Dynamics Using Reinforce-\nment Learning. (January):1–13, 2013.\n[135] A. Lampton, J. Valasek, and M. Kumar. Multi-Resolution State-Space Discretization for\nQ-Learning with Pseudo-Randomized Discretization. In The 2010 International Joint Con-\nference on Neural Networks (IJCNN), pages 1–8, July 2010. doi: 10.1109/IJCNN.2010.\n5596516.\n[136] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement Learning in Robotics: A\nSurvey. The International Journal of Robotics Research, 32(11):1238–1274, 2013. doi:\n10.1177/0278364913495721.\n[137] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan\nMané. Concrete Problems in AI Safety. CoRR, abs/1606.06565, 2016.\n[138] Shervin Shahrdar, Luiza Menezes, and Mehrdad Nojoumian. A Survey on Trust in Au-\ntonomous Systems. In Kohei Arai, Supriya Kapoor, and Rahul Bhatia, editors, Intelligent\nComputing, pages 368–386, Cham, 2019. Springer International Publishing. ISBN 978-3-\n030-01177-2. doi: 10.1007/978-3-030-01177-2_27.\n[139] Daniel Stormont. Analyzing Human Trust of Autonomous Systems in Hazardous Environ-\nments. 2008.\n[140] J Carlson and R R Murphy. An Investigation of MML Methods for Fault Diagnosis in\nMobile Robots. In 2004 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS) (IEEE Cat. No.04CH37566), volume 1, pages 180–186 vol.1, 2004. doi:\n10.1109/IROS.2004.1389349.\n[141] Moaed A Abd, Iker Gonzalez, Mehrdad Nojoumian, and Erik D Engeberg. Trust, Satisfaction\nand Frustration Measurements During Human-Robot Interaction. In 30th Florida Conference\non Recent Advances in Robotics, pages 2–6, 2017.\n[142] Daniel Howard and Danielle Dai. Public Perceptions of Self-Driving Cars: The Case of\nBerkeley, California. In Transportation Research Board 93rd Annual Meeting, volume 14,\n175\npages 1–16, 2014.\n[143] Ananth Uggirala, Anand K Gramopadhye, Brain J Melloy, and Joe E Toler. Measurement\nof Trust in Complex and Dynamic Systems Using a Quantitative Approach. International\nJournal of Industrial Ergonomics, 34(3):175 – 186, 2004. ISSN 0169-8141. doi: https:\n//doi.org/10.1016/j.ergon.2004.03.005.\n[144] Tove Helldin, Göran Falkman, Maria Riveiro, and Staffan Davidsson. Presenting System\nUncertainty in Automotive UIs for Supporting Trust Calibration in Autonomous Driving.\nIn Proceedings of the 5th International Conference on Automotive User Interfaces and\nInteractive Vehicular Applications, AutomotiveUI ’13, page 210–217, New York, NY, USA,\n2013. Association for Computing Machinery. ISBN 9781450324786. doi: 10.1145/2516540.\n2516554.\n[145] L. Kunze, N. Hawes, T. Duckett, M. Hanheide, and T. Krajník. Artiﬁcial Intelligence for\nLong-Term Robot Autonomy: A Survey. IEEE Robotics and Automation Letters, 3(4):\n4023–4030, Oct 2018. ISSN 2377-3766. doi: 10.1109/LRA.2018.2860628.\n[146] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter.\nContinual Lifelong Learning with Neural Networks: A Review. Neural Networks, 113:54 –\n71, 2019. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2019.01.012.\n[147] James H. Block and Robert B. Burns. Mastery Learning. Review of Research in Education, 4\n(1):3–49, 1976. doi: 10.3102/0091732X004001003.\n[148] Chen-Lin C. Kulik, James A. Kulik, and Robert L. Bangert-Drowns. Effectiveness of Mastery\nLearning Programs: A Meta-Analysis. Review of Educational Research, 60(2):265–299,\n1990. doi: 10.3102/00346543060002265.\n[149] Stephen A Anderson. Synthesis of Research on Mastery Learning. 1994.\n[150] David Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press,\nUSA, 2012. ISBN 0521518148.\n[151] Diego Ardila, Atilla P Kiraly, Sujeeth Bharadwaj, Bokyung Choi, Joshua J Reicher, Lily\nPeng, Daniel Tse, Mozziyar Etemadi, Wenxing Ye, Greg Corrado, et al. End-to-End Lung\n176\nCancer Screening with Three-Dimensional Deep Learning on Low-Dose Chest Computed\nTomography. Nature Medicine, 25(6):954, 2019.\n[152] M. R. Vargas, B. S. L. P. de Lima, and A. G. Evsukoff. Deep Learning for Stock Market\nPrediction from Financial News Articles. In 2017 IEEE International Conference on Compu-\ntational Intelligence and Virtual Environments for Measurement Systems and Applications\n(CIVEMSA), pages 60–65, June 2017. doi: 10.1109/CIVEMSA.2017.7995302.\n[153] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A\nBradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249. doi: 10.5555/3312046.\n[154] Michael A Nielsen. Neural Networks and Deep Learning, volume 25. Determination press\nSan Francisco, CA, USA:, 2015.\n[155] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n[156] Ken-Ichi Funahashi. On the Approximate Realization of Continuous Mappings by Neural\nNetworks. Neural Networks, 2(3):183 – 192, 1989. ISSN 0893-6080. doi: 10.1016/\n0893-6080(89)90003-8.\n[157] G. Cybenko. Approximation by Superpositions of a Sigmoidal Function. Mathematics of\nControl, Signals, and Systems (MCSS), 2(4):303–314, December 1989. ISSN 0932-4194.\ndoi: 10.1007/BF02551274.\n[158] K. Hornik, M. Stinchcombe, and H. White. Multilayer Feedforward Networks Are Universal\nApproximators. Neural Networks, 2(5):359–366, July 1989. ISSN 0893-6080.\n[159] Stinchcombe and White. Universal Approximation Using Feedforward Networks with Non-\nSigmoid Hidden Layer Activation Functions. In International 1989 Joint Conference on\nNeural Networks, pages 613–617 vol.1, 1989. doi: 10.1109/IJCNN.1989.118640.\n[160] N. E. Cotter. The Stone-Weierstrass Theorem and its Application to Neural Networks.\nIEEE Transactions on Neural Networks, 1(4):290–295, Dec 1990. ISSN 1941-0093. doi:\n10.1109/72.80265.\n[161] Yoshifusa Ito. Representation of Functions by Superpositions of a Step or Sigmoid Function\nand Their Applications to Neural Network Theory. Neural Networks, 4(3):385–394, June\n177\n1991. ISSN 0893-6080. doi: 10.1016/0893-6080(91)90075-G.\n[162] Vladik Ya. Kreinovich. Arbitrary Nonlinearity is Sufﬁcient to Represent All Functions by\nNeural Networks: A theorem. Neural Networks, 4(3):381 – 383, 1991. ISSN 0893-6080.\ndoi: https://doi.org/10.1016/0893-6080(91)90074-F.\n[163] Kurt Hornik. Approximation Capabilities of Multilayer Feedforward Networks. Neural\nNetworks, 4(2):251 – 257, 1991. ISSN 0893-6080. doi: 10.1016/0893-6080(91)90009-T.\n[164] Brian D. Ripley and N. L. Hjort. Pattern Recognition and Neural Networks. Cambridge\nUniversity Press, USA, 1st edition, 1995. ISBN 0521460867. doi: 10.5555/546466.\n[165] Warren S. McCulloch and Walter Pitts. A Logical Calculus of the Ideas Immanent in Nervous\nActivity. In Neurocomputing: Foundations of Research, page 15–27, Cambridge, MA, USA,\n1988. MIT Press. ISBN 0262010976. doi: 10.5555/65669.104377.\n[166] Bernard Widrow and Marcian E. Hoff. Adaptive Switching Circuits. In Neurocomputing:\nFoundations of Research, page 123–134, Cambridge, MA, USA, 1988. MIT Press. ISBN\n0262010976. doi: 10.5555/65669.104390.\n[167] Frank Rosenblatt. Principles of Neurodynamics. Perceptrons and the Theory of Brain\nMechanisms. Technical report, Cornell Aeronautical Lab Inc Buffalo NY, 1961.\n[168] D. E. Rumelhart and J. L. McClelland. Learning Internal Representations by Error Propaga-\ntion. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition:\nFoundations, pages 318–362. MITP, 1987. ISBN 9780262291408.\n[169] Fei-Fei Li, Andrej Karpathy, and Justin Johnson. CS231n: Convolutional Neural Networks\nfor Visual Recognition 2016.\n[170] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classiﬁcation with\nDeep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.\nWeinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–\n1105. Curran Associates, Inc., 2012.\n[171] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 2nd\nedition, 2000. ISBN 1886529094.\n178\n[172] Warren B Powell. AI, OR and Control Theory: A Rosetta Stone for Stochastic Optimization.\nPrinceton University, page 12, 2012.\n[173] Leemon C Baird III. Advantage Updating. Technical report, Wright Lab Wright-Patterson\nAFB OH, 1993.\n[174] Richard Bellman. On the Theory of Dynamic Programming. Proceedings of the National\nAcademy of Sciences, 38(8):716–719, 1952. ISSN 0027-8424. doi: 10.1073/pnas.38.8.716.\n[175] Richard Bellman. A Markovian Decision Process. Journal of Mathematics and Mechanics,\n6(5):679–684, 1957.\n[176] Christopher J C H Watkins. Learning from Delayed Rewards. PhD thesis, King’s College,\n1989.\n[177] Csaba Szepesvári. Algorithms for Reinforcement Learning. Synthesis Lectures on Artiﬁcial\nIntelligence and Machine Learning, 4(1):1–103, 2010. ISSN 1939-4608. doi: 10.2200/\nS00268ED1V01Y201005AIM009.\n[178] Tom M Mitchell. The Need for Biases in Learning Generalizations. Department of Computer\nScience, Laboratory for Computer Science Research, 1980.\n[179] Kalanit Grill-Spector and Rafael Malach. The Human Visual Cortex. Annual Review of\nNeuroscience, 27(1):649–677, 2004. ISSN 0147-006X. doi: 10.1146/annurev.neuro.27.\n070203.144220.\n[180] Benedetto De Martino, Dharshan Kumaran, Ben Seymour, and Raymond J Dolan. Frames,\nBiases, and Rational Decision-Making in the Human Brain. Science, 313(5787):684–687,\n2006.\n[181] Andrew Y. Ng and Michael I. Jordan. Shaping and Policy Search in Reinforcement Learning.\nPhD thesis, 2003. AAI3105322.\n[182] X Development LLC. Project Wing, 2018.\n[183] Amazon Prime Air. Determining Safe Access with a Best Equipped, Best-Served Model for\nSmall Unmanned Aircraft Systems, 2018.\n[184] The Drone Co. Latest Projects, 2018.\n179\n[185] Unmanned Systems Roadmap 2007-2032. Technical report, United States Ofﬁce of the\nSecretary of Defense, 2007.\n[186] Jr. Richard A. Best. Intelligence, Surveillance, and Reconnaissance (ISR) Programs: Issues\nfor Congress. Technical report, CRS Report for Congress, 2005.\n[187] John Valasek, Kenton Kirkpatrick, James May, and Joshua Harris. Intelligent Motion Video\nGuidance for Unmanned Air System Ground Target Surveillance. Journal of Aerospace\nInformation Systems, 13(1):10–26, 2016. doi: 10.2514/1.I010198.\n[188] Charles Noren, John Valasek, Vinicius G. Goecks, Cameron Rogers, and Ezekiel Bowden.\nFlight Testing of Intelligent Motion Video Guidance for Unmanned Air System Ground\nTarget Surveillance. In 2018 AIAA Information Systems-AIAA Infotech @ Aerospace. doi:\n10.2514/6.2018-1632.\n[189] Caroline Dunn, John Valasek, and Kenton Kirkpatrick. Unmanned Air System Search and\nLocalization Guidance Using Reinforcement Learning. In Infotech@Aerospace 2012. doi:\n10.2514/6.2012-2589.\n[190] J. Egbert and R. W. Beard. Low Altitude Road Following Constraints Using Strap-down EO\nCameras on Miniature Air Vehicles. In 2007 American Control Conference, pages 353–358,\nJuly 2007. doi: 10.1109/ACC.2007.4282767.\n[191] Jeffery Saunders and Randal W. Beard.\nVisual Tracking in Wind with Field of View\nConstraints.\nInternational Journal of Micro Air Vehicles, 3(3):169–182, 2011.\ndoi:\n10.1260/1756-8293.3.3.169.\n[192] John Valasek, editor. Morphing Aerospace Vehicles and Structures. John Wiley & Sons Ltd.,\nChichester, UK, 2012. ISBN 3: 978-0470972861.\n[193] Amanda Lampton, Adam Niksch, and John Valasek. Morphing Airfoils with Four Morphing\nParameters. AIAA Guidance, Navigation and Control Conference and Exhibit, (August),\n2008. doi: 10.2514/6.2008-7282.\n[194] Pedro BC Leal, Hannah R Stroud, and Darren J Hartl. Design and Fabrication of a Shape\nMemory-Based Bio-Inspired Morphing Wing. In VIII ECCOMAS Thematic Conference on\n180\nSmart Structures and Materials (SMART), 2017.\n[195] Mrinal Kumar, Suman Chakravorty, and John Valasek. A Hierarchical Control Approach to\nMorphing Dynamics. AIAA Infotech@Aerospace Conference, (April), 2009. doi: 10.2514/6.\n2009-1830.\n[196] Dimitris Lagoudas. Shape Memory Alloys: Modeling and Engineering Applications. Springer,\nNew York, 1st ed edition, 2008. ISBN 978-0-387-47684-1 978-0-387-47685-8.\n[197] Sean Swei and Kenneth Cheung. Mission Adaptive Digital Composite Aerostructure Tech-\nnologies. NASA Ames Research Center, 2016.\n[198] Armstrong Flight Research Center NASA. NASA Spanwise Adaptive Wing Concept, 2016.\n[199] Amanda Lampton, Adam Niksch, and John Valasek. Reinforcement Learning of a Morphing\nAirfoil-Policy and Discrete Learning Analysis. Journal of Aerospace Computing, Information,\nand Communication, 7(8):241–260, 2010. ISSN 1542-9423. doi: 10.2514/1.48057.\n[200] Robert F. Stengel. Morphing Aerospace Vehicles and Structures. Journal of Guidance,\nControl, and Dynamics, 36(5):1562–1563, 2013. doi: 10.2514/1.61632.\n[201] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. Journal of Ma-\nchine Learning Research, 15:1929–1958, 2014. ISSN 15337928. doi: 10.1214/12-AOS1000.\n[202] Diederik P. Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization.\nInternational Conference on Learning Representations 2015, pages 1–15, 2015.\n[203] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie\nTang, and Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.\n[204] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. AirSim: High-Fidelity Visual\nand Physical Simulation for Autonomous Vehicles. In Marco Hutter and Roland Siegwart,\neditors, Field and Service Robotics, pages 621–635, Cham, 2018. Springer International\nPublishing. ISBN 978-3-319-67361-5. doi: 10.1007/978-3-319-67361-5_40.\n[205] K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectiﬁers: Surpassing Human-\nLevel Performance on ImageNet Classiﬁcation. In 2015 IEEE International Conference on\n181\nComputer Vision (ICCV), pages 1026–1034, Dec 2015. doi: 10.1109/ICCV.2015.123.\n[206] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided Cost Learning: Deep Inverse\nOptimal Control via Policy Optimization. In Proceedings of the 33rd International Confer-\nence on International Conference on Machine Learning - Volume 48, ICML’16, page 49–58.\nJMLR.org, 2016. doi: 10.5555/3045390.3045397.\n[207] Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. In D. D.\nLee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 29, pages 4565–4573. Curran Associates, Inc., 2016.\n[208] Pieter Abbeel and Andrew Y. Ng. Apprenticeship Learning via Inverse Reinforcement\nLearning. In Proceedings of the Twenty-ﬁrst International Conference on Machine Learning,\nICML ’04, pages 1–, New York, NY, USA, 2004. ACM.\nISBN 1-58113-838-5.\ndoi:\n10.1145/1015330.1015430.\n[209] E. T. Jaynes. Information Theory and Statistical Mechanics. Phys. Rev., 106:620–630, May\n1957. doi: 10.1103/PhysRev.106.620.\n[210] Chelsea Finn, Paul F. Christiano, Pieter Abbeel, and Sergey Levine. A Connection be-\ntween Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based\nModels. CoRR, abs/1611.03852, 2016.\n[211] Long-Ji Lin. Reinforcement Learning for Robots Using Neural Networks. PhD thesis,\nPittsburgh, PA, USA, 1992.\n[212] Ramya Ramakrishnan, Ece Kamar, Debadeepta Dey, Julie Shah, and Eric Horvitz. Dis-\ncovering Blind Spots in Reinforcement Learning. In Proceedings of the 17th International\nConference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, page 1017–1025,\nRichland, SC, 2018. International Foundation for Autonomous Agents and Multiagent Sys-\ntems. doi: 10.5555/3237383.3237849.\n[213] Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Józefowicz, Bob McGrew,\nJakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider,\nSzymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning\n182\nDexterous In-Hand Manipulation. The International Journal of Robotics Research, 39(1):\n3–20, 2020. doi: 10.1177/0278364919887447.\n[214] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman,\nEmanuel Todorov, and Sergey Levine. learning complex dexterous manipulation with deep\nreinforcement learning and demonstrations.\n[215] Sham Kakade. A Natural Policy Gradient. In Proceedings of the 14th International Con-\nference on Neural Information Processing Systems: Natural and Synthetic, NIPS’01, page\n1531–1538, Cambridge, MA, USA, 2001. MIT Press. doi: 10.5555/2980539.2980738.\n[216] D. Moorhouse and R. Woodcock. US Military Speciﬁcation MIL–F–8785C. US Department\nof Defense, 1980.\n[217] Ashley Hill, Antonin Rafﬁn, Maximilian Ernestus, Adam Gleave, Rene Traore, Prafulla\nDhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,\nJohn Schulman, Szymon Sidor, and Yuhuai Wu. Stable Baselines, 2018.\n[218] G. E. Uhlenbeck and L. S. Ornstein. On the Theory of the Brownian Motion. Phys. Rev., 36:\n823–841, Sep 1930. doi: 10.1103/PhysRev.36.823.\n[219] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and Accurate Deep\nNetwork Learning by Exponential Linear Units (ELUs). In Yoshua Bengio and Yann LeCun,\neditors, 4th International Conference on Learning Representations, ICLR 2016, San Juan,\nPuerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.\n[220] Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn\nWhat Not to Learn: Action Elimination with Deep Reinforcement Learning. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 31, pages 3562–3573. Curran Associates, Inc.,\n2018.\n[221] Dilip Arumugam, Siddharth Karamcheti, Nakul Gopalan, Lawson L. S. Wong, and Ste-\nfanie Tellex. Accurately and Efﬁciently Interpreting Human-Robot Instructions of Varying\nGranularities. CoRR, abs/1704.06616, 2017.\n183\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.HC",
    "cs.RO",
    "stat.ML",
    "I.2.6; I.2.9; I.5.2; H.1.2"
  ],
  "published": "2020-08-30",
  "updated": "2020-08-30"
}