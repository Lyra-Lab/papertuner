{
  "id": "http://arxiv.org/abs/2009.00197v1",
  "title": "Deep unsupervised learning for Microscopy-Based Malaria detection",
  "authors": [
    "Alexander Tao",
    "Boran Han"
  ],
  "abstract": "Malaria, a mosquito-borne disease caused by a parasite, kills over 1 million\npeople globally each year. People, if left untreated, may develop severe\ncomplications, leading to death. Effective and accurate diagnosis is important\nfor the management and control of malaria. Our research focuses on utilizing\nmachine learning to improve the efficiency in Malaria diagnosis. We utilize a\nmodified U-net architecture, as an unsupervised learning model, to conduct cell\nboundary detection. The blood cells infected by malaria are then identified in\nchromatic space by a Mahalanobis distance algorithm. Both the cell segmentation\nand Malaria detection process often requires intensive manual label, which we\nhope to eliminate via the unsupervised workflow.",
  "text": "Deep unsupervised learning for Microscopy-Based\nMalaria detection\nAlexander Tao, Boran Han ∗\nAbstract\nMalaria, a mosquito-borne disease caused by a parasite, kills over 1 million people globally each year. People, if left untreated,\nmay develop severe complications, leading to death. Effective and accurate diagnosis is important for the management and\ncontrol of malaria. Our research focuses on utilizing machine learning to improve the efﬁciency in Malaria diagnosis. We\nutilize a modiﬁed U-net architecture, as an unsupervised learning model, to conduct cell boundary detection. The blood cells\ninfected by malaria are then identiﬁed in chromatic space by a Mahalanobis distance algorithm. Both the cell segmentation and\nMalaria detection process often requires intensive manual label, which we hope to eliminate via the unsupervised workﬂow.\n1 Introduction\nMalaria, a mosquito-borne infectious disease, is widespread in tropical and subtropical regions around the equator.\nAccording to the 2019 World Malaria Report, malaria infected over 228 million people in 2018, with over 400,000 of those\ncases resulting in death. This disease brings with it an onslaught of symptoms, including fever, vomiting, and in extreme cases,\ncoma. With a usual onset time of 10-15 days post exposure, various diagnostic methods are used to identify the malaria antigen,\nranging from antibody detection to onsite clinical diagnoses, the traditional and most widely practiced method that consists\nof evaluating patients’ signs and symptoms and on physical ﬁndings at the exam1. According to the CDC, malaria must be\nrecognized promptly in order to treat patients in time and to also prevent the further spread of infection. However, they also note\nthat the diagnosis of malaria can be difﬁcult for various reasons: 1) Clinicians seeing malaria patients may forget to consider\nmalaria among the potential diagnoses and laboratorians who examine blood smears under the microscope can fail to detect the\nparasites characteristic of the disease. 2) Technicians may be unfamiliar with, or lack experience with, malaria, and fail to\ndetect parasites. In some areas affected by malaria, the disease is so potent that a large proportion of the population is infected\nby remain asymptomatic2,3. The current malaria diagnosis methods are not only labor costly, but they are also time consuming,\nwhich creates potential problems with regard to labor allocation of clinicians and economic considerations. In order to combat\npotential failures in diagnoses, we propose a deep learning and hue analysis method in order to render the diagnosing process\nfree of human error. Therefore, automation on the diagnosis of malaria and other disease can potentially save the lives of the\nmillions that are infected each year.\nTo improve the efﬁciency of malaria diagnosis, several attempts were performed using machine learning methods, trying\nto optimize different steps during diagnosis4–9. In the effort of improving the segmentation accuracy in thin blood smear,\nRajaraman et al. compared different pre-trained CNN neural networks6. Manescu et al. developed a new deep learning method\nbased on Convolutional Neural Networks (EDoF-CNN) to increase depth of focus in microscope, facilitating malaria detection7.\nLater, Faster Region-based Convolutional Neural Network (Faster R-CNN), was proposed for cell segmentation, extraction of\nseveral single-cell features, followed by classiﬁcation using random forests8. However, all those methods mentioned above are\nsupervised learning and it requires intensive manual label. Here we present a novel unsupervised learning method for malaria\ndiagnosis, using deep learning with U-net architecture10 and hue analysis.\nWith new hybrid loss function proposed in this paper, we present our method in malaria diagnosis using image set\nBBBC041v1311, available from the Broad Bioimage Benchmark Collection. In the method section, we introduce our deep\nlearning and hue analysis method. In the results section, we present the results that we gathered from our learning. In the\ndiscussion section, we deliberate the implications and the logistics of our method.\n2 Methods\n2.1 Malaria Detection Workﬂow\nAs shown in Fig. 1, we took one image as an example. First, we ran it through a preprocessing algorithm that normalizes\nthe image so that the image can be fed into the U-net. The image is then passed through the U-net architecture where an\nunsupervised learning algorithm outputs a ﬁnal image displaying cell boundaries. After this, the image is transformed into the\nHSV color space to utilize hue analysis. At last, the Mahalanobis distance12 of each pixel, a multivariate measure that compares\neach pixel to the entire distribution, is then calculated and used to select out the malaria ridden cells.\n∗Harvard University, Cambridge, MA, US, email: boranhan.dl@gmail.com\narXiv:2009.00197v1  [eess.IV]  1 Sep 2020\nFigure 1. Illustration of our proposed workﬂow.\n2.2 Self-training U-net architecture for cell boundary recognition\nFigure 2 depicts the U-net architecture used for the unsupervised cell boundary detection. The U-net architecture is\nseparated into three distinct parts: The contracting/down-sampling path, bottleneck, and the expanding/upsampling path. The\nU-net structure is important to domains such as biomedical segmentation, since the number of samples, in this case, images of\nthe patient’s blood cells, is usually limited. Both encoding and decoding paths of our U-net is composed of 4 blocks, with each\nblock composed of 2 3x3 convolution layers and their respective activation functions (with batch normalization). There are two\noutput of this U-net architecture: RMS output and SSIM output. The loss function used to minimize two outputs is described in\nthe loss function section. Two outputs are added and convoluted with the Laplacian ﬁlter to be the ﬁnal output. For the purpose\nof this study, the adaptive gradient decent algorithm called Adam13 was used to train the model. The model was trained for 15\nepochs with a learning rate of 0.01.\nThe down-sampling path is represented by 5 layers of green blocks with convolution layers represented by the yellow\narrows. Concatenation with corresponding cropped feature maps from the contracting paths are represented by the blue arrows\nand the max pooling process is depicted by the purple arrows. The bottleneck part of the network is between the contracting\nand expanding paths and is built from two convolutional layers with batch normalization and dropout. The up-sampling path is\ncomprised of layers of green blocks with the deconvolution process represented by the red arrows. Concatenation with cropped\nfeature maps from the contracting path is represented by the brown blocks. Two separate output segmentation maps are returned\nby the U-net.\n2.3 Hybrid loss function\nOur loss function L consists of two parts: 1) Structural Similarity Index (SSIM) loss LSSIM that panelizes the structural\ndifference14, and 2) Root Mean Square (RMS) contrast loss LRMS that panelizes the contrast difference:\nL = LSSIM +LRMS\n(1)\n2/6\nFigure 2. U-net Architecture with two outputs: SSIM output and RMS output\nLSSIM, calculating the SSIM loss between SSIM output and input is deﬁned as:\nLSSIM = 1−1\nN\nN\n∑\ni=0\nfSSIM(∇ˆy(i)\nSSIM,∇X(i))\n(2)\nWhere X is the input of the U-net output, N is the batch size, i represents the ith in each batch, ∇is the gradient operator\nand fSSIM is the function calculating SSIM between two tensors, described by Wang, et. al14. This SSIM loss will ensure the\noutput to have similar structure with the input microscopy images.\nMeanwhile, to segment the image, we further added a LRMS. It is set as the mean squared loss of the RMS contrast between\nˆyRME and binarized X\nLRMS = 1\nN\nN\n∑\ni=0\n(hRMS(ˆy(i)\nRMS)−hRMS(O(X(i)))2\n(3)\nWhere O is the function binarizing the ith image of X(i) using Otsu’s method15, and hRMS is a function measuring the image\ncontrast, equivalent as the standard deviation of the pixel intensities16:\nhRMS(I) =\nv\nu\nu\nt\n1\nC1C2\nC1\n∑\ni=0\nC2\n∑\nj=0\n(Ii j −¯I)2\n(4)\nwhere intensities Ii j are the pixel value at position (i, j) of an image. C1,C2 are the image size in two dimensions. ¯I is the\naverage intensity of all pixel values in the image. The RMS loss ensures that the U-net output images has the contrast has its\ncorresponding binarized images. Please note that although binarized image was used in the loss function, it is only used for\nU-net to “learn” the contrast difference between the ˆy(i)\nRMS and O(X)(i), instead of pixel-wise intensity difference.\n2.4 Malaria detection in chromaticity space\nSecond, we used the hue analysis to select the infected red blood cell. We transformed the RGB color space to an HSV\ncolor model. This model, a conical representation of color in 3D space separates chromaticity and easily allowed us to detect\nand identify the malaria infected cells. Based off the results of our hue detection algorithm, we were able to determine whether\nthe given sample was malaria infected. The following equations were utilized to transform RGB into the HSV color space:\n3/6\nH =\n(\n0\nB ≤G\n360−θ\nB ≥G\nθ = cos−1{\n1\n2 ×[(R−G)+(R−B)]\n[(R−G)2 +(R−B)(G−B)]}\n(5)\nS = 1−\n3\n(R+G+B)[min(R,G,B)]\nV = 1\n3(R+G+B)\nIn order to separate the outlier color pixels from the rest of the image, we utilized Mahalanobis distance12, a multivariate\ndistance metric that allowed us to measure the distance between a vector and a distribution. Euclidean distance, a more\ncommonly used metric, was not used because of its inability to consider how the rest of the points in a dataset vary beside the\ntwo points being compared. Instead, Mahalanobis distance allows us to calculate an accurate representation of a point’s relation\nto the entire distribution. It is calculated in three steps. First, the columns are transformed into uncorrelated variables, then, they\nare scaled so that their variance is equal to one. Finally, it calculates the Euclidean distance between the points after scaling.\nThe formula to compute this is as follows:\nD2(x) = (x−m)TC−1(x−m)\n(6)\nWhere D2 is the square of the Mahalanobis distance, x is the dataset with two features, which is the H and S of each pixel\ndeﬁned in (5), m is the mean values of each feature , and C−1 is the inverse of the covariance matrix of the independent variables.\nAssuming that the test statistic follows chi-square distributed with 2 degree of freedom (H and S), the critical value at a 0.01\nsigniﬁcance level set as a threshold, which means that pixel can be considered as extreme (Malaria) if p < 0.01.\nFigure 3. Results of cell boundary detection. (a) raw image. (b) boundary detection using Laplacian operator, (c) Sobel\noperator on binarized image and (d) prediction from self-supervised U-net model. (e) raw image overlaid with prediction from\nU-net. (f) intensity projection of the cross section indicated as the yellow dashed lines in (a) – (d).\n3 Results\nFirst, we used our trained U-net model to predict the cell boundary, which is described as the ﬁrst step in Figure 1. The\nprediction results are shown in Figure 3d and 3e. To compare with other popular methods, we also performed a Laplacian\noperator directly on the raw image (Figure 3b) and on binarized image (Figure 3c). Despite both methods being able to provide\ncell boundary, the quality of the output is degraded. To further quantify our ﬁndings. We plotted intensity of the marked cross\nsection for all the methods above (Figure 3f). We found that using Laplacian operator directly on raw image, the signal can\neasily be mask by the large noise; using Laplacian operator on the binarized image, unsharpened edge inside cell can be selected\ntoo. In contrast, our U-net model with hybrid loss function ﬁnds a balance between these two methods: it shows strong signals\ndetecting cell boundary, and weaker signals inside cell.\nFigure 4 depicts unique hue analysis approaches that we used in our malaria detection process. The ﬁrst image, raw image\nwith boxing, shows the malaria ridden cell boxed in green. The next three images are the results of the utilization of a K-means\n4/6\nFigure 4. Results of Hue Analysis using mahalanobis distance\nFigure 5. The images above are the results of running our U-net and Mahalanobis algorithm on other images. The left column\ncontains raw images, with blue boxing around normal red blood cells and green boxing around malaria infected ones. The\ncenter column contains the results of unsupervised U-net algorithm. The ﬁnal column contains the results of running the\nimages through our Mahalanobis distance and connected components algorithm.\nalgorithm with different values of K. However, malaria ridden cells were not selected out, which means K-mean is not an\nefﬁcient algorithm for automatic malaria detection. Manual selection, the second technique that we used, involved setting\nspeciﬁc ranges of values for the hue and manually changing them until the most accurate identiﬁcation of malaria was reached.\nAfter ﬁne tuning, we set a high boundary for HSV selection of (180, 180, 255) and a low boundary of (128, 40, 0). Our last and\nmost efﬁcient technique, utilizing a Mahalanobis distance algorithm along with a connected components algorithm, is depicted\n5/6\nin the last box. The connected components algorithm, utilized to get rid of both noise and cells unrelated to the malaria-positive\nred blood cell, labels subsets of connected components based on a manually set pixel blob size. The algorithm traverses the\npixels in the image, labeling pixel vertices based on their connectivity and relative values of neighboring pixels. Following\nthis labeling stage, the graph is divided into subsets. Regions of interest in these divided images are then able to become\nisolated. Compared with our last techniques, manual selection of color range is more accurate, however our proposed method\nis a parameter-free method, which means it requires no manual parameter tuning. Figure 5 shows more examples of malaria\ndetection using our proposed workﬂow.\nConclusion\nMalaria is no doubt among one of the most harmful disease in the developing world. Nearly half of the world’s population\nspread across 91 different countries are at risk of malaria transmission. Therefore, an efﬁcient and accurate diagnoses of it is\nvital to improving the survivability rate of the illness. In recent years, methods utilizing artiﬁcial intelligence have had aspects\nof manual selection and manual identiﬁcation. Through this project however, we demonstrate a more efﬁcient automated\nmalaria identiﬁcation process that utilizes hue analysis to pinpoint malaria ridden cells. At this moment in time, the memory\nallocation of CPU and GPU for Android and IOS machine learning libraries is sufﬁcient, meaning that the computation cost of\nour research should not be an impediment. The model that we have proposed only takes up (insert amount) MB and took little\nRAM to process test data. The model could serve as a tool to help single out malaria ridden red blood cells when fed pictures of\na patient’s blood. It could minimize delays and improve accuracy in malaria diagnosis.\nReferences\n1. Centers for Disease Control and Prevention. Malaria (2019). https://www.cdc.gov/parasites/malaria/index.html, Last\naccessed on 2020-8-30.\n2. Tangpukdee, N., Duangdee, C., Wilairatana, P. & Krudsood, S. Malaria diagnosis: A brief review. The Korean journal\nparasitology 47, 93–102, DOI: 10.3347/kjp.2009.47.2.93 (2009).\n3. World Health Organization.\nWorld malaria report 2019 (2019).\nhttps://www.who.int/publications/i/item/\nworld-malaria-report-2019, Last accessed on 2020-8-30.\n4. Poostchi, M., Silamut, K., Maude, R. J., Jaeger, S. & Thoma, G. Image analysis and machine learning for detecting malaria.\nTransl. Res. 194, 36 – 55, DOI: https://doi.org/10.1016/j.trsl.2017.12.004 (2018). In-Depth Review: Diagnostic Medical\nImaging.\n5. Arunagiri, V. & B, R. Deep learning approach to detect malaria from microscopic images. Multimed. Tools Appl. DOI:\n10.1007/s11042-019-7162-y (2019).\n6. Rajaraman, S. et al. Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite\ndetection in thin blood smear images. PeerJ 6, DOI: 10.7717/peerj.4568 (2018).\n7. Manescu, P. et al. Deep learning enhanced extended depth-of-ﬁeld for thick blood-ﬁlm malaria high-throughput microscopy\n(2019). 1906.07496.\n8. Hung, J. et al. Applying faster R-CNN for object detection on malaria images. CoRR abs/1804.09548 (2018). 1804.09548.\n9. Yang, F. et al. Deep learning for smartphone-based malaria parasite detection in thick blood smears. IEEE J. Biomed. Heal.\nInformatics 24, 1427–1438 (2020).\n10. Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image segmentation (2015).\n1505.04597.\n11. Ljosa, V., Sokolnicki, K. & Carpenter, A. Annotated high-throughput microscopy image sets for validation. Nat. methods\n9, 637, DOI: 10.1038/nmeth.2083 (2012).\n12. De Maesschalck, R., Jouan-Rimbaud, D. & Massart, D. The mahalanobis distance. Chemom. Intell. Lab. Syst. 50, 1 – 18,\nDOI: https://doi.org/10.1016/S0169-7439(99)00047-7 (2000).\n13. Kingma, D. & Ba, J. Adam: A method for stochastic optimization. Int. Conf. on Learn. Represent. (2014).\n14. Zhou Wang, Bovik, A. C., Sheikh, H. R. & Simoncelli, E. P. Image quality assessment: from error visibility to structural\nsimilarity. IEEE Transactions on Image Process. 13, 600–612 (2004).\n15. Otsu, N. A threshold selection method from gray-level histograms. IEEE Transactions on Syst. Man, Cybern. 9, 62–66\n(1979).\n16. Peli, E. Contrast in complex images. J. Opt. Soc. Am. A 7, 2032–2040, DOI: 10.1364/JOSAA.7.002032 (1990).\n6/6\n",
  "categories": [
    "eess.IV",
    "q-bio.QM"
  ],
  "published": "2020-09-01",
  "updated": "2020-09-01"
}