{
  "id": "http://arxiv.org/abs/2009.00540v1",
  "title": "Training Deep Neural Networks with Constrained Learning Parameters",
  "authors": [
    "Prasanna Date",
    "Christopher D. Carothers",
    "John E. Mitchell",
    "James A. Hendler",
    "Malik Magdon-Ismail"
  ],
  "abstract": "Today's deep learning models are primarily trained on CPUs and GPUs. Although\nthese models tend to have low error, they consume high power and utilize large\namount of memory owing to double precision floating point learning parameters.\nBeyond the Moore's law, a significant portion of deep learning tasks would run\non edge computing systems, which will form an indispensable part of the entire\ncomputation fabric. Subsequently, training deep learning models for such\nsystems will have to be tailored and adopted to generate models that have the\nfollowing desirable characteristics: low error, low memory, and low power. We\nbelieve that deep neural networks (DNNs), where learning parameters are\nconstrained to have a set of finite discrete values, running on neuromorphic\ncomputing systems would be instrumental for intelligent edge computing systems\nhaving these desirable characteristics. To this extent, we propose the\nCombinatorial Neural Network Training Algorithm (CoNNTrA), that leverages a\ncoordinate gradient descent-based approach for training deep learning models\nwith finite discrete learning parameters. Next, we elaborate on the theoretical\nunderpinnings and evaluate the computational complexity of CoNNTrA. As a proof\nof concept, we use CoNNTrA to train deep learning models with ternary learning\nparameters on the MNIST, Iris and ImageNet data sets and compare their\nperformance to the same models trained using Backpropagation. We use following\nperformance metrics for the comparison: (i) Training error; (ii) Validation\nerror; (iii) Memory usage; and (iv) Training time. Our results indicate that\nCoNNTrA models use 32x less memory and have errors at par with the\nBackpropagation models.",
  "text": "Training Deep Neural Networks with Constrained\nLearning Parameters\nPrasanna Date\nDepartment of Computer Science\nRensselaer Polytechnic Institute\nTroy, New York 12180\ndatep@rpi.edu\nChristopher D. Carothers\nDepartment of Computer Science\nRensselaer Polytechnic Institute\nTroy, New York 12180\nchris.carothers@gmail.com\nJohn E. Mitchell\nDepartment of Mathematical Sciences\nRensselaer Polytechnic Institute\nTroy, New York 12180\nmitchj@rpi.edu\nJames A. Hendler\nDepartment of Computer Science\nRensselaer Polytechnic Institute\nTroy, New York 12180\nhendler@cs.rpi.edu\nMalik Magdon-Ismail\nDepartment of Computer Science\nRensselaer Polytechnic Institute\nTroy, New York 12180\nmagdon@cs.rpi.edu\nAbstract—Today’s deep learning models are primarily trained\non CPUs and GPUs. Although these models tend to have\nlow error, they consume high power and utilize large amount\nof memory owing to double precision ﬂoating point learning\nparameters. Beyond the Moore’s law, a signiﬁcant portion of\ndeep learning tasks would run on edge computing systems, which\nwill form an indispensable part of the entire computation fabric.\nSubsequently, training deep learning models for such systems will\nhave to be tailored and adopted to generate models that have the\nfollowing desirable characteristics: low error, low memory, and\nlow power. We believe that deep neural networks (DNNs), where\nlearning parameters are constrained to have a set of ﬁnite discrete\nvalues, running on neuromorphic computing systems would\nbe instrumental for intelligent edge computing systems having\nthese desirable characteristics. To this extent, we propose the\nCombinatorial Neural Network Training Algorithm (CoNNTrA),\nthat leverages a coordinate gradient descent-based approach\nfor training deep learning models with ﬁnite discrete learning\nparameters. Next, we elaborate on the theoretical underpinnings\nand evaluate the computational complexity of CoNNTrA. As\na proof of concept, we use CoNNTrA to train deep learning\nmodels with ternary learning parameters on the MNIST, Iris\nand ImageNet data sets and compare their performance to the\nsame models trained using Backpropagation. We use following\nperformance metrics for the comparison: (i) Training error; (ii)\nValidation error; (iii) Memory usage; and (iv) Training time. Our\nresults indicate that CoNNTrA models use 32× less memory and\nhave errors at par with the Backpropagation models.\nIndex Terms—Deep Neural Networks, Training Algorithm,\nDeep Learning, Machine Learning, Artiﬁcial Intelligence\nI. INTRODUCTION\nDeep neural networks (DNNs) have had a signiﬁcant impact\non our lives in the twenty ﬁrst century—from advancing\nscientiﬁc discovery [1] to improving the quality of life [2]–\n[4]. DNNs are trained using traditional learning algorithms like\nBackpropagation on conventional computing platforms using\nCPUs and GPUs. While DNNs trained using this approach\nhave low error and can be trained in a reasonable amount of\ntime, they consume large amount of memory and power. This\nwill not be sustainable in the post Moore’s law era, where\na signiﬁcant portion of deep learning tasks will be ported to\nedge computing systems [5]. Edge computing systems would\nform an indispensable part of the entire computation fabric\nand support critical applications like Internet of Things (IoT),\nautonomous vehicles, embedded systems etc. [6]. Therefore, it\nis important to train DNNs that are tailored for such systems\nand have the following three desirable characteristics: low\nerror, low memory, and low power.\nWhile low power could be achieved using neuromorphic\ncomputing systems [7], we focus on achieving low error and\nlow memory in this work. Our work in this paper can poten-\ntially be extended to neuromorphic systems to achieve these\ndesirable characteristics. In order to achieve low memory, we\nfocus on deep learning models where learning parameters are\nconstrained to have a set of ﬁnite discrete values, for example,\nbinary or ternary values. By constraining the values of learning\nparameters, we signiﬁcantly reduce the memory required to\nstore them. For instance, a learning parameter constrained\nto have ternary values (−1, 0, +1) can be stored using\njust 2 bits, as opposed to a double precision ﬂoating point\nlearning parameter used in traditional learning algorithms,\nwhich requires 64 bits.\nTo train DNNs with constrained learning parameters, we\npropose a novel training algorithm called the Combinatorial\nNeural Network Training Algorithm (CoNNTrA) in Section\nV. Our objective is to demonstrate that CoNNTrA can train\ndeep learning models consuming signiﬁcantly less memory,\nyet achieving errors at par with Backpropagation. In Section\nVI, we use CoNNTrA to train deep learning models for\nthree machine learning benchmark data sets (MNIST, Iris and\nImageNet). We compare the performance of CoNNTrA to that\nof Backpropagation along four performance metrics: training\nerror, validation error, memory usage and training time. Our\nresults indicate that CoNNTrA models have errors at par with\nBackpropagation, and consume 32× less memory.\narXiv:2009.00540v1  [cs.LG]  1 Sep 2020\nII. RELATED WORK\nDeep learning models with binary learning parameters\nhave been proposed in the literature for several use cases.\nCourbariaux et al. propose BinaryConnect, which can train\nbinary neural networks for specialized hardware and test their\napproach on MNIST, CIFAR-10 and SVHN data sets [8].\nRastegari et al. propose XNOR-Net, which can train binary\nconvolutional neural networks (CNN), test their algorithm on\nthe ImageNet data set, and report 32× savings in memory and\n58× faster convolutional operations [9]. Wan et al. propose\nTernary Binary Network (TBN), having ternary inputs and\nbinary learning parameters, for edge computing devices like\nportable devices and wearable devices, test their approach\non ImageNet and PASCAL VOC data sets and achieve 32×\nmemory savings and 40× faster convolutional operations [10].\nAndri et al. propose YodaNN, a hardware accelerator for\nBinaryConnect CNNs and obtain high power efﬁciency [11].\nIn addition to binary and ternary neural networks, several\napproaches have been proposed in the literature to train\nquantized neural networks. Hubara et al. propose a method to\ntrain quantized neural networks having low precision weights\nand test their approach on the MNIST, CIFAR-10, SVHN and\nImageNet data sets [12]. Zhou et al. propose a mechanism\nfor iterative optimizations for training quantized neural net-\nwork and test their approach on AlexNet, GoogLeNet and\nResNet [13]. Blott et al. describe an end-to-end deep learning\nframework for exploration and training of quantized neural\nnetworks that can optimize for a given platform, design target\nor speciﬁc precision [14]. Choi et al. propose a mechanism\nfor parameterized clipping activation for quantized neural\nnetworks that enables training with low precision weights [15].\nWe have previously shown that training deep neural net-\nworks with constrained learning parameters is an NP-complete\nproblem [16]. To address this problem, several evolutionary\noptimization-based approaches have been pursued in the lit-\nerature. Shen et al. propose an evolutionary optimization-\nbased learning mechanism that ﬁnds binary neural networks\nby searching through the entire search space of learning\nparameters [17]. Too et al. use a binary particle swarm\noptimization for feature extraction and compare their approach\nto other evolutionary optimization-based approaches that lever-\nage genetic algorithm, binary gravitational search algorithm\nand competitive binary grey wolf optimizer [18]. Nogami\net al. use a combination of genetic algorithm and simulated\nannealing to optimize the bin boundaries of quantization for\nCNN and test their approach on the ImageNet data set using\nAlexNet and VGG16 [19].\nThere are several limitations of the approaches proposed in\nthe literature, especially with regards to low error, low memory\nand low power edge computing systems. While the algorithms\nto train binary or ternary neural networks have the potential\nto be deployed on edge computing systems, they are very\nspecialized and cannot be used to train neural networks with a\nset of ﬁnite discrete learning parameters directly. On the other\nhand, while evolutionary optimization-based methods produce\naccurate models for quantized neural networks, they cannot\nbe deployed on edge computing systems because evolutionary\noptimization is a compute heavy process and may require large\ncompute clusters. Moreover, most of the approaches proposed\nin the literature cater to a speciﬁc deep learning model, for\nexample, convolutional neural network and it is unclear if they\nare useful for other deep learning models such as recurrent\nneural networks or generative adversarial networks.\nIn this work we propose the Combinatorial Neural Network\nTraining Algorithm (CoNNTrA), which is not restricted to any\nparticular neural network architecture, has an efﬁcient time\ncomplexity (polynomial), and does not necessarily require sig-\nniﬁcant amount of compute power. CoNNTrA is not restricted\nto binary or ternary learning parameters speciﬁcally, but can\ntrain any conﬁguration of learning parameters as long as they\nare ﬁnite and discrete. We believe CoNNTrA would be able to\ntrain deep neural networks having low error, low memory and\nlow power for edge computing systems in the post Moore’s law\nera, especially when combined with neuromorphic computing\nsystems, which are known to be resilient and energy efﬁcient\n[20], and have a wide range of applications such as graph\nalgorithms [21], [22], modeling epidemics [23] and predicting\nsupercomputer failures [24].\nIII. THE DNN TRAINING PROBLEM\nWe deﬁne the DNN training problem using the following\nnotation:\n• R, N, B: Set of real numbers, natural numbers and binary\nnumbers (B = {0, 1}) respectively.\n• T: Ternary set T = {−1, 0, +1}.\n• ω: Set of ﬁnite discrete values that the learning parameters\nW can take, for example, if learning parameters are\nrequired to have binary values, ω = B.\n• N: Number of points in the training dataset.\n• d: Dimension of each point in training dataset, which is\nthe same as number of features in the training dataset.\n• k: Number of classes for classiﬁcation.\n• X: X can be a scalar, vector, matrix or tensor containing\ntraining data.\n• Y : Y contains the labels of training data encoded in\na one-hot format. Since we have N data points and k\nclasses, Y ∈BN×k.\n• W: Set of all learning parameters, including all the\nweights and biases.\n• g(X, W): The DNN learning function.\n• e(P, Y ): The error function which computes the error\nbetween predicted labels P and ground truth labels Y .\nGiven training data X and training labels Y , we would like\nto learn the parameters W of the learning function g(X, W) by\nminimizing the error e(P, Y ). In this regard, the DNN training\nproblem with ﬁnite discrete weights is deﬁned as follows:\nmin\nW\ne(P, Y )\n(1)\nwhere, P = g(X, W) are the labels predicted by the learning\nfunction g; Each learning parameter in the set W can take\nvalues from the ﬁnite discrete set ω.\nIV. DNN TRAINING WITH CONSTRAINED LEARNING\nPARAMETERS IS NP-HARD\nWe show that under the Euclidean error function, training\na single layer neural network with binary weights is NP-Hard\nby reducing the quadratic unconstrained binary optimization\n(QUBO) problem, which is known to be NP-Hard [25]–[27],\nto the DNN training problem with ﬁnite discrete weights. We\nﬁrst deﬁne the QUBO problem:\nThe QUBO Problem:\nmin\nz∈Bd zT Az + zT b + c\n(2)\nwhere, A is a real symmetric positive deﬁnite d × d matrix,\nb is a d-dimensional vector, and c is a real scalar. Note that\nif A is not symmetric, it can be made symmetric by setting\naij = aij+aji\n2\n∀i ̸= j, without changing the QUBO problem.\nIt is known that even when A is positive deﬁnite, the QUBO\nproblem is NP-Hard [28].\nThe DNN training problem with binary weights and Eu-\nclidean error function is deﬁned as follows:\nmin\nW ∈Bd e(P, Y ) = 1\nN ||P −Y ||2\n2\n(3)\nwhere, P = g(X, W) is the vector of values predicted by\nthe learning function g(X, W) = XT W, X ∈RN×d, Y ∈\nRN, W ∈Bd. After expanding Equation 3, the SNN Training\nproblem becomes:\nmin\nW ∈Bd\n1\nN (W T XT XW −2W T XT Y + Y T Y )\n(4)\nGiven the optimal solution, we can compute the objective\nfunction value in polynomial time, so the problem is in NP.\nAlso, Equation 4 is very similar to Equation 2, in that both\nare quadratic minimization problems with binary variables. In\norder to reduce the QUBO problem to Binary SNN Training,\nwe ﬁrst decompose the real symmetric positive deﬁnite QUBO\nmatrix A into a product of a unique lower triangular matrix\nwith real positive diagonal entries L and its transpose LT using\nthe Cholesky decomposition:\nA\nCHOLESKY\n−−−−−−−→LLT\n(5)\nBecause L is a lower triangular matrix with real positive\ndiagonal entries, L−1 exists. The reduction is performed as\nfollows:\n• W = z\n•\nXT X\nN\n= A = LLT\nTherefore, X =\n√\nNLT and XT =\n√\nNL\n•\n−2XT Y\nN\n= b\nTherefore, Y = −\n√\nN\n2 L−1b\n• Because both QUBO and Binary SNN Training are\nunconstrained optimization problems, the scalars c in\nQUBO and 1\nN Y T Y in Binary SNN Training do not affect\nthe optimal solution. In order to equate the scalars in both\nproblems, we can introduce another scalar c′ in Binary\nSNN Training so that Y T Y +c′\nN\n= c without changing the\noptimal solution.\nAlgorithm 1: Discretization Subroutine for CoNNTrA\n1 Function Discretize(Wpre, ω):\nInput:\nWpre: Pretrained Weights\nω: Set of Finite Discrete Values\nOutput:\nW: Discretized Weights\n2\nWeights: W = Zeros(|Wpre|);\n3\nω = Sort(ω);\n4\nfor i = 1 to |Wpre| do\n5\nfor j = 1 to |ω| do\n6\nif j == |ω| then\n7\nW[i] = ω[j];\n8\nend\n9\nelse if Wpre[i] ≤1\n2(ω[j] + ω[j + 1]) then\n10\nW[i] = ω[j];\n11\nbreak;\n12\nend\n13\nend\n14\nend\n15\nreturn W\nBy setting W = z, X =\n√\nNLT and Y = −\n√\nN\n2 L−1b,\nwe have reduced the QUBO problem to Binary SNN Training\nproblem, thus showing that Binary SNN Training problem is\nNP-Hard. With more complex error functions like softmax and\ncomplex neural network architectures like deep convolutional\nor recurrent neural networks, the SNN training problem is at\nleast as hard as the QUBO problem, if not more.\nV. COMBINATORIAL NEURAL NETWORK TRAINING\nALGORITHM (CONNTRA)\nWe propose the Combinatorial Neural Network Training\nAlgorithm (CoNNTrA), which is a coordinate gradient descent\nbased algorithm for training DNNs with ﬁnite discrete weights.\nCoNNTrA is presented in Algorithm 1 and Algorithm 2. Say\nwi = ωj for some i and j. We ﬁrst look at the left and right\ngradients of the error function.\nLeft Gradient:\n∂e\n∂wi\n\f\f\f\f\f\nleft\n= lim\nh→0\ne(wi) −e(wi −h)\nh\n(6)\nRight Gradient:\n∂e\n∂wi\n\f\f\f\f\f\nright\n= lim\nh→0\ne(wi + h) −e(wi)\nh\n(7)\nThese gradients could be used if wi could take continuous\nvalues. Since wi cannot take continuous values, h never tends\nAlgorithm 2: CoNNTrA: Combinatorial Neural Network\nTraining Algorithm\n1 Function CoNNTrA(X, Y , Wpre, ω, g(X, W),\ne(P, Y )):\nInput:\nX: Training Data\nY ∈BN×k: Training Labels (One-Hot Format)\nWpre: Pretrained Weights (Reshaped into a Single\n1-Dimensional Array)\nω: Set of Finite Discrete Values\ng(X, W): Spiking Neural Network Function\ne(P, Y ): Error Function\nOutput:\nWopt: Optimal Weights\nϵopt: Optimal Error\n/* PHASE 1: DISCRETIZATION\n*/\n2\nWeights: W = Discretize(Wpre, ω)\n/* PHASE 2: INITIALIZATION\n*/\n3\nError: ϵ = e(g(X, W), Y );\n4\nOptimal Weights: Wopt = W;\n5\nOptimal Error: ϵopt = ϵ;\n6\nNumber of training iterations: T;\n/* PHASE 3: TRAINING\n*/\n7\nfor t = 1 to T do\n8\nfor i = 1 to |W| do\n9\ni\n′ = RandomInteger(|W|);\n10\nfor j = 1 to |ω| do\n11\nW[i\n′] = ω[j];\n12\nϵ = e(g(X, W), Y );\n13\nif ϵ ≤ϵopt then\n14\nϵopt = ϵ;\n15\nWopt = W;\n16\nend\n17\nend\n18\nW[i\n′] = Wopt[i\n′];\n19\nend\n20\nend\n21\nreturn Wopt, ϵopt\nto 0 in the above equations, but is some ﬁnite number greater\nthan 0. So, we look at the discrete counterparts of gradients:\nLeft Discrete Gradient:\n∆e\n∆wi\n\f\f\f\f\f\nleft\n= e(wi = ωj) −e(wi = ωj−1)\nωj −ωj−1\n(8)\nRight Discrete Gradient:\n∆e\n∆wi\n\f\f\f\f\f\nright\n= e(wi = ωj+1) −e(wi = ωj)\nωj+1 −ωj\n(9)\nThese discrete counterparts of gradients search in the dis-\ncrete vicinity of wi to ﬁnd a value that lowers the error. This is\na local search, and makes up to three calls to the error function,\ni.e. e(wi = ωj−1), e(wi = ωj) and e(wi = ωj+1). We extend\nthis notion of local search and do a global search, i.e. search\nthrough all possible values of wi to ﬁnd the best value that\nminimizes the error function. This makes O(|ω|) calls to the\nerror function. In this case, we have a better chance of ﬁnding\na lower value of error function at each iteration. When we\ndo a similar procedure for all the weights, we iteratively ﬁnd\nbetter weight values that decrease the error function gradually\nas training progresses.\nCoNNTrA takes as inputs the training data X, the training\nlabels Y , pretrained weights W, set of ﬁnite discrete values\nthat the weights can take ω, the SNN function g(X, W) and\nthe error function e(P, Y ). Initial weights are the weights\nobtained when the DNN was trained using Backpropagation\nby relaxing the ﬁnite discrete value constraint. The ﬁrst step is\nto discretize the weights using Algorithm 1. In Algorithm 1,\nwe set all the weights in the vicinity of ωj to ωj. For example,\nif ω = {−1, 0, +1}, then for some i, if the pretrained weight\nwi > 0.5, it would be set to +1, if −0.5 < wi ≤0.5, it would\nbe set to 0, and if wi ≤−0.5, it would be set to −1. We start\nby initializing the weights W to an array of zeros using the\nfunction Zeros(x), which returns a zero initialized array of\nlength x, in line 2 of Algorithm 1. Next, we sort ω so that\nall values in ω are in increasing order in line 3. Next, in the\nfor loop from line 4 through 14, we iterate over each weight\nin W, and in the for loop from line 5 through 13, we iterate\nover each value in ω to ﬁnd an appropriate discretized value\nfor each weight. The discretized weight value is assigned to\nthe appropriate weight on either line 7 or 10.\nIn the second phase of the algorithm, i.e. the initialization\nphase, we ﬁrst compute the error using the discretized weights\nW and assign it to the initial error ϵ on line 3. Next, we\ninitialize the optimal weights Wopt and optimal error ϵopt, by\nsetting them to W and ϵ on lines 4 and 5 respectively. We\nthen deﬁne the number of training iterations T.\nIn the third phase of the algorithm, i.e. the training phase,\nwe iterate over T training iterations in lines 7–20. During\neach iteration, we perform a global search over |W| randomly\nselected weights in lines 8–19. We refer to each random\nselection of weights as an epoch—so, there are a total of\nT × |W| training epochs. During each training epoch, we\nﬁrst randomly select a weight index i\n′ using the function\nRandomInteger(x), which returns a uniform random in-\nteger in the interval [1, x]. For wi′, we perform a global search\nover all possible values of wi′ to ﬁnd the best value that\nminimizes the error function in lines 10–17 of Algorithm 2.\nIf a better value for wi′ is found, we update the optimal error\nϵopt and optimal weights Wopt in lines 14 and 15 respectively.\nAfter a global search is performed for wi′, we set the current\nweights W to the optimal weights Wopt in line 18, so that in\nthe subsequent epochs, we use the current best set of weights.\nFinally, after all the training epochs are completed, we return\nWopt and ϵopt in line 21 of Algorithm 2.\nA. Time Complexity\nWe analyze the running time of CoNNTrA by going over\nthe running time of each line in Algorithm 1 and Algorithm 2.\nIn the ﬁrst phase, initializing the weights (line 2 of Algorithm\n1) takes O(|W|) time, and sorting ω takes O(|ω| log |ω|) time.\nNext, the two for loops in lines 4 through 14 of Algorithm 1\ntake up O(|W|·|ω|) time. So the running time of discretization\nphase is O(|W| · |ω|). We assume that time taken to do a\nforward pass on the SNN (i.e. computing P = g(X, W)) and\ncomputing the error e(P, Y ) takes τ = O(e(g(X, W), Y ))\namount of time. Therefore, it takes O(τ) time to compute\nthe error on line 3 of Algorithm 2. It takes O(|W|) time to\ninitialize Wopt on line 4 of Algorithm 2. Lines 5 and 6 take\nO(1) time. So, the initialization phase takes O(τ +|W|) time.\nIn the training phase, the for loop from lines 7 through 20 in\nAlgorithm 2 runs T times. The for loop from lines 8 through\n19 runs |W| times and the for loop from lines 10 through 17\nruns |ω| times. It takes O(τ) time to compute the error ϵ on\nline 14. Therefore, the training phase takes O(T ·|W|·|ω|·τ)\ntime. Since this dominates the running time of all phases, the\nrunning time for CoNNTrA is O(T · |W| · |ω| · τ). Since τ is\nusually a polynomial time expression in the number of weights\nand size of training dataset, CoNNTrA is a polynomial time\nalgorithm.\nB. Convergence\nDuring each epoch in the training phase, we update the\noptimal weights Wopt and optimal error ϵopt only if the current\nerror ϵ is lower than ϵopt. Thus, with every update, ϵopt\ngets closer and closer to 0.0 demonstrating convergence. If\nCoNNTrA is run for enough number of epochs, the optimal\nerror would converge to a local minimum.\nVI. PERFORMANCE EVALUATION\nWe compare the performance of CoNNTrA (Algorithm 2)\nto traditional Backpropagation using GPU on four bench-\nmark problems: MNIST using a logistic regression classiﬁer,\nMNIST using a convolutional neural network (CNN), Iris\nusing a deep neural network (DNN), and ImageNet using a\nconvolutional neural network (CNN). The performance metrics\nused for this comparison are:\n1) Training Error: Percentage of data points classiﬁed incor-\nrectly in the training dataset.\n2) Validation Error: Percentage of data points classiﬁed\nincorrectly in the validation dataset.\n3) Memory Usage (kilobytes): Amount of memory used to\nstore the weights.\n4) Training Time (seconds): Total time taken to complete\ntraining.\nCoNNTrA was written in Python using the Numpy library\n[29]. The Backpropagation algorithm was run using the Ten-\nsorFlow library [30] on GPUs. All experimental runs were run\non a machine that had 32 cores of two-way multi-threaded\nIntel Xeon CPUs running at 2.60 GHz, three NVIDIA GPUs\n(GeForce GTX 1080 Titan, GeForce GTX 950 and GeForce\nFig. 1: Schematic diagram of MNIST logistic regression model\nTABLE I: Performance metrics for MNIST logistic regression\nPerformance Metric\nBackpropagation\nCoNNTrA\nTraining Error (%)\n6.25\n7.59\nValidation Error (%)\n7.34\n8.44\nMemory Usage (kilobytes)\n62.8\n1.96\nTraining Time (seconds)\n81.70\n236.12\n(a) Training Error Comparison\n(b) Validation Error Comparison\nFig. 2: Error comparison for MNIST logistic regression models\nGTX 670), 112 GB DIMM Synchronous RAM, 32 KB L1\ncache, 256 KB L2 cache and 20 MB L3 cache.\nTABLE II: Performance metrics for MNIST CNN\nPerformance Metric\nBackpropagation\nCoNNTrA\nTraining Error (%)\n1.40\n2.60\nValidation Error (%)\n1.56\n2.39\nMemory Usage (kilobytes)\n649.55\n20.30\nTraining Time (seconds)\n121.97\n4,871.04\n1) MNIST Logistic Regression: We used a logistic regres-\nsion model to classify the MNIST images. The inputs to\nthe logistic regression model were vectorized MNIST images,\neach of size 784×1. The outputs to the model were the labels\nof the input images encoded in a one-hot format. The model\nconsisted of a weight matrix of size 784×10 and a bias vector\nof size 10 × 1. A schematic diagram of the logistic regression\nmodel is shown in Figure 1. The activation function for this\nmodel was softmax and the loss was computed using the cross\nentropy loss function.\nTable I shows the performance metrics of Backpropagation\nand CoNNTrA for the MNIST task using a logistic regression\nclassiﬁer. The training errors for Backpropagation and CoN-\nNTrA are 6.25% and 7.59% respectively, and the validation\nerrors are 7.32% and 8.44% respectively. The memory usage\nfor Backpropagation and CoNNTrA is 62.8 and 1.96 kilobytes\nrespectively. While Backpropagation takes 81.70 seconds to\ncomplete training, CoNNTrA takes 236.12 seconds. Figure 2\nshows the plot of training and validation errors for CoNNTrA\n(red) and Backpropagation (blue). These errors were computed\nas percentage of misclassiﬁed points in the training and\nvalidation datasets respectively. The X-axis in Figures 2a and\n2b shows the percentage of training completed. The Y-axis\nshows the classiﬁcation errors as a percentage. As training\nprogresses, both algorithms converge to the same ballpark of\n6 −8%, which corresponds to an accuracy of 92 −94%.\n2) MNIST CNN: We use the LeNet architecture proposed\nby LeCun et al. [31]. The training results for MNIST CNN\nmodels are shown in Table II and Figure 3. The training\nand validation errors for Backpropagation are 1.40% and\n1.56% respectively, and those for CoNNTrA are 2.60% and\n2.39% respectively. The memory usage for Backpropagation is\n649.55 kilobytes, while that for CoNNTrA is 20.30 kilobytes.\nWhile Backpropagation takes 121.97 seconds, CoNNTrA takes\n4, 871.04 seconds to complete training. Figure 3 shows the\ntraining and validation errors for Backpropagation (blue) and\nCoNNTrA (red). The ﬁnal training and validation errors ob-\ntained by both models are around 1 −3%, which is the state\nof the art for the LeNet CNN, and corresponds to an accuracy\nof 97 −99%. The rate of convergence for CoNNTrA, shows\nan interesting behavior. The rate of convergence gradually\ndecreases until just over 80% of training is completed, after\nwhich, it starts decreasing rapidly and converges to 2.60% in\nFigure 3a and 2.39% in Figure 3b. We attribute this behavior\nto the following reason. At every training epoch in CoNNTrA,\nwe pick a weight at random and perform a global search across\nall possible values to ﬁnd a value that yields the smallest\n(a) Training Error Comparison (MNIST CNN)\n(b) Validation Error Comparison (MNIST CNN)\nFig. 3: Error comparison for MNIST CNN models\nFig. 4: Schematic diagram of Iris multi-layer perceptron\npossible error. When the rate of convergence started decreasing\nrapidly, a weight was picked which had a high impact on the\nclassiﬁcation error. When a global search was performed for\nthis weight, it drastically improved the error and transformed\nthe neural network function in such a way that there was\nabundant room to improve the error for subsequently selected\nweights.\n3) Iris: We use a three layer deep multi-layer perceptron\nmodel having two hidden layers for this classiﬁcation task.\nFigure 4 shows a schematic diagram of the multi-layer per-\nceptron model. Each neuron in the hidden layers is indexed\nTABLE III: Performance metrics for Iris DNN\nPerformance Metric\nBackpropagation\nCoNNTrA\nTraining Error (%)\n1.67\n1.67\nValidation Error (%)\n3.33\n3.33\nMemory Usage (kilobytes)\n1.88\n0.06\nTraining Time (seconds)\n4.56\n4.92\n(a) Training Error Comparison (Iris DNN)\n(b) Validation Error Comparison (Iris DNN)\nFig. 5: Error comparison for Iris DNN models\nusing a superscript and a subscript. The superscript indicates\nthe layer index and the subscript indicates the neuron index\nwithin that layer.\nTable III shows the performance metrics for the Iris DNN\nmodels. Both models achieved the same training and valida-\ntion errors, i.e. 1.67% and 3.33% respectively. The memory\nusage for Backpropagation was 1.88 kilobytes, while that\nfor CoNNTrA was 0.06 kilobytes. The training time for\nBackpropagation was 4.56 seconds and that for CoNNTrA\nwas 4.92 seconds. Figure 5 shows the plot of training and\nvalidation errors for Backpropagation (in blue) and CoNNTrA\n(in red). Training errors for both algorithms follow each other\nclosely and converge at 1.67%. Validation error for CoNNTrA\nis seen to vary abruptly initially until it starts to converge at\naround the 15% mark with sporadic spikes. The validation\nerrors for both algorithms converge to 3.33%.\n4) ImageNet:\nWe use the AlexNet CNN proposed by\nKrizhevsky et al. [32]. Table IV shows the performance\nmetrics. While Backpropagation takes 388, 764.42 seconds,\nTABLE IV: Performance metrics for ImageNet CNN\nPerformance Metric\nBackpropagation\nCoNNTrA\nTraining Error (%)\n15.12\n16.98\nValidation Error (%)\n18.62\n18.50\nMemory Usage (kilobytes)\n499,026.75\n15,594.59\nTraining Time (seconds)\n388,764.43\n647,249.96\n(a) Training Error Comparison (ImageNet CNN)\n(b) Validation Error Comparison (ImageNet CNN)\nFig. 6: Error comparison for ImageNet CNN models\nCoNNTrA takes 647, 249.96 seconds to complete training.\nThe training errors for Backpropagation and CoNNTrA are\n15.12% and 16.98% respectively, and the validation errors\nare 18.62% and 18.50% respectively. All errors computed for\nImageNet CNN model are top-5 errors. The memory used by\nBackpropagation is 499, 026.75 kilobytes, and that used by\nCoNNTrA is 15, 594.59 kilobytes. Figure 6 shows the training\nand validation errors for training the ImageNet CNN model\nusing Backpropagation (blue) and CoNNTrA (red). We see a\nregular trend in Figure 6. Both models converge to around\n15 −17% training error and 16 −18% validation error, which\nare in the same ballpark as the state of the art errors for the\nAlexNet model.\nA. Discussion\nFigure 7 presents the performance of Backpropagation\nand CoNNTrA in a consolidated fashion. The X-axis shows\ndatasets and models, and the Y-axis shows the performance\nmetric. Blue and red bars denote Backpropagation and CoN-\n(a) Training Time\n(b) Training Error\n(c) Validation Error\n(d) Memory Usage\nFig. 7: Comparison of Backpropagation and CoNNTrA\nNTrA performance metrics respectively. In Figure 7a, we\nobserve that CoNNTrA takes more time to complete training\nfor all tasks. This is because we were using a serialized imple-\nmentation of CoNNTrA as our objective was to demonstrate\na proof of concept that CoNNTrA is able to train models\nhaving accuracies at par with Backpropagation. With parallel\nimplementations of CoNNTrA, we expect the training times\nto signiﬁcantly reduce.\nIn Figures 7b and 7c, the training and validation errors for\nBackpropagation and CoNNTrA are within the same ballpark\nand are at par with the state of the art error values for the\nrespective models. The CoNNTrA errors fall slightly short of\nBackpropagation errors for all models except Iris DNN. Given\nthe same architecture of the models for both Backpropagation\nand CoNNTrA, the CoNNTrA weights were ternary whereas\nBackpropagation used double precision ﬂoating point weights.\nGoing from double precision to ternary weights resulted in less\nexpressibility for CoNNTrA, because of which we see slightly\nhigher value of training and validation errors.\nIn Figure 7d, the memory usage for CoNNTrA is about\n32× less than Backpropagation for all models. It requires 2\nbits to store each ternary valued CoNNTrA weight, and 64 bits\nto store each double precision ﬂoating point Backpropagation\nweight. A 32× reduction in memory usage is extremely sig-\nniﬁcant in edge computing applications, especially embedded\nsystems, Internet of Things, autonomous vehicles etc.\nVII. CONCLUSION\nEdge computing systems in applications like Internet of\nThings (IoT), autonomous vehicles and embedded systems\nin the post Moore’s law era will require machine learning\nmodels that not only produce low error and train fast, but also\nconsume low memory and power. While traditional learning\nalgorithms like Backpropagation can train deep learning mod-\nels in a reasonable amount of time and obtain low error, they\nconsume signiﬁcantly large memory and power. In this work,\nwe propose a novel learning algorithm called Combinatorial\nNeural Network Training Algorithm (CoNNTrA), which is\na coordinate gradient descent-based algorithm that can train\ndeep learning models having constrained learning parameters,\nfor example, having binary or ternary values.\nThe objective of this study was to demonstrate that CoN-\nNTrA can train deep learning models with constrained learning\nparameters, which yield errors at par with the Backpropa-\ngation models, and consume signiﬁcantly less memory. We\npresented CoNNTrA in Section V along with its theoretical\nunderpinnings and complexity analysis. In Section VI, we used\nCoNNTrA to train deep learning models for three machine\nlearning benchmark data sets (MNIST, Iris and ImageNet). We\ndemonstrated that CoNNTrA can train these models having\nerrors in the same ballpark as Backpropagation models. More\nimportantly, we showed that the CoNNTrA models consume\n32× less memory than the Backpropagation models.\nIn our future work, we would like to implement CoNNTrA\nin an efﬁcient parallelized fashion to improve the training\ntimes. We believe that such a parallel implementation of CoN-\nNTrA would be able to train deep learning models that are not\njust accurate and consume orders of magnitude less memory\nthan Backpropagation, but can also be trained efﬁciently. This\nwould be invaluable to training machine learning and deep\nlearning models in the post Moore’s law era, especially for\nedge computing systems supporting critical applications. We\nwould also like to study the applicability of CoNNTrA for\nsolving other NP-complete problems like traveling salesman\nproblem, protein folding, and genetic imputation.\nREFERENCES\n[1] A. Karpatne, G. Atluri, J. H. Faghmous, M. Steinbach, A. Banerjee,\nA. Ganguly, S. Shekhar, N. Samatova, and V. Kumar, “Theory-guided\ndata science: A new paradigm for scientiﬁc discovery from data,” IEEE\nTransactions on knowledge and data engineering, vol. 29, no. 10, pp.\n2318–2331, 2017.\n[2] P. Sharma and A. Raglin, “Iot in smart cities: Exploring information the-\noretic and deep learning models to improve parking solutions,” in 2019\nIEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced &\nTrusted Computing, Scalable Computing & Communications, Cloud &\nBig Data Computing, Internet of People and Smart City Innovation\n(SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI).\nIEEE, 2019,\npp. 1860–1865.\n[3] P. Sundaravadivel, K. Kesavan, L. Kesavan, S. P. Mohanty, and\nE. Kougianos, “Smart-log: A deep-learning based automated nutrition\nmonitoring system in the iot,” IEEE Transactions on Consumer Elec-\ntronics, vol. 64, no. 3, pp. 390–398, 2018.\n[4] C. D. Naylor, “On the prospects for a (deep) learning health care\nsystem,” Jama, vol. 320, no. 11, pp. 1099–1100, 2018.\n[5] J. B. Aimone, “Neural algorithms and computing beyond moore’s law,”\nCommunications of the ACM, vol. 62, no. 4, pp. 110–110, 2019.\n[6] C. Mailhiot, “Energy-efﬁcient edge computing: Challenges and opportu-\nnities beyond moore? s law.” Sandia National Lab.(SNL-CA), Livermore,\nCA (United States), Tech. Rep., 2018.\n[7] C. D. Schuman, T. E. Potok, R. M. Patton, J. D. Birdwell, M. E. Dean,\nG. S. Rose, and J. S. Plank, “A survey of neuromorphic computing and\nneural networks in hardware,” arXiv preprint arXiv:1705.06963, 2017.\n[8] M. Courbariaux, Y. Bengio, and J.-P. David, “Binaryconnect: Training\ndeep neural networks with binary weights during propagations,” in\nAdvances in neural information processing systems, 2015, pp. 3123–\n3131.\n[9] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “Xnor-net:\nImagenet classiﬁcation using binary convolutional neural networks,” in\nEuropean Conference on Computer Vision.\nSpringer, 2016, pp. 525–\n542.\n[10] D. Wan, F. Shen, L. Liu, F. Zhu, J. Qin, L. Shao, and H. Tao Shen, “Tbn:\nConvolutional neural network with ternary inputs and binary weights,” in\nProceedings of the European Conference on Computer Vision (ECCV),\n2018, pp. 315–332.\n[11] R. Andri, L. Cavigelli, D. Rossi, and L. Benini, “Yodann: An ultra-\nlow power convolutional neural network accelerator based on binary\nweights,” in 2016 IEEE Computer Society Annual Symposium on VLSI\n(ISVLSI).\nIEEE, 2016, pp. 236–241.\n[12] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio,\n“Quantized neural networks: Training neural networks with low pre-\ncision weights and activations,” The Journal of Machine Learning\nResearch, vol. 18, no. 1, pp. 6869–6898, 2017.\n[13] S. Zhou, H. Wen, T. Xiao, and X. Zhou, “Iqnn: Training quantized neural\nnetworks with iterative optimizations,” in International Conference on\nArtiﬁcial Neural Networks.\nSpringer, 2017, pp. 688–695.\n[14] M. Blott, T. B. Preußer, N. J. Fraser, G. Gambardella, K. O’brien,\nY. Umuroglu, M. Leeser, and K. Vissers, “Finn-r: An end-to-end deep-\nlearning framework for fast exploration of quantized neural networks,”\nACM Transactions on Reconﬁgurable Technology and Systems (TRETS),\nvol. 11, no. 3, pp. 1–23, 2018.\n[15] J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan,\nand K. Gopalakrishnan, “Pact: Parameterized clipping activation for\nquantized neural networks,” arXiv preprint arXiv:1805.06085, 2018.\n[16] P. Date, “Combinatorial neural network training algorithm for neuromor-\nphic computing,” Ph.D. dissertation, Rensselaer Polytechnic Institute,\n2019.\n[17] M. Shen, K. Han, C. Xu, and Y. Wang, “Searching for accurate\nbinary neural architectures,” in Proceedings of the IEEE International\nConference on Computer Vision Workshops, 2019, pp. 0–0.\n[18] J. Too, A. R. Abdullah, and N. Mohd Saad, “A new co-evolution binary\nparticle swarm optimization with multiple inertia weight strategy for\nfeature selection,” in Informatics, vol. 6, no. 2. Multidisciplinary Digital\nPublishing Institute, 2019, p. 21.\n[19] W. Nogami, T. Ikegami, R. Takano, T. Kudoh et al., “Optimizing\nweight value quantization for cnn inference,” in 2019 International Joint\nConference on Neural Networks (IJCNN).\nIEEE, 2019, pp. 1–8.\n[20] C. D. Schuman, J. P. Mitchell, J. T. Johnston, M. Parsa, B. Kay, P. Date,\nand R. M. Patton, “Resilience and robustness of spiking neural networks\nfor neuromorphic systems,” in International Joint Conference on Neural\nNetworks 2020, 2020, pp. 1–10.\n[21] K. Hamilton, T. Mintz, P. Date, and C. D. Schuman, “Spike-based graph\ncentrality measures,” in International Conference on Neuromorphic\nSystems 2020, 2020, pp. 1–8.\n[22] B. Kay, P. Date, and C. Schuman, “Neuromorphic graph algorithms:\nExtracting longest shortest paths and minimum spanning trees,” in\nProceedings of the Neuro-inspired Computational Elements Workshop,\n2020, pp. 1–6.\n[23] K. Hamilton, P. Date, B. Kay, and C. Schuman D, “Modeling epidemic\nspread with spike-based models,” in International Conference on Neu-\nromorphic Systems 2020, 2020, pp. 1–5.\n[24] P. Date, C. D. Carothers, J. A. Hendler, and M. Magdon-Ismail,\n“Efﬁcient classiﬁcation of supercomputer failures using neuromorphic\ncomputing,” in 2018 IEEE Symposium Series on Computational Intelli-\ngence (SSCI).\nIEEE, 2018, pp. 242–249.\n[25] D. Wang and R. Kleinberg, “Analyzing quadratic unconstrained binary\noptimization problems via multicommodity ﬂows,” Discrete Applied\nMathematics, vol. 157, no. 18, pp. 3746–3753, 2009.\n[26] E. Boros, P. L. Hammer, and G. Tavares, “Local search heuristics\nfor quadratic unconstrained binary optimization (qubo),” Journal of\nHeuristics, vol. 13, no. 2, pp. 99–132, 2007.\n[27] P. M. Pardalos and S. Jha, “Complexity of uniqueness and local search\nin quadratic 0–1 programming,” Operations research letters, vol. 11,\nno. 2, pp. 119–123, 1992.\n[28] G. Kochenberger, J.-K. Hao, F. Glover, M. Lewis, Z. L¨u, H. Wang, and\nY. Wang, “The unconstrained binary quadratic programming problem:\na survey,” Journal of Combinatorial Optimization, vol. 28, no. 1, pp.\n58–81, 2014.\n[29] S. Van Der Walt, S. C. Colbert, and G. Varoquaux, “The numpy array:\na structure for efﬁcient numerical computation,” Computing in Science\n& Engineering, vol. 13, no. 2, p. 22, 2011.\n[30] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for large-\nscale machine learning,” in 12th {USENIX} Symposium on Operating\nSystems Design and Implementation ({OSDI} 16), 2016, pp. 265–283.\n[31] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning\napplied to document recognition,” Proc. IEEE, vol. 86, no. 11, pp. 2278–\n2324, 1998.\n[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances Neural Inf.\nProcess. Syst., 2012, pp. 1097–1105.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML",
    "68T07, 90C27",
    "I.2.6"
  ],
  "published": "2020-09-01",
  "updated": "2020-09-01"
}