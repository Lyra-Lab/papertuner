{
  "id": "http://arxiv.org/abs/2009.00953v1",
  "title": "Unsupervised Feature Learning by Autoencoder and Prototypical Contrastive Learning for Hyperspectral Classification",
  "authors": [
    "Zeyu Cao",
    "Xiaorun Li",
    "Liaoying Zhao"
  ],
  "abstract": "Unsupervised learning methods for feature extraction are becoming more and\nmore popular. We combine the popular contrastive learning method (prototypical\ncontrastive learning) and the classic representation learning method\n(autoencoder) to design an unsupervised feature learning network for\nhyperspectral classification. Experiments have proved that our two proposed\nautoencoder networks have good feature learning capabilities by themselves, and\nthe contrastive learning network we designed can better combine the features of\nthe two to learn more representative features. As a result, our method\nsurpasses other comparison methods in the hyperspectral classification\nexperiments, including some supervised methods. Moreover, our method maintains\na fast feature extraction speed than baseline methods. In addition, our method\nreduces the requirements for huge computing resources, separates feature\nextraction and contrastive learning, and allows more researchers to conduct\nresearch and experiments on unsupervised contrastive learning.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nUnsupervised Feature Learning by Autoencoder and Prototypical\nContrastive Learning for Hyperspectral Classiﬁcation\nZeyu Cao1, Xiaorun Li1, Liaoying Zhao2\nUnsupervised learning methods for feature extraction are becoming more and more popular. We combine the popular contrastive\nlearning method (prototypical contrastive learning) and the classic representation learning method (autoencoder) to design an\nunsupervised feature learning network for hyperspectral classiﬁcation. Experiments have proved that our two proposed autoencoder\nnetworks have good feature learning capabilities by themselves, and the contrastive learning network we designed can better\ncombine the features of the two to learn more representative features. As a result, our method surpasses other comparison methods\nin the hyperspectral classiﬁcation experiments, including some supervised methods. Moreover, our method maintains a fast feature\nextraction speed than baseline methods. In addition, our method reduces the requirements for huge computing resources, separates\nfeature extraction and contrastive learning, and allows more researchers to conduct research and experiments on unsupervised\ncontrastive learning.\nIndex Terms—unsupervised learning, autoencoder, contrastive learning, hyperspectral classiﬁcation\nI. INTRODUCTION\nW\nITH\nthe\nrapid\ndevelopment\nof\ndeep\nlearning,\nmuch progress has been made in the hyperspec-\ntral classiﬁcation ﬁeld. Recent years, many great super-\nvised deep learning methods for hyperspectral classiﬁca-\ntion were proposed, such as one-dimensional convolutional\nneural networks(1DCNN)[1], three-dimensional convolutional\nneural networks(3DCNN)[2], deep recurrent neural networks\n(RNN)[3], deep feature fusion network(DFFN)[4], spectral-\nspatial residual network(SSRN)[5]. Nowadays, it is widely\nadmitted that the fusion of spatial information and spectral in-\nformation is the best strategy to extract features for hyperspec-\ntral classiﬁcation. Furthermore, deep learning models showed\ngreat power in ﬁnding the two kinds of information. With\nenough labeled samples, many supervised learning methods\ncan effectively learn the spatial and spectral information si-\nmultaneously(e.g., SSRN) and update the models’ parameters,\nthus getting good hyperspectral classiﬁcation results. However,\nthere are some disadvantages to supervised learning methods.\nFirstly, severe data dependence is the notorious disadvantage\nof supervised deep learning. Without enough labeled samples,\nmany supervised deep learning methods can not work or\nonly get bad results. Secondly, many supervised deep learning\nmethods only rely on the labels to supervise the extraction\nof features. As a result, with the increase of labeled data, the\nmodels need to be retrained entirely, which seems expensive\nin the practical circumstances.\nResearchers are also paying their attention to unsupervised\nlearning methods to avoid the disadvantages of supervised\ndeep learning. Unsupervised learning methods are mostly\nfeature extraction algorithms, with a suitable feature extractor,\nwe can use a simple classiﬁer to get good classiﬁcation results.\nTo some extent, unsupervised learning methods are more\nManuscript received December 1, 2012; revised August 26, 2015. Corre-\nsponding author: Xiaorun Li (email:lxr@zju.edu.cn). 1Zhejiang University,\nCollege of Electrical Engineering, 866 Yuhangtang Rd, Hangzhou, China,\n310058,2HangZhou Dianzi University, China Institute of Computer Applica-\ntion Technology, Xiasha Higher Education Zone, Hangzhou, China, 310018\nconvenient to be applied to practice. Speak of unsupervised\nlearning, there are some classic methods widely applied to all\nkinds of ﬁelds, such as principal component analysis(PCA)[6]\nand independent component analysis(ICA)[7]. These tradi-\ntional algorithms remain efﬁcient in the pre-process or feature\nextraction ﬁeld.\nGenerally speaking, unsupervised learning aims to dig\nout information from data other than labels; there are two\nmajor ways to achieve the objective: representative learn-\ning and discriminative learning. There are two famous rep-\nresentative learning models: autoencoder[8] and generative\nadversarial network(GAN)[9]. The two methods both use the\nself-similarity of data and then construct a model that can\nmap the original data to some certain distributions. With\nthe generated distributions, we can get distributions similar\nto the distributions of real samples. As a result, we can\nget forgery samples that are similar to real samples or get\ngood feature extractors. Inspired by autoencoder, stacked\nsparse autoencoder(SSAE)[10] used autoencoder for sparse\nspectral feature learning and multiscale spatial feature learn-\ning, respectively and then fused these two kinds of feature\nfor hyperspectral classiﬁcation. Furthermore, GAN[11] also\nproved to be useful in extracting spectral-spatial features for\nhyperspectral classiﬁcation. Except for these, 3D convolutional\nautoencoder(3DCAE)[12], semi-supervised GAN[13], GAN-\nAssisted CapsNet(TripleGAN)[14] and many other papers also\noriginated from the representative learning. Generally, many\nresearchers have acknowledged the utility of the representative\nlearning methods in the remote sensing ﬁeld.\nOn the other hand, discriminative learning is also an original\nidea in exploring the information of data. A typical way of us-\ning discriminative learning is the contrastive learning method.\nUnlike autoencoder and GAN, the contrastive learning method\ndoes not care about the details of the data. It cares about\nthe difference between the different samples. In other words,\ncontrastive learning aims to discriminate different samples\nand does not aim to ﬁnd samples’ exact distributions. By\ncomparing different samples, a discriminator is trained and can\narXiv:2009.00953v1  [cs.CV]  2 Sep 2020\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\nbe used for classiﬁcation or other tasks. Because by ﬁnding the\ndifference between two randomly selected samples, somehow\nthe discriminator has learnt to extract features that are helpful\nfor downstream tasks. Contrastive learning can be applied in\nsupervised as well as unsupervised learning. For supervised\ncontrastive learning, contrastive loss[15] is proposed to op-\ntimize the discriminator. In the hyperspectral classiﬁcation\nﬁeld, siamese convolutional neural networks[16] adopted the\noriginal siamese network for scene classiﬁcation. The end-to-\nend siamese convolutional neural network[17] proposed a 1D\nsiamese CNN network for hyperspectral classiﬁcation. Dual-\npath siamese convolutional neural network[18] proposed a\ndual-path siamese network composed of 2D and 1D CNN\nfor hyperspectral classiﬁcation. Supervised learning has drawn\nmany researchers’ attention.\nFor unsupervised contrastive learning, there are no label\ninformation can be utilized at all. So a paradigm is widely\nused for unsupervised learning. Because contrastive learning\nrequires the contrast between two samples, we use two kinds\nof data augmentation methods to transform a single sample\ninto two transformed samples. In this way, we create a kind\nof label: the two samples originate from the same sample.\nThen with a proper structure and a well-designed objective\nfunction, we can train an encoder that can map the samples\nto latent encoding space, and the features from the encoding\nspace can well represent the original samples. When the\nencoder acts as a feature extractor, many downstream tasks\ncan be done. Much work has been done in the computer\nvision ﬁeld, such as momentum contrast[19] and a simple\nframework for contrastive learning[20]. They proposed some\noriginal structures for unsupervised contrastive learning and\ngot promising results in many visual datasets. Furthermore,\nprototypical contrastive learning(PCL)[21] combined cluster\nalgorithm with contrastive learning, got better performances\nthan other methods, even outperformed some supervised meth-\nods. Generally, unsupervised contrastive learning has shown its\ngreat power in the computer vision ﬁeld.\nConsidering the similarity between hyperspectral images\nand normal images, we think it is possible to transfer the unsu-\npervised contrastive learning methods to hyperspectral classiﬁ-\ncation. However, as far as we know, few related methods have\nbeen reported in the hyperspectral classiﬁcation ﬁeld. There\nare some problems to address to transfer the unsupervised\ncontrastive learning to the process of hyperspectral image.\nFirstly, the computer vision ﬁeld’s typical data augmentation\nmethods seem not reasonable in the hyperspectral classiﬁca-\ntion ﬁeld. For example, color distortion to the normal images\nwill not change objects’ spatial features, so it is an acceptable\ntransformation. When distorting hyperspectral images’ spectral\nvalue, the spectral information is destroyed, and the spectral\ninformation is essential for hyperspectral classiﬁcation, so this\nis not an acceptable transformation for hyperspectral images.\nSecondly, unsupervised contrastive learning for normal images\ndemands large batch size and lots of GPUs, if it is directly\napplied to hyperspectral images, the requirement for more\ncomputing resources will be too expensive.\nTo apply the unsupervised contrastive learning to hyper-\nspectral classiﬁcation, we addressed the two problems and\nproposed ContrastNet, a deep learning model for unsupervised\nfeature learning of hyperspectral images. Our innovations can\nbe concluded as follows:\n1) We introduced discriminative learning methods into hy-\nperspectral images. Moreover, we applied the prototypical\ncontrastive learning to unsupervised feature learning of\nhyperspectral classiﬁcation.\n2) We designed an excellent pipeline to process the hyperspec-\ntral images, which combines two kinds of unsupervised\nlearning methods (representative learning and contrastive\nlearning). Furthermore, the pipeline only demands a single\nGPU, which means it is more affordable than the original\nprototypical contrastive learning method.\n3) Enlighted by the autoencoder structure, we designed two\nautoencoder-based modules: adversarial autoencoder mod-\nule(AAE module), variational autoencoder module(VAE\nmodule). The two modules can work alone for feature\nextraction. When using the extracted features for hyper-\nspectral classiﬁcation, they outperformed many baselines,\neven supervised methods.\nII. RELATED WORK\nA. Variational Autoencoder\nAutoencoder structure(AE)[8] is a widely used unsupervised\nfeature extraction model. The standard autoencoder is com-\nposed of an encoder and a decoder. The encoder encodes\nthe input images to features with ﬁxed shape. Moreover,\nthe features are called latent code. The decoder decodes\nthe latent code to original input images. The autoencoder’s\nobjective function is usually reconstruction loss(e.g., Euclidean\ndistances between output and input). Furthermore, the whole\nmodel is optimized by the backpropagation algorithm.\nHowever, original AE does not restrict the distribution of la-\ntent code. Thus variational autoencoder(VAE)[22] is proposed\nto optimize the distribution of latent code. VAE used KL-\ndivergence and the reparameterization trick to restrict latent\ncode distribution so that randomly sampled value from a\nnormal distribution can be decoded to an image with similar\ndistribution to the training samples. The latent code generated\nby VAE performs better in the classiﬁcation task, so VAE is\nwidely used for unsupervised feature extraction.\nB. Adversarial Autoencoder\nAdversarial autoencoder(AAE)[23] is another AE-based\nstructure which adopts adversarial restriction on the latent\ncode. Unlike VAE, AAE does not use KL-divergence to\nmeasure the distance between latent code distribution and\nnormal distribution. AAE chooses to use the idea of adver-\nsarial to restrict the distribution of latent code. By training a\ngenerative adversarial network(GAN)[9] with the latent code\nand randomly selected samples from a normal distribution, the\ndistribution of latent code will become an expected normal\ndistribution. As a result, the latent code generated by AAE\nwill be great for classiﬁcation, and the distributions of it will\nbe similar but different from that generated by VAE.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\nC. Prototypical Contrast Learning\nUsual unsupervised contrastive learning methods aim to ﬁnd\nan embedding space where every sample can be discrimi-\nnated among other instances in the whole dataset, such as\nmomentum contrast(MoCo)[19]. But this objective may be\ntoo hard to achieve without enough computing resources or\nsufﬁcient iteration. Although this kind of distribution may be\nbeneﬁcial in some downstream tasks, it is not very ﬁt for\nclassiﬁcation. Excellent distribution for classiﬁcation demands\nfor clear clusters composed of the data in the same classes.\nAnd typical unsupervised contrastive learning can’t satisfy this\ndemand.\nPrototypical contrastive learning(PCL)[21] is an unsuper-\nvised representation learning method based on contrastive\nlearning. Unlike most contrastive learning methods, PCL\nintroduces prototypes as latent variables to help ﬁnd the\nmaximum-likelihood estimation of the network parameters\nin an expectation-maximization framework. PCL uses Pro-\ntoNCE loss, a generalized version of the InfoNCE[24] loss\nfor contrastive learning by encouraging representations to be\ncloser to their assigned prototypes. The experiments show that\nPCL can get better performances than other baselines in the\nclassiﬁcation tasks, so we choose PCL as our method’s base\nstructure.\nIII. PROPOSED METHOD\nA. Motivation\nPCL showed great power of unsupervised learning in pro-\ncessing normal images. However, as mentioned above, it is\nhard to apply PCL to hyperspectral images directly. So we\nmust make some adjustments to PCL in order to apply it to\nhyperspectral classiﬁcation. With the blossom of AE-based\nmethods in the hyperspectral images ﬁeld, we ﬁnd that the\nencoder in an autoencoder structure can be seen as a trans-\nformation, just like other data augmentation methods. They\nboth map the original images from the original distribution\nto another distribution. The difference is, data augmentations\nusually do not change the shape of images, while encoder\nusually encodes the images to vectors with less dimensions.\nAnother essential truth is that data augmentation methods\nusually keep most information of the original images to make\nthem able to be rightly classiﬁed. Moreover, the encoder in\na trained autoencoder also can keep most information in the\noriginal images.\nAs a result, we ﬁnd it reasonable to treat the encoder as a\ntransformation function that plays the role of data augmenta-\ntion. Then the question is to ﬁnd two encoders with different\ndistributions, just like two kinds of data augmentations. So we\nchoose designed AAE and VAE modules to act as two kinds\nof data augmentation because AAE and VAE can generate\nrelatively ﬁxed and slightly different distributions.\nIn this way, we can not only apply PCL to hyperspectral\nimages but also lower the demand for deep networks. By\nencoding the images to shape ﬁxed vectors(e.g., 1024-d), we\ncan assign the feature extraction and contrastive learning into\ntwo parts, and each part will not be too complicated. Through\nthis method, we make contrastive learning methods affordable\nfor those who have limited computing resources.\nB. VAE Module\nTo process the hyperspectral images, we design a VAE\nnetwork to extract features from the original hyperspectral\nimages. The structure of the VAE module is shown in Fig.1.\nAs the ﬁgure shows, we ﬁrst use PCA preprocess the whole\nhyperspectral image to cut down the channels. Then sliding\nwindows are adopted to split the image into small patches\nwith the shape S × S × K. S is the size of sliding windows,\nand K is the channel of the preprocessed hyperspectral image.\nThe structure of the encoder network is enlighted by the idea\nof hybrid convolution in HybridSN[25]. We used several 3D\nconvolutional layers and 2D convolutional layers to extract the\ninput patches’ spectral and spatial information. We also used\nseveral 3D transposed convolutional layers and 2D transposed\nconvolutional layers to build the decoder network.\nFurthermore, to avoid the risk of overﬁtting, we adopted\nbatch normalization[26] after every convolutional layer. Every\nlinear layer is combined with a rectiﬁed linear unit(ReLU)\nactivation layer. We use a global pooling layer to ﬁx the shape\nof the feature map, thus ﬁx the shape of extracted features(e.g.,\n1024-d) after reshaping operation.\nAccording to the original VAE, we let the encoder output\ntwo variables µ and σ. Then the latent code can be computed\nby Eq. 1. z is the latent code, and ε is a random value\ndrawn from the normal distribution. The loss function for VAE\nnetwork can be divided into two parts: Lµ,σ2 and Lrecon. As\nEq. 2 shows, N is the number of samples, Lµ,σ2 computes the\nKL divergence between latent code and normal distribution.\nMoreover, Eq. 3 shows the detail of reconstruction loss Lrecon,\nN is the number of samples, the input image patch is Ix,y,z\nwith the shape of s×s×k, and ˆIx,y,z is the output image patch\nof the decoder. The whole loss function can be represented by\nEq. 4.\nz = µ + ε × σ\n(1)\nLµ,σ2 = 1\n2\nN\nX\ni=1\n\u0010\nµ2\n(i) + σ2\n(i) −log σ2\n(i) −1\n\u0011\n(2)\nLrecon =\nN\nX\ni=1\n \n1\ns × s × k\ns−1\nX\nx=0\ns−1\nX\ny=0\nk−1\nX\nz=0\n\u0010\nIx,y,z −ˆIx,y,z\u00112\n!\n(3)\nL = Lµ,σ2 + Lrecon\n(4)\nWhen we ﬁnished the VAE module training, we can map\nall the patches to VAE features. Thus we can use these VAE\nfeatures to build the training dataset for ContrastNet.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\n3D Convolution\n2D Convolution\nLinear Layer\n2D Transposed Convolution\n3D Transposed Convolution\nPCA\nLatent \nCode\nMultivariate \nNormal Distribution\nDecoder\nEncoder/Generator\nGlobal Pooling\nVector\nExtracted \nFeatures\nDeviation \nVector\nMean \nVector\nFig. 1: Structure of VAE module. Every (transposed) convolution comprises a (transposed) convolutional layer and a batch\nnormalization layer. Every linear layer means a linear layer and a ReLU layer.\nC. AAE Module\nSimilar to the VAE module, the whole hyperspectral image\nis preprocessed by PCA in the AAE module. The structure\nof the AAE network is shown in Fig.2. The encoder and the\ndecoder in the AAE module are the same with that in the VAE\nmodule except for the linear layers before the latent code. The\nencoder in the AAE module directly outputs the latent code\nand acts as a GAN generator.\nAccording to the original AAE, the latent code is the output\nof the generator and the discriminant input. Another input of\nthe discriminant is a randomly drawn variable from the normal\ndistribution. The training of the AAE can be divided into two\nphases: reconstruction phase and regularization phase. In the\nreconstruction phase, we use reconstruction loss to optimize\nthe parameters in the network. Reconstruction loss is shown\nin Eq. 5, it is the same as the reconstruction loss in the VAE\nmodule. Lrecon is reconstruction loss, N is the number of\nsamples, Ix,y,z represents the input image patch, s × s × k\nrepresents the shape of the input patch and ˆIx,y,z means\nreconstructed image patch.\nIn the regularization phase, the discriminator and the en-\ncoder are optimized. At ﬁrst, we optimize the discrimina-\ntor and then the generator(encoder). We adopt Wasserstein\nGAN(WGAN)[27] loss as our optimization function for the\ndiscriminator and the generator. Discriminant loss and gen-\nerator loss are shown in Eq. 6 and 7, where LD represents\nthe loss of the discriminant, and LG represents the loss of the\ngenerator. Pg is the distribution of the generated samples(latent\ncode), and Pr is the distribution of the real samples(samples\nfrom a normal distribution). x represents random samples from\nPg and Pr.\nLrecon =\nN\nX\ni=1\n \n1\ns × s × k\ns−1\nX\nx=0\ns−1\nX\ny=0\nk−1\nX\nz=0\n\u0010\nIx,y,z −ˆIx,y,z\u00112\n!\n(5)\nLD = Ex∼Pg [D(x)] −Ex∼Pr [D(x)]\n(6)\nLG = −Ex∼Pg [D(x)]\n(7)\nSimilarly, when we ﬁnished the training of the AAE module,\nwe can map all the patches to AAE features. Thus we can\nuse these AAE features to build the training dataset for\nContrastNet.\nD. ContrastNet\nBefore we introduce the ContrastNet, the speciﬁc descrip-\ntion of typical unsupervised contrast learning should be clar-\niﬁed. Let the traing set X = {x1, x2, ..., xn} have n images.\nAnd our goal is to ﬁnd an embedding function fθ(realized\nvia a deep neural network) that maps X to another space\nV = {v1, v2, ..., vn} where vi = fθ(xi), and vi can describe\nxi best. Usually the objective is achieved by the InfoNCE loss\nfunction[28] as Eq. 8 shows. v′\ni is a positive embedding for\ninstance i, and v′\nj includes one positive embedding and r neg-\native embeddings for other instances, and τ is a temperature\nhyper-parameter. In MoCo[19], these embeddings are obtained\nby feeding xi to a momentum encoder parametrized by θ′,\nv′\ni = fθ′(xi), where θ′ is a moving average of θ.\nWhen minimizing the Eq. 8, the distance between vi and v′\ni\nare becoming close, and the distance between vi and r negative\nembeddings are becoming far. However, original InfoNCE loss\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\n3D Convolution\n2D Convolution\nLinear Layer\n2D Transposed Convolution\n3D Transposed Convolution\nPCA\nLatent \nCode\nInput\n[-1,1]\nTanh\nMultivariate \nNormal Distribution\nDecoder\nEncoder/Generator\nDiscriminant\nGlobal Pooling\nVector\nExtracted \nFeatures\nFig. 2: Structure of AAE module. Every (transposed) convolution comprises a (transposed) convolutional layer and a batch\nnormalization layer. Every linear layer means a linear layer and a ReLU layer.\nuses a ﬁxed τ, which means the same concentration on a\ncertain scale. Moreover, it computes the distance between vi\nand all kinds of negative embeddings, some of which are not\nrepresentative.\nAs a result, prototypical contrastive learning proposed a\nnew loss function named ProtoNCE. As Eq. 9 and Eq. 10\nshow, based on InfoNCE, ProtoNCE contains a second part\nLP roto. It use prototypes c instead of v′, and replace the ﬁxed\ntemperature τ with a per-prototype concentration estimation\nφ.\nLInfoNCE =\nn\nX\ni=1\n−log\nexp (vi · v′\ni/τ)\nPr\nj=0 exp\n\u0000vi · v′\nj/τ\n\u0001\n(8)\nLP roto =\nn\nX\ni=1\n−\n \n1\nM\nM\nX\nm=1\nlog\nexp (vi · cm\ns /φm\ns )\nPr\nj=0 exp\n\u0000vi · cm\nj /φm\nj\n\u0001\n!\n(9)\nLP rotoNCE = LP roto + LInfoNCE\n(10)\nThe detailed algorithm of prototypical contrastive learning\ncan be found in Algorithm 1. There are two encoders in the\nPCL algorithm, one encoder maps xi to vi, and the momentum\nencoder maps xi to v′\ni. Practically, we do not use the same xi\nto get vi and v′\ni, two images with proper data augmentation\nwill be the input of two encoders for better performance. The\nﬁrst encoder parameters can be represented by θ, and the\nparameters in the momentum encoder can be represented by θ′\nwhich meets Eq. 11. In algorithm 1, k −means()[29] is the\nGPU implementation of a widely used clustering algorithm.\nAnd Cm is the sets of km prototypes(center of clusters).\nConcentration() is the function of concentration estimation\nφ, as Eq. 12 shows, φ is calculated by the momentum features\n{v′\nz}Z\nz=1 that are within the same clusters as a prototype c, α\nis a smooth parameter to ensure that small clusters do not have\noverly-large φ, and the typical value is 10. SGD()[30] is a\nkind of optimization function to update the parameters in the\nencoder.\nθ′ = 0.999 ∗θ′ + 0.001 ∗θ\n(11)\nφ =\nPZ\nz=1 ∥v′\nz −c∥2\nZ log(Z + α)\n(12)\nThe proposed ContrastNet is similar to the original PCL\nalgorithm, but there are several differences. Firstly, Con-\ntrastNet uses AAE and VAE features as inputs, not two\nperturbed images. Secondly, the structure of ContrastNet is\nmore straightforward and specialized in the ”contrastive” part,\nnot the ”encoding” part. Thirdly, we adopt the projection head\ntrick in sim-CLR[20] to improve the performance.\nFig. 3 is an overview of the ContrastNet. Expectation-\nMaximization (EM) algorithm is adopted to train the Contrast-\nNet. In the E-step, VAE features are fed into the momentum\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\n2D Convolution\nLinear Layer\n2D Transposed Convolution\nProjection Head\nVector\nAAE \nFeatures\nContrast \nFeatures\nBack \nPropagation\nVAE \nFeatures\nClustering\nProtoNCE\nM-step\nE-step\nEncoder\nMomentum Encoder\nFig. 3: Structure of ContrastNet. Every (transposed) convolution comprises a (transposed) convolutional layer and a batch\nnormalization layer. Every linear layer means a linear layer and a ReLU layer. Clustering means k-means algorithm, and\nProtoNCE is LP rotoNCE in Eq. 10. M-step and E-step are the phases of Expectation-Maximization algorithm.\nAlgorithm 1 Prototypical Contrastive Learning.\nInput: encoder fθ, training dataset X, number of clusters\nK = {km}M\nm=1\n1: θ′ = θ\n2: while not MaxEpoch do\n3:\nV ′ = f ′\nθ(X)\n4:\nfor m = 1 to M do\n5:\nCm = k −means(V ′, km)\n6:\nφm = Concentration(Cm, V ′)\n7:\nend for\n8:\nfor x in Dataloader(X) do\n9:\nv = fθ(x), v′ = f ′\nθ(x)\n10:\nLP rotoNCE(v, v′, {Cm}M\nm=1, {φm}M\nm=1)\n11:\nθ = SGD(LP rotoNCE, θ)\n12:\nθ′ = 0.999 ∗θ′ + 0.001 ∗θ\n13:\nend for\n14: end while\nencoder, then Cm and φm are calculated. In the M-step,\nLP rotoNCE is calculated based on the updated features and\nvariables in the E-step, then the two encoders are updated via\nbackpropagation. Similarly, all convolutions in the ContrastNet\nis composed of a convolutional layer and a batch normalization\nlayer, and every linear layer is combined with a ReLU layer.\nThe 1024-d AAE and VAE features will be reshaped to\n4×4×64 feature maps ﬁrst, in this way, the spatial information\nextracted can be preserved. And then, the feature maps are fed\ninto the ContrastNet. The structure of ContrastNet is light and\nstraightforward because the previous process has achieved the\nfeature extraction. The ContrastNet only needs to pay attention\nto the contrastive learning. We adopted the projection head\nstructure in the ContrastNet, that is, we use the output of the\nprojection head for training, and use the features before the\nhead for testing. In this way, the extracted contrast features\nwill contain more original information. In the testing phase,\nwe need to extract contrast features in Fig. 3 from the primary\nencoder, which can save much testing time for the ContrastNet.\nIV. EXPERIMENTS AND ANALYSIS\nA. Datasets\nExperiments are conducted using three publicly available\nHSI datasets: Indian Pines(IP), University of Pavia(PU), and\nSalinas Scene(SA). The IP dataset was gathered using the\nAirborne Visible / Infrared Imaging Spectrometer (AVIRIS)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\nTABLE I: Structure of AAE Encoder. The shape of data is\ndeﬁned in pytorch style. -1 means batchsize in the shape array.\nAAE Encoder\nLayer(type)\nOutput Shape\ninput\n[-1,1,15,27,27]\nConv3d\n[-1,8,9,25,25]\nBatchNorm3d\n[-1,8,9,25,25]\nReLU\n[-1,8,9,25,25]\nConv3d\n[-1,16,5,23,23]\nBatchNorm3d\n[-1,16,5,23,23]\nReLU\n[-1,16,5,23,23]\nConv3d\n[-1,32,3,21,21]\nBatchNorm3d\n[-1,32,3,21,21]\nReLU\n[-1,32,3,21,21]\nConv2d\n[-1,64,19,19]\nBatchNorm2d\n[-1,64,19,19]\nReLU\n[-1,64,19,19]\nAdaptiveAvgPool2d\n[-1,64,4,4]\nLinear\n[-1,512]\nReLU\n[-1,512]\nLinear\n[-1,128]\nhyperspectral sensor. It consisted of 145×145 pixels and\n224 spectral reﬂectance bands in the wavelength range of\n0.4 ∼2.5 × 10−6 meters. After removing bands covering\nthe region of water absorption, 200 bands were left, and the\nground truth was composed of 16 classes. The PU dataset was\na hyperspectral image with 610×340 pixels and 103 spectral\nbands in the wavelength range of 0.43 ∼0.86 × 10−6 meters.\nThe ground truth contained nine classes. The SA dataset was\nan image with 512×217 pixels and 224 spectral bands. This\ndataset was in the wavelength range of 0.36 ∼2.5 × 10−6\nmeters. Twenty water-absorbing bands were discarded from\nthe SA dataset, so it was a dataset with 204 spectral bands\nand 16 landcover classes in the ground truth.\nB. Experiments Details\nAll experiments were implemented on a PC with 16GB\nRAM and a GTX 1080 GPU. The coding environment was\npytorch[31]. The shape of patches is 27 × 27 × 15 in PU and\nSA dataset, and 27 × 27 × 30 in the IP dataset. When we\nuse 27 × 27 × 15 patches, our networks’ details are shown as\nfollows.\nTab. I and Tab. II show the structure of AAE. The structure\nof the discriminant is simple, so it is omitted. The latent code\ndimension is 128 and extracted AAE features are ﬂattened\noutputs of the pooling layer in the encoder(1024-d). In the\ntraining phase, we use two Adam[32] optimizers to optimize\nthe encoder and the decoder, and two SGD[30] optimizers\nto optimize the generator and the discriminant. Two Adam\noptimizers are set with the learning rate of 0.001, and weight\ndecay is set to 0.0005. The optimizer for the generator is set\nwith a 0.0001 learning rate and no weight decay. The optimizer\nfor the discriminant is set with a 0.00005 learning rate and no\nweight decay. The batch size is set to 128. We train the AAE\nmodule for 20 epochs, and only save the parameters of the\nencoder.\nTab. III and Tab. IV show the structure of VAE. The dimen-\nsion of latent code is also 128 and extracted VAE features are\nﬂattened outputs of the pooling layer in the encoder(1024-d)\nTABLE II: Structure of AAE Decoder. The shape of data is\ndeﬁned in pytorch style. -1 means batchsize in the shape array.\nAAE Decoder\nLayer(type)\nOutput Shape\ninput\n[-1,128]\nLinear\n[-1,256]\nReLU\n[-1,256]\nLinear\n[-1,23104]\nReLU\n[-1,23104]\nConvTransposed2d\n[-1,96,21,21]\nBatchNorm2d\n[-1,96,21,21]\nReLU\n[-1,96,21,21]\nConvTransposed3d\n[-1,16,5,23,23]\nBatchNorm3d\n[-1,16,5,23,23]\nReLU\n[-1,16,5,23,23]\nConvTransposed3d\n[-1,8,9,25,25]\nBatchNorm3d\n[-1,8,9,25,25]\nReLU\n[-1,8,9,25,25]\nConvTransposed3d\n[-1,1,15,27,27]\nBatchNorm3d\n[-1,1,15,27,27]\nTABLE III: Structure of VAE Encoder. The shape of data is\ndeﬁned in pytorch style. -1 means batchsize in the shape array.\nVAE Encoder\ninput\n[-1,1,15,27,27]\nConv3d\n[-1,8,9,25,25]\nBatchNorm3d\n[-1,8,9,25,25]\nReLU\n[-1,8,9,25,25]\nConv3d\n[-1,16,5,23,23]\nBatchNorm3d\n[-1,16,5,23,23]\nReLU\n[-1,16,5,23,23]\nConv3d\n[-1,32,3,21,21]\nBatchNorm3d\n[-1,32,3,21,21]\nReLU\n[-1,32,3,21,21]\nConv2d\n[-1,64,19,19]\nBatchNorm2d\n[-1,64,19,19]\nReLU\n[-1,64,19,19]\nAdaptiveAvgPool2d\n[-1,64,4,4]\nLinear\n[-1,512]\nReLU\n[-1,512]\nLinear/Linear\n[-1,128]/[-1,128]\ntoo. In the training phase, we use two Adam[32] optimizers to\noptimize the encoder and the decoder. Two Adam optimizers\nare set with the learning rate of 0.001, and weight decay is\nset to 0.0005. The two outputs of VAE encoder are used to\ncompute the latent code. Then the latent code is fed into the\ndecoder. The batch size is set to 128. We train the VAE module\nfor 30 epochs, and only save the parameters of the encoder.\nThe two encoders in ContrastNet have the same structure,\nwhich is shown in Tab. V. Two inputs are 1024-d vectors, and\nthe dimension of contrast features and projected features are\nboth 128-d. We train the ContrastNet with an SGD optimizer,\nand the learning rate is 0.003, weight decay is set to 0.001.\nNumber of negative samples r = 640, and batch size is 128, τ\nin Eq. 8 is 0.01, number of clusters K = [1000, 1500, 2500].\nAccording to the original PCL method, we only train the\nnetwork with Eq. 8 for 30 epochs, then train the network with\nEq. 10 for 170 epochs. The learning rate will be multiplied\nwith 0.1 when the trained epoch is more than 120 and 160.\nC. Results\nWe conducted classiﬁcation experiments in the three\ndatasets. Eleven other methods were adopted as baselines\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\nTABLE IV: Structure of VAE Decoder. The shape of data is\ndeﬁned in pytorch style. -1 means batchsize in the shape array.\nVAE Decoder\nLayer(type)\nOutput Shape\ninput\n[-1,128]\nLinear\n[-1,256]\nReLU\n[-1,256]\nLinear\n[-1,23104]\nReLU\n[-1,23104]\nConvTransposed2d\n[-1,96,21,21]\nBatchNorm2d\n[-1,96,21,21]\nReLU\n[-1,96,21,21]\nConvTransposed3d\n[-1,16,5,23,23]\nBatchNorm3d\n[-1,16,5,23,23]\nReLU\n[-1,16,5,23,23]\nConvTransposed3d\n[-1,8,9,25,25]\nBatchNorm3d\n[-1,8,9,25,25]\nReLU\n[-1,8,9,25,25]\nConvTransposed3d\n[-1,1,15,27,27]\nBatchNorm3d\n[-1,1,15,27,27]\nTABLE V: Structure of ContrastNet Encoder. The shape of\ndata is deﬁned in pytorch style. -1 means batchsize in the\nshape array.\nContrastNet Encoder\nLayer(type)\nOutput Shape\ninput\n[-1,1024]\nConvTransposed2d\n[-1,64,6,6]\nBatchNorm2d\n[-1,64,6,6]\nReLU\n[-1,64,6,6]\nConvTransposed2d\n[-1,64,8,8]\nBatchNorm2d\n[-1,64,8,8]\nReLU\n[-1,64,8,8]\nConv2d\n[-1,128,6,6]\nBatchNorm2d\n[-1,128,6,6]\nReLU\n[-1,128,6,6]\nConv2d\n[-1,64,4,4]\nBatchNorm2d\n[-1,64,4,4]\nReLU\n[-1,64,4,4]\nConv2d\n[-1,32,2,2]\nReLU\n[-1,32,2,2]\nLinear\n[-1,128]\nReLU\n[-1,128]\nLinear\n[-1,128]\nto compare with our methods. They are linear discriminant\nanalysis(LDA)[33],\nlocality-preserving\ndimensionality\nreduction(LFDA)[34],\nsparse\ngraph-based\ndiscriminant\nanalysis(SGDA)[35],\nsparse\nand\nlow-rank\ngraph\nfor\ndiscriminant analysis(SLGDA)[36], deep convolutional neural\nnetworks(1D-CNN)[1], supervised deep feature extraction(S-\nCNN)[37], tensor principal component analysis(TPCA)[38],\nstacked sparse autoencoder(SSAE)[10], unsupervised deep\nfeature\nextraction(EPLS)[39],\nand\n3D\nconvolutional\nautoencoder(3DCAE)[12]. To illustrate the great feature\nlearning ability of ContrastNet, we used SVM to classify\nthe features we learned by ContrastNet. Moreover, features\nlearned by AAE and VAE were also used to compare with\nthe ContrastNet. 10% samples of each class are randomly\nselected for training SVM in the IP and PU datasets, and 5%\nsamples of each class are used for training in the SA dataset,\nthe rest in each dataset is used for testing. The quantitative\nresults are evaluated by average accuracy (AA) and overall\naccuracy (OA). The results in the Indian Pines dataset are\nshown in Tab. VI. It is clear that the structures we proposed\nperformed best in many classes. AAE got the best results\nin ﬁve classes while ContrastNet got the best results in six\nclasses, and VAE also performed best in 3 classes.\nTo some extent, AAE and VAE are two better feature\nextractors than 3DCAE in terms of AA. However, ContrastNet,\nwhich learns information from AAE and VAE, showed a\nsigniﬁcant increase in OA. It can be evidence indicating\nthat ContrastNet can extract some core information from two\ndistributions by contrast learning. As a result, it got the highest\nOA. However, ContrastNet did not perform well enough in AA\nin the Indian Pines dataset. We guess the reason for it may be\nthe imbalance of the number of samples in the Indian Pines\ndataset. Classes with large numbers dominated the location of\nclusters. Thus the ContrastNet tended to perform well in these\nclasses, which decreased accuracy in other classes.\nThe results in the SA dataset are shown in Tab. VII. Similar\nto the IP dataset results, AAE performed best in ﬁve classes,\nVAE and ContrastNet both performed best in three classes.\nThough all methods got good performances in this dataset,\nContrastNet still got the highest OA and got the highest AA.\nUnlike the IP dataset, the number of samples in the SA dataset\nis relatively balanced, so ContrastNet got a higher AA than\nAAE and VAE. The same conclusion can be drawn from\nthe results in the PU dataset, which are shown in Tab.VIII.\nContrastNet still outperformed any other methods in terms of\nOA and AA.\nGenerally speaking, the proposed AAE, VAE, and Contrast-\nNet in this paper are good models that outperform many super-\nvised and unsupervised methods. Among the three methods,\nAutoencoder-based AAE and VAE are models that make use\nof reconstruction information and concentrate on the single\nsample itself. ContrastNet is a model that learns the similarity\nbetween two different distributions of a single sample, and also\nlearn the dissimilarity between positive samples and negative\nsamples. According to the experiment results, ContrastNet\nperforms better than AAE and VAE in OA and AA when the\nnumber of samples is balanced. When the number of samples\nis imbalanced, ContrastNet tends to get higher OA and lower\nAA than AAE and VAE.\nTo intuitively show the clustering effect of ContrastNet, we\nuse t-SNE[40] method to visualize the features extracted by\nAAE, VAE, and ContrastNet. The visualizations are shown in\nFig. 4, 5, 6. It is not hard to ﬁnd that the features extracted by\nthe ContrastNet are easier to classify. Because the inner-class\ndistances are small, and the inter-class distances are big in the\nContrastNet features.\nThe computational analysis is shown in Tab. IX. Because\nthe training of ContrastNet demands extracted AAE and VAE\nfeatures, which means the demand for AAE and VAE, the\ntraining time of ContrastNet is considerably longer than other\nalgorithms. However, When extracting features, the time con-\nsumption of ContrastNet is fairly fast than others. Because for\nAAE and VAE, only encoders are used in the feature extraction\nphase. Moreover, the structure of AAE and VAE is relatively\nsimple and efﬁcient, so the ContrastNet performs fast in the\nfeature extraction phase.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\nTABLE VI: Comparison in the Indian Pines dataset. 10% labeled samples are used for training, and the rest are used for testing.\nAA menas average accuracy and OA means overall accuracy. The best values in each row are in bold. And the methods in\nbold are structures proposed in this paper. Some of the data in the table is quoted from [12].\nClass\nSupervised Feature Extraction\nUnsupervised Feature Extraction\nLDA\nLFDA\nSGDA\nSLGDA\n1D-CNN\nS-CNN\nPCA\nTPCA\nSSAE\nEPLS\n3DCAE\nAAE\nVAE\nContrastNet\n1\n58.54\n29.63\n42.59\n39.62\n43.33\n83.33\n39.02\n60.97\n56.25\n58.72\n90.48\n100.00\n100.00\n85.37\n2\n69.88\n75.59\n80.89\n85.56\n73.13\n81.41\n72.30\n87.00\n69.58\n59.91\n92.49\n81.63\n78.78\n97.15\n3\n65.86\n75.42\n65.71\n74.82\n65.52\n74.02\n72.02\n94.51\n75.36\n71.34\n90.37\n95.27\n92.37\n97.95\n4\n73.71\n58.12\n64.10\n49.32\n51.31\n71.49\n55.87\n79.34\n64.58\n74.31\n86.90\n99.22\n97.34\n95.62\n5\n90.32\n95.17\n94.57\n95.35\n87.70\n90.11\n93.09\n93.08\n88.81\n97.95\n94.25\n95.17\n93.87\n96.09\n6\n92.09\n96.12\n98.39\n95.59\n95.10\n94.06\n94.67\n96.34\n87.00\n96.44\n97.07\n98.73\n98.27\n96.80\n7\n96.00\n11.53\n50.00\n36.92\n56.92\n84.61\n80.00\n76.00\n90.00\n54.02\n91.26\n96.00\n98.67\n70.67\n8\n98.14\n93.87\n99.80\n99.75\n96.64\n98.37\n98.37\n99.76\n89.72\n88.99\n97.79\n99.84\n99.77\n98.68\n9\n11.11\n0.00\n0.00\n5.00\n28.89\n33.33\n88.89\n100.00\n100.00\n58.89\n75.91\n96.30\n98.15\n70.37\n10\n73.80\n80.89\n59.30\n69.11\n75.12\n86.05\n74.49\n79.51\n77.19\n73.10\n87.34\n87.01\n78.86\n97.45\n11\n55.41\n83.02\n84.44\n89.91\n83.49\n82.98\n69.58\n85.42\n77.58\n70.78\n90.24\n89.08\n81.75\n98.40\n12\n76.92\n86.32\n77.04\n86.78\n67.55\n73.40\n65.29\n84.24\n72.00\n57.51\n95.76\n93.51\n90.64\n93.57\n13\n91.30\n79.25\n99.09\n99.51\n96.86\n87.02\n98.37\n98.91\n87.80\n99.25\n97.49\n98.56\n98.56\n95.32\n14\n93.32\n88.87\n92.50\n96.45\n96.51\n94.38\n91.39\n98.06\n93.48\n95.07\n96.03\n95.73\n93.24\n98.51\n15\n67.72\n60.00\n68.68\n61.79\n39.08\n75.57\n48.99\n87.31\n72.36\n91.26\n90.48\n97.31\n97.02\n96.73\n16\n90.36\n53.68\n85.26\n84.16\n89.40\n79.76\n87.95\n96.38\n97.22\n91.27\n98.82\n98.02\n98.81\n79.76\nAA(%)\n76.89\n66.72\n72.65\n73.10\n71.66\n84.44\n76.89\n89.31\n81.18\n77.43\n92.04\n95.09\n93.51\n91.78\nOA(%)\n76.88\n81.79\n80.05\n85.19\n79.66\n80.72\n76.88\n88.55\n79.78\n77.18\n92.35\n91.80\n88.03\n97.08\nTABLE VII: Comparison in the Salinas dataset. 5% labeled samples are used for training, and the rest are used for testing.\nAA menas average accuracy and OA means overall accuracy. The best values in each row are in bold. And the methods in\nbold are structures proposed in this paper. Some of the data in the table is quoted from [12].\nClass\nSupervised Feature Extraction\nUnsupervised Feature Extraction\nLDA\nLFDA\nSGDA\nSLGDA\n1D-CNN\nS-CNN\nPCA\nTPCA\nSSAE\nEPLS\n3DCAE\nAAE\nVAE\nContrastNet\n1\n99.16\n99.20\n99.65\n98.14\n97.98\n99.55\n97.48\n99.88\n100.00\n99.99\n100.00\n99.98\n99.74\n99.93\n2\n99.94\n99.19\n99.33\n99.44\n99.25\n99.43\n99.52\n99.49\n99.52\n99.92\n99.29\n100.00\n99.79\n99.80\n3\n99.79\n99.75\n99.30\n99.29\n94.43\n98.81\n99.41\n99.04\n94.24\n98.75\n97.13\n100.00\n100.00\n99.95\n4\n99.77\n99.71\n99.21\n99.57\n99.42\n97.45\n99.77\n99.84\n99.17\n98.52\n97.91\n99.09\n99.57\n98.01\n5\n98.98\n98.47\n99.07\n98.06\n96.60\n97.96\n98.70\n98.96\n98.82\n98.33\n98.26\n99.42\n99.71\n99.48\n6\n99.89\n99.09\n99.57\n99.32\n99.51\n99.83\n99.65\n99.80\n100.00\n99.92\n99.98\n99.97\n99.86\n99.94\n7\n99.97\n99.66\n99.27\n99.33\n99.27\n99.59\n99.94\n99.84\n99.94\n97.69\n99.64\n99.93\n99.96\n99.79\n8\n81.84\n86.89\n89.78\n89.48\n86.79\n94.40\n83.90\n84.11\n80.73\n78.86\n91.58\n91.21\n86.91\n99.53\n9\n99.90\n97.34\n100.00\n99.65\n99.08\n98.85\n99.97\n99.60\n99.47\n99.54\n99.28\n99.69\n99.99\n99.71\n10\n96.31\n95.85\n97.99\n97.94\n93.71\n97.35\n96.89\n95.76\n92.12\n95.98\n96.65\n98.46\n96.54\n99.80\n11\n99.61\n97.94\n99.53\n99.06\n94.55\n97.71\n96.84\n96.14\n96.62\n98.60\n97.74\n99.57\n100.00\n99.80\n12\n99.67\n99.64\n100.00\n100.00\n99.59\n98.73\n99.95\n99.07\n97.75\n99.44\n98.84\n100.00\n99.44\n99.98\n13\n99.20\n97.38\n98.47\n97.82\n97.50\n96.72\n99.54\n100.00\n95.81\n98.85\n99.26\n99.89\n97.24\n98.20\n14\n96.56\n93.18\n96.07\n90.47\n94.08\n95.22\n97.24\n95.74\n96.65\n98.56\n97.49\n99.08\n96.49\n98.62\n15\n73.60\n63.04\n65.40\n69.51\n66.52\n95.61\n76.68\n79.54\n79.73\n83.13\n87.85\n93.69\n87.90\n99.53\n16\n98.48\n99.00\n99.34\n99.00\n97.48\n99.44\n97.90\n98.40\n99.12\n99.50\n98.34\n99.73\n99.61\n99.57\nAA(%)\n96.42\n95.33\n96.37\n96.01\n94.73\n97.39\n96.46\n93.24\n95.61\n96.55\n97.45\n98.73\n97.67\n99.48\nOA(%)\n92.18\n91.22\n91.82\n93.31\n91.30\n97.92\n92.87\n96.57\n92.11\n92.35\n95.81\n97.10\n95.23\n99.60\nTABLE VIII: Comparison in the University of Pavia dataset. 10% labeled samples are used for training, and the rest are used\nfor testing. AA menas average accuracy and OA means overall accuracy. The best values in each row are in bold. And the\nmethods in bold are structures proposed in this paper. Some of the data in the table is quoted from [12].\nClass\nSupervised Feature Extraction\nUnsupervised Feature Extraction\nLDA\nLFDA\nSGDA\nSLGDA\n1D-CNN\nS-CNN\nPCA\nTPCA\nSSAE\nEPLS\n3DCAE\nAAE\nVAE\nContrastNet\n1\n78.41\n93.45\n91.37\n94.66\n90.93\n95.40\n90.89\n96.17\n95.72\n95.95\n95.21\n95.39\n81.31\n99.49\n2\n83.69\n97.36\n97.23\n97.83\n96.94\n97.31\n93.27\n97.95\n94.13\n95.91\n96.06\n98.96\n97.65\n99.98\n3\n73.02\n71.41\n66.08\n77.27\n69.43\n81.21\n82.60\n86.50\n87.47\n94.33\n91.32\n97.49\n91.00\n99.06\n4\n93.68\n91.00\n91.19\n93.18\n90.32\n95.83\n92.41\n94.84\n96.91\n99.28\n98.28\n96.94\n94.79\n97.75\n5\n100.00\n97.99\n99.33\n98.51\n99.44\n99.91\n98.98\n100.00\n99.76\n99.92\n95.55\n99.94\n99.94\n99.81\n6\n88.51\n87.55\n80.31\n90.08\n73.69\n95.29\n92.00\n94.76\n95.76\n93.57\n95.30\n99.29\n99.91\n99.90\n7\n85.75\n80.23\n75.26\n85.34\n83.42\n87.05\n85.83\n91.89\n91.18\n98.17\n95.14\n100.00\n99.42\n99.83\n8\n74.49\n86.98\n84.11\n90.49\n83.65\n87.35\n82.96\n89.04\n82.47\n91.23\n91.38\n97.90\n94.77\n98.79\n9\n99.11\n99.26\n95.14\n99.37\n98.23\n95.66\n100.00\n98.94\n100.00\n99.78\n99.96\n96.32\n88.62\n94.84\nAA(%)\n86.29\n89.47\n86.67\n91.86\n87.34\n94.75\n90.99\n95.64\n93.71\n96.33\n95.36\n98.03\n94.16\n98.83\nOA(%)\n83.75\n92.77\n90.58\n94.15\n89.99\n92.78\n91.37\n94.45\n93.51\n95.13\n95.39\n98.14\n94.53\n99.46\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\n(a) AAE\n(b) VAE\n(c) ContrastNet\nFig. 4: Visualization of extracted features in the Indian Pines dataset. Each class is corresponding to a kind of color.\n(a) AAE\n(b) VAE\n(c) ContrastNet\nFig. 5: Visualization of extracted features in the University of Pavia dataset. Each class is corresponding to a kind of color.\n(a) AAE\n(b) VAE\n(c) ContrastNet\nFig. 6: Visualization of extracted features in the Salinas dataset. Each class is corresponding to a kind of color.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n11\nTABLE IX: Computational analysis of different feature extration algorithms. The best value in each row is in bold. And the\nmethods in bold are structures proposed in this paper. Some of the data in the table is quoted from [12].\nIndian Pines dataset\n1D-CNN\nS-CNN\nTPCA\nSSAE\nEPLS\n3DCAE\nContrastNet\nTraining(s)\n20.6\n119\n44\n240.1\n111.9\n1156\n2073\nFeature Extraction (s)\n17.4\n3.2\n150\n2.76\n47.3\n5.22\n12.77\nSalinas dataset\n1D-CNN\nS-CNN\nTPCA\nSSAE\nEPLS\n3DCAE\nContrastNet\nTraining(s)\n43.1\n134\n241\n513\n103.5\n1159\n4784\nFeature Extraction (s)\n83.8\n62\n816\n16.3\n192.2\n26.4\n36.65\nUniversity of Pavia dataset\n1D-CNN\nS-CNN\nTPCA\nSSAE\nEPLS\n3DCAE\nContrastNet\nTraining(s)\n32.1\n332\n44\n491\n127.13\n1168\n3778\nFeature Extraction (s)\n150.07\n106\n150\n31.9\n141.5\n32.04\n27.82\nV. CONCLUSION\nIn this paper, we proposed an unsupervised feature learning\nmethod based on autoencoder and contrastive learning. This\nmethod combines unsupervised representative methods and\nunsupervised discriminative methods, learning to extract better\nhyperspectral classiﬁcation features than other baseline meth-\nods. In the proposed method, we designed two efﬁcient autoen-\ncoder structure: VAE and AAE, and design a ContrastNet for\ncontrastive learning, which reduces the computing resources\ndemand of contrastive learning. Our experiments show that the\nproposed method can extract more representative features and\nkeep a high feature extraction speed in the testing phase. Our\nwork shows that unsupervised learning still has great potential\nin the remote sensing ﬁeld, and we hope others can get more\nexciting ideas through our exploration.\nAPPENDIX A\nACKNOWLEDGMENT\nThe authors would like to thank...\nREFERENCES\n[1] W. Hu, Y. Huang, L. Wei, F. Zhang, and H. Li, “Deep convolutional\nneural networks for hyperspectral image classiﬁcation,” Journal of\nSensors, vol. 2015, 2015.\n[2] Y. Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi, “Deep feature extraction\nand classiﬁcation of hyperspectral images based on convolutional neural\nnetworks,” IEEE Transactions on Geoscience and Remote Sensing,\nvol. 54, no. 10, pp. 6232–6251, 2016.\n[3] L. Mou, P. Ghamisi, and X. X. Zhu, “Deep recurrent neural networks for\nhyperspectral image classiﬁcation,” IEEE Transactions on Geoscience\nand Remote Sensing, vol. 55, no. 7, pp. 3639–3655, 2017.\n[4] W. Song, S. Li, L. Fang, and T. Lu, “Hyperspectral image classiﬁcation\nwith deep feature fusion network,” IEEE Transactions on Geoscience\nand Remote Sensing, vol. 56, no. 6, pp. 3173–3184, 2018.\n[5] Z. Zhong, J. Li, Z. Luo, and M. Chapman, “Spectral–spatial residual\nnetwork for hyperspectral image classiﬁcation: A 3-d deep learning\nframework,” IEEE Transactions on Geoscience and Remote Sensing,\nvol. 56, no. 2, pp. 847–858, 2017.\n[6] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”\nChemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.\n37–52, 1987.\n[7] A. Villa, J. A. Benediktsson, J. Chanussot, and C. Jutten, “Hyperspectral\nimage classiﬁcation with independent component discriminant analysis,”\nIEEE transactions on Geoscience and remote sensing, vol. 49, no. 12,\npp. 4865–4876, 2011.\n[8] D. H. Ballard, “Modular learning in neural networks.” in AAAI, 1987,\npp. 279–284.\n[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in\nAdvances in neural information processing systems, 2014, pp. 2672–\n2680.\n[10] C. Tao, H. Pan, Y. Li, and Z. Zou, “Unsupervised spectralspatial feature\nlearning with stacked sparse autoencoder for hyperspectral imagery\nclassiﬁcation,” IEEE Geoscience and Remote Sensing Letters, vol. 12,\nno. 12, pp. 2438–2442, 2015.\n[11] L. Zhu, Y. Chen, P. Ghamisi, and J. A. Benediktsson, “Generative adver-\nsarial networks for hyperspectral image classiﬁcation,” IEEE Transac-\ntions on Geoscience and Remote Sensing, vol. 56, no. 9, pp. 5046–5063,\n2018.\n[12] S. Mei, J. Ji, Y. Geng, Z. Zhang, X. Li, and Q. Du, “Unsupervised\nspatialspectral feature learning by 3d convolutional autoencoder for\nhyperspectral classiﬁcation,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 57, no. 9, pp. 6808–6820, 2019.\n[13] Y. Zhan, D. Hu, Y. Wang, and X. Yu, “Semisupervised hyperspectral\nimage classiﬁcation based on generative adversarial networks,” IEEE\nGeoscience and Remote Sensing Letters, vol. 15, no. 2, pp. 212–216,\n2018.\n[14] X. Wang, K. Tan, Q. Du, Y. Chen, and P. Du, “Caps-triplegan: Gan-\nassisted capsnet for hyperspectral image classiﬁcation,” IEEE Transac-\ntions on Geoscience and Remote Sensing, vol. 57, no. 9, pp. 7232–7245,\n2019.\n[15] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by\nlearning an invariant mapping,” in 2006 IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition (CVPR’06), vol. 2.\nIEEE, 2006, pp. 1735–1742.\n[16] X. Liu, Y. Zhou, J. Zhao, R. Yao, B. Liu, and Y. Zheng, “Siamese\nconvolutional neural networks for remote sensing scene classiﬁcation,”\nIEEE Geoscience and Remote Sensing Letters, vol. 16, no. 8, pp. 1200–\n1204, 2019.\n[17] M. Rao, L. Tang, P. Tang, and Z. Zhang, “Es-cnn: An end-to-end siamese\nconvolutional neural network for hyperspectral image classiﬁcation,” in\n2019 Joint Urban Remote Sensing Event (JURSE), 2019, pp. 1–4.\n[18] L. Huang and Y. Chen, “Dual-path siamese cnn for hyperspectral image\nclassiﬁcation with limited training samples,” IEEE Geoscience and\nRemote Sensing Letters, pp. 1–5, 2020.\n[19] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast\nfor unsupervised visual representation learning,” in The IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), June\n2020.\n[20] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\nfor contrastive learning of visual representations,” 2020.\n[21] J. Li, P. Zhou, C. Xiong, R. Socher, and S. C. H. Hoi, “Prototypical\ncontrastive learning of unsupervised representations,” 2020.\n[22] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114, 2013.\n[23] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, “Adver-\nsarial autoencoders,” arXiv preprint arXiv:1511.05644, 2015.\n[24] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning\nvia non-parametric instance discrimination,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n3733–3742.\n[25] S. K. Roy, G. Krishna, S. R. Dubey, and B. B. Chaudhuri, “Hybridsn:\nExploring 3-d-2-d cnn feature hierarchy for hyperspectral image classi-\nﬁcation,” IEEE Geoscience and Remote Sensing Letters, 2019.\n[26] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” arXiv preprint\narXiv:1502.03167, 2015.\n[27] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv\npreprint arXiv:1701.07875, 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n12\n[28] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.\n[29] J. Johnson, M. Douze, and H. Jegou, “Billion-scale similarity search\nwith gpus,” IEEE Transactions on Big Data, p. 11, 2019. [Online].\nAvailable: http://dx.doi.org/10.1109/tbdata.2019.2921572\n[30] L. Bottou, “Large-scale machine learning with stochastic gradient de-\nscent,” in Proceedings of COMPSTAT’2010.\nSpringer, 2010, pp. 177–\n186.\n[31] A.\nPaszke,\nS.\nGross,\nF.\nMassa,\nA.\nLerer,\nJ.\nBradbury,\nG.\nChanan,\nT.\nKilleen,\nZ.\nLin,\nN.\nGimelshein,\nL.\nAntiga,\nA. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala,\n“Pytorch:\nAn\nimperative\nstyle,\nhigh-performance\ndeep\nlearning\nlibrary,” in Advances in Neural Information Processing Systems\n32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,\nE.\nFox,\nand\nR.\nGarnett,\nEds.\nCurran\nAssociates,\nInc.,\n2019,\npp.\n8024–8035.\n[Online].\nAvailable:\nhttp://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf\n[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[33] Q. Du, “Modiﬁed ﬁsher’s linear discriminant analysis for hyperspectral\nimagery,” IEEE geoscience and remote sensing letters, vol. 4, no. 4, pp.\n503–507, 2007.\n[34] W. Li, S. Prasad, J. E. Fowler, and L. M. Bruce, “Locality-preserving\ndimensionality reduction and classiﬁcation for hyperspectral image anal-\nysis,” IEEE Transactions on Geoscience and Remote Sensing, vol. 50,\nno. 4, pp. 1185–1198, 2012.\n[35] N. H. Ly, Q. Du, and J. E. Fowler, “Sparse graph-based discriminant\nanalysis for hyperspectral imagery,” IEEE Transactions on Geoscience\nand Remote Sensing, vol. 52, no. 7, pp. 3872–3884, 2014.\n[36] W. Li, J. Liu, and Q. Du, “Sparse and low-rank graph for discriminant\nanalysis of hyperspectral imagery,” IEEE Transactions on Geoscience\nand Remote Sensing, vol. 54, no. 7, pp. 4094–4105, 2016.\n[37] B. Liu, X. Yu, P. Zhang, A. Yu, Q. Fu, and X. Wei, “Supervised\ndeep feature extraction for hyperspectral image classiﬁcation,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 56, no. 4, pp.\n1909–1921, 2018.\n[38] Y. Ren, L. Liao, S. J. Maybank, Y. Zhang, and X. Liu, “Hyperspectral\nimage spectral-spatial feature extraction via tensor principal component\nanalysis,” IEEE Geoscience and Remote Sensing Letters, vol. 14, no. 9,\npp. 1431–1435, Sep. 2017.\n[39] A. Romero, C. Gatta, and G. Camps-Valls, “Unsupervised deep feature\nextraction for remote sensing image classiﬁcation,” IEEE Transactions\non Geoscience and Remote Sensing, vol. 54, no. 3, pp. 1349–1362, 2016.\n[40] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal\nof machine learning research, vol. 9, no. Nov, pp. 2579–2605, 2008.\nZeyu Cao received the B.S. degree in automation from Zhejiang University,\nHangzhou, China, where he is currently pursuing the Ph.D. degree in control\ntheory and control engineering. His research interests include object detection\nand machine learning.\nXiaorun Li received the B.S. degree from the National University of Defense\nTechnology, Changsha, China, in 1992, and the M.S. and Ph.D. degrees from\nZhejiang University, Hangzhou, China, in 1995 and 2008, respectively. Since\n1995, he has been with Zhejiang University, where he is currently a Professor\nwith the College of Electrical Engineering. His research interests include\nhyperspectral image processing, signal and image processing, and pattern\nrecognition.\nLiaoying Zhao received the B.S. and M.S. degrees from Hangzhou Dianzi\nUniversity, Hangzhou, China, in 1992 and 1995, respectively, and the Ph.D.\ndegree from Zhejiang University, Hangzhou, in 2004. Since 1995, she has been\nwith Hangzhou Dianzi University, where she is currently a Professor with the\nCollege of Computer Science. Her research interests include hyperspectral\nimage processing, signal and image processing, pattern recognition, and\nmachine learning.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-09-02",
  "updated": "2020-09-02"
}