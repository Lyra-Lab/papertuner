{
  "id": "http://arxiv.org/abs/2009.01989v1",
  "title": "A Comprehensive Analysis of Information Leakage in Deep Transfer Learning",
  "authors": [
    "Cen Chen",
    "Bingzhe Wu",
    "Minghui Qiu",
    "Li Wang",
    "Jun Zhou"
  ],
  "abstract": "Transfer learning is widely used for transferring knowledge from a source\ndomain to the target domain where the labeled data is scarce. Recently, deep\ntransfer learning has achieved remarkable progress in various applications.\nHowever, the source and target datasets usually belong to two different\norganizations in many real-world scenarios, potential privacy issues in deep\ntransfer learning are posed. In this study, to thoroughly analyze the potential\nprivacy leakage in deep transfer learning, we first divide previous methods\ninto three categories. Based on that, we demonstrate specific threats that lead\nto unintentional privacy leakage in each category. Additionally, we also\nprovide some solutions to prevent these threats. To the best of our knowledge,\nour study is the first to provide a thorough analysis of the information\nleakage issues in deep transfer learning methods and provide potential\nsolutions to the issue. Extensive experiments on two public datasets and an\nindustry dataset are conducted to show the privacy leakage under different deep\ntransfer learning settings and defense solution effectiveness.",
  "text": "A Comprehensive Analysis of Information Leakage in Deep\nTransfer Learning\nCen Chen, Bingzhe Wu, Minghui Qiu, Li Wang, Jun Zhou\n{chencen.cc,fengyuan.wbz,raymond.wangl,jun.zhoujun}@antgroup.com\nminghui.qmh@alibaba-inc.com\nAlibaba Group\nHangzhou, China\nABSTRACT\nTransfer learning is widely used for transferring knowledge from a\nsource domain to the target domain where the labeled data is scarce.\nRecently, deep transfer learning has achieved remarkable progress\nin various applications. However, the source and target datasets\nusually belong to two different organizations in many real-world\nscenarios, potential privacy issues in deep transfer learning are\nposed. In this study, to thoroughly analyze the potential privacy\nleakage in deep transfer learning, we first divide previous methods\ninto three categories. Based on that, we demonstrate specific threats\nthat lead to unintentional privacy leakage in each category. Addi-\ntionally, we also provide some solutions to prevent these threats.\nTo the best of our knowledge, our study is the first to provide a\nthorough analysis of the information leakage issues in deep transfer\nlearning methods and provide potential solutions to the issue. Ex-\ntensive experiments on two public datasets and an industry dataset\nare conducted to show the privacy leakage under different deep\ntransfer learning settings and defense solution effectiveness.\nCCS CONCEPTS\n• Security and privacy →Software and application secu-\nrity; • Computing methodologies →Transfer learning.\nACM Reference Format:\nCen Chen, Bingzhe Wu, Minghui Qiu, Li Wang, Jun Zhou. 2018. A Com-\nprehensive Analysis of Information Leakage in Deep Transfer Learning.\nIn KDD’20. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/\n1122445.1122456\n1\nINTRODUCTION\nTransfer learning is a rapidly growing field of machine learning that\naims to improve the learning of a data-deficient task by knowledge\ntransfer from related data-sufficient tasks [27, 35, 40]. Witness the\nsuccess of deep learning, deep transfer learning has been widely\nstudied and demonstrated remarkable performance over various\napplications, such as medical image classification [28], electronic\nhealth data analysis[5], and credit modeling [33].\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nKDD ’20, August, 2020, San Diego, CA\n© 2018 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06...$15.00\nhttps://doi.org/10.1145/1122445.1122456\nA fundamental block of deep transfer learning is deep neural\nnetwork, which is vulnerable to different attacks aiming to detect\nsensitive information contained in the training dataset [7, 32, 37].\nMoreover, in most of the real-world applications where deep trans-\nfer learning is used, the source and target datasets always reside\nin two different organizations. As a result, deep transfer learning\nalso faces potential privacy threats, i.e, the client in the target or-\nganization can leverage the vulnerability of deep learning models\nto detect sensitive information contained in the source organiza-\ntion. Specifically, applying deep transfer learning comes with the\ninteraction between the source and target domains. Thus, the data\ntransmission between these domains may unintentionally disclose\nprivate information.\nExisting studies on analyzing privacy leakages focus on either\ngeneral machine learning models [25, 32] or in a federated learning\nsetting where model is collaboratively trained by multiple clients by\nsharing and aggregating the gradients via a server[22, 25]. However,\nthere no such study on transfer learning paradigms. To this end, we\nare the first to provide a general categorization for deep transfer\nlearning models based on the potential information leakages. This\nis not trivial since there are numerous methods for deep transfer\nlearning [27, 35, 40]. Given the goal of privacy leakage analysis,\nwe care more about the interaction manner between source and\ntarget domains. Thus, we divide previous works into three cate-\ngories, as illustrated in Figure 1: (1) model-based paradigm where\nthe whole model structure and parameters are shared (2) mapping-\nbased where the hidden features are shared (3) parameter-based\nwhere the parameter gradients are shared. Based on that, the previ-\nous works can fall into the above categories or a hybrid of them.\nFor example, fine-tuning based approaches obviously belong to the\nfirst category. The prior work [34] is based on the mapping-based\nparadigm, since it uses the correlation alignment loss which further\ndepends on the shared hidden features. Similarly, previous works\nthat minimize the domain representation difference by variants of\ndistribution divergence metrics such as maximum mean discrep-\nancy also fall into the second category [17, 18, 29]. Fully-shared\nand shared-private transfer learning models [15] can be regarded\nas parameter-based, as they both jointly train a shared network via\ngradient updates in a multi-task fashion, just to name a few.\nBased on the general categorization, we can build customized at-\ntacks against each paradigm and demonstrate information leakages\nin deep transfer learning. At a high level, we consider inferring\ntwo types of sensitive information, i.e., membership and property\ninformation. This sensitive information can be revealed by the\ntransmission data between the two domains as the above discussed.\nSpecifically, in the model-based paradigm, we build the membership\narXiv:2009.01989v1  [cs.CL]  4 Sep 2020\nKDD ’20, August, 2020, San Diego, CA\nTrovato and Tobin, et al.\nx\ny\n(a) Model-based\nx\ny\nModel\nFine-tune\n(b) Mapping-based\nx\ny\nFeature \nAlignment\nx\ny\nCo-training\n(c) Parameter-based\nx\ny\nx\ny\nParameter \nSharing\nCo-training\nhMsi\nhhi\nhwi\nhwi\nhhi\nDT\nDS\nDT\nDS\nDT\nDS\nFigure 1: An illustration of model-based, mapping-based and parameter-based transfer learning paradigms. DS and DT denote\nthe source domain and target domain data, respectively. Each dataset contains a set of labeled training samples that consist of\nthe inputs x and their corresponding labels y. We use w to denote model parameters, and h for feature representations.\nattack which takes the model (learned using the source dataset) as\ninput and determines whether a specific sample is used for training\nthe model. In the mapping-based setting, we can build the property\nattack to infer properties contained in the training dataset. For\nexample, the attacker resides on the target domain aims to infer\nproperties of the source domain based on the shared hidden fea-\ntures. In the parameter-based setting, we can similarly perform the\nproperty inference attack, i.e., the attacker can infer properties of\nthe source domain data based on the shared gradients. More details\nof these attacks can be found in Section 3.\nEmpirically, to demonstrate the effectiveness of attacks, we con-\nduct a set of experiments under the three types of transfer learning\nsettings. Our key observation is that all these types of models do\nunintentionally leak information of the training data under mem-\nbership/property attacks. Model-based paradigm is possible to leak\nmembership information. Parameter-based paradigm without re-\nvealing individual gradient (i.e., averaged gradients at batch-level)\nleaks much less property information, compared to the mapping-\nbased paradigm where hidden features (i.e., representations at\nsample-level) are shared.\nIn summary, our main contributions are as follows:\n• We are the first to propose a general categorization for differ-\nent deep transfer learning paradigms based on their intrinsic\ninteraction manner between the source and target domains\nand provide a comprehensive analysis of the potential pri-\nvacy leakage profiles for each category respectively.\n• Based on the categorization, we build specific attacks against\neach paradigm to demonstrate their privacy leakages.\n• In contrast to previous works that aim to design privacy-\npreserving transfer learning model for a specific setting (e.g.,\nmapping-based [16, 31]), our analysis has covered a wide\nrange of deep transfer learning methods.\n• We conduct extensive experiments on both public datasets\nand an industry marketing dataset to verify the effectiveness\nof our attacks and defense solutions.\n2\nPRELIMINARIES\n2.1\nDeep Transfer Learning Setting\nIn this work, we focus on deep transfer learning [35, 40], where the\nmodels discussed are neural network based. Without losing gener-\nality, we consider a transfer learning setting with two domain tasks\nT ∈{S,T }, where S and T refer to the source domain and target\ndomain respectively, both containing private sensitive information.\nWe aim to improve the target domain learning task performance by\nutilizing its own data DT and source domain data DS. Each dataset\ncontains a set of labeled examples zi = (xi,yi), where x denotes the\ninputs and y denotes the corresponding labels. The size of source\ndomain data DS is usually much larger than the target domain\ndata DT , i.e., N S >> NT . The goal of a domain task is to learning\na transformation function fw(x, y) : Rd →R, parameterized by\nmodel weights w.\n2.2\nInference Attacks for DNN Models\nThe basic idea of the inference attack is to exploit the leakages when\na model is being trained or released to reveal some unintended infor-\nmation from the training data. In this section, we briefly present two\ntypes of inference attacks, i.e., membership and property attacks,\nfor machine learning models.\nMembership Inference Attack. Membership inference is a typi-\ncal attack that aims to determine whether a sample is used as part\nof the training dataset. Membership inference may reveal sensitive\ninformation that leads to privacy breach. For example, if we can\ninfer a patient’s existence in the training dataset for a medical study\nof a certain disease, we can probably claim that this patient has\nsuch a disease. Recent works have demonstrated the membership\nattack attempts for machine learning models under the black-box\nsetting [19, 30, 32], white-box setting [22, 26] or both [9]. Shadow\ntraining is a widely adopted technique for membership inference,\nwhere multiple shadow models are trained to mimic the behavior\nof the target model[19, 30, 32]. This technique assumes the attacker\nto have some prior knowledge about the population from the tar-\ngeted model training dataset was drawn. Recent works by [22, 26]\nA Comprehensive Analysis of Information Leakage in Deep Transfer Learning\nKDD ’20, August, 2020, San Diego, CA\nexplicitly exploit the vulnerabilities in gradient updates to perform\nattacks with white-box access.\nProperty Inference Attack. Another common type of attack is\nproperty inference that aims to reveal certain unintended or sen-\nsitive properties (e.g., the fraction of the data belongs to a certain\nminority group) of the participating training datasets that the model\nproducer does not intend to share when the model is released. A\nproperty is usually uncorrelated or loosely correlated with the main\ntraining task. Pioneer works of [2, 6, 7] conducted the property at-\ntacks that characterize the entire training dataset. While, [22] aimed\nto infer properties for a subset of the training inputs, i.e., in terms\nof single batches which they termed as single-batch properties. In\nthis regard, membership attack can be viewed as a special case of\nproperty attack when scope for property attack is on a sample basis.\nThe above-mentioned two types of attacks are closely related.\nMost of existing works perform those attacks against general ma-\nchine learning models, while a few focus on the federated learning\nand collaborative learning scenarios [22, 26]. None of the stud-\nies systematically explore the inference attacks explicitly for the\ncontext of deep transfer learning.\n3\nPRIVACY LEAKAGE IN DEEP TRANSFER\nLEARNING\nIn this section, we first provide a general categorization of deep\ntransfer learning according to the interaction manner between\nsource and target domains. Then, based on the categorization, we\nconduct privacy analysis through building specific attacks against\ndifferent transfer learning paradigms.\n3.1\nGeneral Categorization of Deep Transfer\nLearning\nDifferent types of transfer learning models have been proposed over\nthe years, depending on how the knowledge is shared. Although in\ntransfer learning, there may exist several existing categorizations\nin the literature [25, 27, 35], we categorize the different deep transfer\nlearning models based on how the two domains interact, their training\nstrategy (e.g., co-training or self-training), and the potential leakages.\nBroadly speaking, those transfer learning models can be categorized\ninto three types, i.e., model-based, mapping-based, and parameter-\nbased, as illustrated in Figure 1.\n• Model-based (in Figure 1(a)) is a simple but effective transfer\nlearning paradigm, where the pre-trained source domain\nmodel is used as the initialization for continued training on\nthe target domain data. Model-based fine-tuning has been\nbroadly used to reduce the number of labeled data needed\nfor learning new tasks/tasks of new domains [4, 10, 11].\n• Mapping-based (in Figure 1(b)) methods aim to align the\nhidden representations by explicitly reducing the marginal\nor conditional distribution divergence between source and\ntarget domains. More specifically, alignment losses, i.e., usu-\nally in forms of distribution discrepancies/distance metrics\nsuch as MMD and JMMD, between domains are measured\nby such feature mapping approaches and minimized in the\nloss functions [17, 18, 29].\n• Parameter-based (in Figure 1(c)) methods transfer knowledge\nby jointly updating a shared partial network to learn transfer-\nable feature representations across domains in a multi-task\nfashion. This type of methods is mainly achieved by param-\neter sharing. Regardless of the design differences, they all\nutilize a shared network structure to transform the input data\ninto domain-invariant hidden representations [15, 38, 39].\nBased on the general categorization, we will discuss the threat\nmodel, information leakage and the customized attacks against\neach transfer learning paradigm in detail.\n3.2\nThreat Model\nIn this work, we assume all the parties , i.e., the domain-specific data\nowners, are semi-honest, where they follow exactly the computation\nprotocols but may try to infer as much information as possible when\ninteracting with the other parties. More specifically, we work on\nthe threat model under a deep transfer learning setting with two\ndomains. Without loss of generality, we assume the owner of the\ntarget dataset to be the attacker, who intentionally attempts to\ninfer additional information from source domain data beyond what\nis explicitly revealed. Depending on the different deep transfer\nlearning categorizations, the attacker may have different access to\nthe source domain information. Note that we can naturally extend\nto the case where the attacker is the owner of the source dataset\nor the transfer learning setting with more than two data sources,\nhowever, it is not the focus of this paper.\n3.3\nPrivacy Analysis\nAs presented in Table 1, three types of model categorization require\ndifferent information interactions and training strategies, thus pos-\ning different potential leakage profiles. More specifically:\n• Model-based methods rely on the pre-trained source domain\nmodel solely. Despite the necessity for a pre-trained model to\nbe disclosed to the target domain, both the source and target\ntraining processes can be entirely separately, thus the poten-\ntial leakage is only the final source domain model\n\nMS \u000b\n. In\nthis case, the attacker has full access to the source domain\nmodel including both the model structure and parameters.\n• Mapping-based methods are optimized in a co-training fash-\nion and hidden presentations of both domains have to inter-\nact with each other to measure the alignment losses or do-\nmain regularizers. We denotehS\nik andhT\nik as the hidden repre-\nsentation of layer i at a training iteration k ∈[1, 2, ...,K] for\nsource and target domains, respectively. Specifically, such a\nfeature matching process demands the hidden presentations,\ni.e., hS\nik and hT\nik, of both domains to be exposed and aligned\nto reduce the marginal or conditional distribution divergence\nbetween domains. As a result,\n\nhS,hT \u000b\ncan potentially leak\ninformation from the training data.\n• Parameter-based methods jointly updating the shared partial\nnetwork. The interactions between the source and target\nhappen when exchanging the gradients to sync the shared\nnetwork parameters. Letwc\nk denote the model parameters for\nthe shared network structure. At each training iterationk,wc\nk\nis updated by averaging the gradients ▽W learned by a mini-\nbatch of training examples zi sampled from a domain dataset\nKDD ’20, August, 2020, San Diego, CA\nTrovato and Tobin, et al.\nTable 1: General categorization of different deep transfer learning paradigms and their respective leakage profiles. Without\nloss of generality, we assume a semi-honest threat model with attacker to be the target domain dataset owner. Three types of\ntransfer learning paradigms require different information interactions and training strategies, thus pose different potential\nleakage profiles and may suffer from different inference attacks.\nCategorization\nBrief Description\nTraining\nLeakage\nInference\nStrategy\nProfile\nAttack Type\nModel-based\nModel fine-tuning, i.e., continue training on the target domain.\nSelf-training\n\nMS \u000b\nMembership\nMapping-based\nHidden representations alignment, i.e., reducing distribution divergence.\nCo-training\n\nhS,hT \u000b\nProperty\nParameter-based\nJointly updating shared partial network, i.e., hard parameter sharing.\nCo-training\n⟨wc⟩\nBatch property\n(either source or target). In such an alternating process, wc\nk\nhas to be revealed across domains at each iteration k ∈\n[1, 2, ...,K]. Thus, the potential leakage profile\nD\nwc\nk\nE\ncontains\nall the intermediate wc\nk during the training process.\nAs a summary, in this paper, we focus on inferring information\nthat is unintentionally disclosed by the data transmission between\nthe source and target domains, such as membership information of\nan individual sample and property information of a specific sample\nor a subset of samples (see more details in the next subsection.).\n3.4\nInference Attacks for Deep Transfer\nLearning\nInference attacks against deep learning models generally exploit im-\nplicit memorization in the models to recover sensitive information\nfrom the training data. Previously studies have proven that informa-\ntion can be inferred from the leakage profile, such as membership\ninformation that decides whether a sample is used for training\n[14, 20, 22, 32, 36], properties which reveals certain data characteris-\ntics, or reconstructed feature values of the private training records.\nIn this part, to empirically evaluate the privacy leakage in dif-\nferent transfer learning settings (shown in Figure 1), we present\nconcrete attack methods for each setting. Note, we assume the at-\ntacker to be the owner of the target dataset for all the three transfer\nlearning paradigms discussed above.\nModel-based. As illustrated in Table 1, the only leakage source in\nthis setting is the model trained using data from the source domain.\nFor simplicity, we denote the trained model as MS : y = fSw(x),\nwhere y is the prediction label. According to the training protocol\nin the model-based setting, the attacker can obtain the white-box ac-\ncess of MS, i.e., its structure and parameters. Thus, based on recent\nworks [19, 22, 26, 30, 32], an attacker can design powerful mem-\nbership attack methods to detect sensitive information contained\nin the source domain (i.e., membership attack). In the context of\nmembership attack, the goal of the attacker is to train a membership\npredictor, which can take a given sample x as input and output the\nprobability distribution that indicates whether the given sample\nis used for training the source domain model MS. Formally, we\ndenote the membership predictor as Amem : x, MS →{P(m =\n1|x), P(m = 0|x)}. Here, m = 1 means the given input x is used for\ntraining the model MS.\nIn this paper, we employ a widely used technique named shadow\nmodel training for building the membership predictor. Specifically,\nwe assume the attacker can have extra knowledge about the source\ndataset DS. The extra prior knowledge in our setting is the shadow\ntraining dataset Dshadow that comes from the same underlying\ndistribution as DS for training the source model. The core idea of\nthe shadow training dataset is to first train multiple shadow models\nthat mimic the behavior of the source model. Then, the attacker\ncan extract useful features from the shadow models/datasets and\nbuild a machine learning model that characterizes the relationship\nbetween the extracted features and the membership information.\nTo be specific, the attacker first evenly divides the shadow train-\ning dataset Dshadow into two disjoint datasets Dtrain\nshadow and Dout\nshadow.\nThen the attacker trains the shadow model that has the same archi-\ntecture as the source model using Dshadow. Subsequently, features\nof samples from both Dtrain\nshadow and Dout\nshadow can be extracted. For\neach sample in Dshadow, the attacker can use the output predic-\ntion vector of the shadow model as the feature following the prior\nwork [32]. And each feature vector is labeled with 1 (member, if\nthe sample is in Dtrain\nshadow) or 0 (non-member, if the sample is in\nDout\nshadow). At last, all the feature-label pairs are used for training\nthe membership predictor.\nOnce the membership predictor is obtained, we can use it to\npredict the membership label of a sample in DS, i.e., feeding the\noutput vector of the source domain model to the predictor.\nMapping-based. In the mapping-based setting, information leak-\nage can be both posed at the source and target domains since the\ntraining protocol proceeds in an interactive manner. To keep the\nanalysis consistency, we refer the attacker to be the owner of the\ntarget domain dataset.\nThe core idea of the mapping-based method is to align the hidden\nfeatures extracted from the source and target domains, i.e., reducing\nthe discrepancy between feature distributions of these two domains.\nThis can be done by minimizing some pre-defined alignment loss\nfunction (e.g., maximum mean discrepancy[17, 18, 25, 29]). Thus,\nthis method will lead to the hidden features of the source domain\nshare a similar distribution to the features of the target domain,\nwhich comes with a strong privacy implication, i.e., the attacker can\nleverage the feature similarity to build the attack model to detect\nsensitive information contained in the source domain.\nWe consider the property attack [7] in this setting. At a high level,\nthe property attack aims to infer whether a feature comes from the\nsource domain has a specific property or not. Here, we take the\ncase of the k-th iteration as an example. Given a specific property,\nthe attacker can first collect an auxiliary dataset for assisting the\nattack. Specifically, the attacker divides the target domain dataset\nDT into two subsets, namely, DTprop which contains samples with\nthe property and DTnonprop which consists of samples without the\nA Comprehensive Analysis of Information Leakage in Deep Transfer Learning\nKDD ’20, August, 2020, San Diego, CA\nAlgorithm 1: Property Attack\n1 Inputs:\n2\nModel parameters wk(on target domain)\n3\nAuxiliary datasets: DTprop and DTnonprop\n4 Outputs:\n5\nProperty Predictor Aprop\n6 Pprop ←[]\n7 Pnonprop ←[]\n8 for each sample s in DTprop do\n9\nCalculate the hidden feature hT\nk\n10\nAppend {hT\nk , 1} to Pprop\n11 end\n12 for each sample s in DTnonprop do\n13\nCalculate the hidden feature hT\nk of s\n14\nAppend {hT\nk , 0} to Pnonprop\n15 end\n16 Train Aprop using Pprop ∪Pnonprop\n17 return Aprop\nproperty. Subsequently, DTprop and DTnonprop are used as the aux-\niliary datasets for building the attack model. At the k-th iteration,\ngiven the current parameter wk of the model trained using the\ntarget dataset, the attacker calculates the hidden features hT\nk of\nthe alignment layer k with respect to the samples in the auxiliary\ndataset. Hidden features are labeled with 1 if the corresponding\nsample is in DTprop and 0 if the sample is in DTnonprop. Once the\nattacker collects all the feature-label pairs, she can train a prop-\nerty predictor Aprop using these pairs. The whole procedure is\ndemonstrated in Algorithm 1.\nBased on the property predictor obtained above, the attacker can\nconduct an online attack. Specifically, in the joint training process,\nat the beginning of the k-th iteration, the attacker receives a batch\nof hidden features hS\nk from the source domain. Then, the attacker\ncan employ Aprop to predict the property information contained\nin the source domain dataset.\nParameter-based. In the parameter-based setting, information\nleakage is posed by the weight parameter interaction between the\nsource and target domains. Similar to previous settings, we refer\nthe attacker to be the owner of the dataset of the target domain.\nWe consider the batch property attack in this setting. The intu-\nition behind this attack is that the attacker can observe the updates\nof the share layers calculated using a mini-batch of samples from\nthe source domain. Thus the attacker can train a batch property\npredictor Abprop to infer whether the update based on the mini-\nbatch with a given property or not. We take the k-th iteration as\nan example to demonstrate how the attacker conducts the attack.\nWe assume that the attacker has auxiliary dataset Daux consisting\nof samples from the distribution that is similar to the source do-\nmain distribution. Note that, this assumption is commonly used\nin various previous works [23, 31]. Given a specific property, the\nattacker can further divide Daux into two sub-datasets, namely, the\ndataset which has the property (Daux\nprop) and the dataset without the\nproperty (Daux\nnonprop).\nAlgorithm 2: Batch Property Attack\n1 Inputs:\n2\nModel parameters wk of the shared partial network\n3\nAuxiliary datasets: Daux\nprop and Daux\nnonprop\n4\nSample size: Lprop, Lnonprop\n5 Outputs:\n6\nBatch Property Predictor Abprop\n7 Pprop ←[]\n8 Pnonprop ←[]\n9 for l=1 to Lprop do\n10\nSample mini-batch bprop from Daux\nprop\n11\nCalculate the gradient дprop\n12\nAppend {дprop, 1} to Pprop\n13 end\n14 for l=1 to Lnonprop do\n15\nSample mini-batch bnonprop from Daux\nnonprop\n16\nCalculate the gradient дnonprop\n17\nAppend {дnonprop, 0} to Pnonprop\n18 end\n19 Train Abprop using Pprop ∪Pnonprop\n20 return Abprop\nBased on the above setting, at the k-th iteration, the attacker can\nreceive the fresh parameter wk of the shared layer which is updated\nbased on samples from the source dataset. Then, the attacker can\ncalculate gradients дprop using the mini-batch sampled from Daux\nprop\nandдnonprop based on samples from Daux\nnonprop. The batch gradients\nbased on Daux\nprop are labeled with 1 and others are labeled with 0.\nBased on these gradient-label pairs, we can train the batch property\npredictor Abprop. The whole procedure is shown in Algorithm 2.\nOnce Abprop is obtained, the attacker can use it to predict the\nbtach property information of the source domain. Specifically, the\nattacker first generates the gradient дonline based on the parame-\nters of the current and last iteration (i.e., wk and wk−1). Then she\ncan feed дonline to Abprop to obtain the prediction result.\n4\nEXPERIMENTS\nIn this section, we empirically study the potential information leak-\nages in deep transfer learning. We start by describing the datasets\nand the experimental setup. We then discuss in detail the results for\nvarious inference attacks under the aforementioned three transfer\nlearning settings, followed by the examination of viable defenses.\n4.1\nDatasets and Experimental Setup\nWe conduct experiments on two public and one industry datasets,\ni.e., Review, UCI-Adult, and Marketing. The Review dataset is used\nto examine membership attack as fine-tuning methods are often\nused in such NLP task and typically suffer from membership infer-\nence. While the UCI-Adult dataset has some sensitive properties,\nthus is used for conducting property based attacks. Besides, the\nMarketing dataset is sampled from real-world marketing campaigns\nto examine the defense solution. Data statistics are in Table 2.\nKDD ’20, August, 2020, San Diego, CA\nTrovato and Tobin, et al.\nTable 2: Data statistics and tasks used for experiment.\nDataset\nStatistics\nTask\nAmazon Review\nElectronics: #354,301\nReview Quality\nWatches: #9,737\nPrediction\nUCI Adult\nSource Train: #29,170\nSource Test: #14,662\nCensus Income\nTarget Train: #3391\nPrediction\nTarget Test: #1619\nMarketing\nSource: #236,542\nCoupon Adoption\nTarget: #140,964\nPrediction\nReview. We construct our dataset from the public available Amazon\nreview data [21], with two categories of products selected, i.e.,\n‘Watches’ and ‘Electronics. In our transfer learning setting, the\ndata-abundant domain ‘Electronics’ is viewed as the source domain,\nwhile the data-insufficient domain ‘Watches’ is treated as the target.\nEach sample consists of a quality score and a review text.\nFollowing the literature on this task [3], we adopt the TextCNN\n[12] as the base model for textual representation in the transfer\nlearning model. For TextCNN, we set the filter windows as 2,3,4,5\nwith 128 feature maps each. The max sequence length is set as 60\nfor the task. We initialize the embedding lookup table with pre-train\nword embedding from GloVe with embedding dimension set as 300.\nThe batch sizes for training the source domain model, the shadow\nmodel, and the attack model are set as 64.\nUCI-Adult. We consider a second Adult Census Income dataset [13]\nfrom the UC Irvine repository to examine the property attack is-\nsues. The dataset has 14 properties such as country, age, work-class,\neducation, etc. To form the transfer learning datasets, we use data\ninstances with country of “U.S.” as source domain and “non-U.S.”\nas the target. For the UCI dataset, we consider an MLP as our base\nmodel for the transfer learning models.\nMarketing. For transfer learning purpose, two sets of data are sam-\npled from two real-world marketing campaigns data that contains\nuser profile, behaviour features and adoptions. One data-abundant\ncampaign is used as source domain, while the other is used as the\ntarget. The task is to predict whether a user will adopt the coupon.\nImplementation Details. Specifically, for model-based setting, a\nfine-tuning approach is adopted where both the source domain\nmodel and the shadow model employed the same model structure,\ni.e., the above-mentioned TextCNN structure, followed by an MLP\nlayer of size 128. The attack model adopted is an MLP with network\nof [16, 8]. For the mapping-based setting, we consider an MLP\nwith a network structure of [64, 8] for source and target models,\nan MMD metric is used as the alignment loss between the two\ndomains, and another MLP with network of [64, 8] for the attack\nmodel. For parameter-based setting, we consider a fully-shared\nmodel structure with an MLP as the base model with structure of\n[64, 8] at the task training stage, and another MLP with network of\n[16, 8] for the attack model.\nAll the above neural network based transfer learning models\nare implemented with TensorFlow and trained with NVIDIA Tesla\nP100 GPU using Adam optimizer, if not specified. The learning rate\nis set as 0.001. The activation function is set as ReLU.\nTable 3: Membership attack for model-base settings.\nTrain Acc\nTest Acc\nAttack AUC\nAttack Prec.\n10 epoch\n0.9290\n0.7182\n0.5014\n0.4705\n20 epoch\n0.9338\n0.8648\n0.7355\n0.5868\n30 epoch\n0.9589\n0.8563\n0.6744\n0.5832\nFor all the settings, we use Area Under Curve (AUC) to eval-\nuate the overall attack performance. For membership attack, we\nalso adopt precision for evaluation, as membership of the training\ndataset is the concern. Precision is defined as the fraction of samples\npredicted as members are indeed members of the training data. For\nproperty attacks, we further include accuracy for evaluating the\noverall attack performance.\n4.2\nModel-based Setting\nIn the model-based setting, the attacker has the full white-box ac-\ncess to the source model, thus the attacker is able to train shadow\nmodels to mimic the behavior of the source domain mode. In this\nsetting, we explore the possibility of performing membership infer-\nence on the source model. Despite the context differences, mem-\nbership attacks for model-based transfer learning models can be\nconducted the same way as for any trained stand-alone deep learn-\ning models[23, 30, 32].\nTo build the membership predictor introduced in Section 3, we\nsplit the original source dataset into three parts, namely, training\ndataset (248,011 samples), test dataset (70,860 samples), and shadow\ndataset (70,860 samples). We set the number of shadow models to 3.\nTable 3 shows the training and testing accuracy for the attack clas-\nsifier on the shadow dataset and the attack performance, i.e., AUC\nand precision, on the source domain model. Smaller gaps between\ntraining and testing accuracy suggests better generalization and\npredictive power of the attack classifier. Overall, we find that even\nin the model-based setting without direct interactions between the\nsource and target domains during the training process, the source\ndomain model does leak a considerable amount of membership\ninformation. We also examine the effect of over-fitting by adjust-\ning the number of epoch trained for both the source domain and\nshadow models. We observe the more over-fitted a model, the more\ninformation can be potentially leaked. However, it decreases when\nthe number of epochs furthers increases. This echos the findings\nin [32] that over-fitting is an important factor but not the only\nfactor that contributes to the leakage of membership information.\n4.3\nMapping-based Setting\nWe then investigate property attacks under the mapping-based\ntransfer learning setting to infer the properties of the source domain\ndata during the training process. The properties are not the same\nas the prediction label, or even independent of the prediction task.\nTo examine this, we construct two more datasets based on the UCI\nadult dataset: Prop-race and Prop-sex, discussed as follows.\n• Prop-sex: we filter out the property “sex” feature from the\ninput features in UCI-Adult data and use it as the attack task\nlabel. The attack label is set as 1 if the property “sex” is male\nA Comprehensive Analysis of Information Leakage in Deep Transfer Learning\nKDD ’20, August, 2020, San Diego, CA\nTable 4: Property attack on “sex” for mapping-base settings.\nProp-sex\nTest\nAttack\nAttack Prec.\nAUC\nAcc\nAUC\nmale\nfemale\n2 epoch\n0.8513\n0.8011\n0.7357\n0.7010\n0.5918\n5 epoch\n0.8522\n0.8101\n0.7435\n0.7072\n0.6077\n10 epoch\n0.8650\n0.8171\n0.7766\n0.7132\n0.5883\nTable 5: Property attack on “race” for mapping-base settings.\nProp-race\nTest\nAttack\nAttack Prec.\nAUC\nAcc\nAUC\nwhite\nnon-white\n2 epoch\n0.7640\n0.8000\n0.5661\n0.8812\n0.1652\n5 epoch\n0.7680\n0.8012\n0.5815\n0.8914\n0.1845\n10 epoch\n0.7750\n0.8010\n0.5885\n0.8901\n0.2179\nTable 6: The Pearson correlation between the main predic-\ntion task and the property label vs. property attack results.\nCorrelation\nAttack AUC\nAttack Acc\nProp-race\n-0.0837\n0.5885\n0.5646\nProp-sex\n-0.2146\n0.7766\n0.7059\nand 0 otherwise. This is to examine whether a attacker can\ninfer training instance’s gender during the training process.\n• Prop-race: similarly we filter out the “race” feature to study\nwhether this property appears in training instances. Attack\nlabel is set as 1 if property “race” is white and 0 otherwise.\nTable 4 shows the results of the property attack on the Prop-\nsex dataset. The attack has a good AUC of 0.77 in the property\ninference task, which shows the transfer learning setting does have\ninformation leakage. The attack precision is high on the property\n“sex:male”, and less satisfactory on “sex:female”. The female property\nhas much less instances and thus may be harder to be predicted.\nWe have also conducted experiments on another property “race”\nin Table 5, the attack AUC is lower than on Prop-sex, 0.5885 vs.\n0.7766. These results demonstrate that Prop-sex leaks more prop-\nerty information than Prop-race in terms of attack AUC. At the\nfirst glance, “race: white” has much higher attack precision than\n“race:non-white”, 0.8901 vs. 0.2179. Close examination shows “race:\nwhite” dominates the training data with around 80% label coverage.\nWe further examine the correlation between the chosen proper-\nties and the underlying Census Income prediction task, and com-\npare it with the property attack results in Table 6. In general, both\nProp-race and Prop-sex have information leakage issues, and the\nproperty with a higher negative/positive correlation with the main\nprediction task tends to cause higher potential information leakage.\n4.4\nParameter-based Setting\nFor the parameter-based setting, we further process the UCI adult\ndata to perform the batch property attack. Again we use these two\nproperties “race” and “sex” and form two datasets namely BProp-\nrace and BProp-sex respectively. For both datasets, we set the batch\nsize as 8. For each batch in BProp-race, we set the label as 1 if a batch\nhas at least one data instance containing the “non-white” property\nTable 7: Batch property attack data statistics.\nSource data size\nTarget data size\nPositive\nNegative\nPositive\nNegative\nBProp-race\n38\n528\n2225\n2637\nBProp-sex\n65\n501\n418\n4444\nTable 8: Batch property attack for parameter-based settings.\nTest\nAttack\nAUC\nAcc\nAUC\nAcc\nBProp-race\n0.8466\n0.8054\n0.5545\n0.5372\nBProp-sex\n0.8577\n0.8331\n0.5654\n0.9140\nand 0 otherwise. For BProp-sex, we set the label as 1 if a batch has at\nleast one instance containing the “female” property and 0 otherwise.\nThe data statistics are shown in Table 7. We find for BProp-race, the\npositive instance ratio in the source domain is drastically different\nfrom the target, this may bring negative transfer. While for the\nBProp-sex, the positive instance ratio is similar for both domains.\nAs shown in Algorithm 2, the selection of Daux is the key to\nbuilding the batch property predictor. In this paper, we directly\nuse the target dataset as the auxiliary dataset and find it works in\npractice, as both domains commonly are related for the transfer-\nring purpose. Generally, we can obtain better attack performance\nif we can use part of source domain samples to form the auxiliary\ndataset. The results are presented in Table 8. First, we find the at-\ntack AUC for the batch property attack in the parameter-based\nsetting are generally not as high as those from the property attack\nin the mapping-based setting. The AUC is around 0.56 in batch\nproperty attack, while up to 0.77 for property attack. This shows\nthe parameter-based setting is generally less vulnerable from at-\ntacks, which can possibly be justified by the fact that the gradients\nexchanged have been averaged at the batch-level. Second, the re-\nsults for BProp-sex are better than BProp-race, as AUC results are\n0.5654 and 0.5545 respectively. This is intuitive as observed from\nTable 7, the domain difference in BProp-race is larger than the\nBProp-sex. The model performance in BProp-race may suffer from\nthe negative transfer learning due to the domain gap, thus may lead\nto the decrease in attack performance. Overall, the parameter-based\nmethod is possible to suffer from batch property attacks, however,\nthe information leakage problem is not that severe.\n4.5\nDefense Solutions\nA standard way to prevent statistical information leakage is differ-\nential privacy. However, the privacy guarantee provided by differ-\nential privacy comes with the decreasing of the model utility. Some\nrecent works proposed to relax the requirement of differential pri-\nvacy to prevent membership/property attacks while provide better\nmodel utility. For example, some regularization techniques such as\ndropout are helpful to prevent the information leakage of machine\nlearning models [23, 36]. Recent studies consider to use Stochas-\ntic gradient Langevin dynamics (SGLD) [24, 36]. In this paper, to\nprevent the information leakage in deep transfer learning, we em-\nploy SGLD as our optimizer to optimize deep models and show\nits effectiveness in reducing information leakage. Prior work [36]\nKDD ’20, August, 2020, San Diego, CA\nTrovato and Tobin, et al.\nTable 9: A comparison of defenses for property attack.\nProp-sex\nProp-race\nAUCT ask\nAUCAttack\nAUCT ask\nAUCAttack\nOriginal\n0.8466\n0.7766\n0.7750\n0.5885\nSGLD\n0.8501\n0.6862\n0.8012\n0.5442\nDP-SGD\n0.7662\n0.6656\n0.7192\n0.5172\nDropout-0.1\n0.7449\n0.7126\n0.7748\n0.6040\nDropout-0.5\n0.5881\n0.6132\n0.6194\n0.5199\nDropout-0.9\n0.5240\n0.5188\n0.5267\n0.5041\ndemonstrates that SGLD is effective in preventing membership in-\nformation leakage and provides theoretical bounds for membership\nprivacy loss. Empirically, in this paper, SGLD is also effective for\npreventing the leakage of the property information while provides\ncomparable model performance compared with those non-private\ntraining methods.\nTo examine the effectiveness of the defense solutions, we con-\nduct experiments under the mapping-based setting. Specifically,\nwe replace the non-private optimizer (i.e., SGD) with SGLD and\ntrain the source and target models from scratch. The overall result\nis shown in Table 9. We observe that the use of SGLD can prevent\nthe information leakage of the training dataset, based on the above\nattack metrics for evaluating the information leakage. In contrast to\nthe original optimizer, SGLD significantly reduces the attack AUC\nscore of the inference attack from 0.7766 to 0.6862 in Prop-sex, and\n0.5885 to 0.5442 in Prop-race, while achieves better model utility\nin terms of the task AUC score especially in Prop-race (0.7750 to\n0.8012). In both datasets, SGLD achieves the best task AUC and\nslightly outperforms the original task results. We infer the boost of\nthe task AUC score is due to the regularization effect of the noises\ninjected in SGLD, which can prevent model overfitting as pointed\nout in the previous works [24, 36].\nWe also conduct experiments of DP-SGD introduced in the prior\nwork [1]. As the result shows, although DP-SGD can achieve better\nanti-attack ability than SGLD, it comes with considerably model\nutility decrease (the task AUC score decreases from 0.8466 to 0.7662\nin Prop-sex, and 0.7750 to 0.7192 in Prop-race ). Furthermore, ex-\nperiments on Dropout are also performed. We can observe that\nDropout can also prevent privacy leakage. We also note that when\nthe drop ratio increases, the anti-attack ability of Dropout increases\nbut model utility decreases drastically. As a conclusion, through\nexperiments, we found that SGLD can be a good alternative to dif-\nferentially private optimization methods and it can achieve a better\ntrade-off between model utility and anti-attack ability.\n5\nINDUSTRIAL APPLICATION\nWitness the effectiveness of the SGLD, we conduct additional exper-\niments to examine whether it is able to prevent information leakage\nin an industrial application. The main task is to predict whether\na user will use the coupon and the inference task is to predict the\nproperty âĂĲmarital status: marriedâĂİ (the Pearson correlation\nbetween these labels is -0.03986). Follow the original application,\nhere we adopt a fully-shared model [15] under the parameter-based\nsetting. As shown in Table 10, we find the original TL method does\nhelp to improve the target domain performance, boosting AUC\nfrom 0.7513 (Target-only) to 0.7704. However it also suffers from\ninformation leakage with an attack AUC of 0.5553. By combining\nSGLD with transfer learning, with a minor decrease of task perfor-\nmance (-0.8%), the information leakage can be significantly reduced\nby 3.8%.\nTable 10: A comparison of defenses for batch property attack\non the “Marketing” dataset in terms of AUC.\nTarget\nOriginal\nDefense\nonly\nTL\nAttack\nTL+SGLD\nAttack\nAUC\n0.7513\n0.7704\n0.5553\n0.7625(-0.8%)\n0.5176(-3.8%)\n6\nRELATED WORK\nDeep Transfer Learning Models. With the success of deep learn-\ning, deep transfer learning has been widely adopted in various ap-\nplications [25, 27, 35]. According to our categorization based on\nthe potential information leakage, transfer learning models can be\nsummarized into three types, i.e., model-based, mapping-based, and\nparameter-based, or a hybrid of the different types[18].\nModel-based methods, such as model fine-tuning, generally first\npre-train a model using the source domain data and then con-\ntinue training on the target domain data[4, 10, 11]. Mapping-based\nmethods aim to align the hidden representations by explicitly re-\nducing the marginal/conditional distribution divergence between\nsource and target domains which are measured by some distribu-\ntion difference metrics. Commonly used metrics include variants of\nMaximum Mean Discrepancy(MMD), Kullback-Leibler Divergence,\nWasserstein distance, and etc [17, 18, 25, 29, 35]. Parameter-based\nmethods transfer knowledge by jointly updating a shared network\nto learn domain-invariant features across domains, which is mainly\nachieved by parameter sharing [15, 38, 39]. Further works improve\nparameter-based methods by incorporating adversarial training to\nbetter learn domain-invariant shared representations [15].\nThere are few studies [16, 31] for analyzing privacy leakages\nfor general machine learning models or in a federated learning set-\nting, however, there is no such privacy analysis work for transfer\nlearning models. To bridge this gap, we first provide a general cate-\ngorization of deep transfer learning models based on information\nleakage types and conduct thorough privacy analysis through build-\ning specific attacks against different transfer learning paradigms.\nWe we also examine several general defense solutions to alleviate\ninformation leakage for the three paradigms. The privacy preserv-\ning models for transfer learning are rarely studied. Prior work [16]\nproposed a secure transfer learning algorithm under the mapping-\nbased setting, where homomorphic encryption was adopted to\nensure privacy at the expenses of efficiency and some model utility\nloss due to the Taylor approximation. A follow-up work in [31]\nfurther enhanced the security and efficient for the same problem\nsetting by using Secret Sharing technique. A recent work by [8] em-\nployed a privacy preserving logistic regression with ϵ-differential\nprivacy guarantee under a hypothesis transfer learning setting.\nMembership Inference. The study [32] developed the first mem-\nbership attack against machine learning models with only black-box\naccess using a shallow training method. This method assumes that\nA Comprehensive Analysis of Information Leakage in Deep Transfer Learning\nKDD ’20, August, 2020, San Diego, CA\nwe have some prior knowledge of the data for training the attack\nclassifier is from the same distribution as the original training data.\nLater study of [19] followed the idea of shallow training and ex-\nplored two more targeted membership attacks, i.e., frequency-based\nand distance-based. The study[30] further relaxed key attack as-\nsumptions of [32] and demonstrated more applicable attacks. Aside\nfrom the black-box setting, these studies [22, 26] examined the\nmembership attacks against federated/collaborative learning under\nthe white-box setting, where an adversary can access the model and\npotentially is able to observe/eavesdrop the intermediate computa-\ntions at hidden layers. They share a similar idea that leverages the\ngradients or model snapshots to produce the labeled examples for\ntraining a binary membership classifier. This work [9] presented the\nfirst membership attacks on both black-box and white-box for gener-\native models, in particular generative adversarial networks (GANs).\nProperty Inference. Property attack aims to infer the properties\nhold for the whole or certain subsets of the training data. Prior\nworks [2, 6, 7] studied property inference attacks that characterize\nthe entire training dataset. A property attack was developed [7]\nbased on the concept of permutation invariance for fully connected\nneural networks, with the assumption that adversary has white-box\nknowledge. Concurrently, the study [22] developed attacks under\nthe collaborative learning setting, where they focus on inferring\nproperties for single batches of training inputs.\n7\nCONCLUSION\nIn this study, we provide a general categorization of different deep\ntransfer learning paradigms depending on how the domains inter-\nact with each other. Based on that, we then analyze their respective\nprivacy leakage profiles, design different attack models for each\nparadigm and provide potential solutions to prevent these threats.\nExtensive experiments have been conducted to examine the poten-\ntial privacy leakage and effectiveness of defense solutions.\nREFERENCES\n[1] Martín Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov,\nKunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In\nProceedings of the 2016 ACM SIGSAC Conference on Computer and Communications\nSecurity, CCS 2016. 308–318.\n[2] Giuseppe Ateniese, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,\nDomenico Vitali, and Giovanni Felici. 2015.\nHacking smart machines with\nsmarter ones: How to extract meaningful data from machine learning classifiers.\nIJSN 10, 3 (2015), 137–150.\n[3] Cen Chen, Minghui Qiu, Yinfei Yang, Jun Zhou, Jun Huang, Xiaolong Li, and\nForrest Sheng Bao. 2019. Multi-Domain Gated CNN for Review Helpfulness\nPrediction. In WWW 2019. 2630–2636.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT 2019.\n[5] Sébastien Dubois, Nathanael Romano, Kenneth Jung, Nigam Shah, and David C.\nKale. 2017. The Effectiveness of Transfer Learning in Electronic Health Records\nData. In 5th International Conference on Learning Representations, ICLR 2017\nWorkshop. OpenReview.net.\n[6] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion\nAttacks that Exploit Confidence Information and Basic Countermeasures. In\nProceedings of the 22nd ACM SIGSAC Conference on Computer and Communications\nSecurity, CCS. 1322–1333.\n[7] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. 2018.\nProperty inference attacks on fully connected neural networks using permutation\ninvariant representations. In Proceedings of the 2018 ACM SIGSAC Conference on\nComputer and Communications Security, CCS 2018. ACM, 619–633.\n[8] Xiawei Guo, Quanming Yao, Wei-Wei Tu, Yuqiang Chen, Wenyuan Dai, and\nQiang Yang. 2018. Privacy-preserving Transfer Learning for Knowledge Sharing.\nCoRR abs/1811.09491 (2018).\n[9] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2019.\nLOGAN: Membership Inference Attacks Against Generative Models. PoPETs\n2019, 1 (2019), 133–152.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nLearning for Image Recognition. In 2016 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR. 770–778.\n[11] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-\ntuning for Text Classification. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2018. 328–339.\n[12] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2014. 1746–1751.\n[13] Ron Kohavi. 1996. Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-\nTree Hybrid. In Proceedings of the Second International Conference on Knowledge\nDiscovery and Data Mining (KDD-96), Portland, Oregon, USA. 202–207.\n[14] Ninghui Li, Wahbeh H. Qardaji, Dong Su, Yi Wu, and Weining Yang. 2013. Mem-\nbership privacy: a unifying framework for privacy definitions. In 2013 ACM\nSIGSAC Conference on Computer and Communications Security, CCS’13. 889–900.\n[15] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017. Adversarial Multi-task\nLearning for Text Classification. In Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2017. 1–10.\n[16] Yang Liu, Tianjian Chen, and Qiang Yang. 2018. Secure Federated Transfer\nLearning. CoRR abs/1812.03337 (2018).\n[17] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. 2015. Learning\nTransferable Features with Deep Adaptation Networks. In Proceedings of the 32nd\nInternational Conference on Machine Learning, ICML 2015. 97–105.\n[18] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. 2017. Deep\nTransfer Learning with Joint Adaptation Networks. In Proceedings of the 34th\nInternational Conference on Machine Learning, ICML 2017. 2208–2217.\n[19] Yunhui Long, Vincent Bindschaedler, and Carl A. Gunter. 2017. Towards Measur-\ning Membership Privacy. CoRR abs/1712.09136 (2017).\n[20] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu\nTang, Carl A. Gunter, and Kai Chen. 2018. Understanding Membership Inferences\non Well-Generalized Learning Models. CoRR abs/1802.04889 (2018).\n[21] Julian J. McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics:\nunderstanding rating dimensions with review text. In Seventh ACM Conference\non Recommender Systems, RecSys ’13, Hong Kong, China, October 12-16, 2013.\n[22] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.\n2019. Exploiting Unintended Feature Leakage in Collaborative Learning. In 2019\nIEEE Symposium on Security and Privacy, SP 2019. 691–706.\n[23] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.\n2019. Exploiting Unintended Feature Leakage in Collaborative Learning. In 2019\nIEEE Symposium on Security and Privacy, SP 2019. IEEE, 691–706.\n[24] Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. 2018. Generalization\nBounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints. In\nConference On Learning Theory, COLT 2018 (Proceedings of Machine Learning\nResearch), Vol. 75. PMLR, 605–638.\n[25] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive Privacy\nAnalysis of Deep Learning: Passive and Active White-box Inference Attacks\nagainst Centralized and Federated Learning. In 2019 IEEE Symposium on Security\nand Privacy, SP 2019. IEEE, 739–753.\n[26] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive Privacy\nAnalysis of Deep Learning: Passive and Active White-box Inference Attacks\nagainst Centralized and Federated Learning. In 2019 IEEE Symposium on Security\nand Privacy, SP 2019. 739–753.\n[27] Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE\nTrans. Knowl. Data Eng. 22, 10 (2010), 1345–1359.\n[28] Maithra Raghu, Chiyuan Zhang, Jon M. Kleinberg, and Samy Bengio. 2019. Trans-\nfusion: Understanding Transfer Learning for Medical Imaging. In Annual Confer-\nence on Neural Information Processing Systems, NeurIPS 2019. 3342–3352.\n[29] Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. 2019. Beyond Sharing\nWeights for Deep Domain Adaptation. IEEE Trans. Pattern Anal. Mach. Intell. 41,\n4 (2019), 801–814.\n[30] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and\nMichael Backes. 2019. ML-Leaks: Model and Data Independent Membership\nInference Attacks and Defenses on Machine Learning Models. In 26th Annual\nNetwork and Distributed System Security Symposium, NDSS 2019.\n[31] Shreya Sharma, Chaoping Xing, Yang Liu, and Yan Kang. 2019. Secure and\nEfficient Federated Transfer Learning. CoRR abs/1910.13271 (2019).\n[32] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-\nbership Inference Attacks Against Machine Learning Models. In 2017 IEEE Sym-\nposium on Security and Privacy, SP 2017. 3–18.\n[33] Cosmin Stamate, George D. Magoulas, and Michael S. C. Thomas. 2015. Transfer\nlearning approach for financial applications. CoRR abs/1509.02807 (2015).\n[34] Baochen Sun and Kate Saenko. 2016. Deep CORAL: Correlation Alignment for\nDeep Domain Adaptation. In Computer Vision - ECCV 2016 Workshops, Vol. 9915.\n[35] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chun-\nfang Liu. 2018. A Survey on Deep Transfer Learning. In ICANN 2018 - 27th\nKDD ’20, August, 2020, San Diego, CA\nTrovato and Tobin, et al.\nInternational Conference on Artificial Neural Networks. 270–279.\n[36] Bingzhe Wu, Chaochao Chen, Shiwan Zhao, Cen Chen, Yuan Yao, Guangyu Sun,\nLi Wang, Xiaolu Zhang, and Jun Zhou. 2019. Characterizing Membership Privacy\nin Stochastic Gradient Langevin Dynamics. AAAI (2019).\n[37] Bingzhe Wu, Shiwan Zhao, Guangyu Sun, Xiaolu Zhang, Zhong Su, Caihong Zeng,\nand Zhihong Liu. 2019. P3SGD: Patient Privacy Preserving SGD for Regularizing\nDeep CNNs in Pathological Image Classification. In IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2019. 2099–2108.\n[38] Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. 2017. Transfer Learn-\ning for Sequence Tagging with Hierarchical Recurrent Networks. In 5th Interna-\ntional Conference on Learning Representations, ICLR 2017.\n[39] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transfer-\nable are features in deep neural networks?. In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neural Information Processing Systems\n2014 (NeurIPS). 3320–3328.\n[40] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. 2019. A Comprehensive Survey on Transfer\nLearning. CoRR abs/1911.02685 (2019).\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-09-04",
  "updated": "2020-09-04"
}