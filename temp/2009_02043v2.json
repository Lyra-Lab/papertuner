{
  "id": "http://arxiv.org/abs/2009.02043v2",
  "title": "Data Readiness for Natural Language Processing",
  "authors": [
    "Fredrik Olsson",
    "Magnus Sahlgren"
  ],
  "abstract": "This document concerns data readiness in the context of machine learning and\nNatural Language Processing. It describes how an organization may proceed to\nidentify, make available, validate, and prepare data to facilitate automated\nanalysis methods. The contents of the document is based on the practical\nchallenges and frequently asked questions we have encountered in our work as an\napplied research institute with helping organizations and companies, both in\nthe public and private sectors, to use data in their business processes.",
  "text": "Data Readiness for Natural Language Processing\nFredrik Olsson\nRISE\nSweden\nfredrik.olsson@ri.se\nMagnus Sahlgren\nRISE\nSweden\nmagnus.sahlgren@ri.se\nAbstract\nThis document concerns data readiness in the context of machine learning and Natural Language\nProcessing. It describes how an organization may proceed to identify, make available, validate,\nand prepare data to facilitate automated analysis methods. The contents of the document is based\non the practical challenges and frequently asked questions we have encountered in our work as\nan applied research institute with helping organizations and companies, both in the public and\nprivate sectors, to use data in their business processes.\n1\nIntroduction\nAt the Research Institutes of Sweden (RISE)1, we work in cooperation with other organizations and\ncompanies, both in the public and private sectors, with research and innovation in Natural Language\nProcessing (NLP). A major challenge that we often encounter is a lack of readiness with respect to data.\nEven if the research problem is sufﬁciently well deﬁned, and the business value of the proposed solution\nis well described, it is often not clear what type of data is required, if it is available, or if it at all exists.\nWe ﬁnd that there is often not even a framework available to discuss issues related to data. The purpose\nof this document is to outline and highlight issues related to data accessibility, validity, and utility that\nmay arise in such situations. We hope that this document may serve as a guide for working practically\nwith data in the context of applied NLP.\n1.1\nScope\nThis document is concerned exclusively with data readiness in the context of NLP. Other modalities such\nas images, video, or sensor data are not covered, but similar considerations apply in those cases.\nWork on data readiness related to other forms of data include that of Nazabal et al. (2020), who address\ndata wrangling issues from a general stand-point using a set of case studies, as well as the work by van\nOoijen (2019), and Harvey and Glocker (2019) that both deal with data quality in medical imaging. We\nhave not found any work that focuses speciﬁcally on data readiness in the context of NLP.\n1.2\nHow to use this document\nThe intention is for this document to provide insights into the type of challenges one might encounter,\nwith respect to data, when embarking on a project involving NLP. The document is focused on asking\nthe right questions rather than providing an explicit guide that covers all possible challenges in a project:\nsuch a guiding will inevitably vary with the speciﬁc task at hand. The following four sections make up\nthe document:\n• Data Readiness Levels introduce the notion of data readiness.\n• Project phases outlines the typical structure of a research or innovation project, and puts data readi-\nness into context within that structure.\n1https://ri.se\narXiv:2009.02043v2  [cs.CY]  30 Sep 2020\n• Questions for guidance is the central part of the document. Use the questions to understand the data\nreadiness of your own project.\n• How to contribute contains instructions for how to help us improve the document. Please, reach out\nwith any questions, or suggestions!\n2\nData Readiness Levels\nThe notion of Data Readiness Levels (DRLs) introduced by Lawrence (2017), provides a way of talking\nabout data much in the same way Technology Readiness Levels facilitate communication regarding the\nmaturity of technology (Banke, 2017). As such, it is a framework suitable for exchanging information\nwith stakeholders regarding data accessibility, validity, and utility.\nFigure 1 below illustrates the three different major Bands of the DRLs. Each band can be thought of\nas consisting of multiple levels. At the lowest level, i.e., Band C - Level C-4, someone has heard that\nthere is data to be had. This ”hearsay” level is often where a new project starts — someone knows that\nthere should be data available to work with. Since the access to data often dictates the bounds of possible\nanalysis, and thus the level of results attainable, walking a new project or stakeholder through the DRLs\nis not only a nice-to-have, it is a must-do. If there is no data to work with, it does not matter what kind\nof algorithms are available.\nFigure 1: An overview of the different bands of Data Readiness Levels.\n2.1\nBand C\nBand C concerns the accessibility of data. All work at this level serves to grant the team intended to\nwork with the data access to it; once access is provided, the data is considered to be at Band C - Level\nC-1, and ready to be brought into Band B. Issues that fall under Band C include:\n• Does the data exist? Is the data required to address the task even recorded?\n• Data conversion and encoding. One of the major challenges faced within NLP is the conversion of\ndocuments from a source format, e.g., PDF, Word or Excel, to a format suitable for addressing the\ntask at hand. In order to move beyond Band C, data conversion and encoding have to be in place.\n• Legal aspects of accessibility. Not only should the data be available to the intended team, but the\nteam and the result of their efforts to produce a solution to the task at hand should also be cleared\nwith respect to the legal aspects of accessing, and handling of the data. This include, e.g., the\nhandling of personal identiﬁable information, and copyright issues.\n• Programmatic aspects of accessibility. The team should have easy access to the data, by a method\nof their choice, e.g., via an API, or a database interface.\n2.2\nBand B\nBand B concerns the validity of data. In order to pass Band B, the data has to be valid in the sense that\nit is representative of the task at hand. Furthermore, the data should be deduplicated, noise should be\nidentiﬁed, missing values should be characterized, etc. At the top level of Band B, the data should be\nsuitable for exploratory analysis, and the forming of working hypotheses.\n2.3\nBand A\nBand A concerns the utility of data. The utility of the data concerns the way in which the data is intended\nto be used: Is the data enough to solve the task at hand?\nA project should strive for data readiness at Band A - Level A-1. Note that the Data Readiness Levels\nshould be interpreted with respect to a given task. As an example, data that is at Band A for the task\nof pre-training a language model might be considered Band B for the task of training a Named Entity\nRecognizer, simply because the requirements on the data are different in the two tasks. For training a\nlanguage model, it could be enough if the data is unlabelled, clean and drawn from domains of general\ninterest, while for the Named Entity Recognizer, the data will need to be annotated with the appropriate\ninformation, and taken from the very speciﬁc domain in which the recognizer will then operate.\n3\nProject phases\nTo put the need for training, validation, and evaluation data into context, let’s have a look at the structure\nof a typical NLP project. There are many variants of the way organizations work with applied research,\ninnovation, and analytics. Most of them progress roughly by the following steps:\n1. Specify the problem to solve.\n2. Assess the data available to produce and evaluate solutions to the problem.\n3. Select technology to use for solving the problem.\n4. Implement a solution on the form of a demonstrator, prototype or product.\n5. Mutual transfer of knowledge between the research team and the client.\nThe above steps are usually carried out in an iterative fashion in that, e.g., the problem speciﬁcation\nis re-visited and updated as the knowledge of the data available is increased, the transfer of knowledge\nis omnipresent in all stages of the project, and the technology choices and implementation are iteratively\nupdated and tested as the project progresses.\nAlthough the Data Readiness Levels described in the previous section permeate all stages of the project\nmanagement, steps 1 through 3 in the project structure above are where the data readiness is usually\naddressed in depth. Thus, it is crucial that the Data Readiness Levels are in order at the beginning of the\nproject.\nCommon data science, and analytics project processes include Cross-industry standard process for\ndata mining (CRISP-DM, Shearer (2000)), and The Team Data Science Process (TDSP, Microsoft\n(2020)).\n4\nQuestions for guidance\nThe questions below are intended to serve as guidance in the process of reaching the appropriate Data\nReadiness Levels for solving a research or business problem related to NLP.\n4.1\nWhat problem are you trying to solve?\nThe ﬁrst step in every applied project is to make sure that there is a clear and concise deﬁnition of done;\nwhat is the goal of the project, and how do we know when we have reached it? We recommend that the\ngoal is intimately tied to a tangible business value, and that there is a clear idea already from the start how\nthe project ﬁts into the business value chain. To put the point succinctly: if the project is not motivated by\na clear business need, it will not deliver any business value. When specifying the problem, make sure to\nfocus on the actual business need that is to be fulﬁlled, as opposed to a technology to solve the problem.\nTry to make all assumptions about the business need explicit; write them down, and make sure to vet the\nassumptions thoroughly with the stakeholders to have them sign-off on a common understanding of the\nproblem. As with everything else, you should expect to iterate on the speciﬁcation of the problem, and\nas the process proceeds, the adjustments to the speciﬁcation will become smaller.\nHaving a detailed and agreed-on speciﬁcation of the business need and problem is crucial since seem-\ningly subtle details may have a large impact on the space of possible techniques to actually solve the\nproblem. For instance, if the stakeholder is expecting an implemented solution to extract, rank, and\npresent passages from long documents with sub-second latency to a user, from a data point-of-view you\nhave to know whether the document set is expected to change continuously, what format the documents\nare in, and what information there are to guide the learning-to-rank process. The characteristics of the\nproblem you set out to solve dictates the requirements on the data needed to solve it. Thus, the speciﬁ-\ncation of the problem is crucial for understanding the requirements on the data in terms of, e.g., training\ndata, and the need for manual labelling of evaluation or validation data. Only when you know the char-\nacteristics of the data, it will be possible to come up with a candidate technological approach to solve the\nproblem.\n4.2\nWhat data are available to you?\nWhat does ”available to you” mean? There is a difference between knowing that there is data to be\nhad, and actually having the appropriate access to the right data at the right time. The Data Readiness\nLevels framework provides a good way of talking to stakeholders, both data owners and business problem\nowners. Being ”available” in this context, means that the data is at Band B or above.\nWith data at Band B, you will most likely be able to form working hypotheses about the suitability\nof the data with respect to the problem you are trying to solve. For example to gauge whether the data\ncontains the information you need; the amount of data is sufﬁcient; or there are any legal constraints for\nusing the data.\n4.3\nAre you allowed to use the data available?\nThe legal aspects of using the data depend on many things. As such, the legalities should be considered a\nprimary citizen of the Data Readiness Level assessment with respect to your particular challenge. Make\nsure you involve the appropriate legal competence early on in your project. Matters regarding, e.g.,\npersonal identiﬁable information, and GDPR have to be handled correctly. Failing to do so may result in\na project failure, even though all technical aspects of the project are perfectly sound.\n4.4\nWhat data do you need to solve the problem?\nGiven the insight into what data is available, ask yourself the questions:\n• What data do you need to solve the problem?\n• Is that a subset of the data that is already available to you?\n• If not: is there a way of getting all the data you need?\nIf there is a discrepancy between the data available, and the data required to solve the problem, that\ndiscrepancy has to be mitigated. If it is not possible to align the data available with what is needed, then\nthis is a cue to go back to the drawing board and either iterate on the problem speciﬁcation, or collect\nsuitable data. Perhaps there are other ways to solve the business or research need than what has been\nspeciﬁed so far? It is not uncommon to discover that the data actually needed to solve a problem does not\neven exist, despite initial assumptions to the contrary. Such a conclusion need not be entirely negative,\nsince it opens up the possibility to initialize a data collection effort, or, if needed, to reﬁne business\nprocesses to start generating relevant data.\n4.5\nHow do you know if you have succeeded in solving the problem?\nWhen you are in the process of deﬁning and specifying the problem to solve, you should also consider\nhow to evaluate the potential solutions to the problem.\nThe type of data required to evaluate a solution is often tightly connected to the way the solution is\nimplemented: if the solution is based on supervised machine learning, i.e., requiring labelled examples,\nthen the evaluation of the solution will also require labelled data. For example, if the problem you face\nis to help users to ﬁnd topically relevant sections in a large number of yearly reports submitted by public\nagencies, then you will most likely need to construct a collection of sections labelled with the appropriate\ntopic descriptors. Such data can then be used to assess and compare the technical solutions you come up\nwith.\nOn the other hand, if it is possible to produce a solution based on unsupervised machine learning,\nthen perhaps it is possible to conduct the evaluation based on unlabelled data too (although it is far from\ncertain).\nIn any case, if the solution depends on labelled training data, the process of annotation usually also\nresults in the appropriate evaluation data. Any annotation effort should take into account:\n• The quality of the annotations. The agreement between the annotators working on the data, aka\nthe inter-annotator agreement, provides a good starting point for assessing the quality of the an-\nnotations overall. The reasons for low inter-annotator agreement can be related to, e.g., unclear\nannotation guidelines, difference in expertise among the annotators, that the task is simply too hard,\nor a combination of all of the above. The annotations produced should be carefully monitored with\nrespect their quality. Deviations in quality over time should be analyzed so as to facilitate mitigation\nof a potential decrease in the capabilities of the model that relies on the annotated data for training.\n• Temporal aspects of the data characteristics. How often do the distribution of the data to learn\nfrom change, i.e., how often do we need to produce new annotations? When do we know that we\nneed newly annotated data?\n• Representativity of the data. Is the data annotated really suitable for the task at hand? Does it\nreﬂect the way users interact with the system?\nApart from the more quantitative approach to evaluation facilitated by the use of annotated test data,\nthe qualitative aspects of evaluation should also be considered. Qualitative evaluation can take on the\nform of user acceptance tests, and surveys for identifying, e.g., nonsensical output from a model, or to\nget a read on the reduction of cognitive load that a user of a system experiences. Furthermore, qualitative\nevaluation might help identifying issues with missing data that, if it present, would help increase the\nperformance of the model.\nObtaining the training, evaluation, and validation data is at the core of producing a machine learning-\nbased solution to a problem. The quality of the data sets the upper bound to what can be achieved by the\nlearned functionality. Also included in the production process are issues such as model selection, setting\nup infrastructure for machine learning, continuously monitoring the solution’s performance for decrease\nin performance, etc. A good overview of the end-to-end process is presented by Ameisen (2020).\n4.6\nHow does your organization store new data?\nEven if the data processing in your organization is not perfect with respect to the requirements of machine\nlearning, each project you pursue has the opportunity to articulate improvements to your organization’s\ndata storage processes. Ask yourself the questions: How does my organization store incoming data? Is\nthat process a good ﬁt for automatic processing of the data in the context of an NLP project, that is, is\nthe data stored on a format that brings it beyond Band C (accessibility) of the Data Readiness Levels? If\nnot; what changes would need to be made to make the storage better?\nA couple of things we have found important over the course of multiple projects are related to the\nformat in which the data is stored. In particular:\n• Information in the data should not be removed prior to storing it. Destructive processing, such\nas tokenization, stemming, and downcasing of text should not be carried out as a part of the data\nstoring process. Do not conﬂate the intended usage of the data for a particular use case with the\nstorage format; make as few assumptions about how the data will be used in the future as possible.\n• Avoid proprietary formats, and formats not intended for automatic processing. Document\nformats output by regular word processing software, for instance PDF, Word, and Pages, are not\nappropriate formats for input to automatic, machine learning-based processing of information. The\nchallenges of converting, e.g., a PDF ﬁle to a textual format suitable for use in a processing pipeline\nare many. There are currently no general and stable programmatic solution for avoiding:\n– the omission of information, e.g., erroneously broken up words;\n– the introduction of superﬂuous information, e.g., insertion of page headers as part of the text;\n– character encoding issues;\n– the dispersion of information from tables into running text;\n– the mix up of the order of paragraphs or columns, or;\n– a number of additional challenges, as described by Panait (2020).\n• Logical structure. If possible, make sure the logical structure of a document is made accessible\nupon retrieval. That is, ensure that relevant parts of a document are possible to refer to by their\nfunction, enabling document segmentation queries such as:\n– Give me the table of contents of this document.\n– List, in order, all top level subsections and their textual contents from Chapter 3 in this book.\n– Extract all tables from this yearly report.\nFailing to address the above issues may result in your data never making it past Data Readiness Level\nBand C, and thus not be appropriate for automatic analysis.\n5\nHow to contribute\nThe NLP data readiness document is work-in-progress. If you have any questions, suggestions for edits,\nor other input, please email the author or submit a pull request in the document’s GitHub repository,\navailable at: https://github.com/fredriko/nlp-data-readiness.\nGeneral step-by-step instructions for how to contribute to create a pull request is descibed by, e.g,\nData School (2020).\n6\nAcknowledgements\nComments and input on the text were kindly probided by Fehmi ben Abdesslem, Fredrik Carlsson,\nAnders Arpteg, and Armin Catovic.\nReferences\nEmmanuel Ameisen. 2020. Building Machine Learning Powered Applications. O’Reilly Media, Inc.\nJim Banke. 2017. Technology Readiness Levels Demystiﬁed. URL: https://www.nasa.gov/topics/\naeronautics/features/trl_demystified.html. Accessed: 2020-08-31.\nData School. 2020. Step-by-step guide to contributing on GitHub. URL: https://www.dataschool.io/\nhow-to-contribute-on-github/. Accessed: 2020-09-01.\nHugh Harvey and Ben Glocker. 2019. A standardised approach for preparing imaging data for machine learning\ntasks in radiology. In Artiﬁcial Intelligence in Medical Imaging, pages 61–72. Springer.\nNeil D Lawrence. 2017. Data readiness levels. arXiv preprint arXiv:1705.02245.\nMicrosoft.\n2020.\nWhat is the Team Data Science Process?\nURL: https://docs.microsoft.com/\nen-us/azure/machine-learning/team-data-science-process/overview.\nAccessed:\n2020-09-01.\nAlfredo Nazabal, Christopher KI Williams, Giovanni Colavizza, Camila Rangel Smith, and Angus Williams.\n2020. Data Engineering for Data Analytics: A Classiﬁcation of the Issues, and Case Studies. arXiv preprint\narXiv:2004.12929.\nBogdan Panait.\n2020.\nWhat’s so hard about PDF text extraction?\nURL: https://filingdb.com/b/\npdf-text-extraction. Accessed: 2020-09-01.\nColin Shearer. 2000. The CRISP-DM Model: The New Blueprint for Data Mining. Journal of Data Warehousing,\n5(4):13–22.\nPeter MA van Ooijen. 2019. Quality and curation of medical images and data. In Artiﬁcial Intelligence in Medical\nImaging, pages 247–255. Springer.\n",
  "categories": [
    "cs.CY",
    "cs.AI",
    "cs.CL",
    "cs.DB",
    "cs.LG"
  ],
  "published": "2020-09-04",
  "updated": "2020-09-30"
}