{
  "id": "http://arxiv.org/abs/2009.02174v1",
  "title": "Improving Self-Organizing Maps with Unsupervised Feature Extraction",
  "authors": [
    "Lyes Khacef",
    "Laurent Rodriguez",
    "Benoit Miramond"
  ],
  "abstract": "The Self-Organizing Map (SOM) is a brain-inspired neural model that is very\npromising for unsupervised learning, especially in embedded applications.\nHowever, it is unable to learn efficient prototypes when dealing with complex\ndatasets. We propose in this work to improve the SOM performance by using\nextracted features instead of raw data. We conduct a comparative study on the\nSOM classification accuracy with unsupervised feature extraction using two\ndifferent approaches: a machine learning approach with Sparse Convolutional\nAuto-Encoders using gradient-based learning, and a neuroscience approach with\nSpiking Neural Networks using Spike Timing Dependant Plasticity learning. The\nSOM is trained on the extracted features, then very few labeled samples are\nused to label the neurons with their corresponding class. We investigate the\nimpact of the feature maps, the SOM size and the labeled subset size on the\nclassification accuracy using the different feature extraction methods. We\nimprove the SOM classification by +6.09\\% and reach state-of-the-art\nperformance on unsupervised image classification.",
  "text": "Improving Self-Organizing Maps with\nUnsupervised Feature Extraction\nLyes Khacef, Laurent Rodriguez, and Benoˆıt Miramond\nUniversit´e Cˆote d’Azur, CNRS, LEAT, France\nfirstname.lastname@univ-cotedazur.fr\nAbstract. The Self-Organizing Map (SOM) is a brain-inspired neural\nmodel that is very promising for unsupervised learning, especially in em-\nbedded applications. However, it is unable to learn eﬃcient prototypes\nwhen dealing with complex datasets. We propose in this work to im-\nprove the SOM performance by using extracted features instead of raw\ndata. We conduct a comparative study on the SOM classiﬁcation accu-\nracy with unsupervised feature extraction using two diﬀerent approaches:\na machine learning approach with Sparse Convolutional Auto-Encoders\nusing gradient-based learning, and a neuroscience approach with Spiking\nNeural Networks using Spike Timing Dependant Plasticity learning. The\nSOM is trained on the extracted features, then very few labeled samples\nare used to label the neurons with their corresponding class. We inves-\ntigate the impact of the feature maps, the SOM size and the labeled\nsubset size on the classiﬁcation accuracy using the diﬀerent feature ex-\ntraction methods. We improve the SOM classiﬁcation by +6.09% and\nreach state-of-the-art performance on unsupervised image classiﬁcation.\nKeywords: brain-inspired computing · self-organizing map · unsuper-\nvised learning · feature extraction · sparse convolutional auto-encoders ·\nspiking neural networks.\n1\nIntroduction\nWith the fast expansion of Internet of Things (IoT) devices, a huge amount of\nunlabeled data is gathered everyday. While it is a big opportunity for Artiﬁcial\nIntelligence (AI) and Machine Learning (ML), the diﬃcult task of labeling these\ndata makes Deep Learning (DL) techniques slowly reaching the limits of super-\nvised learning [5,8]. Hence, unsupervised learning is becoming one of the most\nimportant and challenging topics in ML. In this work, we use the Self-Organizing\nMap (SOM) proposed by Kohonen [20], an Artiﬁcial Neural Network (ANN)\nthat is very popular in the unsupervised learning category [22]. Inspired from\nthe cortical synaptic plasticity and its self-organization properties, the SOM is\na powerful vector quantization algorithm which models the probability density\nfunction of the data into a set of prototype vectors that are represented by the\nneurons synaptic weights [34]. It has been shown that SOMs perform better in\nrepresenting overlapping structures compared to classical clustering techniques\nsuch as partitive clustering or K-means [3].\narXiv:2009.02174v1  [cs.NE]  4 Sep 2020\n2\nL. Khacef et al.\nIn addition, SOMs are well suited to hardware implementation based on cel-\nlular neuromorphic architectures [15,33,37]. Thanks to a fully distributed archi-\ntecture with local connectivity amongst hardware neurons, the energy-eﬃciency\nof the SOM is highly improved since there is no communication between a cen-\ntralized controller and a shared memory unit, as it is the case in classical Von-\nNeumann architectures. Moreover, the connectivity and computational complex-\nities of the SOM become scalable with respect to the number of neurons [33].\nSOMs are used in a large range of applications [21] going from high-dimensional\ndata analysis to more recent developments such as identiﬁcation of social media\ntrends [36], incremental change detection [28] and energy consumption mini-\nmization on sensor networks [23].\nThis work is an extension of the work done in [14], where we introduced the\nproblem of post-labeled unsupervised learning: no label is available during train-\ning and representations are learned in an unsupervised fashion, then very few\nlabels are available for assigning each representation the class it represents. The\nlatter is called the labeling phase. In [14], we used the MNIST dataset [24] to\ndemonstrate the potential of this unsupervised learning method on the classiﬁca-\ntion problem and compared diﬀerent training and labelling techniques. In order\nto improve the classiﬁcation accuracy of the SOM and be able to work with more\ncomplex datasets, we need to extract useful features from the raw data that will\nthen be classiﬁed with the SOM. In the context of unsupervised learning, fea-\nture extraction can be done using two diﬀerent approaches: a classical ”machine\nlearning approach” using Sparse Convolutional Auto-Encoders (SCAEs), and a\n”neuroscience approach” using Spiking Neural Networks (SNNs). The SCAE is\ntrained using gradient back-propagation while the SNN is trained using Spike\nTiming Dependant Plasticity (STDP). The goal of this work is to compare the\nperformance of both approaches when using a SOM classiﬁer. We also experi-\nment a supervised Convolutional Neural Network (CNN) with the same topology\nfor approximating the best accuracy we can expect from the feature extraction.\nSection 2 describes the unsupervised feature extraction methods and de-\ntails the SOM training and labeling algorithms. Then, Section 3 presents the\nimplementation details of each feature extractor. Next, Section 4 presents the\nexperiments and results on MNIST unsupervised classiﬁcation. Finally, Section\n5 and Section 6 discuss and conclude our work.\n2\nRelated work and methodology\nIn this section, we review the related work and present the proposed methodol-\nogy. We begin with the unsupervised feature extraction learning part, then how\nto train the SOM, and we ﬁnally explain the labeling procedure. Our ﬁrst step\nis to extract relevant features from the raw data using unsupervised learning.\n2.1\nUnsupervised feature extraction\nSparse Convolutional Auto-Encoders (SCAEs) Introduced by Rumelhart,\nHinton and Williams [35], AEs were designed to address the problem of back\nSOM with Unsupervised Feature Extraction\n3\npropagation without supervisor via taking the input data itself as the supervised\nlabel [1]. Today, AEs are typically used for dimensionality reduction or weights\ninitialization in CNNs to improve the classiﬁcation accuracy [26] [19]. In this\nwork, we want to use AEs as feature extractors with unsupervised learning. In\nsuch cases, the feature map representation of a Convolutional AE (CAE) is most\nof the time of a much higher dimensionality than the input image. While this\nfeature representation seems well-suited in a supervised CNN, the overcomplete\nrepresentation becomes problematic in an AE since it gives the autoencoder the\npossibility to simply learn the identity function by having only one weight on\nin the convolutional kernels [26]. Without any further constraints, each convo-\nlutional layer in the AE could easily learn a simple point ﬁlter that copies the\ninput onto a feature map [19]. While this would later simplify a perfect recon-\nstruction of the input, the CAE does not ﬁnd any more suitable representation\nfor our data. To prevent this problem, some constraints have to be applied in\nthe CAE to increase the sparsity of the features representation.\nThe concept of sparsity was introduced in computational neuroscience, as\nsparse representations resemble the behavior of simple cells in the mammalian\nprimary visual cortex, which is believed to have evolved to discover eﬃcient\ncoding strategies [31]. It has been proven that encouraging sparsity when learning\nthe transformed representation can improve the performance of classiﬁcation\ntasks [11]. Indeed, the overcomplete architecture of a CAE allows a larger number\nof hidden units in the code, but this requires that for the given input, most of\nhidden neurons result in very little activation [30]. In a Sparse CAE (SCAE),\nactivations of the encoding layer need to have low values in average. Units in the\nhidden layers usually do not ﬁre [4] so that the few non-zero elements represent\nthe most salient features [30].\nIn order to increase the sparsity of the CAE’s feature representation, several\nmethods can be found in the literature. In [26], the authors use max-pooling to\nenforce the learning of plausible ﬁlters, but the ﬁlters are then ﬁne-tuned with\nsupervised learning for the classiﬁcation. Since we do not want to use any label\nin the training process, we apply additional constraints in the SCAE, namely\nweights and activity constraints of types L2 and L1, respectively [29].\nSpiking Neural Networks (SNNs) Spiking Neural Networks (SNNs) are a\nbrain-inspired family of ANNs used for large-scale simulations in neuroscience\n[10] and eﬃcient hardware implementations for embedded AI [6]. SNNs are char-\nacterized by the spike-based information coding, a computational model of the\nelectrical impulses amongst the biological neurons. The amplitude and duration\nof all spikes are almost the same, so they are mainly characterized by their emis-\nsion time [17]. Furthermore, spiking neurons appear to ﬁre a spike only when\nthey have to send an important message, which leads to the fast and extremely\nenergy-eﬃcient neural computation in the brain.\nMoreover, SNNs have a great potential for unsupervised learning through\nSTDP [7], a biologically plausible local learning mechanism that uses the spike-\ntiming correlation to update the synaptic weights. Kheradpisheh et al. proposed\n4\nL. Khacef et al.\nin [17] a SNN architecture that implements convolutional and pooling layers for\nspike-based unsupervised feature extraction. The SNN processes image inputs as\nfollow. The ﬁrst layer of the network uses Diﬀerence of Gaussians (DoG) ﬁlters\nto detect contrasts in the input image. It encodes the strength of the edges in the\nlatencies of its output spikes, i.e. the higher the contrast, the shorter the latency.\nOn the one hand, neurons in convolutional layers detect complex features by in-\ntegrating input spikes from the previous layer, and emit a spike as soon as they\ndetect their ”preferred” visual feature. A Winner-Take-All (WTA) mechanism\nis implemented so that the neurons that ﬁre earlier perform the STDP learning\nand prevent the others from ﬁring. Hence, more salient and frequent features\ntend to be learned by the network. On the other hand, neurons in the pooling\nlayers provide translation invariance by using a temporal maximum operation,\nand help the network to compress the ﬂow of visual data by propagating the ﬁrst\nspike received from neighboring neurons in the previous layer which are selec-\ntive to the same feature. However, in [17], the extracted features were classiﬁed\nusing a supervised Support Vector Machine (SVM). In this work, we use the\nunsupervised SOM classiﬁer to keep the unsupervised training from end to end.\n2.2\nUnsupervised classiﬁcation with Self-Organizing Maps (SOMs)\nSOM learning The next step consists in training a SOM using the extracted\nfeatures. We use a two-dimensional array of k neurons, that are randomly ini-\ntialized and updated thanks to the following algorithm based on [20]:\nInitialize the network as a two-dimensional array of k neurons, where each\nneuron n with m inputs is deﬁned by a two-dimensional position pn and a\nrandomly initialized m-dimensional weight vector wn.\nfor t from 0 to tf do\nfor every input vector v do\nfor every neuron n in the network do\nCompute the aﬀerent activity an from the distance d:\nd = ∥v −wn∥\n(1)\nan = e−d\nα\n(2)\nend for\nCompute the winner s such that:\nas =\nk−1\nmax\nn=0 (an)\n(3)\nfor every neuron n in the network do\nCompute the neighborhood function hσ(t, n, s):\nhσ(t, n, s) = e\n−∥pn−ps∥2\n2σ(t)2\n(4)\nSOM with Unsupervised Feature Extraction\n5\nUpdate the weight wn of the neuron n:\nwn = wn + ϵ(t) × hσ(t, n, s) × (v −wn)\n(5)\nend for\nend for\nUpdate the learning rate ϵ(t):\nϵ(t) = ϵi\n\u0012ϵf\nϵi\n\u0013t/tf\n(6)\nUpdate the width of the neighborhood σ(t):\nσ(t) = σi\n\u0012σf\nσi\n\u0013t/tf\n(7)\nend for\nIt is to note that tf is the number of epochs, i.e. the number of times the\nwhole training dataset is presented. The α hyper-parameter is the width of the\nGaussian kernel. Its value in Equation 2 is ﬁxed to 1 in the SOM training, but\nit does not have any impact in the training phase since it does not change the\nneuron with the maximum activity. Its value becomes critical though in the\nlabeling process. The SOM hyper-parameters are reported in Section 4.\nSOM labeling The labeling is the step between training and test where we\nassign each neuron the class it represents in the training dataset. We proposed\nin [14] a labeling algorithm based on very few labels. We randomly took a labeled\nsubset of the training dataset, and we tried to minimize its size while keeping the\nbest classiﬁcation accuracy. Our study showed that we only need 1% of randomly\ntaken labeled samples from the training dataset for MNIST classiﬁcation. In this\nwork, we will extend the so-called post-labeled unsupervised learning to SOM\nclassiﬁcation with features extracted by diﬀerent means.\nThe labeling algorithm detailed in [14] can be summarized in ﬁve steps. First,\nwe calculate the neurons activations based on the labeled input samples from\nthe euclidean distance following Equation 2, where v is the input vector, wn and\nan are respectively the weights vector and the activity of the neuron n. The pa-\nrameter α is the width of the Gaussian kernel that becomes a hyper-parameter\nfor the method. Second, the Best Matching Unit (BMU), i.e. the neuron with\nthe maximum activity is elected. Third, each neuron accumulates its normalized\nactivation (simple division) with respect to the BMU activity in the correspond-\ning class accumulator, and the three steps are repeated for every sample of the\nlabeling subset. Fourth, each class accumulator is normalized over the number of\nsamples per class. Fifth and ﬁnally, the label of each neuron is chosen according\nto the class accumulator that has the maximum activity. The complete GPU-\nbased source code is available in https://github.com/lyes-khacef/GPU-SOM.\n6\nL. Khacef et al.\n3\nImplementation details\nMNIST [24] is a dataset of 70000 handwritten digits (60000 for training and\n10000 for test) of 28 × 28 pixels. In order to compare the feature extraction\nperformance, we use the following topologies for the two approaches: 28 × 28 ×\n1 −64c5 −Xc5 −p5 for the SCAE and 28 × 28 × 1 −64c5 −p2 −Xc5 −p2 for\nthe SNN, i.e. two convolutional layers of 64 maps and X maps respectively. Each\nuses 5 × 5 kernels followed by a max-pooling layer. The reason for the diﬀerent\npooling mechanisms is explained in Section 3.3. We explore the impact of the\nnumber of features X on the classiﬁcation accuracy.\n3.1\nCNN training\nThe CNN is modeled in TensorFlow/Keras and trained with Adadelta [39]\ngradient-based algorithm for 100 epochs with a learning rate of 1.0. Since the\ngoal is to estimate the maximum accuracy we can expect from each topology,\nthe CNN is trained with the labeled training set by using 10 neurons with a\nSoftmax activation function on top of the last pooling layer. This network is\nnoted as CNN+MLP in the following.\n3.2\nSCAE training\nThe SCAE is also modeled in TensorFlow/Keras and trained using Adadelta [39]\ngradient-based algorithm for 100 epochs with a learning rate of 1.0. However, no\nlabel is used in the training process, as the goal of the SCAE is to reconstruct the\ninput in the output. The complete SCAE topology is 28×28×1−64c5−Xc5−p5−\nu5 −64d5 −1d5, where u stands for up-sampling and d stands for deconvolution\n(or transposed convolution) layers. The complete architecture is thus symetric.\nWe add to every convolution and deconvoltion layer a weight constraint of type\nL2, and we add to the second convolution layer that produces the features an\nactivity constraint of type L1. The weights and activity regularisation rates are\nset to 10−4. Therefore, the objective function of the SCAE takes in account both\nthe image reconstruction and the sparsity constraints.\n3.3\nSNN training\nThe SNN is modeled in SpykeTorch [27], an open-source simulator of convolu-\ntional SNNs based on PyTorch [32]. The SNN is trained with STDP layer by\nlayer, with a diﬀerent pooling mechanism than the CNN and SCAE. Except for\nthe number of feature maps and kernel sizes, we kept the same hyper-parameters\nas the original implementation of [17] that can be found on [27]. Hence, we used\na pooling layer of 2 × 2 after each convolutional layer, with a padding of 1 be-\nfore the second convolutional layer. The threshold of the neurons in the last\nconvolutional layer were set to be inﬁnite so that their ﬁnal potentials can be\nmeasured [17]. Finally, the global pooling neurons compute the maximum po-\ntential at their corresponding receptive ﬁeld and produce the features that will\nSOM with Unsupervised Feature Extraction\n7\nbe used as input for the SOM. Our experimental study showed that the added\npadding and the pooling mechanism proposed in [27] performs better than the\none used in the CNN and SCAE (i.e. no pooling and one polling layer), with a\ngain of 1.43% on the maximum achievable accuracy.\n4\nExperiments and results\nThe SOM training hyper-parameters were found with a grid search: ϵi = 1.0,\nϵf = 0.01, ηi = 10.0, ηf = 0.01, α = 1.0 and the number of epochs is 10.\nFeature maps in the last convolutional layer\nClassification accuracy (%)\n60.00\n70.00\n80.00\n90.00\n100.00\n0\n100\n200\n300\n400\n500\nSOM\nSNN+SOM\nSCAE+SOM\nCNN+SOM\nFig. 1. SOM classiﬁcation accuracy using CNN, SCAE and SNN feature extraction vs.\nnumber of feature maps with 256 SOM neurons and 10% of labels.\nFirst, ﬁgure 1 shows the impact of the number of feature maps in the sec-\nond convolutional layer, using 256 neurons in the SOM and 10% of labels. We\ndeliberately use a large number of labels to avoid any bias due to the labeling\nperformance, and focus on the impact of the feature maps. The accuracy of\nthe CNN+SOM and SCAE+SOM is increasing with respect to the number of\nfeature maps, reaching a maximum at 256 maps. Interestingly, the CNN+SOM\nperforms better with 8 maps (97.56%) than with 16 (97.25%), 32 (97.00%), 64\n(97.26%) or 128 (97.31%) maps. This is due to the tradeoﬀbetween additional\ninformation and additional noise induced by more feature maps according to the\nSOM classiﬁcation. In fact, the CNN+MLP supervised baseline accuracy is in-\ncreasing from 98.7% to 99% when the feature maps increase from 8 to 512. This\nobservation is more pronounced when we look at the SNN+SOM that reaches\na maximum accuracy for 64 maps then drastically decreases with more feature\nmaps. Following the approach of [17], we used a SNN+SVM supervised base-\nline and its accuracy increases from 97% to 98% when the feature maps increase\nfrom 64 to 512. It means that the increasing number of feature maps for the SNN\nproduces noisy features that do not aﬀect the supervised classiﬁcation but do\n8\nL. Khacef et al.\ndecrease the unsupervised classiﬁcation accuracy, because the SOM prototypes\noverlap and become less descriminative. Thus, we choose 256 maps for the CNN\nand SCAE that produce a feature size of 4096, and 64 maps for the SNN that\nproduces feature maps of size 3136. We remark that the SNN features size is\ndiﬀerent from the CNN/SCAE features size, which is due to the to the added\npadding and the diﬀerent pooling mechanism as explained in Section 3.3.\nSOM neurons\nClassification accuracy (%)\n60.00\n70.00\n80.00\n90.00\n100.00\n50\n100\n500\n1000\n5000\n10000\nSOM\nSNN+SOM\nSCAE+SOM\nCNN+SOM\nFig. 2. SOM classiﬁcation accuracy using CNN, SCAE and SNN feature extraction vs.\nnumber of SOM neurons with the optimal topologies and 10% of labels.\nSecond, with the above mentioned topologies, we investigate the impact of\nthe SOM size with 10% of labels, from 16 to 10000 neurons. We see in Figure 2\nthat the accuracy of the four systems is increasing with respect to the number\nof neurons. We notice that the SNN-SOM reaches the same accuracy as the\nSCAE+SOM starting from 1024 neurons. Nevertheless, for the next step of the\nstudy, it is important to keep the same number of neurons. Hence, we have\nchosen the number of neurons for which one of the SCAE+SOM or SNN+SOM\nreaches the maximum accuracy, which is equal to 256 neurons with respect to\nthe SCAE+SOM accuracy.\nThird, using 256 neurons for the SOM, we investigate the impact of the label-\ning subset size in terms of % of the training set. Figure 3 shows that the accuracy\nincreases when the labeled subset increases. Interestingly, the CNN+SOM and\nSCAE+SOM reach their maximum accuracy with only 1% of labeled data, while\nthe SNN+SOM and SOM need approximately 5% of labeled data. Since the\nSCAE+SOM performs better than the SNN+SOM, we only need 1% of labeled\ndata. It conﬁrms the results obtained in [14].\nFinally, the comparative study of the four settings with the best topology\nof each, using 256 neurons for the SOM and 1% of labeled data for the neu-\nrons labeling is summarized in Figure 4. As expected, the SOM without feature\nextraction has the worst accuracy of 90.91% ± 0.15 and the CNN+SOM with\nSOM with Unsupervised Feature Extraction\n9\nLabeled samples (% of the training subset)\nClassification accuracy (%)\n60.00\n70.00\n80.00\n90.00\n100.00\n0.1\n0.5\n1\n5\n10\nSOM\nSNN+SOM\nSCAE+SOM\nCNN+SOM\nFig. 3. SOM classiﬁcation accuracy using CNN, SCAE and SNN feature extraction vs.\n% of labeled data from the training subset for the neurons labeling with the optimal\ntopologies and 256 SOM neurons.\nClassification accuracy (%)\n80.00\n85.00\n90.00\n95.00\n100.00\nSOM\nSNN+SOM\nSCAE+SOM\nCNN+SOM\nFig. 4. SOM classiﬁcation accuracy using CNN, SCAE and SNN feature extraction vs.\nsummary of the comparative study with the optimal topologies, 256 SOM neurons and\n1% of labels.\nsupervised feature extraction reaches the best accuracy of 97.94% ± 0.22. More\ninterestingly, with fully unsupervised learning, the SCAE performs better than\nthe SNN (+1.53%), with 96.9% ± 0.24 and 95.37% ± 0.58 respectively.\n5\nDiscussion\nTable 1 shows the gap between supervised and unsupervised methods for fea-\nture extraction and classiﬁcation. Interestingly, we only lose about 1% of accu-\nracy when going from CNN+MLP to CNN+SOM, and another 1% when going\n10\nL. Khacef et al.\nTable 1. Comparison of unsupervised feature extraction and classiﬁcation techniques\nin terms of accuracy and hardware cost.\nFeature extraction\nClassiﬁcation\nPerformance\nModel\nLearning\nModel\nLearning\nAccuracy (%) Error (%) Hardware cost\nCNN\nSupervised\nMLP\nSupervised\n99.00\n1.00\nHigh\nCNN\nSupervised\nSOM Unsupervised\n97.94\n2.06\nMedium\nSCAE Unsupervised SOM Unsupervised\n96.90\n3.10\nMedium\nSNN\nUnsupervised SOM Unsupervised\n95.37\n4.63\nLow\nfrom CNN+SOM to SCAE+SOM. The gap is slightly higher when going from\nSCAE+SOM to SNN+SOM, which is about 1.5%. In return, the hardware cost\ndecreases when using SOMs and SNNs, thanks to the brain-inspired computing\nparadigm (distributed and local). Indeed, we showed in [13] that the SNN has a\ngain of approximately 50% in hardware resources and power consumption when\nimplemented in dedicated FPGA and ASIC hardware.\nTable 2. MNIST unsupervised learning with AE-based feature extraction: state of the\nart reported from [12].\nMethod\nAccuracy (%)\nAE + K-means [2]\n81.2\nSparse AE + K-means [30]\n82.7\nDenoising AE + K-means [38]\n83.2\nVariational Bayes AE + K-means [18]\n83.2\nSWWAE + K-means [40]\n82.5\nAdversarial AE [25]\n95.9\nSparse CAE + SOM [Our work]\n96.9\nOverall, the SCAE+SOM reaches the best accuracy of 96.9% ± 0.24 on\nMNIST classiﬁcation with unsupervised learning. As shown in Table 2, we achieved\nstate of the art accuracy compared to similar works that followed an AE-based\napproach. The sparsity constraints of the SCAE through the weights and ac-\ntivities regularization signiﬁcantly improved the SOM classiﬁcation accuracy.\nIndeed, without these constraints, the CAE+SOM with the same conﬁguration\nachieves an accuracy of 94.9% ± 0.24, which means a loss of −2%.\nA similar study was conducted in [9], but it was limited to one layer SCAE\nand SNN, and a supervised SVM was used for classiﬁcation. The authors con-\ncluded that the SCAE reaches a better classiﬁcation accuracy. Our study extands\ntheir ﬁnding to multiple convolutional layers by using unsupervised learning for\nboth feature extraction and classiﬁcation. Nevertheless, the SNN+SOM remains\nattractive due to the hardware-eﬃcient computation of spiking neurons [13] as-\nsociated to the cellular neuromorphic architecture of the SOM [33].\nSOM with Unsupervised Feature Extraction\n11\n6\nConclusion and further works\nIn the context of unsupervised learning, we conducted a comparative study for\nunsupervised feature extraction, and concluded that the SCAE+SOM achieves a\nbetter accuracy thanks to the sparsity constraints that were applied to the SCAE\nthrough weights and activities regularization. However, the SNN+SOM remains\ninteresting due to the hardware eﬃciency of spiking neurons. We achieved state\nof the art performance on MNIST unsupervised classiﬁcation, using post-labeled\nunsupervised learning with the SOM. The future works will focus on using the\nfeature extraction on more complex datasets to improve the accuracy of a mul-\ntimodal unsupervised learning mechanism [16] based on SOMs.\nAcknowledgment\nThis material is based upon work supported by the French National Research\nAgency (ANR) and the Swiss National Science Foundation (SNSF) through\nSOMA project ANR-17-CE24-0036.\nReferences\n1. Baldi, P.: Autoencoders, unsupervised learning, and deep architectures. In: Guyon,\nI., Dror, G., Lemaire, V., Taylor, G., Silver, D. (eds.) Proceedings of ICML Work-\nshop on Unsupervised and Transfer Learning. Proceedings of Machine Learning\nResearch, vol. 27, pp. 37–49. PMLR, Bellevue, Washington, USA (02 Jul 2012),\nhttp://proceedings.mlr.press/v27/baldi12a.html\n2. Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: Greedy layer-wise training\nof deep networks. In: Proceedings of the 19th International Conference on Neural\nInformation Processing Systems. p. 153160. NIPS06, MIT Press, Cambridge, MA,\nUSA (2006)\n3. Budayan, C., Dikmen, I., Birgonul, M.T.: Comparing the performance of tradi-\ntional cluster analysis, self-organizing maps and fuzzy c-means method for strategic\ngrouping. Expert Systems with Applications 36(9), 11772 – 11781 (2009)\n4. Charte,\nD.,\nCharte,\nF.,\nGarca,\nS.,\n[del\nJesus],\nM.J.,\nHerrera,\nF.:\nA\npractical\ntutorial\non\nautoencoders\nfor\nnonlinear\nfeature\nfusion:\nTaxon-\nomy,\nmodels,\nsoftware\nand\nguidelines.\nInformation\nFusion\n44,\n78\n–\n96\n(2018). https://doi.org/https://doi.org/10.1016/j.inﬀus.2017.12.007, http://www.\nsciencedirect.com/science/article/pii/S1566253517307844\n5. Chum, L., Subramanian, A., Balasubramanian, V.N., Jawahar, C.V.: Beyond su-\npervised learning: A computer vision perspective. Journal of the Indian Institute\nof Science 99(2), 177–199 (Jun 2019)\n6. Davies, M., Srinivasa, N., Lin, T., Chinya, G., Cao, Y., Choday, S.H., Dimou, G.,\nJoshi, P., Imam, N., Jain, S., Liao, Y., Lin, C., Lines, A., Liu, R., Mathaikutty,\nD., McCoy, S., Paul, A., Tse, J., Venkataramanan, G., Weng, Y., Wild, A., Yang,\nY., Wang, H.: Loihi: A neuromorphic manycore processor with on-chip learning.\nIEEE Micro 38(1), 82–99 (2018)\n7. Diehl, P., Cook, M.: Unsupervised learning of digit recognition using spike-timing-\ndependent plasticity. Frontiers in Computational Neuroscience 9,\n99 (2015).\nhttps://doi.org/10.3389/fncom.2015.00099\n12\nL. Khacef et al.\n8. Droniou, A., Ivaldi, S., Sigaud, O.: Deep unsupervised network for multimodal per-\nception, representation and classiﬁcation. Robotics and Autonomous Systems 71,\n83 – 98 (2015). https://doi.org/https://doi.org/10.1016/j.robot.2014.11.005, http:\n//www.sciencedirect.com/science/article/pii/S0921889014002474, emerging\nSpatial Competences: From Machine Perception to Sensorimotor Intelligence\n9. Falez, P., Tirilly, P., Bilasco, I.M., Devienne, P., Boulet, P.: Unsupervised vi-\nsual feature learning with spike-timing-dependent plasticity: How far are we\nfrom traditional feature learning approaches? Pattern Recognition 93, 418 –\n429 (2019). https://doi.org/https://doi.org/10.1016/j.patcog.2019.04.016, http:\n//www.sciencedirect.com/science/article/pii/S0031320319301621\n10. Furber, S.B., Galluppi, F., Temple, S., Plana, L.A.: The spinnaker project. Pro-\nceedings of the IEEE 102(5), 652–665 (2014)\n11. Hoyer, P.O.: Non-negative matrix factorization with sparseness constraints. J.\nMach. Learn. Res. 5, 14571469 (Dec 2004)\n12. Ji, X., Vedaldi, A., Henriques, J.F.: Invariant information clustering for unsuper-\nvised image classiﬁcation and segmentation. 2019 IEEE/CVF International Con-\nference on Computer Vision (ICCV) pp. 9864–9873 (2018)\n13. Khacef,\nL.,\nAbderrahmane,\nN.,\nMiramond,\nB.:\nConfronting\nmachine-\nlearning\nwith\nneuroscience\nfor\nneuromorphic\narchitectures\ndesign.\nIn:\n2018 International Joint Conference on Neural Networks (IJCNN) (2018).\nhttps://doi.org/10.1109/IJCNN.2018.8489241\n14. Khacef, L., Miramond, B., Barrientos, D., Upegui, A.: Self-organizing neu-\nrons:\ntoward\nbrain-inspired\nunsupervised\nlearning.\nIn:\n2019\nInternational\nJoint\nConference\non\nNeural\nNetworks\n(IJCNN).\npp.\n1–9\n(July\n2019).\nhttps://doi.org/10.1109/IJCNN.2019.8852098\n15. Khacef, L., Girau, B., Rougier, N.P., Upegui, A., Miramond, B.: Neuromorphic\nhardware as a self-organizing computing system. In: IJCNN 2018 Neuromorphic\nHardware In Practice and Use workshop. Rio de Janeiro, Brazil (2018)\n16. Khacef, L., Rodriguez, L., Miramond, B.: Brain-inspired self-organization with\ncellular neuromorphic computing for multimodal unsupervised learning (2020)\n17. Kheradpisheh, S.R., Ganjtabesh, M., Thorpe, S.J., Masquelier, T.: Stdp-based\nspiking deep convolutional neural networks for object recognition. Neural Networks\n99, 56 – 67 (2018). https://doi.org/https://doi.org/10.1016/j.neunet.2017.12.005,\nhttp://www.sciencedirect.com/science/article/pii/S0893608017302903\n18. Kingma, D.P., Welling, M.: Auto-encoding variational bayes (2013)\n19. Kohlbrenner, M.: Pre-training cnns using convolutional autoencoders (2017)\n20. Kohonen, T.: The self-organizing map. Proceedings of the IEEE 78(9), 1464–1480\n(1990). https://doi.org/10.1109/5.58325\n21. Kohonen, T., Oja, E., Simula, O., Visa, A., Kangas, J.: Engineering applications\nof the self-organizing map. Proceedings of the IEEE 84(10), 1358–1384 (1996).\nhttps://doi.org/10.1109/5.537105\n22. Kohonen, T., Schroeder, M.R., Huang, T.S. (eds.): Self-Organizing Maps. Springer-\nVerlag, Berlin, Heidelberg, 3rd edn. (2001)\n23. Kromes, R., Russo, A., Miramond, B., Verdier, F.: Energy consumption minimiza-\ntion on lorawan sensor network by using an artiﬁcial neural network based appli-\ncation. In: 2019 IEEE Sensors Applications Symposium (SAS). pp. 1–6 (March\n2019). https://doi.org/10.1109/SAS.2019.8705992\n24. LeCun,\nY.,\nCortes,\nC.:\nMNIST\nhandwritten\ndigit\ndatabase.\nhttp://yann.lecun.com/exdb/mnist/ (1998)\n25. Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., Frey, B.: Adversarial autoen-\ncoders (2015)\nSOM with Unsupervised Feature Extraction\n13\n26. Masci, J., Meier, U., Cire¸san, D., Schmidhuber, J.: Stacked convolutional auto-\nencoders for hierarchical feature extraction. In: Proceedings of the 21th Inter-\nnational Conference on Artiﬁcial Neural Networks - Volume Part I. p. 5259.\nICANN11, Springer-Verlag, Berlin, Heidelberg (2011)\n27. Mozafari, M., Ganjtabesh, M., Nowzari-Dalini, A., Masquelier, T.: Spyke-\ntorch: Eﬃcient simulation of convolutional spiking neural networks with at\nmost\none\nspike\nper\nneuron.\nFrontiers\nin\nNeuroscience\n13,\n625\n(2019).\nhttps://doi.org/10.3389/fnins.2019.00625\n28. Nallaperuma, D., Silva, D.D., Alahakoon, D., Yu, X.: Intelligent detection of driver\nbehavior changes for eﬀective coordination between autonomous and human driven\nvehicles. IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics\nSociety pp. 3120–3125 (2018)\n29. Nan Jiang, Wenge Rong, Baolin Peng, Yifan Nie, Zhang Xiong: An empirical anal-\nysis of diﬀerent sparse penalties for autoencoder in unsupervised feature learning.\nIn: 2015 International Joint Conference on Neural Networks (IJCNN). pp. 1–8\n(July 2015). https://doi.org/10.1109/IJCNN.2015.7280568\n30. Ng,\nA.:\nSparse\nautoencoder.\nIn:\nLecture\nnotes\nCS294A.\nStanford\nUni-\nversity.\nStanford,\nCA.\n(2011),\nhttps://web.stanford.edu/class/cs294a/\nsparseAutoencoder.pdf\n31. Olshausen, B.A., Field, D.J.: Sparse coding with an overcomplete basis set: A\nstrategy employed by v1? Vision Research 37(23), 3311 – 3325 (1997)\n32. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,\nLin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,\nRaison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:\nPytorch: An imperative style, high-performance deep learning library. In: Wallach,\nH., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., Garnett, R. (eds.)\nAdvances in Neural Information Processing Systems 32, pp. 8024–8035. Curran\nAssociates, Inc. (2019)\n33. Rodriguez, L., Khacef, L., Miramond, B.: A distributed cellular approach of large\nscale SOM models for hardware implementation. In: IEEE Image Processing and\nSignals. Sophia-Antipolis, France (2018)\n34. Rougier, N., Boniface, Y.: Dynamic self-organising map. Neurocomputing, Elsevier\n74(11), 1840–1847 (May 2011). https://doi.org/10.1016/j.neucom.2010.06.034\n35. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning Internal Representations\nby Error Propagation, p. 673695. MIT Press, Cambridge, MA, USA (1988)\n36. Silva, D.D., Ranasinghe, W.K.B., Bandaragoda, T.R., Adikari, A., Mills, N., Id-\ndamalgoda, L., Alahakoon, D., Lawrentschuk, N.L., Persad, R., Osipov, E., Gray,\nR., Bolton, D.M.: Machine learning to support social media empowered patients\nin cancer care and cancer treatment decisions. In: PloS one (2018)\n37. de\nAbreu\nde\nSousa,\nM.A.,\nDel-Moral-Hernandez,\nE.:\nAn\nfpga\ndis-\ntributed\nimplementation\nmodel\nfor\nembedded\nsom\nwith\non-line\nlearn-\ning.\nIn:\n2017\nInternational\nJoint\nConference\non\nNeural\nNetworks\n(2017).\nhttps://doi.org/10.1109/IJCNN.2017.7966351\n38. Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.A.: Stacked denois-\ning autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. J. Mach. Learn. Res. 11, 33713408 (Dec 2010)\n39. Zeiler, M.D.: Adadelta: An adaptive learning rate method. CoRR abs/1212.5701\n(2012)\n40. Zhao, J., Mathieu, M., Goroshin, R., LeCun, Y.: Stacked what-where auto-encoders\n(2015)\n",
  "categories": [
    "cs.NE",
    "cs.CV"
  ],
  "published": "2020-09-04",
  "updated": "2020-09-04"
}