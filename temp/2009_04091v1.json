{
  "id": "http://arxiv.org/abs/2009.04091v1",
  "title": "Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised Approach for Feature Embedding",
  "authors": [
    "Binh X. Nguyen",
    "Binh D. Nguyen",
    "Gustavo Carneiro",
    "Erman Tjiputra",
    "Quang D. Tran",
    "Thanh-Toan Do"
  ],
  "abstract": "Unsupervised Deep Distance Metric Learning (UDML) aims to learn sample\nsimilarities in the embedding space from an unlabeled dataset. Traditional UDML\nmethods usually use the triplet loss or pairwise loss which requires the mining\nof positive and negative samples w.r.t. anchor data points. This is, however,\nchallenging in an unsupervised setting as the label information is not\navailable. In this paper, we propose a new UDML method that overcomes that\nchallenge. In particular, we propose to use a deep clustering loss to learn\ncentroids, i.e., pseudo labels, that represent semantic classes. During\nlearning, these centroids are also used to reconstruct the input samples. It\nhence ensures the representativeness of centroids - each centroid represents\nvisually similar samples. Therefore, the centroids give information about\npositive (visually similar) and negative (visually dissimilar) samples. Based\non pseudo labels, we propose a novel unsupervised metric loss which enforces\nthe positive concentration and negative separation of samples in the embedding\nspace. Experimental results on benchmarking datasets show that the proposed\napproach outperforms other UDML methods.",
  "text": "BINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n1\nDeep Metric Learning Meets Deep\nClustering: An Novel Unsupervised\nApproach for Feature Embedding 1\nBinh X. Nguyen1\nbinh.xuan.nguyen@aioz.io\nBinh D. Nguyen1\nbinh.duc.nguyen@aioz.io\nGustavo Carneiro3\ngustavo.carneiro@adelaide.edu.au\nErman Tjiputra1\nerman.tjiputra@aioz.io\nQuang D. Tran1\nquang.tran@aioz.io\nThanh-Toan Do2\nthanh-toan.do@liverpool.ac.uk\n1 AIOZ\nSingapore\n2 University of Liverpool\nUnited Kingdom\n3 University of Adelaide\nAustralia\nAbstract\nUnsupervised Deep Distance Metric Learning (UDML) aims to learn sample simi-\nlarities in the embedding space from an unlabeled dataset. Traditional UDML methods\nusually use the triplet loss or pairwise loss which requires the mining of positive and neg-\native samples w.r.t. anchor data points. This is, however, challenging in an unsupervised\nsetting as the label information is not available. In this paper, we propose a new UDML\nmethod that overcomes that challenge. In particular, we propose to use a deep cluster-\ning loss to learn centroids, i.e., pseudo labels, that represent semantic classes. During\nlearning, these centroids are also used to reconstruct the input samples. It hence ensures\nthe representativeness of centroids — each centroid represents visually similar samples.\nTherefore, the centroids give information about positive (visually similar) and negative\n(visually dissimilar) samples. Based on pseudo labels, we propose a novel unsupervised\nmetric loss which enforces the positive concentration and negative separation of samples\nin the embedding space. Experimental results on benchmarking datasets show that the\nproposed approach outperforms other UDML methods.\n1\nIntroduction\nThe objective of deep distance metric learning (DML) is to train a deep learning model that\nmaps training samples into feature embeddings that are close together for samples that be-\nlong to the same category and far apart for samples from different categories [8, 12, 13, 21,\n1This work was partially supported by the Australian Research Council project FT190100525.\nc⃝2020. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:2009.04091v1  [cs.CV]  9 Sep 2020\n2\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n26, 36, 37, 38, 40, 43, 52, 55]. Traditional DML approaches require supervised informa-\ntion, i.e., class labels, to supervise the training. The class labels are used either for training\nthe pointwise softmax loss [1, 32] or mining positive and negative samples for training the\npairwise or triplet losses [5, 13, 29, 30, 34, 37]. Although the supervised DML achieves\nimpressive results on different tasks [4, 16, 34, 35, 46, 56], it requires large amount of anno-\ntated training samples to train the model. Unfortunately, such large datasets are not always\navailable and they are costly to annotate for speciﬁc domains. That disadvantage also lim-\nits the transferability of supervised DML to new domain/applications which do not have\nlabeled data. These reasons have motivated recent studies aiming at learning feature embed-\ndings without annotated datasets [24, 27, 54] — unsupervised deep distance metric learning\n(UDML). Our study is in that same direction, i.e., learning embeddings from unlabeled data.\nThere are two main challenges for UDML. Firstly, how to deﬁne positive and negative\nsamples for a given anchor data point, such that we can apply distance-based losses, e.g.,\npairwise loss or triplet loss, in the embedding space. Secondly, how to make the training\nefﬁcient, given a large number of pairs or triplets of samples, in the order of O(N2) or\nO(N3), respectively, in which N is the number of training samples.\nIn this paper, we\npropose a new method that utilizes deep clustering for deep metric learning to address the\ntwo challenges mentioned above.\nRegarding the ﬁrst challenge, given an anchor point, its transformed versions (obtained,\nfor example, from data augmentation methods) can be used as positive samples [10, 54].\nAlternatively, the Euclidean nearest neighbors from pre-trained features can be used for se-\nlecting positive samples [11]. The problem is more challenging for mining negative samples.\nIn [18], the authors rely on manifold mining which uses the pre-trained features of samples\nto build a similarity graph and the mining is performed on the graph. Hence, this approach\nheavily depends on pre-trained features. In addition, building a full adjacency matrix for\ngraph mining is costly, especially for large scale training datasets. Recently, in [54], the au-\nthors consider all other training samples within the same batch of the anchor point as negative\nsamples (w.r.t. the anchor) — this means that no negative mining is performed at all. This\napproach likely contains false negatives, especially when the number of same class samples\nin a batch is large. Different from these approaches, we propose to learn pseudo labels for\nsamples through a deep clustering loss. The pseudo labels are then used for negative mining.\nRegarding the second challenge, due to the large number of pairs or triplets w.r.t. all\ntraining samples, traditional approaches reduce the space by only considering O(m2) pairs\nor O(m3) triplets within each training batch containing m samples. In our approach, the\nreliance on the pseudo labels allows us to leverage the idea of center loss [7, 51] which\nresults in a training complexity of O(Km) for each batch, in which K is the number of the\nlearned centroids (i.e., pseudo labels).\nOur main contributions can be summarized as follows. (i) We propose an novel UDML\napproach with a novel loss function that consists of a deep clustering loss for learning pseudo\nclass labels, i.e., clusters, where each cluster is represented by a centroid. (ii) To enhance the\nrepresentativeness of centroids, the loss also contains a reconstruction loss term that penal-\nizes high sample reconstruction errors from centroid representations, which will encourage\nvisually similar samples to belong to the same cluster. (iii) Based on pseudo labels, we\npropose a novel center-based loss that encourages samples to be close to their augmented\nversions and far from the centroids from different clusters in the embedding space.\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n3\n2\nRelated Work\nSupervised deep metric learning.\nThe supervised deep metric learning uses the label in-\nformation to supervise training [6, 11, 13, 27, 28, 31, 34, 37, 39, 47, 48, 49, 50]. Generally,\nthe common loss functions used in supervised metric learning can be divided into two types:\nclassiﬁcation-based losses and distance-based losses. The classiﬁcation-based losses focus\non maximizing the probability of correct labels predicted for every sample [6, 31, 47, 48].\nThose methods have linear run-time training complexity O(NC) where N and C < N repre-\nsent the number of training samples and the number of classes, respectively. The distance-\nbased losses [11, 28, 34, 39, 49, 50] focus on learning the similarity or dissimilarity among\nsample embeddings. Two common distance-based losses are the contrastive (pairwise) loss\n[11] and the triplet loss [7, 15]. The run-time training complexity of contrastive loss is O(N2)\nwhile triplet loss is O(N3), in which N is the number of training samples. Hence both losses\nsuffer from slow convergence due to a large number of trivial pairs or triplets as the model\nimproves.\nIn order to overcome the run-time complexity challenge, many studies focused on min-\ning strategies aiming at ﬁnding informative pairs or triplets for the training [13, 27, 34, 37].\nFor example, in [13], the authors propose to use fast approximate nearest neighbor search for\nquickly identifying informative (hard) triplets for training, reducing the training complexity\nto O(N2). In [27], the mining is replaced by the use of P < N proxies, where a triplet is\nre-deﬁned to be an anchor point, a similar and a dissimilar proxy – this reduces the training\ncomplexity to O(NP2). Recently, in [7] the authors propose a center-based metric loss in\nwhich centers are ﬁxed during training and the loss involves the distance calculations be-\ntween feature embeddings and the centers, rather than between features, resulting in O(NC)\ncomplexity, in which C is the number of centers.\nUnsupervised feature learning.\nThe common approach to unsupervised feature learning\nis to learn intermediate features, i.e., latent features, that well represent the input samples.\nRepresentative works are Autoencoder-based methods [33, 44]. Another approach is to lever-\nage the idea of clustering, such as in [17, 19], where features are learned with the objective\nthat they can be clustered into balanced clusters and the clusters are well separated. In [3],\nthe authors propose to jointly learn the parameters of a deep neural network and the clus-\nter assignments of the resulting features. More precisely, the method iterates between the\nk-means clustering of the features produced by the network and the updating of network\nweights by predicting the cluster assignments as pseudo labels using a discriminative loss.\nAnother popular approach to unsupervised feature learning is to replace the labels anno-\ntated by humans by labels generated from the original data with data augmentation methods.\nFor example, in [9] the authors train a deep network to discriminate between a set of sur-\nrogate classes. Each surrogate class is formed by applying a variety of transformations to a\nrandomly sampled image patch. In [23], the authors train a deep network such that it min-\nimizes the distance between the features that describe a reference image and their rotated\nversions. Similarly, in [10], the authors train a convnet to predict the rotation that is applied\nto the input image.\nUnsupervised deep metric learning.\nThe unsupervised deep metric learning can be con-\nsidered as a special case of unsupervised feature learning. In this case, the networks are\ntrained to produce deep embedding features from unlabeled data. Furthermore, the learned\n4\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\nFigure 1: Illustration of the proposed framework which consists of an encoder (G), an embedding\nmodule (F), a decoder (D) and three losses, i.e., clustering loss Lrim, reconstruction loss Lrec and\nmetric loss Lm. The details are presented in the text.\nmodels are expected to generalize well to unseen testing class samples. Regarding this prob-\nlem, there are only few works proposed to tackle the problem. In [18] the authors propose a\nmining strategy using Euclidean neighbors and manifold neighbors to pre-mine positive and\nnegative pairs. The mining process involves building an Euclidean nearest neighbor graph\nfrom pre-computed features, a process that has O(N2) complexity, in which N is the number\nof training samples. This means that the approach is not scalable and also heavily depends on\nthe pre-computed features. To overcome the mining complexity, in [54], the authors propose\na softmax-based embedding which enforces data augmentation invariance. Speciﬁcally, for\neach sample in a batch, its augmentations are considered as the positive samples and other\nsamples are treated as negative ones. During training, the method needs to calculate the pair-\nwise distance between samples in a batch, so its complexity for training a batch is quadratic\nw.r.t. batch size m.\n3\nMethod\nThe proposed framework is presented in Figure 1. For every original image in a batch, we\nmake an augmented version by using a random geometric transformation. Let the number of\ninput images in a batch after augmentation be m. The input images are fed into the backbone\nnetwork which is also considered as the encoder (G – purple) to get image representations\n(red) which have 1024 dimensions. The image representations are passed through the em-\nbedding module which consists of fully connected and L2 normalization layers (F – blue),\nwhich results in unit norm image embeddings with 128 dimensions. The clustering mod-\nule (yellow) takes image embeddings as inputs, performs the clustering with a clustering\nloss (Lrim), and outputs the cluster assignments. Given the cluster assignments, centroid\nrepresentations are computed from image representations, which are then passed through the\ndecoder with a reconstruction loss (Lrec) to reconstruct images that belong to the correspond-\ning clusters. The centroid representations are also passed through the embedding module (F)\nto get centroid embeddings. The centroid embeddings and image embeddings are used as\ninputs for the metric loss (Lm).\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n5\n3.1\nDiscriminative Clustering\nFollowing the idea of Regularized Information Maximization (RIM) for clustering [17, 19],\nwe formulate the clustering of embedding features as a classiﬁcation problem. Given a set\nof embedding features X = {xi}m\ni=1 ∈R128×m in a batch and the number of clusters K ≤m\n(i.e., the number of clusters K is limited by the batch size m), we want to learn a probabilistic\nclassiﬁer, using a softmax activation, that maps each x ∈X into a probabilistic vector y =\nsoftmaxθ(x) that has K dimensions, in which θ is the classiﬁer parameters1. The cluster\nassignment for x then is estimated by c∗= argmaxc y.\nLet Y = {yi}m\ni=1 be the set of softmax outputs for X. Inspired by RIM [17, 19], we use\nthe following objective function for the clustering\nLrim = R(θ)−λ [H(Y)−H(Y|X)],\n(1)\nwhere H(.) and H(.|.) are entropy and conditional entropy, respectively; R(θ) regularizes\nthe classiﬁer parameters (in this work we use l2 regularization); λ is a weighting factor to\ncontrol the importance of two terms. Minimizing (1) is equivalent to maximizing H(Y) and\nminimizing H(Y|X). Increasing the marginal entropy H(Y) encourages cluster balancing,\nwhile decreasing the conditional entropy H(Y|X) encourages cluster separation [2]. Follow-\ning [17] the entropy and the conditional entropy are calculated over the batch as follows\nH(Y) = h\n \n1\nm\nm\n∑\ni=1\nyi\n!\n,\n(2)\nH(Y|X) = 1\nm\nm\n∑\ni=1\nh(yi),\n(3)\nwhere h(yi) ≡−∑K\nj=1 yij logyij is the entropy function.\n3.2\nReconstruction\nIn order to enhance the representativeness of centroids, we introduce a reconstruction loss\nthat penalizes high reconstruction errors from centroids to corresponding samples. Speciﬁ-\ncally, the decoder takes a centroid representation of a cluster and minimizes the difference\nbetween input images that belong to the cluster and the reconstructed image from the centroid\nrepresentation. By jointly training both the clustering loss and the reconstruction loss, we\nexpect that centroids will well represent the samples belonging to the corresponding clusters.\nLet Xj be a set of input images in the batch that belongs to the cluster j. The centroid\nrepresentation for the cluster j is calculated as\nrj =\n1\n|Xj| ∑\nIi∈Xj\nG(Ii),\n(4)\nwhere G(.) is the backbone network (i.e., the encoder), which extracts image representations.\nAfter obtaining the centroid representations, the reconstruction loss function is calculated as\n1In this context, the softmax classiﬁer consists of an FC layer parameterized by θ that projects the feature\nembedding x to a vector with K dimensions, i.e., the logits. Then the softmax function is applied on the logits to\noutput a probabilistic vector.\n6\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\nLrec = 1\nm\nK\n∑\nj=1 ∑\nIi∈Xj\n||Ii −D(r j)||2,\n(5)\nwhere D(.) is the decoder which reconstructs samples in the batch using their corresponding\ncentroid representations and m is the number of images in the batch.\n3.3\nMetric Loss\nFor every original image Ii in a batch, let ˆIi be its augmented version. Let fi ∈R128 and\nˆfi ∈R128 be the image embeddings of Ii and ˆIi, respectively. The proposed metric loss aims\nto minimize the distance between fi and ˆfi while pushing fi far away from negative clusters2.\nGiven the cluster representation rj for cluster j (see Section 3.2), the centroid embedding\ncj is obtained by cj = F(rj) ∈R128, where F(.) is the embedding module. It is worth noting\nthat both the image embedding f and centroid embedding c are unit norm. Let the index\nof the cluster that sample i belongs to be q, 1 ≤q ≤K. In order to minimize the distance\nbetween fi and ˆfi, and maximize the distance between fi and centroids that Ii does not belong\nto, we maximize the following “softmax” like function\nl(Ii, ˆIi) =\nexp(f T\ni ˆfi/τ)\n∑K\nk=1,k̸=q exp(f T\ni ck/τ),\n(6)\nwhere τ is the temperature parameter [14]. Using a higher value for τ produces a softer\nprobability distribution.\nBecause feature embeddings and centroid embeddings are unit norm, maximizing (6)\nminimizes the Euclidean distance between fi and ˆfi and maximizes the Euclidean distances\nbetween fi and the clusters that Ii does not belong to.\nIn addition, to push fi far away from a cluster j that Ii does not belong to, we maximize\nthe following quantity, ∀j ̸= q\nl(Ii,c j) = 1−\nexp(f T\ni cj/τ)\n∑K\nk=1 exp(f T\ni ck/τ).\n(7)\nBy maximizing (7) ∀j ̸= q, we maximize the Euclidean distance between fi and cj while\nminimizing the Euclidean distance between fi and the centroid that Ii belongs to, i.e., cq.\nCombining (6) and (7), we maximize the following quantity for each sample i\nl(i) = l(Ii, ˆIi)\nK\n∏\nj=1,j̸=q\nl(Ii,c j).\n(8)\nApplying the negative log-likelihood to (8) and summing over all training samples in the\nbatch, we minimize the metric loss function which is deﬁned as follows\nLm = −∑\ni\nlog\n\u0000l(Ii, ˆIi)\n\u0001\n−∑\ni\nK\n∑\nj=1,j̸=q\nlog(l(Ii,c j)).\n(9)\n2Hereafter, Ii and ˆIi are interchangeable when calculating losses.\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n7\n3.4\nFinal Loss\nThe network in Figure 1 is trained in an end-to-end manner with the following multi-task\nloss\nL = αLm +βLrim +γLrec,\n(10)\nwhere Lm is the center-based softmax loss for deep metric learning (9), Lrim is the clustering\nloss (1), and Lrec is the reconstruction loss (5).\nComplexity.\nIn our metric loss Lm, we calculate the distance between samples and cen-\ntroids, resulting in a O(Km) complexity. Both clustering loss Lrim and reconstruction loss\nLrec have O(m) complexity. Hence the overall asymptotic complexity for training one batch\nof the proposed method is O(Km).\n4\nExperiments\n4.1\nImplementation Details\nFollowing existing methods [18, 27, 28, 39, 54], the pre-trained GoogLeNet [42] on Ima-\ngeNet is used as the backbone network (G). The embedding module (F) is added after the\nlast average pooling of the GoogLeNet. A softmax layer is used as the clustering module\nto output a probabilistic vector for each image embedding. The decoder (D) is a stack of\ndeconvolutional layers in which each layer is followed by a batch norm layer and ReLU ac-\ntivation. The values of α, β, and γ in (10) are empirically set to 0.9, 0.3, 0.01, respectively.\nWe adopt the SGD optimizer with momentum 0.9. To make a fair comparison to SME [54],\nsimilar to that work, we also use the batchsize 64 (i.e., 128 after augmentation). The value\nof the temperature τ in (6), (7) is set to 0.1.\nFor the data augmentation to create positive\nsamples, the original images are randomly cropped at size 224×224 with random horizontal\nﬂipping which is similar to [18, 54]. In the testing phase, as a standard practice [18], a single\ncenter-cropped image is used as input to extract the image embedding.\n4.2\nDataset and Evaluation Metric\nWe conduct our experiments on two public benchmark datasets that are commonly used\nto evaluate DML methods, where we follow the standard experimental protocol for both\ndatasets [13, 39, 40, 41]. The CUB200-2011 dataset [45] contains 200 species of birds with\n11,788 images, where the ﬁrst 100 species with 5,864 images are used for training and the\nremaining 100 species with 5,924 images are used for testing. The Car196 dataset [20]\ncontains 196 car classes with 16,185 images, where the ﬁrst 98 classes with 8,054 images\nare used for training and the remaining 98 classes with 8,131 images are used for testing. We\nreport the K nearest neighbor retrieval accuracy using the Recall@K metric. We also report\nthe clustering quality using the normalized mutual information (NMI) score [25].\n4.3\nAblation Study\nIn this section, we investigate the impact of each loss term in the proposed method. We also\ninvestigate the effect of the number of centroids in the clustering module which affects the\npseudo labels. Here we consider the work SoftMax Embedding (SME) [54] as the baseline\n8\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\nR@1\nR@2\nR@4\nR@8\nSME [54]\n46.2\n59.0\n70.1\n80.2\nonly Lrim\n40.6\n52.9\n65.7\n77.5\nCBS\n47.3\n59.1\n70.5\n80.2\nCBSwR\n47.5\n59.6\n70.6\n80.5\nTable 1: The impact of each loss component on the performance on CUB200-2011 dataset and the\ncomparison to the baseline [54].\nR@1\nR@2\nR@4\nR@8\nSME [54]\n41.3\n52.3\n63.6\n74.9\nonly Lrim\n35.7\n47.5\n59.8\n71.0\nCBS\n42.2\n54.0\n65.4\n76.0\nCBSwR\n42.6\n54.4\n65.4\n76.0\nTable 2: The impact of each loss component on the performance on Car196 dataset and the comparison\nto the baseline [54].\nDataset\nSME\nonly Lrim\nCBS\nCBSwR\nCUB200-2011\n696\n620\n660\n882\nCar196\n772\n700\n727\n1025\nTable 3: The training time (seconds) of different methods on CUB200-2011 and Car196 datasets with\n20 epochs. The models are trained on a NVIDIA GeForce GTX 1080-Ti GPU.\nmodel. We denote our model that is trained by using only clustering loss (1) as only Lrim and\nour model that is trained with both clustering and the metric losses (1) and (9) as Center-\nbased Softmax (CBS). In this experiment, the number of clusters are ﬁxed to 32 for both\nCUB200-2011 and Car196 datasets. We also investigate the impact of the reconstruction\nin enhancing the representativeness of centroids. This model is denoted as Center-based\nSoftmax with Reconstruction (CBSwR), which is our ﬁnal model.\nTables 1 and 2 present the comparative results between methods. The results show that\nusing only the clustering loss, the accuracy is signiﬁcantly lower than the baseline. How-\never, when using the centroids from the clustering for calculating the metric loss (i.e., CBS),\nit gives the performance boost over the baseline (i.e., SME). Furthermore, the reconstruc-\ntion loss enhances the representativeness of centroids, as conﬁrmed by the improvements of\nCBSwR over CBS on both datasets.\nTable 3 presents the training time of different methods on the CUB200-2011 and Car196\ndatasets. Although the asymptotic complexity of CBSwR for training one batch is O(Km),\nit also consists of a decoder part which affects the real training. It is worth noting that the de-\ncoder is only involved during training. During testing, our method has similar computational\ncomplexity as SME.\nTable 4 presents the impact of the number of clusters K in the clustering loss on the\nCUB200-2011 dataset with our proposed model CBSwR (recall that the number of clusters\nK is limited by the batch size m). During training, the number of samples per clusters vary\ndepending on batches and the number of clusters. At K = 32 which is our ﬁnal setting, the\nnumber of samples per cluster varies from 2 to 11, on the average. The retrieval performance\nis just slightly different for the different number of clusters. This conﬁrms the robustness of\nthe proposed method w.r.t. the number of clusters.\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n9\nNumber of clusters\nR@1\nR@2\nR@4\nR@8\n16\n46.7\n59.0\n70.0\n79.7\n32\n47.5\n59.6\n70.6\n80.5\n48\n47.1\n58.9\n70.6\n80.1\n64\n47.4\n58.9\n69.6\n80.2\nTable 4: The impact of the number of clusters of the ﬁnal model CBSwR on the performance on\nCUB200-2011 dataset.\nNMI\nR@1\nR@2\nR@4\nR@8\nSupervised Learning\nSoftMax\n57.2\n48.3\n60.2\n71.2\n80.3\nN-pair [39]\n57.2\n45.4\n58.4\n69.5\n79.5\nTriplet+smart mining [13]\n59.9\n49.8\n62.3\n74.1\n83.3\nTriplet+proxy [27]\n59.5\n49.2\n61.9\n67.9\n72.4\nHistogram [43]\n-\n50.3\n61.9\n72.6\n82.4\nUnsupervised Learning\nCyclic [22]\n52.6\n40.8\n52.8\n65.1\n76.0\nExemplar [9]\n45.0\n38.2\n50.3\n62.8\n75.0\nNCE [53]\n45.1\n39.2\n51.4\n63.7\n75.8\nDeepCluster [3]\n53.0\n42.9\n54.1\n65.6\n76.2\nMOM [18]\n55.0\n45.3\n57.8\n68.6\n78.4\nSME [54]\n55.4\n46.2\n59.0\n70.1\n80.2\nCBSwR (Ours)\n55.9\n47.5\n59.6\n70.6\n80.5\nTable 5: Clustering and Recall performance on the CUB200-2011 dataset.\nNMI\nR@1\nR@2\nR@4\nR@8\nSupervised Learning\nSoftMax\n58.4\n62.4\n73.0\n80.9\n87.4\nN-pair [39]\n57.8\n53.9\n66.8\n77.8\n86.4\nTriplet+smart mining [13]\n59.5\n64.7\n76.2\n84.2\n90.2\nTriplet+proxy [27]\n64.9\n73.2\n82.4\n86.4\n88.7\nHistogram [43]\n-\n54.3\n66.7\n77.2\n85.2\nUnsupervised Learning\nExemplar [9]\n35.4\n36.5\n48.1\n60.0\n71.0\nNCE [53]\n35.6\n37.5\n48.7\n59.8\n71.5\nDeepCluster [3]\n38.5\n32.6\n43.8\n57.0\n69.5\nMOM [18]\n38.6\n35.5\n48.2\n60.6\n72.4\nSME [54]\n35.8\n41.3\n52.3\n63.6\n74.9\nCBSwR (Ours)\n37.6\n42.6\n54.4\n65.4\n76.0\nTable 6: Clustering and Recall performance on the Car196 dataset.\n4.4\nComparison to the State of the Art\nWe compare our method to the state-of-the-art unsupervised deep metric learning and unsu-\npervised feature learning methods that have reported results on the benchmarking datasets\nCUB200-2011 and CARS196. They are the graph-based method Cyclic [22], Exemplar\nCNN with surrogate classes from image transformations Exemplar [9], feature learning\nwith noise-contrastive estimation NCE [53], alternative feature learning and k-means clus-\n10\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\ntering DeepCluster [3], mining on manifold MOM [18] and SoftmaxEmbedding — SME\n[54]. Among them, MOM [18] and SME [54] are the only methods that claim for unsuper-\nvised deep metric learning. We also compare our method to fully supervised deep metric\nlearning methods, including the softmax loss, the N-pair [39] loss, the histogram loss [43],\nthe triplet with proxies [27] loss, triplet with smart mining [13] loss which uses the fast\nnearest neighbor search for mining triplets.\nTable 5 presents the comparative results on CUB200-2011 dataset. In terms of clustering\nquality (i.e., NMI metric), the proposed method and the state-of-the-art UDML methods\nMOM [18] and SME [54] achieve comparable accuracy. However, in terms of retrieval\naccuracy R@K, our method outperforms other approaches. Our proposed method is also\ncompetitive to most of the supervised DML methods.\nTable 6 presents the comparative results on Car196 dataset. Compared to unsupervised\nmethods, the proposed method outperforms other approaches in terms of retrieval accuracy\nat all ranks of K. Our method is comparable to other unsupervised methods in terms of\nclustering quality.\n5\nConclusion\nWe propose a new method that utilizes deep clustering for deep metric learning to address the\ntwo challenges in UDML, i.e., positive/negative mining and efﬁcient training. The method\nis based on a novel loss that consists of a learnable clustering function, a reconstruction\nfunction, and a center-based metric loss function. Our experiments on CUB200-2011 and\nCar196 datasets show state-of-the-art performance on the retrieval task, compared to other\nunsupervised learning methods.\nReferences\n[1] Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, and Stefan\nCarlsson. From generic to speciﬁc deep representations for visual recognition. In CVPR\nWorkshops, 2015.\n[2] John S Bridle, Anthony JR Heading, and David JC MacKay. Unsupervised classiﬁers,\nmutual information and ‘phantom targets’. In NIPS, 1992.\n[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep cluster-\ning for unsupervised learning of visual features. In ECCV, 2018.\n[4] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss:\na deep quadruplet network for person re-identiﬁcation. In CVPR, 2017.\n[5] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discrimi-\nnatively, with application to face veriﬁcation. In CVPR, 2005.\n[6] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive\nangular margin loss for deep face recognition. In CVPR, 2019.\n[7] Thanh-Toan Do, Toan Tran, Ian Reid, Vijay Kumar, Tuan Hoang, and Gustavo\nCarneiro.\nA theoretically sound upper bound on the triplet loss for improving the\nefﬁciency of deep distance metric learning. In CVPR, 2019.\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n11\n[8] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox.\nDiscriminative unsupervised feature learning with convolutional neural networks. In\nNIPS, 2014.\n[9] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and\nThomas Brox. Discriminative unsupervised feature learning with exemplar convolu-\ntional neural networks. TPAMI, pages 1734–1747, 2015.\n[10] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation\nlearning by predicting image rotations. In ICLR, 2018.\n[11] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning\nan invariant mapping. In CVPR, 2006.\n[12] Xufeng Han, Thomas Leung, Yangqing Jia, Rahul Sukthankar, and Alexander C Berg.\nMatchnet: Unifying feature and metric learning for patch-based matching. In CVPR,\n2015.\n[13] Ben Harwood, B. G. Vijay Kumar, Gustavo Carneiro, Ian D. Reid, and Tom Drum-\nmond. Smart mining for deep metric learning. In ICCV, 2017.\n[14] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a\nneural network. In NIPS Deep Learning Workshop, 2014.\n[15] Elad Hoffer and Nir Ailon. Deep metric learning using triplet network. In International\nWorkshop on Similarity-Based Pattern Recognition, pages 84–92. Springer, 2015.\n[16] Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Discriminative deep metric learning for face\nveriﬁcation in the wild. In CVPR, 2014.\n[17] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama.\nLearning discrete representations via information maximizing self-augmented training.\nIn ICML, 2017.\n[18] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondˇrej Chum. Mining on manifolds:\nMetric learning without labels. In CVPR, 2018.\n[19] Andreas Krause, Pietro Perona, and Ryan G Gomes. Discriminative clustering by reg-\nularized information maximization. In NIPS, 2010.\n[20] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations\nfor ﬁne-grained categorization. In ICCV Workshops, 2013.\n[21] B. G. Vijay Kumar, Gustavo Carneiro, and Ian Reid. Learning local image descrip-\ntors with deep siamese and triplet convolutional networks by minimising global loss\nfunctions. In CVPR, 2016.\n[22] Dong Li, Wei-Chih Hung, Jia-Bin Huang, Shengjin Wang, Narendra Ahuja, and Ming-\nHsuan Yang. Unsupervised visual representation learning by graph-based consistent\nconstraints. In ECCV, 2016.\n[23] Kevin Lin, Jiwen Lu, Chu-Song Chen, and Jie Zhou. Learning compact binary descrip-\ntors with unsupervised deep neural networks. In CVPR, 2016.\n12\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n[24] Chaochao Lu and Xiaoou Tang. Surpassing human-level face veriﬁcation performance\non lfw with gaussianface. In AAAI, 2015.\n[25] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to\nInformation Retrieval. Cambridge University Press, 2008.\n[26] Jonathan Masci, Davide Migliore, Michael M Bronstein, and Jürgen Schmidhuber. De-\nscriptor learning for omnidirectional image matching. In Registration and Recognition\nin Images and Videos, pages 49–62. Springer, 2014.\n[27] Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and\nSaurabh Singh. No fuss distance metric learning using proxies. In ICCV, 2017.\n[28] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning\nvia lifted structured feature embedding. In CVPR, 2016.\n[29] Qi Qian, Rong Jin, Shenghuo Zhu, and Yuanqing Lin. Fine-grained visual categoriza-\ntion via multi-stage metric learning. In CVPR, 2015.\n[30] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. CNN image retrieval learns from\nbow: Unsupervised ﬁne-tuning with hard examples. In ECCV, 2016.\n[31] Rajeev Ranjan, Carlos Castillo, and Ramalingam Chellappa. L2 constrained softmax\nloss for discriminative face veriﬁcation, October 3 2019. US Patent App. 15/938,898.\n[32] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN\nfeatures off-the-shelf: An astounding baseline for recognition. In CVPR Workshops,\n2014.\n[33] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Con-\ntractive auto-encoders: Explicit invariance during feature extraction. In ICML, 2011.\n[34] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embed-\nding for face recognition and clustering. In CVPR, 2015.\n[35] Hailin Shi, Yang Yang, Xiangyu Zhu, Shengcai Liao, Zhen Lei, Weishi Zheng, and\nStan Z Li. Embedding deep metric for person re-identiﬁcation: A study against large\nvariations. In ECCV, 2016.\n[36] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object\ndetectors with online hard example mining. In CVPR, 2016.\n[37] Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, and\nFrancesc Moreno-Noguer. Discriminative learning of deep convolutional feature point\ndescriptors. In ICCV, 2015.\n[38] K. Simonyan, A. Vedaldi, and A. Zisserman. Learning local feature descriptors using\nconvex optimisation. TPAMI, 2014.\n[39] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In\nNIPS, 2016.\n[40] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning\nvia lifted structured feature embedding. In CVPR, 2016.\nBINH X. NGUYEN ET AL.: DEEP METRIC LEARNING MEETS DEEP CLUSTERING\n13\n[41] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy.\nDeep metric\nlearning via facility location. In CVPR, 2017.\n[42] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper\nwith convolutions. In CVPR, 2015.\n[43] Evgeniya Ustinova and Victor S. Lempitsky.\nLearning deep embeddings with his-\ntogram loss. In NIPS, 2016.\n[44] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Ex-\ntracting and composing robust features with denoising autoencoders. In ICML, 2008.\n[45] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.\nThe Caltech-UCSD\nbirds-200-2011 dataset. 2011.\n[46] Faqiang Wang, Wangmeng Zuo, Liang Lin, David Zhang, and Lei Zhang. Joint learning\nof single-image and cross-image representations for person re-identiﬁcation. In CVPR,\n2016.\n[47] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Additive margin softmax for\nface veriﬁcation. IEEE Signal Processing Letters, 25(7):926–930, 2018.\n[48] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng\nLi, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In CVPR,\n2018.\n[49] Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Romain Garnier, and Neil M\nRobertson. Ranked list loss for deep metric learning. In CVPR, 2019.\n[50] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-\nsimilarity loss with general pair weighting for deep metric learning. In CVPR, 2019.\n[51] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature\nlearning approach for deep face recognition. In ECCV, 2016.\n[52] Paul Wohlhart and Vincent Lepetit. Learning descriptors for object recognition and 3d\npose estimation. In CVPR, 2015.\n[53] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learn-\ning via non-parametric instance discrimination. In CVPR, 2018.\n[54] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding\nlearning via invariant and spreading instance feature. In CVPR, 2019.\n[55] Sergey Zagoruyko and Nikos Komodakis.\nLearning to compare image patches via\nconvolutional neural networks. In CVPR, 2015.\n[56] Jiahuan Zhou, Pei Yu, Wei Tang, and Ying Wu. Efﬁcient online local metric adaptation\nvia negative samples for person re-identiﬁcation. In ICCV, 2017.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-09-09",
  "updated": "2020-09-09"
}