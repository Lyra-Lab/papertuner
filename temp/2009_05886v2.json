{
  "id": "http://arxiv.org/abs/2009.05886v2",
  "title": "Differentially Private Language Models Benefit from Public Pre-training",
  "authors": [
    "Gavin Kerrigan",
    "Dylan Slack",
    "Jens Tuyls"
  ],
  "abstract": "Language modeling is a keystone task in natural language processing. When\ntraining a language model on sensitive information, differential privacy (DP)\nallows us to quantify the degree to which our private data is protected.\nHowever, training algorithms which enforce differential privacy often lead to\ndegradation in model quality. We study the feasibility of learning a language\nmodel which is simultaneously high-quality and privacy preserving by tuning a\npublic base model on a private corpus. We find that DP fine-tuning boosts the\nperformance of language models in the private domain, making the training of\nsuch models possible.",
  "text": "Differentially Private Language Models Beneﬁt from Public Pre-training\nGavin Kerrigan∗\nUniversity of California, Irvine\ngavin.k@uci.edu\nDylan Slack∗\nUniversity of California, Irvine\ndslack@uci.edu\nJens Tuyls∗\nPrinceton University\njtuyls@princeton.edu\nAbstract\nLanguage modeling is a keystone task in natu-\nral language processing. When training a lan-\nguage model on sensitive information, differ-\nential privacy (DP) allows us to quantify the\ndegree to which our private data is protected.\nHowever, training algorithms which enforce\ndifferential privacy often lead to degradation\nin model quality. We study the feasibility of\nlearning a language model which is simultane-\nously high-quality and privacy preserving by\ntuning a public base model on a private cor-\npus. We ﬁnd that DP ﬁne-tuning boosts the\nperformance of language models in the private\ndomain, making the training of such models\npossible.1\n1\nIntroduction\nLanguage modeling, the task of assigning a prob-\nability to sequences of words, is a key problem\nin natural language processing. Modern language\nmodels are data-driven, relying on a large corpus\nof text. Many such models are trained on corpora\nfrom a speciﬁc domain, such as Wikipedia or news\narticles (Radford et al., 2019a). These models of-\nten suffer from generalization issues when used\nto model language from a different domain. This\nmotivates the use of model ﬁne-tuning, in which\nthe weights of a pre-trained language model are\ntuned by gradient descent on a second dataset of\ninterest (Radford et al., 2019a; Devlin et al., 2019;\nLiu et al., 2019).\nIn some cases, we would like to ﬁne-tune our\nmodel with respect to a dataset containing private\ninformation. As such, there is an obligation to\npreserve the privacy of individuals who contribute\ntext to the private training corpus. For example,\ntraining a medical chat-bot may require learning\n∗*All authors contributed equally.\n1https://github.com/dylan-slack/\nFinetuning-DP-Language-Models\na language model from transcribed patient-doctor\nconversations; it would be critical that this model\nnot expose sensitive information about the patients\nwhose conversations are used as training data. In\nrecent years, differential privacy (DP) has been a\nkey quantitative measure of privacy which allows\none to use aggregate statistical information about a\ndataset while preserving the privacy of its individ-\nual datapoints.\nIn the case of language modeling, we are in-\nterested in preserving the privacy of individuals\nwho contribute text to a private corpus. As each\nindividual who contributes to this dataset could po-\ntentially contribute several sentences, our notion of\nprivacy is group differential privacy (Dwork and\nRoth, 2014), in which all sentences from a single\nindividual are grouped. In practice, group DP is\nequivalent to DP with re-scaled parameters. A po-\ntential limitation of this approach is that the number\nof contributed sentences may not be uniform over\nusers, leading to sub-optimal bounds on the privacy\nguarantee. There has been some success in directly\ntraining differentially private language models, but\nthese often require access to large datasets in order\nto achieve a reasonable level of quality (McMahan\net al., 2017). Other work has trained a differen-\ntially private base model which was then ﬁne-tuned\nthrough active learning on a non-private dataset\n(Zhao et al., 2019).\nWe instead train a non-private base model on\na large, public dataset, which we proceed to ﬁne-\ntune on a private out-of-distribution dataset through\ndifferentially private stochastic gradient descent\n(DPSGD) (Abadi et al., 2016). By doing so, we\nsuccessfully train a high-quality model which is dif-\nferentially private with respect to our tuning dataset.\nOur experimental results show that DP ﬁne-tuning\nnot only boosts the performance of DP language\nmodeling, but makes it possible.\narXiv:2009.05886v2  [cs.LG]  26 Oct 2020\n2\nRelated Work\nTraining a feedforward neural network with DP\nis achievable through the popular DP-SGD algo-\nrithm (Abadi et al., 2016). However, this method\nmay lead to signiﬁcant decreases in the accuracy\n(or other metrics) of the resulting model. Recent\nwork considers the use of metric privacy for lan-\nguage modeling (Fernandes et al., 2019; Feyisetan\net al., 2020), which is a relaxation of differential\nprivacy where noise is instead added to the vector\nembedding of a word. We leave the exploration of\nmetric privacy for the private ﬁne-tuning task as a\ndirection for future work.\nMany high-quality language models rely on\nsome form of recurrent neural architecture, such\nas RNNs or LSTMs (Sherstinsky, 2018; Hochre-\niter and Schmidhuber, 1997). In (McMahan et al.,\n2017), the authors develop a method for training\nsuch models while achieving differential privacy.\nHowever, this approach requires a large private\ndataset, and the mechanisms to achieve privacy\nlead to a signiﬁcant decrease in model quality.\nIn (Zhao et al., 2019), the authors attempt to\ntrain a language model which is simultaneously\ndifferentially private and of high quality. The ﬁrst\nsolution proposed in (Zhao et al., 2019) is to ﬁne-\ntune the language model with publicly available\ndata, but as this public data is likely distributed\ndifferently than the private data, the resulting model\nis likely mistuned. The second proposed approach\nis to augment the training data by actively selecting\nnon-private data instances. This effectively reduces\nthe privacy cost incurred during each training step,\nbut still requires training with potentially out-of-\ndistribution data.\nIn contrast, our work begins with a pre-trained\nmodel which only has access to publicly available\ndata. This base model is then ﬁne-tuned through\nDPSGD on our private domain of interest, result-\ning in a model that is both differentially private and\ntuned with respect to our protected dataset. By tun-\ning a pre-trained public model, we achieve higher\nquality models without incurring any additional\ncosts to our privacy budget.\n3\nApproach\nLet D be a publicly available corpus, and P be\na protected corpus whose contents we would like\nto protect the privacy of. Denote by X the ﬁxed,\nshared vocabulary of these corpora. At a high level,\nour approach is to ﬁrst train a language model\nMD : X n →[0, 1]. In practice, we choose a\nfeedforward architecture for MD due to limited\ncomputing resources. We ﬁne-tune this model with\nrespect to P by using the DPSGD algorithm (Abadi\net al., 2016) on batches of sentences from P.\n3.1\n(ϵ, δ) Differential Privacy\nIntuitively, an algorithm is (ϵ, δ)-DP if the output\nof the algorithm cannot be used to probabilistically\ndetermine the presence of a single instance in the\ndatabase by more than a factor of exp(ϵ). We ad-\nditionally allow this constraint to be violated with\nprobability δ, with δ typically being small2.\nIn the case of language modeling, an individual\ni may possibly contribute si ≥1 sentences to the\nprivate training corpus. To maintain the privacy\nof said individual, we require that our algorithm\nsatisfy si-group differential privacy, meaning our\nalgorithm cannot be used to determine the presence\nor absence of si sentences in the dataset. How-\never, (ϵ, δ) si-group DP is equivalent to (ϵ/si, δ)-\nDP (Dwork and Roth, 2014). Hence, it is sufﬁcient\nto consider the somewhat unintuitive notion of pre-\nserving the privacy of individual sentences in the\ntraining set. Any mechanism satisfying (ϵ, δ)-DP\non individual sentences will then satisfy (ϵ/γ, δ)-\nDP with respect to contributing individuals, where\nγ = maxi{si}. Formally, an algorithm A satisﬁes\n(ϵ, δ)-DP if for all datasets D1, D2 differing by at\nmost one instance, and for any set S, we have\nP{A(D1) ∈S} ≤exp(ϵ)P{A(D2) ∈S} + δ\nSmaller ϵ values indicate a stronger privacy guar-\nantee. We typically think of S being some query\non the outcome of A. A more complete treatment\nof differential privacy is available in (Dwork and\nRoth, 2014).\n3.2\nDifferentially Private Fine-tuning\nDifferential privacy is achieved in SGD by adding\nappropriately scaled noise to the gradient of the\nloss function. In particular, we ﬁx a noise scale\nσ2 ∈R and a gradient clipping level C ∈R. For\na batch of size L, our loss function is given by\nL(θ) = 1\nL\nP\ni L(xi; θ). For each xi in our batch,\nwe compute the clipped gradient g(xi) by scaling\nthe gradient of the loss at xi to have ℓ2 norm at\nmost C (Dwork and Roth, 2014).\n2Some authors recommend a value of 10−5 (Abadi et al.,\n2016).\ng(xi) =\n1\nmax{1, ||∇θL(xi; θ)||2/C}∇θL(xi; θ)\nWe add appropriately scaled zero-mean Gaus-\nsian noise to our gradients:\neg(xi) = g(xi) + N(0, σ2C2I)\nOur gradient signal used in training is then the\naverage of eg(xi) over a given mini-batch, which\nwe use to determine a descent direction as in SGD.\nNote that our noisy gradient is equal to the true\ngradient in expectation, as we add mean-zero noise.\nAs our access to the private data is done entirely\nin the calculation of g(xi), with appropriately cho-\nsen parameters this method guarantees our algo-\nrithm respects our speciﬁed level of privacy.\nFor given noise σ, we can determine an accept-\nable privacy violation level δ ≪1 and compute the\nresulting privacy parameter ϵ through the compo-\nsition theorem proved in (Abadi et al., 2016). In\nappendix B 2, we plot the (ϵ, δ)-privacy guarantees\nfor various settings of σ. As expected, for a ﬁxed δ,\nmore noise (greater σ) results in a tighter privacy\nguarantee (smaller ϵ).\nThroughout this section, we have assumed a\nmaximum individual contribution size of γ = 1.\nWhen γ > 1, the only necessary change is a post-\nprocessing scaling of ϵ 7→ϵ/γ, as ϵ is computed\nbased on parameters which are independent of γ.\n4\nExperimental Results\n4.1\nDatasets\nFor our public dataset, we choose the Brown cor-\npus (Francis and Kucera, 1979), as it is a fairly\nlarge corpus designed to represent modern English.\nFor our private dataset, we used the Reddit com-\nments dataset (Reddit, 2019). While this corpus is\nnot truly private, we felt it represented the type of\nlanguage data one might be interested in protecting\n– written language generated by individual users\nwhich likely contains personal information. We ran-\ndomly select a subset of 10k comments for private\ntraining data and 5k comments for development\nand testing. For more details, see Appendix C.\n4.2\nModels and Evaluation\nFor our language models, we consider two feed-\nforward architectures: a small network and a large\nnetwork, each with three hidden layers, but with\nvarying numbers of nodes (see appendix A for de-\ntails). For both architectures, we train three base-\nline models:\n• A non-private model trained only on the pub-\nlic corpus.\n• A non-private model trained only on the pri-\nvate corpus.\n• A non-private model pre-trained on the public\ncorpus, and ﬁne-tuned on the private corpus.\nFor each architecture, we compare these baseline\nmodels to a private model which is pre-trained on\nthe public corpus and ﬁne-tuned on the private cor-\npus. For the private models, we hold δ = 1e−5 and\nset gradient clipping to 1.0. We train each private\nmodel with σ = 1.1 and σ = 0.1. Also, we ﬁne-\ntune OpenAI’s pre-trained GPT-2 (Radford et al.,\n2019b) non-privately on both Brown and Reddit.\nFor each model, we report the perplexity scores.\n4.3\nResults\nGPT-2 Fine-tuning\nThe GPT-2 model ﬁne-\ntuned for three epochs on the Brown training data\nset scored 40.0 perplexity on the held out test set.\nThe GPT-2 model ﬁne-tuned for the same time on\nthe Reddit training data set scored 45.14 on the\nheld out test set.\nSmall Feedforward Neural Network\nNext, we\ntrained and evaluated a smaller feedforward neu-\nral network on the evaluation schema from section\n4.2. Figure 1(a) shows the test-set perplexity for\neach of our models as a function of training itera-\ntions. We observe that each of the base non-private\nmodels converges at roughly the same rate, but the\nmodels trained on the Brown corpus converge to a\nlower perplexity than those trained on the Reddit\ncorpus. We also note that the ﬁne-tuned models\nachieve a signiﬁcantly lower perplexity in fewer\niterations, even with the inclusion of differential\nprivacy mechanisms. The increase in perplexity\nseen in the base Reddit model may be indicative of\noverﬁtting.\nLarge Feedforward Neural Network\nNext, we\ntrain and evaluate a large feedforward neural net-\nwork model. The results can be found in ﬁgure 1(b).\nWe found that the larger models performed mostly\nsimilar to the smaller ones. However, the larger\nmodel does signiﬁcantly outperform its smaller\ncounterpart when trained and evaluated on 10,000\ncomments sampled from the Reddit dataset. This\ncan be seen when comparing ﬁgure 1(a) and 1(b).\nThe “Reddit 10k / Reddit 10k” curve reaches a\n(a) Small Language Model\n(b) Large Language Model\nFigure 1: Test-set perplexity as a function of training iterations for the small (a) and large (b) language\nmodels. The legend indicates train-set / evaluation set, with σ being the noise scale used in differentially private\ntraining. The ﬁne-tuned models are trained on the Brown corpus and tuned on the Reddit dataset. The graph for\nσ = 1.1 for the large language model is not visible since all perplexity values are inﬁnity. Note: the graphs are\ntruncated to the ﬁrst epoch of training. Perplexities change marginally after this point.\nmuch lower value much sooner for the larger model.\nAnother difference is that the larger model was not\nable to get ﬁnite perplexity values when ﬁne-tuned\non Reddit 10k in a differentially private way with\nnoise set to 1.1, while the smaller model was able\nto do this.\n5\nAnalysis\nFinetuning improves DP perplexity\nWe sum-\nmarize the perplexities of our ﬁnal small and large\nmodels in table 1 in the appendix. A σ2 of zero\nindicates non-private training while a σ2 > 0 indi-\ncates private training where privacy increases with\nlarger σ2. We additionally provide the ϵ values\nfor the private models in ﬁgure 4. The perplexity\nscores for both the small and large feedforward\nlanguage models are orders of magnitude worse\nthan the GPT-2 models indicating that they are not\ncompetitive with state of the art language models.\nHowever, our results indicate that pre-training\nmay signiﬁcantly improve the perplexity of a dif-\nferentially private language model. We were un-\nsuccessful in training a differentially private model\non the Reddit data alone, as all models tested gave\nunreasonably high perplexities (i.e. useless mod-\nels). When DP ﬁne-tuning was used to create a\nprivate language model for this domain, our small\nmodel outperformed the baseline models (except\nfor its non-private equivalent). This indicates that\npre-training may be highly valuable in facilitating\nthe training of DP language models.\nQualitative Analysis\nWe provide a sample of\nsentences generated from models ﬁne-tuned on the\nReddit 10k data set in table 2 in the appendix.\nAside from the state of the art GPT-2 model, both\nthe small and large feedforward neural networks\nare not able to generate sentences that are coherent.\nAdditionally, there is not a discernible difference\nbetween the various levels of private ﬁne-tuning.\nThis is likely because feedforward neural networks\nare not strong language models. We do see the\npre-training beneﬁts for privacy with such models.\n6\nConclusions\nTraining neural models with differential privacy\noften signiﬁcantly degrades model performance.\nHowever, differential privacy could prove crucial\nwhen doing language modeling on private datasets.\nOur work shows that DP ﬁne-tuning not only boosts\nthe performance of DP language modeling, but\nmakes it possible. We also compared our experi-\nments across two different model sizes and found\nthat increasing the model size while decreasing\nthe number of training epochs does not signiﬁ-\ncantly impact the results in the differentially pri-\nvate transfer learning scenario. Future research\ncould experiment with stronger model architec-\ntures (e.g., LSTM’s, transformers) instead of reg-\nular feedforward neural networks, as well as train\nmodels longer in order to increase performance.\nReferences\nMart´ın Abadi, Andy Chu, Ian Goodfellow, H. Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep Learning with Differential\nPrivacy. arXiv e-prints, page arXiv:1607.00133.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\nCynthia Dwork and Aaron Roth. 2014.\nThe algo-\nrithmic foundations of differential privacy. Found.\nTrends Theor. Comput. Sci., 9(3–4):211–407.\nNatasha Fernandes, Mark Dras, and Annabelle McIver.\n2019. Generalised differential privacy for text doc-\nument processing. In International Conference on\nPrinciples of Security and Trust, pages 123–148.\nSpringer, Cham.\nOluwaseyi Feyisetan, Borja Balle, Thomas Drake, and\nTom Diethe. 2020.\nPrivacy-and utility-preserving\ntextual analysis via calibrated multivariate perturba-\ntions. In Proceedings of the 13th International Con-\nference on Web Search and Data Mining, pages 178–\n186.\nW. N. Francis and H. Kucera. 1979.\nBrown corpus\nmanual. Technical report, Department of Linguis-\ntics, Brown University, Providence, Rhode Island,\nUS.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997.\nLong\nshort-term\nmemory.\nNeural\nComput.,\n9(8):1735–1780.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E. Gonzalez.\n2020. Train large, then compress: Rethinking model\nsize for efﬁcient training and inference of transform-\ners. arXiv preprint arXiv:2002.11794.\nY. Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, M. Lewis, L. Zettle-\nmoyer, and V. Stoyanov. 2019.\nRoberta: A ro-\nbustly optimized bert pretraining approach. ArXiv,\nabs/1907.11692.\nH. Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2017. Learning differentially private\nlanguage models without losing accuracy.\nCoRR,\nabs/1710.06963.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019a. Language\nmodels are unsupervised multitask learners.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019b.\nLan-\nguage models are unsupervised multitask learners.\nReddit. 2019. May 2015 reddit comments.\nAlex Sherstinsky. 2018.\nFundamentals of recurrent\nneural network (RNN) and long short-term memory\n(LSTM) network. CoRR, abs/1808.03314.\nChris Waites. 2019.\nPyvacy: Privacy algorithms for\npytorch.\nhttps://github.com/ChrisWaites/\npyvacy.\nZhengli Zhao, Nicolas Papernot, Sameer Singh, Neok-\nlis Polyzotis, and Augustus Odena. 2019. Improving\ndifferentially private models with active learning.\nA\nArchitectures\nWe consider two language model architectures. We\nﬁrst use a feedforward neural network as our lan-\nguage model with three hidden layers consisting\nof 500, 250, and 50 nodes respectively (“small”\nlanguage model). Recent work suggests large lan-\nguage models may produce better results more\nquickly than smaller models (Li et al., 2020).\nThough the mentioned work considers transformer\nmodels, we also investigate training a larger feed-\nforward neural network with three hidden layers\nconsisting of 10, 000, 5, 000, and 1, 000 nodes\n(“large” language model) in hopes to speed up dif-\nferentially private training and gain better perfor-\nmance.\nFor both models, we consider 20 previous tokens.\nWe trained the public models using the Adam op-\ntimizer with a learning rate of 1e −3. To train\nthe private models we used the DPSGD optimizer\nfrom (Waites, 2019). We used the ReLU activation\nfunction on all nodes and the softmax function on\nthe output layer.\nLastly, we trained the small language model for\n5 epochs during pre-training and 5 epochs during\nﬁne-tuning. We trained the large language model\nfor 2 epochs during pre-training and 2 epochs dur-\ning ﬁne-tuning\nB\n(ϵ −δ)-Privacy Guarantees\nFigure 2: (ϵ, δ)-privacy guarantees for q = 10−3, T =\n105, computed using the moments accountant (Abadi\net al., 2016). Here, σ is a noise-scale parameter spec-\niﬁed by the user. This helps us to select a noise scale\nappropriate to a given application setting.\nC\nDataset Sizes\nIn ﬁgure 3, we provide the number of tokens used\nfor training in each data set.\nDataset\nTokens (train)\nTokens (test)\nReddit 10k\n689,763\n344,120\nBrown\n693,683\n-\nFigure 3: The number of tokens in the training and test\nset of each dataset. Since we don’t test on Brown, this\nentry is left empty.\nD\nAdditional Results\nTest Perplexity\n(σ2, ϵ)\nSmall\nLarge\n(0.1, 107.30)\n1480.84\n1627.56\n(1.1, 9.75)\n1473.49\nNA\nFigure 4: We provide the trade off between ϵ and test\nperplexity for the small and large models from ﬁgure 1.\nWe hold δ to 1e −5 and set the gradient clipping to 1.0.\nWe include the lowest test perplexity for each model.\nRecall the large model with σ = 1.1 never converged\nto ﬁnite perplexity and is denoted NA.\nTraining / Testing Set\nσ2\nPP (dev)\nPP (test)\nPP (dev, large)\nPP (test, large)\nBrown / Reddit 10k\n0\n1561.20\n1584.54\n1652.65\n1677.42\nReddit 10k / Reddit 10k\n0\n3805.83\n3787.68\n1254.48\n1259.23\nﬁne-tuned / Reddit 10k\n0.0\n1035.45\n1037.81\n1016.65\n1019.31\nﬁne-tuned / Reddit 10k\n0.1\n1457.94\n1480.84\n1604.42\n1627.56\nﬁne-tuned / Reddit 10k\n1.1\n1450.01\n1473.48\ninf\ninf\nTable 1: Final test-set perplexities for each of our models. Fine-tuned refers to the model being trained on Brown,\nthen ﬁne-tuned on the Reddit 10K training set. PP marked as “large” are from the second, larger neural network we\ntrained. Note that σ2 = 0.0 refers to a non-DP model while σ2 > 0.0 is a DP model, where the privacy guarantee\nincreases with σ2.\nModel\nPrompt\nSentence\nReddit 10k / Reddit 10k σ2 = 0.0\n“Bob lives close to the”\n“know extent better though\nabout really said breaking will”\nﬁne-tuned / Reddit 10k σ2 = 0.0\n“Bob lives close to the”\n“few alone saw good up done\ncould branch clever been”\nﬁne-tuned / Reddit 10k σ2 = 0.1\n“Bob lives close to the”\n“city plans increase whose even\nreached years relieved con-\nstrued what.”\nﬁne-tuned / Reddit 10k σ2 = 1.1\n“Bob lives close to the”\n“along supply am certain like\nalone before decent exceeding\nother”\nLarge Reddit 10k / Reddit 10k σ2 = 0.0\n“Bob lives close to the”\n“above twice wanted therefore\nwhile unless however defec-\ntive.”\nLarge ﬁne-tuned / Reddit 10k σ2 = 0.0\n“Bob lives close to the”\n“once obviously give found now\nre like exact dislike out.”\nLarge ﬁne-tuned / Reddit 10k σ2 = 0.1\n“Bob lives close to the”\n“leaders kid forward governor\nthought neck let rides orchestral\nshould”\nﬁne-tuned GPT-2 / Reddit 10k\n“Bob lives close to the”\n“station and we only have two\nmiles of travel left to go”\nTable 2: A selection of sentences generated from the prompt “Bob lives close” using models ﬁnetuned on the Reddit\n10k data set. Except for the GPT-2 model, there’s not a strong difference between the coherency of sentences\ngenerated.\n",
  "categories": [
    "cs.LG",
    "cs.CL",
    "cs.CR"
  ],
  "published": "2020-09-13",
  "updated": "2020-10-26"
}