{
  "id": "http://arxiv.org/abs/2009.06681v1",
  "title": "Deep Actor-Critic Learning for Distributed Power Control in Wireless Mobile Networks",
  "authors": [
    "Yasar Sinan Nasir",
    "Dongning Guo"
  ],
  "abstract": "Deep reinforcement learning offers a model-free alternative to supervised\ndeep learning and classical optimization for solving the transmit power control\nproblem in wireless networks. The multi-agent deep reinforcement learning\napproach considers each transmitter as an individual learning agent that\ndetermines its transmit power level by observing the local wireless\nenvironment. Following a certain policy, these agents learn to collaboratively\nmaximize a global objective, e.g., a sum-rate utility function. This\nmulti-agent scheme is easily scalable and practically applicable to large-scale\ncellular networks. In this work, we present a distributively executed\ncontinuous power control algorithm with the help of deep actor-critic learning,\nand more specifically, by adapting deep deterministic policy gradient.\nFurthermore, we integrate the proposed power control algorithm to a\ntime-slotted system where devices are mobile and channel conditions change\nrapidly. We demonstrate the functionality of the proposed algorithm using\nsimulation results.",
  "text": "Deep Actor-Critic Learning for Distributed Power\nControl in Wireless Mobile Networks\nYasar Sinan Nasir and Dongning Guo\nDepartment of Electrical and Computer Engineering\nNorthwestern University, Evanston, IL 60208.\nAbstract—Deep reinforcement learning offers a model-free\nalternative to supervised deep learning and classical optimization\nfor solving the transmit power control problem in wireless\nnetworks. The multi-agent deep reinforcement learning approach\nconsiders each transmitter as an individual learning agent that\ndetermines its transmit power level by observing the local wireless\nenvironment. Following a certain policy, these agents learn to\ncollaboratively maximize a global objective, e.g., a sum-rate\nutility function. This multi-agent scheme is easily scalable and\npractically applicable to large-scale cellular networks. In this\nwork, we present a distributively executed continuous power\ncontrol algorithm with the help of deep actor-critic learning, and\nmore speciﬁcally, by adapting deep deterministic policy gradient.\nFurthermore, we integrate the proposed power control algorithm\nto a time-slotted system where devices are mobile and channel\nconditions change rapidly. We demonstrate the functionality of\nthe proposed algorithm using simulation results.\nI. INTRODUCTION\nWith ever-increasing number of cellular devices, interfer-\nence management has become a key challenge in developing\nnewly emerging technologies for wireless cellular networks.\nAn access point (AP) may increase its transmit power to\nimprove data rate to its devices, but this will cause more\ninterference to nearby devices. Power control is a well-known\ninterference mitigation tool used in wireless networks. It often\nmaximizes a non-convex sum-rate objective. It becomes NP-\nhard when multiple devices share a frequency band [1].\nVarious state-of-the-art optimization methods have been\napplied to power control such as fractional programming (FP)\n[2] and weighted minimum mean square error (WMMSE)\nalgorithm [3] which are model-driven and require a mathe-\nmatically tractable and accurate model [4]. FP and WMMSE\nare iterative and executed in a centralized fashion, neglecting\nthe delay caused by the feedback mechanism between a central\ncontroller and APs. Both require full channel state information\n(CSI), and APs need to wait until centralized controller sends\nthe outcome back over a backhaul once iterations converge.\nData-driven methods are promising in a realistic wireless\ncontext where varying channel conditions impose serious\nchallenges such as imperfect or delayed CSI. Reference [3]\nuses a deep neural network to mimic an optimization algorithm\nthat is trained by a dataset composed of many optimization\nruns. The main motivation in [3] is to reduce the computa-\ntional complexity while maintaining a comparable sum-rate\nThis material is based upon work supported by the National Science\nFoundation under Grants No. CCF-1910168 and No. CNS-2003098 as well\nas a gift from Intel Incorporation.\nperformance with WMMSE. However, the training dataset\nrelies on model-based optimization algorithms. In this paper,\nwe consider a purely data-driven approach called model-free\ndeep reinforcement learning.\nSimilar to this work, we have earlier proposed a centralized\ntraining and distributed execution framework based on deep\nQ-learning algorithm for dynamic (real-time) power control\n[5]. Since Q-learning applies only to discrete action spaces,\ntransmit power had to be quantized in [5]. As a result, the\nquantizer design and the number of levels, i.e., number of\npossible actions, have an impact on the performance. For\nexample, an extension of our prior work shows that quantizing\nthe action space with a logarithmic step size gives better\noutcomes than that of a linear step size [6].\nIn this work, we replace deep Q-learning with an actor-critic\nmethod called deep deterministic policy gradient (DDPG)\n[7] algorithm that applies to continuous action spaces. A\ndistributively executed DDPG scheme has been applied to\npower control for ﬁxed channel and perfect CSI [6]. To the\nbest of our knowledge, we are the ﬁrst to study actor-critic\nbased dynamic power control that involves mobility of cellular\ndevices. Our prior work assumed immobile devices where\nthe large-scale fading component was the steady state of the\nchannel. We adapt our previous approach to make it applicable\nto our new system model that involves mobility where channel\nconditions vary due to both small and large scale fading.\nIn order to ensure the practicality, we assume delayed and\nincomplete CSI, and using simulations, we compare the sum-\nrate outcome with WMMSE and FP that have full perfect CSI.\nII. SYSTEM MODEL AND PROBLEM FORMULATION\nIn this paper, we consider a special case where N mobile\ndevices are uniformly randomly placed in K homogeneous\nhexagonal cells. This deployment scenario is similar to the in-\nterfering multiaccess channel scenario which is also examined\nin [3], [5]. Let N = {1, . . . , N} and K = {1, . . . , K} denote\nthe sets of link and cell indexes, respectively. Here we are\nnot concerned with the device association problem. As device\nn ∈N is inside cell k ∈K, its associated AP n is located at\nthe center of cell k. We denote the cell association of device\nn as bn ∈K and its AP n is positioned at the center of bn.\nAll transmitters and receivers use a single antenna and we\nconsider a single frequency band with ﬂat fading. The network\nis assumed to be a fully synchronized time slotted system with\nslot duration T. We employ a block fading model to denote\narXiv:2009.06681v1  [eess.SP]  14 Sep 2020\nthe downlink channel gain from a transmitter located at the\ncenter of cell k to the receiver antenna of device n in time\nslot t as\n¯g(t)\nk→n =\n\f\f\fh(t)\nk→n\n\f\f\f\n2\nα(t)\nk→n,\nt = 1, 2, . . . .\n(1)\nIn (1), α(t)\nk→n ≥0 represents the large-scale fading component\nincluding path loss and log-normal shadowing which varies as\nmobile device j changes its position. Let xk denote the 2D\nposition, i.e., (x, y)-coordinates, of cell k’s center. Similarly,\nwe represent the location of mobile device n at slot t as x(t)\nn .\nThen, the large-scale fading can be expressed in dB as\nα(t)\ndB,k→n = PL\n\u0010\nxk, x(t)\nn\n\u0011\n+ X (t)\nk→n,\n(2)\nwhere PL is the distance-dependent path loss in dB and X (t)\nk→n\nis the log-shadowing from xk to x(t)\nn . For each device n,\nwe compute the shadowing from all k possible AP posi-\ntions in the network. The shadowing parameter is updated\nby X (t)\nk→n = ρ(t)\ns,nX (t)\nk→n + σse(t)\ns,k→n, where σs is the log-\nnormal shadowing standard deviation and the correlation ρ(t)\ns,n\nis computed by ρ(t)\ns,n = e\n∆x(t)\nn\ndcor\nwith ∆x(t)\nn =\n\r\r\rx(t)\nn −x(t−1)\nn\n\r\r\r\n2\nbeing the displacement of device n during the last slot and\nwith dcor being the correlation length of the environment.\nNote that X (0)\nk→n ∼N\n\u00000, σ2\ns\n\u0001\nand the shadowing innovation\nprocess e(1)\ns,k→n, e(2)\ns,k→n, . . . consists of independent and iden-\ntically distributed (i.i.d.) Gaussian variables with distribution\nN\n\u0012\n0, 1 −\n\u0010\nρ(t)\ns,n\n\u00112\u0013\n. Following [8], we model the change in\nthe movement behavior of each device as incremental steps\non their speed and directions.\nUsing the Jakes fading model [5], we introduce the small-\nscale Rayleigh fading component of (1) as a ﬁrst-order com-\nplex Gauss-Markov process: h(t)\nk→n = ρ(t)\nn h(t−1)\nk→n + e(t)\nk→n,\nwhere h(0)\nk→n ∼CN(0, 1) is circularly symmetric complex\nGaussian (CSCG) with unit variance and the independent\nchannel innovation process e(1)\nk→n, e(2)\nk→n, . . . consists of i.i.d.\nCSCG random variables with distribution CN\n\u00000, 1 −ρ2\u0001\n.\nThe correlation ρ(t)\nn\ndepends on the ρ = J0(2πf (t)\nd,nT), where\nJ0(.) is the zeroth-order Bessel function of the ﬁrst kind and\nf (t)\nd,n = v(t)\nn fc/c is device n’s maximum Doppler frequency at\nslot t with v(t)\nn = ∆x(t)\nn /T being device n’s speed, c = 3×108\nm/s, and fc being carrier frequency.\nLet b(t)\nn\nand p(t)\nn\ndenote device n’s associated cell and\ntransmit power of its associated AP in time slot t, respectively.\nHence the association and allocation in time slot t can be de-\nnoted as b(t) =\nh\nb(t)\n1 , . . . , b(t)\nN\ni⊺\nand p(t) =\nh\np(t)\n1 , . . . , p(t)\nN\ni⊺\n,\nrespectively. The signal-to-interference-plus-noise ratio at re-\nceiver n in time slot t can be deﬁned as a function of the\nassociation b(t) and allocation p(t):\nγ(t)\nn\n\u0010\nb(t), p(t)\u0011\n=\n¯g(t)\nb(t)\nn →np(t)\nn\nP\nm̸=n ¯g(t)\nb(t)\nm →np(t)\nm + σ2 ,\n(3)\nwhere σ2 is the additive white Gaussian noise power spectral\ndensity which is assumed to be the same at all receivers with-\nout loss of generality. Then, the downlink spectral efﬁciency\nof device n at time t is\nC(t)\nn\n= log\n\u0010\n1 + γ(t)\nn\n\u0010\nb(t), p(t)\u0011\u0011\n.\n(4)\nFor a given association b(t), the power control problem at time\nslot t can be deﬁned as a sum-rate maximization problem:\nmaximize\np(t)\nN\nX\nn=1\nC(t)\nn\nsubject to\n0 ≤pn ≤Pmax,\nn = 1, . . . , N ,\n(5)\nwhere Pmax is the maximum power spectral density that an AP\ncan emit. The real-time allocator solves the problem in (5) at\nthe beginning of slot t and its solution becomes p(t). For ease\nof notation, throughout the paper, we use g(t)\nm→n = ¯g(t)\nb(t)\nm →n.\nIII. PROPOSED POWER CONTROL ALGORITHM\nA. Reinforcement Learning Overview\nA learning agent intersects with its environment, i.e., where\nit lives, in a sequence of discrete time steps. At each step t,\nagent ﬁrst observes the state of environment, i.e., key relevant\nenvironment features, s(t) ∈S with S being the set of possible\nstates. Then, it picks an action a(t) ∈A, where A is a\nset of actions, following a policy that is either deterministic\nor stochastic and is denoted by µ with a(t) = µ(s(t)) or\nπ with a(t) ∼π(·|s(t)), respectively. As a result of this\ninteraction, environment moves to a new state s(t+1) following\na transition probability matrix that maps state-action pairs onto\na distribution of states at the next step. Agent perceives how\ngood or bad taking action at at state s(t) is by a reward signal\nr(t+1). We describe the above interaction as an experience at\nt + 1 denoted as e(t+1) =\n\u0000s(t), a(t), r(t+1), s(t+1)\u0001\n.\nModel-free reinforcement learning learns directly from\nthese interactions without any information on the transition\ndynamics and aims to learn a policy that maximizes agent’s\nlong-term accumulative discounted reward at time t,\nR(t) =\n∞\nX\nτ=0\nγτr(t+τ+1),\n(6)\nwhere γ ∈(0, 1] is the discount factor.\nTwo main approaches to train agents with model-free re-\ninforcement learning are value function and policy search\nbased methods [9]. The well-known Q-learning algorithm\nis value based and learns an action-value function Q(s, a).\nThe classical Q-learning uses a lookup table to represent Q-\nfunction which does not scale well for large state spaces, i.e., a\nhigh number of environment features or some continuous en-\nvironment features. Deep Q-learning overcomes this challenge\nby employing a deep neural network to represent Q-function in\nplace of a lookup table. However, the action space still remains\ndiscrete which requires quantization of transmit power levels\nin a power control problem. Policy search methods can directly\nhandle continuous action spaces. In addition, compared to\nQ-learning that indirectly optimize agent’s performance by\nlearning a value function, policy search methods directly\noptimize a policy which is often more stable and reliable [10].\nBy contrast, the policy search algorithms are typically on-\npolicy which means each policy iteration only uses data that\nis collected by the most-recent policy. Q-learning can reuse\ndata collected at any point during training, and consequently,\nmore sample efﬁcient. Another speciﬁc advantage of off-policy\nlearning for a wireless network application is that the agents\ndo not need to wait for the most-recent policy update and can\nsimultaneously collect samples while the new policy is being\ntrained. Since both value and policy based approaches have\ntheir strengths and drawbacks, there is also a hybrid approach\ncalled actor-critic learning [9].\nReference [7] proposed the DDPG algorithm which is\nbased on the actor-critic architecture and allows continuous\naction spaces. DDPG algorithm iteratively trains an action-\nvalue function using a critic network and uses this function\nestimate to train a deterministic policy parameterized by an\nactor network.\nFor a policy π, the Q-function at state-action pair (s, a) ∈\nS × A becomes\nQπ(s, a) = Eπ\nh\nR(t)\f\f\fs(t) = s, a(t) = a\ni\n.\n(7)\nFor a certain state s, a deterministic policy µ : S →A\nreturns action a = µ(s). In a stationary Markov decision\nprocess setting, the optimal Q-function associated with the\ntarget policy µ satisﬁes the Bellman property and we can make\nuse of this recursive relationship as\nQµ(s, a) = E\nh\nr(t+1) + γQµ(s′, µ(s′))\n\f\f\fs(t) = s, a(t) = a\ni\n,\n(8)\nwhere the expectation is over s′ which follows the distribution\nof the state of the environment. As the target policy is\ndeterministic, the expectation in (8) depends only on the en-\nvironment transition dynamics. Hence, an off-policy learning\nmethod similar to deep Q-learning can be used to learn a Q-\nfunction parameterized by a deep neural network called critic\nnetwork. The critic network is denoted as Qφ(s, a) with φ\nbeing its parameters. Similarly, we parameterize the policy\nusing another DNN named actor network µθ(s) with policy\nparameters being θ.\nLet the past interactions be stored in an experience-replay\nmemory D until time t in the form of e = (s, a, r′, s′). This\nmemory needs to be large enough to avoid over-ﬁtting and\nsmall enough for faster training. DDPG also applies another\ntrick called quasi-static target network approach and deﬁne\ntwo separate networks to be used in training which are train\nand target critic networks with their parameters denoted as φ\nand φtarget, respectively. To train φ, at each time slot, DDPG\nminimizes the following mean-squared Bellman error:\nL (φ, D) = E(s,a,r′,s′)∼D\nh\n(y(r′, s′) −Qφ (s, a))2i\n(9)\nwhere the target y(r′, s′) = r′ + γQφtarget (s′, µθ(s′)). Hence,\nφ is updated by sampling a random mini-batch B from D and\nFig. 1: Diagram of the proposed power control algorithm.\nrunning gradient descent using\n∇φ\n1\n|B|\nX\n(s,a,r′,s′)∈B\n(y(r′, s′) −Qφ (s, a))2 .\n(10)\nNote that after each training iteration φtarget is updated by φ.\nIn addition, the policy parameters are updated to learn a\npolicy µθ(s) which gives the action that maximizes Qφ(s, a).\nSince the action space is continuous, Qφ(s, a) is differentiable\nwith respect to action and θ is updated by gradient ascent using\n∇θ\n1\n|B|\nX\n(s,... )∈B\nQφ (s, µθ(s)) .\n(11)\nTo ensure exploration during training, a noise term is added\nto the deterministic policy output [7]. In our multi-agent\nframework to be discussed next section, we employ ϵ-greedy\nalgorithm of Q-learning instead for easier tuning.\nB. Proposed Multi-Agent Learning Scheme for Power Control\nFor the proposed power control scheme in Fig. 1, we let\neach transmitter be a learning agent. Hence, the next state\nof each agent is determined by the joint-actions of all agents\nand the environment is no longer stationary. In order to avoid\ninstability, we gather the experiences of all agents in a single\nreplay memory and train a global actor network θagent to be\nshared by all agents. At slot t, each agent n ∈N observes its\nlocal state s(t)\nn\nand sets its own action a(t)\nn\nby using θagent.\nFor each link n, we ﬁrst describe the neighboring sets that\nallow the distributively execution. Link n’s set of interfering\nneighbors at time slot t consists of nearby AP indexes whose\nreceived signal-to-noise ratio (SNR) at device n was above a\ncertain threshold during the past time slot and is denoted as\nI(t)\nn\n=\nn\ni ∈N, i ̸= n\n\f\f\fg(t−1)\ni→n p(t−1)\ni\n> ησ2o\n.\n(12)\nConversely, we deﬁne link n’s set of interfered neighbors at\ntime slot t using the received SNR from AP n, i.e.,\nO(t)\nn =\nn\no ∈N, o ̸= n\n\f\f\fg(t−1)\nn→o p(t−1)\nn\n> ησ2o\n.\n(13)\nTo satisfy the practical constraints introduced in [5], we limit\nthe information exchange between AP n and its neighboring\nAPs as depicted in Fig. 2. Although, it is assumed in [5] that\nFig. 2: The information exchange in time slot t.\nreceiver n may do a more recent received power measurement\nfrom AP i ∈I(t+1)\nn\njust before the beginning of time slot\nt + 1, i.e., ¯g(t+1)\nb(t)\ni\n→np(t)\ni , we prefer not to require it for our\nnew model that involves mobility. Note that as device n’s\nassociation changes, i.e., b(t+1)\nn\n̸= b(t)\nn , we assume that\nthe neighboring sets are still determined with respect to the\nprevious positioning of AP n and the feedback history from\npast neighbors is preserved at the new AP position to be used\nin agent n’s state. We let the association of device change only\nafter staying within a new cell for Tregister consecutive slots.\nFor the training process, as a major modiﬁcation on [5],\nwe introduce training episodes where we execute a training\nprocess for Ttrain slots and let devices do random walk without\nany training for Ttravel slots before the next training episode.\nThe Ttravel slot-long traveling period induces change in the\nchannel conditions, and consequently allows policy to observe\nmore variant states during its training which intuitively in-\ncreases its robustness to the changes in channel conditions.\nTo train a policy from scratch for a random wireless network\ninitialization, we run E training episodes which are indexed\nby E = 1, . . . , E. The e-th training episode starts at slot\nte = (e −1) (Ttrain + Ttravel) and is composed of two parallel\nprocedures called centralized training and distributed execu-\ntion. After an interaction with the environment, each agent\nsends its newly acquired experience to the centralized trainer\nwhich executes the centralized training process. The trainer\nclears its experience-replay memory at the beginning of each\ntraining episode. Due to the backhaul delay as shown in Fig.\n1, we assume that the most-recent experience that the trainer\nreceived from agent n at slot t is e(t−1)\nn\n. During slot t, after\nacquiring all recent experiences in the memory D, the trainer\nruns one gradient step for the actor and critic networks. Since\nthe purpose of the critic network is to guide the actor network\nduring training, only the actor network needs to be broadcasted\nto the network agents and during the inference mode only\nthe actor network is required. The trainer starts to broadcast\nθbroadcast \u0000 θagent once every Tu slots and we assume θbroadcast\nis received by the agents after Td slots again due to delay. In\naddition, compared to the deep Q-network in [5] that reserves\nan output port for each discrete action, each actor network has\njust one output port.\nThe local state of agent n at time t, i.e., s(t)\nn\nis a tuple\nof local environment features that are signiﬁcantly affected\nby the agent’s and its neighbor’s actions. As described in\n[5], the state set design is a combination of three feature\ngroups. The ﬁrst feature group is called “local information”\nand occupies six neural network input ports. The ﬁrst input\nport is agent n’s latest transmit power p(t−1)\nn\nwhich is followed\nby its contribution to the network objective (5), i.e., C(t−1)\nn\n.\nNext, agent n appends the last two measurements of its direct\ndownlink channel and sum interference-plus-noise power at\nreceiver n: g(t)\nn→n, g(t−1)\nn→n ,\n\u0010P\nm∈N,m̸=n g(t−1)\nm→np(t−1)\nm\n+ σ2\u0011\n,\nand\n\u0010P\nm∈N,m̸=i g(t−2)\nm→np(t−2)\nm\n+ σ2\u0011\n.\nThese are followed by the “interfering neighbors” feature\ngroup. Since we are concerned by the scalability, we limit\nthe number of interfering neighbors the algorithm involves\nto c by prioritizing elements of I(t)\nn\nby their amount of\ninterference at receiver n, i.e., g(t−1)\ni→n p(t−1)\ni\n. We form ¯I(t)\nn\nby\ntaking ﬁrst c sorted elements of I(t)\nn . As |I(t)\nn | < c, we ﬁll this\nshortage by using virtual neighbors with zero downlink and\ninterfering channel gains. We also set its spectral efﬁciency\nto an arbitrary negative number. Hence, a virtual neighbor\nis just a placeholder that ineffectively ﬁlls neural network\ninputs. Next, for each i ∈¯I(t)\nn , we reserve three input ports:\ng(t)\ni→np(t−1)\ni\n, C(t−1)\ni\n. This makes a total of 3c input ports used\nfor current interfering neighbors. In addition, agent n also\nincludes the history of interfering neighbors and appends 3c\ninputs using ¯I(t−1)\nn\n.\nFinally, we have the “interfered neighbors” feature group.\nIf agent n does not transmit during slot t −1, O(t)\nn\n= ∅\nand there will be no useful interfered neighbor information\nto build s(t)\nn . Hence, we deﬁne time slot t′\nn as the last slot\nwith p(t′\nn)\nn\n> 0 and we consider O(t′\nn+1)\nn\nin our state set\ndesign. We also assume that as agent n becomes inactive,\nit will still carry on its information exchange between each\no ∈O(t′\nn+1)\nn\nwithout the knowledge of g(t−1)\nn→o . Similar to\nthe scheme described above, agent i regulates O(t′\nn+1)\nn\nto\nset | ¯O(t)\nn | = c. For o ∈O(t′\nn+1)\nn\n, the prioritization crite-\nria is now agent i’s share on the interference at receiver\no, i.e., g(t−1)\nn→o p(t−1)\nn\n\u0010P\nm∈N,m̸=o g(t−1)\nm→op(t−1)\nm\n+ σ2\u0011−1\n. For\neach interfered neighbor o ∈O(t′\nn+1)\nn\n, s(t)\nn\naccommodates\nfour features which can be listed as: g(t−1)\no→o , C(t−1)\no\n, and\ng(t′\ni)\nn→op(t′\ni)\nn\n\u0010P\nm∈N,m̸=o g(t−1)\nm→op(t−1)\nm\n+ σ2\u0011−1\n.\nThe reward of agent n, r(t+1)\nn\n, is computed by the central-\nized trainer and used in the training process. Similar to [5],\nr(t)\nn\nis deﬁned as agent’s contribution on the objective (5):\nr(t+1)\nn\n= C(t)\nn −\nX\no∈O(t+1)\nn\nπ(t)\nn→o\n(14)\nwith π(t)\nn→o = log\n\u0010\n1 + γ(t)\no\n\u0010\nb(t),\nh\n. . . , p(t)\nn−1, 0, p(t)\nn+1, . . .\ni⊺\u0011\u0011\n−\nC(t)\no\nbeing the externality that link n causes to interfered o.\nIV. SIMULATIONS\nFollowing the LTE standard, the path-loss is simulated by\n128.1 + 37.6 log10(d) (in dB) with fc = 2 GHz, where d\nis transmitter-to-receiver distance in km. We set σs = 10\ndB, dcor = 10 meters, T = 20 ms, Pmax = 38 dBm, and\nσ2 = −114 dBm. We simulate the mobility using Haas’\n1000\n500\n0\n500\n1000\n1500\n2000\nx axis position (meters)\n1000\n500\n0\n500\n1000\ny axis position (meters)\nAP\ne 1\ntravel\ne 2\ne 3\nFig. 3: Example movement until the end of episode e = 3.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\ntraining episodes\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nnormalized sum-rate performance\nWMMSE w perfect CSI\nFP w perfect CSI\npre-trained policy w mobility\npre-trained policy w/o mobility\nFP w 1-slot delayed CSI\nrandom power\nfull power\nFig. 4: Test results for the 10 cells and 20 links scenario.\nmodel [8] with maximum speed being 2.5 m/s. Each mobile\ndevice randomly updates its speed and direction every second\nuniformly within [−0.5, 0.5] m/s and [−0.175, 0.175] radians,\nrespectively. Fig. 3 shows an example movement scenario\nuntil the end of third training episode with Ttrain = 5, 000\nand Ttravel = 50, 000 slots. The DDPG implementation and\nparameters are included in the source code. 1 Both WMMSE\nand FP start from a full power allocation, since it gives better\nperformance than random initialization. WMMSE takes more\niterations to converge than FP, resulting in higher sum-rate.\nWe ﬁrst train two policies for K = 10 cells and N = 20\nlinks network deployment for E = 10 training episodes. The\nﬁrst policy is trained with mobile devices, whereas the latter is\ntrained without mobility, i.e., with steady channel. We set fd\nto 10 Hz for all time slots [5]. We save the policy parameters\nduring training for testing on several random deployments\nwith (K, N) = (10, 20) and mobility. As shown in Fig. 4,\nwithout mobility, there is no signiﬁcant sum-rate gain after\nthe ﬁrst training episode and policy converges to FP’s sum-\nrate performance. As a remark, FP is centralized and it has\nfull CSI, whereas actor network is distributively executed with\nlimited information exchange. As we include device mobility\nand a certain travel time between training episodes, the policy\nis able to experience various device positions and interference\nconditions during training, so its sum-rate performance consis-\ntently increases. Additionally, in Table I, we show that an actor\nnetwork trained for (K, N) = (10, 20) can keep up with the\n1GitHub repository: https://github.com/sinannasir/Power-Control-asilomar\nTABLE I: Average sum-rate performance in bps/Hz per link.\n(cells,links) policy trained for (10,20) WMMSE FP FP w delay random full\n(10,20)\n2.59\n2.61\n2.45\n2.37\n0.93\n0.91\n(20,40)\n1.97\n2.09\n1.98\n1.87\n0.68\n0.68\n(20,60)\n1.58\n1.68\n1.59\n1.50\n0.37\n0.35\n(20,100)\n1.14\n1.23\n1.15\n1.09\n0.18\n0.17\nsum-rate performance of optimization algorithms as network\ngets larger. Hence, running centralized training from scratch\nis not necessary as device positions change or new devices\nregister, since a pre-trained policy for a smaller and different\ndeployment performs quite well. For the 20 link scenario, on\naverage, WMMSE and FP converge in 42 and 24 iterations,\nrespectively. For 100 links, WMMSE requires 74 iterations.\nConversely, learning agent takes just one policy evaluation.\nV. CONCLUSION\nIn this paper, we presented a distributively executed deep\nactor-critic framework for power control. During training, only\nactor network is broadcasted to learning agents. Simulations\nshow that a pre-trained policy gives comparable performance\nwith WMMSE and FP, and a policy trained for a smaller\ndeployment is applicable to a larger network without additional\ntraining thanks to the distributed execution scheme. Further,\nwe have shown that the proposed actor-critic framework\nenables real-time power control under certain practical con-\nstraints and it is compatible with the case of mobile devices.\nDDPG in fact uses the mobility to increase its sum-rate\nperformance by experiencing more variant channel conditions.\nREFERENCES\n[1] Z. Q. Luo and S. Zhang, “Dynamic spectrum management: Complexity\nand duality,” IEEE Journal of Selected Topics in Signal Processing,\nvol. 2, no. 1, pp. 57–73, Feb 2008.\n[2] K. Shen and W. Yu, “Fractional programming for communication\nsystemspart i: Power control and beamforming,” IEEE Transactions on\nSignal Processing, vol. 66, no. 10, pp. 2616–2630, May 2018.\n[3] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropoulos,\n“Learning to optimize: Training deep neural networks for interference\nmanagement,” IEEE Transactions on Signal Processing, vol. 66, no. 20,\npp. 5438–5453, Oct 2018.\n[4] Z. Qin, H. Ye, G. Y. Li, and B. F. Juang, “Deep learning in physical\nlayer communications,” IEEE Wireless Communications, vol. 26, no. 2,\npp. 93–99, 2019.\n[5] Y. S. Nasir and D. Guo, “Multi-agent deep reinforcement learning\nfor dynamic power allocation in wireless networks,” IEEE Journal on\nSelected Areas in Communications, vol. 37, no. 10, pp. 2239–2250,\n2019.\n[6] F. Meng, P. Chen, L. Wu, and J. Cheng, “Power allocation in\nmulti-user cellular networks: Deep reinforcement learning approaches,”\nCoRR, vol. abs/1901.07159, 2019. [Online]. Available: http://arxiv.org/\nabs/1901.07159\n[7] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv e-prints, p. arXiv:1509.02971, Sep. 2015.\n[8] Z. J. Haas, “A new routing protocol for the reconﬁgurable wireless\nnetworks,” in Proceedings of ICUPC 97 - 6th International Conference\non Universal Personal Communications, vol. 2, Oct 1997, pp. 562–566.\n[9] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath,\n“Deep reinforcement learning: A brief survey,” IEEE Signal Processing\nMagazine, vol. 34, no. 6, pp. 26–38, 2017.\n[10] J. Achiam, “Spinning up in deep reinforcement learning,” https://\nspinningup.openai.com, 2018.\n",
  "categories": [
    "eess.SP",
    "cs.IT",
    "math.IT",
    "stat.ML"
  ],
  "published": "2020-09-14",
  "updated": "2020-09-14"
}