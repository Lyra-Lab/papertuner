{
  "id": "http://arxiv.org/abs/2009.06851v1",
  "title": "Unsupervised Abstractive Dialogue Summarization for Tete-a-Tetes",
  "authors": [
    "Xinyuan Zhang",
    "Ruiyi Zhang",
    "Manzil Zaheer",
    "Amr Ahmed"
  ],
  "abstract": "High-quality dialogue-summary paired data is expensive to produce and\ndomain-sensitive, making abstractive dialogue summarization a challenging task.\nIn this work, we propose the first unsupervised abstractive dialogue\nsummarization model for tete-a-tetes (SuTaT). Unlike standard text\nsummarization, a dialogue summarization method should consider the\nmulti-speaker scenario where the speakers have different roles, goals, and\nlanguage styles. In a tete-a-tete, such as a customer-agent conversation, SuTaT\naims to summarize for each speaker by modeling the customer utterances and the\nagent utterances separately while retaining their correlations. SuTaT consists\nof a conditional generative module and two unsupervised summarization modules.\nThe conditional generative module contains two encoders and two decoders in a\nvariational autoencoder framework where the dependencies between two latent\nspaces are captured. With the same encoders and decoders, two unsupervised\nsummarization modules equipped with sentence-level self-attention mechanisms\ngenerate summaries without using any annotations. Experimental results show\nthat SuTaT is superior on unsupervised dialogue summarization for both\nautomatic and human evaluations, and is capable of dialogue classification and\nsingle-turn conversation generation.",
  "text": "Unsupervised Abstractive Dialogue Summarization for Tete-a-Tetes\nXinyuan Zhang1, Ruiyi Zhang2, Manzil Zaheer3, Amr Ahmed3\n1ASAPP Inc.\n2Duke University\n3Google Research\nxzhang@asapp.com\nAbstract\nHigh-quality dialogue-summary paired data is\nexpensive to produce and domain-sensitive,\nmaking abstractive dialogue summarization a\nchallenging task. In this work, we propose the\nﬁrst unsupervised abstractive dialogue summa-\nrization model for tete-a-tetes (SuTaT). Unlike\nstandard text summarization, a dialogue sum-\nmarization method should consider the multi-\nspeaker scenario where the speakers have dif-\nferent roles, goals, and language styles.\nIn\na tete-a-tete, such as a customer-agent con-\nversation, SuTaT aims to summarize for each\nspeaker by modeling the customer utterances\nand the agent utterances separately while re-\ntaining their correlations. SuTaT consists of\na conditional generative module and two un-\nsupervised summarization modules. The con-\nditional generative module contains two en-\ncoders and two decoders in a variational au-\ntoencoder framework where the dependencies\nbetween two latent spaces are captured. With\nthe same encoders and decoders, two unsuper-\nvised summarization modules equipped with\nsentence-level self-attention mechanisms gen-\nerate summaries without using any annota-\ntions. Experimental results show that SuTaT\nis superior on unsupervised dialogue summa-\nrization for both automatic and human evalua-\ntions, and is capable of dialogue classiﬁcation\nand single-turn conversation generation.\n1\nIntroduction\nTete-a-tetes, conversations between two partici-\npants, have been widely studied as an importance\ncomponent of dialogue analysis. For instance, tete-\na-tetes between customers and agents contain infor-\nmation for contact centers to understand the prob-\nlems of customers and improve the solutions by\nagents. However, it is time-consuming for oth-\ners to track the progress by going through long\nand sometimes uninformative utterances. Auto-\nmatically summarizing a tete-a-tete into a shorter\nCustomer:\nI am looking for the Hamilton Lodge in Cam-\nbridge.\nAgent:\nSure, it is at 156 Chesterton Road, postcode\ncb41da.\nCustomer:\nPlease book it for 2 people, 5 nights begin-\nning on Tuesday.\nAgent:\nDone. Your reference number is qnvdz4rt.\nCustomer:\nThank you, I will be there on Tuesday!\nAgent:\nIs there anything more I can assist you with\ntoday?\nCustomer:\nThank you! That’s everything I needed.\nAgent:\nYou are welcome. Any time.\nCustomer\nSummary:\ni would like to book a hotel in cambridge on\ntuesday .\nAgent\nSummary:\ni have booked you a hotel . the reference\nnumber is qnvdz4rt . can i help you with\nanything else ?\nTable 1: An example of SuTaT generated summaries.\nversion while retaining its main points can save a\nvast amount of human resources and has a number\nof potential real-world applications.\nSummarization models can be categorized into\ntwo classes: extractive and abstractive. Extractive\nmethods select sentences or phrases from the input\ntext, while abstractive methods attempt to gener-\nate novel expressions which requires an advanced\nability to paraphrase and condense information. De-\nspite being easier, extractive summarization is often\nnot preferred in dialogues for its limited capabil-\nity to capture highly dependent conversation histo-\nries and produce coherent discourses. Therefore,\nabstractively summarizing dialogues has attracted\nrecent research interest (Goo and Chen, 2018; Pan\net al., 2018; Yuan and Yu, 2019; Liu et al., 2019).\nHowever, existing abstractive dialogue summa-\nrization approaches fail to address two main prob-\nlems. First, a dialogue is carried out between mul-\ntiple speakers and each of them has different roles,\ngoals, and language styles. Taking the example of\na contact center, customers aim to propose prob-\nlems while agents aim to provide solutions, which\narXiv:2009.06851v1  [cs.CL]  15 Sep 2020\nleads them to have different semantic contents and\nchoices of vocabularies. Most existing methods\nprocess dialogue utterances as in text summariza-\ntion without accommodating the multi-speaker sce-\nnario. Second, high-quality annotated data is not\nreadily available in the dialogue summarization do-\nmain and can be very expensive to produce. Topic\ndescriptions or instructions are commonly used as\ngold references which are too general and lack any\ninformation about the speakers. Moreover, some\nmethods use auxiliary information such as dialogue\nacts (Goo and Chen, 2018), semantic scaffolds\n(Yuan and Yu, 2019) and key point sequences (Liu\net al., 2019) to help with summarization, adding\nmore burden on data annotation. To our knowl-\nedge, no previous work has focused on unsuper-\nvised deep learning for abstractive dialogue sum-\nmarization.\nWe propose SuTaT, an unsupervised abstractive\ndialogue summarization approach speciﬁcally for\ntete-a-tetes. In this paper, we use the example of\nagent and customer to represent the two speakers\nin tete-a-tetes for better understanding. In addition\nto summarization, SuTaT can also be used for di-\nalogue classiﬁcation and single-turn conversation\ngeneration.\nTo accommodate the two-speaker scenario, Su-\nTaT processes the utterances of a customer and an\nagent separately in a conditional generative module.\nInspired by Zhang et al. (2019) where two latent\nspaces are contained in one variational autoencoder\n(VAE) framework, the conditional generative mod-\nule includes two encoders to map a customer ut-\nterance and the corresponding agent utterance into\ntwo latent representations, and two decoders to re-\nconstruct the utterances jointly. Separate encoders\nand decoders enables SuTaT to model the differ-\nences of language styles and vocabularies between\ncustomer utterances and agent utterances. The de-\npendencies between two latent spaces are captured\nby making the agent latent variable conditioned on\nthe customer latent variable. Compared to using\ntwo standard autoencoders that learn determinis-\ntic representations for input utterances, using the\nVAE-based conditional generative module to learn\nvariational distributions gives the model more ex-\npressive capacity and more ﬂexibility to ﬁnd the\ncorrelation between two latent spaces.\nThe same encoders and decoders from the con-\nditional generative module are used in two unsu-\npervised summarization modules to generate cus-\ntomer summaries and agent summaries. Divergent\nfrom MeanSum (Chu and Liu, 2019) where the\ncombined multi-document representation is simply\ncomputed by averaging the encoded input texts,\nSuTaT employs a setence-level self-attention mech-\nanism (Vaswani et al., 2017) to highlight more sig-\nniﬁcant utterances and neglect uninformative ones.\nWe also incorporate copying factual details from\nthe source text that has proven useful in supervised\nsummarization (See et al., 2017). Dialogue sum-\nmaries are usually written in the third-person point\nof view, but SuTaT simpliﬁes this problem by mak-\ning the summaries consistent with the utterances\nin pronouns. Table 1 shows an example of SuTaT\ngenerated summaries.\nExperiments are conducted on two dialogue\ndatasets: MultiWOZ (Budzianowski et al., 2018)\nand Taskmaster (Byrne et al., 2019). It is assumed\nthat we can only access utterances in the datasets\nwithout any annotations including dialogue acts,\ndescriptions, instructions, etc. Both automatic and\nhuman evaluations show SuTaT outperforms other\nunsupervised baseline methods on dialogue sum-\nmarization. We further show the capability of Su-\nTaT on dialogue classiﬁcation with generated sum-\nmaries and single-turn conversation generation.\n2\nMethodology\nSuTaT consists of a conditional generative mod-\nule and two unsupervised summarization modules.\nLet X = {x1, · · · , xn} denote a set of customer\nutterances and Y = {y1, · · · , yn} denote a set of\nagent utterances in the same dialogue. Our aim\nis to generate a customer summary and an agent\nsummary for the utterances in X and Y.\nFigure 1 shows the entire architecture of SuTaT.\nGiven a customer utterance x and its consecutive\nagent utterance y, the conditional generative mod-\nule embeds them with two encoders and obtain\nlatent variables zx and zy from the variational la-\ntent spaces, then reconstruct the utterances from zx\nand zy with two decoders. In the latent space, the\nagent latent variable is conditioned on the customer\nlatent variable; during decoding, the generated cus-\ntomer utterances are conditioned on the generated\nagent utterances. This design resembles how a tete-\na-tete carries out: the agent’s responses and the\ncustomer’s requests are dependent on each other.\nThe encoded utterances of a dialogue are the inputs\nof the unsupervised summarization modules. We\nemploy a sentence-level self-attention mechanism\nCustomer \nEncoder\nAgent \nEncoder\nCustomer\nDecoder\nAgent\nDecoder\nCustomer \nLatent Space\nAgent \nLatent Space\nCustomer Utterances X\nAgent Utterances Y\nCustomer Utterances\nAgent Utterances\nCustomer \nDecoder \nSentence-Level\nSelf-Attention\nPartial\nCopy\nSX\nZx\nZy\nSummary\nRepresentation\nCustomer\nLatent Space\n(low variance)\nCustomer Summary\nAgent\nDecoder \nSentence-Level\nSelf-Attention\nPartial\nCopy\nSY\nSummary\nRepresentation\nAgent\nLatent Space\n(low variance)\nAgent Summary\nConditional \nGenerative Module\nUnsupervised \nSummarization Module\nUnsupervised \nSummarization Module\nEncoded Customer Utterances\nEncoded Agent Utterances\nFigure 1: Block diagram of SuTaT. Architectures connected by a blue dashed line are the same. The red arrow\nrepresents the conditional relationship between two latent spaces.\non the utterances embeddings to highlight the more\ninformative ones and combine the weighted embed-\ndings. A summary representation is drawn from\nthe low-variance latent space using the combined\nutterance embedding, which is then decoded into a\nsummary with the same decoder and a partial copy\nmechanism. The whole process does not require\nany annotations from the data.\n2.1\nConditional Generative Module\nWe build the conditional generative module in\na SIVAE-based framework (Zhang et al., 2019)\nto capture the dependencies between two latent\nspaces. The goal of the module is to train two en-\ncoders and two decoders for customer utterances x\nand agent utterances y by maximizing the evidence\nlower bound\nLgen = Eq(zx|x) log p(x|y, zx)−\n(1)\nKL[q(zx|x)||p(zx)] + Eq(zy|y,zx) log p(y|zy)\n−KL[q(zy|y, zx)||p(zy|zx)] ≤log p(x, y),\nwhere q(·) is the variational posterior distribution\nthat approximates the true posterior distribution.\nThe lower bound includes two reconstruction losses\nand two Kullback-Leibler (KL) divergences be-\ntween the priors and the variational posteriors. By\nassuming priors and posteriors to be Gaussian, we\ncan apply the reparameterization trick (Kingma and\nWelling, 2014) to compute the KL divergences in\nclosed forms. q(zx|x), q(zy|y, zx), p(x|y, zx),\nand p(y|zy) represent customer encoder, agent en-\ncoder, customer decoder, and agent decoder.\nThe correlation between two latent spaces are\ncaptured by making the agent latent variable zy\nconditioned on the customer latent variable zx. We\ndeﬁne the customer prior p(zx) to be a standard\nGaussian N(0, I). The agent prior p(zy|zx) is\nalso a Gaussian N(µ, Σ) where the mean and the\nvariance are functions of zx,\nµ = MLPµ(zx),\nΣ = MLPΣ(zx).\nThis process resembles how a tete-a-tete at contact\ncenters carries out: the response of an agent is\nconditioned on what the customer says.\nEncoding\nGiven a customer utterance sequence\nx = {w1, · · · , wt}, we ﬁrst encode it into an ut-\nterance embedding ex using bidirectional LSTM\n(Graves et al., 2013) or a Transformer encoder\n(Vaswani et al., 2017).\nThe Bi-LSTM takes the hidden states hi =\n[−→\nh i; ←−\nh i] as contextual representations by process-\ning a sequence from both directions,\n−→\nh i = LSTM(wi, hi−1), ←−\nh i = LSTM(wi, hi+1).\nThe Transformer encoder produces the contextual\nrepresentations that have the same dimensions as\nword embeddings,\n{ ˙w1, · · · , ˙wt} = TransEnc({w1, · · · , wt}).\nThe customer utterance embedding ex is obtained\nby averaging over the contextual representations.\nSimilarly, we can obtain the agent utterance em-\nbedding ey.\nThe customer latent variable zx is ﬁrst sam-\npled from q(zx|x) = N(µx, Σx) using ex, then\nthe agent latent variable zy is sampled from\nq(zy|y, zx) = N(µy, Σy) using ey and zx. The\nGaussian parameters µx, Σx, µy and Σy are com-\nputed with separate linear projections,\nµx = Linearµx(ex), µy = Linearµy(ey ⊕zx)\nΣx = LinearΣx(ex), Σy = LinearΣy(ey ⊕zx).\nDecoding\nWe ﬁrst decode zy into the agent ut-\nterance from the p(y|zy) using LSTM (Sutskever\net al., 2014) or a Transformer decoder (Vaswani\net al., 2017). The decoded sequence and the latent\nvariable zx are then used in p(x|y, zx) to generate\nthe customer utterance.\nIn the LSTM decoder,\nv(i)\ny\n= LSTM(yi−1, zy, v(i−1)\ny\n)\nv(i)\nx = LSTM(xi−1, zx ⊕y, v(i−1)\nx\n).\nWhile in the Transformer decoder,\nv(i)\ny\n= TranDec(y<i, zy)\nv(i)\nx = TranDec(x<i, zx ⊕y)\nwhere y<i and x<i are the embeddings of the pre-\nviously decoded sequence. The decoded represen-\ntations v(i)\ny and v(i)\nx are put in feedforward layers\nto compute the vocabulary distributions,\np(yi|y<i, zy) = softmax(v(i)\ny WT\ny + by)\np(xi|x<i, zx, y) = softmax(v(i)\nx WT\nx + bx) (2)\nwhere Wx ∈R|x|×l, Wy ∈R|y|×l, bx ∈Rl and\nby ∈Rl are learnable parameters. |x| and |y| are\nthe vocabulary sizes for customer utterances and\nagent utterances.\n2.2\nUnsupervised Summarization Module\nGiven the encoded utterances of a dialogue, an\nunsupervised summarization module learns to gen-\nerate a summary that is semantically similar to the\ninput utterances using trained components from the\nconditional generative module.\nSentence-Level Self-Attention\nSome utterances\nlike greetings or small talk do not contribute to\nthe content of a dialogue. Therefore, we employ\na sentence-level self-attention mechanism, which\nis built upon Multi-head attention (Vaswani et al.,\n2017), to highlight the most signiﬁcant utterances\nin a dialogue.\nThe multi-head attention partitions the queries\nQ, keys K, and values V into h heads along their\ndimensions d, and calculates h scaled dot-product\nattention for the linear projections of the heads.\nMH(Q, K, V) = Concat(head1, · · · , headh)WO\nheadi = SDP(QWQ\ni , KWK\ni , VWV\ni )\nwhere WO, WQ, WK, and WV are trainable pa-\nrameters. The scaled dot-product attention outputs\na weighted sum of values,\nSDP(Q, K, V) = softmax(QKT\n√\nd\n)V.\nIn SuTaT, the sentence-level self-attention is\nachieved by making the queries, keys, and values\nall be the set of encoded agent/customer utterances\nof a dialogue. The self-attention module assigns\nweights on the input utterances such that more sig-\nniﬁcant and informative ones have higher weights.\nThe output is a weighted combined utterance em-\nbedding ˜eX or ˜eY that highlights more informative\nutterances from the dialogue.\nSummary\nGeneration\nSummary\nrepresenta-\ntions sX and sY are sampled from the latent spaces\ntaking the weighted combined utterance represen-\ntations ˜eX and ˜eY as inputs. To limit the amount\nof novelty in the generated summary, we set the\nvariances of the latent spaces close to zero so that\nsX ≈µx and sY ≈µy. sX and sY containing\nkey information from the dialogue are decoded into\na customer summary and an agent summary using\nthe same decoders from the conditional generative\nmodule, which makes the generated summaries\nsimilar to the utterances in pronouns and language\nstyles.\nWe re-encode the generated summaries into eX\nand eY with the same encoders and compare them\nwith each of the utterance embeddings using aver-\nage cosine distance. To constrain the summaries\nto be semantically close to input utterances, the\nsummarization modules are trained by maximizing\na similarity loss,\nLsum = 1\nn\nn\nX\ni=1\n(d(eX, e(i)\nx ) + d(eY , e(i)\ny )), (3)\nwhere d denotes the cosine distance.\nHowever, the summarization modules are prone\nto produce inaccurate factual details. We design a\nsimple but effective partial copy mechanism that\nemploys some extractive summarization tricks to\naddress this problem. We automatically make a\nlist of factual information from the data such as\ndates, locations, names, and numbers. Whenever\nthe decoder predicts a word from the factual infor-\nmation list, the copy mechanism replaces it with a\nword containing factual information from the input\nutterances. If there are multiple factual informa-\ntion words in the dialogue, the one with the highest\npredictive possibility will be chosen. Note that this\npartial copy mechanism does not need to be trained\nand is not activated during training.\n2.3\nTraining Process\nThe objective function we optimize is the weighted\nsum of the reconstruction loss in Equation 1 and\nthe similarity loss in Equation 3,\nL = αLgen + (1 −α)Lsum,\n(4)\nwhere α controls the weights of two objectives.\nSuTaT involves re-encoding the generated agent\nutterance to help with generating the customer ut-\nterance in Equation 2 and re-encoding the gener-\nated summary to compare with utterance embed-\ndings in Equation 3. Directly sampling from the\nmultinomial distribution with argmax is a non-\ndifferentiable operation, so we use the soft-argmax\ntrick (Chen et al., 2019) to approximate the deter-\nministic sampling scheme,\nyi = softmax(v(i)\nY /τ),\n(5)\nwhere τ ∈(0, 1) is the annealing parameter.\nAdam (Kingma and Ba, 2015) is adopted for\nstochastic optimization to jointly train all model\nparameters by maximizing Equation 4. In each\nstep, Adam samples a mini-batch of dialogues and\nthen updates the parameters (Zhang et al., 2018).\n3\nRelated Works\nDialogue Summarization\nEarly dialogue sum-\nmarization works mainly focus on extractively sum-\nmarizing using statistical machine learning meth-\nods (Galley, 2006; Xie et al., 2008; Wang and\nCardie, 2013). Abstractive dialogue summariza-\ntion has been recently explored due to the success\nof sequence-to-sequence neural networks. Pan et al.\n(2018) propose an enhanced interaction dialogue\nencoder and a transformer-pointer decoder to sum-\nmarize dialogues. Li et al. (2019) summarize multi-\nmodal meetings on another encoder-decoder struc-\nture. Some approaches design additional mecha-\nnisms in a neural summarization model to leverage\nauxiliary information such as dialogue acts (Goo\nand Chen, 2018), key point sequences (Liu et al.,\n2019), and semantic scaffolds (Yuan and Yu, 2019).\nHowever, these supervised methods can only use\nconcise topic descriptions or instructions as gold\nreferences while high-quality annotated dialogue\nsummaries are not readily available.\nUnsupervised Summarization\nMany extractive\nsummarization models do not require document-\nsummary paired data and instead they tackle a\nsentence-selection problem.\nTextRank (Mihal-\ncea and Tarau, 2004) and LexRank (Erkan and\nRadev, 2004) encode sentences as nodes in a graph\nto select the most representative ones as a sum-\nmary. Zheng and Lapata (2019) and Rossiello et al.\n(2017) advance upon TextRank and LexRank by\nusing BERT (Devlin et al., 2019) to compute sen-\ntence similarity and replacing TF-IDF weights with\nword2vec embeddings respectively. In abstractive\nsummarization, some approaches focus on learn-\ning unsupervised sentence compression with small-\nscale texts (Fevry and Phang, 2018; Baziotis et al.,\n2019; West et al., 2019), while TED (Yang et al.,\n2020) proposes a transformer-based architecture\nwith pretraining on large-scale data. MeanSum\n(Chu and Liu, 2019) generates a multi-document\nsummary by decoding the average encoding of the\ninput texts, where the autoencoder and the summa-\nrization module are interactive. Bražinskas et al.\n(2020) and Amplayo and Lapata (2020) extend\nMeanSum by using a hierarchical variational au-\ntoencoder and denoising a noised synthetic dataset.\nHowever, none of these methods accommodate the\nmulti-speaker scenario in dialogues.\n4\nExperimental Details\nWe perform experiments with two variants of Su-\nTaT: one equipped with LSTM encoders and de-\ncoders (SuTaT-LSTM), and the other equipped with\nTransformer encoders and decoders (SuTaT-Tran).\nModel\nMultiWOZ\nTaskmaster\nCustomer\nAgent\nCustomer\nAgent\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nLexRank\n23.54\n2.63\n13.43\n24.35\n2.79\n13.29\n21.64\n1.83\n12.86\n21.54\n1.90\n12.15\nWord2Vec\n23.80\n2.96\n13.37\n24.15\n2.72\n13.92\n21.43\n2.03\n12.32\n21.57\n2.07\n12.46\nMeanSum\n25.93\n4.42\n14.52\n26.49\n4.49\n15.43\n24.01\n3.31\n13.55\n24.08\n3.24\n14.31\nCopycat\n26.86\n4.81\n16.35\n26.92\n4.37\n16.12\n24.86\n4.23\n14.81\n25.05\n3.71\n15.19\nVAE\n26.08\n4.25\n14.84\n26.80\n3.76\n15.27\n24.29\n3.15\n14.40\n24.99\n3.29\n14.35\nSuTaT-LSTM\n28.51\n5.60\n17.20\n28.71\n5.67\n17.49\n26.61\n4.89\n16.09\n26.67\n4.80\n15.74\nSuTaT-Tran\n26.82\n4.80\n16.08\n27.11\n4.88\n15.52\n25.20\n3.98\n15.33\n25.19\n4.12\n14.81\nAblation Study (with LSTM Encoders and Decoders)\nSuTaT w/o LS\n24.78\n3.55\n14.08\n25.11\n4.09\n14.16\n23.05\n3.05\n13.00\n23.41\n3.15\n13.12\nSuTaT w/o Att\n26.69\n5.00\n15.59\n27.00\n5.26\n15.97\n25.08\n4.26\n14.65\n25.25\n4.28\n14.93\nSuTaT w/o copy\n27.65\n5.23\n16.01\n27.67\n5.47\n16.42\n25.28\n4.80\n14.97\n25.15\n4.47\n15.16\nTable 2: ROUGE scores on the MultiWOZ and Taskmaster test sets.\n4.1\nDataset\nThe experiments are conducted on two dialogue\ndatasets: MultiWOZ-2.0 (Budzianowski et al.,\n2018) and Taskmaster-1 (Byrne et al., 2019). Mul-\ntiWOZ consists of 10438 goal-oriented human-\nhuman written dialogues between customers and\nagents, spanning over 7 domains such as booking\nhotels, booking taxis, etc. 3406 of them are single-\nlabel and 7302 of them are multi-label. In the ex-\nperiment, we split the dataset into 8438, 1000, and\n1000 dialogues for training, testing, and valida-\ntion. Taskmaster consists of 13215 goal-oriented\ndialogues, including 5507 spoken and 7708 written\ndialogues. In this work we only use the written dia-\nlogues which is created by human workers based\non scenarios outlined for one of the six tasks, such\nas ordering pizza, ordering movie tickets, etc. The\ndataset is split into 6168, 770, and 770 dialogues\nfor training, testing, and validation.\n4.2\nBaselines\nTo validate the effectiveness of SuTaT, we compare\nthe two variants against the following baselines:\nunsupervised extractive summarization methods\nLexRank (Erkan and Radev, 2004) and Word2Vec\n(Rossiello et al., 2017); unsupervised abstractive\nsummarization methods MeanSum (Chu and Liu,\n2019) and Copycat (Bražinskas et al., 2020). In ad-\ndition, we train a vanilla text VAE model (Bowman\net al., 2016) with our unsupervised summarization\nmodule as another baseline.\nSince we are the ﬁrst work that summarizes for\neach speaker in a dialogue, some modiﬁcations\nneed to be made on baselines to make fair com-\nparisons with our model. To make the unsuper-\nvised summarization baseline models adapt to the\ntwo-speaker scenario in tete-a-tetes, we train two\nmodels for each baseline with either customer ut-\nterances or agent utterances. During testing, the\ncustomer summaries and agent summaries are gen-\nerated by the two trained models of each baseline,\nwhich are used either separately for automatic and\nhuman evaluation or concatenated together for the\nclassiﬁcation experiment.\n4.3\nSettings\nWe ﬁne-tune the parameters of SuTaT on the valida-\ntion set. VAE-based text generative models can suf-\nfer from posterior collapse where the model learns\nto ignore the latent variable (Bowman et al., 2016).\nWe employ KL-term annealing and dropping out\nwords during decoding to avoid posterior collapse.\nFor KL annealing, the initial weights of the KL\nterms are 0, and then we gradually increase the\nweights as training progresses, until they reach the\nKL threshold of 0.8; the rate of this increase is set\nto 0.5 with respect to the total number of batches.\nThe word dropout rate during decoding is 0.4. The\nlatent variable size is 300 for both customer and\nagent latent variables. α that controls weights of\ntwo objective functions in Equation 4 is set to 0.4.\nThe word embedding size is 300. For the bidi-\nrectional LSTM encoder and LSTM decoder, the\nnumber of hidden layers is 1 and the hidden unit\nsize is 600. For the Transformer encoder and de-\ncoder, the number of hidden layers is 1 and the\nnumber of heads in the multi-head attention is set\nto 10. The number of heads in the sentence-level\nself-attention is also 10. The hidden unit size of the\nMLPs in p(zy|zx) is 600. The annealing parameter\nτ for soft-argmax in Equation 5 is set to 0.01. Dur-\ning training, the learning rate is 0.0005, the batch\nsize is 16, and the maximum number of epoch is\n10. SuTaT is implemented in pytorch and trained\nusing a NVIDIA Tesla V100 GPU with 16GB.\n4.4\nReference Summaries\nIn this work, we deﬁne the dialogue summary as\nsummarizing for each speaker in a dialogue and\nthere is no such annotated dataset available. To\nvalidate the effectiveness of SuTaT and compare\nwith baselines, we follow the setting in (Chu and\nLiu, 2019) to collect 200 abstractive summaries\nfor a subset of each dataset. Workers were pre-\nsented with 10 dialogues from MultiWOZ and 10\ndialogues from Taskmaster and asked to write sum-\nmaries that “best summarize both the content and\nthe sentiment for each speaker”. We asked work-\ners to “write your summaries as if your were the\nspeaker (e.g. ‘I want to book a hotel.’ instead of\n‘The customer wants to book a hotel.’) and keep\nthe length of the summary no more than one sen-\ntence”. The collected summaries are only used as\nreference summaries for testing and not used for\nmodel-tuning. These reference summaries cover\nall domains in both datasets and will be released\nlater.\n5\nResults\nWe conduct the majority of experiments to show\nthe superiority of SuTaT on unsupervised dialogue\nsummarization. We use the labeled reference sum-\nmaries for ROUGE-score-based automatic evalua-\ntion and human evaluation to compare with base-\nline methods. We further demonstrate the effective-\nness of SuTaT by analyzing the language modeling\nresults and using generated summaries to perform\ndialogue classiﬁcation. In addition, we show that\nSuTaT is capable of single-turn conversation gener-\nation.\n5.1\nUnsupervised Dialogue Summarization\nAutomatic Evaluation\nROUGE (Lin, 2004) is a\nstandard summarization metric to measure the sur-\nface word alignment between a generated summary\nand the reference summary. In the experiments, we\nuse ROUGE-1, ROUGE-2, and ROUGE-L to mea-\nsure the word-overlap, bigram-overlap, and longest\ncommon sequence respectively. Table 2 shows the\nROUGE scores for two SuTaT variants and the\nbaselines. As we can see, our proposed SuTaT with\nLSTM encoders and decoders outperforms all other\nbaselines on both datasets. SuTaT-LSTM performs\nbetter than SuTaT-Transformer on ROUGE scores,\nthe reason could be that Transformer decoders are\ntoo strong so the encoders are weakened during\ntraining. In general, the unsupervised abstractive\nmodels perform better than unsupervised extrac-\ntive models. Compared with other unsupervised\nabstractive summarization baselines equipped with\nLSTM encoders and decoders, SuTaT-LSTM has a\nbig performance improvement. We believe this is\nbecause SuTaT accommodates the two-speaker sce-\nnario in tete-a-tetes so that the utterances from each\nspeaker and their correlations are better modeled.\nIn addition, we evaluate reconstruction perfor-\nmances of the language modeling based methods\nwith perplexity (PPL), and check the posterior col-\nlapse for the VAE-based methods with KL diver-\ngence. The results for MultiWOZ and Taskmaster\nare shown in Table 3. As can be seen, SuTaT-Tran\nhas much better PPL scores than other compet-\ning methods on both datasets, showing the trans-\nformer decoders are effective at reconstructing sen-\ntences.\nConsequently, due to the powerful de-\ncoders, SuTaT-Tran has smaller KL divergences\nwhich can lead to posterior collapse where the en-\ncoders tend to be ignored.\nHuman Evaluation\nHuman evaluation for the\ngenerated summaries is conducted to quantify the\nqualitative results of each model. We sample 50 di-\nalogues that are labeled with reference summaries\nfrom the MultiWOZ and taskmaster test set (25\neach). With the sampled dialogues, summaries\nare generated from the unsupervised abstractive\napproaches: MeanSum, Copycat, VAE, SuTaT-\nLSTM, and SuTaT-Tran. We recruit three workers\nto rank the generated summaries and reference sum-\nmaries from 6 (the best) to 1 (the worst) based on\nthree criteria: Informativeness: a summary should\npresent the main points of the dialogue in a concise\nversion; Readability: a summary should be gram-\nmatically correct and well structured; Correlation:\nthe customer summary should be correlated to the\nagent summary in the same dialogue.\nThe average ranking scores are shown in Ta-\nble 4. As we can see, SuTaT-LSTM achieves the\nbest informativeness and correlation results on both\ndatasets while SuTaT-Tran also has good perfor-\nmances, further demonstrating the ability of SuTaT\non generating informative and coherent dialogue\nsummaries. In general, the two SuTaT models have\nbetter human evaluation scores than baseline mod-\nModel\nMultiWOZ\nTaskmaster\nCustomer\nAgent\nCustomer\nAgent\nPPL\nKL\nPPL\nKL\nPPL\nKL\nPPL\nKL\nMeanSum\n3.58\n-\n3.65\n-\n5.57\n-\n5.48\n-\nCopycat\n3.46\n0.75\n3.42\n0.73\n5.41\n0.96\n5.23\n0.93\nVAE\n3.64\n0.50\n3.59\n0.48\n5.63\n0.63\n5.75\n0.66\nSuTaT-LSTM\n3.27\n0.79\n3.39\n0.82\n5.31\n1.02\n4.56\n0.88\nSuTaT-Tran\n1.77\n0.28\n2.10\n0.34\n2.48\n0.35\n2.52\n0.36\nTable 3: Language modeling results on MultiWOZ and Taskmaster. Lower is better for PPL.\nModel\nMultiWOZ\nTaskmaster\nInfo\nRead\nCorr\nInfo\nRead\nCorr\nReference\n5.43\n4.73\n4.52\n5.39\n4.57\n4.60\nMeanSum\n2.57\n3.15\n2.64\n2.98\n3.29\n3.05\nCopycat\n2.89\n3.37\n3.00\n3.04\n3.49\n3.07\nVAE\n2.96\n3.04\n2.44\n2.97\n2.92\n2.45\nSuTaT-LSTM\n3.68\n3.48\n4.25\n3.61\n3.53\n4.20\nSuTaT-Tran\n3.47\n3.56\n4.15\n3.33\n3.52\n3.96\nTable 4: Human evaluation results on informativeness,\nreadability, and correlation of generated summaries.\nels, especially on correlation scores where the re-\nsults are close to reference summaries. This is be-\ncause SuTaT exploits the dependencies between the\ncustomer latent space and the agent latent space,\nwhich results in generating more correlated cus-\ntomer summaries and agent summaries.\nAblation Study\nWe perform ablations to vali-\ndate each component of SuTaT by: removing the\nvariational latent spaces (SuTaT w/o LS) so the\nencoded utterances are directly used for embed-\nding, removing the sentence-level self-attention\nmechanism (SuTaT w/o Att), and removing the par-\ntial copy mechanism (SuTaT w/o copy). We use\nLSTM encoders and decoders for all ablation mod-\nels. The results for ablation study in Table 2 show\nthat all the removed components play a role in Su-\nTaT. Removing the latent spaces has the biggest\ninﬂuence on the summarization performance, indi-\ncating that the variational latent space is necessary\nto support our design which makes the agent latent\nvariable dependent on the customer latent variable.\nThe performance drop after removing the sentence-\nlevel self-attention mechanism shows that using\nweighted combined utterance embedding is better\nthan simply taking the mean of encoded utterances.\nRemoving the partial copy has the smallest quality\ndrop. However, taking the dialogue example in Ta-\nble 1, without the partial copy mechanism SuTaT\ncan generate the following summaries:\nModel\nMultiWOZ\nTaskmaster\nMeanSum\n0.76\n0.70\nCopycat\n0.77\n0.72\nVAE\n0.66\n0.62\nSuTaT (unsupervised)\n0.85\n0.79\nSuTaT (supervised)\n0.99\n0.96\nTable 5: AUC scores for domain classﬁcation with gen-\nerated summaries, where MultiWOZ is multi-label and\nTaskmaster is single-label.\nCustomer Summary: i would like to\nbook a hotel in cambridge on tuesday .\nAgent Summary: i have booked you a\nhotel . the reference number is lzludtvi .\ncan i help you with anything else ?\nThe generated summaries are the same except for\nthe wrong reference number which is crucial infor-\nmation in this summary.\n5.2\nClassiﬁcation with Summaries\nA good dialogue summary should reﬂect the key\npoints of the utterances. We perform dialogue clas-\nsiﬁcation based on dialogue domains to test the va-\nlidity of generated summaries. First we encode the\ngenerated customer summary and agent summary\ninto eX and eY using the trained encoders of each\nmodel, which are then concatenated as features of\nthe dialogue for classiﬁcation. In this way, the dia-\nlogue features are obtained unsupervisedly. Then\nwe train a separate linear classiﬁer on top of the\nencoded summaries. We use SuTaT with LSTM\nencoders and decoders for this task. As shown\nin Table 5, SuTaT outperforms other baselines on\ndialogue classiﬁcation, indicating the SuTaT gen-\nerated summaries have better comprehension of\ndomain information in the dialogue.\nWe can also perform supervised classiﬁcation\nby using sX and sY from SuTaT as features to\ntrain a linear classiﬁer. The cross entropy loss is\ncombined with Equation 4 as the new objective\nCustomer: yes , yes . are there any multiple sports places\nthat i can visit in ?\nAgent:\nsorry , there are none locations in the center\nof town . would you like a different area ?\nCustomer: yes please . book for the same group of people\nat 13:45 on thursday .\nAgent:\nyour booking was successful and your refer-\nence number is minorhoq .\nCustomer: hi , i am looking for a place to stay . the\nwest should be cheap and doesn’t need to have\ninternet .\nAgent:\nthere are no hotels in the moderate price range\n. would you care to expand other criteria ?\nTable 6: Examples of single-turn conversations gener-\nated by the conditional generative module of SuTaT.\nfunction where all parameters are jointly optimized.\nAs can be seen in Table 5, the supervised classi-\nﬁcation results are as high as 0.99 on MultiWOZ\nand 0.96 on Taskmaster, further demonstrating the\neffectiveness of SuTaT.\n5.3\nSingle-Turn Conversation Generation\nThe design of the conditional generative module in\nSuTaT enables generating novel single-turn conver-\nsations. By sampling the customer latent variable\nfrom the standard Gaussian zx ∼N(0, I) and then\nsampling the agent latent variable zy ∼p(zy|zx),\nSuTaT can produce realistic-looking novel dialogue\npairs using the customer decoder and agent decoder.\nTable 6 shows three examples of novel single-turn\nconversations generated by SuTaT using randomly\nsampled latent variables. We can see that the di-\nalogue pairs are closely correlated, meaning the\ndependencies between two latent spaces are suc-\ncessfully captured.\n6\nConclusion\nWe propose SuTaT, an unsupervised abstractive\ndialogue summarization model, accommodating\nthe two-speaker scenario in tete-a-tetes and sum-\nmarizing them without using any data annotations.\nThe conditional generative module models the cus-\ntomer utterances and agent utterances separately\nusing two encoders and two decoders while re-\ntaining their correlations in the variational latent\nspaces. In the unsupervised summarization module,\na sentence-level self-attention mechanism is used\nto highlight more informative utterances. The sum-\nmary representations containing key information of\nthe dialogue are decoded using the same decoders\nfrom the conditional generative module, with the\nhelp of a partial copy mechanism, to generate a\ncustomer summary and an agent summary. The\nexperimental results show the superiority of SuTaT\nfor unsupervised dialogue summarization and the\ncapability for more dialogue tasks.\nReferences\nReinald Kim Amplayo and Mirella Lapata. 2020. Un-\nsupervised opinion summarization with noising and\ndenoising. In Proceedings of the Annual Meeting\nof the Association for Computational Linguistics\n(ACL).\nChristos Baziotis, Ion Androutsopoulos, Ioannis Kon-\nstas, and Alexandros Potamianos. 2019.\nSeqˆ\n3: Differentiable sequence-to-sequence-to-sequence\nautoencoder for unsupervised abstractive sentence\ncompression. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL).\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew M Dai, Rafal Jozefowicz, and Samy Ben-\ngio. 2016. Generating sentences from a continuous\nspace. In Proceedings of the Conference on Compu-\ntational Natural Language Learning (CoNLL).\nArthur Bražinskas, Mirella Lapata, and Ivan Titov.\n2020.\nUnsupervised opinion summarization as\ncopycat-review generation.\nIn Proceedings of the\nAnnual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Inigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši´c. 2018. Multiwoz-a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling.\nProceedings of the\nconference on empirical methods in natural lan-\nguage processing (EMNLP).\nBill Byrne, Karthik Krishnamoorthi, Chinnadhurai\nSankar, Arvind Neelakantan, Daniel Duckworth,\nSemih Yavuz, Ben Goodrich, Amit Dubey, Andy\nCedilnik, and Kyu-Young Kim. 2019. Taskmaster-\n1: Toward a realistic and diverse dialog dataset. In\nProceedings of the conference on empirical methods\nin natural language processing (EMNLP).\nLiqun Chen, Yizhe Zhang, Ruiyi Zhang, Chenyang\nTao, Zhe Gan, Haichao Zhang, Bai Li, Dinghan\nShen, Changyou Chen, and Lawrence Carin. 2019.\nImproving sequence-to-sequence learning via opti-\nmal transport. In Proceedings of the International\nConference on Learning Representations (ICLR).\nEric Chu and Peter J Liu. 2019. Meansum: a neural\nmodel for unsupervised multi-document abstractive\nsummarization. In Proceedings of the International\nConference on Machine Learning (ICML).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics (NAACL).\nGünes Erkan and Dragomir R Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text\nsummarization. Journal of artiﬁcial intelligence re-\nsearch.\nThibault Fevry and Jason Phang. 2018.\nUnsuper-\nvised sentence compression using denoising auto-\nencoders. In Proceedings of the Conference on Com-\nputational Natural Language Learning (CoNLL).\nMichel Galley. 2006.\nA skip-chain conditional ran-\ndom ﬁeld for ranking meeting utterances by impor-\ntance.\nIn Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nChih-Wen Goo and Yun-Nung Chen. 2018. Abstrac-\ntive dialogue summarization with sentence-gated\nmodeling optimized by dialogue acts. In IEEE Spo-\nken Language Technology Workshop (SLT).\nAlex Graves, Navdeep Jaitly, and Abdel-rahman Mo-\nhamed. 2013. Hybrid speech recognition with deep\nbidirectional lstm. In IEEE workshop on automatic\nspeech recognition and understanding.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the International Conference on Learning Repre-\nsentations (ICLR).\nDiederik P Kingma and Max Welling. 2014.\nAuto-\nencoding variational bayes. In Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nManling Li, Lingyu Zhang, Heng Ji, and Richard J\nRadke. 2019.\nKeep meeting summaries on topic:\nAbstractive multi-modal meeting summarization. In\nProceedings of the Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In ACL Workshop on Text\nSummarization Branches Out.\nChunyi Liu, Peng Wang, Jiang Xu, Zang Li, and\nJieping Ye. 2019.\nAutomatic dialogue summary\ngeneration for customer service. In ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining.\nRada Mihalcea and Paul Tarau. 2004. Textrank: Bring-\ning order into text. In Proceedings of the conference\non empirical methods in natural language process-\ning (EMNLP).\nHaojie Pan, Junpei Zhou, Zhou Zhao, Yan Liu, Deng\nCai, and Min Yang. 2018.\nDial2desc:\nend-to-\nend dialogue description generation. arXiv preprint\narXiv:1811.00185.\nGaetano Rossiello, Pierpaolo Basile, and Giovanni Se-\nmeraro. 2017. Centroid-based text summarization\nthrough compositionality of word embeddings. In\nProceedings of the MultiLing Workshop on Sum-\nmarization and Summary Evaluation Across Source\nTypes and Genres.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL).\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems (NeurIPS).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems (NeurIPS).\nLu Wang and Claire Cardie. 2013.\nDomain-\nindependent abstract generation for focused meeting\nsummarization. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nPeter West, Ari Holtzman, Jan Buys, and Yejin Choi.\n2019. Bottlesum: Unsupervised and self-supervised\nsentence summarization using the information bot-\ntleneck principle. In Proceedings of the conference\non empirical methods in natural language process-\ning (EMNLP).\nShasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating\nthe effectiveness of features and sampling in extrac-\ntive meeting summarization. In 2008 IEEE Spoken\nLanguage Technology Workshop (SLT).\nZiyi Yang, Chenguang Zhu, Robert Gmyr, Michael\nZeng, Xuedong Huang, and Eric Darve. 2020. Ted:\nA pretrained unsupervised summarization model\nwith theme modeling and denoising. arXiv preprint\narXiv:2001.00725.\nLin Yuan and Zhou Yu. 2019. Abstractive dialog sum-\nmarization with semantic scaffolds. arXiv preprint\narXiv:1910.00825.\nXinyuan Zhang,\nYitong Li,\nDinghan Shen,\nand\nLawrence Carin. 2018. Diffusion maps for textual\nnetwork embedding. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS).\nXinyuan Zhang, Yi Yang, Siyang Yuan, Dinghan Shen,\nand Lawrence Carin. 2019.\nSyntax-infused varia-\ntional autoencoder for text generation. In Proceed-\nings of the Annual Meeting of the Association for\nComputational Linguistics (ACL).\nHao Zheng and Mirella Lapata. 2019. Sentence cen-\ntrality revisited for unsupervised summarization. In\nProceedings of the Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-09-15",
  "updated": "2020-09-15"
}