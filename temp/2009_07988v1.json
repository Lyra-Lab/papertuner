{
  "id": "http://arxiv.org/abs/2009.07988v1",
  "title": "Deep Collective Learning: Learning Optimal Inputs and Weights Jointly in Deep Neural Networks",
  "authors": [
    "Xiang Deng",
    "Zhongfei",
    "Zhang"
  ],
  "abstract": "It is well observed that in deep learning and computer vision literature,\nvisual data are always represented in a manually designed coding scheme (eg.,\nRGB images are represented as integers ranging from 0 to 255 for each channel)\nwhen they are input to an end-to-end deep neural network (DNN) for any learning\ntask. We boldly question whether the manually designed inputs are good for DNN\ntraining for different tasks and study whether the input to a DNN can be\noptimally learned end-to-end together with learning the weights of the DNN. In\nthis paper, we propose the paradigm of {\\em deep collective learning} which\naims to learn the weights of DNNs and the inputs to DNNs simultaneously for\ngiven tasks. We note that collective learning has been implicitly but widely\nused in natural language processing while it has almost never been studied in\ncomputer vision. Consequently, we propose the lookup vision networks\n(Lookup-VNets) as a solution to deep collective learning in computer vision.\nThis is achieved by associating each color in each channel with a vector in\nlookup tables. As learning inputs in computer vision has almost never been\nstudied in the existing literature, we explore several aspects of this question\nthrough varieties of experiments on image classification tasks. Experimental\nresults on four benchmark datasets, i.e., CIFAR-10, CIFAR-100, Tiny ImageNet,\nand ImageNet (ILSVRC2012) have shown several surprising characteristics of\nLookup-VNets and have demonstrated the advantages and promise of Lookup-VNets\nand deep collective learning.",
  "text": "1\nDeep Collective Learning: Learning Optimal Inputs\nand Weights Jointly in Deep Neural Networks\nXiang Deng, Zhongfei (Mark) Zhang\nAbstract—It is well observed that in deep learning and\ncomputer vision literature, visual data are always represented\nin a manually designed coding scheme (eg., RGB images are\nrepresented as integers ranging from 0 to 255 for each channel)\nwhen they are input to an end-to-end deep neural network\n(DNN) for any learning task. We boldly question whether the\nmanually designed inputs are good for DNN training for different\ntasks and study whether the input to a DNN can be optimally\nlearned end-to-end together with learning the weights of the\nDNN. In this paper, we propose the paradigm of deep collective\nlearning which aims to learn the weights of DNNs and the\ninputs to DNNs simultaneously for given tasks. We note that\ncollective learning has been implicitly but widely used in natural\nlanguage processing while it has almost never been studied in\ncomputer vision. Consequently, we propose the lookup vision\nnetworks (Lookup-VNets) as a solution to deep collective learning\nin computer vision. This is achieved by associating each color\nin each channel with a vector in lookup tables. As learning\ninputs in computer vision has almost never been studied in the\nexisting literature, we explore several aspects of this question\nthrough varieties of experiments on image classiﬁcation tasks.\nExperimental results on four benchmark datasets, i.e., CIFAR-\n10, CIFAR-100, Tiny ImageNet, and ImageNet (ILSVRC2012)\nhave shown several surprising characteristics of Lookup-VNets\nand have demonstrated the advantages and promise of Lookup-\nVNets and deep collective learning.\nIndex Terms—Deep Learning, Optimal Inputs, Computer Vi-\nsion, Lookup Tables.\nI. INTRODUCTION\nT\nHE advent of large datasets and computing resources\nhas made deep neural networks (DNNs) as the most\npopular technology for varieties of applications [1], [2], [3],\n[4] in computer vision. One of the most used data types in\ndeep vision models [5], [6], [7], [8], [9], [10] is images. It\nis observed that image pixels are represented by integers as\nthey are almost always coded in discrete color spaces. For\nexample, in the RGB space, the pixel values are 8-bit integers\n(0 to 255). The integers representing image pixels are ﬁrst\nstandardized (i.e., a manually designed linear function) and\nthen used as the inputs to DNNs for any learning task. Thus,\nthe data source of the inputs to DNNs is integers which are\ninvolved in the gradient calculation during the training process.\nThe manually designed inputs have a strong assumption that\nthe color changes in images cause linear value changes in\nthe inputs. In lights of this, we boldly question whether the\nXiang Deng is with the Computer Science Department, Watson School,\nState University of New York at Binghamton, Binghamton, NY 13902, USA\n(e-mail: xdeng7@binghamton.edu).\nZhongfei Zhang is with the Computer Science Department, Watson School,\nState University of New York at Binghamton, Binghamton, NY 13902, USA\n(e-mail: zhongfei@cs.binghamton.edu).\nFig. 1. Lookup-VNet\nmanually designed inputs are good for DNN training for\ndifferent tasks although a DNN with a proper size theoretically\ncan approximate any function [11], [12]. We study whether\nthe inputs to DNNs can be learned automatically in computer\nvision. We take the images in the RGB color space as the\nexamples in this paper, but the idea can be easily extended to\nthe images (or videos) in other discrete color spaces.\nIn standard DNNs for computer vision such as VGG [13]\nand ResNet [5], only the weights are learned with the RGB\ninputs during the training process. Based on the idea of learn-\ning, advanced efforts go beyond learning weights and propose\nto learn the activation function [14], [15], the pooling function\n[16], and the optimal regularizer [17] by parameterizing them\ninstead of using manually designed functions. However, to\nthe best of our knowledge, learning the inputs to DNNs\nin computer vision has never been explored in the existing\nliterature.\nIn the paper, we propose deep collective learning which\naims to learn weights in DNNs and inputs to DNNs jointly\ninstead of learning weights alone. Deep collective learning\nhas been implicitly but successfully used in natural language\nprocessing (NLP) where a character or a word is associated\nwith a learned vector [18], [19] as the input to a DNN [20], but\nit has not been studied in computer vision. In light of this, we\npropose lookup vision networks (Lookup-VNets) as a solution\nto deep collective learning in computer vision.\nAs shown in Figure 1, a Lookup-VNet comprises a DNN\nand three lookup tables corresponding to the three GRB\nchannels. The lookup tables are used to parameterize the inputs\nby associating each color in each channel with a vector. The\npixel colors in images are used as indices to look up the\nthree tables. The results are fed into the DNN as the inputs\nto generate the outputs. The lookup tables are not designed\nmanually but learned jointly with the weights of the DNN.\nWe propose two kinds of lookup tables for Lookup-VNets\naccording to whether the pixel color space is compressed, i.e.,\nfull lookup tables and compressed lookup tables. Moreover, we\nintroduce three kinds of table learning strategies, i.e., single-\ntask and single-network learning, cross-network learning, and\narXiv:2009.07988v1  [cs.LG]  17 Sep 2020\n2\ncross-task learning.\nLookup-VNets possess several inherent advantages over the\nstandard DNNs. First, Lookup-VNets enable DNNs to learn\nthe optimal inputs end-to-end for given tasks. Second, from\nthe perspective of image coding, Lookup-VNets can be used\nto learn the optimal image coding scheme automatically for\na given criterion. For example, for the goal of image storage\ncompression with the criterion of accuracy, experimental re-\nsults show that the pixel color space can be compressed 4096\n(163) times (from 256 × 256 × 256 colors to 16 × 16 ×\n16 colors) without accuracy dropping on CIFAR-10, which\nindicates that the pixel bits can be reduced from 24 (8×3)\nbits to 12 (4×3) bits under this setting.\nOn the other hand, due to the vacancy of the existing\nliterature on deep collective learning in computer vision,\nwe explore various aspects of this question with Lookup-\nVNets, such as vector dimensions in lookup tables, pixel\ncolor space compression, and table learning strategies, through\nvarieties of experiments on four benchmark datasets, i.e.,\nCIFAR-10 [21], CIFAR-100 [21], Tiny ImageNet1, and Im-\nageNet (ILSVRC2012) [22]. The experimental results show\nthat Lookup-VNets are able to match the performances of the\ncorresponding standard DNNs on CIFAR-10, CIFAR-100, and\nTiny ImageNet while achieving better performances on the\nlarge-scale and challenging dataset ImageNet than those of the\ncorresponding standard DNNs, which indicates the superiority\nof Lookup-VNets on large-scale and challenging datasets and\nthe promise of deep collective learning in computer vision.\nWe also observe several surprising characteristics of Lookup-\nVNets: (1) the vector dimensions in lookup tables have no\ninﬂuence on the test performance (generalization ability) of\nLookup-VNets; (2) the commonly used color space can be\ncompressed up to 4096 times without accuracy dropping on\nCIFAR-10, and 3375 times on CIFAR-100 and Tiny ImageNet.\nThe main contributions of our work can be summarized as\nfollows:\n• We have studied a new question on whether the inputs\nto deep vision networks can be optimally learned end-to-\nend and have proposed a new paradigm of deep collective\nlearning which aims to learn weights in DNNs and inputs\nto DNNs simultaneously for given tasks.\n• We have proposed a novel framework Lookup-VNets as\na solution to deep collective learning in computer vision.\nLookup-VNets make the inputs to DNNs more ﬂexible\nas an end-to-end solution. From the perspective of image\ncoding, Lookup-VNets can be used to learn the optimal\nimage coding scheme for given goals.\n• Due to the lack of the research on collective learning\nin computer vision, we explore various aspects of this\nquestion with Lookup-VNets such as vector dimensions\nof lookup tables, pixel color space compression, and table\nlearning strategies.\n• Through varieties of experiments on four benchmark\ndatasets, we have found several surprising characteristics\nof Lookup-VNets. The experimental results have also\ndemonstrated the promise and advantages of Lookup-\n1http://tiny-imagenet.herokuapp.com/\nVNets, which indicates the potential of deep collective\nlearning in computer vision.\nII. RELATED WORK\nIn this part, we ﬁrst review the literature on deep collective\nlearning in NLP. Then we review the work on learning\ncomponents of DNNs in computer vision.\nA. Deep Collective Learning in NLP\nDeep Collective Learning has been implicitly but success-\nfully used in the area of NLP. These efforts in NLP mainly\nassociate each character [23], subword unit [24], [25] or word\n[18], [26] with a highly dimensional vector in lookup tables.\nThese vectors are learned for a task, which means the vectors\nthat the lookup tables assign to the characters or words are\nnot designed manually but discovered automatically in the\ntraining process of a neural network on a particular task.\nLearning lookup tables (embeddings) has been well studied in\nNLP with large amounts of well known approaches including\nbut not limited to [18], [26], [24], [25], [23], [27], [28].\nThese learned vectors in lookup tables are used as inputs\nto neural networks (eg., LSTM [29]) [30], [31], [32] and\nare optimized in the training process of a neural network\nfor a given task, which implies the idea of deep collective\nlearning. However, deep collective learning has never been\nstudied in computer vision. We propose Lookup-VNets as a\nsolution to this problem in computer vision, which tries to\nautomatically learn representations for pixel colors to replace\nthe ﬁxed integer representations in RGB space.\nB. Learning Components of DNNs\nFrom another perspective, Lookup-VNets can be considered\nas learning a component (i.e., the input) of a DNN. Thus, it is\nalso related to the efforts on learning the components of DNNs.\nHe et al. [33] and Agostinelli et al. [14] propose to learn\nthe activation function end-to-end by parameterizing them. Lin\net al. [34], Zhu et al. [35], and Sun et al. [36] propose to\nlearn the pooling strategies based on the attention mechanism\ninstead of using the manually designed ones such as average\npooling and max pooling. Streeter et al. [17] propose to learn\nthe optimal regularizer instead of using the manually ﬁne-\ntuned ones. However, to the best of our knowledge, learning\nthe input to a DNN has never been explored in the existing\nliterature.\nIII. OUR FRAMEWORK\nTo illustrate the connection between standard DNNs and\nLookup-VNets, we ﬁrst review the standard DNNs which only\nlearn the weights with RGB inputs during the training process\nwithout computing the gradients with respect to the inputs.\nThen we present Lookup-VNets which learn the weights\nand inputs jointly by associating each color with a vector.\nSpeciﬁcally, we ﬁrst introduce the full lookup tables and\nthe compressed lookup tables in Lookup VNets; then we\npresent different learning strategies for lookup tables; ﬁnally\nwe provide the additional space and computation costs. We\n3\nFig. 2. Full Lookup Tables: vectors r0 (g0, b0), r1 (g1, b1), ..., and r256 (g256, b256) are different vectors.\nrestate that the images in the RGB color space are used as the\nexamples in this paper, but the idea can be easily extended to\nthe images (or videos) in other discrete color spaces.\nA. Standard Deep Neural Networks\nGiven the training data (X, Y ) where X are the images and\nY are the targets, the outputs of a DNN f with weights W\nare f(X, W), and the loss is written as:\nloss = L(f(X, W), Y )\n(1)\nwhere L(.) is any loss function, such as the mean square error\nor cross entropy.\nIn the training process, the loss function is minimized by\nthe gradient descent based optimizer such as SGD or Adam\n[37]. W are updated iteratively based on the gradients on a\nmini-batch of training samples and the update in step t can be\nsimply expressed as:\nWt+1 = Wt −λ∂L(f(Wt, xt\nbat), yt\nbat)\n∂Wt\n(2)\nwhere Wt are the values of W in step t and (xt\nbat, yt\nbat) are a\nmini-batch of training data sampled in step t. As seen from (2),\nonly the weights are learned during the training process for\na standard DNN. Lookup-VNets take a further step to update\nthe inputs and weights simultaneously.\nB. Lookup-VNets\nDifferent from the standard DNNs, Lookup-VNets learn the\nweights and inputs jointly instead of learning weights alone.\nAs shown in Figure 1, a Lookup-VNet consists of a DNN and\nthree lookup tables. The lookup tables associate each color in\neach channel with a vector. According to whether the color\nspace is compressed, we introduce two kinds of lookup tables\nfor Lookup-VNets, i.e., full lookup tables and compressed\nlookup tables. We also develop three different strategies for\nlearning lookup tables, i.e., single-network and single-task\nlearning, cross-network learning, and cross-task learning.\n1) Full Lookup Tables: The images in the RGB space\nhave three channels, i.e., the red (R) channel, the green (G)\nchannel, and the blue (B) channel. Each channel has 256 colors\nrepresented by 8-bit integers (0 to 255), so the full RGB color\nspace is 256 × 256 × 256. The full lookup tables keep the\ncolor space size constant. As shown in Figure 2, there are three\nfull lookup tables corresponding to the three RGB channels,\nand the 256 colors in each channel are associated with 256\ndistinct vectors. Thus, the color space is still 256 × 256 × 256.\nThe vector dimension in lookup tables is a hyperparameter,\nand how it inﬂuences the performances is explored in Section\nIV.\n2) Compressed Lookup Tables: Full lookup tables map\ndifferent colors to different vectors so that the color space\nis still 256×256×256. We question the necessity of the\nlarge color space and propose compressed lookup tables. The\ncompressed lookup tables compress the color space with a\ncompressing rate (CMP-Rate) c. As shown in Figure 3, every\nc colors in each channel are mapped to a number in the\ncompressed lookup tables, and then there are ⌈256\nc ⌉colors\nin each channel. Therefore, the whole RGB color space is\ntotally compressed about c3 times, i.e., from 256×256×256 to\n⌈256\nc ⌉×⌈256\nc ⌉×⌈256\nc ⌉. It is worth noting that in the compressed\ntables, every c colors in a channel are mapped to a number not\na vector for the goal of compression. We study how the CMP-\nRate c affects the performances of Lookup-VNets in Section\nIV. An obvious advantage of compressed lookup-tables is that\nthey can be used to save image storage space as each pixel is\nrepresented with less bits.\n3) Single-Task and Single-Network Learning: In this part,\nwe show how the lookup tables are learned in the scenario of\nsingle task and single network. For an RGB image x with size\nm×n×3 where m, n, and 3 are the height, the width, and the\nchannel number, respectively, we use the pixel colors in each\nchannel as indices to look up each lookup table and obtain x′:\nx′ = lookup(x, T)\n(3)\nwhere lookup(x, T) denotes the result obtained from using the\npixels in image x as indices to look up the tables T. The size of\nx′ is determined by the vector dimension in the tables. Suppose\nthat the vector dimension is u (u is set to 1 in compressed\nlookup tables); then the size of x′ is m×n×3u. The reason is\nthat each color in each channel of the RGB space is denoted\nby an integer, but each color in the lookup tables is represented\nby a vector with u values. x′ is used as the input to the DNN,\nso that the loss of the Lookup-VNet is:\nC(f, X, Y, T, W) = L(f(W, X′), Y )\n(4)\nwhere X′ are the results of lookup(X, T).\n4\nFig. 3. Compressed Lookup Tables with CMP-Rate c: r0 (g0, b0), r1 (g1, b1), ..., and r⌊256\nc ⌋(g⌊256\nc ⌋, b⌊256\nc ⌋) are different numbers.\nNote that in (4), X′ contain the parameters from the lookup\ntables T. The weights W and the lookup tables T are learned\nsimultaneously during the whole training process through a\ngradient descent based optimizer:\nΘt+1 = Θt −λ∂L(f(Wt, x\n′t\nbat), yt\nbat)\n∂Θt\n(5)\nwhere Θ = [W, T], i.e., the list of all parameters in W and\nT; Θt are the values of Θ in step t; x\n′t\nbat are the results of\nlookup(xt\nbat, T); and λ is the learning rate.\n4) Cross-Network Learning: Beside learning lookup tables\non a task with a DNN, lookup tables can also be learned\nacross two or more networks for a task. Suppose that there\nare two DNNs f and g with shared lookup tables T for a task\nwith the training data (X, Y ). We alternately optimize the loss\nfunctions of f and g on the task with a gradient descent based\noptimizer, which can be simply written as:\nΘf\nt+1 = Θf\nt −λf\n∂L(f(W f\nt , x\n′t\nbat), yt\nbat)\n∂Θf\nt\n(6)\nwhere Θf = [W f, T]; W f and W g are the weights of f and\ng, respectively; Θf\nt are the values of Θf in step t; and λf is\nthe learning rate for training f.\nΘg\nt+1 = Θg\nt −λg\n∂L(f(W g\nt , x\n′t\nbat), yt\nbat)\n∂Θg\nt\n(7)\nBy alternately executing (6) and (7), we learn the lookup\ntables across two networks f and g.\n5) Cross-Task Learning: To further explore learning inputs\nto DNNs, we introduce learning lookup tables across tasks.\nIntuitively, the lookup tables learned across two or more tasks\nare more robust than those learned on one task. Suppose that f\nand g are two DNNs with shared tables T for two tasks p and\nq with the training data (Xp, Yp) and (Xq, Yq), respectively.\nWe alternately optimize the loss functions of f on task p and\ng on task q with gradient descent, which is written as:\nΘf\nt+1 = Θf\nt −λf\n∂L(f(W f\nt , x\n′t\npbat), yt\npbat)\n∂Θf\nt\n(8)\nΘg\nt+1 = Θg\nt −λg\n∂L(f(W g\nt , x\n′t\nqbat), yt\nqbat)\n∂Θg\nt\n(9)\nBy alternately executing (8) and (9), the lookup tables are\nlearned across two different tasks p and q.\nC. Additional Costs of Lookup-VNets Compared with Stan-\ndard Deep Neural Networks\nIn this part, we provide the additional space and compu-\ntation costs of a Lookup-VNet compared with those of the\ncorresponding standard DNN.\n1) Additional Space Cost: When a standard DNN is con-\nverted to the corresponding Lookup-VNet with lookup tables\nof 1-dimension vectors, the whole network architecture re-\nmains the same. The only additional parameters are from three\nlookup tables with 768 (256×3) parameters. When the vector\ndimension is greater than 1, only the ﬁrst layer of the standard\nDNN needs to be changed. Without loss of generality, we\nassume that the ﬁrst layer of a standard DNN is a convolutional\nlayer with kernel size k × k and kernel number j. Then the\nparameter number in the ﬁrst layer is k×k×3×j where 3 is\nthe channel number of the input image in the RGB space.\nFor the corresponding Lookup-VNet, suppose that the vector\ndimension of the lookup tables is u; then the space cost for the\nthree lookup tables is 256×3×u. As each color in each channel\nis mapped into a vector with dimension u, the input channel\nnumber is changed from 3 to 3u and the parameter number in\nthe ﬁrst layer is changed to k×k×3u×j. Therefore, the total\nadditional parameter number is 256×3×u+k×k×3(u−1)×j.\nThe experiments suggest that the vector dimension almost has\nno inﬂuence on the performances of Lookup-VNets. Thus, the\nadditional cost is almost ignorable as we can always take\nsmall vector dimensions. For example, the total parameter\nnumber of the standard VGG-16 [13] is 138.4 million while\nthe additional parameter number brought by the corresponding\nLookup-VNet with vector dimension 1 is only 768 which is\nignorable compared with 138.4 million.\n2) Additional Computation Cost: The additional computa-\ntion cost in the forward propagation is related to the input\nimage size. Suppose that the input image size in the standard\nDNN is m×n×3 where m, n, and 3 are the height, the\nwidth, and the channel number, respectively, and assume\nthat the computation cost for looking up tables equals to\nthe number of query indices. Then the computation cost\n5\nfor looking up tables is m×n×3. Suppose both the vertical\nand horizontal strides in the ﬁrst convolutional layer are s\nand the padding strategy is adopted. Then the additional\ncomputation cost in the ﬁrst layer of the Lookup-VNet is\n⌈m\ns ⌉×⌈n\ns ⌉×j×(2k2×3u+1)−⌈m\ns ⌉×⌈n\ns ⌉×j×(2k2×3+1)\nﬂoats. The total additional cost is m × n × 3 + ⌈m\ns ⌉× ⌈n\ns ⌉×\nj × (2k2 × 3u + 1) −⌈m\ns ⌉× ⌈n\ns ⌉× j × (2k2 × 3 + 1) ﬂoats.\nNote that when the vector dimension u is set to 1, the only\nadditional cost is m × n × 3 for looking up tables.\nIV. EXPERIMENTS\nIn the section, we report a series of experiments conducted\non image classiﬁcation tasks to explore deep collective learn-\ning in computer vision with Lookup-VNets. The code will be\nreleased. Through these experiments, we intend to answer the\nfollowing questions:\n• Q1: How much is the accuracy gap between Lookup-\nVNets and the corresponding standard DNNs?\n• Q2: How does the vector dimension in the lookup tables\ninﬂuence the performances of Lookup-VNets?\n• Q3: How does the CMP-Rate (the rate of compressing\nthe color space) inﬂuence the performances of Lookup-\nVNets?\n• Q4: How does the table learning strategy inﬂuence the\nthe performances of Lookup-VNets?\n• Q5: How do images look like when they are represented\nby the learned lookup tables?\nA. Datasets\nIn the experiments, we adopt four benchmark datasets:\nCIFAR-10 [21], CIFAR-100 [21], Tiny ImageNet 2, and Ima-\ngeNet (ILSVRC2012) [22].\nCIFAR-10 is an image classiﬁcation dataset with 10 classes,\ncontaining 50,000 training images and 10,000 test images with\nimage size 32 × 32 in the RGB space. We follow the standard\ndata augmentation on CIFAR datasets. During training time,\nwe pad 4 pixels on each side of an image and randomly ﬂip\nit horizontally. Then the image is randomly cropped to 32 ×\n32 size. During test time, we only evaluate the single view of\nan original 32 × 32 image without padding or cropping.\nCIFAR-100 comprises similar images to those in CIFAR-\n10, but has 100 classes. We adopt the same data augmentation\nstrategy as that in CIFAR-10.\nTiny ImageNet, i.e., a subeset of ImageNet, is an image\nclassiﬁcation dataset with 200 classes, containing 100,000\ntraining images and 10,000 test images with size 64 × 64\nin the RGB space. At training time, we pad 8 pixels on each\nside of an image and randomly ﬂip it horizontally, then the\nimage is randomly cropped to 64 × 64 size. At test time, we\nonly evaluate the original image.\nImageNet is a large-scale image classiﬁcation dataset with\n1000 classes, containing 1.28 million training images and\n50,000 validation images with different sizes in the RGB\nspace. On ImageNet, to reduce the CPU burden, we adopt\na simpler data augmentation strategy than that in the models\n2http://tiny-imagenet.herokuapp.com/\npretrained by Facebook 3. Speciﬁcally, we use a simple scale\nand aspect ratio augmentation strategy from [38]. Test images\nare resized so that the shorter side is set to 256, and then are\ncropped to size 224 × 224.\nNote that in Lookup-VNets, data preprocessing is not\nneeded as the inputs are learned end-to-end . However, in\nstandard DNNs, data preprocessing is necessary as their inputs\nare images which are represented by 8-bit integers ranging\nfrom 0 to 255. Therefore, to ensure the performances of\nstandard DNNs (the baselines), we use data preprocessing\nfor them. On CIFAR and Tiny-ImageNet datasets, we use\nthe widely used data proprocessing strategy: each image is\npreprocessed by subtracting its mean and dividing it by its\nstandard deviation. On ImageNet, each image is preprocessed\nby subtracting the mean of the whole training set and dividing\nit by the standard deviation.\nIn every case below, the experiments are repeated three\ntimes and then we report the average test accuracy as the\nvariance is quite small.\nB. Architectures and Experiment Setup\nWe adopt the widely used network architectures ResNet [5],\nVGG [13], and WRN [39]. Speciﬁcally, ResNet-20, VGG-16,\nand WRN-40-4 are adopted for CIFAR-10, CIFAR-100, and\nTiny ImageNet, and ResNet-18 and ResNet-34 are used for\nImageNet. VGG-16 is modiﬁed for small images as suggested\nby [40], i.e., replacing the two fully connected layers of 4096\nneurons with one fully connected layer with 512 neurons.\nOn CIFAR-10 and CIFAR-100, we use the training hyperpa-\nrameters in the original studies to train ResNet-20 and WRN-\n40-4. For VGG-16, we have trained it for 250 epochs with\nmini-batch size 128 and optimizer SGD with moment 0.9; the\nweight decay is set to 5e-4; the initial learning rate is set to\n0.1 and divided by 2 after every 20 epochs.\nOn Tiny ImageNet, the hyperparameters in weight decay\nfor ResNet-20, VGG-16, and WRN-40-4 are set to 1e-4, 5e-4,\nand 5e-4, respectively. We have trained the three models for\n120 epochs with mini-batch size 128 and optimizer SGD with\nmoment 0.9. The initial learning rate is 0.05 and divided by\n5 after every 30 epochs.\nOn the large-scale dataset ImageNet, we have trained\nResNet-18 and ResNet-34 for 90 epochs with optimizer SGD\nwith moment 0.9. The mini-batch size is set to 128 and the\nlearning rate is set to 0.05 and divided by 10 after every 30\nepochs for both networks.\nC. Performances of Lookup-VNets with Full Lookup Tables of\nDifferent Vector Dimensions\nLookup-VNets parameterize the inputs through associating\neach color with a vector in lookup tables. To investigate the\ninﬂuence of the vector dimension on the performances of\nLookup-VNets, substantial experiments with various vector\ndimensions are conducted on the four datasets. Speciﬁcally,\nwe evaluate Lookup-VNets with different vector dimensions\non CIFAR-10, CIFAR-100, and Tiny ImageNet while we\n3https://github.com/facebookarchive/fb.resnet.torch\n6\nTABLE I\nTEST ACCURACIES (%) ON CIFAR-10 AND CIFAR-100 WITH FULL LOOKUP TABLES OF DIFFERENT VECTOR DIMENSIONS\nStandard\nDim 1\nDim 2\nDim 3\nDim 4\nDim 5\nDim 10\nDim 100\nCIFAR-10\nResNet-20\n91.28\n91.52\n91.72\n91.17\n91.29\n91.23\n91.01\n91.33\nVGG-16\n93.20\n93.29\n93.27\n93.13\n93.10\n93.38\n93.12\n93.28\nWRN-40-4\n95.10\n95.26\n94.83\n95.11\n95.10\n94.95\n95.01\n94.95\nCIFAR-100\nResNet-20\n66.63\n66.37\n66.51\n66.29\n66.65\n66.14\n66.19\n66.67\nVGG-16\n72.48\n72.17\n72.16\n72.46\n72.42\n72.20\n72.52\n72,32\nWRN-40-4\n77.97\n77.60\n78.14\n78.06\n77.50\n77.82\n77.48\n77.34\nTABLE II\nTEST ACCURACIES (%) ON TINY IMAGENET WITH FULL LOOKUP TABLES OF DIFFERENT VECTOR DIMENSIONS\nStandard\nDim 1\nDim 5\nDim 10\nDim 20\nDim 30\nDim 50\nDim 100\nResNet-20\n50.69\n50.68\n50.31\n50.05\n50.21\n50.55\n50.36\n50.21\nVGG-16\n60.61\n61.15\n60.50\n61.13\n61.10\n60.18\n61.47\n61.50\nWRN-40-4\n63.04\n63.02\n62.67\n63.11\n63.22\n63.11\n62.64\n63.03\nFig. 4. Training Curves of ResNet-20 on CIFAR-\n10 with Different Vector Dimensions\nFig. 5. Training Curves of VGG-16 on CIFAR-10\nwith Different Vector Dimensions\nFig. 6. Training Curves of WRN-40-4 on CIFAR-\n10 with Different Vector Dimensions\nFig. 7. Training Curves of VGG-16 on CIFAR-100\nwith Different Vector Dimensions\nFig. 8. Training Curves of WRN-40-4 on CIFAR-\n100 with Different Vector Dimensions\nFig. 9.\nTraining Curves of ResNet-20 on Tiny\nImageNet with Different Vector Dimensions\nTABLE III\nVALIDATION ACCURACIES (%) ON IMAGENET OF STANDARD DNNS AND\nLOOKUP-VNETS WITH 1-DIMENSION FULL LOOKUP TABLES\n#Parameters\nAccuracy\nResnet-18\nStandard\n11.7 M\n69.4\nDim 1\n11.7 M\n70.1\nResnet-34\nStandard\n21.8 M\n73.0\nDim 1\n21.8 M\n73.4\nonly check the performances of Lookup-VNets with vector\ndimension 1 on ImageNet due to the large image size. Full\nlookup tables are initialized evenly between -1 and 1.\nTable I and Table II summarize the performances of the\nstandard DNNs and the corresponding Lookup-VNets with\ndifferent vector dimensions on CIFAR (i.e, CIFAR-10 and\nCIFAR-100) and Tiny ImageNet, respectively. Surprisingly,\nLookup-VNets with different vector dimensions have almost\nthe same performance with a given network architecture. This\nindicates that the vector dimension has almost no inﬂuence on\nthe performances (i.e., generalization ability 4 ) of Lookup-\nVNets while the network architecture matters. Another as-\ntonishing observation is that these Lookup-VNets produce\nalmost the same results as those of the corresponding standard\nDNNs with RGB inputs on the three datasets. We attributed\nthis observation to the small number of training data in the\n4As their training accuracies are all 100%, then test performances repre-\nsent the generalization abilities (Generalization = Training Accuracy - Test\nAccuracy).\n7\nFig. 10.\nTraining Curves of VGG-16 on Tiny\nImageNet with Different Vector Dimensions\nFig. 11.\nTraining Curves of WRN-40-4 on Tiny\nImageNet with Different Vector Dimensions\nFig. 12.\nTraining Curves of ResNet-34 on Ima-\ngeNet with Vector Dimension 1\nFig. 13.\nTest Accuracies (%) on CIFAR-10 with\nDifferent CMP-Rates\nFig. 14. Test Accuracies (%) on CIFAR-100 with\nDifferent CMP-Rates\nFig. 15.\nTest Accuracies (%) on Tiny ImageNet\nwith Different CMP-Rates\nTABLE IV\nTEST ACCURACIES (%) ON CIFAR-10, CIFAR-100, AND TINY IMAGENET OF LOOKUP-VNETS WITH LARGE CMP-RATES\nRandom Guess\nCMP-Rate 100\nCMP-Rate 128\nCMP-Rate 256\nCIFAR-10\nResNet-20\n10.00\n80.77\n75.30\n10.00\nVGG-16\n10.00\n82.66\n78.94\n10.00\nWRN-40-4\n10.00\n85.85\n80.49\n10.00\nCIFAR-100\nResNet-20\n1.00\n50.96\n43.69\n1.00\nVGG-16\n1.00\n55.31\n47.54\n1.00\nWRN-40-4\n1.00\n59.85\n51.05\n1.00\nTiny ImageNet\nResNet-20\n0.50\n40.41\n34.37\n0.50\nVGG-16\n0.50\n49.53\n41.78\n0.50\nWRN-40-4\n0.50\n49.40\n41.20\n0.50\nTABLE V\nVALIDATION ACCURACIES (%) ON IMAGENET WITH DIFFERENT\nCMP-RATES\n#Parameter\nAccuracy\nResNet-18\nStandard\n11.7 M\n69.4\nCMP-Rate 2\n11.7 M\n69.9\nCMP-Rate 10\n11.7 M\n69.7\nCMP-Rate 15\n11.7 M\n69.4\nResNet-34\nStandard\n21.8 M\n73.0\nCMP-Rate 2\n21.8 M\n73.2\nCMP-Rate 10\n21.8 M\n73.1\nCMP-Rate 15\n21.8 M\n73.0\nthree datasets because a different phenomenon is observed on\nthe large-scale and challenging dataset ImageNet as shown in\nTable III.\nAs seen from Table III, 1-dimension Lookup-VNets show\na consistent performance improvement on ImageNet with\nResNet-18 and ResNet-34, which indicates that Lookup-VNets\nhave advantages over the standard DNNs on large-scale and\nchallenging datasets like ImageNet. The possible reason can\nbe that when the the size and complexity of the dataset are\nscaled up, the integers in the RGB space are not appropriate\nfor DNN training anymore, but the Lookup-VNets make the\ninputs to DNNs more ﬂexible, and are able to learn the optimal\ninputs automatically. It is worth noting that the number of\nthe additional parameters brought by 1-dimension Lookup-\nVNets is only 768 which is ignorable compared with 11.7\nmillion parameters in ResNet-18 and 21.8 million parameters\nin ResNet-34.\nFigure 4, Figure 5, and Figure 6 present the training curves\nof ResNet-20, VGG-16, and WRN-40-4 with different vector\ndimensions on CIFAR-10, respectively. Figure 7 and Figure\n8 present the training curves of VGG-16 and WRN-40-4\nwith different vector dimensions on CIFAR-100, respectively.\nFigure 9, Figure 10 and Figure 11 present the training curves\nof ResNet-20, VGG-16, and WRN-40-4 with different vector\ndimensions Tiny ImageNet, respectively. Figure 12 presents\nthe training curve of ResNet-34 with vector dimension 1 on\n8\nTABLE VI\nTEST ACCURACIES (%) ON CIFAR-10(100) AND TINY IMAGENET WITH FULL LOOKUP TABLES LEARNED ACROSS RESNET-20 AND VGG-16\nCIFAR-10\nCIFAR-100\nTiny ImageNet\nResNet-20\nVGG-16\nResNet-20\nVGG-16\nResNet-20\nVGG-16\nStandard\n91.28\n93.20\n66.63\n72.48\n50.69\n60.61\nDim 1\n91.47\n93.48\n66.70\n72.60\n51.00\n60.83\nDim 5\n91.10\n93.21\n66.73\n72.52\n50.68\n61.09\nDim 10\n91.01\n93.20\n66.67\n72.25\n50.55\n61.49\nDim 100\n91.28\n93.20\n66.87\n72.53\n50.64\n60.50\nTABLE VII\nTEST ACCURACIES (%) OF RESNET-20 AND VGG-16 WITH FULL\nLOOKUP TABLES LEARNED ACROSS CIFAR-10 AND TINY IMAGENET\nResNet-20\nVGG-16\nCIFAR\n-10\nTiny\nImageNet\nCIFAR\n-10\nTiny\nImageNet\nStandard\n91.28\n50.69\n93.20\n60.61\nDim 1\n91.93\n50.69\n93.21\n61.10\nDim 5\n91.50\n50.59\n93.20\n61.77\nDim 10\n91.73\n50.61\n93.21\n60.59\nImageNet. It is observed that the Lookup-VNets with different\nvector dimensions have the same converge speeds as those of\nthe standard DNNs on all the four datasets.\nD. Performances of Lookup-VNets with Different CMP-Rates\nEmpirical results have shown that the vector dimension\nin full lookup tables plays no role in the performances\nof Lookup-VNets, which questions the necessity of a large\ncolor space. Now we explore whether compressing the color\nspace inﬂuences the performances of Lookup-VNets. We\ncompare the performances of the standard DNNs with those\nof the corresponding Lookup-VNets with various CMP-Rates\non CIFAR-10, CIFAR-100, and Tiny ImageNet. Compressed\nlookup tables are initialized evenly between -1 and 1.\nFigure 13, Figure 14, and Figure 15 represent the results\nwith different CMP-Rates on CIFAR-10, CIFAR-100, and Tiny\nImageNet, respectively. We notice that the performances do\nnot drop compared with those of the standard DNNs with\nRGB inputs when the compressing rate is less than a threshold.\nTo our surprise, without accuracy dropping, the color space\ncan be compressed 4096 (163), 729 (93), and 1728 (123)\ntimes on CIFAR-10 with ResNet-20, VGG-16, and WRN-40-\n4, respectively, 512 (83) times on CIFAR-100 with the three\nnetworks, and 3375 (153), 2744 (143), and 3375 (153) times\non Tiny ImageNet with ResNet-20, VGG-16, and WRN-40-4,\nrespectively. The experimental results indicate that Lookup-\nVNets can be used to learn the optimal image coding scheme\nfor given tasks and goals such as the storage compression\nand the accuracy. For example, when the color space can be\ncompressed 4096 (163) times without accuracy dropping, the\npixel bits can be reduced from 24 (8 × 3) bits to 12 ( 4 × 3)\nbits under this setting so that the image storage space can be\nsaved one half.\nTo further explore color space compression, we report\nseveral experiments with large CMP-Rates (≥100), i.e., 100,\n128, and 256. The results are presented in Table IV. As\nexpected, the accuracies of Lookup-VNets with CMP-Rate\n256, i.e., 1 color in each channel, are as the same as those\nof the random guess since there is only one color for all\nimages. However, to our surprise, when the color number in\neach channel is only increased from 1 (i.e., CMP-Rate 256) to\n2 and 3 (i.e., CMP-Rate 128 and 100), the performances are\nimproved substantially on the three datasets, i.e., from 10%\nto 75.30% and 80.77%, from 10% to 78.94% and 82.66%,\nand from 10% to 80.49% and 85.85% on CIFAR-10 with\nResNet-20, VGG-16, and WRN-40-4, respectively; from 1%\nto 43.69% and 50.96%, from 1% to 47.54% and 55.31%,\nand from 1% to 51.05% and 59.85% on CIFAR-100 with\nResNet-20, VGG-16, and WRN-40-4, respectively; from 0.5%\nto 34.37% and 40.41%, from 0.5% to 41.78% and 49.53%, and\nfrom 0.5% to 41.20% and 49.40% on Tiny ImageNet with\nResNet-20, VGG-16, and WRN-40-4, respectively.\nWe also report the experimental results on large-large\ndataset ImageNet with different CMP-Rates. As shown in\nTable V, when the color space is compressed 8 (23), 1000\n(103), and 3375 (153) times, Lookup-VNets are still able to\nperform no worse than the standard DNNs, which indicates the\npromise of Lookup-VNets and the potential of deep collective\nlearning in computer vision.\nE. Performances of Lookup-VNets with Different Table Learn-\ning Strategies\nIn this part, we investigate how the table learning strategy\ninﬂuences the performances of Lookup-VNets. We consider\nthe classiﬁcation tasks on CIFAR-10, CIFAR-100, and Tiny\nImageNet as three distinct tasks. For cross-network learning,\nwe learn the lookup tables across ResNet-20 and VGG-16 for\neach of the three tasks. For cross-task learning, we use ResNet-\n20 and VGG-16 on CIFAR-10 and Tiny ImageNet to jointly\nlearn the lookup tables. Table VI and Table VII report the\nresults of the lookup tables learned across two networks and\nacross two tasks, respectively. Compared with the results of the\nindividually learned tables as shown in Table I and Table II, we\nobserve that learning across networks has almost no inﬂuence\non the performances of Lookup-VNets while learning across\ntasks is able to improve the performances.\nF. Visualization\nIn this part, we visualize lookup tables through showing\nimages in the way larger pixel values are visualized as higher\ncolor intensity. Figure 16 shows eight CIFAR-10 images when\nthey are represented in the original RGB space, 1-dimension\nfull lookup tables, and compressed lookup tables with CMP-\nRate 5. The full lookup tables and compressed lookup tables\n9\nFig. 16. Visualization of CIFAR-10 Images. First row: visualization of the CIFAR-10 images coded by the RGB scheme; second row: visualization of the\nimages coded by 1-dimension full lookup tables; third row: visualization of the images coded by compressed lookup tables with CMP-Rate 5.\nare learned with VGG-16 on the CIFAR-10 classiﬁcation task.\nSuppose the human visual system prefers the images coded\nby RGB as shown in the ﬁrst row of Figure 16. We observe\nthat the code scheme that the DNN favors for CIFAR-10\nclassiﬁcation task (i.e., the second row and the third row of\nFigure 16) is different from what the human visual system\nprefers. However, it is reasonable because the DNN as a\nextremely complex function may carry out a task from a\nperspective that is different from that of humans.\nV. CONCLUSION AND FUTURE WORK\nAs visual data are almost always represented in a manually\ndesigned coding scheme when they are input to DNNs, we\nhave explored whether the inputs to DNNs can be optimally\nlearned end-to-end, and have proposed the paradigm of deep\ncollective learning which aims to learn the weights of DNNs\nand the inputs to DNNs simultaneously. Due to the lack of\nthe research on deep collective learning in computer vision,\nwe have proposed Lookup-VNets as a solution. Lookup-VNets\nenable DNNs to learn the optimal inputs automatically for\ngiven tasks. From the perspective of image coding, Lookup-\nVNets can be considered as learning the optimal image cod-\ning scheme automatically for given goals. Additionally, we\nhave explored various aspects of deep collective learning in\ncomputer vision with Lookup-VNets through extensive exper-\niments on four benchmark datasets, i.e., CIFAR-10, CIFAR-\n100, Tiny ImageNet, and Imagenet. The experiments have\nshown several surprising characteristics of Lookup-VNets: (1)\nthe vector dimensions in lookup tables has no inﬂuence on\nthe test performance (generalization ability) of Lookup-VNets;\n(2) the commonly used color space can be compressed up to\n4096 times without accuracy dropping on CIFAR-10, and 3375\ntimes on CIFAR-100 and Tiny ImageNet. We also observe\nthat Lookup-VNets are able to match the performances of\nthe standard DNNs on small datasets and achieve superior\nperformances on large-scale and challenging datasets like\nImageNet since large and complex datasets can fully take\nadvantages of the ﬂexible and optimally learned inputs.\nBesides the basic aspects of Lookup-VNets studied in this\npaper, various other aspects can be explored, one of which is\nto design an effective regularizer with regard to lookup tables\nfor Lookup-VNets. It is widely believed that the generalization\nabilities of DNNs are tightly connected to the stability which\ncan be described as a partial derivative of the output to the\ninput. In Lookup-VNets, the inputs to DNNs are signiﬁcantly\nrelated to the learned lookup tables. Thus, developing an ap-\npropriate regularizer on lookup tables is intuitively promising\nto further improve the performances of Lookup-VNets. We\nleave this to the future work.\nREFERENCES\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural infor-\nmation processing systems, 2012, pp. 1097–1105.\n[2] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2015, pp. 3431–3440.\n[3] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\nonce: Uniﬁed, real-time object detection,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016, pp. 779–\n788.\n[4] H. Mobahi, R. Collobert, and J. Weston, “Deep learning from temporal\ncoherence in video,” in Proceedings of the 26th Annual International\nConference on Machine Learning, 2009, pp. 737–744.\n[5] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[6] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\nobject detection with region proposal networks,” in Advances in neural\ninformation processing systems, 2015, pp. 91–99.\n[7] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.\nBerg, “Ssd: Single shot multibox detector,” in European conference on\ncomputer vision.\nSpringer, 2016, pp. 21–37.\n[8] K. Mistry, L. Zhang, S. C. Neoh, C. P. Lim, and B. Fielding, “A micro-\nga embedded pso feature selection approach to intelligent facial emotion\nrecognition,” IEEE transactions on cybernetics, vol. 47, no. 6, pp. 1496–\n1509, 2016.\n[9] C. Zhang, J. Cheng, and Q. Tian, “Multiview semantic representation\nfor visual recognition,” IEEE transactions on cybernetics, 2018.\n[10] R. Lan, Y. Zhou, Z. Liu, and X. Luo, “Prior knowledge-based proba-\nbilistic collaborative representation for visual recognition,” IEEE trans-\nactions on cybernetics, 2018.\n[11] K. Hornik, M. Stinchcombe, H. White et al., “Multilayer feedforward\nnetworks are universal approximators.” Neural networks, vol. 2, no. 5,\npp. 359–366, 1989.\n[12] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken, “Multilayer\nfeedforward networks with a nonpolynomial activation function can\napproximate any function,” Neural networks, vol. 6, no. 6, pp. 861–867,\n1993.\n10\n[13] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in International Conference on Learning\nRepresentations, 2015.\n[14] F. Agostinelli, M. Hoffman, P. Sadowski, and P. Baldi, “Learning\nactivation functions to improve deep neural networks,” in International\nConference on Learning Representations, 2015.\n[15] A. Molina, P. Schramowski, and K. Kersting, “Pad\\’e activation units:\nEnd-to-end learning of ﬂexible activation functions in deep networks,”\nin International Conference on Learning Representation, 2020.\n[16] C.-Y. Lee, P. W. Gallagher, and Z. Tu, “Generalizing pooling functions\nin convolutional neural networks: Mixed, gated, and tree,” in Artiﬁcial\nintelligence and statistics, 2016, pp. 464–472.\n[17] M. Streeter, “Learning optimal linear regularizers,” Proceedings of the\n36th International Conference on Machine Learning, PMLR 97:5996-\n6004, 2019.\n[18] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-\nbilistic language model,” Journal of machine learning research, vol. 3,\nno. Feb, pp. 1137–1155, 2003.\n[19] X. Ouyang, P. Zhou, C. H. Li, and L. Liu, “Sentiment analysis using\nconvolutional neural network,” in 2015 IEEE International Conference\non Computer and Information Technology; Ubiquitous Computing and\nCommunications; Dependable, Autonomic and Secure Computing; Per-\nvasive Intelligence and Computing.\nIEEE, 2015, pp. 2359–2364.\n[20] B. Zhang, D. Xiong, J. Su, and Y. Qin, “Alignment-supervised bidi-\nmensional attention-based recursive autoencoders for bilingual phrase\nrepresentation,” IEEE transactions on cybernetics, vol. 50, no. 2, pp.\n503–513, 2018.\n[21] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features\nfrom tiny images,” Citeseer, Tech. Rep., 2009.\n[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in 2009 IEEE conference on\ncomputer vision and pattern recognition.\nIeee, 2009, pp. 248–255.\n[23] J. Lee, K. Cho, and T. Hofmann, “Fully character-level neural machine\ntranslation without explicit segmentation,” Transactions of the Associa-\ntion for Computational Linguistics, vol. 5, pp. 365–378, 2017.\n[24] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation\nof rare words with subword units,” in Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume\n1: Long Papers).\nBerlin, Germany: Association for Computational\nLinguistics, Aug. 2016, pp. 1715–1725.\n[25] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey et al., “Google’s neural\nmachine translation system: Bridging the gap between human and\nmachine translation,” 2016.\n[26] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” in International Conference on\nLearning Representation, 2013.\n[27] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in Proceedings of the 2014 conference on\nempirical methods in natural language processing (EMNLP), 2014, pp.\n1532–1543.\n[28] M. Baroni, G. Dinu, and G. Kruszewski, “Dont count, predict! a sys-\ntematic comparison of context-counting vs. context-predicting semantic\nvectors,” in Proceedings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), 2014, pp. 238–\n247.\n[29] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[30] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional\nnetworks for text classiﬁcation,” in Advances in neural information\nprocessing systems, 2015, pp. 649–657.\n[31] A.\nConneau,\nH.\nSchwenk,\nL.\nBarrault,\nand\nY.\nLecun,\n“Very\ndeep convolutional networks for text classiﬁcation,” arXiv preprint\narXiv:1606.01781, 2016.\n[32] L. Yao, C. Mao, and Y. Luo, “Graph convolutional networks for text\nclassiﬁcation,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 33, 2019, pp. 7370–7377.\n[33] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:\nSurpassing human-level performance on imagenet classiﬁcation,” in\nProceedings of the IEEE international conference on computer vision,\n2015, pp. 1026–1034.\n[34] D. Lin, C. Lu, R. Liao, and J. Jia, “Learning important spatial pooling\nregions for scene classiﬁcation,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2014, pp. 3726–3733.\n[35] X. Zhu, D. Cheng, Z. Zhang, S. Lin, and J. Dai, “An empirical study of\nspatial attention mechanisms in deep networks,” in Proceedings of the\nIEEE International Conference on Computer Vision, 2019, pp. 6688–\n6697.\n[36] M. Sun, Z. Song, X. Jiang, J. Pan, and Y. Pang, “Learning pooling for\nconvolutional neural network,” Neurocomputing, vol. 224, pp. 96–104,\n2017.\n[37] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nthe 3rd International Conference for Learning Representations, 2015.\n[38] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2015, pp. 1–9.\n[39] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in BMVC,\n2016.\n[40] S. Liu and W. Deng, “Very deep convolutional neural network based\nimage classiﬁcation using small training sample size,” in 2015 3rd IAPR\nAsian conference on pattern recognition (ACPR). IEEE, 2015, pp. 730–\n734.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2020-09-17",
  "updated": "2020-09-17"
}