{
  "id": "http://arxiv.org/abs/2009.08497v1",
  "title": "The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons from Infant Learning",
  "authors": [
    "Lorijn Zaadnoordijk",
    "Tarek R. Besold",
    "Rhodri Cusack"
  ],
  "abstract": "After a surge in popularity of supervised Deep Learning, the desire to reduce\nthe dependence on curated, labelled data sets and to leverage the vast\nquantities of unlabelled data available recently triggered renewed interest in\nunsupervised learning algorithms. Despite a significantly improved performance\ndue to approaches such as the identification of disentangled latent\nrepresentations, contrastive learning, and clustering optimisations, the\nperformance of unsupervised machine learning still falls short of its\nhypothesised potential. Machine learning has previously taken inspiration from\nneuroscience and cognitive science with great success. However, this has mostly\nbeen based on adult learners with access to labels and a vast amount of prior\nknowledge. In order to push unsupervised machine learning forward, we argue\nthat developmental science of infant cognition might hold the key to unlocking\nthe next generation of unsupervised learning approaches. Conceptually, human\ninfant learning is the closest biological parallel to artificial unsupervised\nlearning, as infants too must learn useful representations from unlabelled\ndata. In contrast to machine learning, these new representations are learned\nrapidly and from relatively few examples. Moreover, infants learn robust\nrepresentations that can be used flexibly and efficiently in a number of\ndifferent tasks and contexts. We identify five crucial factors enabling\ninfants' quality and speed of learning, assess the extent to which these have\nalready been exploited in machine learning, and propose how further adoption of\nthese factors can give rise to previously unseen performance levels in\nunsupervised learning.",
  "text": "arXiv:2009.08497v1  [cs.LG]  17 Sep 2020\nTHE NEXT BIG THING(S) IN UNSUPERVISED MACHINE\nLEARNING: FIVE LESSONS FROM INFANT LEARNING\nLorijn Zaadnoordijk\nTrinity College Institute of Neuroscience\nTrinity College Dublin\nDublin, Ireland\nL.Zaadnoordijk@tcd.ie\nTarek R. Besold\nNeurocat GmbH\nBerlin, Germany\ntb@neurocat.ai\nRhodri Cusack\nTrinity College Institute of Neuroscience\nTrinity College Dublin\nDublin, Ireland\nCusackRh@tcd.ie\nSeptember 21, 2020\nABSTRACT\nAfter a surge in popularity of supervised Deep Learning, the desire to reduce the dependence on\ncurated, labelled data sets and to leverage the vast quantities of unlabelled data available recently\ntriggered renewed interest in unsupervised learning algorithms. Despite a signiﬁcantly improved\nperformance due to approaches such as the identiﬁcation of disentangled latent representations, con-\ntrastive learning, and clustering optimisations, the performance of unsupervised machine learning\nstill falls short of its hypothesised potential. Machine learning has previously taken inspiration from\nneuroscience and cognitive science with great success. However, this has mostly been based on adult\nlearners with access to labels and a vast amount of prior knowledge. In order to push unsupervised\nmachine learning forward, we argue that developmental science of infant cognition might hold the\nkey to unlocking the next generation of unsupervised learning approaches. Conceptually, human\ninfant learning is the closest biological parallel to artiﬁcial unsupervised learning, as infants too\nmust learn useful representations from unlabelled data. In contrast to machine learning, these new\nrepresentations are learned rapidly and from relatively few examples. Moreover, infants learn robust\nrepresentations that can be used ﬂexibly and efﬁciently in a number of different tasks and contexts.\nWe identify ﬁve crucial factors enabling infants’ quality and speed of learning, assess the extent to\nwhich these have already been exploited in machine learning, and propose how further adoption of\nthese factors can give rise to previously unseen performance levels in unsupervised learning.\n1\nIntroduction\n“The objective of machine learning algorithms is to discover statistical structure in data. In particular, representation-\nlearning algorithms attempt to transform the raw data into a form from which it is easier to perform supervised\nlearning tasks, such as classiﬁcation. This is particularly important when the classiﬁer receiving this representation\nas input is linear and when the number of available labeled examples is small.” (Mesnil et al., [110])\nEarly Deep Neural Network (DNN) architectures commonly implemented a paradigm using unsupervised pre-training\nfollowed by supervised ﬁne-tuning [11, 62]. Inclusion of the unsupervised pre-training stage led to breakthrough\nimprovements in the performance of trained deep models and was the key to unlocking effective training strategies for\ndeep architectures in the mid-2000s (see, e.g., [5, 10, 45] for details). Since the mid-2010s, unsupervised pre-training\nhas become less popular and—especially in computer vision applications—has to a certain degree been superseded by\nsupervised pre-training of models using large-scale labelled data sets (such as ImageNet [142]). Systems implementing\nthis approach have shown impressive performance in a variety of applications (see, e.g., [29, 101, 136]). Still, a major\ndrawback of the fully supervised paradigm is its strict dependency on large-scale, well-curated data sets as starting\npoint. Moreover, reasonable doubts have been expressed regarding the efﬁcacy [61] and further expandability [66, 135]\nof the paradigm. These and related considerations over the last few years triggered renewed interest in unsupervised\nlearning within the Deep Learning community (see, e.g., [24, 27, 31]), which inspired our present article.\nSEPTEMBER 21, 2020\nUnsupervised representation learning attracts considerable interest from neuroscience and cognitive science. DNNs\nseem to offer a loosely biologically-inspired modelling and analyses tool for the study of the human brain and/or mind,\nwith representation learning playing a central role (see, e.g., [102, 181]). In fact, there is a longstanding tradition of\nprominent and active exchanges between machine learning (ML) and these ﬁelds, going back at least to Rosenblatt’s\nperceptron [140] and its inspiration by “nerve net” [107]. An increasing number of voices have been calling to again\nlook at animals and humans for insights into how their biological neural machinery works and how these natural-born\nintelligent systems learn (see, e.g., [60, 93, 102, 144, 150, 186]. What is common to such contributions is that they are\nprimarily taking mature human cognizers and their brains as a conceptual starting point. In contrast, we advocate to\nfocus on results from the study of infants and their development: Human infants are in many ways a close counterpart\nto a computational system learning in an unsupervised manner as infants too must learn useful representations from\nunlabelled data. However, infants’ representations are learned rapidly, with relatively few examples, and can be\nused ﬂexibly and efﬁciently in various different tasks and contexts. Developmental science, the ﬁeld studying how\ncognitive capacities develop in human infants and children, has identiﬁed several facilitating factors for this surprising\nperformance, which merit discussion also in the ML context. Speciﬁcally, they may hold the answers to some of the\nlong-standing questions of representation learning (see, e.g., [9]): What can a good representation buy us? What is a\ngood representation in the ﬁrst place? And what training principles can help discovering such good representations?\nWe are not the ﬁrst ones to make recourse to developmental research in an ML context (see, e.g., [84, 162]). However,\nthese prior efforts remain on a very general level regarding their engagement with the relevant insights from develop-\nmental science. In contrast, we give speciﬁc suggestions for which aspects of infant learning ML researchers should\nconsider and discuss to what extent current ML research is already—consciously or by coincidence—working towards\nintegrating these insights. In doing so, we will mostly limit our focus to developmental learning during the ﬁrst 12\nmonths of life. This is the period in an infants’ life that would be most similar to what is happening during unsupervised\nML. In Sec. 2, we outline core results from ﬁve domains within current developmental science that have been crucial\nfor understanding how infants learn and that offer valuable inspiration for the future of unsupervised ML. Sec. 3 then\nprovides a concluding general discussion of relevant chances and challenges arising in the interaction between ML\nas (mostly) “computationally-minded” ﬁeld and developmental science as (predominantly) “psychologically-minded”\ndiscipline.\n2\nFive lessons from developmental science: Key enablers of growing a mind\nWe discuss ﬁve main lessons synthesised from the current state of knowledge regarding infant learning and discuss\nthe direct relevance of these insights for the development of the next generation of unsupervised approaches in ML.\nSec. 2.1 zooms in on the starting conditions for unsupervised learning in infants, for instance tying into recent work on\npre-wiring and pre-training of DNNs. Sec. 2.2 summarises insights about cross-modal statistical learning in infants and\nestablishes the connection to corresponding efforts in ML. Sec. 2.3 provides an overview of developmental research\ninto the temporal structure of infant learning and connects directly to results such as non-stationarity in continual\nrepresentation learning. Sec. 2.4 discusses results from the study of active learning in infants and its ML parallel.\nFinally, Sec. 2.5 addresses the role other agents play in infant learning and what this implies for the computational\ncounterpart thereof.\n2.1\nBabies’ information processing is guided and constrained from birth\nIt is a common—but mistaken—belief that infants’ brain architecture is highly immature. Although the infant brain\nstill grows and develops, a lot of its ultimate structure is already present very early on. For instance, neuroimaging data\nshow that the structural connectivity patterns observed in early infancy are very similar to adult structural connectivity\n[25]. Regarding functional connectivity adult-like patterns can also be observed in infants [40, 77] even for networks\nthat support cognitive capacities that are not yet behaviourally manifested such as speech [34]. Functional visual\ncortex activation of abstract categories (faces and scenes) is adult-like in its spatial organisation in infants as young as\nfour months of age and is reﬁned in its response pattern through development [37]. Moreover, functional connectivity\npatterns in neonates have been shown to be predictive of later development (e.g., [99]). Compared to adults, infants’\nneural structure is still more plastic and can change dramatically depending on the type of input it receives; for\ninstance, research of congenitally blind individuals shows that the visual cortex can be re-purposed for other, non-\nvisual, capacities but that this does not happen in those that become blind later in life [7]. Even so, infants’ brain\nstructure is already fairly determined as a whole, thereby constraining processing and guiding learning.\n[186] argued that creators of neural networks should take into consideration that the highly structured neural connec-\ntivity is what allows animals to learn so rapidly. Indeed, ML researchers have been exploring the impact of pre-wiring\non network learning. A recent example is work in the context of learning general identity rules. Answering a chal-\n2\nSEPTEMBER 21, 2020\nlenge posed in [104] regarding potential limitations on the generalisation capabilities of recurrent neural networks,\n[2] used “delay lines” (a concept ﬁrst suggested in neuroscience [76]) in an Echo State Network [73], i.e., organising\na sub-network of neurons in the network in layers, with connections with weights 1 between corresponding nodes\nand 0 everywhere else connecting each layer to the next. The authors show that intentionally introducing delay lines\nconstituted a signiﬁcant step towards addressing the hypothesised limitations. The ﬁndings from developmental sci-\nence outlined in the previous paragraph suggest that infant neuroscience offers a rich variety of further inspirations for\narchitectural building blocks of neural networks.\nTurning from architecture to ‘pre-programmed’ capabilities, the developmental science community is still heavily\ndebating which cognitive capacities might already be present at birth and what has to be learned through experience\n(see e.g., [151, 155]). As such, many appeals to inborn knowledge as explanation for empirical observations are\nheavily contested. However, there are some ﬁndings of biases in newborns that are generally agreed upon (even if\nthey are interpreted differently across labs). Biases in this regard differ from knowledge in that they are an integral\npart of the learning mechanism rather than an input to the learning algorithm. What initially was considered a bias\nfor faces or face-like stimuli (consisting of 3 dots in an upside-down triangle conﬁguration) has been shown to be a\ngeneral attentional bias for top-heavy visual information [148, 173], which develops into a preference for faces within\nthe ﬁrst months [32, 70]. Newborns further seem to have a preference for some aspect of biological motion [148],\nthough the exact bias mechanisms have yet to be explained. Also in non-visual domains, early processing biases are\nfound. Newborns discriminate [127] and prefer [175] speech versus non-speech, and prefer infant-directed speech over\nregular speech [33]. However, these speech biases may in part have been shaped by auditory experiences in utero.\nGoing beyond the general notion of inductive bias in ML architectures [111, 112], biases already play an important role\nin the training of DNN architectures. Already in the early days of Deep Learning, the initial unsupervised pre-training\nstage—in a way similar to regularisation—also served the purpose of introducing a particular type of ‘starting bias’\ninto the architecture, namely a reduction in variance and a shift in the parameter space towards conﬁgurations that are\nuseful for supervised learning [45]. Still, recently the study of the potential development and inﬂuence of particular\nbiases in networks has enjoyed increasing interest, for example regarding the role of shape bias within networks in\nperforming visual tasks [48] or in the context of learning identity relationships with different network architectures\n[89]. In the latter work, the authors show that Relation Based Patterns [179] can be implemented as a Bayesian prior\non the network weights, helping to overcome limitations neural networks frequently exhibit related to the learning\nof identity rules and to generalisation beyond the training data set. Looking at examples from computer vision, the\nstriking performance of a model that was trained to be sensitive to mover events (i.e., events of a moving image region\ncausing a stationary region to move or change after contact)—a parallel to infants’ bias for biological movement—in\nrecognising hands and gaze direction from video input demonstrated the usefulness of combining learning and innate\nmechanisms [171]. Given these early successes and the insights from developmental science, we believe that there is\noverwhelming evidence for the role biases can play in guiding and augmenting the training of neural networks.\nTL;DR: Even when not invoking rich interpretations of newborn cognition, the earliest processing of information after\nbirth is constrained by the neural architecture and guided by cognitive predispositions. Translating these insights from\ndevelopmental science to the ML world, they ﬁnd a direct counterpart in the importance of starting conditions and\nthe growing efforts invested in the study of pre-wiring and pre-training of neural networks. Developmental science\nsuggests that not only the type of ML architecture selected and corresponding network model, the training algorithm\nor the training data matter, but that the particular instantiation of the architecture and setup of the network play an\nimportant role as enabler of efﬁcient and effective training. Not all inductive biases have been made equal. Infants do\nnot have to start their developmental trajectory from a complete tabula rasa or from an arbitrary starting setup—and\nneither should neural networks.\n2.2\nBabies are learning statistical relations across diverse inputs\nThere is an abundance of literature studying babies’ capacity for detecting and learning from statistical regularities\nin their physical and social environment. Infants readily learn associations, such as sensorimotor contingencies [72],\nincorrect handling of objects [67], and associations between voices and faces [23]. Interest in more complex statistical\nlearning emerged initially in the language domain when researchers showed that infants can use statistical properties\nin the language input to detect a wide range of phenomena such as word segmentation structures [143], phoneme\ndistributions [106], or non-adjacent dependencies [53, 104]. Regarding domains other than language, infants pick\nup statistical regularities in visual stimuli [87] and in action sequences [114]. Probabilistic relations have also been\nshown to affect infants’ attention [166]. Infants’ predictions depend on the distribution in a sample as well as the\nsampling method [56], and infants are sensitive to the difference in likelihood between events [79]. Moreover, while\n9-month-olds update their model regardless of the informational value of a cue [81], 14-month-old infants represent\nthe statistics of their environment and use new cues to update their model as a function of its informational value\n3\nSEPTEMBER 21, 2020\n[80]. Indeed, the neural dynamics of infants’ prediction and surprise change as a function of priors based on previous\nexperience [90]. Combined, these studies demonstrate that statistical learning in infancy happens across different\ndomains. Furthermore, electrophysiological evidence suggests that this type of learning mechanism is already active\nin newborns [161].\nA major difference between statistical learning in infants and in machines resides in the multi- and cross-modal nature\nof infants’ learning processes. In everyday life, infants encounter many situations in which they have to learn from\nand integrate signals from multiple modalities. Many studies have shown that infants are sensitive to the statistical\nrelations between, for example, visual and auditory (linguistic and non-linguistic) stimuli [22, 23, 156], visual and\ntactile stimuli [21, 187], and auditory and tactile stimuli [160]. These multimodal associations are thought to develop\nvia a combination of brain maturation and multimodal experience, which allow for the detection of temporal synchrony\n(see e.g., [98]). The importance of experience was shown in individuals who were temporarily deaf before they\nreceived a cochlear implant: They showed decreased audiotactile [94] and audiovisual [159] integration, even after\ntheir hearing had been restored. Beyond standard multimodal integration, infants have been shown to be sensitive\nto the relation between exteroceptive (e.g. visual stimuli) and interoceptive (bodily) signals, such as heart rate [103].\nFurthermore, infants react physiologically to threat stimuli [64] and are able to associate these stimuli to fearful facial\nreactions of other agents [65]. Finally, multiple infant studies demonstrated that arousal and excitement cause infants\nto engage with and learn from their environment differently and more extensively (see e.g., [138]). Physiological and\nemotional signals thus provide infants with additional learning opportunities.\nInfants’ processing of signals from diverse inputs likely leads to improved representations and task performance. Multi-\nmodal information can support the disambiguation of conﬂicting or seemingly incoherent input otherwise obtained\nfrom a single sensory stream (e.g. [178]). It further enables the performance of tasks for which a single type of input\nis not sufﬁcient. However, importantly, even representations that seem related to one sensory domain beneﬁt from\ninput from other modalities. [26] have shown that spatial representations (e.g., distance) for both auditory as well as\nproprioceptive stimuli are impaired for congenitally blind children and adults, suggesting that visual input is important\nfor these non-visual representations. This work hints at a much greater need for learning from diverse multimodal\ninputs than one might intuitively consider necessary for unimodal tasks.\nML researchers are already actively exploiting the advantages multi- or crossmodal information processing offers.\nMultimodal processing has been exploited, for instance, in robotics in object categorisation tasks [105, 117], and at\nthe intersection between the vision and the language domain [113], including in emotion recognition systems [6, 169]\nand in movie summarisation tasks [46]. Regarding unsupervised learning, early successes have been achieved in\nthe late 1990s, for example by performing category learning through multimodal sensing [36]. Following the early\nsuccesses of Deep Learning, multimodality also moved into the focus of some researchers in the DNN community,\nagain spanning a wide range of application scenarios from image synthesis [132] to unsupervised robot perception\n[42] and, very recently, image captioning [49]. However, whilst the advantages of cross-modal feature learning had\nalready been identiﬁed almost a decade ago [120, 157], most current DNNs are still being trained on unimodal data.\nThe recent surge in interest in contrastive learning (see, e.g., [4, 31]) suggests that this might be about to change. It is\nworth noting that state-of-the-art contributions such as [164] make explicit reference to the structure and performance\nof human multimodal information processing as inspiration and motivation for the approach.\nTL;DR: Infants learn statistical relations across diverse multimodal input streams and the resulting representations\nprovably beneﬁt from these richer sources of information. Multimodal approaches have also successfully been pursued\nin ML already for decades. However, until today these successes have not caused a widespread shift from unimodal to\nmultimodal training of DNNs. The recent surge in interest in contrastive learning in a multiview setting might ﬁnally\ntrigger wider adoption of multimodal representation learning more generally, even for unimodal tasks.\n2.3\nBabies’ input is scaffolded in time\n“Each new sensorimotor achievement—rolling over, reaching, crawling, walking, manipulating objects—opens and\ncloses gates, selecting from the external environment different datasets to enter the internal system for learning.”\n(Smith, [152], p. 326) Babies are not only learning but, importantly, are also developing (i.e. changing over time).\nThe type of input infants receive is critically dependent on what they can do at any given point in time. A newborn\ninfant will primarily see whatever their caregivers bring into their visual ﬁeld, a crawling infant will get extensive\nvisual input of the ﬂoor, and a sitting infant will be able to see as far into the distance as a walking infant, but does\nnot get the experience of optical ﬂow whilst seated. This also changes the level and possibilities of exploration of\nthe environment when infants transition from one motor ability to the next. For example, walkers can access distant\nobjects, carry these objects and are more likely to approach their mothers to share the objects, while crawlers are more\nlikely to remain stationary and explore objects close to them [78]. Furthermore, differences in posture (e.g. sitting\nvs lying in supine or prone position) affect infants’ possibilities for object exploration [100, 154]. Sensory input goes\n4\nSEPTEMBER 21, 2020\nthrough developmental changes unrelated to motor development too. For example, newborns’ visual acuity is low\nand gradually increases in the ﬁrst six months of life [39, 153]. These changes in sensory and motor possibilities do\nnot only allow infants to explore different aspects of their environment, they also drive an expansion of the range of\nobtained inputs in the direction of increasingly varied stimuli occurring in increasingly complex combinations, thereby\nintroducing a phased-structure in infants’ learning experience.\nExactly how much the input is constrained has become more clear through recent experimentation with head-mounted\ncameras (see e.g., [50, 51, 182]). For example, [47] showed that in approximately a quarter of 1- to 24-month-\nold infants’ visual input there is another person present (as measured by the presence of at least a face or a hand).\nImportantly, the frequency of faces vs hands signiﬁcantly changed as a function of age: the proportion of faces in view\ndeclined with age and the proportion of hands in view increased with age. In another study, these authors found that in\nthe ﬁrst year of life there were relatively few different faces in the infant’s view, that they were generally closeby and\nthus visually large, and that mostly both eyes were visible [75]. Providing a conceptual counterpart to these ﬁndings\nin computer vision, improved performance and generalisation of DNNs with low initial visual acuity—obtained by\nstarting the network training with blurred rather than with high-resolution images—corroborate the idea that phased\nsensory input indeed improves learning [174].\nThe general idea of applying a structured learning curriculum—as naturally experienced by infants—to ML systems\nhad already been put forward by [43] and in the Deep Learning literature was prominently addressed under the headline\nof curriculum learning [12]. One of the main challenges in the context of providing a structured training scheme for\nneural networks is the high sensitivity of the curriculum’s effectiveness to the mode of progression through the learning\ntasks, i.e., the syllabus. We zoom in on two of the factors inﬂuencing this sensitivity, namely critical learning periods\nand catastrophic interference. Critical periods, which are well-documented in biological learners, reﬂect a moment of\npeak plasticity during a speciﬁc developmental state (often early in life) that are followed by reduced plasticity (see e.g.\n[119] for review and reﬂection). Critical periods can be contrasted to open-ended learning systems or systems in which\nplasticity increases with maturation or experience [119]. [1] show that DNNs also exhibit critical periods during which\na temporary stimulus deﬁcit can impair the future performance of the network even to an unrecoverable degree. Strong\nconnections that are optimal relative to the input data distribution are created during the initial epochs of training (i.e.,\na “memorisation phase”) and appear to remain relatively unchanged during additional training. As such, the initial\nlearning transient plays a key role in determining the outcome of the training process, and shortcomings or biases,\nfor instance in the variety of input samples, during early training may not be recovered during the remainder of the\ntraining process.\nAs a consequence of the stability/plasticity dilemma [28], neural networks can suffer from catastrophic interference;\na process where new knowledge overwrites rather than integrates previous knowledge [52]. Catastrophic interference\ndoes not only make it challenging to learn tasks sequentially while maintaining performance, it also has consequences\nfor the order of training stimuli. If a neural network is ﬁrst trained on all exemplars of one class, and then on all\nexemplars of another class, it often will not properly retain knowledge about the ﬁrst class. Different solutions to these\nproblems have been proposed over time, including rehearsal and pseudo-rehearsal learning [137], the use of pairs\nof plastic and elastic network weights [63], and brain-inspired approaches suggesting the use of dual-memory archi-\ntectures [82] or building on synaptic consolidation [88]. Regarding unsupervised learning in particular, catastrophic\ninterference has, among others, been addressed in the context of continuous and lifelong learning, including the use\nof undercomplete autoencoders trained for feature transfer across tasks [133] or approaches motivated by results from\nneuroscience such as neurogenesis deep learning [41]. Relatedly, the explicit (meta-)learning of representations for\ncontinual learning that avoid catastrophic interference has been proposed [74]. Still, the general problem of catas-\ntrophic interference remains unsolved [83].\nTL;DR: Infants’ development leads to a phased-structure in their learning input. This creates a naturally guided\ncurriculum for infant learning. Recognising the potential of this type of learning, ML researchers have attempted to\nintegrate phased inputs in their training regimes. However, thus far, these initiatives are hampered by problems such as\ncatastrophic interference. The quest to achieve continual learning without overwriting previously acquired knowledge\nthus remains unﬁnished.\n2.4\nBabies actively seek out learning opportunities\nInfant learning does not just happen passively. Infants play an active role in directing their attention to stimuli from\nwhich they learn. This process has been given different names including curiosity-driven learning, active learning, and\nlearning by intrinsic motivation. Curiosity is taken to be a state of arousal that requires actions to modulate the aroused\nstate [13], with the degree of novelty determining infants’ ability to learn [14]. The spectrum of arousal is subdivided\ninto the three zones relaxation (i.e., insufﬁcient arousal), curiosity (i.e., optimal for learning), and anxiety (i.e., too\nmuch arousal) [35], where relaxation and anxiety are considered to create little opportunity for learning. Empirical\n5\nSEPTEMBER 21, 2020\nresearch shows that infants attend signiﬁcantly longer to stimuli that are at an intermediate level of complexity; a\nﬁnding that was dubbed ‘the Goldilocks effect’ [85, 86]. Related to the U-shaped curve of attention as a function\nof novelty or complexity, infants vary in their familiarity or novelty preferences to stimuli. This is thought to be\ndependent on the degree of encoding of the stimuli [69]. If the encoding is not yet complete, infants will show a\nfamiliarity preference, and move to a novelty preference once encoding is completed. These ideas have led to a\nplethora of looking time studies. While the richness of the interpretation of looking time studies can be questioned\n(e.g. [3, 59]), the theories on active learning and methods to investigate stimulus encoding have provided important\ninsights into the nature of infants’ learning mechanisms.\nActive learning also has a long history in ML (see, e.g., [146, 147]). ML researchers recognised that an algorithm\nmay learn better and more efﬁciently if it is allowed to select the data from which it learns [146]. Over the years,\ndifferent types of curiosity mechanisms have been proposed for artiﬁcial systems. Some researchers suggest that\ncuriosity could be prediction-based, causing agents to attend to input for which predictability is minimal [18] or\nmaximal [97]. In the context of curriculum learning (also see Sec. 2.3), [55] proposed a multi-armed bandit-based\napproach to ﬁnding progress-maximising stochastic policies over different learning tasks. More closely related to the\nﬁndings in developmental science, [145] argued that curiosity-driven learning occurs most optimally when the agent\nseeks out information as a function of its compressibility. Furthermore, it has been suggested that active learning is\ndriven by a goal to maximise learning progress by interacting with the environment in a novel manner [124, 125].\nUsing autoencoder networks, computational modelling approaches that compared presenting stimuli in a ﬁxed order\nor allowing the model to choose its own input showed that maximal learning happens when the model can maximise\nstimulus novelty relative to its internal states [168]. This work emphasised the importance of the interaction between\nthe structure of the environment and the previously acquired knowledge of the learner. Similarly, [57] created an agent\nwith a world-model that learned to predict the consequences of the agent’s actions, and a meta-cognitive self-model\nthat tracks the performance of the world-model. The self-model was able to improve the world-model by adversarially\nchallenging it. This caused the agent to autonomously explore novel interactions with the environment, leading to\nnew behaviours and improved learning. In sum, there has already been some integration of adult and infant cognitive\n(neuro)science into active learning in ML [54].\nThis section has thus far focused primarily on attention allocation. However, other forms of active learning are worth\nmentioning. Particularly, the inﬂuence of obtaining a sense of agency plays an essential role in infants’ possibilities\nto actively shape their own learning environment. Knowledge about what one can do, allows intervention in the world\nin a manner optimised for acquiring new information about the environment [92, 128]. As such, learning about one’s\nbody and agency has been studied both in infants [20, 138, 177, 185] as well as in artiﬁcial systems [38, 58, 184].\nAlthough these topics might seem too embodied for traditional ML, the principles of active intervention for learning\ndo not require a physical body and are thus of relevance nonetheless. Moreover, active exploration of objects has been\nshown to increase learning about those objects compared to passive observation [71, 121]. The beneﬁt of exploration\nand play is most visible in the ﬂexibility and creativity of later metacognitive and problem-solving capacities [30].\nTL;DR: Having opportunities to selectively inﬂuence their learning curriculum or interact with the environment in\ntargeted ways is a crucial aspect to infant learning and has been proven successful when applied to artiﬁcial systems\ntoo. By taking previously encountered and encoded information into account, the mechanism can optimise for those\ninputs that maximally increase learning. Active learning has the potential to dramatically change the speed and quality\nof learning in DNNs, where it has been only sparsely incorporated until now.\n2.5\nBabies learn from other agents\nIn the previous section, we argued that infants do not learn passively. Here, we address the consequences of them not\nlearning in a vacuum either. Infants have access to a rich input from other agents by observing them as well as by active\nengagement from them. Parental scaffolding, a process where parents are helping and guiding their infant to a greater\nor lesser extent depending on the infant’s needs, has been shown to be uniquely important for infant development\n[176, 180]. For example, when infants play with a parent (or other more experienced person), infants’ quality of\nplay increases to an extent that could not be achieved by the infant alone [176]. This heightened level of play is a\nconsequence of the parent’s attention and contingent reactions to the infant’s actions [16]. This means that scaffolding\nand guided play is both interactive as well as dynamic [183], ensuring maximal support to the infant’s learning process.\nDuring parent-child interaction, the parent can direct their attention to the infant’s object of attention, or redirect the\ninfant’s attention to another object. Such joint attention episodes are considered ‘hot spots’ for language learning [165]\nand a ‘major contributor’ to social development [115]. Moreover, parental attention directing behaviours have been\nshown to inﬂuence infants’ exploration behaviour [8] and cognitive development [96, 95]. While playing with infants,\nparents also tend to exaggerate their actions, which has obvious pedagogical beneﬁts [19, 172]. This behaviour was\ncoined ‘motionese’ [19] and is considered to be the action-counterpart to ‘motherese’ [118]. Motherese (or: infant-\n6\nSEPTEMBER 21, 2020\ndirected speech) is thought to help infants learn aspects of language (e.g. word segmentation and word recognition)\ndue to its exaggerated use of changes in pitch and tempo [149, 163]. Thus, although parents do not generally explicitly\nteach their children in early infancy, they do tend to tailor their behaviour in such a way that allows the infant to better\nlearn.\nIn addition to this active engagement from other agents, infants also learn from others by simply observing them.\nFor example, [131] found that infants who had observed an adult activate a knob were more successful at repeating\nthis action than infants who only had had active experience with the knob themselves. Learning by observation and\nimitation has generally been widely explored and debated in the infant ﬁeld [109, 122, 134] and has been shown\nto speed up learning of language acquisition [91, 130], action understanding and production [44, 68, 126], social\ncognition [108], and so on. Notably, some studies have shown that learning from observing others is modulated by\ninfants’ experience with how reliable or knowledgeable the other agent is [129, 158, 167].\nInspired by human learning, scaffolding and other forms of human pedagogy have been successfully adopted in (devel-\nopmental) robotics [116, 123, 139, 170]. However, one might question whether there is a place for these approaches\nin unsupervised ML. Would parenting an algorithm not make it supervised learning? We propose that it is something\nin between. Although parents at times indeed provide direct feedback to infants, their importance for learning is much\nrather driven by their broader perspective. By being a sort of meta-learner, they can provide infants with the push they\nneed to enter a new, steeper learning curve (see e.g. [131]). While there is some work on teaching in artiﬁcial intelli-\ngence research [15], little attention has been directed to using an ‘expert’ network to guide (but not directly instruct)\na ‘novice’ network. Training unsupervised ML networks ﬁrst on auxiliary tasks to boost performance on a target task\n(see e.g. [141]) and curriculum learning (see Sec. 2.3), may be ML’s closest procedures to parental scaffolding as these\ncan be re-conceptualised as the researcher or engineer scaffolding the network.\nTL;DR: Other agents in infants’ everyday life provide essential scaffolds to their learning process. In unsupervised\nML, parental scaffolding has not been widely adopted. However, the successes gained by training networks ﬁrst on\nauxiliary tasks before moving onto the target task suggests that this is a space worth exploring. As concluded by [17]\non computationally modelling infant learning: “[...] to understand power of children’s learning, it is important to\ninvestigate it in a social context.” (p. 98)\n3\nDiscussion\nUnsupervised ML approaches beneﬁt from ﬂexibly and efﬁciently learning ﬂexible and efﬁcient representations. Here\nwe have argued that developmental science offers a unique source of inspiration; infants learn effective, generalised\nand transferable representations from relatively small numbers of quite heterogeneous examples. We have presented\nﬁve core insights from developmental science that we believe can make a fundamental difference to representation\nlearning in ML. We have focused especially on infant learning in the ﬁrst year of life as their learning process also\nrequires them to learn useful representations from unlabelled data. Throughout the paper, we have spelled out to what\nextent components of infant learning are already mirrored in ML algorithms, and where further steps can reasonably\nbe made. Improving the quality, ﬂexibility and efﬁciency of learned representations will directly translate to improved\nML performance.\nWe demonstrated that although infants and ML systems share commonalities (e.g., both perform statistical learning),\nthe currently prevailing practices in ML—to remove targeted interference in the learning process as much as possible\nand leave everything to be learned to the data itself [10]—stands in stark contrast to infant learning. Infants’ input has\nbeen found to be optimised for learning about speciﬁc features of the input. When comparing the ﬁve lessons on infant\nlearning to current approaches in ML, two overarching insights can be extracted:\n1. There is more structure to constrain and guide infants’ learning processes (Sec. 2.1 and 2.3).\n2. Infants’ learning opportunities are more ﬂexible (Sec. 2.4) and richer (Sec. 2.2 and 2.5).\nFactors like innate biases, saliency, curiosity, and development over time all play an important role in shaping infants’\nlearning curriculum and contribute to the speed and ﬂexibility with which infants learn. Reﬂecting these insights back\ninto ML, they cast signiﬁcant doubt on the assumption that ‘the data will ﬁx it’ is indeed the most efﬁcient and effective\napproach to training neural networks.\nNeither the developmental nor the ML research presented in this paper is exhaustive. Nonetheless, we hope to have\nprovided a representative sample of prior work on the topics we have addressed. Importantly, the ﬁve lessons presented\nhere were chosen based on their potential to qualitatively improve the next generation of unsupervised ML algorithms—\neither by introducing previously unconsidered aspects, or by reinforcing and helping to further evolve ongoing work—\n7\nSEPTEMBER 21, 2020\nas well as on their integration with current ML implementations. By focusing on this intersection, we aim to increase\nthe likelihood that these lessons can be meaningfully considered in theory and implementations.\nThe argument to take inspiration from human (infant) learning that we and others have made rests on the observation\nthat human learning leads to robust representations that can be ﬂexibly used in various tasks with an acceptable to\nexcellent level of performance across the board. Clearly, some scepticism is warranted as to whether adopting insights\nfrom infant learning will be equally valuable for all ML purposes. It is possible that some of the lessons provide an\nadvantage across domains whereas others might turn out to be particularly beneﬁcial for speciﬁc tasks. Taking this\npaper as a conceptual anchor, future research will explore the exact interactions of each of the given insights from\ninfant learning with its counterpart(s) in ML.\nAcknowledgements\nThis work was supported by the ERC Advanced Grant FOUNDCOG, No. #787981, awarded to Rhodri Cusack and\nthe MSCA Individual Fellowship InterPlay, No. #891535, awarded to Lorijn Zaadnoordijk.\nReferences\n[1] Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep networks. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net, 2019.\n[2] Raquel G Alhama and Willem Zuidema. Pre-wiring and pre-training: What does a neural network need to learn\ntruly general identity rules? Journal of Artiﬁcial Intelligence Research, 61:927–946, 2018.\n[3] Richard N Aslin. What’s in a look? Developmental Science, 10(1):48–53, 2007.\n[4] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual\ninformation across views. In Advances in Neural Information Processing Systems, pages 15509–15519, 2019.\n[5] Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Proceedings of ICML Workshop\non Unsupervised and Transfer Learning, pages 37–49, 2012.\n[6] Francesco Barbieri, Eric Guizzo, Federico Lucchesi, Giovanni Maffei, Fermín Moscoso del Prado Martín, and\nTillman Weyde. Towards a multimodal time-based empathy prediction system. In 2019 14th IEEE International\nConference on Automatic Face & Gesture Recognition (FG 2019), pages 1–5. IEEE, 2019.\n[7] Marina Bedny. Evidence from blindness for a cognitively pluripotent cortex. Trends in Cognitive Sciences,\n21(9):637–648, 2017.\n[8] Jay Belsky, Mary Kay Goode, and Robert K Most. Maternal stimulation and infant exploratory competence:\nCross-sectional, correlational, and experimental analyses. Child Development, pages 1168–1178, 1980.\n[9] Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceedings of\nICML Workshop on Unsupervised and Transfer Learning, pages 17–36, 2012.\n[10] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013.\n[11] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep\nnetworks. In Advances in Neural Information Processing Systems, pages 153–160, 2007.\n[12] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings\nof the 26th Annual International Conference on Machine Learning, pages 41–48, 2009.\n[13] Daniel E Berlyne. An experimental study of human curiosity. British Journal of Psychology. General Section,\n45(4):256–265, 1954.\n[14] Daniel E Berlyne. Conﬂict, Arousal, and Curiosity. McGraw-Hill Book Company, 1960.\n[15] Jordi Bieger, Kristinn R Thórisson, and Bas R Steunebrink. The pedagogical pentagon: A conceptual framework\nfor artiﬁcial pedagogy. In International Conference on Artiﬁcial General Intelligence, pages 212–222. Springer,\n2017.\n[16] Ann E Bigelow, Kim MacLean, and Jane Proctor. The role of joint attention in the development of infants’ play\nwith objects. Developmental Science, 7(5):518–526, 2004.\n[17] Elizabeth Bonawitz and Patrick Shafto. Computational models of development, social inﬂuences. Current\nOpinion in Behavioral Sciences, 7:95–100, 2016.\n8\nSEPTEMBER 21, 2020\n[18] Matthew M Botvinick, Yael Niv, and Andew G Barto. Hierarchically organized behavior and its neural founda-\ntions: A reinforcement learning perspective. Cognition, 113(3):262–280, 2009.\n[19] Rebecca J Brand, Dare A Baldwin, and Leslie A Ashburn. Evidence for ‘motionese’: modiﬁcations in mothers’\ninfant-directed action. Developmental Science, 5(1):72–83, 2002.\n[20] Andrew J Bremner. Developing body representations in early life: combining somatosensation and vision to\nperceive the interface between the body and the world. Developmental Medicine & Child Neurology, 58:12–16,\n2016.\n[21] Andrew J Bremner, Denis Mareschal, Sarah Lloyd-Fox, and Charles Spence. Spatial localization of touch in\nthe ﬁrst year of life: Early inﬂuence of a visual spatial code and the development of remapping across changes\nin limb position. Journal of Experimental Psychology: General, 137(1):149, 2008.\n[22] Davina Bristow, Ghislaine Dehaene-Lambertz, Jeremie Mattout, Catherine Soares, Teodora Gliga, Sylvain Bail-\nlet, and Jean-François Mangin. Hearing faces: how the infant brain matches the face it sees with the speech it\nhears. Journal of Cognitive Neuroscience, 21(5):905–921, 2008.\n[23] Helen Brookes, Alan Slater, Paul C Quinn, David J Lewkowicz, Rachel Hayes, and Elizabeth Brown. Three-\nmonth-old infants learn arbitrary auditory–visual pairings between voices and faces. Infant and Child Develop-\nment: An International Journal of Research and Practice, 10(1-2):75–82, 2001.\n[24] Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexan-\nder Lerchner. Understanding disentangling in β-VAE. arXiv preprint arXiv:1804.03599, 2018.\n[25] Laura Cabral, Leire Zubiaurre, Conor Wild, Annika Linke, and Rhodri Cusack.\nCategory-selective visual\nregions have distinctive signatures of connectivity in early infancy. bioRxiv, page 675421, 2019.\n[26] Giulia Cappagli, Elena Cocchi, and Monica Gori. Auditory and proprioceptive spatial impairments in blind\nchildren and adults. Developmental Science, 20(3):e12374, 2017.\n[27] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pages\n132–149, 2018.\n[28] Gail A. Carpenter and Stephen Grossberg. The art of adaptive pattern recognition by a self-organizing neural\nnetwork. Computer, 21(3):77–88, 1988.\n[29] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In\nproceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299–6308, 2017.\n[30] David A Caruso. Dimensions of quality in infants’ exploratory behavior: Relationships to problem-solving\nability. Infant Behavior and Development, 16(4):441–454, 1993.\n[31] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\nlearning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\n[32] Sarina Hui-Lin Chien. No more top-heavy bias: Infants and adults prefer upright faces but not top-heavy\ngeometric or face-like patterns. Journal of Vision, 11(6):13–13, 2011.\n[33] Robin Panneton Cooper and Richard N Aslin. Preference for infant-directed speech in the ﬁrst month after birth.\nChild Development, 61(5):1584–1595, 1990.\n[34] Rhodri Cusack, Conor J Wild, Leire Zubiaurre-Elorza, and Annika C Linke. Why does language not emerge\nuntil the second year? Hearing Research, 366:75–81, 2018.\n[35] Hy I Day. Curiosity and the interested explorer. Performance & Instruction, 1982.\n[36] Virginia R. de Sa and Dana H. Ballard. Category learning through multimodality sensing. Neural Computation,\n10(5):1097–1117, 1998.\n[37] Ben Deen, Hilary Richardson, Daniel D Dilks, Atsushi Takahashi, Boris Keil, Lawrence L Wald, Nancy Kan-\nwisher, and Rebecca Saxe. Organization of high-level visual cortex in human infants. Nature Communications,\n8(1):1–10, 2017.\n[38] German Diez-Valencia, Takuya Ohashi, Pablo Lanillos, and Gordon Cheng. Sensorimotor learning for artiﬁcial\nbody perception. arXiv preprint arXiv:1901.09792, 2019.\n[39] Velma Dobson and Davida Y Teller. Visual acuity in human infants: A review and comparison of behavioral\nand electrophysiological studies. Vision research, 18(11):1469–1483, 1978.\n9\nSEPTEMBER 21, 2020\n[40] Valentina Doria, Christian F Beckmann, Tomoki Arichi, Nazakat Merchant, Michela Groppo, Federico E\nTurkheimer, Serena J Counsell, Maria Murgasova, Paul Aljabar, Rita G Nunes, et al. Emergence of resting\nstate networks in the preterm human brain. Proceedings of the National Academy of Sciences, 107(46):20015–\n20020, 2010.\n[41] Timothy J Draelos, Nadine E Miner, Christopher C Lamb, Jonathan A Cox, Craig M Vineyard, Kristofor D\nCarlson, William M Severa, Conrad D James, and James B Aimone. Neurogenesis deep learning: Extending\ndeep networks to accommodate new classes.\nIn 2017 International Joint Conference on Neural Networks\n(IJCNN), pages 526–533. IEEE, 2017.\n[42] Alain Droniou, Serena Ivaldi, and Olivier Sigaud. Deep unsupervised network for multimodal perception,\nrepresentation and classiﬁcation. Robotics and Autonomous Systems, 71:83–98, 2015.\n[43] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition,\n48(1):71–99, 1993.\n[44] Birgit Elsner. Infants’ imitation of goal-directed actions: The role of movements and action effects. Acta\nPsychologica, 124(1):44–59, 2007.\n[45] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio.\nWhy does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625–\n660, 2010.\n[46] Georgios Evangelopoulos, Athanasia Zlatintsi, Alexandros Potamianos, Petros Maragos, Konstantinos Ra-\npantzikos, Georgios Skoumas, and Yannis Avrithis. Multimodal saliency and fusion for movie summarization\nbased on aural, visual, and textual attention. IEEE Transactions on Multimedia, 15(7):1553–1568, 2013.\n[47] Caitlin M Fausey, Swapnaa Jayaraman, and Linda B Smith. From faces to hands: Changing visual input in the\nﬁrst two years. Cognition, 152:101–107, 2016.\n[48] Reuben Feinman and Brenden M Lake. Learning inductive biases with simple neural networks. arXiv preprint\narXiv:1802.02745, 2018.\n[49] Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. Unsupervised image captioning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 4125–4134, 2019.\n[50] J Fiser, R Aslin, A Lathrop, C Rothkopf, and J Markant. An infants’ eye view of the world: Implications for\nlearning in natural contexts. In International Conference on Infant Studies, Kyoto, Japan, 2006.\n[51] John M Franchak, Kari S Kretch, Kasey C Soska, and Karen E Adolph. Head-mounted eye tracking: A new\nmethod to describe infant looking. Child Development, 82(6):1738–1750, 2011.\n[52] Robert M French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3(4):128–\n135, 1999.\n[53] Rebecca Gómez and Jessica Maye. The developmental trajectory of nonadjacent dependency learning. Infancy,\n7(2):183–206, 2005.\n[54] Jacqueline Gottlieb and Pierre-Yves Oudeyer. Towards a neuroscience of active sampling and curiosity. Nature\nReviews Neuroscience, 19(12):758–770, 2018.\n[55] Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum\nlearning for neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pages 1311–1320. JMLR. org, 2017.\n[56] Hyowon Gweon, Joshua B Tenenbaum, and Laura E Schulz. Infants consider both the sample and the sampling\nprocess in inductive generalization. Proceedings of the National Academy of Sciences, 107(20):9066–9071,\n2010.\n[57] Nick Haber, Damian Mrowca, Li Fei-Fei, and Daniel LK Yamins. Emergence of structured behaviors from\ncuriosity-based intrinsic motivation. arXiv preprint arXiv:1802.07461, 2018.\n[58] Verena V Hafner, Pontus Loviken, Antonio Pico Villalpando, and Guido Schillaci. Prerequisites for an artiﬁcial\nself. Frontiers in Neurorobotics, 14, 2020.\n[59] Marshall M Haith. Who put the cog in infant cognition? is rich interpretation too costly? Infant Behavior and\nDevelopment, 21(2):167–179, 1998.\n[60] Demis Hassabis, Dharshan Kumaran, Christopher Summerﬁeld, and Matthew Botvinick. Neuroscience-inspired\nartiﬁcial intelligence. Neuron, 95(2):245–258, 2017.\n[61] Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 4918–4927, 2019.\n10\nSEPTEMBER 21, 2020\n[62] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural\nComputation, 18(7):1527–1554, 2006.\n[63] Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth\nannual conference of the Cognitive Science Society, pages 177–186, 1987.\n[64] Stefanie Hoehl, Kahl Hellmer, Maria Johansson, and Gustaf Gredebäck. Itsy bitsy spider...: Infants react with\nincreased arousal to spiders and snakes. Frontiers in Psychology, 8:1710, 2017.\n[65] Stefanie Hoehl and Sabina Pauen. Do infants associate spiders and snakes with fearful facial expressions?\nEvolution and Human Behavior, 38(3):404–413, 2017.\n[66] Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learning? arXiv\npreprint arXiv:1608.08614, 2016.\n[67] Sabine Hunnius and Harold Bekkering. The early development of object knowledge: A study of infants’ visual\nanticipations during action observation. Developmental Psychology, 46(2):446, 2010.\n[68] Sabine Hunnius and Harold Bekkering.\nWhat are you doing?\nhow active and observational experience\nshape infants’ action understanding. Philosophical Transactions of the Royal Society B: Biological Sciences,\n369(1644):20130490, 2014.\n[69] Michael A Hunter and Elinor W Ames. A multifactor model of infant preferences for novel and familiar stimuli.\nAdvances in Infancy Research, 1988.\n[70] Hiroko Ichikawa, Aki Tsuruhara, So Kanazawa, and Masami K Yamaguchi. Two-to three-month-old infants\nprefer moving face patterns to moving top-heavy patterns. Japanese Psychological Research, 55(3):254–263,\n2013.\n[71] Serena Ivaldi, Natalia Lyubova, Alain Droniou, Vincent Padois, David Filliat, Pierre-Yves Oudeyer, Olivier\nSigaud, et al. Object learning through active exploration. IEEE Transactions on Autonomous Mental Develop-\nment, 6(1):56–72, 2013.\n[72] Lisa Jacquey, Jacqueline Fagard, Rana Esseily, and J Kevin O’Regan. Detection of sensorimotor contingencies\nin infants before the age of one year: a comprehensive review. PsyArXiv, 2019.\n[73] Herbert Jaeger. The “echo state” approach to analysing and training recurrent neural networks-with an erratum\nnote. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,\n148(34):13, 2001.\n[74] Khurram Javed and Martha White. Meta-learning representations for continual learning. In Advances in Neural\nInformation Processing Systems, pages 1818–1828, 2019.\n[75] Swapnaa Jayaraman, Caitlin M Fausey, and Linda B Smith. The faces in infant-perspective scenes change over\nthe ﬁrst year of life. PLoS ONE, 10(5), 2015.\n[76] Lloyd A Jeffress. A place theory of sound localization. Journal of Comparative and Physiological Psychology,\n41(1):35, 1948.\n[77] Frederik S Kamps, Cassandra L Hendrix, Patricia A Brennan, and Daniel D Dilks. Connectivity at the origins\nof domain speciﬁcity in the cortical face and place networks. Proceedings of the National Academy of Sciences,\n117(11):6163–6169, 2020.\n[78] Lana B Karasik, Catherine S Tamis-LeMonda, and Karen E Adolph. Transition from crawling to walking and\ninfants’ actions with objects and people. Child Development, 82(4):1199–1209, 2011.\n[79] Ezgi Kayhan, Gustaf Gredebäck, and Marcus Lindskog. Infants distinguish between two events based on their\nrelative likelihood. Child Development, 89(6):e507–e519, 2018.\n[80] Ezgi Kayhan, S Hunnius, JX O’Reilly, and Harold Bekkering. Infants differentially update their internal models\nof a dynamic environment. Cognition, 186:139–146, 2019.\n[81] Ezgi Kayhan, Marlene Meyer, Jill X O’Reilly, Sabine Hunnius, and Harold Bekkering. Nine-month-old infants\nupdate their predictive models of a changing environment. Developmental Cognitive Neuroscience, 38:100680,\n2019.\n[82] Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. arXiv preprint\narXiv:1711.10563, 2017.\n[83] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes, and Christopher Kanan. Measuring catas-\ntrophic forgetting in neural networks. In Thirty-second AAAI conference on artiﬁcial intelligence, 2018.\n[84] Celeste Kidd. How to Know (Keynote). Thirty-third Conference on Neural Information Processing Systems\n(NeurIPS), 2019.\n11\nSEPTEMBER 21, 2020\n[85] Celeste Kidd, Steven T Piantadosi, and Richard N Aslin. The goldilocks effect: Human infants allocate attention\nto visual sequences that are neither too simple nor too complex. PloS ONE, 7(5), 2012.\n[86] Celeste Kidd, Steven T Piantadosi, and Richard N Aslin. The goldilocks effect in infant auditory attention.\nChild Development, 85(5):1795–1804, 2014.\n[87] Natasha Z Kirkham, Jonathan A Slemmer, and Scott P Johnson. Visual statistical learning in infancy: Evidence\nfor a domain general learning mechanism. Cognition, 83(2):B35–B42, 2002.\n[88] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic\nforgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526, 2017.\n[89] Radha Manisha Kopparti and Weyde Tillman. Weight priors for learning identity relations. arXiv preprint\narXiv:2003.03125, 2020.\n[90] Sid Kouider, Bria Long, Lorna Le Stanc, Sylvain Charron, Anne-Caroline Fievet, Leonardo S Barbosa, and\nSoﬁe V Gelskov. Neural dynamics of prediction and surprise in infants. Nature Communications, 6(1):1–8,\n2015.\n[91] Efﬁe Kymissis and Claire L Poulson. The history of imitation in learning theory: The language acquisition\nprocess. Journal of the Experimental Analysis of Behavior, 54(2):113–127, 1990.\n[92] David A Lagnado and Steven Sloman. Learning causal structure. In Proceedings of the Annual Meeting of the\nCognitive Science Society, volume 24, 2002.\n[93] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that\nlearn and think like people. Behavioral and Brain Sciences, 40, 2017.\n[94] Simon P Landry, Jean-Paul Guillemot, and François Champoux. Temporary deafness can impair multisensory\nintegration: a study of cochlear-implant users. Psychological Science, 24(7):1260–1268, 2013.\n[95] Susan H Landry, Karen E Smith, and Paul R Swank. Responsive parenting: establishing early foundations for\nsocial, communication, and independent problem-solving skills. Developmental Psychology, 42(4):627, 2006.\n[96] Susan H Landry, Karen E Smith, Paul R Swank, and Cynthia L Miller-Loncar. Early maternal and child\ninﬂuences on children’s later independent cognitive and social functioning. Child Development, 71(2):358–375,\n2000.\n[97] Mathieu Lefort and Alexander Gepperth. Active learning of local predictable representations with artiﬁcial\ncuriosity. In 2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics\n(ICDL-EpiRob), pages 228–233. IEEE, 2015.\n[98] David J Lewkowicz. The development of intersensory temporal perception: an epigenetic systems/limitations\nview. Psychological Bulletin, 126(2):281, 2000.\n[99] Annika C Linke, Conor Wild, Leire Zubiaurre-Elorza, Charlotte Herzmann, Hester Duffy, Victor K Han,\nDavid SC Lee, and Rhodri Cusack. Disruption to functional networks in neonates with perinatal brain injury\npredicts motor skills at 8 months. NeuroImage: Clinical, 18:399–406, 2018.\n[100] Michele A Lobo, Elena Kokkoni, Ana Carolina de Campos, and James C Galloway. Not just playing around:\nInfants’ behaviors with objects reﬂect ability, constraints, and object properties. Infant Behavior and Develop-\nment, 37(3):334–351, 2014.\n[101] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431–3440, 2015.\n[102] Wei Ji Ma and Benjamin Peters. A neural network walks into a lab: Towards using deep nets as models for\nhuman behavior. arXiv preprint arXiv:2005.02181, 2020.\n[103] Lara Maister, Teresa Tang, and Manos Tsakiris. Neurobehavioral evidence of interoceptive sensitivity in early\ninfancy. Elife, 6:e25318, 2017.\n[104] Gary F Marcus, Sugumaran Vijayan, S Bandi Rao, and Peter M Vishton. Rule learning by seven-month-old\ninfants. Science, 283(5398):77–80, 1999.\n[105] Zoltan-Csaba Marton, Dejan Pangercic, Nico Blodow, and Michael Beetz. Combined 2d–3d categorization and\nclassiﬁcation for multimodal perception systems. The International Journal of Robotics Research, 30(11):1378–\n1402, 2011.\n[106] Jessica Maye, Janet F Werker, and LouAnn Gerken. Infant sensitivity to distributional information can affect\nphonetic discrimination. Cognition, 82(3):B101–B111, 2002.\n12\nSEPTEMBER 21, 2020\n[107] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The Bulletin\nof Mathematical Biophysics, 5(4):115–133, 1943.\n[108] Andrew N Meltzoff and Jean Decety. What imitation tells us about social cognition: a rapprochement between\ndevelopmental psychology and cognitive neuroscience. Philosophical Transactions of the Royal Society of\nLondon. Series B: Biological Sciences, 358(1431):491–500, 2003.\n[109] Andrew N Meltzoff and M Keith Moore. Explaining facial imitation: A theoretical model. Infant and Child\nDevelopment, 6(3-4):179–192, 1997.\n[110] Grégoire Mesnil, Yann Dauphin, Xavier Glorot, Salah Rifai, Yoshua Bengio, Ian Goodfellow, Erick Lavoie,\nXavier Muller, Guillaume Desjardins, David Warde-Farley, et al. Unsupervised and transfer learning challenge:\na deep learning approach. In Proceedings of the 2011 International Conference on Unsupervised and Transfer\nLearning workshop-Volume 27, pages 97–111, 2011.\n[111] Ryszard S Michalski. A theory and methodology of inductive learning. In Machine learning, pages 83–134.\nSpringer, 1983.\n[112] TM Mitchell. The need for biases in learning generalizations (rutgers computer science tech. rept. cbm-tr-117).\nRutgers University, 1980.\n[113] Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow. Trends in integration of vision and language\nresearch: A survey of tasks, datasets, and methods. arXiv preprint arXiv:1907.09358, 2019.\n[114] Claire D Monroy, Sarah A Gerson, Estefanía Domínguez-Martínez, Katharina Kaduk, Sabine Hunnius, and\nVincent Reid. Sensitivity to structure in action sequences: an infant event-related potential study. Neuropsy-\nchologia, 126:92–101, 2019.\n[115] Peter Mundy and Lisa Newell. Attention, joint attention, and social cognition. Current Directions in Psycho-\nlogical Science, 16(5):269–274, 2007.\n[116] Yukie Nagai and Katharina J Rohlﬁng. Computational analysis of motionese toward scaffolding robot action\nlearning. IEEE Transactions on Autonomous Mental Development, 1(1):44–54, 2009.\n[117] Tomoaki Nakamura, Takayuki Nagai, and Naoto Iwahashi. Multimodal object categorization by a robot. In\n2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 2415–2420. IEEE, 2007.\n[118] Elissa L. Newport. Motherese: The speech of mothers to young children. In I D. B. Pisoni N. J. Castellan and\nG. Potts, editors, Cognitive Theory, volume 2. Erlbaum, Hillsdale N.J., 1977.\n[119] Elissa L Newport, Daphne Bavelier, and Helen J Neville. Critical thinking about critical periods: Perspectives\non a critical period for language acquisition. Language, brain and cognitive development: Essays in honor of\nJacques Mehler, pages 481–502, 2001.\n[120] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y. Ng. Multimodal deep\nlearning. In Proceedings of the 28th International Conference on International Conference on Machine Learn-\ning, ICML’11, page 689–696, Madison, WI, USA, 2011. Omnipress.\n[121] Lisa M Oakes and Heidi A Baumgartner. Manual object exploration and learning about object features in\nhuman infants. In 2012 IEEE International Conference on Development and Learning and Epigenetic Robotics\n(ICDL), pages 1–6. IEEE, 2012.\n[122] Janine Oostenbroek, Jonathan Redshaw, Jacqueline Davis, Siobhan Kennedy-Costantini, Mark Nielsen, Vir-\nginia Slaughter, and Thomas Suddendorf. Re-evaluating the neonatal imitation hypothesis. Developmental\nScience, 22(2):e12720, 2018.\n[123] Nuno Otero, Joe Saunders, Kerstin Dautenhahn, and Chrystopher L Nehaniv. Teaching robot companions: the\nrole of scaffolding and event structuring. Connection Science, 20(2-3):111–134, 2008.\n[124] Pierre-Yves Oudeyer. Computational theories of curiosity-driven learning. arXiv preprint arXiv:1802.10546,\n2018.\n[125] Pierre-Yves Oudeyer, Frédéric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous\nmental development. IEEE Transactions on Evolutionary Computation, 11(2):265–286, 2007.\n[126] Markus Paulus. How and why do infants imitate? an ideomotor approach to social and imitative learning in\ninfancy (and beyond). Psychonomic Bulletin & Review, 21(5):1139–1156, 2014.\n[127] Marcela Peña, Atsushi Maki, Damir Kova˘ci´c, Ghislaine Dehaene-Lambertz, Hideaki Koizumi, Furio Bouquet,\nand Jacques Mehler. Sounds and silence: an optical topography study of language recognition at birth. Pro-\nceedings of the National Academy of Sciences, 100(20):11702–11705, 2003.\n[128] Judea Pearl. Causality. Cambridge University Press, 2009.\n13\nSEPTEMBER 21, 2020\n[129] Diane Poulin-Dubois, Ivy Brooker, and Alexandra Polonia. Infants prefer to imitate a reliable person. Infant\nBehavior and Development, 34(2):303–309, 2011.\n[130] Claire L Poulson, Efﬁe Kymissis, Kenneth F Reeve, Maria Andreatos, and Lori Reeve. Generalized vocal\nimitation in infants. Journal of Experimental Child Psychology, 51(2):267–279, 1991.\n[131] Joëlle Provasi, Corine D Dubon, and Henriette Bloch. Do 9-and 12-month-olds learn means-ends relation by\nobserving? Infant Behavior and Development, 24(2):195–213, 2001.\n[132] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional\ngenerative adversarial networks. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on\nLearning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings,\n2016.\n[133] Amal Rannen, Rahaf Aljundi, Matthew B Blaschko, and Tinne Tuytelaars. Encoder based lifelong learning. In\nProceedings of the IEEE International Conference on Computer Vision, pages 1320–1328, 2017.\n[134] Elizabeth Ray and Cecilia Heyes. Imitation in infancy: The wealth of the stimulus. Developmental Science,\n14(1):92–105, 2011.\n[135] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize\nto imagenet? arXiv preprint arXiv:1902.10811, 2019.\n[136] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with\nregion proposal networks. In Advances in Neural Information Processing Systems, pages 91–99, 2015.\n[137] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123–146,\n1995.\n[138] Philippe Rochat and Tricia Striano. Perceived self in infancy. Infant Behavior and Development, 23(3-4):513–\n530, 2000.\n[139] Katharina J Rohlﬁng, Jannik Fritsch, Britta Wrede, and Tanja Jungmann. How can multimodal cues from\nchild-directed interaction reduce learning complexity in robots? Advanced Robotics, 20(10):1183–1199, 2006.\n[140] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain.\nPsychological Review, 65(6):386, 1958.\n[141] Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098,\n2017.\n[142] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. Interna-\ntional Journal of Computer Vision, 115(3):211–252, 2015.\n[143] Jenny R Saffran, Richard N Aslin, and Elissa L Newport. Statistical learning by 8-month-old infants. Science,\n274(5294):1926–1928, 1996.\n[144] Andrew Saxe, Stephanie Nelli, and Christopher Summerﬁeld. If deep learning is the answer, then what is the\nquestion? arXiv preprint arXiv:2004.07580, 2020.\n[145] Jürgen Schmidhuber. Driven by compression progress: A simple principle explains essential aspects of sub-\njective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes. In\nWorkshop on Anticipatory Behavior in Adaptive Learning Systems, pages 48–76. Springer, 2008.\n[146] Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison Department\nof Computer Sciences, 2009.\n[147] Burr Settles. From theories to queries: Active learning in practice. In Active Learning and Experimental Design\nworkshop In conjunction with AISTATS 2010, pages 1–18, 2011.\n[148] Francesca Simion, Elisa Di Giorgio, Irene Leo, and Lara Bardi. The processing of social stimuli in early\ninfancy: From faces to biological motion perception. In Progress in Brain Research, volume 189, pages 173–\n193. Elsevier, 2011.\n[149] Leher Singh, Sarah Nestor, Chandni Parikh, and Ashley Yull. Inﬂuences of infant-directed speech on early word\nrecognition. Infancy, 14:654–666, 2009.\n[150] Fabian H Sinz, Xaq Pitkow, Jacob Reimer, Matthias Bethge, and Andreas S Tolias. Engineering a less artiﬁcial\nintelligence. Neuron, 103(6):967–979, 2019.\n[151] Linda B Smith. Do infants possess innate knowledge structures? the con side. Developmental Science, 2(2):133–\n144, 1999.\n14\nSEPTEMBER 21, 2020\n[152] Linda B Smith, Swapnaa Jayaraman, Elizabeth Clerkin, and Chen Yu. The developing infant creates a curricu-\nlum for statistical learning. Trends in Cognitive Sciences, 22(4):325–336, 2018.\n[153] Samuel Sokol. Measurement of infant visual acuity from pattern reversal evoked potentials. Vision research,\n18(1):33–39, 1978.\n[154] Kasey C Soska and Karen E Adolph. Postural position constrains multimodal object exploration in infants.\nInfancy, 19(2):138–161, 2014.\n[155] Elizabeth Spelke. Initial knowledge: Six suggestions. Cognition on Cognition, pages 433–447, 1995.\n[156] Elizabeth S Spelke. Perceiving bimodally speciﬁed events in infancy. Developmental Psychology, 15(6):626,\n1979.\n[157] Nitish Srivastava and Russ R Salakhutdinov. Multimodal learning with deep boltzmann machines. In Advances\nin Neural Information Processing systems, pages 2222–2230, 2012.\n[158] Gunilla Stenberg. Why do infants look at and use positive information from some informants rather than others\nin ambiguous situations? Infancy, 17(6):642–671, 2012.\n[159] Ryan Stevenson, Sterling W Shefﬁeld, Iliza M Butera, René H Gifford, and Mark Wallace.\nMultisensory\nintegration in cochlear implant recipients. Ear and Hearing, 38(5):521, 2017.\n[160] Yukari Tanaka, Yasuhiro Kanakogi, Masahiro Kawasaki, and Masako Myowa. The integration of audio- tactile\ninformation is modulated by multimodal social interaction with physical contact in infancy. Developmental\nCognitive Neuroscience, 30:31–40, 2018.\n[161] Tuomas Teinonen, Vineta Fellman, Risto Näätänen, Paavo Alku, and Minna Huotilainen. Statistical language\nlearning in neonates revealed by event-related brain potentials. BMC Neuroscience, 10(1):21, 2009.\n[162] Joshua B Tenenbaum, Charles Kemp, Thomas L Grifﬁths, and Noah D Goodman.\nHow to grow a mind:\nStatistics, structure, and abstraction. Science, 331(6022):1279–1285, 2011.\n[163] Erik D Thiessen, Emily A Hill, and Jenny R Saffran. Infant-directed speech facilitates word segmentation.\nInfancy, 7(1):53–71, 2005.\n[164] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nContrastive multiview coding.\narXiv preprint\narXiv:1906.05849, 2019.\n[165] Michael Tomasello. The role of joint attentional processes in early language development. Language Sciences,\n10(1):69–88, 1988.\n[166] Kristen Swan Tummeltshammer and Natasha Z Kirkham. Learning to look: Probabilistic variation and noise\nguide infants’ eye movements. Developmental Science, 16(5):760–771, 2013.\n[167] Kristen Swan Tummeltshammer, Rachel Wu, David M Sobel, and Natasha Z Kirkham. Infants track the relia-\nbility of potential informants. Psychological Science, 25(9):1730–1738, 2014.\n[168] Katherine E Twomey and Gert Westermann. Curiosity-based learning in infants: a neurocomputational ap-\nproach. Developmental Science, 21(4):e12629, 2018.\n[169] Panagiotis Tzirakis, George Trigeorgis, Mihalis A Nicolaou, Björn W Schuller, and Stefanos Zafeiriou. End-\nto-end multimodal emotion recognition using deep neural networks. IEEE Journal of Selected Topics in Signal\nProcessing, 11(8):1301–1309, 2017.\n[170] Emre Ugur, Hande Celikkanat, Erol ¸Sahin, Yukie Nagai, and Erhan Oztop. Learning to grasp with parental\nscaffolding. In 2011 11th IEEE-RAS International Conference on Humanoid Robots, pages 480–486. IEEE,\n2011.\n[171] Shimon Ullman, Daniel Harari, and Nimrod Dorfman. From simple innate biases to complex visual concepts.\nProceedings of the National Academy of Sciences, 109(44):18215–18220, 2012.\n[172] Johanna E van Schaik, Marlene Meyer, Camila R van Ham, and Sabine Hunnius. Motion tracking of parents’\ninfant-versus adult-directed actions reveals general and action-speciﬁc modulations. Developmental Science,\n23(1):e12869, 2020.\n[173] Cassia Viola Macchi, Chiara Turati, and Francesca Simion. Can a nonspeciﬁc bias toward top-heavy patterns\nexplain newborns’ face preference? Psychological Science, 15(6):379–383, 2004.\n[174] Lukas Vogelsang, Sharon Gilad-Gutnick, Evan Ehrenberg, Albert Yonas, Sidney Diamond, Richard Held, and\nPawan Sinha. Potential downside of high initial visual acuity. Proceedings of the National Academy of Sciences,\n115(44):11333–11338, 2018.\n15\nSEPTEMBER 21, 2020\n[175] Athena Vouloumanos and Janet F Werker. Listening to language at birth: Evidence for a bias for speech in\nneonates. Developmental Science, 10(2):159–164, 2007.\n[176] Lev Semenovich Vygotsky. Mind in Society: The Development of Higher Psychological processes. Harvard\nUniversity Press, 1978.\n[177] Hama Watanabe and Gentaro Taga. Initial-state dependency of learning in young infants. Human Movement\nScience, 30(1):125–142, 2011.\n[178] Drew Weatherhead and Katherine S White. Read my lips: Visual speech inﬂuences word processing in infants.\nCognition, 160:103–109, 2017.\n[179] Tillman Weyde and Radha Manisha Kopparti. Modelling identity rules with neural networks. Journal of Applied\nLogics, 6(4):745–769, 2019.\n[180] David Wood, Jerome S Bruner, and Gail Ross. The role of tutoring in problem solving. Journal of Child\nPsychology and Psychiatry, 17(2):89–100, 1976.\n[181] Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understand sensory cortex.\nNature Neuroscience, 19(3):356, 2016.\n[182] Hanako Yoshida and Linda B Smith. What’s in view for toddlers? using a head camera to study visual experi-\nence. Infancy, 13(3):229–248, 2008.\n[183] Yue Yu, Patrick Shafto, Elizabeth Bonawitz, Scott C-H Yang, Roberta M Golinkoff, Kathleen H Corriveau,\nKathy Hirsh-Pasek, and Fei Xu. The theoretical and methodological opportunities afforded by guided play with\nyoung children. Frontiers in Psychology, 9:1152, 2018.\n[184] Lorijn Zaadnoordijk, Tarek R Besold, and Sabine Hunnius. A match does not make a sense: on the sufﬁciency\nof the comparator model for explaining the sense of agency. Neuroscience of Consciousness, 2019(1):niz006,\n2019.\n[185] Lorijn Zaadnoordijk, Marlene Meyer, Martina Zaharieva, Falma Kemalasari, Stan van Pelt, and Sabine Hunnius.\nFrom movement to action: An eeg study into the emerging sense of agency in early infancy. Developmental\nCognitive Neuroscience, 42:100760, 2020.\n[186] Anthony M Zador. A critique of pure learning and what artiﬁcial neural networks can learn from animal brains.\nNature Communications, 10(1):1–7, 2019.\n[187] Norbert Zmyj, Jana Jank, Simone Schütz-Bosbach, and Moritz M Daum. Detection of visual–tactile contin-\ngency in the ﬁrst year after birth. Cognition, 120(1):82–89, 2011.\n16\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "cs.NE"
  ],
  "published": "2020-09-17",
  "updated": "2020-09-17"
}