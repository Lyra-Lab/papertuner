{
  "id": "http://arxiv.org/abs/2009.09689v4",
  "title": "Reinforcement Learning Approaches in Social Robotics",
  "authors": [
    "Neziha Akalin",
    "Amy Loutfi"
  ],
  "abstract": "This article surveys reinforcement learning approaches in social robotics.\nReinforcement learning is a framework for decision-making problems in which an\nagent interacts through trial-and-error with its environment to discover an\noptimal behavior. Since interaction is a key component in both reinforcement\nlearning and social robotics, it can be a well-suited approach for real-world\ninteractions with physically embodied social robots. The scope of the paper is\nfocused particularly on studies that include social physical robots and\nreal-world human-robot interactions with users. We present a thorough analysis\nof reinforcement learning approaches in social robotics. In addition to a\nsurvey, we categorize existent reinforcement learning approaches based on the\nused method and the design of the reward mechanisms. Moreover, since\ncommunication capability is a prominent feature of social robots, we discuss\nand group the papers based on the communication medium used for reward\nformulation. Considering the importance of designing the reward function, we\nalso provide a categorization of the papers based on the nature of the reward.\nThis categorization includes three major themes: interactive reinforcement\nlearning, intrinsically motivated methods, and task performance-driven methods.\nThe benefits and challenges of reinforcement learning in social robotics,\nevaluation methods of the papers regarding whether or not they use subjective\nand algorithmic measures, a discussion in the view of real-world reinforcement\nlearning challenges and proposed solutions, the points that remain to be\nexplored, including the approaches that have thus far received less attention\nis also given in the paper. Thus, this paper aims to become a starting point\nfor researchers interested in using and applying reinforcement learning methods\nin this particular research field.",
  "text": "Reinforcement Learning Approaches in Social Robotics\nNEZIHA AKALIN, Örebro University, Sweden\nAMY LOUTFI, Örebro University, Sweden\nThis article surveys reinforcement learning approaches in social robotics. Reinforcement learning is a framework for decision-making\nproblems in which an agent interacts through trial-and-error with its environment to discover an optimal behavior. Since interaction\nis a key component in both reinforcement learning and social robotics, it can be a well-suited approach for real-world interactions\nwith physically embodied social robots. The scope of the paper is focused particularly on studies that include social physical robots\nand real-world human-robot interactions with users. We present a thorough analysis of reinforcement learning approaches in social\nrobotics. In addition to a survey, we categorize existent reinforcement learning approaches based on the used method and the design\nof the reward mechanisms. Moreover, since communication capability is a prominent feature of social robots, we discuss and group\nthe papers based on the communication medium used for reward formulation. Considering the importance of designing the reward\nfunction, we also provide a categorization of the papers based on the nature of the reward. This categorization includes three major\nthemes: interactive reinforcement learning, intrinsically motivated methods, and task performance-driven methods. The benefits\nand challenges of reinforcement learning in social robotics, evaluation methods of the papers regarding whether or not they use\nsubjective and algorithmic measures, a discussion in the view of real-world reinforcement learning challenges and proposed solutions,\nthe points that remain to be explored, including the approaches that have thus far received less attention is also given in the paper.\nThus, this paper aims to become a starting point for researchers interested in using and applying reinforcement learning methods in\nthis particular research field.\nNeziha Akalin and Amy Loutfi. 2021. Reinforcement Learning Approaches in Social Robotics. 1, 1 (February 2021), 37 pages.\n1\nINTRODUCTION\nWith the proliferation of social robots in society, these systems will impact users in several facets of life from providing\nassistance, performing cooperation, and taking part in collaboration tasks. In order to facilitate natural interaction,\nresearchers in social robotics have focused on robots that can adapt to diverse conditions and to different user needs.\nRecently, there has been great interest in the use of machine learning methods for adaptive social robots [1–4]. Machine\nLearning (ML) algorithms can be categorized into three sub fields: supervised learning, unsupervised learning and\nreinforcement learning. In supervised learning, correct input/output pairs are available and the goal is to find a correct\nmapping from input to output space. In unsupervised learning, output data is not available and the goal is to find patterns\nin the input data. Reinforcement Learning (RL), on the other hand, is a framework for decision-making problems in\nwhich an agent interacts through trial-and-error with its environment to discover an optimal behavior [5]. The RL agent\nreceives scarce feedback about the actions it has taken in the past. The agent then tunes its behavior over time via this\nfeedback signal, i.e., reward or penalty. The agent’s goal is therefore learning to take actions that maximize the reward.\nRL approaches are gaining increasing attention in the robotics community. As interaction is a key component in\nboth RL and social robotics, RL could provide a suitable approach for social human-robot interaction. Worth noting is\nthat humans perform sequential decision-making in daily life where sequential decision making describes problems\nAuthors’ addresses: Neziha Akalin, neziha.akalin@oru.se, Örebro University, School of Science and Technology, Örebro, SE-701 82, Sweden; Amy Loutfi,\namy.loutfi@oru.se, Örebro University, School of Science and Technology, Örebro, SE-701 82, Sweden.\n© 2021\n1\narXiv:2009.09689v4  [cs.RO]  11 Feb 2021\n2\nNeziha Akalin and Amy Loutfi\nthat require successive observations, i.e., cannot be solved with a single action [6]. Consequently, much of social\nhuman-robot interactions can be formulated as sequential decision-making tasks, i.e., RL problems. The goal of the robot\nin these types of interactions would be to learn an action-selection strategy in order to optimize some performance\nmetric, such as user satisfaction.\nBefore outlining the research related to reinforcement learning in social robots, first it is important to establish the\ndefinition of a social robot in the context of this article. A variety of definitions for a social robot have been proposed\nin the literature [7–12]. Within each of these definitions, there is a wide spectrum of characteristics. However, two\nimportant aspects become prominent in these definitions that are considered in this paper, namely, embodiment and\ninteraction/communication capability. One example can be found in Bartneck and Forlizzi [10] where they define a\nsocial robot as an “... autonomous or semi-autonomous robot that interacts and communicates with humans by following\nthe behavioral norms expected by the people with whom the robot is intended to interact.” Following this definition,\nthe authors stress that a social robot must have a physical embodiment. Based on the presented definitions in [7–12],\nwe consider social robots as embodied agents that can interact and communicate with humans. Figure 1 shows some of\nthe social robots that are used in the reviewed papers.\n(a) Pepper\n(b) Nao\n(c) Mini\n(d) Maggie\n(e) iCat\nFig. 1. Some of the social robots referenced within in the reviewed papers.\nThe pictures of (a) Pepper robot, and (b) Nao robot were taken by the authors. (c) Mini robot, the figure is adapted from [13] —\nlicensed under the Creative Commons Attribution, (d) Maggie robot, the figure is from https://robots.ros.org/maggie/, accessed 20\nMarch, 2020 — licensed under the Creative Commons Attribution, (e) iCat robot, the figure is\nfrom https://www.bartneck.de/wp-content/uploads/2009/08/iCat02.jpg, accessed on 22 March 2020— used with permission, photo\ncredit to Christoph Bartneck.\nThis article presents a survey on RL approaches in social robotics. As such, it is important to emphasize that the scope\nof this survey is focused on studies that include physically embodied robots and real-world interactions. Considering\nthe definition of [10] given above, this paper excludes studies with simulations and virtual agents where no physical\nembodiment is present. The presented review also excludes studies with industrial robots and studies that do not include\nany interaction with humans. Rather, this review exclusively focuses on papers that comprise both a social robot(s) and\nhuman input/user studies. It is worth noting that studies which use simulations for training and test on physical robot\ndeployment with user studies fall within the selection criteria. Likewise, studies that use explicit or implicit human\ninput in the learning process are also included.\nDue to the complexity of the social interactions and the real-world, most of the studies applying RL are trained\nand tested in simulation environments. However, real-world interactions are extremely important not only for social\nReinforcement Learning Approaches in Social Robotics\n3\nrobots but also for understanding the full potential of reinforcement learning. It is mentioned in [14] (p. 391), that\n“the full potential of reinforcement learning requires reinforcement learning agents to be embedded into the flow of\nreal-world experience, where they act, explore, and learn in our world, and not just in their worlds.” Generally speaking,\nthe overall goal of an RL agent is to maximize the expected cumulative reward over time, as stated in the “reward\nhypothesis” [14] (p. 42). The reward in RL is used as a basis for discovering an optimal behavior. Hence, reward design is\nextremely important to elicit desired behaviors in RL-based systems. The choice of reward function is crucial in robotics,\nwhere the problem is also referred to as the “curse of goal specification” [15]. Therefore, in this paper, we provide\na categorization based on reward design which is crucial for RL to be successful. Moreover, since communication\ncapability is a distinctive feature of social robots, we discuss communication mediums utilized for reward design\ntogether with RL algorithms.\nFinally, it is also worth noting that in the general field of robotics there is a plethora of research in RL. There also\nexist review papers on the topic of RL in robotics such as applications of RL in robotics in general [15, 16], policy search\nin robot learning [17], safe RL [18], and Deep Reinforcement Learning (DRL) in soft robotics [19]. Indeed, RL has been\napplied to a variety of scenarios and domains within social robotics, with growing popularity. While the field of social\nrobotics deserves a survey on its own, to the best of our knowledge, there exists no such survey on this particular\nresearch field. Thus, the main purpose of this work is to serve as a reference guide that provides a quick overview of\nthe literature for social robotics researchers who aim to use RL in their research. Depending on the target user group,\nthe application domain or the experimental scenario, different types of rewards, problem formulations or algorithms\ncan be more suitable. In that sense, we believe that this survey paper will be beneficial for social robotics researchers.\nOverview of the Survey\nAfter surveying research on RL and social robotics, we analyze and categorize the studies based on four different criteria:\n(1) RL type, (2) the utilized communication mediums for reward function formulation, (3) the nature of the reward\nfunction, (4) the evaluation methodologies of the algorithms. These categorizations aim to facilitate and guide the\nchoice of a suitable algorithm by social robotics researchers in their application domain. For that purpose, we elaborate\non the different methods that are tested in real-world scenarios with a physical robot.\nCategorization based on RL type includes bandit-based methods, value-based methods, policy-based methods, and\ndeep RL (see Section 4). The utilized communication mediums are verbal communication, nonverbal communication,\naffective communication, tactile communication, and additional communication medium between the robot and the\nhuman. Moreover, there are studies in which higher interaction dynamics are used for reward formulation such as\nengagement, comfort, and attention. There are also other studies that do not use any communication medium at all for\nreward formulation. In the categorization based on the design of the reward mechanisms, three major themes emerged:\n(1) Interactive reinforcement learning: In these methods, humans are involved in the learning process either by\nproviding reward or guidance to the agent (Section 5.1). This approach, in which the human delivers explicit or\nimplicit feedback to the agent, is known as Interactive Reinforcement Learning (IRL).\n(2) Intrinsically motivated methods: There are different intrinsic motivations in the literature on RL [20], however,\nthe most frequently used approaches in social robotics depend on the robot maintaining an optimal internal\nstate by considering both internal and external circumstances (Section 5.2).\n(3) Task performance driven methods: In these methods, the reward the robot receives depends on either the robot’s\ntask performance or the human interactant’s task performance, or a combination of both (Section 5.3).\n4\nNeziha Akalin and Amy Loutfi\nThe evaluation methodologies include (1) the algorithm point of view, (2) the user experience point of view, and (3)\nevaluation of both learning algorithm-related factors and user experience-related factors.\nTo formulate the social interactions as a reinforcement learning problem, researchers need to consider some key\nconcepts such as input data, state representation, robot actions, and reward function. Moreover, after the implementation\nof RL, it should be decided how the evaluation will be performed. Therefore, we extract from each of the cited works\nthe following key points (1) the input data, state space and action space (2) the reward function (3) the communication\nmedium in the HRI scenario (4) the main experimental results (5) the experimental scenario and its validation. Therefore,\nthe contributions of this paper include: (i) analysing and categorising the relevant literature in terms of type of RL used;\n(ii) analysing and categorising the relevant literature based on the reward function; (iii) analysing the relevant literature\nin terms of evaluation methodologies.\nThe paper is organized as follows: In Section 2, we discuss the benefits and challenges of applying RL in the social\nrobotics domain. In Section 3, we present a background on reinforcement learning. Following the formal presentation\nof the methods, in Section 4, we present the applications of these methods in social robotics. Later, we present the\ncategorization based on reward functions in Section 5. Evaluation methods are discussed in Section 6. In Section 7, we\ndiscuss the current approaches in the view of real-world RL challenges and proposed solutions. The section further\nincludes the points that remain to be explored, and the approaches that have thus far received less attention. Finally, in\nSection 8, we conclude the paper.\n2\nRL IN SOCIAL ROBOTICS—BENEFITS AND CHALLENGES\nApplications of social robots are numerous and range from entertainment to eldercare. The robot tasks in such cases\ninvolve interactive elements such as human-robot cooperation, collaboration, and assistance. To achieve longitudinal\ninteraction with social robots, it is important for such robots to learn incrementally from interactions, often with non-\nexpert end-users. In consideration of continuously evolving interactions where user needs and preferences change over\ntime, hand-coded rules are labor-intensive. Even though rule-based systems are deterministic, it can be difficult to create\nrules for complex interaction patterns. Machine learning is bound to play an important role in a wide range of domains\nand applications including robotics. However, the social robot learning problem differs from the traditional ML setting in\nwhich there is a need for collected datasets or assumptions about the distribution of input data [21]. Often, social robots\nshould be able to learn new tasks and task refinements in domestic (unstructured) environments. Furthermore, social\nrobotics researchers need to deal with a particular challenge of learning in real-time from human-robot interactions.\nML paradigms such as supervised learning and unsupervised learning are not designed for learning from real-time\nsocial interactions. On the contrary, RL represents an active process. Unlike other ML methods, it does not need to be\nprovided desired outputs instead, it trains interactively based on reward signals and refines its behavior throughout the\ninteraction. Moreover, interaction is a key component for social robots which makes RL a suitable approach. RL also\nprovides a possibility to learn from natural interaction patterns by utilizing the various social elements in the learning\nprocess. Consideration of all these points suggests that socially guided machine learning [22] could be a more suitable\napproach than traditional ML approaches for social HRI.\nIn general, combining human and machine intelligence may be effective for solving computationally hard prob-\nlems [23]. The term “socially guided machine learning” was first used by Thomaz et al. [22] and refers to approaches\nthat include social interaction between a user and a machine in the learning process. Studies using IRL in social robotics\ncan be considered as socially guided machine learning since they make use of human feedback in different forms in\nthe learning process. The feedback provided by the human can be used for shaping the action policy (the human is\nReinforcement Learning Approaches in Social Robotics\n5\ninvolved in the action selection mechanism), or shaping the reward function [24]. It can be treated either as reward,\nin that the feedback is given based on the agent’s past actions indicating “how good the taken action was”, or policy\nfeedback in which human feedback affects action selection or modification thereby indicating “what to do”.\nThe majority of studies included in this review paper use IRL which may suggest that IRL could be the best suited\napproach in social robotics. However, IRL has its own challenges. Human teachers tend to give less frequent feedback\n(due to boredom and/or fatigue) as learning progresses, resulting in diminished cumulative reward [25]. Likewise,\nhuman teachers tend to provide more positive reward than punishment [26, 27]. Yet another problem in IRL is the\ntransparency issues that might arise during the training of a physical robot via human reward [28, 29]. Reference [29]\nused an audible alarm to alert the trainer about the robot’s loss of sense. Suay et al. [30] observed that experts could\nteach the defined task in a predefined time frame, whereas the same amount of time was not enough for inexperienced\nusers. One solution suggested for this was algorithmic transparency during training, which shows the internal policy to\nthe human teacher. However, the presentation of the model of the agent’s internal policy might be obscure for naive\nhuman teachers. Therefore, this information should be presented in a straight-forward way that is easy to understand\nto avoid causing confusion. To exemplify, in [28] human trainers waited for the Leonardo robot to establish eye contact\nwith them before they continued teaching. The eye contact was considered as the robot being ready for the next action.\nThese kinds of transparent behaviors in which the robot communicates the internal state of the learning process should\nbe taken into account for guiding human trainers in IRL. As noted in several studies, in IRL, the human teacher’s\npositive and negative reward can be much more deliberate than a simple ‘good’ or ‘bad’ feedback [28, 31]. The learning\nagent should be aware of the subtle meanings of these feedback signals. As an example, human trainers tend to have a\npositive bias [28, 31].\nIn addition, there are a variety of technical challenges to address when implementing RL in social robotics and social\nHRI. One of the drawbacks of online learning through interaction with a human is the requirement of long interaction\ntime, which can be tedious and impractical for the users, resulting in fatigue and a loss of interest. A considerable amount\nof interaction time can wear out the robot’s hardware. An alternative is using a simulated world to train the algorithm\nand subsequently deploying it on the real robot. Using a simulated setting has several advantages. It allows the agent to\ncarry out learning repeatedly, which would otherwise be very expensive in the real-world. Simulated environments can\nalso run much faster than the real-world, thus permitting the learning agent to make proportionately more learning\nexperiences. Bridging the gap between the simulated and the real-world is not a simple task. It may be achieved by\nrandomizing the simulator and learning a policy that shows success across many simulators and can ultimately be\nrobust enough to work in the real world. However, simulating the real-world can be very difficult, especially with\nregards to modeling relevant human behaviors. Simulating the human requires a predictive model of human interactive\nbehaviors and social norms as well as modeling the uncertainty of the real-world. Furthermore, the use of RL in social\nrobotics poses other challenges such as devising proper reward functions and policies, as well as dealing with the\nsparseness of the reward signals.\nThe exploration-exploitation dilemma is a well-known problem in RL and refers to the choice of actions to discover\nthe environment or taking actions that have already proven to be effective in producing reward [14]. RL practitioners\nuse different approaches to deal with the trade-off between exploration and exploitation, such as epsilon-greedy\npolicy [32], epsilon-decreasing policy [33] and Boltzmann distribution [34]. The epsilon-greedy strategy exploits\nknowledge for maximizing rewards (greedily choosing the current best option), otherwise to select a random action\nwith probability 𝜀∈[0, 1] [14]. The epsilon-decreasing strategy decreases 𝜀over time, thereby progressing towards\nexploitative behavior [14]. Boltzmann exploration uses Boltzmann distribution to select the action to execute. A\n6\nNeziha Akalin and Amy Loutfi\ntemperature parameter balances between exploration and exploitation (high-temperature value for selecting actions\nrandomly and low-temperature value for selecting actions greedily) [14].\nDespite the mentioned challenges, there are also advantages of using RL in social robotics. One of the main advantages\nis that the robot can learn a personalized adaptation for different interactants, i.e., a different policy for each user.\nSocial robots can learn social skills from their own actions without demonstrations through uncontrolled interaction\nexperiences. This is especially true given that interaction dynamics are difficult to model and sometimes even humans\ncannot explain why they behave in a certain way. Therefore, RL may enable social robots to adapt their behaviors\naccording to their human partners for natural human-robot interaction. In IRL, the immediate reward provided by the\nhuman teacher has the potential to improve the training by reducing the number of required interactions. Human\nteachers’ guidance significantly reduces the number of states explored, and the impact of teacher guidance is proportional\nto the size of the state space, i.e., it increases as the size of the state space grows [26]. In RL, how to achieve a goal is not\nspecified, instead the goal is encoded and the agent can devise its own strategy for achieving that goal. Intrinsically\nmotivated reward signals might be useful in many real-world scenarios, where sparse rewards make the goal-directed\nbehavior challenging. Approaches using human social signals have the advantage of utilizing signals that the user\nexhibits naturally during the interaction. It does not require an extra effort to collect the reward. However, the change\nin social signals would not be so sudden, which would very much affect the time for convergence. The role of human\nsocial factors deserves extra attention in online learning methods. Combination of RL with deep neural networks has\nshown success in many application areas. DRL is also a trending technique in social robotics as we see increasing\nwork in recent years. It has the advantage of not needing manual feature engineering [35] and resulting in human-like\nbehavior for social robots [36].\n3\nREINFORCEMENT LEARNING\nReinforcement learning [5] is a framework for decision-making problems. Markov Decision Processes (MDPs) are\nmathematical models for describing the interaction between an agent and its environment. Formally, an MDP is denoted\nas a tuple of five elements ⟨S, A, P, R,𝛾⟩where S represents the state space (i.e., the set of possible states), A represents\nthe action space (i.e., the set of possible actions), P : S×A×S →[0, 1] represents the probability of transitioning from\none state to another state given a particular action, R : S×A×S →R represents the reward function, and 𝛾is the\ndiscount factor that determines the importance of future rewards, 𝛾∈[0, 1]. The agent interacts with its environment\nin discrete time steps, 𝑡= 0, 1, 2, ..; at each time step 𝑡, the agent gets a representation of the environmental state 𝑆𝑡∈S,\ntakes an action 𝐴𝑡∈A, moves to next state 𝑆𝑡+1, and receives a scalar reward 𝑅𝑡+1 ∈R. Figure 2 depicts the standard\nRL framework.\nEnvironment\nSt+1\nstate \nSt\nAgent\naction \nAt\nRt+1\nreward \nRt\nFig. 2. Standard reinforcement learning framework (reproduced from [14, p. 38])\nReinforcement Learning Approaches in Social Robotics\n7\nThe agent’s behavior that maps states to actions is described as a policy, 𝜋: S × A where 𝜋(𝑠|𝑎) = 𝑃𝑟(𝐴𝑡= 𝑎|𝑆𝑡= 𝑠)\nis the probability of taking action 𝑎∈A given state 𝑠. The agent’s goal is to maximize the expected cumulative\ndiscounted reward, in other words return which is denoted as 𝐺𝑡:\n𝐺𝑡=\n∞\n∑︁\n𝑘=0\n𝛾𝑘𝑅𝑡+𝑘+1\n(1)\nwhere 𝛾is the discount factor and usually 𝛾∈[0, 1]. The optimal behavior that is taking the best action at each state to\nmaximize the reward over time is called optimal policy, 𝜋∗.\nThere exists a large variety of approaches in RL. They can be most broadly distinguished as model-based and\nmodel-free. Model-free approaches can be further subdivided into value-based and policy-based approaches. Below, we\nbriefly explain model-based and model-free RL in Section 4.2, value-based methods in Section 3.2 and policy-based\nmethods in Section 3.3. A shortened version of a RL taxonomy can be seen in Figure 3.\nRL Algorithms\nModel-Free RL\nModel-Based RL\nValue-based\nPolicy-based\nLearn the Model\nModel Given\nOn-policy\nOff-policy\nSARSA\nDQN\nQ-Learning\nGradient-Free\nGradient-Based\nFig. 3. Taxonomy of Reinforcement Learning algorithms (reproduced and shortened from [37]).\n3.1\nModel-Based and Model-Free Reinforcement Learning\nRL algorithms can be divided into two main categories, model-free RL and model-based RL, depending on whether the\nagent does or does not use a model of the environment dynamics, which can be either provided or learned. The model\ndescribes the transition function, P, and the reward function, R. The model-based methods can be divided into two\ncategories: those that use a given model, i.e., the models of the transition and the reward function can be accessed by\nthe agent, and the methods in which the agent learns the model of the environment [37]. In the latter approach, the\nagent learns a model, which it subsequently uses during policy improvement. The agent can collect samples from the\nenvironment by taking actions. From those samples state transitions and reward can be predicted through supervised\nlearning. Planning methods can be used directly on the environment model.\nIn the “model-free” approach, there is no effort to build a model of the environment, instead the agent searches\nfor the optimal policy through trial and error interactions with the environment. Model-free methods are easier to\nimplement in comparison with model-based methods. These methods can be advantageous over more complex methods\nwhen building a sufficiently accurate model is difficult [14, p. 10].\n8\nNeziha Akalin and Amy Loutfi\n3.2\nValue-Based Methods\nThe value of policy 𝜋, namely the value function, is used to evaluate the states based on the total reward the agent\nreceives over time. RL methods that approximate the value function through temporal difference (TD) learning instead\nof directly learning the policy 𝜋are called value-based methods. For each learned policy 𝜋, there are two related value\nfunctions: the state-value function, 𝑣𝜋(𝑠), and state-action value function (quality function), 𝑞𝜋(𝑠,𝑎). The equations for\n𝑞𝜋(𝑠,𝑎) and 𝑣𝜋(𝑠) are given in Eq. (2) and Eq. (3) respectively. 𝐸𝜋in Eq. (2) and Eq. (3) means the agent follows policy 𝜋\nin each step.\n𝑞𝜋(𝑠,𝑎) = 𝐸𝜋[𝑅𝑡+1 + 𝛾𝑅𝑡+2 + 𝛾2 𝑅𝑡+3 + ...|𝑆𝑡= 𝑠,𝐴𝑡= 𝑎] = 𝐸𝜋\n\" ∞\n∑︁\n𝑘=0\n𝛾𝑘𝑅𝑡+𝑘+1\n\f\f\f\f\f𝑆𝑡= 𝑠,𝐴𝑡= 𝑎\n#\n(2)\n𝑣𝜋(𝑠) = 𝐸𝜋[𝑅𝑡+1 + 𝛾𝑅𝑡+2 + 𝛾2 𝑅𝑡+3 + ...|𝑆𝑡= 𝑠] = 𝐸𝜋\n\" ∞\n∑︁\n𝑘=0\n𝛾𝑘𝑅𝑡+𝑘+1\n\f\f\f\f\f𝑆𝑡= 𝑠\n#\n.\n(3)\nThe value functions are expressed via the Bellman equation [38]. The Bellman equation for 𝑣𝜋and 𝑞𝜋is given in Eq. (4)\nand Eq. (5) where 𝑠′ indicates the next states from the set S.\n𝑣𝜋(𝑠) =\n∑︁\n𝑎\n𝜋(𝑎|𝑠)\n∑︁\n𝑠′,𝑟\n𝑝(𝑠′,𝑟|𝑠,𝑎)[𝑟+ 𝛾𝑣𝜋(𝑠′)]\n(4)\n𝑞𝜋(𝑠,𝑎) =\n∑︁\n𝑠′\n𝑝(𝑠′|𝑠,𝑎)\n\"\n𝑟(𝑠,𝑎,𝑠′) + 𝛾\n∑︁\n𝑎′\n𝜋(𝑎′|𝑠′)𝑞𝜋(𝑠′,𝑎′)\n#\n.\n(5)\nComparing policies, a policy 𝜋is better than or equal to a policy 𝜋′ if:\n𝜋≥𝜋′ if ∀𝑠∈S : 𝑣𝜋(𝑠) ≥𝑣𝜋′(𝑠).\n(6)\nThere exists always at least one optimal policy 𝜋∗whose expected return is greater than or equal to the other\npolicy/policies for all states. Optimal policies share the same state-value function, defined as 𝑣∗(𝑠) = 𝑚𝑎𝑥\n𝜋\n𝑣𝜋(𝑠) for all\n𝑠∈S, and action-value function, defined as 𝑞∗(𝑠,𝑎) = 𝑚𝑎𝑥\n𝜋\n𝑞𝜋(𝑠,𝑎) for all 𝑠∈S and 𝑎∈A(𝑠). The Bellman optimality\nequation for 𝑞∗(𝑠,𝑎) is given in Eq. (7).\n𝑞∗(𝑠,𝑎) =\n∑︁\n𝑠′,𝑟\n𝑝(𝑠′,𝑟|𝑠,𝑎)\nh\n𝑟+ 𝛾𝑚𝑎𝑥\n𝑎′ 𝑞∗(𝑠′,𝑎′)\ni\n.\n(7)\nAnother distinction in RL methods comes from the perspective of policy: on-policy vs. off-policy learning. On-policy\nmethods learn the value of the policy that is used to make decisions. In the on-policy setting, the target policy and the\nbehavior policy are the same. The target policy is the policy that is learned about, and the behavior policy is the policy\nthat is used to generate behavior. The state-action-reward-state-action (SARSA) algorithm [39] is one of the on-policy\nmethods in which the agent interacts with the environment, selects an action based on the current policy, then updates\nthe current policy. The Q function update in SARSA is done using Eq. (8). A transition from one state-action pair to the\nnext is expressed as (𝑆𝑡,𝐴𝑡, 𝑅𝑡+1,𝑆𝑡+1,𝐴𝑡+1) which gives rise to the name SARSA. The update given in Eq. (8) is done\nafter every transition from a non-terminal state 𝑆𝑡.\n𝑄(𝑆𝑡,𝐴𝑡) ←𝑄(𝑆𝑡,𝐴𝑡) + 𝛼\n\u0002\n𝑅𝑡+1 + 𝛾𝑄(𝑆𝑡+1,𝐴𝑡+1) −𝑄(𝑆𝑡,𝐴𝑡)\n\u0003\n.\n(8)\nIn the off-policy methods, the target policy is different from the behavior policy. In these methods, the policy that is\nevaluated and improved does not match the policy that is used to generate data. Off-policy methods can re-use the\nexperience from old policies or other agents’ interaction experience to improve the policy. One example of an off-policy\nReinforcement Learning Approaches in Social Robotics\n9\nalgorithm is Q-learning [40]. It is one of the most popular RL algorithms using discounted reward [41]. The Q-learning\nrule is defined by:\n𝑄(𝑆𝑡,𝐴𝑡) ←𝑄(𝑆𝑡,𝐴𝑡) + 𝛼\n\u0002\n𝑅𝑡+1 + 𝛾𝑚𝑎𝑥\n𝑎\n𝑄(𝑆𝑡+1,𝑎) −𝑄(𝑆𝑡,𝐴𝑡)\n\u0003\n.\n(9)\nThe Q-learning algorithm iteratively applies the Bellman optimality equation (given in Eq. (7)). As shown in Eq. (9), the\nmain difference between Q-learning and SARSA (see Eq. (8)) is that in the former the target value is not dependent on\nthe policy being used and only depends on the state-action function. The Q-learning algorithm is given in Algorithm ??\nwhere an episode (or trial) describes a sequence from the initial state to the terminal state, and 𝛼specifies a learning\nrate (controls the amount of new information). Q-learning, along with its different variations, is the most commonly\nused RL method in social robotics.\n3.3\nPolicy-Based Methods\nPolicy-based methods, also known as direct policy search methods, do not use value function models. In these methods,\nthe policy is parameterized with 𝜃and written as 𝜋𝜃. They operate in the space of policy parameters Θ and 𝜃∈Θ [17].\nThe goal is still to maximize the accumulative return. The agent updates its policy by exploring various behaviors and\nexploiting the ones that perform well in regard to some predefined utility function 𝐽(𝜃). In many robot control tasks the\nstate space, which includes both internal states and external states, is high-dimensional. The policy of the robot 𝜋𝜃can\nbe defined as a controller. For any state of the robot, this controller decides which actions to take or which signals to\nsend to the actuators [42]. The robot takes its actions 𝑢according to the controller (please note, actions in policy search\ncontext are represented with 𝑢instead of 𝑎). The robot controller can be stochastic, i.e., 𝜋(𝑢|𝑠) or deterministic, i.e.,\n𝜋(𝑠). After the action execution the robot transitions to another state according to the probabilistic transition function\n𝑝(𝑠𝑡+1|𝑠𝑡,𝑢𝑡). These states and actions of the robot form a trajectory 𝜏= (𝑠0,𝑢0,𝑠1,𝑢1, ...). The corresponding return for\nthe trajectory 𝜏is represented as 𝑅(𝜏). The global utility of the robot is denoted as:\n𝐽(𝜃) = E𝜏∼𝜋𝜃[𝑅(𝜏)].\n(10)\nComputing the expectation in E𝜏∼𝜋𝜃[𝑅(𝜏)] requires to run an infinite number of trajectories with the current controller.\nThe way to go around this difficulty is to sample the expectation. After performing a finite set of trajectories, the return\nis computed over these trajectories. Thus, the goal is:\n𝜃∗= argmax\n𝜃\n𝐽(𝜃) = argmax\n𝜃\n∑︁\n𝜏\n𝑃(𝜏,𝜃)𝑅(𝜏)\n(11)\nwhere 𝜃∗is the estimate of global performance and 𝑃(𝜏,𝜃) is the probability of 𝜏under policy 𝜋𝜃.\nHere RL addresses a black-box optimization problem in that the function which relates the performance to the policy\nparameters is unknown. There are two families of methods: direct policy search and gradient descent [42]. In direct\npolicy search algorithms, approximate gradient descent is performed by “random trial then selection” methods, like\ngenetic algorithms, evolution strategies, finite differences, cross entropy, etc. These algorithms need many samples\nand can escape from local minima if large enough variations are used. In gradient descent methods, a mathematical\ntransformation is used so that policy gradient methods can be applied. In these methods, the policy gradient update is\ngiven by:\n𝜃𝑘+1 = 𝜃𝑘+ 𝛼∇𝜃𝐽(𝜃)\n(12)\n10\nNeziha Akalin and Amy Loutfi\nwhere 𝛼is a learning rate, and the policy gradient is given by [17]:\n∇𝜃𝐽(𝜃) =\n∑︁\n𝜏\n∇𝜃𝑃(𝜏,𝜃)𝑅(𝜏).\n(13)\nThere are different methods to estimate the gradient ∇𝜃𝐽(𝜃), interested readers may refer to [17]. Policy-based methods\nhave the advantage of being effective in high dimensional or continuous action spaces and having better convergence\nproperties.\nSome methods learn both policy and value functions. These methods are called actor-critic methods, where ‘actor’ is\nthe learned policy that is trained using policy gradient with estimations from the critic, and ‘critic’ refers to the learned\nvalue function that evaluates the policy.\n3.4\nDeep Reinforcement Learning\nLearning in RL progresses over discrete time steps by the agent interacting with the environment. Obtaining an\noptimal policy requires a considerable amount of interaction with the environment, which results in high memory and\ncomputational complexity. Therefore, the tabular approaches that represent state-value functions, 𝑣𝜋(𝑠), or state-action\nvalue functions, 𝑞𝜋(𝑠,𝑎), as explicit tables are limited to low-dimensional problems, and they become unsuitable for\nlarge state spaces. A common way to overcome this limitation is to find a generalization for estimating state values by\nusing a set of features in each state. In other words, the idea is to use a parameterized functional form with weight\nvector 𝑤∈R𝑑for representing 𝑣𝜋(𝑠) or 𝑞𝜋(𝑠,𝑎) that are written as ˆ𝑣(𝑠;𝜃) or ˆ𝑞(𝑠,𝑎;𝜃) instead of tables [14, p. 161].\nSuch approximate solution methods are called function approximators. The reduction of the state space by using the\ngeneralization capabilities of neural networks, especially deep neural networks, is becoming increasingly popular. Deep\nLearning (DL) has the ability to perform automatic feature extraction from raw data. Deep Reinforcement Learning\n(DRL) introduces DL to approximate the optimal policy and/or optimal value functions [14, p. 192]. Recently, there has\nbeen an increasing interest in using DL for scaling RL problems with high-dimensional state spaces.\nThe DQN method, first presented by Mnih et al., combines Q-learning with convolutional neural networks for learning\nto play a wide variety of Atari games better than humans [43]. In DQN, the agent’s experiences 𝑒𝑡= (𝑠𝑡,𝑎𝑡,𝑟𝑡,𝑠𝑡+1)\nare stored at each time step 𝑡in a data set 𝐷𝑡= {𝑒1, ...,𝑒𝑡}, so-called experience replay memory. Q-learning updates are\napplied on a mini-batch uniformly sampled from the experience replay memory. The Q-learning update is done using\nEq.(14):\n𝐿𝑖(𝜃𝑖) = E𝑠,𝑎,𝑟,𝑠′∼𝑈(𝐷)\nh\u0010\n𝑟+ 𝛾𝑚𝑎𝑥\n𝑎′ 𝑄(𝑠′,𝑎′; ˆ𝜃𝑖) −𝑄(𝑠,𝑎;𝜃𝑖)\n\u00112i\n(14)\nwhere 𝜃𝑖represents the parameters (weights) of the Q-network at iteration 𝑖and ˆ𝜃𝑖represents the parameters used to\ncompute the target network at iteration 𝑖. The target network parameters ˆ𝜃𝑖are updated to the parameters 𝜃𝑖after\nevery 𝐶iterations.\n4\nCATEGORIZATION OF RL APPROACHES IN SOCIAL ROBOTICS BASED ON RL TYPE\nIn human-human communication, a communication medium is a means of conveying information to other people.\nIt can be in different forms such as verbal, nonverbal, affective, and tactile. Human-robot interaction overlaps with\nhuman-human interaction to a certain extent. Furthermore, there can be an additional physical interface (i.e., a computer,\na tablet, a smart game board, etc.) shared between the robot and the human. In the interaction between the robot and\nthe human, information transmission is bidirectional, the robot and the human can be sender, receiver, or both. In the\nsurveyed papers, we see all these communication channels being utilized, especially for the RL problem formulation.\nReinforcement Learning Approaches in Social Robotics\n11\nAs it has already been stated in the introduction, one of the prominent characteristics of social robots is the ability to\ninteract and communicate. Therefore, we provide two categorizations in this section: first we categorize the papers based\non RL types, after which we provide a further discussion and categorization with respect to the utilized communication\nchannels and interaction dynamics for the reward functions.\n4.1\nBandit-Based Methods\nBandit-based methods can be considered as a simplified case of RL in which the next state does not depend on the\naction taken by the agent. Different bandit-based methods explored in social robotics [4, 44–47], such as dueling\nbandit learning [44], k-armed bandit method (multi-armed bandit) [4, 45, 46], and Exponential-Weight Algorithm for\nExploration and Exploitation (Exp3) algorithm [47].\n4.1.1\nAdditional Physical Communication Medium between the Robot and the Human. Learning user preferences to\npersonalize the user experience is used in customizing advertisements and search results. A similar approach was applied\nin HRI studies [4, 44]. Whereas the customization is done in the background for personalized experiences in websites\nusing users’ clicks, it is adapted for social interactions by asking the user to select their preferences using the buttons.\nIn other words, these studies use a physical communication medium between the robot and the human. Schneider\nand Kummert [44] investigated a dueling bandit learning approach for preference learning. The algorithm draws two\nor more actions, and the relative preference is used as reward. It is defined as follows: In each time step 𝑡> 0 a pair\nof arms (𝑘(1)\n𝑡\n,𝑘(2)\n𝑡\n) is selected and presented to the user, if the user prefers 𝑘(1)\n𝑡\nover 𝑘(2)\n𝑡\nthen 𝑤𝑡= 1, and 𝑤𝑡= 2\notherwise where 𝑤𝑡is a noisy comparison result. The distribution of outcomes is represented by a preference matrix\n𝑃= [𝑝𝑖𝑗]𝐾𝑥𝐾, here 𝑝𝑖𝑗is the probability that the user preferred arm 𝑖over arm 𝑗. The participant provided pairwise\ncomparisons via a button. In the work by Ritschel et al. [4], the robot adapted its linguistic style to the user’s preferences.\nThey defined the learning tasks as k-armed bandit problems. The adaptation was done based on explicit human feedback\ngiven via buttons in the form of numeric reward (−1, +1). The actions of the robot were a set of scripted utterances.\nSimilarly, Ritschel et al. [46] used an additional medium between the robot and the user. They employed the social\nrobot Reeti as a nutrition adviser, where a custom hardware was utilized to obtain the information about the selected\ndrink [46]. Their custom hardware included an electronic vessel holder and a smart scale that could communicate with\nthe robot. The problem was formalized as an k-armed bandit problem where the actions of the robot were scripted\nspoken advice. The reward was calculated from the amount of calories and quantity of the selected drink.\n4.1.2\nVerbal and Nonverbal Communication Plus an Interface. Social robots can use any natural communication channel,\nand benefit from different user interfaces. The studies [45–47] take advantage of a physical medium shared across the\nrobot and the human to simplify the state space representations. Leite et al. [45] used a multi-armed bandit for empathetic\nsupportive strategies in the context of a chess companion robot for children. The difference in the probabilities of\nthe user being in a positive mood before and after employing supportive strategies was used as a reward. The child’s\naffective state was calculated by using visual facial features (smile and gaze) and contextual features of the game (game\nevolution i.e winning/losing, chessboard configuration). Similarly, in the work by Gao et al. [47] the user’s task-related\nparameters were monitored through the puzzle interface. The robot’s behaviors were adapted by combining a decision\ntree model with the Exp3 [48]. The Exp3 algorithm maintains a list of weights for each of the actions, which are used for\nselecting the next action. The reward was the user’s task performance in combination with the user’s verbal feedback.\nThe set of robot actions included four supportive behaviors to help the user to solve the puzzle game.\n12\nNeziha Akalin and Amy Loutfi\n4.2\nModel-Based and Model-Free Reinforcement Learning\nVerbal Communication. Considering the challenge of modeling real-world human-robot interactions, the majority of\npapers included in this survey use model-free RL. Nevertheless, several recent works started to investigate model-based\nRL for HRI [49, 50]. One of the challenges of real-world robot learning is the delayed reward. There is an assumption\nthat the result of an agent’s observations of its environment is available instantly. However, there can be a lag in human\nreaction to robot actions in HRI. When the reward of the robot depends on human responses, reward shaping can\nbe useful for the robot to get more frequent feedback. Reward shaping is a technique that consists of augmenting\nthe natural reward signal so that additional rewards are provided to make the learning process easier [51]. Studies\nin [49, 50] presented methods including model-based RL and reward shaping for HRI. Tseng et al. [49] proposed a\nmodel-based RL strategy for a service robot learning the varying user needs and preferences, and adjusting its behaviors.\nThe proposed reward model was used to shape the reward through human feedback by calculating temporal correlations\nof robot actions and human feedback. Concretely, they modeled human response time using a gamma distribution. This\nformulation was found to be effective (more cumulative reward collected) in dealing with delayed human feedback.\nThe work by Martins et al. [50] presented a user-adaptive decision-making technique based on a simplified version of\nmodel-based RL and POMDP formulation. Three different reward functions were formulated, and compared in the\nexperiments. Their entropy-based reward shaping mechanism devised using an information-based term. The purpose\nof using the information term was to increase the reward given for an action leading to unknown transitions, thereby\nencouraging the robot to investigate the impact of new actions on the user.\n4.3\nValue-Based Methods\nIn recent years, there has been an increasing interest in applying RL methods to social robotics with growing trend\ntowards value-based methods. Q-learning, along with its different variations, is the most commonly used RL method\nin social robotics. The studies using Q-learning are [3, 13, 34, 52–61]. These comprise studies using standard Q-\nlearning [3, 54, 55, 58, 60, 62], studies modify Q-learning for dealing with delayed reward [52], tuning the parameters\nfor Q-learning such as 𝛼[13, 34, 52], dealing with decreasing human feedback over time [52], comparing their proposed\nalgorithm with Q-learning [33, 49, 61, 63, 64], variation of Q-learning called Object Q-learning [64–66], combining\nQ-learning with fuzzy inference [67], SARSA [68, 69], TD(𝜆) [70], MAXQ [33, 71, 72], R-learning [32], and Deep\nQ-learning [35, 36, 73, 74].\n4.3.1\nTactile Communication. When the user is involved in the learning process by providing feedback in the form of\nreward or guidance, the general approach is either using an additional interface or utilizing the sensory information\nsuch as internal (robot’s onboard sensors) or external cameras and microphones. Nowadays, many social robots are\nequipped with tactile capabilities. However, the usage of the robots’ touch sensors as a feedback mechanism has\nreceived relatively little attention in the context of RL in social robotics. Yet [52, 53] benefited from the robot’s tactile\nsensors instead of an additional interface between the user and the robot. Barraquand and Crowley [52] conducted\nfive experiments with different modifications of the classical Q-learning algorithm. The human teacher provided\nfeedback through tactile sensors of the Sony AIBO robot, caressing the robot for the positive feedback and tapping\nthe robot for the negative feedback. The action space comprised two actions; bark and play. The first experiment was\nstandard Q-learning with human reward. Since the human ceased giving feedback over time, they concluded that\nthe learning rate 𝛼should be adapted. In the second experiment, they used the asynchronous Q-learning algorithm.\nIn asynchronous Q-learning, the learning rate 𝛼may be different for different state-action pairs. The learning rate\nReinforcement Learning Approaches in Social Robotics\n13\nis decreased when the system encountered the same situations and actions. In relation to standard Q-learning this\nmodification increased the effectiveness of the algorithm, i.e., it learned faster and forgot more slowly. Because the\nlearning rate was much smaller when there was no feedback. To overcome the delayed reward, they considered to\nincrease the effect of human-delivered positive reward in larger time frames and to decrease the effect of negative\nreward in a shorter time frame. The use of an eligibility trace with a heuristic for delayed reward was found to be more\nefficient than classical Q-learning (generalizing experience to cover similar situations). The authors noted that learning\nrate, reward propagation, and analogy (i.e., propagating information to similar states) can improve the effectiveness\nof learning from social interaction. Yang et al. [53] proposed a Q-learning based approach that combines homeostasis\nand IRL. The internal factors, i.e., the drives and motivations worked as a triggering mechanism to initiate the robot’s\nservices. However, the reward in the real-world experiments was given by the user touching the robot’s head, left hand,\nand right hand to give positive, negative, and dispensable feedback, respectively [53]. The authors trained their model\nin a simulator and deployed it on the Pepper robot.\n4.3.2\nAdditional Physical Communication Medium between the Robot and the Human. Since we identify social robots\nwith interaction, the robot learning within a social scenario stands out in the surveyed papers. Alternatively, there are\nstudies where social interaction is not the main concern however, the main purpose is training a social robot to do a\ntask. As an example, a human teacher trains the agent through a GUI [26, 30], speech and gestures [28, 31]. In Suay and\nChernova [26], human teacher trained a social robot. They performed experiments similar to those presented in [75] in\na real-world scenario with the Nao robot [26]. The human trainer observed the robot in its environment via a webcam\nand provided reward based on the robot’s past actions or anticipatory guidance for selecting future actions through a\nGUI. They conducted four sets of experiments (small state space and only reward, large state space and only reward,\nsmall state space and reward plus guidance, large state space and reward plus guidance) to investigate the effect of\nteacher guidance and state space size on learning performance in IRL. The task was object sorting and the size of state\nspace depended on the object descriptor features. Their results showed that the guidance accelerated the learning by\nsignificantly decreasing the learning time and the number of states explored. They observed that human guidance\nhelped the robot to reduce the action space and its positive effect was more visible in large state-space. In a similar\nvein, Suay et al. [30] conducted a user study in which 31 participants taught a Nao robot to catch the robotics toys by\nusing one of three algorithms: Behavior Networks, IRL, and Confidence-Based Autonomy. The study compared the\nresults of these algorithms in terms of algorithm usability and teaching performance by non-expert users. In IRL, the\nparticipants provided positive or negative feedback in the form of reward through an on-screen interface. In terms of\nteaching performance, users achieved better performance using Confidence-Based Autonomy, however, IRL was better\nof modelling user behavior. It has been noted in much of the literature that teaching with IRL requires more time than\nwith other methods because users had the tendency to stop rewarding or to vary their reward strategy. This affected\nthe training time, which is a drawback to this approach.\n4.3.3\nVerbal and Nonverbal Communication. We discuss different human feedback types in IRL in Section 5.1. When a\nhuman teacher trains an agent, the positive or negative feedback might convey several meanings, even lack of feedback\ncan give information to the agent depending on the teacher’s training strategy [76]. For example, Thomaz and Breazeal\n[31] realized that human trainers might have multiple intentions with the negative reward they are giving, such as the\nlast taken action was bad and future actions should correct the current state. They performed experiments with two\ndifferent platforms: the Leonardo robot learned pressing buttons and a virtual agent learned baking a cake (Sophie’s\nkitchen). The virtual agent responded to the negative reward by taking an UNDO action, i.e., the opposite action. In the\n14\nNeziha Akalin and Amy Loutfi\nexamples with the Leonardo robot, the human teacher provided verbal feedback. After negative feedback, the robot\nexpected the human teacher to guide it through refining the example by using speech and gestures (collaborative dialog).\nAlthough the interactive Q-learning with the addition of UNDO behavior was tested only on the virtual agent, it is worth\nmentioning that the proposed algorithm was more efficient compared to standard IRL. It had several advantages such\nas robust exploration strategy, fewer states visited, fewer failures occurred and fewer action trials done for learning the\ntask. Continuing along these lines, Thomaz and Breazeal [28] explored how self-exploration and human social guidance\ncan be coupled for leveraging intrinsically motivated active learning. They called the presented approach socially\nguided exploration, in which the robot could learn by intrinsic motivations, however, it could also take advantage of a\nhuman teacher’s guidance when available. The robot learner with human guidance generalized better to new starting\nstates and reached the desired goal states faster than the self-exploration.\n4.3.4\nHigher Level Interaction Dynamics: Engagement. Social robots are expected to exhibit flexible and fluent face-to-\nface social conversation. The natural conversational abilities of social robots should not be only limited to short basic\ntask related sentences. However, they should be able to engage users in the interactions with chat and entertainment,\nvarying from storytelling to jokes together with human-like vocalizations and sounds. As an example, Papaioannou\net al. [60] reported that users spent more time with the robot which can carry out small chat together with task-based\ndialogue compared to the robot that conversed only task-based dialogue. In their system, the agent was trained using\nthe standard Q-learning algorithm with simulated users and tested with the Pepper robot where the robot assisted\nvisitors of a shopping mall by providing information about and directions to the shops, current discounts in the shops,\namong other things. In the problem definition, states were represented with 12 features such as user engaged, task\ncompleted, distance, turn taking, etc. The action space consisted of 8 actions, 𝐴= [PerformTask, Greet, Goodbye, Chat,\nGiveDirections, Wait, RequestTask, RequestShop]. The reward was encoded as predefined numerical values based on task\ncompletion by the agent, including the engagement of the user. Another study considering user engagement is Keizer\net al. [1], who applied a range of ML techniques in the presented system that included a modified iCat robot (with\nadditional manipulator arms with grippers) and multimodal input sensors for tracking facial expressions, gaze behavior,\nbody language and location of the users in the environment. The reward function was a weighted sum of task-related\nparameters. For each individual user 𝑖the reward function 𝑅𝑖was defined as 𝑅𝑖= 350 ×𝑇𝐶𝑖−2 ×𝑊𝑖−𝑇𝑂𝑖−𝑆𝑃𝑖. 𝑇𝐶𝑖is\nshort for Task Complete, and is a binary variable. 𝑊𝑖(Waiting) is a binary variable showing whether the user 𝑖is ready\nto order but not engaged with the system. 𝑇𝑂𝑖stands for Task Ongoing and is a binary variable describing whether the\nuser is interacting with the robot but has not been served. 𝑆𝑃𝑖is short for Social Penalties and corresponds to several\nsocial penalties (e.g., while the user 𝑖is still talking to the system, it turns its attention to another user). An experimental\nevaluation compared a hand-coded and trained system. The authors reported that the trained system performed better\nand it was found to be faster at detecting user engagement than the hand-coded one, while the latter was more stable.\nIn [55, 57, 59], the authors investigated the entertainment capabilities of social robots using RL. Ritschel et al. [57]\npresented a social-cues-driven Q-learning approach for adapting the Reeti robot to keep the user engaged during the\ninteraction. The engagement of the user was estimated from the user’s movement through the Kinect 2 sensor by\nusing a Dynamic Bayesian Network. They used the change in the engagement as a reward in the storytelling scenario\nto adapt the robot’s utterance based on the personality of the user. In similar fashion, the work by Weber et al. [59]\nincorporated social signals in the learning process, namely the participants’ vocal laughs and visual smiles as reward.\nIn the problem formulation, they used a two-dimensional vector containing probabilities of laughs and smiles for state\nrepresentation, and the action space consisted of sounds, grimaces and three types of jokes. They used an average\nReinforcement Learning Approaches in Social Robotics\n15\nreward based on all samples from the punchline to the end with a predefined punchline for every joke. The human social\nsignals were captured and processed by using the Social Signals Interpretation (SSI) framework [77]. Their purpose was\nto understand the user’s humor preferences in an unobtrusive manner in order to improve the engagement skills of the\nrobot. In a joke-telling scenario, the Reeti robot adapted its sense of humor (grimaces, sounds, three kinds of jokes and\ntheir combination) by using Q-learning with a linear function approximator. Likewise Addo and Ahamed [55] presented\na joke telling scenario with a torso Nao robot for entertaining a human audience. They used Q-learning in which the\nactions of the robot were pre-classified jokes, and the numerical reward corresponded to affective states of the user.\nHowever, the affective states of the participants were captured by a self-reported feedback signal. After each joke, the\nhuman participant provided a verbal feedback (i.e., reward) such as “very funny”, “funny”, “indifferent” and “not funny”.\n4.3.5\nAffective Communication: Facial Expressions. Human facial expressions are perhaps one of the richest and most\npowerful tools in social communication. Facial expressions analysis is commonly used in HRI for understanding users\nand enhancing their experience. Affective facial expressions can also facilitate robot learning in RL. Recently, it is\nbecoming more popular to use off-the-shelf applications in social robotics for different perception and recognition\nmodules. Affectiva software [78] analyzes facial expressions from videos or in real-time. The studies [58, 68, 69] used\nthis software for affective child-robot interaction. In the work by Gordon et al. [68] a tutoring system for children was\npresented. The system included an Android tablet and the Tega robot setup integrated with the Affectiva software for\nfacial emotion recognition. They used the SARSA algorithm where the reward was a weighted sum of valence and\nengagement. Both valence and engagement values were obtained from the Affectiva software. Similar to [68], Park\net al. [58] used the Tega robot as a language learning companion for young children. A personalized policy was trained\nthrough 6–8 sessions of interaction by using a tabular Q-learning algorithm. The reward function was a weighted sum\nof engagement and learning gains of the child. The engagement was obtained from the Affectiva software. The learning\ngains in the reward function was represented as numerical values ([−100, 0, +50, +100]) depending on the lexical and\nsyntactic complexity of the phrase relative to the child’s level. Gamborino and Fu [69] presented an approach for socially\nassistive robots for children to support them in emotionally difficult situations using SARSA. In the proposed method,\nthe human trainer selects the actions for the social robot RoBoHoN (small humanoid smartphone robot) through an\ninterface with the purpose to improve the mood of the child depending on her/his current affective state. The affective\nstate of the child was based on seven basic facial emotions and engagement obtained by the Affectiva software and\nstored in an input feature vector to classify the mood of the child as good or bad. The emotions were binarized as 1 or 0\ndepending on whether the value was greater or less than the average, respectively. The robot suggested a set of actions\nto the trainer. The aim was to suggest actions that would match with the trainer’s action preferences. This way the\nagent would act independently, without feedback from the trainer. Another study using facial expressions is Zarinbal\net al. [54], in which Q-learning was used for query-based scientific document summarization with a social robot. The\nproblem formulation was as follows: In each state 𝑆𝑡:< 𝑥𝑖,𝑠𝑐𝑜𝑟𝑒𝑡(𝑥𝑖) > a summary that consisted of M sentences was\ngenerated, where 𝑥𝑖is a sentence and 𝑖= 1, 2, ..., 𝑀. The scoring scheme was updated based on the human-delivered\nreward. The reward 𝑟𝑡∈{−1, 0, 1} depended on the classified facial expressions: dislike, neutral and like. In state 𝑆𝑡, the\nrobot presented the sentence 𝑥∗to the user and based on his reward 𝑟𝑡. The authors concluded that user feedback may\nimprove the query-based text summarization.\n4.3.6\nVerbal Communication. The curse of dimensionality is a phenomenon that refers to problems with high dimen-\nsional data. Representing state and action spaces as explicit tables becomes impractical for large spaces. To overcome\nthe problem of large state space, approximate solutions are used, one of them being fuzzy techniques. This approach is\n16\nNeziha Akalin and Amy Loutfi\nalso explored for HRI, e.g., Chen et al. [67] and Patompak et al. [32] used fuzzification and fuzzy inference together\nwith Q-learning. These works employed verbal communication in their user studies. Chen et al. [67] proposed a\nmulti-robot system for providing services in a drinking-at-a-bar scenario. The authors used a modified Q-learning\nalgorithm combined with fuzzy inference which was called information-driven fuzzy friend-Q (IDFFQ) learning for\nunderstanding and adapting the behaviors of the mentioned multi-robot system based on the emotion and intention of\nthe user. The reward function was defined as 𝑟= (𝑟𝑡+ 𝑟ℎ)/2. Task completion 𝑟𝑡(i.e., robots selected the drink the\nuser preferred) and the human’s satisfaction with the robots’ task performance 𝑟ℎwere predefined numerical values.\nFuzzification of emotions was done using the triangular and trapezoidal membership function in the pleasure-arousal\nplane. They compared the proposed algorithm with their previous algorithm, Fuzzy Production Rule-based Friend-Q\nlearning (FPRFQ) [79]. The authors noted that the current algorithm was superior in that it resulted in higher collected\nreward and faster response time of the robots. Patompak et al. [32] proposed a dynamic social force model for social\nHRI. The authors considered two interaction areas: a quality interaction area and a private area. The quality interaction\narea was defined as the distance from which the users can be engaged in high-quality interactions with robots. The\nproposed model was designed by a fuzzy inference system, the membership parameters were optimized by using the\nR-learning algorithm [80]. R-learning is an average reward RL approach; it does not discount future rewards [81]. They\nargued that R-learning was suitable for the scenario since they intended to take every interaction experience into\naccount equally. In the real robot experiments, positive or negative verbal rewards were provided by the participants.\nAnother study that used verbal communication for the reward is [62]. In this study, a gesture recognition system\ncategorized the body trunk patterns as towards (the person is facing the robot), neutral (the trunk is facing the robot\nbetween 3◦–15◦away), and away ( orientation of the trunk is more than 15◦). The recognized gestures were interpreted\nas a person’s accessibility level, which was used to determine the person’s affective state. In the Q-learning-based\ndecision-making system, the robot had drives and emotional states which were utilized for action selection. In particular,\na state is represented as 𝑠(𝑦𝐻,𝑦𝑅,𝑑) where 𝑦𝐻is the accessibility level of the human, 𝑦𝑅is emotional state of the robot\nand 𝑑is the dominant drive. State transition probabilities, Q-values for each state, and reward for each transition were\npredetermined numerical values. The satisfaction of the robot’s drives depended on the robot completing the task. In\nthe experimental scenario, the Brian robot reminded the user about daily activities (eat, use the bathroom, go for a\nwalk and take medication) and the user verbally stated ‘yes’ or ‘no’ after the robot’s action, with ‘no’ meaning that the\nrobot’s drive is not satisfied and it will continue to try to satisfy the drive. The authors mentioned that the robot could\nuse its drives in one or two iterations for the reminders except the drive related to using the bathroom. It was attributed\nto people potentially being uncomfortable with this reminder.\n4.3.7\nHigher Level Interaction Dynamics: Attention. Social robots have the potentials for information acquisition from\nboth verbal and nonverbal communication. Not only can they gesture, maintain eye contact, and share attention with\ntheir users, but they can also estimate the users’ non-verbal cues and behave accordingly. In this interaction, both\nactors can interpret verbal and nonverbal social cues to communicate effectively. For natural fluid HRI, robot non-verbal\nbehaviors together with verbal communication are thoroughly discussed in [82]. These social cues do not only convey a\nbasic message but also carry higher-level interaction dynamics such as attention, engagement, comfort, and so on. The\nfollowing works highlight these in the context of RL in social robotics. Chiang et al. [56] proposed a Q-learning based\napproach for personalizing the human-like robot ARIO’s interruption strategies based on the user’s attention and the\nrobot’s belief in the person’s awareness of itself. The authors called it the “robot’s theory of awareness”. They formulated\nthe problem based on the user attention, which was referred to as a Human-Aware Markov Decision Process. The\nReinforcement Learning Approaches in Social Robotics\n17\nhuman attention was estimated with a trained Hidden Markov Model (HMM) from human social cues (face direction,\nbody direction, and voice detection). The reward consisted in predefined numerical values based on the robot’s theory\nof awareness of the user. The robot had six actions (gestures: head shake and arm wave; navigation: approach the\nuser and move around; audio: make sound and call name) to draw the user’s attention while the user was reading.\nThe optimal policy converged after two hours of interaction. The robot developed personalized policies for each user\ndepending on their interruption preferences. Another study considering human attention in their problem formulation\nis Hemminghaus and Kopp [3]. They used Q-learning to adapt the robot head Furhat’s behavior in a memory game\nscenario. In the game, the robot assisted the participant by guiding their attention towards target objects in a shared\nspatial environment. In the proposed hierarchical approach, the high-level behavior was mapped to low-level behaviors,\nwhich could then be directly executed by the robot. The purpose of using Q-learning was to learn the execution of\nhigh-level behaviors through low-level behaviors. In the problem formulation, states were represented in terms of the\nuser’s gaze, user’s speech, and game state. The game state represented the number of remaining card pairs in the game.\nThe action space included actions such as speaking, gazing, etc. or a combination of those actions. The reward was\ndesigned as 𝑟= 𝑟𝑝𝑜𝑠−𝑐if success 𝑟= 𝑐.𝑟𝑛𝑒𝑔if no effect. The robot received a positive reward 𝑟𝑝𝑜𝑠if the robot’s action\nhelped the user to find the correct pair. The robot received a negative reward 𝑟𝑛𝑒𝑔if the action had no effect on helping\nthe user. 𝑐represents the cost of the chosen action in cases where the costs were determined manually. Moro et al. [61]\nis another study that considered the attention of the user. Their scenario was an assistive tea-making activity for older\npeople with dementia. The authors proposed an algorithm involving Learning from Demonstration (LfD) and Q-learning\nfor personalized robot behavior according to a user’s cognitive abilities [61]. The Casper robot learned to imitate the\ncombination of speech and gestures from a collected data set. The robot learns to select the suitable labeled behavior\n(i.e., speech and gestures initially learned from demonstrations) that is most likely to transition the user into the desired\nstate, i.e., focused on the activity and completing the correct step. The reward function, 𝑅(𝑠,𝑏𝑖\n𝑙), depended on 𝑏𝑖\n𝑙, the\nlabeled behavior displayed by the robot, and the state 𝑠where 𝑠= {𝑠𝑟,𝑠𝑢}. Here, 𝑠𝑟represents a set of robot activity\nstates, and 𝑠𝑢is the user state such that 𝑠𝑢= {𝑠𝑓𝑛𝑐,𝑠𝑎𝑐}. In the user state, 𝑠𝑓𝑛𝑐represents the user functioning state\nwhich is one of five mental functioning states: focused, distracted, having a memory lapse, showing misjudgment, or\nbeing apathetic. The user activity state, 𝑠𝑎𝑐, represents possible actions that can be performed by seniors with cognitive\nimpairment: successfully completing a step, being idle, repeating a step, performing a step incorrectly, or declining to\ncontinue the activity. The robot was rewarded according to the state the user transitioned into—a positive reward if the\nuser was focused and completed the activity, and a negative reward if the user transitioned to an undesirable state. The\nauthors compared the proposed approach with Q-learning, and reported that the proposed approach required fewer\ninteractions for convergence and fewer steps required to complete the tea-making activity. In all the papers explained\nabove, the robot takes the users attention into account for deciding its actions. Shared attention refers to situations\ninvolving mutual gaze, gaze following, imperative pointing and declarative pointing. Da Silva and Francelin Romero [63]\npresented a robotic architecture for shared attention which included an artificial motivational system driving the robot’s\nbehaviors to satisfy its intrinsic needs, so-called necessities. The motivational system comprised necessity units that\nwere implemented as a simple perceptron with recurrent connections. The input to the artificial motivational system\nwas provided by a perception module used to detect the environmental state and to encode the state in first order logic\nwith predicates. This module included face recognition with head pose estimation and a visual attention mechanism.\nThe necessities of the robot were associated with a state-action pair in the training phase of the learning algorithm. The\nactivation of a necessity unit was dependent on the input signal representing a stimulus detected from the environment\n(i.e., the perception module) and empirically defined parameters. They compared the performance of three different RL\n18\nNeziha Akalin and Amy Loutfi\nalgorithms, namely contingency learning, Q-learning and Economic TG (ETG) methods for shared attention in social\nrobotics. ETG is a relational RL algorithm that incorporates a tree-based method for storing examples [83]. Because ETG\nperformed better in the simulation experiments, they decided to employ it in real-world experiments which entailed\none of the authors interacting with the robotic head. The authors reported that the robot’s corrected gaze index, which\nwas defined as frequency of gaze shifts from the human to the location that the human is looking at, was increased over\ntime during learning.\n4.3.8\nAffective Communication. Humans use affective communication consciously or unconsciously in their daily\nconversations by expressing feelings, opinions, or judgments. Social robots can facilitate their learning process through\nsensing and building representations of affective responses. This idea was used in [33, 71, 72]. In these studies, the socially\nassistive robot Brian 2.0 was employed as a social motivator by giving assistance, encouragement, and celebration in a\nmemory game scenario. In the scenario, the participants interacted with the robot one-on-one with the objective to find\nthe matching pictures in the memory card game (4 × 4 grid, 16 picture cards). The robot’s behaviors were adapted using\na MAXQ method to reduce the activity-induced stress in the user. The MAXQ approach is a hierarchical formulation,\nwhich accommodates a hierarchical decomposition of the target problem into smaller subproblems by decomposing\nthe value function of an MDP into combinations of value functions of smaller integral MDPs [84]. The authors argued\nthat the MAXQ algorithm was suitable for memory game scenarios due to its temporal abstraction, state abstraction,\nand sub-task abstraction. These abstractions also helped to reduce the number of Q-values that needed to be stored.\nThe detailed system was presented in [33]. In their system, they used three different types of sensory information: a\nnoise-canceling microphone for recognizing human verbal actions, an emWave earclip heart rate sensor for affective\narousal level and a webcam for monitoring the activity state (depending upon whether matching card pairs were found\nor not). They used a two-stage training process involving offline training followed by online training. The purpose of\nthe first stage was to determine the optimal behaviors for the robot with respect to the card game. The offline training\nwas carried out on a human user simulation model created with the interaction data of ten participants. In the second\nstage, they aimed to personalize the robot according to the user’s state (affective arousal and game state) for different\nparticipants in online interactions. The affective arousal and user activity state formed the user state (e.g., stressed:\nhigh arousal and not matching card, pleased: low arousal and matching card). The success of the robot’s actions was\nsubject to the improvement of a person’s user state from a stressed state to a stress-free state.\n4.4\nDeep Reinforcement Learning\nFor natural interaction, it is important that social robots possess human-like social interaction skills, which requires\nfeatures from high dimensional signals. In these cases, DRL can be useful. In fact, several researchers have begun to\nexamine the applicability of DRL in social robotics [35, 36, 73, 74, 85–87].\n4.4.1\nTactile Communication. One of the pioneering works using DRL in social robotics was presented by [36]. Here, a\nPepper robot learned to choose among predefined actions for greeting people, based on visual input. In their work, they\nsucceeded to map two different visual input sources, the Pepper robot’s RGBD camera and the webcam, to discrete\nactions (waiting, looking towards the human, hand waving and handshaking) of the robot. The reward was provided by\na touch sensor located on the robot’s right hand to detect handshaking. The robot received a predefined numerical\nreward (1 or −0.1) based on a successful or unsuccessful handshake. A successful handshake was detected through the\nexternal touch sensor. The proposed multimodal DQN consists of two identical streams of CNN for action-value function\nestimation—one for grayscale frames and another for depth frames. The grayscale and depth images were processed\nReinforcement Learning Approaches in Social Robotics\n19\nindependently, and the Q-values from both streams were fused for selecting the best possible action. This method\ncomprised two phases: the data generation phase and the training phase. In the data generation phase, the Pepper robot\ninteracted with the environment and collected data. After this phase, the training phase began. This two-stage algorithm\nwas useful in that it did not pause the interaction for training. Qureshi et al. [36] used 14 days of interaction data where\neach day of the experiment corresponded to one episode. The same authors applied a variation of DQN, the Multimodal\nDeep Attention Recurrent Q-Network (MDARQN) [73], to the same handshaking scenario in [36]. In their previous\nstudy, the robot was unable to indicate its attention. For adding perceptibility to the robot’s actions, a recurrent attention\nmodel was used, which enabled the Q-network to focus on certain parts of the input image. Similar to their previous\nwork [36], two identical Q-networks were used (one for grayscale frames and one for depth frames). Each Q-network\nconsisted of convnets, a Long Short-term Memory (LSTM) network, and an attention network [88]. The convnets\nwere used to transform visual frames into feature vectors. The network transforms an input image into D-dimensional\n𝐿feature vectors, each of them representing a part of the image 𝑎𝑡= {𝑎1\n𝑡, ...,𝑎𝐿\n𝑡},𝑎𝑙\n𝑡∈R𝐷. This feature vector was\nprovided as an input to the attention network for generating the annotation vector 𝑧∈R𝐷. The annotation vector 𝑧𝑡\nis the dynamic representation of a part of an input image at time 𝑡. 𝑧𝑡is computed with 𝑧𝑡= Í𝐿\n𝑙=1 𝛽𝑙\n𝑡𝑎𝑙\n𝑡. The LSTM\nnetwork used the annotation vector 𝑧𝑡for computing the next hidden state. Each of the streams of the MDARQN model\nwere trained by using the back-propagation method. The outputs from the two streams were normalized separately and\naveraged to create output Q-values of MDARQN. As in their previous work, handshake detection was used for the\nreward function (−0.1 for unsuccessful handshakes and 1 for successful handshakes). The horizontal and vertical axes\nof the input image were divided into five subregions, and the Q-network enabled to focus on certain parts of the input\nimage. The attention mechanism of the robot used the annotation vector 𝑧𝑡to determine the pixel location to direct\nmaximal attention to the input image. This region selection provided computational benefits by reducing the number\nof training parameters. Another work from the same authors Qureshi et al. [74] proposed an intrinsically motivated\nDRL approach for the same handshaking scenario. The proposed method utilized three basic events to represent the\ncurrent state of the interaction, i.e., eye contact, smile, and handshake. These event occurrences were predicted at the\nnext time step according to the state-action pair by a neural network called Pnet. Another neural network called Qnet\nwas employed for action selection policy guided by the intrinsic reward. The reward was determined based on the\nprediction error of Pnet, i.e., the error between actual occurrences of events 𝑒(𝑡+ 1) and Pnet’s prediction ˆ𝑒(𝑡+ 1). An\nOpenCV-based event detector module provided the labels for three events (i.e., actual event occurrence). The Qnet\nwas a dual stream deep convolutional neural network mapping pixels to q-values of the actions (wait, look towards\nhuman, wave hand, and shake hand). Pnet was a multi-label classifier which was trained to minimize the prediction\nerror between ˆ𝑒and 𝑒by using the Binary Cross Entropy (BCE) loss function. The reward consisted in predetermined\nnumerical values depending on the prediction error between 𝑒and ˆ𝑒. They investigated the impact of three different\nreward functions named strict, neutral and kind. In all reward functions, if all three events are predicted successfully by\nPnet, Qnet receives a reward of 1, if all events are predicted wrong then Qnet gets a reward of −0.1. If one or two events\nare predicted correctly then different reward functions penalize differently, with the strict reward having the highest\npenalties. The authors reported that the reward functions with more positive reward on incorrect predictions yielded\nmore socially acceptable behavior. They compared the collected total reward from 3 days of experiments in a public\nplace, each day following a different policy (random policy, Qnet policy, and the previously employed method [36]).\nThe current proposed model led to more human-like behaviors, according to the human evaluators.\n20\nNeziha Akalin and Amy Loutfi\n4.4.2\nNo Communication Medium. Another study using the Pepper robot and DQN was presented by Cuayáhuitl [35].\nIn their scenario, human participants played a ‘Noughts and Crosses’ game with two different grids (small and big)\nagainst the Pepper robot. They used a CNN for recognizing game moves, i.e., hand-writing on the grid. These visual\nperceptions and the verbal conversations of the participant were given as an input to their modified DQN. The author\nmodified the Deep Q-Learning with Experience Replay [43] by adding the identification of the worst action set ˆ𝐴. ˆ𝐴\nincluded actions with 𝑚𝑖𝑛(𝑟(𝑠,𝑎) < 0 ∀𝑎∈𝐴) and 𝐴is the set of actions leading to win the game. The action selection\nwas done with 𝑚𝑎𝑥\n𝑎∈𝐴\\ ˆ𝐴\n𝑄(𝑠,𝑎;𝜃). In other words, the proposed DQN algorithm refines the action set at each step to make\nthe agent learn to infer the effects of its actions (such as selecting the actions that lead to winning or to avoid losing).\nThe reward consisted in predefined numerical values based on the performance of the robot in the game. Therefore, this\nstudy does not use any communication medium for reward formulation. The robot received the highest reward in the\ncases ‘about to win’ or ‘winning’, whereas the robot received the lowest reward in the cases ‘about to lose’ or ‘losing’.\n4.4.3\nNonverbal Communication. Expressive robot behaviors including facial expressions, gestures, and posture are\nfound to be useful to express the robots’ internal states, goals, and desires [89]. To date, several studies have investigated\nthe production of expressive robot behaviors using DRL, including gaze [85, 86] and facial expressions [87]. Lathuilière\net al. [85] modeled Q-learning with a Long Short Term Memory (LSTM) to fuse audio and visual data for controlling the\ngaze of the robotic head to direct it towards the targets of interest. The reward function was defined as 𝑅𝑡= 𝐹𝑡+1+𝛼Í\n𝑡+1\nwhere 𝛼≥0 serves as an adjustment parameter. If the speech sources lie within the camera’s field of view, large\n𝛼values return large rewards, i.e, 𝛼permits to give importance to speaking persons. The reward function includes\nface reward 𝐹𝑡(𝛼= 0) and speaker reward (𝛼> 0). The number of visible people (face reward) and the presence of\nspeech sources in the camera field of view (speaker reward) were observed from the temporal sequence of camera\nand microphone observations. The proposed DRL model was trained on a simulated environment with simulated\npeople moving and speaking, and on the publicly available AVDIAR dataset. In this offline training, they compared\nthe reward obtained with four different networks: early fusion and late fusion of audio and video data, as well as only\naudio data and only video data. The authors emphasized the importance of audio-visual fusion in the context of gaze\ncontrol for HRI. They reported that the proposed method outperformed the handcrafted strategies. Lathuilière et al.\n[86] extended the study presented in [85] by investigating the impact of the discount factor, the window size (number\nof past observations affects the decision), and LSTM network size. They reported that in the experiments with AVDIAR\ndataset, high discount factors were prone to overfit, whereas in the simulated environment low discount factors resulted\nin worse performance. Using smaller window sizes accelerated the training, however, larger window sizes performed\nbetter in simulated environment. Changing the LSTM size did not make a substantial difference in the results. In a\nsimilar vein, Churamani et al. [87] utilized visual and audial data for enabling the Nico robot to express empathy\ntowards the users. They focused on both recognizing the emotions of the user and generating emotions for the robot to\ndisplay. The presented model consisted of three modules: an emotion perception module, an intrinsic emotion module,\nand an emotion expression module. For the perception module, both the visual and audio channels were used to train a\nGrowing-When-Required (GWR) Network. For the emotion expression module, they used a Deep Deterministic Policy\nGradient (DDPG) based actor-critic architecture. The reward was the symmetry of the eyebrows and mouth in offline\npre-training, whereas in online training the reward was provided by the participant deciding whether the expressed\nfacial expression was appropriate. The Nico robot expressed its emotions through programmable LED displays in the\neyebrow and mouth area.\nReinforcement Learning Approaches in Social Robotics\n21\n4.5\nPolicy-Based Methods\n4.5.1\nHigher Level Interaction Dynamics: Comfort. In the domain of socially assistive robotics, the robots are expected\nto be adaptive to their users to some extent, by using social interaction parameters (for example, the interaction distance,\nthe speed of motion and utterances) regarding to the task, to the users’ comfort and personality. Several studies [90–92]\nexamined the Policy Gradient Reinforcement Learning (PGRL) for adapting the robot behaviors using social interaction\nparameters. Mitsunaga et al. [90, 91] presented a study where the Robovie II robot adjusted its behaviors (i.e., proxemics\nzones, eye contact ratio, waiting time between utterance and gesture, motion speed) according to comfort and discomfort\nsignals of humans (i.e., body re-positioning amount and the time spent gazing at the robot). These signals were used as\nreward. The goal of the robot was to minimize these signals, thereby reducing experienced discomfort in the human\ninteractant. In [92], an ActiveMedia Pioneer 2-DX mobile robot adapted its personality by changing the interaction\ndistance, speed and frequency of motions, and vocal content (what and how the robot says things). The purpose of\nthis adaptation was to improve the user’s task performance. Their reward function was based on user performance,\ndefined as the number of performed exercises. Specifically, the number of performed exercises over the previous 15 s\nwas computed every second and results were averaged over a 60 s period to produce the final evaluation for each policy.\nThey used a threshold for the reward function (7 exercises in the first 10 min) and a time range to adjust the fatigue\nincurred by the participant. The participant’s performance was tracked by the robot through a light-weight motion\ncapture system worn by the participant.\n5\nCATEGORIZATION OF RL APPROACHES IN SOCIAL ROBOTICS BASED ON REWARD\nWe now present a review of the literature but with focus on the reward function. Designing the reward function is\nperhaps the most crucial step in the implementation of an RL framework. One of the main contributions of this paper is\na categorization of different types of reward functions that are used in RL and social robotics. The categorization is\ngiven in Figure 4.\nRL in Social Robotics\nInteractive RL\nIntrinsically\nMotivated Methods\nTask Performance\nDriven Methods\nImplicit Feedback\nExplicit Feedback\nHomeostasis-based\nmethods\nHuman Task\nPerformance\nRobot Task\nPerformance\nHuman and Robot\nTask Performance\nFig. 4. Reinforcement Learning approaches in social robotics.\nAs we have already discussed the used RL methods in Section 4, they are not included here. Moreover, the evaluation\nmethodologies are also discussed in a separate section (see Section 6).\n5.1\nInteractive Reinforcement Learning\nDifferent approaches have been proposed for incorporating the human assistance in the learning process of artificial\nagents, including learning from human feedback [24, 76] and learning from demonstration. Learning from demonstration\n22\nNeziha Akalin and Amy Loutfi\nis beyond the scope of this paper, we focus on learning from human feedback. In traditional RL, the agent receives\nenvironmental reward from a predefined reward function. Interactive RL makes use of human feedback in the learning\nprocess in combination with or without environmental reward. Interactive RL framework is given in Figure 5. Integrating\nhuman feedback with RL can be accomplished in different ways, such as via evaluative feedback [93], corrective\nfeedback [94] or guidance [95].\nAgent\nEnvironment\naction \nat\nst+1\nstate \nst\nHuman\nht+1\nEvaluative\nfeedback \nht\nFig. 5. Interaction in interactive RL (reproduced from [96])\nLi et al. [96] discuss different interpretations of human evaluative feedback in interactive reinforcement learning\n(referred to as human-centered RL throughout the paper). They distinguish between three types of human evaluative\nfeedback: interactive shaping, learning from categorical feedback and learning from policy feedback. In interactive\nshaping, human feedback is interpreted as numeric reward, and this reward can be myopic i.e., 𝛾= 0 [93] or non-\nmyopic i.e., 𝛾is different from 0 [97]. Human feedback might be erroneous when the task is repetitive. Moreover,\nhuman teachers tend to give less frequent feedback (e.g., due to boredom and fatigue) as the learning progresses.\nModeling human feedback has been found to be an efficient strategy when the meaning of human-delivered feedback is\nambiguous [76]. Loftin et al. [76] developed a probabilistic model of human teacher’s feedback. They interpret human\nfeedback as categorical feedback, considering that human teachers may have different feedback strategies. In their\nwork, depending on the human teacher’s training strategy, a lack of feedback can convey information about the agent’s\nbehavior. Human training strategies are categorized into four groups: reward-focused strategy (positive reward for\ncorrect actions and no feedback for incorrect actions), punishment-focused strategy (no feedback for correct actions\nand punishment for incorrect actions), balanced strategy (positive reward for correct actions and punishment for\nincorrect actions) and inactive strategy (the human teacher rarely provides feedback). Corrective feedback can be\ncategorized under policy feedback. As an example, Celemin and Ruiz-del Solar [94] presented a framework named\nCOACH (COrrective Advice Communicated by Humans) which uses human corrective feedback in the action domain\nas binary signals (i.e., increase or decrease the magnitude of the current action). In their comparison with classical\nreinforcement learning approaches, they showed that RL agents can benefit from human feedback, i.e., learning\nprogresses faster [94]. When the agent learns both human feedback and environment reward, the human feedback\ncan be used to guide the agent’s exploration [95]. The guidance includes both providing feedback on past actions and\nguiding the agent in the learning process through future-directed rewards. Human guidance can reduce the action\nspace by narrowing down the action choices [98], which speeds up the training process by accelerating the convergence\ntowards an optimal policy.\nReinforcement Learning Approaches in Social Robotics\n23\nIn the context of HRI, the human can be in the learning loop by way of varying types of inputs, such as providing\nfeedback via a GUI (e.g., by button or mouse clicks). Alternatively, the feedback can be delivered more naturally, via\nemotions, gestures and speech. Therefore, this category comprises two subcategories: (1) explicit feedback, when the\nfeedback is direct, provided through an interface such as ratings, and labels; (2) implicit feedback, if the human feedback\nis spontaneous behavior or reactions such as non-verbal cues and social signals. The terms “explicit feedback” and\n“implicit feedback” are adopted from Schmidt [99]’s “implicit interaction” study in human-computer interaction. For a\nquick summary of the studies, see Table 1.\nTable 1. Summary of Interactive Reinforcement Learning approaches in social robotics.\nReference\nSubcategory\nType of RL\nReward\nSocial robot\nBarraquand et al. [52]\nExplicit\nfeedback\nQ-learning\nUser provides reward by using\nrobot’s tactile sensors\nAibo\nSuay et al. [26, 30]\nExplicit\nfeedback\nQ-learning\nHuman teacher delivers reward\nor guidance through a GUI\nNao\nKnox et al. [29]\nExplicit\nfeedback\nTAMER\nHuman teacher provides reward\nby using a remote\nNexi\nYang et al. [53]\nExplicit\nfeedback\nQ-learning\nUser gives reward by\ntouching robot’s tactile sensors\nPepper\nSchneider et al. [44]\nExplicit\nfeedback\nDueling bandit\nUser provides feedback\nthrough a button\nNao\nChuramani et al. [87]\nExplicit\nfeedback\nDDPG\nUser gives reward whether\nrobot’s expression is appropriate to\naffective context of dialogue\nNico\nTseng et al. [49]\nExplicit\nfeedback\nModified R-Max\nUser provides reward\nthrough a software\nARIO\nGamborino et al. [69]\nExplicit\nfeedback\nSARSA\nUser’s transition between\nbad and good mood state\nRoBoHoN\nRitschel et al. [4]\nExplicit\nfeedback\nk-armed bandit\nUser gives reward via buttons\nReeti\nPatompak et al. [32]\nImplicit\nfeedback\nR-learning\nVerbal reward by the user\nbased on robot’s social distance\nPepper\nThomaz et al. [31]\nImplicit\nfeedback\nQ-learning\nHuman teacher provides\nreward or guidance\nLeonardo\nThomaz et al. [28]\nImplicit\nfeedback\nQ-learning\nHuman teacher provides guidance\nthrough speech or gestures\nLeonardo\n24\nNeziha Akalin and Amy Loutfi\nTable 1 – continued from previous page\nReference\nSubcategory\nType of RL\nReward\nSocial robot\nGruneberg et al.\n[100, 101]\nImplicit\nfeedback\nNot specified\nHuman teacher’s smile and frown\nNao\nAddo et al. [55]\nImplicit\nfeedback\nQ-learning\nVerbal reward of the user\nNao\nZarinbal et al. [54]\nImplicit\nfeedback\nQ-learning\nHuman teacher’s facial expressions\nNao\nMitsunaga et al.\n[90, 91]\nImplicit\nfeedback\nPGRL\nDiscomfort signals of the user\nRobovie II\nLeite et al. [45]\nImplicit\nfeedback\nMulti-armed\nbandit\nUser’s affective cues and\ntask-related features\niCat\nChiang et al. [56]\nImplicit\nfeedback\nQ-learning\nNumerical values based on\nattention and engagement\nlevels of the user\nARIO\nGordon et al. [68]\nImplicit\nfeedback\nSARSA\nWeighted sum of\nfacial valence and engagement\nTega\nRitschel et al. [57]\nImplicit\nfeedback\nQ-learning\nChange in user engagement\nReeti\nWeber et al. [59]\nImplicit\nfeedback\nQ-learning\nVocal laughter and visual smiles\nReeti\nPark et al. [58]\nImplicit\nfeedback\nQ-learning\nWeighted sum of engagement\nand learning\nTega\nRamachandran et al.\n[102]\nImplicit\nfeedback\nPOMDP\nEngagement level of the user\nNao\nMartins et al. [50]\nImplicit\nfeedback\nModel-based RL\nand POMDP\nThe robot’s actions’ impact\non the user\nGrowMu\n5.1.1\nExplicit Feedback. In the explicit feedback approach, the feedback of the human teacher is given by direct\nmanipulations and generally through an artificial interface. The human teacher observes the agent’s actions and\nenvironment states and subsequently provides feedback to the agent through a graphical user interface (GUI) or through\nthe robot’s (touch) sensors. In this approach, the feedback from the human teacher is noiseless and direct in the form\nof numerical values provided via a button, a Graphical User Interface (GUI), or through the robot’s touch sensors. In\ngeneral, the main purpose of the interaction is to teach the robot to do something in this category. Unlike the explicit\nfeedback category, in the implicit feedback category, the majority of studies include a social scenario such as robot\ntutoring, robots supporting the human in a game, etc. The studies under this category are [4, 26, 29, 30, 44, 52, 53].\nReinforcement Learning Approaches in Social Robotics\n25\n5.1.2\nImplicit Feedback. Human social signals are widely used as reward in social human-robot interaction. The most\ncommonly used signals are human emotions, as these have a great influence on decision-making [103]. Computational\nmodels of emotions have been studied by many researchers as part of the agent’s decision making architecture, by\nmodelling the RL agents with emotions or incorporating human emotions as an input to the learning process. As an\nexample, Moerland et al. [104] surveyed RL studies focusing on agent/robot emotions. Since emotions also play an\nimportant role in communication and social robots [7], there exist various studies considering these aspects for RL and\nsocial robotics. In the implicit feedback approach, the agent learns from spontaneous natural behavior and reactions\nof the interactant, i.e., emotions, speech, gestures, etc. This type of feedback is noisy and indirect. In other words, in\nthis approach, human feedback requires pre-processing and the quality of the feedback depends on the perception and\nrecognition algorithms being used. Unlike explicit feedback, the implicit feedback is not provided directly through an\ninterface. Instead, the human’s emotions or verbal instructions serve as reward or guidance signals. The studies in this\ncategory are [28, 31, 32, 45, 50, 55–57, 59, 68, 90, 91, 100–102].\n5.2\nMethods Using Intrinsic Motivation\nIt is a common approach to examine the biological and psychological decision-making mechanisms and to use a similar\nmethod for autonomous systems. One such approach consists in combining intrinsic motivation with reinforcement\nlearning. Intrinsic motivation is a concept in psychology, which denotes the internal natural drive to explore the\nenvironment, as well as gain new knowledge and skills. The activities are performed for inherent satisfaction rather\nthan external rewards [105]. Researchers have proposed computational approaches that use intrinsic motivation [106].\nIn intrinsically motivated RL, the main idea is using intrinsic motivations as a form of reward [107]. There are different\nintrinsic motivation models within the RL framework [20]. However, in social robotics, the idea of maintaining the\ninternal needs of the robot (detailed in Section 5.2) has received much attention [13, 34, 63–66, 108]. One exception\nis [74], in which prediction error of social event occurrences was used as intrinsic motivation. For a quick summary, see\nTable 2.\nTable 2. Summary of Intrinsically Motivated Methods in social robotics.\nReference\nSubcategory\nType of RL\nReward\nSocial Robot\nMalfaz et al. [108]\nHomeostasis based\nQ-learning\nWellbeing of the robot\nMaggie\nCastro-Gonzalez et al.\n[64–66]\nHomeostasis\nbased\nObject\nQ-learning\nVariation of robot’s wellbeing\nMaggie\nMaroto et al. [13]\nHomeostasis\nbased\nQ-learning\nMaximization of robot’s well-being\nMini\nPerula et al. [34]\nHomeostasis\nbased\nQ-learning\nWell-being of the robot\nMini\nDa Silva et al. [63]\n-\nEconomic TG\nGenerated on the basis of\ninternal state estimate\nRobotic head\nQureshi et al. [74]\n-\nDQN\nPrediction error of an action\nconditional prediction network\nPepper\n26\nNeziha Akalin and Amy Loutfi\nHomeostasis-Based Methods. Homeostasis, as defined by Cannon [109], refers to a continuous process of maintaining\nan optimal internal state in the physiological condition of the body for survival. Berridge [110] explains homeostasis\nmotivation with a thermostat example that behaves as a regulatory system by continuously measuring the actual\nroom temperature and comparing it with a predefined set point, and activating the air conditioning system if the\nmeasured temperature deviates from the predefined set point. In the same manner, the body maintains its internal\nequilibrium through a variety of voluntary and involuntary processes and behaviors. The homeostasis-based RL in social\nrobotics is presented in [13, 34, 64–66, 108]. These studies introduced a biologically inspired approach that depends\non homeostasis. The robot’s goal was to keep its well-being as high as possible while considering both internal and\nexternal circumstances. The common theme in these studies is that the robot has motivations and drives (needs), where\neach drive has a connection with a motivation as in Equation (15).\n𝑖𝑓𝐷𝑖< 𝐿𝑑𝑡ℎ𝑒𝑛𝑀𝑖= 0\n𝑖𝑓𝐷𝑖≥𝐿𝑑𝑡ℎ𝑒𝑛𝑀𝑖= 𝐷𝑖+ 𝑤𝑖\n(15)\nMotivations whose drives are below the activation levels do not initiate a robot behavior. This was formulated\nas 𝑖𝑓𝐷𝑖< 𝐿𝑑𝑡ℎ𝑒𝑛𝑀𝑖= 0 where 𝐷𝑖is a drive, 𝐿𝑑the activation level, and 𝑀𝑖is the related motivation. The\nmotivation depends on two factors: the associated drive and the presence of an external stimulus, this was formulated as\n𝑖𝑓𝐷𝑖≥𝐿𝑑𝑡ℎ𝑒𝑛𝑀𝑖= 𝐷𝑖+ 𝑤𝑖where 𝑤𝑖is the related external stimulus. These motivations serve as action stimulation\nto satiate the drives. A drive can be seen as a deficit that leads the agent to take action in order to alleviate this deficit\nand maintain an internal equilibrium. The ideal value for a drive is zero, corresponding to the absence of need. The\nrobot learns how to act in order to maintain its drives within an acceptable range, i.e., to maintain its well-being. The\nwell-being of the robot was defined as:\n𝑊𝑏= 𝑊𝑏𝑖𝑑𝑒𝑎𝑙−\n∑︁\n𝑖\n𝛼𝑖𝐷𝑖\n(16)\nwhere𝑊𝑏𝑖𝑑𝑒𝑎𝑙is the value of the well-being when all drives are satiated, and 𝛼𝑖is the set of the personality factors that\nweight the importance of each drive. The variation of the robot’s well-being is used as reward signal and calculated\nwith the Equation (17)\nΔ𝑊𝑏= 𝑊𝑏𝑡−𝑊𝑏𝑡−1\n(17)\ni.e., the difference between the current well-being 𝑊𝑏𝑡and the well-being in the previous step 𝑊𝑏𝑡−1.\nIn several works [64–66], a variation of the traditional Q-learning algorithm was used in addition to the homeostasis-\nbased approach. In all of these, the authors referred to the proposed algorithm as Object Q-learning. In this approach,\nthere are actions associated with each object in the environment, and the robot considers its state in relation to every\nobject independently. Thus, there is an assumption that an action execution in relation to a certain object does not\ninfluence the state of the robot in relation to other objects. However, in reality, an action execution may create collateral\neffects. In other words, an action associated with a particular object, e.g., approaching it, may affect the robot’s state in\nrelation to other objects, e.g., moving away from them. The update of Q-values accounted for these collateral effects.\nThe purpose of this simplification was to reduce the number of states during the learning process. In their experiments,\nto reduce the state space, the robot learned what to do with each object without considering its relation to other objects.\nIn other words, they assumed that an action execution associated with a certain object will not affect the state of the\nrobot in relation to the rest of the objects. The proposed algorithm was implemented on the social robot Maggie that\nlived in a laboratory and interacted with several objects in the environment (e.g., a music player, a docking station, or\nhumans). Castro-González et al. [65] appears to be closely linked to the other papers discussed here with one difference\nReinforcement Learning Approaches in Social Robotics\n27\nbeing that a discrete emotion, fear, was used as one of the motivations. Unlike other motivation-drive pairs, no drive\nwas associated with the ‘fear motivation’ (i.e., fear is not a deficiency of any need). ‘Fear motivation was’ linked to\ndangerous situations (that can cause damage the robot) and directed the robot to a secure state. As an example, the\nmotivation ‘social’ was not updated if the user who occasionally hit the robot was around.\n5.3\nMethods Driven by Task Performance\nTask performance denotes the effectiveness with which an agent performs a given task, and the performance metrics\ncan vary for different tasks. In these methods, the design of the reward function is based on task-driven measures,\nwhich often include some problem-specific information, especially the task performance of the robot, task performance\nof the human, or both. For a quick summary, see Table 3.\n5.3.1\nHuman Task Performance Driven Methods. In these human task performance driven methods, the reward function\nis based on the user’s success in performing a task related to the interaction with the robot. The studies in this category\nare [47, 92].\n5.3.2\nRobot Task Performance Driven Methods. In these methods, the reward design depends on the robot’s task\nperformance. Robot behaviors that satisfy the user’s preferences, accurate completion of the task, finishing the task\nwithin a desired amount of time, visiting certain states, and robot actions that benefit or satisfy the user are examples\nfor task performance measures. The studies in this category are [1, 3, 35, 36, 46, 60, 62, 67, 70, 85, 86].\n5.3.3\nHuman and Robot Task Performance Driven Methods. In the previous two sections, we listed the studies using\ntask performance of the robot and human as reward signal. There are also studies that use a combination of the human’s\nand the robot’s task performance as reward signal. As an example, in [33, 72] the robot received the highest reward\nif the user completed the task successfully. The robot also received reward for its actions that were suitable for the\ncurrent situation. Likewise, in [61], the robot was rewarded based on actions that transitioned the user into a desirable\nstate (e.g., completing the activity). Other papers in this category are [33, 71, 72].\n6\nEVALUATION METHODOLOGIES\nThe past decade has seen a rapid growth of social robotics in diverse uncontrolled environments such as homes,\nschools, hospitals, shopping centers, or museums. In this review, we have seen various application domains in a range\nof fields including therapy [3], eldercare [62], entertainment [59], navigation [32], healthcare [44], education [58],\npersonal robots [13], and rehabilitation [92]. Research in the field of social robotics and human-robot interaction\nbecomes crucial as more and more robots are entering our lives. This brings many challenges as social robots are\nrequired to deal with dynamic and stochastic elements in social interaction in addition to the challenges in robotics.\nBesides these challenges, validation of social robotics systems with users necessitates efficient evaluation methodologies.\nRecent studies underline the importance of evaluation and assessment methodologies in HRI [111]. However, developing\na standardized evaluation procedure still remains a difficult task. Furthermore, in RL-based robotic systems, there is a\nneed to explore various human-level factors (personal preferences, attitudes, emotions, etc.) to assure that the learned\npolicy leads to better HRI. Additionally, how can we evaluate whether the learned policy conveys the intended social\nskill(s)? As an example, in Qureshi et al. [36, 73, 74]’s study, the model performance on a test dataset was evaluated by\nthree volunteers who judged if the robot’s action was an appropriate one for the current scenario. In [87], there both\nannotators and participants rated whether the robot was able to associate the facial expressions with the conversation\n28\nNeziha Akalin and Amy Loutfi\nTable 3. Summary of Task performance driven methods in social robotics.\nReference\nSubcategory\nType of RL\nReward\nSocial Robot\nTapus et al. [92]\nHuman task\nperformance\nPGRL\nUser performance\nPioneer 2-DX\nGao et al. [47]\nHuman task\nperformance\nMulti-arm\nbandit\nUser task performance and\nuser’s verbal feedback\nPepper\nChan et al. [71, 72]\nHuman and robot\ntask performance\nMAXQ\nSuccess of the robot’s actions\nin helping or improving user’s\naffect and task performance\nBrian 2.0\nChan et al. [33]\nHuman and robot\ntask performance\nMAXQ\nTask performance of\nhuman and robot\nBrian 2.0\nMoro et al. [61]\nHuman and robot\ntask performance\nQ-learning\nNumerical numbers based on\nrobot’s performance on user’s\nactivity state\nCasper\nNejat et al. [62]\nRobot task\nperformance\nQ-learning\nUser provides verbal feedback\nBrian\nRanatunga et al. [70]\nRobot task\nperformance\nTD(𝜆)\nHead and eye kinematic\nscheme of the robot\nZeno\nKeizer et al. [1]\nRobot task\nperformance\nMonte-Carlo\ncontrol\nThe robot’s performance\nas a bartender\niCat\nQureshi et al. [36]\nRobot task\nperformance\nMultimodal\nDQN\nNumerical values based on\nrobot’s handshake success\nPepper\nPapaioannou et al.\n[60]\nRobot task\nperformance\nQ-learning\nTask completion of the robot\nPepper\nQureshi et al. [73]\nRobot task\nperformance\nMDARQN\nNumerical values based on\nrobot’s handshaking success\nPepper\nHemminghaus et al. [3]\nRobot task\nperformance\nQ-learning\nRobot’s task performance and\nexecution cost of the robot’s action\nFurhat\nChen et al. [67]\nRobot task\nperformance\nQ-learning\nNumerical values based on\ncorrectly completed tasks\nMobile robots\nRitschel et al. [46]\nRobot task\nperformance\nn-armed\nbandit\nRobot’s performance at convincing\nuser to select healthy drink\nReeti\nLathuiliere et al. [85, 86]\nRobot task\nperformance\nDQN\nNumber of observed faces and\npresence of speech sources\nin the visual field\nNao\nCuayahuitl [35]\nRobot task\nperformance\nDQN\nNumerical values based on\nrobot’s performance in the game\nPepper\ncontext. The independent annotators’ ratings were higher than the participants’, which, as the authors argued, might\nbe explained by discrepancies between the participants’ actual expressed emotion and the intended emotion. In such\ncases, additional sensory information could be useful for validating that the adaptive robot behaviors lead to better\nReinforcement Learning Approaches in Social Robotics\n29\nHRI. For example, Park et al. [58] analyzed the body poses and electrodermal activity (EDA) of the participants to\ncheck their correlation with participant’s engagement. This kind of approach could be used to support subjective\nevaluations. A comparative evaluation methodology considering both the learned policy and the user’s experience\nabout the interaction is another way of evaluation. As an example [32, 33, 56, 90, 91] presented the policy for each\nparticipant as well as a discussion on the effectiveness of the robot behavior on the user based on their comments and\nsubjective evaluations.\nThe papers in the scope of this manuscript used different evaluation and assessment methodologies for their\nalgorithms and for their systems with users. We identify three types of evaluation methodologies: (1) an evaluation\nfrom the algorithm point of view, (2) evaluation and assessment of user experience-related subjective measures, and\n(3) evaluation of both learning algorithm-related factors and user experience-related factors. Several studies only\nreported the self-rated questionnaire results [45] or user opinions [55]. There are also studies which do not include any\nevaluation, and only a short discussion regarding the learned policy [53, 57, 100, 101]\nThe cumulative collected reward over time is the most commonly used evaluation method. As learning progresses,\nthe frequency of negative rewards is expected to decrease and positive rewards are expected to increase. Thus, the\ncumulative reward and comparing the reward across different settings and variations of algorithms are one of the\nmeasures for evaluating the efficiency of learning [49, 50, 52, 85, 86]. The evolution of the learning algorithm over time\n(e.g., the evolution of 𝑄values) is another evaluation method. Several studies presented only the learning evolution of\ntheir system without mentioning how a participant would perceive the learned robot behaviors [13, 34, 61, 63–66, 108].\nComparison of user experiences (e.g., learning gains of children) for adaptive and non-adaptive robot is another way of\nevaluation [68, 102]. We also see evaluation by using only interaction related objective measures such as the frequency\nof turn-taking and dialogue duration with the robot [60]. Task-related evaluation measures (i.g., the number of moves\nneeded to solve a game with an adaptive versus a random robot) together with Q-matrix [3], or average task success\nand average reward [35] are used. In some IRL studies, the purpose is only teaching a robot. In these studies, evaluation\nmetrics are training time [26, 29], or training related parameters (e.g., the amount of positive and negative feedback) [28].\nStudies reporting both subjective user opinions and algorithm related measures are [30, 44, 46, 59, 92]. Interaction\nrelated objective measures such as interaction duration, distance to the robot, preferred motion speed of the robot in\ncombination with questionnaires are other measures for evaluating the efficiency of the learned policy. Studies also use\na comparison of different algorithms in terms of average steps, average reward, average execution time together with\nquestionnaires [67], and the number of times the preferences of the trainer match with the agent’s action [69], reward\nand discussion of observations from the experiments [46], questionnaires and task-related parameters (e.g., time to\ncomplete the task) [47].\n7\nDISCUSSION\nIn this paper, we present the RL approaches in social robotics. In virtual game environments (e.g., Atari, Go, etc.) which\nare commonly used testbeds for RL implementations, the goal is well defined (e.g., getting higher scores, accomplishing\na game level, or winning the game). In social robotics, the goal is not that clear. Still, we argue that social robots could\nprovide a unique potential testbed for RL implementations in real-world scenarios, in a sense that they can deal with\ntransparency issues by showing their internal states through social cues (e.g., facial expressions, gaze, speech, LEDs on\ntheir body, tablet). In Section 5, we presented RL approaches based on reward types. IRL with implicit reward is the\nmost widely used approach in social robotics since human social cues occur naturally during the interaction. However,\nthe change in social cues can be slow, which leads to sparse reward. A combination of the reward approaches presented\n30\nNeziha Akalin and Amy Loutfi\nSection 5, namely intrinsically motivated methods, IRL with implicit feedback, and task performance-driven methods\ncould be an approach to deal with the sparse reward problem. This way the robot could receive a reward even when\nthere is no dramatic change in social cues or the task is not completed in one step. Similar to the homeostasis-based\napproaches, combining emotional models for robots’ decision-making mechanisms could be helpful. The interested\nreader may refer to [104] which presents a thorough analysis and discussion of computational emotional models\nincorporated within RL. Th sparse reward problem is not the only problem in real-world social HRI. We continue to\nthe discussion with the proposed solutions for real-world RL problems in Section 7.1. Later on, we present possible\ninteresting future directions in Section 7.2.\n7.1\nProposed Solutions to Real-World RL Problems\nRL is a powerful and versatile algorithmic tool and has been shown to perform better than humans in simulated\nenvironments [43] However, the progress on applying RL methods to real-world systems is not so advanced yet. This is\ndue to the complexity of the real-world. Dulac-Arnold et al. [112] discuss nine challenges of realizing RL on real-world\nsystems. Here, we discuss these challenges and how some papers tackled them in real-world HRI with social robots.\nThe first mentioned challenge is “training off-line from the fixed logs of an external behavior policy”. This challenge\napplies to HRI since users would not tolerate the long pauses and action delays of the social robot. As an example, Qureshi\net al. [36] suggested an approach where they divided training into two stages. In the first stage, the robot interacts with\nthe environment and gathers data, whereas in the second stage the robot rests and trains.\nThe second challenge is “learning on the real system from limited samples”. This challenge is especially valid for\nHRI since the interaction time with the users is limited in controlled lab experiments. Moreover, users get bored and\ntired with longer interaction duration. As mentioned [112] exploration must be limited. As an example, in [13, 34],\nexploration and exploitation phases are separated. A predefined duration is set for the exploration phase, in which\nthe robot runs through all possible states and actions. Moreover, they also decreased the learning rate 𝛼throughout\nthe exploration phase to increase the importance of previously learned information as the learning progresses. In the\nexploitation phase, they set 𝛼to 0. As mentioned in [112], for improving the sample efficiency expert demonstrations\ncan be beneficial to avoid learning from scratch. For example, Moro et al. [61] combined LfD with Q-learning for a\nCasper robot helping older people in a tea making scenario. Another mentioned solution was model-based RL, of which\nwe see two examples in social robotics [49, 50]. In addition, long-term interactions (several sessions [58, 68, 102]) are\nimportant for HRI and could be beneficial for RL to collect samples.\nThe third challenge is “high-dimensional continuous state and action spaces”. In the context of social robotics,\nthe problem also needs to be simplified due to the low onboard computational power of most platforms. That might\nbe another reason for a small set of actions in the reviewed papers. To overcome this challenge we see several\napproaches. As an example, human guidance was found to effective [26], as well as Object Q-learning [64–66] and\naction elimination [35].\nThe fourth challenge is “safety constraints that should never or at least rarely be violated”. The mentioned approaches\nfor this challenge in [112] include imposing safety constraints during the training. In the current literature, social\nrobot interactions are generally conducted in a controlled laboratory environment and the researchers are available to\nintervene if any problems occur. Therefore, this challenge seems to get little attention.\nThe fifth challenge is “tasks that may be partially observable, alternatively viewed as non-stationary or stochastic”.\nWe see several attempts in social robotics to deal with this challenge such as in POMDP based approaches [50, 102],\nReinforcement Learning Approaches in Social Robotics\n31\nand in DRL where several frames are stacked together for incorporating the history of the agent observations. Another\nmentioned approach to deal with this challenge was using recurrent networks which were applied in [63].\nThe sixth challenge is “reward functions that are unspecified, multi-objective, or risk-sensitive”. Some papers that\nuse simulated environments for training and testing on real-world interactions. In these papers, there are different\nreward functions for the simulated world and the real-world. Generally, the real-world reward functions are simplified\nto one parameter such as feedback of the user or predefined numeric numbers, whereas the simulated world reward\nfunctions are more complex including several parameters.\nThe seventh challenge is “system operators who desire explainable policies and actions”. This is particularly valid for\nsocial robotics, since ambiguous robot behaviors might affect the user’s willingness to interact again. Moreover, if the\nhuman trains the robot, the intention and internal state of the robot becomes crucial for the success of the training. As\nan example, Knox et al. [29] discussed the transparency challenges and their effect on the training time. Thomaz and\nBreazeal [28] observed that participants had a tendency to wait for eye contact with the robot before saying the next\nutterance while training the robot. These kinds of social cues on the robot could be used for explaining its actions and\ninternal states.\nThe eighth challenge is “inference that must happen in real-time at the control frequency of the system”. The\nreal-world is slower than the simulated world both in reaction and data generation. To deal with this challenge, several\nresearchers used an additional interface between the robot and the human, so that the inference is received from the\ninterface rather than robot control.\nThe ninth challenge is “large and/or unknown delays in the system actuators, sensors, or rewards”. We see several\napproaches to deal with this challenge, as an example [52] considered to increase the effect of human-delivered positive\nreward in larger time frames and to decrease the effect of negative reward in a shorter time frame. Another approach\nwas estimating reward from natural human feedback using the gamma distribution [49].\n7.2\nFuture Outlook\nThere are still many interesting potential problems and open questions to be solved in RL for social robotics. Applications\non physically embodied robots are limited due to the enormous challenge of complexity and uncertainty in real-world\nsocial interactions. The increased prevalence of RL in physical social robots will shed further light on this topic. Another\nunanswered question is how RL-based social robotics may include the generation of reward signals from ambiguous or\nconflicting sources of implicit feedback, and how learned skills can be transferred to different robots. Further work\ncould also investigate larger state-action spaces, as current studies are mostly limited to a small sets.\nDespite the fact that there are goal-oriented approaches for social robot learning [113, 114], in the current literature, the\nsocial robot that learns through RL has only one goal, such as performing a single task and optimizing a single reward\nfunction. However, in many real-world scenarios, a robot may need to perform a diverse set of tasks. As an example,\nsocially assistive robots designed with the purpose of assisting older people in their houses may need to accomplish\nseveral tasks such as medication reminders, detecting issues, informing caregivers, and managing plans. Multi-goal\nRL enables an agent to learn multiple goals, hence the agent can generalize the desired behavior and transfer skills to\nunseen goals and tasks [115]. This has been applied on robotic manipulation tasks in a simulated environment [115].\nHowever, applying the multi-goal RL framework to social robots would be a fruitful area for future work.\nAnother interesting future direction might be the application of multi-objective RL in social robotics. The task\nefficiency and user satisfaction can be two objectives where the robot would try to maximize both objectives by\nformalizing the problem as a multi-objective MDP. As an example, Hao et al. [116] presents a multi-objective weighted\n32\nNeziha Akalin and Amy Loutfi\nRL in which the agent had two objectives: minimizing the cost of service execution and eliminating the user’s negative\nemotions. We refer the interested reader to the survey on multi-objective decision making for a more detailed explanation\nof the topic [117].\nRecent developments in the field of deep neural networks have led to an increasing interest in DRL. Applying DRL\nin social robotics has also received recent attention, however, studies focused on small sets of actions and single task\nscenarios. In this regard, social robots with larger sets of actions would be a promising area for further work. Another\nfuture direction can be a further investigation of hyper-parameters of RL in social robotics. This was briefly discussed\nin [1], as an example, in turn-based interactions relatively small discount factors (i.e., 0.7 ≤𝛾≤0.95) are more common,\nwhereas for the frame-based interactions with rather long trajectories, higher discount factors seem to be more suitable\n(i.e., 𝛾≥0.99). In deep networks, the selection of different hyper-parameters affects the accuracy of the algorithm [118].\nThis also applies to DRL, Lathuilière et al. [86] presented several experiments to evaluate the impact of some of the\nprincipal parameters of their deep network structure.\nThus far, model-free RL learning a value function or a policy through trial and error is the most commonly used\napproach in social robotics. However, model-based RL that focuses on learning a transition model of the environment\nserving as a simulation remains to be further explored. In particular, having a user model can ease the learning process.\nAlthough it is difficult to model human reactions, having a model can play a crucial role in reducing the number\nof required interactions in the real-world. The model-based approach can also help with the problem of hardware\ndepreciation which may arise in model-free RL in robotics because of the considerable amount of interaction time.\nSimulating the interaction environment can ease the training without manual interventions and a need for maintenance.\nNonetheless, transferring the learned policies in simulation directly to the physical robot may not be trivial due to\nundermodeling and uncertainty about system dynamics [15]. A common limitation is that most of the works are not\ngeneralizable, i.e., utilizing the knowledge learned by one robot on the other or utilizing the task knowledge for other\ntasks. The Google AI team trained a model-based Deep Planning Network (PlaNet) agent which achieved six different\ntasks (i.e., cartpole swing-up, cartpole balance, finger spin, cheetah run, etc.) [119]. A similar approach for a physical\nsocial robot would be an interesting future direction.\nRL problems are formalized as MDPs in fully observable environments. However, in the case of HRI not all of the\nrequired observations are available, due to the underlying effect of psychological states on human behavior. It has been\ndemonstrated that POMDPs are able to model the uncertainties and inherent interaction ambiguities in real-world\nHRI scenarios [120]. Hausknecht and Stone [121] proposed a method that couples a Long Short Term Memory with a\nDeep Q-Network to handle the noisy observations characteristic of POMDPs. A similar approach would be useful in\nsocial robotics problems to better capture the dynamics of the environment. We included two examples of POMDP\napproaches in social robotics, [50, 102]. Further investigation would constitute an interesting line of research.\n8\nCONCLUSION\nIn this work, we give an overview of the work on RL in social robotics. We surveyed the literature and presented a\nthorough analysis of RL approaches in social robotics. Social robots have two important characteristics: physical\nembodiment and interaction/communication capabilities. Therefore, we included studies with physically embodied\nrobots. Moreover, we categorize the papers based on the used RL type. In this categorization, we discuss and group the\npapers based on the communication medium used for reward formulation. Considering the importance of designing\nthe reward function, we also categorize the papers based on the nature of the reward. The evaluation methods of the\npapers are also grouped by whether or not they use subjective and algorithmic metrics. We then provide a discussion\nReinforcement Learning Approaches in Social Robotics\n33\nin the view of real-world RL challenges and proposed solutions. The points that remain to be explored, including the\napproaches that have thus far received less attention are also given in the discussion section. To conclude, despite\ntremendous leaps in computing power and advances in learning methods, we are still a long way from general-purpose,\nrobust, and versatile social robots that can learn several skills from naive users with real-world interactions. In spite of\nthe immediate challenges, we see steady progress of RL applications in social robotics with an increasing interest in\nrecent years.\nAUTHOR CONTRIBUTIONS\nN.A. was the main author responsible for conducting literature research, methodology definition, and paper writing.\nA.L. supervised the study, and has been involved in structuring and writing the paper. All authors have read and agreed\nto the published version of the manuscript.\nFUNDING\nThis research was funded by European Union’s Horizon 2020 research and innovation program under the Marie\nSkłodowska-Curie grant agreement No 721619 for the SOCRATES project.\nREFERENCES\n[1] Keizer, S.; Ellen Foster, M.; Wang, Z.; Lemon, O. Machine Learning for Social Multiparty Human–Robot Interaction. ACM Trans. Interact. Intell.\nSyst. 2014, 4, 14:1–14:32, doi:10.1145/2600021.\n[2] de Greeff, J.; Belpaeme, T. Why robots should be social: Enhancing machine learning through social human-robot interaction. PLoS ONE 2015,\n10, e0138061.\n[3] Hemminghaus, J.; Kopp, S. Towards Adaptive Social Behavior Generation for Assistive Robots Using Reinforcement Learning. In Proceed-\nings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction (HRI 2017), Vienna, Austria, 6–9 March 2017; pp. 332–340,\ndoi:10.1145/2909824.3020217.\n[4] Ritschel, H.; Seiderer, A.; Janowski, K.; Wagner, S.; André, E. Adaptive linguistic style for an assistive robotic health companion based on explicit\nhuman feedback. In Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments, Rhodes,\nGreece, 5–7 June 2019; pp. 247–255.\n[5] Sutton, R.S.; Barto, A.G. Introduction to Reinforcement Learning; MIT Press: Cambridge, UK, 1998; Volume 2.\n[6] Barto, A.G.; Sutton, R.S.; Watkins, C. Learning and Sequential Decision Making; University of Massachusetts Amherst: Amherst, MA, USA, 1989.\n[7] Fong, T.; Nourbakhsh, I.; Dautenhahn, K. A survey of socially interactive robots. Robot. Auton. Syst. 2003, 42, 143–166.\n[8] Breazeal, C. Toward sociable robots. Robot. Auton. Syst. 2003, 42, 167–175.\n[9] Duffy, B.R. Anthropomorphism and the social robot. Robot. Auton. Syst. 2003, 42, 177–190.\n[10] Bartneck, C.; Forlizzi, J. A design-centred framework for social human-robot interaction. In Proceedings of the 13th IEEE International Workshop\non Robot and Human Interactive Communication (RO-MAN 2004), Kurashiki, Japan, 20–22 September 2004; pp. 591–594.\n[11] Hegel, F.; Muhl, C.; Wrede, B.; Hielscher-Fastabend, M.; Sagerer, G. Understanding social robots. In Proceedings of the 2009 Second International\nConferences on Advances in Computer-Human Interactions, Cancun, Mexico, 1–7 February 2009; pp. 169–174.\n[12] Yan, H.; Ang, M.H.; Poo, A.N. A survey on perception methods for human–robot interaction in social robots. Int. J. Soc. Robot. 2014, 6, 85–119.\n[13] Maroto-Gómez, M.; Castro-González, Á.; Castillo, J.; Malfaz, M.; Salichs, M. A bio-inspired motivational decision making system for social robots\nbased on the perception of the user. Sensors 2018, 18, 2691.\n[14] Sutton, R.S.; Barto, A.G. Reinforcement Learning: An Introduction; A Bradford Book: Cambridge, MA, USA, 2018.\n[15] Kober, J.; Bagnell, J.A.; Peters, J. Reinforcement learning in robotics: A survey. Int. J. Robot. Res. 2013, 32, 1238–1274.\n[16] Kormushev, P.; Calinon, S.; Caldwell, D. Reinforcement learning in robotics: Applications and real-world challenges. Robotics 2013, 2, 122–148.\n[17] Deisenroth, M.P.; Neumann, G.; Peters, J. A survey on policy search for robotics. Found. Trends® Robot. 2013, 2, 388–403.\n[18] Garcıa, J.; Fernández, F. A comprehensive survey on safe reinforcement learning. J. Mach. Learn. Res. 2015, 16, 1437–1480.\n[19] Bhagat, S.; Banerjee, H.; Ho Tse, Z.T.; Ren, H. Deep reinforcement learning for soft, flexible robots: Brief review with impending challenges.\nRobotics 2019, 8, 4.\n[20] Oudeyer, P.Y.; Kaplan, F. How can we define intrinsic motivation. In Proceedings of the 8th International Conference on Epigenetic Robotics:\nModeling Cognitive Development in Robotic Systems, Brighton, UK, 30–31 July 2008; pp. 93–101.\n[21] Thomaz, A.; Hoffman, G.; Cakmak, M. Computational human-robot interaction. Found. Trends Robot. 2016, 4, 105–223.\n34\nNeziha Akalin and Amy Loutfi\n[22] Thomaz, A.L.; Breazeal, C.; Barto, A.G.; Picard, R. Socially Guided Machine Learning. 2006. Available online: https://scholarworks.umass.edu/cs_\nfaculty_pubs/183 (accessed on 2 February 2020).\n[23] Holzinger, A.; Plass, M.; Holzinger, K.; Crişan, G.C.; Pintea, C.M.; Palade, V. Towards interactive Machine Learning (iML): Applying ant colony\nalgorithms to solve the traveling salesman problem with the human-in-the-loop approach. In Proceedings of the International Conference on\nAvailability, Reliability, and Security (ARES 2016), Salzburg, Austria, 31 August–2 September 2016; pp. 81–95.\n[24] Knox, W.B.; Stone, P. Reinforcement learning from simultaneous human and MDP reward. In Proceedings of the 11th International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS ’12), Valencia, Spain, 4–8 June 2012; pp. 475–482.\n[25] Isbell, C.; Shelton, C.R.; Kearns, M.; Singh, S.; Stone, P. A social reinforcement learning agent. In Proceedings of the Fifth International Conference\non Autonomous Agents, (AGENTS ’01), Montreal, QC, Canada, 28 May–1 June 2001; pp. 377–384.\n[26] Suay, H.B.; Chernova, S. Effect of human guidance and state space size on interactive reinforcement learning. In Proceedings of the 20th IEEE\nInternational Workshop on Robot and Human Communication (RO-MAN 2011), Atlanta, GA, USA, 31 July–3 August 2011; pp. 1–6.\n[27] Thomaz, A.L.; Hoffman, G.; Breazeal, C. Reinforcement learning with human teachers: Understanding how people want to teach robots. In\nProceedings of the 15th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN 2006), Hatfield, UK, 6–8\nSeptember 2006; pp. 352–357.\n[28] Thomaz, A.L.; Breazeal, C. Experiments in socially guided exploration: Lessons learned in building robots that learn with and without human\nteachers. Connect. Sci. 2008, 20, 91–110, doi:10.1080/09540090802091917.\n[29] Knox, W.B.; Stone, P.; Breazeal, C. Training a Robot via Human Feedback: A Case Study. In Social Robotics; Herrmann, G., Pearson, M.J., Lenz, A.,\nBremner, P., Spiers, A., Leonards, U., Eds.; Springer International Publishing: Cham, Switzerland, 2013; pp. 460–470.\n[30] Suay, H.B.; Toris, R.; Chernova, S. A Practical Comparison of Three Robot Learning from Demonstration Algorithm. Int. J. Soc. Robot. 2012,\n4, 319–330, doi:10.1007/s12369-012-0158-7.\n[31] Thomaz, A.L.; Breazeal, C. Asymmetric Interpretations of Positive and Negative Human Feedback for a Social Learning Agent. In Proceedings\nof the 16th IEEE International Symposium Robot and Human Interactive Communication (RO-MAN 2007), Jeju, Korea, 26–29 August 2007; pp.\n720–725, doi:10.1109/ROMAN.2007.4415180.\n[32] Patompak, P.; Jeong, S.; Nilkhamhang, I.; Chong, N.Y. Learning Proxemics for Personalized Human–Robot Social Interaction. Int. J. Soc. Robot.\n2019, 12, 267–280, doi:10.1007/s12369-019-00560-9\n[33] Chan, J.; Nejat, G. Social Intelligence for a Robot Engaging People in Cognitive Training Activities. Int. J. Adv. Robot. Syst. 2012, 9, doi:10.5772/51171.\n[34] Pérula-Martínez, R.; Castro-Gonzalez, A.; Malfaz, M.; Alonso-Martín, F.; Salichs, M.A. Bioinspired decision-making for a socially interactive robot.\nCogn. Syst. Res. 2019, 54, 287–301.\n[35] Cuayáhuitl, H. A data-efficient deep learning approach for deployable multimodal social robots. Neurocomputing 2019, 396, 587–598.\n[36] Qureshi, A.H.; Nakamura, Y.; Yoshikawa, Y.; Ishiguro, H. Robot gains social intelligence through multimodal deep reinforcement learning. In\nProceedings of the 16th IEEE-RAS International Conference on Humanoid Robots, Humanoids, Cancun, Mexico, 15–17 November 2016; pp. 745–751,\ndoi:10.1109/HUMANOIDS.2016.7803357.\n[37] Zhang, H.; Yu, T. Taxonomy of Reinforcement Learning Algorithms. In Deep Reinforcement Learning: Fundamentals, Research and Applications;\nDong, H., Ding, Z., Zhang, S., Eds.; Springer: Singapore, 2020; pp. 125–133, doi:10.1007/978-981-15-4095-0_3.\n[38] Bellman, R. On the theory of dynamic programming. Proc. Natl. Acad. Sci. USA 1952, 38, 716.\n[39] Rummery, G.A.; Niranjan, M. On-line Q-Learning Using Connectionist Systems; University of Cambridge, Department of Engineering: Cambridge,\nUK, 1994; Volume 37.\n[40] Watkins, C.J.C.H. Learning from Delayed Rewards. 1989. Available online: http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf (accessed on 7\nDecember 2019).\n[41] Gosavi, A. Boundedness of iterates in Q-learning. Syst. Control Lett. 2006, 55, 347–349.\n[42] Sigaud, O.; Stulp, F. Policy search in continuous action domains: An overview. Neural Netw. 2019, 113, 28–40.\n[43] Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.; Fidjeland, A.K.; Ostrovski, G.; et al.\nHuman-level control through deep reinforcement learning. Nature 2015, 518, 529–533.\n[44] Schneider, S.; Kummert, F. Exploring embodiment and dueling bandit learning for preference adaptation in human-robot interaction. In Proceedings\nof the 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN 2017), Lisbon, Portugal, 28 August–1\nSeptember 2017; pp. 1325–1331.\n[45] Leite, I.; Pereira, A.; Castellano, G.; Mascarenhas, S.; Martinho, C.; Paiva, A. Modelling empathy in social robotic companions. In Proceedings of the\nInternational Conference on User Modeling, Adaptation, and Personalization (UMAP 2011), Girona, Spain, 11–15 July 2011; pp. 135–147.\n[46] Ritschel, H.; Seiderer, A.; Janowski, K.; Aslan, I.; André, E. Drink-o-mender: An adaptive robotic drink adviser. In Proceedings of the 3rd International\nWorkshop on Multisensory Approaches to Human-Food Interaction, Boulder, CO, USA, 16–20 October 2018; pp. 1–8.\n[47] Gao, Y.; Barendregt, W.; Obaid, M.; Castellano, G. When robot personalisation does not help: Insights from a robot-supported learning study. In\nProceedings of the 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN 2018), Nanjing, China, 27–31\nAugust 2018; pp. 705–712.\n[48] Bubeck, S.; Cesa-Bianchi, N. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Found. Trends® Mach. Learn. 2012,\n5, 1–122, doi:10.1561/2200000024.\nReinforcement Learning Approaches in Social Robotics\n35\n[49] Tseng, S.H.; Liu, F.C.; Fu, L.C. Active Learning on Service Providing Model: Adjustment of Robot Behaviors Through Human Feedback. IEEE Trans.\nCogn. Dev. Syst. 2018, 10, 701–711, doi:10.1109/TCDS.2017.2775621.\n[50] Martins, G.S.; Al Tair, H.; Santos, L.; Dias, J. 𝛼POMDP: POMDP-based user-adaptive decision-making for social robots. Pattern Recognit. Lett. 2019,\n118, 94–103.\n[51] Wiewiora, E. Reward Shaping. In Encyclopedia of Machine Learning; Sammut, C., Webb, G.I., Eds.; Springer: Boston, MA, USA, 2010; pp. 863–865.\n[52] Barraquand, R.; Crowley, J.L. Learning Polite Behavior with Situation Models. In Proceedings of the 3rd ACM/IEEE International Conference on\nHuman Robot Interaction (HRI 2008), Amsterdam, The Netherlands, 12–15 March 2008; pp. 209–216, doi:10.1145/1349822.1349850.\n[53] Yang, C.; Lu, M.; Tseng, S.; Fu, L. A companion robot for daily care of elders based on homeostasis. In Proceedings of the 56th Annual\nConference of the Society of Instrument and Control Engineers of Japan (SICE 2017), Kanazawa, Japan, 19–22 September 2017; pp. 1401–1406,\ndoi:10.23919/SICE.2017.8105748.\n[54] Zarinbal, M.; Mohebi, A.; Mosalli, H.; Haratinik, R.; Jabalameli, Z.; Bayatmakou, F. A New Social Robot for Interactive Query-Based Summarization:\nScientific Document Summarization. In Proceedings of the International Conference on Interactive Collaborative Robotics (ICR 2019), Istanbul,\nTurkey, 20–25 August 2019; pp. 330–340.\n[55] Addo, I.D.; Ahamed, S.I. Applying affective feedback to reinforcement learning in ZOEI, a comic humanoid robot. In Proceedings of the 23rd IEEE\nInternational Symposium on Robot and Human Interactive Communication (RO-MAN 2014), Edinburgh, UK, 25–29 August 2014; pp. 423–428.\n[56] Chiang, Y.S.; Chu, T.S.; Lim, C.; Wu, T.Y.; Tseng, S.H.; Fu, L.C. Personalizing robot behavior for interruption in social human-robot interaction.\nIn Proceedings of the 2014 IEEE International Workshop on Advanced Robotics and its Social Impacts (ARSO 2014), Evanston, IL, USA, 11–13\nSeptember 2014; pp. 44–49, doi:10.1109/ARSO.2014.7020978.\n[57] Ritschel, H.; Baur, T.; André, E. Adapting a Robot’s linguistic style based on socially-aware reinforcement learning. In Proceedings of the 26th IEEE\nInternational Symposium on Robot and Human Interactive Communication (RO-MAN 2017), Lisbon, Portugal, 28 August–1 September 2017; pp.\n378–384, doi:10.1109/ROMAN.2017.8172330.\n[58] Park, H.W.; Grover, I.; Spaulding, S.; Gomez, L.; Breazeal, C. A model-free affective reinforcement learning approach to personalization of an\nautonomous social robot companion for early literacy education. In Proceedings of the AAAI Conference on Artificial Intelligence, Honolulu, HI,\nUSA, 27 January–1 February 2019; Volume 33, pp. 687–694.\n[59] Weber, K.; Ritschel, H.; Aslan, I.; Lingenfelser, F.; André, E. How to Shape the Humor of a Robot - Social Behavior Adaptation Based on Reinforcement\nLearning. In Proceedings of the International Conference on Multimodal Interaction, ICMI 2018, Boulder, CO, USA, 16–20 October 2018; pp.\n154–162, doi:10.1145/3242969.3242976.\n[60] Papaioannou, I.; Dondrup, C.; Novikova, J.; Lemon, O. Hybrid chat and task dialogue for more engaging hri using reinforcement learning. In\nProceedings of the 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN 2017), Lisbon, Portugal, 28\nAugust–1 September 2017; pp. 593–598.\n[61] Moro, C.; Nejat, G.; Mihailidis, A. Learning and Personalizing Socially Assistive Robot Behaviors to Aid with Activities of Daily Living. ACM Trans.\nHum. Robot. Interact. 2018, 7, doi:10.1145/3277903.\n[62] Nejat, G.; Ficocelli, M. Can I be of assistance? The intelligence behind an assistive robot. In Proceedings of the 2008 IEEE International Conference\non Robotics and Automation (ICRA 2008), Pasadena, CA, USA, 19–23 May 2008; pp. 3564–3569.\n[63] Da Silva, R.R.; Francelin Romero, R.A. Modelling Shared Attention Through Relational Reinforcement Learning. J. Intell. Robot. Syst. 2012,\n66, 167–182, doi:10.1007/s10846-011-9624-y.\n[64] Castro-González, Á.; Malfaz, M.; Gorostiza, J.F.; Salichs, M.A. Learning behaviors by an autonomous social robot with motivations. Cybern. Syst.\n2014, 45, 568–598.\n[65] Castro-González, Á.; Malfaz, M.; Salichs, M.A. An autonomous social robot in fear. IEEE Trans. Auton. Ment. Dev. 2013, 5, 135–151.\n[66] Castro-González, Á.; Malfaz, M.; Salichs, M.A. Learning the Selection of Actions for an Autonomous Social Robot by Reinforcement Learning\nBased on Motivations. Int. J. Soc. Robot. 2011, 3, 427–441, doi:10.1007/s12369-011-0113-z.\n[67] Chen, L.; Wu, M.; Zhou, M.; She, J.; Dong, F.; Hirota, K. Information-Driven Multirobot Behavior Adaptation to Emotional Intention in Human–Robot\nInteraction. IEEE Trans. Cogn. Dev. Syst. 2018, 10, 647–658, doi:10.1109/TCDS.2017.2728003.\n[68] Gordon, G.; Spaulding, S.; Westlund, J.K.; Lee, J.J.; Plummer, L.; Martinez, M.; Das, M.; Breazeal, C. Affective personalization of a social robot tutor\nfor children’s second language skills. In Proceedings of the 30th AAAI Conference on Artificial Intelligence, Phoenix, AZ, USA, 12–17 February 2016.\n[69] Gamborino, E.; Fu, L.C. Interactive Reinforcement Learning based Assistive Robot for the Emotional Support of Children. In Proceedings of the\n18th International Conference on Control Automation and Systems (ICCAS 2018), Pyeong Chang, Korea, 17–20 October 2018; pp. 708–713.\n[70] Ranatunga, I.; Rajruangrabin, J.; Popa, D.O.; Makedon, F. Enhanced Therapeutic Interactivity Using Social Robot Zeno. In Proceedings of the 4th\nInternational Conference on PErvasive Technologies Related to Assistive Environments (PETRA 2011), Crete, Greece, 25–27 May 2011; pp. 1–6,\ndoi:10.1145/2141622.2141690.\n[71] Chan, J.; Nejat, G. Minimizing task-induced stress in cognitively stimulating activities using an intelligent socially assistive robot. In Proceedings\nof the 20th International Symposium on Robot and Human Interactive Communication (RO-MAN 2011), Atlanta, GA, USA, 31 July–3 August 2011;\npp. 296–301.\n[72] Chan, J.; Nejat, G. A learning-based control architecture for an assistive robot providing social engagement during cognitively stimulating activities.\nIn Proceedings of the 2011 IEEE International Conference on Robotics and Automation, Shanghai, China, 9–13 May 2011; pp. 3928–3933.\n36\nNeziha Akalin and Amy Loutfi\n[73] Qureshi, A.H.; Nakamura, Y.; Yoshikawa, Y.; Ishiguro, H. Show, attend and interact: Perceivable human-robot social interaction through neural\nattention Q-network. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA 2017), Singapore, Singapore, 29\nMay–3 June 2017; pp. 1639–1645.\n[74] Qureshi, A.; Nakamura, Y.; Yoshikawa, Y.; Ishiguro, H. Intrinsically motivated reinforcement learning for human–robot interaction in the real-world.\nNeural Netw. 2018, 107, 23–33, doi:10.1016/j.neunet.2018.03.014.\n[75] Thomaz, A.; Breazeal, C. Adding guidance to interactive reinforcement learning. In Proceedings of the 20th Conference on Artificial Intelligence\n(AAAI 2006), Boston, MA, USA, 16–20 July 2006.\n[76] Loftin, R.; Peng, B.; MacGlashan, J.; Littman, M.L.; Taylor, M.E.; Huang, J.; Roberts, D.L. Learning behaviors via human-delivered discrete feedback:\nModeling implicit feedback strategies to speed up learning. Auton. Agents Multi Agent Syst. 2016, 30, 30–59.\n[77] Wagner, J.; Lingenfelser, F.; Baur, T.; Damian, I.; Kistler, F.; André, E. The social signal interpretation (SSI) framework: Multimodal signal processing\nand recognition in real-time. In Proceedings of the 21st ACM International Conference on Multimedia, Barcelona, Spain, 21–25 October 2013;\npp. 831–834.\n[78] McDuff, D.; Mahmoud, A.; Mavadati, M.; Amr, M.; Turcot, J.; Kaliouby, R.E. AFFDEX SDK: A cross-platform real-time multi-face expression\nrecognition toolkit. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems, San Jose, CA, USA,\n7–12 May 2016; pp. 3723–3726.\n[79] Chen, L.F.; Liu, Z.T.; Dong, F.Y.; Yamazaki, Y.; Wu, M.; Hirota, K. Adapting multi-robot behavior to communication atmosphere in humans-robots\ninteraction using fuzzy production rule based friend-Q learning. J. Adv. Comput. Intell. Intell. Inform. 2013, 17, 291–301.\n[80] Schwartz, A. A Reinforcement Learning Method for Maximizing Undiscounted Rewards. In Proceedings of the 10th International Conference on\nMachine Learning (ICML 1993), Amherst, MA, USA, 27–29 June 1993; Volume 298, pp. 298–305.\n[81] Mahadevan, S. Average reward reinforcement learning: Foundations, algorithms, and empirical results. Mach. Learn. 1996, 22, 159–195.\n[82] Mavridis, N. A review of verbal and non-verbal human–robot interactive communication. Robot. Auton. Syst. 2015, 63, 22–35.\n[83] Da Silva, R.R.; Policastro, C.A.; Romero, R.A. Relational reinforcement learning applied to shared attention. In Proceedings of the 2009 International\nJoint Conference on Neural Networks (IJCNN 2009), Atlanta, GA, USA, 14–19 June 2009; pp. 2943–2949.\n[84] Dietterich, T.G. Hierarchical reinforcement learning with the MAXQ value function decomposition. J. Artif. Intell. Res. 2000, 13, 227–303.\n[85] Lathuilière, S.; Massé, B.; Mesejo, P.; Horaud, R. Deep Reinforcement Learning for Audio-Visual Gaze Control. In Proceedings of the IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS 2018), Madrid, Spain, 1–5 October 2018; pp. 1555–1562.\n[86] Lathuilière, S.; Massé, B.; Mesejo, P.; Horaud, R. Neural network based reinforcement learning for audio–visual gaze control in human–robot\ninteraction. Pattern Recognit. Lett. 2019, 118, 61–71, doi:10.1016/j.patrec.2018.05.023.\n[87] Churamani, N.; Barros, P.; Strahl, E.; Wermter, S. Learning Empathy-Driven Emotion Expressions using Affective Modulations. In Proceedings of the\nInternational Joint Conference on Neural Networks (IJCNN 2018), Rio de Janeiro, Brazil, 8–13 July 2018; pp. 1–8, doi:10.1109/IJCNN.2018.8489158.\n[88] Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudinov, R.; Zemel, R.; Bengio, Y. Show, attend and tell: Neural image caption generation with\nvisual attention. In Proceedings of the International Conference on Machine Learning (ICML 2015), Lille, France, 6–11 July 2015; pp. 2048–2057.\n[89] Breazeal, C. Role of expressive behaviour for robots that learn from people. Philos. Trans. R. Soc. B Biol. Sci. 2009, 364, 3527–3538.\n[90] Mitsunaga, N.; Smith, C.; Kanda, T.; Ishiguro, H.; Hagita, N. Robot behavior adaptation for human-robot interaction based on policy gradient\nreinforcement learning. J. Robot. Soc. Jpn. 2006, 24, 820–829.\n[91] Mitsunaga, N.; Smith, C.; Kanda, T.; Ishiguro, H.; Hagita, N. Adapting robot behavior for human–robot interaction. IEEE Trans. Robot. 2008,\n24, 911–916.\n[92] Tapus, A.; Ţăpuş, C.; Matarić, M.J. User—Robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy.\nIntell. Serv. Robot. 2008, 1, 169, doi:10.1007/s11370-008-0017-4.\n[93] Knox, W.B.; Stone, P. Interactively shaping agents via human reinforcement: The TAMER framework. In Proceedings of the 5th International\nConference on Knowledge Capture, Redondo Beach, CA, USA, 1–4 September 2009; pp. 9–16.\n[94] Celemin, C.; Ruiz-del Solar, J. An interactive framework for learning continuous actions policies based on corrective feedback. J. Intell. Robot. Syst.\n2019, 95, 77–97.\n[95] Thomaz, A.L.; Breazeal, C. Teachable robots: Understanding human teaching behavior to build more effective robot learners. Artif. Intell. 2008,\n172, 716–737.\n[96] Li, G.; Gomez, R.; Nakamura, K.; He, B. Human-centered reinforcement learning: A survey. IEEE Trans. Hum. Mach. Syst. 2019, 49, 337–349.\n[97] Knox, W.B.; Stone, P. Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance.\nArtif. Intell. 2015, 225, 24–50.\n[98] Thomaz, A.L.; Breazeal, C. Reinforcement Learning with Human Teachers: Evidence of Feedback and Guidance with Implications for Learning\nPerformance. In Proceedings of the 21st National Conference on Artificial intelligence (AAAI 2006), Boston, MA, USA, 16—20 July 2006; Volume 6,\npp. 1000–1005.\n[99] Schmidt, A. Implicit human computer interaction through context. Pers. Technol. 2000, 4, 191–199.\n[100] Grüneberg, P.; Suzuki, K. A lesson from subjective computing: Autonomous self-referentiality and social interaction as conditions for subjectivity.\nIn Proceedings of the AISB/IACAP World Congress 2012: Computational Philosophy, Part of Alan Turing Year, Birmingham, UK, 2–6 July 2012; pp.\n18–28.\nReinforcement Learning Approaches in Social Robotics\n37\n[101] Grüneberg, P.; Suzuki, K. An approach to subjective computing: A robot that learns from interaction with humans. IEEE Trans. Auton. Ment. Dev.\n2013, 6, 5–18.\n[102] Ramachandran, A.; Sebo, S.S.; Scassellati, B. Personalized Robot Tutoring using the Assistive Tutor POMDP (AT-POMDP). In Proceedings of the\n33rd AAAI Conference on Artificial Intelligence (AAAI 2019), Honolulu, HI, USA, 27 January–1 February 2019; pp. 8050–8057.\n[103] Lerner, J.S.; Li, Y.; Valdesolo, P.; Kassam, K.S. Emotion and decision making. Annu. Rev. Psychol. 2015, 66, 799–823.\n[104] Moerland, T.M.; Broekens, J.; Jonker, C.M. Emotion in reinforcement learning agents and robots: A survey. Mach. Learn. 2018, 107, 443–480.\n[105] Ryan, R.M.; Deci, E.L. Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemp. Educ. Psychol. 2000, 25, 54–67.\n[106] Oudeyer, P.Y.; Kaplan, F. What is intrinsic motivation? A typology of computational approaches. Front. Neurorobotics 2009, 1, 6.\n[107] Chentanez, N.; Barto, A.G.; Singh, S.P. Intrinsically motivated reinforcement learning. In Proceedings of the Advances in Neural Information\nProcessing Systems (NIPS 2004), Vancouver, BC, Canada, 13–18 December 2004; pp. 1281–1288.\n[108] Malfaz, M.; Castro-González, Á.; Barber, R.; Salichs, M.A. A biologically inspired architecture for an autonomous and social robot. IEEE Trans.\nAuton. Ment. Dev. 2011, 3, 232–246.\n[109] Cannon, W.B. The Wisdom of the Body; W.W. Norton & Company, Inc.: New York, NY, USA, 1939.\n[110] Berridge, K.C. Motivation concepts in behavioral neuroscience. Physiol. Behav. 2004, 81, 179–209.\n[111] Sim, D.Y.Y.; Loo, C.K. Extensive assessment and evaluation methodologies on assistive social robots for modelling human–robot interaction–A\nreview. Inf. Sci. 2015, 301, 305–344.\n[112] Dulac-Arnold, G.; Mankowitz, D.; Hester, T. Challenges of real-world reinforcement learning. arXiv 2019, arXiv:1904.12901.\n[113] Lockerd, A.; Breazeal, C. Tutelage and socially guided robot learning. In Proceedings of the IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS 2004), Sendai, Japan, 28 September–2 October 2004; Volume 4, pp. 3475–3480.\n[114] Liu, L.; Li, B.; Chen, I.M.; Goh, T.J.; Sung, M. Interactive robots as social partner for communication care. In Proceedings of the IEEE International\nConference on Robotics and Automation (ICRA 2014), Hong Kong, China, 31 May–7 June 2014; pp. 2231–2236.\n[115] Bai, C.; Liu, P.; Zhao, W.; Tang, X. Guided goal generation for hindsight multi-goal reinforcement learning. Neurocomputing 2019, 359, 353–367.\n[116] Hao, M.; Cao, W.; Liu, Z.; Wu, M.; Yuan, Y. Emotion Regulation Based on Multi-objective Weighted Reinforcement Learning for Human-robot\nInteraction. In Proceedings of the 12th Asian Control Conference (ASCC 2019), Kitakyushu-shi, Japan, 9–12 June 2019; pp. 1402–1406.\n[117] Roijers, D.M.; Vamplew, P.; Whiteson, S.; Dazeley, R. A Survey of Multi-objective Sequential Decision-making. J. Artif. Intell. Res. 2013, 48, 67–113.\n[118] Zhang, X.; Yao, L.; Huang, C.; Sheng, Q.Z.; Wang, X. Intent Recognition in Smart Living through Deep Recurrent Neural Networks. In Proceedings\nof the International Conference on Neural Information Processing, Guangzhou, China, 14–18 November 2017; pp. 748–758.\n[119] Hafner, D.; Lillicrap, T.; Fischer, I.; Villegas, R.; Ha, D.; Lee, H.; Davidson, J. Learning Latent Dynamics for Planning from Pixels. In Proceedings of\nthe 36th International Conference on Machine Learning (ICML 2019), Long Beach, CA, USA, 9–15 June 2019; pp. 2555–2565.\n[120] Kostavelis, I.; Giakoumis, D.; Malassiotis, S.; Tzovaras, D. A POMDP Design Framework for Decision Making in Assistive Robots. In Proceedings of\nthe International Conference on Human-Computer Interaction (HCI 2017), Vancouver, BC, Canada, 9–14 July 2017; pp. 467–479.\n[121] Hausknecht, M.; Stone, P. Deep Recurrent Q-Learning for Partially Observable MDPs. In Proceedings of the 2015 AAAI Fall Symposium on\nSequential Decision Making for Intelligent Agents (AAAI-SDMIA15), Arlington, VA, USA, 12–14 November 2015.\n",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "published": "2020-09-21",
  "updated": "2021-02-11"
}