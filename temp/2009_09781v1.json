{
  "id": "http://arxiv.org/abs/2009.09781v1",
  "title": "Rethinking Supervised Learning and Reinforcement Learning in Task-Oriented Dialogue Systems",
  "authors": [
    "Ziming Li",
    "Julia Kiseleva",
    "Maarten de Rijke"
  ],
  "abstract": "Dialogue policy learning for task-oriented dialogue systems has enjoyed great\nprogress recently mostly through employing reinforcement learning methods.\nHowever, these approaches have become very sophisticated. It is time to\nre-evaluate it. Are we really making progress developing dialogue agents only\nbased on reinforcement learning? We demonstrate how (1)~traditional supervised\nlearning together with (2)~a simulator-free adversarial learning method can be\nused to achieve performance comparable to state-of-the-art RL-based methods.\nFirst, we introduce a simple dialogue action decoder to predict the appropriate\nactions. Then, the traditional multi-label classification solution for dialogue\npolicy learning is extended by adding dense layers to improve the dialogue\nagent performance. Finally, we employ the Gumbel-Softmax estimator to\nalternatively train the dialogue agent and the dialogue reward model without\nusing reinforcement learning. Based on our extensive experimentation, we can\nconclude the proposed methods can achieve more stable and higher performance\nwith fewer efforts, such as the domain knowledge required to design a user\nsimulator and the intractable parameter tuning in reinforcement learning. Our\nmain goal is not to beat reinforcement learning with supervised learning, but\nto demonstrate the value of rethinking the role of reinforcement learning and\nsupervised learning in optimizing task-oriented dialogue systems.",
  "text": "Rethinking Supervised Learning and Reinforcement Learning\nin Task-Oriented Dialogue Systems\nZiming Li 1, Julia Kiseleva 2, Maarten de Rijke 1\n1University of Amsterdam, 2Microsoft Research\n{z.li,m.derijke@@uva.nl}, julia.kiseleva@microsoft.com\nAbstract\nDialogue policy learning for Task-oriented\nDialogue Systems (TDSs) has enjoyed great\nprogress recently mostly through employing\nReinforcement Learning (RL) methods. How-\never, these approaches have become very so-\nphisticated. It is time to re-evaluate it. Are\nwe really making progress developing dia-\nlogue agents only based on RL? We demon-\nstrate how (1) traditional supervised learning\ntogether with (2) a simulator-free adversarial\nlearning method can be used to achieve perfor-\nmance comparable to state-of-the-art (SOTA)\nRL-based methods. First, we introduce a sim-\nple dialogue action decoder to predict the ap-\npropriate actions. Then, the traditional multi-\nlabel classiﬁcation solution for dialogue pol-\nicy learning is extended by adding dense lay-\ners to improve the dialogue agent performance.\nFinally, we employ the Gumbel-Softmax esti-\nmator to alternatively train the dialogue agent\nand the dialogue reward model without using\nRL. Based on our extensive experimentation,\nwe can conclude the proposed methods can\nachieve more stable and higher performance\nwith fewer efforts, such as the domain knowl-\nedge required to design a user simulator and\nthe intractable parameter tuning in reinforce-\nment learning. Our main goal is not to beat RL\nwith supervised learning, but to demonstrate\nthe value of rethinking the role of RL and su-\npervised learning in optimizing TDSs.\n1\nIntroduction\nThe aim of dialogue policies in Task-oriented Dia-\nlogue System (TDS) is to select appropriate actions\nat each time step according to the current context\nof the conversation and user feedback (Chen et al.,\n2017). In early work, dialogue policies were manu-\nally designed as a set of rules that map the dialogue\ncontext to a corresponding system action (Weizen-\nbaum, 1966). The ability of rule-based solutions is\nlimited by the domain complexity and task scalabil-\nity. Moreover, the design and maintenance of these\nrules require a lot of effort and domain knowledge.\nDue to recent advantages in deep learning and\nthe availability of labeled conversational datasets,\nsupervised learning can be employed for dialogue\npolicy training to overcome the disadvantages of\nrule-based systems. The downside of the super-\nvised learning approach is that the dialogues ob-\nserved in the datasets are unlikely to represent all\npossible conversation scenarios; in some extreme\ncases, the required conversational dataset cannot\nbe collected or acquiring it might cost-prohibitive.\nThe success of RL in other areas holds promises\nfor dialogue Policy Learning (PL) (Williams and\nYoung, 2007). Using RL techniques, we can train\ndialogue policies and optimize automatically, from\nscratch and utilizing interactions with users (Gaˇsi´c\nand Young, 2014; Su et al., 2017). In RL-based\nsolutions, the dialogue system takes actions that\nare controlled by the dialogue policy, and user feed-\nback (the reward signal), which is provided when\nthe dialogue is ﬁnished, is utilized to adjust the\ninitial policy (Peng et al., 2018b; Williams et al.,\n2017; Dhingra et al., 2016). In practice, reward sig-\nnals are not always available and may be inconsis-\ntent (Su et al., 2016). As it is not practical to ask for\nexplicit user feedback for each dialogue during pol-\nicy training, different strategies have been proposed\nto design a rule-based user simulator along with\na reward function that can approximate the real\nreward function which exists only in each user’s\nmind. Designing an appropriate user simulator and\naccurate reward function requires strong domain\nknowledge. This process has the same disadvan-\ntages as rule-based dialog systems (Walker et al.,\n1997). The difference is that rule-based approaches\nto system design meet this problem at the dialogue\nagent side while rule-based user simulators need to\nsolve it at the environment side.\nIf the task is simple and easy to solve, why not\narXiv:2009.09781v1  [cs.CL]  21 Sep 2020\njust build a rule-based system rather than a user-\nsimulator that is then used with RL techniques to\ntrain the dialogue system, where more uncontrol-\nlable factors are involved? And if the task domain\nis complex and hard to solve, is it easier to design\nand maintain a complicated rule-based user sim-\nulator than to build a rule-based dialogue agent?\nSupervised learning methods do not suffer from\nthese issues but require labeled conversational data;\nin some exceptional cases, if the data cannot be\ncollected for privacy reasons, RL is the solution.\nHowever, collecting labeled data is feasible for\nmany applications (Williams et al., 2014; Weston\net al., 2015; Budzianowski et al., 2018). Therefore\nin this work seek to answer the following research\nquestion: Are we really making progress in TDSs\nfocusing purely on advancing RL-based methods?\nTo address this question, we introduce three di-\nalogue PL methods which do not require a user\nsimulator. The proposed methods can achieve com-\nparable or even higher performance compared to\nSOTA RL methods. The ﬁrst method utilizes an ac-\ntion decoder to predict dialogue combinations. The\nsecond method regards the dialogue PL task as a\nmulti-label classiﬁcation problem. Unlike previous\nwork, we assign a dense layer to each action label\nin the action space. Based on the second method,\nwe propose an adversarial learning method for di-\nalogue PL without utilizing RL. To backpropa-\ngate the loss from the reward model to the policy\nmodel, we utilize the Gumbel-Softmax to connect\nthe policy model and the reward model in our third\nmethod. We compare our methods with RL and\nadversarial RL based dialogue training solutions to\nshow how we can achieve comparable performance\nwithout a utilizing costly user simulator.\nTo summarize, our contributions are:\n• A dialogue action decoder to learn the dialogue\npolicy with supervised learning.\n• A multi-label classiﬁcation solution to learn the\ndialogue policy.\n• A simulation-free adversarial learning method to\nimprove the performance of dialogue agents.\n• Achieving SOTA performance in dialogue PL\nwith fewer efforts and costs compare to existing\nRL-based solutions.\n2\nRelated Work\nA\nnumber\nof\nRL\nmethods,\nincluding\nQ-\nlearning (Peng et al., 2017; Lipton et al.,\n2018; Li et al., 2017; Su et al., 2018; Li et al.,\n2020) and policy gradient methods (Dhingra et al.,\n2016; Williams et al., 2017), have been applied\nto optimize dialogue policies by interacting with\nreal users or user simulators. RL methods help\nthe dialogue agent is able to explore contexts that\nmay not exist in previously observed data. A key\ncomponent in RL is the quality of the reward signal\nused to update the agent policy. Most existing\nRL-based methods require access to a reward\nsignal based on user feedback or a pre-deﬁned\none if feedback loop is not possible.\nBesides,\ndesigning a good reward function and a realistic\nuser simulator is not easy as it typically requires\nstrong domain knowledge, which is similar to\nthe problem that rule-base methods meet. Peng\net al. (2018a) propose to utilize adversarial loss\nas an extra critic in addition to the main reward\nfunction based on task completion. Inspired by the\nsuccess of adversarial training in other NLP tasks,\nLiu and Lane (2018) propose to learn dialogue\nrewards directly from dialogue samples, where a\ndialogue agent and a dialogue discriminator are\ntrained jointly. Following the success of inverse\nreinforcement learning (IRL) in different domains,\nTakanobu et al. (2019) employ adversarial IRL\nto train the dialogue agent.\nThey replace the\ndiscriminator in GAIL (Ho and Ermon, 2016)\nwith a reward function with a speciﬁc architecture.\nThe learned reward function can provide a stable\nreward signal and adversarial training can beneﬁt\nfrom high quality feedback.\nCompared to existing RL based methods, we pro-\npose strategy that can eliminate designing a user\nsimulator and sensitive parameter-tuning process\nwhile bringing a signiﬁcant performance improve-\nment with respect to a number of metrics. The\nabsence of user simulators involved will largely\nreduce the required domain knowledge and su-\npervised learning can lead to robust agent perfor-\nmance.\n3\nMulti-Domain Dialogue Agent\nDialogue State Tracker (DST) In a standard TDS\npipeline, the rule-based DST is deployed to keep\ntrack of information emerging in interactions be-\ntween users and the dialogue agent. The output\nfrom the Natural Language Understanding (NLU)\nmodule is fed to the DST to extract information, in-\ncluding informable slots about the constraints from\nusers and requestable slots that indicate what users\ninquire about. In our setup, the dialogue agents and\nSOA\na1\na2\na1\na2\nEOA\nGRU\nGRU\nGRU\nMLP\nState\nvs\nvs\nvs\nFigure 1: Architecture to approximate a dialogue pol-\nicy with an action decoder2.\nuser-simulators are interacting through predeﬁned\ndialogue actions therefore no NLU is involved. Be-\nsides, a belief vector is maintained and updated for\neach slot in every domain.\nDialogue state We formulate a structured state rep-\nresentation st according to the information result-\ning from the DST at time step t. There are 4 main\ntypes of information in the ﬁnal representation:\n(1) corresponding to the embedded results of re-\nturned entities for a query, (2) the last user action,\n(3) the last system action, and (4) the belief state\nfrom the rule-based state tracker. The ﬁnal state\nrepresentation s is a vector of 553 bits.\nDialogue action We regard the dialogue response\nproblem as a multi-label prediction task, where in\nthe same dialogue turn, several atomic dialogue\nactions can be covered and combined at the same\nmoment. In the action space, each action is a con-\ncatenation of domain name, action type and slot\nname, e.g. ‘attraction-inform-address’, which we\ncall an atomic action1. Lee et al. (2019) proposes\nthat the action space covers both the atomic action\nspace and the top-k most frequent atomic action\ncombinations in the dataset and then the dialogue\nPL task can be regarded as a single label classiﬁ-\ncation task. However, the expressive power of the\ndialogue agent is limited and it is beneﬁcial if the\nagent can learn the action structure from the data\nand this could lead to more ﬂexible and powerful\nsystem responses.\n4\nDialogue Policy Learning (PL)\n4.1\nPL as a sequential decision process\nDifferent atomic dialogue actions contained in the\nsame response are usually related to each other.\nTo fully make use of information contained in co-\noccurrence dependencies, we decompose the multi-\nlabel classiﬁcation task in dialogue PL as follows.\nAssuming the system response consists of two\natomic actions, ‘hotel-inform-address’ and ‘hotel-\n1there are 166 atomic actions in total in the action space\ninform-phone’, the model takes the dialogue state\nas input and predict the atomic actions sequentially.\nThe path could be described as either ‘hotel-inform-\naddress’ →‘hotel-inform-phone’ or ‘hotel-inform-\nphone’ →‘hotel-inform-address’. Before the train-\ning stage, the relative order of all the atomic actions\nwill be predeﬁned and ﬁxed. Following this solu-\ntion, we apply a GRU-based (Cho et al., 2014)\ndecoder to model the conditional dependency be-\ntween the actions in one single turn as shown in\nFigure 1.\nThe proposed model ﬁrst extracts state features\nvs by feeding the raw state input s to an Multilayer\nPerceptron (MLP). In the next state, the state rep-\nresentation vs will be used as the initial hidden\nstate h0 of action decoder GRU. To avoid informa-\ntion loss during decoding, the input to the action\ndecoder is:\ninputt = embedding(at−1) ⊕vs.\n(1)\nThe starting input input0 is the concatenation of\nstarting action SOA and state representation vs.\nat−1 denotes the dialogue action in the prediction\npath at time step t −1 and embedding(a) returns\nthe action embedding of the given action a. In the\nnext steps, actions will be generated consecutively\naccording to:\not, ht = GRU(inputt, ht−1),\n(2)\nwhere ot is the output of the action decoder. We use\ncross-entropy to train the action decoder together\nwith the MLP for feature extracting. We use beam-\nsearch to ﬁnd the most appropriate action path.\n4.2\nPL with adversarial learning\nNext, we introduce an adversarial learning solu-\ntion, DiaAdv, to train the dialogue policy without\na user simulator along with a dialogue discrimina-\ntor. Feedback from the discriminator is used as a\nreward signal to push the policy model to interact\nwith users in a way that is indistinguishable from\nhow a human agent completes the task. However,\nsince the output of the dialogue policy is a set of\ndiscrete dialogue actions, it is difﬁcult to pass the\ngradient update from the discriminator to the pol-\nicy model. To cross this barrier, we propose to\nutilize the Gumbel-Softmax function (Jang et al.,\n2016) to link the discriminator to the generator.\nNext, we will give a brief introduction about the\ndialogue policy model and the dialogue discrimina-\ntor. Afterwards, we will show how we can utilize\nGumbel-Softmax to backpropagate the gradient.\nDialogue policy To generate dialogue actions, we\nemploy an MLP as the action generator Gensa\nfollowed by a set of Gumbel-Softmax functions,\nwhere each function corresponds to a speciﬁc ac-\ntion in the atomic action space (Figure 2) and the\noutput of each function has two dimensions. We\nﬁrst introduce how it works when there is only one\nGumbel-Softmax function in the setting and then\nextend it to multiple function. The Gumbel-Max\ntrick (Gumbel, 1954) is commonly used to draw\nsamples u from a categorical distribution with class\nprobabilities p. The process of Genθ can be formu-\nlated as follows:\np = MLP(s)\n(3)\nu = one hot(arg max\ni\n[gi + log pi]),\n(4)\nwhere gi is independently sampled from Gumbel\n(0,1). However, the argmax operation is not differ-\nentiable, thus no gradient can be backpropagated\nthrough u. Instead, we can employ the soft-argmax\napproximation (Jang et al., 2016) as a continuous\nand differentiable approximation to argmax and\nto generate k-dimensional sample vectors below:\nyi =\nexp((log(pi) + gi)/τ)\nPk\nj=1 exp((log(pj) + gj)/τ)\n(5)\nfor i = 1, . . . , k.\nIn practice, τ should be se-\nlected to balance the approximation bias and the\nmagnitude of gradient variance. In our case, p\ncorresponds to the dialogue action status distri-\nbution p(ai\nl|s) where l ∈{0, . . . , k −1} and\ni ∈{1, . . . , m}. In our setting, k is set to 2 and\neach dimension denotes one speciﬁc action status,\nwhich could be 1 if selected or 0 if not selected. m\nis set to the size of in the action space – 166. By\ntaking into account the multiple actions, we rewrite\nthe sampled vector y as yi\nl where l and i denote\nthe corresponding dialogue action status and the\nith atomic action in the action space respectively.\nThe ﬁnal combined action is:3\nafake = y1\n0 ⊕y1\n1 ⊕. . . ⊕y166\n0\n⊕y166\n1\n.\n(6)\nNext, the generated action afake is fed to the re-\nward model Dω along with the corresponding state\ns. The dialogue policy Genθ aims to get a higher\nreward signal from the discriminator D; the train-\ning loss function for the generator Genθ is:\nLG(θ) = −Es,afake∼Gen(Dω(s, afake))\n(7)\nDialogue reward As to the dialogue discrimina-\ntor, we build a reward model Dω that takes as\ninput the state-action pair (s, a) and outputs the\nreward D(s, a). Instead of using a discriminator to\n3Dim(afake) = 166 ∗2.\nMLP\nState\nDiscriminator\nState, Action\nExpert Data\n+\nReal or Fake?\nfake action\nGumbel-Softmax1\nGumbel-Softmaxm\n.  .  .\n.  .  .\nreal state\nFigure 2: Architecture to approximate the dialogue pol-\nicy with adversarial learning. The dialogue policy di-\nalogue discriminator is linked to the dialogue policy\nthrough a set of Gumbel-Softmax functions4.\npredict the probability of a generated state-action\npair as being real or fake, inspired by Wasserstein\nGANs (Arjovsky et al., 2017), we replace the dis-\ncriminator model with a reward model that scores\na given pair (s, a). Since the reward model’s goal\nassigns a higher reward to the real data and a lower\nvalue to fake data, the objective can be given as the\naverage reward it assigns to the correct classiﬁca-\ntion. Given an equal mixture of real data samples\nand generated samples from the dialogue policy\nGenθ, the loss function for the reward model Dω\nis:\nLD(ω) = −Es,afake∼Genθ(Dω(s, afake))\n(8)\n+ Es,a∼data(Dω(s, a))).\n(9)\nDuring training, the policy network and the reward\nmodel are be updated alternatively.\n4.3\nPL as multi-label classiﬁcation with dense\nlayers\nWe introduced DiaAdv, which can bridge the policy\nnetwork and the reward model together utilizing\nGumbel-Softmax functions. A by-product of this\nframework is the policy network with dense layers\nand a set of Gumbel-Softmax functions. If we dis-\ncard the Gumbel-Softmax functions but keep the\ndense layers, we obtain a new model, DiaMulti-\nDense, to solve the multi-label classiﬁcation prob-\nlem. Each dense layer corresponds to a speciﬁc di-\nalogue action and the output of the dense layer has\ntwo dimensions denoting the two possible values\nfor action status, selected and not selected. We ex-\npect the dense layers can extract informative infor-\nmation particularly for their corresponding actions\nand discard noisy information. During inference,\nthe two possible values for the status of an action\nwill be compared and the higher one will be the la-\nbel for the current dialogue action. DiaMultiDense\ncan be regarded as a simple but efﬁcient state de-\nnoising method for dialogue PL with multi-label\nclassiﬁcation.\n5\nExperimental Setup\nMultiWOZ Datasset (Budzianowski et al., 2018)\nis a multi-domain dialogue dataset with 7 distinct\ndomains5, and 10, 438 dialogues. The main used\nscenario is a dialogue agent is trying to satisfy the\ntourists’ demands such as booking a restaurant or\nrecommending a hotel with speciﬁc requirements.\nEach dialogue trajectory is decomposed into a set\nof state-action pairs with the same TDS that is used\nfor training. In total, we have 56, 700 dialogue\nstate-action pairs in the training set, with 7, 300 in\nthe validation set, and 7, 300 in the test set.\nBaselines Three types of baselines are explored:\n(B1): Supervised Learning, where the dialogue\naction selection task is regarded as a multi-label\nclassiﬁcation problem.\n(B2): Reinforcement Learning (RL), where the re-\nward function is handcrafted and deﬁned as follows:\nat the end of a dialogue, if the dialogue agent ac-\ncomplishes the task within T turns, it will receive\nT ∗2 as a reward; otherwise, it will receive −T as a\npenalty. T is the maximum number of turns in each\ndialogue; we set it to 40 in all experiments. Further-\nmore, the dialogue agent will receive −1 as an in-\ntermediate reward during the dialogue to encourage\nshorter interactions. In our experiments, we used\nthree methods, including: GP-MBCM (Gaˇsi´c et al.,\n2015), ACER (Wang et al., 2016), PPO (Schulman\net al., 2017).\n(B3): Adversarial learning, where dialogue agent\nis trained with a user simulator, we conduct com-\nparisons with two methods: GAIL (Ho and Er-\nmon, 2016) and GDPL (Takanobu et al., 2019).\nThe dialogue agents in GAIL and GDPL are both\nPPO agents while these two methods have differ-\nent reward models. We report the performance of\nALDM (Liu and Lane, 2018) for completeness.\n5.1\nTraining setup\nDiaSeq With respect to DiaSeq, we use a two-layer\nMLP to extract features from the raw state repre-\nsentation. First, we sort the action order according\nto the action frequency in the training set. All ac-\ntion combinations in the dataset will be transferred\n5Attraction, Hospital, Police, Hotel, Restaurant, Taxi,\nTrain\nto an action path based on the action order. Three\nspecial actions – PAD, SOA, EOA, corresponding to\npadding, start of action decoding and end of action\ndecoding – are added to the action space for action\ndecoder training. We use beam search to predict\nthe action combinations and beam size is set to 6.\nThe action embedding size is set to 30; the hidden\nsize of the GRU is 50.\nDiaAdv For the policy network of DiaAdv, a two-\nlayer MLP is used to extract state features fol-\nlowed by 166 dense layers and Gumbel-Softmax\nfunctions consecutively. To sample a discrete ac-\ntion representation, we implemented the “Straight-\nThrough” Gumbel-Softmax Estimator (Jang et al.,\n2016); the temperature τ for each function is set to\n0.005. As to the discriminator, a three-layer MLP\ntakes as input the concatenation of dialogue state\nand action, and outputs a real value as the reward\nfor the state-action pair.\nDiaMultiDense We reuse the policy network from\nDiaAdv except the Gumbel-Softmax functions.\nGDPL (Takanobu et al., 2019) is reused.\nThe\npolicy network and value network are three-layer\nMLPs.\nPPO The policy network in PPO shares the same\narchitecture as GDPL. The difference is that the\nreward model is replaced with a handcrafted one.\nGAIL GAIL shares the same policy network as\nGDPL. The discriminator is a two-layer MLP tak-\ning as input the state-action pair.\nDiaMultiClass The policy network is a three-layer\nMLP and trained with cross entropy. It has the\nsame architecture as the policy network in GDPL.\nWe reuse the reported performance of GP-MBCM,\nACER, and ALDM from (Takanobu et al., 2019)\nsince we share the same TDS and user simulator.\nThe methods based on RL or adversarial learning\nare pre-trained with real human dialogues6.\n5.2\nEvaluation metrics\nBefore a conversation starts, a user goal will\nbe randomly sampled.\nThe sampled user goal\nmainly contain two parts of information.\nThe\nﬁrst part is about the constraints of differ-\nent domain slots or booking requirements, e.g.\n‘restaurant-inform-food’=‘Thai’, ‘restaurant-infor-\narea’=‘east’, ‘restaurant-book-people’=4 which\nmeans the user wants to book a table for 4 per-\nsons to have Thai food in the east area. The in-\n6The code of our work:\nhttps://github.com/\ncszmli/Rethink-RL-Sup\nDialogue agent\nTurn\nMatch\nRec\nF1\nSuccess rate\nGP-MBCM\n2.99\n0.44\n–\n0.19\n28.9\nACER\n10.49\n0.62\n–\n0.78\n50.8\nPPO (human)\n15.56\n0.60\n0.72\n0.77\n57.4\nALDM\n12.47\n0.69\n–\n0.81\n61.2\nGDPL\n7.80\n0.81\n0.89\n0.87\n81.7\nGAIL\n7.96\n0.81\n0.87\n0.86\n80.5\nDiaMultiClass\n12.66\n0.58\n0.71\n0.79\n57.2\nDiaMultiDense\n9.33\n0.85\n0.94\n0.87\n86.3∗\nDiaSeq\n9.03\n0.81\n0.88\n0.85\n81.6\nDiaAdv\n8.80\n0.85\n0.94\n0.85\n87.4∗\nTable 1: The performance of different dialogue agents,\nwhich is calculated based on the average results by run-\nning each method 5 times. * indicates statistically sig-\nniﬁcant improvements (p < 0.005) using a paired t-test\nover the GDPL success rate and the proposed methods.\nformation contained in the second part is about\nthe slot values that the user is looking for, such as\nrestaurant-request-phone=?, ‘restaurant-request-\naddress’=?, which means the user wants to know\nthe phone and address of the recommended restau-\nrant. We use Match, Recall, F1 score to check if all\nthe slot constraints and requested slot information\nhave been satisﬁed. F1 score evaluates whether all\nthe requested information has been provided while\nMatch evaluates whether the booked entities match\nthe indicated constraints. We use Average Turn and\nSuccess rate to evaluate the efﬁciency and level of\ntask completion of dialogue agents. If an agent has\nprovided all the requested information and made a\nbooking according to the requirements, the agent\ncompletes the task successfully.\n6\nResults and Discussion\n6.1\nPerformance of different dialogue agents\nTab. 1 shows the performance of different dialogue\nagents. With respect to success rate, DiaAdv man-\nages to achieve the highest performance by 6%\ncompared to the second highest method GDPL.\nHowever, DiaAdv is not able to beat GDPL in terms\nof average turns. A possible reason is that GDPL\ncan generate more informative and denser dialogue\naction combinations. With a user simulator in the\ntraining loop, the dialogue agent can explore more\nunseen dialogue states in the dataset. Furthermore,\nthe same user simulator will be used to test the dia-\nlogue agent and the dialogue agent will deﬁnitely\nbeneﬁt from what he has explored in the training\nstage. However, more informative and denser re-\nsponses will not guarantee all the users’ require-\nments will be satisﬁed and this will lead to a lower\nMatch score as shown in Tab. 1.\nAs to DiaSeq, it can achieve almost the same\nDialogue agent\nDiaSeq\nDiaMultiClass\nDiaMultiDense\n#Parameters\n251,000\n184,000\n133,000\nTable 2: Total number of parameters for supervised\nlearning models.\nperformance as GDPL from different perspectives\nwhile GDPL has a slightly higher F1 score. How-\never, the potential cost beneﬁts of DiaSeq are huge\nsince it does not require a user simulator in the train-\ning loop. The training of DiaSeq is well-understood\nand we can get rid of tuning the sensitive parame-\nters in RL and Adversarial Learning. To sum up,\nDiaSeq is far more cost-efﬁcient solution.\nAnother supervised learning method, DiaMulti-\nDense achieves remarkable performance with re-\nspect to different metrics. Compared to the tra-\nditional solution DiaMultiClass, joining of dense\nlayers as in DiaMultiDense brings a huge perfor-\nmance gain; it manages to beat DiaMultiClass on\nall the metrics. And it achieves higher F1 score\nthan DiaAdv. Since the only difference between\nDiaMultiDense and DiaMultiClass is that we re-\nplace the last layer of DiaMultiClass with a stack\nof dense layers, the change in the number of param-\neters may lead to the performance gap. We report\nthe number of parameters of three supervised learn-\ning methods in Tab. 2. DiaMultiDense achieves\nthe highest performance among these three meth-\nods while using the fewest parameters. We believe\nthe dense layers have been trained to ﬁlter noisy\ninformation from the previous module and the ﬁ-\nnal classiﬁcation can beneﬁt from the high-quality\ninformation ﬂow.\n6.2\nUser experience evaluation\nAutomatic metrics can only capture part of the per-\nformance difference between different dialogue\nagents. For example, we use success rate to re-\nﬂect the level of task completion and use turn num-\nber to represent the efﬁciency of dialogue agents.\nHowever, the ﬁnal goal of a TDS is to assist real\nusers to complete tasks. To fully evaluate system\nperformance while interacting with real users, we\nlaunch an evaluation task on Amazon Mturk to\nrate the user experience with the proposed dialogue\nsystems. For each evaluation task, we will ﬁrst\npresent an Mturk worker with a randomly sampled\nuser goal, which contains the constraints about spe-\nciﬁc domain slots and some slot information that\nthe user is looking for. In the next step, accord-\ning to the sampled goal, two generated dialogues\nfrom two different dialogue agents are shown to\nDataset\nAgent\nDiaMultiClass\nDiaSeq\nDiaMultiDense\nGDPL\nDiaAdv\nTurn\nSuccess rate\nTurn\nSuccess rate\nTurn\nSuccess rate\nTurn\nSuccess rate\nTurn\nSuccess rate\nMultiWOZ (0.1)\n17.14\n31.7\n10.77\n70.4\n18.36\n27.0\n9.21\n21.2\n16.80\n37.2\nMultiWOZ (0.4)\n12.56\n59.0\n9.99\n75.5\n10.76\n79.4\n8.49\n68.0\n9.90\n81.6\nMultiWOZ (0.7)\n13.1\n53.6\n9.35\n77.2\n10.02\n85.1\n8.10\n73.3\n9.30\n87.0\nTable 3: The performance of different dialogue agents with different amounts of expert dialogues. We only report\nAverage Turn and Success rate here due to limited space.\nDialogue pair\nWin\nLoose\nTie\nDiaMultiDense vs. GDPL\n42\n50\n8\nDiaSeq vs. GDPL\n50\n44\n6\nDiaAdv vs. GDPL\n39\n51\n10\nTable 4: Human evaluation results.\nthe worker. The worker needs to pick up the dia-\nlogue agent that provides a better user experience.\nDifferent factors will be taken into account, such\nas response quality, response naturalness, how sim-\nilar it is compared to a real human assistant. If\nthe worker thinks two dialogue agents perform\nequally good/bad or it’s hard to distinguish which\none is better, the option ‘Neutral’ can be selected.\nFour dialogue agents are evaluated: GDPL, DiaSeq,\nDiaMultiDense and DiaAdv, and there are three\ncomparison pairs DiaMultiDense-GDPL, DiaSeq-\nGDPL, DiaAdv-GDPL since GDPL is regarded as\nthe SOTA method. Each comparison pair has 100\ndialogue goals sampled and 200 corresponding dia-\nlogues from two different dialogue agents. All the\ndialogue actions in the dialogue turns are translated\ninto human readable utterances with the language\ngeneration module from ConvLab (Lee et al., 2019).\nEach dialogue pair is annotated by three Mturk\nworkers. The ﬁnal results are shown in Tab. 4.\nThe method DiaAdv can be regarded as an ex-\ntension of DiaMultiDense by adding a classiﬁer to\nprovide a stronger training signal. According to\nthe results from Section 6.1, these two methods do\nimprove the success rate of dialogue agents. How-\never, as shown in Tab. 4, while the success rate\nimproves, the user experience degrades. Accord-\ning to Tab. 1, GDPL and DiaAdv have similar F1\nscores but the DiaAdv has a higher Recall value;\nthis means that DiaAdv achieves a lower Precision.\nThe unnecessary information mixed in the system\nresponse annoys users and results in a lower user\nexperience. Given the relatively large difference\nin terms of success rate, the trade-off between suc-\ncess rate and user experience should be carefully\nexamined. From another perspective, it is under-\nstandable that GDPL can provide a better user ex-\nperience because a pre-designed user simulator is\ninvolved and the discriminator will encounter more\ndiverse state-action combinations that are not seen\nin the training data. In contrast, the discriminator\nin DiaAdv only has access to the training data and\nthis limits its judging ability. This does not imply\nthat having a user simulator in the loop is essential\nto provide high quality user experience: DiaSeq,\nwhich is a completely supervised learning method,\noutperforms GDPL.\n6.3\nDiscussion\nHow many expert dialogues are enough to train\na dialogue agent with supervised learning? One\nmotivation for dropping supervised learning and\nemploying RL methods in TDS is that building\nhigh-quality conversational datasets is expensive\nand time-consuming. In contrast, training dialogue\nagents with a user-simulator is cheaper and afford-\nable in many cases. Since we have no control on\nhow much domain knowledge should be involved\nto build a user-simulator, we are not able to mea-\nsure the expense of a reliable user-simulator. How-\never, we can conduct an experiment to show how\nmany real human dialogues are required to train a\nhigh-quality dialogue agent.\nBased on the original MultiWoZ dataset, we\nbuild three smaller subsets: MultiWoZ(0.1), Mul-\ntiWoZ(0.4), MultiWoZ(0.7) by only keeping 10%,\n40%, and 70% dialogue pairs from the original\ndataset, respectively. We retrain DiaMultiClass,\nGDPL, DiaAdv, DiaMultiDense, DiaSeq and re-\nport the performance in Tab. 3. With respect to\nsupervised learning agents, with only 10% expert\ndialogue pairs, DiaMultiClass gets half the suc-\ncess rate compared to the original performance\n(Tab. 1). By adding 30% more dialogue pairs to the\ntraining set, DiaMultiClass can achieve the same\nperformance 59% with the original success rate\n57.2%. Beyond this, DiaMultiClass does not bene-\nﬁt from the increase in expert dialogues and starts\nto ﬂuctuate between 55% and 59%. In contrast, Di-\naSeq can achieve higher performance when there\nare only 10% expert dialogue pairs and the suc-\ncess rate increases with the number of available\nexpert dialogues. DiaMultiDense achieves the best\nperformance with the same amount of expert dia-\nlogues compared to the other two supervised learn-\ning methods. The performance difference among\nthe three supervised learning methods shows that\nthe method itself is the main factor to inﬂuence the\nperformance rather than the number of available\nexpert dialogues in the given dialogue environment.\nTo some extent, traditional DiaMultiClass does not\nexert the potential of a given dataset to the fullest\nin dialogue PL.\nCan adversarial learning eliminate expert dia-\nlogues? As can be concluded from Tab. 3, GDPL\nand DiaAdv managed to improve the performance\nwith the increasing number of expert dialogues.\nGDPL and DiaAdv have the reward models that are\nsupposed to distinguish real dialogue pairs from\nthe machine-generated ones. By observing more\nexpert dialogues, the reward model can provide a\ndialogue policy with more reliable and consistent\nupdating signals. Figure 3 shows the success rate\n0.0\n0.1\n0.4\n0.7\n1.0\nData Size\n20\n30\n40\n50\n60\n70\n80\nSuccess rate\nDiaAdv\nGDPL\nDiaMultiDense\nDiaMultiClass\nFigure 3:\nThe performance gain between the pre-\ntrained and their corresponding adversarial learning\nmodels with different amounts of expert dialogues.\ngain by applying adversarial learning methods to\nthe corresponding pre-trained models 7. When the\nsuccess rates of DiaMultiClass with MultiWoZ(0.4)\nand MultiWoZ(1.0) are both around 60%, deploy-\ning GDPL manages to bring 10% performance gain.\nThe performance difference can be caused by the\nimproved quality of the reward model. Conversely,\nif the reward model has no access to sufﬁcient\namount of expert behaviors, it has little clue how\nthe expert dialogues should look like. This can\nlead to poor reward signals for the policy network.\nWe can see it in the case of GDPL that the success\nrate drops to 21% while the pre-trained model can\nachieve 31% success rate on MultiWoZ(0.1). The\nperformance gain between DiaMultiDense and Di-\naAdv is not so remarkable with respect to success\nrate compared to the gain between DiaMultiClass\n7DiaAdv is the adversarial extension of DiaMultiDense\nwhile GDPL is the adversarial extension of DiaMultiClass.\nand DiaAdv. However, DiaAdv does help to reduce\nthe dialogue turns while improving the success rate\nas shown in Tab. 3. We can regard DiaAdv as a\npromising method to ﬁne-tune the DiaMultiDense\nto explore more potential dialogue states.\nHow sensitive are adversarial learning to pre-\ntrained dialogue policy? We explore how pre-\ntrained dialogue policies affect the ﬁnal perfor-\nmance of adversarial learning based dialogue\nagents. We ﬁrst use supervised learning to pre-train\nthe dialogue policies of GDPL and DiaAdv respec-\ntively with different training epochs. As shown in\n0\n1\n4\n7\n10\nPretrain Epoch\n0\n10\n20\n30\n40\n50\n60\n70\n80\nSuccess rate\nDiaAdv\nGDPL\nDiaMultiDense\nDiaMultiClass\nFigure 4:\nThe performance gain between the pre-\ntrained and their corresponding adversarial learning\nmodels with different amounts of pre-taining epochs.\nFigure 4, the performance gain between the pre-\ntrained dialogue policy and the corresponding ad-\nversarial are limited. With respect to GDPL, it\neven degenerates the original performance of the\npre-trained policy when the starting points are rela-\ntively low. In other words, the main contributions\nto the adversarial dialogue agents come from the\nsupervised learning stage; it is challenging for the\ndialogue agents to achieve the same performance\nwithout a promising pre-trained dialogue policy.\n7\nConclusion\nIn this work, we proposed two supervised learning\napproaches and one adversarial learning method to\ntrain the dialogue policy for TDSs without building\nuser simulators. The proposed methods can achieve\nstate-of-the-art performance suggested by existing\napproaches based on Reinforcement Learning (RL)\nand adversarial learning. However, we have demon-\nstrated that our methods require fewer training ef-\nforts, namely the domain knowledge needed to de-\nsign a user simulator and the intractable parameter\ntuning for RL or adversarial learning. Our ﬁnd-\nings have questioned if the full potential of super-\nvised learning for dialogue Policy Learning (PL)\nhas been exerted and if RL methods have been used\nin the appropriate TDS scenarios.\nReferences\nMartin Arjovsky, Soumith Chintala, and L´eon Bot-\ntou. 2017.\nWasserstein gan.\narXiv preprint\narXiv:1701.07875.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I˜nigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz-a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016–5026.\nHongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang\nTang. 2017.\nA survey on dialogue systems: Re-\ncent advances and new frontiers. Acm Sigkdd Ex-\nplorations Newsletter, 19(2):25–35.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014.\nLearning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation.\narXiv preprint\narXiv:1406.1078.\nBhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,\nYun-Nung Chen, Faisal Ahmed, and Li Deng. 2016.\nTowards end-to-end reinforcement learning of dia-\nlogue agents for information access. arXiv preprint\narXiv:1609.00777.\nM Gaˇsi´c, N Mrkˇsi´c, Pei-hao Su, David Vandyke,\nTsung-Hsien Wen, and Steve Young. 2015.\nPol-\nicy committee for adaptation in multi-domain spo-\nken dialogue systems. In 2015 IEEE Workshop on\nAutomatic Speech Recognition and Understanding\n(ASRU), pages 806–812. IEEE.\nMilica Gaˇsi´c and Steve Young. 2014. Gaussian pro-\ncesses for pomdp-based dialogue manager optimiza-\ntion.\nIEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 22(1):28–40.\nEmil Julius Gumbel. 1954.\nStatistical theory of ex-\ntreme values and some practical applications: a se-\nries of lectures, volume 33. US Government Print-\ning Ofﬁce.\nJonathan Ho and Stefano Ermon. 2016. Generative ad-\nversarial imitation learning. In NIPS, pages 4565–\n4573.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-\nical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nSungjin Lee, Qi Zhu, Ryuichi Takanobu, Xiang Li,\nYaoqin Zhang, Zheng Zhang, Jinchao Li, Baolin\nPeng, Xiujun Li, Minlie Huang, et al. 2019. Con-\nvlab: Multi-domain end-to-end dialog system plat-\nform. arXiv preprint arXiv:1904.08637.\nXiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao,\nand Asli Celikyilmaz. 2017.\nEnd-to-end task-\ncompletion neural dialogue systems. arXiv preprint\narXiv:1703.01008.\nZiming Li, Sungjin Lee, Baolin Peng, Jinchao Li,\nShahin Shayandeh, and Jianfeng Gao. 2020. Guided\ndialog policy learning without adversarial learning\nin the loop. arXiv preprint arXiv:2004.03267.\nZachary Lipton, Xiujun Li, Jianfeng Gao, Lihong Li,\nFaisal Ahmed, and Li Deng. 2018. Bbq-networks:\nEfﬁcient exploration in deep reinforcement learn-\ning for task-oriented dialogue systems.\nIn Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence.\nBing Liu and Ian Lane. 2018. Adversarial learning of\ntask-oriented neural dialog models. In Proceedings\nof the SIGDIAL 2018 Conference, pages 350–359.\nBaolin Peng,\nXiujun Li,\nJianfeng Gao,\nJingjing\nLiu, Yun-Nung Chen, and Kam-Fai Wong. 2018a.\nAdversarial advantage actor-critic model for task-\ncompletion dialogue policy learning. In 2018 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 6149–6153.\nIEEE.\nBaolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,\nand Kam-Fai Wong. 2018b.\nDeep dyna-q: Inte-\ngrating planning for task-completion dialogue policy\nlearning. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), volume 1, pages 2182–\n2192.\nBaolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,\nAsli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.\n2017. Composite task-completion dialogue policy\nlearning via hierarchical deep reinforcement learn-\ning. arXiv preprint arXiv:1704.03084.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford,\nand Oleg Klimov. 2017.\nProximal\npolicy optimization algorithms.\narXiv preprint\narXiv:1707.06347.\nPei-Hao Su, Pawel Budzianowski, Stefan Ultes, Mil-\nica Gasic, and Steve Young. 2017. Sample-efﬁcient\nactor-critic reinforcement learning with supervised\ndata for dialogue management.\narXiv preprint\narXiv:1707.00130.\nPei-Hao Su, Milica Gasic, Nikola Mrkˇsi´c, Lina M Ro-\njas Barahona, Stefan Ultes, David Vandyke, Tsung-\nHsien Wen, and Steve Young. 2016. On-line active\nreward learning for policy optimisation in spoken di-\nalogue systems. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n2431–2441.\nShang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu,\nand Yun-Nung Chen. 2018.\nDiscriminative deep\ndyna-q: Robust planning for dialogue policy learn-\ning. In EMNLP.\nRyuichi Takanobu, Hanlin Zhu, and Minlie Huang.\n2019.\nGuided dialog policy learning: Reward es-\ntimation for multi-domain task-oriented dialog. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 100–\n110.\nMarilyn A Walker, Diane J Litman, Candace A Kamm,\nand Alicia Abella. 1997. Paradise: A framework for\nevaluating spoken dialogue agents. arXiv preprint\ncmp-lg/9704004.\nZiyu Wang, Victor Bapst, Nicolas Heess, Volodymyr\nMnih, Remi Munos, Koray Kavukcuoglu, and\nNando de Freitas. 2016.\nSample efﬁcient actor-\ncritic with experience replay.\narXiv preprint\narXiv:1611.01224.\nJoseph Weizenbaum. 1966. Elizaa computer program\nfor the study of natural language communication be-\ntween man and machine.\nCommunications of the\nACM, 9(1):36–45.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexan-\nder M Rush, Bart van Merri¨enboer, Armand Joulin,\nand Tomas Mikolov. 2015.\nTowards ai-complete\nquestion answering: A set of prerequisite toy tasks.\narXiv preprint arXiv:1502.05698.\nJason D Williams,\nKavosh Asadi,\nand Geoffrey\nZweig. 2017.\nHybrid code networks:\npractical\nand efﬁcient end-to-end dialog control with super-\nvised and reinforcement learning.\narXiv preprint\narXiv:1702.03274.\nJason D Williams, Matthew Henderson, Antoine Raux,\nBlaise Thomson, Alan Black, and Deepak Ra-\nmachandran. 2014. The dialog state tracking chal-\nlenge series. AI Magazine, 35(4):121–124.\nJason D Williams and Steve Young. 2007.\nPartially\nobservable markov decision processes for spoken\ndialog systems.\nComputer Speech & Language,\n21(2):393–422.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-09-21",
  "updated": "2020-09-21"
}