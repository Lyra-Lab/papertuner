{
  "id": "http://arxiv.org/abs/2009.10562v1",
  "title": "A Centralised Soft Actor Critic Deep Reinforcement Learning Approach to District Demand Side Management through CityLearn",
  "authors": [
    "Anjukan Kathirgamanathan",
    "Kacper Twardowski",
    "Eleni Mangina",
    "Donal Finn"
  ],
  "abstract": "Reinforcement learning is a promising model-free and adaptive controller for\ndemand side management, as part of the future smart grid, at the district\nlevel. This paper presents the results of the algorithm that was submitted for\nthe CityLearn Challenge, which was hosted in early 2020 with the aim of\ndesigning and tuning a reinforcement learning agent to flatten and smooth the\naggregated curve of electrical demand of a district of diverse buildings. The\nproposed solution secured second place in the challenge using a centralised\n'Soft Actor Critic' deep reinforcement learning agent that was able to handle\ncontinuous action spaces. The controller was able to achieve an averaged score\nof 0.967 on the challenge dataset comprising of different buildings and\nclimates. This highlights the potential application of deep reinforcement\nlearning as a plug-and-play style controller, that is capable of handling\ndifferent climates and a heterogenous building stock, for district demand side\nmanagement of buildings.",
  "text": "A Centralised Soft Actor Critic Deep Reinforcement Learning\nApproach to District Demand Side Management through\nCityLearn\nAnjukan Kathirgamanathan∗\nanjukan.kathirgamanathan@ucdconnect.ie\nSchool of Mechanical and Materials\nEngineering, University College\nDublin\nDublin, Ireland\nKacper Twardowski\nEleni Mangina\nkacper.twardowski@ucdconnect.ie\neleni.mangina@ucd.ie\nSchool of Computer Science,\nUniversity College Dublin\nDublin\nDonal P. Finn\nSchool of Mechanical and Materials\nEngineering, University College\nDublin\nDublin, Ireland\nABSTRACT\nReinforcement learning is a promising model-free and adaptive\ncontroller for demand side management, as part of the future smart\ngrid, at the district level. This paper presents the results of the\nalgorithm that was submitted for the CityLearn Challenge, which\nwas hosted in early 2020 with the aim of designing and tuning a\nreinforcement learning agent to flatten and smooth the aggregated\ncurve of electrical demand of a district of diverse buildings. The\nproposed solution secured second place in the challenge using a\ncentralised ‘Soft Actor Critic’ deep reinforcement learning agent\nthat was able to handle continuous action spaces. The controller\nwas able to achieve an averaged score of 0.967 on the challenge\ndataset comprising of different buildings and climates. This high-\nlights the potential application of deep reinforcement learning as a\nplug-and-play style controller, that is capable of handling different\nclimates and a heterogenous building stock, for district demand\nside management of buildings.\nCCS CONCEPTS\n• Applied computing →Engineering; • Computing method-\nologies →Multi-agent reinforcement learning.\nKEYWORDS\nDeep Reinforcement Learning, Smart Grid, Demand Side Manage-\nment\nACM Reference Format:\nAnjukan Kathirgamanathan, Kacper Twardowski, Eleni Mangina, and Donal\nP. Finn. 2020. A Centralised Soft Actor Critic Deep Reinforcement Learning\nApproach to District Demand Side Management through CityLearn. In\nProceedings of RLEM ’20: First Int’l Workshop on Reinforcement Learning\nfor Energy Management in Buildings & Cities (RLEM ’20). ACM, Yokohama,\nJapan, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nRLEM ’20, November 03–05, 2020, Yokohama, JP\n© 2020 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nBuildings are complex systems influenced by changing weather,\noccupancy, schedules and in a demand response (DR) context -\ngrid signals. Capturing these dynamics in a physics-based simula-\ntion model capable of facilitating DR is highly challenging. This is\ndue to highly non-linear behaviour of the thermal-dynamics and\nthe fact that no one building is identical to another in the highly\nheterogenous building stock [3]. Reinforcement learning (RL) is\na model-free algorithm that learns from historical and real-time\ndata and has shown promise in recent research applied to building\nenergy management problems [8, 9]. Given the novelty associated\nwith RL in this domain, the behaviour of multiple energy consuming\nagents (i.e., buildings), subject to demand-dependent grid signals,\nis an area that is not well understood [8]. The CityLearn project\n(https://github.com/intelligent-environments-lab/CityLearn) is an\nOpenAI Gym environment [1], which aims to facilitate the imple-\nmentation of RL agents in a multi-agent DR context for a diverse\ngroup of buildings [7]. The main objective of CityLearn is to facil-\nitate and standardize the evaluation and comparison of different\nRL agents and algorithms. The CityLearn Challenge was organised\nvirtually and ran from January to July 2020. It invited participants\nto design, develop and tune a RL agent to flatten and smooth the\naggregated curve of electrical demand for a district comprising of\n9 diverse buildings. The current paper presents the results from a\ncentralised \"Soft Actor Critic\" deep RL based algorithm that was\nsubmitted to the challenge.\n2\nRELATED WORK\nThe review of RL for DR by Vázquez-Canteli and Nagy [8] shows its\npromising potential as a model-free technique, mitigating the need\nto develop physics-based control-oriented models, and capable of\ndealing with the heterogenous nature of the building stock. The\nreview found that most studies to date focus on single building\nsystems with demand-independent electricity prices. Focusing on\ndeep RL, which has gained significant interest and traction in recent\nyears, e.g., using Deep Q Networks [5], such approaches have often\nbeen limited to discrete and low-dimensional action spaces [4].\nThere is a research gap in the application of deep reinforcement\nlearning to problems with continuous action spaces in the building\nenergy management domain.\nThe Soft Actor-Critic (SAC) algorithm, an off-policy maximum\nentropy actor-critic algorithm, as first proposed by Haarnoja et al\narXiv:2009.10562v1  [cs.LG]  22 Sep 2020\nRLEM ’20, November 03–05, 2020, Yokohama, JP\nKathirgamanathan, et al.\n[2] in 2018, is one of the algorithms that is capable of operating\nover continuous action spaces. At their core, actor-critic methods\nare a type of policy gradient method, which have separate mem-\nory structures to explicitly represent the policy [6]. The policy\nstructure is known as the actor and the estimated value function is\nknown as the critic. The actor selects the actions whereas the critic\nevaluates the actions made by the actor. The reader is referred to\nSutton [6] for a more detailed explanation of Actor-Critic methods.\nHaarnoja et al. [2] suggest that the SAC algorithm provides for both\nsample-efficient learning and stability and hence extends readily to\ncomplex, high-dimensional tasks. They found the SAC algorithm\nshowed substantial improvement in both performance and sample\nefficiency over both off-policy and on-policy prior methods. This\ncurrent research investigates the suitability of the SAC algorithm\nfor tackling the district DSM problem utilising CityLearn.\n3\nMETHODS\n3.1\nCityLearn Challenge\nThe CityLearn challenge used a multi-objective cost function of\nfive equally weighted metrics applied to an entire district of nine\nbuildings (as outlined in Table 1). These are described below:\n(1) Peak electricity demand (for the evaluation period of 1 year)\n(2) Average daily electricity peak demand (daily peak demand\nof the district averaged over the evaluation period)\n(3) Ramping (a measure of how much the district electricity\nconsumption changes from one timestep to the next)\n(4) 1 - Load factor (the average monthly electricity demand\ndivided by its maximum peak)\n(5) Net electricity consumption of the district over the evalua-\ntion period\nThe multi-objective cost function is normalised by a baseline cost\nobtained from the performance of a predefined manually tuned\nRule-Based Controller (RBC). This implied that a cost function of\nless than 1 resulted in a better performance than the RBC. This RBC\ncontroller charges cooling (and domestic hot water (DHW) if avail-\nable) during the night and discharges during the day based only\non the hour of the day. The adaptive potential of RL to deal with\ndifferent environments, rather than solely the one it was trained on,\nwas tested through evaluating the controller on different datasets.\nParticipants used the design dataset to implement their RL agent\n(including design, tuning and pre-training) and could test their\nagent on the evaluation dataset and receive feedback through the\ncost function based on the generalisation results of their agents.\nEach dataset contains year-long hourly information about the cool-\ning and DHW demand of the building, electricity consumed by\nappliances, solar power generation, as well as weather data and\nother variables. The evaluation dataset featured different buildings\nfrom different cities than the design dataset, albeit within the same\nclimate zones (see Table 2). The challenge dataset is different from\nboth the design and evaluation datasets featuring different buildings\nand climates.\n3.2\nState Space and Hyperparameters\nThe state space is what the RL agent observes for each control step.\nThe CityLearn environment allows for a total of 27 observations\nTable 1: Buildings and Descriptions in CityLearn Challenge\nDistrict (Design Dataset)\nBuilding\nNumber\nType\nType Details\nCooling\nStorage1\nDHW\nStorage1\nPV\n(kW)\n1\nCommercial\nMedium Of-\nfice\n3\n3\n120\n2\nCommercial\nFast-food\nRestaurant\n3\n3\nN/A\n3\nCommercial\nStandalone\nRetail\n3\nN/A\nN/A\n4\nCommercial\nStrip Mall Re-\ntail\n3\nN/A\n40\n5\nResidential\nMedium\nMulti-family\n3\n3\n25\n6\nResidential\nMedium\nMulti-family\n3\n3\n20\n7\nResidential\nMedium\nMulti-family\n3\n3\nN/A\n8\nResidential\nMedium\nMulti-family\n3\n3\nN/A\n9\nResidential\nMedium\nMulti-family\n3\n3\nN/A\n1The storage capacity is the non-dimensional scaling factor given above\nmultiplied by the building maximum cooling or DHW demand.\nTable 2: Climate Zones for the Different Datasets in the\nCityLearn Challenge\nCityLearn Cli-\nmate Zone\nASHRAE\nIdentifier\nDescription\nCity\nDesign Dataset\n1\n2A\nHot-Humid\nNew Orleans\n2\n3A\nWarm-Humid\nAtlanta\n3\n4A\nMixed-Humid\nNashville\n4\n5A\nCold-Humid\nChicago\nEvaluation Dataset\n1\n2A\nHot-Humid\nOrlando\n2\n3A\nWarm-Humid\nDallas\n3\n4A\nMixed-Humid\nKansas City\n4\n5A\nCold-Humid\nOmaha\nper building that may be passed to the agent. The final state design\nused in the submission was determined by utilising a combination\nof expert assessment and trial and error and is outlined in Table\n3. A centralised solution is presented here with one agent, which\nhas complete oversight of all nine buildings. The SAC RL algorithm\nused has several key hyperparameters and the values used in the\nsubmission are detailed in Table 4. Note that some parameters were\nmodified in the deployment (evaluation) phase to allow the agent\nto adapt to new environments (such as a new climate), whilst also\nretaining the initial weights of the pre-trained agent.\n3.3\nReward Function\nThe reward function was designed based on a virtual price signal\n(penalising peak consumption) together with manual reward shap-\ning to incentivise charging during the night and discharging during\nthe day. The reward (R) function used is shown in Eq. 1 where β\nCentralised SAC Deep RL Approach to District Demand Side Management\nRLEM ’20, November 03–05, 2020, Yokohama, JP\nTable 3: State Space used for Centralised Agent Case\nState Variable\nDescription\nmonth\nmonth of timestep (1-12)\nday\nday of timestep (1-7)\nhour\nhour of the day (1-24)\ntout\nOutside drybulb temperature (C)\ndirect_solar_rad\nDirect solar radiation (W /m2)\nnon_shiftable_loadn\nNon-shiftable electricity load of Building n\nsolar_genn\nSolar generation of Building n (if PV present)\ncooling_storage_socn\nState of charge of cooling storage of Building\nn\ndhw_storage_socn\nState of charge of DHW storage of Building\nn (if present)\nTable 4: Hyperparameters for Training and Evaluation\nSymbol\nDescription\nTraining Value\nEvaluation Value\nReplay buffer size\n2x106\n2x106\nMinibatch size\n1024\n64\nγ\nDiscount factor\n0.9\n0.9\nα\nReward temperate pa-\nrameter\n0.2\n0.2\nUpdate interval\n168\n168\nLearning rate\n5 × 10−4\n1 × 10−4\nτ\nTarget smoothing co-\nefficient\n3 × 10−3\n3 × 10−3\nHidden layer size\n256\n256\nis a weighting coefficient (with a value of 0.005), N is the number\nof buildings in the district, etotal is the total district electricity\nconsumption and ei is the building i electricity consumption. The\nfinal reward value was also scaled and clipped to be in the range of\n-1 to 1.\nR =\nN\nÕ\nj=1\n(β ∗etotal ∗ei) + Rniдht + Rday\n(1)\nwhere:\nRniдht =\n\n\n1000,\nif 10pm ≤hour ≤12pm AND mean(actions) > 0.1\n−1000,\nif 10pm ≤hour ≤12pm AND mean(actions) < 0\n0,\notherwise\nRday =\n(\n−1000,\nif 12pm ≤hour ≤08pm AND mean(actions) > 0\n0,\notherwise\n4\nRESULTS\nThe centralised SAC RL Agent shows promising performance ap-\nplied to the district DSM problem. Within 10 episodes of training\non the dataset for climate zone 1 (see Fig. 1), the agent realises an\nimproved multi-objective cost function (as defined in Section 3.1)\nas compared to the RBC baseline (manually predefined controller).\nNote that the cost function is computed relative to the RBC baseline,\ni.e., a cost function of less than 1 is considered to be an improved\nperformance over the baseline and a cost function of greater than 1\nis considered to be a poorer performance compared to the baseline.\nWhen this pre-trained agent is evaluated (i.e., deployed) for the\nsame climate zone, it produces an improvement of 10.7% (a score of\n0.893) over the RBC baseline (see Figure 2 for the district electricity\nconsumption profile and Table 5 for the scores). This table also\nshows the ability of the RL agent to generalise and adapt to new\nclimatic conditions, and although they suffered a performance drop,\nstill managed to outperform the manually tuned RBC.\nThe Challenge dataset scores showed that the agent was further\ngeneralisable to unseen data (see Table 5) featuring both different\nbuilding properties and climates. Overall, an average improvement\nof 3.3% was seen over the manually tuned RBC over the four differ-\nent climate zones tested. Whilst these improvements are modest,\nthey are promising given the adaptability over a range of buildings\nand climates and the limited information required for the state\nspace.\nFigure 1: Training results for Climate Zone 1 (reward and\ncost objectives as function of training episodes).\n5\nCONCLUSIONS AND FURTHER WORK\nIn this paper, a centralised ‘Soft Actor Critic’ reinforcement learning\nagent, capable of handling continuous action spaces, is proposed for\nRLEM ’20, November 03–05, 2020, Yokohama, JP\nKathirgamanathan, et al.\nFigure 2: A comparison of district electricity consumption for (i) no load shifting, (ii) predefined RBC and (iii) SAC RL.\nTable 5: Evaluation Results on the Design & Challenge Dataset\nNote: The values presented below are relative to the baseline predefined RBC controller which has a score of 1. The lower the score,\nthe better the performance of the RL agent.\nClimate\nZone\nRamping\n1-Load Factor\nAvg. Daily Peak\nPeak Demand\nNet\nElec.\nCon-\nsumption\nAvg. Score\nDesign Dataset\n1\n0.735\n0.881\n0.849\n0.986\n1.014\n0.893\n2\n0.777\n1.017\n0.940\n1.187\n1.018\n0.988\n3\n0.810\n0.983\n0.985\n1.077\n1.019\n0.975\n4\n0.789\n0.983\n0.959\n1.004\n1.014\n0.950\nAvg. Score\n0.952\nChallenge Dataset\n1\n0.779\n1.014\n0.982\n1.131\n1.015\n0.984\n2\n0.780\n0.980\n0.959\n0.999\n1.013\n0.946\n3\n0.812\n0.960\n0.939\n1.083\n1.018\n0.962\n4\n0.860\n0.996\n0.991\n1.013\n1.017\n0.976\nAvg. Score\n0.967\nthe district demand side management problem and the performance\nof the agent applied to the CityLearn challenge is outlined. The\nagent was able to secure second place in the competition achiev-\ning an average score of 0.967 over the challenge dataset featuring\ndifferent buildings and climates when compared to the reference\nmanually tuned rule-based controller. This highlights the potential\nof deep reinforcement learning as a plug-and-play style controller\nfor district level demand side management of buildings. Limitations\ninclude the manual reward shaping applied which perhaps limits\nthe generalisation ability of the RL agent to districts with signifi-\ncantly different demand profiles. Given the centralised agent with\noversight of all buildings, it is not known how the computational\nrequirements and performance would scale over a larger number\nbuildings. A further limitation of CityLearn is that the cooling load\nis precomputed and hence currently does not support thermal com-\nfort considerations and utilisation of the passive thermal mass for\nload shifting. Future work aims to further the robustness of the\nRL agent through reducing the amount of manual reward shaping\napplied and testing the performance of the algorithm for different\nhyperparameters. The addition of further energy systems such as\nbatteries and electric vehicles to CityLearn will also be considered.\nACKNOWLEDGMENTS\nThe authors gratefully acknowledge that their contribution em-\nanated from research supported by Science Foundation Ireland\nunder the SFI Strategic Partnership Programme Grant Number\nSFI/15/SPP/E3125.\nREFERENCES\n[1] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schul-\nman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym. arXiv:arXiv:1606.01540\n[2] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft\nactor-critic: Off-policy maximum entropy deep reinforcement learning with a\nstochastic actor. In 35th International Conference on Machine Learning, ICML 2018,\nVol. 5. 2976–2989. arXiv:arXiv:1801.01290v2\n[3] Anjukan Kathirgamanathan, Mattia De Rosa, Eleni Mangina, and Donal P. Finn.\n2020. Data-driven Predictive Control for Unlocking Building Energy Flexibility:\nA Review. Renewable and Sustainable Energy Reviews 135, January 2021 (2020),\n110120. https://doi.org/10.1016/j.rser.2020.110120\n[4] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom\nErez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control\nwith deep reinforcement learning. In 4th International Conference on Learning\nRepresentations, ICLR 2016. arXiv:1509.02971\n[5] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness,\nMarc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg\nOstrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen\nKing, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015.\nHuman-level control through deep reinforcement learning. Nature 518, 7540\n(2015), 529–533. https://doi.org/10.1038/nature14236\n[6] Andrew G Sutton, Richard S.; Barton. 2014. Reinforcement Learning: An Introduction\n(second ed.). MIT Press, Cambridge, Massachusetts.\n[7] José R. Vázquez-Canteli, Jérôme Kämpf, Gregor Henze, and Zoltan Nagy. 2019.\nCityLearn v1.0: An OpenAI gym environment for demand response with deep\nreinforcement learning. BuildSys 2019 - Proceedings of the 6th ACM International\nConference on Systems for Energy-Efficient Buildings, Cities, and Transportation\n(2019), 356–357. https://doi.org/10.1145/3360322.3360998\n[8] José R. Vázquez-Canteli and Zoltán Nagy. 2019. Reinforcement learning for demand\nresponse: A review of algorithms and modeling techniques. Applied Energy 235,\nNovember 2018 (2019), 1072–1089. https://doi.org/10.1016/j.apenergy.2018.11.002\nCentralised SAC Deep RL Approach to District Demand Side Management\nRLEM ’20, November 03–05, 2020, Yokohama, JP\n[9] Zhe Wang and Tianzhen Hong. 2020. Reinforcement Learning for Building Con-\ntrols: The problem, opportunities and challenges. Applied Energy 269, 1 (2020),\n300. https://doi.org/10.1016/j.apenergy.2020.115036\n",
  "categories": [
    "cs.LG",
    "stat.ML",
    "J.2"
  ],
  "published": "2020-09-22",
  "updated": "2020-09-22"
}