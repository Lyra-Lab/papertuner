{
  "id": "http://arxiv.org/abs/2009.10568v1",
  "title": "Adversarial Attack Based Countermeasures against Deep Learning Side-Channel Attacks",
  "authors": [
    "Ruizhe Gu",
    "Ping Wang",
    "Mengce Zheng",
    "Honggang Hu",
    "Nenghai Yu"
  ],
  "abstract": "Numerous previous works have studied deep learning algorithms applied in the\ncontext of side-channel attacks, which demonstrated the ability to perform\nsuccessful key recoveries. These studies show that modern cryptographic devices\nare increasingly threatened by side-channel attacks with the help of deep\nlearning. However, the existing countermeasures are designed to resist\nclassical side-channel attacks, and cannot protect cryptographic devices from\ndeep learning based side-channel attacks. Thus, there arises a strong need for\ncountermeasures against deep learning based side-channel attacks. Although deep\nlearning has the high potential in solving complex problems, it is vulnerable\nto adversarial attacks in the form of subtle perturbations to inputs that lead\na model to predict incorrectly.\n  In this paper, we propose a kind of novel countermeasures based on\nadversarial attacks that is specifically designed against deep learning based\nside-channel attacks. We estimate several models commonly used in deep learning\nbased side-channel attacks to evaluate the proposed countermeasures. It shows\nthat our approach can effectively protect cryptographic devices from deep\nlearning based side-channel attacks in practice. In addition, our experiments\nshow that the new countermeasures can also resist classical side-channel\nattacks.",
  "text": "Adversarial Attack Based Countermeasures\nagainst Deep Learning Side-Channel Attacks\nRuizhe Gu, Ping Wang, Mengce Zheng, Honggang Hu, Nenghai Yu\nKey Laboratory of Electromagnetic Space Information, CAS\nUniversity of Science and Technology of China, Hefei 230026, China\n{zheruigu,wangpin,mczheng}@mail.ustc.edu.cn\nAbstract. Numerous previous works have studied deep learning algorithms applied\nin the context of side-channel attacks, which demonstrated the ability to perform\nsuccessful key recoveries. These studies show that modern cryptographic devices\nare increasingly threatened by side-channel attacks with the help of deep learning.\nHowever, the existing countermeasures are designed to resist classical side-channel\nattacks, and cannot protect cryptographic devices from deep learning based side-\nchannel attacks. Thus, there arises a strong need for countermeasures against deep\nlearning based side-channel attacks. Although deep learning has the high potential\nin solving complex problems, it is vulnerable to adversarial attacks in the form of\nsubtle perturbations to inputs that lead a model to predict incorrectly.\nIn this paper, we propose a kind of novel countermeasures based on adversarial\nattacks that is speciﬁcally designed against deep learning based side-channel attacks.\nWe estimate several models commonly used in deep learning based side-channel\nattacks to evaluate the proposed countermeasures. It shows that our approach can\neﬀectively protect cryptographic devices from deep learning based side-channel attacks\nin practice. In addition, our experiments show that the new countermeasures can\nalso resist classical side-channel attacks.\nKeywords: Side-Channel Attacks · Countermeasures · Adversarial Attack · Deep\nLearning\n1\nIntroduction\nSide-channel attacks (SCA) are a major threat to embedded devices [YSG+19]. They can\nuse only a limited budget to recover the keys of cryptographic devices. The side-channel\nattacks exploit the side-channel information of a cryptographic computation to recover\nsensitive data. The side-channel information includes power consumption, electromagnetic\nradiations, and running-time, etc. They can recover sensitive data values in very few\nside-channel observations. The proﬁling attacks [CRR02] are one of the most powerful\nside-channel attacks. In this scenario, the adversary may precisely tune all the parameters\nof the cryptographic device, and characterize the correlation between the physical leakage\nand sensitive data value. They can predict the sensitive value on a target device containing\na secret they wish to retrieve by using multiple traces.\nVery similar to proﬁling attacks, deep learning algorithms are also used in the context\nof side-channel attacks [CDP17,PSB+18,KPH+19,MPP16,Tim19]. Some recent studies\nhave demonstrated the robustness of deep learning techniques to the most common\ncountermeasures [MPP16, MDP20, CDP17]. Deep learning techniques are at least as\neﬀective as classical proﬁled attacks. Today, security components are embedded everywhere,\nso deep learning based side-channel attacks have become a major threat to many everyday\nlife objects. Facing the application of deep learning techniques in the context of SCA,\narXiv:2009.10568v1  [cs.CR]  22 Sep 2020\n2\nAdversarial Attack Based Countermeasures against Deep Learning SCA\nthe classical security protections designed to thwart classical side-channel attacks can no\nlonger protect modern security components. Therefore, there arises a strong need for new\ncountermeasures that can protect cryptographic devices against deep learning attacks.\n1.1\nRelated Work\nSingh et al. [SKM+18] exploited random dynamic voltage and frequency scaling to thwart\nSCA. Courousse et al. [CBR+16] presented a code morphing runtime framework to resist\nSCA. Boulet et al. [BBL13] described the protection of electronic devices against hidden-\nchannel analysis. The protection converts the original codes to functionally equivalent\ncodes by a modiﬁed compiler. Coron et al. [CK10] mitigated side-channel attacks by\nthe execution of dummy instructions. Ambrose et al. [ARP07] proposed to randomly\ninsert a limited set of randomly selected instructions. They argued that such instructions\ncould protect devices. As compared to our work, these previous works insert randomly\nselected instructions into the entire algorithm or the entire sensitive function. Our main\ncontribution is to select the best suitable noise instructions and determine the exact\ninsertion position.\nSome recent studies have demonstrated the robustness of deep learning techniques\nagainst the most common countermeasures [MPP16,MDP20,CDP17]. Therefore, there\narises a strong need for new countermeasures that can protect cryptographic devices against\ndeep learning attacks. In particular, to the best of our knowledge, the only former work\nthat uses adversarial attacks to resist SCA is carried out by Picek et al. [PJB19]. However,\ndiﬀerent from our work, they just modiﬁed each side-channel trace into adversarial samples.\nThe experiments in this paper show that turning each side-channel trace into an adversarial\ntrace is not an eﬀective countermeasure.\n1.2\nOur Contributions\nIn this paper, we present a kind of countermeasures against the deep learning based\nside-channel attacks. The key idea of our approach is to add adversarial perturbations to\nthe cryptographic algorithm implementation during compilation. We propose an approach\nto select adversarial perturbation instructions, and where to insert these instructions.\nMoreover, we also evaluate the security of our countermeasures by experiments.\nIn our experiments, we use two diﬀerent deep learning techniques to assess the security\nlevel of our countermeasures: Multilayer Perceptron (MLP) and Convolutional Neural\nNetwork (CNN). The experimental results show that our countermeasures can reach a high\nlevel of security under deep learning based SCA. We also evaluate the performance of our\napproach under the classical side-channel attacks. Template attacks (TA) are exploited to\nattack our countermeasures, and the experiment shows that our method can thwart such\nattacks.\n1.3\nOrganization\nThe paper is organized as follows. After describing notations and terminology in Section\n2.1, Sections 2.2 and 2.3 give some background on side-channel techniques and adversarial\nattacks.\nThe threat model is described in Section 3.\nIn Section 4, we describe our\ncountermeasures in detail. Some experiments are implemented in Section 5. Section 6\nconcludes this paper.\nRuizhe Gu et al.\n3\n2\nPreliminaries\n2.1\nNotations and Terminology\nIn this paper, k∗denotes secret keys, K denotes the set of all possible keys, and Dproﬁling\ndenotes the proﬁling dataset which contains the proﬁling traces dataset Tproﬁling and the\nproﬁling labels dataset Lproﬁling. The proﬁling traces dataset contains N proﬁling traces,\nand each trace is composed of n time samples. The proﬁling labels dataset contains the\nclasses/labels which follows the ML classiﬁcation meaning for each proﬁling trace. Dattack\ndenotes the attack dataset, and it contains Tattack and Lattack, where Tattack contains M\nattack traces. We train neural network using Dproﬁling and obtain a deep learning model.\nModel() denotes the model we trained.\nGiven an input trace, Model() aims to compute an output called a prediction vector\nd ∈Rm, where m represents the number of possible classes/labels corresponding to the\ninput trace. Each component in d represents the conﬁdence of corresponding possible\nclass/label. For example, in Figure 1, the label of the traces is the least signiﬁcant bit of\nthe third key byte corresponding to the traces, [0.4294791, 0.57052094] indicates that the\nconﬁdence of label 0 is 0.4294791, and the conﬁdence of label 1 is 0.57052094. The trace\nis classiﬁed as class 1, because the conﬁdence of class 1 is greater than the conﬁdence of\nclass 0.\nFigure 1: One-pixel attack on side-channel traces. The label of (a) is 0. The trace is\nlabeled with the least signiﬁcant bit of the output of the third Sbox during the ﬁrst\nround, LSB(Sbox(p[3] ⊕k[3])). The prediction vector of (a) is [0.8141893, 0.18581069], it\nis classiﬁed as class 0. So the classiﬁcation of (a) is correct. (b) is obtained by modifying\nthe value of (a) at the 440th time sample. (b) is incorrectly classiﬁed as class 1.\nBelow we introduce some deﬁnitions, which are related to adversarial attacks against\ndeep learning based side-channel attacks. The deﬁnitions of these terms are similar to\nthose of adversarial attacks in computer vision area [MMS+18]. The rest of this paper\nfollows these deﬁnitions.\n• Adversarial example/trace is a trace obtained by adding noise on the cryptographic\ndevices to obfuscate the deep learning classiﬁer.\n4\nAdversarial Attack Based Countermeasures against Deep Learning SCA\n• Adversarial perturbation is the noise added to the cryptographic devices when\ngenerating adversarial trace.\n• Black-box attacks mean that the adversary attacks a deep learning model without\nthe structure and parameters knowledge. When the adversary may have information\nabout the structure and parameters of the model, we call it white-box attacks.\n• Targeted attacks fool the deep learning models to make it misclassify adversarial\ntraces into speciﬁed target classes/labels. They are the opposite of non-targeted\nattacks. The goal of the non-targeted attack is to slightly modify the cryptographic\ndevices in a way that the side-channel trace will be classiﬁed incorrectly by generally\nunknown deep learning classiﬁer.\n• Universal perturbation means that the same perturbation is added to diﬀerent power\ntraces, which can make the traces misclassiﬁed by the classiﬁer.\n2.2\nSide-Channel Attacks\nIn the real world, cryptographic algorithms always rely on a physical carrier, such as a\nPC, smart card, or embedded processor. When a cryptographic algorithm is running on\na physical carrier, execution time [Koc96], power consumption [KJJ99], electromagnetic\nemissions [GMO01], and other side-channel information of a cryptographic computation\nare leaked. These side-channel leakages of a cryptographic computation depend on some\nsmall part of the internally used sensitive data or sensitive operations in the cryptographic\ndevices, and can be exploited to recover keys. A key-recovery attack based on side-channel\nleakage analysis is called a side-channel attack for simplicity.\n2.2.1\nTemplate Attack\nTA can be considered as the most successful and in-depth research method in classical\nSCA. In this paper, we use TA to evaluate the security level of our countermeasures.\nLet us consider the target device executing a cryptographic algorithm with the secret\nkey k∗. The adversary may control a copy of the target device called proﬁling device\nand priorly use it to precisely tune all the parameters of the cryptographic computation.\nFor each possible key k the adversary observes N (k) time over a time interval of n time\nsamples the power consumption of proﬁling device and we denote by trace the series of\nobservations T (k)\n(i) =\nn\nT (k)\n(i)(t) ∈R|t ∈[1; n]\no\n, i = 1, . . . , N (k). The most common TA model\nmodelizes the stochastic dependency between k and trace by means of a multivariate\nnormal conditional density:\nP\n\u0010\nT (k)\n(i) |k\n\u0011\n=\n1\np\n(2π)n |Σk|\ne−1\n2\n\u0000T (k)\n(i) −µk\n\u0001\nΣ−1\nk\n\u0000T (k)\n(i) −µk\n\u0001′\n(1)\nwhere µk ∈Rn and Σk ∈Rn×n are the expected value and the covariance of the n variate\ntraces respectively.\nIn the context of TA, two phases may be distinguished:\nProﬁling Phase.\nFor each possible key k, the adversary captures N (k) traces T (k)\n(i) over a\ntime interval of length n. TA estimates the expected value µk and the covariance Σk by\nˆµk =\n1\nN (k)\nN(k)\nX\ni=1\nT (k)\n(i)\n(2)\nRuizhe Gu et al.\n5\nˆΣk =\n1\nN (k) −1\nN (k)\nX\ni=1\n\u0010\nT (k)\n(i) −ˆµk\n\u0011⊤\u0010\nT (k)\n(i) −ˆµk\n\u0011\n(3)\nAttack Phase.\nThe attacker captures a trace T when the target device execute a crypto-\ngraphic algorithm. The adversary estimates the secret key which maximizes the likelihood:\nˆk = arg max\nk\nP (T|k)\n(4)\n2.2.2\nDeep Learning Based SCA\nDeep learning based side-channel attacks focus mainly on two techniques: multi-layer\nperceptrons (MLP) and convolutional neural networks (CNN) [CDP17]. Martinasek et\nal. [MDM16,MHM13,MMT15] compared MLP-based methods with other classical attack\nsuch as template attacks. Cagli et al. [CDP17] have shown that MLP-based attack is far\nmore eﬀective than other classical methods. Prouﬀet al. [PSB+18] have demonstrated\nthat CNN can obtain a great success in attacking cryptographic implementations with\njitter.\nDeep learning based SCA [MPP16] is similar to TA, but uses deep learning techniques\nas a proﬁling method instead of using multivariate Gaussian proﬁling as in TA. To train a\ndeep learning model, the typical leakage models used for the power consumption are the\nHamming Weight (HW) model (9-class classiﬁcation), and the Least Signiﬁcant Bit (LSB)\nmodel (2-class classiﬁcation) [PEvW18]. In this paper, we also uses these two leakage\nmodels.\nMLP.\nMLP is also called artiﬁcial neural networks. It contains at least three layers:\nin addition to the input and output layers, there can be multiple dense layers between\nthem. The number of neurons in the input layer is determined by the number of time\nsamples n in the input data. The MLP layer is fully connected (fully connected means\nthat each neuron in the upper layer is connected to all neurons in lower layer). If the\noutput of the lower layer is represented by a vector X, the output of the higher layer is\nf (wijx + bij), where wij and bij are the weight and bias of the j-th neuron in the i-th\nlayer respectively, and x ∈X. Generally, the function f is sigmoid or tanh. Finally, the\noutput layer can be viewed as multi-class logistic regression, i.e. softmax regression. Thus,\nthe output of the output layer is softmax(wx + b). The parameters of the MLP are all\nthe connection weight w and the bias b between the layers. The process of training these\ndeep learning models is the process of ﬁnding the optimal parameters. How to set the\noptimal parameters is an optimization problem. To solve the optimization problem, the\neasiest method is the gradient descent method.\nCNN.\nCNN can be regarded as a variant of MLP. In addition to the input layer, dense\nlayer and output layer, it also uses one or more convolutional layers and pooling layer.\nA convolution layer includes a convolution operation, followed by an activation function\n(such as ReLU) and a pooling layer. The pooling layer is used to reduce the dimensions.\nThe convolutional layer performs a series of convolutional operations on its inputs (each\ninput is convoluted with a ﬁlter).\n2.3\nAdversarial Attacks\nThe adversary may design a targeted machine learning sample (adversarial example/trace)\nto make the machine learning model misjudge. This is called an adversarial attack. Szegedy\net al. [SZS+13] ﬁrst discovered an interesting weakness of deep neural networks in image\n6\nAdversarial Attack Based Countermeasures against Deep Learning SCA\nclassiﬁcation. Their study shows that, despite the high accuracy of deep learning, it is\nsurprising that they are susceptible to adversarial attacks in the form of applying small\nadversarial perturbations on source images. Mohse et al. [MDFFF17] demonstrated the\nexistence of universal perturbations, which can be used to fool deep learning classiﬁers by\nadding it to arbitrary images. This work inspires us to protect cryptographic devices by\nadding universal perturbations.\nIn fact, the deep learning based side-channel attacks are also classiﬁcation problems.\nThey use deep learning techniques to classify side-channel traces. The labels for the\nside-channel traces are key-related values. Adversarial attacks can be seen as the process\nof seeking a vector v such that:\nModel(T + v) ̸= Model(T)\nside-channel tarce T ∈T . For each trace T, Model() outputs an estimated label Model(T).\nv is an adversarial perturbation. In the context of image classiﬁcation, in order to make the\nadversarial perturbation less perceptible, v is often restricted to satisfy certain restrictions.\nIf there is a v such that\nModel(T + v) ̸= Model(T) for “most\" T ∈T ,\nthen v is universal perturbation.\nThese universal perturbations are not only universal across side-channel traces, but also\ngeneralize well across deep learning models [SZS+13,KGB16,MDFFF17]. The deep learning\nmodels ﬁnd the decision boundaries of the data in the high-dimensional space. In order to\nmake v as small as possible, the adversarial perturbations are all in the neighbourhood\nof decision boundaries. Even diﬀerent models are used to classify side-channel traces,\nas long as the models are eﬃcient, the decision boundaries they ﬁnd are similar. Such\nperturbations are therefore doubly universal, both with respect to the data and the models.\nThat is, if we use one model to generate a set of universal perturbations, we can ﬁnd that\nthese perturbations are still eﬀective for another model even it was trained with diﬀerent\nhyperparameters or it is trained on a diﬀerent set of traces.\n2.3.1\nOne-Pixel Attack\nOne-pixel attack, a type of adversarial attack techniques, which fools the deep learning\nclassiﬁer by changing only one pixel in the image. In order to reduce the number of inserted\nnoise instructions, we hope our countermeasures to modify as few pixels as possible. Thus,\nin this paper we use one-pixel attack to calculate universal perturbations.\nSu et al. [SVS19] claim to achieve an extreme case in adversarial attacks, and they fool\nthe deep learning classiﬁer by changing only one pixel in the image. One-pixel attack is\nalso eﬀective on the side-channel traces, and we show in Figure 1 an adversarial sample\non the side-channel traces generated by one-pixel attack. The upper trace is the original\ntrace captured during the cryptographic computation. The label for the original trace (a)\nis 0, and the prediction vector calculated by classiﬁer is [0.8141893, 0.18581069]. The deep\nlearning classiﬁer can correctly classify the trace (a). We use one-pixel attack to generate\nan adversarial trace (b) based on the trace (a). Trace (b) is obtained by changing one\ntime sample in trace (a). The modiﬁed time sample is highlighted with blue rectangle. We\nuse classiﬁer to calculate the prediction vector of (b): [0.4294791, 0.57052094]. Trace (b)\nis incorrectly classiﬁed as class 1 by the deep learning classiﬁer.\nOne-pixel attack generates the adversarial samples using diﬀerential evolution algorithm\n[DS10]. Diﬀerential evolution (DE) is a population based optimization algorithm for solving\ncomplex multi-modal optimization problems [DS10, SP97]. The diﬀerential evolution\nalgorithm is composed of three phases: mutation, crossover, and selection. Mutation is\na method used to generate random solutions. Crossover is used to enhance the diversity\nRuizhe Gu et al.\n7\nof random solutions. Selection removes solutions that fail to evolve, and leaves solutions\nthat succeed in evolution. The diﬀerential evolution algorithm ﬂow is as follows: ﬁrst, a\nsuﬃcient number of random variables are generated as the initial possible solution. Then,\nthe mutation, crossover, and selection are performed in order. After completing a round, a\ncertain termination condition is checked. If the termination conditions have not been met,\nthe diﬀerential evolution algorithm returns to mutation, crossover, and selection; otherwise,\nthe algorithm terminates, and outputs the best solution of the last round.\nWhen we use adversarial attack to protect cryptographic devices, the less adversarial\nperturbations we insert, the easier our countermeasures can be implemented. For one-pixel\nattack, we only need to add noise at one time sample. Besides, one-pixel attack requires\nless network information as it is a black-box attack.\n3\nThreat Model\nThe adversary targets the secret key k∗of a cryptographic device. We call this cryptographic\ndevice the target device. The adversary has the same device as the target device, called the\nproﬁling device. We consider that the cryptographic device has suﬃcient computational\nresources to compile the code before each encryption (our countermeasures insert noise\ninstructions during compilation), and the adversary cannot get control over the code\ncompilation.\nWe also assume that the adversary does not use any preprocessing techniques on power\ntraces, but we argue that the preprocessing algorithm cannot break our countermeasures.\nThe eﬀect of preprocessing techniques on our countermeasures is discussed further in Section\n6. The adversary uses these two devices to carry out deep learning based side-channel\nattacks. The deep learning based SCA is divided into two phases:\nProﬁling Phase.\nThe attacks are performed on the 3rd key byte of the AES-128 , which\nis the same as previous work [PSB+18]. In this case, we can refer to this previous work to\nobtain an eﬀective deep learning model. For each key candidate value k, the adversary\ncaptures N (k) power traces. All these traces make up proﬁling traces dataset Tprofiling.\nIn order to analyze the eﬀectiveness of our countermeasures on deep learning models with\ndiﬀerent output classes, the side-channel traces are labeled using two leakage models: LSB\nand HW. The adversary trains neural network using Lprofiling and Tprofiling, and obtain\na deep learning model Model().\nAttack Phase.\nThe adversary captures M side-channel traces on the target device. For\neach T(i) ∈Tattack, the adversary uses the deep learning model Model() to get a prediction\nvector d(i) ∈R|K|:\nd(i) = Model\n\u0000T(i)\n\u0001\n.\n(5)\nThe adversary selects the key candidate with the highest sum conﬁdence as the secret key\nk, i.e. k = argmaxk∈K\n\u0010QM\ni=1 d(i)\n\u0011\n[k]. If k = k∗, the key recovery is successful.\n4\nAdversarial Attack Based Security Protections\n4.1\nDiﬀerences\nOne-pixel attacks were ﬁrst proposed to attack deep-learning models in the image classiﬁ-\ncation area. Although both deep learning based image classiﬁcation and deep learning\nbased side-channel attacks use deep learning models as classiﬁers, there are still many\ndiﬀerences between them.\n8\nAdversarial Attack Based Countermeasures against Deep Learning SCA\nDiﬀerent training sets.\nIn the context of image classiﬁcation, we train the deep learning\nmodel on source images. However, for the deep learning based SCA, the proﬁling traces are\ncaptured on the proﬁling device. In our threat model, the proﬁling device is a copy of target\ndevice and it is a cryptographic implementation with our countermeasures. Therefore, if our\ncountermeasures modify each trace to adversarial trace by adding adversarial perturbations,\nthen the proﬁling traces captured on proﬁling device are all adversarial traces. Due to the\nrobustness of deep learning technique, such countermeasures that insert perturbations to\nturn side-channel traces into adversarial traces cannot protect cryptographic devices. The\nattack results shown in Figure 2 conﬁrm this view.\nFigure 2: The mean rank of the unprotected AES and the one-pixel attacked AES on\nCNN-based attack. The rank is a metric to evaluate the security level of countermeasures\n(described in Section 5.2). For unprotected AES, approximately 100 traces are required\nfor a full success of the key recovery. For one-pixel attacked AES, performing successful\nkey recoveries requires approximately 20 traces.\nWe ﬁrst collect 60, 000 power traces of unprotected AES, and we call these 60, 000\ntraces source traces. 50, 000 source traces are used as the training set, and 10, 000 source\ntraces are used as the test set, and CNN is used to attack these source traces. The attack\nresult is shown as unprotected AES in Figure 2. We use the one-pixel attack to generate\n60, 000 adversarial traces, and we call these adversarial traces as one-pixel attacked AES\ntraces. We use 50, 000 one-pixel attacked AES traces as training set and 10, 000 as test set,\nand use CNN to attack these traces. The attack result is shown as one-pixel attacked AES\nin Figure 2. For unprotected AES, approximately 100 traces are required for a full success\nof the key recovery. For one-pixel attacked AES, performing successful key recoveries\nrequires approximately 20 traces. Figure 2 shows that converting the source traces to\nadversarial traces cannot protect the cryptographic devices, but makes the implementation\nmore vulnerable. The reason is that these adversarial traces have high conﬁdence in the\nwrong label, which is generally above 0.95. These adversarial traces can deceive models\ntrained on source traces. However, when adversary trains models on these adversarial\ntraces, these adversarial perturbations which fool the original model will instead become\nfeatures exploited by the deep learning techniques.\nDiﬀerent attack dataset sizes.\nIn the image classiﬁcation area, the purpose of the\nadversarial attack is to make a certain image misclassiﬁed by a deep learning model after\nadding perturbations that is not perceived by humans. We considered it a successful\nadversarial attack if the image was misclassiﬁed after adding perturbations. Therefore, for\nimage classiﬁcation area, the size of the attack dataset can be regarded as 1. In SCA area,\nto perform successful key recoveries, the adversary captures M power traces on the target\ndevice and selects the key candidate with the highest sum conﬁdence as the secret key,\nas mentioned in Section 3. The size of the attack set can be regarded as M. In order to\nRuizhe Gu et al.\n9\nthwart the deep learning SCA, our countermeasures need to modify all the power traces\ngenerated by cryptographic computations instead of modifying one trace.\n4.2\nOur Method\nOur countermeasures insert noise instructions into the code. The power consumption of\nthese noise instructions becomes universal perturbations. These universal perturbations\nmake power traces misclassiﬁed by the deep learning models, and then thwart the deep\nlearning SCA. In this process, we need to solve three problems: how to determine the\nposition where the noise instruction is inserted, which instructions are inserted into the\ncode as noise instructions, and how to insert the noise instruction at the selected position.\nWe address these issues in the following subsections.\n4.3\nLocations of Noise Insertion\nWe want to insert noise instructions at the locations where the universal perturbations are\nlocated. Therefore, we generate universal perturbations, and observe their positions on\npower traces. Before calculating the universal perturbations, we need to determine what\nkind of universal perturbations we need to calculate. Diﬀerent universal perturbations have\ndiﬀerent eﬀects on the deep learning classiﬁers. Some universal perturbations make the\nconﬁdence of a certain class very large, but some make the conﬁdence of a certain class\nvery small. These eﬀects depend on the termination condition of adversary attacks.\nThis subsection calculates the locations where the universal perturbations are located\nbased on the 2-class model (the traces are labeled as LSB of sensitive value). We analyze at\nthe end of this subsection that the positions calculated by 2-class model and 9-class model\nare close. We consider the formula k = argmaxk∈K\n\u0010QM\ni=1 d(i)\n\u0011\n[k], used by the adversary\nto recover the secret key. The adversary selects the key candidate with the highest sum\nconﬁdence as the secret key. To prevent the adversary from recovering the correct key, we\ncan make the conﬁdence corresponding to a certain class of all side-channel traces very large\nor make the conﬁdence corresponding to each class the same. In this way, the adversary\ncannot recover the correct key. We test two termination conditions: the ﬁrst termination\ncondition is that the algorithm terminates when each trace is classiﬁed as 0, and make\nthe conﬁdence of label 0 the as large as possible, i.e. dk=0 ⩾τ, where dk=0 denotes the\nconﬁdence corresponding to label 0 and τ is a constant close to 1. Another termination\ncondition: the algorithm terminates when the diﬀerence between the conﬁdence of label 0\nand label 1 within a small range, i.e. |dk=0 −dk=1| ⩽σ, where σ is a constant close to 0.\nOur experiments ﬁnd that the second termination condition is computationally intensive\nwhen running diﬀerential evolution algorithms. Moreover, when the leakage model is not\nLSB, the labels of the traces are no longer only 0 and 1, and the second termination condition\nis hard to implement. Thus, considering eﬃciency and versatility, our countermeasures\nuse the ﬁrst termination condition. The steps of calculating the insertion position are as\nfollows:\n1. We capture 60, 000 labelled traces of the power consumption of unprotected AES\nimplementation. The temporal acquisition window is set to record the ﬁrst round\nof the AES only. Each trace is composed of 5200 time samples. We experimentally\nvalidate that the deep learning classiﬁer trained on 50, 000 proﬁling traces can\nsuccessfully recover the key in less than 1000 attack traces. Therefore, we select\n50, 000 traces out of 60, 000 traces as proﬁling traces, and train deep learning models\non these proﬁling traces.\n2. Diﬀerential evolution algorithm is applied to generate adversarial perturbations based\non the remaining 10, 000 traces. The termination condition is that the algorithm\nterminates when the trace is classiﬁed as 0.\n10\nAdversarial Attack Based Countermeasures against Deep Learning SCA\n3. We generate 10, 000 adversarial perturbations based on MLP and CNN respectively.\nThe distribution of the 10, 000 adversarial perturbations on 5200 time samples is\nshown in Figures 3(a) and 3(b). Figures 3(a) and 3(b) show that when attacking\nCNN and MLP, the distributions of adversarial perturbation are similar. They all\nhave the largest distribution around the three time samples: 1900th time sample,\n2560th time sample, and 4300th time sample. The perturbations near these three\npositions are universal perturbations.\n4. We use these three points as the locations of the noise insertion. Noise instructions\ninserted near these three time samples can generate universal perturbations.\n(a) 2-class MLP model.\n(b) 2-class CNN model.\n(c) 9-class MLP model.\nFigure 3: Distribution of adversarial perturbation. The horizontal axis represents 5200\ntime samples of the side-channel trace, and the vertical axis represents the number of\nadversarial perturbations falling on a certain time sample.\nIt can be observed in the power traces of AES that these three time samples are\nrespectively included in three functions: AddRoundKey(), SubBytes() and MixColumns().\nThese functions may contain thousands of instructions, and we need to know where these\ntime samples are in the cryptographic code more accurately. Only by knowing the speciﬁc\nlocation of the noise in the code can we accurately insert noise instructions into the code.\nThe C ﬁle is compiled to an assembly code ﬁle. We use the binary search algorithm to\ntraverse the instructions of the assembly ﬁle, and insert the trigger_low() function after the\ninstruction. This function is also the trigger signal used when we collect the power traces.\nThen we compile and run the ﬁle again, and observe whether the power trace becomes\nlow level near the three time samples we selected earlier. If it becomes low level, then the\nposition of trigger_low() is the position where we want to insert the noise instructions. If\nnot, we continue to traverse the assembly ﬁle, and repeat the previous steps. In the process\nof determining the position of insertion, we ﬁnd that even if the position of inserting the\nlow-level signal is the same, the position of the low-level on the power trace is not the\nsame, but their positions are very close on the power trace. The universal perturbations\nwe generated are not all at a certain time sample but are concentrated around those three\nRuizhe Gu et al.\n11\nsamples. Our experiments also ﬁnd that if a perturbation with side-channel proﬁle which\nis similar to the side-channel proﬁle of universal perturbation is inserted near those three\nsamples, the deep learning model can be deceived. Our purpose is only to ﬁnd a fuzzy\nlocation, so that the side-channel leakage generated by the noise instructions is near to\nthose three sampling points.\nWe use the 9-class MLP model to generate the adversarial perturbations, and observe\ntheir distribution on the time sample. The results are shown in Figure 3(c). The distribution\nof these perturbations is similar to the distribution of perturbations generated by the\n2-class model. Since universal perturbations are doubly universal, even if we use diﬀerent\ndeep learning models to generate universal perturbations, the positions of these universal\nperturbations are close.\n4.4\nChoice of Noise Instructions\nPrevious works [CBR+16,BCHC18,AMN+11,ABP19] insert noise instructions between\neach useful instruction in sensitive function Sbox. These inserted noise instructions are\ncommonly used in cryptographic algorithms. However, the purpose of our noise insertion is\ndiﬀerent from these previous literatures. In previous works, the purpose of inserting noise\nis to move the point of information leakage in time and space, and to reduce side-channel\nleakage. Thus, in these previous literatures, the inserted noise instructions do not need\nto be carefully selected. The purpose of our noise instruction is to make the captured\nside-channel traces into adversarial examples.\nSo the instructions need to meet the following requirements: 1) the side-channel proﬁle\n(i.e. power consumption or electromagnetic radiation) of noise instructions should be as\nclose as possible to the proﬁle of useful instructions, so that the adversary cannot distinguish\nthem and ﬁlter them out from the side-channel traces [DRS+12]; 2) the side-channel proﬁle\nof the inserted instructions should be similar to the proﬁle of the adversarial perturbations.\nThe ﬁrst requirement is easy to achieve, we only need to choose the instructions commonly\nused in cryptographic algorithms, such as addition, subtraction, exclusive or, and load. In\norder to meet the second requirement, the side-channel proﬁle of adversarial perturbations\nshould be taken into account. We analyze the distribution of adversarial perturbations\nover amplitude.\nWe perform one-pixel attacks on the 2-class MLP model and CNN model, and generate\n10, 000 adversarial perturbations. Although we use the 2-class model to generate adversarial\nperturbations, we analyze later that the amplitude distribution of the perturbation gener-\nated by 9-class model is the same as the distribution of 2-class model. In the diﬀerential\nevolution algorithm, we limit the position of adversarial perturbation to the vicinity of\nthree time samples we select in Section 4.3. We show the amplitude distribution of these\nadversarial perturbations near 1900th time sample and 2560th time sample in Figures 4\nand 5.\nFigure 4 shows the amplitude distribution of adversarial perturbations on MLP model.\nThe most distributed amplitudes of adversarial perturbations are −5.2 and 4.8. Figure\n5 shows the amplitude distribution of the adversarial perturbations on the CNN model.\nIn Figure 5, the amplitudes of −5 and 3.8 have the most adversarial perturbations. In\norder to deceive both CNN and MLP, the amplitude that the noise instructions need to\ngenerate is within the interval [−5.2, −5] or [3.8, 4.8], such perturbations are more likely\nto become universal perturbation. The purpose of choosing two amplitude intervals as our\ncriterion for selecting noise instructions is to be able to ﬁnd more instructions that meet\nthe requirements, and to make these noise instructions eﬀective for various deep learning\nmodels.\nIn experiments, we target AES implementations running over an ARM Cortex-M3\nprocessor. ARM Thumb1 and Thumb2 instructions are treated as a candidate noise\ninstruction set. We capture the energy consumption traces of instructions candidate on the\n12\nAdversarial Attack Based Countermeasures against Deep Learning SCA\n(a) Amplitude distribution of perturbations near the 1900th time sample\n(b) Amplitude distribution of perturbations near the 2560th time sample\nFigure 4: Amplitude distribution of adversarial perturbation on MLP model. The values\non the horizontal axis corresponds to the amplitude of power traces. The above picture\nshows the amplitude distribution of perturbations near the 1900th time sample, and the\nbelow picture shows the amplitude distribution of perturbations near the 2560th time\nsample. We divide the interval of the amplitude [−5.2, 4.8] into 160 discrete intervals.\ncryptographic device, and select the instruction that can generate a suitable perturbation\nsize as the noise instruction.\nThe power consumption is not only related to instructions, but also related to operated\nconstants. Generally speaking, 0xff causes greater power consumption. In this paper, we\nchose the four instructions listed in Listing 1 as our noise instructions. r24 in the listing\nmay be any free register, which is determined by the compiler during compilation.\nWe use 9-class MLP to generate the adversarial perturbations, and observe their\ndistribution. The results are shown in Figure 6. As with the 2-class model, the perturbations\ngenerated by the 9-class model is concentrated in the largest and smallest amplitude.\nThe noise instructions which we select using 2-class model can still generate universal\nperturbation in 9-class model.\nListing 1: Inserted noise instructions\nmov r24 ,\n0 x f f\nori\nr24 ,\n0 x f f\nldi\nr24 ,\n0 x f f\nin\nr24 ,\n0x3d\n4.5\nInserting Noise Instructions\nThe last step of our countermeasures is to insert the selected noise instructions into the\nselected positions. In this work, we insert noise instructions into the code at compile time.\nRuizhe Gu et al.\n13\n(a) Amplitude distribution of perturbations near the 1900th time sample\n(b) Amplitude distribution of perturbations near the 2560th time sample\nFigure 5: Amplitude distribution of adversarial perturbation on CNN model. The values\non the horizontal axis corresponds to the amplitude of power traces. The above picture\nshows the amplitude distribution of perturbations near the 1900th time samples, and the\nbelow picture shows the amplitude distribution of perturbations near the 2560th time\nsamples. We divide the interval of the amplitude [−5.2, 4.8] into 160 discrete intervals.\nWe start by annotating the assembly ﬁle at target positions. The annotated assembly ﬁle\nis recompiled before each invocation of the cryptosystem: when the compiler recognizes\nthese annotations, it randomly picks ω noise instructions from Listing 1, and inserts\nthem to the code, where ω is an integer in {0, 1, 2}. The purpose of inserting diﬀerent\nnumbers of instructions is to increase the diversity of the code. In order to ensure that\nthe side-channel leakage of each invocation of the cryptosystem becomes an adversarial\nsample, our approach requires that the cryptographic device recompiles the code at each\ninvocation.\n5\nExperimental Evaluation\nWe evaluate our countermeasures as a defense against deep learning based SCA. In order\nto demonstrate that our countermeasures are also eﬀective for the classical side-channel\nattacks, we perform template attacks on our countermeasures. For convenience, we refer\nto our countermeasures as one-pixel protection AES in the following content.\n5.1\nExperimental Setup\nWe use a STM32F3 board ﬁtted with an Arm Cortex-M3 core running at 32MHz, 16kB of\nRAM, and 128kB of ﬂash memory. It does not provide any hardware security mechanisms\nagainst SCA. Our AES implementation is an unprotected 8-bit implementation that follows\nthe NIST speciﬁcation.\n14\nAdversarial Attack Based Countermeasures against Deep Learning SCA\nFigure 6: Amplitude distribution of adversarial perturbation near the 1900th time sample\non 2-class MLP model.\nThe side-channel traces are obtained with a Pico5444B PicoScope. The sampling\nacquisition is performed at 96 Msample/s. In this scenario, the length of one processor\ncycle on the side channel trace is three time samples. To ease the temporal alignment\nof the side-channel traces, a trigger signal is set, and held high during the execution of\nthe ﬁrst AES round. Using this setup, the security evaluation is performed with stricter\nconditions than it would be in practice for an adversary.\n5.2\nEvaluation Metrics\nWe use two metrics to evaluate the performance of diﬀerent AES implementations against\nattacks, which are the rank function and the accuracy.\nWith the same previous notations in Section 2.1, we deﬁne the score function SM[k] of\nthe key candidates k :\nSM[k] =\nM\nY\ni=1\nd(i)[k].\n(6)\nAccording to Equation 6 we can deﬁne the rank function:\nrank (Model, Dproﬁling, Dattack, M) = |{k ∈K|SM[k] > SM [k∗]}| .\n(7)\nWhen the rank of k∗is 0, we perform a successful key recovery. The larger the M required\nto recover the correct key, the better the implementation performs against side-channel\nattacks. To get a better measure of the rank, it is more suitable to estimate its mean value\nover several pairs of datasets.\nThe second metric is the accuracy which is commonly used in machine learning. We\ndeﬁne it as:\nacc (Model, Dproﬁling , Dattack ) =\n\f\f\b\nk∗∈K|k∗= argmaxk∈K d(i)[k]\n\t\f\f\n|Dattack|\n.\n(8)\nIn this paper, accuracy is used to evaluate the performance of 2-class models. The numbers\nof elements of each class are equal. Thus, it is adequate using the accuracy as metric. The\nlower the accuracy, the better the security of the countermeasure.\n5.3\nResistance to Practical Attacks\nIn this section, we use MLP and CNN to attack three diﬀerent AES implementations,\nincluding unprotected AES implementation, random noise AES and one-pixel protection\nAES. In order to demonstrate that the eﬀectiveness of one-pixel protection AES is due\nto the carefully selected insertion position and noise instructions, we implement random\nnoise AES: randomly inserting noise instructions at three random locations in the ﬁrst\nRuizhe Gu et al.\n15\nround of AES. The inserted noise instructions are randomly selected among instructions\nthat are commonly used in AES programs.\nWe capture 60, 000 power traces for each AES implementation, and each trace is\ncomposed of 5, 200 time samples. To get a better measure of the rank function and the\naccuracy, we therefore need to calculate their mean value on several pairs of data sets.\nAmong the 60, 000 traces, 10, 000 traces are randomly selected as the test set, and the\nremaining 50, 000 traces are used as the training set. Repeating this process 10 times, we\nget 10 diﬀerent data sets.\nIn order to analyze the eﬀectiveness of our method on deep learning models with\ndiﬀerent output classes, we train four deep learning models: 9-class CNN model (HW\nleakage model), 2-class CNN model (LSB leakage model), 9-class MLP model and 2-class\nMLP model. The CNN and MLP architecture used in this paper refers to [PSB+18]. Since\nour target are unmasked AES, we reduce the number of epochs. For MLP, the activation\nfunction is Relu and softmax, the optimizer is RMSprop, the learning rate is 0.00001, batch\nsize is 256, and the number of epochs is 100. For CNN, we use the Softmax activation\nfunction in the classiﬁcation layer combined with the Categorical/Binary Cross Entropy\nloss function. The learning rate is 0.0001, the optimizer is RMSprop, batch size is 256,\nand the number of epochs is 10.\n5.3.1\n2-Class Model\nFigure 7 shows the results of MLP-based attack on three AES implementations. (a), (b),\nand (c) respectively represent the unprotected AES implementation, the random noise\nAES and the one-pixel protection AES. Figure 8 shows the results of CNN-based attacks\non these three AES implementations. For unprotected AES implementation, MLP and\nCNN recover secret key in 340 and 300 traces respectively. For random noise AES, MLP\ncannot retrieve the key in less than 10, 000 traces, and CNN needs about 2650 traces to\nrecovery keys. For one-pixel protection AES, MLP and CNN cannot retrieve the secret\nkey in less than 10, 000 traces.\nThe inserted noise instructions make the power traces of random noise AES into\ndesynchronized traces. Therefore, MLP cannot recover the key of random noise AES. The\nconvolution layer is the main diﬀerence between CNN and MLP, and it allows the former\nhas the property of shift-invariant [CDP17]. Because of this, CNN can still recover the\nkey of random noise AES in presence of desynchronization.\nAlthough CNN can learn shift-invariant features, our countermeasures can still thwart\nCNN attacks by inserting noise. The noise instructions and insertion positions are carefully\nselected, and they can generate universal perturbations. Such universal perturbations are\ndoubly universal, both with respect to the data and the network architecture. Therefore,\nwe use a model trained on unprotected AES traces to generate a set of universal pertur-\nbations, which can still deceive other models, even when it was trained with diﬀerent\nhyperparameters or when it was trained on a diﬀerent set of traces.\nWe calculate correlation factor for the captured side-channel traces (see Figure 9). The\nadversary usually selects the points that leak the most information by calculating the\ncorrelation factors of the traces. In general, samples with a large correlation leak more\nside-channel information. Figure 9 shows that the distribution of correlation factors is very\nsimilar to the distribution of adversarial perturbations. Therefore, the positions where we\ninsert the noise are the positions with larger correlation. This makes it more diﬃcult for\nthe adversary to recover the key.\nTable 1 shows the mean accuracy of three AES implementations attacked by deep\nlearning based side-channel attacks. In the experiments in this subsection, the trace is\nlabeled with the LSB of the output of the third Sbox during the ﬁrst round. So, among the\nside-channel traces, two classes may be distinguished: 0 and 1. When the accuracy is closer\nto 0.5, it means that the corresponding AES achieves better resistance to side-channel\n16\nAdversarial Attack Based Countermeasures against Deep Learning SCA\n(a) Unprotected AES\n(b) Random noise AES\n(c) One-pixel protection AES\nFigure 7: The mean rank of diﬀerent AES implementations on MLP-based attacks. (a)\nunprotected AES, 340 traces are required for a successful key recovery. (b) Random noise\nAES, MLP cannot retrieve the secret key in 10, 000 traces. (c) One-pixel protection AES,\nMLP cannot retrieve the secret key in 10, 000 traces.\n(a) Unprotected AES\n(b) Random noise AES\n(c) One-pixel protection AES\nFigure 8: The mean rank of diﬀerent AES implementations on CNN-based attacks. (a)\nunprotected AES, 300 traces are required for a successful key recovery. (b) Random-noise\nAES, 2650 traces are required for a successful key recovery. (c) One-pixel protection AES,\nCNN cannot retrieve the secret key in less than 10, 000 traces.\nRuizhe Gu et al.\n17\nFigure 9: Correlation of side-channel traces\nattacks. The table shows that our security protection is very eﬀective, making the accuracy\nvery close to 0.5. In this situation, the deep learning model can hardly correctly classify\nthe side-channel traces.\nTable 1: Mean accuracy of diﬀerent AES implementations attacked by MLP and CNN\nModel\nMean accuracy\nAES\nUnprotected\nRandom-\nnoise\nOne-pixel\nprotection\nCNN\n0.7231\n0.6296\n0.5063\nMLP\n0.7083\n0.5004\n0.5023\n5.3.2\n9-Class Model\nIn the experiments in this subsection, the trace is labeled with the HW of the third output\nbytes of the Sbox during the ﬁrst round. To compare with one-pixel protection AES, we\nalso use 9-class model to attack unprotected AES. As in the previous sections, we have a\nset of 50, 000 power traces for the proﬁling phase and have a set of 10, 000 power traces\nfor the attack phase.\nFigures 10 and 11 show respectively the mean rank of diﬀerent AES implementations\non 9-class MLP attack and CNN attack. The attack results are similar to those of the\n2-class model. One-pixel protection AES can thwart 9-class CNN and MLP attacks. The\nreason is that the perturbations generated by inserted noise instructions are universal\nperturbations, and they can deceive models trained with diﬀerent hyperparameters.\n(a) Unprotected AES\n(b) One-pixel protection AES\nFigure 10: The mean rank of diﬀerent AES implementations on 9-class MLP attacks.\n(a) unprotected AES, 10 traces are required for a successful key recovery. (b) One-pixel\nprotection AES, MLP cannot retrieve the secret key in 10, 000 traces.\n18\nAdversarial Attack Based Countermeasures against Deep Learning SCA\n(a) Unprotected AES\n(b) One-pixel protection AES\nFigure 11: The mean rank of diﬀerent AES implementations on 9-class CNN attacks.\n(a) unprotected AES, 50 traces are required for a successful key recovery. (b) One-pixel\nprotection AES, CNN cannot retrieve the secret key in 10, 000 traces.\n5.4\nResistance to Classical Side-Channel Attacks\nTA are considered as the most successful method in classical SCA. We use TA to evaluate\nthe security level of our countermeasures. We use the TA algorithm described in Section\n2.2.1. We perform template attacks exploiting quadratic discriminant analysis (QDA)\nwhich is a well-known generative strategy in the machine learning literature [Fis36] to\nperform classiﬁcation. We perform QDA on power traces composed of 5, 200 time samples,\nand do not perform any dimension reduction operation before the TA. As in previous\nsections, we have a set of 50, 000 power traces for the proﬁling phase and have a set of\n10, 000 power traces for the attack phase. The attack results are illustrated in Figure 12.\nFigure 12: Rank of the correct key attacked by a QDA (Template Attack). For unprotected\nAES, the key can be retrieved with 560 traces. For one-pixel protection AES and random\nnoise AES, within 10, 000 traces, the key cannot be retrieved.\nFigure 12 illustrates that, TA only requires 560 traces to perform a successful key\nrecovery on unprotected AES, but it cannot retrieve the secret key of one-pixel protection\nAES and random noise AES in less than 10, 000 traces. This demonstrates that, not\nonly for deep learning based SCA, one-pixel protection AES is also eﬀective for classical\nside-channel attacks. The performance of TA highly depends on some preliminary phases,\nsuch as the traces realignment or the selection of the points of interest. Our method can\nthwart TA for two reasons: 1) we insert noise instructions, and these noise instructions are\nof variable length, which causes the power traces to be synchronized; 2) the positions of\nthe noise instructions we insert is also the point where the power traces have the greatest\ncorrelation, which reduce the correlation between the key k and the power traces.\nRuizhe Gu et al.\n19\n5.5\nExecution Time Overhead\nTable 2 compares the execution time (in cycles, measured for 1000 executions of each AES\nimplementation) of the unprotected AES and our countermeasures. The unprotected AES\nexecutes in 5482 processor cycles. The one pixel protection in 15952 to 21328 processor\ncycles (average 16418 cycles). Table 2 shows that our countermeasures lead to an increase\nin execution time overhead. The increased execution time overhead is mainly caused by\nthe recompilation at each execution.\nTable 2: Execution time of the unprotected AES and one-pixel protection AES\nUnprotected\nOne-pixel protection\nmin.\navg.\nmax.\nmin.\navg.\nmax.\nExecution time\n(cycles)\n5479\n5482\n5486\n15952\n16418\n21328\n6\nDiscussion and Conclusions\nWe argue that the current preprocessing algorithms cannot break our countermeasures. In\norder to break countermeasures of inserting noise instructions, some previous works use\ncorrelation analysis techniques (e.g. Hidden Markov Models [DRS+12]) to detect diﬀerent\ntypes of patterns in leakage traces. So that the adversary can distinguish noise instructions\nand ﬁlter them out from the side-channel traces. The reason for the eﬀectiveness of\nsuch correlation analysis techniques is that the side-channel proﬁle of noise instructions\nis diﬀerent from the proﬁle of useful instructions.\nOur countermeasures insert noise\ninstructions that are often used in programs, and recompile the code at each invocation.\nAlthough this increases the execution time, it ensures that the inserted noise instructions\ndo not have distinguishable headers and tails. Therefore, our countermeasures will not\nbe broken by such techniques. In the context of image classiﬁcation, there are some\nworks that use dimension reduction techniques to thwart adversarial attacks (e.g. image\ncompression [DGR16] , Principal Component Analysis [HG16]). Moreover, most of the\nexisting dimension reduction are less eﬀective. Their works demonstrate that dimension\nreduction techniques can reduce the interference of adversarial samples to the model,\nHowever, it also reduces the accuracy of the model’s classiﬁcation of normal examples.\nExisting methods for generating adversarial examples generally optimize the real\nexamples or add perturbations to the real examples based on the gradient of the model.\nGAN can train a generator to generate adversarial examples without adding disturbances\nto speciﬁc samples. The samples generated by GAN have the advantage of being more\ndiverse. However, GAN also has the problem that the generated samples change too\nmuch compared to the real samples. How to design constraint functions to ensure that\nGAN can generate adversarial examples of the target category without introducing huge\nperturbations is the direction of our future work.\nIn this paper, we present a new direction for achieving protection of cryptographic\ndevices through one-pixel attack techniques. Based on the one-pixel attack techniques,\nwe ﬁnd the most vulnerable time samples on the side-channel observations, and ﬁnd\nthe noise instructions that may deceive the deep learning models. We implement our\ncountermeasures and conduct experiments to evaluate the security level. Experiments\nshow that our countermeasures can protect cryptographic devices against deep learning\nside-channel attacks. Our method is also eﬀective enough against classical side-channel\nattacks, which makes it more competitive.\n20\nAdversarial Attack Based Countermeasures against Deep Learning SCA\nReferences\n[ABP19]\nGiovanni Agosta, Alessandro Barenghi, and Gerardo Pelosi. Compiler-based\ntechniques to secure cryptographic embedded software against side channel\nattacks. IEEE Transactions on Computer-Aided Design of Integrated Circuits\nand Systems, 2019.\n[AMN+11]\nAntoine Amarilli, Sascha Müller, David Naccache, Daniel Page, Pablo Rauzy,\nand Michael Tunstall. Can code polymorphism limit information leakage? In\nIFIP International Workshop on Information Security Theory and Practices,\npages 1–21. Springer, 2011.\n[ARP07]\nJude Angelo Ambrose, Roshan G Ragel, and Sri Parameswaran.\nRijid:\nrandom code injection to mask power analysis based side channel attacks. In\nProceedings of the 44th annual Design Automation Conference, pages 489–492,\n2007.\n[BBL13]\nFrederic Boulet, Michael Barthe, and Thanh-ha Le.\nProtection of ap-\nplets against hidden-channel analyses, November 21 2013. US Patent App.\n13/997,136.\n[BCHC18]\nNicolas Belleville, Damien Couroussé, Karine Heydemann, and Henri-Pierre\nCharles. Automated software protection for the masses against side-channel\nattacks. ACM Transactions on Architecture and Code Optimization (TACO),\n15(4):47, 2018.\n[CBR+16]\nDamien Couroussé, Thierno Barry, Bruno Robisson, Philippe Jaillon, Olivier\nPotin, and Jean-Louis Lanet. Runtime code polymorphism as a protection\nagainst side channel attacks. In IFIP International Conference on Information\nSecurity Theory and Practice, pages 136–152. Springer, 2016.\n[CDP17]\nEleonora Cagli, Cécile Dumas, and Emmanuel Prouﬀ. Convolutional neural\nnetworks with data augmentation against jitter-based countermeasures. In\nInternational Conference on Cryptographic Hardware and Embedded Systems,\npages 45–68. Springer, 2017.\n[CK10]\nJean-Sébastien Coron and Ilya Kizhvatov. Analysis and improvement of\nthe random delay countermeasure of ches 2009. In International Workshop\non Cryptographic Hardware and Embedded Systems, pages 95–109. Springer,\n2010.\n[CRR02]\nSuresh Chari, Josyula R Rao, and Pankaj Rohatgi. Template attacks. In\nInternational Workshop on Cryptographic Hardware and Embedded Systems,\npages 13–28. Springer, 2002.\n[DGR16]\nGintare Karolina Dziugaite, Zoubin Ghahramani, and Daniel M Roy. A\nstudy of the eﬀect of jpg compression on adversarial images. arXiv preprint\narXiv:1608.00853, 2016.\n[DRS+12]\nFrançois Durvaux, Mathieu Renauld, François-Xavier Standaert, Loic van Old-\neneel tot Oldenzeel, and Nicolas Veyrat-Charvillon. Eﬃcient removal of ran-\ndom delays from embedded software implementations using hidden markov\nmodels. In International Conference on Smart Card Research and Advanced\nApplications, pages 123–140. Springer, 2012.\n[DS10]\nSwagatam Das and Ponnuthurai Nagaratnam Suganthan. Diﬀerential evo-\nlution: A survey of the state-of-the-art. IEEE transactions on evolutionary\ncomputation, 15(1):4–31, 2010.\nRuizhe Gu et al.\n21\n[Fis36]\nRonald A Fisher. The use of multiple measurements in taxonomic problems.\nAnnals of eugenics, 7(2):179–188, 1936.\n[GMO01]\nKarine Gandolﬁ, Christophe Mourtel, and Francis Olivier. Electromagnetic\nanalysis: Concrete results. In International workshop on cryptographic hard-\nware and embedded systems, pages 251–261. Springer, 2001.\n[HG16]\nDan Hendrycks and Kevin Gimpel. Early methods for detecting adversarial\nimages. arXiv preprint arXiv:1608.00530, 2016.\n[KGB16]\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in\nthe physical world. arXiv preprint arXiv:1607.02533, 2016.\n[KJJ99]\nPaul Kocher, Joshua Jaﬀe, and Benjamin Jun. Diﬀerential power analysis. In\nAnnual International Cryptology Conference, pages 388–397. Springer, 1999.\n[Koc96]\nPaul C Kocher. Timing attacks on implementations of diﬃe-hellman, rsa, dss,\nand other systems. In Annual International Cryptology Conference, pages\n104–113. Springer, 1996.\n[KPH+19]\nJaehun Kim, Stjepan Picek, Annelie Heuser, Shivam Bhasin, and Alan Han-\njalic. Make some noise. unleashing the power of convolutional neural networks\nfor proﬁled side-channel analysis.\nIACR Transactions on Cryptographic\nHardware and Embedded Systems, pages 148–179, 2019.\n[MDFFF17] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal\nFrossard. Universal adversarial perturbations. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1765–1773, 2017.\n[MDM16]\nZdenek Martinasek, Petr Dzurenda, and Lukas Malina.\nProﬁling power\nanalysis attack based on mlp in dpa contest v4. 2. In 2016 39th International\nConference on Telecommunications and Signal Processing (TSP), pages 223–\n226. IEEE, 2016.\n[MDP20]\nLoïc Masure, Cécile Dumas, and Emmanuel Prouﬀ. A comprehensive study of\ndeep learning for side-channel analysis. IACR Transactions on Cryptographic\nHardware and Embedded Systems, pages 348–375, 2020.\n[MHM13]\nZdenek Martinasek, Jan Hajny, and Lukas Malina. Optimization of power\nanalysis using neural network. In International Conference on Smart Card\nResearch and Advanced Applications, pages 94–107. Springer, 2013.\n[MMS+18]\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,\nand Adrian Vladu. Towards deep learning models resistant to adversarial\nattacks. In International Conference on Learning Representations, 2018.\n[MMT15]\nZdenek Martinasek, Lukas Malina, and Krisztina Trasy. Proﬁling power\nanalysis attack based on multi-layer perceptron network. In Computational\nProblems in Science and Engineering, pages 317–339. Springer, 2015.\n[MPP16]\nHoussem Maghrebi, Thibault Portigliatti, and Emmanuel Prouﬀ. Breaking\ncryptographic implementations using deep learning techniques. In Interna-\ntional Conference on Security, Privacy, and Applied Cryptography Engineering,\npages 3–26. Springer, 2016.\n[PEvW18]\nGuilherme Perin, Baris Ege, and Jasper van Woudenberg. Lowering the bar:\nDeep learning for side channel analysis. 2018.\n22\nAdversarial Attack Based Countermeasures against Deep Learning SCA\n[PJB19]\nStjepan Picek, Dirmanto Jap, and Shivam Bhasin. Poster: When adversary\nbecomes the guardian–towards side-channel security with adversarial attacks.\nIn Proceedings of the 2019 ACM SIGSAC Conference on Computer and\nCommunications Security, pages 2673–2675, 2019.\n[PSB+18]\nEmmanuel Prouﬀ, Remi Strullu, Ryad Benadjila, Eleonora Cagli, and Cecile\nDumas. Study of deep learning techniques for side-channel analysis and\nintroduction to ascad database. IACR Cryptology ePrint Archive, 2018:53,\n2018.\n[SKM+18]\nArvind Singh, Monodeep Kar, Sanu Mathew, Anand Rajan, Vivek De, and\nSaibal Mukhopadhyay. Exploiting on-chip power management for side-channel\nsecurity. In 2018 Design, Automation & Test in Europe Conference & Exhi-\nbition (DATE), pages 401–406. IEEE, 2018.\n[SP97]\nRainer Storn and Kenneth Price. Diﬀerential evolution–a simple and eﬃcient\nheuristic for global optimization over continuous spaces. Journal of global\noptimization, 11(4):341–359, 1997.\n[SVS19]\nJiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai.\nOne pixel\nattack for fooling deep neural networks. IEEE Transactions on Evolutionary\nComputation, 2019.\n[SZS+13]\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru\nErhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural\nnetworks. arXiv preprint arXiv:1312.6199, 2013.\n[Tim19]\nBenjamin Timon. Non-proﬁled deep learning-based side-channel attacks with\nsensitivity analysis. IACR Transactions on Cryptographic Hardware and\nEmbedded Systems, pages 107–131, 2019.\n[YSG+19]\nMengjia Yan, Read Sprabery, Bhargava Gopireddy, Christopher Fletcher, Roy\nCampbell, and Josep Torrellas. Attack directories, not caches: Side channel\nattacks in a non-inclusive world. In 2019 IEEE Symposium on Security and\nPrivacy (SP), pages 888–904. IEEE, 2019.\n",
  "categories": [
    "cs.CR"
  ],
  "published": "2020-09-22",
  "updated": "2020-09-22"
}