{
  "id": "http://arxiv.org/abs/2009.11112v1",
  "title": "ANNdotNET -- deep learning tool on .NET Platform",
  "authors": [
    "Bahrudin Hrnjica"
  ],
  "abstract": "ANNdotNET is an open source project for deep learning written in C# with\nability to create, train, evaluate and export deep learning models. The project\nconsists of the Graphical User Interface module capable to visually prepare\ndata, fine tune hyper-parameters, design network architecture, evaluate and\ntest trained models. The ANNdotNET introduces the Visual Network Designer,\n(VND) for visually design almost any sequential deep learning network. Beside\nVND, ANNdotNET implements Machine Learning Engine, (MLE) based on CNTK - deep\nlearning framework, with ability to train and evaluate models on GPU. For model\nevaluation ANNdotNET contains rich set of visual and descriptive performance\nparameters, history of the training process and set of export/deployment\noptions. The advantage of using ANNdotNET over the classic code based ML\napproach is more focus on deep learning network design and training process\ninstead of focusing on coding and debugging. It is ideal for engineers not\nfamiliar with supported programming languages. The project is hosted at\ngithub.com/bhrnjica/anndotnet.",
  "text": "ANNDOTNET - DEEP LEARNING TOOL ON .NET PLATFORM\nBahrudin I. Hrnjica∗\nFaculty of Engineering Sciences\nUniversity of Bihac\n77000, Bihac\nBosnia and Herzegovina\nbahrudin-hrnjica@unbi.ba\nSeptember 24, 2020\nABSTRACT\nANNdotNET – is an open source project for deep learning written in C# with ability to create, train,\nevaluate and export deep learning models. The project consists of the Graphical User Interface module\ncapable to visually prepare data, ﬁne tune hyper-parameters, design network architecture, evaluate\nand test trained models. The ANNdotNET introduces the Visual Network Designer, (VND) for\nvisually design almost any sequential deep learning network. Beside VND, ANNdotNET implements\nMachine Learning Engine, (MLE) based on CNTK - deep learning framework, with ability to train\nand evaluate models on GPU. For model evaluation ANNdotNET contains rich set of visual and\ndescriptive performance parameters, history of the training process and set of export/deployment\noptions. The advantage of using ANNdotNET over the classic code based ML approach is more focus\non deep learning network design and training process instead of focusing on coding and debugging.\nIt is ideal for engineers not familiar with supported programming languages. The project is hosted at\nhttp://github.com/bhrnjica/anndotnet.\nKeywords ANNdotNET · .NET · ANN · Deep Learning · Machine Learning\n1\nIntroduction\nANNdotNET – is .NET based solution consisting of set of tools for running deep learning models. The process of\ncreating, training, evaluating and exporting models is provided by the GUI based Application and does not require\nknowledge for supported programming language. The ANNdotNET GUI Tool implements functionalities for data\npreparation prior to training process. The module consists of functionalities for data cleaning, feature selection, category\nencoding, missing values handling, creation of training and validation set. Once the data is prepared, the user can create\nempty DL model to start building, training and evaluate it.\nANNdotNET introduces the Visual Network Designer, VND for visually design deep neural networks. Design process\nis completely visual and no coding is required. It helps the user to focus on deep network design rather than debugging\nthe code. VND supports the most popular and widely used network layers such as Dense, LSTM, Convolutional,Pooling,\nDropOut, etc. Also VND can be used in order to design more complex layers such as AutoEncoders, Embedding, etc.\nANNdotNET introduces the ANNdotNET Machine Learning Engine (MLE) which is responsible for training and\nevaluation of DL models. The MLE relies on Microsoft Cognitive Toolkit (CNTK) open source library developed by\nMicrosoft[2].\nFor evaluation and test of the trained DL models, ANNdotNET provides set of visually presented performance\nparameters that can be used for regression, binary and multi-class classiﬁcation models, history of the training process,\nearly stopping, etc.\n∗Personal web page: http://bhrnjica.net, GitHub: http://github.com/bhrnjica.\narXiv:2009.11112v1  [cs.LG]  23 Sep 2020\nSEPTEMBER 24, 2020\nInformation collected during DL creation process are stored in the set of hierarchically organized ﬁles. In ANNdotNET\nstores information into several different ﬁle types such as: project ﬁle (*.ann), mlconﬁng ﬁle (*.mlconﬁg), data ﬁle\n(*.txt), history ﬁle (*.history). Each ﬁle stores different kind of information important for the ML project.\n1.1\nANNdotNET key features\nAs a desktop application ANNdotNET is suitable in several scenarios over the classic code based ML approaches:\n• more focus on network development and training process using classic desktop approach, instead of focusing\non coding,\n• less time spending on debugging source code, more focusing on different conﬁguration and parameter variants,\n• fast development of deep learning network which can be quickly tested and implemented\n• ideal for engineers/users who are not familiar with programming languages,\n• in case the problem requires more complex scenarios where additional coding implementation is required, the\nANNdotNET provides high level API for such implementation,\n• all ML conﬁgurations developed with GUI tool,can be handled with command line based tool and vice versa.\n1.2\nANNdotNET Start Page\nIn order to easy start working with, ANNdotNET comes with dozens of pre-calculated deep learning projects included\nin the installer. They can be accessed from the Start page. The pre-calculated projects are based on famous datasets from\nseveral categories: regression, binary and multi class classiﬁcation problems, image classiﬁcations, times series, etc. In\npre-calculated projects the user can ﬁnd how to use various types of deep neural network conﬁgurations. Also, each\npre-calculated project can be modiﬁed in terms of change its network conﬁguration, learning and training parameters,\nas well as create new ml conﬁgurations based on the existing data set.\nFigure 1: ANNdotNET Start Window\nThe set of pre-calculated deep learning projects are not static. ANNdotNET Examples Feed contains dynamic list of\ndeep learning projects loaded from the GitHub repository. By Adding new deep learning project into the examples feed,\nevery user running ANNdotNET can use it through the feed.\n2\nSEPTEMBER 24, 2020\n2\nIntroduction of the project\n2.1\nHardware requirements\nANNdotNET support training and model evaluation on modern NVIDIA GPUs, however the training and evaluation\ncan also be performed on CPU with older x64 processors with at least 2 GB of RAM. The minimal processors and\nmemory requirements depends of training model.\n2.2\nSoftware requirements\nIn order to run and develop ANNdotNET based solution the following software requirements must be met:\n• Windows 8 x64 or higher,\n• .NET Framework 4.7.2 and newer,\n• .NET Core 2.0 and newer,\n• Visual Studio 2019 (Community, Professional or Enterprise),\n• Git source control tool.\nIn order to run and use GUI Tool for training deep learning models the machine requires the following software\ncomponents:\n• Windows 8 x64 or higher,\n• .NET Framework 4.7.2 and newer,\n• .NET Core 2.0 and newer,\n2.3\nOrganization of the source code\nThe ANNdotNET project is Visual Studio based solution consisted of several projects grouped into logical folders. In\norder to build the solution at least Visual Studio 2019 Community version should be installed on the local machine.\nANNdotNET solution can be grouped on several components:\n• The library\n• Command Tool\n• GUI Tool\n• Excel AddIn\n• Unit Tests and Test applications\nThe library consists of visual studio projects which logically separate the implementation. It provides foundation of\ndata processing and preparation, neural network conﬁguration and layers implementation, training and handling with\nminibatches. Within the library folder each project exposes set of API for the model evaluation, testing, export and\ndeployment.\nCommand Tool is console-based tool which can be run from Visual Studio and can perform model training and\nevaluation using console output.\nGUI Tool is Windows desktop application which provides rich set of options and visualizations during machine learning\nsteps: project and model creation, data preparation, model training, model evaluation and validation, export options and\nmodel deployment.\nExcel AddIn is implementation of Microsoft Ofﬁce AddIn for model deployment into Excel. Using ANNdotNET\nExcel AddIn, trained model can be used in Excel like ordinary excel formula. This is very handy for model deployment\ninto production when only Excel is need in order to use the model.\nUnit Tests – set of unit tests and console projects for testing the implementation of the solution.\n3\nSEPTEMBER 24, 2020\n3\nGUI tool, projects, models and related ﬁles\nThe basic object in ANNdotNET is machine learning conﬁguration ﬁle, shortly named mlconﬁg. The mlconﬁg, with\nﬁle extension ∗.mlconfig, holds information about features, labels, learning and training parameters, neural network\narchitecture and set of paths required for training and evaluation, best trained model, training history etc. Simply said it\nis the representation of a deep learning model. Beside mlconﬁg ﬁle ANNdotNET supports project ﬁle. The project ﬁle\n(∗.ann) holds the information about whole ML project. It can consists of one or more mlconﬁg ﬁles, data ﬁles and\nproject info ﬁle.\nThe user start working in ANNdotNET by creating new project. Then a data is loaded in order to start working\non data preparation and feature selection. Once the project creation and data preparation are completed the new\nmodel (mlcofing ﬁle) can be created. Example of a project ”BreastCancerProject” with two models named:\nFeedForward and CategoryEmbedding are shown in Figure 2. The project is based on famous Breast Cancer data\nset [1].\nFigure 2: ANNdotNET with opened Brest Cancer DL project. The Project explorer shows Breast Cancer project with\ndata set. Data is organized in columns. Each column is identiﬁed as features or label. Also for each column several\nadditional information is deﬁned such: column type, missing value handling and data normalization.\nAs can be seen the project is consisted of two DL models (two mlconﬁg ﬁles). Each model is created from different\nnetwork architecture, different kind of training parameters and the same data set. Figure 2 also show Project explorer\n-tree control which shows a hierarchical representation of a project and related models. The user start with project\ncreation, data loading and preparation and then can create as many models as necessary.\n3.1\nFile structure in ANNdotNET\nWhile creating a new project the project ﬁle and project folder are created on disk. Illustration of a ﬁle and folder\nstructure can be described as follow: Assume one create a new project called Project01. The folder named Projecet01\nis created, at the same time as project ﬁle named Project01.ann. Those two items are shown on the following image:\nOnce the project is created, one can load the data set ﬁle. The data set ﬁle is the ﬁle that contains data used for\ntraining and evaluation of the deep learning model. The structure of the data set is classic table-based textual data. For\nexample one can load https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\nﬁle directly into ANNdotNET and start processing the data in order to implement deep learning model. Once the data is\nloaded, ANNdotNET processes the ﬁle and saved the copy of the data into the root of the project folder.\nDuring data set ﬁle processing the new ﬁle is created in the project folder and named according to the ANNdotNET\nnaming convention e.g. [ProjectName]_rawdata.txt\n4\nSEPTEMBER 24, 2020\nFigure 3: Project ﬁle and folder structure in ANNdotNET\nFigure 4: Raw data set ﬁle within the project folder\nNow that the project has been created and data set have loaded and processed the next step is to start building DL model.\nEach time the new DL model is created a coresponded mlfoncig ﬁle is created on disk. Within a project there can be\ncreated arbitrary number of DL models with different structure and size of training and validation data sets and also\nwith different network, learning and training parameters.\nAs an example 5 shows the ANNdotNET project with 4 DL models: Model0, Model1, Model2 and Model3.\nFigure 5: Models and related mlconﬁg ﬁles and folders structure\nDuring the models creation separate folder and mlconﬁg ﬁle were created. This kind of ﬁle structure offers clean and\neasy way to follow ﬁle structures and information generated in each model (Figure 6), as well an easy way to transfer\nmlcoﬁng ﬁle to different project.\n5\nSEPTEMBER 24, 2020\nFigure 6: Models and related mlconﬁg ﬁles and folders structure\nDepending of stage of completeness, the model may consist of the following folders and ﬁles:\n• data contains training, validation and testing ml ready data set,\n• log contains ﬁles of training information\n• models ﬁles of CNTK format created during various phase of training\n• temp_ models folder holding temporary model ﬁles during training. All content from the folder is deleted\nonce the training process is completed.\n• model checkpoint state ﬁles model ﬁles stored current the state of the trainer. The ﬁles are needed in case\nwhen the user want to continue with training based on the previous training state.\nAll model mlconﬁg ﬁle is always placed at the root of the project folder.\n3.2\nANNdotNET project ﬁle\nANNDotNET project is stored in annproject ﬁle. It contains information about data set and DL models. Each project\nalso consists of project info ﬁle. It is a Rich Text Format (RTF) ﬁle containing the necessary information about the\nproject. The annproject ﬁle is text based ﬁle consisting of:\n1. project contains information of the project and related models\n2. data contains information about raw data set.\n3. parser parser information while parsing data set ﬁle.\nThe project keyword deﬁned the basic project property like:\n• Name name of the project,\n• V alidationSetCount the size of validation data set,\n• PrecentigeSplit is the validation data set size in percentage while creating it,\n• MLConfigs list of created ml conﬁgurations,\n• Info project info ﬁle.\nFor example, the following text represent typical annproject:\n! a n n p r o j c t\nf i l e\nf o r\nd a i l y\ns o l a r\np r o d u c t i o n\np r o j e c t : | Name : S o l a r P r o d u c t i o n\n| Vali dat ionS etCo unt :20\n| P r e c e n t i g e S p l i t :1\n| MLConfigs : LSTMMLConfig\n| Info :\n6\nSEPTEMBER 24, 2020\n! raw\nd a t a s e t\nand\nmetadata\ni n f o r m a t i o n\ndata : | RawData : S o l a r P r o d u c t i o n _ r a w d a t a . t x t\n| Column01 : time ; Ignore ; Ignore ; Ignore ;\n| Column02 : s o l a r . p a s t ; Numeric ; F eature ; Ignore ;\n| Column03 : s o l a r . c u r r e n t ; Numeric ; Label ; Ignore ;\n! p a r s e r\ni n f o r m a t i o n\np a r s e r : | RowSeparator : rn\n| ColumnSeparator :\n;\n| Header :0\n| SkipLines :0\nThe code above deﬁned the example of the annproject named SolarProduction, with raw data set stored in\nSolarProduction_rawdata.txt ﬁle that contains three columns: time, solar.past and solar.total. The ﬁrst column\n(time) is marked as ignored which means it will be excluded from the model training. The solar.past column is\nmarked as feature and solar.current is marked as label. Both feature and label are numeric column. Those information\nis enough that ANNdotNET tool can created mlreadydatasets.\nThe parser keyword is used while the raw datset is loaded into the application memory.\n3.3\nmlconﬁg ﬁle\nThe basic object in ANNdotNET is deep learning model which is represented by the mlconfig ﬁle.\nThe structure of the mlconfig ﬁle is described by the 8 keywords:\n• configid −unique identiﬁer of the mlconﬁg ﬁle,\n• metadata −meta information about data set,\n• features −deﬁnes features for the model,\n• labels - deﬁnes labels for the model,\n• network - deﬁnes neural network architecture to be trained,\n• learning - deﬁnes learning parameters,\n• training - deﬁnes training parameters,\n• path – deﬁnes paths to ﬁles needed during training and evaluation.\nEach of the above keyword consists of several parameters and values. The syntax of the mlconﬁg ﬁle allows you to\ncreate as many empty lines as you like. In case you want to add comment in the ﬁle, the sentence must begin with\nexclamation \"!\". Order of the keywords is irrelevant.\nThe following content represent typical mlconfig ﬁle:\n!∗∗∗∗∗∗∗ANNdotNET v1 .0∗∗∗∗∗∗∗∗∗∗∗∗\n! I r i s\nmlconfig\nf i l e\ni r i s . mlconfig\n! c o n f i g i d\nr e p r e s e n t\nthe\nunique\ni d e n t i f i e d\nof\nthe\nc o n f i g u r a t i o n\nmodelid :33 fe0968−d640−4b53−97dc −982dcf2b1cad\n! metada\nc o n t a i n s\ni n f o r m a t i o n\nabout\ndata\ns e t .\nmetadata : | Column01 : s e p a l _ l e n g t h ; Numeric ; Fe ature ; Ignore ;\n| Column02 : sepal_width ; Numeric ; Fe ature ; Ignore ;\n| Column03 : p e t a l _ l e n g t h ; Numeric ; Fe ature ; Ignore ;\n| Column04 : p e t a l _ w i d t h ; Numeric ; Fe ature ; Ignore ;\n| Column05 : s p e c i e s ; Category ; Label ; Ignore ; s e t o s a ; v e r s i c o l o r ; v i r g i n i c a\n! I n f o r m a t i o n\nabout\nf e a t u r e s .\n! The\nl i n e\nc o n t a i n s\ntwo groups\nof\nf e a t u r e s :\nNumericFeatures\nand\nProduct\nf e t a u r e\nf e a t u r e s : | NumFeatures\n4 0\n| Product\n10 0\n! I n f o r m a t i o n\nabout\nl a b e l\nl a b e l s : | s p e c i e s\n3 0\n7\nSEPTEMBER 24, 2020\n! Network\nc o n f i g u r a t i o n\nnetwork : | Layer : Normalization 0 0 0 None 0 0\n| Layer : Dense 5 0 0 ReLU 0 0\n| Layer : Dense 3 0 0 Softmax 0 0\n! Learning\nparameter\ni n f o r m a t i o n\nl e a r n i n g : | Type : SGDLearner\n| LRate : 0 . 0 1\n| Momentum :1\n| Loss : CrossEntropyWithSoftmax | Eval : C l a s s i f i c a t i o n A c c u r a c y | L1 : 0 | L2 :0\n! T r a i n i n g\nparameters\ni n f o r m a t i o n\nt r a i n i n g : | Type : d e f a u l t\n| BatchSize :65\n| Epochs :1000\n| Normalization :0\n| RandomizeBatch : False\n| SaveWhileTraining :1\n| ProgressFrequency :50\n| ContinueTraining :0\n| TrainedModel : models \\ \\ model \\ _at \\ _952of1000 \\ _epochs \\ _TimeSpan \\ _636720117054117391\n! Components\nof\nthe\nmlconfig\npaths\npaths : | T r a in i ng : data \\ m l d a t a s e t _ t r a i n\n| V a l i d a t i o n : data \\ m l d a t a s e t _ v a l i d\n| Test : data \\ m l d a t a s e t \\ _ v a l i d\n| TempModels : temp \\ _models\n| Models : models\n| Result : FFModel \\ _ r e s u l t . csv\n| Logs : log\nmlconﬁg ﬁle can be deﬁned using only text editor and then use ANNdotNET for training and evaluation. Full description\nof the ﬁle can be found at the project repository documentation.\n4\nML Engine - training and evaluation of deep learning models\nANNdotNET introduces the ANNdotNET Machine Learning Engine (MLEngine) which is responsible for training\nand model evaluation deﬁned in the mlconﬁg ﬁle. The ML Engine relies on Microsoft Cognitive Toolkit, CNTK open\nsource library for deep learning. Through all application ML Engine exposed all great features of the CNTK e.g. GPU\nsupport for training and evaluation, different kind of learners, but also extends CNTK features with more evaluation\nfunctions (RMSE, MSE, Classiﬁcation Accuracy, Coefﬁcient of Determination, etc.), Extended Mini-batch Sources,\nTrainer and model evaluation.\nML Engine also contains the implementation of neural network layers which supposed to be high level CNTK API very\nsimilar as layer implementation in Keras[11] and other python based deep learning APIs. With this implementation the\nANNdotNET implements the Visual Network Designer (VND) which allows to design neural network conﬁguration of\nany size with any type of the layers. The following layers are implemented:\nNormalization Layer – takes the numerical features and normalizes its values at the beginning of the network. Dense –\nclassic neural network layer with activation function LSTM – special version of recurrent network layer with option for\npeephole and self-stabilization. Embedding – Embedding layer, Drop – drop layer,\nComplete list of supported layer can be found in the project documentation.\nDesigning deep neural networks can be simplify by using pre-deﬁned network layer with capability to created any\nnetwork we usually implement through the source code.\n4.1\nTraining and learning parameters\nIN deep learning there are two kind of parameters. The learning and training parameters. The learning parameters are\nparameters needed during network learning. This includes:\n• Learner - optimization method used during learning process,\n• learning rate - the number between 0 and 1 using which determines the step size at each iteration while moving\ntoward a minimum of a loss function.\n• Loss function used to determine how well learner models the given data,\n• Evaluation function to measure how close the model predict the output values.\nANNDotNET supports mini-batch training which allows to make different type of training strategies.The training\nparameters includes: number of epochs, mini-batch size and progress frequency. Epochs and mini-batch size are self\n8\nSEPTEMBER 24, 2020\nexplanatory. However, the progress frequency is the number of epoch skip until the next epoch is shown in the output.\nIn ANNdotNET the full list of training parameters includes:\n• Epoch - the number of full cycles when training.\n• Mini-batch size number of samples in the batch which going into the network,\n• Progress frequency - shows output of the training progress at every n epoch,\n• Randomize mini-batch - randomize mini-batch during training process,\n• Continue training - The parameters indicate if the model will be continue with training, or the training will\nstart from scratch,\n• Save good models during training - saves model which has better performance parameters than previous one.\nTraining process can be visually monitored by using two graphs:\n• Mini- batch training - shows the value of the loss and evaluation functions for each mini-batch.\n• Model evaluation - shows the values of evaluation function for training and validation data set for the current\niteration.\nFigure 7: Training module in ANNdotNET. During training the training progress is monitored by two diagrams\nmini-batch training and model evaluation.\nThe visualization of the training progress can give the user a better picture how training process behave. Is the training\nprocess converges, is it going to over-ﬁtted area. The user can stop the training process at any time. Once the training\nprocess is stopped or completed the best model is determined based on the selected training strategy (with or without\nearly stopping).\n4.2\nTraining with early stopping\nIn ANNdotNET the early stopping is implemented so that the best trained model is selected after the training process.\nThe best model is selected among other models saved during the training. This is kind of training strategy leads that\nregardless of the epoch number the best model is always selected without over-ﬁtting.\n9\nSEPTEMBER 24, 2020\n4.3\nDL Model evaluation\nThe model evaluation module evaluate the best trained model and presents the performance parameters both for training\nand validation sets. Depending of the ML type (regression, binary or multi class classiﬁcation) performance parameters\nare calculated and presented. Figure 8 shows the evaluation of the regression DL model. However, it is supported both\nbinary and multi class classiﬁcation model evaluation.\nFigure 8: Evaluation of regression model\n5\nVisual Network Designer\nBuilding network is one of the most challenging task in deep learning and it is followed once the set of features and\nlabels are deﬁned. The ﬁrst layer in the network is the input layer which directly depends of the input data (features).\nOn the other hand, the output layer is deﬁned by the output data (labels). The ﬁrst and the last layers in the network are\ndeﬁned by the training set while hidden layers are deﬁned with speciﬁc architecture. ANNdotNET introduces the Visual\nNetwork Designer (VND) which allows to visually create different types of deep network architecture. VND supports\nbasic network layers such dense, dropout, LSTM, convolution and allows to create a network with any combination\nof the layers. By using proper combination of the basic network layers one can create network such Feed Forward\nnetworks, Deep Feed Forward networks, Convolutions Network, Recurrent LSTM based Network, Auto-Encoder,\nCUDAStackedLSTM network, CUDAStackedGRU network, etc. VND supports creation popular network architecture\nsuch AlexNet and similar, or create popular layers such as Autoencoder-Decoder, etc.\nBeside classic neural network layers, ANNdotNET implements custom layers such as Normalization and Scale Layer.\nNormalization layer normalizes the training data set by calculating the standard deviation and mean of each numeric\nfeature and produces the z-Score as output. With the normalization layer each numeric feature has zero mean and\nstandard deviation of one. This is typical normalization method in training deep leaning models. Scale layer is suitable\nwhen normalizing the input data in image recognition tasks.\nVND is accessible from the Network Settings tab page in the DL Model. The concept of VND is based on sequential list\nof network layers, so the designer can add, insert remove any available network layer mentioned above. Figure 6 shows\nan example of CNN network architecture designed to model popular Cat vs. Dog data set[3]. As can be seen, the ﬁgure\nshows ﬁrst several network layers sequentially ordered in the list. On the left side one can ﬁnd the information of data\nset (training and validation), the input and the output layer, as well as learning parameters (learning rate, momentum,\nloss and evaluation functions). In order to change the current network or design new one there are set of options located\nat the top of the layer list. There are Combo Box with all supported network layers followed by buttons to add, remove\nand insert network layer. Once the network is designed the Graph option can visually represents the while network with\nits weights, inputs and output parameters.\n10\nSEPTEMBER 24, 2020\nFigure 9: Visual Network Designer presenting convolutions network architecture capable to predict Cat/Dog data set\n6\nANNdotNET Excel Addin\nANNdotNET supports the deployment of the DL model into Microsoft Excel Application by using Excel Add-in. With\nthe Add-in the DL model is used like ordinary formula which can be run from the formula bar. In order to run DL\nmodel within Excel, the model should be exported and saved on known location. In Excel, calling the model is achieved\nby typing the formula:\n= ANNdotNET([cellrange], [modelpath])\nFigure 10: Trained model deployed in Excel\nFigure above shows Iris model exported in Excel. The predicted values are calculated directly in Excel by calling\nANNdotNET funtion within Excel and pointing to cell range and exported model path.\n6.1\nANNdotNET as A Cloud Solution\nBy using the ANNdotNET, it is possible to incorporate Deep Learning, (DL) tasks into a cloud solution, so that the\ncomplete DL process can be automatized and deﬁned into one workﬂow using cloud services.\n11\nSEPTEMBER 24, 2020\nIt can be detected three common tasks in DL cloud solution:\n• Data preparation\n• Training ML model\n• Model Deployment\nIn all three phases ANNdotNET can be incorporated and used.\nFigure 11: Architecture of Deep Learning Cloud Solution\nThe typical scenario can be described on the following:\nOnce the model conﬁguration is loaded using the mlconﬁg ﬁle, the training process can be started by deﬁning the\nnumber of epochs, or by deﬁning the early stopping criteria. The training process can be monitored by reading the\ntraining progress information. The information helps the user to decide is the training process converging at the expected\nspeed, or when to stop the training process in order to prevent model over-ﬁtting. The training module that shows\ntraining history is shown on Figure 11. The model deployment is the last phase of the ML cloud solution, and deﬁnes\nseveral options that can be used for different scenarios. The most common option is to generate a simple web service\nthat contains the implementation of the model evaluation. The web service returns the model output in an appropriate\nformat. The model can also be deployed in Excel, to allow the model to behave as an Excel formula. Excel deployment\nis achieved by implementing additional Excel add-in. The deployment ML model in Excel is usually suitable when\ndealing with the input data which is relatively easy to represent in Excel.\nThe complete cloud ML solution is depicted in Figure 11. By using the ANNdotNET, it is possible to transform data\nand prepare it for training. Moreover, ANNdotNET provides components for training, evaluation, testing and deploying\ndeep learning models. Its components can be used in similar cloud solutions depicted in Figure 11, particularly for\n\"data transformation\" and \"deep learning\" cloud solution components.\n7\nApplication of ANNdotNET for developing deep learning models\nIn this section successful applications are going to be presented. The applications can be classiﬁed with the domain\nproblems.\nANNdotNET has been successfully used in many deep learning and ANN research papers and online articles. [6] used\nANNdotNET in order to develop LSTM based deep learning models for predicting Vrana lake water level in 6 and 12\nmonths ahead located in Croatia. Authors also used the tool to develop Feed Forward model for comparison results. [5]\nused ANNdotNET in order to develop deep learning models to predict lake level for 100 lakes in Poland. Furthermore\n[4] used ANNdotNET in order to develop deep learning model for predicting energy demands in one of the mayor city\nin Cyprus.\nANNdotNET has been used to develop deep learning model for sentiment analysis [7] and Time Series prediction [9].\nBeside using recurrent LSTM and feed forward deep networks successful application has been achieved in using\nconvolutions networks mainly for image classiﬁcations. Using popular CIFAR-10 [10] data set ANNdotNET achieve\nprediction with average accuracy higher than 0.96 [8]. Using Kaggle Cats vs. Dogs data set[3] ANNdotNET achieved\n12\nSEPTEMBER 24, 2020\nthe accuracy higher than 0.95 which can be found as standard example. Also MNIST [12] data set used in order to\ncreated deep learning model based on convolutions network with prediction accuracy with more than 0.95. It is also\npart of the standard package. It is also worth mention that ANNdotNET installation package comes with 12 complete\nand ready to used deep learning project made based on most popular data sets from different problem domains like\nregressions, binary classiﬁcations, multi class classiﬁcation, time series and image classiﬁcations.\n8\nConclusion\nANNdotNET is deep learning framework implemented on .NET Framework which is developed for building, training\nand evaluation of deep learning models. The tool can be used as regular Desktop application providing rich set of\nuser interfaces. The project is completely open sourced and hosted at http://github.com/bhrnjica/anndotnet.\nThe project is mainly targeting academicians, researchers and hobbies to work with designing deep learning networks.\nANNdotNET can also be used through the development environment in order to develop more complex scenario for\ncustom data processing or network design with multiple inputs and outputs or non supported network layers.\nReferences\n[1] W.N. Street, W.H. Wolberg and O.L. Mangasarian., (1993) Nuclear feature extraction for breast tumor diagnosis.\nIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages\n861-870, San Jose, CA.\n[2] Yu, D., Eversole, A., Seltzer, M., Yao, K., Kuchaiev, O., Zhang, Y., ... Huang, X. (2014), An Intro-\nduction to Computational Networks and the Computational Network Toolkit. Microsoft Research. Retrieved\nfrom https://www.microsoft.com/en-us/research/publication/an-introduction-to-computational-networks-and-the-\ncomputational-network-toolkit/\n[3] Kaggle Cats and Dogs Dataset, Retrieved from https://www.kaggle.com/c/dogs-vs-cats/data, accessed\nsept. 2020.\n[4] Hrnjica B., Mehr A.D., (2020), Energy Demand Forecasting Using Deep Learning. In: Al-Turjman F. (eds) Smart\nCities Performability, Cognition, & Security. EAI/Springer Innovations in Communication and Computing. Springer,\nCham. https://doi.org/10.1007/978-3-030-14718-1_4\n[5] Zhu, S., Hrnjica, B., Ptak, M., Choi´nski, A., & Sivakumar, B. (2020), Forecasting of water level in multiple temperate\nlakes using machine learning models. Journal of Hydrology. https://doi.org/10.1016/j.jhydrol.2020.124819\n[6] Hrnjica, B., Bonacci, O., (2019), Lake Level Prediction using Feed Forward and Recurrent Neural Networks. Water\nResources Management 33, 2471–2484 (2019). https://doi.org/10.1007/s11269-019-02255-2\n[7] Hrnjica, B., (2018), Sentiment Analysis using ANNdotNET, CodeProject article, Retrieved from https://www.\ncodeproject.com/Articles/1263862/Sentiment-Analysis-using-ANNdotNET, accessed sept. 2020.\n[8] Hrnjica, B.,(2018), Create CIFAR-10 Deep Learning Model With ANNdotNET GUI Tool, CodeProject article,\nRetrieved from https://www.codeproject.com/Articles/1273368/Create-CIFAR-10-Deep-Learning-Model-With-\nANNdotNET, accessed sept. 2020.\n[9] Hrnjica, B., (2017), CNTK 106 Tutorial – Time Series Prediction with LSTM using C#, CodeProject article, Re-\ntrieved from https://www.codeproject.com/Articles/1220142/CNTK-Tutorial-Time-Series-Prediction-with-LSTM-\nusi, accessed sept. 2020.\n[10] Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images. . . . Science Department, Univer-\nsity of Toronto, Tech. .... https://doi.org/10.1.1.222.9220\n[11] Chollet, Francois and others, (2015), Keras, =https://github.com/fchollet/keras\n[12] LeCun, Yann and Cortes, Corinna and Burges, CJ, 2010, MNIST handwritten digit database, ATT Labs [Online].\nVol 2, Available: http://yann.lecun.com/exdb/mnist\n13\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-09-23",
  "updated": "2020-09-23"
}