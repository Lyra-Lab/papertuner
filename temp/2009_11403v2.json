{
  "id": "http://arxiv.org/abs/2009.11403v2",
  "title": "CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq",
  "authors": [
    "Koundinya Vajjha",
    "Avraham Shinnar",
    "Vasily Pestun",
    "Barry Trager",
    "Nathan Fulton"
  ],
  "abstract": "Reinforcement learning algorithms solve sequential decision-making problems\nin probabilistic environments by optimizing for long-term reward. The desire to\nuse reinforcement learning in safety-critical settings inspires a recent line\nof work on formally constrained reinforcement learning; however, these methods\nplace the implementation of the learning algorithm in their Trusted Computing\nBase. The crucial correctness property of these implementations is a guarantee\nthat the learning algorithm converges to an optimal policy. This paper begins\nthe work of closing this gap by developing a Coq formalization of two canonical\nreinforcement learning algorithms: value and policy iteration for finite state\nMarkov decision processes. The central results are a formalization of Bellman's\noptimality principle and its proof, which uses a contraction property of\nBellman optimality operator to establish that a sequence converges in the\ninfinite horizon limit. The CertRL development exemplifies how the Giry monad\nand mechanized metric coinduction streamline optimality proofs for\nreinforcement learning algorithms. The CertRL library provides a general\nframework for proving properties about Markov decision processes and\nreinforcement learning algorithms, paving the way for further work on\nformalization of reinforcement learning algorithms.",
  "text": "CertRL: Formalizing Convergence Proofs for Value and Policy\nIteration in Coq\nKOUNDINYA VAJJHA, University of Pittsburgh , USA\nAVRAHAM SHINNAR, IBM Research , USA\nBARRY TRAGER, IBM Research , USA\nVASILY PESTUN, IBM Research , USA& IHES\nNATHAN FULTON, IBM Research , USA\nReinforcement learning algorithms solve sequential decision-making problems in probabilistic environments by optimizing\nfor long-term reward. The desire to use reinforcement learning in safety-critical settings inspires a recent line of work on\nformally constrained reinforcement learning; however, these methods place the implementation of the learning algorithm in\ntheir Trusted Computing Base. The crucial correctness property of these implementations is a guarantee that the learning\nalgorithm converges to an optimal policy.\nThis paper begins the work of closing this gap by developing a Coq formalization of two canonical reinforcement learning\nalgorithms: value and policy iteration for finite state Markov decision processes. The central results are a formalization of the\nBellman optimality principle and its proof, which uses a contraction property of Bellman optimality operator to establish that\na sequence converges in the infinite horizon limit. The CertRL development exemplifies how the Giry monad and mechanized\nmetric coinduction streamline optimality proofs for reinforcement learning algorithms. The CertRL library provides a general\nframework for proving properties about Markov decision processes and reinforcement learning algorithms, paving the way\nfor further work on formalization of reinforcement learning algorithms.\nAdditional Key Words and Phrases: Formal Verification, Policy Iteration, Value Iteration, Reinforcement Learning, Coinduction\n1\nINTRODUCTION\nReinforcement learning (RL) algorithms solve sequential decision making problems in which the goal is to\nchoose actions that maximize a quantitative utility function [Bel54, How60, Put94, SB98]. Recent high-profile\napplications of reinforcement learning include beating the world‚Äôs best players at Go [SHM+16], competing against\ntop professionals in Dota [Ope18], improving protein structure prediction [SEJ+20], and automatically controlling\ncomplex robots [GHLL16]. These successes motivate the use of reinforcement learning in safety-critical and\ncorrectness-critical settings.\nReinforcement learning algorithms produce, at a minimum, a policy that specifies which action(s) should be\ntaken in a given state. The primary correctness property for reinforcement learning algorithms is convergence:\nin the limit, a reinforcement learning algorithm should converge to a policy that optimizes for the expected\nfuture-discounted value of the reward signal.\nThis paper contributes CertRL, a formal proof of convergence for value iteration and policy iteration two canon-\nical reinforcement learning algorithms [Bel54, How60, Put94]. They are often taught as the first reinforcement\nlearning methods in machine learning courses because the algorithms are relatively simple but their convergence\nproofs contain the main ingredients of a typical convergence argument for a reinforcement learning algorithm.\nThere is a cornucopia of presentations of these iterative algorithms and an equally diverse variety of proof\ntechniques for establishing convergence. Many presentations state but do not prove the fact that the optimal\npolicy of an infinite-horizon Markov decision process with ùõæ-discounted reward is a stationary policy; i.e., the\noptimal decision in a given state does not depend on the time step at which the state is encountered. Following\nthis convention, this paper contributes the first formal proof that policy and value iteration converge in the\nlimit to the optimal policy in the space of stationary policies for infinite-horizon Markov decision processes. In\naddition to establishing convergence results for the classical iterative algorithms under classical infinitary and\narXiv:2009.11403v2  [cs.AI]  15 Dec 2020\nVajjha et al.\nstationarity assumptions, we also formalize an optimality result about ùëõ-step iterations of value iteration without\na stationarity assumption. The former formalization matches the standard theoretical treatment, while the latter\nis closer to real-world implementations. We shall refer to the former case ‚Äì where the set of time steps is an\ninfinite set ‚Äì as infinite-horizon and the latter case as finite-horizon.\nIn all cases, the convergence argument for policy/value iteration proceeds by proving that a contractive\nmapping converges to a fixed point and that this fixed point is an optimum. This is typical of convergence proofs\nfor reinforcement learning algorithms. CertRL is intentionally designed for ongoing reinforcement learning\nformalization efforts.\nFormalizing the convergence proof directly would require complicated and tedious ùúñ-hacking as well as\nlong proofs involving large matrices. CertRL obviates these challenges using a combination of the Giry monad\n[Gir82, Jac18] and a proof technique called Metric coinduction [Koz07].\nMetric coinduction was first identified by Kozen and Ruozzi as a way to streamline and simplify proofs of\ntheorems about streams and stochastic processes [KR09]. Our convergence proofs use a specialized version of\nmetric coinduction called contraction coinduction [FHM18] to reason about order statements concerning fixed\npoints of contractive maps. Identifying a coinduction hypothesis allows us to automatically infer that a given\n(closed) property holds in the limit whenever it holds ab initio. The coinduction hypothesis guarantees that this\nproperty is a limiting invariant. This is significant because the low level ùúñ‚àíùõøarguments ‚Äì typically needed\nto show that a given property holds of the limit ‚Äì are now neatly subsumed by a single proof rule, allowing\nreasoning at a higher level of abstraction.\nThe finitary Giry monad is a monad structure on the space of all finitely supported probability mass functions\non a set. Function composition in the Kleisli category of this monad recovers the Chapman-Kolmogorov formula\n[Per19, Jac18]. Using this fact, our formalization recasts iteration of a stochastic matrix in a Markov decision\nprocess as iterated Kleisli composites of the Giry monad, starting at an initial state. Again, this makes the\npresentation cleaner since we identify and reason about the basic operations of bind and ret, thus bypassing the\nneed to define matrices and matrix multiplication and substantially simplifying convergence proofs.\nThis paper shows how these two basic building blocks ‚Äì the finitary Giry monad and metric coinduction\n‚Äì provide a compelling foundation for formalizing reinforcement learning theory. CertRL develops the basic\nconcepts in reinforcement learning theory and demonstrates the usefulness of this library by proving several\nresults about value and policy iteration. CertRL contains a proof of the Bellman optimality principle, an inductive\nrelation on the optimal value and policy over the horizon length of the Markov decision process.\nIn practice, reinforcement learning algorithms almost always run in finite time by either fixing a run time\ncutoff (e.g., number training steps) or by stopping iteration after the value/policy changes become smaller than a\nfixed threshold. Therefore, our development also formalizes a proof that ùëõ-step value iteration satisfies a finite\ntime analogue of our convergence results.\nTo summarize, the CertRL library contains:\n(1) a formalization of Markov decision processes and their long-term values in terms of the finitary Giry\nmonad,\n(2) a formalization of optimal value functions and the Bellman operator,\n(3) a formal proof of convergence for value iteration and a formalization of the policy improvement theorem\nin the case of stationary policies, and\n(4) a formal proof that the optimal value function for finitary sequences satisfies the finite time analogue of\nthe Bellman equation.\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\nThroughout the text which follows, hyperlinks to theorems, definitions and lemmas which have formal\nequivalents in the Coq [Tea04] development are indicated by a ‚úø.1\nCertRL is part of a larger project for verifying machine learning theory with applications to program synthesis.\nThe entire development is available online at the following URL: https://github.com/IBM/FormalML.\n2\nBACKGROUND\nWe provide a brief introduction to value/policy iteration and to the mathematical structures upon which our\nformalization is built: contractive metric spaces, metric coinduction, the Giry monad and Kleisli composition.\n2.1\nReinforcement Learning\nThis section gently introduces the basics of reinforcement learning with complete information about the stochastic\nreward and transition functions. In this simplified situation the focus of the algorithm is on optimal exploitation\nof reward. This framework is also known as the stochastic optimal control problem.\nWe give an informal definition of Markov decision processes, trajectories, long-term values, and dynamic\nprogramming algorithms for solving Markov decision processes. Many of these concepts will be stated later in a\nmore formal type-theoretic style; here, we focus on providing an intuitive introduction to the field.\nThe basic mathematical object in reinforcement learning theory is the Markov decision process. A Markov\ndecision process is a 4-tuple (ùëÜ,ùê¥, ùëÖ,ùëá) where ùëÜis a set of states, ùê¥is a set of actions, ùëÖ: ùëÜ√ó ùê¥√ó ùëÜ‚ÜíR is a\nreward function, and ùëáis a transition relation on states and actions mapping each (ùë†,ùëé,ùë†‚Ä≤) ‚ààùëÜ√ó ùê¥√ó ùëÜto the\nprobability that taking action ùëéin state ùë†results in a transition to ùë†‚Ä≤. Markov decision processes are so-called\nbecause they characterize a sequential decision-making process (each action is a decision) in which the transition\nstructure on states and actions depends only on the current state.\nExample 1 (CeRtL the turtle ‚úø). Consider a simple grid world environment in which a turtle can move in\ncardinal directions throughout a 2D grid. The turtle receives +1 point for collecting stars, -10 for visiting red\nsquares, and +2 for arriving at the green square. The turtle chooses which direction to move, but with probability\n1\n4 will move in the opposite direction. For example, if the turtle takes action left then it will go left with\nprobability 3\n4 and right with probability 1\n4. The game ends when the turtle arrives at the green square.\nThis environment is formulated as a Markov decision process as follows:\n‚Ä¢ The set of states ùëÜare the coordinates of each box ‚úø:\n{(ùë•,ùë¶) | 1 ‚â§ùë•‚â§5 and 1 ‚â§ùë¶‚â§5}\nso that (1, 1) is the top-left corner and (5, 5) is the bottom-right corner.\n‚Ä¢ The set of actions ùê¥are {up, down, left, right} ‚úø.\n1We recommend MacOS users view this document in Adobe, Firefox, or Chrome, as Preview and Safari parse the URLs linked to by ‚úø‚Äôs\nincorrectly.\nVajjha et al.\nFig. 1. An example grid-world environment ‚úø.\n‚Ä¢ The reward function is defined as ‚úø:\nùëÖ(1, 4) = 2\nùëÖ(4, 2) = 1\nùëÖ(3, 3) = 1\nùëÖ({1, 2, 3}, 2) = ‚àí10\nùëÖ(4, 3) = ‚àí10\nùëÖ(2, 4) = ‚àí10\nùëÖ(4, 5) = ‚àí10\nùëÖ(¬∑, ¬∑) = 0 otherwise\n‚Ä¢ The transition probabilities are as described ‚úø; e.g.,\nùëá((3, 4), up, (3, 3)) = 3\n4\nùëá((3, 4), up, (3, 5)) = 1\n4\nùëá((3, 4), up, (¬∑, ¬∑)) = 0 otherwise\nand so on.\nWe implement this example in Coq as a proof-of-concept for CertRL. We first define a matrix whose indices are\nstates (ùë•,ùë¶) and whose entries are colors {red, green, star, empty}. We then define a reward function that maps\nfrom matrix entries to a reward depending on the color of the turtle‚Äôs current state. We also define a transition\nfunction that comports with the description given above. At last, we prove that this combination of states, actions,\ntransitions and rewards inhabits our MDP (standing for Markov decision process) type. Therefore, all of the theorems\ndeveloped in this paper apply directly to our Coq implementation of the CertRL Turtle environment.\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\nThe goal of reinforcement learning is to find a policy (ùúã: ùëÜ‚Üíùê¥) specifying which action the algorithm should\ntake in each state. This policy should maximize the amount of reward obtained by the agent. A policy is stationary\nif it is not a function of time; i.e., if the optimal action in some state ùë†‚ààùëÜis always the same and, in particular,\nindependent of the specific time step at which ùë†is encountered.\nReinforcement learning agents optimize for a discounted sum of rewards ‚Äì placing more emphasis on reward\nobtained today and less emphasis on reward obtained tomorrow. A constant discount factor from the open unit\ninterval, typically denoted by ùõæ, quantitatively discounts future rewards and serves as a crucial hyperparameter\nto reinforcement learning algorithms.\nValue iteration, invented by Bellman [Bel54], is a dynamic programming algorithm that finds optimal policies\nto reinforcement learning algorithms by iterating a contractive mapping. Value iteration is defined in terms of a\nvalue function ùëâùúã: ùëÜ‚ÜíR, where ùëâùúã(ùë†) is the expected value of state ùë†when following policy ùúãfrom ùë†.\nData:\nMarkov decision process (ùëÜ,ùê¥,ùëá, ùëÖ)\nInitial value function ùëâ0 = 0\nThreshold ùúÉ> 0\nDiscount factor 0 < ùõæ< 1\nResult: ùëâ‚àó, the value function for an optimal policy.\nfor ùëõfrom 0 to ‚àûdo\nfor each ùë†‚ààùëÜdo\nùëâùëõ+1[ùë†] = maxùëé\n√ç\nùë†‚Ä≤ ùëá(ùë†,ùëé,ùë†‚Ä≤)(ùëÖ(ùë†,ùëé,ùë†‚Ä≤) + ùõæùëâùëõ[ùë†‚Ä≤])\nend\nif ‚àÄùë†|ùëâùëõ+1[ùë†] ‚àíùëâùëõ| < ùúÉthen\nreturn ùëâùëõ+1\nend\nend\nAlgorithm 1: Pseudocode for Value Iteration.\nThe optimal policy ùúã‚àóis then obtained by\nùúã‚àó(ùëé) = argmaxùëé‚ààùê¥\n‚àëÔ∏Å\nùë†‚Ä≤\nùëá(ùë†,ùëé,ùë†‚Ä≤)(ùëÖ(ùë†,ùëé,ùë†‚Ä≤) + ùõæùëâùëõ+1[ùë†‚Ä≤]).\nPolicy iteration follows a similar iteration scheme, but with a policy estimation function ùëÑùúã: ùëÜ√ó ùê¥‚ÜíR\nwhere ùëÑùúã(ùë†,ùëé) estimates the value of taking action ùëéin state ùë†and then following the policy ùúã. In Section 3.4\nwe will demonstrate a formalized proof that ùëâùëõis the optimal value function of a length ùëõMDP; this algorithm\nimplements the dynamic programming principle.\n2.2\nMetric and Contraction Coinduction\nOur formalization uses metric coinduction to establish convergence properties for infinite sequences. This section\nrecalls the Banach fixed point theorem and explains how this theorem gives rise to a useful proof technique.\nA metric space (ùëã,ùëë) is a set ùëãequipped with a function ùëë: ùëã√ó ùëã‚ÜíR satisfying certain axioms that ensure\nùëëbehaves like a measurement of the distance between points in ùëã. A metric space is complete if the limit of every\nCauchy sequence of elements in ùëãis also in ùëã.\nLet (ùëã,ùëë) denote a complete metric space with metric ùëë. Subsets of ùëãare modeled by terms of the function\ntype ùúô: ùëã‚ÜíProp. Another interpretation is that ùúôdenotes all those terms of ùëãwhich satisfy a particular\nproperty. These subsets are also called Ensembles in the Coq standard library.\nVajjha et al.\nA Lipschitz map ‚úøis a mapping that is Lipschitz continuous; i.e., a mapping ùêπfrom (ùëã,ùëëùëã) into (ùëå,ùëëùëå) such\nthat for all ùë•1,ùë•2 ‚ààùëãthere is some ùêæ‚â•0 such that\nùëëùëå(ùêπ(ùë•1), ùêπ(ùë•2)) ‚â§ùêæùëëùëã(ùë•1,ùë•2).\nThe constant ùêæis called a Lipschitz constant.\nA map ùêπ: ùëã‚Üíùëãis called a contractive map ‚úø, or simply a contraction, if there exists a constant 0 ‚â§ùõæ< 1\nsuch that\nùëë(ùêπ(ùë¢), ùêπ(ùë£)) ‚â§ùõæùëë(ùë¢, ùë£)\n‚àÄùë¢, ùë£‚ààùëã.\nContractive maps are Lipschitz maps with Lipschitz constant ùõæ< 1.\nThe Banach fixed point theorem is a standard result of classical analysis which states that contractive maps on\ncomplete metric spaces have a unique fixed point.\nTheorem 2 (Banach fixed point theorem). If (ùëã,ùëë) is a nonempty complete metric space and ùêπ: ùëã‚Üíùëãis a\ncontraction, then ùêπhas a unique fixed point; i.e., there exists a point ùë•‚àó‚ààùëãsuch that ùêπ(ùë•‚àó) = ùë•‚àó. This fixed point is\nùë•‚àó= limùëõ‚Üí‚àûùêπ(ùëõ) (ùë•0) where ùêπ(ùëõ) stands for the ùëõ-th iterate of the function ùêπand ùë•0 is an arbitrary point in ùëã.\nThe Banach fixed point theorem generalizes to subsets of ùëã.\nTheorem 3 (Banach fixed point theorem on subsets ‚úø). Let (ùëã,ùëë) be a complete metric space and ùúôa\nclosed nonempty subset of ùëã. Let ùêπ: ùëã‚Üíùëãbe a contraction and assume that ùêπpreserves ùúô. In other words,\nùúô(ùë¢) ‚Üíùúô(ùêπ(ùë¢))\nThen ùêπhas a unique fixed point in ùúô; i.e., a point ùë•‚àó‚ààùëãsuch that ùúô(ùë•‚àó) and ùêπ(ùë•‚àó) = ùë•‚àó. The fixed point of ùêπis\ngiven by ùë•‚àó= limùëõ‚Üí‚àûùêπ(ùëõ) (ùë•0) where ùêπ(ùëõ) stands for the ùëõ-th iterate of the function ùêπ.\nBoth the Banach fixed point theorem and the more general theorem on subsets were previously formalized\nin Coq by Boldo et al. [BCF+17]. This formalization includes definitions of Lipschitz maps and contractions.\nWe make use of the fact that Boldo et al. prove the above theorem where ùëãis either a CompleteSpace or a\nCompleteNormedModule.\nThe fixed point of ùêπin Theorem 3 is unique, but it depends on an initial point ùë•0 ‚ààùëã, which ùêπthen iterates on.\nUniqueness of the fixed point implies that different choices of the initial point still give the same fixed point ‚úø.\nTo emphasize how this theorem is used in our formalization, we restate it as an inductive proof rule:\nùúôclosed\n‚àÉùë•0,ùúô(ùë•0)\nùúô(ùë¢) ‚Üíùúô(ùêπ(ùë¢))\nùúô(fix ùêπùë•0)\n‚úø\n(1)\nThis proof rule states that in order to prove some closed ùúôis a property of a fixed point of ùêπ, it suffices to\nestablish the standard inductive assumptions: that ùúôholds for some initial ùë•0, and that if ùúôholds at ùë¢then it also\nholds after a single application of ùêπto ùë¢. In this form, the Banach fixed point theorem is called Metric coinduction.\nThe rule (1) is coinductive because it is equivalent to the assertion that a certain coalgebra is final in a category of\ncoalgebras. (Details are given in Section 2.3 of Kozen and Ruozzi [KR09]).\nThe following snippet shows how we use the Banach Fixed Point theorem as proven in [BCF+17] as a proof\nrule.\nTheorem metric_coinduction {phi : X ‚ÜíProp}\n( nephi : phi init) ( Hcphi : closed phi)\n( HFphi : forall x : X, phi x ‚Üíphi (F x)):\nphi ( fixpt F init).\nProof.\nassert (my_complete phi)\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\nby ( now apply closed_my_complete).\ndestruct (FixedPoint K F phi fphi ( ex_intro _ _ init_phi)\nH hF) as [? [Hin [? [? Hsub]]]].\nspecialize (Hsub init init_phi).\nrewrite ‚ÜêHsub in Hin.\napply Hin.\nQed.\nDefinition 4 (Ordered Metric Space). A metric space ùëãis called an ordered metric space if the underlying set ùëã\nis partially ordered and the sets {ùëß‚ààùëã|ùëß‚â§ùë¶} and {ùëß‚ààùëã|ùë¶‚â§ùëß} are closed sets in the metric topology for every\nùë¶‚ààùëã.\nFor ordered metric spaces, metric coinduction specializes to [FHM18, Theorem 1], which we restate as Theorem 5\nbelow.\nTheorem 5 (Contraction coinduction). Let ùëãbe a non-empty, complete ordered metric space. If ùêπ: ùëã‚Üíùëã\nis a contraction and is order-preserving, then:\n‚Ä¢ ‚àÄùë•, ùêπ(ùë•) ‚â§ùë•‚áíùë•‚àó‚â§ùë•‚úøand\n‚Ä¢ ‚àÄùë•,ùë•‚â§ùêπ(ùë•) ‚áíùë•‚â§ùë•‚àó‚úø\nwhere ùë•‚àóis the fixed point of ùêπ.\nWe will use the above result to reason about Markov decision processes. However, doing so requires first\nsetting up an ordered metric space on the function space ùê¥‚ÜíR where ùê¥is a finite set ‚úø.\n2.3\nThe function space ùê¥‚ÜíR.\nLet ùê¥be a finite set ‚úø. We endow the function space ùê¥‚ÜíR with a natural vector space structure and with ùêø‚àû\nnorm ‚úø:\n‚à•ùëì‚à•‚àû= max\nùëé‚ààùê¥|ùëì(ùëé)|\n(2)\nOur development establishes several important properties about this function space. The norm (2) is well-\ndefined because ùê¥is finite and furthermore induces a metric that makes ùê¥‚ÜíR a metric space ‚úø. With this\nmetric, the space of functions ùê¥‚ÜíR is also complete ‚úø. From R this metric inherits a pointwise order; viz., for\nfunctions ùëì,ùëî: ùê¥‚ÜíR,\nùëì‚â§ùëî\n‚áê‚áí‚àÄùëé‚ààùê¥, ùëì(ùëé) ‚â§ùëî(ùëé) ‚úø\nùëì‚â•ùëî\n‚áê‚áí‚àÄùëé‚ààùê¥, ùëì(ùëé) ‚â•ùëî(ùëé) ‚úø\nWe also prove that the sets\n{ùëì|ùëì‚â§ùëî} ‚úø\nand\n{ùëì|ùëì‚â•ùëî} ‚úø\nare closed in the norm topology. Our formalization of the proof of closedness for these sets relies on classical\nreasoning. Additionally, we rely on functional extensionality to reason about equality between functions.\nWe now have an ordered metric space structure on the function space ùê¥‚ÜíR when ùê¥is finite. Constructing a\ncontraction on this space will allow an application of Theorem 5. Once we set up a theory of Markov decision\nprocesses we will have natural examples of such a function space and contractions on it. Before doing so, we first\nintroduce the Giry monad.\nVajjha et al.\n2.4\n(Finitary) Giry Monad\nA monad structure on the category of all measurable spaces was first described by Lawvere in [Law62] and was\nexplicitly defined by Giry in [Gir82]. This monad has since been called the Giry monad. While the construction is\nvery general (applying to arbitrary measures on a space), for our purposes it suffices to consider finitely supported\nprobability measures.\nThe Giry monad for finitely supported probability measures is called the finitary Giry monad, although\nsometimes also goes by the more descriptive names distribution monad and convex combination monad.\nOn a set ùê¥, let ùëÉ(ùê¥) denote the set of all finitely-supported probability measures on ùê¥‚úø. An element of ùëÉ(ùê¥)\nis a list of elements of ùê¥together with probabilities. The probability assigned to an element ùëé: ùê¥is denoted by\nùëù(ùëé).\nIn our development we state this as the record\nRecord Pmf (A : Type) := mkPmf {\noutcomes :> list ( nonnegreal ‚àóA);\nsum1 : list_fst_sum outcomes = R1\n}.\nwhere outcomes stores all the entries of the type A along with their atomic probabilities. The field sum1 ensures\nthat the probabilities sum to 1.\nThe Giry monad is defined in terms of two basic operations associated to this space:\nret : ùê¥‚ÜíùëÉ(ùê¥) ‚úø\nùëé‚Ü¶‚ÜíùúÜùë•: ùê¥, ùõøùëé(ùë•)\nwhere ùõøùëé(ùë•) = 1 if ùëé= ùë•and 0 otherwise. The other basic operation is\nbind : ùëÉ(ùê¥) ‚Üí(ùê¥‚ÜíùëÉ(ùêµ)) ‚ÜíùëÉ(ùêµ) ‚úø\nbind ùëùùëì= ùúÜùëè: ùêµ,\n‚àëÔ∏Å\nùëé‚ààùê¥\nùëì(ùëé)(ùëè) ‚àóùëù(ùëé)\nIn both cases the resulting output is a probability measure. The above definition is well-defined because we\nonly consider finitely-supported probability measures. A more general case is obtained by replacing sums with\nintegrals.\nThe definitions of bind and ret satisfy the following properties:\nbind (ret ùë•) ùëì= ret(ùëì(ùë•)) ‚úø\n(3)\nbind ùëù(ùúÜùë•, ùõøùë•) = ùëù‚úø\n(4)\nbind (bind ùëùùëì) ùëî= bind ùëù(ùúÜùë•, bind (ùëìùë•) ùëî) ‚úø\n(5)\nThese monad laws establish that the triple (ùëÉ, bind, ret) forms a monad.\nThe Giry monad has been extensively studied and used by various authors because it has several attractive\nqualities that simplify (especially formal) proofs. First, the Giry monad naturally admits a denotational monadic\nsemantics for certain probabilistic programs [RP02, JP89, ≈öGG15, APM09]. Second, it is useful for rigorously\nformalizing certain informal arguments in probability theory by providing a means to perform ad hoc notation\noverloading [TTV19]. Third, it can simplify certain constructions such as that of the product measure [EHN15].\nCertRL uses the Giry monad as a substitute for the stochastic matrix associated to a Markov decision process.\nThis is possible because the Kleisli composition of the Giry monad recovers the Chapman-Kolmogorov formula\n[Per18, Per19]. The Kleisli composition is the fish operator in Haskell parlance.\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\n2.4.1\nKleisli Composition. Reasoning about probabilistic processes requires composing probabilities. The Chapman-\nKolmogorov formula is a classical result in the theory of Markovian processes that states the probability of\ntransition from one state to another through two steps can be obtained by summing up the probability of visiting\neach intermediate state. This application of the Chapman-Kolmogorov formula plays a fundamental role in the\nstudy of Markovian processes, but requires formalizing and reasoning about matrix operations.\nKleisli composition provides an alternative and more elegant mechanism for reasoning about compositions of\nprobabilistic choices. This section defines and provides an intuition for Kleisli composition.\nThink of ùëÉ(ùê¥) as the random elements of ùê¥([Per18, page 15]). In this paradigm, the set of maps ùê¥‚ÜíùëÉ(ùêµ) are\nsimply the set of maps with a random outcome. When ùëÉis a monad, such maps are called Kleisli arrows of ùëÉ.\nIn terms of reinforcement learning, a map ùëì: ùê¥‚ÜíùëÉ(ùêµ) is a rule which takes a state ùëé: ùê¥and gives the\nprobability of transitioning to state ùëè: ùêµ. Suppose now that we have another such rule ùëî: ùêµ‚ÜíùëÉ(ùê∂). Kleisli\ncomposition puts ùëìand ùëîtogether to give a map (ùëì\nùëî) : ùê¥‚ÜíùëÉ(ùê∂). It is defined as:\nùëì\nùëî:= ùúÜùë•: ùê¥, bind (ùëìùë•) ùëî\n(6)\n= ùúÜùë•: ùê¥, (ùúÜùëê: ùê∂,\n‚àëÔ∏Å\nùëè:ùêµ\nùëî(ùëè)(ùëê) ‚àóùëì(ùë•)(ùëè))\n(7)\n= ùúÜ(ùë•: ùê¥) (ùëê: ùê∂),\n‚àëÔ∏Å\nùëè:ùêµ\nùëì(ùë•)(ùëè) ‚àóùëî(ùëè)(ùëê)\n(8)\nThe motivation for (6)‚Äì(8) is intuitive. In order to start at ùë•: ùê¥and end up at ùëê: ùê∂by following the rules ùëì\nand ùëî, one must first pass through an intermediate state ùëè: ùêµin the codomain of ùëìand the domain of ùëî. The\nprobability of that point being any particular ùëè: ùêµis\nùëì(ùë•)(ùëè) ‚àóùëî(ùëè)(ùëê).\nSo, to obtain the total probability of transitioning from ùë•to ùëê, simply sum over all intermediate states ùëè: ùêµ. This\nis exactly (8). We thus recover the classical Chapman-Kolmogorov formula, but as a Kleisli composition of the\nGiry monad. This obviates the need for reasoning about operators on linear vector spaces, thereby substantially\nsimplifying the formalization effort.\nIndeed, if we did not use Kleisli composition, we would have to associate a stochastic transition matrix to our\nMarkov process and manually prove various properties about stochastic matrices which can quickly get tedious.\nWith Kleisli composition however, our proofs become more natural and we reason closer to the metal instead of\nadapting to a particular representation.\n3\nTHE CERTRL LIBRARY\nCertRL contains a formalization of Markov decision processes, a definition of the Kleisli composition specialized\nto Markov decision processes, a definition of the long-term value of a Markov decision process, a definition of\nthe Bellman operator, and a formalization of the operator‚Äôs main properties.\nBuilding on top of its library of results about Markov decision processes, CertRL contains proofs of our main\nresults:\n(1) the (infinite) sequence of value functions obtained by value iteration converges in the limit to a global\noptimum assuming stationary policies,\n(2) the (infinite) sequence of policies obtained by policy iteration converges in the limit to a global optimum\nassuming stationary policies, and\n(3) the optimal value function for Markov decision process of length ùëõis computed inductively by application\nof Bellman operator, Section 3.4.\nThe following sections describe the above results more carefully.\nVajjha et al.\n3.1\nMarkov Decision Processes\nWe refer to [Put94] for detailed presentation of the theory of Markov decision processes. Our formalization\nconsiders the theory of infinite-horizon discounted Markov decision processes with deterministic stationary policies.\nWe now elaborate on the above definitions and set up relevant notation. Our presentation will be type-theoretic\nin nature, to reflect the formal development. The exposition (and CertRL formalization) closely follows the work\nof Frank Feys, Helle Hvid Hansen, and Lawrence Moss [FHM18].\n3.1.1\nBasic Definitions.\nDefinition 6 (Markov Decision Process ‚úø). A Markov decision process consists of the following data:\n‚Ä¢ A nonempty finite type ùëÜcalled the set of states.2\n‚Ä¢ For each state ùë†: ùëÜ, a nonempty finite type ùê¥(ùë†) called the type of actions available at state ùë†. This is modelled\nas a dependent type.\n‚Ä¢ A stochastic transition structure ùëá: √é\nùë†:ùëÜ(ùê¥(ùë†) ‚ÜíùëÉ(ùëÜ)). Here ùëÉ(ùëÜ) stands for the set of all probability\nmeasures on ùëÜ, as described in Section 2.4.\n‚Ä¢ A reward function ùëü: √é\nùë†:ùëÜ(ùê¥(ùë†) ‚ÜíùëÜ‚ÜíR) where ùëü(ùë†,ùëé,ùë†‚Ä≤) is the reward obtained on transition from state ùë†\nto state ùë†‚Ä≤ under action ùëé.\nFrom these definitions it follows that the rewards are bounded in absolute value: since the state and action\nspaces are finite, there exists a constant ùê∑such that\n‚àÄ(ùë†ùë†‚Ä≤ : ùëÜ), (ùëé: ùê¥(ùë†)), |ùëü(ùë†,ùëé,ùë†‚Ä≤)| ‚â§ùê∑‚úø\n(9)\nDefinition 7 (Decision Rule / Policy). Given a Markov decision process with state space ùëÜand action space\n√é\nùë†:ùëÜùê¥(ùë†),\n‚Ä¢ A function ùúã: √é\nùë†:ùëÜùê¥(ùë†) is called a decision rule ‚úø. The decision rule is deterministic 3.\n‚Ä¢ A stationary policy is an infinite sequence of decision rules: (ùúã, ùúã, ùúã, ...) ‚úø. Stationary implies that the same\ndecision rule is applies at each step.\nThis policy ùúãinduces a stochastic dynamic process on ùëÜevolving in discrete time steps ùëò‚ààZ‚â•0. In this section\nwe consider only stationary policies, and therefore use the terms policy and decision rule interchangeably.\n3.1.2\nKleisli Composites in a Markov Decision Process. Note that for a fixed decision rule ùúã, we get a Kleisli arrow\nùëáùúã: ùëÜ‚ÜíùëÉ(ùëÜ) defined as ùëáùúã(ùë†) = ùëá(ùë†)(ùúã(ùë†)).\nConventionally, ùëáùúãis represented as a row-stochastic matrix (ùëáùúã)ùë†ùë†‚Ä≤ that acts on the probability co-vectors\nfrom the right, so that the row ùë†of ùëáùúãcorresponding to state ùë†encodes the probability distribution of states ùë†‚Ä≤\nafter a transition from the state ùë†.\nLet ùëùùëò‚ààùëÉ(ùëÜ) for ùëò‚ààZ‚â•0 denote a probability distribution on ùëÜevolving under the policy stochastic map ùëáùúã\nafter ùëòtransition steps, so that ùëù0 is the initial probability distribution on ùëÜ(the initial distribution is usually\ntaken to be ret ùë†0 for a state ùë†0). These are related by\nùëùùëò= ùëù0ùëáùëò\nùúã\n(10)\nIn general (if ùëù0 = ret ùë†0) the number ùëùùëò(ùë†) gives the probability that starting out at ùë†0, one ends up at ùë†after ùëò\nstages. So, for example, if ùëò= 1, we recover the stochastic transition structure at the end of the first step ‚úø.\n2There are various definitions of finite. Our mechanization uses surjective finiteness (the existence of a surjection from a bounded set of\nnatural numbers)‚úø, and assumes that there is a decidable equality on ùëÜ. This pair of assumptions is equivalent to bijectve finitness.\n3if the decision rule takes a state and returns a probability distribution on actions instead, it is called stochastic.\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\nInstead of representing ùëáùëò\nùúãas an iterated product of a stochastic matrix in our formalization, we recognize that\n(10) states that ùëùùëòis the ùëò-fold iterated Kleisli composite of ùëáùúãapplied to the initial distribution ùëù0 ‚úø.\nùëùùëò= (ùëù0\nùëáùúã\n. . .\nùëáùúã\n|            {z            }\nùëòtimes\n)\n(11)\nThus, we bypass the need to define matrices and matrix multiplication entirely in the formalization.\n3.1.3\nLong-Term Value of a Markov Decision Process. Since the transition from one state to another by an action\nis governed by a probability distribution ùëá, there is a notion of expected reward with respect to that distribution.\nDefinition 8 (Expected immediate reward). For a Markov decision process,\n‚Ä¢ An expected immediate reward to be obtained in the transition under action ùëéfrom state ùë†to state ùë†‚Ä≤ is a\nfunction ¬Øùëü: ùëÜ‚Üíùê¥‚ÜíR computed by averaging the reward function over the stochastic transition map to a\nnew state ùë†‚Ä≤\n¬Øùëü(ùë†,ùëé) :=\n‚àëÔ∏Å\nùë†‚Ä≤‚ààùëÜ\nùëü(ùë†,ùëé,ùë†‚Ä≤)ùëá(ùë†,ùëé)(ùë†‚Ä≤)\n(12)\n‚Ä¢ An expected immediate reward under a decision rule ùúã, denoted ¬Øùëüùúã: ùëÜ‚ÜíR is defined to be:\n¬Øùëüùúã(ùë†) := ¬Øùëü(ùë†, ùúã(ùë†)) ‚úø\n(13)\nThat is, we replace the action argument in (12) by the action prescribed by the decision rule ùúã.\n‚Ä¢ The expected reward at time step ùëòof a Markov decision process starting at initial state ùë†, following policy ùúã\nis defined as the expected value of the reward with respect to the ùëò-th Kleisli iterate of ùëáùúãstarting at state ùë†.\nùëüùúã\nùëò(ùë†) := Eùëáùëòùúã(ùë†) [¬Øùëüùúã] =\n‚àëÔ∏Å\nùë†‚Ä≤‚ààùëÜ\nh\n¬Øùëüùúã(ùë†‚Ä≤)ùëáùëò\nùúã(ùë†)(ùë†‚Ä≤)\ni\n‚úø\nThe long-term value of a Markov decision process under a policy ùúãis defined as follows:\nDefinition 9 (Long-Term Value). Let ùõæ‚ààR, 0 ‚â§ùõæ< 1 be a discount factor, and ùúã= (ùúã, ùúã, . . . ) be a stationary\npolicy. Then ùëâùúã: ùëÜ‚ÜíR is given by\nùëâùúã(ùë†) =\n‚àû\n‚àëÔ∏Å\nùëò=0\nùõæùëòùëüùúã\nùëò(ùë†) ‚úø\n(14)\nThe rewards being bounded in absolute value implies that the long-term value function ùëâùúãis well-defined for\nevery initial state ‚úø.\nIt can be shown by manipulating the series in (14) that the long-term value satisfies the Bellman equation:\nùëâùúã(ùë†) = ¬Øùë£(ùë†, ùúã(ùë†)) + ùõæ\n‚àëÔ∏Å\nùë†‚Ä≤‚ààùëÜ\nùëâùúã(ùë†‚Ä≤)ùëáùúã(ùë†)(ùë†‚Ä≤) ‚úø\n(15)\n= ¬Øùëüùúã(ùë†) + ùõæEùëáùúã(ùë†) [ùëâùúã]\n(16)\nDefinition 10. Given a Markov decision process, we define the Bellman operator as\nBùúã:(ùëÜ‚ÜíR) ‚Üí(ùëÜ‚ÜíR)\n(17)\nùëä‚Ü¶‚Üí¬Øùëüùúã(ùë†) + ùõæEùëáùúã(ùë†)ùëä\n(18)\nTheorem 11 (Properties of the Bellman Operator ‚úø). The Bellman operator satisfies the following properties:\n‚Ä¢ As is evident from (15), the long-term value ùëâùúãis the fixed point of the operator Bùúã‚úø.\n‚Ä¢ The operator Bùúã(called the Bellman operator) is a contraction in the norm (2) ‚úø.\nVajjha et al.\n‚Ä¢ The operator Bùúãis a monotone operator. That is,\n‚àÄùë†,ùëä1(ùë†) ‚â§ùëä2(ùë†) ‚áí‚àÄùë†, Bùúã(ùëä1)(ùë†) ‚â§Bùúã(ùëä2)(ùë†)\nThe Banach fixed point theorem now says that ùëâùúãis the unique fixed point of this operator.\nLet ùëâùúã,ùëõ: ùëÜ‚ÜíR be the ùëõ-th iterate of the Bellman operator ùêµùúã. It can be computed by the recursion relation\nùëâùúã,0(ùë†0) = 0\nùëâùúã,ùëõ+1(ùë†0) = ¬Øùëüùúã(ùë†0) + ùõæEùëáùúã(ùë†0)ùëâùúã,ùëõ\nùëõ‚ààZ‚â•0\n(19)\nwhere ùë†0 is an arbitrary initial state. The first term in the reward function ùëâùúã,ùëõ+1 for the process of length ùëõ+ 1 is\nthe sum of the reward collected in the first step (immediate reward), and the remaining total reward obtained in\nthe subsequent process of length ùëõ(discounted future reward). The ùëõ-th iterate is also seen to be equal to the ùëõ-th\npartial sum of the series (14) ‚úø.\nThe sequence of iterates {ùëâùúã,ùëõ}|ùëõ=0,1,2,... is convergent and equals ùëâùúã, by the Banach fixed point theorem.\nùëâùúã= lim\nùëõ‚Üí‚àûùëâùúã,ùëõ‚úø\n(20)\n3.2\nConvergence of Value Iteration\nIn the previous subsection we defined the long-term value function ùëâùúãand showed that it is the fixed point of the\nBellman operator. It is also the pointwise limit of the iterates ùëâùúã,ùëõ, which is the expected value of all length ùëõ\nrealizations of the Markov decision process following a fixed stationary policy ùúã.\nWe note that the value function ùëâùúãinduces a partial order on the space of all decision rules; with ùúé‚â§ùúèif and\nonly if ùëâùúé‚â§ùëâùúè‚úø.\nThe space of all decision rules is finite because the state and action spaces are finite ‚úø.\nThe above facts imply the existence of a decision rule (stationary policy) which maximizes the long-term\nreward. We call this stationary policy the optimal policy and its long-term value the optimal value function.\nùëâ‚àó(ùë†) = max\nùúã{ùëâùúã(ùë†)} ‚úø\n(21)\nThe aim of reinforcement learning, as we remarked in the introduction, is to have tractable algorithms to find\nthe optimal policy and the optimal value function corresponding to the optimal policy.\nBellman‚Äôs value iteration algorithm is such an algorithm, which is known to converge asymptotically to the\noptimal value function. In this section we describe this algorithm and formally prove this convergence property.\nDefinition 12. Given a Markov decision process we define the Bellman optimality operator as:\nÀÜB :(ùëÜ‚ÜíR) ‚Üí(ùëÜ‚ÜíR)\nùëä‚Ü¶‚ÜíùúÜùë†, max\nùëé‚ààùê¥(ùë†)\n\u0000¬Øùëü(ùë†,ùëé) + ùõæEùëá(ùë†,ùëé) [ùëä]\u0001 ‚úø\nTheorem 13. The Bellman optimality operator ÀÜB satisfies the following properties:\n‚Ä¢ The operator ÀÜB is a contraction with respect to the ùêø‚àûnorm (2) ‚úø.\n‚Ä¢ The operator ÀÜB is a monotone operator. That is,\n‚àÄùë†,ùëä1(ùë†) ‚â§ùëä2(ùë†) ‚áí‚àÄùë†, ÀÜB(ùëä1)(ùë†) ‚â§ÀÜB(ùëä2)(ùë†) ‚úø\nNow we move on to proving the most important property of ÀÜB: the optimal value function ùëâ‚àóis a fixed point\nof ÀÜB.\nBy Theorem 13 and the Banach fixed point theorem, we know that the fixed point of ÀÜB exists. Let us denote it\nÀÜùëâ. Then we have:\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\nTheorem 14 (Lemma 1 of [FHM18] ‚úø). For every decision rule ùúé, we have ùëâùúé‚â§ÀÜùëâ.\nProof. Fix a policy ùúé. Note that for every ùëì: ùëÜ‚ÜíR, we have Bùúé(ùëì) ‚â§ÀÜB(ùëì)‚úø. In particular, applying this to\nùëì= ùëâùúéand using Theorem 11, we get that ùëâùúé= Bùúé(ùëâùúé) ‚â§ÀÜB(ùëâùúé). Now by contraction coinduction (Theorem 5\nwith ùêπ= ÀÜB along with Theorem 13) we get that ùëâùúé‚â§ÀÜùëâ.\n‚ñ°\nTheorem 14 immediately implies that ùëâ‚àó‚â§ÀÜùëâ.\nTo go the other way, we introduce the following policy, called the greedy decision rule.\nùúé‚àó(ùë†) := argmaxùëé‚ààùê¥(ùë†)\n\u0010\n¬Øùëü(ùëé,ùë†) + ùõæEùëá(ùë†,ùëé) [ ÀÜùëâ]\n\u0011\n‚úø\n(22)\nWe now have the following theorem:\nTheorem 15 (Proposition 1 of [FHM18] ‚úø). The greedy policy is the policy whose long-term value is the fixed\npoint of ÀÜB:\nùëâùúé‚àó= ÀÜùëâ\nProof. We observe that Bùúé‚àó( ÀÜùëâ) = ÀÜùëâ‚úø. Thus, ÀÜùëâ‚â§Bùúé‚àó( ÀÜùëâ). Note that we have ùëâùúé‚àóis the fixed point of ùêµùúé‚àóby\nTheorem 11. Now applying contraction coinduction with ùêπ= Bùúé‚àó, we get ÀÜùëâ‚â§ùëâùúé‚àó. From Theorem 14 we get that\nùëâùúé‚àó‚â§ÀÜùëâ.\n‚ñ°\nTheorem 15 implies that ùëâ‚àó‚â•ÀÜùëâand so we conclude that ùëâ‚àó= ÀÜùëâ‚úø.\nThus, the fixed point of the optimal Bellman operator ÀÜB exists and is equal to the optimal value function.\nStated fully, value iteration proceeds by:\n(1) Initialize a value function ùëâ0 : ùëÜ‚ÜíR.\n(2) Define ùëâùëõ+1 = ÀÜBùëâùëõfor ùëõ‚â•0. At each stage, the following policy is computed\nùúãùëõ(ùë†) ‚ààargmaxùëé‚ààùê¥(ùë†)\n\u0000¬Øùëü(ùë†,ùëé) + ùõæEùëá(ùë†,ùëé) [ùëâùëõ]\u0001\nBy the Banach Fixed Point Theorem, the sequence {ùëâùëõ} converges to the optimal value function ùëâ‚àó‚úø. In\npractice, one repeats this iteration as many times as needed until a fixed threshold is breached.\nIn Section 3.4 we explain and provide a formalized proof of the dynamic programming principle: the value\nfunctionùëâùëõis equal to the optimal value function of a finite-horizon MDP of lengthùëõwith a possibly non-stationary\noptimal policy.\n3.3\nConvergence of Policy Iteration\nThe convergence of value iteration is asymptotic, which means the iteration is continued until a fixed threshold\nis breached. Policy iteration is a similar iterative algorithm that benefits from a more definite stopping condition.\nDefine the ùëÑfunction to be:\nùëÑùúã(ùë†,ùëé) := ¬Øùëü(ùë†,ùëé) + ùõæEùëá(ùë†,ùëé) [ùëâùúã].\nThe policy iteration algorithm proceeds in the following steps:\n(1) Initialize the policy to ùúã0.\n(2) Policy evaluation: For ùëõ‚â•0, given ùúãùëõ, compute ùëâùúãùëõ.\n(3) Policy improvement: From ùëâùúãùëõ, compute the greedy policy:\nùúãùëõ+1(ùë†) ‚ààargmaxùëé‚ààùê¥(ùë†)\n\u0002\nùëÑùúãùëõ(ùë†,ùëé)\n\u0003\n(4) Check if ùëâùúãùëõ= ùëâùúãùëõ+1. If yes, stop.\n(5) If not, repeat (2) and (3).\nThis algorithm depends on the following results for correctness. We follow the presentation from [FHM18].\nVajjha et al.\nDefinition 16 (Improved policy ‚úø). A policy ùúèis called an improvement of a policy ùúéif for all ùë†‚ààùëÜit holds that\nùúè(ùë†) = argmaxùëé‚ààùê¥(ùë†) [ùëÑùúé(ùë†,ùëé)]]\nSo, step (2) of the policy iteration algorithm simply constructs an improved policy from the previous policy at\neach stage.\nTheorem 17 (Policy Improvement Theorem). Let ùúéand ùúèbe two policies.\n‚Ä¢ If Bùúèùëâùúé‚â•Bùúéùëâùúéthen ùëâùúè‚â•ùëâùúé‚úø.\n‚Ä¢ If Bùúèùëâùúé‚â§Bùúéùëâùúéthen ùëâùúè‚â§ùëâùúé‚úø.\nUsing the above theorem, we have:\nTheorem 18 (Policy Improvement Improves Values ‚úø). If ùúéand ùúèare two policies and if ùúèis an improvement\nof ùúé, then we have ùëâùúè‚â•ùëâùúé.\nProof. From Theorem 17, it is enough to show Bùúèùëâùúé‚â•Bùúéùëâùúé. We have that ùúèis an improvement of ùúé.\nùúè(ùë†) = argmaxùëé‚ààùê¥(ùë†) [ùëÑùúé(ùë†,ùëé)]\n(23)\n= argmaxùëé‚ààùê¥(ùë†)\n\u0002¬Øùëü(ùë†,ùëé) + ùõæEùëá(ùë†,ùëé) [ùëâùúé]\n\u0003\n(24)\nNote that\nBùúèùëâùúé= ¬Øùëü(ùë†,ùúè(ùë†)) + ùõæEùëá(ùë†,ùúè(ùë†)) [ùëâùúé]\n= max\nùëé‚ààùê¥(ùë†)\n\u0002¬Øùëü(ùë†,ùëé) + ùõæEùëá(ùë†,ùëé) [ùëâùúé]\n\u0003\nby (24)\n‚â•¬Øùëü(ùë†, ùúé(ùë†)) + ùõæEùëá(ùë†,ùúé(ùë†)) [ùëâùúé]\n= Bùúéùëâùúé\n‚ñ°\nIn other words, since ùúãùëõ+1 is an improvement of ùúãùëõby construction, the above theorem implies thatùëâùúãùëõ‚â§ùëâùúãùëõ+1.\nThis means that ùúãùëõ‚â§ùúãùëõ+1.\nThus, the policy constructed in each stage in the policy iteration algorithm is an improvement of the policy\nin the previous stage. Since the set of policies is finite ‚úø, this policy list must at some point stabilize. Thus, the\nalgorithm is guaranteed to terminate.\nIn Section 3.4 we will provide formalization of the statement that ùúãùëõis actually the optimal policy to follow for\nan MDP process of any finite length at that timestep when ùëõsteps remain towards the end of the process.\n3.4\nOptimal policy for finite time horizon Markov decision processes\nAll results up to this subsection were stated in terms of the convergences of infinite sequences of states and\nactions. Stating convergence results in terms of the limits of infinite sequences is not uncommon in texts on\nreinforcement learning; however, in practice, reinforcement learning algorithms are always run for a finite\nnumber of steps. In this section we consider decision processes of finite length and do not impose an assumption\nthat the optimal policy is stationary.\nLet ùëâ¬Øùúãdenote the value function of Markov decision process for a finite sequence of policies ¬Øùúã= ùúã0 :: ùúã1 :: ùúã2 ::\n. . . ùúãùëõ‚àí1 of length ùëõ= len( ¬Øùúã). Denote by ùëù0 the probability distribution over the initial state at the start of the\nprocess.\nWe define the probability measure at step ùëòin terms of Kleisli iterates for each decision rule ùúãùëñfor ùëñin 0 . . . (ùëò‚àí1):\nùëù0ùëá¬Øùúã[:ùëò] := (ùëù0\nùëáùúã0 . . .\nùëáùúãùëò‚àí1) ‚úø\n(25)\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\nBelow, we will use the pairing notation (bra-ket notation)\n‚ü®ùëù|ùëâ‚ü©:= Eùëù[ùëâ]\n(26)\nbetween a probability measure ùëùon a finite set ùëÜand a function ùëâ: ùëÜ‚ÜíR, so that |ùëâ‚ü©is an element of the vector\nspace of real valued functions on ùëÜ, ‚ü®ùëù| is a linear form on this vector space associated to a probablity measure ùëù\non ùëÜ, and ‚ü®ùëù|ùëâ‚ü©denotes evaluation of a linear form ‚ü®ùëù| on a vector |ùëâ‚ü©.\nDefinition 19 (expectation value function of MDP of length ùëõ= len( ¬Øùúã) over the initial probability distribution\nùëù0).\n‚ü®ùëù0|ùëâ¬Øùúã‚ü©=\nùëõ‚àí1\n‚àëÔ∏Å\nùëò=0\nùõæùëò‚ü®ùëù0ùëá¬Øùúã[:ùëò]|¬Øùëüùúãùëò‚ü©‚úø\n(27)\nDefinition 19 implies the recursion relation\n‚ü®ùëù0|ùëâùúã0::tail‚ü©= ‚ü®ùëù0|¬Øùëüùúã0 + ùõæùëáùúã0ùëâtail‚ü©‚úø\nùëõ‚ààZ‚â•0\n(28)\nwhere ¬Øùúã= ùúã0 :: tail.\nLet ÀÜùëâ‚àó,ùëõbe the optimal value function of the Markov decision process of length ùëõon the space of all policy\nsequences of length ùëõ:\nÀÜùëâ‚àó,ùëõ:=\nsup\n¬Øùúã|len(ùúã)=ùëõ\nùëâ¬Øùúã‚úø\n(29)\nLet ÀÜùëâùúã0::‚àó,ùëõ+1 be the optimal value function of the Markov decision process of length ùëõ+ 1 on the space of all\npolicy sequences of length ùëõ+ 1 whose initial term is ùúã0. Using the relation (28) and that\nsup\nùúã0::tail\nùëâùúã0::tail,ùëõ+1 = sup\nùúã0\nsup\ntail\nùëâùúã0::tail,ùëõ+1 ‚úø\n(30)\nwe find\n‚ü®ùëù0| ÀÜùëâ‚àó,ùëõ+1‚ü©=\nsup\nùúã0‚àà√é\nùëÜùê¥(ùë†)\n‚ü®ùëù0|¬Øùëüùúã0 + ùõæùëáùúã0 ÀÜùëâ‚àó,ùëõ‚ü©‚úø\nùëõ‚ààZ>=0\n(31)\nwith the initial term of the sequence ùëâ‚àó,0 = 0.\nThe result (31) can be formulated as follows\nTheorem 20 (Bellman‚Äôs finite-time optimal policy theorem). The optimal value function ÀÜùëâ‚àó,ùëõ+1 of a Markov\ndecision process of length ùëõ+ 1 relates to the optimal value function of the same Markov decision process of length ùëõ\nby the inductive relation\nÀÜùëâ‚àó,ùëõ+1 = ÀÜBùëâ‚àó,ùëõ\n(32)\nwhere ÀÜB is Bellman optimality operator (Definition 12).\nThe iterative computation of the sequence of optimal value functions { ÀÜùëâ‚àó,ùëõ}ùëõ‚ààZ‚â•0 of Markov decision processes\nof length ùëõ= 0, 1, 2, . . . from the recursion ÀÜùëâ‚àó,ùëõ+1 = ÀÜB ÀÜùëâ‚àó,ùëõis the same algorithm as value iteration.\n3.5\nComments on Formalization\nCertRL contributes a formal library for reasoning about Markov decision processes. We demonstrate the effec-\ntiveness of this library‚Äôs building blocks by proving the two most canonical results from reinforcement learning\ntheory. In this section we reflect on the structure of CertRL‚Äôs formalization, substantiating our claim that CertRL\nserves as a convenient foundations for a continuing line of work on formalization of reinforcement learning\ntheory.\nVajjha et al.\n3.5.1\nCharacterizing Optimality. Most texts on Markov decision processses (for example [Put94, Section 2.1.6])\nstart out with a probability space on the space of all possible realizations of the Markov decision process. The\nlong-term value for an infinite-horizon Markov decision process is then defined as the expected value over all\npossible realizations:\nùëâùúã(ùë†) = E(ùë•1,ùë•2,... )\n\" ‚àû\n‚àëÔ∏Å\nùëò=0\nùõæùëòùë£(ùë•ùëò, ùúã(ùë•ùëò))|ùë•0 = ùë†; ùúã\n#\n(33)\nwhere each ùë•ùëòis drawn from the distribution ùëá(ùë•ùëò‚àí1, ùúã(ùë•ùëò‚àí1)). This definition is hard to work with because, as\n[Put94] notes, it ignores the dynamics of the problem. Fortunately, it is also unnecessary since statements about\nthe totality of all realizations are rarely made.\nIn our setup, following [FHM18], we only consider the probability space over the finite set of states of the\nMarkov decision process. By identifying the basic operation of Kleisli composition, we generate more realizations\n(and their expected rewards) on the fly as and when needed.\nImplementations of reinforcement learning algorithms often compute the long-term value using matrix\noperators for efficiency reasons. The observation that clean theoretical tools do not necessarily entail efficient\nimplementations is not a new observation; both Puterman [Put94] and H√∂lzl [Hoe17a] make similar remarks.\nFortunately, the design of our library provides a clean interface for future work on formalizing efficiency\nimprovements. Extending CertRL with correctness theorems for algorithms that use matrix operations requires\nnothing more than a proof that the relevant matrix operations satisfy the definition of Kleisli composition.\n3.5.2\nComparison of English and Coq Proofs. Comparing Theorem 14 and Theorem 15 with the the equivalent\nresults from Puterman [Put94, Theorem 6.2.2] demonstrates that CertRL avoids reasoning about low-level ùúñ‚àíùõø\ndetails through strategic use of coinduction.\nThe usefulness of contraction coinduction is reflected in the formalization, sometimes resulting in Coq proofs\nwhose length is almost the same as the English text.\nWe compare in Table 1 the Coq proof of Theorem 15 to an English proof of the same. The two proofs are roughly\nequivalent in length and, crucially, also make essentially the same argument at the same level of abstraction. Note\nthat what we compare is not exactly the proof from Feys et al. [FHM18, Proposition 1], but is as close as possible\nto a restatement of their Proposition 1 and Lemma 1 with the proof of Lemma 1 inlined and the construction\nrestated in terms of our development. The full proof from [FHM18], with Lemma 1 inlined, reads as follows:\nProposition 1: The greedy policy is optimal. That is, ùêøùëáùëâùúé‚àó= ùëâ‚àó.\n(1) Observe that Œ®ùúé‚àó‚â•ùëâ‚àó(in fact, equality holds).\n(2) By contraction coinduction, ùëâ‚àó‚â§ùêøùëáùëâùúé‚àó.\n(3) Lemma 1: For all policies ùúé, ùêøùëáùëâùúé‚â§ùëâ‚àó.\n(4) A straightforward calculation and monotonicity argument shows that for all ùëì‚ààùêµ(ùëÜ, R), Œ®ùúé(ùëì) ‚â§\nŒ®‚àó(ùëì).\n(5) In particular, ùêøùëáùëâùúé= Œ®ùúé(ùêøùëáùëâùúé) ‚â§Œ®‚àó(ùêøùëáùëâùúé).\n(6) By contraction coinduction we conclude that ùêøùëáùëâùúé‚â§ùëâ‚àó.\nTable 1 compares two coinductive proofs ‚Äì one in English and the other in Coq. Another important comparison is\nbetween a Coq coinductive proof and an English non-coinductive proof. The Coq proof of the policy improvement\ntheorem provides one such point of comparison. Recall that theorem states that a particular closed property\n(the set {ùë•|ùë•‚â§ùë¶}) holds of the fixed point of a particular contractive map (the Bellman operator). The most\ncommon argument ‚Äì presented in the most common textbook on reinforcement learning ‚Äì proves this theorem\nby expanding the infinite sum in multiple steps [SB98, Section 4.2]. We reproduce this below:\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\nTheorem 15 (Proposition 1 of [FHM18] ‚úø). The greedy\npolicy is the policy whose long-term value is the fixed point\nof ÀÜB:\nùëâùúé‚àó= ÀÜùëâ\nProof.\n(1) ùëâùúé‚àó‚â§ÀÜùëâfollows by Theorem 14.\n(2) Now we have to show ÀÜùëâ‚â§ùëâùúé‚àó. Note that we have\nùëâùúé‚àóis the fixed point of ùêµùúé‚àóby Theorem 11.\n(3) We can now apply contraction coinduction with\nùêπ= Bùúé‚àó.\n(4) The hypotheses are satisfied since by Theorem 11,\nthe Bùúé‚àóis a contraction and it is a monotone oper-\nator.\n(5) The only hypothesis left to show is ÀÜùëâ‚â§Bùúé‚àóÀÜùëâ.\n(6) But in fact, we have Bùúé‚àó( ÀÜùëâ) = ÀÜùëâby the definition\nof ùúé‚àó.\n‚ñ°\n(a) English proof adapted from [FHM18].\n1 Lemma exists_fixpt_policy\n: forall init,\n2\nlet V'\n:= fixpt (bellman_max_op) in\n3\nlet pi' := greedy init in\n4\nltv gamma pi' = V' init.\n5 Proof.\n6 intros init V' pi';\n7 eapply Rfct_le_antisym; split.\n8\n‚àíeapply ltv_Rfct_le_fixpt.\n9\n‚àírewrite (ltv_bellman_op_fixpt _ init).\n10\napply contraction_coinduction_Rfct_ge'.\n11\n+ apply is_contraction_bellman_op.\n12\n+ apply bellman_op_monotone_ge.\n13\n+ unfold V', pi'.\n14\nnow rewrite greedy_argmax_is_max.\n15 Qed.\n(b) Coq proof ‚úø\nTable 1. Comparison of English and Coq proofs of Theorem 15.\nTheorem 21 (Policy Improvement Theorem [SB98]). Let ùúã, ùúã‚Ä≤ be a pair of deterministic policies such that, for\nall states ùë†,\nùëÑùúã(ùë†, ùúã‚Ä≤(ùë†)) ‚â•ùëâùúã(ùë†)\n(34)\nthen ùëâùúã‚Ä≤(ùë†) ‚â•ùëâùúã(ùë†).\nProof. Starting with (34) we keep expanding the ùëÑùúãside and reapplying (34) until we get ùëâùúã‚Ä≤(ùë†).\nùëâùúã(ùë†) ‚â§ùëÑùúã(ùë†, ùúã‚Ä≤(ùë†))\n= Eùúã‚Ä≤ {ùëüùë°+1 + ùõæùëâùúã(ùë†ùë°+1)|ùë†ùë°= ùë†}\n‚â§Eùúã‚Ä≤ {ùëüùë°+1 + ùõæùëÑùúã(ùë†ùë°+1, ùúã‚Ä≤(ùë†ùë°+1))|ùë†ùë°= ùë†}\n= Eùúã‚Ä≤ {ùëüùë°+1 + ùõæEùúã‚Ä≤ {ùëüùë°+2 + ùõæùëâùúã(ùë†ùë°+2)} |ùë†ùë°= ùë†}\n= Eùúã‚Ä≤ \b\nùëüùë°+1 + ùõæùëüùë°+2 + ùõæ2ùëâùúã(ùë†ùë°+2) | ùë†ùë°= ùë†\n\t\n‚â§Eùúã‚Ä≤ \b\nùëüùë°+1 + ùõæùëüùë°+2 + ùõæ2ùëüùë°+3 + ùõæ3ùëâùúã(ùë†ùë°+3) | ùë†ùë°= ùë†\n\t\n...\n‚â§Eùúã‚Ä≤ \b\nùëüùë°+1 + ùõæùëüùë°+2 + ùõæ2ùëüùë°+3 + ùõæ3ùëüùë°+4 . . . | ùë†ùë°= ùë†\n\t\n= ùëâùúã‚Ä≤(ùë†)\n‚ñ°\nVajjha et al.\nAt a high level, this proof proceeds by showing that the closed property {ùë•|ùë•‚â§ùë¶} holds of each partial sum of\nthe infinite series ùëâùúã(ùë†). By completeness and using the fact that partial sums converge to the full series ùëâùúã, this\nproperty is also shown to hold of the fixed point ùëâùúã.\nHowever, the coinductive version of this proof (Theorem 17) is simpler because it exploits the fact that this\nconstruction has already been done once in the proof of the fixed point theorem: the iterates of the contraction\noperator were already proven to converge to the fixed point and so there is no reason to repeat the construction\nagain. Thus, the proof is reduced to simply establishing the ‚Äúbase case‚Äù of the (co)induction.\nThe power of this method goes beyond simplifying proofs for Markov decision processes. See [KR09] for other\nexamples.\n4\nRELATED AND FUTURE WORK\nTo our knowledge, CertRL is the first formal proof of convergence for value iteration or policy iteration. Related\nwork falls into three categories:\n(1) libraries that CertRL builds upon,\n(2) formalizations of results from probability and machine learning, and\n(3) work at the intersection of formal verification and reinforcement learning.\nDependencies. CertRL builds on the Coquelicot [BLM14] library for real analysis. Our main results are statements\nabout fixed points of contractive maps in complete normed modules. CertRL therefore builds on the formal\ndevelopment of the Lax-Milgram theorem and, in particular, Boldo et al.‚Äôs formal proof of the Banach fixed point\ntheorem [BCF+17]. CertRL also makes extensive use of some utilities from the Q*cert project [AHM+17]. CertRL\nincludes a bespoke implementation of some basic results and constructions from probability theory and also an\nimplementation of the Giry monad. Our use of the monad for reasoning about probabilistic processes, as well as\nthe design of our library, is highly motivated by the design of the Polaris library [TH19]. Many of the thorough\nformalizations of probabilities in Coq ‚Äì such as the Polaris [TH19], Infotheo [AH12], and Alea [APM09] ‚Äì also\ncontain these results. Refactoring CertRL to build on top of one or more of these formalizations might allow\nfuture work on certified reinforcement learning to leverage future improvements to these libraries.\nBuilding on these other foundations, CertRL demonstrates how existing work on formalization enables\nformalization of key results in reinforcement learning theory.\nRelated Formalizations. There is a growing body of work on formalization of machine learning theory [TTV19,\nTTV+20, Hoe17b, SLD17, BS19, BBK19].\nJohannes H√∂lzl‚Äôs Isabelle/HOL development of Markov processes is most related to our own work [Hoe17b,\nHoe17a]. H√∂lzl builds on the probability theory libraries of Isabelle/HOL to develop continuous-time Markov\nchains. Many of H√∂lzl‚Äôs basic design choices are similar to ours; for example, he also uses the Giry monad to\nplace a monadic structure on probability spaces and also uses coinductive methods. CertRL focuses instead on\nformalization of convergence proofs for dynamic programming algorithms that solve Markov decision processes.\nIn the future, we plan to extend our formalization to include convergence proofs for model-free methods, in\nwhich a fixed Markov decision process is not known a priori.\nThe CertiGrad formalization by Selsam et al. contains a Lean proof that the gradients sampled by a stochastic\ncomputation graph are unbiased estimators of the true mathematical function [SLD17]. This result, together with\nour development of a library for proving convergence of reinforcement learning algorithms, provides a path\ntoward a formal proof of correctness for deep reinforcement learning.\nFormal Methods for RL. The likelihood that reinforcement learning algorithms will be deployed in safety-\ncritical settings during the coming decades motivates a growing body of work on formal methods for safe\nreinforcement learning. This approach ‚Äì variously called formally constrained reinforcement learning [HAK18],\nCertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq\nshielding[ABE+18], or verifiably safe reinforcement learning [HFM+20] ‚Äì uses temporal or dynamic logics to\nspecify constraints on the behavior of RL algorithms.\nGlobal convergence is a fundamental theoretical property of classical reinforcement learning algorithms, and\nin practice at least local convergence is an important property for any useful reinforcement learning algorithm.\nHowever, the formal proofs underlying these methods typically establish the correctness of a safety constraint\nbut do not formalize any convergence properties. In future work, we plan to establish an end-to-end proof that\nconstrained reinforcement learning safely converges by combining our current development with the safe RL\napproach of Fulton et al. [FP18] and the VeriPhy pipeline of Bohrer et al. [BTM+18].\n5\nCONCLUSIONS\nReinforcement learning algorithms are an important class of machine learning algorithms that are now being\ndeployed in safety-critical settings. Ensuring the correctness of these algorithms is societally important, but prov-\ning properties about stochastic processes presents several challenges. In this paper we show how a combination\nof metric coinduction and the Giry monad provides a convenient setting for formalizing convergence proofs for\nreinforcement learning algorithms.\nACKNOWLEDGMENTS\nWe thank Larry Moss, Sylvie Boldo and Mark Squillante for discussions related to this paper. Some of the work\ndescribed in this paper was performed while Koundinya Vajjha was an intern at IBM Research. Vajjha was\nadditionally supported by the Alfred P. Sloan Foundation under grant number G-2018-10067.\nREFERENCES\n[ABE+18] Mohammed Alshiekh, Roderick Bloem, R√ºdiger Ehlers, Bettina K√∂nighofer, Scott Niekum, and Ufuk Topcu. Safe reinforcement\nlearning via shielding. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference\non Artificial Intelligence (AAAI 2018). AAAI Press, 2018.\n[AH12] Reynald Affeldt and Manabu Hagiwara. Formalization of Shannon‚Äôs theorems in SSReflect-Coq. In 3rd Conference on Interactive\nTheorem Proving (ITP 2012), Princeton, New Jersey, USA, August 13‚Äì15, 2012, volume 7406 of Lecture Notes in Computer Science,\npages 233‚Äì249. Springer, Aug 2012.\n[AHM+17] Joshua S. Auerbach, Martin Hirzel, Louis Mandel, Avraham Shinnar, and J√©r√¥me Sim√©on. Q*cert: A platform for implementing\nand verifying query compilers. In Semih Salihoglu, Wenchao Zhou, Rada Chirkova, Jun Yang, and Dan Suciu, editors, Proceedings\nof the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017,\npages 1703‚Äì1706. ACM, 2017.\n[APM09] Philippe Audebaud and Christine Paulin-Mohring. Proofs of randomized algorithms in Coq. Science of Computer Programming,\n74(8):568‚Äì589, 2009.\n[BBK19] Alexander Bentkamp, Jasmin Christian Blanchette, and Dietrich Klakow. A formal proof of the expressiveness of deep learning.\nJ. Autom. Reason., 63(2):347‚Äì368, 2019.\n[BCF+17] Sylvie Boldo, Fran√ßois Cl√©ment, Florian Faissole, Vincent Martin, and Micaela Mayero. A Coq formal proof of the Lax‚ÄìMilgram\ntheorem. In 6th ACM SIGPLAN Conference on Certified Programs and Proofs, Paris, France, January 2017.\n[Bel54] Richard Bellman. The theory of dynamic programming. Bull. Amer. Math. Soc., 60(6):503‚Äì515, 11 1954.\n[BLM14] Sylvie Boldo, Catherine Lelay, and Guillaume Melquiond. Coquelicot: A user-friendly library of real analysis for Coq. Mathematics\nin Computer Science, 9, 03 2014.\n[BS19] Alexander Bagnall and Gordon Stewart. Certifying the true error: Machine learning in Coq with verified generalization guarantees.\nIn The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, pages 2662‚Äì2669. AAAI Press, 2019.\n[BTM+18] Brandon Bohrer, Yong Kiam Tan, Stefan Mitsch, Magnus O. Myreen, and Andr√© Platzer. VeriPhy: Verified controller executables\nfrom verified cyber-physical system models. In Dan Grossman, editor, Proceedings of the 39th ACM SIGPLAN Conference on\nProgramming Language Design and Implementation (PLDI 2018), pages 617‚Äì630. ACM, 2018.\n[EHN15] Manuel Eberl, Johannes H√∂lzl, and Tobias Nipkow. A verified compiler for probability density functions. In Jan Vitek, editor,\nESOP 2015, volume 9032 of LNCS, pages 80‚Äì104. Springer, 2015.\n[FHM18] Frank MV Feys, Helle Hvid Hansen, and Lawrence S Moss. Long-term values in Markov decision processes, (co)algebraically. In\nInternational Workshop on Coalgebraic Methods in Computer Science, pages 78‚Äì99. Springer, 2018.\nVajjha et al.\n[FP18] Nathan Fulton and Andr√© Platzer. Safe reinforcement learning via formal methods: Toward safe control through proof and\nlearning. In Sheila McIlraith and Kilian Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial\nIntelligence (AAAI 2018), pages 6485‚Äì6492. AAAI Press, 2018.\n[GHLL16] Shixiang Gu, Ethan Holly, Timothy P. Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation. CoRR,\nabs/1610.00633, 2016.\n[Gir82] Mich√®le Giry. A categorical approach to probability theory. In B. Banaschewski, editor, Categorical Aspects of Topology and\nAnalysis, pages 68‚Äì85, Berlin, Heidelberg, 1982. Springer Berlin Heidelberg.\n[HAK18] Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening.\nLogically-correct reinforcement learning.\nCoRR,\nabs/1801.08099, 2018.\n[HFM+20] Nathan Hunt, N. Fulton, Sara Magliacane, N. Ho√†ng, Subhro Das, and Armando Solar-Lezama. Verifiably safe exploration for\nend-to-end reinforcement learning. ArXiv, abs/2007.01223, 2020.\n[Hoe17a] Johannes Hoelzl. Markov chains and Markov decision processes in Isabelle/HOL. Journal of Automated Reasoning, 2017.\n[Hoe17b] Johannes Hoelzl. Markov processes in Isabelle/HOL. In Proceedings of the 6th ACM SIGPLAN Conference on Certified Programs\nand Proofs, CPP 2017, page 100‚Äì111, New York, NY, USA, 2017. Association for Computing Machinery.\n[How60] R.A. Howard. Dynamic Programming and Markov Processes. Technology Press of Massachusetts Institute of Technology, 1960.\n[Jac18] Bart Jacobs. From probability monads to commutative effectuses. Journal of Logical and Algebraic Methods in Programming,\n94:200 ‚Äì 237, 2018.\n[JP89] C. Jones and Gordon D. Plotkin. A probabilistic powerdomain of evaluations. In Proceedings of the Fourth Annual Symposium on\nLogic in Computer Science (LICS ‚Äô89), Pacific Grove, California, USA, June 5-8, 1989, pages 186‚Äì195. IEEE Computer Society, 1989.\n[Koz07] Dexter Kozen. Coinductive proof principles for stochastic processes. CoRR, abs/0711.0194, 2007.\n[KR09] Dexter Kozen and Nicholas Ruozzi. Applications of metric coinduction. Log. Methods Comput. Sci., 5(3), 2009.\n[Law62] F William Lawvere. The category of probabilistic mappings. preprint, 1962.\n[Ope18] OpenAI. OpenAI five. https://blog.openai.com/openai-five/, 2018.\n[Per18] Paolo Perrone. Categorical Probability and Stochastic Dominance in Metric Spaces. PhD thesis, University of Leipzig, 2018.\n[Per19] Paolo Perrone. Notes on category theory with examples from basic mathematics, 2019.\n[Put94] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley and Sons, Inc., USA, 1st\nedition, 1994.\n[RP02] Norman Ramsey and Avi Pfeffer. Stochastic lambda calculus and monads of probability distributions. In John Launchbury and\nJohn C. Mitchell, editors, Conference Record of POPL 2002: The 29th SIGPLAN-SIGACT Symposium on Principles of Programming\nLanguages, Portland, OR, USA, January 16-18, 2002, pages 154‚Äì165. ACM, 2002.\n[SB98] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.\n[SEJ+20] Andrew Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin ≈Ω√≠dek, Alexander\nNelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David Jones, David\nSilver, Koray Kavukcuoglu, and Demis Hassabis. Improved protein structure prediction using potentials from deep learning.\nNature, 577:1‚Äì5, 01 2020.\n[≈öGG15] Adam ≈öcibior, Zoubin Ghahramani, and Andrew D. Gordon. Practical probabilistic programming with monads. In Ben Lippmeier,\neditor, Proceedings of the 8th ACM SIGPLAN Symposium on Haskell, Haskell 2015, Vancouver, BC, Canada, September 3-4, 2015,\npages 165‚Äì176. ACM, 2015.\n[SHM+16] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis\nAntonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,\nTimothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with\ndeep neural networks and tree search. Nature, 529(7587):484‚Äì489, January 2016.\n[SLD17] Daniel Selsam, Percy Liang, and David L. Dill. Developing bug-free machine learning systems with formal mathematics. In Doina\nPrecup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,\nAustralia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 3047‚Äì3056. PMLR, 2017.\n[Tea04] The Coq Development Team. The Coq Proof Assistant Reference Manual. LogiCal Project, 2004. Version 8.0.\n[TH19] Joseph Tassarotti and Robert Harper. A separation logic for concurrent randomized programs. Proceedings of the ACM on\nProgramming Languages, 3(POPL):1‚Äì30, 2019.\n[TTV19] Joseph Tassarotti, Jean-Baptiste Tristan, and Koundinya Vajjha. A formal proof of PAC learnability for decision stumps. CoRR,\nabs/1911.00385, 2019.\n[TTV+20] Jean-Baptiste Tristan, Joseph Tassarotti, Koundinya Vajjha, Michael L. Wick, and Anindya Banerjee. Verification of ML systems\nvia reparameterization. CoRR, abs/2007.06776, 2020.\n",
  "categories": [
    "cs.AI",
    "cs.LO",
    "cs.PL",
    "D.2.4; I.2.8"
  ],
  "published": "2020-09-23",
  "updated": "2020-12-15"
}