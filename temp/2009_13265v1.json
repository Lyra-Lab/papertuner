{
  "id": "http://arxiv.org/abs/2009.13265v1",
  "title": "Deep Reinforcement Learning for Process Synthesis",
  "authors": [
    "Laurence Illing Midgley"
  ],
  "abstract": "This paper demonstrates the application of reinforcement learning (RL) to\nprocess synthesis by presenting Distillation Gym, a set of RL environments in\nwhich an RL agent is tasked with designing a distillation train, given a user\ndefined multi-component feed stream. Distillation Gym interfaces with a process\nsimulator (COCO and ChemSep) to simulate the environment. A demonstration of\ntwo distillation problem examples are discussed in this paper (a Benzene,\nToluene, P-xylene separation problem and a hydrocarbon separation problem), in\nwhich a deep RL agent is successfully able to learn within Distillation Gym to\nproduce reasonable designs. Finally, this paper proposes the creation of\nChemical Engineering Gym, an all-purpose reinforcement learning software\ntoolkit for chemical engineering process synthesis.",
  "text": "DEEP REINFORCEMENT LEARNING FOR PROCESS SYNTHESIS\nA PREPRINT\nLaurence I. Midgley\nDepartment of Engineering\nUniversity of Cambridge\nCambridge, United Kingdom\nand,\nDepartment of Chemical Engineering,\nUniversity of Cape Town,\nCape Town, South Africa\nlaurencemidgley@gmail.com\nSeptember 29, 2020\nABSTRACT\nThis paper demonstrates the application of reinforcement learning (RL) to process synthesis by\npresenting Distillation Gym, a set of RL environments in which an RL agent is tasked with designing\na distillation train, given a user deﬁned multi-component feed stream. Distillation Gym interfaces\nwith a process simulator (COCO and ChemSep) to simulate the environment. A demonstration of two\ndistillation problem examples are discussed in this paper (a Benzene, Toluene, P-xylene separation\nproblem and a hydrocarbon separation problem), in which a deep RL agent is successfully able to\nlearn within Distillation Gym to produce reasonable designs. Finally, this paper proposes the creation\nof Chemical Engineering Gym, an all-purpose reinforcement learning software toolkit for chemical\nengineering process synthesis.\nKeywords deep reinforcement learning · process synthesis · distillation train\n1\nIntroduction\nReinforcement learning (RL), is a type of machine learning in which an agent makes a sequence of decisions within an\nenvironment to maximize an expected reward. RL has had many recent successful applications, including mastering\ngames such as chess and Go [1]. Reinforcement learning has been recently applied to chemical engineering problems,\nnotably process control [2]. Computer aided process synthesis are currently dominated by optimisation techniques such\nas MINLP [3]. Preliminary work has been done showing the possibility of reinforcement learning for process synthesis\nusing a simple problems simulated using a hand-crafted simulator [4]. This paper builds on this work to present a clear\ndemonstration of RL for process synthesis.1\n2\nReinforcement learning background\n2.1\nReinforcement learning task deﬁnition\nReinforcement learning tasks are structured as a Markov Decision Process (MDP)(Figure 1). In an MDP, for each time\nstep of the environment, the agent (1) makes an observation (or partial observation) ωt, of the state, of the environment\nst, (2) selects an action αt from the set of possible actions A, based on the agent’s policy π, after which (3) the\nenvironment transitions to a new state st+1, and (4) the agent receives reward rt. The transition of the environment\n1Code available at https://github.com/lollcat/DistillationTrain-Gym\narXiv:2009.13265v1  [cs.LG]  23 Sep 2020\nA PREPRINT - SEPTEMBER 29, 2020\nmay be stochastic or deterministic. The environment starts in a given state and follows this sequence of steps until the\nterminal state is reached. The goal of the agent is to learn a policy that maximizes the total expected reward it receives\nduring the episode, given by maxπ E [P\nt rt].\nFigure 1: Markov Decision Process Diagram\n2.2\nSoft Actor Critic\nThis paper utilizes an adapted version of the Soft Actor Critic (SAC) agent [5, 6]. This section gives a short summary\nof SAC, while Section 3.3.1 describes the adaption added to SAC to ﬁt Distillation Gym. SAC utilizes an actor critic\narchitecture where the Q-function, which predicts the value of a state conditional on an action, is approximated by a\nneural network (the “critic”), while the “actor” is a neural network that produces a set of actions aimed at maximizing\nthe Q-value estimated by the “critic”. Instead of the typical value function, SAC uses a soft value function with an\nadditional entropy regularization term that encourages exploration. The soft Q-function is given by,\nQπ(s, a) = Est∼P,α′∼π[\nX\nt=0\nγtR (st, at, st+1) + α\nX\nt=1\nγtH (π (st)) | s0 = s, a0 = a]\n(1)\nwhere H, is the entropy of the policy at a given time step and α is a temperature parameter controlling the trade-off\nbetween maximising the reward and maximising the policy’s entropy. The relation of the soft Q-function the soft\nvalue-function is then given by,\nV (st) = Eat∼π [Q (st, at) −α log π (at | st)]\n(2)\nThe parameters θ, of the critic network, can be found by minimising,\nJQ(θ) = E(st,at)∼D\n\u00141\n2\n\u0000Qθ (st, at) −\n\u0000r (st, at) + γEst+1∼p [V¯θ (st+1)]\n\u0001\u00012\n\u0015\n(3)\nWhere D represents a replay buffer of experience. The term for the value-function is implicitly modelled through the\nrelation to the Q-value given in Equation 2. The parameters φ, of the actor network can be found by minimizing,\nJπ(φ) = Est∼D\n\u0002\nEat∼πφ [α log (πφ (at | st)) −Qθ (st, st)]\n\u0003\n(4)\nA “reparameterization trick” is then used where the policy reparametrized using,\nat = fφ (ϵt; st)\n(5)\nwhere, ϵt is noise sample from an independent distribution (e.g. spherical Gaussian). Which allows for the actor cost\nfunction to be given as,\nJπ(φ) = Est∼D,ϵt∼N [α log πφ (fφ (ϵt; st) | st) −Qθ (st, fφ (ϵt; st))]\n(6)\n2\nA PREPRINT - SEPTEMBER 29, 2020\nThe temperature parameter, α, can be set to a constant, or tuned throughout training for improved performance. In order\nto prevent over-estimation of the Q-function in the training of the actor, SAC uses two Q-networks, where the minimum\nQ-value of the networks is taken [7, 8]. Target Q-networks with soft-updates are also used to stabilize training [9]. For\na detailed description of SAC, see [5, 6].\n3\nDistillation Gym\n3.1\nOverall Description\nDistillation Gym is a reinforcement learning environment involving the synthesis of simple distillation trains to separate\ncompounds. In Distillation Gym, an RL agent is tasked with designing a common tree structured simple distillation\ntrain (Figure 2), which separates a user deﬁned multicomponent stream. The user is tasked with instantiating a speciﬁc\nproblem within distillation gym, deﬁning:\n1. The simulation system: component speciﬁcation, physical and chemical properties of the system\n2. Feed stream speciﬁcation: component ﬂowrates, conditions (temperature, pressure)\n3. Product deﬁnition: required purity, pure product selling stream prices\nGiven the user speciﬁed problem, the RL agent is tasked with designing a distillation train in order to maximize the\nexpected return. For a given step of the environment the reward is given,\nr(st, αt) = revenue −TAC\n(7)\nwhere TAC is the total annual cost of the additional column speciﬁed by action αt, and revenue is the annual revenue\ncorresponding to the sum of the value of any product produced in the distillate and bottoms of the additional column.\nThe distillation train is designed through sequential addition of new columns to the existing process, where the agent\nspeciﬁes each column’s pressure (controlled by a valve before the column), number of stages, reﬂux ratio and reboil\nratio. The agent operates only through the addition of new distillation columns to stream with unconnected endpoints,\nwhich results in the simple distillation train’s resulting simple tree structure. There are no other permitted changes to\nthe process design (e.g. adding recycle streams or editing existing units). All unit simulation is performed using COCO\nand ChemSep [10, 11]. The goal of Distillation Gym is to create a simple illustration of RL’s application to process\nsynthesis. Interfacing with a process simulator, is a key component of this demonstration, as more general process\nsynthesis RL tasks would most likely also follow this form.\nFigure 2: Distillation Gym’s simple tree-structured column design\n3.2\nMarkov Decision Process Framing\nCurrent most computational process synthesis techniques typically frame process synthesis as an optimisation problem\nwhere an objective function governed by a strict set of parameters and equations is maximised. Deﬁning process\nsynthesis as a reinforcement learning tasks, instead frames process synthesis as a sequential decision-making task\n3\nA PREPRINT - SEPTEMBER 29, 2020\n(MDP). To deﬁne the MDP for process synthesis, the state, action, state-transition and reward structure need to be\ndeﬁned. The state should represent all the necessary information describing the current process conﬁguration necessary\nfor future design decisions. Typically for process synthesis tasks, the state should therefore most likely include\ninformation on all of the streams (resembling the stream table), as well as relevant additional information (e.g. on the\nunit operations). To feed the state into a neural network, the array that describes the state needs to be of a ﬁxed shape.\nFor process synthesis, this constraint can cause some difﬁculty as the number of streams changes as the process is\ndesigned. One solution to this is to start with a large empty state (resembling a stream table populated with 0’s), which\ncan be ﬁlled as the process is designed. The action should represent changes to the design of the process conﬁguration\n(e.g. adding a new unit operation). The state transition is the running of the process simulation to calculate the updated\ninformation describing the process at its current point of design (e.g. calculating the ﬂowrates in all of the streams). The\nreward describes the degree to which the objective of the design task (e.g. maximizing proﬁt) is achieved. One intuitive\nway to think of process synthesis as an MDP is to imagine a human designing a process within a process simulator;\nwhat actions do they take? What information is relevant to their decisions? What is their goal?\nIn the Distillation Gym, because of the speciﬁc structure of the distillation problem, an adjusted form of the MDP\ncan be created which allows for a signiﬁcant simpliﬁcation to the state. Speciﬁcally, instead of the state describing\nthe process as a whole, through the change to the structure of the MDP, the state can instead describe a single stream.\nAlgorithm 1 below gives a description of the MDP for Distillation Gym.\nAlgorithm 1: Distillation Gym Environment Episode Structure\ninitialization\n// configure flowsheet\nD = deque()\n// deque of the process streams with unconnected ends\nD.append(FeedStream)\ndone = False\nwhile not done do\nCurrentStream = D[0]\nobserve CurrentStream.state\nchoose whether to separate CurrentStream\nif chose to seperate then\nspecify column\n// inlet pressure, number of stages, reboil ratio, reflux ratio\nsimulate ﬂowsheet\n// using interface with COCO simulator\nreward = - TAC\nfor Stream ∈(DistillateStream, BottomsStream) do\nif Stream.purity >= PuritySpeciﬁcation then\nreward += Stream.revenue\nelse\n// if stream is doesn’t meet product specification then it may be further\nseparated\nD.append(Stream)\nend\nend\n// Can now remove current stream from D as it is now attached to a column\nD.popleft()\nelse\n// if decide not to separate then CurrentStream becomes outlet to process so is\nremoved from D\nD.popleft()\nend\nif D is empty or max steps have been reached then\ndone = True\n// episode is over\nend\nend\nThe reason this structure is possible is because actions only effect streams downstream in the process. This means that\nit is only necessary for the current stream being separated to be fed to the agent as the state, as the other process streams\nare completely independent to this decision. As shown in Algorithm 1, instead of standard linear MDP structure (state\n→next-state), a tree shaped decision process (referred to as tree-MDP) is used, where each state produces two next\nstates (corresponding to the tops and bottoms of the distillation column). Instead of the next state coming immediately\nfollowing the state, D, the deque of next states (unconnected streams) is sequentially stepped through. The episode is\n4\nA PREPRINT - SEPTEMBER 29, 2020\nover when either (1) the maximum number of steps are reached or (2) all of the leaf node states are terminal, which\nis equivalent to D being empty. This tree structured decision process can also be interpreted as a partially observable\nMDP where the agent observes the current stream that it is separating, and both the outlet streams that the separation\nproduces instead of observing the whole system. Total episode reward can still be calculated by summing the rewards\nover all time steps. However, the value of a state is now given by the immediate reward, and the value of all of the\nfuture rewards of states downstream in the tree decision process structure - as shown by the recursively deﬁned value\nfunction below.\nV (s)π = Es′∼P,a∼π\nh\nr + γV\n\u0000s′\ntops\n\u0001\nπ + γV (s′\nbottoms)π\ni\n(8)\n3.3\nDistillation train synthesis examples\n3.3.1\nOverview\nTwo example problems are given below, both largely based (in terms of feed composition and property package\ndeﬁnitions) on examples from the ChemSep example page [11] and are common distillation problems. In both problems\na Soft Actor Critic agent was adapted to include the tree-MDP, using the adapted soft Q-function, as shown in the\nrecursively deﬁned Q-function below.\nQ(s, a)π = Es′∼P,a,∼π\n\u0002\nr + γ\n\u0000Q\n\u0000s′\ntops, a′\ntops\n\u0001\n−αH\n\u0000π\n\u0000s′\ntops\n\u0001\u0001\n+ γ (Q (s′\nbottoms, a′\nbottoms) −αH (π (s′\nbottoms))] (9)\nThe agents action of whether or not to separate a speciﬁc stream is determined by whether the soft Q-value was positive\n(choose separate) or negative (choose not to separate). A consequence of this is that the Q-value of the next state (used\nin Equation 9), has a lower bound of zero - because in situations where the Q-value was negative, the agent would\ndecide not to separate the stream, causing the state to be a terminal leaf. Because the remainder of the actions (column\nspeciﬁcation) are continuous, this addition allowed for a simple extension to add the binary decision of deciding whether\nor not to separate a given stream.\nIn both examples the agent was able to learn within the environment, to produce better and better distillation train\nconﬁgurations. In Figure 3 and Figure 5, this is shown by both the improvement in average return, and the improvement\nof the “best designs” (shown by the peaks in the return). In both examples, the agent is able to achieve progressively\nhigher sum total revenue (through obtaining higher product recovery) and lower sum total TAC. For each example,\nautogenerated block-ﬂow-diagrams (BFD) of the best design throughout training are given, as well as the key outcome\nmetrics. Overall, these problems successfully demonstrate that the framing of process synthesis as a reinforcement\nlearning task.\n3.3.2\nProblem 1: BTX separation\nUser speciﬁcation of problem\n• Simulation System: Benzene, Toluene and p-Xylene. Flowsheet settings copied from COCO ﬁle on ChemSep\nexample page [11]\n• Starting stream deﬁnition: Equimolar (3.35 mol/s of each compound) feed at 25°C, 1 atm\n• Product deﬁnition: Required purity of 95%, price ($/tonne): 488, 488, 510 [12]\n5\nA PREPRINT - SEPTEMBER 29, 2020\nResults\nFigure 3: Agent training on BTX problem\nTable 1: Performance metrics for best design outcome on BTX problem\nTotal Revenue\n$ 13.17 million\nTotal Column TAC\n$ 0.45 million\nTable 2: Recoveries for best design outcome on BTX problem\nCompound\nRecovery\nBenzene\n98.2%\nToluene\n99.7%\np-Xylene\n> 99.9%\n6\nA PREPRINT - SEPTEMBER 29, 2020\nFigure 4: Best Design for BTX problem (autogenerated BFD)\nFor simplicity, the autogenerated BFD has a single block for the valve-distillation column pair\n3.3.3\nProblem 2: Hydrocarbon separation\nUser speciﬁcation of problem\n• Simulation System: Ethane, Propane, Isobutane, N-butane, Isopentane, N-pentane. Flowsheet settings copied\nfrom COCO ﬁle on ChemSep example page [11].\n• Starting stream deﬁnition : Compound ﬂows (mol/s): 17, 1110, 1198, 516, 334, 173. Conditions: 105°C,\n17.4 atm.\n• Product deﬁnition: Required purity of 95%, price ($/tonne): 125, 204, 272, 249, 545, 545 [13]\n7\nA PREPRINT - SEPTEMBER 29, 2020\nResults\nFigure 5: Agent training on Hydrocarbon problem\nTable 3: Performance metrics for best design outcome on Hydrocarbon problem\nTotal Revenue\n$ 1588 million\nTotal Column TAC\n$ 119 million\nTable 4: Recoveries for best design outcome on Hydrocarbon problem\nCompound\nRecovery\nEthane\n0%\nPropane\n98.9%\nIsobutane\n97.3%\nN-butane\n91.1%\nIsopentane\n99.6%\nN-pentane\n97.0%\n8\nA PREPRINT - SEPTEMBER 29, 2020\nFigure 6: Best Design for Hydrocarbon problem\n9\nA PREPRINT - SEPTEMBER 29, 2020\n4\nFurther developments & Concluding Remarks\nDistillation Gym and the corresponding examples have purposely been designed to be simple process synthesis tasks,\nas the purpose of Distillation Gym is primarily as a demonstration of the possibility of RL for process synthesis. The\nkey functions that Distillation Gym demonstrates are (1) structuring process synthesis as a RL task (2) application of\na RL agent, (3) interface with a process simulator (COCO simulator), (4) generality through user speciﬁed problem\ndeﬁnition, (5) reasonable task complexity (branching process conﬁguration, multivariate unit speciﬁcation).\nThe next step in the continuation of this project; Chemical Engineering Gym, an all-purpose software toolkit for\ngenerating process synthesis problems, structured as reinforcement learning tasks. Extending the RL for process\nsynthesis approach to tasks is where RL may be advantageous relative to other conventional computational approaches,\ndue to the ability of RL to solve more open-ended problems. More fundamentally, this paper proposes that the framing\nlevel of “process synthesis as a sequence of decisions” is more apt for general process synthesis problems than the\nframing as “optimizing a set of process parameters/equations”, which conventional optimization techniques follow.\nThe key improvement that Chemical Engineering Gym would add, would be to allow problem speciﬁcations with a far\nlarger action space. The problem speciﬁcation should be able to include all of the relevant actions that a human process\ndesigner takes within a simulator’s GUI when designing a process. This would include allowing the agent to add a\ngreater variety of unit operations to a process (e.g. reactors, heat exchangers), allowing the agent to add streams to the\nprocess (e.g. selecting when to “buy” raw materials, speciﬁed by a price dataset), allowing the agent greater ability to\nmanipulate ﬂowsheet topology (e.g. adding recycle streams), allowing the agent to select which streams exit the process\n(aided with a dataset of product prices/waste removal prices), editing existing unit parameters etc. In the extreme, with\na large database of chemical prices, a reinforcement learning agent could be simply tasked with searching for novel\nproﬁtable process designs, starting from a blank ﬂowsheet, within the bounds of what can be accurately simulated.\nChemical Engineering Gym would require a commercial process simulator(s) to interface with, in order to run the\nprocess simulation. Currently there is a large room for process simulators to improve interface with external programs\n(e.g. python). Process simulators do have some interface functionality with external programs, speciﬁcally they provide\nthe ability to edit existing unit parameters, run the process simulation and retrieve results. However, much of the\nfunctionality in process simulator GUI is left out, most notably changing the ﬂowsheet topology (e.g. through adding\nnew units), which is a key component of process synthesis. Adding improvements to the interface between process\nsimulators and eternal programs would most likely have beneﬁts to the ﬁeld of computational process synthesis in\ngeneral.\nProgress in within the ﬁeld of reinforcement learning has been signiﬁcantly aided by freely available general purpose\nRL toolkits/frameworks, like OpenAI Gym [14]. Similarly, creating a toolkit for RL within a process synthesis context\ncould greatly aid research within the ﬁeld. Distillation Gym presents the key ﬁst step towards such a toolkit.\n10\nA PREPRINT - SEPTEMBER 29, 2020\nReferences\n[1] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot,\nLaurent Sifre, Dharshan Kumaran, and Thore Graepel. A general reinforcement learning algorithm that masters\nchess, shogi, and Go through self-play. Science, 362(6419):1140–1144, 2018. Publisher: American Association\nfor the Advancement of Science.\n[2] Rui Nian, Jinfeng Liu, and Biao Huang. A review On reinforcement learning: Introduction and applications in\nindustrial process control. Computers & Chemical Engineering, 139:106886, August 2020.\n[3] Qi Chen and I.E. Grossmann.\nRecent Developments and Challenges in Optimization-Based Process\nSynthesis.\nAnnual Review of Chemical and Biomolecular Engineering, 8(1):249–283, 2017.\n_eprint:\nhttps://doi.org/10.1146/annurev-chembioeng-080615-033546.\n[4] Laurence Midgley and Michael Thomson. Reinforcement learning for chemical engineering process synthesis.\nTechnical report, Zenodo, November 2019. https://zenodo.org/record/3556549#.X2mx1mhKhPY.\n[5] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy Maximum\nEntropy Deep Reinforcement Learning with a Stochastic Actor. arXiv:1801.01290 [cs, stat], August 2018. arXiv:\n1801.01290.\n[6] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,\nHenry Zhu, Abhishek Gupta, and Pieter Abbeel. Soft actor-critic algorithms and applications. arXiv preprint\narXiv:1812.05905, 2018.\n[7] Hado V. Hasselt. Double Q-learning. In Advances in neural information processing systems, pages 2613–2621,\n2010.\n[8] Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic\nmethods. arXiv preprint arXiv:1802.09477, 2018.\n[9] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, and Georg Ostrovski. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529, 2015.\n[10] Jasper van Baten and Richard Baur.\nCOCO - the CAPE-OPEN to CAPE-OPEN simulator, 2020.\nhttps://www.cocosimulator.org/.\n[11] Harry Kooijman and Ross Taylor. Chemsep, 2020. http://www.chemsep.org/index.html.\n[12] Echemi: Provide Chemical Products and Services to Global Buyers, 2020. https://www.echemi.com.\n[13] EIA. Prices for hydrocarbon gas liquids - U.S. Energy Information Administration (EIA), 2018.\n[14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\nZaremba. OpenAI Gym, 2016.\n11\n",
  "categories": [
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2020-09-23",
  "updated": "2020-09-23"
}