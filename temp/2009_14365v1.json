{
  "id": "http://arxiv.org/abs/2009.14365v1",
  "title": "Toolpath design for additive manufacturing using deep reinforcement learning",
  "authors": [
    "Mojtaba Mozaffar",
    "Ablodghani Ebrahimi",
    "Jian Cao"
  ],
  "abstract": "Toolpath optimization of metal-based additive manufacturing processes is\ncurrently hampered by the high-dimensionality of its design space. In this\nwork, a reinforcement learning platform is proposed that dynamically learns\ntoolpath strategies to build an arbitrary part. To this end, three prominent\nmodel-free reinforcement learning formulations are investigated to design\nadditive manufacturing toolpaths and demonstrated for two cases of dense and\nsparse reward structures. The results indicate that this learning-based\ntoolpath design approach achieves high scores, especially when a dense reward\nstructure is present.",
  "text": "TOOLPATH DESIGN FOR ADDITIVE MANUFACTURING USING\nDEEP REINFORCEMENT LEARNING\nA PREPRINT\nMojtaba Mozaffar\nDepartment of Mechanical Engineering\nNorthwestern University\nEvanston, IL 60208, USA\nmozaffar@u.northwestern.edu\nAblodghani Ebrahimi\nDepartment of Industrial Engineering and Management Sciences\nNorthwestern University\nEvanston, IL 60208, USA\nghani@u.northwestern.edu\nJian Cao∗\nDepartment of Mechanical Engineering\nNorthwestern University\nEvanston, IL 60208, USA\njcao@northwestern.edu\nOctober 1, 2020\nABSTRACT\nToolpath optimization of metal-based additive manufacturing processes is currently hampered by the\nhigh-dimensionality of its design space. In this work, a reinforcement learning platform is proposed\nthat dynamically learns toolpath strategies to build an arbitrary part. To this end, three prominent\nmodel-free reinforcement learning formulations are investigated to design additive manufacturing\ntoolpaths and demonstrated for two cases of dense and sparse reward structures. The results indicate\nthat this learning-based toolpath design approach achieves high scores, especially when a dense\nreward structure is present.\nKeywords Additive Manufacturing · Toolpath · Reinforcement Learning · Deep Learning\n1\nIntroduction\nAdditive Manufacturing (AM) processes offer unique capabilities to build low-volume parts with complex geometries\nand fast prototyping from a variety of materials. Metal-based AM has become increasingly more popular over the\nlast decade for manufacturing and repairing functional parts in automotive, medical and aerospace industries. Despite\nthe great potential in metal-based AM market, the state-of-the-art practices involve rigorous trial and errors before\nachieving consistent parts with the desired geometric and material properties, which is mainly due to the sensitivity of\nthe build on process parameters. While the inﬂuence of process parameters such as laser power, powder parameters,\nand scan speed on the microstructure and ﬁnal properties of the AM build are extensively studied in the literature, the\ninﬂuence of toolpath strategies yet to be fully investigated.\nAuthors in [Steuben et al., 2016] considered three different toolpath patterns for building a part using a fused deposition\nmodeling process and demonstrated that the pattern has a signiﬁcant effect on the ultimate strength and elastic modulus\nof the build. Akram et al. [Akram et al., 2018] formulated a microstructure model using a Cellular Automata (CA) and\ndemonstrated a strong correlation between the toolpath pattern (i.e., unidirectional and bi-directional) and the grain\norientations. In [Bhardwaj and Shukla, 2018], the authors considered bi-directional and cross-directional toolpath\nstrategies to manufacture cubic parts with a Direct Metal Laser Sintering (DMLS) process and studied the surface ﬁnish,\n∗Corresponding author\narXiv:2009.14365v1  [cs.AI]  30 Sep 2020\nToolpath design for additive manufacturing using deep reinforcement learning\nresidual stress and mechanical properties of the parts. Their study indicates that the parts built with cross-directional\nstrategy display better mechanical properties, which is due to their desirable microstructure of columnar cells.\nFrom the above-mentioned research, it can be evidently seen that the choice of the toolpath greatly inﬂuences various\nproperties of AM builds. However, existing research does not offer a robust solution for the analysis of this inﬂuence\nnor tools to prudently design toolpaths. In this work, we present a novel way to represent the toolpath and learn design\nstrategies that lead to optimal performance. The toolpath design is proposed to be modeled as a Reinforcement Learning\n(RL) problem in which an agent learns to design optimal toolpaths as it dynamically interacts and collects data from an\nadditive manufacturing environment.\nRL is a subﬁeld of artiﬁcial intelligence, which focuses on training agents that can interact with an environment and\nmaximize the rewards that the agent collects through this interaction. In an RL schema, the agent is responsible for\ndetermining the actions at each time step (at), which inﬂuences the environment causing it to move from its current state\n(st) to a new state (st+1) and generates a reward feedback (rt) for the agent. Here, “state” refers to the representation of\nthe environment that is visible to the agent. The agent learns to maximize the long-term rewards that it receives in its\nlifespan by attempting more of the strategies that lead to the most favorable rewards.\nMost modern RL algorithms can be categorized into three main classes:\n1. Policy optimization methods parameterize the policy, πθ(a|s), and optimize θ to maximize the expected\nreward. This class of RL algorithms often suffer from poor sample efﬁciency, requiring millions of samples.\nFurthermore, for most policy optimization algorithms, all samples should be generated using the agent’s policy\nat each training step, which exacerbates the sample efﬁciency of these methods as historic data cannot be used\nin the training process.\n2. Value function optimization methods (also called Q-learning) do not optimize the policy directly. Rather,\nthey aim to ﬁnd the optimal action-value function Q∗(s, a), as deﬁned in Eq. 1, to represent the maximum\ndiscounted reward the agent can collect from any state. Following the actions that lead to maximum optimal\naction-values, a∗= argmaxaQ∗(s, a), guides the agent to maximize its reward.\nQ∗(s, a) = maxπE\n\" H\nX\nt=0\nγtrt|π, s0 = s, a0 = a\n#\n(1)\nIn Eq. 1, π represents the policy, H represents the environment horizon and γ is the discount factor—a positive\nvalue smaller than 1 . The action-value function can be obtained using the Bellman equation (Eq. 2) with\nguaranteed convergence in tabular cases. By exploiting the self-consistency of the problem structure through\nthe Bellman equation, action-value function optimization methods can learn the optimal action-value function\nand implicitly determine the policy with fewer samples. However, Q-learning lacks the stability of policy\noptimization methods.\nQ∗(st, at) = r(st, at) + γmaxat+1Q∗(st+1, at+1)\n(2)\n3. Model-based RL algorithms, unlike the ﬁrst two categories (known as model-free RL), attempt to learn an\nexplicit model of the underlying dynamics of the environment. The model is further used for look-ahead\nplanning or as a virtual sample generator. This class of solutions can offer a great sample efﬁciency with orders\nof magnitude less required data. However, the quality of the RL agent heavily depends on the accuracy of the\ndynamics model. Therefore, while there are many successful examples of this approach for robotics and games\nwith perfect environments such as chess, the state-of-the-art algorithms in this class fail in high-dimensional\nspaces (e.g., pixel-level visual inputs) and uncertain environments.\nNote that these categories are not mutually exclusive. In fact, most of the successful existing literature uses a combination\nof these approaches. Most famously, actor-critic methods simultaneously parametrize and train both policy and value\nfunctions. For example, A2C [Mnih et al., 2016] follows the policy gradient theorem while using the value function to\nreduce the variance of gradient estimation and provides a stable solution for continuous action spaces.\nIn this work, we investigate a number of leading model-free candidates that showed promising results in domains such\nas Atari games. Investigation of model-based approaches is not considered here because of their often-suboptimal\nperformance in high-dimensional domains such as ours. Our toolpath design system allows exploring an unknown\ndynamic physics through experiments and opens new avenues for high dimensional design in manufacturing processes.\n2\nToolpath design for additive manufacturing using deep reinforcement learning\nFigure 1: Sample CAD geometries (top row) and pixelized two-dimensional sections (bottom row) for the AM virtual\nenvironment.\n2\nMethods\n2.1\nAM virtual environment\nWe develop a virtual environment of an AM process, resembling Directed Energy Deposition (DED) processes, to\ncollect data and perform the training process. The virtual environment considers two-dimensional sections on which\nmaterials need to be deposited. As we want the strategies learned by the RL agent to be geometry-agnostic, we develop\na database of CAD geometries representing a wide range of spatial structures. The CAD geometries are then processed\ninto multiple sections by slicing them in different heights and converted into over 400 two-dimensional sections, each\nwith 32x32 pixels, to train the agent. A sample of considered CAD geometries acquired from Thingiverse online\nrepository [https://www.thingiverse.com] and one of their corresponding sections are demonstrated in Fig. 1.\nWhile evaluating the virtual environment, one section is randomly selected and the RL agent is asked to design the\ntoolpath for it one action at a time. Eight actions are available for the agent to explore, including four directions of\nmotion each with two deposition status (on/off). The environment keeps track of the desired section, ﬁlled section, and\nthe location and status of the nozzle. A representation of the environment state space (st) is accessible to the agent\nat each time step. Once the agent ﬁnishes its assigned task for a section (e.g., depositing material on all pixels of the\ndesired section), a new section is randomly selected, and the agent is asked to start over. To avoid excessively long\nepisodes of training on one section, a maximum of 400 actions are selected for each section.\nAs can be seen from Fig. 2, we assume a pixelized section representation and discrete action spaces. These two\nassumptions are not inherently restrictive for the proposed methodology as the representation of the section can be\nreplaced with other continuous or discrete heuristics and the action space can be easily extended to higher number of\noption (e.g., 8 or 16 directions) or continuous action spaces with minimal change to the algorithm.\nWe design the state representation (st) as a single-channel two-dimensional image, where the unﬁlled section has a\nvalue of 1.0 and the rest of the pixels are zero. Additionally, we provide the network with a one-hot encoded list of 10\nmost recent action histories. The image is ﬁrst processed through three layers of convolutional neural network, then\nconcatenated with the action history input, followed by two fully connected neural network layers for policy and value\nnetworks in each algorithm.\n2.2\nAnalysis cases\nWe consider two scenarios for the tasks and their corresponding reward systems in this study:\n1. Dense reward system: In this analysis, we consider a scenario in which a reward can be assigned based on\nthe interaction of agents and environments at each time step. Designing a toolpath that deposits material in all\ndesirable locations of the section in optimal time is an example of a dense reward system. In this case, we\nassign a reward of 1.0 to any desirable material deposition, -1.0 to material deposition in incorrect locations,\n3\nToolpath design for additive manufacturing using deep reinforcement learning\nFigure 2: AM virtual environment including section (in blue), ﬁlled partition (in green), and nozzle location and status.\nThe red point indicates the location of the nozzle with “on” status. Valid actions are shown with eight arrows for “on”\n(red) and “off” (brown) status and four directions. The hidden pattern for sparse reward system is plotted in yellow.\nand -0.5 to motions without deposition to provide an incentive to ﬁnish the toolpath in the shortest time. It\nis noteworthy that dense reward structures are not limited to static properties of the environment, such as\nﬁnishing the toolpath. Other examples of dense reward structures include rewards that are assigned based on\nthe meltpool size or shape from an online thermal imaging system. content...\n2. Sparse reward system: As many interesting AM process parameters can only be measured and evaluated after\nthe part is made, a reward can only be assigned to the completed toolpath at the last time step of the episode,\nwhich results in a sparse reward system. To simulate this scenario in the developed virtual environment, we\nconsider the sequence of ordered actions (up, up, right, down, and down in this order, see Fig. 2) as a potential\ndesirable pattern and assign a reward at the end of each episode of the simulation based on the similarity of the\ntoolpath generated by the agent with the selected pattern. Note that this pattern is completely hidden from\nthe agent, i.e., the agent can only interpret the pattern through the sparse reward it receives at the end of each\nepisode. The similarity between the toolpath and the hidden pattern is measured by counting the occurrence of\nthe completed or partially completed (with a minimum of three consecutive actions) hidden patterns in the\ntoolpath history. To encourage the agent to ﬁnish the toolpath while learning the hidden pattern, an auxiliary\nreward of 0.1 and -0.1 is assigned for correct and incorrect material deposition respectively. While this speciﬁc\nsequence is selected as a demonstration in this work, the formulation does not depend on it, and the reward\nstructure can be based on any unknown physics of the environment.\n2.3\nRL algorithms\nIn this work, we mainly focus on investigating and improving three state-of-the-art RL algorithms that have shown to\nbe successful across many tasks.\n2.3.1\nDeep Q-network (DQN) [Mnih et al., 2015], is a Q-learning approach that parametrizes action-value function,\nQ(s,a), using neural network and iteratively solves the Bellman equation (Eq. 2) while using a number of numerical\ntechniques to overcome problems associated with training neural networks in RL non-stationary setting. As neural\nnetworks generalize, the Bellman equation (Eq. 2) tails a dynamic target (i.e., both Q∗(st, at) and Q∗(st+1, at+1)\nchange while training), which impedes the training process. DQN uses an additional neural network as a target network\nto estimate the action-value for future states Q∗(st+1, at+1) and solve the Bellman equation in a more supervised\nfashion. While the neural network training theories stand on the assumption of independent and identically distributed\n(i.i.d.) data, the successive data collected in RL settings are greatly correlated. To overcome this issue, DQN uses a\nreplay buffer that stores all transactions of the environment and randomly draw samples from them during the training\nprocess. DQN uses epsilon-greedy strategy for exploration, in which the agent initially explores while gradually over\nthe training process learns to acts more according to its predicted model.\n4\nToolpath design for additive manufacturing using deep reinforcement learning\nIn this work, we consider a variation of the original DQN paper that empirically showed enhanced performance for\nthis application. A corrected replay buffer, as proposed in [Zhang and Sutton, 2017], is used where the last added\nsample into replay buffer will be added to the randomly selected batch to eliminate the need for excessively large replay\nbuffer. To reduce the overestimation bias of Q value caused by the maximizing operation in Eq. 2, action selection and\naction-value estimation are performed using two separate neural networks, as proposed by [Van Hasselt et al., 2015].\nThe gradient of neural network is clipped at each training step to a value of 0.5 to avoid harmful oscillations on neural\nnetwork parameters. Finally, the hard copy operation in the original DQN paper is replaced by a moving average copy\nto smoothen the training process. It is noteworthy that a number of other DQN improvements in the literature are\ninvestigated but since they provided small to no improvement on the results they are not reported here.\n2.3.2\nProximal policy optimization (PPO) [Schulman et al., 2017], is a widely successful actor-critic method that\nbuilds on top of the policy gradient formulation to update its stochastic policy (πθ):\nLπ\nθ = E\nh\n(πθ(at|st))/(πθold(at|st)) ˆAt\ni\n(3)\nwhere ˆAt is the advantage function and represents the difference between the value function of the selected action,\nQ(st, at), and the average value function for that state over actions. Intuitively, maximizing Eq. 3 encourages the policy\nto increase the probability of action if the selected action performed better than average (i.e., the advantage is positive)\nand decrease the probability of relatively worse actions. However, this vanilla formulation tends to collapse the training\nprocess as taking large steps can easily move the policy into unrecoverable bad parameter spaces. To solve this issue,\nPPO restricts the ratio between current policy and previous policy by pessimistically clipping its value according to Eq.\n4:\nLπ,clip\nθ\n= E\nh\nmin(rt(θ) ˆAt, clip(rt(θ), 1 −epsilon, 1 + ϵ) ˆAt)\ni\nrt = (πθ(at|st))/(πθold(at|st))\n(4)\nwhere ϵ determines the clipping range. Furthermore, PPO loss (Eq. 5) has two additional terms in to train the advantage\nvalue ˆAt and to maximize the entropy (LE\nθ nt) of the policy to encourage exploration.\nLP P O\nθ\n= E\nh\nLπ,clip\nθ\n+ c1LA\nθ + c1LEnt\nθ\ni\n(5)\nPPO can only be trained using samples generated from its current policy (i.e., on-policy algorithm). This characteristic\nof PPO causes this algorithm to require a larger number of samples compared to off-policy algorithms where historic\ndata can be reused for training the agent through the use of replay buffer. Empirically, the effectiveness of the PPO\nalgorithm relies on collecting independent samples from multiple streams of environments often performed in parallel\nvirtual environments.\n2.3.3\nSoft actor critic (SAC) [Haarnoja et al., 2018] is an off-policy actor-critic method that aims to maximize an\nalternate action-value function, called soft action-value, that considers not only the accumulative reward but also the\nentropy of its stochastic policy. Theoretically, soft action-value formulation encourages the agent to explore states with\nuncertain results. The soft action-value loss is presented in Eq. 6:\n(6)\nLQ\nθ = Est,at,st+1,rt,dt∼D\n\u0014\u0010\nmini=1,2Qθi(st, at) −rt\n+ γ(1 −dt)(mini=1,2Qt\nθi(st+1, at+1) −αlog(πφ(at+1|st+1)))\n\u00112\u0015\nwhere θ, θ and φ indicate the neural network parameters for the online action-value function, the target action-value\nfunction, and the policy respectively. The temperature parameter α determines the importance of entropy. SAC\ncompensates for the overestimation of action-value functions by training two independent neural networks and taking\nthe minimum of the two for loss calculations.\nSAC policy loss is deﬁned as the KL-divergence between the current policy and action-value softmax. To calculate the\ngradients of parameters through the stochastic node of policy sampling, the reparameterization trick is used. While the\n5\nToolpath design for additive manufacturing using deep reinforcement learning\nFigure 3: Learning curve of toolpath design system with the three DQN, SAC, and PPO algorithms for (a) dense and (b)\nsparse reward system. The horizontal axes for PPO results are plotted in a different scale (shown on top of each plot)\nfrom DQN and SAC results (shown on the bottom of each plot). As manual zig-zag toolpath strategy is plotted as a\nbaseline for the dense reward system, however, such engineered solution does not apply for the sparse reward system.\nSAC only applies to environments with continuous action spaces, we developed a modiﬁed version of this algorithm\nthat uses Gumble-softmax [Jang et al., 2016] to perform the reparameterization for categorical action spaces (Eq. 7).\nLπ\nφ = −Est∼D,ξ∼G [mini=1,2Qθi(st, ˜aφ(st, ξ)) −αlog(πφ(˜aφ(st, ξ)|st))]\n(7)\nwhere ξ is an independent noise sampled from a Gumble-softmax distribution and a ˜aφ is the reparametrized action.\nWhile the temperature parameter α can be potentially kept as constant, the SAC authors devise a formulation to\nsimultaneously train this parameter in order to constrain it to a minimum target entropy H (Eq. 8).\nLα = Est∼D,at∼π [−αlog(πφ(at|st) −αH)]\n(8)\nAlthough the above-mentioned three algorithms have inherent differences, we have tried to keep hyperparameters of the\nalgorithms as consistent as possible. The hyperparameters of all algorithms are tuned to maximize the achieved score.\n3\nResults and discussion\nWe implemented the three discussed model-free algorithms with the proposed modiﬁcations using Tensorﬂow deep\nlearning library. Each algorithm is trained to design the toolpath in two cases of dense and sparse reward structures as\ndetailed in Sec. 2.2. The learning curve of each algorithm is demonstrated in Fig. 3a and b for dense and sparse reward\nstructure, respectively. The reported score averages resulted scores for all training geometries from random initial\nnozzle location. Since the on-policy nature of the PPO algorithm requires far more episodes of toolpath generation than\nthe two off-policy algorithms, the PPO results are plotted on a different horizontal scale for the number of episodes.\nAs can be seen from Fig. 3a while three algorithms gradually learn to improve their toolpath designs, SAC achieves a\nnotably inferior performance. DQN and PPO reach a close performance easily surpassing a manually coded zig-zag\ntoolpath. While the ﬁnal performance of the PPO algorithm is 3 scores higher than DQN, DQN reaches a stable solution\n6\nToolpath design for additive manufacturing using deep reinforcement learning\nTable 1: Highest score of model-free algorithms for two reward structure cases.\nAlgorithm\nDense reward\nSparse reward\nDQN\n59.31\n28.83\nPPO\n62.98\n40.54\nSAC\n38.71\n5.11\nFigure 4: Three samples of the designed toolpaths by the trained PPO algorithm for random sections and starting\nlocations. The section is depicted in light grey. The toolpath motion starts from the blue diamond shape, following a\ncolor gradient ending in a pink arrow shape.\nusing 10 times less samples. For the sparse case (see Fig. 3b), the score achieved by PPO algorithm surpasses the\ntwo other algorithms, and similar to the previous case, SAC results in the worst performance. The highest score of\nthe algorithms for the two cases is reported in Table 1 and three samples of the designed toolpaths with trained PPO\nalgorithm is demonstrated in Fig. 4.\nOur results show that model-free reinforcement learning is a feasible approach for high-dimensional manufacturing\ndesign systems, such as toolpath design tools, especially if a dense reward system exists or it is feasible to engineer\nsuch a feature by breaking the task into meaningful step-by-step reward increments. DQN-based algorithms show great\npotential in this realm as they offer decent accuracy and sample efﬁciency. Although SAC algorithm is reported to\nproduce state-of-the-art benchmarks in many robotics tasks, it is incapable of handling the intricacies of toolpath design.\nIn the case of a sparse reward structure, investigated model-free approaches struggle to optimize the toolpath. PPO\nprogressively learns better solutions; however, its excessive on-policy sample requirement makes this algorithm only\napplicable to cases where a robust simulation of the physics exists. Novel combinations of model-free and model-based\nsolutions are necessary to solve this class of problem on experimental data.\n4\nConclusions\nIn conclusion, we proposed a new framework for the toolpath design of metal-based additive manufacturing processes\nby formulating a reinforcement learning problem. Modiﬁed versions of three state-of-the-art model-free RL algorithms\nare used to develop toolpaths on a virtual additive manufacturing environment, and our results indicate that model-free\nRL algorithms achieve high scores of the tasks especially in existing of dense reward structures.\n5\nAcknowledgements\nThis work was supported by U.S. Department of Commerce 445 (70NANB19H005) and National Institute of Standards\nand Technology as part of the Center for Hierarchical Materials Design (CHi-MaD).\n7\nToolpath design for additive manufacturing using deep reinforcement learning\nReferences\nJohn C Steuben, Athanasios P Iliopoulos, and John G Michopoulos. Implicit slicing for functionally tailored additive\nmanufacturing. Computer-Aided Design, 77:107–119, 2016.\nJaved Akram, Pradeep Chalavadi, Deepankar Pal, and Brent Stucker. Understanding grain evolution in additive\nmanufacturing through modeling. Additive Manufacturing, 21:255–268, 2018.\nTarun Bhardwaj and Mukul Shukla. Effect of laser scanning strategies on texture, physical and mechanical properties\nof laser sintered maraging steel. Materials Science and Engineering: A, 734:102–109, 2018.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver,\nand Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on\nmachine learning, pages 1928–1937, 2016.\nhttps://www.thingiverse.com. Cad repository. URL https://www.thingiverse.com/about/.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement\nlearning. nature, 518(7540):529–533, 2015.\nShangtong Zhang and Richard S Sutton. A deeper look at experience replay. arXiv preprint arXiv:1712.01275, 2017.\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. arXiv preprint\narXiv:1509.06461, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu,\nAbhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905,\n2018.\nEric Jang, Shixiang Gu, and Ben Poole.\nCategorical reparameterization with gumbel-softmax.\narXiv preprint\narXiv:1611.01144, 2016.\n8\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2020-09-30",
  "updated": "2020-09-30"
}