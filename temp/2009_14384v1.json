{
  "id": "http://arxiv.org/abs/2009.14384v1",
  "title": "Development of Word Embeddings for Uzbek Language",
  "authors": [
    "B. Mansurov",
    "A. Mansurov"
  ],
  "abstract": "In this paper, we share the process of developing word embeddings for the\nCyrillic variant of the Uzbek language. The result of our work is the first\npublicly available set of word vectors trained on the word2vec, GloVe, and\nfastText algorithms using a high-quality web crawl corpus developed in-house.\nThe developed word embeddings can be used in many natural language processing\ndownstream tasks.",
  "text": "Development of Word Embeddings for Uzbek Language\nB. Mansurov and A. Mansurov\nCopper City Labs\n{b,a}mansurov@coppercitylabs.com\nSeptember 29, 2020\nAbstract\nIn this paper, we share the process of developing word embeddings for the Cyrillic variant of the\nUzbek language. The result of our work is the first publicly available set of word vectors trained on\nthe word2vec, GloVe, and fastText algorithms using a high-quality web crawl corpus developed\nin-house. The developed word embeddings can be used in many natural language processing\ndownstream tasks.\nKeywords: word embeddings, word2vec, GloVe, fastText, pre-trained, Uzbek language, Cyrillic\nscript\n1\nIntroduction\nIn natural language processing (NLP) word embeddings are mappings of words to vectors of real\nnumbers. They are used in many downstream tasks such as information retrieval [1], sentiment analysis\n[2], document summarization [3], and machine translation [4]. Pre-trained word embeddings for many\nlanguages are publicly available [5].\nThe Uzbek language is one of many low-resource languages, with very few publicly available data re-\nsources. Currently, both the Cyrillic and Latin scripts are used in the language. There are no known\npublicly available word embeddings for the Cyrillic variant of Uzbek. As far as we’re aware, only\nfastText [5] word embeddings exist for the Latin variant. However, fastText was trained on the rela-\ntively low quality Uzbek Wikipedia and noisy Common Crawl corpus.\nIn this paper, we describe the process of generating three kinds of word embeddings (word2vec [6],\nGloVe [7], and fastText [8]) using high-quality data for the Cyrillic variant of the Uzbek language.\nWe also publicly share our word vectors. Section 2 describes the process of gathering the training\ndata. Section 3 provides the trained models and Section 4 shares the results. Section 5 summarizes our\nresearch and identifies areas for future research.\n2\nData\nTo train our word embeddings, we crawled websites in the ”uz” domain. For each website we manually\ncreated a scraper to extract only the article content written in Cyrillic Uzbek, and not other texts such\nas user interface elements or user comments.\nWe chose to parse Cyrillic texts because parsing texts written in the Latin script poses certain data\nissues. For example, the letter ”ў” in Cyrillic can be written in many different forms in Latin: ”o'”,\n1\n”o`”, ”o´”, ”oʻ”, and ”o’”. Although only one of them is correct, all of the above forms may appear\nin text, thus making it hard to correctly identify words. A similar issue happens with the letter ”ғ”\n(gʻ).\nThe majority of websites we parsed are news websites because their archives are publicly available\nand they are continuously updated. In the future, if we want to release new versions of our models,\nwe can simply retrieve new articles and create embeddings from the combined existing and new data.\nNews articles also cover a wide range of topics. Websites we parsed include articles in the following\ncategories: crime, culture, economics, health, politics, society, sports, and technology among others.\nIt’s common for websites in Uzbek to provide content in Russian as well. To keep the training data\nclean, it was important to retrieve texts written in Uzbek only. Since Russian also uses the Cyrillic\nscript, we made sure to ignore any texts containing even a single letter ”ы” (which doesn’t exist in\nUzbek, but exists in Russian). This rule also ignores mainly Uzbek texts with a little bit of Russian\nincluded, but in order to keep the end result clean, we decided to accept this trade-off.\nOnce article contents were extracted, we lower-cased the words, and extracted tokens containing only\nthe Cyrillic Uzbek letters and a hyphen. The total number of such tokens was 79,579,042.\nWe didn’t parse Uzbek Wikipedia because most of its articles were automatically created from an\nUzbek encyclopedia without human verification, and thus contains typos and abbreviations where usage\nof full words would be appropriate. Moreover, Uzbek Wikipedia is written in the Latin variant of\nUzbek. Although Wikipedia provides a converter from Latin to Cyrillic, the end result contains many\nerrors.\n3\nTraining\nWe trained word2vec, GloVe, and fastText embeddings because they seem to be widely used by NLP\npractitioners. For example, the popular Python library Gensim1 allows loading and using word2vec\nand fastText models. It is also possible to convert GloVe embeddings to word2vec format using the\nlibrary. Another reason for the popularity of these algorithms is that their implementations are publicly\navailable.\n3.1\nWord2vec\nWord2vec was trained using the reference implementation2. The following table summarizes the\ntrained models. The vocabulary generated by word2vec contains 366,276 unique tokens.\nTable 1: Trained word2vec models. CBOW refers to continuous bag of words.\nArchitecture\nTraining Algorithm\nWord Vector Size\nWindow\nCBOW\nNegative Sampling\n100\n5\nCBOW\nHierarchical Softmax\n300\n5\nSkipgram\nNegative Sampling\n100\n10\nSkipgram\nHierarchical Softmax\n300\n10\n1https://radimrehurek.com/gensim/index.html\n2https://github.com/tmikolov/word2vec\n2\n3.2\nGloVe\nWe trained a 300-dimensional word vector model using the reference implementation3.\n3.3\nFastText\nFastText was also trained using the reference implementation4. The following table summarizes the\ntrained models. In all models the minimum and maximum subword sizes were 2 and 5, respectively.\nTable 2: Trained fastText models. CBOW refers to continuous bag of words.\nArchitecture\nWord Vector Size\nCBOW\n100\nCBOW\n300\nSkipgram\n100\nSkipgram\n300\n4\nResults\nBelow are the CC BY 4.0 licensed word embeddings we share via Figshare.\n• Word2vec 100 [9] and 300 [10] dimensional word vectors (CBOW, negative sampling);\n• Word2vec 100 [11] and 300 [12] dimensional word vectors (skipgram, negative sampling);\n• GloVe 300 dimensional word vectors [13];\n• FastText 100 [14] and 300 [15] dimensional word vectors (CBOW);\n• FastText 100 [16] and 300 [17] dimensional word vectors (skipgram).\nThe shared data also include the hyper-parameters used in training.\nNext, we present some snippets from the training data, followed by nearest neighbor examples from the\nmodel outputs. The table below shows the most and least frequently appearing words in the training\ndata.\nTable 3: Most and least frequently appearing words in the training data.\nWord\nFrequency\nWord\nFrequency\nва (and)\n1550236\nликсеич (Likseich)\n5\nбилан (with)\n767708\nжамбатиста (Giambattista)\n5\nҳам (also)\n699727\nглазичевдан (from Glazevich)\n5\nбу (this)\n606904\nжамиятшуносга (to sociologist)\n5\nбир (one)\n544584\nбиг-бэнд (big band)\n5\nучун (for)\n493722\nмайиб-мубтало (utterly disabled)\n5\nшу (this)\n274099\nота-турна (father crane)\n5\nэди (was)\n229156\nнилкантога (to Nilkanto)\n5\nдеб (saying)\n228540\nкингисепп (Kingisepp)\n5\nўз (self)\n215465\nрухсатой (Ruxsatoy)\n5\n3https://github.com/stanfordnlp/GloVe\n4https://github.com/facebookresearch/fastText\n3\nFor our nearest neighbor examples, we chose a frequent and a rare word from the corpus. In the\ncorpus the word сув (water) appeared 40,300 times (frequent word), while the word нордон (sour)\nappeared 200 times (rare word). The ten nearest neighbors for each word and model are given below.\nAn English translation of each word is given in parentheses. Repeating translations (e.g., water) are\nprimarily due the morphological richness of the Uzbek language.\n• 100-dimensional word2vec model (CBOW, Negative Sampling)\n– сув: сувни (water), сувларни (waters), коллектор (water collector), водопровод (water\npipes), суви (water), сувнинг (of water), сувини (water), сувларнинг (of waters), сувлар\n(waters), сувлари (waters)\n– нордон: таъмли (tasteful), шакарли (sweet), тахир (bitter), таъми (taste), ялпизли\n(mint), хушхўр (pleasant), шираси (juice), хуштаъм (delicious), шакари (sugar), шарбатларни\n(juices)\n• 300-dimensional word2vec model (CBOW, Hierarchical Softmax)\n– сув: сувни (water), сувлар (waters), сувнинг (of water), сувини (water), сувларни\n(waters), сувдан (from water), сувлари (waters), суви (water), сувли (watery), лойқа\n(muddy)\n– нордон: мазали (tasty), ёғли (greasy), ширин (sweet), аччиқ (bitter), бемаза (bland),\nхушбўй (fragrant), тансиқ (yummy), ейиш (eating), сархил (flavorful), қуритилган (dried)\n• 100-dimensional word2vec model (Skipgram, Negative Sampling)\n– сув: сувни (water), суви (water), сувнинг (of water), сувдан (from water), сувини (wa-\nter), лойқа (muddy), сувлари (waters), сувга (water), сувларни (waters), сувига (water)\n– нордон: таъмли (tasteful), шарбатлар (juices), цитруслилар (citrus), ўпка-жигар (lung-\nliver), қовурилган (fried), дудланган (smoked), шираси (juice), апельсин (orange), таъм\n(taste), қовурма (fried food)\n• 300-dimensional word2vec model (Skipgram, Hierarchical Softmax)\n– сув: сувни (water), сувнинг (of water), лойқа (muddy), сувга (water), сувдан (from\nwater), сувини (water), суви (water), сувлар (waters), сувлари (waters), сувига (water)\n– нордон: таъмли (tasteful), апельсин (orange), шарбатлар (juices), егуликлар (foods),\nшираси (juice), ёғли (greasy), шарбат (juice), мева (fruit), мандарин (mandarin), газланган\n(carbonated)\n• 300-dimensional GloVe model\n– сув: суви (water), сувни (water), ичимлик (drink), тупроқ (soil), сувдан (from water),\nтоза (clean), ер (land), сувга (water), газ (gas), табиий (natural)\n– нордон: таъмли (tasteful), таъми (taste), егуликлар (foods), болдан (honey), тахир\n(bitter), кифояли (enough), қалампирнинг (of pepper), болдек (like honey), шўр (saline),\nмазали (tasty)\n• 100-dimensional fastText model (CBOW)\n– сув: сув-сув (water-water), сув-пув (water-drinks), сувлиқ (watery), сувд (water), сувлоқ\n(watery), сувлуқ (wateriness), сувин (have water), сувдон (jug), суву (water), сувот (wa-\ntering)\n– нордон: нордон-шўр (sour-salty), гордон (gordon), ордон (sour like), кордон (cordon),\nнордонгина (soury), нортон (norton), гардон (keeper), нор (markedly), норд (sour),\nнорчучук (sour like)\n• 300-dimensional fastText model (CBOW)\n4\n– сув: сув-сув (water-water), сув-пув (water-drinks), сувд (water), суву (water and), сувлиқ\n(watery), сувлуқ (wateriness), сувлоқ (watery), сувин (have water), сувлисой (creek),\nсувг (water)\n– нордон: нордон-шўр (sour-salty), ордон (sour like), гордон (gordon), норд (sour),\nкордон (cordon), нордонгина (soury), нортон (norton), нор (markedly), нордонроқ\n(sour like taste), норча (soury taste)\n• 100-dimensional fastText model (Skipgram)\n– сув: сувни (water), суви (water), сувин (have water), сувлар (waters), сувини (water),\nсувларни (waters), сувлаб (watering), сув-сув (water-water), сувим (my water), сувд\n(water)\n– нордон: нордон-шўр (sour-salty), нордонроқ (sour like taste), нордонгина (soury),\nтаъмли (tasteful), ошқошиқ (spoon), ананас (pineapple), ачқимтил (bitterly), кефир\n(kefir), апельсин (orange), шираси (juice)\n• 300-dimensional fastText model (Skipgram)\n– сув: сувни (water), суви (water), сувин (have water), сувд (water), сув-сув (water-\nwater), сувнинг (of water), сувим (my water), сувини (water), сувлар (waters), сувдан\n(from water)\n– нордон: нордон-шўр (sour-salty), нордонгина (soury), нордонроқ (sour like taste),\nтаъмли (tasteful), бордон (mour), норд (sour), ордон (sour), кефир (kefir), шакарсиз\n(sugarless), шакарли (sugary)\n5\nConclusion\nWe developed and shared multiple word embedding models for the Cyrillic variant of the low-resource\nUzbek language. Our hope is that these models will be useful in research and real-world applications.\nAs part of this work we didn’t analyze the quality and performance of our trained word vectors. We\nleave it as an exercise for future work. In the future we’d also like to train word embeddings using\nvarious sources of data, not just websites.\nAnother improvement is related to correctly identifying texts written in Uzbek. Our heuristic approach\nof ignoring Russian articles may have worked well in this case, but in the future, with a diverse set of\ntraining data, we will need to employ more robust methods of identifying and excluding non-Uzbek\ntexts from our training data set.\nWord vectors that we generated are context-independent — they output only one vector per word.\nCreating context-dependent word embeddings such as generated by BERT [18] and ELMo [19] is\nanother area to explore.\n5\n6\nReferences\n[1] Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth JF Jones. Word embedding based\ngeneralized language model for information retrieval. In Proceedings of the 38th international\nACM SIGIR conference on research and development in information retrieval, pages 795–798,\n2015.\n[2] Maria Giatsoglou, Manolis G Vozalis, Konstantinos Diamantaras, Athena Vakali, George Sari-\ngiannidis, and Konstantinos Ch Chatzisavvas. Sentiment analysis leveraging emotions and word\nembeddings. Expert Systems with Applications, 69:214–224, 2017.\n[3] Kuan-Yu Chen, Shih-Hung Liu, Hsin-Min Wang, Berlin Chen, and Hsin-Hsi Chen. Leveraging\nword embeddings for spoken document summarization. arXiv preprint arXiv:1506.04365, 2015.\n[4] Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Janani Padmanabhan, and Graham\nNeubig. When and why are pre-trained word embeddings useful for neural machine translation?\narXiv preprint arXiv:1804.06323, 2018.\n[5] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning\nword vectors for 157 languages. arXiv preprint arXiv:1802.06893, 2018.\n[6] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word rep-\nresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[7] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), page nil, - 2014. doi: 10.3115/v1/d14-1162. URL https://doi.\norg/10.3115/v1/d14-1162.\n[8] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors\nwith subword information. Transactions of the Association for Computational Linguistics, 5:135–\n146, 2017.\n[9] B. Mansurov.\n100d word2vec (CBOW, negative sampling) embeddings for Cyril-\nlic\nUzbek\nusing\nwebcrawl\nv1\ncorpus.\n9\n2020.\ndoi:\n10.6084/m9.figshare.\n12991472.v2.\nURL\nhttps://figshare.com/articles/dataset/\nuzb-cyrl-webcrawl-v1-word2vec-cbow-ns-100d/12991472.\n[10] B.\nMansurov.\n300d\nword2vec\n(CBOW,\nhierarchical\nsoftmax)\nembeddings\nfor\nCyrillic Uzbek using webcrawl v1 corpus.\n9 2020.\ndoi:\n10.6084/m9.figshare.\n12991454.v3.\nURL\nhttps://figshare.com/articles/dataset/\nuzb-cyrl-webcrawl-v1-word2vec-cbow-hs-300d/12991454.\n[11] B.\nMansurov.\n100d\nword2vec\n(skipgram,\nnegative\nsampling)\nembeddings\nfor\nCyrillic Uzbek using webcrawl v1 corpus.\n9 2020.\ndoi:\n10.6084/m9.figshare.\n12991484.v2.\nURL\nhttps://figshare.com/articles/dataset/\nuzb-cyrl-webcrawl-v1-word2vec-skipgram-ns-100d/12991484.\n[12] B.\nMansurov.\n300d\nword2vec\n(skipgram,\nhierarchical\nsoftmax)\nembeddings\nfor\nCyrillic Uzbek using webcrawl v1 corpus.\n9 2020.\ndoi:\n10.6084/m9.figshare.\n12991475.v2.\nURL\nhttps://figshare.com/articles/dataset/\nuzb-cyrl-webcrawl-v1-word2vec-skipgram-hs-300d/12991475.\n6\n[13] B. Mansurov. 300d glove embeddings for Cyrillic Uzbek using webcrawl v1 corpus. 9 2020.\ndoi: 10.6084/m9.figshare.12991487.v3.\nURL https://figshare.com/articles/\ndataset/uzb-cyrl-webcrawl-v1-glove-300d/12991487.\n[14] B. Mansurov.\n100d fasttext (CBOW) embeddings for Cyrillic Uzbek using webcrawl v1\ncorpus.\n9 2020.\ndoi: 10.6084/m9.figshare.12999098.v2.\nURL https://figshare.\ncom/articles/dataset/uzb-cyrl-webcrawl-v1-fasttext-cbow-100d/\n12999098.\n[15] B. Mansurov.\n300d fasttext (CBOW) embeddings for Cyrillic Uzbek using webcrawl v1\ncorpus.\n9 2020.\ndoi: 10.6084/m9.figshare.12999116.v2.\nURL https://figshare.\ncom/articles/dataset/uzb-cyrl-webcrawl-v1-fasttext-cbow-300d/\n12999116.\n[16] B. Mansurov.\n100d fasttext (skipgram) embeddings for Cyrillic Uzbek using webcrawl v1\ncorpus. 9 2020. doi: 10.6084/m9.figshare.12999815.v2. URL https://figshare.com/\narticles/dataset/uzb-cyrl-webcrawl-v1-fasttext-skipgram-100d/\n12999815.\n[17] B. Mansurov.\n300d fasttext (skipgram) embeddings for Cyrillic Uzbek using webcrawl v1\ncorpus.\n9 2020.\ndoi: 10.6084/m9.figshare.12999839.v1.\nURL https://figshare.\ncom/articles/dataset/300d_fasttext_skipgram_embeddings_for_\nCyrillic_Uzbek_using_webcrawl_v1_corpus/12999839.\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL\nhttp://arxiv.org/abs/1810.04805.\n[19] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. CoRR, abs/1802.05365, 2018.\nURL http://arxiv.org/abs/1802.05365.\n7\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-09-30",
  "updated": "2020-09-30"
}