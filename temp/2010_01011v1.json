{
  "id": "http://arxiv.org/abs/2010.01011v1",
  "title": "Deep Convolutional Transform Learning -- Extended version",
  "authors": [
    "Jyoti Maggu",
    "Angshul Majumdar",
    "Emilie Chouzenoux",
    "Giovanni Chierchia"
  ],
  "abstract": "This work introduces a new unsupervised representation learning technique\ncalled Deep Convolutional Transform Learning (DCTL). By stacking convolutional\ntransforms, our approach is able to learn a set of independent kernels at\ndifferent layers. The features extracted in an unsupervised manner can then be\nused to perform machine learning tasks, such as classification and clustering.\nThe learning technique relies on a well-sounded alternating proximal\nminimization scheme with established convergence guarantees. Our experimental\nresults show that the proposed DCTL technique outperforms its shallow version\nCTL, on several benchmark datasets.",
  "text": "arXiv:2010.01011v1  [cs.LG]  2 Oct 2020\nDeep Convolutional Transform Learning‹\nJyoti Maggu1, Angshul Majumdar1, Emilie Chouzenoux2, and Giovanni\nChierchia3\n1 Indraprastha Institute of Information Technology Delhi, India.\njyotim, angshul@iiitd.ac.in\n2 CVN, Inria Saclay, Univ. Paris-Saclay, CentraleSup´elec, Gif-sur-Yvette, France.\nemilie.chouzenoux@centralesupelec.fr\n3 LIGM, ESIEE Paris, Univ. Gustave Eiﬀel, Noisy-le-Grand, France.\ngiovanni.chierchia@esiee.fr\nAbstract. This work introduces a new unsupervised representation learn-\ning technique called Deep Convolutional Transform Learning (DCTL).\nBy stacking convolutional transforms, our approach is able to learn a\nset of independent kernels at diﬀerent layers. The features extracted in\nan unsupervised manner can then be used to perform machine learning\ntasks, such as classiﬁcation and clustering. The learning technique re-\nlies on a well-sounded alternating proximal minimization scheme with\nestablished convergence guarantees. Our experimental results show that\nthe proposed DCTL technique outperforms its shallow version CTL, on\nseveral benchmark datasets.\nKeywords: Transform Learning · Deep Learning · Convolutional Neural\nNetworks · Classiﬁcation · Clustering · Proximal Methods · Alternating\nMinimization\n1\nIntroduction\nDeep learning and more particularly convolutional neural networks (CNN) have\npenetrated almost every perceivable area of signal/image processing and machine\nlearning. Its performance in traditional machine learning tasks encountered in\ncomputer vision, natural language processing and speech analysis are well as-\nsessed. CNNs are also being used with success in traditional signal processing\ndomains, such as biomedical signal analysis [9], radars [14], astronomy [3] and in-\nverse problems [22]. When large volumes of labeled data are available, CNNs can\nbe trained eﬃciently using back-propagation methods and reach excellent perfor-\nmance [18]. However, training a CNN requires labeled data in a large quantity.\nThe latter issue can be overcome by considering alternate learning paradigms,\nsuch as spiking neural network (SNN) [21] and the associated Hebbian learning\n[10], or alternate optimization strategies such as in [20]. However, none of those\napproaches can overcome the fundamental problem of neural networks, that is\n‹ This work was supported by the CNRS-CEFIPRA project under grant NextGenBP\nPRC2017.\n2\nJ. Maggu et al.\ntheir limited capacity of learning in an unsupervised fashion. This explains the\ngreat recent interest in the machine learning community for investigating rep-\nresentation learning methods, that keep the best of both worlds, that is the\nperformance of multi-layer convolutional representations and the unsupervised\nlearning capacity [2,19,8,12,7].\nIn this work, we propose a deep version of the convolutional transform learn-\ning (CTL) approach introduced in [12], that we call deep convolutional transform\nlearning (DCTL). A proximal alternating minimization scheme allows us to learn\nmultiple layers of convolutional ﬁlters in an unsupervised fashion. Numerical ex-\nperiments illustrate the ability of the method to learn representative features\nthat lead to great performance on a set of classiﬁcation and clustering problems.\nThe rest of the paper is organized into several sections. Section 2 intro-\nduces the transform learning paradigm and brieﬂy reminds our previous CTL\napproach. The proposed DCTL formulation and the associated learning strategy\nare presented in Section 3. The experimental results are described in Section 4.\nThe conclusion of this work is drawn in Section 5.\n2\nTransform learning\n2.1\nThe transform learning paradigm\nTraditional machine learning methods are limited in their ability to work with\nraw data. To perform any machine learning task, careful feature engineering\nis used, which in turn requires domain expertise. Using domain knowledge, a\nfeature extractor is built to transform the raw data into a suitable internal\nrepresentation. This internal representation is then fed to a learning subsystem,\noften a classiﬁer to detect a pattern in the input data. Learning the weights\nbetween input and representation layer is a challenging task since both weights\nand output are unknown. This is called representation learning.\nTransform learning, introduced in [17,16], is a representation learning para-\ndigm that can be viewed as the analysis equivalent of dictionary learning. In\ndictionary learning, a basis is learned such that it synthesizes the data from the\nlearned coeﬃcients. Transform learning analyzes the data by learning a basis to\nproduce the coeﬃcients. Mathematically this is expressed as T X « Z, where\nT is the analysis transform, X is the data, and Z the corresponding coeﬃcient\nmatrix. As proposed in [15], the matrices T and Z could be estimated by solving\nthe following optimization problem\nminimize\nT,Z\n1\n2}T X ´ Z}2\nF ` λp}T }2\nF ´ log det T q ` β}Z}1.\n(1)\nThe logarithmic determinant (log det) term aims at imposing a full rank on\nthe learned transform, and at preventing the degenerate solution T “ 0, Z “ 0.\nThe additional quadratic penalty allows to limit scale indeterminacy. Both these\nadditional penalties improve the conditioning of learnt transforms. Finally, the\nℓ1 term enforces a sparsity constraint on the coeﬃcients. It is worthy to notice\nDeep Convolutional Transform Learning\n3\nthat transform learning is more general than dictionary learning in its notion\nof compressibility. The learning process is also faster because the sparse coding\nstep reads here as a simple step of thresholding, while in dictionary learning, it\nrequires the resolution of a non trivial optimization problem.\n2.2\nConvolutional transform learning\nWe proposed in [12] the CTL approach, where a set of independent convolution\nﬁlters are learnt to produce some data representations, in an unsupervised man-\nner. The CTL strategy aims at generating unique and near to orthogonal ﬁlters,\nwhich in turn produces good features to be used for solving machine learning\nproblems, as we illustrated in our experiments [12]. We present here a brief de-\nscription of this approach, as its notation and concepts will serve as a basis for\nthe deep CTL formulation introduced in this paper.\nWe consider a dataset\n␣\nxpmq(\n1ďmďM with M entries in RN. The CTL for-\nmulation relies on the key assumption that the representation matrix T gathers\na set of K kernels t1, . . . , tK with K entries, namely\nT “ rt1 | . . . | tKs P RKˆK.\n(2)\nThis leads to a linear transform applied to the data to produce some features\np@m P t1, . . . , Muq\nZm « XpmqT,\n(3)\nwhere Xpmq P RNˆK are Toeplitz matrices associated to pxpmqq1ďmďM such that\nXpmqT “\n“\nXpmqt1 | . . . | XpmqtK\n‰\n“\n“\nt1 ˚ xpmq | . . . | tK ˚ xpmq‰\n,\n(4)\nand ˚ is a discrete convolution operator with suitable padding. Let us denote\nZ “\n»\n—–\nZ1\n...\nZM\nﬁ\nﬃﬂP RNMˆK.\n(5)\nThe goal is then to estimate pT, Zq from\n␣\nxpmq(\n1ďmďM. To do so, we proposed\nin [12] a penalized formulation of the problem, introducing suitable condition-\ning constraints on the transforms, and sparsity constraint on the coeﬃcients.\nThe learning of pT, Zq was then performed using an alternating minimization\nscheme with sounded convergence guarantees. The aim of the present paper is\nto introduce a multi-layer formulation of the CTL, in order to learn deeper rep-\nresentations, with the aim of improving the representation power of the features.\n3\nProposed Approach\n3.1\nDeep convolutional transform model\nStarting from the CTL model, we propose to stack several layers of it to obtain a\ndeep architecture. For every ℓP t1, . . . , Lu, we will seek for the transform matrix\nTℓ“ rt1,ℓ| . . . | tK,ℓs P RKˆK,\n(6)\n4\nJ. Maggu et al.\nwhere tk,ℓP RK is the k-th kernel on the ℓ-th layer of the representation. The\nassociated coeﬃcients will be denoted as\nZℓ“\n»\n—–\nZ1,ℓ\n...\nZM,ℓ\nﬁ\nﬃﬂP RNMˆK,\n(7)\nwith\np@m P t1, . . . , Muq\nZm,ℓ“\n“\nzpm,ℓq\n1\n| . . . | zpm,ℓq\nK\n‰\nP RNˆK.\n(8)\nThe learning of pTℓq1ďℓďL and pZℓq1ďℓďL will be performed by solving\nminimize\npTℓq1ďℓďL,pZℓq1ďℓďL\nFpT1, . . . , TL, Z1, . . . , ZLq\n(9)\nwhere\nFpT1, . . . , TL, Z1, . . . , ZLq “\nL\nÿ\nℓ“1\n˜\n1\n2\nM\nÿ\nm“1\n||Zm,ℓ´1Tℓ´ Zm,ℓ||2\nF ` µ||Tℓ||2\nF\n´ λ log detpTℓq ` β||Zℓ||1 ` ι`pZℓq\n¸\n,\n(10)\nHere, we denote ι` the indicator function of the positive orthant, equals to 0 if\nall entries of its input have non negative elements, and `8 otherwise. Moreover,\nby a slight abuse of notation, we denote as log det the sum of logarithms of the\nsingular values of a squared matrix, taking inﬁnity value as soon as one of those\nis non positive. The ﬁrst layer follows the CTL strategy, that is Zm,0 ” Xpmq.\nMoreover, for every ℓP t2, . . . , Lu, we introduced the linear operator Zm,ℓ´1 so\nas to obtain the compact notation for the multi-channel convolution product:\nZm,ℓ´1Tℓ“ rZm,ℓ´1t1,ℓ| . . . |Zm,ℓ´1tK,ℓs\n(11)\n“\n”\nt1,ℓ˚ zpm,ℓ´1q\n1\n| . . . |tK,ℓ˚ zpm,ℓ´1q\nK\nı\n.\n(12)\n3.2\nMinimization algorithm\nProblem (9) is non-convex. However it presents a particular multi-convex struc-\nture, that allows us to make use of an alternating proximal minimization al-\ngorithm to solve it [1,4]. The proximity operator [5] of a proper, lower semi-\ncontinuous, convex function ψ : H ÞÑs´8, `8s, with pH, }¨}q a normed Hilbert\nspace, is deﬁned as4\np@ r\nX P Hq\nproxψp r\nXq “ argmin\nXPH\nψpXq ` 1\n2}X ´ r\nX}2.\n(13)\nThe alternating proximal minimization algorithm then consists in performing\niteratively proximity updates, on the transform matrix, and on the coeﬃcients.\n4 See also http://proximity-operator.net/\nDeep Convolutional Transform Learning\n5\nThe iterates are guaranteed to ensure the monotonical decrease of the loss func-\ntion F. Convergence to a local minimizer of F can also be ensured, under mild\ntechnical assumptions. The algorithm reads as follows:\nFor\ni “ 0, 1, . . .\n—————–\nFor\nℓ“ 1, . . . , L\n———–T ri`1s\nℓ\n“ proxγ1F pT ri`1s\n1\n,...,T ris\nL ,Zri`1s\n1\n,...,Zris\nL q\n´\nT ris\nℓ\n¯\nZri`1s\nℓ\n“ proxγ2F pT ri`1s\n1\n,...,T ris\nL ,Zri`1s\n1\n,...,Zris\nL q\n´\nZris\nℓ\n¯\n(14)\nwith T r0s\nℓ\nP RKˆK, Zr0s\nℓ\nP RNMˆK, and γ1 and γ2 some positive constants.\nWe provide hereafter the expression of the proximity operators involved in the\nalgorithm, whose proof are provided in the appendix.\nUpdate of the transform matrix: Let i P N and ℓP t1, . . . , Lu. Then\nT ri`1s\nℓ\n“ proxγ1F pT ri`1s\n1\n,...,T ris\nL ,Zri`1s\n1\n,...,Zris\nL q\n´\nT ris\nℓ\n¯\n,\n“ argmin\nTℓPRKˆK\n1\n2γ1\n||Tℓ´ T ris\nℓ||2\nF\n` 1\n2\nM\nÿ\nm“1\n||Zri`1s\nm,ℓ´1Tℓ´ Zris\nm,ℓ||2\nF ` µ||Tℓ||2\nF ´ λ log detpTℓq\n“ 1\n2Λ´1V\n´\nΣ ` pΣ2 ` 2λIdq\n1{2¯\nU J,\n(15)\nwith\nΛJΛ “\nM\nÿ\nm“1\npZri`1s\nm,ℓ´1qJpZri`1s\nm,ℓ´1q ` pγ´1\n1\n` 2µqId.\n(16)\nHereabove, we considered the singular value decomposition:\nUΣV J “\n˜ M\nÿ\nm“1\npZris\nm,ℓqJpZri`1s\nm,ℓ´1q ` γ´1\n1 T ris\nℓ\n¸\nΛ´1.\n(17)\nUpdate of the coeﬃcient matrix: Let i P N. We ﬁrst consider the case when\nℓP t1, . . . , L ´ 1u (recall that Zm,0 “ Xpmq when ℓ“ 1). Then\nZri`1s\nℓ\n“ proxγ2F pT ri`1s\n1\n,...,T ris\nL ,Zri`1s\n1\n,...,Zris\nL q\n´\nZris\nℓ\n¯\n,\n“\nargmin\nZℓPRMNˆK\n1\n2γ2\n||Zℓ´ Zris\nℓ||2\nF\n` 1\n2\nM\nÿ\nm“1\n||Zri`1s\nm,ℓ´1T ri`1s\nℓ\n´ Zm,ℓ||2\nF\n` 1\n2\nM\nÿ\nm“1\n||Zm,ℓT ri`1s\nℓ`1\n´ Zris\nm,ℓ`1||2\nF\n` β||Zℓ||1 ` ι`pZℓq.\n(18)\n6\nJ. Maggu et al.\nAlthough the above minimization does not have a closed-form expression, it can\nbe eﬃciently carried out with the projected Newton method. In the case when\nℓ“ L, the second term is dropped, yielding\nZri`1s\nL\n“ proxγ2F pT ri`1s\n1\n,...,T ri`1s\nL\n,Zri`1s\n1\n,...,Zri`1s\nL´1 ,¨q\n´\nZris\nL\n¯\n“\nargmin\nZLPRMNˆK\n1\n2γ2\n}ZL ´ Zris\nL }2\nF\n` 1\n2\nM\nÿ\nm“1\n}Zri`1s\nm,L´1T ri`1s\nL\n´ Zm,L}2\nF ` β}ZL}1 ` ι`pZLq.\n(19)\nHereagain, the projected Newton method can be employed for the minimization.\n4\nNumerical results\n4.1\nDatasets\nTo assess the performance of the proposed approach, we considered the following\nimage datasets5 of small-to-medium size.\n1. YALE [6]: The Yale dataset contains 165 images of 15 individuals, down-\nscaled to 32-by-32 pixels. There are 11 images per subject, one per diﬀerent\nfacial expression or conﬁguration. For our experiments, we shuﬄed all the\nsamples, and took 70% for training and 30% for testing. Moreover, we full-\nsize YALE images of size 150-by-150 pixels.\n2. E-YALE-B [11]: The extended Yale B database contains 2432 images with\n38 subjects under 64 illumination conditions. Each image is cropped to 192-\nby-168 pixels and downscaled to 48-by-42 pixels. For our experiments, we\nshuﬄed all the samples, took 70% for training and 30% for testing.\n3. AR-Face [13]: This database contains more than 4000 images of 126 diﬀerent\nsubjects (70 male and 56 female). The images have various facial expressions,\nthe lighting varies, and some of the images are partially occluded by sun-\nglasses and scarves. For our experiments, we selected 2600 images of 100\nindividuals (50 males and 50 females), that is 26 diﬀerent images for each\nsubject. Train set contains 2000 images, and 600 images are kept in the test\nset. Each image has 540 features.\n4.2\nNumerical results\nWe experiment on the YALE, EYALEB and AR faces datasets; these are well\nknown benchmarking face datasets. In the ﬁrst set of experiments, we want to\nshow that the accuracy of deep transform learning indeed improves when one\ngoes deeper. Going deep beyond three layers makes performance degrade as the\nmodel tends to overﬁt for the small training set. To elucidate, we have used a\n5 http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html\nDeep Convolutional Transform Learning\n7\nTable 1: Accuracy on SVM with layers\nDataset\nCTL DCTL-2 DCTL-3 DCTL-4\nYALE 150 ˆ 150 94.00\n94.28\n96.00\n92.21\nYALE 32 ˆ 32\n88.00\n89.11\n90.00\n87.73\nE-YALE-B\n97.38\n97.00\n98.00\n94.44\nAR-Faces\n88.87\n92.22\n97.67\n82.21\nTable 2: Classiﬁcation Accuracy using KNN\nDataset\nRaw Features CTL DCTL\nYALE 150 ˆ 150\n78.00\n70.00 80.00\nYALE 32 ˆ 32\n60.00\n58.85 60.00\nE-YALE-B\n71.03\n84.00 85.00\nAR-Faces\n55.00\n56.00 58.00\nTable 3: Classiﬁcation Accuracy using SVM\nDataset\nRaw Features CTL DCTL\nYALE 150 ˆ 150\n93.00\n94.00 96.00\nYALE 32 ˆ 32\n68.00\n88.00 90.00\nE-YALE-B\n93.24\n97.38 98.00\nAR-Faces\n87.33\n88.87 97.67\nTable 4: Convolutional Transformed Clustering: ARI\nYALEB/Method Raw Features DCTL-2 DCTL-3\nK-means\n0.785\n0.734\n0.788\nRandom\n0.733\n0.718\n0.738\nPCA-based\n0.734\n0.791\n0.777\nTable 5: Clustering time in sec\nYALEB/Method Raw Features DCTL-2 DCTL-3\nK-means\n2.28\n0.45\n0.14\nRandom\n1.95\n0.33\n0.08\nPCA-based\n0.36\n0.09\n0.03\n8\nJ. Maggu et al.\nsimple support vector machine (SVM) classiﬁer. The results are shown in Ta-\nble 1 for levels 1, 2, 3 and 4. It has already been shown in [12] that the single\nlayer CTL yielded better results than other single layer representation learning\ntools, including dictionary learning and transform Learning. Therefore it is ex-\npected that by going deeper, we will improve upon their deeper counterparts.\nWe do not repeat those baseline experiments here, by lack of space. We also skip\ncomparison with CNNs because of its supervised nature, whereas the proposed\ntechnique is unsupervised. We only show comparison of our proposed technique\nwith raw features and with CTL. We take extracted features from the proposed\nDCTL and perform classiﬁcation using two classiﬁers, namely KNN and SVM.\nThe classiﬁcation accuracy is reported in table 2 and table 3. Then we perform\nclustering on the extracted features of DCTL and report the comparison of Ad-\njusted Rank Index (ARI) in table 4. We also report clustering time on extracted\nfeatures in table 5. It is worthy to remark that the time to cluster extracted\nfeatures from the proposed methodology is comparatively less than others.\n5\nConclusion\nThis paper introduces a deep representation learning technique, named Deep\nConvolutional Transform Learning. Numerical comparisons are performed with\nthe shallow convolutional transform learning formulations on image classiﬁcation\nand clustering tasks. In the future, we plan to compare with several other deep\nrepresentation learning techniques, namely stacked autoencoder and its convo-\nlutional version, restricted Boltzmann machine and its convolutional version,\ndiscriminative variants of deep dictionary and transform Learning.\n6\nAppendix: Proofs of the proximity updates\n6.1\nUpdate of T\nLet us consider M “ 1 for simplicity, but note that all the calculations hold for\nM greater than 1. We want to minimize a function of the form:\nΦpT q “ 1\n2||XT ´ Z||2\nF ` µ||T ||2\nF ´ λ log detpT q `\n1\n2γ1\n||T ´ T rns||2\nF .\n(20)\nUsing some linear algebra, We can easily prove that:\nΦpT q “ 1\n2||W 1{2T ´ Y ||2\nF ´ λ log detpT q ` c\n(21)\nwith c a constant with respect to T ,\nW “ XJX ` p2µ ` 1\nγ1\nqId\n(22)\nand\nY “ W ´1{2pZJX ` 1\nγ1\nT rnsq.\n(23)\nDeep Convolutional Transform Learning\n9\nSince W is invertible, one can perform the change of variable ˜T “ W 1{2T , that\nis T “ W ´1{2 ˜T. Thus,\nargminT ΦpT q “ W ´1{2 argminT ˜TΦpW ´1{2 ˜Tq,\n(24)\nwith\nΦpW ´1{2 ˜Tq “ 1\n2|| ˜T ´ Y ||2\nF ´ λ log detpW ´1{2 ˜Tq ` c.\n(25)\nMoreover,\nlog detpW ´1{2 ˜T q “ log detp ˜Tq.\n(26)\nThus,\nargmin ˜T ΦpW ´1{2 ˜Tq “ proxλ log detp ˜T qpY q,\n(27)\nwhich maps with the proximity operator of the logarithmic determinant function\nwith weight λ. We can then apply [5, Example 24.66] and [5, Proposition 24.68]\nto conclude the proof.\n6.2\nUpdate of Z\nWe have to solve:\nargminZ\n1\n2\nM\nÿ\nm“1\n}XpmqT rn`1s ´ Zm}2\nF ` β||Z||1 ` ι`pZq `\n1\n2γ2\n||Z ´ Zrns||2\nF .\n(28)\nThe function in (28) is fully separable, i.e. it can be written as a sum over all the\nentries of matrix Z. Due to the separability property of the proximity operator,\nit is suﬃcient to resonate on the minimization of scalar function with respect to\nZp,q,r:\n1\n2prXmT i`1sp,q ´ Zp,q,rq2 ` β|Zp,q,r| ` ι`pZp,q,rq ` 1\n2γ2pZp,q,r ´ Zi\np,q,rq2. (29)\nOne can conclude, noticing that the term β| ¨ | ` ι` corresponds to case ’ix’ of\nin [5, Table 10.2], and by applying case ’iv’ of [5, table 10.1] to process the ﬁnal\nquadratic term.\nReferences\n1. Attouch, H., Bolte, J., Svaiter, B.F.: Convergence of descent methods for semi-\nalgebraic and tame problems: proximal algorithms, forward-backward splitting,\nand regularized Gauss-Seidel methods. Mathematical Programming 137, 91–129\n(Feb 2011)\n2. Chabiron, O., Malgouyres, F., Tourneret, J.: Toward fast transform learning. In-\nternational Journal on Computer Vision (114), 195–216 (2015)\n3. Chan, M.C., Stott, J.P.: Deep-cee i: ﬁshing for galaxy clusters with deep neural\nnets. Monthly Notices of the Royal Astronomical Society 490(4), 5770–5787 (2019)\n10\nJ. Maggu et al.\n4. Chouzenoux, E., Pesquet, J.C., Repetti, A.: A block coordinate variable met-\nric forward-backward algorithm. Journal on Global Optimization 66(3), 457–485\n(2016)\n5. Combettes, P.L., Pesquet, J.C.: Proximal splitting methods in signal processing.\nIn: Fixed-Point Algorithms for Inverse Problems in Science and Engineering, pp.\n185–212. Springer-Verlag, New York (2010)\n6. D.J.:\nThe\nyale\nface\ndatabase.\nURL:\nhttp://cvc.\nyale.\nedu/projects/yalefaces/yalefaces. html 1(2), 4 (1997)\n7. El Gheche, M., Chierchia, G., Frossard, P.: Multilayer network data clustering.\nIEEE Transactions on Signal and Information Processing over Networks 6(1), 13–\n23 (Dec 2020)\n8. Fagot, D., Wendt, H., F´evotte, C., Smaragdis, P.: Majorization-minimization algo-\nrithms for convolutive NMF with the beta-divergence. In: Proceedings of the IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP\n2019). pp. 8202–8206 (2019)\n9. Hannun, A.Y., Rajpurkar, P., Haghpanahi, M., Tison, G.H., Bourn, C., Turakhia,\nM.P., Ng, A.Y.: Cardiologist-level arrhythmia detection and classiﬁcation in am-\nbulatory electrocardiograms using a deep neural network. Nature medicine 25(1),\n65 (2019)\n10. Kempter, R., Gerstner, W., Van Hemmen, J.L.: Hebbian learning and spiking\nneurons. Physical Review E 59(4), 4498 (1999)\n11. Lee, K., Ho, J., Kriegman, D.: Acquiring linear subspaces for face recognition under\nvariable lighting. IEEE Transactions on Pattern Analysis & Machine Intelligence\n(5), 684–698 (2005)\n12. Maggu, J., Chouzenoux, E., Chierchia, G., Majumdar, A.: Convolutional transform\nlearning. In: Proceedings of the International Conference on Neural Information\nProcessing (ICONIP 2018). pp. 162–174. Springer (2018)\n13. Martinez, A.M.: The ar face database. CVC Technical Report24 (1998)\n14. Mason, E., Yonel, B., Yazici, B.: Deep learning for radar. In: 2017 IEEE Radar\nConference (RadarConf). pp. 1703–1708. IEEE (2017)\n15. Ravishankar, S., Bresler, Y.: Learning sparsifying transforms. IEEE Trans. Signal\nProcess. 61(5), 1072–1086 (2013)\n16. Ravishankar, S., Bresler, Y.: Online sparsifying transform learning - Part II. IEEE\nJ. Sel. Topics Signal Process. 9(4), 637–646 (2015)\n17. Ravishankar, S., Wen, B., Bresler, Y.: Online sparsifying transform learning - Part\nI. IEEE J. Sel. Topics Signal Process. 9(4), 625–636 (2015)\n18. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-\npropagating errors. nature 323(6088), 533–536 (1986)\n19. Tang, W., Chouzenoux, E., Pesquet, J., Krim, H.: Deep transform and metric\nlearning network: Wedding deep dictionary learning and neural networks. Tech.\nrep. (2020), https://arxiv.org/pdf/2002.07898.pdf\n20. Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., Goldstein, T.: Training\nneural networks without gradients: A scalable admm approach. In: International\nconference on machine learning. pp. 2722–2731 (2016)\n21. Van Gerven, M., Bohte, S.: Artiﬁcial neural networks as models of neural informa-\ntion processing. Frontiers in Computational Neuroscience 11, 114 (2017)\n22. Ye, J.C., Han, Y., Cha, E.: Deep convolutional framelets: A general deep learning\nframework for inverse problems. SIAM Journal on Imaging Sciences 11(2), 991–\n1048 (2018)\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-10-02",
  "updated": "2020-10-02"
}