{
  "id": "http://arxiv.org/abs/2010.01063v1",
  "title": "Syntax Representation in Word Embeddings and Neural Networks -- A Survey",
  "authors": [
    "Tomasz Limisiewicz",
    "David Mareček"
  ],
  "abstract": "Neural networks trained on natural language processing tasks capture syntax\neven though it is not provided as a supervision signal. This indicates that\nsyntactic analysis is essential to the understating of language in artificial\nintelligence systems. This overview paper covers approaches of evaluating the\namount of syntactic information included in the representations of words for\ndifferent neural network architectures. We mainly summarize re-search on\nEnglish monolingual data on language modeling tasks and multilingual data for\nneural machine translation systems and multilingual language models. We\ndescribe which pre-trained models and representations of language are best\nsuited for transfer to syntactic tasks.",
  "text": "Syntax Representation in Word Embeddings and Neural Networks – A Survey\nTomasz Limisiewicz and David Mareˇcek\nInstitute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University\n{limisiewicz,marecek}@ufal.mff.cuni.cz\nAbstract: Neural networks trained on natural language\nprocessing tasks capture syntax even though it is not pro-\nvided as a supervision signal. This indicates that syntactic\nanalysis is essential to the understating of language in ar-\ntificial intelligence systems. This overview paper covers\napproaches of evaluating the amount of syntactic informa-\ntion included in the representations of words for different\nneural network architectures. We mainly summarize re-\nsearch on English monolingual data on language modeling\ntasks and multilingual data for neural machine translation\nsystems and multilingual language models. We describe\nwhich pre-trained models and representations of language\nare best suited for transfer to syntactic tasks.\n1\nIntroduction\nModern methods of natural language processing (NLP) are\nbased on complex neural network architectures, where lan-\nguage units are represented in a metric space [9, 23, 28,\n29, 30]. Such a phenomenon allows us to express linguistic\nfeatures (i.e., morphological, lexical, syntactic) mathemat-\nically.\nThe method of obtaining such representation and their\ninterpretations were described in multiple overview works.\nAlmeida and Xexéo surveyed different types of static word\nembeddings [1], and Liu et al. [18] focused on contextual\nrepresentations found in the most recent neural models.\nBelinkov and Glass [4] surveyed the strategies of interpret-\ning latent representation. Best to our knowledge, we are\nthe first to focus on the syntactic and morphological abil-\nities of the word representations. We also cover the latest\napproaches, which go beyond the interpretation of latent\nvectors and analyze the attentions present in state-of-the-\nart Transformer models.\n2\nVector Representations of Words\nThis section introduces several types of architectures that\nwe will analyze in this work.\n2.1\nStatic Word Embeddings\nIn the classical methods of language representation, each\nword is assigned a vector regardless of its current context.\nIn the Latent Semantic Analysis [8], the representation was\nobtained by counting word frequency across documents on\ndistinct subjects.\nFigure 1: Visualization of attention mechanism in Trans-\nformer architecture. It shows which parts of the text are\nimportant to compute the representation for the word “to”.\nCreated in BertViz framework [33].\nIn more recent approaches, a shallow neural network is\nused to predict each word based on context (Word2Vec\n[23]) or approximate the frequency of coocurence for a\npair of words (GloVe [28]). One explanation of the effec-\ntiveness of these algorithms is the distributional hypothesis\n[11]: \"words that occur in the same contexts tend to have\nsimilar meanings\".\n2.2\nContextual Word Vectors in Recurrent Networks\nThe main disadvantage of the static word embeddings is\nthat they do not take into account the context of words.\nThis is especially an issue for languages rich in words that\nhave multiple meanings.\nThe contextual embeddings introduced in [29] and [22]\nare able to encode both words and their contexts. They are\nbased on recurrent neural networks (RNN) and are typi-\ncally trained on language modeling or machine translation\ntasks using large text corpora. The outputs of the RNN lay-\ners are context-dependent representations that are proven\nto perform well when used as inputs for other NLP tasks\nwith much less training data available.\nAnother improvement of context modeling was possible\nthanks to the attention mechanism [2]. It allowed passing\nthe information from the most relevant part of the RNN en-\ncoder, instead of using only the contextual representation\nof the last token.\narXiv:2010.01063v1  [cs.CL]  2 Oct 2020\nFigure 2: Spatial distribution of word embeddings de-\npends on syntactic roles of words (visualization created by\nAshutosh Singh).\n2.3\nContextual Representation in Transformers\nThe most recent and widely used architecture is the Trans-\nformer [32]. It consists of several (6 to 24) layers, and\neach token position in each layer has the ability to attend\nto any position in the previous layer using a self-attention\nmechanism. Training such architecture can be easily paral-\nlelized since individual tokens can be processed indepen-\ndently; their positions are encoded within the input em-\nbeddings. An example of visualization of attention distri-\nbution computed in Transformer trained for language mod-\neling (BERT [9]) is presented in Figure 1.\nIn addition to vectors, Transformer includes latent rep-\nresentation in the form of self-attention weights, which are\ntwo-dimensional matrices. We summarize the research on\nthe syntactic properties of attention weights in Section 5.\n3\nMeasures of Syntactic Information\nThis sections describes the metrics used to evaluate syn-\ntactic information captured by the word embeddings and\nlatent representation.\n3.1\nSyntactic Analogies\nIn the recent revival of word embeddings[23, 28], a strong\nfocus was put on examining the phenomenon of encoding\nanalogies in multidimensional space. That is to say, the\nshift vector between pairs of analogous words is approxi-\nmately constant, e.g., the pairs drinking – drank, swimming\n– swam in Figure 2.\nSyntactic analogies of this type are particularly relevant\nfor this overview. They include the following relations: ad-\njective – adverb; singular – plural; adjective – compara-\ntive – superlative; verb – present participle – past partici-\nple. The syntactic analogy is usually evaluated on Google\nAnalogy Test Set [23]. 1\n1The test set is called syntactic by authors; nevertheless, it mostly\nfocuses on morphological features.\nAn evaluation example consists of two word pairs rep-\nresented by the embeddings: (v1,v2),(u1,u2). We compute\nthe analogy shift vector as the difference between embed-\ndings of the first pair s = v2 −v1. The result is positive if\nthe nearest word embedding to the vector u1 +s is u2.\nWA = |{(v1,v2,u1,u2) : u2 ≈u1 +v2 −v1}|\n|{(v1,v2,u1,u2)}|\n(1)\n3.2\nSequence Tagging\nSequence tagging is a multiclass classification problem.\nThe aim is to predict the correct tag for each token of a se-\nquence. A typical example is the part of speech (POS) tag-\nging. The accuracy evaluation is straightforward: the num-\nber of correctly assigned tags is divided by the number of\ntokens.\n3.3\nSyntactic structure prediction\nThe inference of reasonable syntactic structures from\nword representations is the most challenging task cov-\nered in our survey. There are attempts to predict both the\ndependency[7, 12, 15, 31] and constituency trees [13, 21].\nDependency trees are evaluated using unlabeled attach-\nment score (UAS) or its undirected variant (UUAS):\nUAS = #correctly_attached_words\n#all_words\n(2)\nThe equation for Labeled Attachment Score is the same,\nbut it requires predicting a dependency label for each edge.\nFor constituency, trees we define precision (P) and recall\n(R) for correctly predicted phrases.\nP = #correct_phrases\n#gold_phrases ,\nR =\n#correct_phrases\n#predicted_phrases (3)\nUsually, F1 is reported, which is a harmonic mean of\nprecision and recall.\n3.4\nAttention’s Dependency Alignment\nIn Section 5 we describe the examination of syntactic\nproperties of self-attention matrices. It can be evaluated\nusing Dependency Alignment [34] which sums the atten-\ntion weights at the positions corresponding to the pairs of\ntokens forming a dependency edge in the tree.\nDepAlA =\n∑(i,j)∈E Ai,j\n∑N\ni=1 ∑N\nj=1 Ai,j\n(4)\nDependency Accuracy [7, 15, 35] is an alternative met-\nric; for each dependency label it measures how often the\nrelation’s governor/dependent is the most attended token\nby the dependent/governor.\nDepAccl,d,A = |{(i, j) ∈El,d : j = argmaxAi,·}|\n|El,d|\n(5)\nNotation: E is a set of all dependency tree edges and El,d\nis a subset of the edges with the label l and with direction\nd, i.e., in dependent to governor direction the first element\nof the tuple i is dependent of the relation and the second\nelement j is the governor; A is a self-attention matrix and\nAi,· denotes ith row of the matrix; N is the sequence length.\n4\nMorphology and Syntax in Word\nEmbeddings and Latent Vectors\nIn this section, we summarize the research on the syntactic\ninformation captured by vector representations of words.\nWe devote a significant attention to POS tagging, which\nis a popular evaluation objective. Even though it is a mor-\nphological task, it is highly relevant to syntactic analysis.\n4.1\nSyntactic Analogies\nThe first wave of research on the vector representation\nof words focused on the statistical distribution of words\nacross distinct topics – Latent Semantic Analysis [8]. It\ncaptured statistical properties of words, yet there were no\npositive results in syntactic analogies retrieval nor encod-\ning syntax.\nGoogle Analogy Test Set was released together with a\npopular word embedding algorithm Word2Vec [23]. One\nof the exceptional properties of this method was its high\naccuracy in the analogy tasks. In particular, the best con-\nfiguration found the correct syntactic analogy in 68.9 % of\ncases.\nThe GloVe embeddings improved the results on syntac-\ntic analogies to 69.3% [28]. Much more significant im-\nprovement was reported for semantic analogies. They also\noutperform the variety of other vectorization methods.\nIn [24] a simple recurrent neural network was trained\nby language modeling objective. The word representation\nis taken from the input layer. The evaluation from [23]\nshows that Word2Vec performs better in syntactic anal-\nogy task. This observation is surprising because repre-\nsentations from RNN were proven effective in transfer to\nother syntactic tasks (we elaborate on that in Sections 4.2\nand 4.3). We think that possible explanations could be: 1.\nthe techniques of RNN training have crucially improved\nin recent years; 2. syntactic analogy focuses on particular\nwords, while for other syntactic tasks, the context is more\nimportant.\n4.2\nPart of Speech Tagging\nMeasuring to what extent a linguistic feature such as POS\nis captured in word representations is usually performed\nby the method called probing. In probing, the parameters\nof the pretrained network are fixed, the output word rep-\nresentations are computed as in the inference mode and\nthen fed to a simple neural layer. Only this simple layer is\noptimized for a new task.\nThe number of probing experiments rose with the ad-\nvent of multilayer 2 RNNs trained for language modeling\nand machine translation.\nBelinkov et al. [3] probe a recurrent neural machine\ntranslation (NMT) system with four layers to predict part\nof speech tags (along with morphological features). They\nuse Arabic, Hebrew, French, German, and Czech to En-\nglish pairs. They observe that adding a character-based\nrepresentation computed by a convolutional neural net-\nwork in addition to word-embedding input is beneficial,\nespecially for morphologically rich languages.\nIn a subsequent study [4], the source language of trans-\nlation now is English and the experiments are conducted\nsolely for this language. It is noted that the most mor-\nphosyntactic representation is usually obtained in the mid-\ndle layers of the network.\nThe influence of using a particular objective in pre-\ntraining RNN model is comprehensively analyzed by\nBlevins et al. [5]. They pre-train models on four objectives:\nsyntactic parsing, semantic role labeling, machine transla-\ntion, and language modeling. The two former objectives\nmay reveal morphosyntactic information to a larger extent\nthan other mentioned here settings. Particularly, the probe\nof RNN syntactic parser achieves near-perfect accuracy in\npart of speech tagging.\nThe introduction of ELMo [29] brought a remarkable\nadvancement in transfer learning from the RNN language\nmodel to a variety of other NLP tasks. The authors ex-\namined POS capabilities of the representations and com-\npared the results with the neural machine translation sys-\ntem CoVe [22], which also uses RNN architecture.\nZhang et al. [39] perform further experiments with\nCoVe and ELMo. They demonstrate that language model-\ning systems are better suited to capture morphology and\nsyntax in the hidden states than machine translation, if\ncomparable amounts of data are used to train both systems.\nMoreover, the corpora for language modeling are typically\nmore extensive than for machine translation, which can\nfurther improve the results.\nAnother comprehensive evaluation of morphological\nand syntactic capabilities of language models was con-\nducted by Liu et al. [17]. Probing was applied to a language\nmodel based on the Transformer architecture (BERT)\nand compared with ELMo and static word embeddings\n(Word2Vec). They observe that the hidden states of Trans-\nformer do not demonstrate a major increase in probed POS\naccuracy over the RNN model, even though it is more com-\nplex and consists of a larger number of parameters.\nPOS tag probing was also performed for languages other\nthan English. For instance, Musil [25] trains translation\nsystems (with RNN and Transformer architecture) from\nCzech to English and examines the learned input embed-\ndings of the model and compares them to a Word2Vec\nmodel trained on Czech.\n2Layer numbering in this work: We are numbering layers starting\nfrom one for the layer closest to the input. Please note that original papers\nmay use different numbering.\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nLanguage Model\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nMachine Translation\nBlevins et al. 2018 [5]\nPeters et al. 2018 [29]\nZhang and\nBowman 2018 [39]\n(a) Neural machine translation compared with language mod-\neling pre-training\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nAuto Encoder\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nMachine Translation\nBelinkov et al. 2017a [3]\nBelinkov et al. 2017b [4]\nZhang and\nBowman 2018 [39]\n(b) Neural machine translation compared with auto encoder\npre-training\nFigure 3: Accuracy of POS tag probing from RNN representation by the pre-training objective.\n80\n85\n90\n95\n100\nStatic Word Embeddings\n80\n85\n90\n95\n100\nRNN Representaion\nBelinkov et al. 2017b [4]\nBlevins et al. 2018 [5]\nMusil 2019 [25]\nLiu et al. 2019 [17]\nFigure 4: Accuracy of POS tag probing from RNN latent\nvectors compared with static word embeddings\nIn Figures 3 and 4, we present a comparison of different\nsettings for POS tag probing. Each point denotes a pair of\nresults obtained in the same paper and the same dataset,\nbut with different types of embeddings or pretraining ob-\njectives. Therefore, we can observe that the setting plotted\non the y-axis is better than the x-axis setting if the points\nare above identity function (red dashed line). We cannot\nsay whether a method represented by another point per-\nforms better, as the evaluation settings differ.\nFigure 4 clearly shows that the RNN contextualization\nhelps in part of speech tagging. As expected, the informa-\ntion about neighboring tokens is essential to predict mor-\nphosyntactic functions of words correctly. It is especially\ntrue for the homographs, which can have various part of\nspeech in different places in the text.\nThe influence of RNN’s pre-training task is presented\nin Figure 3. Machine translation captures much better POS\ninformation than auto-encoders, which can be interpreted\nas translation from and to the same language. It is likely\nthat the latter task is straightforward and therefore does\nnot require to encode morphosyntax in the latent space.\nThe difference between the results of machine translation\nand language modeling is small. Zhang et al. [39] show\nthat using a larger corpus for pre-training improves the\nPOS accuracy. The main advantage of language models is\nthat monolingual data is much easier to obtain than parallel\nsentences necessary to train a machine translation system.\n4.3\nSyntactic Structure Induction\nExtraction of dependency structure is more demanding be-\ncause instead of prediction for single tokens, every pair of\nwords need to be evaluated.\nBlevins et al. [5] propose a feed-forward layer on top\nof a frozen RNN representation to predict whether a de-\npendency tree edge connects a pair of tokens. They con-\ncatenate the vector representation of each of the words and\ntheir element-wise product. Such a representation is fed as\nan input to the binary classifier. It only looks on a pair of\ntokens at a time, therefore predicted edges may not form a\nvalid tree.\nAnother approach, induction of the whole syntactic\nstructures from latent representations was proposed by He-\nwitt and Manning [12]. Their syntactic probing is based on\ntraining a matrix which is used to transform the output of\nnetwork’s layers (they use BERT and ELMo). The objec-\ntive of the probing is to approximate dependency tree dis-\ntances between tokens 3 by the L2 norm of the difference\nof the transformed vectors. Probing produces the approx-\nimate syntactic pairwise distances for each pair of tokens.\nThe minimum spanning tree algorithm is used on the dis-\ntance matrix to find the undirected dependency tree. The\nbest configuration employs the 15th layer of BERT large\nand induces treebank with 82.5% UAS on Penn Treebank\nwith Stanford Dependency annotation (relation directions\nand punctuation were disregarded in the experiments). The\n3Tree distance is the length of the tree path between two tokens\nresult for BERT is significantly higher than for ELMo,\nwhich gave 77.0% when the first layer was probed.\nThe paper also describes an alternative method of ap-\nproximating the syntactic depth by the L2 norm of la-\ntent vector multiplied by a trainable matrix. The estimated\ndepths allow prediction of the root of a sentence with\n90.1% accuracy when representation from the 16th layer\nof BERT large is probed.\n4.4\nMultilingual Representations\nThe subsequent paper by Chi et al. [6] applies the set-\nting from [12] to the multilingual language model mBERT.\nThey train syntactic distance probes on 11 languages and\ncompare UAS of induced trees in four scenarios: 1. train-\ning and evaluating on the same languages; 2. training on\na single language, evaluating on a different one; 3. train-\ning on all languages except the evaluation one; 4. train-\ning on all languages, including the evaluation one. They\ndemonstrate that the transfer is effective as the results in all\nthe configurations outperform the baselines4. Even in the\nhardest case – zero-shot transfer from just one language,\nthe result is at least 6.9 percent points above the base-\nlines (for Chinese). Nevertheless, for all the languages, no\ntransfer-learning setting can beat the training and evaluat-\ning a probe on the same language.\nThe paper includes analysis of intrinsic features of the\nBERT’s vectors transformed by a probe. Noticeably, the\nvector differences between the representations of words\nconnected by dependency relation are clustered by relation\nlabels, see figure 5.\nMultilingual BERT embeddings are also analyzed by\nWang et al. [36]. They show that even for the multilingual\nvectors, the results can be improved by projecting vector\nspaces across languages. They use Biaffine Graph-based\nParser by Dozat and Manning [10], which consists of mul-\ntiple RNN layers. Therefore, the experiment is not strictly\ncomparable with probing as the most of syntactic informa-\ntion is captured by the parser, and not by the embeddings.\nThe article compares different types of vector representa-\ntions fed as an input to the parser. It is demonstrated that\ncross-lingual transformation on mBERT embedding im-\nproves the results significantly in LAS of parser trained\non English and evaluated on 14 languages (including En-\nglish); on average, from 60.53% to 63.54%. In compar-\nison to other cross-lingual representations, the proposed\nmethod outperforms transformed static embeddings (Fast-\nText with SVD) and also slightly outperforms contextual\nembeddings (XLM).\n5\nSyntax in Transformer’s Attention\nMatrices\nBesides the vector representations of individual tokens,\nthe Transformer architecture offers another representation\n4There are two baselines: right-branching tree and probing on ran-\ndomly initialized mBERT without pretraining\nFigure 5: Two dimensional t-SNE visualization of probed\nmBERT embeddings from [6]. Analysis of the clusters\nshows that embeddings encode information about the type\nof dependency relations and, to a lesser extent, language.\nwith a possible syntactic interpretation – the weights of the\nself-attention heads. In each head, information can flow\nfrom each token to any other one. These connections may\nbe easily analyzed and compared to syntactic relations pro-\nposed by linguists. In this section, we will summarize dif-\nferent approaches of extracting syntax from attention. We\npresent the methods both for dependency and constituency\nstructures.\n5.1\nDependency Trees\nRaganato and Tiedemann [31] induce dependency trees\nfrom self-attention matrices of a neural machine transla-\ntion encoder. They use the maximum spanning tree algo-\nrithm to connect pairs of tokens with high attention. Gold\nroot information is used to find the direction of the edges.\nTrees extracted in this way are generally worse than the\nright-branching baseline (35.08 % UAS on PUD) and out-\nperform it slightly in a few heads. The maximum UAS\nis obtained when a dependency structure is induced from\none head of the 5th layer of English to Chinese encoder\n- 38.87% UAS. Nevertheless, their approach assumes that\nthe whole syntactic tree may be induced from just one at-\ntention head.\nRecent articles focused on the analysis of features and\nclassification of Transformer’s self-attention heads. Vig\nand Belinkov [34] apply multiple metrics to examine prop-\nerties of attention matrices computed in a unidirectional\nlanguage model (GPT-2 [30]). They showed that in some\nheads, the attentions concentrate on tokens representing\nspecific POS tags and the pairs of tokens are more often\nattended one to another if an edge in the dependency tree\nResearch\nTransformer Model\nType of tree\nSyntactic\nevaluation\nEvaluation data\nPercentage\nof\nsyntactic\nheads\nRaganato and\nTiedemann 2019 [31]\nNMT Encoder\n(6 layers 8 heads)\nDependency\nTree induction\nPUD [27]\n0% - 8%5\nVig and Belinkov 2019\n[34]\nLM (GPT-2)\nDependency\nDependency\nAlignment\nWikipedia\n(automati-\ncally annotated)\n—\nClark et al. 2019 [7]\nLM (BERT)\nDependency\nDependency\nAccuracy,\nTree induction\nWSJ\nPenn\nTreebank\n[20]\n—\nVoita et al. 2019 [35]\nNMT Encoder\n(6 layers 8 heads)\nDependency\nDependency\nAccuracy\nWMT,\nOpenSubtitles\n[16]\n(both\nautomati-\ncally annotated)\n15% - 19%\nLimisiewicz et al. 2020\n[15]\nLMs\n(BERT, mBERT)\nDependency\nDependency\nAccuracy,\nTree induction\nPUD\n[27],\nEuroParl\n[14]\n(automatically\nannotated)\n46%\nMareˇcek and Rosa 2019\n[21]\nNMT Encoder\n(6 layers 16 heads)\nConstituency\nTree induction\nEuroParl [14] (automat-\nically annotated)\n19% - 33%\nKim et al. 2019 [13]\nLMs (BERT, GPT2,\nRoBERTa, XLNet)\nConstituency\nTree induction\nWSJ\nPenn\nTreebank\n[20], MNLI [37]\n—\nTable 1: Summary of syntactic properties observed in Transformer’s self-attention heads\nconnects them, i.e., dependency alignment is high. They\nobserve that the strongest dependency alignment occurs in\nthe middle layers of the model – 4th and 5th. They also\npoint that different dependency types (labels) are captured\nin different places of the model. Attention in upper lay-\ners aligns more with subject relations whereas in the lower\nlayer with modifying relations, such as auxiliaries, deter-\nminers, conjunctions, and expletives.\nVoita et al. [35] also observed alignment with depen-\ndency relations in the encoders of neural machine transla-\ntion systems from English to Russian, German, or French.\nThey have evaluated dependency accuracy for four depen-\ndency labels: noun subject, direct object, adjective mod-\nifier, and adverbial modifier. They separately address the\ncases where a verb attends to a dependent subject, and sub-\nject attends to governor verb. The heads with more than\n10% improvement over a positional baseline are identified\nas syntactic 6. Such heads are found in all encoder lay-\ners except the first one. In further experiments, the authors\npropose the algorithm to prune the heads from the model\nwith a minimal decrease in translation performance. Dur-\ning pruning, the share of syntactic heads rises from 17%\nin the original model to 40% when 75% heads are cut out,\nwhile a change in translation score is negligible. These\nresults support the claim that the model’s ability to cap-\nture syntax is essential to its performance in non-syntactic\ntasks.\nA similar evaluation of dependency accuracy for the\nBERT language model was conducted by Clark et al. [7].\n5A head is syntactic when the tree extracted from it surpasses the\nright-branching chain in terms of UAS. It is a strong baseline for syntactic\ntrees in English. Thus only a few heads are recognized as syntactic.\n6In the positional baseline, the most frequent offset is added to the in-\ndex of relation’s dependent/governor to find its governor/dependent, e.g.,\nfor adjective to noun relations the most frequent offset is +1 in English\nThey identify syntactic heads that significantly outperform\npositional baseline for the following labels: prepositional\nobject, determiner, direct object, possession modifier, aux-\niliary passive, clausal component, marker, phrasal verb\nparticle. The syntactic heads are found in the middle layers\n(4th to 8th). However, there is no single head that would\ncapture the information for all the relations.\nIn another experiment, Clark et al. [7] induce a depen-\ndency tree from attentions. Instead of extracting structure\nfrom each head [31] they use probing to find the weighted\naverage of all heads. The maximum spanning tree algo-\nrithm is used to induce the dependency structure from the\naverage. This approach produces trees with 61% UAS and\ncan be improved to 77% by making weights dependent on\nthe static word representation (fixed GloVe vectors). Both\nthe numbers are significantly higher than right branching\nbaseline 27%.\nA related analysis for English (BERT) and the multilin-\ngual variant (mBERT) was conducted by Limisiewicz et\nal. [15]. We have observed that the information about one\ndependency type is split across many self-attention heads\nand in other cases, the opposite happens - many heads have\nthe same syntactic function. They extract labeled depen-\ndency trees from the averaged heads and achieves 52%\nUAS and show that in the multilingual model (mBERT)\nspecific relation (noun subject, determines) are found in\nthe same heads across typologically similar languages.\n5.2\nConstituency trees\nThere are fewer papers devoted to deriving constituency\nsyntax tree structures.\nMareˇcek and Rosa [21] examined the encoder of the\nmachine translation system for translation between En-\nglish, French, and German. We observed that in some\nthereis\nconsiderable\nenergy\nsaving\npotentialin\npublic\nbuildings ,\nfor\nexample ,\nwhich\nwould\nfacilitatethe\ntransition\ntowards a\nstable ,\ngreen\neconomy .\nAMOD D2P\nX\nX\nX\nX\nX\nLAYER: 5 HEAD: 7\nthereis\nconsiderable\nenergy\nsaving\npotentialin\npublic\nbuildings ,\nfor\nexample ,\nwhich\nwould\nfacilitatethe\ntransition\ntowards a\nstable ,\ngreen\neconomy .\nthereis\nconsiderable\nenergy\nsaving\npotentialin\npublic\nbuildings ,\nfor\nexample ,\nwhich\nwould\nfacilitatethe\ntransition\ntowards a\nstable ,\ngreen\neconomy .\nOBJ D2P\nX\nX\nLAYER: 7 HEAD: 9\nFigure 6: Self-attentions in particular heads of a language\nmodel (BERT) aligns with dependency relation adjective\nmodifiers and objects. The gold relations are marked with\nXs.\nheads, stretches of words attend to the same token form-\ning shapes similar to balustrades (Figure 7). Furthermore,\nthose stretches usually overlap with syntactic phrases. This\nnotion is employed in the new method for constituency tree\ninduction. In their algorithm, the weights for each stretch\nof tokens are computed by summing the attention focused\non the balustrades and then inducing a constituency tree\nwith CKY algorithm [26]. As a result, we produce trees\nthat achieve up to 32.8% F1 score for English sentences,\n43.6% for German and 44.2% for French. 7 The results can\nbe improved by selecting syntactic heads and using only\nthem in the algorithm. This approach requires a sample of\n100 annotated sentences for head selection and raises F1\n7The evaluation was done on 1000 sentences for each language\nparsed with supervised Stanford Parsed\nhuge\nareas\ncovering\nthousands of\nhectaresof\nvineyards\nhave\nbeen\nburned ;\nthis\nmeans\nthat\nthe\nvine-\ngrowers\nhave\nsuffered\nloss\nand\nthat\ntheir\nplants\nhave\nbeen\ndamaged .\nhuge\nareas\ncovering\nthousands of\nhectaresof\nvineyards\nhave\nbeen\nburned ;\nthis\nmeans\nthat\nthe\nvine-\ngrowers\nhave\nsuffered\nloss\nand\nthat\ntheir\nplants\nhave\nbeen\ndamaged .\nLAYER: 4 HEAD: 13\nFigure 7: Balustrades observed in NMT’s encoder tend to\noverlap with syntactic phrases.\nby up to 8.10 percent points in English.\nThe extraction of constituency trees from language\nmodels was described by Kim et al. [13]. They present\na comprehensive study that covers nine types of pre-\ntrained networks: BERT (base, large), GPT-2 [30] (orig-\ninal, medium), RoBERTa [19] (base, large), XLNet [38]\n(base, large). Their approach is based on computing dis-\ntance between each pair of subsequent words. In each step,\nthey are branching the tree in the place where the distance\nis the highest. The authors try three distance measures on\nthe vector outputs of the encoder layer (cosine, L1, and L2\ndistances for pairs of vectors) and two distance measures\non the distributions of token’s attention (Jason-Shannon\nand Hellinger distances for pairs of distribution). In the\nformer case, distances are computed only per layer and in\nthe latter case for each head and average of heads in one\nlayer. The best setting achieves 40.1% F1 score on WSJ\nPenn Treebank. It uses XLNet-base and Helinger distance\non averaged attentions in the 7th layer. Generally, attention\ndistribution distances perform better than vector ones. Au-\nthors also observe that models trained on regular language\nmodeling objective (i.e., next word prediction in GPT, XL-\nNet) captured syntax better than masked language models\n(BERT, RoBERTa). In line with the previous research, the\nmiddle layers tend to be more syntactic.\n5.3\nSyntactic information across layers\nFigure 8 summarizes the evaluation of syntactic informa-\ntion across layers for different approaches. In Transformer-\nbased language models: BERT, mBERT, and GPT-2, the\nmiddle layers are the most syntactic. In neural machine\ntranslation models, the top layers of the encoder are the\nmost syntactic. However, it is important to note that the\nA) BERT large 1-12\nBERT large 13-24\nB) BERT base\nC) mBERT base\nD) GPT-2\nE) BERT base\nF) NMT en2cs\nen2de\nen2et\nen2fi\nen2ru\nen2tr\nen2zh\nG) ELMo\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12\nLayer number\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFigure 8: Relative syntactic information across attention models and layers. The values are normalized so that the best\nlayer for each method has 1.0. The methods A), B), C), and G) show undirected UAS trees extracted by probing the n-th\nlayer [6, 12]. The method D) shows the dependency alignment averaged across all heads in each layer [34]. The methods\nE) and F) show UAS of trees induced from attention heads by the maximum spanning tree algorithm [15, 31]. The results\nfor the best layer (corresponding to value 1.0 in the plot) are: A) 82.5; B) 79.8; C) 80.1; D) 22.3; E) 24.3; F) en2cs: 23.9,\nen2de: 20.9, en2et: 22.1, en2fi: 24.0, en2ru: 22.4, en2tr: 17.5, en2zh: 21.6; G) 77.0\nNMT Transformer encoder is only the first half of the\nwhole translation architecture, and therefore the most syn-\ntactic layers are, in fact, in the middle of the process. In\nRNN language model (ELMo) the first layer is more syn-\ntactic than the second one.\nWe conjecture that the initial Transformer’s layers cap-\nture simple relations (e.g., attending to next or previous\ntokens) and the last layers mostly capture task-specific in-\nformation. Therefore, they are less syntactic.\nWe also observe that in supervised probing [6, 12], bet-\nter results are obtained from initial and top layers than in\nunsupervised structure induction [15, 31], i.e., the distri-\nbution across layers is smoother.\n6\nConclusion\nIn this overview, we survey that syntactic structures are\nlatently learned by the neural models for natural language\nprocessing tasks. We have compared multiple approaches\nof others and described the features that affect the ability to\ncapture the syntax. The following aspects tend to improve\nthe performance on syntactic tasks such as POS tagging:\n1. Using\ncontextual\nembeddings\nfrom\nRNNs\nor\nTransformer outperforms static word embeddings\n(Word2Vec, GloVe).\n2. Pretraining on tasks with masked input (language\nmodeling or machine translation) produces better\nsyntactic representation than auto encoding.\n3. The advantage of language modeling over machine\ntranslation is the fact that larger corpora are available\nfor pretraining.\nOur meta-analysis of latent states showed that the most\nsyntactic representation could be found in the middle lay-\ners of the model. They tend to capture more complex re-\nlations than initial layers, and the representations are less\ndependent on the pretraining objectives than in the top lay-\ners.\nWe have shown to what extent systems trained for a non-\nsyntactic task can learn grammatical structures. The ques-\ntion we leave for further research is whether providing ex-\nplicit syntactic information to the model can improve its\nperformance on other NLP tasks.\nAcknowledgments\nThis work has been supported by the grant 18-02196S of\nthe Czech Science Foundation. It has been using language\nresources and tools developed, stored and distributed by\ntheLINDAT/CLARIAH-CZ project of the Ministry of Ed-\nucation, Youth and Sports of the Czech Republic (project\nLM2018101).\nReferences\n[1] Felipe Almeida and Geraldo Xexéo. Word embed-\ndings: A survey. CoRR, abs/1901.09069, 2019.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. Neural machine translation by jointly learn-\ning to align and translate.\nCoRR, abs/1409.0473,\n2015.\n[3] Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. What do neural ma-\nchine translation models learn about morphology? In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 861–872, Vancouver, Canada,\nJuly 2017. Association for Computational Linguis-\ntics.\n[4] Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad,\nNadir Durrani, Fahim Dalvi, and James Glass. Eval-\nuating layers of representation in neural machine\ntranslation on part-of-speech and semantic tagging\ntasks.\nIn Proceedings of the Eighth International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1–10, Taipei, Tai-\nwan, November 2017. Asian Federation of Natural\nLanguage Processing.\n[5] Terra Blevins, Omer Levy, and Luke Zettlemoyer.\nDeep RNNs encode soft hierarchical syntax.\nIn\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 14–19, Melbourne, Australia,\nJuly 2018. Association for Computational Linguis-\ntics.\n[6] Ethan A. Chi, John Hewitt, and Christopher D. Man-\nning.\nFinding universal grammatical relations in\nmultilingual BERT. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5564–5577, Online, July 2020.\nAssociation for Computational Linguistics.\n[7] Kevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. What does BERT look at?\nAn analysis of BERT’s attention, 2019.\n[8] Scott Deerwester, Susan T. Dumais, George W.\nFurnas, Thomas K. Landauer, and Richard Harsh-\nman.\nIndexing by latent semantic analysis.\nJour-\nnal of the American Society for Information Science,\n41(6):391–407, 1990.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. In\nNAACL-HLT, 2019.\n[10] Timothy Dozat and Christopher D. Manning. Deep\nbiaffine attention for neural dependency parsing. In\n5th International Conference on Learning Represen-\ntations, ICLR 2017, Toulon, France, April 24-26,\n2017, Conference Track Proceedings, 2017.\n[11] Zellig Harris.\nDistributional structure.\nWord,\n10(23):146–162, 1954.\n[12] John Hewitt and Christopher D. Manning. A struc-\ntural probe for finding syntax in word representa-\ntions. In NAACL-HLT, 2019.\n[13] Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-\ngoo Lee. Are Pre-trained Language Models Aware of\nPhrases? Simple but Strong Baselines for Grammar\nInduction. In International Conference on Learning\nRepresentations, January 2020.\n[14] Philipp Koehn. Europarl: A parallel corpus for sta-\ntistical machine translation. 5, 11 2004.\n[15] Tomasz Limisiewicz, Rudolf Rosa, and David\nMareˇcek.\nUniversal dependencies according to\nBERT: both more specific and more general. ArXiv,\nabs/2004.14620, 2020.\n[16] Pierre Lison, J¨org Tiedemann, and Milen Kouylekov.\nOpenSubtitles2018: Statistical rescoring of sentence\nalignments in large, noisy parallel corpora. In Pro-\nceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018),\nMiyazaki, Japan, May 2018. European Language Re-\nsources Association (ELRA).\n[17] Nelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. Linguistic\nknowledge and transferability of contextual represen-\ntations. In NAACL-HLT, 2019.\n[18] Qi Liu, Matt J. Kusner, and Phil Blunsom. A survey\non contextual embeddings. ArXiv, abs/2003.07278,\n2020.\n[19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[20] Mitchell\nP.\nMarcus,\nBeatrice\nSantorini,\nand\nMary Ann Marcinkiewicz.\nBuilding a large an-\nnotated corpus of English: The Penn Treebank.\nComputational Linguistics, 19(2):313–330, 1993.\n[21] David Mareˇcek and Rudolf Rosa. From balustrades\nto pierre vinken: Looking for syntax in transformer\nself-attentions.\nIn Proceedings of the 2019 ACL\nWorkshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 263–275, Florence,\nItaly, August 2019. Association for Computational\nLinguistics.\n[22] Bryan McCann, James Bradbury, Caiming Xiong,\nand Richard Socher. Learned in translation: Contex-\ntualized word vectors. In Advances in Neural Infor-\nmation Processing Systems, pages 6297–6308, 2017.\n[23] Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean.\nEfficient estimation of word represen-\ntations in vector space. CoRR, abs/1301.3781, July\n2013.\n[24] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\nLinguistic regularities in continuous space word rep-\nresentations.\nIn Proceedings of the 2013 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 746–751, Atlanta, Geor-\ngia, June 2013. Association for Computational Lin-\nguistics.\n[25] Tomáˇs Musil.\nExamining Structure of Word Em-\nbeddings with PCA. In Text, Speech, and Dialogue,\npages 211–223. Springer International Publishing,\n2019.\n[26] H. Ney. Dynamic programming parsing for context-\nfree grammars in continuous speech recognition.\nIEEE Transactions on Signal Processing, 39(2):336–\n340, 1991.\n[27] Joakim Nivre, ˇZeljko Agi´c, Lars Ahrenberg, Lene\nAntonsen, Maria Jesus Aranzabe, Masayuki Asa-\nhara, Luma Ateyah, Mohammed Attia, Aitziber\nAtutxa, Elena Badmaeva, Miguel Ballesteros, Esha\nBanerjee, Sebastian Bank, John Bauer, Kepa Ben-\ngoetxea, Riyaz Ahmad Bhat, Eckhard Bick, Cristina\nBosco, Gosse Bouma, Sam Bowman, Aljoscha Bur-\nchardt, Marie Candito, Gauthier Caron, G¨uls¸en Ce-\nbiro˘glu Eryi˘git, Giuseppe G. A. Celano, Savas Cetin,\nFabricio Chalub, Jinho Choi, Yongseok Cho, Sil-\nvie Cinková, C¸ a˘grı C¸ ¨oltekin, Miriam Connor, Marie-\nCatherine de Marneffe, Valeria de Paiva, Arantza\nDiaz de Ilarraza, Kaja Dobrovoljc, Timothy Dozat,\nKira Droganova, Marhaba Eli, Ali Elkahky, Tomaˇz\nErjavec, Richárd Farkas, Hector Fernandez Al-\ncalde, Jennifer Foster, Cláudia Freitas, Katarína\nGajdoˇsová, Daniel Galbraith, Marcos Garcia, Filip\nGinter, Iakes Goenaga, Koldo Gojenola, Memduh\nG¨okırmak, Yoav Goldberg, Xavier Gómez Guino-\nvart, Berta Gonzáles Saavedra, Matias Grioni, Nor-\nmunds Gr¯uz¯ıtis, Bruno Guillaume, Nizar Habash,\nJan Hajiˇc, Jan Hajiˇc jr., Linh Hà Mỹ, Kim Harris,\nDag Haug, Barbora Hladká, Jaroslava Hlaváˇcová,\nPetter Hohle, Radu Ion, Elena Irimia, Anders Jo-\nhannsen, Fredrik Jørgensen, H¨uner Kas¸ıkara, Hi-\nroshi Kanayama, Jenna Kanerva, Tolga Kayade-\nlen, Václava Kettnerová, Jesse Kirchner, Natalia\nKotsyba, Simon Krek, Sookyoung Kwak, Veronika\nLaippala,\nLorenzo\nLambertino,\nTatiana\nLando,\nPhương Lê H`ông, Alessandro Lenci, Saran Lert-\npradit, Herman Leung, Cheuk Ying Li, Josie Li,\nNikola Ljubeˇsi´c, Olga Loginova, Olga Lyashevskaya,\nTeresa Lynn, Vivien Macketanz, Aibek Makazhanov,\nMichael Mandl, Christopher Manning, Ruli Manu-\nrung, Cătălina Mărănduc, David Mareˇcek, Katrin\nMarheinecke, Héctor Martínez Alonso, André Mar-\ntins, Jan Maˇsek, Yuji Matsumoto, Ryan McDon-\nald, Gustavo Mendonc¸a, Anna Missil¨a, Verginica\nMititelu, Yusuke Miyao, Simonetta Montemagni,\nAmir More, Laura Moreno Romero, Shunsuke\nMori, Bohdan Moskalevskyi, Kadri Muischnek,\nNina Mustafina, Kaili M¨u¨urisep, Pinkey Nainwani,\nAnna Nedoluzhko, Lương Nguy˜ên Thị, Huy`ên\nNguy˜ên ThịMinh, Vitaly Nikolaev, Rattima Ni-\ntisaroj, Hanna Nurmi, Stina Ojala, Petya Osen-\nova, Lilja Øvrelid, Elena Pascual, Marco Pas-\nsarotti, Cenel-Augusto Perez, Guy Perrier, Slav\nPetrov, Jussi Piitulainen, Emily Pitler, Barbara Plank,\nMartin Popel, Lauma Pretkalnin¸a, Prokopis Proko-\npidis, Tiina Puolakainen, Sampo Pyysalo, Alexan-\ndre Rademaker, Livy Real, Siva Reddy, Georg Rehm,\nLarissa Rinaldi, Laura Rituma, Rudolf Rosa, Davide\nRovati, Shadi Saleh, Manuela Sanguinetti, Baiba\nSaul¯ıte, Yanin Sawanakunanon, Sebastian Schus-\nter, Djamé Seddah, Wolfgang Seeker, Mojgan Ser-\naji, Lena Shakurova, Mo Shen, Atsuko Shimada,\nMuh Shohibussirri, Natalia Silveira, Maria Simi,\nRadu Simionescu, Katalin Simkó, Mária ˇSimková,\nKiril Simov, Aaron Smith, Antonio Stella, Jana Str-\nnadová, Alane Suhr, Umut Sulubacak, Zsolt Szántó,\nDima Taji, Takaaki Tanaka, Trond Trosterud, Anna\nTrukhina, Reut Tsarfaty, Francis Tyers, Sumire Ue-\nmatsu, Zdeˇnka Ureˇsová, Larraitz Uria, Hans Uszko-\nreit, Gertjan van Noord, Viktor Varga, Veronika\nVincze, Jonathan North Washington, Zhuoran Yu,\nZdenˇek ˇZabokrtský, Daniel Zeman, and Hanzhi\nZhu.\nUniversal dependencies 2.0 – CoNLL 2017\nshared task development and test data, 2017. LIN-\nDAT/CLARIN digital library at the Institute of For-\nmal and Applied Linguistics (ÚFAL), Faculty of\nMathematics and Physics, Charles University.\n[28] Jeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. Glove: Global vectors for word\nrepresentation.\nIn Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532–1543,\n2014.\n[29] Matthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer.\nDeep contextualized word rep-\nresentations.\nIn Proceedings of the 2018 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), New\nOrleans, Louisiana, June 2018. Association for Com-\nputational Linguistics.\n[30] Alec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. 2019.\n[31] Alessandro Raganato and J¨org Tiedemann. An anal-\nysis of encoder representations in transformer-based\nmachine translation.\nIn Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 287–297,\nBrussels, Belgium, November 2018. Association for\nComputational Linguistics.\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin.\nAttention is all you\nneed. In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Informa-\ntion Processing Systems 2017, 4-9 December 2017,\nLong Beach, CA, USA, pages 5998–6008, 2017.\n[33] Jesse Vig.\nA multiscale visualization of attention\nin the transformer model.\nIn Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28 - August 2, 2019, Volume 3: System Demonstra-\ntions, pages 37–42. Association for Computational\nLinguistics, 2019.\n[34] Jesse Vig and Yonatan Belinkov.\nAnalyzing the\nStructure of Attention in a Transformer Language\nModel.\nIn Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 63–76, Florence, Italy,\nAugust 2019. Association for Computational Lin-\nguistics.\n[35] Elena Voita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov.\nAnalyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the\nrest can be pruned. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5797–5808, Florence, Italy, July\n2019. Association for Computational Linguistics.\n[36] Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu,\nand Ting Liu. Cross-lingual bert transformation for\nzero-shot dependency parsing.\nProceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019.\n[37] Adina Williams, Nikita Nangia, and Samuel Bow-\nman.\nA broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana, June 2018. Association for Computational\nLinguistics.\n[38] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V. Le.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS, 2019.\n[39] Kelly W. Zhang and Samuel R. Bowman. Language\nmodeling teaches you more syntax than translation\ndoes: Lessons learned through auxiliary task anal-\nysis.\nIn Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, November 2018.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-10-02",
  "updated": "2020-10-02"
}