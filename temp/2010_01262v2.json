{
  "id": "http://arxiv.org/abs/2010.01262v2",
  "title": "Integrating Categorical Semantics into Unsupervised Domain Translation",
  "authors": [
    "Samuel Lavoie",
    "Faruk Ahmed",
    "Aaron Courville"
  ],
  "abstract": "While unsupervised domain translation (UDT) has seen a lot of success\nrecently, we argue that mediating its translation via categorical semantic\nfeatures could broaden its applicability. In particular, we demonstrate that\ncategorical semantics improves the translation between perceptually different\ndomains sharing multiple object categories. We propose a method to learn, in an\nunsupervised manner, categorical semantic features (such as object labels) that\nare invariant of the source and target domains. We show that conditioning the\nstyle encoder of unsupervised domain translation methods on the learned\ncategorical semantics leads to a translation preserving the digits on\nMNIST$\\leftrightarrow$SVHN and to a more realistic stylization on\nSketches$\\to$Reals.",
  "text": "Published as a conference paper at ICLR 2021\nINTEGRATING CATEGORICAL SEMANTICS INTO UNSU-\nPERVISED DOMAIN TRANSLATION\nSamuel Lavoie∗, Faruk Ahmed & Aaron Courville†\nDépartement d’Informatique et de Recherche Opérationnelle\nUniversité de Montréal, Mila\nABSTRACT\nWhile unsupervised domain translation (UDT) has seen a lot of success recently, we\nargue that mediating its translation via categorical semantic features could broaden\nits applicability. In particular, we demonstrate that categorical semantics improves\nthe translation between perceptually different domains sharing multiple object\ncategories. We propose a method to learn, in an unsupervised manner, categorical\nsemantic features (such as object labels) that are invariant of the source and target\ndomains. We show that conditioning the style encoder of unsupervised domain\ntranslation methods on the learned categorical semantics leads to a translation\npreserving the digits on MNIST↔SVHN and to a more realistic stylization on\nSketches→Reals.1\n1\nINTRODUCTION\nDomain translation has sparked a lot of interest in the computer vision community following the\nwork of Isola et al. (2016) on image-to-image translation. This was done by learning a conditional\nGAN (Mirza & Osindero, 2014), in a supervised manner, using paired samples from the source and\ntarget domains. CycleGAN (Zhu et al., 2017a) considered the task of unpaired and unsupervised\nimage-to-image translation, showing that such a translation was possible by simply learning a mapping\nand its inverse under a cycle-consistency constraint, with GAN losses for each domain.\nBut, as has been noted, despite the cycle-consistency constraint, the proposed translation problem\nis fundamentally ill-posed and can consequently result in arbitrary mappings (Benaim et al., 2018;\nGalanti et al., 2018; de Bézenac et al., 2019). Nevertheless, CycleGAN and its derivatives have\nshown impressive empirical results on a variety of image translation tasks. Galanti et al. (2018) and\nde Bézenac et al. (2019) argue that CycleGAN’s success is owed, for the most part, to architectural\nchoices that induce implicit biases toward minimal complexity mappings. That being said, CycleGAN,\nand follow-up works on unsupervised domain translation, have commonly been applied on domains\nin which a translation entails little geometric changes and the style of the generated sample is\nindependent of the semantic content in the source sample. Commonly showcased examples include\ntranslating edges↔shoes and horses↔zebras.\nWhile these approaches are not without applications, we demonstrate two situations where unsu-\npervised domain translation methods are currently lacking. The ﬁrst one, which we call Semantic-\nPreserving Unsupervised Domain Translation (SPUDT), is deﬁned as translating, without supervision,\nbetween domains that share common semantic attributes. Such attributes may be a non-trivial compo-\nsition of features obfuscated by domain-dependent spurious features, making it hard for the current\nmethods to translate the samples while preserving the shared semantics despite the implicit bias.\nTranslating between MNIST↔SVHN is an example of translation where the shared semantics, the\ndigit identity, is obfuscated by many spurious features, such as colours and background distractors, in\nthe SVHN domains. In section 4.1, we take this speciﬁc example and demonstrate that using domain\ninvariant categorical semantics improves the digit preservation in UDT.\nThe second situation that we consider is Style-Heterogeneous Domain Translation (SHDT). SHDT\nrefers to a translation in which the target domain includes many semantic categories, with a distinct\n∗Correspondence to: samuel.lavoie.m@gmail.com.\n†CIFAR fellow\n1The public code can be found: https://github.com/lavoiems/Cats-UDT.\n1\narXiv:2010.01262v2  [cs.LG]  16 Mar 2021\nPublished as a conference paper at ICLR 2021\nstyle per semantic category. We demonstrate that, in this situation, the style encoder must be\nconditioned on the shared semantics to generate a style consistent with the semantics of the given\nsource image. In Section 4.2, we consider an example of this problem where we translate an ensemble\nof sketches, with different objects among them, to real images.\nIn this paper, we explore both the SPUDT and SHDT settings. In particular, we demonstrate\nhow domain invariant categorical semantics can improve translation in these settings. Existing\nworks (Hoffman et al., 2018; Bousmalis et al., 2017) have considered semi-supervised variants by\ntraining a classiﬁer with labels on the source domain. But, differently from them, we show that it\nis possible to perform well at both kinds of tasks without any supervision, simply with access to\nunlabelled samples from the two domains. This additional constraint may further enable applications\nof domain translation in situations where labelled data is absent or scarce.\nTo tackle these problems, we propose a method which we refer to as Categorical Semantics Unsuper-\nvised Domain Translation (CatS-UDT). CatS-UDT consists of two steps: (1) learning an inference\nmodel of the shared categorical semantics across the domains of interest without supervision and (2)\nusing a domain translation model in which we condition the style generation by inferring the learned\nsemantics of the source sample using the model learned at the previous step. We depict the ﬁrst step\nin Figure 1b and the second in Figure 2.\nMore speciﬁcally, the contributions of this work are the following:\n• Novel framework for learning invariant categorical semantics across domains (Section 3.1).\n• Introduction of a method of semantic style modulation to make SHDT generations more\nconsistent (Section 3.2).\n• Comparison with UDT baselines on SPUDT and SHDT highlighting their existing challenges\nand demonstrating the relevance of our incorporating semantics into UDT (Section 4).\n2\nRELATED WORKS\nDomain translation is concerned with translating samples from a source domain to a target domain.\nIn general, we categorize a translation that uses pairing or supervision through labels as supervised\ndomain translation and a translation that does not use pairing or labels as unsupervised domain\ntranslation.\nSupervised domain translation methods have generally achieved success through either the use of\npairing or the use of supervised labels. Methods that leverage the use of category labels include Taig-\nman et al. (2017); Hoffman et al. (2018); Bousmalis et al. (2017). The differences between these\napproaches lie in particular architectural choices and auxiliary objectives for training the translation\nnetwork. Alternatively, Isola et al. (2016); Gonzalez-Garcia et al. (2018); Wang et al. (2018; 2019);\nZhang et al. (2020) leverage paired samples as a signal to guide the translation. Also, some works\npropose to leverage a segmentation mask (Tomei et al., 2019; Roy et al., 2019; Mo et al., 2019).\nAnother strategy is to use the representation of a pre-trained network as semantic information (Ma\net al., 2019; Wang et al., 2019; Wu et al., 2019; Zhang et al., 2020). Such a representation typi-\ncally comes from the intermediate layer of a VGG (Liu & Deng, 2015) network pre-trained with\nlabelled ImageNET (Deng et al., 2009). Conversely to our work, (Murez et al., 2018) propose to use\nimage-to-image translation to regularize domain adaptation.\nUnsupervised domain translation considers the task of domain translation without any supervision,\nwhether through labels or pairing of images across domains. CycleGAN (Zhu et al., 2017a) proposed\nto learn a mapping and its inverse constrained with a cycle-consistency loss. The authors demonstrated\nthat CycleGAN works surprisingly well for some translation problems. Later works have improved\nthis class of models (Liu et al., 2017; Kim et al., 2017; Almahairi et al., 2018; Huang et al., 2018;\nChoi et al., 2017; 2019; Press et al., 2019), enabling multi-modal and more diverse generations.\nBut, as shown in Galanti et al. (2018), the success of these methods is mostly due to architectural\nconstraints and regularizers that implicitly bias the translation toward mappings with minimum\ncomplexity. We recognize the usefulness of this inductive bias for preserving low-level features like\nthe pose of the source image. This observation motivates the method proposed in Section 3.2 for\nconditioning the style using the semantics.\n2\nPublished as a conference paper at ICLR 2021\nSketches\nReals\n(a) ResNET-50 trained using MOCO.\nDomain 1\nDomain 2\nRepresentation learning\nClustering\nUnsupervised \nDomain adaptation\n(b) Domain invariant categorial representation learning.\nFigure 1: (a) T-SNE embeddings of the representation of Sketches and Reals taken from a hidden\nlayer for a pre-trained model on ImageNET, (b) Sketch of our method for learning a domain invariant\ncategorial semantics.\n3\nCATEGORICAL SEMANTICS UNSUPERVISED DOMAIN TRANSLATION\nIn this section, we present our two main technical contributions. First, we discuss an unsupervised\napproach for learning categorical semantics that is invariant across domains. Next, we incorporate the\nlearned categorical semantics into the domain translation pipeline by conditioning the style generation\non the learned categorical-code.\n3.1\nUNSUPERVISED LEARNING OF DOMAIN INVARIANT CATEGORICAL SEMANTICS\nThe framework for learning the domain invariant categorical representation, summarized in Figure 1b,\nis composed of three constituents: unsupervised representation learning, clustering and domain\nadaptation. First, embed the data of the source and target domains into a representation that lends\nitself to clustering. This step can be ignored if the raw data is already in a form that can easily be\nclustered. Second, cluster the embedding of one of the domains. Third, use the learned clusters as\nthe ground truth label in an unsupervised domain adaptation method. We provide a background of\neach of the constituents in Appendix A and concrete examples in Section 4. Here, we motivate their\nutilities and describe how they are used in the present framework.\nRepresentation learning. Pre-trained supervised representations have been used in many instances\nas a way to preserve alignment in domain translation (Ma et al., 2019; Wang et al., 2019). In\ncontrast to prior works that use models trained with supervision, we use models trained with self-\nsupervision (van den Oord et al., 2018a; Hjelm et al., 2019; He et al., 2020; Chen et al., 2020a).\nSelf-supervision deﬁnes objectives that depends only on the intrinsic information within data. This\nallows for the use of unlabelled data, which in turn could enable the applicability of domain translation\nto modalities or domains where labelled data is scarce. In this work, we consider the noise contrastive\nestimation (van den Oord et al., 2018b) which minimizes the distance in a normalized representation\nspace between an anchor sample and its transformation and maximizes the distance between the\nsame anchor sample and another sample in the data distribution. Formally, we learn the embedding\nfunction d : X →RD of samples x ∈X as follows:\narg min\nd −Exi∼X log\nexp(d(xi) · d(x′\ni)/τ)\nPK\nj=0 exp(d(xi) · d(xj)/τ)\n,\n(1)\nwhere τ > 0 is a hyper-parameter, xi is the anchor sample with its transformation x′\ni = t(xi) and\nt : X →X deﬁnes the set of transformations that we want our embedding space to be invariant to.\nWhile other works use the learned representation directly in the domain translation model, we propose\nto use it as a leverage to obtain a categorical and domain invariant embedding as described next. In\nsome instances, the data representation is already amenable to clustering. In those cases, this step of\nrepresentation learning can be ignored.\nClustering allows us to learn a categorical representation of our data without supervision. Some\nadvantages of using such a representation are as follows:\n• A categorical representation provides a way to select exemplars without supervision by\nsimply selecting an exemplar from the same categorical distribution of the source sample.\n3\nPublished as a conference paper at ICLR 2021\n2\ng\ng\ng:\nx0\n̂x1\nx0\nx1\n̂x1\n0\n1\n0\n0\n1\n:   Spatial encoder (domain 0)\ne0\n:   Mapping network (domain 1)\nf1\ne0\ne0\nf1\ns1\nConditioning style on x1\nConditioning style on z\n:  Style encode (domain 1) \n:  Generated sample (domain 1)\ns1\n̂x1\nFigure 2: Our proposed adaptation to the image-to-image framework for CatS-UDT. Left: generate\nthe style using a mapping network conditioned on both noise z ∼N(0, 1) and the semantics of the\nsource sample h(x0). Right: infer style of an exemplar x2 using a style encoder and h(x0).\n• The representation is straightforward to evaluate and to interpret. Samples with the same\nsemantic attributes should have the same cluster.\nIn practice, we cluster one domain because, as we see in Figure 1a, the continuous embedding of each\ndomain obtained from a learned model may be disjoint when they are sufﬁciently different. Therefore,\na clustering algorithm would segregate each domain into its own clusters. Also, the domain used to\ndetermine the initial clusters is important as some domains may be more amenable to clustering than\nothers. Deciding which domain to cluster depends on the data and the choice should be made after\nevaluation of the clusters or inspection of the data.\nMore formally, consider X0 ⊂RN be the domain chosen to be clustered. Assume a given embedding\nfunction d : X0 →RD that can be learned using self-supervision. If X0 is already cluster-able, d can\nbe the identity function. Let c : RD →C be a mapping from the embedding of X0 to the space of\nclusters C. We propose to cluster the embedding representation of the data:\narg min\nc\nC(c, d(X0)),\n(2)\nwhere C is a clustering objective. The framework is agnostic to the clustering algorithm used. In our\nexperiments (Section 4), we considered IMSAT (Hu et al., 2017) for clustering MNIST and Spectral\nClustering (Donath & Hoffman, 1973) for clustering the learned embedding of our real images. We\ngive a more thorough background of IMSAT in Appendix B.3 and refer to Luxburg (2007) for a\nbackground of Spectral clustering.\nUnsupervised domain adaptation. Given clusters learned using samples from a domain X0, it is\nunlikely that such clusters will generalize to samples from a different domain with a considerable\nshift. This can be observed in Figure 1a where, if we clustered the samples of the real images, it is\nnot clear that the samples from the Sketches domain would semantically cluster as we expect. That is,\nsamples with the same semantic category may not be grouped in the same cluster.\nUnsupervised domain adaptation (Ben-David et al., 2010) tries to solve this problem where one has\none supervised domain. However, rather than using labels obtained through supervision from a source\ndomain, we propose to use the learned clusters as ground-truth labels on the source domain. This\nmodiﬁcation allows us to adapt and make the clusters learned on one domain invariant to the other\ndomain.\nMore formally, given two spaces X0 ∈RN, X1 ∈RN representing the data space of domains\n0 and 1 respectively, given a C-way one-hot mapping of the embedding of domain 0 to clusters,\nc : d(X0) →C (C ⊂{0, 1}C), we propose to learn an adapted clustering h : X0 ∪X1 →C. We do\nso by optimizing:\narg min\nh −Ex0∼X0c(d(x0)) log h(x0) + Ω(h, X0, X1).\n(3)\nΩrepresents the regularizers used in unsupervised domain adaptation. The framework is also agnostic\nto the regularizers used in practice. In our experiments, the regularizers comprised of gradient\nreversal (Ganin et al., 2016), VADA (Shu et al., 2018) and VMT (Mao et al., 2019). We describe\nthose regularizers in more detail in Appendix B.5.\n4\nPublished as a conference paper at ICLR 2021\n3.2\nCONDITIONING THE STYLE ENCODER OF UNSUPERVISED DOMAIN TRANSLATION\nRecent methods for unsupervised image-to-image translation have two particular assets: (1) they\ncan work with few training examples, and (2) they can preserve spatial coherence such as pose.\nWith that in mind, our proposition to incorporate semantics into UDT, as depicted in ﬁgure 2, is to\nincorporate semantic-conditioning into the style inference of a domain translation framework. We\nwill consider that the semantics is given by a network (h in Figure 2). The rationale behind this\nproposition originates from the conclusions by Galanti et al. (2018); de Bézenac et al. (2019) that the\nunsupervised domain translation methods work due to an inductive bias toward minimum complexity\nmappings. By conditioning only the style encoder on the semantics, we preserve the same inductive\nbias in the spatial encoder, forcing the generated sample to preserve some spatial attributes of the\nsource sample, such as pose, while conditioning its style on the semantics of the source sample. In\npractice, we can learn the domain invariant categorical semantics, without supervision, using the\nmethod described in the previous subsection.\nThere can be multiple ways for incorporating the style into the translation framework. In this work, we\nfollow an approach similar to the one used in StyleGAN (Karras et al., 2019) and StarGAN-V2 (Choi\net al., 2019). We incorporate the style, conditioned on the semantics, by modulating the latent feature\nmaps of the generator using an Adaptive Instance Norm (AdaIN) module (Huang & Belongie, 2017).\nNext, we describe each network used in our domain translation model and the training of the domain\ntranslation network.\n3.2.1\nNETWORKS AND THEIR FUNCTIONS\nContent encoders, denoted e, extract the spatial content of an image. It does so by encoding an\nimage, down-sampling it to a representation of resolution smaller or equal than the initial image, but\ngreater than one to preserve spatial coherence.\nSemantics encoder, denoted h, extracts semantic information deﬁned as a categorical label. In our\nexperiments, the semantics encoder is a pre-trained network.\nMapping networks, denoted f, encode z ∼N(0, 1) and the semantics of the source image to a\nvector representing the style. This vector is used to condition the AdaIN module used in the generator\nwhich modulates the style of the target image.\nStyle encoders, denoted s, extract the style of an exemplar image in the target domain. This style is\nthen used to modulate the feature maps of the generator using AdaIN.\nGenerator, denoted g, generates an image in the target domain given the content and the style. The\ngenerator upsamples the content, injecting the style by modulating each layer using an AdaIN module.\n3.2.2\nTRAINING\nLet x0 ∼Px0 and x1 ∼Px1 be samples from two probability distributions on the spaces of our\ntwo domains of interest. Let z ∼N(0, 1) samples from a Gaussian distribution. Let y ∼B(0.5)\ndeﬁnes the domain, sampled from a Bernoulli distribution, and its inverse ¯y := 1 −y. We deﬁne the\nfollowing objectives for samples generated with the mapping networks f and the style encoder s:\nAdversarial loss (Goodfellow et al., 2014). Constrain the translation network to generate samples in\ndistribution to the domains. Consider d·\n· the discriminators 2.\nLf\nadv := Ey\nh\nEx¯y log df\n¯y(x¯y) + ExyEz log(1 −df\n¯y(g(ey(xy), f¯y(h(xy), z))))\ni\n,\nLs\nadv := Ey\nh\nEx¯y log ds\n¯y(x¯y) + ExyEx¯y∼Px¯y|h(xy) log(1 −ds\n¯y(g(ey(xy), s¯y(h(xy), x¯y))))\ni\n.\n(4)\nCycle-consistency loss (Zhu et al., 2017a). Regularizes the content encoder and the generator by\nenforcing the translation network to reconstruct the source sample.\nLf\ncyc := Ey\n\u0002\nExyEz |xy −g(e¯y(g(ey(xy), f¯y(h(x1), z))), sy(h(xy), xy))|1\n\u0003\n,\nLs\ncyc := Ey\nh\nExyEx¯y∼Px¯y|h(xy) |xy −g(e¯y(g(ey(xy), s¯y(h(x1), x¯y))), sy(h(xy), xy))|1\ni\n.\n(5)\n2Different of d, the embedding function, that we introduced in the previous subsection.\n5\nPublished as a conference paper at ICLR 2021\nStyle-consistency loss (Almahairi et al., 2018; Huang et al., 2018). Regularizes the translation\nnetworks to use the style code.\nLf\nsty := Ey\n\u0002\nExyEz |f¯y(h(xy), z) −s¯y(h(xy), g(ey(xy), f¯y(h(xy), z)))|1\n\u0003\n,\nLs\nsty := Ey\nh\nExyEx¯y∼Px¯y|h(xy) |s¯y(h(xy), x¯y) −s¯y(h(xy), g(ey(xy), s¯y(h(xy), x¯y)))|1\ni\n.\n(6)\nStyle diversity loss (Yang et al., 2019; Choi et al., 2017). Regularizes the translation network to\nproduce diverse samples.\nLf\nsd := −Ey\n\u0002\nExyEz,z′ |g(ey(xy), f¯y(h(xy), z)) −g(ey(xy), f¯y(h(xy), z′))|1\n\u0003\n,\nLs\nsd := −Ey[ExyEx¯y,x′\n¯y∼Px¯y|h(xy)\n\f\fg(ey(xy, s¯y(h(xy), x¯y))) −g(ey(xy, s¯y(h(xy), x′\n¯y)))\n\f\f\n1] (7)\nSemantic loss. We introduce the following semantic loss as the cross-entropy between the semantic\ncode of the source samples and that of their corresponding generated samples. We use this loss to\nregularise the generation to be semantically coherent with the source input.\nLf\nsem := −Ey\n\u0002\nExy,z[h(xy) log(h(g(ey(xy), f¯y(h(xy), z))))]\n\u0003\n,\nLs\nsem := −Ey\nh\nExyEx¯y∼Px¯y|h(xy)[h(xy) log(h(g(ey(xy), s¯y(h(xy), x¯y))))]\ni\n.\n(8)\nFinally, we combine all our losses and solve the following optimization.\narg\nmin\ng,e·,f·,s· arg max\nd··\nLs\nadv + Lf\nadv + λsty(Ls\nsty + Lf\nsty) + λcyc(Ls\ncyc + Lf\ncyc)+\nλsd(Ls\nsd + Lf\nsd) + λsem(Ls\nsem + Lf\nsem),\n(9)\nwhere λsty > 0, λcyc > 0, λsd > 0 and λsem > 0 are hyper-parameters deﬁned as the weight of each\nlosses.\n4\nEXPERIMENTS\nWe compare CatS-UDT with other unsupervised domain translation methods and demonstrate that it\nshows signiﬁcant improvements on the SPUDT and SHDT problems. We then perform ablation and\ncomparative studies to investigate the cause of the improvements on both setups. We demonstrate\nSPUDT using the MNIST (LeCun & Cortes, 2010) and SVHN (Netzer et al., 2011) datasets and\nSHDT using Sketches and Reals samples from the DomainNet dataset (Peng et al., 2019). We present\nthe datasets in more detail and the baselines in Appendix B.1 and Appendix B.2 respectively.\n4.1\nSPUDT WITH MNIST↔SVHN\nAdapted clustering. We ﬁrst cluster MNIST using IMSAT (Hu et al., 2017). We reproduce the\naccuracy of 98.24%. Using the learned clusters as ground-truth labels for MNIST, we adapt the\nclusters using the VMT (Mao et al., 2019) framework for unsupervised domain adaptation. This\ntrained classiﬁer achieves an accuracy of 98.20% on MNIST and 88.0% on SVHN. See Appendix B.3\nand Appendix B.5 for more details on the methods used.\nEvaluation. We consider two evaluation metrics for SPUDT. (1) Domain translation accuracy, to\nindicate the proportion of generated samples that have the same semantic category as the source\nTable 1: Comparison with the baselines. Domain translation accuracy and FID obtained on MNIST\n(M) ↔SVHN (S) for the different methods considered. The last column is the test classiﬁcation\naccuracy of the classiﬁer used to compute the metric. *: Using weak supervision.\nData\nCycleGAN MUNIT\nDRIT\nStargan-V2 EGSC-IT* CatS-UDT Target\nAcc\nM→S\n10.89\n10.44\n13.11\n28.26\n47.72\n95.63\n98.0\nS→M\n11.27\n10.12\n9.54\n11.58\n16.92\n76.49\n99.6\nFID\nM→S\n46.3\n55.15\n127.87\n66.54\n72.43\n39.72\n-\nS→M\n24.8\n30.34\n20.98\n26.27\n19.45\n6.60\n-\n6\nPublished as a conference paper at ICLR 2021\nsem = 0\nsty = 0\ncyc = 0\nsd = 0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nAccuracy\nmnist\nsvhn\nsvhn\nmnist\n(a) Ablation study.\nVGG\nMoCo\nNo-adapt\nAdapt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nAccuracy\nmnist\nsvhn\nsvhn\nmnist\n(b) Training of semantic encoder.\n10\n2\n10\n1\n100\n101\nsem\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nAccuracy\nmnist\nsvhn\nsvhn\nmnist\n(c) Varying λsem.\nFigure 3: Studies on the effect on the translation accuracy on MNIST↔SVHN of (a) Ablating each\nloss by setting their λ = 0. (b) Using VGG, MoCO, the presented method for learning categorical\nsemantics without adaptation and with adaptation respectively to train a semantic encoder. (c) Varying\nλsem.\nsamples. To compute this metric, we ﬁrst trained classiﬁers on the target domains. The classiﬁers\nobtain an accuracy of 99.6% and 98.0% on the test set of MNIST and SVHN respectively – as\nreported in the last column of Table 1. (2) FID (Heusel et al., 2017) to evaluate the generation quality.\nComparison with the baselines. In Table 1, we show the test accuracies obtained on the baselines\nas well as with CatS-UDT. We ﬁnd that all of the UDT baselines perform poorly, demonstrating the\nissue of translating samples through a large domain-shift without supervision. However, we do note\nthat StarGAN-V2 obtains slightly higher than chance numbers for MNIST→SVHN. We attribute this\nto a stronger implicit bias toward identity. EGSC-IT, which uses supervised labels, shows better than\nchance results on both MNIST→SVHN and SVHN→MNIST, but not better than CatS-UDT.\nAblation study – the effect of the losses In Figure 3a, we evaluate the effect of removing each of\nthe losses, by setting their λ = 0, on the translation accuracy. We observe that the semantic loss\nprovides the biggest improvement. We run the same analysis for the FID in Appendix C.2 and ﬁnd the\nsame trend. The integration of the semantic loss, therefore, improves the preservation of semantics in\ndomain translation and also improves the generation quality. We also inspect more closely λsem and\nevaluate the effect of varying it in Figure 3c. We observe a point of diminishing returns, especially\nfor SVHN→MNIST. We observe that the reason for this diminishing return is that for a λsem that is\ntoo high, the generated samples resemble a mixture of the source and the target domains, rendering\nthe samples out of the distribution in comparison to the samples used to train the classiﬁer used for\nthe evaluation. We demonstrate this effect and discuss it in more detail in Appendix C.2 and show the\nsame diminishing returns for the FID.\nComparative study – the effect of the semantic encoder. In Figure 3b, we evaluate the effect\nof using a semantic encoder trained using a VGG (Liu & Deng, 2015) on classiﬁcation, using a\nResNet50 on MoCo (He et al., 2020), to cluster MNIST but not adapted to SVHN and to cluster\nMNIST with adaptation to SVHN. We observe that the use of an adapted semantic network improves\nthe accuracy over its non-adapted counterpart. In Appendix C.2 we present the same plot for the FID.\nWe also observe that the FID degrades when using a non-adapted semantic encoder. Overall, this\ndemonstrates the importance of adapting the network inferring the semantics, especially when the\ndomains are sufﬁciently different.\nCycleGAN\nDRIT\nEGSC-IT\nStarGAN-v2\nCatS-UDT (ours)\nFigure 4: Comparison with baselines. Comparing the baselines with our approach for translating\nsketches to real images. For each sketch (top row), we sample 5 different styles generating 5 images\nin the target domain. For CycleGAN, we copy the generated images 5 times because it is impossible\nto generate multiple samples in the target domain from the same source image.\n7\nPublished as a conference paper at ICLR 2021\nTable 2: Comparison with the baselines. Comparing the FID obtained on Sketch→Real for the\nbaselines and our method. We compute the FID per class and over all the categories.\nDATA\nCYCLEGAN\nDRIT\nEGSC-IT\nSTARGAN-V2\nCATS-UDT (OURS)\nBIRD\n124.10\n141.18\n101.09\n93.58\n92.69\nDOG\n170.12\n153.05\n145.18\n108.62\n105.59\nFLOWER\n242.84\n223.63\n225.24\n209.91\n137.01\nSPEEDBOAT\n189.20\n239.94\n174.78\n127.23\n126.18\nTIGER\n156.54\n245.73\n109.97\n69.08\n41.77\nALL\n102.37\n128.45\n86.86\n65.00\n58.69\n4.2\nSHDT WITH SKETCHES→REALS\nAdapted clustering. The representations of the real images were obtained by using MoCo-V2 – a\nself-supervised model – pre-trained on unlabelled ImageNet. We clustered the learned representation\nusing spectral clustering (Donath & Hoffman, 1973; Luxburg, 2007), yielding 92.13% clustering\naccuracy on our test set of real images. Using the learned cluster as labels for the real images, we\nadapted our clustering to the sketches by using a domain adaptation framework – VMT (Mao et al.,\n2019) – on the representation of the sketches and the reals. This process yields an accuracy of 75.47%\non the test set of sketches and 90.32% on the test set of real images. More details are presented in\nAppendix B.4 and in Appendix B.5.\nEvaluation. For the Sketch→Real experiments, we evaluate the quality of the generations by\ncomputing the FID over each class individually as well as over all the classes. We do the former\nbecause the translation network may generate realistic images that are semantically unrelated to the\nsketch translated.\nComparison with baselines. We depict the issue with the UDT baselines in Figure 4. For DRIT\nand StarGAN-V2, the style is independent of the source image. CycleGAN does not have this issue\nbecause it does not sample a style. However, the samples are not visually appealing. The images\ngenerated with EGSC-IT are dependent on the source, but the style is not realistic for all the source\ncategories. We quantify the difference in sample quality in Table 2 where we present the FIDs.\nAblation study – the effect of the losses. In Table 3a, we evaluate the effect of setting each of\nremoving each of the losses, by setting their λ = 0, on the FIDs on Sketches→Reals. As in SPUDT,\nthe semantic loss plays an important role. In this case, the semantic loss encourages the network to\nuse the semantic information. This can be visualized in Appendix C.3 where we plot the translation.\nWe see that λsem = 0 suffers from the same problem that the baselines suffered, that is that the style\nis not relevant to the semantic of the source sample.\nComparative study – the effect of the methods to condition semantics. We compare different\nmethods of using semantic information in a translation network, in Table 3b. None refers to the case\nwhere the semantics is not explicitly used in the translation network, but a semantic loss is still used.\nThis method is commonly used in supervised domain translation methods such as Bousmalis et al.\nTable 3: Studies on the effect of the translation accuracy on Sketches→Reals on (a) Ablating each loss\nby setting their coefﬁcient λ = 0 . (b) Methods to condition the translation network on the semantics:\nNot conditioning, conditioning the content representation with categorical semantics, conditioning\nthe content representation with VGG, and conditioning the style with categorical semantics.\n(a) Ablation study of the losses\nData\nλsem = 0 λsty = 0 λcyc = 0 λSD = 0\nBird\n148.32\n94.18\n108.68\n101.97\nDog\n131.35\n109.50\n120.39\n106.24\nFlower\n211.84\n124.37\n160.97\n154.77\nSpeedboat 185.11\n97.52\n127.68\n99.67\nTiger\n153.03\n39.24\n52.64\n41.55\nAll\n69.19\n53.43\n67.88\n58.47\n(b) Method to condition on the semantics.\nData\nNone Content Content(VGG) Style\nBird\n101.88 405.29\n129.69\n92.69\nDog\n142.79 343.62\n229.18\n105.59\nFlower\n196.70 323.52\n220.72\n137.01\nSpeedboat 160.57 280.47\n192.38\n126.18\nTiger\n57.29\n212.69\n228.84\n41.77\nAll\n81.69\n275.21\n112.10\n58.59\n8\nPublished as a conference paper at ICLR 2021\n(2017); Hoffman et al. (2018); Tomei et al. (2019). Content refers to the case where we use categorical\nsemantics, inferred using our method, to condition the content representation. Similarly, we also\nconsider the method used in Ma et al. (2019), in which the semantics comes from a VGG encoder\ntrained with classiﬁcation. We label this method Content(VGG). For these two methods, we learn a\nmapping from the semantic representation vector to a feature-map of the same shape as the content\nrepresentation and then multiply them element-wise – as done in EGSC-IT. Style refers the presented\nmethod to modulate the style. First, for None, the network generates only one style per semantic class.\nWe believe that the reason is that the semantic loss penalizes the network for generating samples\nthat are outside of the semantic class, but the translation network is agnostic of the semantic of the\nsource sample. Second, for Content, the network fails to generate sensible samples. The samples are\nreminiscent of what happens when the content representation is of small spatial dimensionality. This\nfailure does not happen for Content(VGG). Therefore, from the empirical results, we conjecture that\nthe failure case is due to a large discrepancy between the content representation and the categorical\nrepresentation in addition to a pressure from the semantic loss. The semantic loss forces the network\nto use the semantic incorporated in the content representation, thereby breaking the spatial structure.\nThis demonstrates that our method allows us to incorporate the semantics category of the source\nsample without affecting the inductive bias toward the identity, in this setup.\n5\nCONCLUSION AND DISCUSSION\nWe discussed two situations where the current methods for UDT are found to be lacking - Semantic\nPreserving Unsupervised Domain Translation and Style Heterogeneous Domain Translation. To\ntackle these issues, we presented a method for learning domain invariant categorical semantics without\nsupervision. We demonstrated that incorporating domain invariant categorical semantics greatly\nimproves the performance of UDT in these two situations. We also proposed to condition the style on\nthe semantics of the source sample and showed that this method is beneﬁcial for generating a style\nrelated to the semantic category of the source sample in SHDT, as demonstrated in Sketches→Reals.\nWhile we demonstrated that using domain invariant categorical semantics improves the translation\nin the SPUDT and SHDT settings, we re-iterate that the quality of the network used to infer the\nsemantics is important. We observe an example of the effect of mis-clustering the source sample in\nFigure 12e third column: the tiger is incorrectly translated into a dog. This failure may potentially\ntranslate to any application, even outside UDT, using learned categorical semantics. Efforts on robust\nmachine learning and detections of failures are also important in this setup for countering this failure.\nACKNOWLEDGMENTS\nWe would like to acknowledge Devon Hjelm, Sébastien Lachapelle, Jacob Leygoni and Amjad\nAlmahairi for the helpful discussions. We also acknowledge Compute-Canada and Mila for providing\nthe computing ressources used for this work. This work would not have been possible without the\ndevelopment of the open-source softwares that we used. This includes: Python, Pytorch (Paszke\net al., 2019), Numpy (Harris et al., 2020) and Matplotlib (Hunter, 2007). We acknowledge ﬁnancial\nsupport of Hitachi, Samsung, CIFAR and the Natural Sciences and Engineering Research Council of\nCanada (NSERC Discovery Grant).\nREFERENCES\nAmjad Almahairi, Sai Rajeshwar, Alessandro Sordoni, Philip Bachman, and Aaron Courville. Aug-\nmented CycleGAN: Learning many-to-many mappings from unpaired data. In Jennifer Dy and\nAndreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,\nvolume 80 of Proceedings of Machine Learning Research, Stockholmsmässan, Stockholm Sweden,\n10–15 Jul 2018. PMLR.\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. A theory of learning from different domains. Machine Learning, 79(1), May 2010.\nSagie Benaim, Tomer Galanti, and Lior Wolf. Estimating the success of unsupervised image to\nimage translation. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss\n9\nPublished as a conference paper at ICLR 2021\n(eds.), Computer Vision – ECCV 2018, Cham, 2018. Springer International Publishing. ISBN\n978-3-030-01228-1.\nK. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain\nadaptation with generative adversarial networks. In 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 95–104, 2017.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsuper-\nvised learning of visual features. In European Conference on Computer Vision, 2018.\nO. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Artiﬁcial\nIntelligence and Statistics. Max-Planck-Gesellschaft, January 2005.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\ncontrastive learning of visual representations. ArXiv, abs/2002.05709, 2020a.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\nYunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan:\nUniﬁed generative adversarial networks for multi-domain image-to-image translation. CoRR,\nabs/1711.09020, 2017.\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for\nmultiple domains. CoRR, abs/1912.01865, 2019.\nEmmanuel de Bézenac, Ibrahim Ayed, and Patrick Gallinari. Optimal unsupervised domain translation.\narXiv preprint arXiv:1906.01292, 2019.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09, 2009.\nW. E. Donath and A. J. Hoffman. Lower bounds for the partitioning of graphs. IBM Journal of\nResearch and Development, 17(5):420–425, 1973.\nTomer Galanti, Lior Wolf, and Sagie Benaim. The role of minimal complexity functions in unsuper-\nvised learning of semantic mappings. In International Conference on Learning Representations,\n2018.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François\nLaviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.\nThe Journal of Machine Learning Research, 17(1), 2016.\nRyan Gomes, Andreas Krause, and Pietro Perona. Discriminative clustering by regularized informa-\ntion maximization. In Neural Information Processing Systems, 2010.\nAbel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Bengio. Image-to-image translation for cross-\ndomain disentanglement. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 1287–1298.\nCurran Associates, Inc., 2018.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Neural Information Process-\ning Systems, 2014.\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. K.\nSaul, Y. Weiss, and L. Bottou (eds.), Neural Information Processing Systems. MIT Press, 2005.\nCharles R. Harris, K. Jarrod Millman, St’efan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\nPicus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern’andez\ndel R’ıo, Mark Wiebe, Pearu Peterson, Pierre G’erard-Marchant, Kevin Sheppard, Tyler Reddy,\nWarren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming\nwith NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2.\n10\nPublished as a conference paper at ICLR 2021\nK. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual repre-\nsentation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 9726–9735, 2020.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in\nNeural Information Processing Systems 30, pp. 6626–6637. Curran Associates, Inc., 2017.\nDevon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\nand maximization. In ICLR 2019. ICLR, April 2019.\nJudy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros,\nand Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In Jennifer G. Dy\nand Andreas Krause (eds.), ICML, volume 80, pp. 1994–2003. PMLR, 2018.\nWeihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning\ndiscrete representations via information maximizing self-augmented training. In International\nConference on Machine Learning, 2017.\nXun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normal-\nization. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.\nXun Huang, Ming-Yu Liu, Serge J. Belongie, and Jan Kautz. Multimodal unsupervised image-to-\nimage translation. In ECCV, 2018.\nJ. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9(3):\n90–95, 2007. doi: 10.1109/MCSE.2007.55.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with con-\nditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016.\nT. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial\nnetworks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 4396–4405, 2019.\nTaeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover\ncross-domain relations with generative adversarial networks. In Doina Precup and Yee Whye\nTeh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of\nProceedings of Machine Learning Research, International Convention Centre, Sydney, Australia,\n06–11 Aug 2017. PMLR.\nYann LeCun and Corinna Cortes. MNIST handwritten digit database, 2010.\nHsin-Ying Lee, Hung-Yu Tseng, Qi Mao, Jia-Bin Huang, Yu-Ding Lu, Maneesh Kumar Singh, and\nMing-Hsuan Yang. Drit++: Diverse image-to-image translation viadisentangled representations.\narXiv preprint arXiv:1905.01270, 2019.\nMing-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks.\nIn I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(eds.), Advances in Neural Information Processing Systems 30, pp. 700–708. Curran Associates,\nInc., 2017.\nS. Liu and W. Deng. Very deep convolutional neural network based image classiﬁcation using small\ntraining sample size. In 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR), pp.\n730–734, 2015.\nS. Lloyd. Least squares quantization in pcm. IEEE Trans. Inf. Theor., 28(2), September 2006.\nUlrike Von Luxburg. A tutorial on spectral clustering, 2007.\n11\nPublished as a conference paper at ICLR 2021\nLiqian Ma, Xu Jia, Stamatios Georgoulis, Tinne Tuytelaars, and Luc Van Gool. Exemplar guided\nunsupervised image-to-image translation with semantic consistency. In International Conference\non Learning Representations, 2019.\nXudong Mao, Yun Ma, Zhenguo Yang, Yangbin Chen, and Qing Li. Virtual mixup training for\nunsupervised domain adaptation. arXiv preprint arXiv:1905.04215, 2019.\nMehdi Mirza and Simon Osindero.\nConditional generative adversarial nets.\narXiv preprint\narXiv:1411.1784, 2014.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a\nregularization method for supervised and semi-supervised learning. IEEE transactions on pattern\nanalysis and machine intelligence, 41(8), 2018.\nSangwoo Mo, Minsu Cho, and Jinwoo Shin. Instagan: Instance-aware image-to-image translation. In\nInternational Conference on Learning Representations, 2019.\nZ. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim. Image to image translation for\ndomain adaptation. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 4500–4509, 2018.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. The street\nview house numbers (svhn) dataset, 2011.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dÁlché Buc, E. Fox, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran\nAssociates, Inc., 2019.\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching\nfor multi-source domain adaptation. In Proceedings of the IEEE International Conference on\nComputer Vision, pp. 1406–1415, 2019.\nOri Press, Tomer Galanti, Sagie Benaim, and Lior Wolf. Emerging disentanglement in auto-encoder\nbased unsupervised image content transfer. In International Conference on Learning Representa-\ntions, 2019.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks, 2015.\nPravakar Roy, Nicolai Häni, and Volkan Isler. Semantics-aware image to image translation and\ndomain transfer. CoRR, abs/1904.02203, 2019.\nRui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-t approach to unsupervised domain\nadaptation. In International Conference on Learning Representations, 2018.\nYaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. In 5th\nInternational Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,\n2017, Conference Track Proceedings. OpenReview.net, 2017.\nM. Tomei, M. Cornia, L. Baraldi, and R. Cucchiara. Art2real: Unfolding the reality of artworks\nvia semantically-aware image-to-image translation. In 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 5842–5852, 2019.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. CoRR, abs/1807.03748, 2018a.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. CoRR, abs/1807.03748, 2018b.\n12\nPublished as a conference paper at ICLR 2021\nM. Wang, G. Yang, R. Li, R. Liang, S. Zhang, P. M. Hall, and S. Hu. Example-guided style-consistent\nimage synthesis from semantic labeling. In 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 1495–1504, 2019.\nT. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and\nsemantic manipulation with conditional gans. In 2018 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 8798–8807, 2018.\nW. Wu, K. Cao, C. Li, C. Qian, and C. C. Loy. Transgaga: Geometry-aware unsupervised image-to-\nimage translation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 8004–8013, 2019.\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In\nProceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings\nof Machine Learning Research, pp. 478–487, New York, New York, USA, 20–22 Jun 2016. PMLR.\nDingdong Yang, Seunghoon Hong, Yunseok Jang, Tiangchen Zhao, and Honglak Lee. Diversity-\nsensitive conditional generative adversarial networks. In International Conference on Learning\nRepresentations, 2019.\nP. Zhang, B. Zhang, D. Chen, L. Yuan, and F. Wen. Cross-domain correspondence learning for\nexemplar-based image translation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 5142–5152, 2020.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. 2017 IEEE International Conference on Computer\nVision (ICCV), 2017a.\nJun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli\nShechtman. Toward multimodal image-to-image translation. In Advances in Neural Information\nProcessing Systems, 2017b.\n13\nPublished as a conference paper at ICLR 2021\nA\nBACKGROUND CROSS-DOMAIN SEMANTICS LEARNING\nUnsupervised representation learning aims at learning an embedding of the input which will\nbe useful for a downstream task, without direct supervision about the task(s) of interest. For a\ndownstream task of classiﬁcation, success is typically deﬁned, but not limited, as the ability to classify\nthe learned representation with a linear classiﬁer. Recent advances have produced very impressive\nresults by exploiting self-supervision, where a useful supervisory signal is concocted from within\nthe unlabeled dataset. Contrastive learning methods such as CPC (van den Oord et al., 2018a),\nDIM (Hjelm et al., 2019), SimCLR (Chen et al., 2020a), and MoCo (He et al., 2020; Chen et al.,\n2020b) have shown very strong success, for example achieving more than 70% top-1 accuracy on\nImageNet by linear classiﬁcation on the learned embeddings.\nClustering separates data (or a representation of the data) into an N-discrete set. N can be known a\npriori, or not. While methods such as K-means (Lloyd, 2006) and spectral clustering (Luxburg, 2007)\nare classic, recent deep learning approaches such as DEC (Xie et al., 2016), IMSAT (Hu et al., 2017)\nand Deep Clustering (Caron et al., 2018) demonstrate that the representation of a neural network can\nbe used for clustering complex, high-dimensional data.\nUnsupervised domain adaptation (Ben-David et al., 2010) aims at adapting a classiﬁer from a\nlabelled source domain to an unlabelled target domain. Ganin et al. (2016) uses the gradient reversal\nmethod to minimize the divergence between the hidden representations of the source and target\ndomains. Follow-up methods have been proposed to adapt the classiﬁer by relevant regularization;\nVADA (Shu et al., 2018) regularizes using the cluster assumption (Grandvalet & Bengio, 2005) and\nvirtual adversarial training (Miyato et al., 2018), VMT (Mao et al., 2019) suggests using virtual\nMixup training.\nB\nADDITIONAL EXPERIMENTAL DETAILS\nOur results on MNIST↔SVHN and Sketches→Reals datasets were obtained using our Py-\ntorch (Paszke et al., 2019) implementation. We provide the code which contains all the details\nnecessary for reproducing the results as well as scripts that will themselves reproduce the results.\nHere, we provide additional experimental and technical details on the methods used. In particular, we\npresent the datasets and the baselines used. We follow with a detailed background on IMSAT (Hu\net al., 2017) which is used to learn a clustering on MNIST in our MNIST↔SVHN. Next, we give\na background on MoCO (Chen et al., 2020b) which is used to learn a representation on the Reals.\nThen, we provide a background on Virtual Mixup Training, which is the domain adaptation technique\nthat we use to adapt either the MNIST to SVHN or Reals to Sketches. Finally, we provide a method\nfor evaluating the clusters across multiple domains.\nB.1\nEXPERIMENTAL DATASETS\nThroughout our SPUDT experiments, we transfer between both the MNIST (LeCun & Cortes, 2010),\nwhich we upsample to 32 × 32 and triple the number of channels, and the SVHN (Netzer et al., 2011)\ndatasets. We don’t alter the SVHN dataset, i.e. we consider 32 × 32 samples with 3 channels RGB\nwithout any data augmentation. But, we consider samples with feature values in the range [-1, 1], as\nit is usually done in the GAN litterature (Radford et al., 2015), for all of our datasets.\nWe use a subset of Sketches and Reals from the DomainNet dataset (Peng et al., 2019) to demonstrate\nthe task of SHDT. We use the following ﬁve categories of the DomainNet dataset: bird, dog, ﬂower,\nspeedboat and tiger; these 5 are among the categories with most samples in both our domains\nand possessing distinct styles which are largely non-interchangeable. We resized every image to\n256 × 256.\nB.2\nBASELINES\nFor our UDT baselines, we compare with CycleGAN (Zhu et al., 2017b), MUNIT (Huang et al.,\n2018), DRIT (Lee et al., 2019) and StarGAN-V2 (Choi et al., 2019). We use these baselines because\nthey are, to our knowledge, the reference models for unsupervised domain translation today. But,\nnone of these baselines use semantics. Also, we are not aware of any UDT method that proposes to\n14\nPublished as a conference paper at ICLR 2021\nuse semantics without supervision. Hence, we also consider EGST-IT (Ma et al., 2019) as a baseline\nalthough it is weakly supervised by the usage of a pre-trained VGG network. EGSC-It proposes to\ninclude the semantics into the translation network by conditioning the content representation. It also\nconsiders the usage of exemplar, unconditionally of the source sample.\nFor each of the baselines, we perform our due diligence to ﬁnd the set of parameters that perform the\nbest and report our results using these parameters.\nB.3\nIMSAT FOR CLUSTERING MNIST\nClustering\nFollowing RIM (Gomes et al., 2010) and IMSAT (Hu et al., 2017), we learn a mapping\nc : X →C, where C ∈Rk is a continuous space representing a soft clustering of X, by optimizing\nthe following objective\nmin\nc\nλR(c) −I(X; C),\n(10)\nwhere λ > 0 is a Lagrange multiplier, I is the mutual information deﬁned as\nI(X; C) = H(C) −H(C|X),\nand R is a regularizer to restrict the class of functions. As in IMSAT, we use the regularizer\nR(c) = Ex∼Px||c(x) −c(x′)||2\n2,\n(11)\nwhere x′ = T(x), and T is a set of transformations of the original image, such as afﬁne transforma-\ntions. Essentially, this ensures that the mapping is invariant under the set of transformations deﬁned\nby T. In particular, we used afﬁne translations such as rotation scaling and skewing.\nIf c is a deterministic function, then H(C|X) = 0, and max I(X; C) = max H(C). Hence, we are\ninterested in a clustering of maximum entropy. This can be achieved if Pc = Pcat where Pcat is the\ncategorical distribution with uniform probability for every category (or a prior distribution, if we have\naccess to it).\nThus, we can maximize the mutual information by mapping to the uniform categorical distribution.\nIMSAT minimizes the KL-divergence, DKL[Pc || Pcat]. Equivalently, we can minimize the EMD\nW1(c#Px, Pcat) using the Wasserstein GAN framework, where # denotes the push-forward function.\nI(X; C) =\nmax\nf:Lipschitz-1 Eˆu∼c#Pxf(ˆu) −Eu∼Pcatf(u).\n(12)\nUsing equation 11 and equation 12 in equation 10, we obtain the following objective for clustering\nmin\nc\nmax\nf:Lipschitz-1 λEx∼Px||c(x) −c(x′)||2 −Eˆu∼c#Pxf(ˆu) −Eu∼Pcatf(u).\nB.4\nSELF-SUPERVISION OF REAL IMAGES WITH MOCO\nWe use MoCo (He et al., 2020), a self-supervised representation-learning algorithm, for learning an\nembedding from the sketches and reals images to a code.\nLet fq and fk be two networks. Assume that fk is a moving average of fq. MoCo principally\nminimizes the following contrastive loss, called InfoNCE (van den Oord et al., 2018b), with respect\nto the parameters of fq.\nLnce = −log\nexp(fq(xi) · fk(xi)/τ)\nPK\nj=0 exp(fq(xi) · fk(xj)/τ)\n.\n(13)\nThe parameters θk of fk are updates as follows\nθk ←mθk + (1 −m)θq\nwhere m ∈[0, 1) and θq are the parameters obtained by minimizing equation 13 by gradient descent.\nFurthermore, a dictionary of the representation fk(x) is preserved and updated throughout the\ntraining, allowing to have more negative samples, i.e., K in equation 13 can be bigger. We refer to\nthe main paper for more technical details.\n15\nPublished as a conference paper at ICLR 2021\nB.5\nVIRTUAL MIXUP TRAINING FOR UNSUPERVISED DOMAIN ADAPTATION\nDomain adaptation aims at adapting a function trained on a domain X so that it can perform well\non a domain Y. Unsupervised domain adaptation refers to the case where the target domain Y is\nunlabelled during training. Normally, it assumes supervised labels on the source domain. Here, we\nwill instead assume that we have a pre-trained inference network c trained to cluster X. In other\nwords, we do not assume ground truth labels. For MNIST and SVHN, we consider X and Y to be the\nraw images. For Sketches and Reals, we consider X and Y to be their learned embeddings.\nIt has been shown that the error of a hypothesis function h on the target domain Y is upper bounded\nby the following (Ben-David et al., 2010)\nLY(h) ≤LX (h) + d(X, Y) + min\nh′ LX (h′) + LY(h′),\nwhere Lx is the risk and can be computed given a loss function, for example the cross entropy. Then\nLX (h) = −Ex∼Pxc(x) log h(x).\nLately, unsupervised domain adaptation has seen major improvements. In this work, we shall leverage\nthe tricks proposed in Ganin et al. (2016); Shu et al. (2018); Mao et al. (2019) because of their\ndemonstrated empirical success in the modalities that interest us in this work. We brieﬂy describe\nthese techniques below.\nGradient reversal\nInitially proposed in Ganin et al. (2016), gradient reversal aims to match the\nmarginal distribution of intermediate hidden representations of a neural network across domains. If h\nis a neural network and can be composed as h = h2 ◦h1, then gradient reversal is deﬁned as\nLgv(h1) = max\nD\nEhx∼h1#Px[log D(hx)] + Ehy∼h1#Py[log(1 −D(hy))].\nwhich can also be seen as applying a GAN loss (Goodfellow et al., 2014) on a representation of a\nneural network.\nCluster assumption\nThe cluster assumption (Chapelle & Zien, 2005) is simply an assumption that\nthe data is clusterable into classes. In other words, it states that the decision boundaries of h should\nbe in low-density regions of the data. To encourage this, Grandvalet & Bengio (2005) propose to\nminimize the following objective on the conditional entropy:\nLc(h) = −Ey∼Pyh(y) log h(y).\nHowever, in practice, such a constraint is applied on an empirical distribution. Hence, nothing\nstops the classiﬁer from abruptly changing its predictions for any samples outside of the training\ndistribution. This motivates the next constraints.\nVirtual adversarial training\nShu et al. (2018) propose to alleviate this problem by constraining\nh to be locally-Llipschitz around an ϵ-ball. Borrowing from Miyato et al. (2018), they propose the\nadditional regularizer\nLvx(h) = max\n||r||2≤ϵ DKL(h(x) || h(x + r)),\nwith ϵ > 0.\nVirtual mixup training\nWith similar motivations, Mao et al. (2019) propose that the prediction\nof an interpolated point ˜x should itself be an interpolation of the predictions at x1 and at x2. We\ncompute interpolates as\n˜x = αx1 + (1 −α)x2,\n˜y = αh(x1) + (1 −α)h(x2),\nwith α ∼U(0, 1), where U(0, 1) is a continuous uniform distribution between 0 and 1.\nThe proposed objective is then simply\nLmx(h) = −Ex∼Px ˜y⊤log h(˜x).\n16\nPublished as a conference paper at ICLR 2021\nMUNIT\nDRIT\nEGSC-IT\nStarGAN-v2\nCatS-UDT (ours)\nM→S\nS→M\nFigure 5: Qualitative comparison of the baselines with our method on MNIST↔SVHN. Even\ncolumns correspond to source samples, and odd columns correspond to their translations.\nThese objectives are composed to give the overall optimization problem:\nmin\nh\nLX (h) + λ1Lgv(h1) + λ2Lc(h) + λ3Lvx(h) + λ4Lvy(h) + λ5Lmx(h) + λ6Lmy(h),\nwhere the second subscript x or y denotes the domain.\nFinally, we note that for MNIST↔SVHN, we perform the adaptation directly on the image space.\nFor Sketches→Reals, we found that it worked better to perform the adaptation on the representation\nspace instead.\nB.6\nEVALUATION OF THE LEARNED CROSS-DOMAIN CLUSTERS\nAn important detail is the evaluation of the clustering across the domains. This evaluation indeed\ngives a signal of how good the categorical representation is. But, because the cluster identities might\nhave been shifted from the pre-deﬁned labels in the validation set, it is important to consider this shift\nwhen performing the evaluation. Therefore, we ﬁrst deﬁne the correspondence for the cluster to label\n˜lk as follows\n˜lk =\narg max\nj∈{0,...,|C|−1}\nN\nX\ni=1\nI(j, li)|k ∩c(xi)j|.\n(14)\nwhere I is the indicator function returning 1 if j = li and 0 otherwise. Essentially, equation 14 is\nnecessary because we want the same labels in both domains to map to the same cluster. Hence, simply\ncomputing the purity evaluation could be misleading in the case where both domains are clustered\ncorrectly, but the clusters do not align to the same labels. Using this correspondence, we can now\nproceed to evaluate the clustering adaptation using the evaluation accuracy as one would normally do.\nC\nADDITIONAL RESULTS\nC.1\nQUALITATIVE RESULTS FOR MNIST-SVHN\nWe present additional qualitative results to provide a better sense of the results that our method\nachieves. In Figure 5, we show qualitative comparisons with samples of translation for the baselines\nand our technique. We observe that the use of semantics in the translation visibly helps with preserving\nthe semantic of the source samples. The qualitative results conﬁrm the quantitative results on the\npreservation of the digit identity presented in Table 1.\nFurthermore, in Figure 6, we present qualitative results of the effect of changing the noise sample z\non the generation of SVHN samples for the same MNIST source sample. The ﬁrst row represents\nthe source samples and each column represents a generation with a different z. Each source sample\nuses the same set of z in the same order. We observe that z indeed grossly controls the style of the\ngeneration. Also, we observe that the generations preserve features of the source sample such as\nthe pose. However, we note that some attributes such as typography are not perfectly preserved. In\nthis instance, we conjecture that this is due to the fact the the “MNIST typography\" is not the same\nas the “SVHN typography\". For example, the ‘4’s are different in the MNIST and SVHN datasets.\nTherefore, due to the adversarial loss, the translation has to modify the typography of MNIST.\n17\nPublished as a conference paper at ICLR 2021\nFigure 6: Multiple sampling for MNIST→SVHN. For each column, the ﬁrst row is the source sample\nand each subsequent row is a generation corresponding to a different z.\nC.2\nADDITIONAL ABLATION STUDIES FOR MNIST-SVHN\nAblation study – the effect of the losses on the FID. In Figure 7a, we evaluate the effect of removing\neach of the losses, by setting their λ = 0, on the FID. We observe that removing the semantic loss\nyields the biggest deterioration for the FID. Hence, the semantic loss does not only improve the\nsemantic preservation as observed in Section 4.1, but also the image quality of the translation.\nAlso, we see a U-curve on the FID on MNIST→SVHN with respect to the parameter λsem. We\nobserve that tuning this parameter allows us to improve the generation quality. We make a similar\nobservation for SVHN→MNIST for both the FID and the accuracy. In Figure 7c, we present\nqualitative results of the effect of setting λsem = 10. We see that the samples are a mix of MNIST\nand SVHN samples. The reduction in generation quality explains why we obtain a worst FID when\nλsem is too high. Moreover, we see that the generated samples are out-of-distribution, explaining why\nwe obtain a low accuracy although the digit identity is preserved.\nComparative study – effect of the method to condition the semantics. In Figure 8a and in\nFigure 8b, we evaluate the effect of the method to condition the semantics – in MNIST↔SVHN – on\nthe translation accuracy and on the FID respectively.\nsem = 0\nsty = 0\ncyc = 0\nsd = 0\n0\n10\n20\n30\n40\n50\nFID\nmnist\nsvhn\nsvhn\nmnist\n(a) Setting one λ = 0.\n10\n2\n10\n1\n100\n101\nsem\n0\n50\n100\n150\n200\n250\nFID\nmnist\nsvhn\nsvhn\nmnist\n(b) Varying λsem.\n(c) SVHN→MNIST, λsem = 10\nFigure 7: Ablation studies on the effect on the FID on MNIST↔SVHN of (a) Setting one λ = 0\nwhile keeping the other λ′ = 1, (b) Varying λsem and (c) Qualitative results of SVHN→MNIST\nwhen λsem = 10.\n18\nPublished as a conference paper at ICLR 2021\nContent\nNone\nStyle\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nAccuracy\nmnist->svhn\nsvhn->mnist\n(a) Accuracy of conditioning methods.\nContent\nNone\nStyle\n0\n10\n20\n30\n40\n50\n60\nFID\nmnist->svhn\nsvhn->mnist\n(b) FID of conditioning methods.\nVGG\nMoCo\nNo-adapt\nAdapt\n0\n50\n100\n150\n200\n250\n300\nFID\nmnist\nsvhn\nsvhn\nmnist\n(c) FID of semantic encoders.\nFigure 8: Comparative studies on the effect (a) on the translation accuracy and (b, c) on the FID on\nMNIST↔SVHN on (a, b) Conditioning the content representation on the semantics, not conditioning\non semantics and conditioning the style representation on the semantics.\nNone refers to the case where the semantics is not explicitly used to condition any part of the\ntranslation network, but the semantic loss is still used. This method is commonly used in supervised\ndomain translation methods such as Bousmalis et al. (2017); Hoffman et al. (2018); Tomei et al.\n(2019). Content refers to the case where categorical semantics are used to condition the content\nrepresentation. This method is similar to the method used in Ma et al. (2019), for example, with the\nexception that the semantic encoder they used is a VGG trained on a classiﬁcation task. Style refers\nto the case where the categorical semantics are used to condition the style, as we propose to do.\nWe see that the method to condition the semantics does not affect the translation accuracy on\nMNIST↔SVHN. However, it does affect the generation quality. This further demonstrates the\nrelevance of injecting the categorical semantics by modulating the style of the generated samples.\nComparative study – effect of adapting the categorical semantics We saw that an adapted cat-\negorical semantics improved the semantics preservation on MNST→SVHN in Figure 3b. Here,\nwe will ﬁnish the comparison of the effect of adapting the semantics categorical representation on\naccuracy for SVHN→MNIST and the FID for MNIST↔SVHN in Figure 8c\nC.3\nADDITIONAL RESULTS FOR SKETCH→REAL\nWe provide more results to support the results presented in Section 4.2 on the Sketch→Real task.\nAdditional quantitative comparisons We observe qualitatively in Table 4 that our method is lacking\nin terms of diversity with respect to the other methods that do not leverage any kind of semantics.\nThis is not surprising because we penalize the network for generating samples that are unrealistic\nwith respect to the semantics of the source sample.\nEffect of setting λsem = 0. We demonstrated that not using the semantic loss considerably degraded\nthe FID, in Table 3a. In Figure 9, we demonstrate qualitatively that the generated samples, when\nλsem = 0 suffers from the same problem as the baseline: the style is not conditional to the semantics\nof the source sample.\nEffect of the method to condition the semantics. The method of conditioning the semantics\nin the network affects the generation, as observed in Table 3b. We present qualitative results in\nFigure 10 demonstrating the effect of not conditioning the semantics into any part of the translation\nnetwork – while still using the semantic loss – and the effect of conditioning the style on the content\nTable 4: Additional quantitative comparisons with the baselines. We qualitatively compare the\nbaselines using all the Sketches→Reals categories using LPIPS (higher is better), NBD and JSD\n(lower is better).\nData\nCycleGAN\nDRIT\nEGSC-IT\nStarGAN-V2\nCatS-UDT (ours)\nLPIPS\n0.713\n0.736\n0.064\n0.672\n0.065\nNDB\n18\n16\n19\n16\n12\nJSD\n0.139\n0.025\n0.044\n0.029\n0.033\n19\nPublished as a conference paper at ICLR 2021\nFigure 9: Sketch→Real using CatS-UDT with λsem = 0. Samples on the ﬁrst row are the source\nsamples. Samples on the subsequent rows are generated samples.\nrepresentation. In the latter case, we consider the semantics as categorical labels adapted to the\nsketches and the reals as well as semantics deﬁned as the representation from a VGG network trained\non classifying ImageNet.\nIn the ﬁrst case, the network fails to generate diverse samples and essentially ignores the style input.\nWe conjecture that this happens due to two reasons: (1) The content network and the generator cannot\nextract the semantics of the source image due to its constraints, relying on the style injected using\nAdaIN. (2) The mapping network generates the style unconditionally of the source samples; the style\nfor one semantic category might not ﬁt for another (e.g. the style of a tiger does not ﬁt in the context\nof generating a speedboat). Therefore, to avoid generating, for example, a speedboat with the style of\na tiger, the translation network ignores the mapping network.\nIn the second case, the network fails to generate samples like real images when using categorical\nsemantics. We demonstrate such phenomenon in Figure 10b. The failure is similar to the one observed\nwhen the content encoder downsamples the source image beyond a certain spatial dimension. In both\nthese cases, the generated samples lose the spatial coherence of the source image. Without the spatial\n(a) Not conditioning the translation\nnetwork.\n(b) Condition the content represen-\ntation with categorical semantics.\n(c) Condition the content represen-\ntation with VGG features.\nFigure 10: Qualitative effect of the method to condition the semantics in the translation network in\nSketches→Reals. Samples on the ﬁrst row are the source samples. Samples on the subsequent rows\nare generated samples.\n20\nPublished as a conference paper at ICLR 2021\n(a) CatS-UDT.\n(b) CycleGAN.\nFigure 11: Effect of the representation spatial dimension on the generation of Sketches→Reals. For\n(a) and (b), we downsample the content representation to a 4 × 4 feature map. Samples on the ﬁrst\nrow are the source samples. Samples on the subsequent rows are generated samples.\nrepresentation, the generator cannot leverage this information to facilitate the generation. Coupled\nwith the fact that the architecture of the generator assumes access to such a spatial representation and\nthe low number of samples, this explains why it fails at generating sensible samples. In this case, the\nspatial representation must be lost due to the addition of the categorical semantic representation and\nthe semantic loss. We conjecture that by minimizing the semantic loss, the network tries to leverage\nthe semantic information, interfering with the content representation. Furthermore, we tested a setup\nsimilar to the one presented in EGST-IT (Ma et al., 2019) where the semantics is deﬁned as the\nfeatures of a VGG network in Figure 10c. We see that this failure is not present in this case.\nEffect of the spatial dimension of the content representation. We present examples of samples\ngenerated when the spatial dimension of the content representation is too small to preserve spatial\ncoherence throughout the translation in Figure 11. In this example, we downsample until we reach\na spatial representation of 4 × 4 for both our method and CycleGAN. We included CycleGAN to\ndemonstrate that this effect is not a consequence of our method. In both cases, we see that the\ntranslation network fails to properly generate the samples as previously observed and discussed. This\nfurther highlights the importance of the inductive biases in these models.\nAdditional generation for each classes. We provide additional generations for each of the categories\nconsidered in Sketches→Reals in Figure 12 for more test source samples. In the fourth column of the\ndog panel in Figure 12b and the third column of the tiger panel in Figure 12e, we see a failure case\nof our method which can happen when a sketch gets mis-clustered. In the ﬁrst case, the semantic\nnetwork miscategorizes the dog for a tiger. In the second case, the semantic network miscategorizes\nthe tiger for a dog. This further demonstrates the importance of a semantics network that categorizes\nthe samples with high accuracy for the source and the target domain.\n21\nPublished as a conference paper at ICLR 2021\n(a) Birds.\n(b) Dogs.\n(c) Flowers.\n(d) Speedboats.\n(e) Tigers.\nFigure 12: Additional Sketches→Reals generations for each semantic categories.\n22\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-10-03",
  "updated": "2021-03-16"
}