{
  "id": "http://arxiv.org/abs/2010.01535v1",
  "title": "A Survey of Unsupervised Dependency Parsing",
  "authors": [
    "Wenjuan Han",
    "Yong Jiang",
    "Hwee Tou Ng",
    "Kewei Tu"
  ],
  "abstract": "Syntactic dependency parsing is an important task in natural language\nprocessing. Unsupervised dependency parsing aims to learn a dependency parser\nfrom sentences that have no annotation of their correct parse trees. Despite\nits difficulty, unsupervised parsing is an interesting research direction\nbecause of its capability of utilizing almost unlimited unannotated text data.\nIt also serves as the basis for other research in low-resource parsing. In this\npaper, we survey existing approaches to unsupervised dependency parsing,\nidentify two major classes of approaches, and discuss recent trends. We hope\nthat our survey can provide insights for researchers and facilitate future\nresearch on this topic.",
  "text": "arXiv:2010.01535v1  [cs.CL]  4 Oct 2020\nA Survey of Unsupervised Dependency Parsing\nWenjuan Han1∗, Yong Jiang2∗, Hwee Tou Ng1, Kewei Tu3†\n1Department of Computer Science, National University of Singapore\n2Alibaba DAMO Academy, Alibaba Group\n3School of Information Science and Technology, ShanghaiTech University\ndcshanw@nus.edu.sg\nyongjiang.jy@alibaba-inc.com\nnght@comp.nus.edu.sg\ntukw@shanghaitech.edu.cn\nAbstract\nSyntactic dependency parsing is an important task in natural language processing. Unsupervised\ndependency parsing aims to learn a dependency parser from sentences that have no annotation\nof their correct parse trees. Despite its difﬁculty, unsupervised parsing is an interesting research\ndirection because of its capability of utilizing almost unlimited unannotated text data. It also\nserves as the basis for other research in low-resource parsing. In this paper, we survey existing\napproaches to unsupervised dependency parsing, identify two major classes of approaches, and\ndiscuss recent trends. We hope that our survey can provide insights for researchers and facilitate\nfuture research on this topic.\n1\nIntroduction\nDependency parsing is an important task in natural language processing that aims to capture syntactic\ninformation in sentences in the form of dependency relations between words. It ﬁnds applications in\nsemantic parsing, machine translation, relation extraction, and many other tasks.\nSupervised learning is the main technique used to automatically learn a dependency parser from data.\nIt requires the training sentences to be manually annotated with their correct parse trees. Such a training\ndataset is called a treebank. A major challenge faced by supervised learning is that treebanks are not al-\nways available for new languages or new domains and building a high-quality treebank is very expensive\nand time-consuming.\nThere are multiple research directions that try to learn dependency parsers with few or even no syn-\ntactically annotated training sentences, including transfer learning, unsupervised learning, and semi-\nsupervised learning. Among these directions, unsupervised learning of dependency parsers (a.k.a. unsu-\npervised dependency parsing and dependency grammar induction) is the most challenging, which aims\nto obtain a dependency parser without using annotated sentences. Despite its difﬁculty, unsupervised\nparsing is an interesting research direction, not only because it would reveal ways to utilize almost un-\nlimited text data without the need for human annotation, but also because it can serve as the basis for\nstudies of transfer and semi-supervised learning of parsers. The techniques developed for unsupervised\ndependency parsing could also be utilized for other NLP tasks, such as unsupervised discourse pars-\ning (Nishida and Nakayama, 2020). In addition, research in unsupervised parsing inspires and veriﬁes\ncognitive research of human language acquisition.\nIn this paper, we conduct a survey of unsupervised dependency parsing research. We ﬁrst introduce the\ndeﬁnition and evaluation metrics of unsupervised dependency parsing, and discuss research areas related\nto it. Then we present in detail two major classes of approaches to unsupervised dependency parsing:\ngenerative approaches and discriminative approaches. Finally, we discuss important new techniques and\nsetups of unsupervised dependency parsing that appear in recent years.\n*Equal contributions.\n†Corresponding author.\n2\nBackground\n2.1\nProblem Deﬁnition\nDependency parsing aims at discovering the syntactic dependency tree z of an input sentence x, where\nx is a sequence of words x1, . . . , xn with length n. A dummy root word x0 is typically added at the\nbeginning of the sentence. A dependency tree z is a set of directed edges between words that form a\ndirected tree structure rooted at x0. Each edge points from a parent word (also called a head word) to a\nchild word.\nIn unsupervised dependency parsing, the goal is to obtain a dependency parser without using annotated\nsentences. Some work requires no training data and derives dependency trees from centrality or saliency\ninformation (Søgaard, 2012). We focus on learning a dependency parser from an unannotated dataset\nthat consists of a set of sentences without any parse tree annotation. In many cases, part-of-speech (POS)\ntags of the words in the training sentences are assumed to be available during training.\nTwo evaluation metrics are widely used in previous work of unsupervised dependency parsing\n(Klein and Manning, 2004): directed dependency accuracy (DDA) and undirected dependency accuracy\n(UDA). DDA denotes the percentage of correctly predicted dependency edges, while UDA is similar to\nDDA but disregards the directions of edges when evaluating their correctness.\n2.2\nRelated Areas\nSupervised Dependency Parsing\nSupervised dependency parsing aims to train a dependency parser\nfrom training sentences that are manually annotated with their dependency parse trees. Generally, super-\nvised dependency parsing approaches can be divided into graph-based approaches and transition-based\napproaches. A graph-based dependency parser searches for the best spanning tree of the graph that is\nformed by connecting all pairs of words in the input sentence. In the simplest form, a graph-based parser\nmakes the ﬁrst-order assumption that the score of a dependency tree is the summation of scores of its\nedges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions\nthat incrementally constructs the parse tree, typically from left to right. While current start-of-the-art\napproaches have achieved strong results in supervised dependency parsing, their usefulness is limited to\nresource-rich languages and domains with many annotated datasets.\nCross-Domain and Cross-Lingual Parsing\nOne useful approach to handling the lack of treebank\nresources in the target domain or language is to adapt a learned parser from a resource-rich source domain\nor language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very\nrelated to unsupervised parsing as both approaches do not rely on treebanks in the target domain or\nlanguage. However, unsupervised parsing is more challenging because it does not have access to any\nsource treebank either.\nUnsupervised Constituency Parsing\nConstituency parsing aims to discover a constituency tree of the\ninput sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent\nphrases. Unsupervised constituency parsing is often considered more difﬁcult than unsupervised depen-\ndency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have\nbeen far more papers in unsupervised dependency parsing than in unsupervised constituency parsing\nover the past decade. More recently, however, there is a surge in interest in unsupervised constituency\nparsing and several novel approaches were proposed in the past two years (Li et al., 2020). While we\nfocus on unsupervised dependency parsing in this paper, most of our discussions on the classiﬁcation of\napproaches and recent trends apply to unsupervised constituency parsing as well.\nLatent Tree Models with Downstream Tasks\nLatent tree models treat the parse tree as a latent vari-\nable that is used in downstream tasks such as sentiment classiﬁcation. While no treebank is used in\ntraining, these models rely on the performance of the downstream tasks to guide the learning of the latent\nparse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick\n(Jang et al., 2017) can be utilized (Yogatama et al., 2016; Choi et al., 2018). There also exists previous\nwork on latent dependency tree models that utilizes structured attention mechanisms (Kim et al., 2017)\nfor applications. Latent tree models differ from unsupervised parsing in that they utilize training sig-\nnals from downstream tasks and that they aim to improve performance of downstream tasks instead of\nsyntactic parsing.\n3\nGeneral Approaches\n3.1\nGenerative Approaches\n3.1.1\nModels\nA generative approach models the joint probability of the sentence and the corresponding parse tree.\nTraditional generative models are mostly based on probabilistic grammars. To enable efﬁcient inference,\nthey typically make one or more relatively strict conditional independence assumptions. The simplest\nassumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent\non its head token and is independent of anything else. Such assumptions make it possible to decompose\nthe joint probability into a product of component probabilities or scores, leading to tractable inference.\nHowever, they also lead to unavailability of useful information (e.g., context and generation history) in\ngenerating each token.\nBased on their respective independence assumptions, different generative models specify different\ngeneration processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992)\nchoose to ﬁrst uniformly sample a dependency tree skeleton and then populate the tokens (words) con-\nditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is\nconditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004)\npropose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree si-\nmultaneously. Without knowing the dependency tree structure, each head token has to sample a decision\n(conditioned on the head token and the dependency direction) of whether to generate a child token or\nnot before actually generating the child token. Besides, the generation of a child token in DMV is ad-\nditionally conditioned on the valence, deﬁned as the number of the child tokens already generated from\na head token. Headden III et al. (2009) propose to also introduce the valence into the condition of de-\ncision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on\nsibling words, sentence completeness, and punctuation context. In addition to these generative depen-\ndency models, other grammar formalisms have also been used for unsupervised dependency parsing,\nsuch as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars\n(Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013).\nSimilar tokens may have similar syntactic behaviors in a grammar.\nFor example, all the verbs\nare very likely to generate a noun to the left as the subject. One way to capture this prior knowl-\nedge is to compute generation probabilities from a set of features that conveys syntactic similar-\nity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-\nsyntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to\nautomatically learn such features. Both approaches are based on DMV.\n3.1.2\nInference\nGiven a model parameterized by Θ and a sentence x, the model predicts the parse z∗with the highest\nprobability.\nz∗= arg max\nz∈Z(x) P(x, z; Θ)\n(1)\nwhere Z(x) is the set of all valid dependency trees of the sentence x. Due to the independence assump-\ntions made by generative models, the inference problem can be efﬁciently solved exactly in most cases.\nFor example, chart parsing can be used for DMV.\n3.1.3\nLearning Objective\nLog marginal likelihood is typically employed as the objective function for learning generative models.\nIt is deﬁned on N training sentences X = {x(1), x(2), ..., x(N)}:\nL(Θ) =\nN\nX\ni=1\nlog P(x(i); Θ)\n(2)\nwhere the model parameters are denoted by Θ. The likelihood of each sentence x is as follows:\nP(x; Θ) =\nX\nz∈Z(x)\nP(x, z; Θ)\n(3)\nwhere Z(x) is the set of all valid dependency trees of sentence x. As we mentioned earlier, the joint\nprobability of a sentence and its dependency tree can be decomposed into the product of the probabilities\nof the components in the dependency tree.\nApart from the vanilla marginal likelihood, priors and regularization terms are often added into the\nobjective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms\ninto the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith\n(2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in\nDMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-speciﬁed universal\ndependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity.\nThe approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees\nbased on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from\nbecoming too ambiguous. Mareˇcek and ˇZabokrtsk`y (2012) insert a term that prefers reducible subtrees\n(i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same\nreducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV.\nNoji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of\nthe parse tree.\n3.1.4\nLearning Algorithm\nThe Expectation-Maximization (EM) algorithm is typically used to optimize log marginal likelihood. For\neach sentence, the EM algorithm aims to maximize the following lower-bound of the objective function\nand alternates between the E-step and M-step.\nlog P(x; Θ) −KL(Q(z)∥P(z|x, Θ))\n(4)\nwhere Q(z) is an auxiliary distribution with regard to z. In the E-step, Θ is ﬁxed and Q(z) is set to\nP(z|x, Θ). A set of so-called expected counts can be derived from Q(z) to facilitate the subsequent M-\nstep and they are typically calculated using the inside-outside algorithm. In the M-step, Θ is optimized\nbased on the expected counts with Q(z) ﬁxed.\nThere are a few variants of the EM algorithm. If Q(z) represents a point-estimation (i.e., the best\ndependency tree has a probability of 1), the algorithm becomes hard-EM or Viterbi EM, which is found\nto outperform standard EM in unsupervised dependency parsing (Spitkovsky et al., 2010b). Softmax-\nEM (Tu and Honavar, 2012) falls between EM (considering all possible dependency trees) and hard-EM\n(only considering the best dependency tree), applying a softmax-like transformation to Q(z). During the\nEM iterations, an annealing schedule (Tu and Honavar, 2012) can be used to gradually shift from hard-\nEM to softmax-EM and ﬁnally to the EM algorithm, which leads to better performance than sticking to a\nsingle algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM,\nwhich is also found to produce better results than both EM and hard-EM.\nApproaches with more complicated objectives often require more advanced learning algorithms, but\nmany of the algorithms can still be seen as extensions of the EM algorithm that revise either the E-\nstep (e.g., to update Q(z) based on posterior regularization terms) or the M-step (e.g., to optimize the\nposterior probability that incorporates parameter priors).\nIntermediate\nRepresentation\nEncoder\nDecoder\nAutoencoder\nCRFAE (Cai et al., 2017)\nZ\nP(z|x)\nP(ˆx|z)\nD-NDMV (Han et al., 2019a)\nDeterministic Variant\nS\nP(s|x)\nP(z, ˆx|s)\nVariational\nAutoencoder\n(Li et al., 2019)\nZ\nP(z|x)\nP(z, x)\nD-NDMV (Han et al., 2019a)\nVariational Variant\nS\nP(s|x)\nP(z, x|s)\n(Corro and Titov, 2018)\nZ\nP(z|x)\nP(x|z)\nTable 1: Major approaches based on autoencoders and variational autoencoders for unsupervised depen-\ndency parsing. Z: dependency tree. S: continuous sentence representation. ˆx is a copy of x representing\nthe reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x.\nBetter\nlearning\nresults\ncan\nalso\nbe\nachieved\nby\nmanipulating\nthe\ntraining\ndata.\nSpitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the\nshortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide\na theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing.\nSpitkovsky et al. (2013) propose to treat different learning algorithms and conﬁgurations as modules\nand connect them to form a network. Some approaches discussed above, such as Lateen EM and cur-\nriculum learning, can be seen as special cases of this approach.\n3.1.5\nPros and Cons\nIt is often straightforward to incorporate various inductive biases and manually-designed local features\ninto generative approaches. Moreover, generative models can be easily trained via the EM algorithm and\nits extensions. On the other hand, generative models often have limited expressive power because of the\nindependence assumptions they make.\n3.2\nDiscriminative Approaches\nBecause of the limitation of generative approaches, more recently, researchers have paid more attention to\ndiscriminative approaches. Discriminative approaches model the conditional probability or score of the\ndependency tree given the sentence. By conditioning on the whole sentence, discriminative approaches\nare capable of utilizing not only local features (i.e., features related to the current dependency) but also\nglobal features (i.e., contextual features from the whole sentence) in scoring a dependency tree.\n3.2.1\nAutoencoder-Based Approaches\nAutoencoder-based approaches aim to map a sentence to an intermediate representation (encoding) and\nthen reconstruct the observed sentence from the intermediate representation (decoding). In the two exist-\ning autoencoder approaches (summarized in Table 1), the intermediate representation is the dependency\ntree and a continuous sentence vector respectively.\nThe reconstruction loss is typically employed as the learning objective function for autoencoder mod-\nels. For a training dataset including N sentences X = {x1, x2, ..., xN}, the objective function is as\nfollows:\nL(Θ) =\nN\nX\ni=1\nlog P(ˆx(i)|x(i); Θ)\n(5)\nwhere Θ is the model parameter and ˆx(i) is a copy of x(i) representing the reconstructed sentence1. In\nsome cases, there is an additional regularization term (e.g., L1) of Θ.\nThe ﬁrst autoencoder model for unsupervised dependency parsing, proposed by Cai et al. (2017), is\nbased on the conditional random ﬁeld autoencoder framework (CRFAE). The encoder is a ﬁrst-order\ngraph-based discriminative dependency parser mapping an input sentence to the space of dependency\n1In Han et al. (2019a), x is the word sequence, while ˆx is the POS tag sequence of the same sentence.\ntrees. The decoder independently generates each token of the reconstructed sentence conditioned on the\nhead of the token speciﬁed by the dependency tree. Both the encoder and the decoder are arc-factored,\nmeaning that the encoding and decoding probabilities can be factorized by dependency arcs. Coordinate\ndescent is applied to minimize the reconstruction loss and alternately updates the encoder parameters\nand the decoder parameters.\nD-NDMV (Han et al., 2019a) (the deterministic variant) is the second autoencoder model proposed\nfor unsupervised dependency parsing, in which the intermediate representation is a continuous vector\nrepresenting the input sentence. The encoder is an LSTM summarizing the sentence with a continuous\nvector s, while the decoder models the joint probability of the sentence and the dependency tree. More\nspeciﬁcally, the decoder is a generative neural DMV that generates the sentence and its parse simulta-\nneously, and its parameters are computed based on the continuous vector s. The reconstruction loss is\noptimized using the EM algorithm. In the E-step, Θ is ﬁxed and Q(z) is set to P(z|x, s; Θ). After we\ncompute all the grammar rule probabilities given Θ, the inside-outside algorithm can be used to calculate\nthe expected counts. In the M-step, Θ is optimized based on the expected counts with Q(z) ﬁxed.\n3.2.2\nVariational Autoencoder-Based Approaches\nAs mentioned in Section 3.1, the training objective of a generative model is typically the probability\nof the training sentence and the dependency tree is marginalized as a hidden variable. However, the\nmarginalized probability cannot usually be calculated accurately for more complex models that do not\nmake strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower\nBound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation\nfollows a distribution, different sampling approaches are used to optimize the objective function (i.e.,\nlikelihood) according to different model schema.\nThree unsupervised dependency parsing models were proposed in recent years based on variational\nautoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability\nof the syntactic structure, the probability of generating the sentence from the syntactic structure (the\ndecoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.\nRecurrent Neural Network Grammars (RNNG) (Dyer et al., 2016) is a transition-based constituent\nparser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs\nthe constituency tree of the input sentence through three kinds of operations: generating a non-terminal\ntoken, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation\noperation and incrementally generates a constituency tree and its corresponding sentence. The probabil-\nity of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency\nparsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational\nautoencoder respectively. However, because RNNG has a strong expressive power, it is prone to over-\nﬁtting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce\nlinguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.\nThe model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is de-\nsigned for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised\ndependency parsing. The encoder of this model is a conditional random ﬁeld model while the decoder\ngenerates a sentence based on a graph convolutional neural network whose structure is speciﬁed by the\ndependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gra-\ndient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel\nrandom perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efﬁcient\napproximate sampling algorithm.\nThe variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic\nvariant described in Section 3.2.1, except that the variational variant probabilistically models the in-\ntermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also\nspeciﬁes a Gaussian prior over the intermediate continuous vector.\n3.2.3\nOther Discriminative Approaches\nApart from the approaches based on autoencoder and variational autoencoder, there are also a few other\ndiscriminative approaches based on discriminative clustering (Grave and Elhadad, 2015), self-training\n(Le and Zuidema, 2015), or searching (Daum´e III, 2009). Because of space limit, below we only intro-\nduce the approach based on discriminative clustering called Convex MST (Grave and Elhadad, 2015).\nConvex MST employs a ﬁrst-order graph-based discriminative parser. It searches for the parses of all\nthe training sentences and learns the parser simultaneously, with a learning objective that the searched\nparses are close to the predicted parses by the parser. In other words, the parses should be easily pre-\ndictable by the parser. The objective function can be relaxed to become convex and then can be optimized\nexactly.\n3.2.4\nPros and Cons\nDiscriminative models are capable of accessing global features from the whole input sentence and are\ntypically more expressive than generative models. On the other hand, discriminative approaches are\noften more complicated and do not admit tractable exact inference.\n4\nRecent Trends\n4.1\nCombined Approaches\nGenerative approaches and discriminative approaches have different pros and cons. Therefore, a nat-\nural idea is to combine the strengths of the two types of approaches to achieve better performance.\nJiang et al. (2017) propose to jointly train two state-of-the-art models of unsupervised dependency pars-\ning, the generative LC-DMV (Noji et al., 2016) and the discriminative Convex MST, with the dual de-\ncomposition technique that encourages the two models to gradually inﬂuence each other during training.\n4.2\nNeural Parameterization\nTraditional generative approaches either directly learn or use manually-designed features to com-\npute dependency rule probabilities.\nFollowing the recent rise of deep learning in the ﬁeld of NLP,\nJiang et al. (2016) propose to predict dependency rule probabilities using a neural network that takes\nas input the vector representations of the rule components such as the head and child tokens.\nThe\nneural network can automatically learn features that capture correlations between tokens and rules.\nHan et al. (2019a) extend this generative approach to a discriminative approach by further introducing\nsentence information into the neural network in order to compute sentence-speciﬁc rule probabilities.\nCompared with generative approaches, it is more natural for discriminative approaches to use neural\nnetworks to score dependencies or parsing actions, so recent discriminative approaches all make use of\nneural networks (Li et al., 2019; Corro and Titov, 2018).\n4.3\nLexicalization\nIn the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS\ntags being the tokens in the sentences. The POS tags are either human annotated or induced from the\ntraining corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may\nhave very different syntactic behavior and hence it should be beneﬁcial to introduce lexical information\ninto unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017)\nuse partial lexicalization in which infrequent words are replaced by special symbols or their POS tags.\nPate and Johnson (2016) and Spitkovsky et al. (2013) experiment with full lexicalization. However, be-\ncause the number of words is huge, a major problem with full lexicalization is that the grammar becomes\nmuch larger and thus learning requires more data. To mitigate the negative impact of data scarcity,\nsmoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict depen-\ndency probabilities that are automatically smoothed.\nIn principle, lexicalized approaches could also beneﬁt from pretrained word embeddings, which cap-\nture syntactic and semantic similarities between words. Recently proposed contextual word embeddings\nMETHODS\n≤10\nALL\nGenerative Approaches (cont’d)\nGenerative Approaches\nSpitkovsky et al. (2011a)\n-\n59.1\nKlein and Manning (2004)\n46.2\n34.9\nGimpel and Smith (2012)\n64.3\n53.1\nCohen et al. (2008)\n59.4\n40.5\nTu and Honavar (2012)\n71.4\n57.0\nCohen and Smith (2009)\n61.3\n41.4\nBisk and Hockenmaier (2012)\n71.5\n53.3\nHeadden III et al. (2009)\n68.8\n-\nSpitkovsky et al. (2013)\n72.0\n64.4\nSpitkovsky et al. (2010a)\n56.2\n44.1\nJiang et al. (2016)\n72.5\n57.6\nBerg-Kirkpatrick et al. (2010)\n63.0\n-\nHan et al. (2017)\n75.1\n59.5\nGillenwater et al. (2010)\n64.3\n53.3\nHe et al. (2018)*\n60.2\n47.9\nSpitkovsky et al. (2010b)\n65.3\n47.9\nDiscriminative Approaches\nBlunsom and Cohn (2010)\n65.9\n53.1\nDaum´e III (2009)\n-\n45.4\nNaseem et al. (2010)\n71.9\n-\nLe and Zuidema (2015) †\n73.2\n65.8\nBlunsom and Cohn (2010)\n67.7\n55.7\nCai et al. (2017)\n71.7\n55.7\nSpitkovsky et al. (2011c)\n-\n55.6\nLi et al. (2019)\n54.7\n37.8\nSpitkovsky et al. (2011b)\n69.5\n58.4\nHan et al. (2019a)\n75.6\n61.4\nTable 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sen-\ntences of length ≤10 and all lengths. *: without gold POS tags. †: with more training data in addition\nto WSJ.\n(Devlin et al., 2019) are even more informative, capturing contextual information. However, word em-\nbeddings have not been widely used in unsupervised dependency parsing. One concern is that word\nembeddings are too informative and may make unsupervised models more prone to overﬁtting. One\nexception is He et al. (2018), who propose to use invertible neural projections to map word embeddings\ninto a latent space that is more amenable to unsupervised parsing.\n4.4\nBig Data\nAlthough unsupervised parsing does not require syntactically annotated training corpora and can theo-\nretically use almost unlimited raw texts for training, most of the previous work conducts experiments\non the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) contain-\ning no more than 6,000 training sentences. There are a few papers that try to go beyond such a small\ntraining corpus. Pate and Johnson (2016) use two large corpora containing more than 700k sentences.\nMareˇcek and Straka (2013) utilize a very large corpus based on Wikipedia in learning an unlexicalized\ndependency grammar. Han et al. (2017) use a subset of the BLLIP corpus that contains around 180k\nsentences. With the advancement of computing power and deep neural models, we expect to see more\nfuture work on training with big data.\n4.5\nUnsupervised Multilingual Parsing\nTo tackle the lack of supervision in unsupervised dependency parsing, some previous work con-\nsiders learning models of multiple languages simultaneously (Berg-Kirkpatrick and Klein, 2010;\nLiu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other\nby identifying shared syntactic behaviors of different languages, especially those in the same language\nfamily. For example, Berg-Kirkpatrick and Klein (2010) propose to utilize the similarity of different lan-\nguages deﬁned by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b)\npropose to learn a uniﬁed multilingual parser with language embeddings as input. Jiang et al. (2019)\npropose to guide the learning process of unsupervised dependency parser from the knowledge of an-\nother language by using three types of regularization to encourage similarity between model parameters,\ndependency edge scores, and parse trees respectively.\n5\nBenchmarking on the WSJ Corpus\nMost papers of unsupervised dependency parsing report the accuracy of their approaches on the test set\nof the Wall Street Journal (WSJ) corpus. We list the reported accuracy on WSJ in Table 2. It must be\nemphasized that the approaches listed in this table may use different training sets and different external\nknowledge in their experiments, and one should check the corresponding papers to understand such\ndifferences before comparing these accuracies.\nWhile the accuracy of unsupervised dependency parsing has increased by over thirty points in the last\nﬁfteen years, it is still well below that of supervised models, which leaves much room for improvement\nand challenges for future research.\n6\nFuture Directions\n6.1\nUtilization of Syntactic Information in Pretrained Language Modeling\nPretrained language modeling (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), as a new\nNLP paradigm, has been utilized in various areas including question answering, machine translation,\ngrammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for\npretraining and then small data sets of speciﬁc tasks for ﬁnetuning, reducing the difﬁculty of downstream\ntasks and boosting their performance. Current state-of-the-art approaches on supervised dependency\nparsing, such as Zhou and Zhao (2019), adopt the new paradigm and beneﬁt from pretrained language\nmodeling. However, pretrained language models have not been widely used in unsupervised dependency\nparsing. One major concern is that pretrained language models are too informative and may make un-\nsupervised models more prone to overﬁtting. Besides, massive syntactic and semantic information is\nencoded in pretrained language models and how to extract the syntactic part from them is a challenging\ntask.\n6.2\nInspiration for Other Tasks\nUnsupervised dependency parsing is a classic unsupervised learning task. Many techniques developed\nfor unsupervised dependency parsing can serve as the inspiration for studies of other unsupervised tasks,\nespecially unsupervised structured prediction tasks. A recent example is Nishida and Nakayama (2020),\nwho study unsupervised discourse parsing (inducing discourse structures for a given text) by borrowing\ntechniques from unsupervised parsing such as Viterbi EM and heuristically designed initialization.\nUnsupervised dependency parsing techniques can also be used as building blocks for transfer learning\nof parsers. Some of the approaches discussed in this paper have already been applied to cross-lingual\nparsing (He et al., 2019), and more such endeavors are expected in the future.\n6.3\nInterpretability\nOne prominent problem of deep neural networks is that they act as black boxes and are generally not\ninterpretable. How to improve the interpretability of neural networks is a research topic that gains much\nattention recently. For natural language texts, their linguistic structures reveal important information of\nthe texts and at the same time can be easily understood by human. It is therefore an interesting direction\nto integrate techniques of unsupervised parsing into various neural models of NLP tasks, such that the\nneural models can build their task-speciﬁc predictions on intermediate linguistic structures of the input\ntext, which improves the interpretability of the predictions.\n7\nConclusion\nIn this paper, we present a survey on the current advances of unsupervised dependency parsing. We\nﬁrst motivate the importance of the unsupervised dependency parsing task and discuss several related\nresearch areas. We split existing approaches into two main categories, and explain each category in\ndetail. Besides, we discuss several recent trends in this research area. While there is a growing body of\nwork that improves unsupervised dependency parsing, its performance is still below that of supervised\ndependency parsing by a large margin. This suggests that more investigation and research are needed\nto make unsupervised parsers useful for real applications. We hope that our survey can promote further\ndevelopment in this research direction.\nAcknowledgments\nKewei Tu was supported by the National Natural Science Foundation of China (61976139).\nReferences\nTaylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic grammar induction. In ACL.\nTaylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised\nlearning with features. In NAACL.\nYonatan Bisk and Julia Hockenmaier. 2012. Simple robust grammar induction with combinatory categorial gram-\nmars. In AAAI.\nYonatan Bisk and Julia Hockenmaier. 2013. An HDP model for inducing combinatory categorial grammars.\nTACL.\nPhil Blunsom and Trevor Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency\nparsing. In EMNLP.\nJiong Cai, Yong Jiang, and Kewei Tu. 2017. CRF autoencoder for unsupervised dependency parsing. In EMNLP.\nGlenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from\ncorpora. Technical report, Department of Computer Science, Brown University.\nJihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Unsupervised learning of task-speciﬁc tree structures with\ntree-lstms. In AAAI.\nShay B Cohen and Noah A Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsuper-\nvised grammar induction. In NAACL.\nShay B Cohen, Kevin Gimpel, and Noah A Smith. 2008. Logistic normal priors for unsupervised probabilistic\ngrammar induction. In NIPS.\nCaio Corro and Ivan Titov. 2018. Differentiable perturb-and-parse: Semi-supervised parsing with a structured\nvariational autoencoder. In ICLR.\nHal Daum´e III. 2009. Unsupervised search-based structured prediction. In ICML.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In NAACL.\nLong Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Cross-lingual transfer for unsupervised dependency\nparsing without parallel data. In CoNLL.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. 2016. Recurrent neural network gram-\nmars. In NAACL.\nJennifer Gillenwater, Kuzman Ganchev, Joao Grac¸a, Fernando Pereira, and Ben Taskar. 2010. Sparsity in depen-\ndency grammar induction. In ACL.\nJennifer Gillenwater, Kuzman Ganchev, Fernando Pereira, Ben Taskar, et al. 2011. Posterior sparsity in unsuper-\nvised dependency parsing. Journal of Machine Learning Research.\nKevin Gimpel and Noah A Smith. 2012. Concavity and initialization for unsupervised dependency parsing. In\nNAACL.\nEdouard Grave and No´emie Elhadad. 2015. A convex and feature-rich discriminative approach to dependency\ngrammar induction. In ACL-IJCNLP.\nWenjuan Han, Yong Jiang, and Kewei Tu. 2017. Dependency grammar induction with neural lexicalization and\nbig training data. In EMNLP.\nWenjuan Han, Yong Jiang, and Kewei Tu. 2019a. Enhancing unsupervised generative dependency parser with\ncontextual information. In ACL.\nWenjuan Han, Ge Wang, Yong Jiang, and Kewei Tu. 2019b. Multilingual grammar induction with continuous\nlanguage identiﬁcation. In EMNLP.\nJunxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2018. Unsupervised learning of syntactic structure\nwith invertible neural projections. In EMNLP.\nJunxian He, Zhisong Zhang, Taylor Berg-Kirkpatrick, and Graham Neubig. 2019. Cross-lingual syntactic transfer\nthrough unsupervised adaptation of invertible projections. In ACL.\nWilliam P Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing\nwith richer contexts and smoothing. In NAACL.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparametrization with gumble-softmax. In ICLR.\nYong Jiang, Wenjuan Han, and Kewei Tu. 2016. Unsupervised neural dependency parsing. In EMNLP.\nYong Jiang, Wenjuan Han, and Kewei Tu. 2017. Combining generative and discriminative approaches to unsuper-\nvised dependency parsing via dual decomposition. In EMNLP.\nYong Jiang, Wenjuan Han, and Kewei Tu. 2019. A regularization-based framework for bilingual grammar induc-\ntion. In EMNLP-IJCNLP.\nYoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. 2017. Structured attention networks. In ICLR.\nDan Klein and Christopher D. Manning. 2004. Corpus-based induction of syntactic structure: Models of depen-\ndency and constituency. In ACL.\nPhong Le and Willem Zuidema. 2015. Unsupervised dependency parsing: Let’s use supervised parsers. In\nNAACL.\nBowen Li, Jianpeng Cheng, Yang Liu, and Frank Keller. 2019. Dependency grammar induction with a neural\nvariational transition-based parser. In AAAI.\nJun Li, Yifan Cao, Jiong Cai, Yong Jiang, and Kewei Tu.\n2020. An empirical comparison of unsupervised\nconstituency parsing methods. In ACL.\nKai Liu, Yajuan L¨u, Wenbin Jiang, and Qun Liu. 2013. Bilingually-guided monolingual dependency grammar\ninduction. In ACL.\nXuezhe Ma and Fei Xia.\n2014.\nUnsupervised dependency parsing with transferring distribution via parallel\nguidance and entropy regularization. In ACL.\nDavid Mareˇcek and Milan Straka. 2013. Stop-probability estimates computed on a large corpus improve unsuper-\nvised dependency parsing. In ACL.\nDavid Mareˇcek and Zdenˇek ˇZabokrtsk`y. 2012. Exploiting reducibility in unsupervised dependency parsing. In\nEMNLP-IJCNLP.\nRyan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using\nspanning tree algorithms. In EMNLP.\nRyan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers.\nIn EMNLP.\nTahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to\nguide grammar induction. In EMNLP.\nNoriki Nishida and Hideki Nakayama. 2020. Unsupervised discourse constituency parsing using Viterbi EM.\nTACL, 8:215–230.\nHiroshi Noji, Yusuke Miyao, and Mark Johnson. 2016. Using left-corner parsing to encode universal structural\nconstraints in grammar induction. In EMNLP.\nMark A Paskin. 2002. Grammatical bigrams. In NIPS.\nJohn K Pate and Mark Johnson. 2016. Grammar induction from (lots of) words alone. In COLING.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In NAACL.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners. OpenAI Blog, 1(8).\nNoah A Smith and Jason Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In\nACL.\nAnders Søgaard. 2012. Unsupervised dependency parsing without training. Natural Language Engineering,\n18(2):187–203.\nValentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2010a. From baby steps to leapfrog: How “less is\nmore” in unsupervised dependency parsing. In NAACL.\nValentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D Manning. 2010b. Viterbi training\nimproves unsupervised dependency parsing. In CoNLL.\nValentin I Spitkovsky, Hiyan Alshawi, Angel X Chang, and Daniel Jurafsky. 2011a. Unsupervised dependency\nparsing without gold part-of-speech tags. In EMNLP.\nValentin I Spitkovsky, Hiyan Alshawi, and Dan Jurafsky. 2011b. Punctuation: Making a point in unsupervised\ndependency parsing. In CoNLL.\nValentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2011c. Lateen EM: unsupervised training with multi-\nple objectives, applied to dependency grammar induction. In EMNLP.\nValentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2012. Three dependency-and-boundary models for\ngrammar induction. In EMNLP-CoNLL.\nValentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2013. Breaking out of local optima with count\ntransforms and model recombination: A study in grammar induction. In EMNLP.\nKewei Tu and Vasant Honavar. 2011. On the utility of curricula in unsupervised learning of probabilistic gram-\nmars. In IJCAI.\nKewei Tu and Vasant Honavar. 2012. Unambiguity regularization for unsupervised learning of probabilistic\ngrammars. In EMNLP-CoNLL.\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. 2016. Learning to compose\nwords into sentences with reinforcement learning. In ICLR.\nJuntao Yu, Mohab El-karef, and Bernd Bohnet. 2015. Domain adaptation for dependency parsing via self-training.\nIn the 14th International Conference on Parsing Technologies.\nJunru Zhou and Hai Zhao. 2019. Head-driven phrase structure grammar parsing on Penn Treebank. In ACL.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-10-04",
  "updated": "2020-10-04"
}