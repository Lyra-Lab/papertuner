{
  "id": "http://arxiv.org/abs/2010.02343v1",
  "title": "Multi-level Feature Learning on Embedding Layer of Convolutional Autoencoders and Deep Inverse Feature Learning for Image Clustering",
  "authors": [
    "Behzad Ghazanfari",
    "Fatemeh Afghah"
  ],
  "abstract": "This paper introduces Multi-Level feature learning alongside the Embedding\nlayer of Convolutional Autoencoder (CAE-MLE) as a novel approach in deep\nclustering. We use agglomerative clustering as the multi-level feature learning\nthat provides a hierarchical structure on the latent feature space. It is shown\nthat applying multi-level feature learning considerably improves the basic deep\nconvolutional embedding clustering (DCEC). CAE-MLE considers the clustering\nloss of agglomerative clustering simultaneously alongside the learning latent\nfeature of CAE. In the following of the previous works in inverse feature\nlearning, we show that the representation of learning of error as a general\nstrategy can be applied on different deep clustering approaches and it leads to\npromising results. We develop deep inverse feature learning (deep IFL) on\nCAE-MLE as a novel approach that leads to the state-of-the-art results among\nthe same category methods. The experimental results show that the CAE-MLE\nimproves the results of the basic method, DCEC, around 7% -14% on two\nwell-known datasets of MNIST and USPS. Also, it is shown that the proposed deep\nIFL improves the primary results about 9%-17%. Therefore, both proposed\napproaches of CAE-MLE and deep IFL based on CAE-MLE can lead to notable\nperformance improvement in comparison to the majority of existing techniques.\nThe proposed approaches while are based on a basic convolutional autoencoder\nlead to outstanding results even in comparison to variational autoencoders or\ngenerative adversarial networks.",
  "text": "1\nMulti-level Feature Learning on Embedding Layer\nof Convolutional Autoencoders and Deep Inverse\nFeature Learning for Image Clustering\nBehzad Ghazanfari, Fatemeh Afghah\nSchool of Informatics, Computing, and Cyber Systems,\nNorthern Arizona University, Flagstaff, AZ 86001, USA.\nAbstract—This paper introduces Multi-Level feature learning\nalongside the Embedding layer of Convolutional Autoencoder\n(CAE-MLE) as a novel approach in deep clustering. We use\nagglomerative clustering as the multi-level feature learning that\nprovides a hierarchical structure on the latent feature space. It\nis shown that applying multi-level feature learning considerably\nimproves the basic deep convolutional embedding clustering\n(DCEC). CAE-MLE considers the clustering loss of agglom-\nerative clustering simultaneously alongside the learning latent\nfeature of CAE. In the following of the previous works in inverse\nfeature learning, we show that the representation of learning of\nerror as a general strategy can be applied on different deep\nclustering approaches and it leads to promising results. We\ndevelop deep inverse feature learning (deep IFL) on CAE-MLE\nas a novel approach that leads to the state-of-the-art results\namong the same category methods. The experimental results\nshow that the CAE-MLE improves the results of the basic\nmethod, DCEC, around 7% -14% on two well-known datasets\nof MNIST and USPS. Also, it is shown that the proposed deep\nIFL improves the primary results about 9%-17%. Therefore,\nboth proposed approaches of CAE-MLE and deep IFL based\non CAE-MLE can lead to notable performance improvement in\ncomparison to the majority of existing techniques. The proposed\napproaches while are based on a basic convolutional autoencoder\nlead to outstanding results even in comparison to variational\nautoencoders or generative adversarial networks.\nIndex Terms—Multi-level feature learning, Image clustering,\nConvolutional autoencoder, Deep embedding clustering, Inverse\nfeature learning.\nI. INTRODUCTION\nWhile images with labels are limited and expensive, unla-\nbeled images can be obtained much cheaper and on a large\nscale. It is promising to provide a decision-making mechanism\non the huge available unlabeled images or visionary sensory\ninput. Clustering methods as a class of unsupervised learning\napproaches attempt to divide the instances into several groups\nbased on similarity measures without considering their labels.\nClustering methods can be categorized into classical and\ndeep clustering approaches. Deep clustering methods as a\nsub-category of unsupervised representation learning methods\nlearn high-level representative forms of input data. High-\nlevel representative forms are obtained by the layers of deep\nstructures that provide complex non-linear transformations [1].\nDeep clustering shows promising performances especially in\nconfront of the datasets with high dimensions like images.\nThus, we focus on clustering with deep learning approaches\nto cluster image datasets in this paper.\nDeep clustering methods depending on their architectures\nor their loss functions can be categorized into several groups\n[2]. Two of the most known architectures are based on\nautoencoders or deep generative approaches. The deep gen-\nerative approaches include generative adversarial networks\n(GANs) and variational autoencoders (VAEs). The generative\nmodels can produce new samples from the distributions of\nthe obtained clusters. The sample generation can provide a\npreferred performance in comparison to discriminated ones.\nConvolutional Neural Networks (ConvNets) as a network\nstructure has a known capability to learn the latent patterns\nof the images that are used in a variety of architecture for\nclustering or classiﬁcation. ConvNets is one of the most known\nnetwork structure in different deep clustering architectures\nsuch as autoencoders, GANs, and VAEs for image datasets.\nAutoencoder that is based on ConvNets called Convolutional\nautoencoder (CAE).\nClassical clustering methods are utilized in some deep\nclustering approaches in different ways. Classical methods\napply to the latent space, a joint process of feature learning\nand clustering, or they are used for pre-training layers [2].\nMost of the approaches which work on the latent feature\nspaces use k-means as a partitional clustering in which k-\nmeans is a ﬂat approach. There are few papers that use spectral\nor hierarchical clustering in the processing of latent feature\nspace [2]. Agglomerative clustering (AC) provides interesting\nabilities to be used jointly alongside the ConvNets for image\nclustering [3]; however, to the best of our knowledge, a multi-\nlevel feature learning such as AC by merging has not been\napplied on the embedding layer in deep clustering methods.\nThe proposed method in [4] utilizes a tree structure that stands\non splitting the latent feature space.\nDeep embedded clustering algorithms generally work based\non two phases that can be done sequentially or simultaneously.\nThe ﬁrst phase is learning the latent feature space. The second\nphase is feature reﬁnement and cluster assignment on the\nembedded feature space. In this paper, we present and apply\na multi-level feature learning based on merging on CAE\nand the objective function of Deep Convolutional embed-\nded clustering (DCEC) [5], as the basic autoencoder, with\nsome modiﬁcations. The DCEC utilizes the CAE structure to\nlearn an embedded feature space in an end-to-end way. The\nreconstruction loss of the ConvNet and k-means clustering\nloss are minimized by the stochastic gradient descent and\narXiv:2010.02343v1  [cs.CV]  5 Oct 2020\n2\n \n28*28*1 \n14*14*32 \n7*7*64 \n3*3*128 \n1152 \n1152 \nh \n3*3*128 \n7*7*64 \n14*14*3\n28*28*1\nConv1 \nStride=2 \nConv2 \nStride=2 \nConv3 \nStride=2 \nFlatten \n clustering layer \nDeConv3 \nStride=2 \nDeConv2 \nStride=2 \nDeConv1 \nStride=2 \nEncoder \nDecoder \nFig. 1: The network structure of a CAE that proposed in DCEC which learns the latent feature space, Z, and provides the\nclustering layer. The convolutional layers are shown with “Conv” and convolutional transpose layers are shown with “DeConv”.\nThere are fully connected layers in the middle of CAE. We introduce CAE-MLE as the CAE with AC as a multi-level feature\nlearning on the embedding layer.\nbackpropagation.\nWe propose the CAE-MLE approach as a multi-level fea-\nture learning on the embedding layer of CAE. AC acts as the\nmulti-level feature learning on latent feature spaces of CAE\nwhich its loss function is simultaneously applied alongside\nthe reconstruction loss of CAE. Multi-level feature learning\napproach is promising for image clustering since the patterns\nin images generally can be decomposed to several sub-high-\nlevel patterns in which some of these sub-high-level patterns\nare shared among different groups of images. The relations of\nthese sub-high-level patterns or their ratio to each other form\na considerable portion of different groups of images. Thus,\nthe hierarchical structure of a multi-level feature learning is\nmore compatible to learn in a down-top manner such inherent\nstructures of the latent features space among different groups\nof images. We use agglomerative clustering based on ward\nlinkage on the latent feature space as the multi-level feature\nlearning. To the best of our knowledge, this is the ﬁrst work\nin the literature that introduces such a mechanism. Also,\nwe extend deep IFL based on the novel proposed method\nand introduce a novel feature, weight of the closest one,\nthat captures the representation of error of the latent feature\nspace alongside conﬁdence and weight [1]. We evaluate the\ncontributions on two known image data set MNIST and USPS\nin which most of approaches have been evaluated on them.\nIn summary, we proposed two contributions. First, we\npropose CAE-MLE as a multi-level feature learning on the\nembedding layer of CAE. AC does the multi-level feature\nlearning role. Second, we extend the deep IFL based on the\nproposed CAE-MLE. The deep IFL as a deterministic solution\nleads to superior results in comparison to most of the methods\nin the literature even GANs and VAEs. In this paper, we\nintroduce deep IFL while we do not use data augmentation\nand develop this model based on a deterministic approach.\nDeep IFL in comparison to deterministic models that do not\nuse data augmentation provides the state of the art. Deep\nIFL also outperforms variational autoencoders or generative\nadversarial networks that do not use data augmentation except\none approach based on a generative adversarial network in one\ndataset.\nII. RELATED WORKS\nThere are a variety of methods in deep clustering [2].\nThey can be divided into deterministic or generative ones.\nGenerally, the generative approaches like VAEs or GANs lead\nto better result in comparison to deterministic ones. Especially,\nthe approaches that are based on GANs lead to the state\nof the art results in image datasets since they provide and\nuse noise for training too. Also, these clustering approaches\ncan be categorized depending on whether they are using data\naugmentation or noise in their training or not. Clearly, the\napproaches that use data augmentation and noise in their\ntraining lead to better results.\nStacked auto-encoders (SAEs) need to be pre-trained layer-\nper-layer. Then, they are ﬁne-tuned in an end-to-end manner\n[6, 7, 8]. Stacked denoising auto-encoders (SDAEs) [9] corrupt\nthe inputs randomly and deliberately by using noise to provide\nrobustness and better results than SAEs. Inﬁnite ensemble\nclustering (IEC) [10] proposed the marginalized denoising\nauto-encoder to learn the concatenated deep features and apply\nk-means on it for their ﬁnal clustering.\nJoint unsupervised learning (JULE) [3] used ConvNets\nand agglomerative clustering jointly in a recurrent approach.\nClustering convolutional neural network (CCNN) [11] obtains\ninitial cluster centroids. Then, it uses mini-batch k-means that\nupdates the clusters alongside the parameters of ConvNets\nby using a stochastic gradient descent approach [2]. Deep\ntensor kernel clustering (DTKC) [12] considers persevering\nconsistent cluster structure through ConvNets.\nDeep multi-manifold clustering (DMC) uses “deep neural\nnetwork to classify and parameterize unlabeled data which lie\non multiple manifolds” [13].\nDeep embedding clustering (DEC) [7] learns the latent\nfeature space with stacked autoencoder and uses k-means\nclustering with KL divergence based on two steps that are\nrepeated alternately. Improved DEC (IDEC) [8] extends DEC\n3\nby involving the reconstruction loss alongside the clustering\nloss in the objective function. Discriminatively boosted image\nclustering with fully convolutional auto-encoders (DBC) [14]\nand DCEC [5] made extensions of DEC and IDEC by replac-\ning SAEs with CAE respectively in an end-to-end way. In fact,\nDBC like DEC considers clustering loss, and DCEC considers\nthe reconstruction loss and clustering loss jointly like IDEC.\nDeep embedded regularized clustering (DEPICT) [15] is\nbased on CAE with speciﬁc characteristics. First, a multi-\nnomial logistic regression function stacked on top of CAE.\nSecond, regularized functions are used in the clustering objec-\ntive function and reconstruction loss. Third, a noisy encoder is\nused to improve robustness. The proposed method in [16] is an\nextension of DEPICT that uses a discriminative loss function\nto enlarge the inter-cluster variation and using the auxiliary\ndata distribution function of DEC to minimize the intra-cluster\nvariation.\nDeep convolutional embedded clustering algorithm with\ninception like block (DCECI) was introduced in [17] in\nwhich “inception-like block with different types of convo-\nlution ﬁlters” is designed for local structure preservation of\nConvNets. Deep k-means [18] reparametrizes the objective\nfunction continuously to cluster and learn latent feature space\njointly in a better way. Deep autoencoder mixture clustering\n(DAMIC) [19] associates each cluster with an autoencoder.\nThe clustering tries to learn the nonlinear data representation\nand the set of the autoencoder. Deep stacked sparse embed-\nded clustering (DSSEC) [20] attempts to preserve the local\nstructure of ConvNets and sparse characteristics of input data.\nDeep adaptive image clustering (DAC) [21] works based on\na binary pairwise-classiﬁcation problem in which ConvNets\nare used to learn label features of images and then convert\nthem to one-hot vectors while it uses data augmentation. Deep\ncontinuous clustering (DCC) [22] is a CAE that stands on\nrobust continuous clustering that considers clustering as a\ncontinuous optimization. The optimization function is based on\nreconstruction loss, data loss, and pairwise loss. The authors in\n[23] optimize an auto-encoder by using a discriminative pair-\nwise loss function during the auto-encoder pre-training phase.\nClustering-driven deep embedding with pairwise constraints\n(CPAC) [24] is based on a Siamese network.\nSpectralNet [25] is based on the spectral clustering and a\nSiamese network and uses k-means in the last step. Spec-\ntral clustering via ensemble deep autoencoder learning (SC-\nEDAE) [26] combines “the spectral clustering and deep au-\ntoencoder strengths in an ensemble learning framework”. The\nauthors in [27] proposed a two-stage deep density-based image\nclustering (DDC). DDC works based on CAE in the ﬁrst stage\nand then applies t-SNE on the latent feature space. The second\nstage utilizes a density-based clustering.\nStructural deep clustering network (SDCN) [28] proposes\nsome operators to transfer the learned representation by the\nautoencoder to graph convolutional network. Deep weighted\nk-subspace clustering (DWSC) [29] is based on an autoen-\ncoder and weighted k-subspace network. Concrete k-means\n(CKM) [30] have developed a gradient-estimator for the\nnon-differentiable k-means objective by the Gumbel-Softmax\nreparameterization way.\nInformation maximizing self-augmented training (IMSAT)\n[31] works based on a fully connected network and regularized\ninformation maximization to learn a probabilistic classiﬁer.\nThe classiﬁer tries to maximize the mutual information be-\ntween the inputs and cluster assignments. The most effective\nelements in the performance return to a data augmentation\nknown as self-augmented training (SAT). Deep embedded\nclustering with data augmentation (DEC-DA) [32] enhances\ndeep clustering with data augmentation. Data augmentation is\nused in supervised deep learning approaches to improve the\ngeneralization for image datasets. Deep embedded cluster tree\n(DeepECT) [4] is the ﬁrst paper that utilizes a hierarchical\nstructure by a cluster tree rather than a ﬂat clustering strategy.\nDeepECT works based on a hierarchical clustering on embed-\nding that stands on splitting while we believe the latent feature\nspace is learned in CAE should be processed in a multi-level\nfeature learning by merging. The optional data augmentation\nin DeepECT improves performance — DeepECT+DA. These\npapers show the strength of data augmentation, but data\naugmentation is limited mostly to image datasets.\nTwo of the known generative architecture in deep learning\nare GANs and VAEs. VAE models’ regularized the encoding\ndistribution during training that helps the latest feature space\nto provide a proper generative model. Variational deep em-\nbedding (VaDE) [33] and Gaussian Mixture VAE (GMVAE)\n[34] are based on VAEs. Latent tree variational autoencoder\n(LTVAE) [35] assumes that the latent variable follows a tree\nstructure model. It iterates to improve its learning of the\nstructure of data and capture the facets of data. The proposed\nmethod in [36] is based on a Gaussian mixture VAE with graph\nembedding (DGG). In fact, their approach uniﬁes model-\nbased and similarity-based for clustering. The authors in [37]\nproposed a simple, scalable, and stable deep clustering based\non a variational deep clustering. Variational autoencoder with\ndistance (VAED) [38] extends the work in [39] by using KL\ndivergence to provide the probability distributions and using\nBayesian Gaussian mixture model to cluster the latent feature\nspace.\nGANs models constituted of two networks that one of\nthem is generative. The two networks compete and learn\nfrom their competition. Adversarial autoencoder (AAE) [40],\ndeep adversarial clustering (DAC) [41], categorical gener-\native adversarial network (CatGAN) [42], and information\nmaximizing generative adversarial network (InfoGAN) [43]\nare based on GANs. The authors in [44] proposed a “joint\nlearning framework for discriminative embedding and spectral\nclustering” by using a dual CAE that enhanced with improved\nreconstruction loss and mutual information. Adversarial deep\nembedded clustering (ADEC) [45] preserves the data structure\nwith AAE by coordinating the distribution of feature repre-\nsentations with the given prior distribution in which k-means\nworks based on distribution distance metrics.\nIII. THE PROPOSED METHODS\nThe ﬁrst contribution of this paper is CAE-MLE as a novel\ndeep clustering with CAE that uses agglomerative clustering\nbased on ward linkage instead of k-means on the embedding\n4\nlayer. In other words, we use CAE of DCEC instead of a\nfully connected autoencoder of DEC. Also, we consider the\nreconstruction loss and the clustering loss in the objective\nfunction like DCEC that makes clustering have direct effects\nthrough learning the latent feature space. Finally, we extend\nthe deep IFL based on the proposed CAE-MLE and we\nintroduce a novel feature of the representation of the error\nas the second part of contributions.\nA. Convolutional Autoencoders\nWe followed the CAE and the loss functions of DCEC\n[5] that are brieﬂy explained in the following. Autoencoder\nis a known unsupervised learning approach which is a type\nof neural network to learn the latent feature space of data\nbased on a reconstruction loss. Autoencoder is constituted of\nan encoder and a decoder. The encoder is a function z = fφ(x)\nthat maps the input x into a latent representation z and the\ndecoder is a function that reconstructs the latent representation\nto input x′ = gθ(z). Parameters φ and θ denote the weights of\nthe encoder and decoder networks, respectively. They can be\nlearned generally through the minimizing reconstruction loss\nin autoencoders. We used the mean square error as the distance\nmeasure [7]; thus, the optimization objective is deﬁned as\nfollow:\nmin\nφ,θ Lrec = min 1\nn\nn\nX\ni=1\n||xi −gθ(fφ(xi))||2\n2.\nConvNets is one of the most successful architectures in\ndeep learning, especially in computer vision domains. Con-\nvNets learns the high-level features of the images through the\nseries of layers. Convolution layer provides the characteristics\nincluding the weight sharing that reduces the number of weight\nparameters by sharing the weights of depth neurons, local\nconnectivity of the receptive ﬁelds which are capable of learn-\ning the relation among neighboring pixels, and handling shift-\ninvariance in images [46]. A CAE is deﬁned based on applying\nthe Convolutional functions in the above reconstruction loss\nformula [5].\nfφ(x) = σ(x ∗φ) ≡h\ngθ(h) = σ(θ ∗h)\nwhere x and h are the matrices, and ∗is the convolution\noperator.\nDCEC replaces SAEs with CAE. DCEC also optimizes\nboth of the reconstruction loss, Lr, and clustering loss, Lc,\nsimultaneously. A coefﬁcient γ > 0 controls the degree\nof distorting embedded space, Lr, in the combination with\nLc. Lr is calculated based on the input space, x, and the\nreconstructed of that through CAE, x′. The embedded layer\nin CAE is connected to a clustering layer which performs\na soft assignment — it assigns each latent feature space of\nzi of the corresponding input space xi to a soft label qi\nby Student’s t-distribution [47]. The cluster centers {µ}s\n1 are\ntrainable weights to map xi to soft labels qi in which s shows\nthe number of clusters.\nqij =\n(1 + ||zi −µj||2)−1\nP\ni(1 + ||zi −µj||2)−1\nwhere qij is the probability that zi belongs to cluster j [5].\nLc is deﬁned similar to DEC and DCEC [5, 7] in the form\nof Kullback-Leibler (KL) divergence as explained in the fol-\nlowing. To this end, the reconstruction loss of autoencoder is\nadded to the objective and optimized along with the clustering\nloss simultaneously. Thus, the autoencoder preserves the local\nstructure of data and avoids the corruption of feature space\n[5].\nL = Lr + γLc\nLr = |x −x′|2\n2\nLc = KL(P||Q) =\nX\ni\nX\nj\npijlog pij\nqij\nWe used the deﬁned target distribution of [5]:\npij =\nq2\nij/ P\ni qij\nP\nj(q2\nij\nP\ni qij)\nDCEC unlike DEC does not detach the decoder in the re-\nﬁnement of the features and cluster assignments. We followed\noptimization DCEC in the update of the autoencoder weights\nand cluster centers and also updating target distribution.\nB. Multi-level Feature Learning on Embedding Layer of CAE\n(CAE-MLE)\nClassical clustering methods can be classiﬁed to different\ngroups based on their underlying clustering approach. For\ninstance, k-means is a partition-based clustering method, spec-\ntral clustering is a density-based approach, and agglomerative\nclustering is a hierarchical clustering method. Despite the good\nperformance of spectral clustering methods, they cannot be\neasily scalable to large datasets [7].\nHierarchical clustering provides a nested clustering through\nmerging or splitting clusters consecutively. Agglomerative\nclustering methods merge two clusters with the largest afﬁnity\nbased on the similarity measures iteratively till reaching a\nstop criterion — a bottom-up way of hierarchical clustering.\nAgglomerative clustering ﬁnds two clusters Ca and Cb based\non an afﬁnity function, A, which measures the similarity of\ntwo clusters by afﬁnity measures. There are several afﬁnity\nmeasures based on some linkage criteria including single\nlinkage, complete linkage, average linkage, and ward linkage.\nWard’s linkage [48] considers all clusters to minimize the\nincrease of the sum of squared distance within the clusters\n— minimizing the variance of the clusters that being merged\nwhich makes it similar to an agglomerative version of k-means\nobjective function. The ward linkage is calculated as follows:\n∆(a,b) =\nnanb\nna + nb\n∥(µa, µb) ∥2\nwhere µa and µb are the centroids of two clusters ca and cb,\nand na and nb are the number of instances of the clusters.\n5\nStep 1\nRun:\nInner\nFolding\nInput data\n1\n2\nr\nFig. 2: Step 1, inner folding, the input data is partitioned to r\nnon-overlapping folds or partitions. Then, a loop with r runs is\napplied, where in each run, one fold is considered as an inner-\ntest and r −1 remaining folds are considered as inner-train.\nThe inner-test fold in each run is colored with green.\nThe hierarchical structure provided by agglomerating clus-\ntering with ward linkage is more compatible with the image\ndatasets. Image datasets are constituted of the signiﬁcant sub-\npatterns which are shared among different groups. CAE learns\nthe latent feature space of the images in which the latent\nfeature spaces are the signiﬁcant sub-patterns of the images\nthat are shared among different groups. Now, AC based on\nward linkage is applied on the latent feature space that is more\ncapable to cluster based on the combination of the shared sub-\npatterns, the ratio, and the relations among sub-patterns. Thus,\nthe latent feature space captures more distinguished patterns\nand the resultant clustering is more powerful.\nWhen CAE is combined with agglomerative clustering as a\nmulti-level feature learning to learn the latent feature space\nsimultaneously with clustering, the embedded features can\ncapture more sophisticated sub-patterns. The reason is that\nthe agglomerative clustering can distinguish more different and\nsophisticated groups based on a combination of the main latent\nfeature spaces that provides a space for CAE to learn more\nkey patterns in the latent feature space.\nSeveral groups can be built based on some of these sub-\npatterns through levels like 2 and 3 that share the convex or 1\nand 4 that share the main vertical line and the smaller oblique\nline, or 6 and 9 that share the circle. Thus, agglomerative clus-\ntering is used in the deep clustering that leads the embedding\nto learn more key and distinguished patterns in latent feature\nspaces. Agglomerative clustering based on ward linkage is\ncompatible with this fact that the hierarchical structure can\nhelp to learn the sophisticated patterns that some sub-patterns\namong them are common. In fact, the structure of multi-level is\ninvolved in learning to capture the relation among sub-patterns\nand different groups.\nC. Deep Inverse Feature Learning based on CAE-MLE\nDeep IFL [1] introduces new perspectives about the error\nand developed in a general form by a basic deep clustering\nthat is based on a fully-connected autoencoder. First, in this\npaper, we proposed CAE-MLE as a deep clustering approach\nfor image clustering. Then, we extend the previous work deep\nIFL by replacing CAE-MLE instead of the fully-connected\nautoencoder and introducing a new feature that can capture\nthe representation of error.\nThe error in most machine learning methods including both\nsupervised and unsupervised approaches is calculated in the\nsame traditional way of the difference between the predicted\nand true labels in supervised learning and as the clustering\nlosses in clustering ones. While the representation learning\nof error introduces that error has other characteristics. The\ncharacteristics describe how instances are placed in the feature\nspace in relation to other instances of different clusters which\nis measured by the level of their similarities and the degree of\naccuracy of assumptions. In other words, this new perspective\nconsiders the error as a dynamic quantity depending on other\ninstances and clusters. IFL methods [49, 1] generates the error\nand captures the relationships among the instances and the\npredicted clusters in clustering. Then, they transform the error\nas high-level features to represent the characteristics of each\ninstance related to the available clusters. The details of the\nnovel perspective of error in deep inverse feature learning in\ncomparison to the error in the literature of machine learning\nare discussed in [1].\nIn this work, we extend our previously proposed deep IFL\n[1] based on the proposed CAE-MLE for the image datasets.\nCAE can learn a latent feature space which is much smaller\nthan the original feature set. CAE and agglomerative clustering\nlead to a better deep embedding clustering since agglomerative\nclustering handles some portion of the pattern learning in its\nown structure. In other words, CAE does a nonlinear mapping\nof the original features to a latent feature space, Z, and\nagglomerative clustering clusters the latent space to s clusters\nby using cluster assignment hardening to generate centroids,\nµ, of the clusters. The network structure of CAE is shown in\nFigure 1.\nThe deep IFL is described in [1] based on four main steps\nthat we follow them based on CAE-MLE instead of DEC with\nsome modiﬁcations. We brieﬂy explain them in the following\n— for more details and formal deﬁnitions, please refer to [1].\nStep 1- Inner folding: Inner folding performs a trial process to\nprovide similar conditions to capture error characteristics for\nall instances. Inner folding is similar to k-fold cross-validation,\nwhere the data set is partitioned to r non-overlapping folds.\nIn each run, one fold of data is considered as inner-test and\nother folds as inner-train. The process is shown in Figure 2.\nThe objective of the inner folding process is to obtain the\nrepresentation of inner-test instances based on the clustered\nrepresentation of inner-train data.\nStep 2- Learning the clustered representation of the inner-\ntrain: The inner-train data is fed as input to the CAE-MLE\nin each round of the inner-training process. CAE-MLE learns\nthe latent feature space, Ztrain, and clustering the inner-train\ninstances with agglomerative clustering in which the objective\nconsiders both of the reconstruction loss, Lr, and the clustering\nloss, Lc. The encoder of the CAE-MLE is used in step 3 and\nthe clusters and their centroids are utilized in step 4 of the\nIFL. Step 2 and 3 are depicted in Figure 3.\nStep 3- Extracting the latent features of inner-test: The\ninner-test instances are given as input to the decoder of the\nCAE-MLE that is trained with inner-train instances to obtain\n6\nEncoder\nXi\nZi\nLatent\nfeatures\nClustering\nLayer\nInner train\nInner test\nAC \n(ward linkage)\nStep 3\nEncoder\nXi\nZi\nLatent\nfeatures\nX'i\nDecoder\nClustering\nLayer\nStep 2\nAC \n(ward linkage)\nRun c\nFig. 3: Steps 2 and 3: Learning the clustered representation of the inner-train data as step 2 by using the CAE-MLE to train\nthe autoencoder and cluster the latent feature space and obtain their centers. The trained encoder is used in step 3 to extract\nthe latent features of inner-test and the centroids of inner-test as step 3.\nthe latent features of inner-test and the clustered representation\nof them. In other words, the encoder part of autoencoder\nwhich was trained in the ﬁrst phase of CAE-MLE, fφ, is\nfed with the inner-test instances of the run, XJ\ninner test, to\nobtain the corresponding latent features of inner-test instances,\nZJ\ninner test, and the centroids of the clusters, {µ}s\n1, as shown\nin step 3 in Figure 3.\nStep 4- Feature learning: In this step, the clusters’ centroids\nof the inner-train which were calculated in step 2 and the latent\nfeatures of inner-test which were obtained in step 3 are used\nhere to measure several features for each inner-test instance.\nThe relations between the latent features of the inner-test and\nthe clusters’ centroids of the inner-train are calculated and\nconsidered as new features for that inner-test sample. Thus,\nwe extract a new set of features for each inner-test instance\nduring each run of inner folding process.\nIn other words, the representation of one set, inner-test\ninstances or test instances, is evaluated based on the clustered\nrepresentation of other set instances, inner-train, or training.\nWe assign each inner-test instance to all possible clusters and\nmeasure the representation of such assignment on the clustered\nrepresentation and map that in the form of features as the\ncorresponding error representation of that inner-test instance\nfor that cluster. Here, we introduce three metric, conﬁdence,\nweight, and weight of the closest one which are measured per\ninstance of each cluster, {ci}s\ni=1.\nConﬁdence is the ratio of the number of elements in a\ncluster in which the inner-test instance has the closest distance\nto its center to the number of all instances in all clusters.\nThus, we need to ﬁnd the closest cluster by measuring the\ndistance between the latent space of the instance to the\ncentroids of the clusters which are obtained from the CAE-\nMLE and calculating the number of instances which belongs\nto that cluster to the number of all instances. If dist(zj, cb) =\nmin(dist(zj, {ci}s\ni=1)), confidence(xj) = |cb|\n|C| in which zj\nis the corresponding latent feature of xj. Conﬁdence is one\nAlgorithm 1: Deep Inverse feature learning based on\nCAE-MLE for clustering.\n1: Input: X.\n2: Output: Features of error representation for X.\n3: Step 1: Inner folding // Partition X to r folds and\nperform r runs, where in each round, r −1 folds are set\nas the inner-training samples and the remaining one fold\nis set as inner-test instances.\n4: for j = 1 : r do\n5:\ninner-test = Jth fold of X — XJ\nInner test.\n6:\ninner-train = All folds except Jth fold of X —\nXJ\nInner train.\n7:\nStep 2: Learning the clustered representation of\nthe inner-train\n8:\nInput of step 2: inner-train, XJ\nInner train.\n9:\n2.1) Applying CAE-MLE on inner-train,\nXJ\nInner train.\n10:\nOutputs of step 2: Encoder of CAE-MLE, fφ,\nclusters, {ci}s\ni=1, and centroid of clusters — {µi}s\ni=1.\n11:\nStep 3: Extracting the latent features of inner-test\n12:\nInputs of step 3: Encoder of CAE-MLE, fφ,\nobtained in step 2, and inner-test — XJ\nInner test.\n13:\n3.1) Feeding encoder with inner-test, XJ\nInner test.\n14:\nOutput of step 3: Latent features of inner-test —\nZJ\nInner test, and their corresponding centroid of\nclusters — {µi}s\ni=1.\n15:\nStep 4: Feature learning\n16:\nInputs of step 4: Clusters, {ci}s\ni=1, and their\ncentroid, {µi}s\ni=1, and latent features of inner-test,\nZJ\nInner test.\n17:\n4.1) Calculating “conﬁdence”, “weight”, and “weight\nof the closest one” for each inner-test instance.\n18:\nOutput of step 4: Features of error representation\nfor the inner-test or Jth fold of X — XJ\nInner test.\n19: end for\n7\nTABLE I: Descriptions of the data sets.\nDATA SETS\n#INSTANCES\n#FEATURES\n#CLASSES\nMNIST\n70000\n26*26 (784)\n10\nUSPS\n9298\n16*16 (256)\n10\nfeature.\nWeight of belonging an instance in inner-test to a cluster on\ninner-train is the distance of the latent feature of the element to\nthe center of the cluster, µ. We calculate the weight for each\ninner-test and the centers of all clusters, {µ}s\ni=1, of inner-\ntrain. In other words, weight is a vector in length of number\nof clusters in which weight(xj, {µ}s\ni=1)= dist (zj, {µ}s\ni=1).\nThus, the number of features of weight is the same as the\nnumber of clusters.\nWeight of the closest one is the distance of the latent feature\nof each element of inner-test to a center of the cluster on\ninner-train instances, µ, that has the lowest distance to it.\nWe calculate the Weight of the closest one for each inner-\ntest and the closest center of all clusters, {µ}s\ni=1, of inner-\ntrain. In other words, weight of the closest one is a number in\nwhich weight−closest(xj, {µ}s\ni=1)= mini(dist(zj, {µ}s\ni=1).\nWeight of the closest one is one feature.\nIV. EXPERIMENTAL RESULTS\nIn this paper, we propose our approach based on an au-\ntoencoder as a discriminative model in a generic and basic\nmanner. The reason that we select a discriminative autoencoder\nis that we want to focus on showing the performance of the\ncontributions including “CAE-MLE” and “deep IFL based on\nCAE-MLE” while they are independent strategies and can be\napplied on deterministic or generative models. We evaluate\nthe performance of the proposed feature learning model and\ncompared the proposed method with both discriminative and\ngenerative ones. It is shown the CAE-MLE and deep IFL based\non CAE-MLE lead to promising results while stands on a basic\narchitecture. We use AC based on ward linkage that is similar\nto k-means objective function while providing a hierarchical\nmechanism for multi-level feature learning.\nWe evaluated the proposed method on two common image\ndatasets that are described in Table I. These data sets are two\nhandwritten digit image data sets (USPS 1 and MNIST [74]).\nThe details of the data sets and the pre-processing process are\ndescribed in the following:\nMNIST: MNIST data set [74] includes 70000 handwritten\ndigits with size 28*28 pixels, in which the range of the values\nis between [0, 255]. The value of each pixel is divided by the\nmax values to normalize them to an interval [0, 1].\nUSPS: USPS data set contains 9298 gray-scale handwritten\ndigits with size 16*16 pixels which are normalized to [-1, 1].\nThe common unsupervised clustering accuracy, denoted by\nACC, is used to evaluate the performance of this model as\ndeﬁned as following:\nACC = max\nm\nPn\ni=1 1{yi = m(ci)}\nn\n(1)\n1http://www.cs.nyu.edu/˜roweis/data.html\nwhere ci is the cluster assignment for instance i determined\nby the clustering approach, yi is the ground-truth label or the\nlabel of ith instance, and m is the mapping function that ranges\nover all possible one-to-one mapping between ci and yi.\nNormalized Mutual Information (NMI) [75] that are deﬁned\nas following:\nNMI(Y,C) =\nI(Y,C)\n1\n2[H(Y ) + H(C)]\n(2)\nwhere I is the mutual information metric and H is the entropy.\nThe performance of the proposed model is compared with\nboth the classical clustering methods such as k-means [32],\nspectral clustering, agglomerative clustering for the ward and\naverage linkage criterion [33], and Gaussian mixture model\n(GMMs) as well as deep clustering learning approaches\nincluding auto-encoder (AE) [53], denoising auto-encoder\n[9], autoencoder+k-means, local discriminant models and\nglobal integration (LDMGI) [59], sparse auto-encoder (SAE)\n[54], autoencoder + LDMGI [7], deconvolutional network\n(DeCNN) [55], auto-encoding variational bayes (AEVB) [57],\nstacked what-where auto-encoder (SWWAE) [56], autoen-\ncoder+GMM, autoencoder+SEC [7], DEC, IDEC [8], JULE\n[3], DCEC [5], DBC [14], DAE [9], DCN [65], DEPICT [15],\nIEC [10], DCC [22], CCNN [11], SDCN [28], CKM [30],\nDAMIC [19], k-DAE [66], DWSC [29], DCECI [17], DTKC\n[12], S3VDC [37], CPAC-VGG [24], FcDEC [32] , ConvDEC\n[32], FcIDEC [32], ConvIDEC [32], FcDCN [32], SC-EDAE\n[26], DDC [27], and SpectralNet [25].\nThe approaches that are based on generative models GANS\nor VAEs including GAN [62], adversarial autoencoder (AAE)\n[40], Conv-CatGAN [42], InfoGAN [43], ClusterGAN [64],\nADEC [45], Conve-ADEC [45], VAE+GMM, GMVAE [34],\nVaDE [60], LTVAE [35], DGG [36], and VAED [38]. The\napproaches that are using data augmentation (DA) as the\nmain part of their models including IMAST (VAT) [31], DAC\n[21], FcDEC-DA [32], ConvDEC-DA [32], FcIDEC-DA [32],\nConvIDEC-DA [32], FcDCN-DA [32], DDC-DA [45], Conve-\nADEC-DA [45], and DeepECT-DA [4].\nHere, we compare the performance of the proposed CAE-\nMLE and deep IFL based on CAE-MLE, deep IFL (CAE-\nMLE), with the state-of-the-art techniques in clustering based\non ACC and NMI. It should be noted that the deep IFL\nfeatures are highly abstract, (i.e., only 2+s in which s shows\nthe number of clusters). In MNIST and USPS, the numbers\nof primary features are 784 and 256, respectively while the\nnumber of deep IFL features is 12 that are added to the primary\nfeatures. In this paper, we set the number of runs for the inner\nfolding process to 10 (i.e., r = 10) for all experiments and\nwe repeat the experiments 5 times to report the average of\nthe results. In this paper, we used the autoencoder network\nthat is based on convolution layers and convolution transpose\nlayers in 9 layers that their dimensions are shown in Figure 1\n— following DCEC [5]. The learned features are normalized\nbetween [-2.5, 2.5].\nTables II and III show the results of a varieties of approaches\nincluding deterministic models, VAE, or GANs for MNIST\nand USPS correspondingly. To have a better comparison of the\nsimilar approaches, we bring the approaches that DA or noise\n8\nTABLE II: The ACC and NMI of a variety of clustering approaches are shown on MNIST.\nMNIST\nMNIST\nClustering Models\nNMI\nACC\nClustering Models\nNMI\nACC\nK- means1 [50]\n49.97\n57.23\nJULE\n[3]\n91.30\n96.40\nSC1 [51]\n66.26\n69.58\nSpectralNet [25]\n92.4\n97.1\nAC1 [52]\n60.94\n69.53\nDBC [14]\n91.7\n96.4\nAE1 [53]\n72.57\n81.23\nDDC [27]\n93.2\n96.5\nSAE1 [54]\n75.65\n82.71\nFcDEC [32]\n87.5\n91.6\nDAE1 [9]\n75.63\n83.16\nConvDEC [32]\n88.8\n90.0\nDeCNN1 [55]\n75.77\n81.79\nFcIDEC [32]\n87.2\n91.2\nSWWAE1 [56]\n73.60\n82.51\nConvIDEC [32]\n89.1\n90.1\nAEVB1 [57]\n73.64\n83.17\nFcDCN [32]\n84.9\n90.1\nGMM2\n–\n53.73\nMethod [23]\n–\n97.4\nAE+GMM2\n–\n82.18\nSC-EDAE [26]\n87.93\n93.23\nSEC3 [58]\n77.9\n80.4\nS3VDC [37]\n–\n93.60\nLDMGI3 [59]\n80.2\n84.2\nDCECI [17]\n92.84\n96.81\nDEC1 [7]\n77.16\n84.30\nVAE [57]+GMM 2\n–\n72.94\nIDEC [8]\n86.72\n88.06\nVaDE [60]\n–\n94.46\nSAE+k-means4\n79.27\n84.90\nLTVAE [35]\n–\n86.32\nDCEC [5]\n88.49\n88.97\nVAED [38]\n81.9\n88.75\nDC-Kmeans [61]\n74.48\n80.15\nGAN 1 [62]\n76.37\n82.79\nDC-GMM [61]\n83.18\n85.55\nAAE 2 [40]\n–\n83.48\nMethod [63]\n82.4\n88.2\nGMVAE [34]\n–\n82.31\nAutoencoder+ k-means5\n–\n81.84\nMethod [16]\n91.9\n96.9\nAutoencoder+ LDMGI5\n–\n83.98\nInfoGAN [43]\n84.00\n87.00\nAutoencoder+ SEC5\n–\n81.56\nClusterGAN [64]\n89.00\n95.00\nDCN [65]\n81.00\n83.00\nConv-CatGAN [42]\n–\n95.73\nIMSAT(RPT) [31]\n–\n89.6\nADEC [45]\n94.3\n96.5\nDSSEC [20]\n85.7\n87.7\nConve-ADEC [45]\n95.5\n97.2\nk-DAE [66]\n86.00\n88.00\nDCAE [67]\n84.97\n92.14\nDWSC [29]\n88.9\n94.8\nMethod [68]\n92.3\n96.1\nIEC [10]\n54.20\n60.86\nDeep IFL (DEC) [1]\n–\n95.79\nDAMIC [19]\n87.00\n89.00\nCAE-MLE\n92.57\n96.77\nCKM [30]\n81.4\n85.4\nDeep IFL (CAE-MLE)\n93.09\n97.50\nDeepECT [4]\n–\n82.00\n1 Results taken from [21].\n2 Results taken from [60].\n3 Results taken from [15].\n4 Results taken from [5].\n5 Results taken from [7].\nreconstructions are the superiority parts of them in separated\ntables — Tables V and IV. Clearly, there are some approaches\nbased on GANs that use noise and DA or some approaches that\nare based on DA in the ﬁrst tables of datasets. In these tables,\n“–” refers to the cases in which the results were not reported.\nIn Tables IV and V, it can be seen that most approaches that\nare based on DA or noise reconstructions lead to promising\nresults while such techniques limit the generality of their\napproaches on different datasets.\nAs shown in Table II, the idea of using multi-level feature\nlearning on CAE, CAE-MLE, improves NMI and ACC of\nDCEC around 4% and 7% that makes it close to methods\nthat are even based on GAN or VAE. Deep IFL (CAE-MLE)\nleads to the state of the art among deterministic and generative\nmodels including GANs or VAE. In Table IV, it can be seen\nmost approaches that are based on DA or noise reconstructions\nlead to promising results while such techniques limit the\ngenerality of their approaches on different kinds of datasets\nlike text [1].\nAs shown in Table III, CAE-MLE improves NMI and ACC\nof DCEC around 5% and 5% for USPS. It is shown that deep\nIFL (CAE-MLE) provides more improvement, especially in\ncases where the dataset is not as large as MNIST. Deep IFL\n(CAE-MLE) provides superior performance over deterministic\nand generative models except for Conve-ADEC as a GAN-\nbased model with 2% differences. It can be justiﬁed that\nConve-ADEC as a generative model can show the superiority\nof generation of data especially for small datasets.\n9\nTABLE III: The ACC and NMI of a variety of clustering approaches are shown on USPS.\nUSPS\nUSPS\nClustering Models\nNMI\nACC\nClustering Models\nNMI\nACC\nk-means1\n65.9\n69.4\nFcDEC [32]\n78.4\n76.3\nLDMGI [59]\n86.5\n81.5\nConvDEC [32]\n82.2\n78.6\nGMM 2\n21.07\n29.00\nFcIDEC [32]\n79.9\n77.1\nDAE+k-means 2\n52.03\n59.55\nConvIDEC [32]\n82.7\n79.2\nDAE+GMM 2\n59.67\n64.22\nFcDCN [32]\n72.2\n69.9\nDAEC 2 [39]\n54.49\n61.11\nDeepECT [4]\n–\n72.00\nDEC 2\n61.91\n62.46\nk-DAE [66]\n80.00\n77.00\nDC-k-means [61]\n57.37\n64.42\nDeep k-Means [18]\n77.6\n75.70\nDC-GMM [61]\n69.39\n64.76\nDTKC [12]\n73.00\n70.00\nSC-ST3 [51]\n72.6\n30.8\nDCECI [17]\n79.06\n81.61\nSC-LS3 [69]\n68.1\n65.9\nSC-EDAE [26]\n83.17\n81.78\nAC-GDL3 [70]\n82.4\n86.7\nVAED [38]\n62.33\n76.13\nAC-PIC3 [71]\n84.00\n85.5\nDDC [27]\n96.7\n91.8\nSEC3 [58]\n51.1\n54.4\nJULE2 [3]\n91.3\n95.0\nDEC3 [7]\n58.6\n61.9\nMethod [72]\n–\n89.23\nSAE+k-means4\n57.27\n61.65\nDDC [27]\n96.7\n91.8\nCAE+k-means4\n57.27\n61.65\nMethod [63]\n86.8\n92.6\nDCEC [5]\n82.57\n79.00\nADEC [45]\n80.6\n81.4\nIDEC [8]\n78.64\n76.05\nConve-ADEC [45]\n92.7\n97.1\nCKM [30]\n70.7\n72.1\nNSC-AGA [73]\n77.27\n72.56\nAGDL [70]\n–\n82.4\nDeep IFL (DEC)\n–\n84.55\nDBC [14]\n72.4\n74.3\nCAE-MLE\n87.92\n93.53\nCPAC-VGG [24]\n92.00\n87.00\nDeep IFL (CAE-MLE)\n92.26\n96.89\n1 Results taken from [63].\n2 Results taken from [61].\n3 Results taken from [15].\n4 Results taken from [5].\nAmong the methods mentioned in Table V, DeepECT which\nuses hierarchical processing on the latent feature space based\non splitting is the most similar approach to the proposed CAE-\nMLE. However, the proposed CAE-MLE leads to considerably\nbetter results compared to DeepECT –about 15% and 21%\nfor MNIST and USPS, respectively. It justiﬁes the proposed\nmerging point of the latent feature space for different groups\nin CAEs for image clustering. This improvement is achieved\nbecause of the proposed deep IFL as a general representation\nlearning strategy which can be even applied on the basic\nclustering methods and leads to the state of the art even in\ncomparison to GANs and VAE models.\nIn summary, we can conclude that the CAE-MLE and deep\nIFL based on CAE-MLE provide new perspectives that offer\nconsiderable accuracy improvement based on the extensive\nexperimental results. We showed the idea of using AC on\nthe CAE instead of k-means that improves the results of\nDCEC, unsupervised clustering accuracy, around 8% and\n14% in USPS and MNIST datasets correspondingly. Also,\nwe showed deep IFL based on CAE-MLE provides superior\nperformance than most papers in the literature even better than\ngenerative ones. Deep IFL leads to the state of the art among\nthe discriminative approaches and its results are just below\none method on one dataset that stands on a GAN. We have\nbeen focused on the improvement that AC provides on the\nembedding layer of the CAE and the representation of error\nrather than attempting to develop the most optimal network\narchitecture.\nV. CONCLUSION\nIn this paper, we introduce CAE-MLE as a novel multi-level\nfeature leaning on the embedding layer of CAE. We use AC as\nthe multi-level feature learning that works simultaneous with\nlearning latent feature space — L = γLc + Lr. AC based on\nward linkage is similar to k-means objective function while\nproviding a hierarchical mechanism for multi-level feature\nlearning. Image datasets patterns are generally constituted of\nsub-patterns in which these sub-patterns are shared among\ndifferent groups; thus, AC’s structure can capture such re-\nlation to learn precisely different groups for image datasets.\nIn other words, CAE-MLE leads the embedding layer to\ncapture more key and distinguished patterns in the latent\nfeature space. It is shown in the experimental results that\nthe proposed contribution considerably improves the results\n10\nTABLE IV: The ACC and NMI of clustering approaches that\nthe network structure has tuned per dataset or or DA or noise\nreconstruction is the superiority part of their mechanisms on\nMNIST.\nMNIST\nClustering Models\nNMI\nACC\nDEPICT [15]\n91.7\n96.5\nDAC [21]\n93.51\n97.75\nMethod [44]\n94.1\n97.8\nDGG [36]\n–\n97.58\nIMSAT(VAT) [31]\n–\n98.4\nDeepECT-DA [4]\n–\n94.00\nFcDEC-DA [32]\n95.9\n98.5\nConvDEC-DA [32]\n96.0\n98.5\nFcIDEC-DA [32]\n96.2\n98.6\nConvIDEC-DA [32]\n95.5\n98.3\nFcDCN-DA [32]\n96.2\n98.6\nDDC-DA [27]\n94.1\n96.9\nConve-ADEC-DA [45]\n97.1\n99.3\nTABLE V: The ACC and NMI of clustering approaches that\nthe network structure has tuned per dataset or DA or noise\nreconstruction is the superiority part of their mechanisms on\nUSPS.\nUSPS\nClustering Models\nNMI\nACC\nMethod [44]\n85.7\n86.9\nDEPICT [15]\n92.7\n96.4\nFcDEC-DA [32]\n94.5\n98.0\nConvDEC-DA [32]\n96.2\n98.7\nFcIDEC-DA [32]\n95.4\n98.4\nConvIDEC-DA [32]\n95.5\n98.4\nFcDCN-DA [32]\n92.9\n96.9\nDDC-DA [45]\n94.1\n96.9\nConve-ADEC-DA [45]\n96.6\n99.1\nDeepECT-DA [4]\n–\n89.00\nDDC-DA [27]\n97.7\n93.9\nof the basic method, DCEC, on two known datasets. The\nstrategy of inverse feature learning can be implemented by\ndifferent learning approaches as tactics [49, 1]. We extend\nthe deep IFL in [1] based on the proposed CAE-MLE and\nwe introduce a novel feature, weight of the closest one, that\ncaptures the representation of error in this paper. While deep\nIFL (CAE-MLE) is proposed as a discriminative model and\nin a generic manner, deep IFL leads to promising results even\nin comparison to GANs and VAEs based approaches. It is\nshown that deep IFL (CAE-MLE) provides the state-of-the-\nart results among discriminative, GANs, and VAEs on MNIST\nthat do not use data augmentation or noise reconstruction as\nthe superiority parts. There is just one approach based on\nGANs for USPS with 2% difference that shows better results\nthan deep IFL (CAE-MLE).\nACKNOWLEDGMENT\nWe gratefully thank the high performance computing team\nof Northern Arizona University. We acknowledge the support\nof NVIDIA Corporation with the donation of the Quadro\nP6000 used for this research.\nREFERENCES\n[1] B. Ghazanfari and F. Afghah, “Deep inverse feature\nlearning: A representation learning of error,” arXiv\npreprint arXiv:2003.04285, 2020.\n[2] E. Min, X. Guo, Q. Liu, G. Zhang, J. Cui, and J. Long,\n“A survey of clustering with deep learning: From the\nperspective of network architecture,” IEEE Access, vol. 6,\npp. 39 501–39 514, 2018.\n[3] J. Yang, D. Parikh, and D. Batra, “Joint unsupervised\nlearning of deep representations and image clusters,” in\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2016, pp. 5147–5156.\n[4] D. Mautz, C. Plant, and C. B¨ohm, “Deep embedded\ncluster tree,” in 2019 IEEE International Conference on\nData Mining (ICDM).\nIEEE, 2019, pp. 1258–1263.\n[5] X. Guo, X. Liu, E. Zhu, and J. Yin, “Deep clustering with\nconvolutional autoencoders,” in International conference\non neural information processing.\nSpringer, 2017, pp.\n373–382.\n[6] G. E. Hinton and R. R. Salakhutdinov, “Reducing the\ndimensionality of data with neural networks,” science,\n313, no. 5786, pp. 504–507, 2006.\n[7] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised\ndeep embedding for clustering analysis,” in International\nconference on machine learning, 2016, pp. 478–487.\n[8] X. Guo, L. Gao, X. Liu, and J. Yin, “Improved deep\nembedded clustering with local structure preservation.”\nin IJCAI, 2017, pp. 1753–1759.\n[9] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-\nA. Manzagol, “Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a local de-\nnoising criterion,” Journal of machine learning research,\nvol. 11, no. Dec, pp. 3371–3408, 2010.\n[10] H. Liu, M. Shao, S. Li, and Y. Fu, “Inﬁnite ensem-\nble clustering,” Data Mining and Knowledge Discovery,\nvol. 32, no. 2, pp. 385–416, 2018.\n11\n[11] C.-C. Hsu and C.-W. Lin, “Cnn-based joint clustering\nand representation learning with feature drift compensa-\ntion for large-scale image data,” IEEE Transactions on\nMultimedia, vol. 20, no. 2, pp. 421–429, 2017.\n[12] D. J. Trosten, M. C. Kampffmeyer, and R. Jenssen, “Deep\nimage clustering with tensor kernels and unsupervised\ncompanion objectives,” arXiv preprint arXiv:2001.07026,\n2020.\n[13] D. Chen, J. Lv, and Y. Zhang, “Unsupervised multi-\nmanifold clustering by learning deep representation,” in\nWorkshops at the thirty-ﬁrst AAAI conference on artiﬁcial\nintelligence, 2017.\n[14] F. Li, H. Qiao, and B. Zhang, “Discriminatively boosted\nimage clustering with fully convolutional auto-encoders,”\nPattern Recognition, vol. 83, pp. 161–173, 2018.\n[15] K. Ghasedi Dizaji, A. Herandi, C. Deng, W. Cai, and\nH. Huang, “Deep clustering via joint convolutional au-\ntoencoder embedding and relative entropy minimization,”\nin Proceedings of the IEEE international conference on\ncomputer vision, 2017, pp. 5736–5745.\n[16] X. Shaol, K. Ge, H. Su, L. Luo, B. Peng, and D. Li,\n“Deep discriminative clustering network,” in 2018 Inter-\nnational Joint Conference on Neural Networks (IJCNN).\nIEEE, 2018, pp. 1–7.\n[17] Q. Wang, J. Xu, R. Li, P. Qiao, K. Yang, S. Li, and\nY. Dou, “Deep image clustering using convolutional\nautoencoder embedding with inception-like block,” in\n2018 25th IEEE International Conference on Image\nProcessing (ICIP).\nIEEE, 2018, pp. 2356–2360.\n[18] M. M. Fard, T. Thonet, and E. Gaussier, “Deep k-\nmeans: Jointly clustering with k-means and learning\nrepresentations,” arXiv preprint arXiv:1806.10069, 2018.\n[19] S. E. Chazan, S. Gannot, and J. Goldberger, “Deep\nclustering based on a mixture of autoencoders,” in 2019\nIEEE 29th International Workshop on Machine Learning\nfor Signal Processing (MLSP).\nIEEE, 2019, pp. 1–6.\n[20] J. Cai, S. Wang, and W. Guo, “Stacked sparse auto-\nencoder for deep clustering,” in 2019 IEEE Intl Conf\non Parallel & Distributed Processing with Applications,\nBig Data & Cloud Computing, Sustainable Computing\n& Communications, Social Computing & Networking\n(ISPA/BDCloud/SocialCom/SustainCom).\nIEEE, 2019,\npp. 1532–1538.\n[21] J. Chang, L. Wang, G. Meng, S. Xiang, and C. Pan,\n“Deep adaptive image clustering,” in Proceedings of the\nIEEE international conference on computer vision, 2017,\npp. 5879–5887.\n[22] S. A. Shah and V. Koltun, “Deep continuous clustering,”\narXiv preprint arXiv:1803.01449, 2018.\n[23] E. Tzoreff, O. Kogan, and Y. Choukroun, “Deep dis-\ncriminative latent space for clustering,” arXiv preprint\narXiv:1805.10795, 2018.\n[24] S. Fogel, H. Averbuch-Elor, D. Cohen-Or, and J. Gold-\nberger, “Clustering-driven deep embedding with pairwise\nconstraints,” IEEE computer graphics and applications,\nvol. 39, no. 4, pp. 16–27, 2019.\n[25] U. Shaham, K. Stanton, H. Li, B. Nadler, R. Basri,\nand Y. Kluger, “Spectralnet: Spectral clustering using\ndeep neural networks,” arXiv preprint arXiv:1801.01587,\n2018.\n[26] S. Affeldt, L. Labiod, and M. Nadif, “Spectral clustering\nvia ensemble deep autoencoder learning (sc-edae),” arXiv\npreprint arXiv:1901.02291, 2019.\n[27] Y. Ren, N. Wang, M. Li, and Z. Xu, “Deep density-based\nimage clustering,” Knowledge-Based Systems, p. 105841,\n2020.\n[28] D. Bo, X. Wang, C. Shi, M. Zhu, E. Lu, and P. Cui,\n“Structural deep clustering network,” in Proceedings of\nThe Web Conference 2020, 2020, pp. 1400–1410.\n[29] W. Huang, M. Yin, J. Li, and S. Xie, “Deep clustering via\nweighted k-subspace network,” IEEE Signal Processing\nLetters, vol. 26, no. 11, pp. 1628–1632, 2019.\n[30] B. Gao, Y. Yang, H. Gouk, and T. M. Hospedales, “Deep\nclusteringwith concrete k-means,” in ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2020, pp. 4252–\n4256.\n[31] W. Hu, T. Miyato, S. Tokui, E. Matsumoto, and\nM. Sugiyama, “Learning discrete representations via in-\nformation maximizing self-augmented training,” in Pro-\nceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70. JMLR. org, 2017, pp. 1558–\n1567.\n[32] X. Guo, E. Zhu, X. Liu, and J. Yin, “Deep embedded\nclustering with data augmentation,” in Asian conference\non machine learning, 2018, pp. 550–565.\n[33] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou,\n“Variational deep embedding: An unsupervised and\ngenerative\napproach\nto\nclustering,”\narXiv\npreprint\narXiv:1611.05148, 2016.\n[34] N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee,\nH. Salimbeni, K. Arulkumaran, and M. Shanahan, “Deep\nunsupervised clustering with gaussian mixture variational\nautoencoders,” arXiv preprint arXiv:1611.02648, 2016.\n[35] X. Li, Z. Chen, L. K. Poon, and N. L. Zhang, “Learn-\ning latent superstructures in variational autoencoders\nfor deep multidimensional clustering,” arXiv preprint\narXiv:1803.05206, 2018.\n[36] L. Yang, N.-M. Cheung, J. Li, and J. Fang, “Deep\nclustering by gaussian mixture variational autoencoders\nwith graph embedding,” in Proceedings of the IEEE\nInternational Conference on Computer Vision, 2019, pp.\n6440–6449.\n[37] L. Cao, S. Asadi, W. Zhu, C. Schmidli, and M. Sj¨oberg,\n“Simple, scalable, and stable variational deep clustering,”\narXiv preprint arXiv:2005.08047, 2020.\n[38] K.-L. Lim, X. Jiang, and C. Yi, “Deep clustering with\nvariational autoencoder,” IEEE Signal Processing Letters,\nvol. 27, pp. 231–235, 2020.\n[39] C. Song, F. Liu, Y. Huang, L. Wang, and T. Tan,\n“Auto-encoder based data clustering,” in Iberoamerican\nCongress on Pattern Recognition.\nSpringer, 2013, pp.\n117–124.\n[40] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and\nB. Frey, “Adversarial autoencoders,” arXiv preprint\narXiv:1511.05644, 2015.\n12\n[41] W. Harchaoui, P.-A. Mattei, and C. Bouveyron, “Deep\nadversarial gaussian mixture auto-encoder for clustering,”\n2017.\n[42] J. T. Springenberg, “Unsupervised and semi-supervised\nlearning with categorical generative adversarial net-\nworks,” arXiv preprint arXiv:1511.06390, 2015.\n[43] X.\nChen,\nY.\nDuan,\nR.\nHouthooft,\nJ.\nSchulman,\nI. Sutskever, and P. Abbeel, “Infogan: Interpretable repre-\nsentation learning by information maximizing generative\nadversarial nets,” in Advances in neural information\nprocessing systems, 2016, pp. 2172–2180.\n[44] X. Yang, C. Deng, F. Zheng, J. Yan, and W. Liu, “Deep\nspectral clustering using dual autoencoder network,” in\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2019, pp. 4066–4075.\n[45] Q. Zhou et al., “Deep embedded clustering with adver-\nsarial distribution adaptation,” IEEE Access, vol. 7, pp.\n113 801–113 809, 2019.\n[46] Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, and M. S.\nLew, “Deep learning for visual understanding: A review,”\nNeurocomputing, vol. 187, pp. 27–48, 2016.\n[47] L. v. d. Maaten and G. Hinton, “Visualizing data using\nt-sne,” Journal of machine learning research, vol. 9, no.\nNov, pp. 2579–2605, 2008.\n[48] J. H. Ward Jr, “Hierarchical grouping to optimize an\nobjective function,” Journal of the American statistical\nassociation, vol. 58, no. 301, pp. 236–244, 1963.\n[49] B. Ghazanfari, F. Afghah, and M. Hajiaghayi, “Inverse\nfeature learning: Feature learning based on representation\nlearning of error,” IEEE Access, vol. 8, pp. 132 937–\n132 949, 2020.\n[50] J. Wang, J. Wang, J. Song, X.-S. Xu, H. T. Shen, and\nS. Li, “Optimized cartesian k-means,” IEEE Transactions\non Knowledge and Data Engineering, vol. 27, no. 1, pp.\n180–192, 2014.\n[51] L. Zelnik-Manor and P. Perona, “Self-tuning spectral\nclustering,” in Advances in neural information processing\nsystems, 2005, pp. 1601–1608.\n[52] K. C. Gowda and G. Krishna, “Agglomerative clustering\nusing the concept of mutual nearest neighbourhood,”\nPattern recognition, vol. 10, no. 2, pp. 105–112, 1978.\n[53] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle,\n“Greedy layer-wise training of deep networks,” in Ad-\nvances in neural information processing systems, 2007,\npp. 153–160.\n[54] A. Ng et al., “Sparse autoencoder,” CS294A Lecture\nnotes, vol. 72, no. 2011, pp. 1–19, 2011.\n[55] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus,\n“Deconvolutional networks,” in 2010 IEEE Computer\nSociety Conference on computer vision and pattern\nrecognition.\nIEEE, 2010, pp. 2528–2535.\n[56] J. Zhao, M. Mathieu, R. Goroshin, and Y. Lecun,\n“Stacked what-where auto-encoders,” arXiv preprint\narXiv:1506.02351, 2015.\n[57] D. P. Kingma and M. Welling, “Auto-encoding varia-\ntional bayes,” arXiv preprint arXiv:1312.6114, 2013.\n[58] F. Nie, Z. Zeng, I. W. Tsang, D. Xu, and C. Zhang,\n“Spectral embedded clustering: A framework for in-\nsample and out-of-sample spectral clustering,” IEEE\nTransactions on Neural Networks, vol. 22, no. 11, pp.\n1796–1808, 2011.\n[59] Y. Yang, D. Xu, F. Nie, S. Yan, and Y. Zhuang, “Image\nclustering using local discriminant models and global\nintegration,” IEEE Transactions on Image Processing,\nvol. 19, no. 10, pp. 2761–2773, 2010.\n[60] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou,\n“Variational deep embedding: an unsupervised and gener-\native approach to clustering,” in Proceedings of the 26th\nInternational Joint Conference on Artiﬁcial Intelligence,\n2017, pp. 1965–1972.\n[61] K. Tian, S. Zhou, and J. Guan, “Deepcluster: a general\nclustering framework based on deep learning,” in Joint\nEuropean Conference on Machine Learning and Knowl-\nedge Discovery in Databases.\nSpringer, 2017, pp. 809–\n825.\n[62] A. Radford, L. Metz, and S. Chintala, “Unsupervised rep-\nresentation learning with deep convolutional generative\nadversarial networks,” arXiv preprint arXiv:1511.06434,\n2015.\n[63] E. Gultepe and M. Makrehchi, “Improving clustering\nperformance using independent component analysis and\nunsupervised feature learning,” Human-centric Comput-\ning and Information Sciences, vol. 8, no. 1, p. 25, 2018.\n[64] S. Mukherjee, H. Asnani, E. Lin, and S. Kannan, “Clus-\ntergan: Latent space clustering in generative adversarial\nnetworks,” in Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 33, 2019, pp. 4610–4617.\n[65] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong,\n“Towards k-means-friendly spaces: Simultaneous deep\nlearning and clustering,” in Proceedings of the 34th\nInternational Conference on Machine Learning-Volume\n70.\nJMLR. org, 2017, pp. 3861–3870.\n[66] Y. Opochinsky, S. E. Chazan, S. Gannot, and J. Gold-\nberger, “K-autoencoders deep clustering,” in ICASSP\n2020-2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP).\nIEEE, 2020,\npp. 4037–4041.\n[67] A. Alqahtani, X. Xie, J. Deng, and M. Jones, “A deep\nconvolutional auto-encoder with embedded clustering,”\nin 2018 25th IEEE international conference on image\nprocessing (ICIP).\nIEEE, 2018, pp. 4058–4062.\n[68] E. Aljalbout, V. Golkov, Y. Siddiqui, M. Strobel, and\nD. Cremers, “Clustering with deep learning: Taxonomy\nand new methods,” arXiv preprint arXiv:1801.07648,\n2018.\n[69] X. Chen and D. Cai, “Large scale spectral cluster-\ning with landmark-based representation,” in Twenty-ﬁfth\nAAAI conference on artiﬁcial intelligence, 2011.\n[70] W. Zhang, X. Wang, D. Zhao, and X. Tang, “Graph\ndegree linkage: Agglomerative clustering on a directed\ngraph,” in European Conference on Computer Vision.\nSpringer, 2012, pp. 428–441.\n[71] W. Zhang, D. Zhao, and X. Wang, “Agglomerative clus-\ntering via maximum incremental path integral,” Pattern\nRecognition, vol. 46, no. 11, pp. 3056–3065, 2013.\n[72] A. Alqahtani, X. Xie, J. Deng, and M. Jones, “Learning\n13\ndiscriminatory deep clustering models,” in International\nConference on Computer Analysis of Images and Pat-\nterns.\nSpringer, 2019, pp. 224–233.\n[73] Q. Ji, Y. Sun, J. Gao, Y. Hu, and B. Yin, “Nonlin-\near subspace clustering via adaptive graph regularized\nautoencoder,” IEEE Access, vol. 7, pp. 74 122–74 133,\n2019.\n[74] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,\n“Gradient-based learning applied to document recogni-\ntion,” Proceedings of the IEEE, 86, no. 11, pp. 2278–\n2324, 1998.\n[75] P. A. Est´evez, M. Tesmer, C. A. Perez, and J. M. Zurada,\n“Normalized mutual information feature selection,” IEEE\nTransactions on neural networks, vol. 20, no. 2, pp. 189–\n201, 2009.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2020-10-05",
  "updated": "2020-10-05"
}