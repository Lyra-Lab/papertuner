{
  "id": "http://arxiv.org/abs/2010.02761v2",
  "title": "Unified Supervised-Unsupervised (SUPER) Learning for X-ray CT Image Reconstruction",
  "authors": [
    "Siqi Ye",
    "Zhipeng Li",
    "Michael T. McCann",
    "Yong Long",
    "Saiprasad Ravishankar"
  ],
  "abstract": "Traditional model-based image reconstruction (MBIR) methods combine forward\nand noise models with simple object priors. Recent machine learning methods for\nimage reconstruction typically involve supervised learning or unsupervised\nlearning, both of which have their advantages and disadvantages. In this work,\nwe propose a unified supervised-unsupervised (SUPER) learning framework for\nX-ray computed tomography (CT) image reconstruction. The proposed learning\nformulation combines both unsupervised learning-based priors (or even simple\nanalytical priors) together with (supervised) deep network-based priors in a\nunified MBIR framework based on a fixed point iteration analysis. The proposed\ntraining algorithm is also an approximate scheme for a bilevel supervised\ntraining optimization problem, wherein the network-based regularizer in the\nlower-level MBIR problem is optimized using an upper-level reconstruction loss.\nThe training problem is optimized by alternating between updating the network\nweights and iteratively updating the reconstructions based on those weights. We\ndemonstrate the learned SUPER models' efficacy for low-dose CT image\nreconstruction, for which we use the NIH AAPM Mayo Clinic Low Dose CT Grand\nChallenge dataset for training and testing. In our experiments, we studied\ndifferent combinations of supervised deep network priors and unsupervised\nlearning-based or analytical priors. Both numerical and visual results show the\nsuperiority of the proposed unified SUPER methods over standalone supervised\nlearning-based methods, iterative MBIR methods, and variations of SUPER\nobtained via ablation studies. We also show that the proposed algorithm\nconverges rapidly in practice.",
  "text": "1\nUniﬁed Supervised-Unsupervised (SUPER)\nLearning for X-ray CT Image Reconstruction\nSiqi Ye†, Zhipeng Li†, Michael T. McCann, Yong Long∗, Saiprasad Ravishankar\nAbstract—Traditional\nmodel-based\nimage\nreconstruction\n(MBIR) methods combine forward and noise models with\nsimple object priors. Recent machine learning methods for\nimage reconstruction typically involve supervised learning or\nunsupervised learning, both of which have their advantages and\ndisadvantages. In this work, we propose a uniﬁed supervised-\nunsupervised (SUPER) learning framework for X-ray computed\ntomography (CT) image reconstruction. The proposed learning\nformulation combines both unsupervised learning-based priors\n(or even simple analytical priors) together with (supervised)\ndeep network-based priors in a uniﬁed MBIR framework\nbased on a ﬁxed point iteration analysis. The proposed training\nalgorithm is also an approximate scheme for a bilevel supervised\ntraining\noptimization\nproblem,\nwherein\nthe\nnetwork-based\nregularizer in the lower-level MBIR problem is optimized using\nan upper-level reconstruction loss. The training problem is\noptimized by alternating between updating the network weights\nand iteratively updating the reconstructions based on those\nweights. We demonstrate the learned SUPER models’ efﬁcacy\nfor low-dose CT image reconstruction, for which we use the\nNIH AAPM Mayo Clinic Low Dose CT Grand Challenge\ndataset for training and testing. In our experiments, we studied\ndifferent combinations of supervised deep network priors and\nunsupervised learning-based or analytical priors. Both numerical\nand visual results show the superiority of the proposed uniﬁed\nSUPER methods over standalone supervised learning-based\nmethods, iterative MBIR methods, and variations of SUPER\nobtained via ablation studies. We also show that the proposed\nalgorithm converges rapidly in practice.\nIndex Terms—Low-dose X-ray CT, image reconstruction, deep\nlearning, transform learning, iterative reconstruction, mixture of\npriors, ﬁxed point iteration, bilevel optimization.\nI. INTRODUCTION\nX-ray computed tomography (CT) image reconstruction is\na fundamental process in medical imaging. It generates latent\nanatomical images from measurements (i.e., sinograms), that\ndo not directly reﬂect anatomical features. Similar to other\nmedical imaging modalities, X-ray CT image reconstruction is\noften formulated as an inverse problem, which can be solved\nby analytical methods, or iterative optimization algorithms for\nmodel-based image reconstruction problems. More recently,\nThis work was supported by NSFC (61501292). † indicates equal contri-\nbution to this work. Asterisk indicates the corresponding author.\nS. Ye, Z. Li and Y. Long are with the University of Michigan - Shang-\nhai Jiao Tong University Joint Institute, Shanghai Jiao Tong University,\nShanghai 200240, China (email: yesiqi@sjtu.edu.cn, zhipengli@sjtu.edu.cn,\nyong.long@sjtu.edu.cn).\nM. McCann is with the Department of Computational Mathematics, Science\nand Engineering, Michigan State University, East Lansing, MI, 48824 USA\n(email: mccann13@msu.edu).\nS. Ravishankar is with the Department of Computational Mathematics,\nScience and Engineering, and the Department of Biomedical Engineering,\nMichigan State University, East Lansing, MI, 48824 USA (email: rav-\nisha3@msu.edu).\ndeep learning methods have also been used for CT recon-\nstruction [1], [2]. In this section, we ﬁrst review existing image\nreconstruction methods, and then propose a uniﬁed framework\ncombining model-based image reconstruction and supervised\nand unsupervised machine learning priors. Although we focus\non X-ray CT image reconstruction in this paper, our proposed\nmethod can be easily adapted for other imaging modalities.\nA. Background\nAnalytical methods for X-ray CT image reconstruction such\nas the ﬁltered backprojection (FBP) method [3], [4], often in-\nvolve short reconstruction times but have poor noise-resolution\ntrade-offs when dealing with incomplete or degraded measure-\nment data, e.g., sparse-view or low-dose sinogram data.\nMore sophisticated iterative algorithms have also been de-\nveloped for image reconstruction. They are often referred\nto as model-based image reconstruction (MBIR) methods,\nas they iteratively optimize a cost function that incorporates\nimaging physics, statistical model of measurements, and prior\nknowledge of the unknown object. Classical MBIR methods in\nCT solve a penalized weighted-least squares (PWLS) problem,\nwhere a weighted quadratic data-ﬁdelity term captures the\nimaging forward model and measurement statistics, and a\npenalty term (a.k.a. regularizer) models prior information\nabout the object [5]–[7]. Many effective regularizers have been\ndesigned to capture sparsity priors. Examples include hand-\ncrafted priors such as edge-preserving regularizers [8] as well\nas data-driven or learning-based priors such as prior image\nconstrained compressive sensing priors [9]–[11], dictionary\nlearning-based priors [12], sparsifying transform learning-\nbased priors [13], [14], etc. In particular, sparsifying transform\nlearning adopts computationally efﬁcient thresholding opera-\ntions to sparsify signals in a learned transform domain [15].\nVarious structures are used for the transforms during learning,\nwhich prove useful during reconstruction, such as doubly-\nsparse transforms [16], unions of transforms [17], rotation\ninvariance [18], and ﬁlterbank models [19].\nIn the past few years, deep learning methods have also\nbeen gaining popularity for medical image reconstruction.\nDepending on whether the training relies on paired data (low-\ndose and corresponding regular-dose CT data) or not, deep\nlearning methods can be roughly categorized into supervised\nlearning and unsupervised learning-based methods. Supervised\nlearning-based image reconstruction methods use the paired\ndata to learn deep neural network mappings that regress low-\nquality inputs to high-quality outputs.\nA typical class of supervised learning methods work in the\nimage domain, with both inputs and outputs of the network\narXiv:2010.02761v2  [eess.SP]  8 Apr 2021\n2\nbeing images. For example, a residual encoder-decoder con-\nvolutional neural network (RED-CNN) framework that com-\nbines the autoencoder, deconvolution network, and shortcut\nconnections was proposed for low-dose CT imaging [20].\nAnother U-Net based framework FBPConvNet [21] learns\na CNN that maps FBP reconstructed X-ray CT images to\nsuitable high-quality images. The WavResNet framework [22]\nlearns a CNN-based image mapping after transforming images\ninto the wavelet domain, where image features may be better\npreserved. Image-domain learning methods do not directly\nneed raw measurement data, so they can be conveniently\ndeployed with existing imaging systems. However, not directly\nexploiting the measurement data and imaging physics may\nlimit the ability of image-domain learning methods for recov-\nering missing or corrupted information in the measurement\ndomain.\nIn order to better exploit the measurement data, several\nattempts have been made to exploit deep learning in the\nmeasurement domain. For example, [23] proposes a neural net-\nwork to learn projection-domain weights in the FBP method.\nHowever, the designed network does not include other compo-\nnents in the FBP, such as ramp ﬁltering and back-projection\noperations. This idea was recently improved in [24], where\nthe designed neural network incorporates all fundamental steps\nin FBP: ﬁltering, back-projection, and image post-processing.\nThis method achieved competitive results with total variation\nbased PWLS methods and some image-domain deep learning-\nbased denoising methods. A drawback of the approach is that\nnetwork trained with data acquired with a speciﬁc imaging\ngeometry may not be suitable for reconstructing images with\nother imaging geometries.\nThe third type of supervised learning methods exploit both\nimaging physics and iterative image reconstruction methods\nin the neural network architecture and learn the parameters of\nsimple unrolled model-based iterative algorithms. Examples\ninclude unrolling the alternating direction method of multi-\npliers (ADMM) algorithm [25], primal-dual algorithms [26],\n‘ﬁelds of experts’ (FoE)-based iterative algorithms [27], the\nblock coordinate descent algorithm [28]–[30], the gradient\ndescent algorithm [31], etc. The unrolled methods can be com-\nbined with the plug-and-play strategy to replace the explicit\nregularizers in the MBIR cost with a denoising step using\nsome off-the-shelf neural networks [32]–[36].\nIn contrast to supervised methods, unsupervised learning-\nbased methods do not need paired training data. Typical\nexamples include iterative methods that use dictionary learn-\ning or sparsifying transform learning based priors, where\nthe dictionaries or transforms can be learned from unpaired\nclean images. The generative adversarial network (GAN)\nbased methods form another important class of unsupervised\nlearning-based schemes. GAN-based approaches attempt to\ngenerate target data by somehow minimizing the difference\nbetween the probability distributions of the generator output\nand the target data. While several recent works [37]–[39] [40]\nhave applied GAN-based methods to low-dose CT or other\nimaging modalities, it is still challenging to avoid artiﬁcial\nfeatures created by generators in such GAN frameworks. A\nrecent unsupervised learning technique named deep image\nprior (DIP) does not need external prior data to train the\nnetwork. Instead, DIP is task-speciﬁc and takes a ﬁxed input\nsuch as random noise to train the network [41]. DIP implicitly\nregularizes the MBIR problem by replacing the unknown\nimage in the MBIR cost with the network, and training is\nperformed via minimizing the network involved MBIR cost\nwith respect to the network parameters. DIP has been used\nfor medical image reconstructions [42]–[44], but selecting a\nproper network architecture to capture informative priors and\ndeciding when to stop iterations to prevent overﬁtting is often\ndone in an ad hoc manner.\nBoth supervised learning and unsupervised learning have\ntheir advantages. Often, supervised learning provides superior\nresults to unsupervised learning when there is a high similarity\nbetween training and testing samples. However, supervised\nlearning methods also usually need large amounts of paired\ndata (to train complex networks), which is not always feasible\nin medical imaging. Unsupervised learning methods involving\ndictionaries or sparsifying transforms typically require rela-\ntively small training sets, and may have better generalization\nproperties than supervised learning methods [14].\nB. Contributions\nIn this paper, we present a uniﬁed reconstruction framework\ncombining supervised and unsupervised learning, and physics\nand statistical models. We signiﬁcantly extend our recent\npreliminary conference work [45] in several aspects. First, we\ndevelop a systematic and uniﬁed mathematical framework for\nsupervised-unsupervised (SUPER) training and reconstruction.\nWe use an MBIR formulation consisting of a data-ﬁdelity\nterm incorporating forward models and statistical models, and\nregularizer terms incorporating unsupervised learning-based\npriors or simple analytical priors together with supervised deep\nnetwork priors. The deep network in the MBIR formulation\nis trained in a supervised way with an alternating scheme to\napproximate solutions to the corresponding challenging ﬁxed\npoint iteration problem or a bilevel optimization problem. At\ntesting or reconstruction time, a similar MBIR optimization\nis used with learned uniﬁed priors. We allow using vari-\nous types of explicit unsupervised regularizers for SUPER,\nincluding nonsmooth regularizers or integer-valued variable-\nbased regularizers, which are usually not compatible with\nmany (plug-and-play) unrolled methods. Second, we selected\nthe recent FBPConvNet [21] and WavResNet [22] as example\nnetworks in our formulation that are learned in a supervised\nmanner. We then incorporate different analytical and unsuper-\nvised learning-based priors in the proposed uniﬁed framework\nincluding the nonadaptive edge-preserving regularizer and a\nregularizer using a union of sparsifying transforms learned\nfrom (unpaired) regular-dose images. The experimental results\nshow that the proposed (uniﬁed) SUPER learning approaches\nachieve much better image reconstruction quality in low-\ndose CT than standalone deep learning methods and iterative\nreconstruction schemes, and they can outperform a plug-\nand-play unrolled method [32] and a state-of-the-art post-\nprocessing method [46] as well. In particular, combining both\nsupervised and unsupervised learning in our framework leads\nto the best reconstruction performance. Our results also show\n3\nthe practical rapid convergence of the MBIR-based SUPER\nreconstruction. Finally, we consider several special cases of\nthe SUPER model (akin to an ablation study) and demonstrate\nthe superior reconstruction performance of the general uniﬁed\napproach compared to the special schemes.\nC. Organization\nWe organize the rest of this paper as follows. In Section\nII, we describe the SUPER training and reconstruction formu-\nlations, and interpret this model in detail. In Section III, we\ndevelop the algorithms for the proposed problems. In Section\nIV, we show experimental results with the proposed method,\ncompare results among various image reconstruction methods,\nand study the proposed methods’ properties and behavior in\ndetail. Finally, we conclude in Section V.\nII. PROPOSED MODEL AND PROBLEM FORMULATIONS\nThis section presents the general SUPER model along\nwith formulations for training the model and using it at\nreconstruction time. The proposed approach could be useful\nfor a variety of imaging modalities. We provide interpretations\nof our formulations, and give speciﬁc examples of SUPER\nmodels for the low-dose CT application that is a focus of this\npaper.\nIn the low-dose CT image reconstruction problem, the goal\nis to reconstruct an image x ∈RNp from its observed noisy\nsinogram data y ∈RNd, and we assume a given measurement\nmatrix or forward operator A ∈RNd×Np.\nA. Proposed Model\nThe main idea of the SUPER framework is to combine\nsupervised deep learning-based approaches with unsupervised\nor iterative model-based reconstruction approaches. First, we\ncan state reconstruction with an image-domain deep network\nlearned in a supervised manner as\nˆxθ(y) = Gθ(ˆx(y)),\n(1)\nwhere ˆx(y) is a reconstructed image using a speciﬁc recon-\nstruction method. While the most common choice of method\nfor ˆx in the X-ray CT application is the ﬁltered back projection\n(FBP) [3], it would be reasonable to consider an iterative\nmethod as well. Here, Gθ(·) denotes the (supervised) deep\nnetwork operator with parameters θ. Note that ˆx and therefore\nˆxθ depend on the measurements, y.\nOn the other hand, a typical iterative reconstruction method\ncan be made to depend on the results of a trained deep model\nvia adding a penalty term to a usual MBIR cost as\nˆx(y) = argmin\nx\nL(Ax, y) + βR(x) + µ∥x −ˆxθ(y)∥2\n2,\n(2)\nwhere ˆxθ(y) is a ﬁxed image (obtained with a pre-trained\nnetwork) when solving (2), L(Ax, y) and R(x) comprise the\ndata-ﬁdelity term and an analytical or unsupervised learning-\nbased regularizer, and β and µ are non-negative weights\n(scalars) that trade off between the data-ﬁdelity term and the\nregularizers.\nEquations (1) and (2) show that the supervised network’s\nreconstruction can depend on an iterative reconstruction and\nvice-versa; our proposed approach is to complete the cycle, i.e.\nsubstitute ˆxθ in (1) into (2). Doing so leads to an expression\nwhere the (unknown) reconstruction, ˆx(y), appears on both\nthe left and right hand side,\nˆx(y) = argmin\nx\nJ(x, y) + µ∥x −Gθ(ˆx(y))∥2\n2,\n(3)\nwhere J(x, y) ≜L(Ax, y) + βR(x). Roughly speaking, (3)\nseeks an image that is the solution to a regularized recon-\nstruction problem, but where a deep neural network applied\nto the same image (solution) acts as a regularizer. In this\nway, regularization effects from the iterative reconstruction\n(involving R(x)) and from the deep network are combined.\nWe assume a unique global minimizer on the right hand side\nof (3), else, we can replace ‘=’ with ‘∈’ therein.\nWhile one could attempt to directly use (3) as a reconstruc-\ntion method, computing ˆx(y) (to say nothing of training the\ndeep network) turns out to be very challenging. Instead, we\nconsider solving (3) via the ﬁxed point iteration\nˆx(l)\nθ (y) = argmin\nx\nJ(x, y) + µ∥x −Gθ\n\u0010\nˆx(l−1)\nθ\n(y)\n\u0011\n∥2\n2, (4)\nwhere ˆx(l)\nθ (y) represents the reconstruction of the lth iteration\n(l = 1, 2, · · · , L) based on the deep network weights θ,\nloss J(x, y), and measurements y. The initial reconstruction\nˆx(0)\nθ (y) is set to some ﬁxed function of the measurements, e.g.,\nFBP. The opposite substitution, i.e., substituting (2) into (1),\nleads to a similar ﬁxed point, but with the opposite alternation\nbetween the deep and iterative reconstructions.\nWe also found that sharing weights between these steps\ndecreased performance, so we learn a different set of su-\npervised weights at each step. Thus, the simpliﬁed SUPER\nreconstruction framework is\nˆx(l)\nθ(l)(y) = argmin\nx\nJ(x, y) + µ∥x −Gθ(l)\n\u0010\nˆx(l−1)\nθ(l−1)(y)\n\u0011\n∥2\n2,\n(P0)\nwhere the ﬁnal reconstruction is ˆx(L)\nθ(L)(y), and ˆx(0)\nθ(0)(y) =\nˆx(0)(y) is an initial reconstruction that does not depend on\na network. We can view the iterations in (P0) as layers in a\nlarger neural network; we call these “SUPER layers”.\nB. SUPER Learning Formulation\nThe SUPER model includes a regularizer R(x) that can be\nan analytical prior or based on unsupervised learning from\nunpaired data (e.g., regular-dose images). On the other hand,\nthe network parameters in the deep network-based regularizer\nare learned in a supervised manner from paired training data.\nFor this training process (referred to as SUPER learning),\nwe denote the paired low-dose and regular-dose (reference)\ntraining images as {(ˆx(0)(yn), x∗\nn)}N\nn=1, and the correspond-\ning low-dose sinograms (measurements) as {yn}N\nn=1. To learn\nthe supervised weights in (P0), we take a greedy approach by\nlearning each θ(l) in sequence according to\nθ(l) = argmin\nθ\nN\nX\nn=1\n∥Gθ\n\u0010\nˆx(l−1)\nθ(l−1)(yn)\n\u0011\n−x∗\nn∥2\n2,\n(P1)\nfor l = 1, 2, . . . , L. We solve the sequence of problems (P1)\nby alternating between learning weights for the supervised\nmethod and solving the iterative reconstruction problem (to\ncompute ˆx(l)\nθ(l)(yn)); we describe this approach in detail in\nSection III-B.\n4\nAnother perspective on this training approach is that it is a\nheuristic for solving the following bilevel problem:\nθ = argmin\nθ\nN\nX\nn=1\n∥Gθ(ˆxθ(yn)) −x∗\nn∥2\n2\ns.t. ˆxθ(yn) = argmin\nx\nJ(x, yn) + µ∥x −Gθ(x)∥2\n2.\n(5)\nThe problem is called bilevel because the network input\nˆxθ(yn) in the main cost arises as the minimizer of another\n(i.e., the lower-level) optimization problem. The bilevel opti-\nmization problem is very challenging to solve in general. Some\nof the authors have proposed an algorithm for a simple form\nof (5) in [47] (without deep networks). Here, the proposed\nalternating training approach could be viewed as a plausible\nheuristic for the challenging bilevel problem (5).\nWe can\ninterpret (5) as learning part of the regularizer of an MBIR\nproblem (involving the network Gθ) in a supervised manner.\nC. SUPER Reconstruction Formulation\nWith the trained supervised network parameters {θ(l)} for\nl = 1, 2, · · · , L, the reconstruction (or testing) step becomes\noptimizing the MBIR formulation constructed in (P0) in every\nSUPER layer to obtain the ﬁnal layer reconstruction ˆx(L)\nθ(L)(y).\nD. Examples of SUPER Modeling\nThe SUPER framework is ﬂexible, and allows incorporat-\ning various deep networks Gθ(·) in the supervised network-\nbased regularizer and various unsupervised regularizers R(·).\nIn this work, we focus on studying some examples of the\nSUPER model. For the supervised component, we choose\nthe recent FBPConvNet (FCN)\n[21] and the (feed-forward\nversion of) WavResNet (WRN) [22]. For the regularizer R(·),\nwe study both a non-adaptive and an unsupervised learning-\nbased one, namely the non-adaptive edge-preserving (EP)\nregularizer, and a state-of-the-art union of learned sparsifying\ntransforms (ULTRA) [17] regularizer. The union of transforms\nis learned in an unsupervised manner from a set of (unpaired)\nregular-dose images. We refer to the resulting SUPER models\nas SUPER-FCN-EP, SUPER-FCN-ULTRA, SUPER-WRN-EP,\nand SUPER-WRN-ULTRA, respectively. For simplicity, we\nrefer to any regularizer R(·) that is not learned in a supervised\nmanner as an unsupervised regularizer. In the following, we\nfurther describe the models chosen above.\n1) Supervised Networks\nWe work with FBPConvNet and WavResNet, both of which\nare CNN-based image-domain denoising architectures. FBP-\nConvNet was originally designed for sparse-view CT, while we\napplied it to the low-dose CT case; it is a U-Net like CNN and\nwe took low-dose FBP images as input. The neural network\nis trained so that the denoised versions of the input images\nclosely match the high-quality reference images. Traditional\nU-Net uses a multilevel decomposition, and a dyadic scale\ndecomposition based on max pooling. Similar to U-Net, FBP-\nConvNet adopts multichannel ﬁlters to increase the capacity\nof the network.\nWavResNet is an interpretable framelet-based denoising\nneural network that employs contourlet transforms, a concate-\nnation layer, and a skip connection. Contourlet transforms\nincrease the input data size according to the number of\ntransform levels, which can create memory bottlenecks during\ntraining. Hence, a patch-based training strategy is adopted.\nWavResNet can be applied either with a feed-forward scheme\nor a recursive scheme [22]. We chose the feed-forward scheme\nin this paper.\n2) Unsupervised MBIR Components\nWe adopt the weighted-least squares (WLS) data-ﬁdelity\nterm L(Ax, y) = ∥y −Ax∥2\nW, where W ∈RNd×Nd is\na diagonal weighting matrix whose diagonal elements are\nthe estimated inverse variance of yi [6]. For the regular-\nizer R(x), we adopt a traditional EP regularizer REP and\na state-of-the-art ULTRA regularizer RULTRA. For the EP\nregularizer, REP(x) = PNp\nj=1\nP\nk∈Nj κjκkϕ(xj −xk), where\nNj is the size of the neighborhood, xj is the jth pixel of\nx, κj and κk are analytically determined weights that en-\ncourage resolution uniformity [48], and the potential function\nϕ(t) ≜δ2(|t/δ| −log(1 + |t/δ|)) with δ > 0 being the EP\nparameter.\nPWLS-ULTRA pre-learns a union of sparsifying transforms\nfrom image patches. With the pre-learned transforms {Ωk},\nthe regularizer RULTRA(x) for image reconstruction is\nmin\n{zj,Ck}\nK\nX\nk=1\nX\nj∈Ck\nτj\n\b\n∥ΩkPjx −zj∥2\n2 + γ2∥zj∥0\n\t\n,\n(6)\nwhere the operator Pj ∈Rm×Np extracts the jth patch of\nsize √m × √m from x, vector zj ∈Rm denotes the sparse\ncoefﬁcients for the jth image patch, Ck denotes the indices of\nall patches matched to the kth transform, {τj} are patch-based\nweights to encourage uniform spatial resolution or uniform\nnoise in the reconstructed images [13], and γ is a parameter\ncontrolling sparsity in the model.\nE. Discussion of the SUPER Framework\nHaving described the SUPER Framework, we now de-\nscribe a few of its conceptual advantages. SUPER uniﬁes\nseveral distinct models—a supervised learning-based model,\nan unsupervised (iterative) model, a physics-based forward\nmodel, and a statistical model of measurements and noise—in\na common MBIR-type framework. This combination affords\ntraining algorithms based on SUPER extra ﬂexibility: the\nsupervised part can beneﬁt from paired training data (e.g., low-\ndose and corresponding regular-dose images/measurements),\nwhile the unsupervised part, e.g., based on ULTRA, can use\na few regular-dose training images without corresponding\nmeasurements. And, because of the physics-based forward\nmodels and statistical models, the algorithm can perform\nwell even when training data of any kind is scarce. The\nrelative importance of each of these models can be tuned\nsimply by adjusting the corresponding scalar parameters (see\nSections IV-D and IV-E in this manuscript).\nOur training approach (P1) can also be viewed as optimiza-\ntion that alternates between two different modules, i.e., the\nsupervised module and the unsupervised (iterative) module.\nWhile the supervised module involves layer-wise neural net-\nwork weights to effectively remove noise and artifacts, the\nunsupervised module could substantially optimize each image\nby incorporating various physical and image properties. The\nSUPER framework is closely related to the idea of plug-and-\n5\nplay [49], with which various unsupervised and supervised\nregularizers can be plugged. Different from many plug-and-\nplay unrolled methods that exclude explicit regularizers in\nthe MBIR step (corresponding to the iterative module in\nSUPER) while implicitly regularizing the reconstruction with\nan additional denoising step using some neural networks\n(corresponding to the supervised module in SUPER) [25],\n[27], [32]–[36], SUPER allows to use explicit regularizers\nin the iterative module, including nonsmooth regularizers\n(e.g., based on the ℓ0 “norm”) and integer-valued variables\n(e.g., clusters with the ULTRA regularizer), and allows opti-\nmizing the MBIR cost with such explicit regularizers using\nmany iterations during training and reconstruction. Moreover,\nrather than learning a common network for all layers (or\niterations in terms of unrolled methods), our approach is\nakin to greedy layer-wise learning to circumvent the need to\npropagate gradients through a large number of iterations of\nMBIR (which is well-known to lead to vanishing gradients),\nand also to more readily handle regularizers incomptabile\nwith gradient backpropagation. The layer-wise training also\nencourages the supervised networks in each SUPER layer to\noptimally denoise the images with certain noise levels (in\nthat layer), which shares similarities with layer-wise training\nin [30] and MBIR with varying regularization strength (and\ntransforms/dictionaries) for varying noise or artifacts over\niterations (sometimes called a continuation strategy) [17], [50].\nIII. ALGORITHMS\nThis section describes the algorithms to solve the training\nand reconstruction optimization problems in Section II. We\nﬁrst brieﬂy introduce the unsupervised learning of a union of\ntransforms [13] and then describe the proposed methods.\nA. Learning a Union of Sparsifying Transforms\nWe pre-learn a union of transforms {Ωk}K\nk=1 to effectively\ngroup and sparsify a training set of image patches by solving\nmin\n{Ωk, Zi, Ck}\nK\nX\nk=1\nX\ni∈Ck\n\b\n∥ΩkXi −Zi∥2\n2 + η2∥Zi∥0\n\t\n+\nK\nX\nk=1\nλkQ(Ωk),\ns.t.\n{Ck} ∈G.\n(7)\nwhere Xi ∈Rm denotes the ith vectorized (overlapping)\nimage patch extracted from training images, Zi ∈Rm is the\ncorresponding transform-domain sparse approximation (with\nsparsity measured using the ℓ0 “norm” that counts the number\nof nonzeros in a vector), parameter K denotes the number of\nclusters, Ck denotes the indices of all the patches matched\nto the kth transform, and the set G is the set of all possible\npartitions of {1, 2, . . . , N ′} into K disjoint subsets, with N ′\ndenoting the total number of patches. We use K regularizers\nQ(Ωk) = ∥Ωk∥2\nF −log | det Ωk|, 1 ≤k ≤K, which\ncontrol the properties of each transform Ωk, and prevent trivial\nsolutions (e.g., matrices with zero or repeated rows). We set\nthe weights λk = λ0\nP\ni∈Ck ∥Xi∥2\n2, where λ0 is a constant\n[13]. We adopt an alternating algorithm for (7) that alternates\nbetween a transform update step (solving for {Ωk}) and a\nsparse coding and clustering step\n(solving for {Zi, Ck}),\nwith closed-form solutions in each step [17]. As a patch-based\nunsupervised learning method, ULTRA typically only needs a\nfew regular-dose training images to learn rich features.\nB. SUPER Training and Reconstruction Algorithms\nAs stated in Section II, we train a sequence of supervised\nnetwork\nparameters\n{θ(l)}L\nl=1\nby\nalternating\nbetween\noptimizing (P1) in a supervised manner to get θ(l) and\noptimizing (P0) with iterative algorithms to obtain ˆx(l)\nθ(l)(y).\nWhen updating the network parameters, the network inputs are\nﬁxed to the most recent iterative reconstructions. Speciﬁcally,\ntraining θ(l) in a single (lth) SUPER layer coincides with a\nconventional network training problem which can be solved by\nstochastic gradient descent (SGD) algorithms or Adam [51].\nAlgorithm 1 SUPER Training Algorithm\nInput:\n1: N pairs of low-dose FBP images and corresponding regular-\ndose reference images {(ˆx(0)\nθ(0)(yn), x∗\nn)}N\nn=1;\n2: Low-dose sinograms yn and weights Wn, ∀n;\n3: Unsupervised (iterative) module regularizer R, e.g., REP or\nRULTRA;\n4: number of SUPER training layers L, number of unsuper-\nvised (iterative) module iterations I, and number of inner\niterations P (P is only used with RULTRA, and denotes the\nnumber of inner iterations in the image update step).\nOutput: A set of layer-wise supervised (deep) model param-\neters {θ(l)}L\nl=1.\n1: for l = 1, 2, · · · , L do\n2:\n(1) update θ(l) : with ﬁxed input {ˆx(l−1)\nθ(l−1)(yn)}N\nn=1,\noptimize (P1) with SGD or Adam [51] to obtain θ(l);\n3:\n(2)\nupdate\nˆx(l)\nθ(l)(yn): a) apply the updated net-\nwork\nGθ(l)(·)\nto\nthe\nprevious\nlayer\nreconstructions\n{ˆx(l−1)\nθ(l−1)(yn)}N\nn=1, i.e., obtain each Gθ(l)(ˆx(l−1)\nθ(l−1)(yn));\n4:\nb) update each image ˆx(l)\nθ(l)(yn) by optimizing the\nPWLS cost in (P0) with Gθ(l)(ˆx(l−1)\nθ(l−1)(yn)) as the initial\nimage, and using I iterations of the relaxed LALM algo-\nrithm [52] for REP based cost, or I alternations and P inner\niterations (with the relaxed LALM algorithm) for RULTRA\nbased cost [13].\n5: end for\nIn updating ˆx(l)\nθ(l)(y), we adopt the relaxed LALM [52]\nalgorithm for both EP based and ULTRA based SUPER\nreconstruction costs. Particularly, when using the ULTRA\nregularizer, we alternate several times between updating x\nand {zj, Ck}. In the image (x) update step, we ﬁx the sparse\ncoefﬁcients {zj} and cluster assignments {Ck} and solve\nˆx(l)\nθ(l)(y) = argmin\nx\n∥y −Ax∥2\nWn+\nβ\nK\nX\nk=1\nX\nj∈Ck\nτj\n\b\n∥ΩkPjx −zj∥2\n2\n\t\n+ µ∥x −Gθ(l)(ˆx(l−1)\nθ(l−1)(y))∥2\n2,\nvia the efﬁcient relaxed LALM algorithm [52]. The initial\nimage in the image update step of each SUPER layer is\nGθ(l)(ˆx(l−1)\nθ(l−1)(y)). We then ﬁx the updated x and jointly\noptimize {zj} and {Ck} (sparse coding and clustering step).\n6\nIn the resulting subproblem, the sparse vectors zj can be\nreplaced with their optimal values zj = Hγ(ΩkPjx) in the\ncost, where Hγ(·) is a hard-thresholding function that sets\nvector elements with magnitudes smaller than γ to 0, and\nleaves other entries unchanged. The optimal clustering is then\nobtained patch-wise as\nˆkj = arg min\n1≤k≤K\n∥ΩkPjx −Hγ(ΩkPjx)∥2\n2 + γ2∥Hγ(ΩkPjx)∥0,\nand\nthe\ncorresponding\noptimal\nsparse\ncoefﬁcients\nare\nˆzj = Hγ(ΩˆkjPjx) [13].\nThe SUPER learning algorithm based on (P1) is illustrated\nin Algorithm 1.\nThe SUPER reconstruction algorithm in each single (lth)\nSUPER layer is the same as that for updating ˆx(l)\nθ(l)(y) in the\ntraining, while using the trained θ(l) for the supervised penalty\nterm.\nIV. EXPERIMENTS AND DISCUSSIONS\nIn this section, we ﬁrst describe the experimental setup,\ntraining procedures, and evaluation metrics. Then, we present\nthe results for SUPER learning with different combinations\nof supervised and unsupervised components, and compare\nthese results with multiple standalone supervised and iterative\nmethods from the literature. Finally, we present several exper-\niments to explore how the SUPER model works, including\nanalysis of the impact of the supervised and unsupervised\ncomponents in SUPER on the reconstruction performance and\nthe convergence behavior of the proposed algorithms.\nA. Experimental Setup\n1) Data and Imaging system\nWe used Mayo Clinics dataset established for “the 2016\nNIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge”\n[53] in our experiments. We randomly selected 520 slices from\ndata (of 3 mm thickness) for six out of ten patients, from which\n500 slices were used for training and 20 slices were used\nfor validation. We extracted 18 regular-dose CT images from\nthe 500 training slices to pre-train a union of ﬁve sparsifying\ntransforms used for the ULTRA regularizer.\nWe tested on\n20 slices that were randomly extracted from the remaining\nfour patients’ data. We simulated low-dose CT sinograms y\nfrom the provided regular-dose images x∗using the Poisson-\nGaussian noise model [14], [54]:\nyi = −log\n\u0010\nI−1\n0\nmax\n\u0000Poisson{I0e−[Ax∗]i} + N{0, σ2}, ϵ\n\u0001\u0011\n,\nwhere the number of incident photons per ray is I0 = 104, the\nGaussian noise variance is σ2 = 25, and ϵ is a small positive\nnumber to avoid negative measurement data when taking\nthe logarithm. We used the Michigan Image Reconstruction\nToolbox1 to construct fan-beam CT geometry with 736 detec-\ntors × 1152 regularly spaced projection views, and a no-scatter\nmonoenergetic source. The width of each detector column is\n1.2858 mm, the source to detector distance is 1085.6 mm,\nand the source to rotation center distance is 595 mm. We\nreconstruct images of size 512×512 with the pixel size being\n0.69 mm × 0.69 mm.\n1Jeffrey A Fessler, available at http://web.eecs.umich.edu/∼fessler/irt/irt.\n2) Parameter Settings for Proposed and Compared Methods\nIn the SUPER model, we adopted two architectures for the\ndeep network, namely FBPConvNet [21] and WavResNet [22].\nSpeciﬁcally, during SUPER training, we ran 4 epochs (over the\ntraining set) of the SGD optimizer for the FBPConvNet mod-\nule in each SUPER layer to reduce overﬁtting risks. For the\nWavResNet case, we chose the faster feed-forward neural net-\nwork architecture in [22] and employed contourlet transforms\nwith 15 channels on input images. We used 256 × 256 × 15\nwavelet domain patches to train WavResNet. During SUPER\ntraining, we ran 50 epochs of the SGD optimizer to update\nthe WavResNet weights each time to capture sufﬁcient wavelet\ndomain features. The SUPER models with both WavResNet\nand FBPConvNet networks were trained with batch size 1\nin each SUPER layer. When running iterative reconstruction\nmethods in our algorithms, we used 20 iterations (each time)\nof the iterative algorithms for the EP regularization case, and\n20 alternations with 5 inner iterations (each time) for the\nULTRA regularization case. For the EP regularizer, we set\nδ = 20 and regularization parameter β = 215; for the ULTRA\nregularizer, we set the regularization parameters β = 5 × 103\nand γ = 20 during training and reconstruction. Since the\nparameter µ controls the balance between the supervised and\nunsupervised modules in our formulation, we empirically set\nµ as 5 × 104 and 5 × 105 for EP and ULTRA based SUPER\nmethods, respectively.\nWe compared our proposed model with the standalone\nsupervised methods, i.e., FBPConvNet and WavResNet, and\nstandalone unsupervised methods, i.e., PWLS methods with\nEP regularizer (PWLS-EP) and ULTRA regularizer (PWLS-\nULTRA), respectively. We ran 100 epochs and 200 epochs of\nSGD for training standalone FBPConvNet and WavResNet,\nrespectively, to sufﬁciently learn image features with low\noverﬁtting risks (also evaluated on validation data). For the\nstandalone PWLS-EP iterative approach, we set β = 216\nand δ = 20, and ran 100 iterations of the relaxed LALM\nalgorithm [52] to obtain convergent images. For the standalone\nPWLS-ULTRA method, we set β = 104 and γ = 25, and\nran 1000 alternations between the image update step (with 5\nrelaxed LALM iterations), and sparse coding and clustering\nstep. Apart from comparing with standalone supervised and\nunsupervised methods that we used for SUPER, we also com-\npared SUPER methods with a state-of-the-art deep learning-\nbased post-processing method MAP-NN [46]. We extracted\nimage patches from the training data used for SUPER and\ncreated a training set consisting of similar amounts of image\npatches as used in [46] for MAP-NN training. Since MAP-\nNN involves output clipping in each of its modules, we\nincreased the clipping window to [0, 2400] HU (shifted HU\nbefore normalization) such that most image structures can be\npreserved while the denoising capability is maintained. We\nalso compared SUPER methods with a recent plug-and-play\nADMM-Net method [32], which unrolls the ADMM algorithm\nand replaces the proximal operator inside the algorithm with\noff-the-shelf denoising methods. We used trained WavResNets\nas the off-the-shelf denoisers for the ADMM-Net method, and\nused the same training setup as for WavResNet in SUPER\nmethods. The MBIR step in ADMM-Net only involves a\n7\nnetwork-based regularizer, which is similar to SUPER with\nβ = 0 while µ ̸= 0. We set the regularization parameter for\nADMM-Net as 1 × 106 and ran 20 iterations for the MBIR\nstep in each ADMM-Net iteration, which worked well in our\nexperiments. We used the default data augmentation tool in\nWavResNet [22] to train all the WavResNet-related networks\nin our experiments. There is no data augmentation in MAP-\nNN by default [46], and since the amount of training data we\nused satisﬁes the required amount in FBPConvNet and MAP-\nNN, we did not use data augmentation for methods related to\nthese two (to reduce the training time). All methods used the\nFBP reconstruction as initialization or network input.\n3) Evaluation metrics\nWe chose root mean square error (RMSE), signal-to-noise\nratio (SNR), and structural similarity index measure (SSIM)\n[12] to quantitatively evaluate the performance of reconstruc-\ntion methods. The RMSE in Hounsﬁeld units (HU) is deﬁned\nas RMSE =\nqPNp\nj=1(ˆxj −x∗\nj)2/Np, where x∗\nj is the jth pixel\nof the reference regular-dose image x∗, ˆxj is the jth pixel of\nthe reconstructed image ˆx, and Np is the number of pixels. The\nSNR in decibels (dB) is deﬁned as SNR = 10 log10\n∥x∗∥2\n∥ˆx−x∗∥2 .\nB. Numerical Results and Comparisons\nFig. 1 shows box plots of RMSE values of (test) reconstruc-\ntions with SUPER methods and their constituent supervised\nand unsupervised methods, as well as an external learned iter-\native reconstruction method, plug-and-play ADMM-Net [32]\nwith WavResNet denoiser (ADMM-Net (WRN)) and a state-\nof-the-art post-processing method MAP-NN [46]. From this\nﬁgure, we observe that the proposed SUPER methods, namely\nSUPER-WRN-EP, SUPER-WRN-ULTRA, SUPER-FCN-EP\nand SUPER-FCN-ULTRA, decrease RMSE values dramati-\ncally in test slices compared to standalone iterative or deep\nlearning methods such as PWLS-EP, PWLS-ULTRA, WavRes-\nNet, and FBPConvNet. Speciﬁcally, SUPER methods can\neffectively handle highly corrupted scans for which either\nthe standalone supervised methods or unsupervised methods\nmay have limited performance. For example, the SUPER\nmethods further reduce the RMSE of the most corrupted (FBP)\nimage (slice 150 from patient L067) by approximately 35 HU\ncompared with the standalone PWLS-EP or PWLS-ULTRA\nmethod; the RMSE value is around 20 HU lower than the result\nusing the feed-forwarded WavResNet method, and 6 HU lower\nthan using the FBPConvNet scheme. The reconstructions of\nthis most corrupted test image using different methods are\nshown in the supplement (Fig. 11). It is also obvious from\nFig. 1 that the methods exploiting SUPER learning reduce the\ninterquartile ranges and shrink the gap between the maximum\nand minimum RMSE values. This indicates the robustness of\nthe proposed schemes and their generalization for reconstruct-\ning images with various noise or artifacts levels. The SNR and\nSSIM comparisons also reﬂect the robustness of the proposed\nschemes. We show the box plots for these two metrics in the\nsupplement (Figs. 8 and 9).\nThe proposed methods are also robust to the choices of\nthe supervised and unsupervised/analytical parts. For exam-\nple, the reconstruction performance of supervised learning\nschemes may be affected by the network architecture, number\nMethods\nRMSE (HU)\n67.6\n37.5\n28.0\n26.5\n67.8\n53.4\n20.1\n33.0\n24.2\n26.6\n25.2\n30.3\n26.0\n25.2\n33.4\n20.6\n32.7\n18.7\n38.0\n22.3\n33.6\n20.1\n31.7\n19.2\n28.3\n36.4\n22.8\n33.6\n49.5\n24.1\nadmm-\nnet (wrn)\nmap-\nnn\nfcn\nFig. 1: RMSE spread (shown using box plots) over 20 test\ncases using different methods. Here, “wrn” stands for WavRes-\nNet and “fcn” stands for FBPConvNet. Each box plot for\na method describes the statistics of RMSE values over the\n20 test slices: the central red line indicates the median; the\nbottom and top edges of the boxes indicate the 25th and\n75th percentiles, respectively; and the whiskers represent the\nextreme values. We marked the median values and the extreme\nvalues for each method in the box plot. The proposed SUPER\ncombinations (highlighted with bold in x-axis) outperform\nboth their supervised and unsupervised counterparts. They also\noutperform the competing plug-and-play ADMM-Net method\nwith WavResNet denoiser (admm-net (wrn)) and a state-of-\nthe-art post-processing method MAP-NN.\nof training samples, hyper parameter tuning, etc. In our\nexperiments, we used two distinct networks learned in a\nsupervised manner, WavResNet [22] and FBPConvNet [21].\nAlthough the performance of WavResNet is worse than FBP-\nConvNet due to the architecture differences (and possibly\nthe somewhat limited training data), the proposed SUPER\nmethods based on these two methods have comparable recon-\nstruction metrics. Among the unsupervised learning or ana-\nlytical prior-based methods, although the PWLS-EP method\nis substantially inferior to the PWLS-ULTRA method, the\nperformance (i.e., RMSE, SNR, SSIM) gap between the EP\nbased SUPER and ULTRA based SUPER is much smaller than\nthat between the standalone PWLS-EP and PWLS-ULTRA\nmethods. ULTRA-based SUPER schemes do outperform EP-\nbased schemes overall, indicating that unsupervised learning\napproaches provide beneﬁts over conventional mathematical\npriors. Comparing WavResNet-based SUPER methods with\nthe ADMM-Net method, both the extreme RMSE values and\nthe median RMSE of the latter are higher than those of the\nformer ones. As there is no unsupervised regularizer in the\nADMM-Net framework, this phenomenon indicates the beneﬁt\nof SUPER that incorporates the unsupervised regularizer, e.g.,\nEP or ULTRA, in the MBIR module. The numerical results\nof MAP-NN is similar to that of WavResNet, which under-\nperforms the SUPER methods. MAP-NN also underperforms\nFBPConvNet, which may be caused by output clippings in\nMAP-NN such that pixel values outside the clipping window\n([0, 2400] HU) are set to either 0 HU (for values smaller than\n0 HU) or 2400 HU (for values larger than 2400 HU).\n8\nTABLE I: Mean metrics of reconstructions of 20 test slices.\nMethod\nRMSE (HU)\nSNR (dB)\nSSIM\nFBP\n128.8\n16.7\n0.347\nPWLS-EP\n41.4\n25.4\n0.673\nPWLS-ULTRA\n32.4\n27.8\n0.716\nWavResNet\n35.3\n27.1\n0.646\nSUPER-WRN-EP\n26.7\n29.1\n0.738\nSUPER-WRN-ULTRA\n25.4\n29.5\n0.744\nFBPConvNet\n29.2\n28.2\n0.688\nSUPER-FCN-EP\n26.0\n29.3\n0.740\nSUPER-FCN-ULTRA\n25.0\n29.7\n0.748\nADMM-Net (WRN)\n28.6\n28.4\n0.702\nMAP-NN\n35.1\n26.7\n0.660\nThe superiority of SUPER learning is also reﬂected in\nthe averaged (over all test slices) reconstruction quality met-\nrics shown in Table I. In Table I, we observe that among\nWavResNet based methods, SUPER-WRN-ULTRA achieves\nthe best RMSE, SNR and SSIM values. Both SUPER-WRN-\nEP and SUPER-WRN-ULTRA provide signiﬁcantly improved\nperformance compared to the standalone (WavResNet, EP, UL-\nTRA) components, and outperformed the competing ADMM-\nNet method. In particular, SUPER-WRN-ULTRA achieves\n9.8 HU, 7.0 HU and 3.2 HU better average RMSE over\nits constituent standalone supervised method (WavResNet),\nunsupervised method (PWLS-ULTRA), and the plug-and-\nplay ADMM-Net method, respectively; SUPER-WRN-EP im-\nproves the average RMSE by 8.6 HU and 15.8 HU respec-\ntively compared with its constituent standalone supervised and\nunsupervised methods (WavResNet and PWLS-EP), and im-\nproves by 1.9 HU compared with the plug-and-play ADMM-\nNet. A similar trend is observed with FBPConvNet-based\nSUPER methods. Speciﬁcally, SUPER-FCN-ULTRA achieves\naverage RMSE improvements of 4.3 HU and 7.5 HU over\nits constituent standalone supervised method (FBPConvNet)\nand unsupervised method (PWLS-ULTRA), respectively; and\nSUPER-FCN-EP achieves average RMSE improvements of\n3.3 HU and 15.5 HU respectively, compared to its constituent\nstandalone FBPConvNet and PWLS-EP methods.\nC. Visual Results and Comparisons\nFig. 2 shows a test example reconstructed using various\nmethods. We observe that PWLS-EP reduces the severe noise\nand streak artifacts observed in the low-dose FBP images, and\nthe transform learning-based method PWLS-ULTRA further\nsuppresses noise and reconstructs more details of the image\nsuch as the zoom-in areas. However, both methods have some\nblurry artifacts. The standalone FBPConvNet method heavily\nremoves noise and streak artifacts, while introducing several\nartiﬁcial features (e.g., feature indicated by the arrow in the\ntop-right box in Fig. 2). WavResNet denoises the image with-\nout introducing artiﬁcal features, but still retains some streaks\naround image boundaries and blurs some details (e.g., feature\nindicated by the arrow in the bottom-left box in Fig. 2). The\nstate-of-the-art MAP-NN method performs slightly better than\nWavResNet in terms of suppressing streak artifacts, while it\nstill loses some details as indicated in the zoomed regions. The\ncompeting plug-and-play unrolled method—ADMM-Net with\nWavResNet denoiser—outperforms the standalone WavResNet\nmethod, but still has some streak artifacts and blurred details.\nCompared to these methods, the proposed SUPER methods\n(SUPER-WRN-EP, SUPER-WRN-ULTRA, SUPER-FCN-EP,\nand SUPER-FCN-ULTRA) improve the reconstruction quality\nin terms of removing noise and artifacts, and recovering details\nmore precisely. Two other example comparisons are included\nin the supplement (Fig. 10 and Fig. 11).\nFig. 3 illustrates the image evolution over SUPER layers\n(i.e., with evolving network weights in the iterative recon-\nstruction process) for one test case, when using SUPER-\nWRN-ULTRA. It is apparent that in the early SUPER layers,\nthe proposed SUPER-WRN-ULTRA method mainly removes\nnoise and artifacts, while later SUPER layers mainly recon-\nstruct details such as the bone structures shown in the zoom-\nin box. A similar behaviour is observed with FBPConvNet-\nbased SUPER methods, which are shown in the supplement\n(Figs. 13 and 14).\nD. Effect of Regularization Parameter µ in SUPER Models\nHere, we study further the effect of the supervised learning-\nbased regularizer weight µ on reconstruction performance.\nTable II shows the average RMSE (over test slices) of re-\nconstructions for different choices of µ during training and\nduring reconstruction. The µ = 0 case here corresponds to\nnot having an explicit network-based regularizer term, but\nthe iterative reconstruction algorithm is re-initialized with\nx = Gθ(l)(ˆx(l−1)\nθ(l−1)(y)) in each SUPER layer. This helps move\nthe MBIR estimator towards a decent optimum when the cost\nof MBIR is not convex (with local optima), such as with\nULTRA, or when the convex MBIR cost such as EP is par-\ntially optimized (i.e., not until algorithm convergence). Thus,\nthis is different from just a conventional standalone PWLS\nalgorithm run until convergence, wherein learned networks\nare not involved. Our experimental results in Table II show\nthat using the supervised learning-based regularizer in iterative\nreconstruction (i.e., µ ̸= 0) provides the best reconstruction\nperformance. For example, in the SUPER-WRN-EP case,\nusing µ = 5×104 in both training and testing leads to around\n0.6 HU lower RMSE than using µ = 0 in training and testing\n(and using supervised learned network re-initializations [45]).\nIn the SUPER-WRN-ULTRA case, using µ = 5 × 105 in\ntraining and testing improves the RMSE by 0.1 HU compared\nto the aforementioned µ = 0 setting. Another observation\nis that using the same µ during training and testing usually\nworks better than using mismatched µ values. There is an\nexception when µ = 0 is used during training. In this\ncase, using an explicit network regularizer with appropriate\n(positive) weighting at reconstruction time works better, i.e.,\nit is better to work with the proposed combined priors during\nreconstruction. In the SUPER-WRN-EP and SUPER-WRN-\nULTRA cases, the mean RMSE over 20 test slices is 0.3 HU\nbetter with appropriate nonzero µ at testing time compared to\nthe µ = 0 setting used during training. Section VII.E in the\nsupplement shows a similar behavior for FBPConvNet-based\nSUPER models.\nFig. 4 plots the mean RMSE values over the number of\nSUPER layers for various choices of (common) µ during\ntraining and testing. The RMSE values converge quickly in\nall cases, with nonzero values of µ leading to lower RMSE\n9\nFBP\nRMSE = 55.27 HU\nFBPConvNet\nRMSE = 22.32 HU\nWavResNet\nRMSE = 24.19 HU\nMAP-NN\nRMSE = 24.08 HU\nPWLS-EP\nRMSE = 26.54 HU\nSUPER-\nFCN-EP\nRMSE = 20.14 HU\nSUPER-\nWRN-EP\nRMSE = 20.60 HU\nADMM-Net\n(WRN)\nRMSE = 22.80 HU\nPWLS-ULTRA\nRMSE = 20.08 HU\nSUPER-\nFCN-ULTRA\nRMSE = 19.16 HU\nSUPER-\nWRN-ULTRA\nRMSE = 18.69 HU\nReference\nFig. 2: Reconstructions of slice 100 from patient L192 using various methods. The display window is [800 1200] HU.\nLayer 1\nRMSE =27.44 HU\nLayer 5\nRMSE = 26.03 HU\nLayer 11\nRMSE = 25.91HU\nReference\nFig. 3: Image evolution over SUPER layers using the SUPER-\nWRN-ULTRA method. RMSE values are also indicated.\nvalues than µ = 0 (which concurs with Table II). ULTRA-\nbased SUPER especially achieves bigger drops in RMSE in\nearly SUPER layers compared to EP-based SUPER.\nE. Effect of Regularization Parameter β in SUPER Models\nWe investigated the effect of the unsupervised regularization\nparameter β in SUPER by taking the SUPER-FCN-ULTRA\nmethod as an example. We ﬁxed the regularization parameter\nµ = 5 × 105 for ULTRA-based SUPER methods. Table III\nshows mean RMSE results of SUPER-FCN-ULTRA using\nTABLE II: Mean reconstruction RMSE (HU) over 20 test\nslices using different µ values during training/testing in\nWavResNet-based SUPER.\n(a) SUPER-WRN-EP\nTrain\nTest\nµ = 0\nµ = 5 × 104\nµ = 1 × 106\nµ = 0\n27.3\n27.0\n78.7\nµ = 5 × 104\n27.8\n26.7\n45.3\nµ = 1 × 106\n31.2\n30.3\n26.6\n(b) SUPER-WRN-ULTRA\nTrain\nTest\nµ = 0\nµ = 5 × 105\nµ = 1 × 108\nµ = 0\n25.5\n25.2\n44.5\nµ = 5 × 105\n26.0\n25.4\n41.2\nµ = 1 × 108\n29.6\n28.3\n26.3\ndifferent β values during training and testing. We notice that\nfor the cases where β ̸= 0 during training, reconstructing\nimages in the testing set with the same β value as used during\ntraining achieved the best RMSE values. Training and testing\nwith nonzero β values (e.g., 5 × 103) achieved the best mean\nRMSE values overall in Table III. Moreover, when SUPER is\ntrained with β = 0, which means no unsupervised learning-\nbased prior was involved during training, one can improve the\nreconstruction quality by selecting a proper β value during\nthe testing stage. This suggests the beneﬁts of including the\nunsupervised learned union of transforms prior with a proper\nweight during testing time. For example, training SUPER with\nβ = 0 and then testing with β = 5 × 103 achieved around\n17 HU RMSE improvement compared to testing with β = 0.\n10\nTABLE III: Mean RMSE (HU) of 20 test slices using different\nβ values in SUPER-FCN-ULTRA.\ntrain\ntest\nβ = 0\nβ = 5 × 103\nβ = 1 × 104\nβ = 0\n45.6\n27.9\n30.7\nβ = 5 × 103\n133.5\n25.0\n25.9\nβ = 1 × 104\n172.7\n26.9\n25.4\n0\n5\n10\n15\nSUPER Layers\n20\n25\n30\n35\n40\n45\n50\nRMSE (HU)\nseq-WRN\nSUPER-WRN-EP- =0\nSUPER-WRN-EP- =5e4\nSUPER-WRN-ULTRA- =0\nSUPER-WRN-ULTRA- =5e5\n(a) WavResNet based SUPER\n0\n5\n10\n15\nSUPER Layers\n20\n25\n30\n35\n40\n45\n50\nRMSE (HU)\nseq-FCN\nSUPER-FCN-EP- =0\nSUPER-FCN-EP- =5e4\nSUPER-FCN-ULTRA- =0\nSUPER-FCN-ULTRA- =5e5\n(b) FBPConvNet based SUPER\nFig. 4: Mean RMSE (over test slices) comparisons among the\nproposed SUPER methods, and seq-WRN or seq-FCN, where\nthe WavRestNet and FBPConvNet architectures are repeated\n(x-axis indicates number of times repeated) or connected in\nsequence.\nF. Special Cases of SUPER Models\nWe now present experiments on some special cases of\nSUPER.\n1) Sequential Supervised Networks\nSequential supervised networks are a special case of the\nSUPER model with J(x, y) = 0 in (P0).\nThis is equiv-\nalent to deep networks connected in sequence and learned\nin a supervised and greedy manner, with the initial image\npassed through the sequence of deep models to obtain a\nreconstruction, with no MBIR components involved. We re-\nfer to the sequential supervised networks formed with the\nFBPConvNet and WavResNet architectures for the individual\nnetworks as seq-FCN and seq-WRN, respectively. Fig. 4 shows\nthe evolution of mean RMSE (over test slices) in seq-FCN\nand seq-WRN over the number of networks (SUPER layers)\nconnected sequentially. The sequential supervised networks\nunderperform the proposed SUPER methods (with WRN or\nFCN, and EP or ULTRA) by around 6 HU, which indicates\nthat the uniﬁed optimization approach incorporating data-\nﬁdelity terms and various priors can dramatically improve the\nreconstruction quality over deep networks in sequence.\n2) SUPER with Data-ﬁdelity only cost\nTo further explore the different special cases of SUPER,\nhere we empirically validate the relative effect of the data-\nﬁdelity term used in (P0) by setting β\n= µ = 0. In\nparticular, at reconstruction time, the initial FBP image is\npassed through networks learned in a supervised manner,\neach time followed by few iterations of descent on the data-\nﬁdelity cost, which enforces data consistency in conjunction\nwith the supervised learning-based network. Fig. 5 shows\nreconstructions using FBPConvNet, SUPER-FCN-DataTerm\n(i.e., β = µ = 0), PWLS-ULTRA, and SUPER-FCN-ULTRA,\nrespectively. For SUPER-FCN-DataTerm, when optimizing the\ndata-ﬁdelity term, we start with the deep network’s output and\nFBP\nRMSE = 194.09 HU\nPWLS-ULTRA\nRMSE = 43.40 HU\nFBPConvNet\nRMSE = 34.24 HU\nSUPER-FCN-\nDataTerm\nRMSE = 31.21 HU\nSUPER-FCN-\nULTRA\nRMSE = 28.82 HU\nReference\nFig. 5: Reconstructed images of slice 150 of patient L192\nusing of FBP, PWLS-ULTRA, FBPConvNet, SUPER-FCN-\nDataTerm, and SUPER-FCN-ULTRA, respectively, shown\nalong with the reference.\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15\nSUPER Layers\n20\n25\n30\n35\n40\nRMSE (HU)\n(a) SUPER-WRN-EP\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15\nSUPER Layers\n20\n25\n30\n35\n40\nRMSE (HU)\n(b) SUPER-WRN-ULTRA\nFig. 6: RMSE (box plots showing the spread over 20 test\nslices) over super layers for SUPER-WRN-EP and SUPER-\nWRN-ULTRA.\nran 5 iterations for the data-ﬁdelity term to avoid overﬁtting\nto the analytical FBP images. In Fig. 5, obviously, FBPCon-\nvNet signiﬁcantly suppresses noise and artifacts compared to\nPWLS-ULTRA, but it also over-smooths many details (e.g.,\nfeatures in the zoom-in box) in the reconstruction. SUPER-\nFCN-DataTerm, by enforcing data consistency, helps reduce\noverﬁtting issues and reconstructs image details and tissue\nboundaries better compared to the standalone FBPConvNet.\nOur SUPER-FCN-ULTRA method, however, exploits richer\nprior information (via the union of learned sparsifying trans-\nforms) and explicit network regularizer and outperforms the\nSUPER-FCN-DataTerm approach. Additional such compar-\nisons for other selected test slices are included in the sup-\nplement (Fig. 15).\n11\n3) SUPER with only a Supervised Regularizer\nThis special case of SUPER corresponds to the case where\nSUPER excludes the unsupervised regularizer, while only\ninvolves the data-ﬁdelity and a supervised regularizer in the\nMBIR cost, i.e., β = 0 and µ ̸= 0. In this case, the\nproposed SUPER model is similar to a generalized block\ncoordinate descent-based network by replacing a simple de-\nnoising autoencoder [28]–[30] with a general CNN that forms\nour supervised regularizer. This SUPER is also similar to the\nplug-and-play ADMM-Net method except that the inputs to\neach supervised network are the preceding reconstructions,\nwhile plug-and-play ADMM-Net updates inputs to the net-\nwork (denoiser) based on auxiliary variables in the ADMM\nalgorithm. Here, we used µ = 1 × 106, which worked well\nfor the plug-and-play ADMM-Net method, for this special\ncase of SUPER. Fig. 7 shows a comparison between plug-and-\nplay ADMM-Net, SUPER without unsupervised regularizers\n(in both training and testing), and the full SUPER version\nwith ULTRA regularization (β = 5 × 103, µ = 5 × 105). All\nthese methods used WavResNet as their denoisers/supervised\nnetworks. In this example, SUPER with only a supervised\nregularizer (SUPER-WRN-β = 0) outperforms the plug-and-\nplay ADMM-Net (ADMM-Net (WRN)) by 1.6 HU RMSE\nand provides sharper image details. Comparing SUPER-WRN-\nβ = 0 and the full SUPER-WRN-ULTRA scheme, we observe\nthat the latter provides a lower RMSE and higher contrast\nimage features than the former that excludes the unsupervised\ncomponent. This again shows the effect of the unsupervised\nULTRA model (in capturing local image details better with a\nunion of learned transforms) in the SUPER scheme.\nADMM-Net (WRN)\nRMSE = 32.90 HU\nSUPER-WRN-β = 0\nRMSE = 31.32 HU\nSUPER-WRN\n-ULTRA\nRMSE = 29.74 HU\nReference\nFig. 7: Reconstructed images of L192 slice 150 with WavRes-\nNet plugged ADMM-Net, SUPER-WRN-β = 0 (both training\nand testing), and SUPER-WRN-ULTRA (β = 5×103 in both\ntraining and testing), respectively, shown with the reference\nimage.\nG. Convergence Behavior of SUPER Methods\nFrom Fig. 4, we observe that our proposed SUPER methods\nwith proper choice of the regularization parameter µ provide\nconvergent mean RMSE curves within 15 SUPER layers.\nMoreover, Fig. 6 shows that the RMSE values of SUPER-\nWRN-EP and SUPER-WRN-ULTRA decrease and converge\nquickly over SUPER layers for reconstructing the test slices.\nWe observe a similar behavior for FBPConvNet based SUPER\nmethods and show it in the supplement (Section VII.G). We\nalso show in the supplement (Section VII.G) the convergence\nof the unsupervised iterative module’s cost at reconstruction\ntime (i.e., (P0)) for two test slices, as well as the training losses\ncorresponding to (P1) and the upper-level cost of the bilevel\nproblem in (5). The experimental results indicate that our\nproposed algorithm for SUPER can give practical convergent\nreconstructions with several layers (e.g., 15 layers).\nH. Computational Cost of SUPER Methods\nThe computational cost of SUPER methods closely relates\nto the run time of each SUPER layer and the total number of\nSUPER layers. In general, SUPER training takes longer time\ncompared to training the corresponding standalone supervised\nnetwork because of the sequential greedy training process\ninvolving iterative MBIR updates. In the testing stage, the\ncomputational cost of SUPER mainly depends on the number\nof iterations in each iterative module and the number of\nSUPER layers.\nOur experiments were performed with a 2.40 GHz Intel\nXeon Gold-6148 processor with (maximum) 40 threads and a\ngraphics processing unit Tesla P40. We report the runtime of\ntraining and testing for SUPER methods and their supervised\nand unsupervised counterparts in Table IV. In particular, for\nSUPER training, we report the approximate runtime of training\na single SUPER layer by averaging the runtime of training\n15 layers. From this table, we observe that SUPER training\n(with 15 layers) is more time consuming than training the\nstandalone supervised methods. During testing, the EP-based\nSUPER methods ran more slowly than PWLS-EP because of\noverall more iterations through all the SUPER layers (300\niterations for 15 layers in SUPER versus 100 iterations in\nstandalone PWLS-EP). The ULTRA-based SUPER methods\ncan work faster than the typical standalone PWLS-ULTRA\nmethod because of fewer overall required iterations (600\niterations for 15 layers in SUPER versus 1000 iterations in\nPWLS-ULTRA).\nTABLE IV: Runtime of SUPER methods and the constituent\nstandalone supervised and unsupervised methods during train-\ning and testing.\nMethod\nTrain\nTest (per image)\nPWLS-EP\n-\n1 minute (100 iters)\nPWLS-ULTRA\n5.5 hours\n1.5 hours (1000 iters)\nWavResNet\n20 hours (200 epochs)\n5 seconds\nSUPER-WRN-EP\n3 hours/layer\n5 minutes (15 layers)\nSUPER-WRN-ULTRA\n5.5 hours/layer\n30 minutes (15 layers)\nFBPConvNet\n10 hours (100 epochs)\n5 seconds\nSUPER-FCN-EP\n1 hour/layer\n5 minutes (15 layers)\nSUPER-FCN-ULTRA\n4 hours/layer\n30 minutes (15 layers)\n12\nI. Performance with Various Dose Levels and Generalization\nHere, we trained SUPER methods under another dose level\nwith I0 = 1×105. The training set was created using the same\nregular-dose images that were used to generate the training\ndata with I0 = 1 × 104 in the preceding experiments. The\nhyper-parameters for training the SUPER methods as well\nas the corresponding standalone supervised methods under\nI0 = 1×105 were the same as those under I0 = 1×104 which\nworked well. To evaluate the trained networks, we used the\nsame 20 test slices but generated their low-dose realizations\nwith I0 = 2 × 105, I0 = 1 × 105, I0 = 8 × 104, and\nI0 = 2 × 104, respectively. We used the same parameters as\nthose for I0 = 1 × 104 to reconstruct testing images under\nvarious dose levels to study model generalization.\nTable V reports averaged RMSE, SNR, and SSIM over the\n20 test samples under different dose levels using SUPER-\nFCN-ULTRA and its constituent unsupervised and supervised\nmethods, i.e., PWLS-ULTRA and FBPConvNet. For testing\nsamples under I0 = 1×105, where the dose levels of training\nand testing are matched, SUPER-FCN-ULTRA achieves the\nbest results compared to PWLS-ULTRA and FBPConvNet.\nWe show an example test image reconstructed by different\nmethods in Section VII.H in the supplement. When applying\nthe trained networks with I0\n= 1 × 105 to reconstruct\nimages under neighboring doses, e.g., I0\n=\n2 × 105 and\nI0 = 8 × 104, SUPER-FCN-ULTRA still performs better than\nPWLS-ULTRA and FBPConvNet. However, when the dose\nlevel of the testing samples differs too much from that of train-\ning samples, results with SUPER methods may not beat unsu-\npervised learning-based methods such as PWLS-ULTRA, but\nthey still outperform the corresponding supervised methods,\nbecause the MBIR components in SUPER moderate overﬁtting\nissues that easily happen to supervised methods. The bottom\nrow of Table V shows that when reconstructing images under\nI0 = 2 × 104 using the trained networks under I0 = 1 × 105,\nthe mean RMSE and SNR values of SUPER-FCN-ULTRA are\nin between of those of PWLS-ULTRA and FBPConvNet. The\nmean SSIM of SUPER-FCN-ULTRA is slightly better than\n(or comparable to) that of the unsupervised PWLS-ULTRA,\nand is much better than that of FBPConvNet, indicating\nthat the MBIR component in SUPER helps preserve image\nstructures and improves the generalization property compared\nwith the standalone supervised FBPConvNet method. The\nsupplement (Section VII.H) shows example reconstructions\nat I0 = 2 × 104, where SUPER-FCN-ULTRA outperforms\n(generalizes better than) PWLS-ULTRA and FBPConvNet.\nTABLE V: Averaged numerical results of 20 test slices under\ndifferent dose levels using networks trained with data under\nI0 = 1 × 105. The units of RMSE and SNR are HU and dB,\nrespectively.\nI0\nPWLS-ULTRA\n(RMSE/SNR/SSIM)\nFBPConvNet\n(RMSE/SNR/SSIM)\nSUPER-FCN-ULTRA\n(RMSE/SNR/SSIM)\n2 × 105\n15.0 / 34.1 / 0.84\n14.8 / 34.1 / 0.83\n13.3 / 35.0 / 0.84\n1 × 105\n15.9 / 33.2 / 0.81\n15.7 / 33.6 / 0.82\n14.7 / 34.1 / 0.82\n8 × 104\n16.7 / 32.9 / 0.78\n16.3 / 33.2 / 0.82\n15.2 / 33.8 / 0.82\n2 × 104\n23.3 / 30.6 / 0.72\n47.1 / 25.4 / 0.60\n31.6 / 28.5 / 0.74\nV. CONCLUSIONS\nThis paper proposed a mathematical framework for uni-\nﬁed supervised and unsupervised learning, dubbed SUPER,\nfor low-dose X-ray CT image reconstruction. The proposed\nSUPER framework combines physics-based forward models,\nstatistical models of measurements and noise, machine learned\nmodels, and analytical image models in a common frame-\nwork. Regularizers based on both supervised and unsuper-\nvised learning are jointly incorporated into model-based image\nreconstruction formulations and algorithms. We proposed an\nefﬁcient approximate algorithm for learning the proposed\nSUPER model. We studied four example SUPER methods by\ncombining the FBPConvNet and WavResNet architectures for\nthe supervised learning-based regularizer, and edge-preserving\nand union of learned transforms models for the analytical or\nunsupervised learning-based regularizer. We also performed\nablation studies of the SUPER model and demonstrated that\nusing the proposed mixture of models and priors in SUPER\nhelps improve the reconstruction quality in terms of reducing\nnoise and artifacts and reconstructing structural details. In\nfuture work, we plan to study acceleration of the algorithm\nas well as convergence theory for the proposed uniﬁed (ﬁxed\npoint or bilevel optimization based) training schemes, and will\napply SUPER methods in other imaging modalities. Additional\ndata augmentation tools will also be considered to further\nimprove the performance especially in limited training data\nsetups.\nVI. ACKNOWLEDGMENT\nThe authors thank Dr. Cynthia McCollough, the Mayo\nClinic, the American Association of Physicists in Medicine,\nand the National Institute of Biomedical Imaging and Bio-\nengineering for providing the Mayo Clinic data.\nREFERENCES\n[1] G. Wang, Y. Zhang, X. Ye, and X. Mou,\nMachine learning for\ntomographic imaging, IOP Publishing, 2019.\n[2] G. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and\nR. Willett, “Deep learning techniques for inverse problems in imaging,”\nIEEE Journal on Selected Areas in Information Theory, vol. 1, no. 1,\npp. 39–56, May 2020.\n[3] L. A. Shepp and B. F. Logan, “The fourier reconstruction of a head\nsection,” IEEE Trans. Nucl. Sci, vol. 21, no. 3, pp. 21–43, 1974.\n[4] L. C. Feldkamp, L. A .and Davis and J. W. Kress, “Practical cone-\nbeam algorithm,” J. Opt. Soc. Amer. A, Opt. Image Sci., vol. 1, no. 6,\npp. 612–619, 1984.\n[5] J. A. Fessler, “Penalized weighted least-squares image reconstruction\nfor positron emission tomography,” IEEE Trans. Med. Imag., vol. 13,\nno. 2, pp. 290–300, 1994.\n[6] J-B. Thibault, K. Sauer, C. Bouman, and J. Hsieh, “A three-dimensional\nstatistical approach to improved image quality for multi-slice helical\nCT,” Med. Phys., vol. 34, no. 11, pp. 4526–44, Nov. 2007.\n[7] M. Beister, D. Kolditz, and W. A Kalender, “Iterative reconstruction\nmethods in X-ray CT,” Physica Medica: European Journal of Medical\nPhysics, vol. 28, no. 2, pp. 94–108, 2012.\n[8] A. H. Delaney and Y. Bresler, “Globally convergent edge-preserving\nregularized reconstruction: an application to limited-angle tomography,”\nIEEE Trans. Im. Proc., vol. 7, no. 2, pp. 204–221, 1998.\n[9] G.-H. Chen, J. Tang, and S. Leng, “Prior image constrained compressed\nsensing (PICCS): a method to accurately reconstruct dynamic CT images\nfrom highly undersampled projection data sets,” Med. Phys., vol. 35,\nno. 2, pp. 660–663, 2008.\n[10] J.C. Ramirez-Giraldo, J. Trzasko, S. Leng, L. Yu, A. Manduca, and C. H.\nMcCollough, “Nonconvex prior image constrained compressed sensing\n(NCPICCS): Theory and simulations on perfusion CT,” Med. Phys., vol.\n38, no. 4, pp. 2157–2167, 2011.\n13\n[11] G.-H. Chen, P. Th´eriault-Lauzier, J. Tang, B. Nett, S. Leng, J. Zambelli,\nZ. Qi, N. Bevins, A. Raval, S. Reeder, et al.,\n“Time-resolved inter-\nventional cardiac c-arm cone-beam CT: An application of the PICCS\nalgorithm,” IEEE Trans. Med. Imag., vol. 31, no. 4, pp. 907–923, 2012.\n[12] Q. Xu, H. Yu, X. Mou, L. Zhang, J. Hsieh, and G. Wang, “Low-dose\nX-ray CT reconstruction via dictionary learning,” IEEE Trans. Med.\nImag., vol. 31, no. 9, pp. 1682–97, Sept. 2012.\n[13] X. Zheng, S. Ravishankar, Y. Long, and J. A. Fessler, “PWLS-ULTRA:\nAn efﬁcient clustering and learning-based approach for low-dose 3D\nCT image reconstruction,” IEEE Trans. Med. Imag., vol. 37, no. 6, pp.\n1498–1510, 2018.\n[14] S. Ye, S. Ravishankar, Y. Long, and J. A. Fessler, “SPULTRA: Low-\ndose CT image reconstruction with joint statistical and learned image\nmodels,” IEEE Trans. Med. Imag., vol. 39, no. 3, pp. 729–741, 2020.\n[15] S. Ravishankar and Y. Bresler, “Learning sparsifying transforms,” IEEE\nTrans. Signal Process., vol. 61, no. 5, pp. 1072–1086, 2013.\n[16] S. Ravishankar and Y. Bresler, “Learning doubly sparse transforms for\nimages,” IEEE Trans. Im. Proc., vol. 22, no. 12, pp. 4598–4612, 2013.\n[17] S. Ravishankar and Y. Bresler,\n“Data-driven learning of a union of\nsparsifying transforms model for blind compressed sensing,”\nIEEE\nTrans. Comput. Imag., vol. 2, no. 3, pp. 294–309, 2016.\n[18] B. Wen, S. Ravishankar, and Y. Bresler, “FRIST- ﬂipping and rotation\ninvariant sparsifying transform learning and applications,”\nInverse\nProblems, vol. 33, no. 7, pp. 074007, 2017.\n[19] L. Pﬁster and Y. Bresler, “Learning ﬁlter bank sparsifying transforms,”\nIEEE Trans. Signal Process., vol. 67, no. 2, pp. 504–519, 2018.\n[20] H. Chen, Y. Zhang, M. K. Kalra, F. Lin, Y. Chen, P. Liao, J. Zhou, and\nG. Wang, “Low-dose CT with a residual encoder-decoder convolutional\nneural network,” IEEE Trans. Med. Imag., vol. 36, no. 12, pp. 2524–\n2535, 2017.\n[21] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, “Deep convo-\nlutional neural network for inverse problems in imaging,” IEEE Trans.\nIm. Proc., vol. 26, no. 9, pp. 4509–22, 2017.\n[22] E. Kang, W. Chang, J. Yoo, and J. C. Ye, “Deep convolutional framelet\ndenoising for low-dose CT via wavelet residual network,” IEEE Trans.\nMed. Imag., vol. 37, no. 6, pp. 1358–1369, 2018.\n[23] T. W¨urﬂ, F. C. Ghesu, V. Christlein, and A. Maier,\n“Deep learning\ncomputed tomography,” in Med. Image Comput. Comput. Assist. Interv.\nSpringer, 2016, pp. 432–440.\n[24] J. He, Y. Wang, and J. Ma, “Radon inversion via deep learning,” IEEE\nTrans. Med. Imag., vol. 39, no. 6, pp. 2076–2087, 2020.\n[25] Y. Yang, J. Sun, H. Li, and Z. Xu, “Deep ADMM-Net for compressive\nsensing MRI,” in Proceedings of the 30th International Conference on\nNeural Information Processing Systems. Curran Associates Inc., 2016,\npp. 10–18.\n[26] J. Adler and O. ¨Oktem, “Learned primal-dual reconstruction,” IEEE\nTrans. Med. Imag., vol. 37, no. 6, pp. 1322–1332, 2018.\n[27] H. Chen, Y. Zhang, Y. Chen, J. Zhang, W. Zhang, H. Sun, Y. Lv, P. Liao,\nJ. Zhou, and G. Wang, “LEARN: Learned experts’ assessment-based\nreconstruction network for sparse-data CT,” IEEE Trans. Med. Imag.,\nvol. 37, no. 6, pp. 1333–1347, 2018.\n[28] S. Ravishankar, I. Y. Chun, and J. A. Fessler,\n“Physics-driven deep\ntraining of dictionary-based algorithms for MR image reconstruction,”\nin 2017 51st Asilomar Conference on Signals, Systems, and Computers,\n2017, pp. 1859–1863.\n[29] I. Y. Chun and J. A. Fessler, “Deep BCD-Net using identical encoding-\ndecoding CNN structures for iterative image recovery,” in 2018 IEEE\n13th Image, Video, and Multidimensional Signal Processing Workshop\n(IVMSP), 2018, pp. 1–5.\n[30] I. Y. Chun, X. Zheng, Y. Long, and J. A. Fessler, “BCD-Net for low-\ndose CT reconstruction: Acceleration, convergence, and generalization,”\nin Proc. Med. Image Computing and Computer Assist. Interven., Shen-\nzhen, China, Oct. 2019.\n[31] J. Adler and O. ¨Oktem,\n“Solving ill-posed inverse problems using\niterative deep neural networks,” Inverse Problems, vol. 33, no. 12, pp.\n124007, 2017.\n[32] S. H Chan, X. Wang, and O. A Elgendy, “Plug-and-play admm for image\nrestoration: Fixed-point convergence and applications,”\nIEEE Trans.\nComput. Imag., vol. 3, no. 1, pp. 84–98, 2016.\n[33] J. He, Y. Yang, Y. Wang, D. Zeng, Z. Bian, H. Zhang, J. Sun, Z. Xu, and\nJ. Ma, “Optimizing a parameterized plug-and-play admm for iterative\nlow-dose CT reconstruction,” IEEE Trans. Med. Imag., vol. 38, no. 2,\npp. 371–382, 2018.\n[34] J. Schlemper, J. Caballero, J. V Hajnal, A. Price, and D. Rueckert,\n“A deep cascade of convolutional neural networks for MR image\nreconstruction,” in International conference on information processing\nin medical imaging. Springer, 2017, pp. 647–658.\n[35] M. Mardani, H. Monajemi, V. Papyan, S. Vasanawala, D. Donoho,\nand J. Pauly, “Recurrent generative adversarial networks for proximal\nlearning and automated compressive image recovery,” arXiv preprint\narXiv:1711.10046, 2017.\n[36] H. K Aggarwal, M. P Mani, and M. Jacob, “MoDL: Model-based deep\nlearning architecture for inverse problems,” IEEE Trans. Med. Imag.,\nvol. 38, no. 2, pp. 394–405, 2018.\n[37] J. M. Wolterink, T. Leiner, M. A. Viergever, and I. Iˇsgum, “Generative\nadversarial networks for noise reduction in low-dose CT,” IEEE Trans.\nMed. Imag., vol. 36, no. 12, pp. 2536–2545, 2017.\n[38] E. Kang, H. J. Koo, D. H. Yang, J. B. Seo, and J. C. Ye,\n“Cycle-\nconsistent adversarial denoising network for multiphase coronary CT\nangiography,” Med. Phys., vol. 46, no. 2, pp. 550–562, 2019.\n[39] B. Sim, G. Oh, S. Lim, and J. C. Ye, “Optimal transport, cycleGAN,\nand penalized LS for unsupervised learning in inverse problems,” arXiv\npreprint arXiv:1909.12116, 2019.\n[40] M. Mardani, E. Gong, J. Y Cheng, S. Vasanawala, G. Zaharchuk, M.s\nAlley, N. Thakur, S. Han, W. Dally, J. M Pauly, et al., “Deep generative\nadversarial networks for compressed sensing automates MRI,” arXiv\npreprint arXiv:1706.00051, 2017.\n[41] D. Ulyanov, A. Vedaldi, and V. Lempitsky,\n“Deep image prior,”\nin\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 9446–9454.\n[42] K. Gong, C. Catana, J. Qi, and Q. Li, “PET image reconstruction using\ndeep image prior,” IEEE Trans. Med. Imag., vol. 38, no. 7, pp. 1655–\n1665, 2018.\n[43] K. H. Jin, H. Gupta, J. Yerly, M. Stuber, and M. Unser, “Time-dependent\ndeep image prior for dynamic MRI,” arXiv preprint arXiv:1910.01684,\n2019.\n[44] D O. Baguer, J. Leuschner, and M. Schmidt, “Computed tomography\nreconstruction using deep image prior and learned reconstruction meth-\nods,” Inverse Problems, vol. 36, no. 9, pp. 094004, 2020.\n[45] Z. Li, S. Ye, Y. Long, and S. Ravishankar,\n“SUPER learning: A\nsupervised-unsupervised framework for low-dose CT image reconstruc-\ntion,” in Proceedings of the IEEE International Conference on Computer\nVision Workshops, 2019, pp. 3959–3968.\n[46] H. Shan, A. Padole, F. Homayounieh, U. Kruger, R. D. Khera, C. Niti-\nwarangkul, M. K Kalra, and G. Wang, “Competitive performance of a\nmodularized deep neural network compared to commercial algorithms\nfor low-dose ct image reconstruction,” Nat. Mach. Intell., vol. 1, no. 6,\npp. 269–276, 2019.\n[47] M. T McCann and S. Ravishankar, “Supervised learning of sparsity-\npromoting regularizers for denoising,” arXiv preprint arXiv:2006.05521,\n2020.\n[48] J. H. Cho and J. A. Fessler, “Regularization designs for uniform spatial\nresolution and noise properties in statistical image reconstruction for 3D\nX-ray CT,” IEEE Trans. Med. Imag., vol. 34, no. 2, pp. 678–89, Feb.\n2015.\n[49] S. V. Venkatakrishnan, C. A. Bouman, and B. Wohlberg, “Plug-and-play\npriors for model based reconstruction,” in 2013 IEEE Global Conference\non Signal and Information Processing, 2013, pp. 945–948.\n[50] S. G. Lingala and M. Jacob, “Blind compressive sensing dynamic MRI,”\nIEEE Trans. Med. Imag., vol. 32, no. 6, pp. 1132–1145, 2013.\n[51] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimiza-\ntion,” in Proc. ICLR 2015, San Diego, CA, May 2015, pp. 1–15.\n[52] H. Nien and J. A. Fessler, “Relaxed linearized algorithms for faster\nX-ray CT image reconstruction,” IEEE Trans. Med. Imag., vol. 35, no.\n4, pp. 1090–8, Apr. 2016.\n[53] C. McCollough, “TU-FG-207A-04: Overview of the low dose CT grand\nchallenge.,” Med. Phys., vol. 43, no. 2, pp. 3759–60, 2016.\n[54] L. Fu, T. C. Lee, S. M. Kim, A. M. Alessio, P. E. Kinahan, Z. Q. Chang,\nK. Sauer, M. K. Kalra, and B. De Man, “Comparison between pre-\nlog and post-log statistical models in ultra-low-dose CT reconstruction,”\nIEEE Trans. Med. Imag., vol. 36, no. 3, pp. 707–720, 2017.\n[55] R. Zeng, N. Petrick, M. A Gavrielides, and K. J Myers, “Approximations\nof noise covariance in multi-slice helical CT scans: impact on lung\nnodule size estimation,” Phys. Med. Biol., vol. 56, no. 19, pp. 6223,\n2011.\n[56] S. Ye, Z. Li, M. T. McCann, Y. Long, and S. Ravishankar, “Uniﬁed\nsupervised-unsupervised (SUPER) learning for X-ray CT image recon-\nstruction,” IEEE Trans. Med. Imag., 2020. Submitted.\n14\nUniﬁed Supervised-Unsupervised (SUPER)\nLearning for X-ray CT Image Reconstruction\n– Supplementary Materials\nThis supplement provides additional details and results to\naccompany our manuscript.\nVII. ADDITIONAL EXPERIMENTAL RESULTS\nA. SNR and SSIM Comparisons\nWe have shown the RMSE spread over 20 test cases using\ndifferent methods in the main paper (Fig. 1). Here, we show\nthe SNR and SSIM variations over these test cases in Fig. 8\nand Fig. 9, respectively. Both ﬁgures show the superiority of\nthe proposed SUPER methods compared to standalone super-\nvised and unsupervised methods in terms of robustness and\nnumerical improvements. SUPER methods also outperform the\nplug-and-play ADMM-Net method (ADMM-Net (WRN)).\nMethods\nSNR (dB)\nFig. 8: SNR spread (shown using box plots) over 20 test cases\nusing different methods. Here, “wrn” stands for WavResNet,\n“fcn” stands for FBPConvNet.\nMethods\nSSIM\nFig. 9: SSIM spread (shown using box plots) over 20 test cases\nusing different methods. Here, “wrn” stands for WavResNet,\n“fcn” stands for FBPConvNet.\nB. Visual Results of SUPER\nFig. 2 of our main paper compared the reconstructions\nwith various methods for one test sample. Here, Fig. 10 and\nFig. 11 show another two sets of comparisons for slice 100\nand slice 150 of patient L067. Particularly, Fig. 11 shows\nreconstructions with the most corrupted measurement data in\nthe test set, wherein both standalone iterative (unsupervised)\nmethods PWLS-EP and PWLS-ULTRA have limited perfor-\nmance. The proposed methods (SUPER-FCN-EP, SUPER-\nWRN-EP, SUPER-FCN-ULTRA, and SUPER-WRN-ULTRA)\nsigniﬁcantly reduce noise and artifacts, and improve the edge\nsharpness in the soft tissues and bones compared to other\ncompeting methods.\nC. Comparison of Bias and Standard Deviation Trade-offs\nWe investigated the bias and standard deviation (STD) trade-\noffs of SUPER methods and the compared methods based on\nseveral uniform regions (ROIs) of the reconstructed images\nusing such methods. The bias of a uniform ROI was computed\nas the mean error between the reconstructed ROI (ˆxROI) and\nthe reference ROI (x∗\nROI), i.e., Bias = mean(ˆxROI −x∗\nROI),\nwhile the STD was calculated as STD(xROI), where x = ˆx\nfor reconstructed images and x = x∗for the reference image.\nTo better illustrate the bias-STD trade-offs, we deﬁne a “B-\nS Index” as\np\nBias2 + STD2. Smaller values of this metric\nimply better bias-STD trade-offs [55]. In Fig. 12, we show two\nuniform regions (ROI 1 and ROI 2) of the test sample L192\nslice 100 in the reference image and plot (bias, STD) pairs\n(blue markers for ROI 1 and red markers for ROI 2 for these\ntwo ROIs reconstructed by different methods. From Fig. 12b,\nwe notice that although the two ROIs are located in different\nparts of the image, they have similar STD values (close to\n10 HU) in the reference image (which also contains noise).\nFor both ROIs, we observe that SUPER methods (indicated in\nsolid markers) have relatively stable and good bias-STD trade-\noffs with B-S indexes scattered around isocontours of 6 ∼\n8 HU. The bias-STD trade-off of the plug-and-play ADMM\nmethod with WavResNet denoiser varies a lot for the different\nROIs, while that of PWLS-ULTRA is small and stable for\nboth ROIs. The supervised methods, MAP-NN, FBPConvNet,\nand WavResNet, have relatively stable bias-STD trade-offs in\nboth ROIs, while FBPConvNet has the smallest B-S indexes\nand WavResNet has the largest B-S indexes among these three\nmethods.\nD. Image Evolution over SUPER Layers with FBPConvNet-\nbased Methods\nWe show examples of image evolution across SUPER layers\nusing SUPER-FCN-ULTRA in Fig. 13, and using SUPER-\n15\nFBP\nRMSE = 101.1 HU\nFBPConvNet\nRMSE = 29.1 HU\nWavResNet\nRMSE = 30.2 HU\nMAP-NN\nRMSE = 33.0 HU\nPWLS-EP\nRMSE = 33.7 HU\nSUPER-\nFCN-EP\nRMSE = 25.7 HU\nSUPER-\nWRN-EP\nRMSE = 26.1 HU\nADMM-Net\n(WRN)\nRMSE = 28.5 HU\nPWLS-ULTRA\nRMSE = 26.7 HU\nSUPER-\nFCN-ULTRA\nRMSE = 25.0 HU\nSUPER-\nWRN-ULTRA\nRMSE = 25.2 HU\nReference\nFig. 10: Reconstructions of slice 100 from patient L067 using various methods. The display window is [800 1200] HU.\nFBP\nRMSE = 231.7 HU\nFBPConvNet\nRMSE = 38.0 HU\nWavResNet\nRMSE = 53.1 HU\nMAP-NN\nRMSE = 49.5 HU\nPWLS-EP\nRMSE = 67.6 HU\nSUPER-\nFCN-EP\nRMSE = 33.6 HU\nSUPER-\nWRN-EP\nRMSE = 33.4 HU\nADMM-Net\n(WRN)\nRMSE = 36.4 HU\nPWLS-ULTRA\nRMSE = 67.8 HU\nSUPER-\nFCN-ULTRA\nRMSE = 31.7 HU\nSUPER-\nWRN-ULTRA\nRMSE = 32.7 HU\nReference\nFig. 11: Reconstructions of slice 150 from patient L067 using various methods. The display window is [800 1200] HU. This\nsample is with the most corrupted measurement data in the test set, and SUPER methods obviously reconstruct better images\nthan the compared methods.\n16\nROI 1\nROI 2\n(a) Reference Image (L192 slice 100)\n-10\n-5\n0\n5\n10\n15\n20\nBias (HU)\n2\n4\n6\n8\n10\n12\n14\nSTD (HU)\n4\n6\n6\n8\n8\n8\n10\n10\n10\n12\n12\n12\n14\n14\n14\n14\n16\n16\n16\n18\n18\n20\n20\n22\nreference\npwls-ep\npwls-ultra\nfcn\nsuper-fcn-ep\nsuper-fcn-ultra\nwrn\nsuper-wrn-ep\nsuper-wrn-ultra\nadmm-net(wrn)\nmap-nn\n(b) Bias-STD Plot\nFig. 12: Bias-STD plots for two uniform ROIs indicated in (a)\nwith different reconstruction methods. In (b), the blue markers\nrepresent bias-std pairs for ROI 1 and the red markers represent\nbias-std pairs for ROI 2.\nFCN-EP in Fig. 14, respectively. Both examples indicate that\nearly SUPER layers strongly suppress noise and artifacts,\nwhile later SUPER layers help with reconstructing detailed\nstructures.\nE. Inﬂuence of µ Choice for FBPConvNet-based SUPER\nTable V shows that choosing the same µ value (i.e., the\nweight for the regularizer involving the deep CNN learned\nin a supervised manner) during training and testing is quite\neffective.\nF. SUPER with Only Data-Fidelity Cost\nSection IV.F (2) of our manuscript [56] shows reconstruc-\ntions of one test sample (slice 150 of patient L192) using\nFBPConvNet, SUPER-FCN-DataTerm, PWLS-ULTRA, and\nSUPER-FCN-ULTRA. Fig. 15 shows the comparisons for\nanother test slice. We observe the similar phenomenon as in\nthe main paper that SUPER-FCN-DataTerm outperforms the\nstandalone FBPConvNet method and PWLS-ULTRA method,\nwhile the unsupervised regularizer involved SUPER-FCN-\nULTRA method further improves the reconstruction qualities.\nLayer 1\nRMSE = 37.00 HU\nLayer 5\nRMSE = 29.78 HU\nLayer 11\nRMSE = 29.14 HU\nReference\nFig. 13: Image evolution over SUPER layers using SUPER-\nFCN-ULTRA method.\nLayer 1\nRMSE = 28.47 HU\nLayer 5\nRMSE = 21.46 HU\nLayer 11\nRMSE = 20.91 HU\nReference\nFig. 14: Image evolution over SUPER layers using SUPER-\nFCN-EP method.\nG. Convergence Behavior of SUPER Reconstruction\nWe have shown in Fig. 6 in the manuscript the convergence\nbehavior in terms of RMSE of test slices using WavResNet\nbased SUPER. Here, Fig. 16 demonstrates a similar RMSE\nconvergence behavior (over SUPER layers) using FBPCon-\nvNet based SUPER. We also plot in Fig. 17 the ULTRA-\nbased iterative module’s costs of two test examples at the end\n(MBIR) iteration in each SUPER layer to indicate the achieved\n(P0) cost in each SUPER layer. In these two test examples,\n17\nTABLE V: Mean RMSE (HU) of 20 test slices using different\nµ values in FBPConvNet based SUPER.\n(a) SUPER-FCN-EP\ntrain\ntest\nµ = 0\nµ = 5 × 104\nµ = 5 × 105\nµ = 0\n26.7\n27.0\n59.9\nµ = 5 × 104\n27.3\n26.0\n41.0\nµ = 5 × 105\n30.2\n29.3\n26.3\n(b) SUPER-FCN-ULTRA\ntrain\ntest\nµ = 0\nµ = 5 × 105\nµ = 5 × 106\nµ = 0\n25.3\n25.3\n29.5\nµ = 5 × 105\n25.3\n25.0\n28.3\nµ = 5 × 106\n26.3\n26.2\n25.1\nFBP\nRMSE = 60.48 HU\nPWLS-ULTRA\nRMSE = 26.73 HU\nFBPConvNet\nRMSE = 23.83 HU\nSUPER-FCN-DataTerm\nRMSE = 22.04 HU\nSUPER-FCN-ULTRA\nRMSE = 21.72 HU\nReference\nFig.\n15:\nReconstructed\nimages\nof\nL067\nslice20\nusing\nFBP, PWLS-ULTRA, FBPConvNet, SUPER-FCN-DataTerm,\nSUPER-FCN-ULTRA, and the reference image, respectively.\nthe nonconvex cost function decreases quickly during initial\nSUPER layers and varies only slightly in later layers for both\nSUPER-WRN-ULTRA and SUPER-FCN-ULTRA methods.\nThe same behavior happens for other test samples as well. This\nindicates that our proposed algorithm can achieve empirically\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSUPER Layers\n20\n25\n30\n35\n40\nRMSE (HU)\n(a) SUPER-FCN-EP\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nSUPER Layers\n20\n25\n30\n35\n40\nRMSE (HU)\n(b) SUPER-FCN-ULTRA\nFig. 16: RMSE spread of 20 test slices over SUPER layers of\nthe SUPER-FCN-EP and SUPER-FCN-ULTRA algorithms.\n0\n5\n10\n15\nSUPER layers\n1.181\n1.182\n1.183\n1.184\n1.185\nMBIR cost\n10 15\n(a) Slice 100 of patient L067\n0\n5\n10\n15\nSUPER layers\n1.2000\n1.2005\n1.2010\n1.2015\n1.2020\nMBIR cost\n10 15\n(b) Slice 100 of patient L192\n0\n5\n10\n15\nSUPER layers\n1.200\n1.220\n1.240\n1.260\n1.280\nMBIR cost\n10 15\n(c) Slice 100 of patient L067\n0\n5\n10\n15\nSUPER layers\n1.210\n1.220\n1.230\n1.240\n1.250\n1.260\nMBIR cost\n10 15\n(d) Slice 100 of patient L192\nFig.\n17:\nULTRA-based\nreconstruction’s\ncost\nfunction\n(achieved cost in (P0)) plotted over the SUPER layers of\nSUPER-WRN-ULTRA (the ﬁrst row) and SUPER-FCN-\nULTRA (the second row), when reconstructing two selected\ntest slices.\n0\n200\n400\n600\n800\nEpochs x Layers\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nUpper-Level Cost\n10 5\nlayer 1\nlayer 2\nlayer 3\nlayer 4\nlayer 5\nlayer 6\nlayer 7\nlayer 8\nlayer 9\nlayer 10\nlayer 11\nlayer 12\nlayer 13\nlayer 14\nlayer 15\n(a) Cost correspond to (P1).\n0\n5\n10\n15\nSUPER Layers\n500\n600\n700\n800\n900\n1000\n1100\n1200\nUpper-Level Cost\n(b) Upper-level cost in Eq. (5).\nFig. 18: Convergence of Training. (a) The cost corresponding\nto (P1) is plotted over training epochs and over all SUPER\nlayers sequentially. (b) The cost corresponding to the upper-\nlevel cost of (5) is plotted over each SUPER layer based on\nreconstructions and network weights from the same (lth) layer,\ni.e., ˆx(l)\nθ(l)(y) and θ(l).\n18\nstable results even with nonconvex cost functions. In Fig. 18,\nwe plot the training cost (P1) as well as the upper-level cost\ncorresponding to the bilevel problem in Eq. (5) of [56] for\nSUPER-WRN-ULTRA. The training loss in (P1) is plotted\nover training epochs and over each SUPER layer sequentially.\nIt is obvious that the training loss is convergent within 15\nSUPER layers (with 50 epochs in each layer). Fig. 18b roughly\nreﬂects the upper-level cost of the bilevel problem (Eq. (5)),\nas we plugged (back) into the upper-level cost (as network\ninputs), the approximate reconstructions from the lower-level\ncost, using networks trained in the current layer. We observe\nthat the upper-level cost in the bilevel problem decreases\ndramatically in the ﬁrst ﬁve SUPER layers and then tends\nto converge with small oscillations within 15 layers, which\nprovided practical stable reconstructions in our experiments.\nThis indicates potential for the proposed alternating training\nalgorithm as a heuristic for the bilevel problem in Eq. (5)\nof [56].\nH. Examples with Various Dose Levels and Generalization\nThis subsection shows examples of reconstructions with\ndifferent dose levels (corresponding to Section IV.I in the\nmain paper). Fig. 19 shows an example test slice recon-\nstructed by different methods under I0 = 1 × 105 in both\ntraining and testing. In this ﬁgure, the SUPER reconstructed\nimage looks cleaner and sharper than the ones reconstructed\nby the constituent standalone supervised and unsupervised\niterative methods. In Fig. 20, we show reconstructions for\ndose I0 = 2 × 104 using the supervised networks trained\nunder the very different dose I0 = 1 × 105. In this example,\nthe image provided by FBPConvNet is much noiser than\nthe PWLS-ULTRA reconstruction, while combining these two\nmodels in SUPER, i.e., SUPER-FCN-ULTRA, achieved better\nresults than both of its standalone counterparts in terms of\nRMSE values and image quality (less noise and sharper\nedges). This indicates that it is still possible that SUPER-\nFCN-ULTRA outperforms (generalizes better than) the recent\nunsupervised PWLS-ULTRA when signiﬁcant dose mismatch\nappears between training and testing. We show the reference\nimages of these two examples in Fig. 21.\nFBP\nRMSE =26.7 HU\nPWLS-ULTRA\nRMSE = 16.4 HU\nFBPConvNet\nRMSE = 12.2 HU\nSUPER-FCN-ULTRA\nRMSE = 11.1 HU\nFig. 19: Reconstructed images of slice 100 of patient L192\nunder I0 = 1 × 105 using different methods.\nFBP\nRMSE =54.8 HU\nPWLS-ULTRA\nRMSE = 24.8 HU\nFBPConvNet\nRMSE = 29.9 HU\nSUPER-FCN-ULTRA\nRMSE = 21.5 HU\nFig. 20: Reconstructed images of slice 100 of patient L067\nunder I0 = 2 × 104 using different methods. FBPConvNet\nand SUPER-FCN-ULTRA used networks trained under I0 =\n1 × 105.\nL192 slice100\nL067 slice100\nFig. 21: Reference images corresponding to Fig. 19 and\nFig. 20, respectively.\n",
  "categories": [
    "eess.SP",
    "eess.IV"
  ],
  "published": "2020-10-06",
  "updated": "2021-04-08"
}