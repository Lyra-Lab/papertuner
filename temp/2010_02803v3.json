{
  "id": "http://arxiv.org/abs/2010.02803v3",
  "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning",
  "authors": [
    "George Zerveas",
    "Srideepika Jayaraman",
    "Dhaval Patel",
    "Anuradha Bhamidipaty",
    "Carsten Eickhoff"
  ],
  "abstract": "In this work we propose for the first time a transformer-based framework for\nunsupervised representation learning of multivariate time series. Pre-trained\nmodels can be potentially used for downstream tasks such as regression and\nclassification, forecasting and missing value imputation. By evaluating our\nmodels on several benchmark datasets for multivariate time series regression\nand classification, we show that not only does our modeling approach represent\nthe most successful method employing unsupervised learning of multivariate time\nseries presented to date, but also that it exceeds the current state-of-the-art\nperformance of supervised methods; it does so even when the number of training\nsamples is very limited, while offering computational efficiency. Finally, we\ndemonstrate that unsupervised pre-training of our transformer models offers a\nsubstantial performance benefit over fully supervised learning, even without\nleveraging additional unlabeled data, i.e., by reusing the same data samples\nthrough the unsupervised objective.",
  "text": "A TRANSFORMER-BASED FRAMEWORK FOR MULTI-\nVARIATE TIME SERIES REPRESENTATION LEARNING\nGeorge Zerveas\nBrown University\nProvidence, RI, USA\ngeorge zerveas@brown.edu\nSrideepika Jayaraman,\nIBM Research\nYorktown Heights, NY, USA\nj.srideepika@ibm.com\nDhaval Patel\nIBM Research\nYorktown Heights, NY, USA\npateldha@us.ibm.com\nAnuradha Bhamidipaty\nIBM Research\nYorktown Heights, NY, USA\nanubham@us.ibm.com\nCarsten Eickhoff\nBrown University\nProvidence, RI, USA\ncarsten@brown.edu\nABSTRACT\nIn this work we propose for the ﬁrst time a transformer-based framework for un-\nsupervised representation learning of multivariate time series. Pre-trained models\ncan be potentially used for downstream tasks such as regression and classiﬁcation,\nforecasting and missing value imputation. By evaluating our models on several\nbenchmark datasets for multivariate time series regression and classiﬁcation, we\nshow that our modeling approach represents the most successful method employ-\ning unsupervised learning of multivariate time series presented to date; it is also\nthe ﬁrst unsupervised approach shown to exceed the current state-of-the-art per-\nformance of supervised methods. It does so by a signiﬁcant margin, even when\nthe number of training samples is very limited, while offering computational efﬁ-\nciency. Finally, we demonstrate that unsupervised pre-training of our transformer\nmodels offers a substantial performance beneﬁt over fully supervised learning,\neven without leveraging additional unlabeled data, i.e., by reusing the same data\nsamples through the unsupervised objective.\n1\nINTRODUCTION\nMultivariate time series (MTS) are an important type of data that is ubiquitous in a wide variety\nof domains, including science, medicine, ﬁnance, engineering and industrial applications. As the\nname suggests, they typically represent the evolution of a group of synchronous variables (e.g.,\nsimultaneous measurements of different physical quantities) over time, but they can more generally\nrepresent a group of dependent variables (abscissas) aligned with respect to a common independent\nvariable, e.g., absorption spectra under different conditions as a function of light frequency. Despite\nthe recent abundance of MTS data in the much touted era of “Big Data”, the availability of labeled\ndata in particular is far more limited: extensive data labeling is often prohibitively expensive or\nimpractical, as it may require much time and effort, special infrastructure or domain expertise. For\nthis reason, in all aforementioned domains there is great interest in methods which can offer high\naccuracy by using only a limited amount of labeled data or by leveraging the existing plethora of\nunlabeled data.\nThere is a large variety of modeling approaches for univariate and multivariate time series, with\ndeep learning models recently challenging and at times replacing the state of the art in tasks such as\nforecasting, regression and classiﬁcation (De Brouwer et al., 2019; Tan et al., 2020a; Fawaz et al.,\n2019b). However, unlike in domains such as Computer Vision or Natural Language Processing\n(NLP), the dominance of deep learning for time series is far from established: in fact, non-deep\nlearning methods such as TS-CHIEF (Shifaz et al., 2020), HIVE-COTE (Lines et al., 2018), and\nROCKET (Dempster et al., 2020) currently hold the record on time series regression and classiﬁca-\ntion dataset benchmarks (Tan et al., 2020a; Bagnall et al., 2017), matching or even outperforming\n1\narXiv:2010.02803v3  [cs.LG]  8 Dec 2020\nsophisticated deep architectures such as InceptionTime (Fawaz et al., 2019a) and ResNet (Fawaz\net al., 2019b).\nIn this work, we investigate, for the ﬁrst time, the use of a transformer encoder for unsupervised\nrepresentation learning of multivariate time series, as well as for the tasks of time series regression\nand classiﬁcation. Transformers are an important, recently developed class of deep learning models,\nwhich were ﬁrst proposed for the task of natural language translation (Vaswani et al., 2017) but have\nsince come to monopolize the state-of-the-art performance across virtually all NLP tasks (Raffel\net al., 2019). A key factor for the widespread success of transformers in NLP is their aptitude for\nlearning how to represent natural language through unsupervised pre-training (Brown et al., 2020;\nRaffel et al., 2019; Devlin et al., 2018). Besides NLP, transformers have also set the state of the art\nin several domains of sequence generation, such as polyphonic music composition (Huang et al.,\n2018).\nTransformer models are based on a multi-headed attention mechanism that offers several key ad-\nvantages and renders them particularly suitable for time series data (see Appendix section A.4 for\ndetails).\nInspired by the impressive results attained through unsupervised pre-training of transformer models\nin NLP, as our main contribution, in the present work we develop a generally applicable method-\nology (framework) that can leverage unlabeled data by ﬁrst training a transformer model to extract\ndense vector representations of multivariate time series through an input denoising (autoregressive)\nobjective. The pre-trained model can be subsequently applied to several downstream tasks, such\nas regression, classiﬁcation, imputation, and forecasting. Here, we apply our framework for the\ntasks of multivariate time series regression and classiﬁcation on several public datasets and demon-\nstrate that transformer models can convincingly outperform all current state-of-the-art modeling\napproaches, even when only having access to a very limited amount of training data samples (on\nthe order of hundreds of samples), an unprecedented success for deep learning models. To the best\nof our knowledge, this is also the ﬁrst time that unsupervised learning has been shown to confer an\nadvantage over supervised learning for classiﬁcation and regression of multivariate time series with-\nout utilizing additional unlabeled data samples. Importantly, despite common preconceptions about\ntransformers from the domain of NLP, where top performing models have billions of parameters\nand require days to weeks of pre-training on many parallel GPUs or TPUs, we also demonstrate that\nour models, using at most hundreds of thousands of parameters, can be practically trained even on\nCPUs; training them on GPUs allows them to be trained as fast as even the fastest and most accurate\nnon-deep learning based approaches.\n2\nRELATED WORK\nRegression and classiﬁcation of time series: Currently, non-deep learning methods such as TS-\nCHIEF (Shifaz et al., 2020), HIVE-COTE (Lines et al., 2018), and ROCKET (Dempster et al.,\n2020) constitute the state of the art for time series regression and classiﬁcation based on evaluations\non public benchmarks (Tan et al., 2020a; Bagnall et al., 2017), followed by CNN-based deep archi-\ntectures such as InceptionTime (Fawaz et al., 2019a) and ResNet (Fawaz et al., 2019b). ROCKET,\nwhich on average is the best ranking method, is a fast method that involves training a linear classi-\nﬁer on top of features extracted by a ﬂat collection of numerous and various random convolutional\nkernels. HIVE-COTE and TS-CHIEF (itself inspired by Proximity Forest (Lucas et al., 2019)),\nare very sophisticated methods which incorporate expert insights on time series data and consist\nof large, heterogeneous ensembles of classiﬁers utilizing shapelet transformations, elastic similarity\nmeasures, spectral features, random interval and dictionary-based techniques; however, these meth-\nods are highly complex, involve signiﬁcant computational cost, cannot beneﬁt from GPU hardware\nand scale poorly to datasets with many samples and long time series; moreover, they have been\ndeveloped for and only been evaluated on univariate time series.\nUnsupervised learning for multivariate time series: Recent work on unsupervised learning for\nmultivariate time series has predominantly employed autoencoders, trained with an input recon-\nstruction objective and implemented either as Multi-Layer Perceptrons (MLP) or RNN (most com-\nmonly, LSTM) networks. As interesting variations of the former, Kopf et al. (2019) and Fortuin\net al. (2019) additionally incorporated Variational Autoencoding into this approach, but focused on\nclustering and the visualization of shifting sample topology with time. As an example of the lat-\n2\nter, Malhotra et al. (2017) presented a multi-layered RNN sequence-to-sequence autoencoder, while\nLyu et al. (2018) developed a multi-layered LSTM with an attention mechanism and evaluated both\nan input reconstruction (autoencoding) as well as a forecasting loss for unsupervised representation\nlearning of Electronic Healthcare Record multivariate time series.\nAs a novel take on autoencoding, and with the goal of dealing with missing data, Bianchi et al.\n(2019) employ a stacked bidirectional RNN encoder and stacked RNN decoder to reconstruct the\ninput, and at the same time use a user-provided kernel matrix as prior information to condition\ninternal representations and encourage learning similarity-preserving representations of the input.\nThey evaluate the method on the tasks of missing value imputation and classiﬁcation of time series\nunder increasing “missingness” of values. For the purpose of time series clustering, Lei et al. (2017)\nalso follow a method which aims at preserving similarity between time series by directing learned\nrepresentations to approximate a distance such as Dynamic Time Warping (DTW) between time\nseries through a matrix factorization algorithm.\nA distinct approach is followed by Zhang et al. (2019), who use a composite convolutional - LSTM\nnetwork with attention and a loss which aims at reconstructing correlation matrices between the\nvariables of the multivariate time series input. They use and evaluate their method only for the task\nof anomaly detection.\nFinally, Jansen et al. (2018) rely on a triplet loss and the idea of temporal proximity (the loss rewards\nsimilarity of representations between proximal segments and penalizes similarity between distal\nsegments of the time series) for unsupervised representation learning of non-speech audio data. This\nidea is explored further by Franceschi et al. (2019), who combine the triplet loss with a deep causal\nCNN with dilation, in order to make the method effective for very long time series. Although on the\ntask of univariate classiﬁcation the method is outperformed by the aforementioned supervised state-\nof-the-art methods, it is the best performing method leveraging unsupervised learning for univariate\nand multivariate classiﬁcation datasets of the UEA/UCR archive (Bagnall et al., 2017).\nTransformer models for time series: Recently, a full encoder-decoder transformer architecture\nwas employed for univariate time series forecasting: Li et al. (2019) showed superior performance\ncompared to the classical statistical method ARIMA, the recent matrix factorization method TRMF,\nan RNN-based autoregressive model (DeepAR) and an RNN-based state space model (DeepState)\non 4 public forecasting datasets, while Wu et al. (2020) used a transformer to forecast inﬂuenza\nprevalence and similarly showed performance beneﬁts compared to ARIMA, an LSTM and a GRU\nSeq2Seq model with attention, and Lim et al. (2020) used a transformer for multi-horizon uni-\nvariate forecasting, supporting interpretation of temporal dynamics. Finally, Ma et al. (2019) use an\nencoder-decoder architecture with a variant of self-attention for imputation of missing values in mul-\ntivariate, geo-tagged time series and outperform classic as well as the state-of-the-art, RNN-based\nimputation methods on 3 public and 2 competition datasets for imputation.\nBy contrast, our work aspires to generalize the use of transformers from solutions to speciﬁc gen-\nerative tasks (which require the full encoder-decoder architecture) to a framework which allows for\nunsupervised pre-training and with minor modiﬁcations can be readily used for a wide variety of\ndownstream tasks; this is analogous to the way BERT (Devlin et al., 2018) converted a translation\nmodel into a generic framework based on unsupervised learning, an approach which has become a\nde facto standard and established the dominance of transformers in NLP.\n3\nMETHODOLOGY\n3.1\nBASE MODEL\nAt the core of our method lies a transformer encoder, as described in the original transformer work\nby Vaswani et al. (2017); however, we do not use the decoder part of the architecture. A schematic\ndiagram of the generic part of our model, common across all considered tasks, is shown in Figure 1.\nWe refer the reader to the original work for a detailed description of the transformer model, and here\npresent the proposed changes that make it compatible with multivariate time series data, instead of\nsequences of discrete word indices.\nIn particular, each training sample X ∈Rw×m, which is a multivariate time series of length w\nand m different variables, constitutes a sequence of w feature vectors xt ∈Rm: X ∈Rw×m =\n3\nFigure 1: Left: Generic model architecture, common to all tasks. The feature vector xt at each time\nstep t is linearly projected to a vector ut of the same dimensionality d as the internal representation\nvectors of the model and is fed to the ﬁrst self-attention layer to form the keys, queries and values\nafter adding a positional encoding. Right: Training setup of the unsupervised pre-training task.\nWe mask a proportion r of each variable sequence in the input independently, such that across each\nvariable, time segments of mean length lm are masked, each followed by an unmasked segment\nof mean length lu = 1−r\nr lm. Using a linear layer on top of the ﬁnal vector representations zt, at\neach time step the model tries to predict the full, uncorrupted input vectors xt; however, only the\npredictions on the masked values are considered in the Mean Squared Error loss.\n[x1, x2, . . . , xw]. The original feature vectors xt are ﬁrst normalized (for each dimension, we sub-\ntract the mean and divide by the variance across the training set samples) and then linearly projected\nonto a d-dimensional vector space, where d is the dimension of the transformer model sequence\nelement representations (typically called model dimension):\nut = Wpxt + bp\n(1)\nwhere Wp ∈Rd×m, bp ∈Rd are learnable parameters and ut ∈Rd, t = 0, . . . , w are the model\ninput vectors1, which correspond to the word vectors of the NLP transformer. These will become\nthe queries, keys and values of the self-attention layer, after adding the positional encodings and\nmultiplying by the corresponding matrices.\nWe note that the above formulation also covers the univariate time series case, i.e., m = 1, although\nwe only evaluate our approach on multivariate time series in the scope of this work. We additionally\nnote that the input vectors ut need not necessarily be obtained from the (transformed) feature vectors\nat a time step t: because the computational complexity of the model scales as O(w2) and the number\nof parameters2 as O(w) with the input sequence length w, to obtain ut in case the granularity\n(temporal resolution) of the data is very ﬁne, one may instead use a 1D-convolutional layer with 1\ninput and d output channels and kernels Ki of size (k, m), where k is the width in number of time\nsteps and i the output channel:\nut\ni = u(t, i) =\nX\nj\nX\nh\nx(t + j, h)Ki(j, h),\ni = 1, . . . , d\n(2)\n1Although equation 1 shows the operation for a single time step for clarity, all input vectors are embedded\nconcurrently by a single matrix-matrix multiplication\n2Speciﬁcally, the learnable positional encoding, batch normalization and output layers\n4\nIn this way, one may control the temporal resolution by using a stride or dilation factor greater than\n1. Moreover, although in the present work we only used equation 1, one may use equation 2 as an\ninput to compute the keys and queries and equation 1 to compute the values of the self-attention\nlayer. This is particularly useful in the case of univariate time series, where self-attention would\notherwise match (consider relevant/compatible) all time steps which share similar values for the\nindependent variable, as noted by Li et al. (2019).\nFinally, since the transformer is a feed-forward architecture that is insensitive to the ordering of in-\nput, in order to make it aware of the sequential nature of the time series, we add positional encodings\nWpos ∈Rw×d to the input vectors U ∈Rw×d = [u1, . . . , uw]: U ′ = U + Wpos.\nInstead of deterministic, sinusoidal encodings, which were originally proposed by Vaswani et al.\n(2017), we use fully learnable positional encodings, as we observed that they perform better for all\ndatasets presented in this work. Based on the performance of our models, we also observe that the\npositional encodings generally appear not to signiﬁcantly interfere with the numerical information\nof the time series, similar to the case of word embeddings; we hypothesize that this is because they\nare learned so as to occupy a different, approximately orthogonal, subspace to the one in which the\nprojected time series samples reside. This approximate orthogonality condition is much easier to\nsatisfy in high dimensional spaces.\nAn important consideration regarding time series data is that individual samples may display con-\nsiderable variation in length. This issue is effectively dealt with in our framework: after setting a\nmaximum sequence length w for the entire dataset, shorter samples are padded with arbitrary values,\nand we generate a padding mask which adds a large negative value to the attention scores for the\npadded positions, before computing the self-attention distribution with the softmax function. This\nforces the model to completely ignore padded positions, while allowing the parallel processing of\nsamples in large minibatches.\nTransformers in NLP use layer normalization after computing self-attention and after the feed-\nforward part of each encoder block, leading to signiﬁcant performance gains over batch normaliza-\ntion, as originally proposed by Vaswani et al. (2017). However, here we instead use batch normaliza-\ntion, because it can mitigate the effect of outlier values in time series, an issue that does not arise in\nNLP word embeddings. Additionally, the inferior performance of batch normalization in NLP has\nbeen mainly attributed to extreme variation in sample length (i.e., sentences in most tasks) (Shen\net al., 2020), while in the datasets we examine this variation is much smaller. In Table 11 of the\nAppendix we show that batch normalization can indeed offer a very signiﬁcant performance beneﬁt\nover layer normalization, while the extent can vary depending on dataset characteristics.\n3.2\nREGRESSION AND CLASSIFICATION\nThe base model architecture presented in Section 3.1 and depicted in Figure 1 can be used for the\npurposes of regression and classiﬁcation with the following modiﬁcation: the ﬁnal representation\nvectors zt ∈Rd corresponding to all time steps are concatenated into a single vector ¯z ∈Rd·w =\n[z1; . . . ; zw], which serves as the input to a linear output layer with parameters Wo ∈Rn×(d·w),\nbo ∈Rn, where n is the number of scalars to be estimated for the regression problem (typically\nn = 1), or the number of classes for the classiﬁcation problem:\nˆy = Wo¯z + bo\n(3)\nIn the case of regression, the loss for a single data sample will simply be the squared error L =\n∥ˆy −y∥2, where y ∈Rn are the ground truth values. We clarify that regression in the context of\nthis work means predicting a numeric value for a given sequence (time series sample). This numeric\nvalue is of a different nature than the numerical data appearing in the time series: for example,\ngiven a sequence of simultaneous temperature and humidity measurements of 9 rooms in a house,\nas well as weather and climate data such as temperature, pressure, humidity, wind speed, visibility\nand dewpoint, we wish to predict the total energy consumption in kWh of a house for that day. The\nparameter n corresponds to the number of scalars (or the dimensionality of a vector) to be estimated.\n5\nIn the case of classiﬁcation, the predictions ˆy will additionally be passed through a softmax function\nto obtain a distribution over classes, and its cross-entropy with the categorical ground truth labels\nwill be the sample loss.\nFinally, when ﬁne-tuning the pre-trained models, we allow training of all weights; instead, freezing\nall layers except for the output layer would be equivalent to using static, pre-extracted time-series\nrepresentations of the time series. In Table 12 in the Appendix we show the trade-off in terms of\nspeed and performance when using a fully trainable model versus static representations.\n3.3\nUNSUPERVISED PRE-TRAINING\nAs a task for the unsupervised pre-training of our model we consider the autoregressive task of de-\nnoising the input: speciﬁcally, we set part of the input to 0 and ask the model to predict the masked\nvalues. The corresponding setup is depicted in the right part of Figure 1. A binary noise mask\nM ∈Rw×m, is created independently for each training sample, and the input is masked by elemen-\ntwise multiplication: ˜X = M ⊙X. On average, a proportion r of each mask column of length w\n(corresponding to a single variable in the multivariate time series) is set to 0 by alternating between\nsegments of 0s and 1s. We choose the state transition probabilities such that each masked segment\n(sequence of 0s) has a length that follows a geometric distribution with mean lm and is succeeded by\nan unmasked segment (sequence of 1s) of mean length lu = 1−r\nr lm. We chose lm = 3 for all pre-\nsented experiments. The reason why we wish to control the length of the masked sequence, instead\nof simply using a Bernoulli distribution with parameter r to set all mask elements independently at\nrandom, is that very short masked sequences (e.g., of 1 masked element) in the input can often be\ntrivially predicted with good approximation by replicating the immediately preceding or succeeding\nvalues or by the average thereof. In order to obtain enough long masked sequences with relatively\nhigh likelihood, a very high masking proportion r would be required, which would render the over-\nall task detrimentally challenging. Following the process above, at each time step on average r · m\nvariables will be masked. We empirically chose r = 0.15 for all presented experiments. This input\nmasking process is different from the “cloze type” masking used by NLP models such as BERT,\nwhere a special token and thus word embedding vector replaces the original word embedding, i.e.,\nthe entire feature vector at affected time steps. We chose this masking pattern because it encour-\nages the model to learn to attend both to preceding and succeeding segments in individual variables,\nas well as to existing contemporary values of the other variables in the time series, and thereby to\nlearn to model inter-dependencies between variables. In Table 10 in the Appendix we show that this\nmasking scheme is more effective than other possibilities for denoising the input.\nUsing a linear layer with parameters Wo ∈Rm×d, bo ∈Rm on top of the ﬁnal vector repre-\nsentations zt ∈Rd, for each time step the model concurrently outputs its estimate ˆxt of the full,\nuncorrupted input vectors xt; however, only the predictions on the masked values (with indices in\nthe set M ≡{(t, i) : mt,i = 0}, where mt,i are the elements of the mask M), are considered in the\nMean Squared Error loss for each data sample:\nˆxt = Wozt + bo\n(4)\nLMSE =\n1\n|M|\nX X\n(t,i)∈M\n(ˆx(t, i) −x(t, i))2\n(5)\nThis objective differs from the one used by denoising autoencoders, where the loss considers re-\nconstruction of the entire input, under (typically Gaussian) noise corruption. Also, we note that the\napproach described above differs from simple dropout on the input embeddings, both with respect to\nthe statistical distributions of masked values, as well as the fact that here the masks also determine\nthe loss function. In fact, we additionally use a dropout of 10% when training all of our supervised\nand unsupervised models.\n4\nEXPERIMENTS & RESULTS\nIn the experiments reported below we use the predeﬁned training - test set splits of the benchmark\ndatasets and train all models long enough to ensure convergence. We do this to account for the fact\n6\nthat training the transformer models in a fully supervised way typically requires more epochs than\nﬁne-tuning the ones which have already been pre-trained using the unsupervised methodology of\nSection 3.3. Because the benchmark datasets are very heterogeneous in terms of number of samples,\ndimensionality and length of the time series, as well as the nature of the data itself, we observed that\nwe can obtain better performance by a cursory tuning of hyperparameters (such as the number of\nencoder blocks, the representation dimension, number of attention heads or dimension of the feed-\nforward part of the encoder blocks) separately for each dataset. To select hyperparameters, for each\ndataset we randomly split the training set in two parts, 80%-20%, and used the 20% as a validation\nset for hyperparameter tuning. After ﬁxing the hyperparameters, the entire training set was used to\ntrain the model again, which was ﬁnally evaluated on the test set. A set of hyperparameters which\nhas consistently good performance on all datasets is shown in Table 14 in the Appendix, alongside\nthe hyperparameters that we have found to yield the best performance for each dataset (Tables 15,\n16, 17, 18.\n4.1\nREGRESSION\nWe select a diverse range of 6 datasets from the Monash University, UEA, UCR Time Series Regres-\nsion Archive Tan et al. (2020a) in a way so as to ensure diversity with respect to the dimensionality\nand length of time series samples, as well as the number of samples (see Appendix Table 3 for\ndataset characteristics). Table 1 shows the Root Mean Squared Error achieved by of our models,\nnamed TST for “Time Series Transformer”, including a variant trained only through supervision,\nand one ﬁrst pre-trained on the same training set in an unsupervised way. We compare them with\nthe currently best performing models as reported in the archive. Our transformer models rank ﬁrst\non all but two of the examined datasets, for which they rank second. They thus achieve an average\nrank of 1.33, setting them clearly apart from all other models; the overall second best model, XG-\nBoost, has an average rank of 3.5, ROCKET (which outperformed ours on one dataset) on average\nranks in 5.67th place and Inception (which outperformed ours on the second dataset) also has an\naverage rank of 5.67. On average, our models attain 30% lower RMSE than the mean RMSE among\nall models, and approx. 16% lower RMSE than the overall second best model (XGBoost), with ab-\nsolute improvements varying among datasets from approx. 4% to 36%. We note that all other deep\nlearning methods achieve performance close to the middle of the ranking or lower. In Table 1 we\nreport the ”average relative difference from mean” metric rj for each model j, the over N datasets:\nrj = 1\nN\nN\nX\ni=1\nR(i, j) −¯Ri\n¯Ri\n,\n¯Ri = 1\nM\nM\nX\nk=1\nR(i, k)\n, where R(i, j) is the RMSE of model j on dataset i and M is the number of models.\nImportantly, we also observe that the pre-trained transformer models outperform the fully super-\nvised ones in 3 out of 6 datasets. This is interesting, because no additional samples are used for\npre-training: the beneﬁt appears to originate from reusing the same training samples for learning\nthrough an unsupervised objective. To further elucidate this observation, we investigate the follow-\ning questions:\nQ1: Given a partially labeled dataset of a certain size, how will additional labels affect perfor-\nmance? This pertains to one of the most important decisions that data owners face, namely, to what\nextent will further annotation help. To clearly demonstrate this effect, we choose the largest dataset\nwe have considered from the regression archive (12.5k samples), in order to avoid the variance in-\ntroduced by small set sizes. The left panel of Figure 2 (where each marker is an experiment) shows\nhow performance on the entire test set varies with an increasing proportion of labeled training set\ndata used for supervised learning. As expected, with an increasing proportion of available labels\nperformance improves both for a fully supervised model, as well as the same model that has been\nﬁrst pre-trained on the entire training set through the unsupervised objective and then ﬁne-tuned. In-\nterestingly, not only does the pre-trained model outperform the fully supervised one, but the beneﬁt\npersists throughout the entire range of label availability, even when the models are allowed to use all\nlabels; this is consistent with our previous observation on Table 1 regarding the advantage of reusing\nsamples.\nQ2: Given a labeled dataset, how will additional unlabeled samples affect performance? In\nother words, to what extent does unsupervised learning make it worth collecting more data, even\n7\nRoot MSE\nOurs\nDataset\nSVR\nRandom\n˜Forest\nXGBoost\n1-NN-ED\n5-NN\n-ED\n1-NN-\nDTWD\n5-NN-\nDTWD\nRocket\nFCN\nResNet\nInception\nTST\n(sup. only)\nTST\n(pretrained)\nAppliancesEnergy\n3.457\n3.455\n3.489\n5.231\n4.227\n6.036\n4.019\n2.299\n2.865\n3.065\n4.435\n2.228\n2.375\nBenzeneConcentr.\n4.790\n0.855\n0.637\n6.535\n5.844\n4.983\n4.868\n3.360\n4.988\n4.061\n1.584\n0.517\n0.494\nBeijingPM10\n110.574\n94.072\n93.138\n139.229\n115.669\n139.134\n115.502\n120.057\n94.348\n95.489\n96.749\n91.344\n86.866\nBeijingPM25\n75.734\n63.301\n59.495\n88.193\n74.156\n88.256\n72.717\n62.769\n59.726\n64.462\n62.227\n60.357\n53.492\nLiveFuelMoisture\n43.021\n44.657\n44.295\n58.238\n46.331\n57.111\n46.290\n41.829\n47.877\n51.632\n51.539\n42.607\n43.138\nIEEEPPG\n36.301\n32.109\n31.487\n33.208\n27.111\n37.140\n33.572\n36.515\n34.325\n33.150\n23.903\n25.042\n27.806\nAvg Rel. diff. from mean\n0.097\n-0.172\n-0.197\n0.377\n0.152\n0.353\n0.124\n-0.048\n0.021\n0.005\n-0.108\n-0.301\n-0.303\nAvg Rank\n7.166\n4.5\n3.5\n10.833\n8\n11.167\n7.667\n5.667\n6.167\n6.333\n5.666\n1.333\nTable 1: Performance on multivariate regression datasets, in terms of Root Mean Squared Error.\nBold indicates best values, underlining indicates second best.\nif no additional annotations are available? This question differs from the above, as we now only\nscale the availability of data samples for unsupervised pre-training, while the number of labeled\nsamples is ﬁxed. The right panel of Figure 2 (where each marker is an experiment) shows that,\nfor a given number of labels (shown as a percentage of the totally available labels), the more data\nsamples are used for unsupervised learning, the lower the error achieved (note that the horizontal\naxis value 0 corresponds to fully supervised training only, while all other values to unsupervised\npre-training followed by supervised ﬁne-tuning). This trend is more linear in the case of supervised\nlearning on 20% of the labels (approx. 2500). Likely due to a small sample (here, meaning set)\neffect, in the case of having only 10% of the labels (approx. 1250) for supervised learning, the error\nﬁrst decreases rapidly as we use more samples for unsupervised pre-training, and then momentarily\nincreases, before it decreases again (for clarity, the same graphs are shown separately in Figure 3\nin the Appendix). Consistent with our observations above, it is interesting to again note that, for a\ngiven number of labeled samples, even reusing a subset of the same samples for unsupervised pre-\ntraining improves performance: for the 1250 labels (blue diamonds of the right panel of Figure 2 or\nleft panel of Figure 3 in the Appendix) this can be observed in the horizontal axis range [0, 0.1], and\nfor the 2500 labels (blue diamonds of the right panel of Figure 2 or right panel of Figure 3 in the\nAppendix) in the horizontal axis range [0, 0.2].\n50\n52\n54\n56\n58\n60\n62\n64\n66\n68\n70\n0\n0.2\n0.4\n0.6\n0.8\n1\nAvailability of labels\nUnsup. pretrained\nSupervised\n58\n60\n62\n64\n66\n68\n70\n0\n0.2\n0.4\n0.6\n0.8\n1\nUse of data for unsupervised pre-training\n0.1 of labels\n0.2 of labels\nRMSE\n58\n60\n62\n64\n66\n68\n70\n0\n0.2\n0.4\n0.6\n0.8\n1\nUse of data for unsupervised pre-training\n0.1 of labels\n0.2 of labels\nRMSE\nFigure 2: Dataset: BeijingPM25Quality. Left: Root Mean Squared Error of a fully supervised\ntransformer (orange circles) and the same model pre-trained (blue diamonds) on the training set\nthrough the unsupervised objective and then ﬁne-tuned on available labels, versus the proportion of\nlabeled data in the training set. Right: Root Mean Squared Error of a given model as a function of\nthe number of samples (here, shown as a proportion of the total number of samples in the training\nset) used for unsupervised pre-training. For supervised learning, two levels of label availability\nare depicted: 10% (purple circles) and 20% (green squares) of all training data labels. Note that a\nhorizontal axis value of 0 means fully supervised learning only, while all other values correspond to\nunsupervised pre-training followed by supervised ﬁne-tuning.\n8\n4.2\nCLASSIFICATION\nWe select a set of 11 multivariate datasets from the UEA Time Series Classiﬁcation Archive (Bag-\nnall et al., 2018) with diverse characteristics in terms of the number, dimensionality and length of\ntime series samples, as well as the number of classes (see Appendix Table 4). As this archive is new,\nthere have not been many reported model evaluations; we follow Franceschi et al. (2019) and use\nas a baseline the best performing method studied by the creators of the archive, DTWD (dimension-\nDependent DTW), together with the method proposed by Franceschi et al. (2019) themselves (a\ndilation-CNN leveraging unsupervised and supervised learning). Additionally, we use the publicly\navailable implementations Tan et al. (2020b) of ROCKET, which is currently the top performing\nmodel for univariate time series and one of the best in our regression evaluation, and XGBoost,\nwhich is one of the most commonly used models for univariate and multivariate time series, and\nalso the best baseline model in our regression evaluation (Section 4.1). Finally, we did not ﬁnd any\nreported evaluations of RNN-based models on any of the UCR/UEA archives, possibly because of\na common perception for long training and inference times, as well as difﬁculty in training (Fawaz\net al., 2019b); therefore, we implemented a stacked LSTM model and also include it in the compar-\nison. The performance of the baselines alongside our own models are shown in Table 2 in terms of\naccuracy, to allow comparison with reported values.\nOurs\nDataset\nTST (pretrained)\nTST (sup. only)\nRocket\nXGBoost\nLSTM\nFrans. et al\nDTW D\nEthanolConcentration\n0.326\n0.337\n0.452\n0.437\n0.323\n0.289\n0.323\nFaceDetection\n0.689\n0.681\n0.647\n0.633\n0.577\n0.528\n0.529\nHandwriting\n0.359\n0.305\n0.588\n0.158\n0.152\n0.533\n0.286\nHeartbeat\n0.776\n0.776\n0.756\n0.732\n0.722\n0.756\n0.717\nJapaneseVowels\n0.997\n0.994\n0.962\n0.865\n0.797\n0.989\n0.949\nInsectWingBeat\n0.687\n0.684\n-\n0.369\n0.176\n0.16\n-\nPEMS-SF\n0.896\n0.919\n0.751\n0.983\n0.399\n0.688\n0.711\nSelfRegulationSCP1\n0.922\n0.925\n0.908\n0.846\n0.689\n0.846\n0.775\nSelfRegulationSCP2\n0.604\n0.589\n0.533\n0.489\n0.466\n0.556\n0.539\nSpokenArabicDigits\n0.998\n0.993\n0.712\n0.696\n0.319\n0.956\n0.963\nUWaveGestureLibrary\n0.913\n0.903\n0.944\n0.759\n0.412\n0.884\n0.903\nAvg Accuracy\n(excl. InsectWingBeat)\n0.748\n0.742\n0.725\n0.659\n0.486\n0.703\n0.669\nAvg Rank\n1.7\n2.3\n3.8\n5.4\n3.7\n4.1\nTable 2: Accuracy on multivariate classiﬁcation datasets. Bold indicates best and underlining\nsecond best values. A dash indicates that the corresponding method failed to run on this dataset.\nIt can be seen that our models performed best on 7 out of the 11 datasets, achieving an average rank\nof 1.7, followed by ROCKET, which performed best on 3 datasets and on average ranked 2.3th.\nThe dilation-CNN (Franceschi et al., 2019) and XGBoost, which performed best on the remaining\n1 dataset, tied and on average ranked 3.7th and 3.8th respectively. Interestingly, we observe that\nall datasets on which ROCKET outperformed our model were very low dimensional (speciﬁcally,\n3-dimensional). Although our models still achieved the second best performance for UWaveG-\nestureLibrary, in general we believe that this indicates a relative weakness of our current models\nwhen dealing with very low dimensional time series. As discussed in Section 3.1, this may be\ndue to the problems introduced by a low-dimensional representation space to the attention mecha-\nnism, as well as the added positional embeddings; to mitigate this issue, in future work we intend\nto use a 1D-convolutional layer to extract more meaningful representations of low-dimensional in-\nput features (see Section 3.1). Conversely, our models performed particularly well on very high-\ndimensional datasets (FaceDetection, HeartBeat, InsectWingBeat, PEMS-SF), and/or datasets with\nrelatively more training samples. As a characteristic example, on InsectWingBeat (which is by far\nthe largest dataset with 30k samples and contains time series of 200 dimensions and highly irregular\nlength) our model reached an accuracy of 0.689, while all other methods performed very poorly - the\nsecond best was XGBoost with an accuracy of 0.369. However, we note that our model performed\nexceptionally well also on datasets with only a couple of hundred samples, which in fact constitute\n8 out of the 11 examined datasets.\nFinally, we observe that the pre-trained transformer models performed better than the fully super-\nvised ones in 8 out of 11 datasets, sometimes by a substantial margin.Again, no additional samples\n9\nwere available for unsupervised pre-training, so the beneﬁt appears to originate from reusing the\nsame samples.\n5\nADDITIONAL POINTS & FUTURE WORK\nExecution time for training: While a precise comparison in terms of training time is well out of\nscope for the present work, in Section A.3 of the Appendix we demonstrate that our transformer-\nbased method is economical in terms of its use of computational resources. Alternative self-attention\nschemes, such as sparse attention patterns (Li et al., 2019), recurrence (Dai et al., 2019) or com-\npressed (global-local) attention (Beltagy et al., 2020), can help drastically reduce the O(w2) com-\nplexity of the self-attention layers with respect to the time series length w, which is the main perfor-\nmance bottleneck.\nImputation and forecasting: The model and training process described in Section 3.3 is exactly the\nsetup required to perform imputation of missing values, without any modiﬁcations, and we observed\nthat it was possible to achieve very good results following this method; as a rough indication, our\nmodels could reach Root Mean Square Errors very close to 0 when asked to perform the input\ndenoising (autoregressive) task on the test set, after being subjected to unsupervised pre-training on\nthe training set. We also show example results of imputation on one of the datasets presented in this\nwork in Figure 5. However, we defer a systematic quantitative comparison with the state of the art to\nfuture work. Furthermore, we note that one may simply use different patterns of masking to achieve\ndifferent objectives, while the rest of the model and setup remain the same. For example, using a\nmask which conceals the last part of all variables simultaneously, one may perform forecasting (see\nFigure 4 in Appendix), while for longer time series one may additionally perform this process within\na sliding window. Again, we defer a systematic investigation to future work.\nExtracted representations: The representations zt extracted by the transformer models can be\nused directly for evaluating similarity between time series, clustering, visualization and any other\nuse cases where time series representations are used in practice. A valuable beneﬁt offered by\ntransformers is that representations can be independently addressed for each time step; this means\nthat, for example, a greater weight can be placed at the beginning, middle or end of the time series,\nwhich allows to selectively compare time series, visualize temporal evolution of samples etc.\n6\nCONCLUSION\nIn this work we propose a novel framework for multivariate time series representation learning\nbased on the transformer encoder architecture. The framework includes an unsupervised pre-training\nscheme, which we show that can offer substantial performance beneﬁts over fully supervised learn-\ning, even without leveraging additional unlabeled data, i.e., by reusing the same data samples. By\nevaluating our framework on several public multivariate time series datasets from various domains\nand with diverse characteristics, we demonstrate that it is currently the best performing method for\nregression and classiﬁcation, even for datasets where only a few hundred training samples are avail-\nable, and the only top performing method based on deep learning. It is also the ﬁrst unsupervised\nmethod shown to push the state-of-the-art performance for multivariate time series regression and\nclassiﬁcation.\nREFERENCES\nA. Bagnall, J. Lines, A. Bostrom, J. Large, and E. Keogh.\nThe great time series classiﬁcation\nbake off: a review and experimental evaluation of recent algorithmic advances. Data Mining and\nKnowledge Discovery, 31:606–660, 2017.\nAnthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul\nSoutham, and Eamonn Keogh. The UEA multivariate time series classiﬁcation archive, 2018.\narXiv:1811.00075 [cs, stat], October 2018. arXiv: 1811.00075.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-Document Transformer.\narXiv:2004.05150 [cs], April 2020. URL http://arxiv.org/abs/2004.05150. arXiv:\n2004.05150.\n10\nFilippo Maria Bianchi, Lorenzo Livi, Karl Øyvind Mikalsen, Michael Kampffmeyer, and Robert\nJenssen.\nLearning representations of multivariate time series with missing data.\nPattern\nRecognition, 96:106973, December 2019. ISSN 0031-3203. doi: 10.1016/j.patcog.2019.106973.\nT. Brown, B. Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, P. Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, G. Kr¨uger,\nTom Henighan, R. Child, Aditya Ramesh, D. Ziegler, Jeffrey Wu, Clemens Winter, Christopher\nHesse, Mark Chen, E. Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, J. Clark, Christopher\nBerner, Sam McCandlish, A. Radford, Ilya Sutskever, and Dario Amodei. Language models are\nfew-shot learners. ArXiv, abs/2005.14165, 2020.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive Language Models Beyond a Fixed-Length Context. arXiv:1901.02860\n[cs, stat], June 2019. URL http://arxiv.org/abs/1901.02860. arXiv: 1901.02860.\nEdward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. GRU-ODE-Bayes: Continuous\nModeling of Sporadically-Observed Time Series. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d\\textquotesingle Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information\nProcessing Systems 32, pp. 7379–7390. Curran Associates, Inc., 2019.\nAngus Dempster, Franccois Petitjean, and Geoffrey I. Webb. ROCKET: exceptionally fast and accu-\nrate time series classiﬁcation using random convolutional kernels. Data Mining and Knowledge\nDiscovery, 2020. doi: 10.1007/s10618-020-00701-z.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. CoRR, abs/1810.04805, 2018. eprint:\n1810.04805.\nH. Fawaz, B. Lucas, G. Forestier, Charlotte Pelletier, D. Schmidt, Jonathan Weber, Geoffrey I. Webb,\nL. Idoumghar, Pierre-Alain Muller, and Franccois Petitjean. InceptionTime: Finding AlexNet for\nTime Series Classiﬁcation. ArXiv, 2019a. doi: 10.1007/s10618-020-00710-y.\nHassan Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller.\nDeep learning for time series classiﬁcation: a review. Data Mining and Knowledge Discovery, 33\n(4):917–963, July 2019b. ISSN 1573-756X. doi: 10.1007/s10618-019-00619-1.\nVincent Fortuin, M. H¨user, Francesco Locatello, Heiko Strathmann, and G. R¨atsch. SOM-VAE:\nInterpretable Discrete Representation Learning on Time Series. ICLR, 2019.\nJean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised Scalable Represen-\ntation Learning for Multivariate Time Series. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d\\textquotesingle Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information\nProcessing Systems 32, pp. 4650–4661. Curran Associates, Inc., 2019.\nSepp Hochreiter. The Vanishing Gradient Problem During Learning Recurrent Neural Nets and\nProblem Solutions. Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 6(2):107–116, April 1998.\nISSN 0218-4885. doi: 10.1142/S0218488598000094. Place: River Edge, NJ, USA Publisher:\nWorld Scientiﬁc Publishing Co., Inc.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam\nShazeer, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas Eck. Music\ntransformer: Generating music with long-term structure. In International Conference on Learning\nRepresentations, 2018.\nA. Jansen, M. Plakal, Ratheet Pandya, D. Ellis, Shawn Hershey, Jiayang Liu, R. C. Moore, and R. A.\nSaurous. Unsupervised Learning of Semantic Audio Representations. 2018 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2018. doi: 10.1109/ICASSP.\n2018.8461684.\nA. Kopf, Vincent Fortuin, Vignesh Ram Somnath, and M. Claassen. Mixture-of-Experts Variational\nAutoencoder for clustering and generating from similarity-based representations. ICLR 2019,\n2019.\n11\nQi Lei, Jinfeng Yi, R. Vacul´ın, Lingfei Wu, and I. Dhillon. Similarity Preserving Representation\nLearning for Time Series Analysis. ArXiv, 2017.\nShiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng\nYan. Enhancing the locality and breaking the memory bottleneck of transformer on time series\nforecasting. In Advances in Neural Information Processing Systems, pp. 5243–5253, 2019.\nBryan Lim, Sercan O. Arik, Nicolas Loeff, and Tomas Pﬁster. Temporal fusion transformers for\ninterpretable multi-horizon time series forecasting, 2020.\nJ. Lines, Sarah Taylor, and Anthony J. Bagnall. Time Series Classiﬁcation with HIVE-COTE. ACM\nTrans. Knowl. Discov. Data, 2018. doi: 10.1145/3182382.\nBenjamin Lucas, Ahmed Shifaz, Charlotte Pelletier, Lachlan O’Neill, Nayyar Zaidi, Bart Goethals,\nFrancois Petitjean, and Geoffrey I. Webb. Proximity Forest: An effective and scalable distance-\nbased classiﬁer for time series. Data Mining and Knowledge Discovery, 33(3):607–635, May\n2019. ISSN 1384-5810, 1573-756X. doi: 10.1007/s10618-019-00617-3. arXiv: 1808.10594.\nXinrui Lyu, Matthias Hueser, Stephanie L. Hyland, George Zerveas, and Gunnar Raetsch. Improving\nClinical Predictions through Unsupervised Time Series Representation Learning. In Proceedings\nof the NeurIPS 2018 Workshop on Machine Learning for Health, 2018. eprint: 1812.00490.\nJ. Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, A. Vetro, and S. Chang.\nCdsa:\nCross-dimensional self-attention for multivariate, geo-tagged time series imputation.\nArXiv,\nabs/1905.09904, 2019.\nP. Malhotra, T. Vishnu, L. Vig, Puneet Agarwal, and G. Shroff. TimeNet: Pre-trained deep recurrent\nneural network for time series classiﬁcation. ESANN, 2017.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\nnetworks. In Proceedings of the 30th International Conference on International Conference on\nMachine Learning - Volume 28, ICML’13, pp. III–1310–III–1318, Atlanta, GA, USA, June 2013.\nJMLR.org.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, W. Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uniﬁed Text-to-\nText Transformer. ArXiv, abs/1910.10683, 2019.\nSheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. PowerNorm:\nRethinking Batch Normalization in Transformers. arXiv:2003.07845 [cs], June 2020. arXiv:\n2003.07845.\nAhmed Shifaz, Charlotte Pelletier, F. Petitjean, and Geoffrey I. Webb. TS-CHIEF: a scalable and\naccurate forest algorithm for time series classiﬁcation. Data Mining and Knowledge Discovery,\n2020. doi: 10.1007/s10618-020-00679-8.\nC. Tan, C. Bergmeir, Franc¸ois Petitjean, and Geoffrey I. Webb. Monash University, UEA, UCR\nTime Series Regression Archive. ArXiv, 2020a.\nChang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I Webb. Time series regres-\nsion. arXiv preprint arXiv:2006.12672, 2020b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural\nInformation Processing Systems 30, pp. 5998–6008. Curran Associates, Inc., 2017.\nNeo Wu, Bradley Green, Xue Ben, and Shawn O’Banion. Deep transformer models for time series\nforecasting: The inﬂuenza prevalence case, 2020.\nChuxu Zhang, Dongjin Song, Yuncong Chen, Xinyang Feng, C. Lumezanu, Wei Cheng, Jingchao\nNi, B. Zong, H. Chen, and Nitesh V. Chawla. A Deep Neural Network for Unsupervised Anomaly\nDetection and Diagnosis in Multivariate Time Series Data. In AAAI, 2019. doi: 10.1609/aaai.\nv33i01.33011409.\n12\nA\nAPPENDIX\nA.1\nADDITIONAL POINTS & FUTURE WORK\nExecution time for training: While a precise comparison in terms of training time is well out of\nscope for the present work, in Section A.3 of the Appendix we demonstrate that our transformer-\nbased method is economical in terms of its use of computational resources. However, alternative\nself-attention schemes, such as sparse attention patterns (Li et al., 2019), recurrence (Dai et al.,\n2019) or compressed (global-local) attention (Beltagy et al., 2020), can help drastically reduce the\nO(w2) complexity of the self-attention layers with respect to the time series length w, which is the\nmain performance bottleneck.\nImputation and forecasting: The model and training process described in Section 3.3 is exactly the\nsetup required to perform imputation of missing values, without any modiﬁcations, and we observed\nthat it was possible to achieve very good results following this method; as a rough indication, our\nmodels could reach Root Mean Square Errors very close to 0 when asked to perform the input\ndenoising (autoregressive) task on the test set, after being subjected to unsupervised pre-training on\nthe training set. We also show example results of imputation on one of the datasets presented in this\nwork in Figure 5. However, we defer a systematic quantitative comparison with the state of the art to\nfuture work. Furthermore, we note that one may simply use different patterns of masking to achieve\ndifferent objectives, while the rest of the model and setup remain the same. For example, using a\nmask which conceals the last part of all variables simultaneously, one may perform forecasting (see\nFigure 4 in Appendix), while for longer time series one may additionally perform this process within\na sliding window. Again, we defer a systematic investigation to future work.\nExtracted representations: The representations zt extracted by the transformer models can be\nused directly for evaluating similarity between time series, clustering, visualization and any other\nuse cases where time series representations are used in practice. A valuable beneﬁt offered by\ntransformers is that representations can be independently addressed for each time step; this means\nthat, for example, a greater weight can be placed at the beginning, middle or end of the time series,\nwhich allows to selectively compare time series, visualize temporal evolution of samples etc.\nDataset\nTrain Size\nTest Size\nLength\nDimension\nMissing Values\nAppliancesEnergy\n96\n42\n144\n24\nNo\nBenzeneConcentration\n3433\n5445\n240\n8\nYes\nBeijingPM10Quality\n12432\n5100\n24\n9\nYes\nBeijingPM25Quality\n12432\n5100\n24\n9\nYes\nLiveFuelMoistureContent\n3493\n1510\n365\n7\nNo\nIEEEPPG\n1768\n1328\n1000\n5\nNo\nTable 3: Multivariate Regression Datasets\nDataset\nTrainSize\nTestSize\nNumDimensions\nSeriesLength\nNumClasses\nEthanolConcentration\n261\n263\n3\n1751\n4\nFaceDetection\n5890\n3524\n144\n62\n2\nHandwriting\n150\n850\n3\n152\n26\nHeartbeat\n204\n205\n61\n405\n2\nInsectWingbeat\n30000\n20000\n200\n30\n10\nJapaneseVowels\n270\n370\n12\n29\n9\nPEMS-SF\n267\n173\n963\n144\n7\nSelfRegulationSCP1\n268\n293\n6\n896\n2\nSelfRegulationSCP2\n200\n180\n7\n1152\n2\nSpokenArabicDigits\n6599\n2199\n13\n93\n10\nUWaveGestureLibrary\n120\n320\n3\n315\n8\nTable 4: Multivariate Classiﬁcation Datasets\n13\nDataset\nStandard deviation\nSupervised TST\nPre-trained TST\nAppliancesEnergy\n0.240\n0.163\nBenzeneConcentration\n0.031\n0.092\nBeijingPM10Quality\n0.689\n0.813\nBeijingPM25Quality\n0.189\n0.253\nLiveFuelMoistureContent\n0.735\n0.013\nIEEEPPG\n1.079\n1.607\nTable 5: Standard deviation of the Root Mean Square Error displayed by the Time Series Trans-\nformer models on multivariate regression datasets\nA.2\nCRITERIA FOR DATASET SELECTION\nWe select a diverge range of datasets from the Monash University, UEA, UCR Time Series Regres-\nsion and Classiﬁcation Archives, in a way so as to ensure diversity with respect to the dimensionality\nand length of time series samples, as well as the number of samples. Additionally, we have tried to\ninclude both ”easy” and ”difﬁcult” datasets (where the baselines perform very well or less well). In\nthe following we provide a more detailed rationale for each of the selected multivariate datasets.\nEthanolConcentration: very low dimensional, very few samples, moderate length, large number\nof classes, challenging\nFaceDetection: very high dimensional, many samples, very short length, minimum number of\nclasses\nHandwriting: very low dimensional, very few samples, moderate length, large number of classes\nHeartbeat: high dimensional, very few samples, moderate length, minimum number of classes\nJapaneseVowels: very heterogeneous sample length, moderate num. dimensions, very few samples,\nvery short length, moderate number of classes, all baselines perform well\nInsectWingBeat: very high dimensional, many samples, very short length, moderate number of\nclasses, very challenging\nPEMS-SF: extremely high dimensional, very few samples, moderate length, moderate number of\nclasses\nSelfRegulationSCP1: Few dimensions, very few samples, long length, minimum number of classes,\nbaselines perform well\nSelfRegulationSCP2: similar to SelfRegulationSCP1, but challenging\nSpokenArabicDigits: Moderate number of dimensions, many samples, very heterogeneous length,\nmoderate number of classes, most baselines perform well\nUWaveGestureLibrary: very low dimensional, very few samples, moderate length, moderate num-\nber of classes, baselines perform well\nA.3\nEXECUTION TIME\nWe recorded the times required for training our fully supervised models until convergence on a\nGPU, as well as for the currently fastest and top performing (in terms of classiﬁcation accuracy and\nregression error) baseline methods, ROCKET and XGBoost on a CPU. These have been shown to be\norders of magnitude faster than methods such as TS-CHIEF, Proximity Forest, Elastic Ensembles,\nDTW and HIVE-COTE, but also deep learning based methods Dempster et al. (2020). Although\nXGBoost and ROCKET are incomparably faster than the transformer on a CPU, as can be seen in\nTable 7 in the Appendix, exploiting commercial GPUs and the parallel processing capabilities of a\ntransformer typically enables as fast (and sometimes faster) training times as these (currently fastest\navailable) methods. In practice, despite allowing for many hundreds of epochs, using a GPU we\nnever trained our models longer than 3 hours on any of the examined datasets.\n14\nDataset\nStandard deviation\nSupervised TST\nPre-trained TST\nEthanolConcentration\n0.024\n0.002\nFaceDetection\n0.007\n0.006\nHandwriting\n0.020\n0.006\nHeartbeat\n0.018\n0.018\nInsectWingbeat\n0.003\n0.026\nJapaneseVowels\n0.000\n0.0016\nPEMS-SF\n0.017\n0.003\nSelfRegulationSCP1\n0.005\n0.006\nSelfRegulationSCP2\n0.020\n0.003\nSpokenArabicDigits\n0.0003\n0.001\nUWaveGestureLibrary\n0.005\n0.003\nTable 6: Standard deviation of accuracy displayed by the Time Series Transformer models on mul-\ntivariate classiﬁcation datasets\nDataset\nRocket\nXGBoost\nTST (GPU)\nEthanolConcentration\n41.937\n3.760\n34.72\nFaceDetection\n279.033\n57.832\n67.8\nHandwriting\n6.705\n1.836\n134.4\nHeartbeat\n35.825\n3.013\n2.57\nInsectWingBeat\n-\n64.883\n4565\nJapaneseVowels\n5.032\n0.527\n4.71\nPEMS-SF\n369.198\n150.879\n341\nSelfRegulationSCP1\n30.578\n0.967\n3.46\nSelfRegulationSCP2\n28.286\n1.213\n97.3\nSpokenArabicDigits\n65.143\n3.129\n73.2\nUWaveGestureLibrary\n3.078\n0.636\n2.90\nTable 7: Total training time (time until maximum accuracy is recorded) in seconds: for the fastest\ncurrently available methods (Rocket, XGBoost) on the same CPU, as well as for our fully supervised\ntransformer models on a GPU. On a CPU, training for our model is typically at least an order of\nmagnitude slower.\nAs regards deep learning models, LSTMs are well known to be slow, as they require O(w) sequential\noperations (where w is the length of the time series) for each sample, with the complexity per layer\nscaling as O(N · d2), where d is the internal representation dimension (hidden state size). We refer\nthe reader to the original transformer paper (Vaswani et al., 2017) for a detailed discussion about\nhow tranformers compare to Convolutional Neural Networks in terms of computational efﬁciency.\nDataset\nTST\nSVR\nRandom\nForest\nXGBoost\n1-NN\n-ED\n5-NN\n-ED\n1-NN-\nDTWD\n5-NN-\nDTWD\nRocket\nFCN\nResNet\nInception\nAppliancesEnergy\n1\n6\n5\n7\n11\n9\n12\n8\n2\n3\n4\n10\nBenzeneConcentration\n1\n7\n3\n2\n12\n11\n9\n8\n5\n10\n6\n4\nBeijingPM10Quality\n1\n7\n3\n2\n12\n9\n11\n8\n10\n4\n5\n6\nBeijingPM25Quality\n1\n10\n6\n2\n11\n9\n12\n8\n5\n3\n7\n4\nLiveFuelMoistureContent\n2\n3\n5\n4\n12\n7\n11\n6\n1\n8\n10\n9\nIEEEPPG\n2\n10\n5\n4\n7\n3\n12\n8\n11\n9\n6\n1\nTable 8: Ranks of all methods on regression datasets based on Root Mean Squared Error.\n15\nDataset\nTST\nRocket\nXGBoost\nLSTM\nFranseschi et al\nDTW D\nEthanolConcentration\n4\n1\n2\n3\n6\n5\nFaceDetection\n1\n2\n3\n4\n6\n5\nHandwriting\n3\n1\n5\n6\n2\n4\nHeartbeat\n1\n2\n4\n5\n3\n6\nJapaneseVowels\n1\n3\n5\n6\n2\n4\nPEMS-SF\n2\n3\n1\n6\n5\n4\nSelfRegulationSCP1\n1\n2\n3\n6\n4\n5\nSelfRegulationSCP2\n1\n4\n5\n6\n2\n3\nSpokenArabicDigits\n1\n4\n5\n6\n3\n2\nUWaveGestureLibrary\n2\n1\n5\n6\n4\n3\nTable 9: Ranks of all methods on classiﬁcation datasets based on Root Mean Squared Error.\nDataset\nTask (Metric)\nSep., Bern.\nSync., Bern.\nSep., Stateful\nSync., Stateful\nHeartbeat\nClassif. (Accuracy)\n0.761\n0.756\n0.776\n0.751\nInsectWingbeat\nClassif. (Accuracy)\n0.641\n0.632\n0.687\n0.689\nSpokenArabicDigits\nClassif. (Accuracy)\n0.994\n0.994\n0.998\n0.996\nPEMS-SF\nClassif. (Accuracy)\n0.873\n0.879\n0.896\n0.879\nBenzeneConcentration\nRegress. (RMSE)\n0.681\n0.493\n0.494\n0.684\nBeijingPM25Quality\nRegress. (RMSE)\n57.241\n59.529\n53.492\n59.632\nLiveFuelMoistureContent\nRegress. (RMSE)\n44.398\n43.519\n43.138\n43.420\nTable 10: Comparison of four different input value masking schemes evaluated for unsupervised\nlearning on 4 classiﬁcation and 3 regression datasets. Two of the variants involve separately gen-\nerating the mask for each variable, and two involve a single distribution over “time steps”, applied\nsynchronously to all variables. Also, two of the variants involve sampling each “time step” inde-\npendently based on a Bernoulli distribution with parameter p = r = 15%, while the remaining two\ninvolve using a Markov chain with two states, “masked” or “unmasked”, with different transition\nprobabilities pm =\n1\nlm and pu = pm\nr\n1−r, such that the masked sequences follow a geometric dis-\ntribution with a mean length of lm = 3 and each variable is masked on average by r = 15%. We\nobserve that our proposed scheme, separately masking each variable through stateful generation,\nperforms consistently well and shows the overall best performance across all examined datasets.\nDataset\nTask (Metric)\nLayerNorm\nBatchNorm\nHeartbeat\nClassif. (Accuracy)\n0.741\n0.776\nInsectWingbeat\nClassif. (Accuracy)\n0.658\n0.684\nSpokenArabicDigits\nClassif. (Accuracy)\n0.993\n0.993\nPEMS-SF\nClassif. (Accuracy)\n0.832\n0.919\nBenzeneConcentration\nRegress. (RMSE)\n2.053\n0.516\nBeijingPM25Quality\nRegress. (RMSE)\n61.082\n60.357\nLiveFuelMoistureContent\nRegress. (RMSE)\n42.993\n42.607\nTable 11: Performance comparison between using layer normalization and batch normalization in\nour supervised transformer model. The batch size is 128.\n16\n50\n52\n54\n56\n58\n60\n62\n64\n66\n68\n70\n0\n0.2\n0.4\n0.6\n0.8\n1\nRMSE\nUse of data for unsupervised pre-training\n50\n52\n54\n56\n58\n60\n62\n64\n66\n68\n0\n0.2\n0.4\n0.6\n0.8\n1\nRMSE\nUse of data for unsupervised pre-training\nFigure 3: Root Mean Squared Error of a given model as a function of the number of samples (here,\nshown as a proportion of the total number of samples in the training set) used for unsupervised pre-\ntraining. Two levels of label availability (used for supervised learning) are depicted: 10% (left panel)\nand 20% (right panel) of all training data labels. Note that a horizontal axis value of 0 means fully\nsupervised learning only, while all other values correspond to unsupervised pre-training followed by\nsupervised ﬁne-tuning.\nDataset\nTask (Metric)\nStatic\nFine-tuned\nMetric\nEpoch time (s)\nMetric\nEpoch time (s)\nHeartbeat\nClassif. (Accuracy)\n0.756\n0.082\n0.776\n0.14\nInsectWingbeat\nClassif. (Accuracy)\n0.236\n4.52\n0.687\n6.21\nSpokenArabicDigits\nClassif. (Accuracy)\n0.996\n1.29\n0.998\n2.00\nPEMS-SF\nClassif. (Accuracy)\n0.844\n0.208\n0.896\n0.281\nBenzeneConcentration\nRegress. (RMSE)\n4.684\n0.697\n0.494\n1.101\nBeijingPM25Quality\nRegress. (RMSE)\n65.608\n1.91\n53.492\n2.68\nLiveFuelMoistureContent\nRegress. (RMSE)\n48.724\n1.696\n43.138\n3.57\nTable 12: Performance comparison between allowing all layers of a pre-trained transformer to be\nﬁne-tuned, versus using static (“extracted”) representations of the time series as input to the output\nlayer (which is equivalent to freezing all model layers except for the output layer). The per-epoch\ntraining time on a GPU is also shown.\n. . .\nt1\nt2\nt3\ntw\nx1\nx2\nxm\n𝒙𝑤\n𝒙1\nforecasting\n. . .\nt1\nt2\nt3\ntw\nx1\nx2\nxm\n𝒙𝑤\n𝒙1\nsynchronous noise\nFigure 4: Masking schemes within our transformer encoder framework: for implementation of fore-\ncasting objective (left), for an alternative unsupervised learning objective involving a single noise\ndistribution over time steps, applied synchronously to all variables (right).\n17\n0\n50\n100\n150\n200\n250\ntime (hours)\n1\n0\n1\n2\nConcentration [mg/m3]\nTrue, input\nTrue, masked\nPrediction\n0\n50\n100\n150\n200\n250\n1\n0\n1\n2\n0\n50\n100\n150\n200\n250\n0\n2\n0\n50\n100\n150\n200\n250\n0\n2\n0\n50\n100\n150\n200\n250\n0\n2\n4\n0\n50\n100\n150\n200\n250\n0\n2\n0\n50\n100\n150\n200\n250\n1\n0\n1\n2\n0\n50\n100\n150\n200\n250\n1\n0\n1\n2\n0\n50\n100\n150\n200\n250\n2\n1\n0\n1\n0\n50\n100\n150\n200\n250\n0\n2\n4\n0\n50\n100\n150\n200\n250\n0\n2\n4\n0\n50\n100\n150\n200\n250\n0\n2\n0\n50\n100\n150\n200\n250\n2\n0\n2\n0\n50\n100\n150\n200\n250\n2\n0\n2\n4\n0\n50\n100\n150\n200\n250\n2\n0\n2\n0\n50\n100\n150\n200\n250\n2\n1\n0\n1\n2\n0\n50\n100\n150\n200\n250\n0\n2\n0\n50\n100\n150\n200\n250\n2\n0\n2\n0\n50\n100\n150\n200\n250\n2\n0\n2\n4\n0\n50\n100\n150\n200\n250\n2\n0\n2\n4\n0\n50\n100\n150\n200\n250\n2\n0\n2\nDimensions\nSamples\nFigure 5: Top: Imputation of missing values in the test set of BenzeneConcentration dataset. The\ncontinuous blue line is the ground truth signal, the light blue circles indicate the values hidden\nfrom the model and the orange dots its prediction. We observe that imputed values approximate\ntrue values very well, even in cases of rapid transitions and in cases where many contiguous values\nare missing. Bottom: Same, shown for 5 different dimensions of the time series (here, these are\nconcentrations of different substances) as columns and 4 different, randomly selected samples as\nrows.\n18\nA.4\nADVANTAGES OF TRANSFORMERS\nTransformer models are based on a multi-headed attention mechanism that offers several key advan-\ntages and renders them particularly suitable for time series data:\n• They can concurrently take into account long contexts of input sequence elements and\nlearn to represent each sequence element by selectively attending to those input sequence\nelements which the model considers most relevant. They do so without position-dependent\nprior bias; this is to be contrasted with RNN-based models: a) even bi-directional RNNs\ntreat elements in the middle of the input sequence differently from elements close to the two\nendpoints, and b) despite careful design, even LSTM (Long Short Term Memory) and GRU\n(Gated Recurrent Unit) networks practically only retain information from a limited number\nof time steps stored inside their hidden state (vanishing gradient problem (Hochreiter, 1998;\nPascanu et al., 2013)), and thus the context used for representing each sequence element is\ninevitably local.\n• Multiple attention heads can consider different representation subspaces, i.e., multiple as-\npects of relevance between input elements. For example, in the context of a signal with\ntwo frequency components, 1/T1 and 1/T2 , one attention head can attend to neighboring\ntime points, while another one may attend to points spaced a period T1 before the currently\nexamined time point, a third to a period T2 before, etc. This is to be contrasted with at-\ntention mechanisms in RNN models, which learn a single global aspect/mode of relevance\nbetween sequence elements.\n• After each stage of contextual representation (i.e., transformer encoder layer), attention is\nredistributed over the sequence elements, taking into account progressively more abstract\nrepresentations of the input elements as information ﬂows from the input towards the out-\nput. By contrast, RNN models with attention use a single distribution of attention weights\nto extract a representation of the input, and most typically attend over a single layer of\nrepresentation (hidden states).\nA.5\nHYPERPARAMETERS\nParameter\nValue\nactivation\ngelu\ndropout\n0.1\nlearning rate\n0.001\npos. encoding\nlearnable\nTable 13: Common (ﬁxed) hyperparameters used for all transformer models.\n19\nParameter\nValue\ndim. model\n128\ndim. feedforward\n256\nnum. heads\n16\nnum. encoder blocks\n3\nbatch size\n128\nTable 14: Hyperparameter conﬁguration that performs reasonably well for all transformer models.\nDataset\nnum. blocks\nnum. heads\ndim. model\ndim. feedforward\nAppliancesEnergy\n3\n8\n128\n512\nBenzeneConcentration\n3\n8\n128\n256\nBeijingPM10Quality\n3\n8\n64\n256\nBeijingPM25Quality\n3\n8\n64 (128)\n256\nLiveFuelMoistureContent\n3\n8\n64\n256\nIEEEPPG\n3\n8\n512\n512\nTable 15: Supervised TST model hyperparameters for the multivariate regression datasets\nDataset\nnum. blocks\nnum. heads\ndim. model\ndim. feedforward\nAppliancesEnergy\n3\n16\n128\n512\nBenzeneConcentration\n1\n8\n128\n256\nBeijingPM10Quality\n3\n8\n64\n256\nBeijingPM25Quality\n3\n8\n128\n256\nLiveFuelMoistureContent\n3\n8\n64\n256\nIEEEPPG\n4\n16\n512\n512\nTable 16: Unsupervised TST model hyperparameters for the multivariate regression datasets\nDataset\nnum. blocks\nnum. heads\ndim. model\ndim. feedforward\nEthanolConcentration\n1\n8\n64\n256\nFaceDetection\n3\n8\n128\n256\nHandwriting\n1\n8\n128\n256\nHeartbeat\n1\n8\n64\n256\nJapaneseVowels\n3\n8\n128\n256\nPEMS-SF\n1\n8\n128\n512\nSelfRegulationSCP1\n3\n8\n128\n256\nSelfRegulationSCP2\n3\n8\n128\n256\nSpokenArabicDigits\n3\n8\n64\n256\nUWaveGestureLibrary\n3\n16\n256\n256\nTable 17: Supervised TST model hyperparameters for the multivariate classiﬁcation datasets\nDataset\nnum. blocks\nnum. heads\ndim. model\ndim. feedforward\nEthanolConcentration\n1\n8\n64\n256\nFaceDetection\n3\n8\n128\n256\nHandwriting\n3\n16\n64\n256\nHeartbeat\n1\n8\n64\n256\nJapaneseVowels\n3\n8\n128\n256\nPEMS-SF\n1\n8\n256\n512\nSelfRegulationSCP1\n3\n16\n256\n512\nSelfRegulationSCP2\n3\n8\n256\n512\nSpokenArabicDigits\n3\n8\n64\n256\nUWaveGestureLibrary\n3\n16\n256\n512\nTable 18: Unsupervised TST model hyperparameters for the multivariate classiﬁcation datasets\n20\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-10-06",
  "updated": "2020-12-08"
}