{
  "id": "http://arxiv.org/abs/2010.04259v1",
  "title": "Unsupervised Joint $k$-node Graph Representations with Compositional Energy-Based Models",
  "authors": [
    "Leonardo Cotta",
    "Carlos H. C. Teixeira",
    "Ananthram Swami",
    "Bruno Ribeiro"
  ],
  "abstract": "Existing Graph Neural Network (GNN) methods that learn inductive unsupervised\ngraph representations focus on learning node and edge representations by\npredicting observed edges in the graph. Although such approaches have shown\nadvances in downstream node classification tasks, they are ineffective in\njointly representing larger $k$-node sets, $k{>}2$. We propose MHM-GNN, an\ninductive unsupervised graph representation approach that combines joint\n$k$-node representations with energy-based models (hypergraph Markov networks)\nand GNNs. To address the intractability of the loss that arises from this\ncombination, we endow our optimization with a loss upper bound using a\nfinite-sample unbiased Markov Chain Monte Carlo estimator. Our experiments show\nthat the unsupervised MHM-GNN representations of MHM-GNN produce better\nunsupervised representations than existing approaches from the literature.",
  "text": "Unsupervised Joint k-node Graph Representations\nwith Compositional Energy-Based Models\nLeonardo Cotta∗\nPurdue University\ncotta@purdue.edu\nCarlos H. C. Teixeira\nUniversidade Federal de Minas Gerais, Brazil\ncarlos@dcc.ufmg.br\nAnanthram Swami\nUnited States Army Research Laboratory\nananthram.swami.civ@mail.mil\nBruno Ribeiro\nPurdue University\nribeiro@cs.purdue.edu\nAbstract\nExisting Graph Neural Network (GNN) methods that learn inductive unsupervised\ngraph representations focus on learning node and edge representations by predicting\nobserved edges in the graph. Although such approaches have shown advances in\ndownstream node classiﬁcation tasks, they are ineffective in jointly representing\nlarger k-node sets, k>2. We propose MHM-GNN, an inductive unsupervised\ngraph representation approach that combines joint k-node representations with\nenergy-based models (hypergraph Markov networks) and GNNs. To address\nthe intractability of the loss that arises from this combination, we endow our\noptimization with a loss upper bound using a ﬁnite-sample unbiased Markov Chain\nMonte Carlo estimator. Our experiments show that the unsupervised joint k-node\nrepresentations of MHM-GNN produce better unsupervised representations than\nexisting approaches from the literature.\n1\nIntroduction\nInductive unsupervised learning using Graph Neural Networks (GNNs) in (dyadic) graphs is currently\nrestricted to node and edge representations due to their reliance on edge-based losses [8, 21, 29, 64]. If\nwe want to tackle downstream tasks that require jointly reasoning about k > 2 nodes, but whose input\ndata are dyadic relations (i.e., standard graphs) rather than hyperedges, we must develop techniques\nthat can go beyond edge-based losses.\nJoint k-node representation tasks with dyadic relational inputs include drone swarms that com-\nmunicate amongst themselves to jointly act on a task [56, 59], but also include more traditional\nproduct-recommendation tasks. For instance, an e-commerce website might want to predict which\nk products could be jointly purchased in the same shopping cart, while the database only records\n(product, product) dyads to safeguard user information.\nSrinivasan and Ribeiro [57] have recently shown that GNN node representations are insufﬁcient to\ncapture joint characteristics of k nodes that are unique to this group of nodes. Indeed, our experiments\nshow that using existing unsupervised GNN —with their node representations and edge losses— one\ncannot accurately detect these k-product carts on an e-commerce website. Unfortunately, existing\nGNN extensions that give joint k-node representations require supervised graph-wide losses [39, 36],\nleaving a signiﬁcant gap between edge and supervised whole-graph losses (i.e., we need multiple\nlabeled graphs for these to work). The main reason for this gap is scalability: to obtain true\n∗http://cottascience.github.io/\nPreprint. Under review.\narXiv:2010.04259v1  [cs.LG]  8 Oct 2020\nInput\nGraph\nGraph Energy Function\nInverse of\nPartition Function\nSet of all induced\nsubgraphs of size k in G\nFinite-sample\nunbiased MCMC\nestimate\nEstimated via\nNoise Contrastive\nEstimation\nAdjacency\nmatrix\nNode\nfeatures\nModel\nparams.\nHypergraph Markov Network Model\nMotif representation\nFigure 1: The proposed unsupervised graph representation using motif compositions. Here, we present the MHM-\nGNN model from Equation (1), the energy estimator bΦ from Equation (4), the motif energy and representation\nfrom Equation (2).\nunsupervised joint k-node representations, one must optimize a model deﬁned over all k-node\ninduced subgraphs of a graph.\nOur approach MHM-GNN (Motif Hypergraph Markov Graph Neural Networks) leverages the\ncompositionality of hypergraph Markov network models (HMNs) [53, 74, 31] that allows us to deﬁne\nan unsupervised objective (energy-based model) over GNN representations of motifs (see upper half\nof Figure 1).\nScalability is the main challenge we have to overcome, a type of scalability issue not addressed\nin the hypergraph Markov network literature [53, 74, 31]. First, there is the traditional likelihood\nintractability associated with computing the partition function Z(W) of energy models —Z(W) is\nshown in the likelihood P(A, X|W) in Figure 1 and also in Equation (1). There are standard solutions\nfor this challenge (e.g., Noise-Contrastive Estimation (NCE) [20]). The more vexing challenge comes\nfrom the intractability created by our inductive graph representation that applies motif representations\nto all k-node subgraphs, which requires\n\u0000n\nk\n\u0001\noperations per gradient step, typically with n ≫k. To\nmake this step tractable, we leverage recent advances in ﬁnite-sample unbiased Markov Chain Monte\nCarlo estimation for sums of subgraph functions over large graphs [61]. This unbiased estimate,\ncombined with Jensen’s inequality, allows us to optimize a lower bound on the intractable likelihood\n(assuming Z(W) is known). Fold that into the asymptotics of NCE and we get a principled, tractable\noptimization.\nContributions. Our contributions are three-fold. First, we introduce MHM-GNN, which produces\njoint (k > 2)-node representations, where k is a hyperparameter of the model. Second, we introduce\na principled and scalable stochastic optimization method that learns MHM-GNN with a ﬁnite-\nsample unbiased estimator of the graph energy (see Fig. 1) and a NCE objective. Finally, we show\nhow the joint k-node representations from MHM-GNN produce better unsupervised joint k-node\nrepresentations than existing approaches that aggregate node representations.\n2\nRelated Work\nIn this section, we brieﬂy review existing approaches to inductive unsupervised representation\nlearning of graphs, discuss existing work with higher-order graph representations and overview\nenergy-based models. Finally, we present what in literature is not related to this work.\nEdge-based graph models. Although graph models are prominent in many areas of research [43],\nmost of the proposed models, such as the initial Erdös-Rényi model [15], stochastic block models [25]\nand the more recent neural network-based approaches [29, 21, 8] assume conditional independence of\nedges, resulting in what is often called an edge-based loss function. That is, all such models assume\nthe appearance of edges in the graph is independent given the edge representations, which is usually\ncomputed via their endpoints’ representations. This important conditional independence assumption\nappears in what we call edge-based graph models. There exist alternatives such as Markov Random\nGraphs [18], where an edge is dependent on every other edge that shares one of its endpoints, but\ngraph models without any conditional independence assumption are still not commonly used.\nInductive unsupervised node representations with GNNs. Recently, GraphSAGE [21] introduced\nthe use of GNNs to learn inductive node representations in an unsupervised manner by applying an\n2\nedge-based loss while using short random walks. There are also auto-encoder approaches [29, 45, 54],\nwhere one tries to reconstruct the edges in the graph using node representations. Auto-encoders also\nassume conditional independence of edges and can be classiﬁed as edge-based models. In contrast to\nedge-based loss models, DGI [64] minimizes the mutual entropy between node representations and a\nwhole-graph representation —it does not model a probability distribution. Whilst the combination\nof GNNs and edge-based models has been shown to be effective in representing nodes and edges,\ni.e. k = 1 and k = 2 representations, moving to k > 2 joint representations requires a model with\nhigher-order factorization. To this end, we introduce MHM-GNN, a model that leverages hypergraph\nMarkov networks and GNNs to generate k-node motif representations.\nJoint k-node representations with dyadic graph. Recently, Morris et al. [39] and Maron et al.\n[36] proposed higher-order neural architectures to represent entire graphs in a supervised learning\nsetting, as opposed to the unsupervised setting discussed in this work. Moreover, we also point\nhow since these higher-order GNN approaches are concerned with representing entire graphs in a\nsupervised setting, the subgraph size k is treated as a constant and scalability is not addressed (models\nalready overﬁt with small k). Our approach can incorporate higher-order GNNs and also the more\nrecent Relational Pooling framework [40](see Equation (2)). We can summarize previous efforts to\nrepresent subgraphs in an unsupervised manner as sums of the individual nodes’ representations [22].\nHypergraph neural network models [69, 3, 16] require observing polyadic data, while here we are\ninterested in modeling dyadic data. We provide a broader discussion of higher-order graph models\nand the challenges of translating supervised approaches to an unsupervised setting in the supplement.\nEnergy-based models. Energy-Based Models (EBMs) have been widely used to learn representations\nof images [50], text [4], speech [60] and many other domains. In general, works in EBMs come in two\nﬂavors: what model to use for the energy and how to estimate the partition function Z(W), which\nis usually intractable. For the latter, there are model-speciﬁc MCMC methods, such as Contrastive\nDivergence [23] and standard solutions, such as the one we choose in this work: Noise-Constrastive\nEstimation (NCE). As for the energy model, we opt for a hypergraph Markov network [53, 74, 31].\nThe energy of a graph is given by all of its\n\u0000n\nk\n\u0001\nsubgraphs, which induces a new kind of intractability\nin the energy computation. Thus, we propose an unbiased energy estimation procedure in Section 4,\nwhich provides an upper bound on our NCE objective.\nUnrelated work. It is important to not confuse learning inductive unsupervised joint k-node\nrepresentations and other existing graph representation methods [39, 36, 19, 49, 51, 52, 34, 70].\nAlthough motif-aware methods [34, 52] explicitly use motif information, they are used to build\nnode representations rather than joint k-node representations, and thus, are equatable to other more\npowerful node representations, such as those in Hamilton et al. [21], Veliˇckovi´c et al. [64]. Here, we\nare interested in inductive tasks, hence transductive node representations, like Grover and Leskovec\n[19], Perozzi et al. [49], are unrelated. Nevertheless, as a matter of curiosity, we provide results\nfor transductive node representations in our joint k-node tasks in the supplement, showing that\nour approach also works well compared to transductive settings even though our approach was\nnot designed for transductive tasks. Supervised higher-order approaches [39, 36] extract whole-\ngraph representations, which cannot be directly translated to existing unsupervised settings (see\nsupplement for more details on these challenges). We are interested in methods that can be used\nin end-to-end representation learning, thus feature engineering and extraction, such as those used\nin graph kernels [70] are not of interest. Finally, the large body of work exploring hyperlink\nprediction in hypergraphs [7, 48, 73, 68, 71, 72] requires observing polyadic data (hypergraphs) and\nare transductive, as opposed to our work, where we consider observing dyadic data and propose an\ninductive model.\n3\nMotif Hypergraph Markov Graph Neural Networks (MHM-GNN)\nIn this section we start by introducing notation to then brieﬂy introduce hypergraph Markov networks\n(HMNs), describe MHM-GNN with an HMN model, and discuss possible GNN-based energy\nfunctions used to represent motifs.\nNotation. The i-th row of a matrix M will be denoted Mi·, and its j-th column M·j. For the\nsake of simplicity, we will focus on graphs without edge attributes, even though our model can\nhandle them using the GNN formulation from Battaglia et al. [6]. We denote a graph with n\nnodes by G = (V, E, X), where V is the set of nodes, E ⊆V 2 the edge set, A ∈{0, 1}n×n its\n3\ncorresponding adjacency matrix and the matrix X ∈Rn×p encodes the p node features of all n\nnodes. Each set of k nodes from a graph C ⊆V : |C| = k has an associated induced subgraph\nG(C) = (V (C), E(C), X(C)) (see Deﬁnition 1). Induced subgraphs are also referred to as motifs,\ngraphlets, graph fragments or subgraphs. Here, we will interchangeably refer to them as (induced)\nsubgraphs or motifs.\nDeﬁnition 1 (Induced Subgraph). Let C ⊆V : |C| = k be a set of k nodes from V with cor-\nresponding sorted sequence −→\nC = [C1, ..., Ck] : Ci < Ci+1, Ci ∈C ∀i ∈{1, ..., k}. Then,\nG(C) = (V (C), E(C), X(C)) is the induced subgraph of C in G, with adjacency matrix A(C), where\nV (C) = {1, ..., k}, A ∈{0, 1}k×k : A(C)\nij\n= ACiCj and X(C) ∈Rk×p : X(C)\ni·\n= XCi·.\nHypergraph Markov Networks (HMNs). A Markov Network (MN) deﬁnes a joint probability\ndistribution as a product of non-negative functions (potentials) over maximal cliques of an undirected\ngraphical model [27, 5]. Although deﬁned over maximal cliques, scalable techniques often assume\nfactorization over non-maximal cliques [53, 5, 74], such as Pairwise Markov Networks (PMNs) [24],\nwhere the distribution is expressed as a product of edge potentials. In contrast, since we are interested\nin learning joint representations of k-node subgraphs, we need a hypergraph Markov network (HMN)\n(Deﬁnition 2), which is an MN model that can encompass all the variables of a (k > 2)-node\nsubgraph.\nOur graph model is an HMN. HMNs are to PMNs what hypergraphs are to graphs. In HMNs, the\njoint distribution is expressed as a product of potentials of hyperedges rather than edges. Since in\nHMNs potentials are deﬁned over subsets of random variables of any size, we have the ﬂexibility to\ndo it over k-node subgraphs. There are previous works referring to HMNs as higher-order graphical\nmodels [53, 74], however we ﬁnd the hypergraph analogy more clarifying. Next, we provide a formal\ndeﬁnition of HMNs.\nDeﬁnition 2 (Hypergraph Markov Networks (HMNs)). A hypergraph Markov network is a Markov\nnetwork where the joint probability distribution of Y = {Y1, ..., Yl} can be expressed as P(Y =\ny) =\n1\nZ Πh∈Hφh(yh), where Z is the partition function Z = P\ny′∈Y Πh∈Hφh(y′\nh), φh(·) ≥0\nare non-negative, H ⊆P(Y)\\{∅}, where P(Y) is the powerset of a set Y, and H is the set of\nhyperedges in the Markov network, Yh are the random variables associated with hyperedge h and\ny, yh assignments of Y and Yh respectively. Finally, an energy-based HMN assumes strictly positve\npotentials, resulting in the model P(Y = y) = 1\nZ Πh∈H exp(−φh(yh)) = 1\nZ exp(−P\nh∈H φh(yh)),\nwhere φh(·) is called the energy function of h.\n3.1\nMHM-GNNs\nWe model P(A, X|W) with an energy-based HMN, as described in Deﬁnition 2, where a hyperedge\ncorresponds to an induced subgraph of k nodes in the graph G. More precisely, for every set of k > 1\nnodes in the graph C ⊆V, |C| = k, we deﬁne a hyperedge h = {Aij : (i, j) ∈C2}∪{Xi,· : i ∈C}\nin the HMN to encompass every node variable in the k-node set and every edge variable with both\nendpoints in it. A hyperedge can be indexed by a set of nodes C, since its corresponding set of random\nvariables is given by the features X(C) and the adjacency matrix A(C) of the subgraph induced by\nC, following Deﬁnition 1. Thus, a graph with n nodes will have an HMN with\n\u0000n\nk\n\u0001\npotentials. We\nformally deﬁne the model in Deﬁnition 3.\nDeﬁnition 3 (MHM-GNN). Let C(k) denote the set of all\n\u0000n\nk\n\u0001\ncombinations of k nodes from G. We\ndeﬁne a hypergraph Markov Network with a set of hyperedges {{Aij : (i, j) ∈C} ∪{Xi,· : i ∈\nC} : C ∈C(k)}, which following Deﬁnitions 1 and 2, entails the model\nP(A,X|W)= exp\n\u0000−P\nC∈C(k) φ(A(C),X(C);W)\n\u0001\nZ(W)\n,\n(1)\nwhere φ(·, ·; W) is an energy function with parameters W and Z(W) is the partition function given\nby Z(W) = P∞\nn=1\nP\nA′∈{0,1}n×n\nR\nX′∈Rn×p exp(−P\nC∈C(k) φ(A′(C), X′(C); W))dX′.\nAlthough MHM-GNN factorizes the total energy of a graph, the model does not assume any con-\nditional independence between edge variables for k > 3. For k = 2, the model recovers existing\nedge-based models and for k = 3 edge variables are dependent only on edges that share one of their\nendpoints, recovering the Markov random graphs class [18]. Furthermore, MHM-GNN will learn a\n4\njointly exchangeable distribution [44] if the subgraph energy function φ(., .; W) is jointly exchange-\nable, such as a GNN. In the supplement we connect MHM-GNN assumptions, exchangeability and\nExponential Random Graph Models (ERGMs).\nSubgraph energy function and representations. As mentioned, to have a jointly exchangeable\nmodel with MHM-GNN, we need an energy function φ(A(C), X(C); W) that is jointly exchange-\nable with respect to the subgraph G(C). To this end, we break down φ(A(C), X(C); W) into a\ncomposition of two functions. First, we compute a jointly exchangeable representation of G(C),\nthen we use it as input to a more general function that assigns an energy value to the subgraph.\nFollowing recent GNN advances [14, 67, 39], we deﬁne the subgraph representation with a permu-\ntation invariant (READOUT) function over the nodes’ representations given by a GNN, denoted\nby h(C)(A(C), X(C); WGNN, WR) = READOUT(GNN(A(C), X(C); WGNN); WR). Usually, the\nREADOUT function is a row-wise sum followed by a multi-layer perceptron. Note that, although we\nchoose a 1-GNN approach to represent the subgraph here, any jointly exchangeable graph representa-\ntion can be used to represent the subgraph, such as k-GNNs [39] and Relational Pooling [40].\nFinally, we can deﬁne the energy of a subgraph G(C) as\nφ(A(C), X(C); W) = WT\nenergyρ(h(C)(A(C), X(C); WGNN, WR); Wρ)\n(2)\nwhere the model set of weights is W = {Wenergy, WR, Wρ, WGNN}, ρ(·; Wρ) is a permutation\nsensitive function with parameters Wρ such as a multi-layer perceptron with range in R1×H and\nWenergy ∈R1×H is a (learnable) weight matrix.\nAlthough the functional form of the distribution and subgraph representations are properly deﬁned,\ndirectly computing both the partition function and the total energy of a graph are computationally\nintractable for an arbitrary k. Therefore, in the next section we discuss how to properly learn the\ndistribution parameters, providing a principled and scalable approximate method.\n4\nLearning MHM-GNNs\nIn this section, we ﬁrst deﬁne our unsupervised objective through Noise-Contrastive Estimation\n(NCE) and then show how to approximate it.\nNoise-Contrastive Estimation (NCE). Since directly computing Z(W) of Equation (1) is in-\ntractable, we use Noise-Contrastive Estimation (NCE) [20]. In NCE, the model parameters are\nlearned by contrasting observed data and negative (noise) sampled examples. Given the set Dtrue of\nobserved graphs and M|Dtrue| sampled noise graphs from a noise distribution Pn(A, X) composing\nthe set Dnoise, we can deﬁne the loss function to be minimized as\nL(A, X; W) = −\nX\nA∈Dtrue\nlog(ˆy(Φ(A, X; W), Pn(A, X)))\n−\nX\nA∈Dnoise\nlog(1 −ˆy(Φ(A, X; W), Pn(A, X))).\nwith ˆy(Φ(A, X; W), Pn(A, X)) = σ(−Φ(A, X; W) −log(MPn(A, X))), where σ(·) is the\nsigmoid function and Φ(A, X; W) = P\nC∈C(k) φ(A(C), X(C); W) denotes the total energy of a\ngraph G = (V, E, X) in MHM-GNN.\nIf the largest graph in Dtrue ∪Dnoise has n nodes, directly computing the gradient of the loss\n∇L(A, X; W) would take O(M|Dtrue|2nk) operations. Traditional Stochastic Gradient Descent\n(SGD) methods get rid of the dataset size M|Dtrue|2 term by uniformly sampling graph examples.\nThus, naively optimizing the NCE loss with SGD would still require O(nk) operations to compute\nΦ(A, X; W). In what follows we rely on a stochastic optimization procedure that requires a ﬁnite-\nsample unbiased estimator of Φ(A, X; W), where we can also control the estimator’s variance with a\nhyperparameter. We show that the resulting stochastic optimization is theoretically sound by proving\nthat it optimizes an upper bound of the original loss.\nEstimating the MHM-GNN energy Φ(A, X;W). To estimate Φ(A, X; W), we need to ﬁrst\nobserve that —due to sparsity in real-world graphs— an arbitrary set of k nodes from a graph will\ninduce an empty subgraph with high probability [43]. Therefore, to estimate Φ(A, X; W) with low\n5\nvariance, we focus on estimating it on connected induced subgraphs (CISes) [61], while assuming\nsome constant high energy for disconnected subgraphs. To this end, if C(k)\nconn is the set of all k-node\nsets that induce a connected subgraph in G, we are now making the reasonable assumption\nΦ(A, X; W) =\nX\nC∈C(k)\nconn\nφ(A(C), X(C); W) + constant,\n(3)\nwhere w.l.o.g. we assume the constant to be zero. Since enumerating all CISes is computationally\nintractable for arbitrary k [11], we introduce next a ﬁnite-sample unbiased estimator for Φ(A, X; W)\nof Equation (3) over CISes, denoted by bΦ(A, X; W).\nWe start by presenting the concept of the higher-order network (k-HON) of a graph G and its variant\ncalled collapsed node HON (k-CNHON). An ordinary k-HON G(k) is a network where the nodes\nV (k) correspond to k-node CISes from G and edges E(k) connect two CISes that share k −1 nodes.\nOn the other hand, a k-CNHON or G(k,I) is a multigraph where a subset of the nodes of G(k),\nI ⊂V (k), are collapsed into a single node in G(k,I). The collapsed node, henceforth denoted the\nsupernode, is now node v(k)\nI\nin G(k,I). The edges in G(k) of the collapsed nodes v ∈I among\nthemselves, i.e., the edges in I × I, do not exist in G(k,I). The edges between the collapsed nodes\nv ∈I and other nodes V \\ I are added to G(k,I) by replacing the endpoint v with endpoint v(k)\nI ,\nmaking G(k,I) a multigraph (a graph with multiple edges between the same two nodes). All the\nremaining edges in G(k) are preserved in G(k,I). In Figure 2 we show a graph and its k-CNHON\nwith a Random Walk Tour (Deﬁnition 4) example. A formal deﬁnition is given in supplement.\nDeﬁnition 4 (Random Walk Tour (RWT)). Consider a simple random walk over a multigraph starting\nat node vinit. A Random Walk Tour (RWT) is represented by a sequence of nodes T = {v1, ..., vt, vt+1}\nvisited by the random walk such that v1 = vinit, vt+1 = vinit and vi ̸= vinit ∀1 < i < t + 1.\nIn this work, we construct the estimator bΦ(A, X; W) via random walk tours (RWTs) on the k-\nCNHON G(k,I) starting at the collapsed node v(k)\nI\n(i.e, vinit = v(k)\nI\nin Deﬁnition 4). As previously\nintroduced and discussed in Avrachenkov et al. [2] and Teixeira et al. [61], increasing the number\nof tours and the supernode size allow for variance reduction. Using these insights, we propose the\nestimator bΦ(A, X; W), whose properties are deﬁned in Theorem 1.\nTheorem 1. Let G(k) be the k-HON of a graph G, a set I of k-node sets that induce CISes in\nG (as described above) and N (k)(C) the set of neighbors of the corresponding node of CIS C\nin G(k). In addition, consider the sample-path T r = (vr\n1, ..., vr\ntr, vr\ntr+1) visited by the r-th RWT\non G(k,I) starting from supernode v(k)\nI , where vr\ni is the node reached at step i for 1 ≤r ≤q\n(Deﬁnition 4), and q ≥1 is the number of RWTs. Since T r is a RWT, vr\n1 = v(k)\nI , vr\ntr+1 = v(k)\nI\nand\nvr\ni ̸= v(k)\nI\n: 1 < i < tr + 1. The nodes (vr\n2, ..., vr\ntr) in the sample path T r have a corresponding\nsequence of induced k-node subgraphs in the graph G, denoted T r\nC = (Cr\ni )tr\ni=2. Then, the estimator\nbΦ(A, X; W)=\nX\nv∈I\nφ(A(v), X(v); W)\n|\n{z\n}\nEnergy of k-node CISes in I (supernode)\n+\n\u0010P\nu∈I |N (k)(u)\\I|\nq\n\u0011\nq\nX\nr=1\ntr\nX\ni=2\nφ(A(Cr\ni ), X(Cr\ni ); W)\n|N (k)(Cr\ni )|\n|\n{z\n}\nRWT-estimated energy of remaining k-node CISes in G\nis an unbiased and consistent estimator of Φ(A, X; W) in Equation (3) with constant=0.\nThe proof of Theorem 1 is in the supplement.\nWe can now replace Φ(A, X; W) in L(A, X; W) with its estimator bΦ(A, X; W), resulting in a\nloss estimate bL(A, X; W). It follows from Theorem 1 and Jensen’s inequality that our loss estimate\nis in expectation an upper bound to the true NCE loss, i.e. EbΦ[ bL(A, X; W)] ≥L(A, X; W).\nMoreover, note that using an estimator of this nature in higher-order GNNs, such as k-GNNs [39],\ndoes not allow for a bound in the loss estimation (please, see the supplement for further discussion).\nNote that the variance of bΦ is controlled by the hyperparameter q, the number of tours.\n6\n1\n3\n5\n4\n7\n6\n2\n(a) Original graph.\n2\n5\n1\n3\n5\n4\n1\n5\n3\n2\n5\n4\n1\n5\n7\n1\n2\n3\n1\n5\n4\n2\n3\n6\n5\n3\n6\n4\n5\n7\n2\n3\n4\n3\n5\n7\n2\n5\n3\n3\n6\n4\n2\n5\n7\n5\n4\n6\n(b) k-CNHON with a supernode of size 3 highlighted in\nblue and an RWT example. Dashed red edges exist in the\nk-HON but are removed in the k-CNHON.\nFigure 2: A graph and its corresponding k-CNHON with an RWT example.\n5\nResults\nIn this section, we evaluate the quality of the unsupervised motif representations learned by MHM-\nGNN over six datasets using two joint k-node transfer learning tasks. The tasks consider three citation\nnetworks, one coauthorship network and two product networks to show how the pre-trained motif\nrepresentations consistently outperform pooling pre-trained node representations in predicting k-node\nhidden hyperedge labels in downstream tasks — details of these tasks are in the Hyperedge Detection\nand DAG Leaf Counting subsections.\nA good k-node representation of a graph is able to capture hidden k-order relationships while\nonly observing pairwise interactions. To this end, our tasks evaluate the quality of the unsupervised\nrepresentations using two hidden hyperedge label prediction tasks. Using the pre-trained unsupervised\nlearned representation as input, we train a simple logistic regression classiﬁer to predict the hidden\nhyperedge label of a k-node set.\nDatasets. We use the Cora, Citeseer and Pubmed [55] citation networks, the DBLP coauthorship\nnetwork [69], the Steam [47] and the Rent the Runway [38] product networks (more details about\nthe datasets are in the supplement). These datasets were chosen since they contain joint k-node\ninformation. In the coauthorship network, nodes correspond to authors and edges to the coauthorship\nof a paper, hidden from the training data we also have the papers and their corresponding author list.\nIn the product networks, nodes correspond to products and an edge exists if the same user bought the\ntwo end-point products, hidden from the training data we have the list of products each user bought.\nIn the citation network, nodes correspond to papers and edges to citations, hidden from the training\ndata we also have the direction in which the citation occurred. These directions, paper author list\nand users purchase history which are hidden in the training data used by the unsupervised GNN and\nMHM-GNN representations, give us two transfer learning k-node downstream tasks, described in\nwhat follows.\nHyperedge Detection. This hyperedge task, inspired by Yadati et al. [69], creates a k-node hyperedge\nin a citation network whenever a paper cites k −1 other papers, in a coauthorship network whenever\nk authors write a paper together and in a product network whenever a user buys k products. Examples\nare in the citation networks k-size subgraphs with at least one node with degree k −1 and k-cliques in\nthe other networks. Note how Yadati et al. [69] directly learns its representations from the hypergraph,\na signiﬁcantly easier task. The downstream classiﬁer —a simple logistic regression classiﬁer— uses\nthe unsupervised pre-trained representations to classify whether a set of k nodes forms a (hidden)\nhyperege or not. This task allows us to compare the quality of the unsupervised node representations\nof GNNs against that of MHM-GNN.\nDAG Leaf Counting. This task considers the citation networks. Again, baselines and MHM-GNN\nare trained over the undirected graphs. Due to the temporal order of citations, subgraphs correspond to\nDirected Acyclic Graphs (DAGs) in the directed structure. For a connected k-node induced subgraph\n7\nin the directed graph, we want to predict the number of leaves of the resulting DAG. Again, the\ndownstream classiﬁer —a simple logistic regression classiﬁer— uses the unsupervised pre-trained\nrepresentations of a set of k-nodes to predict the exact number of leaves formed by the (hidden)\nk-node DAG. The number of leaves deﬁnes the number of inﬂuential papers in the k-node set.\nMHM-GNN architecture. The energy function of MHM-GNN is as described in Equation (2),\nwhere we use a one-hidden layer feedforward network with LeakyReLU activations as ρ, a row-wise\nsum followed by also a one-hidden layer feedforward network with LeakyReLU activations as the\nREADOUT function and a single layer GraphSAGE-mean Hamilton et al. [21] as the GNN.\nTraining the model. Since the datasets used in this section contain only one large graph for training\n—as in most of the real-world graph datasets— we need to construct a larger set of positive examples\nDtrue to learn the distribution P(A, X|W). One way to overcome this issue is by subsampling the\noriginal large graph. While sampling smaller graphs that preserve the original graph properties,\nwe can approximate the true P(A, X|W) distribution and control the complexity of bΦ(A, X; W))\n(since tour return times are affected by the size of the graph). To this end, we construct Dtrue by\nsubsampling the original graph with Forest Fire [35]. As for the noise distribution, we turn to the\none used by Veliˇckovi´c et al. [64], where for each positive example we generate M negative samples\nby keeping the adjacency matrix and shufﬂing the feature matrix. This noise distribution allows us\nto keep structural properties of the graph, e.g. connectivity, while signiﬁcantly changing how node\nfeatures affect the distribution. We precisely describe all hyperparameters and hyperparameter tuning\nin the supplement.\nExperimental setup. To evaluate the performance of the pre-trained MHM-GNN representations in\nthe above downstream tasks, we ﬁrst train the model accordingly for k = 3 and k = 4 motif sizes over\nall six datasets. In the citation and coauthorship networks, we have a single graph, thus these tasks\nrequire dividing the graph into training and test sets when evaluating the representations, such that\nthe distribution of observed subgraphs is preserved. To this end, for each dataset, we perform min-cut\nclustering and use the two cuts for training and test data in the downstream task. For the product\nnetworks, to explore the inductive nature of our method, we create two graphs, one for training the\nmodels and one for testing the representations. For the Steam dataset, we train on the user-product\ndata from 2014 and test considering the data from 2015. Similarly, for the Rent the Runway dataset,\nwe train on data from 2016 and test on data from 2017. Tables 1 and 2 show our results for the\nHyperedge Detection and Table 3 for the DAG Leaf Counting tasks. MHM-GNN uses motif sizes\nk = 3, 4. In the supplement, we also show results for k = 5. For each task (and k), we report the\nmean and the standard deviation of the balanced accuracy (mean recall of each class) achieved by\nlogistic regression over ﬁve different runs. Furthermore, the pre-trained representations (baselines\nand our approach) have dimension 128. Additional implementation details and hyperparameter search\ncan be found in the supplement.\nBaselines. We evaluate the motif representations from MHM-GNN against two alternatives repre-\nsenting the k nodes using state-of-the-art unsupervised GNN representations: GraphSAGE [21] and\nDeep Graph Infomax (DGI) [64]. As a naive baseline, we compare against summing the original\nfeatures from the nodes, i.e. a representation that ignores structural information. Moreover, we\ncompare our pre-trained MHM-GNN representations with an untrained (random parameters) version\nof MHM-GNN. Further, since the citation and coauthorship networks consider single graphs, in the\nsupplement we show results for these datasets with two prominent transductive node embedding\nmethods [49, 19], evidencing how even in transductive settings node embeddings fail to capture joint\nk-node relationships.\nResults. The hidden hyperedge downstream tasks are designed to better understand how well\npre-trained unsupervised representations can capture joint k-node properties. A good joint k-node\nrepresentation should be able to disentangle (hidden) polyadic relationships, even though they\nonly have access to dyadic data. In our Hyperedge Detection task using pre-trained unsupervised\nrepresentations, Tables 1 and 2 show that MHM-GNN representations consistently outperform GNN\nnode representations across all datasets. In particular, MHM-GNN increases classiﬁcation accuracy\nby up to 11% over the best-performing baseline. The results of our the DAG Leaf Counting task,\nshown in Table 3, reinforce that pre-trained unsupervised MHM-GNN representations can better\ncapture joint k-node interactions. In particular, MHM-GNN representations observe classiﬁcation\naccuracies by up to 24% in this downstream task.\n8\nTable 1: Balanced accuracy for the Hyperedge Detection task over subgraphs of size k = 3. We report mean\nand standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nDBLP\nSteam\nRent the Runway\nk = 3\nk = 3\nk = 3\nk = 3\nk = 3\nk = 3\nGS-mean[21]\n0.490 ± 0.03\n0.509 ± 0.07\n0.499 ± 0.00\n0.560 ± 0.08\n0.565 ± 0.01\n0.665 ± 0.00\nGS-max[21]\n0.486 ± 0.04\n0.493 ± 0.06\n0.498 ± 0.00\n0.569 ± 0.06\n0.579 ± 0.02\n0.667 ± 0.00\nGS-lstm[21]\n0.483 ± 0.04\n0.486 ± 0.05\n0.510 ± 0.02\n0.585 ± 0.06\n0.518 ± 0.01\n0.518 ± 0.01\nDGI[64]\n0.487 ± 0.03\n0.508 ± 0.07\n0.509 ± 0.02\n0.497 ± 0.00\n0.588 ± 0.01\n0.612 ± 0.00\nRaw Features\n0.499 ± 0.00\n0.588 ± 0.00\n0.502 ± 0.00\n0.518 ± 0.00\n0.534 ± 0.00\n0.649 ± 0.00\nMHM-GNN (Rnd)\n0.498 ± 0.00\n0.520 ± 0.05\n0.498 ± 0.01\n0.491 ± 0.01\n0.571 ± 0.01\n0.650 ± 0.00\nMHM-GNN\n0.618 ± 0.03\n0.621 ± 0.01\n0.602 ± 0.06\n0.773 ± 0.02\n0.611 ± 0.01\n0.676 ± 0.00\nTable 2: Balanced accuracy for the Hyperedge Detection task over subgraphs of size k = 4. We report mean\nand standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nDBLP\nSteam\nRent the Runway\nk = 4\nk = 4\nk = 4\nk = 4\nk = 4\nk = 4\nGS-mean[21]\n0.450 ± 0.11\n0.544 ± 0.03\n0.524 ± 0.05\n0.530 ± 0.15\n0.640 ± 0.03\n0.851 ± 0.00\nGS-max[21]\n0.462 ± 0.09\n0.538 ± 0.04\n0.558 ± 0.05\n0.511 ± 0.14\n0.688 ± 0.01\n0.855 ± 0.00\nGS-lstm[21]\n0.444 ± 0.09\n0.536 ± 0.04\n0.566 ± 0.06\n0.653 ± 0.02\n0.504 ± 0.01\n0.546 ± 0.03\nDGI[64]\n0.463 ± 0.10\n0.526 ± 0.04\n0.549 ± 0.06\n0.500 ± 0.00\n0.664 ± 0.02\n0.749 ± 0.03\nRaw Features\n0.529 ± 0.01\n0.581 ± 0.00\n0.498 ± 0.02\n0.558 ± 0.01\n0.535 ± 0.01\n0.857 ± 0.00\nMHM-GNN (Rnd)\n0.490 ± 0.10\n0.478 ± 0.03\n0.510 ± 0.02\n0.492 ± 0.02\n0.679 ± 0.01\n0.832 ± 0.01\nMHM-GNN\n0.575 ± 0.03\n0.659 ± 0.08\n0.701 ± 0.10\n0.740 ± 0.05\n0.750 ± 0.00\n0.860 ± 0.00\nNode representations and joint k-node graph tasks. Our experiments further validate the theoretical\nclaims in Srinivasan and Ribeiro [57], that structural node representations are not capable of perform-\ning joint k-node tasks. That is, the inductive node representations baselines perform similarly to a\nrandom classiﬁer in most settings in Tables 1 to 3. In contrast, the greater accuracy of MHM-GNN\nshows that joint k-node representations are informative.\nAblation study. As an ablation, we test whether our optimization in MHM-GNN improves the\nunsupervised joint k-node representations, when compared against random neural network weights.\nAnd while Tables 1 to 3 show that MHM-GNN with random weights perform well in the tasks,\nsince they are effectively a type of motif feature, the higher accuracy of the optimized joint k-node\nrepresentations shows that the optimized representations in MHM-GNN are indeed learned.\nTable 3: Balanced accuracy for the DAG Leaf Counting task over subgraphs of size k = 3 and k = 4. We\nreport mean and standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nk = 3\nk = 4\nk = 3\nk = 4\nk = 3\nk = 4\nGS-mean[21]\n0.468 ± 0.05\n0.245 ± 0.06\n0.492 ± 0.04\n0.356 ± 0.02\n0.502 ± 0.00\n0.384 ± 0.03\nGS-max[21]\n0.467 ± 0.06\n0.245 ± 0.07\n0.486 ± 0.04\n0.347 ± 0.01\n0.499 ± 0.00\n0.371 ± 0.03\nGS-lstm[21]\n0.473 ± 0.05\n0.263 ± 0.07\n0.482 ± 0.04\n0.348 ± 0.01\n0.507 ± 0.02\n0.372 ± 0.03\nDGI[64]\n0.478 ± 0.05\n0.278 ± 0.07\n0.504 ± 0.06\n0.350 ± 0.02\n0.505 ± 0.02\n0.362 ± 0.03\nRaw Features\n0.501 ± 0.01\n0.325 ± 0.00\n0.567 ± 0.00\n0.380 ± 0.00\n0.503 ± 0.00\n0.339 ± 0.00\nMHM-GNN (Rnd)\n0.497 ± 0.00\n0.327 ± 0.00\n0.518 ± 0.04\n0.319 ± 0.01\n0.499 ± 0.01\n0.343 ± 0.01\nMHM-GNN\n0.593 ± 0.03\n0.452 ± 0.03\n0.606 ± 0.01\n0.469 ± 0.02\n0.626 ± 0.02\n0.475 ± 0.08\nAs opposed to node GNN representations and other non-compositional unsupervised graph represen-\ntation approaches, MHM-GNN does not take graph-wide information as input. Thus, it is natural to\nwonder to what extent pre-trained MHM-GNN joint k-node representations are informative of the\nentire graph to which they belong to. Hence, in the supplement we consider whole-graph classiﬁ-\ncation as the downstream task. In this setting, we show how composing (by pooling) MHM-GNN\nmotif representations can perform better than non-compositional methods, further indicating how our\nlearned motif representations can capture the underlying graph distribution P(A, X; W).\n6\nConclusions\nBy combining hypergraph Markov networks, an unbiased ﬁnite-sample MCMC estimator, and\ngraph representation learning, we introduced MHM-GNN, a new scalable class of energy-based\nrepresentation learning methods capable of learning joint k-node representations over dyadic graphs\nin an inductive unsupervised manner. Finally, we show how pre-trained MHM-GNN representations\nachieve more accurate results in downstream joint k-node tasks. The energy-based optimization in\nthis work allows for many extensions, such as designing different k-node subgraph representation\nlearning methods, new subgraph function estimators for MHM-GNN’s loss function, and formulating\nnew joint k-node tasks.\n9\nBroader Impact\nThis work presents an unsupervised model together with a stochastic optimization procedure to\ngenerate k-node representations from graphs, such as online social networks, product networks,\ncitation networks, coauthorship networks, etc. As is the case with any learning algorithm, it is\nsusceptible to produce biased representations if trained with biased data. Moreover, although the\nrepresentations might be bias free, the downstream task deﬁned by the user might be biased and thus,\nalso produce biased decisions.\nAcknowledgments\nThis work was funded in part by the National Science Foundation (NSF) Awards CAREER IIS-\n1943364, CCF-1918483, and by the ARO, under the U.S. Army Research Laboratory contract\nnumber W911NF-09-2-0053, the Purdue Integrative Data Science Initiative, the Purdue Research\nFoundation, and the Wabash Heartland Innovation Network. Any opinions, ﬁndings and conclusions\nor recommendations expressed in this material are those of the authors and do not necessarily reﬂect\nthe views of the sponsors. Further, we would like to thank Mayank Kakodkar for his invaluable\nfeedback and discussion on subgraph function estimation.\nReferences\n[1] Aldous, D. and Fill, J. (1995). Reversible markov chains and random walks on graphs.\n[2] Avrachenkov, K., Ribeiro, B., and Sreedharan, J. K. (2016). Inference in OSNs via Lightweight Partial\nCrawls. In SIGMETRICS, volume 44, pages 165–177, New York, New York, USA. ACM Press.\n[3] Bai, S., Zhang, F., and Torr, P. H. (2019). Hypergraph convolution and hypergraph attention. arXiv preprint\narXiv:1901.08150.\n[4] Bakhtin, A., Deng, Y., Gross, S., Ott, M., Ranzato, M., and Szlam, A. (2020). Energy-based models for text.\narXiv preprint arXiv:2004.10188.\n[5] Barber, D. (2012). Bayesian reasoning and machine learning. Cambridge University Press.\n[6] Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti,\nA., Raposo, D., Santoro, A., Faulkner, R., et al. (2018). Relational inductive biases, deep learning, and graph\nnetworks. arXiv preprint arXiv:1806.01261.\n[7] Benson, A. R., Abebe, R., Schaub, M. T., Jadbabaie, A., and Kleinberg, J. (2018). Simplicial closure and\nhigher-order link prediction. Proceedings of the National Academy of Sciences, 115(48):E11221–E11230.\n[8] Bojchevski, A. and Günnemann, S. (2018). Deep Gaussian embedding of graphs: Unsupervised inductive\nlearning via ranking. In International Conference on Learning Representations.\n[9] Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., and Kriegel, H.-P. (2005).\nProtein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47–i56.\n[10] Brémaud, P. (2013). Markov chains: Gibbs ﬁelds, Monte Carlo simulation, and queues, volume 31.\nSpringer Science & Business Media.\n[11] Bressan, M., Chierichetti, F., Kumar, R., Leucci, S., and Panconesi, A. (2017). Counting graphlets: Space\nvs time. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, pages\n557–566.\n[12] Cai, J.-Y., Fürer, M., and Immerman, N. (1992). An optimal lower bound on the number of variables for\ngraph identiﬁcation. Combinatorica, 12(4):389–410.\n[13] Christopher Morris, Gaurav Rattan, P. M. (2020). Weisfeiler and leman go sparse: Towards scalable\nhigher-order graph embeddings. In Graph Representation Learning and Beyond (GRL+, ICML 2020).\n[14] Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and Adams,\nR. P. (2015). Convolutional networks on graphs for learning molecular ﬁngerprints. In Advances in neural\ninformation processing systems, pages 2224–2232.\n[15] Erdös, P. and Rényi, A. (1959). On random graphs. Publicationes Mathematicae Debrecen, 6:290.\n[16] Feng, Y., You, H., Zhang, Z., Ji, R., and Gao, Y. (2019). Hypergraph neural networks. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 3558–3565.\n[17] Fey, M. and Lenssen, J. E. (2019). Fast graph representation learning with PyTorch Geometric. In ICLR\nWorkshop on Representation Learning on Graphs and Manifolds.\n10\n[18] Frank, O. and Strauss, D. (1986). Markov graphs. Journal of the american Statistical association,\n81(395):832–842.\n[19] Grover, A. and Leskovec, J. (2016). node2vec: Scalable feature learning for networks. In Proceedings of\nthe 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864.\n[20] Gutmann, M. and Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for\nunnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artiﬁcial\nIntelligence and Statistics, pages 297–304.\n[21] Hamilton, W. L., Ying, R., and Leskovec, J. (2017a). Inductive representation learning on large graphs. In\nProceedings of the 31st International Conference on Neural Information Processing Systems.\n[22] Hamilton, W. L., Ying, R., and Leskovec, J. (2017b). Representation learning on graphs: Methods and\napplications. IEEE Data Eng. Bull., 40(3):52–74.\n[23] Hinton, G. E. (2002).\nTraining products of experts by minimizing contrastive divergence.\nNeural\ncomputation, 14(8):1771–1800.\n[24] Höﬂing, H. and Tibshirani, R. (2009). Estimation of sparse binary pairwise markov networks using\npseudo-likelihoods. Journal of Machine Learning Research, 10(Apr):883–906.\n[25] Holland, P. W., Laskey, K. B., and Leinhardt, S. (1983). Stochastic blockmodels: First steps. Social\nnetworks, 5(2):109–137.\n[26] Kersting, K., Kriege, N. M., Morris, C., Mutzel, P., and Neumann, M. (2016). Benchmark data sets for\ngraph kernels.\n[27] Kindermann, R. and Snell, J. (1982). Markov Random Fields and Their Application, Providence, RI: Amer.\nMath. Soc.\n[28] Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In 3rd International\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings.\n[29] Kipf, T. N. and Welling, M. (2016). Variational graph auto-encoders. NIPS Workshop on Bayesian Deep\nLearning.\n[30] Kipf, T. N. and Welling, M. (2017). Semi-supervised classiﬁcation with graph convolutional networks. In\nInternational Conference on Learning Representations (ICLR).\n[31] Kohli, P., Torr, P. H., et al. (2009). Robust higher order potentials for enforcing label consistency.\nInternational Journal of Computer Vision, 82(3):302–324.\n[32] Kolaczyk, E. D. and Csárdi, G. (2014). Statistical analysis of network data with R, volume 65. Springer.\n[33] Lauritzen, S., Rinaldo, A., and Sadeghi, K. (2018). Random networks, graphical models and exchangeabil-\nity. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3):481–508.\n[34] Lee, J. B., Rossi, R. A., Kong, X., Kim, S., Koh, E., and Rao, A. (2019). Graph convolutional networks\nwith motif-based attention. In Proceedings of the 28th ACM International Conference on Information and\nKnowledge Management, pages 499–508.\n[35] Leskovec, J. and Faloutsos, C. (2006). Sampling from large graphs. In Proceedings of the 12th ACM\nSIGKDD international conference on Knowledge discovery and data mining, pages 631–636.\n[36] Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y. (2019). Invariant and equivariant graph networks.\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\n[37] Meng, C., Mouli, C. S., Ribeiro, B., and Neville, J. (2018). Subgraph pattern neural networks for high-order\ngraph evolution prediction. In AAAI.\n[38] Misra, R., Wan, M., and McAuley, J. (2018). Decomposing ﬁt semantics for product size recommendation\nin metric spaces. In Proceedings of the 12th ACM Conference on Recommender Systems, pages 422–426.\n[39] Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., and Grohe, M. (2019).\nWeisfeiler and Leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, volume 33, pages 4602–4609.\n[40] Murphy, R. L., Srinivasan, B., Rao, V., and Ribeiro, B. (2019). Relational pooling for graph representations.\nIn ICML. PMLR.\n[41] Nair, V. and Hinton, G. E. (2009). Implicit mixtures of restricted boltzmann machines. In Koller, D.,\nSchuurmans, D., Bengio, Y., and Bottou, L., editors, Advances in Neural Information Processing Systems 21,\npages 1145–1152. Curran Associates, Inc.\n[42] Narayanan, A., Chandramohan, M., Venkatesan, R., Chen, L., Liu, Y., and Jaiswal, S. (2017). graph2vec:\nLearning distributed representations of graphs. arXiv preprint arXiv:1707.05005.\n11\n[43] Newman, M. (2018). Networks. Oxford University Press.\n[44] Orbanz, P. and Roy, D. M. (2014). Bayesian models of graphs, arrays and other exchangeable random\nstructures. IEEE transactions on pattern analysis and machine intelligence, 37(2):437–461.\n[45] Pan, S., Hu, R., Long, G., Jiang, J., Yao, L., and Zhang, C. (2018). Adversarially regularized graph\nautoencoder for graph embedding. In IJCAI, pages 2609–2615.\n[46] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,\nAntiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. In Advances\nin neural information processing systems, pages 8026–8037.\n[47] Pathak, A., Gupta, K., and McAuley, J. (2017). Generating and personalizing bundle recommendations on\nsteam. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, pages 1073–1076.\n[48] Patil, P., Sharma, G., and Murty, M. N. (2020). Negative sampling for hyperlink prediction in networks. In\nPaciﬁc-Asia Conference on Knowledge Discovery and Data Mining, pages 607–619. Springer.\n[49] Perozzi, B., Al-Rfou, R., and Skiena, S. (2014). Deepwalk: Online learning of social representations. In\nProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,\npages 701–710.\n[50] Ranzato, M., Poultney, C., Chopra, S., and Cun, Y. L. (2007). Efﬁcient learning of sparse representations\nwith an energy-based model. In Advances in neural information processing systems, pages 1137–1144.\n[51] Rossi, R. A., Ahmed, N. K., Koh, E., Kim, S., Rao, A., and Abbasi-Yadkori, Y. (2020). A structural graph\nrepresentation learning framework.\n[52] Rossi, R. A., Zhou, R., and Ahmed, N. K. (2018). Deep inductive network representation learning. In\nCompanion Proceedings of the The Web Conference 2018, pages 953–960.\n[53] Rowland, M. and Weller, A. (2017). Uprooting and rerooting higher-order graphical models. In Advances\nin Neural Information Processing Systems, pages 209–218.\n[54] Samanta, B., De, A., Ganguly, N., and Gomez-Rodriguez, M. (2018). Designing random graph models\nusing variational autoencoders with applications to chemical design. arXiv preprint arXiv:1802.05283.\n[55] Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T. (2008). Collective classiﬁca-\ntion in network data. AI magazine, 29(3):93–93.\n[56] Shi, G., Hönig, W., Yue, Y., and Chung, S.-J. (2020). Neural-swarm: Decentralized close-proximity\nmultirotor control using learned interactions. arXiv preprint arXiv:2003.02992.\n[57] Srinivasan, B. and Ribeiro, B. (2020). On the equivalence between positional node embeddings and\nstructural graph representations. In ICLR.\n[58] Sun, F.-Y., Hoffman, J., Verma, V., and Tang, J. (2020). Infograph: Unsupervised and semi-supervised\ngraph-level representation learning via mutual information maximization. In International Conference on\nLearning Representations.\n[59] Taylor, A., Singletary, A., Yue, Y., and Ames, A. (2019). Learning for safety-critical control with control\nbarrier functions. arXiv preprint arXiv:1912.10099.\n[60] Teh, Y. W., Welling, M., Osindero, S., and Hinton, G. E. (2003). Energy-based models for sparse\novercomplete representations. Journal of Machine Learning Research, 4(Dec):1235–1260.\n[61] Teixeira, C. H., Cotta, L., Ribeiro, B., and Meira, W. (2018). Graph pattern mining and learning through\nuser-deﬁned relations. In 2018 IEEE International Conference on Data Mining (ICDM), pages 1266–1271.\nIEEE.\n[62] Tsitsulin, A., Mottin, D., Karras, P., Bronstein, A., and Müller, E. (2018). Netlsd: hearing the shape of a\ngraph. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining, pages 2347–2356.\n[63] Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. (2017). Graph Attention\nNetworks. arXiv preprint arXiv:1710.10903.\n[64] Veliˇckovi´c, P., Fedus, W., Hamilton, W. L., Liò, P., Bengio, Y., and Hjelm, R. D. (2019). Deep Graph\nInfomax. In International Conference on Learning Representations.\n[65] Wang, P., Lui, J., Ribeiro, B., Towsley, D., Zhao, J., and Guan, X. (2014). Efﬁciently estimating motif\nstatistics of large networks. ACM Transactions on Knowledge Discovery from Data (TKDD), 9(2):8.\n[66] Weisfeiler, B. and Lehman, A. A. (1968). A reduction of a graph to a canonical form and an algebra arising\nduring this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12–16.\n[67] Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2019). How powerful are graph neural networks? In\nInternational Conference on Learning Representations.\n12\n[68] Xu, Y., Rockmore, D., and Kleinbaum, A. M. (2013). Hyperlink prediction in hypernetworks using latent\nsocial features. In International Conference on Discovery Science, pages 324–339. Springer.\n[69] Yadati, N., Nimishakavi, M., Yadav, P., Nitin, V., Louis, A., and Talukdar, P. (2019). Hypergcn: A new\nmethod for training graph convolutional networks on hypergraphs. In Advances in Neural Information\nProcessing Systems, pages 1509–1520.\n[70] Yanardag, P. and Vishwanathan, S. (2015). Deep graph kernels. In Proceedings of the 21th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, pages 1365–1374.\n[71] Yoon, S.-e., Song, H., Shin, K., and Yi, Y. (2020). How much and when do we need higher-order\ninformationin hypergraphs? a case study on hyperedge prediction. In Proceedings of The Web Conference\n2020, pages 2627–2633.\n[72] Zhang, M., Cui, Z., Jiang, S., and Chen, Y. (2018). Beyond link prediction: Predicting hyperlinks in\nadjacency space. In AAAI.\n[73] Zhang, M., Cui, Z., Oyetunde, T., Tang, Y., and Chen, Y. (2016). Recovering metabolic networks using a\nnovel hyperlink prediction method. arXiv preprint arXiv:1610.06941.\n[74] Zheleva, E., Getoor, L., and Sarawagi, S. (2010). Higher-order graphical models for classiﬁcation in\nsocial and afﬁliation networks. In NIPS Workshop on Networks Across Disciplines: Theory and Applications,\nvolume 2.\nA\nThe Estimator bΦ(A, X;W)\nA.1\nThe k-CNHON network\nDeﬁnition 5 (k-CNHON of G given I, or G(k,I)). Let G(k) = (V (k), E(k)) be the higher-order network\n(k-HON) of the input graph G, where each node v(k) ∈V (k) corresponds to a k-node set C ∈C(k)\nconn. For\nease of understanding, we will levarege this correspondence and refer to nodes from V (k) with k-node sets\nfrom C(k)\nconn interchangeably. The edge set E(k) is deﬁned such that E(k) = {(v(k)\ni\n, v(k)\nj\n) : v(k)\ni\n, v(k)\nj\n∈\nC(k)\nconn and |v(k)\ni\n∩v(k)\nj\n| = k −1}. Moreover, let I be a set of k-nodes sets I ⊂C(k)\nconn. Then, a k-CNHON\nG(k,I) = (V (k,I), E(k,I)) with supernode v(k)\nI\nis a multigraph with node set V (k,I) = (V (k)\\I) ∪v(k)\nI\nand\nedge multiset E(k,I) = E(k)\\(E(k)∩(I×I))⊎{(v(k)\nI , v(k)) : ∃(u(k), v(k)) ∈E(k), u(k) ∈I and v(k) /∈I},\nwhere ⊎is the multiset union operation.\nA.2\nProof of Theorem 1\nTo prove Theorem 1, we assume that G(k,I) has a stationary distribution π with\nπ(Ci) =\n|N (k)(Ci)|\nP\nC′∈V (k)\\I |N (k)(C′)| + P\nu∈I |N (k)(u)\\I| ∀Ci ∈V (k,I)\\{v(k)\nI },\nand\nπ(v(k)\nI ) =\nP\nu∈I |N (k)(u)\\I|\nP\nC′∈V (k)\\I |N (k)(C′)| + P\nu∈I |N (k)(u)\\I|.\nFortunately, Wang et al. [65] showed that such a statement is true whenever I contains at least one k-node set\nfrom each connected component of G and if each such component contains at least one vertex which is not a part\nof any k-node set in I and is contained in more than 2 edges in G. First, we show that the estimate bΦ(A, X; W)\nof each tour is unbiased.\nLemma 1. Let T r\nC = (Cr\ni )tr\ni=2 be a k-node set chain formed by the samples from the r-th RWT on G(k,I)starting\nat the supernode v(k)\nI . Then, ∀r ≥1,\nE\nh X\nv∈I\nφ(A(v), X(v); W) +\n\u0010 X\nu∈I\n|N (k)(u)\\I|\n\u0011 tr\nX\ni=2\nφ(A(Cr\ni ), X(Cr\ni ); W)\n|N (k)(Cr\ni )|\ni\n= Φ(A, X; W),\n(4)\nassuming Φ(A, X; W) with zero constant.\n13\nProof of Lemma 1. Let’s ﬁrst rewrite Equation (4) as\n\u0010 X\nu∈I\n|N (k)(u)\\I|\n\u0011\nE\nh tr\nX\ni=2\nφ(A(Cr\ni ), X(Cr\ni ); W)\n|N (k)(Cr\ni )|\ni\n= Φ(A, X; W) −\nX\nv∈I\nφ(A(v), X(v); W).\n(5)\nSince the RWT starts at node v(k)\nI , we may rewrite the expected value in Equation (5) as\nE\n\" tr\nX\ni=2\nφ(A(Cr\ni ), X(Cr\ni ); W)\n|N (k)(Cr\ni )|\n#\n=\nX\nCi∈C(k)\nconn\\I\nE\n\u0014\nT(Ci)φ(A(Ci), X(Ci); W)\n|N (k)(Ci)|\n\u0015\n,\n(6)\nwhere T(C) represents the number of times the RWT reaches state C.\nConsider a renewal reward process with inter-renewal time distributed as tr, r ≥1 and reward as T(Cr\ni ).\nFurther, note that the chain is positive recurrent, thus E[tr] < ∞, E[T(Cr\ni )] < ∞and T(Cr\ni ) < ∞. Then, from\nthe renewal reward theorem and the ergodic theorem [10] we have\nπ(Cr\ni ) = E[tr]−1E[T(Cr\ni )].\nMoreover, it follows from Kac’s formula [1] that E[tr] =\n1\nπ(v(k)\nI\n). Therefore, Equation (6) can be rewritten as\nE\n\" tr\nX\ni=2\nφ(A(Cr\ni ), X(Cr\ni ); W)\n|N (k)(Cr\ni )|\n#\n=\nX\nCi∈C(k)\nconn\\I\nπ(Ci)φ(A(Ci), X(Ci); W)\nπ(v(k)\nI )|N (k)(Ci)|\n.\n(7)\nNow, knowing the stationary distribution of G(k,I), we may simplify Equation (7) to\nE\n\" tr\nX\ni=2\nφ(A(Cr\ni ), X(Cr\ni ); W)\n|N (k)(Cr\ni )|\n#\n=\n1\nP\nu∈I |N (k)(u)\\I|\nX\nCi∈C(k)\nconn\\I\nφ(A(Ci), X(Ci); W),\n(8)\nand replace it in Equation (5), concluding our proof.\nProof of Theorem 1. By Lemma 1, linearity of expectation and knowing that each RWT is independent from the\nother tours by the Strong Markov Property, Theorem 1 holds.\nB\nDiscussion of MHM-GNN properties\nConditional independence.\nAlthough HMNs factorize distributions, the potentials themselves do not\nprovide information on conditional and marginal distributions. Rather, we need to analyze how every pair of\nvariables interacts through all potentials. For the sake of simplicity, consider the model described in Deﬁnition 3\nfor undirected simple graphs, i.e. Aij = Aji ∀(i, j) ∈V 2, Aii = 0 ∀i ∈V . If we set k = 2, each hyperedge\nwill contain exactly one edge variable and two node variables, which is equivalent to assuming all edges are\nindependent given their nodes’ representations. Thus, for k = 2 MHM-GNN can recover edge-based models\nwhere representations don’t use graph-wide information. Furthermore, if we allow the node representation to\ntake graph-wide information, we can recover the recent Graph Neural Networks approaches [21, 29, 8]. If we opt\nfor k = 3, a hyperedge deﬁned by nodes i, j, l will contain the set of edge variables {Aij, Ail, Ajl} and node\nvariables {Xi,·, Xj,·, Xl,·}. Thus, a hyperedge will encompass only edge variables that share one endpoint.\nIn this case, an edge variable Aij is independent from {Alm : l, m ∈V, {l, m} ∩{i, j} = ∅} others given\n{Ail : l ∈V } ∪{Aim : m ∈V } ∪{Xi,· : i ∈V } . Thus, MHM-GNN with k = 3 can be cast as an instance\nof the Markov random graphs class proposed by Frank and Strauss [18]. With k ≥4, for every pair of edge\nvariables Aij, Alm there exists at least one C ∈C(k) such that i, j, l, m ⊆C. Thus, there exists at least one\nhyperedge covering every pair of edge variables in the model, resulting in a fully connected hypergraph Markov\nNetwork. Therefore, for k ≥4 the model does not assume any conditional independence between edge variables\nwhich, since subgraphs share edge variables, is a vital feature for joint k-node representations of graphs.\nExchangeability.\nAlthough with inﬁnite data and an arbitrary energy function φ(·, ·; W) MHM-GNN would\nlearn a jointly exchangeable [44] distribution, we would like to impose such condition on the model, deﬁning a\nproper graph model. Equivalently, we would like to guarantee that any two isomorphic graphs have the same\nprobability under MHM-GNN. By deﬁnition, the sets of subgraphs from two isomorphic graphs are equivalent\nunder graph isomorphism. Thus, if the subgraph energy function φ(·, ·; W) is jointly exchangeable, the set of\nsubgraph energies from two isomorphic graphs are equivalent. Since the sum operation is permutation invariant\nand the partition function is a constant, a jointly exchangeable subgraph energy function φ(·, ·; W), such as a\nGNN, is enough to make MHM-GNN jointly exchangeable.\n14\nExponential Random Graph Models (ERGMs).\nThe form of MHM-GNN presented in Deﬁnition 3\nresembles the general and classical expression of Exponential Random Graph Models (ERGMs) [32]. Indeed,\nas any energy-based network model, we can cast ours as an ERGM where the sufﬁcient statistics are given by\nall k-size subgraphs. However, we do stress how any exchangeable graph model has a correspondent ERGM\nrepresentation [33], even when it is not as clear as it in MHM-GNN.\nC\nAdditional Experiments and Implementation Details from Section 5\nC.1\nResults for k = 5\nHere, we extend the results from Section 5 to a k = 5 setting in Table 5 and table 4. Due to the lack of papers\nwith 5 authors (less than 10), we were not able to extend them to the DBLP dataset. Moreover, the conclusions\nfrom Section 5 also hold here. That is, MHM-GNN consistently outperforms the baselines. However, on Rent\nthe Runway we see the raw features achieving the highest performance. That is, structural information does not\nseem to be relevant to this speciﬁc task. Nevertheless, we still see that MHM-GNN and GraphSAGE are the\nmethods able to perform the task similarly to the raw features.\nTable 4: Balanced accuracy for the Hyperedge detection task over subgraphs of size k = 5. We report mean\nand standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nSteam\nRent the Runway\nk = 5\nk = 5\nk = 5\nk = 5\nk = 5\nGS-mean[21]\n0.447 ± 0.10\n0.530 ± 0.03\n0.697 ± 0.08\n0.696 ± 0.07\n0.933 ± 0.00\nGS-max[21]\n0.384 ± 0.09\n0.543 ± 0.08\n0.722 ± 0.06\n0.765 ± 0.03\n0.940 ± 0.00\nGS-lstm[21]\n0.422 ± 0.04\n0.525 ± 0.03\n0.736 ± 0.08\n0.532 ± 0.05\n0.557 ± 0.05\nDGI[64]\n0.504 ± 0.00\n0.500 ± 0.00\n0.500 ± 0.00\n0.626 ± 0.11\n0.827 ± 0.04\nRaw Features\n0.500 ± 0.00\n0.513 ± 0.00\n0.526 ± 0.00\n0.602 ± 0.00\n0.944 ± 0.00\nMHM-GNN (Rnd)\n0.460 ± 0.05\n0.453 ± 0.03\n0.493 ± 0.07\n0.748 ± 0.02\n0.924 ± 0.00\nMHM-GNN\n0.543 ± 0.06\n0.703 ± 0.04\n0.815 ± 0.10\n0.823 ± 0.00\n0.943 ± 0.01\nTable 5: Balanced accuracy for the DAG Leaf Counting task over subgraphs of size k = 5. We report mean\nand standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nk = 5\nk = 5\nk = 5\nGS-mean[21]\n0.223 ± 0.04\n0.259 ± 0.02\n0.284 ± 0.02\nGS-max[21]\n0.150 ± 0.07\n0.263 ± 0.01\n0.288 ± 0.02\nGS-lstm[21]\n0.214 ± 0.03\n0.259 ± 0.00\n0.295 ± 0.04\nDGI[64]\n0.236 ± 0.02\n0.249 ± 0.00\n0.249 ± 0.00\nRaw Features\n0.251 ± 0.05\n0.266 ± 0.00\n0.290 ± 0.00\nMHM-GNN (Rnd)\n0.231 ± 0.01\n0.277 ± 0.02\n0.244 ± 0.01\nMHM-GNN\n0.363 ± 0.04\n0.364 ± 0.02\n0.330 ± 0.04\nC.2\nHyperparameters and Hyperparameter Search for MHM-GNN\nAll MHM-GNN models were implemented in PyTorch [46] and PyTorch Geometric [17] with the Adam\noptimizer [28]. All hyperparameters were chosen to minimize training loss. For learning rate, we searched\nin {0.01, 0.001, 0.0001} ﬁnding the best learning rate to be 0.001 for all models. We used a single hidden\nlayer feedforward network with LeakyReLU activations for both ρ and READOUT functions in all models.\nFurthermore, following GraphSAGE Hamilton et al. [21], for all models we do an L2 normalization in the motif\nrepresentation layer, i.e. in the output of the READOUT function. Finally, for all models we use M = 1 negative\nexample for each positive example. In what follows, we give speciﬁc hyperparameters and their search for\nexperiments from Section 5, show results for transductive baselines, and introduce new whole-graph downstream\ntasks together with their speciﬁc hyperparameters and search as well.\nC.3\nPre-trained HMH-GNN for k-node downstream tasks (Section 5)\nMHM-GNN architecture. The energy function of MHM-GNN is as described in Equation (2), where we use a\none-hidden layer feedforward network with LeakyReLU activations as ρ, a row-wise sum followed by also a\none-hidden layer feedforward network with LeakyReLU activations as the READOUT function and a single\nlayer GraphSAGE-mean Hamilton et al. [21] as the GNN, except for k = 5 in the citation networks where we\nused two layers of the GraphSAGE-mean GNN to achieve faster convergence in training.\n15\nTable 6: Results for transductive baselines in the Hyperedge Detection task over k = 3, k = 4 and k = 5 size\nsubgraphs.\nMethod\nCora\nCiteseer\nPubmed\nDBLP\nk = 3\nk = 3\nk = 3\nk = 3\nnode2vec[19]\n0.534 ± 0.04\n0.525 ± 0.02\n0.501 ± 0.00\n0.461 ± 0.05\nnode2vec[19] + Features\n0.545 ± 0.01\n0.534 ± 0.01\n0.500 ± 0.00\n0.479 ± 0.04\nDeepWalk[49]\n0.472 ± 0.02\n0.433 ± 0.01\n0.499 ± 0.00\n0.481 ± 0.00\nDeepWalk[49] + Features\n0.512 ± 0.01\n0.591 ± 0.01\n0.502 ± 0.00\n0.485 ± 0.02\n(a) (k = 3) Balanced accuracy for the Hyperedge Detection task over subgraphs of size k = 3. We report\nmean and standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nDBLP\nk = 4\nk = 4\nk = 4\nk = 4\nnode2vec[19]\n0.537 ± 0.04\n0.513 ± 0.03\n0.504 ± 0.01\n0.405 ± 0.01\nnode2vec[19] + Features\n0.626 ± 0.03\n0.540 ± 0.01\n0.502 ± 0.00\n0.548 ± 0.10\nDeepWalk[49]\n0.515 ± 0.07\n0.494 ± 0.10\n0.504 ± 0.01\n0.460 ± 0.01\nDeepWalk[49] + Features\n0.597 ± 0.05\n0.570 ± 0.01\n0.516 ± 0.01\n0.560 ± 0.03\n(b) (k = 4) Balanced accuracy for the Hyperedge Detection task over subgraphs of size k = 4. We report\nmean and standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nk = 5\nk = 5\nk = 5\nnode2vec[19]\n0.446 ± 0.08\n0.544 ± 0.08\n0.623 ± 0.13\nnode2vec[19] + Features\n0.519 ± 0.00\n0.500 ± 0.00\n0.502 ± 0.01\nDeepWalk[49]\n0.446 ± 0.07\n0.568 ± 0.05\n0.568 ± 0.13\nDeepWalk[49] + Features\n0.490 ± 0.01\n0.523 ± 0.01\n0.472 ± 0.11\n(c) (k = 5) Balanced accuracy for the Hyperedge Detection task over subgraphs of size k = 5. We report mean\nand standard deviation over ﬁve runs.\nSubsampling positive examples. We use positive examples subsampled with Forest Fire [35] of size 100 for\nCora, Citeseer and DBLP datasets, while for Pubmed, a larger network, we use examples of size 500. For Steam,\na smaller network, we use 75 and for Rent the Runway, a mid-size network we use 150.\nNumber of tours. We did 80 tours for all datasets except Pubmed with k = 4, which due to a larger k-CNHON\nnetwork, we did 120 tours. A small number of tours will result in high variance in the gradient which, as we\nobserved, tends to impair the learning process. Therefore, we tested training models, each with a different ﬁx\nnumber of tours, starting with 1 tour and increasing it 10 by 10 until we reached the reported number of tours,\nwhich results in training loss convergence.\nSupernode size. To construct the supernode, we do a BFS on the k-HON of the original input graph, similarly\nto Teixeira et al. [61]. We have a parameter that controls the maximum number of subgraphs visited by the BFS,\nwhich we call supernode budget. This parameter was set to 100K for Pubmed with k = 3 and k = 4, 5K for\nCora with k = 3 and k = 4, Citeseer with k = 3 and DBLP with k = 3, 10K for Citeseer with k = 4 and 50K\nfor DBLP with k = 4. For Steam, we set to 1K for k = 3 and to 10K for k = 4. For Rent the Runway, we set to\n10K for k = 3 and to 30K for k = 4. For k = 5, we used 50K in Cora, 75K in Citeseer, 120K in Pubmed, 50K\nin Steam and 100K in Rent the Runway. In the same way of tours, we started with a small supernode budget of\n100 and increased it by 100 until we observed the tours being completed and the training loss converging.\nMinibatch size. We used a minibatch size of 50 for Cora, Citeseer and Steam with k = 3 and 25 for Cora and\nCitesser with k = 4. For Pubmed, Rent the Runway and DBLP, larger networks, we used minibatches of size\n40 for k = 3 and 10 for k = 4. For Steam, we used 20 for k = 4. Again, we tested small minibatch sizes,\nincreasing them until we had training loss convergence and GPU memory space to use. For k = 5, we used a\nminibatch of size 5 in all datasets.\nC.3.1\nTransductive baselines\nSince we deﬁned the tasks from Section 5 over single graphs in the citation and couathorship networks,\nin Tables 6a to 6c, and Tables 7a to 7c we show for those datasets results for two prominent transductive\nnode embedding methods, node2vec [19] and DeepWalk [49] together with concatenating the raw features to\nthem, evidencing how even in transductive settings, transductive node embeddings fail to capture joint k-node\nrelationships in most settings, performing similarly to the inductive approaches to node representations, thus,\nperforming consistently worse than our MHM-GNN joint k-node representations.\n16\nTable 7: Results for transductive baselines in the DAG Leaf Counting task over k = 3, k = 4 and k = 5 size\nsubgraphs.\nMethod\nCora\nCiteseer\nPubmed\nk = 3\nk = 3\nk = 3\nnode2vec[19]\n0.538 ± 0.05\n0.546 ± 0.03\n0.502 ± 0.01\nnode2vec[19] + Features\n0.556 ± 0.02\n0.527 ± 0.01\n0.501 ± 0.00\nDeepWalk[49]\n0.466 ± 0.02\n0.503 ± 0.06\n0.499 ± 0.00\nDeepWalk[49] + Features\n0.543 ± 0.01\n0.584 ± 0.00\n0.503 ± 0.00\n(a) (k = 3) Balanced accuracy for the DAG Leaf Counting task over subgraphs of size k = 3. We report mean\nand standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nk = 4\nk = 4\nk = 4\nnode2vec[19]\n0.374 ± 0.06\n0.329 ± 0.04\n0.333 ± 0.00\nnode2vec[19] + Features\n0.410 ± 0.04\n0.388 ± 0.00\n0.339 ± 0.00\nDeepWalk[49]\n0.322 ± 0.00\n0.349 ± 0.04\n0.339 ± 0.00\nDeepWalk[49] + Features\n0.349 ± 0.00\n0.381 ± 0.00\n0.345 ± 0.00\n(b) (k = 4) Balanced Accuracy for the DAG Leaf Counting task over subgraphs of size k = 4. We report mean\nand standard deviation over ﬁve runs.\nMethod\nCora\nCiteseer\nPubmed\nk = 5\nk = 5\nk = 5\nnode2vec[19]\n0.265 ± 0.05\n0.262 ± 0.03\n0.298 ± 0.03\nnode2vec[19] + Features\n0.263 ± 0.01\n0.240 ± 0.02\n0.259 ± 0.01\nDeepWalk[49]\n0.254 ± 0.02\n0.240 ± 0.01\n0.238 ± 0.05\nDeepWalk[49] + Features\n0.255 ± 0.00\n0.269 ± 0.00\n0.269 ± 0.01\n(c) (k = 5) Balanced Accuracy for the DAG Leaf Counting task over subgraphs of size k = 5. We report mean\nand standard deviation over ﬁve runs.\nC.4\nPre-trained MHM-GNN representations for whole-graph downstream tasks\nIn Section 5, we have seen that the motif representations learned by MHM-GNN can better predict hyperedge\nproperties than existing unsupervised GNN representations. In the following experiments we investigate: Are\nMHM-GNN motif representations capturing graph-wide information (learning P(A, X; W))? To this end,\ninspired by Nair and Hinton [41]’s evaluation of RBM representations through supervised learning, we now\ninvestigate if MHM-GNN’s pre-trained motif representations can do similarly or better than non-compositional\nmethods that take graph-wide information in (inductive) whole-graph classiﬁcation.\nDatasets. We use four multiple graphs datasets, namely PROTEINS, ENZYMES, IMDB-BINARY and IMDB-\nMULTI [70, 26]. We are interested in evaluating whole-graph representations under two different scenarios, one\nwhere the nodes have high-dimensional feature vectors and the other where the nodes do not have features. To\nthis end, we chose the two biological networks PROTEINS and ENZYMES, where nodes contain feature vectors\nof size 32 and 21 respectively and the social networks IMDB-BINARY and IMDB-MULTI where nodes do not\nhave features. More details in Section D of this supplement.\nTraining the model. Since we have multiple graphs in our datasets, our set of positive graph examples is already\ngiven in the data, unlike in Section 5, where we had to subsample positives from a single graph. The negative\nexamples still need to be sampled. For the biological networks, we used the same negative sampling approach\nused in Section 5. For the social networks, where the nodes do not have features, for each positive example, we\nuniformly at random add n edges to it, generating a negative sample (where n is the number of nodes in the\ngraph).\nExperimental setup. We equally divide the graphs in each dataset between training (unsupervised) and\ntraining+testing (supervised). We use two thirds of the graphs in the supervised dataset to train a logistic\nclassiﬁer for the downstream task over the graph’s representation. We use a third of the supervised dataset to\ntest the method’s accuracy. The classiﬁcation tasks used here are the same as in Borgwardt et al. [9] and Xu\net al. [67]. Again, we set the representation dimension of both MHM-GNN and our baselines to 128. We show\nresults for k = 3, 4, 5 motifs representations, k = n whole-graph representations, and unsupervised GNN node\nrepresentations. To create these representations, we tested both sum and mean pooling for MHM-GNN (except\nk = n) and all the node-based baselines. We report the best performance of each for a fair comparison.\nBaselines. We compare MHM-GNN against non-compositional methods: pooling node representations from\nGraphSAGE and DGI, directly pooling node features, two recent whole-graph embedding methods, NetLSD [62]\nand graph2vec [42] and a recent unsuperved whole-graph representation, InfoGraph [58]. Apart from pooling\nnode features, all methods input graph-wide information to their representations. Pooling node features is not\napplicable to the social networks, since they do not have such information. Additionally, DGI also generates a\nwhole-graph representation to minimize the mutual entropy with the nodes’ representations. Note how by setting\n17\nTable 8: Results for the whole-graph classiﬁcation task evaluated over balanced accuracy. We report mean and\nstandrad deviation over ﬁve runs.\nMethod\nPROTEINS\nENZYMES\nIMDB-BIN.\nIMDB-MULT\nGS-mean[21]\n0.753 ± 0.01\n0.435 ± 0.02\n0.454 ± 0.01\n0.347 ± 0.01\nGS-max[21]\n0.729 ± 0.01\n0.400 ± 0.04\n0.447 ± 0.01\n0.360 ± 0.01\nGS-lstm[21]\n0.739 ± 0.01\n0.404 ± 0.04\n0.442 ± 0.00\n0.342 ± 0.01\nDGI (Nodes)[64]\n0.743 ± 0.02\n0.349 ± 0.04\n0.469± 0.00\n0.367 ± 0.02\nDGI (Joint)[64]\n0.756 ± 0.00\n0.263 ± 0.03\n0.568 ± 0.03\n0.376 ± 0.01\nRaw Features\n0.665 ± 0.05\n0.210 ± 0.02\n–\n–\nNetLSD[62]\n0.760 ± 0.00\n0.250 ± 0.00\n0.550 ± 0.00\n0.430 ± 0.01\ngraph2vec[42]\n0.685 ± 0.00\n0.166 ± 0.00\n0.507 ± 0.00\n0.335 ± 0.00\nInfoGraph[58]\n0.690 ± 0.04\n0.278 ± 0.04\n0.691 ± 0.04\n0.466 ± 0.02\nMHM-GNN (Rnd) (k = 3)\n0.733 ± 0.01\n0.293 ± 0.02\n0.586 ± 0.00\n0.369 ± 0.001\nMHM-GNN (k = 3)\n0.777 ± 0.01\n0.445 ± 0.01\n0.586 ± 0.00\n0.376 ± 0.00\nMHM-GNN (Rnd) (k=4)\n0.720 ± 0.02\n0.229 ± 0.04\n0.580 ± 0.00\n0.371 ± 0.00\nMHM-GNN (k = 4)\n0.780 ± 0.02\n0.390 ± 0.04\n0.621 ± 0.00\n0.390 ± 0.002\nMHM-GNN (Rnd) (k = 5)\n0.722 ± 0.01\n0.213 ± 0.03\n0.580 ± 0.00\n0.378 ± 0.005\nMHM-GNN (k = 5)\n0.773 ± 0.01\n0.326 ± 0.04\n0.600 ± 0.01\n0.397 ± 0.001\nMHM-GNN (Rnd) (k = n)\n0.704 ± 0.03\n0.266 ± 0.02\n0.707 ± 0.02\n0.446 ± 0.005\nMHM-GNN (k = n)\n0.753 ± 0.00\n0.327 ± 0.01\n0.694 ± 0.02\n0.451 ± 0.01\nk = n, we consider the entire graph as a single motif and thus, learn a whole-graph representation. Again, all\nmodels were trained according to their original implementation.\nResults. We show in Table 8 the results for whole-graph classiﬁcation downstream tasks. For each task and\neach model, we report the mean and the standard deviation of the balanced accuracy (mean recall of each class)\nachieved by logistic regression over ﬁve different runs. We observe how our method consistently outperforms\nrepresentations computed over the entire graph: the joint DGI approach, graph2vec and node representations\npooling. Interestingly, we observe that when the graph has high-dimensional feature vectors of the nodes,\npooling small motif representations better generalizes than all other methods to unseen graphs. On the other\nhand, we observe that using a joint whole-graph representation, either with k = n in our model or with NetLSD\nor with InfoGRAPH, can perform better without node features. In fact, there is no signiﬁcant difference between\nusing a random and a trained model for the joint representation. It is known how a random GNN model simply\nassigns a unique representation to each class of graphs indistinguishable under the 1-WL test [67]. Therefore, for\ngraphs without node features, assigning unique representations seems to be the best in this setting, which means\nthat the tested graph embedding and unsupervised representation methods are not really capturing signiﬁcant\ngraph information. Overall, we observe that indeed motif representations are capable of representing the entire\ngraph to which they belong and even give better results, evidencing how MHM-GNN is learning graph-wide\ninformation, i.e. capturing P(A, X; W) and how motif compositionality can explain networks functionality.\nMHM-GNN architecture. We use the same ρ and READOUT functions as in Section 5, while changing the\nGNN to GIN Xu et al. [67] (which gave better validation results than the GAT, GCN, and GraphSAGE GNNs).\nAgain, we use M = 1, i.e., we sample one negative example for each positive sample. We show results of\nMHM-GNN for k = 3, 4, 5, n. For the estimator bΦ(A, X; W), we perform 30 tours for every model and\ndataset.\nGNN layer. We use a single-layer GIN Xu et al. [67] as the GNN layer in our method. For k = n, where the\nGNN is applied over large graphs, we used GIN with two layers. Note that we also tested GraphSAGE-mean,\nGCN and GAT GNN layers here, but GIN resulted in faster training loss convergence.\nNumber of tours. We did 30 tours for all datasets. Again, we tested training models, each with a different ﬁx\nnumber of tours, starting with 1 tour and increasing 10 by 10 until we reached the reported number of tours,\nwhich results in training loss convergence.\nSupernode size. We did a BFS with the maximum number of subgraphs visited as 5K for all models (and all\nk). Again, we started with a small supernode budget of 100 and increased it by 100 until we observed the tours\nbeing completed and the training loss converging.\nMinibatch size. We used a minibatch size of 50 for ENZYMES and PROTEINS for all reported k. For\nIMDB-BINARY and IMDB-MULTI, which have larger networks we used a minibatch size of 10. Again, we\ntested small minibatch sizes and increased until we had training loss convergence and GPU memory to use.\nPooling functions. We tested both sum and mean pooling motif (our model) and node (baselines) representations\nfor all models here. We observed that mean pooling performs the best for all models in all datasets, except for\nthe ENZYMES dataset, where sum pooling performed the best for all models. Thus, Table 8 contain results with\n18\nmean pooling for all models in the PROTEINS, IMDB-BINANRY and IMDB-MULTI datasets and sum pooling\nfor all models in the ENZYMES dataset.\nD\nDatasets\nWe present the datasets statistics in Table 9 and Table 10. For the PROTEINS and ENZYMES datasets, we\nadded the node labels as part of the node features. For the DBLP, we subsampled (with Forest Fire) the original\nlarge network from Yadati et al. [69]. For the Steam graphs, we consider user-product relations from 2014 to\ncreate the training graph and data from 2015 to create the test graph. Similarly, we use 2016 data to create the\nRent the Runway training graph and 2017 data to create the test graph. For both product networks, the node\nfeatures we created are sparse bag-of-words from the user text reviews.\nTable 9: Single graph datasets statistics.\nDataset\nType\nNodes\nEdges\nFeatures\nCora [55]\nCitation Network\n2,708\n5,429\n1,433\nCiteseer [55]\nCitation Network\n3,327\n4,732\n3,703\nPubmed [55]\nCitation Network\n19,717\n44,338\n500\nDBLP [69]\nCoauthorship Network\n4,309\n12,863\n1,425\nSteam [47] (Train)\nProduct Network\n1,098\n7,839\n775\nSteam [47] (Test)\nProduct Network\n1,322\n7,547\n775\nRent the Runway [38] (Train)\nProduct Network\n2,985\n55,979\n1,475\nRent the Runway [38] (Test)\nProduct Network\n5,003\n67,365\n1,475\nTable 10: Multiple graphs datasets statistics.\nDataset\nType\nGraphs\nFeatures\nClasses\nPROTEINS [26]\nBiological Network\n1,113\n32\n2\nENZYMES [26]\nBiological Network\n600\n21\n6\nIMDB-BINARY [26]\nSocial Network\n1,000\n0\n2\nIMDB-MULTI [26]\nSocial Network\n1,500\n0\n3\nE\nRelated Work: Higher-order Graph Representations\nIn what follows, we review the existing approaches to higher-order graph representations in literature.\nHigher-order graph representations. Morris et. al [39] showed how to expand the concept of a GNN, an\napproach based on the 1-WL algorithm [66], to a k-GNN, an approach based on the class of k-WL [12] algorithms,\nwhere instead of generating node representations, one can derive higher-order (k-size) representations later used\nto represent the entire graph. Although such approaches to represent entire graphs have been recently used in\nsupervised graph classiﬁcation tasks, how to systematically use them in an inductive unsupervised manner was\nnot clear. Since edge-based models require factorizing over a 2-node representation, only 1-WL [30, 67, 21, 63]\nand 2-WL [39]-based GNNs can be used. Additionally, k-GNNs can be thought of as a GNN over an extended\ngraph, where nodes are k-node tuples and edges exist between k-tuples that share exactly k −1 nodes. One\ncould indeed think of applying an edge-based loss to the extended graph, where the nodes (k-node tuples)\nrepresentations are given by a k-GNN. However, an edge-based model assumes independence among edges and\nan edge in the extended graph is repeated several times in the extended graphs, thus they are not independent.\nFinally, even if one could provide an unsupervised objective to k-GNNs, it would still require O(nk(kδ)L) steps\nto compute an L-layer k-GNN over a graph with n nodes and maximum degree δ. Due to the non-linearities\nin the READOUT function and in the neighborhood aggregations in k-GNNs, unbiased subgraph estimators\nsuch as the one presented in this work and neighborhood sampling technique such as the one from Hamilton\net al. [21] would not provide an unbiased or a bounded loss estimation such as MHM-GNN does. Moreover,\nthe more recent sparser version of k-GNNs [13] uses k-node tuple representations, instead of k-node subgraph\nrepresenations as in the original paper. Finally, MHM-GNN can take advantage of any graph representation\nmethod, including k-GNNs [39] and non-GNN approaches such the ones presented in Relational Pooling [40].\nSum-based subgraph representations. There has been recent work representing subgraphs by equating them\nwith sets of node representations [22]. In general, these approaches use graph models able to generate node\nrepresentations and then add a module on top to aggregate these individual representations in the downstream\n19\ntask. The most prominent efforts have treated subgraph representations as sums of the individual nodes’ repre-\nsentations [22], namely sum-based techniques. These approaches do not rely on joint subgraph representations,\ni.e. subgraphs that share nodes will tend to have similar representations, constraining their representational\npower and thus relying more on the downstream task model.\nHypergraph models. In this work, we wish to learn a graph model through motif representations in the presence\nof standard dyadic (graph) data, i.e. we are only observing pairwise relationships. Therefore, we emphasize\nthat hypergraph models, despite dealing with higher-order representations of graphs, require observing polyadic\n(hypergraph) data and therefore are not an alternative to the problem studied here.\nSupervised learning with subgraphs. Meng et al. [37] made the ﬁrst effort towards supervised learning with\nsubgraphs, where the authors predict higher-order properties from temporal dyadic data, as opposed to the\nproblem presented here, where we are are interested in inductive unsupervised learning of k-node sets from\nstatic graphs. Moreover, Meng et. al learned subgraph properties while optimizing a pseudo-likelihood function,\ni.e. ignoring the dependencies among different subgraphs in the loss function. Because different node sets share\nedge variables, it is vital to learn dependencies among them. Hence, here we presented the ﬁrst graph model\nbased on k-size motif structures trained with a proper Noise-Contrastive Estimation function, i.e. our model\naccounts for dependencies between every edge to represent k-size node sets.\n20\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-10-08",
  "updated": "2020-10-08"
}