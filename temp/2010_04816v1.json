{
  "id": "http://arxiv.org/abs/2010.04816v1",
  "title": "Characterizing Policy Divergence for Personalized Meta-Reinforcement Learning",
  "authors": [
    "Michael Zhang"
  ],
  "abstract": "Despite ample motivation from costly exploration and limited trajectory data,\nrapidly adapting to new environments with few-shot reinforcement learning (RL)\ncan remain a challenging task, especially with respect to personalized\nsettings. Here, we consider the problem of recommending optimal policies to a\nset of multiple entities each with potentially different characteristics, such\nthat individual entities may parameterize distinct environments with unique\ntransition dynamics. Inspired by existing literature in meta-learning, we\nextend previous work by focusing on the notion that certain environments are\nmore similar to each other than others in personalized settings, and propose a\nmodel-free meta-learning algorithm that prioritizes past experiences by\nrelevance during gradient-based adaptation. Our algorithm involves\ncharacterizing past policy divergence through methods in inverse reinforcement\nlearning, and we illustrate how such metrics are able to effectively\ndistinguish past policy parameters by the environment they were deployed in,\nleading to more effective fast adaptation during test time. To study\npersonalization more effectively we introduce a navigation testbed to\nspecifically incorporate environment diversity across training episodes, and\ndemonstrate that our approach outperforms meta-learning alternatives with\nrespect to few-shot reinforcement learning in personalized settings.",
  "text": "Characterizing Policy Divergence for\nPersonalized Meta-Reinforcement Learning\nMichael Zhang\nHarvard University\nCambridge, MA 02138\nmichael_zhang@college.harvard.edu\nAbstract\nDespite ample motivation from costly exploration and limited trajectory data,\nrapidly adapting to new environments with few-shot reinforcement learning (RL)\ncan remain a challenging task, especially with respect to personalized settings. Here,\nwe consider the problem of recommending optimal policies to a set of multiple\nentities each with potentially different characteristics, such that individual entities\nmay parameterize distinct environments with unique transition dynamics. Inspired\nby existing literature in meta-learning, we extend previous work by focusing on\nthe notion that certain environments are more similar to each other than others\nin personalized settings, and propose a model-free meta-learning algorithm that\nprioritizes past experiences by relevance during gradient-based adaptation. Our\nalgorithm involves characterizing past policy divergence through methods in inverse\nreinforcement learning, and we illustrate how such metrics are able to effectively\ndistinguish past policy parameters by the environment they were deployed in,\nleading to more effective fast adaptation during test time. To study personalization\nmore effectively we introduce a navigation testbed to speciﬁcally incorporate\nenvironment diversity across training episodes, and demonstrate that our approach\noutperforms meta-learning alternatives with respect to few-shot reinforcement\nlearning in personalized settings.\n1\nIntroduction\nWhile reinforcement learning (RL) has demonstrated success in various sequential decision making\nsettings Barto et al. [1990], Shortreed et al. [2011], challenges remain in the personalized domain. For\nexample, we may be interested in recommending treatments to a cohort of patients, where individuals\nmay respond to the same treatment differently, even while conditioning on the same health state.\nFrom an RL framework, each patient can be modeled as an entity whose individual characteristics\nparameterize a distinct Markov decision process (MDP). With this framing, one could, in principle,\ntrain a separate policy from scratch for each individual. However, this approach is impractical in\nsettings where exploration is costly and individual trajectory data are limited. As a result, there is\nan increasing need to think about how to leverage prior data in sophisticated manners to develop\nfew-shot learning approaches for deployment in unseen environments.\nIn this work, we take a meta-learning approach to this problem and propose a method tailored to\nfew-shot learning across a diverse set of environments. Our approach builds on model agnostic meta-\nlearning (MAML), a paradigm for learning a single prior initialization of policy gradient parameters\nthat can adapt quickly to a set of observed tasks. We extend this approach by leveraging the notion\nthat certain environments are more similar to each other than others in personalized settings, and\naccordingly their corresponding experiences should be given higher attention during adaptation. We\nAccepted at the Workshop on Deep Reinforcement Learning at the 33rd Conference on Neural Information\nProcessing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:2010.04816v1  [cs.LG]  9 Oct 2020\nthus propose generalizing this approach by learning several different potential initializations, and\nchoosing an appropriate initialization to use for a given environment at test time.\nOur proposed algorithm–cluster adaptive meta-learning (CAML)–explicitly seeks to determine which\npast parameters should be factored in during adaptation. Because the core idea of our adaptation\nrelies on being able to quickly identify the most relevant past trajectories, we seek to characterize\nthese experiences through a distance metric that establishes a notion of similarity between policies,\nborrowing from related work in inverse reinforcement learning (IRL) and determining policy diver-\ngence. We rely on whole trajectories−which provide a richer source of information over alternatives\nused in previous work Clavera et al. [2018], Finn et al. [2017], Nagabandi et al. [2018]−and would\nlike to call upon a diverse set of environment interactions for reference. Accordingly, we retain an\nepisodic model-free approach in contrast to more recent model-based online-learning methods where\neach timestep is considered a new task Clavera et al. [2018]. Our contributions center around how to\n(1) study personalization effectively in simulated contexts, (2) characterize the divergence of policies\nacross a population of environments, and (3) learn effectively across different and previously unseen\nenvironments with minimum exploration cost. We introduce a K-medoids-inspired algorithm for\npolicy adaptation and few-shot learning across multiple environments, and demonstrate approach’s\nsuccesses over existing methods in a 2D testbed.\n2\nBackground\n2.1\nPersonalized Markov decision processes (MDPs)\nWe consider a modiﬁed version of a typical continuous state-space MDP, parameterized speciﬁcally\nby an individual entity type Ti among a population of possible types T . For lexical consistency, we\nrefer to the agent as the actual policy or decision-maker, and deﬁne an “entity” to be the object through\nwhich an agent interacts with the larger environment (the “patient” in our working example). Each\nentity type then introduces an MDP Mi ≜(S, A, P i, ri, γ, si\n0) with unique transition probabilities\nP i : S × A × S →R. Given shared state and action spaces S and A, we thus allow for diversity in\nbehavior across entities from different types.\nFor any two different entity types Ti ̸= Tj, P i may not necessarily differ from P j, but crucially at the\nonset of training for some subset of unseen types Ti ⊆T we make no assumption that these transition\ndynamics are the same. Finally, γ is a discount factor assumed to be the same for all agents, and si\n0 is\nthe initial state distribution for environment type i. Our goal across a population of entity types is\nthen to learn some function f : T →Π mapping from entity types to policies. For each entity type\ngiven si\n0 and a minimal number of timesteps t, we can then output an optimal policy πi\nθ with regard\nto maximizing the cumulative discounted reward PT −1\nt=0 γtr(st, at) for T-length episodes, where θ\ndenotes the parameters for personalized policy πi taking action at ∈A given state st ∈S.\n2.2\nMeta-reinforcement learning\nIn the typical meta-learning scenario, we are interested in automatically learning learning algorithms\nthat are more efﬁcient and effective than learning from scratch Clavera et al. [2018]. To do so, instead\nof looking at individual data points, a meta-learning model is trained on a set of tasks, treating entire\ntasks as training examples with the goal being to quickly adapt to new tasks using only a small\nnumber of examples at test time. We can consider a task to be any type of desirable machine learning\nactivity, such as correctly classifying cat images in a supervised learning scenario, or learning how to\nwalk forwards at a certain velocity in RL. Expanding on this latter example, meta-RL tries to learn a\npolicy for new tasks using only a small amount of experience in the test setting. More formally, each\nRL task τi contains an initial state distribution si\n0 and transition distribution P i(st+1|st, at). In the\nfew-shot RL setting parameterized by K shots, after training on train tasks {τtrain}, we are allowed to\nuse K rollouts on test task τtest, trajectory (s0, a0, . . . sT ), and rewards R(st, at) for adapting to τtest.\n2.3\nModel-agnostic meta-learning for RL\nMore speciﬁcally, our approach builds on model-agnostic meta-learning (MAML) for reinforcement\nlearning Finn et al. [2017], which optimizes over the average reward of multiple rollouts. Doing\nso we wish to learn an initial set of policy gradient parameters capable of quickly adapting to a\n2\nnew RL task in a few gradient descent steps. Given n ∈N meta-training environments, and initial\npolicy parameters θ, MAML ﬁrst samples an MDP Mi, i ∈{1, 2, . . . , n}. For each sampled\nMDP, we then train a policy given K rollouts, arriving at task-adapted parameters θ′\ni through\ngradient descent θ′\ni = θ −α∇θLTi(πθ) (described as the inner training loop). α is our learning\nrate and LTi(π) describes the inverse reward from following policy π with entity type Ti. Feedback\nthrough the reward generated on the (K + 1)st rollout is then saved. After n tasks are sampled,\nwe perform a meta-update using each saved reward, updating our initialization parameters θ′ with\nθ′ ←θ −β∇θ\nP\nTi∼p(T ) LTi(πθ′\ni), where β is the meta-learning rate. MAML thus seeks to leverage\nthe gradient updates for each task during the inner loop of training, reaching a point in parameter\nspace relatively close to the optimal parameters of all potential tasks. We then evaluate performance\nby sampling Mj, j ∈N \\ n, observing the rewards generated from the (K + 1)st trajectory after\nadapting with K rollouts and policy parameters θ′.\n2.4\nCharacterizing policy divergence\nAside from gaining further knowledge with respect to how policies adapt to their environments over\ntime, we also seek to establish a valid distance metric between various policies to inform policy\nparameter initialization in few-shot RL. Conceptually we can imagine that given pairs of MDPs and\npolicies (Mi, πi) for types Ti ∈T , the policies start with an indistinguishable parameter initialization\nand diverge when adapting to their personalized environments after t timesteps of training. If adapted\npolicies π′\ni and π′\nj are still similar to each other, then this suggests that their environments Mi and\nMj may be similar as well, and accordingly we can use this information to form a representation of\noptimal policy parameter clusters, making sure to only reference relevant prior experiences when\nadapting to a new environment.\nTo begin our search for such a distance metric we look to the ﬁeld of imitation learning, and\nspeciﬁcally inverse reinforcement learning (IRL). Here the goal is to learn a cost function explaining\nan expert policy’s behavior Ng and Russell [2000]. However, while typical IRL methods ﬁrst learn\nthis cost function and then optimize against it to train a policy to imitate the expert, we are content\nwith the cost function alone, whose components we can borrow to deﬁne our own quantiﬁable metric\ndeﬁning the distance between two policies. As noted in Ho and Ermon [2016], we can uniquely\ncharacterize a policy by its occupancy measure given by\nρπ(s, a) = π(a|s)\nT\nX\nt=1\nP(St = s|π), where ρπ : S × A →R\n(1)\nWe essentially view this metric as the state-action distribution for a policy. Using a symmetric\nmeasure such as the Jensen-Shannon (JS) divergence Lin [1991], we can thus compare the occupancy\nmeasures of multiple policies at speciﬁc time steps over training as a metric into policy divergence\nover time. While we study a very different problem from traditional IRL–where we do not explicitly\ntry to minimize policy divergence–such measures of divergence allow for both descriptive analysis in\ntracking divergence and informing new learning algorithms. In principle we can then characterize\npolicies based on their observed trajectories, and later show that this is enough to measure similarities\nacross their respective environments as well.\n3\nRelated Work\nWhile there is much work summarizing the general meta-learning literature Clavera et al. [2018],\nHsu et al. [2018], Ren et al. [2018], we focus on initialization-based methods for RL. Here we wish\nto learn a set of optimal model parameters such that after meta-learning, the policy is initialized\nto an optimal position in parameter space to adapt to new environments relatively quickly. Finn\net al. demonstrate this paradigm in MAML, which achieves meta-reinforcement learning through\ngradient-based optimization Finn et al. [2017]. They introduce the generalizable notion of doing well\nacross a variety of tasks, where each task in the RL setting is a similar goal such as trying to reach a\ncertain point in 2D space. Given training on reaching a certain support set of points, we would like\nto be able to quickly learn how to reach a new batch of unseen query points. Instead of trying to\ngeneralize across strictly different tasks, we adopt this framework by modeling different entity types\nencountered−each harboring their own potentially unique rewards and transition probabilities given\nthe same states and actions−as different tasks.\n3\nImplementation-wise, although modern day software packages make MAML’s computation relatively\nstraightforward, it’s reliance on unrolling the computation graph behind gradient descent and taking\nsecond derivatives has motivated follow-on work on simpler methods, such as ﬁrst-order MAML\nand Reptile, which avoid doing so with ﬁrst-order approximations and direct gradient movements\nrespectively Finn et al. [2017], Nichol et al. [2018]. We focus on learning algorithms with similar\ncomputational dependencies due to their comparable performance with vanilla MAML.\nLearning similarities between tasks through clustering or other distance-based metrics has also\nbeen explored before in non-RL few-shot settings, where a central motivation lies in being able\nto achieve comparable results with alternatives while maintaining much simpler inductive biases.\nSnell et al. propose prototypical networks for few-shot classiﬁcation Snell et al. [2017], which\naims to learn an embedding where data points cluster around a single prototype representation in\neach class. Accordingly, given a learned transformation between input features and some vector\nspace, we can simply use the computed prototypes on unseen data points for classiﬁcation. Although\nwe do not explore learning further embeddings on our proposed distance metric, we nonetheless\ndemonstrate that calculating similarities given observed occupancy measures is enough to identify\nsimilar environments, drawing upon a similar intuition for RL.\n4\nPersonalizing policy learning\nWe now present our approach for (1) measuring and interpreting divergence of policies in personaliza-\ntion, and (2) deﬁning few-shot meta-learning algorithms for online adaptation to new entity types. We\nalso introduce a testbed to study the effects of personalization across entities on training. As outlined\nin Section 2.1, we work with a set of personalized MDPs representing a population of entity types.\n4.1\nA personalized particle environment for 2D navigation\nFigure 1: 2D personalized particles. Policy π\ntries to move entity (blue, red, yellow) to tar-\nget (gray), but entities behave in different ways\nunknown to the policy. Right: Two remapping\nschemes, where some cardinal direction e.g. ‘up’\ncorresponds to a different transition vector. Code\navailable upon request.\nWe begin our study of divergence in personal-\nizing policies with a 2D continuous episodic\ngridworld environment, where a point agent\nmust move to a target position in a constrained\namount of time. The state space is given by\nthe agent’s current 2D position, and at each\ntimestep the agent may choose to go left, right,\nup, or down one pixel unit. Following these\ncommands, the agent tries to maximize its re-\nward, calculated to be the negative squared dis-\ntance to the goal after each timestep. Finally\nepisodes end every 100 timesteps, or when the\nagent is within 0.01 units of the goal. Parti-\ncles start at origin (0, 0), and try to reach target\nat (1, 1) before the episode ends. S is deﬁned\nby the coordinates of the particle at timestep\nt ∈{0, 1, . . . T}. Actions are discrete a, corre-\nsponding to moving one unit left, right, down,\nup, i.e. at ∈{(−1, 0), (1, 0), (0, −1), (0, 1)}.\nWe personalize this environment with the introduction of a population of entities, where we may\neither stick with one throughout the policy’s entire training run or randomly introduce new entities\ninto the world at the start of every episode. Each agent is initialized with a personalization function\nF : A →A′, which ﬁrst remaps the cardinal directions of each action and then imposes additional\nvariance. For example, given default action ar = (1, 0), telling a default agent to go right, our\npolicy may encounter an agent who upon receiving action input (1, 0) actually moves with action\n(0.42, −1). This is achieved by ﬁrst remapping ar to (0, −1), and then imposing further variance\non the x-coordinate. We can think of this as having the same set of interventions for all agents, but\nfacing different responses in the form of varied actual transition outcomes.\n4\n4.2\nPractical divergence estimation\nAs introduced in Section 2, by characterizing policies through their occupancy measures, we derive a\nsimilarity metric and quantitatively analyze their divergence. Furthermore, individual policies may\nmove closer or further to others over time, forming new neighbors that reﬂect personalization over\ntime. However, as discussed in [Ho and Ermon, 2016], simply using the occupancy measure as\npresented and matching on all states and actions is not practically useful. Instead of looking at an\noccupancy measure deﬁned over all states as above then, we propose an alternative measure reliant\nonly on a sampled subset of states. We observe that two policies may intuitively be different if for\neach of various given states, they differ in their action distributions. However, the states must have a\nnon-zero chance of occurring in both policies of interest. Accordingly, in the K-shot N-ways setting,\nwhere for each of N MDPs Mi we get K trajectories {τ 1\ni , . . . , τ K\ni } to observe before updating and\nan additional trajectory τ ′\ni after the update, we can calculate a kernel density estimate (KDE) ˆq(τ1:N)\nbased on the observed trajectories τ ′\n1:N = {τ ′\n1, τ ′\n2, . . . τ ′\nN}. States ˆsj ∼ˆq(τ ′\n1:N) can then be sampled,\nwith probability density ˆq(τ ′\n1:N)(ˆsj).\nReferring back to (1), for any policy πi, we estimate the conditional probability π(·|s) by feeding\nin ˆsj to our model network and obtaining the probability vector π′\ni(ˆsj) = (pj\n1, . . . , pj\nn) where pj\na\ndenotes the probability of taking action a given state sj. Towards estimating PT\nt=1 P(St = s|π), we\nagain calculate the KDE over our observed trajectories, but now only consider τ ′\ni for MDP Mi.\nAccordingly, for any set of policies we ﬁrst obtain a respective observed batch of trajectories, calculate\nan estimated state-marginalized variant of occupancy measure ˆρi\ns for s ∈S and entity types Ti, and\ncompute pairwise symmetric divergences across entity types. In our case we use the Jensen-Shannon\ndivergence Lin [1991].\nDJS(ρi\ns, ρj\ns) ≜DKL\n\u0010\nρi\ns\n\f\f\f\f ρi\ns + ρj\ns\n2\n\u0011\n+ DKL\n\u0010\nρj\ns\n\f\f\f\f ρi\ns + ρj\ns\n2\n\u0011\nfor i ̸= j\n(2)\nwhere DKL(·, ·) is the Kullback–Leibler divergence. As a ﬁnal measure, we employ K-medoids\nclustering to group our policies using the pairwise DJS(·, ·) as a distance metric. The overall\nprocedure is summarized in Algorithm 1. Besides describing our policies, we use these metrics to\ninform algorithms for few-shot learning, as described in the next section.\nAlgorithm 1 Policy Divergence Estimation through Observed Trajectories\nRequire: Distribution over entity types p(T ), initial policies πi\nθ ∈Π and episode horizon T\nRequire: Initialize update counter t = 0, and batch size n for computing estimates\n1: while not done do\n2:\nSample entity type Ti ∼p(T ) and initialize new policy πi\nθ\n3:\nSample K trajectories τ i = {s1, a1, . . . , sT } following πi\nθ\n4:\nUpdate θ′ through vanilla policy gradient with REINFORCE Sutton et al. [2000]\n5:\nSave trajectory τ ′\ni = {s1, a1, . . . , sT } using updated πi\nθ′\n6:\nIncrement t ←t + 1\n7:\nif t = n then\n8:\nCompute type-speciﬁc KDE ˆq(τ ′\ni) for all saved τ\n′\ni\n9:\nObtain concatenated updated trajectories τ ′\n1:N = {τ ′\n1, . . . , τ ′\nN}\n10:\nCompute overall KDE ˆq(τ1:N) and generate samples s ∼ˆq(τ1:N)\n11:\nfor all s do\n12:\nCalculate πi(a|s) for all policies πi and compute state probability densities ˆq(τ ′\ni)(s)\n13:\nEstimate ˆρi\ns = πi(a|s)ˆq(τ ′\ni)(s) for given state s\n14:\nend for\n15:\nCompute time-indexed pairwise Jensen-Shannon divergences Djs(ˆρi\ns, ˆρj\ns) for all Ti, Tj ∈T\n16:\nObtain divergence metric P\ns∼ˆq(τ ′\n1:N) Djs(ˆρi\ns, ˆρj\ns)ˆq(τ ′\n1:N)(s)\n17:\nReset t = 0\n18:\nend if\n19: end while\n5\n4.3\nCluster-adapting meta-learning\nGiven our policy divergence estimators, we hope to better organize our previous experiences for\nadaptation in new environments. In this section, we describe a K-medoids-inspired meta-learning\nalgorithm called cluster-adapting meta-learning (CAML). Similar to serial versions of meta-learning\nalgorithms such as MAML, CAML iteratively learns an initialization for parameters of a neural\nnetwork model, such that given new MDP Mi, after a few trajectory rollouts and a single batch update\nduring test time, our policy performs competitively to those pretrained on the same environment.\nHowever, while previous algorithms update a single set of optimal parameters drawn from parameter\nspace, CAML maintains parameters representative of larger clusters.\nIn setting up CAML, we consider the following design choices: (1) a valid distance metric to compare\npast trajectories and their corresponding policies in a model-free setting, (2) the clustering algorithm\nto organize past experiences, and (3) an initialization method to select the most appropriate policy\nparameters at test time. We consider (1) to be the most important contribution, and included details\nin Section 4.2. As described, we implement (2) using the K-medoids algorithm Kaufmann [1987],\nalthough in practice any clustering method may be used. Finally with regard to (3) at the end of our\ntraining iterations we obtain k medoid policies, and during evaluation we view fast-adaptation to\neach individual MDP as a bandit problem. Given k available arms (the policies) and K arm pulls (the\nfew number of shots, i.e. episodes allowed), we want to maximize the corresponding reward from\nfollowing the arm policy for an entire episode. While more sophisticated multi-armed bandit methods\nexist Merentitis et al. [2019], we found that simply sampling policies and saving their cumulative\nassociated rewards for each rollout r ∈{1, 2, . . . , K}−then initializing with the medoid policy\nparameters corresponding to the highest reward−performed competitively against baselines. The\nfull method is described in Algorithm 2. Following these considerations, we found that in few-shot\nsettings (K = 10 trajectory rollouts) CAML performs favorably to alternatives.\nAlgorithm 2 Training with CAML (K-medoids version)\nRequire: Distribution over entity types p(T ), initial policy parameters θ, episode length T\nRequire: Number of medoids k, batch size to cluster with n\n1: for initial iterations 1, . . . , n do\n2:\nSample entity type Ti ∼p(T ) and initialize new policy πi\nθ\n3:\nObtain and save updated πi\nθ′ and τ ′\ni through Steps 3, 4, 5 in Algorithm 1\n4: end for\n5: Compute distance matrix D with pairwise distances on the saved τ ′\ni using Algorithm 1.\n6: Perform K-medoids clustering on D, saving set of k corresponding medoid policies Πk\nθ and\ntrajectories τ k\n1:N. Save trajectories τ k ∈τ k\n1:N to compute next iteration of occupancy measures.\n7: for iterations n + 1, n + 2 . . . do\n8:\nSample entity type Ti ∼p(T ) and randomly sample medoid policy πθ ∈Πk\n9:\nObtain and save updated πi\nθ′ and τ ′\ni through Steps 3, 4, 5 in Algorithm 1\n10:\nif number of saved policies equals n then\n11:\nRepeat steps 5 and 6, updating k medoid policies and trajectories.\n12:\nend if\n13: end for\n5\nExperiments\nIn our experimental evaluation, we validate if: (1) clustering on our occupancy measure estimate\nreasonably characterizes divergence of policies over time, (2) our environments suggest that per-\nsonalization is necessary for fast adaptation, and (3) building on these metrics can inspire effective\nfew-shot learning algorithms. To focus on the effect of our algorithm we use vanilla policy gradient\n(VPG) for both meta-learning and evaluation.\n5.1\nMatching policy divergence to training environments\nTowards evaluating both the dynamics of our testbed and the strength of our policy divergence\ndistance metric, we ﬁrst sought to see if clustering on trained VPG trajectories could recover their\n6\nFigure 2: Policy divergence over time. T-SNE visualization of policy divergences over 40 training\nupdates. Colors denote original latent group. Computing pairwise distances using our divergence\nmetric leads to noticeable grouping over time.\ncorresponding initial environmental dynamics. To do so, we ﬁrst initiated six latent entity types by\nremapping controls completely (e.g. the yellow particle in Figure 1). Afterwards, for each latent\ntype, we generated four variants by further introducing uniform variance for given entity and action\nwhile preserving the deﬁning cardinal direction (red particle in Figure 1). In total we ended up with\n24 entity types. To eludidate divergence across optimal policies, for each type we then trained an\nindividual VPG policy from scratch over 40 updates performing a batch parameter update every 10\niterations, and calculating estimated occupancy-measure-based distances according to Algorithm 1.\nAs observed in Figure 2, based on estimated occupancy measure, trained policies show increasing\nsigns of neighboring with members of their respective groups over training updates. One takeaway\nis that our occupancy measure-based divergence is effective at measuring policy divergence, where\ndirected through their respective environments, we can thus evaluate how different two policies are\nbased on their trajectories. Accordingly, in the few-shot RL setting, one valid strategy for producing\nan optimal policy without having to train from scratch would be to identify prior expert policies\ntrained on similar environments. Taking this one step further, because our occupancy measures\ndirectly derive from π(a|s) which is itself derived from a policy network’s parameters, this provides\nanother interpretation into the effectiveness of ﬁnding nearby parameters in policy space.\n5.2\nEvaluating fast adaptation to personalized environments\nGiven training on a set support types T S, our learning algorithms should quickly perform well on\nunseen query type set T Q. In-line with K-shot RL, for each support type T S\ni\n∈T S, our policies are\nallowed K rollouts for adaptation during each training iteration. Towards evaluation, for each learning\nalgorithm we train with 100 iterations using its policy-speciﬁc training algorithm. All underlying\ngradients were calculated with VPG, updating with batch size 10. For evaluation, due to the on-policy\nnature of policy gradient, we ﬁrst allow K rollouts to collect new samples from unseen type T Q\ni\n∈T .\nAllocating these trajectories as an initialization period, for each T Q\ni\nwe then evaluate fast adaptation\nby comparing rewards across policies for up to ﬁve updates with ﬁne-tuning via VPG.\nWe compare CAML to ﬁve other methods: (1) pretraining a policy by randomly sampling from\nall support environments during training, similar to joint training Yu et al. [2017], (2) pretraining\na policy in the same query environment that we use for evaluation, (3) pretraining a policy in a\ndifferent environment, (4) training with Reptile, a meta-learning algorithm comparable to MAML\nin performance but much simpler in computation that performs K > 1 steps of stochastic gradient\nFigure 3: Evaluation of learning algorithms on personalized environments. CAML performs\ncomparatively well across various evaluation entity types.\n7\nFigure 4: Performance on 2D navigation. Qualitative comparisons marking the ending location of\nparticles at the end of episodes for the ﬁrst evaluated type, with attention to performance after initial-\nization and a few updates. Only the matched pretrained model and CAML reach positions noticeably\ncloser to the target coordinate (1, 1) in 5 updates (left). Visualization of example trajectories taken by\npretrained and random policies (right).\ndescent for randomly sampled support environments before directly moving initialization weights in\nthe direction of the weights obtained during SGD Nichol et al. [2018], and (5) randomly initializing\nweights.\nThe results in Figure 3 suggest that CAML is able to quickly adapt to unseen entity types after\none update during evaluation. We note that given three different environments, CAML consistently\noutperforms a support type-pretrained policy and a randomly-initialized policy, and also seems to\noutperform Reptile to varying degrees as well. The results also hint at differences in the environments\ndynamics. For example, entities with evaluation types 1 or 2 demonstrate instances where jointly\ntrained and models pretrained on different environments initialize in poor states, suggesting that they\nrequire heavier personalization for success. On the other hand, evaluation type 3 represents a scenario\nwhere entity types may be representative of the larger population. While the matched pretrained\npolicy serves as an upper bound in performance for all cases, we observe that the joint trained and\nunmatched pretrained policy model also seem to effectively transfer to the new environment.\nLastly we acknowledge the potential challenges of pretraining to personalized environments. Observ-\ning the ending coordinates of various policies in Figure 4, we note that in certain situations, such\nas when the query environment is not representative of a policy’s past experiences, methods such\nas joint training or pretraining with another environment may initiate detrimental behavior such as\nmoving in the wrong direction.\n6\nDiscussion and Future Work\nWe tackle meta-learning for fast adaptation in personalized environments, where environments\nmay share the same reward, but differ dramatically in state transition probabilities. Using ideas\nfrom characterizing policy divergence, we introduce competitive improvements to a promising\nmeta-learning algorithm paradigm, better initializing to a set of optimal policy parameters by better\norganizing past experiences into relevant clusters. While we believe further baselines and experiments\nare needed, our results also show preliminary support that meta-learning to minimize total distance\nbetween all potential optimal parameters may be sub-optimal when trying to adapt to diverse-enough\nenvironments. Our method builds on this original motivating curiosity, and suggests one possible\nalternative to better approach personalized reinforcement learning.\n8\nReferences\nAndrew G Barto, Richard S Sutton, and Christopher JCH Watkins. Sequential decision problems and\nneural networks. In Advances in neural information processing systems, pages 686–693, 1990.\nIgnasi Clavera, Anusha Nagabandi, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea\nFinn. Learning to adapt: Meta-learning for model-based control. CoRR, abs/1803.11347, 2018.\nURL http://arxiv.org/abs/1803.11347.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. CoRR, abs/1703.03400, 2017. URL http://arxiv.org/abs/1703.03400.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. CoRR, abs/1606.03476,\n2016. URL http://arxiv.org/abs/1606.03476.\nKyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. CoRR,\nabs/1810.02334, 2018. URL http://arxiv.org/abs/1810.02334.\nLeonard Kaufmann. Clustering by means of medoids. In Proc. Statistical Data Analysis Based on\nthe L1 Norm Conference, Neuchatel, 1987, pages 405–416, 1987.\nJianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information\nTheory, 37(1):145–151, January 1991. ISSN 0018-9448. doi: 10.1109/18.61115. URL http:\n//ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=61115.\nAndreas Merentitis, Kashif Rasul, Roland Vollgraf, Abdul-Saboor Sheikh, and Urs Bergmann. A\nbandit framework for optimal selection of reinforcement learning agents. CoRR, abs/1902.03657,\n2019. URL http://arxiv.org/abs/1902.03657.\nAnusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning:\nContinual adaptation for model-based RL. CoRR, abs/1812.07671, 2018. URL http://arxiv.\norg/abs/1812.07671.\nAndrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In Proceedings\nof the Seventeenth International Conference on Machine Learning, ICML ’00, pages 663–670,\nSan Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860-707-2. URL\nhttp://dl.acm.org/citation.cfm?id=645529.657801.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. CoRR,\nabs/1803.02999, 2018. URL http://arxiv.org/abs/1803.02999.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum,\nHugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classiﬁcation.\nCoRR, abs/1803.00676, 2018. URL http://arxiv.org/abs/1803.00676.\nSusan M Shortreed, Eric Laber, Daniel J Lizotte, T Scott Stroup, Joelle Pineau, and Susan A Murphy.\nInforming sequential clinical decision-making through reinforcement learning: an empirical study.\nMachine learning, 84(1-2):109–136, 2011.\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning.\nCoRR, abs/1703.05175, 2017. URL http://arxiv.org/abs/1703.05175.\nRichard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour.\nPolicy\ngradient methods for reinforcement learning with function approximation.\nIn S. A.\nSolla, T. K. Leen, and K. Müller, editors, Advances in Neural Information Processing\nSystems 12, pages 1057–1063. MIT Press, 2000.\nURL http://papers.nips.cc/paper/\n1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.\npdf.\nWenhao Yu, Greg Turk, and C. Karen Liu. Multi-task learning with gradient guided policy specializa-\ntion. CoRR, abs/1709.07979, 2017. URL http://arxiv.org/abs/1709.07979.\n9\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-10-09",
  "updated": "2020-10-09"
}