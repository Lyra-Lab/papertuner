{
  "id": "http://arxiv.org/abs/2010.05125v2",
  "title": "Learning Task-aware Robust Deep Learning Systems",
  "authors": [
    "Keji Han",
    "Yun Li",
    "Xianzhong Long",
    "Yao Ge"
  ],
  "abstract": "Many works demonstrate that deep learning system is vulnerable to adversarial\nattack. A deep learning system consists of two parts: the deep learning task\nand the deep model. Nowadays, most existing works investigate the impact of the\ndeep model on robustness of deep learning systems, ignoring the impact of the\nlearning task. In this paper, we adopt the binary and interval label encoding\nstrategy to redefine the classification task and design corresponding loss to\nimprove robustness of the deep learning system. Our method can be viewed as\nimproving the robustness of deep learning systems from both the learning task\nand deep model. Experimental results demonstrate that our learning task-aware\nmethod is much more robust than traditional classification while retaining the\naccuracy.",
  "text": "Learning Task-aware Robust Deep Learning Systems\n1,2Keji Han, 1,2Yun Li*, 1,2Xianzhong Long, 1,2Yao Ge,\n1Nanjing University of Posts and Telecommunications\n2Jiangsu Key Laboratory of Big Data Security & Intelligent Processing\nliyun@njupt.edu.cn\nAbstract\nMany works demonstrate that deep learning system is vulner-\nable to adversarial attack. A deep learning system consists of\ntwo parts: the deep learning task and the deep model. Nowa-\ndays, most existing works investigate the impact of the deep\nmodel on robustness of deep learning systems, ignoring the\nimpact of the learning task. In this paper, we adopt the binary\nand interval label encoding strategy to redeﬁne the classiﬁca-\ntion task and design corresponding loss to improve robustness\nof the deep learning system. Our method can be viewed as im-\nproving the robustness of deep learning systems from both the\nlearning task and deep model. Experimental results demon-\nstrate that our learning task-aware method is much more ro-\nbust than traditional classiﬁcation while retaining the accu-\nracy.\nIntroduction\nDeep neural networks have been applied to many learning\ntasks, such as image recognition, speech recognition, and\nmachine translation (LeCun, Bengio, and Hinton 2015).\nWith increasing real-world applications, the robustness of\ndeep neural networks arouses increasing attention from\nboth academia and industry. Deep neural networks are\ndemonstrated to be vulnerable to the adversarial example\n(Szegedy et al. 2014). The adversarial example is crafted\nby adding imperceptible adversarial perturbation to the\noriginal legitimate example. In essence, the existence of\nadversarial examples is rooted in the difference between\nhuman intelligence and machine intelligence (Cao et al.\n2021).\nThere are many adversarial attack methods proposed\nto explore the vulnerability of deep learning systems.\nAccording to the phase that adversarial attack happens, ad-\nversarial attack methods fall into two categories: poisoning\nattack and evasion attack (Yuan et al. 2019). In this paper,\nwe focus on the evasion attack. According to manner to\ncraft adversarial examples, existing evasion attack methods\ncan be divided into two categories, namely single-step\nattack and multi-step attack. A single-step attack explores\nadversarial perturbation in one step or directly maps the\noriginal example as an adversarial example. Multi-step\n*Corresponding Author\nFor Preview\nattack explores adversarial perturbation iteratively. For\nthe single-step attack, FGSM (Goodfellow, Shlens, and\nSzegedy 2015) crafts the adversarial example with the\ngradient sign; AdvGAN (Xiao et al. 2018) and ATN (Baluja\nand Fischer 2018) directly map the legitimate example\nas an adversarial example. As to the multi-step attack,\nPGD (Kurakin, Goodfellow, and Bengio 2017) crafts the\nadversarial example in an iterative way; CW (Carlini and\nWagner 2017) and Deepfool (Moosavi-Dezfooli, Fawzi,\nand Frossard 2016) formulate the attack as an optimization\nproblem then solve it in an iterative way.\nExisting methods to improve the robustness of deep\nlearning systems focus on even all elements related\nto the deep model, such as the training data/features,\nthe model architecture, training loss/regularization loss,\nand parameter-updating strategy. For training data-level\nmethods, feature nulliﬁcation (Wang et al. 2017), image\ncompress (Das et al. 2018), and feature squeezing (Xu,\nEvans, and Qi 2018) are demonstrated to be efﬁcient to\nimprove the robustness of the deep learning system. As to\nthe model architecture-level method, the denoising module\n(Xie et al. 2019), Euler skip connection (Li, He, and Lin\n2020), and additional batch normalization module (Xie\nand Yuille 2020) are introduced as additional modules\nto improve the robustness of the deep model. When it\ncomes to training loss-based methods, adversarial training\nmethods, such as TRADES (Zhang et al. 2019) and MART\n(Wang et al. 2020), introduce regularization loss. For the\nparameter-updating strategy methods, Reluplex (Katz et al.\n2017) updates model parameters with the simplex-like\nmethod. Some generative classiﬁers (Lee et al. 2019) update\nthe parameter of deep model with Bayesian Backpropaga-\ntion (BBP) (Blundell et al. 2015). Moreover, the adversarial\nexample detection methods (Meng and Chen 2017; Ma\net al. 2019) can keep the adversarial example away from the\ndeep model, which is also demonstrated to be efﬁcient in\nimproving the robustness of the deep learning system.\nActually, a deep learning system consists of the learn-\ning task and a deep model. The learning task provides a\nformalized deﬁnition of the problem, while the deep model\nimplements the learning task. So we can also improve the\nrobustness of the deep learning system by deﬁning robust\narXiv:2010.05125v2  [cs.LG]  2 Dec 2021\nlearning tasks. The traditional classiﬁcation is so simplistic\nthat the model can be lazy. It just needs to remember instead\nof learning (Feldman and Zhang 2020). Moreover, in the\ntraditional classiﬁcation, the accuracy and robustness are\nat odds (Tsipras et al. 2019). Above all, the learning task-\naware method may be promising to address the adversarial\nissue for deep learning systems.\nMoreover, there are some works that focus on the\nlearning task to improve the accuracy of the deep learning\nsystem. For instance, DeepBE (Li et al. 2016) adopts\nbinary encoding labels to improve the accuracy of the\ndeep learning system. In this paper, we explore methods\nthat improve the robustness of the deep learning system\nfrom both the learning task and model loss prospects. In\ndetail, we introduce robust binary-label classiﬁcation with\na scaling factor to improve the accuracy and robustness.\nMoreover, we deﬁne the interval-label classiﬁcation, which\nmarks inputs with predeﬁned nonoverlapping intervals.\nOur contributions can be summarized as follows.\n• We introduce task-aware robust deep learning systems\nbased on binary-label and interval-label classiﬁcations;\n• Experimental results demonstrate that our method\nachieves a lower adversarial transfer rate than traditional\nclassiﬁcation;\n• Both analysis and experimental results demonstrate that\nthe accuracy and robustness are no more at odds for the\nrobust binary-label classiﬁcation;\n• We ﬁnd that the adversarial training does not constantly\nimprove the robustness of robust binary-label classiﬁca-\ntion, demonstrating that learning task affects the adver-\nsarial robustness.\nInterval-label and Robust Binary-label\nclassiﬁcation Systems\nMost existing works to improve the robustness of the deep\nlearning system focus on the deep model. As introduced\nin (Cao et al. 2021), the adversarial example has roots in\nthe difference between machine intelligence and human\nintelligence. We think that the decision process of the deep\nlearning task may also impact the robustness of the deep\nlearning system. That is another motivation behind propos-\ning robust binary-label and interval-label classiﬁcation\nsystems.\nA robust classiﬁcation system consists of three mod-\nules: the classiﬁer, label encoding module, and label\ndecoding module, as shown in Fig. 1. The classiﬁer maps\ninputs into the corresponding output vectors then turns\noutput vectors as codewords. The codewords (Dietterich\nand Bakiri 1995) is the encoding of decimal labels, i.e.,\nhard labels. Label encoding translates hard labels as cor-\nresponding codewords, while the label decoding module\ntranslates the codewords provided by the classiﬁer as the\nhard labels, namely the predicted label. If the codeword do\nnot correspond to any predeﬁned hard label, the input will\nbe marked as None, i.e., an abnormal example.\nLabel Encoding\nLabel Decoding\n0\nNone\nClassifier\n•\nLabel Encoding: mapping hard \nlabels as corresponding \ncodewords.\n•\nLabel Decoding: mapping the \ncodewords as hard labels. \nInput: Example\nInput: Hard Label\n0\nOutput Vector\nCodeword\nFigure 1: The workﬂow of the learning task-aware robust\ndeep classiﬁcation system.\nDeﬁnition\nTraditional Classiﬁcation: the traditional deep classiﬁcation\ncan be deﬁned as follows: ∀x ∈RD, T : x →RC, where x\nis an example, D is the dimension of x. C is the number of\ncategories.\nInterval-label Classiﬁcation: the traditional deep clas-\nsiﬁcation can be deﬁned as follows: ∀x ∈RD, T : x →R1,\nwhere x is an example, D is the dimension of x. It prede-\nﬁnes some nonoverlapping intervals. The interval that the\nmodel output falls is corresponding interval label. If the\noutput does not fall into any predeﬁend intervals, the input\nwill be marked as an anomaly example.\nBinary-label Classiﬁcation: the binary-label classiﬁca-\ntion can be deﬁned as follows: ∀x ∈RD, T : x →RB,\nwhere x is an example, D is the dimension of x. B is the\nnumber of bits in the binary label.\nThe decision process of traditional classiﬁcation is different\nfrom the binary-label and interval-label classiﬁcations.\nThe traditional classiﬁcation maximizes the probability\ncorresponding to the ground-truth label. The binary-label\nclassiﬁcation focuses on each element’s sign for the output\nvector. The interval-label classiﬁcation tries to decide the\ninterval that the output falls into. Generally speaking,\nthe traditional classiﬁcation decides the category that the\ninput most likely belongs to, while the binary-label and\ninterval-label classiﬁcations have to decide the category that\nthe input exactly belongs to. The following paragraphs will\nintroduce details of robust deep classiﬁcation systems based\non robust binary-label and interval-label classiﬁcations.\nImplementation Details of Interval-label\nClassiﬁcation System\nAs introduced above, the interval-label classiﬁcation system\nconsists of three modules: the interval-label classiﬁer,\nlabel encoding module, and label decoding module. The\ninterval-label classiﬁer can be a deep neural network whose\noutput is a scalar. The label encoding turns the hard label\ninto an interval label, namely codeword, while the label\ndecoding translates the output of the classiﬁer as a hard\nlabel.\nLabel Encoding module of interval-label classiﬁcation\nsystem transforms the hard label into an interval label. The\nmap function can be formulated as follows.\nL(y) = s0 + y · (α + β);\nU(y) = L(y) + β,\nwhere s0 is the smallest lower bound of interval labels.\nα is the length of the gap between two adjacent interval\nlabels, while β is the length of the interval label. ML(·)\nand MU(·) are lower bound and upper bound map function,\nrespectively. According to Eq. (), for the hard label ‘3’,\ns0 = 0, α = 1, β = 3, the corresponding interval label is\n[12, 15].\nThe label decoding function can be formulated as fol-\nlows.\ney = ⌊I(x) −s0\nα + β\n⌋,\nwhere x is the input example. I(·) is the interval-label\nclassiﬁer. ⌊·⌋is the ﬂoor function. If I(x) does not belong\nto any interval label, the corresponding input example will\nviewed as an abnormal example.\nThe\nloss\nfunction\nof\nthe\ninterval-label\nclassiﬁcation\ncan be formulated as follows.\nL(B(X, Y\n′); θ) = ∥r(L(Y ) −I(X))∥2\n2\n|\n{z\n}\nlower bound loss\n+\n∥r(I(X) −U(Y )∥2\n2\n|\n{z\n}\nupper bound loss\n,\nwhere θ is the parameter set of the classiﬁer I. B(X, Y\n′) is\na mini batch. X and Y is the examples set and hard label\nset, and Y\n′ = [L(Y ), U(Y )] is the interval labels set. L(Y )\nand U(Y ) is the lower bound set and upper bound set, re-\nspectively. r(·) is the ReLU (Nair and Hinton 2010) activa-\ntion function. lower bound loss and upper bound loss in\nEq. are the distance between the output and lower and upper\nbounds, respectively.\nImplementation Details of Robust Binary-label\nClassiﬁcation System\nThere are three hyperparameters for interval-label classi-\nﬁcation, which may limit its generalization. So we also\nexplore another classiﬁcation for a robust deep classiﬁca-\ntion system, namely robust binary-label classiﬁcation. As\nintroduced above, a robust binary-label classiﬁcation (RBC)\nsystem involves three modules: the binary-label classiﬁer,\nthe label-encoding module, and the label decoding module.\nThe binary-label classiﬁer is the same as the traditional\nneural network classiﬁer, except for the output dimension.\nThe label-encoding module converts the traditional decimal\nlabel into a binary label, while the label decoding module\nconverts the binary label into a decimal label.\nIn this paper, the binary-label classiﬁer is a deep neu-\nral network. We set the output dimension of the network as\nB, 2B ≥C. C is the number of predeﬁned categories. If\ndenoting the robust binary-label classiﬁer as B, the process\nto get binary label b of the example x can be formulated as\nfollows.\nbi =\n\u001a1, B(x)i > 0;\n0, B(x)i ≤0,\n(0)\nwhere i ∈0, 1, · · · , B −1. bi and B(x) are i-th element of\nthe binary label and output of binary-label classiﬁer.\nLabel-encoding and label-decoding modules are intro-\nduced for conversion between the decimal and binary labels.\nThe loss function of the RBC can be formulated as\nfollows.\nL(x, b; θ) = ∥r(S · 1 −B(x) · (2b −1))∥2\n2,\n(0)\nwhere x and b are the input example and its correspond-\ning binary label, respectively. θ is the parameter set of the\nbinary-label classiﬁer. r(·) is the ReLU activation function.\nS scaling factor is a hyperparameter to control margins be-\ntween different categories. 1 is the all-ones vector. In detail,\n∀x and its binary label b, when S = 100, if bi = 0, the loss\nin Eq. will force the B(x)i ≤−100. Otherwise, if bi = 1,\nthe loss in Eq. will force the B(x)i ≥100, i = 0, · · · , B−1.\nIn other words, loss in Eq. scales the margin between ele-\nments in binary label from 1 to 2S.\nExperiment\nExperiment Settings\nIn this paper, two data sets are applied to evaluate the\neffectiveness of our method, namely MNIST (Lecun et al.\n1998), CIFAR-10, and CIFAR-100 (Krizhevsky 2009).\nMNIST is a 10-class grayscale hand-written digit image\ndataset, consisting of 60,000 training examples and 10,000\ntesting examples. CIFAR-10 is a 10-class color image\ndataset, consisting of 50,000 training examples and 10,000\ntesting examples. CIFAR100 is a 100-class color image\ndataset, consisting of 50,000 training examples and 10,000\ntesting examples.\nFor adversarial robustness evaluation, FGSM and PGD\nare adopted. FGSM is a classical single-step attack, while\nPGD is a multi-step attack. The two attacks are adopted\nfor the reason that they can conveniently control the attack\nintensity. Moreover, since the decision process of the\nrobust binary-label and interval-label classiﬁcations are\ndifferent from the traditional classiﬁcation, the existing\noptimization-based attack can not attack the binary-label\nclassiﬁcation, such as CW and Deepfool. The reason is that\nadversary can not directly get the probability that the input\nbelongs to a speciﬁc category.\nConvergence of Robust Binary-label Classiﬁcation\nSystem\nIn this subsection, we experiment to investigate the con-\nvergence of robust binary-label classiﬁcation system. We\n0\n20\n40\n60\n80\n100\nTraining Epoch\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nLoss\n95\n96\n97\n98\n99\n100\nAccuracy (%)\nHighest Test Accuracy: 99.49\nLoss\nTrain Acc\nTest Acc\n(a) TRA MNIST\n0\n20\n40\n60\n80\n100\nTraining Epoch\n0\n250\n500\n750\n1000\n1250\n1500\n1750\nLoss\n70\n80\n90\n100\nAccuracy (%)\nHighest Test Accuracy: 99.52\nLoss\nTrain Acc\nTest Acc\n(b) RBC MNIST\n0\n50\n100\n150\n200\nTraining Epoch\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nLoss\n40\n50\n60\n70\n80\n90\n100\nAccuracy (%)\nHighest Test Accuracy: 90.59\nLoss\nTrain Acc\nTest Acc\n(c) TRA CIFAR10\n0\n50\n100\n150\n200\nTraining Epoch\n0\n1000\n2000\n3000\n4000\n5000\nLoss\n20\n40\n60\n80\n100\nAccuracy (%)\nHighest Test Accuracy: 91.63\nLoss\nTrain Acc\nTest Acc\n(d) RBC CIFAR10\n0\n50\n100\n150\n200\nTraining Epoch\n0\n1\n2\n3\n4\nLoss\n20\n40\n60\n80\n100\nAccuracy (%)\nHighest Test Accuracy: 68.85\nLoss\nTrain Acc\nTest Acc\n(e) TRA CIFAR100\n0\n100\n200\n300\n400\nTraining Epoch\n0\n2000\n4000\n6000\n8000\nLoss\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nHighest Test Accuracy: 64.89\nLoss\nTrain Acc\nTest Acc\n(f) RBC CIFAR100\nFigure 2: Comparison of convergence between the tradi-\ntional and robust binary-label classiﬁcations. TRA and RBC\nrepresent the traditional and robust binary-label classiﬁca-\ntion systems, respectively.\ncompare the convergence of the traditional and binary-\nlabel classiﬁcation systems on MNIST, CIFAR10, and\nCIFAR100. B is set as 4 for MNIST and CIFAR10, while\nB is set as 7 for CIFAR100. For the robust binary-label\nclassiﬁcation, the scaling factor S is set as 350 for all\ndatasets. In this paper, EfﬁcientNet is EfﬁcientNetB0\nwith 5.3M parameters (Tan and Le 2019), and ResNet is\nResNet18 with 9.2M parameters (He et al. 2016).\nAs shown in Fig. 2, on MNIST and CIFAR10, robust\nbinary-label classiﬁcation system achieves higher test\naccuracy than the traditional classiﬁcation system. While\non CIFAR100, the binary-label classiﬁcation system con-\nvergences more slowly than the traditional classiﬁcation\nsystem. The reason is that the binary-label classiﬁcation\nis more complex than the traditional classiﬁcation. So the\nbinary-label classiﬁcation system needs a higher model\ncapacity to ﬁt the data distribution. The test accuracies of\ndifferent classiﬁcations with variant models are similar,\nwhich demonstrates that when model capacity is sufﬁcient,\nthe robust binary-label and traditional classiﬁcation systems\ngeneralize similarly. Moreover, that explains why the test\naccuracy of binary-label classiﬁcation system is slightly\nlower than the traditional classiﬁcation system in some\ncases.\nRobustness Evaluation for Different Classiﬁcation\nSystems\nIn this subsection, we experiment to investigate the robust-\nness of different classiﬁcation systems. We compare the\nrobustness of the traditional classiﬁcation, interval-label\nclassiﬁcation, and binary-label classiﬁcations with Efﬁ-\ncientNet and ResNet on MNIST, CIFAR10, and CIFAR100.\nSince FGSM and PGD are easy to control the attack\nintensity, we set three different attack intensities, namely I,\nII, and III. I represents the attack intensity is 0.1 for MNIST\nand 3/255 for CIFAR10 and CIFAR100. II represents the\nattack intensity is 0.2 for MNIST and 6/255 for CIFAR10\nand CIFAR100. III represents the attack intensity is 0.3 for\nMNIST and 9/255 for CIFAR10 and CIFAR100. LEG in\nTable 1 represents the accuracy of the legitimate examples.\nTRA, DeepBE, INT, and RBC represent the traditional\nclassiﬁcation, binary encoding classiﬁcation, interval-label\nclassiﬁcation, and robust binary-label classiﬁcation, respec-\ntively. For the binary-label classiﬁcation, the S is set 350,\nwhile for the interval-label classiﬁcation, the interval label\ninitial point s0 is set as 0, interval label length β is set as 16,\nand the gap length α is set as 4.\nAs shown in Table 1, our method achieves the highest\naccuracies in variant evaluation scenarios. Compared to the\ntraditional and interval-label classiﬁcation, our method can\nbalance the adversarial robustness and accuracy better. We\nalso note that the binary-label and interval-label classiﬁ-\ncations are insensitive to the variant in adversarial attack\nintensity, which is different from the traditional classiﬁca-\ntion. The reason is that the traditional classiﬁcation depends\non the relative magnitude for the values of elements of\nthe output vector, while the binary-label and interval-label\nclassiﬁcation depend on the actual magnitude for the values\nof elements in the outputs vector. Different adversarial\nattack intensities have a variant impact on the relative\nmagnitude for the values of elements in the output vector,\nnamely the logits. However, only when the adversarial\nattack intensity exceeds a speciﬁc threshold, the adversarial\nattack can change the sign of elements in the output vector\nor cause the values of elements in the output vector to vary\nfrom one interval to another.\nWe also experiment to investigate the robustness of our\nmethod combining with the adversarial training or retraining\nwith Gaussian Noise perturbed examples. The adversar-\nial training method in this paper is Madry adversarial\ntraining (Madry et al. 2018). The intensity of adversarial\nexamples and Gaussian Noise for the adversarial training\nand retraining is set as I, as mentioned above. In Table\n2, the TRA+AT and RBC+AT represent the traditional\nclassiﬁcation with Madry adversarial training and robust\nbinary-label classiﬁcation with Madry adversarial training,\nrespectively. RBC+GN represents the robust binary-label\nclassiﬁcation retrained with Gaussian Noise perturbed ex-\namples. ↑represents that corresponding retraining method\nTable 1: Comparison of adversarial robustness and accuracy among different classiﬁcation systems.\nData set\nModel\nMethod\nFGSM\nPGD\nLEG\nI\nII\nIII\nI\nII\nIII\nMNIST\nEfﬁcientNetB0\nTRA\n20.38\n14.03\n8.10\n6.50\n4.54\n2.96\n99.49\nDeepBE\n86.87\n69.89\n48.95\n71.33\n32.69\n11.56\n99.26\nINT\n99.12\n99.12\n99.13\n99.04\n98.71\n99.11\n99.18\nRBC\n99.26\n99.27\n99.26\n99.20\n98.89\n97.90\n99.52\nResNet18\nTRA\n68.52\n14.88\n10.03\n34.59\n4.40\n2.19\n99.65\nDeepBE\n90.26\n56.51\n39.14\n64.54\n9.43\n6.47\n99.51\nINT\n99.25\n99.21\n99.21\n99.16\n98.96\n98.23\n99.25\nRBC\n99.21\n99.22\n99.21\n99.50\n95.19\n74.55\n99.55\nCIFAR10\nEfﬁcientNetB0\nTRA\n62.95\n38.50\n25.55\n61.05\n31.07\n15.12\n90.59\nDeepBE\n52.48\n38.43\n33.64\n51.59\n36.44\n30.19\n76.82\nINT\n80.66\n80.37\n80.46\n80.48\n80.19\n80.30\n83.94\nRBC\n85.43\n85.40\n85.38\n85.52\n85.30\n85.23\n91.63\nResNet18\nTRA\n75.26\n65.15\n52.84\n73.17\n52.13\n38.36\n93.67\nDeepBE\n71.65\n66.64\n62.95\n70.33\n64.37\n60.33\n89.08\nINT\n86.47\n86.55\n86.56\n86.40\n86.55\n86.47\n87.54\nRBC\n90.92\n90.91\n90.94\n90.86\n90.99\n90.93\n93.55\nCIFAR100\nEfﬁcientNetB0\nTRA\n40.86\n22.38\n14.21\n39.39\n17.90\n8.21\n68.85\nDeepBE\n9.80\n5.56\n3.65\n9.76\n5.31\n3.22\n17.90\nINT\n6.10\n6.12\n6.21\n6.04\n6.06\n6.09\n8.24\nRBC\n54.19\n54.12\n54.37\n53.88\n53.76\n53.75\n64.68\nResNet18\nTRA\n46.52\n31.87\n24.06\n42.94\n23.16\n12.37\n72.05\nDeepBE\n16.68\n11.06\n8.76\n16.40\n10.32\n7.98\n27.99\nINT\n17.59\n17.61\n17.65\n17.43\n17.52\n17.62\n20.13\nRBC\n58.33\n58.35\n58.45\n58.35\n58.26\n57.94\n69.09\ncan improve the adversarial robustness, while ↓represents\nthe retraining suppresses the adversarial robustness.\nAs shown in Table 2, the robust binary-label classiﬁcation\nis still more robust than the traditional classiﬁcation, even\nwith the adversarial training. We note that the adversarial\ntraining with a speciﬁc adversarial attack intensity can not\nendow the deep model robustness against variant-intensity\nattacks, especially when the attack intensity is bigger than\nthat in the adversarial training. However, the variant of the\nattack intensity has a limited impact on the robustness of\nthe RBC. Different from the traditional classiﬁcation, we\nnote that adversarial training can not constantly improve\nthe robustness of RBC, but retraining with Gaussian Noise\nperturbed examples can. Here we deﬁne a stability metric\nof robust accuracy.\nR = acca\nacct\n,\n(0)\nwhere acca and acct are robust accuracy and test accuracy,\nwhen adopting speciﬁc defense strategy. R can be employed\nto evaluate the sufﬁciency of defense method. For instance,\nif the R is smaller than 1 for the adversarial training, we\nmay need to increase the training epochs or the attack inten-\nsity. According to Table 2, adversarial training improves the\nrobustness of RBC, when R is abnormally small. However,\nwhen R is large enough, adversarial training will inhibit the\nrobustness of RBC.\nTo further intestigate robustness of variant classiﬁca-\ntion systems, we also experiment to visualize features with\nt-SNE (Maaten and Geoffrey 2008). Since the Efﬁcient-\nNetB0 has been employed as the backbone model for many\nlearning tasks, we apply t-SNE visualization to its penul-\ntimate layer features. We randomly select 1,000 examples\nfrom the MNIST for the visualization. The scaling factor S\nis set as 350 for the robust binary-label classiﬁcation.\nAs shown in Fig. 3, EfﬁcientNetB0 can learn a reasonable\nfeature representation for the traditional, binary-label, and\ninterval label classiﬁcation. There is even no intersection for\nessential features of different examples belong to different\ncategories. Moreover, we note that features of examples\nfrom the same category are distributed in long and narrow\nareas for the robust binary-label and interval-label classiﬁ-\ncation. The reason is that robust binary-label and interval\nlabel classiﬁcation depends on actual values of the outputs,\nand the loss function constraints their outputs into some\nspeciﬁc regions. We also note that the distances between\nfeatures of different categories for robust binary-label and\ninterval-label classiﬁcation are more signiﬁcant than that\nof traditional classiﬁcation or DeepBE. Increasing the\nmargin between the boundaries of different categories can\nimprove the robustness of deep learning systems (Tanay and\nGrifﬁn 2016). That is the reason why robust binary-label\nand interval-label classiﬁcations are more robust than the\ntraditional classiﬁcation.\nFig. 3 also can explain different impacts of adversarial\nTable 2: Comparison of adversarial robustness and accuracy between traditional and robust bianry-label classiﬁcation systems\nwith Madry Adversarial Training.\nData set\nModel\nMethod\nFGSM\nPGD\nLEG\nI\nII\nIII\nI\nII\nIII\nMNIST\nEfﬁcientNetB0\nTRA+AT\n98.48↑\n96.89↑\n88.65↑\n98.37↑\n80.51↑\n10.36↑\n99.44↓\nRBC+AT\n99.21↓\n99.20↓\n99.20↓\n99.19↓\n99.18↑\n99.14↑\n99.54↑\nRBC+GN\n99.34↑\n99.33↑\n99.35↑\n99.36↑\n99.21↑\n98.78↑\n99.44↓\nResNet18\nTRA+AT\n98.97↑\n98.00↑\n85.03↑\n98.98↑\n94.87↑\n30.25↑\n99.57↓\nRBC+AT\n99.04↓\n98.87↓\n98.88↓\n98.96↓\n98.89↑\n98.77↑\n99.53↓\nRBC+GN\n99.30↑\n99.30↑\n99.30↑\n99.33↓\n99.00↑\n94.28↑\n99.56↑\nCIFAR10\nEfﬁcientNetB0\nTRA+AT\n80.33↑\n69.25↑\n58.78↑\n80.30↑\n69.41↑\n57.81↑\n88.58↓\nRBC+AT\n83.31↓\n79.81↓\n79.22↓\n83.25↓\n79.60↓\n79.18↓\n91.30↓\nRBC+GN\n88.43↑\n88.48↑\n88.49↑\n88.46↑\n88.53↑\n88.27↑\n92.09↑\nResNet18\nTRA+AT\n86.81↑\n80.11↑\n72.96↑\n84.74↑\n79.66↑\n71.41↑\n92.63↓\nRBC+AT\n86.94↓\n86.10↓\n86.03↓\n86.92↓\n86.07↓\n85.95↓\n93.29↓\nRBC+GN\n91.75↑\n91.77↑\n91.78↑\n91.72↑\n91.70↑\n91.52↑\n93.78↑\nCIFAR100\nEfﬁcientNetB0\nTRA+AT\n53.66↑\n43.16↑\n34.03↑\n53.63↑\n42.78↑\n33.14↑\n64.70↓\nRBC+AT\n50.91↓\n49.60↓\n49.21↓\n50.52↓\n49.03↓\n48.86↓\n63.36↓\nRBC+GN\n57.30↑\n57.18↑\n57.44↑\n56.85↑\n56.75↑\n56.67↑\n64.30↓\nResNet18\nTRA+AT\n59.07↑\n50.38↑\n42.32↑\n58.87↑\n49.69↑\n40.87↑\n68.96↓\nRBC+AT\n52.81↓\n51.17↓\n51.08↓\n52.54↓\n51.18↓\n51.00↓\n66.94↓\nRBC+GN\n61.22↑\n61.29↑\n61.40↑\n61.24↑\n61.09↑\n60.95↑\n68.56↓\n40\n20\n0\n20\n40\n40\n20\n0\n20\n40\n5\n0\n4\n1\n9\n2\n3\n6\n7\n8\n(a) TRA\n40\n20\n0\n20\n40\n40\n20\n0\n20\n40\n5\n0\n4\n1\n9\n2\n3\n6\n7\n8\n(b) DeepBE\n40\n20\n0\n20\n40\n60\n40\n20\n0\n20\n40\n5\n0\n4\n1\n9\n2\n3\n6\n7\n8\n(c) RBC\n75\n50\n25\n0\n25\n50\n75\n60\n40\n20\n0\n20\n40\n5\n0\n4\n1\n9\n2\n3\n6\n7\n8\n(d) INT\nFigure 3: The t-SNE visualization of features in penultimate layer of EfﬁcientNetB0 for traditional classiﬁcation (TRA),\nDeepBE, robust binary-label classiﬁcation (RBC), and interval-label classiﬁcation (INT) on MNIST. The ‘red’, ‘green’, ‘blue’,\n‘orange’, ‘purple’, ‘brown’, ‘gray’, ‘pink’, ‘olive’, and ‘cyan’ colorful points represent examples of ‘0’, ‘1’, ‘2’, ‘3’, ‘4’, ‘5’,\n‘6’, ‘7’, ‘8’, and ‘9’ category, respectively.\ntraining on the traditional classiﬁcation and robust binary\nclassiﬁcaiton. The feature distribution manifold for tra-\nditional classiﬁcation approximates a hypersphere. The\nadversarial example is near to the decision boundary, which\ncan make the decision boundary more smooth and decrease\nthe dimension of adversarial space (Tram`er et al. 2017).\nHowever, features of RBC distribute in a long and narrow\nregions. Moreover, RBC depends on actual values of ele-\nments in output vector. The adversarial training will extend\nthe feature manifold and decrease the margin between\ndifferent categories. The feature manifold extension has a\nlimited impact on traditional classiﬁcation since it depends\non the relative values of elements in the output vector.\nImpact of S on Accuracy and Robustness of\nRobust Binary-label Classiﬁcation System\nIn this subsection, we experiment to investigate the impact\nof S in Eq. on the accuracy and robustness of the robust\nbinary-label classiﬁcation system. In detail, we set variant S\nfor RBC to check its accuracy and robustness on CIFAR100.\nThe deep model to implement RBC is the EfﬁcientNetB0.\nMoreover, the FGSM and PGD-5 are adopted, and the\nadversarial intensity is set as 9/255.\nAs shown in Fig. 4, with increasing S, the robust binary-\nlabel classiﬁcation achieves higher test accuracy and\nrobustness. The reason is that S determines the margin\nbetween different categories. Bigger S endows RBC with a\nbigger margin between different categories. Furthermover,\nbigger margin endows the robust binary-label classiﬁcation\nto generalize better. However, we also note that when\nFGSM\nPGD\nLEG\n0\n10\n20\n30\n40\n50\n60\n70\nAccuracy(%)\n9.11\n7.91\n26.22\n27.34\n19.71\n61.33\n44.88\n42.44\n63.08\n54.37\n53.79\n64.68\n59.53\n58.97\n64.47\nS=1\nS=150\nS=250\nS=350\nS=500\nFigure 4: Comparison of accuracy and adversarial robust-\nness between robust binary-label classiﬁcations with variant\nS.\nS exceeds a threshold, the generalization of the robust\nbinary-label classiﬁcation improves slowly. Moreover, we\nnote that the increasing S will slow down the convergence.\nWhen S\n=\n350, the robust binary-label classiﬁcation\nachieves reasonable convergence. According to Fig. 4, we\nconclude that the accuracy and robustness for the robust\nbinary-label classiﬁcation are consistent, which is different\nfrom the traditional classiﬁcation (Tsipras et al. 2019).\nInvestigation of Adversarial Transferability for\nDifferent Classiﬁcation Systems\nIn this subsection, we experiment to investigate the ad-\nversarial\ntransferability\nbetween\ndifferent\nmodels\nfor\nthe traditional classiﬁcation, interval-label, and robust\nbinary-label classiﬁcation systems. In detail, CIFAR100 is\nemployed evaluation dataset to investigate the adversarial\ntransferability. The adopted adversarial attack method is\nPGD-5, and the adversarial attack intensity is set as 9/255.\nIn Fig. 5, the model in the row is the threat model, while the\nmodel in the column is the victim model. The threat model\ncrafts adversarial examples to attack victim models. In order\nto eliminate the impact of test accuracy of the threat model,\nwe remove examples that are misclassiﬁed by the threat\nmodel from the testing set of CIFAR100.\nAs shown in Fig. 5, the adversarial transfer rate of the\ntraditional classiﬁcation is higher than that of the robust\nbinary-label and interval-label classiﬁcation. We also\nattribute it to the reason that the traditional classiﬁcation\ndepends on the relative magnitude for the values of elements\nin the output vector, while the binary-label and interval-label\nclassiﬁcation depend on the actual magnitude for the values\nof elements in the outputs vector. The adversarial attack\ncan change the relative magnitude for element values but\ndifferent magnitude of actual values in the output vector\n(Baluja and Fischer 2018).\n1.0\n0.93\n0.95\n0.91\n1.0\n0.87\n0.93\n0.9\n1.0\nD\nR\nE\nE\nR\nD\n0.90\n0.95\n1.00\n(a) TRA\n1.0\n0.34\n0.27\n0.13\n1.0\n0.26\n0.21\n0.47\n1.0\nD\nR\nE\nE\nR\nD\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) INT\n1.0\n0.64\n0.63\n0.71\n1.0\n0.7\n0.74\n0.75\n1.0\nS=500\nS=350\nS=250\nS=250\nS=350\nS=500\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n(c) RBC S\n1.0\n0.58\n0.51\n0.64\n1.0\n0.65\n0.72\n0.75\n1.0\nD\nR\nE\nE\nR\nD\n0.6\n0.7\n0.8\n0.9\n1.0\n(d) RBC ARC\nFigure 5: The adversarial transferability of different classiﬁ-\ncation systems. The values in the ﬁgure represent the success\nattack rate. E, R, and D represent EfﬁcientNetB0, ResNet18,\nand DenseNet121, respectively. Models in the row are threat\nmodels, while models in the column are victim models. Fig.\n5(c) investigates the adversarial transfer rate for RBC with\nvariant S, and the adopted deep model is EfﬁcientNetB0.\nFig. 5(d) investigates the adversarial transfer rate for RBC\nimplemented by different deep models, and S is set as 350.\nDiscussion\nThe DeepBE can be viewed as a special case of RBC. The\nloss of DeepBE (Li et al. 2016) can be reformulated as\n∥r(0 −(B(x) −0.5)(2b −1))∥2\n2. So the DeepBE can be\nviewed as RBC with S = 0. Here B and r the binary-label\nclassiﬁer and ReLU function, respectively. x is an example,\nand b is its corresponding binary label. The relationship\nbetween our method and regularization methods, such as\nadversarial training, is parallel. They focus on different\nelements of the deep learning system. As introduced above,\nour method with adversarial training can further improve\nthe robustness of the deep learning system in some cases.\nThe interval-label classiﬁcation and RBC adopt differ-\nent label encoding strategies to redeﬁne the classiﬁcation\ntask. Actually, with an increasing numerical system, we\ncan adopt fewer bits to represent a number. For instance,\ndecimal number 9 is only a bit in decimal encoding,\nwhile it needs four bit in binary encoding. Interval-label\nclassiﬁcation can be viewed as adopting a larger numerical\nsystem than binary-label classiﬁcation to encoding the label.\nThey both mark the input depending on the actual values of\nelements in the output vector.\nConclusion\nIn this paper, we improve the robustness of deep learn-\ning systems from both the learning task and deep model\nprospects. The interval-label classiﬁcation can reduce the\nadversarial transferability of adversarial examples. Unlike\nthe traditional classiﬁcation, the robustness of robust binary-\nlabel classiﬁcation is no more at odds with the accuracy. The\nscaling factor of the robust binary-label classiﬁcation con-\nstrains the minima boundary margin. Moreover, the experi-\nmental results demonstrate that adversarial training does not\nconstantly improve the adversarial robustness of the deep\nlearning system, demonstrating that the learning task im-\npacts the robustness of deep learning systems.\nReferences\nBaluja, S.; and Fischer, I. 2018. Learning to Attack: Ad-\nversarial Transformation Networks. In Proceedings of the\nThirty-Second AAAI Conference on Artiﬁcial Intelligence,\n(AAAI-18), the 30th innovative Applications of Artiﬁcial In-\ntelligence (IAAI-18), and the 8th AAAI Symposium on Edu-\ncational Advances in Artiﬁcial Intelligence (EAAI-18), New\nOrleans, Louisiana, USA, February 2-7, 2018, 2687–2695.\nBlundell, C.; Cornebise, J.; Kavukcuoglu, K.; and Wierstra,\nD. 2015. Weight Uncertainty in Neural Networks. CoRR,\nabs/1505.05424.\nCao, X.; Shi, Y.; Yu, H.; Wang, J.; Wang, X.; Yan, Z.; and\nChen, Z. 2021. DEKR: Description Enhanced Knowledge\nGraph for Machine Learning Method Recommendation. In\nDiaz, F.; Shah, C.; Suel, T.; Castells, P.; Jones, R.; and Sakai,\nT., eds., SIGIR ’21: The 44th International ACM SIGIR Con-\nference on Research and Development in Information Re-\ntrieval, Virtual Event, Canada, July 11-15, 2021, 203–212.\nACM.\nCarlini, N.; and Wagner, D. A. 2017. Towards Evaluating the\nRobustness of Neural Networks. In 2017 IEEE Symposium\non Security and Privacy, SP 2017, San Jose, CA, USA, May\n22-26, 2017, 39–57.\nDas, N.; Shanbhogue, M.; Chen, S.; Hohman, F.; Li, S.;\nChen, L.; Kounavis, M. E.; and Chau, D. H. 2018. SHIELD:\nFast, Practical Defense and Vaccination for Deep Learn-\ning using JPEG Compression. In Guo, Y.; and Farooq, F.,\neds., Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, KDD\n2018, London, UK, August 19-23, 2018, 196–204. ACM.\nDietterich, T. G.; and Bakiri, G. 1995. Solving Multiclass\nLearning Problems via Error-Correcting Output Codes. J.\nArtif. Intell. Res., 2: 263–286.\nFeldman, V.; and Zhang, C. 2020. What Neural Networks\nMemorize and Why: Discovering the Long Tail via Inﬂu-\nence Estimation. In Larochelle, H.; Ranzato, M.; Hadsell,\nR.; Balcan, M.; and Lin, H., eds., Advances in Neural Infor-\nmation Processing Systems 33: Annual Conference on Neu-\nral Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explain-\ning and Harnessing Adversarial Examples. In 3rd Interna-\ntional Conference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference Track Pro-\nceedings.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\nual Learning for Image Recognition. In 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV, USA, June 27-30, 2016, 770–778.\nIEEE Computer Society.\nKatz, G.; Barrett, C. W.; Dill, D. L.; Julian, K.; and Kochen-\nderfer, M. J. 2017. Reluplex: An Efﬁcient SMT Solver for\nVerifying Deep Neural Networks.\nIn Majumdar, R.; and\nKuncak, V., eds., Computer Aided Veriﬁcation - 29th Inter-\nnational Conference, CAV 2017, Heidelberg, Germany, July\n24-28, 2017, Proceedings, Part I, volume 10426 of Lecture\nNotes in Computer Science, 97–117. Springer.\nKrizhevsky, A. 2009. Learning Multiple Layers of Features\nfrom Tiny Images. Technical report.\nKurakin, A.; Goodfellow, I. J.; and Bengio, S. 2017. Adver-\nsarial Machine Learning at Scale. In 5th International Con-\nference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings.\nLeCun, Y.; Bengio, Y.; and Hinton, G. E. 2015. Deep learn-\ning. Nature, 521(7553): 436–444.\nLecun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.\nGradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11): 2278–2324.\nLee, K.; Yun, S.; Lee, K.; Lee, H.; Li, B.; and Shin, J.\n2019. Robust Inference via Generative Classiﬁers for Han-\ndling Noisy Labels. In Chaudhuri, K.; and Salakhutdinov,\nR., eds., Proceedings of the 36th International Conference\non Machine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings of Ma-\nchine Learning Research, 3763–3772. PMLR.\nLi, C.; Kang, Q.; Ge, G.; Song, Q.; Lu, H.; and Cheng, J.\n2016. DeepBE: Learning Deep Binary Encoding for Multi-\nlabel Classiﬁcation. In 2016 IEEE Conference on Computer\nVision and Pattern Recognition Workshops, CVPR Work-\nshops 2016, Las Vegas, NV, USA, June 26 - July 1, 2016,\n744–751. IEEE Computer Society.\nLi, M.; He, L.; and Lin, Z. 2020.\nImplicit Euler Skip\nConnections: Enhancing Adversarial Robustness via Nu-\nmerical Stability. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18 July\n2020, Virtual Event, volume 119 of Proceedings of Machine\nLearning Research, 5874–5883. PMLR.\nMa, S.; Liu, Y.; Tao, G.; Lee, W.; and Zhang, X. 2019. NIC:\nDetecting Adversarial Samples with Neural Network Invari-\nant Checking. In 26th Annual Network and Distributed Sys-\ntem Security Symposium, NDSS 2019, San Diego, Califor-\nnia, USA, February 24-27, 2019. The Internet Society.\nMaaten, L. V. D.; and Geoffrey, H. 2008.\nVisualizing\nData using t-SNE. Journal of Machine Learning Research,\n9(2605): 2579–2605.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and\nVladu, A. 2018.\nTowards Deep Learning Models Resis-\ntant to Adversarial Attacks.\nIn 6th International Confer-\nence on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Pro-\nceedings.\nMeng, D.; and Chen, H. 2017. MagNet: A Two-Pronged De-\nfense against Adversarial Examples. In Proceedings of the\n2017 ACM SIGSAC Conference on Computer and Commu-\nnications Security, CCS 2017, Dallas, TX, USA, October 30\n- November 03, 2017, 135–147.\nMoosavi-Dezfooli, S.; Fawzi, A.; and Frossard, P. 2016.\nDeepFool: A Simple and Accurate Method to Fool Deep\nNeural Networks. In 2016 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2016, Las Vegas, NV,\nUSA, June 27-30, 2016, 2574–2582.\nNair, V.; and Hinton, G. E. 2010. Rectiﬁed Linear Units\nImprove Restricted Boltzmann Machines. In F¨urnkranz, J.;\nand Joachims, T., eds., Proceedings of the 27th International\nConference on Machine Learning (ICML-10), June 21-24,\n2010, Haifa, Israel, 807–814. Omnipress.\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\nD.; Goodfellow, I. J.; and Fergus, R. 2014. Intriguing proper-\nties of neural networks. In 2nd International Conference on\nLearning Representations, ICLR 2014, Banff, AB, Canada,\nApril 14-16, 2014, Conference Track Proceedings.\nTan, M.; and Le, Q. V. 2019. EfﬁcientNet: Rethinking Model\nScaling for Convolutional Neural Networks. In Chaudhuri,\nK.; and Salakhutdinov, R., eds., Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, volume 97\nof Proceedings of Machine Learning Research, 6105–6114.\nPMLR.\nTanay, T.; and Grifﬁn, L. D. 2016.\nA Boundary Tilting\nPersepective on the Phenomenon of Adversarial Examples.\nCoRR, abs/1608.07690.\nTram`er, F.; Papernot, N.; Goodfellow, I.; Boneh, D.; and Mc-\nDaniel, P. 2017. The Space of Transferable Adversarial Ex-\namples. arXiv e-prints, arXiv:1704.03453.\nTsipras, D.; Santurkar, S.; Engstrom, L.; Turner, A.; and\nMadry, A. 2019. Robustness May Be at Odds with Accu-\nracy. In 7th International Conference on Learning Repre-\nsentations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nWang, Q.; Guo, W.; Zhang, K.; Ororbia, A. G.; and Giles,\nC. L. 2017. Adversary Resistant Deep Neural Networks with\nan Application to Malware Detection.\nWang, Y.; Zou, D.; Yi, J.; Bailey, J.; Ma, X.; and Gu, Q.\n2020.\nImproving adversarial robustness requires revisit-\ning misclassiﬁed examples. In International Conference on\nLearning Representations.\nXiao, C.; Li, B.; Zhu, J.-Y.; He, W.; Liu, M.; and Song, D.\n2018. Generating Adversarial Examples with Adversarial\nNetworks. arXiv e-prints, arXiv:1801.02610.\nXie, C.; Wu, Y.; van der Maaten, L.; Yuille, A. L.; and He, K.\n2019. Feature Denoising for Improving Adversarial Robust-\nness. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-\n20, 2019, 501–509.\nXie, C.; and Yuille, A. L. 2020.\nIntriguing Properties of\nAdversarial Training at Scale.\nIn 8th International Con-\nference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\nXu, W.; Evans, D.; and Qi, Y. 2018. Feature Squeezing: De-\ntecting Adversarial Examples in Deep Neural Networks. In\n25th Annual Network and Distributed System Security Sym-\nposium, NDSS 2018, San Diego, California, USA, February\n18-21, 2018.\nYuan, X.; He, P.; Zhu, Q.; and Li, X. 2019.\nAdversarial\nExamples: Attacks and Defenses for Deep Learning. IEEE\nTrans. Neural Netw. Learning Syst., 30(9): 2805–2824.\nZhang, H.; Yu, Y.; Jiao, J.; Xing, E. P.; Ghaoui, L. E.; and\nJordan, M. I. 2019. Theoretically Principled Trade-off be-\ntween Robustness and Accuracy.\nIn Chaudhuri, K.; and\nSalakhutdinov, R., eds., Proceedings of the 36th Interna-\ntional Conference on Machine Learning, ICML 2019, 9-\n15 June 2019, Long Beach, California, USA, volume 97\nof Proceedings of Machine Learning Research, 7472–7482.\nPMLR.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2020-10-11",
  "updated": "2021-12-02"
}