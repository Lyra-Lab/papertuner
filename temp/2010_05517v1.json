{
  "id": "http://arxiv.org/abs/2010.05517v1",
  "title": "Unsupervised Semantic Aggregation and Deformable Template Matching for Semi-Supervised Learning",
  "authors": [
    "Tao Han",
    "Junyu Gao",
    "Yuan Yuan",
    "Qi Wang"
  ],
  "abstract": "Unlabeled data learning has attracted considerable attention recently.\nHowever, it is still elusive to extract the expected high-level semantic\nfeature with mere unsupervised learning. In the meantime, semi-supervised\nlearning (SSL) demonstrates a promising future in leveraging few samples. In\nthis paper, we combine both to propose an Unsupervised Semantic Aggregation and\nDeformable Template Matching (USADTM) framework for SSL, which strives to\nimprove the classification performance with few labeled data and then reduce\nthe cost in data annotating. Specifically, unsupervised semantic aggregation\nbased on Triplet Mutual Information (T-MI) loss is explored to generate\nsemantic labels for unlabeled data. Then the semantic labels are aligned to the\nactual class by the supervision of labeled data. Furthermore, a feature pool\nthat stores the labeled samples is dynamically updated to assign proxy labels\nfor unlabeled data, which are used as targets for cross-entropy minimization.\nExtensive experiments and analysis across four standard semi-supervised\nlearning benchmarks validate that USADTM achieves top performance (e.g.,\n90.46$\\%$ accuracy on CIFAR-10 with 40 labels and 95.20$\\%$ accuracy with 250\nlabels). The code is released at https://github.com/taohan10200/USADTM.",
  "text": "Unsupervised Semantic Aggregation and Deformable\nTemplate Matching for Semi-Supervised Learning\nTao Han†, Junyu Gao†, Yuan Yuan and Qi Wang∗\nSchool of Computer Science and Center for OPTical IMagery Analysis and Learning\nNorthwestern Polytechnical University\nXi’an, Shaanxi, P.R. China.\nhantao10200@mail.nwpu.edu.cn, {gjy3035, y.yuan1.ieee, crabwq}@gmail.com\nAbstract\nUnlabeled data learning has attracted considerable attention recently. However, it\nis still elusive to extract the expected high-level semantic feature with mere unsu-\npervised learning. In the meantime, semi-supervised learning (SSL) demonstrates\na promising future in leveraging few samples. In this paper, we combine both to\npropose an Unsupervised Semantic Aggregation and Deformable Template Match-\ning (USADTM) framework for SSL, which strives to improve the classiﬁcation\nperformance with few labeled data and then reduce the cost in data annotating.\nSpeciﬁcally, unsupervised semantic aggregation based on Triplet Mutual Informa-\ntion (T-MI) loss is explored to generate semantic labels for unlabeled data. Then\nthe semantic labels are aligned to the actual class by the supervision of labeled data.\nFurthermore, a feature pool that stores the labeled samples is dynamically updated\nto assign proxy labels for unlabeled data, which are used as targets for cross-entropy\nminimization. Extensive experiments and analysis across four standard semi-\nsupervised learning benchmarks validate that USADTM achieves top performance\n(e.g., 90.46% accuracy on CIFAR-10 with 40 labels and 95.20% accuracy with 250\nlabels). The code is released at https://github.com/taohan10200/USADTM.\n1\nIntroduction\nDeep learning is booming driven by massive labeled data over the past few years, such as image\nclassiﬁcation [1, 2], semantic segmentation [3, 4], object detection [5, 6], natural language processing\n[7, 8]. Besides, learning with unlabeled data also makes much progress in reducing the labeling costs\n[9–11]. The two most important branches are unsupervised and semi-supervised learning. For the\nimage classiﬁcation task, semi-supervised learning has shown that it can achieve a performance close\nto supervised results under certain conditions, while unsupervised learning remains a huge challenge\nfor machine learning. A new perspective is to combine unsupervised learning with supervised learning\nto achieve better performance.\nIn this paper, we are working on the following problems. 1) Most of the unsupervised methods\ncannot directly output the classiﬁcation results of the object [12–14]. Some end-to-end unsupervised\nlearning methods [9] also output a semantic label that does not correspond to the actual category. The\nspeciﬁc category to which the object belongs still depends on the clustering [15, 16]. 2) Due to the\nlack of manually injected supervised information, unsupervised learning split the data on their own.\nTo verify, as shown in Fig. 1, we create a classiﬁcation task of circles, triangles, and pentagons. The\nleft samples are given color to the border, while the right samples are ﬁlled in the entire graph. We\nexpect the unsupervised classiﬁcation in these two different datasets to yield circles, triangles, and\n†\nT. Han and J. Gao are co-ﬁrst authors of the paper.\n*\nQ. Wang is the corresponding author.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2010.05517v1  [cs.CV]  12 Oct 2020\nBlue\nRed\nGreen\nCircle\nTriangle\nPentagon\nBlue\nRed\nGreen\nCircle\nTriangle\nPentagon\nCircle\nTriangle\nPentagon\nBlue\nGreen\nRed\nFigure 1: Two simple experiments on unsupervised classiﬁcation with IIC [9]. Both of which we\nexpect would yield shape-based categorization results. However, the data on the right, with the\ninterference of color information, deviated far from our expectations.\npentagons. Nevertheless, the box on the left gives us a result based on shape, and the box on the right\ngives us a result based on color. The results are not expected. So it demonstrates that the features\nextracted by unsupervised learning with no supervised information are not all useful in promoting\nthe task, even in some cases deviate far from the expectation. 3) Pure unsupervised learning is still\ndifﬁcult to deal with complex classiﬁcation tasks. Taking the excellent IIC [9] as an example, it\ncan achieve 99.2% accuracy in the simple handwritten numeric dataset MNIST [17], but it can only\nachieve 25.7% accuracy on the CIFAR100-20 [18].\nTo extract the beneﬁcial feature and ignore the useless information in unsupervised learning, we make\na constraint by injecting a part of the supervised information into unsupervised learning. In other\nwords, this paper aims to establish an SSL framework combining unlabeled data with few labeled\ndata. Unlike the existing excellent SSL methods based on consistent regularization and entropy\nminimization [19–22], we propose a new SSL framework via unsupervised semantic aggregation and\ndeformable template matching. Speciﬁcally, the unlabeled data are explored to make self-supervised\nlearning by maximizing mutual information. Then, few annotated samples are provided to align\nthe semantic labels with the real categories. Besides, for further leveraging the unlabeled data, we\nestablish a dynamic pattern pool for each class and assign proxy labels by template matching in the\nfeature-level. It is a new and more reasonable pseudo label generation method that compares with\nother semi-supervised methods.\nFor problem 1), such a framework can eliminate the clustering operations that the traditional unsuper-\nvised learning required. For problem 2), the introduced supervised information can be an effective\nmeasure that helps unsupervised networks learn the expected representation under the interference of\nuseless information (e.g., background, color). For question 3), maximum mutual information applied\nin semi-supervision learning will further enhance the classiﬁcation performance on the complex\ndatasets. Our main contributions are the following:\n• Exploit triplet mutual information loss to achieve semantic labels clustering for unlabeled\ndata in SSL, which has a better performance in unsupervised semantic aggregation than\nsingle paired MI loss.\n• Propose a deformable template matching method for generating pseudo labels, which assigns\nproxy labels for unlabeled data in the high-level feature space. It is a more effective way\ncompared with conﬁdence based methods.\n• Experiments on several standard classiﬁcation benchmarks demonstrate that USADTM\nachieves state-of-the-art by integrating the supervised learning with unsupervised learning.\n2\nRelated work\nIn this section, we brieﬂy review the related unsupervised and semi-supervised learning in the\nclassiﬁcation task. More concrete introductions are provided in [23, 24].\nUnsupervised Learning. Unsupervised learning focuses on learning the deep representation of\nunlabeled data [25–27]. AutoEncoder (AE) is a kind of neural network for unsupervised data\nrepresentation [28–31]. The conventional process is training an autoencoder to compress and reduce\ndimensionality. Furthermore, classiﬁcation learning or cluster algorithm is carried out based on the\ncompressed feature of a middle layer. Variational Autoencoder (VAE), as a generative variant of\nAE, enforces the latent code of AE to follow a predeﬁned distribution [32, 33]. Then Generative\n2\nDeformable Template Matching\nMax cosine_similarity\nMin euclidean_dist\nClass center\nUpdate feature queues\nUnsupervised Semantic Aggregation\nfc\n(1x128)\nfc\n(\n)\nw\nu\nu\nx\nW x\n\n(\n)\ns\nu\nu\nx\nS x\n\n( )\nw\nl\nl\nx\nW x\n\n(\n)\ns\nl\nl\nx\nS x\n\nfc\nfc\n(\n)\n( |\n)\nu\nu\nX\nx\np y x\n\n\n(\n)\n( |\n))\nw\nw\nu\nu\nY\nx\np y x\n\n\n(\n)\n( |\n)\ns\ns\nu\nu\nZ\nx\np y x\n\n\n(\n,\n)\nI X Z\n( ,\n)\nI Y Z\n(\n, )\nI X Y\nPseudo label\n label\n(\n,( |\n))\ns\nl\nl\nH p\ny x\nˆ\n(\n, ( |\n))\ns\nu\nu\nH p\np y x\n( |\n)\ns\nl\np y x\nThe top K  similarity sample for each \nclass are recorded and added to the \nlabeled data in next epoch \nux\nSupervised  Learining with Few Samples \nquery\nindex\nclass\nbird\nif\nf\nf\n( ,\n)\nC K\nFigure 2: The ﬂowchart of our proposed USADTM, which mainly consists of three components: 1)\nSupervised learning is trained with the labeled data; 2) A triplet mutual information loss are designed\nto generate the sematic labels for unlabeled data. 3) A label guesser based feature-level deformable\ntemplate matching are used to generate proxy labels. The dotted lines between the model represent\nshared parameters.\nAdversarial Network (GAN) is another popular deep generative model in recent years [13, 34, 35].\nAlthough the above generative learning has achieved excellent performance in some simple datasets\n(e.g., MINST [17]), it is still elusive for the generative model to achieve acceptable results in some\ncomplex datasets (e.g., CIFAR10 [18], CIFAR100 [18]). Another signiﬁcant branch of unsupervised\nfeature learning is based on information theory, which can extract cluster features by maximizing\nmutual information objective function [36, 12, 9, 37]. Notable is the IIC [9], which demonstrates the\ncomplete unsupervised SOTA results achieved on MNIST, CIFAR10, CIFAR100-20, and STL10 [38].\nUnlike other works that focus on mutual information, it no longer estimates the mutual information\nbetween input and output, but directly train an end-to-end unsupervised classiﬁcation model by\nmaximizing the mutual information of paired samples on the output.\nSemi-supervised Learning. Semi-supervised learning is a mixture of unsupervised and supervised\nlearning [39]. Earlier work focused on consistency regularization [40, 41], Π-model [40] combines\nthe supervised cross-entropy (CE) loss and the unsupervised consistency loss MSE. The former\nrefers to the loss of labeled data, while the latter constrains the prediction of an unlabelled sample\nand its randomly augmented sample. Mean teacher [41] is based on the Π-model and Temporal\nEnsembling [40]. It uses an exponential moving average of model parameters to create predictions.\nPseudo-labeling is a powerful technique for the unlabeled data entropy minimization in recent work.\n[42] is an early approach for estimating labels of unknown data. MixMatch [19] proposes a novel\nsharping method over multiple weak augmentation samples predictions to improve the quality of\nthe Pseudo-Labels, L2 loss is used for unlabeled loss. UDA [21] proposes a augment scheme by\ncombining AutoAugment [43] with Cutout [44] to generate the pseudo labels. The unsupervised loss\nis the Kullback Leiber divergence (KL). ReMixMatch [20] also uses weak augmentation to get pseudo\nlabels. Then take the multiple predictions with strong augmentation to participate in the unlabeled\nloss calculation. FixMatch [22], a substantially simpliﬁed version of UDA and ReMixMatch, uses\na conﬁdence threshold to generate the pseudo labels. As far as we know, it is also the best semi-\nsupervised framework at present.\n3\nFramework\nIn this section, we describe the SSL framework in detail. First, a triple mutual information maximiza-\ntion strategy is proposed to extract the semantic information for unlabeled data. Unlike the previous\nsemi-supervised network [19, 20, 45, 21, 22], we then present a new pseudo-labeling approach based\n3\non feature template matching, which assigns proxy labels to unlabeled data for entropy minimization.\nFinally, we introduce the object function for optimizing the proposed SSL framework.\n3.1\nUnsupervised Semantic Aggregation\nFor a semi-supervised classiﬁcation task, we deﬁne it as follows: the training set has C categories and\nX samples. Then X is divided into the unlabeled set Xu and labeled set Xl. Let {xw\nl , xs\nl } ∈Xl are\npaired samples with the same one-hot labels pl, and {xu, xw\nu , xs\nu} ∈Xu are triple paired unlabeled\nsamples. Here, x∗refer to the original images without any transform. Following [22], we also use\ntwo image enhancement strategies. xw\n∗and xs\n∗are converted data from the original image based on\nthe weak augment W(·) and strong augment S(·), respectively. W(·) is a standard ﬂip-and-shift\nstrategy. S(·) is the RandAugment [46] strategy and followed by Cutout [44].\nFeature extraction is an essential and basic task in unsupervised learning. A useful feature vector\nis to distinguish the sample from the whole dataset, that is, to extract the information belonging to\nthe example. Mutual information ﬁts this expectation well. The mutual information measures the\nKL divergence of the marginal distribution and joint distribution. If (X, Y ) ∼p(x, y), the mutual\ninformation between random variables X and Y is deﬁned as follows:\nI(X; Y ) ≡KL(p(x, y)∥p(x)p(y))\n=\nX\ny∈Y\nX\nx∈X\np(x, y) log\n\u0012 p(x, y)\np(x)p(y)\n\u0013\n= H(X) −H(X|Y ),\n(1)\nwhere the ﬁrst row deﬁnes mutual information, the second row shows how to calculate mutual\ninformation in practical applications, and the third row reveals the relationship between mutual\ninformation and entropy and conditional entropy. IIC [9] regards the above mutual information as a\ncriterion and directly maximizes it for paired samples to achieve automatically cluster for unlabeled\ndata. However, single paired mutual information loss is challenging to deal with complex datasets. IIC\nis hard to optimize when the transformation used to generate the paired data is too strong. Therefore,\nwe add weak augment data to serve as a transition. Every two groups among the three types of data\nare wrapped to obtain Triplet Mutual Information (T-MI) loss functions.\nAs illustrate in Fig. 2, the standard Wide ResNet-28-2 [47] is the classiﬁcation network φ(·). Let the\noutput value of the last layer is y. φ(xu) = p(y|xu), φ(xw\nu ) = p(y|xw\nu ) and φ(xs\nu) = p(y|xs\nu) are the\npredicted class distribution produced by the model for input xu,xw\nu and xs\nu, respectively. We deﬁne\ntriple mutual information loss as follows:\nLu\nT −MI(xu, xw\nu , xs\nu) = −1\n3(I(φ(xu); φ(xw\nu )) + I(φ(xu); φ(xs\nu)) + I(φ(xw\nu ); φ(xs\nu))),\n(2)\nwhere each item is calculated according to the Eq. 1. For a batch, the marginal distribution probability\np(x), p(y), and the joint distribution probability p(x, y) is based on the prediction results. The\nspeciﬁc calculation method is ﬂowing [9]. This maximum mutual information can make unlabeled\ndata aggregation in semantic level, and clustering data on its own. Most importantly, T-MI makes\nthe optimization become stable and a little bit simple. Next, we introduce the supervised learning to\nalign semantic labels with labeled categories.\n3.2\nDeformable Template Matching\nThe annotated are exploited to play two roles. One is to align the clustering labels learned from mutual\ninformation with the labeled one-hot-labels, making end-to-end category predictions of unlabeled\ndata. This purpose is realized by minimizing the cross-entropy loss of annotated data. The other is\nthat with the mutual information loss, the unlabeled data’s features will aggregate in feature space. A\ntemplate matching algorithm can be conducted to assign proxy labels for unlabeled data by exploiting\nthe feature distribution of labeled data in the ﬁnal pooling layer. We call this Deformable Template\nMatching (DTM) because the feature distribution of labeled data is optimized continuously.\nAs shown in Fig. 2, the predictions of xs\nl are used to minimize the cross-entropy. In the meanwhile,\nthe weak augment data xw\nl are used to generate template centers. Speciﬁcally, deﬁne a template\nfeature pool where the feature of each class produced by the average pooling is stored as a queue\n4\nQi = {f 1\ni , f 2\ni , · · · , f N\ni }, (i ∈[1, 2, · · · , C]). f j\ni is a vector of length 1×128 (according to the Wide\nResNet-28-2 [47]). Queue length N is generally set up to 5× length of per class labeled data. The\nnew entered feature will edge out the end of the element when the queue is full. For each queue\nupdate, the template center are recalculated as below:\nci = ¯\nQi =\nN\nX\nj=1\nf j\ni,\n(3)\nwhere ci(i ∈[1, 2, · · · , C]) are the clustering centers of C categories. Then, we match the unlabeled\nweak augment data to clustering centers. Given the feature vector f(1 × 128) of an unlabeled sample,\nwe calculate two metrics to determine the proxy label of the unlabeled data: cosine similarity and\nEuclidean distance. The index corresponding to the maximum cosine similarity and the minimum\nEuclidean distance is respectively calculated as follows:\nm = arg max\ni\n{\nfcT\ni\n∥f∥2∥ci∥2\n},\nn = arg min\ni\n{\nq\n(f −ci)(f −ci)T },\n(4)\nwhere m, n represent the class index. We calculate the two similarities to ensure that they are closest\nto the cluster center in both direction and distance. Finally, for an unlabeled image xi, we deﬁne the\nfollowing rules to assign labels:\nˆpi\nu =\n\u001a\nm\n(m = n\nand\nsim(f, cm) ≥τ)\n−1\n(m ̸= n\nor\nsim(f, cm) < τ)) ,\n(5)\nwhere −1 is the ignored label, which indicates the sample will not be included in calculating loss.\nsim(f, cm) represents the cosine similarity between the under matched vector f and the potential\ncluster center cm. The purpose of setting the threshold τ is to ﬁlter out part of the wrong allocation.\nIn the absence of special instructions, τ is generally set at 0.85. In each iteration, the proxy label is\nassigned to the unlabeled data using the proposed method above.\nBesides, the available annotation data is often limited (e.g., four samples per class in CIFAR-10 [18],\nCIFAR-100 [18], and SVHN [48]). When a few examples are used to represent the category center,\nthere is a deviation. To makes the cluster center more representative, as shown in the dashed box of\nFig. 2, in each epoch, we set up a memory bank with K lengths for each category, where samples\nin each class with the top K conﬁdence will be recorded and added to the labeled set in the next\nepoch. Memory bank is a matrix of C × K that stores the image ID and conﬁdence. For a unlabeled\nsample, we query the corresponding row according to the predicted category. If the query sample has\nhigher conﬁdence than the lowest sample in the existing conﬁdence values, then the query sample\nwill replace it. In the experiment, the maximum value of K follows the rules:\nK = len(Xl)/C ∗2.\n(6)\nFinally, there are two sets of annotated data, namely precisely labeled set Xl and fake labeled set\nˆ\nXl, where C × K samples are selected from ˆ\nXl to enrich the samples of the feature pool. Noting\nthat a large K would make the class center too dependent on the additional data. It will create a\nnegative effect when the K samples contain some wrong predictions. Eq. 6 deﬁnes the K based\non our experiment. That is, the additional samples are controlled at two times of the labeled data.\nIn the range of K that we set up, the proposed method is robust. For a batch that (xs\nl , pl) ∈Xl\nand (xs\nu, ˆpu) ∈Xu with N annotated samples, we minimized the cross-entropy of annotated and\nunannotated data:\nLl\nCE = 1\nN\nX\nH(pl, p(y|xs\nl )),\nLu\nCE = 1\nN\nX\nH(ˆpu, p(y|xs\nu)).\n(7)\n3.3\nObjective Function\nIn section 3.1 and 3.2, we deﬁne the unsupervised loss Lu\nMI functions, supervised loss Lu\nCE and\nLl\nCE. The training of the proposed SSL framework is to minimize a weighted combination loss\nfunction of the three. The ﬁnal objective function is summed as:\nL(xs\nl , xu, xw\nu , xs\nu) = Ll\nCE + Lu\nCE + αLu\nT −MI,\n(8)\nwhere α denotes the weighting parameter of mutual information loss. Since T-MI loss is designed\nto assist the network in clustering, a large α will result in strong automatic clustering, which is not\nconducive to the alignment of cluster labels with actual categories. The recommended setting is 0.1.\n5\n(c) Loss and accuracy curves of two unsupervised learning \n(a) Training with IIC on unlabelled MNIST  \n(b) Training with our T-MI loss on unlabelled MNIST  \nFigure 3: Unsupervised learning on MNIST by using IIC and our proposed T-MI loss. The cluster\ndistribution in (a) and (b) is drawn with the last average pooling layer ’s feature of 1,000 samples (100\nsamples per class). To vivid show this cluster distribution, TSNE is used to make a dimensionality\nreduction. (c) shows the variation of training loss and test set accuracy for the two unsupervised\nlosses under the same codebase.\n4\nImplementation Details\nTraining Details.\nFollowing the previous work [19, 21, 21, 22], we use a standard Wide ResNet-28-\n2 [47] architectural as the model. During the training phase, the setting of batch size for labeled data\nand unlabeled data follows [22]. The labeled data and the unlabeled data and their transformations\nare input in parallelly. The SGD algorithm with 0.03 initialization learning rate is adopted to optimize\nthe network. The training and evaluation are performed on NVIDIA GTX 1080Ti GPU. To make the\ntraining process smoother, T-MI loss will not join the training until after some epochs. More speciﬁc\nsettings are visible in the provided code.\nTesting Details. Fllowing the previous work [41, 19–22], all of the datasets are evaluated on the test\nset, we also use the exponential moving average (EMA) of the model parameters when performing\nevaluation. In training phase, it is not adopted.\n5\nExperiments\nWe conduct our experiments from the following aspects: 1) For the proposed T-MI loss, we compare\nit with the single mutual loss in IIC by performing unsupervised learning on the MNIST dataset\n(section 5.2). 2) Compare the performance of our DTM with other label guessers in generating proxy\nlabel (section 5.3). 3) To test the effectiveness of USADTM, we conduct the experiments on four\nstandard SSL benchmarks (section 5.4). 4) An ablation study are performed to verify the contribution\nof each of USADTS’s components (section 5.5).\n5.1\nDatasets\nCIFAR-10 and CIFAR-100 [18] are large datasets of tiny RGB images with size 32x32. Both\ndatasets contain 60,000 images belonging to 10 or 100 classes respectively. Both sets provide 50,000\ntraining labels and 10,000 validation labels.\nSTL-10 [38] is dataset designed for unsupervised and semi-supervised learning. It only consists of\n5,000 training labels and 8,000 valdiation labels. However, 100,000 unlabeled example image are\nalso provided. These unlabeled examples belong to the training classes and some different classes.\nThe images are 96x96 color images.\nSVHN [48] Dateset is derived from Google Street View House Number.The data set contains the\ntrain, test, and the extra folder. It contains 33402, 13068 and 202353 labeled samples respectively.\nAll Numbers have been adjusted to a ﬁxed 32 x 32 resolution.\n5.2\nComparison of Mutual Information Loss\nSince our unsupervised loss is the inheritance and development of IIC [9], we are supposed to\ncompare the triplet mutual information loss proposed in this paper with the unsupervised loss of IIC.\nThe experimental conﬁguration is as follows: 1) The codebase and hyperparameters conﬁguration is\ncompletely consistent. The difference lies in the loss function. IIC used only the original image and\nits transformation as a pair, that is, the I(X, Z) in Fig. 2. 2) To abandon the clustering operation of\n6\na)\nb)\nc)\nFigure 4: The comparison of label guessers in ours(DTM) and FixMatch. a) is the accuracy of valid\nlabels over the whole unlabeled data. b) is the accuracy of the valid labels over the valid labels. c) is\nthe accuracy of test set with the two label guessers.\nIIC, we use 49,000 training samples from the MNIST training set for unsupervised learning. The\nremaining 1000 samples are exploited to determine the corresponding relationship between clustering\nlabels and actual categories. It is not involved in network training. The training duration is 60 epochs.\nThe experimental results are shown in Fig. 3. Both Fig. 3 (a) and Fig. 3 (b) are from the model of 60\nepoch. For the clustering performance, the T-MI loss achieves better feature differentiation. Fig. 3\n(c) shows the training process. In terms of loss variation, T-MI converges faster and makes lower\nloss value. From the perspective of testing accuracy, T-MI performs more stable while IIC ﬂuctuates\ngreatly. The reason is that the weak data augmentation is introduced to constitute three pairs of MI\nloss. So it is more robust than single pair MI loss. The best T-MI result on the test set is 98.82%, and\nthe IIC is 95.93% in this experiment.\n5.3\nComparison of Proxy Label Generator\nTo demonstrate that our deformable template matching is more conducive to pseudo-label generation,\nwe compared it with the current state-of-the-art FixMatch’s label guesser. In this experiment, we\ntransplant our DTM to a third-party PyTorch version of the FixMatch framework [49]. To make the\ncomparison more comprehensive, we test the performance of the two methods in 40, 250, and 4,000\nlabeled samples.\nTable 1: The accuracy for DTM and the Fix-\nMatch’s conﬁdence based method.\nLabel Generator 40 labels 250 labels 4000 labels\nFixMatch\n88.51\n94.16\n94.5\nDTM(ours)\n89.48\n94.49\n95.2\nThe experimental results are shown in Table 1,\nand it can be seen that our DTM achieves higher\naccuracy under the same FixMatch framework.\nTo look for internal mechanisms, in Fig. 4, we\nplot the variation of the pseudo label’s precision\nin the training phase for different generators. Ac-\ncording to Fig. 4 a), DTM can achieve a higher\ncorrect proportion over the whole unlabeled set,\nwhile FixMatch has higher accuracy with valid labels, as shown in Fig. 4 b). Overall, DTM achieves\na lower error rate in the test set than Fixmatch, as shown in Fig. 4 c). DTM outperforms FixMatch\nbecause the feature matching can capture more hard samples than conﬁdence based methods.\n5.4\nComparison of methods\nWe compare USADTM and other semi-supervised methods on the following four datasets: CIFAR-10,\nCIFAR -100, SVHN, and STL-10. Considering that FixMatch reimplements all the methods on\nthe same codebase, we also use the same network architecture (Wide ResNet-28-2) as FixMatch to\nmake the comparison as fair as possible. Besides, the optimizer’s parameters, data preprocessing,\nand learning rate schedule are the same as FixMatch. The selection of unlabeled data also follows\nFixMatch. But on STL10, we added two additional sets of 40 labels and 250 labels. The results\nof the experiment are reported in Table 2. Our reports are the mean and variance value of the ﬁve\ntimes training under different random seeds. As the table shows, the proposed SSL framework\nproduces state-of-the-art results in most of the settings. Taking CIFAR-10 and STL-10 as examples,\nwe achieved an accuracy of 90.46% and 90.37% at the cost of four labeled samples in each category.\n7\nTable 2: Error rates for CIFAR-10, CIFAR-100, SVHN and STL-10 on different baseline models\n(Π-Model [40], Pseudo-Labeling [42], Mean Teacher [41], MixMatch [19], UDA [21], ReMixMatch\n[20], FixMatch [22]) and our proposed USADTM.\nCIFAR-10\nCIFAR-100\nSVHN\nMethod\n40 labels\n250 labels 4000 labels\n400 labels 2500 labels 10000 labels\n40 labels\n250 labels 1000 labels\nΠ -Model\n- 54.26±3.97 14.01±0.38\n- 57.25±0.48 37.88±0.11\n- 18.96±1.92\n7.54±0.36\nPseudo-Labeling\n- 49.78±0.43 16.09±0.28\n- 57.38±0.46 36.21±0.19\n- 20.21±1.09\n9.94±0.61\nMean Teacher\n- 32.32±2.30\n9.19±0.19\n- 53.91±0.57 35.83±0.24\n-\n3.57±0.11\n3.42±0.07\nMixMatch\n47.54±11.50 11.05±0.86\n6.42±0.10 67.61±1.32 39.94±0.37 28.31±0.33 42.55±14.53\n3.98±0.23\n3.50±0.28\nUDA\n29.05±5.93\n8.82±1.08\n4.88±0.18 59.28±0.88 33.13±0.22 24.50±0.25 52.63±20.51\n5.69±2.76\n2.46±0.24\nReMixMatch\n19.10±9.64\n5.44±0.05\n4.72±0.13 44.28±2.06 27.43±0.31 23.03±0.56\n3.34±0.20\n2.92±0.48\n2.65±0.08\nFixMatch (RA)\n13.81±3.37\n5.07±0.65\n4.26±0.05 48.85±1.75 28.29±0.11 22.60±0.12\n3.96±2.17\n2.48±0.38\n2.28±0.11\nFixMatch (CTA)\n11.39±3.35\n5.07±0.33\n4.31±0.15 49.95±3.01 28.64±0.24 23.18±0.11\n7.65±7.65\n2.64±0.64\n2.36±0.19\nSupervised(ours)\n74.92±4.73 44.48±0.57 16.03±0.38 86.07±1.14 61.03±0.50 38.81±0.21\n86.60±2.65 51.68±1.65 14.44±0.98\nUSADTM (ours)\n9.54±1.04\n4.80±0.32\n4.40±0.15 43.36±1.89 28.11±0.21 21.35±0.17\n3.01±1.97\n2.11±0.65\n1.96±0.05\nFully Supervised\n2.74\n16.84\n1.48\nSTL-10\nMethod\n1000 labels\nMethod\n1000 labels\nMethod\n40 labels\n250 labels\n1000 labels\nΠ -Model\n26.23±0.82\nUDA\n7.66±0.56\nSupervised(ours)\n62.80±3.05\n46.40 ±1.56\n28.89±0.84\nPseudo-Labeling\n27.99±0.80\nReMixMatch\n5.23±0.45\nUSADTM (ours)\n9.63±1.35\n6.85±1.09\n4.01±0.59\nMean Teacher\n21.43±2.39\nFixMatch (RA)\n7.98±1.50\nFully Supervised\n1.48\nMixMatch\n10.41±0.61\nFixMatch (CTA)\n5.17±0.63\n5.5\nAblation Study\nUSADTM consists of unsupervised learning and supervised learning with few labeled data. Unsuper-\nvised learning is the core of this paper, which includes T-MI loss and proxy label generator DTM.\nIn this section, we study the impact of these components in the entire SSL framework by taking the\nCIFAR-10 dataset as an example. Explicitly, we deﬁne some comparative experiments by removing\nsome components or changing some hyper-parameters.\nThe results in table 3 show that USA and DTM both contributes to USADTM’s performance. With\nthe integration of the USA and DTM, it achieves a more comparable result. Comparing Exp3 and\nExp7, we can ﬁnd the DTM makes signiﬁcant progress for unsupervised learning both in the 250 and\n4000 labels setting. The comparison between Exp4 and Exp7 shows that the additional K feature\nsamples can help build a more reasonable feature center. Compared with Exp5∼8, the experimental\nperformance is the best when τ is set at 0.85. The last two experiments and Exp7 are about the weight\nof T-MI loss. It can be found that the larger value will have a negative impact, while the smaller value\nhas no apparent effect. So in other experiments, It is ﬁxed as 0.1.\nTable 3: Accuracy of ablation study on CIFAR-10 with 250 and 4000 labels. (α is the weight of T-MI\nloss, τ is cosine similarity threshold. Without special labeling, α is 0.1, and τ is 0.85 )\nAblation\ndescribe\n250 labels 4000 labels\nExp1:Labeled(baseline)\nSupervised learning with few samples\n45.52\n83.97\nExp2:Labeled+DTM\nAdd the deformable template matching to baseline\n94.27\n95.01\nExp3:Labeled+USA\nAdd unsupervised semantic (T-MI) to baseline\n81.23\n88.34\nExp4:1Labeled+USADTM (K ≡0)\nC × K samples are not added in the SSL framework\n94.58\n95.22\nExp5:Labeled+USADTM (α = 0.1, τ = 0.95)\nA complete SSL framework with α = 0.1, τ = 0.95\n93.45\n94.49\nExp6:Labeled+USADTM (α = 0.1, τ = 0.90)\nA complete SSL framework with α = 0.1, τ = 0.90\n94.02\n95.16\nExp7:Labeled+USADTM (α = 0.1, τ = 0.85)\nA complete SSL framework with α = 0.1, τ = 0.85\n95.21\n95.74\nExp8:Labeled+USADTM (α = 0.1, τ = 0.80)\nA complete SSL framework with α = 0.1, τ = 0.80\n95.02\n95.45\nExp9:Labeled+USADTM (α = 1, τ = 0.85)\nA complete SSL framework with α = 1, τ = 0.85\n93.68\n94.07\nExp10:Labeled+USADTM (α = 0.01, τ = 0.85) A complete SSL framework with α = 0.01, τ = 0.85\n94.86\n95.14\n8\n6\nConclusion\nIn this paper, we explore unsupervised learning in a semi-supervised classiﬁcation task. A new\ntriplet mutual information loss is proposed for unlabeled data’s semantic aggregation, which is more\nstable and effective than the single paired realization. Besides, we propose a feature-level deformable\ntemplate matching to generate proxy labels aiming to improve the accuracy in existing pseudo-label\ngenerators. Experiments show that the new generator can capture more unlabeled samples for cross-\nentropy minimization. On several standard semi-supervised benchmarks, our proposed USADTM\nachieves the best performance on the whole at present, especially with a small number of labeled\nsamples.\nBroader Impact\nThis work has the following potential positive impacts on the computer vision community. a) It con-\ntributes to the development of semi-supervised learning in combining the unsupervised and supervised\nlearning. b) It can provide reference signiﬁcances for various tasks related to image classiﬁcation.\nExamples include but are not limited to medical image classiﬁcation, scene classiﬁcation, garbage\nclassiﬁcation, etc. c)This work may inspire other domains’ research in the future. This work will not\nraise ethical problems for the foreseeable time.\nAcknowledgements\nThis work was supported by the National Natural Science Foundation of China under Grant U1864204,\n61773316, 61632018, and 61825603.\nReferences\n[1] J. Deng, W. Dong, R. Socher, L. J. Li, and F. F. Li, “Imagenet: A large-scale hierarchical image database,”\nin CVPR, 2009, pp. 20–25.\n[2] M. Tan and Q. V. Le, “Efﬁcientnet: Rethinking model scaling for convolutional neural networks,” in ICML,\n2019, pp. 6105–6114.\n[3] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and\nB. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in CVPR, 2016, pp. 3213–\n3223.\n[4] Y. Zhu, K. Sapra, F. A. Reda, K. J. Shih, S. Newsam, A. Tao, and B. Catanzaro, “Improving semantic\nsegmentation via video propagation and label relaxation,” in CVPR, 2019, pp. 8856–8865.\n[5] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, “Microsoft\ncoco: Common objects in context,” in ECCV, 2014, pp. 740–755.\n[6] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, Z. Zhang, H. Lin, Y. Sun, T. He, J. Mueller, R. Manmatha et al.,\n“Resnest: Split-attention networks,” arXiv preprint arXiv:2004.08955, 2020.\n[7] P. Rajpurkar, R. Jia, and P. Liang, “Know what you don’t know: Unanswerable questions for squad,” in\nACL, 2018, pp. 784–789.\n[8] S. Vashishth, N. Yadati, and P. Talukdar, “Graph-based deep learning in natural language processing,” in\nACM IKDD, 2020, pp. 371–372.\n[9] X. Ji, J. F. Henriques, and A. Vedaldi, “Invariant information clustering for unsupervised image classiﬁca-\ntion and segmentation,” in CVPR, 2019, pp. 9865–9874.\n[10] Z. Zheng and Y. Yang, “Rectifying pseudo label learning via uncertainty estimation for domain adaptive\nsemantic segmentation,” arXiv preprint arXiv:2003.03773, 2020.\n[11] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox, “Discriminative unsupervised feature\nlearning with convolutional neural networks,” in NIPS, 2014, pp. 766–774.\n[12] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman, A. Trischler, and Y. Bengio,\n“Learning deep representations by mutual information estimation and maximization,” arXiv preprint\narXiv:1808.06670, 2018.\n9\n[13] W. Harchaoui, P.-A. Mattei, and C. Bouveyron, “Deep adversarial gaussian mixture auto-encoder for\nclustering,” in ICLR, 2017, pp. 1–5.\n[14] O. Kilinc and I. Uysal, “Learning latent representations in neural networks for clustering through pseudo\nsupervision and graph-based activity regularization,” arXiv preprint arXiv:1802.03063, 2018.\n[15] Q. Wang, X. He, X. Jiang, and X. Li, “Robust bi-stochastic graph regularized matrix factorization for data\nclustering,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n[16] Q. Wang, Z. Qin, F. Nie, and X. Li, “Spectral embedded adaptive neighbors clustering,” IEEE Transactions\non Neural Networks and Learning Systems, vol. 30, no. 4, pp. 1265–1271, 2019.\n[17] L. Deng, “The mnist database of handwritten digit images for machine learning research [best of the web],”\nIEEE Signal Processing Magazine, vol. 29, no. 6, pp. 141–142, 2012.\n[18] A. Krizhevsky, “Learning multiple layers of features from tiny images,” Master’s thesis, University of\nTront, 2009.\n[19] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel, “Mixmatch: A holistic\napproach to semi-supervised learning,” in NIPS, 2019, pp. 5050–5060.\n[20] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel, “Remixmatch:\nSemi-supervised learning with distribution alignment and augmentation anchoring,” arXiv preprint\narXiv:1911.09785, 2019.\n[21] Q. Xie, Z. Dai, E. Hovy, M.-T. Luong, and Q. V. Le, “Unsupervised data augmentation for consistency\ntraining,” arXiv preprint arXiv:1904.12848, 2019.\n[22] K. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D. Cubuk, A. Kurakin, H. Zhang, and C. Raf-\nfel, “Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence,” arXiv preprint\narXiv:2001.07685, 2020.\n[23] L. Schmarje, M. Santarossa, S.-M. Schröder, and R. Koch, “A survey on semi-, self-and unsupervised\ntechniques in image classiﬁcation,” arXiv preprint arXiv:2002.08721, 2020.\n[24] J. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised learning,” Machine Learning, vol. 109,\nno. 2, pp. 373–440, 2020.\n[25] Y. Yuan, X. Zheng, and X. Lu, “Discovering diverse subset for unsupervised hyperspectral band selection,”\nIEEE Transactions on Image Processing, vol. 26, no. 1, pp. 51–64, 2016.\n[26] X. Li, M. Chen, and Q. Wang, “Self-tuned discrimination-aware method for unsupervised feature selection,”\nIEEE transactions on neural networks and learning systems, vol. 30, no. 8, pp. 2275–2284, 2018.\n[27] Y. Zhao, Y. Yuan, and Q. Wang, “Fast spectral clustering for unsupervised hyperspectral image classiﬁca-\ntion,” Remote Sensing, vol. 11, no. 4, p. 399, 2019.\n[28] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong, “Towards k-means-friendly spaces: Simultaneous deep\nlearning and clustering,” in ICML.\nJMLR. org, 2017, pp. 3861–3870.\n[29] P. Huang, Y. Huang, W. Wang, and L. Wang, “Deep embedding network for clustering,” in ICPR, 2014, pp.\n1532–1537.\n[30] D. Chen, J. Lv, and Y. Zhang, “Unsupervised multi-manifold clustering by learning deep representation,”\nin Workshop on AAAI, 2017.\n[31] K. Ghasedi Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang, “Deep clustering via joint convolutional\nautoencoder embedding and relative entropy minimization,” in ICCV, 2017, pp. 5736–5745.\n[32] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou, “Variational deep embedding: An unsupervised and\ngenerative approach to clustering,” arXiv preprint arXiv:1611.05148, 2016.\n[33] N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and M. Shana-\nhan, “Deep unsupervised clustering with gaussian mixture variational autoencoders,” arXiv preprint\narXiv:1611.02648, 2016.\n[34] J. T. Springenberg, “Unsupervised and semi-supervised learning with categorical generative adversarial\nnetworks,” arXiv preprint arXiv:1511.06390, 2015.\n10\n[35] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, “Infogan: Interpretable\nrepresentation learning by information maximizing generative adversarial nets,” in NIPS, 2016, pp. 2172–\n2180.\n[36] W. Hu, T. Miyato, S. Tokui, E. Matsumoto, and M. Sugiyama, “Learning discrete representations via\ninformation maximizing self-augmented training,” in JMLR, 2017, pp. 1558–1567.\n[37] P. Bachman, R. D. Hjelm, and W. Buchwalter, “Learning representations by maximizing mutual information\nacross views,” in NIPS, 2019, pp. 15 509–15 519.\n[38] A. Coates, A. Ng, and H. Lee, “An analysis of single-layer networks in unsupervised feature learning,” in\nAIS, 2011, pp. 215–223.\n[39] F. Nie, H. Zhang, R. Wang, and X. Li, “Semi-supervised clustering via pairwise constrained optimal graph,”\nin IJCAI, 2020, pp. 3160–3166.\n[40] S. Laine and T. Aila,\n“Temporal ensembling for semi-supervised learning,”\narXiv preprint\narXiv:1610.02242, 2016.\n[41] A. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged consistency targets\nimprove semi-supervised deep learning results,” in NIPS, 2017, pp. 1195–1204.\n[42] D.-H. Lee, “Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural\nnetworks,” in Workshop on ICML, vol. 3, 2013, p. 2.\n[43] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “Autoaugment: Learning augmentation\nstrategies from data,” in CVPR, 2019, pp. 113–123.\n[44] T. DeVries and G. W. Taylor, “Improved regularization of convolutional neural networks with cutout,”\narXiv preprint arXiv:1708.04552, 2017.\n[45] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, “Virtual adversarial training: a regularization method for\nsupervised and semi-supervised learning,” IEEE transactions on pattern analysis and machine intelligence,\nvol. 41, no. 8, pp. 1979–1993, 2018.\n[46] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: Practical automated data augmentation\nwith a reduced search space,” arXiv preprint arXiv:1909.13719, 2019.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in CVPR, 2016, pp.\n770–778.\n[48] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading digits in natural images with\nunsupervised feature learning,” in Workshop on NIPS, 2011.\n[49] CoinCheung, “A pytorch version of ﬁxmatch,” https://github.com/CoinCheung/ﬁxmatch-pytorch, 2019.\n11\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2020-10-12",
  "updated": "2020-10-12"
}