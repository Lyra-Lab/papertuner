{
  "id": "http://arxiv.org/abs/2010.05522v1",
  "title": "Pre-trained Language Model Based Active Learning for Sentence Matching",
  "authors": [
    "Guirong Bai",
    "Shizhu He",
    "Kang Liu",
    "Jun Zhao",
    "Zaiqing Nie"
  ],
  "abstract": "Active learning is able to significantly reduce the annotation cost for\ndata-driven techniques. However, previous active learning approaches for\nnatural language processing mainly depend on the entropy-based uncertainty\ncriterion, and ignore the characteristics of natural language. In this paper,\nwe propose a pre-trained language model based active learning approach for\nsentence matching. Differing from previous active learning, it can provide\nlinguistic criteria to measure instances and help select more efficient\ninstances for annotation. Experiments demonstrate our approach can achieve\ngreater accuracy with fewer labeled training instances.",
  "text": "Pre-trained Language Model Based Active Learning for Sentence\nMatching\nGuirong Bai1,2, Shizhu He1,2, Kang Liu1,2, Jun Zhao1,2, Zaiqing Nie3\n1 National Laboratory of Pattern Recognition, Institute of Automation,\nChinese Academy of Sciences\n2 School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences\n3 Alibaba AI Labs\n{guirong.bai, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn\nzaiqing.nzq@alibaba-inc.com\nAbstract\nActive learning is able to signiﬁcantly reduce the annotation cost for data-driven techniques.\nHowever, previous active learning approaches for natural language processing mainly depend\non the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In\nthis paper, we propose a pre-trained language model based active learning approach for sentence\nmatching. Differing from previous active learning, it can provide linguistic criteria to measure\ninstances and help select more efﬁcient instances for annotation. Experiments demonstrate our\napproach can achieve greater accuracy with fewer labeled training instances.\n1\nIntroduction\nSentence matching is a fundamental technology in natural language processing. Over the past few years,\ndeep learning as a data-driven technique has yielded state-of-the-art results on sentence matching (Wang\net al., 2017; Chen et al., 2016; Gong et al., 2017; Yang et al., 2016; Parikh et al., 2016; Gong et al.,\n2017; Kim et al., 2019). However, this data-driven technique typically requires large amounts of manual\nannotation and brings much cost. If large labeled data can’t be obtained, the advantages of deep learning\nwill signiﬁcantly diminish.\nTo alleviate this problem, active learning is proposed to achieve better performance with fewer labeled\ntraining instances (Settles, 2009). Instead of randomly selecting instances, active learning can measure\nthe whole candidate instances according to some criteria, and then select more efﬁcient instances for\nannotation (Zhang et al., 2017; Shen et al., 2017; Erdmann et al., ; Kasai et al., 2019; Xu et al., 2018).\nHowever, previous active learning approaches in natural language processing mainly depend on the\nentropy-based uncertainty criterion (Settles, 2009), and ignore the characteristics of natural language.\nTo be more speciﬁc, if we ignore the linguistic similarity, we may select redundant instances and waste\nmany annotation resources. Thus, how to devise linguistic criteria to measure candidate instances is an\nimportant challenge.\nRecently, pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018;\nYang et al., 2019) have been shown to be powerful for learning language representation. Accordingly,\npre-trained language models may provide a reliable way to help capture language characteristics. In this\npaper, we devise linguistic criteria from a pre-trained language model to capture language characteristics,\nand then utilize these extra linguistic criteria (noise, coverage and diversity) to enhance active learning. It\nis shown in Figure 1. Experiments on both English and Chinese sentence matching datasets demonstrate\nthe pre-trained language model can enhance active learning.\n2\nMethodology\nIn a general active learning scenario, there is a small set of labeled training data P and a large pool of\navailable unlabeled data Q. Active learning is to select instances in Q according to some criteria, and then\nlabel them and add them into P, so as to maximize classiﬁer M performance and minimize annotation\ncost. More details of preliminaries about sentence matching and active learning are in the Appendix.\narXiv:2010.05522v1  [cs.CL]  12 Oct 2020\nAnnotator\nUnlabeled Data\nLabeled Data\nClassifier\nCriteria\nuncertainty criterion\nnoise criterion*\ncoverage criterion*\ndiversity criterion*\nActive Learning\nMeasuring\nLabeling\nTraining\nQuery\nLanguage Model\nlinguistic\nlinguistic\nlinguistic\nFigure 1: Pipeline of our pre-trained language model based active learning. Noise, coverage and diversity\nare proposed linguistic criteria from a pre-trained language model.\n2.1\nPre-trained Language Model\nWe choose the widely used language model BERT (Devlin et al., 2018) as the pre-trained language model.\nFrom BERT, we can obtain two kinds of information to provide linguistic criteria. One is the cross entropy\nloss sai of reconstructing of the i-th word ai in sentence A (the same with another B) by masking only\nai and predicting ai again. The other is word embeddings (contextual representations of the last layer)\na=[e(a1),e(a2),. . . ,e(alA)] in the sentence, where lA is the length of sentence A.\n2.2\nCriteria for Instance Selection\n(1) Uncertainty: The uncertainty criterion indicates classiﬁcation uncertainty of an instance and is the\nstandard criterion in active learning. Instances with high uncertainty are more helpful to optimize the\nclassiﬁer and thus are worthier to be selected. The uncertainty is computed as the entropy, and we can\nobtain uncertainty rank rankuncer(xi) for the i-th instance in Q based on the entropy. Formally,\nrankuncer(xi) ∝−Ent(xi)\n(1)\nwhere Ent(xi) = −P\nk P(yi = k|xi) log P(yi = k|xi).\n(2) Noise: The noise criterion indicates how much potential noise there is in an instance. Intuitively,\ninstances with noise may degrade the labeled data P, and we want to select noiseless instances. Noisy\ninstances usually have rare expression with low generating probability. Thus, noisy tokens may be hard\nto be reconstructed with context by the pre-trained language model. Based on this assumption, noise\ncriterion is formulated about losses of reconstructing masked tokens:\nranknoise(xi) ∝−P(A) −P(B)\n(2)\nwhere P(A) = P(a1a2 . . . alA) ∝\nlA\nP\ni∈lA sai . P(B) is similar. ranknoise(xi) denotes noise rank of the\ni-th instance in Q, sai/sbi is the reconstruction loss of the i-th word ai/bi in sentence A/B from the\npre-trained language model.\n(3) Coverage: The coverage criterion indicates whether the language expression of the current instance\ncan enrich representation learning. On the one hand, some tokens like stop words are meaningless and easy\nto model (high coverage). On the other hand, the classiﬁer needs fresh instances (low coverage) to enrich\nrepresentation learning. These fresh instances like relatively low-frequency professional expressions\nusually have lower generating probabilities than common ones. Thus, we can employ reconstruction\nlosses to capture the low coverage ones as follows:\nrankcover(xi) ∝−\nP\nj∈lA cajsaj\nP\nj∈lA caj\n−\nP\nj∈lB cbjsbj\nP\nj∈lB cbj\n(3)\ncaj =\n\u001a 0\nif saj > β\n1\nothers\n, cbj =\n\u001a 0\nif sbj > β\n1\nothers\n(4)\nwhere β denotes a hyperparameter to distinguish noise and is set as 10.0.\n(4) Diversity: The diversity criterion indicates the diversity of instances. Redundant instances are\ninefﬁcient and waste annotation resources. In contrast, diverse ones can help learn more various language\nexpressions and matching patterns.\nFirst, we use a vector vi for instance representation of a sentence pair instance xi. To model the\ndifference between two sentences, we employ the subtraction of word embeddings between “Delete\nSequence” LD and “Insert Sequence” LI from Levenshtein Distance (when we transform sentence A\nto sentence B by deleting and inserting tokens, these tokens are added into LD and LI respectively).\nIt is illustrated in the Appendix. Besides, the word embeddings in the subtraction are weighted by\nreconstruction losses. Intuitively, meaningless tokens such as preposition should have less weight, and\nthey are usually easier to predict with lower reconstruction losses. Formally,\nvi =\nX\nj∈LI\nwbje(bj) −\nX\nj∈LD\nwaje(aj)\n(5)\nwaj =\nsaj\nP\nk∈lA sak\n, wbj =\nsbj\nP\nk∈lB sbk\n(6)\nwhere sai/sbj is the reconstruction loss of the i/j-th word of sentence A/B. e(aj)/e(bj) denotes word\nembdeddings. wai/wbj denotes the weight for tokens.\nWith instance representation, we want to select diverse ones that are representative and different from\neach other. Speciﬁcally, we employ k-means clustering algorithm for diversity rank as follows:\nrankdiver(xi) =\n\u001a 0\nif vi ◦vi ∈Odiver\nn\nothers\n(7)\nwhere Odiver are the centers of n clusters of {vi ◦vi}. ◦denotes multiplication on element.\n2.3\nInstance Selection\nIn practice, according to different effectiveness of criteria, we combine ranks of criteria and select the\ntop n candidate instances in unlabeled data Q. Speciﬁcally, we sequentially use rankuncer, rankdiver,\nrankcover, ranknoise to select top 8n, 4n, 2n, n candidate instances, and add the ﬁnal n instances into\nlabeled data P for training at every round.\n3\nExperiments\n3.1\nSettings and Comparisons\nWe conduct experiments on Both English and Chinese datasets, including SNLI (Bowman et al., 2015),\nMultiNLI (Williams et al., 2017), Quora (Iyer et al., 2017), LCQMC (Liu et al., 2018), BQ (Chen et al.,\n2018). The number of instances to select at every round is n = 100. We choose (Devlin et al., 2018) as\nclassiﬁer M and perform 25 rounds of active learning. There is a held-out test set for evaluation after all\nrounds. We compare the following active learning approaches:\n(1)Random sampling (Random) randomly selects instances for annotation and training at each round.\n(2)Uncertainty sampling (Entropy) is the standard entropy criterion (Tong and Koller, 2001; Zhu et al.,\n2008).\n(3)Expected Gradient Length (EGL) aims to select instances expected to result in the greatest change\nto the gradients of tokens. (Settles and Craven, 2008; Zhang et al., 2017).\n(4)Pre-trained language model (LM) is our proposed active learning approach.\n3.2\nResults\nTable 1 and Figure 2 (1-5) report accuracy and learning curves of each approach on the ﬁve datasets.\nOverall, our approach obtains better performance on both English and Chinese datasets. We can know that\nextra linguistic criteria are effective, demonstrating that a pre-trained language model can substantially\ncapture language characteristics and provide more efﬁcient instances for training. Besides, active learning\nSNLI\nMultiNLI\nQuora\nLCQMC\nBQ\nRandom\n77.90\n67.83\n79.01\n82.04\n71.44\nEntropy\n79.80\n70.27\n80.21\n83.25\n73.60\nEGL\n77.86\n66.80\n77.91\n80.35\n71.59\nLM\n80.99\n71.79\n81.79\n84.29\n74.73\nEnt\nE+Cov\nE+Noi\nE+Div\nE+All\nAblation\n79.80\n80.99\n81.11\n81.45\n80.99\nTable 1: The upper part lists accuracy of different approaches on ﬁve datasets. The low part lists accuracy\nof combining different linguistic criterion with uncertainty on SNLI dataset for ablation.\nFigure 2: The ﬁgures 1-5 are learning curves of comparisons on the ﬁve datasets. The 6-th ﬁgure illustrates\nlearning curves on four SNLI subsets to show the relation between data size and accuracy.\napproaches always obtain better performance than random sampling. It demonstrates that the amount of\nlabeled data for sentence matching can be substantially reduced by active learning. And EGL performs\nworse than the standard approach active learning, maybe gradient based active learning is not suitable\nfor sentence matching. In fact, sentence matching needs to capture the difference between sentences and\ngradients of a single token can’t reﬂect the relation. Moreover, we show the relation between the size of\nunlabeled data and accuracy in Figure 2 (6), we can see the superiority of the pre-trained model based\napproach is more signiﬁcant for larger data size.\n3.3\nAblation Study\nTo validate the effectiveness of extra linguistic criteria, we separately combining them with standard\nuncertainty criterion. “Ent” denotes the standard uncertainty criterion, “E+Noi/E+Cov/E+Div/E+All”\ndenotes combining uncertainty with noise/coverage/diversity/all criteria. Table 1 reports the accuracy.\nCurves are also illustrated in the Appendix.\nWe can see each combined criterion performs better than a single uncertainty criterion. It demonstrates\nthat each linguistic criterion from a pre-trained language model helps capture language characteristics and\nenhances selection of instances. More ablation discussions are shown in the Appendix.\n4\nConclusion\nIn this paper, we combine active learning with a pre-trained language model. We devise extra linguistic\ncriteria from a pre-trained language model, which can capture language characteristics and enhance active\nlearning. Experiments show that our proposed active learning approach obtains better performance.\nAcknowledgements\nThe work is supported by the National Natural Science Foundation of China under Grant Nos.61533018,\nU1936207, 61976211, and 61702512, and the independent research project of National Laboratory of\nPattern Recognition under Grant. This research work was also supported by Youth Innovation Promotion\nAssociation CAS.\nReferences\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv.\nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2016. Enhanced lstm for natural\nlanguage inference. arXiv.\nJing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu, and Buzhou Tang. 2018. The bq corpus: A large-scale\ndomain-speciﬁc chinese corpus for sentence semantic equivalence identiﬁcation. In EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv.\nAlexander Erdmann, David Joseph Wrisley, Benjamin Allen, Christopher Brown, Sophie Cohen-Bod´en`es, Micha\nElsner, Yukun Feng, Brian Joseph, B´eatrice Joyeux-Prunel, and Marie-Catherine de Marneffe. Practical, efﬁ-\ncient, and customizable active learning for named entity recognition in the digital humanities. In NAACL.\nYichen Gong, Heng Luo, and Jian Zhang. 2017. Natural language inference over interaction space. arXiv.\nShankar Iyer, Nikhil Dandekar, and Korn´el Csernai. 2017. First quora dataset release: Question pairs. data. quora.\ncom.\nJungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, and Lucian Popa. 2019. Low-resource deep entity resolution\nwith transfer and active learning. In ACL.\nSeonhoon Kim, Inho Kang, and Nojun Kwak. 2019. Semantic sentence matching with densely-connected recur-\nrent and co-attentive information. In AAAI.\nXin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, and Buzhou Tang. 2018. Lcqmc: A\nlarge-scale chinese question matching corpus. In COLING.\nAnkur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model\nfor natural language inference. arXiv.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. arXiv.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding\nwith unsupervised learning. Technical report, Technical report, OpenAI.\nBurr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In\nEMNLP.\nBurr Settles. 2009. Active learning literature survey. Technical report, University of Wisconsin-Madison Depart-\nment of Computer Sciences.\nYanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod, and Animashree Anandkumar. 2017. Deep active\nlearning for named entity recognition. arXiv.\nSimon Tong and Daphne Koller. 2001. Support vector machine active learning with applications to text classiﬁca-\ntion. JMLR.\nZhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natural language\nsentences. arXiv.\nAdina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence\nunderstanding through inference. arXiv.\nYang Xu, Yu Hong, Huibin Ruan, Jianmin Yao, Min Zhang, and Guodong Zhou. 2018. Using active learning to\nexpand training data for implicit discourse relation recognition. In EMNLP.\nLiu Yang, Qingyao Ai, Jiafeng Guo, and W Bruce Croft. 2016. anmm: Ranking short answer texts with attention-\nbased neural matching model. In ACM international on conference on information and knowledge management.\nACM.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language understanding. arXiv.\nYe Zhang, Matthew Lease, and Byron C Wallace. 2017. Active discriminative text representation learning. In\nAAAI.\nJingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin K. Tsou. 2008. Active learning with sampling by\nuncertainty and density for word sense disambiguation and text classiﬁcation. COLING.\nAppendix A: More Details and Discussions\nSentence Matching Task: Given a pair of sentences as input, the goal of the task is to judge the relation\nbetween them, such as whether they express the same meaning. In formal, we have two sentences\nA=[a1,a2,...,alA] and B=[b1,b2,...,blB], where ai and bj denote the i-th and j-th word respectively in\ncorresponding sentences, and lA and lB denote the length of corresponding sentences.\nThrough a shared word embedding matrix We ∈Rne×d, we can obtain word embeddings of input\nsentences a=[e(a1),e(a2),...,e(alA)] and b=[e(b1),e(b2),...,e(blB)], where ne denotes the vocabulary\nsize, d denotes the embedding size and e(ai) and e(bj) denote the word embedding of the i-th and j-th\nword respectively in corresponding sentences. And there is a sentence matching model M to predict a\nlabel ˆy based on a and b. When testing, we choose the label with the highest probability in prediction\ndistribution P(yi|a, b; θM) as output, where θM denotes parameters of the model M and yi denotes a\npossible label. When training, the model M is optimized by minimizing cross entropy:\nLoss = −P(y|a, b; θM) log P(y|a, b; θM)\n(8)\nwhere y denotes the golden label.\nStandard Active Learning: In a general active learning scenario, there exists a small set of labeled\ndata P and a large pool of available unlabeled data Q. P is for training a classiﬁer and can absorb new\ninstances from Q. The task for the active learning is to select instances in Q based on some criteria, and\nthen label them and add them into P, so as to maximize classiﬁer performance and minimize annotation\ncost. In the selection criteria, a measure is used to score all candidate instances in Q, and instances\nmaximizing this measure are selected into P.\nThe process is illustrated in Algorithm 1. The instance selection process is iterative, and the process\nwill repeat until a ﬁxed annotation budget is reached. At every round, there are n instances to be selected\nand labeled.\nAlgorithm 1 Active learning algorithm ﬂow.\nInput:\nlabeled data set P={∅}, unlabeled data set Q={qi}, the classiﬁer M, criteria of instance selection C,\nthe number of instances for annotation at every round n\nOutput:\nlabeled data set P={pi}, the classiﬁer M\n1: repeat\n2:\nSort Q based on M and C\n3:\nSelect top n instances from Q to label, update Q\n4:\nAdd labeled n instances into P, update P\n5:\nTrain and update classiﬁer M based on P\n6: until The annotation budget is exhausted\nWith the same amount of labeled data P, criteria for instance selection in active learning determine the\nclassiﬁer performance. Commonly, the criteria is mainly based on uncertainty criterion (uncertainty\nsampling), in which ones near decision boundaries have priority to be selected. A general uncertainty\ncriterion uses entropy, which is deﬁned as follows:\nEnt(xi) = −\nX\nk\nP(yi = k|xi) log P(yi = k|xi)\n(9)\nwhere k indexes all possible labels, xi denotes a candidate instance that is made up of a pair of sentences\nA and B in available unlabeled data Q.\nVisualization of Delete Sequence and Insert Sequence: To model the difference between two sentences,\nwe employ the subtraction of word embeddings between “Delete Sequence” and “Insert Sequence” from\nLevenshtein Distance (when we transform sentence A to sentence B by deleting and inserting tokens,\nthese tokens are added into “Delete Sequence” and “Insert Sequence” espectively). We illustrate it in\nare\nyou\nnow\nHow\nare\nyou\n-Delete\nSentence B\nWhere\n-Delete\nInsert Sequence:\nDelete Sequence:\nRemain Sequence:\nSentence A\nWhere\nare\nyou\nRemain\nRemain\nHow\n+Insert\nnow\nFigure 3: “Delete Sequence” and “Insert Sequence”.\nFigure 3.\nDatasets: We conduct experiments on three English datasets and two Chinese dataset. Table 2 provides\nstatistics of these datasets.\n(1)SNLI: an English natural language inference corpus based on image captioning.\n(2)MultiNLI: an English natural language inference corpus with greater linguistic difﬁculty and\ndiversity.\n(3)Quora: an English question matching corpus from the online question answering forum Quora.\n(4)LCQMC: an open-domain Chinese question matching corpus from the community question answer-\ning website Baidu Knows.\n(5)BQ: an in-domain Chinese corpus question matching corpus from online bank custom service logs.\ntraining\nvalidation\ntest\nSNLI\n549,367\n9,842\n9,824\nMultiNLI\n392,702\n9,815\n9,832\nQuora\n384,348\n10,000\n10,000\nLCQMC\n238,766\n8,802\n12,500\nBQ\n100,000\n1,000\n1,000\nTable 2: Statistics of sentence matching datasets.\nFigure 4: Learning curves of combining each proposed linguistic criterion with uncertainty on SNLI\ndataset.\nConﬁguration: The number of instances to select n is 100 at every round and we perform 25 rounds of\nactive learning, that is there are total of 2500 labeled instances for training in the end. Batch size is 16 for\nEnglish and 32 for Chinese, Adam is used for optimization. We evaluated performance by calculating\naccuracy and learning curves on a held-out test set (classes are fairly balanced in datasets) after all rounds.\nFigure 5: Learning curves of different instance representation methods.\nFigure 6: Learning curves of subtraction operation on Levenshtein Distance.\nCurves of Ablation Study: Figure 4 shows learning curves of combining each proposed linguistic\ncriterion with uncertainty on SNLI dataset.\nDiscussion:\n(1)Effectiveness of different instance representation methods: We validate the effectiveness of\ndifferent instance representation methods in diversity criterion on SNLI dataset. We compare our method\nwith 4 baselines: (a) using the ﬁrst word embedding layer in BERT as context-dependent representations\n(Uncontext); (b) using the subtraction between sentence vectors from auto-encoding (AE); (c) using\nthe subtraction between sentence vectors from topic model (Topic); (d) using the subtraction between\nsentence vectors from Skip-Thoughts (Skip).\nEntroy\nUncontext\nAE\nTopic\nSkip\nLM\n79.80\n80.63\n80.42\n80.54\n80.71\n80.99\nTable 3: Accuracy of different instance representation methods.\nTable 3 and Figure 5 report accuracy and learning curves respectively. We can see contextual repre-\nsentations are better than context-dependent representations. In intuition, contextual representations are\nmore exact especially when dealing with polysemy. Next, we ﬁnd our proposed method outperforms\nsentence vector based methods (Topic, AE, and Skip). It is possibly because BERT used more data to\nlearn language representations.\n(2)Effectiveness of subtraction operation on Levenshtein Distance: Here we validate the effective-\nness of the operation that uses the subtraction of word embeddings between “Delete Sequence” and “Insert\nSequence” in diversity criterion on SNLI dataset. We compare it with 4 baselines: (a) using the sum of\nword embeddings of the two sentences (Sum); (b) directly using the subtraction of word embeddings of\nthe two sentences without “Delete Sequence” and “Insert Sequence” (Sub); (c) without weight for word\nembeddings (Nowei); (d) without absolute value operation for symmetry (Noabs).\nEntroy\nSum\nSub\nNowei\nNoabs\nLM\n79.80\n80.35\n80.67\n80.29\n80.44\n80.99\nTable 4: Accuracy of subtraction operation on Levenshtein Distance.\nTable 4 and Figure 6 report accuracy and learning curves respectively. We can see subtraction operation\nis better than sum operation. It demonstrates that subtraction has better ability to capture the difference\nbetween two sentences, and provides better instance representation for diversity rank. We can see the\nresults without “Delete Sequence” and “Insert Sequence” performs a little worse, proving its necessity.\nAnd the results without weight operation for word embeddings perform worse. We can know weight\nfor meaningless tokens is effective. Besides, we can see the results without absolute value operation for\nsymmetry is worse, demonstrating absolute value operation is necessary.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-10-12",
  "updated": "2020-10-12"
}