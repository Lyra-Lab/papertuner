{
  "id": "http://arxiv.org/abs/2010.06209v3",
  "title": "Deep Reservoir Networks with Learned Hidden Reservoir Weights using Direct Feedback Alignment",
  "authors": [
    "Matthew Evanusa",
    "Cornelia Fermüller",
    "Yiannis Aloimonos"
  ],
  "abstract": "Deep Reservoir Computing has emerged as a new paradigm for deep learning,\nwhich is based around the reservoir computing principle of maintaining random\npools of neurons combined with hierarchical deep learning. The reservoir\nparadigm reflects and respects the high degree of recurrence in biological\nbrains, and the role that neuronal dynamics play in learning. However, one\nissue hampering deep reservoir network development is that one cannot\nbackpropagate through the reservoir layers. Recent deep reservoir architectures\ndo not learn hidden or hierarchical representations in the same manner as deep\nartificial neural networks, but rather concatenate all hidden reservoirs\ntogether to perform traditional regression. Here we present a novel Deep\nReservoir Network for time series prediction and classification that learns\nthrough the non-differentiable hidden reservoir layers using a\nbiologically-inspired backpropagation alternative called Direct Feedback\nAlignment, which resembles global dopamine signal broadcasting in the brain. We\ndemonstrate its efficacy on two real world multidimensional time series\ndatasets.",
  "text": "Deep Reservoir Networks with Learned Hidden\nReservoir Weights using Direct Feedback Alignment\nMatthew S. Evanusa ∗\nDepartment of Computer Science\nUniversity of Maryland\nCollege Park, MD\nmevanusa@umd.edu\nCornelia Fermüller\nInstitute for Adv. Computer Studies\nUniversity of Maryland\nCollege Park, MD\nYiannis Aloimonos\nDepartment of Computer Science\nUniversity of Maryland\nCollege Park, MD\nAbstract\nDeep Reservoir Computing has emerged as a new paradigm for deep learning,\nwhich is based around the reservoir computing principle of maintaining random\npools of neurons combined with hierarchical deep learning. The reservoir paradigm\nreﬂects and respects the high degree of recurrence in biological brains, and the\nrole that neuronal dynamics play in learning. However, one issue hampering\ndeep reservoir network development is that one cannot backpropagate through\nthe reservoir layers. Recent deep reservoir architectures do not learn hidden or\nhierarchical representations in the same manner as deep artiﬁcial neural networks,\nbut rather concatenate all hidden reservoirs together to perform traditional regres-\nsion. Here we present a novel Deep Reservoir Network for time series prediction\nand classiﬁcation that learns through the non-differentiable hidden reservoir layers\nusing a biologically-inspired backpropagation alternative called Direct Feedback\nAlignment, which resembles global dopamine signal broadcasting in the brain. We\ndemonstrate its efﬁcacy on two real world multidimensional time series datasets.\n1\nIntroduction\n1.1\nBackpropagation Struggles with Temporal Data\nWhile much progress has been made towards optimizing feed-forward networks with backpropagation\n[16], the workhorse for deep learning weight updates since its invention in the 1980s, research has\nstruggled to expand this synchronous, time-less network structure to networks that are capable of\nlearning temporal sequences. Effectiveness aside, while there are some voices in the community that\nbackpropagation can be biologically plausible [8], the available evidence seems to indicate that the\nbrain, while potentially performing an operation that reduces error according to some metric, cannot\nin fact backpropagate errors due to the synaptic structure and strict rules about symmetrical weight\nupdates [10]. Because feed-forward networks contain no memory, one way to use feed-forward\nnetworks to encode temporal data is to remove the temporal component entirely, and ﬂatten the entire\n\"series\" into one long vector. Alternatively, the current mode of training gradient-based recurrent\nneural networks (RNNs) is to directly apply the backpropagation techniques optimized for feed-\nforward non-temporal networks, to cyclical networks, in the form of RNNs or Long Short Term\n∗Corresponding author. Code available at: https://github.com/Symbiomancer\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2010.06209v3  [cs.NE]  15 Oct 2020\nMemory Networks (LSTMs) [6]. These networks can be difﬁcult to train, may require large datasets,\nand suffer from issues of vanishing gradients and a move away from biological plausibility, although\nLSTMs aim to address the vanishing gradient concerns. More recent variants of LSTMs do away\nentirely with components in an attempt to simplify the architecture in the Gated Recurrent Unit (GRU)\n[3]. Other recent branches of backpropagation-led time series learning - Transformer architectures -\nhave eschewed entirely with the recurrent network structure [18] and go back to learning \"time\" as\na concatenated vector. Even with these supercharged LSTM, GRU, and Transformer architectures,\ntime series learning remains an open challenge. Here, we encode time directly in the dynamics of\nsimpliﬁed neurons with random connectivity, directly attempting to encode the dynamical system of\nthe real world in the dynamical system of the network - with the hopes of gleaning insights into how\nthe brain does this.\n1.2\nReservoir Computing\nReservoir computing [11] is a paradigm for recurrent neural network learning that tries to move away\nand relax the rigid per-time-step gradient updates of LSTMs. In the reservoir paradigm, there are three\ncomponents: an input weight matrix (generally untrained) that takes in the input and expands it into a\nhigher dimensional \"reservoir\". This second component, the \"reservoir\", contains random feedback\nconnections, with the maximum eigenvalue set to be less than 1, that bounces around the combined\ninput and recurrent activity for a set time period (while the activity is required to continuously \"die\nout\", in order to wash out old inputs [11]). The reservoir keeps a history of the expanded temporal\ninput as the different time steps are fed in. Third, a \"readout\" mechanism looks at the patterns of\nactivity of the reservoir and performs some learning, which can be a regression or classiﬁcation task.\nFor this paper, we show the results of a classiﬁcation task, although chaotic time series are of high\ninterest to the reservoir community [15].\n1.3\nCurrent Deep Reservoir Computing Frameworks Do Not Allow for Error-Based Hidden\nTemporal Representation Learning\nIt is well understood that the deep layers of the brain allow for increasingly more complex feature\nextraction, across multiple domain input types [1]. In the same way that deep ANNs leveraged deeper\nnetworks for more complex feature extraction, Deep ESNs [5] attempt to leverage the same hierarchy\nto extract temporal features from input time series. However, the current state of deep reservoirs does\nnot learn hidden representations in the same manner that deep learning of ANNs does, for the simple\nreason that one cannot backpropagate gradients through the reservoir-layers. Unsupervised learning\nof these connections does exist in the form of PCA [12], although this does not allow for the same\nkind of powerful prediction-based error learning as in the hidden layers of deep ANNs. Here, we take\none step towards a deep reservoir that can learn hidden representations with an error generated by the\nprediction, via direct feedback alignment. This affords the capability of learning complex temporal\nhidden representations from time series data.\n2\nContributions\nOur contributions are as follows:\n• Introduce a novel Deep Reservoir Computer (Deep ESN), with novel trained hidden weights\nusing Direct Feedback Alignment\n• Demonstrate that Direct Feedback Alignment can induce learning through extremely non-\ndifferentiable jumps, here over randomly connected neuron pools\n3\nProposed Architecture\nReservoir networks consists of the rate encoding variant known as Echo State Networks (ESN)\n[7], and a spike (event) based variant, known as Liquid State Machines (LSM) [13], discovered\nindependently. Here we adopt the ESN approach, although transferring to a LSM is in development.\nEach reservoir updates according to the following formula of a simpliﬁed reservoir taken from [17]:\nxt = (1 −α)xt−1 + αf(Winut + Wrecxt−1)\n(1)\n2\nFigure 1: A schematic of the Deep Reservoir Computing with Learned Hidden Representations using\nFeedback Alignment framework. The hidden layer representations, stored as the Win input matrices\nto the reservoir-layers, are learned through the direct feedback alignment learning mechanism [14].\nThe error for the last layer is directly fed back to the previous layers through a random matrix B into\nthe corresponding Win weight matrix for that reservoir-layer, and the network learns to align with this\nrandom matrix. The weights to the output from the last reservoir layer are computed using the standard\ndelta rule for single-layer ANNs, where x is the activity vector of the last reservoir. The yellow lines\nindicate learned connection weights. The blue circles are the randomly connected reservoir for that\nlayer (while shown with identical connectivity here, they are not necessarily identical). B is a random\nfeedback matrix per [10, 14] with appropriate dimensions, η is the learning rate. Shown here are two\nreservoir-layers, although in our experiments we run with deeper networks effectively.\nyt = Woutxt,\n(2)\nwhere yt is the output vector at time t, xt is the N-dimensional vector of the states of each reservoir\nneuron at time t, ut is the input vector at time t, α is the leak rate in the interval (0,1), f(·) is\na saturating non-linear activation function (here we use a sigmoid), and Win, Wrec, Wout are the\ninput-to-reservoir, reservoir-to-reservoir, and reservoir-to-output weight matrices, respectively. If the\ninput vector has length A, and the output vector has length B, and the reservoir vector size is N, then\nthe dimensions of Win, Wrec, Wout are N × A, N × N, and B × N, respectively,\n3.1\nFeedback Alignment and Direct Feedback Alignment\nFeedback alignment (FA) has been shown to be a simple yet powerful tool in the quest to move\ntowards more local-update and biologically inspired variants of backpropagation, while at the same\ntime, achieving the same stated end goal of reduction of prediction error for some task. In the\noriginal paper [10], the authors point out a few of the major weaknesses of relating backpropagation\nto biological processes: the fact that these updates must occur in synchronous lock-step, that the\nsynaptic updates must coordinate across multiple neuron layers in a chain to deliver the correct\ngradient update, and that these updates require weight updates that are symmetric with the output of\nthe neuron. Feedback alignment attempts to tackle the issue of symmetric weight updates by showing\none can simply use a random feedback matrix for the gradient updates, completely unrelated to the\nfeed-forward activity, and still wind up with weight changes within 90 degrees of the true gradient\nupdate. Direct Feedback Alignment [14] takes this idea one step further, and does away with the\nlocality requirements of FA. There, it was shown that not only can this matrix be random, but it\nneed not come from the layer above: all the updates can be \"projected\" from the output layer. This\naddition throws FA and DFA much closer towards biological theories about reward-modulated STDP\nwith dopamine projections [9, 4], and other neuromodulators, with the error signal e acting as the\nglobally-generated neuromodulator.\n3\n3.2\nTraining the Deep ESN with Direct Feedback Alignment\nFor the Deep ESN formulation [5], reservoirs are stacked in layers (which we refer to as \"reservoir-\nlayers\"), analogous to how perceptrons are stacked for deep ANNs. Each sub-reservoir is given its\nown unique weight matrices. The reservoir equations in (1) and (2) remain the same for all hidden\nreservoir-layers but there are two differences. First the size of xt: for the input reservoir-layer, xt is\nthe size of the input vector at time t, whereas for \"hidden\" reservoirs, xt is the size of the preceding\nreservoir-layer. This also is analogous to deep ANNs and perceptrons, as each layer takes in as input\nthe output from the last layer. Second, in the deep formulation, there is only a single Wout matrix\nat the terminal reservoir-layer. The Win of each subsequent non-terminal reservoir-layer serves as\nthe Wout for the previous reservoir-layer. At regularly sampled time intervals, the correct label y is\ncompared against the output of the network, y′. The last layer of the network is trained using the\ntraditional delta rule for perceptron learning (i.e., the last layer of a deep ANN) which reduces error\nvia gradient descent; the Win matrices of each preceding reservoir-layer are trained using the direct\nfeedback alignment algorithm. The hypothesis is that the error from the last output layer will \"align\"\nthese intermediary reservoir-layer weights to reduce the output error, even though the \"true\" gradient\nis not propagated backwards through all time steps. We illustrate the architecture in ﬁgure 1 for a\nsimple two-reservoir-layer network. We refer to the deep ESN trained with DFA as DFA-DeepESN.\nThe values of the input weight matrix leading into reservoir i, Wi, are updated at each update step\nusing the direct feedback alignment update:\n∆Wi = ηeBT\ni\n(3)\nwhere η is the learning rate, e is the last-layer error, and BT is the transpose of the random matrix of\nappropriate dimension (See Fig. 1).\n4\nResults\nWe tested our network on several challenging real-world high-dimensional input time-series classiﬁ-\ncation datasets: BasicMotion and ERing, which are taken from the UEA Multivariate Time Series\ndatabase [2] and are freely and publicly available online. The results are shown below. BasicMotions\nhas 6 input dimensions per time step and 4 outputs corresponding to different movements (walking,\nrunning, standing, and playing badminton) taken from a HAR sensor. ERing has 4 input dimensions\nof a prototype ﬁnger sensor and 6 outputs corresponding to the ﬁnger. For all series, the reservoir\nactivity was sampled at regular intervals for readout training to the target label for the given series.\nThe weight change updates were performed as one large batch at the end of each epoch for both\nthe last layer and the Wi weights. For BasicMotion, reservoirs of size 800 were used. For the deep\nnetworks, 4 reservoir-layers were used for all experiments. We set α to 0.1, weight decay at 10e−9,\nlearning rate η to 0.01, and learning rate decay at 10e−7 per epoch.\nBasicMotions\ntrain\ntest\nSingle-Reservoir\n82.5\n77.5\nDeepESN w/out DFA\n100\n97.5\nDFA-DeepESN\n100.0\n100.0\nERing\ntrain\ntest\nSingle-Reservoir\n68.15\n73.3\nDeepESN w/out DFA\n93.3\n78.15\nDFA-DeepESN\n96.7\n79.26\nTable 1: Results of the accuracy of variants of the network, either without DFA or non-deep reservoirs,\non both the BasicMotions and ERing datasets. Results are shown as % correct classiﬁcation accuracy.\nConclusion and Future Work\nHere we propose a novel deep reservoir network with learned hidden weight matrices (DFA-\nDeepESN), using the Direct Feedback Alignment method. Our hope is that this work will allow\nfuture recurrent networks that have highly non-differentiable components to train effectively towards\ntemporal predictions. Future work will involve analysis of the hidden temporal features themselves,\nand testing of alternative backpropagation alternatives for the weight training.\n4\nAcknowledgments and Disclosure of Funding\nThe authors would like to thank Vaishnavi Patil for her invaluable assistance in this endeavor. This\nwork was supported in part by NSF award DGE-1632976.\nReferences\n[1] C. A. Atencio, T. O. Sharpee, and C. E. Schreiner. Hierarchical computation in the canonical\nauditory cortical circuit. Proceedings of the National Academy of Sciences, 106(51):21894–\n21899, 2009.\n[2] A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom, P. Southam, and E. Keogh.\nThe uea multivariate time series classiﬁcation archive, 2018. arXiv preprint arXiv:1811.00075,\n2018.\n[3] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and\nY. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078, 2014.\n[4] N. Frémaux and W. Gerstner. Neuromodulated spike-timing-dependent plasticity, and theory of\nthree-factor learning rules. Frontiers in neural circuits, 9:85, 2016.\n[5] C. Gallicchio and A. Micheli. Deep echo state network (deepesn): A brief survey. arXiv preprint\narXiv:1712.04323, 2017.\n[6] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–\n1780, 1997.\n[7] H. Jaeger. The “echo state” approach to analysing and training recurrent neural networks-with an\nerratum note. Bonn, Germany: German National Research Center for Information Technology\nGMD Technical Report, 148(34):13, 2001.\n[8] B. J. Lansdell, P. R. Prakash, and K. P. Kording. Learning to solve the credit assignment\nproblem. arXiv preprint arXiv:1906.00889, 2019.\n[9] R. Legenstein, D. Pecevski, and W. Maass. A learning theory for reward-modulated spike-timing-\ndependent plasticity with application to biofeedback. PLoS Comput Biol, 4(10):e1000180, 2008.\n[10] T. P. Lillicrap, D. Cownden, D. B. Tweed, and C. J. Akerman. Random synaptic feedback\nweights support error backpropagation for deep learning. Nature communications, 7(1):1–10,\n2016.\n[11] M. Lukoševiˇcius and H. Jaeger. Reservoir computing approaches to recurrent neural network\ntraining. Computer Science Review, 3(3):127–149, 2009.\n[12] Q. Ma, L. Shen, and G. W. Cottrell. Deep-esn: A multiple projection-encoding hierarchical\nreservoir computing framework. arXiv preprint arXiv:1711.05255, 2017.\n[13] W. Maass. Liquid state machines: motivation, theory, and applications. In Computability in\ncontext: computation and logic in the real world, pages 275–296. World Scientiﬁc, 2011.\n[14] A. Nøkland. Direct feedback alignment provides learning in deep neural networks. In Advances\nin neural information processing systems, pages 1037–1045, 2016.\n[15] J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott. Model-free prediction of large spatiotem-\nporally chaotic systems from data: A reservoir computing approach. Physical review letters,\n120(2):024102, 2018.\n[16] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating\nerrors. nature, 323(6088):533–536, 1986.\n[17] Z. Tong and G. Tanaka. Reservoir computing with untrained convolutional neural networks\nfor image recognition. In 2018 24th International Conference on Pattern Recognition (ICPR),\npages 1289–1294. IEEE, 2018.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in neural information processing systems,\npages 5998–6008, 2017.\n5\n",
  "categories": [
    "cs.NE",
    "cs.LG"
  ],
  "published": "2020-10-13",
  "updated": "2020-10-15"
}