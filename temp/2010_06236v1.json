{
  "id": "http://arxiv.org/abs/2010.06236v1",
  "title": "Average Cost Optimal Control of Stochastic Systems Using Reinforcement Learning",
  "authors": [
    "Jing Lai",
    "Junlin Xiong"
  ],
  "abstract": "This paper addresses the average cost minimization problem for discrete-time\nsystems with multiplicative and additive noises via reinforcement learning. By\nusing Q-function, we propose an online learning scheme to estimate the kernel\nmatrix of Q-function and to update the control gain using the data along the\nsystem trajectories. The obtained control gain and kernel matrix are proved to\nconverge to the optimal ones. To implement the proposed learning scheme, an\nonline model-free reinforcement learning algorithm is given, where recursive\nleast squares method is used to estimate the kernel matrix of Q-function. A\nnumerical example is presented to illustrate the proposed approach.",
  "text": "Average Cost Optimal Control of Stochastic Systems Using\nReinforcement Learning\nJing Lai, IEEE Student Member, Junlin Xiong, IEEE Member\nAbstract— This paper addresses the average cost minimiza-\ntion problem for discrete-time systems with multiplicative and\nadditive noises via reinforcement learning. By using Q-function,\nwe propose an online learning scheme to estimate the kernel\nmatrix of Q-function and to update the control gain using the\ndata along the system trajectories. The obtained control gain\nand kernel matrix are proved to converge to the optimal ones.\nTo implement the proposed learning scheme, an online model-\nfree reinforcement learning algorithm is given, where recursive\nleast squares method is used to estimate the kernel matrix of\nQ-function. A numerical example is presented to illustrate the\nproposed approach.\nI. INTRODUCTION\nReinforcement learning (RL) [1] has been widely applied\nfor solving optimizing problems in uncertain environments.\nRL has performed impressively in many challenging tasks\nincluding playing Atari games [2], [3], playing video game\nDoom [4] and has been extensively studied to control dy-\nnamical systems [5], [6]. Recently, extensions to RL-based\ncontrol schemes for stochastic systems have sprung up as\nwell [7]–[10].\nAs system parameter uncertainties are often modeled\nas multiplicative noises [11], [12] and some external dis-\nturbances are modeled as additive noises [13], stochastic\nsystems subjected to multiplicative and additive noises have\nbeen studied extensively by taking advantage of RL meth-\nods [12], [14], [15]. For systems with additive noises, the\nauthors of [16] evaluated the value function ﬁrst and then Q-\nfunction. The control policies were updated with respect to\nthe average of all previous Q-function estimations. Compared\nwith [16], the authors of [9] developed a model-free RL\nalgorithm for the stochastic linear quadratic regulator (LQR)\nproblem with additive noises in both system states and\nsystem measurements. In [10], RL algorithms were proposed\nto solve a class of coupled algebraic Riccati equations for\nlinear stochastic dynamics and to minimize the variance of\nthe cost function. For systems with multiplicative noises,\nthe authors of [17] presented the value iteration learning\nalgorithm to ﬁnd the optimal control policy where system\nmatrices were partially needed to implement the algorithm.\nIn [12], policy iteration were employed to solve a zero-\nsum dynamic linear quadratic game where systems were\nsubjected to multiplicative noise terms. For systems suffering\nfrom both multiplicative and additive noises, a RL-based\nThis work was supported by National Natural Science Foundation of\nChina under Grant 61773357\nJing. Lai and Junlin. Xiong are with the Department of Automation, Uni-\nversity of Science and Technology of China, Hefei 230026, China. (E-mail:\nlj120@mail.ustc.edu.cn, xiong77@ustc.edu.cn).\nmodel-free control methodology was proposed to solve the\ndiscount-optimal control problem for continuous-time linear\nstochastic systems in [18]. Motivated by [18], the authors\nof [15] considered the adaptive optimal control problem for\nstochastic systems with complementally unmeasurable states.\nBased on the separation principle, a data-driven optimal\nobserver and an off-policy data-driven RL algorithm were\ncombined to yield the optimal control policy without the\nknowledge of the system matrices.\nIn this paper, the average cost optimal control problem\nis investigated for a class of discrete-time stochastic systems\nsubject to both multiplicative and additive noises. This paper\naims to ﬁnd the optimal admissible control policy in the sense\nof minimizing the average expected cost. Firstly, with the\nsystem matrices known, the control gain and kernel matrix of\nvalue function sequences are evaluated in an ofﬂine manner.\nThe obtained sequences are proved to converge to the optimal\ncontrol gain and to the solution to the stochastic algebra\nriccati equation (SARE), respectively. Then, by using Q-\nfunction, we estimate the kernel matrix of Q-function and\nupdate the control gain in an online manner without the\nknowledge of the system matrices. Finally, an RL-based al-\ngorithm is presented to implement the online learning scheme\n, where recursive least squares method is used to estimate\nthe iterative kernel matrix of Q-function. Our algorithm\nremoves the assumption that the noises are measurable as\nneeded in [15], [19], where optimal control problems were\nconsidered for continuous-time systems with multiplicative\nand additive noises. Moreover, compared with [20], [21], the\njudicious selection of the discount factor has been avoided.\nA numerical example is presented to illustrate the obtained\nresults.\nNotation: The notation Rn×m denotes the set of n × m\nreal matrices. For matrix or vector X, X⊤denotes its\ntranspose, and ρ(X) denotes its spectral radius. The notation\n∥.∥denotes the L2 norm for both matrices and vectors. The\ntrace of a square matrix X is denoted by tr(X). The notation\nI denotes an identity matrix with appropriate dimensions,\n1 is a column vector with all of its elements being 1. For\nmatrix X, X > 0 (resp. X ≥0) means that X⊤= X\nand X is positive (resp. positive semi-) deﬁnite. Let ⊗\ndenote the Kronecker product and E denote the mathematical\nexpectation. For symmetric matrix X ∈Rn×n, vech(X) =\n[X11, X12, . . . , X1n, X22, X23, . . . , Xn−1,n, Xnn], vecs(X)\n=\n[X11, 2X12, . . . , 2X1n, X22, 2X23, . . . , 2Xn−1,n, Xnn],\nand both vech(X) and vech(X) are in the set of R\nn(n+1)\n2\n.\narXiv:2010.06236v1  [eess.SY]  13 Oct 2020\nII. PROBLEM DESCRIPTION\nConsider the following discrete-time stochastic system\nxk+1 = (A +\np\nX\ni=1\nαikAi)xk + (B +\nq\nX\nj=1\nβjkBj)uk + dk (1)\nwhere xk ∈Rn is the system state at time k, uk ∈Rm\nis the control input, x0 is the system initial state, which is\na Gaussian random vector with zero mean and covariance\nX0 ≥0. The matrices A, Ai ∈Rn×n, B, Bj ∈Rn×m are\nsystem matrices. The system noise sequence {(αik, βjk, dk) :\ni = 1, 2, . . . , p, j = 1, 2, . . . , q, k = 0, 1, 2, . . .} is deﬁned\non a given complete probability space (Ω, F, P). We assume\nthat there exist linear admissible control policies for system\n(1). Furthermore, assume that: 1)multiplicative noises αik\nand βjk are scalar Gaussian random variables with zero\nmeans and covariances ¯αi and ¯βj, respectively; 2) additive\nnoise dk is a Gaussian random vector with zero mean and\ncovariance D > 0; 3) x0, αik, βjk and dk are mutually\nindependent.\nDeﬁnition 1: System (1) with control input uk ≡0 is\ncalled asymptotically square stationary (ASS) if there exists\na matrix X > 0 such that ∥lim\nk→∞E(xkx⊤\nk ) −X∥= 0.\nDeﬁnition 2: A control policy is admissible if and only if\nthe system followed the control policy is ASS.\nRemark 1: With an admissible control policy u = Lx, we\nhave\nE(xk+1x⊤\nk+1) = (A + BL)E(xkx⊤\nk )(A + BL)⊤\n+\np\nX\ni=1\n¯αiAiE(xkx⊤\nk )A⊤\ni +\nq\nX\nj=1\n¯βjBjLE(xkx⊤\nk )L⊤B⊤\nj + D.\nHence, E(xkx⊤\nk ) is positive deﬁnite.\nDeﬁne the value function associated with an admissible\ncontrol policy u = Lx as\nJ(xk) = E\n∞\nX\nt=k\n(c(xt, ut) −λ),\n(2)\nwhere the term c(xt, ut) = x⊤\nt Qxt + u⊤\nt Rut with Q ≥\n0, R > 0 is the one step cost, and λ is the average expected\ncost\nλ = lim\nN→∞\n1\nN E\nN\nX\nt=0\n(c(xt, ut)),\ni.e., the expected quadratic running cost in the steady state.\nThis paper aims to ﬁnd the optimal admissible control\npolicy u∗= L∗x in the sense of minimizing the average\nexpected cost λ.\nLemma 1: Let the control policy u = Lx be admissible.\nThen λ = tr(PD), where P > 0 is the unique solution to\nthe following stochastic Lyapunov equation (SLE)\nP = (A + BL)⊤P(A + BL) +\np\nX\ni=1\n¯αiA⊤\ni PAi\n+\nq\nX\nj=1\n¯βjL⊤B⊤\nj PBjL + Q + L⊤RL.\n(3)\nProof: This lemma is a extension of [22, Section III],\nand therefore is omitted here.\nBased on equation (2), one has a Bellman equation for the\nvalue function J(.)\nJ(xk) = E\n\u0000c(xk, uk)\n\u0001\n−λ + J(xk+1).\n(4)\nLemma 2: Let the control policy u = Lx be admissible.\nThen, the value function (2) can be written as\nJ(xk) = E(x⊤\nk Pxk) + ¯s,\n(5)\nwhere P > 0 is the unique solution to SLE (3) and the term\n¯s is a const.\nProof:\nWithout loss of generality, suppose J(xk) =\nE(x⊤\nk ¯Pxk) + sk, where sk is independent on xk. Let the\ncontrol policy u = Lx be admissible, xk+1 is given by\nxk+1 = (A + BL)xk +\np\nX\ni=1\nαikAixk +\nq\nX\nj=1\nβjkBjLxk + dk.\nTherefore,\nE(x⊤\nk+1 ¯Pxk+1) = E\n\u0000x⊤\nk\n\u0000(A + BL)⊤¯P(A + BL)\n+\np\nX\ni=1\n¯αiA⊤\ni ¯PAi +\nq\nX\nj=1\n¯βjL⊤B⊤\nj ¯PBjL\n\u0001\nxk\n\u0001\n+ tr( ¯PD).\nBased on the Bellman equation for the value function J(.),\none has\nJ(xk) −J(xk+1) = E\n\u0000x⊤\nk (Q + L⊤RL)xk\n\u0001\n−λ\n= E(x⊤\nk ¯Pxk) + sk −E(x⊤\nk+1 ¯Pxk+1) + sk+1\n= E\n\u0000x⊤\nk\n\u0000 ¯P −(A + BL)⊤¯P(A + BL) −\np\nX\ni=1\n¯αiA⊤\ni ¯PAi\n−\nq\nX\nj=1\n¯βjL⊤B⊤\nj ¯PBjL\n\u0001\nxk\n\u0001\n+ sk −sk+1 −tr( ¯PD).\nBy matching terms, one has\n¯P = (A + BL)⊤¯P(A + BL) +\np\nX\ni=1\n¯αiA⊤\ni ¯PAi\n+\nq\nX\nj=1\n¯βjL⊤B⊤\nj ¯PBjL + Q + L⊤RL.\nBased on the admissibility of the control policy u = Lx\nand Lemma 1, we obtain ¯P = P, which means that the\nkernel matrix of the average expected cost λ is equal to the\nkernel matrix of the value function J(.) in face of the same\nadmissible control policy. Thus, λ = tr(PD) = tr( ¯PD) and\nsk = sk+1. This completes the proof.\nRemark 2: The authors of [9], [16] considered the value\nfunction (2) to be quadratic functions without the constant\n¯s. Here we give a particular proof to show that the constant\nmay exist independent of xk. Note that one can also prove\nthe existence of the term ¯s by the method in the proof of\n[17, Lemma 1].\nFrom Lemma 1 and Lemma 2, one sees that minimizing\nthe average expected cost λ is equivalent to minimizing\nthe value function J(.) in the sense that the kernel matrix\nP is equal in face of the same admissible control policy.\nHence, we convert the original problem into ﬁnding the\noptimal admissible control policy u∗= L∗x in the sense\nof minimizing the value function J(.).\nPutting the value function (5) into equation (4), one obtains\nthe Bellman equation in terms of the kernel matrix P of the\nvalue function\nE(x⊤\nk Pxk) = E\n\u0000c(xk, uk)\n\u0001\n−λ + E(x⊤\nk+1Pxk+1).\n(6)\nThe stochastic LQR problem can be solved based on a\nstochastic algebra Riccati equation (SARE) given in the\nfollowing lemma.\nLemma 3: The optimal control gain for the stochastic\nLQR problem is\nL∗= −(R + B⊤P ∗B +\nq\nX\nj=1\n¯βjB⊤\nj P ∗Bj)−1B⊤P ∗A, (7)\nand P ∗> 0 is the unique solution to the following SARE\nP ∗= Q + A⊤P ∗A +\np\nX\ni=1\n¯αiA⊤\ni P ∗Ai −A⊤P ∗B\n× (R + B⊤P ∗B +\nq\nX\nj=1\n¯βjB⊤\nj P ∗Bj)−1B⊤P ∗A. (8)\nHence, the minimal average expected cost is given as\nλ∗= tr(P ∗D).\n(9)\nProof:\nThe proof for Lemma 3 is based on the ﬁrst\norder necessary condition, and therefore is omitted here.\nIII. MODEL-BASED SCHEME TO SOLVE STOCHASTIC LQR\nIn this section, a model-based iterative scheme is provided\nto ﬁnd the minimum P ∗and the optimal control gain L∗.\nA set of control gains are evaluated in an off-line manner\nin Lemma 4 requiring complete knowledge of the system\nmatrices.\nLemma 4: Let the initial control gain L(0) be admissible.\nConsider the two sequences {P (τ)}∞\nτ=0 and {L(τ)}∞\nτ=1 ob-\ntained via solving\nP (τ) = (A + BL(τ))⊤P (τ)(A + BL(τ)) +\np\nX\ni=1\n¯αiA⊤\ni P (τ)Ai\n+\nq\nX\nj=1\n¯βj(L(τ))⊤B⊤\nj P (τ)BjL(τ) + (L(τ))⊤RL(τ) + Q\n(10)\nand\nL(τ+1) = −\n\u0000R + B⊤P (τ)B +\nq\nX\nj=1\n¯βjB⊤\nj P (τ)Bj\n\u0001−1B⊤P (τ)A.\n(11)\nThen, the following statements hold:\n1) P ∗≤P (τ+1) ≤P (τ);\n2)\nlim\nτ→∞P (τ) = P ∗,\nlim\nτ→∞L(τ) = L∗, where P ∗is the\nunique solution to SARE (8) and L∗is computed as\n(7);\n3) L∗and L(τ) are admissible.\nProof: The proof of Lemma 4 is a simpliﬁed version\nof [23, Lemma 6], by letting the discount factor γ = 1.\nRemark 3: Different from [20], [21], [23], the need for\njudicious selection the discount factor has been avoided and\nthe iterative control gains are admissible naturally with the\ncondition that the initial control gain is admissible.\nIV. MODEL-FREE SCHEME TO SOLVE STOCHASTIC LQR\nIn this section, in order to avoid resorting to the system\nmatrices, a new model-free learning scheme is proposed to\nsolve the stochastic LQR problem by using Q-function.\nBased on Bellman equation (4), deﬁne a Q-function as\nQ(xk, ηk) = E\n\u0000c(xk, ηk)\n\u0001\n−λ + J(xk+1),\n(12)\nwhere ηk is an arbitrary control input at time k and the\ncontrol policy u = Lx is followed from time k +1 onwards.\nIf ηk = uk, one knows that\nQ(xk, uk) = J(xk).\n(13)\nBased on Lemma 2 and system (1), the above Q-function\nbecomes\nQ(xk, uk) = E\n\u0000 \u0014xk\nuk\n\u0015⊤\u0014Hxx\nHxu\nHux\nHuu\n\u0015 \u0014xk\nuk\n\u0015 \u0001\n+ ¯s\n≜E\n\u0000 \u0014\nxk\nuk\n\u0015⊤\nH\n\u0014\nxk\nuk\n\u0015 \u0001\n+ ¯s,\n(14)\nwith\nHxx = Q + A⊤PA +\np\nX\ni=1\n¯αiA⊤\ni PAi\nHxu = A⊤PB = H⊤\nux\nHuu = R + B⊤PB +\nq\nX\nj=1\n¯βjB⊤\nj PBj.\nDenote the optimal Q-function as [1]\nQ∗(xk, uk) = E\n\u0000c(xk, uk)\n\u0001\n−λ∗+ J∗(xk+1).\nThrough solving\n∂Q∗(xk,uk)\n∂uk\n= 0, one obtains the optimal\ncontrol gain\nL∗= −(H∗\nuu)−1H∗\nux,\n(15)\nwhere H∗\nuu = R + B⊤P ∗B + Pq\nj=1 ¯βjB⊤\nj P ∗Bj, Hux =\nB⊤P ∗A, and P ∗satisﬁes SARE (8). Therefore, from (15)\nwe know that the optimal control gain can be obtained by\nﬁnding the optimal kernel matrix of Q-function.\nFrom (5), (13), (14) and the positive deﬁniteness of\nE(xkx⊤\nk ) in Remark 1, we get\nP =\n\u0014I\nL\n\u0015⊤\nH\n\u0014I\nL\n\u0015\n.\n(16)\nHence, we obtain the Bellman function for Q-function\nQ(xk, uk) = E\n\u0000c(xk, uk)\n\u0001\n−tr(H\n\u0014I\nL\n\u0015\nD\n\u0014I\nL\n\u0015⊤\n)\n+ Q(xk+1, uk+1)\n(17)\nbased on Lemma 1, (12), (13) and (16). Furthermore, substi-\ntuting (14) into (17), one has the Bellman equation in terms\nof the kernel matrix H of Q-function\nE\n\u0000 \u0014xk\nuk\n\u0015⊤\nH\n\u0014xk\nuk\n\u0015 \u0001\n= E\n\u0000c(xk, uk)\n\u0001\n−tr\n\u0000H\n\u0014I\nL\n\u0015\nD\n\u0014I\nL\n\u0015⊤\u0001\n+ E\n\u0000 \u0014xk+1\nuk+1\n\u0015⊤\nH\n\u0014xk+1\nuk+1\n\u0015 \u0001\n.\n(18)\nIn the following lemma, we give a model-free learning\nscheme, which is inspired by equations (18) and (15), to learn\nthe optimal control policy online where no system matrices\nare needed.\nLemma 5: Let the initial control gain L(0) be admissible.\nConsider the two sequences {H(τ)}∞\nτ=0 and {L(τ)}∞\nτ=1 ob-\ntained through the following two steps:\n1) estimate H(τ) by solving\nE\n\u0000\"\nxk\nu(τ)\nk\n#⊤\nH(τ)\n\"\nxk\nu(τ)\nk\n#\n\u0001\n= E\n\u0000c(xk, u(τ)\nk )\n\u0001\n−tr\n\u0000H(τ)\n\u0014\nI\nL(τ)\n\u0015\nD\n\u0014\nI\nL(τ)\n\u0015⊤\u0001\n+ E\n\u0000\"\nxk+1\nu(τ)\nk+1\n#⊤\nH(τ)\n\"\nxk+1\nu(τ)\nk+1\n#\n\u0001\n;\n(19)\n2) update the control gain through\nL(τ+1) = −(H(τ)\nuu )−1H(τ)\nux .\n(20)\nThen, lim\nτ→∞H(τ) = H∗=\n\u0014H∗\nxx\nH∗\nxu\nH∗\nux\nH∗\nuu\n\u0015\nand lim\nτ→∞L(τ) =\nL∗, where\nH∗\nxx = Q + A⊤P ∗A +\np\nX\ni=1\n¯αiA⊤\ni P ∗Ai\nH∗\nxu = A⊤P ∗B = (H∗\nux)⊤\nH∗\nuu = R + B⊤P ∗B +\nq\nX\nj=1\n¯βjB⊤\nj P ∗Bj\nwith P ∗being the unique solution to SARE (8).\nProof:\nUsing c(xk, u(τ)\nk ) = x⊤\nk Qxk + (u(τ)\nk )⊤Ru(τ)\nk\nand u(τ)\nk\n= L(τ)xk, equation (19) becomes\nE\n\u0000x⊤\nk\n\u0014\nI\nL(τ)\n\u0015⊤\nH(τ)\n\u0014\nI\nL(τ)\n\u0015\nxk\n\u0001\n= E(x⊤\nk Qxk + (u(τ)\nk )⊤Ru(τ)\nk )\n−tr\n\u0000H(τ)\n\u0014 I\nL(τ)\n\u0015\nD\n\u0014 I\nL(τ)\n\u0015⊤\u0001\n+ E\n\u0000x⊤\nk+1\n\u0014 I\nL(τ)\n\u0015⊤\nH(τ)\n\u0014 I\nL(τ)\n\u0015\nxk+1\n\u0001\n.\nThe above equation can be written as\nE(x⊤\nk P (τ)xk) = E(x⊤\nk Qxk + (u(τ)\nk )⊤Ru(τ)\nk ) −tr(P (τ)D)\n+ E(x⊤\nk+1P (τ)xk+1)\n(21)\naccording to equation (16). Substituting system (1) and\nu(τ)\nk\n= L(τ)xk into (21), one gets\nE(x⊤\nk P (τ)xk)\n= E\n\u0010\nx⊤\nk\n\u0000(A + BL(τ))⊤P (τ)(A + BL(τ)) +\np\nX\ni=1\n¯αiA⊤\ni PAi\n+ (L(τ))⊤(\nq\nX\nj=1\n¯βjB⊤\nj PBj + R)L(τ) + Q\n\u0001\nxk\n\u0011\n.\nFrom Remark 1, one knows that E(xkx⊤\nk ) > 0. Hence, we\nconclude that equation (19) is equivalent to equation (10).\nMoreover, the equation (20) is equivalent to equation (11)\naccording to equations (14) and (15). From Lemma 4, we\nknow that lim\nτ→∞P (τ) = P ∗and lim\nτ→∞L(τ) = L∗. Hence, we\nconclude that lim\nτ→∞H(τ) = H∗and lim\nτ→∞L(τ) = L∗. This\ncompletes the proof.\nRemark 4: Compared with the iterative scheme in Lemma\n4, the scheme in Lemma 5 evaluates the iterative matrix\nH(τ) and updates the control gain in an online manner only\nusing system states and control inputs without requiring any\nknowledge of the system matrices.\nV. IMPLEMENTATION OF ONLINE MODEL-FREE RL\nSCHEME\nIn this section, recursive least squares (RLS) [24] is\nleveraged to estimate the kernel matrix H(τ) of Q-function.\nThe implementation of the model-free learning scheme in\nLemma 5 is given in Algorithm 1.\nVectorize the equation (19) as\n\u0010\nE\n\u0000φ(z(τ)\nk )\n\u0001\u0011⊤\nvecs(H(τ)) =\n\u0010\nE\n\u0000φ(z(τ)\nk+1)\n\u0001\u0011⊤\nvecs(H(τ))\n+ E\n\u0000c(z(τ)\nk )\n\u0001\n−\n\u0000vech(κ(τ))\n\u0001⊤vecs(H(τ)),\nwhere\nz(τ)\nk\n= [x⊤\nk (u(τ)\nk )⊤]⊤∈Rn+m=r, φ(z(τ)\nk ) = vech\n\u0000z(τ)\nk (z(τ)\nk )⊤\u0001\n,\nand\nκ(τ) =\n\u0014 I\nL(τ)\n\u0015\nD\n\u0014 I\nL(τ)\n\u0015⊤\n.\nThe iterative kernel matrix H(τ) of Q-function is estimated\nusing data which are generated under the admissible control\npolicy uk = L(τ)xk for N time steps. Ignoring the expecta-\ntion operation, the general bath least squares (BLS) estimator\nof H(τ) is given by [25]:\nvecs(H(τ)) =\n\u0000(Φ(τ))⊤(Φ(τ) −¯Φ(τ) + K(τ))\n\u0001−1\n× (Φ(τ))⊤Υ(τ),\n(22)\nwhere Φ(τ), ¯Φ(τ) ∈RN× r(r+1)\n2\nand Υ(τ) ∈RN are the data\nmatrices constructed by\nΦ(τ) =\nh\nφ(z(τ)\n0 )\nφ(z(τ)\n1 )\n· · ·\nφ(z(τ)\nN−1)\ni⊤\n¯Φ(τ) =\nh\nφ(z(τ)\n1 )\nφ(z(τ)\n2 )\n· · ·\nφ(z(τ)\nN )\ni⊤\nΥ(τ) =\nh\nc(z(τ)\n0 )\nc(z(τ)\n1 )\n· · ·\nc(z(τ)\nN−1)\ni⊤\n,\nand K(τ) is a N × r(r+1)\n2\nmatrix whose rows are vectors\nvech(κ(τ)). Note that control input u(τ)\nk\nis dependent on\nsystem state xk linearly. Generally, it is necessary to add\na probing noise to control input u(τ)\nk\nto guarantee that the\npersistency of excitation condition holds [5].\nRemark 5: If the additive noise covariance matrix D is\nunknown in practice, one can leverage the empirical average\ncost ¯λ =\n1\nN\nPN−1\nk=0 c(z(τ)\nk ) to approximate the average ex-\npected cost λ(τ) ≜tr(P (τ)D) =\n\u0000vech(κ(τ))\n\u0001⊤vecs(H(τ)).\nIn this case, the BLS estimator of H(τ) is given as following\nvecs(H(τ)) =\n\u0000(Φ(τ))⊤(Φ(τ) −¯Φ(τ))\n\u0001−1(Φ(τ))⊤(Υ(τ) −¯λ1).\nHowever, the condition of full rank in (22) may not be\nsatisﬁed until a sufﬁcient number of states and inputs has\nbeen collected. Moreover, with the dimension of state and\ninput increasing, the inverse operation has higher compu-\ntational complexity and lower accuracy. According to the\nderivation in [26, Section 6], we use RLS to compute the\ninverses recursively, which is more time efﬁcient. Finally, the\nimplementation of the model-free learning scheme is given\nin Algorithm 1.\nAlgorithm 1 Online Model-Free RL\nInput: Admissible control gain L(0), initial state covariance\nmatrix X0, additive noise covariance matrix D, roll out\nlength N, variance σ2\nu, large positive constant ϖ, maximum\nnumber of iterations τmax, convergence tolerance ε\nOutput: The estimated optimal control gain ˆL\n1: for τ = 0 : τmax do\n2:\nPolicy Evaluation: ¡¡¡¡\n3:\nSample x0 from a Gaussian distribution with zeros\n4:\nmean and covariance X0. Let ξ0 = ϖI, ψ0 = 0\n5:\nfor k = 0 : N −1 do\n6:\nInput uk = L(τ)xk +ek into system (1) to obtain\n7:\nxk+1, where ek sampling from a Gaussian distri-\n8:\nbution with zeros mean and covariance σ2\nu. Then\n9:\nuk+1 = L(τ)xk+1\n10:\nξk+1 = ξk −\nξkφ(z(τ)\nk\n)\n\u0000φ(z(τ)\nk\n)−φ(z(τ)\nk+1)+κ(τ)\u0001⊤\nξk\n1+\n\u0000φ(z(τ)\nk\n)−φ(z(τ)\nk+1)+κ(τ)\u0001⊤\nξkφ(z(τ)\nk\n)\n11:\nψk+1 = ψk + φ(z(τ)\nk )c(xk, u(τ)\nk )\n12:\nendfor\n13:\nvecs(H(τ)) = ξNψN\n14:\nPolicy Improvement:\n15:\nL(τ+1) = −(H(τ)\nuu )−1H(τ)\nux ¡¡¡¡\n16:\nif ∥L(τ+1) −L(τ)∥< ε then\n17:\nBreak ¡¡¡¡\n18:\nendif\n19: endfor\n20: ˆL = L(τ+1)\nRemark 6: In [17], the system matrices were partially\nneeded in the implementation of the value iteration al-\ngorithm. In this paper, we have no requirement of the\nknowledge of the system matrices. In [19] and [15], the\nauthors assumed that the terms with noises were measurable.\nHere we remove this assumption. Furthermore, the need for\njudicious selection the discount factor has been avoid, which\nis necessary in [20], [21].\nVI. NUMERICAL EXAMPLE\nIn this section, a numerical example is presented to evalu-\nate the proposed method. Consider the following open-loop\nasymptotically square stationary linear discrete-time system:\nxk+1 =\n\n\n0.8672\n0.0519\n0.1028\n0.0519\n0.7576\n0.0475\n0.1028\n0.0475\n0.7681\n\nxk + I3uk\n+ α1k\n\n\n0\n−1\n0\n−1\n0\n0\n0\n0\n0\n\nxk + α2k\n\n\n0\n0\n−1\n0\n0\n0\n−1\n0\n0\n\nxk\n+ β1k\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\nuk + β2k\n\n\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\nuk + dk,\nwhere ¯α1 = ¯β1 = 0.05, ¯α2 = ¯β2 = 0.015. Let initial state\nvariance matrix X0 = I, additive noise covariance matrix\nD = 0.5I, and the weight matrices are selected as Q = R =\nI. The exact solution to SARE (8) is\nP ∗=\n\n\n1.5864\n0.0673\n0.1208\n0.0673\n1.4252\n0.0528\n0.1208\n0.0528\n1.3770\n\n\nand the optimal control gain is\nL∗=\n\n\n−0.5175\n−0.0394\n−0.0761\n−0.0404\n−0.4419\n−0.0353\n−0.0776\n−0.0352\n−0.4466\n\n.\nThus, one can obtain the optimal average cost λ∗= 2.1943\naccording to Lemma 3.\nPerform Algorithm 1 on system (1) with parameters:\nL(0) = 0, N = 42000, σ2\nu = 0.64, ϖ = 108, τmax = 10,\nand ε = 0.05. Algorithm 1 stops after four iterations and\nreturns the estimated optimal control gain\nˆL =\n\n\n−0.5192\n−0.0443\n−0.0795\n−0.0342\n−0.4437\n−0.0336\n−0.0747\n−0.0371\n−0.4497\n\n\nand the estimated optimal average cost ˆλ = 2.2159.\nTo make comparison, evaluate the following algorithms\non system (1): (a) Algorithm 1 in this paper; (b) MFLQv3\nin [16], where the authors evaluated the value function ﬁrst\nand then Q-function. The iterative control policies were\ngreedy with respect to the average of all previous Q-function\nestimations ˆQ1, . . . , ˆQi−1; (c) Q-learning in [27], where the\nkernel matrix H(i) was learned based on Bellman residual\nmethods and RLS.\nInstead of using the stop condition ε, we run the above\nthree algorithms for 10 iterations. The remaining parameter\nsettings keep unchanged, except that for MFLQv3 in [16], in\na single iteration, we use 21000 and 21000 time steps data to\nestimate the value function and Q-function, respectively. The\ncurves of ∥L(τ) −L∗∥and |λ(τ)−λ∗|\nλ∗\nare shown in Fig. 1 and\nFig. 2, respectively. From Fig 1 we can see that the control\ngain obtained by Algorithm 1 is closer to the optimal control\ngain than the other two algorithms. Furthermore, Fig.2 shows\nthat Algorithm 1 achieves much lower relative cost error.\nIteration number\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n∥L(i) −L∗∥\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAlgorithm 1\nMFLQv3 in [16]\nQ-learning in [27]\nFig. 1.\nThe distance ∥L(τ) −L∗∥\nIteration Number\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n|V (i) −V ∗|\nV ∗\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAlgorithm 1\nMFLQv3 in [16]\nQ-learning in [27]\nFig. 2.\nRelative cost error ∥λ(τ)−λ∗∥\n∥λ∗∥\nVII. CONCLUSION\nThis paper investigates the average cost optimal control\nproblem for a class of discrete-time stochastic systems. The\nsystem under consideration suffers from both multiplica-\ntive and additive noises. Both model-based and model-free\nschemes are proposed to solve the stochastic LQR. The\ncontrol policies and associated kernel matrices obtained from\nthe two schemes are proved to converge to the optimal ones.\nAn model-free RL algorithm is presented to learn the optimal\ncontrol policy in an online manner using the data of the\nsystem states and control inputs. The proposed approach is\nillustrated through a numerical example.\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduc-\ntion.\nMIT press, 2018.\n[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 2, pp. 529–533, 2015.\n[3] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep re-\ninforcement learning,” in Proceedings of Machine Learning Research,\n2016, pp. 1928–1937.\n[4] L. Jiang, D. Meng, Q. Zhao, S. Shan, and A. G. Hauptmann, “Self-\npaced curriculum learning,” in Proceedings of Association for the\nAdvance of Artiﬁcial Intelligence, 2015, pp. 2694–2700.\n[5] L. Bus¸oniu, T. de Bruin, D. Toli´c, J. Kober, and I. Palunko, “Re-\ninforcement learning for control: Performance, stability, and deep\napproximators,” Annual Reviews in Control, vol. 46, no. 9, pp. 8–28,\n2018.\n[6] K. G. Vamvoudakis, H. Modares, B. Kiumarsi, and F. L. Lewis, “Game\ntheory-based control system algorithms with real-time reinforcement\nlearning: How to solve multiplayer games online,” IEEE Control\nSystems Magazine, vol. 37, no. 1, pp. 33–52, 2017.\n[7] B. Recht, “A tour of reinforcement learning: The view from contin-\nuous control,” Annual Review of Control, Robotics, and Autonomous\nSystems, vol. 2, pp. 253–279, 2019.\n[8] B. Demirel, A. Ramaswamy, D. E. Quevedo, and H. Karl, “DeepCAS:\nA deep reinforcement learning algorithm for control-aware schedul-\ning,” IEEE Control Systems Letters, vol. 2, no. 4, pp. 737–742, 2018.\n[9] F. A. Yaghmaie and F. Gustafsson, “Using reinforcement learning\nfor model-free linear quadratic control with process and measurement\nnoises,” in Proceedings of Conference on Decision and Control, 2019,\npp. 6510–6517.\n[10] G. Jing, H. Bai, J. George, and A. Chakrabortty, “Model-free rein-\nforcement learning of minimal-cost variance control,” IEEE Control\nSystems Letters, vol. 4, no. 4, pp. 916–921, 2020.\n[11] S. Boyd, L. Ghaoui, E. Feron, and V. Balakrishnan, Linear Matrix\nInequality in Systems and Control Theory. Philadelphia: SIAM, 1994.\n[12] B. Gravell, K. Ganapathy, and T. Summers, “Policy iteration for linear\nquadratic games with stochastic parameters,” IEEE Control Systems\nLetters, vol. 5, no. 1, pp. 307–312, 2021.\n[13] T. Bian and Z.-P. Jiang, “Continuous-time robust dynamic program-\nming,” SIAM Journal on Control and Optimization, vol. 57, no. 6, pp.\n4150–4174, 2019.\n[14] E. Todorov and M. I. Jordan, “Optimal feedback control as a theory of\nmotor coordination,” Nature Neuroscience, vol. 5, no. 11, pp. 1226–\n1235, 2002.\n[15] M. Zhang, M.-G. Gan, and J. Chen, “Data-driven adaptive optimal\ncontrol for stochastic systems with unmeasurable state,” Neurocom-\nputing, vol. 397, no. 7, pp. 1 – 10, 2020.\n[16] Y. Abbasi-Yadkori, N. Lazic, and C. Szepesvari, “Model-free linear\nquadratic control via reduction to expert prediction,” in Proceedings\nof International Conference on Artiﬁcial Intelligence and Statistics,\n2019, pp. 3108–3117.\n[17] T. Wang, H. Zhang, and Y. Luo, “Stochastic linear quadratic optimal\ncontrol for model-free discrete-time systems based on Q-learning\nalgorithm,” Neurocomputing, vol. 312, no. 10, pp. 1–8, 2018. doi:\nhttps://doi.org/10.1016/j.neucom.2018.04.018\n[18] T. Bian and Z. Jiang, “Adaptive optimal control for linear stochastic\nsystems with additive noise,” in Proceedings of Chinese Control\nConference, 2015, pp. 3011–3016.\n[19] T. Bian, Y. Jiang, and Z. Jiang, “Adaptive dynamic programming\nfor stochastic systems with state and control dependent noise,” IEEE\nTransactions on Automatic Control, vol. 61, no. 12, pp. 4170–4175,\n2016.\n[20] H. Modares, F. L. Lewis, and Z. Jiang, “Optimal output-feedback\ncontrol of unknown continuous-time linear systems using off-policy\nreinforcement learning,” IEEE Transactions on Cybernetics, vol. 46,\nno. 11, pp. 2401–2410, 2016.\n[21] B. Luo, D. Liu, T. Huang, and J. Liu, “Output tracking control\nbased on adaptive dynamic programming with multistep policy evalua-\ntion,” IEEE Transactions on Systems, Man, and Cybernetics: Systems,\nvol. 49, no. 10, pp. 2155–2165, 2019.\n[22] D. Kleinman, “Optimal stationary control of linear systems with\ncontrol-dependent noise,” IEEE Transactions on Automatic Control,\nvol. 14, no. 6, pp. 673–677, 1969.\n[23] J. Lai, J. Xiong, and Z. Shu, “Model-free optimal control of discrete-\ntime systems with additive and multiplicative noises,” arXiv preprint\narXiv:2008.08734, 2020.\n[24] G. C. Goodwin and K. S. Sin, Adaptive Filtering Prediction And\nControl.\nCourier Corporation, 2014.\n[25] H. Yu and D. P. Bertsekas, “Convergence results for some temporal\ndifference methods based on least squares,” IEEE Transactions on\nAutomatic Control, vol. 54, no. 7, pp. 1515–1531, 2009.\n[26] M. G. Lagoudakis and R. Parr, “Least-squares policy iteration,”\nJournal of Machine Learning Research, vol. 4, no. 12, pp. 1107–1149,\n2003.\n[27] S. J. Bradtke, B. E. Ydstie, and A. G. Barto, “Adaptive linear quadratic\ncontrol using policy iteration,” in Proceedings of American Control\nConference, 1994, pp. 3475–3479.\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.SY",
    "math.OC"
  ],
  "published": "2020-10-13",
  "updated": "2020-10-13"
}