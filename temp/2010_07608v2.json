{
  "id": "http://arxiv.org/abs/2010.07608v2",
  "title": "Fully Unsupervised Person Re-identification viaSelective Contrastive Learning",
  "authors": [
    "Bo Pang",
    "Deming Zhai",
    "Junjun Jiang",
    "Xianming Liu"
  ],
  "abstract": "Person re-identification (ReID) aims at searching the same identity person\namong images captured by various cameras. Unsupervised person ReID attracts a\nlot of attention recently, due to it works without intensive manual annotation\nand thus shows great potential of adapting to new conditions. Representation\nlearning plays a critical role in unsupervised person ReID. In this work, we\npropose a novel selective contrastive learning framework for unsupervised\nfeature learning. Specifically, different from traditional contrastive learning\nstrategies, we propose to use multiple positives and adaptively sampled\nnegatives for defining the contrastive loss, enabling to learn a feature\nembedding model with stronger identity discriminative representation. Moreover,\nwe propose to jointly leverage global and local features to construct three\ndynamic dictionaries, among which the global and local memory banks are used\nfor pairwise similarity computation and the mixture memory bank are used for\ncontrastive loss definition. Experimental results demonstrate the superiority\nof our method in unsupervised person ReID compared with the state-of-the-arts.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nFully Unsupervised Person Re-identiﬁcation via\nSelective Contrastive Learning\nBo Pang, Deming Zhai, Member, IEEE, Junjun Jiang, Member, IEEE, and Xianming Liu, Member, IEEE\nAbstract—Person re-identiﬁcation (ReID) aims at searching the\nsame identity person among images captured by various cameras.\nExisting fully-supervised person ReID methods usually suffer\nfrom poor generalization capability caused by domain gaps.\nUnsupervised person ReID attracts a lot of attention recently, due\nto it works without intensive manual annotation and thus shows\ngreat potential of adapting to new conditions. Representation\nlearning plays a critical role in unsupervised person ReID. In\nthis work, we propose a novel selective contrastive learning\nframework for fully unsupervised feature learning. Speciﬁcally,\ndifferent from traditional contrastive learning strategies, we\npropose to use multiple positives and adaptively selected neg-\natives for deﬁning the contrastive loss, enabling to learn a\nfeature embedding model with stronger identity discriminative\nrepresentation. Moreover, we propose to jointly leverage global\nand local features to construct three dynamic memory banks,\namong which the global and local ones are used for pairwise\nsimilarity computation and the mixture memory bank are used\nfor contrastive loss deﬁnition. Experimental results demonstrate\nthe superiority of our method in unsupervised person ReID\ncompared with the state-of-the-arts.\nIndex Terms—Person Re-identiﬁcation Unsupervised learning\nContrastive learning\nI. INTRODUCTION\nP\nERSON re-identiﬁcation (ReID), also referred to as per-\nson retrieval, aims at searching the same identity per-\nson among images captured by various cameras at different\ntime and locations. Thanks to the rapid development of\nconvolutional neural networks (CNN), in recent years, the\nperformance of person ReID is improved remarkably by using\ndiscriminative features from labeled person images. However,\nthe success of such systems relies on a large amount of\nlabeled data, which is often prohibitively expensive to acquire.\nAs a result, a large research effort is currently focused on\nunsupervised systems without leveraging intensive manual\nsupervision, which attracts a lot of attention due to the great\npotential of adapting to new conditions.\nThis effort includes recent advances on transfer learning and\nunsupervised learning. Among them, cross-domain transfer\nlearning, also called domain adaptation, offers an effective\nmanner to reduce the labelling cost. The basic idea is to learn\nan attribute-semantic and identity discriminative representation\nfrom a labeled source dataset, which is then transferred to\nthe target domain for person ReID [1, 2, 3, 4]. However, the\nperformance of this approach largely relies on the assumption\nB. Pang, D. Zhai, J. Jiang and X. Liu are with the School of Com-\nputer Science and Technology, Harbin Institute of Technology, Harbin\n150001, China, and also with the Peng Cheng Laboratory, Shenzhen 518052,\nChina (cspb@hit.edu.cn; zhaideming@hit.edu.cn; jiangjunjun@hit.edu.cn;\ncsxm@hit.edu.cn).\nthat there is sufﬁcient knowledge overlap between the source\nand target domains, which is not always valid however. If the\nsource domain exhibits signiﬁcantly different characteristics\nwith the target domain, the ReID performance would degrade\nheavily. Moreover, this kind of methods require a large amount\nof annotated source data, which are thus not purely unsuper-\nvised.\nThe fully unsupervised learning based approach receives\nmore and more attention recently, since it works without\nleveraging any labeled data and thus shows better potential\nto deploy person ReID for real-world applications. The basic\nidea of this line is to alternate between predicting pseudo labels\nby clustering or classiﬁcation and training the network with\ngenerated pseudo classes. For instance, Lin et al. [5] proposed\na bottom-up clustering framework that iteratively trains a\nnetwork based on the pseudo labels generated by unsupervised\nclustering. Wang et al. [6] formulated unsupervised person\nReID as a multi-label classiﬁcation task to progressively seek\ntrue labels. Lin et al. [7] proposed a classiﬁcation network\nwith softened labels to eliminate errors incurred from hard\nquantization in clustering. However, the performance of these\nmethods relies on the accuracy of label prediction, which is a\nnon-trivial task under unsupervised setting.\nInstead of explicit label prediction, in this paper, we con-\ncentrate on contrastive self-supervised visual representation\nlearning, taking advantage of the principle that a good fea-\nture representation model should map images of the same\nperson closer to each other, while push images of different\nidentities apart away. Speciﬁcally, we propose a novel selective\ncontrastive learning framework with dynamic memory banks,\nwhich is specially tailored for the task of unsupervised person\nReID. Although there is also some work attempts to tackle\nperson ReID based on contrast learning such as [4], which is\nupon the domain adaption paradigm as thus requires a labeled\nsource domain. In contrast, the proposed method is purely\nunsupervised.\nThe technical contributions of this work are three-fold:\n• Considering that a person may be captured by various\ncameras, i.e., each person may have multiple images in\nthe training set, one key technical novelty is that we\nchoose multiple positives for each anchor, as opposed\nto SimCLR [8], MoCo [9] and [4] that use only a single\npositive to deﬁne the contrastive loss. Moreover, different\nfrom the conventional contrastive learning strategies that\ntake all samples except the positive as the negatives\n[4, 8, 9], we propose to select samples that are plausibly\nsimilar to the anchor as the negatives, so as to improve\nthe discrimination ability of representation learning. More\narXiv:2010.07608v2  [cs.CV]  4 Mar 2021\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\nspeciﬁcally, using the deﬁned distance metric, we rank\nthe similarity order between the anchor and all training\nsamples, according to which we divide the training set\ninto three subsets: similar set, borderline set, and dissim-\nilar set. By taking samples in similar set as the positives\nand samples in borderline set as the negatives, we deﬁne\nthe contrastive loss to encourage the feature embedding\nfunction to produce closely aligned representations to all\nimages of the same identity.\n• The other contribution of this work is that we propose\nthree dynamic dictionaries which jointly leverage global\nand local discriminative information for unsupervised\nrepresentation learning. The global feature is widely used\nin existing unsupervised ReID, such as [5, 6]. However,\nit suffers from discriminative information loss in some\ncases, leading to images of different persons may have\nsimilar feature representations. On the other hand, the\npart-level features offer ﬁne-grained discriminative infor-\nmation for pedestrian image description [10]. However,\ncompared with the global feature, the local features\nbring much more search freedom, making the optimiza-\ntion of feature representation learning hard to converge.\nMoreover, learning discriminative local features requires\nthat parts should be accurately located. It can be done\neither by external assistance from human pose estimation\n[11] or well-designed partition strategies [10, 12], which\nare expensive and hinder the generalization in practical\napplications. Considering their respective limitations, we\npropose to jointly leverage the global feature and the local\nfeatures for deﬁning distance metric and contrastive loss.\n• By combining the above contributions, we propose an ef-\nfective unsupervised person ReID algorithm. Our scheme\nachieves encouraging performance with respect to rank-1\nand mAP so far on public Market-1501, DukeMTMC-\nreID, DukeMTMC-VideoReID and MARS. For instance,\nwe achieve rank-1 accuracy of 82.2%, signiﬁcantly out-\nperforming the latest unsupervised person ReID methods\nSNR [13], SSLR [7], MMCL [6] and TSSL [14] by\n15.5%, 10.5%, 15.6% and 11%, respectively.\nThe paper is organized as follows: Section 2 overviews\nsome related works. Section 3 introduces the proposed scheme,\nincluding the representation learning framework, positives and\nnegatives sampling strategy, the deﬁned contrastive loss and\nthe optimization strategy, and the memory bank update strat-\negy. Section 4 provides the experimental results and ablation\nstudy. We conclude this paper in Section 5.\nII. RELATED WORK\nA. Unsupervised Domain Adaptation Person ReID\nTransfer learning is a common strategy for addressing\nunsupervised person ReID. These domain adaption methods\n[1, 2, 3, 13, 15] attempt to tackle the unsupervised person\nReID problem on the target unlabeled dataset by leveraging\nother dataset’s labeled information. TJ-AIDL model [1] aims\nat learning an attribute-semantic and identity-discriminative\nfeature representation space which is transferrable to the any\nunlabelled target dataset. HHL [15] aims to improve the\ngeneralization ability of re-ID models on the target testing set\nwith enforcing two properties, camera invariance and domain\nconnectednes, simultaneously. Thanks to the development of\nthe Generative Adversarial Network (GAN), this type of style\ntransfer network is used for cross-domain transfer learning for\nunsupervised person ReID. SPGAN [2] generates transferred\nimages from the source labeled dataset and then do the super-\nvised learning with two constraints which are self-similarity of\nan image before and after translation and domain-dissimilarity\nof a translated source image and a target image. ATNet [3]\ndecomposes the complicated cross-domain transfer into a set\nof factor-wise sub-transfers, each of which concentrates on\nstyle transfer with respect to a certain imaging factor, e.g.,\nillumination, resolution and camera view etc. Considering poor\ngeneralization capability caused by domain gaps with existing\nmethods, Baseline-SNR [13] ﬁlter out identity-irrelevant in-\nterference and learn domain-invariant person representations.\nIn [4], a hybrid memory is proposed to encode all available\ninformation from both source and target domains for feature\nlearning, which achieves the best unsupervised person ReID\nperformance so far. Although achieving promising perfor-\nmance, these methods require an annotated source dataset.\nIn contrast, our work focuses on purely unsupervised person\nReID, which only relies on the unlabeled target dataset.\nB. One Shot Person ReID\nMethods based on one-shot learning for person ReID at-\ntempt to solve the problem with condition where each identity\nhas only one labeled example and many unlabeled examples.\nEUG [16] and ProLearn [17] propose to gradually and steadily\nimprove the discriminative capability of the CNN via stepwise\nlearning. Especially, for video-based person re-identiﬁcation,\nRACE [18] ﬁrstly adopt anchor sequences to formulate an\nanchor graph. And then for accurately estimate labels from\nunlabeled sequences with noisy frames, robust anchor embed-\nding is introduced based on the regularized afﬁne hull. These\nmethods solved the cost of annotation in a degree compared\nwith the supervised methods and the domain adaptation based\nmethods. But still, the labeled information is needed.\nC. Fully Unsupervised Person ReID\nFor fully unsupervised person ReID, traditional methods\n[19, 20] utilize hand-craft features, which are hardly designed\nto be discriminative by hand. Recently, the cluster based\nmethods and the mutli-label based methods estimate pseudo\nlabels to train the neural network. BUC [5] jointly optimize\na convolutional neural network (CNN) and the relationship\namong the individual samples with bottom-up clustering pro-\ncedure. TSSL [14] thought these bottom-up clustering methods\nmerely utilise suboptimal global clustering. They design a\ncomprehensive unsupervised learning objective that accounts\nfor tracklet frame coherence, tracklet neighbourhood com-\npactness, and tracklet cluster structure in a uniﬁed formula-\ntion, which is capable of capitalising directly from abundant\nunlabelled tracklet data. Wang et al. formulate the problem\nas a multi-label classiﬁcation task to progressively seek true\nlabels and adopt the memory-based multi-label classiﬁcation\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\nloss (MMCL) to boost the ReID model training efﬁciency.\nSSLR [7] proposed a similarity learning framework with\nsoftened labels to relief the hard quantization loss in clustering.\nHowever, these methods rely on the accuracy of the pseudo\nlabels. Our proposed framework will adopt contrastive self-\nsupervised visual representation learning, mapping the person\nimages with same identity close to each other while push the\nperson with different identity apart away.\nD. Unsupervised Representation Learning\nUnsupervised representation learning utilize unlabeled data\nto learn an effective embedding space for downstream tasks\nlike image classiﬁcation and etc. Gidaris et al. [21] propose\nto learn image features by training ConvNets to recognize the\n2d rotation that is applied to the image that it gets as input.\nRecently, contrastive learning has attracted a lot of attention.\nSuch methods aim at mapping the representation close to the\npositives and away from the negatives. MoCo [9] build a\nlarge and consistent dictionary on-the-ﬂy that facilitates con-\ntrastive unsupervised learning. SimCLR [8] propose a simple\nframework for contrastive learning of visual representations.\nBoth the Moco and the SimCLR use only one positive to\nconduct contrastive loss. In our paper, we propose a new\ncontrastive learning framework with selective positives and\nnegatives and three dynamic dictionaries will be conducted\nto help the training process.\nIII. METHODOLOGY\nGiven a training set X = {x1, x2, ..., xN}, the goal of unsu-\npervised person ReID is to learn a model F(θ; xi) for visual\nfeature representation without using any manual annotation,\nwhere parameters related to F are denoted as θ. The learned\nrepresentation model is then applied on the query image xq\nand the gallery set ˆ\nX = {ˆx1, ˆx2, ..., ˆxM}, so as to derive the\nquery result by ranking distance between features of the query\nand all gallery images. It is clear to see that, unsupervised\nrepresentation learning plays a central role in unsupervised\nperson ReID.\nThe feature representation learning is challenging in unsu-\npervised setting, wherein the critical problem is how to learn\nto discriminate between individual images, without any notion\nof semantic categories. A good feature representation model\nshould map images of the same person closer to each other,\nwhile push images of different identities apart away. In this\nwork, inspired by recent progress of self-supervised learning,\nwe address the unsupervised feature learning problem from\nthe perspective of contrastive learning.\nA. Representation Learning Framework\nAs illustrated in Fig. 1, our representation learning frame-\nwork consists of the following components:\n• A neural network base encoder E(·), which is leveraged\nto extract feature vector for a given person image xi.\nIt allows various choices of the network architecture\nwithout any constraints. Here we adopt ResNet-50 with-\nout the last classiﬁcation layer as the backbone, whose\nparameters are pre-trained on ImageNet, to obtain feature\nmap fi = E(xi) ∈R2048.\n• Pooling operator AP(·), for which we consider both\nglobal and local average pooling to obtain global and\nlocal features. The global feature f g\ni\nis obtained by\naverage pooling (AP) of feature map fi of the whole\nimage:\nf g\ni = AP(fi)\n(1)\nHowever, the global feature suffers from discriminative\ninformation loss in some cases, leading to images of dif-\nferent identities may have similar feature representation.\nWe further consider the part-level features, which offer\nﬁne-grained discriminative information for pedestrian im-\nage description [10]. We obtain part-level feature maps by\nequally partitioning the global feature map fi into Nl = 8\nhorizontal stripes. The local features are then obtained by\napplying average pooling on each part:\nf l\ni,j = AP(fi,j), j = 1, · · · , Nl\n(2)\n• A small projection neural network P(·), which is a learn-\nable nonlinear operation that transforms image features\nto the latent space where the contrastive learning is con-\nducted. P(·) is deﬁned with fully-connected (FC) layer,\nbatch normalization (BN) layer and L2-normalization\n(L2-Norm) layer. The usage of P(·) is demonstrated to\nbe beneﬁcial to deﬁne the contrastive loss [8].\n• Global and local memory bank Mg and Ml, which are\nleveraged to store global and local features for pairwise\nsimilarity computation. The keys in global memory bank\nMg are deﬁned as:\nvg\ni = P (AP(fi)) , i = 1, · · · , N\n(3)\nAnd the keys in local memory bank Ml are deﬁned as:\nvl\ni,j = P (AP(fi,j)) , i = 1, · · · , N; j = 1, · · · , Nl\n(4)\nThe global and local memory banks are then used to de-\nﬁne global and local distance metrics respectively, which\nare further coupled with cross-camera encouragement\nterm [7] to deﬁne the total distance metric for pairwise\nsimilarity computation, as shown in Eq. (11). According\nto the ranked similarity order, we identify the positives\nK+ and the negatives K−that are used to deﬁne the\ncontrastive loss. The details will be elaborated in the\nfollowing subsection.\n• Mixture memory bank Mt, which is required to store fea-\ntures of training images to deﬁne the contrastive loss, so\nas to maximize the similarity between representations of\nthe anchor and the positives, while minimizing that of the\nanchor and the negatives. To improve the discrimination\nability, we construct a mixture memory bank Mt which\nincludes the fusion of the global and local features as\nkeys. It is worth noting that, for the local feature in Mt,\ninstead of using the one deﬁned in Eq. (4), we turn to\nconcatenate all Nl ones to form a single local feature:\nvl\ni = CONCAT\n\u0010\n{AP(fi,j)}Nl\nj=1\n\u0011\n(5)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nFig. 1: The overall framework of our proposed unsupervised person ReID.\nThe mixture memory bank Mt is initialized with all\nzeros. We update its keys corresponding to all positives\nby fusing with the global and local features of the anchor\nxi progressively. Speciﬁcally, the positive keys are ﬁrstly\nupdated with global feature vg\ni :\nMt[k] = ∥Mt[k] + vg\ni\n2\n∥2, k ∈K+\n(6)\nwhich are further updated with local feature vl\ni:\nMt[k] = ∥Mt[k] + vl\ni\n2\n∥2, k ∈K+\n(7)\nwhere ∥· ∥2 represents L2-normalization. In this way,\nthe mixture memory bank jointly employs the global and\nlocal discriminative information.\nB. Positives and Negatives Sampling\nAs the name suggests, contrastive learning requires to obtain\ntwo opposing powers: for a given anchor sample, one power\nis to pull the anchor closer in representation space to other\nsamples, which is known as the positive; while the other power\nis to push the anchor farther away from other samples, which\nis known as the negatives. To identify the positive and negative\nsamples, we rely on the constructed global and local memory\nbanks Mg and Ml to compute pairwise similarity of samples,\nand apply two well-designed similarity metrics in the literature\nto this end.\nSpeciﬁcally, for an anchor image xi, we learn its global\nfeature vg\ni and local feature {vl\ni,j}Nl\nj=1 in the way as described\nin last subsection. The similarity between xi and another\nimage xj are calculated by measuring Euclidean distance of\nfeature vectors and keys of dual dictionary Mg and Ml. More\nprecisely, we deﬁne the global and local distances as Sg and\nSl respectively as follows:\nSg(xi, xj) = ∥vg\ni −Mg[j]∥2\n(8)\nand\nSl(xi, xj) =\nPNl\nk=1 ∥vl\ni,k −Ml[j, k]∥2\nNl\n(9)\nMoreover, to encourage the consistency of images of the\nsame person captured by different cameras, we add the cross-\ncamera encouragement term (CCE) proposed in [7] as a part\nof the similarity metric. Set the camera IDs of person images\nxi and xj as ci and cj respectively, CCE is deﬁned as\nCCE(xi, xj) =\n\u001a λc\nci = cj\n0\nci ̸= cj\n(10)\nFinally, the total similarity metric S between xi and xj is\nformulated as:\nS(xi, xj) = βSg(xi, xj) + (1 −β)Sl(xi, xj) + CCE(xi, xj)\n(11)\nwhere β is the trade-off parameter that balances the contribu-\ntion of global and local similarity and is set 0.5.\nAccording to the deﬁned total similarity metric, we perform\npositive and negative sampling. We rank the similarity order\nbetween the anchor and all training samples, according to\nwhich we divide the training set into three subsets: similar\nset, borderline set, and dissimilar set. Considering that each\nperson may have multiple images in dataset, we choose\nto sample multiple positives for the anchor, as opposed to\nSimCLR [8] and MoCo [9] that use only a single positive to\ndeﬁne the contrastive loss. Speciﬁcally, we consider samples\nin similar set as the positives, whose index sets are denoted\nas K+ ∈RN+. Moreover, we propose to select samples that\nare plausibly similar to the anchor as the negatives, so as to\nimprove the discrimination ability of representation learning.\nSpeciﬁcally, we consider in borderline set as the negatives,\nwhose index sets are denoted as K−∈RN−. This is different\nfrom the conventional contrastive learning strategies that take\nall samples except the positive as the negatives [8, 9]. And it\nis also different from an intuitive idea that chooses samples\nin dissimilar set as the negatives. Since samples in dissimilar\nset is already differentiable enough, learning on them cannot\nimprove the discrimination ability of the model signiﬁcantly.\nInstead, we enforce the model to consider samples in border-\nline set, which are hard cases with respect to discrimination.\nWe refer to as the proposed manner as selective contrastive\nlearning.\nC. Contrastive Loss and Optimization\nWith the identiﬁed postives and negatives, we deﬁne the\nfollowing contrastive loss function with respect to the mixture\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\nAlgorithm 1 Network Training Flow.\nRequire: Training set X = {x1, x2, ..., xN}; Global memory\nbank Mg; Local memory bank Ml; Mixture memory\nbank Mt; network with parameter F(θ; xi) ; initial\nlearning rate γ ;\nEnsure: The optimal parameters θ∗\n1: for j = 1: num epochs do\n2:\nPick up training batch set {xi}.\n3:\nfor m = 1:n do\n4:\nf g\ni = AP(fi); f l\ni,j = AP(fi,j), j = 1, · · · , Nl\n5:\nvg\ni = P (AP(fi))\n6:\nvl\ni,j = P (AP(fi,j)) ; j = 1, · · · , Nl\n7:\nvl\ni = CONCAT\n\u0010\n{AP(fi,j)}Nl\nj=1\n\u0011\n8:\nif i < Ne then\n9:\nLg = Linit(vg\ni |Mt)\n10:\nLl = Linit(vl\ni|Mt)\n11:\nelse\n12:\nCalculate similarities between the anchor im-\nage and the other images.\n13:\nfor j = 1:N do\n14:\nif j ̸= i then\n15:\nSg(xi, xj) = ∥vg\ni −Mg[j]∥2\n16:\nSl(xi, xj) =\nPNl\nk=1 ∥vl\ni,k−Ml[j,k]∥2\nNl\n17:\nS(xi, xj)\n=\nβSg(xi, xj) + (1 −\nβ)Sl(xi, xj) + CCE(xi, xj)\n18:\nend if\n19:\nend for\n20:\nGenerate\nset\nwith\ndescending\nsimilarity\n[xj1, xj2, ..., xjN−1]\n21:\nSample positives [xi, xj1, xj2, ..., xjN+]\n22:\nSample negatives [xjN+, ..., xjN−+N+]\n23:\nLg = L(vg\ni |Mt)\n24:\nLl = L(vl\ni|Mt)\n25:\nend if\n26:\nLt(θ) = (1 −λp)Lg + λpLl\n27:\nend for;\n28:\nθ = θ −γ ∗SGD(∇θLt(θ));\n29:\nUpdate M g, M l, M t\n30: end for;\n31: θ∗= θ.\nmemory bank Mt:\nL(vi|Mt) = −log\nP\nk∈K+ exp(vi · Mt[k]/τ) ∗µk\nP\nk∈K+∪K−exp(vi · Mt[k]/τ)\n(12)\nwhere · represents dot product, τ is a temperature hyper-\nparameter [22]. In order to emphsize the contribution of the\nmost similar positive sample, we introduce the contribution\nfactor µk for positive distance, which is deﬁned as\nµk =\n(\nλt\nk = i\nα(1−λt)\n|K+|\nk ∈K+&k ̸= i\n(13)\nwhere α is the expanding coefﬁcient and is set 1.75.\nAccording to the loss form in Eq. (12), we calculate the\nglobal loss Lg and the local loss Ll with the global feature\nvg\ni deﬁned as Eq. (3) and the local feature vl\ni deﬁned as Eq.\n(5), respectively:\nLg =\nN\nX\ni=0\nL(vg\ni |Mt)\n(14)\nand\nLl =\nN\nX\ni=0\nL(vl\ni|Mt)\n(15)\nAnd ﬁnally the total contrastive loss is deﬁned as:\nLt(θ) = (1 −λp)Lg + λpLl\n(16)\nwhere λp is the trade-off parameter that balances the con-\ntributions of global and local losses and is set 0.5. In our\nframework, the parameters of E(·) and P(·) are collectively\ndenoted as θ.\nThe optimal parameter θ∗of ReID model F(θ, ·) can be\nobtained by:\nθ∗= arg min\nθ Lt(θ)\n(17)\nThis minimization problem can be addressed by stochastic\ngradient descent (SGD):\nθ = θ −γ ∗SGD(∇θLt(θ))\n(18)\nwhere γ is the learning rate, and SGD(∇θLt(θ)) represents\nthe updated value based on SGD. The whole network training\nﬂow is summarized in Algorithm 1.\nIt is worth noting that, at the beginning of network training,\nwe set a few epochs to initialize the network and the memory\nbanks. During this process, the positive is the anchor itself\nand the negatives are randomly chosen. The loss function in\ninitialization stage is deﬁned as follows:\nLinit = −\nN\nX\ni=0\nlog\nexp(vi · Mt[i]/τ)\nP\nk∈{i}∪K−exp(vi · Mt[k]/τ)\n(19)\nD. Memory Banks Update\nContrastive learning can be thought of as training an encoder\nfor a dictionary look-up task [9]. In our method, as stated\nabove, there are three dictionaries involved, including the\nglobal and local memory banks Mg and Ml that are jointly\nexploited to compute pairwise similarity, and the mixture\nmemory bank Mt that is used to deﬁne constrative loss.\nTo facilitate contrastive unsupervised learning, these three\ndictionaries should be dynamic, i.e., be updated on-the-ﬂy to\nprovide evolutionary keys during training.\nIn this work, we propose to use different update strategies\nfor the global and local memory banks and the mixture\nmemory bank, considering that they serve for different pur-\nposes. Speciﬁcally, for Mg and Ml, we only update the key\ncorresponding to the anchor xi by fusing with the newest\nglobal and local feature of xi respectively:\nMg[i] = ∥Mg[i] + vg\ni\n2\n∥2,\n(20)\nand\nMl[i] = ∥Ml[i] + vl\ni\n2\n∥2,\n(21)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\nFor the mixture memory bank, we will update the key corre-\nsponding to the anchor xi and its positives. The update strategy\nis consistent with the construction strategy, as formulated in\nEq. (6) and Eq. (7).\nE. Why Using Three Dynamic Dictionaries?\nIn our framework, three dynamic dictionaries are used,\nincluding the global, local and mixture memory banks. We\ndiscuss here about the necessity of using these three dictio-\nnaries.\nIn pairwise similarity computation, we use both the global\nand local memory banks instead of the mixture one. This is\nbecause the keys of the mixture memory bank are the fusion\nof the global and local features. The fusion operation would\nremove some useful information. In order to preserve ﬁne\ninformation, we thus use both the original global and local\nfeatures, which construct the global and local memory banks.\nIn deﬁning the contrastive loss, we use the mixture memory\nbank instead of the global and local ones. For the task of\nperson ReID, we except the model to generate similar feature\nrepresentation for the samples with same identity and dissim-\nilar representation otherwise. According to this principle, the\nmixture memory bank is tailored, in which we update the\nkeys corresponding to the positives of the anchor xi. This\nwould encourage the anchor and its positives have similar\nrepresentation and update the keys with global features and\nlocal features. It also could provide more discriminative ability\nto pull the similar samples closer and push dissimilar samples\napart away.\nIV. EXPERIMENTS\nIn this section, we provide extensive experimental results to\ndemonstrate the superior performance of our method.\nA. Dataset\nWe evaluate our method on ﬁve widely used image and\nvideo person ReID datasets, including:\n• Market1501 [23], which consists of 32,668 images of\n1,501 identities under 6 cameras;\n• DukeMTMC-ReID [24], which contains 1,812 identities\nand 36,411 images under 8 cameras;\n• MSMT17 [25], which contains 126,411 person images of\n4101 identities under 15 cameras. It is a more challenging\ndataset due to the effect of substantial variations of scene\nand lighting.\n• DukeMTMC-VideoReID [26], which is a video-based\nReID dataset containing 2,196 tracklets of 702 identities\nfor training, 2,636 tracklets of other 702 identities for\ntesting;\n• MARS [27], which is a video-based dataset for person\nReID containing 17,503 video tracklets of 1,261 identi-\nties.\nThe ﬁrst three datasets are image-based, and the last two\nones are video-based. We test both image and video based\nto comprehensively evaluate the performance of our method.\nB. Experimental Setting\nWe follow the same experimental setting as [7]. All exper-\niments are implemented on PyTorch. The input images are\nresized to 256*128 and we use random horizontal ﬂip as the\ndata argument strategy. We adopt SGD with momentum as 0.9\nto optimize the model. The learning rate is set as 1e-3. The\ntraining epoch for image-based dataset is set as 50 and for the\nvideo-based dataset is set as 60. The batch size is set as 8.\nFor the video-based dataset, we randomly sample four frames\nduring training, and all frames during testing, in the tracklet.\nWe take the average feature of all frames within a tracklet to\nbe the tracklet feature.\nIn the proposed framework, there are a few hyper-\nparameters involved. We set λc = 0.005 in the cross-camera\nencouragement term, β = 0.5 in the total similarity metric,\nλt\n= 0.5 and α = 1.75 in the contribution factor of\ncontrastive loss, the positive samples number N+ = 7 and\nthe negative samples number N−= 500, and λp = 0.5 in the\ntotal contrastive loss. The experimental analysis about hyper-\nparameters setting can be found in Fig. 2, which is conducted\non Market-1501.\nC. Comparison with the state-of-the-arts\nOur method is comprehensively compared with state-of-the-\nart unsupervised domain adaption based (UDA), one exam-\nple based (OneEx) and unsupervised learning based (Unsup)\nmethods. The comparison is conducted on both image-based\nand video-based datasets.\n• Evaluation on Image-based Datasets: The compar-\nisons with the state-of-the-art algorithms are conducted\non Market-1501, DukeMTMC-ReID and MSMT17, as\nshown in Table I. It can be found that, under the\nsame setting, our method achieves the best accuracy\non both Market-1501 and DukeMTMC-ReID among the\n14 compared methods with respect to four performance\nevaluation metrics: rank-1, rank-5, rank-10 and mAP.\nOn Market-1501, we obtain the best performance among\nthe compared methods with rank-1 = 82.2% and mAP\n= 54.4%. SNR [13], SSLR [7], MMCL [6] and TSSL\n[14] are four latest methods on unsupervised person\nReID. Compared with SNR, we achieve 15.5% and 20.5%\nimprovement on rank-1 accuracy and mAP; compared\nwith SSLR, the gains are 10.5% and 16.6% respectively;\ncompared with MMCL, the gains are 15.6% and 19.1%\nrespectively; compared with TSSL, the gains are 11% and\n11.1% respectively. On DukeMTMC-ReID, our method\nalso works the best and achieves accuracy improvement\nby a large margin. Compared with the second best per-\nformed method, we achieve 7.7% and 8.7% improvement\non rank-1 accuracy and mAP respectively. On MSMT17,\nas shown in Table II, compared with the best UDA\nmethod SSG, we achieve 9.2% improvement on rank-1.\nCompared with fully unsupervised method MMCL, we\nachieve 6.0% and 2.1% improvement on rank-1 and mAP\nrespectively.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\n(a)\n(b)\n(c)\n(d)\nFig. 2: Experimental analysis about hyper-parameters setting\nMethod\nReference\nSetting\nMarket-1501\nDukeMTMC-reID\nSource\nRank-1\nRank-5\nRank-10\nmAP\nSource\nRank-1\nRank-5\nRank-10\nmAP\nEUG [16]\nCVPR’2018\nOneEx\nMarket\n49.8\n66.4\n72.7\n22.5\nDuke\n45.2\n59.2\n63.4\n24.5\nATNet [3]\nCVPR’2019\nUDA\nDuke\n55.7\n73.2\n74.9\n25.6\nMarket\n45.1\n59.5\n64.2\n24.9\nProLearn [17]\nTIP’2019\nOneEx\nMarket\n55.8\n72.3\n78.4\n26.2\nDuke\n48.8\n63.4\n68.4\n28.5\nSPGAN [2]\nCVPR’2018\nUDA\nDuke\n58.1\n76.0\n82.7\n26.7\nMarket\n46.9\n62.6\n68.5\n26.4\nTJ-AIDL [1]\nCVPR’2018\nUDA\nDuke\n58.2\n-\n-\n26.5\nMarket\n44.3\n-\n-\n23.0\nBUC [5]\nAAAI’2019\nUnsup\nNone\n61.0\n71.6\n76.4\n30.6\nNone\n40.2\n52.7\n57.4\n21.9\nHHL [15]\nECCV’2018\nUDA\nDuke\n62.2\n78.8\n84.0\n31.4\nMarket\n46.9\n61.0\n66.7\n27.2\nDBC [28]\nBMVC’2019\nUnsup\nNone\n69.2\n83.0\n87.8\n41.3\nNone\n51.5\n64.6\n70.1\n30.0\nSNR [13]\nCVPR’2020\nUDA\nDuke\n66.7\n-\n-\n33.9\nMarket\n55.1\n-\n-\n33.6\nSSLR [7]\nCVPR’2020\nUnsup\nNone\n71.7\n83.8\n87.4\n37.8\nNone\n52.5\n63.5\n68.9\n28.6\nMMCL [6]\nCVPR’2020\nUnsup\nNone\n66.6\n-\n-\n35.3\nNone\n58.0\n-\n-\n36.3\nTSSL [14]\nAAAI’2020\nUnsup\nNone\n71.2\n-\n-\n43.3\nNone\n62.2\n-\n-\n38.5\nOurs\nThis paper\nUnsup\nNone\n82.2\n89.9\n92.6\n54.4\nNone\n69.9\n79.7\n82.2\n47.2\nTABLE I: The evaluation results with respect to rank-k/mAP on image-based dataset Market-1501 and DukeMTMC. The best\nand the second ones are highlighted by bold and underline.\nThe impressive performance demonstrates that the pro-\nposed selective contrastive learning framework is able to\nlearn a powerful discrimination model.\n• Evaluation on Video-based Datasets: We further com-\npare our method with the state-of-the-art algorithms on\nthe two video-based datasets: DukeMTMC-VideoReID\nand MARS. The comparison results are shown in Table\nIII. On DukeMTMC-VideoReID, we obtain rank-1 =\n82.3%, mAP = 78.4%, which are best among all methods.\nCompared with SSLR, the gains are 5.8% and 9.1%\nrespectively; compared with TTSL, the gains are 8.3%\nand 13.8% respectively. On MARS, we obtain rank-1 =\n66.7%, mAP = 46.8%, which are also the best results.\nD. Ablation Study\nIn this section, we provide ablation study about the two\nmain contributions of this work: selective contrastive learning\nand joint usage of global and local features. The experiments\nare conducted on Market-1501 and DukeMTMC-ReID. The\nresults are reported in Table IV and Table V.\n• Inﬂuence of local and global features to the ﬁnal\nperformance: We ﬁrst study the role of global and local\nfeatures to the ﬁnal performance. We investigate three\nscenarios:\n– Global feature only: in this case, we use global\nfeature only in pairwise similarity computation and\ncontrastive loss. This is achieved by setting β = 1\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\nMethod\nReference\nSetting\nMSMT17\nSource\nRank-1\nRank-5\nRank-10\nmAP\nPTGAN [25]\nCVPR’2018\nUDA\nMarket\n10.2\n-\n24.4\n2.9\nECN [29]\nCVPR’2020\nUDA\nMarket\n25.3\n36.3\n42.1\n8.5\nSSG [30]\nICCV’2019\nUDA\nMarket\n31.6\n-\n49.6\n13.2\nPTGAN [25]\nCVPR’2018\nUDA\nDuke\n11.8\n-\n27.4\n3.3\nECN [29]\nCVPR’2020\nUDA\nDuke\n30.2\n41.5\n46.8\n10.2\nSSG [30]\nICCV’2019\nUDA\nDuke\n32.2\n-\n51.2\n13.3\nMMCL [31]\nCVPR’2020\nUnsup\nNone\n35.4\n44.8\n49.8\n11.2\nOurs\nThis paper\nUnsup\nNone\n41.4\n53.6\n58.7\n13.3\nTABLE II: The evaluation results with respect to rank-k/mAP on image-based dataset MSMT17. The best and the second ones\nare highlighted by bold and underline.\nMethod\nReference\nSetting\nDukeMTMC-VideoReID\nMARS\nRank-1\nRank-5\nRank-10\nmAP\nRank-1\nRank-5\nRank-10\nmAP\nRACE [18]\nECCV’2018\nOneEx\n-\n-\n-\n-\n43.2\n57.1\n62.1\n24.5\nDAL [32]\nBMVC’2018\nUnsup\n-\n-\n-\n-\n49.3\n65.9\n72.2\n23.0\nBUC [5]\nAAAI’2019\nUnsup\n76.2\n88.3\n91.0\n68.3\n57.9\n72.3\n75.9\n34.7\nEUG [33]\nCVPR’2018\nUnsup\n72.7\n84.1\n-\n63.2\n62.6\n74.9\n-\n42.4\nSSLR [7]\nCVPR’2020\nUnsup\n76.4\n88.7\n91.0\n69.3\n62.8\n77.2\n80.1\n43.6\nTTSL [14]\nAAAI’2020\nUnsup\n73.9\n-\n-\n64.6\n56.3\n-\n-\n30.5\nOurs\nThis paper\nUnsup\n82.2\n93.2\n95.2\n78.4\n66.6\n77.0\n79.8\n46.6\nTABLE III: The evaluation results with respect to rank-k/mAP on video-based dataset DukeMTMC-VideoReID and MARS.\nThe best and the second ones are highlighted by bold and underline.\nScenarios\nMarket-1501\nDukeMTMC\nmAP\nRank-1\nmAP\nRank-1\nGlobal feature only\n38.3\n62.0\n26.9\n41.2\nLocal feature only\n42.4\n70.8\n34.1\n57.8\nOur joint usage\n54.4\n82.2\n47.2\n69.9\nTABLE IV: The ablation study about the inﬂuence of local\nand global features to the ﬁnal performance\nScenarios\nMarket-1501\nDukeMTMC\nmAP\nRank-1\nmAP\nRank-1\nN+ = 1, N−= all\n37.1\n70.9\n37.9\n60.3\nN+ = 7, N−= all\n53.6\n79.6\n42.4\n65.8\nOurs (N+ = 7, N−= 500)\n54.4\n82.2\n47.2\n69.9\nTABLE V: The ablation study about the inﬂuence of positives\nand negatives to the ﬁnal performance\nand λp = 0 in Eq. (11) and Eq. (16), respectively.\nThe mixture memory bank used in contrastive loss\nstores global features only.\n– Local feature only: in this case, we use local feature\nfor these purposes. This is achieved by setting β = 0\nand λp = 1. The mixture memory bank stores local\nfeatures only.\n– Our joint usage. This is what we do in this work,\ni.e., jointly using global and local features.\nFrom Table IV, it can be found that, the joint usage\nstrategy shows the best performance, which achieves\nimprovements with respect to mAP and rank-1 accuracy\nwith a large margin compared with strategies with global\nor local feature only. This demonstrates that our proposal\nof jointly using global and local features in dictionary\nconstruction is reasonable and works well.\n• Inﬂuence of positives and negatives to the ﬁnal perfor-\nmance: In our proposed selective contrastive learning, we\nleverage multiple positives and selective negatives in con-\ntrastive loss deﬁnition. Here we study the role of multiple\npositives and selected negatives to the ﬁnal performance,\nand investigate the following three scenarios:\n– N+ = 1, N−= all: This case means that, a single\npositive is used, and all samples except the positive\nare used as the negatives, which is what MoCo does\n[9].\n– N+ = 7, N−= all: This case means that multiple\npositives are used and all samples except the posi-\ntives are used as the negatives.\n– N+ = 7, N−= 500: This is what our method\ndoes. We use 7 similar samples as positives, and 500\nborderline similar samples as negatives.\nFrom Table V, it can be found that, when N−= all,\nusing N+ = 7 positives can signiﬁcantly improve rank-\n1 and mAP accuracy compared with using a single\npositive. This result demonstrates that our proposal of\nusing multiple positives in contrastive learning is useful.\nMoreover, when N+ = 7, using selected N−= 500\nnegatives achieves higher rank-1 and mAP accuracy than\nthat using N−= all negatives. This result demonstrates\nthat our proposed selective choice strategy of negatives\nis also helpful for contrastive learning.\nV. CONCLUSION\nIn this work, we presented a novel unsupervised person\nReID scheme based on selective contrastive learning. We\npropose to use multiple positives and adaptively selected\nnegatives for deﬁning the contrastive loss, so as to learn a\nfeature embedding model with stronger discriminative rep-\nresentation ability. We deﬁne three dynamic dictionaries for\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\npairwise similarity computation and contrastive loss deﬁnition,\nwhich jointly leverage the global and local discriminative\ninformation. Experimental results show that our proposed\nmethod outperforms the state-of-the-art algorithms.\nREFERENCES\n[1] J. Wang, X. Zhu, S. Gong, and W. Li, “Transferable joint\nattribute-identity deep learning for unsupervised person\nre-identiﬁcation,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp.\n2275–2284.\n[2] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and\nJ. Jiao, “Image-image domain adaptation with preserved\nself-similarity and domain-dissimilarity for person re-\nidentiﬁcation,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2018, pp.\n994–1003.\n[3] J. Liu, Z.-J. Zha, D. Chen, R. Hong, and M. Wang,\n“Adaptive transfer network for cross-domain person re-\nidentiﬁcation,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp.\n7202–7211.\n[4] Y. Ge, F. Zhu, D. Chen, R. Zhao, and H. Li, “Self-paced\ncontrastive learning with hybrid memory for domain\nadaptive object re-id,” in Advances in Neural Information\nProcessing Systems, 2020.\n[5] Y. Lin, X. Dong, L. Zheng, Y. Yan, and Y. Yang, “A\nbottom-up clustering approach to unsupervised person re-\nidentiﬁcation,” in Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 33, 2019, pp. 8738–8745.\n[6] D. Wang and S. Zhang, “Unsupervised person re-\nidentiﬁcation via multi-label classiﬁcation,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 10 981–10 990.\n[7] Y. Lin, L. Xie, Y. Wu, C. Yan, and Q. Tian, “Unsu-\npervised person re-identiﬁcation via softened similarity\nlearning,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2020, pp.\n3390–3399.\n[8] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,\n“A simple framework for contrastive learning of visual\nrepresentations,” arXiv preprint arXiv:2002.05709, 2020.\n[9] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Mo-\nmentum contrast for unsupervised visual representation\nlearning,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2020, pp.\n9729–9738.\n[10] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang,\n“Beyond part models: Person retrieval with reﬁned part\npooling (and a strong convolutional baseline),” in Pro-\nceedings of the European Conference on Computer Vi-\nsion (ECCV), 2018, pp. 480–496.\n[11] J. Long, E. Shelhamer, and T. Darrell, “Fully convo-\nlutional networks for semantic segmentation,” in 2015\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2015, pp. 3431–3440.\n[12] L. Zhao, X. Li, Y. Zhuang, and J. Wang, “Deeply-learned\npart-aligned representations for person re-identiﬁcation,”\nin The IEEE International Conference on Computer\nVision (ICCV), 2017, pp. 3219–3228.\n[13] X. Jin, C. Lan, W. Zeng, Z. Chen, and L. Zhang,\n“Style normalization and restitution for generalizable\nperson re-identiﬁcation,” in 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\n2020, pp. 3140–3149.\n[14] G. Wu, X. Zhu, and S. Gong, “Tracklet self-supervised\nlearning for unsupervised person re-identiﬁcation,” Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, vol. 34, no. 7, pp. 12 362–12 369, 2020.\n[15] Z. Zhong, L. Zheng, S. Li, and Y. Yang, “Generalizing\na person retrieval model hetero-and homogeneously,” in\nProceedings of the European Conference on Computer\nVision (ECCV), 2018, pp. 172–188.\n[16] Y. Wu, Y. Lin, X. Dong, Y. Yan, and Y. Yang, “Exploit\nthe unknown gradually: One-shot video-based person re-\nidentiﬁcation by stepwise learning,” in 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[17] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Bian, and Y. Yang,\n“Progressive learning for person re-identiﬁcation with\none example,” IEEE Transactions on Image Processing,\nvol. PP, no. 6, pp. 1–1, 2019.\n[18] M. Ye, X. Lan, and P. C. Yuen, “Robust anchor embed-\nding for unsupervised video person re-identiﬁcation in\nthe wild,” in Proceedings of the European Conference\non Computer Vision (ECCV), 2018, pp. 170–186.\n[19] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, “Person re-\nidentiﬁcation by local maximal occurrence representation\nand metric learning,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2015,\npp. 2197–2206.\n[20] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and\nQ. Tian, “Scalable person re-identiﬁcation: A bench-\nmark,” in Proceedings of the IEEE international con-\nference on computer vision, 2015, pp. 1116–1124.\n[21] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised\nrepresentation learning by predicting image rotations,”\nin The International Conference on Learning Represen-\ntations, 2018.\n[22] Z. Wu, Y. Xiong, S. Yu, and D. Lin, “Unsupervised\nfeature learning via non-parametric instance-level dis-\ncrimination,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2018.\n[23] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and\nQ. Tian, “Scalable person re-identiﬁcation: A bench-\nmark,” in 2015 IEEE International Conference on Com-\nputer Vision (ICCV), 2015, pp. 1116–1124.\n[24] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and\nC. Tomasi, “Performance measures and a data set for\nmulti-target, multi-camera tracking,” in European Con-\nference on Computer Vision workshop on Benchmarking\nMulti-Target Tracking, 2016.\n[25] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer\ngan to bridge domain gap for person re-identiﬁcation,”\nin 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2018.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\n[26] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, and Y. Yang,\n“Exploit the unknown gradually: One-shot video-based\nperson re-identiﬁcation by stepwise learning,” in 2018\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 5177–5186.\n[27] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang,\nand Q. Tian, “Mars: A video benchmark for large-scale\nperson re-identiﬁcation,” in Computer Vision – ECCV\n2016, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds.\nCham: Springer International Publishing, 2016, pp. 868–\n884.\n[28] G. Ding, S. H. Khan, and Z. Tang, “Dispersion based\nclustering for unsupervised person re-identiﬁcation.” in\nBMVC, 2019, p. 264.\n[29] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang, “In-\nvariance matters: Exemplar memory for domain adaptive\nperson re-identiﬁcation,” in 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\n2020.\n[30] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, and T. Huang,\n“Self-similarity grouping: A simple unsupervised cross\ndomain adaptation approach for person re-identiﬁcation,”\nin 2019 International Conference on Computer Vision\n(ICCV), 2018.\n[31] D. Wang and S. Zhang, “Unsupervised person re-\nidentiﬁcation via multi-label classiﬁcation,” in 2020\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020.\n[32] Y. Chen, X. Zhu, and S. Gong, “Deep association\nlearning for unsupervised video person re-identiﬁcation,”\narXiv preprint arXiv:1808.07301, 2018.\n[33] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, and Y. Yang,\n“Exploit the unknown gradually: One-shot video-based\nperson re-identiﬁcation by stepwise learning,” in Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2018, pp. 5177–5186.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-10-15",
  "updated": "2021-03-04"
}