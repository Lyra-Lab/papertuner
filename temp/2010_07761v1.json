{
  "id": "http://arxiv.org/abs/2010.07761v1",
  "title": "Unsupervised Bitext Mining and Translation via Self-trained Contextual Embeddings",
  "authors": [
    "Phillip Keung",
    "Julian Salazar",
    "Yichao Lu",
    "Noah A. Smith"
  ],
  "abstract": "We describe an unsupervised method to create pseudo-parallel corpora for\nmachine translation (MT) from unaligned text. We use multilingual BERT to\ncreate source and target sentence embeddings for nearest-neighbor search and\nadapt the model via self-training. We validate our technique by extracting\nparallel sentence pairs on the BUCC 2017 bitext mining task and observe up to a\n24.5 point increase (absolute) in F1 scores over previous unsupervised methods.\nWe then improve an XLM-based unsupervised neural MT system pre-trained on\nWikipedia by supplementing it with pseudo-parallel text mined from the same\ncorpus, boosting unsupervised translation performance by up to 3.5 BLEU on the\nWMT'14 French-English and WMT'16 German-English tasks and outperforming the\nprevious state-of-the-art. Finally, we enrich the IWSLT'15 English-Vietnamese\ncorpus with pseudo-parallel Wikipedia sentence pairs, yielding a 1.2 BLEU\nimprovement on the low-resource MT task. We demonstrate that unsupervised\nbitext mining is an effective way of augmenting MT datasets and complements\nexisting techniques like initializing with pre-trained contextual embeddings.",
  "text": "Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings\nPhillip Keung•\nJulian Salazar•\nYichao Lu•\nNoah A. Smith†‡\n•Amazon\n†University of Washington\n‡Allen Institute for AI\n{keung,julsal,yichaolu}@amazon.com\nnasmith@cs.washington.edu\nAbstract\nWe describe an unsupervised method to cre-\nate pseudo-parallel corpora for machine\ntranslation (MT) from unaligned text. We\nuse multilingual BERT to create source\nand target sentence embeddings for nearest-\nneighbor search and adapt the model via\nself-training.\nWe validate our technique\nby extracting parallel sentence pairs on the\nBUCC 2017 bitext mining task and ob-\nserve up to a 24.5 point increase (abso-\nlute) in F1 scores over previous unsuper-\nvised methods. We then improve an XLM-\nbased unsupervised neural MT system pre-\ntrained on Wikipedia by supplementing it\nwith pseudo-parallel text mined from the\nsame corpus, boosting unsupervised trans-\nlation performance by up to 3.5 BLEU on\nthe WMT’14 French-English and WMT’16\nGerman-English tasks and outperforming\nthe previous state-of-the-art.\nFinally, we\nenrich the IWSLT’15 English-Vietnamese\ncorpus with pseudo-parallel Wikipedia sen-\ntence pairs, yielding a 1.2 BLEU improve-\nment on the low-resource MT task.\nWe\ndemonstrate that unsupervised bitext min-\ning is an effective way of augmenting MT\ndatasets and complements existing tech-\nniques like initializing with pre-trained con-\ntextual embeddings.\n1\nIntroduction\nLarge corpora of parallel sentences are prereq-\nuisites for training models across a diverse set\nof applications, such as neural machine transla-\ntion (NMT; Bahdanau et al., 2015), paraphrase\ngeneration (Bannard and Callison-Burch, 2005),\nand aligned multilingual sentence embeddings\n(Artetxe and Schwenk, 2019b). Systems that ex-\ntract parallel corpora typically rely on various\ncross-lingual resources (e.g., bilingual lexicons,\nparallel corpora), but recent work has shown that\nunsupervised parallel sentence mining (Hangya\net al., 2018) and unsupervised NMT (Artetxe et al.,\n2018; Lample et al., 2018a) produce surprisingly\ngood results.1\nExisting approaches to unsupervised parallel\nsentence (or bitext) mining start from bilingual\nword embeddings (BWEs) learned via an unsuper-\nvised, adversarial approach (Lample et al., 2018b).\nHangya et al. (2018) created sentence representa-\ntions by mean-pooling BWEs over content words.\nTo disambiguate semantically similar but non-\nparallel sentences, Hangya and Fraser (2019) ad-\nditionally proposed parallel segment detection by\nsearching for paired substrings with high similar-\nity scores per word. However, using word em-\nbeddings to generate sentence embeddings ignores\nsentential context, which may degrade bitext re-\ntrieval performance.\nWe describe a new unsupervised bitext mining\napproach based on contextual embeddings.\nWe\ncreate sentence embeddings by mean-pooling the\noutputs of multilingual BERT (mBERT; Devlin\net al., 2019), which is pre-trained on unaligned\nWikipedia sentences across 104 languages. For a\npair of source and target languages, we ﬁnd candi-\ndate translations by using nearest-neighbor search\nwith margin-based similarity scores between pairs\nof mBERT-embedded source and target sentences.\nWe bootstrap a dataset of positive and negative\nsentence pairs from these initial neighborhoods\nof candidates, then self-train mBERT on its own\noutputs. A ﬁnal retrieval step gives a corpus of\npseudo-parallel sentence pairs, which we expect\nto be a mix of actual translations and semantically-\nrelated non-translations.\nWe apply our technique on the BUCC 2017 par-\n1By unsupervised, we mean that no cross-lingual re-\nsources like parallel text or bilingual lexicons are used. Unsu-\npervised techniques have been used to bootstrap MT systems\nfor low-resource languages like Khmer and Burmese (Marie\net al., 2019).\narXiv:2010.07761v1  [cs.CL]  15 Oct 2020\nFigure 1: Our self-training scheme. Left: We index sentences using our two encoders. For each source sentence,\nwe retrieve k nearest-neighbor target sentences per the margin criterion (Eq. 1), depicted here for k = 4. If the\nnearest neighbor is within a threshold, it is treated with the source sentence as a positive pair, and the remaining\nk −1 are treated with the source sentence as negative pairs. Right: We reﬁne one of the encoders such that the\ncosine similarity of the two embeddings is maximized on positive pairs and minimized on negative pairs.\nallel sentence mining task (Zweigenbaum et al.,\n2017). We achieve state-of-the-art F1 scores on\nunsupervised bitext mining, with an improvement\nof up to 24.5 points (absolute) on published re-\nsults (Hangya and Fraser, 2019).\nOther work\n(e.g., Libovick´y et al., 2019) has shown that re-\ntrieval performance varies substantially with the\nlayer of mBERT used to generate sentence repre-\nsentations; using the optimal mBERT layer yields\nan improvement as large as 44.9 points.\nFurthermore, our pseudo-parallel text improves\nunsupervised NMT (UNMT) performance.\nWe\nbuild upon the UNMT framework of Lample\net al. (2018c) and XLM (Lample and Conneau,\n2019) by incorporating our pseudo-parallel text\n(also derived from Wikipedia) at training time.\nThis boosts performance on WMT’14 En-Fr and\nWMT’16 En-De by up to 3.5 BLEU over the XLM\nbaseline, outperforming the state-of-the-art on un-\nsupervised NMT (Song et al., 2019).\nFinally, we demonstrate the practical value of\nunsupervised bitext mining in the low-resource\nsetting. We augment the English-Vietnamese cor-\npus (133k pairs) from the IWSLT’15 translation\ntask (Cettolo et al., 2015) with our pseudo-bitext\nfrom Wikipedia (400k pairs), and observe a 1.2\nBLEU increase over the best published model\n(Nguyen and Salazar, 2019). When we reduced\nthe amount of parallel and monolingual Viet-\nnamese data by a factor of ten (13.3k pairs),\nthe model trained with pseudo-bitext performed 7\nBLEU points better than a model trained on the\nreduced parallel text alone.\n2\nOur approach\nOur aim is to create a bilingual sentence em-\nbedding space where, for each source sentence\nembedding, a sufﬁciently close nearest neighbor\namong the target sentence embeddings is its trans-\nlation.\nBy aligning source and target sentence\nembeddings in this way, we can extract sentence\npairs to create new parallel corpora. Artetxe and\nSchwenk (2019a) construct this space by training\na joint encoder-decoder MT model over multiple\nlanguage pairs and using the resulting encoder to\ngenerate sentence embeddings. A margin-based\nsimilarity score is then computed between embed-\ndings for retrieval (Section 2.2). However, this ap-\nproach requires large parallel corpora to train the\nencoder-decoder model in the ﬁrst place.\nWe investigate whether contextualized sentence\nembeddings created with unaligned text are use-\nful for unsupervised bitext retrieval.\nPrevious\nwork explored the use of multilingual sentence\nencoders taken from machine translation mod-\nels (e.g., Artetxe and Schwenk, 2019b; Lu et al.,\n2018) for zero-shot cross-lingual transfer.\nOur\nwork is motivated by recent success in tasks\nlike zero-shot text classiﬁcation and named en-\ntity recognition (e.g., Keung et al., 2019; Mulcaire\net al., 2019) with multilingual contextual embed-\ndings, which exhibit cross-lingual properties de-\nspite being trained without parallel sentences.\nWe illustrate our method in Figure 1. We ﬁrst\nretrieve the candidate translation pairs:\n• Each source and target language sentence\nis converted into an embedding vector with\nmBERT via mean-pooling.\n• Margin-based scores are computed for each\nsentence pair using the k nearest neighbors\nof the source and target sentences (Sec. 2.2).\n• Each source sentence is paired with its near-\nest neighbor in the target language based on\nthis score.\n• We select a threshold score that keeps some\ntop percentage of pairs (Sec. 2.2).\n• Rule-based ﬁlters are applied to further re-\nmove mismatched sentence pairs (Sec. 2.3).\nThe remaining candidate pairs are used to boot-\nstrap a dataset for self-training mBERT as follows:\n• Each candidate pair (a source sentence and its\nclosest nearest neighbor above the threshold)\nis taken as a positive example.\n• This source sentence is also paired with its\nnext k −1 neighbors to give hard negative\nexamples (we compare this with random neg-\native samples in Sec. 3.3).\n• We ﬁnetune mBERT to produce sentence em-\nbeddings that discriminate between positive\nand negative pairs (Sec. 2.4).\nAfter self-training, the ﬁnetuned mBERT model\nis used to generate new sentence embeddings. Par-\nallel sentences should be closer to each other in\nthis new embedding space, which improves re-\ntrieval performance.\n2.1\nSentence embeddings and\nnearest-neighbor search\nWe use mBERT (Devlin et al., 2019) to create sen-\ntence embeddings for both languages by mean-\npooling the representations from the ﬁnal layer.\nWe use FAISS (Johnson et al., 2017) to perform\nexact nearest-neighbor search on the embeddings.\nWe compare every sentence in the source language\nto every sentence in the target language; we do\nnot use links between Wikipedia articles or other\nmetadata to reduce the size of the search space. In\nour experiments, we retrieve the k = 4 closest tar-\nget sentences for each source sentence; the source\nlanguage is always non-English, while the target\nlanguage is always English.\n2.2\nMargin-based score\nWe compute a margin-based similarity score be-\ntween each source sentence and its k nearest tar-\nget neighbors. Following Artetxe and Schwenk\n(2019a), we use the ratio margin score, which\ncalibrates the cosine similarity by dividing it by\nthe average cosine distance of each embedding’s k\nnearest neighbors:\nmargin(x, y) =\ncos(x, y)\nP\nz∈NNtgt\nk (x)\ncos(x,z)\n2k\n+ P\nz∈NNsrc\nk (y)\ncos(y,z)\n2k\n. (1)\nWe remove the sentence pairs with margin scores\nbelow some pre-selected threshold. For BUCC,\nwe do not have development data for tuning the\nthreshold hyperparameter, so we simply use the\nprior probability. For example, the creators of the\ndataset estimate that ∼2% of De sentences have\nan En translation, so we choose a score threshold\nsuch that we retrieve ∼2% of the pairs. We set\nthe threshold in the same way for the other BUCC\npairs. For UNMT with Wikipedia bitext mining,\nwe set the threshold such that we always retrieve\n2.5 million sentence pairs for each language pair.\n2.3\nRule-based ﬁltering\nWe also apply two simple ﬁltering steps before ﬁ-\nnalizing the candidate pairs list:\n• Digit ﬁltering:\nSentence pairs which are\ntranslations of each other must have digit se-\nquences that match exactly.2\n• Edit distance:\nSentences from English\nWikipedia sometimes appear in non-English\npages and vice versa.\nWe remove sen-\ntence pairs where the content of the source\nand target share substantial overlap (i.e., the\ncharacter-level edit distance is ≤50%).\n2In Python, set(re.findall(\"[0-9]+\",sent1))\n== set(re.findall(\"[0-9]+\",sent2)).\nMethod\nDe-En\nFr-En\nRu-En\nZh-En\nHangya and Fraser (2019)\navg.\n30.96\n44.81\n19.80\n-\nalign-static\n42.81\n42.21\n24.53\n-\nalign-dyn.\n43.35\n43.44\n24.97\n-\nOur method\nmBERT (ﬁnal layer)\n42.1\n45.8\n36.9\n35.8\n+ digit ﬁltering (DF)\n47.0\n49.3\n41.2\n38.0\n+ edit distance (ED)\n47.0\n49.3\n41.2\n38.0\n+ self-training (ST)\n60.6\n60.2\n49.5\n45.7\nmBERT (layer 8)\n67.0\n65.3\n59.3\n53.3\n+ DF, ED, ST\n74.9\n73.0\n69.9\n60.1\nTable 1: F1 scores for unsupervised bitext retrieval on BUCC 2017. Results with mBERT are from our method\n(Sec. 2) using the ﬁnal (12th) layer. We also include results for the 8th layer (e.g., Libovick´y et al., 2019), but do\nnot consider this part of the unsupervised setting as we would not have known a priori which layer was best to use.\n2.4\nSelf-training\nWe devise an unsupervised self-training tech-\nnique to improve mBERT for bitext retrieval using\nmBERT’s own outputs. For each source sentence,\nif the nearest target sentence is within the thresh-\nold and not ﬁltered out, the pair is treated as a pos-\nitive sentence. We then keep the next k−1 nearest\nneighbors as negative sentences. Altogether, these\ngive us a training set of examples which are la-\nbeled as positive or negative pairs.\nWe train mBERT to discriminate between posi-\ntive and negative sentence pairs as a binary classi-\nﬁcation task. We distinguish the mBERT encoders\nfor the source and target languages as fsrc, ftgt re-\nspectively. Our training objective is\nL(X, Y ; Θsrc) =\n\f\f\f\f\nfsrc(X; Θsrc)⊤ftgt(Y )\n||fsrc(X; Θsrc)||||ftgt(Y )|| −Par(X, Y )\n\f\f\f\f ,\n(2)\nwhere fsrc(X) and ftgt(Y ) are the mean-pooled\nrepresentations of the source sentence X and tar-\nget sentence Y , and where Par(X, Y ) is 1 if\nX, Y are parallel and 0 otherwise. This loss en-\ncourages the cosine similarity between the source\nand target embeddings to increase for positive\npairs and decrease otherwise. The process is de-\npicted in Figure 1.\nNote that we only ﬁnetune fsrc (parameters\nΘsrc) and we hold ftgt ﬁxed. If both fsrc and ftgt\nare updated, then the training process collapses\nto a trivial solution, since the model will map all\npseudo-parallel pairs to one representation and all\nnon-parallel pairs to another. We hold ftgt ﬁxed,\nwhich forces fsrc to align its outputs to the target\n(in our experiments, always English) mBERT em-\nbeddings.\nAfter ﬁnetuning, we use the updated fsrc to gen-\nerate new non-English sentence embeddings. We\nthen repeat the retrieval process with FAISS, yield-\ning a ﬁnal set of pseudo-parallel pairs after thresh-\nolding and ﬁltering.\n3\nUnsupervised bitext mining\nWe apply our method to the BUCC 2017 shared\ntask, “Spotting Parallel Sentences in Comparable\nCorpora” (Zweigenbaum et al., 2017). The task\ninvolves retrieving parallel sentences from mono-\nlingual corpora derived from Wikipedia.\nParal-\nlel sentences were inserted into the corpora in a\ncontextually-appropriate manner by the task or-\nganizers. The shared task assessed retrieval sys-\ntems for precision, recall, and F1-score on four\nlanguage pairs: De-En, Fr-En, Ru-En, and Zh-\nEn.\nPrior work on unsupervised bitext mining\nhas generally studied the European language pairs\nto avoid dealing with Chinese word segmentation\n(Hangya et al., 2018; Hangya and Fraser, 2019).\n3.1\nSetup\nFor each BUCC language pair, we take the cor-\nresponding source and target monolingual corpus,\nwhich have been pre-split into training, sample,\nand test sets at a ratio of 49%–2%–49%. The iden-\ntity of the parallel sentence pairs for the test set\nwere not publicly released, and are only available\nfor the training set. Following the convention es-\ntablished in Hangya and Fraser (2019) and Artetxe\nLanguage pair\nParallel sentence pair\nDe-En\nBeide Elemente des amerikanischen Traums haben heute einen Teil ihrer Anziehungskraft verloren.\nBoth elements of the American dream have now lost something of their appeal.\nFr-En\nL’Allemagne `a elle seule s’attend `a recevoir pas moins d’un million de demandeurs d’asile cette ann´ee.\nGermany alone expects as many as a million asylum-seekers this year.\nRu-En\nОднако\nпо\nрешению\nБерлинского\nконгресса\nв\n1881\nгоду\nк\nтерритории\nГреции\nприсоединилась Фессалия и часть Эпира.\nNevertheless, in 1881, Thessaly and small parts of Epirus were ceded to Greece as part of the Treaty of\nBerlin.\nZh-En\n在如今这个奇怪的新世界里，现代和前现代相互依存。\nIn the strange new world of today, the modern and the pre-modern depend on each other.\nTable 2: Examples of parallel sentences that were extracted by our method on the BUCC 2017 shared task.\nand Schwenk (2019a), we use the test portion for\nunsupervised system development and evaluate on\nthe training portion.\nWe use the reference FAISS implementation3\nfor nearest-neighbor search.\nWe used the Glu-\nonNLP toolkit (Guo et al., 2020) with pre-trained\nmBERT weights4 for inference and self-training.\nWe compute the margin similarity score in Eq. 1\nwith k = 4 nearest neighbors. We set a threshold\non the score such that we retrieve the prior propor-\ntion (e.g., ∼2%) of parallel pairs in each language.\nWe then ﬁnetune mBERT via self-training. We\ntake minibatches of 100 sentence pairs. We use\nthe Adam optimizer with a constant learning rate\nof 0.00001 for 2 epochs. To avoid noisy transla-\ntions, we ﬁnetune on the top 50% of the highest-\nscoring pairs from the retrieved bitext (e.g., if the\nprior proportion is 2%, then we would use the top\n1% of sentence pairs for self-training).\nWe considered performing more than one round\nof self-training but found it was not helpful for the\nBUCC task. BUCC has very few parallel pairs\n(e.g., 9,000 pairs for Fr-En) per language and thus\nfew positive pairs for our unsupervised method to\nﬁnd. The size of the self-training corpus is lim-\nited by the proportion of parallel sentences, and\nmBERT rapidly overﬁts to small datasets.\n3.2\nResults\nWe provide a few examples of the bitext we re-\ntrieved in Table 2.\nThe examples were chosen\nfrom the high-scoring pairs and veriﬁed to be cor-\nrect translations.\n3https://github.com/facebookresearch/\nfaiss\n4https://github.com/google-research/\nbert/blob/master/multilingual.md\nOur retrieval results are in Table 1. We compare\nour results with strictly unsupervised techniques,\nwhich do not use bilingual lexicons, parallel text,\nor other cross-lingual resources. Using mBERT\nas-is with the margin-based score works reason-\nably well, giving F1 scores in the range of 35.8 to\n45.8, which is competitive with the previous state-\nof-the-art for some pairs, and outperforming by 12\npoints in the case of Ru-En. Furthermore, apply-\ning simple rule-based ﬁlters (Sec. 2.3) on the can-\ndidate translation pairs adds a few more points, al-\nthough the edit distance ﬁlter has a negligible ef-\nfect when compared with the digit ﬁlter.\nWe see that ﬁnetuning mBERT on its own cho-\nsen sentence pairs (i.e., unsupervised self-training)\nyields signiﬁcant improvements, adding another 8\nto 14 points to the F1 score on top of ﬁltering. In\nall, these F1 scores represent a 34% to 98% rela-\ntive improvement over existing techniques in un-\nsupervised parallel sentence extraction for these\nlanguage pairs.\nLibovick´y et al. (2019) explored bitext mining\nwith mBERT in the supervised context and found\nthat retrieval performance signiﬁcantly varies with\nthe mBERT layer used to create sentence embed-\ndings. In particular, they found layer 8 embed-\ndings gave the highest precision-at-1. We also ob-\nserve an improvement (Table 1) in unsupervised\nretrieval of another 13 to 20 points by using the 8th\nlayer instead of the default ﬁnal layer (12th). We\ninclude these results but do not consider them un-\nsupervised, as we would not know a priori which\nlayer was best to use.\n3.3\nChoosing negative sentence pairs\nOther authors (e.g., Guo et al., 2018) have noted\nthat the choice of negative examples has a con-\nsiderable impact on metric learning. Speciﬁcally,\nusing negative examples which are difﬁcult to dis-\ntinguish from the positive nearest neighbor is of-\nten beneﬁcial for performance. We examine the\nimpact of taking random sentences instead of the\nremaining k −1 nearest neighbors as the negatives\nduring self-training.\nOur results are in Table 3. While self-training\nwith random negatives still greatly improves the\nuntuned baseline, the use of hard negative exam-\nples mined from the k-nearest neighborhood can\nmake a signiﬁcant difference to the ﬁnal F1 score.\nMethod\nDe-En\nFr-En\nRu-En\nZh-En\nmBERT w/o ST\n47.0\n49.3\n41.2\n38.0\nw/ ST (random)\n57.7\n55.7\n48.1\n45.2\nw/ ST (hard)\n60.6\n60.2\n49.5\n45.7\nTable 3: F1 scores for bitext retrieval on BUCC 2017\nusing random sentences as negative samples instead of\nnearest neighbors.\n4\nBitext for neural machine translation\nA major application of bitext mining is to cre-\nate new corpora for machine translation.\nWe\nconduct an extrinsic evaluation of our unsuper-\nvised bitext mining approach on unsupervised\n(WMT’14 French-English, WMT’16 German-\nEnglish) and low-resource (IWSLT’15 English-\nVietnamese) translation tasks.\nWe perform large-scale unsupervised bitext ex-\ntraction on the October 2019 Wikipedia dumps\nin various languages. We use wikifil.pl5 to\nextract paragraphs from Wikipedia and remove\nmarkup. We then use the syntok6 package for\nsentence segmentation. Finally, we reduce the size\nof the corpus by removing sentences that aren’t\npart of the body of Wikipedia pages. Sentences\nthat contain *, =, //, ::, #, www, (talk), or the\npattern [0-9]{2}:[0-9]{2} are ﬁltered out.\nWe index, retrieve, and ﬁlter candidate sentence\npairs with the procedure in Sec. 3. Unlike BUCC,\nthe Wikipedia dataset does not ﬁt in GPU mem-\nory.\nThe processed corpus is quite large, with\n133 million, 67 million, 36 million, and 6 mil-\nlion sentences in English, German, French, and\nVietnamese respectively. We therefore shard the\n5https://github.com/facebookresearch/\nfastText/blob/master/wikifil.pl\n6https://github.com/fnl/syntok\ndataset into chunks of 32,768 sentences and per-\nform nearest-neighbor comparisons in chunks for\neach language pair. We use a simple map-reduce\nalgorithm to merge the intermediate results back\ntogether.\nWe follow the approach outlined in Sec. 2 for\nWikipedia bitext mining.\nFor each source sen-\ntence, we retrieve the 4 nearest target neighbors\nacross the millions of sentences that we extracted\nfrom Wikipedia and compute the margin-based\nscores for each pair.\n4.1\nUnsupervised NMT\nWe show that our pseudo-parallel text can comple-\nment existing techniques for unsupervised transla-\ntion (Artetxe et al., 2018; Lample et al., 2018c).\nIn line with existing work on UNMT, we evaluate\nour approach on the WMT’14 Fr-En and WMT’16\nDe-En test sets.\nOur UNMT experiments build upon the refer-\nence implementation7 of XLM (Lample and Con-\nneau, 2019). The UNMT model is trained by al-\nternating between two steps: a denoising auto-\nencoder step and a backtranslation step (refer to\nLample et al. (2018c) for more details). The back-\ntranslation step generates pseudo-parallel training\ndata, and we incorporate our bitext during UNMT\ntraining in the same way, as another set of pseudo-\nparallel sentences. We also use the same initial-\nization as Lample and Conneau (2019), where the\nUNMT models have encoders and decoders that\nare initialized with contextual embeddings trained\non the source and target language Wikipedia cor-\npora with the masked language model (MLM) ob-\njective; no parallel data is used.\nWe performed the exhaustive (Fr Wiki)-(En\nWiki) and (De Wiki)-(En Wiki) nearest-neighbor\ncomparison on eight V100 GPUs, which requires\n3 to 4 days to complete per language pair. We\nretained the top 2.5 million pseudo-parallel Fr-En\nand De-En sentence pairs after mining.\n4.2\nResults\nOur results are in Table 4. The addition of mined\nbitext consistently increases the BLEU score in\nboth directions for WMT’14 Fr-En and WMT’16\nDe-En.\nMuch of the existing work on improv-\ning UNMT focuses on improved initialization with\ncontextual embeddings like XLM or MASS (Song\n7https://github.com/facebookresearch/\nxlm\nReference\nArchitecture\nPre-training\nEn-De\nDe-En\nEn-Fr\nFr-En\nArtetxe et al. (2018)\n2-layer RNN\n6.89\n10.16\n15.13\n15.56\nLample et al. (2018a)\n3-layer RNN\n9.75\n13.33\n15.05\n14.31\nYang et al. (2018)\n4-layer Transformer\n10.86\n14.62\n16.97\n15.58\nLample et al. (2018c)\n4-layer Transformer\n17.16\n21.00\n25.14\n24.18\nSong et al. (2019)\n6-layer Transformer\nMASS\n28.3\n35.2\n37.5\n34.9\nXLM Baselines\nLample and Conneau (2019)\n6-layer Transformer\nXLM\n–\n–\n33.4\n33.3\nSong et al. (2019)\n6-layer Transformer\nXLM\n27.0\n34.3\n33.4\n33.3\nXLM reference implementation\n6-layer Transformer\nXLM\n–\n–\n36.6\n34.0\nMaximum performance across baselines\n6-layer Transformer\nXLM\n27.0\n34.3\n36.6\n34.0\nOurs\nOur XLM baseline\n6-layer Transformer\nXLM\n27.7\n34.5\n36.7\n34.5\nw/ pseudo-parallel text before ST\n6-layer Transformer\nXLM\n30.4\n36.3\n39.7\n35.9\nw/ pseudo-parallel text after ST\n6-layer Transformer\nXLM\n30.7\n37.3\n40.2\n36.9\nTable 4: BLEU scores for unsupervised NMT performance on WMT’14 English-French and WMT’16 English-\nGerman test sets. All methods only use unaligned Wikipedia corpora for pre-training and/or bitext mining. ‘ST’\nrefers to self-training.\net al., 2019). These embeddings were already pre-\ntrained on Wikipedia data, so it is surprising that\nadding our pseudo-parallel Wikipedia sentences\nleads to a 2 to 3 BLEU improvement. In other\nwords, our approach is complementary to pre-\ntrained initialization techniques.\nPreviously (in Table 1), we saw that self-\ntraining improved the F1 score for BUCC bitext\nretrieval. The improvement in bitext quality car-\nries over to UNMT, and providing better pseudo-\nparallel text yields a consistent improvement for\nall translation directions.\nOur results are state-of-the-art in UNMT, but\nthey should be interpreted relative to the strength\nof our XLM baseline.\nWe are building on top\nof the XLM initialization, and the effectiveness\nof the initialization (and the various hyperparame-\nters used during training and decoding) affects the\nstrength of our ﬁnal results. For example, we ad-\njusted the beam width on our XLM baselines to\nattain BLEU scores which are similar to what oth-\ners have published. One can apply our method\nto MASS, which performs better than XLM on\nUNMT, but we chose to report results on XLM\nbecause it has been validated on a wider range of\ntasks and languages.\nWe also trained a standard 6-layer transformer\nencoder-decoder model directly on the pseudo-\nparallel text. We used the standard implementa-\ntion in Sockeye (Hieber et al., 2018) as-is, and\ntrained models for French and German on 2.5 mil-\nlion Wikipedia sentence pairs. We withheld 10k\npseudo-parallel pairs per language pair to serve as\na development set. We achieved BLEU scores of\n20.8, 21.1, 28.2, and 28.0 on En-De, De-En, En-\nFr, and Fr-En respectively.\nBLEU scores were\ncomputed with SacreBLEU (Post, 2018).\nThis\ncompares favorably with the best UNMT results\nin Lample et al. (2018c), while avoiding the use of\nparallel development data altogether.\n4.3\nLow-resource NMT\nFrench and German are high-resource languages\nand are linguistically close to English.\nWe\ntherefore evaluate our mined bitext on a low-\nresource, linguistically distant language pair. The\nIWSLT’15 English-Vietnamese MT task (Cettolo\net al., 2015) provides 133k sentence pairs de-\nrived from translated TED talks transcripts and is\na common benchmark for low-resource MT. We\ntake supervised training data from the IWSLT task\nand augment it with different amounts of pseudo-\nparallel text mined from English and Vietnamese\nWikipedia. Furthermore, we construct a very low-\nresource setting by downsampling the parallel text\nand monolingual Vietnamese Wikipedia text by a\nfactor of ten (13.3k sentence pairs).\nWe use the reference implementation8 for\nthe state-of-the-art model (Nguyen and Salazar,\n2019), which is a highly regularized 6+6-layer\ntransformer with pre-norm residual connections,\n8https://github.com/tnq177/\ntransformers_without_tears\nscale normalization, and normalized word embed-\ndings. We use the same hyperparameters (except\nfor the dropout rate) but train on our augmented\ndatasets. To mitigate domain shift, we ﬁnetune\nthe best checkpoint for 75k more steps using only\nthe IWSLT training data, in the spirit of “trivial”\ntransfer learning for low-resource NMT (Kocmi\nand Bojar, 2018).\nIn Table 5, we show BLEU scores as more\npseudo-parallel text is included during training.\nAs in previous works on En-Vi (cf. Luong and\nManning, 2015), we use tst2012 (1553 pairs)\nand tst2013 (1268 pairs) as our development\nand test sets respectively, we tokenize all data\nwith Moses, and we report tokenized BLEU\nvia multi-bleu.perl. The BLEU score in-\ncreases monotonically with the size of the pseudo-\nparallel corpus and exceeds the state-of-the-art\nsystem’s BLEU by 1.2 points. This result is con-\nsistent with improvements observed with other\ntypes of monolingual data augmentation like pre-\ntrained UNMT initialization, various forms of\nback-translation (Hoang et al., 2018; Zhou and\nKeung, 2020), and cross-view training (CVT;\nClark et al., 2018):\nEn-Vi\nLuong and Manning (2015)\n26.4\nClark et al. (2018)\n28.9\nClark et al. (2018), with CVT\n29.6\nXu et al. (2019)\n31.4\nNguyen and Salazar (2019)\n32.8 (28.8)\n+ top 100k mined pairs\n33.2 (29.5)\n+ top 200k mined pairs\n33.9 (29.8)\n+ top 300k mined pairs\n34.0 (30.0)\n+ top 400k mined pairs\n34.1 (29.9)\nTable 5: Tokenized BLEU scores on tst2013 for the\nlow-resource IWSLT’15 English-Vietnamese transla-\ntion task using bitext mined with our method. Added\npairs are sorted by their score. Development scores on\ntst2012 in parentheses.\nWe describe our hyperparameter tuning and in-\nfrastructure following Dodge et al. (2019). The\ntranslation sections of this work mostly used de-\nfault parameters, but we did tune the dropout rate\n(at 0.2 and 0.3) for each amount of mined bitext\nfor the supervised En-Vi task (at 100k, 200k, 300k\nand 400k sentence pairs).\nWe include develop-\nment scores for our best models; dropout of 0.3\ndid best for 0k and 100k, while 0.2 did best other-\nwise. Training takes less than a day on one V100\nGPU.\nTo simulate a very low-resource task, we use\none-tenth of the training data by downsampling\nthe IWSLT En-Vi train set to 13.3k sentence pairs.\nFurthermore, we mine bitext from one-tenth of the\nmonolingual Wiki Vi text and extract proportion-\nately fewer sentence pairs (i.e., 10k, 20k, 30k and\n40k pairs). We use the implementation and hy-\nperparameters for the regularized 4+4-layer trans-\nformer used by Nguyen and Salazar (2019) in a\nsimilar setting. We tune the dropout rate (0.2, 0.3,\n0.4) to maximize development performance; 0.4\nwas best for 0k, 0.3 for 10k and 20k, and 0.2 for\n30k and 40k. In Table 6, we see larger improve-\nments in BLEU (4+ points) for the same relative\nincreases in mined data (as compared to Table 5).\nIn both cases, the rate of improvement tapers off\nas the quality and relative quantity of mined pairs\ndegrades at each increase.\nEn-Vi, one-tenth\n13.3k pairs (from 133k original)\n20.7 (19.5)\n+ top 10k mined pairs\n25.0 (22.9)\n+ top 20k mined pairs\n26.7 (24.1)\n+ top 30k mined pairs\n27.3 (24.5)\n+ top 40k mined pairs\n27.7 (24.7)\nTable 6: Tokenized BLEU scores (tst2013), where the\nbitext was mined from one-tenth of the monolingual\nVietnamese data. Development scores on tst2012 in\nparentheses.\n4.4\nUNMT ablation study: Pre-training and\nbitext mining corpora\nIn Sec. 4.2, we mined bitext from the October\n2019 Wikipedia snapshot whereas the pre-trained\nXLM embeddings were created prior to January\n2019. Hence, it is possible that the UNMT BLEU\nincrease would be smaller if the bitext were mined\nfrom the same corpus used for pre-training. We\nran an ablation study to show the effect (or lack\nthereof) of the overlap between the pre-training\nand pseudo-parallel corpora.\nFor the En-Vi language pair, we used 5 million\nEnglish and 5 million Vietnamese Wiki sentences\nto pre-train the XLM model. We only use text\nfrom the October 2019 Wiki snapshot. We mined\n300k pseudo-parallel sentence pairs using our ap-\nproach (Sec. 2) from the same Wiki snapshot.\nWe created two datasets for XLM pre-training: a\n10 million-sentence corpus that is disjoint from\nthe 600k sentences of the mined bitext, and a 10\nmillion-sentence corpus that contains all 600k sen-\ntences of the bitext. In Table 7, we show the BLEU\nincrease on the IWSLT En-Vi task with and with-\nout using the mined bitext as parallel data, using\neach of the two XLM models as the initialization.\nThe beneﬁt of using pseudo-parallel text is very\nclear; even if the pre-trained XLM model saw\nthe pseudo-parallel sentences during pre-training,\nusing mined bitext still signiﬁcantly improves\nUNMT performance (23.1 vs. 28.3 BLEU). In\naddition, the baseline UNMT performance with-\nout the mined bitext is similar between the two\nXLM initializations (23.1 vs. 23.2 BLEU), which\nsuggests that removing some of the parallel text\npresent during pre-training does not have a major\neffect on UNMT.\nw/o PP as bitext\nw/ PP as bitext\nXLM excl. PP text\n23.2\n28.9\nXLM incl. PP text\n23.1\n28.3\nTable 7: Tokenized UNMT BLEU scores on IWSLT’15\nEnglish-Vietnamese (tst2013) with XLM initialization.\nWe mined 300k pseudo-parallel (PP) sentence pairs\nfrom En and Vi Wikipedia (Oct. 2019). We created two\nXLM models, with the pre-training corpus including or\nexcluding the PP pairs. We compare their downstream\nUNMT performance with and without PP pairs as “bi-\ntext” during UNMT training.\nFinally, we trained a standard encoder-decoder\nmodel on the 300k pseudo-parallel pairs only, us-\ning the same Sockeye recipe in Sec. 4.2.\nThis\nyielded a BLEU score of 27.5 on En-Vi, which is\nlower than the best XLM-based result (i.e., 28.9),\nwhich suggests that the XLM initialization im-\nproves unsupervised NMT. A similar outcome was\nalso reported in Lample and Conneau (2019).\n5\nRelated work\n5.1\nParallel sentence mining\nApproaches to parallel sentence (or bitext) mining\nhave been historically driven by the data require-\nments of statistical machine translation.\nSome\nof the earliest work in mining the web for large-\nscale parallel corpora can be found in Resnik\n(1998) and Resnik and Smith (2003). Recent in-\nterest in the ﬁeld is reﬂected by new shared tasks\non parallel extraction and ﬁltering (Zweigenbaum\net al., 2017; Koehn et al., 2018) and the creation\nof massively-multilingual parallel corpora mined\nfrom the web, like WikiMatrix (Schwenk et al.,\n2019a) and CCMatrix (Schwenk et al., 2019b).\nExisting parallel corpora have been exploited in\nmany ways to create sentence representations for\nsupervised bitext mining. One approach involves\na joint encoder with a shared wordpiece vocabu-\nlary, trained as part of multiple encoder-decoder\ntranslation models on parallel corpora (Schwenk,\n2018). Artetxe and Schwenk (2019b) apply this\napproach at scale, and shared a single encoder\nand joint vocabulary across 93 languages.\nAn-\nother approach uses negative sampling to align\nthe encoders’ sentence representations for nearest-\nneighbor retrieval (Gr´egoire and Langlais, 2018;\nGuo et al., 2018).\nHowever, these approaches require training\nwith initial parallel corpora. In contrast, Hangya\net al. (2018) and Hangya and Fraser (2019) pro-\nposed unsupervised methods for parallel sentence\nextraction that use bilingual word embeddings in-\nduced in an unsupervised manner. Our work is\nthe ﬁrst to explore using contextual representa-\ntions (mBERT; Devlin et al., 2019) in an unsu-\npervised manner to mine for bitext, and to show\nimprovements over the latest UNMT systems\n(Lample and Conneau, 2019; Song et al., 2019),\nfor which transformers and encoder/decoder pre-\ntraining have doubled or tripled BLEU scores on\nunsupervised WMT’16 En-De since Artetxe et al.\n(2018) and Lample et al. (2018c).\n5.2\nSelf-training techniques\nSelf-training refers to techniques that use the out-\nputs of a model to provide labels for its own train-\ning. Yarowsky (1995) proposed a semi-supervised\nstrategy where a model is ﬁrst trained on a small\nset of labeled data and then used to assign pseudo-\nlabels to unlabeled data.\nSemi-supervised self-\ntraining has been used to improve sentence en-\ncoders that project sentences into a common se-\nmantic space. For example, Clark et al. (2018)\nproposed cross-view training (CVT) with labeled\nand unlabeled data to achieve state-of-the-art re-\nsults on a set of sequence tagging, MT, and depen-\ndency parsing tasks.\nSemi-supervised methods require some anno-\ntated data, even if it is not directly related to the\ntarget task. Our work is the ﬁrst to apply unsu-\npervised self-training for generating cross-lingual\nsentence embeddings. The most similar approach\nto ours is the prevailing scheme for unsupervised\nNMT (Lample et al., 2018c), which relies on mul-\ntiple iterations of backtranslation (Sennrich et al.,\n2016) to create a sequence of pseudo-parallel sen-\ntence pairs with which to bootstrap an MT model.\n6\nConclusion\nIn this work, we describe a novel approach for\nstate-of-the-art unsupervised bitext mining using\nmultilingual contextual representations.\nWe ex-\ntract pseudo-parallel sentences from unaligned\ncorpora to create models that achieve state-of-the-\nart performance on unsupervised and low-resource\ntranslation tasks. Our approach is complementary\nto the improvements derived from initializing MT\nmodels with pre-trained encoders and decoders,\nand helps narrow the gap between unsupervised\nand supervised MT. We focused on mBERT-based\nembeddings in our experiments, but we expect un-\nsupervised self-training to improve the unsuper-\nvised bitext mining and downstream UNMT per-\nformance of other forms of multilingual contex-\ntual embeddings as well.\nOur ﬁndings are in line with recent work show-\ning that multilingual embeddings are very use-\nful for cross-lingual zero-shot and zero-resource\ntasks.\nEven without using aligned corpora,\nmBERT can embed sentences across different lan-\nguages in a consistent fashion according to their\nsemantic content. More work will be needed to\nunderstand how contextual embeddings discover\nthese cross-lingual correspondences.\nAcknowledgments\nWe would like to thank the anonymous reviewers\nfor their thoughtful comments.\nReferences\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018.\nUnsupervised neu-\nral machine translation.\nIn 6th International\nConference on Learning Representations, ICLR\n2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings. Open-\nReview.net.\nMikel Artetxe and Holger Schwenk. 2019a.\nMargin-based parallel corpus mining with mul-\ntilingual sentence embeddings. In Proceedings\nof the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3197–\n3203, Florence, Italy. Association for Compu-\ntational Linguistics.\nMikel Artetxe and Holger Schwenk. 2019b. Mas-\nsively multilingual sentence embeddings for\nzero-shot cross-lingual transfer and beyond.\nTransactions of the Association for Computa-\ntional Linguistics, 7:597–610.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by\njointly learning to align and translate. In 3rd In-\nternational Conference on Learning Represen-\ntations, ICLR 2015, San Diego, CA, USA, May\n7-9, 2015, Conference Track Proceedings.\nColin Bannard and Chris Callison-Burch. 2005.\nParaphrasing with bilingual parallel corpora.\nIn Proceedings of the 43rd Annual Meeting of\nthe Association for Computational Linguistics\n(ACL’05), pages 597–604, Ann Arbor, Michi-\ngan. Association for Computational Linguistics.\nMauro Cettolo, Niehues Jan, St¨uker Sebastian,\nLuisa Bentivogli, Roldano Cattoni, and Mar-\ncello Federico. 2015. The IWSLT 2015 eval-\nuation campaign.\nIn Proceedings of the 12th\nInternational Workshop on Spoken Language\nTranslation, pages 2–14, Da Nang, Vietnam.\nKevin Clark, Minh-Thang Luong, Christopher D.\nManning, and Quoc Le. 2018. Semi-supervised\nsequence modeling with cross-view training. In\nProceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing,\npages 1914–1925, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding.\nIn Proceedings of the 2019\nConference of the North American Chapter\nof the Association for Computational Linguis-\ntics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 4171–4186,\nMinneapolis, Minnesota. Association for Com-\nputational Linguistics.\nJesse Dodge, Suchin Gururangan, Dallas Card,\nRoy Schwartz, and Noah A. Smith. 2019. Show\nyour work: Improved reporting of experimen-\ntal results.\nIn Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International\nJoint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2185–2194,\nHong Kong, China. Association for Computa-\ntional Linguistics.\nFrancis Gr´egoire and Philippe Langlais. 2018. Ex-\ntracting parallel sentences with bidirectional re-\ncurrent neural networks to improve machine\ntranslation. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguis-\ntics, pages 1442–1453, Santa Fe, New Mexico,\nUSA. Association for Computational Linguis-\ntics.\nJian Guo, He He, Tong He, Leonard Lausen,\nMu Li, Haibin Lin, Xingjian Shi, Chenguang\nWang, Junyuan Xie, Sheng Zha, Aston Zhang,\nHang Zhang, Zhi Zhang, Zhongyue Zhang,\nShuai Zheng, and Yi Zhu. 2020. GluonCV and\nGluonNLP: Deep learning in computer vision\nand natural language processing.\nJournal of\nMachine Learning Research, 21:23:1–23:7.\nMandy Guo, Qinlan Shen, Yinfei Yang, Heming\nGe, Daniel Cer, Gustavo Hernandez Abrego,\nKeith Stevens,\nNoah Constant,\nYun-Hsuan\nSung, Brian Strope, and Ray Kurzweil. 2018.\nEffective parallel corpus mining using bilingual\nsentence embeddings.\nIn Proceedings of the\nThird Conference on Machine Translation: Re-\nsearch Papers, pages 165–176, Brussels, Bel-\ngium. Association for Computational Linguis-\ntics.\nViktor Hangya, Fabienne Braune, Yuliya Kala-\nsouskaya, and Alexander Fraser. 2018. Unsu-\npervised parallel sentence extraction from com-\nparable corpora.\nIn Proceedings of the 15th\nInternational Workshop on Spoken Language\nTranslation, pages 7–13, Bruges, Belgium.\nViktor Hangya and Alexander Fraser. 2019. Unsu-\npervised parallel sentence extraction with paral-\nlel segment detection helps machine translation.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 1224–1234, Florence, Italy. Association\nfor Computational Linguistics.\nFelix\nHieber,\nTobias\nDomhan,\nMichael\nDenkowski,\nDavid\nVilar,\nArtem\nSokolov,\nAnn Clifton, and Matt Post. 2018. The Sockeye\nneural machine translation toolkit at AMTA\n2018. In Proceedings of the 13th Conference\nof the Association for Machine Translation in\nthe Americas (Volume 1:\nResearch Papers),\npages 200–207, Boston, MA. Association for\nMachine Translation in the Americas.\nVu Cong Duy Hoang, Philipp Koehn, Gholam-\nreza Haffari, and Trevor Cohn. 2018. Iterative\nback-translation for neural machine translation.\nIn Proceedings of the 2nd Workshop on Neu-\nral Machine Translation and Generation, pages\n18–24, Melbourne, Australia. Association for\nComputational Linguistics.\nJeff Johnson, Matthijs Douze, and Herv´e J´egou.\n2017.\nBillion-scale similarity search with\nGPUs. CoRR, abs/1702.08734v1.\nPhillip Keung, Yichao Lu, and Vikas Bhardwaj.\n2019. Adversarial learning with contextual em-\nbeddings for zero-resource cross-lingual classi-\nﬁcation and NER. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 1355–1360,\nHong Kong, China. Association for Computa-\ntional Linguistics.\nTom Kocmi and Ondˇrej Bojar. 2018. Trivial trans-\nfer learning for low-resource neural machine\ntranslation. In Proceedings of the Third Confer-\nence on Machine Translation: Research Papers,\npages 244–252, Brussels, Belgium. Association\nfor Computational Linguistics.\nPhilipp\nKoehn,\nHuda\nKhayrallah,\nKenneth\nHeaﬁeld, and Mikel L. Forcada. 2018. Findings\nof the WMT 2018 shared task on parallel corpus\nﬁltering.\nIn Proceedings of the Third Con-\nference on Machine Translation: Shared Task\nPapers, pages 726–739, Belgium, Brussels.\nAssociation for Computational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019.\nCross-lingual language model pretraining.\nIn\nAdvances in Neural Information Processing\nSystems 32: Annual Conference on Neural In-\nformation Processing Systems 2019, NeurIPS\n2019, 8-14 December 2019, Vancouver, BC,\nCanada, pages 7057–7067.\nGuillaume Lample, Alexis Conneau, Ludovic De-\nnoyer, and Marc’Aurelio Ranzato. 2018a. Un-\nsupervised machine translation using monolin-\ngual corpora only. In 6th International Confer-\nence on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3,\n2018, Conference Track Proceedings. OpenRe-\nview.net.\nGuillaume\nLample,\nAlexis\nConneau,\nMarc’Aurelio\nRanzato,\nLudovic\nDenoyer,\nand Herv´e J´egou. 2018b.\nWord translation\nwithout parallel data.\nIn 6th International\nConference on Learning Representations, ICLR\n2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings.\nOpenReview.net.\nGuillaume Lample, Myle Ott, Alexis Conneau,\nLudovic Denoyer, and Marc’Aurelio Ranzato.\n2018c.\nPhrase-based & neural unsupervised\nmachine translation. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing, pages 5039–5049, Brus-\nsels, Belgium. Association for Computational\nLinguistics.\nJindrich Libovick´y, Rudolf Rosa, and Alexander\nFraser. 2019. How language-neutral is multilin-\ngual BERT? CoRR, abs/1911.03310v1.\nYichao Lu, Phillip Keung, Faisal Ladhak, Vikas\nBhardwaj, Shaonan Zhang, and Jason Sun.\n2018. A neural interlingua for multilingual ma-\nchine translation. In Proceedings of the Third\nConference on Machine Translation: Research\nPapers, pages 84–92, Brussels, Belgium. Asso-\nciation for Computational Linguistics.\nMinh-Thang Luong and Christopher D. Manning.\n2015. Stanford neural machine translation sys-\ntems for spoken language domains.\nIn Pro-\nceedings of the 12th International Workshop on\nSpoken Language Translation, pages 76–79, Da\nNang, Vietnam.\nBenjamin Marie, Hour Kaing, Aye Myat Mon,\nChenchen\nDing,\nAtsushi\nFujita,\nMasao\nUtiyama, and Eiichiro Sumita. 2019.\nSuper-\nvised and unsupervised machine translation\nfor Myanmar-English and Khmer-English. In\nProceedings of the 6th Workshop on Asian\nTranslation, pages 68–75, Hong Kong, China.\nAssociation for Computational Linguistics.\nPhoebe Mulcaire, Jungo Kasai, and Noah A.\nSmith. 2019.\nPolyglot contextual representa-\ntions improve crosslingual transfer. In Proceed-\nings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages\n3912–3918, Minneapolis, Minnesota. Associa-\ntion for Computational Linguistics.\nToan Q. Nguyen and Julian Salazar. 2019. Trans-\nformers without tears: Improving the normal-\nization of self-attention. In Proceedings of the\n16th International Workshop on Spoken Lan-\nguage Translation, Hong Kong, China. Zenodo.\nMatt Post. 2018.\nA call for clarity in reporting\nBLEU scores. In Proceedings of the Third Con-\nference on Machine Translation: Research Pa-\npers, pages 186–191, Brussels, Belgium. Asso-\nciation for Computational Linguistics.\nPhilip Resnik. 1998. Parallel strands: A prelimi-\nnary investigation into mining the web for bilin-\ngual text. In Machine Translation and the In-\nformation Soup, Third Conference of the Asso-\nciation for Machine Translation in the Ameri-\ncas, AMTA ’98, Langhorne, PA, USA, October\n28-31, 1998, Proceedings, volume 1529 of Lec-\nture Notes in Computer Science, pages 72–82.\nSpringer.\nPhilip Resnik and Noah A. Smith. 2003. The web\nas a parallel corpus. Computational Linguistics,\n29(3):349–380.\nHolger Schwenk. 2018. Filtering and mining par-\nallel data in a joint multilingual space. In Pro-\nceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Vol-\nume 2: Short Papers), pages 228–234, Mel-\nbourne, Australia. Association for Computa-\ntional Linguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm´an. 2019a.\nWikiMatrix: Mining 135M parallel sentences\nin 1620 language pairs from Wikipedia. CoRR,\nabs/1907.05791v2.\nHolger Schwenk, Guillaume Wenzek, Sergey\nEdunov, Edouard Grave, and Armand Joulin.\n2019b.\nCCMatrix: Mining billions of high-\nquality parallel sentences on the WEB. CoRR,\nabs/1911.04944v2.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016. Improving neural machine transla-\ntion models with monolingual data. In Proceed-\nings of the 54th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1:\nLong Papers), pages 86–96, Berlin, Germany.\nAssociation for Computational Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu. 2019. MASS: Masked sequence\nto sequence pre-training for language genera-\ntion. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA,\nvolume 97 of Proceedings of Machine Learning\nResearch, pages 5926–5936. PMLR.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxi-\nang Zhao, and Junyang Lin. 2019. Understand-\ning and improving layer normalization. In Ad-\nvances in Neural Information Processing Sys-\ntems 32:\nAnnual Conference on Neural In-\nformation Processing Systems 2019, NeurIPS\n2019, 8-14 December 2019, Vancouver, BC,\nCanada, pages 4383–4393.\nZhen Yang, Wei Chen, Feng Wang, and Bo Xu.\n2018. Unsupervised neural machine translation\nwith weight sharing. In Proceedings of the 56th\nAnnual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers),\npages 46–55, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nDavid Yarowsky. 1995. Unsupervised word sense\ndisambiguation rivaling supervised methods.\nIn 33rd Annual Meeting of the Association\nfor Computational Linguistics, pages 189–196,\nCambridge, Massachusetts, USA. Association\nfor Computational Linguistics.\nJiawei Zhou and Phillip Keung. 2020.\nImprov-\ning non-autoregressive neural machine transla-\ntion with monolingual data. In ACL.\nPierre Zweigenbaum, Serge Sharoff, and Reinhard\nRapp. 2017.\nOverview of the second BUCC\nshared task: Spotting parallel sentences in com-\nparable corpora.\nIn Proceedings of the 10th\nWorkshop on Building and Using Comparable\nCorpora, pages 60–67, Vancouver, Canada. As-\nsociation for Computational Linguistics.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-10-15",
  "updated": "2020-10-15"
}