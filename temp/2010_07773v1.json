{
  "id": "http://arxiv.org/abs/2010.07773v1",
  "title": "NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of Code-Mixed Dravidian text using XLNet",
  "authors": [
    "Shubhanker Banerjee",
    "Arun Jayapal",
    "Sajeetha Thavareesan"
  ],
  "abstract": "Social media has penetrated into multilingual societies, however most of them\nuse English to be a preferred language for communication. So it looks natural\nfor them to mix their cultural language with English during conversations\nresulting in abundance of multilingual data, call this code-mixed data,\navailable in todays' world.Downstream NLP tasks using such data is challenging\ndue to the semantic nature of it being spread across multiple languages.One\nsuch Natural Language Processing task is sentiment analysis, for this we use an\nauto-regressive XLNet model to perform sentiment analysis on code-mixed\nTamil-English and Malayalam-English datasets.",
  "text": "NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020:\nSentiment Analysis of Code-Mixed Dravidian text\nusing XLNet\nShubhanker Banerjeea, Arun Jayapalb and Sajeetha Thavareesanc\naNational University Of Ireland Galway, Ireland\nbTrinity college Dublin, Ireland\ncEastern University, Sri Lanka\nAbstract\nSocial media has penetrated into multi-lingual societies, however most of them use English to be a\npreferred language for communication. So it looks natural for them to mix their cultural language with\nEnglish during conversations resulting in abundance of multilingual data â€“ call this code-mixed data,\navailable in todayâ€™s world. Downstream NLP tasks using such data is challenging due to the semantic\nnature of it being spread across multiple languages. One such NLP task is Sentiment analysis; for this we\nuse an auto-regressive XLNet model to perform sentiment analysis on code-mixed Tamil-English and\nMalayalam-English datasets.\nKeywords\ncode-mixed, XLNet, auto-regressive, attention\n1. Introduction\nSocial media content results in large data feeds from wide geographies. Since multiple geogra-\nphies are involved, the data is multilingual in nature resulting in code mixing 1 often. Sentiment\nanalysis on code-mixed text allows to gain insights on the trends prevalent in different geogra-\nphies however is a challenge due to the non-trivial nature involved in inferring the semantics\nof such data. In this paper, we address these challenges using XLNet[1]2 framework. We have\nfine tuned the pre-trained XLNet model with the available data without any additional pre-\nprocessing mechanisms. The rest of the paper is organized as follows. Section 2 illustrates the\nrelated work done in the field. Section 3 explains the dataset and the task. Section 4 demonstrates\nthe approach we used and briefly explains the architecture of XLNet. Results are discussed in\nSection 5.\nFIREâ€™20:\n\" S.Banerjee3@nuigalway.ie (S. Banerjee); jayapala@tcd.ie (A. Jayapal); sajeethas@esn.ac.lk (S. Thavareesan)\n\u0012 0000-0002-6252-5393 (S. Thavareesan)\nÂ© 2020 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedings\nhttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\n1Code mixing refers to the linguistic units from different languages being used together by multilingual users\n2XLNet is an auto-regressive model[2] which is built with the transformer architecture[3] with a two stream\nattention mechanism[4]. The two stream attention mechanism ensures that the language model obtained through\ntraining can predict missing words on the basis of bidirectional context. Bidirectional context in XLNet is achieved\nby permutation language modelling.\narXiv:2010.07773v1  [cs.CL]  15 Oct 2020\n2. Related Work\nMultilingual users have the tendency to mix linguistic units in the social media resulting\nin code-mixed data being easily available. The phenomenon of code-mixing is explained in\n[5, 6, 7, 8, 9, 10] and provides an analysis on the possible reasons behind code-mixing. This is\ndone by identifying the languages involved in the code-mixed data which looks inevitable. In\nthe past, several approaches were taken and experiments were conducted aimed at the detection\nof languages in code-mixed data [11][12][13]. A review of many research works on code-mixing\nis discussed in [14].\n2.1. Code-mixed data\nSince code-mixed is mostly sourced from social media platforms, the data in itâ€™s raw form is\nhighly unstructured and hence corpus creation to organize this unstructured data into datasets\nfor further analysis pose a challenge. For some of the Indian languages, [15] has compiled a\nTamil-English code-mixed dataset, the first annotated Tanglish3 dataset. Similarly, [16] published\na dataset for Malayalam-English code-mixed data, where the authors also provided references\nto the availability of other code-mixed datasets such as Chinese-English and Spanish-English.\nBut significant work hasnâ€™t been done in the area of corpus creation for code-mixing for Indian\nlanguages. The Indian languages are considered to be under-resourced and so there is less\ninterest in performing NLP tasks on these languages.\n2.2. Sentiment-analysis\nSentiment analysis is a well known NLP task that infers the positive, negative and neutral\nsentiments from a statement in question. However there are very few works on the sentiment\nanalysis over code-mixed data; [17][18] provides an overview of the work done on sentiment\nanalysis of Dravidian code-mixed text. Another work, [19] compares the performance of\ndifferent transformer architectures on the task of sentiment analysis of code-mixed data. [20]\nemployed an approach based on lexicon to assign sentiment to Hindi-English code-mixed text.\n[21] illustrates a method to detect hate speech in code-mixed Hinglish dataset. For the purpose\nof conducting this research they used FIRE 2013 and FIRE 2014 datasets. [22] used a LSTM\n[23] based approach to improve the state-of-art performance [20] on the hinglish datasets by\n18 percent. [24] used shared parameters in a siamese network [25] to project the code-mixed\nsentences and sentences in standard languages into a common sentiment space. The similarity\nof projected sentences is an indicative of how similar their sentiments are, similar sentences\nhave similar sentiment. Ensemble based techniques have also been used for sentiment analysis\nof code-mixed data, [26] proposed an ensemble model of a character-trigrams based LSTM and\na word-ngrams naive bayes to detect the sentiment in Hindi-english code-mixed data. [27]\nhave used a multilayer perceptron to perform sentiment analysis on code-mixed data extracted\nfrom social media platforms. [28] used an ensemble of a convolutional neural network and a\nself-attention based LSTM for sentiment analysis of Spanglish and Hinglish text.\n3Tanglish refers to code switching between Tamil and English, a term predominantly used in the Tamil\ncommunity\nTable 1\nDataset size and splits\nDataset\nTraining\nValidation\nTesting\nTamil-English\n1,335\n1,260\n3,149\nMalayalam-English\n4,716\n674\n1,348\n3. Dataset for sentiment analysis\nIn spoken and written conversations, it is observed that the usage of lexicon, connectives and\nphrases from English are used in combination with other languages; this can very well be seen\nin the social media text and in spoken conversations across geographies, especially in India.\nSentiment Analysis in social media has drawn attention in recent years. However, sentiment\nanalysis on Tamil-English (Tanglish) and Malayalam-English code-mixed data are not readily\navailable for research. The authors of [15] and [16] have collected 184,573 sentences for Tamil\nand 116,711 sentences for Malayalam from YouTube comments which are based on the trailers of\nthe movies released in 2019 for building Tamil-English and Malayalam-English datasets where\nnon-code-mixed sentences were removed from the collection. Further, emoticons were removed\nand sentence length filters were applied to render the mentioned datasets. In the end two data\nsets of size 15,744 and 6,738 sentences were reported for Tanglish and Malayalam-English texts.\nTo get this dataset ready for sentiment analysis [15] and [16] refers to manual annotation\nactivity carried out with three annotators annotating each sentence in the data set. The Krip-\npendorffâ€™s alpha (Î±) is used to measure inter-annotator agreement which is 0.6585 and 0.890 for\nTamil and Malayalam code-mixed data sets respectively.\nThe dataset was provided for this task in three parts training, validation and testing. The\nnumber of sentences used for the dataset splits are provided in table 1. Both these datasets are\nreleased in DravidianCodeMix FIRE 2020 competition organized by dravidiancodemixed. These\ncomments were grouped into five categories positive, negative, neutral, mixed emotions, or not\nin the intended languages.\n4. Methodology\nLanguage models have been integral to the recent advances made in the field of NLP due to its\nability to predict the next token in a sequence. Traditionally this achieved by computing the\njoint distribution of the tokens in a sequence as a function of conditional probability distribution\nof each token given other tokens in the sequence.\nHowever, XLNet[1] takes a different approach; when these models are trained on large\ndatasets, it achieves state-of-art performances on downstream NLP tasks. This uses permutation\nlanguage modelling, which trains an autoregressive model on all possible permutation of\nwords in a sentence â€“ see equation 1. During prediction of a word in a sequence, it takes into\naccount bidirectional context and predicts the masked tokens on the basis of the words/tokens\nto the right as well as the left of the masked token in the sequence. XLNet is based on the\ntransformer architecture[3], which uses the concept of attention[4] to learn the long range\ntoken dependencies. Another important aspect of XLnet is two-stream attention; this refers\nTable 2\nPrecision, Recall and F-score measures on the Test set\nData\nClasses\nPrecision\nRecall\nF1 Score\nWeighted Average-F1\nAccuracy\nMalayalam-English\nMixed feelings\n0.03\n0.22\n0.05\n0.52\n0.49\nNegative\n0.12\n0.40\n0.19\nPositive\n0.72\n0.50\n0.59\nnot-malayalam\n0.36\n0.58\n0.44\nunknown state\n0.42\n0.46\n0.44\nTamil-English\nMixed feelings\n0.23\n0.13\n0.17\n0.32\n0.35\nNegative\n0.50\n0.16\n0.24\nPositive\n0.39\n0.73\n0.51\nnot-Tamil\n0.10\n0.40\n0.16\nunknown state\n0.03\n0.31\n0.05\nto attention streams working in parallel, one which encodes the content of the tokens and the\nother which incorporates the positional information. This property would be useful and so is\nexploited to perform the sentiment analysis on code-mixed data.\nThe following equation formally describes the language modelling objective using XLNet. In\neqn.1, for a give text sequence x, and set of all permutations of the sequence ğ‘ğ‘‡and z ğœ–ğ‘ğ‘‡.\nğ‘š\nğœƒğ‘ğ‘¥ğ¸ğ‘§âˆ¼ğ‘ğ‘‡[\nğ‘‡\nâˆ‘\nğ‘¡=1\nlog ğ‘ğœƒğ‘¥ğ‘§ğ‘¡|ğ‘¥ğ‘§<ğ‘¡) ]\n(1)\nFor the purpose of experiments, we fine-tuned the XLNet model using the given datasets â€“\nrefer to table 1 and sentiment analysis was conducted on this labelled dataset. The training\nand testing were carried out as per the numbers mentioned in Table 1. Two experiments were\nconducted on the mentioned datasets and for those experiments the XLNet embeddings were\nfine-tuned for 4 epochs each with a maximum learning rate of 0.005 to perform sentiment\nanalysis of the given datasets. Results of the experiments carried out are illustrated in the next\nsection.\n5. Results and Discussion\nThe experiment results for the experiments outlined in the previous section are provided in table\n2. We were able to achieve 0.49 & 0.35 accuracies and 0.52 & 0.32 F-scores on both the datasets\nrespectively. The results are biased towards Positive class because of the class-imbalance seen in\nthe training set. Further, it can be seen that the model performs better on the Malayalam-English\ndataset despite the Tanglish dataset having more samples; this can be attributed to more noise\nin the Tamil-English data and hence relatively poor performance. Our results do not perform\nbetter than the baseline-results described in [15] and [16]. We hypothesize that these results\ncan further be improved by training the model for more epochs with a pre-processing step\nperformed in combination with oversampling and undersampling of the minority and majority\nclasses respectively.\nReferences\n[1] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, Q. V. Le, Xlnet: Generalized\nautoregressive pretraining for language understanding, 2019. arXiv:1906.08237.\n[2] K. Gregor, I. Danihelka, A. Mnih, C. Blundell, D. Wierstra, Deep autoregressive networks,\nin: International Conference on Machine Learning, PMLR, 2014, pp. 1242â€“1250.\n[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polo-\nsukhin, Attention is all you need, 2017. arXiv:1706.03762.\n[4] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning to align\nand translate, 2014. arXiv:1409.0473.\n[5] E. Kim, Reasons and motivations for code-mixing and code-switching, Issues in EFL 4\n(2006) 43â€“61.\n[6] B. R. Chakravarthi, M. Arcan, J. P. McCrae, WordNet gloss translation for under-resourced\nlanguages using multilingual neural machine translation, in: Proceedings of the Second\nWorkshop on Multilingualism at the Intersection of Knowledge Bases and Machine Trans-\nlation, European Association for Machine Translation, Dublin, Ireland, 2019, pp. 1â€“7. URL:\nhttps://www.aclweb.org/anthology/W19-7101.\n[7] B. R. Chakravarthi, R. Priyadharshini, B. Stearns, A. Jayapal, S. S, M. Arcan, M. Zarrouk, J. P.\nMcCrae, Multilingual multimodal machine translation for Dravidian languages utilizing\nphonetic transcription, in: Proceedings of the 2nd Workshop on Technologies for MT of\nLow Resource Languages, European Association for Machine Translation, Dublin, Ireland,\n2019, pp. 56â€“63. URL: https://www.aclweb.org/anthology/W19-6809.\n[8] B. R. Chakravarthi, M. Arcan, J. P. McCrae, Comparison of different orthographies for\nmachine translation of under-resourced Dravidian languages, in: 2nd Conference on\nLanguage, Data and Knowledge (LDK 2019), Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-\nmatik, 2019.\n[9] B. R. Chakravarthi, P. Rani, M. Arcan, J. P. McCrae, A survey of orthographic information\nin machine translation, arXiv preprint arXiv:2008.01391 (2020).\n[10] B. R. Chakravarthi, Leveraging orthographic information to improve machine translation\nof under-resourced languages, Ph.D. thesis, NUI Galway, 2020.\n[11] S. Mandal, S. Banerjee, S. Naskar, P. Rosso, S. Bandyopadhyay, Adaptive voting in multiple\nclassifier systems for word level language identification, 2015. doi:10.13140/RG.2.1.\n3976.0246.\n[12] K. Bali, J. Sharma, M. Choudhury, Y. Vyas, \"i am borrowing ya mixing ?\" an analysis of\nenglish-hindi code mixing in facebook, 2014, pp. 116â€“126. doi:10.3115/v1/W14-3914.\n[13] T. Solorio, M. Sherman, Y. Liu, L. M. Bedore, E. D. PeÃ±a, A. Iglesias, Analyzing language\nsamples of spanish-english bilingual children for the automated prediction of language dom-\ninance, Nat. Lang. Eng. 17 (2011) 367â€“395. URL: https://doi.org/10.1017/S1351324910000252.\ndoi:10.1017/S1351324910000252.\n[14] N. Jose, B. R. Chakravarthi, S. Suryawanshi, E. Sherly, J. P. McCrae, A survey of current\ndatasets for code-switching research, in: 2020 6th International Conference on Advanced\nComputing and Communication Systems (ICACCS), IEEE, 2020, pp. 136â€“141.\n[15] B. R. Chakravarthi, V. Muralidaran, R. Priyadharshini, J. P. McCrae, Corpus creation for\nsentiment analysis in code-mixed Tamil-English text, in: Proceedings of the 1st Joint\nWorkshop on Spoken Language Technologies for Under-resourced languages (SLTU)\nand Collaboration and Computing for Under-Resourced Languages (CCURL), European\nLanguage Resources association, Marseille, France, 2020, pp. 202â€“210. URL: https://www.\naclweb.org/anthology/2020.sltu-1.28.\n[16] B. R. Chakravarthi, N. Jose, S. Suryawanshi, E. Sherly, J. P. McCrae, A sentiment analysis\ndataset for code-mixed Malayalam-English, in: Proceedings of the 1st Joint Workshop on\nSpoken Language Technologies for Under-resourced languages (SLTU) and Collaboration\nand Computing for Under-Resourced Languages (CCURL), European Language Resources\nassociation, Marseille, France, 2020, pp. 177â€“184. URL: https://www.aclweb.org/anthology/\n2020.sltu-1.25.\n[17] B. R. Chakravarthi, R. Priyadharshini, V. Muralidaran, S. Suryawanshi, N. Jose, E. Sherly,\nJ. P. McCrae, Overview of the track on Sentiment Analysis for Dravidian Languages in\nCode-Mixed Text, in: Working Notes of the Forum for Information Retrieval Evaluation\n(FIRE 2020). CEUR Workshop Proceedings. In: CEUR-WS. org, Hyderabad, India, 2020.\n[18] B. R. Chakravarthi, R. Priyadharshini, V. Muralidaran, S. Suryawanshi, N. Jose, E. Sherly,\nJ. P. McCrae, Overview of the track on Sentiment Analysis for Dravidian Languages in\nCode-Mixed Text, in: Proceedings of the 12th Forum for Information Retrieval Evaluation,\nFIRE â€™20, 2020.\n[19] S. Banerjee, B. R. Chakravarthi, J. P. McCrae, Comparison of pretrained embeddings to\nidentify hate speech in Indian code-mixed text, in: 2nd IEEE International Conference on\nAdvances in Computing, Communication Control and Networking â€“ICACCCN (ICAC3N-\n20), 2020.\n[20] S. Sharma, P. Srinivas, R. C. Balabantaray, Text normalization of code mix and sentiment\nanalysis, in: 2015 International Conference on Advances in Computing, Communications\nand Informatics (ICACCI), 2015, pp. 1468â€“1473.\n[21] P. Rani, S. Suryawanshi, K. Goswami, B. R. Chakravarthi, T. Fransen, J. P. McCrae, A\ncomparative study of different state-of-the-art hate speech detection methods in Hindi-\nEnglish code-mixed data, in: Proceedings of the Second Workshop on Trolling, Aggression\nand Cyberbullying, European Language Resources Association (ELRA), Marseille, France,\n2020, pp. 42â€“48. URL: https://www.aclweb.org/anthology/2020.trac-1.7.\n[22] A. Prabhu, A. Joshi, M. Shrivastava, V. Varma, Towards sub-word level compositions for\nsentiment analysis of hindi-english code mixed text (2016).\n[23] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation 9 (1997)\n1735â€“1780.\n[24] N. Choudhary, R. Singh, I. Bindlish, M. Shrivastava, Sentiment analysis of code-mixed\nlanguages leveraging resource rich languages, 2018. arXiv:1804.00806.\n[25] S. K. Roy, M. Harandi, R. Nock, R. Hartley, Siamese networks: The tale of two manifolds,\nin: Proceedings of the IEEE International Conference on Computer Vision, 2019, pp.\n3046â€“3055.\n[26] M. G. Jhanwar, A. Das, An ensemble model for sentiment analysis of hindi-english code-\nmixed data, arXiv preprint arXiv:1806.04450 (2018).\n[27] S. Ghosh, S. Ghosh, D. Das, Sentiment identification in code-mixed social media text, arXiv\npreprint arXiv:1707.01184 (2017).\n[28] A. Kumar, H. Agarwal, K. Bansal, A. Modi, Baksa at semeval-2020 task 9: Bolstering cnn\nwith self-attention for sentiment analysis of code mixed text, 2020. arXiv:2007.10819.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2020-10-15",
  "updated": "2020-10-15"
}