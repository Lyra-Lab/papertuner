{
  "id": "http://arxiv.org/abs/2010.07893v2",
  "title": "MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents",
  "authors": [
    "Stephen Chung"
  ],
  "abstract": "Nearly all state-of-the-art deep learning algorithms rely on error\nbackpropagation, which is generally regarded as biologically implausible. An\nalternative way of training an artificial neural network is through treating\neach unit in the network as a reinforcement learning agent, and thus the\nnetwork is considered as a team of agents. As such, all units can be trained by\nREINFORCE, a local learning rule modulated by a global signal that is more\nconsistent with biologically observed forms of synaptic plasticity. Although\nthis learning rule follows the gradient of return in expectation, it suffers\nfrom high variance and thus the low speed of learning, rendering it impractical\nto train deep networks. We therefore propose a novel algorithm called MAP\npropagation to reduce this variance significantly while retaining the local\nproperty of the learning rule. Experiments demonstrated that MAP propagation\ncould solve common reinforcement learning tasks at a similar speed to\nbackpropagation when applied to an actor-critic network. Our work thus allows\nfor the broader application of the teams of agents in deep reinforcement\nlearning.",
  "text": "MAP Propagation Algorithm: Faster Learning with a\nTeam of Reinforcement Learning Agents\nStephen Chung\nDepartment of Computer Science\nUniversity of Massachusetts Amherst\nAmherst, MA 01003\nminghaychung@umass.edu\nAbstract\nNearly all state-of-the-art deep learning algorithms rely on error backpropagation,\nwhich is generally regarded as biologically implausible. An alternative way of\ntraining an artiﬁcial neural network is through treating each unit in the network\nas a reinforcement learning agent, and thus the network is considered as a team\nof agents. As such, all units can be trained by REINFORCE, a local learning rule\nmodulated by a global signal that is more consistent with biologically observed\nforms of synaptic plasticity. Although this learning rule follows the gradient of\nreturn in expectation, it suffers from high variance and thus the low speed of\nlearning, rendering it impractical to train deep networks. We therefore propose a\nnovel algorithm called MAP propagation to reduce this variance signiﬁcantly while\nretaining the local property of the learning rule. Experiments demonstrated that\nMAP propagation could solve common reinforcement learning tasks at a similar\nspeed to backpropagation when applied to an actor-critic network. Our work thus\nallows for the broader application of the teams of agents in deep reinforcement\nlearning.\n1\nIntroduction\nError backpropagation algorithm (backprop) [1] efﬁciently computes the gradient of an objective\nfunction with respect to parameters, by iterating backward from the last layer of a multi-layer artiﬁcial\nneural network (ANN). However, backprop is generally regarded as being biologically implausible [2–\n7]. First, the learning rule given by backprop is non-local, as it relies on information other than input\nand output of a neuron-like unit computed in feedforward phase; while biologically-observed synaptic\nplasticity depends mostly on local information (e.g. spike-timing-dependent plasticity (STDP) [8])\nand possibly some global signals (e.g. reward-modulated spike-timing-dependent plasticity (R-STDP)\n[8–10]). Second, backprop requires a precise coordination between feedforward and feedback\nconnections, because the feedforward value has to be retained until the error signals arrive; while it is\nunclear how a biological system can coordinate an entire network to alternate between feedforward\nand feedback phases precisely. Third, backprop requires synaptic symmetry in the forward and\nbackward paths, rendering it biologically implausible. Nonetheless, recent work has demonstrated\nthat this symmetry may not be necessary for backprop due to the ‘feedback alignment’ phenomenon\n[11–13].\nAlternatively, REINFORCE [14] could be applied to all units in the network to train an ANN as a\nmore biologically plausible way of learning. It is shown that the learning rule gives an unbiased\nestimate of the gradient of return [14]. Another interpretation of this relates to viewing each unit as a\nreinforcement learning (RL) agent, with each agent trying to maximize the global reward. Such a\nteam of agents is also known as coagent network [15]. However, coagent networks can only solve\nPreprint. Under review.\narXiv:2010.07893v2  [cs.LG]  5 Oct 2021\nsimple tasks due to the high variance associated with the learning rule and thus the low speed of\nlearning. The high variance stems from the lack of structural credit assignment, i.e. a single scalar\nreward is used to evaluate the action of all agents in the network.\nTo address this high variance associated with REINFORCE, we propose a novel algorithm that\nsigniﬁcantly reduces the variance while retaining the local property of the learning rule. We call this\nnewly proposed algorithm maximum a posteriori (MAP) propagation. Essentially, MAP propagation\nreplaces the hidden units’ values with their MAP estimates conditioned on the action chosen, or\nequivalently, minimizes the energy function of the network, before applying REINFORCE. We prove\nthat for a network with normally distributed hidden units, by minimizing the energy function of the\nnetwork, the parameter update given by REINFORCE and backprop (with the reparametrization\ntrick) becomes the same, thus establishing a connection between REINFORCE and backprop. Our\nexperiments show that a team of agents trained with MAP propagation can learn much faster than\nREINFORCE, such that the team of agents can solve common RL tasks at a similar (or higher) speed\ncompared to an ANN trained by backprop, as well as exhibiting sophisticated exploration that differs\nfrom an ANN trained by backprop.\nThe novel MAP propagation algorithm represents a new class of algorithm to train an ANN that\nis more biologically plausible than backprop; at the same time, maintaining a comparable learning\nspeed to backprop. Our work also opens the prospect of the broader application of teams of agents,\ncalled coagent networks by [15], in deep RL.\n2\nBackground and Notation\nWe consider a Markov Decision Process (MDP) deﬁned by a tuple (S, A, P, R, γ, d0), where S is\na ﬁnite set of states of an agent’s environment (although this work can be extended to the inﬁnite\nstate case), A is a ﬁnite set of actions, P : S × A × S →[0, 1] is a transition function giving\nthe dynamics of the environment, R : S × A →R is a reward function, γ ∈[0, 1] is a discount\nfactor, and d0 : S →[0, 1] is an initial state distribution. Denoting the state, action, and reward\nsignal at time t by St, At, and Rt respectively, P(s, a, s′) = Pr(St+1 = s′|St = s, At = a),\nR(s, a) = E[Rt|St = s, At = a], and d0(s) = Pr(S0 = s), where P and d0 are valid probability\nmass functions. An episode is a sequence of states, actions, and rewards, starting from t = 0\nand continuing until reaching the terminal state. For any learning methods, we can measure its\nperformance as it improves with experience over multiple episodes, which makes up a run.\nLetting Gt = P∞\nk=t γk−tRk denote the inﬁnite-horizon discounted return accrued after acting at\ntime t, we are interested in ﬁnding, or approximating, a policy π : S × A →[0, 1] such that for any\ntime t > 0, selecting actions according to π(s, a) = Pr(At = a|St = s) maximizes the expected\nreturn E[Gt|π]. The value function for policy π is V π where for all s ∈S, V π(s) = E[Gt|St = s, π],\nwhich can be shown to be independent of t for the inﬁnite-horizon case we are considering.\nHere we restrict attention to policies computed by multi-layer networks consisting of L layers of\nstochastic units. Let Hl\nt ∈Rn(l) denotes the activation values of the units in layer l at time t and\nn(l) denotes the number of units in layer l. For any t > 0, we also let H0\nt = St, HL\nt = At, and\nHt = {H1\nt , H2\nt , ..., HL−1\nt\n}. We call any elements in Ht to be a hidden layer and HL\nt to be the output\nlayer. The distribution of Hl\nt conditional on Hl−1\nt\nis given by πl : Rn(l−1) × Rn(l) →[0, 1], such\nthat for any t > 0, πl(hl−1, hl; W l) = Pr(Hl\nt = hl|Hl−1\nt\n= hl−1; W l), where W l is the parameter\nof layer l. We also denote all parameters of the network as W = {W 1, W 2, ..., W L}. To sample an\naction At from the network, we iteratively sample Hl\nt ∼πl(Hl−1\nt\n, ·; W l) from l = 1 to L.\nWe call layer l to be normally distributed if πl(Hl−1\nt\n, ·; W l) = N(gl(Hl−1\nt\n; W l), σ2\nl ), the normal\ndistribution with mean gl(Hl−1\nt\n; W l), where gl : Rn(l−1) →Rn(l) is a differentiable function, and a\nﬁxed standard deviation σl. A common choice of g is a linear transformation followed by an activation\nfunction; that is, g(Hl−1\nt\n; W l) = f(W lHl−1\nt\n) where f is a non-linear activation function such as\nsoftplus or rectiﬁed linear unit (ReLU) and W l ∈Rn(l)×n(l−1). We also deﬁne the energy function\nE : Rn(1) × Rn(2) × ... × Rn(L−1) →[0, ∞) to be E(h; s, a) = −log Pr(Ht = h|St = s, At = a),\nwhich can be shown to be independent of t.\n2\nThe case we consider here is one in which all the units of the network implement an RL algorithm\nand share the same reward signal. These networks can therefore be considered to be teams of agents\n(agent here refers to an RL agent [16]), which have also been called coagent networks [15].\nWe denote ∇xf as the gradient of f w.r.t. x, AT as the transpose of matrix A, and ∇Af(Pr(A))\nas the shorthand for ∇af(Pr(A = a)). For a random variable X with a distribution that depends\non parameter W and a random variable Y , we call h(Z; W, Y ) to be the re-parameterization of\nX by Z conditioned on Y if h(Z; W, Y ) and X have the same conditional distribution; that is,\nPr(h(Z; W, Y ) = x|Y = y) = Pr(X = x|Y = y; W) for all values of x, y and W, where Z is an\nindependent random variable with a distribution that does not depend on parameter W and h is an\ninvertible and differentiable function. In case X has a multi-layer structure, we denote hl(Z; W, Y )\nto be the lth layer in h(Z; W, Y ) and h−1 as the inverse of h. In general, we use the superscript l to\ndenote the lth layer in a variable if the variable has a multi-layer structure. Also, for all distributions\ndiscussed in this paper, the probability mass function is replaced by probability density function if\nthe random variable is continuous.\n3\nAlgorithm\n3.1\nMAP Propagation\nMAP propagation is based on REINFORCE applied to each hidden unit with the same global\nreinforcement signal. To reduce the variance associated with the learning rule, we note that this\nvariance can be reduced by using the expected parameter update conditioned on the state and the\nselected action instead. However, this expected parameter update is generally intractable to compute\nanalytically. Therefore, we propose to use the MAP estimate to approximate the expected parameter\nupdate. This makes the resulting learning rule biased but reduces the variance signiﬁcantly. The\ndetails of MAP propagation are as below.\nThe gradient of return with respect to W l (where l ∈{1, 2, ..., L} in all discussion below unless\nstated otherwise) can be estimated by REINFORCE, also known as likelihood ratio estimator:\n∇W l E[Gt] =\n∞\nX\nk=t\nγ(k−t) E[Gk∇W l log Pr(Ak|Sk)].\n(1)\nWe can show that the terms in the summation of (1) also equals E[Gk∇W l log πl(Hl−1\nk\n, Hl\nk; W l)],\nwhich is the REINFORCE learning rule applied to each hidden unit with the same global reinforce-\nment signal Gk:\nTheorem 1. Let the policy be a multi-layer network of stochastic units as deﬁned in Section 2. For\nany t > 0 and l ∈{1, 2, ..., L},\nE[Gt∇W l log Pr(At|St; W)] = E[Gt∇W l log πl(Hl−1\nt\n, Hl\nt; W l)].\n(2)\nThe proof is in Appendix B.1. Note that this theorem is also proved in [14]. This shows that we can\napply REINFORCE to each unit of the network, and the learning rule still gives an unbiased estimate\nof the gradient of the return. Therefore, denoting α as the step size, we can update parameters by the\nfollowing stochastic gradient ascent rule:\nW l ←W l + αGt∇W l log πl(Hl−1\nt\n, Hl\nt; W l).\n(3)\nHowever, this learning rule suffers from high variance since a single reward, which results from the\nstochastic noise of all units, is used to evaluate actions of all units, making the learning rule scales\npoorly with the number of units in the network. To reduce the variance, we notice that we can replace\n∇W l log πl(Hl−1\nt\n, Hl\nt; W l) in learning rule (3) by E[∇W l log πl(Hl−1\nt\n, Hl\nt; W l)|St, At], noting that\n(see Appendix B.1 for the details; at the R.H.S., the outer expectation is taken over St, At and Gt,\nwhile the inner expectaion is taken over Hl−1\nt\nand Hl\nt):\nE[Gt∇W l log πl(Hl−1\nt\n, Hl\nt; W l)] = E[Gt E[∇W l log πl(Hl−1\nt\n, Hl\nt; W l)|St, At]].\n(4)\nThis can reduce variance since the variance associated with the stochastic noise of hidden units is\nremoved in the learning rule (see Appendix B.6 for the proof). The learning rule now becomes:\nW l ←W l + αGt E[∇W l log πl(Hl−1\nt\n, Hl\nt; W l)|St, At].\n(5)\n3\nSince (3) is following gradient of return in expectation, and the expected update value of (3) and\n(5) is the same, we conclude that (5) is also a valid learning rule as it follows the gradient of\nreturn in expectation. However, we note that E[∇W l log πl(Hl−1\nt\n, Hl\nt; W l)|St, At] in (5) is generally\nintractable to compute analytically. Instead, we propose to use maximum a posteriori (MAP) estimate\nto approximate this term:1\nE[∇W l log πl(Hl−1\nt\n, Hl\nt; W l)|St, At] ≈∇W l log πl( ˆHl−1\nt\n, ˆHl\nt; W l),\n(6)\nwhere ˆHt = argmaxht Pr(Ht = ht|St, At). There are many methods to approximate ˆHt, such as\nhill-climbing methods. In case of hidden units being continuous, we can approximate ˆHt by running\ngradient ascent on log Pr(Ht|St, At) as a function of Ht for ﬁxed St and At, such that Ht approaches\nˆ\nHt. Ht can be initialized as the value sampled from the network when sampling action At. To be\nspeciﬁc, before applying learning rule (3), we ﬁrst run gradient ascent on Ht for N steps:\nHt ←Ht + α∇Ht log Pr(Ht|St, At).\n(7)\nFor l ∈{1, 2, ..., L −1}, this is equivalent to (see Appendix B.4 for the details):\nHl\nt ←Hl\nt + α(∇Hl\nt log πl+1(Hl\nt, Hl+1\nt\n; W l+1) + ∇Hl\nt log πl(Hl−1\nt\n, Hl\nt; W l)).\n(8)\nThe update rule is maximizing the probability of the value of a hidden unit given the value of units\none layer below and above by updating the value of that hidden unit. Using the deﬁnition of energy\nfunction in Section 2, then the update rule of hidden units can be seen as minimizing the energy\nfunction E(Ht; St, At) [17].\nAfter updating Ht for N steps by (8), we obtain an estimate of ˆHt, denoted as ˜Ht, and apply the\nfollowing learning rule to learn the parameters of network:\nW l ←W l + αGt∇W l log πl( ˜Hl−1\nt\n, ˜Hl\nt; W l).\n(9)\nWe call the algorithm that uses an estimate of ˆHt in the REINFORCE learning rule as MAP propaga-\ntion. The pseudo-code of MAP propagation with gradient ascent to approximate ˆHt can be found in\nAlgorithm 1 in Appendix A. Note that N = 0 recovers the special case of pure REINFORCE.\nSimilar to actor-critic networks [16], we can also train a critic network to estimate the state-value,\nV π(St), so Gt in (9) can be replaced by TD error δt = Rt + γV π(St+1) −V π(St) and the whole\nalgorithm can be implemented online. To better facilitate temporal credit assignment, we can also\nuse eligibility traces to replace the gradient in (9), using the same idea of actor-critic networks with\neligibility trace [16]. The pseudo-code of it can be found in Algorithm 2 in Appendix A.\nA team of agents can also be trained by MAP propagation to estimate the state-value, such that a\nseparate team of agents can fulﬁll the role of a critic network, and the whole actor-critic network can\nbe trained without backprop. A simple way to achieve this is to convert the estimation of state-value\nto an RL task but this conversion is inefﬁcient since the information of optimal actions is lost (the\nagent only knows a scalar reward but not the target output). Appendix C proposes a new learning\nrule to train a team of agents to estimate the state-value by MAP propagation efﬁciently based on the\ninformation of optimal actions.\nEssentially, MAP propagation is equivalent to applying REINFORCE after minimizing the energy\nfunction. As there are many studies on the biological plausibility of REINFORCE, we refer readers\nto chapter 15 of [16] for a review and discussion of the connection between REINFORCE and\nneuroscience. The main difference between MAP propagation and REINFORCE is the minimization\nof the energy function given by the update rule (8). This update rule is local as it only depends on\nthe units one layer above and below based on feedforward and feedback connections. There is much\nevidence that feedback signals in brains alter neural activity [7, 18], supporting the use of feedback\nconnections in MAP propagation. The update rule can also be performed in parallel for all layers,\nremoving the need for precise coordination between feedforward and feedback connections as in\nbackprop. However, the update rule requires the feedback weight to be symmetric of the feedforward\nweight and different values to be propagated through feedforward and feedback connections.\nMAP propagation ﬁts well into the recently proposed NGRAD hypothesis [7], which hypothesizes that\nthe cortex use differences in activity states to drive learning. The main idea of NGRAD is that “higher-\nlevel activities can nudge lower-level activities towards values that are more consistent with the\n1We let ˆH0\nt = H0\nt = St, ˆHl\nt denotes the lth layer in ˆHt for l ∈{1, 2, ..., L −1}, and ˆHL\nt = HL\nt = At.\n4\nhigher-level activity”, which also describes the process of energy minimization in MAP propagation.\nA detailed discussion of the biological plausibility of MAP propagation and its relationship with the\nNGRAD hypothesis can be found in Appendix F.\n3.2\nRelationship with Backpropagation\nA network of stochastic units cannot be directly trained by backprop. However, assuming that there\nexist a re-parameterization of Ht by Zt conditioned on St, denoted by h(Zt; W, St), then we can\nupdate parameters using backprop with the re-parameterization trick [19]; that is, for l ∈{1, 2, ..., L}:\nW l ←W l + αGt∇W l log πL(hL−1(Zt; W, St), At; W L).\n(10)\nIt can be shown that this learning rule follows the gradient of return in expectation (See Appendix\nB.5 for the proof). Using a similar argument as in MAP propagation, we can reduce the variance\nassociated with the learning rule by minimizing the energy function before applying the learning rule.\nInterestingly, for a network with all hidden layers being normally distributed, when the values of\nhidden layers are settled to a stationary point of the energy function, the parameter update given by\nbackprop (with the reparametrization trick) in (10) is equivalent to the parameter update given by\nREINFORCE in (3):2\nTheorem 2. Let the policy be a multi-layer network of stochastic units with all hidden layers normally\ndistributed as deﬁned in Section 2. There exists a re-parameterization of Ht by Zt conditioned on\nSt that is independent of t, denoted by h(Zt; W, St), such that for any l ∈{1, 2, ..., L}, s ∈S,\nˆh ∈Rn(1)×Rn(2)×...×Rn(L−1), ˆz ∈Rn(1)×Rn(2)×...×Rn(L−1) and a ∈A, if ∇hE(ˆh; s, a) = 0\nand ˆz = h−1(ˆh; W, s), then\n∇W l log πl(ˆhl−1, ˆhl; W l) = ∇W l log πL(hL−1(ˆz; W, s), a; W L).\n(11)\nThe proof is in Appendix B.2. In other words, by nudging the value of units in lower layers towards\nvalues that are more consistent with the value of units in the ﬁnal layer, the parameter update given\nby REINFORCE becomes the same as backprop (with the reparametrization trick). With N →∞\nand α sufﬁciently small, under update rule (7), Ht will converge to the stationary point of energy\nfunction. Therefore, the parameter update given by MAP propagation converges to the parameter\nupdate given by backprop (with the reparametrization trick) after minimizing the energy function.\nDespite the close relationship between MAP propagation and backprop, there are key differences\nbetween the two algorithms. Compared to backprop, one major limitation of MAP propagation is that\nit can only be applied to RL tasks. MAP propagation is also more computationally expensive than\nbackprop due to the minimizing of the energy function in every step. However, MAP propagation\ncan be applied to a network of discrete units. Moreover, MAP propagation does not require non-local\nfeedback signals or precise coordination between feedforward and feedback pathways, which makes\nit more biologically plausible than backprop.\nTo see that MAP propagation is more computationally expensive than backprop, denote L as the\nnumber of layers in the network and N as the number of steps in the energy minimization (the inner\nloop). MAP propagation requires LN layer updates during each step of energy minimization, while\nbackprop requires L layer updates to compute the feedback signals (iterating from the top layers).\nTherefore, MAP propagation takes N times more layer updates than backprop. However, the LN\nlayer updates in MAP propagation can be done in parallel for all layers, so the time complexity\nfor a single step of MAP propagation can be reduced to O(N) from O(LN) if the update is done\nin parallel. For backprop, parallel computation of feedback signals is not possible, so the time\ncomplexity for a single step of backprop remains O(L).\n4\nRelated Work\nLocal learning rules based on MAP estimates of latent variables have been proposed in both unsu-\npervised and supervised learning tasks. For unsupervised learning tasks, [5] proposed training a\ndeep generative model by using MAP estimate to infer the value of latent variables, conditioned on\n2Similar to footnote 1, we let ˆh0 = s, ˆhl denotes the lth layer in ˆh for l ∈{1, 2, ..., L −1}, and ˆhL = a.\n5\nobserved variables; in the same work, they also proposed to learn the feedback weights such that\nthe constraint of symmetric weight can be removed. This idea can also possibly be applied to our\nalgorithm. For supervised learning tasks, [20] proposed training a deep network with local learning\nrules based on MAP inference and clipping the output value of the network to the target value. In\ncontrast to these works, MAP propagation applies to RL tasks and does not require clipping any units’\nvalues.\nBesides algorithms based on MAP estimates, many biologically plausible alternatives to backprop\nhave been proposed. [21–23] introduced biologically plausible learning rules based on reward\nprediction errors and attentional feedback; but these learning rules mostly require a non-local\nfeedback signal. Moreover, [24–26] introduced local learning rules based on contrastive divergence\nor nudging the values of output units towards the target value. See [7] for a comprehensive review\nof algorithms that approximate backprop with local learning rules based on the differences in units’\nvalues. Contrary to [24–26], MAP propagation requires neither the temporal difference in units’ value\nnor multiple phases of learning.\nAnother perspective of training a multi-level network of stochastic units relates to viewing each\nunit as an RL agent, forming a hierarchy of agents. In hierarchical RL, [27] proposed learning a\nmulti-level hierarchy with hindsight actions, which is similar to our idea of replacing the values of\nhidden layers with the MAP estimates. The special case of a team of agents forming a network to\nsolve a task in a cooperative way was ﬁrst proposed by [28, 29], and a comprehensive review can be\nfound in chapter 15.10 of [16]. Such a team of agents is recently called coagent networks [15] and\n[15, 30, 31] introduced theories relating to training coagent networks. However, coagent networks\nlearn much slower than an ANN trained by backprop due to the high variance associated with the\nlearning rule. To reduce the variance, [15] proposed to disable exploration of units randomly, but the\nlearning speed is still not comparable to backprop.\nIn addition, there is a large amount of literature on methods for training a network of stochastic units.\nA review can be found in [32], which includes the re-parametrization trick [33] and REINFORCE [14].\nThey introduced methods to reduce the variance of the estimate, such as baseline and critic. These\nideas are orthogonal to the use of MAP estimate to reduce the variance associated with REINFORCE.\n5\nExperiments\nTo test the algorithm, we ﬁrst consider a single-time-step MDP that is similar to the multiplexer task\n[29]. This is to test the performance of the algorithm as an actor. Then we consider a scalar regression\ntask to test the performance of the algorithm as a critic. Finally, we consider some standard RL tasks\nto test the performance of the algorithm as both an actor and a critic.\nIn the below tasks, all the teams of agents (or coagent network) have the same architecture: a\ntwo-hidden-layer network, with the ﬁrst hidden layer having 64 units, the second hidden layer having\n32 units, and the output layer having one unit. All hidden layers are normally distributed with\nπl(Hl−1\nt\n, ·; W l) = N(f(W lHl−1\nt\n), σ2\nl ) for l = 1, 2, and f(x) = log(1 + exp(x)), the softplus\nfunction. For the network in multiplexer task and the actor network with discrete output, the output\nunit’s distribution is given by the softmax function on the previous layer, i.e. πL(HL−1, a; W L) =\nsoftmaxa(TW LHL−1), where T > 0 is a scalar hyperparameter representing the temperature. For\nthe network in the scalar regression task, the critic network and the actor network with continuous\noutput, the output unit’s distribution is normally distributed with mean given by a linear transformation\nof the previous layer’s value and a ﬁxed variance, i.e. πL(HL−1, ·; W L) = N(W LHL−1, σ2\nL). We\nused N = 20 in MAP propagation. Other hyperparameters and details of experiments can be found\nin Appendix D.\n5.1\nMultiplexer Task\nWe consider a single-time-step MDP that is similar to the k-bit multiplexer task. In our single-time-\nstep MDP, the state is sampled from all possible values of a binary vector of size k + 2k with equal\nprobability. The action set is {−1, 1}, and we give a reward of 1 if the action of the agent is the\ndesired action and -1 otherwise. The desired action is given by the output of a multiplexer, with the\ninput of the multiplexer being the state. We consider k = 5 here, so the dimension of the state space\nis 37.\n6\n0\n5000\n10000\n15000\n20000\n25000\n30000\nTraining Step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRunning Average Reward\nMAP Propagation\nREINFORCE\nREINFORCE with [15]\nBac prop\n(a) Multiplexer\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nTraining S ep\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nL2 Error\nMAP Prop (SL)\nMAP Prop (RL)\nBackprop\n(b) Scalar Regression\nFigure 1: Running average rewards over last 10 episodes in multiplexer task and scalar regression\ntask. Results are averaged over 10 independent runs, and shaded area represents standard deviation\nover the runs.\n100\n150\n200\n250\n300\n350\n400\n450\n500\nEpisode\n−300\n−250\n−200\n−150\n−100\nEpisode Re urn\nMAP Prop\nREINFORCE\nREINFORCE wi h [15]\nBackprop\n(a) Acrobot\n200\n400\n600\n800\n1000\nEpisode\n0\n100\n200\n300\n400\n500\nEpisode Return\nMAP Pro agation\nREINFORCE\nREINFORCE with [15]\nBack ro \n(b) CartPole\n0\n500\n1000\n1500\n2000\n2500\nEpisode\n−200\n−100\n0\n100\n200\nEpisode Ret rn\nMAP Propagation\nREINFORCE\nREINFORCE with [15]\nBackprop\n(c) LunarLander\n100\n150\n200\n250\n300\n350\n400\n450\n500\nEpisode\n−100\n−75\n−50\n−25\n0\n25\n50\n75\nEpisode Retu n\nMAP P opagation\nREINFORCE\nREINFORCE with [15]\nBackp op\n(d) MountainCar\nFigure 2: Running average returns over the last 100 episodes in Acrobot, CartPole, LunarLander and\nMountainCar. Results are averaged over 10 independent runs, and shaded area represents standard\ndeviation over the runs.\n7\nTable 1: Average return over all episodes.\nAcrobot\nCartPole\nLunarLander\nMountainCar\nMean\nStd.\nMean\nStd.\nMean\nStd.\nMean\nStd.\nMAP Propagation\n-100.29\n5.40\n459.70\n13.89\n127.88\n24.57\n39.45\n30.48\nREINFORCE\n-148.42\n47.65\n47.29\n8.22\n-62.05\n16.16\n-35.52\n0.65\nREINFORCE with [15]\n-149.11\n33.50\n112.58\n42.54\n-54.61\n23.47\n-4.65\n0.21\nBackprop\n-106.29\n15.00\n458.96\n9.44\n104.92\n31.98\n4.30\n59.28\nWe used Algorithm 1 for training a team of agents by MAP propagation. We consider three baselines:\n1. REINFORCE - A team of agents trained entirely by REINFORCE; 2. REINFORCE with [15] - A\nteam of agents trained entirely by REINFORCE but with the variance reduction method from [15]; 3.\nBackprop - An ANN with a similar architecture where the output unit is trained by REINFORCE and\nhidden units are trained by backprop.\nThe results are shown in Fig 1. We observe that MAP propagation performs much better than the two\nREINFORCE baselines. The result suggests that MAP propagation can improve the learning speed\nof REINFORCE signiﬁcantly, such that its learning speed is comparable to backprop.\n5.2\nScalar Regression Task\nIn the following, we consider a scalar regression task. The dimension of input is 8 and follows the\nstandard normal distribution. The target output (a real scalar) is computed by a one-hidden-layer\nANN with weights chosen randomly. The goal of the task is to predict the target output given the\ninput.\nFor MAP propagation, we used Algorithm 1 and tested two variants. In the ﬁrst variant that is labeled\nas ‘MAP Prop (RL)’, we treated the task as a single-time-step MDP with the negative L2 loss as the\nreward and trained the network using Algorithm 1. In the second variant that is labeled as ‘MAP Prop\n(SL)’, we replaced the learning rule in Algorithm 1 with the learning rule proposed in Appendix C,\nwhich incorporates the value of target output. For the baseline, we trained an ANN with a similar\narchitecture by gradient descent on the L2 loss.\nThe results are shown in Fig 1. We observe that if we directly use the negative L2 loss as the reward,\nthen the learning speed of MAP propagation is signiﬁcantly lower than backprop since the information\nof target output is not incorporated. On the other hand, if we use the learning rule in Appendix\nC to incorporate the information of target output, then the learning speed of MAP propagation is\ncomparable to backprop. However, the asymptotic performance of MAP propagation is slightly worse\nthan backprop, which is due to the stochastic property of teams of agents. Nonetheless, this may not\nbe a problem when applying MAP propagation to train a critic network, since the value function to\nbe estimated is also constantly changing with the policy function.\n5.3\nReinforcement Learning Task\nIn the following, we consider four standard RL tasks: Acrobot, CartPole, LunarLander, and continuous\nMountainCar in OpenAI’s Gym. For MAP propagation, we use the actor-critic network with eligibility\ntraces given by Algorithm 2, and the critic network is trained by the learning rule proposed in\nAppendix C. We consider three baselines. For the ﬁrst and second baseline, the actor network is a\nteam of agents trained entirely by REINFORCE. However, since REINFORCE cannot train a critic\nnetwork directly and it is inefﬁcient to convert the state-value estimation task to an RL task, we\nused an ANN with a similar architecture trained by backprop as the critic network. We also used\nthe variance reduction method from [15] in the second baseline. We used eligibility traces in the\ntraining of both the actor and the critic network. For the third baseline, we used actor-critic with\neligibility traces (episodic) [16] trained by backprop. Both actor and critic networks are ANNs with\nan architecture similar to the team of agents.\nThe average return over ten independent runs is shown in Fig 2. Let ¯G denote the average return of\nall episodes. The mean and standard deviation of ¯G over the ten runs can be found in Table 1. For all\nRL tasks, we observe that MAP propagation has a better performance than the baselines in terms of\n8\nthe average return ¯G. The result demonstrates that a team of agents trained with MAP propagation\ncan learn much faster than a team of agents trained with REINFORCE, such that the team of agents\ncan solve common RL tasks at a similar (or higher) speed compared to an ANN trained by backprop.\nAlthough there are other algorithms besides actor-critic networks that can solve RL tasks more\nefﬁciently, the present work aims to compare different training methods for hidden units in an actor-\ncritic network. Teams of agents trained by MAP propagation can also be applied to algorithms besides\nactor-critic networks, such as variants of actor-critic networks like Proximal Policy Optimization [34]\nand action-value based methods like Q-Learning [16].\nWe also notice that MAP Prop performs better than backprop on tasks where a high degree of\nexploration is required. For example, MAP propagation performs slightly worse than backprop on\nthe multiplexer task but much better than backprop on the MountainCar task. This may suggest that\nteams of agents trained with MAP propagation can have better exploration than an ANN trained\nwith backprop. This is further corroborated by the analysis of agents’ behaviors on MountainCar,\na task where an agent can easily be stuck in the local optima. For backprop, we found that agents\nin most of the runs are stuck in early episodes even with the use of entropy regularization [35]. In\ncontrast, a team of agents trained by MAP propagation can reach the goal of the task successfully in\nall runs. A detailed analysis of this can be found in Appendix E. One possible explanation for the\nbetter exploration is that actions of agents in lower layers can be considered as abstract actions, and\nthe exploration of these agents corresponds to exploration beyond the primitive actions.\n6\nFuture Work and Conclusion\nThe ability to train teams of agents efﬁciently leads to many possible future directions. First, the\nlocal property of the learning rule points to the possibility of implementing MAP propagation\nasynchronously, such that it can be implemented efﬁciently with neuromorphic circuits [36]. Second,\nagents in the team can have a different temporal resolution, such that the actions of agents can be\nextended temporally and become options [37], yielding better exploration and learning behavior.\nIn conclusion, we propose a new algorithm that reduces the variance associated with REINFORCE\nand thus signiﬁcantly increases the learning speed in the training of a team of agents. The proposed\nalgorithm is also more biologically plausible than backprop while maintaining a comparable learning\nspeed to backprop. Our work opens the prospect of the broader application of teams of agents in\ndeep RL. Our experiments also suggest a team of agents trained by MAP propagation can have more\nsophisticated exploration compared to an ANN trained by backprop.\n7\nAcknowledgment\nWe would like to thank Andrew G. Barto, who inspired this research and provided valuable insights\nand comments, as well as Andy K.P. Chan for feedback and discussions.\nReferences\n[1] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by\nback-propagating errors. Nature, 323(6088):533–536, 1986.\n[2] Francis Crick. The recent excitement about neural networks. Nature, 337(6203):129–132, 1989.\n[3] Pietro Mazzoni, Richard A Andersen, and Michael I Jordan. A more biologically plausi-\nble learning rule for neural networks. Proceedings of the National Academy of Sciences,\n88(10):4433–4437, 1991.\n[4] Randall C O’Reilly. Biologically plausible error-driven learning using local activation differ-\nences: The generalized recirculation algorithm. Neural computation, 8(5):895–938, 1996.\n[5] Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, and Zhouhan Lin. Towards\nbiologically plausible deep learning. arXiv preprint arXiv:1502.04156, 2015.\n[6] Demis Hassabis, Dharshan Kumaran, Christopher Summerﬁeld, and Matthew Botvinick.\nNeuroscience-inspired artiﬁcial intelligence. Neuron, 95(2):245–258, 2017.\n9\n[7] Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton.\nBackpropagation and the brain. Nature Reviews Neuroscience, 21(6):335–346, 2020.\n[8] Wulfram Gerstner, Werner M Kistler, Richard Naud, and Liam Paninski. Neuronal dynamics:\nFrom single neurons to networks and models of cognition. Cambridge University Press, 2014.\n[9] R˘azvan V Florian. Reinforcement learning through modulation of spike-timing-dependent\nsynaptic plasticity. Neural computation, 19(6):1468–1502, 2007.\n[10] Verena Pawlak, Jeffery R Wickens, Alfredo Kirkwood, and Jason ND Kerr. Timing is not\neverything: neuromodulation opens the stdp gate. Frontiers in synaptic neuroscience, 2:146,\n2010.\n[11] Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random\nfeedback weights support learning in deep neural networks. arXiv preprint arXiv:1411.0247,\n2014.\n[12] Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synap-\ntic feedback weights support error backpropagation for deep learning. Nature communications,\n7(1):1–10, 2016.\n[13] Qianli Liao, Joel Leibo, and Tomaso Poggio. How important is weight symmetry in back-\npropagation? In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30,\n2016.\n[14] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning. Machine learning, 8(3-4):229–256, 1992.\n[15] Philip S Thomas. Policy gradient coagent networks. In Advances in Neural Information\nProcessing Systems, pages 1944–1952, 2011.\n[16] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[17] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based\nlearning. Predicting structured data, 1(0), 2006.\n[18] Pieter R Roelfsema and Anthony Holtmaat. Control of synaptic plasticity in deep cortical\nnetworks. Nature Reviews Neuroscience, 19(3):166, 2018.\n[19] Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n[20] James CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algo-\nrithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation,\n29(5):1229–1262, 2017.\n[21] Isabella Pozzi, Sander Bohte, and Pieter Roelfsema. Attention-gated brain propagation: How\nthe brain can implement reward-based error backpropagation. Advances in Neural Information\nProcessing Systems, 33, 2020.\n[22] Pieter R Roelfsema and Arjen van Ooyen. Attention-gated reinforcement learning of internal\nrepresentations for classiﬁcation. Neural computation, 17(10):2176–2214, 2005.\n[23] Jaldert O Rombouts, Sander M Bohte, and Pieter R Roelfsema. How attention can create\nsynaptic tags for the learning of working memories in sequential tasks. PLoS Comput Biol,\n11(3):e1004060, 2015.\n[24] Javier R Movellan. Contrastive hebbian learning in the continuous hopﬁeld model. In Connec-\ntionist models, pages 10–17. Elsevier, 1991.\n[25] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural\ncomputation, 14(8):1771–1800, 2002.\n10\n[26] Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between\nenergy-based models and backpropagation. Frontiers in computational neuroscience, 11:24,\n2017.\n[27] Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierar-\nchies with hindsight. arXiv preprint arXiv:1712.00948, 2017.\n[28] Mikhail Lvovich Tsetlin. Automaton theory and modeling of biological systems. Academic\nPress, 1973.\n[29] Andrew G Barto. Learning by statistical cooperation of self-interested neuron-like computing\nelements. Human Neurobiology, 4(4):229–256, 1985.\n[30] James E Kostas, Chris Nota, and Philip S Thomas. Asynchronous coagent networks.\n[31] Philip S Thomas and Andrew G Barto. Conjugate markov decision processes. In ICML, 2011.\n[32] Théophane Weber, Nicolas Heess, Lars Buesing, and David Silver. Credit assignment techniques\nin stochastic computation graphs. arXiv preprint arXiv:1901.01761, 2019.\n[33] Diederik Kingma and Max Welling. Efﬁcient gradient-based inference through transformations\nbetween bayes nets and neural nets. In International Conference on Machine Learning, pages\n1782–1790, 2014.\n[34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[35] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. In International conference on machine learning, pages 1928–1937,\n2016.\n[36] Giacomo Indiveri, Bernabé Linares-Barranco, Tara Julia Hamilton, André Van Schaik, Ralph\nEtienne-Cummings, Tobi Delbruck, Shih-Chii Liu, Piotr Dudek, Philipp Häﬂiger, Sylvie Renaud,\net al. Neuromorphic silicon neuron circuits. Frontiers in neuroscience, 5:73, 2011.\n[37] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A\nframework for temporal abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-\n2):181–211, 1999.\n[38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[39] Paul S Khayat, Arezoo Pooresmaeili, and Pieter R Roelfsema. Time course of attentional modula-\ntion in the frontal eye ﬁeld during curve tracing. Journal of neurophysiology, 101(4):1813–1822,\n2009.\n[40] Pieter R Roelfsema, Arjen van Ooyen, and Takeo Watanabe. Perceptual learning rules based on\nreinforcers and attention. Trends in cognitive sciences, 14(2):64–71, 2010.\n[41] Geoffrey E Hinton and James McClelland. Learning representations by recirculation. In\nD. Anderson, editor, Neural Information Processing Systems. American Institute of Physics,\n1988.\n[42] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for\nboltzmann machines. Cognitive science, 9(1):147–169, 1985.\n[43] Xiaohui Xie and H Sebastian Seung. Equivalence of backpropagation and contrastive hebbian\nlearning in a layered network. Neural computation, 15(2):441–454, 2003.\n[44] Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target\npropagation. arXiv preprint arXiv:1407.7906, 2014.\n11\nAlgorithm 1: MAP Propagation - Monte-Carlo Policy-Gradient Control\n1 Input: differentiable policy function: πl(hl−1, hl; W l) for l ∈{1, 2, ..., L};\n2 Algorithm Parameter: step size α > 0, αh > 0; update step N ≥1;\n3 discount rate γ ∈[0, 1];\n4 Initialize policy parameter: W l for l ∈{1, 2, ..., L};\n5 Loop forever (for each episode):\n6\nGenerate an episode S0, H0, A0, R1, ..., ST −1, HT −1, AT −1, RT following πl(·, ·; W l) for\nl ∈{1, 2, ..., L};\n7\nLoop for each step of the episode, t = 0, 1, ..., T −1:\n8\nG ←PT\nk=t+1 γk−t−1Rk ;\n/* MAP Gradient Ascent\n*/\n9\nfor n := 1, 2, ..., N do\n10\nHl\nt ←Hl\nt + αh(∇Hl\nt log πl(Hl−1\nt\n, Hl\nt; W l) + ∇Hl\nt log πl+1(Hl\nt, Hl+1\nt\n; W l+1)) for\nl ∈{1, 2, ..., L −1};\n11\nend\n/* Apply REINFORCE\n*/\n12\nW l ←W l + αG∇W l log πl(Hl−1\nt\n, Hl\nt; W l) for l ∈{1, 2, ..., L};\nAlgorithm 2: MAP Propagation - Actor Network with Eligibility Trace\n1 Input: differentiable policy function: πl(hl−1, hl; W l) for l ∈{1, 2, ..., L};\n2 Algorithm Parameter: step size α > 0, αh > 0; update step N ≥1;\n3 trace decay rate λ ∈[0, 1]; discount rate γ ∈[0, 1];\n4 Initialize policy parameter: W l for l ∈{1, 2, ..., L};\n5 Loop forever (for each episode):\n6\nInitialize S (ﬁrst state of episode) ;\n7\nInitialize zero eligibility trace zl for l ∈{1, 2, ..., L} ;\n8\nLoop while S is not terminal (for each time step):\n9\nH0 ←S ;\n/* 1.\nFeedforward phase\n*/\n10\nSample Hl from πl(Hl−1, ·; W l) for l ∈{1, 2, ..., L} ;\n11\nA ←HL;\n/* 2.\nREINFORCE phase\n*/\n12\nif episode not in ﬁrst time step then\n13\nReceive TD error δ from the critic network;\n14\nW l ←W l + αδzl for l ∈{1, 2, ..., L};\n15\nend\n/* 3.\nMinimize energy phase\n*/\n16\nfor n := 1, 2, ..., N do\n17\nHl ←Hl + αh(∇Hl log πl(Hl−1, Hl; W l) + ∇Hl log πl+1(Hl, Hl+1; W l+1)) for\nl ∈{1, 2, ..., L −1} ;\n18\nend\n/* 4.\nTrace accumluation phase\n*/\n19\nzl ←γλzl + ∇W l log πl(Hl−1, Hl; W l) for l ∈{1, 2, ..., L};\n20\nTake action A, observe S, R ;\n12\nA\nAlgorithms\nB\nProof\nIn the proofs below, we may omit the subscript t whenever it is unnecessary. In addition to the\nnotation in Section 2, we deﬁne Dxf as the Jacobian matrix of f w.r.t. x.\nB.1\nProof of Theorem 1\nE[G∇W l log Pr(A|S; W)]\n(12)\n= E[\nG\nPr(A|S; W)∇W l Pr(A|S; W)]\n(13)\n= E[\nG\nPr(A|S; W)∇W l\nX\nh\nπL(hL−1, A; W L)πL−1(hL−2, hL−1; W L−1)...\nπ2(h1, h2; W 2)π1(S, h1; W 1)]\n(14)\n= E[\nG\nPr(A|S; W)\nX\nhl,hl−1\nPr(A, Hl = hl, Hl−1 = hl−1|S)∇W l log πl(hl−1, hl; W l)]\n(15)\n= E[G\nX\nhl−1,hl\nPr(Hl−1 = hl−1, Hl = hl|S, A)∇W l log πl(hl−1, hl; W l)]\n(16)\n= E[G E[∇W l log πl(Hl−1, Hl; W l)|S, A]]\n(17)\n= E[E[G|S, A] E[∇W l log πl(Hl−1, Hl; W l)|S, A]]\n(18)\n= E[E[G∇W l log πl(Hl−1, Hl; W l)|S, A]]\n(19)\n= E[G∇W l log πl(Hl−1, Hl; W l)].\n(20)\n(17) to (18) uses the fact that, for any random variables Z and Y , E[E[Z|Y ]f(Y )] = E[Zf(Y )].\n(18) to (19) uses the fact that G is conditional independent of Hl, Hl−1 given S, A.\n(19) to (20) uses the law of total expectation.\nNote that (17) to (20) also shows the steps for (4).\nB.2\nProof of Theorem 2\nUsing ∇hE(ˆh; s, a) = 0, ˆhL−1 can be expressed as:\n∇hE(ˆh; s, a) = 0,\n(21)\n−∇hL−1 log π(ˆhL−2, ˆhL−1; W L−1) = ∇hL−1 log π(ˆhL−1, a; W L),\n(22)\n1\n(σL−1)2 (ˆhL−1 −gL−1(ˆhL−2; W L−1)) = ∇hL−1 log π(ˆhL−1, a; W L),\n(23)\nˆhL−1 = gL−1(ˆhL−2; W L−1)+(σL−1)2∇hL−1 log π(ˆhL−1, a; W L).\n(24)\nAnd for l = 1, 2, ..., L −2, we have:\n∇hE(ˆh; s, a) = 0,\n(25)\n−∇hl log π(ˆhl−1, ˆhl; W l) = ∇hl log π(ˆhl, ˆhl+1; W l+1),\n(26)\n1\n(σl)2 (ˆhl −gl(ˆhl−1; W l)) =\n1\n(σl+1)2 Dhlgl+1(ˆhl; W l+1)T (ˆhl+1 −gl+1(ˆhl; W l+1)),\n(27)\n13\nˆhl = gl(ˆhl−1; W l) +\n\u0012 σl\nσl+1\n\u00132\nDhlgl+1(ˆhl; W l+1)T (ˆhl+1 −gl+1(ˆhl; W l+1)),\n(28)\nˆhl = gl(ˆhl−1; W l) +\n\u0012 σl\nσl+2\n\u00132\nDhlgl+1(ˆhl; W l+1)T Dhl+1gl+2(ˆhl+1; W l+2)T\n(ˆhl+2 −gl+2(ˆhl+1; W l+2)),\n(29)\nˆhl = gl(ˆhl−1; W l) + (σl)2Dhlgl+1(ˆhl; W l+1)T Dhl+1gl+2(ˆhl+1; W l+2)T\n...DhL−2gL−1(ˆhL−2; W L−1)T ∇hL−1 log πL(ˆhL−1, a; W L).\n(30)\nSubstituting back to the REINFORCE update, we have:\n∇W l log πl(ˆhl−1, ˆhl; W l)\n(31)\n=\n1\n(σl)2 DW lgl(ˆhl−1; W l)T (ˆhl −gl(ˆhl−1; W l))\n(32)\n=DW lgl(ˆhl−1; W l)T Dhlgl+1(ˆhl; W l+1)T Dhl+1gl+2(ˆhl+1; W l+2)T ...\nDhL−2gL−1(ˆhL−2; W L−1)T ∇hL−1 log πL(ˆhL−1, a; W L).\n(33)\nWe will show that the R.H.S. also equals (33). Consider the re-parameterization of Hl by Zl\nconditioned on Hl−1 using gl(Hl−1; W l) + σlZl and Zl are independent standard Gaussian noises\nfor l ∈{1, 2, ..., L −1}. Then by re-parameterizing all hidden layers, we can ﬁnd h(Z; W, S) that\nis the re-parameterization of H by Z := {Z1, Z2, ..., ZL−1} conditioned on S. To be concrete,\nhl(Z; W, S) = gl(hl−1(Z; W, S); W l) + σlZl for l ∈{1, 2, ..., L −1}, and h0(Z; W, S) := S.\nThen, for l ∈{1, 2, ..., L −2}, we have:\n∇W l log πL(hL−1(z; W, s), a; W L)\n(34)\n=DW l(hL−1(z; W, s))T ∇hL−1 log πL(hL−1(z; W, s), a; W L)\n(35)\n=DW l(gL−1(hL−2(z; W, s); W L−1) + σL−1zL−1)T ∇hL−1 log πL(hL−1(z; W, s), A; W L) (36)\n=DW l(hL−2(z; W, s))T DhL−2gL−1(hL−2(z; W, s); W L−1)T\n∇hL−1 log πL(hL−1(z; W, s), a; W L)\n(37)\n=DW lgl(hl−1(z; W, s); W l)T Dhlgl+1(hl(z; W, s); W l+1)T Dhl+1gl+2(hl+1(z; W, s); W l+2)T ...\nDhL−2gL−1(hL−2(z; W, s); W L−1)T ∇hL−1 log πL(hL−1(z; W, s), a; W L).\n(38)\nIf we evaluate (34) at z = ˆz, then (38) becomes (33), which completes the proof for l ∈{1, 2, ..., L −\n1}. The proof for l = L is similar and is omitted here.\nB.3\nProof of Theorem 3\nSimilar to the proof of Theorem 2, for l ∈{1, 2, ..., L −1}, the L.H.S. can be expressed as:\nA∗(s) −ˆµL\na −ˆµL\n∇W l log πl(ˆhl−1, ˆhl; W l)\n(39)\n=A∗(s) −ˆµL\na −ˆµL\nDW lgl(ˆhl−1; W l)T Dhlgl+1(ˆhl; W l+1)T Dhl+1gl+2(ˆhl+1; W l+2)T ...\nDhL−2gL−1(ˆhL−2; W L−1)T ∇hL−1 log πL(ˆhL−1, a; W L)\n(40)\n=A∗(s) −ˆµL\n(σL)2\nDW lgl(ˆhl−1; W l)T Dhlgl+1(ˆhl; W l+1)T Dhl+1gl+2(ˆhl+1; W l+2)T ...\nDhL−2gL−1(ˆhL−2; W L−1)T ∇hL−1gL(ˆhL−1; W L).\n(41)\nWe then prove that the R.H.S. also equals (41). Consider the same re-parameterization of H by\nZ := {Z1, Z2, ..., ZL−1} conditioned on S, denoted by h(Z; W, S), as in the proof of Theorem 2.\n14\nThen, for l ∈{1, 2, ..., L −1},\n∇W l −\n\u0000A∗(s) −gL(hL−1(z; W, s); W L)\n\u00012\n(42)\n=2(A∗(s) −gL(hL−1(z; W, s); W L))∇W lgL(hL−1(z; W, s); W L)\n(43)\n=2(A∗(s) −gL(hL−1(z; W, S); W L))DW l(hL−1(z; W, s))T ∇hL−1gL(hL−1(z; W, s); W L)\n(44)\n=2(A∗(s) −gL(hL−1(z; W, S); W L))DW lgl(hl−1(z; W, s); W l)T Dhlgl+1(hl(z; W, s); W l+1)T\nDhl+1gl+2(hl+1(z; W, s); W l+2)T ... ∇hL−1gL(hL−1(z; W, s); W L).\n(45)\nIf we evaluate (42) at z = ˆz, then (45) becomes proportional to (41) with a ratio 2(σL)2, which\ncompletes the proof for l ∈{1, 2, ..., L −1}. The proof for l = L is similar and is omitted here.\nB.4\nDetails of (7) to (8)\n∇Hl\nt log Pr(Ht|St, At)\n(46)\n=∇Hl\nt (log Pr(Ht, At|St) −log Pr(At|St))\n(47)\n=∇Hl\nt log Pr(Ht, At|St)\n(48)\n=∇Hl\nt log\n L−1\nY\ni=0\nπi(Hi\nt, Hi+1\nt\n; W i+1)\n!\n(49)\n=∇Hl\nt log πl+1(Hl\nt, Hl+1\nt\n; W l+1) + ∇Hl\nt log πl(Hl−1\nt\n, Hl\nt; W l).\n(50)\nB.5\n(10) Follows the Gradient of Return in Expectation\nWe will show that the learning rule given by (10) follows the gradient of return in expectation. For\nl ∈{1, 2, ..., L −1}:\n∇W l E[G]\n(51)\n= E[G∇W l log Pr(A|S; W)]\n(52)\n= E[\nG\nPr(A|S; W)∇W l Pr(A|S; W)]\n(53)\n= E[\nG\nPr(A|S; W)∇W l\nX\nz\nPr(A|Z = z, S; W) Pr(Z = z|S)]\n(54)\n= E[\nG\nPr(A|S; W)\nX\nz\nPr(A|Z = z, S; W) Pr(Z = z|S)∇W l log Pr(A|Z = z, S; W)]\n(55)\n= E[G\nX\nz\nPr(Z = z|S, A; W)∇W l log Pr(A|Z = z, S; W)]\n(56)\n= E[G E[∇W l log Pr(A|Z, S; W)|S, A]]\n(57)\n= E[G∇W l log Pr(A|Z, S; W)]\n(58)\n= E[G∇W l log πL(hL−1(Z; W, S), A; W L)].\n(59)\n(51) to (52) uses REINFORCE and other steps are similar to those in the proof of Theorem 1.\nB.6\nVariance Reduction of (4)\nWe will show that for l ∈{1, 2, ..., L}:\nVar[G E[∇W l log πl(Hl−1, Hl; W l)|S, A]] ≤Var[G∇W l log πl(Hl−1, Hl; W l)].\n(60)\n15\nThe proof is as follows:\nVar[G∇W l log πl(Hl−1, Hl; W l)]\n(61)\n= Var[E[G∇W l log πl(Hl−1, Hl; W l)|S, A, G]]\n+ E[Var[G∇W l log πl(Hl−1, Hl; W l)|S, A, G]]\n(62)\n≥Var[E[G∇W l log πl(Hl−1, Hl; W l)|S, A, G]]\n(63)\n= Var[G E[∇W l log πl(Hl−1, Hl; W l)|S, A, G]]\n(64)\n= Var[G E[∇W l log πl(Hl−1, Hl; W l)|S, A]].\n(65)\n(61) to (62) uses the law of total variance.\n(62) to (63) uses the fact that the second term must be non-negative.\n(64) to (65) uses the fact that G is conditional independent with H given S and A.\nC\nMAP Propagation for Critic Networks\nHere we consider how to apply MAP propagation to a critic network. As the function of a critic\nnetwork can be seen as approximating the scalar value Rt + γˆv(St+1), we consider how to learn a\nscalar regression task by MAP propagation in general.\nA scalar regression task can be converted into a single-time-step MDP with the appropriate reward\nand R as the action set. For example, we can set the reward function to be R(S, A) = −(A−A∗(S))2\n(we dropped the subscript t as it only has a single time step), with A ∈R being the output of the\nnetwork and A∗(S) ∈R being the target output given input S. The maximization of rewards in this\nMDP is equivalent to the minimization of the L2 distance between the predicted value and the target\nvalue.\nBut this conversion is inefﬁcient since the information of the reward function is lost. In the following\ndiscussion, we restrict our attention to a network of stochastic units where all hidden layers and the\noutput layer are normally distributed as deﬁned in Section 2. Let µL be the conditional mean of the\noutput layer; that is, µL = g(HL−1; W L). For this network, we propose an alternative learning rule\nthat is similar to REINFORCE but with the return G replaced by (A∗(S) −µL)/(A −µL); that is,\nfor l ∈{1, 2, ..., L}:\nW l ←W l + αA∗(S) −µL\nA −µL\n∇W l log πl(Hl−1, Hl; W l).\n(66)\nIt can be shown that after minimizing the energy function, the learning rule (66) for the network is\nequivalent to gradient descent on the L2 error by backprop with the re-parameterization trick:\nTheorem 3. Let the policy be a multi-layer network of stochastic units with all hidden layers\nnormally distributed as deﬁned in Section 2 and the output layer has a single unit. There exists a re-\nparameterization of H by Z conditioned on S, denoted by h(Z; W, S), such that for any A∗: S →R,\nl ∈{1, 2, ..., L}, s ∈S, ˆh, ˆz ∈Rn(1) × Rn(2) × ... × Rn(L−1) and a ∈R, if ∇hE(ˆh; s, a) = 0 and\nˆz = h−1(ˆh; W, s), then\nA∗(s) −ˆµL\na −ˆµL\n∇W l log πl(ˆhl−1, ˆhl; W l) ∝−∇W l\n\u0000A∗(s) −˜µL\u00012 ,\n(67)\nwhere ˆµL := gL(ˆhL−1; W L) and ˜µL := gL(hL−1(ˆz; W, s); W L).\nTherefore, we can apply the learning rule (66) after minimizing the energy function by (8). The pseudo-\ncode is the same as Algorithm 1 in Appendix A, but with G replaced by (A∗(S) −µL)/(A −µL) in\nline 12.\nWe then consider applying the above method to train a critic network, where the output of the network,\nAt ∈R, is an estimation of the current value. In a critic network, the target output is Rt + γAt+1.\nHowever, a more stable estimate of target output is Rt + γµL\nt+1 since the difference between At+1\nand µL\nt+1 is an independent Gaussian noise that can be removed. Therefore, we chose A∗(S), the\n16\ntarget output, as Rt + γµL\nt+1 and TD error as δt := Rt + γµL\nt+1 −µL\nt . Substituting back into (66),\nthe learning rule for the critic network becomes:\nW l ←W l + α\nδt\nAt −µL\nt\n∇W l log πl(Hl−1\nt\n, Hl\nt; W l),\n(68)\nwhich is almost the same as the update rule for the actor network except the additional denominator\nAt −µL\nt . The pseudo-code of training a critic network with eligibility trace using MAP propagation\nis the same as Algorithm 2 in Appendix A, except (i) line 13 is replaced with δ ←γµ + R −µ′\nwhere µ = gL(HL−1; W L) and µ′ is µ in the previous time step, and (ii) the gradient term in line 19\nis multiplied by (A −µ)−1.\nBoth the critic and the actor network can be trained together, and the TD error δ computed by the\ncritic network can be passed to the actor network in line 13 of Algorithm 2.\nD\nDetails of Experiments\nIn the multiplexer task, there are k + 2k binary inputs, where the ﬁrst k bits represent the address\nand the last 2k bits represent the data, each of which is associated with an address. The output of\nthe multiplexer is given by the value of data associated with the address. This is similar to the 2-bit\nmultiplexer considered in [29].\nWe used Algorithm 1 in the multiplexer and the scalar regression experiment, and the hyperparameters\ncan be found in Table 2. We used a different learning rate α for each layer of the network, and we\ndenote the learning rate for the lth layer to be αl. We denote the variance of the Gaussian distribution\nfor the lth layer to be σ2\nl . The step size of hidden units when minimizing energy, αh, is selected to\nbe 0.5 times the variance of the unit. For the learning rule in line 12 of the pseudo-code, we used\nAdam optimizer [38] instead, with β1 = 0.9 and β2 = 0.999. We used batch update in both tasks,\nwhich means that we compute the parameter update for each sample in a batch, then we update the\nparameter using the average of these parameter updates. These hyperparameters are selected based\non manual tuning to optimize the learning curve. We did the same manual tuning for the baseline\nmodels.\nWe used Algorithm 2 to train both the critic and the actor network in the experiments on RL tasks,\nand the hyperparameters can be found in Table 3. We did not use any batch update in our experiments,\nand we used a discount rate of 0.98 for all tasks. The step size of hidden units when minimizing\nenergy, αh, is selected to be 0.5 times the variance of the unit. We used Acrobot-v1, CartPole-v1,\nLunarLander-v2, and MountainCarContinuous-v0 in OpenAI’s Gym for the implementation of the\nRL tasks.\nFor the update rules in line 14 of Algorithm 2, we used Adam optimizer instead, with β1 = 0.9 and\nβ2 = 0.999. Again, these hyperparameters are selected based on manual tuning to maximize the\naverage return across all episodes. We did the same manual tuning for the baseline models.\nFor the ANNs in the baseline models, the architecture is similar to the team of agents: 64 units on\nthe ﬁrst hidden layer and 32 units on the second hidden layer. If the output range is continuous, the\noutput layer is a linear layer. If the output range is discrete, the output layer is a softmax layer. We\nused the softplus function as the activation function in the ANNs, which performs similarly to the\nReLu function in our experiments.\nWe annealed the learning rate linearly such that the learning rate is reduced to 1\n10 of the initial learning\nrate at 50000 and 100000 steps in CartPole and Acrobat respectively, and the learning rate remains\nunchanged afterward. We also annealed the learning rate linearly for the baseline models. We found\nthat this can make the ﬁnal performance more stable. For LunarLander and MountainCar, we did not\nanneal the learning rate. For MountainCar, we bound the reward by ±5 to stabilize learning.\nE\nExperiments on MountainCar\nIn the continuous version of MountainCar, the state is composed of two scalar values, which are the\nposition and the velocity of the car, and the action is a scalar value corresponding to the force applied\nto the car. The goal is to reach the peak of the mountain on the right, where a large positive reward\nis given. However, to reach the peak, the agent has to ﬁrst push the car to the left in order to gain\n17\nFigure 3: Illustration of MountainCar.\n100\n150\n200\n250\n300\n350\n400\n450\n500\nEpisodes\n0\n20\n40\n60\n80\nRunning Average Rewards\nMAP Propaga ion\nBackprop - Successful\nBackprop - Failed\nFigure 4: Running average rewards over the last\n100 episodes of the selected runs in MountainCar.\n−1.0\n−0.5\n0.0\n0.5\nPosition\n−0.05\n0.00\n0.05\nVelocity\n1ˢᵗ Episode\n−1.0\n−0.5\n0.0\n0.5\nPosition\n−0.05\n0.00\n0.05\n50ᵗʰ Episode\n−1.0\n−0.5\n0.0\n0.5\nPosition\n−0.05\n0.00\n0.05\n100ᵗʰ Episode\nMʰP Propagation\nBackprop - Successful\nBackprop - Failed\nFigure 5: State trajectories of the 1st, 50th and 100th episode of the selected runs. If the position\nis larger than 0.45, then the agent reaches the goal. Although the team of agents trained by MAP\npropagation did not reach the goal in both the 1st and 50th episode, the team was still exploring a large\nportion of the state space, which is in contrast to the failed baseline that stayed at the center. For the\nbaseline, if it does not reach the goal in the ﬁrst several episodes, it will be stuck in the local optima.\nenough momentum to overcome the slope. A small negative reward that is proportional to the force\napplied is given on every time step to encourage the agent to reach the goal with minimum force. An\nillustration of MountainCar is shown in Fig 3.\nOne locally optimal policy is to apply zero force at every time step so that the car always stays at\nthe bottom. In this way, the return is zero since no force is applied. We found that in many runs,\nthe ANN trained by backprop was stuck in this locally optimal policy. However, in a few runs, the\nANN can accidentally reach the goal in early episodes, which makes the ANN able to learn quickly\nand achieve an asymptotic reward of over +90, slightly higher than that of a team of agents trained\nby MAP propagation. The learning curve of a successful and a failed run of the agent trained by\nbackprop is shown in Fig 4.\nIn contrast, for teams of agents trained by MAP propagation, the teams in all runs can learn a policy\nthat reaches the goal successfully. However, this comes at the expense of slower learning and a\nslightly worse asymptotic performance, as seen from the learning curve of a typical run of the team\nof agents trained by MAP propagation shown in Fig 4. This is likely due to the larger degree of\nexploration in MAP propagation, which can be illustrated by the state trajectories shown in Fig 5.\nWe used the same variance for the output unit in both MAP propagation and the baseline models. We\nfound that even using a larger variance or adding entropy regularization cannot prevent the baseline\nmodels from being stuck in the local optima. This suggests that MAP propagation allows more\nsophisticated exploration instead of merely more exploration. In a team of agents, each agent in the\nteam is exploring its own action space, thus allowing exploration in different levels of the hierarchy.\n18\nThis may explain the difference in exploration behavior observed in a team of agents trained by MAP\npropagation compared to an ANN trained by backprop.\nF\nBiological Plausibility of MAP Propagation\nAs discussed in the paper, backprop has three major problems with biological plausibility due to the\nrequirement of 1. non-local information in the learning rule, 2. precise coordination between the\nfeedforward and feedback phase, and 3. symmetry of feedforward and feedback weights. However,\nREINFORCE does not have any of these issues. Other than the global reinforcement signal, the\nlearning rule of REINFORCE does not depend on non-local information. Also, since REINFORCE\ndoes not require any feedback connections, the second and third issues do not exist for REINFORCE.\nWe refer readers to chapter 15 of [16] for a review and discussion of the connection between\nREINFORCE and neuroscience.\nCompared to backprop, REINFORCE is more consistent with biologically-observed forms of synaptic\nplasticity. When applied to a Bernoulli-logistic unit, REINFORCE gives a three-factor learning\nrule which depends on a reinforcement signal, input, and output of the unit. This is similar to\nR-STDP observed biologically, which depends on a neuromodulatory input, presynaptic spikes,\nand postsynaptic spikes. It has been proposed that dopamine, a neurotransmitter, plays the role of\nneuromodulatory input in R-STDP and corresponds to the TD error from RL.\nDespite the elegance of REINFORCE, its learning speed is far lower than backprop and scales poorly\nwith the number of units in the network since only a scalar feedback is used to assign credit to all\nunits in the network. It can be argued that learning speed may not be the issue for the biological\nplausibility of REINFORCE, given that evolution already equips the brain with prior knowledge,\nand the brain can learn from experience accumulated over the entire lifetime. However, the learning\nspeed of REINFORCE may not explain many remarkable learning behaviors of humans, such as\nmastering Go, despite the fact that the ability to play Go likely does not come from prior knowledge\nshaped by evolution. Given billions of neurons in the brain, it is likely that the brain employs some\nforms of structural credit assignment to speed up learning.\nMAP propagation presents one possible solution for structural credit assignment. Essentially, MAP\npropagation is equivalent to applying REINFORCE after minimizing the energy function. The idea\nof minimizing the energy function is to nudge the values of hidden units towards those that are more\nconsistent with the values of units on the ﬁrst and last layer, i.e. the state and the action. In this\nprocess, feedback connections are required to drive the value of units, so as to propagate information\nfrom the layers above.\nAlthough the purpose of feedback connections in the brain is still not completely understood, there\nhas been evidence showing that feedback connections can drive the activity of neurons [39, 40, 18].\nFor example, in the visual system, the activity of neurons that is responsible for the selected action\nwill be enhanced by feedback connection [39]. This is analogous to the updates of hidden units in\nMAP propagation. The general idea that feedback connection drives the activity of units in lower\nlayers to facilitate local learning rules underlies many proposals for biological learning and machine\nlearning algorithms [7]. This idea is also fundamental to the NGRAD hypothesis [7], which will be\ndiscussed next.\nNGRAD hypothesis is a recently proposed hypothesis that uniﬁes many biologically plausible\nalternatives to backprop with local learning rules [4, 5, 20, 25, 26, 41–44]. It hypothesizes that\nthe cortex uses the differences in activity states to drive learning, and the induced differences are\nbrought by the nudging of lower-level activities towards those values that are more consistent with\nthe high-level activities. In this way, local learning rules can yield an approximation to backprop\nwithout storing units’ values and error signals at the same time.\nMAP propagation ﬁts well into the NGRAD hypothesis. When normally distributed hidden units\nare settled to the minima of the energy function, REINFORCE, a local learning rule except for the\nglobal reinforcement signal, yields the same parameter update given by backprop. Therefore, MAP\npropagation can be seen as an approximation of backprop by changing the values of hidden units.\nHowever, MAP propagation has major differences with many other algorithms based on the NGRAD\nhypothesis (NGRAD algorithms). First, most NGRAD algorithms require storage of past units’\nvalues (e.g. in target propagation [44], the unit has to store its past value to compute the reconstruction\n19\nerror) or separate phases of learning (e.g. the positive and negative phase in contrastive divergence\n[25]), but MAP propagation requires neither of them. Second, most NGRAD algorithms require\nprecise coordination between feedforward and feedback connections (e.g. in target propagation, the\nunit has to coordinate between computing the reconstruction error and adding it to the uncorrelated\ntarget). In contrast, the updates for all layers can be done in parallel without any coordination between\nfeedforward and feedback connections in MAP propagation. Third, MAP propagation is derived\nbased on RL, while NGRAD algorithms are derived based on either supervised or unsupervised\nlearning. Given the observation of R-STDP in biological systems and the correspondence between\nR-STDP and REINFORCE, MAP propagation presents a new paradigm of explaining biological\nlearning in NGRAD algorithms. However, a major limitation of MAP propagation is that it requires a\ndifferent value to be propagated through feedforward and backward connections.\nTo see this, we will closely examine the update rule (8) for minimizing energy function in MAP\npropagation. Assuming all units are normally distributed with a ﬁxed variance; i.e. πl(Hl−1, ·; W l) =\nN(f(W lHl−1), σ2) for l ∈{1, 2, ..., L} and f is a non-linear activation function, then the update\nrule (8) and the learning rule (3) becomes 3:\n∆Hl = 1\nσ2\n\u0000−el + (W l+1)T (el+1 ⊙δl+1)\n\u0001\n,\n(69)\n∆W l = 1\nσ2\n\u0000G · el(HL−1)T \u0001\n,\n(70)\nwhere µl = f(W lHl−1), δl = f ′(W lHl−1), el = Hl −µl for l ∈{1, 2, ..., L}.\nBoth the update rule (8) and the learning rule (3) are local and can be applied to all hidden layers\nin parallel. There are two components in the update rule: i. the feedforward signal −el and ii. the\nfeedback signal (W l+1)T (el+1 ⊙δl+1). The feedforward signal nudges the value of the unit, Hl,\ncloser to the mean value of the unit, µl, which only depends on feedforward signals. However,\nit is not yet clear how the feedback signal can be implemented with biological systems. First, it\nrequires information to be propagated through the feedback weight (W l+1)T that is symmetric of the\nfeedforward weight in the next layer, which may not be biologically plausible. Nonetheless, recent\nwork has shown that symmetric weights may not be necessary for backprop due to the phenomenon\nof ‘feedback alignment’ [11–13], and similar phenomenons may also exist for MAP propagation.\nSecond, the information to be propagated backward is el+1 ⊙δl+1, which is different from the\ninformation to be propagated forward (Hl). An illustration of this is shown in Fig 6.\nThe issue of propagating two different values also exists for backprop since error signals, instead of\nunits’ values, are propagated backward in backprop. However, in backprop, the backpropagated error\nsignals have to be stored separately from the units’ values, so as to compute the next error signals to\nbe passed to the lower layers. In contrast, the feedback signal in MAP propagation is only used to\nnudge the value of units and does not need to be stored separately. In other words, backprop requires\nnon-local information in the computation of feedback signal, but not MAP propagation.\nIt is not yet clear how a neuron can propagate two different values at the same time, even if both\nvalues are locally available. But there are many possible solutions to avoid propagating different\nvalues in MAP propagation. For instance, since the feedback signal (W l+1)T (el+1 ⊙δl+1) can\nbe expressed as a function of Hl and Hl+1, it might be possible to approximate this term based\non feedback connections (for Hl+1) and recurrent connections (for Hl), and learn the weights in\nthese connections, such that all units are propagating the same value. Another possible solution is to\nminimize the energy function by hill-climbing methods instead of gradient ascent, such that only the\nscalar energy, instead of feedback connection, is required to guide the minimization of the energy\nfunction. Further work can be done on these possible solutions.\nDespite the limitations of MAP propagation, we argue that MAP propagation is more biologically\nplausible than backprop. The two major limitations of MAP propagation also exist in backprop, but\nMAP propagation does not require non-local information in both the learning rule or the computation\nof feedback signals. Also, the update of all layers can be done in parallel in MAP propagation,\nremoving the requirement of precise coordination between feedforward and feedback connections\nthat is required in backprop.\n3We ignore the subscript t here since it does not affect our discussion, and ⊙denotes element-wise multipli-\ncation.\n20\nFigure 6: Illustration of minimizing the energy function in MAP propagation. The mean value of\nunits on the layer l, denoted by µl, is computed as a function of the value of units on the previous\nlayer, denoted by Hl−1. The difference between the current and the mean value of units, denoted\nby el, is then used to drive the value of units on the same layer (Hl) and the value of units on the\nprevious layer (Hl−1).\n21\nTable 2: Hyperparameters used in multiplexer and scalar regression experiments.\nMultiplexer\nScalar Regression\nBatch Size\n128\n128\nN\n20\n20\nα1\n4e-2\n6e-2\nα2\n4e-5\n6e-5\nα3\n4e-6\n6e-6\nσ2\n1\n0.3\n0.0075\nσ2\n2\n1\n0.025\nσ2\n3\nn.a.\n0.025\nT\n1\nn.a.\nTable 3: Hyperparameters used in Acrobat, Cartpole, Lunarlander and MountainCar experiments.\nAcrobat\nCartPole\nLunarLander\nMountainCar\nCritic\nActor\nCritic\nActor\nCritic\nActor\nCritic\nActor\nN\n20\n20\n20\n20\n20\n20\n20\n20\nα1\n2e-2\n1e-2\n2e-2\n1e-2\n1e-2\n4e-3\n1e-2\n4e-3\nα2\n2e-5\n1e-5\n2e-5\n1e-5\n1e-5\n4e-6\n1e-5\n4e-6\nα3\n2e-6\n1e-6\n2e-6\n1e-6\n1e-6\n4e-7\n1e-6\n4e-7\nσ2\n1\n0.06\n0.03\n0.03\n0.03\n0.003\n0.06\n0.003\n0.03\nσ2\n2\n0.2\n0.1\n0.1\n0.1\n0.01\n0.2\n0.01\n0.1\nσ2\n3\n0.2\nn.a.\n0.1\nn.a.\n0.01\nn.a.\n0.05\n0.5\nT\nn.a.\n4\nn.a.\n2\nn.a.\n8\nn.a.\nn.a.\nλ\n.97\n.97\n.95\n.95\n.97\n.97\n.97\n.97\n22\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "I.2.8"
  ],
  "published": "2020-10-15",
  "updated": "2021-10-05"
}