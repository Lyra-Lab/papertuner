{
  "id": "http://arxiv.org/abs/2010.09465v1",
  "title": "A Nesterov's Accelerated quasi-Newton method for Global Routing using Deep Reinforcement Learning",
  "authors": [
    "S. Indrapriyadarsini",
    "Shahrzad Mahboubi",
    "Hiroshi Ninomiya",
    "Takeshi Kamio",
    "Hideki Asai"
  ],
  "abstract": "Deep Q-learning method is one of the most popularly used deep reinforcement\nlearning algorithms which uses deep neural networks to approximate the\nestimation of the action-value function. Training of the deep Q-network (DQN)\nis usually restricted to first order gradient based methods. This paper\nattempts to accelerate the training of deep Q-networks by introducing a second\norder Nesterov's accelerated quasi-Newton method. We evaluate the performance\nof the proposed method on deep reinforcement learning using double DQNs for\nglobal routing. The results show that the proposed method can obtain better\nrouting solutions compared to the DQNs trained with first order Adam and\nRMSprop methods.",
  "text": "arXiv:2010.09465v1  [cs.LG]  15 Oct 2020\nA Nesterov’s Accelerated quasi-Newton method for Global Routing using Deep\nReinforcement Learning\nS. Indrapriyadarsini† , Shahrzad Mahboubi§, Hiroshi Ninomiya§, Takeshi Kamio∗, Hideki Asai‡\n†Graduate School of Science and Technology / ‡Research Institute of Electronics, Shizuoka University\n3-5-1 Johoku, Naka-ku, Hamamatsu, Shizuoka Prefecture 432-8011, Japan\n§Graduate School of Electrical and Information Engineering, Shonan Institute of Technology\n1-1-25 Tsujidonishikaigan, Fujisawa, Kanagawa Prefecture 251-0046, Japan\n∗Graduate School of Information Sciences, Hiroshima City University,\n3-4-1 Ozukahigashi, Asaminami Ward, Hiroshima, 731-3194, Japan\nEmail: s.indrapriyadarsini.17@shizuoka.ac.jp, 20T2502@sit.shonan-it.ac.jp, ninomiya@info.shonan-it.ac.jp,\nkamio@hiroshima-cu.ac.jp, asai.hideki@shizuoka.ac.jp\nAbstract—Deep Q-learning method is one of the most\npopularly used deep reinforcement learning algorithms\nwhich uses deep neural networks to approximate the es-\ntimation of the action-value function. Training of the deep\nQ-network (DQN) is usually restricted to ﬁrst order gradi-\nent based methods. This paper attempts to accelerate the\ntraining of deep Q-networks by introducing a second order\nNesterov’s accelerated quasi-Newton method. We evalu-\nate the performance of the proposed method on deep rein-\nforcement learning using double DQNs for global routing.\nThe results show that the proposed method can obtain bet-\nter routing solutions compared to the DQNs trained with\nﬁrst order Adam and RMSprop methods.\n1. Introduction\nReinforcement learning (RL) is a machine learning tech-\nnique where an agent perceives its current state s and takes\nactions a by interacting with an environment. The envi-\nronment, in return provides a reward R, while the rein-\nforcement learning algorithm attempts to ﬁnd a policy π\nfor maximizing the cumulative reward for the agent over\nthe course of the problem [1]. The Q-learning algorithm\nis one of the popular oﬀ-policy reinforcement learning al-\ngorithms that chooses the best action based on estimates\nof the state-action value Q(s, a) represented in the form of\na table called the Q-table. As the state and action space\nof the problem increases, the estimation of the state-action\nvalue can be slow and time consuming and hence estimated\nas a function approximation. These function approxima-\ntions can be represented as a non-convex, non-linear un-\nconstrained optimization problem and can be solved using\ndeep neural networks (known as deep Q-networks).\nTraining of Deep Q-Networks (DQN) are usually re-\nstricted to ﬁrst order methods such as stochastic gradient\ndescent (SGD), Adam [2], RMSprop [3] etc. Using sec-\nond order curvature information have shown to improve\nthe performance and convergence speed for non convex\noptimization problems [4]. The BFGS method is one of\nthe most popular second order quasi-Newton method. The\nNesterov’s accelerated quasi-Newton (NAQ) method [5]\nwas shown to accelerate the BFGS method using the Nes-\nterov’s accelerated gradient term. In extension to our pre-\nvious work in [6], we show that the adaptive stochastic\nNesterov’s accelerated quasi-Newton method (aSNAQ) al-\nlows more stable approximations and is eﬃcient in training\nDQNs for deep reinforcement learning applications. We\nevaluate the performance of the proposed method in com-\nparison to popular ﬁrst order methods in solving the global\nrouting problem using reinforcement learning.\nSynthesis and physical design optimizations are the core\ntasks of the VLSI / ASIC design ﬂow. Global routing has\nbeen a challenging problem in IC physical design. Given\na netlist with the description of all the components, their\nconnections and positions, the goal of the router is to de-\ntermine the path for all the connections without violating\nthe constraints and design rules. Conventional routing au-\ntomation tools are usually based on analytical and path\nsearch algorithms which are NP complete. Hence a ma-\nchine learning approach would be more suitable for this\nkind of automation problem. Studies that propose AI tech-\nniques such as machine learning, deep learning, genetic\nalgorithms deal with only prediction of routability, short\nviolations, pin-access violations, etc. Moreover, the non-\navailability of large labelled training datasets for a super-\nvised learning model is another challenge. Thus deep rein-\nforcement learning (DRL) is a potential approach to such\napplications. Recently, a reinforcement learning approach\nto global routing that uses ﬁrst order gradient based method\nfor training the DQN was proposed in [7].\nThis paper attempts to accelerate training of deep Q-\nnetworks by introducing a second order Nesterov’s accel-\nerated quasi-Newton method to get better routing solutions\nusing a deep reinforcement learning approach. Also, to fur-\nther enhance the performance of the DRL model for global\nrouting, we use double deep Q-learning [8]. The obtained\nrouting solution is evaluated in terms of total wirelength\nand overﬂow and compared with the results of Adam and\nRMSprop.\n2. Background\nThe Reinforcement Learning problem is modelled as a\nMarkov’s Decision Process (MDP). In order to solve the\nMDP, the estimates of the value function of all possible ac-\ntions is learnt using Q-learning method, a form of temporal\ndiﬀerence learning [4]. The optimal action-value function\nQ∗(s, a) that maximizes the cummulative reward satisﬁes\nthe Bellman equation and is given as\nQ∗(s, a) = Es′∼ζ[R + γ maxa′Q∗(s′, a′)|s, a]\n(1)\nwhere γ is the discount factor. The Q-learning algorithm is\nan oﬀ-policy, model-free reinforcement learning algorithm\nthat iteratively learns the optimal action-value function. In\ndeep Q-learning, this function is optimized by a neural net-\nwork parameterized by w. The inputs to the neural network\nare the states s and the output predicted by the neural net-\nwork correspond to the action values Q(s, a; w) for each\naction a. Deep Q-networks use experience replay to train\nthe network. A replay buﬀer of ﬁxed memory D stores\nthe transitions (s, a, r, s′) of the agent’s experiences from\nwhich samples are randomly drawn for training the DQN.\nThe loss function L(w) as shown in (2) is used in back-\npropagation and calculation of the gradients for updating\nthe parameters w.\nL(w) = E(s,a)∼ζ[(Y −Qw(s, a))2]\n(2)\nwhere the target function Yi is given as\nY = E(s′)∼ζ[R + γ maxa′Qw(s′, a′)]\n(3)\nDQNs are said to overestimate the update of the action-\nvalue function since Qw(s, a) is used to select the best next\naction at state s′ and apply the action value predicted by the\nsame Qw(s, a). Double Deep Q-learning [8] resolves this\nissue by decoupling the action selection and action value\nestimation using two Q-networks.\nY = E(s′)∼ζ[R + γQw−(s′, argmaxa′Qw(s′, a′))]\n(4)\nw and w−represent the parameters of the two Q networks –\nprimary and target networks respectively. The primary net-\nwork parameters are periodically copied to the target net-\nwork using Polyak averaging (5), where τ is a small value\nsuch as 0.05.\nw−←τw + (1 −τ)w−\n(5)\nAt each iteration of the training, the parameters of the\nDQN are updated as wk+1 = wk + vk. The update vector is\ngiven as vk = −α∇L(wk) where ∇L(wk) is the gradient of\nthe loss function calculated on a small mini-batch sample\ndrawn at random from the experience replay buﬀer D.\n3. Nesterov’s Accelerated Quasi-Newton Method for\nQ-learning\nFirst order gradient based methods have been commonly\nused in training DQNs due to their simple complexity.\nHowever approximated second order quasi-Newton meth-\nods have shown to signﬁcantly speed up convergence in\nnon-convex optimization problems.\nThe Nesterov’s ac-\ncelerated quasi-Newton (NAQ) [5] and its variants have\nshown to accelerate convergence compared to the stan-\ndard quasi-Newton method in various supervised learning\nframeworks. In this paper, we propose a variant of the Nes-\nterov’s accelerated quasi-Newton method and investigate\nits feasibility in the reinforcement learning framework. The\nalgorithm is shown in Algorithm 1.\nNAQ achieves faster convergence by quadratic approxi-\nmation of the objective function at wk+µvk and by incorpo-\nrating the Nesterov’s accelerated gradient ∇L(wk + µvk) in\nits Hessian update. The search direction gk = −Hk∇L(wk+\nµvk) is computed using the two-loop recursion [4], where\nAlgorithm 1 Proposed aSNAQ for DQN\nRequire: minibatch Xk, µmin, µmax, kmax, Emax, aFIM buﬀer\nF of size mF and curvature pair buﬀer (S, Y) of size mL,\nmomentum update factor φ, experience replay buﬀer D\nInitialize: wo=wk ∈Rd, µ = µmin, vk, vo , ws, vs, & t = 0\n1: for episode E = 1, 2, ..., Emax do\n2:\nInitialize state s\n3:\nfor step k = 1, 2, ..., kmax do\n4:\nTake action a based on epsilon greedy strategy\n5:\nStore transistion (s, a, r, s′, a′) in D\n6:\nSample random minibatch Xk from D\n7:\nCalculate ∇L(wk + µvk)\n8:\nDetermine gk using two loop recursion\n9:\ngk = gk/||gk||2\n10:\nvk+1 ←µvk + αkgk\n11:\nwk+1 ←wk + vk+1\n12:\nCalculate ∇L(wk+1) and store in F\n13:\nws = ws + wk and vs = vs + vk\n14:\nif mod(k , L) = 0 then\n15:\nCompute avg wn = ws/L and vn = vs/L\n16:\nws = 0 and vs = 0\n17:\nif t > 0 then\n18:\nif L(wn) > ηL(wo) then\n19:\nClear (S, Y) and F buﬀers\n20:\nReset wk = wo and vk = vo\n21:\nUpdate µ = max(µ/φ, µmin)\n22:\ncontinue\n23:\nend if\n24:\ns = wn −wo\n25:\ny =\n1\n|F|(\n|F|P\ni=1\nFi · s)\n26:\nUpdate µ = min(µ · φ, µmax)\n27:\nif sTy > σ yTy then\n28:\nStore curvature pairs (s,y) in (S, Y)\n29:\nend if\n30:\nend if\n31:\nUpdate wo = wn and vo = vn\n32:\nt ←t + 1\n33:\nend if\n34:\nend for\n35: end for\nH(0)\nk\nis initialized based on the accumulated gradient infor-\nmation given as\n[H(0)\nk ]ii =\n1\nqPk\nj=0 ∇L(w j)2\ni + ǫ\n.\n(6)\naSNAQ uses an accumulated Fisher matrix aFIM for com-\nputing the curvature information pair (s, y) for the Hessian\ncomputation as shown in Eq. (7) and Eq. (8)\ns = wt −wt−1,\n(7)\ny = 1\n|F|\n|F|\nX\ni=1\nFi · s ,\n(8)\nwhere wt is the average aggregated weight, t is the cur-\nvature pair update counter, Fi = ∇L(wk+1)∇L(wk+1)T\nand |F| is the number of Fi entries present in F.\nThe\ny vector is computed without explicitly constructing the\n∇L(wk+1)∇L(wk+1)T matrix by just storing the ∇L(wk+1)\nvector. The use of the Fisher Information matrix (aFIM)\ngives a better estimate of the curvature of the problem. The\ncurvature pair information (s, y) computed based on the av-\nerage of the weight aggregates and Hessian-vector prod-\nuct reduces the eﬀect of noise and allows for more stable\napproximations. The curvature pairs are computed every\nL steps and stored in the (S, Y) buﬀer only if suﬃciently\nlarge. This allows for the updates being made only based\non useful curvature information. Further, the curvature pair\ninformation (s, y) and hence the Hessian approximation is\nupdated once in L iterations, thus reducing computational\ncost. The size of the (S, Y) buﬀer and aFIM buﬀer F are set\nto mL and mF respectively, thus optimizing storage cost.\n4. Global Routing\nVLSI physical design requires to compute the best phys-\nical layout of millions to billions of circuit components on\na tiny silicon surface (< 5cm2). It is carried out in sev-\neral stages such as partitioning, ﬂoor-planning, placement,\nrouting and timing-closure. In the placement stage, the lo-\ncations of the circuit components, i.e. cells, are determined.\nOnce all cell locations are set, the paths for all the connec-\ntions of the circuit, i.e. nets, are determined in the routing\nstage. Global routing involves a large and arbitrary number\nof nets to be routed, where each net may consist of many\npins to be interconnected with wires. In addition, the IC\ndesign consideration may impose several constraints such\nas number of wire crossings (capacity) and routing direc-\ntions, blockages, etc. The global routing problem can be\nmodelled as a grid maze with multiple start-goal pairs that\ncorrespond to the location of the pins to be routed. The ob-\njective of the router is to ﬁnd the optimum connections for\nall the pins (routing solutions) such that the total wirelength\nis minimum and no overﬂow occurs. An overﬂow is said\nto occur when the number of wirecrossings exceed the set\ncapacity for a particular edge. For every wire routed, the\ncorresponding capacity decreases. Routing is sequential\nand hence a common problem is net ordering. Nets routed\nearly can block the routes for later nets due to utilization\nof the capacity. The study proposed in [7] shows potential\nscope for reinforcement learning based global routing. In\nthis paper we evaluate the eﬃciency of our proposed algo-\nrithm on the reinforcement learning framework for global\nrouting. For each two-pin, the enviroment provides the Q-\nnetwork with 12 states as input and the output is the esti-\nmated action value for all 6 actions as described in [7]. A\nreward R(a, s′) of +100 is obtained if s′ is the target pin,\notherwise -1. Further we apply double DQN to enhance\nthe performance.\n5. Simulation Results\nThe performance of the proposed second-order Nes-\nterov’s accelerated quasi-Newton method is evaluated on\nsolving global routing. We use the architecture similar to\nthat in [7]. In order to further enhance the performance and\nstability we use double DQN. The neural network structure\nused is 12–32–64–32–6with ReLU activation. A batch size\nof 32 is chosen. In this paper we consider a two-layer 8x8\ngrid with a total of 50 nets and maximum two pins in each\nnet. The default capacity is set to 5 and the number of\nblockages is set to 3. A total of 15 benchmarks were gener-\nated using the open-sourced problem set generator [7]. The\nmaximum number of episodes was set to 500. An episode\nconstitutes a single pass over the entire set of pin pairs over\nall nets. The maximum steps is set to 50. The discount\nfactor γ is set to 0.9. We evaluate the performance of the\nproposed aSNAQ method in comparison with Adam and\nRMSprop. All hyperparameters are set to their default val-\nues. The performance metrics include the total wirelength\nand overﬂow. For all successful solutions i.e all nets routed\nwithin the maximum number of episodes, zero overﬂow\nwas obtained and the corresponding total wirelength was\ncalculated using the ISPD’08 contest evaluator. The A*\nsearch solution is set as the baseline for comparison of the\nperformance metrics. A summary of the results of the 15\nbenchmarks are shown in Table 1. The table shows the to-\nFigure 1: Variation of loss over episodes\nTable 1: Summary of the results on 15 random trials\nTrial\nA*\nAdam\nRMSprop\naSNAQ\nNum\nWL\nWL\ndiﬀ\nRbest\nE\nPins\nWL\ndiﬀ\nRbest\nE\nPins\nWL\ndiﬀ\nRbest\nE\nPins\n1\n390\n-\n-\n4386\n465\n48\n-\n-\n4363\n490\n48\n368\n-22\n4667\n231\n50\n2\n386\n-\n-\n4505\n399\n49\n-\n-\n4513\n483\n49\n376\n-10\n4610\n148\n50\n3\n379\n-\n-\n4234\n478\n47\n-\n-\n4533\n401\n49\n-\n-\n4382\n344\n48\n4\n369\n348\n-21\n4690\n288\n50\n350\n-19\n4685\n492\n50\n345\n-24\n4699\n75\n50\n5\n366\n362\n-4\n4679\n422\n50\n361\n-5\n4681\n430\n50\n369\n+3\n4656\n458\n50\n6\n352\n348\n-4\n4691\n437\n50\n344\n-8\n4697\n296\n50\n335\n-17\n4701\n157\n50\n7\n430\n-\n-\n4053\n485\n46\n-\n-\n4322\n393\n48\n-\n-\n4324\n285\n48\n8\n398\n-\n-\n4522\n205\n49\n-\n-\n4513\n455\n49\n377\n-21\n4663\n361\n50\n9\n369\n369\n0\n4669\n497\n50\n347\n-22\n4687\n252\n50\n348\n-21\n4693\n189\n50\n10\n366\n359\n-7\n4674\n112\n50\n375\n+9\n4660\n327\n50\n357\n-9\n4683\n480\n50\n11\n379\n380\n+1\n4660\n252\n50\n380\n+1\n4658\n429\n50\n-\n-\n4523\n428\n49\n12\n351\n346\n-5\n4692\n293\n50\n351\n0\n4689\n340\n50\n348\n-3\n4692\n93\n50\n13\n395\n411\n+16\n4616\n456\n50\n397\n+2\n4645\n422\n50\n394\n-1\n4640\n193\n50\n14\n340\n343\n+3\n4700\n409\n50\n338\n-2\n4706\n381\n50\n341\n+1\n4699\n49\n50\n15\n375\n374\n-1\n4659\n319\n50\n384\n9\n4660\n313\n50\n371\n-4\n4668\n490\n50\ntal wirelength (WL) if all pins were successfully routed.\nThe diﬀcolumn shows the amount of wirelength reduc-\ntion obtained in comparison to the baseline (A* solution)\nwirelength. Rbest indicates the best cummulative reward\nobtained and E is the corresponding episode and maximum\nnumber of pins successfully routed. From the table, it can\nbe observed that for 12 out 15 cases aSNAQ was successful\nin routing all the pins within 500 episodes while Adam and\nRMSprop were successful in only 10 out of 15. Further-\nmore, in most of the cases the routing solution obtained by\naSNAQ had signiﬁcant wirelength reduction compared to\nthe baseline and in fewer episodes compared to Adam and\nRMSprop. Fig. 1 shows the average loss over 500 episodes\nfor one of the benchmarks. It can be noted that aSNAQ has\nthe least average loss compared to Adam and RMSprop,\nthus indicating that aSNAQ is eﬀective in training the deep\nQ-Network.\n6. Conclusion\nFirst order gradient based methods are popular in train-\ning deep neural networks.\nIncorporating second order\ncurvature information such as the QN and NAQ methods\nhave shown to be eﬃcient in supervised models. This pa-\nper shows the feasibility and eﬃciency of the proposed\nstochastic Nesterov’s accelerated quasi-Newton (aSNAQ)\nmethod in deep reinforcement learning applications as\nwell. Further we apply the proposed algorithm in a deep\nreinforcement learning framework for global routing. To\nfurther enhance the performance, double DQN was used.\nThe results indicate that the DQNs trained using aSNAQ\nhad better routing solutions compared to those trained with\nAdam and RMSprop and with fewer training episodes. In\nfuture works, further analysis of the proposed algorithm on\nlarger netlists, nets with multipins and study on application\nto other problems will be studied.\nAcknowledgments\nThe authors thank H. Liao et. al. [7] for the publicly\navailable global routing problem set generator.\nReferences\n[1] R. S. Sutton and A. G. Barto, “Reinforcement Learn-\ning: An Introduction,” MIT Press, Cambridge, MA,\n1st edition, 1998\n[2] D. P. Kingma, and J. Ba,\n“Adam:\nA method\nfor\nstochastic\noptimization,”\narXiv\npreprint\narXiv:1412.6980, December 2014.\n[3] T. Tieleman and G. Hinton, “Lecture 6.5 - RMSProp,”\nCOURSERA Neural Networks for Machine Learning.\nTechnical report, 2012.\n[4] J. Nocedal and S.J. Wright, “Numerical Optimization\nSecond Edition”, Springer, 2006.\n[5] H. Ninomiya, “A novel quasi-newton-based optimiza-\ntion for neural network training incorporating nes-\nterov’s accelerated gradient,” NOLTA Journal, IEICE\nvol. 8, no.4, pp. 289–301, October 2017.\n[6] S. Indrapriyadarsini, S. Mahboubi, H. Ninomiya, and\nH. Asai,\nAn adaptive stochastic nesterov acceler-\nated quasi-Newton method for training RNNs. Proc.\nNOLTA’19, pp. 208–211, December 2019\n[7] H. Liao, W. Zhang, X. Dong, B. Poczos, K. Shimada,\nand L. Burak Kara, “A Deep Reinforcement Learning\nApproach for Global Routing,” Journal of Mechanical\nDesign, vol. 142 no. 6, June 2020.\n[8] H. Van Hasselt, A. Guez and D. Silver,\n“Deep Re-\ninforcement Learning with Double Q-learning,” 30th\nAAAI Conf. on Artiﬁcial Intelligence, March 2016\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-10-15",
  "updated": "2020-10-15"
}