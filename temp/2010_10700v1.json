{
  "id": "http://arxiv.org/abs/2010.10700v1",
  "title": "Geometry-based Occlusion-Aware Unsupervised Stereo Matching for Autonomous Driving",
  "authors": [
    "Liang Peng",
    "Dan Deng",
    "Deng Cai"
  ],
  "abstract": "Recently, there are emerging many stereo matching methods for autonomous\ndriving based on unsupervised learning. Most of them take advantage of\nreconstruction losses to remove dependency on disparity groundtruth. Occlusion\nhandling is a challenging problem in stereo matching, especially for\nunsupervised methods. Previous unsupervised methods failed to take full\nadvantage of geometry properties in occlusion handling. In this paper, we\nintroduce an effective way to detect occlusion regions and propose a novel\nunsupervised training strategy to deal with occlusion that only uses the\npredicted left disparity map, by making use of its geometry features in an\niterative way. In the training process, we regard the predicted left disparity\nmap as pseudo groundtruth and infer occluded regions using geometry features.\nThe resulting occlusion mask is then used in either training, post-processing,\nor both of them as guidance. Experiments show that our method could deal with\nthe occlusion problem effectively and significantly outperforms the other\nunsupervised methods for stereo matching. Moreover, our occlusion-aware\nstrategies can be extended to the other stereo methods conveniently and improve\ntheir performances.",
  "text": "1\nGeometry-based Occlusion-Aware Unsupervised\nStereo Matching for Autonomous Driving\nLiang Peng, Dan Deng, and Deng Cai\nAbstract—Recently, there are emerging many stereo matching\nmethods for autonomous driving based on unsupervised learning.\nMost of them take advantage of reconstruction losses to remove\ndependency on disparity groundtruth. Occlusion handling is a\nchallenging problem in stereo matching, especially for unsuper-\nvised methods. Previous unsupervised methods failed to take\nfull advantage of geometry properties in occlusion handling. In\nthis paper, we introduce an effective way to detect occlusion\nregions and propose a novel unsupervised training strategy to\ndeal with occlusion that only uses the predicted left disparity\nmap, by making use of its geometry features in an iterative\nway. In the training process, we regard the predicted left\ndisparity map as pseudo groundtruth and infer occluded regions\nusing geometry features. The resulting occlusion mask is then\nused in either training, post-processing, or both of them as\nguidance. Experiments show that our method could deal with\nthe occlusion problem effectively and signiﬁcantly outperforms\nthe other unsupervised methods for stereo matching. Moreover,\nour occlusion-aware strategies can be extended to the other stereo\nmethods conveniently and improve their performances.\nIndex Terms—Stereo matching, unsupervised learning, occlu-\nsion aware, autonomous driving.\nI. INTRODUCTION\nS\nTEREO matching is a classical problem in computer\nvision. It has been widely used in many ﬁelds, especially\nfor autonomous driving. Given a pair of rectiﬁed left and right\nimages, the displacement of corresponding matching pixels is\ncalled disparity. Depth can be derived by disparity according\nto triangulation.\nBeneﬁting from the development of Convolutional Neural\nNetworks(CNNs), many deep learning methods show amazing\nresults in stereo matching. However, most of these methods\nrequire massive high-quality groundtruth of disparities for\ntraining, which is expensive to obtain. And therefore, many\nunsupervised-learning-based methods have been proposed,\nmainly based on image reconstruction losses [1], [2], [3]. Such\nlosses evaluate the differences between the reconstructed left\nimage and the original left image via similarity metrics, e.g.,\nStructure Similarity (SSIM)[4], L1. The reconstruction process\nusually relies on the differentiable warping mechanism similar\nto Spatial Transformer Network (STN)[5]. Pixels on the left\nimage are recovered by bilinear sampling from the original\nright image according to the corresponding disparity values.\nHowever, in binocular settings, regions visible in one cam-\nera may be unseen in another. It is difﬁcult or even impossible\nto reconstruct such pixels in one image from the other.\nAnd therefore, the reconstruction losses calculated on these\npixels are meaningless and noisy, imposing a negative impact\non model performances, especially on occluded regions. As\nFig. 1: Comparison examples of disparity prediction results\nfrom KITTI 2015[6]. Regions in the dashed ellipses of the\nzoomed left image are occluded in the zoomed right image\nby the bicycler. Among the compared state-of-the-art unsuper-\nvised stereo methods[1], [3], ours performs the best, especially\non occluded regions. Best viewed in color.\nobserved in experiments, disparity images inferred by unsuper-\nvised methods tend to be blurred and indistinguishable from\ntheir surrounding areas in occlusion regions. Some examples\nare shown in Fig. 1. Recent unsupervised methods usually\nadopt the left-right consistency check[1], [2], [7], [3] or\na CNN[8] to detect or predict occlusions. These methods\nignore or fail to make full use of the geometry properties of\nocclusions.\nIn this paper, we propose Geometry-based Occlusion-Aware\nUnsupervised Stereo Matching(GOUSM), a novel occlusion-\naware method. The disparity prediction from the model being\ntrained is used as pseudo groundtruth, from which occluded\nregions can be located based on epipolar geometry constraints\nand represented as an occlusion mask. It is then applied in the\ntraining process to mask the reconstruction loss calculated in\noccluded regions. The whole process is carried out iteratively\nepoch by epoch, and the quality of disparity predictions and\nocclusion masks are improved gradually. The occlusion mask\nis also used in our post-processing to reﬁne predictions in\noccluded regions. Experiments show that, by taking advantage\nof occlusion geometry constraints, our method could deal with\nocclusion more effectively, and signiﬁcantly outperforms the\narXiv:2010.10700v1  [cs.CV]  21 Oct 2020\n2\n(a)\n(b)\nFig. 2: Occlusion illustration. The Left and Right are the two cameras in a typical binocular system. The Object is opaque to\nlight. (a): Deﬁnitions of different types of regions in a typical binocular system. (b): Illustration of the condition for a point\nto be occluded. O1, O2 are the optical centers of the cameras. d denotes disparity. P1 and P2 in the left image are mapped to\na same point of right image, so P1 is occluded by P2.\nother unsupervised stereo matching methods. Moreover, our\napproaches can be easily extended to other stereo methods\nand improve their performances.\nOur main contributions can be summarized as below:\n1) We propose a novel and effective geometry-based ap-\nproach to deal with the occlusion problem for unsuper-\nvised stereo matching.\n2) Our method can be adopted conveniently by the other\nstereo methods and helps to improve their performances.\n3) Our method achieves state-of-the-art performances, sig-\nniﬁcantly outperforming the other unsupervised learning\nmethods.\nII. RELATED WORK\nA. Supervised Stereo Matching\nEarly supervised methods focus on accurately computing\nthe matching cost with CNNs and reﬁning the disparity\nmap with semi-global matching (SGM)[9], such as [10],\n[11], [12]. Inspired by pixel-wise labeling tasks, Mayer et\nal.[13] developed end-to-end learning of disparity maps by\nusing Fully-Convolution Network(FCN[14]). Chang proposed\nPSMNet[15], getting much better results by taking advantage\nof pyramid spatial pooling and hourglass network, which are\ntime-consuming. To achieve real-time inference, Khamis et al.\nproposed StereoNet[16], with small network architecture and\nhigh speed. Most supervised methods [13], [15], [17], [18],\n[19], [20], [21] build cost volumes by concatenation or multi-\nplication. GwcNet[22] is proposed to calculate cost volume by\ngroup-wise correlation to balance speed and accuracy. High-\nquality groundtruth is indispensable for supervised methods.\nHowever, it’s expensive to yield enough.\nB. Unsupervised Stereo Matching\nUnsupervised stereo matching is becoming popular recently\nbecause it does not need groundtruth. Chao et al.[2] proposed\nan iterative unsupervised learning network that uses left and\nright checks to select the appropriate point matching pair,\nwhich is then cycled a certain number of times to optimize.\nGodard et al.[1] proposed the reconstruction mechanism for\nunsupervised stereo matching, and has been followed by most\nlater unsupervised stereo methods. UnOS[3] employs both\nconsequent frames and stereo pairs to jointly learn optical ﬂow\nand disparity maps. Some work takes the unsupervised loss to\nget better generalization, such as MADNet[23]. It uses the\nunsupervised loss for online adaptation.\nC. Occlusion-Aware Methods\nTo tackle occlusions in stereo matching, Fua[24] created\ntwo disparity maps relative to each image: one from left to\nright and another from right to left to check their consistency,\ni.e.,left-right consistency check. Many later methods[25], [1],\n[3] follow this strategy. By exploiting the symmetric property\nof optical ﬂow[26], [27], this check can also be extended\nto optical ﬂow for occlusion handling. Zitnick[28] examines\nthe magnitude of the converged match values in conjunction\nwith the uniqueness constraint, and Luo et al.[29] rely on\ncontinuity. Li et al.[8] detect occlusions depend on direct\nmodel inference, i.e., predicting an occlusion mask by adding\nspecial submodules in the CNN model.\n3\n(a) Input left\n(b) Input right\n(c) groundtruth left disparity\n(d) Occlusion mask\n(e) Reconstructed left image, without deal-\ning with occlusion\n(f) Reconstructed left image after dealing\nwith occlusion\n(g) Reconstruction error of (e)\n(h) Reconstruction error of (f)\nFig. 3: Illustration of the inﬂuences of occlusion in the reconstruction process. The occlusion mask is derived only from left\ndisparity. The black regions represents occlusion in (d). In the error images (g) and (h), brighter color mean larger error. It\ncan be seen that reconstruction without being occlusion-aware produces a high error in occlusion regions.\nIII. OCCLUSION ANALYSIS\nA. Occlusion Deﬁnition and Detection\nBefore elaborating on the whole pipeline, we will show how\nto detect occluded regions by geometry and their inﬂuences\non model training. Some deﬁnitions are declared at ﬁrst for a\nbetter explanation. As illustrated in Fig. 2(a),\n1) Common Regions, the intersection of left and right\nviewing frustums.\n2) Exclusive Regions, the regions not in Common Regions.\n3) Occluded Regions, the regions in Common Regions only\nvisible in one camera.\nAll occlusions we mentioned in the following sections refer\nto occluded regions of the left camera, i.e., visible in left but\nunseen from right, if not speciﬁed.\nThen, we will derive the condition for a given point to be\noccluded. As is illustrated in Fig. 2(b), O1 = (0, 0, 0), O2 =\n(b, 0, 0) are the optical centers of left and right cameras in a\nbinocular system after rectiﬁcation[30], respectively. The ”Ob-\nject” is opaque to light. P1 is a 3D point on the object surface.\nP2 is the crosspoint where straight line P1O2 intersects with\nthe object surface. In the imaging process, P1 and P2 are both\nvisible in the left image. However, since P1 is occluded by\nP2, it is invisible from the right.\nDenote the projection points of P1, P2 on the left image as\np1 = (u1, v1),\n(1)\np2 = (u2, v2),\n(2)\nand their disparities as d1, d2, respectively. Their projection\npoints on the right image can be written as\np′\n1 = (u1 −d1, v1),\n(3)\np′\n2 = (u2 −d2, v2).\n(4)\nBecause P1, P2, O2 are all in the same straight line, P1 and\nP2 are mapped to the same pixel on the right image, then\nu1 −d1 = u2 −d2,\n(5)\nv1 = v2.\n(6)\nLet ∆u = u2 −u1, ∆d = d2 −d1, we get ∆u = ∆d for the\noccluded point P1.\nIt can be further concluded that for a given pixel p1 in\nthe left image, it belongs to Left-Occluded Regions when\nthere exists another pixel p2 satisfying\n∆u = ∆d > 0.\n(7)\nFurthermore, if\nd1 −u1 > 0,\n(8)\np1 belongs to Left-Exclusive Regions since it is already\nbeyond the FOV (Field Of View) of the right camera.\nGiven a disparity image, occluded pixels can be located via\nEq. 7 and Eq. 8, and represented as a mask, namely, occlusion\nmask.\nB. The Inﬂuences of Occlusion on Unsupervised Stereo Meth-\nods\nIn binocular systems, occluded regions exist extensively.\nUnsupervised stereo matching methods mainly depend on\nlosses calculated from reconstruction in training. However,\ngiven the disparity and right images, it is difﬁcult or even\nimpossible to reconstruct the occluded regions since they are\ninvisible on the right image. And therefore, the reconstruction\nlosses calculated on these pixels are meaningless and noisy,\nimposing a negative impact on model performances, especially\non occluded regions.\nAs is shown in Tab. I and Fig. 3, occluded regions occupy\nabout 20% on left images from SceneFlow[13], in average.\nThe total L1 reconstruction error from occlusion regions is\n2.32 times as that on non-occlusion. Consequently, the impact\n4\nTABLE I: Occlusion statistical on SceneFlow[13]. SceneFlow\nis a synthesized dataset containing 35,454 pairs of stereo\nimages with precise and dense disparity groundtruth. L1 error\nis calculated on the whole dataset. Mean Error: mean L1 error\nper pixel; Total error: the proportion of error from all the pixels\nfrom different regions w.r.t. the total error from all regions;\nArea: the proportion of the areas of different regions w.r.t.\ntotal area of all regions.\nMean Error\nTotal Error\nArea\nOccluded\n39.78\n69.88%\n16.86%\nNon-Occluded\n3.47\n30.12%\n83.14%\nOccluded\nNon−occluded\n11.46\n2.32\n0.20\nFig. 4: Qualitative results of GOAT and GOAPP. By employing\nGOAT and GOAPP, it can be observed that the result is\ngradually less affected by occlusions and retain more details.\nBest viewed in color.\nof occlusion is too signiﬁcant to be ignored for unsupervised\nstereo matching methods. As shown in Fig. 1, disparity images\ninferred by the state-of-the-art unsupervised methods tend to\nbe blurred and indistinguishable from their surrounding pixels\nin occluded regions.\nIV. GEOMETRY-BASED OCCLUSION-AWARE\nUNSUPERVISED STEREO MATCHING\nA. Occlusion-Aware Pipeline\n1) Gometry-based Occlusion-Aware Training (GOAT): Oc-\nclusions would impose a negative impact on unsupervised\nstereo matching methods if not properly processed during\ntraining. A basic and intuitive solution is to ﬁnd the occluded\npixels via Eq. 7 and Eq. 8, then ignore them in loss calculation.\nHowever, groundtruths of disparities are supposed to be absent\nin unsupervised training, while accurate occlusion detection\nrelies on precise disparity and vice versa. In a word, accurate\nocclusion ﬁrst or accurate disparity ﬁrst is a chicken-and-\negg problem. Our method deals with this contradiction in an\niterative way. We use disparity predictions to ﬁnd occlusions\nand ignore them in the training process. As is listed in Alg.1,\nafter the ﬁrst epoch trained on all pixels, occlusion masks are\nupdated ofﬂine after each epoch, instead of each iteration, to\nmake the training more stable and efﬁcient in convergence. At\nﬁrst, the predicted disparities and occlusion masks might be\nnoisy, but their precisions would increase as the training goes\non.\nAlgorithm 1 GOAT:Geometry-based Occlusion-Aware Train-\ning\n1: Initial model parameters\n2: Train on all pixels in the 1st epoch\n3: for i ←from 2 to total epochs do\n4:\nUpdate occlusion masks of all training samples\n5:\nTrain without occluded regions\n6: end for\n7: Obtain the ﬁnal model\n2) Geometry-based\nOcclusion-Aware\nPost\nProcessing\n(GOAPP): We believe that disparity predictions for occluded\nregions tend to be more unreliable than those for non-\nocclusions, no matter GOAT is adopted or not. And therefore,\nthe occlusion mask is then used in post-processing to reﬁne\npredictions for occluded pixels. Different type occlusion\nregions are dealt using different ways via Alg.2. The basic\nidea behind is re-calculating the disparities of occluded pixels\nfrom\ntheir\nnon-occluded\nneighbors,\nby\nnearest-neighbor\nsampling and linear interpolation. N\nis the number of\nneighbors used in sampling and interpolation and is set to 10\nin default. It’s worth noting that GOAPP is much different\nfrom the usual post-processing applied in previous methods.\nOn the one hand, GOAPP requires only a single predicted\ndisparity map to detect occluded regions, nevertheless, other\nmethods usually need to obtain both left and right disparity\nmaps or a CNN, limiting their application scenarios. On the\nother hand, we adopt different strategies to handle different\ntypes of occlusions to be tailored to ﬁt better into the scenes\nof autonomous driving.\nB. Network Details.\nOur proposed occlusion-aware training method can be ap-\nplied in most stereo methods that make use of reconstruction\nloss, and the post-processing strategy can be used in any stereo\n5\nFig. 5: Our baseline network architecture. The red dot denotes concatenation opreation.\nTABLE II: Network architecture.\nLayer\nKernel\nStride\nChannels\nIn\nOut\nInput\nconv1\n5 × 5\n2\n3/32\n1\n2\nStereo-\n-images\nconv2\n5 × 5\n2\n32/32\n2\n4\nconv1\nconv3\n5 × 5\n2\n32/32\n4\n8\nconv2\nres1\n3 × 3\n1\n32/32\n8\n8\nconv3\nres2\n3 × 3\n1\n32/32\n8\n8\nres1\nres3\n3 × 3\n1\n32/32\n8\n8\nres2\nres4\n3 × 3\n1\n32/32\n8\n8\nres3\nres5\n3 × 3\n1\n32/32\n8\n8\nres4\nres6\n3 × 3\n1\n32/32\n8\n8\nres5\nconv4\n3 × 3\n1\n32/32\n8\n8\nres6\n3dconv1\n3 × 3 × 3\n1\n32/32\n8\n8\ncost\n3dBN1\n-\n1\n32/32\n8\n8\n3dconv1\n3dconv2\n3 × 3 × 3\n1\n32/32\n8\n8\n3dBN1\n3dBN2\n-\n1\n32/32\n8\n8\n3dconv2\n3dconv3\n3 × 3 × 3\n1\n32/32\n8\n8\n3dBN2\n3dBN3\n-\n1\n32/32\n8\n8\n3dconv3\n3dconv4\n3 × 3 × 3\n1\n32/32\n8\n8\n3dBN3\n3dBN4\n-\n1\n32/32\n8\n8\n3dconv4\n3dconv5\n3 × 3 × 3\n1\n32/1\n8\n8\n3dBN4\nbillnear1\n-\n-\n32/32\n8\n4\ncoarse1\nconcat1\n-\n-\n-/65\n4\n4\nbillnear1,\nconv2,\nleft image\natrous1\n3 × 3\n1\n65/1\n4\n4\nconcat1\nbillnear2\n-\n-\n32/32\n4\n2\natrous1\nconcat2\n-\n-\n-/65\n2\n2\nbillnear2,\nconv1,\nleft image\natrous2\n3 × 3\n1\n65/32\n2\n2\nconcat2\nbillnear3\n-\n-\n32/32\n2\n1\natrous2\nconcat3\n-\n-\n-/35\n1\n1\nbillnear3,\nleft image\natrous3\n3 × 3\n1\n65/32\n1\n1\nconcat3\nconv5\n3 × 3\n1\n32/1\n1\n1\natrous3\nrelu1\n-\n-\n1/1\n1\n1\nconv5\nAlgorithm 2 GOAPP: Geometry-based Occlusion-Aware Post\nProcessing\nRequire: Occlusion mask M\nRequire: Disparity image D\n1: for i ←from 0 to NumRows(D) do\n2:\nﬁnd k satisfying: All(Mi,j<k)=0 and Mi,k+1 =1\n3:\nfor j ←from k to 0 do\n4:\nDi,j ←1\nN\nP1\nm=N+1 M(i, j + m)\n5:\nend for\n6:\nfor j ←from k to NumCols(D) do\n7:\nDi,j ←1\nN\nP−1\nm=−N−1 M(i, j + m)\n8:\nend for\n9: end for\n10: Return D\nTABLE III: Atrous block.\nLayer\nDilate rate\nChannels\nIn\nOut\nInput\nconv1\n-\n65/32\n-\n-\ninput featrues\natrous res1\n1\n32/32\n-\n-\nconv1\natrous res2\n2\n32/32\n-\n-\natrous res1\natrous res3\n4\n32/32\n-\n-\natrous res2\natrous res4\n8\n32/32\n-\n-\natrous res3\natrous res5\n1\n32/32\n-\n-\natrous res4\natrous res6\n1\n32/32\n-\n-\natrous res5\nconv2\n-\n32/32\n-\n-\natrous res5\nrelu1\n-\n32/32\n-\n-\nconv2\nmethods with dense disparity output. However, a speciﬁc de-\nsign of network architecture and loss calculation is introduced,\nfor the convenience of explanation and experiments.\nAs shown in Tab. II and Fig. 5, our network, inspired by\nStereoNet[16], is a U-net like architecture, which contains sub-\nsampling layers and skip connection layers. Following previ-\nous methods, Siamese architecture that each branch processes\nthe left and right images is employed to gain image features\n6\nTABLE IV: Quantitative results on KITTI 2012 and 2015[31] validation set.\nApproaches\nDataset\nLower is better\nHigher is better\nAbs Rel\nSq Rel\nRMSE\nRMSE log\nD1-all\nδ < 1.25\nδ < 1.252\nδ < 1.253\nOASM-Net [8]\nKITTI\n2012\n\\\n\\\n\\\n\\\n6.690%\n\\\n\\\n\\\nGodard et al[1]\n0.049\n0.356\n2.525\n0.109\n8.771%\n0.959\n0.985\n0.993\nGOUSM(Ours)\n0.034\n0.272\n2.126\n0.089\n5.526%\n0.975\n0.990\n0.995\nZhou et al.[2]\nKITTI\n2015\n\\\n\\\n\\\n\\\n8.35%\n\\\n\\\n\\\nSegStereo[7]\n\\\n\\\n\\\n\\\n7.70%\n\\\n\\\n\\\nLuo et al.[29]\n\\\n\\\n\\\n\\\n6.310%\n\\\n\\\n\\\nOASM-Net [8]\n\\\n\\\n\\\n\\\n6.650%\n\\\n\\\n\\\nTeng et al.[26]\n0.149\n1.223\n5.732\n0.225\n\\\n0.829\n0.944\n0.978\nGodard et al[1]\n0.068\n0.835\n4.392\n0.146\n9.194%\n0.942\n0.978\n0.989\nUnOS [3]\n0.060\n0.833\n4.187\n0.135\n7.073%\n0.955\n0.981\n0.990\nGOUSM(Ours)\n0.054\n0.739\n3.898\n0.124\n5.530%\n0.966\n0.985\n0.992\nand discrimination along the disparity range. Speciﬁcally, left\nand right images as input and are passed through 3 convolution\nlayers with stride 2 for downsampling, and then 6 resnet\nblocks are used to learn deep features. At the end of them,\na convolution layer is appended without any activation. Then,\nimage features of left and right are fed to build cost volume\nwhich is as described in [22]. The cost volume is passed to 3D\nconvolution blocks that each one contains a 3D convolution\nlayer, batch normalization, and a leaky ReLU activation to\nexploit the spatial relationship of disparity so that the coarse\ndisparity map is derived. To further reﬁne the disparity, we take\nadvantage of a reﬁnement module similar to [16], i.e., task the\nreﬁnement module of only ﬁnding a residual to add or subtract\nfrom the coarse prediction. The output is then passed through\n3 reﬁne parts. Each part consists of image resizing, feature\nconcat, and an atrous block. Coarse disparity map bilinearly\nupsampled as well as the original left RGB image resized\nand the previous feature jointly concat to be formed as the\ninput of atrous block. This block mainly contains 6 residual\nblocks. Every block employs 3 × 3 atrous convolutions, batch-\nnormalization, and a leaky ReLu activation, and its output is a\n1-dimensional disparity residual map that is then added to the\nprevious prediction, Details of the atrous block can be found\nin Tab. III.\nC. Loss Calculation\nA weighted sum of reconstruction loss and smooth loss is\ncalculated for every pixel. Then, the occlusion mask is applied\nto exclude the occluded pixels and the ﬁnal loss is the average\nvalue of all kept pixel-wise losses.\n1) Reconstruction Loss: The left image is reconstructed\nby sampling from the right image according to predicted\ndisparities, the same as the method used in [1]. Structure\nSimilarity loss (SSIM)[4] and L1 loss are used to measure\nthe reconstruction quality,\nCr\ni,j = α1−SSIM(Il\ni,j, eIl\ni,j)\n2\n+(1−α)\n\r\r\r(Il\ni,j−eIl\ni,j)\n\r\r\r,\n(9)\nwhere Il\ni,j and eIl\ni,j stand for the intensity values on the original\nand recovered left image at location (i, j), respectively; α is\nthe weight and 0 ≤α ≤1.\nThe ﬁnal pixel-wise reconstruction loss is an aggregation\nin different directions using Adaptive Support Weights(ASW)\nproposed in [32]. Cost Car\ni,j aggregated in a window of size\n2k × 2k is:\nCar\ni,j =\nPi+k−1\nx=i−k\nPj+k−1\ny=j−k wx,yCr\nx,y\nPi+k−1\nx=i−k\nPj+k−1\ny=j−k wx,y\n,\n(10)\nwhere wx,y\n=\ne−\nIi,j −Ix,y\n2\nand Ii,j denotes intensity of\npixel(i, j) in input left image.\n2) Smooth Loss: Edge-aware smooth loss[1] is used as a\nregularization term, to make the prediction locally smooth. The\nsmooth loss on pixel (i, j) is\nCs\ni,j = 1\nN\nX \f\f∂xdl\ni,j\n\f\f e−∥∂xIl\ni,j∥+\n\f\f∂ydl\ni,j\n\f\f e−∥∂yIl\ni,j∥\n(11)\nwhere ∂d, ∂I denote the gradients of disparity and intensity,\nrespectively.\n3) Occlusion-Aware Loss: The total loss on pixel (i, j) is\nCi,j = w1Car\ni,j + w2Cs\ni,j,\n(12)\nwhere w1, w2 are the weights.\nThe overall loss on the whole image is\nC =\n1\nP\ni,j M(i, j)\nX\ni,j\nM(i, j)Ci,j,\n(13)\nwhere M is the occlusion mask, where values for occluded\npixels are zeros.\nIn this way, occlusions are excluded in loss calculation.\nV. EXPERIMENTS\nA. Dataset and Evaluation Metrics\nKITTI[31] raw data covers 61 scenes, containing 42,382\npairs of rectiﬁed stereo images without disparity groundtruth.\n7\nFig. 6: Qualitative comparison of different methods on KITTI 2015. Ours shows the smoothest results and gives the cleanest\npredictions around edges, where occluded regions usually exist.\nTABLE V: Ablation study. Neither GOAT nor GOAPP is used in the baseline.\nApproaches\nLower is better\nHigher is better\nAbs Rel\nSq Rel\nRMSE\nRMSE log\nD1-all\nδ < 1.25\nδ < 1.252\nδ < 1.253\nBaseline\n0.059\n0.857\n4.132\n0.135\n6.834%\n0.958\n0.980\n0.989\nBaseline+GOAPP\n0.059\n0.673\n3.902\n0.128\n6.410%\n0.960\n0.983\n0.991\nBaseline+GOAT\n0.056\n0.768\n4.032\n0.131\n6.047%\n0.962\n0.982\n0.990\nGOUSM(Baseline+\nGOAT+GOAPP)\n0.054\n0.739\n3.898\n0.124\n5.530%\n0.966\n0.985\n0.992\nTABLE VI: Quantitative results on the FlyingThings3D test\nset.\nApproaches\nSupervised\nEPE\n3px-error\nPSMNet[15]\nYes\n1.09\n\\\nGC-Net Full[18]\nYes\n2.51\n9.3%\nGC-Net Fast[18]\nYes\n7.27\n24.2%\nSGM[9]\nNo\n8.70\n\\\nGodard et al.[1]\nNo\n8.28\n28.7%\nGOUSM(Ours)\nNo\n6.22\n16.2%\nKITTI 2015[6] and KITTI 2012 is the most widely used\ndataset for supervised stereo matching. It provides 200 and 194\npairs of stereo images with high-quality disparity groundtruth,\nrespectively. Furthermore, some experiments have been con-\nducted on SceneFlow[13], in which FlyingThings3D is its\nlargest subset, with 21, 818 and 4,248 frames in its training\nand test set respectively, and Cityscapes[33] is another popular\ndataset for autonomous driving that contains many scenes in\nthe city.\nStandard depth metrics from [34] and disparity metric D1-\nall from [31] are used to measure and compare performances\nof different methods. For depth metrics, maximum predictions\nfrom models are capped to 80 meters, to keep pace with the\nother literatures[1], [3], [2], [7]. Depth groundtruth is derived\nfrom disparity, and the error calculated in depth space is more\nsensitive for small disparity values since disparity is in inverse\nproportion to depth.\nSome commonly used depth metrics are listed as follows:\n• Threshold: % of yi s.t. max( yi\ny∗\ni , y∗\ni\nyi ) = δ < thr\n• Absolute\nRelative\nDifference\n(Abs\nRel):\n1\n|T |\nP\ny∈T |y −y∗| /y∗,\n• Squared\nRelative\nDifference\n(Sq\nRel),\n: 1\n|T |\nP\ny∈T ||y −y∗||2 /y∗\n• Root\nof\nMean\nSqure\nError(RMSE),\nq\n1\n|T |\nP\ny∈T ||yi −y∗\ni ||2,\n• RMSE(log):\nq\n1\n|T |\nP\ny∈T ||logyi −logy∗\ni ||2\n8\nTABLE VII: Quantitative results of GOAPP extended to Godard et al.[1] on the KITTI 2015 validation set. ’All’ in the table\nrepresents the measured area from KITTI, where ’No’ refers to regions for which the matching correspondence is inside the\nimage domain, ’Yes’ refers to all image regions for which groundtruth could be measured, including regions which map to\npoints outside the image domain in the other view.\nApproaches\nAll\nLower is better\nHigher is better\nAbs Rel\nSq Rel\nRMSE\nRMSE log\nD1-all\nδ < 1.25\nδ < 1.252\nδ < 1.253\nGodard et al.\nNo\n0.068\n0.835\n4.392\n0.146\n9.194%\n0.942\n0.978\n0.989\nGodard et al.+\nGOAPP\nNo\n0.068\n0.914\n4.364\n0.140\n8.618%\n0.947\n0.981\n0.990\nGodard et al.\nYes\n0.102\n2.280\n5.728\n0.202\n10.755%\n0.928\n0.966\n0.980\nGodard et al.+\nGOAPP\nYes\n0.069\n0.923\n4.368\n0.142\n8.956%\n0.946\n0.980\n0.990\nFig. 7: Qualitative comparison of different methods on FlyingThings3D. Best viewed in color.\nB. Implementation Details\nThe\nwhole\npipeline\nis\nimplemented\nin\nPython\nand\nTensorﬂow[35]. RMSProp[36] is used for optimization with an\nexponentially-decaying learning rate initially set to 0.001. The\nbatch size is set to 1. In loss calculation, the values of w1, w2,\nα, k are set to 0.85, 0.15, 0.8, 16, respectively. Following other\nunsupervised methods, our model is trained on the remaining\n30k stereo samples from KITTI raw data excluding scenes of\nKITTI 2015, for about 20 epochs. The whole training process\ntakes about 2 days on an Nvidia 1080Ti GPU.\nC. Results on KITTI\nAs is shown in Tab. IV, our occlusion-aware approach\nperforms better than the other state-of-the-art unsupervised\nmethods, proving the effectiveness of our method. Most meth-\nods in the table deal with occlusion using left-right consistency\ncheck, i.e.,[1], [2], [7], [3], [29] , or direct CNN prediction,\ni.e., OASM-Net[8]. Fig. 6 compares the disparity outputs\nqualitatively in KITTI 2015 validation set. Compared with\ntheir outputs, the disparities from our method tend to have\nmore smooth and clearer near edges, where occluded regions\nusually exist.\n9\nFig. 8: Qualitative results in Cityscapes. Our approach outperforms other methods. Even for ill-posed regions such as glass,\nwe still get promising results and preserve many details. Best viewed in color.\nFig. 9: Disparity maps and occlusion masks during training from different epochs. Black in occlusion masks refers to occlusions.\nAs the training goes on, the qualities of both disparities and occlusions increase gradually. Best viewed in color.\n10\nFig. 10: Qualitative results of extending GOAPP to Godard\net al.[1]. Occlusion regions tend to be blurred in the original\nmethod, and become clear after GOAPP. From top to bottom:\nleft image, Godard et al., Godard et al.+GOAPP. Best viewed\nin color.\nD. Results on SceneFlow\nFig. 7 and Tab. VI shows the results on SceneFlow. Since\nFlyingThings3D is a large-scale synthesized dataset, and all\nsamples are generated with precise dense groundtruths, the\nstate-of-the-art supervised methods behave much better than\nunsupervised methods. However, the yield of such full and\nhigh-quality dense groundtruth is usually impossible in real-\nworld scenarios. Most stereo data from the real-world is born\nwithout groundtruth and is suitable for unsupervised methods\nonly. Our method performs the best among all compared\nunsupervised methods.\nE. Ablation Experiments\nTo show the contribution of being occlusion-aware during\ntraining and post-processing, some ablation experiments are\nconducted, and the results are shown in Tab. V. The adoption\nof GOAT contributes to an improvement of about 0.8% in D1-\nall, the performance already a litter better than the other state-\nof-the-art methods shown in Tab. IV, and GOAT + GOAPP\ngives an improvement of 1.3%. It means the relative error w.r.t.\nD1-all has been reduced by 11.7%, 19.1%, respectively.\nF. Extending to the other methods\nAs has been mentioned in Sec.4.1, our proposed occlusion-\naware pipeline can be easily extended to the other stereo\nmatching methods, by adopting the GOAT or GOAPP, or both\nof them. Two experiments as application examples have been\nconducted to demonstrate the advantages of occlusion-aware\nstrategies in stereo matching.\nTABLE VIII: Quantitative results of extending GOAT to\nsupervised stereo methods, by providing pre-trained models.\nKITTI 2015 dataset is randomly split into training and val-\nidation subset, containing 160 and 40 samples, respectively.\nPSMNet[15] and GwcNet[22] are trained with their public\ncode using our batch size.\nApproaches\nTraining\nmode\nD1-all\nEPE (pixels)\nTimes (ms)\nStereoNet[16]\nSupervised\n5.827%\n1.214\n85\nCombined\n4.058%\n0.946\nPSMNet[15]\nSupervised\n3.454%\n0.918\n410\nCombined\n3.193%\n0.854\nGwcNet[22]\nSupervised\n2.825%\n0.821\n330\nCombined\n2.651%\n0.767\nDense disparity prediction is the only requirement for\nthe adoption of GOAPP. Tab. VII and Fig. 10 show the\nresults of Godard’s[1] method after adding GOAPP as the\nlast post-processing step. When only considering the results\nfrom common regions, GOAPP brings improvements for most\nmetrics. After taking all regions into consideration, including\nthe exclusive regions on left images, the improvements are\nquite obvious. It means that GOAPP does work in removing\noutputs of low-quality in post-processing.\nMost supervised stereo methods need pretraining on the syn-\nthetic dataset SceneFlow[13] because the size of an available\nreal-scene dataset with high-quality groundtruths is far from\nlarge enough. For example, KITTI provides only hundreds of\ndata samples with high groundtruth, and tens of thousands of\nsamples in its raw data, however, without groundtruth. And\ntherefore, GOAT can provide help to supervised methods by\nproviding high-quality pre-trained models and avoiding the\ndomain shift from pretraining to ﬁnetuning. Tab. VIII shows\nthe results of such combinations. All the listed methods yield\nimprovements.\nG.\nGeneralize to other datasets\nIn this section, we show the generalization ability of our\napproach and illustrate some examples from Cityscapes. Our\nmodel is trained on KITTI and conducts inferences on images\nfrom Cityscapes[33] without ﬁnetuning. Some results are\nshown in Fig. 8. Despite the fact that our model has not been\ntrained on Cityscapes, our method still shows good results\nand performs better than the other approaches. It’s a strong\nimplication of the generalization ability and robustness of our\napproach, even for the scenes never seen, or image data from\ndifferent cameras with different parameters.\nH. The gradual improvement of disparity and occlusion mask\nAs shown in Fig. 9, in early epochs, the quality of disparities\ntends to be poor, so as the resulting occlusion masks. However,\ntheir qualities keep improving as the training goes on. This\nprocess implies that our proposed method, i.e., detecting\nocclusion from predicted disparity directly and iteratively, is\nworking well.\n11\nVI. CONCLUSION\nIn this paper, we point out that occlusion imposes negative\ninﬂuences on unsupervised stereo matching methods because\nthe reconstruction losses from occluded regions are mean-\ningless and harmful for model training. A novel geometry-\nbased method is proposed to detect occluded regions, resulting\nin an occlusion mask. It can then be used in training and\npost-processing iteratively to deal with occlusions. A speciﬁc\ndesign of network architecture and loss calculation is intro-\nduced for the convenience of explanation and experiments. It\nis then shown by experiments that this occlusion-aware design\ncould achieve better performances than the other unsupervised\nmethods, without or with different strategies of occlusion\nhandling. What’s more, it’s worth noting that the proposed\nocclusion-aware strategies, i.e., GOAT and GOAPP, can be\neasily extended to the other stereo methods and achieve\nperformance gains. Three extending approaches have been\nexperimented in this paper. However, many other possible\nways to make use of Occlusion-Awareness can be explored\nin the future.\nREFERENCES\n[1] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular\ndepth estimation with left-right consistency,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2017, pp. 270–\n279.\n[2] C. Zhou, H. Zhang, X. Shen, and J. Jia, “Unsupervised learning of\nstereo matching,” in Proceedings of the IEEE International Conference\non Computer Vision, 2017, pp. 1567–1575.\n[3] Y. Wang, P. Wang, Z. Yang, C. Luo, Y. Yang, and W. Xu, “Unos: Uni-\nﬁed unsupervised optical-ﬂow and stereo-depth estimation by watching\nvideos,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2019, pp. 8071–8081.\n[4] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli et al., “Image\nquality assessment: from error visibility to structural similarity,” IEEE\ntransactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.\n[5] M. Jaderberg, K. Simonyan, A. Zisserman et al., “Spatial transformer\nnetworks,” in Advances in neural information processing systems, 2015,\npp. 2017–2025.\n[6] M. Menze and A. Geiger, “Object scene ﬂow for autonomous vehicles,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015, pp. 3061–3070.\n[7] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, “Segstereo: Exploiting\nsemantic information for disparity estimation,” in Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2018, pp. 636–651.\n[8] A. Li and Z. Yuan, “Occlusion aware stereo matching via coopera-\ntive unsupervised learning,” in Asian Conference on Computer Vision.\nSpringer, 2018, pp. 197–213.\n[9] H. Hirschmuller, “Stereo processing by semiglobal matching and mu-\ntual information,” IEEE Transactions on pattern analysis and machine\nintelligence, vol. 30, no. 2, pp. 328–341, 2007.\n[10] J. Zbontar, Y. LeCun et al., “Stereo matching by training a convolutional\nneural network to compare image patches.” Journal of Machine Learning\nResearch, vol. 17, no. 1-32, p. 2, 2016.\n[11] W. Luo, A. G. Schwing, and R. Urtasun, “Efﬁcient deep learning for\nstereo matching,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2016, pp. 5695–5703.\n[12] A. Shaked and L. Wolf, “Improved stereo matching with constant\nhighway networks and reﬂective conﬁdence learning,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition,\n2017, pp. 4641–4650.\n[13] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and\nT. Brox, “A large dataset to train convolutional networks for disparity,\noptical ﬂow, and scene ﬂow estimation,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2016, pp.\n4040–4048.\n[14] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2015, pp. 3431–3440.\n[15] J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching network,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 5410–5418.\n[16] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, and\nS. Izadi, “Stereonet: Guided hierarchical reﬁnement for real-time edge-\naware depth prediction,” in Proceedings of the European Conference on\nComputer Vision (ECCV), 2018, pp. 573–590.\n[17] L. Yu, Y. Wang, Y. Wu, and Y. Jia, “Deep stereo matching with explicit\ncost aggregation sub-architecture,” in Thirty-Second AAAI Conference\non Artiﬁcial Intelligence, 2018.\n[18] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy,\nA. Bachrach, and A. Bry, “End-to-end learning of geometry and context\nfor deep stereo regression,” in Proceedings of the IEEE International\nConference on Computer Vision, 2017, pp. 66–75.\n[19] X. Song, X. Zhao, H. Hu, and L. Fang, “Edgestereo: A context integrated\nresidual pyramid network for stereo matching,” in Asian Conference on\nComputer Vision.\nSpringer, 2018, pp. 20–35.\n[20] Z. Liang, Y. Feng, Y. Guo, H. Liu, W. Chen, L. Qiao, L. Zhou, and\nJ. Zhang, “Learning for disparity estimation through feature constancy,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 2811–2820.\n[21] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade residual\nlearning: A two-stage convolutional neural network for stereo matching,”\nin Proceedings of the IEEE International Conference on Computer\nVision, 2017, pp. 887–895.\n[22] X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, “Group-wise correlation\nstereo network,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2019, pp. 3273–3282.\n[23] A. Tonioni, F. Tosi, M. Poggi, S. Mattoccia, and L. D. Stefano, “Real-\ntime self-adaptive deep stereo,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp. 195–204.\n[24] P. Fua, “A parallel stereo algorithm that produces dense depth maps\nand preserves image features,” Machine vision and applications, vol. 6,\nno. 1, pp. 35–49, 1993.\n[25] J. Sun, Y. Li, S. B. Kang, and H.-Y. Shum, “Symmetric stereo matching\nfor occlusion handling,” in 2005 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’05), vol. 2.\nIEEE,\n2005, pp. 399–406.\n[26] Q. Teng, Y. Chen, and C. Huang, “Occlusion-aware unsupervised learn-\ning of monocular depth, optical ﬂow and camera pose with geometric\nconstraints,” Future Internet, vol. 10, no. 10, p. 92, 2018.\n[27] Y. Wang, Y. Yang, Z. Yang, L. Zhao, P. Wang, and W. Xu, “Occlusion\naware unsupervised learning of optical ﬂow,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\npp. 4884–4893.\n[28] C. L. Zitnick and T. Kanade, “A cooperative algorithm for stereo match-\ning and occlusion detection,” IEEE Transactions on pattern analysis and\nmachine intelligence, vol. 22, no. 7, pp. 675–684, 2000.\n[29] N. Luo, C. Yang, W. Sun, and B. Song, “Unsupervised stereo matching\nwith occlusion-aware loss,” in Paciﬁc Rim International Conference on\nArtiﬁcial Intelligence.\nSpringer, 2018, pp. 746–758.\n[30] R. Hartley and A. Zisserman, Multiple view geometry in computer vision.\nCambridge university press, 2003.\n[31] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\ndriving? the kitti vision benchmark suite,” in 2012 IEEE Conference on\nComputer Vision and Pattern Recognition. IEEE, 2012, pp. 3354–3361.\n[32] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich,\nM. Schoenberg, S. Izadi, T. Funkhouser, and S. Fanello, “Activestere-\nonet: End-to-end self-supervised learning for active stereo systems,” in\nProceedings of the European Conference on Computer Vision (ECCV),\n2018, pp. 784–801.\n[33] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-\nnenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset\nfor semantic urban scene understanding,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016, pp. 3213–\n3223.\n[34] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a\nsingle image using a multi-scale deep network,” in Advances in neural\ninformation processing systems, 2014, pp. 2366–2374.\n[35] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for large-\nscale machine learning,” in 12th {USENIX} symposium on operating\nsystems design and implementation ({OSDI} 16), 2016, pp. 265–283.\n[36] G. Hinton, N. Srivastava, and K. Swersky, “Neural networks for machine\nlearning lecture 6a overview of mini-batch gradient descent,” Cited on,\nvol. 14, p. 8, 2012.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-10-21",
  "updated": "2020-10-21"
}