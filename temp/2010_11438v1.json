{
  "id": "http://arxiv.org/abs/2010.11438v1",
  "title": "GAN based Unsupervised Segmentation: Should We Match the Exact Number of Objects",
  "authors": [
    "Quan Liu",
    "Isabella M. Gaeta",
    "Bryan A. Millis",
    "Matthew J. Tyska",
    "Yuankai Huo"
  ],
  "abstract": "The unsupervised segmentation is an increasingly popular topic in biomedical\nimage analysis. The basic idea is to approach the supervised segmentation task\nas an unsupervised synthesis problem, where the intensity images can be\ntransferred to the annotation domain using cycle-consistent adversarial\nlearning. The previous studies have shown that the macro-level (global\ndistribution level) matching on the number of the objects (e.g., cells,\ntissues, protrusions etc.) between two domains resulted in better segmentation\nperformance. However, no prior studies have exploited whether the unsupervised\nsegmentation performance would be further improved when matching the exact\nnumber of objects at micro-level (mini-batch level). In this paper, we propose\na deep learning based unsupervised segmentation method for segmenting highly\noverlapped and dynamic sub-cellular microvilli. With this challenging task,\nboth micro-level and macro-level matching strategies were evaluated. To match\nthe number of objects at the micro-level, the novel fluorescence-based\nmicro-level matching approach was presented. From the experimental results, the\nmicro-level matching did not improve the segmentation performance, compared\nwith the simpler macro-level matching.",
  "text": "GAN based Unsupervised Segmentation:\nShould We Match the Exact Number of Objects\nQuan Liua, Isabella M. Gaetab, Bryan A. Millisb, Matthew J. Tyskab, and Yuankai Huoa\naDepartment of Electrical Engineering and Computer Science, Vanderbilt University,\nNashville, TN, 37235, USA\nbDepartment of Cell and Developmental Biology, Vanderbilt University, Nashville, TN, 37235,\nUSA\nABSTRACT\nThe unsupervised segmentation is an increasingly popular topic in biomedical image analysis. The basic idea is to\napproach the supervised segmentation task as an unsupervised synthesis problem, where the intensity images can\nbe transferred to the annotation domain using cycle-consistent adversarial learning. The previous studies have\nshown that the macro-level (global distribution level) matching on the number of the objects (e.g., cells, tissues,\nprotrusions etc.) between two domains resulted in better segmentation performance. However, no prior studies\nhave exploited whether the unsupervised segmentation performance would be further improved when matching\nthe exact number of objects at micro-level (mini-batch level). In this paper, we propose a deep learning based\nunsupervised segmentation method for segmenting highly overlapped and dynamic sub-cellular microvilli. With\nthis challenging task, both micro-level and macro-level matching strategies were evaluated. To match the number\nof objects at the micro-level, the novel ï¬‚uorescence-based micro-level matching approach was presented. From\nthe experimental results, the micro-level matching did not improve the segmentation performance, compared\nwith the simpler macro-level matching.\nKeywords: Unsupervised learning, Segmentation, Synthesis, GAN\n1. INTRODUCTION\nSemantic segmentation is one of the central tasks in microscope image analysis, which segments targeting objects\nfrom background context.1 Traditionally, the semantic segmentation was performed by unsupervised intensity\nbased methods, such as watershed,2 Gaussian mixture model (GMM),3 graph-cut4 etc. In the past few years,\nthe deep learning based methods have been increasingly popular in microscopy imaging, due to the superior\naccuracy and better generalizability.5 However, one of the major limitations in deep learning based semantic\nsegmentation is the need of large-scale annotated images, which is not only tedious, but also resource intensive.6\nCycleGAN,7 a breakthrough generative adversarial network (GAN)8 was proposed recently, which shed light on\nsemantic segmentation with minimal or even no manual annotation.9â€“12\nWithin the CycleGAN framework,7 many previous studies have tackled the unsupervised semantic segmen-\ntation in microscopy imaging. Ihle et al.13 proposed to use the CycleGAN framework to segment bright-ï¬eld\nimages of cell cultures, a live-dead assay of C.Elegans, and X-ray-computed tomography of metallic nanowire\nmeshes. A similar approach was proposed by14 for facilitating stain-independent supervised and unsupervised\nsegmentation on kidney histology. DeepSynth15 was proposed to further extend the CycleGAN framework from\n2D to 3D nuclear segmentation. Even though the CycleGAN based unsupervised segmentation approaches have\nshown decent performance on microscope images, very few studies have investigated the challenging sub-cellular\nmicrovilli segmentation with ï¬‚uorescence microscopy imaging. The sub-cellular microvilli segmentation is chal-\nlenging due to the highly overlapping and dynamic nature of such small sub-celluar objects.16,17\nDiï¬€erent from Pix2Pix GAN,18 which requires pixel-level matching between images across two domains,\nCycleGAN is able to perform image synthesis without paired images. However, the previous studies emphasized\nFurther author information: Quan Liu : E-mail: quan.liu@vanderbilt.edu\nContact Author: Yuankai Huo: E-mail: yuankai.huo@vanderbilt.edu\narXiv:2010.11438v1  [cs.CV]  22 Oct 2020\nMicro-level matching\nCycleGAN\nSimulator\nSimulator\nImages\nMasks\nCycleGAN\nImages\nMasks\nMacro-level matching\nReal \nImages\nReal \nImages\nMicrovilli \nProtein\nMicrovilli \nDistribution\nMicro-level \nMatching\nMacro-level \nMatching\nFigure 1. This ï¬gure shows the general framework of the image synthesis. CycleGAN is the used to perform synthesis\nbetween real images and simulated masks. In micro-level matching, the green protein channel in ï¬‚uorescence images is\nused to achieve the cell counting automatically, which provides the rough numbers of microvilli in the real images. Then,\nthe corresponding simulated masks with the same numbers of sticks are generated from the simulator when forming a\nmini-batch for training. In macro-level matching, the numbers of sticks in the simulated masks are randomly generated\nfrom the prior global distribution, without mini-batch level correspondence.\nthat the macro-level (global distribution level) matching on the number of objects between intensity images and\nsimulated masks improved the segmentation performance.13 That fact inspired us with the question that if the\nsegmentation performance could be further improved by doing more careful matching than the macro-level. To\nanswer the question, we propose a new micro-level matching (mini-batch level) strategy to match the rough\nnumber of objects across two domains when training the CycleGAN framework.\nIn this paper, we develop a deep learning based unsupervised semantic segmentation method for sub-cellular\nmicrovilli segmentation using ï¬‚uorescence microscopy. Meanwhile, we evaluate the performance of micro-level\nmatching strategy, which is enabled by the multi-channel nature of ï¬‚uorescence images. The contributions of this\nstudy are three-fold: (1) We propose the ï¬rst deep learning based unsupervised sub-cellular microvilli segmenta-\ntion method; (2) We propose the micro-level matching to ensure the roughly same number of objects across two\nmodalities within each mini-batch, without introducing extra human annotation eï¬€orts; (3) Comprehensive anal-\nyses are provided to evaluate the outcomes of diï¬€erent augmentation strategies when generating the simulated\nmasks for unsupervised microvilli segmentation.\n2. METHODS\nOur proposed unsupervised segmentation method consists of two parts: (1) image synthesis, and (2) segmenta-\ntion. In image synthesis, our goal is to synthesize realistic looking images from the simulated masks. Then, the\npaired synthetic images and masks are used to train another segmentation network. Note that, no manual\nannotations are used in our training either for CycleGAN or U-Net, as an unsupervised frame-\nwork.\n2.1 Cycle-Consistent Image Synthesis\nThe CycleGAN7 is used to generate our synthetic training data. As the standard CycleGAN implementation,\ngenerators and discriminators are used to transfer the styles between two image modalities. The role of the\ngenerators is to convert the real images to another domain, which are typically called â€fakeâ€ images.\nThe\ndiscriminators then judge if a given image is real or fake. The CycleGAN model design creatively forms the\nSimulator\nSmooth\nSmooth + Noise + Brightness\nSmooth + Noise\nSimulator\nAugmentation\nğ‘€ğ´\nCycleGAN\nCycleGAN\nğ‘€\nğ¼\nFigure 2. This ï¬gure demonstrates four experimental designs of providing diï¬€erent augmentations of masks for training\nthe CycleGAN. The ï¬rst design employs the binary mask directly, while the remaining three designs utilize diï¬€erent\naugmentation strategies.\nentire learning process as a cycle-consistent loop, where the reconstructed fake images after two generators\nshould be close to the original real images. In each branch, the generator tries to generate realistic images, while\nthe discriminator try to distinguish the fake images from the real ones.\nIn our unsupervised image segmentation framework (Fig. 1), the CycleGAN is employed to synthesize seg-\nmentation masks (or called â€annotationsâ€) from the real images I, and synthesize realistic looking images from\nsimulated segmentation masks M. In the ideal case, the trained generator GIâˆ’>M can be directly used as a\nsegmentation network to segment new images. However, the quality of synthesis between real images and clean\nbinary masks is typically not optimal since the underlying Poisson distribution of the binary masks is not a re-\nalistic distribution in real images.13 Moreover, the optimization of the KL divergence for training discriminators\nis more diï¬ƒcult to converge14 using clean binary masks. Therefore, the Gaussian smoothing, random noise, and\nbrightness variations are used to generate augmented masks MA in additional to the simulated mask images\nM for better synthetic performance (Fig. 2). Then, the trained generator GMAâˆ’>I will provide us unlimited\nfake but realistic looking images IF from the simulated and augmented masks MA. Eventually, the fake images\nIF and the clean binary masks MA (before augmentation) are used to train another independent segmentation\nnetwork (see Image Segmentation section).\n2.2 Micro- and Macro-level Matching\nCompared with traditional pixel-to-pixel conditional GAN design which needs pixel-level correspondence be-\ntween two modalities, the CycleGAN does not need paired images for training. However, the superior synthetic\nsegmentation performance is typically achieved if the distributions of the number of objects in real image modal-\nity and annotation modality are roughly matched,13,14 named as â€macro-levelâ€ matching. However, no studies\nhave explored the level of matching in the middle of pixel-level and macro-level. In this study, we proposed\nthe idea called â€micro-levelâ€ matching, which matches the number of objects in each mini-batch (Fig. 1). For\nexample, if a real image has roughly 21 microvilli, we will provide a simulated mask with the same 21 sticks,\nwhen forming the mini-batch. Then, the next question is that how can we get rough number of objects from the\nreal images. In this study, we utilize the multi-channel nature of ï¬‚uorescence microscopy to split the microvilli\nmarker mCherry-Espin (magenta color objects) and microvilli tip marker EGFP-EPS8 (green color objects).\nUsing the simple intensity thresholding based cell counting algorithm,19 the rough number of protein objects are\neasily achieved. The numbers are then used as the rough number of microvilli to simulate the corresponding\nmask ï¬les with the same number of objects, as the micro-level matching. Note that we only match the number\nof the objects in the micro-level matching, where the spatial distribution of the objects is still random.\nGenerator\nIn CycleGAN\nU-Net\nSegmentations\nSynthetic Images\nSimulated Masks\nDice Loss\nAugmented Masks\nFigure 3. This ï¬gure shows the segmentation pipeline. The input images of the U-Net model are the synthetic fake images\nfrom trained CycleGANâ€™s generator, while the annotations the simulated masks. Note that, no manual annotations are\nused in our training either for CycleGAN or U-Net, as an unsupervised framework.\n2.3 Image Segmentation\nU-Net20 is employed as the segmentation backbone network, which is a fully convolutional neural network and\nwidely used in image segmentation tasks. The segmentation part of our framework is shown in Fig. 3. In our\nproposed unsupervised segmentation framework, the input images of U-Net are the fake microvilli images, which\nare generated from the simulated masks using GMâˆ’>I or GMAâˆ’>I from trained CycleGAN. Then the Dice loss\nfunction is calculated by comparing the predicted segmentation with the simulated binary masks. Traditional\ndeep neural network typically needs a large number of annotated images to train a segmentation network. Using\nour design, however, we can generate unlimited number of training data to train the segmentation network\nwithout any manual annotation eï¬€orts.\n3. DATA AND EXPERIMENTAL DESIGN\n3.1 Microvilli Images\nTwelve microvilli images acquired using ï¬‚uorescence microscopy were used as training data, where each image\nhad â‰ˆ900 Ã— 900 pixels with pixel resolution 1.1 Âµm. Then, 500 image patches with 128 Ã— 128 pixels were ran-\ndomly sampled from the twelve images to train the CycleGAN as the real images. Then, another independent\nmicrovilli video with 20 frames was used as testing data to evaluate the performance of the proposed unsu-\npervised segmentation methods. Each frame has 256 Ã— 256 pixels with pixel resolution 1.1 Âµm. All microvilli\nin each frame were densely annotated manually by an experienced biologist as the gold standard segmenta-\ntion. The video of microvilli frames and manual annotations are presented in the supplementary materials:\nhttps://github.com/iamliuquan/GAN based segmentation.\n3.2 Experimental Design\nIn order to test if micro-level matching can improve the unsupervised segmentation performance, we performed\nexperiments using both macro-level and micro-level matching. For micro-level matching, the number of sticks of\neach images was obtained by automatically counting the number of green proteins. For macro-level matching,\nthe number of sticks for each image was randomly sampled from a uniform distribution (range from 11 to 63),\naccording to distribution of proteins.\nAs shown in Fig 2, we have four diï¬€erent augmentation settings to generate the simulated masks in annotation\ndomain:\nBinary masks: The binary masks were directly simulated as the images in the annotation domain, without\nany augmentation. Based on17 and the prior biological knowledge of microvilli, the width of each microvilli was\nsimulated between 2 to 5 Âµm, while the length was simulated between 10 to 50 Âµm. As the pixel resolution of\nall our images was 1.1 Âµm, we randomly generated sticks with 2 to 4 pixels width and 9 to 45 pixels length from\nuniform distribution.\nGaussian Smoothing: The ï¬rst augmentation was Gasussian smoothing, where a Gaussian ï¬lter with kernal\nsize of 5 Ã— 5 was applied to the binary masks.\nRandom noise: Upon the Gaussian smoothing, the random Gaussian noise was further applied to the entire\n25\n15\n35\n45\n55\nMicro Level Matching\nMacro Level Matching\nUnpaired Example \nReal Images \nSimulated \nMasks\nGaussian Smooth\nRandom Noise\nDifferent Brightness\nNumbers of Objects\nFigure 4. The synthesis results of diï¬€erent experimental designs are provided in this ï¬gure. The ï¬rst column is the initial\nsimulated masks with diï¬€erent numbers of objects (sticks). The middle columns exhibit the synthetic images from the\nmasks using diï¬€erent augmentation strategies. The last column are ï¬ve randomly selected real images, which are unpaired\nto the masks in CycleGAN framework.\nmask image. The values of random noise ranged from 0 to 255 follows Gaussian distribution.\nDiï¬€erent brightness: To further introduce the global intensity variations, random intensity values (200 to\n255) were assigned to each stick in binary masks, where maximum foreground intensity value was 255.\nTo improve the segmentation performance, CycleGAN was employed to synthesis cell images for U-Net model\ntraining. In our experiment, CycleGAN is used to learn the mapping from simulated masks to real microvilli\ncell images. We built up dataset in these two domains as CycleGAN modelâ€™s input. Our CycleGAN model was\ntrained for 60 epochs. According to the training loss, generator trained for 50 epochs shows the best performance.\nGenerator trained in CycleGAN will be used to synthesis microvilli cell images based on simulated mask images.\nCycleGAN model cannot cover all details using original frames as input which has too many cells. For both\nCycleGAN and U-Net, the input images were all with 128 Ã— 128 resolution cropped from original frames, and\nthen resized to 256 Ã— 256 during training.\nWhen applying trained U-Net on testing microvilli images, each\ntesting image was ï¬rst split to four 128Ã—128 images, and the ï¬nal segmentation were achieved by concatenating\nthe corresponding four predictions back to the original resolution.\nThe Dice results were calculated in the\noriginal 256 Ã— 256 resolution for testing images. The CycleGAN and U-Net were deployed on a computer with\nGeForce GTX 1060 Graphic Card with 6 GB memory. To get better synthesised data and avoid over-ï¬tting, the\nCycleGAN was trained with 50 epochs and the U-Net was trained with 10 epochs for all experiments. According\nto the prediction performance, U-Net has the best performance after 10 epochs. The results from the last epochs\nwere reported in this paper.\nMicro Level Matching\nMacro Level Matching\nManual\nAnnotation\nReal\nImages\nGaussian Smooth\nRandom Noise\nDifferent Brightness\n20th\n1st\n15th\n10th\n5th\nFrame Number\nFigure 5. The ï¬nal segmentation results from U-Net are presented in this ï¬gure. Each row shows the segmentation results\nat diï¬€erent frames in a microvilli video (the video of microvilli frames and our unsupervised segmentation results are\npresented in the supplementary materials: https://github.com/iamliuquan/GAN based segmentation.\nTable 1. The average Dice values of diï¬€erent experiments.\nExp.\nSmooth Noise Bright. D(w=1) D(w=2) D(w=3) D(w=4) D(w=5)\nMicro-level\nmatching\n0.3818\n0.4860\n0.5459\n0.5605\n0.5628\nâœ“\n0.3650\n0.4691\n0.5301\n0.5535\n0.5667\nâœ“\nâœ“\n0.3738\n0.4783\n0.5367\n0.5511\n0.5547\nâœ“\nâœ“\nâœ“\n0.3810\n0.4865\n0.5479\n0.5650\n0.5730\nMacro-level\nmatching\n0.3639\n0.4717\n0.5364\n0.5583\n0.5675\nâœ“\n0.3811\n0.4918\n0.5557\n0.5776\n0.5888\nâœ“\nâœ“\n0.3902 0.4981 0.5607 0.5965 0.6169\nâœ“\nâœ“\nâœ“\n0.3894\n0.4903\n0.5467\n0.5615\n0.5631\nâ€œDâ€ indicate the Dice score, w means the width of the ground truth.\n4. RESULTS\nConsidering both micro- and macro-level matching with diï¬€erent augmentation strategies, we performed eight\nexperiments by training eight diï¬€erent CycleGAN networks. The qualitative results of image synthesis from\neight diï¬€erent CycleGAN networks are provided in Fig. 4.\nThen, the synthetic images were used to train eights diï¬€erent U-Net models using synthetic training im-\nage patches and applied to the real testing images. For testing images, the manual annotation was performed\nby tracking center line fragments of each microvillus (annotated by experienced biologist) since the traditional\ncontour based annotations were extremely diï¬ƒcult on the tiny sub-cellular structures. To evaluate the segmen-\ntation results, we assigned diï¬€erent width to the manual segmentation and reported the results in Table. 1. The\ncorresponding qualitative results of segmentation are provided in Fig. 5. According to microvilli cellâ€™s biologic\ncharacteristic, manual annotation images are presented with width=3. From the results, the macro-level match-\ning with Gaussian smoothing and random noise achieved the best performance across diï¬€erent widths of manual\nannotation. The micro-level matching did not improve the segmentation performance. Micro-level pairing can\nachieve higher accuracy on training dataset because its pairing is detailed to ï¬t training dataset properties.\nMacro-level pairing is more robust. U-Net model trained by macro-level pairing performs better than micro-level\npairing on new dataset. The standard Dice similarity coeï¬ƒcient metrics were used to evaluate diï¬€erent methods.\nThe video of microvilli frames and our unsupervised segmentation results are presented in the supplementary\nmaterials: https://github.com/iamliuquan/GAN based segmentation.\n5. CONCLUSION\nIn this study, we proposed the ï¬rst deep learning solution to enable unsupervised sub-cellular microvilli seg-\nmentation. Beyond the current standard macro-level matching strategy, we utilized the multi-channel nature of\nï¬‚uorescence microscopy to enable the micro-level matching of the number of objects in each mini-batch without\nintroducing new human annotation eï¬€orts. From the experimental results, we conclude that the micro-level\nmatching of object numbers at the mini-batch level did not lead to better segmentation performance. From\nthe comprehensive analyses of introducing noise, smoothness and brightness, the Gaussian smoothing and ran-\ndom noise on the simulated annotations with macro-level matching resulted in the best microvilli segmentation\nperformance.\nREFERENCES\n[1] Wu, K., Gauthier, D., and Levine, M. D., â€œLive cell image segmentation,â€ IEEE Transactions on biomedical\nengineering 42(1), 1â€“12 (1995).\n[2] Pinidiyaarachchi, A. and WÂ¨ahlby, C., â€œSeeded watersheds for combined segmentation and tracking of cells,â€\nin [International Conference on Image Analysis and Processing], 336â€“343, Springer (2005).\n[3] Ragothaman, S., Narasimhan, S., Basavaraj, M. G., and Dewar, R., â€œUnsupervised segmentation of cervical\ncell images using gaussian mixture model,â€ in [Proceedings of the IEEE conference on computer vision and\npattern recognition workshops], 70â€“75 (2016).\n[4] LeskÂ´o, M., Kato, Z., Nagy, A., Gombos, I., Torok, Z., Vigh Jr, L., and Vigh, L., â€œLive cell segmentation\nin ï¬‚uorescence microscopy via graph cut,â€ in [2010 20th International Conference on Pattern Recognition],\n1485â€“1488, IEEE (2010).\n[5] Moen, E., Bannon, D., Kudo, T., Graf, W., Covert, M., and Van Valen, D., â€œDeep learning for cellular\nimage analysis,â€ Nature methods , 1â€“14 (2019).\n[6] Zhang, L., Lu, L., Nogues, I., Summers, R. M., Liu, S., and Yao, J., â€œDeeppap: deep convolutional networks\nfor cervical cell classiï¬cation,â€ IEEE journal of biomedical and health informatics 21(6), 1633â€“1643 (2017).\n[7] Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A., â€œUnpaired image-to-image translation using cycle-consistent\nadversarial networks,â€ in [Proceedings of the IEEE international conference on computer vision], 2223â€“2232\n(2017).\n[8] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY., â€œGenerative adversarial nets,â€ in [Advances in neural information processing systems], 2672â€“2680 (2014).\n[9] Huo, Y., Xu, Z., Bao, S., Assad, A., Abramson, R. G., and Landman, B. A., â€œAdversarial synthesis learning\nenables segmentation without target modality ground truth,â€ in [2018 IEEE 15th international symposium\non biomedical imaging (ISBI 2018)], 1217â€“1220, IEEE (2018).\n[10] Huo, Y., Xu, Z., Moon, H., Bao, S., Assad, A., Moyo, T. K., Savona, M. R., Abramson, R. G., and Landman,\nB. A., â€œSynseg-net: Synthetic segmentation without target modality ground truth,â€ IEEE transactions on\nmedical imaging 38(4), 1016â€“1025 (2018).\n[11] Zhang, Z., Yang, L., and Zheng, Y., â€œTranslating and segmenting multimodal medical volumes with cycle-\nand shape-consistency generative adversarial network,â€ in [Proceedings of the IEEE conference on computer\nvision and pattern recognition], 9242â€“9251 (2018).\n[12] Chen, C., Dou, Q., Chen, H., Qin, J., and Heng, P.-A., â€œSynergistic image and feature adaptation: Towards\ncross-modality domain adaptation for medical image segmentation,â€ in [Proceedings of the AAAI Conference\non Artiï¬cial Intelligence], 33, 865â€“872 (2019).\n[13] Ihle, S., Reichmuth, A. M., Girardin, S., Han, H., Stauï¬€er, F., Bonnin, A., Stampanoni, M., VÂ¨orÂ¨os, J., and\nForrÂ´o, C., â€œUdct: Unsupervised data to content transformation with histogram-matching cycle-consistent\ngenerative adversarial networks,â€ bioRxiv , 563734 (2019).\n[14] Gadermayr, M., Gupta, L., Appel, V., Boor, P., Klinkhammer, B. M., and Merhof, D., â€œGenerative ad-\nversarial networks for facilitating stain-independent supervised and unsupervised segmentation: a study on\nkidney histology,â€ IEEE transactions on medical imaging 38(10), 2293â€“2302 (2019).\n[15] Dunn, K. W., Fu, C., Ho, D. J., Lee, S., Han, S., Salama, P., and Delp, E. J., â€œDeepsynth: Three-\ndimensional nuclear segmentation of biological images using neural networks trained with synthetic data,â€\nScientiï¬c reports 9(1), 1â€“15 (2019).\n[16] Julio, G., Merindano, M. D., Canals, M., and RallÂ´o, M., â€œImage processing techniques to quantify micro-\nprojections on outer corneal epithelial cells,â€ Journal of anatomy 212(6), 879â€“886 (2008).\n[17] Meenderink, L. M., Gaeta, I. M., Postema, M. M., Cencer, C. S., Chinowsky, C. R., Krystoï¬ak, E. S.,\nMillis, B. A., and Tyska, M. J., â€œActin dynamics drive microvillar motility and clustering during brush\nborder assembly,â€ Developmental cell 50(5), 545â€“556 (2019).\n[18] Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A., â€œImage-to-image translation with conditional adversarial\nnetworks,â€ in [Proceedings of the IEEE conference on computer vision and pattern recognition], 1125â€“1134\n(2017).\n[19] Refai, H., Li, L., Teague, T. K., and Naukam, R., â€œAutomatic count of hepatocytes in microscopic images,â€\nin [Proceedings 2003 International Conference on Image Processing (Cat. No. 03CH37429)], 2, IIâ€“1101,\nIEEE (2003).\n[20] Ronneberger, O., Fischer, P., and Brox, T., â€œU-net: Convolutional networks for biomedical image seg-\nmentation,â€ in [International Conference on Medical image computing and computer-assisted intervention],\n234â€“241, Springer (2015).\n",
  "categories": [
    "cs.CV",
    "eess.IV"
  ],
  "published": "2020-10-22",
  "updated": "2020-10-22"
}