{
  "id": "http://arxiv.org/abs/2010.11560v1",
  "title": "Deep Learning is Singular, and That's Good",
  "authors": [
    "Daniel Murfet",
    "Susan Wei",
    "Mingming Gong",
    "Hui Li",
    "Jesse Gell-Redman",
    "Thomas Quella"
  ],
  "abstract": "In singular models, the optimal set of parameters forms an analytic set with\nsingularities and classical statistical inference cannot be applied to such\nmodels. This is significant for deep learning as neural networks are singular\nand thus \"dividing\" by the determinant of the Hessian or employing the Laplace\napproximation are not appropriate. Despite its potential for addressing\nfundamental issues in deep learning, singular learning theory appears to have\nmade little inroads into the developing canon of deep learning theory. Via a\nmix of theory and experiment, we present an invitation to singular learning\ntheory as a vehicle for understanding deep learning and suggest important\nfuture work to make singular learning theory directly applicable to how deep\nlearning is performed in practice.",
  "text": "Deep Learning is Singular, and That’s Good\nDaniel Murfet∗, Susan Wei∗, Mingming Gong, Hui Li, Jesse Gell-Redman, and Thomas\nQuella\nSchool of Mathematics and Statistics\nUniversity of Melbourne\nMelbourne, Australia\nOctober 23, 2020\nAbstract\nIn singular models, the optimal set of parameters forms an analytic set with singularities\nand classical statistical inference cannot be applied to such models. This is signiﬁcant\nfor deep learning as neural networks are singular and thus “dividing\" by the determinant\nof the Hessian or employing the Laplace approximation are not appropriate. Despite its\npotential for addressing fundamental issues in deep learning, singular learning theory\nappears to have made little inroads into the developing canon of deep learning theory.\nVia a mix of theory and experiment, we present an invitation to singular learning theory\nas a vehicle for understanding deep learning and suggest important future work to make\nsingular learning theory directly applicable to how deep learning is performed in practice.\n1\nIntroduction\nIt has been understood for close to twenty years that neural networks are singular statistical\nmodels [Amari et al., 2003, Watanabe, 2007]. This means, in particular, that the set of\nnetwork weights equivalent to the true model under the Kullback-Leibler divergence forms a\nreal analytic variety which fails to be an analytic manifold due to the presence of singularities.\nIt has been shown by Sumio Watanabe that the geometry of these singularities controls\nquantities of interest in statistical learning theory, e.g., the generalisation error. Singular\nlearning theory [Watanabe, 2009] is the study of singular models and requires very diﬀerent\ntools from the study of regular statistical models. The breadth of knowledge demanded by\nsingular learning theory – Bayesian statistics, empirical processes and algebraic geometry – is\nrewarded with profound and surprising results which reveal that singular models are diﬀerent\nfrom regular models in practically important ways. To illustrate the relevance of singular\nlearning theory to deep learning, each section of this paper illustrates a key takeaway idea1.\nThe real log canonical threshold (RLCT) is the correct way to count the eﬀective\nnumber of parameters in a deep neural network (DNN) (Section 4).\nTo every\n(model, truth, prior) triplet is associated a birational invariant known as the real log canonical\nthreshold. The RLCT can be understood in simple cases as half the number of normal\ndirections to the set of true parameters. We will explain why this matters more than the\ncurvature of those directions (as measured for example by eigenvalues of the Hessian) laying\nbare some of the confusion over “ﬂat” minima.\n∗Equal contribution\n1Source code is available at https://github.com/susanwe/RLCT.\n1\narXiv:2010.11560v1  [cs.LG]  22 Oct 2020\nFor singular models, the Bayes predictive distribution is superior to MAP and\nMLE (Section 5).\nIn regular statistical models, the 1) Bayes predictive distribution,\n2) maximum a posteriori (MAP) estimator, and 3) maximum likelihood estimator (MLE)\nhave asymptotically equivalent generalisation error (as measured by the Kullback-Leibler\ndivergence). This is not so in singular models. We illustrate in our experiments that even\n“being Bayesian” in just the ﬁnal layers improves generalisation over MAP. Our experiments\nfurther conﬁrm that the Laplace approximation of the predictive distribution Smith and Le\n[2017], Zhang et al. [2018] is not only theoretically inappropriate but performs poorly.\nSimpler true distribution means lower RLCT (Section 6).\nIn singular models the\nRLCT depends on the (model, truth, prior) triplet whereas in regular models it depends only\non the (model, prior) pair. The RLCT increases as the complexity of the true distribution\nrelative to the supposed model increases. We verify this experimentally with a simple family\nof ReLU and SiLU networks.\n2\nRelated work\nIn classical learning theory, generalisation is explained by measures of capacity such as the\nl2 norm, Radamacher complexity, and VC dimension [Bousquet et al., 2003]. It has become\nclear however that these measures cannot capture the empirical success of DNNs [Zhang et al.,\n2017]. For instance, over-parameterised neural networks can easily ﬁt random labels [Zhang\net al., 2017, Du et al., 2018, Allen-Zhu et al., 2019a] indicating that complexity measures\nsuch as Rademacher complexity are very large. There is also a slate of work on generalisation\nbounds in deep learning. Uniform convergence bounds [Neyshabur et al., 2015, Bartlett\net al., 2017, Neyshabur and Li, 2019, Arora et al., 2018] usually cannot provide non-vacuous\nbounds. Data-dependent bounds [Brutzkus et al., 2018, Li and Liang, 2018, Allen-Zhu et al.,\n2019b] consider the “classiﬁability” of the data distribution in generalisation analysis of neural\nnetworks. Algorithm-dependent bounds [Daniely, 2017, Arora et al., 2019, Yehudai and\nShamir, 2019, Cao and Gu, 2019] consider the relation of Gaussian initialisation and the\ntraining dynamics of (stochastic) gradient descent to kernel methods [Jacot et al., 2018].\nIn contrast to many of the aforementioned works, we are interested in estimating the\nconditional distribution q(y|x). Speciﬁcally, we measure the generalisation error of some\nestimate ˆqn(y|x) in terms of the Kullback-Leibler divergence between q and ˆqn, see (5.1). The\nnext section gives a crash course on singular learning theory. The rest of the paper illustrates\nthe key ideas listed in the introduction. Since we cover much ground in this short note, we will\nreview other relevant work along the way, in particular literature on “ﬂatness\", the Laplace\napproximation in deep learning, etc.\n3\nSingular Learning Theory\nTo understand why classical measures of capacity fail to say anything meaningful about DNNs,\nit is important to distinguish between two diﬀerent types of statistical models. Recall we\nare interested in estimating the true (and unknown) conditional distribution q(y|x) with a\nclass of models {p(y|x, w) : w ∈W} where W ⊂Rd is the parameter space. We say the\nmodel is identiﬁable if the mapping w 7→p(y|x, w) is one-to-one. Let q(x) be the distribution\nof x. The Fisher information matrix associated with the model {p(y|x, w) : w ∈W} is the\nmatrix-valued function on W deﬁned by\nI(w)ij =\nZ Z\n∂\n∂wi\n[log p(y|x, w)] ∂\n∂wj\n[log p(y|x, w)]q(y|x)q(x)dxdy,\n2\nif this integral is ﬁnite. Following the conventions in Watanabe [2009], we have the following\nbifurcation of statistical models. A statistical model p(y|x, w) is called regular if it is 1)\nidentiﬁable and 2) has positive-deﬁnite Fisher information matrix. A statistical model is\ncalled strictly singular if it is not regular.\nLet ϕ(w) be a prior on the model parameters w. To every (model, truth, prior) triplet,\nwe can associate the zeta function, ζ(z) =\nR\nK(w)zϕ(w) dw, z ∈C, where K(w) is the\nKullback-Leibler (KL) divergence between the model p(y|x, w) and the true distribution\nq(y|x):\nK(w) :=\nZ Z\nq(y|x) log\nq(y|x)\np(y|x, w)q(x) dx dy.\n(3.1)\nFor a (model, truth, prior) triplet (p(y|x, w), q(y|x), ϕ), let −λ be the maximum pole of\nthe corresponding zeta function. We call λ the real log canonical threshold (RLCT)\n[Watanabe, 2009] of the (model, truth, prior) triplet. The RLCT is the central quantity of\nsingular learning theory.\nBy Watanabe [2009, Theorem 6.4] the RLCT is equal to d/2 in regular statistical models\nand bounded above by d/2 in strictly singular models if realisability holds: let\nW0 = {w ∈W : p(y|x, w) = q(y|x)}\nbe the set of true parameters, we say q(y|x) is realisable by the model class if W0 is non-\nempty. The condition of realisability is critical to standard results in singular learning theory.\nModiﬁcations to the theory are needed in the case that q(y|x) is not realisable, see the\ncondition called relatively ﬁnite variance in Watanabe [2018].\nNeural networks in singular learning theory.\nLet W ⊆Rd be the space of weights\nof a neural network of some ﬁxed architecture, and let f(x, w) : RN × W −→RM be the\nassociated function. We shall focus on the regression task and study the model\np(y|x, w) =\n1\n(2π)M/2 exp\n\u0010\n−1\n2∥y −f(x, w)∥2\u0011\n(3.2)\nbut singular learning theory can also apply to classiﬁcation, for instance. It is routine to\ncheck (see Appendix A.1) that for feedforward ReLU networks not only is the model strictly\nsingular but the matrix I(w) is degenerate for all nontrivial weight vectors and the Hessian of\nK(w) is degenerate at every point of W0.\nRLCT plays an important role in model selection.\nOne of the most accessible results\nin singular learning theory is the work related to the widely-applicable Bayesian information\ncriterion (WBIC) Watanabe [2013], which we brieﬂy review here for completeness.\nLet\nDn = {(xi, yi)}n\ni=1 be a dataset of input-output pairs.\nLet Ln(w) be the negative log\nlikelihood\nLn(w) = −1\nn\nn\nX\ni=1\nlog p(yi|xi, w)\n(3.3)\nand p(Dn|w) = exp(−nLn(w)). The marginal likelihood of a model {p(y|x, w) : w ∈W} is\ngiven by p(Dn) =\nR\nW p(Dn|w)ϕ(w) dw and can be loosely interpreted as the evidence for the\nmodel. Between two models, we should prefer the one with higher model evidence. However,\nsince the marginal likelihood is an intractable integral over the parameter space of the model,\none needs to consider some approximation.\nThe well-known Bayesian Information Criterion (BIC) derives from an asymptotic approxi-\nmation of −log p(Dn) using the Laplace approximation, leading to BIC = nLn(wMLE)+ d\n2 log n.\n3\nSince we want the marginal likelihood of the data for some given model to be high one should\nalmost never adopt a DNN according to the BIC, since in such models d may be very large.\nHowever, this argument contains a serious mathematical error: the Laplace approximation\nused to derive BIC only applies to regular statistical models, and DNNs are not regular. The\ncorrect criterion for both regular and strictly singular models was shown in Watanabe [2013]\nto be nLn(w0) + λ log n where w0 ∈W0 and λ is the RLCT. Since DNNs are highly singular\nλ may be much smaller than d/2 (Section 6) it is possible for DNNs to have high marginal\nlikelihood – consistent with their empirical success.\n4\nVolume dimension, eﬀective degrees of freedom, and ﬂatness\nVolume codimension.\nThe easiest way to understand the RLCT is as a volume codimension\n[Watanabe, 2009, Theorem 7.1]. Suppose that W ⊆Rd and W0 is nonempty, i.e., the true\ndistribution is realisable.\nWe consider a special case in which the KL divergence in a\nneighborhood of every point v0 ∈W0 has an expression in local coordinates of the form\nK(w) =\nd′\nX\ni=1\nciw2\ni ,\n(4.1)\nwhere the coeﬃcients c1, . . . , cd′ > 0 may depend on v0 and d′ may be strictly less than d.\nIf the model is regular then this is true with d = d′ and if it holds for d′ < d then we say\nthat the pair (p(y|x, w), q(y|x)) is minimally singular. It follows that the set W0 ⊆W of true\nparameters is a regular submanifold of codimension d′ (that is, W0 is a manifold of dimension\nd −d′ where W has dimension d). Under this hypothesis there are, near each true parameter\nv0 ∈W0, exactly d −d′ directions in which v0 can be varied without changing the model\np(y|x, w) and d′ directions in which varying the parameters does change the model. In this\nsense, there are d′ eﬀective parameters near v0.\nThis number of eﬀective parameters can be computed by an integral. Consider the volume\nof the set of almost true parameters V (t, v0) =\nR\nK(w)<t ϕ(w)dw where the integral is restricted\nto a small closed ball around v0. As long as the prior ϕ(w) is non-zero on W0 it does not\naﬀect the relevant features of the volume, so we may assume ϕ is constant on the region\nof integration in the ﬁrst d′ directions and normal in the remaining directions, so up to a\nconstant depending only on d′ we have\nV (t, v0) ∝\ntd′/2\n√c1 · · · cd′\n(4.2)\nand we can extract the exponent of t in this volume in the limit\nd′ = 2 lim\nt→0\nlog\n\b\nV (at, v0)/V (t, v0)\n\t\nlog(a)\n(4.3)\nfor any a > 0, a ̸= 1. We refer to the right hand side of (4.3) as the volume codimension at v0.\nThe function K(w) has the special form (4.1) locally with d′ = d if the statistical model\nis regular (and realisable) and with d′ < d in some singular models such as reduced rank\nregression (Appendix A.2). While such a local form does not exist for a singular model\ngenerally (in particular for neural networks) nonetheless under natural conditions [Watanabe,\n2009, Theorem 7.1] we have V (t, v0) = ctλ + o(tλ) where c is a constant. We assume that in a\nsuﬃciently small neighborhood of v0 the point RLCT λ at v0 [Watanabe, 2009, Deﬁnition 2.7]\nis less than or equal to the RLCT at every point in the neighborhood so that the multiplicity\nm = 1, see Section 7.6 of [Watanabe, 2009] for relevant discussion. It follows that the limit on\n4\nthe right hand side of (4.3) exists and is equal to λ. In particular λ = d′/2 in the minimally\nsingular case.\nNote that for strictly singular models such as DNNs 2λ may not be an integer. This may\nbe disconcerting but the connection between the RLCT, generalisation error and volume\ndimension strongly suggests that 2λ is nonetheless the only geometrically meaningful “count”\nof the eﬀective number of parameters near v0.\nRLCT and likelihood vs temperature.\nAgain working with the model in (3.2), consider\nthe expectation over the posterior at temperature T as deﬁned in (A.7) of the negative log\nlikelihood (3.3)\nE(T) = E1/T\nw\n\u0002\nnLn(w)\n\u0003\n= E1/T\nw\nh\n1\n2\nn\nX\ni=1\n∥yi −f(xi, w)∥2i\n+ nM\n2\nlog(2π) .\nNote that when n is large Ln(v0) ≈M\n2 log(2π) for any v0 ∈W0 so for T ≈0 the posterior\nconcentrates around the set W0 of true parameters and E(T) ≈nM\n2 log(2π). Consider the\nincrease ∆E = E(T + ∆T) −E(T) corresponding to an increase in temperature ∆T. It can\nbe shown that ∆E ≈λ∆T where the reader should see [Watanabe, 2013, Corollary 3] for a\nprecise statement. As the temperature increases, samples taken from the tempered posterior\nare more distant from W0 and the error E will increase. If λ is smaller then for a given increase\nin temperature the quantity E increases less: this is one way to understand intuitively why a\nmodel with smaller RLCT generalises better from the dataset Dn to the true distribution.\nFlatness.\nIt is folklore in the deep learning community that ﬂatness of minima is related\nto generalisation [Hinton and Van Camp, 1993, Hochreiter and Schmidhuber, 1997] and\nthis claim has been revisited in recent years [Chaudhari et al., 2017, Smith and Le, 2017,\nJastrzebski et al., 2017, Zhang et al., 2018]. In regular models this can be justiﬁed using the\nlower order terms of the asymptotic expansion of the Bayes free energy [Balasubramanian,\n1997, §3.1] but the argument breaks down in strictly singular models, since for example the\nLaplace approximation of Zhang et al. [2018] is invalid. The point can be understood via an\nanalysis of the version of the idea in [Hochreiter and Schmidhuber, 1997]. Their measure of\nentropy compares the volume of the set of parameters with tolerable error t0 (our almost true\nparameters) to a standard volume\n−log\nhV (t0, v0)\ntd/2\n0\ni\n= d −d′\n2\nlog(t0) + 1\n2\nd\nX\ni=1\nlog ci .\n(4.4)\nHence in the case d = d′ the quantity −1\n2\nP\ni log(ci) is a measure of the entropy of the set of\ntrue parameters near w0, a point made for example in Zhang et al. [2018]. However when\nd′ < d this conception of entropy is inappropriate because of the d −d′ directions in which\nK(w) is ﬂat near v0, which introduce the t0 dependence in (4.4).\n5\nGeneralisation\nThe generalisation puzzle [Poggio et al., 2018] is one of the central mysteries of deep learning.\nTheoretical investigations into the matter is an active area of research Neyshabur et al. [2017].\nMany of the recent proposals of capacity measures for neural networks are based on the\neigenspectrum of the (degenerate) Hessian, e.g., Thomas et al. [2019], Maddox et al. [2020].\nBut this is not appropriate for singular models, and hence for DNNs.\n5\nSince we are interested in learning the distribution, our notion of generalisation is slightly\ndiﬀerent, being measured by the KL divergence. Precise statements regarding the generalisation\nbehavior in singular models can be made using singular learning theory. Let the network\nweights be denoted θ rather than w for reasons that will become clear. Recall in the Bayesian\nparadigm, prediction proceeds via the so-called Bayes predictive distribution, p(y|x, Dn) =\nR\np(y|x, θ)p(θ|Dn) dθ. More commonly encountered in deep learning practice are the MAP\nand MLE point estimators. While in a regular statistical model, the three estimators 1) Bayes\npredictive distribution, 2) MAP, and 3) MLE have the same leading term in their asymptotic\ngeneralisation behavior, the same is not true in singular models. More precisely, let ˆqn(y|x)\nbe some estimate of the true unknown conditional density q(y|x) based on the dataset Dn.\nThe generalisation error of the predictor ˆqn(y|x) is\nG(n) := KL(q(y|x)||ˆqn(y|x)) =\nZ Z\nq(y|x) log q(y|x)\nˆqn(y|x)q(x) dy dx.\n(5.1)\nTo account for sampling variability, we will work with the average generalisation error, EnG(n),\nwhere En denotes expectation over the dataset Dn. By Watanabe [2009, Theorem 1.2 and\nTheorem 7.2], we have\nEnG(n) = λ/n + o(1/n) if ˆqn is the Bayes predictive distribution,\n(5.2)\nwhere λ is the RLCT corresponding to the triplet (p(y|x, θ), q(y|x), ϕ(θ)).\nIn contrast,\nwe should note that Zhang et al. [2018] and Smith and Le [2017] rely on the Laplace\napproximation to explain the generalisation of the Bayes predictive distribution though both\nworks acknowledge the Laplace approximate is inappropriate. For completeness, a quick sketch\nof the derivation of (5.2) is provided in Appendix A.4. Now by [Watanabe, 2009, Theorem\n6.4] we have\nEnG(n) = C/n + o(1/n) if ˆqn is the MAP or MLE,\n(5.3)\nwhere C (diﬀerent for MAP and MLE) is the maximum of some Gaussian process. For regular\nmodels, the MAP, MLE, and the Bayes predictive distribution have the same leading term\nfor EnG(n) since λ = C = d/2. However in singular models, C is generally greater than λ,\nmeaning we should prefer the Bayes predictive distribution for singular models.\nThat the RLCT has such a simple relationship to the Bayesian generalisation error\nis remarkable.\nOn the other hand, the practical implications of (A.9) are limited since\nthe Bayes predictive distribution is intractable.\nWhile approximations to the Bayesian\npredictive distribution, say via variational inference, might inherit a similar relationship\nbetween generalisation and the (variational) RLCT, serious theoretical developments will be\nrequired to rigorously establish this. The challenge comes from the fact that for approximate\nBayesian predictive distributions, the free energy and generalisation error may have diﬀerent\nlearning coeﬃcients λ. This was well documented in the case of a neural network with one\nhidden layer [Nakajima and Watanabe, 2007].\nWe set out to investigate whether certain very simple approximations of the Bayes\npredictive distribution can already demonstrate superiority over point estimators. Suppose\nthe input-target relationship is modeled as in (3.2) but we write θ instead of w. We set\nq(x) = N(0, I3). For now consider the realisable case, q(y|x) = p(y|x, θ0) where θ0 is drawn\nrandomly according to the default initialisation in PyTorch when model (3.2) is instantiated.\nWe calculate En G(n) using multiple datasets Dn and a large testing set, see Appendix A.5\nfor more details.\nSince f is a hierarchical model, let’s write it as fθ(·) = h(g(·; v); w) with the dimension\nof w being relatively small. Let θMAP = (vMAP, wMAP) be the MAP estimate for θ using\nbatch gradient descent. The idea of our simple approximate Bayesian scheme is to freeze the\n6\nFigure 1: Realisable and full batch gradient descent for MAP. Average generalisation errors En G(n)\nare displayed for various approximations of the Bayes predictive distribution. The results of the\nLaplace approximations are reported in the Appendix and not displayed here because they are higher\nthan other approximation schemes by at least an order of magnitude. Each subplot shows a diﬀerent\ncombination of hidden layers in g (1 or 5) and activation function in h (ReLU or identity). Note that\nthe y-axis is not shared.\nnetwork weights at the MAP estimate for early layers and perform approximate Bayesian\ninference for the ﬁnal layers2. e.g., freeze the parameters of g at vMAP and perform MCMC\nover w. Throughout the experiments, g : R3 →R3 is a feedforward ReLU block with each\nhidden layer having 5 hidden units and h : R3 →R3 is either BAx or B ReLU(Ax) where\nA ∈R3×r, B ∈Rr×3. We set r = 3. We shall consider 1 or 5 hidden layers for g.\nTo approximate the Bayes predictive distribution, we perform either the Laplace approx-\nimation or the NUTS variant of HMC [Hoﬀman and Gelman, 2014] in the last two layers,\ni.e., performing inference over A, B in h(g(·; vMAP); A, B). Note that MCMC is operating in a\nspace of 18 dimensions in this case, which is small enough for us to expect MCMC to perform\nwell. We also implemented the Laplace approximation and NUTS in the last layer only, i.e.\nperforming inference over B in h2(h1(g(·; vMAP); AMAP); B). Further implementation details\nof these approximate Bayesian schemes are found in Appendix A.5.\nFrom the outset, we expect the Laplace approximation over w = (A, B) to be invalid since\nthe model is singular. We do however expect the last-layer-only Laplace approximation over\nB to be sound. Next, we expect the MCMC approximation in either the last layer or last two\n2This is similar in spirit to Kristiadi et al. [2020] who claim that even “being Bayesian a little bit\" ﬁxes\noverconﬁdence. They approach this via the Laplace approximation for the ﬁnal layer of a ReLU network. It is\nalso worth noting that Kristiadi et al. [2020] do not attempt to formalise what it means to \"ﬁx overconﬁdence\";\nthe precise statement should be in terms of G(n).\n7\nTable 1: Companion to Figure 1. The learning coeﬃcient is the slope of the linear ﬁt 1/n versus\nEn G(n) (no intercept since realisable). The R2 value gives a sense of the goodness-of-ﬁt.\n(a) 1 hidden layer(s) in g, identity activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n9.709027\n0.966124\nlast layer only (B) MCMC\n6.410380\n0.988921\nlast two layers (A,B) Laplace\ninf\nNaN\nlast layer only (B) Laplace\n2154.989266\n0.801077\nMAP\n10.714216\n0.951051\n(b) 5 hidden layer(s) in g, identity activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n1.286290\n0.985161\nlast layer only (B) MCMC\n1.298504\n0.982298\nlast two layers (A,B) Laplace\ninf\nNaN\nlast layer only (B) Laplace\n2038.605589\n0.803736\nMAP\n1.437473\n0.983411\n(c) 1 hidden layer(s) in g, ReLU activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n3.117187\n0.977313\nlast layer only (B) MCMC\n3.152710\n0.980132\nlast two layers (A,B) Laplace\ninf\nNaN\nlast layer only (B) Laplace\n1120.648298\n0.742412\nMAP\n5.343311\n0.972212\n(d) 5 hidden layer(s) in g, ReLU activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n0.835593\n0.957824\nlast layer only (B) MCMC\n1.466273\n0.920716\nlast two layers (A,B) Laplace\ninf\nNaN\nlast layer only (B) Laplace\n1416.294288\n0.808991\nMAP\n1.981483\n0.889519\nlayers to be superior to the Laplace approximations and to the MAP. We further expect the\nlast-two-layers MCMC to have better generalisation than the last-layer-only MCMC since\nthe former is closer to the Bayes predictive distribution. In summary, we anticipate the\nfollowing performance order for these ﬁve approximate Bayesian schemes (from worst to best):\nlast-two-layers Laplace, last-layer-only Laplace, MAP, last-layer-only MCMC, last-two-layers\nMCMC.\nThe results displayed in Figure 1 are in line with our stated expectations above, except for\nthe surprise that the last-layer-only MCMC approximation is often superior to the last-two-\nlayers MCMC approximation. This may arise from the fact that MCMC ﬁnds the singular\nsetting in the last-two-layers more challenging. In Figure 1, we clarify the eﬀect of the\nnetwork architecture by varying the following factors: 1) either 1 or 5 layers in g, and 2)\nReLU or identity activation in h. Table 1 is a companion to Figure 1 and tabulates for each\napproximation scheme the slope of 1/n versus EnG(n), also known as the learning coeﬃcient.\nThe R2 corresponding to the linear ﬁt is also provided. In Appendix A.5, we also show the\ncorresponding results when 1) the data-generating mechanism and the assumed model do not\nsatisfy the condition of realisability and/or 2) the MAP estimate is obtained via minibatch\nstochastic gradient descent instead of batch gradient descent.\n6\nSimple functions and complex singularities\nIn singular models the RLCT may vary with the true distribution (in contrast to regular\nmodels) and in this section we examine this phenomenon in a simple example. As the true\ndistribution becomes more complicated relative to the supposed model, the singularities of\nthe analytic variety of true parameters should become simpler and hence the RLCT should\nincrease [Watanabe, 2009, §7.6]. Our experiments are inspired by [Watanabe, 2009, §7.2] where\ntanh(x) networks are considered and the true distribution (associated to the zero network) is\nheld ﬁxed while the number of hidden nodes is increased.\nConsider the model p(y|x, w) in (3.2) where f(x, w) = c + PH\ni=1 qi ReLU(⟨wi, x⟩+ bi) is a\ntwo-layer ReLU network with weight vector w = ({wi}H\ni=1, {bi}H\ni=1, {qi}H\ni=1, c) ∈R4H+1 and\nwi ∈R2, bi ∈R, qi ∈R for 1 ≤i ≤H. We let W be some compact neighborhood of the origin.\nGiven an integer 3 ≤m ≤H we deﬁne a network sm ∈W and qm(y|x) := p(y|x, sm) as\nfollows. Let g ∈SO(2) stand for rotation by 2π/m, set w1 = √g (1, 0)T . The components\nof sm are the vectors wi = gi−1w1 for 1 ≤i ≤m and wi = 0 for i > m, bi = −1\n3 and qi = 1\nfor 1 ≤i ≤m and bi = qi = 0 for i > m, and ﬁnally c = 0. The factor of 1\n3 ensures the\nrelevant parts of the decision boundaries lie within X = [−1, 1]2. We let q(x) be the uniform\n8\nFigure 2: Increasingly complicated true distributions qm(x, y) on [−1, 1]2 × R.\nTable 2: RLCT estimates for ReLU and SiLU networks. We observe the RLCT increasing as m increases,\ni.e., the true distribution becomes more “complicated” relative to the supposed model.\nm\nNonlinearity\nRLCT\nStd\nR squared\n3\nReLU\n0.526301\n0.027181\n0.983850\n3\nSiLU\n0.522393\n0.026342\n0.978770\n4\nReLU\n0.539590\n0.024774\n0.991241\n4\nSiLU\n0.539387\n0.020769\n0.988495\n5\nReLU\n0.555303\n0.002344\n0.993092\n5\nSiLU\n0.555630\n0.021184\n0.990971\ndistribution on X and deﬁne qm(x, y) = qm(y|x)q(x). The functions f(x, sm) are graphed in\nFigure 2. It is intuitively clear that the complexity of these true distributions increases with\nm.\nWe let ϕ be a normal distribution N(0, 502) and estimate the RLCTs of the triples\n(p, qm, ϕ). We conducted the experiments with H = 5, n = 1000. For each m ∈{3, 4, 5},\nTable 2 shows the estimated RLCT. Algorithm 1 in Appendix A.3 details the estimation\nprocedure which we base on [Watanabe, 2013, Theorem 4]. As predicted the RLCT increases\nwith m verifying that in this case, the simpler true distributions give rise to more complex\nsingularities.\nNote that the dimension of W is d = 21 and so if the model were regular the RLCT would\nbe 10.5. It can be shown that when m = H the set of true parameters W0 ⊆W is a regular\nsubmanifold of dimension m. If such a model were minimally singular its RLCT would be\n1\n2((4m + 1) −m) = 1\n2(3m + 1). In the case m = 5 we observe an RLCT more than an order\nof magnitude less than the value 8 predicted by this formula. So the function K does not\nbehave like a quadratic form near W0.\nStrictly speaking it is incorrect to speak of the RLCT of a ReLU network because the\nfunction K(w) is not necessarily analytic (Example 4). However we observe empirically\nthat the predicted linear relationship between Eβ\nw[nLn(w)] and 1/β holds in our small ReLU\nnetworks (see the R2 values in Table 2) and that the RLCT estimates are close to those for\nthe two-layer SiLU network [Hendrycks and Gimpel, 2016] which is analytic (the SiLU or\nsigmoid weighted linear unit is σ(x) = x(1 + e−τx)−1 which approaches the ReLU as τ →∞.\nWe use τ = 100.0 in our experiments). The competitive performance of SiLU on standard\nbenchmarks [Ramachandran et al., 2017] shows that the non-analyticity of ReLU is probably\nnot fundamental.\n9\n7\nFuture directions\nDeep neural networks are singular models, and that’s good: the presence of singularities is\nnecessary for neural networks with large numbers of parameters to have low generalisation\nerror. Singular learning theory clariﬁes how classical tools such as the Laplace approximation\nare not just inappropriate in deep learning on narrow technical grounds: the failure of this\napproximation and the existence of interesting phenomena like the generalisation puzzle have\na common cause, namely the existence of degenerate critical points of the KL function K(w).\nSingular learning theory is a promising foundation for a mathematical theory of deep learning.\nHowever, much remains to be done. The important open problems include:\nSGD vs the posterior.\nA number of works [ŞimŠekli, 2017, Mandt et al., 2017, Smith\net al., 2018] suggest that mini-batch SGD may be governed by SDEs that have the posterior\ndistribution as its stationary distribution and this may go towards understanding why SGD\nworks so well for DNNs.\nRLCT estimation for large networks.\nTheoretical RLCTs have been cataloged for small\nneural networks, albeit at signiﬁcant eﬀort3 [Aoyagi and Watanabe, 2005a,b]. We believe\nRLCT estimation in these small networks should be standard benchmarks for any method\nthat purports to approximate the Bayesian posterior of a neural network. No theoretical\nRLCTs or estimation procedure are known for modern DNNs. Although MCMC provides\nthe gold standard it does not scale to large networks. The intractability of RLCT estimation\nfor DNNs is not necessarily an obstacle to reaping the insights oﬀered by singular learning\ntheory. For instance, used in the context of model selection, the exact value of the RLCT is\nnot as important as model selection consistency. We also demonstrated the utility of singular\nlearning results such as (5.2) and (5.3) which can be exploited even without knowledge of the\nexact value of the RLCT.\nReal-world distributions are unrealisable.\nThe existence of power laws in neural lan-\nguage model training [Hestness et al., 2017, Kaplan et al., 2020] is one of the most remarkable\nexperimental results in deep learning. These power laws may be a sign of interesting new\nphenomena in singular learning theory when the true distribution is unrealisable.\nReferences\nShun-ichi Amari, Tomoko Ozeki, and Hyeyoung Park. Learning and inference in hierarchical\nmodels with singularities. Systems and Computers in Japan, 34(7):34–42, 2003.\nSumio Watanabe. Almost All Learning Machines are Singular. In 2007 IEEE Symposium on\nFoundations of Computational Intelligence, pages 383–388, 2007.\nSumio Watanabe. Algebraic Geometry and Statistical Learning Theory. Cambridge University\nPress, USA, 2009.\nSamuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic\ngradient descent. arXiv preprint arXiv:1710.06451, 2017.\nYao Zhang, Andrew M. Saxe, Madhu S. Advani, and Alpha A. Lee. Energy-entropy competition\nand the eﬀectiveness of stochastic gradient descent in machine learning. Molecular Physics,\n116(21-22):3214–3223, 2018.\n3Hironaka’s resolution of singularities guarantees existence. However it is diﬃcult to do the required blowup\ntransformations in high dimensions to obtain the standard form.\n10\nOlivier Bousquet, Stéphane Boucheron, and Gábor Lugosi. Introduction to statistical learning\ntheory. In Summer School on Machine Learning, pages 169–207. Springer, 2003.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-\ning deep learning requires rethinking generalization. In Proceedings of the 5th International\nConference on Learning Representations, 2017. arXiv: 1611.03530.\nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably\noptimizes over-parameterized neural networks. In International Conference on Learning\nRepresentations, 2018.\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via\nover-parameterization. In International Conference on Machine Learning, pages 242–252,\n2019a.\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in\nneural networks. In Conference on Learning Theory, pages 1376–1401, 2015.\nPeter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin\nbounds for neural networks. In Advances in Neural Information Processing Systems, pages\n6240–6249, 2017.\nBehnam Neyshabur and Zhiyuan Li. Towards understanding the role of over-parametrization in\ngeneralization of neural networks. In International Conference on Learning Representations\n(ICLR), 2019.\nSanjeev Arora, R Ge, B Neyshabur, and Y Zhang. Stronger generalization bounds for deep\nnets via a compression approach. In 35th International Conference on Machine Learning,\nICML 2018, 2018.\nAlon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-\nparameterized networks that provably generalize on linearly separable data. In International\nConference on Learning Representations, 2018.\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic\ngradient descent on structured data. In Advances in Neural Information Processing Systems,\npages 8157–8166, 2018.\nZeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overpa-\nrameterized neural networks, going beyond two layers. In Advances in Neural Information\nProcessing Systems, pages 6155–6166, 2019b.\nAmit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural\nInformation Processing Systems, pages 2422–2430, 2017.\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis\nof optimization and generalization for overparameterized two-layer neural networks. In\nInternational Conference on Machine Learning, pages 322–332, 2019.\nGilad Yehudai and Ohad Shamir. On the power and limitations of random features for\nunderstanding neural networks. In Advances in Neural Information Processing Systems,\npages 6594–6604, 2019.\nYuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide\nand deep neural networks. In Advances in Neural Information Processing Systems, pages\n10835–10845, 2019.\n11\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. In Advances in Neural Information Processing Systems,\npages 8571–8580, 2018.\nSumio Watanabe. Mathematical Theory of Bayesian Statistics. CRC Press, 2018.\nSumio Watanabe. A Widely Applicable Bayesian Information Criterion. Journal of Machine\nLearning Research, 14:867–897, 2013.\nGeoﬀrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing\nthe description length of the weights. In Proceedings of the sixth annual conference on\nComputational learning theory, pages 5–13, 1993.\nSepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42,\n1997.\nPratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian\nBorgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing\ngradient descent into wide valleys. In International Conference on Learning Representations,\n2017.\nStanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua\nBengio, and Amos Storkey. Three factors inﬂuencing minima in SGD. arXiv preprint\narXiv:1711.04623, 2017.\nVijay Balasubramanian. Statistical inference, Occam’s razor and statistical mechanics on the\nspace of probability distributions. Neural Computation, 9(2):349–368, 1997.\nTomaso A. Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier\nBoix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning III: explaining the\nnon-overﬁtting puzzle. CoRR, abs/1801.00173, 2018.\nBehnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring\ngeneralization in deep learning. In Advances in neural information processing systems, pages\n5947–5956, 2017.\nValentin Thomas, Fabian Pedregosa, Bart van Merriënboer, Pierre-Antoine Mangazol, Yoshua\nBengio, and Nicolas Le Roux. Information matrices and generalization. arXiv:1906.07774\n[cs, stat], 2019. arXiv: 1906.07774.\nWesley J Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter\ncounting in deep models: Eﬀective dimensionality revisited. arXiv preprint arXiv:2003.02139,\n2020.\nShinichi Nakajima and Sumio Watanabe. Variational Bayes Solution of Linear Neural Networks\nand Its Generalization Performance. Neural Computation, 19(4):1112–53, 2007.\nAgustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being Bayesian, even just a bit,\nﬁxes overconﬁdence in ReLU networks. arXiv preprint arXiv:2002.10118, 2020.\nMatthew D Hoﬀman and Andrew Gelman. The No-U-Turn sampler: adaptively setting path\nlengths in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593–1623, 2014.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\narXiv:1606.08415, 2016.\n12\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Swish: a self-gated activation function.\narXiv preprint arXiv:1710.05941, 2017.\nUmut ŞimŠekli. Fractional Langevin Monte Carlo: exploring Levy driven stochastic diﬀerential\nequations for Markov Chain Monte Carlo. In Proceedings of the 34th International Conference\non Machine Learning-Volume 70, pages 3200–3209, 2017.\nStephan Mandt, Matthew D Hoﬀman, and David M Blei. Stochastic gradient descent as\napproximate Bayesian inference. The Journal of Machine Learning Research, 18(1):4873–\n4907, 2017.\nSamuel L Smith, Daniel Duckworth, Semon Rezchikov, Quoc V Le, and Jascha Sohl-Dickstein.\nStochastic natural gradient descent draws posterior samples in function space. arXiv preprint\narXiv:1806.09597, 2018.\nMiki Aoyagi and Sumio Watanabe. Stochastic complexities of reduced rank regression in\nBayesian estimation. Neural Networks, 18(7):924–933, 2005a.\nMiki Aoyagi and Sumio Watanabe. Resolution of Singularities and the Generalization Error\nwith Bayesian Estimation for Layered Neural Network. In IEICE Trans., pages 2112–2124,\n2005b.\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\nis predictable, empirically. CoRR, abs/1712.00409, 2017.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models. arXiv preprint arXiv:2001.08361, 2020.\nLevent Sagun, Léon Bottou, and Yann LeCun. Singularity of the Hessian in deep learning.\nCoRR, abs/1611.07476, 2016.\nJeﬀrey Pennington and Pratik Worah. The spectrum of the Fisher information matrix of a\nsingle-hidden-layer neural network. In Advances in Neural Information Processing Systems,\npages 5410–5419, 2018.\nMary Phuong and Christoph H. Lampert. Functional vs. parametric equivalence of ReLU\nnetworks. In International Conference on Learning Representations, 2020.\nWilliam M Boothby. An introduction to diﬀerentiable manifolds and Riemannian geometry.\nAcademic press, 1986.\nA\nAppendix\nA.1\nNeural networks are strictly singular\nMany-layered neural networks are strictly singular [Watanabe, 2009, §7.2]. The degeneracy\nof the Hessian in deep learning has certainly been acknowledged in e.g., Sagun et al. [2016]\nwhich recognises the eigenspectrum is concentrated around zero and in Pennington and Worah\n[2018] which deliberately studies the Fisher information matrix of a single-hidden-layer, rather\nthan multilayer, neural network.\n13\nWe ﬁrst explain how to think about a neural network in the context of singular learning\ntheory. A feedforward network of depth c parametrises a function f : RN −→RM of the form\nf = Ac ◦σc−1 ◦Ac−1 · · · σ1 ◦A1\nwhere the Al : Rdl−1 −→Rdl are aﬃne functions and σl : Rdl −→Rdl is coordinate-wise\nsome ﬁxed nonlinearity σ : R −→R.\nLet W be a compact subspace of Rd containing\nthe origin, where Rd is the space of sequences of aﬃne functions (Al)c\nl=1 with coordinates\ndenoted w1, . . . , wd so that f may be viewed as a function f : RN × W −→RM. We deﬁne\np(y|x, w) as in (3.2).\nWe assume the true distribution is realisable, q(y|x) = p(y|x, w0)\nand that a distribution q(x) on RN is ﬁxed with respect to which p(x, y) = p(y|x)q(x) and\nq(x, y) = q(y|x)q(x). Given some prior ϕ(w) on W we may apply singular learning theory to\nthe triplet (p, q, ϕ).\nBy straightforward calculations we obtain\nK(w) = 1\n2\nZ\n∥f(x, w) −f(x, w0)∥2q(x)dx\n(A.1)\n∂2\n∂wi∂wj K(w) =\nZ D\n∂\n∂wi f(x, w),\n∂\n∂wj f(x, w)\nE\nq(x)dx\n+\nZ D\nf(x, w) −f(x, w0),\n∂2\n∂wi∂wj f(x, w)\nE\nq(x)dx\n(A.2)\nI(w)ij =\n1\n2(M−3)/2π(M−2)/2\nZ D\n∂\n∂wi f(x, w),\n∂\n∂wj f(x, w)\nE\nq(x)dx\n(A.3)\nwhere ⟨−, −⟩is the dot product. We assume q(x) is such that these integrals exist.\nIt will be convenient below to introduce another set of coordinates for W. Let wl\njk denote\nthe weight from the kth neuron in the (l −1)th layer to the jth neuron in the lth layer and\nlet bl\nj denote the bias of the jth neuron in the lth layer. Here 1 ≤l ≤c and the input is\nlayer zero. Let ul\nj and al\nj denote the value of the jth neuron in the lth layer before and after\nactivation, respectively. Let ul and al denote the vectors with values ul\nj and al\nj, respectively.\nLet dl denote the number of neurons in the lth layer. Then\nul\nj =\ndl−1\nX\nk=1\nwl\njkal−1\nk\n+ bl\nj,\n1 ≤l ≤c, 1 ≤j ≤dl\nal\nj = σ(ul\nj)\n1 ≤l < c, 1 ≤j ≤dl\nwith the convention that a0 = x is the input and uc = y is the output.\nIn the case where σ = ReLU the partial derivatives\n∂\n∂wj f do not exist on all of RN.\nHowever given w ∈W we let D(w) denote the complement in RN of the union over all hidden\nnodes of the associated decision boundary, that is\nRN \\ D(w) =\n[\n1≤l<c\n[\n1≤j≤dl\n{x ∈RN : ul\nj(x) = 0} .\nThe partial derivative\n∂\n∂wj f exists on the open subset {(x, w) : x ∈D(w)} of RN × W.\nLemma 1. Suppose σ = ReLU and there are c > 1 layers. For any hidden neuron 1 ≤j ≤dl\nin layer l with 1 ≤l < c there is a diﬀerential equation\nn dl−1\nX\nk=1\nwl\njk\n∂\n∂wl\njk\n+ bl\nj\n∂\n∂bl\nj\n−\ndl+1\nX\ni=1\nwl+1\nij\n∂\n∂wl+1\nij\no\nf = 0\n14\nwhich holds on D(w) for any ﬁxed w ∈W.\nProof. Without loss of generality assume M = 1, to simplify the notation. Let ei ∈Rdl+1\ndenote a unit vector and let H(x) =\nd\ndx ReLU(x). Writing\n∂f\n∂ul+1 for a gradient vector\n∂f\n∂wl+1\nij\n=\nD ∂f\n∂ul+1 , ∂ul+1\n∂wl+1\nij\nE\n=\nD ∂f\n∂ul+1 , al\njei\nE\n=\n∂f\n∂ul+1\ni\nul\njH(ul\nj)\n∂f\n∂wl\njk\n=\nD ∂f\n∂ul+1 , ∂ul+1\n∂wl\njk\nE\n=\nD ∂f\n∂ul+1 ,\ndl+1\nX\ni=1\nwl+1\nij al−1\nk\nH(ul\nj)ei\nE\n=\ndl+1\nX\ni=1\n∂f\n∂ul+1\ni\nwl+1\nij al−1\nk\nH(ul\nj)\n∂f\n∂bl\nj\n=\nD ∂f\n∂ul+1 , ∂ul+1\n∂bl\nj\nE\n=\nD ∂f\n∂ul+1 ,\ndl+1\nX\ni=1\nwl+1\nij H(ul\nj)ei\nE\n=\ndl+1\nX\ni=1\n∂f\n∂ul+1\ni\nwl+1\nij H(ul\nj).\nThe claim immediately follows.\nLemma 2. Suppose σ = ReLU, c > 1 and that w ∈W has at least one weight or bias at a\nhidden node nonzero. Then the matrix I(w) is degenerate and if w ∈W0 then the Hessian of\nK at w is also degenerate.\nProof. Let w ∈W be given, and choose a hidden node where at least one of the incident\nweights (or bias) is nonzero. Then Lemma 1 gives a nontrivial linear dependence relation\nP\ni λi ∂\n∂wi f = 0 as functions on D(w). The rows of I(w) satisfy the same linear dependence\nrelation. At a true parameter the second summand in (A.2) vanishes so by the same argument\nthe Hessian is degenerate.\nRemark 3. Lemma 2 implies that every true parameter for a nontrivial ReLU network is a\ndegenerate critical point of K. Hence in the study of nontrivial ReLU networks it is never\nappropriate to divide by the determinant of the Hessian of K at a true parameter, and in\nparticular Laplace or saddle-point approximations at a true parameter are invalid.\nThe well-known positive scale invariance of ReLU networks [Phuong and Lampert, 2020] is\nresponsible for the linear dependence of Lemma 1, in the precise sense that the given diﬀerential\noperator is the inﬁnitesimal generator [Boothby, 1986, §IV.3] of the scaling symmetry. However,\nthis is only one source of degeneracy or singularity in ReLU networks. The degeneracy, as\nmeasured by the RLCT, is much lower than one would expect on the basis of this symmetry\nalone (see Section 6).\nExample 4. In general the KL function K(w) for ReLU networks is not analytic. For the\nminimal counterexample, let q(x) be uniform on [−N, N] and zero outside and consider\nK(b) =\nZ\nq(x)(ReLU(x −b) −ReLU(x))2dx .\nIt is easy to check that up to a scalar factor\nK(b) =\n(\n−2\n3b3 + b2N\n0 ≤b ≤N\n−1\n3b3 + b2N\n−N ≤b ≤0\nso that K is C2 but not C3 let alone analytic.\n15\nA.2\nReduced rank regression\nFor reduced rank regression, the model is\np(y|x, w) =\n1\n(2πσ2)N/2 exp\n\u0012\n−1\n2σ2 |y −BAx|2\n\u0013\n,\nwhere x ∈RM, y ∈RN, A an M × H matrix and B an H × N matrix; the parameter w\ndenotes the entries of A and B, i.e. w = (A, B), and σ > 0 is a parameter which for the\nmoment is irrelevant.\nIf the true distribution is realisable then there is w0 = (A0, B0) such that q(y|x) =\np(y|x, w0). Without loss of generality assume q(x) is the uniform density. In this case the KL\ndivergence from p(y|x, w) to q(y|x) is\nK(w) =\nZ\nq(y|x) log\nq(y|x)\np(y|x, w)dxdy = ∥BA −B0A0∥2 (1 + E(w))\nwhere the error E is smooth and E(w) = O(∥BA −B0A0∥2) in any region where ∥BA −\nB0A0∥< C, so K(w) is equivalent to ∥BA −B0A0∥2. We write K(w) = ∥BA −B0A0∥2 for\nsimplicity below.\nNow assume that B0A0 is symmetric and that B0 is square, i.e. N = H. Then the zero\nlocus of K(w) is explicitly given as follows\nW0 = {(A, B) : det B ̸= 0 and A = B−1B0A0}.\nIt follows that W0 is globally a graph over GL(H; R). Indeed, the set (B−1B0A0, B) with B ∈\nGL(H; R) is exactly W0. Thus W0 is a smooth H2-dimensional submanifold of RH2 × RH×M.\nTo prove that W0 is minimally singular in the sense of Section 4 it suﬃces to show that\nrank(D2\nA,BK) ≥HM where D2\nA,BK denotes the Hessian, but as it is no more diﬃcult to do\nso, we ﬁnd explicit local coordinates (u, v) near an arbitrary point (A, B) ∈W0 for which\n{v = 0} = W0 and K(u, v) = a(u, v)|u|2 in this neighborhood, where a is a C∞function with\na ≥c > 0 for some c. Write\nA(v) = (B + v)−1B0A0.\nThen u, v 7→(A(v) + u, B + v) gives local coordinates on RH2 × RH×M near (A, B), and\nK(u, v) = |(B + v)((B + v)−1B0A0 + u) −B0A0|\n= |B0A0 + (B + v)u −B0A0|2\n= |(B + v)u|2,\nso for v suﬃciently small (and hence B + v invertible) we can take a(u, v) = |(B + v)u|2/|u|2.\nA.3\nRLCT estimation\nIn this section we detail the estimation procedure for the RLCT used in Section 6. Let Ln(w)\nbe the negative log likelihood as in (3.3). Deﬁne the data likelihood at inverse temperature\nβ > 0 to be pβ(Dn|w) = Πn\ni=1p(yi|xi, w)β which can also be written\npβ(Dn|w) = exp(−βnLn(w)).\n(A.4)\nThe posterior distribution, at inverse temperature β, is deﬁned as\npβ(w|Dn) =\nΠn\ni=1p(yi|xi, w)βϕ(w)\nR\nW Πn\ni=1p(yi|xi, w)βϕ(w) = pβ(Dn|w)ϕ(w)\npβ(Dn)\n(A.5)\n16\nAlgorithm 1 RLCT via Theorem 4 in Watanabe [2013]\nInput: range of β’s, set of training sets T each of size n, approximate samples {w1, . . . , wR}\nfrom pβ(w|Dn) for each training set Dn and each β\nfor training set Dn ∈T do\nfor β in range of β’s do\nApproximate Eβ\nw[nLn(w)] with\n1\nR\nPR\ni=1 nLn(wr) where w1, . . . , wR are approximate\nsamples from pβ(w|Dn)\nend for\nPerform generalised least squares to ﬁt λ in (A.8), call result ˆλ(Dn)\nend for\nOutput:\n1\n|T |\nP\nDn∈T ˆλ(Dn)\nwhere ϕ is the prior distribution on the network weights w and\npβ(Dn) =\nZ\nW\npβ(Dn|w)ϕ(w) dw\n(A.6)\nis the marginal likelihood of the data at inverse temperature β. Finally, denote the expectation\nof a random variable R(w) with respect to the tempered posterior pβ(w|Dn) as\nEβ\nw[R(w)] =\nZ\nW\nR(w)pβ(w|Dn) dw\n(A.7)\nIn the main text, we drop the superscript in the quantities (A.4), (A.5), (A.6), (A.7) when\nβ = 1, e.g., p(Dn) rather than p1(Dn).\nAssuming the conditions of Theorem 4 in Watanabe [2013] hold, we have\nEβ\nw[nLn(w)] = nLn(w0) + λ\nβ + Un\ns\nλ\n2β + Op(1)\n(A.8)\nwhere β0 is a positive constant and Un is a sequence of random variables satisfying EnUn = 0.\nIn Algorithm 1, we describe an estimation procedure for the RLCT based on the asymptotic\nresult in (A.8).\nFor the estimates in Table 2 the a posteriori distribution was approximated using the\nNUTS variant of Hamiltonian Monte Carlo [Hoﬀman and Gelman, 2014] where the ﬁrst 1000\nsteps were omitted and 20, 000 samples were collected. Each ˆλ(Dn) estimate in Algorithm\n1 was performed by linear regression on the pairs {(1/βi, Eβi\nw [nLn(w)])}5\ni=1 where the ﬁve\ninverse temperatures βi are centered on the inverse temperature 1/ log(20000).\nA.4\nConnection between RLCT and generalisation\nFor completeness, we sketch the derivation of (5.2) which gives the asymptotic expansion\nof the average generalisation error EnG(n) of the Bayes prediction distribution in singular\nmodels. The exposition is an amalgamation of various works published by Sumio Watanabe,\nbut is mostly based on the textbook [Watanabe, 2009].\nTo understand the connection between the RLCT and G(n), we ﬁrst deﬁne the so-called\nBayes free energy as\nF(n) = −log p(Dn)\nwhose expectation admits the following asymptotic expansion [Watanabe, 2009]:\nEnF(n) = EnnSn + λ log n + o(log n)\n17\nwhere Sn = −1\nn\nPn\ni=1 log q(yi|xi) is the entropy. The expected Bayesian generalisation error\nis related to the Bayes free energy as follows\nEnG(n) = E F(n + 1) −E F(n)\nThen for the average generalisation error, we have\nEnG(n) = λ/n + o(1/n).\n(A.9)\nSince models with more complex singularities have smaller RLCTs, this would suggest that\nthe more singular a model is, the better its generalisation (assuming one uses the Bayesian\npredictive distribution for prediction). In this connection it is interesting to note that simpler\n(relative to the model) true distributions lead to more singular models (Section 6).\nA.5\nDetails for generalisation error experiments\nSimulated data.\nThe distribution of x ∈R3 is set to q(x) = N(0, I3). In the realisable\ncase, y ∈R3 is drawn according to q(y|x) = p(y|x, θ0). In the nonrealisable setting, we set\nq(y|x) ∝exp{−||y −hw0(x)||2/2}, where w0 = (A0, B0) is drawn according to the PyTorch\nmodel initialisation of h.\nMAP training.\nThe MAP estimator is found via gradient descent using the mean-squared-\nerror loss with either the full data set or minibatch set to 32. Training was set to 5000 epochs.\nNo form of early stopping was employed.\nCalculating the generalisation error.\nUsing a held-out-test set Tn′ = {(x′\ni, y′\ni)}n′\ni=1, we\ncalculate the average generalisation error as\n1\nn′\nn′\nX\ni=1\nlog q(y′\ni|x′\ni) −En\n1\nn′\nn′\nX\ni=1\nlog ˆqn(y′\ni|x′\ni)\n(A.10)\nAssume the held-out test set is large enough so that the diﬀerence between EnG(n) and (A.10)\nis negligible. We will refer to them interchangeably as the average generalisation error. In our\nexperiments we use n′ = 10, 000 and 30 draws of the dataset Dn to estimate En.\nLast layer(s) inference.\nWithout loss of generality, we discuss performing inference in\nthe w parameters of h while freezing the parameters of g at the MAP estimate. The steps\neasily extend to performing inference over the ﬁnal layer only of f = h ◦g. Let ˜xi = gvMAP(xi).\nDeﬁne a new transformed dataset ˜Dn = {(˜xi, yi)}n\ni=1. We take the prior on w to be standard\nGaussian. Deﬁne the posterior over w given ˜Dn as:\np(w| ˜Dn) ∝p( ˜Dn|w)ϕ(w) = Πn\ni=1 exp{−||yi −hw(˜xi)||2/2}ϕ(w)\n(A.11)\nDeﬁne the following approximation to the Bayesian predictive distribution\n˜p(y|x, Dn) =\nZ\np(y|x, (vMAP, w))p(w| ˜Dn) dw.\nLet w1, . . . , wR be some approximate samples from p(w| ˜Dn). Then we approximate ˜p(y|x, Dn)\nwith\n1\nR\nR\nX\nr=1\np(y|x, (vMAP, wr))\nwhere R is a large number, set to 1000 in our experiments.\nWe consider the Laplace\napproximation and the NUTS variant of HMC for drawing samples from p(w| ˜Dn):\n18\n• Laplace in the last layer(s) Recall θMAP = (vMAP, wMAP) is the MAP estimate for fθ\ntrained with the data Dn. With the Laplace approximation, we draw w1, . . . wR from the\nGaussian\nN(wMAP, Σ)\nwhere Σ = (−∇2 log p(w| ˜Dn)|wMAP)−1 is the inverse Hessian4 of the negative log posterior\nevaluated at the MAP estimate of the mode.\n• MCMC in the last layer(s) We used the NUTS variant of HMC to draw samples from\n(A.11) with the ﬁrst 1000 samples discarded.. Our implementation used the pyro package\nin PyTorch.\n4Following Kristiadi et al. [2020], the code for the exact Hessian calculation is borrowed from https:\n//github.com/f-dangel/hbp\n19\nFigure 3: Realisable and minibatch gradient descent for MAP training.\nTable 3: Companion to Figure 3.\n(a) 1 hidden layer(s) in g, identity activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n36.721594\n0.992839\nlast layer only (B) MCMC\n20.676920\n0.983695\nlast two layers (A,B) Laplace\ninf\nNaN\nlast layer only (B) Laplace\n1768.655088\n0.838035\nMAP\ninf\nNaN\n(b) 5 hidden layer(s) in g, identity activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n13.729278\n0.924049\nlast layer only (B) MCMC\n9.170642\n0.945613\nlast two layers (A,B) Laplace\ninf\nNaN\nlast layer only (B) Laplace\n1943.793236\n0.794679\nMAP\n14.123308\n0.917502\n(c) 1 hidden layer(s) in g, ReLU activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n22.175448\n0.975450\nlast layer only (B) MCMC\n10.675455\n0.968584\nlast two layers (A,B) Laplace\ninf\nNaN\nlast layer only (B) Laplace\ninf\nNaN\nMAP\n35.647464\n0.983284\n(d) 5 hidden layer(s) in g, ReLU activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n4.652483\n0.922693\nlast layer only (B) MCMC\n3.533366\n0.862125\nlast two layers (A,B) Laplace\ninf\nNaN\nlast layer only (B) Laplace\n1004.852367\n0.901899\nMAP\n6.256696\n0.940437\n20\nFigure 4: Nonrealisable and full batch gradient descent for MAP training.\nTable 4: Companion to Figure 4. The learning coeﬃcient is the slope of the linear ﬁt 1/n versus\nEn G(n) (with intercept since nonrealisable).\n(a) 1 hidden layer(s) in g, identity activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n11.086023\n0.969991\nlast layer only (B) MCMC\n7.377871\n0.957824\nlast two layers (A,B) Laplace\nNaN\nNaN\nlast layer only (B) Laplace\n30.692954\n0.029238\nMAP\n12.947959\n0.970173\n(b) 5 hidden layer(s) in g, identity activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n0.808601\n0.144260\nlast layer only (B) MCMC\n0.799114\n0.127686\nlast two layers (A,B) Laplace\nNaN\nNaN\nlast layer only (B) Laplace\n-33.817429\n0.009074\nMAP\n1.204743\n0.242671\n(c) 1 hidden layer(s) in g, ReLU activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n5.987187\n0.848490\nlast layer only (B) MCMC\n5.384686\n0.801313\nlast two layers (A,B) Laplace\nNaN\nNaN\nlast layer only (B) Laplace\n38.629167\n0.059012\nMAP\n8.560722\n0.816794\n(d) 5 hidden layer(s) in g, ReLU activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n0.794055\n0.088305\nlast layer only (B) MCMC\n1.141580\n0.162585\nlast two layers (A,B) Laplace\nNaN\nNaN\nlast layer only (B) Laplace\n-5.682602\n0.000365\nMAP\n1.648073\n0.284088\n21\nFigure 5: Nonrealisable and minibatch gradient descent for MAP training. Missing points on the\nMAP learning curve are due to estimated probabilities too close to 0.\nTable 5: Companion to Figure 5. The learning coeﬃcient is the slope of the linear ﬁt 1/n versus\nEn G(n) (with intercept since nonrealisable).\n(a) 1 hidden layer(s) in g, identity activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n11.086023\n0.969991\nlast layer only (B) MCMC\n7.377871\n0.957824\nlast two layers (A,B) Laplace\nNaN\nNaN\nlast layer only (B) Laplace\n30.692954\n0.029238\nMAP\n12.947959\n0.970173\n(b) 5 hidden layer(s) in g, identity activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n0.808601\n0.144260\nlast layer only (B) MCMC\n0.799114\n0.127686\nlast two layers (A,B) Laplace\nNaN\nNaN\nlast layer only (B) Laplace\n-33.817429\n0.009074\nMAP\n1.204743\n0.242671\n(c) 1 hidden layer(s) in g, ReLU activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n5.987187\n0.848490\nlast layer only (B) MCMC\n5.384686\n0.801313\nlast two layers (A,B) Laplace\nNaN\nNaN\nlast layer only (B) Laplace\n38.629167\n0.059012\nMAP\n8.560722\n0.816794\n(d) 5 hidden layer(s) in g, ReLU activation in h\nmethod\nlearning coeﬃcient\nR squared\nlast two layers (A,B) MCMC\n0.794055\n0.088305\nlast layer only (B) MCMC\n1.141580\n0.162585\nlast two layers (A,B) Laplace\nNaN\nNaN\nlast layer only (B) Laplace\n-5.682602\n0.000365\nMAP\n1.648073\n0.284088\n22\nFigure 6: Realisable and full batch gradient descent for MAP. average generalisation errors of Laplace\napproximations of the predictive distribution. The last-two-layers Laplace approximation results in\nnumerical instabilities due to degenerate Hessian. Any missing points are due to estimated probabilities\ntoo close to 0.\n23\nFigure 7: Realisable and minibatch gradient descent for MAP training. Details are same as for Figure\n6\n24\nFigure 8: Nonrealisable and full batch gradient descent for MAP training. Details are same as for\nFigure 6\n25\nFigure 9: Nonrealisable and minibatch gradient descent for MAP training. Details are same as for\nFigure 6\n26\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-10-22",
  "updated": "2020-10-22"
}