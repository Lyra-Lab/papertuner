{
  "id": "http://arxiv.org/abs/2010.11738v1",
  "title": "Optimising Stochastic Routing for Taxi Fleets with Model Enhanced Reinforcement Learning",
  "authors": [
    "Shen Ren",
    "Qianxiao Li",
    "Liye Zhang",
    "Zheng Qin",
    "Bo Yang"
  ],
  "abstract": "The future of mobility-as-a-Service (Maas)should embrace an integrated system\nof ride-hailing, street-hailing and ride-sharing with optimised intelligent\nvehicle routing in response to a real-time, stochastic demand pattern. We aim\nto optimise routing policies for a large fleet of vehicles for street-hailing\nservices, given a stochastic demand pattern in small to medium-sized road\nnetworks. A model-based dispatch algorithm, a high performance model-free\nreinforcement learning based algorithm and a novel hybrid algorithm combining\nthe benefits of both the top-down approach and the model-free reinforcement\nlearning have been proposed to route the \\emph{vacant} vehicles. We design our\nreinforcement learning based routing algorithm using proximal policy\noptimisation and combined intrinsic and extrinsic rewards to strike a balance\nbetween exploration and exploitation. Using a large-scale agent-based\nmicroscopic simulation platform to evaluate our proposed algorithms, our\nmodel-free reinforcement learning and hybrid algorithm show excellent\nperformance on both artificial road network and community-based Singapore road\nnetwork with empirical demands, and our hybrid algorithm can significantly\naccelerate the model-free learner in the process of learning.",
  "text": "OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS\nWITH MODEL ENHANCED REINFORCEMENT LEARNING\nSHEN REN, QIANXIAO LI, LIYE ZHANG, ZHENG QIN, AND BO YANG\nAbstract. The future of mobility-as-a-Service (Maas) should embrace an in-\ntegrated system of ride-hailing, street-hailing and ride-sharing with optimised\nintelligent vehicle routing in response to a real-time, stochastic demand pat-\ntern.\nWe aim to optimise routing policies for a large ﬂeet of vehicles for\nstreet-hailing services, given a stochastic demand pattern in small to medium-\nsized road networks. A model-based dispatch algorithm, a high performance\nmodel-free reinforcement learning based algorithm and a novel hybrid algo-\nrithm combining the beneﬁts of both the top-down approach and the model-\nfree reinforcement learning have been proposed to route the vacant vehicles.\nWe design our reinforcement learning based routing algorithm using proxi-\nmal policy optimisation and combined intrinsic and extrinsic rewards to strike\na balance between exploration and exploitation.\nUsing a large-scale agent-\nbased microscopic simulation platform to evaluate our proposed algorithms,\nour model-free reinforcement learning and hybrid algorithm show excellent\nperformance on both artiﬁcial road network and community-based Singapore\nroad network with empirical demands, and our hybrid algorithm can signiﬁ-\ncantly accelerate the model-free learner in the process of learning.\n1. Statement of Contribution/Potential Impact\nIn this paper, we propose methodologies for optimising vacant vehicle routing\nin a decentralised way to meet stochastic spatio-temporal demand patterns. The\nresearch is motivated by the grand challenge of urban mobility to meet increasing\ndemands for highly eﬃcient yet high-quality transport services with minimal costs.\nThe diﬃculty of the problem stems from the stochastic spatio-temporal distribution\nof the commuter demand, on a large road network with complex structures. We\ndesign a set of methodologies for intelligent routing of taxi ﬂeet with reinforcement\nlearning primed with a top-down dispatch model, evaluated on both synthetic and\nempirical data for commuter demand patterns and road networks. The proposed\nmethodologies can potentially beneﬁt everyone in tackling the urban mobility chal-\nlenges: shorter waiting time for commuters, higher proﬁts for service providers,\neasy-to-implement and versatile solutions for government agencies, and less emis-\nsions to the environment. The methodologies presented in this paper are in time for\n(Shen Ren, Liye Zhang, Zheng Qin) Institute of High Performance Computing, A*STAR,\nSingapore\n(Qianxiao Li) Department of Mathematics, National University of Singapore, Singa-\npore\n(Bo Yang) School of Physical and Mathematical Sciences, Nanyang Technological\nUniversity, Singapore\nE-mail addresses: renshen@ihpc.a-star.edu.sg, matlq@nus.edu.sg,\nzhangly@ihpc.a-star.edu.sg, qinz@ihpc.a-star.edu.sg, yang.bo@ntu.edu.sg.\nThe last author is the corresponding author.\n1\narXiv:2010.11738v1  [cs.LG]  22 Oct 2020\n2 OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nreal-world applications under current technology and in near future when wireless\ncommunication and autonomous vehicles enable new ways of taxi ﬂeet operation.\nThey could also have wider applications beyond taxi systems, for on-demand trans-\nportation and logistics services such as dynamic reallocation of shared resources\nand last-mile delivery.\n2. Introduction\nDiversiﬁed transportation services are needed in urban cities with high popula-\ntion density to meet the demands of commuters with high-quality services — to\ntransport from location A to B within designated time and with minimal costs. Tra-\nditional public transports including scheduled trains and buses serve large groups of\ncommuters simultaneously, but follow ﬁxed routes and pickup/drop-oﬀlocations on\nscheduled time, mostly in central areas where demand is high. Private vehicles on\nthe other hand provide highly ﬂexible and customised services for short to medium\ndistance travel requests. But the utilisation rate for private vehicles is low, lead-\ning to high cost and increasing environmental concerns. The scalability for private\nvehicles is also highly constrained especially in metropolitan cities due to limited\nparking space and traﬃc congestion.\nIn recent decades, there have been substantial behavioural changes in using mo-\nbility as a service in urban cities. These include shared vehicles, bridging buses\nand real-time, on-demand shuttles. These bridging services ﬁll the gap between\nscheduled public transports and private vehicles, that can balance between service\neﬃciency and service quality. The idea of demand responsive transit (DRT) is not\nnew, but attracts great attentions in recent years for the purpose of eﬃcient city\nplanning and environmental concerns. While the ﬁrst DRT trial is implemented\nin Mansﬁeld, Ohio, USA in 1970 [38], there has been a resurgence of DRT ap-\nplications and trials recently throughout the world, especially in light of the new\ntechnological development including GPS, GIS solutions, tele-communications and\nsmart devices. Examples including trials at Nijmegen-Arnhem region and Amster-\ndam of the Netherlands in 2016[25] and 2017 [11], Inner East Sydney of Australia\nin 2018[39], Sutton and Ealing of UK in 2019 [16] and Singapore in 2018[24]. Mean-\nwhile, DRT applications have been especially popular in airport transportation [40],\nhealthcare [28], and disruption handling [23].\nWhile DRT systems have been on trial for more than 50 years, some signiﬁcant\nchallenges hindered a wide adoption of DRT systems in urban transportation ser-\nvices. Firstly, most of DRT systems operate under a Dial-A-Ride scheme, requiring\ncommuters to phone in with deterministic pickup and drop-oﬀrequests ahead of\ntime. Even with the pervasive use of smartphones, the requirement of the com-\nmuters to have advanced travel plans is in many cases a big constraint. Secondly,\nDRT systems are designed to complement the scheduled train and bus services.\nThus they often operate in relatively lower demand areas. Without an eﬃcient\nplanning, scheduling and routing algorithm, it is diﬃcult for DRT to provide high-\nquality services. Thirdly, DRT operating on large-scale complex networks requires\na high-level central management, operation, booking and tracking system. This in\nturn pushes up the operation cost, rendering it economically infeasible for many\npotential commuters.\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL 3\nOn the other hand, taxis as a form of on-demand services has been successfully\nserving urban cities for more than 100 years. The taxi services can cater to com-\nmuters without the necessity of advanced booking or planning. Street-hailing is\nalways a choice for people in need or people with unscheduled demands. The taxi\nsystem is also inherently distributive, since each taxi driver routes his/her vehi-\ncle independently to optimise proﬁts based on personal experience. Thus the taxi\nservice can achieve both lower operation costs and high ﬂexibility, as compared to\nother DRT services. However, there is still much room for improvement, since in\ngeneral the cruising time for taxi is still long compared to the hiring time [21] and\nat the same time, commuters can face diﬃculties hailing a taxi especially during\npeak hours [50]. This is mainly because the demand and supply of the taxi services\nare not optimised in terms of their spatio-temporal distributions, due to individ-\nual taxi driver’s lack of information on the real-time dynamics of the demand and\nother drivers’ behaviours. Under-utilisation of the taxis is also a major cause of\ntraﬃc congestion, with a lot of additional trips by vacant taxis looking for potential\ncommuters.\nThe future of “Mobility-as-a-Service” (MaaS) should embrace a combination of\nDRT system and taxi services — integrating ride-hailing, street-hailing and ride-\nsharing with highly-optimised intelligent vehicle routing to enhance service quality\nand to maintain cost-eﬀectiveness. Emerging technologies including wireless com-\nmunication and autonomous vehicles allow innovative ways of operating MaaS with\nnovel algorithms that were hard to implement in a traditional manner. Individual\nvehicles can thus be highly ﬂexible yet globally optimised to balance demand-and-\nsupply with new technologies expected in the not too distant future.\nIn this paper, we present our ﬁrst work towards next generation urban mobil-\nity with on-demand services. We focus on distributed vehicle routing algorithm of\nMaaS with stochastic spatio-temporal demand patterns. Our methodology com-\nbines both the top-down model construction, and the bottom up data-driven re-\ninforcement learning (RL) approach, to develop a turn-by-turn routing algorithm\nfor free agents (e.g. the vacant taxis) so as to eﬃciently seek potential customers\nto maximise jobs and to minimise waiting time of customers. The de-centralised\nnature of our method allows the supply of free agents to match the non-trivial sto-\nchastic demand over a complex network in a self-organised way, which can be easily\nscaled to large systems. The algorithm we develop can be applied to many trans-\nportation and logistics problems with stochastic requests such as dynamic shared\nbicycle re-allocation and last-mile delivery. Throughout this work, we will use the\ntaxi system as the example for both the development of the methodology and for\nthe agent based simulations.\nMore speciﬁcally, we start with the construction of a centralised dispatch algo-\nrithm for vacant taxis, which can dispatch them to hot-spot locations in the road\nnetwork where there is a higher probability of picking up potential commuters.\nFrom extensive microscopic agent based simulations, the dispatch algorithm is used\nto generate stochastic model-based turn-by-turn routing policies for each taxi. Both\nthe dispatch algorithm and the model-based policy are used as benchmarks for\nmodel-free turn-by-turn manoeuvring policies from state-of-the-art RL algorithm.\nThe latter leads to highly eﬃcient distributed routing strategies for vacant taxis\nand at the same time requires minimal global information about the commuter\ndemand, which is hard to obtain in realistic settings. A hybrid model-free RL with\n4 OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nmodel-based initialisation algorithm is then proposed, that incorporates the beneﬁts\nof both the model-free RL, and an optimised initial estimation from model-based\npolicies.\nNumerical experiments were conducted with agent-based modelling on\nartiﬁcial lattice road networks and community-based Singapore road network with\nboth synthetic and empirical demand patterns. Experiments showed the eﬀective-\nness of the proposed dispatch algorithm, model-based eﬀective policies, model-free\nRL and hybrid algorithm. Among them, the proposed hybrid algorithm showed\nbest performance in both artiﬁcial network and Singapore road network, and can\nsigniﬁcantly accelerate the model-free learner in the process of training.\nIn the following parts of this paper, we begin with related works reviewed in\nSec.3. The research problem is formulated in Sec.4 and the simulated environment\nand the Markov Decision Process are illustrated in Sec.5. Sec.6 gives a thorough\nexplanation on our proposed model-based methods, model-free RL and the hybrid\napproach.\nSec.7 shows a brief summary of our experimental design and results\nare presented in Sec.8. Finally, we discuss our results and concludes with future\noutlooks at Sec.9.\n3. Related Works\nVehicle routing problem is the general problem of dispatching a ﬂeet of vehicles\nto serve a given set of demands, ﬁrst proposed in [12]. Taxi routing as a special\ncase of vehicle routing problem, consists of assigning m homogeneous vehicles and\ndesigning routes for n given commuters with speciﬁed pickup and drop-oﬀrequests\nbetween origins and destinations. The street-hailing taxis and DRT routing problem\nhave long been studied under Dial-a-Ride Problem (DARP) [9].\nDepending on\nwhether all the requests are known beforehand or dynamically revealed during the\nservice period, DARPs can be divided into static and dynamic cases, in which\nthe vehicle routes are either pre-designed or real-time adjusted to meet demands.\nDepending on whether the information received at the time of decision is certain\nwithout imperfect information or further changes, DARP can be classiﬁed into\ndeterministic and stochastic cases [20]. The optimisation objectives of this problem\ncan be either minimising costs subject to full demand satisfaction or maximising\ndemand satisfactions subject to vehicle supplies [9].\nWithin the four categories, the dynamic stochastic multi-vehicle DARP, applied\nto ride-hailing and delivery services, is considered to be one of the most diﬃcult\nto solve, where the demands are revealed during service period and subject to\ndynamic traﬃc conditions, service request changes and cancellations. Various algo-\nrithms have been proposed to optimise service quality of real-life dynamic DARP\napplications while meeting the constraint of vehicle capacity, maximum route dura-\ntion and service time for small-sized instances, including heuristics and fuzzy-logic\n[29, 48], branch-and-bound [8], integrated oﬄine and online phases for new request\ninsertion [10], adaptive large neighbourhood search [18], hybrid tabu search and\nconstraint programming [2]. A prediction module is proposed to solve stochastic\ninformation in dynamic DARP, which is to use given imperfect information to pre-\ndict future scenarios needed for decision making [36, 34].\nSome variants of the\ndynamic DARP have also been investigated to understand the lower bound of com-\npetitiveness ratio, for example, Uber problem [13] and online k-taxi problem [7].\nIn recent years, emerging technologies including electrical vehicles and autonomous\nvehicles have opened up new opportunities and also imposed new challenges to the\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL 5\ntraditional DARP [35]. Some prospective studies have presented the formulation\nand algorithms for electric autonomous DARP on small instances [3], and for static\nautonomous DARP with ride-sharing variant and dynamic traﬃc [27].\nIn practice, taxi services includes both ride-hailing and street-hailing. The vacant\ntaxis route to maximise demand satisfaction with pickup drop-oﬀservice requests\nrevealed during service period either via ride-hailing (the taxi central management\nwill assign taxis for booking requests using various taxi assignment algorithms or\nheuristics) or via street-hailing instantaneously. The challenge of street-hailing is\nthat the demands are not explicitly phone-in by commuters but revealed only until\na vacant taxi happens to pass by the commuters’ origins before the commuters\nchange their minds. The existing solutions mainly consists of two steps: ﬁrst, to\nconstruct a model for demand prediction with a quantiﬁcation of uncertainty; then,\nto formulate the routing problem with prediction uncertainty as an optimisation\nproblem and solve by exact methods, heuristics and meta-heuristics [30, 15, 53]. A\nvery recent study presented centralised and decentralised dispatch algorithms for\nautonomous taxis that can serve both reserved requests and immediate requests\nafter departure using a network ﬂow model [14]. In comparison with this work, our\nmodel-free RL approach does not require pre-knowledge of the demand patterns\nfor immediate requests prediction, and our model-based approach does not require\nexact requests to be revealed to taxis. There are also a few studies apply RL and\nformulates the vacant taxi routing as a Markov Decision Process to plan for long-\nterm rewards [19, 49, 53], including our previous work [51]. Compared with the\nexisting studies and our previous work, we have signiﬁcantly enhanced our routing\nperformance by algorithm design and the hybrid approach allows for model-free\nRL ﬁne-tuning upon existing methods with better ﬁnal performance and signiﬁcant\nhigher sample-eﬃciency.\nThough RL has not been widely applied in vacant taxi routing, it is widely\nconsidered as a strong AI paradigm with successful applications in games [32, 44],\nrobotics, navigation [26, 55], natural language processing, computer vision, health-\ncare, smart grid, train rescheduling [43] and a lot of others. RL consists of a group\nof algorithms with their own advantages and trade-oﬀs for diﬀerent problems. A\nRL problem can be formulated with solution methods model-based or model-free,\nvalue-based or policy-based, on-policy or oﬀ-policy, with function approximation\nor not, and with sample backups or full backups.\nIn general, model-based RL\nmethod learns value function to model the environment that the agents interact\nwith, while model-free RL deals with unknown dynamical systems. Continuous at-\ntempts are proposed to combine the advantages of model-free and model-base RL\napproaches, including model predictive control [6] and guided policy search [26].\nVarious advanced approaches have also been proposed for reinforcement learning\nwith function approximators, including some successful applied algorithms like deep\nQ-learning [33], policy gradient method [31], trust region policy gradient method\n[41], and proximal policy optimisation [42]. The fundamental trade-oﬀof RL is\nexploration-exploitation. Designing better exploration strategy working for diﬀer-\nent RL schemes and problems is also an active area at the moment [37, 5].\n4. Problem Formulation\nIn our framework, an agent-based model simulates a ﬂeet of taxis roaming on\nthe road network based on a stochastic routing policy, picking up and delivering\n6 OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\ncommuters generated in the road network from their origins to their respective\ndestinations. The positions, behaviours and status of the taxis (vacant or occupied)\nare recorded and used as inputs for a separate RL algorithm, which is learned to\noptimise the routing policy in an iterative manner. The challenge in tackling this\ncomplex system with a large number of agents is to have a mathematical formulation\nthat satisﬁes several requirements. Firstly, the formulation has to be ﬂexible enough\nto easily incorporate realistic details of the transportation system.\nSecondly, it\nshould allow highly eﬃcient numerical simulations for reasonably large road network\ncontaining thousands of nodes. There are also generally up to thousands of taxis\nroaming in the network with sophisticated behaviours, delivering up to tens of\nthousands of commuters from their origins to destinations over each simulation\nperiod. Thirdly, the formulation needs to be fully integrated to RL algorithms,\nwhere the behaviours of the taxis as well as the “rewards” of their actions (to be\ndeﬁned more precisely later on) can be easily collected and analysed by the modern\nmachine learning algorithm.\nTo that end, we abstract the road network with a connected directed graph\ndenoted as G (N, E), where N represents the collection of nodes of the graph, and\nE is the collection of directed weighted edges. The nodes correspond to all points-\nof-interest of a region (i.e. a city) where commuters can board or alight taxis. The\nedges are represented by an adjacency matrix A characterising the travel time on\neach road segment with respect to traﬃc conditions. For nodes pair i, j, the node\nj is directly connected from node i if and only if the taxi can drive from i to j\nwithout passing through any other nodes in G. In our deﬁnition of the adjacency\nmatrix Aij, if j is not directly connected from i, Aij = 0; otherwise for i ̸= j, Aij\nis the reciprocal of the travel time from i to j. When i = j, Aii = 1.\nTwo arrays are used to characterise commuters’ behaviours: gi gives the proba-\nbility of a commuter appearing and waiting at node i for each time step; Mij gives\nthe probability of a commuter with origin at node i is going to the destination\nat node j. The constraints for these two arrays are given in Eq.(2) and Eq.(3).\nIn principle, Aij, gi, Mij can all depend on time. In this paper we focus on the\nstationary case, and we will leave the time-dependent cases for the future works.\nThe routing algorithm we develop is entirely encapsulated in a stochastic policy\nmatrix Pij for vacant taxis, which is also the decision variable of the optimisation.\nThe stochastic routing policy deﬁnes the the probabilities of moving to a directly\nconnected node j when a vacant taxi is located at i. If Aij = 0, Pij = 0. The Pii\ngives the probability of a vacant taxi staying at node i for one time step, which can\nbe non-zero. For hired taxis, the routing follows shortest path on A−1\nij . A simple\ninitial policy is the random policy, deﬁned as Pij = 1/ni if Pij ̸= 0. Here ni is the\nnumber of neighbouring nodes for each i where Aij ̸= 0, including j = i.\nLet n be the total number of commuters generated at each time step, the inputs\nfor the large scale agent-based simulations are Aij, gi, Mij, Pij subjecting to the\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL 7\nfollowing constraints:\nX\nj∈Aij̸=0\nPij = 1,\n∀i ∈N\n(1)\nX\nj∈N\nMij = 1,\n∀i ∈N\n(2)\nX\ngi = nc,\n∀i ∈N\n(3)\nPij ∈{0, 1}, Mij ∈{0, 1}, gi ∈{0, 1}\n∀i, j\n(4)\nThere are two types of agents in the simulation: the taxis and the commuters.\nThe behaviour of the commuters is rather simple. At every time step, we scan\nover every node in the network. There is a probability gi of increasing the queuing\ncommuter at node i by 1. We can thus denote ci as the number of commuters\nwaiting at node i. The commuters do not move around the network, and ci decreases\nby 1 when a vacant taxi reaches node i.\nThus at time step t, the dynamical\nbehaviour of ci is given by:\nci (t) =\n\u001a max (ci (t −1) + 1 −Nvacant (i, t) , 0)\nwith probability gi\nmax (ci (t −1) −Nvacant (i, t) , 0)\nwith probability 1 −gi\n(5)\nwhere Nvacant (i, t) is the number of vacant taxis at node i at time t. For total\nsimulation time H, the total waiting time of the commuters can be calculated as:\nTwaiting =\nH\nX\nt=0\nN\nX\ni=1\nci (t)\n(6)\nLet nc represents number of commuters generated at each time step, the average\nwaiting time of all commuters is given by:\n¯Twaiting = Twaiting\nHnc\n(7)\nThe dynamics for the taxis are more complicated. We denote the location of the\nlth taxi by a time-dependent array\n\u0000kl\n1, kl\n2, dl, sl\u0001\n(t), when the taxi is in between\ntwo connected nodes kl\n1, kl\n2 and moving towards kl\n2, with time distance dl from\nkl\n1 (so the taxi is dl time steps away from kl\n1). Thus dl ≤A−1\nkl\n1kl\n2. If dl = 0, the\ntaxi is right at the node kl\n1. We use sl to indicate the status of the taxi, which\nin our simulation is either vacant (sl = v) or occupied (sl = o). We clearly have\n\u0000kl\n1, kl\n2, A−1\nk1k2, sl\u0001\n(t) =\n\u0000kl\n2, kl\n3, 0, sl\u0001\n(t). The formal dynamical equations are given\nas follows:\ndl (t) =\n(\ndl (t −1) + 1\ndl (t −1) < A−1\nkl\n1kl\n2\n0\ndl (t −1) = A−1\nkl\n1kl\n2\n(8)\n\u0000kl\n2, kl\n3, 0, v\n\u0001\n(t) =\n\u0000kl\n1, kl\n2, 0, v\n\u0001\n(t −1)\nif ckl\n2 = 0\n(9)\n\u0000kl\n2, ¯kl\n3, 0, o\n\u0001\n(t) =\n\u0000kl\n1, kl\n2, 0, e\n\u0001\n(t −1)\nif ckl\n2 > 0\n(10)\nIf sl = v and no commuter is waiting at the node, kl\n3 is chosen with probability\nPkl\n2kl\n3; if sl = o, then ¯kl\n3 is chosen from the shortest path to the destination of the\ncommuter in the taxi. We can now deﬁne a reward function R for the taxis based\non whether or not there is a commuter in the taxi. A simple example we use in\n8 OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nNetwork\nCommuter\nPolicy\nAdjacency matrix\nAij\nOrigin probability distribution gi\nPolicy matrix\nPij\nDestination probability distribution Mij\nTable 1. Notations of the Formulated Problem\nthe paper is to give a unit reward to every occupied taxi for each time step. The\nreward function is further detailed in Sec.6.2.1.\nNow we can treat the task formally as an optimisation problem. Given a road\nnetwork Aij, a stochastic taxi demand gi and Mij, and a ﬁxed supply of N taxis,\nwe would like to ﬁnd the optimal taxi stochastic routing policy Pij as decision\nvariables that maximise or minimise certain objective functions that we can freely\ndeﬁne. Diﬀerent objective functions can be deﬁned based on prioritising diﬀerent\nparties involved (e.g. the passengers, the taxi drivers, and the policy-makers). In\nthis paper, we choose to maximise total reward deﬁned in Eq.(27). The summarised\nnotations are shown in Table. 1.\n5. System Overview\n5.1. Agent-based Simulation. We have designed and implemented a large scale\nmicroscopic agent-based simulation platform [52] for evaluation of the performance\nof Pij, using diﬀerent Aij, gi, Mij as inputs. A collection of instances containing\nstate information on the positions and actions of vacant taxis, together with rel-\nevant extrinsic rewards needed for reinforcement learning is generated from the\nsimulation after running the simulation for ﬁnite time horizon H (measured in time\nsteps). While diﬀerent physical time unit can be used to correspond to each time\nstep, in this study, each time step is taken to be one second for generality. The\nplatform simulates the dynamics of two types of agents: the commuters and the\ntaxis, and produces outputs characterising the performance of stochastic policy Pij\nfrom diﬀerent perspectives using various performance metrics. We focus on aver-\nage extrinsic reward and average waiting time of commuters in this paper, further\nexplained in Sec.7.\nThe behaviours of commuters are determined by probability distributions gi\nand Mij, and the initial positions of all taxis are assigned to random nodes at\nthe beginning of each epoch of the simulation. Vacant taxis will only pick up a\ncommuter when the location of a vacant taxi and the origin of a waiting commuter\ncoincides. The movements of the occupied taxis are computed eﬃciently by shortest\npath algorithm using our customised Contraction Hierarchy algorithm [52], while\nthe movements of the vacant taxis will follow the stochastic routing policy Pij.\nBoth real-time booking and road-side hailing with three potential status for the\ntaxis: vacant, booked, and occupied, can be implemented. In this paper, we focus\non street-hailing instead of real-time booking so taxis will not take bookings. But\nin scenarios where some of the taxis take bookings, the methods proposed in this\npaper can still be applied to route those vacant taxis available for both booking\nand street-hailing by potential commuters.\n5.2. The Markov Decision Process. The simulation platform can be used to\nevaluate policies Pij from top-down construction using model-based methods, or\nfrom bottom-up learning using reinforcement learning. An illustration of our pro-\nposed framework for optimising taxi routing policies is shown in Figure 1.\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL 9\nFigure 1. An illustration of framework for optimising vacant taxi\nrouting using model-based and model-free approaches.\nThe sequential decision makings of the taxis in above framework can be modelled\nas a Multi-agent Markov Decision Process (MMDP) involving a collective of taxis\nas agents, each with their own set of actions, operating in a common environment\nwith the same shared goal (utility function) to achieve a fully cooperative system\n[4]. The challenge of a MMDP modelling of this problem (formulated in Sec.4) lies\nin the set of actions to be represented and the policies to be optimised, as each\nvacant taxi will perform their own policies and choose actions at every location in\nthe road network accordingly. While all agents are improving their own policies\nconcurrently, the environment faced by each agent may become non-stationary.\nAnd the joint action space increases exponentially with the number of agents [54],\nwhich may cause scalability issues in our problem when the number of taxis in\nsystems is large.\nTo deal with the above challenges and to work for realistic taxi system with\na large taxi ﬂeet, we apply a ”mean-ﬁeld” approximation and model the given\nproblem as a single-agent Markov Decision Process (MDP), assuming that each\ntaxi will follow the same stochastic policy to be learned. Instead of each taxi as\nan agent, a learner to optimise routing policies Pij is formulated as the agent and\nthe simulation that the learner interacts with is the environment. The resulting\nstochastic routing policies Pij is used to determine the routing decisions for each\nvacant taxi in system.\nMathematically, the modelled ﬁnite MDP is speciﬁed by (S, A, P, r, γ, ρ), where:\nS is a state space of the environment, A is an action space, P is a transition model\nP(s′|s, a) as the probability of transition into state s′ from s by taking action a,\nr is a reward function where r(s, a) is the immediate reward in taking action a in\nstate s, γ is a discount factor γ ∈[0, 1], ρ is a starting state distribution over S.\nIn this problem, the state S is given as the state of all taxis in system. The action\nA are decisions made for vehicles at each node on which adjacent node to move\nto. As a consequence of the actions, the agent receives reward r = r(st, at), r ∈R\ncharacterising the optimisation objective deﬁned in Sec.4, and transit to the next\nstate s′ according to transition model P. The construction of the reward function\nis further explained in Sec.6.2.1.\n10OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nLet π : S →∆(A) be a stochastic policy speciﬁes the decision-making strategy\nin choosing the probability of actions for each taxi in system based on current state,\nwhere ∆(A) is the probability simplex over action space A, and at ∼π(·|st). π is\nrepresented as the policy matrix Pij in this problem. By following π, a distribution\nof a state-action sequence τ = (s0, a0, · · · , aH−1, sH) is denoted as Eq.(11), starting\nfrom state s0 drawing from starting state distribution ρ, and for all subsequent\ni, i ∈[0, H) where ai ∼π(·|si) and si+1 ∼P(·|si, ai).\n(11)\nDπ(τ) :=\nH−1\nY\ni=0\nπ(si, ai)P(si, ai, si+1)\nGiven a discount factor 0 ⩽γ ⩽1, the value function for a policy Pij is deﬁned\nas discounted sum of future rewards from starting state s over a ﬁnite horizon H,\ni.e.\n(12)\nV π(s) := E\n\"H−1\nX\nt=0\nγt−1r(st, at)|π, s0 = s\n#\nThe expected value of initial state is further deﬁned as Eq.(13).\n(13)\nV π(ρ) := Es0∼ρ[V π(s0)]\nFor a given policy π, the state-action value function (Q-value function) Qπ and\nthe advantage function Aπ are deﬁned as Eq.(14) and (15).\n(14)\nQπ(s, a) := E\n\"H−1\nX\nt=0\nγt−1r(st, at)|π, s0 = s, a0 = a\n#\n(15)\nAπ(s, a) := Qπ(s, a) −V π(s)\nThe objective of the learning agent is to use ascent methods for the optimisation\nproblem given as Eq.(16), which is to maximise expected value from the initial state\nover all policies.\n(16)\nmax\nθ∈Θ V πθ(ρ)\nwhere π in this study is parameterised by linear function approximator as\n(17)\nπθ(a|s) = θs,a\nwhere θ is a row stochastic matrix, θs,a ⩾0 and P\na∈A θs,a = 1 for s ∈S and\na ∈A.\nIt has been proved in [1] that V πθ(s) is non-concave in θ for the above parameter-\nisation, so the standard solvers for convex optimisation problems are not applicable\nfor this target problem.\nThroughout this paper, we will use RL approaches to\noptimise the routing policies based on policy gradient theorem [46] as follows.\nLet R(τ) be the total reward of state-action sequence τ on following π starting\nfrom s0, where in this problem, the reward R(τ) is given by\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL11\n(18)\nR(τ) := R(s0, a0)\nwhere R(s0, a0) is deﬁned by Eq. (27) in Sec.6.2.1.\nThen,\n(19) ∇θV πθ(ρ) = Eτ [r(τ)∇θ log(Dπθ(τ))|ρ] = Eτ\n\"\nr(τ)\nH−1\nX\nt=0\n∇θ log(πθ(st, at))|ρ\n#\nThough the problem of this paper is formally framed as a MDP, the RL method\nproposed in this paper is based on Monte-Carlo method with undiscounted setting\nwhere γ →1, which does not require the system to be Markovian. The details of\nthe model-free reinforcement learning will be further explained in Sec.6.\n6. Methodology\nWe present our proposed model-based approaches, model-free RL and hybrid\nlearner in this section, based on the design shown in Figure 1. The performance of\nRL depends on 1) meaningful state space, action space and reward functions de-\nﬁned, 2) good designed algorithms to learn from data, and 3) eﬃcient data collection\nby exploring the environment in a self-organised way from the above formulation.\nTo achieve that, we ﬁrst present our model-based routing policy constructed from\ntop-down approach, which requires no sample data.\nWe then demonstrate our\ndesign of RL algorithm and the construction of reward function. At the end, a\nsimple and eﬀective hybrid approach is described to initialise model-free RL with\nmodel-based eﬀective policy.\n6.1. Model-based Routing Policy. For this target problem, instead of learning\na model of the dynamic function of environment (like in most model-based RL), we\ncan construct routing policy from transportation theory using the MDP. In order\nto ﬁnd the optimal Pij which allows us to route a vacant taxi at any location\nin a given road network, the intuition behind our proposed model-based dispatch\nalgorithm is to direct the vacant taxis to a set of “hotspots” in the road network,\nwhich are identiﬁed either statically or dynamically. These hotspots are locations\nwhere the probability of ﬁnding a waiting commuter is high, so that vacant taxis\nare dispatched to these hotspots, which can be formally described by a dispatch\nmatrix Dij: given a vacant taxi at node i, Dij gives the probability of the taxi\nbeing dispatched to node j (which potentially can be quite far away). The dispatch\nprobability is also inversely proportional to the distance between i and j. There will\nalso be some routing choice from node i to node j (the simplest being the shortest\npath between the two nodes). Once Dij and the routing choice are speciﬁed, the\nturn-by-turn routing of Pij as model-based routing policy can also be uniquely\ndetermined via microscopic simulations and sampling. Throughout this paper we\nassume shortest path routing between two nodes for Dij, as the speciﬁc choice of\nrouting algorithm is not essential in this particular context.\n6.1.1. Dispatch Algorithm for Hotspots. One of the common approaches for vacant\ntaxi manoeuvring is a good algorithm for dispatching them to certain locations\non the map where higher probabilities of commuter generation are expected (the\nhotspots). This is also implicitly the strategy of experienced taxi drivers. Following\n12OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nthe problem statements and deﬁnitions in Sec. 4, we ﬁrst propose models of eﬃcient\ndispatch algorithm, using Aij, gi, Mij as inputs, to obtain an eﬃcient dispatch ma-\ntrix Dij. Physically, for a vacant taxi appearing at node i, Dij gives the probability\nfor this vacant taxi to be dispatched to a (not necessarily adjacent) node j, with\nthe constraint P\nj Dij = 1. We also allow Dii ̸= 0, implying the vacant taxi stay\nput at its location waiting for commuters to appear. If node j is far away from\nnode i, the vacant taxi will be dispatched to node j via the shortest path obtained\nfrom the network adjacency matrix A−1\nij . Without loss of generality, here we only\nmake Dij dependent on gi and Aij, and not explicitly dependent on Mij.\nLet dij be the shortest distance in terms of travel time between node i and j,\nas computed from the adjacency matrix A−1\nij . For i = j we set dii = 1. A rather\ngeneral dispatch model is given as follows:\nDij = f\n\u0000d−1\nij , gj\n\u0001\n(20)\nIntuitively, Dij should increase monotonically with gj, while decreasing monotoni-\ncally with dij. This is because if all other conditions are identical, we should have\na higher probability of dispatching vacant taxis to nodes with higher rate of com-\nmuter generation, the so-called hotspots. On the other hand, for two nodes with\nthe same rate of commuter generation, there should be a reduced probability to\ndispatch the vacant taxi to a node that is further away. Optimisation of Eq.(20)\ncan be done by Taylor expanding f with respect to d−1\nij and gj, and tuning the ex-\npansion coeﬃcients at diﬀerent orders to maximise the utility functions computed\nfrom agent based simulations. Here we use a rather simple model with the lowest\norder expansion as follows:\nDij = λi\ngj\ndij\n(21)\nwhere λi =\n\u0010P\nj Dij\n\u0011−1\nis the normalisation factor. The performance of this model\nhas been satisfactory based on extensive agent-based numerical simulations shown\nin later section Sec.8.1.\n6.1.2. Eﬀective Routing Policy from Dispatch Model. One major diﬀerence between\nDij and Pij for vacant taxi manoeuvring is that for the former, nodes i and j can\nbe quite far away. On the other hand, Pij is only non-zero when nodes i and j\nare neighbours. The degrees of freedom for Dij is thus much larger and scale as\nthe square of the network size. This makes optimising Dij a much more diﬃcult\nproblem, as compared to optimising Pij (whose degrees of freedom scale linearly\nwith network size). On the other hand, when the vacant taxi is dispatched from\nnode i to j, the path between nodes i and j is deterministic (and we choose the\nshortest path in this work). Eﬀectively, given a speciﬁc Aij, gi, Mij, Dij there is still\na statistical distribution for the choice of neighbouring nodes from any node i, if we\nlook at a large number of vacant taxis arriving at node i (either as the destination\nof the dispatch, or as an intermediate node enroute to the dispatch destination),\nand how these vacant taxis choose the next node. We can thus generate an eﬀective\nPij via agent based simulations, and this particular Pij is unique and well-deﬁned\nin the limit of long time simulations.\nFormally, for a simulation time with a total number of discrete time-steps H,\nwe denote nH\ni\nas the number of times when an vacant taxi lands on node i, and\nwe do not distinguish between diﬀerent taxis. Let pH\nij be the number of times the\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL13\nvacant taxi at node i moves towards one of the neighbouring node j. The eﬀective\nturn-by-turn routing policy is deﬁned as follows:\n¯Pij = lim\nH→∞\n1\nnT\ni\npH\nij\n(22)\nIn this way, the eﬀective policy can be unambiguously deﬁned for almost any ma-\nnoeuvring strategies. If the routing strategy for vacant taxis is a turn-by-turn policy\nPij to start with, then obviously we will have ¯Pij = Pij. If the routing strategy\nis from a dispatch policy Dij, there is a unique one-to-one mapping from Dij to\n¯Pij, given all other conditions are ﬁxed. We can thus evaluate and compare the\nperformances of Dij and ¯Pij in a meaningful manner. More importantly, ¯Pij can\nbe used as an optimised initial state for reinforcement learning, as we will explain\nin the rest of this paper.\n6.2. Model-free Reinforcement Learning with Model-based Initialisation.\nTop-down approaches in constructing Pij can be highly eﬃcient using no data\nsamples on the states st, given a speciﬁc Aij, gi, Mij, but they generally do not\nyield optimal turn-by-turn maneuvring policies. Model-free RL, on the other hand,\ncan optimise the policies variationally, starting with any speciﬁc initial policy using\npolicy gradient based methods. To combine the beneﬁts of model-based approach\nand model-free RL, we propose to train a model-free RL from either random policy\nor the eﬀective policy from dispatch algorithm as our initialisation policy. Thus,\nwe can use RL to obtain high performance routing policies either initialised from a\nrandom policy without the information on Aij, gi, Mij, or from model-based policies\nconstructed from top-down approaches.\n6.2.1. Model-free Reinforcement Learning. Our model-free RL is initialised with\nsome speciﬁc policies, which are then updated by Actor-Critic Method with linear\nfunction approximators and on-policy policy gradient [45]. To improve the per-\nformance and sample eﬃciency, Proximal Policy Optimisation (PPO) [42] is used\nto optimise a “surrogate” objective function using stochastic gradient ascent to\nupdate policy Pij based on data sampled from Pij. An exploration bonus is fur-\nther introduced to encourage the learning agent to visit novel states even when the\nenvironmental extrinsic reward is sparse.\nPolicy Optimisation: More concretely, we employ Advantage Actor-Critic\n(A2C) [31] combined with PPO as a RL framework which serves as a well-performed\nbaseline for a lot of discrete and continuous benchmarks, for example Atari, 3D\nhumanoid, and MuJoCo Physics engine. In general, advantage actor-critic has two\nnetworks named as actor and critic accordingly. The actor is to learn the optimised\npolicy using a policy network, as deﬁned in Eq.(19), and the critic is to evaluate the\npolicy created by the actor and suggest the adjustment using a value network with\ncalculated advantage, as deﬁned in Eq.(14) and (15. PPO is to used to reduce the\nvariance of vanilla policy gradient by reducing reinforcement learning to a numerical\noptimisation problem on the policy.\nIn essence, the policy matrix Pij(θ) is parameterised by actor parameter vector θ,\nwhere θ are trainable weights to maximise future rewards. An “surrogate” objective\nis maximised subject to the constraint that the distribution of the policy before\nupdate Pij(θold) is close to the distribution of update policy Pij(θ). Hence, the\nobjective can be modiﬁed to penalise changes to the policy that moves too far from\nthe Pij(θold). Formally, the objective to be optimised is shown as:\n14OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\n(23)\nLclip(θ) = ˆEt[min(rt(θ) ˆAt, clip(rt(θ)), 1 −ϵ, 1 + ϵ) ˆAt]\nwhere ϵ is a small hyperparameter to be tuned, rt(θ) is the probability ratio\nPij(θ)(at|st)\nPij(θold)(at|st). ˆAt is the estimated advantage at time t from the critic, which cap-\ntures how a particular policy is better than the others at a given state. The expec-\ntation in equation 23 is the empirical average of samples from the distribution of\nPij(θold)(at|st), using importance sampling to estimate expectation of Pij(θ)(at|st).\nThe processing of policy optimisation is shown in Fig. 2. The PPO algorithm\nin A2C style is explained in detail in [42].\nFigure 2. An illustration of proximal policy optimiser\nCombining Intrinsic and Extrinsic Reward: For this problem of taxi rout-\ning, the extrinsic reward is deﬁned by bestowing a unit reward for every time step\nwhen a commuter is in taxi, for each taxi operating in the road network, charac-\nterising the optimisation objective deﬁned in Sec.4. There is no punishment for\nvacant taxis cruising. The extrinsic reward at each time t is formally deﬁned as\ncumulative future reward from time t to ﬁnite horizon H, shown in the following\nEq.(25).\nRe(st, at) =\nH\nX\nt\nX\nv∈V\n(ov)\n(24)\nov =\n(\n1, when vehicle v is occupied\n0, when vehicle v is vacant\n(25)\nWell-performed policy in RL maintain a right balance of maximising extrinsic\nrewards (exploitation) and trying out new states (exploration), especially when the\nextrinsic rewards are sparse and the target task is temporally extended. With-\nout proper exploration method, the proposed RL algorithm may converge to sub-\noptimal solutions without attempting enough novel actions for potentially higher\nexpected rewards. Though ﬁnding optimal exploration strategies under large, inﬁ-\nnite MDPs (like this problem) is theoretically intractable, many exploration strate-\ngies are proposed to practically solve this problem, including optimistic exploration\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL15\n[5, 17, 47], Thompson sampling style algorithms [37], information gain style algo-\nrithms [22], etc. We adopt a random network distillation (RND) approach [5] to\nconstruct our intrinsic rewards, which is a heuristic estimation of state counts via\nprediction errors as exploration bonus.\nGenerally, the intuition behind this ap-\nproach is to encourage the agent to visit novel states by telling if the current state\nis novel or not. The RND approach takes inspirations of count-based optimistic\nexploration and information gain approach. This approach can also handle large\ninput states where most states are visited at most once, as compared to the other\napproaches.\nSpeciﬁcally, the RND includes two networks: a target network and a predictor\nnetwork. The target network takes input state to an embedding f ∗(s, a) = fψ(s, a),\nwhere ψ is a random parameter vector. The predictor network is trained by gradient\ndescent to minimise the Mean Squared Error (MSE) between output prediction\nˆfφ(s, a) and the embedding f ψ(s, a). The prediction error Ri(s, a) = || ˆfφ(s, a) −\nfψ(s, a)||2 is set as the intrinsic reward (exploration bonus).\nThe ﬁnal reward function is a linear combination of the extrinsic reward and\nintrinsic reward. The reward function proposed does not tell the vehicles anything\nabout where the commuters are and how to reach the potential commuters. Further\nnormalisation has been done for the intrinsic reward to divide it by a running\nestimate of standard deviation, so as to keep the intrinsic reward within a reasonable\nscale when combined with the extrinsic reward. The mathematical representation\nof the combined reward is in Eq.(27), where Re and Ri are extrinsic reward and\nintrinsic reward respectively, ns represents the number of total observed states and\nthe corresponding number of actions, σ is the normalisation parameter, and R is\nthe total reward.\nσ =\ns\nns\nPns\nj=0 Ri(sj, aj)2 −(Pns\nj=0 Ri(sj, aj))2\nns(ns −1)\n(26)\nR(st, at) = Re(st, at) + Ri(st, at)\nσ\n(27)\n6.2.2. Initialising Model-free Reinforcement Learning. In practice, the model-based\nrouting policies leverage the full knowledge of the environment and the MDP, but\nthe best-performing dispatch algorithm (non-committed normalised dispatch al-\ngorithm as shown in Sec.8.1) still in most cases lags behind the pure model-free\nlearners especially when the road network size is small and/or the normalised en-\ntropy of given demand pattern is also relatively small. The reasons may lie in the\nmismatch between the optimal state-action distribution and the dispatch algorithm\nguided state-action distribution as a result of open-loop control algorithm.\nTo combine the beneﬁts of model-free learners and model-based routing policies,\nwe ﬁrst use imitation learning to train a model-free RL agent to imitate the model-\nbased routing policy as an initialisation, and then further train the model-free\nlearner to gather on-policy data in order to minimise the mismatch between the\ngathered data state-action distribution and the learned state-action distribution.\nThe model-based routing policy Pij generated from Sec.6.1 is ﬁrst collected to\ntrain a initial policy of our model-free learner described in Sec.6.2.1. The policy\nnetwork parameterised by θ is trained to match the trained policy ˆPij (θ) to the\ntarget eﬀective policy Pij. Let n be the number of nodes in road network G, and\n16OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nlet J represent the number of direct connections from i where i ̸= j and Aij ̸= 0,\nthe objective is to minimise cross entropy loss as Eq.(28) between the model-based\ntarget policy and trained policy, using the Adam optimiser.\n(28)\nargmin\nθ\n\n−\nn\nX\ni=0\nJ\nX\nj=0\nPij log ˆPij (θ)\n\n\nThe trained policy is used as initial policy of our model-free learner. After initial-\nisation, on-policy data is collected at each optimisation iteration to update learned\npolicy Pij (θ) parameterised by θ using model-free RL proposed. The process is\ncarried out until the optimisation objective based on average hiring time of taxis\nfor each second meets expectation.\n7. Experiments\nThe performance of our proposed reinforcement learning methods have been\nevaluated on large scale simulation platform described in Sec.5.1. Linear function\napproximators, optimised by the Adam optimiser, are used for both value network\nand policy network. At each epoch, a large number of instances consisting of vacant\ntaxis’ positions, choices of actions, as well as the extrinsic rewards of such actions\nover horizon H, are generated from the simulation.\nExperiments were conducted on two types of datasets: an artiﬁcial road net-\nwork where the commuter demands gi and origins/destinations Mij were randomly\ngenerated, and a community-based Singapore road network with both random and\nempirical commuter demands gi. The community-based Singapore road network\ncovers the entire Woodlands, Sembawang, Simpang, Mandai and Yishun areas at\nthe north of Singapore, as shown in Figure 3. The network consists of 2591 nodes\nand 5833 edges. Empirical commuter demands gi were generated from trip data of\na weekday in 2018.\nFigure 3. Community-based Singapore road network for experi-\nment. The size of the node represents gi, indicating the probability\nof generating a commuter at each second in percentage.\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL17\nThe benchmark tasks include evaluations of the produced routing policy on road\nnetwork speciﬁed in the following Table 2.\nThe artiﬁcial road networks consist\nof square lattices with random bi-directional edge weights. gi is non-uniform and\ngenerated randomly with normalised entropy η within (0.6, 0.8) representing ran-\ndomness of the information, calculated by Eq.(29) and (30), where pi is normalised\ngi that sum up to 1 and n represents the number of nodes of given road network.\nMij is generated uniformly — at each origin i in network, the probability of generat-\ning a commuter to any other destination node j is draw from a uniform distribution\nunif ∼{0, 1}, and Mii = 0. Note that with many diﬀerent realisations of the com-\nmuter generation and Mij patterns are tested, the results are qualitatively very\nsimilar.\n(29)\npi =\ngi\nPn\ni=1 gi\n(30)\nη (gi) = −\nPn\ni pi log pi\nlog n\nSetting\nNetwork Type\nNodes\nTaxis\ngi\nη (gi)\nMij\nS1\nLattice\n100\n[0, 200]\nRandom\n0.6649\nRandom\nS2\nLattice\n1089\n[0, 200]\nRandom\n0.7124\nRandom\nS3\nRoad Network\n2591\n[30, 200]\nRandom\n0.7438\nRandom\nS4\nRoad Network\n2591\n[30, 200]\nEmpirical\n0.7821\nRandom\nTable 2. Benchmark Task Settings\nIn our implementation, the performance metrics used for evaluation are the aver-\nage extrinsic reward over the simulation horizon H denoted as Eq.(31), and average\nwaiting time of all commuters given by Eq.(7). In general maximising the average\nreward of taxis will result in minimising the average waiting time of commuters,\nespecially in our experimental scenarios where the origins and the length of the trip\nhave no correlations. For simpliﬁcation, the average extrinsic reward ¯\nRe is named\nas reward during the discussion of the results in Sec.8.\n(31)\n¯\nRe = Re(s0, a0)\nH\nThe implementations, experimental details and hyper-parameter values are fur-\nther listed in the Appendix.\n8. Results\nWe ﬁrst examine the design decisions for our model-based approaches and model-\nfree RL, presented in Sec.8.1 and 8.2, using artiﬁcial road network and randomised\ngi and Mij. We then compared our model-based and model-free approaches, as well\nas our model-free RL with model-based initialisation to demonstrate the eﬀective-\nness of our hybrid approach using both artiﬁcial road network and community-based\nSingapore road network with empirical demands, presented in Sec.8.3.\n18OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\n8.1. Model-based Routing Policy from Dispatch Algorithm. Multiple dis-\npatch algorithms have been proposed and compared based on 1) whether the taxis\nare committed to dispatch to the target node to serve commuters (do not serve other\ncommuters during their travels) and 2) whether normalisation factor λi in Equation\n21 has been used. Experiments were conducted to evaluate the performance of the\nproposed dispatch algorithms on artiﬁcial network of 100 nodes (experiment setting\nS1). The performance of four dispatch algorithms: committed dispatch, commit-\nted normalised dispatch, non-committed dispatch, and non-committed normalised\ndispatch in terms of average waiting time of commuters are shown in Figure 4.\nFrom the comparison, non-committed normalised dispatch algorithm outperforms\nthe other algorithms and is selected to be used to derive eﬀective policy Pij to\ninitialise model-free RL.\nGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n0\n50\n100\n150\n200\n0\n1\n2\n3\n4\nNumber of Taxis\nLog of Average Waiting Time\nGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nG\nCommitted Dispatch\nCommitted Normalized Dispatch\nNon−committed Dispatch\nNon−committed Normalized Dispatch\nFigure 4. Performance comparison of dispatch algorithms on ar-\ntiﬁcial lattice network (experiment setting S1)\n8.2. Design Decisions for Model-free RL. The design decisions for our pro-\nposed model-free RL were evaluated by comparing the ﬁnal reward and average\nwaiting time of commuters from learned policies by vanilla policy gradient, mode-\nfree RL without exploration bonus, and proposed model-free RL algorithm (Figure\n5). All the other design decisions including epochs, learning rate, iterations, clipping\nrange, simulation time, and number of instances collected have been ﬁne-tuned to\nthe best design choices for each method. The results show that our designed model-\nfree algorithm combining intrinsic and extrinsic rewards outperform vanilla policy\ngradient and the design without exploration bonus, especially when the average\nwaiting time of commuters is aﬀected sub-linearly with increased number of taxis\n(discussed in detail later).\n8.3. Evaluation on Artiﬁcial Road Network. The ﬁnal performances of our\nproposed three approaches — model-based dispatch algorithm, model-free RL and\nmodel-free RL with model-based initialisation are evaluated on the same benchmark\ntasks speciﬁed in Sec.7. In particular, ﬁve diﬀerent policies are compared, including\nrandom policy, learned policy from model-free RL, dispatch policy from model-\nbased approach, eﬀective policy for initialisation, and ﬁnal learned policy from\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL19\nFigure 5. Evaluation of design decisions for our model-free RL on\nlattice network of 100 nodes (experiment setting S1). The number\nof epochs is 1000. The ﬁnal reward and average waiting time pre-\nsented are average results over the last 10 epochs. (a) ﬁnal reward\n(average hiring time of taxis for each second), (b) ﬁnal average\nwaiting time of commuters.\nmodel-free RL with model-based initialisation. The comparison of ﬁnal results on\nthe artiﬁcial road networks (lattice networks of 100 nodes and 1089 nodes) with\nvaried number of taxis from 0 to 200 is ﬁrst presented in Figure 6. The results\nshow that the ﬁnal performance (after training 1000 epochs) in terms of reward\nand average waiting time of commuters between learned policies from model-free\nRL, and model-free RL with initialisation, are similar. Both are better than policy\nderived from non-committed normalised dispatch algorithm, eﬀective policy and\nrandom policy. This is especially true when the average waiting time of commuters\nare relatively long when demand exceeds supply.\nRegarding the eﬃciency of learning, both the model-free RL and the hybrid ap-\nproach can learn a fairly good policy with better performance than random policy\nand eﬀective policy from model-based approach very quickly (within 200 epochs),\nas shown in Figure 7. Even with similar converged performance at the end as in\nFigure 6, the hybrid approach with model-based initialisation can be much more\nsample-eﬃcient in learning compared with randomised initialisation. A reasonable\nreward can be achieved by signiﬁcantly less training epochs (less than 100 epochs of\ntraining). Note that each epoch entails a simulation run of 50, 000 seconds (simu-\nlation time) with maximum 512, 000 data samples collected, detailed in Appendix.\nWe have previously identiﬁed a transition between the oversaturated phase when\ndemand exceeds supply, and the undersaturated phase when supply exceeds de-\nmand, when the structure of road network is given, as discussed in our previous\nwork on phase transition in taxi dynamics [52]. The phase transition is also shown\nin Figure 6 that in the oversaturated phase the average waiting time of commuters is\naﬀected exponentially with increased number of taxis, while in the undersaturated\nphase it is aﬀected sub-linearly. In practice, we are more interested in optimising\nvacant taxi routing in the oversaturated phase to meet the service requirements of\ncommuters eﬃciently. When the random policy for vacant taxis can already achieve\nlow average waiting time in the undersaturated phase, there is no much room left\nfor optimisation and the redundancy of taxis on roads creates more problems than\n20OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nthey save. Speciﬁcally for these benchmark tasks, we are mainly evaluating the\nperformance of our proposed algorithm when the number of taxi is less than 50 for\nlattice network of 100 nodes (experiment setting S1) and less than 150 for lattice\nnetwork of 1089 nodes (experiment setting S2).\n8.4. Evaluation on Singapore Road Network. The performance of our pro-\nposed approaches is also evaluated on real community-based Singapore road net-\nwork as described in Sec.7. We compare the results using both randomised demands\nand empirical demands with taxi numbers ranging from 30 to 200, as shown in Fig-\nure 8. Results demonstrate a consistently better performance of proposed hybrid\napproach, in compare with all the other approaches on a medium-sized real-world\nroad network of 2591 nodes. Figure 8 also shows that when the normalised en-\ntropy of the empirical demands is larger as compared with random demands (the\nFigure 6. Comparison of performance on artiﬁcial road network\namong random policy, learned policy from model-free RL (MF-\nRL), dispatch algorithm from model-based approach (MB Ap-\nproach), eﬀective policy as model-based initialisation (MB Init),\nand learned policy from model-free RL with model-based initial-\nisation (MF-RL with MB Init). The number of epochs is 1000.\nThe ﬁnal reward and average waiting time presented are average\nresults over the last 10 epochs. (a) and (b) are ﬁnal reward and av-\nerage waiting time of commuters experimented on artiﬁcial lattice\nof 100 nodes (experiment setting S1), (c) and (d) are corresponding\nresults on artiﬁcial lattice of 1089 nodes (experiment setting S2).\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL21\nFigure 7. Eﬃciency evaluation between proposed model-free RL\nand hybrid approach on artiﬁcial road network. (a) and (b) show\nreward experimented on lattice network of 100 nodes using 20 taxis\nand 40 taxis respectively, (c) and (d) are corresponding results on\nlattice network of 1089 nodes.\nempirical demands of these areas are more uniformly distributed spatially with less\n“hotspots”), the number of taxis needed to operate on the road network can be\nsigniﬁcantly less, but still with higher average hiring time of taxi drivers (reward)\nand lower average waiting time of commuters when using eﬃcient routing policies\nproposed in this work.\nBy comparing the eﬃciency of proposed hybrid approach and pure model-free\napproach on Singapore road network (shown in Figure 9), the hybrid approach\ncan achieve much higher ﬁnal performance and uses signiﬁcantly less data samples\nand training epochs, especially when using empirical demand data (for which the\nnormalised entropy is larger compared to the randomised demands).\n9. Summary and Outlook\nIn summary, we presented in this paper methodologies in optimising vacant\ntaxi routing for street-hailing in response to a stochastic demand pattern, so as\nto maximise the average time of hiring of taxis, and thus reducing the average\nwaiting time of commuters. A model-based dispatch algorithm, a model-free RL\nbased on PPO and RND, and a hybrid RL approach with model-based initialisation\n22OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nFigure 8. Comparison of performance on community-based Sin-\ngapore road network among random policy, learned policy from\nmodel-free RL (MF-RL), dispatch algorithm from model-based ap-\nproach (MB Approach), eﬀective policy as model-based initialisa-\ntion (MB Init) for model-free RL, and learned policy from model-\nfree RL with model-based initialisation (MF-RL with MB Init).\nThe number of epochs is 3000. The ﬁnal reward and average wait-\ning time presented are average results over the last 10 epochs. (a)\nand (b) are ﬁnal reward and average waiting time of commuters\nexperimented on randomised demands (experiment setting S3), (c)\nand (d) are corresponding results on empirical demands (experi-\nment setting S4).\nare proposed and evaluated under various experimental scenarios using a large-\nscale agent-based simulation. In general, the proposed model-free RL with model-\nbased initialisation achieves best ﬁnal performance on both artiﬁcial road network\nand Singapore road network with empirical demands, and are much more sample\neﬃcient than pure model-free RL approach. The sample eﬃcient hybrid approach\ndeveloped here is compelling, especially for useful applications in managing large\ntaxi ﬂeet with human drivers, as well as intelligent routing software for driverless\ntaxis in the near future. The methodologies proposed can be potentially applied\nto a wide context in urban transportation and logistics problems with stochastic\ndemands, such as dynamic reallocation of shared resources and last-mile delivery.\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL23\n0\n200\n400\n600\n800\n1000\nNumber of Epochs\n0.75\n0.80\n0.85\n0.90\n0.95\nExtrinsic Reward\n(a) Real Network (Randomised demands, No. of Taxis = 100)\nmodel-free RL\nmodel-free RL with model-based Initialisation\n0\n200\n400\n600\n800\n1000\nNumber of Epochs\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nExtrinsic Reward\n(b) Real Network (Randomised demands, No. of Taxis = 130)\nmodel-free RL\nmodel-free RL with model-based Initialisation\n0\n500\n1000\n1500\n2000\n2500\n3000\nNumber of Epochs\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nExtrinsic Reward\n(c) Real Network (Empirical demands, No. of Taxis = 100)\nmodel-free RL\nmodel-free RL with model-based Initialisation\n0\n500\n1000\n1500\n2000\n2500\n3000\nNumber of Epochs\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nExtrinsic Reward\n(d) Real Network (Empirical demands, No. of Taxis = 130)\nmodel-free RL\nmodel-free RL with model-based Initialisation\nFigure 9. Eﬃciency evaluation between proposed model-free RL\nand hybrid approach on community-based Singapore road network.\n(a) and (b) show reward experimented on same network with ran-\ndom demands using 100 taxis and 130 taxis respectively, (c) and\n(d) are corresponding results with empirical demands.\nOur proposed model-free RL uses only the positions and actions of all taxis with\nextrinsic rewards to learn and optimise routing policy on a given road network.\nWith training epochs less than 200, even if the street-hailing demand pattern is not\ngiven, our model-free RL can in most cases achieve high performance compared to\nmodel policies, including dispatch policy with given demand patterns. This method\nis good for vacant vehicle routing when the demand patterns and the dynamics of\nthe environment are too complex to be modelled or to be predicted.\nThe model-based methods we proposed are not as eﬀective as model-free RL\non their owns in most of our experimental settings. However, they oﬀer extremely\nsample eﬃcient way to derive routing policies with reasonably good performance,\nwhen the dynamics of the demand-and-supply can be modelled.\nThey are also\nuseful for real-world tasks when real-time optimisation is needed, long-term training\nis infeasible or data required for model-free RL is hard to get.\nBased on the model-free RL and the model-based top-down approach, we then\ndesign a hybrid approach using model-free RL with model-based initialisation to\ncombine the beneﬁts of RL and proposed dispatch algorithm. While lattice network\nis a regular graph, real-world road network with empirical demands may encode\nmore complex dynamics. The proposed hybrid approach is versatile in multiple\n24OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nscenarios under artiﬁcial and real road networks of diﬀerent complexities, and with\ndiﬀerent size of taxi ﬂeet, which demonstrates its potential in a general setting.\nThe hybrid approach is simple and eﬀective in terms of design and implementa-\ntion, thus can be widely applied to real-world applications to enhance performance\nof existing solutions based on heuristics or meta-heuristics, or to accelerate learn-\ning process for model-free learner with better performance and less data samples.\nThe on-policy RL approaches proposed in this paper can also be readily extended\nto consider changing traﬃc conditions (i.e. congestion) or demand patterns (i.e.\npeak or oﬀ-peak demand) in many real-world problems, by modifying the road net-\nwork adjacency matrix according to the empirical data and update routing policies\naccordingly.\nThe methodologies presented in this paper could have wider applications beyond\ntaxi systems, for a wide range of empirical and synthetic systems.\nFor future\nwork, we will work on incorporating time-dependent traﬃc conditions and demand\npatterns for real-world impact. We will also extend the agent-based simulation and\nthe proposed model-based and model-free approaches to cases with ride-sharing, for\nthe optimal routing and route-matching between shared commuters. Our ultimate\ngoal is to optimise routing of demand-responsive transit system in a hybrid form\nconsisting of real-time booking, street-hailing, as well as ride-sharing.\nAcknowledgements\nThis research is supported by the National Research Foundation, Singapore,\nand the Land Transport Authority under its Urban Mobility Grand Challenge\nProgramme (Award No.\nUMGC-L005) and the National Research Foundation,\nSingapore under the NRF fellowship award (NRF-NRFF12-2020-005). Any opin-\nions, ﬁndings and conclusions or recommendations expressed in this material are\nthose of the authors and do not reﬂect the views of National Research Foundation,\nSingapore and the Land Transport Authority.\nReferences\n[1] A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. Optimality and ap-\nproximation with policy gradient methods in markov decision processes. arXiv\npreprint arXiv:1908.00261, 2019.\n[2] G. Berbeglia, J.-F. Cordeau, and G. Laporte.\nA hybrid tabu search\nand constraint programming algorithm for the dynamic dial-a-ride problem.\nINFORMS Journal on Computing, 24(3):343–355, 2012.\n[3] C. Bongiovanni, M. Kaspi, and N. Geroliminis. The electric autonomous dial-a-\nride problem. Transportation Research Part B: Methodological, 122:436–456,\n2019.\n[4] C. Boutilier.\nPlanning, learning and coordination in multiagent decision\nprocesses.\nIn Proceedings of the 6th conference on Theoretical aspects of\nrationality and knowledge, pages 195–210. Morgan Kaufmann Publishers Inc.,\n1996.\n[5] Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random\nnetwork distillation. arXiv preprint arXiv:1810.12894, 2018.\n[6] E. F. Camacho and C. B. Alba. Model predictive control. Springer Science &\nBusiness Media, 2013.\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL25\n[7] C. Coester and E. Koutsoupias. The online k-taxi problem. arXiv preprint\narXiv:1807.06645, 2018.\n[8] A. Colorni and G. Righini. Modeling and optimizing dynamic dial-a-ride prob-\nlems. International transactions in operational research, 8(2):155–166, 2001.\n[9] J.-F. Cordeau and G. Laporte. The dial-a-ride problem: models and algo-\nrithms. Annals of operations research, 153(1):29–46, 2007.\n[10] L. Coslovich, R. Pesenti, and W. Ukovich. A two-phase insertion technique of\nunexpected customers for a dynamic dial-a-ride problem. European Journal of\nOperational Research, 175(3):1605–1615, 2006.\n[11] F. M. Coutinho and N. van Oort. Impacts of replacing a ﬁxed transit line by\na demand responsive transit system.\n[12] G. B. Dantzig and J. H. Ramser. The truck dispatching problem. Management\nscience, 6(1):80–91, 1959.\n[13] S. Dehghani, S. Ehsani, M. Hajiaghayi, V. Liaghat, and S. Seddighin. Sto-\nchastic k-server: How should uber work?\narXiv preprint arXiv:1705.05755,\n2017.\n[14] L. Duan, Y. Wei, J. Zhang, and Y. Xia. Centralized and decentralized au-\ntonomous dispatching strategy for dynamic autonomous taxi operation in hy-\nbrid request mode. Transportation Research Part C: Emerging Technologies,\n111:397–420, 2020.\n[15] G. Feng, G. Kong, and Z. Wang. We are on the way: Analysis of on-demand\nride-hailing systems. Available at SSRN 2960991, 2017.\n[16] T. for London Consultation Hub. Demand responsive bus trial ealing consul-\ntation report, 2019.\n[17] J. Fu, J. Co-Reyes, and S. Levine. Ex2: Exploration with exemplar models\nfor deep reinforcement learning. In Advances in neural information processing\nsystems, pages 2577–2587, 2017.\n[18] T. Gschwind and M. Drexl.\nAdaptive large neighborhood search with a\nconstant-time feasibility test for the dial-a-ride problem.\nTransportation\nScience, 53(2):480–491, 2019.\n[19] M. Han, P. Senellart, S. Bressan, and H. Wu. Routing an autonomous taxi\nwith reinforcement learning. In Proceedings of the 25th ACM International\non Conference on Information and Knowledge Management, pages 2421–2424,\n2016.\n[20] S. C. Ho, W. Szeto, Y.-H. Kuo, J. M. Leung, M. Petering, and T. W. Tou.\nA survey of dial-a-ride problems: Literature review and recent developments.\nTransportation Research Part B: Methodological, 111:395–421, 2018.\n[21] M. A. Hoque, X. Hong, and B. Dixon. Analysis of mobility patterns for urban\ntaxi cabs.\nIn 2012 international conference on computing, networking and\ncommunications (ICNC), pages 756–760. IEEE, 2012.\n[22] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel.\nVime: Variational information maximizing exploration. In Advances in Neural\nInformation Processing Systems, pages 1109–1117, 2016.\n[23] J. G. Jin, L. C. Tang, L. Sun, and D.-H. Lee.\nEnhancing metro network\nresilience via localized integration with bus services. Transportation Research\nPart E: Logistics and Transportation Review, 63:17–30, 2014.\n[24] Z. R. Jin and A. Z. Qiu. Mobility-as-a-service (maas) testbed as an integrated\napproach for new mobility-a living lab case study in singapore. In International\n26OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nConference on Human-Computer Interaction, pages 441–458. Springer, 2019.\n[25] P. Jittrapirom, W. van Neerven, K. Martens, D. Trampe, and H. Meurs. The\ndutch elderly’s preferences toward a smart demand-responsive transport ser-\nvice. Research in Transportation Business & Management, 30:100383, 2019.\n[26] S. Levine and V. Koltun. Guided policy search. In International Conference\non Machine Learning, pages 1–9, 2013.\n[27] X. Liang, G. H. de Almeida Correia, K. An, and B. van Arem. Automated\ntaxis’ dial-a-ride problem with ride-sharing considering congestion-based dy-\nnamic travel times. Transportation Research Part C: Emerging Technologies,\n112:260–281, 2020.\n[28] A. Lim, Z. Zhang, and H. Qin. Pickup and delivery service with manpower\nplanning in hong kong public hospitals. Transportation Science, 51(2):688–705,\n2017.\n[29] O. B. Madsen, H. F. Ravn, and J. M. Rygaard. A heuristic algorithm for\na dial-a-ride problem with time windows, multiple capacities, and multiple\nobjectives. Annals of operations Research, 60(1):193–208, 1995.\n[30] J. Miller and J. P. How.\nDemand estimation and chance-constrained ﬂeet\nmanagement for ride hailing. In 2017 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 4481–4488. IEEE, 2017.\n[31] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,\nand K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning.\nIn International conference on machine learning, pages 1928–1937, 2016.\n[32] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,\nand M. Riedmiller.\nPlaying atari with deep reinforcement learning.\narXiv\npreprint arXiv:1312.5602, 2013.\n[33] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,\nA. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level\ncontrol through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n[34] D. Mu˜noz-Carpintero, D. S´aez, C. E. Cort´es, and A. N´u˜nez.\nA methodol-\nogy based on evolutionary algorithms to solve a dynamic pickup and delivery\nproblem under a hybrid predictive control approach. Transportation Science,\n49(2):239–253, 2015.\n[35] S. Narayanan, E. Chaniotakis, and C. Antoniou. Shared autonomous vehicle\nservices: A comprehensive review. Transportation Research Part C: Emerging\nTechnologies, 111:255–293, 2020.\n[36] A. N´u˜nez, C. E. Cort´es, D. S´aez, B. De Schutter, and M. Gendreau. Multi-\nobjective model predictive control for dynamic pickup and delivery problems.\nControl Engineering Practice, 32:73–86, 2014.\n[37] I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via\nbootstrapped dqn.\nIn Advances in neural information processing systems,\npages 4026–4034, 2016.\n[38] P. Oxley. Dial/a/ride: a review. Transportation Planning and Technology,\n6(3):141–148, 1980.\n[39] S. Perera, C. Ho, and D. Hensher. Resurgence of demand responsive transit\nservices–insights from bridj trials in inner west of sydney, australia, 2019.\n[40] L. B. Reinhardt, T. Clausen, and D. Pisinger. Synchronized dial-a-ride trans-\nportation of disabled passengers at airports. European Journal of Operational\nResearch, 225(1):106–117, 2013.\nOPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL27\n[41] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region\npolicy optimization. In International conference on machine learning, pages\n1889–1897, 2015.\n[42] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[43] D. ˇSemrov, R. Marsetiˇc, M. ˇZura, L. Todorovski, and A. Srdic.\nRein-\nforcement learning approach for train rescheduling on a single-track railway.\nTransportation Research Part B: Methodological, 86:250–267, 2016.\n[44] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,\nT. Hubert, L. Baker, M. Lai, A. Bolton, et al.\nMastering the game of go\nwithout human knowledge. Nature, 550(7676):354–359, 2017.\n[45] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT\npress, 2018.\n[46] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient\nmethods for reinforcement learning with function approximation. In Advances\nin neural information processing systems, pages 1057–1063, 2000.\n[47] H. Tang, R. Houthooft, D. Foote, A. Stooke, O. X. Chen, Y. Duan, J. Schul-\nman, F. DeTurck, and P. Abbeel. # exploration: A study of count-based ex-\nploration for deep reinforcement learning. In Advances in neural information\nprocessing systems, pages 2753–2762, 2017.\n[48] D. Teodorovic and G. Radivojevic. A fuzzy logic approach to dynamic dial-a-\nride problem. Fuzzy sets and systems, 116(1):23–33, 2000.\n[49] T. Verma, P. Varakantham, S. Kraus, and H. C. Lau.\nAugmenting deci-\nsions of taxi drivers through reinforcement learning for improving revenues.\nIn Twenty-Seventh International Conference on Automated Planning and\nScheduling, 2017.\n[50] Y. Wang, B. Zheng, and E.-P. Lim. Understanding the eﬀects of taxi ride-\nsharing—a case study of singapore.\nComputers, Environment and Urban\nSystems, 69:124–132, 2018.\n[51] B. Yang and Q. Li. Turn-by-turn intelligent manoeuvring of driverless taxis:\nA recursive value model enhanced by reinforcement learning. In 2018 IEEE\nIntelligent Vehicles Symposium (IV), pages 1659–1664. IEEE, 2018.\n[52] B. Yang, S. Ren, E. F. Legara, Z. Li, E. Y. Ong, L. Lin, and C. Monterola.\nPhase transition in taxi dynamics and impact of ridesharing. Transportation\nScience, 2020.\n[53] X. Yu, S. Gao, X. Hu, and H. Park.\nA markov decision process approach\nto vacant taxi routing with e-hailing.\nTransportation Research Part B:\nMethodological, 121:114–134, 2019.\n[54] K. Zhang, Z. Yang, and T. Ba¸sar.\nMulti-agent reinforcement learning: A\nselective overview of theories and algorithms. arXiv preprint arXiv:1911.10635,\n2019.\n[55] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi.\nTarget-driven visual navigation in indoor scenes using deep reinforcement\nlearning. In 2017 IEEE international conference on robotics and automation\n(ICRA), pages 3357–3364. IEEE, 2017.\n28OPTIMISING STOCHASTIC ROUTING FOR TAXI FLEETS WITH MODEL-ENHANCED RL\nAppendix A. Implementation Details\nThe simulation was designed for inﬁnite time horizons and inﬁnite number of\nruns. For each run, the unit for the length of simulation is deﬁned as “Second” for\nsimplicity. The settings that are consistent for both model-based approaches and\nmodel-free approaches are listed below.\nLength of each simulation run\n50, 000\nMaximum number of commuters waiting at each node\n1, 000, 001\nMaximum sample size collected for each run\n512, 000\nThe imitation learning used to initialise model-free leaner will update the policy\nnetwork until the cross entropy loss stop decreasing or reach maximum update steps\n30, 000. The hyperparameters used in training the model-free RL and the hybrid\napproach are the same for all experiment settings and are listed below.\nLearning rate\n0.01\nClipping parameter epsilon\n0.1\nIterations\n10\nBatch size\nInput data size\nAppendix B. Experimental Details\nThe experimental details for both artiﬁcial road network and community-based\nSingapore road network are listed below. All experiments were conducted on a\ncommodity server with two NVIDIA Quadro P4000 GPUs with 8 GB GDDR5\nmemory.\nExperiment\nRoad Network Type\nEdge Travel Time\nEpochs\nS1\n10 × 10 Lattice\nX ∼unif{1, 10}\n1000\nS2\n33 × 33 Lattice\nX ∼unif{1, 10}\n1000\nS3\nRoad network (2591 nodes)\nTravel time (s)\n1000\nS4\nRoad network (2591 nodes)\nTravel time (s)\n3000\n",
  "categories": [
    "cs.LG",
    "nlin.AO",
    "physics.soc-ph"
  ],
  "published": "2020-10-22",
  "updated": "2020-10-22"
}