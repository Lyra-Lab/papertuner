{
  "id": "http://arxiv.org/abs/2010.12717v2",
  "title": "Deep Learning for Radio-based Human Sensing: Recent Advances and Future Directions",
  "authors": [
    "Isura Nirmal",
    "Abdelwahed Khamis",
    "Mahbub Hassan",
    "Wen Hu",
    "Xiaoqing Zhu"
  ],
  "abstract": "While decade-long research has clearly demonstrated the vast potential of\nradio frequency (RF) for many human sensing tasks, scaling this technology to\nlarge scenarios remained problematic with conventional approaches. Recently,\nresearchers have successfully applied deep learning to take radio-based sensing\nto a new level. Many different types of deep learning models have been proposed\nto achieve high sensing accuracy over a large population and activity set, as\nwell as in unseen environments. Deep learning has also enabled detection of\nnovel human sensing phenomena that were previously not possible. In this\nsurvey, we provide a comprehensive review and taxonomy of recent research\nefforts on deep learning based RF sensing. We also identify and compare several\npublicly released labeled RF sensing datasets that can facilitate such deep\nlearning research. Finally, we summarize the lessons learned and discuss the\ncurrent limitations and future directions of deep learning based RF sensing.",
  "text": "©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any\ncurrent or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new\ncollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other\nworks.\nJournal: IEEE Communications Surveys and Tutorials\narXiv:2010.12717v2  [eess.SP]  7 Feb 2021\n1\nDeep Learning for Radio-based Human Sensing:\nRecent Advances and Future Directions\nIsura Nirmal\n, Student Member, IEEE, Abdelwahed Khamis\n, Member, IEEE, Mahbub Hassan\n, Senior\nMember, IEEE, Wen Hu\n, Senior Member, IEEE and Xiaoqing Zhu\n, Senior Member, IEEE\nAbstract—While decade-long research has clearly demon-\nstrated the vast potential of radio frequency (RF) for many\nhuman sensing tasks, scaling this technology to large scenarios\nremained problematic with conventional approaches. Recently,\nresearchers have successfully applied deep learning to take radio-\nbased sensing to a new level. Many different types of deep\nlearning models have been proposed to achieve high sensing\naccuracy over a large population and activity set, as well as in\nunseen environments. Deep learning has also enabled detection\nof novel human sensing phenomena that were previously not\npossible. In this survey, we provide a comprehensive review and\ntaxonomy of recent research efforts on deep learning based RF\nsensing. We also identify and compare several publicly released\nlabeled RF sensing datasets that can facilitate such deep learning\nresearch. Finally, we summarize the lessons learned and discuss\nthe current limitations and future directions of deep learning\nbased RF sensing.\nI. INTRODUCTION\nAs we increasingly focus on creating smart environments\nthat are ubiquitously aware of their inhabitants, the need for\nsensing humans in those environments is becoming ever more\npressing [8]. Human-sensing refers to obtaining a range of\nspatio-temporal information regarding the human, such as the\ncurrent and past locations, or some actions performed by\nthe human, such as a gesture or falling to the ground. Such\ninformation then can be used by a range of smart-home or\nsmart-building applications such as turning on/off heating and\ncooling systems when humans enter/leave certain areas of\nthe building, detecting trespassers, or monitoring the daily\nactivities of an independently living elderly resident or patient\nundergoing rehabilitation from an injury or illness.\nTwo fundamental approaches to human-sensing are (a)\ndevice-based, which requires the person to wear or carry\na device/sensor, such as smartphones or inertial sensors [9,\n10], stretch sensors [11], radio frequency (RF) identiﬁcation\ntags [12], and so on, and (b) device-free, which uses sensing\nelements located in the ambient environment to monitor human\nactions without requiring the human to carry any device or\nsensor at all. Device-based approaches, although generally\naccurate, are not practical or convenient in many important\nreal-life scenarios, e.g., requiring the elderly or a dementia\npatient to carry a device at all times. Device-free human\nsensing provides clear advantage for such scenarios.\nFor device-free human sensing, there is a wide range of\nexisting sensing technology including ultrasound motion sen-\nsors, thermal imaging, microphones/speakers, cameras, light\nsensors, and so on. Some of these sensors, i.e., motion detec-\ntors, thermal imagers, and cameras are not typically available\nubiquitously, so must be pre-deployed speciﬁcally for human\nsensing. Some sensors, such as microphones and camera\nraise privacy issues. Compared to these sensors, radio signals\nprovide unique advantages as they are often available ubiqui-\ntously, such as the WiFi signals at home, and unlike cameras\nand microphones, they are not privacy-intrusive. Radio signals\ncan ‘see’ behind the walls and in the dark. Indeed, RF-\nbased device-free human sensing has become an active area\nof research with signiﬁcant advancements reported in recent\nyears. Several start-ups [13–18] now offer commercial RF\nsensing solutions for sleep monitoring, vital sign monitoring,\nfall detection, localization and tracking, activity monitoring,\npeople counting, and so on.\nEarly works in RF human sensing made extensive use of\nconventional machine learning algorithms to extract manually\ndesigned features from radio signals to classify human actions\nand contexts. Although conventional machine learning was\ncapable of detecting many human contexts in small-scale ex-\nperiments, they struggled to achieve good accuracy for large-\nscale deployments. Researchers are now increasingly making\nuse of the latest developments in deep learning to further\nimprove the accuracy, scale, and ubiquity of RF sensing.\nThis trend is clearly evidenced by the growing number of\npublications in major conferences and journals, as shown in\nFigure 1, that explore many different deep neural network\narchitectures and algorithms for advancing RF-based human\nsensing. The success of deep learning for device-free RF hu-\nman sensing calls for a comprehensive review of the literature\nfor successive researchers to better understand the strengths\nand weaknesses, and application scenarios of these models\nand algorithms.\nHow this survey is different from existing ones? Although\nthere are several survey articles published in recent years on\nthe topic of wireless device-free human sensing, none of them\nprovides a systematic review of the advancements made in\nregards to the application of deep learning to this ﬁeld of\nresearch. Since use of deep learning in wireless human sensing\nstarted only about ﬁve years ago, we compare our review\nwith those surveys published in recent years. Table I compares\nthis survey against seven other recent surveys highlighting the\ndifferences in terms of their scope and focus as well as the\nnumber of reviewed publications that applied deep learning in\nwireless sensing. We can see that none of the existing surveys\nfocus their work on deep learning. They rather restrict\ntheir surveys on speciﬁc radio measurement technology, such\nas Channel State Information (CSI) [1, 3–5, 7], or on the\nsensing application, such as through-the-wall sensing [6],\n2\nTABLE I: Summary on related surveys\nReference\nApplication Scope\nTechnology Scope\nTopic Focus and Taxonomy\nReviewed\nDL works\nMa et al.\n[1]\nAny human as well as be-\nyond human (object, ani-\nmal, environment) sensing\nCSI only\nSignal processing techniques and algorithms of WiFi\nsensing in three categories: detection, recognition, and\nestimation.\n< 5\nLiu et al.\n[2]\nAny human sensing\nAny RF based technique\n(RSS,\nCSI,\nFMCW,\nDoppler, etc.)\nRF sensing technologies and their use in human sensing\ncategorized by different applications\n< 10\nYouseﬁ\net\nal.\n[3]\nAny human activity and\nbehavior recognition\nCSI only\nSuccinct review of CSI based human sensing techniques\nand demonstration of performance improvement achieved\nwith LSTM-RNN-based deep learning compared to con-\nventional machine learning\nNone\nFarhana et\nal.\n[4]\nGesture recognition\nCSI only\nComprehensive review of CSI-based gesture recognition\nbased on two approaches: model-based and learning-\nbased; both ML and DL are covered under learning-based\napproach.\n< 15\nHe et al.\n[5]\nWiﬁimaging and all types\nof human sensing\nCSI only\nConcise review of CSI-based sensing applications in-\ncluding imaging and human sensing; the taxonomy is\nbased on applications with minimal coverage of literature\ninvolving deep learning\n< 10\nWang et al.\n[6]\nThrough-wall\nhuman\nsensing\nCSI only\nPrinciples, methods and applications of through-the-wall\nhuman sensing\n< 5\nWang et al.\n[7]\nAny type of human sens-\ning\nCSI only\nComprehensive review of CSI-based human sensing ap-\nplications based on three categories of classiﬁcation tech-\nniques: model-based (no ML), pattern-based (including\nconventional ML), and deep learning-based.\n< 25\nThis\nsurvey\nAny type of human sens-\ning\nAny RF based technique\n(CSI,\nFMCW,\nDoppler,\nRadar, RFID, etc. )\nA systematic review of the application of deep learning\nto RF-based human sensing classiﬁed based on the types\nof employed deep learning techniques. Publicly available\ndatasets are also identiﬁed and reviewed\n83\nwhich prevents them from achieving a comprehensive analysis\nof the progress made in deep learning application to wireless\nsensing. The survey conducted by Wang et al. [7] is the closest\nto our work as they have speciﬁcally reviewed deep learning\npublications as one of their categories. However, as the survey\nwas restricted to CSI, they covered only about 25 deep learning\npapers and missed many important recent advancements.\nGiven the rising popularity of the application of deep learn-\ning in wireless sensing, a more inclusive review would be of\nhigh value to the research community to gain deeper insight to\nthese advancements. We conduct a systematic review without\nany restriction on the radio technology or human sensing\napplication. To this end, more than 80 deep learning works\nhave been surveyed and classiﬁed to provide a comprehensive\npicture of the latest advancements in this research. We also\nreview 20 public datasets of labeled radio measurements,\nwhich is not covered in existing surveys. Finally, we provide\na more comprehensive discussion on the lessons learned and\nfuture directions for this growing ﬁeld of research.\nHow did we select the papers? Semantic Scholar and\nGoogle Scholar are the two main databases used to perform\nthe initial search for the relevant papers using combinations\nof several keywords including: WiFi, wireless, device-free,\nactivity recognition, localization, and deep learning. We also\nspeciﬁcally inspected the proceedings of the following major\nconferences from 2018 onwards: MobiCom, MobiSys, In-\nfocom, Ubicomp, PerCom, IPSN, SenSys, NSDI, and SIG-\nCOMM. In addition, we inspected the following three spe-\ncialised machine learning conferences: CVPR, ICCV, and\nICML. The entire literature review was managed in Mendeley,\nwhich provided its own recommendations of relevant papers\nfrom time to time. Our search revealed in excess of 130\npublications that considered some form of deep learning for\nRF human sensing, but we ﬁnally selected about 90, i.e., only\nthose that were published in major conferences and journals\nwith noteworthy contributions to the ﬁeld. When preparing the\n“dataset section” of our survey, we searched public academic\ndataset repositories such as IEEE Dataport, Harvard Dataverse,\nFigshare, Mendeley Data, and so on, in addition to the web\npages of the authors who mentioned public data release in\ntheir publications.\nFig. 1: Recent growth in the number of scientiﬁc publications\nreporting the application of deep learning for RF human\nsensing.\nContributions of this survey. The goal of this survey is to\nthoroughly review the literature to understand the landscape of\nrecent advancements made in deep learning-based RF human\nsensing. It serves as a quick guide for the reader to understand\n3\nwhich deep learning techniques were successful in solving\nwhich aspects of the RF sensing problem, what limitations they\nfaced, and what are some of the future directions for research.\nIt also serves as a ‘dataset guide’ for those researchers who do\nnot have the means to collect and label own data, but wishes to\nventure into deep learning-based RF human sensing research\nusing only publicly available data. We believe that the detailed\npublic dataset information provided in this survey will also be\nuseful for researchers who have their own data, but would\nlike to evaluate their proposed algorithms with additional\nindependent datasets. Our survey therefore is expected to be\na useful reference for future researchers and help accelerate\ndeep learning research in RF sensing. The key contributions\nof this survey can be summarized as follows:\n1) We provide a comprehensive review and taxonomy of\nrecent advancements in deep learning-based RF sensing.\nWe ﬁrst classify all works based on the fundamental\ndeep learning algorithms used. Different approaches\nwithin a given class are then compared and contrasted\nto provide a more ﬁne-grained view of the application\nof deep learning to the speciﬁc problems of RF sensing.\n2) We identify and review 20 recently released public\ndatasets of radio signal measurements of labeled human\nactivities that can be readily used by future researchers\nfor exploring novel deep learning methods for RF sens-\ning.\n3) We discuss current limitations as well as opportunities\nand future directions of deep learning based RF sensing\ncovering recent developments in cognate ﬁelds such\nas drone-mounted wireless networks and metamaterials-\nbased programmable wireless environments.\nThe rest of this paper is organized as follows. Section II\nintroduces the preliminaries for RF sensing and deep neural\nnetworks. Section III presents our classiﬁcation framework and\nprovides a detailed analysis of the state-of-the-art. Section IV\nintroduces the recently released RF sensing datasets that are\nfreely available to conduct future research in this area. Lessons\nlearned and future research directions are discussed in Section\nV and Section VI concludes the paper.\nFig. 2: Principles of RF human sensing.\nFig. 3: WiFi CSI spectograms obtained in our laboratory for\ntwo different gestures: (a) the right leg moving back-and-forth,\nand (b) the right hand doing push-and-pull.\nFig. 4: Principles of FMCW.\nII. OVERVIEW OF RF HUMAN SENSING AND DEEP\nLEARNING\nIn this section, we ﬁrst review the basic principles, instru-\nments, and techniques for both RF human sensing and deep\nlearning. We then brieﬂy discuss the potential of deep learning\nin RF sensing.\nA. RF Human Sensing\nFigure 2 illustrates the basic principles of RF human sens-\ning. The presence and movements of a human in the vicinity of\nan ongoing wireless transmission cause changes in the wireless\nsignal reﬂections, which in turn results in variation in the\nreceived signal properties, i.e., its amplitude, phase, frequency,\nangle of arrival (AoA), time of ﬂight (ToF) and so on. Since\ndifferent human movements and postures affect the wireless\nsignal in unique ways, it is possible to detect a wide variety of\nhuman contexts, such as location, activity, gesture, gait, etc.,\nby modeling the signal changes or simply learning the signal\nchanging patterns with machine learning.\nTo model changes in signal properties, they must be mea-\nsured precisely. There is a wide range of metrics of varied\ncomplexity to measure different signal properties. The RF\nsignal metrics widely used for human and other sensing are\nreviewed below.\n4\nReceived Signal Strength (RSS): RSS is the most basic\nand pervasively available metric, which represents the average\nsignal amplitude over the whole channel bandwidth. By mov-\ning in front of the wireless receiver, a human can noticeably\naffect the RSS, which has been successfully exploited by re-\nsearchers to recognize hand gestures performed near a mobile\nphone ﬁtted with WiFi [19]. RSS, however, does not capture\nthe signal phase changes caused by reﬂection and it varies\nrandomly to some extent without even any changes occurring\nin the environment. RSS, therefore, is considered good only\nfor detecting very basic contexts and cannot be used for ﬁne-\ngrained human activity recognition.\nChannel State Information (CSI): CSI captures the fre-\nquency response of the wireless channel, i.e., it tells us how\ndifferent frequencies will attenuate and change their phases\nwhile travelling from the transmitter to the receiver. The re-\nceiver calculates the CSI by comparing the known transmitted\nsignals in the packet preamble or pilot carriers to the received\nsignals, and then use the CSI to accurately detect the unknown\ndata symbols contained in the packet. In contrast to a single\npower value returned by RSS, CSI provides a set of values\n𝑎𝑛𝑒𝑗𝜃, capturing signal attenuation 𝑎𝑛and phase shift 𝜃𝑛\nfor each frequency (sub-carrier) that makes up the commu-\nnication channel. For example, a typical 20MHz Orthogonal\nFrequency-Division Multiplexing (OFDM) WiFi channel has\n52 data sub-carriers, which allows the receiver to compute\n52 amplitude and phase values for each packet received. For\nhuman sensing, a series of packets are transmitted, which\nyields a time series of CSI at the receiver. The patterns in\nthe raw CSI time series, or in their fast Fourier transforms\n(FFTs), which is referred to as CSI spectogram, reﬂect\nthe corresponding human activity as illustrated in Figure 3.\nSuch CSI spectograms are the popular choice for training\nmachine learning models for the recognition of various human\nactivities [20, 21].\nIn commodity WiFi, CSI is computed and used at the\nphysical layer of the communications protocol. Use of CSI in\nhuman sensing algorithms therefore requires additional tools\nand techniques for the extraction of the CSI from the physical\nlayer to the user space. In the past, only expensive software\ndeﬁned radios like the Wireless Open Access Research Plat-\nform (WARP) [22] and the Universal Software Radio Periph-\neral (USRP) [23] could provide CSI to the user application.\nNow publicly available software tools, such as nexmon [24],\nare available freely that allow WiFi CSI extraction in most\ncommodity platforms including mobile phones, laptops, and\neven Raspberry Pi. A detailed comparison of all available CSI\nextraction tools can be found in [24]. Easy access to such tools\nhave made CSI one of the most widely used signal metric for\nRF human sensing [1, 3].\nAlthough both amplitude and phase information are avail-\nable in CSI, the amplitude is by far the most commonly\nused metric in WiFi because the returned phase values are\nusually very noisy in commodity WiFi platforms due to\nthe absence of synchronization between the sender and the\nreceiver [25]. Simple transformations of CSI values, however,\nproved to be very useful. For example, phase differences\nbetween sub-carriers have been shown to mitigate the noise\neffect [25] and was successfully employed in a number vital\nsign sensing applications [26, 27]. FullBreathe [28] applied\nconjugate multiplication of CSI from two receiver antennas to\nremove the phase offset, which enabled accurate detection of\nhuman respiration using CSI.\nTime of Flight (ToF) and range estimation: RSS and\nCSI cannot be used to estimate the range or distance of a\nperson from a radio receiver. Range estimation can be very\nuseful for human sensing because it can help localizing a\nperson and detect the presence of multiple persons in the\nenvironment located at different distances from the receiver. If\nToF of the signal is known, then the range can be estimated\nas the product of ToF and the speed of light. Typically,\nexpensive and bulky radar systems are used in most military\nand civilian applications to detect objects and estimate their\nranges by transmitting a series of ultra short pulses of duration\non the order of nano or micro-seconds and then recording their\nreﬂections from the object at the receiver located in the same\ndevice and using the same clock. ToF is measured directly\nfrom the time measurements of the transmitted and received\npulses. However, as short pulses consume massive bandwidth,\nvery high sampling rate is required to process the received\npulses, which in turn leads to high analog-to-digital power\nconsumption. Due to the lack of a low-power compact radar\ndevice, use of radar technology for ubiquitous human sensing\nwas not considered a viable option until recently.\nFrequency Modulated Continuous Wave (FMCW) is an\nalternative radar technology that transmits continuous waves\nallowing the transmitted signal to stay within a constant-power\nenvelop (as opposed to an impulse). Use of continuous wave\nenables low-power and low-cost signal processing, which has\nrecently led to the commercial development of commodity em-\nbedded FMCW radars [29] that can be ubiquitously deployed\nin indoor spaces for human sensing. The principle of FMCW\nis illustrated in Figure 4. Basically, the transmitter sends a\nchirp with linearly increasing frequency and then the received\nsignal is compared with the transmitted signal at any point of\ntime to compute the frequency difference, Δ 𝑓. Since the 𝑠𝑙𝑜𝑝𝑒\nof the linear chirp is known, the ToF is simply obtained as\n𝑇𝑜𝐹=\nΔ 𝑓\n𝑠𝑙𝑜𝑝𝑒. If there are multiple persons in the environment\nlocated at different distances from the radar, FMCW can detect\nall of them because each person’s reﬂection would produce a\ndifferent received chirp at the radar.\nDoppler shift: The ability to measure the motion, i.e.,\nthe velocity of different human body parts, is critical to\naccurately detect human activities irrespective of the wireless\nenvironment where the activities are performed. Doppler shift\nis a well-known theory [30] that captures the effect of mobility\non the observed frequency of the wireless signal. According\nto this theory, the observed frequency would appear to be\nhigher than the transmitted frequency if the transmitter moves\ntowards the receiver, and lower than the transmitted frequency\nif moving away from the receiver. The amount of frequency\nincrease or decrease, i.e., the Doppler shift, is obtained as\nΔ 𝑓= ±𝑣𝑓\n𝑐,\nwhere 𝑓is the transmitted frequency, 𝑣is the velocity at which\n5\nthe transmitter moves towards the receiver, and 𝑐is the speed\nof light.\nNow imagine that the person in Figure 2 moves his hand\ntowards the receiver and then pulls it back as part of a\ncomplete gesture. The frequency of the reﬂected signal will\nthen increase ﬁrst and then decrease, which provides a unique\nfrequency change (Doppler shift) pattern for that gesture.\nIndeed, Doppler shift has been exploited successfully for many\nhuman sensing applications [31–33]. If different users are\nmoving at different speeds towards the receiver, then it is also\npossible to track multiple people [32] in the same environ-\nment, which is difﬁcult to achieve using CSI. Unfortunately,\nexisting commodity WiFi hardware do not explicitly report\nDoppler shifts. It is however possible to estimate Doppler\nshift from the CSI by using signals from multiple receivers\nlocated at different locations in the space [31, 34]. Pu et.\nal. [32] explains a detailed implementation of USRP-based\nDoppler shift extraction method from OFDM signals. Using\na 2-dimensional FFT on the ToF estimates, some FMCW\nradar products, e.g., the mmWave industrial radar sensors from\nTexas Instruments [29], can generate velocities as well. With\naccess to velocity measurements, it is possible to detect and\nmonitor multiple persons even if they are located at the same\ndistance from the radar but moving at different speeds; such\nas performing different gestures.\nAngle of Arrival (AoA): Human sensing could be further\nfacilitated with the detection of the direction of arrival (DoA)\nor angle of arrival (AoA) of the signal reﬂected by the human.\nFortunately, AoA can be accurately computed with an antenna\narray at the receiver. Although commodity WiFi hardware do\nnot report AoA even if they are ﬁtted with multiple antennas,\nthe TI FMCW radar sensors provide multiple antenna options\nand reporting of AoA.\nAs different signal metrics capture different aspects of the\nenvironment, they can be combined for more detailed and\ncomplex human sensing. For example, range and Doppler\neffect were combined for multi-user gait recognition [35],\nwhile researchers were able to signiﬁcantly improve WiFi lo-\ncalization by combining Doppler effect, range, and AoA [36].\nB. Deep Learning\nDeep learning refers to the branch of machine learning\nthat employs artiﬁcial neural networks (ANNs) with many\nlayers (hence called “deep”) of interconnected neurons to\nextract relevant information from a vast amount of data.\nFundamentally, each neuron employs an activation function to\nproduce a output signal from a set of weighted inputs coming\nfrom other neurons in adjacent layers. The key to successful\nlearning is the iterative adjustment of all these weights as\nmore and more data samples are fed to the network during\nthe training phase. Historically, such deep neural networks\nwere not considered attractive due to the massive computing\nresources and the enormously long time that would be required\nto train them. With recent advances in computing architectures,\ne.g., graphical processing units (GPUs), and algorithmic break-\nthroughs during the training procedures, e.g., works by LeCun\net al. [37], deep learning has become much more affordable.\nThis has sparked intense research exploring new deep learning\narchitectures and their use cases in many domains such as face\nrecognition, image processing, natural language processing,\nand so on.\nThe extensive research in recent years has produced a\nplethora of deep learning architectures, each with its own\nspeciﬁc characteristics and advantages. While some of them\nare too specialized targeting very niche applications, others are\ngeneral enough to be applied in different application areas.\nIn this section, we provide a brief introduction to some of\nthe widely used general architectures which also have been\nsuccessfully applied to RF sensing in recent years.\nBefore discussing speciﬁc deep learning architectures, we\nwould like to highlight a few fundamental concepts concerning\ntheir training and usage. A deep learning architecture is said\nto work unsupervised when we do not have to label the\ndata used for its training. On the other hand, supervised\nlearning refers to the situation when the input data has\nto be labeled. Generally speaking, data labeling is often a\nlabour-intensive task, especially for deep learning due to the\nhuge amount of data required for training such architectures.\nUnfortunately, certain use cases must employ some levels of\nsupervised learning, although there are use cases that require\nonly unsupervised deep learning. Finally, some deep learning\narchitectures are called generative as they are designed and\ntrained to generate new data samples. Some of the impressive\nuse cases of generative deep learning includes generating\nrealistic photographs of human faces, image-to-image transla-\ntion, text-to-image translation, clothing translation, 3D object\ngeneration, and so on.\nIn the following, we brieﬂy examine the characteristics and\nuse cases of the widely used deep learning architectures with a\nsummary provided in Table II. For more detailed guidance on\nhow to construct and implement these networks, readers are\nreferred to many available tutorials on deep learning, e.g., [38,\n39]. Applications of these networks to RF sensing is covered\nin Section III.\nMultilayer Perceptron (MLP) is the most basic and also\nthe classical deep neural network consisting of an input layer,\nan output layer, and one or more hidden layers which are\nfully connected as illustrated in the topology column in Table\nII. Each layer in turn consists of one or more neurons or\nperceptrons. The main function of the input layer is to accept\nthe input vector from a data sample and as such the number\nof perceptrons in this layer is scaled to the feature vector\nof the problem. Each perceptron in a hidden layer uses a\nnon-linear activation function to produce an output from the\ninput weights and then passes the output to the next layer\n(forward propagation). MLPs make use of supervised learning\nwhere the labeled data is used for training. Learning occurs\nincrementally by updating the learned weights after each data\nsample is processed, based on the amount of loss in the output\ncompared to the expected result (backward propagation). The\noutput layer mostly uses an activation function depending on\nthe expected result (classiﬁcation, regression, etc.)\nRestricted Boltzman Machine (RBM) is a generative\nunsupervised ANN with only two layers, an input (visible)\nlayer and one hidden layer. Neurons from one layer can\n6\ncommunicate with neurons from another layer, but intra-layer\ncommunication is not allowed (hence the word “restricted”),\nwhich basically makes RBM a bipartite graph. RBM has been\nsuccessfully used for recommending movies for users.\nConvolutional Neural Networks (CNN) or ConvNets are\ndesigned to process visual images consisting of rows and\ncolumns of pixels. As such, it expects to work with 2D grid-\nlike inputs with spatial relationships between them. CNNs\nemploy a set of ﬁlters (or kernels) to convolve in the in-\nputs to learn the spatial features. When multiple layers are\nemployed, CNNs learn the hierarchical representations from\nthe given data set. Further pooling layers are also added\nto reduce the learned dimentionality when designing the\nnetwork. Interestingly, although originally designed to work\nwith images, CNNs are also found to be effective in learning\nspatial relationships in one-dimensional data, such as the order\nrelationship between words in a text document or in the time\nsteps of a time series.\nRecurrent Neural Networks (RNNs) were designed to\nwork with sequence prediction problems by utilizing the\nfeedback mechanism in each recurrent unit. This intra-hidden-\nunit connections make it possible to memorize the temporal\nfeatures of the inputs. However, RNNs suffer from two issues.\nVanishing gradient problem occurs when gradient updates are\nso insigniﬁcant that the network stops learning. Exploded gra-\ndient problem occurs when the cumulative weights’ gradients\nin back propagation result a large update to the network. Due\nto these shortcomings, RNNs were traditionally difﬁcult to\ntrain and did not become popular until the variants called\nLong Short-Term Memory (LSTM) and Gates Recurrent Unit\n(GRU) were invented. Instead of, single non-linear activation\nfunction, multiples of functions and copying/concatenation\nwere added to memorize long term dependencies of the inputs.\nThe difference between LSTM and GRU comes from the\nnumber of internal activation functions and how the inter-\nconnections are handled. RNN’s successors have been used\nsuccessfully for many sequence detection problems, especially\nnatural language processing.\nAutoencoder (AN) is fundamentally a dimension reduc-\ntion (or compression) technique, which contains two main\ncomponents called encoder and decoder. Encoder transforms\ninput data into encoded representation with the lowest possible\ndimensions. The decoder then learns to reconstruct the input\nfrom this compact representation. Because the input serves as\nthe target output, the autoencoder can self-supervise itself re-\nquiring no explicit data labeling. Variants including Denoising\nAutoencoders(DAE) are increasingly used to produce cleaner\nand sharper speech, image, and video from their noisy sources.\nVariational autoencoder (VAE) is a more advanced form of\nautoencoder designed to learn the probability distribution of\nthe input data using principles of Bayesian statistics. The VAE\nthus can generate new samples with interesting use cases\nsuch as generating artiﬁcial (non-existent) fashion models,\nsynthesizing new music or art, etc., that are drawn from the\nlearned distribution and hence perceived as real.\nGenerative Adversarial Networks (GANs) are another\ntype of unsupervised generative deep learning architecture\ndesigned to learn any data distribution from a training set. The\nmain technical difference with VAE is in the method used to\nlearn the distribution. Unlike VAE, which explicitly estimates\nthe parameters of the distribution, GAN simultaneously trains\ntwo networks using a two-player game, hence the word “ad-\nvarsarial”, to directly generate the samples without having to\nexplicitly obtain the distribution parameters. The ﬁrst network,\ngenerator, tries to fool the second network, discriminator, by\ngenerating new samples that look like real samples. The job of\nthe discriminator is to detect the generated samples as fakes.\nThe performance of the two networks improve over time and\nthe training ends when the discriminator cannot distinguish the\ngenerated data from the real data. GANs have undoubtedly rev-\nolutionized the deep learning research with multiple variants of\nGAN models in state-of-the-art. It is noteworthy to mention\nthat architectures like Domain Adversarial Neural Networks\n(DANN) [40] removes the generative property but makes\nit possible to learn the distributions between two different\ndomains and perform accurate classiﬁcations for both domains\nusing a single model. Since we discuss both generative and\nnon-generative adversarial networks in our work, we use\nAdversarial Networks (AN) henceforward to refer to both\ntypes of networks.\nFinally, hybrid models contain the characteristics of more\nthan two primary deep neural networks and hence can help\novercome the hybrid nature of the problems they address.\nFor example, CNN and LSTM are often combined to capture\ninformation latent in both spatial and temporal dimensions of\nthe dataset.\nC. Why Deep Learning in RF Sensing\nMapping RF signals to humans and their activities is a\ncomplex task as the signals can reﬂect from many objects\nand people in the surrounding. In most cases, the problem\nis mathematically not tractable, which motivated researchers\nto adopt machine learning as an effective tool for RF human\nsensing. Conventional machine learning algorithms, however,\nare limited in their capacity to fully capture the rich informa-\ntion contained in complex unstructured RF data. Deep learning\nprovides the researchers exceptional ﬂexibility to tune the\n‘depth’ of the learning networks until the necessary features\nare successfully learned for a given sensing application. With\nthe emergence of more powerful radio hardware and proto-\ncols, such as multi-input-multi-output (MIMO) systems, multi-\nantenna radar sensors, and so on, researchers now have the\nability to generate a vast amount of RF data for any given\nhuman scene, which help train deep neural networks. Deep\nlearning therefore becomes a new tool to push the boundaries\nof RF sensing on multiple fronts such as enhancing existing\nsensing applications in terms of accuracy and scale, realizing\ncompletely new applications, and achieving more generalized\nmodels that work reliably across many different, and even\nunseen, environments.\nIn Figure 5, we highlight evidence from the recent literature\nconﬁrming the capability of deep learning in enhancing the\ndetection accuracy signiﬁcantly compared to the conventional\nshallow learning for three popular RF sensing applications.\nFigure 6 shows a completely new RF sensing application,\n7\nTABLE II: Popular deep learning architectures\nArchitecture\nStrengths\nWeaknesses\nLearning type\nUse cases in RF sensing\nExample Topology\nMLP\nSimple structure\nSlow to converge\nModest performance\nSupervised\nActivity classiﬁcation\nPattern recognition\nTransfer learning\nRBM\nSimple structure\nSpeciﬁc training require-\nments\nUnsupervised\nFeature extraction\nActivity classiﬁcation\nCollaborative ﬁltering\nCNN\nSpatial feature\nidentiﬁcation\nHigh complexity in pa-\nrameter tuning\nSupervised\nRadio image processing\nVideo analysis\nRNN\nTemporal feature\nidentiﬁcation\nHigh\ncomplexity\nin\nmodel\nand\ngradient\nvanishing\nSupervised\nRadio time series data analysis\nAE\nRepresentational\nlearning\nDenosing\nFeature compression\nCostly to train\nUnsupervised\nRadio feature extraction\ntranslation\nAN\nRobust against\nthe adverserial attacks\nHigh model complexity\nSemi-\nsupervised\nReinforcement\nSignal feature extraction\nfeature synthesis\nClassiﬁcation\n∗Legend for the notation\n†Topology ﬁgures are courtesy of [41]\nnamely RF-Pose3D [42], which uses a specialized CNN\narchitecture to estimate simultaneously the 3D locations of\n14 keypoints on the body to generate and track humans in\n3D. Finally, researchers are now discovering deep learning\nsolutions that can remove the environment and subject spe-\nciﬁc information contained in the RF data to generalize RF-\nbased human sensing for ubiquitous deployments [43]. In the\nfollowing section, we are going to survey many more recent\nadvances in deep learning for RF sensing.\nIII. SURVEY OF DEEP LEARNING BASED RF SENSING\nIn this section, we survey device-free RF human sensing\nresearch that used deep learning to analyse RF signals. In\nparticular, we survey a total of 84 research publications and\nclassify them according to the main deep learning architecture\nemployed. Table III provides a summary of this classiﬁcation\nwith rows indicating the deep learning architecture and the\ncolumns showing the application domain of the research. The\ntable reveals that deep learning has expanded its footprint\nacross all popular sensing domains, from localization through\nto gait recognition, using a good mix of neural network\narchitectures. We cover each deep learning architecture (each\nrow of Table III) in a separate subsection, where we further\ncompare and contrast the ways the architecture is implemented\nand investigated by different researchers.\nA. RF Sensing with MLP\nAmong major works in MLP based localization which\nconsider the deep neural network training as a black box,\nLiu et al. [46] conducted a visual analysis to understand the\nsignature features of wireless localization using visual analyt-\nics techniques, namely, dimensionality reduction visualization\nand visual analytics and information visualization to better\nunderstand the learning process of MLP for localizing a human\nsubject. The activations of deep models last layers (for a 3-\nhidden layer MLP) have shown well separated clusters of\nthe learned weights (using 𝑡-SNE) after training process,For\n16 predeﬁned target locations, 86.06% average precision was\nachieved.\nAmong a large number of object localization based on\nwireless sensing literature, FreeTrack[47] presented a MLP\nbased localization approach for moving targets. Denoised\nCSI amplitude information is used taken as inputs to the\nMLP model (5 fully connected layers) which achieve 86 cm\nmean distance error and reduced to 54 cm with particle ﬁlter\nand map matching which are able to detect the obstacles\n8\nTABLE III: Categorization of reviewed publications based on their deep learning techniques.\nCategory\nLocalization\nActivity\nRecognition\nGesture\nRecognition\nHuman\nDetection\nVital Sign Mon-\nitoring\nPose\nEstimation\nGait\nRecognition\nMLP\n[46][47]\n[48]\n[49]\n[50][51][52]\n-\n-\n[49]\nRBM\n[53]\n-\n[54]\n-\n-\n-\n-\nCNN\n[55][56]\n[57][44][58][59]\n[60][56]\n[61][62]\n[63] [64][65]\n[66][67][68]\n[45]\n[69]\n[70][71][72][73]\n[74][75][20]\n[76][77]\n-\nRNN\n-\n[78][21][79]\n[80][3][81]\n[82][83]\n[84]\n-\n-\n[85][86]\nAE\n[87][88][89]\n[90][91][92]\n[93] [94]\n[87][91][92]\n-\n[95]\n-\n[77][73]\n[96]\nAN\n-\n[97][98][99,\n100][43]\n[101][102][103]\n-\n[75]\n-\n-\nHybrid\n-\n[104][105][106]\n[107][108][109]\n[110] [111][112]\n[113]\n[114][115]\n[34][116]\n[117][118]\n-\n[42][119]\n-\nTABLE IV: MLP architectures in RF sensing\nPaper\nApplication\nRadio Measure-\nment\nLayers\nPerformance\n[51]\nHuman Detection\nCSI\n3\nAccuracy\nFixed Location 0.96\nArbitrary Location 0.88\n[52]\nHuman Detection\nCSI\n-\nAccuracy 0.93\n[46]\nLocalization\nCSI\n3\nPrecision 0.86\n[47]\nLocalization\nCSI\n5\nMean Distance Error 0.54 m\n[50]\nHuman Detection\nCSI\n2\nAccuracy 0.823\n[49]\nGait/Gesture Recognition\nCSI\n7\nAccuracy Gait/Gesture 0.94/0.98\n[48]\nActivity Recognition\nCSI\n1\nAccuracy 0.94\nActivity[44]\nFall[20]\nGesture[45]\n20\n40\n60\n80\n100\n23\n16\n68\n84\n94\n97\nAccuracy (%)\nShallow Learning\nDeep Learning\nFig. 5: Evidence of deep learning’s capability to signiﬁcantly\nenhance the detection accuracy for popular RF sensing ap-\nplications. Specialized versions of CNN were used for deep\nlearning in all these experiments. Random Forest, SVM, and\nkNN were used as the baseline shallow learning for Activity,\nFall, and Gesture detection, respectively.\nin the environment.Extensive tests were introduced including\nmultiple walking speeds, subjects, sampling rates have proven\nthe extendability and robustness of the model with state-of-the\nart.\nWiCount [50] utilized a MLP to count the crowd using WiFi\ndevices in environment. Its noteworthy to mention that the\nmulti user sensing is rarely researched area due to its difﬁculty.\nWiCount used both amplitude and phase information of the\nWiFi signal. CSI data is preprocessed by using a Butterworth\nﬁlter and moving average before being input to the DNN\nthat consists of 2 hidden layers with 300 and 100 neurons\nrespectively. The accuracy of 82.3% for up to ﬁve people were\nobserved for total of 6 activities in multi user environment.\nCheng et al. [51] achieved 88% accuracy with up to 9\npeople in an indoor environment in arbitrary locations. The\nauthors claimed that the conventional de-noising and feature\nextraction methods were susceptible to information loss. They\nthus proposed a new feature called “difference between the\nvalue and sample mean” and appended it as an additional\nfeature to the CSI feature vector. This scheme has signiﬁcantly\nimproved the performance of 3-layer MLP model.\nFang et al.[52] proposed a hybrid feature involving both\namplitude and phase to learn three human modes, i.e., absence,\nworking, and sleeping, in an indoor environment. The hybrid\nfeature reduced the need of training data and the model\nachieved 93% accuracy with 6% training samples only. The\nﬁrst hybrid feature contained the magnitudes of the complex\nnumbers in a given CSI vector and the second hybrid feature\n9\nFig. 6: Generating and tracking 3D human skeletons from RF\nsignals by leveraging the power of deep learning. The top ﬁg-\nure is a camera capture of ﬁve people while the bottom ﬁgure\nshows the 3D skeletons of all of these persons constructed\nfrom FMCW radio data with the help of a specialized CNN\nmodel designed by Zhao et al. [42] (Figure courtesy of [42]).\ncontained the calibrated amplitudes and phases. The authors\ntested model’s robustness against the cross environment but\nthe model failed to perform in unseen environment without\nretraining.\nAmong the other notable works, TW-See [48] proposed a\nthrough-wall activity recognition system which used MLP with\none hidden layer for the activity recognition task. The model\nclassiﬁed 7 activities in two environments where the senders\nand the receivers were separated by walls. The authors studied\nthe model robustness with different wall materials, and TW-\nSee achieved above 90% classiﬁcation accuracy for different\nwall materials.\nCrossSense [49] tried to address problem of domain gen-\neralization by incorporating MLP into a deep transnational\nmodel. Also the large scale sensing which includes numerous\nsubjects and domains are not supported by many works. To\nthis end, Crosssense used MLP for generating the virtual\nsamples for a given target domain using a feed forward fully\nconnected network with 7 hidden layers which uses data from\ntwo domains in order to learn the mapping relation between\nthem. The trained network is then used to generate the virtual\nsamples.\nThe summary of MLP related literature is shown in Ta-\nble IV. MLP has shown a simple yet powerful deep learning\napproach for feature learning from CSI data.It was applied to\nboth classiﬁcation and regression tasks. Large scale sensing\napplications like [49] also proved the MLP’s ability in tran-\nferable feature learning between domains from CSI data.\nDenoising and sanitizing of both amplitude and phase\nwere given major attention but some works [47] only choose\namplitude due to challenges in phase sanitation.\nDeep model optimization was a given a major part in\nmodel evaluation using hyper parameter tuning to maximize\nthe models performance. However, the model training time is\nnot reported by many works.\nB. RF Sensing with RBN\nThere are only two works so far that used RBM for RF\nsensing. For number (0 to 9) gesture recognition, DeNum [54]\nstacked multiple RBMs, i.e., the output of one RBM was fed\nas input to the next, to extract the discriminating features from\nthe complex WiFi CSI data. At the end, an SVM was used\nfor the classiﬁcation task using the features extracted by the\nstacked RBM. The average classiﬁcation accuracy reported\nwas 94%. Although this was an interesting use of deep learning\nfor gesture recognition, no benchmark results were available\nto gauge the utility of stacked RBM against conventional\nmachine learning.\nZhao et al.[53] used RBM in a special way to address\nthe challenging problem of localization using only the RSS\nof WiFi signal, which is easily accessible but known to be\nvery unstable. Instead of using the basic RBM, which allows\nonly binary input, the authors considered a variant called\nGaussian Bernoulli RBM (GBRBM)[120] to input real values\nof RSS. They designed a series of GBRBM blocks to extract\nfeatures from the raw RSS data, which is then used as input to\nfurther train an autoencoder (AE) for location classiﬁcation.\nThe combined GBRBM-AE deep learning model achieved\n97.1% classiﬁcation accuracy and outperformed conventional\nAEs, i.e., when the AE is not augmented with GBRBM in the\npre-training stage, in both location accuracy and robustness\nagainst noise.\nC. RF Sensing with CNN\nRF data, when organized properly, convey visual features\nwith spatial pattern similar to those in real images. In RF\nheatmaps [121], reﬂections from a speciﬁc object tend to be\nclustered together while those from different objects appear\nas distanced blobs. Similarly in spatial spectrograms [68],\nmotions from different sources have their corresponding en-\nergy spatially distributed on beam sectors, and in CSI varia-\ntions, neighbouring sub-carriers are correlated. Such behaviour\naligns with the locality assumption of CNN and make CNN a\nfavourable option for RF representation learning. Additionally,\ntemporal features can be acquired as well by restructuring\nthe input to be continuous sequence of RF samples rather\nthan individual samples. This allows CNN convolutions to\naggregate temporal information in the input sequence hence\nextending its role to spatio-temporal processing [20]. These\nreasons indeed drive the popularity of CNN among RF sensing\nsystems.\nCNN architectural patterns can be broadly grouped into two\ncategories; Uni-Modal CNN (see Figure 7) that handles only\nRF input data and Multi-Modal CNN (see Figure 8) which\nexploits support from another modality such as vision mostly\nduring the learning process. We discuss the representative\n10\nTABLE V: CNN Representative Architectures in RF Sensing\nRepresentative Architecture\nArchitecture Key Features\nExample Usage in RF Sensing Context\nEncoder (E)\ninvariance to translations in space and time\nextracting spatio-temporal features from CSI\nin sign language recognition system [45]\naggregating information over temporal dimen-\nsion\ntolerating temporally missing reﬂections from\nlimbs when capturing body pose [75, 77]\nCascaded Encoder\nmultistage robust classiﬁcation\ndealing with sample unbalance and scarcity of\nfall data in fall detection system [20]\nEncoder with Attention (EA)\nencoding importance weights of features rele-\nvant to sensing task\nfocus on feature representations from spectro-\ngram relevant to ASL signs [68]\nMultistream Encoder (ME)\nencoding features across different channels\nchannel-wise\nfeature\nconcatenation\nof\nRF\nheatmaps from horizontal and vertical anten-\nnas [20]\nMultistream Encoder with Attention (MEA)\nweighted aggregation of features from differ-\nent channels\ncombine features from different receiving an-\ntennas based on quality weights for activity\ndetection [44]\nEncoder with Sequence Model (ES)\ntracking state change in classiﬁer predictions\nestimate fall state duration in a fall detection\nsystem [20]\narchitectures in each category. In the literature, however,\none can see that complex sensing systems tend to aggregate\nsome of these architectures as building blocks into a larger\ncomplicated architecture. This is motivated by the need to\ncombine the features offered by each architecture (see Table\nV) to suit the sensing task. As an example, the CNN Encoder\n(E in Figure 7) alone was sufﬁcient for SignFi [45] to achieve\n86.6% gesture recognition accuracy on a dataset of 150 sign\ngestures. In contrast, Aryokee [20] combines the features of\nMultistream Encoder (ME) and Encoder with Sequence Model\n(ES) for robust fall detection in real world settings.\n1) Uni-Modal CNN: The vanilla CNN architecture (En-\ncoder (E)) consists of a few convolutional layers that encodes\nthe extracted features into a latent space followed by a\npredictor. The predictor can produce either a single output as\nshown in most of the published papers, or multiple outputs.\nDespite its simplicity, the Encoder architecture can achieve\ngreat success in many practical applications. This was ﬁrst\ndemonstrated by SignFi [45] that successfully managed to\nsigniﬁcantly expand the classiﬁcation capability of RF systems\nto accommodate 150 gestures. Also, Aryokee [20] was able to\nreliably detect falls among 40 activities on a large scale dataset\ncollected in 57 environments from 140 people. By cascading\ntwo Encoders sequentially, it built a two-stage fall detection\nclassiﬁer., which enhanced the performance of the classiﬁer by\nallowing it to reject non-fall samples that resemble fall samples\n(hard negatives). As a result, a dramatic improvement in the\nprecision by more than 29% was achieved.\nIn some cases, a single RF sensor can export multiple\nindependent measurements. Stacking them in a single input\nvector is not favourable as the measurement contains inde-\npendent information. Alternatively, a Multistream Encoder\n(ME) could be used to extract the unique features of each\nmeasurement stream independently and subsequently combine\nthem into latent feature vectors. For example, vertical and\nhorizontal RF heatmaps [20] [58] are processed by a two-\nstream CNN Encoder for fall detection and person identiﬁca-\ntion, respectively. Similarly, DeepMV’s [44] multi-stream En-\ncoder processed CSI measurements from nine WiFi antennas\ndistributed spatially across the room for activity recognition.\nRF-ReID [59] employs a deep learning framework that\nE Encoder\nEA Encoder with Attention\nES Encoder with Sequence model\nME Mulitstream Encoder\nMEA Mulitstream Encoder with Attention\nRF(1)\nRF(2)\nPredictor\nConvolutional Encoder\nConvolutional Encoder\nAttention\nPredictor\nRF\nConvolutional\nEncoder\nS1\nS3\nS2\nRF\nConvolutional\nEncoder\nPredictor\nAttention\nRF(1)\nRF(2)\nConvolutional Encoder\nConvolutional Encoder\nPredictor\nPredictor\nRF\nConvolutional\nEncoder\nFig. 7: Representative Uni-Modal CNN architectures used in\nRF sensing\npredicts human action from the 3D skeletons produced from\nRF heatmaps. The framework is based on a version of the\n11\ntwo-stream Hierarchical Co-occurrence Network (HCN) [122]\naugmented with an attention module to tolerate inaccuracies\nin the input skeletons estimated from RF. This is an example\nof Multistream Encoder with Attention (MAE), which is\nemployed to allow the model to focus on keypoints with higher\nprediction conﬁdence in RF heatmap snapshots when making\npredictions. In cases where RF samples contains various\ntypes of information that are relevant to the sensing task,\nHierarchical Attention was employed. For example, in Person\nRe-identication systems [72], the body shape (short temporal\nwindow) and the walking style (long temporal window) are\nboth relevant to the sensing task. Thus a hierarchical two-level\nattention blocks can be integrated to attend to each information\ntype.\nLA Late Association\nIA In-network Association\nEA Early Association\nRF\nnon-RF\nloss\nTeacher Network\nPredictor\nStudent Network\nPredictor\nStudent Network\nTeacher Network\nRF\nnon-RF\nloss\n prediction\nprocessing\nPredictor Predictor\nMulti-modal Network\nCNN subnet\nCNN subnet\nRF\nnon-RF\nPredictor\nuni-modal Network\nPredictor\nRF\nnon-\nRF\nCNN\nmodality\nindepende\nnt\nsubnet\nCNN net\nPredictor\nCNN net\nPredictor\nFig. 8: Representative Multi-Modal CNN architectures used in\nRF sensing. Dashed blocks denote training-time only process-\ning.\n2) Multi-Modal CNN: Moving to Multi-modal CNN archi-\ntectures, one can see three key approaches followed in order\nto fuse information from RF and supporting modalities (i.e.\nnon-RF modalities). The key difference between them is the\nstage at which data from supporting modality is utilized in the\nlearning. In Late Association (LA), the supporting modality is\nhandled separately by different model called Teacher Network\n(usually pre-trained) and the output is used for providing labels\nfor the RF model (Student Netwrok).This was adopted as a\nway to tackle the difﬁculty of labelling RF data. For example,\nRF-Pose [77] uses this techniques to train RF pose prediction\nnetwork (student network) with human pose heatmap labels\nacquired from AlphaPose [123] on RGB frames of synchro-\nnized camera. Since the RF samples are synchronized with the\ncamera samples, the teacher network predictions can be used\nas labels for RF samples. A similar approach was followed by\nCSI-UNet [76] and Person-in-WiFi [73]. It should be noted\nthat data from the supporting modality is utilized only during\nthe learning process and not at the run time.\nIn-network Association(IA) fuses information directly\nfrom the RF and the supporting modality in a single architec-\nture. For instance, in the behavioural tracking system, Marko\n[58], tracklets from synchronized accelerometer was used for\ncontinuous masking of RF samples that carry extra information\nirrelevant to the user’s actions.\nFinally, the Early Association(EA) scheme depends on a\nunimodal network that can process intermediate representa-\ntion produced from either RF or the supporting modality.\nRF-Action [59] systems for human action recognition is a\nrepresentative example of this scheme. The intermediate rep-\nresentation is the 3D human skeleton and can be produced\nfrom either RF radar or RGB camera using deep CNN nets.\nThe uni-modal network is agnostic to the original input\ntype as it accepts the intermediate representation. A main\nadvantage of this approach is that the uni-modal network can\nbe trained and ﬁne-tuned using data only from the supporting\nmodality without the need for collecting additional RF data. In\nfact, RF-Action [59] leverages 20K additional samples from\nPKU-MDD multimodal dataset [124] to improve the system\nperformance.\nD. RF Sensing with Recurrent Neural Networks\nAs explained in Section II-A, RF sensing often use time\nseries RF data, such as RSS and CSI obtained from successive\nframes, to detect changes during a human activity. Such\ntime series data contain important temporal information about\nhuman behavior. Shallow learning techniques and conventional\nmachine learning algorithms do not take this temporal factor\ninto account when the data is provided as inputs, which leads\nto poor performance of the models. RNNs have proven their\nability to produce promising results in speech recognition and\nhuman behaviour recognition in video as they are inherently\ndesigned to work with temporal data. RF sensing research\nhas also recognized this beneﬁt of RNN. Recently, RNN\nvariants like LSTM and GRU have become popular in RF-\nbased localization and human activity recognition applications.\nTable VI summarizes the RNN-based RF sensing works we\nsurvey in this section.\nLSTM, which has a gated structure for forgetting and\nremembering control, has dominated the state-of-the-art of\nrecurrent networks. Youseﬁet al. [3] were the ﬁrst to explore\nthe beneﬁt of LSTM-based classiﬁcation against the con-\nventional machine learning algorithms using CSI for human\n12\nactivity recognition. In their experiments, LSTM signiﬁcantly\noutperformed two popular machine learning methods, Random\nForest (RF) and Hidden Markov Model (HMM). Later, Shi et\nal. [78, 81] further improved this process with two feature\nextraction techniques, namely Local Mean and Differential\nMethod, that removed unrelated static information from the\nCSI data. As a result, accuracy improvements were observed\nup to 18% against the original method of [3].\nLSTM quickly became a popular choice for detecting many\nother human contexts. HumanFi [86] achieved 96% accuracy\nin detecting human gaits using LSTM; Haseeb et al. [82]\nutilized an LSTM with 2 hidden layers to detect gestures with\nmobile phone’s WiFi RSSI achieving recognition accuracy up\nto 94%; WiMulti [80] used LSTM for multi-person activity\nrecognition with an overall accuracy of 96.1%. Ibrahim et\nal. [84] proposed a human counting system, called CrossCount,\nthat leverages an LSTM to map a sequence of link-blockage\ntemporal pattern to the human count using a single WiFi link.\nThe intuition behind this success is that, the higher the number\nof people in an area of interest, the shorter the time between\nblocking a single WiFi link and vice versa.\nCSAR [21] proposed a channel hopping mechanism that\ncontinuously switches to less noisy channels for improved\nhuman activity recognition. They proposed an LSTM network\nas a classiﬁer, which takes the Time-Frequency features gen-\nerated from Discrete Wavelet Transform (DWT) spectrograms\nas the model inputs. LSTM is designed to work with inherent\nrelationships in the frequency changes in the spectrogram data\nin long time intervals. As in most of deep learning architecture,\nLSTM can work with bigger data sets effectively. Along with\na 200 hidden unit LSTM layer with 2 other fully connected\nlayers, CSAR achieved 90% accuracy for detecting 8 different\nactivities.\nBidirectional LSTM (BLSTM) is a variant of conventional\nLSTM mode, which has two LSTM layers to represent the\nsequence data in both forwards and backwards simultaneously.\nThis enables the network to learn about both the forward and\nbackward information of a given data point at a given time\ninstance. BLSTM has been successfully applied to activity\nrecognition model by Chen et al.[79] along with an attention-\nbased deep learning module. Rather than assigning the same\nlevel of importance, attention-based modules assign higher\nweights to the features that are more critical for the activity\nrecognition task.\nGRU, a variant of LSTM, contains only 3 gates and\nconnections, which makes it simpler and easier to train than\nLSTM. For effective sequential information learning, Wang et\nal. [85] utilized two GRU layers stacked together to achieve\n98.45% average accuracy compared with a baseline shallow\nCNN network (with 2 layers), which achieved an accuracy of\nonly 79.59%.\nE. RF sensing with Autoencoder\nAs explained in Section II-A, autoencoder is fundamentally\na deep learning technique to extract a compressed knowledge\nrepresentation of the original input. In recent years, this\nproperty of autoencoders has been exploited by researchers\nin different ways to accomplish different RF sensing tasks. In\nthis survey, we propose the taxonomy shown in Figure 9 to\nanalyze state-of-the-art contributions under four different cat-\negories: unsupervised pretraining, data augmentation, do-\nmain translation, and unconventional encoding-decoding.\nIn the following, we brieﬂy review the works in each of these\ncategories.\n1) Unsupervised Pretraining: Training deep neural net-\nworks from a completely random state requires large labelled\ndatasets, which is a fundamental challenge for RF human\nsensing researchers. Also, with random initial weights, deep\nlearning faces the well-known vanishing gradient problem,\nwhich basically means that the gradient descent used for\nbackpropagation fails to update the layers closer to the input\nlayer when the network is deep. This in turn increases the\nrisk of not ﬁnding a good local minimum for the non-\nconvex cost function. It turns out that autoencoders can help\naddress these problems through a two-phase learning protocol\ncalled unsupervised pretraining, which basically builds up\nan unsupervised autoencoder model ﬁrst using only unlabelled\ndata, and later drops the decoder part of the autoencoder\nbut adds a supervised output layer to the encoder part for\nclassiﬁcation. The supervised learning phase may involve\ntraining a simple classiﬁer on top of the compressed features\nlearned by the autoencoder in the pretraining phase, or it\nmay involve supervised ﬁne-tuning of the entire network\nlearned in the pretraining phase. Research has shown that such\nunsupervised pretraining [125, 126] can signiﬁcantly improve\nthe performance of deep learning models in some domains.\nIn RF sensing domain, several researchers reported good\nresults with autoencoder-based pretraining. Shi et al. [95]\nemployed a deep neural network (DNN) with 3 hidden layers\nto detect user activities and authenticate the user at the same\ntime based on the unique ways a user performs each activity.\nWiFi CSI was used as the input for the deep learning. The\nDNN was ﬁrst pretrained with only unlabelled CSI layer-\nby-layer using a stacked autoencoder [127] where a trained\nhidden layer became the input for the next autoencoder. In\nthe supervised learning phase, each of the layer is appended\nwith a softmax classiﬁer, where the ﬁrst layer is used to\ndetect whether the user is stationary or active, the second\nlayer for detecting the activities of the user, and the ﬁnal layer\nto identify the user based on the user behavior during her\nactivities. With the pretraining, the authors of [95] reported\nthat over 90% accuracy on user identiﬁcation and activity\nrecognition could be achieved for 11 subjects even with the\ntraining size of only 4 labelled examples per user.\nSimilar to [95], Chang et al. [90] also used autoencoder for\nlayer-by-layer unsupervised pretraininhg of a 3-layer DNN for\nlocalization based on CSI, which achieved good performance\nfor two different environments. In another localization work\nbased on RSS inputs, Khatab et al. [89] conﬁrmed that layer-\nby-layer autoencoder-based pretraining of a 2-layer extreme\nlearning machine (ELM) improves performance compared to\nthat the case when the ELM is initialized with random weights.\nFinally, in a CSI-based deep learning for localization, Gao et\nal. [87, 91, 92] also demonstrated positive outcomes when\nusing layer-by-layer pretraining with sparse autoencoders,\n13\nTABLE VI: RNN architectures in RF sensing\nPaper\nApplication\nRadio Measurement\nRNN Varient/Layer(s)\nPerformance\n[78]\nActivity Recognition\nCSI\nLSTM 1\nBest Accuracy 0.975\n[85]\nGait Recognition\nCW Radar\nGRU 2\nAvg. Accuracy 0.9177\n[21]\nActivity Recognition\nCSI\nLSTM 1\nAccuracy\n0.95\n[84]\nHuman Counting\nRSSI\nLSTM 1\nAccuracy\nUpto 2 persons 1.0\nUp to 10 persons 0.59\n[80]\nMulti Activity Recognition\nCSI\nLSTM 3\nAvg. Accuracy 0.962\n[3]\nActivity Recognition\nCSI\nLSTM 1\nBest Accuracy 0.97\n[81]\nActivity Recognition\nCSI\nLSTM1\nBest Accuracy 0.991\n[82]\nGesture Recognition\nRSSI\nLSTM 2\nBest Accuracy 0.91\n[79]\nActivity Recognition\nCSI\nBLSTM 2\nBest Accuracy 0.973\n[86]\nGait Recognition\nCSI\nLSTM 1\nAccuracy 0.96\nAutoencoder usage in RF Sensing\nPretraining\nLayer-by-Layer [90, 91, 95]\nWhole Autoencoder [93]\nData Augmentation\nFiDo [94]\nDomain Adaptation\nAuto-Fi [88]\nUnconventional Encoding-Decoding\nRNN Encoder-Decoder [96]\nRFPose [77]\nPerson-in-WiFi [73]\nFig. 9: Taxonomy of autoencoder usage in RF sensing.\nFig. 10: Convolutional Autoencoder used in [93]\nwhich works even when the the successive hidden layers of\nthe deep neural architecture do not reduce in size.\nZhao et al. [93] combines the merits of convolutional\nspatial learning of CNNs with the unsupervised pretraining\ncapability of autoencoders to design a so called convolutional\nautoencoder (CAE) to localize a user on a grid layout based\non 2D RSS images. Unlike the layer-by-layer pretraining\nimplemented with stacked autoencoders in [89, 90, 95], Zhao\net al. [93] pretrained the entire CAE, after which the decoder\npart is dropped and the fully connected layers together with a\nSoftmax layer are added for localization. The CAE architecture\nand its two-phase pretraining process are illustrated in Figure\n10.\n2) Data Augmentation: One of the challenges in WiFi-\nbased localization is that WiFi location ﬁngerprints experience\nsigniﬁcant inconsistency across different users. This means\nthat deep networks trained on RF data collected from one user\nmay not produce good accuracy when used for other users.\nChen et al., [94] trained a variational autoencoder (VAE) on\na real user and then generated a large number (10 times the\noriginal data) of synthetic CSI data to further train a classiﬁer.\nThe proposed VAE-augmented classiﬁer, called FiDo, resulted\nin 20% accuracy improvement compared to the classiﬁer that\nwas trained without the VAE outputs.\n3) Domain Adaptation: WiFi CSI proﬁles are signiﬁcantly\naffected by environment changes, which makes it challenging\nto generalize a trained model across many domains (’domain’\nrefers to ’environment’). Chen et al., [88] used an autoencoder\nto ‘preserve’ the critical features of the original environment\nwhere the initial training CSI data is collected from. Such\nfeature preservation is achieved during the training phase by\ntraining the autoencoder with unlabelled CSI data. During\nthe inference phase at another environment, the previously\ntrained autoencoder is used to convert the CSI vector from\nthe new environment to another vector that now inherits the\nfeatures of the previous environment. By using the converted\nCSI vector, instead of the actual CSI, as an input to the pre-\ntrained classiﬁer, the detection accuracy for WiFi localization\nis signiﬁcantly improved.\n14\n4) Unconventional Encoding-Decoding: Xu et al.[96] pro-\npose an attention based RNN encoder-decoder model for the\nclassiﬁcation of direction and gait recognition. Attention based\nmachine translations can further improve the accuracy as it\nmimics the human visual attention only to the vital parts when\nrecognition occurs, which improves the performance of the\nmodels when the data collected are noisy. Attention based\nsystems do not give equal importance to all features; instead,\nit focuses more on the important features which signiﬁcantly\nleverage the training effort as well. As depicted in Figure 11,\nthe encoder part consists of a bi-directional RNN with GRU\ncells to maintain the simplicity.\nRF-Pose [77] propose encoder-decoder based deep learning\narchitecture for human pose estimation. It made use of a cross\nmodel approach by ﬁrst generating 2D-human skeletal images\nusing RGB images from camera which works as a teacher\nnetwork and radio heat maps images captured from the FMCW\nhorizontal and vertical arrays as the student network.The\nteacher network facilitates annotation of the radio signal from\nRGB stream to the key point conﬁdence maps. The proposed\nstudent network consists of two autoencoders to correspond\nwith the vertical and horizontal RF images and concatenate the\noutputs at the end. The student network uses fractionaly strided\nconvolutional[128] layers which are used for upscaling the low\nresolution inputs to a higher resolutions while preserving the\nabstract details of the output. This serves as the decoder part\nof the proposed architecture where the up sampling process\nis learned by the network itself rather than Hard coding the\nprocess. The architecture of the proposed network is depicted\nin Figure 12. The Teacher-Student design of the deep learning\narchitecture facilitate the cross model pose estimation which\nachieves 62.4% average precision compared with the baseline\n68.8% but through the wall scenario achieves 58.1% precision\nwhere the vision based baseline system completely fails. More\nimportantly, RF-Pose tracks multiple persons simultaneously.\nFig. 11: The RNN encoder-decoder architecture used in [96]\nPerson-in-WiFi[73] utilizes a U-net style autoencoder to\nmap CSI data captures by 3 × 3 MIMO WiFi setup with\ncorresponding 2D-pose of people in the sensing area. CSI\nis concurrently being mapped to 3 pose representations to\nthe body Segmentation Mask (SM), Joint Heatmaps (JHMs)\nand Part Afﬁnity Fields (PAFs) consecutively. SMs and JMMs\nshare one U-net and PAFs share another thus the architecture\ncontains two autoencoders. It is noteworthy to mention that the\nloss function, Mathew weight to optimize the learning process\nof JMMs and PAFs is chosen such a way that more attention\nis payed for improving the skeletal representation of the body\nthan background of the image (which is black). The solution\nproves that the person’s 2D pose can be perceived through 1D\nWiFi data.\nF. RF Sensing with Adversarial Networks\nRF measurements of human activities usually contain sig-\nniﬁcant information that is speciﬁc to the user, i.e., the\nbody shape, position, and orientation to the radio receiver,\nas well as the physical environment, i.e., walls, furniture etc.\nConsequently, an activity classiﬁer trained with one user in\na speciﬁc environment do not perform reliably when tested\nwith another person in another environment. In the literature,\nthe user-environment combination is often referred to as a\ndomain.\nTo achieve ubiquitous RF sensing models that can be\ndeployed across different domains, it is imperative to extract\nfeatures from the ‘noisy’ RF measurements that only represent\nthe activities of the user without being inﬂuenced by domain\nspeciﬁc properties as much as possible. One way to achieve\nthis is to design hand-crafted features to model the motion\nor velocity components of the activity, which clearly do not\ndepend on the domain yet can identify activities based on their\nunique motion proﬁles. Examples of this approach include\nCARM [129], Widar 3.0 [34], and WiPose [119]. While these\nmodeling-based solutions can achieve generalization across\ndomains (a.k.a. domain adaptation) to some extent, they\nrequire rather precise knowledge of the physical layout in\nterms of the user location/orientation and the radio transmitters\nand receivers. In some cases [34], they work well only when\nmultiple RF receivers are installed in the sensing area.\nIn recent years, researchers have demonstrated that adver-\nsarial networks can be an effective deep learning tool to\nrealize RF domain adaptation without having to worry about\nthe speciﬁc positions and orientations of the users and the\nRF receivers. For RF domain adaptation, adversarial networks\nwere used in two different ways, unsupervised adversarial\ntraining and semi-supervised generative adversarial net-\nwork (SGAN).\n1) Unsupervised Adversarial Training: Unsupervised ad-\nversarial training is a well-known domain adaptation technique\nused in many ﬁelds [130, 131]. Its basic principle is illustrated\nin Figure 13. There are three main interconnected components:\nfeature extractor, activity classiﬁer, and domain discrim-\ninator. The feature extractor takes labeled input from the\nsource domain, but only unlabeled data from the target domain.\nThe goal of the classiﬁer is to predict the activity, while the\ndiscriminator tries to predict the domain label. The feature\nextractor tries its best to cheat the domain discriminator,\ni.e., minimize the accuracy for domain prediction, and at\nthe same time maximize the predictive performance of the\nactivity classiﬁer. By playing this minimax game, the network\neventually learns the features for all the activities that are\ndomain invariant. Table VII compares several works that\nemployed the basic philosophy of Figure 13 to generalize RF\nsensing classiﬁers across multiple domains.\n2) Semi-supervised GAN: In Section II-B, we have learned\nthat GAN is a special kind of adversarial network that trains\na generator to produce realistic fake samples. Although the\n15\nFig. 12: The Teacher-Student Network used in [77]\nTABLE VII: Summary of works involving adversarial networks in RF sensing\nPaper\nMonitoring\nApplication\nRF\nMeasurement\nDomain Adaptation Across\nAccuracy %\nw/o Adapt.\nw/ Adapt.\nDIRT-T [98]\nActivity\nCSI (WiFi)\n2 rooms\n35.7\n53\nEIGUR [102]\nGesture\nRSS & Phase\n(RFID)\n3 rooms and 15 subjects\n87.2 (Prec.)\n86.2 (Recall)\n96.6 (Prec.)\n96 (Recall)\nRF-Sleep [75]\nSleep\nFMCW Radar\n25 subjects\n-\n79.8\nDeepMV [44]\nActivity\nCSI (WiFi)\n3 rooms and 8 subjects\n-\n83.7\nEI [43]\nActivity\nCSI(WiFi)\nCIR(mmWave)\n3 rooms & 11 subjects (WiFi)\n4 rooms & 10 subjects (mmWave)\n-\n78\n65\nWiCAR [99, 100]\nActivity\nCSI (WiFi)\n4 cars, 4 subs & 4 driving conditions\n53\n83\nCsiGAN [97]\nGesture/Fall\nCSI (WiFi)\n5 subs (Gest.)/3 subs (Fall)\n-\n-\n84.17 (Gest.)\n86.27 (Fall)\nActivity\nClassifier\nFeature \nExtraction\nDomain \nDiscriminator\nActivity\nLabel\nLabeled Data \nfrom Source Domain \nUnlabeled Data \nfrom Target Domain \nDomain\nLabel\nFig. 13: Principles of unsupervised adversarial training.\ngenerator of a GAN is trained with the help of a discriminator,\nit is the generator that is used eventually to fake samples while\nthe discriminator is of no further use in the post-training phase.\nSemi-supervised GAN (SGAN) [132] is a recent proposal\nthat extends GAN to achieve classiﬁcation as an added func-\ntionality in addition to the generation of fake samples. As\nillustrated in Figure 14, only the discriminator is extended\nwhile the generator remains intact. In terms of its input, the\ndiscriminator now takes some labeled real samples in addition\nto the unlabeled real samples. The discriminator network is\nextended to classify the samples detected as real into 𝑘classes\nby learning these classes from the labeled samples. A key\nbeneﬁt of SGAN as a classiﬁer is to learn to classify reliably\nwith only a small amount of labeled samples as it can still\nlearn signiﬁcantly from the vast amount of unlabeled samples\nwhile playing the minimax game with the generator.\nIn their proposed RF sensing system called CSI-GAN, Xiao\nD\nG\nNoise\nFake\nReal (unlabeled)\nReal\nFake\nD\nG\nNoise\nFake\nReal (unlabeled)\nReal\nFake\nReal (labeled)\nClass 1\nClass 2 \nClass k \n(a) GAN\n(b) Semi-supervised GAN \nFig. 14: GAN vs. semi-supervised GAN.\net al. [97] successfully applied the concept of SGAN to realize\ndomain adaptation across unseen (target) users. The main\nchallenge of this application was that the amount of unlabeled\nCSI samples that could be collected from the target user is\nseverely limited due to the need for avoiding lengthy training\nfor new users. It was observed though that the performance\nof SGAN deteriorated in the case of limited unlabeled data,\n16\nbecause the generator could produce fake samples of only\nlimited diversity due to the limited unlabeled data available\nfrom the target user.\nCSI-GAN addressed the limited unlabeled data issue in\nSGAN by adding a second complement generator that used\nthe concept of CycleGAN [133] to transfer the CSI from the\nsource user to the target user style, thus creating additional\nfake samples. It was shown that such fake sample boosting\nmethod could effectively overcome the issue of limited unla-\nbeled data in SGAN.\nG. Hybrid Deep Learning Models\nFor complex tasks, the basic deep learning models are often\ncombined in a hybrid model. In this section, we summarise\nthe existing hybrid models that proved to be effective in RF\nsensing.\nConvolutional Recurrent Models. This category of models\nstacks convolutions and recurrent blocks sequentially in the\nsame architecture as a way to combine the best of the two\nworlds, i.e., the spatial pattern extraction property of CNNs\nand the temporal modelling capability of RNNs. Empirical\nstudies [134] have conﬁrmed the effectiveness of such hybrid\nmodels across tasks as diverse as car tracking from motion\nsensors, human activity recognition, and user identiﬁcation.\nMoreover, by dividing the input layer into multiple subnets\nfor each input sensor tensor, the model can be used for\nsensor fusion as well. These attractive features were leveraged\nby several researchers for various RF sensing applications.\nDeepSoli [115] uses a CNN followed by an LSTM to map\na sequence of radar frames into a prediction of the gesture\nperformed by the user. The model can recognize 11 complex\nmicro-gestures collected from 10 subjects with 87% accuracy.\nRadHar [108] uses a similar architecture composed of a CNN\nfollowed by a Bi-directional LSTM to predict human activities\nfrom point clouds collected by a mmWave Radar.\nWhile the basic Convolutional Recurrent model worked\nwell across various tasks, it was further enhanced in some\nworks to enable additional input or output processing to\nenhance the accuracy. WiPose [119] uses the Convolutional\nRecurrent model enhanced with post-processing component\nto map 3D Body-coordinate Velocity Proﬁle (BVP) to human\nposes. In addition to CNN and RNN components, the model in\nWiPose was supported by a “Forward Kinematics Layer” that\nrecursively estimates the rotation of the body segments, which\nprovide a smooth skeleton reconstruction of the human body.\nZhou et. al. [109] preﬁx the architecture with an auto-encoder\nas a pre-processing component for reconstructing a de-noised\nversion of the input CSI measurements before forwarding it\nto the core Convolutional Recurrent model.\nDomain Specialized Neural Models. STFNets [107] in-\ntroduced a novel Short-Time Fourier Neural Network that\nintegrates neural networks with time-frequency analysis, which\nallows the network to learn frequency domain representations.\nIt was shown that it improves the learning performance for\napplications that deal with measurements that are fundamen-\ntally a function of signal frequencies such as the signals from\nmotion sensors, WiFi, ultrasound and visible light. The archi-\ntecture was used for several recognition tasks including CSI-\nbased human activity recognition and the evaluation showed\nthat STFNets signiﬁcantly outperformed the state-of-the-art\ndeep learning models.\nIV. REVIEW OF PUBLIC DATASETS\nDeep learning research requires access to large amount of\ndata for training and evaluating proposed neural networks.\nUnfortunately, collecting and labeling radio data for various\nhuman activities is a labor-intensive task. Although most re-\nsearchers are currently collecting their own datasets to evaluate\ntheir deep learning algorithms, access to public datasets would\nhelp accelerate the future research in this area. Besides, due\nto the sensitiveness of radio signals to the actual experimental\nsettings and equipment, comparison of different related works\nbased on different datasets becomes problematic. Fortunately,\nsome researchers have released their datasets in recent years,\ncreating an opportunity for future researchers to reuse them in\ntheir deep learning work.\nWe perform a survey of the publicly available datasets\nthat have already been used in radio-based human sensing\npublications. Our survey only analyzes those datasets that we\nwere able to download and look into. Table VIII reviews\nthe source of the surveyed datasets, year of creation, size\nof the data, radio signal feature collected, hardware used for\ndata collection, and the scope of the data in terms of types\nand numbers of human activities, data collection environment,\nnumber of human participants and so on. We also indicate any\nadditional materials, such as codes implementing deep learning\nmodels that use the datasets, that may have been released along\nwith the datasets. Important observations from this survey are\nsummarized as follows:\n• There are already 20 different datasets from 18 sep-\narate research groups that are publicly available for\nany researchers. Some datasets are released without\nany licenses, while others are under different licensing\nacts mostly for restricting non-academic use. All these\ndatasets were created only in recent years.\n• Activity and gesture are the dominant applications tar-\ngeted by these datasets. Other applications include loca-\ntion/tracking, fall detection, respiratory monitoring, and\npeople counting.\n• The size of these datasets vary widely from mere 18MB\nto 325GB.\n• Number of human participants vary from a single subject\nto 20 subjects.\n• Although half of the datasets collected data from a single\nenvironment, there are several offering data from ﬁve or\nmore different environments with the maximum being\nseven.\n• WiFi CSI collected by Intel 5300 NIC is the most\ncommon data type.\n• Codes implementing the authors’ proposed deep learning\nmodels are also released for most datasets.\nWhile the availability of these datasets is certainly encour-\naging for deep learning research in RF human sensing, we\nidentify several limitations and learn some lessons as follows:\n17\n• Number of participants in the datasets were rather\nlow.\nAlthough\nthe\nassociated\npublication\nfor\nthe\nCrossSense [49] dataset reports deep learning training\nwith data collected from 100 subjects, the publicly re-\nleased dataset actually contains data from only 20 sub-\njects.\n• Many datasets do not mention the gender and age distri-\nbution of the participants. Even when they are mentioned,\nthe actual data is not labeled with gender and age, making\nit difﬁcult to study gender and age speciﬁc characteristics\nof RF sensing.\n• Although our survey in Table III shows that RF device-\nfree localization is a popular application for deep learn-\ning, there appears to be only 2 localization datasets\navailable for public use and both are from the same\nresearch group.\n• All the 20 public datasets were mainly used by the\ncreators themselves. Cross-use of the datasets is still\nrare with the exception of CSIGAN [97], [61], [79]\nand [114] which used the public datasets SignFi [45],\nFallDeFi [135], [3] and [34], respectively.\nV. LESSONS LEARNED AND FUTURE DIRECTIONS\nAlthough deep learning is proving to be an effective tool for\nenhancing RF-based human sensing beyond the state-of-the-\nart, there still exist several roadblocks to fully beneﬁt from it.\nIn this section we discuss some lessons learned and potential\nfuture research directions to combat them.\nA. The Scale of Human Sensing Experiments\nA clear lesson learned from the recent works is that the\nshallow machine learning algorithms cannot cope with human\nsensing tasks at larger scale, where deep learning exhibits\ngreat potentials (see Figure 5). Human sensing can scale in\nmany dimensions, i.e., the practical RF sensing systems will\nbe expected to work reliably over a large user population,\nactivities, physical environments, and RF devices. Deep learn-\ning research therefore must explore all of these dimensions.\nHowever, recent deep learning research considered the scaling\nonly along one of these dimensions. For example, SignFi [45]\nexperiments with 276 sign language gestures, but recruits\nonly 5 subjects working in 2 different physical environments.\nSimilarly, FallDeFi [135] increases the number of physical\nenvironments to 7, but recruits only 3 subjects for the ex-\nperiments. An important future direction, therefore, would be\nto conduct truly large-scale experiments with scaling achieved\nsimultaneously in multiple dimensions of the sensing problem.\nB. Automatic Labeling\nManual labeling of RF sensing data is extremely inefﬁcient\nbecause, unlike vision data labeling which can be done off-\nline by watching camera recordings, RF data usually is not\nintuitive and humans cannot directly interpret it through vi-\nsual inspection. This forces RF labeling to be done on-line\neither by external persons observing the experiments, or the\nsubjects carefully following explicit instructions to perform\nthe activities, which increases labeling effort and reduces\nthe quality1 of the data considerably. To facilitate large-scale\ndeep learning research for human sensing, a future direction\nshould focus on developing novel tools and techniques that\ncan automatically label RF data collected passively in the wild\nfrom many environments capturing data from a vast population\nperforming a myriad of activities as part of their daily routines.\nOne option for automatic labeling could be the use of a\nnon-RF modality to record the same scene at the same time\nas observed by the RF. Then, if the events and activities could\nbe labeled automatically from the non-RF sensor data, then the\nsame labels could be used for the RF data as well. Zhao et\nal. [42, 77] has recently pursued this philosophy successfully\nusing camera as the non-RF modality, where multiple cameras\nwere installed in the RF environment, synchronized with the\nRF recording device, and human pose was later detected\nfrom camera output automatically using image processing\nto generate the labels for the RF source. This is clearly a\npromising direction and worthy of further exploration.\nC. Learning from Unlabeled CSI Data\nA fundamental pitfall of deep learning is that it requires\nmassive amount of training data to adequately learn the\nlatent features. As acquisition of vast amount of labeled RF\ndata incurs signiﬁcant difﬁculty and overhead, in addition\nto automatic labeling, future research should also investigate\nefﬁcient exploitation of unlabeled data, which is much easier\nto collect or may be already available elsewhere. Indeed, over\nthe years, the machine learning community has discovered\nefﬁcient methods for exploiting freely available unlabeled data\nto reduce the burden of labeled data collection. As these\nmethods have proven very successful in image, audio and\ntext classiﬁcations, it would be worth exploring them for WiFi\nsensing.\nSemi-supervised learning is a machine learning approach\nthat combines a small amount of labeled data with a large\namount of unlabeled data during training. In this approach,\nthe knowledge gained from the vast amount of rather easily\nobtainable unlabeled data can signiﬁcantly help the supervised\nclassiﬁcation task at hand, which consequently requires only\na small amount of labeled data to achieve good performance.\nAlthough typical semi-supervised learning methods would\nhelp reduce the burden of collecting massive amount of labeled\ndata to some extend, they usually require [147] the unlabeled\ndata to contain the same label classes that the classiﬁer is\ntrained to classify. For CSI-based activity classiﬁcation, this\nmeans that the unlabeled data must also collect CSI when\nthe humans in the area are performing some speciﬁc set of\nactivities of interest, such as falling to the ground if fall\ndetection is the sensing task. Conventional semi-supervised\nlearning therefore is not applicable to WiFi sensing tasks\nresponsible for detecting rare events, such as falls, or have\na very large number of activity classes, such as detection of\nsign language.\n1Data collected naturally in the wild is preferred over data collected in\ncontrolled laboratory settings.\n18\nTABLE VIII: Public datasets of labeled radio signal measurements for human activities\nSource/Year\nLicense/Repository\n#Envs Applications\nSubjects\n#ActivitiesSize\nData-\ntype/Hardware\nAdditional Items\nCrossSense [49] 2018\nApache license v2\nGitHub\n3\nGait &\nGesture\nRecognition\n20\n40\n4.21 GB\nIntel5300(CSI)/\nXiaoMI\nNote2\nSmartphone\n(RSSI)\n✓Implementation code\nWidar 1.0 [136]\n2017\nPrivate repo\n1\nLocalization &\nTracking\n5[M4,F1]\nAge[20-25]\n5\n2.76GB\nIntel5300(CSI)\n✓Implementation code\nWidar 2.0 [137]\n2018\nPrivate repo\n3\nLocalization &\nTracking\n6[M4,F2]\n3\n303MB\nIntel5300(CSI)\n✓Implementation code\nWidar 3.0 [34]\n2019\nPrivate repo\n3\nGesture\nRecognition\n[M12,F4]\nAge[23-28]\n22\n325GB\nIntel5300(CSI)\n✓Implementation code\n✓Performing videos\nWiAG[138]\n2017\nPrivate repo\n3\nGesture\nRecognition\n1\n6\n3GB\nIntel5300(CSI)\n-\nSignFi[45]\n2018\nUniversity license\nGitHub\n2\nSign Language\nGesture Recognition\nM5\n276\n6.07GB\nIntel5300(CSI)\n✓Implementation code\n✓Performing videos\n[56]\n2019\nGitHub\n1\nActivity\nRecognition\n1\n6\n300MB\nUSRP\nN210\n(CSI)\n✓Implementation code\nWisture[82]\n2017\nGitHub\n2\nGesture\nRecognition\n1\n3\n58MB\nAndroid\nSmartphone\n(RSS)\n✓Implementation code\n[139]\n2018\nIEEE DataPort\n1\nRespiratory\nMonitoring\n20[M11,F9]\nAverage\nAge[M:55,F:60]\n1\n60GB\nCC1200\nRadio(sub-1\ndB RSS)\nCC2530\nRadio(RSS)\nAtheros\nAR9462(WiFi\nCSI)\nDecawave\nEVB1000(CIR)\nFallDeFi [135]\n2018\nMIT license\nHarvard Dataverse\n7\nFall\nDetection\n3\nAge[27-30]\n11\n2.1GB\nIntel5300(CSI)\n✓Implementation code\n[3]2017\nGNU General Public\nLicense\n1\nActivity\nRecognition\n6\n6\n3.59GB\nIntel5300(CSI)\n✓Implementation code\n[140] 2019\nCC\nBY-NC-SA\nlicense\nGitHub\n1\nActivity\nRecognition\n9\n6\n2.04GB\nIntel5300(CSI)\n✓Visualization code\nRadHAR[108]\n2019\nBSD 3-Clause license\nGitHub\n1\nActivity\nRecognition\nM2\n5\n881MB\nFMCW\nRadar\nIWR1443BOOST\n(Point cloud)\n✓Implementation code\nWiAR[141]\n2019\nDATA4U\n1\nActivity\nRecognition\n10[M5,F5]\n16\n667MB\nIntel5300(CSI)\n✓Implementation code\nCSI-net[76]\n2018\nMIT license\nGitHub\n1\nSign Recognition\nFalling Detection\n1\n10\n18.9 MB\nIntel5300(CSI)\n✓Implementation code\nThere is a particular type of semi-supervised learning,\nknown as self-taught learning (STL) [148], that relaxes the\nrequirement of the unlabeled data to contain the same classes\nas used in the classiﬁcation task. This has vastly enhanced the\napplicability of unlabeled data for challenging classiﬁcation\ntasks in domains such as image, audio, and text. Using\nSTL, the authors of [148] have demonstrated that rhino and\nelephants can be accurately classiﬁed with the knowledge\ngathered from the vast amount of random images, not of\nelephants or rhinos, freely available in the Internet. They\nhave also successfully applied STL to audio classiﬁcation,\nby downloading random speech data to classify speakers,\nand text classiﬁcation. STL for WiFi sensing would mean\nthat any available CSI data, irrespective of the actual human\nactivities involved in the data, could be potentially used by\nany other activity classiﬁcation applications. For example,\nunlabeled CSI collected passively when arbitrary people are\nsimply carrying out their usual activities, such as walking,\n19\nPublic datasets of labeled radio signal measurements for human activities continued from Table VIII\nSource/Year\nLicense/Repository\n#Envs Applications\nSubjects\n#ActivitiesSize\nData-\ntype/Hardware\nAdditional Items\nEHUCOUNT[142]\n2018\nPrivate repo\n6\nPeople\nCounting\n5\n1\n183MB\nAnritsu\nMS2690A(CSI)\n-\nmmGaitNet[143]\n2020\nGitHub\n2\nGait\nRecognition\n95[M45,F50]\nAge[19-27]\nHeight[150-185]cm\nweight[45-115]kg\n1\n913MB\nIWR 1443(Point\ncloud)\n-\n[144]\n2020\nGoogle Drive.\n1\nFacial\nemotions\nrecognition\n10 [M7,F3]\nAge[23-25]\n7\n43GB\nIntel5300(CSI)\nLaptop\nwebcam(video)\n✓Annotated video\n[145] 2020\nCC BY 4.0 licence\nMendeley Data\n1\nHuman-to-Human In-\nteraction recognition\n66 [M6,F3]\nAge(avg±std) [22.1 ±\n3.7]\n12\n4.3GB\nIntel5300(CSI)\n✓Well documented with\ninteraction steps\n[146] 2020\nGitHub\n1\nRespiratory & Vital\nsigns recognition\n11[M7,F4]\nAge\n[34.73± 15.94]\nBMI\n[23.19±3.61] kg/𝑚2\n1\n558MB\nSix-Port-based\nradar system (24\nGHz)\n✓Monitoring is performed\nunder multiple scenarios\n✓Implementation code\nsitting, etc., may provide valuable knowledge when training a\ndeep neural classiﬁer to detect rare and specialised activities,\nsuch as fall or sign language.\nD. Deep Learning on Multi-modal RF Sensing\nThe vast majority of recent works explored learning from a\nsingle RF mode, such as WiFi CSI, mmWave FMCW radar,\nor even the sub-GHz LoRa signals [149]. Since these RF\nmodes work on different spectrum and operate on different\nprinciples, opportunities exist to improve human sensing by\ntraining deep learning networks on the combination of such\nmultiple RF data streams. It is also worthwhile to investigate\ndeep learning networks that can learn from the combination\nof RF and other signals, e.g., acoustic and infrared. To\nachieve power automation, many Internet of Things products\nin future smart homes are expected to be ﬁtted with solar\ncells [150]. Researchers [151] have recently demonstrated that\nphotovoltaic (PV) signals generated by such solar cells contain\ndiscriminating features to detect hand gestures. Thus, deep\nlearning that can be simultaneously trained from both RF and\nPV may lead to more robust human sensing neural networks\nfor ubiquitous deployments.\nE. Privacy and Security for WiFi Sensing\nDeep learning is enhancing WiFi sensing capability on\nmultiple fronts. First, it helps to recognise human actions\nwith greater accuracy. Second, more detailed and ﬁne-grained\ninformation about humans, such as smooth tracking of 3D\npose [119], can be detected with deep learning. Finally, re-\nsearchers are now exploring deep learning solutions that make\ncross-domain sensing less strenuous. While the combined\neffect of these deep learning advancements no doubt will make\nWiFi a powerful human sensing tool, they will unfortunately\nalso pose a serious privacy threat. For example, armed with\na cross-domain deep learning classiﬁer trained elsewhere, a\nburglar can easily detect whether any target house is currently\nempty (no one in the house), and if not empty then where\nin the house the occupants are located etc. without raising an\nalarm. Similarly, given the WiFi signals can penetrate walls,\nwindows, and fabrics, neighbours can pry on us even with\ncurtains shut.\nPrivacy protection against WiFi sensing, therefore, could be\nan important future research direction. This is a challenging\nproblem though, because any solution to foil the sensing\nattempt of an attacker should neither affect any legitimate\nsensing nor any ongoing data communication over the very\nWiFi signals used for sensing. Work on this topic is rare with\nthe exception of [152, 153]. For a single antenna system,\nauthors of [152] showed that it is possible for a legitimate\nsensing device to regenerate a carefully distorted version of the\nsignal to obfuscate the physical information in the WiFi signal\nwithout affecting the logical information, i.e., the actual bits\ncarried in the signal. This is a promising direction, but more\nwork is required to make such techniques work for multi-\nantenna systems, which are becoming increasingly available\nin commodity hardware. It would be also an interesting\nresearch to explore deep learning architectures that can fool\nsuch signal obfuscating and still detect human activities to\nsome extend. This would further push researchers to design\nmore advanced obfuscation techniques resilient to even highly\nsophisticated attackers. To this end, specialised adversarial\nnetworks, as explored in [153], could be designed to effectively\nprevent such adversarial sensing. Zhou et al. [153] have shown\nthat with proper design of the loss function, an adversarial\nnetwork can only reveal some target human behaviour from\nthe CSI data, such as falling of a person, while not allowing\nthe detection of other private behaviours, such as bathing.\nThese are encouraging developments conﬁrming the privacy\nprotection capabilities of deep learning.\nF. Deep Learning for Wide Area RF Sensing\nExisting literature on RF sensing is heavily centred around\nWiFi mainly because of its ubiquity. However, WiFi is mostly\nused indoors and severely limited in range, hindering its use\nfor many wide area and outdoor human sensing applications,\nsuch as gesture control for outdoor utilities (e.g., a vending\n20\nmachine), search and rescue of human survivors in disaster\nzones, terrorist spotting and activity tracking, and so on.\nWide area RF sensing traditionally had not been considered\npractical due to very weak reﬂections off the human targets\nfor the signals that were generated from a distant radio tower.\nSome recent technological developments, however, are creat-\ning new opportunities for wide area RF sensing. Dense de-\nployments of shorter-range cellular towers means that outdoor\nlocations can receive cellular signals from a close-by radio\ntower, increasing the opportunity for a stronger reﬂection off\nthe human body. To support wide area connectivity for various\nlow-power Internet of Things (IoT) sensors, novel wide area\nwireless communications technologies, e.g., LoRa [154] and\nSigFox [155], are being developed. A key distinguishing\nfeature of these wide area IoT communications technologies\nis their capability to process very weak signals. For example,\nLoRa can decode signals as weak as −148dBm. Finally, it\nis now becoming possible to carry wireless base stations in\nlow cost ﬂying drones [156] providing further opportunity to\nextend the sensing coverage over a wide area.\nIndeed, researchers are beginning to explore wide area RF\nsensing by taking advantage of these new developments. Chen\net.al.[157] showed that gestures can be accurately detected in\noutdoor areas using LTE signals, and using a drone-mounted\nLoRa transmitter-receiver pair, Chen et al.[158] demonstrated\nfeasibility of outdoor human localization using LoRa signals.\nWhile these experiments clearly indicate the feasibility of wide\narea RF sensing, they also highlight the severe challenges it is\nfacing. LTE-based gesture was only possible if the user was\nlocated at some speciﬁc spots between the tower and the ter-\nminal[157], which severely reduces the quality of user experi-\nence. Similarly, the LoRa-based outdoor localisation accuracy\nwas limited to 4.6m, which may not be adequate for some\napplications. Finally, for drone-mounted LoRa transceivers,\nthe authors[157] found that drone vibrations cause signiﬁcant\ninterference to the LoRa signals, which had to be addressed\nusing algorithms speciﬁcally designed for the drone in use.\nThese challenges highlight the potential beneﬁt of deep learn-\ning in improving the performance and generalization of wide\narea RF sensing for a wider range of use case and hardware\nscenarios.\nG. RF Sensing in Programmable Wireless Environment\nProgrammable wireless environment (PWE) [159] is a novel\nconcept rapidly gaining attention in the wireless communica-\ntions research community. According to the PWE, the walls\nor any object surface can be coated with a special artiﬁcial\nmetamaterial, that can arbitrarily control the reﬂection, i.e.,\nthe amplitude, phase, and angle, of impinging electromagnetic\nwaves under software control. These surfaces are often dubbed\nas intelligent reﬂective surfaces (IRSs). Thus, with IRS, the\nmultipath of any environment can be precisely controlled to\nrealise the desired effects at the intended receivers promis-\ning unprecedented performance improvements for wireless\ncommunications. Indeed, many research works have recently\nconﬁrmed that IRS-assisted solutions can signiﬁcantly improve\nthe capacity, coverage and energy efﬁciency of existing mobile\nnetworks [160–163].\nWhile current research in PWE is mainly focusing on en-\nhancing the communication performance, the dynamic control\nof the multipath will also affect any sensing task that relies\non wireless multipath for sensing. We envisage the following\nchallenges and future research opportunities for WiFi-based\nhuman sensing in PWE.\n1) Deep learning for IRS-affected CSI: Current WiFi sens-\ning research largely assumes that the multipath reﬂections\nfrom the environment is rather stationary because they bounce\nfrom ﬁxed surfaces, such as walls, tables, and chairs. It\nmakes it easier to detect human activities from the CSI by\nfocusing on the dynamic elements of the multipath created\nby the moving human body parts. However, in PWE, the\nreﬂections from walls and other environmental surfaces can\nbe highly non-stationary due to the dynamic control of their\nreﬂection properties. As a result, the amplitude and phase\nof CSI measurements will be affected not only due to the\nmovement of the human, but also due to the speciﬁc control\npatterns of the IRSs in the environment. This will make it\nmore challenging to classify human activities, which will\nrequire more advanced learning and classiﬁcation techniques\nto separate the IRS-related effect on CSI from the ones caused\nby human activity. New deep learning algorithms may be\ndesigned that can be trained to separate such IRS effects from\nthe CSI measurements.\n2) IRS as a sensor for detecting human activities: The\nPWE vision indicates that an entire wall may be an IRS\nwith massive number of passive elements that can record the\nangle, amplitude, and phase of the impinging electromagnetic\nwaves. Thus as the reﬂections from the human body impinge\non the IRS-coated wall, the wall will have a high-resolution\nview of the human activity and hence can assist in detecting\nﬁne-grained human movements with much greater accuracy\nand ease compared to a single WiFi receiver often considered\nin conventional research. How to design the human activity\ndetection intelligence for the IRS would be an interesting new\nresearch direction, which is likely to beneﬁt from the power\nof deep learning.\nH. Deep learning for multi-person and complex activity recog-\nnition\nTo date, RF has been successfully used to detect only single\nperson and simple (atomic) activities, such as sitting, walking,\nfalling, etc. To take RF sensing to the next level where it can\nused to analyse high level human behaviour, such as whether a\nperson is having dinner in a restaurant or having a conversation\nwith another person, more sophisticated deep learning would\nbe required. Such deep learning would be capable of detecting\nactivities of multiple person simultaneously. Deep multi-task\nlearning, a technique that can learn multiple tasks jointly,\nhas been used by Peng et al. [164] successfully to detect\ncomplex human behaviour from wearable sensors. It would\nbe an interesting future direction to extend such models to\nwork with RF signal data, such WiFi CSI.\nVI. CONCLUSION\nWe have presented a comprehensive survey of deep learn-\ning techniques, architectures, and algorithms recently applied\n21\nto radio-based device-free human sensing. Our survey has\nrevealed that although the utilization of deep learning in\nRF sensing is a relatively new trend, signiﬁcant exploration\nhas already been achieved. It has become clear that deep\nlearning can be an effective tool for improving both the\naccuracy and scope of device-free RF sensing. Researchers\nhave demonstrated deep learning capabilities for sensing new\nphenomena that were not possible with conventional methods.\nDespite these important achievements, progress on domain or\nenvironment independent deep learning models has been slow,\nlimiting their ubiquitous use. Dependency on large amounts of\nlabeled data for training is another major drawback of current\ndeep learning models that must be overcome. Through this\nsurvey, we have also unveiled the existence of many publicly\navailable datasets for labeled radio signal measurements cor-\nresponding to various human activities. With many new deep\nlearning algorithms being discovered each year, these datasets\ncan be readily used in future studies to evaluate and compare\nnew algorithms for RF sensing. We also believe that to further\ncatalyse the deep learning research for RF sensing, researchers\nshould come forward and release more comprehensive datasets\nfor public use.\nVII. ACKNOWLEDGMENT\nThis work is partially supported by a research grant from\nCisco Systems, Inc.\nREFERENCES\n[1]\nYongsen Ma, Gang Zhou, and Shuangquan Wang.\n“WiFi Sensing with Channel State Information: A\nSurvey”. In: ACM Comput. Surv. 52.3 (June 2019),\n46:1–46:36.\n[2]\nJ. Liu, H. Liu, Y. Chen, Y. Wang, and C. Wang.\n“Wireless Sensing for Human Activity: A Survey”.\nIn: IEEE Communications Surveys and Tutorials 22.3\n(2020), pp. 1629–1645.\n[3]\nSiamak Youseﬁ, Hirokazu Narui, Sankalp Dayal, Ste-\nfano Ermon, and Shahrokh Valaee. “A Survey on\nBehavior Recognition Using WiFi Channel State In-\nformation”. In: IEEE Communications Magazine 55.10\n(2017). (Accessed on 13/03/2020), pp. 98–104. URL:\nhttps : / / github . com / ermongroup / WiﬁActivity\nRecognition.\n[4]\nHasmath Farhana Thariq Ahmed, Haﬁsoh Ahmad,\nand Aravind C.V. “Device free human gesture recog-\nnition using Wi-Fi CSI: A survey”. In: Engineer-\ning Applications of Artiﬁcial Intelligence 87 (2020),\np. 103281. ISSN: 0952-1976. DOI: https : / / doi . org /\n10 . 1016 / j . engappai . 2019 . 103281.\nURL: http :\n/ / www . sciencedirect . com / science / article / pii /\nS0952197619302441.\n[5]\nY. He, Y. Chen, Y. Hu, and B. Zeng. “WiFi Vision:\nSensing, Recognition, and Detection with Commodity\nMIMO-OFDM WiFi”. In: IEEE Internet of Things\nJournal (2020), pp. 1–1.\n[6]\nZ. Wang, K. Jiang, Y. Hou, Z. Huang, W. Dou, C.\nZhang, and Y. Guo. “A Survey on CSI-Based Human\nBehavior Recognition in Through-the-Wall Scenario”.\nIn: IEEE Access 7 (2019), pp. 78772–78793.\n[7]\nZ. Wang, K. Jiang, Y. Hou, W. Dou, C. Zhang, Z.\nHuang, and Y. Guo. “A Survey on Human Behavior\nRecognition Using Channel State Information”. In:\nIEEE Access 7 (2019), pp. 155986–156024. ISSN:\n2169-3536.\n[8]\nBin Guo, Yanyong Zhang, Daqing Zhang, and Zhu\nWang. “Special Issue on Device-Free Sensing for Hu-\nman Behavior Recognition”. In: Personal Ubiquitous\nComput. 23.1 (Feb. 2019), 1–2. ISSN: 1617-4909. DOI:\n10.1007/s00779-019-01201-8. URL: https://doi.org/\n10.1007/s00779-019-01201-8.\n[9]\nT. Gu, L. Wang, Z. Wu, X. Tao, and J. Lu. “A Pattern\nMining Approach to Sensor-Based Human Activity\nRecognition”. In: IEEE Transactions on Knowledge\nand Data Engineering 23.9 (2011), pp. 1359–1372.\n[10]\nYuezhong Wu, Qi Lin, Hong Jia, Mahbub Hassan, and\nWen Hu. “Auto-Key: Using Autoencoder to Speed Up\nGait-Based Key Generation in Body Area Networks”.\nIn: Proc. ACM Interact. Mob. Wearable Ubiquitous\nTechnol. 4.1 (Mar. 2020). DOI: 10.1145/3381004. URL:\nhttps://doi.org/10.1145/3381004.\n[11]\nQ. Lin, S. Peng, Y. Wu, J. Liu, W. Hu, M. Hassan,\nA. Seneviratne, and C. H. Wang. “E-Jacket: Posture\nDetection with Loose-Fitting Garment using a Novel\nStrain Sensor”. In: 2020 19th ACM/IEEE International\nConference on Information Processing in Sensor Net-\nworks (IPSN). 2020, pp. 49–60.\n[12]\nR. L. Shinmoto Torres, D. C. Ranasinghe, Qinfeng\nShi, and A. P. Sample. “Sensor enabled wearable RFID\ntechnology for mitigating the risk of falls near beds”.\nIn: 2013 IEEE International Conference on RFID\n(RFID). 2013, pp. 191–198.\n[13]\nCeleno. Wi-Fi Doppler Imaging: Celeno - Wi-Fi Be-\nyond Connectivity. 2020. URL: https://www.celeno.\ncom/wiﬁ-doppler-imaging (visited on 08/10/2020).\n[14]\nEmerald. 2020. URL: https://www.emeraldinno.com/.\n[15]\nWalabot Fall Alert System: Detect Falls with No Wear-\nables. 2020. URL: https://walabot.com/walabot-home\n(visited on 08/10/2020).\n[16]\nKris Thompson, Tina Danelsen Sr. Writer, Tina\nDanelsen, and Sr. Writer. 2020. URL: https://xkcorp.\ncom/ (visited on 08/10/2020).\n[17]\nWireless Artiﬁcial Intelligence. 2020. URL: https : / /\nwww.originwirelessai.com/.\n[18]\nLinksys Aware. 2020. URL: https://www.linksys.com/\nus/linksys-aware/ (visited on 08/10/2020).\n[19]\nH. Abdelnasser, M. Youssef, and K. A. Harras.\n“WiGest: A ubiquitous WiFi-based gesture recognition\nsystem”. In: 2015 IEEE Conference on Computer\nCommunications (INFOCOM). 2015, pp. 1472–1480.\n[20]\nYonglong Tian, Guang-He Lee, Hao He, Chen-Yu Hsu,\nand Dina Katabi. “RF-based fall monitoring using\nconvolutional neural networks”. In: Proceedings of the\n22\nACM on Interactive, Mobile, Wearable and Ubiquitous\nTechnologies 2.3 (2018), p. 137.\n[21]\nF. Wang, W. Gong, J. Liu, and K. Wu. “Channel\nSelective Activity Recognition with WiFi: A Deep\nLearning Approach Exploring Wideband Information”.\nIn: IEEE Transactions on Network Science and Engi-\nneering (2019), pp. 1–1.\n[22]\nWARP Project. URL: http://warpproject.org.\n[23]\nMatt Ettus and Martin Braun. “The Universal Software\nRadio Peripheral (USRP) Family of Low-Cost SDRs”.\nIn: Opportunistic Spectrum Sharing and White Space\nAccess. John Wiley and Sons, Ltd, 2015. Chap. 1,\npp. 3–23.\nISBN: 9781119057246.\nDOI: 10 . 1002 /\n9781119057246 . ch1. eprint: https : / / onlinelibrary .\nwiley.com/doi/pdf/10.1002/9781119057246.ch1. URL:\nhttps : / / onlinelibrary. wiley. com / doi / abs / 10 . 1002 /\n9781119057246.ch1.\n[24]\nFrancesco Gringoli, Matthias Schulz, Jakob Link, and\nMatthias Hollick. “Free Your CSI: A Channel State\nInformation Extraction Platform For Modern Wi-Fi\nChipsets”. In: Proceedings of the 13th International\nWorkshop on Wireless Network Testbeds, Experimental\nEvaluation and Characterization. WiNTECH ’19. Los\nCabos, Mexico: Association for Computing Machin-\nery, 2019, 21–28. ISBN: 9781450369312. DOI: 10 .\n1145 / 3349623 . 3355477. URL: https : / / doi . org / 10 .\n1145/3349623.3355477.\n[25]\nXuyu Wang, Chao Yang, and Shiwen Mao. “Tensor-\nBeat: Tensor decomposition for monitoring multiper-\nson breathing beats with commodity WiFi”. In: ACM\nTransactions on Intelligent Systems and Technology\n(TIST) 9.1 (2017), p. 8.\n[26]\nAbdelwahed Khamis, Chun Tung Chou, Branislav\nKusy, and Wen Hu. “Cardioﬁ: Enabling heart rate\nmonitoring on unmodiﬁed cots wiﬁdevices”. In: Pro-\nceedings of the 15th EAI International Conference on\nMobile and Ubiquitous Systems: Computing, Network-\ning and Services. 2018, pp. 97–106.\n[27]\nXuyu Wang, Chao Yang, and Shiwen Mao. “Phase-\nBeat: Exploiting CSI phase data for vital sign moni-\ntoring with commodity WiFi devices”. In: 2017 IEEE\n37th International Conference on Distributed Comput-\ning Systems (ICDCS). IEEE. 2017, pp. 1230–1239.\n[28]\nYouwei Zeng, Dan Wu, Ruiyang Gao, Tao Gu, and\nDaqing Zhang. “FullBreathe: Full human respiration\ndetection exploiting complementarity of CSI phase and\namplitude of WiFi signals”. In: Proceedings of the\nACM on Interactive, Mobile, Wearable and Ubiquitous\nTechnologies 2.3 (2018), pp. 1–19.\n[29]\nCesar Iovescu and Sandeep Rao. “The fundamentals\nof millimeter wave sensors”. In: Texas Instruments,\nSPYY005 (2017).\n[30]\nAndrea Goldsmith. “Path Loss and Shadowing”.\nIn:\nWireless\nCommunications.\nCambridge\nUniversity Press, 2005, 27–63.\nDOI: 10 . 1017 /\nCBO9780511841224.003.\n[31]\nKun Qian, Chenshu Wu, Zimu Zhou, Yue Zheng,\nZheng Yang, and Yunhao Liu. “Inferring motion direc-\ntion using commodity wi-ﬁfor interactive exergames”.\nIn: Proceedings of the 2017 CHI Conference on Hu-\nman Factors in Computing Systems. 2017, pp. 1961–\n1972.\n[32]\nQifan Pu, Sidhant Gupta, Shyamnath Gollakota, and\nShwetak Patel. “Whole-Home Gesture Recognition\nUsing Wireless Signals”. In: Proceedings of the 19th\nAnnual International Conference on Mobile Comput-\ning and Networking. MobiCom ’13. Miami, Florida,\nUSA: Association for Computing Machinery, 2013,\n27–38. ISBN: 9781450319997. DOI: 10.1145/2500423.\n2500436. URL: https : / / doi . org / 10 . 1145 / 2500423 .\n2500436.\n[33]\nWei Wang, Alex X. Liu, and Muhammad Shahzad.\n“Gait Recognition Using WiﬁSignals”. In: Proceed-\nings of the 2016 ACM International Joint Conference\non Pervasive and Ubiquitous Computing. UbiComp\n’16. Heidelberg, Germany: Association for Computing\nMachinery, 2016, 363–373.\nISBN: 9781450344616.\nDOI: 10.1145/2971648.2971670. URL: https://doi.\norg/10.1145/2971648.2971670.\n[34]\nYue Zheng, Yi Zhang, Kun Qian, Guidong Zhang,\nYunhao Liu, Chenshu Wu, and Zheng Yang. “Zero-\nEffort Cross-Domain Gesture Recognition with Wi-\nFi”. In: Proceedings of the 17th Annual Interna-\ntional Conference on Mobile Systems, Applications,\nand Services. (Accessed on 13/03/2020). ACM. 2019,\npp. 313–325. URL: http://tns.thss.tsinghua.edu.cn/\nwidar3.0/index.html.\n[35]\nXin Yang, Jian Liu, Yingying Chen, Xiaonan Guo,\nand Yucheng Xie. “MU-ID: Multi-user Identiﬁcation\nThrough Gaits Using Millimeter Wave Radios”. In:\nIEEE INFOCOM 2020-IEEE Conference on Computer\nCommunications. IEEE. 2020.\n[36]\nYaxiong Xie, Jie Xiong, Mo Li, and Kyle Jamieson.\n“mD-Track: Leveraging multi-dimensionality for pas-\nsive indoor Wi-Fi tracking”. In: The 25th Annual\nInternational Conference on Mobile Computing and\nNetworking. ACM. 2019, pp. 1–16.\n[37]\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton.\n“Deep learning”. In: nature 521.7553 (2015), pp. 436–\n444.\n[38]\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep Learning. http : / / www. deeplearningbook . org.\nMIT Press, 2016.\n[39]\nSamira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman\nTian, Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu,\nShu-Ching Chen, and S. S. Iyengar. “A Survey on\nDeep Learning: Algorithms, Techniques, and Appli-\ncations”. In: ACM Comput. Surv. 51.5 (Sept. 2018).\nISSN: 0360-0300. DOI: 10.1145/3234150. URL: https:\n//doi.org/10.1145/3234150.\n[40]\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Franc¸ois Laviolette,\nMario Marchand, and Victor Lempitsky. “Domain-\nAdversarial Training of Neural Networks”. In: J.\nMach. Learn. Res. 17.1 (Jan. 2016), 2096–2030. ISSN:\n1532-4435.\n23\n[41]\nFjodor van Veen. The Neural Network Zoo. 2019. URL:\nhttps://www.asimovinstitute.org/neural-network-zoo/.\n[42]\nMingmin Zhao, Yonglong Tian, Hang Zhao, Moham-\nmad Abu Alsheikh, Tianhong Li, Rumen Hristov,\nZachary Kabelac, Dina Katabi, and Antonio Torralba.\n“RF-based 3D Skeletons”. In: Proceedings of the 2018\nConference of the ACM Special Interest Group on Data\nCommunication. SIGCOMM ’18. Budapest, Hungary:\nACM, 2018, pp. 267–281. ISBN: 978-1-4503-5567-4.\n[43]\nWenjun\nJiang,\nChenglin\nMiao,\nFenglong\nMa,\nShuochao Yao, Yaqing Wang, Ye Yuan, Hongfei Xue,\nChen\nSong,\nXin\nMa,\nDimitrios\nKoutsonikolas,\net\nal.\n“Towards\nenvironment\nindependent\ndevice\nfree human activity recognition”. In: Proceedings\nof the 24th Annual International Conference on\nMobile Computing and Networking. ACM. 2018,\npp. 289–304.\n[44]\nHongfei Xue, Wenjun Jiang, Chenglin Miao, Fenglong\nMa, Shiyang Wang, Ye Yuan, Shuochao Yao, Aidong\nZhang, and Lu Su. “DeepMV: Multi-View Deep\nLearning for Device-Free Human Activity Recogni-\ntion”. In: Proceedings of the ACM on Interactive,\nMobile, Wearable and Ubiquitous Technologies 4.1\n(2020), pp. 1–26.\n[45]\nYongsen\nMa,\nGang\nZhou,\nShuangquan\nWang,\nHongyang Zhao, and Woosub Jung. “SignFi: Sign\nLanguage Recognition Using WiFi”. In: Proc. ACM\nInteract. Mob. Wearable Ubiquitous Technol. 2.1 (Mar.\n2018). (Accessed on 13/03/2020), 23:1–23:21. URL:\nhttps://github.com/yongsen/SignFi.\n[46]\nShing-Jiuan Liu, Ronald Y. Chang, and Feng-Tsun\nChien. “Analysis and Visualization of Deep Neural\nNetworks in Device-Free Wi-Fi Indoor Localization”.\nIn: IEEE Access 7 (2019), pp. 69379–69392.\n[47]\nR. Zhou, M. Tang, Z. Gong, and M. Hao. “FreeTrack:\nDevice-Free Human Tracking With Deep Neural Net-\nworks and Particle Filtering”. In: IEEE Systems Jour-\nnal (2019), pp. 1–11.\n[48]\nX. Wu, Z. Chu, P. Yang, C. Xiang, X. Zheng, and\nW. Huang. “TW-See: Human Activity Recognition\nThrough the Wall With Commodity Wi-Fi Devices”.\nIn: IEEE Transactions on Vehicular Technology 68.1\n(2019), pp. 306–319. DOI: 10 . 1109 / TVT . 2018 .\n2878754.\n[49]\nJie Zhang, Zhanyong Tang, Meng Li, Dingyi Fang,\nPetteri Nurmi, and Zheng Wang. “CrossSense: To-\nwards Cross-Site and Large-Scale WiFi Sensing”. In:\nProceedings of the 24th Annual International Confer-\nence on Mobile Computing and Networking. MobiCom\n’18. (Accessed on 13/03/2020). New Delhi, India:\nACM, 2018, pp. 305–320. ISBN: 978-1-4503-5903-0.\nURL: https://github.com/nwuzj/CrossSense.\n[50]\nShangqing Liu, Yanchao Zhao, and Bingzhang Chen.\n“WiCount: A Deep Learning Approach for Crowd\nCounting Using WiFi Signals”. In: 2017 IEEE In-\nternational Symposium on Parallel and Distributed\nProcessing with Applications and 2017 IEEE Inter-\nnational Conference on Ubiquitous Computing and\nCommunications (ISPA/IUCC) (2017), pp. 967–974.\n[51]\nY. Cheng and R. Y. Chang. “Device-Free Indoor\nPeople Counting Using Wi-Fi Channel State Informa-\ntion for Internet of Things”. In: GLOBECOM 2017 -\n2017 IEEE Global Communications Conference. 2017,\npp. 1–6.\n[52]\nS. Fang, C. Li, W. Lu, Z. Xu, and Y. Chien.\n“Enhanced Device-Free Human Detection: Efﬁcient\nLearning From Phase and Amplitude of Channel State\nInformation”. In: IEEE Transactions on Vehicular\nTechnology 68.3 (2019), pp. 3048–3051.\n[53]\nL. Zhao, H. Huang, S. Ding, and X. Li. “An Accu-\nrate and Efﬁcient Device-Free Localization Approach\nBased on Gaussian Bernoulli Restricted Boltzmann\nMachine”. In: 2018 IEEE International Conference\non Systems, Man, and Cybernetics (SMC). 2018,\npp. 2323–2328.\n[54]\nQ. Zhou, J. Xing, J. Li, and Q. Yang. “A Device-\nFree Number Gesture Recognition Approach Based\non Deep Learning”. In: 2016 12th International Con-\nference on Computational Intelligence and Security\n(CIS). 2016, pp. 57–63.\n[55]\nChenwei Cai, Li Juan Deng, Mingyang Zheng, and\nShufang Li. “PILC: Passive Indoor Localization Based\non Convolutional Neural Networks”. In: 2018 Ubiq-\nuitous Positioning, Indoor Navigation and Location-\nBased Services (UPINLBS) (2018), pp. 1–6.\n[56]\nFeng Wang, Jianwei Feng, Yinliang Zhao, Xiaobin\nZhang, Shiyuan Zhang, and Jinsong Han. “Joint Ac-\ntivity Recognition and Indoor Localization With WiFi\nFingerprints”. In: IEEE Access 7 (2019). (Accessed on\n13/03/2020), pp. 80058–80068. URL: https://github.\ncom/geekfeiw/ARIL.\n[57]\nShaohe Lv, Yong Lu, Mianxiong Dong, Xiaodong\nWang, Yong Dou, and Weihua Zhuang. “Qualitative\nAction Recognition by Wireless Radio Signals in\nHuman–Machine Systems”. In: IEEE Transactions on\nHuman-Machine Systems 47 (2017), pp. 789–800.\n[58]\nChen-Yu Hsu, Rumen Hristov, Guang-He Lee, Ming-\nmin Zhao, and Dina Katabi. “Enabling identiﬁcation\nand behavioral sensing in homes using radio reﬂec-\ntions”. In: Proceedings of the 2019 CHI Conference on\nHuman Factors in Computing Systems. 2019, pp. 1–13.\n[59]\nTianhong Li, Lijie Fan, Mingmin Zhao, Yingcheng\nLiu, and Dina Katabi. “Making the invisible visible:\nAction recognition through walls and occlusions”. In:\nProceedings of the IEEE International Conference on\nComputer Vision. 2019, pp. 872–881.\n[60]\nXu Yang, Fangyuan Xiong, Yuan Shao, and Qiang\nNiu. “WmFall: WiFi-based multistage fall detection\nwith channel state information”. In: International Jour-\nnal of Distributed Sensor Networks 14.10 (2018),\np. 1550147718805718.\n[61]\nBo Wei, Kai Li, Chengwen Luo, Weitao Xu, and\nJin Zhang. “No Need of Data Pre-processing: A Gen-\neral Framework for Radio-Based Device-Free Context\nAwareness”. In: 2019. arXiv: 1908.03398.\n24\n[62]\nH. Zou, J. Yang, H. P. Das, H. Liu, Y. Zhou, and\nC. J. Spanos. “WiFi and Vision Multimodal Learning\nfor Accurate and Robust Device-Free Human Activ-\nity Recognition”. In: 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops\n(CVPRW). 2019, pp. 426–433.\n[63]\nX. Ma, Y. Zhao, L. Zhang, Q. Gao, M. Pan, and\nJ. Wang. “Practical Device-Free Gesture Recognition\nUsing WiFi Signals Based on Metalearning”. In: IEEE\nTransactions on Industrial Informatics 16.1 (2020),\npp. 228–237.\n[64]\nShahzad Ahmed, Faheem Khan, Asim Ghaffar, Farhan\nHussain, and Sung Ho Cho. “Finger-Counting-Based\nGesture Recognition within Cars Using Impulse Radar\nwith Convolutional Neural Network”. In: Sensors 19.6\n(2019).\n[65]\nS.\nSkaria,\nA.\nAl-Hourani,\nM.\nLech,\nand\nR.\nJ. Evans. “Hand-Gesture Recognition Using Two-\nAntenna Doppler Radar With Deep Convolutional\nNeural Networks”. In: IEEE Sensors Journal 19.8\n(2019), pp. 3041–3048.\n[66]\nQ. Zhou, J. Xing, W. Chen, X. Zhang, and Q. Yang.\n“From Signal to Image: Enabling Fine-Grained Ges-\nture Recognition with Commercial Wi-Fi Devices”. In:\nSensors (Basel) 18.9 (2018).\n[67]\nHan Zou, Yuxun Zhou, Jianfei Yang, Hao Lin Jiang,\nLihua Xie, and Costas J. Spanos. “WiFi-enabled\nDevice-free Gesture Recognition for Smart Home Au-\ntomation”. In: 2018 IEEE 14th International Con-\nference on Control and Automation (ICCA) (2018),\npp. 476–481.\n[68]\nPanneer Selvam Santhalingam, Al Amin Hosain,\nDing\nZhang,\nParth\nPathak,\nHuzefa\nRangwala,\nand\nRaja\nKushalnagar.\n“mmASL:\nEnvironment-\nIndependent ASL Gesture Recognition Using 60\nGHz\nMillimeter-wave\nSignals”.\nIn:\nProceedings\nof the ACM on Interactive, Mobile, Wearable and\nUbiquitous Technologies 4.1 (2020), pp. 1–30.\n[69]\nB. Vandersmissen, N. Knudde, A. Jalalvand, I. Couck-\nuyt, A. Bourdoux, W. De Neve, and T. Dhaene. “In-\ndoor Person Identiﬁcation Using a Low-Power FMCW\nRadar”. In: IEEE Transactions on Geoscience and\nRemote Sensing 56.7 (2018), pp. 3941–3952.\n[70]\nIker Sobron, Javier Del Ser, I˜naki Eizmendi, and\nManuel Velez. “A Deep Learning Approach to Device-\nFree People Counting from WiFi Signals”. In: Intelli-\ngent Distributed Computing XII. Ed. by Javier Del Ser,\nEneko Osaba, Miren Nekane Bilbao, Javier J. Sanchez-\nMedina, Massimo Vecchio, and Xin-She Yang. Cham:\nSpringer International Publishing, 2018, pp. 275–286.\n[71]\nHua Huang and Shan Lin. “WiDet: Wi-Fi Based\nDevice-Free Passive Person Detection with Deep Con-\nvolutional Neural Networks”. In: Proceedings of the\n21st ACM International Conference on Modeling,\nAnalysis and Simulation of Wireless and Mobile Sys-\ntems. MSWIM ’18. Montreal, QC, Canada: Associa-\ntion for Computing Machinery, 2018, 53–60. ISBN:\n9781450359603. DOI: 10 . 1145 / 3242102 . 3242119.\nURL: https://doi.org/10.1145/3242102.3242119.\n[72]\nLijie Fan, Tianhong Li, Rongyao Fang, Rumen Hris-\ntov, Yuan Yuan, and Dina Katabi. “Learning Longterm\nRepresentations for Person Re-Identiﬁcation Using\nRadio Signals”. In: arXiv preprint arXiv:2004.01091\n(2020).\n[73]\nFei Wang, Sanping Zhou, Stanislav Panev, Jinsong\nHan, and Dong Huang. “Person-in-WiFi: Fine-grained\nperson perception using WiFi”. In: Proceedings of the\nIEEE International Conference on Computer Vision.\n2019, pp. 5452–5461.\n[74]\nU. M. Khan, Z. Kabir, S. A. Hassan, and S. H. Ahmed.\n“A Deep Learning Framework Using Passive WiFi\nSensing for Respiration Monitoring”. In: GLOBECOM\n2017 - 2017 IEEE Global Communications Confer-\nence. 2017, pp. 1–6.\n[75]\nMingmin Zhao, Shichao Yue, Dina Katabi, Tommi S\nJaakkola, and Matt T Bianchi. “Learning sleep stages\nfrom radio signals: A conditional adversarial archi-\ntecture”. In: Proceedings of the 34th International\nConference on Machine Learning-Volume 70. JMLR.\norg. 2017, pp. 4100–4109.\n[76]\nFei Wang, Jinsong Han, Shiyuan Zhang, Xu He,\nand Dong Huang. CSI-Net: Uniﬁed Human Body\nCharacterization and Pose Recognition. (Accessed on\n13/03/2020). 2018. eprint: arXiv:1810.03064. URL:\nhttps://github.com/geekfeiw/CSI-Net.\n[77]\nM. Zhao, T. Li, M. A. Alsheikh, Y. Tian, H. Zhao, A.\nTorralba, and D. Katabi. “Through-Wall Human Pose\nEstimation Using Radio Signals”. In: 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion. 2018, pp. 7356–7365.\n[78]\nZ. Shi, J. A. Zhang, R. Xu, and G. Fang. “Hu-\nman Activity Recognition Using Deep Learning Net-\nworks with Enhanced Channel State Information”. In:\n2018 IEEE Globecom Workshops (GC Wkshps). 2018,\npp. 1–6.\n[79]\nZ. Chen, L. Zhang, C. Jiang, Z. Cao, and W. Cui.\n“WiFi CSI Based Passive Human Activity Recognition\nUsing Attention Based BLSTM”. In: IEEE Transac-\ntions on Mobile Computing 18.11 (2019), pp. 2714–\n2724.\n[80]\nChunhai Feng, Sheheryar Arshad, Siwang Zhou, Dun\nCao, and Yonghe Liu. “Wi-multi: A Three-phase Sys-\ntem for Multiple Human Activity Recognition with\nCommercial WiFi Devices”. In: IEEE Internet of\nThings Journal (2019).\n[81]\nZ. Shi, J. A. Zhang, R. Xu, and Q. Cheng. “Deep\nLearning Networks for Human Activity Recognition\nwith CSI Correlation Feature Extraction”. In: ICC\n2019 - 2019 IEEE International Conference on Com-\nmunications (ICC). 2019, pp. 1–6.\n[82]\nMohamed Abudulaziz Ali Haseeb and Ramviyas Para-\nsuraman. “Wisture: Rnn-based learning of wireless\nsignals for gesture recognition in unmodiﬁed smart-\nphones”. In: arXiv preprint arXiv:1707.08569 (2017).\n(Accessed on 13/03/2020). URL: https://ieee-dataport.\n25\norg/documents/wi-ﬁ-signal-strength-measurements-\nsmartphone-various-hand-gestures.\n[83]\nHao Kong, Li Lu, Jiadi Yu, Yingying Chen, Linghe\nKong, and Minglu Li. “FingerPass: Finger Gesture-\nBased Continuous User Authentication for Smart\nHomes Using Commodity WiFi”. In: Proceedings of\nthe Twentieth ACM International Symposium on Mo-\nbile Ad Hoc Networking and Computing. Mobihoc ’19.\nCatania, Italy: Association for Computing Machinery,\n2019, 201–210. ISBN: 9781450367646. DOI: 10.1145/\n3323679 . 3326518. URL: https : / / doi . org / 10 . 1145 /\n3323679.3326518.\n[84]\nO. T. Ibrahim, W. Gomaa, and M. Youssef. “Cross-\nCount: A Deep Learning System for Device-Free Hu-\nman Counting Using WiFi”. In: IEEE Sensors Journal\n19.21 (2019), pp. 9921–9928.\n[85]\nM. Wang, G. Cui, X. Yang, and L. Kong. “Human\nbody and limb motion recognition via stacked gated\nrecurrent units network”. In: IET Radar, Sonar Navi-\ngation 12.9 (2018), pp. 1046–1051.\n[86]\nX. Ming, H. Feng, Q. Bu, J. Zhang, G. Yang, and T.\nZhang. “HumanFi: WiFi-Based Human Identiﬁcation\nUsing Recurrent Neural Network”. In: 2019 IEEE\nSmartWorld,\nUbiquitous\nIntelligence\nComputing,\nAdvanced Trusted Computing, Scalable Computing\nCommunications,\nCloud\nBig\nData\nComputing,\nInternet of People and Smart City Innovation (Smart-\nWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI).\n2019, pp. 640–647.\n[87]\nJ. Wang, X. Zhang, Q. Gao, H. Yue, and H.\nWang. “Device-Free Wireless Localization and Ac-\ntivity Recognition: A Deep Learning Approach”. In:\nIEEE Transactions on Vehicular Technology 66.7\n(2017), pp. 6258–6267.\n[88]\nXi Chen, Chen Ma, Michel Allegue, and Xue Liu.\n“Taming the inconsistency of Wi-Fi ﬁngerprints for\ndevice-free passive indoor localization”. In: IEEE IN-\nFOCOM 2017 - IEEE Conference on Computer Com-\nmunications (2017), pp. 1–9.\n[89]\nZ. E. Khatab, A. Hajihoseini, and S. A. Ghorashi. “A\nFingerprint Method for Indoor Localization Using Au-\ntoencoder Based Deep Extreme Learning Machine”.\nIn: IEEE Sensors Letters 2.1 (2018), pp. 1–4.\n[90]\nRonald Y. Chang, Shing-Jiuan Liu, and Yen-Kai\nCheng. “Device-Free Indoor Localization Using Wi-\nFi Channel State Information for Internet of Things”.\nIn: 2018 IEEE Global Communications Conference\n(GLOBECOM) (2018), pp. 1–7.\n[91]\nQ. Gao, J. Wang, X. Ma, X. Feng, and H. Wang. “CSI-\nBased Device-Free Wireless Localization and Activity\nRecognition Using Radio Image Features”. In: IEEE\nTransactions on Vehicular Technology 66.11 (2017),\npp. 10346–10356.\n[92]\nX. Zhang, J. Wang, Q. Gao, X. Ma, and H. Wang.\n“Device-free wireless localization and activity recogni-\ntion with deep learning”. In: 2016 IEEE International\nConference on Pervasive Computing and Communica-\ntion Workshops (PerCom Workshops). 2016, pp. 1–5.\n[93]\nL. Zhao, H. Huang, X. Li, S. Ding, H. Zhao, and Z.\nHan. “An Accurate and Robust Approach of Device-\nFree Localization With Convolutional Autoencoder”.\nIn: IEEE Internet of Things Journal 6.3 (2019),\npp. 5825–5840.\n[94]\nXi Chen, Hang Li, Chenyi Zhou, Xue Liu, Di Wu,\nand Gregory Dudek. “FiDo: Ubiquitous Fine-Grained\nWiFi-Based Localization for Unlabelled Users via\nDomain Adaptation”. In: Proceedings of The Web\nConference 2020. WWW ’20. Taipei, Taiwan: Associ-\nation for Computing Machinery, 2020, 23–33. ISBN:\n9781450370233. DOI: 10 . 1145 / 3366423 . 3380091.\nURL: https://doi.org/10.1145/3366423.3380091.\n[95]\nCong Shi, Jian Liu, Hongbo Liu, and Yingying Chen.\n“Smart User Authentication Through Actuation of\nDaily Activities Leveraging WiFi-enabled IoT”. In:\nProceedings of the 18th ACM International Sympo-\nsium on Mobile Ad Hoc Networking and Computing.\nMobihoc ’17. Chennai, India: ACM, 2017, 5:1–5:10.\nISBN: 978-1-4503-4912-3.\n[96]\nYang Xu, Min Chen, Wei Yang, Siguang Chen, and\nLiusheng Huang. “Attention-based Walking Gait and\nDirection Recognition in Wi-Fi Networks”. In: ArXiv\nabs/1811.07162 (2018).\n[97]\nC. Xiao, D. Han, Y. Ma, and Z. Qin. “CsiGAN: Robust\nChannel State Information-Based Activity Recognition\nWith GANs”. In: IEEE Internet of Things Journal 6.6\n(2019), pp. 10191–10204. ISSN: 2372-2541. DOI: 10.\n1109/JIOT.2019.2936580.\n[98]\nRui Shu, Hung H. Bui, Hirokazu Narui, and Ste-\nfano Ermon. “A DIRT-T Approach to Unsupervised\nDomain Adaptation”. In: 6th International Conference\non Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net, 2018. URL: https:\n//openreview.net/forum?id=H1q-TM-AW.\n[99]\nFangxin Wang, Jiangchuan Liu, and Wei Gong.\n“WiCAR: Wiﬁ-Based in-Car Activity Recognition\nwith Multi-Adversarial Domain Adaptation”. In: Pro-\nceedings of the International Symposium on Qual-\nity of Service. IWQoS ’19. Phoenix, Arizona: As-\nsociation for Computing Machinery, 2019.\nISBN:\n9781450367783. DOI: 10 . 1145 / 3326285 . 3329054.\nURL: https://doi.org/10.1145/3326285.3329054.\n[100]\nF. Wang, J. Liu, and W. Gong. “Multi-Adversarial\nIn-Car Activity Recognition using RFIDs”. In: IEEE\nTransactions on Mobile Computing Early Access\n(2020), pp. 1–1. ISSN: 1558-0660. DOI: 10.1109/TMC.\n2020.2977902.\n[101]\nHan Zou, Jianfei Yang, Yuxun Zhou, and Costas J\nSpanos. “Joint Adversarial Domain Adaptation for\nResilient WiFi-Enabled Device-Free Gesture Recogni-\ntion”. In: 2018 17th IEEE International Conference on\nMachine Learning and Applications (ICMLA). IEEE.\n2018, pp. 202–207.\n[102]\nYinggang Yu, Dong Wang, Run Zhao, and Qian Zhang.\n“RFID based real-time recognition of ongoing gesture\nwith adversarial learning”. In: Proceedings of the 17th\n26\nConference on Embedded Networked Sensor Systems.\n2019, pp. 298–310.\n[103]\nJ. Wang, Q. Gao, X. Ma, Y. Zhao, and Y. Fang.\n“Learning to Sense: Deep Learning for Wireless Sens-\ning with Less Training Efforts”. In: IEEE Wireless\nCommunications (2020), pp. 1–7.\n[104]\nHongfei Xue, Wenjun Jiang, Chenglin Miao, Ye Yuan,\nFenglong Ma, Xin Ma, Yijiang Wang, Shuochao Yao,\nWenyao Xu, Aidong Zhang, and Lu Su. “DeepFu-\nsion: A Deep Learning Framework for the Fusion of\nHeterogeneous Sensory Data”. In: Proceedings of the\nTwentieth ACM International Symposium on Mobile\nAd Hoc Networking and Computing. Mobihoc ’19.\nCatania, Italy: Association for Computing Machinery,\n2019, 151–160. ISBN: 9781450367646. DOI: 10.1145/\n3323679 . 3326513. URL: https : / / doi . org / 10 . 1145 /\n3323679.3326513.\n[105]\nJ. Zhang, F. Wu, W. Hu, Q. Zhang, W. Xu, and J.\nCheng. “WiEnhance: Towards Data Augmentation in\nHuman Activity Recognition Using WiFi Signal”. In:\n2019 15th International Conference on Mobile Ad-Hoc\nand Sensor Networks (MSN). 2019, pp. 309–314.\n[106]\nD. A. Khan, S. Razak, B. Raj, and R. Singh. “Hu-\nman Behaviour Recognition Using WiﬁChannel State\nInformation”. In: ICASSP 2019 - 2019 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). 2019, pp. 7625–7629.\n[107]\nShuochao Yao, Ailing Piao, Wenjun Jiang, Yiran Zhao,\nHuajie Shao, Shengzhong Liu, Dongxin Liu, Jinyang\nLi, Tianshi Wang, Shaohan Hu, et al. “Stfnets: Learn-\ning sensing signals from the time-frequency perspec-\ntive with short-time fourier neural networks”. In: The\nWorld Wide Web Conference. 2019, pp. 2192–2202.\n[108]\nAkash Deep Singh, Sandeep Singh Sandha, Luis Gar-\ncia, and Mani Srivastava. “RadHAR: Human Activity\nRecognition from Point Clouds Generated through a\nMillimeter-wave Radar”. In: Proceedings of the 3rd\nACM Workshop on Millimeter-wave Networks and\nSensing Systems. (Accessed on 13/03/2020). ACM.\n2019, pp. 51–56. URL: https : / / github . com / nesl /\nRadHAR.\n[109]\nH. Zou, Y. Zhou, J. Yang, H. Jiang, L. Xie, and C.\nJ. Spanos. “DeepSense: Device-Free Human Activity\nRecognition via Autoencoder Long-Term Recurrent\nConvolutional Network”. In: 2018 IEEE International\nConference on Communications (ICC). 2018, pp. 1–6.\n[110]\nXiaoyi Fan, Fangxin Wang, Fei Wang, Wei Gong, and\nJiangchuan Liu. “When RFID Meets Deep Learning:\nExploring Cognitive Intelligence for Activity Identiﬁ-\ncation”. In: IEEE Wireless Communications 26 (2019),\npp. 19–25.\n[111]\nF. Wang, W. Gong, and J. Liu. “On Spatial Diver-\nsity in WiFi-Based Human Activity Recognition: A\nDeep Learning-Based Approach”. In: IEEE Internet\nof Things Journal 6.2 (2019), pp. 2035–2047.\n[112]\nXiaoyi Fan, Wei Gong, and Jiangchuan Liu. “TagFree\nActivity Identiﬁcation with RFIDs”. In: IMWUT 2\n(2018), 7:1–7:23.\n[113]\nHan Zou, Yuxun Zhou, Jianfei Yang, and Costas\nJ Spanos. “Towards occupant activity driven smart\nbuildings via WiFi-enabled IoT devices and deep\nlearning”. In: Energy and Buildings 177 (2018).\n[114]\nChenning Li, Manni Liu, and Zhichao Cao. “WiHF:\nEnable User Identiﬁed Gesture Recognition with\nWiFi”. In: IEEE INFOCOM 2020-IEEE Conference\non Computer Communications. IEEE. 2020.\n[115]\nSaiwen Wang, Jie Song, Jaime Lien, Ivan Poupyrev,\nand Otmar Hilliges. “Interacting with Soli: Exploring\nFine-Grained Dynamic Gesture Recognition in the\nRadio-Frequency Spectrum”. In: Proceedings of the\n29th Annual Symposium on User Interface Software\nand Technology. UIST ’16. Tokyo, Japan: ACM, 2016,\npp. 851–860. ISBN: 978-1-4503-4189-9.\n[116]\nZ. Zhang, Z. Tian, and M. Zhou. “Latern: Dynamic\nContinuous Hand Gesture Recognition Using FMCW\nRadar Sensor”. In: IEEE Sensors Journal 18.8 (2018),\npp. 3278–3289.\n[117]\nAnna Huang, Dong Wang, Run Zhao, and Qian Zhang.\n“Au-Id: Automatic User Identiﬁcation and Authenti-\ncation Through the Motions Captured from Sequential\nHuman Activities Using RFID”. In: Proc. ACM In-\nteract. Mob. Wearable Ubiquitous Technol. 3.2 (June\n2019), 48:1–48:26.\n[118]\nShangqing Liu, Yanchao Zhao, Fanggang Xue, Bing\nChen, and Xiang Chen. DeepCount: Crowd Counting\nwith WiFi via Deep Learning. 2019. arXiv: 1903.05316\n[cs.LG].\n[119]\nWenjun Jiang, Hongfei Xue, Chenglin Miao, Shiyang\nWang, Sen Lin, Chong Tian, Srinivasan Murali,\nHaochen Hu, Zhi Sun, and Lu Su. “Towards 3D\nHuman Pose Construction Using Wiﬁ”. In: Proceed-\nings of the 26th Annual International Conference on\nMobile Computing and Networking. MobiCom ’20.\nLondon, United Kingdom: Association for Computing\nMachinery, 2020. ISBN: 9781450370851. DOI: 10 .\n1145 / 3372224 . 3380900. URL: https : / / doi . org / 10 .\n1145/3372224.3380900.\n[120]\nKyungHyun Cho, Alexander Ilin, and Tapani Raiko.\n“Improved learning of Gaussian-Bernoulli restricted\nBoltzmann machines”. In: International conference on\nartiﬁcial neural networks. Springer. 2011, pp. 10–17.\n[121]\nFadel Adib, Zachary Kabelac, Dina Katabi, and Rob\nMiller. “WiTrack: motion tracking via radio reﬂections\noff the body”. In: Proc. of NSDI. 2014.\n[122]\nWentao Zhu, Cuiling Lan, Junliang Xing, Wenjun\nZeng, Yanghao Li, Li Shen, and Xiaohui Xie. “Co-\noccurrence feature learning for skeleton based action\nrecognition using regularized deep LSTM networks”.\nIn: Thirtieth AAAI Conference on Artiﬁcial Intelli-\ngence. 2016.\n[123]\nZhe Cao, Tomas Simon, Shih-En Wei, and Yaser\nSheikh. “Realtime Multi-Person 2D Pose Estimation\nusing Part Afﬁnity Fields”. In: CVPR. 2017.\n[124]\nChunhui Liu, Yueyu Hu, Yanghao Li, Sijie Song, and\nJiaying Liu. “PKU-MMD: A large scale benchmark for\n27\ncontinuous multi-modal human action understanding”.\nIn: arXiv preprint arXiv:1703.07475 (2017).\n[125]\nDumitru Erhan, Yoshua Bengio, Aaron Courville,\nPierre-Antoine Manzagol, Pascal Vincent, and Samy\nBengio. “Why Does Unsupervised Pre-Training Help\nDeep Learning?” In: J. Mach. Learn. Res. 11 (Mar.\n2010), 625–660. ISSN: 1532-4435.\n[126]\nGeoffrey E Hinton and Ruslan R Salakhutdinov. “Re-\nducing the dimensionality of data with neural net-\nworks”. In: science 313.5786 (2006), pp. 504–507.\n[127]\nPascal\nVincent,\nHugo\nLarochelle,\nIsabelle\nLa-\njoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\n“Stacked Denoising Autoencoders: Learning Useful\nRepresentations in a Deep Network with a Local\nDenoising Criterion”. In: J. Mach. Learn. Res. 11\n(Dec. 2010), pp. 3371–3408. ISSN: 1532-4435. URL:\nhttp://dl.acm.org/citation.cfm?id=1756006.1953039.\n[128]\nMatthew D. Zeiler, Dilip Krishnan, Graham W. Taylor,\nand Rob Fergus. “Deconvolutional networks”. English\n(US). In: 2010 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition, CVPR\n2010. 2010, pp. 2528–2535. ISBN: 9781424469840.\nDOI: 10.1109/CVPR.2010.5539957.\n[129]\nWei Wang, Alex X Liu, Muhammad Shahzad, Kang\nLing, and Sanglu Lu. “Understanding and modeling\nof wiﬁsignal based human activity recognition”. In:\nProceedings of the 21st annual international con-\nference on mobile computing and networking. 2015,\npp. 65–76.\n[130]\nGarrett Wilson and Diane J. Cook. “A Survey of\nUnsupervised Deep Domain Adaptation”. In: ACM\nTransactions on Intelligent Systems and Technology\n11.5 (2020), 1–46. ISSN: 2157-6912. DOI: 10.1145/\n3400066. URL: http://dx.doi.org/10.1145/3400066.\n[131]\nYaroslav Ganin and Victor Lempitsky. “Unsupervised\ndomain adaptation by backpropagation”. In: arXiv\npreprint arXiv:1409.7495 (2014).\n[132]\nTim Salimans, Ian Goodfellow, Wojciech Zaremba,\nVicki Cheung, Alec Radford, and Xi Chen. “Improved\ntechniques for training gans”. In: Advances in neural\ninformation processing systems. 2016, pp. 2234–2242.\n[133]\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei\nA Efros. “Unpaired image-to-image translation using\ncycle-consistent adversarial networks”. In: Proceed-\nings of the IEEE international conference on computer\nvision. 2017, pp. 2223–2232.\n[134]\nWenpeng Yin, Katharina Kann, Mo Yu, and Hin-\nrich Sch¨utze. “Comparative study of cnn and rnn\nfor natural language processing”. In: arXiv preprint\narXiv:1702.01923 (2017).\n[135]\nSameera Palipana, David Rojas, Piyush Agrawal, and\nDirk Pesch. “FallDeFi: Ubiquitous Fall Detection Us-\ning Commodity Wi-Fi Devices”. In: Proc. ACM In-\nteract. Mob. Wearable Ubiquitous Technol. 1.4 (Jan.\n2018). (Accessed on 13/03/2020), 155:1–155:25. ISSN:\n2474-9567. DOI: 10 . 1145 / 3161183. URL: https : / /\ngithub.com/dmsp123/FallDeFi.\n[136]\nKun Qian, Chenshu Wu, Zheng Yang, Yunhao Liu,\nand Kyle Jamieson. “Widar: Decimeter-Level Passive\nTracking via Velocity Monitoring with Commodity\nWi-Fi”. In: Proceedings of the 18th ACM Interna-\ntional Symposium on Mobile Ad Hoc Networking and\nComputing. Mobihoc ’17. (Accessed on 13/03/2020).\nChennai, India: ACM, 2017, 6:1–6:10. ISBN: 978-1-\n4503-4912-3. DOI: 10.1145/3084041.3084067. URL:\nhttp://tns.thss.tsinghua.edu.cn/wiﬁradar/WidarProject.\nzip.\n[137]\nKun Qian, Chenshu Wu, Yi Zhang, Guidong Zhang,\nZheng Yang, and Yunhao Liu. “Widar2.0: Passive Hu-\nman Tracking with a Single Wi-Fi Link”. In: Proceed-\nings of the 16th Annual International Conference on\nMobile Systems, Applications, and Services. MobiSys\n’18. (Accessed on 13/03/2020). Munich, Germany:\nACM, 2018, pp. 350–361. ISBN: 978-1-4503-5720-3.\nDOI: 10.1145/3210240.3210314. URL: http://tns.thss.\ntsinghua.edu.cn/wiﬁradar/Widar2.0Project.zip.\n[138]\nAditya Virmani and Muhammad Shahzad. “Position\nand Orientation Agnostic Gesture Recognition Using\nWiFi”. In: Proceedings of the 15th Annual Interna-\ntional Conference on Mobile Systems, Applications,\nand Services. MobiSys ’17. (Accessed on 13/03/2020).\nNiagara Falls, New York, USA: ACM, 2017, pp. 252–\n264. ISBN: 978-1-4503-4928-4. URL: https://people.\nengr . ncsu . edu / mshahza / publications / Datasets /\nWiAGData.zip.\n[139]\nPeter Hillyard, Anh Luong, Alemayehu Solomon\nAbrar, Neal Patwari, Krishna Sundar, Robert Farney,\nJason Burch, Christina Porucznik, and Sarah Hatch\nPollard. “Experience: Cross-Technology Radio Respi-\nratory Monitoring Performance Study”. In: Proceed-\nings of the 24th Annual International Conference on\nMobile Computing and Networking. MobiCom ’18.\n(Accessed on 13/03/2020). New Delhi, India: ACM,\n2018, pp. 487–496. ISBN: 978-1-4503-5903-0. URL:\nhttps://dataverse.harvard.edu/dataverse/rf respiration\nmonitoring.\n[140]\nJeroen Klein Brinke and Nirvana Meratnia. “Dataset:\nChannel State Information for Different Activities,\nParticipants and Days”. In: Proceedings of the 2nd\nWorkshop on Data Acquisition To Analysis. DATA’19.\n(Accessed on 13/03/2020). New York, NY, USA:\nAssociation for Computing Machinery, 2019, 61–64.\nISBN: 9781450369930.\nURL: https : / / data . 4tu .\nnl / repository / uuid : 42bffa4c - 113c - 46eb - 84a1 -\nc87b6a31a99f.\n[141]\nL. Guo, L. Wang, C. Lin, J. Liu, B. Lu, J. Fang,\nZ. Liu, Z. Shan, J. Yang, and S. Guo. “Wiar: A\nPublic Dataset for Wiﬁ-Based Activity Recognition”.\nIn: IEEE Access 7 (2019). (Accessed on 13/03/2020),\npp. 154935–154945. ISSN: 2169-3536. URL: https://\ngithub.com/linteresa/WiAR.\n[142]\nI. Sobron, J. Del Ser, I. Eizmendi, and M. V´elez.\n“Device-Free People Counting in IoT Environments:\nNew Insights, Results, and Open Challenges”. In:\nIEEE Internet of Things Journal 5.6 (2018). (Accessed\n28\non 13/03/2020), pp. 4396–4408.\nISSN: 2372-2541.\nDOI: 10.1109/JIOT.2018.2806990. URL: http://www.\nehu . eus / tsr radio / index . php / research - areas / data -\nanalytics-in-wireless-networks.\n[143]\nZhen Meng, Song Fu, Jie Yan, Hongyuan Liang,\nAnfu Zhou, Shilin Zhu, Huadong Ma, Jianhua Liu,\nand Ning Yang. “Gait Recognition for Co-Existing\nMultiple People Using Millimeter Wave Sensing”.\nIn: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence. Vol. 34. 01. 2020, pp. 849–856. URL:\nhttps://github.com/mmGait/people-gait.\n[144]\nYu Gu, Xiang Zhang, Zhi Liu, and Fuji Ren.\n“WiFE:\nWiFi\nand\nVision\nbased\nIntelligent\nFacial-Gesture\nEmotion\nRecognition”.\nIn:\narXiv\npreprint\narXiv:2004.09889\n(2020).\nURL:\nhttps : / / drive . google . com / drive / folders /\n1OdNhCWDS28qT21V8YHdCNRjHLbe042eG.\n[145]\nRami Alazrai, Ali Awad, Baha’A. Alsaify, Moham-\nmad Hababeh, and Mohammad I. Daoud. “A dataset\nfor Wi-Fi-based human-to-human interaction recogni-\ntion”. In: Data in Brief 31 (2020), p. 105668. ISSN:\n2352-3409. DOI: https://doi.org/10.1016/j.dib.2020.\n105668. URL: http://www.sciencedirect.com/science/\narticle/pii/S235234092030562X.\n[146]\nKilin Shi, Sven Schellenberger, Christoph Will, To-\nbias Steigleder, Fabian Michler, Jonas Fuchs, Robert\nWeigel, Christoph Ostgathe, and Alexander Koelpin.\n“A dataset of radar-recorded heart sounds and vital\nsigns including synchronised reference sensor signals”.\nIn: Scientiﬁc Data 7 (2020). URL: https://gitlab.com/\nkilinshi/scidata vsmdb.\n[147]\nKamal Nigam, Andrew Kachites McCallum, Sebastian\nThrun, and Tom Mitchell. “Text classiﬁcation from\nlabeled and unlabeled documents using EM”. In: Ma-\nchine learning 39.2-3 (2000), pp. 103–134.\n[148]\nRajat Raina, Alexis Battle, Honglak Lee, Benjamin\nPacker, and Andrew Y. Ng. “Self-Taught Learning:\nTransfer Learning from Unlabeled Data”. In: Proceed-\nings of the 24th International Conference on Machine\nLearning. ICML ’07. Corvalis, Oregon, USA: Associ-\nation for Computing Machinery, 2007, 759–766. ISBN:\n9781595937933. DOI: 10 . 1145 / 1273496 . 1273592.\nURL: https://doi.org/10.1145/1273496.1273592.\n[149]\nFusang Zhang, Zhaoxin Chang, Kai Niu, Jie Xiong,\nBeihong Jin, Qin Lv, and Daqing Zhang. “Explor-\ning LoRa for Long-Range Through-Wall Sensing”.\nIn: Proc. ACM Interact. Mob. Wearable Ubiquitous\nTechnol. 4.2 (June 2020). DOI: 10.1145/3397326. URL:\nhttps://doi.org/10.1145/3397326.\n[150]\nM. Youssef and M. Hassan. “Next Generation IoT:\nToward Ubiquitous Autonomous Cost-Efﬁcient IoT\nDevices”. In: IEEE Pervasive Computing 18.4 (2019),\npp. 8–11.\n[151]\nDong Ma, Guohao Lan, Mahbub Hassan, Wen Hu,\nMushﬁka B. Upama, Ashraf Uddin, and Moustafa\nYoussef. “SolarGest: Ubiquitous and Battery-Free\nGesture Recognition Using Solar Cells”. In: The 25th\nAnnual International Conference on Mobile Com-\nputing and Networking. MobiCom ’19. Los Cabos,\nMexico: Association for Computing Machinery, 2019.\nISBN: 9781450361699.\nDOI: 10 . 1145 / 3300061 .\n3300129. URL: https : / / doi . org / 10 . 1145 / 3300061 .\n3300129.\n[152]\nYue Qiao, Ouyang Zhang, Wenjie Zhou, Kannan\nSrinivasan, and Anish Arora. “PhyCloak: Obfuscat-\ning Sensing from Communication Signals”. In: 13th\nUSENIX Symposium on Networked Systems Design\nand Implementation (NSDI 16). Santa Clara, CA:\nUSENIX Association, Mar. 2016, pp. 685–699. ISBN:\n978-1-931971-29-4. URL: https : / / www. usenix . org /\nconference / nsdi16 / technical - sessions / presentation /\nqiao.\n[153]\nS. Zhou, W. Zhang, D. Peng, Y. Liu, X. Liao, and H.\nJiang. “Adversarial WiFi Sensing for Privacy Preserva-\ntion of Human Behaviors”. In: IEEE Communications\nLetters 24.2 (2020), pp. 259–263.\n[154]\nLoRa™Modulation Basics. Semtech Corporation.\n2015.\nURL: https : / / web . archive . org / web /\n20190718200516/https://www.semtech.com/uploads/\ndocuments/an1200.22.pdf.\n[155]\nSigfox Device Radio Speciﬁcations. Sigfox. 2020. URL:\nhttps : / / build . sigfox . com / sigfox - device - radio -\nspeciﬁcations.\n[156]\nA. Fotouhi, H. Qiang, M. Ding, M. Hassan, L. G.\nGiordano, A. Garcia-Rodriguez, and J. Yuan. “Survey\non UAV Cellular Communications: Practical Aspects,\nStandardization Advancements, Regulation, and Secu-\nrity Challenges”. In: IEEE Communications Surveys\nTutorials 21.4 (2019), pp. 3417–3442.\n[157]\nWeiyan Chen, Kai Niu, Deng Zhao, Rong Zheng,\nDan Wu, Wei Wang, Leye Wang, and Leye Zhang.\n“Robust Dynamic Hand Gesture Interaction using LTE\nTerminals”. In: IPSN 2020 - Proceedings of the 2020\nInformation Processing in Sensor Networks (2020).\nDOI: 10.1109/IPSN48710.2020.00017.\n[158]\nLili Chen, Jie Xiong, Xiaojiang Chen, Sunghoon Ivan\nLee, Kai Chen, Dianhe Han, Dingyi Fang, Zhanyong\nTang, and Zheng Wang. “WideSee: Towards Wide-\nArea Contactless Wireless Sensing”. In: Proceedings\nof the 17th Conference on Embedded Networked Sen-\nsor Systems. SenSys ’19. New York, New York: As-\nsociation for Computing Machinery, 2019, 258–270.\nISBN: 9781450369503.\nDOI: 10 . 1145 / 3356250 .\n3360031. URL: https : / / doi . org / 10 . 1145 / 3356250 .\n3360031.\n[159]\nC. Liaskos, A. Tsioliaridou, S. Nie, A. Pitsillides,\nS. Ioannidis, and I. F. Akyildiz. “On the Network-\nLayer Modeling and Conﬁguration of Programmable\nWireless Environments”. In: IEEE/ACM Transactions\non Networking 27.4 (2019), pp. 1696–1713.\n[160]\nX. Tan, Z. Sun, D. Koutsonikolas, and J. M. Jornet.\n“Enabling Indoor Mobile Millimeter-wave Networks\nBased on Smart Reﬂect-arrays”. In: IEEE INFOCOM\n2018 - IEEE Conference on Computer Communica-\ntions. 2018, pp. 270–278.\n29\n[161]\nQ. Wu and R. Zhang. “Beamforming Optimization\nfor Intelligent Reﬂecting Surface with Discrete Phase\nShifts”. In: ICASSP 2019 - 2019 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP). 2019, pp. 7830–7833.\n[162]\nDong Ma, Ming Ding, and Mahbub Hassan. “En-\nhancing Cellular Communications for UAVs via In-\ntelligent Reﬂective Surface”. In: IEEE WCNC 2020 -\nIEEE Wireless Communications and Networking Con-\nference. 2020.\n[163]\nMarco Di Renzo, Alessio Zappone, Merouane Debbah,\nMohamed-Slim Alouini, Chau Yuen, Julien de Rosny,\nand Sergei Tretyakov. “Smart Radio Environments\nEmpowered by Reconﬁgurable Intelligent Surfaces:\nHow it Works, State of Research, and Road Ahead”.\nIn: 17th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI ’20). 2020.\n[164]\nLiangying Peng, Ling Chen, Zhenan Ye, and Yi Zhang.\n“AROMA: A Deep Multi-Task Learning Based Simple\nand Complex Human Activity Recognition Method\nUsing Wearable Sensors”. In: Proc. ACM Interact.\nMob. Wearable Ubiquitous Technol. 2.2 (July 2018).\nDOI: 10.1145/3214277. URL: https://doi.org/10.1145/\n3214277.\nIsura Nirmal is currently a PhD researcher in\nSchool of Computer Science and Engineering in\nUniversity of New South Wales(UNSW), Sydney,\nAustralia. He received his BSc in Information\nand Communication Technology from University\nof Colombo, Sri Lanka. His research interests are\nwireless sensing, IoT and deep learning which is the\nfocus for his forthcoming PhD.\nAbdelwahed Khamis is a Research Associate at\nthe School of Computer Science and Engineering,\nUNSW, Sydney and a Visiting Scientist in Data61,\nCSIRO. His current research interests include ubiq-\nuitous and device-free sensing, mobile computing\nand wireless security. Abdelwahed completed his\nPh.D in Computer Science and Engineering from\nUNSW, Australia in 2020. Prior to that he got Bsc\nand Msc in Computer Science from Zagazig Uni-\nversity, Egypt. His PhD research focused on the use\nof RF technologies for medical sensing applications\nand resulted in a number of innovative contact-free sensing system for Hand\nHygiene tracking and vital sign monitoring. His postdoctoral and doctoral\nresearch was sponsored by industry leading companies such as CISCO and\nHuawei.\nMahbub Hassan is a Full Professor in the School\nof Computer Science and Engineering, University of\nNew South Wales, Sydney, Australia. He has PhD\nfrom Monash University, Australia, and MSc from\nUniversity of Victoria, Canada, both in Computer\nScience. He served as IEEE Distinguished Lec-\nturer and held visiting appointments at universities\nin USA, France, Japan, and Taiwan. He has co-\nauthored three books, over 200 scientiﬁc articles,\nand a US patent. He served as editor or guest editor\nfor many journals including IEEE Communications\nMagazine, IEEE Network, and IEEE Transactions on Multimedia. His cur-\nrent research interests include Mobile Computing and Sensing, Nanoscale\nCommunication, and Wireless Communication Networks. More information\nis available from http://www.cse.unsw.edu.au/∼mahbub.\nWen Hu is currently an Associate Professor with the\nSchool of Computer Science and Engineering, Uni-\nversity of New South Wales (UNSW). He has pub-\nlished regularly in the top-rated sensor network and\nmobile computing venues, such as IPSN, SenSys,\nMobiCom, UbiComp, TOSN, the TMC, TIFS and\nthe PROCEEDINGS OF THE IEEE. His research\ninterests focus on the novel applications, low-power\ncommunications, security and compressive sensing\nin sensor network systems and the Internet of Things\n(IoT). He is a Senior Member of ACM and IEEE.\nHe is an Associate Editor of ACM TOSN. He is the General Chair of CPS-\nIoT Week 2020, and serves on the organizing and program committees of\nnetworking conferences, including IPSN, SenSys, MobiSys, MobiCom, and\nIoTDI.\nXiaoqing Zhu is a Sr. Technical Leader at the\nInnovation Labs of Cisco Systems, Inc. Her research\ninterests include Internet video delivery, real-time\ninteractive multimedia communications, distributed\nresource optimization, and machine learning for\nwireless. She holds a B.Eng. in Electronics Engi-\nneering from Tsinghua University, Beijing, China.\nShe received both M.S. and Ph.D. degrees in Electri-\ncal Engineering from Stanford University, CA, USA.\nShe has published over 80 peer-reviewed journal and\nconference papers, receiving the Best Student Paper\nAward at ACM Multimedia in 2007, the Best Presentation Award at IEEE\nPacket Video Workshop in 2013, and the Best Research Paper Award for\nVEHCOM 2017. She has over 30 granted U.S. patents. Xiaoqing has served\nextensively within the multimedia research community, as TPC member and\narea chair for conferences, guest editor for special issues of leading journals.\nand more recently as chair of the MCDIG (Multimedia Content Distribution:\nInfrastructure and Algorithms) Interest Group in Multimedia Communication\nTechnical Committee (MMTC) and Associate Editor for IEEE Transactions\non Multimedia.\n",
  "categories": [
    "eess.SP",
    "cs.LG"
  ],
  "published": "2020-10-23",
  "updated": "2021-02-07"
}