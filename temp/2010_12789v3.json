{
  "id": "http://arxiv.org/abs/2010.12789v3",
  "title": "New Approaches for Natural Language Understanding based on the Idea that Natural Language encodes both Information and its Processing Procedures",
  "authors": [
    "Limin Zhang"
  ],
  "abstract": "We must recognize that natural language is a way of information encoding, and\nit encodes not only the information but also the procedures for how information\nis processed. To understand natural language, the same as we conceive and\ndesign computer languages, the first step is to separate information (or data)\nand the processing procedures of information (or data). In natural language,\nsome processing procedures of data are encoded directly as the structure chunk\nand the pointer chunk (this paper has reclassified lexical chunks as the data\nchunk, structure chunk, and the pointer chunk); some processing procedures of\ndata imply in sentences structures; some requests of processing procedures are\nexpressed by information senders and processed by information receivers. For\nthe data parts, the classification encoding system of attribute information and\nthe information organization architecture (including constitutional structures\nof information sets and the hierarchy between the information sets) were\ndiscussed. In section 2, the theoretical part elaborated in section 2 has been\nverified in examples and proofed that the studies in this paper have achieved\nthe goal of enabling machines to understand the information conveyed in the\ndialogue. In section 4, the author summarizes the basic conditions of\n\"Understanding\", rethinks what \"Understanding\" is and how to proceed. The study\nin this paper provides a practical, theoretical basis and research methods for\nNLU. It also can be applied in large-scale and multi-type information\nprocessing in the artificial intelligence (AI) area.",
  "text": " New Approaches for Natural Language Understanding based on the Idea --  \nthat Natural Language encodes both Information and its Processing Procedures  \nLimin Zhang \nE-mail: liminzhang08@qq.com \n \nAbstract — we must recognize that natural language is a \nway of information encoding, and it encodes not only the \ninformation but also the procedures for how information is \nprocessed. To understand natural language, the same as we \nconceive and design computer languages, the first step is to \nseparate information (or data) and the processing procedures of \ninformation (or data). In natural language, some processing \nprocedures of data are encoded directly as the structure chunk \nand the pointer chunk (this paper has reclassified lexical \nchunks as the data chunk, structure chunk, and the pointer \nchunk); some processing procedures of data imply in sentences \nstructures; some requests of processing procedures are \nexpressed by information senders and processed by information \nreceivers.  For the data parts, the classification encoding system \nof attribute information and the information organization \narchitecture (including constitutional structures of information \nsets and the hierarchy between the information sets) were \ndiscussed. \nIn section III, the theoretical part elaborated in section II \nhas been verified in examples and proofed that the studies in \nthis paper have achieved the goal of enabling machines to \nunderstand the information conveyed in the dialogue. In section \nIV, \nthe \nauthor \nsummarizes \nthe \nbasic \nconditions \nof \n“Understanding”, rethinks what “Understanding” is and how \nto proceed. \nThe study in this paper provides a practical, theoretical \nbasis and research methods for NLU. It also can be applied in \nlarge-scale and multi-type information processing in the \nartificial intelligence (AI) area. \nI. \nRELATIONS BETWEEN INFORMATION, REAL WORLD, \nAND NATURAL LANGUAGE \nA. Introduction \nNatural language processing (NLP) techniques based on \nstatistical models have achieved great success in machine \ntranslation. However, we are still far from letting machines \nunderstand natural languages, even for the simplest words: \n“Apple”.   \nUnlike most previous approaches of NLP that focused on \nthe structure study of words in sentences and context, this \npaper goes deeper to study the information represented by \nnatural language. At the very beginning, the author tries to \nfind out how human beings relate the text “Apple” to the \nphysical “Apple”. Inspired by the elementary information \nperception, transformation, and processing mechanisms in \nNeuroscience [1], the author discovers that human beings \nperceive the color, shape, smell, taste, and other information \nof physical “Apple” through their sensory systems; in brains, \nall this attribute information related to physical “Apple”, \nsuch as color, shape, smell, taste, etc. forms an information \nset. Then, this information set can be encoded into texts, \nsuch as: “Apple”, “苹果”, “かんち”, etc., so that, the \nunderstanding of text “Apple” is to understand the \ninformation set represented by text “Apple”. Likewise, the \ninformation in the set represented by text “Apple” can also \nbe encoded into texts (e.g. Color: red; Taste: sweet; Shape: \nround, etc.). As a result, the author discovered that there are \narchitecture structures between words which represent \ninformation at different abstraction levels, and the \nunderstanding process of natural languages occurs on the \ninformation level rather than the lexical level (or morpheme), \nthe author then takes these as inspiration to carry out the \nstudies. \nThis paper also inspired by the study on the relational \nmodel of data by E.F. Codd’s [2]; and draws on a wide range \nof elementary theories, ideas, and thinking methods in the \nfollowing disciplines: discrete mathematics [3], structure of \ncomputer programs [4], computer operating systems [5], \ncomputer architecture [6], introduction to algorithms [17], \netc. \nB. Some key concepts and relations between them \nHuman beings perceive the world through information \nreceived by neural systems; this information is a tiny fraction \nof all the information in the universe. The processes of \ninformation identification, classification, memorization, \nanalysis, abstraction, association, etc. are the component \nactivities of human thinking. To make the discourse more \neasily in section II, we must first introduce some key \nconcepts: entities, attribute information, attribute space, \ninformation set, information encoding, memory-sheet, and \nexpound the relations between them. \n \nFigure 1.  Three kinds of spaces are exhibit here, they are: Attribute \nInformation Space, Real world, and Natural Language Space. The relations \nbetween them are highlighted in yellow arrows. \nEntities in the real world can abstract out lots of different \nattribute information. Some are the basic attribute \ninformation sensed by human neural s\nolfaction, gustation, audition, and soma\nothers are abstracted upon basic attribu\nclassification, movement, relations, pref\nMeanwhile, the entity also is defined an\nits attribute information.   \nThe attribute information which \nentities in the real world can be class\nproperties. Attribute spaces (AS) are use\ninformation that has similar propertie\ninformation sets, the elements (or attribu\nare strongly related to the personal \ncognitive basis. In the research of NLU\nof the data structure and the related alg\nAS are the keys of NLU, which allow p\nthe changing-rules and changing-ra\nattributes.  \nAs illustrated in Fig.1, both entities\nattribute information in attribute sp\nencoded into text information (natura\nspeech or text, in this paper, we only\nMemory-sheet (in Fig.1) is a data struc\nentity word (text encoding of entities in\nattribute words and phrases (text en\ninformation) together. All the attribute w\na Memory-sheet form a set, these a\nphrases are the elements of the set; the\nseen as the representation or the name\nMemory-sheet can display the mathemat\nthe entity and its attribute information\nwritten as: \nEntity A = { ai | ai is an attribute\nnatural number}, ai ∈A.  \nUp to now, we have seen attribute, a\nattribute words and phrases. The easy \nthem is (i) when we talk about the attrib\nspecified entity, we use “attribute”, \nemphasized that the attribute is a par\nshould not be considered as independe\nwhen we talk about attribute informat\n“attribute information” to emphasize its \nobjectively existing information; (iii) fo\nand phrases, they are the text coding\nattribute information, and won’t reflect i\nActually, people understand an entit\nits attributes. The more attribute are kn\nentity is understood. The changes of an e\nthe changes of its attributes. Thus, when\nan apple.”, in essence, it is a request to\nposition attribute of the apple. The ver\nencoding that represents change featu\nspatial-position attribute information.  \nWords in natural language are sy\ninformation. Besides the above entity wo\nor phrases, and changing feature words (\nfind the measuring words, interrog\nconjunction, punctuations, and specific s\nnatural language. Thus, understandin\nsystems (e.g. vision, \natic sensation, etc.), \nute information (e.g. \nference degree, etc.) \nnd represented by all \nis abstracted from \nsified by its natural \ned to group attribute \ns. AS are dynamic \nute information) in it \nexperience and the \nU, the understanding \ngorithms of specific \npeople to understand \nanges of entities’ \ns in real world and \npaces (AS) can be \nal language can be \ny discuss the text.) \ncture that stores the \nn real world) and its \nncoding of attribute \nwords and phrases in \nattribute words and \ne entity word can be \ne of the set. Thus, a \ntic relations between \nn, Entity A can be \ne of Entity A, i is a \nattribute information, \nway to distinguish \nbute information of a \nin this case, we \nrt of the entity and \nent information; (ii) \ntion in AS, we use \nindependence as the \nr the attribute words \ngs of attribute and \nits independence.  \nty by understanding \nnown, the better the \nentity essentially are \nn we say: “Give me \no change the spatial-\nrb “give ” is a text \nures of a sequence \nymbols that encode \nords, attribute words \n(verbs),  we also can \ngative, preposition, \nsentence structure in \nng the information \ncarried by words, punctua\nstructures; distinguishing\npunctuations, and sentenc\ntransmission is the fundamen\nC. The relation between Nat\nLet’s think again about\nlanguage and information. N\nhuman beings use to commun\nis also one of the carrier\nconstantly changes its carrie\ntransmission and processing\nexists in the forms of ele\nmolecules, and ions, the kin\nbeings perceive this inform\nbiochemical and bioelectricit\nbrains [1]; (ii) when people \nworld, information is then \nnatural language, body gestur\nCPU, information is proces\nObviously, the kinds and den\nnatural language are much\nNatural language has highly\nthe information. Therefore, \nand conceptualizes informat\nNLU.  \nII. \nNEW CLASSIFICATI\nINFORMATION ORGAN\nAccording to the gram\nstandard, and the meanin\nvocabulary has been divided\nadjectives, adverbs, prepositi\netc. [8]. On this basis, the aut\ninformation carried by each\nreclassifying them into the\npointer chunk, and task chu\nand the roles they played in\nFig.2, the task chunk is com\nchunks, and pointer chunks.\nFigure 2.  New classi\nations, and specific sentence \nthe function of the words, \nce structures in information \nntal work in NLU.  \ntural Language and Information \nt the relation between natural \nNatural language is a tool that \nnicate with the outside world; it \nrs of information. Information \ners (or forms) in the process of \n, (i) in real world, information \nectromagnetic wave, chemical \nnetic energy of air, etc., human \nmation and transform them into \nty signals then process them in \ncommunicate with the outside \ntransformed into the form of \nre, body movements, etc.; (iii) in \nssed in form of binary code. \nnsity of information contained in \nh higher than in other forms. \ny abstracted and conceptualized \nhow natural language abstracts \nion is the essential problem of \nION OF LEXICAL CHUNKS AND \nNIZATION ARCHITECTURE  \nmmar function, morphological \nng standard, modern Chinese \nd into nouns, pronouns, verbs, \nions, quantifiers, onomatopoeia, \nthor dismantles and analyzes the \nh type of lexical chunks, then \n data chunk, structure chunk, \nunk according to their functions \nn information transmission. See \nmposed of data chunks, structure \n \nification of lexical chunks \nDue to the constraint of capability and time, the author \ncannot list all categories, items, and their usage scenarios, \nbut only present the approach for reference and discussion. \nThe items marked with an asterisk in Fig.2 are expounded in \nmore detail below. \nA. Data Chunk \nAs the name implies, data chunk plays the role of data in \nnatural language, thus, the data structures of data chunks are \nthe focus of NLU study. The classification of data chunks in \nFig.2 is structurally based, for the sake of understanding, we \nfirst understand each sub-classification of data chunks \nthrough examples, and then elaborate them on the \nperspective of structure. There are hierarchical structures \nbetween data chunks; the concept of elements and sets in \ndiscrete mathematics can perfectly express this kind of \nhierarchical relationship, which also will be discussed.  \n1) \nAttribute Chunk: Attribute chunk can be words or \nphrases, they represent the attribute information which \nabstracted from entities. According to the different \nabstraction levels and methods, they can be further divided \ninto the following four types.  \na) Descriptive Attribute Chunk and Positional Attribute \nChunk: Descriptive attribute chunks are the text encodings \nof the most basal kinds of attribute information perceived by \nhuman neural systems. The author prefers to classify the \ndescriptive attribute chunks based on the study of the basic \ninformation perception system in the neurobiology of the \nbrain, and they are the vision system (color, shape), the \nchemical senses system (taste, or gustation, and smell, or \nolfaction), the auditory and vestibular system, and the \nsomatic sensory system [1]. It is easy to classify the related \nattribute words to the above sensory systems, see examples \nin braces below: \n \nColor {red, blue, green, orange}, \n \nTaste {sweet, sour, acerb, bitter}, \n \nShape {square, round, cubic}, \n \nSmell {smelly, balmy, pungent, apple-flavored}, \n \nSomatic sensation {smooth, soft, furry}, etc. \nThe positional attribute chunks encode the spatial and \ntemporal attributes of entities and usually appear in sentences \nin the form of phrases. (e.g. on this afternoon, in 1949, in the \nfridge, at school, etc.) The reason why this paper classifies \npositional attribute information as basic attribute information \nis that positional attribute chunks together with descriptive \nattribute chunks mapping the basic information dimension \nthat humans needed to understand the world (time, space, \nand matter.) \nb) Verb: Verbs are text encodings of change features \nwhich abstract from sequences of attribute information \nchange records. Finally, the abstracted change features are \nrecorded and encode as verbs in the memory, but the \ncorresponding sequences of change records won’t be \nrecorded. Thus, this type of set is called change  features set. \n \nFall – represent the change features which abstract \nfrom \nsequences \nof \nspatial-position \nattribute \ninformation in spatial AS. \n \nSweeten – represent the change features that abstract \nfrom sequences of taste attribute information in taste \nAS.  \n \nRun – represent the change features that abstract from \nsequences of spatial-position information, distance \nattribute information, body posture information, etc. in \ncorresponding  AS. \nc) Measuring Chunk: Measuring chunk is the text \nencoding of the reclassification result of the selected \nattribute information clusters. There are various methods \nand standards to implement the reclassify action: according \nto the different frames of reference, it can be divided into \nthe subjective measuring and the objective measuring, \naccording to the number of the measuring dimensions (or \nAS), it can be divided into the single-dimension measuring \nand the multi-dimension measuring, for example, speed is a \nmulti-dimension measuring which needs to measure both \nthe distance and the duration of time of a movement at the \nsame time, and both the measuring of a distance and \nduration of time are the single-dimension measuring. And if \nthe measuring result only has two values, such as “like” and \n“dislike”, “agree” and “not agree”, “yes” and “no”, they can \nbe called the binary measuring; if the measuring result has \nmultiple values, such as “good”, “better”, “best” and “fast”, \n“faster”, “fastest”, they can be called the distribution \nmeasuring. Some examples are given below for better \nunderstanding.  \n \nDistribution measuring: Describe the distribution area \nof target objects after performing the statistical \nanalysis on the attribute information in the selected \nmeasuring area. A distribution model is given in Table \nI to describe the data distribution, and each distribution \narea has the corresponding measuring words to \ndescribe it. Some examples of distribution measuring \nwords are given in Table I. \n \nSubjective measuring: This type of measuring is \nadopted unified measuring standard to minimize the \nrecognition tolerance of the same thing between \nindividuals. E.g., Area (km2, m2), Speed (m/s, km/h), \nTemperature (℃, ℉), Weight (g, kg, ton), Pressure \n(Pa), etc. \n \nQuantity/Order/Ranking measuring: We assume that \nthe quantity measuring is based on the shape and \nspatial attribute information; the order measuring is \nbased on the quantity, time, spatial-position, and other \nattribute information; and the ranking measuring is \nbased on the order and other attribute information. It \ncan be seen that the measuring dimensions of these \nthree measurings are gradually superimposed and \nincreased. \nd) Extended Attribute Chunk: The extended attributes of \nan object are not defined by their content, but depend on \ntheir structural relations with the object. Extended attribute \nchunks are the text encodings of information chunks that \nhave a specific structural association with the target object \nand will be elaborate in the information organization \narchitecture section.  \n \n \nTA\nNormal Distribution  \nModel \nn\nb\nn\nb\nF\n2) \nAttribute Space Chunks (ASC):\nencoding of the AS, the underlined wor\nthe examples in the attribute chunk \nModeling the AS is crucial to NLU\nunderstanding of attribute informatio\nidentifying which AS they belong to, w\nhow this AS organizes and operates its a\nthe variation-rules of these attribute in\npredictions of the variation-boundaries\ninformation in the AS, etc. Each AS\nsubsystem of the whole thinking. In se\nmodels a spatial-positioning AS to h\nconcept of the AS more intuitively.   \n3) \nClassification \nEncoding \nSyst\nInformation: Looking back at all the abo\nattribute information, we can discove\ninformation is not independent and disor\nABLE I.  \nEXAMPLES OF THE DISTRIBUTION MEASURING CH\nMeasuring Chunks \nAttribute Spaces (AS) being \nVolume \nSpeed\n(distance + time)\nnever, \nbeyond\nnever seen,\nbeyond the limit\nextremely, very, -est \nbiggest \nfastest \nNormal \na little bit, -er \nbigger \nfaster \naverage, \nproper \nAverage size \nProper speed \na little bit, -er \nsmaller \nslower \nextremely, very, -est \nsmallest \nslowest \nnever, \nbeyond \nnever heard,\nbeyond the cognitive \nigure 3.  Structures of different attribute information sets \n ASC are the text \nrds outside braces of \nsection are ASC. \nU, because, for the \non, in addition to \nwe also need to know \nattribute information, \nnformation, and the \ns of these attribute \nS is a independent \nection III, the author \nhelp understand the \ntem \nof \nAttribute \nove encoding laws of \ner that the attribute \nrderly, but organized \ntogether in some forms, the\nsystems of the attribute in\nauthor would like to use fig\nconstitutive modes of differe\nthe first figure, the descrip\ndirectly encoded as the basic\nin the corresponding AS. In t\nof the selected sample attribu\ninto three child clusterings a\nmeasuring chunks encode th\nsmall, middle, and large, wh\nthe selected sample attribute\nthird figure, the sequences o\nAS are been selected, and\nencoded as sweeten (a ve\nencoding of the selected \nsequence. If we observe the\nHUNKS \nMeasured \nTemperature \nvery hot \nhot \nwarm \ncool \ncold \nextremely cold \n \n \nere is a classification encoding \nnformation in our brains. The \ngures (in Fig.3) to illustrate the \nent attribute information sets. In \nptive attribute of samples are \nc attribute chunks and recorded \nthe second figure, the clustering \nute information is been divided \naccording to their size, and the \nhese three child clusterings as \nhich are the indirect encoding of \ne information clustering. In the \nof attribute information in taste \nd its change feature are been \nerb), this also is an indirect \nsample attribute information \nusage of natural language, it is \nnot hard to discover that the selected sample attribute \ninformation clusterings or sequences can be the basic \nattribute information, the extended attribute information, the \nadvanced attribute information, or a mix of them. In the \nstudy of advanced attribute chunks, the coverage scope of \nthe attribute information abstracted by the specific advanced \nattribute chunk is the key content of the study.  \nTABLE II.  THE ENCODING COMPARATION OF COLOR ATTRIBUTE \nINFORMATION BETWEEN ENGLISH AND MODERN CHINESE \nOriginal Classification \nNew Classification \nNoun \nAdjective \nDescriptive \nAttribute Chunk\nASC\nStructure \nChunk \nRed \nRed \nRed\n \ncolor\n \n红色 \n红色的 \n红 \n色 \n的a \nGreen  \nGreen \nGreen\n \ncolor\n \n绿色 \n绿色的 \n绿 \n色 \n的 \nOrange \nOrange \nOrange\n \ncolor\n \n桔色 \n桔色的 \n桔 \n色 \n的 \na. Structrue chunks are highlighted in blue. \nIf compare several natural languages, we can discover \nthat the encoding granularity and encoding overlap degree \nof the attribute information vary greatly in different natural \nlanguages. See examples given in Table II, for the color AS, \nthe English text encoding is “color”, and the Modern \nChinese text encoding is “色”, the English text encodings of \nthe attribute information in color AS are “red”, “green” and \n“orange”, but these three words connotate the text encoding \nof color AS (color), which we think there is overlapping \nphenomenon. In Modern Chinese, text encoding of the \nattribute information in color AS are “红”, “绿”, “桔”, \nduring use, we can simply read the text encodings of the \nattribute information and the text encoding of the AS which \nthey belong to, and put them together to describe the \nattribute information in the corresponding AS. When \ndescribing the inclusion relation between an entity and its \nattributes, simply add a structure word “的” between them, \nwhich will be elaborated in the structure chunk section. \nWe also can find out that, despite the above differences \nin encoding granularity and degree of overlap, but the overall \nencoding architectures are roughly the same. Perhaps we can \nascribe this phenomenon to the common brain physiological \nstructure human beings shared. And the application of this \nclassification encoding systems in natural language greatly \nreduces the number of words and improves the expression \nefficiency.  \n4) \nEntity Chunks (EC): EC are the text encoding of the \nconcept of entities in the real world. As we can see in Fig. 3, \nthe EC “Apple” is the text encoding of the information cube. \nThe operation of categorizing the attribute of the samples of \nphysical apples and storing them in corresponding ASs in \ncertain ways which form an information cube is the \nconceptualization \nof \nthe \nphysical \napples. \nThese \nconceptualization (or informatization) processes also are the \nprocesses of perceiving, classifying, and storing information \nfrom the environment by the nervous system. In natural \nlanguage, there are three main methods to instantiate \nconcepts, which will be elaborate in the pointer chunk \nsection.  \nEC can be divided into the explicit EC and the implicit \nEC, according to whether they can be perceived by the \nvisual system, some examples are given in Table III to help \nthe understanding. The Explicit EC can be further divided \ninto the dynamic EC and the static EC, according to the \nobvious difference in their spatial-position attribute. In \nspatial-position AS, static ECs are used as anchors to build \nthe relative reference coordinate system, which will be \nelaborated in section III.  \nTABLE III.  \nCLASSIFICATION OF EC \nEntity Chunk  \nClassification \nExamples \nExplicit\nEC \nDynamic EC\n Human being, cat, car, cloud \n Apple, bag, laptop, cup \nStatic EC \n Sofa, house,  school,  shopping mall\nImplicit EC \n Protein, carbohydrate, oxygen \nIdentifying the EC in a sentence is crucial to \nunderstanding the sentence. Because, all information \nconveyed in a sentence revolves around the EC, no matter \nthe target of the sentence is to convey the information or to \nrequest an action. But the use of polysemous words in \nnatural languages makes it difficult to accurately identify \nthe information set encoded by the EC, for example, “Apple” \nis a kind of fruit, it also represents a smartphone. We can \nobserve a large number of lieu representations used in \nnatural languages, which use the partial attributes of the \nwhole to represent the whole.  for instance, a little child \nimitates the barking of dogs (which is one attribute of \nphysical dogs) to represent the physical dog instead of the \nspeech of physical dog, or to use the brand of a smartphone \n(the brand is the extended attribute of a smartphone) to \nrepresent the smartphone, such as, my Blackberry, my \nNokia, etc. The lieu representation usage is particularly \nprominent in English, that a large number of English words \nare both nouns and adjectives. Therefore, when we using the \npreset classification method to classify words, often leading \nto ambiguity in understanding, because the text encodings \nand the information they represent are not one-to-one \nmappings. But, this problem can be solved if we know the \ninformation organization architecture and can use the other \ndata chunks in a sentence to accurately classify and locate \nthe target data chunk. \n5) \nInformation Organization Architecture: Sets are the \nelementary organization forms of information. The author \nwill discuss the information organization architecture in \nterms of the constitutional structures of information sets and \nthe hierarchy between the information sets. \na) Structural \nclassification \nof \ninformation \nsets \nrepresented by different data chunks: Let’s understand the \nconstitutional structures of information sets in terms of the \nconceptualizing process of physical objects. The nervous \nsystem perceives basic attribute information from different \nphysical samples, classifying and storing them into \ncorresponding AS. In the information cube in Fig.3, the \ncolumn along the sample axis is the inter\nof informationalized samples, the row a\nthe union set of different informationa\nsample. Then add the third axis: \ninformation cube which categorizes\ninformationalized attributes of the sa\nsample, AS, and timeline dimensions fo\nthat sample clustering. Thus, the \nrepresented by EC are the union sets, \nalso represents a union set. As we can se\nto understand that the information set r\nverbs, and measuring chunk are the in\nfeature enables us to make  a prelimina\ninformation set when it is been read. In\nbe we can identify the type of the stimul\nwhether the activated neurons are in the\nin different zones.  \nFigure 4.  Structural classification of informati\ndifferent data chunks\nLooking at the encoding process o\nASC and EC are the direct encoding of \nstored in memory database, but verbs an\nare the indirect encoding of the informat\nto implement extra algorithms on the s\nsets, then to encoding the results. Th\nclassify the information sets that repres\nas the hard set, and the information \nverbs and measuring chunk are the soft s\nFigure 5.  Memory-tree\nb) Hierarchical relation between data\nbelieves there is no isolated informat\ninformation must be connected to other \nhierarchy is the endogenous structu\nlanguage. The tree data structure can\nhierarchical relation between data ch\nnatural language, we can easily draw the\nrsection of attributes \nalong the AS axis is \nalized attributes of a \nthe timeline. The \n and records the \nample clustering in \nforms the concept of \ninformation sets \nthe instantialted EC \nee in Fig.3, it is easy \nrepresented by ASC, \nntersection set. This \nary judgment of the \nn neurobiology, may \nlus signal by observe \ne same brain zone or \n \nion sets represented by \nof information sets, \nthe information sets \nnd measuring chunks \ntion sets, which need \nselected information \nherefore, the author \nent by ASC and EC \nsets represented by \nset.  \n \ne \na chunks: The author \ntion in brains, any \ninformation, and the \nure of the natural \nn well express this \nhunks. Observe the \ne memory-tree in Fig. \n5. There are three kinds of v\nelemental information whic\nchunks, the intersection sets \nthe union sets which are rep\nthe memory-tree are represe\nwill be expounded in the stru\nIn the previous paragraph\nEC “Apple” represents the un\nas red, sweet, sour, apple-sha\nrelationship between the EC\ndisplayed as the vertex and \nthe vertices “Apple”, “Ban\nchildren vertices of the verte\nrepresents the union of the \n“Banana” information cube, \ncube. By that analogy, we ca\n(the union operation) again a\nmemory-tree. And in a mem\nthe bigger the information cu\ndensity of information it cont\nof a higher abstract EC enab\nlarger scale and requires \ncapability. \nFor instance: \nrepresentation of the set {cl\nstudents, teachers, blackboard\nand the interactive activities\nabove set. When we use the\nand attribute information are \nThe words “Apple”, “Fru\ntree are all EC, but there is o\nbetween them, which can be\nFood.) \nTABLE IV. \nSN\nTarget \nObject\n \nBasic\n \n \n \nDescriptive \nPositional \n1\nApple\nis a\nred\n2\nApple\nis\nin frid\n3\nApple\nis \n4\nApple\nis \n5\nApple\nis\n6\nApple\nis\n7\nFruit\nis\nIn natural language, we \nthe data chunks which are a\nin Table IV, and ignore the m\nin these sentences, we take \nobject, and use the basic attr\nwith and lower than “Apple\nthen we get sentences 1 and\nmeasuring chunk “bad” and \nvertices in the memory-tree, the \nch is represented by attribute \nwhich are represented by  ASC, \npresented by EC. The arrows in \nented by structure chunks and \nucture chunk section. \nh, we already discussed that the \nnion set of all its attributes, such \naped, etc. In Fig.5, the inclusion \nC “Apple” and its attributes is \nits children vertices. Likewise, \nnana”, and “Orange” are the \nex “Fruit”, thus, the EC “Fruit” \n“Apple” information cube, the \nand the “Orange” information \nn continually do this abstraction \nand again in higher layers of the \nmory-tree, the higher the vertex, \nube it represents and the larger \ntains. Or, we can say that the use \nbles us to call information on a \nbetter information processing \nthe EC “School” is the \nassrooms, playground, canteen, \nds, chalks, books, courses, etc.} \ns between the elements in the \ne word “School,” all its subsets \nbeen incorporated in. \nuit” and “Food” in the memory-\nobvious hierarchical relationship \ne written as: (Apple ⸦ Fruit ⸦ \nEXAMPLES \nAttribute\n \nAdvanced\nExtended\nMeasuring \nChunk \nVerb \nHigher  \nLayer EC \n \n.\ndge\n.\nbad\n.\nroll\n.\nfruit\n.\nfood\n.\nfood\n.\na. Structure chunks are highlighted in blue.  \noften describe an EC by using \nssociated with it. See sentences \nmissing articles and plural forms \nthe EC “Apple” as the target \nribute chunks which is associate \ne” to describe the EC “Apple”, \nd 2. In sentences 3 and 4, the \nthe verb “roll” are not showing \nin the memory-tree, but they are associate with the EC \n“Apple”, so we can use them to describe the EC “Apple”. \nFor the EC “Fruit” and “Food”, in the memory-tree, they are \nassociate with and higher than the EC “Apple”, we also can \nuse them to describe the EC “Apple”, and get sentences 5 \nand 6. In a sentence, when the higher-positioned EC is used \nto describe the lower-positioned target EC, the higher-\npositioned EC is considered as an extended attribute of the \nlower-positioned EC. This explains why extended attributes \nare not defined by the content in particular, but by the \nstructural relationship between them and the target \ndescription object.  \nThe preset classification of data chunks allows for \npreliminary classification of the target data chunks, while \nfurther classification requires an auxiliary judgment from the \ninformation of the structural position of the target data chunk \nin the sentences. And the preset classifications of data \nchunks in this paper are also based on the compositional \nstructure of the information sets they represent. Thus, the \nclassification \nof \ndata \nchunks \nis \nstructural \nbased \ndiscrimination.  \nThe extended attributes of an EC increase as the \nconnections between this EC and other higher-positioned EC \nincrease. And the basic attributes of an EC are also expanded \nbecause of technological advances (e.g. the electron \nmicroscopy, the radio telescope, endoscopy, and MRI, etc.) \nIn fact, the magnificent edifices of human thinking are built \nwith these basic attributes information as bricks.  \nc) Memory-graph: Memory-graph is a cluster of \nmemory-trees, and these memory-trees are connected in \nmany ways. See the example of the memory-graph in Fig.6. \nWhen people build up their memory-graph, they take \nthemselves as the center of this graph. In the process of \ngrowing and learning, people continually knit new \ninformation into their memory-graph by connecting the new \ninformation with the existing. Other important or close \nhuman beings can be set as the vice centers. These kinds of \ndata structures which have one center and several vice \ncenters are beneficial to improve the efficiency of search \noperations.  \nConnections: In a memory-graph, connections between \ndata chunks are represented as the directed edges (arrows). \nThere are two types of connections in Fig.6:  \n \nSolid arrow: represent the real connection which \nimplicates structural hierarchy, and always been used \nin memory-tree structures.  \n \nDashed arrow: represent the virtual connection that \ndoes not implicate structural hierarchy. The usage of a \ndashed arrow assumes that there is no hierarchical \nrelation between different tree structures, so the dashed \narrows are always been used to connect trees in a \nmemory-graph (see subtree of “Cat” and subtree of \n“Dog” in Fig.6).  \nThe above memory-tree and memory-graph are both data \nstructures that can be used in a memory-database. Based on \nthe above structures, the identification, classification, \nmemorization, \nand \neven \nassociation \noperations \nof \ninformation are possible. However, all these operations are \nonly related to the construction and the maintenance of the \nmemory-database, which is also just a repository of \ninformation. And to understand natural language, we still \nneed to build processing systems for information. Beside the \nAS which are used to process all kinds of attribute \ninformation, there still lots of high-leveled information \nprocessing systems, such as: decision system, motion control \nsystem, life support system, etc. The memory-database and \ninformation processing system can be considered as mutually \nindependent systems. In section III, the author will introduce \na spatial-position AS model and analyze its spatial \ninformation processing mechanism, to provide a reference \nfor future study on information processing systems. \nB. Structure Chunk \nConnections between information sets can be interpreted \nas various kinds of relations, for example, the representation \nrelation (defining relation), the inclusion relation, the causal \nrelation, and so on. In this section, we will discuss the \ndefining relation and inclusion relation represent by structure \nchunks: “Be,” “Of,” “’s” and “Have,” and elaborate on two \ncorresponding data reading modes: the defining reading \nmode and the set reading mode.  \n1)  “Be”: In dictionaries, “Be” and “Have” are \nclassified as verbs, which is against the verb classification \nrule introduced in the data chunk section. In natural \nlanguage usage habits, we can observe that the data chunks \nafter “Be” always are used to explain or define the data \nchunk before “Be”. Although the author say that an entity is \ndefined and represented by all its attribute information, but \npeople do not need and impossible to completely describe \nan entity in course of natural language usage. Usually, \npeople just partially describe an EC by giving one or several \nof its attributes. The given attributes are also used to help \nlocate the EC. In natural language, the data chunks after “Be” \nare used to describe and define the data chunks before “Be”.  \n2) “Of”, “’s” and “Have”:  These words interpret the \nconnections as inclusion relations between data chunks. We \nassume that there are no equal sets in a memory-graph. So, \nthe inclusion relation of sets can be written as: \n \nA ⸧ B    A has B. or A’s B. \n \nB ⸦ A    B of A.  \nNow, we can simulate the process of how brains read the \ndata in memory-graph (in Fig.6) in below two modes, the \nread-out sentences are list in Table V. \n \nFigure 6.  Queen’s memory-graph \na) \nDefining Reading Mode (DRM):\nfull reading mode, which read whole \nselected reading chunk. As we can see \nthe article and punctuation are missing\nwe still can roughly get the information\ndefining reading mode, if the followin\nTABLE V. \nData Reading Mode   \nStructural CLS  c\nTarget Object C\nLexical CLS   \nEntities\n \nItems \nReading \nChunk\n \n \n1\nH  ASC   ml \nApple\n’s\n2\nH  ASC   nl  \nApple\n’s\n3\nD  ASC   jl    \nDog\n’s\n4\n[ F , G ] ASC  kl \nTail and paw\n’s\n5 \n \nA ASC  E \nQueen Elizabeth\n’s\n6 \n \nA  ASC [ C, D ]\nQueen Elizabeth\n’s\n7a\nA  ASC1 B \nQueen Elizabeth\n’s\n7b\nA  ASC2 B \nQueen Elizabeth\n8a\nB  ASC1 A \nCharles \n’s\n8b\nB  ASC2 A \nCharles \n \n9a\nD  ASC C \nDog\n’s\n9b\nD  ASC C  \nDog\n9c\nC  ASC D \nCat\n’s\nb) \nSet Reading Mode (SRM): th\nrelation reading mode, which only rea\n(EC and ASC) and the inclusion relation\na selected reading chunk. We can find \nin natural language: \n \n A   ASC   bl: A ⸧ ASC, b∈ASC.  \n“b” is the elements of “ASC”, and\nonly read the “A ⸧ ASC” part from\nomit the “b∈ASC” part. E.g., item\nTable V. \n \n A   ASC   Bl: A ⸧ ASC ⸧ B. \nIn this case, we select A as the targ\nthen, we can choose the “A ⸧ ASC\nB” part to read-out which is \nrequirement. E.g., items 5 and 6 in \n \n A   ASC   Bl: ASC ⸦ A, B  A. \nDue to there is a virtual connectio\nstructurally, there is no inclusion \nand B, just the virtual abstract re\nOr we can call it the \ndata chunks from a \nin Table V, though \ng in those sentences, \nn they conveyed. In \nng attribute chunk’s \nconnotation (the data chunk a\nASC’s, the ASC can be omi\nsee this phenomenon in M\nclassification coding system \ncoding granularity and low\nEnglish.  \nEXAMPLES OF THE DEFINING READING MODE AND THE SET READ\nDefining Reading Mode \n(Full Reading Mode) \n(In\nChunk \n Attribute Chunk \nSe\nASC \nAttribute \nInformation \nEntities\nEnti\n \n \n \n \ncolor a \nis b    red\nspatial-position\nis \nin fridge\nname\nis\nWirote\ncolor\nare\nblack\nTail \njewelry\nis \ncrown\nQueen E\nQueen E\npet\nare \ncat and dog\nQueen E\nQueen E\nson\nis \nCharles\nQueen E\nis\nmother \nof\nCharles\nmother\nis\nQueen Elizabeth\nis \nson\nof\nQueen\nfriend\nis\ncat\nis\nfriend\nof\ncat\nfriend\nis \n \ndog\n   a. If the following attribute chunk’s connotation can cover the curren\nhis is an inclusion \nads information sets \nn between them from \nthree typical usages \nd b is not a set, thus, \nm “A ASC  bl”,  and \nms 1, 2, 3, and 4 in \nget description object, \nC” part, or the “A ⸧ \naccording to the \nTable V.   \nn between A and B, \nrelation between A \nelation. This virtual \nabstract relation can be\nIn this case, we can c\nread-out. E.g., items 7a,\n3) Punctuations and c\nconjunctions segment the inf\nthe following purposes:  \n \nDistinguish the task\nchunks (e.g., period\nmarks.) \n \nDistinguish the pr\ninformation chunks\nsemicolons, parenth\n \nIndicate \nstructura\ninformation chunks\ntherefore, so.) \nExcept for structure chu\nstructures and paragraph s\nchunks on an even larger \nrepresented by those stru\nafter “be”) can cover the current \nitted in expression. It is rare to \nModern Chinese because the \nof Modern Chinese has smaller \nwer coding overlap compare to \nDING MODE \nSet Reading Mode\nnclusion relation Reading Mode) \net \n \nSubset \nities \n \nASC \nEntities\n \n \n \nApple\nhas\ncolor\nApple\nhas \nspatial-position\nDog\nhas\nname\nand paw\nhave\ncolor\nElizabeth\nhas \njewelry\nElizabeth\nhas\ncrown\nElizabeth\nhas \npet\nElizabeth\nhas\ncat and dog\nElizabeth\nhas \nson\nCharles\nhas\nmother\nDog\nhas\nfriend\nCat\nhas \nfriend\nnt ASC’s, the ASC can be omitted in expression. \nb. Structure chunks are highlighted in blue. \nc. CLS is the abbreviation of classification.\ne unidirectional or bidirectional. \nhoose the “ASC ⸦ A” part to \n, 8a, 9a, and 9c in Table V.  \nconjunctions: Punctuation and \nformation on a larger scale, for \nk type of segmented information \nd, question marks, exclamation \nrocessing order of segmented \ns (e.g., punctuations: commas, \neses.) \nal \nrelations \nof \nsegmented \ns (e.g., conjunctions: and, or, \nunks listed in Fig.2, sentence \nstructures segment information \nscale. The structural relations \nuctural forms are also more \ndiversified. Sentence structures will elaborate in the task \nchunk section.  \nC. Pointer Chunk:  \n1) \n Pointer \nof \ninstantiation: \nDuring \nuse, \nthe \ninstantiation of a concept can be achieved by adding a \npointer chunk “the” in front of the EC and giving \ncharacteristics that distinguish the entity from other entities \nof the same class. The instantiated EC are underlined and \npointer chunks are hightlight in orange in below examples:    \n The apple in the fridge.  \n The apple with a scar. \nOr directly point out the target object from existing \nentities by using pointer chunks (e.g. this, that, these, those, \netc.)  \n This cat.  \n That house. \n Those people. \nOr to provide the other EC which are already known and \nadjacent to it to help to locate it accurately in a memory-\ndatabase, thereby instantiating the target EC.  \n Her (she’s) dog. \n Queen Elizabeth’s dog.  \n2) \nPointer of search-scope: The interrogatives together \nwith the AS in sentences are used to limit the area or scopes \nthat should carry out the information search operation, \ntherefore, they been called the pointer of search-scope in \nthis paper.   \n \nSearch scope \nWhat color is the apple? \nAS of color \nWhere are we going? \nAS of spatial-position \nWhen shall we leave?\nAS of timeline\nHow old are you? \nAS of age \nHow fast the car is? \nAS of speed measuring \n \n3) \nPositioning pointer: Positioning pointers used upon \ndifferent AS represent different kinds of positional \ninformation. In spatial-position AS, pointer chunks are used \nfor positioning the target object in space. On the timeline, \npositioning pointers are used to positioning target objects on \nthe timeline. Thus, the understanding of the positioning \npointers \nis \nrelying \non \nthe \nunderstanding \nof \nthe \ncharacteristics of the corresponding AS, some examples are \ngiven below. And no matter the positioning of the target \nobjects are in space or the temporal systems, the accuracy of \npositioning, to choose the relative-position positioning or \nthe absolute-position positioning should be extraordinarily \nconsidered, which will not be expanded here. \n in the fridge / on the table / at home / behind the door \n in this morning / on Monday / at 6 clock / after that day \nD. Task Chunk: \nTask chunks usually are sentences which composed of \ndata chunks, structure chunks, pointer chunks, and other \nlexical chunks to express specific task requirements \n(operations on data chunks). According to different sentence \nstructures that express the specific type of task, sentences can \nbe divided into the data description task, the data verification \ntask, and the data search task.  See Table VI, the above task \ntypes can be subdivided again according to different data \nreading modes.  \nTABLE VI.  \nNEW CLASSIFICATIONS OF SENTENCES \nData Reading Mode\nDRM\nSRM \nPRM\nTask Type  \n \nA. Data description task\nA-DRM\nA-SRM\nA-PRM\nB. Data verification task\nB-DRM\nB-SRM\nB-PRM\nC. Data search task\nC-DRM\nC-SRM\nC-PRM\nBefore introducing the classification of task chunks, the \nauthor would like to introduce another type of data reading \nmode: the attribute-changing process reading mode (PRM). \nAs the name implies, PRM reads EC and their attribute-\nchanging processes. Items 6, 7, and 8 in Table VII are the \nsentences read out in PRM, attribute-changing processes of \nentities in sentences are represented by verbs which are the \nchange features abstract out from sequences of attribute \ninformation changing records. Attribute-changing processes \ncan be interactive or noninteractive. Item 6 in Table VII is \nthe description of the noninteractive attribute-changing \nprocess of the entity. Items 7 and 8 are the description of \ninteractive attribute-changing processes between entities. In \nthe interactive attribute-changing processes, it is important to \ndistinguish the active role and the passive role. Usually, \nentities before verbs are active roles; entities after verbs are \npassive roles (the passive tense is not discussed in this paper).  \nNow, the author will put the information processing \nentity (IPE) in the natural language receiver’s shoes, and \nthen elaborate on the understanding process of each type of \ntask chunks in Table VI. The scenarios that put IPE in the \nnatural language sender’s shoes won’t be discussed in this \npaper.  \n1) \nType A tasks: When IPE gets information input of \ndata description type tasks, they understand the input \ninformation by activating or mobilizing the data chunks \ndescribed by input information in memory-database. The \nactivating or mobilizing processes of the data chunks \ndescribed by input information are also been considered as  \ndata reading operations in IPE’s memory-database (the data \nreading operation is highlighted in bold in Fig.9.) The \nunderstanding of A-DRM and A-SRM types of sentences \nfocus on the understanding of structural relations between \ndata chunks in sentences. The understanding of the A-PRM \ntype of sentences focuses on the understanding of attribute-\nchanging progress of EC in sentences. \nIt is easy to find out that Imperative Sentences are the \nabridged expression of A-PRM type of sentences that request \nthe IPE to take immediate action. Exclamatory Sentences are \nthe abridged expression of data description type of sentences \nthat emphasize the extraordinary attributes of target objects, \nthe target objects are assumed to be known information by \ndefault and are omitted in the expression.  \nTABLE VII.  \nEXAMPLES OF NEW SENTENCE CLASSIFICATIONS \nColumn A\nColumn B\nColumn C\nOriginal CLS\nDeclarative & Exclamatory \nSentences \n  Interrogative Sentences \n (Yes/No & choice question) \n  Interrogative Sentences \n(WH-word question) \nTask Type\nA: Data Description Task\nB: Data Verification Task\nC: Data Search Task\nData Reading \nMode \nText- pointer words; Text- structure words;\nText- measurment words; Text- verbs; \n“Be”, “Have” and verbs that need to be \nvarified are marked in V.      \n“      ” Red triangles in column A mark out the missing data chunks \nand the reading direction.  \nText- Pointer chunk of search-scope \n1\nDRM\nThis apple is red.\nIs this apple is red ?\nWhat color red is this apple ? \n2a\nDRM\nThe dog’s name is Wirote.\nIs the dog’s name is Wirote ? \nWhat Wirote is the name of the dog ? \n2b\nThe dog’s name is Wirote.\nWhose the dog’s name is Wirote ? \n3a\nDRM\nThey are Queen’s crowns.\nAre they are Queen’s crowns ? \nWhose  Queen’s crowns are they ? \n3b\nThey are Queen’s crowns.\nWhich they are Queen’s crowns ? \n4\nSRM\nThe cat has a black tail.\nDoes the cat hasve a black tail ? \nWho the cat has a black tail ? \n5a\nSRM\nQueen has twelve crowns.\nDoes Queen hasve twelve crowns ?\nWho Queen has twelve crowns ? \n5b\nSRM\nQueen has twelve crowns.\nHow many twelve crowns does Queen hasve ? \n6\nPRM\nWirote run away.\nDoes Wirote run away ? \nWho Wriote run away ? \n7\nPRM\nQueen read the book.\nDid Queen read the book ? \nWhich the book did Queen read ? \n8\nPRM\nQueen likes coffee and tea.\nDoes Queen likes coffee and tea ? \nWhat kind of drink coffee and tea does Queen likes ? \nNotes: Auxiliary words which highlight in yellow are still under study.\n2) \nType B tasks: Human beings do information \nrecognition all the time while they are awake. Information \nrecognition is the operation to compare input information \nwith the existing information in their memory-database. \nWhen the input information is new and does not have a \nrelated record in IPE’s memory-database, the IPE can \ncomplete this round of information recognition by calling \nsomeone else’s memory-database.  \nIn natural language, the information senders use the data \nverification task type of sentences to express data \nverification requests to the information receivers that request \nto invoke corresponding information in information receivers’ \nmemory-database to help to complete the data verification \ntask. See Table VII, the sentences in column B express the \nverification request for information described in column A. \nSpecific implementation methods are as follows:  \na) \nFor the B-DRM type of sentences: This type of \nsentences request to verify the “Be” structural relation \nbetween the input data chunks. The request is achieved by \npop out the “Be” in verification data chunks and relocate it \nto the beginning of the data verification task chunks \n(sentences), and add question marks to end these data \nverification task chunks. \nb) \nFor the B-SRM type of sentences: This type of \nsentences request to verify the inclusion relation between \nthe input data chunks. The request is achieved by add “Do” \nat the beginning of the data verification task chunks, and \nalso add question marks to end these data verification task \nchunks.  \nWe can find some clues in Fig.6 for why the extra \nauxiliary words “do” are needed in B-SRM type of task \nchunks. It is obvious that “Be” is a bidirectional structure \nrelation, and “Have” is a unidirectional structure relation. \nThus, “Have” needs to be kept between the set chunk and its \nsubset chunk to indicate this unidirectional structure relation. \nTherefore, extra auxiliary words “Do” are added at the \nbeginning of the data verification task chunks to express the \nverification requests of the inclusion relation.  \nc) \nFor the B-PRM type of sentences: This type of \nsentences requests to verify the attribute-changing processes \nrepresented by verbs. Same as B-SRM type of sentences, the \nB-PRM type of task chunks express the attribute-changing \nprocess verification request by add “Do” at the beginning of \ndata verification task chunks, and add question marks to end \nthese data verification task chunks.     \nAll attribute-changing processes are directional, so verbs \nalso need to be kept in sentences and extra auxiliary words \n“Do” need to be added at the beginning of the data \nverification task chunks to express the verification requests \nof the attribute-changing process.  \nThis paper won't discuss the usage of variants of the \nwords “Be”, “Have”, “Do” and verbs. The classification of \nthe auxiliary word “Do” is still under study. \n3) Type C tasks: In the process of information \nprocessing, if some information is found missing, IPE can \nsend requests to other IPE to assist in searching the missing \ninformation through the data search task chunks. In the data \nsearch task chunks, the missing information is substitute by \nthe pointer chunks of search-scope (the red text in Table VII) \nwhich indicates the area or scopes where shall carry out the \nsearch operation. \n In the data search task chunks, the\nsearch-scope are always been placed at \nwhole data search task chunks. Thus, wh\nis at the lower-leveled position in a struc\nmissing parts are the passive roles in\nprocesses, the reading order of the data\nadjusted accordingly. The different scen\nbelow: \na) \nFor the C-DRM type of sentenc\nstructure relation is bidirectional, no ma\nchunk is before or after the “Be,” tak\nchunk as the start reading point, then to \nchunks one by one. The readout task ch\nC and mark out with underlines. Of cour\nchunk needs to be substituted by the\nsearch-scope, and a question mark is ad\nsearch task chunk.  \nb) \nFor the C-SRM type of \nrepresents the unidirectional structure\nchunk behind “Have” is the subset of th\nbefore “Have”. Thus, when the missing \n“Have”, the order of data chunks in the\nbe changed (e.g. items 4 and 5a in Table\nstructurally lower-leveled data chunk (th\n“Have”) is the missing part, the missing\nset as the start reading point, but the \nattached behind the higher-leveled da\nchunk before “have”) to indicate i\nunidirectional structure relation, extra au\nis added between the lower-leveled d\nhigher-leveled data chunk to separate th\nTable VII), the readout sentences are list\nmark out with underlines. Of course, the\nis substituted by the pointer chunk of \nquestion mark is added to end this data s\nc) \nFor the C-PRM type of sentenc\ntype of task chunks, when the missing\nactive role in the attribute-changing pr\ndata chunks in the task chunk won’t be\nthe missing parts are the passive roles, th\nchunk needs to start with the missing p\nverbs need to be attached behind the ac\nits active position, the extra auxiliary w\nbetween the active role and the passive r\n(e.g., item 7 and 8 in Table VII). Finally\nadded to end this data search task chunk\nAll the above studies do not involv\nsystem in natural language is a descrip\nposition relation between the current ti\ntimeline and the time position when the\nthe timeline.  \nSo far, the author separates the inform\nprocessing procedure of information\nlanguage and briefly introduced the rese\ninformation organization architecture and\nof lexical and sentence type. In the nex\ntakes the spatial-position attribute inform\ne pointer chunks of \nthe beginning of the \nhen the missing part \nctural relation or the \nn attribute-changing \na chunk needs to be \nnarios are discussed \nces: Due to the “Be” \natter the missing data \nke the missing data \nread the whole data \nhunks list in column \nrse, the missing data \ne pointer chunk of \ndded to end this data \nsentences: “Have” \ne relation, the data \nhe data chunk which \ndata chunk is before \ne task chunks won’t \ne VII); but when the \nhe data chunk behind \ng data chunk still be \n“Have” need to be \nata chunk (the data \nts position in the \nuxiliary words “Do” \ndata chunk and the \nhem (e.g. item 5b in \nted in column C and \ne missing data chunk \nsearch-scope, and a \nsearch task chunk. \nces: Same as C-SRM \ng data chunk is the \nrocess, the order of \ne changed; but when \nhe whole data search \npassive part, and the \nctive role to indicate \nword “Do” is added \nrole to separate them \ny, a question mark is \n.  \nve tenses. The tense \nption of the relative \nime position on the \ne data is recorded on \nmation (data) and the \n(data) in natural \nearch thinking of the \nd new classifications \nxt section, the author \nmation as an example \nto illustrate how the spatia\norganized and processed in sp\nIII. \nSOM\nA. One Spatial-position Attr\nAn attribute space (AS\nrelations between entities in \nposition AS. In spatial-positio\nrecognized by the human \nabstracted out the scope an\nentities, and then stored these\nand direction relations in the\nintroducing the spatial projec\nthe spatial-position AS mod\nintroduced: \nTABLE VIII.  \nSPATIAL-\nCategory \nExamples\nScope Relation Recognition\nInside\nOutside\nIn, at, inside, wi\nOut of, outside, \nDirection Relation Recognition \n Relative direction \nUpper side\nDown side\nFront:\nBack:\nLeft side\nRight side\nOthers\non, above, up, o\nunder, below, b\n before  \nafter, behind \non the left side\non the right side\nagainst, toward\n Absolute direction a \n \nEast, west, south\nmiddle \nDistance Relation Recognition \n \nBy, beside, alon\naround, close to,\n      a. The absolute concept only exists\n1) \nSpatial-positioning w\npositioning points (SAPP): \na) \nSpatial-positioning \nto represent the position info\nThere are not many words \nnatural language, and accord\njudging mechanisms, they c\nscope relation recognition,\nrecognition, and the spatial\nAccording to the coordin\ndirection recognition can \ndirection recognition and the\nsee the examples in Table VI\nb) \nSpace-assisted Pos\nstatic EW (see Table III)\npositioning purpose in natura\nthis paper. \nal-position information is been \npatial AS.  \nME EXAMPLES \nribute Space Model \nS) that represents the spatial \nthe real world is called spatial-\non AS, the spatial information is \nvisual sensory system, then \nnd direction relations between \ne entities and their spatial scope \ne corresponding models. Before \nction map (SPM) which is one of \ndels, two concepts need to be \n-POSITIONING WORDS AND PHRASES \ns of SPW \nSymbol &\nCoordinate System\n \nithin, among \nbeyond \n∈，⸦，⸧\n∉, \n \n \nover \neneath \ne \n \nh, north, \n \ngside, nearby,  \n, next to\ns within a specific range and scale, which won’t be \ndiscussed in this paper \nwords (SPW) and space-assisted \nWords (SPW): SPWs are used \normation in spatial-position AS. \nused for spatial-positioning in \nding to different recognition and \ncan be divided into the spatial \nthe spatial direction relation \nl distance relation recognition. \nate system used, the spatial \nbe divided into the relative \ne absolute direction recognition, \nIII. \nsitioning Point (SAPP): The \nwhich used for space-assisted \nal language, are called SAPP in \n \nFigure 7.  Graph-tree structure of SPM \n2) Spatial Projection Map (SPM): Now, we will model \nthe spatial relation between entities using SPW and SAPP. \nSpatial projection map (SPM) is a graph-tree model \nrepresenting the scope and relative direction relations \nbetween relative static entities. Static entities are being \nabstracted out as vertices elements of the model. Scope and \nrelative direction relations are being abstracted out as edges \nelements of the model. As we can see in Fig.7, SPM is a \nhybrid model combined with horizontal graph structures and \nvertical tree structures. The relative direction relations are \nstored in the horizontal graph structure, and scope relations \nare stored in the vertical tree structure. The author use the \nadjacency matrix to represent the graph structure of the \nSPM in Fig.7, details are as follows:  \na) Graph Structure Model (in-layer or horizontal \nstructure): The vertices or SAPPs are stored in graph \nstructure according to their relative direction relations in the \nreal world. They also can be represented in mathematic as \nbelow:  \n Graph G=<V, E>  consists of V, a set of all the \nSAPPs in the same layer called vertices, and E, a set of six \nfixed relative direction relation of  V called edges.  \n  𝐺௜= < ⋃\n𝐺\n𝑛\n𝑗=1\n𝑖\n𝑗>, i: serial number of layer, j: serial \nnumber of the subgraph. \nTABLE IX.  \nADJACENCY MATRIX REPRESENTION OF SUBGRAPH \n𝐺଴\nଵ=< 𝑉଴\nଵ, 𝐸>  \nSN\n𝑽𝟎\n𝟏  E\nLeft \nside\nRight\nside\nFront\nBack\nUpper\nside\nDown\nside\n1\nTable\nФ \nФ \nФ \nSofa\nApplea\nФ\n2\nFridge\nSofa\nФ \nФ \nФ \nCata \nФ\n3\nSofa \nФ \nFridge\nTable\nФ \nФ \nФ\n a. “Apple” and “Cat” are Dynamic EW, thus, they are not been listed in 𝑉଴\nଵ set.   \nTake the red dotted box portion of Layer 0 in Fig.7 as an \nexample, Table IX is a adjacency matrix which represents \nthe subgraph 𝐺଴\nଵ=< 𝑉଴\nଵ, 𝐸>. In the subgraph 𝐺଴\nଵ, the vertices \nset 𝑉଴\nଵ = {table, fridge, sofa} and the edges set E = (left side, \nright side, front, back, upper side, downside), the elements in \nthe vertices set can be added or subtracted according to the \nactual situation, but may not be repeated, the edges set is a \ntuple consisting of six fixed spatial direction relation. If \nvertices are not on these six directions, the spatial direction \nrelation between the subject and object needs to be \ncalculated, the calculation method won’t be discussed here.   \nb) Tree Structure Model (between-layer or vertical \nstructure): See Fig.8, the tree structure is the vertical \nstructure of the graph-tree model, which is a between-layer \nstructure. It represents the spatial scope relation of vertices \nbetween adjacent layers. A graph-tree model can consist of \nmany trees. The root, leaves, and internal vertices of each \ntree can be distributed in different layers. The spatial scope \nof each vertex is represented by the union of the spatial \nscope of all its children vertices. See Fig.8, the spatial scope \nof the internal vertex “House” is the union of {Table, fridge, \nsofa …}.  \n \nFigure 8.  Vertical Tree Structure in SPM \nIn the tree structure, the spatial scope of the children \nvertices that own the same parent should be independent of \neach other; If not, they should be moved up or down until all \nthe children vertices are been independent of each other on \nthe spatial scope. \n3) \nTypical Expression of Spatial-position Information in \nNatural Language.: We can position a target object by \ndeducing its spatial relations with the SAPPs in specific \nSPM. We use two typical ways to represent the spatial-\nposition of the cat in Fig.7. \na) Type A expression method: This is the spatial \ndirection relation recognition method that executes on the \ngraph structure of SPM, which needs to find out the adjacent \nnodes of the target object on the fixed six directions. See \nTable X, we respectively take the “Cat” and “Sofa” as target \nobjects to find out their adjacent nodes (SAPP) and the \nrelative direction relation between them on graph structure \nof SPM and output the sentences. When people (the IPE) \nuse this expression method, they (the IPE) and the target \nobject are usually in the same layer of the SPM. \nTABLE X.  TYPE A EXPRESSION METHOD OF SPATIAL-POSITION \nINFORMATION \nTarget \nObject\nSpatial-position attribute \nOutput sentences \nSAPP\nRelative direction (SPW)\nCat\nFridge\nUpside (on)\nThe cat is on the fridge. \nSofa\nTable\nBack side (behind)\nThe sofa is behind the table.\nSofa\nFridge\nLeft side\n(on the left side of)\nThe sofa is on the left side of\nthe fridge.\nb) Type B expression method: This is a spatial scope \nrecognition method that executes on the tree structure of \nSPM, which needs to find out the parent node of the target \nobject, then use the parent node to assist in positioning the \ntarget object. In Fig.8, the parent node of “Apples” is \n“Fridge”, the parent node of “Cat” is “House”, and the \nparent node of “House” is “**community”. We use the \nparent node to define the scope of the target project.   People \n(the IPE) prefer to use this expression method when they \n(the IPE) are not in the same layer with the target object in \nSPM.  \nTABLE XI.  \nTYPE B EXPRESSION METHOD OF SPATIAL-POSITION \nINFORMATION \nTarget \nObject\nSpatial-position attribute \nOutput sentences \nSAPP \nScope relation (SPW)\nApples\nFridge\nInside (in)\nThe apples are in the fridge. \nCat\nHouse\nInside (in)\nThe cat is in the house.\nHouse\n**community\nInside (in)\nMy house is in **community.\nIn general, when people (the IPE) need to describe the \nposition of an object, they need to find out the location of \nboth the object and themselves (IPE) in their SPM, and then \nto decide which method to choose to express the spatial-\npositioning information in natural language. \nThe author just takes the spatial-position AS as an \nexample to elaborate on the spatial attribute information \nprocessing mechanism in SPM. Similarly, different attribute \ninformation is been processed in different corresponding AS, \nthe corresponding information-processing mechanisms are \nthe foundation of NLU.  \nB. Understanding Process of a Dialogue  \nSince the author has briefed the new classification of the \nwords and introduced the information process mechanism in \none of the spatial-position AS. Now let us put the theory into \npractice through the example below. The example of a \nDialogue and the background information is given in Table \nXII, and the understanding process is listed in Table XIII.\nTABLE XII.  \nDIALOGUE EXAMPLE AND THE BACKGROUND INFORMATION \nBackground Information:\nDialogue (CTS a: 1st Oct, 17:05): \nJack: the owner of the house and the home service robot.\nNana: the home service robot. \nDefault setting: the ownership of all the things in the house are belongs to \nJack, which means the ownership here can be defined by the spatial \nattribute. The SPM in Fig.7 is taken as the spatial-position AS in Nana’s \nbrain. \nJack: “Nana, do we have any apple?”\nNana: “Yes.” \nJack: ”Give me an apple.” \nNana: “Sure.” \na. Timestamps are important tool to identify data creation and termination coordinates on timeline (one of time AS).  \nCTS: creation timestamp, TTS: termination timestamp.  \nTABLE XIII.  \nINFORMATION UNDERSTANDING PROCESS ON NANA SIDE \nSentence 1: “Nana, do we have any apple?”\nIPE-1: Jack \nSentence 2: “Give me an apple.”\nIPE-1: Jack \nIPE-2: Nana\nIPE-2: Nana\nStep 1：Word segmentation \nNana  ,  do  we  have  any  apple  ?\nStep 1：Word segmentation \nGive   me  an  apple  . \nStep 2：Identify the sentence pattern \nDo : Auxiliary word.  \nhave: Structure words   Inclusion relation of sets. \ndo…have…? :  \n     A-DRM     A-SRM     A-PRM    \n     B-DRM     B-SRM     B-PRM    \n     C-DRM     C-SRM     C-PRM     \nNana  ,  do  we  have  any  apple  ?\nStep 2：Identify the sentence pattern \nGive : Data chunk  Verb. \nGive……. :  \n     A-DRM     A-SRM     A-PRM    \n     B-DRM     B-SRM     B-PRM    \n     C-DRM     C-SRM     C-PRM  \nGive  me  an  apple  . \nStep 3：Identify the objects in sentence. \nObject 1: Jack (we) \nJack\nSpatial-position\nTime Position\nOn sofa\nCTS: 1st Oct, 16:30\nTTS: current time\nObject 2: Apple (apple) \nApple\nSpatial-position\nTime Position\nIn fridge (Qty: 3)\nCTS: 29th Sep, 11:00\nTTS: current time\nNana  ,  do  we  have  any  apple  ?\nStep 3：Identify the objects in sentence. \nObject 1: Jack (me) \nJack\nSpatial-position\nTime Position\nOn sofa\nCTS: 1st Oct, 16:30\nTTS: current time\nObject 2: Apple (apple) \nApple\nSpatial-position\nTime Position\nIn fridge (Qty: 3)\nCTS: 29th Sep, 11:00\nTTS: current time\nGive  me  an  apple  .\nStep 4: Identify the inclusion relation verification task between \nObject 1 (Jack) and Object 2 (Apple). \ndo  Object 1 have any Object 2 ? : \nTask: to verify whether Object 2 (apple) is a subset of Object 1(Jack). \n \n    Any: Data word  Measuring word. \n Subtask: to verify whether the quantity of Object 2 > 0. \nStep 4: Identify the action task \nGive Object 1 (sb) Object 2 (sth). \nTask: to take a action that to change the spatial-position of Object 2 \n(apple) from the current position (in fridge) to the position of Object 1 \n(Jack). \n \nAn: Data word  Measuring word. \nSubtask: give out the quantity of Object 2 that need to be moved.\nStep 5: Run the verification tasks, and return the result. \nTask:  According to the default setting, all the things in SPM in Fig.7 \nbelongs to Jack. Apple is found in SPM in Fig.7 which means Object 2 is \na subset of Object 1 \nReturn: True \n \n    Subtask: Look up the Memory-sheet of Object 2 in step 3, the quantity \nof Object 2 = 3 >0. \n        Return: True \n         \nReturn: True (Yes.) \n             Nana  ,  do  we  have  any  apple  ? \n \nAll information has been processed \nContinue\nStep 5: Take the action (mobile robot field won’t be discussed here) \nReturn: Achieveable (Sure.)\nStep 6: Update the relevant information in Nana’s Memory-sheet.  \nApple\nSpatial-position\nTime Position\nIn fridge (Qty:3)\nCTS: 29th Sep, 11:00\nTTS: 1st Oct, 17:06 \nIn fridge (Qty:2)\nCTS: 1st Oct, 17:06\nTTS: current time\nGive  me  an  apple  . \n \nAll information has been processed \n    Continue \nSo far, Nana still needs to figure out how to deliver the \napple into Jack’s hands instead of to deliver it into the sofa. \nThere are still many details to deal with, and the \nunderstanding is not entirely precise, but Nana (the home \nservice robot) is already able to understand commands given \nin natural language. \nIV. \nTHE \nBASIC \nCONDITIONS \nAND \nDEFINITION \nOF \nUNDERSTANDING \nWe roughly discussed the information architecture \nthrough introducing the new classification of lexical and \nsentences types, which mainly involves the storage structure \nof data, and the information processing operations \nimplemented on existing information storage database (e.g. \ninformation reading, information verification, etc.). It is easy \nto find out that the same information perception systems, the \nsame information processing systems, and the same \ninformation storage database are the basic conditions of \nNLU. The same physiological structure of human beings \nensures that different individuals have the same information \nperception and processing system. And learning from each \nother can make up for differences in information storage \ndatabases in different brains, thus reducing differences in \nunderstanding.  \n \nFigure 9.  The simplified diagram of the process of reading, transmitting, \nand understanding of information between information sender and receiver \nNow, the author tries to redefine the “understanding” in \nthe following ways: \n \nAt the data level, “Understanding” is mainly about \nunderstanding the attributes of EC which include \nbut not limited to the understanding of the \nvariation-rules of their basic attributes, the \npredictions of the variation-boundaries of their \nbasic attributes in different attribute spaces, the \nhierarchical relations with other related EC \n(extended attributes), and the position (advanced \nattributes) of an EC in the  population of a class \nsamples after instantiation.  \n \nAt \nthe \ninformation-processing \nlevel, \n“Understanding” \nis \nabout \nunderstanding \nthe \noperational requirements for the target data chunks \nas expressed in the specific task chunks. Whether it \nis a data description task, a data verification task, or \na data search task, they all involve the following \noperations: read, write, modify, search, etc.  \n \nIPE always uses their own memory-databases to \nunderstand ( or interpret) the input data chunks, the \nunderstanding varies when the memory-databases \nare different. When the input data is entirely new, \nthe IPE needs to learn and build the related data in \ntheir memory-database before they can use the \nmemory-database to interpret the input data. \nJust like human learning in infancy, the construction of \nthe information memory-database needs to start from the \nmost basic information related to human beings. After \nhaving the basic and necessary information memory-\ndatabase, more abstract information systems such as \ndiscipline research and the corresponding knowledge-graph \ncan be built on it. The researches of different disciplines \nactually are the researches and constructions of specific AS.  \nIn this way, all the knowledge in human history can be \nincorporated into the information memory-database, and \nbeen inherited and applied. When this becomes a reality, all \nhuman beings will have a shared decision-making system, \nand humanity will enter a whole new era.  \nREFERENCES \n[1] Mark F. Bear, Barry W. Connors, Michael A. Paradiso, Neuroscience: \nExploring the Brain, 2nd ed., Higher Education Press, Beijing, 2004. \n[2] E. F. Codd, “A Relational Model of Data for Large Shared Data \nBanks,” Communications of the ACM, Vol 13, June. 1970. \n[3] Kenneth H. Rosen, Discrete Mathematics and Its Applications, 4th ed., \nChina Machine Press, Beijing, 1996. \n[4] Harold Abelson, Gerald Jay Sussman, Julie Sussman, Structure and \nInterpretation of Computer Programs, 2nd ed., MIT Press, London, \n1999. \n[5] Andrew S. Tanenbaum and Herbert Bos, Modern Operating Systems, \n4th ed., China Machine Press, Beijing, 2017. \n[6] John L. Hennessy, David A. Patterson, Computer Architecture: A \nQuantitative Approach, 4th ed., Morgan Kaufmann Publishers, USA, \n2007. \n[7] Fu ZuYun,  Information Theory: Principles and Applications, 4th ed., \nPublishing House of Electronics Industry, Beijing, 2014. \n[8] Zhao Zhongxing, “Research on the classification standards of word \nclasses in different textbooks of Morden Chinese,”  Chinese \nCharacter Culture, July. 2017 \n[9] Daniel Kahneman, Thinking, fast and slow, China CITIC Press, \nBeijing, 2012. \n[10] Donella H. Meadows, Thinking in Systems: A Primer, Zhejiang \nPeople Publishing House, Hang Zhou, 2012. \n[11] James Gleick, The Information: A History, a Theory, a Flood, Posts \n& Telecom Press, Beijing, 2013. \n[12] Judea Pearl and Dana Mackenzie, The Book of Why: The New \nScience of Cause and Effect, China CITIC Press, Beijing, 2019. \n[13] Ma Qingzhu, The Chinese Verb and Verbal Constructions, Peking \nUniversity Press, Beijing, 2004. \n[14] Peter Carruthers, Stephen Stich and Michael Siegal, The Cognitive \nBasis of Science, Science Press, Beijing, 2015. \n[15] Richard J. Gerrig and Philip G. Zimbardo, Psychology and Life, 16 th \ned., Posts & Telecom Press, Beijing, 2003. \n[16] Sun Jixiang, Modern Pattern Recognition, 2nd ed., Higher Education \nPress, Beijing, 2007. \n[17] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and \nClifford Stein, Introduction to Algorithms, 3rd ed., The MIT Press, \nLondon, 2009. \n[18] Alfred V. Aho, Monica S. Lam, Ravi Sethi and Jeffrey D. Ullman, \nCompliers: Principles, Techniques and Tools, 2nd ed., China \nTechniques and Tools, Beijing, 2009. \n[19] Wu WeiShan, Cognitive Linguistics and Chinese Studies, Fudan \nUniversity Press, 2011. \n[20] Zhou ZhiHua, Machine Learning, Tsinghua University Press, Beijing, \n2015. \n[21] Andrew S. Tanenbaum, Structured computer organization 5th ed., \nPosts & Telecom Press, Beijing, 2006. \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-10-24",
  "updated": "2021-01-12"
}