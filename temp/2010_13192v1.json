{
  "id": "http://arxiv.org/abs/2010.13192v1",
  "title": "The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task",
  "authors": [
    "Alexandra Chronopoulou",
    "Dario Stojanovski",
    "Viktor Hangya",
    "Alexander Fraser"
  ],
  "abstract": "This paper describes the submission of LMU Munich to the WMT 2020\nunsupervised shared task, in two language directions, German<->Upper Sorbian.\nOur core unsupervised neural machine translation (UNMT) system follows the\nstrategy of Chronopoulou et al. (2020), using a monolingual pretrained language\ngeneration model (on German) and fine-tuning it on both German and Upper\nSorbian, before initializing a UNMT model, which is trained with online\nbacktranslation. Pseudo-parallel data obtained from an unsupervised statistical\nmachine translation (USMT) system is used to fine-tune the UNMT model. We also\napply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more\nrobust system. We additionally experiment with residual adapters and find them\nuseful in the Upper Sorbian->German direction. We explore sampling during\nbacktranslation and curriculum learning to use SMT translations in a more\nprincipled way. Finally, we ensemble our best-performing systems and reach a\nBLEU score of 32.4 on German->Upper Sorbian and 35.2 on Upper Sorbian->German.",
  "text": "The LMU Munich System for the WMT 2020 Unsupervised Machine\nTranslation Shared Task\nAlexandra Chronopoulou, Dario Stojanovski, Viktor Hangya, Alexander Fraser\nCenter for Information and Language Processing, LMU Munich, Germany\n{achron, stojanovski, hangyav, fraser}@cis.lmu.de\nAbstract\nThis paper describes the submission of LMU\nMunich to the WMT 2020 unsupervised\nshared task,\nin two language directions,\nGerman↔Upper Sorbian. Our core unsuper-\nvised neural machine translation (UNMT) sys-\ntem follows the strategy of Chronopoulou et al.\n(2020), using a monolingual pretrained lan-\nguage generation model (on German) and ﬁne-\ntuning it on both German and Upper Sorbian,\nbefore initializing a UNMT model, which is\ntrained with online backtranslation. Pseudo-\nparallel data obtained from an unsupervised\nstatistical machine translation (USMT) system\nis used to ﬁne-tune the UNMT model. We also\napply BPE-Dropout to the low-resource (Up-\nper Sorbian) data to obtain a more robust sys-\ntem. We additionally experiment with resid-\nual adapters and ﬁnd them useful in the Up-\nper Sorbian→German direction. We explore\nsampling during backtranslation and curricu-\nlum learning to use SMT translations in a more\nprincipled way. Finally, we ensemble our best-\nperforming systems and reach a BLEU score\nof 32.4 on German→Upper Sorbian and 35.2\non Upper Sorbian→German.\n1\nIntroduction\nNeural machine translation achieves remarkable re-\nsults (Bahdanau et al., 2015; Vaswani et al., 2017)\nwhen large parallel training corpora are available.\nHowever, such corpora are only available for a\nlimited number of languages.\nUNMT addresses\nthis issue by using monolingual data only (Artetxe\net al., 2018c; Lample et al., 2018). The perfor-\nmance of UNMT models is further improved using\ntransfer learning from a pretrained cross-lingual\nmodel (Lample and Conneau, 2019; Song et al.,\n2019). However, pretraining also demands large\nmonolingual corpora for both languages. Without\nabundant data, UNMT methods are often ineffective\n(Guzm´an et al., 2019). Therefore, effectively trans-\nlating between a high-resource and a low-resource\nlanguage, in terms of monolingual data, which is\nthe target of this year’s unsupervised shared task,\nis challenging.\nWe participate in the WMT 2020 unsuper-\nvised machine translation shared task. The task\nincludes two directions:\nGerman→Upper Sor-\nbian (De→Hsb) and Upper Sorbian→German\n(Hsb→De). Our systems are constrained, using\nonly the provided Hsb monolingual data and De\nNewsCrawl monolingual data released for WMT.\nWe pretrain a monolingual encoder-decoder model\non a language generation task with the Masked Se-\nquence to Sequence model (MASS) (Song et al.,\n2019) and ﬁne-tune it on both languages of interest,\nfollowing Chronopoulou et al. (2020). We then\ntrain it on UNMT, using online backtranslation. We\nuse our USMT system to backtranslate monolin-\ngual data in both languages. This pseudo-parallel\ncorpus serves to ﬁne-tune our UNMT model. Itera-\ntive ofﬂine backtranslation is later leveraged, yield-\ning a performance boost. We use BPE-Dropout\n(Provilkov et al., 2020) as a data augmentation\ntechnique, sampling instead of greedy decoding\nin online backtranslation, and curriculum learning\nto best include the SMT pseudo-parallel data. We\nalso use residual adapters (Houlsby et al., 2019) to\ntranslate to the low-resource language (Hsb).\nResults Summary.\nThe ensemble of our best-\nperforming systems yields the best performance\nin terms of BLEU1 among the participants of the\nunsupervised machine translation shared task. We\nrelease the code and our best models2 in order to\nfacilitate reproduction of our work and experimen-\ntation in this ﬁeld. We note that we have built upon\n1http://matrix.statmt.org/matrix/\nsystems_list/1920\n2https://github.com/alexandra-chron/\numt-lmu-wmt2020\narXiv:2010.13192v1  [cs.CL]  25 Oct 2020\nﬁne-tuned MASS \n+ online BT (2)\nwith sampling\nﬁne-tuned MASS \nwith adapters \n+ online BT\n \nonline BT +\npseudo-SMT with \nsampling\nonline BT + \npseudo-SMT with \ncurriculum\nonline BT +\npseudo-NMT\nonline BT + \npseudo-SMT  \n(Hsb-De)\nonline BT +  \npseudo-SMT with  \nsampling (with \nadapters)\nonline BT with\nBPE Dropout \nonline BT with \nBPE Dropout \nVanilla UMT\nUNMT + 7.5M \nBPE-Dropout upsampled \nSMT pseudo-parallel\nUSMT\nUNMT + \n750K SMT \npseudo-parallel\nUNMT +\n 750K NMT \npseudo-parallel\nUNMT +\n 10M Hsb-De SMT \npseudo-parallel\n1\n3\n4\n5\n6\n8\n7\n9\n11\n10\nMASS\n0\nLanguage generation\npretraining\nFigure 1: Illustration of our system. We denote with green the systems that were ensembled for the De→Hsb direc-\ntion and with maroon the systems that were ensembled for the Hsb→De direction. Right arrows indicate transfer\nof weights. The numbers in gray correspond to the rows of Table 1. Online BT refers to the backtranslation\nof sentences with the actual model and updating it with the generated pseudo-parallel data. Pseudo-S M T refers\nto data obtained by backtranslating using the USMT baseline system while pseudo-NMT to our translations using\nsystem 5. The components of our approach are explained in Section 2.\nthe MASS codebase3 for our experiments.\n2\nModel Description\nFigure 1 presents all the different components of\nour system and how they are connected to each\nother. We train both an unsupervised SMT (#1) and\nNMT (#2) model. The UNMT model is based on a\npretrained MASS model (#0), which is monolingual\n(De). The model is later ﬁne-tuned on both Hsb\nand De. We additionally explore ﬁne-tuning only\non Hsb using adapters. These models are used to\ninitialize an NMT model (#2, #4) which is trained\nwith online backtranslation. We additionally exper-\niment with sampling (#3) during backtranslation.\nThe USMT model is used to backtranslate Hsb and\nDe data. This synthetic bi-text is used to ﬁne-tune\nthe baseline UNMT model (#5). We use the syn-\nthetic bi-text also to ﬁne-tune directly the adapter-\naugmented MASS model, while employing online\nbacktranslation and sampling (#8). We experiment\nwith curriculum learning (#6) to estimate the op-\ntimal way to feed the model this pseudo-parallel\ndata. We also use our UNMT model to generate\nbacktranslations and ﬁne-tune existing models (#7).\nFurther USMT-backtranslated data is used in #9.\nFinally, some models are ﬁne-tuned with mono-\nlingual data which is oversampled and segmented\n3https://github.com/microsoft/MASS\nwith BPE-Dropout (#10, #11). The details of these\ncomponents are outlined in the following.\n2.1\nUnsupervised SMT\nFirst we describe the USMT system which we use to\ngenerate pseudo-parallel data to ﬁne-tune our NMT\nsystem. We use monoses (Artetxe et al., 2018b),\nwhich builds unsupervised bilingual word embed-\ndings (BWEs) and integrates them to Moses (Koehn\net al., 2006), but apply some modiﬁcations to it.\nAs a ﬁrst step, we build unsupervised BWEs\nwith fastText (Bojanowski et al., 2017) and VecMap\n(Artetxe et al., 2018a) containing representations of\n1-, 2- and 3-grams. Since the size of the available\nmonolingual Hsb data is low, mapping monolin-\ngual embeddings to BWEs without any bilingual\nsignal fails, i.e., we ﬁnd no meaningful translations\nby manually investigating the most similar cross-\nlingual pairs of a few words. Instead, we rely on\nidentical words occurring in both De and Hsb cor-\npora as the initial seed dictionary. The BWEs are\nthen converted to phrase-tables using cosine sim-\nilarity of words and a language model is trained\non the available monolingual data. The shared task\norganizers released a validation set which we use to\ntune the parameters of the system with MERT, in-\nstead of running unsupervised tuning as described\nin Artetxe et al. (2018b). Finally, we run 4 itera-\ntive reﬁnement steps to further improve the system.\nOther than the above, all steps and parameters are\nunchanged.\nWe use this system in inference mode to back-\ntranslate 7M De and 750K Hsb sentences. We\nrefer to this pseudo-parallel dataset as 7.7M SMT\npseudo-parallel. We also backtranslate 10M\nmore De sentences. This dataset is later used to\nﬁne-tune one of our systems. We refer to it as 10M\nHsb-De SMT pseudo-parallel.\n2.2\nMASS\nWe initialize our UNMT systems with an encoder-\ndecoder Transformer (Vaswani et al., 2017), which\nis pretrained using the MASS (Song et al., 2019)\nobjective. The model is pretrained by trying to\nreconstruct a sentence fragment given the remain-\ning part of the sentence. The encoder takes a ran-\ndomly masked fragment as input, while the de-\ncoder tries to predict the masked fragment. MASS\nis inspired by BERT (Devlin et al., 2019), but is\nmore suitable for machine translation, as it pre-\ntrains the encoder-decoder and the attention mech-\nanism, whereas BERT is an encoder Transformer.\nIn order to pretrain the model, instead of training\nMASS on both De and Hsb, we initially train it\non De. After this, we ﬁne-tune it on both De and\nHsb, following RE-LM (Chronopoulou et al., 2020).\nThe intuition behind this is that, if we simultane-\nously train a cross-lingual model on unbalanced\ndata, where X is much larger than Y , the model\nstarts to overﬁt the low-resource side Y before be-\ning trained on all the high-resource language data\n(X). This results in poor translations. We refer to\nour pretrained model as FINE-TUNED MASS.\n2.2.1\nVocabulary Extension for NMT\nTo ﬁne-tune the pretrained De MASS model on\nHsb, we need to overcome the following issue: the\npretrained model uses BPE segmentation and vo-\ncabulary based only on De. To this end, we again\nfollow RE-LM. We denote these BPE tokens as\nBPEDe and the resulting vocabulary as VDe. We\naim to ﬁne-tune the monolingual MASS model to\nHsb. Splitting Hsb with BPEDe would result in\nheavy segmentation of Hsb words. To prevent this\nfrom happening, we learn BPEs on the joint De and\nHsb corpus (BPEjoint). We then use BPEjoint to-\nkens to split the Hsb data, resulting in a vocabulary\nVHsb. This method increases the number of shared\ntokens and enables cross-lingual transfer of the pre-\ntrained model. The ﬁnal vocabulary is the union\nof the VDe and VHsb vocabularies. We extend the\ninput and output embedding layer to account for\nthe new vocabulary items. The new parameters are\nthen learned during ﬁne-tuning.\n2.3\nAdapters\nBesides initializing our UNMT systems with FINE-\nTUNED MASS, we also experiment with pretraining\nMASS on De and ﬁne-tuning only on Hsb. During\nﬁne-tuning, we freeze the encoder and decoder\nTransformer layers and add adapters (Houlsby\net al., 2019) to each of the Transformer layers.\nAdapters can prevent catastrophic forgetting (Good-\nfellow et al., 2013) and show promising results in\nvarious tasks (Bapna and Firat, 2019; Artetxe et al.,\n2020). We ﬁne-tune only the output layer, the em-\nbeddings and the decoder’s attention to the encoder\nas well as the lightweight adapter layers.\nWe investigate adapters as ﬁne-tuning in this\nway is considerably more computationally efﬁcient.\nWe also experimented with freezing the decoder’s\nattention to the encoder as well as adding an adapter\non top of it, but these architecture designs are worse\nin terms of perplexity during MASS ﬁne-tuning as\nwell as BLEU scores during UNMT.\nWe use the ﬁne-tuned model to initialize an\nencoder-decoder Transformer, augmented with\nadapters. The adapter-augmented model is then\ntrained in an unsupervised way, using online back-\ntranslation. All layers are trainable during unsu-\npervised NMT training. We refer to this model as\nFINE-TUNED MASS + ADAPTERS.\n2.4\nUnsupervised NMT (online\nbacktranslation)\nWe initialize our UNMT models with FINE-TUNED\nMASS. Following Song et al. (2019), we train\nthe systems in an unsupervised manner, using on-\nline backtranslation (Sennrich et al., 2016a) of the\nmonolingual Hsb and De data, that were also used\nfor pretraining. As proposed in Song et al. (2019),\nwe do not use denoising auto-encoding (Vincent\net al., 2008). We use online backtranslation to gen-\nerate pseudo bilingual data for training. We refer\nto the resulting model as UNMT BASELINE.\n2.5\nSampling\nWe experiment with sampling instead of greedy\ndecoding during online backtranslation. Edunov\net al. (2018) show that sampling is beneﬁcial for\nbacktranslation compared to greedy decoding or\nbeam search for systems trained on larger amounts\nof parallel data. Although we do not use any paral-\nlel data, we assumed that our initial UNMT baseline\nis of reasonable quality and that sampling would be\nbeneﬁcial. However, in order to provide a balance,\nwe randomly use either greedy decoding or sam-\npling during training. The frequency with which\nsampling is used is a hyperparameter which we set\nto 0.5. Sampling temperature is set to 0.95.\n2.6\nCurriculum learning\nConsidering the high improvements achieved by\nincluding SMT backtranslated data, we conduct ex-\nperiments to determine a more meaningful way to\nfeed the data to the model using curriculum learn-\ning (Kocmi and Bojar, 2017; Platanios et al., 2019;\nZhang et al., 2019). We learn the curriculum us-\ning Bayesian Optimization (BO) for which we use\nan open source implementation4. Similar work\nhas been proposed for transfer learning (Ruder and\nPlank, 2017) and NMT (Wang et al., 2020). As we\nalready have a reasonably trained NMT model, we\nuse it to compute instance-level features for learn-\ning the curriculum. Each sentence pair from the\nSMT backtranslated data is represented with two\nfeatures: the model scores for this pair in the origi-\nnal (backtranslation →monolingual sentence) and\nreverse direction (monolingual →backtranslation).\nThe weights that determine the importance of\nthese features are learned separately for De→Hsb\nand Hsb→De, so that we have 4 features in total.\nBO runs for 30 trials. The feature weights are con-\nstrained in the range [−1, 1]. Each trial runs 5.4K\nNMT updates. The curriculum optimizes the sum of\nHsb→De and De→Hsb validation perplexity. For\nthe optimization trials, we only use the SMT back-\ntranslated data as pseudo-parallel data and do not\nuse online backtranslation. Finally, based on the\nfeature weights and the features for each sentence,\nwe sort the pseudo-parallel data and ﬁne-tune the\nUNMT BASELINE with SMT backtranslations and\nonline backtranslation. It would be interesting to\nstudy if a similar approach can be used to estimate\na more optimal loading of monolingual data during\nMASS pretraining and UNMT.\n2.7\nOfﬂine Iterative Backtranslation\nWe also experiment with creating synthetic train-\ning data using ofﬂine backtranslation with one of\nour UNMT systems (#5 in Table 1). We translate\n750K De sentences to Hsb and 750K Hsb sen-\n4https://ax.dev/\ntences to De. The resulting pseudo-parallel system\nis denoted as 750K NMT pseudo-parallel\ncorpus and is used to ﬁne-tune the same system.\n2.8\nBPE-Dropout\nBPE segmentation is useful in machine translation,\nas it efﬁciently addresses the open vocabulary prob-\nlem. This approach keeps the most frequent words\nintact and splits the rare ones into multiple tokens.\nIt builds a vocabulary of subwords and a merge ta-\nble, specifying which subwords have to be merged\nand the priority of the merges. BPE segmenta-\ntion always splits a word deterministically. Intro-\nducing stochasticity to the algorithm (Provilkov\net al., 2020), by simply removing a merge from\nthe merges with a pre-deﬁned probability p, results\nin signiﬁcant BLEU improvements for various lan-\nguages in low- and medium-resource datasets.\nWe use BPE-Dropout in the following way: we\noversample the Hsb monolingual data by a factor\nof 10 and apply BPE-Dropout. In that way, we get\ndifferent segmentations of the same sentences and\nfeed this data to the model. We also oversample\nthe 750K SMT pseudo-parallel corpus in\nthe same manner, but only apply BPE-Dropout\nto the Hsb side. These monolingual and pseudo-\nparallel oversampled datasets are used to ﬁne-tune\nour models. These systems perform better than our\nother single systems.\n2.9\nEnsembling\nFor the ﬁnal models, we perform ensemble decod-\ning with the best training models obtained in our\nexperiments. We evaluate several combinations\nof model ensembles. Based on BLEU scores on\nthe test set provided during development, we de-\ncide on two separate ensembles for De→Hsb and\nHsb→De for the ﬁnal submission.\n3\nExperiments\n3.1\nData Pre-processing\nIn line with the rules of the WMT 2020 unsuper-\nvised shared task5, we used 327M sentences from\nWMT monolingual News Crawl6 dataset for Ger-\nman, collected over the period of 2007 to 2019. We\nalso used the Upper Sorbian side of the provided\nparallel data as well as all of the monolingual data,\na total amount of 756K sentences, provided by the\n5http://www.statmt.org/wmt20/unsup_\nand_very_low_res/\n6http://data.statmt.org/news-crawl/de/\n#\nMethods\nDe→Hsb\nHsb→De\n0\nMASS\n5.6\n7.0\n1\nUSMT\n19.3\n21.4\n2\n0⃝UNMT baseline (ﬁne-tuned MASS)\n24.4\n27.1\n3\n2⃝UNMT baseline + sampling\n25.4\n27.4\n4\n0⃝UNMT baseline (ﬁne-tuned MASS with adapters)\n18.8\n21.7\n5\n3⃝+ online BT + pseudo-SMT + sampling\n29.9\n31.9\n6\n3⃝+ online BT + pseudo-SMT + curriculum\n30.0\n32.5\n6*\n3⃝+ online BT + pseudo-SMT + curriculum + sampling\n30.2\n32.8\n7\n5⃝+ online BT + pseudo-NMT\n29.8\n33.2\n8\n0⃝+ online BT + pseudo-SMT + sampling (with adapters)\n29.0\n32.3\n9\n7⃝+ online BT + pseudo-SMT (Hsb-De)\n30.0\n32.7\nData oversampling with BPE-Dropout\n10\n5⃝+ BPE-Dropout\n30.7\n33.4\n11\n7⃝+ BPE-Dropout\n31.8\n34.0\n12\nModel Ensemble (8, 9, 10, 11)\n32.4\n35.2\n13\nModel Ensemble (6, 9, 11)\n31.9\n34.8\nTable 1: BLEU scores of UMT for De-Hsb and Hsb-De systems. The systems with the underlined results were\nensembled and used in our primary submissions. #12 is our primary system submitted to the organizers in the\nDe→Hsb direction, while #13 is our primary system submitted in the Hsb→De direction. 6* was trained after the\nshared task and is not used for the ﬁnal submission.\norganizers. We used the provided parallel data\nfor validation/testing (2K/2K sentences). We nor-\nmalized punctuation, tokenized and true-cased the\ndata using standard scripts from the Moses toolkit\n(Koehn et al., 2006). We note that we tokenized\nHsb data using Czech as the language of tokeniza-\ntion, since these two languages are very closely\nrelated and there are no tokenization rules for Hsb\nin Moses.\nWe used BPE (Sennrich et al., 2016b) segmenta-\ntion for our neural system. Speciﬁcally, we learned\n32K codes and computed the vocabulary using the\nDe data. We then also learned the same amount\nof BPEs on the joint corpus (De, Hsb) and com-\nputed the joint vocabulary. We extended the initial\nvocabulary, adding to it unseen items. We used\nthis augmented vocabulary to ﬁne-tune the MASS\nmodel and run all the UNMT training experiments.\n3.2\nData Post-processing\nWe ﬁxed the quotes to be the same as in the source\nsentences (German-style). We also applied a re-\ncaser using Moses (Koehn et al., 2006) to convert\nthe translations to mixed case.\n3.3\nTraining\nUnsupervised SMT.\nAs mentioned before, we\nused fastText (Bojanowski et al., 2017) to build 300\ndimensional embeddings on the available monolin-\ngual data. We build BWEs with VecMap (Artetxe\net al., 2018a) using identical words as the seed\ndictionary and restricting the vocabulary to the\nmost frequent 200K, 400K and 400K 1-, 2- and\n3-grams respectively. We used monoses (Artetxe\net al., 2018b) as the USMT pipeline but used the\navailable validation data for parameter tuning and\nran 4 iterative reﬁnement steps.\nMASS. We use a Transformer, which consists of\n6-layer encoder and 6-layer decoder with 1024 em-\nbedding/hidden size, 4096 feed-forward network\nsize and 8 attention heads. We pretrain MASS on\nDe monolingual data, using Adam (Kingma and\nBa, 2015) optimizer with inverse square root learn-\ning rate scheduling and a learning rate of 10−4. We\nused a per-GPU batch size of 32. We trained the\nmodel for approximately 2 weeks on 8 NVIDIA\nGTX 1080 Ti 11 GB GPUs. The rest of the hyper-\nparameters follows the original MASS paper. We\nﬁne-tune MASS on both De and Hsb using the same\nsetup, but on 4 GPUs of the same type. Fine-tuning\nwas performed for 2 days.\nUnsupervised NMT. For unsupervised NMT, we\nfurther train the ﬁne-tuned MASS using online\nbacktranslation. We use 4 GPUs to train each one\nof our UNMT models. We report BLEU using\nSacreBLEU (Post, 2018)7 on the provided test set.\nUnsupervised NMT + Pseudo-parallel MT. We\ntrain our UNMT systems using a pseudo-parallel su-\npervised translation loss, in addition to the online\nbacktranslation objective. We found out that aug-\n7BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+ver-\nsion.1.4.13\nmenting UNMT systems with pseudo-parallel data\nobtained by USMT leads to major improvements in\ntranslation quality, as previous work has showed\n(Artetxe et al., 2018b; Stojanovski et al., 2019).\n4\nResults\nThe results of our systems on the test set provided\nduring development are presented in Table 1. Our\nUSMT model (#1) performs competitively, but is\nlargely outperformed by the UNMT baseline (#2).\nThese results are interesting considering that both\nsystems are trained using small amounts of mono-\nlingual Hsb data. We believe that the performance\nof the UNMT model is largely due to the MASS\nﬁne-tuning scheme which allowed us to obtain a\nstrong pretrained model for both languages. We\nalso observe (#3) that mixing greedy decoding and\nsampling during backtranslation is beneﬁcial com-\npared to always using greedy decoding (#2), espe-\ncially for De →Hsb which improved by 1.0 BLEU.\nHowever, it is likely that sampling is useful only\nif the model is of reasonable quality. We note that\nthe adapter-augmented model (#4) is worse than\nthe UNMT baseline.\nAfter these initial experiments, we use the USMT\nmodel (#1) to backtranslate all Hsb monolingual\ndata and 7M De sentences. This pseudo-parallel\ndata is leveraged to ﬁne-tune our UNMT models\nalongside online backtranslation. This approach,\ndenoted as model #5, improves the UNMT baseline\n(#3) by more than 5.5 BLEU for De→Hsb and\n4.5 BLEU for Hsb→De. The curriculum learning\napproach (#6) yields a small improvement of 0.6\nBLEU for Hsb→De. Unfortunately, the curricu-\nlum learning model ran without the use of sampling.\nWe later train the model with sampling (#6*) and\nobtain slight improvements in both directions.\nUsing NMT backtranslations in an ofﬂine man-\nner (#7) provides for a large improvement in the\nHsb→De direction, obtaining 33.2 BLEU. Further\ntraining our high scoring model #7 on USMT back-\ntranslations, depicted as model #9, degrades per-\nformance on Hsb→De. This might indicate that\nUSMT backtranslations alone are not very impor-\ntant for high performance, but simply adding any\nkind of pseudo-parallel data during training.\nThe adapter-augmented model with USMT back-\ntranslations (#8) manages to close the gap to the\nbaseline model. Comparing #5 and #8, we can see\nthat the model with adapters is worse by 0.9 BLEU\non De→Hsb, but better by 0.4 on Hsb→De. Due\nto time constraints, we train #4 and #8 in parallel\nand #8 is not ﬁne-tuned from #4. Overall, adapters\nare a promising research direction as they lead to\nfaster MASS ﬁne-tuning and comparable perfor-\nmance.\nWe observe considerable improvements using\nBPE-Dropout. As noted before, we oversample the\nparallel and Hsb monolingual data and apply BPE-\nDropout only on Hsb. We use this data to ﬁne-tune\nsome of our already trained models, speciﬁcally #5\nand #7 which results in models #10 and #11, re-\nspectively. This approach improves the Hsb→De\ndirection by up to 1.5 BLEU and up to 1.0 BLEU\nfor De→Hsb. System #11 proved to be our best\nsingle system in both translation directions. We\nhypothesize that using BPE-Dropout while simulta-\nneously oversampling the data provides for a data\naugmentation effect. In future work, it would be\ninteresting to decouple these two steps and measure\ntheir effect separately.\nEnsembling further boosts performance. En-\nsemble #12 is used for De→Hsb and #13 for\nHsb→De. We note that while computing ensem-\nble BLEU scores during development, we did not\nﬁx the issue with German-style quotes. This re-\nsulted in ensemble #13 obtaining better scores on\nHsb→De. We later ﬁx the quotes issue and ﬁnd\nout that ensemble #12 is better on both translation\ndirections and is the best system overall.\n5\nConclusion\nIn this paper, we present the LMU Munich sys-\ntem for the WMT 2020 unsupervised shared task\nfor translation between German and Upper Sorbian.\nOur system is a combination of an SMT and an NMT\nmodel trained in an unsupervised way. The UNMT\nmodel is trained by ﬁne-tuning a MASS model, ac-\ncording to the recently proposed RE-LM approach.\nThe experiments show that the MASS ﬁne-tuning\ntechnique is efﬁcient even if little monolingual data\nis available for one language and results in a strong\nUNMT model. We also show that using pseudo-\nparallel data from USMT and UNMT backtransla-\ntions improves performance considerably. Further-\nmore, we show that oversampling the low-resource\nUpper Sorbian and applying BPE-Dropout, which\ncan effectively be seen as data augmentation, re-\nsults in further improvements. Adapters in MASS\nﬁne-tuning provided for a balance between per-\nformance and computational efﬁciency. Finally,\nsmaller but noticeable gains are obtained from us-\ning curriculum learning and sampling during de-\ncoding in backtranslation.\nAcknowledgments\nThis work was supported by the European Re-\nsearch Council (ERC) under the European Union’s\nHorizon 2020 research and innovation programme\n(grant agreement No. 640550) and by the German\nResearch Foundation (DFG; grant FR 2829/4-1).\nWe would like to thank Jindˇrich Libovick´y for fruit-\nful discussions regarding the use of BPE-Dropout\nas a data augmentation technique.\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics, pages\n789–798.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018b.\nUnsupervised statistical machine transla-\ntion. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3632–3642.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018c. Unsupervised neural ma-\nchine translation.\nIn International Conference on\nLearning Representations.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015.\nNeural machine translation by jointly\nlearning to align and translate. In International Con-\nference on Learning Representations.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable\nadaptation for neural machine translation. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing and the International\nJoint Conference on Natural Language Processing,\npages 1538–1548.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching Word Vectors with\nSubword Information. Transactions of the Associa-\ntion for Computational Linguistics, pages 135–146.\nAlexandra Chronopoulou,\nDario Stojanovski,\nand\nAlexander Fraser. 2020.\nReusing a Pretrained\nLanguage Model on Languages with Limited cor-\npora for Unsupervised NMT.\narXiv preprint\narXiv:2009.07610.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 4171–4186.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n489–500.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron\nCourville, and Yoshua Bengio. 2013.\nAn em-\npirical investigation of catastrophic forgetting in\ngradient-based neural networks.\narXiv preprint\narXiv:1312.6211.\nFrancisco Guzm´an, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nFLORES evaluation datasets for low-resource ma-\nchine translation:\nNepali–English and Sinhala–\nEnglish. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\nand the International Joint Conference on Natural\nLanguage Processing, pages 6100–6113.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the International Conference on\nMachine Learning.\nDiederick P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nTom Kocmi and Ondˇrej Bojar. 2017. Curriculum learn-\ning and minibatch bucketing in neural machine trans-\nlation. In Proceedings of the International Confer-\nence Recent Advances in Natural Language Process-\ning, pages 379–386.\nPhilipp\nKoehn,\nMarcello\nFederico,\nWade\nShen,\nNicola Bertoldi, Ondrej Bojar, Chris Callison-Burch,\nBrooke Cowan, Chris Dyer, Hieu Hoang, Richard\nZens, et al. 2006. Open source toolkit for statisti-\ncal machine translation: Factored translation models\nand confusion network decoding. In Final Report of\nthe 2006 JHU Summer Workshop.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nIn Advances\nin Neural Information Processing Systems, page\n7057–7067.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5039–5049.\nEmmanouil Antonios Platanios, Otilia Stretcu, Graham\nNeubig, Barnabas Poczos, and Tom Mitchell. 2019.\nCompetence-based curriculum learning for neural\nmachine translation. In Proceedings of the Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1162–1172.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores.\nIn Proceedings of the Conference on Ma-\nchine Translation:\nResearch Papers, pages 186–\n191.\nIvan Provilkov, Dmitrii Emelianenko, and Elena Voita.\n2020. BPE-Dropout: Simple and effective subword\nregularization. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics,\npages 1882–1892.\nSebastian Ruder and Barbara Plank. 2017. Learning to\nselect data for transfer learning with Bayesian op-\ntimization.\nIn Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing,\npages 372–382.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics, pages 86–96.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b.\nNeural machine translation of rare words\nwith subword units. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1715–1725.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019.\nMASS: Masked Sequence to Se-\nquence pre-training for language generation. In Pro-\nceedings of the International Conference on Ma-\nchine learning, pages 5926–5936.\nDario Stojanovski, Viktor Hangya, Matthias Huck, and\nAlexander Fraser. 2019. The LMU Munich unsuper-\nvised machine translation system for WMT19. In\nProceedings of the Fourth Conference on Machine\nTranslation (Volume 2: Shared Task Papers, Day 1),\npages 393–399.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, page 5998–6008.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008.\nExtracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the International Confer-\nence on Machine Learning, pages 1096–1103.\nWei Wang, Ye Tian, Jiquan Ngiam, Yinfei Yang, Isaac\nCaswell, and Zarana Parekh. 2020. Learning a multi-\ndomain curriculum for neural machine translation.\nIn Proceedings of the Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7711–\n7723.\nXuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul Mc-\nNamee, Marine Carpuat, and Kevin Duh. 2019. Cur-\nriculum learning for domain adaptation in neural ma-\nchine translation. In Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 1903–1915.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-10-25",
  "updated": "2020-10-25"
}