{
  "id": "http://arxiv.org/abs/2010.13529v2",
  "title": "Lyapunov-Based Reinforcement Learning State Estimator",
  "authors": [
    "Liang Hu",
    "Chengwei Wu",
    "Wei Pan"
  ],
  "abstract": "In this paper, we consider the state estimation problem for nonlinear\nstochastic discrete-time systems. We combine Lyapunov's method in control\ntheory and deep reinforcement learning to design the state estimator. We\ntheoretically prove the convergence of the bounded estimate error solely using\nthe data simulated from the model. An actor-critic reinforcement learning\nalgorithm is proposed to learn the state estimator approximated by a deep\nneural network. The convergence of the algorithm is analysed. The proposed\nLyapunov-based reinforcement learning state estimator is compared with a number\nof existing nonlinear filtering methods through Monte Carlo simulations,\nshowing its advantage in terms of estimate convergence even under some system\nuncertainties such as covariance shift in system noise and randomly missing\nmeasurements. To the best of our knowledge, this is the first reinforcement\nlearning based nonlinear state estimator with bounded estimate error\nperformance guarantee.",
  "text": "1\nLyapunov-Based Reinforcement Learning\nState Estimator\nLiang Hu1, Chengwei Wu2, and Wei Pan3\nAbstract—In this paper, we consider the state estimation prob-\nlem for nonlinear discrete-time stochastic systems. We combine\nLyapunov’s method in control theory and deep reinforcement\nlearning to design the state estimator. We theoretically prove the\nconvergence of the bounded estimate error solely using the data\nsimulated from the model. An actor-critic reinforcement learning\nalgorithm is proposed to learn the state estimator approximated\nby a deep neural network. The convergence of the algorithm is\nanalysed. The proposed Lyapunov-based reinforcement learning\nstate estimator is compared with a number of existing nonlinear\nﬁltering methods through Monte Carlo simulations, showing its\nadvantage in estimating convergence even under some system\nuncertainties such as covariance shift in system noise and\nrandomly missing measurements. This is the ﬁrst reinforcement\nlearning-based nonlinear state estimator with bounded estimate\nerror performance guarantee to the best of our knowledge.\nIndex Terms—Nonlinear ﬁltering, deep reinforcement learning,\nLyapunov stability.\nI. INTRODUCTION\nState estimation, inferring unknown states using noisy\nmeasurements and underlying model of the dynamic system,\nhas found its important application in different areas ranging\nfrom control engineering, robotics, tracking and navigation to\nmachine learning [1]–[4]. For linear stochastic systems with\nGaussian noise, the renowned Kalman ﬁlter [5] is proved to\ngive the optimal estimate with elegant convergence properties\nin estimate error covariance. For nonlinear stochastic systems,\nsince the probability distribution of the state does not preserve\nthe property of Gaussian, a closed-form estimator like Kalman\nﬁlter generally does not exist. As a result, many nonlinear\nstate estimation methods based on different approximation\ntechniques have been proposed, among which the extended\nKalman ﬁlter (EKF), unscented Kalman ﬁlter (UKF) [6] and\nparticle ﬁlter (PF) [7] have been most widely used. For\nnonlinear systems with Gaussian noise, the EKF and UKF\napproximate the system state by a Gaussian distribution using\nthe linearisation technique and the deterministic sampling\nmethod. For nonlinear non-Gaussian systems, the PF uses\nsequential Monte Carlo methods to approximate distribution\nof the state by a ﬁnite number of particles.\nA fundamental problem of state estimation is how to design a\nstate estimator such that the estimation error can be guaranteed\nto be bounded. This problem is challenging and scarcely\naddressed only in certain scenarios under some assumptions\n1Liang Hu is with the School of Computer Science and Electronic\nEngineering, University of Essex, UK. Email: l.hu@essex.ac.uk.\n2Chengwei Wu is with the School of Astronautics, Harbin Institute of Tech-\nnology, China. He is also with the Department of Cognitive Robotics, Delft Uni-\nversity of Technology, Netherlands. Email: c.wu-1@tudelft.nl.\n3Wei Pan is with the Department of Cognitive Robotics, Delft University\nof Technology, Netherlands. Email: wei.pan@tudelft.nl.\n[8]–[10]. In [8], it is proved that if the linearisation error is\nnegligible, the local asymptotic stability of the EKF can be\nensured. In [9], the estimate error of the discrete-time EKF\nremains bounded only if the initial estimate error is sufﬁciently\nsmall and system noise intensity is small. In [10], similar\nresults can also be found for the UKF. Nonetheless, it is hard\nto reduce the initial estimate errors and linearisation errors in\nthe deployment of the EKF and UKF, making these convergence\nconditions less applicable. For the PF, though it is theoretically\nproved that the estimate performance converges asymptotically\nto the optimal estimates as the number of particles goes to\ninﬁnity in [11], only a limited particle can be used in practice\ndue to the expensive online computation as the number of\nparticle increases. Unfortunately, there are not any guarantees\non estimation performance for the PF with limited particles.\nBy summarising the EKF, UKF and PF observations, it is\nexpected that an ideal state estimator can estimate the state with\nbounded estimate error performance guarantee for nonlinear\nnon-Gaussian systems. Moreover, the performance guarantee\nshould not rely on conditions of the initial estimate error,\nthe system noise level, or the degree of nonlinearity of the\nunderlying dynamic system. In this paper, we are interested in\nseeking a reinforcement learning (RL) based method to design\nsuch a state estimator.\nRL was ﬁrst applied to the state estimation problem in\n[12], where impressive estimation performance was shown.\nHowever, the theoretical guarantee on the convergence of\nbounded estimate error was unavailable, thus making it less\napplicable for practical applications [13]. More recently, an RL\nbased Kalman ﬁlter design method was proposed in [14], in\nwhich the bounded estimate error can be guaranteed by using\nﬁnite samples for stable linear systems. Another closely related\nresearch is (deep) neural network based state estimation [15]–\n[17] where neural networks are used to learn system models.\nRecurrent neural networks (RNNs) for state estimation has\nbeen explored in [15], [16], and DNNs has been introduced\nfor Kalman ﬁltering in [17]. Inspired by these works, we will\ncombine deep learning and reinforcement learning, a.k.a., deep\nreinforcement learning (DRL), to design a state estimator in\nwhich the ﬁlter gain function will be learned using a deep\nneural network (DNN) from the sequence of estimate errors.\nOnce the estimator is trained well ofﬂine, it can be deployed\nonline and supposed to be efﬁcient given the advance in DNN\nmicroprocessor for online applications. The key questions are\nhow to prove the bounded estimate error guarantee in an RL\nsetup and design an efﬁcient learning algorithm with such a\nguarantee.\nMotivated by [8]–[10], we plan to prove the stability of\nthe estimate error dynamics; after that, the bounded estimate\nerror can be guaranteed. In particular, we make no assumptions\narXiv:2010.13529v2  [cs.LG]  7 Jan 2021\n2\ntypically used in these results, such as initial estimate error,\nlinearisation errors or assumptions on model nonlinearities,\netc. Similar to [12], the mathematical model can be seen as\na simulator, and the training of estimator is solely from data\nsimulated using the mathematical model. Unfortunately, the\ndata-based stability analysis of closed-loop systems in a model-\nfree RL manner is still an open problem [13], [18]. Typically,\nLyapunov’s method in control theory is widely used to analyse\nthe stability of dynamical systems. In [19], the stability of a\ndeterministic nonlinear system was analysed using Lyapunov’s\nmethod, assuming that the discount of the inﬁnite-horizon\ncost is sufﬁciently close to 1. However, such an assumption\nmakes it difﬁcult to guarantee the learned policy’s optimality\nwithout introducing certain extra assumptions [20]–[22]. As a\nbasic tool in control theory, the construction/learning of the\nLyapunov function is not trivial [23], [24]. In [25], the RL\nagent controls the switch between designed controllers using\nLyapunov domain knowledge so that any policy is safe and\nreliable. [26] proposes a straightforward approach to construct\nthe Lyapunov functions for nonlinear systems using DNNs. In\n[27], [28], a learning-based approach for constructing Lyapunov\nneural networks to ensure stability was proposed based on\nthe assumption that the learned model is a Gaussian process,\nLipschitz continuous and on discretised points in the subset of\nlow-dimensional state space. Only until recently, the asymptotic\nstability in model-free RL is given for robotic control tasks\n[29]. In [30], [31], the stability of a system with a combination\nof a classic baseline controller and an RL controller is proved\nfor autonomous surface vehicles with collisions.\nIn summary, we will combine Lyapunov’s method in\ncontrol theory and deep reinforcement learning to design state\nestimators for nonlinear stochastic discrete-time systems with\nbounded estimate error guarantee. The theoretical result is\nobtained by solely using the data simulated from the model.\nIn our method, the estimator is trained/learned ofﬂine and then\ndeployed directly using a DNN. Moreover, our state estimator\nis shown to be robust to system uncertainties, i.e., unknown\nmeasurement noise, missing measurement and non-Gaussian\nnoise, which makes our method more applicable. The main\ncontribution of the paper has twofold:\n1) For the ﬁrst time, a deep reinforcement learning method\nhas been employed for nonlinear state estimator design;\n2) The bounded estimate error can be theoretically guar-\nanteed by solely using data regardless of the degree of\nmodel nonlinearities and noise distribution.\nThis is the ﬁrst reinforcement learning-based nonlinear state\nestimator design method with bounded estimate error guarantee\nto the best of our knowledge.\nThe rest of the paper is organised as follows. In Section II,\nthe state estimation problem of nonlinear stochastic discrete-\ntime systems is formulated. In Section III, the theoretical result\non bounded estimate error guarantee is proved. In Section IV,\nthe learning algorithm is derived and the algorithm convergence\nis analysed. In Section V, our method is compared with EKF,\nUKF and PF in simulations. Conclusion is given in Section\nVI.\nNotation: The notation used here is fairly standard except\nwhere otherwise stated. Z+ denotes the set of non-negative\nintegrals. Rn and Rn×m denote, respectively, the n dimensional\nEuclidean space and the set of all n × m real matrices.\nA⊤represents the transpose of A, and E{x} stands for the\nexpectation of the stochastic variable x. x ∼N(m, N) with\nm ∈Rn and N ∈Rn×n denotes the probability function of\nthe random variable x follows a Gaussian distribution with m\nand N as the expectation and covariance, respectively. ∥x∥\ndenotes 2-norm of the vector x, i.e., ∥x∥= x⊤x.\nII. PROBLEM FORMULATION\nIn this paper, we consider the state estimation problem for\nthe following nonlinear discrete-time stochastic systems:\nxk+1 = f(xk) + wk,\nyk = g(xk) + vk,\n(1)\nwhere xk ∈Rn, yk ∈Rm are the state and measurement,\nrespectively, and wk and vk are stationary stochastic noise.\nThe nonlinear functions f(·) and g(·) and the probability\ndistributions of process noise wk and measurement noise vk are\nassumed to be all known, and the noise may be non-Gaussian.\nA. State estimation\nTo estimate the state of system (1), the state estimator is\ntypically designed in the following form:\nˆxk+1 = f(ˆxk) + a(ˆxk)ek+1\n(2)\nwhere ˆxk is the estimate of state xk, a(·) is a linear/nonlinear\nfunction that can be calculated using various approximation\nmethods and the measurement prediction error is given as\nfollows:\nek+1 = yk+1 −g(f(ˆxk))\n(3)\nThe form of the state estimator in (2) is standard and\nwidely used in some existing estimation algorithms, such as\nthe Kalman ﬁlter (KF) and extended Kalman ﬁlter (EKF). In\nthe EKF, the state estimator of (1) is given as follows:\nˆxk+1 = f(ˆxk) + Kkek+1\n(4)\nwhere Kk is the estimator gain at time instant k that is\ncalculated using partial derivatives of the f and g at ˆxk. As\na result, the estimator gain Kk is actually a function of ˆxk.\nSimilarly, in the unscented Kalman ﬁlter (UKF), a(ˆxk) in (2)\nis instantiated using deterministic sampling methods. As for\nthe particle ﬁlter (PF), even though no estimator gain is used\nexplicitly, the importance weights of particles could be viewed\nas the function of a(ˆxk) in the sampling form. In this paper, our\nhigh-level plan is to approximate a(·) as a generic nonlinear\nfunction, i.e., a deep neural network, which can be learned\nfrom estimate errors data xk −ˆxk over time.\nRemark 1. This paper will focus on the state estimation\nproblem while two other closely related problems need to\nbe remarked, i.e., state prediction and smoothing. In the state-\nprediction, measurement up to the current time instant is\nused to predict the state in the future; in the state-smoothing,\nmeasurement up to the current time instant is used to interpolate\nthe state in the past. Our method proposed can be easily\nadapted to state prediction and smoothing problems as well.\n3\nDeﬁnition 1.\n[9] The estimate error ˜xk ≜xk −ˆxk is said\nto be exponentially bounded in mean square if ∃η > 0 and\n0 < ϕ < 1, such that\nE[∥˜xk∥2] ≤ηE[∥˜x0∥2]ϕk + p,\n(5)\nholds at all the time instants k ≥0, where p is a positive\nconstant number.\nIn this paper, we aim to learn the state estimator policy\nfunction a(·) in the estimator (2) using deep reinforcement\nlearning such that the estimate error of the estimator (2) is\nguaranteed to converge exponentially to a positive bound in the\nmean square, as deﬁned in Deﬁnition 1. In the following, we\nwill introduce the background on reinforcement learning and\nshow how to analyse the stability using Lypapunov’s method.\nIII. REINFORCEMENT LEARNING AND DATA-BASED\nLYAPUNOV STABILITY ANALYSIS\nIn this section, the ﬁltering error dynamics is described\nas a Markov decision process. Then, some preliminaries of\nreinforcement learning are presented. Next, a theorem is\nproposed to prove the boundness of estimate error.\nA. Markov Decision Process\nThe estimate error dynamics of the state estimator design\n(2) can be described by a Markov decision process (MDP),\nwhich is deﬁned as a tuple < S, A, P, C, γ >. Here, S\nis the state space, A is the action space, P is the transition\nprobability distribution, C is the cost1, and γ ∈[0, 1) is the\ndiscount factor. Then we have\n˜xk+1 ∼P (˜xk+1|˜xk, ak) , ∀k ∈Z+,\n(6)\nwhere ˜xk and Ck are the state and cost at time instant k and\n˜xk ∈S. An action a(·) at the state ˜x(·) is sampled from the\npolicy π(a(·)|˜x(·)). The standard state estimator (2) is naturally\na special case of (6).\nB. Reinforcement learning\nIn this paper, the objective is to ﬁnd an optimal policy to\nminimise the expected accumulated cost as a value function:\nVπ (˜xk) =\n∞\nX\nk\nX\nak\nπ (ak|xk)\nX\n˜xk+1\nPk+1|k\n\u0000Ck + γVπ(˜xk+1)\n\u0001\n,\n(7)\nwhere Pk+1|k = P (˜xk+1 |˜xk, ak ) is the transition probability\nof the estimate error ˜xk, Ck = C(˜xk, ak) is the cost function\nwhere we are interested in the quadratic form Ck = ˜x⊤\nk ˜xk in\nthis paper, γ ∈[0, 1) is a constant discount factor, and π (ak|˜xk)\nis a policy to be learned. In RL, the policy (nonlinear ﬁlter\ngain function in this paper) π (ak|˜xk) is typically a Gaussian\ndistribution:\nπ (a|˜x) = N (a (˜x) , σ) ,\n(8)\nfrom which an action ak ∈U at the state ˜xk ∈S is sampled\n[32]. During the inference, the mean value a (˜x) is applied.\n1We will use cost instead of reward in this paper which is often used in\ncontrol literature. Maximisation in RL setup will be minimisation instead.\nDuring the training process, a Q-function Qπ (˜xk, ak) (i.e.,\nthe action-value function) is practically minimised. Qπ (˜xk, ak)\nis given as\nQπ (˜xk, ak) = Ck + γE˜xk+1 [Vπ(˜xk+1)] ,\n(9)\nwhere E˜xk+1 [·] = P\n˜xk+1 Pk+1|k [·] is an expectation operator\nover the distribution of ˜xk+1.\nSoft actor-critic (SAC) algorithm is one of the state-of-the-art\noff-policy actor-critic RL algorithms [33]. In SAC, an entropy\nitem is added to the Q-function as a regulariser, with which\nthe exploration performance becomes adjustable. Based on (9),\nthe Q-function with the entropy item is described as\nQπ (˜xk, ak) =Ck + γE˜xk+1 [Vπ(˜xk+1)\n−αH (π (ak+1|˜xk+1))] ,\n(10)\nwhere H (π (ak|˜xk)) = −P\nak π (ak|˜xk) ln (π (ak|˜xk)) =\n−Eπ [ln (π (ak|˜xk))] is the entropy of the policy, and α is\na temperature parameter. It is deﬁned to determine the relative\nimportance of the entropy term [33].\nThus, the algorithm is to solve the following optimisation\nproblem.\nπ∗= arg min\nπ∈Π\n\u0000Ck + γE˜xk+1 [Vπ(˜xk+1)\n−αH (π (ak+1|˜xk+1))]\n\u0001\n,\n(11)\nwhere Π is the policy set.\nAn optimal policy π∗(a|˜x) = N (a∗(˜x) , σ∗) can be\nobtained by solving (11). Here, σ∗is close to 0, which further\nimplies the optimal policy a∗converges to a deterministic mean\nvalue. Once such a policy is obtained, it can be deployed to\nthe target system. For a∗, it can be parameterised as a DNN\nand learned using stochastic gradient descent algorithms.\nTraining/learning process will repeatedly execute policy\nevaluation and policy improvement. In the policy evaluation, the\nQ-value in (10) is computed by applying a Bellman operation\nQπ (xk, al,k) = T πQπ (xk, al,k) where\nT πQπ (xk, al,k) = Ck + γExk+1 [Eπ [Qπ (xk+1, ak+1)]] ,\n(12)\nwhere Qπ (xk, al,k) = Qπ (xk, ak) + α ln (π (ak|xk)).\nIn the policy improvement, the policy is updated by\nπnew = arg min\nπ′∈Π DKL\n \nπ′ (·|xk)\n\r\r\re\n−1\nα Qπold(xk,·)\nZπold\n!\n,\n(13)\nwhere πold denotes the policy from the last update, Qπold is\nthe Q-value of πold, DKL denotes the Kullback-Leibler (KL)\ndivergence, and Zπold is a normalisation factor. The objective\ncan be transformed into\nπ∗= arg min\nπ∈Π Eπ\nh\nα ln (π (ak|xk)) + Q (xk, ak)\ni\n.\n(14)\nMore details can be found in [33].\n4\nC. Data-based Stability Analysis\nThe convergence of bounded estimate error ˜xk is essentially\nequivalent to show the stability of (6). In this paper, we are\ninterested in establishing the stability theorem by only using\nsamples {˜xk+1, ˜xk, ak}, ∀k. The most useful and general\napproach for studying a dynamical system’s stability is the\nLyapunov method [34]–[36]. In Lyapunov method, a suitable\n“energy-like” Lyapunov function L(˜xk) (for succinctness L(k)\nis used in the following context) is selected. Its derivative along\nthe system trajectories is ensured to be negative semi-deﬁnite,\ni.e., L(k + 1) −L(k) ≤0 for all time instants and states, so\nthat the state goes in the direction of decreasing the value of\nLyapunov function and eventually converges to the origin or a\nsub-level set of the Lyapunov function. In this subsection, such\na function is introduced to analyse the stability of the estimate\nerror dynamical system (6). Unlike the model-based stability\nanalysis literature, we will learn a Lyapunov function instead of\nan explicit expression regardless of the degree of nonlinearity\nof the system and the time-consuming human expert design.\nThe Lyapunov candidate L(k) can be chosen as the Q-function\nQπ(˜xk, ak) [25]–[28]. It is well-known that the Q-function\nQπ(˜xk, ak) is related to the cost Ck. Thus, there always exist\na positive function\nL(k) = Ck + δk = Qπ(˜xk, ak)\n(15)\nwhere Qπ(˜xk, ak) can be parameterised by a DNN. Here,\nδk > 0 should be satisﬁed due to the property of the Q-\nfunction, and it should be also non-increasing. Once the trace\nof the covariance of estimate error converges, the cost Ck and\nQ-function both converge as well, as a result δk will converge\nto a constant number. We ﬁrst prove the stability theorem by\nexploiting the function L(k) and the characteristic of δk. The\ndetails of the learning algorithm will be discussed in Section\nIV.\nWe need the following assumption and lemma for the\nproof.\nAssumption 1. A Markov chain induced by a policy π is\nergodic with a unique distribution probability qπ(˜xk) with\nqπ(˜xk) = limk→∞P(˜xk | ρ, π, k), where ρ denotes the\ndistribution of starting states.\nRemark 2. The veriﬁcation of ergodicity is, in general, an\nopen question and also in the long history of ergodic theory.\nMany systems have proved to be ergodic in physics, statistic\nmechanics, economics [37], [38]. The study of ergodicity of\nvarious systems and its veriﬁcation composes a signiﬁcant\nbranch of mathematics. If the transition probability is known\nfor all states, the validation is possible but requires an extensive\nresource of computation power to enumerate through the state\nspace. The existence of the stationary state distribution is\ngenerally assumed to hold in the RL literature [39]–[41]. In\nthis paper, based on the ergodicity assumption, we focus on\nanalysing the stability of such systems and developing an\nalgorithm to ﬁnd the ﬁlter gain.\nBefore proving the main theorem, we also need the following\nLemma\nLemma 1. [42] (Lebesgue’s Dominated convergence theorem)\nSuppose fn : R →[−∞, +∞] (Lebesgue) measurable func-\ntions such that the point-wise limit f(x) = limn→∞f(x) exists.\nAssume there is an integrable function g : R →[−∞, +∞]\nwith |fn(x)| ≤g(x) for each x ∈R, then f is integrable (in\nthe Lebesgue sense) and\nlim\nn→∞\nZ\nR\nfndµ =\nZ\nR\nlim\nn→∞fndµ =\nZ\nR\nfdµ.\nBased on Assumption 1 and Lemma 1, we prove our main\ntheorem in the following.\nTheorem 1. If there exist a function L(k) ≥0, and constants\nα1 ≥0, α2 > 0, and β ≥0 such that the following inequalities\nhold\nα1Ck ≤L(k) ≤α2Ck,\nE˜xk∼µπ\n\u0002\nE˜xk+1∼Pπ [L(k + 1)] −L(k)\n\u0003\n≤−βE˜xk∼µπ\n\b\n∥˜xk∥2\t\n+ δk, ∀k ∈Z+.\n(16)\nThen the estimate error ˜xk is guaranteed to be exponentially\nbounded in mean square, i.e.,\nE˜xk∼µπ\nh\n∥˜xk∥2i\n≤σkE˜x(0)∼µπ\nh\n∥˜x0∥2i\n+ p,\n(17)\nwhere\nµπ(˜xk) ≜lim\nN→∞\n1\nN\nN\nX\nk=0\nP (˜xk | ρ, π, k)\nis the estimate error distribution, and p = Pk−1\nι=0 σk−ι−1δι,\nσ ∈(0, 1).\nProof. The existence of the sampling distribution µπ(˜xk) is\nguaranteed by the existence of qπ(˜xk). Since the sequence\n{P(˜xk|ρ, π, k), k ∈Z+} converges to qπ(˜xk) as k approaches\n∞, then by the Abelian theorem that if a sequence or function\nbehaves regularly, then some average of it behaves regularly. We\nhave the sequence\nn\n1\nN\nPN\nk=0 P(˜xk | ρ, π, k), N ∈Z+\no\nalso\nconverges and µπ(˜xk) = qπ(˜xk). Then, (16) can be rewritten\nas\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk|ρ, π, k)\n\u0000EPπ(˜xk+1|s)L(k + 1) −L(k)\n\u0001\nd˜xk\n≤−βE˜xk∼µπ∥˜xk∥2 + δk\nThe left-hand-side of the above inequality can be derived as\n5\nfollows:\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk | ρ, π, k)\n\u0012Z\nS\nPπ (˜xk+1|˜xk) L(k + 1)d˜xk+1 −L(k)\n\u0013\nd˜xk\n=\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk | ρ, π, k)\nZ\nS\nPπ (˜xk+1|˜xk) L(k + 1)d˜xk+1d˜xk\n−\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk | ρ, π, k)L(k)d˜xk\n=\nZ\nS\nlim\nN→∞\n1\nN\nZ\nS\nN\nX\nk=0\nP(˜xk | ρ, π, k)Pπ (˜xk+1|˜xk) L(k + 1)d˜xkd˜xk+1\n−\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk | ρ, π, k)L(k)d˜xk\n=\nZ\nS\nlim\nN→∞\n1\nN L(k + 1)d˜xk+1\nZ\nS\nN\nX\nk=0\nP(˜xk | ρ, π, k)Pπ (˜xk+1|˜xk) d˜xk\n−\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk | ρ, π, k)L(k)d˜xk\n=\nZ\nS\nlim\nN→∞\n1\nN\nZ\nS\nN\nX\nk=0\nP(˜xk+1 | ρ, π, k + 1)L(k + 1)d˜xk+1\n−\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk | ρ, π, k)L(k)d˜xk\n(18)\nSince\nL(k)\n≤\nα2Ck\nand\nP(˜xk|ρ, π, k)\n≤\n1,\nthen\nP(˜xk|ρ, π, k)L(k)\n≤\nα2Ck.\nBesides,\nthe\nsequence\nn\n1\nN\nPN\nk=0 P(˜xk|ρ, π, k)L(k)\no\nconverges\npoint-wise\nto\nthe function qπ(˜xk)L(k). According to the Lebesgue’s\ndominated convergence theorem (Lemma 1), it follows from\n(18) that :\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk | ρ, π, k)\n\u0012Z\nS\nPπ (˜xk+1|˜xk) L(k + 1)d˜xk+1 −L(k)\n\u0013\nd˜xk\n=\nZ\nS\nlim\nN→∞\n1\nN\nZ\nS\nN\nX\nk=0\nP(˜xk+1 | ρ, π, k + 1)L(k + 1)d˜xk+1\n−\nZ\nS\nlim\nN→∞\n1\nN\nN\nX\nk=0\nP(˜xk | ρ, π, k)L(k)d˜xk\n= lim\nN→∞\n1\nN\n N+1\nX\nk=1\nEP (˜xk|ρ,π,k)L(k) −\nN\nX\nk=0\nEP (˜xk|ρ,π,k)L(k)\n!\n= lim\nN→∞\n1\nN\nN\nX\nk=0\n\u0000EP (˜xk+1|ρ,π,k+1)L(k + 1) −EP (˜xk|ρ,π,k)L(k)\n\u0001\n.\n(19)\nIt further follows from (16) and (19) that\nEP(˜xk+1|ρ,π,k+1)L(k + 1) −EP(˜xk|ρ,π,k)L(k)\n≤−βE˜xk∼µπ\n\b\n∥˜xk∥2\t\n+ δk\n(20)\nGiven α2 and β, there always exists a scalar σ such that the\nfollowing equation holds\n\u0012 1\nσ −1\n\u0013\nα2 −β\nσ = 0.\n(21)\nUsing (20) and (21), the following inequality can be derived\n1\nσι+1 EP(˜xι+1|ρ,π,ι+1)L(ι + 1) −1\nσι EP(˜xι|ρ,π,ι)L(ι)\n=\n1\nσι+1\n\u0000EP(˜xι+1|ρ,π,ι+1)L(ι + 1) −EP(˜xι|ρ,π,ι)L(ι)\n\u0001\n+ 1\nσι\n\u0012 1\nσ −1\n\u0013\nEP(˜xι|ρ,π,ι)L(ι)\n≤1\nσι\n\u0012\n−β\nσ + ( 1\nσ −1)α2\n\u0013\nE˜xk∼µπ\n\b\n∥˜xk∥2\t\n+\nδι\nσι+1 ,\n∀ι ≥0,\nwhich implies\n1\nσι+1 EP(˜xι+1|ρ,π,ι+1)L(ι + 1) −1\nσι EP(˜xι|ρ,π,ι)L(ι)\n≤\nδι\nσι+1 .\n(22)\nTo sum the above inequality from ι = 0, 1, . . . , k −1 yields\n1\nσk EP(˜xk|ρ,π,k)L(k) −EP(˜x0|ρ,π,0)L(0)\n≤\nk−1\nX\nι=0\nδι\nσι+1 ,\nwhich implies\nE˜xk∼µπ\nh\n∥˜xk∥2i\n≤σkE˜x0∼µπ\nh\n∥˜x0∥2i\n+ p.\nwhere the scalar constant p is a upper bound of the sequence\n{Pk−1\nt=0 σk−t−1δt, k ≥0}. Since σ ∈(0, 1) and δk is non-\nincreasing, we can conclude that the upper bound p exists.\nThus, the estimate error is exponentially bounded in mean\nsquare, which completes the proof.\nIV. REINFORCEMENT LEARNING FILTER DESIGN\nIn this section, we will discuss the reinforcement learning\nalgorithm design and implementation. We will design the\nalgorithm based on the actor-critic RL algorithms which are\nwidely used in continuous control tasks [32]. In actor-critic RL,\ntypically DNNs are used to approximate the “critic” and the\n“actor”. In the following, we will introduce how to incorporate\nconvergence conditions derived in Subsection III-C into such\nalgorithm architectures. Then the convergence of the algorithm\nwill be analysed.\nA. Deep neural networks approximation\nDNNs approximation are used due to the continuous state\nand action spaces in this paper. The DNNs are constructed by\nfully connected multiple layer perceptrons (MLP), in which\nthe rectiﬁed linear unit (ReLU) nonlinearities are chosen as the\nactivation functions [43]. The ReLU nonlinearities are deﬁned\nas ρ (z) = max {z, 0} when z is a scalar. Given a vector\nz = [z1, . . . , ,zn]⊤∈Rn, then ρ (z) = [ρ (z1) , . . . ,ρ (zn)]⊤.\nAn example of a MLP with two hidden layers is described as\nMLP2\nw (z) = w2\nh\nρ\n\u0010\nw1\nh\nρ\n\u0010\nw0\n\u0014\nz\n1\n\u0015 \u0011\n,1\ni⊤\u0011⊤\n,1\ni⊤\n,\n(23)\n6\nwhere\n\u0002\nz⊤, 1\n\u0003⊤is a vector composed of z and a constant bias\n1, the superscript “2” denotes the total number of hidden layers,\nthe subscript “w” denotes the parameter set to be trained in a\nMLP with w = {w0, w1, w2}, and w0, w1, and w2 are weight\nmatrices with appropriate dimensions.\nIf there is a set of inputs z = {z1, . . . , zL} for the MLP\nin (23) with z1, . . ., zL denoting vector signals, we have\nMLP2\nw (z) = MLP2\nw\n\u0000 \u0002\nz⊤\n1 , . . . , z⊤\nL\n\u0003⊤\u0001\n.\n(24)\nBesides, MLP2\nw (z1, z2) = MLP2\nw\n\u0000 \u0002\nz⊤\n1 , z⊤\n2\n\u0003⊤\u0001\nfor two vector\ninputs z1 and z2. If z1 = {z11, . . . , z1L} is a set of vectors,\nMLP2\nw (z1, z2) = MLP2\nw\n\u0000 \u0002\nz⊤\n11, . . . , z⊤\n1L, z⊤\n2\n\u0003⊤\u0001\n.\nIn this paper, the constructed MLPs are used to approximate\nthe “critic” Qπ(˜xk, ak) and the “actor” π(ak|˜xk). We respec-\ntively use θ and φ to parameterise Q(˜xk, ak) and π(ak|˜xk), i.e.,\nQθ(˜xk, ak) and πφ(ak|˜xk). As discussed in Section III-C, the\nQ-function Q(˜xk, ak) is regarded as a Lyapunov candidate L(k)\nin our paper. Namely, the “critic” is the Lyapunov function. In\nthe following context, we replace Q(˜xk, ak) with L(k), and\nQθ(˜xk, ak) with Lθ(k). The direct output of the constructed\nMLP may not satisfy the requirements of a Lyapunov function\nL(k), for example, L(k) > 0, ∀k ≥0, so it is necessary to\nmodify the representation of MLP. Following (23) and (24),\nthe Lyapunov function approximation Lθ(k) is chosen as\nLθ(k) =\n\u0010\nMLPK1\nθ\n(˜xk)\n\u00112\n,\n(25)\nwhere θ = {θ0, . . . , θK1} with θi denoting the weight matrices\nof proper dimensions, 0 ≤i ≤K1. The DNN for Lθ is\nillustrated in Fig. 1.\nThe ﬁlter gain ak is also approximated using a MLP. The\napproximated ﬁlter gain of ak with a parameter set φ is\naφ = MLPK2\nφ (˜xk) .\n(26)\nThe illustration of aφ is given in Figure 1. In the learning\nsetup, there are two outputs for the MLP in (26). One is\nthe control law aφ, the other one is σφ that is the standard\ndeviation (SD) of the exploration noise [33]. According to (8),\nthe parameterised policy πφ in our learning is\nπφ = N\n\u0000aφ (˜x) , σ2\nφ\n\u0001\n.\n(27)\nB. Lyapunov reinforcement learning ﬁlter (LRLF)\nThe actor-critic RL training process is depicted in Fig. 2.\nIn the training process, the dynamic system (1) and the state\nestimator to be trained (2) repeatedly run to collect the data,\nwhich is restored as the replay memory M. After M is\ncollected, the policy evaluation and improvement are executed\nby randomly sampling a batch of data in M. Then, the\nimproved policy πφ(ak|˜xk) is applied to the state estimator to\ngenerate data until Lθ(k) converges.\nIn the learning stage, the policy is obtained by repeatedly\nexecuting the policy evaluation and policy improvement. For\nthe policy evaluation, it starts from any function L : S×A →R\nunder a ﬁxed policy π, and repeatedly applies a Bellman backup\noperator T π, which is deﬁned as\nT πLπ(k) = Ck + γE˜xk+1 [Eπ [Lπ(k + 1)]] .\n(28)\nAt each policy evaluation step, the policy πφ(ak|˜xk) should\nminimise the following Bellman residual equation\nJL(θ) = E(˜xk,ak∼M)\n\u001a1\n2 (Lθ(k) −Ltarget)2\n\u001b\n,\nwhere (˜xk, ak ∼M) represents the operation that randomly\ntakes (˜xk, ak) from the memory M, and\nLtarget = Ck + γE˜xk+1 [Eπ [L¯θ(k + 1) + α ln(πφ)]] ,\nwith ¯θ being the target network parameter.\nWe can obtain the following by using stochastic gradient\ndescent:\n∇θJL(θ) =\nX ∇θLθ\n|B|\n(Lθ(k) −Ltarget) ,\nwhere |B| denotes the batch size.\nAt the policy improvement stage, the improved policy should\nguarantee the Lyapunov inequality in Theorem 1 holds. Thus,\nthe policy is updated as\nπnew = arg min\nπ′∈Π DKL\n \nπ′(·|˜xk)∥e\n−1\nα Lπold (˜xk, ·)\nZπold(˜xk)\n!\ns.t. Lθ(k + 1) −L(k) ≤−β Tr(˜xk˜x⊤\nk ) + δk,\n(29)\nwhere Π is the policy set, πold is the last updated policy, Lπold\nis the action value function of πold, DKL means the Kullback-\nLeibler divergence, and Zπold is a partition function which is\nintroduced to normalise the distribution.\nRemark 3. Different from the SAC algorithm, the Lyapunov\nconstraint, that is, Lθ(k + 1) −L(k) ≤−β Tr(˜xk˜x⊤\nk ) + δk\nis considered when the policy improvement step is executed.\nIn this way, we can guarantee that the estimate error always\nconverges to a positive constant in mean square, which is\nproved in Theorem 1.\nThen we can rewrite (29) as\nπnew = arg min\nπ∈Π E [α ln(π(ak|˜xk)) + L(k)]\ns.t. Lθ(k + 1) −L(k) ≤−β Tr(˜xk˜x⊤\nk ) + δk.\n(30)\nFor the optimisation of (30), the Lagrangian multiplier can\nbe introduced to deal with the constraint. Thus, (30) can be\nfurther described as\nπnew = arg min\nπ∈Π E [α ln(π(ak|˜xk)) + L(k)]\n+ λ\n\u0000Lθ(k + 1) −L(k) + β Tr(˜xk˜x⊤\nk ) −δk\n\u0001\n,\n(31)\nwhere λ is a Lagrangian multiplier.\nBased on the setup of RL, the policy improvement in (31) is\nconverted into ﬁnding π∗by minimising the following function\nJπ(φ) = E(˜xk,ak∼M) {α ln(πφ) + Lθ(k)} ,\nwhose gradient in terms of φ is derived as\n∇φJπ(φ) =\nX (α∇ak ln πφ + ∇akLθ(k)) ∇φaφ + α∇φ ln πφ\n|B|\n.\nFor the temperature α, it is updated by minimising the\nfollowing function\nJα = Eπ {−α ln π(ak|˜xk) −αH} ,\n7\nInput layer\nHidden layers\nOutput layer\nCritic Neural Network\nState \nestimation error\n State estimation\nInput layer\nHidden layers\nOutput layer\nActor Neural Network\nMeasurement\nLθ\nπφ\nˆx\ny\nˆx −x\nFig. 1: Approximation of Lθ and πφ using MLP\nwhere H is a target entropy.\nFor the Lagrangian multiplier λ, it is learned by maximising\nJ (λ) = E\n\u0002\nLθ(k + 1) −L(k) + β Tr(˜xk˜x⊤\nk ) −δk\n\u0003\n.\nOur algorithm is implemented based on SAC algorithm [33],\nin which ιL, ιπ, ια, and ιλ are the positive learning rates, and\nτ > 0 is a constant scalar. The optimal parameters for the DNN\nin (25) and (26) will be learned, and the ﬁlter gain policy will\nbe approximated by πφ∗from which action will be sampled.\nDuring inference, the mean value of πφ∗will be deployed\nsince the policy is often assumed to be Gaussian distributed in\nSAC [33].\nThe inference procedure is illustrated in Fig. 3. The learned\npolicy is deployed to tune the error yk+1 −g(f(ˆxk)) in the\nestimator. We sample the measurement output signal yk+1 from\nthe real system. Then, the estimator starts to estimate states\nfor the real system.\nRemark 4. Bayesian nonlinear ﬁltering methods such as the\nEKF, UKF, PF adjust the estimator gains or sampling weights\nvia online computation. The proposed LRLF is trained ofﬂine,\nand the ﬁlter gain is approximated by a DNN which will\nbe deployed directly for online applications. In this paper,\nlike other methods, the training needs full knowledge of the\nmathematical model, i.e., (1). It should also be noted that\nthe ﬁlter (2) is trained by only using the samples simulated\nfrom (1) instead of any other assumptions on the model. In\nother words, the mathematical model is a simulator, and our\ntraining is performed in a model-free manner. In Figs. 2 and\n3, the statistical information of the system’s noise such as the\ncovariance is not directly used by the LRLF, which is different\nfrom the EKF, UKF and PF where such statistical information\nis used explicitly.\nC. Algorithm convergence analysis\nNext, a lemma is given to show that the policy evaluation\ncan guarantee the action value function to converge. Since the\nproof is standard, it is omitted here. Readers can refer to [33]\nfor more details.\nLemma 2. (Policy evaluation) Consider the backup operator\nT π in (28) and deﬁne Lt+1(k) ≜T πLt(k). The sequence\nAlgorithm 1 Lyapunov-Based Reinforcement Learning Filter\nAlgorithm (LRLF)\n1: Set the initial parameters θ for the Lyapunov function\nLθ, φ for the ﬁltering policy πφ, λ for the Lagrangian\nmultiplier, α for the temperature parameter, and the replay\nmemory M\n2: Set the target parameter ¯θ as ¯θ ←θ\n3: while Training do\n4:\nfor each data collection step do\n5:\nChoose ak using πθ(ak|˜xk)\n6:\nRun the system (1) and the ﬁlter system (2) and\ncollect data ˜xk\n7:\nM ←M ∪˜xk\n8:\nend for\n9:\nfor each gradient step do\n10:\nθ ←θ −ιL∇θJL(θ),\n11:\nφ ←φ −ιπ∇φJπ(φ)\n12:\nα ←α −ια∇αJα(α)\n13:\nλ ←λ −ιλ∇λJλ(λ)\n14:\nφ¯θ ←τθ + (1 −τ)φ¯θ,\n15:\nend for\n16: end while\n17: Output optimal parameters θ∗, φ∗, λ∗, and α∗\nLt+1(k) can converge to a soft value Lπ of the policy π as\nthe iteration t →∞.\nFor policy improvement, a lemma is given to show that the\nupdated policy is better than the last one.\nLemma 3. (Policy improvement) Considering the last updated\npolicy πold and the new policy πnew to be obtained from (30),\nLπnew(k) ≤Lπold(k) holds, ∀˜xk ∈S and ∀ak ∈A.\nProof. According to (30), we can obtain\nEπnew [α ln(πnew(ak|˜xk)) + Lπold(k)] ≤\nEπold [α ln(πold(ak|˜xk)) + Lπold(k)] ,\nwhich implies\nE [Lπold + α ln(πnew(ak|˜xk))] ≤Vπold(˜xk).\n(32)\n8\nSimulator\nActor (filter) \nneural network\nCritic neural \nnetwork\nRun the simulator using the latest learned filter and collect data\nUpdate critic and actor neural networks using historical data\nReplay memory\nRandomly sample a batch of \nstate estimation error data\nExploration noise\nstate estimation error\nReward\nx −˜x\nξ\n˜x\ny and\nFig. 2: Ofﬂine training process of LRLF\nThen, the following inequality holds\nLπold(k) = Ck + γE˜xk+1 [Vπold(˜xk)]\n≥Ck + γE˜xk+1 [Eπnew [\nLπold(k + 1)\n+α ln(πnew(ak+1|˜xk+1))]]\n...\n≥Lπnew(k),\nwhere (32) is repeatedly used and hence omitted. It completes\nthe proof.\nNext, a theorem is derived to show that the convergence of\nAlgorithm 1 can be guaranteed.\nTheorem 2. Deﬁne πi (i = 1, 2, . . . , ∞) as the policy obtained\nat the i-th policy improvement step, starting from any policy\nπ0 ∈Π, where Π is the policy set, then πi will converge to an\noptimal policy π⋆, ensuring Lπ⋆(k) converges to its minimal\nvalue as k →∞.\nProof. Based on Lemma 3, we know that the policy can achieve\na better estimate performance after each policy improvement,\nthat is, Lπi(k) ≤Lπi−1(k). By repeatedly executing the policy\nevaluation and policy improvement, a policy πi can converge\nto π⋆as i →∞. Thus, Lπ⋆(k) can converge based on the\nconclusion in Lemma 2.\nV. SIMULATION\nThe experiment setup for the LRLF is a three-stage procedure.\nFirst, a number of N estimation policies are trained for\ndifferent initial conditions and noise is sampled from a known\ndistribution. During training, the simulator will generate sample\ntrajectories {xi\nk, yi\nk, i = 1, . . . , I, k = 1, . . . , K}, where I, K\nare the number of training trajectories and that of the trajectory\nlength respectively. Second, the estimation performance of each\nDRL-based state estimator (2) will be evaluated by running M\nMonte Carlo simulations, again for different initial conditions\nand noise sampled from a known distribution. Finally, the state\nestimator with the lowest trace of estimate error covariance\nduring inference (unless diverged) will be deployed online and\nused for comparison with other nonlinear ﬁltering algorithms.\nWe ﬁrst consider a free-pendulum tracking example widely\nused as a benchmark for nonlinear state estimation [4], [12],\n[44]. The pendulum has unity mass of 1 kg and length of 1\nm. The discrete-time dynamics of the pendulum is given as\nfollows:\n\u0014x1,k+1\nx2,k+1\n\u0015\n=\n\u0014\nx1,k + x2,kδt\nx2,k −g sin(x1,k)δt\n\u0015\n+ wk\n(33)\nwhere x1,k = θk, x2,k = ωk are the angle and angle velocity of\nthe pendulum at time instant k, respectively, δt is the sampling\ntime and set as 0.1 second in the simulation. The process noise\nwk is Gaussian distributed as\nwk ∼N\n\u0012 \u00140\n0\n\u0015\n,\n\u0014 1\n3(δt)3q1\n1\n2(δt)2q1\n1\n2(δt)2q1\nδtq1\n\u0015 \u0013\n, q1 = 0.01\nThe measurement equation is given as:\nyk = sin(x1,k) + vk\n(34)\nwhere the measurement noise is also Gaussian distributed with\nvk ∼N(0, 0.01). Since the scalar measurement solely depends\non the angle (x1), the estimate of the latent state x2 has to be\nreconstructed using the cross-correlation information between\nthe angle and the angular velocity in the dynamics (33).\n9\nReal System\nf(ˆxk)\nMultiply\nUpdate \nˆxk\nˆxk+1\nyk+1\nwk\nvk+1\nEstimator\nyk+1 −g(f(ˆxk))\ng(f(ˆxk))\nxk+1 =\nf(xk) + wk\nyk+1 =\ng(xk+1) + vk+1\na(ˆxk)\nFig. 3: Estimator structure\nTo test if the estimate error of LRLF converges regardless of\nthe initial state and estimate, for each trajectory in the training,\nthe initial state were sampled from uniform distribution:\nθ(0) ∼U[−0.5π, 0.5π],\nω(0) ∼U[−0.5π, 0.5π],\n(35)\nand the initial estimate was sampled from uniform distribution:\nˆθ(0) ∼U[−0.25π + θ(0), 0.25π + θ(0)],\nˆθ(0) ∼U[−0.25π + ω(0), 0.25π + ω(0)].\n(36)\nWe compared LRLF with three other classic nonlinear\nBayesian estimation algorithms, the EKF, UKF, and PF (103\nand 104 particles respectively) [4]. The same initial state and\nstate estimate in (35) and (36) were used for all estimation\nalgorithms.\nIn the training, each trajectory has K = 100 data points\n(10 seconds simulation of (33)). We trained N = 10 policy\nnetworks and evaluated each network for M = 500 Monte\nCarlo simulations. The training details are give as follows:\nFor the LRLF, there are two networks: the policy network\nand the Lyapunov critic network. We use a fully-connected\nMLP with one hidden layer for the policy network, outputting\nthe mean and SD of a Gaussian distribution. As mentioned\nin section IV, it should be noted that the output of the\nLyapunov critic network is a square term, which is always\nTABLE I: Hyperparameters of LRLF\nHyperparameters\nPendulum\nVehicle\nTime horizon K\n100\n100\nMinibatch size\n256\n256\nActor learning rate\n1e-4\n1e-4\nCritic learning rate\n3e-4\n3e-4\nLyapunov learning rate\n3e-4\n3e-4\nTarget entropy\nNaN\nNaN\nSoft replacement(τ)\n0.005\n0.005\nDiscount(γ)\n0.995\n0.995\nα3\n0.1\n0.1\nStructure of aφ\n(32,16)\n(32,16)\nStructure of Lθ\n(64,32)\n(64,32)\nnon-negative. More speciﬁcally, we use a fully-connected\nMLP with one hidden layer and one output layer with\ndifferent units as in Table I, outputting the feature vector\nQθ (see Fig. 1). The Lyapunov critic function is obtained by\nLc(s, a) = Q⊤\nθ (s, a)Qθ(s, a). All the hidden layers use ReLu\nactivation function, and we adopt the same invertible squashing\nfunction technique as [33] to the output layer of the policy\nnetwork.\nThe proposed LRLF was evaluated for the following aspects:\n1) Algorithm convergence: does the proposed training algo-\nrithm converge with random initial states and estimate\ninitialisation?\n2) Estimate error convergence: does the estimate error\n10\n0\n20\n40\n60\n80\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 4: The value of Lagrange multiplier λ during the training\nof LAC policies. The Y-axis indicates the value of λ and the\nX-axis indicates the total time steps. The shadowed region\nshows the 1-SD conﬁdence interval over 10 randomly training\npolicies.\nvariance converge compared with other state estimation\nalgorithms?\n3) Performance comparison: how does the proposed LRLF\nperform compared with other state estimation algorithms\nunder various initial state conditions?\n4) Robustness to uncertainty: how do the trained estimator\nperform during inference when faced with uncertainties\nunseen during training, such as noise with covariance\nshift and randomly-occurring missing measurement?\nA. Algorithm convergence\nWe will validate the convergence guarantee by checking the\nvalues of Lagrange multipliers. When the Lyapunov constraint\nin (31) is satisﬁed, the parameter λ should continuously\ndecrease to zero. In Fig. 4, the value of λ during training is\ndemonstrated. In all training trials of the ten policy networks, λ\nconverges to zero eventually, which implies the state estimate’s\nconvergence guarantee.\nB. Estimation performance\nThe proposed LRLF is compared with the EKF, UKF and PF.\nFor each method, a number of 500 Monte Carlo tests were run\nwith random initial state given in (35) and (36). The average\nvalue and SD of estimate error ˜xk are illustrated in the left\ncolumn of Fig. 5 (see Figs. 5(a, d, g, j, m)) for EKF, UKF, PF\nand LRLF. These comparisons show that only the PF with a\nlarger number of particles (104) has comparable performance\nwith the LRLF.\nTo verify estimation performance of the LRLF for nonlinear\nsystems with non-Gaussian noise, we tested the LRLF over\nthree typical non-Gaussian distributions as shown in Fig. 6.\nSpeciﬁcally, the measurement noise vk of the pendulum model\nin (33)-(34) is assumed to take three kinds of non-Gaussian\nprobability distributions: 1) a Gaussian distribution N(0, 0.01)\ntruncated between the interval [0, 1]; 2) a uniform distribution\nat the interval [−0.3, 0.3]; or 3) an exponential distribution\nwith the PDF p(wk) =\n\u001a\n0.04 exp (−0.04wk)\nif wk ≥0\n0\nif wk < 0 .\nA number of 500 Monte Carlo simulations with random initial\nstate in (35) and (36) were test for each kind of measurement\nnoise. The average value and SD of estimate error ˜xk are\nillustrated in Fig. 7. It is found that the LRLF still generates\nstate estimate with bounded estimate errors under all the three\nkinds of non-Gaussian noises.\nC. Robustness of the estimator\nWe considered the robustness of state estimators for two sce-\nnarios. (1) Against larger measurement noise: in the inference,\nthe measurement noise variance increases from 0.01 to 0.1.\n(2) Against missing measurement: yk is sampled/contaminated\nrandomly with Bernoulli distribution, in which p(missing) =\np(not missing) = 0.5. This means half of the measurements\nare not received/set to zeros during inference.\n1) Larger measurement noise: Similarly, we run 500 Monte\nCarlo tests with random initial states and initial state estimates\ngiven in (35)-(36) for each state estimator. The average value\nand variance of estimate error ˜xk are illustrated in Figs.5 (b), (e),\n(h), (k), (n). From Figs.5 (b), (e), (h), it can be found that under\na large measurement noise with variance 0.1, the estimates of\nthe EKF, UKF and PF (103 particles) all diverge, while the\nestimate error of our proposed LRLF and PF (104 particles)\nincrease compared with those under noise with variance 0.01\nbut are both still bounded, as shown in Figs.5 (k), (n). The\nLRLF produces comparable estimation performance to the\ncomputationally expensive PF with 104 particles. It is worth\npointing out that our proposed state estimator, though trained\nunder noise with the variance of 0.01, still works well under a\nchanging noise level, showing its robustness to the covariance\nshift of system noise. From the left and middle column in\nFig. 5, it is found that the EKF, UKF and PF (103 particles)\nhave poor performance when measurement noise is large.\n2) Missing measurement: Missing measurement is a com-\nmon network-induced phenomenon and has been extensively\ninvestigated in state estimation for decades [45], [46]. In\nour experiment, the measurement are sampled/contaminated\nrandomly with a Bernoulli distribution, e.g., p(missing) =\np(no missing) = 0.5. The average value and variance of\nestimate error ˜xk are illustrated in Fig. 5 (c), (f), (i), (l), (o).\nIt is found that the EKF, UKF and PF with 103 particles all\ndiverge, LRLF has comparable performance as the PF with\n104 particles. These comparisons imply that LRLF is robust\nto randomly missing measurement.\nD. A linear system case study\nWe also consider the linear systems, to compare with the\nclassic Kalman ﬁlter which is known to be optimal for linear\nsystems with Gaussian noise. The example is to track a vehicle\nrunning at a constant but unknown speed and subject to random\nnoise [4]. The motion dynamics of the vehicle is given as\nfollows:\n\u0014x1,k+1\nx2,k+1\n\u0015\n=\n\u00141\n1\n0\n1\n\u0015 \u0014x1,k\nx2,k\n\u0015\n+\n\u00140\n1\n\u0015\nwk\n(37)\nand the measurement model is given as follows:\nyk =\n\u00021\n0\u0003\nxk + vk\n(38)\n11\n0\n2\n4\n6\n8\n10\ntime (sec)\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(a) EKF\n0\n2\n4\n6\n8\n10\ntime (sec)\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(b) EKF with noise variance R = 0.1\n0\n2\n4\n6\n8\n10\ntime (sec)\n15\n10\n5\n0\n5\n10\n15\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(c) EKF with measurement missing\n0\n2\n4\n6\n8\n10\ntime (sec)\n0.4\n0.2\n0.0\n0.2\n0.4\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(d) UKF\n0\n2\n4\n6\n8\n10\ntime (sec)\n3\n2\n1\n0\n1\n2\n3\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(e) UKF with noise variance R = 0.1\n0\n2\n4\n6\n8\n10\ntime (sec)\n10\n5\n0\n5\n10\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(f) UKF with measurement missing\n0\n2\n4\n6\n8\n10\ntime (sec)\n4\n2\n0\n2\n4\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(g) PF (103 particles)\n0\n2\n4\n6\n8\n10\ntime (sec)\n4\n2\n0\n2\n4\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(h) PF with noise variance R = 0.1\n0\n2\n4\n6\n8\n10\ntime (sec)\n3\n2\n1\n0\n1\n2\n3\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(i) PF with measurement missing\n0\n2\n4\n6\n8\n10\ntime (sec)\n0.4\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(j) PF (104 particles)\n0\n2\n4\n6\n8\n10\ntime (sec)\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(k) PF with noise variance R = 0.1\n0\n2\n4\n6\n8\n10\ntime (sec)\n0.4\n0.2\n0.0\n0.2\n0.4\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(l) PF with measurement missing\n0\n2\n4\n6\n8\n10\ntime (sec)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(m) LRLF\n0\n2\n4\n6\n8\n10\ntime (sec)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(n) LRLF with noise variance R = 0.1\n0\n2\n4\n6\n8\n10\ntime (sec)\n1.0\n0.5\n0.0\n0.5\n1.0\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(o) LRLF with measurement missing\nFig. 5: State estimate error of different methods under different conditions. The left column corresponds to those with simulation setup with\nthe measurement noise variance R = 0.01; the middle column corresponds to those with the increased measurement noise variance R = 0.1;\nthe right column corresponds to those with measurement missing happening with the probability 0.5. Solid line indicates the average estimate\nerror ˜x1,k (red) and ˜x2,k (blue) and shadowed region for the 1-SD conﬁdence interval. All results are obtained from 500 Monte Carlo runs.\n12\n0.4\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nthe value of noise\n0\n5\n10\n15\n20\n25\nthe probability density function\nExponential distribution\nTruncated Gaussian distribution\nUniform distribution\nFig. 6: The PDFs of the three kinds of non-Gaussian noise,\ne.g., the truncated Gaussian, the exponential and the uniform\ndistributions that we tested with the LRLF.\nwhere x1,k, x2,k are the distance and speed of the ve-\nhicle at time instant k, respectively. The Gaussian noise\nwk ∼N(0, 0.01) and vk ∼N(0, 0.02), and the initial\nstate and the initial state estimate are x0 =\n\u0014\n0\n10\n\u0015\n, ˆx0 ∼\nN\n\u0012 \u0014 0\n10\n\u0015\n,\n\u00140.02\n0\n0\n0.03\n\u0015 \u0013\n, respectively.\nWe use the same experimental setup as in the previous\nexample of tracking pendulum. The estimate error of the LRLF\nis obtained from 500 Monte Carlo simulations, as shown in\nFig. 8. The SD of state estimate error of the Kalman ﬁlter\nis plotted in Fig. 8 for comparison. It can be found that our\nproposed LRLF estimate both the distance and speed of the\nvehicle quite accurately, with slightly bigger estimate error\nthan that of the optimal Kalman ﬁlter, which is expected since\nno state estimators can perform better than the optimal Kalman\nﬁlter for linear stochastic systems with Gaussian noise.\nE. Airborne target tracking case study\nWe consider the scenario presented in [47], where an\nunmanned aerial vehicle equipped with a gimballed camera to\ntrack a ground vehicle manoeuvring on a road section. Assume\nthat the bearing-only camera platform is kept at the altitude\nzs = 100m above the origin of the local coordinate and the\nroad is ﬂat. The camera provides the azimuth angle (ζ) and\nelevation angle (η) to the target with respect to the camera\nplatform, as described by the following observation model:\nzk = h(xk) =\n\u0014\nζk\nηk\n\u0015\n=\n\u0014\narctan2(yk, xk)\narctan2(zs,\np\nx2\nk + y2\nk)\n\u0015\n+ vk (39)\nwhere xk =\n\u0002sx\nk, sy\nk, ˙sx\nk, ˙sy\nk\n\u0003T is the target vehicle’s state of\nposition and speed components in x and y direction. The sensor\nnoise vk is zero-mean Gaussian noise with the covariance\nR =\n\u0014\n8\n0\n0\n0.002\n\u0015\n.\nThe target vehicle dynamics is described by a white noise\nacceleration motion model [48]:\nxk+1 =\n\n\n1\n0\nT\n0\n0\n1\n0\nT\n0\n0\n1\n0\n0\n0\n0\n1\n\nxk +\n\n\n0.5T 2\n0\n0\n0.5T 2\nT\n0\n0\nT\n\nwk\n(40)\nwhere the sampling time T = 0.1s and the process noise is\nzero-mean Gaussian noise with the covariance Q =\n\u00141\n0\n0\n1\n\u0015\n,\nthe initial state and the initial state estimate are x0\n=\n\u000298\n0\n0\n10\u0003T , ˆx0 ∼N\n\u0012\n\n\n98\n0\n0\n10\n\n,\n\n\n25\n0\n0\n0\n0\n25\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n\u0013\n, re-\nspectively.\nWe use the same experimental setup as in the previous\nexample on tracking pendulum. The state estimate of the LRLF\nobtained from 500 Monte Carlo simulations is shown in Fig. 9.\nIt shows that the proposed LRLF can track the target vehicle’s\nstate quite well.\nVI. CONCLUSION AND DISCUSSION\nThis paper has combined Lyapunov’s method in control\ntheory and deep reinforcement learning to design state estimator\nfor discrete-time nonlinear stochastic systems. We theoretically\nprove the convergence of bounded estimate error solely using\nthe data in a model-free manner. In our approach, the ﬁlter\ngain is approximated by a deep neural network and trained\nofﬂine. During inference, the learned ﬁlter can be deployed\nefﬁciently without extensive online computations. Simulation\nresults show the superiority of our state estimator design method\nover existing nonlinear ﬁlters, in terms of estimate convergence\neven under some system uncertainties such as covariance shift\nin system noise and randomly missing measurements. As initial\nresearch developing the RL approach for state estimation, there\nare still quite a few issues that have not yet addressed in the\nwork. For example, How to quantify uncertainty of the state\nestimate, i.e., the covariance of state estimate error? What is the\nconvergence bound of state estimate error with ﬁnite samples?\nThey will be left as our future research directions.\nREFERENCES\n[1] B. D. Anderson and J. B. Moore, Optimal ﬁltering. Courier Corporation,\n2012.\n[2] S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics.\nMIT press\nCambridge, 2005.\n[3] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan, Estimation with applications\nto tracking and navigation: theory algorithms and software.\nJohn Wiley\n& Sons, 2004.\n[4] S. S¨arkk¨a, Bayesian ﬁltering and smoothing.\nCambridge University\nPress, 2013.\n[5] R. E. Kalman, “A new approach to linear ﬁltering and prediction\nproblems,” Transactions of the ASME – Journal of Basic Engineering,\nvol. 82, pp. 35–45, 1960.\n[6] S. J. Julier and J. K. Uhlmann, “Unscented ﬁltering and nonlinear\nestimation,” Proceedings of the IEEE, vol. 92, no. 3, pp. 401–422,\n2004.\n[7] A. Doucet, S. Godsill, and C. Andrieu, “On sequential monte carlo\nsampling methods for bayesian ﬁltering,” Statistics and Computing,\nvol. 10, no. 3, pp. 197–208, 2000.\n[8] M. Boutayeb, H. Rafaralahy, and M. Darouach, “Convergence analysis of\nthe extended kalman ﬁlter used as an observer for nonlinear deterministic\ndiscrete-time systems,” IEEE Transactions on Automatic Control, vol. 42,\nno. 4, pp. 581–586, 1997.\n[9] K. Reif, S. Gunther, E. Yaz, and R. Unbehauen, “Stochastic stability of the\ndiscrete-time extended kalman ﬁlter,” IEEE Transactions on Automatic\nControl, vol. 44, no. 4, pp. 714–728, 1999.\n[10] L. Li and Y. Xia, “Stochastic stability of the unscented kalman ﬁlter\nwith intermittent observations,” Automatica, vol. 48, no. 5, pp. 978–981,\n2012.\n13\n0\n2\n4\n6\n8\n10\ntime (sec)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(a) Truncated Gaussian distribution\n0\n2\n4\n6\n8\n10\ntime (sec)\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(b) Exponential distribution\n0\n2\n4\n6\n8\n10\ntime (sec)\n1.0\n0.5\n0.0\n0.5\n1.0\n (rad); \n (rad/sec)\nx1(k)\nx2(k)\n(c) uniform distribution\nFig. 7: State estimate error of the LRLF with different kinds of measurement noise shown in Fig. 6. Solid line indicates the\naverage estimate error ˜x1,k (red) and ˜x2,k (blue) and shadowed region for the 1-SD conﬁdence interval. The results are obtained\nfrom 500 Monte Carlo runs.\n0\n2\n4\n6\n8\n10\ntime (sec)\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\ndistance (m); speed (m/s)\nx1(k)\nx2(k)\nthe stand deviation of x1(k) by KF\nthe stand deviation of x2(k) by KF\nFig. 8: State estimate error of the LRLF vs the SD of state\nestimate of the Kalman ﬁlter. Solid line indicates the average\nestimate error ˜x1,k (red) and ˜x2,k (blue) and shadowed region\nfor the 1-SD conﬁdence interval of the LRLF. The results of\nLRLF are obtained with 500 Monte Carlo runs.\n0\n20\n40\n60\n80\n100\ntime (sec)\n0\n20\n40\n60\n80\n100\nPosition (m); Speed (m/sec)\nTrue and Estimated State\nx_1\nx_1\nx_2\nx_2\nx_3\nx_3\nx_4\nx_4\nFig. 9: The true state of target vehicle and state estimate using\nthe LRLF. Solid lines indicate true state and dashed lines the\nstate estimate and shadowed region for the 1-SD conﬁdence\ninterval of the LRLF. The results of LRLF are obtained with\n500 Monte Carlo runs.\n[11] D. Crisan and A. Doucet, “A survey of convergence results on parti-\ncle ﬁltering methods for practitioners,” IEEE Transactions on Signal\nProcessing, vol. 50, no. 3, pp. 736–746, 2002.\n[12] J. Morimoto and K. Doya, “Reinforcement learning state estimator,”\nNeural Computation, vol. 19, no. 3, pp. 730–756, 2007.\n[13] L. Bus¸oniu, T. de Bruin, D. Toli´c, J. Kober, and I. Palunko, “Reinforce-\nment learning for control: Performance, stability, and deep approximators,”\nAnnual Reviews in Control, 2018.\n[14] A. Tsiamis, N. Matni, and G. J. Pappas, “Sample complexity of kalman\nﬁltering for unknown systems,” arXiv preprint arXiv:1912.12309, 2019.\n[15] N. Yadaiah and G. Sowmya, “Neural network based state estimation of\ndynamical systems,” in The 2006 IEEE International Joint Conference\non Neural Network Proceedings.\nIEEE, 2006, pp. 1042–1049.\n[16] R. Wilson and L. Finkel, “A neural implementation of the kalman ﬁlter,”\nAdvances in neural information processing systems, vol. 22, pp. 2062–\n2070, 2009.\n[17] R. G. Krishnan, U. Shalit, and D. Sontag, “Deep kalman ﬁlters,” arXiv\npreprint arXiv:1511.05121, 2015.\n[18] D. Gorges, “Relations between model predictive control and reinforce-\nment learning,” IFAC-PapersOnLine, vol. 50, no. 1, pp. 4920–4928,\n2017.\n[19] R. Postoyan, L. Bus¸oniu, D. Neˇsi´c, and J. Daafouz, “Stability analysis\nof discrete-time inﬁnite-horizon optimal control with discounted cost,”\nIEEE Transactions on Automatic Control, vol. 62, no. 6, pp. 2736–2749,\n2017.\n[20] J. J. Murray, C. J. Cox, and R. E. Saeks, “The adaptive dynamic\nprogramming theorem,” in Stability and control of dynamical systems\nwith applications.\nSpringer, 2003, pp. 379–394.\n[21] M. Abu-Khalaf and F. L. Lewis, “Nearly optimal control laws for\nnonlinear systems with saturating actuators using a neural network hjb\napproach,” Automatica, vol. 41, no. 5, pp. 779–791, 2005.\n[22] Y. Jiang and Z.-P. Jiang, “Global adaptive dynamic programming for\ncontinuous-time nonlinear systems,” IEEE Transactions on Automatic\nControl, vol. 60, no. 11, pp. 2917–2929, 2015.\n[23] D. V. Prokhorov, “A Lyapunov machine for stability analysis of nonlinear\nsystems,” in Proceedings of 1994 IEEE International Conference on\nNeural Networks (ICNN’94), vol. 2.\nIEEE, 1994, pp. 1028–1031.\n[24] D. V. Prokhorov and L. A. Feldkamp, “Application of svm to Lyapunov\nfunction approximation,” in IJCNN’99. International Joint Conference\non Neural Networks. Proceedings (Cat. No. 99CH36339), vol. 1.\nIEEE,\n1999, pp. 383–387.\n[25] T. J. Perkins and A. G. Barto, “Lyapunov design for safe reinforcement\nlearning,” Journal of Machine Learning Research, vol. 3, no. Dec, pp.\n803–832, 2002.\n[26] V. Petridis and S. Petridis, “Construction of neural network based\nlyapunov functions,” in The 2006 IEEE International Joint Conference\non Neural Network Proceedings.\nIEEE, 2006, pp. 5059–5065.\n[27] S. M. Richards, F. Berkenkamp, and A. Krause, “The Lyapunov neural\nnetwork: Adaptive stability certiﬁcation for safe learning of dynamical\nsystems,” in Conference on Robot Learning, 2018, pp. 466–476.\n[28] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, “Safe model-\nbased reinforcement learning with stability guarantees,” in Advances in\nNeural Information Processing Systems, 2017, pp. 908–918.\n[29] M. Han, L. Zhang, J. Wang, and W. Pan, “Actor-critic reinforcement learn-\ning for control with stability guarantee,” arXiv preprint arXiv:2004.14288,\n2020.\n14\n[30] Q. Zhang, W. Pan, and V. Reppa, “Model-reference reinforcement\nlearning control of autonomous surface vehicles with uncertainties,” arXiv\npreprint arXiv:2003.13839, 2020.\n[31] ——, “Model-reference reinforcement learning for collision-free\ntracking control of autonomous surface vehicles,” arXiv preprint\narXiv:2008.07240, 2020.\n[32] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[33] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in Proceedings of the 35th International Conference on Machine\nLearning (ICML 2018), vol. 80, Stockholmsm¨assan, Stockholm Sweden,\nJul. 2018, pp. 1861–1870.\n[34] A. M. Lyapunov, The general problem of the stability of motion (in\nRussian).\nPhD Dissertation, Univ. Kharkov, 1892.\n[35] Y. Jiang and Z.-P. Jiang, “Computational adaptive optimal control for\ncontinuous-time linear systems with completely unknown dynamics,”\nAutomatica, vol. 48, no. 10, pp. 2699–2704, 2012.\n[36] F. L. Lewis, D. Vrabie, and V. L. Syrmos, Optimal control.\nJohn Wiley\n& Sons, 2012.\n[37] C. C. Moore, “Ergodic theorem, ergodic theory, and statistical mechanics,”\nProceedings of the National Academy of Sciences, vol. 112, no. 7, pp.\n1907–1911, 2015.\n[38] O. Peters, “The ergodicity problem in economics,” Nature Physics, vol. 15,\nno. 12, pp. 1216–1221, 2019.\n[39] R. S. Sutton, H. R. Maei, and C. Szepesv´ari, “A convergent o(n)\ntemporal-difference algorithm for off-policy learning with linear function\napproximation,” in Advances in neural information processing systems,\n2009, pp. 1609–1616.\n[40] J. Bhandari, D. Russo, and R. Singal, “A ﬁnite time analysis of temporal\ndifference learning with linear function approximation,” arXiv preprint\narXiv:1806.02450, 2018.\n[41] S. P. Meyn and R. L. Tweedie, Markov chains and stochastic stability.\nSpringer Science & Business Media, 2012.\n[42] H. L. Royden, Real analysis.\nKrishna Prakashan Media, 1968.\n[43] G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural\nnetworks for lvcsr using rectiﬁed linear units and dropout,” in Proceedings\nof 2013 IEEE International Conference on Acoustics, Speech and Signal\nProcessing, Vancouver, BC, Canada, May 2013.\n[44] M. P. Deisenroth, R. D. Turner, M. F. Huber, U. D. Hanebeck, and C. E.\nRasmussen, “Robust ﬁltering and smoothing with gaussian processes,”\nIEEE Transactions on Automatic Control, vol. 57, no. 7, pp. 1865–1871,\n2011.\n[45] B. Sinopoli, L. Schenato, M. Franceschetti, K. Poolla, M. I. Jordan, and\nS. S. Sastry, “Kalman ﬁltering with intermittent observations,” IEEE\nTransactions on Automatic Control, vol. 49, no. 9, pp. 1453–1464, 2004.\n[46] Z. Wang, B. Shen, and X. Liu, “H∞ﬁltering with randomly occurring\nsensor saturations and missing measurements,” Automatica, vol. 48, no. 3,\npp. 556–562, 2012.\n[47] C. Liu, B. Li, and W.-H. Chen, “Particle ﬁltering with soft state constraints\nfor target tracking,” IEEE Transactions on Aerospace and Electronic\nSystems, vol. 55, no. 6, pp. 3492–3504, 2019.\n[48] X. R. Li and V. P. Jilkov, “Survey of maneuvering target tracking. part\ni. dynamic models,” IEEE Transactions on aerospace and electronic\nsystems, vol. 39, no. 4, pp. 1333–1364, 2003.\n",
  "categories": [
    "cs.LG",
    "cs.RO",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2020-10-26",
  "updated": "2021-01-07"
}