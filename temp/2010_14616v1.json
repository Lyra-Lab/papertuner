{
  "id": "http://arxiv.org/abs/2010.14616v1",
  "title": "Lineage Evolution Reinforcement Learning",
  "authors": [
    "Zeyu Zhang",
    "Guisheng Yin"
  ],
  "abstract": "We propose a general agent population learning system, and on this basis, we\npropose lineage evolution reinforcement learning algorithm. Lineage evolution\nreinforcement learning is a kind of derivative algorithm which accords with the\ngeneral agent population learning system. We take the agents in DQN and its\nrelated variants as the basic agents in the population, and add the selection,\nmutation and crossover modules in the genetic algorithm to the reinforcement\nlearning algorithm. In the process of agent evolution, we refer to the\ncharacteristics of natural genetic behavior, add lineage factor to ensure the\nretention of potential performance of agent, and comprehensively consider the\ncurrent performance and lineage value when evaluating the performance of agent.\nWithout changing the parameters of the original reinforcement learning\nalgorithm, lineage evolution reinforcement learning can optimize different\nreinforcement learning algorithms. Our experiments show that the idea of\nevolution with lineage improves the performance of original reinforcement\nlearning algorithm in some games in Atari 2600.",
  "text": "Lineage Evolution Reinforcement Learning\nZeyu Zhang, Guisheng Yin\nAbstract\nWe propose a general agent population learn-\ning system, and on this basis, we propose lin-\neage evolution reinforcement learning algo-\nrithm. Lineage evolution reinforcement learn-\ning is a kind of derivative algorithm which ac-\ncords with the general agent population learn-\ning system. We take the agents in DQN and its\nrelated variants as the basic agents in the pop-\nulation, and add the selection, mutation and\ncrossover modules in the genetic algorithm to\nthe reinforcement learning algorithm. In the\nprocess of agent evolution, we refer to the\ncharacteristics of natural genetic behavior, add\nlineage factor to ensure the retention of po-\ntential performance of agent, and comprehen-\nsively consider the current performance and\nlineage value when evaluating the performance\nof agent. Without changing the parameters of\nthe original reinforcement learning algorithm,\nlineage evolution reinforcement learning can\noptimize different reinforcement learning al-\ngorithms. Our experiments show that the idea\nof evolution with lineage improves the perfor-\nmance of original reinforcement learning algo-\nrithm in some games in Atari 2600.\n1\nINTRODUCTION\nDeep reinforcement learning has been widely concerned\nsince it was proposed. Deep Q-Networks (Mnih, 2013,\n2015) algorithm combines Q-learning with convolutional\nneural network, introduces the concept of deep reinforce-\nment learning, and achieves excellent results in Atari\ngames (Bellemare, 2013). Double DQN (Hasselt, 2016)\nsolves the overestimate problem in the original DQN by\nseparating the selection function and the evaluation func-\ntion of the action. Prioritized experience replay (Schaul,\n2015) improves the efﬁciency of sampling by marking\nthe priority of sample data. Dueling DQN (Wang, 2016)\nseparates the value of environment and action, and im-\nproves the accuracy of value evaluation of action. A3C\n(Mnih, 2016) integrates DQN with policy gradient, ab-\nsorbs the idea of asynchronous and multi-step bootstrap\ntargets (Sutton, 1988), adjusts the balance between vari-\nance and deviation, and improves the convergence speed\nof agent performance.\nNAF (Shixiang Gu, 2016) en-\nables agents to be used in continuous action space by us-\ning positive deﬁnite matrix to represent advantage func-\ntion.\nDistribution RL (Bellemare, 2017) changes the\nvalue evaluation object from average value to distribu-\ntion, avoiding the evaluation deviation caused by average\nvalue ﬁtting. Noisy DQN (Fortunato, 2017) adds noisy\nlinear layer to DQN algorithm to adjust the exploratory\nstrategy so that the agent can explore stably. Rainbow\n(Hessel, 2017) integrates six DQN extensions, includ-\ning DDQN, Prioritized replay, Dueling networks, Multi-\nstep learning, Distributional RL and Noisy Nets, which\nfurther improves the performance of the agent on Atari\n2600, and analyzes the impact of different components\non the performance. QR-DQN (Dabney, 2017) added\nthe concept of quantile regression to Distributional RL\nand proposed. By introducing risk neutral policy, IQN\n(Dabney, 2018) was proposed based on QR-DQN, and\nfurther improves the performance of quantile network.\nGenetic algorithm, like reinforcement learning algo-\nrithm, is also an algorithm that borrows from natural\nbiological characteristics. Genetic algorithm (Holland,\n1975) was ﬁrst proposed in 1975. This paper system-\natically expounded the theoretical basis of genetic algo-\nrithm. Then, Goldberg further improved the theory of\ngenetic algorithm (Goldberg, 1989), and laid the foun-\ndation of modern genetic algorithm research. In recent\nyears, the ideas of genetic algorithm, evolution strategy\nand neural network are gradually integrated. Kenneth\nproposed NEAT (Kenneth, 2002, 2009, 2012) by com-\nbining genetic algorithm with neural network, and fur-\narXiv:2010.14616v1  [cs.NE]  26 Sep 2020\nther optimized NEAT in 2009 and 2012.\nReferring to the intelligent evolution process of natural\norganisms, we believe that a single genetic algorithm\nor reinforcement learning algorithm cannot achieve the\nmost efﬁcient learning process. In order to improve the\nlearning ability of agents, we propose an agent popu-\nlation learning system, which combines the core ideas\nof neural network, reinforcement learning algorithm and\ngenetic algorithm. We try to integrate some mainstream\nartiﬁcial intelligence algorithms and propose a general\nagent design system.\nAccording to the agent population learning system,\nwe propose lineage evolution reinforcement learning\n(LERL). Lineage evolution reinforcement learning al-\ngorithm combines the core idea of deep reinforcement\nlearning algorithm and genetic algorithm. In the applica-\ntion of genetic algorithm, we use parameter perturbation\nand network crossover to replace the original mutation\noperation and crossover operation. Referring to the lin-\neage factor in nature, we integrate lineage factor and con-\ntemporary performance into the agent evaluation module\nto retain the potential performance value of the agent.\nThe value of lineage will be determined by the histori-\ncal performance of agents, and will be inherited in the\nprocess of evolution.\nThe idea of lineage evolution can be applied to differ-\nent reinforcement learning algorithms.\nIn this paper,\nwe choose the reinforcement learning algorithm (DQN,\nC51, Rainbow and IQN) provided by Dopamine (Castro,\n2018) framework as the research object, and use its base-\nline data as the reference data for experimental analysis.\nThe experimental results show that LERL is superior to\nthe original algorithm in terms of learning speed and per-\nformance.\n2\nBACKGROUND\nThis part will introduce the core idea and principle of\nreinforcement learning and genetic algorithm.\n2.1\nREINFORCEMENT LEARNING\nMarkov Decision Process. Markov Decision Process\n(MDP), which is a tuple (S; A; T; r; γ), is a kind of\nmathematical model to simulate agent strategy and re-\nward in the environment. In the tuple, S stands for the\nstate space, A stands for the action space, T is the transi-\ntion function, r is the reward function, and γ is a discount\nfactor. Its theoretical basis is Markov chain. Accord-\ning to the different degree of perception of the environ-\nment, there are some variants of Markov decision pro-\ncess, such as partial Markov decision process and con-\nstrained Markov decision process. Markov decision pro-\ncess has Markov property, that is, the state of the current\nmoment is only related to the state of the last moment\nand the action taken at the last moment, but not to the\nstate before the last moment. Its conditional probability\nformula is\np(si+1|si, ai, ...s0, a0) = p(si+1|si, ai).\n(1)\nDeep Q-Networks. Deep Q-Networks (DQN) is the ﬁrst\nalgorithm that combines Q learning with neural network.\nMost of the original Q-learning algorithms access the\nstate and corresponding rewards in the form of tables,\nwhich is difﬁcult to deal with the complex environment\nof state and action. Neural network just makes up for\nits shortcomings. In DQN, the selection of agent action\nis decided by neural network. When an agent makes a\ndecision, the current observation of the environment is\nthe input of the neural network, and the Q value of each\ndiscrete action is the output of the neural network. The\naction with the largest Q value is selected to execute. The\nloss function of training DQN is\n(Rt+1 + γt+1maxq¯θ(St+1, a′) −qθ(St, At))2.\n(2)\nDistributional RL. Distributional RL is a kind of rein-\nforcement learning algorithm which can ﬁt the distribu-\ntion of action value. In the original dqn, only one value is\noutput for each action, which represents the mean value\nof the action. Other attributes of the action value distri-\nbution are not considered too much. However, some val-\nues with large distribution gap may have the same mean\nvalue, which makes a lot of important information lost\nin value function ﬁtting. In order to solve this problem,\nthe output of distribution RL is no longer the traditional\nmean value, but the value distribution of actions. After\nsetting the upper and lower limits of all action values,\nthe network will output the value distribution of an ac-\ntion. In their paper, the most representative algorithm is\nC51, which outputs a 51 dimensional vector for each ac-\ntion, and each element represents the probability of its\ncorresponding value.\nRainbow. In Rainbow (Dopamine), researchers imple-\nmented the three most important modules in the original\nRainbow algorithm: n-step Berman update, priority ex-\nperience replay and distributional RL. Among them, pri-\noritized replay means that the algorithm deﬁnes the pri-\nority of training data according to absolute TD error. The\nalgorithm is more inclined to take out the samples with\nhigh priority when sampling, which greatly improves the\nefﬁciency of agent learning.\nImplicit Quantile Networks. Based on C51, Dabney\nproposed QR-DQN. QR-DQN takes quantile as ﬁtting\nobject, which improves the accuracy and robustness of\nvalue distribution evaluation. After that, Dabney added\nan extra neural network to the original QR-DQN to dy-\nnamically determine the ﬁtting accuracy, and proposed\nImplicit Quantile Networks (IQN), which has risk sensi-\ntive property.\n2.2\nGENETIC ALGORITHM\nThe idea of genetic algorithm comes from Darwin’s law\nof natural selection. Genetic algorithm encodes the solu-\ntion of the problem ﬁrst, and then optimizes the solution\nof binary encoding format through selection, mutation\nand crossover. Genetic algorithm keeps the individuals\nwith high ﬁtness through the ﬁtness evaluation function,\nand takes the retained individuals as the parents of the\nnew population to generate new solutions. Genetic algo-\nrithm mainly includes ﬁve parts: ﬁtness evaluation, se-\nlection operation, mutation operation, crossover opera-\ntion and judgment of termination conditions.\nFitness evaluation. Fitness evaluation is the basis of\nselection, crossover and mutation in genetic algorithm,\nwhich is used to measure the performance of an indi-\nvidual in the current generation. Generally, the ﬁtness\nfunction needs to be adjusted according to the applica-\ntion environment.\nSelection operation. After getting the ﬁtness evaluation\nresults of all individuals in the population, genetic algo-\nrithm will select the individuals with higher ﬁtness from\nthe current population according to certain rules as the\nparent samples of the next generation population. The\npurpose of this operation is to retain genes with high ﬁt-\nness in the evolution process.\nMutation operation. The mutation operation originates\nfrom the mutation of biological chromosomes in na-\nture.\nIn mutation operation, genetic algorithm deter-\nmines whether to perform mutation operation according\nto mutation probability, and randomly selects one or a\ngroup of basic bits for mutation.\nCrossover operation.\nThe object of traditional\ncrossover operation is two string binary codes. Follow-\ning the crossover and recombination of biological chro-\nmosomes, two binary codes are used to realize crossover\noperation.\nJudgment of termination conditions. The goal of ge-\nnetic algorithm is to ﬁnd the local optimal solution rather\nthan the global optimal solution in the high-dimensional\nsolution space. Therefore, it is difﬁcult for genetic al-\ngorithm to stop by ﬁnding the global optimal solution.\nThere are two common termination conditions. The ﬁrst\nis to limit the number of evolutions. The second is to\nterminate the algorithm when the performance reaches a\ncertain score or the performance ﬂuctuation is small.\n3\nAGENT POPULATION LEARNING\nSYSTEM\nAt present, a lot of artiﬁcial intelligence algorithms are\nbased on the speciﬁc problems and analyze the charac-\nteristics of the problems. For example, natural language\nprocessing focuses more on the structure of language,\nand image recognition focuses more on the processing\nof image features. If we analyze these tasks from a more\nabstract perspective, we can get a more general algorithm\ndesign pattern. On the one hand, a general algorithm\nsystem can reduce the limitations of algorithm applica-\ntion, on the other hand, it can improve the efﬁciency of\nalgorithm design when researchers deal with new tasks.\nThe inspiration of agent population learning system pro-\nposed in this paper comes from human perception, learn-\ning and evolution. In the general agent population learn-\ning system (GAPLS), we divide the architecture of agent\ninto three levels, from bottom to top, they are perception\nlayer, thinking layer and population layer.\nIn the perception layer, the agents mainly complete the\ntask of information processing. The agents use some al-\ngorithms directly deal with the external information ob-\ntained by a certain perception medium, abstract this in-\nformation, and transmit the information obtained from\nthe abstraction to the thinking level.\nThe function of\nperception layer corresponds to the function of percep-\ntion organ in human intelligence. The machine learn-\ning of this module focuses on improving the recognition\nability of agents to environmental information, aiming to\nextract higher-level features through complex and large-\nscale representation information.\nThe task of the thinking layer is to study and explore.\nIn many current algorithms, the concept of thinking is\nusually blurred or bound with the perception layer. In\nfact, there is a certain deviation from the way of think-\ning of human intelligence. Although human intelligence\nis essentially the transmission of neurotransmitters, per-\nception and decision-making are realized by different\nbrain regions.\nTherefore, we think that the thinking\nlayer should be treated as a single layer.\nAt present,\nmany algorithms can be regarded as thinking layer al-\ngorithms. The input of the thinking layer is a feature\ngroup transmitted from the perception layer, and its out-\nput is the decision-making of the agent. The task goal of\nthe thinking layer is to gradually improve the adaptabil-\nity of agents to the environment without changing the\ninput characteristics, and improve their performance in a\nspeciﬁc environment or task through existing data. The\nexploration of agents in the thinking layer is relatively\nRealization\nInteraction\nPopulation\nThinking\nPerception\nEvolution\nLearning and \nExploration\nInformation \nProcessing \nReinforcement Learning (Q-Learning, Hierarchical \nReinforcement Learning, Course Learning, etc.),\n Supervised Learning, Semi-Supervised Learning, etc.\nNeural Network (DNN, DBN, CNN, etc.), \nDecision Tree, Bayes Classifier, etc.\n1. Same Task (PSO, ACO, etc.)\n2. Different Tasks (Cooperation and \nCompetition of Multi-Agent)\n1. Multi Feature Collaborative Processing \n(Image, Sound, Temperature, Pressure)\n2. Imagination and Recollection  (On Model, \nRNN, etc.)\nGenetic Algorithm \n(Selection, Crossover, Mutation), \nAncestry, etc.\nFigure 1: General Agent Population Learning System\nstable.\nThe population layer is at the top of the whole agent pop-\nulation learning system, and it is also the layer with the\nmost exploratory and the least call times in the learn-\ning process of agent population. The population layer of\nagents corresponds to the population of human beings,\nboth of which take independent intelligent units as basic\ncomputing units. The most typical algorithm in the pop-\nulation layer of the same task is genetic algorithm, and\nthe algorithms of different tasks are mainly focused on\nthe ﬁeld of game theory. The goal of population level\nis to break the limitation of current learning, jump out\nof local optimal solution, and try to make breakthroughs\nto improve the adaptability of agents to the environment.\nIn addition, if the population layer adopts the elite strat-\negy, the excellent and rapid evolution of a single agent\ncan improve the average performance of the whole agent\npopulation in a limited time, and drive other agents to\nmake leaping progress.\nTask objectives at different layers are relatively indepen-\ndent. However, due to the difference of perception infor-\nmation, task characteristics and the deﬁnition of environ-\nment, we still need to supplement the interaction mod-\nules between different layers to improve the whole agent\npopulation learning system and ensure its universality.\nThe interaction between population layer and thinking\nlayer can be divided into the same task and different tasks\naccording to different task settings. To some extent, in-\nteraction patterns will affect the way of evolution.\nIn the interaction process of thinking layer and percep-\ntion layer, if the perception mode is not unique, the inter-\naction process may need to preprocess the feature groups\nacquired by different perceptions to ensure the unity of\ndimensions. In addition, some tasks involving prediction\nor timing can be supplemented with imagination and re-\ncall modules in the interaction process to improve the\npertinence of tasks.\nAccording to the design of agent population learning sys-\ntem, we give the corresponding algorithm ﬂow. In Algo-\nrithm 1, we deﬁne the structure of perceptions, agents\nand population.In the evolution operation, we keep the\nevaluation, selection, mutation and crossover in genetic\nalgorithm, and adopt the elite strategy in the agent selec-\ntion. In order to ensure the accuracy of the evaluation,\nwe add a potential performance factor in the evaluation\nprocess. Potential performance can be derived directly\nfrom past performance or predicted based on the map-\nping relationship between past environment and past per-\nformance, and updated in each generation.\nIn the general agent population learning algorithm, we\ntrain the mapping function of thinking layer and per-\nception layer independently according to the structure of\nagent population learning system. Taking the neural net-\nwork algorithm as an example, we divide the agent net-\nwork into two parts. The perception part can generate\nas many different features as possible through semi su-\npervised learning, while the thinking part uses another\ncompletely independent network for supervised learn-\ning. When the network of the thinking part is trained,\nthe network parameters of the perception part remain un-\nchanged. When the agent interacts with the environment,\nwe make the thinking part and the perception part work\ntogether. This training mode has the following advan-\ntages: 1) The perception function can be reused. 2) In\nline with the design concept of agent population learning\nsystem. 3) Training is more efﬁcient.\nIn the general agent population learning algorithm, if\nip is equal to 1, the agent’s thinking layer and percep-\ntion layer can be regarded as bundling training, which\nis consistent with the current DQN and its variant algo-\nrithms.Considering the different types of tasks, we take\nenvironment interaction as an option in the learning pro-\nAlgorithm 1: General Agent Population Learning\nInput: Data set D and task set T = {t0, ..., tNtask}\nOutput: Agent population E\n1 Initialize perceptions P(x; ω)\n2 Initialize agents A(P0, ..., PNperception; θ)\n3 Initialize agent populations Et = {At\n0, ..., At\nNagent} of\nthe same task\n4 Initialize agent population E = {E0, ..., ENtask}\n5 for generation g ∈{0, ..., Ngeneration} do\n6\nif g > 0 then\n7\nfor task t ∈T do\n8\nfor agent At\nk ∈Et do\n9\nCompute performance ρt,k\ng\n10\nΓt,k\ng\n←wρρt,k\ng\n+ wφφt,k\ng\n11\nend\n12\nDivide Et into E1\nt , E2\nt and E3\nt\n13\nGenerate Ec\nt and Em\nt from E1\nt\n14\nEt ←E1\nt ∪E2\nt ∪Ec\nt ∪Em\nt\n15\nfor agent At\nk ∈Et do\n16\nUpdate potential φt,k\ng\n17\nend\n18\nend\n19\nend\n20\nfor iteration ia ∈{1, ..., Ia} do\n21\nfor agent A′ ∈E0 ∪... ∪ENtask do\n22\nfor p ∈{0, ..., Nperception} do\n23\nfor iteration ip ∈{1, ..., Ip} do\n24\nUpdate ωA′,p by ML algorithm\n25\nend\n26\nend\n27\nUpdate θA′ by ML algorithm\n28\nend\n29\nif environment is interactive then\n30\nInteract with the environment\n31\nUpdate D\n32\nend\n33\nend\n34 end\ncess. In addition, under the condition of ensuring that\nthe agents of evolution operation are in the same gener-\nation, agents with the same task can save running time\nin parallel when they are learning and updating training\ndata.\n4\nLINEAGE EVOLUTION\nREINFORCEMENT LEARNING\nAccording to the general agent population learning sys-\ntem and the corresponding algorithm proposed above, we\nbring the current reinforcement learning algorithm into\nit and propose lineage evolution reinforcement learning\nalgorithm (LERL). LERL is a derivative of the above al-\ngorithm in a speciﬁc environment. In LERL, we train\nthe thinking and the perception of the agent together.\nIn addition to the combination of reinforcement learning\nalgorithm and genetic algorithm, LERL also introduces\nlineage factor to evaluate the potential performance of\nagents.\n4.1\nEVALUATION\nThe evaluation of agent performance is the ﬁrst step of\nagent evolution. The evaluation process occurs before\neach selection, mutation and crossover, and after the\nlearning of all agents of the previous generation. Agents\nperform evolution operations on a regular basis in the\nprocess of reinforcement learning. When the agent pop-\nulation is in the evolution generation, we evaluate the\nagent performance Γ. Because of the parallelism of the\nalgorithm, the time for each agent to reach the evolution-\nary operation is different, so we also set the evolution\nlock for the agent population. When an agent reaches\nthe evolutionary operation, the evolution lock is closed.\nWhen all agents are ready for evolution operation, the\nevolution lock is opened, and the algorithm evaluates the\nperformance of all agents.\nDue to the randomness and complexity of the environ-\nment, only using the current performance of agents for\nevaluation will not be able to accurately measure the per-\nformance of agents in the following situations:\n1) The current performance of agent is accidental, which\nis far from the result of re running in the same environ-\nment. Due to the uncertainty of the environment, the per-\nformance of the current agent may be accidental, and the\nagent cannot guarantee that the performance of the next\nrun is the same as the current one.\n2) Although the current performance of the agent may be\ngood, its mobility is poor. Reinforcement learning is a\nkind of unsupervised learning algorithm, but due to the\nlimitation of learning samples, its learning process will\nalso have the phenomenon of over ﬁtting. When over\nﬁtting occurs, the agent can perform well in the same\nor similar environment as the current learning samples.\nHowever, the agent may not perform well in other envi-\nronments under the same settings, its learning potential\nis low.\n3) There is a big gap between the current learning sam-\nples and the historical samples. Although the current per-\nformance of the agent is not good, it has great potential.\nAs we all know, with the running of reinforcement learn-\ning algorithm, the replay buffer used for agent training\nwill be constantly updated. Although the agent will fol-\nlow certain random principle to extract samples during\ntraining, it is undeniable that the learning samples in dif-\nferent periods have different characteristics. In the learn-\ning process, some agents with good performance may\nencounter an environment having a large gap with the\nsamples in the current buffer. In this case, the current\nperformance of these agents may be lower than that of\nhistory or other agents of the same generation. However,\nthis kind of agent may have high potential. After training\nin the current unfamiliar environment, this kind of agent\nwill have stronger robustness.\nIn order to solve the above problems, based on the gen-\neral agent population learning system, we imitated the\nlineage theory in human reproduction, and added the lin-\neage factor as one of the evaluation indexes. The new\nevaluation method comprehensively considers the cur-\nrent performance and implicit performance of the agent,\nthus improving the accuracy of agent evaluation.\nThe evaluation of agent can be divided into two sub steps.\nThe ﬁrst step is to normalize the current performance of\nthe agent. The current performance ρ of an agent is rep-\nresented by the score of the agent in the environment, so\nthe performance distribution in different environments is\ndifferent. During the comprehensive evaluation, it is nec-\nessary to ensure that the value range of lineage is consis-\ntent with that of current performance. After normalizing\nthe current performance of the agent, the performance\nand lineage have the same value range [0,1]. The second\nstep is computing the comprehensive evaluation value Γ\nof the agent according to the lineage value φ and the cur-\nrent performance ρ. The weight of lineage value and cur-\nrent performance is wφ and wρ respectively.\n4.2\nLINEAGE VALUE UPDATING\nAfter the evaluation of the agent, the algorithm enters\nthe lineage value updating phase. In LERL, lineage will\nbe updated with the learning of agents, and the new lin-\neage value will be determined by the historical lineage\nvalue and the current performance of agents. The pro-\ncess of lineage value renewal can be divided into three\nsteps. The ﬁrst step is to give the lineage update value\n∆φ according to the current performance of the agents.\nThe second step is to generate new lineage value accord-\ning to the old lineage value and lineage renewal value.\nThe third step is to normalize the new lineage value.\nWhen calculating the lineage update value, we rank\nthe agents according to their current performance. The\nhigher the performance of the agent, the higher the lin-\neage update value. The maximum lineage update value\nis 1, and all of them are positive.\n∆φi = An −Arank\ni\n+ 1\nAn\n(3)\nφi =\n(∆φi + φi ∗ζo) −min(∆φi + φi ∗ζo)\nmax(∆φi + φi ∗ζo) −min(∆φi + φi ∗ζo) (4)\nIt should be noted that the renewal of lineage value must\nbe strictly carried out after the comprehensive evaluation.\nBecause if the lineage value update is performed ﬁrst, the\ncomprehensive evaluation value will consider the current\nperformance twice. Although from the perspective of\nalgorithm execution, only adjusting the weight of per-\nformance can achieve the same effect as the current al-\ngorithm ﬂow, but multiple consideration of performance\nwill increase the complexity of lineage update function\nand increase the difﬁculty of parameter adjustment.\n4.3\nSELECTION\nIn the selection operation part, we adopt the elite strat-\negy to select the agents to be retained. According to the\ncomprehensive evaluation value of agents, the agent pop-\nulation can be divided into three parts.\nThe ﬁrst part is the elite agents E1. Each time a selection\noperation is performed, several agents with the best per-\nformance are selected as elite agents. As the parents of\nnew agents, elite agents will generate new agents through\nmutation or crossover operation.The second part is gen-\neral agents E2. The performance evaluation of general\nagent is in the middle of the whole population. In the\nevolution, the part of agent is directly a member of the\nnext generation population.The third part is to eliminate\nthe agents Em and Ec. The performance of the elimi-\nnated agents is at the end of the current population, which\nwill be discarded in evolution due to their poor perfor-\nmance. Because the population size is ﬁxed, the posi-\ntions of the eliminated agents will be replaced by the new\nagents. The new agents come from the mutation opera-\ntion and crossover operation of elite agents.\nThe elite strategy has the following functions: 1) It en-\nsures that the excellent agents will not be lost due to the\ninterference of mutation or crossover. 2) It ensures that\nthe upper performance limit of LERL is not lower than\nthat of the original algorithm. 3) If evolution operation\ncannot improve the performance of the agent, the whole\nlearning process of the agent will not be affected. 4) If\nthe performance of the agent declines due to the uncer-\ntainty of reinforcement learning, the corresponding agent\ncan be discarded in time.\n4.4\nMUTATION\nMutation operation and cross operation are the key to\nspeed up learning and improve performance. Mutation\noperation can be divided into three parts: agent replica-\ntion, network mutation and lineage inheritance. Agent\nreplication refers to the online network replication of an\nelite agent. It should be noted that in the agent repli-\ncation phase, the mutation operation only deals with the\nnetwork parameters, and does not copy the data in the\nreplay buffer.\nIn agent mutation, we disturb the parameters of the net-\nwork, and the disturbance range is all the layers of online\nnetwork. The disturbance operation includes two param-\neters: the ﬁrst is the probability of disturbance vpart, and\nthe second is the amplitude of disturbance vrange. When\nthe network is disturbed, we traverse the network layer\nby layer, and judge whether the corresponding layer is\ndisturbed according to the disturbance probability. When\na layer of the network is disturbed, the shape of the layer\nnetwork is obtained ﬁrst, and the disturbed network is\ngenerated according to its shape and disturbance ampli-\ntude. The parameters in the disturbed network are uni-\nformly distributed.\nVij ∼U(1 −vrange, 1 + vrange)\n(5)\nAfter the disturbed network is generated, we get a new\nnetwork by Hadamard product between the online pa-\nrameter matrix and the disturbed parameter matrix.\nWe introduce crossover operation Om, which is deﬁned\nas follows:\nOm(Q(x, a; θ)) := Q(x, a; θ ∗V )\n(6)\nThe disturbance amplitude vrange is very important for\nmutation operation. If vrange is too large, it will lead to\nthe loss of previous learning. If vrange is too small, the\nexploration of mutation operation cannot be guaranteed.\nIn addition, because of the less randomness of the agent\nin the learning process and the gradual convergence of\nthe algorithm, vrange should have different values in dif-\nferent periods. In LERL, with the increase of the number\nof iterations, vrange will decrease.\nvrange = min(vrange0 −Gn ∗vrange decay, vrange min)\n(7)\nWhen the disturbance amplitude decays to a certain ex-\ntent, the disturbance parameters will not change, so as to\nensure that the mutation operation remains exploratory.\nAfter the network disturbance, the new mutation agent\nneeds to inherit the lineage value of the original agent.\nDue to the inﬂuence of mutation operation, the inherited\nlineage value should be attenuated, and its attenuation\nparameter is ζm.\n4.5\nCROSSOVER\nThe ﬁrst step in crossover operation is similar to mutation\noperation, which is to select and copy agents from elite\nagents as the parent agents in crossover operation. Then,\nwe carry out network crossover operation on the selected\ntwo replicated elite agents.\nNo matter the binary code crossover operation in the\ntraditional genetic algorithm or the expected value\ncrossover operation in the evolution strategy, it is un-\ndoubtedly difﬁcult to be directly used in the neural net-\nwork. Referring to the description of human neural in\ncognitive neuroscience (Gazzaniga, 2000), we think that\nconvolution network is more emphasis on feature extrac-\ntion, while full connection layer is more emphasis on\ndecision-making according to the extracted features. The\nformer corresponds to the perception layer in the agent\npopulation learning system, and the latter to the thinking\nlayer. Therefore, we split the convolution network and\nthe full connection network in the online network as two\nbasic units of crossover operation. In the crossover oper-\nation, we crossover the convolution network part and the\nfull connection network part of the two elite agents, and\ntake the two new networks as the online networks of the\nnew agents.\nWe introduce crossover operation Oc, which is deﬁned\nas follows:\nQ(x, a) := Qf(Qc(x; θc), a; θf)\n(8)\nOc(Qj, Qk) := Qf\nj (Qc\nk(x; θc\nk), a; θf\nj )\n(9)\nSimilar to mutation operation, crossover operation also\nneeds to inherit and decay the lineage value of the parent\nagents, and its decay parameter is ζc.\nThe lineage decay formula of crossover operation is dif-\nferent from that of mutation operation. There are two\nparent agents of crossover operation, so it is necessary\nto calculate the average lineage value of parent agents\nbefore inheriting and decaying. In addition, because mu-\ntation and crossover do not affect each other and there\nis no strict logical order, the two operations can run in\nparallel.\n0\n50\n100\n150\n200\nIteration\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nScore\nRainbow_0\nRainbow_1\nRainbow_2\nRainbow_3\nRainbow_4\nLE_Rainbow_0\nLE_Rainbow_1\nLE_Rainbow_2\nLE_Rainbow_3\nLE_Rainbow_4\nLE_Rainbow_5\nLE_Rainbow_6\n0\n50\n100\n150\n200\nIteration\n0\n5000\n10000\n15000\n20000\n25000\n30000\nScore\nRainbow_Average\nLE_Rainbow_Average\nLE_Rainbow_Smooth\n0\n50\n100\n150\n200\nIteration\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nScore\nRainbow_Median\nLE_Rainbow_Median\nRainbow_Best\nLE_Rainbow_Best\n(a) Asterix\n0\n50\n100\n150\n200\nIteration\n1000\n2000\n3000\n4000\nScore\nRainbow_0\nRainbow_1\nRainbow_2\nRainbow_3\nRainbow_4\nLE_Rainbow_0\nLE_Rainbow_1\nLE_Rainbow_2\nLE_Rainbow_3\nLE_Rainbow_4\nLE_Rainbow_5\nLE_Rainbow_6\n0\n50\n100\n150\n200\nIteration\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nScore\nRainbow_Average\nLE_Rainbow_Average\nLE_Rainbow_Smooth\n0\n50\n100\n150\n200\nIteration\n1000\n2000\n3000\n4000\nScore\nRainbow_Median\nLE_Rainbow_Median\nRainbow_Best\nLE_Rainbow_Best\n(b) Assault\n0\n50\n100\n150\n200\nIteration\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nRainbow_0\nRainbow_1\nRainbow_2\nRainbow_3\nRainbow_4\nLE_Rainbow_0\nLE_Rainbow_1\nLE_Rainbow_2\nLE_Rainbow_3\nLE_Rainbow_4\nLE_Rainbow_5\nLE_Rainbow_6\n0\n50\n100\n150\n200\nIteration\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nRainbow_Average\nLE_Rainbow_Average\nLE_Rainbow_Smooth\n0\n50\n100\n150\n200\nIteration\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nRainbow_Median\nLE_Rainbow_Median\nRainbow_Best\nLE_Rainbow_Best\n(c) ChopperCommand\n0\n50\n100\n150\n200\nIteration\n0\n10000\n20000\n30000\n40000\nScore\nRainbow_0\nRainbow_1\nRainbow_2\nRainbow_3\nRainbow_4\nLE_Rainbow_0\nLE_Rainbow_1\nLE_Rainbow_2\nLE_Rainbow_3\nLE_Rainbow_4\nLE_Rainbow_5\nLE_Rainbow_6\n0\n50\n100\n150\n200\nIteration\n0\n10000\n20000\n30000\n40000\nScore\nRainbow_Average\nLE_Rainbow_Average\nLE_Rainbow_Smooth\n0\n50\n100\n150\n200\nIteration\n0\n10000\n20000\n30000\n40000\nScore\nRainbow_Median\nLE_Rainbow_Median\nRainbow_Best\nLE_Rainbow_Best\n(d) KungFuMaster\nFigure 2: Performance of LE Rainbow and Rainbow in Different Games\nAlgorithm 2: Evolution with Lineage\nInput: Agent population E = {A1, ..., An}, Current\nperformance P = {ρ1, ..., ρn} and Lineage\nevaluation Φ = {φ1, ..., φn}\nOutput: Agent population E\n1 for agent Ai ∈E do\n2\nρi ←(ρi −min(ρ))/(max(ρ) −min(ρ))\n3\nCompute comprehensive evaluation value\nΓi ←wρρi + wφφi\n4 end\n5 Update Φ using Equation (4)\n6 Divide E into E1, E2, Em and Ec\n7 for agent Au ∈Em do\n8\nSelect Aj from E1\n9\nQu ←Om(Qj)\n10\nφu ←ζmφj\n11 end\n12 for agent Aw ∈Ec do\n13\nSelect A1\nk and A2\nk from E1\n14\nQw ←Oc(Q1\nk, Q2\nk)\n15\nφw ←ζc(φ1\nk + φ2\nk)/2\n16 end\n17 E ←E1 ∪E2 ∪Em ∪Ec\n5\nEXPERIMENT\nWe use the data and algorithm in the Dopamine frame-\nwork for experimental analysis. Among them, the rein-\nforcement learning algorithm for LERL includes DQN,\nC51, Rainbow and IQN in Dopamine. The running en-\nvironment includes Asterix, Assault, ChopperCommand\nand KungFuMaster. The baseline data of each algorithm\nis provided by Dopamine project.\nFigure 2 shows the performance of LE Rainbow, which\nuses Rainbow algorithm as the basic reinforcement learn-\ning algorithm, in four games. The images in the ﬁrst\nrow show the original performance of the agents in Rain-\nbow and LE Rainbow. Among them, the agent popu-\nlation of LE Rainbow includes 7 agents. In evolution\noperations, the number of elite agents is 2, the number\nof general agents is 2, and the number of eliminated\nagents is 3. Rainbow’s curve is the performance data\nof agents running independently ﬁve times. The evolu-\ntion cycle of LE Rainbow is 5 iterations. The images\nin the second row are the average performance curve\nof the two algorithms. In addition, due to the random-\nness of evolution, the performance of LE Rainbow algo-\nrithm changes greatly, so its average performance is not\nrepresentative. We introduce the smooth average per-\nformance curve LE Rainbow Smooth for LE Rainbow.\nThe images in the third row show the best performance\ncurve and median performance curve of the two algo-\nrithms. The best performance curve of the agent repre-\nsents the best performance of the agent in each iteration,\nwhich can better help us to analyze the performance of\nLE Rainbow.\nAs can be seen from Figure 2, LE Rainbow improves the\nperformance of Rainbow greatly in Asterix and KungFu-\nMaster. Whether it’s average performance, best perfor-\nmance or median performance, LE Rainbow has always\nbeen higher than Rainbow since the early stage. In As-\nsault and ChopperCommand, although the performance\nof LE Rainbow is not signiﬁcantly improved, it also en-\nsures that the performance is not lower than that of Rain-\nbow. In addition, from the best performance curve, we\ncan see that LE Rainbow is more exploratory. In the pro-\ncess of exploration, the best performance LE Rainbow\nhas achieved is higher than Rainbow.\n0\n10\n20\n30\n40\nGeneration\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nGrowth Rate\nRainbow\nLE_Rainbow\nLE_Rainbow/Rainbow\n(a) Asterix\n0\n10\n20\n30\n40\nGeneration\n1.0\n1.2\n1.4\n1.6\n1.8\nGrowth Rate\nRainbow\nLE_Rainbow\nLE_Rainbow/Rainbow\n(b) Assault\n0\n10\n20\n30\n40\nGeneration\n1.0\n1.2\n1.4\n1.6\n1.8\nGrowth Rate\nRainbow\nLE_Rainbow\nLE_Rainbow/Rainbow\n(c) ChopperCommand\n0\n10\n20\n30\n40\nGeneration\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\nGrowth Rate\nRainbow\nLE_Rainbow\nLE_Rainbow/Rainbow\n(d) KungFuMaster\nFigure 3: The Performance Growth Rate of LE Rainbow\nand Rainbow in Different Games\nFigure 3 shows the performance growth rate curves of\nLE Rainbow and Rainbow in different evolution cycles\nand the performance growth rate curves of LE Rainbow\ncompared with Rainbow. The performance growth rate\nis equal to the ratio of the best performance in the cur-\nrent evolution cycle to the best performance in the pre-\nvious evolution cycle. Two hundred iterations can be\ndivided into 40 evolutionary cycles, that is, 40 genera-\ntions. The purple curve in Figure 3 represents the ratio\nof LE Rainbow performance to Rainbow performance.\nThe curve is affected by the performance growth rate of\nthe two algorithms.\nIt can be seen from Figure 3 that the ratio of the best\nperformance of the two algorithms may rise ﬁrst and\nthen decline during agents evolution.\nThe ascending\npart indicates that the evolution operation improves the\nperformance of the original reinforcement learning algo-\nrithm. The decreasing part can be regarded as a perfor-\nmance convergence phenomenon. The convergence phe-\nnomenon is inﬂuenced by performance bottleneck (the\nupper limit of scores available in the environment), func-\ntion bottleneck (the capacity limit of neural network) and\nparameter setting (the range of exploration).\nIn addition to the experiment of Rainbow algorithm, we\nalso test the LERL algorithm using DQN, C51 or IQN\nin Asterix. The default evolution cycle of related exper-\niments is still 5 iterations. (a-c) in Figure 4 show the\nbest and median performance of LE DQN, LE C51 and\nLE IQN in Asterix. We also try to adjust the evolution\n0\n50\n100\n150\n200\nIteration\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nScore\nDQN_Median\nLE_DQN_Median\nDQN_Best\nLE_DQN_Best\n(a) LE DQN\n0\n50\n100\n150\n200\nIteration\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nScore\nC51_Median\nLE_C51_Median\nC51_Best\nLE_C51_Best\n(b) LE C51\n0\n50\n100\n150\n200\nIteration\n0\n5000\n10000\n15000\n20000\n25000\n30000\nScore\nIQN_Median\nLE_IQN_Median\nIQN_Best\nLE_IQN_Best\n(c) LE IQN\n0\n50\n100\n150\n200\nIteration\n0\n5000\n10000\n15000\n20000\n25000\nScore\nRainbow(10)_Median\nLE_Rainbow(10)_Median\nRainbow(10)_Best\nLE_Rainbow(10)_Best\n(d) LE Rainbow(10)\nFigure 4: The Best Performance of Different LERL Al-\ngorithms in Asterix\ncycle and give the performance data of LE Rainbow with\nthe evolution cycle of 10 iterations, as shown in Fig-\nure 4(d). As can be seen from Figure 4, in addition to\nLE DQN, the other two kinds of LERL algorithms have\nachieved a large degree of performance improvement.\nCombining with the performance data of LE Rainbow\n(10) in Asterix, it can be analyzed that in Asterix envi-\nronment, the expression ability of neural network is the\nmain limiting factor for the performance improvement of\nLERL (function bottleneck).\n6\nCONCLUSION\nWe propose a general agent population learning system\n(GAPLS) and give the corresponding algorithm ﬂow ac-\ncording to its system structure. On this basis, we pro-\npose lineage evolutionary reinforcement learning algo-\nrithm (LERL) which combines genetic algorithm, rein-\nforcement learning algorithm and lineage factor.\nThe\nperformance of LERL in the experiment reached our\noverdue and veriﬁed our theoretical analysis. Compared\nwith the original reinforcement learning algorithm, the\nperformance and learning speed of the algorithm with LE\nare improved. In the future research, we think that some\nimprovements in mutation operations and crossover op-\nerations may be able to reduce the performance ﬂuctu-\nations caused by evolution operations. In addition, the\ninitial values of some parameters in evolution operations\nalso can be further studied.\nReferences\n[1] Gabriel Barth-Maron, Matthew W Hoffman, David\nBudden, Will Dabney, Dan Horgan, Dhruva Tb, Al-\nistair Muldal, Nicolas Heess, and Timothy Lilli-\ncrap. Distributed distributional deterministic policy\ngradients. arXiv preprint arXiv:1804.08617, 2018.\n[2] Marc G Bellemare,\nWill Dabney,\nand R´emi\nMunos. A distributional perspective on reinforce-\nment learning. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning-Volume\n70, pages 449–458. JMLR. org, 2017.\n[3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and\nMichael Bowling.\nThe arcade learning environ-\nment: An evaluation platform for general agents.\nJournal of Artiﬁcial Intelligence Research, 47:253–\n279, 2013.\n[4] Pablo Samuel Castro, Subhodeep Moitra, Car-\nles Gelada, Saurabh Kumar, and Marc G Belle-\nmare.\nDopamine:\nA research framework for\ndeep reinforcement learning.\narXiv preprint\narXiv:1812.06110, 2018.\n[5] Will Dabney, Georg Ostrovski, David Silver, and\nR´emi Munos. Implicit quantile networks for dis-\ntributional reinforcement learning. arXiv preprint\narXiv:1806.06923, 2018.\n[6] Will Dabney, Mark Rowland, Marc G Bellemare,\nand R´emi Munos.\nDistributional reinforcement\nlearning with quantile regression. In Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, 2018.\n[7] Meire Fortunato, Mohammad Gheshlaghi Azar, Bi-\nlal Piot, Jacob Menick, Ian Osband, Alex Graves,\nVlad Mnih, Remi Munos, Demis Hassabis, Olivier\nPietquin, et al.\nNoisy networks for exploration.\narXiv preprint arXiv:1706.10295, 2017.\n[8] Michael S Gazzaniga.\nThe new cognitive neuro-\nsciences. MIT press, 2000.\n[9] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and\nSergey Levine. Continuous deep q-learning with\nmodel-based acceleration.\nIn International Con-\nference on Machine Learning, pages 2829–2838,\n2016.\n[10] Nikolaus Hansen. The cma evolution strategy: a\ncomparing review. In Towards a new evolutionary\ncomputation, pages 75–102. Springer, 2006.\n[11] Matteo Hessel, Joseph Modayil, Hado Van Hasselt,\nTom Schaul, Georg Ostrovski, Will Dabney, Dan\nHorgan, Bilal Piot, Mohammad Azar, and David\nSilver. Rainbow: Combining improvements in deep\nreinforcement learning.\nIn Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[12] Volodymyr Mnih, Adria Puigdomenech Badia,\nMehdi Mirza, Alex Graves, Timothy Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu.\nAsynchronous methods for deep reinforcement\nlearning. In International conference on machine\nlearning, pages 1928–1937, 2016.\n[13] Volodymyr Mnih,\nKoray Kavukcuoglu,\nDavid\nSilver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller.\nPlaying atari\nwith deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\n[14] Volodymyr Mnih, Koray Kavukcuoglu, David Sil-\nver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K\nFidjeland, Georg Ostrovski, et al.\nHuman-level\ncontrol through deep reinforcement learning. Na-\nture, 518(7540):529–533, 2015.\n[15] Tom Schaul, John Quan, Ioannis Antonoglou, and\nDavid Silver. Prioritized experience replay. arXiv\npreprint arXiv:1511.05952, 2015.\n[16] Richard S Sutton, Andrew G Barto, et al. Introduc-\ntion to reinforcement learning, volume 135. MIT\npress Cambridge, 1998.\n[17] Hado Van Hasselt, Arthur Guez, and David Sil-\nver. Deep reinforcement learning with double q-\nlearning. In Thirtieth AAAI conference on artiﬁcial\nintelligence, 2016.\n[18] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado\nVan Hasselt, Marc Lanctot, and Nando De Freitas.\nDueling network architectures for deep reinforce-\nment learning. arXiv preprint arXiv:1511.06581,\n2015.\n[1–18]\n",
  "categories": [
    "cs.NE",
    "cs.AI",
    "cs.LG",
    "cs.MA"
  ],
  "published": "2020-09-26",
  "updated": "2020-09-26"
}