{
  "id": "http://arxiv.org/abs/2011.00177v1",
  "title": "Evaluation of Inference Attack Models for Deep Learning on Medical Data",
  "authors": [
    "Maoqiang Wu",
    "Xinyue Zhang",
    "Jiahao Ding",
    "Hien Nguyen",
    "Rong Yu",
    "Miao Pan",
    "Stephen T. Wong"
  ],
  "abstract": "Deep learning has attracted broad interest in healthcare and medical\ncommunities. However, there has been little research into the privacy issues\ncreated by deep networks trained for medical applications. Recently developed\ninference attack algorithms indicate that images and text records can be\nreconstructed by malicious parties that have the ability to query deep\nnetworks. This gives rise to the concern that medical images and electronic\nhealth records containing sensitive patient information are vulnerable to these\nattacks. This paper aims to attract interest from researchers in the medical\ndeep learning community to this important problem. We evaluate two prominent\ninference attack models, namely, attribute inference attack and model inversion\nattack. We show that they can reconstruct real-world medical images and\nclinical reports with high fidelity. We then investigate how to protect\npatients' privacy using defense mechanisms, such as label perturbation and\nmodel perturbation. We provide a comparison of attack results between the\noriginal and the medical deep learning models with defenses. The experimental\nevaluations show that our proposed defense approaches can effectively reduce\nthe potential privacy leakage of medical deep learning from the inference\nattacks.",
  "text": "Evaluation of Inference Attack Models for\nDeep Learning on Medical Data\nMaoqiang Wua, Xinyue Zhangb, Jiahao Dingb, Hien Nguyenb, Rong Yua,\nMiao Panb, Stephen T. Wongc\naSchool of Automation, Guangdong University of Technology, Guangzhou 510006, China\nbElectrical and Computer Engineering Department, University of Houston, TX 77004, USA\ncSystems Medicine and Bioengineering, Houston Methodist Cancer Center, 6445 Main\nStreet, TX 77030, USA\nAbstract\nDeep learning has attracted broad interest in healthcare and medical communi-\nties. However, there has been little research into the privacy issues created by\ndeep networks trained for medical applications. Recently developed inference\nattack algorithms indicate that images and text records can be reconstructed\nby malicious parties that have the ability to query deep networks. This gives\nrise to the concern that medical images and electronic health records containing\nsensitive patient information are vulnerable to these attacks. This paper aims\nto attract interest from researchers in the medical deep learning community to\nthis important problem. We evaluate two prominent inference attack models,\nnamely, attribute inference attack and model inversion attack. We show that\nthey can reconstruct real-world medical images and clinical reports with high\nﬁdelity.\nWe then investigate how to protect patients’ privacy using defense\nmechanisms, such as label perturbation and model perturbation. We provide a\ncomparison of attack results between the original and the medical deep learning\nmodels with defenses. The experimental evaluations show that our proposed de-\nfense approaches can eﬀectively reduce the potential privacy leakage of medical\ndeep learning from the inference attacks.\nKeywords:\nattribute inference attack, model inversion attack, medical\nmachine learning, collaborative inference\nPreprint submitted to Journal of LATEX Templates\nNovember 3, 2020\narXiv:2011.00177v1  [cs.LG]  31 Oct 2020\n1. Introduction\nDeep learning has become increasingly popular in healthcare and medicine\nareas.\nMedical institutions hold various modalities of medical data, such as\nelectronic health records, biomedical images, and pathology test results. Based\non medical data, deep neural network models are trained to address necessary\nhealthcare concerns [1]. Examples include but not limited to: 1) deep learning\nmodels based on medical records outperformed traditional clinical models for\ndetecting patterns in health trends and risk factors [2]; 2) deep learning model\nhad high sensitivity and speciﬁcity for detecting diabetic retinopathy and mac-\nular edema in retinal fundus photographs [3]; 3) a mammography-based deep\nlearning model was more accurate than traditional clinical models for predicting\nbreast cancer risk [4].\nHowever, the application of deep learning in healthcare is confronted with\nprivacy threats. The medical records contain personal private information like\ndrug usage patterns of the individual patient. Medical institutions also hold\npatients’ proﬁle information such as home address, gender, age, etc. The private\ninformation might be unwittingly leaked when the aforementioned data is used\nfor training a deep learning model [5, 6]. For example, attribute inference attacks\n[5] can utilize the trained model and incomplete information about a data point\nto infer the missing information for that point. The adversary could exploit such\nan attack to infer the target private information according to partial information\non medical records. Another instance is model inversion attacks that enable\nthe image data used for classiﬁcation inference to be recovered based on the\nintermediate output of convolutional neural networks (CNNs) [7]. The adversary\ncould deploy model inversion attack to recover the medical images of any target\npatient and infer private health conditions accordingly.\nThe risk of privacy leakage makes medical institutions increasingly less will-\ning to share their data. This inevitably slows down the research progress at the\nintersection of deep learning and healthcare. Thus, it is necessary to evaluate\nthe potential hazards of various attack models on medical data and develop\n2\ncorresponding defenses on such attacks.\nIn this paper, we attempt to implement two types of attack models on med-\nical data, which are shown in Fig. 1. We use attribute inference attack to infer\nthe sensitive attributes in medical record data according to the rest attributes\nand class labels, when training a deep learning model. We also employ model\ninversion attack to recover the medical image data based on the intermediate\ninference output. Against those attacks, we present two types of inference at-\ntack defense mechanisms. Label perturbation provides a way to add noise into\nconﬁdence scores and thus hinders the privacy leakage from model prediction.\nModel perturbation is proposed to add noise into parameters of deep network\nmodels and thus disturb the privacy disclosure in model inference. Experimental\nresults show that both attacks successfully disclose the private medical informa-\ntion used in training and inference processes, and the attacks are not eﬀective\nany more under the proposed inference attack defense mechanisms.\nThe main contributions of this paper are summarized as follows.\n• We evaluate attribute inference attack and model inversion attack on med-\nical data. That demonstrates the privacy vulnerability of deep learning\nmodels, which limits their applications in the medical area. As far as we\nknow, we are the ﬁrst to evaluate the model inversion attack on medical\ndata.\n• We present inference attack defenses based on label perturbation and\nmodel perturbation. The mechanisms can signiﬁcantly alleviate the pri-\nvacy breaches of medical data in both the training phase and the inference\nphase.\nThe rest of this paper is organized as follows. We summarize the related\nwork in Section 2. In Section 3, we describe the attribute inference attack and\nmodel inversion attack. We propose label perturbation mechanism and model\nperturbation mechanism against the two attacks in Section 4. Simulation results\nare presented in Section 5. Then we conclude the paper in Section 6.\n3\n2. Related Work\nThere are diﬀerent types of privacy attacks against training and inference\ndata. These attacks severely threaten patients’ privacy when deep learning is\nused in the healthcare area.\nThe ﬁrst type is membership inference attacks\n[8, 6], which tries to infer whether a target sample is contained in the dataset.\nThe second type is model encoding attacks [9], the adversary who directly ac-\ncesses to the training data can encode the sensitive data into the trained model\nand then retrieve the encoded sensitive information. The third type is attribute\ninference attack, given some attributes of the dataset, the adversary could infer\nthe sensitive attribute. The fourth type is model inversion attack, given a deep\nlearning model and some features of input data, the adversary could recover the\nrest of the features of the input data. In this paper, we choose two prominent\ninference attacks, namely attribute inference attack and model inversion attack,\nwhich may reconstruct medical images and clinical reports and be more threat-\nening to patients’ privacy. We evaluate their attack performance on medical\nrecords and medical images, and then propose defense methods against these\ntwo inference attacks.\nAttribute Inference Attack Attribute inference attack is studied in var-\nious areas. Gong et al. [10, 11] studied attribute inference attacks to infer the\nusers’ sensitive attribute of social networks by integrating social friends and be-\nhavioral records. May et al. [12] proposed a new framework for inference attacks\nin social networks, which smartly integrates and modiﬁes the existing state-of-\nthe-art CNN models. Qian et al. [13] demonstrated that knowledge graphs can\nstrengthen de-anonymization and attribute inference attacks, and thus increase\nthe risk of privacy disclosure. However, few studies evaluate the attribute infer-\nence attacks in the healthcare area. In this paper, we adopt the same attribute\ninference attack in [14] which infers the sensitive attributes based on conﬁdent\ncores in predictions and is conveniently deployed on healthcare data. We pro-\npose a label perturbation method to eﬀectually defend against the attribute\ninference attack.\n4\nModel inversion attack Model inversion attack is an outstanding attack to\nrecover the input data of deep neuron networks. He et al. [5] proposed a model\ninversion attack to recover input images via the conﬁdence score generated in the\nsoftmax model. He et al. [7] proposed a model inversion attack to reconstruct\ninput images via the intermediate outputs of the neural network.\nHitaj et\nal. [15] utilized Generative adversarial network (GAN) to recover the image\nin a collaborative training system. In this paper, we adopt the same model\ninversion attack in [7] by considering the medical collaborative deep learning\nscenario, where two hospitals hold diﬀerent parts of a deep neuron network\nand collaborate to complete the training and inference via transmitting the\nintermediate output information. As far as we know, we are the ﬁrst to evaluate\nthe model inversion attack on medical data via intermediate output information.\nWe propose an eﬀective and convenient perturbation method instead of using\nthe defenses suggested in [7], i.e., combining Trust Execution Environment and\nHomomorphic Encryption that requires special architecture support and huge\ncomputational burden.\nOther Attacks against Machine Learning Besides attribute inference\nattack and membership inference attack, there exist numerous other types of\nattacks against ML models [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n30, 31, 32, 33, 34, 35, 36, 37, 38]. A major attack type is adversarial examples\n[30, 28, 29, 35].\nIn this setting, the adversary tries to carefully craft noise\nand add them to the data samples aiming to mislead the target classifying. In\naddition, a similar type of attack is backdoor attack, where the adversary tries to\nembed a trigger into a trained model and to exploit when the model is deployed\n[18, 26, 32]. Another line of work is model stealing attack, [36] proposed the\nﬁrst attack on inferring a model’s parameters and the related works focus on\ninferring a model’s hyperparameters [27, 37].\nPossible Defenses To defend against the privacy attack, many researchers\nfocused on defense methods. Trust Execution Environment [39] is specialized\nhardware for secure remote computation and data conﬁdentiality protection\nagainst privileged adversaries. Homomorphic Encryption [40] allows the train-\n5\ning and inference operations on encrypted input data, so the sensitive informa-\ntion will not be leaked. However, these methods require special architecture\nsupport and a huge computational burden. Diﬀerential Privacy (DP) [41] adds\nnoise into the training model and there exists a trade-oﬀbetween usability and\nprivacy. However, our attacks mainly focus on the inference phase rather than\nthe training phase and thus the DP methods are not suitable to defend against\nour attacks. We propose label perturbation that adds noise in the predicted la-\nbel to defend attribute inference attack and mode perturbation that adds noise\ninto the after-trained model to defend model inversion attack. The proposed\nmethods are eﬀective and convenient for application. We also give the results\nof the trade-oﬀbetween model accuracy and attack performance. These results\nprovide an intuitive guide for medical staﬀto adjust the defenses against the\ntwo inference attacks.\n(a) Attribute inference attack\n(b) Model inversion attack\nFigure 1: Inference attack models and defense approaches for medical deep learning.\n6\n3. Inference Attack Models\n3.1. Vulnerability of Medical Deep Learning\nVia the use of deep learning algorithms, medical institutions can improve\nthe rate of correct diagnosis [2]. In the training phase, deep neural networks\nare trained based on input medical data and output diagnosis results to learn\nthe inherent relationships between them.\nIn the inference phase, the after-\ntrained deep neural networks can achieve high-accurate diagnosis results given\nnew medical data as input. However, during the training phase and inference\nphase, the adversary could adopt attack methods to infer or recover the input\nmedical data which contains sensitive information of patients. In this paper,\nwe evaluate two prominent inference attacks, i.e., attribute inference attack and\nmodel inversion attack. As for attribute inference attack, we assume that the\nadversary knows the attributes of all the patients except the sensitive attributes\nwhen the patients’ medical records are used as input. This assumption applies to\nthe cases that the attacker can search the rest attributes such as age and gender\nfrom the public database.\nThe adversary utilizes the inherent relationships\namong attributes and labels to recover the patients’ sensitive attributes of input\nmedical records. As for model inversion attack, we take the vulnerability of\ncollaborative deep learning as an example, which provides an eﬃcient paradigm\nto accelerate the learning and prediction process. The fundamental idea is to\nsplit a deep neural network into two parts. For example as Fig. 1b, in medical\ncollaborative learning, the ﬁrst few layers are stored in Hospital A while the\nrest are kept in Hospital B. In the collaborative training mode [42], Hospital A\nsends the outputs of the cut layer to Hospital B and then retrieve the gradients\nof the cut layer. In the collaborative inference mode [7, 43], Hospital A sends\nthe outputs of the cut layer to Hospital B and retrieves the ﬁnal results. The\nmodel training and inference processes are collaboratively carried out without\nsharing the raw data. However, the shared intermediate output information\nmay be leaked during the transmission. Given the information, the adversary\ncould recover the raw data with model inversion attack and thus compromise\n7\nthe data privacy of Hospital A.\n3.2. Attribute Inference Attack\nAs shown in Fig. 1a, attribute inference attack [14] enables an adversary\nto deduce sensitive attributes in patients’ medical records. In this setting, the\ngoal of the adversary is to guess the value of the sensitive features of a data\npoint, e.g., sex attribute, given public knowledge about the data sample and\nthe access to the model. Let (x, y) denote a data sample where x denotes the\ninput patient information, and y is the label of this data sample. We assume\nthat a deep network f(x) takes the input x to predict the output y.\nThe\nnetwork’s parameters are optimized by reducing the discrepancy between the\npredicted value f(x) and the true outcome y measured by the cross-entropy loss.\nWe assume there are d attributes in a data sample x and let xd be a sensitive\nattribute in x that an attacker wants to learn. Given the values of attributes\nx1, x2, · · · , xd−1, the prior probabilities of all attributes and the access to the\nmodel f(x), the attacker aims to ﬁnd the value of xd to maximize the posterior\nprobability P(xd|x1, x2, · · · , xd−1, f(x)). Therefore, the attacker can obtain the\nvalue of the sensitive attribute xd.\n3.3. Model Inversion Attack\nAs shown in Fig. 1b, model inversion attack [7] enables an adversary to\nrecover an input medical image x0 from the corresponding intermediate output\nv0 = fθ(x0), where fθ is the former layers of the model in Hospital A. We\nconsider the black box attack setting, where the adversary does not know the\nstructure or parameters θ of fθ but he could query the black-box model, i.e., he\ncould input the arbitrary data X into the model and observe the intermediate\noutputs fθ(X).\nThis assumption happens to the use case where Hospital A\nreleases its APIs to other medical entities as training and inference services. In\nthis setting, we build an inverse network model that learns the inverse mapping\nfrom output to input without the original model information.\nRoughly, the\n8\ninverse model gω ≈f −1\nθ\ncan be regarded as the approximated inverse function\nof fθ, where v = fθ(x) is input and x is output.\nAlgorithm 1 shows the detailed model inversion attack consisting of four\nphases.\nIn the observation phase, the adversary uses a cluster of samples\nX = x1, · · · , xn as inputs to query fθ and obtain V = fθ(x1), · · · , fθ(xn).\nHere the sample set X is assumed to follow the same distribution of x0. The\nassumption applies to the case that the radiologic images usually follows the\nsame distribution. In the training phase, the adversary trains the inverse net-\nwork gω by using V as inputs and X as targets. We exploit the l2 norm in the\npixel space as the loss function, which is given as\nl(ω; X) = 1\nn\nn\nX\ni=1\n\r\rgω\n\u0000fθ(xi)\n\u0001\n−xi\n\r\r2\n2 .\n(1)\nIn particular, the structure of gω is not necessarily related to fθ. In our ex-\nperiment, an entirely diﬀerent architecture is leveraged for the attack. In the\nrecovery phase, the adversary leverages the trained inverse model to recover the\nraw data from the intermediate value: x′\n0 = gω(v0).\nAlgorithm 1 Model Inversion Attack Algorithm\nInput: input data X = x1, x2, · · · , xn of the same distribution from target\ndata x0, output v0 of target data, batch size B, epoch number E, learning rate\nη\nOutput: recovered data x′\n0\n1: query the model by input data V = fθ(X)\n2: initialize ω0\n3: for each epoch t ∈T do\n4:\nβ ←(split V into batches of size B)\n5:\nfor each batch b ∈β do\n6:\nωt+1 ←ωt −η∇l(ωt; b)\n7: recover the target data x′\n0 = gω(v0)\n8: return x′\n0\n9\n4. Inference Attack Defense Mechanisms\n4.1. Label Perturbation Based Protection\nWe apply randomized responses [44] to protect the learning model output\nlabels of each data sample against attribute attacks. Intuitively, given a ﬂipping\nprobability p, for a binary classiﬁcation, the predicted label y will be ﬂipped with\np. Similarly, we assume the class set C = {1, 2, · · · , C}(C > 2), the predicted\nlabel y ∈C will be perturbed with the probability of p. If the predicted label y is\ngoing to be replaced, there is 1/(C−1) probability for each one of the other C−1\nclasses that the original label y will be substituted by the corresponding class.\nThe inference accuracy of the attribute inference attack will deteriorate when\nthe adversary obtains the inaccurate predicted label.\nAlthough the training\nperformance can be inﬂuenced by the label perturbation, by controlling the\nﬂipping probability p carefully, we can still have an acceptable training model.\n4.2. Model Perturbation Based Protection\nTo defend against model inversion attack, we adopt model perturbation in\nCNN model. Diﬀerent from the label perturbation that adds noise into predicted\nlabel, model perturbation adds noise into model parameters θ (weights and bias)\nbefore the forward propagation is implemented. Speciﬁcally, we use Gaussian\nmechanism with expectation 0 and variance σ to generate noise and add it into\nmodel parameters, which is given as\nθ = θ + N(0, σ2I).\n(2)\nAccordingly, the output of the cut layer is perturbed in collaborative deep learn-\ning. Model inversion attack becomes diﬃcult to build an accurate mapping from\nthe output to the input image and thus the image recovery quality decreases.\n10\n5. Performance Evaluation\n5.1. Attribute Inference Attack\n5.1.1. Experiment Settings\nWe evaluate attribute inference attack and label perturbation approach on\ntwo public medical record datasets: cardiovascular disease dataset and heart\ndisease dataset. The cardiovascular disease dataset consists of 70, 000 records,\n11 feature attributes including sensitive information such as age and gender,\nand labels indicating the presence or absence of cardiovascular disease. Heart\ndisease dataset [45] contains 13 attributes, 303 instances, and labels referring\nto the presence of heart disease in individual patients. We split the dataset\ninto training set and testing set with 80% and 20%. Our experiments use a\nmultilayer perceptron classiﬁer including 2 hidden layers with 100 neurons in\neach hidden layer.\n(a) Smoking\n(b) Alcohol intake\nFigure 2: Attribute attack performance on the “cardiovascular disease” dataset.\n5.1.2. Evaluation Results\nAs described in Section 3.2, in the experiments, we assume the attacker\ncan obtain other information of a patient except for only one attribute and\nthe marginal prior knowledge of the targeted attribute. We implement each\nexperiment 10 times and show the mean value as the curve and the standard\ndeviation as the error bar. The ﬂip probability denotes the defense level. The\nhigher ﬂip probability means better defense. When the ﬂip probability is equal\nto 0, it means no defense mechanism is applied. Fig. 2 and 3 demonstrates the\n11\nattack and defense performance on two datasets. We select two attributes from\nthe “heart” dataset, fasting blood sugar and gender, as the attacker’s targets.\nFor the “cardiovascular” dataset, we choose smoking and alcohol intake as the\ntarget attribute. We can observe that the attack accuracy reduces with higher\nﬂip probability. Also, the testing accuracy degrades slightly, if the defense level\nis high.\n(a) Smoking\n(b) Alcohol intake\nFigure 3: Attribute attack performance on the “cardiovascular disease” dataset.\n5.2. Model Inversion Attack\n5.2.1. Experiment Settings\nWe evaluate model inversion attack and model perturbation-based defence\non two public mammography datasets: MIAS [46] and CBIS-DDSM [47]. All\nthe images of MIAS dataset have been padded/clipped to 1024 × 1024. A total\nof 280 samples are obtained from MIAS for training (181 normal, 57 benign,\n42 malignant) while 50 samples are used for testing (26 normal, 12 benign, 12\nmalignant). We clip and compress all the images of CBIS-DDSM to 256 × 256.\nA total of 2, 326 samples are obtained from CBIS-DDSM for training (1, 263\nbenign, 1, 063 malignant) while 772 samples are used for testing (419 benign, 353\nmalignant). We adopt an CNN with 6 convolution layers and 2 fully connection\nlayers on the two datasets. Each convolution layer has 32 channels and kernel\nsize is 3. There is a maxpool layer after every two convolution layers. The\nmodel is split at 2nd, 4th, and 6th convolution layers. We select ADAM as our\noptimizer and set the learning rate as 0.001.\n12\nFigure 4: Recovered MIA inputs via model inversion attack.\nFigure 5: Recovered CBIS-DDSM inputs via model inversion attack.\nTable 1: MSE, PSNR, SSIM for model inversion attack with diﬀerent split layers\nMIAS\nDDSM\nlayer 2\nlayer 4\nlayer 6\nlayer 2\nlayer 4\nlayer 6\nMSE\n2.925\n55.042\n88.839\n1.672\n29.649\n110.460\nPSNR\n44.162\n31.039\n28.995\n46.385\n33.962\n28.269\nSSIM\n0.999\n0.994\n0.990\n0.999\n0.995\n0.984\n13\n5.2.2. Evaluation Results\nFig. 4 and Fig. 5 show the recovered images via model inversion attack.\nWhen the split point is in a lower layer, the recovered images have high quality.\nWhen the split point is in a deeper layer, the recovered images become relatively\nblurry and lose certain details. But even if in the recovered image from the out-\nput of the 6th layer, the details within the breasts can still be clearly identiﬁed.\nThe attackers could diagnose the patient’s breast health given the recovered\nmammography image with the help of classiﬁcation models or radiologists.\nTo quantify the attack results, we adopt three metrics, Mean-Square Er-\nror (MSE), Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index\n(SSIM) [48], which are shown in Table 1. MSE reﬂects pixel-wise similarity while\nPSNR measures the pixel-level recovery quality of the image. SSIM measures\nthe human perceptual similarity of two images by considering their luminance,\ncontrast, and structure. It ranges from [0, 1], where 1 represents the most simi-\nlar. When the split point is in a deeper layer, the recovered inputs have higher\nMSE, PSNR, and lower SSIM, which means the attack becomes harder.\nFig. 6 and Fig. 7 show the defense performance of model perturbation with\ndiﬀerent noise scale when the split point is in the 4th layer. We experiment\nGaussian noise distributions with scale 0.02 to 0.05 and central 0. When the\nscale increases, the recovered inputs become more blurry and lose more details.\nTable 2 shows the inference accuracy of CNN models injected by diﬀerent\nscale of noises as well as MSE, PSNR and SSIM metrics for the attack. Non-\nnoise means the original trained model without perturbation. When the scale\nincreases, the recovered inputs have higher MSE, PSNR, and lower SSIM. Model\nperturbation reduces the quality of recovered inputs while slightly decreases the\ninference performance. These results provide an intuitive guide for medical staﬀ\nto adjust the scale of noises in defense against the model inversion attack while\nkeeping a satisﬁed model performance.\n14\nFigure 6: Recovered MIAS inputs with and without model perturbation.\nFigure 7: Recovered CBIS-DDSM inputs with and without model perturbation.\n6. Conclusion\nIn this paper, we have evaluated two types of inference attacks on medi-\ncal images and clinical records, and demonstrated that these attacks can infer\nsensitive attributes of medical health records as well as recover medical images\nat high ﬁdelity. Our research ﬁnding exposes the risk of privacy leakage for\n15\nTable 2: MSE, PSNR, SSIM for deep learning model with and without model perturbation\nDatasets\nMIAS\nDDSM\nNoise\nnon\n0.02\n0.05\nnon\n0.02\n0.05\nAccuracy\n0.62\n0.60\n0.55\n0.618\n0.582\n0.553\nMSE\n55.042\n1992.121\n6163.927\n29.649\n327.7\n4238.92\nPSNR\n31.039\n15.479\n10.528\n33.962\n23.072\n11.897\nSSIM\n0.994\n0.714\n0.170\n0.995\n0.608\n0.523\nusing deep learning models in training medical data. To circumvent this prob-\nlem, we proposed inference attack defenses based on label perturbation and\nmodel perturbation. Experimental results showed that the proposed defenses\ncan eﬀectively defend the malicious inference attacks while the deep learning\nperformance can still be preserved commendably. The experimental results and\nthe approaches presented help to raise awareness about the privacy issues of\ndeploying deep learning networks in medicine and potentially open up a new\nvista to ensure patients’ privacy and conﬁdentiality in the increasing adapta-\ntion of AI-enabled information infrastructure in healthcare delivery and medical\nresearch.\nReferences\n[1] R. Miotto, F. Wang, S. Wang, X. Jiang, J. T. Dudley, Deep learning for\nhealthcare: review, opportunities and challenges, Brieﬁngs in bioinformat-\nics 19 (6) (2018) 1236–1246.\n[2] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,\nX. Liu, J. Marcus, M. Sun, et al., Scalable and accurate deep learning with\nelectronic health records, NPJ Digital Medicine 1 (1) (2018) 18.\n[3] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,\nS. Venugopalan, K. Widner, T. Madams, J. Cuadros, et al., Develop-\nment and validation of a deep learning algorithm for detection of diabetic\nretinopathy in retinal fundus photographs, Jama 316 (22) (2016) 2402–\n2410.\n16\n[4] A. Yala, C. Lehman, T. Schuster, T. Portnoi, R. Barzilay, A deep learn-\ning mammography-based model for improved breast cancer risk prediction,\nRadiology 292 (1) (2019) 60–66.\n[5] M. Fredrikson, S. Jha, T. Ristenpart, Model inversion attacks that exploit\nconﬁdence information and basic countermeasures, in: Proceedings of the\n22nd ACM SIGSAC Conference on Computer and Communications Secu-\nrity, 2015, pp. 1322–1333.\n[6] R. Shokri, M. Stronati, C. Song, V. Shmatikov, Membership inference at-\ntacks against machine learning models, in: 2017 IEEE Symposium on Se-\ncurity and Privacy (SP), IEEE, 2017, pp. 3–18.\n[7] Z. He, T. Zhang, R. B. Lee, Model inversion attacks against collaborative\ninference, in: Proceedings of the 35th Annual Computer Security Applica-\ntions Conference, 2019, pp. 148–162.\n[8] M. Nasr, R. Shokri, A. Houmansadr, Machine learning with membership\nprivacy using adversarial regularization, in: Proceedings of the 2018 ACM\nSIGSAC Conference on Computer and Communications Security, 2018, pp.\n634–646.\n[9] C. Song, T. Ristenpart, V. Shmatikov, Machine learning models that re-\nmember too much, in: Proceedings of the 2017 ACM SIGSAC Conference\non Computer and Communications Security, 2017, pp. 587–601.\n[10] N. Z. Gong, B. Liu, You are who you know and how you behave: Attribute\ninference attacks via users’ social friends and behaviors, in: 25th {USENIX}\nSecurity Symposium ({USENIX} Security 16), 2016, pp. 979–995.\n[11] N. Z. Gong, B. Liu, Attribute inference attacks in online social networks,\nACM Transactions on Privacy and Security (TOPS) 21 (1) (2018) 1–30.\n[12] B. Mei, Y. Xiao, R. Li, H. Li, X. Cheng, Y. Sun, Image and attribute\nbased convolutional neural network inference attacks in social networks,\nIEEE Transactions on Network Science and Engineering.\n17\n[13] J. Qian, X.-Y. Li, C. Zhang, L. Chen, De-anonymizing social networks and\ninferring private attributes using knowledge graphs, in: IEEE INFOCOM\n2016-The 35th Annual IEEE International Conference on Computer Com-\nmunications, IEEE, 2016, pp. 1–9.\n[14] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, T. Ristenpart, Privacy\nin pharmacogenetics: An end-to-end case study of personalized warfarin\ndosing, in: 23rd {USENIX} Security Symposium ({USENIX} Security 14),\nSan Diego, CA, 2014.\n[15] B. Hitaj, G. Ateniese, F. Perez-Cruz, Deep models under the gan: infor-\nmation leakage from collaborative deep learning, in: Proceedings of the\n2017 ACM SIGSAC Conference on Computer and Communications Secu-\nrity, 2017, pp. 603–618.\n[16] Y. Chen, S. Wang, D. She, S. Jana, On training robust PDF malware\nclassiﬁers, in: 29th USENIX Security Symposium (USENIX Security 20),\nUSENIX Association, Boston, MA, 2020.\nURL\nhttps://www.usenix.org/conference/usenixsecurity20/\npresentation/chen-yizheng\n[17] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, N. Borisov, Property inference\nattacks on fully connected neural networks using permutation invariant\nrepresentations, in: Proceedings of the 2018 ACM SIGSAC Conference on\nComputer and Communications Security, 2018, pp. 619–633.\n[18] T. Gu, B. Dolan-Gavitt, S. Garg, Badnets: Identifying vulnerabilities in\nthe machine learning model supply chain, arXiv preprint arXiv:1708.06733.\n[19] W. Guo, D. Mu, J. Xu, P. Su, G. Wang, X. Xing, Lemna: Explaining\ndeep learning based security applications, in: Proceedings of the 2018 ACM\nSIGSAC Conference on Computer and Communications Security, 2018, pp.\n364–379.\n18\n[20] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, N. Papernot, High ac-\ncuracy and high ﬁdelity extraction of neural networks, in: 29th {USENIX}\nSecurity Symposium ({USENIX} Security 20), 2020.\n[21] Y. Ji, X. Zhang, S. Ji, X. Luo, T. Wang, Model-reuse attacks on deep\nlearning systems, in: Proceedings of the 2018 ACM SIGSAC Conference\non Computer and Communications Security, 2018, pp. 349–363.\n[22] K. Leino, M. Fredrikson, Stolen memories:\nLeveraging model memo-\nrization for calibrated white-box membership inference, arXiv preprint\narXiv:1906.11798.\n[23] J. Li, N. Li, B. Ribeiro, Membership inference attacks and defenses in su-\npervised learning via generalization gap, arXiv preprint arXiv:2002.12062.\n[24] Z. Li, C. Hu, Y. Zhang, S. Guo, How to prove your model belongs to\nyou: a blind-watermark based framework to protect intellectual property\nof dnn, in: Proceedings of the 35th Annual Computer Security Applications\nConference, 2019, pp. 126–137.\n[25] X. Ling, S. Ji, J. Zou, J. Wang, C. Wu, B. Li, T. Wang, Deepsec: A\nuniform platform for security analysis of deep learning model, in: 2019\nIEEE Symposium on Security and Privacy (SP), IEEE, 2019, pp. 673–690.\n[26] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, X. Zhang, Trojaning\nattack on neural networks, in: 25nd Annual Network and Distributed Sys-\ntem Security Symposium, NDSS 2018, San Diego, California, USA, Febru-\nary 18-221, 2018, The Internet Society, 2018.\n[27] S. J. Oh, B. Schiele, M. Fritz, Towards reverse-engineering black-box neu-\nral networks, in: Explainable AI: Interpreting, Explaining and Visualizing\nDeep Learning, Springer, 2019, pp. 121–144.\n[28] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, A. Swami,\nPractical black-box attacks against machine learning, in: Proceedings of the\n19\n2017 ACM on Asia conference on computer and communications security,\n2017, pp. 506–519.\n[29] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, A. Swami,\nThe limitations of deep learning in adversarial settings, in: 2016 IEEE\nEuropean symposium on security and privacy (EuroS&P), IEEE, 2016, pp.\n372–387.\n[30] N. Papernot, P. McDaniel, A. Sinha, M. P. Wellman, Sok: Security and pri-\nvacy in machine learning, in: 2018 IEEE European Symposium on Security\nand Privacy (EuroS&P), IEEE, 2018, pp. 399–414.\n[31] E. Quiring, A. Maier, K. Rieck, Misleading authorship attribution of source\ncode using adversarial learning, in: 28th {USENIX} Security Symposium\n({USENIX} Security 19), 2019, pp. 479–496.\n[32] A. Salem, R. Wen, M. Backes, S. Ma, Y. Zhang, Dynamic backdoor attacks\nagainst machine learning models, arXiv preprint arXiv:2003.03675.\n[33] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,\nT. Goldstein, Poison frogs! targeted clean-label poisoning attacks on neural\nnetworks, in: Advances in Neural Information Processing Systems, 2018,\npp. 6103–6113.\n[34] D. She, Y. Chen, A. Shah, B. Ray, S. Jana, Neutaint: Eﬃcient dynamic\ntaint analysis with neural networks, in: 2020 IEEE Symposium on Security\nand Privacy (SP), 2020, pp. 364–380.\n[35] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, P. Mc-\nDaniel, Ensemble adversarial training: Attacks and defenses, in: Interna-\ntional Conference on Learning Representations, 2018.\n[36] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, T. Ristenpart, Stealing ma-\nchine learning models via prediction apis, in: 25th {USENIX} Security\nSymposium ({USENIX} Security 16), 2016, pp. 601–618.\n20\n[37] B. Wang, N. Z. Gong, Stealing hyperparameters in machine learning, in:\n2018 IEEE Symposium on Security and Privacy (SP), IEEE, 2018, pp.\n36–52.\n[38] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, B. Y. Zhao,\nNeural cleanse: Identifying and mitigating backdoor attacks in neural net-\nworks, in: 2019 IEEE Symposium on Security and Privacy (SP), IEEE,\n2019, pp. 707–723.\n[39] M. Sabt, M. Achemlal, A. Bouabdallah, Trusted execution environment:\nwhat it is, and what it is not, in: 2015 IEEE Trustcom/BigDataSE/ISPA,\nVol. 1, IEEE, 2015, pp. 57–64.\n[40] A. Acar, H. Aksu, A. S. Uluagac, M. Conti, A survey on homomorphic en-\ncryption schemes: Theory and implementation, ACM Computing Surveys\n(CSUR) 51 (4) (2018) 1–35.\n[41] J. Ding, S. M. Errapotu, H. Zhang, Y. Gong, M. Pan, Z. Han, Stochastic\nadmm based distributed machine learning with diﬀerential privacy, in: In-\nternational conference on security and privacy in communication systems,\nSpringer, 2019, pp. 257–277.\n[42] P. Vepakomma, O. Gupta, T. Swedish, R. Raskar, Split learning for health:\nDistributed deep learning without sharing raw patient data, arXiv preprint\narXiv:1812.00564.\n[43] E. Li, L. Zeng, Z. Zhou, X. Chen, Edge ai: On-demand accelerating deep\nneural network inference via edge computing, IEEE Transactions on Wire-\nless Communications.\n[44] S. L. Warner, Randomized response:\nA survey technique for eliminat-\ning evasive answer bias, Journal of the American Statistical Association\n60 (309) (1965) 63–69.\n[45] R. Detrano, A. Janosi, W. Steinbrunn, M. Pﬁsterer, J.-J. Schmid,\nS. Sandhu, K. H. Guppy, S. Lee, V. Froelicher, International application\n21\nof a new probability algorithm for the diagnosis of coronary artery disease,\nThe American journal of cardiology 64 (5) (1989) 304–310.\n[46] P. SUCKLING J, The mammographic image analysis society digital mam-\nmogram database, Digital Mammo (1994) 375–386.\n[47] R. S. Lee, F. Gimenez, A. Hoogi, K. K. Miyake, M. Gorovoy, D. L. Rubin,\nA curated mammography data set for use in computer-aided detection and\ndiagnosis research, Scientiﬁc data 4 (2017) 170177.\n[48] A. Hore, D. Ziou, Image quality metrics: Psnr vs. ssim, in: 2010 20th\ninternational conference on pattern recognition, IEEE, 2010, pp. 2366–\n2369.\n22\n",
  "categories": [
    "cs.LG",
    "cs.CR",
    "cs.CV"
  ],
  "published": "2020-10-31",
  "updated": "2020-10-31"
}