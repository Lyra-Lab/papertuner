{
  "id": "http://arxiv.org/abs/2011.00341v1",
  "title": "Unsupervised Deep Persistent Monocular Visual Odometry and Depth Estimation in Extreme Environments",
  "authors": [
    "Yasin Almalioglu",
    "Angel Santamaria-Navarro",
    "Benjamin Morrell",
    "Ali-akbar Agha-mohammadi"
  ],
  "abstract": "In recent years, unsupervised deep learning approaches have received\nsignificant attention to estimate the depth and visual odometry (VO) from\nunlabelled monocular image sequences. However, their performance is limited in\nchallenging environments due to perceptual degradation, occlusions and rapid\nmotions. Moreover, the existing unsupervised methods suffer from the lack of\nscale-consistency constraints across frames, which causes that the VO\nestimators fail to provide persistent trajectories over long sequences. In this\nstudy, we propose an unsupervised monocular deep VO framework that predicts\nsix-degrees-of-freedom pose camera motion and depth map of the scene from\nunlabelled RGB image sequences. We provide detailed quantitative and\nqualitative evaluations of the proposed framework on a) a challenging dataset\ncollected during the DARPA Subterranean challenge; and b) the benchmark KITTI\nand Cityscapes datasets. The proposed approach outperforms both traditional and\nstate-of-the-art unsupervised deep VO methods providing better results for both\npose estimation and depth recovery. The presented approach is part of the\nsolution used by the COSTAR team participating at the DARPA Subterranean\nChallenge.",
  "text": "Unsupervised Deep Persistent Monocular Visual Odometry and Depth\nEstimation in Extreme Environments\nYasin Almalioglu1,2, Angel Santamaria-Navarro2, Benjamin Morrell2, and Ali-akbar Agha-mohammadi2\nAbstract— In recent years, unsupervised deep learning ap-\nproaches have received signiﬁcant attention to estimate the\ndepth and visual odometry (VO) from unlabelled monocu-\nlar image sequences. However, their performance is limited\nin challenging environments due to perceptual degradation,\nocclusions and rapid motions. Moreover, the existing unsu-\npervised methods suffer from the lack of scale-consistency\nconstraints across frames, which causes that the VO estimators\nfail to provide persistent trajectories over long sequences. In\nthis study, we propose an unsupervised monocular deep VO\nframework that predicts six-degrees-of-freedom pose camera\nmotion and depth map of the scene from unlabelled RGB image\nsequences. We provide detailed quantitative and qualitative\nevaluations of the proposed framework on a) a challenging\ndataset collected during the DARPA Subterranean challenge1;\nand b) the benchmark KITTI and Cityscapes datasets. The\nproposed approach outperforms both traditional and state-of-\nthe-art unsupervised deep VO methods providing better results\nfor both pose estimation and depth recovery. The presented\napproach is part of the solution used by the COSTAR team\nparticipating at the DARPA Subterranean Challenge.\nI. INTRODUCTION\nAutonomous robot traversal and 3D structure reconstruc-\ntion capabilities have a wide variety of applications in ex-\ntreme environments, such as autonomous driving [1]; search\nand rescue in emergency responses [2]; inspection of under-\nground habitats [3], [4]; or planetary surface exploration [5],\n[6]. The ability to estimate ego-motion and the scene map\nis critical to enable these capabilities. In this sense, vision-\nbased solutions for localization and 3D structure reconstruc-\ntion are prevailing thanks to the camera characteristics, being\nlow cost; with low weight and low power consumption; and\noffering reasonably rich exteroceptive information.\nCamera motion estimation and depth map reconstruction\nare fundamental and well-studied problems in computer\nvision. Many traditional techniques have been proposed in\nthe last decade, achieving reasonably good results [7]–[11].\nHowever, they are usually committed to ﬁnding accurate\nimage correspondences between consecutive frames, which\n1Y. Almalioglu is with the Computer Science Department, The University\nof Oxford, UK. {yasin.almalioglu}@cs.ox.ac.uk\n2Y. Almalioglu, A. Santamaria-Navarro, B. Morrell and A. Agha-\nmohammadi are with the NASA - Jet Propulsion Laboratory, California\nInstitute of Technology, Pasadena, CA, US. {yasin.almalioglu,\nangel.santamaria.navarro, benjamin.morrell,\naliakbar.aghamohammadi}@jpl.nasa.gov\nThis work was carried out at the Jet Propulsion Laboratory, California\nInstitute of Technology, under a contract with the National Aeronautics and\nSpace Administration. Copyright 2020 California Institute of Technology.\nU.S. Government sponsorship acknowledged.\n1https://www.subtchallenge.com\nis a frequently violated condition in challenging environ-\nments. For instance, such matching can only be established\nfor a subset of all pixels, which leaves the problem of\nestimating ill-posed depth. These scenarios typically involve\noff-nominal conditions such as perceptual degradation; vari-\nable lighting conditions; non-Lambertian surfaces or variable\nsurface colors and textures; potential presence of obscurants\n(e.g., fog, smoke, dust or water puddles); and physical\nobstructions within the ﬁeld-of-view [12], [13].\nFollowing the success of deep learning in different do-\nmains, recent approaches solve the ill-posed depth estimation\nby using data-driven techniques. Even if the data is insuf-\nﬁcient to resolve ambiguities, deep networks can estimate\nthe camera pose and depth maps by generalizing from prior\nexamples they have learned [14], [15]. In this sense, super-\nvised deep-learning-based methods have shown good per-\nformance, successfully alleviating issues such as scale drift,\nwhich affects traditional feature extraction and parameter\ntuning [16]–[19]. Eigen et al. [20] show that a Convolution\nNeural Network (CNN) can predict the depth map from\na single image using the ground truth depths acquired by\nrange sensors. Although the supervised approaches [20]–[22]\nshow high-quality motion and depth estimation results, the\nacquisition of ground truth can be either impractical or even\nimpossible in real-world scenes.\nIn recent years, unsupervised deep learning approaches\nhave achieved remarkable results, comparable to those from\nsupervised techniques [23]–[29]. Unsupervised approaches\nallow learning from raw camera frames alone, without the\nneed for supervision signals (e.g., depth sensors) and the\ntrained networks are able to infer a depth map from a\nsingle image and ego-motion from consecutive images. SfM-\nLearner [23] is among the ﬁrst unsupervised methods that\njointly learn camera motion and depth estimation. Geonet\n[30] and Ranjan et al. [31] incorporate optical ﬂow into\nthe joint unsupervised training framework. SC-SFM [32]\nenforces depth consistency to solve the scale inconsistency\nissue in SfM-Learner [23].\nAlthough existing unsupervised learning methods provide\nstate-of-the-art performance, their estimations are still limited\nin challenging environments. Some visual degradation might\nviolate their underlying frame correspondence assumptions\nthat use geometric image reconstruction. Further, and more\nimportantly, recent works suffer from the per-frame scale\nambiguity due to the lack of a single and consistent scaling\nof the camera motion. As a result, the ego-motion network\ncannot predict a full camera trajectory over a long image\nsequence. Multiple approaches propose to disconnect the\narXiv:2011.00341v1  [cs.CV]  31 Oct 2020\ngeometric constraints from the unsupervised architecture to\nhandle occlusions in optical ﬂow estimation [33], [34]. On\nthe other hand, differentiable mesh rendering [35], [36] offers\nan alternative geometric approach to handle occlusions. In\nthe context of joint learning of depth recovery and ego-\nmotion estimation, several works propose a learned explain-\nability mask [23] by penalizing the minimum re-projection\nloss between the frames or use optical ﬂow [37] to solve\nocclusion problems. Gordon et al. [38] propose a geometric\nmethod for occlusion handling. Drawing inspiration of some\nof these methods, we address both the occlusion problem and\nscale ambiguity across frames without incurring a substantial\nadditional computational cost.\nIn this study, we propose a novel monocular visual odom-\netry estimation and depth recovery approach that can operate\nin challenging environments, able to produce persistent re-\nsults over a long duration. We train an unsupervised deep\nneural network that takes a sequence of monocular images\nand estimates 6-Degrees-of-Freedom (DoF) camera motion\nand the depth map. Similar to [29], [39], [40], we utilize\nfor the training a view reconstruction approach as part of\nthe objective function. The entire pose estimation and depth\nmap reconstruction pipeline is a persistent framework thanks\nto the occlusion-aware and scale-aware objectives imposed\nduring the optimization of the network.\nIn summary, the main contributions of our method are the\nfollowing:\n• Two new loss functions to tackle the problems of\nocclusions and trajectory scale. Further, we describe\nthe total loss function to incorporate them into the\nunsupervised architecture. These contributions alleviate\nthe need for separate networks to handle occlusions and\nscale-ambiguity across frames.\n• A novel depth enhancement technique for unsupervised\ndepth reconstruction methods, which enable the gener-\nation of depth images in challenging environments.\nThese contributions enable long-duration operations in per-\nceptually degraded environments, which to the best of au-\nthors’ knowledge, is the ﬁrst unsupervised deep-learning\napproach to estimate the camera (robot) odometry while\nreconstructing the depth map using images from a monocular\ncamera.\nTo validate the proposed approach, we evaluate it on\nthe KITTI [41] and Cityscapes [42] datasets as benchmarks\nfor comparative analysis with other state-of-the-art methods.\nThis evaluation criterion has been widely accepted in the\nrobotics community in recent years. This approach is part\nof the state estimation framework developed by the team\nCoSTAR2 for the DARPA Subterranean Challenge3. Hence,\nwe also show results on a dataset from the NASA-JPL,\nCalifornia Institute of Technology, with images captured\nby a Husky Clearpath4 robot under perception-challenging\nconditions, during the exploration of the underground urban\n2https://costar.jpl.nasa.gov\n3https://subtchallenge.com\n4https://clearpathrobotics.com\ncircuit of the DARPA Challenge.\nThe rest of this paper is organized as follows. Section II\npresents the proposed approach, with detailed descriptions\nof the architecture and its mathematical background. Section\nIII shows our quantitative and qualitative results with com-\nparisons to the existing methods in the literature. Finally,\nSection IV brieﬂy discusses the ﬁndings and concludes the\nstudy.\nII. UNSUPERVISED DEPTH AND POSE ESTIMATION\nA. Architecture Overview\nThe proposed architecture is based on unsupervised deep\nlearning to learn ego-motion and depth from monocular\nimage sequences jointly. The raw RGB sequences, consisting\nof a target and source views, are stacked together to form\nan input batch to the multi-view pose estimation and depth\nrecovery modules. The motion-prediction network predicts a\nmotion of every pixel with respect to the background and\na residual translation ﬁeld to account for moving objects.\nIn parallel, a second network generates a depth map of the\ntarget view. The view reconstruction module reconstructs\nthe target image using the predicted depth map, estimated\n6-DoF camera pose and nearby colour values from source\nimages. In this architecture, a) we impose scale-consistency\nacross consecutive frames through a geometry consistent\nloss function; b) we estimate occlusions geometrically, based\non the estimated depth maps to apply this loss only in\nnon-occluded pixels; c) we regularize motion ﬁelds based\non residual translations that indicate which pixels might\nbelong to moving objects; and d) we include other state-\nof-the-art loss functions to handle dissimilarity or edge-\naware smoothness in a total loss function. Furthermore, e)\nwe introduce spatial–channel combinational attention into\ngeometry understanding to explore the effectiveness of self-\nattention for scene geometry understanding. This architecture\nis shown in Fig. 1 and its details are explained hereafter.\nB. Networks\nWe rely on two convolutional networks based on the\nResNet-18 model [43], one predicting depth from a single\nimage, and the other predicting ego-motion and the motion\nﬁeld relative to the scene, using three input images.\na) Depth Network: The ﬁrst part of the architecture\nis a depth network that recovers a single-view depth map\nof the target frame. The depth prediction network uses a\nUNet architecture and a softplus activation (z = log(1+eℓ))\nto convert the logits (ℓ) to depth values (z). We embed\ndepth enhancement modules (DE) into both encoder and\ndecoder sub-networks, which re-calibrate depth features and\ncan produce more useful and important features to capture\nﬁne details in the scene.\nDepth Enhancement Module: Given the original feature\nmap F = {F1,F2,...,Fc}, where c is the number of channels,\nthe DE module produces a channel attention map Ac and a\nspatial attention map As to reﬁne F as shown in Fig. 2. The\nmax-pooling and average-pooling operations aggregate the\nglobal information of input features. Then, we feed these\n208x128x32\n104x64x64\n52x32x128\n26x16x256\n13x4x512\nDepth Encoder\nRes1\nRes2\nRes3\nRes4\nRes5\nDE\nDE\nDE\n26x16x1024\nUpconv1\n52x32x512\nUpconv2\n104x64x256\nUpconv3\n208x128x128\nUponv4\n416x256x1\nUpconv5\nDepth Decoder\nDE\nDE\nDE\nSkip Connections\n26x16x1024\nUpconv1\n52x32x512\nUpconv2\n104x64x256\nUpconv3\n208x128x128\nUponv4\n416x256x1\nUpconv5\nPose Decoder\n208x128x32\n104x64x64\n52x32x128\n26x16x256\n13x4x512\nPose Encoder\nRes2 Res3 Res4\nRes5\n416x256x1\nUpconv5\nBackground \nTranslation\nField\nResidual\nTranslation\nField\n+\nPose\nView\nreconstruction\nSkip Connections\nRes1\nMonocular sequence\nIt+1:It 416x128x6\nIt-1:It 416x256x6\nIt 416x256x3\nFig. 1: Proposed unsupervised deep-learning architecture for pose estimation and depth map generation. The spatial\ndimensions on layers and output channels show the tensor shapes that ﬂow through the network. The depth network generates\na depth map from a single input image, using our depth enhancement module (DE). The pose network estimates a background\nand a residual motion ﬁeld of the given three consecutive frames. The network is optimized using scale-aware and occlusion-\naware loss functions along with photometric and smoothness losses.\nHxWxC\nInput\nFeatures\nMax\nPooling\nAverage\nPooling\nFully\nConnected\nLayer\nSigmoid\nx\nHxWxC\nMax\nPooling\nAverage\nPooling\nConvolution\nSigmoid\nx\nInput\nFeatures\nHxWxC\nOutput\nFeatures\nFig. 2: Architecture of the depth enhancement module.\ntwo features Fmax and Favg into a fully-connected layer with\none hidden layer to recover the original channel size.\nb) Pose Network: The second network shown in the\nbottom of Fig. 1 tries to estimate relative pose p ∈SE(3)\nintroduced by motion ﬁelds across frames. The motion\nestimation network is a UNet architecture based on FlowNet\n[15]. A stack of pose encoder and decoder sub-networks\npredicts the global rotation angles (r0) and the global trans-\nlation vector (t0) that represent the movement of the entire\nscene with respect to the camera due to ego-motion. The\ndecoder layers progressively reﬁne the translation, from a\nsingle vector to a residual translation vector ﬁeld δt(x,y).\nThe translation ﬁeld is deﬁned as the sum of the global\ntranslation vector plus the masked residual translation:\nt(x,y) = t0 +m(x,y)δt(x,y),\n(1)\nwhere m(x,y) equals one at pixels that could belong to\nmobile objects and zero otherwise as described in Sec. II-C.\nC. Loss functions\na) Occlusion-aware loss: When the camera and the\nscene move relatively to each other, points in the scene that\nare visible in one frame may become occluded in another.\nThe cross-frame consistency cannot be enforced on the\noccluded pixels by a loss. Given a depth map and a motion\nﬁeld in one frame, we geometrically detect where occlusions\noccur, and exclude the occluded areas from the consistency\nloss. The occlusion-aware loss re-projects the depth values\nonto the camera frame and detects if the depth value on\nthe re-projected is visible. Gordon et al. [38] propose to\nasymmetrically choose source points that land in front of\nthe depth map in the target frame. However, the projected\npoints at the occluded areas need interpolation that can fall\ninto a region instead of speciﬁc locations. We propose to\nchoose points that fall within the neighborhood distance dn\nof the occluded area. We also choose points that not only\nfall in front of the target map but also behind it to obtain\na symmetric mask, which eliminates unnecessary reversed\nsource-target depth computation.\nb) Scale-aware loss: Given source and target depth\nmaps Da and Db, and the relative pose Pab, we minimize\nthe difference between the re-projected 3D scene structure:\nDdiff(p) = |Da\nb(p)−D′\nb(p)|\nDa\nb(p)+D′\nb(p)\n(2)\nwhere Da\nb is the computed depth map of Ib by warping Da\nusing Pab; and D′\nb is the re-projected depth map from Db.\nThis optimization imposes a scale consistency constraint in\nMethods\nSeq. 09\nSeq. 10\nterr (%)\nrerr (◦/100m)\nterr (%)\nrerr (◦/100m)\nORB-SLAM [48]\n15.30\n0.26\n3.68\n0.48\nZhou et al. [23]\n17.84\n6.78\n37.91\n17.78\nZhan et al. [40]\n11.93\n3.91\n12.45\n3.46\nGANVO [29]\n11.52\n3.53\n11.60\n5.17\nSC-SFM [32]\n11.2\n3.35\n10.1\n4.96\nOurs\n10.87\n3.14\n8.91\n4.45\nTABLE I: Visual odometry results on KITTI [41] odometry\ndataset. We report the performance of ORB-SLAM [48] as\na reference to compare with state-of-the-art deep learning\nmethods.\nthe entire sequence as previously shown by Bian et al. [32]\nusing a point-wise distance across all pixels. Unlike [32] that\nuses an occlusion mask based on the depth difference, we ge-\nometrically handle the occluded areas as explained in Sec. II-\nC, which is more sensitive to ﬁne details in the depth map\n(see Fig. 5 for example results). With the scale-aware training\nof the network, the pose network predicts globally scale-\nconsistent trajectories even in challenging environments.\nc) Total loss: Previous works [23], [30], [31], [44]\nleveraged the brightness constancy and spatial smoothness\npriors proposed in [45], and have showed how the photomet-\nric error between the warped and the target frames is effective\nin an unsupervised loss function to optimize the network.\nWe apply an occlusion-aware L1 loss for the photometric\nerror due to its robustness to outliers. In addition, we\nimpose occlusion-aware cycle consistency for the predicted\nmotion ﬁelds. We require that the inverse motion ﬁeld is the\nopposite of the inverse motion. We add an additional image\ndissimilarity loss SSIM [46] to handle the varying ambient\nlighting in a complex environment as it normalizes the pixel\nillumination. Finally, we include the edge-aware smoothness\nloss used in [31] to compensate for the inferior performance\nof the photometric loss in low-texture and non-homogeneous\nregions:\nLs = ∑\np\n(e−∇Ia(p) ·∇Da(p))2,\n(3)\nwhere ∇is the ﬁrst derivative along spatial directions, which\nguides the smoothness by the edge of images.\nIII. EXPERIMENTAL RESULTS\nWe implemented our unsupervised architecture with the\npublicly available Tensorﬂow framework [47]. We optimized\nthe weights of the network using Adam optimization with\nthe parameters β1 = 0.9, β2 = 0.999, learning rate of 0.001\nand mini-batch size of 8. We used sequential images of size\n416×256 as the input tensors of the model. We trained the\nmodel on an NVIDIA TITAN V model GPU.\nThe validation of the proposed approach is two fold. On\nthe one hand we use the KITTI [1] and the Cityscapes [42]\ndatasets for benchmarking, where we compare our method\nwith standard training/test splits for the odometry and\nmonocular depth map estimation tasks. Second, we evaluate\nour method on a perception-challenging dataset recorded\nduring the DARPA Subterranean Challenge, in order to prove\n7m\n14m\n21m\n28m\n35m\nDistance travelled [m]\n5\n10\n15\n20\n25\n30\n35\n40\ntrel(%)\nMethod\nGANVO\nSC-SFM\nDPVO\n7m\n14m\n21m\n28m\n35m\nDistance travelled [m]\n2\n3\n4\n5\n6\n7\n8\n9\nrrel( /100m)\nMethod\nGANVO\nSC-SFM\nDPVO\nFig. 3: Relative pose errors for the subterranean dataset,\nusing different segment lengths ({7,14,21,28,35} m) based\non the shortest sequence throughout the whole trajectory in\nmultiple segments (in total: 1h run and\n2km traversed),\nwhere our proposed approach (DPVO) outperforms existing\ndeep-learning state-of-art methods.\nthe effectiveness of our architecture for both depth map\nreconstruction and pose estimation over long sequences in\ncomplex environments. This subterranean dataset was gath-\nered during the DARPA competition during an autonomous\nexploration of an underground environment in the Satsop\nNuclear Plant, Elma, Washington.\nA. Pose estimation benchmark\nWe evaluated the ego-motion prediction performance on\nthe standard KITTI visual odometry split. Speciﬁcally, the\nKITTI sequences 09-10. In this sense, the standard 5-point\nAbsolute Trajectory Error (ATE) metric [23], [30], [49] mea-\nsures local agreement between the estimated trajectories and\nthe respective ground truth. However, we believe that in this\ncase, a relative pose error metric is better suited to measure\nthe drift of an odometry system [38], [40], [50], [51]. Thus,\nwe show statistics for the relative translation and rotation\nerror, divided by the distance travelled and averaged over\nthe trajectory segments of lengths {7,14,21,28,35} m over\nall sequences based on the shortest sequence. Table I summa-\nrizes both metrics. As shown in Table I, the proposed method\n0\n20\n40\n60\n80\nx[m]\n0\n10\n20\n30\n40\n50\n60\ny[m]\nSeq09-15\nDPVO\nLO\nWIO\n(a)\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\nx[m]\n0\n1\n2\n3\n4\n5\n6\n7\n8\ny[m]\nSeq16-17\nDPVO\nLO\nWIO\n(b)\nFig. 4: Two sample trajectories from the statistical analysis\nusing the dataset recorded by the COSTAR team during\nthe DARPA Subterranean Challenge. Here, we compare our\nunsupervised learning method (DPVO) with a very accurate\nLIDAR odometry [53] and a less-accurate wheel-inertial\nodometry, as ground-truth estimates are not available for\nthis environment. DPVO is more resistant to drifts than\nwheel-inertial odometry, achieving performances comparable\nto those from the LIDAR odometry, in both rotational and\ntranslational motions.\noutperforms all the competing unsupervised baselines on\nthe KITTI sequences 09-10, without any need for global\noptimization steps such as loop closure detection, bundle\nadjustment and re-localization, revealing that out method\npersistently predicts ego-motion over long sequences. Since\nmost of the compared methods are monocular approaches\nand lack a scaling factor to match with real-world scale, we\nscaled and aligned (7DoF optimization) the predictions to\nthe ground truth associated poses during the evaluation by\nminimizing ATE [52].\nFurthermore, we evaluated our approach on our challeng-\ning subterranean dataset (DARPA Challenge) the standard\nanalysis criteria to show how it persistently estimates the ego-\nmotion over a long duration in complex environments. Fig.\n3 shows the results of analyzing sub-sequences of lengths\nRGB\nLIDAR\nSC-SFM\nOurs\nRGB\nLIDAR\nSC-SFM\nOurs\nFig. 5: Samples of monocular depth estimation results on\nthe KITTI dataset for qualitative comparison of the unsuper-\nvised methods. Our DPVO captures details in challenging\nscenes that contain occlusions and uneven road lines. Some\nexamples of these important differences are highlighted with\ndashed boxes.\n{7,14,21,28,35} m and reports the average translational\nerror terr(%) and rotational errors rerr(◦/100m). As seen in\nFig. 3, our approach outperforms state-of-the-art methods\nin terms of both average translational error terr(%) and\nrotational errors rerr(◦/100m). Moreover, to validate the\nusefulness of the approach we present in Fig. 4 two sample\nsequences (two evaluated segments) from this subterranean\ndataset. In this case, we compare our approach (DPVO) with\na highly accurate LIDAR odometry (360o ﬁeld-of-view) [53]\nand (less-accurate) wheel-inertial odometry as baselines. The\nsequence in Fig. 4a has 208.14 m length, which shows\nthe odometry estimation performance of DPVO over long\nsubterranean sequences. The sequence in Fig. 4b has 54.40\nm length and contains complex camera motions, proving that\nDPVO is resistant to abrupt motions.\nB. Single-view depth evaluation\nOur proposed approach produces (and in most cases\nimproves) state-of-the-art results on single view depth pre-\ndictions, as shown in Table II. Here, the depth is evaluated on\nthe Eigen et al. [20] split of the raw KITTI dataset [41] fol-\nlowing the previous works [20], [27], [30], [54]. As shown in\nTable II, our method outperforms the other competitors [29],\n[31], [32] on several benchmarks. Previous works in the\nliterature [29], [32], [38] proved that transfer learning from\nCityscapes dataset to KITTI is beneﬁcial and leads to more\naccurate depth estimation; thus we include CS+K benchmark\nin this work to compare cross-dataset generalizability of our\nDPVO. DPVO signiﬁcantly improves the performance on\ndepth estimation benchmarks using Cityscapes in the training\n(see CS+K in Table II).\nFigure 5 shows examples of depth map results predicted\nby our DPVO and SC-SFM methods along with the RGB\ninput and ground-truth. We highlight the notable differences\nwith SC-SFM, which fails to capture distant objects in\nthe scene. Furthermore, Fig. 5 also shows that the depth\nmaps predicted by the proposed DPVO capture the small\nError ↓\nAccuracy ↑\nMethods\nDataset\nAbsRel\nSqRel\nRMS\nRMSlog\n< 1.25\n< 1.252\n< 1.253\nZhou et al. [23]\nK\n0.208\n1.768\n6.856\n0.283\n0.678\n0.885\n0.957\nMahjourian et al. [27]\nK\n0.163\n1.240\n6.220\n0.250\n0.762\n0.916\n0.968\nGeonet [30]\nK\n0.155\n1.296\n5.857\n0.233\n0.793\n0.931\n0.973\nDF-Net [44]\nK\n0.150\n1.124\n5.507\n0.223\n0.806\n0.933\n0.973\nCC [31]\nK\n0.140\n1.070\n5.326\n0.217\n0.826\n0.941\n0.975\nGANVO [29]\nK\n0.150\n1.141\n5.448\n0.216\n0.808\n0.939\n0.975\nSC-SFM [32]\nK\n0.137\n1.089\n5.439\n0.217\n0.830\n0.942\n0.975\nOurs\nK\n0.127\n1.077\n5.312\n0.214\n0.835\n0.941\n0.975\nZhou et al. [23]\nCS+K\n0.198\n1.836\n6.565\n0.275\n0.718\n0.901\n0.960\nMahjourian et al. [27]\nCS+K\n0.159\n1.231\n5.912\n0.243\n0.784\n0.923\n0.970\nGeonet [30]\nCS+K\n0.153\n1.328\n5.737\n0.232\n0.802\n0.934\n0.972\nDF-Net [44]\nCS+K\n0.146\n1.182\n5.215\n0.213\n0.818\n0.943\n0.978\nCC [31]\nCS+K\n0.139\n1.032\n5.199\n0.213\n0.827\n0.943\n0.977\nGANVO [29]\nCS+K\n0.138\n1.155\n4.412\n0.232\n0.820\n0.939\n0.976\nSC-SFM [32]\nCS+K\n0.128\n1.047\n5.234\n0.208\n0.846\n0.947\n0.976\nOurs\nCS+K\n0.122\n1.039\n5.184\n0.208\n0.851\n0.948\n0.976\nCC [31]\nSubT\n0.214\n1.486\n6.280\n0.284\n0.713\n0.912\n0.952\nGANVO [29]\nSubT\n0.190\n1.391\n5.899\n0.266\n0.746\n0.920\n0.962\nSC-SFM [32]\nSubT\n0.175\n1.309\n5.772\n0.260\n0.765\n0.925\n0.964\nOurs\nSubT\n0.149\n1.338\n5.484\n0.229\n0.792\n0.935\n0.969\nTABLE II: Monocular single-view depth estimation results, testing on the odometry split of KITTI dataset [41]. The methods\ntrained on KITTI raw [41] and the DARPA subterranean datasets are denoted by K and SubT, respectively. Models with\npre-training on CityScapes [42] are denoted by CS+K. The best performance in each block is highlighted with bold font.\nInput\nRGB\nIntel\nRealSense\nSC-SFM\nOur DPVO\nFig. 6: Qualitative comparison of two unsupervised monocular depth estimation methods on the challenging DARPA\nsubterranean dataset. The stereo depth output of the Intel RealSense camera (D435i model) is shown for visual comparison\npurposes. Our DPVO captures details in challenging scenes containing low textured areas, poorly illuminated regions, and\nwith strong occlusions, preserving accurate and detailed depth map predictions both in close and distant regions.\nobjects in the scene, whereas the other methods tend to\nignore them. Most importantly, as shown in the bottom rows\nof Table II (quantitatively) and in Fig. 6 (qualitatively),\nour unsupervised approach signiﬁcantly outperforms state-\nof-the-art methods in challenging scenarios. The proposed\nDPVO also accurately predicts the depth values of the objects\nin low-textured areas caused by the perceptual degradation\nin a scene. A simple loss function on the depth map without\nhandling occlusions leads to averaging all likely locations of\ndetails, whereas the depth enhancement modules in feature\nspace with a natural depth prior and geometric loss con-\nstraints make the proposed DPVO more sensitive to the likely\npositions of the details in the scene.\nIV. CONCLUSIONS\nIn this study, we proposed an unsupervised deep learning\nmethod for pose and depth map estimation using monocular\nimage sequences. This work addresses critical challenges for\nunsupervised learning of depth and visual odometry through\ngeometric occlusion-aware and scale-aware loss functions as\nwell as depth enhancement modules. The proposed method\noutperforms all the competing unsupervised and traditional\nbaselines in terms of pose estimation and depth map re-\nconstruction by a signiﬁcant margin in challenging envi-\nronments. As a path forward, we plan to explicitly address\noptical ﬂow in order to improve the performance in such\nperception-challenging environments.\nREFERENCES\n[1] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\ndriving? the kitti vision benchmark suite,” in Computer Vision and\nPattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012,\npp. 3354–3361.\n[2] K. Nagatani, S. Kiribayashi, Y. Okada, K. Otake, K. Yoshida, S. Ta-\ndokoro, T. Nishimura, T. Yoshida, E. Koyanagi, M. Fukushima et al.,\n“Emergency response to the nuclear accident at the fukushima daiichi\nnuclear power plants using mobile rescue robots,” Journal of Field\nRobotics, vol. 30, no. 1, pp. 44–63, 2013.\n[3] A. Santamaria-Navarro, R. Thakker, D. D. Fan, B. Morrell, and A. ak-\nbar Agha-mohammadi, “Towards resilient autonomous navigation of\ndrones,” 2020.\n[4] K. Ebadi, Y. Chang, M. Palieri, A. Stephens, A. Hatteland, E. Hei-\nden, A. Thakur, N. Funabiki, B. Morrell, S. Wood et al., “Lamp:\nLarge-scale autonomous mapping and positioning for exploration\nof perceptually-degraded subterranean environments,” in 2020 IEEE\nInternational Conference on Robotics and Automation (ICRA). IEEE,\n2020, pp. 80–86.\n[5] A. Agha, K. Mitchell, and P. Boston, “Robotic exploration of planetary\nsubsurface voids in search for life,” AGUFM, vol. 2019, pp. P41C–\n3463, 2019.\n[6] T. Sasaki, K. Otsu, R. Thakker, S. Haesaert, and A.-a. Agha-\nmohammadi, “Where to map? iterative rover-copter path planning\nfor mars exploration,” IEEE Robotics and Automation Letters, vol. 5,\nno. 2, pp. 2123–2130, 2020.\n[7] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison, “Dtam: Dense\ntracking and mapping in real-time,” in Computer Vision (ICCV), 2011\nIEEE International Conference on.\nIEEE, 2011, pp. 2320–2327.\n[8] G. Klein and D. Murray, “Parallel tracking and mapping for small ar\nworkspaces,” in Mixed and Augmented Reality, 2007. ISMAR 2007.\n6th IEEE and ACM International Symposium on.\nIEEE, 2007, pp.\n225–234.\n[9] Y. Furukawa, B. Curless, S. M. Seitz, and R. Szeliski, “Towards\ninternet-scale multi-view stereo,” in Computer Vision and Pattern\nRecognition (CVPR), 2010 IEEE Conference on.\nIEEE, 2010, pp.\n1434–1441.\n[10] M. Turan, Y. Y. Pilavci, I. Ganiyusufoglu, H. Araujo, E. Konukoglu,\nand M. Sitti, “Sparse-then-dense alignment-based 3d map reconstruc-\ntion method for endoscopic capsule robots,” Machine Vision and\nApplications, vol. 29, no. 2, pp. 345–359, 2018.\n[11] Y. Almalioglu, M. Turan, C. X. Lu, N. Trigoni, and A. Markham,\n“Milli-rio: Ego-motion estimation with low-cost millimetre-wave\nradar,” IEEE Sensors Journal, 2020.\n[12] S. Davide and F. Friedrich, “Visual odometry: part i: the ﬁrst 30 years\nand fundamentals,” IEEE Robotics & Automation Magazine, vol. 18,\nno. 4, pp. 80–92, 2011.\n[13] D. G. Lowe, “Distinctive image features from scale-invariant key-\npoints,” International journal of computer vision, vol. 60, no. 2, pp.\n91–110, 2004.\n[14] S. Wang, R. Clark, H. Wen, and N. Trigoni, “Deepvo: Towards end-\nto-end visual odometry with deep recurrent convolutional neural net-\nworks,” in Robotics and Automation (ICRA), 2017 IEEE International\nConference on.\nIEEE, 2017, pp. 2043–2050.\n[15] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,\nP. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning\noptical ﬂow with convolutional networks,” in Proceedings of the IEEE\ninternational conference on computer vision, 2015, pp. 2758–2766.\n[16] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “Vinet:\nVisual-inertial odometry as a sequence-to-sequence learning problem.”\nin AAAI, 2017, pp. 3995–4001.\n[17] M. R. U. Saputra, P. P. de Gusmao, C. X. Lu, Y. Almalioglu, S. Rosa,\nC. Chen, J. Wahlstr¨om, W. Wang, A. Markham, and N. Trigoni,\n“Deeptio: A deep thermal-inertial odometry with visual hallucination,”\nIEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1672–1679,\n2020.\n[18] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti,\n“Deep endovo: A recurrent convolutional neural network (rcnn) based\nvisual odometry approach for endoscopic capsule robots,” Neurocom-\nputing, vol. 275, pp. 1861–1870, 2018.\n[19] M. R. U. Saputra, P. P. de Gusmao, S. Wang, A. Markham, and\nN. Trigoni, “Learning monocular visual odometry through geometry-\naware curriculum learning,” in 2019 International Conference on\nRobotics and Automation (ICRA).\nIEEE, 2019, pp. 3549–3555.\n[20] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a\nsingle image using a multi-scale deep network,” in Advances in neural\ninformation processing systems, 2014, pp. 2366–2374.\n[21] Y. Kuznietsov, J. Stuckler, and B. Leibe, “Semi-supervised deep\nlearning for monocular depth map prediction,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2017,\npp. 6647–6655.\n[22] F. Liu, C. Shen, G. Lin, and I. Reid, “Learning depth from single\nmonocular images using deep convolutional neural ﬁelds,” IEEE\ntransactions on pattern analysis and machine intelligence, vol. 38,\nno. 10, pp. 2024–2039, 2015.\n[23] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised\nlearning of depth and ego-motion from video,” in CVPR, vol. 2, no. 6,\n2017, p. 7.\n[24] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monoc-\nular depth estimation with left-right consistency,” in CVPR, vol. 2,\nno. 6, 2017, p. 7.\n[25] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy,\nand T. Brox, “Demon: Depth and motion network for learning monoc-\nular stereo,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2017, pp. 5038–5047.\n[26] M. Turan, E. P. Ornek, N. Ibrahimli, C. Giracoglu, Y. Almalioglu,\nM. F. Yanik, and M. Sitti, “Unsupervised odometry and depth learn-\ning for endoscopic capsule robots,” in 2018 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS).\nIEEE, 2018,\npp. 1801–1807.\n[27] R. Mahjourian, M. Wicke, and A. Angelova, “Unsupervised learning\nof depth and ego-motion from monocular video using 3d geometric\nconstraints,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2018, pp. 5667–5675.\n[28] Y. Almalioglu, M. Turan, A. E. Sari, M. R. U. Saputra, P. P.\nde Gusm˜ao, A. Markham, and N. Trigoni, “Selfvio: Self-supervised\ndeep monocular visual-inertial odometry and depth estimation,” arXiv\npreprint arXiv:1911.09968, 2019.\n[29] Y. Almalioglu, M. R. U. Saputra, P. P. de Gusmao, A. Markham, and\nN. Trigoni, “Ganvo: Unsupervised deep monocular visual odometry\nand depth estimation with generative adversarial networks,” in 2019\nInternational Conference on Robotics and Automation (ICRA). IEEE,\n2019, pp. 5474–5480.\n[30] Z. Yin and J. Shi, “Geonet: Unsupervised learning of dense depth,\noptical ﬂow and camera pose,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), vol. 2, 2018.\n[31] A. Ranjan, V. Jampani, L. Balles, K. Kim, D. Sun, J. Wulff, and\nM. J. Black, “Competitive collaboration: Joint unsupervised learning\nof depth, camera motion, optical ﬂow and motion segmentation,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2019, pp. 12 240–12 249.\n[32] J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M.-M. Cheng, and\nI. Reid, “Unsupervised scale-consistent depth and ego-motion learning\nfrom monocular video,” in Advances in neural information processing\nsystems, 2019, pp. 35–45.\n[33] Y. Wang, Y. Yang, Z. Yang, L. Zhao, P. Wang, and W. Xu, “Occlusion\naware unsupervised learning of optical ﬂow,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\npp. 4884–4893.\n[34] J. Janai, F. Guney, A. Ranjan, M. Black, and A. Geiger, “Unsupervised\nlearning of multi-frame optical ﬂow with occlusions,” in Proceedings\nof the European Conference on Computer Vision (ECCV), 2018, pp.\n690–706.\n[35] T. H. Nguyen-Phuoc, C. Li, S. Balaban, and Y. Yang, “Rendernet:\nA deep convolutional network for differentiable rendering from 3d\nshapes,” in Advances in Neural Information Processing Systems, 2018,\npp. 7891–7901.\n[36] H. Kato, Y. Ushiku, and T. Harada, “Neural 3d mesh renderer,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 3907–3916.\n[37] Z. Yang, P. Wang, Y. Wang, W. Xu, and R. Nevatia, “Every pixel\ncounts: Unsupervised geometry learning with holistic 3d motion un-\nderstanding,” in Proceedings of the European Conference on Computer\nVision (ECCV), 2018, pp. 0–0.\n[38] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, “Depth from\nvideos in the wild: Unsupervised monocular depth learning from un-\nknown cameras,” in Proceedings of the IEEE International Conference\non Computer Vision, 2019, pp. 8977–8986.\n[39] R. Szeliski, “Prediction error as a quality metric for motion and\nstereo,” in Computer Vision, 1999. The Proceedings of the Seventh\nIEEE International Conference on, vol. 2.\nIEEE, 1999, pp. 781–788.\n[40] H. Zhan, R. Garg, C. Saroj Weerasekera, K. Li, H. Agarwal, and\nI. Reid, “Unsupervised learning of monocular depth estimation and\nvisual odometry with deep feature reconstruction,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition,\n2018, pp. 340–349.\n[41] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\nThe kitti dataset,” The International Journal of Robotics Research,\nvol. 32, no. 11, pp. 1231–1237, 2013.\n[42] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-\nnenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset\nfor semantic urban scene understanding,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016, pp.\n3213–3223.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2016, pp. 770–778.\n[44] Y. Zou, Z. Luo, and J.-B. Huang, “Df-net: Unsupervised joint learning\nof depth and ﬂow using cross-task consistency,” in Proceedings of the\nEuropean conference on computer vision (ECCV), 2018, pp. 36–53.\n[45] S. Baker and I. Matthews, “Lucas-kanade 20 years on: A unifying\nframework,” International journal of computer vision, vol. 56, no. 3,\npp. 221–255, 2004.\n[46] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image\nquality assessment: from error visibility to structural similarity,” IEEE\ntransactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.\n[47] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: a system for\nlarge-scale machine learning.” in OSDI, vol. 16, 2016, pp. 265–283.\n[48] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a\nversatile and accurate monocular slam system,” IEEE Transactions\non Robotics, vol. 31, no. 5, pp. 1147–1163, 2015.\n[49] V. Casser, S. Pirk, R. Mahjourian, and A. Angelova, “Unsupervised\nlearning of depth and ego-motion: A structured approach,” in Thirty-\nThird AAAI Conference on Artiﬁcial Intelligence (AAAI-19), vol. 2,\n2019, p. 7.\n[50] T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monoc-\nular visual-inertial state estimator,” IEEE Transactions on Robotics,\nvol. 34, no. 4, pp. 1004–1020, 2018.\n[51] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale,\n“Keyframe-based visual–inertial odometry using nonlinear optimiza-\ntion,” The International Journal of Robotics Research, vol. 34, no. 3,\npp. 314–334, 2015.\n[52] S. Umeyama, “Least-squares estimation of transformation parameters\nbetween two point patterns,” IEEE Transactions on Pattern Analysis\n& Machine Intelligence, no. 4, pp. 376–380, 1991.\n[53] M. Palieri, B. Morrell, A. Thakur, K. Ebadi, J. Nash, L. Carlone,\nC. Guaragnella, and A. Aga-mohammadi, “LOCUS - LiDAR odometry\nfor consistent operation in uncertain settings,” Under review at IEEE\nRobotics and Automation Letters. July 2020.\n[54] F. Liu, C. Shen, G. Lin, and I. D. Reid, “Learning depth from single\nmonocular images using deep convolutional neural ﬁelds.” IEEE Trans.\nPattern Anal. Mach. Intell., vol. 38, no. 10, pp. 2024–2039, 2016.\n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2020-10-31",
  "updated": "2020-10-31"
}