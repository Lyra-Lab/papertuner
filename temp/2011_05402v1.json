{
  "id": "http://arxiv.org/abs/2011.05402v1",
  "title": "OCR Post Correction for Endangered Language Texts",
  "authors": [
    "Shruti Rijhwani",
    "Antonios Anastasopoulos",
    "Graham Neubig"
  ],
  "abstract": "There is little to no data available to build natural language processing\nmodels for most endangered languages. However, textual data in these languages\noften exists in formats that are not machine-readable, such as paper books and\nscanned images. In this work, we address the task of extracting text from these\nresources. We create a benchmark dataset of transcriptions for scanned books in\nthree critically endangered languages and present a systematic analysis of how\ngeneral-purpose OCR tools are not robust to the data-scarce setting of\nendangered languages. We develop an OCR post-correction method tailored to ease\ntraining in this data-scarce setting, reducing the recognition error rate by\n34% on average across the three languages.",
  "text": "OCR Post Correction for Endangered Language Texts\nShruti Rijhwani,1 Antonios Anastasopoulos,2, † Graham Neubig1\n1Language Technologies Institute, Carnegie Mellon University\n2Department of Computer Science, George Mason University\n{srijhwan,gneubig}@cs.cmu.edu,\nantonis@gmu.edu\nAbstract\nThere is little to no data available to build nat-\nural language processing models for most en-\ndangered languages.\nHowever, textual data\nin these languages often exists in formats that\nare not machine-readable, such as paper books\nand scanned images. In this work, we address\nthe task of extracting text from these resources.\nWe create a benchmark dataset of transcrip-\ntions for scanned books in three critically en-\ndangered languages and present a systematic\nanalysis of how general-purpose OCR tools\nare not robust to the data-scarce setting of en-\ndangered languages. We develop an OCR post-\ncorrection method tailored to ease training in\nthis data-scarce setting, reducing the recogni-\ntion error rate by 34% on average across the\nthree languages.1\n1\nIntroduction\nNatural language processing (NLP) systems exist\nfor a small fraction of the world’s over 6,000 liv-\ning languages, the primary reason being the lack\nof resources required to train and evaluate models.\nTechnological advances are concentrated on lan-\nguages that have readily available data, and most\nother languages are left behind (Joshi et al., 2020).\nThis is particularly notable in the case of endan-\ngered languages, i.e., languages that are in danger\nof becoming extinct due to dwindling numbers of\nnative speakers and the younger generations shift-\ning to using other languages. For most endangered\nlanguages, ﬁnding any data at all is challenging.\nIn many cases, natural language text in these\nlanguages does exist. However, it is locked away\nin formats that are not machine-readable — pa-\nper books, scanned images, and unstructured web\npages. These include books from local publishing\n†: Work done at Carnegie Mellon University.\n1Code and data are available at https://shrutirij.\ngithub.io/ocr-el/.\n(a) Ainu (left) – Japanese (right)\n(b) Griko (top) – Italian (bottom)\n(c) Yakkha (top) – Nepali (middle) – English (bottom)\n(d) Handwritten Shangaji – typed English glosses\nnxúuzi wa náńtiíkwa \nmu-xuzi o-a nantikwa \n3-sauce 3-Conn 1a.cashew \nA sauce of green cashew nuts. \nFigure 1: Examples of scanned documents in endan-\ngered languages accompanied by translations from the\nsame scanned book (a, b, c) or linguistic archive (d).\nhouses within the communities that speak endan-\ngered languages, such as educational or cultural ma-\nterials. Additionally, linguists documenting these\nlanguages also create data such as word lists and\ninterlinear glosses, often in the form of handwrit-\nten notes. Examples from such scanned documents\nare shown in Figure 1. Digitizing the textual data\nfrom these sources will not only enable NLP for\nendangered languages but also aid linguistic docu-\nmentation, preservation, and accessibility efforts.\narXiv:2011.05402v1  [cs.CL]  10 Nov 2020\nIn this work, we create a benchmark dataset and\npropose a suite of methods to extract data from\nthese resources, focusing on scanned images of\npaper books containing endangered language text.\nTypically, this sort of digitization requires an opti-\ncal character recognition (OCR) system. However,\nthe large amounts of textual data and transcribed\nimages needed to train state-of-the-art OCR models\nfrom scratch are unavailable in the endangered lan-\nguage setting. Instead, we focus on post-correcting\nthe output of an off-the-shelf OCR tool that can\nhandle a variety of scripts. We show that targeted\nmethods for post-correction can signiﬁcantly im-\nprove performance on endangered languages.\nAlthough OCR post-correction is relatively well-\nstudied, most existing methods rely on consider-\nable resources in the target language, including a\nsubstantial amount of textual data to train a lan-\nguage model (Schnober et al., 2016; Dong and\nSmith, 2018; Rigaud et al., 2019) or to create syn-\nthetic data (Krishna et al., 2018). While readily\navailable for high-resource languages, these re-\nsources are severely limited in endangered lan-\nguages, preventing the direct application of existing\npost-correction methods in our setting.\nAs an alternative, we present a method that\ncompounds on previous models for OCR post-\ncorrection, making three improvements tailored\nto the data-scarce setting. First, we use a multi-\nsource model to incorporate information from the\nhigh-resource translations that commonly appear in\nendangered language books. These translations are\nusually in the lingua franca of the region (e.g., Fig-\nure 1 (a,b,c)) or the documentary linguist’s primary\nlanguage (e.g., Figure 1 (d) from Devos (2019)).\nNext, we introduce structural biases to ease learn-\ning from small amounts of data. Finally, we add\npretraining methods to utilize the little unanno-\ntated data that exists in endangered languages.\nWe summarize our main contributions as follows:\n• A benchmark dataset for OCR post-correction\non three critically endangered languages: Ainu,\nGriko, and Yakkha.\n• A systematic analysis of a general-purpose OCR\nsystem, demonstrating that it is not robust to the\ndata-scarce setting of endangered languages.\n• An OCR post-correction method that adapts the\nstandard neural encoder-decoder framework to\nthe highly under-resourced endangered language\nsetting, reducing both the character error rate and\nthe word error rate by 34% over a state-of-the-art\ngeneral-purpose OCR system.\n2\nProblem Setting\nIn this section, we ﬁrst deﬁne the task of OCR\npost-correction and introduce how we incorporate\ntranslations into the correction model. Next, we\ndiscuss the sources from which we obtain scanned\ndocuments containing endangered language texts.\n2.1\nFormulation\nOptical Character Recognition\nOCR tools are\ntrained to ﬁnd the best transcription corresponding\nto the text in an image. The system typically con-\nsists of a recognition model that produces candidate\ntext sequences conditioned on the input image and\na language model that determines the probability\nof these sequences in the target language. We use\na general-purpose OCR system (detailed in Sec-\ntion 4) to produce a ﬁrst pass transcription of the\nendangered language text in the image. Let this be\na sequence of characters x = [x1, . . . , xN].\nOCR\npost-correction\nThe\ngoal\nof\npost-\ncorrection is to reduce recognition errors in the\nﬁrst pass transcription — often caused by low\nquality scanning, physical deterioration of the\npaper book, or diverse layouts and typefaces (Dong\nand Smith, 2018). The focus of our work is on\nusing post-correction to counterbalance the lack\nof OCR training data in the target endangered\nlanguages.\nThe correction model takes x as\ninput and produces the ﬁnal transcription of the\nendangered language document, a sequence of\ncharacters y = [y1, . . . , yK].\ny = arg max\ny′\npcorr(y′∣x)\nIncorporating translations\nWe use information\nfrom high-resource translations of the endangered\nlanguage text. These translations are commonly\nfound within the same paper book or linguis-\ntic archive (e.g., Figure 1).\nWe use an exist-\ning OCR system to obtain a transcription of the\nscanned translation, a sequence of characters t =\n[t1, . . . , tM]. This is used to condition the model:\ny = arg max\ny′\npcorr(y′∣x, t)\n2.2\nEndangered Language Documents\nWe explore online archives to determine how many\nscanned documents in endangered languages exist\nas potential sources for data extraction (as of this\nwriting, October 2020).\nThe Internet Archive,2 a general-purpose archive\nof web content, has scanned books labeled with the\nlanguage of their content. We ﬁnd 11,674 books la-\nbeled with languages classiﬁed as “endangered” by\nUNESCO. Additionally, we ﬁnd that endangered\nlanguage linguistic archives contain thousands of\ndocuments in PDF format — the Archive of the\nIndigenous Languages of Latin America (AILLA)3\ncontains ≈10,000 such documents and the Endan-\ngered Languages Archive (ELAR)4 has ≈7,000.\nHow common are translations? As described in\nthe introduction, endangered language documents\noften contain a translation into another (usually\nhigh-resource) language. While it is difﬁcult to es-\ntimate the number of documents with translations,\nmultilingual documents represent the majority in\nthe archives we examined; AILLA contains 4,383\nPDFs with bilingual text and 1,246 PDFs with trilin-\ngual text, while ELAR contains ≈5,000 multilin-\ngual documents. The structure of translations in\nthese documents is varied, from dictionaries and\ninterlinear glosses to scanned multilingual books.\n3\nBenchmark Dataset\nFrom the sources described above, we select docu-\nments from three critically endangered languages5\nfor annotation — Ainu, Griko, and Yakkha. These\nlanguages were chosen in an effort to create a ge-\nographically, typologically, and orthographically\ndiverse benchmark. We focus this initial study\non scanned images of printed books as opposed\nto handwritten notes, which are a relatively more\nchallenging domain for OCR.\nWe manually transcribed the text correspond-\ning to the endangered language content. The text\ncorresponding to the translations is not manually\ntranscribed. We also aligned the endangered lan-\nguage text to the OCR output on the translations,\nper the formulation in Section 2.1. We describe the\nannotated documents below and example images\nfrom our dataset are in Figure 1 (a), (b), (c).\nAinu is a severely endangered indigenous lan-\nguage from northern Japan, typically considered\n2https://archive.org/\n3https://ailla.utexas.org\n4https://elar.soas.ac.uk/\n5UNESCO deﬁnes critically endangered languages as\nthose where the youngest speakers are grandparents and older,\nand they speak the language partially and infrequently.\na language isolate. In our dataset, we use a book\nof Ainu epic poetry (yukara), with the “Kutune\nShirka” yukara (Kindaichi, 1931) in Ainu tran-\nscribed in Latin script.6 Each page in the book\nhas a two-column structure — the left column has\nthe Ainu text, and the right has its Japanese trans-\nlation already aligned at the line-level, removing\nthe need for manual alignment (see Figure 1 (a)).\nThe book has 338 pages in total. Given the effort\ninvolved in annotation, we transcribe the Ainu text\nfrom 33 pages, totaling 816 lines.\nGriko is an endangered Greek dialect spoken in\nsouthern Italy. The language uses a combination\nof the Latin alphabet and the Greek alphabet as its\nwriting system. The document we use is a book of\nGriko folk tales compiled by Stomeo (1980). The\nbook is structured such that in each fold of two\npages, the left page has Griko text, and the right\npage has the corresponding translation in Italian.\nOf the 175 pages in the book, we annotate the\nGriko text from 33 pages and manually align it at\nthe sentence-level to the Italian translation. This\nresults in 807 annotated Griko sentences.\nYakkha is an endangered Sino-Tibetan language\nspoken in Nepal. It uses the Devanagari writing\nsystem.\nWe use scanned images of three chil-\ndren’s books, each of which has a story written\nin Yakkha along with its translation in Nepali and\nEnglish (Schackow, 2012). We manually transcribe\nthe Yakkha text from all three books. We also align\nthe Yakkha text to both the Nepali and the English\nOCR at the sentence level with the help of an exist-\ning Yakkha dictionary (Schackow, 2015). In total,\nwe have 159 annotated Yakkha sentences.\n4\nOCR Systems: Promises and Pitfalls\nAs brieﬂy alluded to in the introduction, training an\nOCR model for each endangered language is chal-\nlenging, given the limited available data. Instead,\nwe use the general-purpose OCR system from the\nGoogle Vision AI toolkit7 to get the ﬁrst pass OCR\ntranscription on our data.\nThe Google Vision OCR system (Fujii et al.,\n2017; Ingle et al., 2019) is highly performant and\nsupports 60 major languages in 29 scripts. It can\ntranscribe a wide range of higher resource lan-\nguages with high accuracy, ideal for our proposed\nmethod of incorporating high-resource translations\n6Some transcriptions of Ainu also use the Katakana script.\nSee Howell (1951) for a discussion on Ainu folklore.\n7https://cloud.google.com/vision\nLanguage\nCER\nWER\nAinu\n1.34\n6.27\nGriko\n3.27\n15.63\nYakkha\n8.90\n31.64\nTable 1: Character error rate and word error rate with\nthe Google Vision OCR system on our dataset.\ninto the post-correction model. Moreover, it is par-\nticularly well-suited to our task because it provides\nscript-speciﬁc OCR models in addition to language-\nspeciﬁc ones. Per-script models are more robust\nto unknown languages because they are trained\non data from multiple languages and can act as a\ngeneral character recognizer without relying on a\nsingle language’s model. Since most endangered\nlanguages adopt standard scripts (often from the\nregion’s dominant language) as their writing sys-\ntems, the per-script recognition models can provide\na stable starting point for post-correction.\nThe metrics we use for evaluating performance\nare character error rate (CER) and word error rate\n(WER), representing the ratio of erroneous char-\nacters or words in the OCR prediction to the total\nnumber in the annotated transcription. More de-\ntails are in Section 6. The CER and WER using the\nGoogle Vision OCR on our dataset are in Table 1.\n4.1\nOCR Performance\nAcross the three languages, the error rates indicate\nthat we have a ﬁrst pass transcription that is of rea-\nsonable quality, giving our post-correction method\na reliable starting point. We note the particularly\nlow CER for the Ainu data, reﬂecting previous\nwork that has evaluated the Google Vision system\nto have strong performance on typed Latin script\ndocuments (Fujii et al., 2017). However, there re-\nmains considerable room for improvement in both\nCER and WER for all three languages.\nNext, we look at the edit distance between the\npredicted and the gold transcriptions, in terms of\ninsertion, deletion, and replacement operations. Re-\nplacement accounts for over 84% of the errors in\nthe Griko and Ainu datasets, and 55% overall. This\npattern is expected in the OCR task, as the recogni-\ntion model uses the image to make predictions and\nis more likely to confuse a character’s shape for an-\nother than to hallucinate or erase pixels. However,\nwe observe that the errors in the Yakkha dataset do\nnot follow this pattern. Instead, 87% of the errors\nfor Yakkha occur because of deleted characters.\nOCR\n−−−→\nexi i kaddin`ara\nOCR\n−−−→\n_खा!िनङ् गो\nFigure 2: Examples of errors in Griko (top) and Yakkha\n(bottom) when using the Google Vision OCR.\n4.2\nTypes of Errors\nTo better understand the challenges posed by the\nendangered language setting, we manually inspect\nall the errors made by the OCR system. While\nsome errors are commonly seen in the OCR task,\nsuch as misidentiﬁed punctuation or incorrect word\nboundaries, 85% of the total errors occur due to\nspeciﬁc characteristics of endangered languages\nthat general-purpose OCR systems do not account\nfor. Broadly, they can be categorized into two types,\nexamples of which are shown in Figure 2:\n• Mixed scripts\nThe existing scripts that most\nendangered languages adopt as writing systems\nare often not ideal for comprehensively represent-\ning the language. For example, the Devanagari\nscript does not have a grapheme for the glottal\nstop — as a solution, printed texts in the Yakkha\nlanguage use the IPA symbol ‘P’ (Schackow,\n2015). Similarly, both Greek and Latin charac-\nters are used to write Griko. The Google Vision\nOCR is trained to detect script at the line-level\nand is not equipped to handle multiple scripts\nwithin a single word. As seen in Figure 2, the\nsystem does not recognize the Greek character χ\nin Griko and the IPA symbol P in Yakkha. Mixed\nscripts cause 11% of the OCR errors.\n• Uncommon characters and diacritics\nEn-\ndangered languages often use graphemes and di-\nacritics that are part of the standard script but are\nnot commonly seen in high-resource languages.\nSince these are likely rare in the OCR system’s\ntraining data, they are frequently misidentiﬁed,\naccounting for 74% of the errors. In Figure 2,\nwe see that the OCR system substitutes the un-\ncommon diacritic d. in Griko. The system also\ndeletes the Yakkha character ङ्, which is a ‘half\nform’ alphabet that is infrequent in several other\nDevanagari script languages (such as Hindi).\n5\nOCR Post-Correction Model\nIn this section, we describe our proposed OCR\npost-correction model. The base architecture of\nthe model is a multi-source sequence-to-sequence\nAinu\nOCR\nx1 . . . xN\nencoder\nhx\n1 . . . hx\nN\nJapanese\nOCR\nt1 . . . tM\nencoder\nht\n1 . . . ht\nM\nattention\nattention\nc1 . . . cK\ndecoder\ns1 . . . sK\nsoftmax\nP(y1 . . . yK)\nFigure 3: The proposed multi-source architecture with\nthe encoder for an endangered language segment (left)\nand an encoder for the translated segment (right). The\ninput to the encoders is the ﬁrst pass OCR over the\nscanned images of each segment.\nFor example, the\nOCR on the scanned images of some Ainu text (left)\nand its Japanese translation (right).\nframework (Zoph and Knight, 2016; Libovick´y and\nHelcl, 2017) that uses an LSTM encoder-decoder\nmodel with attention (Bahdanau et al., 2015). We\npropose improvements to training and modeling for\nthe multi-source architecture, speciﬁcally tailored\nto ease learning in data-scarce settings.\n5.1\nMulti-source Architecture\nOur post-correction formulation takes as input the\nﬁrst pass OCR of the endangered language segment\nx and the OCR of the translated segment t, to\npredict an error-free endangered language text y.\nThe model architecture is shown in Figure 3.\nThe model consists of two encoders — one that\nencodes x and one that encodes t. Each encoder is\na character-level bidirectional LSTM (Hochreiter\nand Schmidhuber, 1997) and transforms the input\nsequence of characters to a sequence of hidden\nstate vectors: hx for the endangered language text\nand ht for the translation.\nThe model uses an attention mechanism during\nthe decoding process to use information from the\nencoder hidden states. We compute the attention\nweights over each of the two encoders indepen-\ndently. At the decoding time step k:\nex\nk,i = vx tanh (Wx\n1sk−1 + Wx\n2hx\ni )\n(1)\nαx\nk = softmax (ex\nk)\ncx\nk = [Σiαx\nk,ihx\ni ]\nwhere sk−1 is the decoder state of the previous time\nstep and vx, Wx\n1 and Wx\n2 are trainable parameters.\nThe encoder hidden states hx are weighted by the\nattention distribution αx\nk to produce the context\nvector cx\nk. We follow a similar procedure for the\nsecond encoder to produce ct\nk. We concatenate\nthe context vectors to combine attention from both\nsources (Zoph and Knight, 2016):\nck = [cx\nk; ct\nk]\nck is used by the decoder LSTM to compute the\nnext hidden state sk and subsequently, the proba-\nbility distribution for predicting the next character\nyk of the target sequence y:\nsk = lstm (sk−1, ck, yk−1)\n(2)\nP (yk) = softmax (Wsk + b)\n(3)\nTraining and Inference\nThe model is trained for\neach language with the cross-entropy loss (Lce)\non the small amount of transcribed data we have.\nBeam search is used at inference time.\n5.2\nModel and Training Improvements\nWith the minimal annotated data we have, it is\nchallenging for the neural network to learn a good\ndistribution over the target characters. We propose\na set of adaptations to the base architecture that\nimproves the post-correction performance without\nadditional annotation. The adaptations are based\non characteristics of the OCR task itself and the\nperformance of the upstream OCR tool (Section 4).\nDiagonal attention loss\nAs seen in Section 4,\nsubstitution errors are more frequent in the OCR\ntask than insertions or deletions; consequently,\nwe expect the source and target to have similar\nlengths. Moreover, post-correction is a monotonic\nsequence-to-sequence task, and reordering rarely\noccurs (Schnober et al., 2016). Hence, we expect\nattention weights to be higher at characters close to\nthe diagonal for the endangered language encoder.\nWe modify the model such that all the elements\nin the attention vector that are not within j steps\n(we use j = 3) of the current time step k are added\nto the training loss, thereby encouraging elements\naway from the diagonal to have lower values. The\ndiagonal loss summed over all time steps for a\ntraining instance, where N is the length of x, is:\nLdiag = ∑\nk\n⎛\n⎜\n⎝\nk−j\n∑\ni=1\nαx\nk,i +\nN\n∑\ni=k+j\nαx\nk,i\n⎞\n⎟\n⎠\nCopy mechanism\nTable 1 indicates that the ﬁrst\npass OCR predicts a majority of the characters\naccurately. In this scenario, enabling the model to\ndirectly copy characters from the ﬁrst pass OCR\nrather than generate a character at each time step\nmight lead to better performance (Gu et al., 2016).\nWe incorporate the copy mechanism proposed\nin See et al. (2017). The mechanism computes a\n“generation probability” at each time step k, which\nis used to choose between generating a character\n(Equation 3) or copying a character from the source\ntext by sampling from the attention distribution αx\nk.\nCoverage\nGiven the monotonicity of the post-\ncorrection task, the model should not attend to the\nsame character repeatedly. However, repetition is a\ncommon problem for neural encoder-decoder mod-\nels (Mi et al., 2016; Tu et al., 2016). To account for\nthis problem, we adapt the coverage mechanism\nfrom See et al. (2017), which keeps track of the\nattention distribution over past time steps in a cov-\nerage vector. For time step k, the coverage vector\nwould be gk = ∑k−1\nk′=0 αx\nk′.\ngk is used as an extra input to the attention mech-\nanism, ensuring that future attention decisions take\nthe weights from previous time steps into account.\nEquation 1, with learnable parameter wg, becomes:\nex\nk,i = vx tanh (Wx\n1sk−1 + Wx\n2hx\ni + wggk,i)\ngk is also used to penalize attending to the same\nlocations repeatedly with a coverage loss. The\ncoverage loss summed over all time steps k is:\nLcov = ∑\nk\nn\n∑\ni=1\nmin (αx\nk,i, gk,i)\nTherefore, with our model adaptations, the loss for\na single training instance:\nL = Lce + Ldiag + Lcov\n(4)\n5.3\nUtilizing Untranscribed Data\nAs discussed in Section 3, given the effort in-\nvolved, we transcribe only a subset of the pages in\neach scanned book. Nonetheless, we leverage the\nremaining unannotated pages for pretraining our\nmodel. We use the upstream OCR tool to get a ﬁrst\npass transcription on all the unannotated pages.\nWe then create “pseudo-target” transcriptions for\nthe endangered language text as described below:\n• Denoising rules\nUsing a small fraction of\nthe available annotated pages, we compute\nthe edit distance operations between the ﬁrst\npass OCR and the gold transcription. The\noperations of each type (insertion, deletion,\nand replacement) are counted for each char-\nacter and divided by the number of times that\ncharacter appears in the ﬁrst pass OCR. This\nforms a probability of how often the operation\nshould be applied to that speciﬁc character.\nWe use these probabilities to form rules, such\nas p(replace d with d.) = 0.4 for Griko and\np(replace ? with P) = 0.7 for Yakkha. These\nrules are applied to remove errors from, or\n“denoise”, the ﬁrst pass OCR transcription.\n• Sentence alignment\nWe use Yet Another\nSentence Aligner (Lamraoui and Langlais,\n2013) for unsupervised alignment of the en-\ndangered language and translation on the\nunannotated pages.\nGiven the aligned ﬁrst pass OCR for the endan-\ngered language text and the translation along with\nthe pseudo-target text, x, t and ˆy respectively, the\npretraining steps, in order, are:\n• Pretraining the encoders\nWe pretrain both\nthe forward and backward LSTMs of each\nencoder with a character-level language model\nobjective: the endangered language encoder\non x and the translation encoder on t.\n• Pretraining the decoder\nThe decoder is\npretrained on the pseudo-target ˆy with a\ncharacter-level language model objective.\n• Pretraining the seq-to-seq model\nThe\nmodel is pretrained with x and t as the sources\nand the pseudo-target ˆy as the target transcrip-\ntion, using the post-correction loss function L\nas deﬁned in Equation 4.\n6\nExperiments\nThis section discusses our experimental setup and\nthe post-correction performance on the three en-\ndangered languages on our dataset.\n6.1\nExperimental Setup\nData Splits\nWe perform 10-fold cross-validation\nfor all experimental settings because of the small\nsize of the datasets. For each language, we divide\nthe transcribed data into 11 segments — we use one\nsegment for creating the denoising rules described\nin the previous section and the remaining ten as the\nCharacter Error Rate\nWord Error Rate\nAinu\nGriko\nYakkha\nAinu\nGriko\nYakkha\nModel\nMulti Single\nMulti Single\nMulti Single\nMulti Single\nMulti Single\nMulti Single\nFP-OCR\n–\n1.34\n–\n3.27\n–\n8.90\n–\n6.27\n–\n15.63\n–\n31.64\nBASE\n1.56\n1.41\n6.78\n5.95\n70.39 71.71\n8.56\n7.88\n15.13 13.67\n98.15 99.10\nCOPY\n2.04\n1.99\n2.54\n2.28\n14.77 12.30\n9.48\n8.57\n9.33\n8.90\n30.36 27.81\nOURS\n0.92\n0.80\n1.66\n1.70\n7.75\n8.44\n5.75\n5.19\n7.46\n7.51\n20.95 21.33\nTable 2: Our method improves performance over all baselines (10-fold cross-validation averaged over ﬁve ran-\ndomly seeded runs). We present multi- and single-source variants and highlight the best model for each language.\nfolds for cross-validation. In each cross-validation\nfold, eight segments are used for training, one for\nvalidation and one for testing.\nWe divide the dataset at the page-level for the\nAinu and Griko documents, resulting in 11 seg-\nments of three pages each. For the Yakkha docu-\nments, we divide at the paragraph-level, due to the\nsmall size of the dataset. We have 33 paragraphs\nacross the three books in our dataset, resulting in 11\nsegments that contain three paragraphs each. The\nmulti-source results for Yakkha reported in Table 2\nuse the English translations. Results with Nepali\nare similar and are included in Appendix A.\nMetrics\nWe use two metrics for evaluating our\nsystems: character error rate (CER) and word error\nrate (WER). Both metrics are based on edit distance\nand are standard for evaluating OCR and OCR post-\ncorrection (Berg-Kirkpatrick et al., 2013; Schulz\nand Kuhn, 2017). CER is the edit distance between\nthe predicted and the gold transcriptions of the doc-\nument, divided by the total number of characters\nin the gold transcription. WER is similar but is\ncalculated at the word level.\nMethods\nIn our experiments, we compare the\nperformance of our proposed method with the ﬁrst\npass OCR and with two systems from recent work\nin OCR post-correction. All the post-correction\nmethods have two variants – the single-source\nmodel with only the endangered language encoder\nand the multi-source model that additionally uses\nthe high-resource translation encoder.\n• FP-OCR: The ﬁrst pass transcription obtained\nfrom the Google Vision OCR system.\n• BASE: This system is the base sequence-to-\nsequence architecture described in Section 5.1.\nBoth the single-source and multi-source vari-\nants of this system are used for English OCR\npost-correction in Dong and Smith (2018).\n• COPY: This system is the base architecture\nwith a copy mechanism as described in Sec-\ntion 5.2. The single-source variant of this\nmodel is used for OCR post-correction on Ro-\nmanized Sanskrit in Krishna et al. (2018).8\n• OURS: The model with all the adaptations\nproposed in Section 5.2 and Section 5.3.\nImplementation\nThe post-correction models are\nimplemented using the DyNet neural network\ntoolkit (Neubig et al., 2017), and all reported re-\nsults are the average of ﬁve training runs with dif-\nferent random seeds. We assume knowledge of\nthe entire alphabet of the endangered language for\nall the methods, which is straightforward to ob-\ntain for most languages. The decoder’s vocabulary\ncontains all these characters, irrespective of their\npresence in the training data, with corresponding\nrandomly-initialized character embeddings.\n6.2\nMain Results\nTable 2 shows the performance of the baselines and\nour proposed method for each language. Overall,\nour method results in an improved CER and WER\nover existing methods across all three languages.\nThe BASE system does not improve the recog-\nnition rate over the ﬁrst pass transcription, apart\nfrom a small decrease in the Griko WER. The per-\nformance on Yakkha, particularly, is signiﬁcantly\nworse than FP-OCR: likely because the data size\nof Yakkha is much smaller than that of Griko and\nAinu, and the model is unable to learn a reasonable\ndistribution. However, on adding a copy mecha-\nnism to the base model in the COPY system, the\nperformance is notably better for both Griko and\nYakkha. This indicates that adaptations to the base\nmodel that cater to speciﬁc characteristics of the\n8Although Krishna et al. (2018) use BPE tokenization,\npreliminary experiments showed that character-level models\nresult in much better performance on our dataset, likely due\nto the limited data available for training the BPE model.\nErrors ﬁxed by post-correction\n(a) Griko\n(b) Yakkha\n[Image]\nÈÈ↓\nÈÈ↓\n[First pass OCR]\nexi i kaddin`ara\n_खा!िनङ् गो\nÈÈ↓\nÈÈ↓\n[Post-corrected]\neχi i kad. d. in`ara\n\"#  खा$िनङ् गो\nErrors introduced by post-correction\n(c) Griko\n(d) Yakkha\nÈÈ↓\nÈÈ↓\n`e ffacilo\nहाङ्चाङ्चाङ्\nÈÈ↓\nÈÈ↓\n`e ffa´cilo\nहाङ् चाङ्  %&'(\nFigure 4: Our model ﬁxes many mixed script and uncommon diacritics errors such as (a) and (b). In rare cases, it\n“over-corrects” the ﬁrst pass OCR transcription, introducing errors such as (c) and (d).\n0\n2\n4\n6\n8\nall\n-diag\n-copy\n-coverage\n-pretr. dec\n-pretr. enc\n-pretr. s2s\n5.19\n5.49\n6.56\n5.6\n6.86\n6.7\n5.65\nAinu\n0 2 4 6 8 10\n7.46\n8.06\n8.66\n10.19\n7.87\n9.47\n9.43\nGriko\n0\n10\n20\n30\n40\nall\n-diag\n-copy\n-coverage\n-pretr. dec\n-pretr. enc\n-pretr. s2s\n24.29\n22.73\n37.83\n26.71\n20.95\n28.41\n27.68\nWord Error Rate\nYakkha\nFigure 5: WER with model component ablations on\nthe best model setting in Table 2. “all” includes all the\nadaptations we propose. Each ablation removes a sin-\ngle component from the “all” model, e.g. “-pretr. s2s”\nremoves the seq-to-seq model pretraining.\npost-correction task can alleviate some of the chal-\nlenges of learning from small amounts of data.\nThe single-source and the multi-source variants\nof our proposed method improve over the baselines,\ndemonstrating that our proposed model adaptations\ncan improve recognition even without translations.\nWe see that using the high-resource translations\nresults in better post-correction performance for\nGriko and Yakkha, but the single-source model\nachieves better accuracy for Ainu. We attribute\nthis to two factors: the very low error rate of the\nﬁrst pass transcription for Ainu and the relatively\nhigh error rate (based on manual inspection) of the\nOCR on the Japanese translation. Despite being\na high-resource language, OCR is difﬁcult due to\nthe complexity of Japanese characters and low scan\nquality. The noise resulting from the Japanese OCR\nerrors likely hurts the multi-source model.\n6.3\nAblation Studies\nNext, we study the effect of our proposed adapta-\ntions and evaluate their beneﬁt to the performance\nof each language. Figure 5 shows the word error\nrate with models that remove one adaptation from\nthe model with all the adaptations (“all”).\nFor Ainu and Griko, removing any single compo-\nnent increases the WER, with the complete (“all”)\nmethod performing the best. There is little variance\nin the Ainu ablations, likely due to the high-quality\nﬁrst pass transcription.\nOur proposed adaptations add the most beneﬁt\nfor Yakkha, which has the fewest training data and\nrelatively less accurate ﬁrst pass OCR. The copy\nmechanism is crucial for good performance, but re-\nmoving the decoder pretraining (“pretr. dec”) leads\nto the best scores among all the ablations. The de-\nnoising rules used to create the pseudo-target data\nfor Yakkha are likely not accurate since they are\nderived from only three paragraphs of annotated\ndata. Consequently, using it to pretrain the decoder\nleads to a poor language model.\n6.4\nError Analysis\nWe systematically inspect all the recognition errors\nin the output of our post-correction model to deter-\nmine the sources of improvement with respect to\nthe ﬁrst pass OCR. We also examine the types of\nerrors introduced by the post-correction process.\nWe observe a 91% reduction in the number of\nerrors due to mixed scripts and a 58% reduction\nin the errors due to uncommon characters and dia-\ncritics (as deﬁned in Section 4). Examples of these\nare shown in Figure 4 (a) and (b): mixed script\nerrors such as the χ character in Griko and the\nglottal stop P in Yakkha are successfully corrected\nby the model. The model is also able to correct\nuncommon character errors like d. in Griko and ङ्\nin Yakkha.\nExamples of errors introduced by the model are\nshown in Figure 4 (c) and (d). Example (c) is in\nGriko, where the model incorrectly adds a diacritic\nto a character. We attribute this to the fact that the\nﬁrst pass OCR does not recognize diacritics well;\nhence, the model learns to add diacritics frequently\nwhile generating the output. Example (d) is in\nYakkha. The model inserts several incorrect char-\nacters, and can likely be attributed to the lack of a\ngood language model due to the relatively smaller\namount of training data we have in Yakkha.\n7\nRelated Work\nPost-correction for OCR is well-studied for high-\nresource languages. Early approaches include lexi-\ncal methods and weighted ﬁnite-state methods (see\nSchulz and Kuhn (2017) for an overview). Re-\ncent work has primarily focused on using neural\nsequence-to-sequence models. H¨am¨al¨ainen and\nHengchen (2019) use a BiLSTM encoder-decoder\nwith attention for historical English post-correction.\nSimilar to our base model, Dong and Smith (2018)\nuse a multi-source model to combine the ﬁrst pass\nOCR from duplicate documents in English.\nThere has been little work on lower-resourced\nlanguages. Kolak and Resnik (2005) present a\nprobabilistic edit distance based post-correction\nmodel applied to Cebuano and Igbo, and Krishna\net al. (2018) show improvements on Romanized\nSanksrit OCR by adding a copy mechanism to a\nneural sequence-to-sequence model.\nMulti-source encoder-decoder models have been\nused for various tasks including machine transla-\ntion (Zoph and Knight, 2016; Libovick´y and Helcl,\n2017) and morphological inﬂection (Kann et al.,\n2017; Anastasopoulos and Neubig, 2019). Perhaps\nmost relevant to our work is the multi-source model\npresented by Anastasopoulos and Chiang (2018),\nwhich uses high-resource translations to improve\nspeech transcription of lower-resourced languages.\nFinally, Bustamante et al. (2020) construct cor-\npora for four endangered languages from text-\nbased PDFs using rule-based heuristics. Data cre-\nation from such unstructured text ﬁles is an impor-\ntant research direction, complementing our method\nof extracting data from scanned images.\n8\nConclusion\nThis work presents a ﬁrst step towards extracting\ntextual data in endangered languages from scanned\nimages of paper books. We create a benchmark\ndataset with transcribed images in three endan-\ngered languages: Ainu, Griko, and Yakkha. We\npropose an OCR post-correction method that facili-\ntates learning from small amounts of data, which\nresults in a 34% average relative error reduction in\nboth the character and word recognition rates.\nAs future work, we plan to investigate the effect\nof using other available data for the three languages\n(for example, word lists collected by documentary\nlinguists or the additional Griko folk tales collected\nby Anastasopoulos et al. (2018)).\nAdditionally, it would be valuable to examine\nwhether our method can improve the OCR on high-\nresource languages, which typically have much\nbetter recognition rates in the ﬁrst pass transcription\nthan the endangered languages in our dataset.\nFurther, we note our use of the Google Vi-\nsion OCR system to obtain the ﬁrst pass OCR\nfor our experiments, primarily because it provides\nscript-speciﬁc models as opposed to other general-\npurpose OCR systems that rely on language-\nspeciﬁc models (as discussed in Section 4). Future\nwork that focuses on overcoming the challenges of\napplying language-speciﬁc models to endangered\nlanguage texts would be needed to conﬁrm our\nmethod’s applicability to post-correcting the ﬁrst\npass transcriptions from different OCR systems.\nLastly, given the annotation effort involved, this\npaper explores only a small fraction of the en-\ndangered language data available in linguistic and\ngeneral-purpose archives. Future work will focus\non large-scale digitization of scanned documents,\naiming to expand our OCR benchmark on as many\nendangered languages as possible, in the hope of\nboth easing linguistic documentation and preserva-\ntion efforts and collecting enough data for NLP sys-\ntem development in under-represented languages.\nAcknowledgements\nWe thank David Chiang, Walter Scheirer, and\nWilliam Theisen for initial discussions on the\nproject, the University of Notre Dame Library\nfor the scanned “Kutune Shirka” Ainu-Japanese\nbook, and Josep Quer for the scanned Griko folk-\ntales book. We also thank Taylor Berg-Kirkpatrick,\nShuyan Zhou, Zi-Yi Dou, Yansen Wang, Zhen Fan,\nand Deepak Gopinath for feedback on the paper.\nThis material is based upon work supported in\npart by the National Science Foundation under\nGrant No. 1761548. Shruti Rijhwani is supported\nby a Bloomberg Data Science Ph.D. Fellowship.\nReferences\nAntonios Anastasopoulos and David Chiang. 2018.\nLeveraging translations for speech transcription in\nlow-resource settings. In Proc. INTERSPEECH.\nAntonios Anastasopoulos, Marika Lekakou, Josep\nQuer, Eleni Zimianiti, Justin DeBenedetto, and\nDavid Chiang. 2018. Part-of-speech tagging on an\nendangered language: a parallel Griko-Italian re-\nsource.\nIn Proceedings of the 27th International\nConference on Computational Linguistics, pages\n2529–2539, Santa Fe, New Mexico, USA. Associ-\nation for Computational Linguistics.\nAntonios Anastasopoulos and Graham Neubig. 2019.\nPushing the limits of low-resource morphological in-\nﬂection. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n984–996, Hong Kong, China. Association for Com-\nputational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015.\nNeural machine translation by jointly\nlearning to align and translate.\nIn 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015.\nTaylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.\n2013. Unsupervised transcription of historical docu-\nments. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 207–217, Soﬁa, Bul-\ngaria. Association for Computational Linguistics.\nGina Bustamante,\nArturo Oncevay,\nand Roberto\nZariquiey. 2020.\nNo data to crawl? monolingual\ncorpus creation from PDF ﬁles of truly low-resource\nlanguages in Peru. In Proceedings of The 12th Lan-\nguage Resources and Evaluation Conference, pages\n2914–2923, Marseille, France. European Language\nResources Association.\nMaud Devos. 2019. Shangaji. a maka or swahili lan-\nguage of mozambique. grammar, texts and wordlist.\nhttps://elar.soas.ac.uk/Collection/\nMPI1029699. Accessed: 2020-02-02.\nRui Dong and David Smith. 2018. Multi-input atten-\ntion for unsupervised OCR correction. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2363–2372, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nYasuhisa Fujii, Karel Driesen, Jonathan Baccash, Ash\nHurst, and Ashok C Popat. 2017. Sequence-to-label\nscript identiﬁcation for multilingual ocr.\nIn 2017\n14th IAPR International Conference on Document\nAnalysis and Recognition (ICDAR), volume 1, pages\n161–168. IEEE.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.\nLi. 2016.\nIncorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1:\nLong Papers),\npages 1631–1640, Berlin, Germany. Association for\nComputational Linguistics.\nMika H¨am¨al¨ainen and Simon Hengchen. 2019. From\nthe paft to the ﬁiture: a fully automatic NMT and\nword embeddings method for OCR post-correction.\nIn Proceedings of the International Conference on\nRecent Advances in Natural Language Processing\n(RANLP 2019), pages 431–436, Varna, Bulgaria. IN-\nCOMA Ltd.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997.\nLong short-term memory.\nNeural computation,\n9(8):1735–1780.\nRichard W Howell. 1951. The classiﬁcation and de-\nscription of ainu folklore. The Journal of American\nFolklore, 64(254):361–369.\nR Reeve Ingle, Yasuhisa Fujii, Thomas Deselaers,\nJonathan Baccash, and Ashok C Popat. 2019.\nA\nscalable handwritten text recognition system. arXiv\npreprint arXiv:1904.09150.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld.\nIn Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6282–6293, Online. Association for Computa-\ntional Linguistics.\nKatharina Kann, Ryan Cotterell, and Hinrich Sch¨utze.\n2017. Neural multi-source morphological reinﬂec-\ntion. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers, pages\n514–524, Valencia, Spain. Association for Compu-\ntational Linguistics.\nKy¯osuke Kindaichi. 1931.\nAinu Jojishi Y¯ukara no\nKenky¯u [Research on Ainu Epic Yukar].\nT¯oky¯o:\nT¯oky¯o Bunko.\nOkan Kolak and Philip Resnik. 2005.\nOCR post-\nprocessing for low density languages. In Proceed-\nings of Human Language Technology Conference\nand Conference on Empirical Methods in Natural\nLanguage Processing, pages 867–874, Vancouver,\nBritish Columbia, Canada. Association for Compu-\ntational Linguistics.\nAmrith Krishna, Bodhisattwa P. Majumder, Rajesh\nBhat, and Pawan Goyal. 2018. Upcycle your OCR:\nReusing OCRs for post-OCR text correction in Ro-\nmanised Sanskrit. In Proceedings of the 22nd Con-\nference on Computational Natural Language Learn-\ning, pages 345–355, Brussels, Belgium. Association\nfor Computational Linguistics.\nFethi Lamraoui and Philippe Langlais. 2013. Yet an-\nother fast, robust and open source sentence aligner.\ntime to reconsider sentence alignment? In XIV Ma-\nchine Translation Summit, Nice, France.\nJindˇrich Libovick´y and Jindˇrich Helcl. 2017. Attention\nstrategies for multi-source sequence-to-sequence\nlearning. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 196–202, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nHaitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe\nIttycheriah. 2016. Coverage embedding models for\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 955–960, Austin,\nTexas. Association for Computational Linguistics.\nGraham Neubig, Chris Dyer, Yoav Goldberg, Austin\nMatthews, Waleed Ammar, Antonios Anastasopou-\nlos, Miguel Ballesteros, David Chiang, Daniel Cloth-\niaux, Trevor Cohn, Kevin Duh, Manaal Faruqui,\nCynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng\nKong, Adhiguna Kuncoro, Gaurav Kumar, Chai-\ntanya Malaviya, Paul Michel, Yusuke Oda, Matthew\nRichardson, Naomi Saphra, Swabha Swayamdipta,\nand Pengcheng Yin. 2017.\nDynet:\nThe dy-\nnamic neural network toolkit.\narXiv preprint\narXiv:1701.03980.\nC. Rigaud, A. Doucet, M. Coustaty, and J. Moreux.\n2019.\nICDAR 2019 competition on post-OCR\ntext correction. In 2019 International Conference\non Document Analysis and Recognition (ICDAR),\npages 1588–1593.\nDiana Schackow. 2012.\nDocumentation and gram-\nmatical description of yakkha, nepal.\nhttps:\n//elar.soas.ac.uk/Collection/MPI186180.\nAccessed: 2020-02-02.\nDiana Schackow. 2015. A grammar of Yakkha. Lan-\nguage Science Press.\nCarsten Schnober, Steffen Eger, Erik-Lˆan Do Dinh, and\nIryna Gurevych. 2016. Still not there? comparing\ntraditional sequence-to-sequence models to encoder-\ndecoder neural networks on monotone string trans-\nlation tasks.\nIn Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics:\nTechnical Papers, pages 1703–1714,\nOsaka, Japan. The COLING 2016 Organizing Com-\nmittee.\nSarah Schulz and Jonas Kuhn. 2017. Multi-modular\ndomain-tailored OCR post-correction. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2716–2726,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nPaolo Stomeo. 1980.\nRacconti greci inediti di Ster-\nnat´ıa. La nuova Ellade, s.I.\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,\nand Hang Li. 2016. Modeling coverage for neural\nmachine translation. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 76–\n85, Berlin, Germany. Association for Computational\nLinguistics.\nBarret Zoph and Kevin Knight. 2016.\nMulti-source\nneural translation. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 30–34, San Diego, Cali-\nfornia. Association for Computational Linguistics.\nA\nAppendix\nA.1\nImplementation Details\nThe hyperparameters used are:\n• Character embedding size = 128\n• Number of LSTM layers = 1\n• Hidden state size of the LSTM = 256\n• Attention size = 256\n• Beam size = 4\n• For the diagonal loss, j = 3\n• Minibatch size for training = 1\n• Maximum number of epochs = 150\n• Patience for early stopping = 10 epochs\n• Pretraining epochs for encoder/decoder = 10\n• Pretraining epochs for seq-to-seq model = 5\nWe use the same values of the hyperparameters for\neach language and all the systems. We select the\nbest model with early stopping on the character\nerror rate of the validation set.\nA.2\nAdditional Experimental Results\nPerformance on Yakkha with Nepali\nTable 3\nshows the performance for the Yakkha dataset\nwhen using Nepali as the high-resource translation\ninput to the multisource model. The performance\nis similar to those of the experiments using the\nEnglish translations, as presented in Table 2.\nStandard deviation on the main results\nTa-\nble 4 and Table 5 show the character error rate and\nword error rate respectively including the standard\ndeviation over ﬁve randomly seeded runs, corre-\nsponding to the systems described in Table 2.\nModel\nCER\nWER\nFP-OCR\n8.90\n31.64\nBASE\n70.89\n100.00\nCOPY\n11.60\n26.74\nOURS\n7.95\n20.83\nTable 3: Character error rate (CER) and word error\nrate (WER) for the Yakkha dataset with the multi-\nsource model that uses the OCR on Nepali as the high-\nresource translation. The table shows the mean over\nﬁve random runs.\n(a) Ainu\nModel\nMulti\nSingle\nFP-OCR\n–\n1.34\nBASE\n1.56 ± 0.23\n1.41 ± 0.16\nCOPY\n2.04 ± 0.62\n1.99 ± 0.41\nOURS\n0.92 ± 0.05 0.80 ± 0.07\n(b) Griko\nModel\nMulti\nSingle\nFP-OCR\n–\n3.27\nBASE\n6.78 ± 0.62 5.95 ± 0.52\nCOPY\n2.54 ± 0.31 2.28 ± 0.28\nOURS\n1.66 ± 0.03 1.70 ± 0.21\n(c) Yakkha\nModel\nMulti\nSingle\nFP-OCR\n–\n8.90\nBASE\n70.39 ± 0.49 71.71 ± 0.71\nCOPY\n14.77 ± 0.97 12.30 ± 2.39\nOURS\n7.75 ± 0.46\n8.44 ± 0.90\nTable 4: Mean and standard deviation of the character\nerror rate with 10-fold cross-validation over ﬁve ran-\ndom seeds. The results presented are the same as Ta-\nble 2 with the added information of standard deviation.\nThe best models for each language are highlighted.\n(a) Ainu\nModel\nMulti\nSingle\nFP-OCR\n–\n6.27\nBASE\n8.56 ± 1.01\n7.88 ± 0.64\nCOPY\n9.48 ± 3.07\n8.57 ± 1.45\nOURS\n5.75 ± 0.24 5.19 ± 0.31\n(b) Griko\nModel\nMulti\nSingle\nFP-OCR\n–\n15.63\nBASE\n15.13 ± 0.99 13.67 ± 1.17\nCOPY\n9.33 ± 0.49\n8.90 ± 0.51\nOURS\n7.46 ± 0.09\n7.51 ± 0.31\n(c) Yakkha\nModel\nMulti\nSingle\nFP-OCR\n–\n31.64\nBASE\n98.15 ± 1.55 99.10 ± 2.20\nCOPY\n30.36 ± 1.39 27.81 ± 1.65\nOURS\n20.95 ± 1.04 21.33 ± 0.53\nTable 5: Mean and standard deviation of the word er-\nror rate with 10-fold cross-validation over ﬁve random\nseeds. The results presented are the same as Table 2\nwith the added information of standard deviation. The\nbest models for each language are highlighted.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-11-10",
  "updated": "2020-11-10"
}