{
  "id": "http://arxiv.org/abs/2011.05499v2",
  "title": "Unsupervised Learning of Dense Visual Representations",
  "authors": [
    "Pedro O. Pinheiro",
    "Amjad Almahairi",
    "Ryan Y. Benmalek",
    "Florian Golemo",
    "Aaron Courville"
  ],
  "abstract": "Contrastive self-supervised learning has emerged as a promising approach to\nunsupervised visual representation learning. In general, these methods learn\nglobal (image-level) representations that are invariant to different views\n(i.e., compositions of data augmentation) of the same image. However, many\nvisual understanding tasks require dense (pixel-level) representations. In this\npaper, we propose View-Agnostic Dense Representation (VADeR) for unsupervised\nlearning of dense representations. VADeR learns pixelwise representations by\nforcing local features to remain constant over different viewing conditions.\nSpecifically, this is achieved through pixel-level contrastive learning:\nmatching features (that is, features that describes the same location of the\nscene on different views) should be close in an embedding space, while\nnon-matching features should be apart. VADeR provides a natural representation\nfor dense prediction tasks and transfers well to downstream tasks. Our method\noutperforms ImageNet supervised pretraining (and strong unsupervised baselines)\nin multiple dense prediction tasks.",
  "text": "Unsupervised Learning of Dense\nVisual Representations\nPedro O. Pinheiro1, Amjad Almahairi, Ryan Y. Benmalek2,\nFlorian Golemo13, Aaron Courville34\n1Element AI, 2Cornell University, 3Mila, Université de Montréal,\n4CIFAR Fellow\nAbstract\nContrastive self-supervised learning has emerged as a promising approach to\nunsupervised visual representation learning.\nIn general, these methods learn\nglobal (image-level) representations that are invariant to different views (i.e.,\ncompositions of data augmentation) of the same image. However, many visual\nunderstanding tasks require dense (pixel-level) representations. In this paper, we\npropose View-Agnostic Dense Representation (VADeR) for unsupervised learning\nof dense representations. VADeR learns pixelwise representations by forcing local\nfeatures to remain constant over different viewing conditions. Speciﬁcally, this\nis achieved through pixel-level contrastive learning: matching features (that is,\nfeatures that describes the same location of the scene on different views) should be\nclose in an embedding space, while non-matching features should be apart. VADeR\nprovides a natural representation for dense prediction tasks and transfers well to\ndownstream tasks. Our method outperforms ImageNet supervised pretraining (and\nstrong unsupervised baselines) in multiple dense prediction tasks.\n1\nIntroduction\nSince the introduction of large-scale visual datasets like ImageNet [9], most success in computer\nvision has been primarily driven by supervised learning. Unfortunately, most successful approaches\nrequire large amounts of labeled data, making them expensive to scale. In order to take advantage\nof the huge amounts of unlabeled data and break this bottleneck, unsupervised and semi-supervised\nlearning methods have been proposed.\nRecently, self-supervised methods based on contrastive learning [23] have shown promising results\non computer vision problems [73, 55, 30, 80, 29, 63, 1, 26, 49, 5]. Contrastive approaches learn\na similarity function between views of images—bringing views of the same image closer in a\nrepresentation space, while pushing views of other images apart. The deﬁnition of a view varies\nfrom method to method but views are typically drawn from the set of data augmentation procedures\ncommonly used in computer vision.\nCurrent contrastive self-supervision methods have one thing in common: similarity scores are\ncomputed between global representations of the views (usually by a global pooling operation on the\nﬁnal convolutional layer). See Figure 1a. Global representations are efﬁcient to compute but provide\nlow-resolution features that are invariant to pixel-level variations. This might be sufﬁcient for few\ntasks like image classiﬁcation, but are not enough for dense prediction tasks.\nDense representations, contrary to their global counterpart, yield encodings at pixel level. They provide\na natural way to leverage intrinsic spatial structure in visual perception [70] during training (e.g.,\nnearby pixels tend to share similar appearances, object boundaries have strong gradients, object-centric\nproperties). Moreover, many—arguably, most—visual understanding tasks rely on structured, dense\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2011.05499v2  [cs.CV]  7 Dec 2020\n(a)\n(b)\n\"\n\"\n#\n#\nFigure 1: Contrastive methods learn representations by ﬁrst generating two (correlated) views of a same\nscene and then bringing those views closer in an embedding space, while pushing views of other scenes\napart. (a) Current methods compute similarity on global, image-level representations by applying\na pooling operation on the output of the feature encoder. (b) VADeR, on the other hand, utilizes an\nencoder-decoder architecture and computes similarity on pixel-level representations. VADeR uses\nknown pixel correspondences, derived from the view generation process, to match local features.\nrepresentations (e.g. pixel-level segmentation, depth prediction, optical-ﬂow prediction, keypoint\ndetection, visual correspondence).\nIn this paper, we propose a method for unsupervised learning of dense representations. The key\nidea is to leverage perceptual constancy [56]—the idea that local visual representations should\nremain constant over different viewing conditions—as a supervisory signal to train a neural network.\nPerceptual constancy is ubiquitous in visual perception and provides a way to represent the world\nin terms of invariants of optical structure [18, 69]. The local representations of a scene (e.g., an eye\nof a dog) should remain invariant with respect to which viewpoint the scene is been observed.\nOur method—View-Agnostic Dense Representation (VADeR)—imposes perceptual constancy by\ncontrasting local representations. Here, representations of matching features (that is, features that\ndescribe the same location of a scene on different views) should be close in an embedding space, while\nnon-matching features should be apart. Our method leverages known pixel correspondences, derived\nfrom the view generation process, to ﬁnd matching features in each pair of views. VADeR can be seen\nas a generalization of previous contrastive self-supervised methods in the sense that it learns dense\n(i.e., per-pixel) features instead of global ones. Figure 1b describes our general approach and compares\nit to common contrastive approaches.\nVADeR provides a natural representation for dense prediction tasks. We evaluate its performance by see-\ninghowthelearnedfeaturescanbetransferredtodownstreamtasks, eitherasfeatureextractororusedfor\nﬁne-tuning. We show that (unsupervised) contrastive learning of dense representation are more effective\nthan its global counterparts in many visual understanding tasks (instance and semantic segmentation,\nobject detection, keypoint detection, correspondence and depth prediction). Perhaps more interestingly,\nVADeR unsupervised pretraining outperforms ImageNet supervised pretraining at different tasks.\n2\nRelated Work\nSelf-supervised learning.\nSelf-supervised learning is a form of unsupervised learning that leverages\nthe intrinsic structure of data as supervisory signal for training. It consists of formulating a predictive\npretext task and learning features in a way similar to supervised learning. A vast range of pretext tasks\nhave been recently proposed. For instance, [65, 57, 78, 2] corrupt input images with noise and train a\nneural network to reconstruct input pixels. Adversarial training [20] can also be used for unsupervised\nrepresentation learning [16, 12, 13]. Other approaches rely on heuristics to design the pretext task,\ne.g., image colorization [77], relative patch prediction [10], solving jigsaw puzzle [52], clustering [3]\nor rotation prediction [19].\nMore recently, methods based on contrastive learning [23] have shown promising results for learning\nunsupervised visual representations. In this family of approaches, the pretext task is to distinguish\nbetween compatible (same instance) and incompatible (different instances) views of images, although\nthe deﬁnition of view changes from method to method. Contrastive predictive coding (CPC) [55, 29]\nhas been applied to sequential data (space and time) and deﬁne a view as been either past or future\n2\nFigure 2: Pixel-level retrieval results. On left (large), we show the query pixel (center of image). On the\nright (small), we show the two images with the closest pixels (in embedding space) to the query pixel\nand the similarity map between the query pixel embedding and all pixel embeddings on the image.\nobservations. DeepInfomax [30, 1] train a network by contrasting between global (entire image) and\nlocal (a patch) features from same image.\nOther methods [11, 73] propose a non-parametric version of [15], where the network is trained\nto discriminate every instance in the dataset. In particular, InstDisc [73] rely on noise-contrastive\nestimation [22] to contrast between views (composition of image transformations) of the same instance\nand views of distinct instances. The authors propose a memory bank to store the features of every\ninstance to efﬁciently consider large number of negative samples (views from different images) during\ntraining. Many works follow this approach and use a memory bank to sample negative samples, either\nconsidering the same deﬁnition of view [80, 74, 26] or not [63, 49]. SimCLR [5] also consider a view\nas a stochastic composition of image transformations, but do not use memory bank, instead improving\nperformance by using larger batches. Our method can be seen as an adaptation of these methods, where\nwe learn pixel-level, instead of image-level, features that are invariant to views.\nDense visual representations.\nHand-crafted features like SIFT [46] and HOG [8] has been heavily\nused in problems involving dense correspondence [43, 68, 35, 32]. Long et al. [45] show that a deep\nfeatures trained for classiﬁcation on large dataset can ﬁnd correspondence between object instances,\nperforming on par with hand-crafted methods like SIFT Flow [43]. This motivated many research\non training deep networks to learn dense features [25, 75, 76, 6, 34, 53, 60, 64, 54], but these methods\nusually require labeled data and have very specialized architectures. In particular, [34] leverage\ncorrespondence ﬂow (generated by image transformation) to tackle the problem of keypoint matching.\nRecently, unsupervised/self-supervised methods that learn structured representations have been\nproposed. In [79], the authors propose a method that learn dense correspondence through 3D-guided\ncycle-consistency. Some work learn features by exploiting temporal signal, e.g., by learning optical\nﬂow [14], coloring future frames [66], optical-ﬂow similarity [47], contrastive predictive coding [24]\nor temporal cycle-consistency [67, 40]. Others, propose self-supervised methods to learn structured\nrepresentations that encode keypoints [62, 33, 61, 48] or parts [31]. Our approach differs from previous\nmethods in terms of data used, loss functions and general high-level objectives. VADeR learns general\npixel-level representations that can be applied in different downstream tasks. Moreover, our method\ndoes not require speciﬁc data (such as videos or segmented images of faces) for training.\n3\nMethod\nOur approach, VADeR, learns a latent space by forcing representations of pixels to be viewpoint-\nagnostic. This invariance is imposed during training by maximizing per-pixel similarity between\ndifferent views of a same scene via a contrastive loss. Conceptually, a good pixel-level embedding\nshould map pixels that are semantically similar close to each other. Figure 2 shows selected results\nof VADeR learned representations. These qualitative examples hints that VADeR can, to a certain\nextend, cluster few high-level visual concepts (like eye of dog, beak of swan, ear of cat), independent of\nviewpoint and appearance and without any supervision. In the following, we ﬁrst describe how we learn\nviewpoint-agnostic dense features. Then we describe our architecture and implementation details.\n3\n3.1\nView-Agnostic Dense Representations\nWe represent a pixel u in image x∈I ⊂R3×h×w by the tuple (x,u). We follow current self-supervised\ncontrastive learning literature [73, 26, 5] and deﬁne a view as a stochastic composition of data\ntransformations applied to an image (and its pixels). Let (vx,vu) be the result of applying the view v\non (x,u). A view can modify both appearance (e.g. pixel-wise noise, change in illumination, saturation,\nblur, hue) and geometry (e.g. afﬁne transform, homographies, non-parametric transformations) of\nimages (and their pixels).\nLet f and g be encoder-decoder convolutional networks that produce d-dimensional embedding for\nevery pixel in the image, i.e., f,g:(x,u)7→z ∈Rd. The parameters of f and g can be shared, partially\nshared or completely different. Our objective is to learn an embedding function that encodes (x,u)\ninto a representation that is invariant w.r.t. any view v1,v2 ∈Vu containing the original pixel u.\nThat is, f(v1x,v1u) = g(v2x,v2u) for every pixel u and every viewpoint pairs (v1,v2). To alleviate\nthe notation, we write the pixel-level embeddings as fv1u = f(v1x,v1u), gv2u = g(v2x,v2u) and\ngu′ =g(x′,u′). Ideally, we would like to satisfy the following constraint:\nc(fv1u,gv2u) > c(fv1u,gu′),∀u,u′,v1,v2 ,\n(1)\nwhere u and u′ are different pixels, and c(·,·) is a measure of compatibility between representations.\nIn practice, the constraint above is achieved through contrastive learning of pixelwise features.\nThere are different instantiations of contrastive loss functions [39], e.g., margin loss, log-loss,\nnoise-contrastive estimation (NCE). Here, we adapt the NCE loss [22, 55] to contrast between\npixelwise representations. Intuitively, NCE can be seen as a binary classiﬁcation problem, where\nthe objective is to distinguish between compatible views (different views of the same pixel) and\nincompatible ones (different views of different pixels).\nFor every pixel (x,u), we construct a set of N random negative pixels U−= {(x′,u′)}. The loss\nfunction for pixel u and U−can be written as:\nL(u,U−)=−E(v1,v2)∼Vu\n\"\nlog\nexp(c(fv1u,gv2u))\nexp(c(fv1u,gv2u))+P\nu′∈U−exp(c(fv1u,gu′))\n#\n.\n(2)\nWe consider the compatibility measure to be the temperature-calibrated cosine similarity, c(x1,x2)=\n1\nτ xT\n1 x2/∥x1∥∥x2∥, where τ is the temperature parameter. By using a simple, non-parametric\ncompatibility function, we place all the burden of representation learning on the network parameters.\nThis loss function forces representations of a pixel to be more compatible to other views of the same\npixel than views of other pixels. The ﬁnal loss consists on minimizing the empirical risk over every\npixel on every image of the dataset.\n3.2\nImplementation Details\nSampling views.\nWe consider view as been a composition of (i) appearance transformations (random\nGaussian blur, color jitter and/or greyscale conversion) and (ii) geometric transformations (random\ncrop followed by resize to 224×224). In this work, we use the same transformations as in [26].\nPositive training pairs are generated by sampling pairs of views on each training image and making\nsure at least 32 pixels belong to both views (we tried different minimum number of matching pixels per\npair, and did not notice any quantitative difference in performance). Pixel-level matching supervision\ncomes for free: the two views induce a correspondence map between them. We use this correspondence\nmap to ﬁnd the matching features between the pair of views and consider each matching pair as a\npositive training sample.\nThe number of negative samples is particularly important if we consider pixelwise representations,\nas the number of pixels in modern datasets can easily achieve the order of hundreds of billions. In\npreliminary experiments, we try to use different pixels of the same image as negative examples but\nfail to make it work. We argue that using pixels from other images is more natural for negative samples\nand it ﬁts well in the context of using a queue for negative samples. We use the recently proposed\nmomentum contrast mechanism [26] to efﬁciently use a large number of negative samples during\ntraining. The key idea is to represent the negative samples as a queue of large dynamic dictionary\nthat covers a representative set of negative samples and to model the encoder g as a momentum-based\n4\n(b)\nInputs\nOutputs\nInputs\nOutputs\n(a)\nFigure 3: Example of task evaluated (shown are the output of VADeR in different settings). (a) Semantic\nsegmentation and depth estimation can be seen as structured multi-class classiﬁcation and regression\nproblem, respectively. (b) Video instance segmentation, where the instance label is given for the ﬁrst\nframe, and is propagated through the frames.\nmoving average of f. Following [26], we set the size of the dictionary to 65,536 and use a momentum\nof 0.999. We observe similar behavior w.r.t. the size of memory bank as reported in [26].\nArchitecture.\nWe adopt the feature pyramid network (FPN) [41] as our encoder-decoder architecture.\nFPN adds a lightweight top-down path to a standard network (we use ResNet-50 [28]) and generates\na pyramid of features (with four scales from 1/32 to 1/4 resolution) with dimension 256. Similar to\nthe semantic segmentation branch of [36], we merge the information of FPN into a single dense output\nrepresentation. At each resolution of the pyramid, we add a number of upsampling blocks so that\neach pyramid yield a feature map of dimension 128 and scale 1/4 (e.g., we add 3 upsampling blocks\nfor res. 1/32, 1 upsampling for 1/8 res.). Each upsampling block consists of a 3×3 convolution,\ngroup norm [71], ReLU [50] and 2× bilinear upsampling. Finally, we element-wise sum the pyramid\nrepresentations and pass through a ﬁnal 1×1 convolution. The ﬁnal representation has dimension\n128 and scale 1/4. Since we use images of size 224×224 during training, the feature map generated\nby VADeR has dimension 128×56×56.\nTraining.\nWe train our model on the ImageNet-1K [9] train split, containing approximately 1.28M\nimages. The weights of f are optimized with stochastic gradient descent with weight decay of 0.0001\nand momentum 0.9. We train using 4 GPUs with a batch size of 128 for about 6M iterations. We use\na learning rate of 3e−7 and 3e−3 for the encoder and decoder, respectively. We multiply Equation 2 by\na factor of 10, as we observed it provides more stable results when ﬁne-tuning with very small amount\nof labeleld data. We set the temperature τ to 0.07.\nTraining samples are generated by sampling a random image and two views. We store the corre-\nspondence ﬂow between the pixels that belongs to both view-transformed images. After forwarding\neach view through the networks, we apply the loss in Equation 2 considering 32 pairs of matching\npixels as positive pairs (we chose randomly 32 pairs out of all matching pairs). The negative samples\nare elements from the MoCo queue. We then update the parameters of network f with SGD and the\nweights of g with moving average of the weights of f. Finally, we update the elements of the dynamic\ndictionary and repeat the training.\nWe opt to initialize the weights of VADeR’s encoder with MoCo [26] to increase training speed. The\nweights of the decoder are initialized randomly. We consider MoCo1 as our most important baseline for\ntwo reasons: (i) it is current state of the art in unsupervised visual representation and (ii) we start from\nMoCo initialization. We also compare our model with a ResNet-50 [28] pre-trained on ImageNet-1K\nwith labels. MoCo contains the same capacity of ResNet-50 and VADeR contains around 2M extra\nparameters due to the decoder.\n4\nExperimental Results\nAn important objective of unsupervised learning is to learn features that are transferable to downstream\ntasks. We evaluate the quality of the features learned by VADeR on a variety of tasks ranging from\n1We use the ofﬁcial MoCo version 2 for both the baseline and the initialization of VADeR. Training MoCov2\nfor extra 5M iterations does not give any statistically signiﬁcant improvement.\n5\nrecognition to geometry (see Figure 3 for examples of tasks evaluated). We consider two transfer\nlearning experimental protocols: (i) feature extraction, where we evaluate the quality of ﬁxed learned\nfeatures, and (ii) ﬁne-tuning, where we use the learned features as weight initialization and the whole\nnetwork is ﬁne-tuned. For each experiment, we consider identical models and hyperparameters for\nVADeR and baselines, only changing the ﬁxed features or the initialization for ﬁne-tuning. For more\ndetails about datasets and implementations of downstream tasks, see supplementary material.\n4.1\nFeature Extraction Protocol\nSemantic segmentation and depth prediction.\nWe follow common practice of self-supervised\nlearning [21, 37] and assess the quality of features on ﬁxed image representations with a linear predictor.\nThe linear classiﬁer is a 1×1 convolutional layer that transform the features into logits (followed by\na softmax) for per-pixel classiﬁcation (semantic segmentation) or into a single value for per-pixel\nregression (depth prediction). We train the semantic segmentation tasks with cross entropy and the\ndepth prediction with L1 loss. We test the frozen features in two datasets for semantic segmentation\n(PASCAL VOC12 [17] and Cityscapess [7]) and one for depth prediction (NYU-depth v2 [51]). In\nall datasets, we train the linear model on the provided train set2 and evaluate on the validation set.\nsem. seg. (mIoU)\ndepth (RMSE)\nVOC\nCS\nNYU-d v2\nrandom\n04.9\n10.6\n1.261\nsup. IN\n54.4\n47.1\n0.994\nMoCo\n43.0\n32.3\n1.136\nVADeR\n56.7\n44.3\n0.964\nTable 1: Sem. seg. and depth prediction\nevaluated with ﬁxed features.\nWe compare VADeR with randomly initialized network\n(which serves as a lower bound), supervised ImageNet\npretraining and MoCo (all with ResNet-50 architecture).\nBecause baselines are trained for global representations,\nwe need to adapt them to be more competitive for dense\nprediction tasks. We ﬁrst remove the last average pooling\nlayer so that the ﬁnal representation has a resolution of 1/32.\nThen, we reduce the effective stride to 1/4 by replacing\nstrided convolution with dilated ones, following the large\nﬁeld-of-view design in [4]. This way, the baselines produce\na feature map with same resolution as VADeR.\nTable 1 compares results (averaged over 5 trials) in standard mean intersection-over-union (mIoU)\nand root mean square error (RMSE). VADeR outperform MoCo in all tasks by a considerable margin.\nIt also achieves better performance than supervised ImageNet pretraining in one semantic segmenta-\ntion task and in depth prediction. This corroborates our intuition that explicitly learning structured\nrepresentations provides advantages for pixel-level downstream tasks.\nVideo instance segmentation.\nWe also use ﬁxed representations to compute dense correspondence\nfor instance segmentation propagation in videos. Given the instance segmentation mask of the ﬁrst\nframe, the task is to propagate the masks to the the rest of the frames through nearest neighbours in\nembedding space. We evaluate directly on learned features, without any additional learning. We follow\nthe testing protocol of [67] and report results in standard metrics, including region similarity J (IoU)\nand contour-based accuracy F3.\nTable 2 shows results on DAVIS-2017 validation set [59], considering input resolution of 320×320\nand 480×480. VADeR results are from a model trained on 2.4M iterations, as we observed that it\nperforms slightly better on this problem (while performing equal or slightly worse on other tasks).\nOnce again, we observe the advantage of explicitly modelling dense representations: VADeR surpass\nrecent self-supervised methods [66, 67, 38] and achieve comparable results with current state of the\nart [40]. VADeR, contrary to competitive methods, achieve these results without using any video data\nnor specialized architecture. We note that MoCo alone is already a strong baseline, achieving similar\nperformance to some specialized methods and close to supervised ImageNet pretrain.\n4.2\nFine-tuning Protocol\nIn this section, we compare how good features are for ﬁne-tuning on downstream tasks. All baselines\nhave the same FPN architecture of VADeR (described in Section 3.2) and are trained with identical\nhyperparameters and data. Apart from weight initialization, everything else is kept identical—this\nallows for straight comparison of different initialization methods. We compare VADeR with two\n2We consider train_aug for VOC.\n3We use the evaluation code provided by [67] in: https://github.com/xiaolonw/TimeCycle\n6\nJ (Mean)\nF (Mean)\n320\n480\n320\n480\nSIFT Flow [44]\n33.0\n-\n35.0\n-\nVideo Colorization [66]\n34.6\n-\n32.7\n-\nTimeCycle [67]\n41.9\n46.4\n39.4\n50.0\nCorrFlow [38]\n-\n47.7\n-\n51.3\nUVC [40]\n52.0\n56.8\n52.6\n59.5\nsup. IN\n50.3\n53.2\n49.2\n56.2\nMoCo [26]\n42.3\n51.4\n40.5\n54.7\nVADeR\n52.4\n54.7\n55.1\n58.4\nTable 2: Results on instance mask propagation on\nDAVIS-2017 [59] val.\nset, with input resolution of\n320 × 320 and 480 × 480. Results are reported on re-\ngion similarity J (IoU) and contour-based accuracy F.\nsem. seg.\ncorr.\nview sampling\nmIoU\nJ\nunmatch\nﬁx.\n32.4\n9.8\nf.t.\n74.1\n-\nsame view\nﬁx.\n48.0\n46.7\nf.t.\n75.2\n-\ndiff. view\nﬁx.\n56.7\n52.4\nf.t.\n75.4\n-\nTable 3: Results of VADeR considering dif-\nferent view sampling strategy for semantic\nsegmentation (mIoU in VOC) and dense cor-\nrespondence (J in DAVIS-2017). See text\nfor details.\nFigure 4: Results on semantic segmentation (VOC) and depth prediction (NYU-d v2) evaluated with\nﬁne-tuned features, considering different amount of labeled data. Evaluation is on val. set of each\ndataset. We show mean/std results over 5 trials in standard mean intersection-over-union (mIoU) and\nroot mean square error (RMSE).\nencoder initializations: supervised ImageNet pretraining and MoCo. VADeR, contrary to baselines,\ninitializes both the FPN’s encoder and decoder. In these experiments, we use the setup for ﬁne-tuning\nproposed in [26]: the batch normalization layers are trained with Sync Batch-Norm [58] and we add\nbatch-norm on all FPN layers.\nSegmentation and depth.\nAs in the previous experiment, we add an extra 1×1 convolutional layer\non the output of the FPN architecture to perform either multiclass classiﬁcation or regression. We use\nthe same training and validation data as previous experiment (for training details, see supplementary\nmaterial).\nFigure 4 shows results of ﬁne-tuning on semantic segmentation (in PASCAL VOC12) and in depth\nprediction (NYU-d v2), assuming different amount of labeled data (we consider 2, 5, 10, 20, 50 and\n100% of dataset). When considering 100% of labeled images, VADeR achieves similar performance\nas MoCo (no statistical difference) and surpass supervised ImageNet pretraining. The beneﬁts of\nVADeR, however, increase as the amount of labeled data on ﬁne-tuning stages is reduced. This\nresult corroborates current research that show that self-supervised learning methods achieve better\nperformance than supervised pretraining when the number of labeled data is limited. Table 5 in\nsupplementary material show results in tabular format.\nObject detection, instance segmentation and keypoint detection.\nWe use Mask R-CNN [27] with\nFPN backbone [41]. All methods are trained and evaluated on COCO [42] with the standard metrics.\nWe use the implementation of Detectron2 [72] for training and evaluation4. We train one model\nfor object detection/segmentation and one for keypoint detection, using the default hyperparameters\nprovided by [72] (chosen for ImageNet supervised pretraining). All models are trained on a controlled\nsetting for around 12 epochs (schedule 1x).\nTable 4 compares VADeR with baselines on the three tasks. MoCo is already a very strong baseline\nachieving performance similar to supervised ImageNet pretraining. We observe that VADeR consis-\ntently outperform MoCo (and the supervised baseline) on these experiments, showing advantages of\nlearning dense representations contrary to global ones.\n4Moreover, we consider the default FPN implementation provided on the repository (instead of the one\ndescribed on this paper) to train VADeR and baselines for these experiments.\n7\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n55\nAPmk\n75\nAPkp\nAPkp\n50\nAPkp\n75\nrandom\n31.0\n49.5\n33.2\n28.5\n46.8\n30.4\n65.4\n85.8\n70.8\nsup. IN\n39.0\n59.8\n42.5\n35.4\n56.6\n37.8\n65.3\n87.0\n71.0\nMoCo [26]\n38.9\n59.4\n42.5\n35.4\n56.3\n37.8\n65.7\n86.8\n71.7\nVADeR\n39.2\n59.7\n42.7\n35.6\n56.7\n38.2\n66.1\n87.3\n72.1\nTable 4: Results on mask rcnn on object detection, instance segmentation and keypoint detection\nﬁne-tuned on COCO. We show results on val2017, averaged over 5 trials.\nFigure 5: Visualizing the learned features. We show query images, VADeR (big) and MoCo (small,\nbottom-right) features. The features are projected into three dimensions with PCA and visualized as\nRGB (similar to [66]). Similar color implies similarity in feature space. Best viewed in color.\n4.3\nAblation Studies\nInﬂuence of views.\nWe analyze the importance of correctly matching pixels during training. Table 3\nshows results considering three pixel-matching strategies. The ﬁrst row (“unmatch”) ignores the\ncorrespondence map between views of the same scene. It considers random pairs of pixels as positive\npairs. Second and third rows utilize the correct correspondence map. The second row (“same view”)\nalways consider identical crop for each pair of views, while the last (“diff. view”) consider different\ncrops (with different location and scale). VADeR uses the third approach as default. Results are reported\nfor semantic segmentation on Pascal VOC (in mIoU) and for dense correspondence on DAVIS-2017\n(in region similarity J ). As expected, using random pixel matching between views is worse than using\ncorrect pixel pairing (row 1 vs. rows 2 and 3). We also note that performance in recognition task almost\ndoes not change when considering same or different crops between pairing views (row 2 vs. row 3).\nHowever, for correspondence, having difference crops per view provides a considerable advantage.\nSemantic grouping.\nFigure 5 shows features learned by VADeR (big) and MoCo (small, bottom-\nright) projected to three dimensions (using PCA) and plotted as RGB. Similar colors implies that\nfeatures are semantically similar. We observe qualitatively that semantically meaningful grouping\nemerges from VADeR training, without any supervision. This is an interesting fact that can potentially\nbe useful in unsupervised or non-parametric segmentation problems.\n5\nConclusion\nWe present VADeR—View-Agnostic Dense Representations—for unsupervised learning of dense\nrepresentations. Our method learns representations through pixel-level contrastive learning. VADeR is\ntrained by forcing representations of matching pixels (that is, features from different views describing\nsame location) to be close in an embedding space, while non-matching features to be far apart. We\nleverage known pixel correspondences, derived from randomly generated views of a scene, to generate\npositive pairs. Qualitatively examples hints that VADeR features can discovery high-level visual\nconcepts and that semantic grouping can emerge from training, without any label. We show that\nlearning unsupervised dense representations are more efﬁcient to downstream pixel-level tasks than\ntheir global counterparts. VADeR achieves positive results when compared to strong baselines in\nmany structured prediction tasks, ranging from recognition to geometry. We believe that learning\nunsupervised dense representations can be useful for many structured problems involved transfer\nlearning, as well as unsupervised or low-data regime problems.\n8\n6\nBroader Impact\nOur research falls under the category of advancing machine learning techniques for computer vision\nand scene understanding. We focus on improving image representations for dense prediction tasks,\nwhich subsumes a large array of fundamental vision tasks, such as image segmentation and object\ndetection. While there are potentially many implications for using these applications, here we discuss\ntwo aspects. First, we highlight some social implications for image understanding with no or very\nlittle labeled data. Second, we provide some insights on foundational research questions regarding the\nevaluation of general purpose representation learning methods.\nImproving capabilities of image understanding using unlabeled data, especially for pixel-level tasks,\nopens up a wide range of applications that are beneﬁcial to the society, and which cannot be tackled\notherwise. Medical imagery applications suffers from lack of labeled data due to the need of very\nspecialized labelers. Another application, tackling harmful online content—including but not limited\nto terrorist propaganda, hateful speech, fake news and misinformation—is a huge challenge for\ngovernments and businesses. What makes these problems especially difﬁcult is that it is very difﬁcult\nto obtain clean labeled data for training machine learning models—think of ﬁlming a terrorist attack on\nlive video as in the unfortunate Christchurch attack. Self-supervised learning can potentially move the\nneedle in advancing models for detecting extremely rare yet highly impactful incidents. On the other\nhand, such technologies can be potentially misused for violating privacy and freedom of expression.\nWe acknowledge these risks as being a feature of any amoral technology, and we invite governments,\npolicy makers and all citizens—including the research community—to work hard on striking a balance\nbetween those beneﬁts and risks.\nAnother interesting aspect of our research is highlighting the importance of aligning representation\nlearning methods with the nature of downstream applications. With our method, we show that learning\npixel-level representations from unlabeled data we can outperform image-level methods on a variety of\ndense prediction tasks. Our ﬁndings highlight that the research community should go beyond limited\ntest-beds for evaluating generic representation learning techniques. We invite further research on\ndeveloping comprehensive evaluation protocols for such methods. In fact, we see many research\nopportunities in the computer vision domain, such as developing a sweep of standardized benchmarks\nacross a variety of geometric and semantic image understanding tasks, and designing methods that can\nbridge the gap between ofﬂine and online performance.\nReferences\n[1] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual\ninformation across views. In NeurIPS, 2019.\n[2] Mohamed Belghazi, Maxime Oquab, and David Lopez-Paz. Learning about an exponential amount of\nconditional distributions. In NeurIPS, 2019.\n[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, 2018.\n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nPAMI, 2017.\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\nlearning of visual representations. In ICML, 2020.\n[6] Christopher B Choy, JunYoung Gwak, Silvio Savarese, and Manmohan Chandraker. Universal correspon-\ndence network. In NIPS, 2016.\n[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.\nIn CVPR, 2016.\n[8] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR, 2009.\n[10] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context\nprediction. In ICCV, 2015.\n[11] Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In ICCV, 2017.\n9\n[12] Jeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\n[13] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In NeurIPS, 2019.\n[14] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick\nVan Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical ﬂow with convolutional\nnetworks. In ICCV, 2015.\n[15] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox.\nDiscriminative unsupervised feature learning with exemplar convolutional neural networks. PAMI, 2015.\n[16] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and\nAaron Courville. Adversarially learned inference. ICLR, 2017.\n[17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal\nvisual object classes (voc) challenge. IJCV, 2010.\n[18] Peter Földiák. Learning invariance from transformation sequences. Neural Computation, 1991.\n[19] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations. In ICLR, 2018.\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.\n[21] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In ICCV, 2019.\n[22] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for\nunnormalized statistical models. In AISTATS, 2010.\n[23] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.\nIn CVPR, 2006.\n[24] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding.\nIn ICCV Workshops, 2019.\n[25] Xufeng Han, Thomas Leung, Yangqing Jia, Rahul Sukthankar, and Alexander C. Berg. Matchnet: Unifying\nfeature and metric learning for patch-based matching. In CVPR, 2015.\n[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\n[27] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nCVPR, 2016.\n[29] Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efﬁcient image\nrecognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.\n[30] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler,\nand Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In\nICLR, 2019.\n[31] Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, and Jan Kautz. Scops:\nSelf-supervised co-part segmentation. In CVPR, 2019.\n[32] Junhwa Hur, Hwasup Lim, Changsoo Park, and Sang Chul Ahn. Generalized deformable spatial pyramid:\nGeometry-preserving dense correspondence estimation. In CVPR, 2015.\n[33] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of object landmarks\nthrough conditional image generation. In NeurIPS, 2018.\n[34] Angjoo Kanazawa, David W Jacobs, and Manmohan Chandraker. Warpnet: Weakly supervised matching\nfor single-view reconstruction. In CVPR, 2016.\n[35] Jaechul Kim, Ce Liu, Fei Sha, and Kristen Grauman. Deformable spatial pyramid matching for fast dense\ncorrespondences. In CVPR, 2013.\n[36] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollár. Panoptic feature pyramid networks. In\nCVPR, 2019.\n[37] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation\nlearning. In CVPR, 2019.\n[38] Z. Lai and W. Xie. Self-supervised learning for video correspondence ﬂow. In BMVC, 2019.\n[39] Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu Jie Huang. A tutorial on energy-\nbased learning. Predicting Structured Data, 2006.\n10\n[40] Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang, Jan Kautz, and Ming-Hsuan Yang. Joint-task\nself-supervised learning for temporal correspondence. In NeurIPS, 2019.\n[41] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In CVPR, 2017.\n[42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n[43] Ce Liu, Jenny Yuen, and Antonio Torralba. Sift ﬂow: Dense correspondence across scenes and its applications.\nPAMI, 2010.\n[44] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and William T Freeman. Sift ﬂow: Dense correspondence\nacross different scenes. In ECCV, 2008.\n[45] Jonathan L Long, Ning Zhang, and Trevor Darrell. Do convnets learn correspondence? In NIPS, 2014.\n[46] David G Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.\n[47] Aravindh Mahendran, James Thewlis, and Andrea Vedaldi. Cross pixel optical-ﬂow similarity for self-\nsupervised learning. In ACCV, 2018.\n[48] Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole, Kevin P Murphy, and Honglak Lee. Unsuper-\nvised learning of object structure and dynamics from videos. In NeurIPS, 2019.\n[49] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In\nCVPR, 2020.\n[50] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In ICML,\n2010.\n[51] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In ECCV, 2012.\n[52] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn ECCV, 2016.\n[53] David Novotny, Diane Larlus, and Andrea Vedaldi. Anchornet: A weakly supervised network to learn\ngeometry-sensitive features for semantic matching. In CVPR, 2017.\n[54] Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. Lf-net: learning local features from images. In\nNeurIPS, 2018.\n[55] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n[56] Stephen E Palmer. Vision science: Photons to phenomenology. MIT Press, 1999.\n[57] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:\nFeature learning by inpainting. In CVPR, 2016.\n[58] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, and Jian Sun. Megdet:\nA large mini-batch object detector. In CVPR, 2018.\n[59] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alexander Sorkine-Hornung, and Luc\nVan Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017.\n[60] Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Convolutional neural network architecture for geometric\nmatching. In CVPR, 2017.\n[61] James Thewlis, Samuel Albanie, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of landmarks by\ndescriptor vector exchange. In ICCV, 2019.\n[62] James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of object frames by dense\nequivariant image labelling. In NeurIPS, 2017.\n[63] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nContrastive multiview coding.\narXiv preprint\narXiv:1906.05849, 2019.\n[64] Nikolai Ufer and Bjorn Ommer. Deep semantic feature matching. In CVPR, 2017.\n[65] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing\nrobust features with denoising autoencoders. In ICML, 2008.\n[66] Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking\nemerges by colorizing videos. In ECCV, 2018.\n[67] Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n[68] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepﬂow: Large displacement\noptical ﬂow with deep matching. In ICCV, 2013.\n11\n[69] Laurenz Wiskott and Terrence J Sejnowski. Slow feature analysis: Unsupervised learning of invariances.\nNeural computation, 2002.\n[70] Andrew P Witkin and Jay M Tenenbaum. On the role of structure in vision. In Human and machine vision.\nElsevier, 1983.\n[71] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\n[72] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https:\n//github.com/facebookresearch/detectron2, 2019.\n[73] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In CVPR, 2018.\n[74] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via invariant and\nspreading instance feature. In CVPR, 2019.\n[75] Sergey Zagoruyko and Nikos Komodakis. Learning to compare image patches via convolutional neural\nnetworks. In CVPR, 2015.\n[76] Jure Žbontar and Yann LeCun. Stereo matching by training a convolutional neural network to compare\nimage patches. JMLR, 2016.\n[77] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\n[78] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by\ncross-channel prediction. In CVPR, 2017.\n[79] Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei A Efros. Learning dense\ncorrespondence via 3d-guided cycle consistency. In CVPR, 2016.\n[80] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of visual\nembeddings. In ICCV, 2019.\n12\nA\nSupplementary Material\nA.1\nImplementation Details: feature extraction protocol\nSemantic segmentation.\nThe output of each model (baselines or VADeR) follows a 1×1 conv. layer,\n4× upsample and softmax. The ﬁnal output models the probability of each pixel belonging to a category\n(21 for PASCAL VOC [17] and 19 for Cityscapes [7]). W train on train_aug set of VOC and train\nset of Cityscapes and evaluate on val set of each dataset. We use crop size 512 on PASCAL VOC and\n768 in Cityscapes and evaluation is done on original image size. Each training sample is generated with\nrandom cropping, scaling (by ratio in [0.5,2.0]) and normalization. We train with batch size of 64 for\n60 epochs for VOC and batch size 16 and 100 epochs for Cityscapes. In both datasets, the linear layer\nis trained with SGD (on 4 GPUs) with learning rate 0.005/0.01 (for VOC and Cityscapes), momentum\n0.9 and weight decay 0.0001. The learning rate is decayed by a factor of 10 at epochs 42,55 for VOC\nand 70,90 for Cityscapes.\nDepth Estimation.\nWe add a 1×1 conv. layer on the top of each model to predict a single value\nat each location. The per-pixel prediction is 4× bilinear-upsampled. We train and evaluate on the\nofﬁcial training split of NYU-depth v2 provided by [51]. Both training and evaluation are performed on\noriginal image sizes and random horizontal ﬂip is applied to each training sample. We train with batch\nsize of 16 for 30 epochs. The linear layer is trained to minimize the Huber loss between predicted and\nground-truth depth values. We use a learning rate of 0.005, momentum 0.9 and weight decay 0.0001.\nThe learning rate is reduced by a factor of 10 at epochs 10,20.\nVideo instance segmentation.\nGiven the instance mask of an initial frame, the task is to propagate\nthe mask to the rest of the frames. We report results on DAVIS-2017 [59] val set. We follow identical\nevaluation setup of [67, 40] 5: for frame t, we average the the predictions of [t−7,t] to obtain the ﬁnal\npropagated map. We use a k-NN propagation schema with k=5.\nA.2\nImplementation Details: ﬁne-tunning protocol\nSemantic segmentation.\nWe use the same architecture of VADeR (described in 3.2) for all baselines.\nAs above, we transform the 128-dimensional output of each model into a per-pixel probability map\nwith 1×1 conv, upsampling and softmax. We use the same dataset and preprocessing as described on\nthe feature extraction protocol. We train with batch size of 64 for 60 epochs on VOC and batch size 16\nfor 100 epochs on Cityscapes. The models are ﬁne-tunned with SGD and learning rate of 0.005/0.01\n(for VOC and Cityscapes), momentum 0.9 and weight decay 0.0001.\nDepth Estimation.\nWe also use the same VADeR architecture for all baselines. As in the feature\nextractor protocol, we upsample the predictions to original input size and the model is trained to\nminimize the Huber loss. We use the same dataset and preprocessing as described on the feature\nextraction protocol. We train with batch size of 16 for 30 epochs (we reduce the learning rate by 10 on\nepochs 10,20). The models are ﬁne-tunned with SGD and learning rate of 0.005, momentum 0.9 and\nweight decay 0.0001.\nA.3\nFine-tuning results with varying number of training samples\nTable 5 shows results of the plots in Figure 4. We run experiments with varying number of (labeled)\nsamples. Reported results are averaged over 5 trials.\n5From https://github.com/xiaolonw/TimeCycle\n13\n% of data\n2\n5\n10\n20\n50\n100\nVOC\nsup. IN\n37.4\n48.4\n53.7\n59.0\n69.3\n73.9\n(mIoU)\nMoCo\n44.7\n53.3\n57.2\n59.3\n70.1\n75.3\nVADeR\n47.7\n56.9\n60.6\n60.8\n70.9\n75.4\nNYU-d v2\nsup. IN\n1.288\n1.060\n0.999\n0.896\n0.805\n0.731\n(RMSE)\nMoCo\n1.383\n1.340\n0.897\n0.804\n0.707\n0.640\nVADeR\n1.128\n0.951\n0.868\n0.791\n0.700\n0.641\nTable 5: Results from Figure 4 in tabular format. We show results averaged over 5 trials.\n14\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-11-11",
  "updated": "2020-12-07"
}