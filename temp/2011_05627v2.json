{
  "id": "http://arxiv.org/abs/2011.05627v2",
  "title": "Skin disease diagnosis with deep learning: a review",
  "authors": [
    "Hongfeng Li",
    "Yini Pan",
    "Jie Zhao",
    "Li Zhang"
  ],
  "abstract": "Skin cancer is one of the most threatening diseases worldwide. However,\ndiagnosing skin cancer correctly is challenging. Recently, deep learning\nalgorithms have emerged to achieve excellent performance on various tasks.\nParticularly, they have been applied to the skin disease diagnosis tasks. In\nthis paper, we present a review on deep learning methods and their applications\nin skin disease diagnosis. We first present a brief introduction to skin\ndiseases and image acquisition methods in dermatology, and list several\npublicly available skin datasets for training and testing algorithms. Then, we\nintroduce the conception of deep learning and review popular deep learning\narchitectures. Thereafter, popular deep learning frameworks facilitating the\nimplementation of deep learning algorithms and performance evaluation metrics\nare presented. As an important part of this article, we then review the\nliterature involving deep learning methods for skin disease diagnosis from\nseveral aspects according to the specific tasks. Additionally, we discuss the\nchallenges faced in the area and suggest possible future research directions.\nThe major purpose of this article is to provide a conceptual and systematically\nreview of the recent works on skin disease diagnosis with deep learning. Given\nthe popularity of deep learning, there remains great challenges in the area, as\nwell as opportunities that we can explore in the future.",
  "text": "arXiv:2011.05627v2  [eess.IV]  6 Dec 2020\nSkin disease diagnosis with deep learning: a review\nHongfeng Lia,∗, Yini Panb, Jie Zhaoa, Li Zhangc\naCenter for Data Science in Health and Medicine, Peking University, Beijing 100871, China\nbAcademy for Advanced Interdisciplinary Studies, Peking University, Beijing 100871, China\ncCenter for Data Science, Peking University, Beijing 100871, China\nAbstract\nSkin cancer is one of the most threatening diseases worldwide. However, diag-\nnosing skin cancer correctly is challenging. Recently, deep learning algorithms\nhave emerged to achieve excellent performance on various tasks. Particularly,\nthey have been applied to the skin disease diagnosis tasks. In this paper, we\npresent a review on deep learning methods and their applications in skin disease\ndiagnosis. We ﬁrst present a brief introduction to skin diseases and image acqui-\nsition methods in dermatology, and list several publicly available skin datasets\nfor training and testing algorithms. Then, we introduce the conception of deep\nlearning and review popular deep learning architectures. Thereafter, popular\ndeep learning frameworks facilitating the implementation of deep learning algo-\nrithms and performance evaluation metrics are presented. As an important part\nof this article, we then review the literature involving deep learning methods\nfor skin disease diagnosis from several aspects according to the speciﬁc tasks.\nAdditionally, we discuss the challenges faced in the area and suggest possible\nfuture research directions. The major purpose of this article is to provide a con-\nceptual and systematically review of the recent works on skin disease diagnosis\nwith deep learning. Given the popularity of deep learning, there remains great\nchallenges in the area, as well as opportunities that we can explore in the future.\nKeywords:\nSkin disease diagnosis, Deep learning, Convolutional neural\nnetwork, Image classiﬁcation, Image segmentation\n1. Introduction\nSkin disease is one of the most common diseases among people worldwide.\nThere are various types of skin diseases, such as basal cell carcinoma (BCC),\nmelanoma, intraepithelial carcinoma, and squamous cell carcinoma (SCC) [1].\nParticularly, skin cancer has been the most common cancer in United States\nand researches showed that one-ﬁfth of Americans will suﬀer from a skin cancer\nduring their lifetime [2, 3]. Melanoma is reported as the most fatal skin cancer\n∗Corresponding author\nEmail address: lihongfeng@math.pku.edu.cn (Hongfeng Li)\nPreprint submitted to Neurocomputing\nDecember 8, 2020\nFigure 1: Several examples of diﬀerent types of skin diseases. These images come from the\nDermoﬁt Image Library [10].\nwith a mortality rate of 1.62% among other skin cancers [4]. According to the\nAmerican Cancer Society’s estimates for melanoma in the United States for\n2020, there will be about 100, 350 new cases of melanoma and 6, 850 people are\nexpected to die of melanoma [5]. On the other hand, BCC is the most common\nskin cancer, and although not usually fatal, it places large burdens on health\ncare services [6]. Fortunately, early diagnosis and treatment of skin cancer can\nimprove the ﬁve-year survival rate by around 14% [7].\nHowever, diagnosing a skin disease correctly is challenging since a variety of\nvisual clues, such as the individual lesional morphology, the body site distribu-\ntion, color, scaling and arrangement of lesions, should be utilized to facilitate\nthe diagnosis. When the individual components are analyzed separately, the\ndiagnosis process can be complex [8]. For instance, there are four major clin-\nical diagnosis methods for melanoma: ABCD rules, pattern analysis, Menzies\nmethod and 7-Point Checklist. Often only experienced physicians can achieve\ngood diagnosis accuracy with these methods [9]. The histopathological exami-\nnation on the biopsy sampled from a suspicious lesion is the gold standard for\nskin disease diagnosis. Several examples of diﬀerent types of skin diseases are\ndemonstrated in Fig. 1. Developing an eﬀective method that can automatically\ndiscriminate skin cancer from non-cancer and diﬀerentiate skin cancer types\nwould therefore be beneﬁcial as an initial screening tool.\nDiﬀerentiating a skin disease with dermoscopy images may be inaccurate or\nirreproducible since it depends on the experience of dermatologists. In practice,\nthe diagnostic accuracy of melanoma from the dermoscopy images by an inex-\nperienced specialist is between 75% to 84% [7]. One limitation of the diagnosis\nperformed by human experts is that it heavily depends on subjective judgment\nand varies largely among diﬀerent experts. By contrast, a computer aided di-\nagnostic (CAD) system is more objective. By utilizing handcrafted features,\ntraditional CAD systems for skin disease classiﬁcation can achieve excellent\nperformance in certain skin disease diagnosis tasks [11, 12, 13]. However, these\nsystems usually focus on limited types of skin diseases, such as melanoma and\nBCC. Therefore, they are typically unable to be generalized to perform diag-\nnosis over broader classes of skin diseases. The reason is that the handcrafted\nfeatures are not suitable for a universal skin disease diagnosis. On one hand,\n2\nhandcrafted features are usually speciﬁcally extracted for limited types of skin\ndiseases.\nThey can hardly be adapted to other types of skin diseases.\nOne\nthe other hand, due to the diversity of skin diseases, human-crafted features\ncannot be eﬀective for every kind of skin disease [8]. Feature learning can be\none solution to this problem, which eliminates the need of feature engineering\nand extracts eﬀective features automatically [14]. Many feature learning meth-\nods have been proposed in the past few years [15, 16, 17]. However, most of\nthem were applied on dermoscopy or histopathology images processing tasks\nand mainly focused on the detection of mitosis and indicator of cancer [18].\nRecently, deep learning methods have become popular in feature learning\nand achieved excellent performances in various tasks, including image classi-\nﬁcation [19, 20], segmentation [21, 22], object detection [23, 24] and localiza-\ntion [25, 26]. A variety of researches [9, 23, 12, 27, 25] showed that the deep\nlearning methods were able to surpass humans in many computer vision tasks.\nOne thing behind the success of deep learning is its ability to learn seman-\ntic features automatically from large-scale datasets. In particular, there have\nbeen many works on applying deep learning methods to skin disease diagno-\nsis [27, 28, 29, 30, 31]. For example, Esteva et al. [27] proposed a universal skin\ndisease classiﬁcation system based on a pretrained convolutional neural network\n(CNN). The top-1 and top-3 classiﬁcation accuracies they achieved were 60.0%\nand 80.3% respectively, which signiﬁcantly outperformed the performances of\nhuman specialists. Deep neural networks can deal with the large variations in-\ncluded in the images of skin diseases through learning eﬀective features with\nmultiple layers. Despite these technological advances, however, lack of available\nhuge volume of labeled clinical data has limited the wide application of deep\nlearning in skin disease diagnosis.\nIn this paper, we present a comprehensive review of the recent works on\ndeep learning for skin disease diagnosis. We ﬁrst give a brief introduction to\nskin diseases. Through literature research, we then introduce common data ac-\nquisition methods and list several commonly used and publicly available skin\ndisease datasets for training and testing deep learning models. Thereafter, we\ndescribe the basic conception of deep learning and present the popular deep\nlearning architectures. Accordingly, prevalent deep learning frameworks are de-\nscribed and compared. To make it clear that how to evaluate a deep learning\nmethod, we introduce the evaluation metrics according to diﬀerent tasks. We\nthen draw on the literature of applications of deep learning in skin disease diag-\nnosis and introduce the content according to diﬀerent tasks. Through analyzing\nthe reviewed literature, we present the challenges remained in the area of skin\ndisease diagnosis with deep learning and provide guidelines to deal with these\nchallenges in the future. Considering the lack of in-depth comprehension of skin\ndiseases and deep learning by broader communities, this paper could provide\nthe understanding of the major concepts related to skin disease and deep learn-\ning at an appropriate level. It should be noted that the goal of the review is\nnot to exhaust the literature in the ﬁeld. Instead, we summarize the related\nrepresentative works published before/in the year 2019 and provide suggestions\nto deal with current challenges faced in the ﬁeld by referring recent works until\n3\nthe year 2020.\nCompared with previous related works, the contributions in this paper can be\nsummarized as follows. First, we systematically introduce the recent advances in\nskin disease diagnosis with deep learning from several aspects, including the skin\ndisease and public datasets, concepts of deep learning and popular architectures,\napplications of deep learning in skin disease diagnosis tasks. Though there have\nbeen papers that reviewed works on skin disease diagnosis, some of them [32]\nfocused on traditional machine learning and deep learning only occupied a small\nsection of them. Alternatively, others [33] only discussed speciﬁc skin diseases\ndiagnosis task (e.g., classiﬁcation) and the presented deep learning methods were\nout of date. By contrast, this paper provides a systematic survey of the ﬁeld\nof skin disease diagnosis focusing on recent applications of deep learning. With\nthis article, one could obtain an intuitive understanding of the essential concepts\nof the ﬁeld of skin disease diagnosis with deep learning. Second, we present\ndiscussions about the challenges faced in the ﬁeld and suggest several possible\ndirections to deal with these issues. These can be taken into consideration by\nones who are willing to work further in this ﬁeld in the future.\nThe remainder of the paper is structured as follows. Section 2 brieﬂy in-\ntroduces the skin disease and Section 3 touches upon the common skin image\nacquisition methods and available public skin disease datasets for training and\ntesting deep learning models. In section 4, we introduce the conception of deep\nlearning and popular architectures. Section 5 brieﬂy introduces the common\ndeep learning frameworks and evaluation metrics for testing the eﬀectiveness of\nan algorithm are presented in section 6. After that, we investigate the applica-\ntions of deep learning methods in skin disease diagnosis according to the types\nof tasks in section 7. Then we highlight the challenges in the area of skin disease\ndiagnosis with deep learning and suggest future directions dealing with these\nchallenges in section 8. Finally, we conclude the article in Section 9.\n2. Skin disease\nSkin is the largest immense organ of the human body, consisting of epidermis,\ndermis and hypodermis. The skin has three main functions: auspice, sensation\nand thermoregulation, providing an excellent aegis against aggression of the\nenvironment. Stratum corneum is the top layer of the epidermis and optically\nneutral protective layer with varying thickness. The stratum corneum consists of\nkeratinocytes that produce keratin responsible for beneﬁting the skin to protect\nthe body.\nThe incident of light on the skin is scattered due to the stratum\ncorneum. The epidermis includes melanocytes in its basal layer. Particularly,\nmelanocytes make the skin generate pigment called as melanin, which provides\nthe tan or brown color of the skin. Melanocytes act as a ﬁlter and protect the\nskin from harmful ultraviolet (UV) sunrays by generating more melanin. The\nextent of absorption of UV rays depends on the concentration of melanocytes.\nHowever, the unusual growth of melanocytes causes melanoma. The dermis is\nlocated at the middle layer of the skin, consisting of collagen ﬁbers, sensors,\n4\nreceptors, blood vessels and nerve ends. It provides elasticity and vigor to the\nskin [32].\nDeoxyribonucleic acid (DNA) consists of molecules called nucleotides. A nu-\ncleotide comprises of a phosphate and a sugar group along with a nitrogen base.\nThe order of nitrogen bases in the DNA sequence forms the genes. Genes decide\nthe formation, multiplication, division and death of cells. Oncogenes are respon-\nsible for the multiplication and division of cells. Protective genes are known as\ntumor suppressor genes. Usually, they inhibit cell growth by monitoring how\nexpeditiously cells divide into incipient cells, rehabilitating mismatched DNA\nand controlling when a cell dies. The uncontrollability of a cell occurs due to\nthe mutation of the tumor suppressor genes, eventually forming a mass called\ntumor (cancer). UV rays can damage the DNA, which causes the melanocytes\nto produce melanin at a high abnormal rate. Appropriate amount of UV rays\nbeneﬁts the skin to form vitamin D, but excess will cause pigmented skin le-\nsions [34]. Particularly, the malignant tumor occurred due to abnormal growth\nof the melanocytes is called as melanoma [35].\nThere are three major types of skin cancers, i.e., malignant melanoma (MM),\nsquamous cell carcinoma, and basal cell carcinoma. In particular, the latter two\nare developed from basal and squamous keratinocytes and also known as ker-\natinocyte carcinoma (KC). They are the most commonly occurring skin cancers\nin men and women, with over 4.3 million cases of BCC and 1 million cases\nof SCC diagnosed each year in the United States, although these numbers are\nlikely to be underestimated [36]. However, MM, an aggressive malignancy of\nmelanocytes, is a less common but far more deadly skin cancer. It often starts\nas minuscule, with a gradual change in size and color. The color of melanin\nessentially depends on its localization in the skin. The color ebony is due to\nmelanin located in the stratum corneum. Light to dark brown, gray to gray-\nblue and steel-blue are observed in the upper epidermis, papillary dermis and\nreticular dermis respectively. In case of benign lesions, the exorbitant melanin\ndeposit presents in the epidermis. Melanin presence in the dermis is the most\nconsequential designation of melanoma causing prominent vicissitude in skin\ncoloration. There are several other designations for melanoma, including thick-\nened collagen ﬁbers in addition to pale lesion areas with a large blood supply\nat the periphery. The gross morphologic features additionally include shape,\nsize, coloration, border and symmetry of the pigmented lesion.\nBiopsy and\nhistology are required to perform explicit diagnosis in case the ocular approxi-\nmation corroborates a suspicion of skin cancer [37]. According to microscopic\ncharacterizations of the lesion, there are four major categories of melanoma,\ni.e., superﬁcial spreading melanoma (SSM), nodular melanoma (NM), lentigo\nmalignant melanoma (LMM) and acral lentiginous melanoma (ALM).\n3. Image acquisition and datasets\n3.1. Image acquisition\nDermatology is termed as a visual specialty wherein most diagnosis can be\nperformed by visual inspection of the skin.\nEquipment-aided visual inspec-\n5\ntion is important for dermatologists since it can provide crucial information for\nprecise early diagnosis of skin diseases. Subtle features of skin diseases need\nfurther magniﬁcation such that experienced dermatologists can visualize them\nclearly [38]. In some cases, a skin biopsy is needed which provides the opportu-\nnity for a microscopic visual examination of the lesion in question. Lots of image\nacquisition approaches were developed to facilitate dermatologists to overcome\nproblems caused by apperception of minuscule sized skin lesions.\nDermoscopy, one of the most widely used image acquisition methods in der-\nmatology, is a non-invasive imaging technique that allows the visualization of\nskin surface by the light magnifying device and immersion ﬂuid [39]. Statistics\nshows that dermoscopy has improved the diagnosis performance of malignant\ncases by 50% [40]. Kolhaus was the ﬁrst one to start skin surface microscopy\nin 1663 to inspect minuscule vessels in nail folds [41]. The term dermatoscopy\nwas coined by Johann Saphier, a German dermatologist, in 1920 and then der-\nmatoscopy is employed for skin lesion evaluation [42]. Dermoscopy additonally\nkenned as epiluminescence microscopy (ELM) is a non-invasive method that can\nbe utilized in vivo evaluation of colors and microstructure of the epidermis. The\ndermo-epidermal junction and papillary dermis cannot be observed by unclad\nocular techniques [43]. These structures form the histopathological features that\ndetermine the level of malignancy and indicate whether the lesion is necessary to\nbe biopsied [44]. The basic principal of dermoscopy is transillumination of the\nskin lesion. The stratum corneum is optically neutral. Due to the incidence of\nvisible radiation on the surface of skin, reﬂection occurs at the stratum corneum\nair interface [45]. Oily skin enables light to pass through it; therefore, linkage\nﬂuids applied on the surface of the skin make it possible to magnify the skin\nand access to deeper layers of the skin structures [46]. However, the scope of\nobservable structures is restricted compared with other techniques, presenting\na potentially subjective diagnosis precision. It was shown that the diagnosis\nprecision depended on the experience of dermatologists [47].\nDermoscopy is\nutilized by most of the dermatologists in order to reduce patient concern and\npresent early diagnosis.\nIn vivo, the confocal laser scanning microscopy (CLSM), a novel image acqui-\nsition equipment, enables the study of skin morphology in legitimate period at a\nresolution equal to that of the traditional microscopes [48]. In CSLM, a focused\nlaser beam is used to enlighten a solid point inside the skin and the reﬂection\nof light starting there is measured. Gray-scale image is obtained by examining\nthe territory paralleling to the skin surface. According to the review [49], a\nsensitivity of 88% and speciﬁcity of 71% were obtained with CSLM. However,\nthe confocal magnifying lens in CSLM involves high cost (up to $50, 000 to\n$100, 000).\nOptical coherence tomography (OCT) is a high-determination non-obtrusive\nimaging approach that has been utilized in restorative examinations. The sen-\nsitivity and speciﬁcity vary between 79% to 94% and 85% to 96%, respec-\ntively [50].\nThe diagnosis performed with OCT is less precise than that of\nclinical diagnosis. However, a higher precision can be obtained for distinguish-\ning lesions from the normal skin.\n6\nThe utilization of a skin imaging contrivance is referred as spectrophoto-\nmetric or spectral intracutaneous analysis (SIA) of skin lesions. The SIA scope\ncan improve the performance of practicing clinicians in the early diagnosis of\nthe deadly disease. A study has reported that SIA scope presented the same\nsensitivity and speciﬁcity as these of dermatoscopy performed by skilled der-\nmatologists [51].\nThe interpretation of these images is laborious due to the\ninvolution of the optical processes involved.\nUltrasound imaging [52] is an important tool for skin disease diagnosis. It\nprovides information in terms of patterns associated with lymph nodes and\ndepth extent of the underlying tissues respectively, which is very useful when\ntreating inﬂammatory diseases such as scleroderma or psoriasis.\nMagnetic resonance imaging (MRI) [53] has also been widely utilized in the\nexamination of pigmented skin lesions. The application of MRI to dermatology\nhas become practice with the use of specialized surface coils that allow higher\nresolution imaging than standard MRI coils. The application of MRI in der-\nmatology can provide a detailed picture of a tumor and its depth of invasion in\nrelation to adjacent anatomic structures as well as delineate pathways of tumor\ninvasion [54]. For instance, MRI has been used to diﬀerentially evaluate malig-\nnant melanoma tumors and subcutaneous and pigmented skin of nodular and\nsuperﬁcial spreading melanoma [55].\nWith the development of machine learning, there have been many works\nusing images obtained by digit cameras or smart phones for skin disease di-\nagnosis [56, 57]. Though the quality of these images are not as high as these\nobtained with professional equipments, such as dermatoscopies, excellent diag-\nnosis performance can also be achieved with advanced image processing and\nanalysis methods.\nApart from the above methods, there are a few other imaging acquisition\napproaches, including Mole Max, Mole Analyzer, real time Raman spectroscopy,\nelectrical impedance spectroscopy, ﬁber diﬀraction, and thermal imaging. Due\nto the limited space, we omit the detailed introduction of these methods here\nand the readers may refer to related literature if interested.\n3.2. Datasets\nHigh-quality data has always been the primary requirement of learning re-\nliable algorithms. Particularly, training a deep neural network requires large\namount of labeled data. Therefore, high-quality skin disease data with reliable\ndiagnosis labels is signiﬁcant for the development of advanced algorithms. Three\nmajor types of modalities are utilized for skin disease diagnosis, i.e., clinical im-\nages, dermoscopy images and pathological images. Speciﬁcally, clinical images\nof skin lesions are usually captured with mobile cameras for remote examina-\ntion and taken as medical records for patients [58]. Dermoscopy images are\nobtained with high-resolution digital single-lens reﬂex (DSLR) or smart phone\ncamera attachments. Pathological images, captured by scanning tissue slides\nwith microscopes and digitalized as images, are served as a gold standard for\nskin disease diagnosis. Recently, many public datasets for skin disease diagnosis\n7\ntasks have started to emerge. There exists growing trend in the research com-\nmunity to list these datasets for reference. In the following, we present several\npublicly available datasets for skin disease.\nThe publicly available PH2 dataset1 of dermoscopy images was built by\nMendonca et. al. in 2003, including 80 common nevi, 80 atypical nevi, and\n40 melanomas [59]. The dermoscopy images were obtained at the Dermatology\nService of Hospital Pedro Hispano (Matosinhos, Portugal) under the same con-\nditions through Tuebinger Mole Analyzer system using a magniﬁcation of 20x.\nThey are 8-bit RGB color images with a resolution of 768 × 560 pixels. The\ndataset includes medical annotation of all the images, namely medical segmen-\ntation of lesions, clinical and histological diagnoses and the assessment of several\ndermoscopic criteria (i.e., colors, pigment network, dots/globules, streaks, re-\ngression areas, blue-whitish veil).\nSince the dataset includes comprehensive\nmetadata, it is often utilized as a benchmark dataset for evaluating algorithms\nfor melanoma diagnosis.\nLiao [60] built a skin disease dataset for universal skin disease classiﬁcation\nfrom two diﬀerent resources: Dermnet and OLE. Dermnet is one of the largest\npublicly available photo dermatology sources [61]. It contains more than 23, 000\nimages of skin diseases with various skin conditions and the images are organized\nin a two-level taxonomy. Speciﬁcally, the bottom-level includes images of more\nthan 600 kind of skin diseases in a ﬁne-grained granularity and the top-level\nincludes images of 23 kind of skin diseases. Each class of the top-level includes\na subcollection of the bottom-level.\nOLE dataset includes more than 1, 300\nimages of skin diseases from the New York State Department of Health. The\nimages can be categorized into 19 classes and each class can be mapped to one\nof the bottom-level class of the Dermnet dataset.\nIn light of this, Liao [60]\nlabeled the 19 classes of images from OLE with their top-level counterparts\nfrom Dermnet. It should be noted that the images from the above two datasets\ncontain watermarks. To utilize the two datasets, Liao performed two diﬀerent\nexperiments. One was to train and test CNN models on the Dermnet dataset\nonly, while the other was to train CNN models on the Dermnet dataset and test\nthem on the OLE dataset.\nThe International Skin Imaging Collaboration (ISIC) aggregated a large-\nscale publicly available dataset of dermoscopy images [62]. The dataset contains\nmore than 20, 000 images from leading clinical centers internationally, acquired\nfrom various devices used at each center. The ISIC dataset was ﬁrst released for\nthe public benchmark challenge on dermoscopy image analysis in 2016 [63, 64].\nThe goal of the challenge was to provide a dataset to promote the development\nof automated melanoma diagnosis algorithms in terms of segmentation, dermo-\nscopic features detection and classiﬁcation. In 2017, the ISIC hosted the second\nterm of the challenge with an extended dataset. The extended dataset pro-\nvides 2, 000 images for training, with masks for segmentation, superpixel masks\nfor dermoscopic feature extraction and annotations for classiﬁcation [65]. The\n1http://www.fc.up.pt/addi/\n8\nimages are categorized into three classes, i.e., melanoma, seborrheic keratosis\nand nevus. Melanoma is malignant skin tumor while the other two are the be-\nnign skin tumors derived from diverse cells. Additionally, the ISIC provides a\nvalidation set with extra 150 images for evaluation.\nThe HAM10000 (Human Against Machine with 10, 000 training images)\ndataset released by Tschandl et. al. includes dermoscopy images from diverse\npopulations acquired and stored by diﬀerent modalities [66].\nThe dataset is\npublicly available through the ISIC archive and consists of 10, 015 dermoscopy\nimages, which are utilized as a training set for testing machine learning algo-\nrithms.\nCases include a representative collection of all important diagnostic\ncategories in the realm of pigmented lesions. The diagnoses of all melanomas\nwere veriﬁed through histopathological evaluation of biopsies, while the diag-\nnoses of nevi were made by either histopathological examination (24%), expert\nconsensus (54%) or another diagnosis method, such as a series of images that\nshowed no temporal changes (22%).\nThe Interactive Atlas of Dermoscopy (IAD) [67] is a multimedia project for\nmedical education based on a CD-ROM dataset and the dataset includes 2, 000\ndermoscopy images and 800 context images, i.e. non-dermoscopy regular photos.\nImages in the dataset are labeled as either a melanoma or benign lesion based\non pathology report.\nThe MED-NODE dataset2 consists of 70 melanoma and 100 naevus images\nfrom the digital image archive of the Department of Dermatology of the Uni-\nversity Medical Center Groningen (UMCG). It is used for the development and\ntesting of the MED-NODE system for skin cancer detection from macroscopy\nimages [67].\nDermnet is the largest independent photo dermatology source dedicated to\nonline medical education through articles, photos and videos [61].\nDermnet\nprovides information on a wide variety of skin conditions through innovative\nmedia. It contains over 23, 000 images of skin diseases. Images can be enlarged\nvia a click and located by browsing image categories or using a search engine.\nThe images and videos are available without charge, and users can purchase and\nlicense high-resolution copies of images for publishing purposes.\nThe Dermoﬁt Image Library is a collection of 1, 300 focal high-quality skin\nlesion images collected under standardized conditions with internal color stan-\ndards [10]. The lesions span across ten diﬀerent classes, including actinic ker-\natosis, basal cell carcinoma, melanocytic nevus, seborrhoeic keratosis, squamous\ncell carcinoma, intraepithelial carcinoma, pyogenic granuloma, haemangioma,\ndermatoﬁbroma, and malignant melanoma. Each image has a gold standard di-\nagnosis based on expert opinions (including dermatologists and dermatopathol-\nogists). Images consist of a snapshot of the lesion surrounded by some normal\nskin. A binary segmentation mask that denotes the lesion area is included with\neach lesion.\nThe Hallym dataset consists of 152 basal cell carcinoma images obtained\n2http://www.cs.rug.nl/˜imaging/databases/melanoma naevi/\n9\nfrom 106 patients treated between 2010 and 2016 at Dongtan Sacred Heart\nHospital, Hallym University, and Sanggye Paik Hospital, Inje University [68].\nAtlasDerm contains 10, 129 images of all kinds of dermatology diseases.\nSamuel Freire da Silva, M.D. created it in homage to The Master And Pro-\nfessor Delso Bringel Calheiros [69].\nDanderm contains more than 3, 000 clinical images of common skin diseases.\nThis atlas of clinical dermatology is based on photographs taken by Niels K.\nVeien in a private practice of dermatology [70].\nDerm101 is an online and mobile resource3 for physicians and healthcare\nprofessionals to learn the diagnosis and treatment of dermatologic diseases [71].\nThe resource includes online textbooks, interactive quizzes, peer-reviewed open\naccess dermatology journals, a dermatologic surgery video library, case studies,\nthousands of clinical photographs and photomicrographs of skin diseases, and\nmobile applications.\n7-point criteria evaluation dataset4 includes over 2, 000 dermoscopy and clin-\nical images of skin lesions, with 7-point checklist criteria and disease diagnosis\nannotated [72]. Additionally, derm7pt5, a Python module, serves as a starting\npoint to use the dataset. It preprocesses the dataset and converts the data into\na more accessible format.\nThe SD-198 dataset6 is a publicly available clinical skin disease image dataset.\nIt was built by Sun et al. and includes 6, 584 images from 198 classes, varying\nin terms of scale, color, shape and structure [73].\nDermIS.net is the largest dermatology information service available on the\ninternet. It oﬀers elaborate image atlases (DOIA and PeDOIA) complete with\ndiagnoses and diﬀerential diagnoses, case reports and additional information on\nalmost all skin diseases [74].\nMoleMap7 is a dataset that contains 102, 451 images with 25 skin conditions,\nincluding 22 benign categories and 3 cancerous categories. In particular, the\ncancerous categories include melanoma (pink melanoma, normal melanoma and\nlentigo melanoma), basal cell carcinoma and squamous cell carcinoma [75]. Each\nlesion has two images: a close-up image taken at a distance of 10 cm from the\nlesion (called the macro) and a dermoscopy image of the lesion (called the micro).\nImages were selected according to four criterion: 1) each image has a disease\nspeciﬁc diagnosis (e.g., blue nevus); 2) there are at least 100 images with the\nsame diagnosis; 3) the image quality is acceptable (e.g., with good contrast); 4)\nthe lesion occupies most of the image without much surrounding tissues.\nAsan dataset [68] was collected from the Department of Dermatology at\nAsan Medical Center. It contains 17, 125 clinical images of 12 types of skin\ndiseases found in Asian people. In particular, the Asan Test dataset containing\n1, 276 images is available to be downloaded for research.\n3www.derm101.com\n4http://derm.cs.sfu.ca\n5https://github.com/jeremykawahara/derm7pt\n6https://drive.google.com/ﬁle/d/1YgnKz3hnzD3umEYHAgd29n2AwedV1Jmg/view\n7http://molemap.co.nz\n10\nTable 1: List of public datasets for skin disease.\nDataset\nNo. of images\nType of skin disease\nPH2 dataset [59]\n200\nCommon nevi, melanomas, atypical\nnevi\n[60]\n> 3,600\n19 classes\nISIC [62]\n> 20,000\nMelanoma, seborrheic keratosis, be-\nnign nevi\nHAM10000 [66]\n10,015\nImportant diagnostic categories of\npigmented lesions\nIAD [67]\n2,000\nMelanoma and benign lesion\nMED-NODE dataset [67]\n170\nMelanoma and nevi\nDermnet [61]\n23,000\nAll kinds of skin diseases\nDermoﬁt Image Library [10]\n1,300\n10 diﬀerent classes\nHallym dataset [68]\n152\nBasal cell carcinoma\nAtlasDerm [69]\n10,129\nAll kinds of skin diseases\nDanderm [70]\n3,000\nCommon skin diseases\nDerm101 [71]\nThousands\nAll kinds of skin diseases\n7-point\ncriteria\nevaluation\ndataset [72]\n> 2,000\nMelanoma and non-melanoma\nSD-198 dataset [73]\n6,584\n198 classes\nDermIS [74]\nThousands\nAll kinds of skin diseases\nMoleMap [75]\n102, 451\n22 benign categories and 3 cancerous\ncategories\nAsan dataset [68]\n17, 125\n12 types of skin diseases found in\nAsian people\nThe Cancer Genome Atlas [76]\n2, 860\nCommon skin diseases\nThe Cancer Genome Atlas [76] is one of the largest collections of pathological\nskin lesion slides that contains 2, 860 cases. The atlas is publicly available to be\ndownloaded for research.\nThe above publicly available datasets for skin diseases are listed in Table 1.\nThis may not an exhaustive list for skin disease diagnosis and readers could\nresearch the internet for that purpose if interested. From the description of the\nabove skin datasets we can observe that these datasets are usually small in terms\nof the samples and patients. Compared to the datasets for general computer\nvision tasks, where datasets typically contain a few hundred thousand and even\nmillions of labeled data, the data sizes for skin disease diagnosis tasks are too\nsmall.\n4. Deep learning\nIn the area of machine learning, people design models to enable computers to\nsolve problems by learning from experiences. The aim is to develop models that\ncan be trained to produce valuable results when fed with new data. Machine\nlearning models transform their input into output with statistical or data-driven\nrules derived from large numbers of examples [77]. They are tuned with training\ndata to obtain accurate predictions.\nThe ability of generalizing the learned\nexpertise to make correct predictions for new data is the main goal of the models.\nThe generalization ability of a model is estimated during the training process\nwith a separate validation dataset and utilized as feedback for further tuning.\nThen the fully tuned model is evaluated on a testing dataset to investigate how\nwell the model makes predictions for new data.\n11\nThere are several types of machine learning models, which can be classi-\nﬁed into three categories, i.e., supervised learning, semi-supervised learning and\nunsupervised learning models, according to how the data is used for training a\nmodel. In supervised learning, a model is trained with labeled or annotated data\nand then used to make predictions for new, unseen data. It is called supervised\nlearning since the process of learning from the training data can be considered\nas a teacher supervising the learning process. Most of machine learning models\nadopt supervised learning. For instance, classifying skin lesions into classes of\n“benign” or “malignant” is a task using supervised learning [78]. By contrast,\nin unsupervised learning, the model is aimed to discover the underlying distri-\nbution or structure in the data in order to learn more about the data without\nguidance. Clustering [79] is a typical unsupervised learning model. Problems\nwhere you have large amounts of data and only some of the data is labeled are\ncalled semi-supervised learning problems [80]. These problems sit in between\nboth supervised learning and unsupervised learning. Actually, many real-world\nmachine learning problems, especially medical image processing, fall into this\ntype. It is because that labeling large amounts of data can be expensive or\ntime-consuming.\nBy contrast, unlabeled data is more common and easy to\nobtain.\nMachine learning has a long history and can be split into many subareas.\nParticularly, deep learning is a branch of machine learning and has been pop-\nular in the past few years. Previously, designing a machine learning algorithm\nrequired domain information or human engineering to extract meaningful fea-\ntures that can be a representation of data and input to an algorithm for pattern\nrecognition. However, a deep learning model consisting of multiple layers is\na kind of representation learning method that transforms the input raw data\ninto needed representation for pattern recognition without much human inter-\nference. The layers in a deep learning architecture are arranged sequentially\nand composed of large numbers of predeﬁned, nonlinear operations, such that\nthe output of one layer is input to the next layer to form more complex and ab-\nstract representations. In this way, a deep learning architecture is able to learn\ncomplex functions. With the ability of running on specialized computational\nhardware, deep learning models adapt large-scale data and can be optimized\nwith more data continually. As a result, deep learning algorithms outperform\nmost of conventional machine learning algorithms in many problems. People\nhave witnessed the huge development of deep learning algorithms and their ex-\ntensive applications in various tasks, such as object classiﬁcation [20, 81, 82],\nmachine translation [83, 84] and speech recognition [85, 86, 87]. Particularly,\nhealthcare and medicine beneﬁt a lot from the prevalence of deep learning due to\nthe huge volume of medical data [77, 88]. Three major factors have contributed\nthe success of deep learning for solving complex problems of modern society,\nincluding: 1) availability of massive training data. With the ubiquitous digiti-\nzation of information in recent world, public suﬃciently large volumes of data\nis available to train complex deep learning models; 2) availability of powerful\ncomputational resources. Training complex deep learning models with massive\ndata requires immense computational power. Only the availability of powerful\n12\ncomputational resources, especially the improvements in graphic processing unit\n(GPU) performance and the development of methods to use the GPU for compu-\ntation, in recent times fulﬁlls such requirements; 3) availability of deep learning\nframeworks. People in diverse research communities are more and more willing\nto share their source codes on public platforms. Easy access to deep learning\nalgorithm implementations, such as GoogLeNet [89], ResNet [19], DenseNet [90]\nand SENet [91], has accelerated the speed of applying deep learning to practical\ntasks.\nCommonly, deep learning models are trained in a supervised way, i.e., the\ndatasets for training contain data points (e.g., images of skin diseases) and\ncorresponding labels (e.g., “benign” or “malignant”) simultaneously. However,\ndata labels are limited for healthcare data since labeling large numbers of data\nis expensive and diﬃcult. Recently, semi-supervised and unsupervised learning\nhave attracted much attention to alleviate the issues caused by limited labeled\ndata. There have been many excellent reviews and surveys of deep learning [92,\n93, 94, 95] and interested readers can refer them for more details.\nIn the following, we brieﬂy introduce the essential part of deep learning,\naiming to provide a useful guidance to the area of skin disease diagnosis that\nare currently inﬂuenced by deep learning.\n4.1. Neural networks\nNeural networks are a type of learning algorithm that formulates the basis\nof most deep learning algorithms. A neural network consists of neurons or units\nwith activation z and parameters Θ = {ω, β}, where ω is a set of weights and\nβ a set of biases. The activation z is expressed as a linear combination of the\ninput x to the neuron and parameters, followed with an element-wise nonlinear\nactivation function σ(·):\nz = σ(wT x + b),\n(1)\nwhere w ∈ω is the weight and b ∈β is the bias. Typical activation functions for\nneural networks include the sigmoid function and hyperbolic tangent function.\nParticularly, the multi-layer perceptrons (MLPs) are the most well-known neural\nnetworks, containing multiple layers of this kind of transformations:\nf(x; Θ) = σ(W L(σ(W L−1 · · · σ(W 0 + b0) · · · + bL−1) + bL),\n(2)\nwhere W n, n = 1, 2, · · · , L is a matrix consisting of rows wk, k = 1, 2, · · · , nc\nwhich are associated with the k-th activation in the output, L indicates the total\nnumber of layers and nc indicates the number of nodes at the n-th layer. The\nlayers between the input and output layers are often called as “hidden” layers.\nWhen a neural network contains multiple layers, then we say it is a deep neural\nnetwork. Hence, we have the term “deep learning”.\nCommonly, the activations of the ﬁnal layer of a network are mapped to a\ndistribution over classes p(y|x; Θ) via a softmax function [95]:\np(y|x; Θ) = softmax(x; Θ) =\ne(wL\nc )T x+bL\nc\nPC\nc=1 e(wL\nc )T x+bL\nc ,\n(3)\n13\nFigure 2: An example of a 4-layer MLPs.\nwhere wL\nc indicates the weight that produces the output node corresponding to\nclass c. An example of a 4-layer MLPs is illustrated in Fig. 2.\nCurrently, stochastic gradient descent (SGD) is the most popular method\nused for tuning the parameters Θ for a speciﬁc dataset. In SGD, a mini-batch,\ni.e., a small subset of the dataset, is utilized for the gradient update instead\nof the whole dataset. Tuning the parameters is to minimize the negative log-\nlikelihood:\narg minΘ −\nN\nX\nn=1\nlog(p(yn|xn; Θ)).\n(4)\nPractically, one can design the loss function according to the speciﬁc tasks. For\nexample, the binary cross-entropy loss is used for two-class classiﬁcation prob-\nlems and the categorical cross-entropy loss for multi-class classiﬁcation prob-\nlems.\nFor a long time, people considered that deep neural networks (DNNs) were\nhard to train. Major breakthrough was made in 2006 when researchers showed\nthat training DNNs layer-by-layer in an unsupervised way (pretraining), fol-\nlowed with a supervised ﬁne-tuning of the stacked layers, could obtain promis-\ning performance [96, 97, 96]. Particularly, the two popular networks trained in\nsuch a manner are stacked autoencoders (SAEs) [98] and deep belief networks\n(DBNs) [99]. However, such techniques are complicated and require many en-\ngineering tricks to obtain satisfying performance.\nCurrently, most popular architectures are trained end-to-end in a supervised\nway, which greatly simpliﬁes the training processes. The most prevalent models\nare convolutional neural networks (CNNs) [20] and recurrent neural networks\n(RNNs) [100]. In particular, CNNs are extensively applied in the ﬁeld of medical\nimage analysis [101, 102, 103]. They are powerful tools for extracting features\nfrom images and other structured data. Before it became possible to utilize\nCNNs eﬃciently, features were typically obtained by handcrafted engineering\nmethods or less powerful traditional machine learning models.\nThe features\nlearned from the data directly with CNNs show superior performance compared\nwith the handcrafted features. There are strong preferences about how CNNs\n14\nFigure 3: An illustration of a typical CNN.\nare constructed, which can beneﬁt us to understand why they are so powerful.\nTherefore, we give a brief introduction to the building blocks of CNNs in the\nfollowing.\n4.2. Convolutional neural networks\nOne can utilize the feedforward neural networks discussed above to process\nimages. However, having connections between all the nodes in one layer and\nall the nodes in the next layer is quite ineﬃcient.\nA careful pruning of the\nconnections based on the structure of images can lead to better performance\nwith high eﬃciency. CNNs are special kind of neural networks that preserve\nthe spatial relationships in the data with very few connections between layers.\nCNNs are able to extract meaningful representations from input data, which\nare particularly appropriate for image-oriented problems. A CNN consists of\nmultiple layers of convolutions and activations, with pooling layers interspersed\nbetween diﬀerent convolution layers.\nIt is trained via backpropagation and\nSGD similar with the standard neural networks. Additionally, a CNN typically\nincludes fully-connected layers at the end of the architecture to produce the\noutput. A typical CNN is demonstrated in Fig. 3.\n4.2.1. Convolutional layers\nIn the convolutional layers, the output activations of the previous layer are\nconvolved with a set of ﬁlters represented with a tensor Wj,i, where j is the\nﬁlter number and i is the layer number. Fig. 4 demonstrates a 2D convolution\noperation. The operation involves moving a small window of size 3×3 over a 2D\ngrid (e.g., an image or a feature map) in a left-to-right and up-to-down order.\nAt each step, the corresponding elements of the window and grid are multiplied\nand summed up to obtain a scalar value. With all the obtained values, another\n2D grid is produced, referred as feature map in a CNN. By having each ﬁlter\nshare the same weights across the whole input domain, much less number of\nweights is needed. The motivation of the weight-sharing mechanism is that the\nfeatures appearing in one part of the image are likely to appear in other parts\nas well [104]. For example, if you have a ﬁlter that can detect vertical lines,\nthen it can be utilized to detect lines wherever they appear. Applying all the\nconvolutional ﬁlters to all locations of the input results in a set of feature maps.\n15\nFigure 4: An illustration of a 2D convolution operation.\n4.2.2. Activation layers\nThe outputs from convolutional layers are fed into a nonlinear activation\nfunction, which makes it possible for the neural network to approximate almost\nany nonlinear functions [105].\nIt should be noted that a multi-layer neural\nnetwork constructed with linear activation functions can only approximate linear\nfunctions. The most common activation function is rectiﬁed linear units (ReLU),\nwhich is deﬁned as ReLU(z) = max(0, z). There have many variants of ReLU,\nsuch as leaky ReLU (LeakyReLU) [106] and parametric ReLU (PReLU) [107].\nThe outputs of the activation functions are new tensors and we call them feature\nmaps.\n4.2.3. Pooling layers\nThe feature maps output by the activation layers are then typically pooled\nin the pooling layers. The pooling operations are performed on a small region\n(e.g., a square region) of the input feature maps and only one single value is\nobtained with certain scheme. The common schemes utilized to compute the\nvalue are max function (max pooling) and average function (average pooling).\nA small shift in the input image will lead to small changes in the activation\nmaps; however, the pooling operation enables the CNNs to have the translation\ninvariance property. Another way to obtain the same downsampling eﬀect as\nthe pooling operation is to perform convolution with a stride larger than one\npixel. Researches have shown that removing pooling layer could simplify the\nnetworks without sacriﬁcing performances [108].\nBesides the above building blocks, other important elements in many CNNs\ninclude dropout and batch normalization. Dropout [109] is a simple but pow-\nerful tool to boost the performance of CNNs. Averaging the performance of\nseveral models in an ensemble one tends to obtain better performance than any\nof the single model. Dropout performs similar averaging operation based on the\nstochastic sampling of neural networks. With dropout, one randomly removes\nneurons in the networks during training process, ending up utilizing slightly\ndiﬀerent networks for each batch of the training data. As a result, the weights\nof the networks are tuned based on optimizing multiple diﬀerent variants of\nthe original networks. Batch normalization is often placed after the activation\nlayers and produces normalized feature maps by subtracting the mean and di-\nviding with the standard deviation for each training batch [110]. With batch\n16\nnormalization, the networks are forced to keep their activations being zero mean\nand unit standard deviation, which works as a network regularization. In this\nway, the networks training process can be speeded up and less dependent on the\ncareful parameter initialization.\nWhen designing new and more advanced CNN architectures, these compo-\nnents are combined together in a more complicated way and other ingredients\ncan be added as well. To construct a speciﬁc CNN architecture for a practical\ntask, there are a few factors to be considered, including understanding the tasks\nto be solved and the requirements to be satisﬁed, ﬁnding out how to prepro-\ncess the data before input to a network, and making full use of the available\nbudget of computation. In the early days of modern deep learning, people de-\nsigned networks simply with the combination of the above building blocks, such\nas LeNet [111] and AlexNet [20]. Later, the architectures of networks became\nmore and more complex in a way that they were built based on the ideas and\ninsights of previous models. Table 2 and 3 demonstrate a few popular deep net-\nwork architectures, hoping to show how the building blocks can be combined to\ncreate networks with excellent performances. These DNNs are typically imple-\nmented in one or more of a small number of deep learning frameworks that are\nintroduced in detail in the next section. Thanks to the software development\nplatform, such as GitHub, the implementation of large numbers of DNNs with\nthe main deep learning frameworks have been made publicly accessible, which\nmakes it easier for people to reproduce or reuse these models.\n5. Deep learning frameworks\nWith the prevalence of deep learning, there are several open source deep\nlearning frameworks aiming to simplify the implementation of complex and\nlarge-scale deep learning models. Deep learning frameworks provide building\nblocks for designing, training and validating DNNs with high-level program-\nming interfaces. Thus, people can implement complex models like CNNs con-\nveniently.\nIn the following, we present a brief introduction to popular deep\nlearning frameworks.\nTensorFlow [125] was developed by researchers and engineers from the Google\nBrain team. It is by far the most popular software library in the ﬁeld of deep\nlearning (though others are catching up quickly). One of the biggest reasons ac-\ncounting for the popularity of TensorFlow is that it supports multiple program-\nming languages, such as Python, C++ and R, to build deep learning models.\nIt is handy for creating and experimenting with deep learning architectures. In\naddition, its formulation is convenient for data (such as inputting graphs, SQL\ntables, and images) integration. Moreover, it provides proper documentations\nand walkthroughs for guidance. The ﬂexible architecture of TensorFlow makes\nit easy for people to run their deep learning models on one or more CPUs and\nGPUs. It is backed by Google, which guarantees that it will stay around for a\nwhile. Therefore, it makes sense to invest time and resources to use it.\nKeras [126] is written with Python and can run on top of TensorFlow (as well\nas CNTK and Theano). The interface of TensorFlow can be a little challenging\n17\nTable 2: A few popular deep network architectures (part 1).\nArchitecture Year\nReference\nDescription\nLeNet\n1990\n[111]\nProposed by Yann LeCun to solve the task of handwritten digit recog-\nnition.\nSince then, the basic architecture of CNN has been ﬁxed:\nconvolutional layer, pooling layer and fully-connected layer.\nAlexNet\n2012\n[20]\nConsidered as one of the most inﬂuential works in the ﬁeld of computer\nvision since it has spurred many more papers utilizing CNN and GPUs\nto accelerate deep learning [112]. The building blocks of the network\ninclude convolutional layers, ReLU activation function, max-pooling\nand dropout regularization. In addition, the authors split the com-\nputations on multiple GPUs to make training faster. It won the 2012\nILSVRC competition by a huge margin.\nVGG-nets\n2014\n[113]\nProposed by the Visual Geometry Group (VGG) of the Oxford Univer-\nsity and won the ﬁrst place for the localization task and the second\nplace for the classiﬁcation task in the 2014 ImageNet competition.\nVGG-nets can be seen as a deeper version of AlexNet. They adopt a\npretraining method for network initialization: train a small network\nﬁrst and ensure that this part of the network is stable, and then go\ndeeper gradually based on this.\nGoogLeNet\n2015\n[89]\nDefeated VGG-nets in the classiﬁcation task of 2014 ImageNet com-\npetition and won the championship.\nDiﬀerent from networks like\nAlexNet, VGG-nets which rely solely on deepening networks to im-\nprove performance, GoogLeNet presents a novel network structure\nwhiling deepens the network (22 layers).\nA inception structure re-\nplaces the traditional operations of convolution and activation. This\nidea was ﬁrst proposed by the Network in Network [114]. In the in-\nception structure, multiple ﬁlters of diverse sizes are performed to the\ninput and the corresponding results are concatenated.\nThis multi-\nscale processing enables the network to extract features at diﬀerent\nscales eﬃciently.\nResNet\n2016\n[19]\nIntroduces the residual module, which makes it easier to train much\ndeeper networks. The residual module consists of a standard pathway\nand a skip connection, providing options to the network to simply copy\nthe activations from one residual module to the next module. In this\nway, information can be preserved when data goes through the layers.\nSome features are best extracted with shallow networks, while oth-\ners are best extracted with deeper ones. Residual modules enable the\nnetwork to include both cases simultaneously, which performs simi-\nlarly as ensemble and increases the ﬂexibility of the network.\nThe\n152-layer ResNet won the 2015 ILSVRC competition, and the authors\nalso successfully trained a version with 1, 001 layers.\nResNext\n2017\n[115]\nBuilt based on ResNet and GoogLeNet by incorporating inception\nmodules between skip connections.\nDenseNet\n2017\n[90]\nA neural network with dense connections. In this network, there is a\ndirect connection between any two layers. That is to say, the input\nof each layer is the union of the outputs of all previous layers, and\nthe feature map learned by the layer is also directly transmitted to\nall layers afterwards. In this way, the network mitigates the problem\nof gradient disappearance, enhances feature propagation, encourages\nfeature reuse, and greatly reduces the amount of parameters.\n18\nTable 3: A few popular network architectures (part 2).\nArchitecture Year\nReference\nDescription\nSENets\n2018\n[91]\nSqueeze-and-Excitation (SE) network, which is built by introducing\nSE modules into existing networks. The SE modules are trained to\nweight the feature maps channel-wise. Consequently, the SENets are\nable to model spatial and channel information separately, enhancing\nthe model capacity with negligible increase in computational costs.\nNASNet\n2018\n[116]\nA CNN architecture designed by AutoML which is a reinforcement\nlearning approach used for neural network architecture searching [117].\nA controller network proposes architectures aimed to perform at a\nspeciﬁc level for a speciﬁc task, and learns to propose better models by\ntrial and error. NASNet was built based on CIFAR-10 with relatively\nmodest computation requirements, outperforming all previous human-\ndesigned networks in the ILSVRC competition.\nGAN\n2014\n[118]\nGenerative adversarial network (GAN) was proposed by Goodfellow\net al. in 2014 and developed rapidly in recent years. A GAN consists\nof two networks that compete against each other. The generative net-\nwork G creates samples to make the discriminative network D think\nthey come from the training data rather than the generative network.\nThe two networks are trained alternatively, where G aims to maxi-\nmize the probability that D makes a mistake while D aims to obtain\nhigh classiﬁcation accuracy.\nThere have been a variety of variants\n(DCGANs [119], CycleGAN [120], SAGAN [121] etc.) so far and they\ndeveloped into a subarea of machine learning.\nU-net\n2015\n[122]\nA very popular and successful network for 2-D medical image segmen-\ntation. Fed with an image, the network ﬁrst downsamples the image\nwith a traditional CNN architecture and then upsamples the resulting\nfeature maps through a serial of transposed convolution operations to\nthe same size as the original input image. Additional, there have skip\nconnections between the downsampling and upsampling counterparts.\nFaster\nR-CNN\n2015\n[26]\nThe faster region-based convolutional network was built based on the\nprevious Fast R-CNN [123] for object detection. The major contri-\nbution of the method is to develop a region proposal network (RPN)\nto further reduce the region proposal computation time. The region\nproposal is nearly cost-free, and therefore the object detection system\ncan run at near real-time frame rates.\nMask\nR-CNN\n2017\n[124]\nExtends Faster R-CNN by adding a branch for predicting an object\nmask in parallel with the existing branch for bounding box recognition.\nThe method can generate a high-quality segmentation mask for each\ninstance while eﬃciently detect the objects in the image.\nMask R-\nCNN is simple to train and adds only a small overhead to Faster\nR-CNN. It outperforms all previous, single-model entries on all three\ntracks of the COCO suite of challenges.\n19\nfor new users since it is a low-level library, and therefore new users may ﬁnd it\nhard to understand certain implementations. By contrast, Keras is a high-level\nAPI, developed with the aim of enabling fast experimentation. It is designed to\nminimize the user actions and make it easy to understand models. However, this\nstrategy makes Keras a less conﬁgurable environment than low-level frameworks.\nEven so, Keras is appropriate for deep learning beginners that are unable to\nunderstand complex models properly.\nIf you want to obtain results quickly,\nKeras will automatically take care of the core tasks and produce outputs. It\nruns seamlessly on multiple CPUs and GPUs.\nPyTorch [127], released by Facebook, is a primary software tool for deep\nlearning after Tensorﬂow. It is a port to the Torch deep learning framework\nthat can be used for building DNNs and executing tensor computations. Torch\nis a Lua-based framework while PyTorch runs on Python. PyTorch is a Python\npackage that oﬀers Tensor computations. Tensors are multidimensional arrays\nlike ndarrays in numpy that can run on GPUs as well. PyTorch utilizes dy-\nnamic computation graphs. Autograd package of PyTorch builds computation\ngraphs from tensors and automatically computes gradients. Instead of prede-\nﬁned graphs with speciﬁc functionalities, PyTorch oﬀers us a framework to build\ncomputation graphs as we go, and even change them during runtime. This is\nvaluable for situations where we do not know how much memory is needed for\ncreating a DNN. The process of training a neural network is simple and clear,\nand PyTorch contains many pretrained models.\nCaﬀe [128] is another popular open source deep learning framework designed\nfor image processing. It was developed by Yangqing Jia during his Ph.D. at\nthe University of California, Berkeley. First of all, it should be noted that its\nsupport for recurrent networks and language modeling is not as great as the\nabove three frameworks. However, Caﬀe presents advantages in terms of the\nspeed of processing and learning from images. Caﬀe provides solid support for\nmultiple interfaces, including C, C++, Python, MATLAB as well as traditional\ncommand line. Moreover, the Caﬀe Model Zoo framework allows us to utilize\npretrained networks, models and weights that can be used to solve deep learning\ntasks.\nSonnet [129] is a deep learning framework built based on top of TensorFlow.\nIt is designed to construct neural networks with complex architectures by the\nworld-famous company DeepMind. The idea of Sonnet is to construct primary\nPython objects corresponding to a speciﬁc part of the neural network. Further-\nmore, these objects are independently connected to computational TensorFlow\ngraphs. Separating the process of creating objects and associating them with\na graph simplify the design of high-level architectures. The main advantage of\nSonnet is that you can utilize it to reproduce the research demonstrated in the\npapers of DeepMind. In summary, it is a ﬂexible functional abstraction tool\nthat is absolutely a worthy opponent for TensorFlow and PyTorch.\nMXNet is a highly scalable deep learning framework that can be applied on\na wide variety of devices [130]. Although it is not as popular as TensorFlow, the\ngrowth of MXNet is likely to be boosted by becoming an Apache project. The\nframework initially supports a large number of programming languages, such as\n20\nC++, Python, R, Julia, JavaScript, Scala, Go and even Perl. The framework is\nvery eﬃcient for parallel computing on multiple GPUs and machines. MXNet\nhas detailed documentation and is easy to use with the ability to choose between\nimperative and symbolic programming styles, making it a great candidate for\nboth beginners and experienced engineers.\nBesides the above six frameworks, there have other less popular but useful\ndeep learning frameworks, such as Microsoft Cognitive Toolkit, Gluon, Swift,\nChainer, DeepLearning4J, Theano, PaddlePaddle and ONNX. Due to the limi-\ntation of space, we cannot detail them all here. If interested, readers may ﬁnd\nmore related information by searching the internet. Note that all the frame-\nworks are built on top of NVIDIA’s CUDA platform and the cuDNN library,\nand are open source and under active development.\n6. Evaluation metrics\n6.1. Segmentation tasks\nFor segmentation tasks, the most common evaluation metric is Intersection-\nover-Union (IoU), also known as Jaccard Index. IoU is to measure the overlap\nbetween the segmented area predicted by algorithms and that of the ground-\ntruth, i.e.,\nIoU = Area of overlap\nArea of union\n(5)\nwherer Area of overlap indicates the overlap of the segmented area predicted\nby algorithms and that of the ground-truth, and Area of union indicates the\nunion of the two items. The value of IoU ranges from 0 to 1 and higher value\nmeans better performance of the algorithms.\nBesides IoU, the following indices are utilized for evaluating a segmentation\nalgorithm as well.\nPixel-level accuracy:\nAC =\nT P + T N\nT P + FP + T N + FN\n(6)\nwhere TP, TN, FP, FN denote true positive, true negative, false positive and\nfalse negative at the pixel level, respectively. Pixel values above 128 are consid-\nered positive, and pixel values below 128 are considered negative.\nPixel-level sensitivity:\nSE =\nT P\nT P + FN\n(7)\nPixel-level speciﬁcity:\nSP =\nT N\nT N + FP\n(8)\nDice Coeﬃcient:\nDI =\n2T P\n2T P + FN + FP\n(9)\n21\nFigure 5: The taxonomy of literature review of skin disease diagnosis with deep learning.\n6.2. Classiﬁcation tasks\nFor classiﬁcation tasks, common evaluation metrics include accuracy, sensi-\ntivity and speciﬁcity, which are the same with those deﬁned for segmentation\ntasks. However, metrics are measured at the whole image level instead of the\npixel level.\nIn addition, the area under the receiver operation characteristic\n(ROC) curve (AUC) and precision are also common measurements.\nThe AUC measures how well a parameter can be distinguished between two\ndiverse groups and is computed by taking the integral of true positive rate\nregarding the false positive rate:\nAUC =\nZ 1\n0\ntpr(fpr)δfpr\n(10)\nPrecision is deﬁned as the following:\nPREC =\nT P\nT P + FP\n(11)\n7. Skin disease diagnosis with deep learning\nGiven the popularity of deep learning, there have been numerous applications\nof deep learning methods in the tasks of skin disease diagnosis. In this section,\nwe review the existing works in skin disease diagnosis that exploit the deep\nlearning technology. From a machine learning perspective, we ﬁrst introduce\nthe common data preprocessing and augmentation methods utilized in deep\nlearning and then present the review of existing literature on applications of\ndeep learning in skin disease diagnosis according to the type of tasks.\nThe\ntaxonomy of the literature review of this section is illustrated in Fig. 5.\n7.1. Data preprocessing and augmentation\n7.1.1. Data preprocessing\nData preprocessing plays an important role in skin disease diagnosis with\ndeep learning. Since there is a huge variation in image resolutions of skin dis-\nease datasets (e.g., ISIC, PH2 and AtlasDerm) and deep networks commonly\n22\nreceive inputs with certain square sizes (e.g., 224 × 224 and 512 × 512), it is\nnecessary to crop or resize the images from these datasets to adapt them to\ndeep learning networks. It should be noted that resizing and cropping images\ndirectly into required sizes might introduce object distortion or substantial in-\nformation loss [131, 132].\nFeasible methods to resolve this issue is to resize\nimages along the shortest side to a uniform scale while maintaining the aspect\nratio. Typically, images are normalized by subtracting the mean value and then\ndivided by the standard deviation, which are calculated over the whole training\nsubset, before fed into a deep learning network. There have works [133, 132]\nreported that subtracting a uniform mean value does not well normalize the\nillumination of individual images since the lighting, skin tones and viewpoints\nof skin disease images may vary greatly across a dataset. To address this issue,\nYu et al. [132] normalized each image by subtracting it with channel-wise mean\nintensity values calculated over the individual image. The experimental results\nin their paper showed that simply subtracting a uniform mean pixel value will\ndecrease the performance of a deep network. In addition, for more accurate seg-\nmentation and classiﬁcation, hair or other unrelated stuﬀs should be removed\nfrom skin images with algorithms including thresholding methods [134, 135],\nmorphological methods [136], and deep learning algorithms [122, 21, 22].\n7.1.2. Data augmentation\nAs is known that large numbers of data are usually required for training a\ndeep learning network to avoid overﬁtting and achieve excellent performances.\nUnfortunately, many applications, such as skin disease diagnosis, can hardly\nhave access to massive labeled training data. In fact, limited data are common\nin the ﬁeld of medical image analysis due to the rarity of disease, patient pri-\nvacy, the requirement of labeling by medical experts and the high cost to obtain\nmedical data [137]. To alleviate this issue, data augmentation, indicating artiﬁ-\ncially transforming original data with some appropriate methods to increase the\namount of available training data, are developed. With feasible data augmenta-\ntion, one can enhance the size and quality of the available training data. With\nadditional data, deep learning architectures are able to learn more signiﬁcant\nproperties, such as rotation and translation invariance.\nPopular data augmentation methods include geometric transformations (e.g.,\nﬂip, crop, translation, and rotation), color space augmentations, kernel ﬁl-\nters, mixing images, random erasing, feature space augmentation, adversar-\nial training, generative adversarial networks, neural style transfer, and meta-\nlearning [137]. For example, Al-Masni et al. [138] augmented training data by\nrotating all of the 4, 000 dermoscopy images with angles of 0◦, 90◦, 180◦and\n270◦. In this way, overﬁtting was reduced and robustness of deep networks was\nimproved. Yu et al. [132] rotated each image by angles of 0◦, 90◦and 180◦,\nand then performed random pixel translation (with a shift between −10 and 10\npixels) to the rotated images. Signiﬁcant improvement was achieved with data\naugmentation in their experiments on the ISIC skin dataset. Detailed discussion\non data augmentation is beyond the scope of this paper and readers may refer\nto the work by Shorten et al. [137] for more information.\n23\nFigure 6: The workﬂow of a typical skin disease segmentation task.\n7.2. Applications of deep learning in skin disease diagnosis\n7.2.1. Skin lesion segmentation\nSegmentation aims to divide an image into distinct regions that contain pix-\nels with similar attributes. Segmentation is signiﬁcant for skin disease diagnosis\nsince it avails clinicians to perceive the boundaries of lesions. The success of\nimage analysis depends on the reliability of segmentation, whereas a precise\nsegmentation of an image is generally challenging. Manual boarder detection\nconsiders the quandary caused by collision of tumors, wherein there is proximity\nof lesions of more than one types. Therefore, higher caliber knowledge of lesion\nfeatures should be taken into account [139].\nParticularly, the morphological\ndiﬀerences in appearance of skin lesions bring more diﬃculties to skin diseases\nsegmentation. The foremost reason is that a relatively poor contrast between\nthe mundane and skin lesion exists. Other reasons that make the segmentation\ndiﬃcult include variations in skin tones, presence of artifacts such as hair, ink,\nair bubbles, ruler marks, non-uniform lighting, physical location of lesions and\nlesion variations in respect to color, texture, shape, size and location in the\nimage [140, 32]. These factors should be considered when designing a segmenta-\ntion algorithm for skin disease images. Generally, eﬀective image preprocessing\nshould be adopted to eliminate the impact of these factors before images are\ninput to segmentation algorithms [60, 141]. In the past few years, deep learn-\ning has been extensively applied to image segmentations for skin diseases and\nachieved promising performance [142, 143, 144, 21, 145]. The workﬂow of a\ntypical skin disease segmentation task is illustrated in Fig. 6.\nFully convolutional neural network with an encoder-decoder architecture\n(e.g., fully convolutional network (FCN) [146] and SegNet [21]) was one of the\nearliest deep learning models proposed for semantic image segmentation. Par-\nticularly, deep learning models based on FCN have been used for skin lesion\nsegmentation. For instance, Attia et al. [147] proposed a network combining a\nFCN with a long short term memory (LSTM) [148] to perform segmentation\nfor melanoma images. The method did not require any preprocessing to the\ninput images and achieved state-of-the-art performances with an average seg-\nmentation accuracy of 0.98 and Jaccard index of 0.93 on the ISIC dataset. The\nauthors found that the hybrid method utilizing RNN and CNN simultaneously\nwas able to outperform methods that rely on CNN only. Bi et al. [149] proposed\na FCN based method to automatically segment skin lesions from dermoscopy\nimages. Speciﬁcally, multiple embedded FCN stages were proposed to learn im-\nportant visual characteristics of skin lesions and these features were combined\n24\ntogether to segment the skin lesion accurately.\nGoyal et al. [150] proposed\na multi-class segmentation method based on FCN for benign nevi, melanoma\nand seborrhoeic keratoses images. The authors tested the method on the ISIC\ndataset and obtained dice coeﬃcient indices of 55.7%, 65.3%, and 78.5% for\nthe 3 classes respectively.\nPhillips et al. [151] proposed a novel multi-stride\nFCN architecture for segmentation of prognostic tissue structures in cutaneous\nmelanoma using whole slide images. The weights of the proposed multi-stride\nnetwork were initiated with multiple networks pretrained on the PascalVOC\nsegmentation dataset and ﬁne-tuned on the whole slide images. Results showed\nthat the proposed approach had the possibility to achieve a level of accuracy\nrequired to manually perform the Breslow thickness measurement.\nThe well-known neural network, U-net [122], was proposed for medical im-\nage segmentation in 2015. The network was constructed based on FCN, and its\narchitecture has been modiﬁed and extended to many works that yielded better\nsegmentation results [152, 153]. Naturally, there have been several works apply-\ning U-net to the task of skin lesion segmentation. Chang et al. [141] implemented\nU-net to segment dermoscopy images of melanoma. Then both the segmented\nimages and original dermoscopy images were input to a deep network consisting\nof two Inception V3 networks for skin lesion classiﬁcation. Experimental results\nshowed that both the segmentation and classiﬁcation models achieved excellent\nperformances on the ISIC dataset. Lin et al. [154] compared two methods, i.e.,\nU-net and a C-Means based approach, for skin lesion segmentation. When eval-\nuated on the ISIC dataset, U-net and C-Means based approach achieved 77%\nand 61% Dice coeﬃcient indices respectively. The results showed that U-net\nachieved a signiﬁcantly better performance compared to the clustering method.\nBased on the previous two important architectures, a series of deep learn-\ning models were developed for skin lesion segmentation. Yuan [143] proposed\na framework based on deep fully convolutional-deconvolutional neural networks\nto automatically segment skin lesions in dermoscopy images. The method was\ntested on the ISIC dataset and took the ﬁrst place with an average Jaccard\nindex of 0.784 on the validation dataset. Later, Yuan et al. [155] extended their\nprevious work [143] by proposing a deeper network architecture with smaller\nkernels to enhance its discriminant capacity. Moreover, color information from\nmultiple color spaces was included to facilitate network training. When eval-\nuated on the ISIC dataset, the method achieved an average Jaccard index of\n0.765, which took the ﬁrst place in the challenge then. Codella et al. [28] pro-\nposed a fully-convolutional U-Net structure with joint RGB and HSV channel\ninputs for skin lesion segmentation. Experimental results showed that the pro-\nposed method obtained competitive segmentation performance to state-of-the-\nart, and presented agreement with the groundtruth that was within the range\nof human experts. Al-Masni et al. [138] developed a skin lesion segmentation\nmethod via deep full resolution convolutional networks. The method was able\nto directly learn full resolution result of each input image without the need of\npreprocessing or postprocessing operations. The method achieved an average\nJaccard index of 77.11% and overall segmentation accuracy of 94.03% on the\nISIC dataset, and 84.79% and 95.08% on the PH2 dataset, respectively.\nJi\n25\net al. [156] proposed a skin image segmentation method based on salient ob-\nject detection. The proposed method modiﬁed the original U-net by adding\na hybrid convolution module to skip connections between the down-sampling\nand up-sampling stages. Besides, the method employed a deeply supervised\nstructure at each stage of up-sampling to learn from the output features and\nground truth. Finally, the multi-path outputs were integrated to obtain better\nperformance. Canalini [157] proposed a novel strategy to perform skin lesion\nsegmentation. They explored multiple pretrained models to initialize a feature\nextractor without the need of employing biases-inducing datasets. An encoder-\ndecoder segmentation architecture was employed to take advantage of each pre-\ntrained feature extractor. In addition, GANs were used to generate both the\nskin lesion images and corresponding segmentation masks, serving as additional\ntraining data. Tschandl et. al. [158] trained VGG and ResNet networks on\nimages from the HAM10000 dataset [66] and then transferred corresponding\nlayers as encoders into the LinkNet model [159]. The model with transferred\ninformation was further trained for a binary segmentation task on the oﬃcial\nISIC 2017 challenge dataset [62]. Experimental results showed that the model\nwith ﬁne-tuned weights achieved a higher Jaccard index than that obtained by\nthe network with random initializations on the ISIC 2017 dataset.\nConsidering the excellent performance of ResNet [19] and DenseNet [90]\nin image classiﬁcation tasks, people incorporated the idea of residual block or\ndense block into existing image segmentation architectures to design eﬀective\ndeep networks for skin lesion segmentation. For example, Yu et al. [142] claimed\nthat they were the ﬁrst to apply very deep CNNs to automated melanoma recog-\nnition. They ﬁrst constructed a fully convolutional residual network (FCRN)\nwhich incorporated multi-scale feature representations for skin lesion segmen-\ntation.\nThen the trained FCRN was utilized to extract patches with lesion\nregions from skin images and the patches were used to train a very deep resid-\nual network for melanoma classiﬁcation. The proposed framework ranked the\nﬁrst in classiﬁcation competition and the second in segmentation competition\non the ISIC dataset. Li et al. [160] proposed a dense deconvolutional network\nfor skin lesion segmentation based on residual learning. The network consisted\nof dense deconvolutional layers, chained residual pooling, and hierarchical su-\npervision. The method can be trained in an end-to-end manner without the\nneed of prior knowledge or complicated postprocessing procedures and obtained\n0.866%, 0.765%, and 0.939%, of Dice coeﬃcient, Jaccard index, and accuracy,\nrespectively, on the ISIC dataset. Li et al. [161] proposed a dense deconvo-\nlutional network for skin lesion segmentation based on encoding and decoding\nmodules. The proposed network consisted of convolution units, dense deconvo-\nlutional layers (DDL) and chained residual pooling blocks. Speciﬁcally, DDL\nwas adopted to restore the original high resolution input via upsampling, while\nthe chained residual pooling was for fusing multi-level features. In addition,\nhierarchical supervision was enforced to capture low-level detailed boundary\ninformation.\nRecently, GANs [118] have achieved great success in image generation and\nimage style transfer tasks. The idea of adversarial training was adopted by peo-\n26\nple for constructing eﬀective semantical segmentation networks and achieved\npromising results [162]. In particular, there have been a few works utilizing\nGANs for skin disease image segmentation [163, 164, 165, 166]. Udrea et al. [167]\nproposed a deep network based on GANs for segmentation of both pigmented\nand skin colored lesions in images acquired with mobile devices. The network\nwas trained and tested on a large set of images acquired with a smart phone\ncamera and achieved a segmentation accuracy of 91.4%. Peng et al. [145] pre-\nsented a segmentation architecture based on adversarial networks. Speciﬁcally,\nthe architecture employed a segmentation network based on U-net as generator\nand a network consisting of certain number of convolutional layers as discrim-\ninator. The method was tested on the PH2 and ISIC datasets, achieving an\naverage segmentation accuracy of 0.97 and dice coeﬃcient of 0.94. Sarker et\nal. [168] proposed a lightweight and eﬃcient GAN model (called MobileGAN)\nfor skin lesion segmentation. The MobileGAN combined 1-D non-bottleneck\nfactorization networks with position and channel attention modules in a GAN\nmodel. With only 2.35 million parameters, the MobileGAN still obtained com-\nparable performance with an accuracy of 97.61% on the ISIC dataset. Singh et\nal. [169] presented a skin lesion segmentation method based on a modiﬁed con-\nditional GAN (cGAN). They introduced a new block (called factorized channel\nattention, FCA) into the encoder of cGAN, which exploited both channel at-\ntention mechanism and residual 1-D kernel factorized convolution. In addition,\nmulti-scale input strategy was utilized to encourage the development of ﬁlters\nthat were scale-variant.\nBesides designing novel architectures, people also considered developing ef-\nfective deep learning models for skin lesion segmentation from other aspects.\nFor example, Jafari et al. [170] proposed a deep CNN architecture to segment\nthe lesion regions of skin images taken by digital cameras. Local and global\npatches were utilized simultaneously such that the CNN architecture was able\nto capture the global and local information of images. Experimental results\non the Dermquest dataset showed that the proposed method obtained a high\naccuracy of 98.5% and sensitivity of 95.0%. Yuan et al. [143] proposed a new\nloss function for a deep network to adapt it to a skin lesion segmentation task.\nSpeciﬁcally, they designed a novel loss function based on the Jaccard distance\nfor a fully convolutional neural network and performed skin lesion segmentation\non dermoscopy images. CNNs for skin lesion segmentation commonly accept\nlow-resolution images as inputs to reduce computational cost and network pa-\nrameters. This situation may lead to the loss of important information con-\ntained in images. To resolve this issue and develop a resolution independent\nmethod for skin lesion segmentation, ¨Unver et al. [144] proposed a method by\ncombining the YOLO model and GrabCut algorithm for skin lesion segmen-\ntation. Speciﬁcally, the YOLO model was ﬁrst employed to locate the lesions\nand image patches were extracted according to the location results. Then the\nGrabCut algorithm was utilized to perform segmentation on the image patches.\nDue to the small size of the labeled training dataset and large variations of\nskin lesions, the generalization property of segmentation models is limited. To\naddress this issue, Cui et al. [171] proposed an ensemble transductive learning\n27\nTable 4: References of skin lesion segmentation with deep learning (part 1).\nReference Year\nDataset\nNo. of images\nSegmentation method\n[170]\n2016\nDerm101\n126\nA CNN architecture consisting of two\nsubpaths,\nwith one\naccounting for\nglobal information and another for lo-\ncal information.\n[142]\n2016\nISIC\n1,250\nFully convolutional residual network.\n[149]\n2017\nISIC and PH2\n1,279 and 200\nMultistage\nfully\nconvolutional\nnet-\nworks with parallel integration.\n[150]\n2017\nISIC\n2,750\nA transfer learning approach which\nuses both partial transfer learning and\nfull transfer learning to train FCNs for\nmulti-class semantic segmentation.\n[154]\n2017\nISIC\n2,000\nU-Nets with a histogram equalization\nbased preprocessing step.\n[28]\n2017\nISIC\n1,279\nAn ensemble system combining tradi-\ntional machine learning methods with\ndeep learning methods.\n[147]\n2017\nISIC\n1,275\nAn architecture combining an auto-\nencoder network with a four-layer re-\ncurrent network with four decoupled\ndirections.\n[141]\n2017\nISIC\n2,000\nA deep network similar as U-net.\n[143]\n2017\nISIC and PH2\n1,279 and 200\nA fully convolutional neural network\nwith a novel loss function deﬁned\nbased on the Jaccard distance.\n[155]\n2017\nISIC\n2,750\nA convolutional-deconvolutional neu-\nral network.\n[167]\n2017\nA proprietary\ndatabase\n3,000\nA GAN with U-net being the genera-\ntor.\n[138]\n2018\nISIC and PH2\n2,750 and 200\nA full resolution convolutional net-\nwork.\n[156]\n2018\nFrom ISIC\nand other\nsources\n2,600\nModiﬁed U-net with hybrid convolu-\ntion modules and deeply supervised\nstructure.\n[160]\n2018\nISIC\n2,900\nA\ndense\ndeconvolutional\nnetwork\nbased on residual learning.\n[161]\n2018\nISIC\n1,950\nA\ndense\ndeconvolutional\nnetwork\nbased on encoding and decoding mod-\nules.\n[157]\n2019\nISIC\n10,015\nAn encoder-decoder architecture with\nmultiple pretrained models as feature\nextractors.\nIn addition, GANs were\nused to generate additional training\ndata.\nstrategy for skin lesion segmentation. By learning directly from both training\nand testing sets, the proposed method can eﬀectively reduce the subject-level\ndiﬀerence between training and testing sets. Thus, the generalization perfor-\nmance of existing segmentation models can be improved. Soudani et al. [172]\nproposed a segmentation method based on crowdsourcing and transfer learning\nfor skin lesion extraction. Speciﬁcally, they utilized two pretrained networks,\ni.e., VGG-16 and ResNet-50, to extract features from the convolutional parts.\nThen a classiﬁer with an output layer composed of ﬁve nodes was built. In this\nway, the proposed method was able to dynamically predict the most appropriate\nsegmentation technique for the detection of skin lesions in any input image.\nFor convenient reference, we list the aforementioned works on skin lesion\nsegmentation with deep learning methods in Table 4 and Table 5.\n28\nTable 5: References of skin lesion segmentation with deep learning (part 2).\nReference Year\nDataset\nNo. of images\nSegmentation method\n[144]\n2019\nISIC and PH2\n2750 and 200\nDetect skin lesion location with the\nYOLO model and segment images\nwith the GrabCut algorithm.\n[158]\n2019\nHAM10000,\nISIC and PH2\nAround 20,000\nA\nLinkNet\narchitecture\nwith\npre-\ntrained ResNet as encoders.\n[151]\n2019\nTCGA\n50\nA multi-stride fully convolutional net-\nwork.\n[145]\n2019\nISIC and PH2\n1, 279 and 200\nAn architecture based on adversar-\nial networks with a segmentation net-\nwork based on U-net and a discrimi-\nnation network linked by certain con-\nvolutional layers.\n[168]\n2019\nISIC\n3, 344\nMobileGAN\ncombining\n1-D\nnon-\nbottleneck\nfactorization\nnetworks\nwith position and channel attention\nmodules.\n[169]\n2019\nISBI 2016,\nISBI 2017 and\nISIC\n1, 279, 2, 750\nand 3, 694\nA\nmodiﬁed\ncGAN\nwith\nfactorized\nchannel attention as the encoder.\n[171]\n2019\nISIC\n3, 694\nA\ntransductive\napproach\nwhich\nchooses some of the pixels in test\nimages\nto\nparticipate\nthe\ntraining\nof the segmentation model together\nwith the training set.\n[172]\n2019\nISIC\n2, 750\nA segmentation recommender based\non crowdsourcing and transfer learn-\ning.\n7.2.2. Skin disease classiﬁcation\nSkin disease classiﬁcation is the last step in the typical workﬂow of a CAD\nsystem for skin disease diagnosis. Depending on the purpose of the system,\nthe output of a skin disease classiﬁcation algorithm can be binary (e.g., benign\nand malignant), ternary (e.g., melanoma, dysplastic nevus and common nevus)\nor n ≥4 categories.\nTo accomplish the task of classiﬁcation, various deep\nlearning methods have been proposed to classify skin disease images. In the\nfollowing, we present a brief review of the existing deep learning methods for\nskin disease classiﬁcation. The workﬂow for a typical skin disease classiﬁcation\ntask is illustrated in Fig. 7.\nInitially, traditional machine learning methods were employed to extract\nfeatures from skin images and then the features were input to a deep learning\nbased classiﬁer for classiﬁcation. The study by Masood et al. [173] was one of\nthe earliest works that applied modern deep learning methods to skin disease\nclassiﬁcation tasks. The authors ﬁrst detected skin lesions with a histogram\nbased thresholding algorithm, and then extracted features with three machine\nlearning algorithms. Finally, they classiﬁed the features with a semi-supervised\nclassiﬁcation model that combined DBNs and a self-advising support vector\nmachine (SVM) [174]. The proposed model was tested on a collection of 100\ndermoscopy images and achieved better results than other popular algorithms.\nPremaladha et al. [175] proposed a CAD system to classify dermoscopy images\nof melanoma. With enhanced images, the system segmented aﬀected skin lesion\nfrom normal skin. Then ﬁfteen features were extracted from these segmented\n29\nFigure 7: The workﬂow for a typical skin disease classiﬁcation task.\nimages with a few machine learning algorithms and input to a deep neural net-\nwork for classiﬁcation. The proposed method achieved a classiﬁcation accuracy\nof 93% on the testing data.\nWith the development of deep learning, more and more novel networks are\ndesigned such that they can be trained in an end-to-end manner. In particular,\nvarious such kind of advanced deep networks were proposed for skin disease\nclassiﬁcation in the past few years. In 2016, Nasr et al. [176] implemented a\nCNN for melanoma classiﬁcation with non-dermoscopy images taken by digital\ncameras. The algorithm can be applicable in web-based and mobile applica-\ntions as a telemedicine tool and also as a supporting system to assist physi-\ncians. Demyanov et al. [177] trained a ﬁve-layer CNN for classifying two types\nof skin lesion data. The method was tested on the ISIC dataset and the best\nmean classiﬁcation accuracies for the “Typical Network” and “Regular Glob-\nules” datasets were 88% and 83%, respectively.\nIn 2017, Esteva et al. [29]\ntrained a single CNN using only pixels and disease labels as inputs for skin\nlesion classiﬁcation. The dataset in their study consists of 129, 450 clinical im-\nages of 2, 032 diﬀerent diseases. Moreover, they compared the performance of\nthe CNN with 21 board-certiﬁed dermatologists on biopsy-proven clinical images\nwith two critical binary classiﬁcation use cases: keratinocyte carcinomas versus\nbenign seborrheic keratoses; and malignant melanomas versus benign nevi. Re-\nsults showed that the CNN achieved performances on par with all tested experts\nacross both tasks, demonstrating that an artiﬁcial intelligence was capable of\nclassifying skin cancer with a level of competence comparable to dermatologists.\nWalker et. al. [178] reported a work on dermoscopy images classiﬁcation which\nevaluated two diﬀerent inputs derived from a dermoscopy image: visual features\ndetermined via a deep neural network (System A) based on the Inception V2\nnetwork [110]; and soniﬁcation of deep learning node activations followed by\nhuman or machine classiﬁcation (System B). A laboratory study (LABS) and\na prospective observational study (OBS) each conﬁrmed the accuracy level of\nthis decision support system. In both LABS and OBS, System A was highly\n30\nspeciﬁc and System B was highly sensitive. Combination of the two systems\npotentially facilitated clinical diagnosis. Brinker et. al. [179] trained a CNN\nwith dermoscopy images from the HAM10000 dataset exclusively for identify-\ning melanoma in clinical photographs. They compared the performance of the\nautomated digital melanoma classiﬁcation algorithm with that of 145 dermatol-\nogists from 12 German university hospitals. This was the ﬁrst time that a CNN\nwithout being trained on clinical images performed on par with dermatologists\non a clinical image classiﬁcation task.\nGenerally, deep neural networks have a high variance and it can be frustrat-\ning when trying to develop a ﬁnal model for decision making. One solution to\nthis issue is to train multiple models instead of a single one and combine the\npredictions from these models to form the ﬁnal results, which is called ensemble\nlearning [180, 181]. Ensemble learning commonly produces better results than\nany of the single model, and has been applied to skin disease classiﬁcation. Han\net al. [182] created datasets of standardized nail images using a region-based\nCNN (R-CNN). Then the datasets were utilized to ﬁne-tune the pretrained\nResNet-152 and VGG-19 networks. The outputs of the two networks were com-\nbined together and input to a two-hidden-layered feedforward neural network\nfor ﬁnal prediction. Experimental results showed that the diagnostic accuracy\nfor onychomycosis using deep learning was superior to that of most of the der-\nmatologists who participated in this study. Though CNNs achieved expert-level\naccuracy in the diagnosis of pigmented melanocytic lesions, the most common\ntypes of skin cancer are nonpigmented and nonmelanocytic which are diﬃcult to\nbe diagnosed. Tschandl et al. [183] trained a model combining the Inception V3\nand ResNet-50 for skin lesion classiﬁcation with 7, 895 dermoscopy and 5, 829\nclose-up images and tested the model on a set of 2, 072 images. The authors\ncompared the performance of the model with 95 human raters and the results\nshowed that the model can classify dermoscopy and close-up images of non-\npigmented lesions as accurate as human experts in the experimental settings.\nMahbod et al. [78] proposed a hybrid CNN ensemble scheme that combined\nintra-architecture and inter-architecture networks for skin lesion classiﬁcation.\nThrough ﬁne-tuning networks of diﬀerent architectures with diﬀerent settings\nand combining the results from multiple sets of ﬁne-tuned networks, the pro-\nposed method yielded excellent results in the ISIC 2017 skin lesion classiﬁcation\nchallenge without requiring extensive preprocessing, or segmentation of lesion\nareas, or additional training data. Perez et al. [184] evaluated 9 diﬀerent CNN\narchitectures for melanoma classiﬁcation, with 5 sets of splits created on the\nISIC Challenge 2017 dataset, and 3 repeated measures, resulting in 135 models.\nThe author found that ensembles of multiple models can always outperform the\nindividual model.\nDespite deep learning models achieved excellent performance on various ex-\nperimental datasets, one should also consider the fact that most deep learn-\ning models require a whole lot of labeled data for training, and obtaining\nvast amounts of labeled data (especially medical data) can be really diﬃcult\nand expensive in terms of both time and money. Fortunately, transfer learn-\ning [185, 101] can be a strategy to alleviate this issue, enabling deep learning\n31\nmodels to achieve satisfying performance on small datasets. The basic concept\nof transfer learning is to train a model on a large dataset and transfer its knowl-\nedge to a smaller one. Thus, one can utilize a deep network trained on unrelated\ncategories in a massive dataset (usually ImageNet [186]) and apply it to our own\nproblems (e.g., skin disease classiﬁcation).\nAs only limited skin disease data can be obtained publicly, transfer learning\nis widely adopted in skin disease classiﬁcation tasks. Liao [60] used the pre-\ntrained VGG-16, VGG-19 and GoogLeNet networks to construct a universal\nskin disease diagnosis system. The author trained the networks on the Dermnet\ndataset and tested them on the Dermnet and OLE datasets. When tested on\nthe Dermnet dataset, the proposed system achieved a top-1 accuracy of 73.1%\nand top-5 accuracy of 91%, respectively.\nFor the test on the OLE dataset,\nthe top-1 and top-5 accuracies are 31.1% and 69%, respectively.\nIn a more\nrecent work by Liao et al. [31], the authors utilized the pretrained AlexNet\nfor both disease-targeted and lesion-targeted classiﬁcation tasks. They pointed\nout that lesion type tags should also be considered as the target of an auto-\nmated diagnosis system such that the system can achieve a high accuracy in\ndescribing skin lesions. Kawahara et al. [187] extracted multi-scale features of\nskin lesions with a pretrained fully convolutional AlexNet. Then the features\nwere pooled and used to train a logistic regression classiﬁer to classify non-\ndermoscopic skin images. Sun et al. [73] built a benchmark dataset for clinical\nskin diseases and ﬁne-tuned the pretrained VGG-16 model on the dataset for\nskin disease classiﬁcation. Zhang et al. [188] utilized the pretrained Inception\nV3 network to classify dermoscopy images into four classes. The model was\nevaluated on a private dataset collected from the Peking Union Medical Col-\nlege Hospital and experimental results showed that deep learning algorithms\nwere promising for automated skin disease diagnosis. Fujisawa et al. [189] pro-\nposed to apply the pretrained GoogleLetNet to skin tumor classiﬁcation with\na dataset containing 4, 867 clinical images of 21 skin diseases. Compared with\nboard-certiﬁed dermatologists, the algorithm achieved better performances with\nan accuracy of 92.4% ± 2.1%. Lopez et al. [190] utilized the VGG-16 network\nto perform melanoma classiﬁcation. The authors trained the network in three\ndiﬀerent ways: 1) training the network from scratch; 2) using the transfer learn-\ning paradigm to leverage features from a VGG-net pretrained on ImageNet; and\n3) performing the transfer learning paradigm and ﬁne-tuning the network. In\nthe experiments, the proposed approach achieved state-of-the-art classiﬁcation\nresults with a sensitivity of 78.66% and precision of 79.74%. Han et. al. [68]\nemployed the pretrained ResNet-152 model to classify clinical images of 12 skin\ndiseases. The model was further ﬁne-tuned with 19, 398 images from multi-\nple dermoscopy image datasets.\nHaenssle et al. [191] employed a pretrained\nInception V4 network for melanoma classiﬁcation. In the study, the authors\ncompared the performance of the algorithm with that of an international group\nof 58 dermatologists. The results demonstrated that the performance of CNN\noutperformed that of most but not all dermatologists. Zhang et al. [192] utilized\nthe Inception V3 network to classify dermoscopy images of four common skin\ndiseases. To further facilitate the application of the algorithm to CAD support,\n32\nthe authors generated a hierarchical semantic structure based on domain expert\nknowledge to represent classiﬁcation/diagnosis scenarios. The proposed algo-\nrithm achieved an accuracy of 87.25 ± 2.24% on the testing dataset. Joanna et\nal. [193] proposed to perform preoperative melanoma thickness evaluation with\na pretrained VGG-19 network. Experimental results showed that the developed\nalgorithm achieved state-of-the-art melanoma thickness prediction result with\nan overall accuracy of 87.2%. Yu et al. [132] proposed a novel framework for\ndermoscopy image classiﬁcation. Speciﬁcally, the authors ﬁrst extracted image\nrepresentations via a pretrained deep residual network and obtained global im-\nage descriptors with the ﬁsher vector encoding method. After that, the obtained\ndescriptors were utilized to classify melanoma images with SVM. Menegola et\nal. [194] systematically investigated the applications of knowledge transfer of\ndeep learning in dermoscopy image recognition. Their results suggested that\ntransfer learning from a related task can lead to better results on target tasks.\nHekler et al. [195] claimed that they were the ﬁrst to implement a deep learning\nmethod for histopathologic melanoma diagnosis and compare the performance\nof the algorithm with that of an experienced histopathologist. In the study, they\nutilized a pretrained ResNet-50 network [19] to classify histopathologic slides of\nskin lesions into classes of nevi and melanoma. They demonstrated that the dis-\ncordance between the CNN and expert pathologist was comparable with that\nbetween diﬀerent pathologists as reported in the literature. Polevaya et al. [196]\nutilized the pretrained VGG-16 network to classify primary morphology images\nof macule, nodule, papule and plaque. Experimental results showed that the\nmethod was able to achieve an accuracy of 77.50% for 4 classes and 81.67% for\n3 classes on the testing dataset.\nAttention mechanism aims to learn a context vector to weight the input such\nthat salient features can be highlighted and unrelated ones can be suppressed. It\nwas ﬁrst extensively used in the ﬁeld of natural language processing (NLP) [197,\n198], and has been applied to skin disease classiﬁcation recently.\nBarata et\nal. [199] proposed a hierarchical attention model combining CNNs with LSTM\nand attention modules for skin disease classiﬁcation. The model made use of\nthe hierarchical organization of skin lesions, as identiﬁed by dermatologists, so\nas to incorporate medical knowledge into the decision process. Particularly, the\nattention modules were able to identify relevant regions in the skin lesions and\nguide the classiﬁcation decision. The proposed approach achieved state-of-the-\nart results on the two dermoscopy datasets of ISIC 2017 and ISIC 2018.\nAs discussed above, GANs [118] with the capability of generating synthetic\nreal-world like samples developed rapidly during the past few years. In partic-\nular, GANs or the ideas of adversarial training have been utilized to construct\neﬀective algorithms for skin diseases classiﬁcation [200]. The applicability of\ndeep learning methods to melanoma detection is compromised by the limita-\ntion of available skin lesion datasets that are small, heavily imbalanced, and\ncontain images with occlusions. To alleviate this issue, Bisla et al. [201] pro-\nposed to purify data with deep learning based methods and augment data with\nGANs, for populating scarce lesion classes, or equivalently creating virtual pa-\ntients with predeﬁned types of lesions. These preprocesses can be used in a\n33\ndeep neural network for lesion classiﬁcation. Experimental results showed that\nthe proposed preprocesses can boost the performance of a deep neural network\nin melanoma detection. Yi et al. [75] utilized the categorical GAN assisted by\nWasserstein distance for dermoscopy image classiﬁcation in an unsupervised and\nsemi-supervised way. Experimental results on the ISIC dataset showed that the\nproposed method achieved an average precision score of 0.424 with only 140\nlabeled images. In addition, the method was able to generate real-world like\ndermoscopy images. Gu et al. [202] proposed two methods for cross-domain\nskin disease classiﬁcation.\nThey ﬁrst explored a two-step progressive trans-\nfer learning technique by ﬁne-tuning pretrained networks on two skin disease\ndatasets. Then they utilized adversarial learning as a domain adaptation tech-\nnique to perform invariant attribute translation from source domain to target\ndomain. Evaluation results on two skin disease datasets showed that the pro-\nposed method was eﬀective in solving the domain shift problem.\nBesides the above research directions for skin disease classiﬁcation, people\nalso worked on the problem of skin disease classiﬁcation from other aspects.\nMishra et al. [203] investigated the eﬀectiveness of current deep learning methods\nfor skin disease classiﬁcation. The authors analyzed the classiﬁcation processes\nof several deep neural networks (including Resnet-34, ResNet-50, ResNet-101\nand ResNet-152) for common East Asian dermatological conditions. The au-\nthors chose ten common categories of skin diseases based on their prevalence\nfor evaluation. With an accuracy of more than 85% in the experiments, the\nauthors tried to investigate why existing models were unable to achieve com-\nparable results with those in object identiﬁcation tasks. The study suggested\nthat the deep learning based dermoscopy identiﬁcation and dataset creation\ncan be improved. By integrating segmentation results with skin disease clas-\nsiﬁcation process, better classiﬁcation results tend to be obtained. Wan [204]\nimplemented several deep networks (including U-net, Deeplab, Inception V3,\nMobileNet [205] and NASNet [29]) for skin lesion segmentation and classiﬁca-\ntion on the ISIC 2017 challenge dataset. Particularly, the author cropped skin\nimages with the trained segmentation model and trained a classiﬁcation model\nbased on the cropped data. In this way, the classiﬁcation accuracy was further\nimproved. Shi et al. [206] presented a novel active learning framework for cost-\neﬀective skin lesion analysis. They proposed a dual-criteria to select samples\nand an intraclass sample aggregation scheme to enhance the model. Using only\nup to 50% of samples, the proposed approach achieved state-of-the-art perfor-\nmance on both tasks on the ISIC dataset. Tschandl et al. [207] trained a neural\nnetwork to classify dermatoscopy images from three retrospectively collected\nimage datasets. The authors obtained diagnosis predictions through two ways,\ni.e., based on the most commonly occurring diagnosis in visually similar images\n(obtained via content-based image retrieval), or based on the top-1 class pre-\ndiction of the network. Experimental results showed that presenting visually\nsimilar images based on features from a network showed comparable accuracy\nwith the softmax probability-based diagnoses of deep networks.\nFor convenient comparison, we list the references of skin disease image clas-\nsiﬁcation with deep learning in Table 6 and 7.\n34\nTable 6: References of skin disease classiﬁcation with deep learning (part 1).\nReference Year\nDataset\nNo. of\ndata\nClassiﬁcation method\n[173]\n2015\nSelf-collected dataset\n290\nDetect skin lesions with a thresholding\nalgorithm, extract features with three\nmachine learning algorithms, and per-\nform classiﬁcation with a model com-\nbining DBNs and self-advised SVM.\n[177]\n2016\nISIC\n29,323\nA CNN with three convolutional lay-\ners and pooling layers, and two fully-\nconnected layers.\n[60]\n2016\nDermnet and OLE\n>24,300\nPretrained\nVGG-16,\nVGG-19\nand\nGoogLeNet.\n[31]\n2016\nSelf-collected dataset\n75,665\nPretrained AlexNet.\n[187]\n2016\nDermoﬁt Image Library\n1,300\nPretrained fully convolutional AlexNet.\n[176]\n2016\nMED-NODE\n170\nA CNN with two convolutional layers\nand two fully-connected layers.\n[73]\n2016\nSD-198\n6,584\nPretrained VGG-16.\n[175]\n2016\nSelf-collected dataset\n992\nSegment\nskin\nlesions\nwith\nOtsu’s\nmethod\nand\nextract\nﬁfteen\nfeatures\nwith several algorithms, then classify\nimages with deep networks and a hy-\nbrid adaboost-SVM.\n[188]\n2017\nCollected from the Peking\nUnion Medical College\nHospital\n>28,000\nPretrained GoogleNet Inception V3.\n[29]\n2017\nImages from 18 online\nrepositories and clinical\ndata from the Stanford\nUniversity Medical Center.\n129,450\nPretrained GoogleNet Inception V3.\n[189]\n2017\nImages collected from the\nUniversity of Tsukuba\nHospital\n4,867\nPretrained GoogLeNet.\n[190]\n2017\nISIC\n1,279\nVGG-16 trained from scatch and pre-\ntrained VGG-16.\n[194]\n2017\nIAD and ISIC\n≥1000\nand\n1,279\nTransfer learning with VGG-M model.\n[68]\n2018\nAsan dataset, MED-NODE\ndataset, atlas site images,\nHallym and Edinburgh\ndatasets\n19,878\nPretrained ResNet-152.\n[182]\n2018\nSelf-collected dataset\n54,666\nThe outputs of the pretrained ResNet-\n152 and VGG-19 are combined together\nand input to two fully-connected layers\nfor classiﬁcation.\n35\nTable 7: References of skin disease classiﬁcation with deep learning (part 2).\nReference Year\nDataset\nNo. of\ndata\nClassiﬁcation method\n[191]\n2018\nTest-set-300 and ISIC\n300 and\n100\nPretrained GoogLeNet Inception V4.\n[192]\n2018\nCollected from the Peking\nUnion Medical College\nHospital\n>2,800\nPretrained GoogleNet Inception V3.\n[132]\n2018\nISIC\n1,279\nExtract image features via a pretrained\nCNN, obtain global descriptors based\non ﬁsher vector encoding method and\nperform classiﬁcation with SVM.\n[204]\n2018\nISIC\n>\n20,000\nGoogLeNet Inception V3,\nMobileNet\nand NASNet.\n[75]\n2018\nISIC and PH2\n1,279\nand 200\nCategorical GAN assisted by Wasser-\nstein distance.\n[184]\n2019\nISIC\n2,750\n9 diﬀerent pretrained networks.\n[199]\n2019\nISIC\n2,750\nA model combining CNNs with LSTM\nand attention modules.\n[193]\n2019\nIAD\n244\nPretrained VGG-19.\n[183]\n2019\nSelf-collected dataset\n15,796\nA model combining GoogLeNet Incep-\ntion V3 and ResNet50.\n[179]\n2019\nISIC and HAM10000\n20,735\nPretrained ResNet50.\n[178]\n2019\nISIC and IAD\n2,361\nand\n2,800\nGoogLeNet Inception V2.\n[78]\n2019\nISIC\n2,787\nCombine multiple networks (AlexNet,\nVGG-nets and ResNet), and ﬁne-tune\nthe networks multiple times and ensem-\nble the multiple results.\n[207]\n2019\nEDRA, ISIC and PRIV\n888,\n2,750\nand\n16,691\nPretrained ResNet-50.\n[203]\n2019\nSelf-collected\n7,264\nPretrained\nResnet-34,\nResNet-50,\nResNet-101, and ResNet-152.\n[195]\n2019\nCollected from the institute\nof Dr. Krahl\n695\nPretrained ResNet-50.\n[201]\n2019\nISIC, PH2 and Dermoﬁt\nImage Library\n3,982\nSegment lesions with U-net, generate\ndata with DCGANs [119] and classify\nlesions with pretrained ResNet-50.\n[196]\n2019\nSelf-collected\n-\nPretrained VGG-16.\n[202]\n2019\nMoleMap and HAM1000\n102,451\nand\n10,015\nProgressive transfer learning of deep\nCNN models and GAN based method.\n[206]\n2019\nISIC\n3,582\nA novel active learning method.\n36\nFigure 8: The workﬂow for a typical multi-task learning.\n7.2.3. Multi-task learning for skin disease diagnosis\nIn machine learning, people generally train a single model or an ensemble\nof models to complete their desired tasks. While they can achieve acceptable\nresults in this way, information that might contribute to better performance is\nignored. Speciﬁcally, this information comes from the training data of related\ntasks.\nBy sharing representations among related tasks, existing models are\nable to generalize better in the original task. This approach is called multi-\ntask learning (MTL) [208]. MTL enables multiple learning tasks to be solved\nsimultaneously, while exploring the commonalities and diﬀerences across tasks.\nThis can result in the improvement of learning eﬃciency and prediction accuracy\nof the task-speciﬁc models, when compared to training models separately [209].\nThe workﬂow for a typical MTL is illustrated in Fig. 8.\nMany works have adopted MTL for skin disease diagnosis. Yang et. al. [210]\nproposed a multi-task CNN based model for skin lesion analysis. In the model,\neach input dermoscopy image is associated with multiple labels that describe dif-\nferent characteristics of the skin lesion. Then multi-task methods are utilized to\nperform skin lesion segmentation and classiﬁcations simultaneously. Experimen-\ntal results showed that the multi-task method achieved promising performance\nin both tasks. Diﬀerent with existing deep learning approaches that commonly\nuse two networks to separately perform lesion segmentation and classiﬁcation,\nLi et.\nal. [65] proposed a deep learning framework consisting of multi-scale\nfully convolutional residual networks and a lesion index calculation unit to si-\nmultaneously perform the two tasks.\nTo investigate the correlation between\nskin lesions and their body site distributions, the authors in work [211] trained\na deep multi-task learning framework to jointly optimize skin lesion classiﬁca-\ntion and body location classiﬁcation.\nThe experimental results veriﬁed that\nfeatures jointly learned with body location information indeed boosted the per-\nformance of skin lesion classiﬁcation. Kawahara et al. [72] proposed a multi-task\ndeep neural network, trained on a multi-modal dataset (including clinical and\ndermoscopy images, and patient meta-data), to classify the 7-point melanoma\nchecklist criteria and perform skin lesion diagnosis. The network trained with\nseveral multi-task loss functions was able to handle the combination of input\n37\nmodalities. The model classiﬁed the 7-point checklist and performed skin con-\ndition diagnosis, and produced multi-modal feature vectors suitable for image\nretrieval and localization of clinically discriminative regions.\n7.2.4. Miscellany\nApart from the above applications of deep learning in skin disease diagnosis,\nthere are several works applying deep learning to skin disease diagnosis from\nother aspects.\nGANs have been utilized to synthesize skin images so as to facilitate skin\ndisease diagnosis [212, 213, 214]. To address the problems caused by lack of suf-\nﬁcient labeled data in skin disease diagnosis tasks, Bissoto et al. [215] proposed\nto use GAN to generate realistic synthetic skin lesion images. Experimental\nresults showed that they could generate high-resolution (up to 1024×512) sam-\nples containing ﬁne-grained details. Moreover, they employed a classiﬁcation\nnetwork to evaluate the generated images and results showed that the syn-\nthetic images comprised clinically meaningful information. With the help of\nprogressive growing and GANs, Baur et al. [216] generated extremely realistic\nhigh-resolution dermoscopy images. Experimental results showed that even ex-\npert dermatologists found it hard to distinguish the synthetic images from real\nones. Therefore, this method can be served as a new direction to deal with the\nproblem of data scarcity and class imbalance. Yang et al. [217] proposed a novel\ngenerative model based on a dual discrimination training algorithm for autoen-\ncoders to synthesize dermoscopy images. In contrast to other related methods,\nan adversarial loss was added to the pixel-wise loss during the image construc-\ntion phase. Through experiments, they demonstrated that the method can be\napplied to various tasks including data augmentation and image denoising. Baur\net al. [218] utilized GANs to generate realistically looking high-resolution skin\nlesion images with only a small training dataset (2, 000 samples). They both\nquantitatively and qualitatively compared state-of-the-art GAN architectures\nsuch as DCGAN and LAPGAN against a modiﬁcation of the latter one for the\nimage generation task at a resolution of 256×256. Experimental results showed\nthat all the models can approximate the real data distribution. However, major\ndiﬀerences when visually rating sample realism, diversity and artifacts can be\nobserved.\nBesides the above GAN-based applciations, Han et al. [219] proposed a\nmethod based on R-CNN for detecting keratinocytic skin cancer on the face.\nThey ﬁrst used R-CNN to create 924, 538 possible lesions by extracting nodular\nbenign lesions from 182, 348 clinical photographs. After labeling these possible\nlesions, CNNs were trained with 1, 106, 886 image crops to locate and diagnose\ncancer. Experimental results showed that the proposed algorithm achieved a\nhigher F1 score of 0.831 and Youden index score of 0.675 than those of nonder-\nmatologic physicians. Additionally, the accuracy of the algorithm was compa-\nrable with that of dermatologists. Galdran et al. [220] utilized computational\ncolor constancy techniques to construct an artiﬁcial data augmentation method\nsuitable for dermoscopy images. Speciﬁcally, they applied the shades of gray\ncolor constancy technique to color-normalize images of the entire training set,\n38\nwhile retaining the estimated illuminants. Then they drew one sample from the\ndistribution of training set and applied it to the normalized image. They per-\nformed experiments on the ISIC dataset by employing this technique to train\ntwo CNNs for skin lesion segmentation and classiﬁcation.\nAttia et al. [221]\nproposed a deep learning method based on a hybrid network consisting of con-\nvolutional and recurrent layers for hair segmentation with weakly labeled data.\nDeep encoded features were utilized for detection and delineation of hair in skin\nimages. The encoded features were then fed into the recurrent layers to encode\nthe spatial dependencies between disjointed patches. Experiments conducted\non the ISIC dataset showed that the proposed method obtained excellent seg-\nmentation results with a Jaccard Index of 77.8% and tumour disturb pattern of\n14%.\n8. Discussion\nSkin disease diagnosis with deep learning methods has attracted much atten-\ntion and achieved promising progress in recent years [29, 60, 190]. In the pub-\nlished literature, the performances achieved by deep learning methods for skin\ndisease diagnosis are similar as those achieved by dermatologists. To develop\nand validate excellent algorithms or systems supporting new imaging techniques,\nlots of research and innovative system development are required [32]. The ma-\njor drawback of dermoscopy examination by dermatologists is that the process\nis subjective and results may vary with experience. Thus, biopsy is needed to\ndiﬀerentiate benign cases from malignant ones. Biopsying benign lesions of skin\ndiseases may lead to increased anxieties to patients and aggravate the expense\nto healthcare systems. Factors, such as training, time, and experience needed\nto properly utilize various available and upcoming techniques, present a huge\nbarrier to early and accurate diagnosis of skin diseases. Although many auto-\nmated skin disease diagnosis methods have been developed, a complete decision\nsupport system has not been developed.\nIn this section, we discuss the major challenges faced in the ﬁeld of skin\ndisease diagnosis with deep learning. Instead of describing speciﬁc cases en-\ncountered, we focus more on the fundamental challenges and explain the root\ncauses of these issues. Then, we try to provide suggestions to deal with these\nproblems.\n8.1. Challenges\nWith the development of deep learning in the past few years, a variety of\nworks on skin disease diagnosis with deep learning methods have been proposed\nand achieved promising performance. However, there are still several issues that\nshould be resolved before deep learning can be extensively applied to real-life\nclinical scenarios of skin disease diagnosis.\n39\n8.1.1. Limited labeled skin disease data\nPrevious works on skin disease diagnosis with deep learning were commonly\ntrained and tested on datasets with limited number of images.\nThe biggest\npublicly available skin disease dataset that can be found in literate until now\nis the ISIC dataset [62] containing more than 20, 000 skin images. Though one\nmay obtain large numbers of skin disease data without any diagnosis information\nfrom websites or medical institutes, labeling vast amounts of skin disease data\nrequires expertise knowledge and can be really diﬃcult and expensive in terms of\nboth time and money. As is known that training a deep neural network requires\na large amount of labeled data. Overﬁtting tends to occur when only small\ndataset is available. Therefore, larger datasets with labeled information are in\ndemand to train an eﬀective deep neural network for skin disease diagnosis.\nHowever, considering the practical challenges in developing a large dataset, it is\nalso imperative to simultaneously develop approaches that exploit deep learning\nwith less labeled data for skin disease diagnosis.\n8.1.2. Imbalanced skin disease datasets\nOne common problem occurred in skin disease diagnosis tasks is the im-\nbalance of samples in skin disease datasets. Actually, many datasets contain\nsigniﬁcant disproportions in the number of data points among diﬀerent skin\nclasses and are heavily dominated by data of the benign lesions. For exam-\nple, one skin disease dataset may contain a large number of negative samples\nbut only limited positive samples. Training deep learning models with imbal-\nanced data may result in biased results, despite employing training tricks such\nas penalization of false negative cases found in a minor skin lesion class with\nweighted loss function. In light of the low frequency of occurrences of certain\npositive samples in skin diseases, obtaining a balanced dataset from the available\noriginal data can be as hard as developing a large-scale dataset.\n8.1.3. Noisy data obtained from heterogeneous sources\nDermoscopy images of most existing skin disease datasets are obtained with\nhigh-resolution DSLR cameras in an optimal environment of lighting and dis-\ntance of capture. Deep learning algorithms trained on these high-quality skin\ndisease datasets are capable of achieving excellent diagnostic performance. How-\never, when tested with images captured with low-resolution cameras (e.g., cam-\neras of smart phones) in diﬀerent lighting conditions and distances, the same\nmodel may be hard to achieve the same performance. Actually, deep learn-\ning algorithms are found to be highly sensitive to images captured by diﬀerent\nequipments. In addition, self-captured images are often of inferior-quality with\nmuch noise. Therefore, noisy data obtained from heterogeneous sources bring\nchallenges to skin disease diagnosis with deep learning.\n8.1.4. Lack of diversity among cases in existing skin disease datasets\nMost cases in existing skin disease datasets are fair-skinned individuals\nrather than dark-skinned ones. Though the incident rate of skin cancer is rela-\n40\ntively higher among fair-skinned population than that of the dark-skinned pop-\nulation, people with dark skin can also suﬀer from skin cancer and are usually\ndiagnosed in later stage [222]. Deep learning algorithms trained with skin dis-\nease data of fair-skinned population may fail to diagnose for the people with\ndark skin [223]. Another problem with existing skin disease datasets is that\nonly categorizes of high incident rate (e.g., BCC, SCC and melanoma) are in-\ncluded and other (e.g., Merkel cell carcinoma (MCC), appendageal carcinomas,\ncutaneous lymphoma, sarcoma, kaposi sarcoma, and cutaneous secondaries) are\nignored. Consequently, if deep learning algorithms are trained on datasets that\ndo not contain data captured from dark-skinned population and have not ade-\nquate cases of rare skin diseases, misdiagnosis on data with these skin conditions\nmay occur with a high probability. Therefore, developing skin disease datasets\nwith high diversity is signiﬁcant for constructing eﬀective skin disease diagnosis\nsystems.\n8.1.5. Missing of medical history and clinical meta-data of patients\nBesides performing visual inspection for a suspected skin lesion with the help\nof medical equipment (e.g., dermoscopy), clinicians also take the medical history,\nsocial habits and clinical meta-data of patients into account when making a\ndiagnostic decision. Actually, it is of great importance to know the diagnostic\nmeta-data, such as skin cancer history, age, sex, ethnicity, general anatomic site,\nsize and structure of skin lesions of patients (sometimes related information of\ntheir families are also needed). It has been proved in the work [191] that the\nperformance of the beginner or skilled dermatologists can be improved with\nadditional clinical information. However, most existing works on skin disease\ndiagnosis with deep learning merely considered skin images and ignored medical\nhistory and clinical information of patients. One possible factor leading to this\nsituation is the missing of such information in the most publicly available skin\ndisease datasets.\n8.1.6. Explainability of deep learning methods\nThere has been much controversy about the topic of “black box” of deep\nlearning models. That is, people may not be possible to understand how the\ndetermination of output is made by deep neural networks. This opaqueness has\nled to demands for explainability before a deep learning algorithm can be applied\nto clinical diagnosis. Clinicians, scientists, patients, and regulators would all\nprefer having a simple explanation for how a neural network makes a decision\nabout a particular case. In the example of predicting whether a patient has a\ndisease, people would like to know what hidden factors the network is using.\nHowever, when a deep neural network is trained to make predictions on a large\ndataset, it typically uses its layers of learned, nonlinear features to model a\nhuge number of complicated but weak regularities in the data. It is generally\ninfeasible to interpret these features since their meaning depends on complex\ninteractions with uninterpreted features in other layers. If the same network\nis reﬁt to the same data but with changes in the initializations, there may be\ndiﬀerent features in the intermediate layers. This indicates that unlike models in\n41\nwhich an expert speciﬁes the hidden factors, a neural network has many diﬀerent\nand equally good ways to model the same dataset. It is not trying to identify the\n“correct” hidden factors, but merely use hidden factors to model the complicated\nrelationship between the input variables and output variables. In the future,\nmore eﬀorts should be made to deal with the “black box” phenomenon.\n8.1.7. Selection of deep neural networks for a speciﬁc skin disease diagnosis task\nAs the literature presented in the previous sections showed that most exist-\ning skin disease diagnosis tasks typically employed the currently popular deep\narchitectures for image segmentation or classiﬁcation. Additionally, ensemble\nmethods of combining two or more deep networks were also utilized to analyze\nskin images. However, few works have made it clear how to select an appro-\npriate type of deep neural network for a speciﬁc skin disease diagnosis task.\nTherefore, it is necessary to investigate the characteristics of skin diseases and\ncorresponding data, and then design deep networks with domain knowledge for\nthe speciﬁc task. In this way, better performance can be achieved.\n8.2. What can we do next?\nWith the increasing trend of applying deep learning methods to skin disease\ndiagnosis recently, people are likely to witness a large number of works in this\nﬁeld in the near future. However, as discussed above, several challenges exist\nand need to be resolved in this ﬁeld. To cope with the challenges and obtain\nsatisfying performance for skin disease diagnosis, there are a few possible direc-\ntions that we can explore. We draw insights from the literature in the ﬁeld of\nskin disease diagnosis and other ﬁelds (e.g., computer vision and pattern recog-\nnition), and present possible guidelines and directions for future works in the\nfollowing.\n8.2.1. Obtain massive labeled skin disease data\nTo obtain excellent performance for skin disease diagnosis, deep neural net-\nworks commonly require large amounts of data for training. However, limited\nlabeled skin disease data is common in practice. To deal with this problem, we\ncan seek solutions from several aspects. On one hand, people may employ expe-\nrienced clinicians to label skin disease data manually, though it would be expen-\nsive and time-consuming. One the other hand, automated or semi-automated\ndata labeling tools, such as Fiji [224], LabelMe [225] and Imagetagger [226],\ncan be utilized to label massive data eﬃciently.\nMoreover, existing publicly\navailable skin datasets can be comprehensively integrated to form a large-scale\nskin image dataset, as ImageNet in the computer vision ﬁeld, for testing deep\nlearning algorithms. In addition, to cope with the issue caused by noisy data\nwith heterogenous sources, color constancy algorithms, such as Shades of Gray,\nmax-RGB, can be utilized to boost the performance of deep learning mod-\nels [227, 228]. These algorithms can be used as image preprocessing methods to\nnormalize the lighting eﬀect of dermoscopy images.\n42\n8.2.2. Increase the diversity of clinical skin data\nFrom the previous section we can observe that only limited skin disorders\nwere involved in most works on skin disease diagnosis with deep learning meth-\nods [60, 179, 68, 29].\nAs a result, the trained algorithms can only decide\nwhether a lesion is more likely a predeﬁned type of skin disease, such as ne-\nvus or melanoma, without even determining any subtype of it. By contrast, an\nexperienced pathologist can diagnose any given images of a broad spectrum of\ndiﬀerential diagnoses and decide a skin lesion belonging to any possible subtype\nof a skin disease. A more powerful and reliable skin disease diagnosis system\nthat can be adapted to analyze all kinds of skin lesions is in huge demand. Con-\nsequently, it is necessary to expand the existing skin image datasets to include\nother cutaneous tumors and normal skin types. Moreover, it is also imperative\nto include skin data captured from the dark-skinned population to improve the\ndiversity of current skin datasets. In this way, deep learning models trained\non these general and complex datasets can adapt to more general skin disease\ndiagnosis tasks.\n8.2.3. Include additional clinical information to assist skin disease diagnosis\nIn most cases, only dermoscopy or histopathological images are input to\ndeep learning models for skin disease diagnosis. However, in the clinical set-\ntings, accurate diagnosis also relies on the history of skin lesions, risk proﬁle of\nindividuals, and global assessment of the skin. Thus, dermatologists commonly\nincorporate additional clinical information to identify skin cancers. The authors\nin [191] investigated the eﬀect of including additional information and close-up\nimages for skin disease diagnosis and found a great improvement in the per-\nformance. Therefore, additional clinical information can be incorporated into\nthe model training and testing processes for skin disease diagnosis. Other ex-\nisting medical record data, such as un-organized documents, can be processed\nwith techniques including NLP, document analysis [229] and data mining [230]\nand taken into account in the diagnosis process as well. Skin images and related\nmedical documents can be combined together to construct multi-view paradigms\nfor the diagnosis tasks. Multi-view models have proved their eﬀectiveness in re-\ncent works and can be extended to the ﬁeld of skin disease diagnosis. Besides,\nintegrating human knowledge into existing deep learning algorithms is likely to\nfurther improve the diagnosis performance as well.\n8.2.4. Fuse handcrafted features with deep networks extracted features\nHandcrafted features are typically extracted with less powerful traditional\nmachine learning models and can be obtained with relatively small labeled data\nand less computational cost. However, they sometimes can achieve excellent\nperformance in certain skin disease diagnosis tasks. Though handcraft features\ncommonly lack generalization properties and showed inferior performance com-\npared with the features directly learned from massive data with deep neural\nnetworks, they can be served as a supplementary to deep features.\nFor ex-\nample, decorrelated color spaces can be investigated to analyze the impact of\n43\ncolor spaces in border detection and use them to facilitate skin image process-\ning [231]. Skin lesion elevation and evolution features and geometrical features\nprovide important clues for diagnosing a skin disease. Combining these features\nwith deep features can further enhance the performance of current deep learn-\ning methods. Particularly, it would be promising if one could ﬁnd a way to\nintegrate the handcrafted feature extracting process with the learning process\nof deep networks. Through fusing handcrafted features with deep features, we\nmay not only reduce the requirement of large amounts of labeled data to train a\ndeep network, but also achieve better performance. Additionally, we need to in-\nvestigate the characteristics of skin diseases ﬁrst, and then design deep networks\nwith domain knowledge for the speciﬁc task. In this way, better performance\ncan be expected.\n8.2.5. Employ GANs to synthesize additional data for training deep networks\nGANs [118] are attracting lots of attention from the computer vision commu-\nnity due to their ability to generate realistic synthetic images for various tasks.\nThen these images can be utilized as additional labeled data to train deep learn-\ning models. In this way, models commonly show superior performance compared\nwith the situation where models are trained with limited original data. This\nproperty of GANs can be of great help for skin disease diagnosis when large-scale\nlabeled datasets are unavailable. Actually, there have been a few works in the\nliterature applying GANs to skin disease diagnosis [167, 232, 75, 233]. However,\nit should be very careful when exploiting GANs for medical applications. As\nwe know, GANs are trying to mimic the realistic images instead of learning the\noriginal distribution of images. Thus, images generated with GANs can greatly\ndiﬀer from the original ones. In light of this, it is feasible to train a deep learning\nmodel with images generated by GANs at the beginning and then ﬁne-tune the\nﬁnal model with only the original images.\n8.2.6. Exploit transfer learning and domain adaptation for skin disease diagno-\nsis\nTransfer learning [101, 234] and domain adaptation [235, 236, 237] have\nbeen exploited to deal with the issues caused by lack of large-scale labeled data.\nAs presented above, there have been many works utilizing transfer learning\nor domain adaptation techniques to improve the performance of deep learning\nmodels in skin disease diagnosis tasks [202, 196, 193, 172]. One way to implement\ntransfer learning is to utilize existing pretrained deep learning models to extract\nsemantic features and perform further learning based on these features [238,\n239, 240].\nFor instance, Akhtar et al. [238] utilized deep models to extract\nfeatures and these features were further used to learn higher level features with\ndictionary learning. Another way to implement transfer learning is freezing part\nof a deep network and training the remainder. It is known that the initial layers\nof a deep network learn similar ﬁlters from diverse images. Therefore, one can\ndirectly borrow the values of parameters corresponding to initial layers from a\nnetwork trained in similar tasks and freeze these layers. Then the remainder\nof the network is trained as normal with limited labeled data. In addition, we\n44\ncan take advantage of recent development [241] of transfer learning in the other\nﬁelds (e.g., computer vision) to facilitate the success of deep learning in skin\ndisease diagnosis tasks.\n8.2.7. Develop semi-supervised deep learning methods for skin disease diagnosis\nIt is known that large amounts of labeled data is required to train a deep\nlearning model. However, collecting massive labeled skin data is expensive since\nexpert knowledge is required and the labeling process is time-consuming. By\ncontrast, it is much easier or cheaper to obtain large-scale unlabeled skin data.\nSemi-supervised learning [242] aims to greatly alleviate the issues caused by lack\nof large-scale labeled data by allowing a model to leverage the available massive\nunlabeled data. Particularly, there have been a few works [173, 75, 243] involving\nsemi-supervised learning for skin disease diagnosis. Recently, semi-supervised\ndeep learning attracts increasing attention in the ﬁeld of computer vision and\na few successful models have been proposed [244, 245, 246].\nUnderstanding\nthese models and developing semi-supervised deep learning models speciﬁcally\nfor skin disease diagnosis can be a promising direction.\n8.2.8. Explore the possibility of applying reinforce learning for skin disease di-\nagnosis\nReinforce learning (RL) [247, 248] has achieved tremendous success in recent\nyears, reaching human-level performance in several areas such as Atari video\ngames [247], the ancient games of Go [249] and chess [250]. The success in part\nhas been made possible by the powerful function approximation abilities of deep\nlearning algorithms. Many medical decision problems are by nature sequential;\ntherefore, RL can be employed to solve these problems. Particularly, there have\nbeen several works utilizing RL to solve medical image processing tasks and\nachieved promising results [251, 252, 253]. To the best of our knowledge, there\nhave not works applying RL to skin disease diagnosis tasks so far. Therefore,\nRL can be a potential tool to solve skin disease diagnosis problems.\n8.2.9. Reasonable explanation for predictions produced by deep learning algo-\nrithms\nExplainability is one of the key factors that hinders the application of deep\nlearning methods to clinical diagnosis scenarios. To assist diagnosis, people need\nreasonable explanation for the predictions produced by deep learning algorithms\nrather than just a conﬁdence score of the skin diseases. One possible solution to\nthis problem is to provide a reasonable explanation for the predictions according\nto the ABCDE criteria (asymmetry, border, color, diameter, and evolution) or\n7-point skin lesion malignancy checklist (pigment network, regression structures,\npigmentation, vascular structures, streaks, dots and globules, and blue whitish\nveil) [58].\n45\n9. Summary\nIn this review, we present an overview on the advances in the ﬁeld of skin\ndisease diagnosis with deep learning. First, we brieﬂy introduce the domain and\ntechnical aspects of skin disease. Second, skin image acquisition methods and\npublicly available datasets are presented. Third, the conception and popular\narchitectures of deep learning and commonly used deep learning frameworks\nare introduced. Then, we introduce the performance evaluation metrics and\nreview the applications of deep learning in skin disease diagnosis according to\nthe speciﬁc tasks. Thereafter, we discuss the challenges remained in the area of\nskin disease diagnosis with deep learning and suggest possible future research\ndirections. Finally, we summarize the whole article.\nCompared with existing related literature reviews, this article provides a\nsystematic survey of the ﬁeld of skin disease diagnosis focusing on recent ap-\nplications of deep learning.\nWith this article, one could obtain an intuitive\nunderstanding of the essential concepts in the ﬁeld of skin disease diagnosis\nwith deep learning and challenges faced in this ﬁeld as well. Moreover, several\npossible directions to deal with these challenges can be taken into consideration\nby ones who are willing to work further in this ﬁeld in the future.\nThe potential beneﬁts of automated diagnosis of skin diseases with deep\nlearning are tremendous. However, accurate diagnosis increases the demand of\nreliable automated diagnosis process that can be utilized in the diagnostic pro-\ncess by experts and non-expert clinicians. From the review, we can observe that\nnumerous deep learning systems have been proposed and achieved comparable\nor superior diagnosis performance on experimental skin disease datasets. How-\never, we should be aware that a computer-aided skin disease diagnosis system\nshould be critically tested before it is accepted for real-life clinical diagnosis\ntasks.\nAcknowledgment\nThis work was supported by the National Key Research and Development\nProgram of China under Grant 2018YFC0910700 and 2019YFC0840706, the\nClinical Medicine Plus X - Young Scholars Project, Peking University 71006Y2408\nand the National Natural Science Foundation of China 11701018.\nReferences\nReferences\n[1] S. A. Gandhi, J. Kampp, Skin cancer epidemiology, detection, and man-\nagement, Medical Clinics 99 (6) (2015) 1323–1335.\n[2] G. P. Guy Jr, C. C. Thomas, T. Thompson, M. Watson, G. M. Massetti,\nL. C. Richardson, Vital signs: melanoma incidence and mortality trends\nand projectionsnited states, 1982–2030, MMWR. Morbidity and mortality\nweekly report 64 (21) (2015) 591.\n46\n[3] R. S. Stern, Prevalence of a history of skin cancer in 2007: results of an\nincidence-based model, Archives of dermatology 146 (3) (2010) 279–282.\n[4] T. Tarver, American cancer society. cancer facts and ﬁgures 2014, J Con-\nsumer Health Internet 16 (2012) 366–367.\n[5] The\namerican\ncancer\nsociety,\nhttps://www.cancer.org/cancer/\nmelanoma-skin-cancer/about/key-statistics.html,\naccessed\nDec.\n02, 2020.\n[6] A. Lomas, J. Leonardi-Bee, F. Bath-Hextall, A systematic review of world-\nwide incidence of nonmelanoma skin cancer, British Journal of Dermatol-\nogy 166 (5) (2012) 1069–1080.\n[7] A.-R. A. Ali, T. M. Deserno, A systematic review of automated melanoma\ndetection in dermatoscopic images and its ground truth data, in: Medical\nImaging 2012: Image Perception, Observer Performance, and Technology\nAssessment, Vol. 8318, International Society for Optics and Photonics,\n2012, p. 83181I.\n[8] T. P. Habif, M. S. Chapman, J. G. Dinulos, K. A. Zug, Skin disease\ne-book: diagnosis and treatment, Elsevier Health Sciences, 2017.\n[9] J. D. Whited, J. M. Grichnik, Does this patient have a mole or a\nmelanoma?, Jama 279 (9) (1998) 696–701.\n[10] Dermoﬁt image library, https://licensing.edinburgh-innovations.\ned.ac.uk/i/software/dermofit-image-library.html, accessed Sept.\n11, 2019.\n[11] J. L. G. Arroyo, B. G. Zapirain, Automated detection of melanoma in\ndermoscopic images, in: Computer vision techniques for the diagnosis of\nskin cancer, Springer, 2014, pp. 139–192.\n[12] A. Madooei, M. S. Drew, Incorporating colour information for computer-\naided diagnosis of melanoma from dermoscopy images: A retrospective\nsurvey and critical analysis, International journal of biomedical imaging\n2016.\n[13] A. S´aez, B. Acha, C. Serrano, Pattern analysis in dermoscopic images,\nin: Computer vision techniques for the diagnosis of skin Cancer, Springer,\n2014, pp. 23–48.\n[14] Y. Bengio, A. Courville, P. Vincent, Representation learning: A review\nand new perspectives, IEEE transactions on pattern analysis and machine\nintelligence 35 (8) (2013) 1798–1828.\n[15] H. Chang, Y. Zhou, A. Borowsky, K. Barner, P. Spellman, B. Parvin,\nStacked predictive sparse decomposition for classiﬁcation of histology sec-\ntions, International journal of computer vision 113 (1) (2015) 3–18.\n47\n[16] A. A. Cruz-Roa, J. E. A. Ovalle, A. Madabhushi, F. A. G. Osorio, A\ndeep learning architecture for image representation, visual interpretabil-\nity and automated basal-cell carcinoma cancer detection, in: International\nConference on Medical Image Computing and Computer-Assisted Inter-\nvention, Springer, 2013, pp. 403–410.\n[17] J. Arevalo, A. Cruz-Roa, V. Arias, E. Romero, F. A. Gonz´alez, An unsu-\npervised feature learning framework for basal cell carcinoma image anal-\nysis, Artiﬁcial intelligence in medicine 64 (2) (2015) 131–145.\n[18] H. Wang, A. Cruz-Roa, A. Basavanhally, H. Gilmore, N. Shih, M. Feld-\nman, J. Tomaszewski, F. Gonzalez, A. Madabhushi, Cascaded ensemble\nof convolutional neural networks and handcrafted features for mitosis de-\ntection, in: Medical Imaging 2014: Digital Pathology, Vol. 9041, Interna-\ntional Society for Optics and Photonics, 2014, p. 90410B.\n[19] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-\nnition, in: Proceedings of the IEEE conference on computer vision and\npattern recognition, 2016, pp. 770–778.\n[20] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with\ndeep convolutional neural networks, in: Advances in neural information\nprocessing systems, 2012, pp. 1097–1105.\n[21] V. Badrinarayanan, A. Kendall, R. Cipolla, Segnet: A deep convolutional\nencoder-decoder architecture for image segmentation, IEEE transactions\non pattern analysis and machine intelligence 39 (12) (2017) 2481–2495.\n[22] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. L. Yuille,\nDeeplab:\nSemantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected crfs, IEEE transactions on pattern\nanalysis and machine intelligence 40 (4) (2017) 834–848.\n[23] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang,\nZ. Wang, C.-C. Loy, et al., Deepid-net: Deformable deep convolutional\nneural networks for object detection, in: Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2015, pp. 2403–2412.\n[24] G. Li, Y. Yu, Deep contrast learning for salient object detection, in: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition, 2016, pp. 478–487.\n[25] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, Y. LeCun,\nOverfeat: Integrated recognition, localization and detection using convo-\nlutional networks, arXiv preprint arXiv:1312.6229.\n[26] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object\ndetection with region proposal networks, in: Advances in neural informa-\ntion processing systems, 2015, pp. 91–99.\n48\n[27] A. Esteva, B. Kuprel, S. Thrun, Deep networks for early stage skin disease\nand skin cancer classiﬁcation, Project Report, Stanford University.\n[28] N. C. Codella, Q.-B. Nguyen, S. Pankanti, D. Gutman, B. Helba,\nA. Halpern, J. R. Smith, Deep learning ensembles for melanoma recog-\nnition in dermoscopy images, IBM Journal of Research and Development\n61 (4/5) (2017) 5–1.\n[29] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau,\nS. Thrun, Dermatologist-level classiﬁcation of skin cancer with deep neural\nnetworks, Nature 542 (7639) (2017) 115.\n[30] M. Binder, A. Steiner, M. Schwarz, S. Knollmayer, K. Wolﬀ, H. Peham-\nberger, Application of an artiﬁcial neural network in epiluminescence mi-\ncroscopy pattern analysis of pigmented skin lesions: a pilot study, British\nJournal of Dermatology 130 (4) (1994) 460–465.\n[31] H. Liao, Y. Li, J. Luo, Skin disease classiﬁcation versus skin lesion char-\nacterization: Achieving robust diagnosis using multi-label deep neural\nnetworks, in: 2016 23rd International Conference on Pattern Recognition\n(ICPR), IEEE, 2016, pp. 355–360.\n[32] S. Pathan, K. G. Prabhu, P. Siddalingaswamy, Techniques and algorithms\nfor computer aided diagnosis of pigmented skin lesions review, Biomedical\nSignal Processing and Control 39 (2018) 237–262.\n[33] T. J. Brinker, A. Hekler, J. S. Utikal, N. Grabe, D. Schadendorf, J. Klode,\nC. Berking, T. Steeb, A. H. Enk, C. von Kalle, Skin cancer classiﬁcation\nusing convolutional neural networks: Systematic review., Journal of Med-\nical Internet Research 20 (10).\n[34] A. Uong, L. I. Zon, Melanocytes in development and cancer, Journal of\ncellular physiology 222 (1) (2010) 38–41.\n[35] J. Feng, N. Isern, S. Burton, J. Hu, Studies of secondary melanoma on\nc57bl/6j mouse liver using 1h nmr metabolomics, Metabolites 3 (4) (2013)\n1011–1035.\n[36] H. W. Rogers, M. A. Weinstock, S. R. Feldman, B. M. Coldiron, Incidence\nestimate of nonmelanoma skin cancer (keratinocyte carcinomas) in the us\npopulation, 2012, JAMA dermatology 151 (10) (2015) 1081–1086.\n[37] D. D. G´omez, C. Butakoﬀ, B. K. Ersboll, W. Stoecker, Independent his-\ntogram pursuit for segmentation of skin lesions, IEEE transactions on\nbiomedical engineering 55 (1) (2007) 157–161.\n[38] A. Marghoob, R. Braun, An atlas of dermoscopy, CRC Press, 2012.\n49\n[39] G. Pellacani, S. Seidenari, Comparison between morphological parame-\nters in pigmented skin lesion images acquired by means of epilumines-\ncence surface microscopy and polarized-light videomicroscopy, Clinics in\ndermatology 20 (3) (2002) 222–227.\n[40] C. Sinz, P. Tschandl, C. Rosendahl, B. N. Akay, G. Argenziano, A. Blum,\nR. P. Braun, H. Cabo, J.-Y. Gourhant, J. Kreusch, et al., Accuracy of der-\nmatoscopy for the diagnosis of nonpigmented cancers of the skin, Journal\nof the American Academy of Dermatology 77 (6) (2017) 1100–1109.\n[41] W. Stolz, O. Braun-Falco, P. Bilek, M. Landthaler, W. H. Burgdorf, A. B.\nCognetta, Color atlas of dermatoscopy, Wiley-Blackwell, 2002.\n[42] S. Sacchidanand, Nail & Its Disorders, JP Medical Ltd, 2013.\n[43] H. P. Soyer, G. Argenziano, R. Hofmann-Wellenhof, I. Zalaudek, Der-\nmoscopy E-Book: The Essentials: Expert Consult-Online and Print, El-\nsevier Health Sciences, 2011.\n[44] O. Noor, A. Nanda, B. K. Rao, A dermoscopy survey to assess who is using\nit and why it is or is not being used, International journal of dermatology\n48 (9) (2009) 951–952.\n[45] A. F. Jerant, J. T. Johnson, C. Demastes Sheridan, T. J. Caﬀrey, Early\ndetection and treatment of skin cancer., American family physician 62 (2).\n[46] E. Erdei, S. M. Torres, A new understanding in the epidemiology of\nmelanoma, Expert review of anticancer therapy 10 (11) (2010) 1811–1823.\n[47] H. Lorentzen, K. Weismann, C. S. Petersen, F. Grønhøj Larsen, L. Secher,\nV. Skødt, Clinical and dermatoscopic diagnosis of malignant melanoma:\nassessed by expert and non-expert groups., Acta dermato-venereologica\n79 (4).\n[48] A. Gerger, S. Koller, T. Kern, C. Massone, K. Steiger, E. Richtig, H. Kerl,\nJ. Smolle, Diagnostic applicability of in vivo confocal laser scanning mi-\ncroscopy in melanocytic skin tumors, Journal of investigative dermatology\n124 (3) (2005) 493–498.\n[49] P. Guitera, S. W. Menzies, C. Longo, A. M. Cesinaro, R. A. Scolyer,\nG. Pellacani, In vivo confocal microscopy for diagnosis of melanoma and\nbasal cell carcinoma using a two-step method: analysis of 710 consecutive\nclinically equivocal cases, Journal of investigative dermatology 132 (10)\n(2012) 2386–2394.\n[50] J. G. Fujimoto, Optical coherence tomography for ultrahigh resolution in\nvivo imaging, Nature biotechnology 21 (11) (2003) 1361.\n50\n[51] A. Blum, R. Hofmann-Wellenhof, H. Luedtke, U. Ellwanger, A. Steins,\nS. Roehm, C. Garbe, H. Soyer, Value of the clinical history for diﬀer-\nent users of dermoscopy compared with results of digital image analysis,\nJournal of the European Academy of Dermatology and Venereology 18 (6)\n(2004) 665–669.\n[52] C. Passmann, H. Ermert, A 100-mhz ultrasound imaging system for der-\nmatologic and ophthalmologic diagnostics, IEEE transactions on ultra-\nsonics, ferroelectrics, and frequency control 43 (4) (1996) 545–552.\n[53] H. Tran, F. Charleux, M. Rachik, A. Ehrlacher, M. Ho Ba Tho, In vivo\ncharacterization of the mechanical properties of human skin derived from\nmri and indentation techniques, Computer methods in biomechanics and\nbiomedical engineering 10 (6) (2007) 401–407.\n[54] B. Jalil, F. Marzani, Multispectral image processing applied to dermatol-\nogy, Le2i laboratory Universite de Bourgogne.\n[55] M. R. Rajeswari, A. Jain, A. Sharma, D. Singh, N. Jagannathan,\nU. Sharma, M. Degaonkar, Evaluation of skin tumors by magnetic res-\nonance imaging, Laboratory investigation 83 (9) (2003) 1279–1283.\n[56] E. Chao, C. K. Meenan, L. K. Ferris, Smartphone-based applications\nfor skin monitoring and melanoma detection, Dermatologic clinics 35 (4)\n(2017) 551–557.\n[57] R. R. Jahan-Tigh, G. M. Chinn, R. P. Rapini, A comparative study be-\ntween smartphone-based microscopy and conventional light microscopy in\n1021 dermatopathology specimens, Archives of pathology & laboratory\nmedicine 140 (1) (2016) 86–90.\n[58] M. Goyal, T. Knackstedt, S. Yan, A. Oakley, S. Hassanpour, Artiﬁcial\nintelligence-based image classiﬁcation for diagnosis of skin cancer: Chal-\nlenges and opportunities, arXiv preprint arXiv:1911.11872.\n[59] T. Mendonc¨ya, P. Ferreira, J. Marques, A. Marc¨yal, J. Rozeira, A der-\nmoscopic image database for research and benchmarking, Presentation in\nProceedings of PH 2.\n[60] H. Liao, A deep learning approach to universal skin disease classiﬁcation,\nUniversity of Rochester Department of Computer Science, CSC.\n[61] Dermnet, http://www.dermnet.com/, accessed Sept. 11, 2019.\n[62] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W.\nDusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, et al., Skin lesion\nanalysis toward melanoma detection: A challenge at the 2017 international\nsymposium on biomedical imaging (isbi), hosted by the international skin\nimaging collaboration (isic), in: 2018 IEEE 15th International Symposium\non Biomedical Imaging (ISBI 2018), IEEE, 2018, pp. 168–172.\n51\n[63] D. Gutman, N. C. Codella, E. Celebi, B. Helba, M. Marchetti, N. Mishra,\nA. Halpern, Skin lesion analysis toward melanoma detection: A challenge\nat the international symposium on biomedical imaging (isbi) 2016, hosted\nby the international skin imaging collaboration (isic), arXiv preprint\narXiv:1605.01397.\n[64] M. A. Marchetti, N. C. Codella, S. W. Dusza, D. A. Gutman, B. Helba,\nA. Kalloo, N. Mishra, C. Carrera, M. E. Celebi, J. L. DeFazio, et al.,\nResults of the 2016 international skin imaging collaboration international\nsymposium on biomedical imaging challenge: Comparison of the accuracy\nof computer algorithms to dermatologists for the diagnosis of melanoma\nfrom dermoscopic images, Journal of the American Academy of Derma-\ntology 78 (2) (2018) 270–277.\n[65] Y. Li, L. Shen, Skin lesion analysis towards melanoma detection using\ndeep learning network, Sensors 18 (2) (2018) 556.\n[66] P. Tschandl, C. Rosendahl, H. Kittler, The ham10000 dataset, a large\ncollection of multi-source dermatoscopic images of common pigmented\nskin lesions, Scientiﬁc data 5 (2018) 180161.\n[67] G. Argenziano, H. Soyer, V. De Giorgi, D. Piccolo, P. Carli, M. Delﬁno,\net al., Dermoscopy: a tutorial, EDRA, Medical Publishing & New Media\n16.\n[68] S. S. Han, M. S. Kim, W. Lim, G. H. Park, I. Park, S. E. Chang, Classiﬁ-\ncation of the clinical images for benign and malignant cutaneous tumors\nusing a deep learning algorithm, Journal of Investigative Dermatology\n138 (7) (2018) 1529–1538.\n[69] Atlasderm, www.atlasdermatologico.com.br, accessed Sept. 11, 2019.\n[70] Danderm, http://www.danderm.dk/, accessed Sept. 11, 2019.\n[71] A. Boer, K. Nischal, et al., www. derm101. com: A growing online re-\nsource for learning dermatology and dermatopathology, Indian Journal of\nDermatology, Venereology, and Leprology 73 (2) (2007) 138.\n[72] J. Kawahara, S. Daneshvar, G. Argenziano, G. Hamarneh, Seven-point\nchecklist and skin lesion classiﬁcation using multitask multimodal neural\nnets, IEEE journal of biomedical and health informatics 23 (2) (2018)\n538–546.\n[73] X. Sun, J. Yang, M. Sun, K. Wang, A benchmark for automatic visual\nclassiﬁcation of clinical skin disease images, in: European Conference on\nComputer Vision, Springer, 2016, pp. 206–222.\n[74] Dermis,\nhttp://www.dermis.net/dermisroot/en/home/indexp.htm,\naccessed Sept. 11, 2019.\n52\n[75] X. Yi, E. Walia, P. Babyn, Unsupervised and semi-supervised learn-\ning with categorical generative adversarial networks assisted by wasser-\nstein distance for dermoscopy image classiﬁcation,\narXiv preprint\narXiv:1804.03700.\n[76] The\ncancer\ngenome\natlas,\nhttps://www.cancer.gov/about-nci/\norganization/ccg/research/structural-genomics/tcga,\naccessed\nFeb. 28, 2020.\n[77] A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo,\nK. Chou, C. Cui, G. Corrado, S. Thrun, J. Dean, A guide to deep learning\nin healthcare, Nature medicine 25 (1) (2019) 24.\n[78] A. Mahbod, G. Schaefer, I. Ellinger, R. Ecker, A. Pitiot, C. Wang, Fus-\ning ﬁne-tuned deep features for skin lesion classiﬁcation, Computerized\nMedical Imaging and Graphics 71 (2019) 19–29.\n[79] J. A. Hartigan, M. A. Wong, Algorithm as 136: A k-means clustering al-\ngorithm, Journal of the Royal Statistical Society. Series C (Applied Statis-\ntics) 28 (1) (1979) 100–108.\n[80] X. J. Zhu, Semi-supervised learning literature survey, Tech. rep., Univer-\nsity of Wisconsin-Madison Department of Computer Sciences (2005).\n[81] A. Eitel, J. T. Springenberg, L. Spinello, M. Riedmiller, W. Burgard,\nMultimodal deep learning for robust rgb-d object recognition, in: 2015\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), IEEE, 2015, pp. 681–687.\n[82] C. R. Qi, H. Su, K. Mo, L. J. Guibas, Pointnet: Deep learning on point\nsets for 3d classiﬁcation and segmentation, in: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2017, pp. 652–\n660.\n[83] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey, et al., Google’s neural ma-\nchine translation system: Bridging the gap between human and machine\ntranslation, arXiv preprint arXiv:1609.08144.\n[84] J. Zhou, Y. Cao, X. Wang, P. Li, W. Xu, Deep recurrent models with\nfast-forward connections for neural machine translation, Transactions of\nthe Association for Computational Linguistics 4 (2016) 371–383.\n[85] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, Y. Bengio, Attention-\nbased models for speech recognition, in: Advances in neural information\nprocessing systems, 2015, pp. 577–585.\n[86] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,\nC. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al., Deep speech\n2: End-to-end speech recognition in english and mandarin, in: Interna-\ntional conference on machine learning, 2016, pp. 173–182.\n53\n[87] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, A. Zisserman, Deep audio-\nvisual speech recognition, IEEE transactions on pattern analysis and ma-\nchine intelligence.\n[88] J. He, S. L. Baxter, J. Xu, J. Xu, X. Zhou, K. Zhang, The practical\nimplementation of artiﬁcial intelligence technologies in medicine, Nature\nmedicine 25 (1) (2019) 30.\n[89] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in:\nProceedings of the IEEE conference on computer vision and pattern recog-\nnition, 2015, pp. 1–9.\n[90] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely con-\nnected convolutional networks, in: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017, pp. 4700–4708.\n[91] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, in: Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2018,\npp. 7132–7141.\n[92] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, nature 521 (7553) (2015)\n436.\n[93] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.\n[94] Y. Bengio, I. Goodfellow, A. Courville, Deep learning, Vol. 1, Citeseer,\n2017.\n[95] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoo-\nrian, J. A. Van Der Laak, B. Van Ginneken, C. I. S´anchez, A survey on\ndeep learning in medical image analysis, Medical image analysis 42 (2017)\n60–88.\n[96] G. E. Hinton, R. R. Salakhutdinov, Reducing the dimensionality of data\nwith neural networks, science 313 (5786) (2006) 504–507.\n[97] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle, Greedy layer-wise\ntraining of deep networks, in: Advances in neural information process-\ning systems, 2007, pp. 153–160.\n[98] P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, Extracting and\ncomposing robust features with denoising autoencoders, in: Proceedings\nof the 25th international conference on Machine learning, ACM, 2008, pp.\n1096–1103.\n[99] G. E. Hinton, Deep belief networks, Scholarpedia 4 (5) (2009) 5947.\n[100] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock`y, S. Khudanpur, Recurrent\nneural network based language model, in: Eleventh annual conference of\nthe international speech communication association, 2010.\n54\n[101] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mol-\nlura, R. M. Summers, Deep convolutional neural networks for computer-\naided detection: Cnn architectures, dataset characteristics and transfer\nlearning, IEEE transactions on medical imaging 35 (5) (2016) 1285–1298.\n[102] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural\nnetworks for volumetric medical image segmentation, in: 2016 Fourth\nInternational Conference on 3D Vision (3DV), IEEE, 2016, pp. 565–571.\n[103] P. Lakhani, B. Sundaram, Deep learning at chest radiography: automated\nclassiﬁcation of pulmonary tuberculosis by using convolutional neural net-\nworks, Radiology 284 (2) (2017) 574–582.\n[104] A. S. Lundervold, A. Lundervold, An overview of deep learning in medical\nimaging focusing on mri, Zeitschrift f¨ur Medizinische Physik 29 (2) (2019)\n102–127.\n[105] M. Leshno, V. Y. Lin, A. Pinkus, S. Schocken, Multilayer feedforward\nnetworks with a nonpolynomial activation function can approximate any\nfunction, Neural networks 6 (6) (1993) 861–867.\n[106] B. Xu, N. Wang, T. Chen, M. Li, Empirical evaluation of rectiﬁed activa-\ntions in convolutional network, arXiv preprint arXiv:1505.00853.\n[107] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation, in: Proceedings of\nthe IEEE international conference on computer vision, 2015, pp. 1026–\n1034.\n[108] J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller, Striving for\nsimplicity: The all convolutional net, arXiv preprint arXiv:1412.6806.\n[109] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\nDropout: a simple way to prevent neural networks from overﬁtting, The\njournal of machine learning research 15 (1) (2014) 1929–1958.\n[110] S. Ioﬀe, C. Szegedy, Batch normalization:\nAccelerating deep net-\nwork training by reducing internal covariate shift,\narXiv preprint\narXiv:1502.03167.\n[111] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. E. Hubbard, L. D. Jackel, Handwritten digit recognition with a back-\npropagation network, in: Advances in neural information processing sys-\ntems, 1990, pp. 396–404.\n[112] A. Deshpande, The 9 deep learning papers you need to know about (un-\nderstanding cnns part 3), adeshpande3. github. io. Retrieved (2018) 12–04.\n[113] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-\nscale image recognition, arXiv preprint arXiv:1409.1556.\n55\n[114] M. Lin,\nQ. Chen,\nS. Yan,\nNetwork in network,\narXiv preprint\narXiv:1312.4400.\n[115] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, K. He, Aggregated residual transfor-\nmations for deep neural networks, in: Proceedings of the IEEE conference\non computer vision and pattern recognition, 2017, pp. 1492–1500.\n[116] B. Zoph, V. Vasudevan, J. Shlens, Q. V. Le, Learning transferable ar-\nchitectures for scalable image recognition, in: Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018, pp. 8697–\n8710.\n[117] I. Bello, B. Zoph, V. Vasudevan, Q. V. Le, Neural optimizer search with\nreinforcement learning, in: Proceedings of the 34th International Confer-\nence on Machine Learning-Volume 70, JMLR. org, 2017, pp. 459–468.\n[118] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, in: Ad-\nvances in neural information processing systems, 2014, pp. 2672–2680.\n[119] A. Radford, L. Metz, S. Chintala, Unsupervised representation learning\nwith deep convolutional generative adversarial networks, arXiv preprint\narXiv:1511.06434.\n[120] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image trans-\nlation using cycle-consistent adversarial networks, in: Proceedings of the\nIEEE international conference on computer vision, 2017, pp. 2223–2232.\n[121] H. Zhang, I. Goodfellow, D. Metaxas, A. Odena, Self-attention generative\nadversarial networks, arXiv preprint arXiv:1805.08318.\n[122] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for\nbiomedical image segmentation, in: International Conference on Medical\nimage computing and computer-assisted intervention, Springer, 2015, pp.\n234–241.\n[123] R. Girshick, Fast r-cnn, in: Proceedings of the IEEE international confer-\nence on computer vision, 2015, pp. 1440–1448.\n[124] K. He, G. Gkioxari, P. Doll´ar, R. Girshick, Mask r-cnn, in: Proceedings\nof the IEEE international conference on computer vision, 2017, pp. 2961–\n2969.\n[125] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, et al., Tensorﬂow: A system for large-\nscale machine learning, in: 12th {USENIX} Symposium on Operating\nSystems Design and Implementation ({OSDI} 16), 2016, pp. 265–283.\n[126] F. Chollet, et al., Keras (2015).\n56\n[127] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, A. Lerer, Automatic diﬀerentiation in pytorch.\n[128] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, T. Darrell, Caﬀe: Convolutional architecture for fast fea-\nture embedding, in: Proceedings of the 22nd ACM international confer-\nence on Multimedia, ACM, 2014, pp. 675–678.\n[129] Sonnet, https://sonnet.dev/, accessed Sept. 11, 2019.\n[130] T. Chen,\nM. Li,\nY. Li,\nM. Lin,\nN. Wang, M. Wang,\nT. Xiao,\nB. Xu, C. Zhang, Z. Zhang, Mxnet: A ﬂexible and eﬃcient machine\nlearning library for heterogeneous distributed systems, arXiv preprint\narXiv:1512.01274.\n[131] L. Zheng, Y. Zhao, S. Wang, J. Wang, Q. Tian, Good practice in cnn\nfeature transfer, arXiv preprint arXiv:1604.00133.\n[132] Z. Yu, X. Jiang, F. Zhou, J. Qin, D. Ni, S. Chen, B. Lei, T. Wang,\nMelanoma recognition in dermoscopy images via aggregated deep convo-\nlutional features, IEEE Transactions on Biomedical Engineering 66 (4)\n(2018) 1006–1016.\n[133] M. Rastgoo, R. Garcia, O. Morel, F. Marzani, Automatic diﬀerentiation\nof melanoma from dysplastic nevi, Computerized Medical Imaging and\nGraphics 43 (2015) 44–52.\n[134] H. J. Vala, A. Baxi, A review on otsu image segmentation algorithm,\nInternational Journal of Advanced Research in Computer Engineering &\nTechnology (IJARCET) 2 (2) (2013) 387–389.\n[135] Z.-K. Huang, K.-W. Chau, A new image thresholding method based on\ngaussian mixture model, Applied Mathematics and Computation 205 (2)\n(2008) 899–907.\n[136] K. Parvati, P. Rao, M. Mariya Das, Image segmentation using gray-\nscale morphology and marker-controlled watershed transformation, Dis-\ncrete Dynamics in Nature and Society 2008.\n[137] C. Shorten, T. M. Khoshgoftaar, A survey on image data augmentation\nfor deep learning, Journal of Big Data 6 (1) (2019) 60.\n[138] M. A. Al-Masni, M. A. Al-antari, M.-T. Choi, S.-M. Han, T.-S. Kim,\nSkin lesion segmentation in dermoscopy images via deep full resolution\nconvolutional networks, Computer methods and programs in biomedicine\n162 (2018) 221–231.\n[139] M. E. Celebi, H. Iyatomi, G. Schaefer, W. V. Stoecker, Lesion border de-\ntection in dermoscopy images, Computerized medical imaging and graph-\nics 33 (2) (2009) 148–153.\n57\n[140] M. E. Celebi, Q. Wen, H. Iyatomi, K. Shimizu, H. Zhou, G. Schaefer, A\nstate-of-the-art survey on lesion border detection in dermoscopy images,\nDermoscopy image analysis 10 (2015) 97–129.\n[141] H. Chang, Skin cancer reorganization and classiﬁcation with deep neural\nnetwork, arXiv preprint arXiv:1703.00534.\n[142] L. Yu, H. Chen, Q. Dou, J. Qin, P.-A. Heng, Automated melanoma recog-\nnition in dermoscopy images via very deep residual networks, IEEE trans-\nactions on medical imaging 36 (4) (2016) 994–1004.\n[143] Y. Yuan, M. Chao, Y.-C. Lo, Automatic skin lesion segmentation using\ndeep fully convolutional networks with jaccard distance, IEEE transac-\ntions on medical imaging 36 (9) (2017) 1876–1886.\n[144] H. M. ¨Unver, E. Ayan, Skin lesion segmentation in dermoscopic images\nwith combination of yolo and grabcut algorithm, Diagnostics 9 (3) (2019)\n72.\n[145] Y. Peng, N. Wang, Y. Wang, M. Wang, Segmentation of dermoscopy im-\nage using adversarial networks, Multimedia Tools and Applications 78 (8)\n(2019) 10965–10981.\n[146] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for se-\nmantic segmentation, in: Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2015, pp. 3431–3440.\n[147] M. Attia, M. Hossny, S. Nahavandi, A. Yazdabadi, Skin melanoma seg-\nmentation using recurrent and convolutional neural networks, in: 2017\nIEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),\nIEEE, 2017, pp. 292–296.\n[148] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural compu-\ntation 9 (8) (1997) 1735–1780.\n[149] L. Bi, J. Kim, E. Ahn, A. Kumar, M. Fulham, D. Feng, Dermoscopic\nimage segmentation via multistage fully convolutional networks, IEEE\nTransactions on Biomedical Engineering 64 (9) (2017) 2065–2074.\n[150] M. Goyal, M. H. Yap, Multi-class semantic segmentation of skin lesions\nvia fully convolutional networks, arXiv preprint arXiv:1711.10449.\n[151] A. Phillips, I. Teo, J. Lang, Segmentation of prognostic tissue structures\nin cutaneous melanoma using whole slide images, in: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition Work-\nshops, 2019, pp. 0–0.\n[152] ¨O. C¸i¸cek, A. Abdulkadir, S. S. Lienkamp, T. Brox, O. Ronneberger, 3d\nu-net: learning dense volumetric segmentation from sparse annotation,\nin: International conference on medical image computing and computer-\nassisted intervention, Springer, 2016, pp. 424–432.\n58\n[153] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,\nK. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, et al., Attention u-net:\nLearning where to look for the pancreas, arXiv preprint arXiv:1804.03999.\n[154] B. S. Lin, K. Michael, S. Kalra, H. R. Tizhoosh, Skin lesion segmentation:\nU-nets versus clustering, in: 2017 IEEE Symposium Series on Computa-\ntional Intelligence (SSCI), IEEE, 2017, pp. 1–7.\n[155] Y. Yuan, Y.-C. Lo, Improving dermoscopic image segmentation with en-\nhanced convolutional-deconvolutional networks, IEEE journal of biomed-\nical and health informatics 23 (2) (2017) 519–526.\n[156] W. Ji, L. Cai, W. Chen, M. Chen, G. Chai, Segmentation of lesions in skin\nimage based on salient object detection with deeply supervised learning,\nin: 2018 IEEE 4th International Conference on Computer and Communi-\ncations (ICCC), IEEE, 2018, pp. 1567–1573.\n[157] L. Canalini, F. Pollastri, F. Bolelli, M. Cancilla, S. Allegretti, C. Grana,\nSkin lesion segmentation ensemble with diverse training strategies, in:\n18th International Conference on Computer Analysis of Images and Pat-\nterns, 2019.\n[158] P. Tschandl, C. Sinz, H. Kittler, Domain-speciﬁc classiﬁcation-pretrained\nfully convolutional network encoders for skin lesion segmentation, Com-\nputers in biology and medicine 104 (2019) 111–116.\n[159] A. Chaurasia, E. Culurciello, Linknet: Exploiting encoder representations\nfor eﬃcient semantic segmentation, in: 2017 IEEE Visual Communica-\ntions and Image Processing (VCIP), IEEE, 2017, pp. 1–4.\n[160] H. Li, X. He, F. Zhou, Z. Yu, D. Ni, S. Chen, T. Wang, B. Lei, Dense\ndeconvolutional network for skin lesion segmentation, IEEE journal of\nbiomedical and health informatics 23 (2) (2018) 527–537.\n[161] H. Li, X. He, Z. Yu, F. Zhou, J.-Z. Cheng, L. Huang, T. Wang, B. Lei,\nSkin lesion segmentation via dense connected deconvolutional network,\nin: 2018 24th International Conference on Pattern Recognition (ICPR),\nIEEE, 2018, pp. 671–675.\n[162] P. Luc, C. Couprie, S. Chintala, J. Verbeek, Semantic segmentation using\nadversarial networks, arXiv preprint arXiv:1611.08408.\n[163] Z. Wei, H. Song, L. Chen, Q. Li, G. Han, Attention-based denseunet net-\nwork with adversarial training for skin lesion segmentation, IEEE Access\n7 (2019) 136616–136629.\n[164] F. Jiang, F. Zhou, J. Qin, T. Wang, B. Lei, Decision-augmented gen-\nerative adversarial network for skin lesion segmentation, in: 2019 IEEE\n16th International Symposium on Biomedical Imaging (ISBI 2019), IEEE,\n2019, pp. 447–450.\n59\n[165] L. Bi, D. Feng, M. Fulham, J. Kim, Improving skin lesion segmentation via\nstacked adversarial learning, in: 2019 IEEE 16th International Symposium\non Biomedical Imaging (ISBI 2019), IEEE, 2019, pp. 1100–1103.\n[166] W. Tu, X. Liu, W. Hu, Z. Pan, X. Xu, B. Li, Segmentation of lesion in\ndermoscopy images using dense-residual network with adversarial learn-\ning, in: 2019 IEEE International Conference on Image Processing (ICIP),\nIEEE, 2019, pp. 1430–1434.\n[167] A. Udrea, G. D. Mitra, Generative adversarial neural networks for pig-\nmented and non-pigmented skin lesions detection in clinical images, in:\n2017 21st International Conference on Control Systems and Computer\nScience (CSCS), IEEE, 2017, pp. 364–368.\n[168] M. Sarker, M. Kamal, H. A. Rashwan, M. Abdel-Nasser, V. K. Singh,\nS. F. Banu, F. Akram, F. U. Chowdhury, K. A. Choudhury, S. Chambon,\net al., Mobilegan: Skin lesion segmentation using a lightweight generative\nadversarial network, arXiv preprint arXiv:1907.00856.\n[169] V. K. Singh, M. Abdel-Nasser, H. A. Rashwan, F. Akram, N. Pandey,\nA. Lalande, B. Presles, S. Romani, D. Puig, Fca-net: Adversarial learning\nfor skin lesion segmentation based on multi-scale features and factorized\nchannel attention, IEEE Access 7 (2019) 130552–130565.\n[170] M. H. Jafari, N. Karimi, E. Nasr-Esfahani, S. Samavi, S. M. R. Soroush-\nmehr, K. Ward, K. Najarian, Skin lesion segmentation in clinical images\nusing deep learning, in: 2016 23rd International conference on pattern\nrecognition (ICPR), IEEE, 2016, pp. 337–342.\n[171] Z. Cui, L. Wu, R. Wang, W.-S. Zheng, Ensemble transductive learning for\nskin lesion segmentation, in: Chinese Conference on Pattern Recognition\nand Computer Vision (PRCV), Springer, 2019, pp. 572–581.\n[172] A. Soudani, W. Barhoumi, An image-based segmentation recommender\nusing crowdsourcing and transfer learning for skin lesion extraction, Ex-\npert Systems with Applications 118 (2019) 400–410.\n[173] A. Masood, A. Al-Jumaily, K. Anam, Self-supervised learning model for\nskin cancer diagnosis, in: 2015 7th International IEEE/EMBS Conference\non Neural Engineering (NER), IEEE, 2015, pp. 1012–1015.\n[174] Y.\nMaali,\nA.\nAl-Jumaily,\nSelf-advising\nsupport\nvector\nmachine,\nKnowledge-Based Systems 52 (2013) 214–222.\n[175] J. Premaladha, K. Ravichandran, Novel approaches for diagnosing\nmelanoma skin lesions through supervised and deep learning algorithms,\nJournal of medical systems 40 (4) (2016) 96.\n60\n[176] E. Nasr-Esfahani, S. Samavi, N. Karimi, S. M. R. Soroushmehr, M. H.\nJafari, K. Ward, K. Najarian, Melanoma detection by analysis of clinical\nimages using convolutional neural network, in: 2016 38th Annual Inter-\nnational Conference of the IEEE Engineering in Medicine and Biology\nSociety (EMBC), IEEE, 2016, pp. 1373–1376.\n[177] S. Demyanov, R. Chakravorty, M. Abedini, A. Halpern, R. Garnavi, Clas-\nsiﬁcation of dermoscopy patterns using deep convolutional neural net-\nworks, in: 2016 IEEE 13th International Symposium on Biomedical Imag-\ning (ISBI), IEEE, 2016, pp. 364–368.\n[178] B. Walker, J. Rehg, A. Kalra, R. Winters, P. Drews, J. Dascalu, E. David,\nA. Dascalu, Dermoscopy diagnosis of cancerous lesions utilizing dual deep\nlearning algorithms via visual and audio (soniﬁcation) outputs: Labora-\ntory and prospective observational studies, EBioMedicine 40 (2019) 176–\n183.\n[179] T. J. Brinker, A. Hekler, A. H. Enk, J. Klode, A. Hauschild, C. Berking,\nB. Schilling, S. Haferkamp, D. Schadendorf, S. Fr¨ohling, et al., A convolu-\ntional neural network trained with dermoscopic images performed on par\nwith 145 dermatologists in a clinical melanoma image classiﬁcation task,\nEuropean Journal of Cancer 111 (2019) 148–154.\n[180] S. Singh, D. Hoiem, D. Forsyth, Swapout: Learning an ensemble of deep\narchitectures, in:\nAdvances in neural information processing systems,\n2016, pp. 28–36.\n[181] I. Lee, D. Kim, S. Kang, S. Lee, Ensemble deep learning for skeleton-based\naction recognition using temporal sliding lstm networks, in: Proceedings of\nthe IEEE International Conference on Computer Vision, 2017, pp. 1012–\n1020.\n[182] S. S. Han, G. H. Park, W. Lim, M. S. Kim, J. Im Na, I. Park, S. E. Chang,\nDeep neural networks show an equivalent and often superior performance\nto dermatologists in onychomycosis diagnosis: Automatic construction of\nonychomycosis datasets by region-based convolutional deep neural net-\nwork, PloS one 13 (1) (2018) e0191493.\n[183] P. Tschandl, C. Rosendahl, B. N. Akay, G. Argenziano, A. Blum, R. P.\nBraun, H. Cabo, J.-Y. Gourhant, J. Kreusch, A. Lallas, et al., Expert-\nlevel diagnosis of nonpigmented skin cancer by combined convolutional\nneural networks, JAMA dermatology 155 (1) (2019) 58–65.\n[184] F. Perez, S. Avila, E. Valle, Solo or ensemble? choosing a cnn architecture\nfor melanoma classiﬁcation, in: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition Workshops, 2019, pp. 0–0.\n[185] J. Yosinski, J. Clune, Y. Bengio, H. Lipson, How transferable are features\nin deep neural networks?, in: Advances in neural information processing\nsystems, 2014, pp. 3320–3328.\n61\n[186] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A large-\nscale hierarchical image database, in: 2009 IEEE conference on computer\nvision and pattern recognition, Ieee, 2009, pp. 248–255.\n[187] J. Kawahara, A. BenTaieb, G. Hamarneh, Deep features to classify skin\nlesions, in: 2016 IEEE 13th International Symposium on Biomedical Imag-\ning (ISBI), IEEE, 2016, pp. 1397–1400.\n[188] X. Zhang, S. Wang, J. Liu, C. Tao, Computer-aided diagnosis of four com-\nmon cutaneous diseases using deep learning algorithm, in: 2017 IEEE In-\nternational Conference on Bioinformatics and Biomedicine (BIBM), IEEE,\n2017, pp. 1304–1306.\n[189] Y. Fujisawa, Y. Otomo, Y. Ogata, Y. Nakamura, R. Fujita, Y. Ishitsuka,\nR. Watanabe, N. Okiyama, K. Ohara, M. Fujimoto, Deep-learning-based,\ncomputer-aided classiﬁer developed with a small dataset of clinical images\nsurpasses board-certiﬁed dermatologists in skin tumour diagnosis, British\nJournal of Dermatology 180 (2) (2019) 373–381.\n[190] A. R. Lopez, X. Giro-i Nieto, J. Burdick, O. Marques, Skin lesion clas-\nsiﬁcation from dermoscopic images using deep learning techniques, in:\n2017 13th IASTED International Conference on Biomedical Engineering\n(BioMed), IEEE, 2017, pp. 49–54.\n[191] H. A. Haenssle, C. Fink, R. Schneiderbauer, F. Toberer, T. Buhl, A. Blum,\nA. Kalloo, A. B. H. Hassen, L. Thomas, A. Enk, et al., Man against\nmachine: diagnostic performance of a deep learning convolutional neu-\nral network for dermoscopic melanoma recognition in comparison to 58\ndermatologists, Annals of Oncology 29 (8) (2018) 1836–1842.\n[192] X. Zhang, S. Wang, J. Liu, C. Tao, Towards improving diagnosis of skin\ndiseases by combining deep neural network and human knowledge, BMC\nmedical informatics and decision making 18 (2) (2018) 59.\n[193] J. Jaworek-Korjakowska, P. Kleczek, M. Gorgon, Melanoma thickness pre-\ndiction based on convolutional neural network with vgg-19 model transfer\nlearning, in: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, 2019, pp. 0–0.\n[194] A. Menegola, M. Fornaciali, R. Pires, F. V. Bittencourt, S. Avila, E. Valle,\nKnowledge transfer for melanoma screening with deep learning, in: 2017\nIEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),\nIEEE, 2017, pp. 297–300.\n[195] A. Hekler, J. S. Utikal, A. H. Enk, C. Berking, J. Klode, D. Schadendorf,\nP. Jansen, C. Franklin, T. Holland-Letz, D. Krahl, et al., Pathologist-\nlevel classiﬁcation of histopathological melanoma images with deep neural\nnetworks, European Journal of Cancer 115 (2019) 79–83.\n62\n[196] T. Polevaya, R. Ravodin, A. Filchenkov, Skin lesion primary morphology\nclassiﬁcation with end-to-end deep learning network, in: 2019 Interna-\ntional Conference on Artiﬁcial Intelligence in Information and Communi-\ncation (ICAIIC), IEEE, 2019, pp. 247–250.\n[197] Z. Yang, X. He, J. Gao, L. Deng, A. Smola, Stacked attention networks\nfor image question answering, in: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 21–29.\n[198] Y. Liu, C. Sun, L. Lin, X. Wang, Learning natural language infer-\nence using bidirectional lstm model and inner-attention, arXiv preprint\narXiv:1605.09090.\n[199] C. Barata, J. S. Marques, M. Emre Celebi, Deep attention model for the\nhierarchical diagnosis of skin lesions, in: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition Workshops, 2019, pp.\n0–0.\n[200] H. Rashid, M. A. Tanveer, H. A. Khan, Skin lesion classiﬁcation using gan\nbased data augmentation, in: 2019 41st Annual International Conference\nof the IEEE Engineering in Medicine and Biology Society (EMBC), IEEE,\n2019, pp. 916–919.\n[201] D. Bisla, A. Choromanska, R. S. Berman, J. A. Stein, D. Polsky, Towards\nautomated melanoma detection with deep learning: Data puriﬁcation and\naugmentation, in: Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition Workshops, 2019, pp. 0–0.\n[202] Y. Gu, Z. Ge, C. P. Bonnington, J. Zhou, Progressive transfer learning and\nadversarial domain adaptation for cross-domain skin disease classiﬁcation,\nIEEE journal of biomedical and health informatics.\n[203] S. Mishra, H. Imaizumi, T. Yamasaki, Interpreting ﬁne-grained dermato-\nlogical classiﬁcation by deep learning, in: Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition Workshops, 2019,\npp. 0–0.\n[204] F. Wan, Deep learning method used in skin lesions segmentation and\nclassiﬁcation (2018).\n[205] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,\nM. Andreetto, H. Adam, Mobilenets: Eﬃcient convolutional neural net-\nworks for mobile vision applications, arXiv preprint arXiv:1704.04861.\n[206] X. Shi, Q. Dou, C. Xue, J. Qin, H. Chen, P.-A. Heng, An active learning\napproach for reducing annotation cost in skin lesion analysis, in: Inter-\nnational Workshop on Machine Learning in Medical Imaging, Springer,\n2019, pp. 628–636.\n63\n[207] P. Tschandl, G. Argenziano, M. Razmara, J. Yap, Diagnostic accuracy\nof content-based dermatoscopic image retrieval with deep classiﬁcation\nfeatures, British Journal of Dermatology 181 (1) (2019) 155–165.\n[208] S. Ruder, An overview of multi-task learning in deep neural networks,\narXiv preprint arXiv:1706.05098.\n[209] R. Caruana, Multitask learning, Machine learning 28 (1) (1997) 41–75.\n[210] X. Yang, Z. Zeng, S. Y. Yeo, C. Tan, H. L. Tey, Y. Su, A novel multi-task\ndeep learning model for skin lesion segmentation and classiﬁcation, arXiv\npreprint arXiv:1703.01025.\n[211] H. Liao, J. Luo, A deep multi-task learning approach to skin lesion classi-\nﬁcation, in: Workshops at the Thirty-First AAAI Conference on Artiﬁcial\nIntelligence, 2017.\n[212] A. Ghorbani, V. Natarajan, D. Coz, Y. Liu, Dermgan:\nSynthetic\ngeneration of clinical skin images with pathology,\narXiv preprint\narXiv:1911.08716.\n[213] I. S. Ali, M. F. Mohamed, Y. B. Mahdy, Data augmentation for skin le-\nsion using self-attention based progressive generative adversarial network,\narXiv preprint arXiv:1910.11960.\n[214] T. Ny´ıri, A. Kiss, Style transfer for dermatological data augmentation, in:\nProceedings of SAI Intelligent Systems Conference, Springer, 2019, pp.\n915–923.\n[215] A. Bissoto, F. Perez, E. Valle, S. Avila, Skin lesion synthesis with genera-\ntive adversarial networks, in: OR 2.0 Context-Aware Operating Theaters,\nComputer Assisted Robotic Endoscopy, Clinical Image-Based Procedures,\nand Skin Image Analysis, Springer, 2018, pp. 294–302.\n[216] C. Baur, S. Albarqouni, N. Navab, Generating highly realistic images\nof skin lesions with gans, in: OR 2.0 Context-Aware Operating Theaters,\nComputer Assisted Robotic Endoscopy, Clinical Image-Based Procedures,\nand Skin Image Analysis, Springer, 2018, pp. 260–267.\n[217] H.-Y. Yang, L. H. Staib, Dual adversarial autoencoder for dermoscopic\nimage generative modeling, in: 2019 IEEE 16th International Symposium\non Biomedical Imaging (ISBI 2019), IEEE, 2019, pp. 1247–1250.\n[218] C. Baur, S. Albarqouni, N. Navab, Melanogans: high resolution skin lesion\nsynthesis with gans, arXiv preprint arXiv:1804.04338.\n[219] S. S. Han, I. J. Moon, W. Lim, I. S. Suh, S. Y. Lee, J.-I. Na, S. H.\nKim, S. E. Chang, Keratinocytic skin cancer detection on the face using\nregion-based convolutional neural network, JAMA dermatology.\n64\n[220] A. Galdran, A. Alvarez-Gila, M. I. Meyer, C. L. Saratxaga, T. Ara´ujo,\nE. Garrote, G. Aresta, P. Costa, A. M. Mendon¸ca, A. Campilho, Data-\ndriven color augmentation techniques for deep skin image analysis, arXiv\npreprint arXiv:1703.03702.\n[221] M. Attia, M. Hossny, H. Zhou, S. Nahavandi, H. Asadi, A. Yazdabadi,\nDigital hair segmentation using hybrid convolutional and recurrent neural\nnetworks architecture, Computer Methods and Programs in Biomedicine\n177 (2019) 17–30.\n[222] S. Hu, R. M. Soza-Vento, D. F. Parker, R. S. Kirsner, Comparison of\nstage at diagnosis of melanoma among hispanic, black, and white patients\nin miami-dade county, ﬂorida, Archives of Dermatology 142 (6) (2006)\n704–708.\n[223] G. Marcus, E. Davis, Rebooting AI: building artiﬁcial intelligence we can\ntrust, Pantheon, 2019.\n[224] J. Schindelin, I. Arganda-Carreras, E. Frise, V. Kaynig, M. Longair,\nT. Pietzsch, S. Preibisch, C. Rueden, S. Saalfeld, B. Schmid, et al., Fiji: an\nopen-source platform for biological-image analysis, Nature methods 9 (7)\n(2012) 676–682.\n[225] B. C. Russell, A. Torralba, K. P. Murphy, W. T. Freeman, Labelme: a\ndatabase and web-based tool for image annotation, International journal\nof computer vision 77 (1-3) (2008) 157–173.\n[226] N. Fiedler, M. Bestmann, N. Hendrich, Imagetagger: An open source\nonline platform for collaborative image labeling, in: Robot World Cup,\nSpringer, 2018, pp. 162–169.\n[227] C. Barata, M. E. Celebi, J. S. Marques, Improving dermoscopy image\nclassiﬁcation using color constancy, IEEE journal of biomedical and health\ninformatics 19 (3) (2014) 1146–1152.\n[228] J. hua Ng, M. Goyal, B. Hewitt, M. H. Yap, The eﬀect of color constancy\nalgorithms on semantic segmentation of skin lesions, in: Medical Imaging\n2019: Biomedical Applications in Molecular, Structural, and Functional\nImaging, Vol. 10953, International Society for Optics and Photonics, 2019,\np. 109530R.\n[229] J. Xu, W. B. Croft, Quary expansion using local and global document\nanalysis, in: Acm sigir forum, Vol. 51, ACM, 2017, pp. 168–175.\n[230] X. Wu, X. Zhu, G.-Q. Wu, W. Ding, Data mining with big data, IEEE\ntransactions on knowledge and data engineering 26 (1) (2013) 97–107.\n[231] S. Sengupta, N. Mittal, M. Modi, Improved skin lesions detection using\ncolor space and artiﬁcial intelligence techniques, Journal of Dermatological\nTreatment 31 (5) (2019) 1–27.\n65\n[232] S. Izadi, Z. Mirikharaji, J. Kawahara, G. Hamarneh, Generative adver-\nsarial networks to segment skin lesions, in: 2018 IEEE 15th International\nSymposium on Biomedical Imaging (ISBI 2018), IEEE, 2018, pp. 881–884.\n[233] Z. Qin, Z. Liu, P. Zhu, Y. Xue, A gan-based image synthesis method for\nskin lesion classiﬁcation, Computer Methods and Programs in Biomedicine\n195 (2020) 105568.\n[234] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, C. Liu, A survey on deep\ntransfer learning, in: International Conference on Artiﬁcial Neural Net-\nworks, Springer, 2018, pp. 270–279.\n[235] E. Tzeng, J. Hoﬀman, K. Saenko, T. Darrell, Adversarial discriminative\ndomain adaptation, in: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2017, pp. 7167–7176.\n[236] K. You, M. Long, Z. Cao, J. Wang, M. I. Jordan, Universal domain adap-\ntation, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2019, pp. 2720–2729.\n[237] Z. Cao, K. You, M. Long, J. Wang, Q. Yang, Learning to transfer examples\nfor partial domain adaptation, in: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp. 2985–2994.\n[238] N. Akhtar, A. Mian, F. Porikli, Joint discriminative bayesian dictionary\nand classiﬁer learning, in: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2017, pp. 1193–1202.\n[239] Y. Bar, I. Diamant, L. Wolf, H. Greenspan, Deep learning with non-\nmedical training used for chest pathology identiﬁcation, in: Medical Imag-\ning 2015: Computer-Aided Diagnosis, Vol. 9414, International Society for\nOptics and Photonics, 2015, p. 94140V.\n[240] U. Lopes, J. F. Valiati, Pre-trained convolutional neural networks as\nfeature extractors for tuberculosis detection, Computers in biology and\nmedicine 89 (2017) 135–143.\n[241] D. Haritha, An inductive transfer learning approach using cycle-consistent\nadversarial domain adaptation with application to brain tumor segmen-\ntation.\n[242] O. Chapelle, B. Scholkopf, A. Zien, Semi-supervised learning (chapelle, o.\net al., eds.; 2006)[book reviews], IEEE Transactions on Neural Networks\n20 (3) (2009) 542–542.\n[243] Y. He, J. Shi, C. Wang, H. Huang, J. Liu, G. Li, R. Liu, J. Wang, Semi-\nsupervised skin detection by network with mutual guidance, in: 2019\nIEEE/CVF International Conference on Computer Vision (ICCV), 2020.\n66\n[244] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, C. Raf-\nfel, Mixmatch: A holistic approach to semi-supervised learning, arXiv\npreprint arXiv:1905.02249.\n[245] X. W. a, X. W. a, X. K. a, S. L. b, W. X. a, W. L. a, Fmixcutmatch for\nsemi-supervised deep learning, Neural Networks.\n[246] R. Dupre, J. Fajtl, V. Argyriou, P. Remagnino, Improving dataset volumes\nand model accuracy with semi-supervised iterative self-learning, IEEE\nTransactions on Image Processing 29 (2020) 4337–4348.\n[247] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., Human-level control through deep reinforcement learning, Nature\n518 (7540) (2015) 529.\n[248] A. Jonsson, Deep reinforcement learning in medicine, Kidney Diseases\n5 (1) (2019) 18–22.\n[249] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driess-\nche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot,\net al., Mastering the game of go with deep neural networks and tree search,\nnature 529 (7587) (2016) 484.\n[250] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,\nM. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al., Mastering chess and\nshogi by self-play with a general reinforcement learning algorithm, arXiv\npreprint arXiv:1712.01815.\n[251] F.-C. Ghesu, B. Georgescu, Y. Zheng, S. Grbic, A. Maier, J. Hornegger,\nD. Comaniciu, Multi-scale deep reinforcement learning for real-time 3d-\nlandmark detection in ct scans, IEEE transactions on pattern analysis and\nmachine intelligence 41 (1) (2017) 176–189.\n[252] F. Sahba, H. R. Tizhoosh, M. M. Salama, A reinforcement learning frame-\nwork for medical image segmentation, in: The 2006 IEEE International\nJoint Conference on Neural Network Proceedings, IEEE, 2006, pp. 511–\n517.\n[253] S. M. B. Netto,\nV. R. C. Leite,\nA. C. Silva,\nA. C. de Paiva,\nA. de Almeida Neto, Application on reinforcement learning for diagno-\nsis based on medical image, Reinforcement Learning (2008) 379.\n67\n",
  "categories": [
    "eess.IV",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2020-11-11",
  "updated": "2020-12-06"
}