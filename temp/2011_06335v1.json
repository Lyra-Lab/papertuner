{
  "id": "http://arxiv.org/abs/2011.06335v1",
  "title": "Hierarchical reinforcement learning for efficient exploration and transfer",
  "authors": [
    "Lorenzo Steccanella",
    "Simone Totaro",
    "Damien Allonsius",
    "Anders Jonsson"
  ],
  "abstract": "Sparse-reward domains are challenging for reinforcement learning algorithms\nsince significant exploration is needed before encountering reward for the\nfirst time. Hierarchical reinforcement learning can facilitate exploration by\nreducing the number of decisions necessary before obtaining a reward. In this\npaper, we present a novel hierarchical reinforcement learning framework based\non the compression of an invariant state space that is common to a range of\ntasks. The algorithm introduces subtasks which consist of moving between the\nstate partitions induced by the compression. Results indicate that the\nalgorithm can successfully solve complex sparse-reward domains, and transfer\nknowledge to solve new, previously unseen tasks more quickly.",
  "text": "Hierarchical reinforcement learning for efﬁcient exploration and transfer\nLorenzo Steccanella 1 Simone Totaro 1 Damien Allonsius 1 Anders Jonsson 1\nAbstract\nSparse-reward domains are challenging for rein-\nforcement learning algorithms since signiﬁcant\nexploration is needed before encountering reward\nfor the ﬁrst time.\nHierarchical reinforcement\nlearning can facilitate exploration by reducing the\nnumber of decisions necessary before obtaining a\nreward. In this paper, we present a novel hierarchi-\ncal reinforcement learning framework based on\nthe compression of an invariant state space that is\ncommon to a range of tasks. The algorithm intro-\nduces subtasks which consist in moving between\nthe state partitions induced by the compression.\nResults indicate that the algorithm can success-\nfully solve complex sparse-reward domains, and\ntransfer knowledge to solve new, previously un-\nseen tasks more quickly.\n1. Introduction\nIn reinforcement learning, an agent attempts to maximize\nits cumulative reward through interaction with an unknown\nenvironment. In each round, the agent observes a state, takes\nan action, receives an immediate reward, and transitions\nto a next state. The aim of the agent is to learn a policy,\ni.e. a mapping from states to actions, that maximizes the\nexpected future sum of rewards. To do so, the agent has to\nexplore the environment by taking actions and observing\ntheir effects, and exploit its current knowledge by repeating\naction choices that have been successful in the past.\nAn important challenge in reinforcement learning is solving\ndomains with sparse rewards, i.e. when the immediate re-\nward signal is almost always zero. In this case, all actions\ninitially appear equally good, and it becomes important to\nexplore efﬁciently until the agent ﬁnds a high-reward state.\nOnly then does it become possible to distinguish actions\nthat eventually lead to high reward.\nHierarchical reinforcement learning (HRL) exploits struc-\n1DTIC,\nUniversitat\nPompeu\nFabra,\nBarcelona,\nSpain.\nCorrespondence\nto:\nLorenzo\nSteccanella\n<lorenzo.steccanella@upf.edu>.\nPresented to the 4th Lifelong Learning Workshop at ICML 2020\nture in the environment to decompose complex tasks into\nsimpler subtasks (Dayan & Hinton, 1993; Sutton et al., 1999;\nDietterich, 2000). HRL provides a mechanism for acting\non different timescales by introducing temporally extended\nactions that solve the subtasks, and can help alleviate the\nproblem of exploration in sparse-reward domains since tem-\nporally extended actions reduce the number of decisions\nnecessary to reach high-reward states.\nEarly work on HRL showed that it is important to exploit\nstructure both in time and space, i.e. for the decomposi-\ntion to accelerate learning, the subtasks have to be sig-\nniﬁcantly easier to solve than the original task. This is\nusually made possible by a compressed state space in the\nform of state abstraction (Dietterich, 2000). When the state\nspace is sufﬁciently compressed, state-based methods can\nbe used to solve the subtasks, signiﬁcantly outperforming\nnon-hierarchical methods in many cases. However, even\ncompressed state spaces become large in complex tasks.\nOn the other hand, HRL methods using subgoals to guide\nexploration, either as part of the value function representa-\ntion (Nachum et al., 2018; Schaul et al., 2015; Sutton et al.,\n2017), or as pseudo-reward (Eysenbach et al., 2019; Flo-\nrensa et al., 2017), have shown progress in hard exploration\ntasks, even for high-dimensional state and action spaces.\nThese methods are not as sample efﬁcient, however.\nIn this paper we propose a novel hierarchical reinforce-\nment learning framework that attempts to exploit the best of\nboth worlds. We use a ﬁxed, state-dependent compression\nfunction to deﬁne a hierarchical decomposition of complex,\nsparse-reward tasks. The agent deﬁnes subtasks which con-\nsist in navigating across state-space partitions by jointly\nlearning the policy of each temporally extended action. The\ncompression function makes it possible to use tabular meth-\nods at the top level to effectively explore large state spaces\neven in sparse reward settings. Furthermore we show that\nour method is suitable for transfer learning across tasks that\nare deﬁned by introducing additional learning components.\n2. Background\nIn this section we deﬁne several concepts and associated\nnotation used throughout the paper.\narXiv:2011.06335v1  [cs.LG]  12 Nov 2020\nHierarchical reinforcement learning for exploration and transfer\n2.1. Markov Decision Process\nWe consider a Markov Decision Process (MDP) (Puterman,\n2014) deﬁned by the tuple M = ⟨S, A, r, P⟩, where S is the\nﬁnite state space, A is the ﬁnite action space, r : S×A →R\nis the Markovian reward function, and P : S×A →∆(S) is\nthe transition kernel. Here, ∆(S) is the probability simplex\non S, i.e. the set of all probability distributions over S. At\ntime t, the agent observes state st ∈S, takes an action at ∈\nA, obtains reward rt with expected value E[rt] = r(st, at),\nand transitions to a new state st+1 ∼P(·|st, at). We refer\nto (st, at, rt, st+1) as a transition.\nLet π denote a stochastic policy π : S →∆(A) and η(π) its\nexpected discounted cumulative reward under some initial\ndistribution d0 ∈∆(S) over states:\nη(π) = Es∼d0[V π(s)].\nHere, V π(s) is the value function of policy π in state s,\nV π(s) = E\n\" ∞\nX\nt=1\nγt−1r(st, at)\n\f\f\f\f\f s1 = s\n#\n,\nwhere γ ∈(0, 1] is a discount factor and the expectation is\nover P and π. We also deﬁne the action-value function Qπ\nof π in state-action pair (s, a) as\nQπ(s, a) = E\n\" ∞\nX\nt=1\nγt−1r(st, at)\n\f\f\f\f\f s1 = s, a1 = a\n#\n.\nThe goal of the agent is to ﬁnd a policy that maximizes the\nexpected discounted cumulative reward η(π):\nπ∗= arg max\nπ\nη(π).\n2.2. Options Framework\nGiven an MDP M = ⟨S, A, r, P⟩, an option is a temporally\nextended action o = ⟨Io, πo, βo⟩, where Io ⊆S is an\ninitiation set, πo : S →∆(A) is a policy and βo : S →\n[0, 1] is a termination function (Sutton et al., 1999). Adding\noptions to the action set A of M forms a Semi-Markov\nDecision Process (SMDP), which enables an agent to act\nand reason on multiple timescales. To train the policy πo, it\nis common to deﬁne an option-speciﬁc reward function ro.\nIf an option o is selected in state st ∈Io at time t, the\noption takes actions using policy πo until it reaches a state\nst+k in which the termination condition βo(st+k) triggers.\nAlthough the option takes multiple actions, from the per-\nspective of the SMDP a single decision takes place at\ntime t, and the reward accumulated until time t + k is\nrt + γrt+1 + . . . + γk−1rt+k−1. The theory of MDPs ex-\ntends to SMDPs, e.g. we can deﬁne a policy π : S →∆(O)\nover a set of options O, a value function V π of this policy\nand an action-value function Qπ over state-option pairs.\n3. Algorithm\nIn this section we describe our algorithm for constructing\nan SMDP that can solve a range of different tasks.\n3.1. Task MDPs\nWe assume that each task T is described by an MDP MT =\n⟨Si × ST , Ai ∪AT , rT , Pi ∪PT ⟩. Crucially, the state-\naction space Si × Ai as well as the transition kernel Pi :\nSi × Ai →∆(Si) are invariant, i.e. shared among all\ntasks. On the other hand, the state-action space ST × AT ,\nreward function rT : ST × AT →R and transition kernel\n(Si ∪ST ) × AT →∆(ST ) are task-speciﬁc. We assume\nthat actions in Ai incur zero reward, and that states in Si are\nunaffected by actions in AT . Task T is only coupled to the\ninvariant MDP through the transition kernel PT , since the\neffect of actions in AT depend on the states in Si.\n3.2. Invariant SMDP\nWe further assume that the agent has access to a partition\nZ = {Z1, . . . , Zm} of the invariant state space, i.e. Si =\nZ1 ∪· · ·∪Zm and Zi ∩Zj = ∅for each pair (Zi, Zj) ∈Z2.\nEven though each element of Z is a subset of Si, we often\nuse lower-case letters to denote elements of Z, and we refer\nto each element z ∈Z as a region. We use the partition\nZ to form an SMDP over the invariant part of the state-\naction space. This SMDP is deﬁned as S = ⟨Z, O, PZ⟩,\nwhere Z is the set of regions, O is a set of options, and\nPZ : Z × O →∆(Z) is a transition kernel.\nWe ﬁrst deﬁne the set of neighbors of a region z ∈Z as\nN(z) = {z′ : ∃(s, a, s′) ∈z × Ai × z′, Pi(s′|s, a) > 0}.\nHence neighbors of z can be reached in one step from some\nstate in z. For each neighbor z′ ∈N(z), we deﬁne an option\noz,z′ = ⟨z, πz,z′, βz⟩whose subtask is to reach region z′\nfrom z. Hence the initiation set is z, the termination function\nis βz(s) = 0 if s ∈z and βz(s) = 1 otherwise, and the\npolicy πz,z′ should reach region z′ as quickly as possible.\nThe set of options available to the agent in region z ∈Z\nis Oz = {oz,z′ : z′ ∈N(z)} ⊆O, i.e. all options that\ncan be initiated in z and that transition to a neighbor of z.\nNote that the option sets Oz are disjoint, i.e. each region z\nhas its own set of admissible options. The transition ker-\nnel PZ determines how successful the options are; ideally,\nPZ(z′|z, oz,z′) should be close to 1 for each pair of neigh-\nboring regions (z, z′), but can be smaller to reﬂect that oz,z′\nsometimes ends up in a region different from z′.\n3.3. Option MDPs\nWe do not assume that the policy πz,z′ of each option oz,z′\nis given; rather, the agent has to learn the policy πz,z′ from\nHierarchical reinforcement learning for exploration and transfer\nexperience. For this purpose, we deﬁne an option-speciﬁc\nMDP Mz,z′ = ⟨Sz, Ai, Pz, rz,z′⟩associated with option\noz,z′. Here, the state space Sz = z ∪N(z) consists of all\nstates in the region z, plus all the neighboring regions of z.\nThe set of actions Ai are those of the invariant part of the\nstate-action space. All the states in N(z) are terminal states.\nThe transition kernel Pz is a projection of the invariant tran-\nsition kernel Pi onto the state-action space z × Ai involving\nnon-terminal states, and is deﬁned as\nPz(s′|s, a) =\n\u001a\nPi(s′|s, a),\nif s′ ∈z,\nP\ns′′∈s′ Pi(s′′|s, a)\nif s′ ∈N(z).\nHence the probability of transitioning to a neighbor s′ of z\nis the sum of probabilities of transitioning to any state in s′.\nIn the deﬁnition of Mz,z′, the state-action space Sz × Ai\nand transition kernel Pz are shared among all options in Oz.\nThey only differ in the reward function rz,z′ : z×Ai×Sz →\nR deﬁned on triples (s, a, s′), i.e. it depends on the resulting\nnext state s′. The theory of MDPs easily extends to this\ncase. Speciﬁcally, the reward function rz,z′ is deﬁned as\nrz,z′(s, a, s′) =\n\u001a +0.8,\nif s′ = z′,\n−0.1,\nif s′ ∈N(z) \\ {z′}.\n(1)\nIn other words, successfully terminating in region z′ is\nawarded with a reward of +0.8, while terminating in a\nregion different from z′ is penalized with a reward of −0.1.\nIf the option can’t terminate in a time limit of 100 steps\nthe same negative reward −0.1 is given. In practice, option\noz,z′ can compute the policy πz,z′ indirectly by maintaining\na value function Vz,z′ associated to the option MDP Mz,z′.\n3.4. Algorithm\nIn practice, we do not assume that the agent has access to\nthe invariant SMDP S = ⟨Z, O, PZ⟩. Instead, the agent\ncan only observe the current state s ∈Si, select an action\na ∈Ai, and observe the next state s′ ∼Pi(·|s, a). Rather\nthan observing regions in Z, the agent has oracle access to\na compression function f : Si →N+ from invariant states\nto non-negative integers. Each region z has an associated\ninteger ID N(z) and is implicitly deﬁned as z = {s ∈\nSi : f(s) = N(z)}. To identify regions, the agent has to\nrepeatedly query the function f on observed states and store\nthe integers returned. By abuse of notation we often use z\nto denote both a region in Z and its associated ID.\nOur algorithm iteratively grows an estimate of the invariant\nSMDP S = ⟨Z, O, PZ⟩. Initially, the agent only observes a\nsingle state s ∈Si and associated region z = f(s). Hence\nthe state space Z contains a single region z, whose associ-\nated option set Oz is initially empty. In this case, the only\nalternative available to the agent is to explore. For each\nregion z, we add an exploration option oe\nz = ⟨z, πe\nz, βz⟩to\nthe option set O. This option has the same initiation set and\nz1\nz2\nz3\nz4\nz5\noz1,z3\noz1,z2\noz1,z5\noz5,z2\noz2,z3\noz3,z4\noz4,z5\nFigure 1. Example representation discovered by the algorithm.\ntermination condition as the options in Oz, but the policy\nπe\nz is an exploration policy that selects actions at random or\nimplements a more advanced exploration strategy.\nOnce the agent discovers a neighboring region z′ of z, it\nadds region z′ to the set Z and the associated option oz,z′\nto the option set O. The agent also maintains and updates\na directed graph whose nodes are regions and whose edges\nrepresent the neighbor relation. Hence next time the agent\nvisits region z, one of its available actions is to select option\noz,z′. When option oz,z′ is selected, it chooses actions using\nits policy πz,z′ and simultaneously updates πz,z′ based on\nthe rewards of the option MDP Mz,z′. Figure 1 shows an\nexample representation discovered by the algorithm.\nAlgorithm 1 shows pseudo-code of the algorithm. As ex-\nplained, Z is initialized with the region z of the initial state\ns, and O is initialized with the exploration option oe\nz. In\neach iteration, the algorithm selects an option o which is\napplicable in the current region z. This option then runs\nfrom the current state s until terminating in a state s′ whose\nassociated region z′ is different from z. If this is the ﬁrst\ntime region z′ has been observed, it is added to Z and the ex-\nploration option oe\nz′ is appended to O. If this is the ﬁrst time\nz′ has been reached from z, the option oz,z′ is appended to\nO. The process then repeats from state s′ and region z′.\nThe subroutine GETOPTION that selects an option o in the\ncurrent region z can be implemented in different ways. If the\naim is just to estimate the invariant SMDP S = ⟨Z, O, PZ⟩,\nthe optimal choice of option is that which maximizes the\nchance of discovering new regions or, alternatively, that\nwhich improves the ability of options to successfully solve\ntheir subtasks. If the aim is to solve a task T , the optimal\nchoice of option is that which maximizes the reward of T .\nOn the other hand, the subroutine RUNOPTION executes\nthe policy of the option while simultaneously improving the\nassociated option policy.\n3.5. Properties\nThe proposed algorithm has several advantages. Both the\ninvariant SMDP S and the option MDPs Mz,z′ have much\nHierarchical reinforcement learning for exploration and transfer\nAlgorithm 1 INVARIANTHRL\n1: Input: Action set Ai, oracle compression function f\n2: s ←initial state, z ←f(s)\n3: Z ←{z}, O ←{oe\nz}\n4: while within budget do\n5:\no ←GETOPTION(z, O)\n6:\ns′ ←RUNOPTION(s, o, Ai), z′ ←f(s′)\n7:\nif z′ /∈Z then\n8:\nZ ←Z ∪{z′}\n9:\nO ←O ∪{oe\nz′}\n10:\nend if\n11:\nif oz,z′ /∈O then\n12:\nO ←O ∪{oz,z′}\n13:\nend if\n14:\ns ←s′, z ←z′\n15: end while\nsmaller state-action spaces than Si × Ai, which leads to\nfaster learning. In addition, on the SMDP level, distant\nregions are reached by relatively few decisions, which fa-\ncilitates exploration. Even if the state space Si is high-\ndimensional, the number of regions is relatively small,\nwhich makes it possible to store region-speciﬁc informa-\ntion. Once learned, the estimate of the invariant SMDP S\ncan be reused in many tasks, which facilitates transfer.\nThe main drawback of the algorithm is that the number\nof options grows with the size of the region set Z, each\nrequiring the solution of an additional option MDP.\n3.6. Solving tasks\nRecall that each task T is deﬁned by a task MDP MT =\n⟨Si × ST , Ai ∪AT , rT , Pi ∪PT ⟩.\nGiven an estimate\nS = ⟨Z, O, PZ⟩, we deﬁne an associated task SMDP\nST = ⟨ZT , O ∪OT , rT , PZ ∪P ′\nT ⟩. Here, OT is a set\nof task-speciﬁc options whose purpose is to change the task\nstate in ST , and P ′\nT is the transition kernel corresponding\nto these options. The state space is ZT = Z × ST , i.e. a\nstate (z, s) ∈ZT consists of a region z and a task state s.\nAs before, we do not assume that the agent has access to\noptions in OT . Instead, the agent has to discover from expe-\nrience how to change the task state in ST . For this purpose,\nwe redeﬁne the exploration option oe\nz of each region z so\nthat it has access to actions in AT . When selected in state\n(z, s), oe\nz may terminate for one of two reasons: either the\ncurrent region changes, i.e. the next state is (z′, s) for some\nneighbor z′ of z, or the current task state changes, i.e. the\nnext state is (z, s′) for some task state s′. In the latter case,\nthe agent will add an option os,s′\nz\nto OT which is applicable\nin (z, s) and whose subtask is to reach state (z, s′). Option\nos,s′\nz\nhas an associated option MDP M s,s′\nz\n, analogous to\nMz,z′ except that it assigns positive reward to (z, s′).\nTo solve task T , the agent need to maintain and update\na high-level policy πT : ZT →∆(O ∪OT ) for the task\nSMDP ST . In a state (z, s), policy πT has to decide whether\nto change regions by selecting an option in O, or to change\ntask states by selecting an option in OT . Because of our pre-\nvious assumption on the reward rT , only options in OT will\nincur non-zero reward, which has to be appropriately dis-\ncounted after applying each option. Note that in Algorithm\n1, policy πT plays the role of the subroutine GetOption.\nThe transition kernel P ′\nT measures the ability of task op-\ntions in OT to successfully solve their subtasks. Hence\nP ′\nT ((z, s′)|(z, s), os,s′\nz\n) should be close to 1, but is lower in\ncase option os,s′\nz\nsometimes terminates in the wrong state.\nIn our experiments, however, the agent performs model-free\nlearning and never estimates the transition kernel P ′\nT .\n3.7. Controllability\nAccording to the deﬁnition of the option reward function\nrz,z′ in (1), option oz,z′ is equally rewarded for reaching\nany boundary state between regions z and z′. However, all\nboundary states may not be equally valuable, i.e. from some\nboundary states the options in Oz′ may have a higher chance\nof terminating successfully. To encourage option oz,z′ to\nreach valuable boundary states and thus make the algorithm\nmore robust to the choice of compression function f, we\nadd a reward bonus when the option successfully terminates\nin a state s′ belonging to region z′.\nOne possibility is that the reward bonus depends on the\nvalue of state s′ of options in the set Oz′. However, this\nintroduces a strong coupling between options in the set O:\nthe value function Vz,z′ of option oz,z′ will depend on the\nvalue functions of options in Oz′, which in turn depend on\nthe value functions of options in neighboring regions of z′,\netc. We want to avoid such a strong coupling since learning\nthe option value functions may become as hard as learning\na value function for the original invariant state space Si.\nInstead, we introduce a reward bonus which is a proxy\nfor controllability, by counting the number of successful\napplications of subsequent options after oz,z′ terminates.\nLet M be the number of options that are selected after oz,z′,\nand let N ≤M be the number of such options that terminate\nsuccessfully. We deﬁne a controllability coefﬁcient ρ as\nρ(z) = N\nM .\n(2)\nWe then deﬁne a modiﬁed reward function ¯rz,z′ which\nequals rz,z′ except when oz,z′ terminates successfully,\ni.e. ¯rz,z′(s, a, s′) = rz,z′(s, a, s′) + ρ(z) if s′ ∈z′. In\nexperiments we use a ﬁxed horizon M = 10 after which\nwe consider successful options transitions as not relevant.\nIn practice, the algorithm has to wait for 10 more options\nbefore assigning reward to the last transition of option oz,z′.\nHierarchical reinforcement learning for exploration and transfer\n4. Implementation\nIn this section we describe the implementation of our al-\ngorithm. We distinguish between a manager in charge of\nsolving the task SMDP ST , and workers in charge of solving\nthe option MDPs Mz,z′ (or M s,s′\nz\nfor task options).\n4.1. Manager\nSince the space of regions Z is small, the manager performs\ntabular Q-learning over the task SMDP ST . This procedure\nis shown in Algorithm 2. Similar to Algorithm 1, the task\nstate space ST and option set OT are successively grown as\nthe agent discovers new states and transitions.\nAlgorithm 2 MANAGER\n1: Input: Task action set AT , invariant SMDP S\n2: z ←initial region, s ←initial task state\n3: ST ←{s}, OT ←∅\n4: πT ←initial policy\n5: while within budget do\n6:\no ←GETOPTION(πT , (z, s), O ∪OT )\n7:\n(z′, s′), r ←RUNOPTION((z, s), o, Ai ∪AT )\n8:\nUPDATEPOLICY(πT , (z, s), o, r, (z′, s′))\n9:\nif s′ /∈ST then\n10:\nST ←ST ∪{s′}\n11:\nend if\n12:\nif os,s′\nz\n/∈OT then\n13:\nOT ←OT ∪{os,s′\nz\n}\n14:\nend if\n15:\n(z, s) ←(z′, s′)\n16: end while\n4.2. Worker\nThe worker associated with option oz,z′ ∈O (resp. os,s′\nz\n∈\nOT ) should learn a policy πz,z′ (resp. πs,s′\nz\n) that allows\nthe manager to transition between two abstract states z, z′\n(resp. task states s, s′). We use Self-Imitation Learning\n(SIL) (Oh et al., 2018) which beneﬁts from an exploration\nbonus coming from the self-imitation component of the loss\nfunction. Moreover, since the critic update is off-policy, one\ncan relabel failed transitions in order to speed up learning of\nthe correct option behavior, similar to Hindsight Experience\nReplay (Andrychowicz et al., 2017).\nThe architecture is made of two separate neural networks,\none for the policy πθ\nz,z′, parameterized on θ, and one for\nthe value function V ψ\nz,z′, parameterized on ψ. The agent\nminimizes the loss in (3) via mini batch stochastic gradient\ndescent, with on-policy samples:\nL(θ, ψ) = L(ˆηθ) + αHπ + L( ˆVψ).\n(3)\n(a)\n(b)\nFigure 2. Key-door-treasure-1 (a) and Montezuma’s Revenge (b)\nwith compression function superimposed.\n5. Experiments\nTo evaluate the proposed algorithm we use two benchmark\ndomains: a Key-door-treasure GridWorld, and a simpliﬁed\nversion of Montezuma’s Revenge where the agent only has\nto pick up the key in the ﬁrst room. In both domains, the\ninvariant part of the state consists of the agent’s location,\nand the compression function f imposes a grid structure on\ntop of the location (cf. Figure 2). Results are averaged over\n5 seeds and each experiment is run for 4e5 all the agents\nhave been trained with the choice of hyperparameters in\nFigure 7.\nIn the Key-door-treasure domain we make the reward pro-\ngressively more sparse. In the simplest setting the agent\nobtains reward in each intermediate goal state, while in the\nhardest setting the agent obtains reward only in the terminal\nstate. We also tested the transfer learning ability of our\nalgorithm in new tasks generated by moving the position of\nthe Key, Door and Treasure objects.\nIn Montezuma’s Revenge, we evaluate whether our control-\nlability proxy helps transition between regions. Montezuma\ndoes present an ideal environment to test this since impos-\ning a grid set of regions on it does not respect the structural\nsemantics of the environment and transitioning to the wrong\nstate in another region may cause the agent to fall and die.\nKey-door-treasure is a stochastic variant of the original do-\nmain (Oh et al., 2018) taking random actions with proba-\nbility 20%. The agent has a budget of 300 time steps. We\ndeﬁne two variants and randomly generate multiple tasks\nby changing the location of the Key, Door and Treasure. In\nKey-door-treasure-1 (Oh et al., 2018) the key is in the same\nroom as the door, while in Key-door-treasure-2 the key is in\na different room, making exploration harder.\n5.1. Exploration\nTo investigate the exploration advantage of the proposed\nalgorithm, we compare it against SIL (Oh et al., 2018) and\nagainst a version of SIL augmented with count-based explo-\nration (Strehl & Littman, 2008) that gives an exploration\nHierarchical reinforcement learning for exploration and transfer\n(a) Reward for all objects.\n(b) Reward for treasure only.\nFigure 3. Results in Key-door-treasure-1.\nbonus reward rexp(s, a) = β/\np\nN(s), where N(s) is the\nvisit count of state s and β is a hyperparameter. In the\nﬁgures, our algorithm is labelled HRL-SIL, while SIL and\nSIL-EXP refer to SIL without/with the exploration bonus.\nIn Key-door-treasure-1 (Figure 3) we observe that when the\nreward is given for every object, all the algorithms perform\nwell, while by making the reward more sparse, our algorithm\nclearly outperforms the others, because of its ability to act\non different timescales through the compressed state space\nand the option action space.\nWe further investigate this in Key-door-treasure-2 (Figure 4)\nwhere the key and door are placed in different rooms. This\nmakes exploration harder, and SIL struggles even in the\nsetting with intermediate rewards, only learning to pick up\nthe key, while SIL-EXP slowly learns to open the door and\nget the treasure thanks to the exploration bonus.\n5.2. Transfer Learning\nTo investigate the transfer ability of the algorithm, we train\n’HRL-SIL’ subsequently on a set of tasks and compared to\n’SIL-EXP’. In the ﬁrst task, the goal is just to pick up a\nkey and open a door. Once trained on this task, the agent\nis presented with a more complex task that also involves a\ntreasure. The third task is the same as the second with the\nlocation of the objects mirrored.\nOur agent is evaluated by resetting the manager policy from\ntask to task, while ’SIL-EXP’ is evaluated by clearing the\nExperience Replay buffer between every task. We omit\n’SIL’ since it always performs worse than ’SIL-EXP’. From\nFigure 6 we observe that the learned set of options O and\nset of regions Z transfer well across tasks. In contrast,\n’SIL-EXP’ struggles to solve new tasks. In the ﬁgure, ’NO-\nTRANSFER-HRL-SIL’ and ’NO-TRANSFER-SIL-EXP’\nrefer to the versions that relearn tasks from scratch.\n5.3. Controllability\nLastly we test whether the controllability proxy helps tran-\nsition successfully between regions. We compare two ver-\nsions of our algorithm, one with controllability (’HRL-CO’)\nFigure 4. Results in Key-door-treasure-2, reward for all objects.\nFigure 5. Results in Montezuma’s Revenge with controllability.\nand one without (’HRL’), in the ﬁrst room of Montezuma’s\nRevenge with the task of collecting the key. This envi-\nronment is challenging, since the agent could learn unsafe\ntransitions that lead to successful moves between regions\nbut subsequently dying. As we can see from Figure 5 the\ncontrollability proxy does indeed help in learning success-\nful and safe transitions between regions, outperforming the\nsimpler reward scheme of ’HRL’.\nHyperparameters\nValue\nArchitecture\n-FC(64)\n-FC(64)\nLearning rate\n0.0007\nEnvironments\nKey-door-treasure\nMontezumaRevenge-\n-ramNoFrameskip-v4\nNumber of steps per iteration\n6\nEntropy regularization ( α )\n0.01\nSIL update per iteration ( M )\nSIL : 4, HRL : [1, 4]\nSIL batch size\n512\nSIL loss weight\n1\nSIL value loss weight ( βs il)\n0.01\nReplay buffer size\n104\nExponent for prioritization\n0.6\nBias correction, prioritized replay\n0.4\nManager ϵ-greedy\n[0.05, 0.005]\nCount exploration β\n0.2\nObservation in Key-door-treasure\n(x, y, inventory)\nObservation in Montezuma\n(x, y)\nFigure 7. Hyperparameters used in the experiments.\nHierarchical reinforcement learning for exploration and transfer\n(a) Env 0\n(b) Env 1\n(c) Env 2\n(d) results in Env 0\n(e) results in Env 1\n(f) results in Env 2\nFigure 6. Results of transfer learning, with reward given for all objects.\n6. Related work\nHierarchical reinforcement learning (Dayan & Hinton, 1993;\nSutton et al., 1999; Dietterich, 2000) has a long history.\nOf particular relevance to this work are algorithms that\nautomatically discover goals (Florensa et al., 2017; Bacon\net al., 2017; Levine, 2020). The ability to compress the\nstate space is also critical to our work (Mannor et al., 2004;\nVezhnevets et al., 2017). Design choices of how to use\nthe task compression, and how to distribute the reward,\nidentiﬁes different instances of such methods.\nOur compression function is similar to that of Go-Explore\n(Ecoffet et al., 2019), which also partitions the state space\ninto regions and performs greedy best-ﬁrst search to solve\nMontezuma’s revenge. The main difference is that our al-\ngorithm can learn near-optimal policies for transitioning\nbetween regions, while Go-Explore does not improve on the\nﬁrst action sequence generated randomly.\nOther authors have proposed algorithms for sparse-reward\ndomains that involve a notion of hierarchy. Keramati et al.\n(2018) propose a model-based framework to solve sparse-\nreward domains, and incorporate macro-actions in the form\nof ﬁxed action sequences that can be selected as a single\ndecision. Shang et al. (2019) use variational inference to\nconstruct a world graph similar to our region space. How-\never, unlike our model-free method, the option policies are\ntrained using dynamic programming, which requires knowl-\nedge of the environment dynamics. Eysenbach et al. (2019)\nbuild distance estimates between pairs of states, and use\nthe distance estimate to condition reinforcement learning in\norder to reach speciﬁc goals, which is similar to deﬁning\ntemporally extended actions.\n7. Discussion\nIn spite of the encouraging results in Section 5, the current\nversion of the proposed algorithm has several limitations. In\nthis section we discuss potential future improvements aimed\nat addressing these limitations.\nInvariant state-action space\nThe current version of the\nalgorithm assumes that the agent has prior knowledge of the\ninvariant part of the state-action space, i.e. Si × Ai. In some\napplications, this seems like a reasonable assumption, e.g. in\nenvironments such as MineCraft or DeepMind Lab where\nthe agent has access to a basic set of actions, and is later\nasked to solve speciﬁc tasks. In case prior knowledge of\nSi × Ai is not available, previous work on lifelong learning\nhas shown how to automatically learn a latent state space\nthat is common to a range of tasks (Bou Ammar et al., 2015).\nCompression function\nThe algorithm also assumes that\nthe agent has access to a compression function f which\nmaps invariant states to regions. In case such a function is\nnot available, the agent would need to automatically group\nstates into regions. We believe that the algorithm is rea-\nsonably robust to changes in the compression function, but\nan important feature is that neighboring states should be\ngrouped into the same region. Dilated recurrent neural net-\nworks (Chang et al., 2017) are designed to maintain constant\ninformation during a given time period, similar to the idea\nof remaining in a given region for multiple timesteps, and\nhave been previously applied to hierarchical reinforcement\nlearning (Vezhnevets et al., 2017).\nHierarchical reinforcement learning for exploration and transfer\nOption policies\nAnother limitation of the algorithm is\nthat it needs to learn a large number of policies which scales\nas the number of regions times the number of neighbors.\nIn large-scale experiments it would be necessary to com-\npress the number of policies in some way. Since regions\nare mutually exclusive, in principle one could use a sin-\ngle neural network to represent the policy of |Z| different\noptions. However, in preliminary experiments such a rep-\nresentation suffers from catastrophic forgetting, struggling\nto maintain the optimal policy of a given option while train-\ning the policies of other options. We believe that a more\nintelligent compression scheme would be necessary for the\nalgorithm to scale, potentially sharing a single policy among\na carefully selected subset of options.\n8. Conclusion\nWe presented a hierarchical reinforcement learning algo-\nrithm that decomposes the state space using a compression\nfunction and introduces subtasks that consist in moving\nbetween the resulting partitions. We illustrated that the\nalgorithm can successfully solve relatively complex sparse-\nreward domains. As discussed in Section 7, there are many\nopportunities for extending the work in the future.\nHierarchical reinforcement learning for exploration and transfer\nReferences\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,\nR., Welinder, P., McGrew, B., Tobin, J., Abbeel, O. P., and\nZaremba, W. Hindsight experience replay. In Advances in\nNeural Information Processing Systems, pp. 5048–5058,\n2017.\nBacon, P.-L., Harb, J., and Precup, D. The option-critic\narchitecture. In Proceedings of the 31st AAAI Conference\non Artiﬁcial Intelligence, 2017.\nBou Ammar, H., R, T., and E, E. Safe Policy Search for\nLifelong Reinforcement Learning with Sublinear Regret.\nIn Proceedings of the 32nd International Conference on\nMachine Learning, 2015.\nChang, S., Zhang, Y., Han, W., Yu, M., Guo, X., Tan,\nW., Cui, X., Witbrock, M., Hasegawa-Johnson, M., and\nHuang, T. Dilated Recurrent Neural Networks. In Ad-\nvances in Neural Information Processing Systems, 2017.\nDayan, P. and Hinton, G. E. Feudal reinforcement learning.\nIn Advances in Neural Information Processing Systems,\npp. 271–278, 1993.\nDietterich, T. G. Hierarchical reinforcement learning with\nthe MAXQ value function decomposition. Journal of\nArtiﬁcial Intelligence Research, 13:227–303, 2000.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O.,\nand Clune, J. Go-explore: a new approach for hard-\nexploration problems. arXiv preprint arXiv:1901.10995,\n2019.\nEysenbach, B., Salakhutdinov, R. R., and Levine, S. Search\non the replay buffer: Bridging planning and reinforce-\nment learning. In Advances in Neural Information Pro-\ncessing Systems, pp. 15220–15231, 2019.\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic\ngoal generation for reinforcement learning agents. arXiv\npreprint arXiv:1705.06366, 2017.\nKeramati, R., Whang, J., Cho, P., and Brunskill, E. Fast Ex-\nploration with Simpliﬁed Models and Approximately Op-\ntimistic Planning in Model-Based Reinforcement Learn-\ning. arXiv preprint arXiv:1806.00175, 2018.\nLevine, S. Unsupervised Reinforcement Learning. In Pro-\nceedings of the 19th International Conference on Au-\ntonomous Agents and MultiAgent Systems, pp. 5–6, 2020.\nMannor, S., Menache, I., Hoze, A., and Klein, U. Dynamic\nabstraction in reinforcement learning via clustering. In\nProceedings of the 21st International Conference on Ma-\nchine Learning, pp. 71, 2004.\nNachum, O., Gu, S., Lee, H., and Levine, S. Near-optimal\nrepresentation learning for hierarchical reinforcement\nlearning. arXiv preprint arXiv:1810.01257, 2018.\nOh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learn-\ning. arXiv preprint arXiv:1806.05635, 2018.\nPuterman, M. L.\nMarkov decision processes: discrete\nstochastic dynamic programming. John Wiley & Sons,\n2014.\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. Uni-\nversal value function approximators. In International\nConference on Machine Learning, pp. 1312–1320, 2015.\nShang, W., Trott, A., Sheng, S., Xiong, C., and Socher, R.\nLearning World Graphs to Accelerate Hierarchical Rein-\nforcement Learning. arXiv preprint arXiv:1907.00664,\n2019.\nStrehl, A. L. and Littman, M. L. An analysis of model-\nbased interval estimation for Markov decision processes.\nJournal of Computer and System Sciences, 74(8):1309–\n1331, 2008.\nSutton, R. S., Precup, D., and Singh, S. Between MDPs and\nsemi-MDPs: A framework for temporal abstraction in\nreinforcement learning. Artiﬁcial intelligence, 112(1-2):\n181–211, 1999.\nSutton, R. S., Modayil, J., Degris, M. D. T., Pilarski, P. M.,\nand White, A. Horde: A scalable real-time architecture\nfor learning knowledge from unsupervised sensorimotor\ninteraction. 2017.\nVezhnevets, A. S., Osindero, S., Schaul, T., Heess, N.,\nJaderberg, M., Silver, D., and Kavukcuoglu, K. Feu-\ndal networks for hierarchical reinforcement learning. In\nProceedings of the 34th International Conference on Ma-\nchine Learning, pp. 3540–3549. JMLR. org, 2017.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-11-12",
  "updated": "2020-11-12"
}