{
  "id": "http://arxiv.org/abs/2011.08523v3",
  "title": "Self-supervised Document Clustering Based on BERT with Data Augment",
  "authors": [
    "Haoxiang Shi",
    "Cen Wang"
  ],
  "abstract": "Contrastive learning is a promising approach to unsupervised learning, as it\ninherits the advantages of well-studied deep models without a dedicated and\ncomplex model design. In this paper, based on bidirectional encoder\nrepresentations from transformers, we propose self-supervised contrastive\nlearning (SCL) as well as few-shot contrastive learning (FCL) with unsupervised\ndata augmentation (UDA) for text clustering. SCL outperforms state-of-the-art\nunsupervised clustering approaches for short texts and those for long texts in\nterms of several clustering evaluation measures. FCL achieves performance close\nto supervised learning, and FCL with UDA further improves the performance for\nshort texts.",
  "text": "Self-supervised Document Clustering Based on BERT with Data\nAugmentation\nHaoxiang Shi1, Cen Wang2\nAbstract— Contrastive learning is a promising approach\nto unsupervised learning, as it inherits the advantages of\nwell-studied deep models without a dedicated and complex\nmodel design. In this paper, based on bidirectional encoder\nrepresentations from transformers, we propose self-supervised\ncontrastive learning (SCL) as well as few-shot contrastive\nlearning (FCL) with unsupervised data augmentation (UDA) for\ntext clustering. SCL outperforms state-of-the-art unsupervised\nclustering approaches for short texts and those for long texts in\nterms of several clustering evaluation measures. FCL achieves\nperformance close to supervised learning, and FCL with UDA\nfurther improves the performance for short texts.\nI. INTRODUCTION\nText clustering is the task of grouping a set of unlabeled\ntexts such that texts in the same cluster are more similar\nto each other than to those in other clusters. This task\nis useful for various applications, such as opinion mining,\nautomatic topic labeling [1], language modeling, recommen-\ndation [2], and query expansion to improve retrieval [3].\nUnsupervised learning is a practical approach to clustering,\nas often there are no “gold clusters” in real applications.\nCommon unsupervised approaches are generative: they use\nan encoding network to learn latent representations for input\ntexts and feed the latent representations into another gener-\native network. By minimizing the generation similarity, the\nmost appropriate latent representations can be learned. These\ngenerative approaches usually require complex architectures,\nbut are limited in terms of effectiveness.\nOn the other hand, the recent success of a discriminative\ncontrastive learning (CL) framework in the image classi-\nﬁcation ﬁeld [4] inspired similar approaches in the NLP\nﬁeld (see Section II). CL can help achieve model training\nwith weak labels or totally without labels. Hence, in the\npresent study, we explore CL-based approaches to tackle\nthe text clustering task. Speciﬁcally, based on bidirectional\nencoder representations from transformers (BERT) [5], we\npropose self-supervised contrastive learning (SCL) and few-\nshot contrastive learning (FCL). Our contributions are as\nfollows:\n• We propose multi-language back translation (BT) and\nrandom masking (RM) to generate positive samples for\nSCL.\n1Haoxiang Shi and Tetsuya Sakai are with Graduate School of\nFundamental\nScience\nand\nEngineering,\nWaseda\nUniversity,\n169-\n8050\nTokyo,\nJapan.\nhollis.shi@toki.waseda.jp\nand\ntetsuyasakai@acm.org\n2Cen Wang is with KDDI Research, Inc., 356-8502 Saitama, Japan.\nce-wang@kddi-research.jp\n• We propose FCL with unsupervised data augmentation\n(UDA) [6].\nWe evaluated the aforementioned learning approaches on\ntwo short-text datasets and two long-text datasets, and the\nresults show that SCL achieves state-of-the-art clustering\naccuracy, and FCL achieves performance close to supervised\nlearning. Moreover, FCL with UDA further improves perfor-\nmance for short texts.\nII. RELATED WORK\nA. Contrastive Learning in NLP\nThe recent years have witnessed substantial development\nin studies related to CL. Gunel et al. [7] proposed a su-\npervised CL architecture for multiple language tasks. Fang\net al. [8] proposed contrastive self-supervised encoder rep-\nresentations from transformers to predict whether two aug-\nmented sentences originated from the same sentence. Li et\nal. [9] proposed CL with mutual information maximization\nfor cross-domain sentiment classiﬁcation. Wu et al. [10]\ndesigned a metric that covers both linguistic qualities and\nsemantic informativeness using BERT-based CL. Xiong et\nal. [11] proposed approximate nearest neighbor negative\ncontrastive estimation for dense text retrieval.\nB. Text Clustering\nIn traditional unsupervised clustering, Yuan et al. [12]\nproposed feature clustering hashing (FCH), and Li et al. [13]\nproposed subspace clustering guided convex non-negative\nmatrix factorization to perform text clustering. Subsequently,\nWang et al. [14] proposed Gaussian bidirectional adversarial\ntopic (G-BAT) models to achieve higher accuracy among\nthese methods. Autoencoder (AE) [15], graph autoencoder\n(GAE), and variational graph autoencoder (VGAE) [16] can\nbe used for generative unsupervised clustering. Chiu et al. [1]\nprovided a summary of comparisons of these methods. Semi-\nsupervised deep clustering methods have also been proposed\nbased on deep architectures, such as deep clustering network\n(DCN) [17] and text convolutional Siamese network [18].\nIII. METHODOLOGY\nFigure 1 illustrates the learning framework for FCL and\nSCL. During a mini-batch, (1) m pairs of texts are se-\nlected/generated (We detail the speciﬁc methods of selecting\nand generating in Section III-A), where intra-pair texts are\nused as positive samples and inter-pair texts are treated as\nnegative samples; (2) BERT takes each text pair (with stop\nwords removed) as inputs and transforms the texts into the\nlatent representations. (3) BERT is tuned by a contrastive\narXiv:2011.08523v3  [cs.CL]  17 Sep 2021\nBT/RM\nK-means\nContrastive \nloss \nStrictly selection\nContrastive loss \n(opt. UDA loss)\nFine-tuned \nEncoder\nBERT\nBERT\nTexts after \nremoving \nstop words \n!!!,#\n!!!,$\n!′#\n!′′#\n!! ($ = 1,2, ⋯, *)\n,\"#$%, ,\"#(- = 1,2, ⋯, .)\n,\"#$%, ,\"#(- = 1,2, ⋯, .)\nFig. 1: Learning framework.\nloss, LCL (Equation\n1) which is calculated based on the\nlatent representations. After a learning epoch completes, we\ninput all the texts in a dataset into the tuned BERT and\nobtain the representation ui (i = 1, 2, · · · , N, where N is\nthe number of items in the dataset) for clustering.\nA. Mini-batch Construction\nFor SCL, the entire dataset was used for tuning. m texts\nare ﬁrst randomly selected from the dataset. We do not\nrequire that the texts are from the different classes. For\na selected text xi, i = 1, 2, · · · , m, x′\ni and x′′\ni are two\ntexts generated by BT (i.e., back translation: given a text\nin language A, this text is translated into language B, and\nthen is translated back to language A again) using different\nlanguages, or by RM (i.e., random masking, to randomly\nmask some words in a text). The original text is excluded\nin a mini-batch. Thus, the size of a mini-batch is 2m. For\nFCL, BERT is tuned by m pairs of few-shot labeled items\nin the dataset. The size of a mini-batch is also 2m. Each\npair of (original) texts, xci,i and xci,j, are selected from the\nsame class ci, and different pairs are strictly from n different\nclasses. To make full contrasts of the texts in the different\nclasses, we suggest m ≥n.\nB. Contrastive Loss\nThe contrastive loss is the average of the pair losses. In a\npair loss l(i, j) (Equation 2), where i = 2p −1 and j = 2p\nin the p-th (p = 1, 2, · · · , m) pair, the si,j is the cosine\nsimilarity of vi and vj. The τ(> 0) denotes a temperature\nparameter that can impact the intra-cluster and inter-cluster\ndistance, thereby impacting the clustering accuracy.\nLCL =\n1\n2m\nm\nX\nk=1\n[l(2p −1, 2p) + l(2p, 2p −1)]\n(1)\nl(i, j) = −log\nexp (si,j/τ)\nP2m\nk=1,k̸=i exp (si,k/τ)\n(2)\nC. Unsupervised Data Augmentation\nUDA was originally proposed for emotion analysis [6],\nwhich is a binary classiﬁcation task. When we apply UDA for\nFCL, every text in a dataset D is back translated to construct\nD′. BERT takes a text xi in D and three texts x′\ni,q(q =\n1, 2, 3) in D′ as inputs, and feeds outputs into a UDA model\nwith parameter set θ to obtain the distributions, pθ(y|xi)\nand pθ(y|x′\ni,q). The UDA loss LUDA is the average KL-\ndivergence for each pair of pθ(y|xi) and pθ(y|x′\ni,q) in a mini-\nbatch (Equation 3). The model loss shows in Equation 4.\nLUDA =\nm\nX\ni=1\nKL(pθ(y|xi) ∥pθ(y|x′\ni))\n(3)\nL = LCL + LUDA\n(4)\nIV. EXPERIMENT\nA. Data\nFollowing previous work in text clustering [1], [14], [2],\n[19], we evaluated text clustering methods using four text\ncategorization datasets. In these four datasets, SearchSnip-\npets [20] and Stackoverﬂow [21] have short texts, whereas\n20Newsgroup [22] and Reuters [23] have long texts. The\ndataset statistics are in Appendix A.\nB. Settings\nIn our main experiments (to perform both SCL and FCL),\nwe used the basic and uncased BERT and τ = 0.5 (see\nAppendix B for details). The dimension of a latent repre-\nsentation vi is 20. The mini-batch size is 2m = 160 and\n2m = 40 for the short texts and the long texts, respectively.\nThe learning rate is 2 × 10−5, and the optimizer is Adam.\nIn SCL, we chose Google Translate to perform English-\nSpanish-English and English-French-English BT to generate\npositive pairs. 30% and 15% of words are masked for a short\ntext and a long text in RM, respectively. This is because a\nsmall percentage setting may cause no words to be masked\nin a short text, whereas a large percentage may cause a long\ntext to lose contexts. We chose K-means as the method to\ncluster the texts. For FCL as well as FCL with UDA (FCL +\nUDA), we take 10% labeled items to tune BERT and cluster\nall the texts for each dataset.\nC. Evaluation Measures\nFor SCL, to enable comparisons with previous works [1],\n[2], we evaluated clustering methods using clustering accu-\nracy (ACC) and normalized mutual information (NMI) for\nthe short texts, and ACC and adjusted mutual information\n(AMI) [24] for the long texts. These measures assume that\nthe desired number of clusters n (i.e., the number of gold\nclasses) is given to the system. However, this assumption\nmay not be entirely practical. We therefore additionally\nadopted BCubed F1 Score [25] to evaluate our frameworks\nwhile varying the estimated number of gold classes ˆn. For\nFCL, we choose ACC, AMI, and adjusted random index\n(ARI) for comparisons. The deﬁnitions and usages of the\nmeasures are shown in Appendix C.\nMethod\nSearchSnippets\nStackoverﬂow\nACC\nNMI\nACC\nNMI\nTF\n24.7∗\n9.0∗\n13.5∗\n7.8∗\nTF-IDF\n33.8∗\n21.4∗\n20.3∗\n15.6∗\nSkip-Thought\n33.6∗\n13.8∗\n9.3∗\n2.7∗\nSIF\n53.4∗\n36.9∗\n30.5∗\n28.9∗\nSTC2\n77.0∗\n62.9∗\n51.1∗\n49.0∗\nSIF + Aut.,Self-Train.\n77.1∗\n56.7∗\n59.8∗\n54.8∗\nSCL (BT)\n78.2\n63.6\n77.7\n72.5\nSCL (RM)\n77.2\n63.8\n67.5\n57.4\nTABLE I:\nSCL performance comparisons for short-text\ndatasets: ∗denotes the results reported by [2]; the bold\nresults are the best scores.\nMethod\n20Newsgroup\nReuters\nACC\nAMI\nACC\nAMI\nNMF\n31.9⋆\n45.3⋆\n49.6⋆\n43.8⋆\nTF-IDF\n33.7⋆\n41.7⋆\n35.0⋆\n45.6⋆\nFCH\n33.8†\nLDA\n37.2⋆\n28.8⋆\n54.9⋆\n50.3⋆\nG-BAT\n41.3‡\nGAE\n41.4⋆\n49.3⋆\n52.3⋆\n48.4⋆\nAE\n41.7⋆\n48.7⋆\n53.7⋆\n55.0⋆\nVGAE\n43.1⋆\n48.1⋆\n53.1⋆\n53.3⋆\nDCN\n44.0∗\nSS-SB-MT\n47.4⋆\n53.0⋆\n56.3⋆\n58.4⋆\nSCL (BT)\n50.1\n47.1\n61.7\n45.4\nSCL (RM)\n44.1\n42.2\n64.4\n46.3\nTABLE II:\nSCL performance comparisons for long-text\ndatasets: ⋆, †, ‡, and ∗denote the results reported by [1], [12],\n[14], and [19] respectively; the bold results are the best\nscores.\nD. Baselines\nFor short texts, we compared SCL with SIF + Aut.,Self-\nTrain. (i.e., the state-of-the-art model) and the other methods\nreported by Hadifar et al [2]. For long texts, we primarily\ncompared SCL with SS-SB-MT [1] (i.e., the state-of-the-\nart model) and G-BAT [14]. We also included the other\nmethods such as GAE, AE and VAGE reported by Chiu et\nal [1]. Simple illustrations of these baselines are presented\nin Appendix D.\nWe compared FCL with a supervised method that takes\ntraining data and cross-entropy loss to tune BERT [26].\nExcept for 20Newsgroup which has the training data (which\noccupy approximately 70% of the total data), for the datasets\nwhich are not originally divided into training and test parts,\nwe randomly select 70% of the items in each dataset to tune\nthe models, and the other 30% for clustering evaluation.\nE. Results\nSCL performance. In Table I (results for short texts),\nfor SearchSnippets, SCL (BT) outperforms SIF + Aut.,Self-\nTrain. by 1.1 points in terms of ACC, and SCL (RM) out-\nperforms that by 7.1 points in terms of NMI. In contrast, for\nStackoverﬂow, our results show greater improvements. More\nspeciﬁcally, SCL (BT) outperforms SIF + Aut.,Self-Train.\nMethod\nn\nSearchS.\nStack.\n20N.G.\nReut.\n5\n44.8\n27.2\n25.7\n48.9\nSCL\n10\n46.2\n43.0\n34.3\n47.4\n(BT)\n15\n37.6\n55.9\n36.0\n40.4\n20\n31.0\n64.5\n34.7\n33.4\n5\n49.1\n26.2\n23.2\n53.5\nSCL\n10\n49.3\n38.8\n30.2\n51.5\n(RM)\n15\n40.6\n47.1\n31.5\n45.6\n20\n34.03\n51.3\n30.9\n35.0\nTABLE III: BCubed F1 Scores under SCL for the datasets\nwith unknown n from gold clusters; the bold results are the\nbest scores.\nby 17.9 and 17.7 points in ACC and NMI, respectively. As\nshown in Table II (results for long texts), for 20Newsgroup,\nSCL (BT) outperforms SS-SB-MT and G-BAT by 2.7 points\nand 8.8 points in ACC, respectively; meanwhile, for Reuters,\nboth SCL (BT) and SCL (RM) outperform SS-SB-MT and\nG-BAT in ACC. Furthermore, SCL (RM) achieves the best\nACC. However, SS-SB-MT outperforms SCL in AMI for\n20Newsgroup and Reuters. This may be because SS-SB-MT\nuses topics to build graphs for the texts, and the clustering\nresults are more interpretable. We further compare BCubed\nF1 Score for SCL (BT) and SCL (RM) in Table III under\ndifferent numbers of clusters, ˆn. For SearchSnippets and\nStackoverﬂow, the best scores are obtained when ˆn is close to\nthe n of the gold clusters (8 and 20, respectively). However,\nfor 20Newsgroup and Reuters, we obtained the best scores\nwhen ˆn are 15 and 5, respectively.\nFCL Performance. Comparing FCL (a semi-supervised\nmodel) to SCL and other unsupervised methods may not\nbe fair. Therefore, we compare the FCL results to the\naforementioned supervised method (see Section IV-D). In\nTable IV, FCL underperforms the supervised method by 6.4\npoints and 3.0 points for SearchSnippets and Stackoverﬂow\nin ACC, respectively. When UDA is added in addition (FCL\n+ UDA), we can obtain higher ACC and AMI scores for\nboth datasets. FCL underperforms the supervised method\nby 3.1 points and 17.2 points in ACC for 20Newsgroup\nand Reuters, respectively. However, for FCL + UDA, this\ndifference reduces to 10.4 points for Reuters.\nDiscussion on the fairness. As SCL is an unsupervised\nmethod, we compared it with state-of-the-art unsupervised\nmethods. However, we acknowledge that SCL exploits BT\nand RM as an additional knowledge source. The main\ndisadvantage of SCL is that, in a mini-batch, two positive\npairs may be generated from two texts in the same class.\nHowever, we treat these two pairs as negative samples for\neach other, which limits the performance of SCL. As for\nFCL, we compared it with a supervised method rather than\nother few-shot learning methods. Note that while FCL is\nsemi-supervised, it is different from traditional few-short\nlearning methods. More speciﬁcally, although we use the\ntexts for all classes when constructing a mini-batch, the only\nprior knowledge needed is whether the two texts are similar\nor not. That is, we can transfer an n-clustering problem using\nMethod\nSearchSnippets\nStackoverﬂow\n20Newsgroup\nReuters\nACC\nAMI\nARI\nACC\nAMI\nARI\nACC\nAMI\nARI\nACC\nAMI\nARI\n10%FCL\n89.6\n75.9\n77.4\n86.0\n79.2\n79.1\n66.0\n59.5\n59.5\n71.3\n62.0\n67.9\n10%FCL + UDA\n90.7\n78.1\n79.7\n87.2\n80.6\n80.7\n62.1\n54.6\n54.0\n78.1\n58.5\n58.5\nSupervised (ref.)\n96.0\n88.7\n90.5\n89.0\n81.0\n78.2\n69.1\n58.8\n49.7\n88.5\n70.0\n80.7\nTABLE IV: FCL performance comparisons: the bold results are the best scores.\nn classes of labels to one using binary discriminative labels.\nV. CONCLUSION\nIn this paper, we proposed SCL and FCL with UDA for\ntext clustering. We tuned BERT by our learning methods, and\nused the learned latent representations to perform clustering.\nIn SCL, we introduced two data augmentation methods, back\ntranslation and random masking. Our experimental results\nshow that SCL achieves the best ACC for all the datasets.\nIn particular, the SCL outperforms the state-of-the-art in\nterms of both ACC and NMI for short-text clustering. To\nthe best of our knowledge, we are the ﬁrst to apply CL in\nan unsupervised manner to perform NLP tasks. The results\nalso show that FCL can obtain performance very close to a\nsupervised method, and FCL with UDA appears to further\nimprove the performance for short texts.\nAPPENDIX\nA. Dataset Statistics\nThe dataset statistics are shown in Table V, where N is\nthe total number of texts in a dataset, T is the total number\nof tokens, LAvg. is the average length of the texts, and n\nis the number of the gold clusters. As for Reuters, due to\nthe imbalance in the amount of texts in different classes, we\nchoose the 10 largest classes.\nDataset\nN\nT\nLAvg.\nn\nSearchSnippets\n12.3k\n31k\n17.9\n8\nStackoverﬂow\n20k\n23k\n8.3\n20\n20Newsgroup\n1.8k\n56k\n245\n20\nReuters\n7.6k\n28k\n141\n10\nTABLE V: Datasets statistics.\nB. Tuning\nTuning batch size. The batch size in our experiment was\nsmall (i.e., 2m = 160 for short texts and 2m = 40 for\nlong texts) due to resource (i.e., GPUs) constraints, however,\naccording to [4] who uses TPUs, we think we can improve\nperformance given a larger batch size.\nTuning τ. To examine the effect of the choice of the\ntemperature parameter τ on clustering accuracy, we report\non the results of an additional experiment using different τ\nvalues. Note that the purpose of this experiment is to inves-\ntigate the best possible performance; as we are exploiting\ngold clusters to compute cluster accuracy, tuning τs in this\nmanner is not practical. From the clustering accuracies for\nthe 20Newsgroup under τ variations shown in Table VI, it\ncan be seen that both 10% FCL with UDA and SCL with\nBT can achieve the best performance when τ is 0.5, and the\naccuracy decreases neither when τ < 0.5 or when τ > 0.5.\nTherefore, we suggest setting τ = 0.5 as the default for the\nclustering tasks.\nMethod\nτ =\n0.25\n0.5\n0.75\n1\nSCL (BT)\n45.3\n50.1\n49.6\n44.6\n10%FCL + UDA\n61.0\n62.1\n56.4\n50.5\nTABLE VI:\nClustering ACC for 20Newsgroup under τ\nvariations.\nC. Evaluation Measures for Comparisons\nACC. The ACC requires the mapping between the pre-\ndicted clusters and the gold clusters, which is deﬁned as\nEquation 5:\nACC =\nPn\ni=1 δ(cti, map(ˆcti))\nN\n(5)\nwhere n is the number of the clusters, and N is the total\nnumber of texts. cti is the ground truth cluster of text i,\nand ˆcti is the predicted cluster of text i. If cti = map(ˆcti),\nthen δ(cti, map(ˆcti)) = 1. The function map(·) indicates a\npermutation mapping that best matches the predicted clusters\nto the ground truth classes.\nThe method to calculate clustering accuracy follows Co-\nClust [27]. First, we construct a L × L matrix, where L =\nmax (|C|, | ˆC|), and C and ˆC are the ground true partition\n(i.e., the gold clusters) and the predicted partition of a\ndataset. This measure assumes a previously known |C| = n,\nthus | ˆC| = |C| = n. Then, we count the number of repeated\ntexts in Ci and ˆCj as L(i, j), where Ci or ˆCj is the i-th\nclass or j-th cluster of C or ˆC, respectively. L now can\nbe seen as an adjacent matrix of a bi-graph, and we can\napplies the method such as the Hungarian algorithm to ﬁnd\nthe maximum perfect match of this bi-graph. Thus, we can\nget the best mapping between C and ˆC. We can calculate\nACC and other metrics based on this mapping.\nNMI. Denote MI(C, ˆC) as the mutual information:\nMI(C, ˆC) =\n|C|\nX\ni=1\n| ˆ\nC|\nX\nj=1\nPC ˆ\nC(i, j) log\nPC ˆ\nC(i, j)\nPC(i)P ˆ\nC(j)\n(6)\nwhere PC(i) and P ˆ\nC(j) denote the probabilities of i in C\nand j in ˆC, respectively. PC ˆ\nC(i, j) then denotes the joint\nprobability of i in C and j in ˆC.\nThen NMI is deﬁned as:\nNMI =\nMI(C, ˆC)\nH(C) + H( ˆC)\n(7)\nwhere H(C) and H( ˆC) are the entropy of C and\nˆC,\nrespectively.\nAMI. To further consider the randomness of ˆC, AMI\nintroduce the expected MI between C and ˆC to adjust the\nchance:\nE{MI(C, ˆC)} =\n|C|\nX\ni=1\n| ˆ\nC|\nX\nj=1\nX\nnij=(ai+bj−N)+\nnij\nN log (N · nij\naibj\n)×\nai!bj!(N −ai)!(N −bj)!\nN!nij!(ai −nij)!(bj −nij)!(N −ai −bj + nij)!\n(8)\nwhere nij = |Ci ∩ˆCj|, ai = P| ˆ\nC|\nj=1 nij, bj = P|C|\ni=1 nij, and\n(ai + bj −N)+ denotes max(1, ai + bj −N). The adjusted\nmeasure for the mutual information is deﬁned to be:\nAMI(C, ˆC) =\nMI(C, ˆC) −E{MI(C, ˆC)}\nmax{H(C), H( ˆC)} −E{MI(C, ˆC)}\n(9)\nAMI takes a value of 1 when the two partitions are identical\nand 0 when the MI between two partitions equals the value\nexpected due to chance alone.\nARI. ARI is the corrected-for-chance version of the Rand\nIndex, which can also measure the similarity between C and\nˆC when there are no labels. ARI is deﬁned as:\nARI =\nP\ni\nP\nj\n\u0000nij\n2\n\u0001\n−\nP\ni\n\u0000ai\n2\n\u0001 P\nj\n\u0000bj\n2\n\u0001\n\u0000N\n2\n\u0001\n1\n2\nhP\ni\n\u0000ai\n2\n\u0001\n+ P\nj\n\u0000bj\n2\n\u0001i\n−\nP\ni\n\u0000ai\n2\n\u0001 P\nj\n\u0000bj\n2\n\u0001\n\u0000N\n2\n\u0001\n(10)\nwhere\n\u0000x\ny\n\u0001\ndenotes the number of combinations when select-\ning y items from x. nij, ai and bj have the same meanings\nwith those in Equation 8.\nBCubed F1 Score. The precision of an item i in the\ndataset is like:\np(i) =\nN ˆ\nC(i)∩C(i)\nN ˆ\nC(i)\n(11)\nwhere ˆC(i) is the predicted cluster that i belongs to, and\nC(i) is the gold cluster that i belongs to. N ˆ\nC(i)∩C(i) is the\nnumber of items both in ˆC(i) and in C(i). N ˆ\nC(i) is the\nnumber of items in ˆC(i).\nThen, the recall of i is deﬁned as:\nr(i) =\nN ˆ\nC(i)∩C(i)\nNC(i)\n(12)\nwhere C(i) is the number of items in C(i).\nThen, the score for i is like:\nf(i) = (1 + β2)\np(i) · r(i)\nβ2p(i) + r(i)\n(13)\nFinally, the Bcubed Precision P, Recall R and Fβ Score F\nfor a dataset with N items are as follows:\nP = 1\nN\nX\ni\np(i)\n(14)\nR = 1\nN\nX\ni\nr(i)\n(15)\nF = 1\nN\nX\ni\nf(i)\n(16)\nD. Baselines\nSIF + Aut.,Self-Train. The model includes three steps: (1)\nshort texts are embedded using Smooth Inverse Frequency\n(SIF) embeddings; (2) during a pre-training phase, a deep\nautoencoder is applied to encode and reconstruct the short-\ntext SIF embeddings; (3) in a self-training phase, we use\nsoft cluster assignments as an auxiliary target distribution,\nand jointly ﬁne-tune the encoder weights and the clustering\nassignments.\nG-BAT. The proposed bidirectional adversarial training\n(BAT) consists of three components: (1) the Encoder E\ntakes the V -dimensional document representation dr sam-\npled from text corpus C as input and transforms it into the\ncorresponding K-dimensional topic distribution θr; (2) the\nGenerator G takes a random topic distribution θf drawn from\na Dirichlet prior as input and generates a V -dimensional\nfake word distribution df ; (3) the Discriminator D takes the\nreal distribution pair pr = [θr; dr] and fake distribution pair\npf = [θf; df] as input and discriminates the real distribution\npairs from the fake ones. The outputs of the discriminator\nare used as supervision signals to learn E, G and D during\nadversarial training.\nIn BAT, the generator models topics based on the bag-\nof-words assumption as in most other neural topic models.\nTo incorporate the word relatedness information captured in\nword embeddings into the inference process, we modify the\ngenerator of BAT and propose Gaussian-BAT, in which G\nmodels each topic with a multivariate Gaussian.\nSS-SB-MT. This method builds a keyword correlation\ngraph (KCG) for a text using node features (embeddings\nfrom a SBERT), word co-occurrence edges, sentence similar-\nity edges and sentence position edges. Then the constructed\ngraphs of the texts are fed into a Multi-Task GAE (MTGAE).\nClustering is based on the latent representations from MT-\nGAE. The name SS-SB-MT comes from Sentence Similarity,\nSBERT and MTGAE.\nACKNOWLEDGMENT\nWe sincerely thank Dr. Zhaohao Zeng for the paper\nreviewing and Miss. Jing Shen for the provision of the GPU.\nREFERENCES\n[1] Billy Chiu, Sunil Kumar Sahu, Derek Thomas, Neha Sengupta and\nMohammady Mahdy, “Autoencoding keyword correlation graph for\ndocument clustering,” in Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 2020, pp. 3974–3981.\n[2] Amir Hadifar, Lucas Sterckx, Thomas Demeester and Chris Develder,\n“A self-training approach for short text clustering,” in Proceedings of\nthe 4th Workshop on Representation Learning for NLP (RepL4NLP-\n2019), 2019, pp. 194–199.\n[3] Charu C. Aggarwal and ChengXiang Zhai, “A survey of text clustering\nalgorithms,” In Mining text data”, Springer, 2012, pp. 77–128.\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi and Geoffrey\nHinton, “A simple framework for contrastive learning of visual repre-\nsentations,” arXiv preprint, 2020, arXiv:2002.05709.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova,\n“Bert: Pre-training of deep bidirectional transformers for language\nunderstanding,” arXiv preprint, 2018, arXiv:1810.04805.\n[6] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong and Quoc V\nLe, “Unsupervised data augmentation for consistency training,” arXiv\npreprint, 2019, arXiv:1904.12848.\n[7] Beliz Gunel, Jingfei Du, Alexis Conneau and Ves Stoyanov, “Super-\nvised contrastive learning for pre-trained language model ﬁne-tuning,”\narXiv preprint, 2020, arXiv:2011.01403.\n[8] Hongchao Fang and Pengtao Xie, “Cert: Contrastive self-supervised\nlearning\nfor\nlanguage\nunderstanding,”\narXiv\npreprint,\n2020,\narXiv:2005.12766.\n[9] Tian Li, Xiang Chen, Shanghang Zhang, Zhen Dong and Kurt\nKeutzer, “Cross-domain sentiment classiﬁcation with contrastive learn-\ning and mutual information maximization,” arXiv preprint, 2020,\narXiv:2010.16088.\n[10] Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa and Shouling\nJi, “Unsupervised 485 reference-free summary quality evaluation via\ncontrastive learning,” arXiv preprint, 2020, arXiv:2010.01781.\n[11] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul\nBennett, Junaid Ahmed and Arnold Overwijk, “Approximate nearest\nneighbor negative contrastive learning for dense text retrieval,” arXiv\npreprint, 2020, arXiv:2007.00808.\n[12] Tongtong Yuan, Weihong Deng, Jiani Hu, Zhanfu An and Yinan\nTang, “Unsupervised adaptive hashing based on feature clustering,”\nNeurocomputing, vol. 323, 2019, pp. 373–382.\n[13] Xiaocui Li, Hongzhi Yin, Ke Zhou and Xiaofang Zhou, “Semi-\nsupervised clustering with deep metric learning and graph embedding,”\nWorld Wide Web, vol. 23, no. 2, 2020, pp. 781–798.\n[14] Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong,\nChenchen Ye and Haiyang Xu, “Neural topic modeling with bidirec-\ntional adversarial training,” arXiv preprint, 2020, arXiv:2004.12331.\n[15] Geoffrey E Hinton and Ruslan R Salakhutdinov, “Reducing the dimen-\nsionality of data with neural networks,” Science, vol. 313, no. 5786,\n2019, pp. 504–507.\n[16] Thomas N Kipf and Max Welling, “Variational graph auto-encoders,”\narXiv preprint, 2016, arXiv:1611.07308.\n[17] Ankita Shukla, Gullal S Cheema and Saket Anand, “Semi-supervised\nclustering with neural networks,” in 2020 IEEE Sixth International\nConference on Multimedia Big Data (BigMM), 2020, IEEE, pp.\n152–465.\n[18] Lucas Akayama Vilhagra, Eraldo Rezende Fernandes and Bruno\nMagalhaes Nogueira, “Textcsn: a semi-supervised approach for text\nclustering using pairwise constraints and convolutional siamese net-\nwork,” in Proceedings of the 35th Annual ACM Symposium on\nApplied Computing, 2020, pp. 1135–1142.\n[19] Bo Yang, Xiao Fu, Nicholas D Sidiropoulos and Mingyi Hong,\n“Towards k-means-friendly spaces: simultaneous deep learning and\nclustering,” in International Conference on Machine Learning, 2017,\nPMLR, pp. 3861–3870.\n[20] Xuan Hieu Phan, Minh Le Nguyen and Susumu Horiguchi, “Learn-\ning to classify short and sparse text web with hidden topics from\nlarge-scale data collections,” in Proceedings of the 17th International\nConference on World Wide Web, Beijing, China, April 21-25, 2008.\n[21] Jiaming Xu, Bo Xu, Peng Wang, Suncong Zheng, Guanhua Tian and\nJun Zhao, “Self-taught con- 497 volutional neural networks for short\ntext clustering,” Neural Networks, vol. 88, 2017, pp. 22–31.\n[22] Ken Lang, “Newsweeder: Learning to ﬁlter netnews,” in Machine\nLearning Proceedings, 1995, Elsevier, pp. 331–339.\n[23] David D. Lewis, Yiming Yang, Tony G. Rose and Fan Li, “Rcv1: A\nnew benchmark collection for text categorization research,” Journal of\nMachine Learning Research, 2004, vol. 5, no. 2, pp. 361–397.\n[24] Nguyen Xuan Vinh, Julien Epps and James Bailey, “Information\ntheoretic measures for clusterings comparison: Variants, properties,\nnormalization and correction for chance,” the Journal of Machine\nLearning Research, vol. 11, 2010, pp. 2837–2854.\n[25] Yanan Qian, Qinghua Zheng, Tetsuya Sakai, Junting Ye and Jun Liu,\n“Dynamic author name disambiguation for growing digital libraries,”\nInformation Retrieval Journal, vol. 18, no. 5, 2015, pp. 379–412.\n[26] Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah, “Semi-supervised\nclustering for short text via deep representation learning,” arXiv\npreprint, 2016, arXiv:1602.06797.\n[27] Francois Role, Stanislas Morbieu and Mohamed Nadif, “Coclust: a\npython package for co-clustering,” Journal of Statistical Software, vol.\n88, no. 7, 2018, pp. 1–29.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2020-11-17",
  "updated": "2021-09-17"
}