{
  "id": "http://arxiv.org/abs/2011.08743v1",
  "title": "Curiosity Based Reinforcement Learning on Robot Manufacturing Cell",
  "authors": [
    "Mohammed Sharafath Abdul Hameed",
    "Md Muzahid Khan",
    "Andreas Schwung"
  ],
  "abstract": "This paper introduces a novel combination of scheduling control on a flexible\nrobot manufacturing cell with curiosity based reinforcement learning.\nReinforcement learning has proved to be highly successful in solving tasks like\nrobotics and scheduling. But this requires hand tuning of rewards in problem\ndomains like robotics and scheduling even where the solution is not obvious. To\nthis end, we apply a curiosity based reinforcement learning, using intrinsic\nmotivation as a form of reward, on a flexible robot manufacturing cell to\nalleviate this problem. Further, the learning agents are embedded into the\ntransportation robots to enable a generalized learning solution that can be\napplied to a variety of environments. In the first approach, the curiosity\nbased reinforcement learning is applied to a simple structured robot\nmanufacturing cell. And in the second approach, the same algorithm is applied\nto a graph structured robot manufacturing cell. Results from the experiments\nshow that the agents are able to solve both the environments with the ability\nto transfer the curiosity module directly from one environment to another. We\nconclude that curiosity based learning on scheduling tasks provide a viable\nalternative to the reward shaped reinforcement learning traditionally used.",
  "text": "Curiosity Based Reinforcement Learning on Robot\nManufacturing Cell\nMohammed Sharafath Abdul Hameed, Md Muzahid Khan, Andreas Schwung\nDepartment of Automation Technology\nSouth Westphalia University of Applied Sciences\nSoest, Germany.\nsharafath.mohammed, khan.mdmuzahid, schwung.andreas@fh-swf.de\nAbstract—This paper introduces a novel combination of\nscheduling control on a ﬂexible robot manufacturing cell with\ncuriosity based reinforcement learning. Reinforcement learning\nhas proved to be highly successful in solving tasks like robotics\nand scheduling. But this requires hand tuning of rewards in\nproblem domains like robotics and scheduling even where the\nsolution is not obvious. To this end, we apply a curiosity based\nreinforcement learning, using intrinsic motivation as a form of\nreward, on a ﬂexible robot manufacturing cell to alleviate this\nproblem. Further, the learning agents are embedded into the\ntransportation robots to enable a generalized learning solution\nthat can be applied to a variety of environments. In the ﬁrst\napproach, the curiosity based reinforcement learning is applied to\na simple structured robot manufacturing cell. And in the second\napproach, the same algorithm is applied to a graph structured\nrobot manufacturing cell. Results from the experiments show\nthat the agents are able to solve both the environments with the\nability to transfer the curiosity module directly from one envi-\nronment to another. We conclude that curiosity based learning\non scheduling tasks provide a viable alternative to the reward\nshaped reinforcement learning traditionally used.\nIndex Terms—reinforcement learning, manufacturing cell, cu-\nriosity based learning, planning robots\nI. INTRODUCTION\nReinforcement Learning (RL) is becoming a popular al-\ngorithm to solve complex games and tasks like Atari [1],\nphysics simulation [2], Go [3] and Chess [4], Robotics [5],\nand optimization problems [6] etc. In these approaches, the RL\nagent is given a continuous supply of dense extrinsic reward\nby the environment for the action it takes. While providing\na reward or deﬁning a reward function is not a problem in\ncommon gaming tasks, it quickly becomes cumbersome in\ncomplex engineering tasks like the optimization problem [7].\nAnd traditionally when the rewards are not directly available,\nthey need to be shaped to guide the agent in the direction of an\noptimal solution. This approach creates three problems: one,\nthis introduces a huge inductive bias in the reward scheme that\nit cripples the generalized learning of the RL agent from the\nstart. So much so that it might be only able to solve in the\nknown solution space; two, reward shaping is a notoriously\ndifﬁcult engineering problem, where un-optimized rewards\ncould break the learning process or provide only sub-optimal\nsolutions; and ﬁnally, it creates solutions which cannot be\ntransferred to new environments, since the rewards have been\nheavily tuned for a particular environment. In this paper, we\npropose to apply curiosity based learning to a ﬂexible Robot\nManufacturing Cell.\nIn [8], a Robot Manufacturing Cell (RMC) was proposed\nas an optimization problem similar to the scheduling problems\nencountered in the production planning. And in [9], the same\nRMC was solved using central and distributed advantage actor\ncritic (A2C), through hand engineered reward signals. Similar\nreward shaping methods are also followed in [10] [11] [12],\nwhere rewards for each of the scheduling problem is hand en-\ngineered to suit the environment they work in. This obviously\ncreates a problem of non-generalized solutions, where the\nlearning of an agent from one environment is not transferable\nto an another, even though they both solve similar optimization\nproblems.\nTo overcome this limitation of the reinforcement learning\napplication in engineering problems, in this paper we apply\ncuriosity [13] to the RMC environment. Curiosity uses the\nintrinsic motivation of the agent to create rewards based on the\nsurprise factor of the agents familiarity with the environment\nstates. Through the application of curiosity in reinforcement\nlearning of the RMC we overcome all the problems stated\nabove. The main contributions of this paper as follows:\n• We propose curiosity as an effective tool to eliminate\nthe inductive bias induced by the reward shaping nor-\nmally used in reinforcement learning of scheduling tasks.\nFurther, curiosity eliminates complex reward shaping for\nengineering tasks like scheduling.\n• We show that a direct transfer of the reward function from\none manufacturing environment to another is possible\nwithout any tuning requirements.\n• We combine curiosity with gradient monitoring, an effec-\ntive approach to stabilize training. We apply the curiosity-\nbased RL to two multi-robot environments where we\nparticularly show the improvements achieved by the\nproposed approach in both environments.\nThe paper is organized as follows. Section II introduces the\nlearning problem, RMC, in detail, describing the operations\nand setup. Section III introduces the RL algorithms used in\nthis paper along with further optimizations that were used,\nwhile Section IV, Section IV-A, and Section IV-B explain\narXiv:2011.08743v1  [cs.RO]  17 Nov 2020\nthe curiosity module along with other optimizations used.\nSection V describes the results that were achieved on the RMC\nthrough the application of the RL along with its optimizations,\nwhile Section VI provides the conclusion with future scope of\nwork.\nII. ROBOT MANUFACTURING CELL\nIn this paper, RL model is applied to a cooperative self-\nlearning RMC [8]. The RMC consists of two industrial robots,\nan Adept Cobra i600 SCARA-robot and an ABB IRB1400 6-\nDOF-robot, both of which share a common work platform\nas shown in Figure 1. The setup has six stations totally, two\ninput stations (IB1, IB2), three processing or work stations\n(M1, M2, M3) and one output station (OB). There are two\ndifferent types of work-piece in this system (WP-1, WP-2).\nThe WP-1 goes through IB1, M1, M2, M3, and then OB,\nwhile WP-2 goes through IB2, M2, M1, M3 and then OB.\nBoth the robots have their own proprietary control software.\nHence a supervisory Siemens Programmable Logic Control\n(PLC) system is developed to control and coordinate the robot\nmovement. Two experiments are conducted to demonstrate the\napplicability of the curiosity based approach on scheduling\nproblems. The ﬁrst experiment is conducted on a simple\nstructured Robot Manufacturing Cell (sRMC) environment,\nsimilar to [9] and the second experiment is conducted on\na graph-network based Robot Manufacturing Cell (gRMC)\nenvironment, which was developed in [14]. gRMC is a scalable\nvariant of the sRMC environment, where the machines and\nthe buffers are represented by the nodes of a graph while the\nconnections between them are expressed as edges. A work-\npiece is transported between the nodes when the edge is\nactivated.\nFigure 1. Schematic drawing of the robot manufacturing cell\nIn sRMC no buffers are considered for the processing\nstations environment, i.e., there is no queue in front or after\nthe processing stations. The RL agent’s actions here are\nthe movements of the work-pieces between the processing\nstations. While in gRMC three buffer-stations are considered,\none after each of the processing stations (MB1, MB2, MB3) as\nshown in Figure 2. Here the agent’s action is the activation of\nthe edges connecting the nodes. If a node is activated then the\nwork-piece is moved from the ’from’-node to the ’to’-node.\nTwo different types of edges are deﬁned in the system, to move\nthe different work-pieces. Both the work-pieces are embedded\nwith the RFID chip which gives the required information to\nthe robots.\nFigure 2. In the gRMC environment, the nodes are indexed from 0-8 and are\ncolored according to their function, while the different colored lines represent\nthe two work-pieces to be processed and transported.\nIII. REINFORCEMENT LEARNING\nRL is a ﬁeld of machine learning where an agent is trained to\ntake a sequence of decisions on an environment, modeled as a\nMarkov Decision Process (MDP), to maximize the cumulative\nreward signal. The RL system has two major components\nnamely, the agent and the environment. The agent interacts\nwith the environment, takes an action at based on the agent’s\npolicy π(at|st) from its current state st and moves to a\nnew state st+1. Based on the action taken by the agent, the\nenvironment sends a reward signal rt to the agent. The goal\nof the agent is to ﬁnd an optimal policy that maximizes the\ncumulative reward [15]. Hence, the agent tries to maximize\nthe expected return E[Rt|π] with the discounted reward given\nby R = P\nt γtrt and discount factor 0 ≤γ ≤1. The\nMDP is deﬁned by the tuple (S, A, P, R, p0), with the set\nof states S, the ﬁnite set of actions A, a transition model\nP for a given state s and an action a, the reward function\nR : S × A × S →R, which provides a reward r for each state\ntransition st →st+1 with an initialization probability p0.\nThe policy is deﬁned using either value based methods\nor policy gradient methods. Value based methods use either\na state-value function, v(s) = E[Rt|St = s] or a state-\naction-value function, q(s, a) = E[Rt|St = s, At = a]. The\npolicy is then deﬁned by an ϵ−greedy strategy where greedy\nactions are given by π(a|s) = argmax(q(s, A)), where the\nagent exploits, or a non-greedy action is given, where the\nagent explores. In policy gradient methods, a parameterized\npolicy, π(a|s, θ), is used to take action. The policy can still\nbe improved using value functions as seen in [1]. The the\nagent’s objective is to derive an optimal policy, π∗(a|s), that\nmaximizes the reward collection. Correspondingly, the policy\nis updated in a way to maximize a cost function J(θt) given\nby,\nθt+1 = θt + ρ∇J(θt),\n(1)\nwhere θ is the policy parameters of π and ρ is the learning\nrate.\nA. Advantage Actor Critic (A2C):\nAdvantage Actor Critic is a policy gradient algorithm which\ncombines policy gradient method with value based approach\nlike SARSA [15]. In A2C, the actor tries to ﬁnd the optimal\npolicy and the critic evaluates the actor’s policy. Here, the\nactor refers to the policy π(a|s, θ1) and the critic refers to the\nvalue function ν(s, θ2), where θ1 is the parameter of the actor\nand the θ2 is the parameter of the critic. The cost function\nequation of the A2C is given by Eqn. 2 and 3 respectively.\nJ(θ) = E∼πθ\n\u0014 X\n(st,at)ϵ\nlogπθ(at, st)Aπθ\n\u0015\n(2)\nAπθ(st, at) = Qπθ(st, at) −Vπθ(st)\n(3)\nIV. CURIOSITY DRIVEN LEARNING\nChildren often learn through intrinsic motivation such as\ncuriosity. Psychologists deﬁne a behavior as intrinsically mo-\ntivated when a human works for his own sake. Similarly,\nintrinsic motivation in RL keeps an agent engaged in the\nexploration of new knowledge and skills when the rewards\nare rare [16]. There are lots of approaches to develop the\nintrinsic reward of an agent like prediction error, prediction\nuncertainty, or development of the forward model trained along\nwith the policy of an agent [17]. These approaches drive the\nagent to explore the environment in the absence of a dense\nextrinsic reward. In this paper, we use prediction error as\nthe intrinsic reward, which is produced by the agent itself\nwith a model called Intrinsic Curiosity Model (ICM). ICM\nis composed of two subsystems: an inverse dynamic model\nthat produce an intrinsic reward signal and forward dynamic\nmodel that outputs a sequence of actions which guides the\nagent to maximize the reward signal [13]. Figure 3 represents\nthe schematic architecture of the ICM. The ICM encodes\nthe current state st and the next state st+1 into the feature\nof current state φ(st) and feature of next state φ(st + 1).\nThe feature of current state φ(st) and the feature of next\nstate φ(st + 1) are trained with inverse dynamic model that\npredicts action ˆat [13]. Mathematically, this transition can be\nrepresented as,\nˆat = g(st, st+1; θI)\n(4)\nwhere g is a soft-max distribution function of all possible\nactions that could be taken and θI is the neural network\nparameter. The loss function of the inverse dynamic model\ncan represent by the following,\nmin\nθI\n= LI( ˆat, at)\n(5)\nwhere LI the loss function that calculates the deviation\nbetween the actual action at and the predicted action ˆat.\nThe forward dynamic model takes φ(st) and action at as\nthe inputs and outputs the predicted feature representation\nˆφ(at+1) of next state st+1. The forward dynamic model can\nbe represented by the following,\nˆφ(st+1) = f(φ(st), at; θF )\n(6)\nwhere f is the learned function and θF is the ICM neural\nnetwork parameter. The loss function of the forward model is\ngiven below:\nLF (φ(st), ˆφ(st+1) = 1\n2||φ(st+1) −φ(st)||2\n2\n(7)\nwhere LF is the loss function that calculates the variance\nbetween the predicted feature representation of next state\nˆφ(at+1) and the actual predicted feature of next state φ(st+1).\nSo, the intrinsic reward ri\nt can be calculated as:\nri\nt = η\n2||φ(st+1) −φ(st)||2\n2\n(8)\nwhere η > 0 is the scaling factor of the intrinsic reward\nsignal. The intrinsic reward is generated by the agent at time\nt. Besides intrinsic rewards, the agent may also get extrinsic\nreward re\nt from the environment.\nTo generate the intrinsic reward signal and to optimize the\nICM, the inverse dynamic model and forward dynamic model\nloss should be combined. So, the overall optimization problem\nof ICM that helps the agent to learn can be represented as:\nmin\nθP ,θI,θF\n\"\n−λEπ(st;θP )\n\u0014 X\nt rt\n\u0015\n+ (1 −β)LI + βLF\n#\n(9)\nwhere 0≤β≤1 is a scaling factor that weights the inverse\nmodel against the forward model, and λ > 0 is a scaling factor\nof policy gradient loss against the intrinsic reward [13]. In\nthis paper, to speed up the learning process of the RL agents\ntwo optimization techniques called Gradient Monitoring and\nCurriculum Learning are used. They are explained further\nbelow.\nA. Gradient Monitoring\nRL has a non stationary learning problem. This makes\nensuring stability in learning a critical part of RL. In this paper\nan optimization method called Gradient Monitoring (GM) is\nused to stabilize the RL training of the RMC environments.\nGM provides for a faster convergence and better generalization\nperformance of the trained RL agent, for a small computational\noverhead, by steering the gradients during the learning process\ntowards the most important weights. The added beneﬁt of\nusing GM is that it allows for a continuous adjustment of\nthe neural network’s utilization capacity, i.e., the size of the\nneural network need not be tuned with as much care since GM\nautomatically derives the required size of the neural network\nrequired for the training process [18].\nFigure 3. Graphical representation of the Intrinsic Curiosity Module\nB. Curriculum Learning\nHumans learn things efﬁciently, when the tasks are or-\nganized in such a way that the complexity levels increase\ngradually. This strategy is called Curriculum Learning (CL).\nCL is a training process where the criterion of the training\nprocess is co-related with different set of weights. It is a form\nof re-weighting of the training distribution. At the beginning,\nthe associated weights represent the simpler task, that can\nbe learned most easily. The next training criterion involves\na small change in the weighting of task that increases the\nprobability of sampling slightly more complex task. At the\nend, the re-weighting of the tasks is uniform, and the ﬁnal\ntarget is trained [19]. In RL, sometimes it becomes difﬁcult\nfor the RL agent to handle complex tasks directly and to solve\nthis problem CL technique is used.\nV. EXPERIMENTAL RESULTS\nIn this paper, two experiments are conducted. First sRMC is\nsolved using ICM and CL, since the ﬁnal task of planning of\n20 WP-1 and 20 WP-2 was too difﬁcult for the agent. Then\nin the same experiment, GM is introduced which helps the\nagent to converge without the use of CL. Next in the second\nexperiment, the ICM used in sRMC is used in the gRMC\nenvironment. Here, only CL is used since the overhead GM did\nnot provide a corresponding gain. The results thence obtained\nare discussed below in two parts based on their environment,\nsRMC and gRMC. The parts output in each of the case is\nreported, along with the rewards collected by in setup.\nA. sRMC:\nIn this section, ﬁrst the results of curriculum learning\nwithout gradient monitoring approach in sRMC environment\nis discussed. The result in parts output, WP-1 and WP-2, with\nrespect to iterations is shown in Figure 4. Here, the target is\n20 work-pieces for each agent. Initially, without the additional\noptimizations the agents were able to solve only seven work-\npieces each as opposed to the target of 20 each. Hence, CL\nwas introduced to divide the ﬁnal target into smaller targets\nof increasing difﬁculty, i.e. target of 5, 10, 15, and 20 work-\npieces of WP-1 and WP-2. Both the agents are trained for 1e5\nFigure 4. Parts Output (sRMC without gradient monitoring)\nepisodes and the convergence for 20 target is achieved after\n0.78e5 episodes.\nTo increase the convergence speed of the agents, GM was\nintroduced into the agent learning the sRMC environment.\nThe improvement posed by GM was good enough to remove\nthe CL completely from the training regime of sRMC envi-\nronment. This quickened the learning progress as shown in\nFigure 4. A target of 20 work-pieces for each agent was set\ndirectly rather than dividing the targets into smaller tasks. The\nagents solved the target directly in 5e3 steps compared to\nthe 0.78e5 without GM. With this, the objective of training\nthe agents on the sRMC environment was completed. The\nresults graph shows an interesting trend. Figure 6 shows the\ncombined, intrinsic and extrinsic, rewards collected by the\nagents. The rewards graph can be used to describe the agent’s\nlearning behavior. Normally, in extrinsic reward-based RL\nalgorithm, the slope of the reward curves becomes zero after\nreaching a solution. But the intrinsic reward-based system\nworks differently. When the agent remains curious about\ncompleting the target, it generates the intrinsic reward by itself.\nFigure 5. Parts Output (sRMC with gradient monitoring)\nFigure 6. Combined Rewards (sRMC with gradient monitoring)\nBut once the agent ﬁnishes exploring its environment and ﬁnds\nthe an optimal policy, the agent remains no more curious and\nloses its motivation, hence receives less reward for the same\ntask. That means, intrinsic rewards obviously reduce once the\nagent has converged. This is illustrated in the Figure 6, where\nafter the iteration number 1e5, the rewards collected reduces.\nThe next objective is to transfer the ICM incorporated here in\nto another environment without any architectural changes.\nB. Graph network based curriculum learning:\nIn the next experiment, the intrinsic curiosity module is\nplugged to train the agents on gRMC. The learning process\nwas a lot more difﬁcult in this setup. This difﬁculty was also\nFigure 7. Parts Output (gRMC without gradient monitoring)\nnoted in the reward shaping method, where the agent did not\nconverge in regular training. Hence, ﬁrst CL was introduced\nin to the training regime. The agent’s target in curriculum\nlearning was designed keeping in mind the difﬁculty i.e.,\ntargets of 1, 2, 3, 4, etc., were used. The agent solved the\ntargets until a target of seven work-pieces each as shown in\nFigure V-B, although the training times were extremely high.\nAnother optimization trial using GM produced similar results,\nbut instead increased the training times marginally. This could\nbe because of the complex nature of the gRMC interaction\nand the higher size of the action space. When looking at\nFigure V-B though, the agent starts exploring the environment\nfrom its early stage and the agent reaches to the peak of\nexploration faster compared to sRMC. When the agent learns\nto solve its target, the curiosity of the agent reduces, and this\nleads to a reduction of further exploration of the environment.\nIn addition to that, the combined rewards collection by the\nagent in gRMC is much more higher compared to the sRMC.\nThe objective of solving the a different environment like\ngRMC using the same architecture of ICM as used in sRMC\nis achieved.\nVI. CONCLUSION\nRL on industrial scheduling problems are difﬁcult to train\nbecause of the inherent bias related to hand tuned rewards.\nA novel combination of the ﬂexible RMC and a curiosity\nbased RL has been proposed in this study. We found that the\napplication of the curiosity based RL provides good solutions\neven with very scarce external reward. Additionally, we found\nthat the reward function architecture used in one environment\ncould be easily transferred to another, which was is impossible\nin hand tuned rewards. In future work, we plan to improve the\nperformance of the agent by using a hybrid reward function\nproviding rewards with little inductive bias for a quicker\nFigure 8. Combined Rewards (gRMC without gradient monitoring)\ninitial learning but with enough generalized performance. This\ncombined with a gRMC with a custom message passing\nnetwork could provide highly generalized solutions that can\nbe directly transferable from one robot to another.\nREFERENCES\n[1]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J.\nVeness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, G. Ostrovski, et al., “Human-level\ncontrol through deep reinforcement learning,” nature,\nvol. 518, no. 7540, pp. 529–533, 2015.\n[2]\nE. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics\nengine for model-based control,” in 2012 IEEE/RSJ\nInternational Conference on Intelligent Robots and Sys-\ntems, 2012, pp. 5026–5033.\n[3]\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou,\nM. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran,\nT. Graepel, T. Lillicrap, K. Simonyan, and D. Hassabis,\n“A general reinforcement learning algorithm that mas-\nters chess, shogi, and go through self-play,” Science,\nvol. 362, pp. 1140–1144, 2018.\n[4]\nJ. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan,\nL. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis,\nT. Graepel, et al., “Mastering atari, go, chess and shogi\nby planning with a learned model,” arXiv preprint\narXiv:1911.08265, 2019.\n[5]\nS. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep\nreinforcement learning for robotic manipulation with\nasynchronous off-policy updates,” in 2017 IEEE In-\nternational Conference on Robotics and Automation\n(ICRA), 2017, pp. 3389–3396.\n[6]\nB. Cunha, A. M. Madureira, B. Fonseca, and D. Coelho,\n“Deep reinforcement learning as a job shop schedul-\ning solver: A literature review,” in ser. Advances in\nintelligent systems and computing, 2194-5357, vol. 923,\nSpringer, 2020, pp. 350–359.\n[7]\nK. Xia, C. Sacco, M. Kirkpatrick, C. Saidy, L. Nguyen,\nA. Kircaliali, and R. Harik, “A digital twin to train deep\nreinforcement learning agent for smart manufacturing\nplants: Environment, interfaces and intelligence,” Jour-\nnal of Manufacturing Systems, 2020.\n[8]\nD. Schwung, F. Csaplar, A. Schwung, and S. X. Ding,\n“An application of reinforcement learning algorithms to\nindustrial multi-robot stations for cooperative handling\noperation,” in 2017 IEEE 15th International Conference\non Industrial Informatics (INDIN), 2017, pp. 194–199.\n[9]\nA. Schwung, D. Schwung, and M. S. Abdul Hameed,\n“Cooperative robot control in ﬂexible manufacturing\ncells: Centralized vs. distributed approaches,” in 2019\nIEEE 17th International Conference on Industrial In-\nformatics (INDIN), 2019, pp. 233–238.\n[10]\nI.-B. Park, J. Huh, J. Kim, and J. Park, “A reinforce-\nment learning approach to robust scheduling of semi-\nconductor manufacturing facilities: Ieee transactions\non automation science and engineering, 1-12,” IEEE\nTransactions on Automation Science and Engineering,\npp. 1–12, 2020.\n[11]\nT. Gabel and M. Riedmiller, “Adaptive reactive job-\nshop scheduling with reinforcement learning agents,”\nInternational Journal of Information Technology and\nIntelligent Computing, vol. 24, no. 4, 2008.\n[12]\nB. Waschneck, A. Reichstaller, L. Belzner, T. Al-\ntenm¨uller, T. Bauernhansl, A. Knapp, and A. Kyek,\n“Optimization of global production scheduling with\ndeep reinforcement learning,” Procedia CIRP, vol. 72,\nno. 1, pp. 1264–1269, 2018.\n[13]\nD. Pathak, P. Agrawal, A. A. Efros, and T. Dar-\nrell, “Curiosity-driven exploration by self-supervised\nprediction,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR)\nWorkshops, 2017.\n[14]\nM. S. A. Hameed and A. Schwung, Reinforcement\nlearning on job shop scheduling problems using graph\nnetworks, 2020. arXiv: 2009.03836.\n[15]\nR. S. Sutton and A. G. Barto, Reinforcement learning:\nAn introduction. MIT press, 2018.\n[16]\nN. Chentanez, A. G. Barto, and S. P. Singh, “Intrinsi-\ncally motivated reinforcement learning,” in Advances in\nneural information processing systems, 2005, pp. 1281–\n1288.\n[17]\nY. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell,\nand A. A. Efros, “Large-scale study of curiosity-driven\nlearning,” in International Conference on Learning Rep-\nresentations, 2018.\n[18]\nM. S. A. Hameed, G. S. Chadha, A. Schwung, and S. X.\nDing, Gradient monitored reinforcement learning, 2020.\narXiv: 2005.12108.\n[19]\nY. Bengio, J. Louradour, R. Collobert, and J. We-\nston, “Curriculum learning,” in Proceedings of the 26th\nannual international conference on machine learning,\n2009, pp. 41–48.\n",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.MA"
  ],
  "published": "2020-11-17",
  "updated": "2020-11-17"
}