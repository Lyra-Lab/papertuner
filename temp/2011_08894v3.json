{
  "id": "http://arxiv.org/abs/2011.08894v3",
  "title": "Contrastive Registration for Unsupervised Medical Image Segmentation",
  "authors": [
    "Lihao Liu",
    "Angelica I Aviles-Rivero",
    "Carola-Bibiane Schönlieb"
  ],
  "abstract": "Medical image segmentation is a relevant task as it serves as the first step\nfor several diagnosis processes, thus it is indispensable in clinical usage.\nWhilst major success has been reported using supervised techniques, they assume\na large and well-representative labelled set. This is a strong assumption in\nthe medical domain where annotations are expensive, time-consuming, and\ninherent to human bias. To address this problem, unsupervised techniques have\nbeen proposed in the literature yet it is still an open problem due to the\ndifficulty of learning any transformation pattern. In this work, we present a\nnovel optimisation model framed into a new CNN-based contrastive registration\narchitecture for unsupervised medical image segmentation. The core of our\napproach is to exploit image-level registration and feature-level from a\ncontrastive learning mechanism, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-image\ntransformation pattern via registration for unsupervised medical image\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\nregistration architecture to enhance the discriminating capacity of the network\nin the feature-level. We show that our proposed technique mitigates the major\ndrawbacks of existing unsupervised techniques. We demonstrate, through\nnumerical and visual experiments, that our technique substantially outperforms\nthe current state-of-the-art unsupervised segmentation methods on two major\nmedical image datasets.",
  "text": "1\nContrastive Registration for\nUnsupervised Medical Image Segmentation\nLihao Liu, Angelica I Aviles-Rivero, and Carola-Bibiane Sch¨onlieb\nAbstract—Medical image segmentation is an important task in\nmedical imaging, as it serves as the ﬁrst step for clinical diagnosis\nand treatment planning. Whilst major success has been reported\nusing deep learning supervised techniques, they assume a large\nand well-representative labelled set. This is a strong assumption\nin the medical domain where annotations are expensive, time-\nconsuming, and inherent to human bias. To address this problem,\nunsupervised segmentation techniques have been proposed in the\nliterature. Yet, none of the existing unsupervised segmentation\ntechniques reach accuracies that come even near to the state of\nthe art of supervised segmentation methods. In this work, we\npresent a novel optimisation model framed in a new CNN-based\ncontrastive registration architecture for unsupervised medical\nimage segmentation called CLMorph. The core idea of our\napproach is to exploit image-level registration and feature-level\ncontrastive learning, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-\nimage transformation mapping via registration for unsupervised\nmedical image segmentation. Secondly, we embed a contrastive\nlearning mechanism in the registration architecture to enhance\nthe discriminative capacity of the network at the feature level.\nWe show that our proposed CLMorph technique mitigates\nthe major drawbacks of existing unsupervised techniques. We\ndemonstrate, through numerical and visual experiments, that our\ntechnique substantially outperforms the current state-of-the-art\nunsupervised segmentation methods on two major medical image\ndatasets.\nIndex Terms—Contrastive Learning, Image Registration, Im-\nage Segmentation, Deep Learning, Brain Segmentation\nI. INTRODUCTION\nMedical image segmentation is the task of partitioning an\nimage into multiple regions, which ideally reﬂect qualities\nsuch as well-deﬁned structures guided by boundaries in the\nimage domain. This task has been successfully applied in a\nrange of medical applications using data coming from different\nparts of human anatomy including the heart [1], [2], lungs [3]\nand brain [4]. Medical image segmentation is a relevant task;\nas it is the ﬁrst step for several clinical applications such as\ntumor localisation and neuropsychiatric disorder diagnosis.\nThe body of literature has reported several successful\ntechniques for medical image segmentation, in which major\nprogress has been reported using fully-supervised convolu-\ntional neural networks (CNNs) [5], [6], [7], [8] or partially-\nsupervised CNN methods [9], [10], [11], [12].These tech-\nniques rely on learning prior mapping information from pair-\nwise data (mapping from medical images to their manual\nsegmentation) to achieve astonishing performance at the same\nL. Liu., A.I. Aviles-Rivero and C. Sch¨onlieb are with the Department\nof Applied Mathematics and Theoretical Physics, University of Cambridge.\nCambridge CB3 0WA, UK. Corresponding author: ll610@cam.ac.uk.\nlevel as, and sometimes outperforming, radiologists. However,\na major constraint of these methods is the assumption of\nhaving a well-representative set of annotations for training,\nwhich is not always possible in the medical domain. Moreover,\nobtaining such annotations is time-consuming, expensive, and\nrequires expert knowledge. To deal with these constraints,\na body of research has explored unsupervised segmentation\ntechniquess [13], [14], [15], [16], in which no annotations are\nneeded.\nNotably, in recent years unsupervised techniques have\nbecome a great focus of attention as they do not require\nsegmentation labels. In this context, clustering algorithms\nare often applied to perform unsupervised segmentation by\ngrouping the image content with similar intensities [17],\n[18], [19], [20]. However, these methods are still limited\nperformance-wise; as it is difﬁcult to learn any image-to-\nmask transformation mapping when relying solely on images.\nTo further embed the transformation mapping information\nwithin an unsupervised architecture for better medical image\nsegmentation performance, one feasible solution is to cast the\nsegmentation task as an unsupervised deformable registration\nproblem [21], [22], [23], [24], [16], [25], [26], [27], [28], [29],\n[30]. The goal of this perspective is to ﬁnd an optimal image-\nto-image transformation mapping to align a set of images in\none coordinate system.\nIn this work, we aim to ﬁnd an optimal image-to-image\ntransformation mapping z between an unaligned image x\nand a reference image y, which is usually formulated by a\nmetric function φz. Beneﬁting from similar morphological\nattributes between the medical image and its corresponding\nsegmentation mask (such as the shape of different organs),\none can also transfer the segmentation mask of the reference\nimage yseg back to the coordinate system of the unaligned\nimage. This process is with the purpose of obtaining the\nsegmentation mask of the unaligned image xseg using the\noptimal mapping information z. Hence, when the image is\ncorrectly registered, the segmentation mask is automatically\nobtained, which we called registration-based segmentation\n(aka atlas-based segmentation).\nFollowing this philosophy, we develop an unsupervised\nsegmentation model based on a registration architecture. A\ncentral observation of existing registration methods is that they\nonly focus on capturing the mapping information on the image\nlevel, but fail to enhance the feature-level representation.\nMost recently, unsupervised feature representation learning\nhas demonstrated promising results. In particular, contrastive-\nbased models, such as SimCLR [31] and BYOL [32], are\nreaching performance comparable to those produced by su-\narXiv:2011.08894v3  [cs.CV]  20 Jul 2022\n2\npervised techniques for different tasks. The main idea is\nthat by contrasting images to others, the differences between\nimages are easily remembered within a network (i.e., learning\ndistinctiveness); thus making the learned feature more robust\nto discriminate images with different labels. Therefore, to\nfurther improve the feature-level learning by contrasting the\nunaligned image x to the reference image y, we propose\nto embed the contrast feature learning in the registration\narchitecture to extract feature maps with richer information,\nproducing better unsupervised segmentation results.\nIn this paper, we present a novel Contrastive Learning\nregistration architecture based on VoxelMorph for unsuper-\nvised medical image segmentation, which we named named\nCLMorph. Our proposed CLMorph is a simple yet effective\nunsupervised segmentation model. Unlike existing techniques,\nour approach combines image-level registration and feature-\nlevel contrastive representation learning. More speciﬁcally, our\ntechnique works as follows. We ﬁrst propose two weight-\nshared feature encoders, where CNN features are extracted\nfrom the unaligned images and reference images, respectively.\nThen, by contrasting the extracted CNN features from the\nunaligned images and reference images, one can obtain CNN\nfeatures with richer information. Moreover, we use one single\ndecoder to capture the mapping information z from the con-\ntrasted CNN features. Finally, we use the spatial transform\nnetwork of [33] along with the captured z to align the\nsegmentation mask, of the reference image to the coordinate\nsystem of unaligned images, to obtain the segmentation mask\nin an unsupervised manner. Our main contributions are:\n• We propose a simple yet effective contrastive registration\narchitecture for unsupervised medical image segmenta-\ntion, in which we highlight the combination of image-\nlevel registration architecture and feature-level contrastive\nlearning, which we called CLMorph. To the best of our\nknowledge, this is the ﬁrst work that embeds contrastive\nlearning in a registration architecture for unsupervised\nsegmentation.\n• We evaluate our technique using a range of numerical\nand visual results on two major benchmark datasets. We\nshow that, by casting the unsupervised segmentation task\nvia registration with feature-level contrast, we can largely\nimprove the unsupervised segmentation performance and\nreduce the performance gap between supervised and\nunsupervised segmentation techniques.\n• We demonstrate that our contrastive registration architec-\nture, as a by-product, can also lead to a better registration\nperformance than the state-of-the-art unsupervised regis-\ntration techniques.\nII. RELATED WORK\nThe body of literature has reported impressive results for\nimage segmentation. In this section, we review the existing\ntechniques in turn.\nA. Supervised Medical Image Segmentation\nIn this section, we revise exiting techniques that heavily\nrely on annotations (i.e. supervised techniques), and the close\nrelated paradigm of one-shot learning where labels are still\nneeded for the technique to work.\nFully-supervised medical image segmentation. Medical\nimage segmentation has been extensively investigated in the\nliterature, in which supervised methods have been most suc-\ncessful. Early works learn to segment the different organs\nbased on hand-crafted features including thresholding [34],\nstatistical model [35] and Bayesian model [36]. However, a\nmajor drawback is that they are not capable of capturing\nhigh-level semantic information. Thereby, they tend to fail in\nsegmenting those organs accurately.\nMore recently, convolutional neural networks (CNN) based\nmethods, which train with annotations to capture the high-\nlevel semantic information, have demonstrated remarkable\nperformance beyond radiologist execution, such as U-Net [5],\nnnU-Net [6], VoxResNet [7], and MedicalNet [8]. Although\neffective in biomedical image segmentation, these supervised\nmethods rely heavily on a well-representative set of annota-\ntions. This is a strong assumption in the medical domain as\nannotations are expensive to obtain (in many medical tasks\nthere is a need for at least two readers), highly uncertain, and\nrequire expert knowledge. Hence, when applying the trained\nmodel to another dataset such approaches often fail to segment\ncorrectly.\nOne-shot medical image segmentation. Recently, the com-\nmunity has moved towards the perspective of developing\ntechniques that require a tiny labelled set. For example, the\nprinciples of one-shot learning have been reported in the\nliterature for medical image segmentation. For example, the\nauthors of [9] proposed DataAug, which uses the learned\nspatial and intensity transformation to synthesize labelled\nimages for one-shot medical image segmentation. Another\nwork using this philosophy is LT-Net [10]. In that work, the\nauthors embedded a cycle consistency loss into a registration\narchitecture to perform one-shot medical image segmentation.\nHowever, in the training process, one still needs to provide a\nset of labels that are well-representative for the task at hand.\nAn alternative to those sets of techniques is unsupervised\nlearning which has recently been a great focus of attention in\nthe medical imaging community. This paradigm is the focus\nof this work and existing techniques are discussed next.\nB. Unsupervised Medical Image Segmentation\nClustering. For unsupervised segmentation, clustering algo-\nrithms [17], [18], [19], [20] have been extensively explored.\nThe idea is to divide the image into different groups of pix-\nels/voxels according to the similarity of image intensities (i.e.,\neach pixel value). Besides clustering techniques, works based\non Autoencoders [37] and GANs [38] have also been explored\nfor unsupervised medical image segmentation. However, due\nto the lack of segmentation masks, which determines if it is\nimpossible to learn any image-to-mask mapping information,\nthese algorithms are still inefﬁcient for the task of segmenta-\ntion.\nRegistration-based segmentation. To enable the mapping\ninformation in the training process, another feasible solution\nis to cast the segmentation task an unsupervised deformable\n3\nSmooth \nLoss \n STN\nUnaligned \nImage (x) \nReference \nImage (y) \nTransform \nPattern (  )        \nReconstruction\nLoss \nSTN\nRegistered\nImage\nWeighed Layer \nConcatenation \nProjection Layer\nLoss Input \nFeature Map \nTraining \nContrastive \nLoss \nTesting \nFig. 1. The overall workﬂow of the proposed network architecture: CLMorph. It takes two images as input for the two CNNs, which generates two sets of\nfeature maps (see blue and yellow blocks). Two projection layers are applied to the feature maps to produce two vector presentations of the input images.\nBased on these vector presentations, a contrastive loss is employed. It then recursively concatenates and enlarges the contrasted CNN feature maps, from the\ntwo CNNs, using a single decoder until the transformation mapping is obtained. Based on the transformation mapping, the reconstruction and smooth loss\nare adopted to ensure that the registration is well-performed. After the above training process is completed, we adopt the learned transformation mapping to\ntransfer the reference mask to obtain the segmentation mask of the unaligned image.\nregistration process [21], [22], [23], [24], [16], [25], [26], [27],\n[28], [29], [30]. Instead of seeking the image-to-mask transfor-\nmation mapping, the goal of unsupervised registration methods\nis to ﬁnd an optimal image-to-image transformation mapping.\nSpeciﬁcally, given an unaligned image x and a reference image\ny, the main goal of unsupervised registration methods is to\ncalculate the latent variable z, which contains the image-to-\nimage transformation mapping information [26], [27]. Bene-\nﬁted from the morphological similarity between the medical\nimage and its corresponding segmentation mask, unsupervised\nregistration architecture can transfer the segmentation mask\nof the reference image yseg back to the coordinate system\nof the unaligned image to obtain the segmentation mask of\nthe unaligned image xseg, using the optimal transformation\nmapping z. Hence, as long as the image is correctly registered,\nthe segmentation mask is automatically obtained, which we\ncall registered segmentation.\nContrastive learning. The current unsupervised registration\nmethods only focus on capturing the mapping information\nfrom the image level and ignore the feature-level represen-\ntation learning. To further enhance the feature representa-\ntion learning without annotations, a body of researchers has\ndemonstrated promising results using contrastive representa-\ntion learning including Momentum Contrast (MoCo) [39],\nSimCLR [31], Contrastive Multiview Coding (CMC) [40],\nand BYOL [32]. The main idea of contrastive representa-\ntion learning is to maximise the differences between images\nfrom different groups as well as to maximise the agreements\nbetween images and their different augmented views, such\nas SimCLR [31]. Another research line relies on teacher-\nstudent learning mechanisms to learn from each other without\ngroup difference (no negative pairs), such as BYOL [32]. Both\ndirections are effective for learning features more robust to\ndiscriminate images from different groups. We show these\nﬁndings in our experiments.\nCurrent contrastive learning methods mainly focus on im-\nproving the discriminating capacity of CNN-based models on\nimage classiﬁcation tasks. Likewise, other techniques directly\nemploy contrastive learning methods for the downstream seg-\nmentation tasks in a supervised setting [41] or semi-supervised\nmanner [41], [42]. To further transfer the robust feature-\nlevel learning of the contrastive learning mechanism to the\ndownstream tasks (i.e., segmentation), we propose to embed\nthe contrast feature learning in the registration architecture to\nextract feature maps with richer information for unsupervised\nsegmentation. To our best knowledge, this is the ﬁrst work that\nembeds contrastive learning in a registration architecture for\nunsupervised segmentation\nIII. PROPOSED TECHNIQUE\nThis section contains two key parts, the description of:\ni) the registration architecture which employs a CNN-based\nprobabilistic model to calculate image-to-image transforma-\ntion mapping z, and ii) our new full optimisation model that is\ncomposed of a reconstruction and a smooth loss and combined\nwith a contrastive learning mechanism.\nA. Our Technique Overview\nIn this section, we describe the workﬂow of our technique,\nin which we highlight their main components that are de-\nscribed in detail in the following subsections.\n4\nWe display the overall workﬂow of our unsupervised seg-\nmentation technique in Figure 1. Our technique takes as input\ntwo 3D images, the unaligned image x and the reference\nimage y, which are fed into a siamese (two-symmetric) weight-\nshared 3D CNNs to extract the highly semantic feature maps\nfrom the unaligned and reference images – these parts are\nillustrated in blue and yellow colours in Figure 1. We then\nemploy a contrastive loss on the projection of the two extracted\nCNN feature maps, which forces the network to contrast\nthe difference between the two CNN feature maps. The two\nsymmetric weight-shared CNNs can generate more robust\nCNN feature maps via back-propagating the contrastive loss\nduring training.\nBased on the contrasted feature maps of the two CNNs,\nwe use a single decoder to integrate all the feature maps,\nwith different resolutions of the two CNNs, and estimate the\ntransformation mapping z – see the green part in Figure 1. We\nﬁrst concatenate the feature maps with the same resolutions\nof the two CNNs. We then adopt a decoder to recursively use\nthe feature maps, with high-level semantic information (low\nresolutions) to reﬁne the feature maps with low-level detailed\ninformation (high resolutions), until we obtain a feature map\nwith the same resolution as the input images. We introduce\na probabilistic model to estimate the optimal transformation\nmapping z, which is based on the recovered image-resolution\nfeature map. After training and ﬁnding the optimal z, we\nobtain the segmentation output by aligning the segmentation\nmask of the reference images to the coordinate system of the\nunaligned image. We do this using the optimal z via a spatial\ntransform network [33].\nB. Transformation Mapping Estimation\nGiven a pair images called unaligned image x : X →Ωand\na reference image y : X →Ω, where X = [w]×[h]×[d] ⊂Z3\nis the input domain and Ωis the value data domain (i.e., gray\nscale), we seek to determine an optimal transformation map-\nping z, which parametrises a spatial transformation function\ndenoted as ψz, such that the transformed unaligned image,\nx ◦φz, is aligned with y.\nTo compute the transformation mapping, we estimate z\nby maximising the posterior registration probability p(z|y; x)\ngiven x and y – that is, to estimate the central tendency of\nthe posterior probability (maximum a posteriori estimation\nMAP). However, solving the partition function is intractable\nand cannot be solved analytically. To address this problem,\none can use variational inference and approximate the solution\nthrough an optimisation problem over variational parameters.\nFollowing this principle, we adopt a CNN-based variational\napproach to compute p(z|y; x). We ﬁrst introduce an ap-\nproximate posterior probability qψ(z|y; x) which we assume\nis normally distributed. To measure the similarity between\nthese two distributions, a divergence D(qψ(z|y; x)||p(z|y; x))\nmeasure can be applied, e.g., [43], [44], [45]. We use the most\ncommonly used divergence: the Kullback-Leibler (KL) diver-\ngence [46], [47]. With this purpose, we seek to minimise the\nKL divergence from qψ(z|y; x) to p(z|y; x) which expression\nreads:\nψ∗= min\nψ KL[qψ(z|y; x) || p(z|y; x)]\n= min\nψ Eq[log qψ(z|y; x) −log p(z|y; x)]\n= min\nψ Eq[log qψ(z|y; x) −log p(z, y; x)\np(y; x) ]\n= min\nψ Eq[log qψ(z|y; x) −log p(z, y; x) + log p(y; x)]\n= min\nψ Eq[log qψ(z|y; x) −log p(z, y; x)] + log p(y; x)\n= min\nψ KL[qψ(z|y; x) || p(z)] −Eq[log p(y|z; x)] + const,\n(1)\nwhere Eq[log p(y|z; x)] is the evidence, KL[qψ(z|y; x) is the\nevidence lower bound (ELBO), and const is a normalisation\nconstant. qψ(z|y; x) comes from a multivariate normal distri-\nbution N:\nqψ(z|y; x) = N(z; µz|y;x, σ2\nz|y;x) ,\n(2)\nwhere µz|y;x and σ2\nz|y;x denote the mean and variance of the\ndistribution respectively, which can be directly learned through\nthe convolutional layers; as shown in Figure 1. Whilst p(z) and\np(y|z; x) follow the multivariate normal distribution, which are\nmodelled as:\np(z) = N(z; 0, σ2\nz) ,\n(3)\np(y|z; x) = N(y; x ◦φz, σ2) ,\n(4)\nwhere σz is the variance (a diagonal matrix) of this distribution\nand x◦φz is the noisy observed registered image in which σ2\nis the variance of the noisy term. We remark that whilst several\nworks in the literature have addressed the problem of image\nregistration from a probabilistic perspective, for example, the\nworks of that [21], [16], we emphasise that our framework\ngoes beyond a solely probabilistic principle, and in fact, our\nnovel function is based on other principles such as contrastive\nterm, smooth and reconstruction terms. They are explained in\nthe next subsection.\nC. Full Optimisation Model\nIn this section, we detail our optimisation model which\nis composed of a reconstruction and smooth loss, and a\ncontrastive mechanism.\nReconstruction & Smooth loss. According to the deriva-\ntion from (1), there are two terms to be optimised. The ﬁrst\nterm is the KL divergence between the approximate posterior\nprobability qψ(z|y; x) and the prior probability p(z), and the\nsecond term is the expected log-likelihood Eq[log p(y|z; x)].\nBased on our assumptions from (2)-(4), the derivation is\nwritten as:\nLcs(ψ; x, y) =KL[qψ(z|y; x) || p(z)] −Eq[log p(y|z; x)]\n=1\n2[tr(σ2\nz|y;x) + ||µz|y;x||2 −log det(σ2\nz|y;x)]\n+\n1\n2σ2 ||y −x ◦φz||2.\n(5)\n5\nFor sake of clarity in the notation, we decouple Lcs to detail\nthe terms. The ﬁrst term (the second line) is a close form of\nKL[qψ(z|y; x). It enforces the posterior qψ(z|y; x) to be close\nto the prior p(z). We called this term the smooth loss:\nLsmooth = 1\n2[tr(σ2\nz|y;x) + ||µz|y;x||2 −log det(σ2\nz|y;x)], (6)\nwhere the log det(σ2\nz|y;x) is a Jacobian determinant which\nspatially smooths the means. The second term (the third line)\nis the expected log-likelihood Eq[log p(y|z; x)]. It enforces\nthe registered image x ◦φZ to be similar to reference image\ny, which we refer as our reconstruction loss:\nLrecon =\n1\n2σ2 ||y −x ◦φz||2\n(7)\nContrastive\nloss.\nContrastive\nlearning\nis\na\nlearning\nparadigm that seeks to learn distinctiveness. It aims to max-\nimise agreements between images and their augmented views,\nand from different groups via a contrastive loss in a latent\nspace. In our work, we follow a four components principle\nfor the contrastive learning process, which is described next.\nThe ﬁrst component in our contrastive learning process is\nthe 3D images. The image contents are basically the same\nincluding the number of brain structures and the relative loca-\ntions of each structure. They are mainly different in structure\nsizes. Therefore, we view the unaligned and the reference\nimages as images sampled from different augmented views.\nAs our second component, we set the two CNN encoders\nas weight-shared. This with the purpose of ensuring that the\nCNN-based encoder can extract uniﬁed CNN features from the\nunaligned and reference images. For the third component, we\nadopt a fully-connected layer, as the projection layer, to map\nthe CNN features to a latent space where contrastive loss is\napplied. Finally, our fourth component is our contrastive loss\nfollowing the standard deﬁnition presented in [48], [31].\nFrom our four-component process, we can now formalise\nthe loss we used in our framework. It is based on a\ncontrastive loss [48], [31], [39] which is a function whose\nvalue is low when the image is similar to its augmented\nsample and dissimilar to all other samples. Formally, given\na set of images I, we view our unaligned image x and\nreference image y as an augmented image pair, and any\nother images in I as negative samples. Moreover, we denote\nsim(u, v) =\nuTv\n||u||·||v|| as the cosine similarity between u and\nv. We then deﬁne the contrastive loss function as:\nLcontrast = −log\nexp (sim(f(x), f(y))/τ)\nP\ni∈I\n1i̸=x exp (sim(f(x), f(i))/τ)\n(8)\nwhere f() denotes the CNN encoder, f(x) and f(y) are the\ngenerated features from the CNN encoder, and 1i̸=x ∈{0, 1}\nis an indicator, which values 1 only when i ̸= x. We also\ndeﬁne τ as a temperature hyperparameter [49].\nOptimisation model. Our unsupervised framework is com-\nposed of three loss functions. The ﬁrst two ones are di-\nrectly derived from the optimisation model described in (5),\nin which the image-to-image reconstruction loss and the\ntransformation mapping (deformation ﬁeld) smooth loss are\nintroduced. Whilst the third loss is the contrastive loss as\nin (8) which forces the network to contrast the difference\nbetween unaligned images and reference images. The total loss\nis formulated as:\nLtotal = Lrecon + αLsmooth + βLcontrast\n=\n1\n2σ2 ||y −x ◦φz||2 + α\n\u00121\n2[tr(σ2\nz|y;x) + ||µz|y;x||2\n−logdet(σ2\nz|y;x)]\n\u0013\n+ β\n\u0012\n−log\nexp (sim(f(x), f(y))/τ)\nP\ni∈I\n1i̸=x exp (sim(f(x), f(i))/τ)\n\u0013\n,\n(9)\nwhere α and β are the hyper-parameters balancing Lrecon,\nLsmooth and Lcontrast, which we empirically set as 1 and\n0.01, respectively.\nIV. EXPERIMENTAL RESULTS\nIn this section, we describe the range of experiments that\nwe conducted to validate our proposed technique.\nA. Dataset Description & Evaluation Protocol\nBenchmark datasets. We evaluate our technique using two\nbenchmarking datasets: the LONI Probabilistic Brain Atlas\n(LPBA40) dataset [50] and the MindBoggle101 dataset [51].\nThe characteristics are detailed next.\nThe LPBA40 dataset [50] is composed of a series of maps\nfrom the regions of the brain. It contains 40 T1-weighted\n3D brain images, from 40 human healthy volunteers, of size\n181×217×181 with a uniform space of 1×1×1 mm3. The\n3D brain volumes were manually segmented to identify 56\nstructures. Whilst the MindBoggle101 dataset [51] is com-\nposed of a collection of 101 T1-weighted 3D brain MRIs\nfrom several public datasets: OASIS-TRT-20, NKI-TRT-20,\nNKI-RS-22, MMRR-21, and Extra-18. It contains 101 skull-\nstripped T1-weighted 3D brain MRI volumes, from healthy\nsubjects, of size 182×218×182 that are evenly spaced by\n1×1×1 mm3. From this dataset, we use 62 MRI images\nfrom OASIS-TRT-20, NKI-TRT-20, and NKI-RS-22, since\nthey are already wrapped to MNI152 space and have manual\nsegmentation masks (50 anatomical labels).\nEvaluation metrics. To evaluate our technique against the\nstate-of-the-art unsupervised brain image segmentation, we use\nthree widely-used metrics: the Dice similarity coefﬁcient [52],\nHausdorff distance (HD) [53], and average symmetric sur-\nface distance (ASSD) [54]. Dice measures the region-based\nsimilarity between the predicted segmentation and the ground\ntruth. The higher the Dice is, the better the model performs.\nWhilst HD and ASSD measure the longest distance (a.k.a.\nlargest difference) and average surface distance (a.k.a. average\nboundary difference) between the predicted segmentation and\nthe ground truth, respectively. The lower the HD and ASSD\nis, the better the model performs. The detailed formulation can\nbe found in [55]\n6\nTABLE I\nNUMERICAL COMPARISON OF OUR TECHNIQUE VS SOTA TECHNIQUES FOR THE LPBA40 AND MINDBOGGLE101 DATASETS. THE NUMERICAL VALUES\nDISPLAY THE AVERAGE OF DICE, HD, AND ASSD OVER ALL REGIONS. THE HIGHER THE DICE IS THE BETTER THE MODEL PERFORMS. WHILST, THE\nLOWER THE HD AND ASSD ARE THE BETTER THE MODEL PERFORMS. THE BEST PERFORMANCE IS DENOTED IN BOLD FONT. THE PER-REGION RESULTS\nARE ILLUSTRATED IN FIG. 2 AND FIG. 3.\nLPBA40 DATASET\nMindBoggle101 DATASET\nDice\nHD\nASSD\nDice\nHD\nASSD\nUtilzReg [56]\n0.665±0.06\n14.16±2.34\n1.906±0.09\n0.439±0.17\n18.21±3.04\n0.972±0.09\nSyN [57]\n0.701±0.04\n13.61±2.56\n1.986±0.08\n0.543±0.16\n16.83±3.68\n0.926±0.07\nVoxelMorph [26]\n0.716±0.08\n11.38±3.46\n1.874±0.41\n0.559±0.12\n15.03±2.92\n1.171±0.10\nFAIM [58]\n0.729±0.07\n11.04±4.11\n1.809±0.40\n0.583±0.04\n14.50±2.71\n1.108±0.08\nLapIRN [28]\n0.739±0.07\n10.77±3.85\n1.727±0.60\n0.607±0.04\n14.54±2.58\n1.041±0.08\nVTN [29]\n0.745±0.08\n11.11±3.41\n1.601±0.31\n0.626±0.04\n14.64±2.57\n1.007±0.09\nPDD-Net [30]\n0.749±0.06\n10.75±3.77\n1.714±0.42\n0.632±0.05\n15.21±2.92\n0.990±0.10\nCLMorph (Ours)\n0.763±0.07\n10.38±2.59\n1.458±0.31\n0.646±0.04\n12.76±2.52\n0.892±0.05\nFrontal\nParietal\nOccipital\nTemporal\nCingulate\nPutamen\nHippo\nAverage\n0\n10\n20\n30\nHD Comparision on LPBA40\nFrontal\nParietal\nOccipital\nTemporal\nCingulate\nPutamen\nHippo\nAverage\n0\n1\n2\n3\nASSD Comparision on LPBA40\nFrontal\nParietal\nOccipital\nTemporal\nCingulate\nPutamen\nHippo\nAverage\n0.2\n0.4\n0.6\n0.8\n1.0\nDice Comparision on LPBA40 \nVoxelMorph\nFAIM\nLapIRN\nVTN\nPDD-Net\nOurs\nFig. 2. Numerical comparisons between our framework and existing SOTA\ntechniques on the LPBA40 dataset. The results are reported in terms of the\nDice, HD, and ASSD metrics reﬂecting a per region, of the brain, evaluation.\nB. Implementation Details & Running Scheme\nIn this section, we provide the implementation details and\nthe training & testing scheme that we followed to produce the\nreported results.\nImplementation details. In our experiments, we follow\nFrontal\nParietal\nOccipital\nTemporal\nCingulate\nAverage\n0.2\n0.4\n0.6\n0.8\nDice Comparision on MindBoggle101\nFrontal\nParietal\nOccipital\nTemporal\nCingulate\nAverage\n0\n10\n20\n30\nHD Comparision on MindBoggle101\nFrontal\nParietal\nOccipital\nTemporal\nCingulate\nAverage\n0.6\n0.9\n1.2\n1.5\n1.8\n ASSD Comparision on MindBoggle101\nFig. 3.\nComparisons on the MindBoggle101 dataset. We compare our\ntechnique against existing SOTA methods. The comparisons displayed are\nin terms of the Dice, HD, and ASSD metrics for each region on the brain.\nthe training and testing splitting setting from [59] for the\nLPBA40 dataset, where the ﬁrst 30 images are used as training\ndatasets and the last 10 images are used as testing datasets. We\ngroup the 56 structures into seven large regions such that we\ncan show the segmentation results more intuitively. Moreover\nand for the MindBoggle101 dataset, we follow the protocol\n7\nZoom-In\nViews\nZoom-In\nViews\nZoom-In\nViews\nUnaligned Image\nReference Image\nGround Truth\nFaim\nVoxelMorph\nOurs\nFig. 4. Visual comparisons between our technique and unsupervised SOTA techniques for segmentation. The rows show the three views from the 3D data. The\nthird column displays the ground truth whilst the last three samples segmentation from our technique and the compared ones. Zoom-in views are displayed\nto highlight interesting regions where our technique performs better than the other models.\nfrom [58] and use 42 images from NKI-TRT-20 and NKI-RS-\n22 for training and 20 images from OASIS-TRT-20 for testing.\nFor evaluation purposes, we also group the 50 small regions\ninto ﬁve larger regions of interest. As for the reference image,\nwe select the image that is the most similar to the anatomical\naverage as the atlas. In particular, we selected image #30\nas the reference image for the LPBA40 dataset, and image\n#39 for the Mindboggle101. Our proposed technique has been\nimplemented in Pytorch [60].\nWe follow the standard pre-processing protocol to normalise\nthe images to have zero mean and unit variance. We randomly\nselect two images as the pair-wise input data (unaligned image\nand reference image). Moreover, we enrich our input data by\napplying three different transformations (data augmentation) in\nthe following order: random ﬂips in the y coordinate, random\nrotation with an angle of fewer than 10 degrees, and random\ncrops of size 160×192×160.\nTraining scheme. In our training scheme, we initialise the\nparameters of all convolutional layers following the initialisa-\ntion protocol of that [61]. We also initialise the parameters of\nall batch normalisation layers using random variables drawn\nfrom Gaussian distributions with zero mean and 0.1 deriva-\ntion [62]. Moreover, we use Adam optimiser [63] with a batch\nsize of 8. We set the initial learning rate to 3×10−3 and then\ndecrease it by multiplying 0.1 every 20 epochs, and terminate\nthe training process after 200 epochs. Due to the small number\nof the dataset, which is a common issue in the 3D medical\nimage area, we did not split the data into a validation set from\nthe training or testing sets for model selection. Hence, we use\nthe above learning rate decay policy to decrease the learning\nrate to a small value in the last few epochs to ensure the model\ndoes not diverge, and save the last epoch model as our ﬁnal\nmodel. This is a standard protocol followed in the area. Our\ntechnique took ∼20 hours to train on a single Tesla P100\nGPU.\nTesting scheme. After training, we performed unsupervised\nsegmentation based on the learned parameters. We ﬁrst fed the\nunaligned image x and the reference image y into the trained\nnetwork to calculate the transformation relation z. Then, we\nused a spatial transform network (STN) [33], to align the\nsegmentation mask of the reference image according to the\ncalculated z, to obtain the segmentation result of the unaligned\nimage. On average, our method takes less than 10 seconds to\nprocess one whole MRI image on a single GPU (Tesla P100\nGPU).\nC. Results & Discussion\nIn this section, we present the numerical and visual results\noutlined in previous subsections, and discuss our ﬁndings and\nhow they are compared with current existing techniques.\n8\nReference Image\nUnaligned Image\nFaim\nVoxelMorph\nOurs\nZoom-In\nViews\nZoom-In\nViews\nZoom-In\nViews\nTransformation \nPattern \nFig. 5. Visual comparison of our technique and the SOTA techniques for image registration. The rows display the x, y, and z views from the 3D medical\ndata. The columns display outputs samples from FAIM, VoxelMorph, and our technique. The zoom-in views show interesting structures that clearly show the\nimprovement in terms of preserving the brain structures and ﬁne details. The last column (deformation ﬁeld) presents the transformation mapping z between\nunaligned image and reference image produced by our method.\nTABLE II\nNUMERICAL RESULTS FOR OUR REGISTRATION PROCESS. THE REPORTED\nRESULTS DENOTE THE AVERAGE DICE SCORE OVER ALL REGIONS OF THE\nBRAIN. THE BEST PERFORMANCE IS IN BOLD.\nLPBA40\nMindBoggle101\nUtilzReg [56]\n0.665±0.06\n0.440±0.16\nSyN [57]\n0.700±0.04\n0.540±0.15\nVoxelMorph [26]\n0.709±0.07\n0.558±0.10\nFAIM [58]\n0.720±0.08\n0.566±0.09\nLapIRN [28]\n0.727±0.09\n0.608±0.08\nVTN [29]\n0.722±0.06\n0.598±0.04\nPDD-Net [30]\n0.733±0.05\n0.612±0.08\nCLMorph (Ours)\n0.750±0.08\n0631±0.05\nNumerical comparison with the state-of-the-art tech-\nniques. We begin by evaluating our method against the state-\nof-the-art methods unsupervised brain image registration meth-\nods on the LPBA40 dataset: UtilzReg [56], VoxelMorph [27],\nFAIM [58], LapIRN [28], VTN [29], and PDD-Net [30]. We\nremark that we use the registration architecture from other\nstate-of-the-arts methods to predict the transformation map-\nFig. 6. Statistical analysis. Multiple comparisons are performed followed by\na paired Wilcoxon test and p-values adjusted using the Bonferroni method.\nOur technique reported signiﬁcant statistically different among all compared\ntechniques.\nping z, and then adopt the predicted transformation mapping\nz to perform segmentation as illustrated in the testing stage in\nFig. 1 for image segmentation. We report the global results in\nTable I for the LPBA40 and MindBoggle101 dataset, in order\nto understand the general behaviour and performance of our\ntechnique over the SOTA methods. The displayed numbers are\nthe average of the image metrics across the dataset. In a close\nlook at the results, we observe that the compared techniques\nperform similarly close to each other, whilst our technique\noutperforms all other techniques for all evaluation metrics by\n9\nTABLE III\nABLATION STUDY, IN TERMS OF THE MEAN DICE, HD, AND ASSD METRICS, OF OUR PROPOSED TECHNIQUE. THE TOP PART DISPLAYS A NUMERICAL\nCOMPARISON OF OUR TECHNIQUE VS FULLY SUPERVISED SEGMENTATION. THE BOTTOM PART DISPLAYS A COMPARISON OF OUR TECHNIQUE WITH ITS\nDIFFERENT COMPONENTS. THE LAST TWO ROWS DISPLAY OUR FRAMEWORK UNDER DIFFERENT CONTRASTIVE PRINCIPLES BYOL [32].\nLPBA40\nMindBoggle101\nDice\nHD\nASSD\nDice\nHD\nASSD\nFULLY SUPERVISED BASELINE\nU-Net (Upper Bound)\n0.832\n-\n-\n0.811\n-\n-\nOUR UNSUPERVISED TECHNIQUE\nLrecon\n0.702±0.07\n12.82±3.33\n1.842±0.58\n0.552±0.20\n15.11±2.26\n1.210±0.13\nLrecon + Lsmooth\n0.716±0.08\n11.38±3.46\n1.874±0.41\n0.559±0.12\n15.03±2.92\n1.171±0.10\nLrecon + Lcontrast\n0.751±0.10\n10.92±2.28\n1.801±0.47\n0.604±0.14\n14.85±2.77\n0.993±0.21\nLrecon + Lsmooth + Lcontrast (ours)\n0.763±0.07\n10.38±2.59\n1.458±0.31\n0.646±0.04\n12.76±2.52\n0.892±0.05\nLrecon + Lbyol\n0.744±0.18\n11.41±3.29\n1.693±0.77\n0.619±0.13\n14.73±2.39\n1.153±0.89\nLrecon + Lsmooth + Lbyol\n0.761±0.08\n10.41±2.12\n1.462±0.53\n0.649±0.08\n12.88±2.44\n0.894±0.10\nSagittal View\nCoronal View\nAxial View\nCLMorph\nVoxelMorph\nFig. 7.\nDeformation ﬁeld generated from the non-contrasted feature maps\n(VoxelMorph) and contrasted feature maps (our CLMorph). The red box\nhighlight the region with large deformation, in which deformation ﬁeld\ngenerated from contrasted feature maps is smoother than the deformation\nﬁeld generated from non-contrasted feature maps.\na signiﬁcant margin. This behaviour is consistent on the per\nregion results on the two datasets whose results are reported\nin Fig. 2 and Fig. 3. From these boxplots, we can observe that\nthe performance gain of our technique is signiﬁcant for all\nregions in both datasets. These comparisons, therefore, show\nthat the combination of the contrastive learning mechanism\nand the registration architecture can better discriminate brain\nstructures and generate more accurate segmentation outputs.\nVisual comparisons. To further support our numerical\nresults, we present a set of visual comparisons of a selection\nof images for our technique and the compared ones. We\nstart by displaying the unsupervised segmentation outputs in\nFigure 4. By visual inspection, we observe that the segmen-\ntation outputs, generated from FAIM and VoxelMorph, tend\nto fail to segment correctly several relevant regions of the\nbrain, and they do not adapt correctly to the contour of the\nbrain structure. The zoom-in views in Figure 4 highlight these\neffects – for example the red region on the brain that the\ncompared techniques fail to correctly segment; and the yellow\nregion that is not well-captured by FAIM and VoxelMorph. By\ncontrast, our technique was able to perform better in this regard\nby capturing ﬁne details in both cases. Overall, our technique\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.001\n0.01\n0.1\n1\n10\nAlpha\n0.001\n0.01\n0.1\n1\n10\nBeta\n0.71\n0.715\n0.72\n0.725\n0.73\n0.735\n0.74\n0.745\n0.75\n0.755\n0.76\nDice\n0.763\nFig. 8. Hyper-parameter Searching. We display different combination values\nfor α and β used in (9). The parameters values are evaluated using the Dice\nmetric. The best combination is α = 1 and β = 0.01, which gives the highest\nDice result of 0.763.\nis able to accommodate better with the ﬁne details in the brain\nregions, producing segmentation closer to the ground truth.\nWe also included the visualisation of the deformation ﬁeld\ngenerated from the contrasted feature (w/ contrastive loss)\nand non-contrasted feature (w/o contrastive loss). According\nto Fig. 7, when the deformation between the moving and\nﬁxed images is large, the deformation ﬁeld generated from\nthe contrasted feature is smoother, because the contrasted\nfeature maps have high alikeness. Whilst the deformation ﬁeld\ngenerated from non-contrasted features tends to be less smooth\nbecause there is a huge difference between the feature maps\nextracted from the moving and ﬁxed images.\nStatistical Analysis. We support statistically our visual\nand numerical ﬁndings. To do this, we run a Friedman test\nfor multiple comparisons, χ2(5)=29.78,p < 0.0001, followed\nby a Wilcoxon test for pair-wise comparisons. As shown in\nFig. 6, the pair-wise test between groups revealed a statistically\nsigniﬁcant difference metric-wise of our technique against the\nrest of compared techniques.\nBy-product registration accuracy. As a by-product of\n10\nTABLE IV\nNUMERICAL COMPARISON OF OUR TECHNIQUE VS SOTA TECHNIQUES\nFOR THE ACDC CARDIAC DATASET.\nDice\nHD\nASSD\nSyN [57]\n0.761±0.12\n11.9±1.79\n1.581±0.07\nVoxelMorph [26]\n0.787±0.09\n11.8±2.01\n1.591±0.05\nLVM-S3 [21]\n0.795±0.06\n9.90±2.22\n1.544±0.06\nCLMorph (Ours)\n0.810±0.05\n10.23±2.08\n1.482±0.05\nour methods, we also provide the numerical and the visual\nregistration results. Based on the learned deformation ﬁeld,\nwe register the segmentation mask of the unaligned image\nto the coordinate system of the reference image to get the\naligned segmentation mask. Then compare it with the refer-\nence image’s mask to get the dice scores; see Table II. We also\npresent visual results displayed in Figure 5. We observe that\nour technique was able to better preserve the ﬁne details in\nthe registration outputs than the compared techniques. Most\nnotable, this can be observed in the zoom-in views. For\nexample, one can observe that our technique is able to preserve\nbetter the brain structures whilst the compared techniques fail\nin these terms. Moreover, we can see that our registration\noutputs are closer to the reference image displaying less blurry\neffects whilst keeping ﬁne details. We now underline the\nmain message of our paper: our optimisation model fulﬁlls\nthe intended purposes, and at this point in time our technique\noutperforms the SOTA unsupervised segmentation results.\nHyperparameter searching. We also run a set of new\nexperiments for the hyperparameter analysis. Speciﬁcally, we\nset the weight for the smooth loss (α) and contrastive loss\n( β) as 0.001, 0.01, 0.1, 1, 10, respectively. As shown in\nFig. 8, the best combination is our current setting (alpha=1\nand beta=0.01), which gives the highest dice result of 0.763.\nAblation study. To demonstrate that each component of our\ntechnique fulﬁlls a purpose, we include an ablation study to\nevaluate their inﬂuence on performance. We consider our three\nmajor components in our model whose results are displayed in\nTable III. Our ablation study is performed on the two bench-\nmark datasets, in order to understand the general behaviour\nof our technique. The results are displayed in terms of the\nDice metric and we progressively evaluate our method with\ndifferent losses combinations. From the results in Table III,\nwe can observe that whilst the contrastive mechanism, in\nour technique, indeed provides a positive effect in terms of\nperformance, it beneﬁts our carefully designed components.\nIn this work, we also pose the question of – at this point\nin time, what is the performance gap between supervised\nand unsupervised segmentation techniques? To respond to this\nquestion, we use as baseline U-Net [5]. From the results,\nin Table III, one can observe that our technique opens the\ndoor for further investigation in unsupervised techniques, as\nthe performance shows potential towards the one reported by\nsupervised techniques.\nAdditional experiments on a cardiac dataset. We also\nprovide a set of new results using an additional cardiac dataset\ncalled ACDC [64]. The dataset is for medical image registra-\ntion. It is composed of 150 labelled images. We selected 100\nimages for training whilst the remaining 50 images for testing.\nThe results are reported in Table IV. From the results, we\nobserve that our model outperforms the compared techniques\nin terms of Dice and ASSD metrics by a large margin.\nWhilst readily competing against LVM-S3 in terms of HD\nmetric. These results further support the generalisation and\nperformance of our CLMorph technique.\nV. CONCLUSION\nThis paper presents a novel CNN-based registration archi-\ntecture for unsupervised medical image segmentation. Firstly,\nwe proposed to use unsupervised registration-based segmenta-\ntion by capturing the image-to-image transformation mapping.\nSecondly, to promote the image- and feature-level learning, for\nbetter segmentation results, we embed a contrastive feature\nlearning mechanism into the registration architecture. Our\nnetwork can learn to be more discriminative to the different\nimages via contrasting unaligned image and reference images.\nWe show that our carefully designed optimisation model\nmitigates some major drawbacks of existing unsupervised\ntechniques. We demonstrate, through several experiments, that\nour technique is able to report state-of-the-art results for\nunsupervised medical image segmentation. Whilst supervised\ntechniques still report better performance than unsupervised\nones, in this work, we show the potentials in terms of\nperformance when no labels are available. This is of a great\ninterest particularly in domains such as the medical area where\nannotations require expert knowledge and are expensive.\nACKNOWLEDGEMENTS\nLL gratefully acknowledges the ﬁnancial support from a\nGSK scholarship and a Girton College Graduate Research\nFellowship at the University of Cambridge. AIAR gratefully\nacknowledges the ﬁnancial support of the CMIH, CCIMI\nand C2D3 University of Cambridge. CBS acknowledges sup-\nport from the Philip Leverhulme Prize, the Royal Soci-\nety Wolfson Fellowship, the EPSRC grants EP/S026045/1\nand EP/T003553/1, EP/N014588/1, EP/T017961/1, the Well-\ncome Innovator Award RG98755, the Leverhulme Trust\nproject Unveiling the invisible, the European Union Horizon\n2020 research and innovation programme under the Marie\nSkodowska-Curie grant agreement No. 777826 NoMADS, the\nCantab Capital Institute for the Mathematics of Information\nand the Alan Turing Institute.\nREFERENCES\n[1] C. Chen, C. Qin, H. Qiu, G. Tarroni, J. Duan, W. Bai, and D. Rueckert,\n“Deep learning for cardiac image segmentation: A review,” Frontiers in\nCardiovascular Medicine, vol. 7, p. 25, 2020.\n[2] J. Liu, A. I. Aviles-Rivero, H. Ji, and C.-B. Sch¨onlieb, “Rethinking\nmedical image reconstruction via shape prior, going deeper and faster:\nDeep joint indirect registration and reconstruction,” Medical Image\nAnalysis, vol. 68, p. 101930, 2021.\n[3] S. Hu, E. A. Hoffman, and J. M. Reinhardt, “Automatic lung segmen-\ntation for accurate quantitation of volumetric X-ray CT images,” IEEE\ntransactions on medical imaging, vol. 20, no. 6, pp. 490–498, 2001.\n[4] A. de Brebisson and G. Montana, “Deep neural networks for anatomical\nbrain segmentation,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition (CVPR), 2015, pp. 20–28.\n11\n[5] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\nworks for biomedical image segmentation,” in International Conference\non Medical image computing and computer-assisted intervention (MIC-\nCAI).\nSpringer, 2015, pp. 234–241.\n[6] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,\n“nnu-net: a self-conﬁguring method for deep learning-based biomedical\nimage segmentation,” Nature Methods, vol. 18, no. 2, pp. 203–211, 2021.\n[7] H. Chen, Q. Dou, L. Yu, J. Qin, and P.-A. Heng, “VoxResNet: Deep\nvoxelwise residual networks for brain segmentation from 3D MR\nimages,” NeuroImage, vol. 170, pp. 446–455, 2018.\n[8] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical\nimage analysis,” arXiv preprint arXiv:1904.00625, 2019.\n[9] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, and A. V. Dalca,\n“Data augmentation using learned transformations for one-shot medical\nimage segmentation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 8543–8553.\n[10] S. Wang, S. Cao, D. Wei, R. Wang, K. Ma, L. Wang, D. Meng,\nand Y. Zheng, “Lt-net: Label transfer by learning reversible voxel-\nwise correspondence for one-shot medical image segmentation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 9162–9171.\n[11] N. M. Portela, G. D. Cavalcanti, and T. I. Ren, “Semi-supervised\nclustering for MR brain image segmentation,” Expert Systems with\nApplications, vol. 41, no. 4, pp. 1492–1497, 2014.\n[12] W. Cui, Y. Liu, Y. Li, M. Guo, Y. Li, X. Li, T. Wang, X. Zeng, and\nC. Ye, “Semi-supervised brain lesion segmentation with an adapted\nmean teacher model,” in International Conference on Medical image\ncomputing and computer-assisted intervention (MICCAI).\nSpringer,\n2019, pp. 554–565.\n[13] B. Alfano, A. Brunetti, E. M. Covelli, M. Quarantelli, M. R. Panico,\nA. Ciarmiello, and M. Salvatore, “Unsupervised, automated segmenta-\ntion of the normal brain using a multispectral relaxometric magnetic\nresonance approach,” Magnetic resonance in medicine, vol. 37, no. 1,\npp. 84–93, 1997.\n[14] C. Lee, S. Huh, T. A. Ketter, and M. Unser, “Unsupervised connectivity-\nbased thresholding segmentation of midsagittal brain MR images,”\nComputers in biology and medicine, vol. 28, no. 3, pp. 309–338, 1998.\n[15] A. V. Dalca, J. Guttag, and M. R. Sabuncu, “Anatomical priors in\nconvolutional networks for unsupervised biomedical segmentation,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition (CVPR), 2018, pp. 9290–9299.\n[16] A. V. Dalca, E. Yu, P. Golland, B. Fischl, M. R. Sabuncu, and\nJ. E. Iglesias, “Unsupervised deep learning for bayesian brain MRI\nsegmentation,” in International Conference on Medical image computing\nand computer-assisted intervention (MICCAI). Springer, 2019, pp. 356–\n365.\n[17] H. Ng, S. Ong, K. Foong, P.-S. Goh, and W. Nowinski, “Medical\nimage segmentation using k-means clustering and improved watershed\nalgorithm,” in 2006 IEEE southwest symposium on image analysis and\ninterpretation.\nIEEE, 2006, pp. 61–65.\n[18] B. N. Li, C. K. Chui, S. Chang, and S. H. Ong, “Integrating spatial\nfuzzy clustering with level set methods for automated medical image\nsegmentation,” Computers in biology and medicine, vol. 41, no. 1, pp.\n1–10, 2011.\n[19] A. Jose, S. Ravi, and M. Sambath, “Brain tumor segmentation using\nk-means clustering and fuzzy c-means algorithms and its area calcu-\nlation,” International Journal of Innovative Research in Computer and\nCommunication Engineering, vol. 2, no. 3, 2014.\n[20] K. Tian, S. Zhou, and J. Guan, “DeepCluster: A general clustering\nframework based on deep learning,” in Joint European Conference on\nMachine Learning and Knowledge Discovery in Databases.\nSpringer,\n2017, pp. 809–825.\n[21] J. Krebs, H. Delingette, B. Mailh´e, N. Ayache, and T. Mansi, “Learning\na probabilistic model for diffeomorphic registration,” IEEE transactions\non medical imaging, vol. 38, no. 9, pp. 2165–2176, 2019.\n[22] S. Zhao, T. Lau, J. Luo, I. Eric, C. Chang, and Y. Xu, “Unsupervised 3d\nend-to-end medical image registration with volume tweening network,”\nIEEE journal of biomedical and health informatics, vol. 24, no. 5, pp.\n1394–1404, 2019.\n[23] B. D. de Vos, F. F. Berendsen, M. A. Viergever, H. Sokooti, M. Staring,\nand I. Iˇsgum, “A deep learning framework for unsupervised afﬁne and\ndeformable image registration,” Medical image analysis, vol. 52, pp.\n128–143, 2019.\n[24] J. Zhang, “Inverse-consistent deep networks for unsupervised de-\nformable image registration,” arXiv preprint arXiv:1809.03443, 2018.\n[25] A. V. Dalca, G. Balakrishnan, J. Guttag, and M. R. Sabuncu, “Unsu-\npervised learning for fast probabilistic diffeomorphic registration,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention.\nSpringer, 2018, pp. 729–738.\n[26] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca,\n“Voxelmorph: a learning framework for deformable medical image\nregistration,” IEEE transactions on medical imaging, vol. 38, no. 8, pp.\n1788–1800, 2019.\n[27] ——, “An unsupervised learning model for deformable medical image\nregistration,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2018, pp. 9252–9260.\n[28] T. C. Mok and A. Chung, “Large deformation diffeomorphic image\nregistration with laplacian pyramid networks,” in International Confer-\nence on Medical Image Computing and Computer-Assisted Intervention.\nSpringer, 2020, pp. 211–221.\n[29] S. Zhao, Y. Dong, E. I. Chang, Y. Xu et al., “Recursive cascaded\nnetworks for unsupervised medical image registration,” in Proceedings\nof the IEEE/CVF international conference on computer vision, 2019,\npp. 10 600–10 610.\n[30] M. P. Heinrich, “Closing the gap between deep and conventional\nimage registration using probabilistic dense displacement networks,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention.\nSpringer, 2019, pp. 50–58.\n[31] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple frame-\nwork for contrastive learning of visual representations,” arXiv preprint\narXiv:2002.05709, 2020.\n[32] J.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. Richemond, E. Buchatskaya,\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al.,\n“Bootstrap your own latent-a new approach to self-supervised learning,”\nAdvances in neural information processing systems, vol. 33, pp. 21 271–\n21 284, 2020.\n[33] M. Jaderberg, K. Simonyan, A. Zisserman et al., “Spatial transformer\nnetworks,” in Advances in neural information processing systems, 2015,\npp. 2017–2025.\n[34] H. Suzuki and J.-i. Toriwaki, “Automatic segmentation of head MRI im-\nages by knowledge guided thresholding,” Computerized medical imaging\nand graphics, vol. 15, no. 4, pp. 233–240, 1991.\n[35] B. Fischl, D. H. Salat, E. Busa, M. Albert, M. Dieterich, C. Haselgrove,\nA. Van Der Kouwe, R. Killiany, D. Kennedy, S. Klaveness et al., “Whole\nbrain segmentation: automated labeling of neuroanatomical structures in\nthe human brain,” Neuron, vol. 33, no. 3, pp. 341–355, 2002.\n[36] Y. Yang, A. Tannenbaum, D. Giddens, and A. Stillman, “Automatic seg-\nmentation of coronary arteries using bayesian driven implicit surfaces,”\nin 2007 4th IEEE International Symposium on Biomedical Imaging:\nFrom Nano to Macro.\nIEEE, 2007, pp. 189–192.\n[37] C. Baur, B. Wiestler, S. Albarqouni, and N. Navab, “Scale-space\nautoencoders for unsupervised anomaly segmentation in brain mri,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention.\nSpringer, 2020, pp. 552–561.\n[38] Y. Song, T. Zhou, J. Y.-C. Teoh, J. Zhang, and J. Qin, “Unsupervised\nlearning for ct image segmentation via adversarial redrawing,” in In-\nternational Conference on Medical Image Computing and Computer-\nAssisted Intervention.\nSpringer, 2020, pp. 309–320.\n[39] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast\nfor unsupervised visual representation learning,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition (CVPR),\n2020, pp. 9729–9738.\n[40] Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,” arXiv\npreprint arXiv:1906.05849, 2019.\n[41] X. Zhao, R. Vemulapalli, P. Mansﬁeld, B. Gong, B. Green, L. Shapira,\nand Y. Wu, “Contrastive learning for label-efﬁcient semantic segmenta-\ntion,” arXiv preprint arXiv:2012.06985, 2020.\n[42] K. Chaitanya, E. Erdil, N. Karani, and E. Konukoglu, “Contrastive\nlearning of global and local features for medical image segmentation\nwith limited annotations,” arXiv preprint arXiv:2006.10511, 2020.\n[43] S.-I. Amari, “\\alpha -divergence is unique, belonging to both f-\ndivergence and bregman divergence classes,” IEEE Transactions on\nInformation Theory, vol. 55, no. 11, pp. 4925–4931, 2009.\n[44] C. Stein et al., “A bound for the error in the normal approximation to the\ndistribution of a sum of dependent random variables,” in Proceedings\nof the Sixth Berkeley Symposium on Mathematical Statistics and Prob-\nability, Volume 2: Probability Theory.\nThe Regents of the University\nof California, 1972.\n[45] T. Minka et al., “Divergence measures and message passing,” Technical\nreport, Microsoft Research, Tech. Rep., 2005.\n[46] S. Kullback and R. A. Leibler, “On information and sufﬁciency,” The\nannals of mathematical statistics, vol. 22, no. 1, pp. 79–86, 1951.\n[47] T. M. Cover, Elements of information theory.\nJohn Wiley & Sons,\n1999.\n12\n[48] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by\nlearning an invariant mapping,” in 2006 IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition (CVPR’06), vol. 2.\nIEEE, 2006, pp. 1735–1742.\n[49] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning\nvia non-parametric instance discrimination,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n3733–3742.\n[50] D. W. Shattuck, M. Mirza, V. Adisetiyo, C. Hojatkashani, G. Salamon,\nK. L. Narr, R. A. Poldrack, R. M. Bilder, and A. W. Toga, “Construction\nof a 3d probabilistic atlas of human cortical structures,” Neuroimage,\nvol. 39, no. 3, pp. 1064–1080, 2008.\n[51] A. Klein and J. Tourville, “101 labeled brain images and a consistent\nhuman cortical labeling protocol,” Frontiers in neuroscience, vol. 6, p.\n171, 2012.\n[52] L. R. Dice, “Measures of the amount of ecologic association between\nspecies,” Ecology, vol. 26, no. 3, pp. 297–302, 1945.\n[53] D. P. Huttenlocher, G. A. Klanderman, and W. J. Rucklidge, “Comparing\nimages using the hausdorff distance,” IEEE Transactions on pattern\nanalysis and machine intelligence, vol. 15, no. 9, pp. 850–863, 1993.\n[54] V. Yeghiazaryan and I. D. Voiculescu, “Family of boundary overlap\nmetrics for the evaluation of medical image segmentation,” Journal of\nMedical Imaging, vol. 5, no. 1, p. 015006, 2018.\n[55] L. Liu, X. Hu, L. Zhu, C.-W. Fu, J. Qin, and P.-A. Heng, “ψ-net:\nStacking densely convolutional lstms for sub-cortical brain structure\nsegmentation,” IEEE transactions on medical imaging, vol. 39, no. 9,\npp. 2806–2817, 2020.\n[56] F.-X. Vialard, L. Risser, D. Rueckert, and C. J. Cotter, “Diffeomorphic\n3D image registration via geodesic shooting using an efﬁcient adjoint\ncalculation,” International Journal of Computer Vision, vol. 97, no. 2,\npp. 229–241, 2012.\n[57] B. B. Avants, N. Tustison, G. Song et al., “Advanced normalization tools\n(ants),” Insight j, vol. 2, no. 365, pp. 1–35, 2009.\n[58] D. Kuang and T. Schmah, “Faim–a convnet method for unsupervised\n3d medical image registration,” in International Workshop on Machine\nLearning in Medical Imaging.\nSpringer, 2019, pp. 646–654.\n[59] L. Liu, X. Hu, L. Zhu, and P.-A. Heng, “Probabilistic multilayer\nregularization network for unsupervised 3d brain image registration,” in\nInternational Conference on Medical image computing and computer-\nassisted intervention (MICCAI).\nSpringer, 2019, pp. 346–354.\n[60] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An\nimperative style, high-performance deep learning library,” in Advances\nin neural information processing systems, 2019, pp. 8026–8037.\n[61] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:\nSurpassing human-level performance on imagenet classiﬁcation,” in\nProceedings of the IEEE international conference on computer vision,\n2015, pp. 1026–1034.\n[62] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” Communications of the ACM,\nvol. 60, no. 6, pp. 84–90, 2017.\n[63] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[64] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A.\nHeng, I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester et al.,\n“Deep learning techniques for automatic mri cardiac multi-structures\nsegmentation and diagnosis: is the problem solved?” IEEE transactions\non medical imaging, vol. 37, no. 11, pp. 2514–2525, 2018.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-11-17",
  "updated": "2022-07-20"
}