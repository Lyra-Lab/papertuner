{
  "id": "http://arxiv.org/abs/2011.09857v1",
  "title": "On tuning deep learning models: a data mining perspective",
  "authors": [
    "M. M. Ozturk"
  ],
  "abstract": "Deep learning algorithms vary depending on the underlying connection\nmechanism of nodes of them. They have various hyperparameters that are either\nset via specific algorithms or randomly chosen. Meanwhile, hyperparameters of\ndeep learning algorithms have the potential to help enhance the performance of\nthe machine learning tasks. In this paper, a tuning guideline is provided for\nresearchers who cope with issues originated from hyperparameters of deep\nlearning models. To that end, four types of deep learning algorithms are\ninvestigated in terms of tuning and data mining perspective. Further, common\nsearch methods of hyperparameters are evaluated on four deep learning\nalgorithms. Normalization helps increase the performance of classification,\naccording to the results of this study. The number of features has not\ncontributed to the decline in the accuracy of deep learning algorithms. Even\nthough high sparsity results in low accuracy, a uniform distribution is much\nmore crucial to reach reliable results in terms of data mining.",
  "text": "research paper ()\nOn tuning deep learning models: a data\nmining perspective\nMuhammed Maruf ¨OZT¨URK\nComputer Engineering Department, Suleyman\nDemirel University\nIsparta, TURKEY\nemail: muhammedozturk@sdu.edu.tr\nAbstract. Deep learning algorithms vary depending on the underlying\nconnection mechanism of nodes of them. They have various hyperpa-\nrameters that are either set via speciﬁc algorithms or randomly chosen.\nMeanwhile, hyperparameters of deep learning algorithms have the po-\ntential to help enhance the performance of the machine learning tasks.\nIn this paper, a tuning guideline is provided for researchers who cope\nwith issues originated from hyperparameters of deep learning models. To\nthat end, four types of deep learning algorithms are investigated in terms\nof tuning and data mining perspective. Further, common search meth-\nods of hyperparameters are evaluated on four deep learning algorithms.\nNormalization helps increase the performance of classiﬁcation, according\nto the results of this study. The number of features has not contributed\nto the decline in the accuracy of deep learning algorithms. Even though\nhigh sparsity results in low accuracy, a uniform distribution is much more\ncrucial to reach reliable results in terms of data mining.\n1\nIntroduction\nDeep learning (DL) dates back to 1999 [34] when GPU was ﬁrst developed.\nIt is a sophisticated type of neural network (NN) that has a limited num-\nber of hidden layers compared to DL. It has been reviewed in several studies\n[22, 66, 18] which show to what extent DL has progressed in recent years. DL\nComputing Classiﬁcation System 1998: I.2.6, I.5.1\nMathematics Subject Classiﬁcation 2010: 68T20\nKey words and phrases: deep learning, hyperparameter optimization, classiﬁcation, data\nmining\n1\narXiv:2011.09857v1  [cs.LG]  19 Nov 2020\n2\nM.M. Ozturk\nalgorithms have various hyperparameters that are mostly conﬁgurable. Some\nof these hyperparameters are learning rate, hidden layers, and the number of\niterations. The number of hyperparameters changes depending on the type of\nDL. However, existing approaches generally refer to a speciﬁc hyperparameter\nin order to delve into how the results take shape depending on the research\nproblem. An extensive study explaining the correlation between hyperparam-\neters and performance evaluation in terms of the type of DL algorithms is\nneeded in this ﬁeld.\nDL algorithms are developed based on shallow neural networks that have\na limited number of hidden layers. In this respect, the low computational ca-\npacity of shallow neural networks limits their functional potential. To solve\nthat problem, a great number of hidden layers is designed to give DL models\nimpressive computational capacity [53] compared to the traditional neural net-\nworks. DL algorithms are mainly executed for speciﬁc tasks, including classiﬁ-\ncation [31, 10, 9] and regression [23, 67, 60]. The purpose of those applications\nis to reduce errors or to increase the accuracy of classiﬁcation experiments. On\nthe other hand, regression aims at reducing mean squared error on the basis\nof a regression rule.\nHyperparameter optimization and tuning parameters are used interchange-\nably in related studies [38, 54, 68]. Attaining an optimal hyperparameter set\nmeans that the most suitable conﬁguration yielding the best performance\nhas been found. However, optimal hyperparameters change depending on the\nstructural properties of the data set. For example, a highly balanced data set\ncould lead to overﬁtting. More importantly, dividing a data set into train-\ning, validation, and testing parts makes the optimization results method more\nreliable.\nTo date, hyperparameter optimization strategies implemented to DL algo-\nrithms have been successful. Notwithstanding the success of these strategies,\na comprehensive study that could make a major advance in understanding\nto what extent hyperparameter optimization is depended on the type of DL\nis needed in this ﬁeld. To address this issue, this paper reveals which ways\nare the best to conduct hyperparameter tuning for DL methods. To that end,\nfour DL algorithms including deep belief network (DBN), recurrent neural net-\nwork (RNN), feed-forward neural network (FFNN), and stacked autoencoder\n(SAE) are analyzed. Moreover, some data mining strategies are involved in\nthe experiment to enhance the comprehensiveness of the study.\nThe remainder of the paper is organized as follows: Section 2 describes DL\nmodels and summarizes tuning studies using DL. A data processing perspec-\ntive with regard to DL is presented in Section 3. The most common hyper-\nOn tuning deep learning\n3\nparameter search methods are given in Section 4. Experimental settings are\ndetailed in Section 5. The results of the experiment are elaborated in Section\n6. Last, overall conclusions are drawn and discussed in Section 7.\n2\nDeep learning\n2.1\nDeep belief network\nA DBN is constructed via various stacked RBMs or Autoencoders [24]. All\nthe layers of DBN are directed except for the top two layers. An undirected\nconnection provides an associative memory. Further, unsupervised learning can\nbe conducted through a DBN as well as classiﬁcation. Observed variables are\nextracted from the lower layers. Visible units in a DBN take input as binary or\nreal data. Figure 1 shows a general structure of DBN. h0 refers to the lowest\nlayer which takes input data. The elements in that layer are called visible\nunits VU. Hidden layers h and VU establish the model according to Equation\n1. Here conditional distributions of VU are represented with M(hi|hi+1). For\ntop level of DBN, joint distribution is denoted with M(hn-1,hn).\nFigure 1: An overview of DBN.\nM(VU, h1, ..., hn) =\nn\nY\ni=0\nM(hi|hi+1).M(hn−1, hn)\n(1)\n4\nM.M. Ozturk\n2.2\nFeed-forward neural network\nOne of the fundamental types of DL algorithms is FFNN which has a fully-\nconnected structure [20]. Unlike the convolutional neural network (CNN),\nFFNN does not have a convolutional layer. On the other hand, CNN has\na backward propagation in the convolution layer. That property makes CNN\na good alternative in image classiﬁcation in which a great number of images\nare ﬁltered via convolutional layers.\nFully-connected structure of FFNN creates a signiﬁcant computational bur-\nden for machine learning tasks. Performing a pruning on the DL network may\nalleviate that burden. The hidden number of layers and hidden units in each\nlayer are of great importance to solve a given problem. Although a wrong con-\nﬁguration of hyperparameters sometimes gives promising results, it may lead\nto overﬁtting. Allocating a good representative validation set is a possible so-\nlution to avoid overﬁtting. Getting more training data is another option that\nrequires a large memory.\n2.3\nRecurrent neural network\nRNN provides a powerful alternative for predicting the patterns of sequen-\ntial data [21]. Text sequence prediction and speech recognition are the main\napplication areas of RNN. It gives each output as input in the hidden layers\nwhich generates a memorizing mechanism. RNN is much more useful in time\nseries prediction because each hidden layer remembers its previous input. If a\nclassiﬁcation is performed with RNN, it assumes that features are somehow\ncorrelated. For that reason, training RNN takes far more time than that of\nother types of DL. If previous state and input state are represented with ps\nand is, respectively. Current state cs of an RNN can be formulated with the\nfollowing function:\ncs = f(ps, is)\n(2)\nwhere cs−1 = ps. If tanh is chosen to establish activation function, the formula\ngiven below describes the activation formula:\ncs = tanh(wnn.ps + win.is)\n(3)\nwhere wnn is the weight of recurrent neuron and win is the wight of input\nneuron. Equation 4 describes the general output of a RNN in which wo denotes\nthe weight of output layer.\ny = wo.cs\n(4)\nOn tuning deep learning\n5\nTable 1: Overview of some hyperparameters of DL.\nName\nDescription\nMethod\niteration\nthe number of iterations over training data to train the model DBN, FFNN, RNN, SAE\nbatch size\nthe batch size used for training DBN, FFNN, RNN, SAE\nhidden dropout\ndrop out fraction for hidden layer\nDBN, FFNN, SAE\nvisible dropout\ndrop out fraction for input layer\nDBN, SAE\nlearning rate\nlearning rate for gradient descent DBN, FFNN, RNN, SAE\nhidden dim\ndimensions of hidden layers or number of units of hidden layers DBN, FFNN, RNN, SAE\n2.4\nStacked autoencoder\nAn SAE consists of a lot of autoencoders that each of them has a single layer\n[49]. An autoencoder has two parts: encoder and decoder. A high dimensional-\ninput is coded in an encoder. On the other hand, a decoder transforms a coded\ninput into a high-dimensional input. More speciﬁcally, SAE oﬀers a mechanism\nfor stacking autoencoders, thereby enabling compression for input data. The\nnumber of inputs of autoencoders decreases as the level of SAE increases. An\nSAE thus is very eﬀective for applications in which data compression is needed.\n2.5\nHyperparameter tuning of deep learning\nThis section details DL studies including hyperparameter optimization. The\nbrief gathered from the most recent studies presents the current status of\nhyperparameter optimization in DL.\nSome parameters are considered when optimizing a DL algorithm as follows:\nlearning rate, loss function, mini-batch size, the number of training iterations,\nand momentum. Table 1 presents hyperparameters which are common in DL\ntuning studies. The column namely ”Method” includes DL algorithms that\nare involved in the experiment. Despite the fact that the number of hyperpa-\nrameters of a DL algorithm can be up to 11, some experimental constraints\nsuch as time and memory conﬁne the bound of a tuning experiment.\nIlievski et al. [26] developed a new algorithm namely HORD for optimizing\nDL models. The evaluation performed with 200 iterations showed that HORD\noutperformed the other three comparison methods in terms of the validation\nerror. HORD is much more suitable for high-dimensional problems and it runs\nnearly six times faster than its counterparts.\nYoo [74] asserts that nonlinear search methods ﬁnd optimal hyperparameters\nfaster and with relatively less complexity compared to random Search. He also\ndetected that the success of derivative-free methods can be improved through\nnonlinear search methods. If a parallelization is required in hyperparameter\noptimization, some methods such as Bayesian optimization can not meet ex-\n6\nM.M. Ozturk\npectations of optimization above a certain level. To address that problem,\nLoshchilov and Frank Hutter [41] proposed a novel method called CMA-ES\nthat does not include any derivative operation. The method signiﬁcantly alle-\nviates the computational burden thanks to its design which is fully compatible\nwith parallelization. To increase the speed of DL networks, Domhan et al. [15]\ndeveloped a technique based on controlling the learning curve. The success\nachieved by that method is two times greater than that of the state-of-the-art\nmethods. In [13], a derivative-free method was presented for hyperparameter\noptimization. Despite the fact that it does not always reach global optimum,\nthree benchmarks yielded high accuracy for the experimental data set. Yaseen\net al. [73] stressed that setting the learning rate of a DL model to low-value\nresults in successful classiﬁcation for video data set. In addition to engineering\napplications, hyperparameter optimization was also evaluated in theoretical\nstudies. For instance, Baldi and Sadowski [4] utilized a Bayes optimization\nalgorithm to detect the decay in the Higgs Boson particle. In the detection\nof decay, an improvement of up to 25% was able to achieve for both shal-\nlow and deep networks. Traditional methods such as Bayesian optimization\nrequire a piece of expert knowledge. To solve this problem, Nelder-Mead was\ntried [45]. According to the obtained results, Nelder-Mead convergences faster\nthan other methods. Young et al.’s work [75] proposes that using historical re-\nsults along with an evolutionary approach produces more reliable results than\nthose of random search. However, the method developed by them needs to be\nvalidated as it was only tested on one type of DL network.\n2.5.1\nLearning rate\nTuning the learning rate of a DL algorithm requires an automatic control mech-\nanism to alleviate the computational burden. To that end, Duchi et al. [16]\ndevised an adaptive online learning algorithm called ADAGRAD which estab-\nlishes an inverse relationship between the occurrence of features and learning\nrate. However, ADAGRAD is very sensitive to the initialization of parameters\nof gradients that leads to giving low learning rate for some parts of training.\nADADELTA [76] addressed that issue, thereby controlling ﬁrst-order informa-\ntion. Although ADADELTA outperformed stochastic gradient descent (SGD)\nand momentum in test error, it needs to be re-designed to trivial computations.\nZhao et al. [79] utilized an energy neuron model to decide the learning rate of\nDL by analyzing features. Further, they pointed out that there are tradeoﬀs\nin all couple hyperparameters. To set the learning rate, in [55], the laplacian\nscore is employed to increase the success of classiﬁcation. Laplacian score has\nOn tuning deep learning\n7\na great potential to give information about the signiﬁcance of neurons of DL.\nIn [27], cross-validation helped to attain optimistic biases in the learning rate\nof DL. In addition, a layer-speciﬁc adaptive scheme [56] was found beneﬁcial\nto speed up learning in the initial layers of DL. Smith [59] depicted that using\na cyclic learning rate improves the classiﬁcation accuracy of DL. He also noted\nthat optimal classiﬁcation result is obtained via a cyclic learning rate in few\niterations.\nThe studies mentioned above mostly recommend comprehensive trials on the\nlearning rate of DL. Further, each adaptive learning scheme for the learning\nrate is highly dependent on the type of DL model established to conduct a\nspeciﬁc machine learning task.\n2.5.2\nBatch size\nThe batch size determines the number of instances that are used in training\nfor each update of model parameters. Employing a large batch size requires a\nhigh memory capacity. Therefore, batch size should be optimized in compliance\nwith the machine conﬁguration.\nLi et al. [37] developed a batch normalization method called AdaBN for DL\nmodels. They concluded that setting a small number of instances for batch\nsize may not yield consistent accuracy. For that reason, a threshold should\nbe set for batch size. Above this value, adding more instances to the batch\nsize does not change the accuracy. Besides reaching a stable accuracy, batch\nsize helps accelerate DL models according to the experiment performed by\nLiu et al. [39]. Santurkar et al. [52] examined the internal covariate shift to\nobserve the advantages of employing batch normalization. They stressed that\nbatch normalization makes gradients of training more predictive that results\nin faster yet eﬀective optimization. For a robustness analysis of DL, Yao et\nal. [72] devised a hessian-based experiment on the CIFAR-10 data set. The\nrobustness of a DL model is highly dependent on the batch size according to\ntheir experiment. They also depicted that batch size should be kept small.\nBjorck et al. [7] argue that activations of DL grow at a fast pace if the learn-\ning rate is too large. In their study, batch normalization was found as the\nsole way to prevent the explosion of DL networks. For complex optimization\nproblems, a mini-batch stochastic gradient descent was proposed in [36]. The\nmost important facility of the method is that it helps keep the convergence\nrate at a reasonable level even if the batch size increases signiﬁcantly. Some\nresearchers preferred to analyze a speciﬁc DL model in terms of batch size. For\ninstance, Laurent et al. [33] investigated the eﬀects of optimizing batch size\n8\nM.M. Ozturk\non recurrent neural networks. They proposed to use batch normalization to\nachieve fast training in recurrent neural networks. Besides fast training, using\nan optimal batch size reduces the need for parameter updates [57].\n2.5.3\nHidden node and hidden layer\nDesigning many hidden layers in DL models leads to a high memory require-\nment. Further, such models spend too much time to complete training. To\naddress this problem, Alvarez and Salzmann [1] proposed an approach to de-\ntermine the number of neurons in layers of a DL network. The method achieved\na great speedup at testing. Some works are designed for determining both the\nnumber of neurons and the number of hidden layers. Thomas et al. [62] per-\nformed such an experiment for feed-forward neural networks. They were able\nto achieve high accuracy in classiﬁcation. Another study was performed by\nXu et al. [71]. They revealed that despite the fact that using a great number\nof hidden layers sharpens the learning model with respect to the training ac-\ncuracy, they remarkably increase the eﬀort needed for training. Morales [30]\ninvestigated multi-layer perceptron for ﬁnding the optimal number of hidden\nneurons. They concluded from a comprehensive experiment that determining\nthe correct number of neurons is highly correlated with the size of the training\ndata. neurons. They concluded from a comprehensive experiment that deter-\nmining the correct number of neurons is highly correlated with the size of the\ntraining data.\n2.5.4\nDropout\nDropout is a conﬁguration parameter that is used in input and output layers\nas a rate for ignoring neurons. Dropout helps avoid overﬁtting to generalize a\nDL model.\nSome researchers argue that dropout should be set according to the un-\nderlying mechanism of DL being used. Ba and Frey [3] proposed an adap-\ntive algorithm for dropout. Their method achieved a remarkable reduction in\nclassiﬁcation error when using shallow networks. Classiﬁcation performance\nof using adaptive dropout was also investigated by Kingma et al. [28]. They\ndepicted that choosing an adaptive dropout helps reduce classiﬁcation error,\nremarkably. Bayesian approaches were mostly utilized in studies dealing with\ndropout rates. For instance, in [80], a Bayesian dropout learning method was\nproposed classiﬁcation. The experiment showed that employing an adaptive\ndropout rate reduces the eﬀort allocated to perform tuning. Phum et al. [8] an-\nOn tuning deep learning\n9\nalyzed the eﬀects of tuning dropout of recurrent neural networks. Performing\na tuning on dropout not only reduces classiﬁcation error but also provides a\nrelatively easy way to perform tuning. Ko et al. [29] pointed out that dropout\nshould be set according to its corresponding network model. Using a dropout\nrange between 0.4 and 0.8 was strongly advised by them to keep test error low.\nZhang et al. [77] utilized a distribution function to conﬁgure dropout. They\nfound adaptive dropout learning to have a high potential to conduct big data\nlearning in IoT.\n3\nData processing\nThis section is divided into two subsections detailing DL studies in terms\nof data cleansing and data normalization. Since data preparation is of great\nimportance for shallow neural networks [40], as well as DL [46], two essential\ndata processing methods are summarized to explain their relationships with\nDL.\n3.1\nData cleansing\nChuck et al. [32] proposed a data cleansing algorithm based on low-conﬁdence\ncorrection for the planar part extraction task. The method was able to achieve\nup to 10% decrease in training loss. They also argue that inconsistency in the\nresults obtained via machine learning algorithms is not originated from wrong\nformatting, but rather due to misinterpretation of data. Noisy data distribu-\ntion can be detected via modiﬁed deep learning models. Sukhbaatar and Fergus\n[61] proposed a deep learning model for noisy data. Their method was trained\non clean data to predict noise distribution. Noisy data was found beneﬁcial to\nreduce the error rate of training in that study. In [65], a kernel mean matching\ntechnique was devised to learn from noisy data. It achieved a good general-\nization along with an improved classiﬁcation using FlickR images. Zhang and\nSabuncu [78] argued that mean absolute error has some drawbacks to evalu-\nate the performance of a deep neural network. When using complicated data\nsets, the mean absolute error creates a remarkable diﬃculty in training as it\nprovides robustness against noisy labels. To address this problem, they pro-\nposed two loss functions for classiﬁcation. They yielded high accuracy on the\ninstances featuring noisy labels. Bekker and Goldberger [5] proposed a new\ndeep learning algorithm which does not need any clean data to perform train-\ning on noisy data. Their method showed great resistance against high noise\nfraction. Massouh et al. [42] described external noise as a wrong label which is\n10\nM.M. Ozturk\nnot available in the instances. They depicted that CNN shows higher robust-\nness to external noise than to internal noise. Choi et al. [11] employed CNN to\ntagging music. They argue that the tag-wise performance of a data set shows\nthe noisiness of it. Wu et al. [70] designed a light CNN for face recognition.\nIt works faster than traditional CNN’s due to its feature map operation that\nmakes CNN relatively small. They also proposed a bootstrapping to cope with\nnoisy labels in images. Li et al. [35] devised a CNN which yields high accu-\nracy on data sets where noise distribution of them is ambiguous. Most of the\nresearches concerning deep learning with noisy data is focused on label noise.\nInstead, noisy data problems should be addressed regarding other data prob-\nlems such as sparsity. Further, there is a need to develop learning techniques\nin the context of the experiments in which both the labels and the features\nhave noisy points.\n3.2\nData normalization\nIn deep learning methods, data normalization can be divided into three cat-\negories: batch normalization, input weight normalization, and raw input nor-\nmalization. Liu et al. [39] proposed a batch normalization technique for char-\nacter recognition. In their experimental setup, a batch normalized layer was\nadded to a fully connected deep neural network to improve the generaliza-\ntion of their method. They detected that employing ReLU brings the results\n12 times faster than sigmoid. In [48], data augmentation was utilized to in-\ncrease the accuracy of classiﬁcation. To that end, an augmentation network\nwas established in training. According to the results of the experiment, data\naugmentation helps increase the success of classiﬁcation in which there is a\nlack of training data.\nData augmentation was also applied to predicting gait sequences [48]. Per-\nforming training and testing on diﬀerent data sets by using either real data\nor synthetic data results in low accuracy. To address that problem, training\nthe model using mixed data is a good way to achieve 95% of accuracy. Zhong\net al. proposed a data augmentation method for various recognition tasks of\nimage recognition. Their method randomly selects a rectangle region in an\nimage and then changes the values of pixels of that rectangle by using random\nvalues. Even though their method increased the accuracy of CNN up to 3%,\nsome questions remained unanswered. For example, does expanding an image\nwith arbitrary pixels rather than changing the values of the pixels of a spe-\nciﬁc region in it give promising results? Moreover, the critical question is, to\nwhat extent image-quality based value generation is compatible with the CNN\nOn tuning deep learning\n11\ndesigned by [43]. They argue that cropping is the best way to perform data\naugmentation for CNN. Further, they detected color-jittering as the second\nsuccessful method for data augmentation. In [12], Tanh Estimator was found\nto be the most eﬀective normalization method for recurrent neural networks.\nHowever, to generalize the results, other types of deep learning methods such\nas stacked autoencoders should be involved in a comprehensive experiment.\nPassalis et al. [47] proposed a new normalization method called DAIN for deep\nlearning models. They evaluated the method on three types of deep learning\nalgorithms. The method yielded the highest accuracy among the comparison\nmethods as it has an adaptive scheme to perform normalization in which data\ndistribution is analyzed to avoid a ﬁxed scheme. Tran et al. [63] developed a\ndata augmentation method on the basis of Bayesian reasoning. The method\nachieved high accuracy on the classiﬁcation of image data sets. Sound data has\nthe potential to perform deep learning experiments as well as image data. In\n[51], data augmentation in sound data sets resulted in a 0.5 increase in mean\naccuracy.\n4\nTuning strategies\n4.1\nGrid Search\nGrid search is one of the most common hyperparameter search methods and\nit searches all the parameter space [25, 17]. Unlike random search, it performs\nexhaustive searching in speciﬁc distinct places which depend on the number\nof types of hyperparameters [6]. Let T1 be training set and T2 denotes testing\nset, each conﬁguration of hyperparameter set is trained on T1 to test with\nT2. Despite the fact that grid search provides an exhaustive evaluation of\nhyperparameters, unlike other search methods, it requires a great number of\niterations. d denotes the dimension of hyperparameter and c is the possible\nchoice of hyperparameters where n is the number of iterations, the complexity\nof grid search can be calculated with the following equation:\nO(V\nn.cd.F(Q, λ))\n(5)\nwhere F(Q, λ) minimizes the criterion which decides when training is sus-\npended. V represents the total number of predictions.\n12\nM.M. Ozturk\n4.2\nDerivative-free methods\nThis subsection describes four derivative-free optimization methods which are\nalso suitable for hyperparameter optimization. However, we did not include\nany derivative-free method in the experimental study except Random Search\nbecause if the problem is very big, it leads to an exponential increase in the\nnumber of function evaluations so that an adaptive parallelization [44] is re-\nquired.\nRandom search prefers to search parameters in various distinct places\ndepending on the number of parameters [69]. If the time allocated for searching\nhyperparameters is limited, the random search could be a possible solution to\nperform hyperparameter optimization. Further, parallelization can be easily\nestablished in a random search as it does not require communication between\nworkers. The following equation describes the computational complexity of\nrandom search:\nO(V\nn.R.F(Q, λ))\n(6)\nwhere V is the targeted parameter volume for n iterations in R space.\nGenetic algorithm. Three concepts constitute the mechanism of evolu-\ntion: natural selection [58], mutation [2], and genetic [14]. Analyzing these\nthree concepts resulted in genetic algorithm which is useful for any optimiza-\ntion problem. The main objective of the genetic algorithm is to ﬁnd global\noptimum by searching a space including many local optima. For that reason,\nit requires a large number of computations. The genetic algorithm utilizes mu-\ntation that leads to varying outcomes in which the problem is not adaptable\nto diﬀerentiable or continuous objective functions.\nBayesian optimization. Bayes theorem is directly related to Bayesian op-\ntimization. It builds a probabilistic model by assuming a past event is a base\nto evaluate a current event. The approximation of Bayes is called a surrogate\nfunction that is used for future sampling. In Bayesian optimization, the acqui-\nsition criterion decides which sample will be selected in the next evaluation\n[50].\nNelder-Mead optimization, which is very eﬀective for stochastic re-\nsponses, was ﬁrst introduced in 1965 [19]. Nelder-Mead performs an iterative\ncomputation to complete optimization at low-cost. It aims at minimizing the\nerror e of an objective function f(x). Here x is a member of solution space and\nf(x) is updated for each response as follows:\nf(x)2 = f(x)1 + e\n(7)\nOn tuning deep learning\n13\nwhere f(x)1 is the output of the previous iteration that is used to calculate\nf(x)2 new result of next iteration, thereby adding error value e.Even though\nNelder-Mead converges at a fast pace, it is not eﬀective for high-dimensional\nproblems.\n5 | EXPERIMENTAL SETTINGS\nThe experiment was performed on a machine having CentOS Linux, 64-bit,\nIntel(R) Xenon(R) 2.9 GHz, 32 CPU Cores server with 263 GB RAM, and\nTesla C1060 graphics processor. The R codes of the study can be downloaded\nvia the link (https://github.com/muhammedozturk/deepLearning).\n5.1 | Data Sets\n24 data sets were collected from the OpenML platform [64] which enables\nresearchers to share their data sets to perform machine learning tasks. All the\ndata sets are for classiﬁcation experiments. Six data sets have factor values\nthat were converted to numeric to make them suitable for DL algorithms. The\ncrucial point in their conversion is making sure that there is no mathematical\nmodel in factor values. Otherwise, the conversion should be conducted by\ngiving relational values to them. Table 2 gives the summary of experimental\ndata sets having a various number of instances which range from 540 to 45312.\nAlgorithm 1 was designed to perform preprocessing on the experimental data\nsets. For M matrix, n is the number of data sets. FactorAnalysis checks the\nlast column of a matrix Mi[, n] to determine whether the label column includes\nfactor values. Thereafter, CountFactor calculates the number of factor labels\nto convert factor values to numeric with the help of Random function. The list\ncalled SList is generated to collect sparsity results of the data sets. lengthC\nrepresents the number of columns and lengthR is the number of rows. The\nfunction called normalize conducts a minmax normalization on each data\ncell of matrix M. Last, the processed data group and sparsity list SList are\nobtained.\n5.2 | Conﬁgurations for training and testing\nA two-sided classiﬁcation experiment is designed to evaluate the results: one\nside is a learning rate based comparison of DL algorithms, the other comprises\naccuracy evaluations of DL algorithms in terms of some data mining tasks.\nThe value space ranging from 0.005 to 0.823 is set for the learning rate. The\nstep size is 0.05 that resulted in 208 values by adding some new values to space\n14\nM.M. Ozturk\ninput : Data sets M1, ..., Mn\noutput: Processed data sets M1, ..., Mn\nfor i ←0 to n do\nFA ←FactorAnalysis(Mi[, n]);\nif FA==TRUE then\nCF ←CountFactor(Mi[, n])\nMi[, n] ←Random(1, CF)\nend\nSList ←sparsity(Mi)\nlengthC ←Mi[1, ]\nlengthR ←Mi[, 1]\nfor j ←0 to lengthR do\nfor k ←0 to lengthC do\nMi[j, k] ←normalize(Mi[j, k])\nend\nend\nend\nReturn (M1, ..., Mn, Slist)\nAlgorithm 1: Preprocessing for deep learning.\nby ignoring that step size. Each data set is divided into 70% for training and\n30% for testing. Since we observe the eﬀect of a speciﬁc hyperparameter on the\naccuracy, a validation set is not considered for the ﬁrst side of the experiment.\nMean accuracy results are then obtained via 10*10 cross-validation.\nThe second side of the experiment is to evaluate the accuracy of DL algo-\nrithms in terms of data mining tasks. To that end, sparsity and normalization\nanalyzes are involved in the experiment. For a matrix M, sparsity is calculated\nvia the following formula:\nS = Mz\nMt\n(8)\nwhere Mz is the number of zero values, Mt is the number of total elements,\nand S refers to the sparsity rate. Minmax normalization, which is applied on\neach feature of the data set, is calculated using (9) as follows:\nM[i, j] =\nM[i,j] −min(M[,j])\nmax(M[,j]) −min(M[,j])\n(9)\nwhich yields a normalized M. Here j denotes the feature and i refers to the\ninstance.\nOn tuning deep learning\n15\nTable 2: Summary of the experimental data sets. NF=>number of features,\nNI=>number of instances, RF=>T (true), F (false):indicates whether the\ndata set require an operation for converting factor values to numeric.\nName\nDescription NF\nNI RF\nbank-marketing#\nincludes marketing campaigns of a banking institution\n16 45211\nT\nblood-transfusion#\nthis data is of explicit use in classiﬁcation\n4\n748\nF\nclimate-simulation#\nincludes failure analysis of simulation crashes in climate models\n20\n540\nF\ncredit-g#\nincludes credit risks of people for classiﬁcation\n20\n1000\nT\ndiabetes-37#\nincludes some diabet test results\n8\n768\nF\ntic-tac-toe#\nincludes encoding information of tic-tac-toe game\n9\n958\nT\nelectricity#\nincludes summary information about electricity consumption\n8 45312\nF\ngina-agnostic #\nincludes values of handwritten digit recognition 970\n3469\nF\nhill-valley#\nevery instance in this data set represent a two-dimensional graph 100\n1212\nF\nilpd#\nincludes some patient records\n10\n583\nT\nkr-vs-kp#\nincludes chess game records\n36\n3196\nT\nmadelon#\nan artiﬁcial data set for classiﬁcation 500\n2600\nF\nmonks-problems-1#\ncontains some values of an algorithm selection problem\n6\n556\nF\nmonks-problems-2#\nsecond version of monks-problems\n6\n601\nF\nmonks-problems-3#\nthird version of monks-problems\n6\n554\nF\nmozilla4#\nincludes information about a conditional model\n5 15545\nF\nmusk#\nincludes values of a molecule classiﬁcation 162\n6598\nT\nnomao#\nthis data set can be used for location classiﬁcation 118 34464\nF\nozone-level-8hr#\nincludes an analysis for predicting ozone level\n72\n2534\nF\nphoneme#\nincludes information about acoustic observation\n5\n5404\nF\nqsar-biodeg#\nthis data set is prepared for chemical classiﬁcation\n41\n1055\nF\nscene#\nimage recognition data set 296\n2407\nF\nsteel-plates-fault#\nincludes fault types of plates\n33\n1941\nF\nwdbc#\nincludes characteristics of the cell nuclei of breast images\n30\n569\nF\n16\nM.M. Ozturk\nThe data sets are divided into three parts including training, validation,\nand testing sets for the second side of the experiment. 70% of each data set\nis used for training, 15% of remaining parts are employed for validation that\nshows the degree to which conﬁguration of hyperparameter sets ﬁts training.\nThe last 15% of remaining parts are used for testing which assesses the general\nperformance of the classiﬁer.\nWe ran random search and grid search algorithms for three hyperparam-\neters as follows: learning rate (0.1,0.2,...,0.9), batch size (10,20,...,100), and\nnumber of hidden nodes (1,2,...,10) are for FFNN; learning rate (0.1,0.2,...,0.9),\nbatch size (10,20,...,100), and number of hidden nodes (1,2,...,10) are for DBN;\nlearning rate (0.1,0.2,...,0.9), numepochs (10,20,...,100), and dimension of hid-\nden layers (1,2,...,10) are for RNN; learning rate (0.1,0.2,...,0.9), batch size\n(10,20,...,100), and number of hidden nodes (1,2,...,10) are for SAE. Drop out\nrates for visible and hidden layers are set to 0 as default.\n5\nResults\nThe change of accuracy values depending on the learning rate is given in\nFigure 2. FFNN has a cyclic and highest learning rate among the comparison\nmethods. Finding a reasonable learning rate is relatively easy in FFNN due to\nits form of repetitive accuracy. RNN starts converging at 0.47 of the learning\nrate so that setting maximum learning rate to 0.51 is reasonable for that\nalgorithm. It is clearly seen from Figure 2c that training peaks at 0.52 of the\nlearning rate for SAE. However, managing the ups and downs of learning rate\nof SAE is easy compared to the other methods. Even though DBN yields a\nstagnant performance in changing learning rates, it has the lowest accuracy\namong those algorithms. In addition to this, DBN is not sensitive to learning\nrates which are greater than 0.2. In that case, DBN becomes ﬂexible in which\na wide range of learning rate is employed to train a neural network.\nDBN requires less time and eﬀort compared with FFNN. Further, DBN\nachieves higher value of accuracy than FFNN has. A fully connected feed for-\nward neural network needs to be pruned in pre-training. In that way, FFNN\ncan be made fast enough. Moreover, a class-imbalance analysis helps decide\nwhich part of data set is suitable for training. An oversampling or undersam-\npling operation may be performed to address that issue.\nOn tuning deep learning\n17\nLearning rate\nAccuracy\n0.2\n0.4\n0.6\n0.8\n0.55\n0.60\n0.65\n0.70\n0.75\n(a) FFNN\nLearning rate\nAccuracy\n0.2\n0.4\n0.6\n0.8\n0.40\n0.45\n0.50\n0.55\n0.60\n(b) RNN\nLearning rate\nAccuracy\n0.2\n0.4\n0.6\n0.8\n0.56\n0.58\n0.60\n(c) SAE\nLearning rate\nAccuracy\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n(d) DBN\nFigure 2: Accuracy changes of DL algorithms depending on the learning rate.\nThe results were validated with Kruskal Wallis Test which performs a non-\nparametric analysis between the groups. That test does not perform matching\nto pursue statistical analysis. The results are signiﬁcantly diﬀerent (p<0.05)\naccording to the Kruskal Wallis Test (reject the populations have the same\ndistributions H0).\nThe validation part is generally utilized to attain a reasonable conﬁguration\nof hyperparameters. A generalization is thus achieved in the testing phase.\nHowever, using a precise ratio to decide the size of training-validation-testing\nparts may lead to unimproved performance. To solve that problem, in the\niterations of cross-validation, division rates of those parts might be changed\nto achieve a reasonable generalization in the validation phase.\nRNN is generally employed for sequential data models such as text and\nspeech prediction. RNN requires much time when there are a lot of features\nas it is sensitive to the number of features. In the experiment, the data set\n18\nM.M. Ozturk\nTable 3: Sparsity rates of experimental data sets.\nName\nS\nbank-marketing#\n0.3101071\nblood-transfusion#\n0.001336898\nclimate-simulation#\n0\ncredit-g#\n0.07585714\ndiabetes-37#\n0.1103877\ntic-tac-toe#\n0.2066806\nelectricity#\n0.06653621\ngina-agnostic #\n0.689833\nhill-valley#\n0.004950495\nilpd#\n0\nkr-vs-kp#\n0.00189426\nmadelon#\n7.676954e-07\nmonks-problems-1#\n0.07142857\nmonks-problems-2#\n0.09389113\nmonks-problems-3#\n0.06859206\nmozilla4#\n0.1706229\nmusk#\n0.00775547\nnomao#\n0.01575731\nozone-level-8hr#\n0.01229849\nphoneme#\n0\nqsar-biodeg#\n0.4520876\nscene#\n0.02767761\nsteel-plates-fault#\n0.2011243\nwdbc#\n0.004422019\nnamely gina-agnostic has 970 features. The time passed to complete the grid\nsearch of RNN for gina-agnostic is 10 times greater than those of other data\nsets.\nThe results of two types of hyperparameter search methods are presented in\nFigure 3 for four DL algorithms. In total, 16 box plots are obtained by adding\nthe results obtained via normalized data sets. The accuracy of the random\nsearch is 5.34 higher than that of the grid search. It can be seen from Figure\n3 that normalization does not adversely aﬀect the success of DL algorithms.\nOn the contrary, normalization has a favorable eﬀect on the success of DL\nin general. Normalization has created the highest increase in the accuracy\nof RNN. From this point of view, we can conclude that RNN is the most\ncompatible with normalization. Meanwhile, it is worth noting that RNN needs\nall the features of the testing set to predict sequential data in which an eﬀort-\nintensive operation is performed. Normalization has decreased the success of\nstacked autoencoders. It is rather concerned with the dimension of training\ndata instead of the scale. The decline in the success of grid search in SAE\nmay have originated from the direct use of feature space. Normalization has\nonly created an adverse eﬀect on the grid search of SAE given in Figure 3-\n(m,n). An SAE is inherently able to reduce the number of inputs as the level\nof autoencoders increase. It is trivial to perform normalization when using an\nSAE in which the deviation is not remarkably high. As such, it is extremely\nimportant to avoid normalization in SAEs to sustain a successful classiﬁcation.\nFor all the data sets, random search gives relatively better results than the\ngrid search. However, at that point, the search methods used in this study\nshould be validated with diﬀerent machine learning tasks such as regression.\nOn tuning deep learning\n19\nFigure 3: Box-plots illustrating the eﬀects of normalization on hyperparameter\ntuning. a:DBN-grid search with original data set, b:DBN-grid search with nor-\nmalized data set, c:DBN-random search with original data set, d:DBN-random\nsearch with normalized data set, e:FFNN-grid search with original data set,\nf:FFNN-grid search with normalized data set, g:FFNN-random search with\noriginal data set, h:FFNN-random search with normalized data set, i:RNN-\ngrid search with original data set, j:RNN-grid search with normalized data\nset, k:RNN-random search with original data set, l:RNN-random search with\nnormalized data set, m:SAE-grid search with original data set, n:SAE-grid\nsearch with normalized data set, o:SAE-random search with original data set,\np:SAE-random search with normalized data set,\n20\nM.M. Ozturk\nDetail accuracy results of the data sets are given in Figs. 4-7. Here the data\nsets are ranked by accuracy success. It is obviously seen from these ﬁgures that\nozone-level-8hr has the highest value of accuracy. Low sparsity (0.01229849)\nmay have contributed to the success of ozone-level-8hr. But sparsity is not\nthe sole indicator of the success of hyperparameter search methods. Because,\nalthough kr-vs-kp has a low sparsity (0.00189426), it yielded a bad accuracy\ncompared to the other data sets. ozone-level-8hr has a uniform distribution\nin terms of class labels which is a sign to produce high accuracy. On the\nother hand, kr-vs-kp has not a uniform distribution in class labels. Further,\nit completely consists of factor values that were converted to numeric. An\ninteresting result is that the bank-marketing data set has yielded reasonable\nperformance regardless of the type of hyperparameter search method. This\ndata set needs factor to numeric conversion before training. Further, although\nblood-transfusion has not a high number of features (4), it yielded promising\nresults for all DL algorithms. We can conclude from these results that taking\ntraining data from a uniform distribution of class labels is crucial for obtaining\nhigh performance for classiﬁcation. Moreover, the number of features has not\na remarkable eﬀect on the success of accuracy.\ndataset_50_tic-tac-toe\nkr-vs-kp\nmusk\nmozilla4\ndataset_37_diabetes\nhill-valley\ngina_agnostic\nmadelon\nmonks-problems-1\nmonks-problems-3\nelectricity-normalized\nnomao\nsteel-plates-fault\ncredit_g\nmonks-problems-2\nilpd\nphoneme\nscene\nclimated-model-simulation-crashes\nqsar-biodeg\nwdbc\nbank-marketing\nblood-transfusion\nozone-level-8hr\n0.25\n0.50\n0.75\naccuracy\n(a) Grid search results of DBN.\ncredit_g\nnomao\nmozilla4\ndataset_37_diabetes\nkr-vs-kp\nmonks-problems-3\nhill-valley\ngina_agnostic\nmadelon\nmonks-problems-1\nelectricity-normalized\nmonks-problems-2\nsteel-plates-fault\ndataset_50_tic-tac-toe\nilpd\nwdbc\nphoneme\nqsar-biodeg\nblood-transfusion\nscene\nbank-marketing\nclimated-model-simulation-crashes\nmusk\nozone-level-8hr\n0.25\n0.50\n0.75\naccuracy\n(b) Random Search results of DBN.\nFigure 4: Accuracy results of DBN.\nOn tuning deep learning\n21\nkr-vs-kp\ndataset_37_diabetes\ndataset_50_tic-tac-toe\nmozilla4\nmadelon\ngina_agnostic\nhill-valley\nmonks-problems-1\nmonks-problems-3\nelectricity-normalized\nclimated-model-simulation-crashes\ncredit_g\nmusk\nnomao\nsteel-plates-fault\nmonks-problems-2\nilpd\nphoneme\nscene\nbank-marketing\nblood-transfusion\nqsar-biodeg\nwdbc\nozone-level-8hr\n0.25\n0.50\n0.75\naccuracy\n(a) Grid search results of FFNN.\ncredit_g\ndataset_37_diabetes\nmozilla4\nclimated-model-simulation-crashes\nkr-vs-kp\nhill-valley\nnomao\nmonks-problems-3\ngina_agnostic\nmadelon\nelectricity-normalized\nmonks-problems-1\ndataset_50_tic-tac-toe\nmonks-problems-2\nsteel-plates-fault\nilpd\nwdbc\nphoneme\nblood-transfusion\nqsar-biodeg\nbank-marketing\nscene\nmusk\nozone-level-8hr\n0.25\n0.50\n0.75\naccuracy\n(b) Random search results of FFNN.\nFigure 5: Accuracy results of FFNN.\nmozilla4\ncredit_g\nmadelon\ngina_agnostic\nclimated-model-simulation-crashes\ndataset_37_diabetes\nkr-vs-kp\nscene\nsteel-plates-fault\ndataset_50_tic-tac-toe\nilpd\nmonks-problems-1\nmonks-problems-3\nelectricity-normalized\nhill-valley\nmusk\nnomao\nmonks-problems-2\nphoneme\nbank-marketing\nqsar-biodeg\nwdbc\nblood-transfusion\nozone-level-8hr\n0.4\n0.6\n0.8\naccuracy\n(a) Grid search results of RNN.\ndataset_50_tic-tac-toe\nkr-vs-kp\nmozilla4\ndataset_37_diabetes\nhill-valley\ngina_agnostic\nmadelon\nmonks-problems-1\nmonks-problems-3\nelectricity-normalized\nnomao\ncredit_g\nsteel-plates-fault\nmonks-problems-2\nclimated-model-simulation-crashes\nilpd\nphoneme\nscene\nbank-marketing\nqsar-biodeg\nwdbc\nblood-transfusion\nmusk\nozone-level-8hr\n0.25\n0.50\n0.75\n1.00\naccuracy\n(b) Random search results of RNN.\nFigure 6: Accuracy results of RNN.\n22\nM.M. Ozturk\nTable 4: Comparison of the time cost of DL algorithms. Each cell refers to the\nmean training time of 24 data sets.\nMethod\nTime (second)\nDBN\n177\nDBN-grid search\n789\nDBN-random search\n215\nFFNN\n122\nFFNN-grid search\n850\nFFNN-random search\n171\nRNN\n10803\nRNN-grid search\n57967\nRNN-random search\n23441\nSAE\n200\nSAE-grid search\n1401\nSAE-random search\n310\ndataset_50_tic-tac-toe\nkr-vs-kp\nmozilla4\ndataset_37_diabetes\nhill-valley\nmadelon\ngina_agnostic\nmonks-problems-1\nnomao\nsteel-plates-fault\ncredit_g\nmonks-problems-2\nelectricity-normalized\nilpd\nphoneme\nscene\nbank-marketing\nclimated-model-simulation-crashes\nmonks-problems-3\nqsar-biodeg\nwdbc\nblood-transfusion\nmusk\nozone-level-8hr\n0.25\n0.50\n0.75\n1.00\naccuracy\n(a) Grid search results of SAE.\ndataset_50_tic-tac-toe\nkr-vs-kp\nmozilla4\ndataset_37_diabetes\nhill-valley\ngina_agnostic\nmadelon\nelectricity-normalized\nnomao\ncredit_g\nmonks-problems-3\nsteel-plates-fault\nmonks-problems-2\nclimate-model-simulation-crashes\nilpd\nphoneme\nscene\nbank-marketing\nmonks-problems-1\nqsar-biodeg\nwdbc\nblood-transfusion\nmusk\nozone-level-8hr\n0.25\n0.50\n0.75\n1.00\naccuracy\n(b) Random search results of SAE.\nFigure 7: Accuracy results of SAE.\nMean training times of 24 data sets for DL algorithms are given in Ta-\nble 4. Inherently, a hyperparameter search method increases the time passed\nfor training a DL algorithm. RNN takes more time (57697sn) than the other\nOn tuning deep learning\n23\nalgorithms. Elapsed times of DBN are the lowest values of Table 4. How-\never, if searching hyperparameters is not a must for a DL experiment, FFNN\ncompletes the training in minimum time (122sn) compared to the other DL\nalgorithms.\n6\nConclusion\nThis paper presented a comprehensive evaluation of some hyperparameters of\nDL algorithms. The results showed that each hyperparameter should be set in\ncompliance with the design of its associated DL algorithm. High sparsity has\na negative eﬀect on the accuracy of DL algorithms. Although DL algorithms\nhave several hyperparameters, the learning rate is the key regardless of the\ntype of DL algorithms. The optimal value of the learning rate mostly depends\non the distribution of class labels for classiﬁcation.\nThe experiment performed on 24 classiﬁcation data sets indicate that nor-\nmalization has a favorable eﬀect on the increase in accuracy. Further, convert-\ning factor values to numeric should be done by considering whether there is a\nrelational pattern among the features having factor values. Hyperparameters\nof DL algorithms were tuned via a validation set for each data set, thereby\nchanging the optimal hyperparameter set of DL algorithms according to the\nstructure of the data set. We can conclude from the results that disregard-\ning normalization precisely creates a negative impact on the performance of\ntraining. Further, evaluating sparsity rates of a data set is strongly related to\nthe class distribution of the data sets that some data sets having zero sparsity\nproduced bad accuracy. On the other hand, having low sparsity can not be con-\nsidered an obstacle to increase the accuracy where a uniform class distribution\nis available in the experimental data sets.\nThere are some ways to extend this work as follows:\n1) Derivative-free search methods were not involved in the study. A com-\nparison of derivative-free and blackbox optimization methods may deepen our\nknowledge about hyperparameter optimization,\n2) Using highly-balanced data sets could help improve classiﬁcation results,\n3) Employing an image recognition data set gives a possibility to run an\nexperiment through CNN that may provide new insight into hyperparameter\noptimization,\n4) DL algorithms can be evaluated in a new experimental environment for\nparallel computation.\n24\nM.M. Ozturk\nAcknowledgements\nWe thank TUBITAK ULAKBIM, High Performance and Grid Computing\nCenter (TRUBA Resources) for the numerical calculations reported in this\nwork.\nReferences\n[1] J. M. Alvarez and M. Salzmann, Learning the number of neurons in deep net-\nworks, In Advances in Neural Information Processing Systems, 2016, pp. 2270-\n2278. ⇒8\n[2] G. E. Allen, Hugo de Vries and the reception of the “mutation theory”, Journal\nof the History of Biology, 2, 1 (1969) 55-87. ⇒12\n[3] J. Ba and B. Frey, Adaptive dropout for training deep neural networks, In Ad-\nvances in neural information processing systems, 2013, pp. 3084-3092. ⇒8\n[4] P. Baldi, P. Sadowski, and D. Whiteson, Enhanced Higgs boson to τ+ and τ-\nsearch with deep learning, Phys. Rev. Lett., 114, 11 (2015) 111801. ⇒6\n[5] A. J. Bekker and J. Goldberger, Training deep neural-networks based on unreli-\nable labels, In IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2016, pp. 2682-2686. ⇒9\n[6] J. Bergstra and Y. Bengio, Random search for hyper-parameter optimization, J.\nMach. Learn. Res., (2012) 281-305. ⇒11\n[7] N. Bjorck, C. P. Gomes, B. Selman, and K. Q. Weinberger, Understanding batch\nnormalization, In Advances in Neural Information Processing Systems, 2018, pp.\n7694-7705. ⇒7\n[8] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour, Dropout improves re-\ncurrent neural networks for handwriting recognition, In 14th International Con-\nference on Frontiers in Handwriting Recognition, 2014, pp. 285-290. ⇒8\n[9] T. H. Chan, K. Jia, S. Gao, J. Lu, Z. Zeng, and Y. Ma, PCANet: A simple deep\nlearning baseline for image classiﬁcation?, IEEE trans. on image processing, 24,\n12 (2015) 5017-5032. ⇒2\n[10] Y. Chen, Z. Lin, X. Zhao, G. Wang, and Y. Gu, Deep learning-based classiﬁ-\ncation of hyperspectral data, IEEE Journal of Selected topics in applied earth\nobservations and remote sensing, 7, 6 (2014) 2094-2107. ⇒2\n[11] K. Choi, G. Fazekas, K. Cho, and M. Sandler, The eﬀects of noisy labels on\ndeep convolutional neural networks for music tagging, IEEE Transactions on\nEmerging Topics in Computational Intelligence, 2, 2 (2018) 139-149. ⇒10\n[12] S. Bhanja and A. Das, Impact of data normalization on deep neural network for\ntime series forecasting, arXiv preprint arXiv:1812.05519, (2018). ⇒11\n[13] G. I. Diaz, A. Fokoue-Nkoutche, G. Nannicini, and H. Samulowitz, An eﬀective\nalgorithm for hyperparameter optimization of neural networks, IBM Journal of\nResearch and Development, 61, 4/5 (2017) 9-1. ⇒6\nOn tuning deep learning\n25\n[14] T. Dobzhansky, The genetic basis of evolution, Scientiﬁc American, 182, 1\n(1950) 32-41. ⇒12\n[15] T. Domhan, J. T. Springenberg, and F. Hutter, Speeding up automatic hyper-\nparameter optimization of deep neural networks by extrapolation of learning\ncurves, In Twenty-Fourth International Joint Conference on Artiﬁcial Intelli-\ngence, 2015. ⇒6\n[16] J. Duchi, E. Hazan, and Y. Singer, Adaptive subgradient methods for online\nlearning and stochastic optimization. J. Mach. Learn. Res., (2011) 2121-2159.\n⇒6\n[17] K. B. Ensor and P. W. Glynn, Stochastic optimization via grid search. Lectures\nin Applied Mathematics-American Mathematical Society, 33, (1997) 89-100. ⇒\n11\n[18] H. I Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P. A. Muller, Deep learn-\ning for time series classiﬁcation: a review, Data Min. Knowl. Discov., 33, 4 (2019)\n917-963. ⇒1\n[19] R. Glaudell, R. T. Garcia, and J. B. Garcia, Nelder-mead simplex method, Com-\nputer Journal, 7, 4 (1965) 308-313. ⇒12\n[20] A. Tomar, F. Godin, B. Vandersmissen, W. De Neve, and R. Van de Walle,\nTowards Twitter hashtag recommendation using distributed word representa-\ntions and a deep feed forward neural network, In International Conference on\nAdvances in Computing, Communications and Informatics (ICACCI), 2014, pp.\n362-368. ⇒4\n[21] A. Graves, A. R. Mohamed, and G. Hinton, Speech recognition with deep recur-\nrent neural networks, In IEEE international conference on acoustics, speech and\nsignal processing, 2013, pp. 6645-6649. ⇒4\n[22] Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, and M. S. Lew, Deep learning for\nvisual understanding: A review, Neurocomputing, 187, (2016) 27-48. ⇒1\n[23] D. Held, S. Thrun, and S. Savarese, Learning to track at 100 fps with deep\nregression networks, In European Conference on Computer Vision, 2016, pp.\n749-765. ⇒2\n[24] G. E. Hinton, S. Osindero, and Y. W. Teh, A fast learning algorithm for deep\nbelief nets, Neural Comput., 18, 7 (2006) 1527-1554. ⇒3\n[25] F. Hutter, J. L¨ucke, and L. Schmidt-Thieme, Beyond manual tuning of hyper-\nparameters, KI-K¨unstliche Intelligenz 29, 4 (2015) 329-337. ⇒11\n[26] I. Ilievski, T. Akhtar, J. Feng, and C. A. Shoemaker, Eﬃcient hyperparameter\noptimization for deep learning algorithms using deterministic rbf surrogates, In\nThirty-First AAAI Conference on Artiﬁcial Intelligence, 2017. ⇒5\n[27] N. S. Keskar and G. Saon, A nonmonotone learning rate strategy for SGD train-\ning of deep neural networks, In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2015, pp. 4974-4978. ⇒7\n[28] D. P. Kingma, T. Salimans, and M. Welling, Variational dropout and the local\nreparameterization trick, In Advances in neural information processing systems,\n2015, pp. 2575-2583. ⇒8\n[29] B. Ko, H. G. Kim, K. J. Oh, and H. J. Choi, Controlled dropout: A diﬀerent\n26\nM.M. Ozturk\napproach to using dropout on deep neural network, In IEEE International Con-\nference on Big Data and Smart Computing (BigComp), 2017, pp. 358-362. ⇒\n9\n[30] A. Kuri-Morales, Closed determination of the number of neurons in the hidden\nlayer of a multi-layered perceptron network, Soft Computing, 21, 3 (2017) 597-\n609. ⇒8\n[31] N. Kussul, M. Lavreniuk, S. Skakun, and A. Shelestov, Deep learning classiﬁca-\ntion of land cover and crop types using remote sensing data, IEEE Geoscience\nand Remote Sensing Letters, 14, 5 (2017) 778-782. ⇒2\n[32] C. Chuck, M. Laskey, S. Krishnan, R. Joshi, R. Fox, and K. Goldberg, Statistical\ndata cleaning for deep learning of automation tasks from demonstrations, In 13th\nIEEE Conference on Automation Science and Engineering (CASE), 2017, pp.\n1142-1149. ⇒9\n[33] C. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio, Batch normal-\nized recurrent neural networks, In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2016, pp. 2657-2661. ⇒7\n[34] Y. LeCun, Y. Bengio, Y., and G. Hinton, Deep learning, nature, 521, 7553 (2015)\n436-444. ⇒1\n[35] P. Li, X. He, X. Cheng, X. Gao, R. Li, M. Qiao, and Z. Li, Object Extraction\nFrom Very High-Resolution Images Using a Convolutional Neural Network Based\non a Noisy Large-Scale Dataset, IEEE Access, 7, (2019), 122784-122795. ⇒10\n[36] M. Li, T. Zhang, Y. Chen, and A. J. Smola, Eﬃcient mini-batch training for\nstochastic optimization, In Proceedings of the 20th ACM SIGKDD international\nconference on Knowledge discovery and data mining, 2014, pp. 661-670. ⇒7\n[37] Y. Li, N. Wang, J. Shi, X. Hou, and J. Liu, Adaptive batch normalization for\npractical domain adaptation, Pattern Recognition, 80, (2018) 109-117. ⇒7\n[38] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar, Hyperband: A\nnovel bandit-based approach to hyperparameter optimization, The Journal of\nMachine Learning Research, 18, 1 (2017) 6765-6816. ⇒2\n[39] M. Liu, W. Wu, Z. Gu, Z. Yu, F. Qi, and Y. Li, Deep learning based on Batch\nNormalization for P300 signal detection, Neurocomputing, 275, (2018) 288-297.\n⇒7, 10\n[40] M. Lopez-Martin, B. Carro, A. Sanchez-Esguevillas, and J. Lloret, Shallow neu-\nral network with kernel approximation for prediction problems in highly de-\nmanding data networks, Expert Systems with Applications, 124, (2019) 196-208.\n⇒9\n[41] I. Loshchilov and F. Hutter, CMA-ES for hyperparameter optimization of deep\nneural networks, arXiv preprint arXiv:1604.07269, (2016). ⇒6\n[42] N. Massouh, F. Babiloni, T. Tommasi, J. Young, N. Hawes, and B. Caputo,\nLearning deep visual object models from noisy web data: How to make it work, In\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),\npp. 5564-5571. ⇒9\n[43] L. Taylor and G. Nitschke, Improving deep learning with generic data augmen-\ntation, In IEEE Symposium Series on Computational Intelligence (SSCI), 2018,\nOn tuning deep learning\n27\npp. 1542-1547. ⇒11\n[44] Y. Ozaki, S. Watanabe, and M. Onishi, Accelerating the Nelder-Mead method\nwith predictive parallel evaluation, In 6th ICML Workshop on Automated Ma-\nchine Learning, 2019. ⇒12\n[45] Y. Ozaki, M. Yano, and M. Onishi, Eﬀective hyperparameter optimization using\nNelder-Mead method in deep learning, IPSJ Transactions on Computer Vision\nand Applications, 9, 1 (2017) 20. ⇒6\n[46] K. K. Pal and K. S. Sudeep, Preprocessing for image classiﬁcation by convo-\nlutional neural networks, In IEEE International Conference on Recent Trends\nin Electronics, Information & Communication Technology (RTEICT), 2016, pp.\n1778-1781. ⇒9\n[47] N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosiﬁdis, Deep Adaptive\nInput Normalization for Time Series Forecasting, IEEE Transactions on Neural\nNetworks and Learning Systems, (2019). ⇒11\n[48] L. Perez and J. Wang, The eﬀectiveness of data augmentation in image classiﬁ-\ncation using deep learning, arXiv preprint arXiv:1712.04621, (2017). ⇒10\n[49] Y. Qi, Y. Wang, X. Zheng, and Z. Wu, Robust feature learning by stacked au-\ntoencoder with maximum correntropy criterion, In IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 6716-6720.\n⇒5\n[50] L. M. Rios and N. V. Sahinidis, Derivative-free optimization: a review of al-\ngorithms and comparison of software implementations, Journal of Global Opti-\nmization, 56, 3 (2013) 1247-1293. ⇒12\n[51] J. Salamon and J. P. Bello, Deep convolutional neural networks and data aug-\nmentation for environmental sound classiﬁcation, IEEE Signal Processing Let-\nters, 24, 3 (2017) 279-283. ⇒11\n[52] S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry, How does batch normaliza-\ntion help optimization?, In Advances in Neural Information Processing Systems,\n2018, pp. 2483-2493. ⇒7\n[53] J. Schmidhuber, Deep learning in neural networks: An overview, Neural net-\nworks, 61, (2015) 85-117. ⇒2\n[54] E. Scornet, Tuning parameters in random forests, ESAIM Proc. Surveys, 60,\n(2017) 144-162. ⇒2\n[55] B. Chandra and R. K. Sharma, Deep learning with adaptive learning rate using\nlaplacian score, Expert Systems with Applications, 63, (2016) 1-7. ⇒6\n[56] B. Singh, S. De, Y. Zhang, T. Goldstein, and G. Taylor, Layer-speciﬁc adaptive\nlearning rates for deep networks, In IEEE 14th International Conference on\nMachine Learning and Applications (ICMLA), 2015, pp. 364-368. ⇒7\n[57] S. L. Smith, P. J. Kindermans, C. Ying, and Q. V. Le, Don’t decay the learning\nrate, increase the batch size, arXiv preprint arXiv:1711.00489, (2017). ⇒8\n[58] E. A. Smith and B. Winterhalder, Natural selection and decision-making: Some\nfundamental principles, In Evolutionary ecology and human behavior, 2017, pp.\n25-60, Routledge. ⇒12\n[59] N. L. Smith, Cyclical learning rates for training neural networks, In IEEE Winter\n28\nM.M. Ozturk\nConference on Applications of Computer Vision (WACV), 2017, pp. 464-472. ⇒\n7\n[60] H. I. Suk, S. W. Lee, and D. Shen, Deep ensemble learning of sparse regression\nmodels for brain disease diagnosis, Medical image analysis, 37, (2017) 101-113.\n⇒2\n[61] S. Sukhbaatar and R. Fergus, Learning from noisy labels with deep neural net-\nworks, arXiv preprint arXiv:1406.2080, 2, 3 (2014) 4. ⇒9\n[62] L. Thomas, M. Kumar, and B. Annappa, Discovery of optimal neurons and hid-\nden layers in feed-forward Neural Network, In IEEE International Conference\non Emerging Technologies and Innovative Business Practices for the Transfor-\nmation of Societies (EmergiTech), 2016, pp. 286-291. ⇒8\n[63] T. Tran, T. Pham, G. Carneiro, L. Palmer, and I. Reid, A bayesian data aug-\nmentation approach for learning deep models, In Advances in neural information\nprocessing systems, 2017, pp. 2797-2806. ⇒11\n[64] J. Vanschoren, J. N. Van Rijn, B. Bischl, and L. Torgo, OpenML: networked sci-\nence in machine learning, ACM SIGKDD Explorations Newsletter, 15, 2 (2014),\n49-60. ⇒13\n[65] P. D. Vo, A. Ginsca, H. Le Borgne, and A. Popescu, Harnessing noisy web\nimages for deep representation., Computer Vision and Image Understanding,\n164, (2017) 68-81. ⇒9\n[66] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, Deep learn-\ning for computer vision: A brief review, Comput. Int. and neuroscience, (2018).\n⇒1\n[67] Q. Wang, J. Wan, and Y. Yuan, Deep metric learning for crowdedness regression,\nIEEE Trans. on Circuits and Systems for Video Technology, 28, 10 (2017) 2633-\n2643. ⇒2\n[68] G. Wang, J. Xu, and B. He, A novel method for tuning conﬁguration parameters\nof spark based on machine learning. In 2016 IEEE 18th International Conference\non High Performance Computing and Communications, 2016, pp. 586-593. ⇒2\n[69] P. Probst, M. N. Wright, and A. L. Boulesteix, Hyperparameters and tuning\nstrategies for random forest, Wiley Interdisciplinary Reviews: Data Mining and\nKnowledge Discovery, 9, 3 (2019) e1301. ⇒12\n[70] X. Wu, R. He, Z. Sun, and T. Tan, A light cnn for deep face representation with\nnoisy labels, IEEE Transactions on Information Forensics and Security, 13, 11\n2018 2884-2896. ⇒10\n[71] Q. Xu, C. Zhang, L. Zhang, and Y. Song, The learning eﬀect of diﬀerent hid-\nden layers stacked autoencoder, In 8th International Conference on Intelligent\nHuman-Machine Systems and Cybernetics (IHMSC), 2, 2016, pp. 148-151. ⇒8\n[72] Z. Yao, A. Gholami, Q. Lei, K. Keutzer, and M. W. Mahoney, Hessian-based\nanalysis of large batch training and robustness to adversaries, In Advances in\nNeural Information Processing Systems, 2018, pp. 4949-4959. ⇒7\n[73] M. U. Yaseen, A. Anjum, O. Rana, and N. Antonopoulos, Deep learning hyper-\nparameter optimization for video analytics in clouds, IEEE Transactions on\nSystems, Man, and Cybernetics: Systems, 49, 1 (2018) 253-264. ⇒6\nOn tuning deep learning\n29\n[74] Y. Yoo, Hyperparameter optimization of deep neural network using univariate\ndynamic encoding algorithm for searches, Knowledge-Based Systems, 178, (2019)\n74-83. ⇒5\n[75] S. R. Young, D. C. Rose, T. P. Karnowski, S. H. Lim, and R. M. Patton, Op-\ntimizing deep learning hyper-parameters through an evolutionary algorithm, In\nProceedings of the Workshop on Machine Learning in High-Performance Com-\nputing Environments, 2015, pp. 1-5. ⇒6\n[76] M. D. Zeiler, Adadelta: an adaptive learning rate method. arXiv preprint\narXiv:1212.5701, (2012). ⇒6\n[77] Q. Zhang, L. T. Yang, Z. Chen, P. Li, and F. Bu, An adaptive dropout deep\ncomputation model for industrial IoT big data learning with crowdsourcing to\ncloud computing, IEEE Transactions on Industrial Informatics, 15, 4 (2018)\n2330-2337. ⇒9\n[78] Z. Zhang and H. Sabuncu, Generalized cross entropy loss for training deep neu-\nral networks with noisy labels, In Advances in neural information processing\nsystems, 2018, pp. 8778-8788. ⇒9\n[79] H. Zhao, F. Liu, H. Zhang, and Z. Liang, Research on a learning rate with energy\nindex in deep learning, Neural Networks, 110, (2019) 225-231. ⇒6\n[80] J. Zhuo, J. Zhu, and B. Zhang, Adaptive dropout rates for learning with cor-\nrupted features, In Twenty-Fourth International Joint Conference on Artiﬁcial\nIntelligence, 2015. ⇒8\nReceived: • Revised:\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-11-19",
  "updated": "2020-11-19"
}