{
  "id": "http://arxiv.org/abs/2011.10239v1",
  "title": "Shuffle and Learn: Minimizing Mutual Information for Unsupervised Hashing",
  "authors": [
    "Fangrui Liu",
    "Zheng Liu"
  ],
  "abstract": "Unsupervised binary representation allows fast data retrieval without any\nannotations, enabling practical application like fast person re-identification\nand multimedia retrieval. It is argued that conflicts in binary space are one\nof the major barriers to high-performance unsupervised hashing as current\nmethods failed to capture the precise code conflicts in the full domain. A\nnovel relaxation method called Shuffle and Learn is proposed to tackle code\nconflicts in the unsupervised hash. Approximated derivatives for joint\nprobability and the gradients for the binary layer are introduced to bridge the\nupdate from the hash to the input. Proof on $\\epsilon$-Convergence of joint\nprobability with approximated derivatives is provided to guarantee the\npreciseness on update applied on the mutual information. The proposed algorithm\nis carried out with iterative global updates to minimize mutual information,\ndiverging the code before regular unsupervised optimization. Experiments\nsuggest that the proposed method can relax the code optimization from local\noptimum and help to generate binary representations that are more\ndiscriminative and informative without any annotations. Performance benchmarks\non image retrieval with the unsupervised binary code are conducted on three\nopen datasets, and the model achieves state-of-the-art accuracy on image\nretrieval task for all those datasets. Datasets and reproducible code are\nprovided.",
  "text": "1\nShufﬂe and Learn: Minimizing Mutual Information\nfor Unsupervised Hashing\nFangrui Liu∗and Zheng, Liu†\nFaculty of Applied Science, University of British Columbia\nEmail: ∗fangrui.liu@ubc.ca, †zheng.liu@ubc.ca\nAbstract—Unsupervised binary representation allows fast data\nretrieval without any annotations, enabling practical application\nlike fast person re-identiﬁcation and multimedia retrieval. It\nis argued that conﬂicts in binary space are one of the major\nbarriers to high-performance unsupervised hashing as current\nmethods failed to capture the precise code conﬂicts in the full\ndomain. A novel relaxation method called Shufﬂe and Learn\nis proposed to tackle code conﬂicts in the unsupervised hash.\nApproximated derivatives for joint probability and the gradients\nfor the binary layer are introduced to bridge the update from the\nhash to the input. Proof on ε-Convergence of joint probability\nwith approximated derivatives is provided to guarantee the\npreciseness on update applied on the mutual information. The\nproposed algorithm is carried out with iterative global updates\nto minimize mutual information, diverging the code before\nregular unsupervised optimization. Experiments suggest that the\nproposed method can relax the code optimization from local\noptimum and help to generate binary representations that are\nmore discriminative and informative without any annotations.\nPerformance benchmarks on image retrieval with the unsuper-\nvised binary code are conducted on three open datasets, and\nthe model achieves state-of-the-art accuracy on image retrieval\ntask for all those datasets. Datasets and reproducible code are\nprovided1.\nIndex Terms—Hashing, Unsupervised Learning, Data Re-\ntrieval, Mutual Information\nI. INTRODUCTION\nB\nINARY representations are designed to be compact and\ninformative. It is also called as data hashing, which is\nefﬁcient on both computation and storage. High dimensional\ndata like images and audio clips consumes large room on\nthe disk, and it was almost impossible to index, search and\nunderstand on computers. With the advances in machine\nlearning and pattern recognition, binary representation learned\nby deep neural networks takes smaller space on storage. Those\nlearned binary codes can measure similarity and classify with\nconsiderable accuracy compared to the continuous counterpart.\nGenerally, concrete supervision, eg. class categories, is needed\nto achieve high performance on tasks like image retrieval with\nbinary representation. However, in most realistic scenarios,\nannotations like labels are hard to obtain for a generic im-\nage retrieval application. Researches on unsupervised hashing\nalgorithms leverage the representation learning with smaller\ncode size and more ﬂexibility on application.\nUnsupervised low dimensional binary representation learn-\ning, also known as unsupervised hashing, is the main topic in\n1https://github.com/mpskex/Minimizing-Mutual-Information\nairplane\nautomobile\nbird\ncat\ndeer\n(a) Without Relaxation\nairplane\nautomobile\nbird\ncat\ndeer\n(b) With Shufﬂe and Learn\nFig. 1: VISUALIZED EFFECT ON MINIMIZING MUTUAL IN-\nFORMATION OVER BINARY SPACE\nthis paper. Binary hash boosts the speed on image searching,\nas the representation is more sparse than the continuous code.\nEven simple metrics like Hamming distance is effective to\nretrieve similar images in the domain with binary represen-\ntations. And also other applications like cross-modal retrieval\n[1], [2], Face identiﬁcation [3] have proven that binary code is\ninformative enough to distinguish samples in a large domain.\nOn the other hand, unsupervised binary representations are\nideal for unknown data. No prior knowledge like bounding\nboxes and labels is needed to train an unsupervised binary\nhashing algorithm. It is also capable of carrying sufﬁcient\ninformation in application like multimedia retrieval [4], [5]\nwithout concern on annotations. In conclusion, across different\ndomains, an unsupervised hash can be widely utilized in\napplications that require compact representation for unlabeled\ndata.\nUnsupervised binary representation learning, unlike the\nsupervised hash, is a challenging topic due to its strong\nsparsity and insufﬁciency on constraints with ground truth.\nLow dimensional binary space has fewer keys in code space\nto map the samples in the domain. It will drastically reduce\nthe expressiveness by shrinking the value set from the real\nnumbers to binary. To recapture the information, current meth-\nods borrow the concept of ‘similar’ and ‘dissimilar’ from the\nsupervised hashing algorithms. Metrics like cosine similarity\nis applied to preserve the similarity of the binarized repre-\nsentation from the feature holds for the original data. Other\nconstraints are also used to minimize the error between binary\ncode and the learned continuous representation. Intuitively,\nlearning unsupervised representations can be considered as\nlearning a dictionary in the domain. Better representations\nusually use up more keys in the binary space. Hence, in this\nwork, a novel constraint will be introduced which encourages\narXiv:2011.10239v1  [cs.CV]  20 Nov 2020\n2\nbinary code to diverge and ﬁll up the binary space for the\ndomain.\nConventional unsupervised hashing suffers from inefﬁcient\nuse of the code space, which harms the performance on\ndistinguishing samples in the domain. Samples will tend to\nshare the same binary code which is incapable of identifying\nthem in learned unsupervised hash database. Models always\nsuffer from code conﬂict issue in Fig. 1a, while models with\nproper relaxation will generalize better, separating samples in\na wider range as shown in Fig. 1b. Current methods [6], [7],\n[8], [9], [10] only apply losses that maximize the similarity\nand consistency of feature, hidden code and the hashed code,\nwhich may push the solution into a local optimum. It will\ncause conﬂicts where samples that should be discriminated\nsharing the same or similar binary code during optimization.\nTherefore, proper relaxation needs to be proposed to help\nthe network jump out of the local optimum and scatter\nrepresentation better in the binary space.\nMutual information is a good criterion to evaluate the corre-\nlation between random variables and can be used as a reference\nto optimize coding efﬁciency during training. Nevertheless, it\nis hard to directly obtain gradients with mutual information\nas both the binarization and joint probability block the gra-\ndients that update network parameters. Fortunately, a greedy\nbinarization has already been proposed with code consistency\nloss [9]. Then the only challenge left is to bridge gradient on\njoint probability. To tackle that, an approximated derivative is\nproposed for joint probability to connect those gradients from\nthe output to inputs, enabling back-propagation with mutual\ninformation over the learned binary representation.\nOur contribution can be summarized as:\n1) An relaxation on unsupervised hash called Shufﬂe and\nLearn is proposed to encourage less code conﬂict in the\nunsupervised hash by minimizing mutual information.\n2) Proof on the ε-Convergence is provided to justify the\neffectiveness of minimizing mutual information with\napproximated derivatives on joint probability.\n3) Experiment result suggests that the proposed method\nachieves SOTA on image retrieval task with unsuper-\nvised binary representation (50.7% on CIFAR-10(II),\n71.5% on NUS-Wide, 70.3% on MS-COCO with 16-\nbits binary code)\nThe paper will be delivered into ﬁve sections including\nthe introduction. Related researches will be discussed in\nSection II, covering work on unsupervised binary representa-\ntion, mutual information in binary representation learning and\nregularization on unsupervised deep clustering. The proposed\nmethod is introduced in Section III, coming along with how the\ngradients are bridged during mutual information minimization,\nε-Convergence proof on joint probability with approximated\nderivatives and the algorithm proposed as an application on\nmutual information minimization. Experiment results will be\nreported in Section IV. At the end of this paper, we will\nconclude the contribution we made in this work in Section\nV.\nII. RELATED WORK\nThere is much research on deep binary representation\nlearning in the past decade. Supervised binary representation\nlearning has been in trend for many years [2], [3], [11],\n[12], [13], [14], [15]. The proposed approach is related to\nthree major research directions, which are unsupervised binary\nrepresentation learning, mutual information in binary represen-\ntation learning and regularization on deep clustering.\nA. Unsupervised Binary Representation Learning\nUnsupervised hashing methods always take features from\nsophisticated extractors like VGG [16] or ResNet [17] and\nlearn a binary hash code which describes its semantic rep-\nresentation without any labels or hints. Current researches\nin unsupervised hashing can be categorized into two series.\nDiscriminative networks constrained by unsupervised losses is\none major direction in unsupervised hashing methods. Rotation\ninvariant is considered to retain maximum semantic informa-\ntion from the binary representation [18]. Quantization loss\nand even distribution on the learned hash are also considered\nto keep the capability of the binary hash to the original\ncontinuous feature representation. A triplet loss [19] is used\nto generate discriminative hash while keeping consistency and\ncapability on the binarized hash. Samples are considered into\npositives and negatives when being compared to the original.\nPositive samples, which are generated with random rotation\nfrom the original, should be closer to the negatives that are\nrandomly picked from the dataset. Also, the angular distance\namong samples provides evidence to similarity [20] as the\nbinary code only take the sign of features into its account.\nFurthermore, the relation can be learned with mild assumption\n[10]. A Similarity is predicted with Bayesian optimal classiﬁer\nduring the training process to ﬁnd the distil data pair which\nis further used in hash learning. Alternatively, a graph can\nalso describe feature similarity [21], guiding the network to\nsearch better hash for the data distribution. Samples are deﬁned\nas vertices and the similarity are described as edges in the\ngraph. For each iteration, the edges will be reinitialized with\nthe current similarity on learned features.\nGenerative methods are also popular in unsupervised bi-\nnary representation learning. Adversarial learning seems to\nbe effective on extracting binary hash code. BinGAN [22]\nintroduces distance matching and entropy regularization with\ndiscriminator to learn how to hash effectively. Similarly, Hash-\nGAN [23] also adopted a generator-discriminator architecture\nto learn binary representation. Both even distribution and\nminimum entropy are considered in an adversarial learning\nframework. Variational Autoencoders are also effective in\nﬁnding a proper hash function on datasets. DVB [24] considers\nreconstruction on the learned bits, which can reﬂect the\ninformation that a hash can retain and also can be a criterion to\nmaximize the representativity of the binarized representations.\nIn conclusion, unsupervised binary representation learning\nalways considers semantic representativity among the binary\nrepresentations as well as fewer code conﬂicts in the full\ndomain. Current unsupervised hashing methods are usually\ntargeted at keeping semantic information in learned binary\n3\ncode, which may lead to local optimum during learning. How-\never, they failed to relax the code conﬂicts when extracting\nunsupervised semantic hash. Those code collision in hashing\nmodels will also harm binary code’s representativity in the\nbinary space. Therefore, proper regularization is necessary to\nrelax code conﬂicts when learning unsupervised hash.\nB. Regularization on Unsupervised Deep Clustering\nUnsupervised hashing is similar to deep clustering as each\nkey in the code space can be interpreted as a cluster gathering\nsimilar samples for the learned feature. Regularization on deep\nclustering can be applied to unsupervised hash to improve\ncode quality. Researches on deep clustering regularization con-\nvinced us that inter-class conﬂicts are crucial for good binary\nrepresentation learning [25]. Relative entropy minimization\nis introduced to gather similar data into a rough cluster by\nreconstructing the original example from distorted data [26].\nEntropy regularization on embedding with adaptive weights\nis proposed to enhance the feature robustness in fuzzy k-\nmean clustering [27]. Other regularizations on clustering, like\na differentiable constraint on cluster size [28] and structural\nregularization [29], are also introduced to avoid embedding\nconﬂicts in the continuous space.\nRegularization is also important to eliminate code conﬂicts\nin the code space on deep clustering if we consider clusters as\ncode in the binary space. However, deep clustering often works\non continuous representation to form small groups without\nany supervision. Most inter-class collision regularization from\ndeep clustering can not be directly applied to unsupervised\nbinary representations.\nIII. SHUFFLE AND LEARN\nTABLE I: TABLE OF NOTATION\nNotation\nDescription\nBi\nOutput binary random variable on i-th position\nP(Bi)\nProbability of when Bi is true\nH(Bi)\nEntropy of Bi\nI(Bi;B j)\nMutual information between Bi and Bj\nθ\nNetwork parameter\nη\nLearning rate for parameter update\nε\nError on approximated joint probability\n∆i\nUpdate gradient on P(Bi)\nMutual information plays an important role in binary repre-\nsentation learning as it provides a correlation between groups\nin the domain. Mutual information is introduced to divergence\nclass-wise representation distribution in supervised binary rep-\nresentation learning [30], [31]. Conversely, the proposed Shuf-\nﬂe and Learn tries to minimize mutual information between\nbit pairs in hash code, encouraging less code conﬂict in the\nlearned unsupervised binary representation.\nMutual information indicates the mutual dependencies be-\ntween two random variables. The learned binary representation\ncan also be considered as a group of random variables P(Bi)\nfor the i-th in N bits. Intuitively, the correlation between arbi-\ntrary pair {Bi,B j} can be eliminated by minimizing the mutual\ninformation I(Bi;B j). Minimizing the mutual information can\ndiverge the samples to use up the whole binary coding space.\nGenerally, the unsupervised binary representation learning\nmay suffer from local optimum, while mutual information\nminimization would help to jump out of it. Gradients from\nmutual information minimization guide the model to a more\nspare solution that provides better accuracy and discriminative\nrepresentations to describe the original data.\nIn this section, we will ﬁrst discuss how those gradients are\nbridged the approximated gradient for joint probability. As the\ngradients are obtained from the approximated joint probability,\nwe need to prove the approximation will eventually converge\nat the real magnitude of joint probability. At the end of this\nsection, we will introduce an application of the proposed\nmethod, regularizing the unsupervised binary representation\nlearning by minimizing the mutual information among bits it\nlearned. Used notations are listed in Table I.\nA. Approximated Gradient for Joint Probability\nMinimizing mutual information is an optimization problem\nand gradients are required to update parameters. The partial\nderivative of I(Bi;Bj) on P(Bi,Bj) is easy to obtain but it is not\ntrivial for the partial derivative of P(Bi,Bj) on Bi. Therefore,\nwe will mainly focus on bridging gradient for P(Bi,Bj).\nLet f be our differentiable learning function and bi be the i-\nth output among N bits from function f( · ;θ) with parameter\nθ. The target function is mutual information I(Bi,Bj) which\nis going to be minimized. The partial derivative ∂I(Bi,Bj)\n∂θ\nis\nneeded to minimize the observed mutual information. Then\nthe overall partial derivative chain can be derived as Eq. (1).\n∂I(Bi,Bj)\n∂θ\n= ∂I(Bi,Bj)\n∂P(Bi,Bj)\n∂P(Bi,Bj)\n∂Bi\n∂Bi\n∂f\n∂f\n∂θ\n(1)\nThere are two parts that is ‘broken’ in the derivative\nchain. One is the binary layer part\n∂bi\n∂f\nand another is the\njoint probability part ∂P(Bi,Bj)\n∂bi\n. For the binary part, a simple\nstraight through gradient strategy [9] is applied to update the\nbinary representation with the continuous gradient with a code\nconstraint. Then the only problem would be the derivative on\njoint probability.\nStatistically, accumulating partial derivatives to a random\nvariable from joint probability does not make sense. How-\never, the gradient is required to link the output to inputs\nin differentiable learning functions. By introducing positive\nand negative association on binary random variable pairs, the\ngradient can be easily obtained to update according to the\ncomputed statistics.\nPositive association [32] is proposed to describe a pair\nof variables Bi and Bj that satisﬁes Cov[Bi,Bj] ≥0 where\nCov[Bi,Bj] = E[Bi,Bj]−E[Bi]E[Bj]. Speciﬁcally, the expecta-\ntion of a binary random variable is the probability when itself\nis positive, making it easier to ﬁnd its lower bound for the joint\nprobability. Hence, for a pair of positively associated binary\nvariables, the upper bound can be easily derived as Eq. (2).\nP(Bi,Bj) ≥P(Bi)P(Bj)\n(2)\nNegative association [33] is the opposite to its positive coun-\nterpart, where Cov[Bi, ¯Bj] ≤0. We inherit the notation from\nprevious derivation for clariﬁcation by assuming pair {Bi,Bj}\n4\nis in positive association. So that for a pair of negatively\nassociated binary variables {Bi, ¯B j}, the lower bound can be\neasily obtained as Eq. (3).\nP(Bi, ¯B j) ≤P(Bi)P( ¯B j)\n(3)\nWith Eq. (2) and (3) it is trivial to obtain the inequality\nfor joint probability for a pair of positively associated binary\nrandom variables, as derived in Eq. (4).\nP(Bi,B j) ≥P(Bi)P(B j)\nP(Bi, ¯B j) ≤P(Bi)−P(Bi)P(B j)\nP( ¯Bi,B j) ≤−P(BI)P(B j)+P(B j)\nP( ¯Bi, ¯B j) ≥1−P(Bi)−P(B j)+P(Bi)P(B j)\n(4)\nSimilarly, the negative will also provide reversed boundary\nfor P(Bi,B j). A pair of associated binary code is either\npositively or negatively associated in statistics. Intuitively, each\npair of independent binary code will be not associated when\nit is at the optimal solution subject to the mutual information.\nOur goal is to eliminate association among pairs of hash. When\nit is in its optimal solution, all inequalities in Eq. (4) will turn\ninto equalities, or it will have a positive or negative error from\nthe joint to multiplication of marginals. To notate the error, we\ndeﬁne a slack variable ε to describe the upper or lower limit\nof the error. Then we can use this slackness to prove the ε-\nConvergence on this approximation.\nJoint probability for associated pair then can be rewritten\nas Eq. (5) with the slack variable ε:\nP(Bi,B j) = P(Bi)P(B j)−ε ≥P(Bi)P(B j)\n(5)\nwhere the slack variable ε is bounded, which means it will\nnever go inﬁnity as both the joint P(Bi,B j) and P(Bi)P(Bj)\nare bounded. Both the upper bound and lower bound of joint\nprobability can be obtained by mixing positive and negative\nassociation condition. The slack variable ε will converge and\nsqueeze the joint probability to be independently multiplied.\nIt will stagger around zero as the variable pairs will switch\nbetween positive and negative association frequently. The\nderivative obtained by the expanded value can provide an\napproximated gradient that establishes a relationship between\njoint probability and the output binary representation. In the\nnext section, we will discuss the convergence on the optimiza-\ntion with approximated derivatives with the slack variable ε.\nWith the slackness ε, the derivative can be trivially derived\nas Eq. (6).\n∂P(Bi,B j)\n∂Bi\n≈∂P(Bi)P(B j)\n∂Bi\n= P(B j)\n(6)\nwhere\n∂P(Bi)\n∂Bi\n= 1\nN for statistics over N samples as the un-\nconditional probability density function of binary variable Bi\ncan be considered as summation over positive samples. The\nderivatives of the rest conditions are stated in Eq. (7). And the\ngradient can be calculated according to those approximated\nderivatives.\n∂P(Bi, ¯Bj)\n∂Bi\n≈∂P(Bi)P( ¯Bj)\n∂Bi\n= 1−P(Bj)\n∂P(Bi, ¯Bj)\n∂Bi\n≈∂P( ¯Bi)P(Bj)\n∂¯Bi\n= −P(Bj)\n∂P( ¯Bi, ¯Bj)\n∂Bi\n≈∂P( ¯Bi)P( ¯Bj)\n∂¯Bi\n= −\n\u00001−P(Bj)\n\u0001\n(7)\nFrom a analytic perspective, compensating the bi-variant\nfunction with a function of one variable is an over-acting\non the gradient. The slack variable will bounce between the\noptimum which would cause diverge with large step sizes.\nHowever, it will eventually provide more precise gradient with\nthe convergence of ε. Further discussion will be covered in\nnext section and we proved that with small step sizes the slack\nvariable will converge to 0 at last.\nThe ﬁnal gradient will accumulate the gradients on Bi with\nrespect to other bits {Bj1,...BjN}, and the actual accumulated\njoint probability will be used to compute loss when the\nnetwork feeds forward.\nB.\nε-Convergence of Joint Probability Approximation\nThe convergence of joint probability approximation needs\nto be proved to ensure accurate update before being applied\nduring optimization. In the previous section, we introduced a\nslack variable ε which deﬁnes the upper and lower bound of\nthe difference between the estimated and approximated joint\nprobability.\nIn Eq. (6), the partial derivative is obtained using the\napproximated joint probability P(Bi)P(Bj)−ε. We will intro-\nduce a learning step-size ηt to formulate the updated joint\nprobability with the approximated gradient. Then a single\niteration at i-th step can be derived as Eq. (8).\nPt+1(Bi,Bj) = Pt(Bi,Bj)−ηt ∂Pt(Bi,Bj)\n∂Pt(Bi)\n= Pt(Bi,Bj)−ηtPt(Bj)\n(8)\nBy introducing the slack variable εt and the gradient ∆t\ni and\n∆t\ni on Pt(Bi) and Pt(Bj) respectively for t-th iteration, we can\nreplace Pt(Bi,Bj) with Pt(Bi)Pt(Bj)+ε according to Eq. (5)\nand also substitute Pt(Bi) −∆t\nP(Bi) for Pt+1(Bi). Then the\nupdate can be trivially derived as Eq. (9).\n−∆t\njP(Bi)−∆t\niP(Bj)+∆t\ni∆t\nj +εt+1\n= εt −ηtPt(Bj)\n(9)\nObviously, we can easily obtain the error difference on slack\nvariable ε from iterations to iterations by deriving the update\nfunction to the joint probability with approximation. The error\ndifference εt+1 −εtcan be illustrated as Eq. (10).\n∆t\niPt(Bj)+∆t\njPt(Bi)−∆t\ni∆t\nj −ηtPt(Bj)\n(10)\nTo prove the convergence, we add all differences to form a\nseries in t. Then we can build a relationship between εT −ε1\nand the series described in Eq. (10). As both the learning\nrate converges to 0 and the probability is non-negative and\nbounded, we can imply that series ∑∞\nt −ηtP(Bj) is convergent.\n5\nThen all we need to do is to prove series in Eq. (11) is\nconvergent.\nεT −ε1\n= ∆t\ni\n\u0000Pt(B j)−1\n2∆t\nj\n\u0001\n+∆t\nj\n\u0000Pt(Bi)−1\n2∆t\ni\n\u0001\n= ∆t\ni\nPt(B j)+Pt−1(B j)\n2\n+∆t\nj\nPt(Bi)+Pt−1(Bi)\n2\n(11)\nSince\nPt(Bj)+Pt−1(Bj)\n2\n∈[0,2], we can split and expand the\ncomponent in Eq. (11) into two individual terms |∆t\ni| and |∆t\nj|.\nThose terms are controlled by the learning rate ηt which is\nconvergent as t increases. Then both |∆t\ni| and |∆t\nj| converges,\nwhich means the original series in Eq. (11) is convergent. Then\nthe combines series in Eq. (10) is convergent, which means\nεT −ε1 is convergent while T →∞. It means the error εt will\nconverge to zero with approximated joint probability.\nProof on ε-Convergence is a weak condition that can guide\nus to design our algorithm. In our implementation, a small\nmultiplier is designed to diminish the error produced during\napproximation. Larger multiplier will push the solution too\nhard and increase ε, which will cause divergence on the\nderivative that comes from an approximated joint probability.\nWe will discuss the detailed design in the next section.\nC. Minimizing Mutual Information for Unsupervised Hash\nMinimizing mutual information can diverge code distribu-\ntion in binary space. In this section, we will introduce the\nproposed relaxation method Shufﬂe and Learn. The minimiza-\ntion is served as a shufﬂing process which encourages the\nencoder to use up the full binary space. A proper amount of\npush will guide the model to escape local optimum. We will\nmainly follow both the network and loss function setup in\nGreedyHash [9] as the shufﬂing process is independent to the\nregular unsupervised hashing optimization.\nTo make the update precisely, the full dataset will be\nused to provide an accurate estimation on probabilities. For\na ﬁnite dataset, the probability distribution can be treated\nas an approximation of the realistic distribution. Hence, we\nneed to assume that we have enough samples to the observed\nprobability distribution function ˆP(Bi) to be closed to the\nrealistic probability distribution function P(Bi).\nThe proposed approach Shufﬂe and Learn is described as\npseudo-code in Algorithm 1. Shufﬂing process always happens\nat the beginning of every epoch, encouraging the network\nto ﬁll up the whole binary space. It will diverge the code\ndistribution and help to jump out of the local optimum on the\nlearned hash.\nA regular optimization on the unsupervised learning process\nis also needed to collaborate optimization on binary code. In\nour implementation, we adopted a cosine similarity loss [9]\non binary code from the actual learning part in the proposed\nShufﬂe and Learn algorithm. It will stimulate the learned\nbinary code to imitate angular relationship on the input feature\nas shown in Eq. (12).\nLsim = ∥sim(H1,H2)−sim(B1,B2)∥2\n2\n(12)\nwhere H1 and H2 are input feature. B1 and B2 are the hash and\nthe function sim(·) is the cosine similarity function deﬁned in\nEq. (13).\nsim(A,B) =\nAB\n∥A∥∥B∥\n(13)\nwhere A and B are vectors with same number of dimension.\nRecalling the objective function that optimizes the model,\nwe combined the consistency regularization Lreg deﬁned in\nEq. (14) with the cosine similarity loss Lsim with hyper-\nparameter α as the regular unsupervised hashing loss Lr. The\nconsistency loss will align representation between two sides at\nthe binarization and the cosine similarity loss Lsim is a regular\nunsupervised hash learning step. Mutual information loss Lm is\nupdated with all samples collected in training set before every\nepoch to capture accurate estimation on mutual information.\nThe algorithm is described as Algorithm 1.\nLreg = ∥H −B∥2\n2\n(14)\nAlgorithm 1: Shufﬂe and Learn\nResult: Obtain function parameter θ\nInitialize network parameter θ, learning rate η,\nhyper-parameter α and β;\nforeach epoch do\nEstimate joint probability ˆP(Bi,Bj) for i, j ∈1..N;\nCalculate mutual information loss βLm;\nUpdate θ with the approximated derivative η ∂Lm\n∂θ ;\nforeach minimatch in dataset do\nCalculate regular loss Lr;\nUpdate θ according to Lr = Lsim +αLreg\nwith learning rate η;\nend\nend\nIn our implementation, we extract feature from images with\na pre-trained VGG-16 [16] without optimizing its parameters\nand the generated binary representation is collected over the\nwhole dataset to compute the mutual information. Only a\nnaive fully connected layer is adopted to consume the input\nfeature to the learned hash. To stabilize the training, we\nminimize mutual information only on P(Bi,Bj) and gradients\nfrom marginal probability are cut from the back propagation.\nAlso, the mutual information is accumulated as a triangular\nmatrix to stabilize training and the loss function is applied\nwith a multiplier β to balance the magnitude on gradients.\nNotably, the minimization process is concave so that the\nwhole process will not have global solution to the opti-\nmization. The convexity on the proposed minimization is\nnot necessary as we adopt this minimization as a relaxation\nprocess. Experiments results also suggest that the proposed\nrelaxation is effective on encouraging less code conﬂict in the\nbinary space.\nIV. EXPERIMENTS\nWe conducted experiments to evaluate the performance of\nthe proposed unsupervised hashing algorithm on open datasets.\n6\nCIFAR-10\nNUS-WIDE\nMS-COCO\n16 bits\n32 bits\n64bits\n16 bits\n32 bits\n64 bits\n16 bits\n32 bits\n64 bits\nSpherH [6]\n0.254\n0.291\n0.333\n0.495\n0.558\n0.582\n0.516\n0.547\n0.589\nITQ [7]\n0.305\n0.325\n0.349\n0.627\n0.645\n0.664\n0.598\n0.624\n0.648\nDGH [8]\n0.335\n0.353\n0.361\n0.572\n0.607\n0.627\n0.613\n0.631\n0.638\nDeepBit [18]\n0.194\n0.249\n0.277\n0.392\n0.403\n0.429\n0.407\n0.419\n0.430\nSGH [34]\n0.435\n0.437\n0.433\n0.593\n0.590\n0.607\n0.594\n0.610\n0.618\nBinGAN [22]\n0.476\n0.512\n0.520\n0.654\n0.709\n0.713\n0.651\n0.673\n0.696\nHashGAN [23]\n0.447\n0.463\n0.481\n-\n-\n-\n-\n-\n-\nDVB [24]\n0.403\n0.422\n0.446\n0.604\n0.632\n0.665\n0.570\n0.629\n0.623\nDistillHash [10]\n0.284\n0.285\n0.288\n0.667\n0.675\n0.677\n-\n-\n-\nGreedyHash [9]\n0.448\n0.473\n0.501\n0.633\n0.691\n0.731\n0.582\n0.668\n0.710\nShufﬂe and Learn (Ours)\n0.507\n0.562\n0.592\n0.715\n0.752\n0.777\n0.703\n0.756\n0.789\nTABLE II: MAP EVALUATION WITH UNSUPERVISED BINARY REPRESENTATION\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrecision\nOurs\nGreedyHash\nSGH\nITQ\nDGH\nSpherH\n(a) 16 Bits\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrecision\nOurs\nGreedyHash\nSGH\nITQ\nDGH\nSpherH\n(b) 32 Bits\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrecision\nOurs\nGreedyHash\nSGH\nITQ\nDGH\nSpherH\n(c) 64 Bits\nFig. 2: PRECISION-RECALL CURVES OF OURS AND COMPARED METHODS ON CIFAR-10 DATASET\nFor every experiment in this section, ﬁxed random seed with\ndeterministic behaviour is applied to train the model. We used\n0 as our random seed for both network and data loader to\nimprove reproducibility.\nA. Datasets\nThe experiments is conducted on three open datasets:\nCIFAR-10 [35], NUS-Wide [36] and MS-COCO [37]. We did\nnot apply any data augmentation technique during training\nand all class labels are not used in our unsupervised setup.\nAll input images are resized to 256×256 before being centre\ncropped to size 224 × 224 before being fed into the network\nand are normalized according to mean pixel and standard\ndeviation of pixels on ImageNet [38].\n1) CIFAR-10\nis an RGB image dataset with 60K 32 ×\n32 images with class annotations from 10 different\ncategories. We followed the CIFAR-10 (II) setting in\nGreedyHash [9] which takes 5,000 images each class\nfor training and the rest of 10,000 images for querying.\nThe training dataset will also be served as the retrieval\nset in this setting. The top-1000 similar images will be\nconsidered in our mean average precision (MAP@1000)\nevaluation.\n2) NUS-Wide\ncontains about 270,000 images with 81\ndifferent concepts. There can be multiple concepts for\na single image. A subset of 21 most common concepts\nis used in our experiments, picking 195,834 images for\nthe experiment. We followed the data split setup in [39],\ntaking 500 images from each concept for training and\n100 images each category for querying. The rest of\nthe data are kept as retrieval dataset, providing similar\nsamples for every query image in the test set. We\nadopted the same setting as other work [39], taking top-\n5000 neighbours to evaluate MAP on NUS-Wide.\n3) MS-COCO\nprovides images from a large scope of\nconcepts. In our experiments we used trainval2014 for\nCOCO dataset to match the setup in HashNet [40].\nThe pruned dataset contains 12,2218 images from 80\ndifferent classes. 5,000 images are randomly selected\nfor query and another 10,000 are also picked for training\npurpose. The remaining is reserved as database during\nMAP evaluation. We follow common evaluation setting\n[40], [41] on COCO dataset, considering top-5000 sim-\nilar images to accumulate MAP score.\nB. Experimental Setup\nExperiments are conducted using a batch size of 32 and a\nlearning rate of 1e −3. We set hyper-parameters α = 0.1 in\nour experiments and 1e −4, 1e −3, 1e −2 on β for our 16-\nbits, 32-bits, 64 bits model respectively. A multi-step learning\nrate decay is applied in our experiments. It will decay by a\nrate of 0.1 every 100 epoch. We trained the network for 300\nepochs with standard SGD optimizer with the momentum of\n0.9 and weight decay of 5e−4. Notably, a special optimizer is\napplied to mutual information minimization, which does not\nhave any momentum or weight decay. This strategy can ensure\nthat the network will only take the precise gradient to shufﬂe\nthe binary representation.\nC. Evaluation Metrics\nMean average precision (MAP) is used to evaluate the\nmodel’s performance on Image retrieval. It is widely used in\nretrieval evaluations [6], [7], [8], [18], [34], [22], [23], [24],\n7\nFig. 3: RETRIEVED IMAGES ON RIGHT WITH QUERY IMAGE ON LEFT FROM NUS-WIDE WITH 16 BITS\n[10], [9]. Average precision(AP) is obtained by accumulating\nretrieval precision on each classes. AP will decrease as we\nincrease the number of retrieved images. Mean of average\nprecision will average all collected average precision score,\nwhich is a good criterion for evaluation over different retrieval\nsetups.\nPrecision recall curve will present the performance with\nmore visual details. It illustrates the relation between preci-\nsion and recall, which is another form to describe retrieval\nperformance over different settings. Notably, the integral of\nthe precision-recall curve is positively related to MAP score.\nD. Evaluation Results\n1) Benchmark on Image Retrieval: Evaluation results on\nMAP are illustrated in Table II. All methods compared are\nusing the same deep VGG-16 feature to ensure a fair com-\nparison. Identical training and test split setup are also used to\nevaluate the proposed method.\nWe compared the proposed algorithm with several state-\nof-the-art methods including SpherH [6], ITQ [7], DGH [8],\nDeepBit [18], SGH [34], BinGAN [22], HashGAN [23],\nDVB [24], DistillHash [10], GreedyHash [9]. According to\nevaluation results on image retrieval in Table II, the proposed\nmethod has advantages on all code length settings comparing\nto current state-of-the-art methods. Large gaps can be observed\non 64-bits with the proposed Shufﬂe and Learn algorithm. The\nproposed regular optimization step is similar to GreedyHash\n[9] as they both use cosine similarity loss and the same\nregularization loss during training. It can be observed that\nthe mutual information minimization did help the network\nto improve representativity over the whole domain. Even\ncomparing to the generative method like BinGAN [22] and\nHashGAN [23], retrieval accuracy of the proposed method can\nstill over-perform them with considerable improvement.\nThe proposed Shufﬂe and Learn achieves higher perfor-\nmance on larger code length. Results with 64 bits code on\nCIFAR-10(II) achieved 59.2% on MAP score which is 9.1%\nhigher than GreedyHash [9] and 7.2% higher than BinGAN\n[22]. The improvement made on NUS-Wide and MS-COCO is\n4.5% and 7.5% comparing to the highest score among SOTA.\nIt suggests that code conﬂict is indeed a challenge to high-\nquality hashing. And also proper shufﬂing on code distribution\nairplane\nautomobile\nbird\ncat\ndeer\n(a) 32 Bits\nairplane\nautomobile\nbird\ncat\ndeer\n(b) 64 Bits\nFig. 4: T-SNE VISUALIZATION ON CIFAR-10 DATASET\ncan signiﬁcantly improve hashing quality in unsupervised\nsetups.\nPrecision-Recall curves are also collected to compare the\nproposed approach to others. We used CIFAR-10 and β is\nset as described in Section III-C. Results are demonstrated in\nFig. 2. Most compared methods cannot retrieve semantically\nrelated clusters from the database, which means those hashing\napproaches may not be able to generalize semantic hash\nfor similar samples. Those methods surely provide accurate\nneighbours but are trapped in local optimum with no con-\nstraint on code generation. However, with the proposed mutual\ninformation, precision is well kept at low recall, which means\nthe hashing model trained with mutual information loss can be\nmore robust and informative on clustering semantically related\nsamples.\n2) Visualization: We adopted t-SNE [42] to visualize code\nquality on the learned binary representation. Results on vi-\nsualization with our 32 bits and 64 bits hash is illustrated in\nFig. 4. The unsupervised binarized representation can separate\nsamples according their semantic contents. And even with\nsmaller form, for example, binary representation with only\n32 bits, can till gather samples as neighbours concerning the\nsemantic label without any supervision from the ground truth.\nWe also visualized the retrieved images with the query\nimage in Fig. 3. We randomly select 4 query images and\nretrieved 10 most similar images from the database set of\nNUS-Wide. The top-10 nearest neighbours are semantically\nrelated, which suggest that the proposed relaxation can help a\nsimple model to retrieve semantic neighbours more effectively\nwith the unsupervised binary hash.\n8\n0\n10\n20\n30\n40\n50\n0\n500\n1000\n1500\n2000\n2500\n3000\nmin:259.0\nmax:3063.0\n(a) β = 0.0001\n0\n10\n20\n30\n40\n50\n0\n500\n1000\n1500\n2000\n2500\nmin:265.0\nmax:2564.0\n(b) β = 0.001\n0\n10\n20\n30\n40\n50\n0\n500\n1000\n1500\n2000\n2500\nmin:307.0\nmax:2519.0\n(c) β = 0.01\nFig. 5: BINARY SPACE UTILIZATION UNDER DIFFERENT β\nE. Empirical Analysis\n1) Ablation Study: According to our assumption, a proper\namount of pushing can improve performance on hashing.\nHyper-parameter β is designed to control the strength of push\nduring optimization. A smaller value for β will diminish the\neffect of minimizing mutual information while larger β will\ndegrade the code quality in performance on retrieval.\nTABLE III: MAP RESULTS ON MODELS TRAINED WITH\nDIFFERENT β\nβ\n0.1\n0.01\n0.001\n0.0001\n0.00001\n0\nOurs 16 bits\n0.465\n0.474\n0.482\n0.507\n0.493\n0.477\nOurs 32 bits\n0.539\n0.553\n0.562\n0.554\n0.560\n0.549\nOurs 64 bits\n0.591\n0.592\n0.591\n0.590\n0.589\n0.589\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\nPrecision\nbeta: 0.1\nbeta: 0.01\nbeta: 0.001\nbeta: 0.0001\nbeta: 1e-05\nFig. 6: PRECISION-RECALL CURVE WITH DIFFERENT β ON\n16 BITS\nIn Table III, we demonstrated our controlled experiment\nwith hyper-parameter β on CIFAR-10(II) dataset. Results are\nevaluated on MAP and every setup is the same except β. Table\nIII proved our assumption on β, showing us that appropriate\npush is needed to improve the code quality. A larger β, for\nexample β = 0.1 will disrupt the regular unsupervised training\nas it leads to a lower the performance that is even worse\nthan the baseline(β = 0). Also, if β is set to a too small\nvalue, eg. β = 1e−5, the optimization will degrade to baseline\nmethod. Precision-Recall curve with different β values on 16\nbits setup in Fig. 6 also gives a strong evidence on the previous\nassumption. Overall code quality is incrementally improved as\nwe move closer to the optimal value on β.\nOn the other hand, the optimal value for β increases as\nthe code length goes up. Smaller code length will cause\nmore accumulative gradients on each bit so that the mutual\ninformation will be more dominant compared to the regular\nunsupervised constraints. A larger code length could help the\ngradients to relax by distributing the error to other bits. But\nstill, the proper β is crucial to encourage the binary code to\nﬁll up the full space.\n2) Code Analysis: We collected statistics to evaluate how\nwill minimization on mutual information would affect the code\nutilization in binary space. We controlled hyper-parameter β\nto assess the binary space utilization with different setups. We\nused CIFAR-10 dataset and follows the same training protocol\nas described above. The code statistics are sorted according to\nthe counted number to evaluate the utilization of binary space.\nAs shown in Fig 5, minimization on mutual information\ncan encourage the network to use more keys in the binary\nspace. The maximal value on single key decreases as we\nincrease the value of β. Also, the minimum of the code count\nincreases which means the code distribution is more ﬂattened.\nIt can be interpreted as more binary space is used as we\nshufﬂe harder by minimizing the mutual information with the\nproposed method. Though it is achieving what we desired,\nit does not mean larger β is good for a hashing algorithm.\nAccording to the conducted ablation study in the previous\nsection, larger β will cause performance drop as it may shufﬂe\nthe code too hard when searching for generalized semantic\nhash in the binary space.\n3) Effect of Mutual Information Minimization on Binary\nCode: We investigated effects on the proposed algorithm\nwith approximated joint probability during optimization to\nsupport the proof on its ε-Convergence discussed in Section\nIII-B. First, we conducted this experiment with CIFAR-10\n(II) dataset, trying to encourage the binary code to utilize\nthe whole binary space. The network will be only optimized\naccording to the mutual information loss on binary code.\nTo visualize, the embedding is divided into two subsets\nand converted into integers respectively. Then we used two\nconverted integers to represent the sample’s coordinates on a\n2D space. The visualization result will not demonstrate the\nsemantic relationship but optimization process which justiﬁes\nour motivation. Result is demonstrated in Fig. 7.\nWe train the network with a learning rate of 1e −5 with\nonly the optimization step on mutual information loss. The\nalgorithm converges in about 30 iterations. We visualized the\nbinary representation using the strategy discussed above. The\nbinary code is diverging into the full space by minimizing\nmutual information with the proposed approach. The solution\nis stable after it converged, which veriﬁes our motivation and\ndesign discussed in Section III. There will be an optimal\n9\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n20\n40\n60\n80\n100\n120\n(a) Step 0\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n(b) Step 1\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n250\n(c) Step 2\n0\n50\n100\n150\n200\n250\n0\n50\n100\n150\n200\n250\n(d) Step 3\nFig. 7: MINIMIZING MUTUAL INFORMATION OVER BINARY\nREPRESENTATION\nsolution for minimizing mutual information with the joint\nprobability and the optimization with approximated derivatives\nof joint probability will converge as ε converges. With this\ncondition, we can ensure that proper minimization on mutual\ninformation will not disturb the regular optimization on unsu-\npervised binary representation learning.\nV. CONCLUSION\nBinary representation has fewer coding space than the\ncontinuous and there is a higher chance when two identically\ndifferent samples share a same code for binary representations.\nThis is often recognized as hash conﬂict or code conﬂict\nin binary space. In this paper, we identiﬁed code conﬂict\nin low dimensional space as a new barrier to high-quality\nunsupervised hashing. A novel meta-algorithm called Shufﬂe\nand Learn is introduced to diverge code distribution in binary\nspace which can enhance hash quality and also help the model\nescape local minimum. The paper also provides proof on the ε-\nConvergence of the proposed algorithm. Mutual information\nloss on binary representation can encourage the network to\nfully utilize the binary space, which mitigates hash conﬂict\non semantically dissimilar samples. With a proper amount of\nshufﬂing, the network can jump out of local minimum and\nalso generalize better with the mutual information loss. The\nproposed approach is ﬂexible and can be applied to other\nhashing algorithms to enhance model generality.\nFuture research should consider studies on a more concrete\ncondition on convergence. More speciﬁcally, both theoretical\nand numerical study is necessary to investigate. Furthermore,\ncurrent estimation on joint probability is to heavy to perform\non very large databases. A ﬂexible on-the-ﬂy technique on the\njoint probability estimation will broaden its application and\nalso accelerate the training possibility. Minimizing mutual in-\nformation on continuous outputs is also an interesting direction\nas it will encourage independence among the output neurons.\nLearning more identical neurons will eliminate redundancy\nin networks and encourage more node to be pruned when\nreducing the size of neural networks.\nREFERENCES\n[1] M. Yu, L. Liu, and L. Shao, “Binary set embedding for cross-modal\nretrieval,” IEEE Transactions on Neural Networks and Learning Systems,\nvol. 28, no. 12, pp. 2899–2910, 2016.\n[2] L. Jin, K. Li, Z. Li, F. Xiao, G.-J. Qi, and J. Tang, “Deep semantic-\npreserving ordinal hashing for cross-modal similarity search,” IEEE\nTransactions on Neural Networks and Learning Systems, vol. 30, no. 5,\npp. 1429–1440, 2018.\n[3] J. Tang, J. Lin, Z. Li, and J. Yang, “Discriminative deep quantization\nhashing for face image retrieval,” IEEE Transactions on Neural Net-\nworks and Learning Systems, vol. 29, no. 12, pp. 6154–6162, 2018.\n[4] L. Xie, L. Zhu, and G. Chen, “Unsupervised multi-graph cross-modal\nhashing for large-scale multimedia retrieval,” Multimedia Tools and\nApplications, vol. 75, no. 15, pp. 9185–9204, 2016.\n[5] G. Wu, J. Han, Y. Guo, L. Liu, G. Ding, Q. Ni, and L. Shao,\n“Unsupervised deep video hashing via balanced code for large-scale\nvideo retrieval,” IEEE Transactions on Image Processing, vol. 28, no. 4,\npp. 1993–2007, 2018.\n[6] J.-P. Heo, Y. Lee, J. He, S.-F. Chang, and S.-E. Yoon, “Spherical\nhashing,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition.\nIEEE, 2012, pp. 2957–2964.\n[7] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin, “Iterative quantiza-\ntion: A procrustean approach to learning binary codes for large-scale\nimage retrieval,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 35, no. 12, pp. 2916–2929, 2012.\n[8] W. Liu, C. Mu, S. Kumar, and S.-F. Chang, “Discrete graph hashing,” in\nAdvances in Neural Information Processing Systems, 2014, pp. 3419–\n3427.\n[9] S. Su, C. Zhang, K. Han, and Y. Tian, “Greedy hash: Towards fast\noptimization for accurate hash coding in cnn,” in Advances in neural\ninformation processing systems, 2018, pp. 798–807.\n[10] E. Yang, T. Liu, C. Deng, W. Liu, and D. Tao, “Distillhash: Unsupervised\ndeep hashing by distilling data pairs,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n2946–2955.\n[11] X. Zhe, S. Chen, and H. Yan, “Deep class-wise hashing: Semantics-\npreserving hashing via class-wise loss,” IEEE Transactions on Neural\nNetworks and Learning Systems, vol. 31, no. 5, pp. 1681–1695, 2019.\n[12] D. Wu, Q. Dai, J. Liu, B. Li, and W. Wang, “Deep incremental hashing\nnetwork for efﬁcient image retrieval,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n9069–9077.\n[13] T.-T. Do, T. Hoang, D.-K. Le Tan, A.-D. Doan, and N.-M. Cheung,\n“Compact hash code learning with binary deep neural network,” IEEE\nTransactions on Multimedia, vol. 22, no. 4, pp. 992–1004, 2019.\n[14] Y. Shen, J. Qin, J. Chen, L. Liu, and F. Zhu, “Embarrassingly simple\nbinary representation learning,” 2019.\n[15] J. Li, W. W. Ng, X. Tian, S. Kwong, and H. Wang, “Weighted multi-deep\nranking supervised hashing for efﬁcient image retrieval,” International\nJournal of Machine Learning and Cybernetics, vol. 11, no. 4, pp. 883–\n897, 2020.\n[16] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2016.\n[18] K. Lin, J. Lu, C.-S. Chen, and J. Zhou, “Learning compact binary\ndescriptors with unsupervised deep neural networks,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition,\n2016, pp. 1183–1192.\n[19] S. Huang, Y. Xiong, Y. Zhang, and J. Wang, “Unsupervised triplet\nhashing for fast image retrieval,” in Proceedings of the on Thematic\nWorkshops of ACM Multimedia, 2017, pp. 84–92.\n[20] M. Hu, Y. Yang, F. Shen, N. Xie, and H. T. Shen, “Hashing with angular\nreconstructive embeddings,” IEEE Transactions on Image Processing,\nvol. 27, no. 2, pp. 545–555, 2017.\n[21] F. Shen, Y. Xu, L. Liu, Y. Yang, Z. Huang, and H. T. Shen, “Unsu-\npervised deep hashing with similarity-adaptive and discrete optimiza-\ntion,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 40, no. 12, pp. 3034–3044, 2018.\n10\n[22] M. Zieba, P. Semberecki, T. El-Gaaly, and T. Trzcinski, “Bingan: Learn-\ning compact binary descriptors with a regularized gan,” in Advances in\nNeural Information Processing Systems, 2018, pp. 3608–3618.\n[23] K. Ghasedi Dizaji, F. Zheng, N. Sadoughi, Y. Yang, C. Deng, and\nH. Huang, “Unsupervised deep generative adversarial hashing network,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 3664–3673.\n[24] Y. Shen, L. Liu, and L. Shao, “Unsupervised binary representation learn-\ning with deep variational networks,” International Journal of Computer\nVision, vol. 127, no. 11-12, pp. 1614–1628, 2019.\n[25] K. Zhao, J. Xu, and M.-M. Cheng, “Regularface: Deep face recognition\nvia exclusive regularization,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, June 2019.\n[26] K. Ghasedi Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang,\n“Deep clustering via joint convolutional autoencoder embedding and\nrelative entropy minimization,” in Proceedings of the IEEE International\nConference on Computer Vision, 2017, pp. 5736–5745.\n[27] R. Zhang, X. Li, H. Zhang, and F. Nie, “Deep fuzzy k-means with\nadaptive loss and entropy regularization,” IEEE Transactions on Fuzzy\nSystems, 2019.\n[28] A. Genevay, G. Dulac-Arnold, and J.-P. Vert, “Differentiable deep clus-\ntering with cluster size constraints,” arXiv preprint arXiv:1910.09036,\n2019.\n[29] H. Tang, K. Chen, and K. Jia, “Unsupervised domain adaptation via\nstructurally regularized deep clustering,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n8725–8735.\n[30] F. Cakir, K. He, S. Adel Bargal, and S. Sclaroff, “Mihash: Online hashing\nwith mutual information,” in Proceedings of the IEEE International\nConference on Computer Vision, 2017, pp. 437–445.\n[31] F. Cakir, K. He, S. A. Bargal, and S. Sclaroff, “Hashing with mutual\ninformation,” IEEE transactions on pattern analysis and machine intel-\nligence, vol. 41, no. 10, pp. 2424–2437, 2019.\n[32] J. D. Esary, F. Proschan, and D. W. Walkup, “Association of random\nvariables, with applications,” The Annals of Mathematical Statistics, pp.\n1466–1474, 1967.\n[33] K. Joag-Dev and F. Proschan, “Negative association of random variables\nwith applications,” The Annals of Statistics, pp. 286–295, 1983.\n[34] B. Dai, R. Guo, S. Kumar, N. He, and L. Song, “Stochastic generative\nhashing,” arXiv preprint arXiv:1701.02815, 2017.\n[35] A. Krizhevsky, “Learning multiple layers of features from tiny images,”\nUniversity of Toronto, 05 2012.\n[36] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, “Nus-wide: a\nreal-world web image database from national university of singapore,” in\nProceedings of the ACM International Conference on Image and Video\nRetrieval, 2009, pp. 1–9.\n[37] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in European Conference on Computer Vision. Springer, 2014,\npp. 740–755.\n[38] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition.\nIEEE, 2009,\npp. 248–255.\n[39] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan, “Supervised hashing for\nimage retrieval via image representation learning.” in AAAI Conference\non Artiﬁcial Intelligence, vol. 1, no. 2014, 2014, p. 2.\n[40] Z. Cao, M. Long, J. Wang, and P. S. Yu, “Hashnet: Deep learning to hash\nby continuation,” in Proceedings of the IEEE International Conference\non Computer Vision, 2017, pp. 5608–5617.\n[41] H. Zhu, M. Long, J. Wang, and Y. Cao, “Deep hashing network for efﬁ-\ncient similarity retrieval,” in AAAI Conference on Artiﬁcial Intelligence,\n2016.\n[42] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal\nof Machine Learning Research, vol. 9, no. Nov, pp. 2579–2605, 2008.\n",
  "categories": [
    "cs.CV",
    "cs.IR",
    "cs.LG"
  ],
  "published": "2020-11-20",
  "updated": "2020-11-20"
}