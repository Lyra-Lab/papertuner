{
  "id": "http://arxiv.org/abs/2011.10714v1",
  "title": "Double Meta-Learning for Data Efficient Policy Optimization in Non-Stationary Environments",
  "authors": [
    "Elahe Aghapour",
    "Nora Ayanian"
  ],
  "abstract": "We are interested in learning models of non-stationary environments, which\ncan be framed as a multi-task learning problem. Model-free reinforcement\nlearning algorithms can achieve good asymptotic performance in multi-task\nlearning at a cost of extensive sampling, due to their approach, which requires\nlearning from scratch. While model-based approaches are among the most data\nefficient learning algorithms, they still struggle with complex tasks and model\nuncertainties. Meta-reinforcement learning addresses the efficiency and\ngeneralization challenges on multi task learning by quickly leveraging the\nmeta-prior policy for a new task. In this paper, we propose a\nmeta-reinforcement learning approach to learn the dynamic model of a\nnon-stationary environment to be used for meta-policy optimization later. Due\nto the sample efficiency of model-based learning methods, we are able to\nsimultaneously train both the meta-model of the non-stationary environment and\nthe meta-policy until dynamic model convergence. Then, the meta-learned dynamic\nmodel of the environment will generate simulated data for meta-policy\noptimization. Our experiment demonstrates that our proposed method can\nmeta-learn the policy in a non-stationary environment with the data efficiency\nof model-based learning approaches while achieving the high asymptotic\nperformance of model-free meta-reinforcement learning.",
  "text": "Double Meta-Learning for Data Efﬁcient Policy Optimization in\nNon-Stationary Environments\nElahe Aghapour and Nora Ayanian\nAbstract— We are interested in learning models of non-\nstationary environments, which can be framed as a multi-\ntask learning problem. Model-free reinforcement learning al-\ngorithms can achieve good asymptotic performance in multi-\ntask learning at a cost of extensive sampling, due to their\napproach, which requires learning from scratch. While model-\nbased approaches are among the most data efﬁcient learning\nalgorithms, they still struggle with complex tasks and model\nuncertainties. Meta-reinforcement learning addresses the efﬁ-\nciency and generalization challenges on multi task learning by\nquickly leveraging the meta-prior policy for a new task. In\nthis paper, we propose a meta-reinforcement learning approach\nto learn the dynamic model of a non-stationary environment\nto be used for meta-policy optimization later. Due to the\nsample efﬁciency of model-based learning methods, we are\nable to simultaneously train both the meta-model of the non-\nstationary environment and the meta-policy until dynamic\nmodel convergence. Then, the meta-learned dynamic model\nof the environment will generate simulated data for meta-\npolicy optimization. Our experiment demonstrates that our\nproposed method can meta-learn the policy in a non-stationary\nenvironment with the data efﬁciency of model-based learning\napproaches while achieving the high asymptotic performance\nof model-free meta-reinforcement learning.\nI. INTRODUCTION\nMany robotic control tasks can be captured as rein-\nforcement learning (RL) problems, where the task objective\nfunction is being optimized by using collected data. RL\napproaches have been applied to a wide range of applications,\nfrom autonomous navigation [1], [2] to manipulation [3], [4].\nModel-free RL algorithms are able to outperform human\nperformance in many applications, but they are known to be\ndata inefﬁcient [9], [10] and must be trained from scratch\nfor a new task, which makes it impractical for many real\nworld implementations. Moreover, RL methods demonstrate\nadmirable performance in simulation [5]–[7], however, their\nperformance may degrade or even fail in slightly different\ntest environments [8]; this is exacerbated in non-stationary\nenvironments, since the number of collected experiences, or\ndata samples, is limited before changes occur.\nFor many robotic control tasks, collecting data is costly\nand the test environment is often different than the training\nenvironment due to non-stationary conditions, simulation to\nreal world gap, and other perturbations. Hence, the robots’\ncapability to quickly generalize to a new condition is critical.\nOne technique to enable robots to use data efﬁciently under\nnon-stationary conditions is meta-learning. Meta-learners are\nable to learn a meta-prior over model parameters of given\nAuthors are with the Department of Computer Science, University of\nSouthern California, USA {aghapour, ayanian}@usc.edu\ntasks that can be quickly generalized to a new task using\nsmall amount of data [11]–[13].\nIn this paper, we build on gradient-based meta-learning\nmethods [14]–[17]. We apply model agnostic meta-learning\n(MAML) to meta-learn a policy and dynamic models.\nMAML, which was ﬁrst proposed in [18], is a gradient-\nbased meta-learning algorithm with two connected phases: 1)\nmeta-training learns a meta-prior model that can be quickly\nleveraged for new tasks, and 2) meta-testing adapts the model\nto the current new task. Many have taken inspiration from\nMAML [19]–[23] due to its efﬁciency and ﬂexibility.\nThe model-free (MF) meta-learning approaches tend to\nattain asymptotically optimal performance at the expense\nof data inefﬁciency [18], [31]–[33]. On the other hand,\nmodel-based (MB) meta-learning methods are data efﬁcient,\nachieved by learning the dynamic model of environment\ninstead of policy optimization [21], [30], [38]. However,\nlearning an accurate dynamic model is often more complex\nthan learning good policies. The performance of most model-\nbased methods relies on accurate learned dynamic models.\nIf the learned dynamic model is not accurate enough, it\ncould lead to model bias. Then, the learned policy using the\ndynamic model could yield suboptimal performance or even\nfailure. Recent works tried to reduce model bias by different\napproaches, e.g., by incorporating model uncertainty into\naction planning [24], [25], or model ensembles [26], [27].\nFinding an optimal policy in a non-stationary environment\nis challenging. A non-stationary environment can be treated\nas a sequence of stationary tasks where the model-free meta-\npolicy optimization can be solved by continuous adaptation\nof meta-learning [29]. Meta-model-based RL algorithms en-\nable sample-efﬁcient learning [21], [30], [38]. The ability to\nadapt online in meta-learning alleviates the need to create a\nperfect model of a complex environment. However, due to the\nchallenge of learning a sufﬁciently accurate dynamic model,\nmodel-based learning approaches struggle to accomplish\ncomplex tasks and achieve the super-human performance of\nmodel-free methods.\nTaking advantage of data efﬁciency in model-based learn-\ning in combination with the asymptotic performance of\nmodel-free learning is an appealing idea for researchers. An\nalternative method is to accommodate the model-free policy\noptimization with learned dynamic model by generating\ntrajectories to ﬁne-tune the policy model initialization [38],\n[39]; however, these approaches still rely on large amounts\nof real-world data. In other works, model-based training and\nmodel-free training are decoupled and the model-free policy\nis trained by generated samples from learned dynamic model\narXiv:2011.10714v1  [cs.LG]  21 Nov 2020\n[40], [41]. However, these methods still rely on accurate\nlearned models of environment. Another alternative is to use\nan ensemble of model-based reinforcement learners which\nare separately learning different dynamic models correspond-\ning to different tasks [34]. Then, an ensemble of learned\nmodels generate sample trajectories to train a meta-policy.\nAlthough this approach is more data efﬁcient than training a\nmodel-free meta-policy, a sufﬁciently large amount of data\nfrom different tasks is required to train separate dynamic\nmodels corresponding to each task.\nIn this paper, we propose a double meta-reinforcement\nlearning (DM-RL) approach to ﬁnd a meta-policy in a non-\nstationary environment when data is costly. Our approach\nhas two training phases. Phase 1: The meta-dynamic model\nand meta-policy are concurrently meta-trained by collecting\nsamples from the environment while actions are selected\nby the learned meta-policy. When the meta-prior for the\ndynamic model converges, we move to phase 2. Phase 2:\nThe meta-policy is trained using meta-data generated by the\nlearned dynamic model. Then, the learned meta-policy can be\nquickly adapted to uncertainties and perturbations between\nthe learned model and the current context in the environment\nat test time. DM-RL is signiﬁcantly more data efﬁcient than\ntraining a meta-policy [18] and training separate model-based\nRLs [34], while its performance closely matches the model-\nfree meta-policy learning approaches. In addition, it often\noutperforms model-based approaches using MPC [30] on\nchallenging tasks.\nII. PRELIMINARIES\nA. Deﬁnitions\nWe\nconsider\nlearning\nproblems\nin\ndiscrete\ntime\nMarkov Decision Processes (MDP) described by a tuple\n{S, A, P, r, p0, γ, }, where S ∈Rn is the state space,\nA ∈Rm is the action space, P : S × A =⇒S is the state\ntransition function, r : S × A =⇒R is the reward function,\np0 is the initial state distribution, γ is a discount factor,\nand H is the length of time horizon. A trajectory of length\nL is denoted by τ = {s0,a0,··· ,sL−1,aL−1,sL} where sk\nand ak denote state and action at time k, respectively. The\ncardinality of a set G is denoted by |G| and the p-norm of\na vector V is represented by ∥V∥p.\nB. Meta-Policy Learning\nThe return is the discounted sum of the expected re-\nward from a trajectory (i.e., R(τ) = ∑(st,at)∈τ γ tr(st,at)). The\nreinforcement learning goal is to ﬁnd an optimal policy\nπ : S =⇒A that selects an optimal action in each state to\nmaximize the expected return. The policy will be modeled\nby a neural network with inertial weights Φ, and its weights\ncan be found by minimizing the RL objective function:\nL(Φ,D) =−1\n|D| ∑\nt=0:|D|\nγ t r(st,at),\ns0 ∼p0, s ∼p(s′|s,a), at ∼πΦ(at|st),\n(1)\nwhere D is training data and the loss function L is a function\nof D and Φ.\nWhile deep reinforcement learning struggles with data\ninefﬁciency and limited generalization, meta-reinforcement\nlearning is a data efﬁcient approach which learns how\nto\nextend\npast\nlearned\npolicies\nto\nnew\ntasks.\nThe\nmeta-reinforcement learning algorithm will ﬁnd a meta-\nprior parameter Φ which can be adapted to new tasks\nquickly. During meta-training, we have a task distribu-\ntion\np(T ) from which tasks T i\nare sampled. Each\ntask can be deﬁned by a different MDP tuple T i =\n{Si, Ai, pi(s j+1|sj,aj), ri(sj,aj), pi\n0, γ, H}. Here, our\nfocus is on learning in non-stationary environments; this is\nanalogous to a distribution of tasks represented by MDPs\nwhere all tasks share the same state space Si = S, action space\nAi = A, reward function ri = r, and initial state distribution\npi\n0 = p0 while the transition function varies over different\ntasks (e.g. T i = {S, A, pi(sj+1|sj,aj), r(sj,aj), p0, γ, H}).\nOur proposed solution is built upon the gradient-based meta-\nlearning framework. We apply the model agnostic meta-\nlearning (MAML) framework [18] where the training data\nDi from task T i is split into meta-train data Di\ntr and meta-\ntest data Di\nts. The MAML objective function is looking for\nΦ such that:\nmin\nΦ\n∑\nT i∼p(T )\nL\n\u0000Φ−α∇ΦL(Φ, Di\ntr), Di\nts\n\u0001\n(2)\nwhere α is learning rate hyper-parameters. An algorithm to\nsolve (2) is discussed in [18].\nC. Learning The Prior Model By Meta-Training\nIn model-based RL methods, we would like to learn the\ntransition function P of the environment. Here, the dynamic\nmodel is represented by a Neural Network (NN) with internal\nweights θ that takes a pair {st,at} as input and predicts the\nstate st+1. Given data D, the loss function to ﬁnd internal\nweights θ is:\nL(θ,D) = 1\n|D|\n∑\n(st,at,st+1)∈D\n∥st+1 −pθ(st, at)∥2\n2.\n(3)\nFor a non-stationary environment, the meta-dynamic\nmodel is required to obtain θ0 that can be quickly leveraged\nto a new task with few gradient steps. Here, we rely on the\nmodel-based meta-reinforcement learning approach detailed\nin [30] to meta-learn θ such that:\nmax\nθ\n1\n|Di|\n∑\nT i∼p(T ) ∑\nt=0:N\nγtr(st,at)\nwhere\ns0 ∼p(s0),s′ ∼pθ(s,a).\n(4)\nThe proposed solution for (4) iterates between two steps:\n1) meta-learn the transition function model Pθ in (2) using\ngiven data and MAML algorithm in [30], and 2) use model\npredictive control (MPC) and learned model pθ to ﬁnd a\nsequence of actions with maximum return. The ﬁrst action\nat will be executed, and leads to state st+1. The tuple\n{st,at,st+1} will be added to dataset Di to update the meta-\nprior model Pθ.\nSolving MPC imposes high computational cost which\nleads to a sampling based MPC scheme: ncandidate random\n(a) Phase 1\n(b) Phase 2\nFig. 1.\nData ﬂow diagram of double meta-learning algorithm. The blue and black lines represent lower and higher frequency of data, respectively. In\nphase 1, at each time instant, the meta-policy must choose the next action, interacting with environment (high frequency black lines) while meta-policy\nand meta-dynamic models are updated using batch of accumulated data (low frequency blue lines). In phase 2, at each time instant, meta-policy chooses\nthe next action, interacting with meta-dynamic model (high frequency black lines). However, The meta-dynamic model updates once before generating a\nbatch of data and meta-policy updates once using batch of simulated data (low frequency blue lines).\ncandidate action sequences of length H are generated, and\nthe sequence of actions with the highest predicted reward is\nselected. The reward of the action sequence is computed with\nthe given reward function and learned dynamic model. The\nsampling-based MPC solution has sub-optimal performance\nand could even lead to failure, especially in high dimensional\nor continuous action spaces. Here, our main goal is to\nbe as data efﬁcient as model-based meta-RL with optimal\nperformance as good as meta-policy algorithms.\nIII. DOUBLE META-LEARNING FOR POLICY\nOPTIMIZATION\nIn this section, we propose a DM-RL approach to learn the\nmeta-policy parameter Φ in a non-stationary environment.\nA non-stationary condition can be captured as a sequence\nof tasks where the problem of learning is seen as a few\nshot learning problem: we can collect limited samples from\nthe environment before it changes. We assume that during\ntraining, a distribution of tasks p(T ) is given, from which\ntasks T i are drawn. Since the on-policy methods can learn\nexploration strategies during meta-training, we collect data\nsamples Di corresponding to task T i using meta-RL policy\nπΦ. We seek meta-dynamic model parameter θ and meta-\npolicy parameter Φ such that:\nmax\nθ,Φ\n1\n|Di|\n∑\nT i∼p(T ) ∑\nt=0:|Di|\nγtr(st,at)\nwhere\ns0 ∼p(s0), s′ ∼pθ(s,a), a ∼πΦ(a,s),\n(5)\nwhere the reward function r(s,a) is given. The optimization\nsolution can be divided into two sub-problems:\n• Policy ﬁtting: Applying meta-learning approaches to\nmeta-train policy parameter Φ. Here, Φ is learned using\nthe gradient based meta-learning approach described in\n[18].\n• Dynamic model ﬁtting: Meta-training the dynamic\nmodel of the environment θ using data sample Dis.\nHere, gradient based meta-learning is used as described\nin [30].\nNote that these two sub-problems can be solved concurrently.\nThe algorithm to solve optimization (5) is outlined in Al-\ngorithm 1, where the policy is meta-trained in lines 7 and\n10 and the dynamic model is ﬁtted by lines 8 and 11. We\nstore data Di collected from the real-world in a buffer D and\nrepeat Algorithm 1 until the meta-learned dynamic model\nconverges. Figure 1(a) demonstrates the algorithm described\nhere.\nMoving into the next phase, the learned model Pθ will be\nused to generate simulated data until meta-policy parameter\nΦ satisﬁes the performance condition. The meta-learned\ndynamic model is capable of quickly adapting to new tasks\nat test time. The pool of samples D f is assumed as the true\ndistribution of data. We uniformly draw data trajectories Dis\nfrom the data buffer D f , then selected Di will be used for\ndynamic model adaptation Pθi to task T i. Afterwards, the\nadapted dynamic model Pθi can generate simulated data to\nmeta-train policy parameters Φ. This algorithm is described\nin Fig. 1(b) and Algorithm 2.\nAs discussed in Achille et al., the early transient in training\na neural network is critical in determining the ﬁnal solution\nand its convergence [35]. Here, the meta-policy parameters\nΦ are initially meta-trained directly by true environment\ndata. The generated data by Pθ is not exactly the same as\ntrue environment data due to model inaccuracy. However,\nmeta-policy parameters Φ are capable of fast adaptation to\nnew tasks and overcome small errors between the learned\ndynamic model and real data at test time.\nAlgorithm 1 Phase 1: Meta-learning dynamic model and\npolicy\nInput: Distribution over tasks p(T )\nInput: Learning rates α and β\n1: Randomly initialize the dynamic model Pθ and the policy\nπΦ\n2: Initialize data buffer Df = /0\n3: while not done do\n4:\nSample batch of tasks T i from p(T )\n5:\nfor all T do\n6:\nSample trajectories Di using policy πΦ and D f =\nD f ∪Di split Di into Di\ntr and Di\nts\n7:\nUpdate Φ = Φ−β ∇Φ L(Φ, Di\ntr)\n8:\nUpdate θ = θ −α ∇θ L(θ, Di\ntr)\n9:\nend for\n10:\nΦ ←−Φ−β ∇Φ 1\n|T | ∑T i L(Φ, Di\nts)\n11:\nθ ←−θ −α ∇θ\n1\n|T | ∑T i L(θ, Di\nts)\n12: end while\nOutput: Meta-learned dynamic model Pθ and meta-policy\nparameters Φ, data buffer Df\nThe standard approach to train a model-free RL using the\ndata efﬁciency of model-based RL is to alternate between\nmodel learning and policy optimization. In learning the\nmodel, environment samples are used to ﬁt the dynamic\nmodel then the learned model is used to search for policy\nimprovement [42], [43]. This setting can work well with\nlow dimensional simple environments. However, it could be\nhighly unstable in more challenging continuous control tasks\nsince the policy tends to exploit the regions where insufﬁcient\ndata is available [41]. Here, to prevent this instability, the\nmeta-policy is being used as a controller to explore and\nexploit the environment in phase 1 and both meta-policy and\nmeta-dynamic models are being meta-learned using the same\nenvironment data.\nIV. EXPERIMENTS\nThe main goal of this section is to study the following\nquestions:\n1) Does our proposed solution successfully train a meta-\npolicy which can be quickly adapted at test time? How\nrobust is our proposed method when it experiences a\nnew task that is outside the distribution of the training\ntasks?\n2) How does our proposed algorithm perform in com-\nparison with model-based RL MAML and model-free\nRL MAML with respect to sample efﬁciency and\nperformance? How quickly does our solution adapt to\na new task at test time in comparison with model-based\nRL MAML and Model-free RL MAML?\nWe evaluate the performance of DM-RL in comparison\nwith the following methods:\n• Model-free\nmeta-reinforcement\nlearning\n(MF-\nMAML-RL): to evaluate data efﬁciency, we compare\nAlgorithm 2 Phase 2: Learning meta-policy from simulated\ndata\nInput: Meta-learned dynamic model Pθ, learning rate α\nInput: Meta-policy parameters Φ, learning rate β\nInput: Data buffer Df\n1: while not done do\n2:\nRandomly choose batch of Dis from D f\n3:\nfor all Di do\n4:\nwhile not done do\n5:\nUse Di to update θ corresponding to T i by:\nθ i ←−θ −α ∇θL(θ, Di)\n6:\nend while\n7:\nSample batch of data ¯Di using dynamic model\nPθi and policy πΦ\n8:\nUpdate Φ = Φ−β ∇ΦL(Φ, ¯Di)\n9:\nend for\n10:\nUpdate Φ ←−Φ−β ∇Φ 1\n|T | ∑T i L(Φ, ¯Di)\n11: end while\nOutput: Meta-policy parameters Φ\nwith the model-free model agnostic meta-learning\n(MAML) RL method, as described in [18].\n• Model-based\nmeta-reinforcement\nlearning\n(MB-\nMAML-RL): To study the computational cost and ﬁnal\nperformance, we implemented the model-based model\nagnostic meta-learning (MAML) RL method where\nmodel-based MAML RL uses MPC to choose its future\naction as described in [30].\nFor the sake of consistency, the hyper-parameter settings\nfor meta-policy learning and meta-dynamic model learning\nin DM-RL is chosen to be the same as MF-MAML-RL and\nMB-MAML-RL hyper-parameters, respectively. The MPC\nparameters and implementation is the same as the original\npaper [30].\nA. Implementation Setup\nThe main motivation of our work is to propose a learning-\nbased controller for unmanned aerial vehicles, ﬂying in\nhighly dynamic environments. This requires an environment\nto reﬂect our real world problem. Our evaluation environment\nis derived from OpenAi Gym’s LunarLander-v2 environment\nwhere the goal is to train the lunar lander to safely land on\nrandomly generated speciﬁed surfaces on the moon [45]. To\nadapt the LunarLander environment to be a satisfying option\nfor meta-learning experiments, we add a wind generator\nfunction that generates wind with different speeds along the\nx-y axis. Different wind speed corresponds to a different state\ntransition function, and subsequently a different task.\nDuring training, a suite of tasks is deﬁned by generating\nwinds with different speeds along the x-y axis, drawn from\na uniform distribution U[−2, 2]. The wind speed is assumed\nto be static throughout the duration of each rollout. For\ntest time, we have two different scenarios: 1) Generating\nwind with constant speed drawn from the same uniform\ndistribution during training but the lander has not seen it\nin its training data; and 2) Generating sinusoidal wind to\nstudy whether LunarLander can learn to extend its past\nexperiences to a new situation which is different than its\ntraining distribution.\nIn terms of hyper-parameter settings of our proposed\nalgorithm, the dynamic model is trained on a four layer\nneural network with two hidden layers of size 64 and 32. The\nactivation function is Rectiﬁed Linear Unit (ReLU) and both\nReLU and softmax are used in the output layer. The policy\nmodel is also represented by a four layer neural network with\ntwo hidden layers of size 64. The activation function is ReLU\nand categorical for the output layer. We applied TRPO as the\nmeta-optimizer [44]. We utilize ﬁnite differences to compute\nthe Hessian-vector products for TRPO in order to avoid third\nderivatives. Table I shows the value of the hyper-parameters.\nTABLE I\nVALUE OF HYPER-PARAMETERS\nHyper-parameters\nValue\nMeta-batch size\n10\nncandidate (MPC)\n1000\nHorizon length H (MPC)\n10\nMaximum rollout length\n150\nNumber of iterations\n200\nMonte Carlo trials\n10\nB. Results\nFigure 2 shows the average returns of meta-batches during\nmeta-training. Each iteration uses one meta-batch of data.\nThe red curve is meta-policy return in DM-RL. The blue and\ngreen curves represent MF-MAML-RL and MB-MAML-\nRL, respectively. Both MF MAML and MB MAML are\nonly using true environment data and their return increases\nover time until convergence. The MF approach achieves\ngood asymptotic performance but at the cost of extensive\ndata, converging around iteration = 140, while the MB ap-\nproach exhibits data efﬁcient learning and converges around\niteration = 40. However, due to model uncertainty and\nsuboptimality in the sample-based approximation of MPC, it\nstruggles to perform well on accomplishing tasks that require\nrobust planning.\nFor the ﬁrst 40 iterations, the meta-policy of DM-RL is\nbeing trained by Algorithm 1, which uses true environment\ndata, and its return increases over time. At iteration 40, once\nthe meta-dynamic model converges, we switch to phase 2\n(Algorithm 2), where the meta-learned dynamic model will\ngenerate simulated data. This switching from environment\ndata to simulated data, combined with learned model inaccu-\nracies, causes a drop at iteration = 40. Then, using generated\ndata, the meta-policy return again increases to convergence.\nAlthough DM-RL needs more training iterations for conver-\ngence, only the ﬁrst 40 iterations use true environment data.\nThe performance of all three approaches is summarized in\nTable II.\nAt test time, we evaluate the capability of the meta-learned\nmodels to leverage to a new task. The ﬁrst scenario is\nin a new static environment. We applied the meta-learned\nFig. 2.\nTraining curve of DM-RL in comparison with model-free MAML\nRL and model-based MAML RL averaged over 10 trials. DM-RL switches\nto algorithm 2 at iteration= 40 which means it does not use true environment\ndata anymore. DM-RL is able to closely match the asymptotic performance\nof MF methods with fewer true environment samples.\nTABLE II\nPERFORMANCE COMPARISON AT TRAINING.\nMF-MAML-RL\nMB-MAML-RL\nDM-RL\nReturn mean\n71.59\n55.08\n69.67\nafter convergence\nRequired batches\n1400\n400\n1600\nto converge\nRequired batches\n1400\n400\n400\nfrom env to converge\napproaches in an environment where new constant wind was\nselected from the range [−2,2], but none of the learning\nmodels had seen it during training. Figure 3 shows the\nperformance of each algorithm. The MF-MAML-RL uses\n20 rollouts to quickly adapt to the new task. DM-RL starts\nwith lower returns due to dynamic model uncertainty in the\nsecond phase of training, but achieves the MF performance,\nusing 25 rollouts. The MB-MAML-RL starts similarly to our\nmethod due to learned dynamic model uncertainty, but its\nperformance is not as robust as the other methods due to the\napproximation in sample based MPC implementation. Table\nIII summarizes the comparison. Note that all three methods\nare only using environment samples at test time. Thus, last\ntwo rows of Table III and IV have the same values.\nThe constant velocity assumption on generated wind is not\nrealistic compared to real-world conditions where wind speed\ncan change over time. To approach real-world conditions,\nin the second scenario the environment is dynamic. The\ngenerated wind speed v is sinusoidal along the x-y axis:\nv(tk) =\n\u0014\nAx sin(ωxtk)\nAy sin(ωytk)\n\u0015\nwith amplitude A = 2 and frequency ω = 0.01Hz. The\ngenerated wind remains in [−2,2] but changes over time.\nThe new given environment is dynamically different than\nthe meta-training data. However, the meta-learned models\nmust be capable of extending their past experiences to a\nnew task quickly. Figure 4 shows the performance of the\nFig. 3.\nFirst scenario (static environment): The comparison of returns\nat test time. Both the model-free MAML RL and DM-RL achieve the\nasymptotic performance.\nTABLE III\nPERFORMANCE COMPARISON AT TEST TIME FOR FIRST SCENARIO\n(STATIC ENVIRONMENT)\nMF-MAML-RL\nMB-MAML-RL\nDM-RL\nReturn mean\n69.08\n60.36\n67.39\nafter convergence\nRequired batches\n20\n25\n25\nto converge\nRequired batches\n20\n25\n25\nfrom env to converge\ndifferent approaches. MF-MAML-RL return increases over\ntime and converges at iteration = 40. DM-RL return starts\nlower than the model-free approach due to the learned model\ndynamic inaccuracy but achieves the asymptotic performance\nof MF-MAML-RL using 70 rollouts. Although DM-RL uses\n30 more batches at test time to adapt to a new task, it uses\nonly 400 batches from the environment during training while\nMF-MAML-RL used 1400 batches to be trained (i.e., more\nthan three times the meta-training data). The MB-MAML-RL\nperformance is not as good as the other methods since the\nenvironment is dynamic and it requires learning a dynamic\nmodel accurate enough to work with MPC. Performance\nvalues are summarized in Table IV.\nOur results demonstrate that:\n1) DM-RL can successfully train a meta-policy that is\nquickly generalized to a new task at test time even\nwhen the new task was changing over time in the test\nenvironment (see Figures 3 and 4); and\n2) DM-RL can be trained as data efﬁciently as model-\nbased meta-RL (see Figure 2) while achieving the\nperformance of model-free meta-RL at the cost of a\nfew more trajectories from the test environment (see\nFigures 3 and 4). Since DM-RL trains the meta-policy\nwith true environment samples at phase 1 and meta-\ndata generated by the meta-dynamic model at phase 2,\na few more trajectories at test time are required due\nto the difference between the data generated by meta-\ndynamic model and true environment data. However, it\nFig. 4.\nSecond scenario (dynamic environment): DM-RL outperforms\nmodel-based MAML RL when new task is dynamic and need online\nadaptation. Our method converges to model-free MAML RL within 70\nrollouts.\nTABLE IV\nPERFORMANCE COMPARISON AT TEST TIME FOR SECOND SCENARIO\n(DYNAMIC ENVIRONMENT)\nMF-MAML-RL\nMB-MAML-RL\nDM-RL\nReturn mean\n71.59\n55.08\n69.67\nafter convergence\nRequired batches\n40\n70\n70\nto converge\nRequired batches\n40\n70\n70\nfrom env to converge\nis still signiﬁcantly more data efﬁcient than MF meta-\nRL.\nV. CONCLUSIONS\nIn this paper, we presented a simple and generally appli-\ncable approach to efﬁciently meta-learn a policy in a non-\nstationary environment. We combined the advantage of data\nefﬁciency in model-based learning with asymptotic perfor-\nmance of model-free approaches in the context of double\nmeta-learning where initially the meta-dynamic model of\nthe environment and meta-policy are concurrently being\ntrained using environment data until the meta-dynamic model\nconverges. Then, the meta-dynamic model is used to generate\ndata to meta-train the policy.\nDue to applying meta-learning to optimize the policy\nin a dynamic environment, the meta-policy at test time is\nrobust to environment changes and can be quickly adapted to\nnew tasks. Our experimental results show that our proposed\napproach achieves the asymptotic performance of model-\nfree meta-reinforcement learning approach with considerably\nsmaller sets of meta-training data. We also evaluate our\nmethod against model-based meta-reinforcement learning\nwhile using the same amount of meta-training data; our\nmethod demonstrates better performance than the model-\nbased method at test time. An exciting future direction is to\napply our approach for real world system implementation.\nOur proposed approach relies on on-policy data, limiting its\nsample efﬁciency. Applying off-policy meta-reinforcement\nlearning to our approach would also be of interest.\nREFERENCES\n[1] Abbeel, Pieter, Adam Coates, and Andrew Y. Ng. ”Autonomous heli-\ncopter aerobatics through apprenticeship learning.” The International\nJournal of Robotics Research 29.13 (2010): 1608-1639.\n[2] Zhang, Tianhao, Gregory Kahn, Sergey Levine, and Pieter Abbeel.\n”Learning deep control policies for autonomous aerial vehicles with\nmpc-guided policy search.” IEEE international conference on robotics\nand automation (ICRA). 2016.\n[3] Kalakrishnan, Mrinal, Ludovic Righetti, Peter Pastor, and Stefan\nSchaal. ”Learning force control policies for compliant manipulation.”\nIEEE International Conference on Intelligent Robots and Systems.\n2011.\n[4] Gu, Shixiang, Ethan Holly, Timothy Lillicrap, and Sergey Levine.\n”Deep reinforcement learning for robotic manipulation with asyn-\nchronous off-policy updates.” IEEE international conference on\nrobotics and automation (ICRA). 2017\n[5] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den\nDriessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V.,\nLanctot, M. and Dieleman, S. ”Mastering the game of Go with deep\nneural networks and tree search.” nature 529.7587 (2016): 484-489.\n[6] Levine, Sergey, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. ”End-\nto-end training of deep visuomotor policies.” The Journal of Machine\nLearning Research 17.1 (2016): 1334-1373.\n[7] Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves,\nIoannis Antonoglou. ”Playing atari with deep reinforcement learning.”\narXiv preprint arXiv:1312.5602 (2013).\n[8] Bousmalis, K., Irpan, A., Wohlhart, P., Bai, Y., Kelcey, M., Kalakr-\nishnan, M., Downs, L., Ibarz, J., Pastor, P., Konolige, K. and Levine.\n”Using simulation and domain adaptation to improve efﬁciency of\ndeep robotic grasping.” IEEE international conference on robotics and\nautomation (ICRA). 2018.\n[9] Kamthe, Sanket, and Marc Deisenroth. ”Data-efﬁcient reinforcement\nlearning with probabilistic model predictive control.” International\nConference on Artiﬁcial Intelligence and Statistics. PMLR, 2018.\n[10] Dulac-Arnold,\nGabriel,\nDaniel\nMankowitz,\nand\nTodd\nHester.\n”Challenges of real-world reinforcement learning.” arXiv preprint\narXiv:1904.12901 (2019).\n[11] Schmidhuber, J¨urgen. ”On learning to think: Algorithmic information\ntheory for novel combinations of reinforcement learning controllers\nand recurrent neural world models.” arXiv preprint arXiv:1511.09249\n(2015).\n[12] Zintgraf, Luisa, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze,\nYarin Gal, Katja Hofmann, and Shimon Whiteson. ”VariBAD: A\nVery Good Method for Bayes-Adaptive Deep RL via Meta-Learning.”\nInternational Conference on Learning Representations (ICRL), 2020.\n[13] Duan, Yan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever,\nand Pieter Abbeel. ”Rl\n2: Fast reinforcement learning via slow\nreinforcement learning.” International Conference on Learning Rep-\nresentations (ICRL), 2017.\n[14] Andrychowicz, Marcin, Misha Denil, Sergio Gomez, Matthew W.\nHoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando\nDe Freitas. ”Learning to learn by gradient descent by gradient de-\nscent.” Advances in neural information processing systems. 2016.\n[15] Hochreiter, Sepp, A. Steven Younger, and Peter R. Conwell. ”Learning\nto learn using gradient descent.” International Conference on Artiﬁcial\nNeural Networks. 2001.\n[16] Ravi, Sachin, and Hugo Larochelle. ”Optimization as a model for few-\nshot learning.” (2016).\n[17] Nichol, Alex, Joshua Achiam, and John Schulman. ”On ﬁrst-order\nmeta-learning algorithms.” arXiv preprint arXiv:1803.02999 (2018).\n[18] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. ”Model-agnostic\nmeta-learning for fast adaptation of deep networks.” International\nConference on Machine Learning, PMLR 70:1126-1135, 2017.\n[19] Antoniou, Antreas, Harrison Edwards, and Amos Storkey. ”How to\ntrain your MAML.” International Conference on Learning Represen-\ntations (ICRL), 2019.\n[20] Nichol, Alex, Joshua Achiam, and John Schulman. ”On ﬁrst-order\nmeta-learning algorithms.” arXiv preprint arXiv:1803.02999 (2018).\n[21] Nagabandi, Anusha, Chelsea Finn, and Sergey Levine. ”Deep online\nlearning via meta-learning: Continual adaptation for model-based RL.”\nInternational Conference on Learning Representations (ICRL), 2019.\n[22] Li, Zhenguo, Fengwei Zhou, Fei Chen, and Hang Li. ”Meta-SGD:\nLearning to learn quickly for few-shot learning.” arXiv preprint\narXiv:1707.09835 (2017).\n[23] Grant, Erin, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas\nGrifﬁths. ”Recasting gradient-based meta-learning as hierarchical\nbayes.” International Conference on Learning Representations (ICRL),\n2018.\n[24] Zhou, Kemin, John Comstock Doyle, and Keith Glover. Robust and\noptimal control. Vol. 40. New Jersey: Prentice hall, 1996.\n[25] Deisenroth, Marc, and Carl E. Rasmussen. ”PILCO: A model-based\nand data-efﬁcient approach to policy search.” International Conference\non machine learning (ICML). 2011.\n[26] Rajeswaran, Aravind, et al. ”EPOpt: Learning robust neural network\npolicies using model ensembles.” International Conference on Learn-\ning Representations (ICRL), 2017.\n[27] Lim, Shiau Hong, Huan Xu, and Shie Mannor. ”Reinforcement\nlearning in robust markov decision processes.” Advances in Neural\nInformation Processing Systems. 2013.\n[28] Clavera, Ignasi, Jonas Rothfuss, John Schulman, Yasuhiro Fujita,\nTamim Asfour, and Pieter Abbeel. ”Model-based reinforcement learn-\ning via meta-policy optimization.” arXiv preprint arXiv:1809.05214\n(2018).\n[29] Al-Shedivat, Maruan, Trapit Bansal, Yuri Burda, Ilya Sutskever,\nIgor Mordatch, and Pieter Abbeel. ”Continuous adaptation via meta-\nlearning in non-stationary and competitive environments.” Interna-\ntional Conference on Learning Representations (ICRL), 2018.\n[30] Clavera, Ignasi, Anusha Nagabandi, Ronald S. Fearing, Pieter Abbeel,\nSergey Levine, and Chelsea Finn. ”Learning to adapt: Meta-learning\nfor model-based control.” arXiv preprint arXiv:1803.11347 3 (2018).\n[31] Wang, Jane X., Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer,\nJoel Z. Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and\nMatt Botvinick. ”Learning to reinforcement learn.” Cognitive Science\nSociety (CogSci), 2017\n[32] Mishra, Nikhil, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel.\n”A simple neural attentive meta-learner.” International Conference on\nLearning Representations (ICRL), 2018.\n[33] Rothfuss, Jonas, Dennis Lee, Ignasi Clavera, Tamim Asfour, and\nPieter Abbeel. ”Promp: Proximal meta-policy search.” arXiv preprint\narXiv:1810.06784 (2018).\n[34] Clavera, Ignasi, Jonas Rothfuss, John Schulman, Yasuhiro Fujita,\nTamim Asfour, and Pieter Abbeel. ”Model-based reinforcement learn-\ning via meta-policy optimization.” International Conference on Learn-\ning Representations (ICRL), 2019.\n[35] Achille, Alessandro, Matteo Rovere, and Stefano Soatto. ”Critical\nlearning periods in deep neural networks.” International Conference\non Learning Representations (ICRL), 2019.\n[36] Williams, Ronald J. ”Simple statistical gradient-following algorithms\nfor connectionist reinforcement learning.” Machine learning 8.3-4\n(1992): 229-256.\n[37] Lenz, Ian, Ross A. Knepper, and Ashutosh Saxena. ”DeepMPC:\nLearning deep latent features for model predictive control.” Robotics:\nScience and Systems. 2015.\n[38] Clavera, Ignasi, Jonas Rothfuss, John Schulman, Yasuhiro Fujita,\nTamim Asfour, and Pieter Abbeel. ”Neural network dynamics for\nmodel-based deep reinforcement learning with model-free ﬁne-\ntuning.” 2018 IEEE International Conference on Robotics and Au-\ntomation (ICRA). IEEE, 2018.\n[39] Gu, Shixiang, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine.\n”Continuous deep Q-learning with model-based acceleration.” Inter-\nnational Conference on Machine Learning. 2016.\n[40] Feinberg, Vladimir, Alvin Wan, Ion Stoica, Michael I. Jordan,\nJoseph E. Gonzalez, and Sergey Levine. ”Model-based value estima-\ntion for efﬁcient model-free reinforcement learning.” arXiv preprint\narXiv:1803.00101 (2018).\n[41] Kurutach, Thanard, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter\nAbbeel. ”Model-ensemble trust-region policy optimization.” Interna-\ntional Conference on Learning Representations (ICRL), 2018.\n[42] Kumar, Vikash, Emanuel Todorov, and Sergey Levine. ”Optimal con-\ntrol with learned local models: Application to dexterous manipulation.”\nIEEE International Conference on Robotics and Automation (ICRA).\n2016.\n[43] Levine, Sergey, and Pieter Abbeel. ”Learning neural network policies\nwith guided policy search under unknown dynamics.” Advances in\nNeural Information Processing Systems. 2014.\n[44] Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and\nPhilipp Moritz. ”Trust region policy optimization.” International con-\nference on machine learning. 2015.\n[45] Brockman, Greg, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wojciech Zaremba. ”Openai gym.”\narXiv preprint arXiv:1606.01540 (2016).\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-11-21",
  "updated": "2020-11-21"
}