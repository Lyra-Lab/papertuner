{
  "id": "http://arxiv.org/abs/2011.11128v3",
  "title": "Deep Learning in EEG: Advance of the Last Ten-Year Critical Period",
  "authors": [
    "Shu Gong",
    "Kaibo Xing",
    "Andrzej Cichocki",
    "Junhua Li"
  ],
  "abstract": "Deep learning has achieved excellent performance in a wide range of domains,\nespecially in speech recognition and computer vision. Relatively less work has\nbeen done for EEG, but there is still significant progress attained in the last\ndecade. Due to the lack of a comprehensive and topic widely covered survey for\ndeep learning in EEG, we attempt to summarize recent progress to provide an\noverview, as well as perspectives for future developments. We first briefly\nmention the artifacts removal for EEG signal and then introduce deep learning\nmodels that have been utilized in EEG processing and classification.\nSubsequently, the applications of deep learning in EEG are reviewed by\ncategorizing them into groups such as brain-computer interface, disease\ndetection, and emotion recognition. They are followed by the discussion, in\nwhich the pros and cons of deep learning are presented and future directions\nand challenges for deep learning in EEG are proposed. We hope that this paper\ncould serve as a summary of past work for deep learning in EEG and the\nbeginning of further developments and achievements of EEG studies based on deep\nlearning.",
  "text": "Deep Learning in EEG: Advance of the Last\nTen-Year Critical Period\nShu Gong, Kaibo Xing ∗, Andrzej Cichocki †, and Junhua Li ‡§\nMay 24, 2021\nAbstract:\nDeep learning has achieved excellent performance in a wide range of do-\nmains, especially in speech recognition and computer vision. Relatively less\nwork has been done for EEG, but there is still signiﬁcant progress attained in\nthe last decade. Due to the lack of a comprehensive and topic widely covered\nsurvey for deep learning in EEG, we attempt to summarize recent progress\nto provide an overview, as well as perspectives for future developments. We\nﬁrst brieﬂy mention the artifacts removal for EEG signal and then introduce\ndeep learning models that have been utilized in EEG processing and classiﬁ-\ncation. Subsequently, the applications of deep learning in EEG are reviewed\nby categorizing them into groups such as brain-computer interface, disease\ndetection, and emotion recognition. They are followed by the discussion, in\nwhich the pros and cons of deep learning are presented and future directions\n∗S. Gong and K. Xing are with the College of Life Sciences, Sichuan University, Chengdu\n610065, China.\n†A. Cichocki is with the Skolkovo Institute of Science and Technology (SKOLTECH),\nMoscow 121205, Russia, and also with the Systems Research Institute, Polish Academy of\nSciences, 01-447 Warsaw, Poland.\n‡J. Li is with the School of Computer Science and Electronic Engineering, Univer-\nsity of Essex, Colchester CO4 3SQ, UK, and also with the Laboratory for Brain-Bionic\nIntelligence and Computational Neuroscience, Wuyi University, Jiangmen 529020, China.\n§This work was supported by the National Natural Science Foundation of China under\nGrant 61806149 and the Guangdong Basic and Applied Basic Research Foundation under\nGrant 2020A1515010991. This work was also supported by the Ministry of Education and\nScience of the Russian Federation under Grant 14.756.31.0001. Corresponding author:\nJunhua Li (e-mail: juhalee.bcmi@gmail.com)\n1\narXiv:2011.11128v3  [eess.SP]  20 May 2021\nand challenges for deep learning in EEG are proposed. We hope that this pa-\nper could serve as a summary of past work for deep learning in EEG and the\nbeginning of further developments and achievements of EEG studies based\non deep learning.\nKeywords:\nDeep Learning, Electroencephalogram (EEG), Classiﬁcation, Brain Com-\nputer Interface, Disease, Emotion, Sleep, Mental State\n1\nIntroduction\nMachine learning technology has beneﬁted to diverse domains in our modern\nsociety [1], [2]. Deep learning, a subcategory of machine learning technology,\nhas been showing excellent performance in pattern recognition [3], dramat-\nically improving classiﬁcation accuracy. It is worth noting that new world\nrecords were created by using deep learning in many competitions such as\nImageNet Competition [4]. The research outcomes of deep learning in speech\nrecognition [5] and computer vision [6] have been successfully utilized to de-\nvelop practical application systems, which are remarkably inﬂuencing our life\nand even changing our lifestyle.\nDeep learning is an enhanced variant of traditional neural network, which\nis thought to be established based on the inspiration of hierarchical structure\nexisting in visual cortex of the human brain. The adjective ’deep’ in the\nterm of deep learning describes the attribute of multiple processing layers\nforming a long-cascaded architecture. The extracted information becomes\nmore and more abstract from the lowest layer to the highest layer. This is\none of the advantages for the deep learning as information expression could\nbe more meaningful when passing onto a higher layer.\nMeanwhile, deep\nlearning suﬀers from the issues of slow convergence and high computation\ndemand. These disadvantages have been released by introducing training\nstrategies such as dropout [7] and batch normalization [8], and the availability\nof high-performance computers. The high performance is not only due to the\ncapacity improvement of central processing units, but also new computing\nunits such as graphic processing unit and tensor processing unit. These new\ncomputing units are designed to suit matrix manipulation, which greatly\nreduce computational time in deep learning. Moreover, the availability of\nlarge scale of data and increased capacity of data storage also promote the\nuse of deep learning.\n2\nFigure 1: Numbers of the published papers in each year. Note that numbers\nbefore 2015 are omitted because of rare papers.\n3\nElectroencephalogram (EEG) signal was ﬁrst recorded by Hans Berger\nin the year of 1924 [9], which manifests underlying brain activity. Multiple\nelectrodes can be set to record EEG signal by placing them on diﬀerent lo-\ncations of the scalp and temporal ﬂuctuations in voltage can be captured\nin a high resolution (e.g., in milliseconds) by using a high sampling rate.\nWith the advantages of multi-channel recording and high temporal resolu-\ntion, EEG has been applied to numerous domains from brain-computer in-\nterface [10, 11, 12, 13], to emotion [14, 15], to cognition [16], to brain diseases\n[17]. EEG processing methodology is evolved from simple methods such as\nmean and amplitude comparison to complicated methods such as connectiv-\nity topology and deep learning. In particular, deep learning exhibits better\nperformance in EEG classiﬁcation (a.k.a., recognition or identiﬁcation) com-\npared to conventional methods (e.g., support vector machine).\nBy using\ndeep learning, discriminative features could be extracted without handcraft,\nwhich requires speciﬁc knowledge and expertise. It could avoid the low per-\nformance derived from unsuitable handcrafted features. However, deep learn-\ning is not a destination because model architecture and parameters have to\nbe set manually. A good classiﬁcation performance is usually not obtained\nby just feeding data into a deep learning model. This is because the target\nsignal is much weaker than the background signal and noise, resulting in a\nlow signal-to-noise ratio. Therefore, artifacts removal is commonly adopted\nto remove artifacts so that the signal-to-noise ratio can be improved before\nfeeding into a deep learning model. This is quite diﬀerent compared to image\nor video processing, where image or video is directly fed into a deep learning\nmodel. To date, diﬀerent kinds of deep learning models have been employed\nto process and classify EEG signal. Cecotti et al. used convolutional neural\nnetwork (CNN) to extract features from steady-state visual evoked potential\nin 2008 [18]. Li et al. employed denoising autoencoder to classify two classes\nof motor imagery using EEG recorded from 14 electrodes on the sensorimo-\ntor cortex [19]. Tsiouris et al. applied recurrent neural network (RNN) to\ncapture sequential relationships for seizure detection [20]. A survey covering\nsix EEG-based applications was done in 2019, where studies were reviewed\nseparately for task type, model type and so on [21].\nA more specialized\nsurvey on motor imagery classiﬁcation can be found in [22]. A distribution\nsummary showing which disease is dominantly targeted in the studies of deep\nlearning-based disease diagnosis can be found in [23]. If you want to read a\nsurvey on brain-computer interface (more beyond motor imagery), it can be\nfound in Section 5 of [24]. If a wide range of topics of deep learning in EEG\n4\nis sought, this survey can be an option.\nAlthough EEG domain is far behind compared to the domains such as\ncomputer vision [25] and speech recognition [26] in terms of adopting deep\nlearning, signiﬁcant progress has been achieved in the last decade. It is time\nto summarize the achievements of deep learning in EEG for the past 10 years\nand discuss current existing issues and future directions. The searching crite-\nrion [”Deep Learning” AND ”EEG” AND ”Classiﬁcation” OR ”Recognition”\nOR ”Identiﬁcation”] was used for literature retrieval in the Web of Science\nin March 2020. After manual selection, 193 papers were included in this\nsurvey. During the revision in February 2021, we applied the same searching\ncriterion to ﬁnd newly-published literature after the previous searching and\nselected 20 papers to be included in this survey. After the acceptance, seven\nmore papers were further included, but they were not used to update the\nﬁgures and tables due to the constrained time.\nAs shown in Fig. 1, the majority of these papers were published after\n2017 while there was a rapid increase from the year of 2019. In 2019, the\nnumber of papers in the topic of brain-computer interface and disease de-\ntection are signiﬁcantly more than the other topics. In 2020, the numbers\nof the published papers in more topics are rapidly increased, although dis-\nease detection is still a leading topic. The rapid increase of the published\npapers about deep learning in EEG is continued in 2021. The remainder of\nthe survey is organized as follows. In Section II, artifacts removal is brieﬂy\nintroduced. This is followed by the detailed descriptions of all deep learning\nmodels which have been applied to EEG in Section III. In this section, we\nalso mention the advantages and limitations of each deep learning model.\nSubsequently, the applications of deep learning in EEG are detailed along\nwith publicly available EEG datasets used in these applications in Section\nIV. Finally, discussions are given and future directions are drawn at the end\nof the survey. All abbreviations used in this survey are listed in Table 1.\n2\nArtifacts Removal\nIn general, artifacts are larger than that we intend to extract from EEG sig-\nnal in terms of scale, leading to a low signal-to-noise ratio (SNR). In order\nto improve SNR, EEG signal is preprocessed to remove or mitigate the eﬀect\nof artifacts on the signal before the signal is further processed. For exam-\nple, a notch ﬁlter [16] is eﬀective for eliminating the interference of power\n5\nFigure 2: (A) Generic framework of a deep learning model. (B) Classical\nunits that are employed in a deep learning model.\nline. Independent component analysis [27] is usually utilized to remove eye\nmovements-related and muscular activity-related artifacts. Classical meth-\nods of artifacts removal and their targeted artifacts are summarized in Table\n2.\nWhen deep learning emerges, the step of artifacts removal is kept. EEG\nsignal is preprocessed as usual to remove artifacts before inputting into a deep\nlearning model. This is an eﬀective way as all artifacts removal methods can\nbe applied with deep learning models to be of both beneﬁts inherited from\nthe artifacts removal methods and deep learning models. This is also a nat-\nural and straightforward way that researchers are able to easily implement.\nHowever, an independent step of artifacts removal is not always necessary.\nThe ﬁrst several layers in a deep learning model could be functioned as arti-\nfacts removal, where noise is removed through the layers. To this end, a few\nattempts were done. For example, Supratak et al. inputted raw EEG data\ninto a CNN for the classiﬁcation of sleep stages. Their study showed that\nan acceptable performance can be achieved without an independent step of\nartifacts removal [28]. In addition, Bahador et al. mapped the correlation of\nEEG channels into a 2D space and used a CNN model to learn representa-\ntions related to particular artifacts. With respect to artifact detection, this\n6\nmethod outperformed spectrogram-based CNNs [29]. Moreover, no auxiliary\nreference signal was required in their method.\n3\nDeep Learning Models\nIn this section, we describe each fundamental deep learning model. Their\nvariants and combinations are not included as they share the similar ra-\ntionale with fundamental models. A deep learning model is a hierarchical\nstructure, comprising layers through which data are mapped into more and\nmore abstract. Whatever a deep learning model is, there are an input layer,\nan output layer, and one or more hidden units (see Fig. 2(A)). The hidden\nunit might be one of the layer structures illustrated in Fig. 2(B) or their com-\nbinations. In the following subsections, we introduce classical deep learning\nmodels where typical units illustrated in Fig. 2(B) are embedded.\n3.1\nRestricted Boltzmann Machine and Deep Belief\nNetworks\nA restricted Boltzmann machine (RBM) [30] is an undirected graph model\n(see Fig. 2(B): RBM Unit), which has a visible layer v = (v1, v2, . . . , vn) and\na hidden layer h = (h1, h2, . . . , hn). Connections exist only between visible\nlayer v and hidden layer h and there are no connections between nodes within\nthe visible layer or hidden layer. The energy function for an RBM is deﬁned\nas:\nE(v, h) = −vTWh −aTv −bTh\n(1)\nwhere W is the weight matrix, a and b are bias vectors. The joint probability\nof v and h is constructed in terms of E:\nP(v, h) = 1\nZ e−E(v,h)\n(2)\nwhere Z is a normalizing constant deﬁned as:\nZ =\nX\nv,h\ne−E(v,h)\n(3)\n7\nThe marginal distribution over the visible variables is obtained as:\nP(v) = 1\nZ\nX\nh\ne−E(v,h)\n(4)\nThe conditional probabilities can be described as:\nP (hj = 1|v) = σ (Wjv + bj)\n(5)\nP (vi = 1|h) = σ (Wih + ai)\n(6)\nwhere σ is logistic function deﬁned as:\nσ(x) =\n\u00001 + e−x\u0001−1\n(7)\nA deep belief network (DBN) is constructed by stacking multiple RBMs\n[31]. Each RBM in the DBN is trained using an unsupervised manner at\nﬁrst. Then, the output of previous RBM is inputted into the next RBM. All\nRBMs are ﬁne-tuned together by supervised optimization.\n3.2\nConvolutional Neural Network\nConvolutional neural network (CNN) [32] is good at capturing spatial infor-\nmation of data (see Fig. 2(B): Convolutional Unit). Most CNNs consist of\ntwo types of layers: convolutional layer, pooling layer.\nIn speciﬁc, a convolutional layer has ﬁlters kl\nij, the size of which is usually\nmuch smaller than the dimension of input data and forms a locally connected\nstructure. Filter at layer l can produce feature maps Xl\nj by convolving with\nthe input Xl−1\ni\nplus biases bl\nj. These features are subjected to a non-linear\ntransformation f(·) and can be mathematically expressed as:\nXl\nj = f\n\n\nMl−1\nX\ni=1\nXl−1\ni\n∗kl\nij + bl\nj\n\n\n(8)\nWhere M l−1 represents the number of feature maps in layer l −1, and ∗\ndenotes convolution operation.\nA pooling layer is responsible for feature selection and information ﬁlter-\ning. Two kinds of pooling operations are widely used: max pooling and av-\nerage pooling. In max pooling, maximum value is mapped from a sub-region\n8\nby pooling operator. In average pooling, the average value of a sub-region\nis selected as the result. A fully-connected layer is usually added at the last\npart of a CNN in the case of classiﬁcation. It transforms a long 1D vector\nand outputs to the next layer (usually softmax).\nWeight sharing and sparse connections are two basic strategies in CNN\nmodels, which lead to dramatic reduction in the number of parameters. These\nstrategies are helpful to reduce training time and enhance training eﬀective-\nness. Moreover, they also mitigate the overﬁtting problem while retaining a\ngood capability of complex feature extraction.\n3.3\nRecurrent Neural Networks\nRecurrent neural network (RNN) [33] was developed to deal with sequen-\ntial data because of its unique recurrent structure (see Fig. 2(B): Recurrent\nUnit), which allows previous outputs to be used as inputs while having hid-\nden states. It is widely used in applications that need to extract sequential\ninformation, such as natural language processing, speech recognition, and\nEEG classiﬁcation.\n3.3.1\nGRU\nGated Recurrent Unit (GRU) [34] has two gates, reset rt and update zt. Let\nxt be the input at time step t to a GRU layer and ht be the output vector.\nThe output activation is a linear interpolation between the activation from\nthe previous time step and a candidate activation ˆht.\nht = zt ⊙ht−1 + (1 −zt) ⊙˜ht\n(9)\nwhere zt decides the interpolation weight, which is computed by:\nzt = f (Wzxt + Uzht−1 + bz)\n(10)\nwhere W and U are weight matrices for the update gate, b is a bias vector,\nand f(·) is a non-linear function (usually sigmoid function). The candidate\nactivation is also controlled by an additional reset gate and computed as\nfollows:\n˜ht = g (Whxt + Uh (rt ⊙ht−1) + bh)\n(11)\nwhere ⊙represents an element-wise multiplication and g(·) is often a non-\nlinear tanh function. The reset gate is computed in a similar manner as the\n9\nupdate gate:\nrt = f (Wrxt + Urht−1 + br)\n(12)\n3.3.2\nLSTM\nDiﬀerent from GRU, Long Short-Term Memory (LSTM) [35] has three gates,\ninput it, output ot, and forget gates ft. Each LSTM cell has an additional\nmemory component ct. The gates are calculated in a similar manner as the\nGRU but LSTM has additional memory components.\nit = f (Wixt + Uiht−1 + bi)\n(13)\not = f (Woxt + Uoht−1 + bo)\n(14)\nft = f (Wfxt + Ufht−1 + bf)\n(15)\nA memory component is updated by forgetting the existing content and\nadding a new memory component as:\nct = ft ⊙ct−1 + it ⊙ˆct\n(16)\nwhere ˆct can be computed by:\nˆct = g (Wcxt + Ucht−1 + bc)\n(17)\nThe updated equation for the memory component is controlled by the forget\nand input gates. Then, the output of the LSTM unit is computed from the\nmemory modulated by the output gate according to the following equation:\nht = ot ⊙g (ct)\n(18)\n3.4\nAutoencoder and Stacked Autoencoder\nAutoencoder (AE) is a symmetrical structure with two layers [36] (see Fig.\n2(B): Autoencoder Unit).\nAn encoder learns latent representation from the input data while a de-\ncoder restores the latent representation as close to the input data as possible.\nThe goal of an autoencoder is to minimize the reconstruction error between\nthe input and the output.\n10\nGiven the inputs x ∈R, the encoding process ﬁrst maps it into a latent\nrepresentation h ∈R through a weight matrix Wv, bias bv, and an activation\nfunction f(·):\nh = f (Wvx + bv)\n(19)\nThen the decoding process transforms the latent representation h into the\nreconstruction y through a weight matrix Wh, bias bh, and an activation\nfunction g(·):\ny = g (Whh + bh)\n(20)\nTo simplify the network architecture, the tied weights strategy Wv = Wh =\nW are usually employed. The parameters to be determined are {W, bv, bh}.\nThe training of an autoencoder is to minimize the loss:\narg\nmin\nW,bv,bh J (W, bv, bh)\n(21)\nGiven the training samples Dn, the loss function is deﬁned as:\nJ (W, bv, bh) =\n1\nNDn\nX\nx∈Dn\nL(x, y)\n(22)\nwhere L is the error of the reconstruction and NDn is the number of the\ntraining samples.\nStacked autoencoder (SAE) is a neural network, where autoencoders are\nconnected one another to form a cascade.\n3.5\nOthers\nIn addition to the aforementioned models, there are other models aiming to\nsolve particular shortcomings existing in the above models. For example,\ncapsule network (CapsNet) was proposed to overcome the shortcoming that\nCNN does not well capture the relationships between the parts of an image\n[37]. When it applied to fMRI [38] and EEG [15], it is expected to capture\ncomprehensive relationships among brain regions, channels, or frequencies,\nand so on. To shorten training time, extreme learning machine (ELM) was\nproposed, where the weights of hidden layers are randomly assigned and\nﬁxed during the training [39]. Weight randomization is also implemented in\necho state network (ESN) [40]. ESN is a recurrent neural network where the\nweights of hidden layers are randomly and sparsely assigned and ﬁxed while\nthe weights of output layer can be tuned. Spiking neural network (SNN)\n11\nis a biologically inspired model and has been used to explore brain activity\npatterns in [41]. Deep polynomial network (DPN) uses a quadratic function\nto process its inputs and is able to learn features between diﬀerent samples\nor dimensions. It was implemented in [42] to utilize features from multiple\nviews for motor imagery classiﬁcation, including common spatial pattern,\npower spectral density, and wavelet packet transform.\nIn addition, some\nvariants of deep learning models were proposed by using diﬀerent training\nstrategies, such as generative adversarial network.\n4\nApplications\nWe summarized applications, in which deep learning was utilized for EEG\nprocessing and classiﬁcation, in this section. For your convenience, we group\ndiverse applications into six topics, which are brain-computer interface (see\nTable 3 for the details of studies), disease detection (see Table 4), emotion\nrecognition (see Table 5), operator functional states (see Table 6), sleep stage\nclassiﬁcation (see Table 7), as well as the applications other than above topics\n(see Table 8). According to statistics, the majority of selected papers belong\nto the topics of brain-computer interface (account for 26%) and disease de-\ntection (account for 25%). The percentages of each topic and the percentages\nof each model used in each topic are illustrated in Fig. 3. In addition, we\ncollected the information of the publicly available datasets which had been\nused in the studies and listed them in Table 9.\n4.1\nBrain-Computer Interface\nA brain-computer interface (BCI) can be deﬁned as a system that decodes\nbrain activity and translate user’s intentions into messages or commands for\nthe purposes of communication or the control of external devices, and more.\nIn this topic, deep learning was mainly applied to establish motor imagery\n(MI)- and P300-based BCIs (see Fig. 4).\nTransfer learning is utilized to mitigate the cost of re-training or solve the\nproblem of data lack in the target domain. A deep learning model trained\non the data collected from a session or a subject can be transferred to clas-\nsify/recognise the data of another session or another subject with a ﬁne-\ntuning. In some cases, the ﬁne-tuning is omitted. In general, the ﬁne-tuning\npositively contributes to the performance.\nThe extent of ﬁne-tuning was\n12\nFigure 3: Percentages of application topics and deep learning models. The\nouter ring represents paper percentages for each topic. The models within\neach topic are distinguished from the darkest to lightest colors, which stand\nfor CNN, RNN, SAE, DBN, and other models in order.\ninvestigated in a recent study[43]. It shows that the best performance of mo-\ntor imagery classiﬁcation was achieved when all layers were tuned except the\n13\nFigure 4: (A) Paradigms of brain-computer interface. (B) Percentages of the\nselected papers for each paradigm by the year of 2020\nﬁrst hidden layer under the condition of a low learning rate. Another study\ncomparing cross-session transferring and cross-subject transferring demon-\nstrated that the cross-session transferring was feasible and the cross-subject\ntransferring was ineﬃcient [44]. With the combination of transfer learning\nand CNN, Hang et al. proposed a deep domain adaption network [45]. They\nused maximum mean discrepancy to minimize the distribution discrepancy\nbetween target and source subjects and used the center-based discriminative\nfeature learning method to make deep features closer to corresponding class\ncenters. The evaluation on BCI Competition datasets (i.e., Dataset IVa of\nCompetition III and Dataset IIa of Competition IV) demonstrated a good\nclassiﬁcation performance. In the study of cross-subject transferring [46],\nnetwork weights were transferred. Dose et al. used a pool of data to obtain\na universal model of CNN [47]. This model was then adapted based on a\nsmall amount of data from a subject before applying to this subject. Their\nresults showed that an average improvement of 6∼9% was achieved for motor\n14\nimagery classiﬁcation in terms of classiﬁcation accuracy.\nTransferring can also be conducted between domains.\nA CNN-based\nmodel (VGG-16) trained on image data (the data from ImageNet) was trans-\nferred to recognize EEG data by freezing the parameters in the ﬁrst several\nlayers and ﬁne-tuning the parameters in the last several layers using an EEG\ndataset [48]. The performance was better than that of support vector ma-\nchine. Similar to the domain of image recognition, the amount of EEG data\ncan also be increased by augmentation procedure. Li et al. produced new\nsamples by adding noise into EEG data [49]. They claimed that adding noise\ninto amplitudes of power spectra was superior to that adding noise into EEG\ntime series in terms of classiﬁcation accuracy. Zhang et al. used intrinsic\nmode functions derived from empirical mode decomposition to generate new\nEEG samples so that the total number of samples was increased [50].\nClassical models such as CNN and RNN were originally developed for\nimage or speech recognition, so they did not well match the characteristics\nof EEG signal. They should be adapted before applying to EEG recognition.\nLi et al. designed a CNN-based network consisted of three blocks to cap-\nture spatial and temporal dependencies [49]. Multi-channel raw EEG signals\nwere fed into temporal convolutional layer and spatial convolutional layer\nsuccessively in the ﬁrst block. In the second block, a standard convolutional\nlayer and a dilated convolutional layer were utilized to extract temporal in-\nformation at diﬀerent scales while reducing the number of parameters. The\nextracted features were ﬁnally used for motor imagery classiﬁcation in the\nthird block.\nIn another CNN-based network [51], a layer was fed by all\noutputs from previous layers and its output was inputted to all following\nlayers. By using such dense inter-layer connections, information loss could\nbe reduced. In [50], EEG signals were transformed into tensors and fed into\na CNN-like network where convolution were replaced with complex Morlet\nwavelets, resulting in parameter reduction. Wavelet kernel was also used to\nlearn time-frequency features [46]. Their results demonstrated that wavelet\nkernels can provide faster convergence rate and higher classiﬁcation accu-\nracy compared to plain CNN. Alazrai et al. used CNN to extract features\nfrom time-frequency images, which were transformed using a quadratic time-\nfrequency distribution [52]. The methods were compared to a support vector\nmachine, and it suggested that CNN can achieve good performance in MI\ntasks of the same hand.\nIn order to accelerate the training course and alleviate the overﬁtting\nproblem, Liu et al. adjusted the number and position of batch normaliza-\n15\ntion layers in a CNN-based network for P300 detection [8]. Kshirsagar et al.\nemployed leaky rectiﬁed linear unit activation function at each convolutional\nlayer [53]. To evaluate whether the number of convolutional layers needs\nto be adjusted for diﬀerent BCI tasks and ﬁnd out an optimal structure,\nLawhern et al. compared networks with diﬀerent numbers of convolutional\nlayers [54]. Their results showed that deep CNN (i.e., ﬁve convolutional lay-\ners) tended to perform better on the oscillatory BCI dataset than on the\nevent-related potential BCI dataset, while shallow CNN (i.e., two convolu-\ntional layers) achieved better performance on the event-related potential BCI\ndataset. Apart from CNN, Lu et al. used a DBN (i.e., three RBMs and an\noutput layer) to extract features of motor imagery [44]. Some studies aimed\nto compare performances of diﬀerent deep learning models. For example, Pei\net al. compared SAE and CNN in the classiﬁcation of reaching movements\n[55]. They found that SAE was better than CNN and suggested that poorer\nperformance in CNN might be due to the lack of training data. One year\nlater, another study comparing between these two models showed that SAE\nhad satisfactory performance in some trials, but ineﬃcient to those trials of\nthe subjects who were less attentive in P300 detection, while CNN performed\nwell in terms of accuracy and information transfer rate [53].\nThe combination of deep learning model and traditional model or the\nmixture of two or more types of deep learning models is applied to EEG\nclassiﬁcation. For example, SAE was combined with support vector machine\nto classify EEG signal [56]. SAE was also combined with CNN to develop\na new model [57], where CNN layers were used to extract features from 2D\ntime-frequency images (obtained by Fourier transform over EEG signals) and\nSAE was further used to extract features. In [58], the features extracted by\nCNN were fed into an autoencoder for cross-subject MI classiﬁcation. This\ncombination achieved a better accuracy for the cross-subject classiﬁcation,\nbut worse for the subject-speciﬁc classiﬁcation, compared to the combination\nof CNN and multilayer perceptron (MLP). Zhang et al. presented a hybrid\nnetwork comprised of CNN and LSTM, in which EEG signals were sequen-\ntially processed through common spatial pattern, CNN, and LSTM [59]. The\nidea of using CNN and LSTM to extract spatial and temporal features was\nalso conceived by Yang et al. [60]. However, they inserted a discrete wavelet\ntransformation (DWT) between CNN and LSTM, which led to better per-\nformance in the MI classiﬁcation compared to that of pure combination of\nCNN and LSTM.\nIn addition to P300- and MI-based BCIs, deep learning models also ap-\n16\nFigure 5: Percentages of the selected papers across diseases.\nplies to the other BCIs, including motion-onset visual evoked potentials [61]\nand self-paced reaching movements [55]. Nguyen et al. developed a steady\nstate visually evoked potential (SSVEP)-based BCI speller system, in which\nonly one channel was used [62]. They used fast Fourier transform to ex-\ntract features from this channel and then fed the features into a CNN model.\nAccording to their results, frequency resolution and time window length inﬂu-\nence classiﬁcation performance. The frequency resolution of 0.0625 Hz and\ntime window of 2s were optimal for the ﬁve-class classiﬁcation [62]. Way-\ntowich et al. proposed a compact CNN to deal with asynchronous problem\nin SSVEP classiﬁcation [63]. It outperformed canonical correlation analysis\n(CCA) and combined-CCA.\n4.2\nDisease Detection\nMachine learning could beneﬁt disease diagnosis by providing assistant in-\nformation and preliminary diagnostic results. In this topic, deep learning\nmodels were also widely employed to detect a variety of diseases (see the\ndistribution of the selected papers over diseases in Fig. 5). In this subsec-\ntion, commonly used models and model designing strategies were introduced\nat ﬁrst, including the examples of single or hybrid models, as well as the\ndetailed architecture (e.g., layer settings). Afterwards, we described other\ntechniques that have an inﬂuence on the performance of deep learning.\nCNN is a deep learning model, which has been widely adopted for the\n17\ndetection of brain diseases (e.g., seizure detection [64] and schizophrenia iden-\ntiﬁcation [65]). Cao et al. stacked multiple CNNs to classify epileptic signals.\nIn this study, the proposed model was compared to a few classiﬁcation algo-\nrithms (i.e., Support Vector Machine (SVM), k-Nearest Neighbours (kNN),\nELM) under diﬀerent conditions (i.e., 1. Two-class, seizure/non-seizure; 2.\nThree-class, interictal/preictal/ictal; 3. Five-class, interictal/three preictal\nstates/ictal) [66].\nTo enhance the performance of epilepsy classiﬁcation,\noriginal binary labels, namely interictal epileptiform discharge (IED) and\nnon-IED, were converted into multiple labels used for model training [67].\nSpeciﬁcally, samples were further divided into ﬁve subclasses according to\nspatial distribution and morphology of EEG waveforms and were then fed\ninto a CNN model for the training. A new sample was ﬁrst classiﬁed to one\nof these subclasses and then the ﬁnal classiﬁcation result (IED versus non-\nIED) was obtained by applying a threshold at the last layer. Compared to\nthe CNN model training with binary labels, the training with further ﬁner\ntags could enhance the discriminative power of the model and led to better\nperformance in the most subjects.\nWhen CNN is combined with other models, classiﬁcation performance\ncan be improved. In [68], CNN and autoencoder (AE) were combined to\nlearn robust features in an unsupervised way. The integrated network had\nan encoder consisting of convolution and down-sampling and a decoder con-\nsisting of deconvolution and up-sampling. Their results demonstrated that\nCNN+AE is superior to principal component analysis (PCA) and sparse ran-\ndom projection (SRP) in epilepsy related feature extraction. In [69], a hybrid\nmodel combining CNN, AE, and LSTM achieved remarkable prediction of\nseizure. Combined deep learning model was used for pre-training and la-\ntent representation learning. By this, the accuracy of focal and non-focal\nclassiﬁcation was improved [70]. However, model combination is not always\npositive to the performance improvement. Some studies showed that perfor-\nmance may decline in some cases. For instance, Mumtaz et al. combined\nCNN and LSTM to detect unipolar depression. Their results showed that\nthe hybrid model did not outperform single model of CNN [71].\nBeyond the selection of deep learning models, model settings also vary\nacross studies. Tsiouris et al. found that overﬁtting problem can be miti-\ngated by shuﬄing input EEG segments, which could replace the dropout role\npartially [20]. Qiu et al. applied data corruption in the stacked autoencoder\nfor seizure detection [72]. Speciﬁcally, they designed a denoising sparse au-\ntoencoder, in which some of the input data were set to zero. This improved\n18\nmodel robustness and reduced overﬁtting problem. In addition, performance\nis also inﬂuenced by the condition of data recording. Mumtaz et al. found\nthat unipolar depression can be more accurately detected using the EEG\nrecorded under the condition of eyes open compared to that of eyes closed\n[71]. In the study of attention deﬁcit hyperactivity disorder (ADHD) detec-\ntion using a CNN model, EEG signals at diﬀerent channels were rearranged\nto make adjacent channels together in the connectivity matrix to improve\naccuracy [73]. Moreover, Tsiouris et al. shuﬄed interictal and preictal seg-\nments of EEG to avoid the overﬁtting in seizure detection [20]. Yuan et al.\nused a channel-aware module to enhance the capability of feature learning\nand concentrate on important and relevant EEG channels [74]. Daoud et al.\ncomputed the statistical variance and entropy of the channels, and selected\nthose with the highest variance entropy product for seizure prediction [69].\nThe performance of deep learning for disease detection is aﬀected by\nEEG data arrangement. For example, EEG data are reshaped into 2D for-\nmat before inputting into a deep learning model. In [75], EEG data were\ntransformed into 2D images of spectral powers. Then, these images were fed\ninto a CNN network for distinguishing Alzheimer’s disease and mild cognitive\nimpairment from healthy controls. To diﬀerentiate patients with schizophre-\nnia [76], Pearson correlation coeﬃcients were calculated between channels\nand assembled as a correlation matrix. Correlation matrices of each subject\nwere fed into a CNN network. Moreover, fast Fourier transform [77] and\ncontinuous wavelet transform [78] were used to transform EEG data into 2D\nimages for motor impairment neural disorders and epilepsy classiﬁcation, re-\nspectively. Wei et al. further converted 2D images into 3D stacked images\naccording to the mutual correlation intensity between channels [79]. To uti-\nlize comprehensive information from diﬀerent data forms, Tian et al. used\nthree CNNs to respectively obtain features existing in the time, frequency,\nand time-frequency domain, and then ultilized these features for seizure de-\ntection [80]. By comparing with the methods that ultilizing features from\nonly one domain, the proposed method exhibited better performance. Ac-\ncording to the study comparing among raw EEG signal, Fourier transform,\nwavelet transform, and empirical mode decomposition, raw signals and em-\npirical mode decomposition were better than the others in distinguishing\nfocal EEG from non-focal EEG, while Fourier transform was best in ictal\nand non-ictal classiﬁcation [81]. To handle the problem of inadequate data,\nsliding time window was used to split continuous EEG signal into segments\nwith partial overlapping to increase the data amount in [82]. Cao et al. de-\n19\nveloped an interactive system to help experts label the new data, and the\ndata can be added to ﬁne-tune the deep learning model to gradually improve\nthe interictal-ictal continuum classiﬁcation accuracy [17].\nFigure 6: Four illustrative emotions classiﬁed based on the scores of arousal\nand valence.\n4.3\nEmotion Recognition\nEmotion conveys lots of underlying information during conversations and is\npart of communication between people. People can understand emotion by\nreading facial expression, voice tone, and gestures.\nFrom the perspective\nof artiﬁcial intelligence, emotion can be recognized based on the data of\nfacial expression [83], eye movement measures [84], EEG [85], or galvanic\nskin response signal [86]. According to the arousal and valence, emotion can\nbe categorized into diﬀerent classes (see Fig. 6). Based on the statistics\n20\nof the included papers in this survey, the studies mainly aimed to classify\nthree classes (i.e., positive, neutral, and negative) or more classes (partitioned\nbased on the scores of arousal and valence). Within these papers, the datasets\nnamed ’SEED’ [87] and ’DEAP’ [88] were frequently used to evaluate deep\nlearning models for emotion recognition.\nSEED dataset was published by the BCMI laboratory at the Shanghai\nJiao Tong University [87]. For this dataset, 62 channels were used to collect\nEEG data from 15 subjects when they were watching positive, negative, and\nneutral video clips. The data were collected from the subjects three times\nwith an interval of one week or longer. Thus, it enables cross-session in-\nvestigations. Zheng et al. demonstrated the stable patterns of EEG signals\nover time for emotion recognition [89]. Besides, they found that diﬀerential\nentropy could provide better performance than other features such as dif-\nferential asymmetry and rational asymmetry. Using this dataset, Yang et\nal. proposed a hierarchical network which consists of subnetwork node, and\nthis method boosted 5%-10% accuracy [90]. Li et al. trained a CNN and\naccomplished around 88% of recognition accuracy based on features of the\ngamma band [91]. Zhang et al. proposed a two-layer RNN model to extract\nspatial and temporal features, respectively. The ﬁrst layer of their model is\nan RNN layer that takes EEG signals from electrodes as inputs. The outputs\nof the ﬁrst layer were concatenated along the time dimension and fed into\nthe second RNN layer. The performance evaluated on the SEED dataset was\n89.5% [83]. In [92], Zeng et al. used an architecture that adapted from Sinc-\nNet (a CNN-based network proposed for speaker recognition [93]) to classify\nemotion. Their results demonstrated that the adapted SincNet (i.e., three\nconvolutional layers and three fully connected layers) was promising for emo-\ntion classiﬁcation, reaching an accuracy of around 95% as evaluated on the\nSEED dataset.\nAnother dataset named ’DEAP’ [88], was collected from 32 subjects when\nthey watched 40 one-minute-long music videos. Perceptual emotion was as-\nsessed in terms of arousal, valence, liking, and dominance. Studies using this\ndataset have showed that deep learning was successful and eﬀective to clas-\nsify emotion categories based on EEG. [85], [94]. Even using raw EEG as the\ninput, LSTM achieved an acceptable accuracy of around 85% in the emotion\nclassiﬁcation [95]. In [96], various handcrafted EEG features (e.g. sample\nentropy, mean, and power spectral density) were fed into three stacked au-\ntoencoders in a parallel way for voting. Chao et al. also designed a parallel\narchitecture to process EEG signal. However, they used DBN as the basic\n21\nunit [97]. To improve the classiﬁcation performance and utilize strengths of\ndiﬀerent models. Li et al. combined CNN and LSTM to extract representa-\ntions from multi-channel EEG, in which CNN was used to learn inter-channel\nand inter-frequency correlation while LSTM was used to extract contextual\ninformation [98]. The model combination was also used in [99], where feature\nextraction was done by graph convolutional networks, temporal information\nwas memorized by LSTM, and classiﬁcation was done by a SVM. The same\nidea of model combination was also used in [100], where CNN was used for\nfeature extraction.\nBesides the two commonly used datasets (i.e., SEED and DEAP), Serap\nAydın used aﬀective video clips to induce nine emotional states (fear, anger,\nhappiness, sadness, amusement, surprise, excitement, calmness, and disgust)\nand investigated gender eﬀect on emotion recognition [101]. This paper re-\nvealed that emotion is more aﬀected by individual experience than gender.\nZhu et al. designed an experiment to explored the emotion in the scenario\nof two-person interaction. In their experiment, two person need to rate their\nemotions induced by the same piciture one by one. They extracted the intra-\nbrain and inter-brain phase synchronization features from emotional EEG\nsignals and applied a CNN model to evaluate [102]. As we know, deep learn-\ning needs parameter tuning and it is time-consuming. To mitigate this prob-\nlem, various strategies were proposed. Hemantha et al. modiﬁed the back-\npropagation neural network by arranging layers in a circular manner that the\noutput can access the parameters of the input and hidden layers [103]. This\nmodiﬁcation reduced convergence time by around 20%. Jirayucharoensak\net al. used principal component analysis for dimension reduction to lower\ncomputation cost [104]. Gao et al. utilized gradient priority particle swarm\noptimization to optimize parameters of a CNN model [105].\n4.4\nOperator Functional States\nThe operator functional states (OFS) describe the mental states of opera-\ntors in speciﬁc working conditions [106]. Two of them are mental workload\nand mental fatigue. In speciﬁc, mental workload is a measure of cognitive\nresources consumed in the human working memory while mental fatigue is\nidentiﬁed by an accumulated process of a disinclination of eﬀort and drowsi-\nness. To date, deep learning was used to identify mental states based on\nEEG signal. For example, drivers’ [107] [108] [109] [110] and pilots’ [111]\nfatigue was monitored for the purposes of preventing fatigued operation.\n22\nGeneralization is one of the important metrics to evaluate a model. In the\nclassiﬁcation of operator functional states, large variance across subjects is\nchallenging. Many studies employed subject-speciﬁc classiﬁers. For example,\nTao et al. fused multiple ELMs and Naive Bayesian model to build a subject-\nspeciﬁc classiﬁer. This ensemble model with ﬁne-tuned hyper-parameters was\nof the higher subject-speciﬁc accuracy in mental workload assessment [112].\nIn the study of [113], Zhang et al. selected the most relevant EEG chan-\nnels for each subject and used these subject-speciﬁc channels for calculating\nweights between the input layer and the ﬁrst hidden layer in the DBN. In\ncontrast to the subject-speciﬁc models, the cross-subject model aims to have\na general model for tolerating variance of subjects. For example, Heron et\nal. used multi-path convolutional layers and bi-directional LSTM layers to\nlearn frequency and temporal features over subjects. This model achieved\nlow variance in performance across subjects and showed better generaliza-\ntion compared to subject-speciﬁc models [114]. Another cross-subject model\nwas proposed using an adaptive DBN with the weights of the ﬁrst hidden\nlayer iteratively updated to track the EEG changes in a new subject [115].\nWhen diﬀerent tasks were used to induce mental workload, the induced work-\nload might be variable across tasks. The cross-task workload classiﬁcation\nwas made by using a CNN+RNN model [116]. Another study used trans-\nfer learning strategy to improve model generalization for the classiﬁcation of\nmental workload [117].\nMultiple kinds of features can be fused to improve assessment perfor-\nmance of mental workload. Gao et al. presented a temporal convolutional\nblock to extract sequential information of EEG. The block orderly consists\nof a 1D convolution, a rectiﬁed linear activation, and a batch normalization.\nTemporal convolutional blocks and dense layers for spatial feature fusion were\ncombined to form a novel network. Their results showed that this architec-\nture can achieve higher accuracy for fatigue classiﬁcation, when compared\nto these networks that replace convolutional block by 1D convolution [109].\nZhang et al. proposed a two-stream CNN network to learn spectral and tem-\nporal features [118]. One stream of CNN was fed by power spectral density\ntopographic maps and the other was fed by topographic maps of amplitude\ndistributions. At the same year (2019), they designed another network for\nthe same propose of learning spectral and temporal features for mental work-\nload classiﬁcation. In this network, CNN with 3D kernels were ﬁrst applied to\nEEG cubes, then extracted features from CNN were ﬂatten to 1D vectors and\nfed to a bidirectional LSTM for further processing and classiﬁcation [116].\n23\nBoth models (i.e. two-stream CNN and CNN+LSTM) showed a signiﬁcant\nimprovement in mental workload classiﬁcation.\n4.5\nSleep Stage Classiﬁcation\nSleep stage classiﬁcation helps us understand the course of sleep to assess\nsleep quality and diagnose sleep-related disorders. Table 10 brieﬂy summa-\nrized the characteristics of each sleep stage. With the aid of EEG recording,\nsleep quality can be assessed objectively. In the processing of sleep quality,\nsleep staging is a precedent step. To date, deep learning has been applied to\nsleep staging. For instance, LSTM model was used for sleep stage classiﬁca-\ntion based on a single channel EEG [119]. CNN+LSTM model was proposed\nto classify sleep stages [120] [28] and detect sleep spindles [121].\nSleep consists of a sequence of stages. Therefore, temporal information\nshould be useful for sleep stage classiﬁcation.\nMorlet wavelets [122] and\ntime-frequency representations [119] [123] were applied to retain temporal\ninformation in the extraction of spectral features. These extracted features\nwere then learned by deep learning models for sleep stage classiﬁcation, show-\ning promising performance. Using the time-frequency representation of EEG,\nCNN model achieved good performance [124]. In another study, the CNN\nwas combined with LSTM to capture both temporal and spatial information\nfor sleep stage classiﬁcation [125]. The CNN was also combined with atten-\ntion mechanism for sleep stage classiﬁcation [126]. In contrast to the super-\nvised learning, unsupervised learning can perform with unlabeled data, which\nis preferable when the data labelling is expensive or very time-consuming.\nZhang et al. presented a CNN model with a greedy layer-wise training strat-\negy, in which complex-valued k-means was utilized to train ﬁlters used in the\nconvolution with unlabeled EEG data [127]. In [128], unsupervised sparse\nDBN was used to extract features. Subsequent classiﬁers (e.g., kNN or SVM)\nperformed well on sleep stage classiﬁcation by using these unsupervised-\nextracted features. Jaoude et al. demonstrated that a large training data can\nhelp validate classiﬁcation performance. They trained a deep learning model\n(CNN+RNN) on sleep data from more than six thousand participants and\ntested on several publicly available datasets. The model achieved as good\nas humam experts in sleep staging accuracy [129]. Usually, the numbers of\nsamples for each sleep stage are unbalanced. To date, several methods have\nbeen proposed to release this issue, including the class-balanced random sam-\npling [122], data augmentation [130], class-balance training set design [28],\n24\nand synthetic minority oversampling technique [131].\n4.6\nOthers\nThose studies that cannot be grouped into the above topics are presented in\nthis subsection. A summary table with key information of those studies is\nprepared (see Table 8). On the one hand, EEG with deep learning can be\nused for person identiﬁcation [132], [133], age and gender prediction [134]. On\nthe other hand, it can also be used to decode brain activity related to vision,\naudio [135], and pain [136]. In a study of image classiﬁcation [137], LSTM\nwas used to extract EEG features while CNN was used to extract image\nfeatures. This study claimed that features extracted from EEG could help\nimage classiﬁcation so that classiﬁcation performance was improved. In [138],\na CNN+LSTM hybrid network was used to extracted visual representations\nfrom EEG, and a generative adversarial network was applied to reconstruct\nimages from the learnt EEG representations. Deep learning and EEG were\nalso applied to understand brain functions and structure.\nThese studies\naimed to understand functional brain connectivity [139], speech laterality\n[140], as well as memory under speciﬁc conditions. For example, Baltatzis\net al. investigated the brain’s activity of diﬀerent people (ever experienced\nschool bullying or not) to diﬀerent stimuli (2D videos or Virtual Reality)\n[141]. Doborjeh et al. used EEG and spiking neural network to decode how\nthe brain react to various commercial brands (locally familiar or not) [142].\nArora et al. studied the memory loss after seizure surgery [143].\n5\nDiscussion\nIn this survey, we reviewed the researches of deep learning in EEG for the\nlast ten years, which is a critical period for the development of deep learning\nused in EEG. An introduction about deep learning in EEG was ﬁrst pre-\nsented in the ﬁrst section. Subsequently, we presented classical methods of\nartifacts removal which is an important step in EEG processing. We detailed\nprevalent deep learning models, followed by the comprehensive reviews on\ndiﬀerent applications that used deep learning to process and classify EEG\nsignals. These applications were categorised into several topics for presen-\ntation. The increase in the number of published papers suggested that the\nresearch of deep learning in EEG are expanding over time. Although remark-\n25\nable achievements were obtained, challenges and limitations still exist, which\nneed to be addressed. We discuss them below and provide our perspectives.\nThe performance of deep learning-based classiﬁcation should be further\nimproved. Although the published papers showed the advantages of deep\nlearning in EEG classiﬁcation and demonstrated that deep learning is supe-\nrior to conventional methods, the performance is much lower compared to the\nperformance achieved by deep learning in image or speech classiﬁcation [25],\n[26]. The reasons for the lower performance are mainly due to two aspects:\nEEG signal itself and deep learning models. On the one hand, EEG signal is\nnon-stationary and much variable over time, which makes the extraction of\nrobust features diﬃcult. An eﬀective solution for this problem is to partition\ncontinuous EEG signal into short segments, which can be seen as a station-\nary signal. However, this is only an approximation but not a ﬁnal solution.\nWhen performing cross-subject classiﬁcation or cross-session classiﬁcation,\nEEG over subjects or sessions is largely variable, making the above prob-\nlem more dominant. On the other hand, most deep models are originally\nproposed to process other signals (e.g., images) rather than EEG. Although\ncertain adaptions of the models have been done, the performance is still\nnot ideal because of mismatch between the models and EEG characteristics.\nTaking CNN as an example, it is more suitable for image processing. Raw\nimages can be directly fed into the CNN. However, this is not the case when\napplying to EEG signals. Although we have seen some studies, in which raw\nEEG was fed into CNN directly without pre-processing, it is not mainstream.\nThe mainstream is still to pre-process EEG before feeding into a deep learn-\ning model because the pre-processing is very eﬀective for removing noises to\nimprove signal-to-noise ratio. Another advantage of the pre-processing step\nis that EEG data can be transformed into other representations and/or re-\norganised to facilitate the following processing in the deep learning model.\nFor instance, spectral power density is one of the most widely used feature\nfor EEG signal. Without a separate pre-processing step, this kind of feature\ncannot be obtained because temporal EEG signal cannot be transformed into\nspectral domain within the deep learning model.\nAvailable data size in EEG studies is signiﬁcantly smaller than that avail-\nable in image or speech studies [25], [26]. As we know, the deep learning\nmodel requires extensive training and a large data size can beneﬁt model\ntraining to a great extent.\nCompared to the millions of training data in\nimage or speech recognition, the scale of training data is much less in EEG\nclassiﬁcation, only from tens, hundreds, or at most thousands of participants.\n26\nOne potential solution for the lack of EEG data in the model training is the\nuse of transfer learning. Deep learning model can be trained by the data\nwhich are not collected at the moment and the trained model can be used\nfor recognition or classiﬁcation on the new collected EEG data after ﬁne-\ntuning or even without ﬁne-tuning [44], [45], [46]. Unlike image classiﬁca-\ntion, for which there are mature existing pre-trained models (e.g., ImageNet\npre-trained VGG model), there is no publicly available pre-trained model for\nEEG classiﬁcation. If VGG model is directly applied to EEG, reorganization\nof EEG has to be done in order to meet the input data format of VGG model.\nThis reorganization might lead to information loss and give detrimental ef-\nfect on the EEG classiﬁcation. In addition, there is no idea how well a model\ntrained on images can be tuned to classify EEG signal.\nBased on the eﬀectiveness comparison of transfer learning, greater perfor-\nmance improvement was observed in image classiﬁcation compared to EEG\nclassiﬁcation. This might be due to the lack of eﬀective training framework\nand strategies that are suitable for transferring EEG patterns. There was an\nattempt to transfer the model trained on images to EEG classiﬁcation [48].\nThis transferring is across distinct modalities. It is likely to have a better\nperformance when transferring across relevant modalities. As we know, there\nare diﬀerent modalities (e.g., functional near-infrared spectroscopy (fNIRS)\nand EEG) that can be used to measure underlying brain activity. A deep\nlearning model can be trained on one modality and then ﬁne-tuned by the\nother modality to classify signals of that modality. Or, diﬀerent modalities\ncan be used together to train a deep learning model so that the training can\nbe beneﬁted from the complementary information existing in the diﬀerent\nmodalities. It is a fusion of modalities. It has been seen that classiﬁcation\nperformance was elevated by feature fusion in the case of using conventional\nclassiﬁers [144]. The fusion could be done at the diﬀerent stages of the classi-\nﬁcation process (e.g., at the beginning of initial feature fusion or at the later\nstage of decision fusion [145], [146]). Wu et al. utilized both EEG and Elec-\ntrooculogram (EOG) to classify the level of vigilance by fusing the features\nextracted from EEG and EOG [147]. In the future, more extensive research\nshould be carried out to elevate the development of fusion in deep learning\nmodels. Especially, to address how to eﬀectively fuse multiple modalities in\ndeep learning models for neurophysiological signal classiﬁcation and analysis.\nOf course, collecting adequate data is a straightforward solution for the lack\nof EEG data. However, this results in new issues, such as cost increase and\ntime delay. If data collection involves diﬀerent institutes, extra communi-\n27\ncation eﬀort should be paid to coordinate the data collection. Meanwhile,\ncomputation demand will be increased with the increase of data size, which\nrequires to upgrade computational hardware or replace with the new gener-\nation hardware (e.g., central processing unit (CPU) and graphics processing\nunit (GPU)). As mentioned in [148], cloud computing service is an eﬀective\nway to share hardware resources so that the hardware cost in individual in-\nstitutes will be reduced. Using the cloud computing service, data protection\nand privacy have to be considered, especially for clinical data.\nWhen applying a deep learning model to EEG, we need to adapt the deep\nlearning model in compliance with the characteristics of EEG. For example,\nhow to arrange the input data or how to set kernel size should be consid-\nered. EEG signal is usually not directly used and commonly transformed\nbefore feeding into a deep learning model. There are strong relationships\namong temporal domain, spectral domain, and spacial domain. It is impor-\ntant these relationships should be kept as much as possible when arranging\nthe input data. When EEG channels are stacked along a dimension, their\nspacial layout is distorted. In this case, kernels, such as square kernel, that\nusually-used in image recognition are no longer eﬀective for EEG classiﬁca-\ntion. A column kernel (covering all channels) is a better choice, which has\nbeen supported by the study in [149]. Further, Wang et al. extended the\ncolumn kernel by considering brain anatomic structure to develop multiple\nkernels with the sizes matching brain region sizes, achieving a better perfor-\nmance in schizophrenia identiﬁcation compared to the usually-used kernels,\nsuch as square kernel [38].\nWe believe deep learning models should be changed to be more ﬂexible.\nThe trained model can be adapted dynamically in real-time as needed. This\nis not limited to dynamic parameter tuning. Ideally, model architecture can\nalso be adjusted when needed.\nAlso, we hope the newly-developed deep\nlearning model could perform multiple tasks at the same time in the future.\nPlease see the detailed description in [150].\nApart from the purposes of deep learning-based EEG classiﬁcation, deep\nlearning may also be a useful tool to reveal neural mechanisms of the brain.\nWhen a deep learning model achieves a satisfactory classiﬁcation perfor-\nmance, it captures essential diﬀerences existing between the classes. There-\nfore, we can look at what information the deep learning model focuses on to\nroughly infer the underlying associated brain activity. For example, Goh et\nal. presented spatial distribution of brain activations associated with lower\nlimb movements by probing into the model of spatio-spectral representation\n28\nlearning [149]. We expect that advanced deep learning models developed in\nthe future could reversely decompose EEG signal back into the representa-\ntion in the brain to reveal underlying brain mechanisms. It is unrealistic at\nthe current stage, but paying eﬀorts to make progress towards to this target.\nA prominent advance we need to mention is the EEGNet [54], which is\nproven eﬀective for diﬀerent BCI paradigms. Another promising model is\nSincNet, which was initially proposed for speaker recognition and also well\nfor the classiﬁcation of EEG signal [92]. New deep learning architectures,\nsuch as capsule network [38], are also required to enhance the chance of\nsuccess of EEG applications.\nLastly, a mix of diﬀerent deep learning units has been increasingly seen,\nwhich integrates the characteristics of these units to beneﬁt data learning.\nBecause there is not deﬁnite guidance to set optimal deep learning architec-\nture (e.g., model depth and model width) currently, model complexity might\nbe considered to determine the model architecture. The model should have\nenough capacity for learning information in accordance with classiﬁcation\ntasks while its complexity should be kept as low as possible to minimize\ncomputational cost.\n6\nConclusion\nOur survey is a glimpse of what have been done for the deep learning in EEG\nover the past ten years. There are still many researches currently on-going\nat laboratories and hospitals, dealing with challenges we mentioned above\nand beyond. We hope that our survey can provide the researchers who are\nworking in this ﬁeld with a summary and facilitate their researches.\nReferences\n[1] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspec-\ntives, and prospects,” Science, vol. 349, no. 6245, pp. 255–260, 2015.\n[2] D. Michie, D. J. Spiegelhalter, C. Taylor et al., “Machine learning,”\nNeural and Statistical Classiﬁcation, vol. 13, no. 1994, pp. 1–298, 1994.\n[3] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol.\n521, no. 7553, pp. 436–444, 2015.\n29\n[4] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet\nlarge scale visual recognition challenge,” International journal of com-\nputer vision, vol. 115, no. 3, pp. 211–252, 2015.\n[5] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,\nC. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen et al., “Deep\nspeech 2: End-to-end speech recognition in english and mandarin,”\nInternational conference on machine learning, pp. 173–182, 2016.\n[6] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with\ndeep learning: A review,” IEEE transactions on neural networks and\nlearning systems, vol. 30, no. 11, pp. 3212–3232, 2019.\n[7] W. P. He, G. Wang, J. Hu, C. Li, B. L. Guo, and F. P. Li, “Simul-\ntaneous Human Health Monitoring and Time-Frequency Sparse Rep-\nresentation Using EEG and ECG Signals,” IEEE Access, vol. 7, pp.\n85 985–85 994, 2019.\n[8] M. F. Liu, W. Wu, Z. H. Gu, Z. L. Yu, F. F. Qi, and Y. Q. Li, “Deep\nlearning based on Batch Normalization for P300 signal detection,” Neu-\nrocomputing, vol. 275, pp. 288–297, 2018.\n[9] L. F. Haas, “Hans berger (1873–1941), richard caton (1842–1926), and\nelectroencephalography,” Journal of Neurology, Neurosurgery & Psy-\nchiatry, vol. 74, no. 1, pp. 9–9, 2003.\n[10] Y. Wang, R. Wang, X. Gao, B. Hong, and S. Gao, “A practical vep-\nbased brain-computer interface,” IEEE Transactions on neural systems\nand rehabilitation engineering, vol. 14, no. 2, pp. 234–240, 2006.\n[11] J. Li, Y. Wang, L. Zhang, and T.-P. Jung, “Combining erps and eeg\nspectral features for decoding intended movement direction,” 2012 An-\nnual International Conference of the IEEE Engineering in Medicine\nand Biology Society, pp. 1769–1772, 2012.\n[12] Y. Zhang, G. Zhou, J. Jin, Q. Zhao, X. Wang, and A. Cichocki,\n“Sparse bayesian classiﬁcation of eeg for brain–computer interface,”\nIEEE transactions on neural networks and learning systems, vol. 27,\nno. 11, pp. 2256–2267, 2015.\n30\n[13] J. Li, C. Li, and A. Cichocki, “Canonical polyadic decomposition with\nauxiliary information for brain–computer interface,” IEEE journal of\nbiomedical and health informatics, vol. 21, no. 1, pp. 263–271, 2015.\n[14] W.-L. Zheng, J.-Y. Zhu, Y. Peng, and B.-L. Lu, “Eeg-based emotion\nclassiﬁcation using deep belief networks,” 2014 IEEE International\nConference on Multimedia and Expo (ICME), pp. 1–6, 2014.\n[15] H. Chao, L. Dong, Y. L. Liu, and B. Y. Lu, “Emotion Recognition\nfrom Multiband EEG Signals Using CapsNet,” Sensors, vol. 19, no. 9,\n2019.\n[16] F. Li, G. F. Zhang, W. Wang, R. Xu, T. Schnell, J. Wen, F. McKenzie,\nand J. Li, “Deep models for engagement assessment with scarce label\ninformation,” IEEE Transactions on Human-Machine Systems, vol. 47,\nno. 4, pp. 598–605, 2017.\n[17] L. Cao, W. B. Tao, S. T. E. An, J. Jin, Y. Z. Yan, X. Y. Liu, W. D.\nGe, A. Sah, L. Battle, J. M. Sun, R. Chang, B. Westover, S. Madden,\nand M. Stonebrakerl, “Smile: A System to Support Machine Learning\non EEG Data at Scale,” Proceedings of the VLDB Endowment, vol. 12,\nno. 12, pp. 2230–2241, 2019.\n[18] H. Cecotti and A. Graeser, “Convolutional neural network with embed-\nded fourier transform for eeg classiﬁcation,” 2008 19th International\nConference on Pattern Recognition, pp. 1–4, 2008.\n[19] J. Li and A. Cichocki, “Deep learning of multifractal attributes from\nmotor imagery induced eeg,” International Conference on Neural In-\nformation Processing, pp. 503–510, 2014.\n[20] K. M. Tsiouris, V. C. Pezoulas, M. Zervakis, S. Konitsiotis, D. D.\nKoutsouris, and D. I. Fotiadis, “A Long Short-Term Memory deep\nlearning network for the prediction of epileptic seizures using EEG\nsignals,” Computers in Biology and Medicine, vol. 99, pp. 24–37, 2018.\n[21] A. Craik, Y. He, and J. L. Contreras-Vidal, “Deep learning for elec-\ntroencephalogram (eeg) classiﬁcation tasks: a review,” Journal of neu-\nral engineering, vol. 16, no. 3, p. 031001, 2019.\n31\n[22] A. Al-Saegh, S. A. Dawwd, and J. M. Abdul-Jabbar, “Deep learning for\nmotor imagery eeg-based classiﬁcation: A review,” Biomedical Signal\nProcessing and Control, vol. 63, p. 102172, 2021.\n[23] M. J. Rivera, M. A. Teruel, A. Mat´e, and J. Trujillo, “Diagnosis and\nprognosis of mental disorders by means of eeg and deep learning: a\nsystematic mapping study,” Artiﬁcial Intelligence Review, pp. 1–43,\n2021.\n[24] X. Gu, Z. Cao, A. Jolfaei, P. Xu, D. Wu, T.-P. Jung, and C.-T. Lin,\n“Eeg-based brain-computer interfaces (bcis): A survey of recent stud-\nies on signal sensing technologies and computational intelligence ap-\nproaches and their applications,” IEEE/ACM transactions on compu-\ntational biology and bioinformatics, 2021.\n[25] W. Rawat and Z. Wang, “Deep convolutional neural networks for image\nclassiﬁcation: A comprehensive review,” Neural computation, vol. 29,\nno. 9, pp. 2352–2449, 2017.\n[26] A. B. Nassif, I. Shahin, I. Attili, M. Azzeh, and K. Shaalan, “Speech\nrecognition using deep neural networks: A systematic review,” IEEE\nAccess, vol. 7, pp. 19 143–19 165, 2019.\n[27] M. P. Hosseini, D. Pompili, K. Elisevich, and H. Soltanian-Zadeh, “Op-\ntimized Deep Learning for EEG Big Data and Seizure Prediction BCI\nvia Internet of Things,” IEEE Transactions on Big Data, vol. 3, no. 4,\npp. 392–404, 2017.\n[28] A. Supratak, H. Dong, C. Wu, and Y. K. Guo, “DeepSleepNet: A\nModel for Automatic Sleep Stage Scoring Based on Raw Single-Channel\nEEG,” IEEE Transactions on Neural Systems and Rehabilitation En-\ngineering, vol. 25, no. 11, pp. 1998–2008, 2017.\n[29] N. Bahador, K. Erikson, J. Laurila, J. Koskenkari, T. Ala-Kokko, and\nJ. Kortelainen, “A Correlation-Driven Mapping For Deep Learning\napplication in detecting artifacts within the EEG,” Journal of Neural\nEngineering, vol. 17, no. 5, p. 056018, Oct. 2020. [Online]. Available:\nhttps://iopscience.iop.org/article/10.1088/1741-2552/abb5bd\n32\n[30] R. Salakhutdinov, A. Mnih, and G. Hinton, “Restricted boltzmann ma-\nchines for collaborative ﬁltering,” Proceedings of the 24th international\nconference on Machine learning, pp. 791–798, 2007.\n[31] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm\nfor deep belief nets,” Neural computation, vol. 18, no. 7, pp. 1527–1554,\n2006.\n[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” Advances in neural informa-\ntion processing systems, pp. 1097–1105, 2012.\n[33] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of\nrecurrent neural networks for sequence learning,” arXiv preprint\narXiv:1506.00019, 2015.\n[34] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evalua-\ntion of gated recurrent neural networks on sequence modeling,” arXiv\npreprint arXiv:1412.3555, 2014.\n[35] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal\nrepresentations by error propagation,” California Univ San Diego La\nJolla Inst for Cognitive Science, Tech. Rep., 1985.\n[37] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between cap-\nsules,” Advances in neural information processing systems, pp. 3856–\n3866, 2017.\n[38] T. Wang, A. Bezerianos, A. Cichocki, and J. Li, “Multikernel capsule\nnetwork for schizophrenia identiﬁcation,” IEEE Transactions on Cy-\nbernetics, 2020.\n[39] S. F. Ding, N. Zhang, X. Z. Xu, L. L. Guo, and J. Zhang, “Deep\nExtreme Learning Machine and Its Application in EEG Classiﬁcation,”\nMathematical Problems in Engineering, 2015.\n[40] L. Bozhkov, P. Koprinkova-Hristova, and P. Georgieva, “Reservoir com-\nputing for emotion valence discrimination from EEG signals,” Neuro-\ncomputing, vol. 231, pp. 28–40, 2017.\n33\n[41] M. G. Doborjeh, G. Y. Wang, N. K. Kasabov, R. Kydd, and B. Russell,\n“A Spiking Neural Network Methodology and System for Learning and\nComparative Analysis of EEG Data From Healthy Versus Addiction\nTreated Versus Addiction Not Treated Subjects,” IEEE Transactions\non Biomedical Engineering, vol. 63, no. 9, pp. 1830–1841, 2016.\n[42] B. Y. Lei, X. L. Liu, S. Liang, W. L. Hang, Q. Wang, K. S. Choi, and\nJ. Qin, “Walking Imagery Evaluation in Brain Computer Interfaces via\na Multi-view Multi-level Deep Polynomial Network,” IEEE Transac-\ntions on Neural Systems and Rehabilitation Engineering, vol. 27, no. 3,\npp. 497–506, 2019.\n[43] K. Zhang, N. Robinson, S.-W. Lee, and C. Guan, “Adaptive transfer\nlearning for eeg motor imagery classiﬁcation with deep convolutional\nneural network,” Neural Networks, vol. 136, pp. 1–10, 2021.\n[44] N. Lu, T. F. Li, X. D. Ren, and H. Y. Miao, “A Deep Learning Scheme\nfor Motor Imagery Classiﬁcation based on Restricted Boltzmann Ma-\nchines,” IEEE Transactions on Neural Systems and Rehabilitation En-\ngineering, vol. 25, no. 6, pp. 566–576, 2017.\n[45] W. L. Hang, W. Feng, R. Y. Du, S. Liang, Y. Chen, Q. Wang, and\nX. J. Liu, “Cross-Subject EEG Signal Recognition Using Deep Domain\nAdaptation Network,” IEEE Access, vol. 7, pp. 128 273–128 282, 2019.\n[46] D. Y. Zhao, F. Z. Tang, B. L. Si, and X. S. Feng, “Learning joint space-\ntime-frequency features for EEG decoding on small labeled data,” Neu-\nral Networks, vol. 114, pp. 67–77, 2019.\n[47] H. Dose, J. S. Moller, H. K. Iversen, and S. Puthusserypady, “An\nend-to-end deep learning approach to MI-EEG signal classiﬁcation for\nBCIs,” Expert Systems with Applications, vol. 114, pp. 532–542, 2018.\n[48] G. W. Xu, X. A. Shen, S. R. Chen, Y. S. Zong, C. Y. Zhang, H. Y. Yue,\nM. Liu, F. Chen, and W. L. Che, “A Deep Transfer Convolutional Neu-\nral Network Framework for EEG Signal Classiﬁcation,” IEEE Access,\nvol. 7, pp. 112 767–112 776, 2019.\n[49] Y. Li, X. R. Zhang, B. Zhang, M. Y. Lei, W. G. Cui, and Y. Z. Guo,\n“A Channel-Projection Mixed-Scale Convolutional Neural Network for\n34\nMotor Imagery EEG Decoding,” IEEE Transactions on Neural Systems\nand Rehabilitation Engineering, vol. 27, no. 6, pp. 1170–1180, 2019.\n[50] Z. W. Zhang, F. Duan, J. Sole-Casals, J. Dinares-Ferran, A. Cichocki,\nZ. L. Yang, and Z. Sun, “A Novel Deep Learning Approach With Data\nAugmentation to Classify Motor Imagery Signals,” IEEE Access, vol. 7,\npp. 15 945–15 954, 2019.\n[51] D. L. Li, J. H. Wang, J. C. Xu, and X. K. Fang, “Densely Feature\nFusion Based on Convolutional Neural Networks for Motor Imagery\nEEG Classiﬁcation,” IEEE Access, vol. 7, pp. 132 720–132 730, 2019.\n[52] R. Alazrai, M. Abuhijleh, H. Alwanni, and M. I. Daoud, “A Deep\nLearning Framework for Decoding Motor Imagery Tasks of the Same\nHand Using EEG Signals,” IEEE Access, vol. 7, pp. 109 612–109 627,\n2019.\n[53] G. B. Kshirsagar and N. D. Londhe, “Improving Performance of De-\nvanagari Script Input-Based P300 Speller Using Deep Learning,” IEEE\nTransactions on Biomedical Engineering, vol. 66, no. 11, pp. 2992–3005,\n2019.\n[54] V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P.\nHung, and B. J. Lance, “EEGNet: a compact convolutional neural\nnetwork for EEG-based brain-computer interfaces,” Journal of Neural\nEngineering, vol. 15, no. 5, 2018.\n[55] D. Y. Pei, M. Burns, R. Chandramouli, and R. Vinjamuri, “Decod-\ning Asynchronous Reaching in Electroencephalography Using Stacked\nAutoencoders,” IEEE Access, vol. 6, pp. 52 889–52 898, 2018.\n[56] S. Kundu and S. Ari, “P300 based character recognition using sparse\nautoencoder with ensemble of SVMs,” Biocybernetics and Biomedical\nEngineering, vol. 39, no. 4, pp. 956–966, 2019.\n[57] Y. R. Tabar and U. Halici, “A novel deep learning approach for classiﬁ-\ncation of EEG motor imagery signals,” Journal of Neural Engineering,\nvol. 14, no. 1, 2017.\n[58] S. U. Amin, M. Alsulaiman, G. Muhammad, M. A. Mekhtiche, and\nM. S. Hossain, “Deep Learning for EEG motor imagery classiﬁcation\n35\nbased on multi-layer CNNs feature fusion,” Future Generation Com-\nputer Systems-the International Journal of Escience, vol. 101, pp. 542–\n554, 2019.\n[59] R. L. Zhang, Q. Zong, L. Q. Dou, and X. Y. Zhao, “A novel hybrid deep\nlearning scheme for four-class motor imagery classiﬁcation,” Journal of\nNeural Engineering, vol. 16, no. 6, 2019.\n[60] J. Yang, S. W. Yao, and J. Wang, “Deep Fusion Feature Learning\nNetwork for MI-EEG Classiﬁcation,” IEEE Access, vol. 6, pp. 79 050–\n79 059, 2018.\n[61] T. Ma, H. Li, H. Yang, X. L. Lv, P. Y. Li, T. J. Liu, D. Z. Yao,\nand P. Xu, “The extraction of motion-onset VEP BCI features based\non deep learning and compressed sensing,” Journal of Neuroscience\nMethods, vol. 275, pp. 80–92, 2017.\n[62] T. H. Nguyen and W. Y. Chung, “A Single-Chanel SSVEP-Based BCI\nSpeller Using Deep Learning,” IEEE Access, vol. 7, pp. 1752–1763,\n2019.\n[63] N. Waytowich, V. J. Lawhern, J. O. Garcia, J. Cummings, J. Faller,\nP. Sajda, and J. M. Vettel, “Compact convolutional neural networks for\nclassiﬁcation of asynchronous steady-state visual evoked potentials,”\nJournal of Neural Engineering, vol. 15, no. 6, 2018.\n[64] U. R. Acharya, S. L. Oh, Y. Hagiwara, J. H. Tan, and H. Adeli, “Deep\nconvolutional neural network for the automated detection and diagnosis\nof seizure using EEG signals,” Computers in Biology and Medicine, vol.\n100, pp. 270–278, 2018.\n[65] S. L. Oh, J. Vicnesh, E. J. Ciaccio, R. Yuvaraj, and U. R. Acharya,\n“Deep Convolutional Neural Network Model for Automated Diagnosis\nof Schizophrenia Using EEG Signals,” Applied Sciences-Basel, vol. 9,\nno. 14, 2019.\n[66] J. Cao,\nJ. Zhu,\nW. Hu,\nand A. Kummert,\n“Epileptic Signal\nClassiﬁcation\nWith\nDeep\nEEG\nFeatures\nby\nStacked\nCNNs,”\nIEEE\nTransactions\non\nCognitive\nand\nDevelopmental\nSystems,\nvol.\n12,\nno.\n4,\npp.\n709–722,\nDec.\n2020.\n[Online].\nAvailable:\nhttps://ieeexplore.ieee.org/document/8807291/\n36\n[67] A. Antoniades, L. Spyrou, D. Martin-Lopez, A. Valentin, G. Alarcon,\nS. Sanei, and C. C. Took, “Detection of Interictal Discharges With\nConvolutional Neural Networks Using Discrete Ordered Multichannel\nIntracranial EEG,” IEEE Transactions on Neural Systems and Reha-\nbilitation Engineering, vol. 25, no. 12, pp. 2285–2294, 2017.\n[68] T. X. Wen and Z. N. Zhang, “Deep Convolution Neural Network and\nAutoencoders-Based Unsupervised Feature Learning of EEG Signals,”\nIEEE Access, vol. 6, pp. 25 399–25 410, 2018.\n[69] H. Daoud and M. A. Bayoumi, “Eﬃcient Epileptic Seizure Prediction\nBased on Deep Learning,” IEEE Transactions on Biomedical Circuits\nand Systems, vol. 13, no. 5, pp. 804–813, 2019.\n[70] H. Daoud and M. Bayoumi, “Deep Learning Approach for Epileptic\nFocus Localization,” IEEE Transactions on Biomedical Circuits and\nSystems, vol. 14, no. 2, pp. 209–220, Apr. 2020. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/8918409/\n[71] W. Mumtaz and A. Qayyum, “A deep learning framework for auto-\nmatic diagnosis of unipolar depression,” International Journal of Med-\nical Informatics, vol. 132, 2019.\n[72] Y. Qiu, W. D. Zhou, N. N. Yu, and P. D. Du, “Denoising Sparse\nAutoencoder-Based Ictal PEG Classiﬁcation,” IEEE Transactions on\nNeural Systems and Rehabilitation Engineering, vol. 26, no. 9, pp.\n1717–1726, 2018.\n[73] H. Chen, Y. Song, and X. L. Li, “A deep learning framework for iden-\ntifying children with ADHD using an EEG-based brain network,” Neu-\nrocomputing, vol. 356, pp. 83–96, 2019.\n[74] Y. Yuan, G. X. Xun, K. B. Jia, and A. D. Zhang, “A Multi-View\nDeep Learning Framework for EEG Seizure Detection,” IEEE Journal\nof Biomedical and Health Informatics, vol. 23, no. 1, pp. 83–94, 2019.\n[75] C. Ieracitano, N. Mammone, A. Bramanti, A. Hussain, and F. C. Mora-\nbito, “A Convolutional Neural Network approach for classiﬁcation of\ndementia stages based on 2D-spectral representation of EEG record-\nings,” Neurocomputing, vol. 323, pp. 96–107, 2019.\n37\n[76] C. A. T. Naira and C. J. L. Del Alamo, “Classiﬁcation of People who\nSuﬀer Schizophrenia and Healthy People by EEG Signals using Deep\nLearning,” International Journal of Advanced Computer Science and\nApplications, vol. 10, no. 10, pp. 511–516, 2019.\n[77] G. Vrbancic and V. Podgorelec, “Automatic Classiﬁcation of Motor\nImpairment Neural Disorders from EEG Signals Using Deep Convolu-\ntional Neural Networks,” Elektronika Ir Elektrotechnika, vol. 24, no. 4,\n2018.\n[78] O. Turk and M. S. Ozerdem, “Epilepsy Detection by Using Scalogram\nBased Convolutional Neural Network from EEG Signals,” Brain Sci-\nences, vol. 9, no. 5, 2019.\n[79] X. Y. Wei, L. Zhou, Z. Y. Chen, L. J. Zhang, and Y. Zhou, “Automatic\nseizure detection using three-dimensional CNN based on multi-channel\nEEG,” Bmc Medical Informatics and Decision Making, vol. 18, 2018.\n[80] X. B. Tian, Z. H. Deng, W. H. Ying, K. S. Choi, D. R. Wu, B. Qin,\nJ. Wang, H. B. Shen, and S. T. Wang, “Deep Multi-View Feature\nLearning for EEG-Based Epileptic Seizure Detection,” IEEE Transac-\ntions on Neural Systems and Rehabilitation Engineering, vol. 27, no. 10,\npp. 1962–1972, 2019.\n[81] R. San-Segundo, M. Gil-Martin, L. F. D’Haro-Enriquez, and J. M.\nPardo, “Classiﬁcation of epileptic EEG recordings using signal trans-\nforms and convolutional neural networks,” Computers in Biology and\nMedicine, vol. 109, pp. 148–158, 2019.\n[82] I. Ullah, M. Hussain, E. U. Qazi, and H. Aboalsamh, “An automated\nsystem for epilepsy detection using EEG brain signals based on deep\nlearning approach,” Expert Systems with Applications, vol. 107, pp.\n61–71, 2018.\n[83] T. Zhang, W. M. Zheng, Z. Cui, Y. Zong, and Y. Li, “Spatial-Temporal\nRecurrent Neural Network for Emotion Recognition,” IEEE Transac-\ntions on Cybernetics, vol. 49, no. 3, pp. 839–847, 2019.\n[84] W. L. Zheng, W. Liu, Y. F. Lu, B. L. Lu, and A. Cichocki, “Emotion-\nMeter: A Multimodal Framework for Recognizing Human Emotions,”\nIEEE Transactions on Cybernetics, vol. 49, no. 3, pp. 1110–1122, 2019.\n38\n[85] A. Al-Nafjan, A. Al-Wabil, M. Hosny, and Y. Al-Ohali, “Classiﬁcation\nof Human Emotions from Electroencephalogram (EEG) Signal using\nDeep Neural Network,” International Journal of Advanced Computer\nScience and Applications, vol. 8, no. 9, pp. 419–425, 2017.\n[86] Y. H. Kwon, S. B. Shin, and S. D. Kim, “Electroencephalography Based\nFusion Two-Dimensional (2D)-Convolution Neural Networks (CNN)\nModel for Emotion Recognition System,” Sensors, vol. 18, no. 5, 2018.\n[87] W.-L. Zheng and B.-L. Lu, “Investigating critical frequency bands and\nchannels for eeg-based emotion recognition with deep neural networks,”\nIEEE Transactions on Autonomous Mental Development, vol. 7, no. 3,\npp. 162–175, 2015.\n[88] S. Koelstra,\nC. Muhl,\nM. Soleymani,\nJ.-S. Lee,\nA. Yazdani,\nT. Ebrahimi, T. Pun, A. Nijholt, and I. Patras, “Deap: A database\nfor emotion analysis; using physiological signals,” IEEE transactions\non aﬀective computing, vol. 3, no. 1, pp. 18–31, 2011.\n[89] W.-L. Zheng, J.-Y. Zhu, and B.-L. Lu, “Identifying Stable Patterns\nover Time for Emotion Recognition from EEG,” IEEE Transactions\non Aﬀective Computing, vol. 10, no. 3, pp. 417–429, Jun. 2017.\n[Online]. Available: https://ieeexplore.ieee.org/document/7938737/\n[90] Y. Yang, Q. M. J. Wu, W.-L. Zheng, and B.-L. Lu, “EEG-Based\nEmotion Recognition Using Hierarchical Network With Subnetwork\nNodes,” IEEE Transactions on Cognitive and Developmental Systems,\nvol.\n10,\nno.\n2,\npp.\n408–419,\nJun.\n2018.\n[Online].\nAvailable:\nhttps://ieeexplore.ieee.org/document/7883875/\n[91] J. P. Li, Z. X. Zhang, and H. G. He, “Hierarchical Convolutional Neural\nNetworks for EEG-Based Emotion Recognition,” Cognitive Computa-\ntion, vol. 10, no. 2, pp. 368–380, 2018.\n[92] H. Zeng, Z. H. Wu, J. M. Zhang, C. Yang, H. Zhang, G. J. Dai, and\nW. Z. Kong, “EEG Emotion Classiﬁcation Using an Improved SincNet-\nBased Deep Learning Model,” Brain Sciences, vol. 9, no. 11, 2019.\n[93] M. Ravanelli and Y. Bengio, “Speaker recognition from raw wave-\nform with sincnet,” 2018 IEEE Spoken Language Technology Workshop\n(SLT), pp. 1021–1028, 2018.\n39\n[94] E. J. Choi and D. K. Kim, “Arousal and Valence Classiﬁcation Model\nBased on Long Short-Term Memory and DEAP Data for Mental\nHealthcare Management,” Healthcare Informatics Research, vol. 24,\nno. 4, pp. 309–316, 2018.\n[95] S. Alhagry, A. A. Fahmy, and R. A. El-Khoribi, “Emotion Recognition\nbased on EEG using LSTM Recurrent Neural Network,” International\nJournal of Advanced Computer Science and Applications, vol. 8, no. 10,\npp. 355–358, 2017.\n[96] S. Bagherzadeh, K. Maghooli, J. Farhadi, and M. Z. Soroush, “Emotion\nRecognition from Physiological Signals Using Parallel Stacked Autoen-\ncoders,” Neurophysiology, vol. 50, no. 6, pp. 428–435, 2018.\n[97] H. Chao, H. L. Zhi, L. Dong, and Y. L. Liu, “Recognition of Emotions\nUsing Multichannel EEG Data and DBN-GC-Based Ensemble Deep\nLearning Framework,” Computational Intelligence and Neuroscience,\n2018.\n[98] X. Li, D. W. Song, P. Zhang, Y. X. Hou, and B. Hu, “Deep fusion\nof multi-channel neurophysiological signal for emotion recognition and\nmonitoring,” International Journal of Data Mining and Bioinformat-\nics, vol. 18, no. 1, pp. 1–27, 2017.\n[99] Y. Yin, X. Zheng, B. Hu, Y. Zhang, and X. Cui, “Eeg emotion recog-\nnition using fusion model of graph convolutional neural networks and\nlstm,” Applied Soft Computing, vol. 100, p. 106954, 2021.\n[100] A. Topic and M. Russo, “Emotion recognition based on eeg feature\nmaps through deep learning network,” Engineering Science and Tech-\nnology, an International Journal, 2021.\n[101] S. Aydin, “Deep Learning Classiﬁcation of Neuro-Emotional Phase\nDomain\nComplexity\nLevels\nInduced\nby\nAﬀective\nVideo\nFilm\nClips,”\nIEEE\nJournal\nof\nBiomedical\nand\nHealth\nInformatics,\nvol. 24,\nno. 6,\npp. 1695–1702,\nJun. 2020. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/8933102/\n[102] L.\nZhu,\nC.\nSu,\nJ.\nZhang,\nG.\nCui,\nA.\nCichocki,\nC.\nZhou,\nand J. Li,\n“EEG-based approach for recognizing human social\n40\nemotion perception,” Advanced Engineering Informatics, vol. 46, p.\n101191, Oct. 2020. [Online]. Available:\nhttps://linkinghub.elsevier.\ncom/retrieve/pii/S1474034620301609\n[103] D. J. Hemanth, J. Anitha, and L. H. Son, “Brain signal based hu-\nman emotion analysis by circular back propagation and Deep Kohonen\nNeural Networks,” Computers & Electrical Engineering, vol. 68, pp.\n170–180, 2018.\n[104] S. Jirayucharoensak, S. Pan-Ngum, and P. Israsena, “EEG-Based Emo-\ntion Recognition Using Deep Learning Network with Principal Com-\nponent Based Covariate Shift Adaptation,” Scientiﬁc World Journal,\n2014.\n[105] Z. K. Gao, Y. L. Li, Y. X. Yang, X. M. Wang, N. Dong, and H. D.\nChiang, “A GPSO-optimized convolutional neural networks for EEG-\nbased emotion recognition,” Neurocomputing, vol. 380, pp. 225–235,\n2020.\n[106] Z. Yin and J. H. Zhang, “Task-generic mental fatigue recognition based\non neurophysiological signals and dynamical deep extreme learning ma-\nchine,” Neurocomputing, vol. 283, pp. 266–281, 2018.\n[107] Y. L. Ma, B. Chen, R. H. Li, C. S. Wang, J. Wang, Q. S. She, Z. Z. Luo,\nand Y. C. Zhang, “Driving Fatigue Detection from EEG Using a Mod-\niﬁed PCANet Method,” Computational Intelligence and Neuroscience,\nvol. 2019, 2019.\n[108] H. Zeng, C. Yang, G. J. Dai, F. W. Qin, J. H. Zhang, and W. Z. Kong,\n“EEG classiﬁcation of driver mental states by deep learning,” Cognitive\nNeurodynamics, vol. 12, no. 6, pp. 597–606, 2018.\n[109] Z. K. Gao, X. M. Wang, Y. X. Yang, C. X. Mu, Q. Cai, W. D. Dang,\nand S. Y. Zuo, “EEG-Based Spatio-Temporal Convolutional Neural\nNetwork for Driver Fatigue Evaluation,” IEEE Transactions on Neural\nNetworks and Learning Systems, vol. 30, no. 9, pp. 2755–2763, 2019.\n[110] R. F. Chai, S. H. Ling, P. P. San, G. R. Naik, T. N. Nguyen, Y. Tran,\nA. Craig, and H. T. Nguyen, “Improving EEG-Based Driver Fatigue\nClassiﬁcation Using Sparse-Deep Belief Networks,” Frontiers in Neu-\nroscience, vol. 11, 2017.\n41\n[111] E. Q. Wu, X. Y. Peng, C. Z. Z. Zhang, J. X. Lin, and R. S. F. Sheng,\n“Pilots’ Fatigue Status Recognition Using Deep Contractive Autoen-\ncoder Network,” IEEE Transactions on Instrumentation and Measure-\nment, vol. 68, no. 10, pp. 3907–3919, 2019.\n[112] J. D. Tao, Z. Yin, L. Liu, Y. Tian, Z. Q. Sun, and J. H. Zhang,\n“Individual-Speciﬁc Classiﬁcation of Mental Workload Levels Via an\nEnsemble Heterogeneous Extreme Learning Machine for EEG Model-\ning,” Symmetry-Basel, vol. 11, no. 7, 2019.\n[113] J. H. Zhang and S. A. Li, “A deep learning scheme for mental work-\nload classiﬁcation based on restricted Boltzmann machines,” Cognition\nTechnology & Work, vol. 19, no. 4, pp. 607–631, 2017.\n[114] R. Hefron, B. Borghetti, C. S. Kabban, J. Christensen, and J. Estepp,\n“Cross-Participant EEG-Based Assessment of Cognitive Workload Us-\ning Multi-Path Convolutional Recurrent Neural Networks,” Sensors,\nvol. 18, no. 5, 2018.\n[115] Z. Yin and J. H. Zhang, “Cross-subject recognition of operator func-\ntional states via EEG and switching deep belief networks with adaptive\nweights,” Neurocomputing, vol. 260, pp. 349–366, 2017.\n[116] P. B. Zhang, X. Wang, W. H. Zhang, and J. F. Chen, “Learning\nSpatial-Spectral-Temporal EEG Features With Recurrent 3D Convolu-\ntional Neural Networks for Cross-Task Mental Workload Assessment,”\nIEEE Transactions on Neural Systems and Rehabilitation Engineering,\nvol. 27, no. 1, pp. 31–42, 2019.\n[117] Z. Yin, M. Y. Zhao, W. Zhang, Y. X. Wang, Y. G. Wang, and\nJ. H. Zhang, “Physiological-signal-based mental workload estimation\nvia transfer dynamical autoencoders in a deep learning framework,”\nNeurocomputing, vol. 347, pp. 212–229, 2019.\n[118] P. B. Zhang, X. Wang, J. F. Chen, W. You, and W. H. Zhang, “Spectral\nand Temporal Feature Learning With Two-Stream Neural Networks for\nMental Workload Assessment,” IEEE Transactions on Neural Systems\nand Rehabilitation Engineering, vol. 27, no. 6, pp. 1149–1159, 2019.\n42\n[119] H. Dong, A. Supratak, W. Pan, C. Wu, P. M. Matthews, and Y. Guo,\n“Mixed Neural Network Approach for Temporal Sleep Stage Classi-\nﬁcation,” IEEE Transactions on Neural Systems and Rehabilitation\nEngineering, vol. 26, no. 2, pp. 324–333, 2018.\n[120] E. Bresch, U. Grossekathofer, and G. Garcia-Molina, “Recurrent Deep\nNeural Networks for Real-Time Sleep Stage Classiﬁcation From Sin-\ngle Channel EEG,” Frontiers in Computational Neuroscience, vol. 12,\n2018.\n[121] P. M. Kulkarni, Z. D. Xiao, E. J. Robinson, A. S. Jami, J. P. Zhang,\nH. C. Zhou, S. E. Henin, A. A. Liu, R. S. Osorio, J. Wang, and Z. Chen,\n“A deep learning approach for real-time detection of sleep spindles,”\nJournal of Neural Engineering, vol. 16, no. 3, 2019.\n[122] O. Tsinalis, P. M. Matthews, and Y. K. Guo, “Automatic Sleep Stage\nScoring Using Time-Frequency Analysis and Stacked Sparse Autoen-\ncoders,” Annals of Biomedical Engineering, vol. 44, no. 5, pp. 1587–\n1597, 2016.\n[123] S. Hartmann and M. Baumert, “Automatic A-Phase Detection of\nCyclic Alternating Patterns in Sleep Using Dynamic Temporal Infor-\nmation,” IEEE Transactions on Neural Systems and Rehabilitation En-\ngineering, vol. 27, no. 9, pp. 1695–1703, 2019.\n[124] J. M. Zhang, R. X. Yao, W. G. Ge, and J. F. Gao, “Orthogonal\nconvolutional neural networks for automatic sleep stage classiﬁcation\nbased on single-channel EEG,” Computer Methods and Programs in\nBiomedicine, vol. 183, 2020.\n[125] Y. Jeon, S. Kim, H. S. Choi, Y. G. Chung, S. A. Choi, H. Kim, S. Yoon,\nH. Hwang, and K. J. Kim, “Pediatric Sleep Stage Classiﬁcation Us-\ning Multi-Domain Hybrid Neural Networks,” IEEE Access, vol. 7, pp.\n96 495–96 505, 2019.\n[126] E. Eldele, Z. Chen, C. Liu, M. Wu, C.-K. Kwoh, X. Li, and C. Guan,\n“An attention-based deep learning approach for sleep stage classiﬁca-\ntion with single-channel eeg,” IEEE Transactions on Neural Systems\nand Rehabilitation Engineering, 2021.\n43\n[127] J. M. Zhang and Y. Wu, “Complex-valued unsupervised convolutional\nneural networks for sleep stage classiﬁcation,” Computer Methods and\nPrograms in Biomedicine, vol. 164, pp. 181–191, 2018.\n[128] J. M. Zhang, Y. Wu, J. Bai, and F. Q. Chen, “Automatic sleep stage\nclassiﬁcation based on sparse deep belief net and combination of multi-\nple classiﬁers,” Transactions of the Institute of Measurement and Con-\ntrol, vol. 38, no. 4, pp. 435–451, 2016.\n[129] M. A. Jaoude, H. Sun, K. R. Pellerin, M. Pavlova, and R. A. Sarkis,\n“Expert-level automated sleep staging of long-term scalp electroen-\ncephalography recordings using deep learning,” p. 12.\n[130] Z. Mousavi, T. Y. Rezaii, S. Sheykhivand, A. Farzamnia, and S. N.\nRazavi, “Deep convolutional neural network for classiﬁcation of sleep\nstages from single-channel EEG signals,” Journal of Neuroscience\nMethods, vol. 324, 2019.\n[131] P. Chriskos, C. A. Frantzidis, P. T. Gkivogkli, P. D. Bamidis, and\nC. Kourtidou-Papadeli, “Automatic Sleep Staging Employing Convo-\nlutional Neural Networks and Cortical Connectivity Images,” IEEE\nTransactions on Neural Networks and Learning Systems, vol. 31, no. 1,\npp. 113–123, 2020.\n[132] O. Ozdenizci, Y. Wang, T. Koike-Akino, and D. Erdogmus, “Adversar-\nial Deep Learning in EEG Biometrics,” IEEE Signal Processing Letters,\nvol. 26, no. 5, pp. 710–714, 2019.\n[133] M. Wang, H. El-Fiqi, J. K. Hu, and H. A. Abbass, “Convolutional Neu-\nral Networks Using Dynamic Functional Connectivity for EEG-Based\nPerson Identiﬁcation in Diverse Human States,” IEEE Transactions\non Information Forensics and Security, vol. 14, no. 12, pp. 3259–3272,\n2019.\n[134] P. Kaushik, A. Gupta, P. P. Roy, and D. P. Dogra, “EEG-Based Age\nand Gender Prediction Using Deep BLSTM-LSTM Network Model,”\nIEEE Sensors Journal, vol. 19, no. 7, pp. 2634–2641, 2019.\n[135] N. Huang, M. Slaney, and M. Elhilali, “Connecting Deep Neural Net-\nworks to Physical, Perceptual, and Electrophysiological Auditory Sig-\nnals,” Frontiers in Neuroscience, vol. 12, 2018.\n44\n[136] M. X. Yu, Y. C. Sun, B. F. Zhu, L. Q. Zhu, Y. Z. Lin, X. Y. Tang, Y. K.\nGuo, G. K. Sun, and M. L. Dong, “Diverse frequency band-based con-\nvolutional neural networks for tonic cold pain assessment using EEG,”\nNeurocomputing, vol. 378, pp. 270–282, 2020.\n[137] J. M. Jiang, A. Fares, and S. H. Zhong, “A Context-Supported Deep\nLearning Framework for Multimodal Brain Imaging Classiﬁcation,”\nIEEE Transactions on Human-Machine Systems, vol. 49, no. 6, pp.\n611–622, 2019.\n[138] X. Zheng, W. Z. Chen, M. Y. Li, T. Zhang, Y. You, and Y. Jiang,\n“Decoding human brain activity with deep learning,” Biomedical Signal\nProcessing and Control, vol. 56, 2020.\n[139] C. C. Hua, H. Wang, H. Wang, S. W. Lu, C. Liu, and S. M. Khalid,\n“A Novel Method of Building Functional Brain Network Using Deep\nLearning Algorithm with Application in Proﬁciency Detection,” Inter-\nnational Journal of Neural Systems, vol. 29, no. 1, 2019.\n[140] S. Toraman, S. A. Tuncer, and F. Balgetir, “Is it possible to detect\ncerebral dominance via EEG signals by using deep learning?” Medical\nHypotheses, vol. 131, 2019.\n[141] V. Baltatzis, K. M. Bintsi, G. K. Apostolidis, and L. J. Hadjileontiadis,\n“Bullying incidences identiﬁcation within an immersive environment\nusing HD EEG-based analysis: A Swarm Decomposition and Deep\nLearning approach,” Scientiﬁc Reports, vol. 7, 2017.\n[142] Z. G. Doborjeh, N. Kasabov, M. G. Doborjeh, and A. Sumich, “Mod-\nelling Peri-Perceptual Brain Processes in a Deep Learning Spiking Neu-\nral Network Architecture,” Scientiﬁc Reports, vol. 8, 2018.\n[143] A. Arora, J. J. Lin, A. Gasperian, J. Maldjian, J. Stein, M. Kahana,\nand B. Lega, “Comparison of logistic regression, support vector ma-\nchines, and deep learning classiﬁers for predicting memory encoding\nsuccess using human intracranial EEG recordings,” Journal of Neural\nEngineering, vol. 15, no. 6, 2018.\n[144] T. K. K. Ho, J. Gwak, C. M. Park, and J. I. Song, “Discrimination\nof Mental Workload Levels From Multi-Channel fNIRS Using Deep\n45\nLeaning-Based Approaches,” IEEE Access, vol. 7, pp. 24 392–24 403,\n2019.\n[145] J. Harvy, E. Sigalas, N. Thakor, A. Bezerianos, and J. Li, “Perfor-\nmance improvement of driving fatigue identiﬁcation based on power\nspectra and connectivity using feature level and decision level fusions,”\nin 2018 40th Annual International Conference of the IEEE Engineering\nin Medicine and Biology Society (EMBC).\nIEEE, 2018, pp. 102–105.\n[146] Z. Pei, H. Wang, A. Bezerianos, and J. Li, “Eeg-based multiclass work-\nload identiﬁcation using feature fusion and selection,” IEEE Transac-\ntions on Instrumentation and Measurement, vol. 70, pp. 1–8, 2020.\n[147] W. Wu, W. Sun, Q. J. Wu, Y. Yang, H. Zhang, W.-L. Zheng, and B.-\nL. Lu, “Multimodal vigilance estimation using deep learning,” IEEE\nTransactions on Cybernetics, 2020.\n[148] J. Li, A. Bezerianos, and N. Thakor, “Cognitive state analysis, un-\nderstanding, and decoding from the perspective of brain connectivity,”\narXiv preprint arXiv:2005.12191, 2020.\n[149] S. K. Goh, H. A. Abbass, K. C. Tan, A. Al-Mamun, N. Thakor, A. Bez-\nerianos, and J. H. Li, “Spatio-Spectral Representation Learning for\nElectroencephalographic Gait-Pattern Classiﬁcation,” IEEE Transac-\ntions on Neural Systems and Rehabilitation Engineering, vol. 26, no. 9,\npp. 1858–1867, 2018.\n[150] J. Li, “Thoughts on neurophysiological signal analysis and classiﬁca-\ntion,” Brain Science Advances, vol. 6, no. 3, pp. 210–223, 2020.\n[151] X. L. Ma, S. Qiu, W. Wei, S. P. Wang, and H. G. He, “Deep Channel-\nCorrelation Network for Motor Imagery Decoding From the Same\nLimb,” IEEE Transactions on Neural Systems and Rehabilitation En-\ngineering, vol. 28, no. 1, pp. 297–306, 2020.\n[152] X. Y. Zhu, P. Y. Li, C. B. Li, D. Z. Yao, R. Zhang, and P. Xu, “Sepa-\nrated channel convolutional neural network to realize the training free\nmotor imagery BCI systems,” Biomedical Signal Processing and Con-\ntrol, vol. 49, pp. 396–403, 2019.\n46\n[153] A. M. Chiarelli, P. Croce, A. Merla, and F. Zappasodi, “Deep learning\nfor hybrid EEG-fNIRS brain-computer interface: application to motor\nimagery classiﬁcation,” Journal of Neural Engineering, vol. 15, no. 3,\n2018.\n[154] Z. Tayeb, J. Fedjaev, N. Ghaboosi, C. Richter, L. Everding, X. W.\nQu, Y. Y. Wu, G. Cheng, and J. Conradt, “Validating Deep Neural\nNetworks for Online Decoding of Motor Imagery Movements from EEG\nSignals,” Sensors, vol. 19, no. 1, 2019.\n[155] M. X. Dai, D. Z. Zheng, R. Na, S. Wang, and S. L. Zhang, “EEG Clas-\nsiﬁcation of Motor Imagery Using a Novel Deep Learning Framework,”\nSensors, vol. 19, no. 3, 2019.\n[156] K. W. Ha and J. W. Jeong, “Motor Imagery EEG Classiﬁcation Using\nCapsule Networks,” Sensors, vol. 19, no. 13, 2019.\n[157] T. W. Shi, L. Ren, and W. H. Cui, “Feature recognition of motor\nimaging EEG signals based on deep learning,” Personal and Ubiquitous\nComputing, vol. 23, no. 3-4, pp. 499–510, 2019.\n[158] Z. J. Wang, L. Cao, Z. Zhang, X. L. Gong, Y. R. Sun, and H. R.\nWang, “Short time Fourier transformation and deep neural networks\nfor motor imagery brain computer interface recognition,” Concurrency\nand Computation-Practice & Experience, vol. 30, no. 23, 2018.\n[159] S. U. Amin, M. Alsulaiman, G. Muhammad, M. A. Bencherif, and\nM. S. Hossain, “Multilevel Weighted Feature Fusion Using Convolu-\ntional Neural Networks for EEG Motor Imagery Classiﬁcation,” IEEE\nAccess, vol. 7, pp. 18 940–18 950, 2019.\n[160] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstet-\nter, K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and\nT. Ball, “Deep learning with convolutional neural networks for eeg de-\ncoding and visualization,” Human brain mapping, vol. 38, no. 11, pp.\n5391–5420, 2017.\n[161] A. Hassanpour, M. Moradikia, H. Adeli, S. R. Khayami, and P. Sham-\nsinejadbabaki, “A novel end-to-end deep learning scheme for classify-\ning multi-class motor imagery electroencephalography signals,” Expert\nSystems.\n47\n[162] Q. S. She, B. Hu, Z. Z. Luo, T. Nguyen, and Y. C. Zhang, “A hi-\nerarchical semi-supervised extreme learning machine method for EEG\nrecognition,” Medical & Biological Engineering & Computing, vol. 57,\nno. 1, pp. 147–157, 2019.\n[163] L. F. S. Uribe, C. A. Stefano, V. A. de Oliveira, T. B. D. Costa, P. G.\nRodrigues, D. C. Soriano, L. Boccato, G. Castellano, and R. Attux,\n“A correntropy-based classiﬁer for motor imagery brain-computer in-\nterfaces,” Biomedical Physics & Engineering Express, vol. 5, no. 6,\n2019.\n[164] L. J. Duan, M. H. Bao, S. Cui, Y. H. Qiao, and J. Miao, “Motor Im-\nagery EEG Classiﬁcation Based on Kernel Hierarchical Extreme Learn-\ning Machine,” Cognitive Computation, vol. 9, no. 6, pp. 758–765, 2017.\n[165] H. Wu, Y. Niu, F. Li, Y. C. Li, B. X. Fu, G. M. Shi, and M. H. Dong,\n“A Parallel Multiscale Filter Bank Convolutional Neural Networks for\nMotor Imagery EEG Classiﬁcation,” Frontiers in Neuroscience, vol. 13,\n2019.\n[166] I. Majidov and T. Whangbo, “Eﬃcient Classiﬁcation of Motor Im-\nagery Electroencephalography Signals Using Deep Learning Methods,”\nSensors, vol. 19, no. 7, 2019.\n[167] X. L. Tang, W. C. Ma, D. S. Kong, and W. Li, “Semisupervised Deep\nStacking Network with Adaptive Learning Rate Strategy for Motor\nImagery EEG Recognition,” Neural Computation, vol. 31, no. 5, pp.\n919–942, 2019.\n[168] B. G. Xu, L. L. Zhang, A. G. Song, C. C. Wu, W. L. Li, D. L. Zhang,\nG. Z. Xu, H. J. Li, and H. Zeng, “Wavelet Transform Time-Frequency\nImage and Convolutional Network-Based Motor Imagery EEG Classi-\nﬁcation,” IEEE Access, vol. 7, pp. 6084–6093, 2019.\n[169] O.-Y. Kwon,\nM.-H. Lee,\nC. Guan,\nand S.-W. Lee,\n“Subject-\nIndependent Brain–Computer Interfaces Based on Deep Convolutional\nNeural Networks,”\nIEEE Transactions on Neural Networks and\nLearning Systems, vol. 31, no. 10, pp. 3839–3852, Oct. 2020. [Online].\nAvailable: https://ieeexplore.ieee.org/document/8897723/\n48\n[170] N.\nMammone,\nC.\nIeracitano,\nand\nF.\nC.\nMorabito,\n“A\ndeep\nCNN approach to decode motor preparation of upper limbs from\ntime–frequency maps of EEG signals at source level,”\nNeural\nNetworks, vol. 124, pp. 357–372, Apr. 2020. [Online]. Available:\nhttps://linkinghub.elsevier.com/retrieve/pii/S089360802030037X\n[171] D. Zhang,\nK. Chen,\nD. Jian,\nand L. Yao,\n“Motor Imagery\nClassiﬁcation via Temporal Attention Cues of Graph Embedded\nEEG Signals,” IEEE Journal of Biomedical and Health Informatics,\nvol. 24,\nno. 9,\npp. 2570–2579,\nSep. 2020. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/8961150/\n[172] J. Chen, Z. Yu, Z. Gu, and Y. Li, “Deep Temporal-Spatial Feature\nLearning for Motor Imagery-Based Brain–Computer Interfaces,” IEEE\nTransactions on Neural Systems and Rehabilitation Engineering,\nvol. 28, no. 11, pp. 2356–2366, Nov. 2020. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/9201542/\n[173] J.-H. Jeong, K.-H. Shim, D.-J. Kim, and S.-W. Lee, “Brain-Controlled\nRobotic Arm System Based on Multi-Directional CNN-BiLSTM\nNetwork Using EEG Signals,” IEEE Transactions on Neural Systems\nand Rehabilitation Engineering,\nvol. 28,\nno. 5,\npp. 1226–1238,\nMay 2020. [Online]. Available: https://ieeexplore.ieee.org/document/\n9040397/\n[174] W. Gao, J. A. Guan, J. F. Gao, and D. Zhou, “Multi-ganglion ANN\nbased feature learning with application to P300-BCI signal classiﬁca-\ntion,” Biomedical Signal Processing and Control, vol. 18, pp. 127–137,\n2015.\n[175] A. Farahat, C. Reichert, C. M. Sweeney-Reed, and H. Hinrichs, “Con-\nvolutional neural networks for decoding of covert attention focus and\nsaliency maps for EEG feature visualization,” Journal of Neural Engi-\nneering, vol. 16, no. 6, 2019.\n[176] A. J. Solon, V. J. Lawhern, J. Touryan, J. R. McDaniel, A. J. Ries,\nand S. M. Gordon, “Decoding P300 Variability Using Convolutional\nNeural Networks,” Frontiers in Human Neuroscience, vol. 13, 2019.\n[177] L. Vareka and P. Mautner, “Stacked Autoencoders for the P300 Com-\nponent Detection,” Frontiers in Neuroscience, vol. 11, 2017.\n49\n[178] S. Morabbi, M. Keyvanpour, and S. V. Shojaedini, “A new method for\nP300 detection in deep belief networks: Nesterov momentum and drop\nbased learning rate,” Health and Technology, vol. 9, no. 4, pp. 615–630,\n2019.\n[179] A. Ditthapron, N. Banluesombatkul, S. Ketrat, E. Chuangsuwanich,\nand T. Wilaiprasitporn, “Universal Joint Feature Extraction for P300\nEEG Classiﬁcation Using Multi-Task Autoencoder,” IEEE Access,\nvol. 7, pp. 68 415–68 428, 2019.\n[180] L. Citi, R. Poli, and C. Cinel, “Documenting, modelling and exploit-\ning p300 amplitude changes due to variable target delays in donchin’s\nspeller,” Journal of Neural Engineering, vol. 7, no. 5, p. 056006, 2010.\n[181] M. Schreuder, B. Blankertz, and M. Tangermann, “A new auditory\nmulti-class brain-computer interface paradigm: spatial hearing as an\ninformative cue,” PloS one, vol. 5, no. 4, 2010.\n[182] L. Acqualagna and B. Blankertz, “Gaze-independent bci-spelling using\nrapid serial visual presentation (rsvp),” Clinical Neurophysiology, vol.\n124, no. 5, pp. 901–908, 2013.\n[183] M. S. Treder, H. Purwins, D. Miklody, I. Sturm, and B. Blankertz,\n“Decoding auditory attention to instruments in polyphonic music using\nsingle-trial eeg classiﬁcation,” Journal of neural engineering, vol. 11,\nno. 2, p. 026009, 2014.\n[184] B. Boloukian and F. Saﬁ-Esfahani, “Recognition of words from brain-\ngenerated signals of speech-impaired people: Application of autoen-\ncoders as a neural Turing machine controller in deep neural networks,”\nNeural Networks, vol. 121, pp. 186–207, 2020.\n[185] U. Hoﬀmann, J.-M. Vesin, T. Ebrahimi, and K. Diserens, “An eﬃcient\np300-based brain–computer interface for disabled subjects,” Journal of\nNeuroscience methods, vol. 167, no. 1, pp. 115–125, 2008.\n[186] J. X. Chen, Z. J. Mao, R. Zheng, Y. F. Huang, and L. F. He, “Feature\nSelection of Deep Learning Models for EEG-Based RSVP Target De-\ntection,” Ieice Transactions on Information and Systems, vol. E102D,\nno. 4, pp. 836–844, 2019.\n50\n[187] J. Touryan, G. Apker, S. Kerick, B. Lance, A. J. Ries, and K. Mc-\nDowell, “Translation of eeg-based performance prediction models to\nrapid serial visual presentation tasks,” International Conference on\nAugmented Cognition, pp. 521–530, 2013.\n[188] R. Manor and A. B. Geva, “Convolutional Neural Network for Multi-\nCategory Rapid Serial Visual Presentation BCI,” Frontiers in Compu-\ntational Neuroscience, vol. 9, 2015.\n[189] R. Manor, L. Mishali, and A. B. Geva, “Multimodal Neural Network for\nRapid Serial Visual Presentation Brain Computer Interface,” Frontiers\nin Computational Neuroscience, vol. 10, 2016.\n[190] Q. Q. Liu, Y. Jiao, Y. Y. Miao, C. L. Zuo, X. Y. Wang, A. Cichocki, and\nJ. Jin, “Eﬃcient representations of EEG signals for SSVEP frequency\nrecognition based on deep multiset CCA,” Neurocomputing, vol. 378,\npp. 36–44, 2020.\n[191] M. Nakanishi, Y. Wang, Y.-T. Wang, and T.-P. Jung, “A comparison\nstudy of canonical correlation analysis based methods for detecting\nsteady-state visual evoked potentials,” PloS one, vol. 10, no. 10, 2015.\n[192] X. J. Bi and H. B. Wang, “Early Alzheimer’s disease diagnosis based\non EEG spectral images using deep learning,” Neural Networks, vol.\n114, pp. 119–135, 2019.\n[193] F.\nC.\nMorabito,\nM.\nCampolo,\nN.\nMammone,\nM.\nVersaci,\nS. Franceschetti, F. Tagliavini, V. Soﬁa, D. Fatuzzo, A. Gambardella,\nA. Labate, L. Mumoli, G. G. Tripodi, S. Gasparini, V. Cianci, C. Sueri,\nE. Ferlazzo, and U. Aguglia, “Deep Learning Representation from Elec-\ntroencephalography of Early-Stage Creutzfeldt-Jakob Disease and Fea-\ntures for Diﬀerentiation from Rapidly Progressive Dementia,” Interna-\ntional Journal of Neural Systems, vol. 27, no. 2, 2017.\n[194] K. Hayase, K. Hayashi, and T. Sawa, “Hierarchical Poincare analysis\nfor anaesthesia monitoring,” Journal of Clinical Monitoring and Com-\nputing.\n[195] Q. Liu, J. F. Cai, S. Z. Fan, M. F. Abbod, J. S. Shieh, Y. C. Kung, and\nL. S. Lin, “Spectrum Analysis of EEG Signals Using CNN to Model\n51\nPatient’s Consciousness Level Based on Anesthesiologists’ Experience,”\nIEEE Access, vol. 7, pp. 53 731–53 742, 2019.\n[196] Y.\nPark,\nS.-H.\nHan,\nW.\nByun,\nJ.-H.\nKim,\nH.-C.\nLee,\nand\nS.-J. Kim, “A Real-Time Depth of Anesthesia Monitoring System\nBased on Deep Neural Network With Large EDO Tolerant EEG\nAnalog Front-End,” IEEE Transactions on Biomedical Circuits and\nSystems, vol. 14, no. 4, pp. 825–837, Aug. 2020. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/9103093/\n[197] S. Kim, J. Kim, and H. W. Chun, “Wave2Vec: Vectorizing Electroen-\ncephalography Bio-Signal for Prediction of Brain Disease,” Interna-\ntional Journal of Environmental Research and Public Health, vol. 15,\nno. 8, 2018.\n[198] H. Chen, Y. Song, and X. L. Li, “Use of deep learning to detect\npersonalized spatial-frequency abnormalities in EEGs of children with\nADHD,” Journal of Neural Engineering, vol. 16, no. 6, 2019.\n[199] R. Boshra, K. I. Ruiter, C. DeMatteo, J. P. Reilly, and J. F. Connolly,\n“Neurophysiological Correlates of Concussion: Deep Learning for Clin-\nical Assessment,” Scientiﬁc Reports, vol. 9, 2019.\n[200] H. Q. Sun, E. Kimchi, O. Akeju, S. B. Nagaraj, L. M. McClain, D. W.\nZhou, E. Boyle, W. L. Zheng, W. D. Ge, and M. B. Westover, “Auto-\nmated tracking of level of consciousness and delirium in critical illness\nusing deep learning,” Npj Digital Medicine, vol. 2, 2019.\n[201] B. Ay, O. Yildirim, M. Talo, U. B. Baloglu, G. Aydin, S. D. Puthankat-\ntil, and U. R. Acharya, “Automated Depression Detection Using Deep\nRepresentation and Sequence Learning with EEG Signals,” Journal of\nMedical Systems, vol. 43, no. 7, 2019.\n[202] U. R. Acharya, S. L. Oh, Y. Hagiwara, J. H. Tan, H. Adeli, and\nD. P. Subha, “Automated eeg-based screening of depression using deep\nconvolutional neural network,” Computer methods and programs in\nbiomedicine, vol. 161, pp. 103–113, 2018.\n[203] U. R. Acharya, S. Oh, Y. Hagiwara, J. H. Tan, H. Adeli, and D. P.\nSubha, “Automated EEG-based screening of depression using deep\n52\nconvolutional neural network,” Computer Methods and Programs in\nBiomedicine, vol. 161, pp. 103–113, 2018.\n[204] X. W. Li, X. Zhang, J. Zhu, W. D. Mao, S. T. Sun, Z. H. Wang, C. Xia,\nand B. Hu, “Depression recognition using machine learning methods\nwith diﬀerent feature generation strategies,” Artiﬁcial Intelligence in\nMedicine, vol. 99, 2019.\n[205] J. Zhu, Y. Wang, R. La, J. W. Zhan, J. H. Niu, S. Zeng, and X. P. Hu,\n“Multimodal Mild Depression Recognition Based on EEG-EM Syn-\nchronization Acauisition Network,” IEEE Access, vol. 7, pp. 28 196–\n28 210, 2019.\n[206] G. Bouallegue, R. Djemal, S. A. Alshebeili, and H. Aldhalaan,\n“A\nDynamic\nFiltering\nDF-RNN\nDeep-Learning-Based\nApproach\nfor EEG-Based Neurological Disorders Diagnosis,” IEEE Access,\nvol.\n8,\npp.\n206 992–207 007,\n2020.\n[Online].\nAvailable:\nhttps:\n//ieeexplore.ieee.org/document/9258906/\n[207] R. G. Andrzejak, K. Lehnertz, F. Mormann, C. Rieke, P. David,\nand C. E. Elger, “Indications of nonlinear deterministic and ﬁnite-\ndimensional structures in time series of brain electrical activity: De-\npendence on recording region and brain state,” Physical Review E,\nvol. 64, no. 6, p. 061907, 2001.\n[208] R. G. Andrzejak, K. Schindler, and C. Rummel, “Nonrandomness,\nnonlinear dependence, and nonstationarity of electroencephalographic\nrecordings from epilepsy patients,” Physical Review E, vol. 86, no. 4,\np. 046206, 2012.\n[209] A. M. Karim, M. S. Guzel, M. R. Tolun, H. Kaya, and F. V. Celebi, “A\nnew framework using deep auto-encoder and energy spectral density for\nmedical waveform data classiﬁcation and processing,” Biocybernetics\nand Biomedical Engineering, vol. 39, no. 1, pp. 148–159, 2019.\n[210] D. K. Thara, B. G. PremaSudha, and F. Xiong, “Epileptic seizure\ndetection and prediction using stacked bidirectional long short term\nmemory,” Pattern Recognition Letters, vol. 128, pp. 529–535, 2019.\n53\n[211] M. Abu Sayeed, S. P. Mohanty, E. Kougianos, and H. P. Zaveri, “Neuro-\nDetect: A Machine Learning-Based Fast and Accurate Seizure Detec-\ntion System in the IoMT,” IEEE Transactions on Consumer Electron-\nics, vol. 65, no. 3, pp. 359–368, 2019.\n[212] M. Stead, M. Bower, B. H. Brinkmann, K. Lee, W. R. Marsh, F. B.\nMeyer, B. Litt, J. Van Gompel, and G. A. Worrell, “Microseizures and\nthe spatiotemporal scales of human partial epilepsy,” Brain, vol. 133,\nno. 9, pp. 2789–2797, 2010.\n[213] B. H. Brinkmann, M. R. Bower, K. A. Stengel, G. A. Worrell, and\nM. Stead, “Large-scale electrophysiology: acquisition, compression, en-\ncryption, and storage of big data,” Journal of neuroscience methods,\nvol. 180, no. 1, pp. 185–192, 2009.\n[214] R. Hussein, H. Palangi, R. K. Ward, and Z. J. Wang, “Optimized deep\nneural network architecture for robust detection of epileptic seizures\nusing EEG signals,” Clinical Neurophysiology, vol. 130, no. 1, pp. 25–\n37, 2019.\n[215] A. M. Abdelhameed and M. Bayoumi, “Semi-Supervised EEG Signals\nClassiﬁcation System for Epileptic Seizure Detection,” IEEE Signal\nProcessing Letters, vol. 26, no. 12, pp. 1922–1926, 2019.\n[216] R. Akut, “Wavelet based deep learning approach for epilepsy detec-\ntion,” Health Information Science and Systems, vol. 7, 2019.\n[217] A. Emami, N. Kunii, T. Matsuo, T. Shinozaki, K. Kawai, and H. Taka-\nhashi, “Seizure detection by convolutional neural network-based analy-\nsis of scalp electroencephalography plot images,” Neuroimage-Clinical,\nvol. 22, 2019.\n[218] U. B. Baloglu and O. Yildirim, “Convolutional Long-Short Term Mem-\nory Networks Model For Long Duration EEG Signal Classiﬁcation,”\nJournal of Mechanics in Medicine and Biology, vol. 19, no. 1, 2019.\n[219] A. O’Shea, G. Lightbody, G. Boylan, and A. Temko, “Neonatal\nseizure\ndetection\nfrom\nraw\nmulti-channel\nEEG\nusing\na\nfully\nconvolutional architecture,” Neural Networks, vol. 123, pp. 12–25,\nMar.\n2020.\n[Online].\nAvailable:\nhttps://linkinghub.elsevier.com/\nretrieve/pii/S0893608019303910\n54\n[220] C. Jansen, S. Hodel, T. Penzel, M. Spott, and D. Krefting, “Feature\nrelevance in physiological networks for classiﬁcation o obstructive sleep\napnea,” Physiological Measurement, vol. 39, no. 12, 2018.\n[221] G. Klosh,\nB. Kemp,\nT. Penzel,\nA. Schlogl,\nP. Rappelsberger,\nE. Trenker, G. Gruber, J. Zeithofer, B. Saletu, W. Herrmann et al.,\n“The siesta project polygraphic and clinical database,” IEEE Engi-\nneering in Medicine and Biology Magazine, vol. 20, no. 3, pp. 51–57,\n2001.\n[222] S. Jonas, A. O. Rossetti, M. Oddo, S. Jenni, P. Favaro, and F. Zubler,\n“EEG-based outcome prediction after cardiac arrest with convolutional\nneural networks: Performance and visualization of discriminative fea-\ntures,” Human Brain Mapping, vol. 40, no. 16, pp. 4606–4617, 2019.\n[223] M. C. Tjepkema-Cloostermans, C. D. Lourenco, B. J. Ruijter, S. C.\nTromp, G. Drost, F. H. M. Kornips, A. Beishuizen, F. H. Bosch,\nJ. Hofmeijer, and M. van Putten, “Outcome Prediction in Postanoxic\nComa With Deep Learning*,” Critical Care Medicine, vol. 47, no. 10,\npp. 1424–1432, 2019.\n[224] S. U. Amin, M. S. Hossain, G. Muhammad, M. Alhussein, and M. A.\nRahman, “Cognitive Smart Healthcare for Pathology Detection and\nMonitoring,” IEEE Access, vol. 7, pp. 10 745–10 753, 2019.\n[225] G. Ruﬃni, D. Ibanez, M. Castellano, L. Dubreuil-Vall, A. Soria-Frisch,\nR. Postuma, J. F. Gagnon, and J. Montplaisir, “Deep Learning With\nEEG Spectrograms in Rapid Eye Movement Behavior Disorder,” Fron-\ntiers in Neurology, vol. 10, 2019.\n[226] A. Piryatinska, B. Darkhovsky, and A. Kaplan, “Binary classiﬁcation\nof multichannel-eeg records based on the e-complexity of continuous\nvector functions,” Computer methods and programs in biomedicine, vol.\n152, pp. 131–139, 2017.\n[227] C.-R. Phang, F. Noman, H. Hussain, C.-M. Ting, and H. Ombao,\n“A Multi-Domain Connectome Convolutional Neural Network for\nIdentifying Schizophrenia From EEG Connectivity Patterns,” IEEE\nJournal of Biomedical and Health Informatics, vol. 24, no. 5, pp.\n55\n1333–1343, May 2020. [Online]. Available: https://ieeexplore.ieee.org/\ndocument/8836535/\n[228] Z. Yin, M. Y. Zhao, Y. X. Wang, J. D. Yang, and J. H. Zhang,\n“Recognition of emotions using multimodal physiological signals and\nan ensemble deep learning model,” Computer Methods and Programs\nin Biomedicine, vol. 140, pp. 93–110, 2017.\n[229] T. Chen, S. H. Ju, X. H. Yuan, M. Elhoseny, F. J. Ren, M. Y. Fan,\nand Z. G. Chen, “Emotion recognition using empirical mode decompo-\nsition and approximation entropy,” Computers & Electrical Engineer-\ning, vol. 72, pp. 383–392, 2018.\n[230] S. K. Kim and H. B. Kang, “An analysis of smartphone overuse recog-\nnition in terms of emotions using brainwaves and deep learning,” Neu-\nrocomputing, vol. 275, pp. 1393–1406, 2018.\n[231] J. Teo, L. H. Chew, J. T. Chia, and J. Mountstephens, “Classiﬁcation\nof Aﬀective States via EEG and Deep Learning,” International Journal\nof Advanced Computer Science and Applications, vol. 9, no. 5, pp. 132–\n142, 2018.\n[232] J. X. Chen, D. M. Jiang, and N. Zhang, “A Hierarchical Bidirectional\nGRU Model With Attention for EEG-Based Emotion Classiﬁcation,”\nIEEE Access, vol. 7, pp. 118 530–118 540, 2019.\n[233] O.\nBalan,\nG.\nMoise,\nA.\nMoldoveanu,\nM.\nLeordeanu,\nand\nF. Moldoveanu, “Fear Level Classiﬁcation Based on Emotional Dimen-\nsions and Machine Learning Techniques,” Sensors, vol. 19, no. 7, 2019.\n[234] Y. Cimtay and E. Ekmekcioglu, “Investigating the Use of Pretrained\nConvolutional Neural Network on Cross-Subject and Cross-Dataset\nEEG Emotion Recognition,” Sensors, vol. 20, no. 7, p. 2034, Apr. 2020.\n[Online]. Available: https://www.mdpi.com/1424-8220/20/7/2034\n[235] Y. Kim and A. Choi, “EEG-Based Emotion Classiﬁcation Using\nLong Short-Term Memory Network with Attention Mechanism,”\nSensors, vol. 20, no. 23, p. 6727, Nov. 2020. [Online]. Available:\nhttps://www.mdpi.com/1424-8220/20/23/6727\n56\n[236] B.\nH.\nKim\nand\nS.\nJo,\n“Deep\nPhysiological\nAﬀect\nNetwork\nfor\nthe\nRecognition\nof\nHuman\nEmotions,”\nIEEE\nTransactions\non\nAﬀective\nComputing,\npp.\n1–1,\n2018.\n[Online].\nAvailable:\nhttp://ieeexplore.ieee.org/document/8249871/\n[237] J. H. Jeong, B. W. Yu, D. H. Lee, and S. W. Lee, “Classiﬁcation of\nDrowsiness Levels Based on a Deep Spatio-Temporal Convolutional\nBidirectional LSTM Network Using Electroencephalography Signals,”\nBrain Sciences, vol. 9, no. 12, 2019.\n[238] Z. Yin and J. H. Zhang, “Cross-session classiﬁcation of mental workload\nlevels using EEG and an adaptive deep learning model,” Biomedical\nSignal Processing and Control, vol. 33, pp. 30–47, 2017.\n[239] Z. C. Jiao, X. B. Gao, Y. Wang, J. Li, and H. J. Xu, “Deep Convo-\nlutional Neural Networks for mental load classiﬁcation based on EEG\ndata,” Pattern Recognition, vol. 76, pp. 582–595, 2018.\n[240] S. Yang, Z. Yin, Y. G. Wang, W. Zhang, Y. X. Wang, and J. H. Zhang,\n“Assessing cognitive mental workload via EEG signals and an ensemble\ndeep learning classiﬁer based on denoising autoencoders,” Computers\nin Biology and Medicine, vol. 109, pp. 159–170, 2019.\n[241] O. Yildirim, U. B. Baloglu, and U. R. Acharya, “A Deep Learning\nModel for Automated Sleep Stages Classiﬁcation Using PSG Signals,”\nInternational Journal of Environmental Research and Public Health,\nvol. 16, no. 4, 2019.\n[242] A. Patanaik, J. L. Ong, J. J. Gooley, S. Ancoli-Israel, and M. W. L.\nChee, “An end-to-end framework for real-time automatic sleep stage\nclassiﬁcation,” Sleep, vol. 41, no. 5, 2018.\n[243] Y. Yuan, K. B. Jia, F. L. Ma, G. X. Xun, Y. Q. Wang, L. Su, and\nA. D. Zhang, “A hybrid self-attention deep learning framework for\nmultivariate sleep stage classiﬁcation,” Bmc Bioinformatics, vol. 20,\n2019.\n[244] L. D. Zhang, D. Fabbri, R. Upender, and D. Kent, “Automated sleep\nstage scoring of the Sleep Heart Health Study using deep neural net-\nworks,” Sleep, vol. 42, no. 11, 2019.\n57\n[245] F. Chapotot and G. Becq, “Automated sleep-wake staging combining\nrobust feature extraction, artiﬁcial neural network classiﬁcation, and\nﬂexible decision rules,” International Journal of Adaptive Control and\nSignal Processing, vol. 24, no. 5, pp. 409–423, 2010.\n[246] A. Malafeev, D. Laptev, S. Bauer, X. Omlin, A. Wierzbicka, A. Wich-\nniak, W. Jernajczyk, R. Riener, J. Buhmann, and P. Achermann, “Au-\ntomatic Human Sleep Stage Scoring Using Deep Neural Networks,”\nFrontiers in Neuroscience, vol. 12, 2018.\n[247] H. Phan, F. Andreotti, N. Cooray, O. Y. Chen, and M. De Vos, “Joint\nClassiﬁcation and Prediction CNN Framework for Automatic Sleep\nStage Classiﬁcation,” IEEE Transactions on Biomedical Engineering,\nvol. 66, no. 5, pp. 1285–1296, 2019.\n[248] S. Chambon, M. N. Galtier, P. J. Arnal, G. Wainrib, and A. Gramfort,\n“A Deep Learning Architecture for Temporal Sleep Stage Classiﬁcation\nUsing Multivariate and Multimodal Time Series,” IEEE Transactions\non Neural Systems and Rehabilitation Engineering, vol. 26, no. 4, pp.\n758–769, 2018.\n[249] C.\nL.\nRosen,\nD.\nAuckley,\nR.\nBenca,\nN.\nFoldvary-Schaefer,\nC. Iber,\nV. Kapur,\nM. Rueschman,\nP. Zee,\nand S. Redline,\n“A Multisite Randomized Trial of Portable Sleep Studies and\nPositive Airway Pressure Autotitration Versus Laboratory-Based\nPolysomnography for the Diagnosis and Treatment of Obstructive\nSleep Apnea:\nThe HomePAP Study,”\nSleep,\nvol. 35,\nno. 6,\npp. 757–767, 2012,\neprint: https://academic.oup.com/sleep/article-\npdf/35/6/757/26619641/aasm.35.6.757.pdf. [Online]. Available: https:\n//doi.org/10.5665/sleep.1870\n[250] J. P. Bakker, A. Tavakkoli, M. Rueschman, W. Wang, R. Andrews,\nA. Malhotra, R. L. Owens, A. Anand, K. A. Dudley, and S. R.\nPatel, “Gastric Banding Surgery versus Continuous Positive Airway\nPressure for Obstructive Sleep Apnea:\nA Randomized Controlled\nTrial,” American Journal of Respiratory and Critical Care Medicine,\nvol. 197, no. 8, pp. 1080–1083, Apr. 2018. [Online]. Available:\nhttp://www.atsjournals.org/doi/10.1164/rccm.201708-1637LE\n58\n[251] S. Biswal, H. Q. Sun, B. Goparaju, M. B. Westover, J. M. Sun, and\nM. T. Bianchi, “Expert-level sleep scoring with deep neural networks,”\nJournal of the American Medical Informatics Association, vol. 25,\nno. 12, pp. 1643–1650, 2018.\n[252] A. Sors, S. Bonnet, S. Mirek, L. Vercueil, and J. F. Payen, “A convo-\nlutional neural network for sleep stage scoring from raw single-channel\nEEG,” Biomedical Signal Processing and Control, vol. 42, pp. 107–114,\n2018.\n[253] J. B. Blank, P. M. Cawthon, M. L. Carrion-Petersen, L. Harper, J. P.\nJohnson, E. Mitson, and R. R. Delay, “Overview of recruitment for the\nosteoporotic fractures in men study (mros),” Contemporary clinical\ntrials, vol. 26, no. 5, pp. 557–568, 2005.\n[254] S. Redline, R. Amin, D. Beebe, R. D. Chervin, S. L. Garetz, B. Gior-\ndani, C. L. Marcus, R. H. Moore, C. L. Rosen, R. Arens et al., “The\nchildhood adenotonsillectomy trial (chat): rationale, design, and chal-\nlenges of a randomized controlled trial evaluating a standard surgical\nprocedure in a pediatric population,” Sleep, vol. 34, no. 11, pp. 1509–\n1517, 2011.\n[255] M. H. AlMeer, H. Hassen, and N. Nawaz, “ROM-based Inference\nMethod Built on Deep Learning for Sleep Stage Classiﬁcation,” Tem\nJournal-Technology Education Management Informatics, vol. 8, no. 1,\npp. 28–40, 2019.\n[256] W. Qu, Z. Wang, H. Hong, Z. Chi, D. D. Feng, R. Grunstein,\nand C. Gordon, “A Residual Based Attention Model for EEG Based\nSleep Staging,” IEEE Journal of Biomedical and Health Informatics,\nvol. 24,\nno. 10,\npp. 2833–2843,\nOct. 2020. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/9022981/\n[257] S. Charnbon, V. Thorey, P. J. Arnal, E. Mignot, and A. Gramfort,\n“DOSED: A deep learning approach to detect multiple sleep micro-\nevents in EEG signal,” Journal of Neuroscience Methods, vol. 321, pp.\n64–78, 2019.\n[258] O. Andlauer, H. Moore, L. Jouhier, C. Drake, P. E. Peppard,\nF. Han, S.-C. Hong, F. Poli, G. Plazzi, R. O’Hara et al., “Nocturnal\n59\nrapid eye movement sleep latency for identifying patients with nar-\ncolepsy/hypocretin deﬁciency,” JAMA neurology, vol. 70, no. 7, pp.\n891–902, 2013.\n[259] T. Young, L. Finn, P. E. Peppard, M. Szklo-Coxe, D. Austin, F. J.\nNieto, R. Stubbs, and K. M. Hla, “Sleep disordered breathing and\nmortality: eighteen-year follow-up of the wisconsin sleep cohort,” Sleep,\nvol. 31, no. 8, pp. 1071–1078, 2008.\n[260] B. Kaur, D. Singh, and P. P. Roy, “Age and gender classiﬁcation us-\ning brain–computer interface,” Neural Computing and Applications,\nvol. 31, no. 10, pp. 5887–5900, 2019.\n[261] D. F. Wulsin, J. R. Gupta, R. Mani, J. A. Blanco, and B. Litt, “Model-\ning electroencephalography waveforms with semi-supervised deep belief\nnets: fast classiﬁcation and anomaly measurement,” Journal of Neural\nEngineering, vol. 8, no. 3, 2011.\n[262] J. Anem, G. S. Kumar, and R. Madhu, “Cat Swarm Fractional Cal-\nculus optimization-based deep learning for artifact removal from EEG\nsignal,” Journal of Experimental & Theoretical Artiﬁcial Intelligence.\n[263] S. Jacob, V. G. Menon, F. Al-Turjman, P. G. Vinoj, and L. Mostarda,\n“Artiﬁcial Muscle Intelligence System With Deep Learning for Post-\nStroke Assistance and Rehabilitation,” IEEE Access, vol. 7, pp.\n133 463–133 473, 2019.\n[264] B. H. Yang, K. W. Duan, C. C. Fan, C. X. Hu, and J. L. Wang, “Auto-\nmatic ocular artifacts removal in EEG using deep learning,” Biomedical\nSignal Processing and Control, vol. 43, pp. 148–158, 2018.\n[265] C. Spampinato, S. Palazzo, I. Kavasidis, D. Giordano, N. Souly, and\nM. Shah, “Deep learning human mind for automated visual classiﬁ-\ncation,” Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 6809–6817, 2017.\n[266] P. Croce, F. Zappasodi, L. Marzetti, A. Merla, V. Pizzella, and\nA. M. Chiarelli, “Deep Convolutional Neural Networks for Feature-\nLess Automatic Classiﬁcation of Independent Components in Multi-\nChannel Electrophysiological Brain Recordings,” IEEE Transactions\non Biomedical Engineering, vol. 66, no. 8, pp. 2372–2380, 2019.\n60\n[267] Y. R. Ming, W. P. Ding, D. Pelusi, D. R. Wu, Y. K. Wang, M. Prasad,\nand C. T. Lin, “Subject adaptation network for EEG data analysis,”\nApplied Soft Computing, vol. 84, 2019.\n[268] P. Nagabushanam, S. T. George, and S. Radha, “EEG signal classi-\nﬁcation using LSTM and improved neural network algorithms,” Soft\nComputing.\n[269] A. Fares, S. H. Zhong, and J. M. Jiang, “EEG-based image classiﬁca-\ntion via a region-level stacked bi-directional deep learning framework,”\nBmc Medical Informatics and Decision Making, vol. 19, 2019.\n[270] H. Akbari, B. Khalighinejad, J. L. Herrero, A. D. Mehta, and N. Mes-\ngarani, “Towards reconstructing intelligible speech from the human\nauditory cortex,” Scientiﬁc Reports, vol. 9, 2019.\n[271] A. Antoniades, L. Spyrou, D. Martin-Lopez, A. Valentin, G. Alarcon,\nS. Sanei, and C. C. Took, “Deep Neural Architectures for Mapping\nScalp to Intracranial EEG,” International Journal of Neural Systems,\nvol. 28, no. 8, 2018.\n[272] J. J. Bird, D. R. Faria, L. J. Manso, A. Ekart, and C. D. Buckingham,\n“A Deep Evolutionary Approach to Bioinspired Classiﬁer Optimisation\nfor Brain-Machine Interaction,” Complexity, vol. 2019, 2019.\n[273] V. Singhal, A. Majumdar, and R. K. Ward, “Semi-Supervised Deep\nBlind Compressed Sensing for Analysis and Reconstruction of Biomed-\nical Signals From Compressive Measurements,” IEEE Access, vol. 6,\npp. 545–553, 2018.\n[274] A. Gogna, A. Majumdar, and R. Ward, “Semi-supervised Stacked\nLabel Consistent Autoencoder for Reconstruction and Analysis of\nBiomedical Signals,” IEEE Transactions on Biomedical Engineering,\nvol. 64, no. 9, pp. 2196–2205, 2017.\n[275] H. J. Jang and K. O. Cho, “Dual deep neural network-based classi-\nﬁers to detect experimental seizures,” Korean Journal of Physiology &\nPharmacology, vol. 23, no. 2, pp. 131–139, 2019.\n61\n[276] T. Ogawa, Y. Sasaka, K. Maeda, and M. Haseyama, “Favorite Video\nClassiﬁcation Based on Multimodal Bidirectional LSTM,” IEEE Ac-\ncess, vol. 6, pp. 61 401–61 409, 2018.\n[277] A. Ben Said, M. F. Al-Sa’d, M. Tlili, A. A. Abdellatif, A. Mohamed,\nT. Elfouly, K. Harras, and M. D. O’Connor, “A Deep Learning Ap-\nproach for Vital Signs Compression and Energy Eﬃcient Delivery in\nmhealth Systems,” IEEE Access, vol. 6, pp. 33 727–33 739, 2018.\n62\nTable 1: The Abbreviations in This Survey\nAbbreviation\nFull Name\nAD\nAlzheimer’s Disease\nADHD\nAttention Deﬁcit Hyperactivity Disorder\nAE\nAutoencoder\nBCI\nBrain-Computer Interface\nCAM-ICU\nConfusion Assessment Method for the ICU\nCapsNet\nCapsule Network\nCJD\nCreutzfeldt-Jakob Disease\nCNN\nConvolutional Neural Network\nDBCS\nDeep Blind Compresed Sensing\nDBN\nDeep Belief Network\nDMCCA\nDeep Multiset Canonical Correlation Analysis\nDN-AE-NTM\nDeep Network Autoencoder Neural Turing Machine\nDPN\nDeep Polynomial Network\nDTI\nDiﬀusion Tensor Imaging\nDWT\nDiscrete Wavelet Transformation\nEEG\nElectroencephalogram\nELM\nExtreme Learning Machine\nEOG\nElectrooculogram\nESN\nEcho State Network\nfMRI\nfunctional Magnetic Resonance Imaging\nfNIRS\nfunctional Near-Infrared Spectroscopy\nGPED\nGeneralized Periodic Epileptiform Discharge\nGRU\nGated Recurrent Unit\nHC\nHealthy Controls\nIED\nIterictal Epileptiform Discharge\nkNN\nk-Nearest Neighbor\nLSTM\nLong Short-Term Memory\nMCI\nMild Cognitive Impairment\nMI\nMotor Imagery\nMLP\nMultilayer Perceptron\nNREM\nNon-Rapid Eye Movement\nOFS\nOperator Functional States\nPCA\nPrincipal Component Analysis\nPLED\nPeriodic Lateralized Epileptiform Discharge\nRASS\nRichmond Agitation-Sedation Scale\nRBM\nRestricted Boltzmann Machine\nREM\nRapid Eye Movement\nRNN\nRecurrent Neural Network\nRPD\nRapidly Progressive Dementia\nRSVP\nRapid Serial Visual Presentation\nSAE\nStacked Autoencoder\nSAN\nSubject Adaption Network\nSNN\nSpiking Neural Network\n63\nTable 2: Typical Methods for Artifacts Removal\nMethods\nTarget Artifacts\nProperty\nNotch Filter\nLine Noise\nSignal distortion in speciﬁc frequencies\nBand-Pass Filter\nArtifacts concentrated on a particular fre-\nquency band\nPreclude certain frequency signals\nIndependent Component Analysis\nOcular and muscular noise removal\nDecompose channels into independent components\nReject Contaminated Data Segments\nOcular noise, muscular noise etc., which are\ndiﬃcultly mitigated\nReject gross eye movement and occasional recording artifacts\nWavelet Transformation Analysis\nOcular and muscular noise removal\nSignals are reconstructed based on the corrected coeﬃcient\nCommon Average Reference\nArtifacts equivalently aﬀect all channels\nAmplitudes can be overall reduced\nZ-Score Calculation\nNoisy channels or time periods\nGenerates zero-mean data with unitary variance\nDenoise AutoEncoder\nGeneral Noises\nDenoise in an unsupervised manner\n64\nTable 3: Key Information of Papers about Brain-Computer Interface\nAuthors\nModels\nParadigms\nClasses\nData (Private/Public: No. of Participants, No. of Channels, Sampl\nRate)\nMa et al. 2020 [151]\nCNN\nMI\nRest, Right Hand, and Right Elbow\nPrivate: 25 Participants, 64 Channels, 1000 Hz\nZhang et al. 2019 [50]\nCNN\nMI\nLeft, Right Hand\nBCI Competition II Dataset III\nXu et al. 2019 [48]\nCNN\nMI\nLeft, Right Hand\nBCI Competition IV Dataset 2b\nZhu et al. 2019 [152]\nCNN\nMI\nLeft, Right Hand\n1. Private: 25 Participants, 15 Channels, 1000 Hz\n2. BCI Competition IV Dataset 2b\nLu et al. 2017 [44]\nDBM\nMI\nLeft, Right Hand\nBCI Competition IV Dataset 2b\nChiarelli et al. 2018 [153]\nDNN\nMI\nLeft, Right Hand\nPrivate: 15 Participants, 128 Channels, 250 Hz\nTayeb et al. 2019 [154]\nCNN,\nLSTM,\nCNN+LSTM\nMI\nLeft, Right Hand\n1. Private: 20 Participants, 32 Channels, 256 Hz\n2. BCI Competition IV Dataset 2b\nDai et al. 2019 [155]\nCNN+AE\nMI\nLeft, Right Hand\nBCI Competition IV Dataset 2b\nHa et al. 2019 [156]\nCapsNet\nMI\nLeft, Right Hand\nBCI Competition IV Dataset 2b\nShi et al. 2019 [157]\nCNN\nMI\nLeft, Right Hand\nPrivate: - Participants, 118 Channels, - Hz\nWang et al. 2018 [158]\nCNN, LSTM\nMI\nLeft, Right Hand\nPrivate: 14 Participants, 11 Channels, 256 Hz\n65\nTabar et al. 2017 [57]\nCNN,\nSAE,\nCNN+SAE\nMI\nLeft, Right Hand\n1. BCI Competition II Dataset III\n2. BCI Competition IV Dataset 2b\nAmin et al. 2019 [159]\nCNN\nMI\nLeft Hand, Right Hand, Feet, and Tongue\n1. High Gamma Dataset [160]\n2. BCI Competition IV Dataset 2a\nAmin et al. 2019 [58]\nCNN, MLP, AE\nMI\nLeft Hand, Right Hand, Feet, and Tongue\n1. BCI Competition IV Dataset 2a\n2. High Gamma Dataset [160]\nLi et al. 2019 [51]\nCNN\nMI\nLeft Hand, Right Hand, Feet, and Tongue\nBCI Competition IV Dataset 2a\nHassanpour et al.\n2019\n[161]\nDBN, SAE\nMI\nLeft Hand, Right Hand, Feet, and Tongue\nBCI Competition IV Dataset 2a\nZhang et al. 2019 [59]\nCNN+LSTM\nMI\nLeft Hand, Right Hand, Feet, and Tongue\nBCI Competition IV Dataset 2a\nShe et al. 2018 [162]\nELM\nMI\nLeft Hand, Right Hand, Feet, and Tongue\nBCI Competition IV Dataset 2a\nUribe et al. 2019 [163]\nELM\nMI\nLeft Hand, Right Hand, Feet, and Tongue\nBCI Competition IV Dataset 2a\nLei et al. 2019 [42]\nMMDPN\nMI\nIdle, Preparation, Walking Imagery, and Restoration\nPrivate: 9 Participants, 32 Channels, 512 Hz\nDuan et al. 2017 [164]\nELM\nMI\nCortical Positivity and Negativity\nBCI Competition II Dataset Ia\nAlazrai et al. 2019 [52]\nCNN\nMI\nRest, Grasp-Related (Small Diameter, Lateral, and Extension-\nType),\nWrist-Related\n(Ulnar/Radial\nDeviation.\nFlex-\nion/Extension),\nFingers-Related\n(\nFlexion\nand\nExtension\nof The Index, The Middle, The Ring, The Little, and The\nThumb Finger)\nPrivate: 22 Participants (18 Able-Bodied and 4 with Transradial A\nputations), 16 Channels, 2048 Hz\n66\nHang et al. 2019 [45]\nCNN\nMI\n1. Right Hand, Foot\n2. Left Hand, Right Hand, Feet, and Tongue\n1. BCI Competition III Dataset IVa\n2. BCI Competition IV Dataset IIa\nYang et al. 2018 [60]\nCNN+LSTM\nMI\n1. Left Hand, Right Foot\n2. Left, Right Hand\n3. Left Hand, Tongue\n1. Private: 6 Participants, 64 Channels, 500 Hz\n2. BCI Competition III Dataset -\n3. BCI Competition IV Dataset -\nZhao et al. 2019 [46]\nCNN\nMI\n1. Left Hand, Right Hand, Feet, and Tongue\n2. Left, Right Hand\n3. Elbow Flexion/Extension, Forearm Supination/Pronation, Hand Open/Close\n1. BCI Compeition IV Dataset 2a\n2. BCI Compeition IV Dataset 2b\n3. From Ofner et al., 15 Participants, 61 Channels, 512 Hz\nWu et al. 2019 [165]\nCNN\nMI\n1. Left Hand, Right Hand, Feet, and Tongue\n2. Left, Right Hand\n1. BCI Competition IV Dataset 2a\n2. BCI Competition IV Dataset 2b\n3. High Gamma Dataset [160]\nMajidov et al. 2019 [166]\nCNN\nMI\n1. Left Hand, Right Hand, Feet, and Tongue\n2. Left, Right Hand\n1. BCI Competition IV Dataset 2a\n2. BCI Competition IV Dataset 2b\nLi et al. 2019 [49]\nCNN\nMI\n1. Left Hand, Right Hand, Feet, and Tongue\n2. Left Hand, Right Hand, Feet, and Rest\n1. BCI Competition IV Dataset 2a\n2. High Gamma Dataset [160]\nDose et al. 2018 [47]\nCNN\nMI\nLeft/Right Fist or Both Fists/Both Feet\nEEG Motor Movement/MI Dataset\nTang et al. 2019 [167]\nDBN\nMI\nLeft, Right Hand\nPrivate: 7 Participants, 14 Channels, 128 Hz\nXu et al. 2018 [168]\nCNN\nMI\n1. Left, Right Hand\n2. Left Hand, Right Hand, Feet, and Tongue\n1. BCI Competition II Dataset III\n2. BCI Competition IV Dataset 2a\nKwon et al. 2020 [169]\nCNN\nMI\nLeft and Right Hnad\nPrivate: 54 Participants, 62 Channels, 1000 Hz\n67\nMammone et al. 2020 [170]\nCNN\nMI\nElbow Flexion/Extension, Forearm Supination/Pronation, Hand\nOpen/Close, Resting\nBNCI Horizon Dataset\nZhang et al. 2020 [171]\nCNN+LSTM\nMI\n1. Left/Right Fist Open and Close\n2. Left hand, right hand, feet, and tongue\n1. PhysioNet Dataset\n2. BCI Competition IV Dataset 2a\nChen et al. 2020 [172]\nCNN\nMI\n1. Left hand, right hand, feet, and tongue\n2. Right hand and feet\n1. BCI Competition IV Dataset 2a\n2. SMR-BCI Dataset\nJeong et al. 2020 [173]\nCNN+LSTM\nReaching\nMovements\nand MI\nLeft, Right, Forward, Backward, Up, and Down\nPrivate: 15 Participants, 64 Channels, 1000 Hz\nDing et al. 2015 [39]\nELM\n-\nCortical Positivity and Negativity\nBCI Competition II Dataset Ia\nMa et al. 2017 [61]\nDBN\nmVEP\nTarget Stimulus Signal and The Standard Stimulus Signal\nPrivate: 11 Participants, 10 Channels, 1000 Hz\nGao et al. 2015 [174]\nANN\nP300\nP300 and Non-P300\nPrivate: 5 Participants, 32 Channels, 2048 Hz\nKundu et al 2019 [56]\nSAE\nP300\nP300 and Non-P300\n1. BCI Competition II Dataset IIb\n2. BCI Competition III Dataset II\n3. BNCI Horizon Dataset\nKshiragar et al. 2019 [53]\nSAE, CNN\nP300\nP300 and Non-P300\nPrivate: 10 Participants, 16 Channels, 500 Hz\nLiu et al. 2018 [8]\nCNN\nP300\nP300 and Non-P300\n1. BCI Competition III Dataset II\n2. BCI Competition II Dataset IIb\nFarahat et al. 2019 [175]\nCNN\nP300\nP300 and Non-P300\nPrivate: 19 Participants, 29 Channels, 508.63 Hz\nSolon et al. 2019 [176]\nCNN\nP300\nP300 and Non-P300\nPrivate: 67 Participants, 64 Channels, - Hz\n68\nVareka et al. 2017 [177]\nSAE\nP300\nP300 and Non-P300\nPrivate: 25 Participants, 19 Channels, 1000 Hz\nMorabbi et al. 2018 [178]\nDBN\nP300\nP300 and Non-P300\nEPFL BCI Dataset\nDitthapron\net\nal.\n2019\n[179]\nCNN+LSTM+AE\nP300\nP300 and Non-P300\n1. From Citi et al. [180], 12 Participants, 64 Channels, 2048 Hz\n2. BCI Competition III Dataset II\n3. From Schreuder et al. [181], 10 Participants, 60 Channels, 240 H\n4. From Acqualagna et al. [182], 13 Participants, 63 Channels, 250\n5. EEG Database Data Set/UCI EEG Dataset\n6. From Treder et al. [183], 11 Participants, 63 Channels, 200 Hz\nLawhern et al. 2018 [54]\nCNN\nP300, MI, etc.\n1. P300 and Non-P300\n2. Correct and Incorrect\n3. The Left Index, Left Middle, Right Index, and Right Middle Finger\n4. Left Hand, Right Hand, Feet, and Tongue\n1. Private: 15 Participants, 64 Channels, 512 Hz\n2. BCI Challenge\n3. Private: 13 Participants, 256 Channels, 1024 Hz\n4. BCI Competition IV Dataset 2a\nBoloukian et al. 2020 [184]\nDN-AE-NTM\nP300, MI, etc.\n1. P300 and Non-P300\n2. Alcoholic and Control\n3. Left/Right Fist or Both Fists/Both Feet\n1. From Hoﬀmann et al. [185], 9 Participants (5 with disablement a\n2. EEG Database Data Set/UCI EEG Dataset\n3. EEG Motor Movement/Imagery Dataset\nPei et al. 2018 [55]\nSAE\nReaching\nMovements\nLeft, Central and Right\nPrivate: 5 Participants, 32 Channels, 256 Hz\nChen et al. 2019 [186]\nCNN\nRSVP\nTarget and Non-Target\nFrom Touryan et al. [187], 10 Participants, 64 Channels, 512Hz\nManor et al. 2015 [188]\nCNN\nRSVP\nTarget and Non-Target\nPrivate: 15 Participants, 64 Channels, 256 Hz\nManor et al. 2016 [189]\nCNN\nRSVP\nTarget and Non-Target\nPrivate: 15 Participants, 64 Channels, 256 Hz\n69\nNguyen et al. 2019 [62]\nCNN\nSSVEP\n6.67, 7.5, 8.57, 10, and 12 Hz\nPrivate: 8 Participants, 1 Channel, 128 Hz\nLiu et al. 2020 [190]\nDMCCA\nSSVEP\n6, 8, 9, and 10 Hz\nPrivate: 10 Participants, 8 Channels, 250 Hz\nWaytowich et al. 2018 [63]\nCNN\nSSVEP\n12 SSVEP Stimuli Flashed at Frequencies Ranging from 9.25 Hz\nTo 14.75 Hz in Steps of 0.5 Hz\nFrom Nakanishi et al. [191], - Participants, - Channel, 2048 Hz\n’-’ indicates that the information is unavailable\n70\nTable 4: Key Information of Papers about Disease Detection\nAuthor\nModels\nCategories\nClasses\nData (Private/Public: No. of Participants, No. of Channels, Sampling R\nDoborjeh et al. 2016 [41]\nSNN\nAddiction\nHealthy,\nAddiction\nTreated,\nand\nAddiction\nNot\nTreated Subjects\nPrivate: 74 Participants, 26 Channels, - Hz\nIeracitano et al. 2019 [75]\nCNN\nAlzheimer’s Disease\n1. AD vs. HC, AD vs. MCI, MCI vs. HC\n2. AD, MCI, and HC\nPrivate: 189 Participants (63 AD, 63 MCI, 63 HC), 19 Channels, 1024 H\nBi et al. 2019 [192]\nDBN\nAlzheimer’s Disease\n1. AD, HC, and MCI\n2. Identiﬁcation: determine EEG spectral image come from which person\n3. Veriﬁcation: wheather two EEG spectral images come from the same person\nPrivate: 12 Participants (4 HC, 4 MCI, and 4 AD), 64 Channels, 500 H\nMorabito et al. 2016 [193]\nSAE, MLP\nAlzheimer’s Disease\nCJD/RPD, CJD/HC, and CJD/AD\nPrivate: 76 Participants, 19 Channels, - Hz\nHayase et al. 2019 [194]\nMLP\nAnaesthesia\n-\nPrivate: 30 Participants, - Channels, 128 hZ\nLiu et al. 2019 [195]\nCNN\nAnaesthesia\nAnesthetic Ok, Deep, and Light\nPrivate: 50 Participants, - Channel, - Hz\nPark et al. 2020 [196]\nCNN\nAnesthesia\n-\nVitalDB\nKim et al. 2018 [197]\nCNN,\nLSTM,\nDNN\nBrain Disease\n1. Normal and Dementia\n2. Normal and Alcoholism\nEEG Database Data Set/UCI EEG Dataset\nChen et al. 2019 [73]\nCNN\nChildren with ADHD\nAdhd and Controls\nPrivate: 107 Participants (50 Children with ADHD and 57 Controls), 12\nnels, 1000 Hz\n71\nChen et al. 2019 [198]\nCNN\nChildren with ADHD\nAdhd and Controls\nPrivate: 107 Participants (50 Children with ADHD and 57 Controls), 62\nnels, 1000 Hz\nBoshra et al. 2019 [199]\nCNN\nConcussion\nNormal and Concussion\nPrivate: 54 Participants (26 with Concussion and 28 Controls), 64 Ch\n512 Hz\nSun et al. 2019 [200]\nCNN+LSTM\nConsciousness\nand\nDelir-\nium Tracking\n1. Rass: -5, -4, -3, -2, -1, 0\n2. Cam-Icu: 0, 1\nPrivate: 295 Participants (174 for RASS and 121 for CAM-ICU), 4 Ch\n250 Hz\nAy et al. 2019 [201]\nCNN+LSTM\nDepression\nNormal and Depression\nFrom Acharya et al.\n[202], 30 Participants (15 Depressed and 15 Nor\nChannel (FP1-T3, FP2-T4), 256 Hz\nAcharya et al. 2018 [203]\nCNN\nDepression\nDepression and Normal\nPrivate: 30 Participants (15 Deoressed and 15 Normal), FP1-T3 and\nChannel, 256 Hz\nLi et al. 2019 [204]\nCNN\nDepression\nDepression and Normal\nPrivate: 28 Participants (14 Deoressed and 14 Normal), 16 Channels, 25\nMumtaz et al. 2019 [71]\nCNN,\nCNN+LSTM\nDepression\nDepression and Normal\nPrivate: 63 Participants (33 Deoressed and 30 Normal)\nZhu et al. 2019 [205]\nMDAE\nDepression\nMild Depression and Normal\nPrivate: 51 Participants (24 Mild Deoression and 27 Normal), 16 Chann\nHz\nBouallegue\net\nal.\n2020\n[206]\nRNN+CNN\nAutism and Epilepsy\n1. Normal and Autistic\n2. Normal and Seizure\n1. Private: 19 Participants (10 normal and 9 autistic), 16 Channels, 256\n2. CHB-MIT Scalp EEG database\n3. From Andrzejak et al.[207], 10 participants (5h healthy and 5 epilept\n72\nCao et al. 2020 [66]\nCNN+ELM\nEpilepsy\n1. Seizure/Non-Seizure\n2. Interictal, Preictal, Ictal\n3. Interictal, Three Preictal States, Ictal\n1. CHB-MIT Scalp EEG database\n2. Private: 10 Participants, 18 Channels, 256 Hz\nDaoud et al. 2020 [70]\nCNN+AE+MLP\nEpilepsy\nFocal and Non-Focal\n1. From Andrzenak et al.[208], 5 epileptic patients\n2. From Andrzejak et al.[207], 10 participants (5h healthy and 5 epilept\nTsiouris et al. 2018 [20]\nLSTM\nEpilepsy\nPreictal and Interictal\nCHB-MIT Scalp EEG Database\nYuan et al. 2019 [74]\nAE\nEpilepsy\nIctal and Non-Ictal\nCHB-MIT Scalp EEG Database\nKarim et al. 2019 [209]\nSAE\nEpilepsy\nHealthy and Epileptic Activiy\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nUllah et al. 2018 [82]\nCNN\nEpilepsy\n1. Seizure, and Non-Seizure\n2. Normal, Interical, and Ictal\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nSan-Segundo et al.\n2019\n[81]\nCNN\nEpilepsy\n1. Focal and Non-Focal\n2. Healthy/Ictal, Ictal/Non-Ictal, Healthy/\nNon-Focal/Ictal, and Healthy/Focal/Ictal\n1. The Bern-Barcelona EEG Database\n2. Epileptic Seizure Recognition Data Set\nWen et al. 2018 [68]\nCNN+AE\nEpilepsy\n1. Health With Eyes Open/Closed (A, B),\nInterictal (C, D), and Ictal (E)\n2. Epileptic Seizure and Non-Epileptic Seizure\n1. From Andrzejak et al.[207], 10 Participants (5 Healthy and 5 Epilept\n2. CHB-MIT Scalp Database\nAcharya et al. 2018 [64]\nCNN\nEpilepsy\nNoraml, Preictal, and Seizure\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nQiu et al. 2018 [72]\nSAE\nEpilepsy\nNormal, Interictal, and Ictal\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\n73\nTurk et al. 2019 [78]\nCNN\nEpilepsy\n1. A and B\n2. A, B, and E\n3. A, C, D, and E\n4. A, B, C, D, and E\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nThara et al. 2019 [210]\nLSTM\nEpilepsy\n1. Seizure and Non-Seizure\n2. Preictal, Interictal, and Ictal\nFrom Bonn University, 500 Participants (missing detial)\nSayeed et al. 2019 [211]\nDNN\nEpilepsy\n1. Normal and Ictal\n2. Normal. Interictal, and Ictal\nFrom Andrzejak et al.[207], 10 Participants (5 Healthy and 5 Epileptic P\nHosseini et al. 2017 [27]\nCNN, SAE\nEpilepsy\nInterictal, and Preictal\n1. Private: 9 Participants, 70 Channels, 1000 Hz\n2. From Upenn and the Mayo Clinic [212] [213], 2 Participants, 15 Chan\nHussein et al. 2019 [214]\nLSTM\nEpilepsy\n1. Normal and Seizure\n2. Normal, Inter-Ictal, and Ictal\n3. Health With Eyes Open/Closed (A, B), Interictal (C, D), and Ictal (E)\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nAbdelhameed et al.\n2019\n[215]\nCNN+AE\nEpilepsy\n1. Normal and Ictal\n2. Normal. Interictal, and Ictal\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nHe et al. 2019 [7]\nCNN\nEpilepsy\nFive Classes: Health With Eyes Open/Closed (A, B),\nInterictal (C, D), and Ictal (E)\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nCao et al. 2019 [17]\nCNN+LSTM\nEpilepsy\nIic Patterns and Others\nFrom MGH, over 2500 Participants, 20 Channels, - Hz\n74\nAkut 2019 [216]\nCNN\nEpilepsy\n1. Normal and Ictal\n2. Normal. Interictal, and Ictal\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nEmami et al. 2019 [217]\nCNN\nEpilepsy\nSeizure and Non-Seizure\n1. Private: 8 Participants, 19 Channels, 1000 Hz\n2. Private: 16 Participants, 19 Channels, 500 Hz\nDaoud et al. 2019 [69]\nMLP,\nCNN,\nLSTM, SAE\nEpilepsy\nInterictal and Preictal\nCHB-MIT Scalp EEG Database\nTian et al. 2019 [80]\nCNN\nEpilepsy\nSeizure and Non-Seizure\nCHB-MIT Scalp EEG Database\nWei et al. 2018 [79]\nCNN\nEpilepsy\nInterictal, Preictal, and Ictal\nPrivate: 13 Participants, 22 Channels, 500 Hz\nAntoniades et al. 2017 [67]\nCNN\nEpilepsy\nIED and Non-IED\nPrivate: 18 Participants, 20 Channels, 200 Hz\nBaloglu et al. 2019 [218]\nCNN+LSTM\nEpilepsy\nNormal/Ictal,\nInterictal/Ictal,\nNormal/Epilepsy,\nNonictal/Ictal, Normal/Interictal/Ictal\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic P\nOshea et al. 2019 [219]\nCNN\nEpilepsy\nSeizure and Non-Seizure\n1. Private: 18 Participants, 8 Channels, 256 Hz\n2. Helsinki Dataset\nVrbancic et al. 2018 [77]\nCNN\nMotor Impairment Neural\nDisorders\nNormal and Motor Impairments\nCSU BCI collection\nJansen et al. 2018 [220]\nANN\nObstructive Sleep Apnea\nOSA Patients and Controls\nFrom Klosch et al.\n[221], 247 Participants (50 Patients and 197 Cont\nChannels, - Hz\nJonas et al. 2019 [222]\nCNN\nOutcome Prediction after\nCardiac Arrest\nFavorable and Unfavorable Outcome\nPrivate: 267 Participants, 19 Channels, 250 Hz\n75\nHofmejer et al. 2018 [223]\nCNN\nOutcome\nPrediction\nin\nPostanoxic Coma\nGood and Poor\nPrivate: 456 Participants, - Channels, - Hz\nAmin et al. 2019 [224]\nCNN\nPathology\nNormal and Pathology\nTUH Abnormal EEG Dataset\nRuﬃni et al. 2019 [225]\nCNN\nREM\nBehavior\nDisorder\n(RBD)\n1. HC and Parkinson’S Disease (PD)\n2. HC+ RBD Vs. PD+Dementia with Lewy Bodies(DLB)\nPrivate: 206 Participants (121 with Idiopathic RBD), 14 Channels, 256\nNaira et al. 2019 [76]\nCNN\nSchizophrenia\nNormal and Schizophrenia\nFrom Piryatinska et al.\n[226],\n84 Participants (39 Healthy and 4\nSchizophrenia), 16 Channels, 128 Hz\nOh et al. 2019 [65]\nCNN\nSchizophrenia\nNormal and Schizophrenia\nPrivate: 28 Participants (14 with Schizophrenia and 14 Normal), 19 Ch\n250 Hz\nPhang et al. 2020 [227]\nCNN\nSchizophrenia\nNormal and schizophrenia\nLomonosov Moscow State University Dataset\n76\nTable 5: Key Information of Papers about Emotion Recognition\nAuthors\nModels\nClasses\nData (Private/Public: No.\nof Participants,\nNo. of Channels, Sampling Rate)\nJirayucharoensak et al. 2014 [104]\nSAE\nHappy, Pleased, Relaxed, Excited, Neutral, Calm, Distressed,\nMiserable, and Depressed\nDEAP Dataset\nZheng et al. 2014 [14]\nDBN\nPositive and Negative\nPrivate:\n6 Participants, 62 Channels, 1000\nHz\nAl-Nafjan et al. 2017 [85]\nDNN\nExcitement, Meditation, Boredom, and Frustration\nDEAP Dataset\nAlhagry et al. 2017 [95]\nLSTM\nHigh/Low Arousal, High/Low Valence, High/Low Liking\nDEAP Dataset\nLi et al. 2017 [98]\nCNN+LSTM\nHigh/Low Valence, High/Low Arousal\nDEAP Dataset\nYin et al. 2017 [228]\nSAE\nHigh/Low Valence, High/Low Arousal\nDEAP Dataset\nBozhkov et al. 2017 [40]\nESN\nPositive and Negative\nPrivate:\n26\nParticipants,\n21\nChannels,\n1000Hz\nZheng et al. 2017 [89]\nELM\n1. High/Low Valence, High/Low Arousal\n2. Positive, Neutral, and Negative\n1. DEAP Dataset\n2. SEED Dataset\nYang et al. 2018 [90]\nHierarchical Network\nPositive, Neutral, and Negative\nSEED Dataset\n77\nChen et al. 2018 [229]\nDBN\nHappy, Calm, Sad, and Fear\nPrivate:\n10\nParticipants,\n16\nChannels,\n128Hz\nHemanth et al. 2018 [103]\nDNN\nHappy, Sad, Relaxed, and Angry\nDEAP Dataset\nChoi et al. 2018 [94]\nLSTM\nHigh/Low Valence, High/Low Arousal\nDEAP Dataset\nKwon et al. 2018 [86]\nCNN\nHigh/Low Valence, High/Low Arousal\nDEAP Dataset\nBagherzadeh et al. 2018 [96]\nSAE\nHigh/Low Valence, High/Low Arousal\nDEAP Dataset\nChao et al. 2018 [97]\nDBN, RBM\nPleasant, Unpleasant, Aroused, and Relaxed\nDEAP Dataset\nLi et al. 2018 [91]\nCNN\nPositive, Neutral, and Negative\nSEED Dataset\nKim et al. 2018 [230]\nDBN\nRelaxed, Fear, Joy and Sad\nPrivate:\n25\nParticipants,\n64\nChannels,\n1000Hz\nTeo et al. 2018 [231]\nDNN\n1. Like and Dislike\n2. Rest and Excited\nPrivate: 16 Participants, 9 Channels, - Hz\nZheng et al. 2019 [84]\nRBM, AE\nHappy, Sad, Fear, and Neutral\nSEED-IV Dataset\nChao et al. 2019 [15]\nCapsNet\nHigh/Low Arousal, High/Low Valence, High/Low Dominance\nDEAP Dataset\nChen et al. 2019 [232]\nGRU\nHigh/Low Valence, High/Low Arousal\nDEAP Dataset\nBalan et al. 2019 [233]\nDNN\nNo, Low, Medium, and High Fear\nDEAP Dataset\nZhang et al. 2019 [83]\nRNN\nPositive, Neutral, and Negtive\nSEED Dataset\nZeng et al. 2019 [92]\nCNN\nPositive, Neutral, and Negtive\nSEED Dataset\nGao et al. 2020 [105]\nCNN\nHappy, Sad, and Fear\nPrivate: 15 Participants, 30 Channels, 1000\nHz\n78\nSerap Aydin 2020 [101]\nLSTM\nFear, Anger, Happiness, Sadness, Amusement, Surprise, Ex-\ncitement, Calmness, Disgust\nPrivate:\n23 Participants, 16 Channels, 128\nHz\nCimtay et al. 2020 [234]\nCNN\n1. Positive and Negative\n2. Positive, Neutral, and Negative\n1. SEED Dataset\n2. DEAP Dataset\n3. LUMED Dataset\nKim et al. 2020 [235]\nCNN+LSTM, LSTM\n1. Low and High\n2. Low, Medium, and High\nDEAP Dataset\nKim et al. 2020 [236]\nCNN+LSTM\nHigh/Low Valence, High/Low Arousal\nDEAP Dataset\nZhu et al. 2020 [102]\nCNN\nAnger, Disgust, Neutral, and Happy\nPrivate: 40 Participants, 62 Channels, 1000\nHz\n79\nTable 6: Key Information of Papers about Operator Functional States\nAuthors\nModels\nCategories\nClasses\nData (Private/Public:\nNo.\nof Participants, No.\nof\nChannels, Sampling Rate)\nChai et al. 2017 [110]\nDBN\nFatigue\nAlert and Fatigue\nPrivate: 43 Participants, 32 Channels, 2048 Hz\nZeng et al. 2018 [108]\nCNN\nFatigue\nSober and Fatiuge\nPrivate: 10 Participants, 16 Channels, 256 Hz\nYin et al. 2018 [106]\nELM\nFatigue\nLow and High Mental Workload Levels\nPrivate: 14 Participants, 11 Channels, 500 Hz\nMa et al. 2019 [107]\nPCANet\nFatigue\nAwake and Fatigue\nPrivate: 6 Participants, 32 Channels, 500 Hz\nGao et al. 2019 [109]\nCNN\nFatigue\nAlert and Fatigue\nPrivate: 8 Participants, 30 Channels, 1000 Hz\nJeong et al. 2019 [237]\nCNN+LSTM\nMental State and Drowsi-\nness\n1. Alert and Drowsy\n2. Very Alert, Fairly Alert, neither Alert nor Sleepy, Sleepy but No Eﬀort to Keep Awake, and Very Sleepy\nPrivate: 8 Participants, 30 Channels, 1000 Hz\nZhang\net\nal.\n2017\n[113]\nDBN\nMental Workload\n1. Unloaded/Low/Normal/High Level\n2. Unloaded/Very/Low/Low/\nMedium/High/Very High/Overloaded Level\nPrivate: 6 Participants, 15 Channels, 500 Hz\nYin et al. 2017 [238]\nSAE\nMental Workload\nLow and High\nPrivate: 7 Participants, 11 Channels, 500 Hz\nHefron\net\nal.\n2018\n[114]\nCNN+LSTM\nMental Workload\nLow and High\nPrivate: 8 Participants, 128 Channels, 4096 Hz\nJiao et al. 2018 [239]\nCNN\nMental Workload\n4 Levels (1, 2, 3, and 4)\nPrivate: 13 Participants, 64 Channels, 500 Hz\n80\nYang et al. 2019 [240]\nSAE\nMental Workload\nLow and High\nPrivate: 8 Participants, 11 Channels, 500 Hz\nTao et al. 2019 [112]\nELM\nMental Workload\nLow and High\nPrivate: 8 Participants, 11 Channels, 500 Hz\nZhang\net\nal.\n2019\n[116]\nCNN+LSTM\nMental Workload\nLow and High\nPrivate: 20 Participants, 16 Channels, 1000 Hz\nYin et al. 2019 [117]\nDAE\nMental Workload\nLow and High\n1. Private: 14 Participants, 11 Channels, 500 Hz\n2. DEAP Dataset\nZhang\net\nal.\n2019\n[118]\nCNN\nMental Workload\nLow, Medium, and High\nPrivate: 17 Participants, 16 Channels, 1000 Hz\nWu et al. 2019 [111]\nCAE\nMental Workload and Fa-\ntigue\nNormal, Mild Fatigue, and Excessive Fatigue\nPrivate: 40 Participants, 1 Channel, - Hz\nYin et al. 2017 [115]\nDBN\nMental Workload and Fa-\ntigue\n1. Low, Medium and High Mental Workload\n2. Low, Medium and High Fatigue\nPrivate: 8 Participants, 11 Channels, 500 Hz\nLi et al. 2017 [16]\nDBN, SAE\nMental Workload and Fa-\ntigue\nEngagement Levels\nPrivate: 15 Participants, 32 Channels, 200 Hz\n81\nTable 7: Key Information of Papers about Sleep Stage Classiﬁcation\nAuthors\nModels\nDimension\nClasses\nData (Private/Public: No. of Participants, No. of Channels, Samplin\nYildirim et al. 2019 [241]\nCNN\nEEG, EOG\nW, N1, N2, N3, N4, REM\n1. Sleep-EDF Database\n2. Sleep-EDF Database Expanded\nPatanaik et al. 2018 [242]\nCNN\nEEG, EOG,\nW, N1, N2, N3, REM\nPrivate: Healthy Adolescents and Adults, Sleep Disorders Patients, P\nDisease Patients\nYuan et al. 2019 [243]\nCNN+GRU\nEEG, EOG, EMG\nW, S1, S2, SWS, REM\nUCD Database\nZhang et al. 2019 [244]\nCNN+LSTM\nEEG, EOG, EMG\nW, N1, N2, N3, REM\nSHHS\nChapotot et al. 2010 [245]\nMLP\nEEG, EOG, EMG\nW,\nN1,\nN2,\nN3,\nParadoxical\nSleep,\nand\nMovement Time\nPrivate: 13 Participants, 4 Channels, 128 Hz\nMalafeev 2018 [246]\nLSTM, CNN+LSTM\nEEG, EOG, EMG\nW, N1, N2, N3, REM\nPrivate: 18 Healthy Participants, 12 Channels, 256 Hz\nPrivate: 28 patients with narcolepsy and hypersomnia, 6 Channels, 2\nZhang et al. 2016 [128]\nDBN\nEEG, EOG, EMG\nW, S1, S2, SWS, REM\nUCD Database\nPhan et al. 2019 [247]\nCNN\nEEG, EOG, EMG\nW, N1, N2, N3, REM\n1. MASS Database\n2. Sleep-EDF Database\nChambon et al. 2018 [248]\nCNN\nEEG, EOG, EMG\nW, N1, N2, N3, REM\nMASS Database\n82\nJaoude et al. 2020 [129]\nCNN+RNN\nEEG, EOG, EMG\nW, N1, N2, N3, REM\n1. Private: 6341 Participants, 6 channels, - Hz\n2. Private: 93 participants, 6 channels, - Hz\n3. From Rosen et al. [249], 243 patients\n4. From Bakker et al. [250], 49 patients\nBiswal et al. 2018 [251]\nCNN+RNN\nEEG, EMG\nSleep Staging, Sleep Apnea, and Limb Move-\nments\n1. SHHS Database\n2. From Massachusetts General Hospital Sleep Lab, 10000 Participan\nSors et al. 2018 [252]\nCNN\nSingle Channel EEG\nW, N1, N2, N3, REM\nSHHS\nKulkarni et al. 2019 [121]\nCNN+LSTM\nSingle Channel EEG\nSpindles, Non-Spindles in N2 and N3 Stages\n1. MASS Database\n2. The DREAMS Sleep Spindles Database\n3. From Blank et al. [253], 5 Participants, 2 Channels, 200-512 Hz\n4. From Redline et al. [254], 5 Participants, 2 Channels, 200-512 Hz\n5. Private: 18 Epileptic Patients, 1 Channel, 512 Hz\nTsinalis et al. 2016 [122]\nSAE\nSingle Channel EEG\nW, N1, N2, N3, REM\nSleep-EDF Database Expanded\nZhang et al. 2018 [127]\nCNN\nSingle Channel EEG\nW, S1, S2, SWS, REM\n1.UCD Database\n2.MIT-BIH Polysomnographic Database\nMousavi et al. 2019 [130]\nCNN\nSingle Channel EEG\nW, N1, N2, N3, N4, REM\nSleep-EDF Database\nSupratak et al. 2017 [28]\nCNN+LSTM\nSingle Channel EEG\nW, N1, N2, N3, REM\n1. MASS Databse\n2. Sleep-EDF Database\nDong et al. 2018 [119]\nLSTM\nSingle Channel EEG\nW, N1, N2, N3, REM\nPrivate:62 Participants, 20 Channels, - Hz\n83\nZhang et al. 2020 [124]\nCNN\nSingle Channel EEG\nW, S1, S2, SWS, REM\n1.UCD Database\n2. MIT-BIH Polysomnographic Database\nBresch et al. 2018 [120]\nCNN+LSTM\nSingle Channel EEG\nW, N1, N2, N3, REM\n1. The SIESTA Normative Database\n2. Private: 29 Participants, 1 Channel, 1000 Hz\nAlMeer et al. 2019 [255]\nDNN\nSingle Channel EEG\nW, N1, N2, N3, REM\nSleep-EDF Database\nQu et al. 2020 [256]\nCNN\nSingle channel EEG\nW, N1, N2, N3, REM\n1. MASS Database\n2. Sleep-EDF Database\nHartmann et al. 2019 [123]\nLSTM\nMultiple Channels EEG\nConsecutive Activation Phases and Back-\nground Phase\nCAP Sleep Database\nCharnbon et al. 2019 [257]\nCNN\nMultiple Channels EEG\nSpindles, K-complexes, and Arousals\n1. MASS Database\n2. Stanford Sleep Cohort Dataset [258], 26 Participants, 1 Channel (\n3. WisConsin Sleep Cohort Dataset [259], 30 Participants, 1 Channel\n4. MESA\nJeon et al. 2019 [125]\nCNN+LSTM\nMultiple Channels EEG\nW, N1, N2\nPrivate: 218 Pediatric Participants, 32 Channels, 200 Hz\nChriskos et al. 2020 [131]\nCNN\nMultiple Channels EEG\nN1, N2, N3, REM\nPrivate: 22 Participants, 19 Channels, - Hz\n84\nTable 8: Key Information about Other Applications\nAuthors\nModels\nCategories\nClasses\nData (Private/Public: No. of Participants, No. of Channels, Sampling\nRate)\nKaushik et al.\n2019\n[134]\nLSTM\nAge and Gender Prediction\nClass from 0 to 5, Varies from Age and Gen-\nder\nFrom Kaur et al. [260], 60 Participants (35 males and 25 females), 14\nChannels, - Hz\nWulsin\net\nal.\n2012\n[261]\nDBN\nAnomaly Detection\n5 Classes:\nSpike and Sharp Wave, GPED\nand Triphasic, PLED, Eye Blink, and Back-\nground\nPrivate: 11 Participants, 17 Channels, 256 Hz\nAnem et al. 2019 [262]\nCNN+LSTM\nArtifacts Removal\n-\n-\nJacob et al. 2019 [263]\n-\nArtiﬁcial Muscle Intelligence Sys-\ntem\nGrasp,\nRelease,\nRollup,\nRolldown,\nand\nRollup Release\nPrivate: 20 Participants (10 Healthy and 10 Paralyzed), 16 Channels,\n- Hz\nHuang\net\nal.\n2018\n[135]\nCNN\nAuditory Salience\n4923 Classes of Video Classiﬁcation\nPrivate: - Participants, 128 Channels, 2048 Hz\nYang et al. 2018 [264]\nSAE\nAutomatic\nOcular\nArtifacts\nRe-\nmoval\n-\nBCI Competition IV Dataset 1\nJiang et al. 2019 [137]\nCNN+LSTM\nBrain Imaging Classiﬁcation\n40 Classes of Images\nImageNet-EEG Dataset [265]\n85\nBaltatzis et al.\n2017\n[141]\nCNN\nBullying Incidences Identiﬁcation\nBullying 2D/VR, Non-Bullying 2D/VR\nPrivate: 18 Participants, 256 Channels, 250 Hz\nToraman et al.\n2019\n[140]\nCNN\nCerebral Dominance Detection\nLeft and Right-Hemisphere Dominance\nPrivate:\n67 Participants (35 Right-Hand Dominat and 32 Left-Hand\nDominat), 18 Channels, - Hz\nDoborjeh et al.\n2018\n[142]\nSNN\nClassiﬁcation of Familiarity of Mar-\nketing Stimuli\nFamiliar and Unfamiliar Brands\nPrivate: 20 Participants, 19 Channels, 256 Hz\nCroce et al. 2019 [266]\nCNN\nClassiﬁcation of Independent Com-\nponents\nBrain ICs and Artifact ICs\nPrivate: - Participants, 128 Channels, 500 Hz\nZheng\net\nal.\n2020\n[138]\nLSTM+CNN,\nGAN\nDecoding Human Brain Activity\n40 Classes of Images\nFrom Spampinato et al. [265], 6 Participants, 128 Channels, 1000 Hz\nMing et al. 2019 [267]\nSAN\nEEG Data Analysis\n1. Diﬀerent Vigilance Stages\n2. P300 and Non-P300\n1. Private: - Participants, - Channel, 500 Hz\n2. From Wu et al., 18 Participants, 64 Channels, 512 Hz\nNagabushanam et al.\n2019 [268]\nLSTM\nEEG Signal Classiﬁcation\n-\nFrom Bonn University, - Participants, 20 Channels, - Hz\nHua et al. 2019 [139]\nSAE\nFunctional Brain Network\nHigh and Low Proﬁciency Operators\nPrivate: 20 Participants, 8 Channels, 1000 Hz\nGoh et al. 2018 [149]\nSSRL\nGait Pattern Classiﬁcation\nFree Walking, Exoskeleton-Assisted Walking\nat Zero, Low, and High Assistive Forces\nPrivate: 27 Participants, 20 Channels, 1000 Hz\nFares et al. 2019 [269]\nLSTM\nImage Classiﬁcation\n40 Classes of Images\nFrom Spampinato et al. [265],6 Participants, 128 Channels, 1000 Hz\nAkbari\net\nal.\n2019\n[270]\nDNN\nIntelligible Speech Recognition\n-\nPrivate: 5 Participants, - Channel, 3000 Hz\n86\nAntoniades 2018 [271]\nCNN\nMapping Scalp EEG to iEEG\n-\nPrivate: 18 Participants, 32 Channels (12 FO and 20 Scalp), 200 Hz\nBird et al. 2019 [272]\nMLP, LSTM\nOptimise the Topology of ANN\n1. Relaxed, Concentrative, and Neutral\n2. Positive, Neutral, and Negative\n3. 0 to 9 Imaginary EEG\n1. EEG Brainwave Dataset: Mental State\n2. EEG Brainwave Dataset: Feeling Emotions\n3. MindBigData Dataset\nWang et al. 2019 [133]\nCNN\nPerson Identiﬁcation\n-\n1. From PhysioNet (missing detail), 109 Participants, 64 Channels, 160 Hz\n2. Private: 59 Participants, 46 Channels, 250 Hz\nOzdenizci et al.\n2019\n[132]\nCNN\nPerson Identiﬁcation\n-\nPrivate: 10 Participants, 16 Channels, 256 Hz\nSinghal et al.\n2018\n[273]\nDBCS\nReconstruction\nand\nAnalysis\nof\nBiomedical Signals\n-\n1. From Andrzejak et al. [207], 10 Participants (5h Healthy and 5 Epileptic\n2. BCI Competition II and III\nGogna\net\nal.\n2017\n[274]\nSAE\nReconstruction\nand\nAnalysis\nof\nBiomedical Signals\n-\nFrom Andrzejak et al. [207], 10 Participants (5 Healthy and 5 Epileptic\nPatients)\nJang et al. 2019 [275]\nCNN\nSeizure Detection of Mice\nSeizure and Non-Seizure\nPrivate: Total 4704h of EEG Recording, 1000 Hz\nArora et al. 2018 [143]\nLSTM\nSuccessful Episodic Memory Encod-\ning Prediction\nSuccessful and Unsuccessful Recall\nFrom UT Southwesetern Medical Center: 30 Participants (15 Dominat\nand 15 Non-Dominant Hemisphere), 13 and 17 Channel (8-14 Contacts\nper Electrode), 1000 Hz\nYu et al. 2020 [136]\nCNN\nTonic Cold Pain Assessment\nNo Pain, Moderate Pain, and Sever Pain\nPrivate: 32 Participants, 32 Channels, 500 Hz\nOgawa\net\nal.\n2018\n[276]\nLSTM\nVideo Classiﬁcation\nLiked Video and Not Liked Video\nPrivate: 11 Participants, 1 Channel, 1024 Hz\n87\nSaid et al. 2018 [277]\nSAE\nVital Signs Compression and En-\nergy Eﬃcient Delivery\n-\nDEAP Dataset\n88\nTable 9: A Summary of Datasets Mentioned in This Survey\nDataset Name\nModality\nData Information\nCategory\nURL\nBCI Challenge\nEEG\n26 Participants, 56 Channels, 600\nHz\nP300 and Non-P300\nhttps://www.kaggle.com/c/inria-bci-challenge\nBCI Competition Data\nEEG\nMultiple Datasets\nMultiple Categories\nhttp://www.bbci.de/competition\nBNCI Horizon\nEEG\nMultiple Datasets\nMultiple Categories\nhttp://bnci-horizon-2020.eu/database/data-sets\nCAP Sleep Database\nEEG, EOG, EMG, ECG\n16 Participants, 3 EEG Channels\nW, S1, S2, S3, S4, and REM\nhttps://physionet.org/content/capslpdb/1.0.0\nCHB-MIT Scalp EEG Database\nEEG\n22 Participants, 23 Channels, 256\nHz\nIctal Activity, Siezure Onset, and\nOfsset\nhttps://physionet.org/content/chbmit/1.0.0\nCSU BCI Collection\nEEG\nVary with data sets in the database\nNormal and Motor Impairments\nhttps://www.cs.colostate.edu/eeg\nDEAP Dataset\nEEG and Physiological Signals\n32 Participants, 32 Channels, 512\nHz\nScores For Arousal, Valence, Iiking,\nDominance and Familiarity\nhttp://www.eecs.qmul.ac.uk/mmv/datasets/deap\nEEG\nBrainwave\nDataset:\nFeeling\nEmotions\nEEG\n2 Participants, 4 Channels, - Hz\nPositive, Neutral, and Negative\nhttps://www.kaggle.com/birdy654/eeg-brainwave-dat\nemotions\nEEG\nBrainwave\nDataset:\nMental\nState\nEEG\n4 Participants, 4 Channels, - Hz\nRelaxed, Concentrating, and Neu-\ntral\nhttps://www.kaggle.com/birdy654/eeg-brainwave-dat\n89\nEEG Database Data Set/UCI EEG\nDataset\nEEG\n122 Participants, 64 Channels, 256\nHz\nAlcoholic and Control\nhttps://archive.ics.uci.edu/ml/datasets/eeg+database\nEEG\nMotor\nMovement/Imagery\nDataset\nEEG\n109 Participants, 64 Channels, 160\nHz\nLeft/Right Fist or Both Fists/Both\nFeet\nhttps://physionet.org/content/eegmmidb/1.0.0\nEPFL BCI Dataset\nEEG\n9 Participants, 34 Channels, 2047\nHz\nP300 and Non-P300\nhttps://www.epﬂ.ch/labs/mmspg/research/page-5831\n2/bci datasets/emotion dataset/\nEpileptic\nSeizure\nRecognition\nData\nSet\nEEG\n500 Participants, - Channels, 173.61\nHz\nHealthy With Eyes Open/Closed,\nPatients\nduring\nSeizure/Interictal\nfrom\nHippocampal\nLoca-\ntion/Interictal from Epileptogenic\nZone\nhttps://archive.ics.uci.edu/ml/datasets/Epileptic+Se\nMASS Database\nEEG, EOG, EMG, ECG\n200 Participants, 4–20 EEG Chan-\nnels, 256Hz\nW, N1, N2, N3, and REM\nhttp://www.ceams-carsm.ca/en/MASS\nMESA\nEEG, EOG\n6814 Participants,\nFz-Cz,\nCz-Oz,\nC4, 256Hz\nArousal Level\nhttps://www.sleepdata.org/datasets/mesa\nMindBigData\nEEG\nVary with data sets in the dataset\nBrain Reaction from Seeing A Digit\n(0 to 9)\nhttp://www.mindbigdata.com/opendb\nMIT-BIH Polysomnographic Database\nEEG, ECG, EOG, EMG, Res-\npiration Signals,\nand Physio-\nlogical Signals\n60 subjects, 7 PSG Channels, 250\nHz\nW, N1, N2, N3, N4, and REM\nhttps://www.physionet.org/content/slpdb/1.0.0\n90\nSEED Dataset\nEEG and Eye Movement\n15\nParticipants,\n62\nChannels,\n1000Hz\nPositive/Neutral/Negative\nand Happy/Sad/Neutral/Fear\nhttps://bcmi.sjtu.edu.cn/home/seed/\nSHHS\nEEG, EOG, EMG\n6,441 Participants, C4-A1 and C3-\nA2, 125 Hz\nW, N1, N2, N3, N4, and REM\nhttps://sleepdata.org/datasets/shhs/\nSleep-EDF Database Expanded\nEEG, EOG, EMG\n61 Participants, Fpz-Cz and Pz-Oz,\n100 Hz\nW, S1, S2, S3, S4, and REM\nhttps://physionet.org/content/sleep-edfx/1.0.0\nSleep-EDF Database\nEEG, EOG, EMG\n20 Participants, Fpz-Cz and Pz-Oz,\n100 Hz\nW, N1, N2, N3, and REM\nhttps://physionet.org/content/sleep-edf/1.0.0\nThe Bern-Barcelona EEG Database\nEEG\n5 Participants, 7500 Pairs of Sig-\nnals, 512 or 1024 Hz\nFocal and Non-Focal\nhttps://www.upf.edu/web/mdm-dtic/datasets\nThe\nSIESTA\nNormative\nDatabase\n(cross-institute)\nEEG, EOG, EMG, ECG\n292 Participants, 6 EEG Channels,\nVariable (minimum 100Hz)\nW, N1, N2, N3, and REM\nhttp://ofai.at/siesta/database.html\nUCD Database\nEEG and Physiological Signals\n25 Participants, C3–A2 and C4–A1,\n128Hz\nW, S1, S2, Sws, and REM\nhttps://physionet.org/content/ucddb/1.0.0\nLUMED Dataset\nEEG and Physiological Signals\n11 Participants, 8 Channels, 500 Hz\nNegative and Positive Valence\nhttps://www.dropbox.com/s/xlh2orv6mgweehq/LUM\n91\nTable 10: A Brief Summary of Sleep Stages\nSleep Stages\nMain Features of EEG in Each Stage\nBrief Description\nWake\nAlpha Waves\nBefore Sleep\nStage N1 NREM\nLow-Voltage Theta Waves\nBlood Pressure Falls\nStage N2 NREM\nTheta Waves with K Complexes and Sleep Spindles\nCardiac Activity Decre\nStage N3 NREM\nHigh-Amplitude Delta Waves\nHigh Threshold for Aro\nStage REM Sleep\nLow-Amplitude Theta Waves\nBlood Pressure and Pu\n92\n",
  "categories": [
    "eess.SP",
    "cs.LG",
    "q-bio.NC"
  ],
  "published": "2020-11-22",
  "updated": "2021-05-20"
}