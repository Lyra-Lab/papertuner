{
  "id": "http://arxiv.org/abs/2011.11263v2",
  "title": "Evaluating Input Representation for Language Identification in Hindi-English Code Mixed Text",
  "authors": [
    "Ramchandra Joshi",
    "Raviraj Joshi"
  ],
  "abstract": "Natural language processing (NLP) techniques have become mainstream in the\nrecent decade. Most of these advances are attributed to the processing of a\nsingle language. More recently, with the extensive growth of social media\nplatforms focus has shifted to code-mixed text. The code-mixed text comprises\ntext written in more than one language. People naturally tend to combine local\nlanguage with global languages like English. To process such texts, current NLP\ntechniques are not sufficient. As a first step, the text is processed to\nidentify the language of the words in the text. In this work, we focus on\nlanguage identification in code-mixed sentences for Hindi-English mixed text.\nThe task of language identification is formulated as a token classification\ntask. In the supervised setting, each word in the sentence has an associated\nlanguage label. We evaluate different deep learning models and input\nrepresentation combinations for this task. Mainly, character, sub-word, and\nword embeddings are considered in combination with CNN and LSTM based models.\nWe show that sub-word representation along with the LSTM model gives the best\nresults. In general sub-word representations perform significantly better than\nother input representations. We report the best accuracy of 94.52% using a\nsingle layer LSTM model on the standard SAIL ICON 2017 test set.",
  "text": "Evaluating Input Representation for Language\nIdentiﬁcation in Hindi-English Code Mixed Text\nRamchandra Joshi1 and Raviraj Joshi2\n1Department of Computer Engineering, Pune Institute of Computer Technology\n2Department of Computer Science and Engineering, Indian Institute of Technology\nMadras\n{rbjoshi1309, ravirajoshi}@gmail.com\nAbstract. Natural language processing (NLP) techniques have become\nmainstream in the recent decade. Most of these advances are attributed\nto the processing of a single language. More recently, with the extensive\ngrowth of social media platforms focus has shifted to code-mixed text.\nThe code-mixed text comprises text written in more than one language.\nPeople naturally tend to combine local language with global languages\nlike English. To process such texts, current NLP techniques are not suf-\nﬁcient. As a ﬁrst step, the text is processed to identify the language of\nthe words in the text. In this work, we focus on language identiﬁcation\nin code-mixed sentences for Hindi-English mixed text. The task of lan-\nguage identiﬁcation is formulated as a token classiﬁcation task. In the\nsupervised setting, each word in the sentence has an associated language\nlabel. We evaluate diﬀerent deep learning models and input representa-\ntion combinations for this task. Mainly, character, sub-word, and word\nembeddings are considered in combination with CNN and LSTM based\nmodels. We show that sub-word representation along with the LSTM\nmodel gives the best results. In general sub-word representations per-\nform signiﬁcantly better than other input representations. We report the\nbest accuracy of 94.52% using a single layer LSTM model on the standard\nSAIL ICON 2017 test set.\nKeywords: code mixing, language identiﬁcation, sub word representa-\ntion, convolutional neural networks, long short term memory\n1\nIntroduction\nThe automatic processing of text to derive insights has been widely used in the\nindustry. It can be used to process product or movie reviews in order to derive a\ngeneral sentiment. Other applications include analysis of tweets to derive percep-\ntion of a brand or thoughts of people on a speciﬁc topic. All of these applications\ncan be boiled down to classiﬁcation or summarization tasks. The state of the\nart NLP techniques performs very well on these tasks for single language texts.\nHowever, they may not perform well on code-mixed text due to the unavailabil-\nity of enough labeled data. The code-mixed text has become relevant these days\nbecause of diﬀerent social media platforms [4] [1]. Most of these platforms prefer\narXiv:2011.11263v2  [cs.CL]  25 Nov 2020\n2\nR. Joshi et al.\nEnglish as the preferred medium of communication. In a multi-lingual country\nlike India people tend to mix local language with English while using social me-\ndia platforms. This is because people are more comfortable in local languages.\nIt is natural to describe local terms or entities in local languages which result in\ncode-mixing. Code-mixing essentially allows us to borrow terms from diﬀerent\nlanguages thus aiding ease of communication. A local touch can be given to the\nmovie reviews, product reviews, and comments by adding some details in the\nlocal language. All of these factors have led to a rise in the popularity of code-\nmixed text [6]. In order to understand such code-mixed text, it is important to\nidentify the language used in diﬀerent parts of text followed by language-speciﬁc\nprocessing [3]. In this work, we present diﬀerent approaches for language identi-\nﬁcation in code-mixed text. In the code-mixed text, languages can be interleaved\nin diﬀerent forms. One form of code-mixing is represented in this example, ”this\nis not union budget, ye to aam admi ka budget hai” with language tagged as\n”eng eng eng eng eng hin hin hin hin hin hin hin”. Another form can be seen\nas ”maine aaj WhatsApp and Facebook uninstall kiya h” tagged as ”hin hin eng\neng eng eng hin hin”. It is challenging to determine the language of individual\nwords as the same word can be used in both Hindi and Engish depending on the\ncontext. For example English words like ”are” and ”maze” can also be used in\nHindi as ”are mai ghar jaa raha hu”, and ”appke to maze hai”. To make it more\nchallenging the social media text is normally noisy where words are written in\ndiﬀerent ways just to emphasize them. For example the in word ”good” the letter\n’o’ can be repeated multiple times to get diﬀerent variations like ”the movie was\ngooood”. This makes it important to consider diﬀerent input representations.\nThe sentence can be processed word by word or character by character. The\nmore recent form of representation is sub-word where a word is split into logical\nsub-word units [8]. The character and sub-word based representations are more\nagnostic to noisy text variations as compared to word representation which will\ntreat each variation as a separate word. The focus of our work is to evaluate\nthe performance of these input representations. This is the ﬁrst work to explore\nsub-word based representations for Hi-En language identiﬁcation. The task is to\ndetermine the language of each word in the sentence. The task is formulated as a\ntoken classiﬁcation task. The deep learning models based on convolutional neural\nnetworks (CNNs) and long short term memory (LSTM) networks are the most\npopular techniques used for token classiﬁcation. We use these simple models in\ncombination with diﬀerent input representations to evaluate their eﬀectiveness.\nThese models are often used with a conditional random ﬁeld (CRF) to improve\nthe performance. However, the work is restricted to simple architectures with a\nfocus on input representation. We show that sub-word based representation cou-\npled with these simple models perform better than other complex architectures\nreported in the literature. Simple architectures are also favorable as it reduces\nruntime speed and complexity. The language identiﬁcation module should be\nfast and eﬃcient as it will often be followed by other NLP modules. With this\nperspective, we experiment with single-layer CNN and LSTM models. The ex-\nLanguage Identiﬁcation in Hindi-English Code Mixed Text\n3\nperiments show that these architectures are suﬃcient to reach desired accuracy\nlevels. The main contributions of this work are:\n– The eﬀectiveness of character, sub-word, and word-based representations are\nevaluated for the task of Hindi-English language identiﬁcation.\n– The combinations of popular model architectures and input representations\nare also compared.\n2\nRelated Work\nIn this section, we review some of the deep learning-based approaches used for\ncode mixed language identiﬁcation. Simple feed-forward neural networks utilizing\ncharacter n-gram and lexicon features have shown to produce a good performance\nfor this task [15]. Although the task can be performed at the word level, it\nis a common practice to use neighboring words contextual information to aid\nthe classiﬁcation process [12]. The word vectors can be directly passed to a bi-\ndirectional LSTM to encode the contextual information. Alternatively, both word\nvectors and character-based word vectors can be provided to the LSTM. The\ncharacter-based word representations can be generated using another CNN or\nLSTM [10]. Multi-channel CNNs are commonly used to capture such character-\nbased word representations [7]. The primary motive behind using character-\nbased representation is to avoid the out of vocabulary problem. It also helps in\nbetter classiﬁcation of words that have very low representation in the training\ndata. There have been few works related to code mixed Hindi-English languages\napplying similar concepts [14]. The Bengali-English code mixed text has also\nbeen equally explored in literature [2]. Other Indian languages like Telugu and\nthe Assamese have also be analyzed from the code-mixing perspective [5] [13]. We\nhave limited ourselves to Hindi-English text because of the lack of standardized\nand meaningful data sets in other languages. We study the usage of plain word\nembedding and its combination with character-based word embedding. Sub-word\nbased representations are also evaluated to highlight its importance in identifying\ncode-mixed languages.\n3\nDataset\nThe ICON 2017 code mixed sentiment analysis data set is used for evaluation\n[11]. The data set also has each and every word tagged with its corresponding\nlanguage. The sentiment tags are ignored and only language information is used\nfor the experiments. The code mixed text in this data set was extracted from\ntwitter and manually annotated for sentiment and language. There are a total of\n12936 training sentences and 5525 test sentences. The split is predeﬁned and the\nresults reported are using the same split. A small validation set comprising of\n10% of train sentences is using for hyper-parameter tuning of the models. There\nare around 80k Hindi and English tokens individually in the train data. In the\ntest data, there are around 30k tokens in each class.\n4\nR. Joshi et al.\nFig. 1. Input Representations\n4\nInput Representations\nThe input representations evaluated in this work are characters, sub-words, and\nwords. The structure of each representation is shown in Figure 1.\n4.1\nWord representation\nThe word embeddings are the default representations used for processing text.\nIn this approach, each word is considered as a token and the sentence can be\nseen as a series of tokens. Each token is represented using a 300-dimensional\nvector also known as the word vector. This distributed representation allows us\nto capture the semantic relationship between individual words. The sentence can\nnow be seen as a series of word vectors that are processed using CNN or LSTM\nto get their contextual representations. The output of CNN or LSTM is again\na series of vectors but these vectors now encode the contextual information as\nopposed to word vectors. This contextual vector is passed through dense layers\nto get ﬁnal predictions. So, each word corresponds to a time step, and for each\ntime step its corresponding label in terms of language id is predicted.\n4.2\nCharacter representation\nAnother approach is to use character-based representations to augment the word\nvectors. The idea is to use another shallow network to process each word char-\nacter by character. The series of characters can now be seen as a time series that\nis passed through CNN to get contextual character representation. The repre-\nsentations are max-pooled over time to get corresponding word representations.\nThese character-based word representations are then concatenated with token-\nbased word representations discussed earlier and then processed using neural\nnetworks. Simply using character-based word representations performs similar\nto the ﬁrst word-based approach at a cost of increased complexity. So we only\nreport results for model where character representations are used in conjunction\nLanguage Identiﬁcation in Hindi-English Code Mixed Text\n5\nwith word embeddings. The usage of character-based word embeddings miti-\ngates the out of vocabulary problem to some extent. The token-based word\nrepresentations for unknown words will be mapped to a single unk token and\nthe neural network will have to rely on contextual information to make pre-\ndictions. Whereas the character-based word representations will always give us\nmeaningful representations since the number of characters is ﬁxed and known.\n4.3\nSub-word representation\nThe ﬁnal representation strategy explored here is the sub-word embeddings [9].\nThis can be seen as an intermediate form with granularity somewhere between\nwords and characters. Some of the possible sub words for word ”hello” are\n”hell/o”, ”he/llo”, and ”he/l/l/o”. This type of representation is very useful\nto mitigate the open vocabulary problem. The exact sub word split is deter-\nmined by the statistical character n-gram properties of the training corpus. We\ntrain a uni-gram based subword model using Google sentence piece tokenizer [8].\nThe subword vocab size is set to 12k. This subword model is used to split each\nword into constituent sub words. The ﬁrst subword of each word is assigned the\nparent language label. The subsequent subwords are assigned a dummy label.\nThe problem is again formulated as a token classiﬁcation problem where we are\nonly concerned about the label of the ﬁrst sub-word token of each word. How-\never, during cross-entropy training, all the tokens contribute to the loss. The\nmasking of loss from dummy labels is not explored in this work.\n5\nModel Architecture\n– CNN: This is a basic CNN model based on 1D convolutions. The word\nor sub-word embeddings of 300 dimensions are passed through a single 1D\nconvolution. The kernel size is 4 and the number of ﬁlters is 64. This is\nfollowed by two dense layers of size 100 and 1. The output of the individual\ntime step is subjected to a dense layer so as to have a prediction for each\nword. The convolutional layer and dense layers are followed by relu and\nsigmoid activation functions respectively in all the models described here.\nAdam is used as an optimizer. The binary cross-entropy is used as the loss\nfunction as there are two output classes. The optimizer and loss function are\ncommon across all the models.\n– Multi-CNN: In this model, three parallel 1D convolutions are applied on\nthe word or sub-word embeddings. The ﬁlter sizes are 2, 3, and 4 with 64 ﬁl-\nters each. The output of these convolutions is concatenated. This is followed\nby two dense layers of size 100 and 1.\n– LSTM: This is a basic LSTM model. A single Bi-LSTM with 300 hidden\nunits is used. The word or sub-word embeddings of 300 dimensions are passed\nthrough this layer. The output at each time step is subjected to two dense\nlayers of size 100 and 1. A dropout of 0.4 is used in the recurrent layer.\n6\nR. Joshi et al.\n– CNN+LSTM: This combines the basic CNN and LSTM models sequen-\ntially. The 1D CNN as described above is followed by the Bi-LSTM layer.\nThe output is then subjected to two dense layers.\n– CharCNN+LSTM: The three parallel convolutions as described in the\nMulti-CNN network is used to process characters. The output of each con-\nvolutional layer is max-pooled over time and concatenated to produce word\nembedding of size 192 dimension. The time axis corresponds to the number\nof characters in the word. The word embedding generated by this multi-\ncnn network is then concatenated with 300 dimension learnable word em-\nbeddings as used in previous models. The concatenated representations are\npassed through a single Bi-LSTM layer and dense layers. The post embed-\nding setup is the same as the basic LSTM model described above.\nTable 1. Classiﬁcation metrics of diﬀerent models and input combinations\nModel\nInput\nlang\nprecision recall\nf1-score\nacc\nCNN\nword\nen\n90.42\n92.15\n91.27\n91.49\nhi\n90.52\n90.87\n91.69\nsub-word\nen\n95.21\n94.97\n95.09\n93.86\nhi\n91.62\n92.01\n91.81\nMulti-CNN\nword\nen\n91.68\n91.00\n91.34\n91.66\nhi\n91.64\n92.28\n91.96\nsub-word\nen\n95.60\n94.99\n95.30\n94.13\nhi\n91.71\n92.69\n92.20\nLSTM\nword\nen\n91.25\n91.40\n91.33\n91.61\nhi\n91.95\n91.81\n91.88\nsub-word\nen\n95.15\n96.13\n95.64\n94.52\nhi\n93.41\n91.81\n92.60\nCNN+LSTM\nword\nen\n91.36\n91.61\n91.48\n91.76\nhi\n92.13\n91.90\n92.02\nsub-word\nen\n95.82\n95.17\n95.50\n94.39\nhi\n92.01\n93.06\n92.54\nCharCNN+LSTM char+word\nen\n91.94\n93.06\n92.49\n92.71\nhi\n93.44\n92.39\n92.91\n6\nResults and Discussion\nA combination of models and input representations is evaluated on the ICON\n2017 data set. The word and sub-word representations are used as input to CNN\nand LSTM based models. The models used are simple CNN, simple LSTM,\nLanguage Identiﬁcation in Hindi-English Code Mixed Text\n7\nMulti-CNN, and CNN+LSTM. The character representations are used with the\nCharCNN+LSTM model. In this model, characters are processed using Multi-\nCNN and passed to LSTM along with word representation. This model archi-\ntecture was chosen as it performs better than other variations of character em-\nbedding based models. Their variations can have either CNN or LSTM for pro-\ncessing characters and the main network can again be based on either CNN or\nLSTM. The metrics used for evaluation are precision, recall, f1-score, and over-\nall accuracy. Table 1 shows the results for model and input combinations. The\nLSTM model utilizing subword embeddings performs the best. Augmentation\nof character-based word embeddings with vanilla word embeddings boosts the\nperformance of word-based models. Thus performance-wise word < char + word\n< sub-word relationship holds. From the model perspective CNN < Multi-CNN\n<= LSTM when the only word or sub-word embeddings are considered. The\nCNN+LSTM gives the best score when word embeddings are given as input to\nthe models. In general sub-word based models are signiﬁcantly better as com-\npared to models utilizing word and character representations. This is primarily\nbecause sub words not only handle unknown words but also go well with the\nnoisy data.\n7\nConclusion\nIn this work, we study diﬀerent deep learning approaches for language identiﬁ-\ncation in Hindi-English code mixed text. The problem can be seen as a token\nclassiﬁcation problem. A combination of diﬀerent models and input represen-\ntations are compared for their eﬀectiveness. The models include simple CNN,\nsimple LSTM, multi-channel CNN, CNN+LSTM, and charCNN+LSTM. The\ninput representations used are characters, words, and subwords. We show that\nsub-word based models are superior as compared to word and character-based\napproaches. The sub word representation when coupled with simple LSTM per-\nforms the best. Just changing the input representation helps basic models achieve\nhigh performance. The sub word representation helps tackle the out of vocabu-\nlary problem and at the same time handles noisy data well. These attributes are\nvery analogous with the data set explored in this work.\nReferences\n1. Barman, U., Das, A., Wagner, J., Foster, J.: Code mixing: A challenge for language\nidentiﬁcation in the language of social media. In: Proceedings of the ﬁrst workshop\non computational approaches to code switching. pp. 13–23 (2014)\n2. Chanda, A., Das, D., Mazumdar, C.: Unraveling the english-bengali code-mixing\nphenomenon. In: Proceedings of the Second Workshop on Computational Ap-\nproaches to Code Switching. pp. 80–89 (2016)\n3. Das, A., Gamb¨ack, B.: Code-mixing in social media text: the last language identi-\nﬁcation frontier? (2015)\n8\nR. Joshi et al.\n4. Gamb¨ack, B., Das, A.: Comparing the level of code-switching in corpora. In: Pro-\nceedings of the Tenth International Conference on Language Resources and Eval-\nuation (LREC’16). pp. 1850–1855 (2016)\n5. Gundapu, S., Mamidi, R.: Word level language identiﬁcation in english telugu code\nmixed data. In: PACLIC (2018)\n6. Khanuja, S., Dandapat, S., Srinivasan, A., Sitaram, S., Choudhury, M.: Gluecos:\nAn evaluation benchmark for code-switched nlp. arXiv preprint arXiv:2004.12376\n(2020)\n7. Kim, Y.: Convolutional neural networks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882 (2014)\n8. Kudo, T.: Subword regularization: Improving neural network translation models\nwith multiple subword candidates. arXiv preprint arXiv:1804.10959 (2018)\n9. Kudo, T., Richardson, J.: Sentencepiece: A simple and language independent\nsubword tokenizer and detokenizer for neural text processing. arXiv preprint\narXiv:1808.06226 (2018)\n10. Mandal, S., Singh, A.K.: Language identiﬁcation in code-mixed data using mul-\ntichannel neural networks and context capture. arXiv preprint arXiv:1808.07118\n(2018)\n11. Patra, B.G., Das, D., Das, A.: Sentiment analysis of code-mixed indian lan-\nguages: An overview of sail code-mixed shared task@ icon-2017. arXiv preprint\narXiv:1803.06745 (2018)\n12. Samih, Y., Maharjan, S., Attia, M., Kallmeyer, L., Solorio, T.: Multilingual code-\nswitching identiﬁcation via lstm recurrent neural networks. In: Proceedings of the\nSecond Workshop on Computational Approaches to Code Switching. pp. 50–59\n(2016)\n13. Sarma, N., Singh, S.R., Goswami, D.: Identifying languages at the word level in\nassamese-bengali-hindi-english code-mixed social media text\n14. Veena, P., Anand Kumar, M., Soman, K.: Character embedding for language iden-\ntiﬁcation in hindi-english code-mixed social media text. Computaci´on y Sistemas\n22(1), 65–74 (2018)\n15. Zhang, Y., Riesa, J., Gillick, D., Bakalov, A., Baldridge, J., Weiss, D.: A fast, com-\npact, accurate model for language identiﬁcation of codemixed text. arXiv preprint\narXiv:1810.04142 (2018)\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-11-23",
  "updated": "2020-11-25"
}