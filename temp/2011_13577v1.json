{
  "id": "http://arxiv.org/abs/2011.13577v1",
  "title": "A survey of benchmarking frameworks for reinforcement learning",
  "authors": [
    "Belinda Stapelberg",
    "Katherine M. Malan"
  ],
  "abstract": "Reinforcement learning has recently experienced increased prominence in the\nmachine learning community. There are many approaches to solving reinforcement\nlearning problems with new techniques developed constantly. When solving\nproblems using reinforcement learning, there are various difficult challenges\nto overcome. To ensure progress in the field, benchmarks are important for\ntesting new algorithms and comparing with other approaches. The reproducibility\nof results for fair comparison is therefore vital in ensuring that improvements\nare accurately judged. This paper provides an overview of different\ncontributions to reinforcement learning benchmarking and discusses how they can\nassist researchers to address the challenges facing reinforcement learning. The\ncontributions discussed are the most used and recent in the literature. The\npaper discusses the contributions in terms of implementation, tasks and\nprovided algorithm implementations with benchmarks. The survey aims to bring\nattention to the wide range of reinforcement learning benchmarking tasks\navailable and to encourage research to take place in a standardised manner.\nAdditionally, this survey acts as an overview for researchers not familiar with\nthe different tasks that can be used to develop and test new reinforcement\nlearning algorithms.",
  "text": "SACJ Vol TBC(Num TBC) MONTH OF PUBLICATION 2020\nResearch Article\nA survey of benchmarking frameworks for\nreinforcement learning\nBelinda Stapelberg\na, Katherine M. Malan\na\na Department of Decision Sciences, University of South Africa, Pretoria, South Africa\nABSTRACT\nReinforcement learning has recently experienced increased prominence in the machine learning community. There\nare many approaches to solving reinforcement learning problems with new techniques developed constantly. When\nsolving problems using reinforcement learning, there are various diﬃcult challenges to overcome.\nTo ensure progress in the ﬁeld, benchmarks are important for testing new algorithms and comparing with\nother approaches. The reproducibility of results for fair comparison is therefore vital in ensuring that improvements\nare accurately judged. This paper provides an overview of diﬀerent contributions to reinforcement learning\nbenchmarking and discusses how they can assist researchers to address the challenges facing reinforcement learning.\nThe contributions discussed are the most used and recent in the literature. The paper discusses the contributions\nin terms of implementation, tasks and provided algorithm implementations with benchmarks.\nThe survey aims to bring attention to the wide range of reinforcement learning benchmarking tasks available\nand to encourage research to take place in a standardised manner. Additionally, this survey acts as an overview for\nresearchers not familiar with the diﬀerent tasks that can be used to develop and test new reinforcement learning\nalgorithms.\nKeywords: reinforcement learning, benchmarking\nCategories:\n• Computing methodologies ∼Reinforcement learning\nEmail:\nBelinda Stapelberg belinda.stapelberg@up.ac.za (CORRESPONDING),\nKatherine M. Malan malankm@unisa.ac.za\nArticle history:\nReceived:\nAccepted:\nAvailable online:\n1\nINTRODUCTION\nReinforcement learning (RL) is a subﬁeld of machine learning, based on rewarding desired\nbehaviours and/or punishing undesired ones of an agent interacting with its environment [1]. The\nagent learns by taking sequential actions in its environment, observing the state of the environment\nand receiving a reward. The agent needs to learn a strategy, called a policy, to decide which action\nto take in any state. The goal of RL is to ﬁnd the policy that maximises the long-term reward of\nthe agent.\nIn recent years RL has experienced dramatic growth in research attention and interest due to\npromising results in areas including robotics control [2], playing Atari 2600 [3, 4], competitive video\nBelinda Stapelberg and Katherine M. Malan (2020). A survey of benchmarking frameworks for reinforcement\nlearning. South African Computer Journal Vol TBC(Num TBC), 1–35. DOI TBC\nCopyright © the author(s); published under a Creative Commons NonCommercial 4.0 License (CC BY-NC 4.0).\nSACJ is a publication of the South African Institute of Computer Scientists and Information Technologists. ISSN\n1015-7999 (print) ISSN 2313-7835 (online).\narXiv:2011.13577v1  [cs.LG]  27 Nov 2020\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n2\ngames [5, 6], traﬃc light control [7] and more. In 2016, RL came into the general spotlight when\nGoogle DeepMind’s AlphaGo [8] program defeated the Go world champion, Lee Sedol. Even more\nrecently, Google DeepMind’s AlphaStar AI program defeated professional StarCraft II players\n(considered to be one of the most challenging real-time strategy games) and OpenAI Five defeated\nprofessional Dota 2 players.\nProgress in machine learning is driven by new algorithm development and the availability of\nhigh-quality data. In supervised and unsupervised machine learning ﬁelds, resources such as the\nUCI Machine Learning repository1, the Penn Treebank [9], the MNIST database of handwritten\ndigits2, the ImageNet large scale visual recognition challenge [10], and Pascal Visual Object\nClasses [11] are available. In contrast to the datasets used in supervised and unsupervised machine\nlearning, progress in RL is instead driven by research on agent behaviour within challenging\nenvironments. Games have been used for decades to test and evaluate the performance of artiﬁcial\nintelligence systems. Many of the benchmarks that are available for RL are also based on games,\nsuch as the Arcade Learning Environment for Atari 2600 games [12] but others involve tasks that\nsimulate real-world situations, such as locomotion tasks in Garage (originally rllab) [13]. These\nbenchmarking tasks have been used extensively in research and signiﬁcant progress has been made\nin using RL in ever more challenging domains.\nBenchmarks and standardised environments are crucial in facilitating progress in RL. One\nadvantage of the use of these benchmarking tasks is the reproducibility and comparison of\nalgorithms to state-of-the-art RL methods. Progress in the ﬁeld can only be sustained if existing\nwork can be reproduced and accurately compared to judge improvements of new methods [14, 15].\nThe existence of standardised tasks can facilitate accurate benchmarking of RL performance.\nThis paper provides a survey of the most important and most recent contributions to bench-\nmarking for RL. These are OpenAI Gym [16], the Arcade Learning Environment [12], a continuous\ncontrol benchmark rllab [13], RoboCup Keepaway soccer [17] and Microsoft TextWorld [18]. When\nsolving RL problems, there are many challenges that need to be overcome, such as the fundamental\ntrade-oﬀproblem between exploration and exploitation, partial observability of the environment,\ndelayed rewards, enormous state spaces and so on. This paper discusses these challenges in terms\nof important RL benchmarking contributions and in what manner the benchmarks can be used to\novercome or address these challenges.\nThe rest of the paper is organised as follows. Section 2 introduces the key concepts and\nterminology of RL, and then discusses the approaches to solving RL problems and the challenges\nfor RL. Section 3 provides a survey on the contributions to RL benchmarking and Section 4\ndiscusses the ways that the diﬀerent contributions to RL benchmarking deal with or contribute to\nthe challenges for RL. A conclusion follows in Section 5.\n1http://archive.ics.uci.edu/ml/index.php\n2http://yann.lecun.com/exdb/mnist/\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n3\nAgent\nEnvironment\nAction at\nNew state st+1\nReward rt+1\nFigure 1: Illustration of an RL system.\n2\nREINFORCEMENT LEARNING\nRL focuses on training an agent by using a trial-and-error approach. Figure 1 illustrates the\nworkings of an RL system. The agent evaluates a current situation (state), takes an action, and\nreceives feedback (reward) from the environment after each act. The agent is rewarded with either\npositive feedback (when taking a “good” action) or negative feedback as punishment for taking a\n“bad” action. An RL agent learns how to act best through many attempts and failures. Through\nthis type of trial-and-error learning, the agent’s goal is to receive the best so-called long-term\nreward. The agent gets short-term rewards that together lead to the cumulative, long-term reward.\nThe key goal of RL is to deﬁne the best sequence of actions that allow the agent to solve a problem\nwhile maximizing its cumulative long-term reward. That set of optimal actions is learned through\nthe interaction of the agent with its environment and observation of rewards in every state.\nThis section provides the key concepts and terminology of RL used throughout this paper.\nThe challenges of RL are also discussed.\n2.1\nConcepts and terminology\nThe core idea behind RL is to learn from the environment through interactions and feedback,\nand ﬁnd an optimal strategy for solving the problem. The agent takes actions in its environment\nbased on a (possibly partial) observation of the state of the environment and the environment\nprovides a reward for the actions, which is usually a scalar value. The set of all valid actions is\nreferred to as the action space, which can be either discrete (as in Atari and Go) or continuous\n(controlling a robot in the physical world). The goal of the agent is to maximise its long-term\ncumulative reward.\n2.1.1\nPolicy\nA policy of an agent is the control strategy used to make decisions, and is a mapping from states\nto actions. A policy can be deterministic or stochastic and is denoted by π. A deterministic policy\nmaps states to actions without uncertainty while a stochastic policy is a probability distribution\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n4\nover actions for a given state. Therefore, when an agent follows a deterministic policy it will\nalways take the same action for a given state, whereas a stochastic policy may take diﬀerent\nactions in the same state. The immediate advantage of a stochastic policy is that an agent is not\ndoomed to repeat a looped sequence of non-advancing actions.\n2.1.2\nOn-policy and oﬀ-policy learning\nThere are two types of policy learning methods. On-policy learning is when the agent “learns\non the job”, i.e. it evaluates or improves the policy that is used to make the decisions directly.\nOﬀ-policy learning is when the agent learns one policy, called the target policy, while following\nanother policy, called the behaviour policy, which generates behaviour. The oﬀ-policy learning\nmethod is comparable to humans learning a task by observing others performing the task.\n2.1.3\nValue functions\nHaving a value for a state (or state-action pair) is often useful in guiding the agent towards the\noptimal policy. The value under policy π is the expected return if the agent starts in a speciﬁc\nstate or state-action pair, and then follows the policy thereafter. So the state-value function vπ is\na mapping from states to real numbers and represents the long-term reward obtained by starting\nfrom a particular state and executing policy π. The action-value function qπ is a mapping from\nstate-action pairs to real numbers. The action-value qπ(s, a) of state s and action a (where a\nis an arbitrary action and not necessarily in line with the policy) is the expected return from\nstarting in state s, taking action a and then following policy π. The optimal value function v∗\ngives the expected return starting in a state and then following the optimal policy π∗. The optimal\naction-value function q∗is the expected return starting in some state, taking an arbitrary action\nand then following the optimal policy π∗.\nThese state-value and action-value functions all obey so-called Bellman equations, where the\nidea is that the value of the agent’s starting point is the reward that is expected to be obtained\nfrom being there, plus the value of wherever the agent lands next. These Bellman equations are\nused in most RL approaches where the Bellman-backup is used, i.e. for a state or state-action\npair the Bellman-backup is the (immediate) reward plus the next value.\n2.1.4\nFunction approximators\nIn many RL problems the state space can be extremely large. Traditional solution methods where\nvalue functions are represented as arrays or tables mapping all states to values are therefore very\ndiﬃcult [1]. One approach to this shortcoming is to use features to generalise an estimation of\nthe value of states with similar features. Methods that compute these approximations are called\nfunction approximators. There are many techniques used for implementing function approximators\nincluding linear combinations of features, neural networks, decision trees, nearest neighbours, etc.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n5\n2.1.5\nMonte Carlo methods\nMonte Carlo methods are a class of learning methods where value functions are learned [1]. The\nvalue of a state, si, is estimated by running many trials starting from si and then averaging the\ntotal rewards received on those trials.\n2.1.6\nTemporal diﬀerence algorithms\nTemporal diﬀerence (TD) learning algorithms are a class of learning methods that are based on the\nidea of comparing temporally successive predictions [1]. These methods are a fundamental idea in\nRL and use a combination of Monte Carlo learning and dynamic programming [1]. TD methods\nlearn value functions directly from experience by using the so-called TD error and bootstrapping\n(not waiting for a ﬁnal outcome).\n2.1.7\nMarkov decisions processes\nThe standard formalism for RL settings is called a Markov decision process (MDP). MDPs are\nused to deﬁne the interaction between an agent and its environment in terms of states, actions, and\nrewards. For an RL problem to be an MDP, it has to satisfy the Markov property: “The future is\nindependent of the past given the present”. This means that once the current state is known, then\nthe history encountered so far can be discarded and that state completely characterises all the\ninformation needed as it captures all the relevant information from the history. Mathematically,\nan MDP is a tuple: ⟨S, A, R, P, γ⟩, where S is a (ﬁnite) set of states, A is a (ﬁnite) set of actions,\nR : S × A × S →R is the reward function, P is a state transition probability matrix and γ ∈[0, 1]\nis a discount factor included to control the reward.\n2.1.8\nModel-free and model-based reinforcement learning approaches\nThere are diﬀerent aspects of RL systems that can be learnt. These include learning policies (either\ndeterministic or stochastic), learning action-value functions (so-called Q-functions or Q-learning),\nlearning state-value functions, and/or learning a model of the environment. A model of the\nenvironment is a function that predicts state transitions and rewards, and is an optional element of\nan RL system. If a model is available, i.e. if all the elements of the MDP are known, particularly\nthe transition probabilities and the reward function, then a solution can be computed using classic\ntechniques before executing any action in the environment. This is known as planning: computing\nthe solution to a decision-making problem before executing an actual decision.\nWhen an agent does not know all the elements of the MDP, then the agent does not know how\nthe environment will change in response to its actions or what its immediate reward will be. In\nthis situation the agent will have to try out diﬀerent actions, observe what happens and in some\nway ﬁnd a good policy from doing this. One approach to solve a problem without a complete\nmodel is for the agent to learn a model of how the environment works from its observations and\nthen plan a solution using that model. Methods that use the framework of models and planning\nare referred to as model-based methods.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n6\nAnother way of solving RL problems without a complete model of the environment is to learn\nthrough trial-and-error. Methods that do not have or learn a model of the environment and do\nnot use planning are called model-free methods. The two main approaches to represent and train\nagents with model-free RL are policy optimisation and Q-learning. In policy optimisation methods\n(or policy-iteration methods) the agent learns the policy function directly. Examples include policy\ngradient methods, asynchronous advantage actor-critic (A3C) [19], trust region policy optimization\n(TRPO) [20] and proximal policy optimization (PPO) [21]. Q-Learning methods include deep\nQ-networks (DQN) [3], C51 algorithm [22] and Hindsight Experience Replay (HER) [23]. Hybrid\nmethods combining the strengths of Q-learning and policy gradients exist as well, such as deep\ndeterministic policy gradients (DDPG) [2], soft actor-critic algorithm (SAC) [24] and twin delayed\ndeep deterministic policy gradients (TD3) [25].\nIn the current literature, the most used approaches incorporates a mixture of model-based\nand model-free methods, such as Dyna and Monte Carlo tree search (MCTS) [1], and temporal\ndiﬀerence search [26].\n2.2\nChallenges for reinforcement learning\nThis section discusses some of the challenges faced by RL. These challenges will be discussed in\nterms of how they are addressed by diﬀerent contributions in Section 4.\n2.2.1\nPartially observable environment\nHow the agent observes the environment can have a signiﬁcant impact on the diﬃculty of the\nproblem. In most real-world environments the agent does not have a complete or perfect perception\nof the state of its environment due to incomplete information provided by its sensors, the sensors\nbeing noisy or some of the state being hidden. However, for learning methods that are based on\nMDPs, the complete state of the environment should be known. To address the problem of partial\nobservability of the environment, the MDP framework is extended to the partially observable\nMarkov decision process (POMDP) model.\n2.2.2\nDelayed or sparse rewards\nIn an RL problem, an agent’s actions determine its immediate reward as well as the next state of\nthe environment. Therefore, an agent has to take both these factors into account when deciding\nwhich action to take in any state. Since the goal is to learn which actions to take that will give\nthe most reward in the long-run, it can become challenging when there is little or no immediate\nreward. The agent will consequently have to learn from delayed reinforcement, where it may take\nmany actions with insigniﬁcant rewards to reach a future state with full reward feedback. The\nagent must therefore be able to learn which actions will result in an optimal reward, which it\nmight only receive far into the future.\nIn line with the challenge of delayed or sparse rewards is the problem of long-term credit\nassignment [27]: how must credit for success be distributed among the sequence of decisions that\nhave been made to produce the outcome?\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n7\n2.2.3\nUnspeciﬁed or multi-objective reward functions\nMany tasks (especially real-world problems) have multiple objectives. The goal of RL is to optimise\na reward function, which is commonly framed as a global reward function, but tasks with more\nthan one objective could require optimisation of diﬀerent reward functions. In addition, when an\nagent is training to optimise some objective, other objectives could be discovered which might\nhave to be maintained or improved upon. Work on multi-objective RL (MORL) has received\nincreased interest, but research is still primarily devoted to single-objective RL.\n2.2.4\nSize of the state and action spaces\nLarge state and action spaces can result in enormous policy spaces in RL problems. Both state\nand action spaces can be continuous and therefore inﬁnite. However, even discrete states and\nactions can lead to infeasible enumeration of policy/state-value space. In RL problems for which\nstate and/or action spaces are small enough, so-called tabular solutions methods can be used,\nwhere value functions can be represented as arrays or tables and exact solutions are often possible.\nFor RL problems with state and/or action spaces that are too large, the goal is to instead ﬁnd\ngood approximate solutions with the limited computational resources available and to avoid the\ncurse of dimensionality [28].\n2.2.5\nThe trade-oﬀbetween exploration and exploitation\nOne of the most important and fundamental overarching challenges in RL is the trade-oﬀbetween\nexploration and exploitation. Since the goal is to obtain as much reward as possible, an agent has\nto learn to take actions that were previously most eﬀective in producing a reward. However, to\ndiscover these desirable actions, the agent has to try actions that were not tried before. It has to\nexploit the knowledge of actions that were already taken, but also explore new actions that could\npotentially be better selections in the future. The agent may have to sacriﬁce short-term gains to\nachieve the best long-term reward. Therefore, both exploration and exploitation are fundamental\nin the learning process, and exclusive use of either will result in failure of the task at hand. There\nare many exploration strategies [1], but a key issue is the scalability to more complex or larger\nproblems. The exploration vs. exploitation challenge is aﬀected by many of the other challenges\nthat are discussed in this section, such as delayed or sparse rewards, and the size of the state or\naction spaces.\n2.2.6\nRepresentation learning\nRepresentation (or feature) learning involves automatically extracting features or understanding\nthe representation of raw input data to perform tasks such as classiﬁcation or prediction. It is\nfundamental not just to RL, but to machine learning and AI in general, even with a conference\ndedicated to it: International Conference on Learning Representations (ICLR).\nOne of the clearest challenges that representation learning tries to solve in an RL context is to\neﬀectively reduce the impact of the curse of dimensionality, which results from very large state\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n8\nand/or action spaces. Ideally an eﬀective representation learning scheme will be able to extract\nthe most important information from the problem input in a compressed form.\n2.2.7\nTransfer learning\nTransfer learning [29, 30] uses the notion that, as in human learning, knowledge gained from a\nprevious task can improve the learning in a new (related) task through the transfer of knowledge\nthat has already been learned. The ﬁeld of transfer learning has recently been experiencing growth\nin RL [31] to accelerate learning and mitigate issues regarding scalability.\n2.2.8\nModel learning\nModel-based RL methods (Section 2.1.8) are important in problems where the agent’s interactions\nwith the environment are expensive. These methods are also signiﬁcant in the trade-oﬀbetween\nexploration and exploitation, since planning impacts the need for exploration. Model learning can\nreduce the interactions with the environment, something which can be limited in practice, but\nintroduces additional complexities and the possibility of model errors. Another challenge related\nto model learning is the problem of planning using an imperfect model, which is also a diﬃcult\nchallenge that has not received much attention in the literature.\n2.2.9\nOﬀ-policy learning\nOﬀ-policy learning methods (e.g. Q-learning) scale well in comparison to other methods and the\nalgorithms can (in principle) learn from data without interacting with the environment. An agent\nis trained using data collected by other agents (oﬀ-policy data) and data it collects itself to learn\ngeneralisable skills.\nDisadvantages of oﬀ-policy learning methods include greater variance and slow convergence,\nbut are more powerful and general than on-policy learning methods [1]. Advantages of using\noﬀ-policy learning is the use of a variety of exploration strategies, and learning from training data\nthat are generated by unrelated controllers, which includes manual human control and previously\ncollected data.\n2.2.10\nReinforcement learning in real-world settings\nThe use of RL in real-world scenarios has been gaining attention due to the success of RL in\nartiﬁcial domains. In real-world settings, more challenges become apparent for RL. Dulac-Arnold\net al. [32] provide a list of nine challenges for RL in the real-world, many of which have been\nmentioned in this section already. Further challenges not discussed here include safety constraints,\npolicy explainability and real-time inference. Many of these challenges have been studied extensively\nin isolation, but there is a need for research on algorithms (both in artiﬁcial domains and real-world\nsettings) that addresses more than one or all of these challenges together, since many of the\nchallenges are present in the same problem.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n9\n2.2.11\nA standard methodology for benchmarking\nA diverse range of methodologies is currently common in the literature, which brings into question\nthe validity of direct comparisons between diﬀerent approaches. A standard methodology for\nbenchmarking is necessary for the research community to compare results in a valid way and\naccelerate advancement in a rigorous scientiﬁc manner.\n3\nCONTRIBUTIONS TO REINFORCEMENT LEARNING BENCHMARKING\nThis section discusses some important reinforcement learning benchmarks currently in use. The\nlist of contributions is by no means exhaustive, but includes the ones that are most in use currently\nin the RL research community.\n3.1\nOpenAI Gym\nReleased publicly in April 2016, OpenAI’s Gym [16] is a toolkit for developing and comparing\nreinforcement learning algorithms.\nIt includes a collection of benchmark problems which is\ncontinuing to grow as well as a website where researchers can share their results and compare\nalgorithm performance. It provides a tool to standardise reporting of environments in research\npublications to facilitate the reproducibility of published research. OpenAI Gym has become very\npopular since its release, with [16] having over 1300 citations on Google Scholar to date.\n3.1.1\nImplementation\nThe OpenAI Gym library is a collection of test problems (environments) with a common interface\nand makes no assumptions about the structure of an agent. OpenAI Gym currently supports\nLinux and OS X running Python 2.7 or 3.5 – 3.7. Windows support is currently experimental, with\nlimited support for some problem environments. OpenAI Gym is compatible with any numerical\ncomputation library, such as TensorFlow or Theano. To get started with OpenAI Gym, visit the\ndocumentation site3 or the actively maintained GitHub repository4.\n3.1.2\nBenchmark tasks\nThe environments available in the library are diverse, ranging from easy to diﬃcult and include a\nvariety of data. A brief overview of the diﬀerent environments is provided here with the full list\nand descriptions of environments available on the main site3.\nClassic control and toy text: These small-scale problems are a good starting point for\nresearchers not familiar with the ﬁeld. The classic control problems include balancing a pole on a\nmoving cart (Figure 2a), driving a car up a steep hill, swinging a pendulum and more. The toy\ntext problems include ﬁnding a safe path across a grid of ice and water tiles, playing Roulette,\nBlackjack and more.\n3https://gym.openai.com\n4https://github.com/openai/gym\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n10\nAlgorithmic: The objective here is for the agent to learn algorithms such as adding multi-digit\nnumbers and reversing sequences, purely from examples. The diﬃculty of the tasks can be varied\nby changing the sequence length.\nAtari 2600: The Arcade Learning Environment (ALE) [12] has been integrated into OpenAI\nGym in easy-to-install form, where classic Atari 2600 games (see Figure 2b for an example) can\nbe used for developing agents (see Section 3.2 for a detailed discussion). For each game there are\ntwo versions: a version which takes the RAM as input and a version which takes the observable\nscreen as the input.\nMuJoCo: These robot simulation tasks use the MuJoCo proprietary software physics engine\n[33], but free trial and postgraduate student licences are available. The problems include 3D\nrobot walking or standing up tasks, 2D robots running, hopping, swimming or walking (see\nFigure 2c for an example), balancing two poles vertically on top of each other on a moving cart,\nand repositioning the end of a two-link robotic arm to a given spot.\nBox2D: These are continuous control tasks in the Box2D simulator, which is a free open source\n2-dimensional physics simulator engine. Problems include training a bipedal robot (Figure 2d) to\nwalk (even on rough terrain), racing a car around a track and navigating a lunar lander to its\nlanding pad.\nRoboschool: Most of these problems are the same as in MuJoCo, but use the open-source\nsoftware physics engine, Bullet. Additional tasks include teaching a 3D humanoid robot to walk\nas fast as possible (see Figure 2e) as well as a continuous control version of Atari Pong.\nRobotics: Released in 2018, these environments are used to train models which work on\nphysical robots.\nIt includes four environments using the Fetch5 research platform and four\nenvironments using the ShadowHand6 robot. These manipulation tasks are signiﬁcantly more\ndiﬃcult than the MuJoCo continuous control environments. The tasks for the Fetch robot are to\nmove the end-eﬀector to a desired goal position, hitting a puck across a long table such that it\nslides and comes to rest on the desired goal, moving a box by pushing it until it reaches a desired\ngoal position, and picking up a box from a table using its gripper and moving it to a desired goal\nabove the table. The tasks for the ShadowHand are reaching with its thumb and a selected ﬁnger\nuntil they meet at a desired goal position above the palm, manipulating a block (see Figure 2f),\nan egg, and a pen, until the object achieves a desired goal position and rotation.\nAlongside these new robotics environments, OpenAI also released code for Hindsight Experience\nReplay (HER), a reinforcement learning algorithm that can learn from failure. Their results show\nthat HER can learn successful policies on most of the new robotics problems from only sparse\nrewards. A set of requests for research has also been released7 in order to encourage and facilitate\nresearch in this area, with a few ideas of ways to improve HER speciﬁcally.\n5https://fetchrobotics.com/\n6https://www.shadowrobot.com/products/dexterous-hand/\n7https://openai.com/blog/ingredients-for-robotics-research/\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n11\n(a) A screenshot of the classic con-\ntrol task Cart-Pole, with the objec-\ntive to keep the pole balanced by\nmoving the cart.\n(b) A screenshot of the Atari 2600\ngame Breakout.\n(c) A screenshot of the MuJoCo\nsimulator, where a four-legged 3D\nrobot has to learn to walk.\n(d) A screenshot of the Box2D sim-\nulator, where a bipedal robot has\nto learn to walk.\n(e) A screenshot of the 3D hu-\nmanoid robot learning to walk as\nfast as possible in the Roboschool\nsimulator.\n(f) A screenshot of the Shadow-\nHand robot manipulating a block.\nFigure 2: Some examples of the environments used in OpenAI Gym.\n3.2\nThe Arcade Learning Environment\nThe Atari 2600 gaming console was released in September 1977, with over 565 games developed\nfor it over many diﬀerent genres. The games are considerably simpler than modern era video\ngames. However, the Atari 2600 games are still challenging and provide interesting tasks for\nhuman players.\nThe Arcade Learning Environment (ALE) [12] is an object-oriented software framework allowing\nresearchers to develop AI agents for the original Atari 2600 games. It is a platform to empirically\nassess and evaluate AI agents designed for general competency. ALE allows interfacing through\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n12\nthe Atari 2600 emulator Stella and enables the separation of designing an AI agent and the details\nof emulation. There are currently over 50 game environments supported in the ALE.\nThe ALE has received a lot of attention since its release in 2013 (over 1200 citations on Google\nScholar to date), perhaps the most note-worthy being the success of Deep Q-networks (DQN),\nwhich was the ﬁrst algorithm to achieve human-level control performance in many of the Atari\n2600 games [4].\n3.2.1\nImplementation\nThe Stella emulator interfaces with the Atari 2600 games by receiving joystick movements and\nsending screen and/or RAM information to the user. For the reinforcement learning context, ALE\nhas a game-handling layer to provide the accumulated score and a signal for whether the game has\nended. The default observation of a single game screen or frame is made up of a two-dimensional\narray of 7-bit pixels, 160 pixels wide by 210 pixels high. The joystick controller deﬁnes 18 discrete\nactions, which makes up the action space of the problem. Only some actions are needed to play a\ngame and the game-handling layer also provides the minimum set of actions needed to play any\nparticular game. The simulator generates 60 frames per second in real-time and up to 6000 frames\nper second at full speed. The reward the agent receives depends on each game, but is generally\nthe score diﬀerence between frames. A game episode starts when the ﬁrst frame is shown and\nends when the goal of the game has been achieved or after a predeﬁned number of frames. The\nALE therefore oﬀers access to a variety of games through one common interface.\nThe ALE also has the functionality of saving and restoring the current state of the emula-\ntor. This functionality allows the investigation of topics including planning and model-based\nreinforcement learning.\nALE is free, open-source software8, including the source code for the agents used in associated\nresearch studies [12]. ALE is written in C++, but there are many interfaces available that allow\nthe interaction with ALE in other programming languages, with detail provided in [12].\nDue to the increase in popularity and importance in the AI literature, another paper was\npublished in 2018 by some of the original proposers of the ALE [15], providing a broad overview\nof how the ALE is used by researchers, highlighting overlooked issues and discussing propositions\nfor maximising the future use of the testbed. Concerns are raised at how agents are evaluated in\nthe ALE and new benchmark results are provided.\nIn addition, a new version of the ALE was introduced in 2018 [15], which supports multiple\ngame modes and includes so called sticky actions, providing some form of stochasticity to the\ncontroller. When sticky actions are used, there is a possibility that the action requested by the\nagent is not executed, but instead the agent’s previous action is used, emulating a sticky controller.\nThe probability that an action will be sticky can be speciﬁed using a pre-set control parameter.\nThe original ALE is fully deterministic and consequently it is possible for an agent to memorise a\ngood action sequence, instead of learning how to make good decisions. Introducing sticky actions\ntherefore increases the robustness of the policy that the agent has to learn.\n8http://arcadelearningenvironment.org\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n13\nOriginally the ALE only allowed agents to play games in their default mode and diﬃculty. In\nthe latest version of the ALE [15] it is possible to select among diﬀerent game modes and diﬃculty\nlevels for single player games, where each mode-diﬃculty pair is referred to as a ﬂavour. Changes\nin the mode and diﬃculty of the games can impact game dynamics and introduce new actions.\n3.2.2\nPublished benchmark results\nBellemare et al. [12] provide performance results on the ALE tasks using an augmented version of\nthe SARSA(λ) [1] algorithm, where linear function approximation is used. For comparison, the\nperformance results of a non-expert human player and three baseline agents (Random, Const and\nPerturb) are also provided. A set of games is used for training and parameter tuning, and another\nset for testing. The ALE can also be used to study planning techniques. Benchmark results for\ntwo traditional search methods (Breadth-ﬁrst search and UCT: Upper Conﬁdence Bounds Applied\nto Trees) are provided, as well as the performance results of the best learning agent and the best\nbaseline policy.\nMachado et al. [15] provide benchmark results for 60 Atari 2600 games with sticky actions for\nDQN and SARSA(λ) + Blob-PROST [34] (an algorithm that includes a feature representation\nwhich enables SARSA(λ) to achieve performance that is comparable to that of DQN).\n3.3\nContinuous control: rllab\nThe Arcade Learning Environment (Section 3.2) is a popular benchmark to evaluate algorithms\nwhich are designed for tasks with discrete actions. Duan et al. [13] present a benchmark of 31\ncontinuous control tasks, ranging in diﬃculty, and also implement a range of RL algorithms on\nthe tasks.\nThe benchmark as well as the implementations of the algorithms are available at the rllab\nGitHub repository9, however this repository is no longer under development but is currently\nactively maintained at the garage GitHub repository10, which includes many improvements. The\ndocumentation11 for garage is a work in progress and the available documentation is currently\nlimited. Both rllab and garage are fully compatible with OpenAI Gym and only support Python\n3.5 and higher.\nOther RL benchmarks for continuous control have also been proposed, but many are not in use\nanymore. Duan et al. [13] provide a comprehensive list of benchmarks containing low-dimensional\ntasks as well as a wide range of tasks with high-dimensional continuous state and action spaces.\nThey also discuss previously proposed benchmarks for high-dimensional control tasks do not\ninclude such a variety of tasks as in rllab. Where relevant, we mention some of these benchmarks\nin the next section that have additional interesting tasks.\n9https://github.com/rll/rllab\n10https://github.com/rlworkgroup/garage\n11https://garage.readthedocs.io/en/latest/\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n14\n3.3.1\nBenchmark tasks\nThere are four categories for the rllab continuous control tasks: basic, locomotion, partially\nobservable and hierarchical tasks.\nBasic tasks: These ﬁve tasks are widely analysed in the reinforcement learning and control\nliterature. Some of these tasks can also be found in the “Classic control” section of OpenAI Gym\n(Section 3.1). The tasks are cart-pole balancing, cart-pole swing up, mountain car, acrobot swing\nup and double inverted pendulum balancing (which can be found in OpenAI Gym Roboschool).\nA related benchmark involving a 20 link pole balancing task is proposed as part of the Tdlearn\npackage [35].\nLocomotion tasks: Six locomotion tasks of varying dynamics and diﬃculty are implemented\nwith the goal to move forward as quickly as possible. These tasks are challenging due to high\ndegrees of freedom as well as the need for a lot of exploration, since getting stuck at a local optima\n(such as staying at the origin or diving forward slowly) can happen easily when the agent acts\ngreedily. These tasks are: Swimmer, Hopper, Walker, Half-Cheetah, Ant, Simple Humanoid and\nFull Humanoid.\nOther environments with related locomotion tasks include dotRL [36] with a variable segment\noctopus arm [37], PyBrain [38], and SkyAI [39] with humanoid robot tasks like jumping, crawling\nand turning.\nPartially observable tasks: Realistic agents often do not have access to perfect state\ninformation due to limitations in sensory input. To address this, three variations of partially\nobservable tasks are implemented for each of the ﬁve basic tasks mentioned above. This leads to 15\nadditional tasks. The three variations are limited sensors (only positional information is provided,\nno velocity), noisy observations and delayed actions (Gaussian noise is added to simulate sensor\nnoise, and a time delay is added between taking an action and an action being executed) and\nsystem identiﬁcation (the underlying physical model parameters vary across diﬀerent episodes).\nThese variations are not currently available in OpenAI Gym.\nHierarchical tasks: In many real-world situations higher level decisions can reuse lower level\nskills, for example a robot learning to navigate a maze can reuse learned locomotion skills. Here\ntasks are proposed where low-level motor controls and high-level decisions are needed, which\noperate on diﬀerent time scales and a natural hierarchy exists in order to learn the task most\neﬃciently. The tasks are as follows. Locomotion and food collection: where the swimmer or the\nant robot operates in a ﬁnite region and the goal is to collect food and avoid bombs. Locomotion\nand maze: the swimmer or the ant robot has the objective to reach a speciﬁc goal location in a\nﬁxed maze environment. These tasks are not currently available in OpenAI Gym.\n3.3.2\nPublished benchmark results\nDuan et al. [13] provide performance results on the rllab tasks. The algorithms implemented\nare mainly gradient-based policy search methods, but two gradient-free methods are included for\ncomparison. Almost all of the algorithms are batch algorithms and one algorithm is an online\nalgorithm. The batch algorithms are REINFORCE [40], truncated natural policy gradient (TNPG)\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n15\n[13], reward-weighted regression (RWR) [41], relative entropy policy search (REPS) [42], trust\nregion policy optimization (TRPO) [20], cross entropy method (CEM) [43] and covariance matrix\nadaptation evolution strategy (CMA-ES) [44]. The online algorithm used is deep deterministic\npolicy gradient (DDPG) [2]. Direct applications of the batch-based algorithms to recurrent policies\nare implemented with minor modiﬁcations.\nOf the implemented algorithms, TNPG, TRPO and DDPG were eﬀective in training deep\nneural network policies. However, all algorithms performed poorly on the hierarchical tasks, which\nsuggest that new algorithms should be developed for automatic discovery and exploitation of the\ntasks’ hierarchical structure.\nRecently a new class of reinforcement learning algorithms called proximal policy optimisation\n(PPO) [21] was released by OpenAI. PPO’s performance is comparable or better than state-of-the-\nart approaches to solving 3D locomotion, robotic tasks (similar to the tasks in the benchmark\ndiscussed above) and also Atari 2600, but it is simpler to implement and tune. OpenAI has\nadopted PPO as its go-to RL algorithm, since it strikes a balance between ease of implementation,\nsample complexity, and ease of tuning.\n3.4\nRoboCup Keepaway Soccer\nRoboCup [45] simulated soccer has been used as the basis for successful international competitions\nand research challenges since 1997. Keepaway is a subtask of RoboCup that was put forth as a\ntestbed for machine learning in 2001 [17]. It has since been used for research on temporal diﬀerence\nreinforcement learning with function approximation [46], evolutionary learning [47], relational\nreinforcement learning [48], behaviour transfer [49, 50, 51, 52, 53, 54, 55], batch reinforcement\nlearning [56] and hierarchical reinforcement learning [57].\nIn Keepaway, one team (the keepers) tries to maintain possession of the ball within a limited\nregion, while the opposing team (the takers) attempts to gain possession [17]. The episode ends\nwhenever the takers take possession of the ball or the ball leaves the region. The players are then\nreset for another episode with the keepers being given possession of the ball again. Task parameters\ninclude the size of the region, the number of keepers, and the number of takers. Figure 3 shows\nan example episode with 3 keepers and 2 takers (called 3v2) playing in a 20m × 20m region [17].\nIn 2005 Stone et al. [58] elevated the Keepaway testbed to a benchmark problem for machine\nlearning and provided infrastructure to easily implement the standardised task.\nAn advantage of the Keepaway subtask is that it allows for direct comparison of diﬀerent\nmachine learning algorithms. It is also good for benchmarking machine learning since the task is\nsimple enough to be solved successfully, but complex enough that straightforward solutions are\nnot suﬃcient.\n3.4.1\nImplementation\nA standardized Keepaway player framework is implemented in C++ and the source code is\navailable for public use at an online repository12. The repository provides implementation for\n12http://www.cs.utexas.edu/∼AustinVilla/sim/keepaway/\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n16\nall aspects of the Keepaway problem except the learning algorithm itself. It also contains a\nstep-by-step tutorial of how to use the code, with the goal of allowing researchers who are not\nexperts in the RoboCup simulated soccer domain to easily become familiar with the domain.\n3.4.2\nStandardised task\nFigure 3: A screen shot from a 3v2 keepaway episode\nin a 20m × 20m region from [17].\nRobocup simulated soccer (and therefore also\nKeepaway) is a fully distributed, multiagent\ndomain with both teammates and adversaries\n[59]. The environment is partially observable\nfor each agent and the agents also have noisy\nsensors and actuators. Therefore, the agents\ndo not perceive the world exactly as it is, nor\ncan they aﬀect the world exactly as intended.\nThe perception and action cycles of the agent\nare asynchronous, therefore perceptual input\ndoes not trigger actions as is traditional in AI.\nCommunication opportunities are limited, and\nthe agents must make their decisions in real-\ntime. These domain characteristics all result in\nsimulated robotic soccer being a realistic and\nchallenging domain [59].\nThe size of the Keepaway region, the num-\nber of keepers, and the number of takers can\neasily be varied to change the task. Stone et al.\n[58] provide a framework with a standard interface to the learner in terms of macro-actions, states,\nand rewards.\n3.4.3\nPublished benchmark results\nStone et al. [58] performed an empirical study for learning Keepaway by training the keepers using\nepisodic SMDP SARSA(λ) [46, 1], with three diﬀerent function approximators: CMAC function\napproximation [60, 61], Radial Basis Function (RBF) [1] networks (a novel extension to CMACs\n[58]), and neural network function approximation. The RBF network performed comparably to the\nCMAC method. The Keepaway benchmark structure allows for these results to be quantitatively\ncompared to other learning algorithms to test the relative beneﬁts of diﬀerent techniques.\n3.4.4\nHalf Field Oﬀense: An extension to Keepaway\nHalf Field Oﬀense (HFO) [62, 63] is an extension of Keepaway, which is played on half of the\nsoccer ﬁeld with more players on each team. The task was originally introduced in 2007 [62], but\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n17\nno code was made publicly available. In 2016 [63] the HFO environment was released publicly\n(open-source)13, however this repository is not currently being maintained.\nSuccess in HFO means that the oﬀensive players have to keep possession of the ball (the same\nas in Keepaway), learn to pass or dribble to get closer to the goal and shoot when possible. Agents\ncan also play defence where they have to prevent goals from being scored. HFO also supports\nmulti-agents which could be controlled manually or automatically.\nIn the same way as the Keepaway environment [58], the HFO environment allows ease of use\nin developing and deploying agents in diﬀerent game scenarios, with C++ and Python interfaces.\nThe performance of three benchmark agents are compared in [63], namely a random agent, a\nhandcoded agent and a SARSA agent.\nA similar platform to the Arcade Learning Environment (Section 3.2), the HFO environment\nplaces less emphasis on generality (the main goal of the ALE) and more emphasis on cooperation\nand multiagent learning.\n3.5\nMicrosoft TextWorld\nRecently, researchers from the Microsoft Research Montreal Lab released an open source project\ncalled TextWorld [18], which attempts to train reinforcement learning agents using text-based\ngames.\nIn a time where AI agents are mastering complex multi-player games such as Dota 2 and\nStarCraft II, it might seem unusual to do research on text-based games. Text-based games can\nplay a similar role to multi-player graphic environments which train agents to learn spatial and\ntime-based planning, in advancing conversational skills such as aﬀordance extraction (identifying\nwhich verbs are applicable to a given object), memory and planning, exploration etc. Another\npowerful motivation for the interest in text-based games is that language abstracts away complex\nphysical processes, such as a robot trying not to fall over due to gravity. Text-based games require\nlanguage understanding and successful play requires skills like long-term memory and planning,\nexploration (trial and error), common sense, and learning with these challenges.\nTextWorld is a sandbox environment which enables users to handcraft or automatically generate\nnew games. These games are complex and interactive simulations where text is used to describe\nthe game state and players enter text commands to progress though the game. Natural language is\nused to describe the state of the world, to accept actions from the player, and to report subsequent\nchanges in the environment. The games are played through a command line terminal and are\nturn-based, i.e. the simulator describes the state of the game through text and then a player\nenters a text command to change its state in some desirable way.\n3.5.1\nImplementation\nIn Figure 4 an example game is shown in order to illustrate the command structure of a typical\ntext-based game generated by TextWorld.\n13https://github.com/LARG/HFO\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n18\nFigure 4: An example game generated by TextWorld to\nillustrate the command structure of a game.\nTextWorld enables interactive play-\nthrough of text-based games and, unlike\nother text-based environments such as\nTextPlayer14 and PyFiction15, enables\nusers to handcraft games or to construct\ngames automatically. The TextWorld\nlogic engine automatically builds game\nworlds, populates them with objects and\nobstacles, and generates quests that de-\nﬁne a goal state and how to reach it [18].\nTextWorld requires Python 3 and cur-\nrently only supports Linux and macOS\nsystems. The code and documentation\nare available publicly16 and the learning\nenvironment is described in full detail\nin Section 3 of [18], including descrip-\ntions of the two main components of the\nPython framework: a game generator\nand a game engine. To interact with\nTextWorld, the framework provides a\nsimple application programming inter-\nface (API) which is inspired by OpenAI\nGym.\nIn an RL context, TextWorld games\ncan be seen as partially observable\nMarkov decision processes. The envi-\nronment state at any turn t contains a complete description of the game state, but much of\nthis is hidden from the agent. Once an agent has issued a command (of at least one word), the\nenvironment transitions to a next state with a certain probability. Since the interpreter in parser-\nbased games can accept any sequence of characters (of any length), but only a fraction thereof is\nrecognised, the resulting action space is very large. Therefore, two simplifying assumptions are\nmade in [18]: the commands are sequences of at most L words taken from a ﬁxed vocabulary V\nand the commands have to follow a speciﬁc structure: a verb, a noun phrase and an adverb phrase.\nThe action space of the agent is therefore the set of all permissible commands from the ﬁxed\nvocabulary V followed by a certain special token (“enter”) that signiﬁes the end of the command.\nThe agent’s observation(s) at any time in the game is the text information perceived by the\nagent. A probability function takes in the environment state and selects what information to\nshow the agent based on the command entered. The agent receives points based on completion of\n14https://github.com/danielricks/textplayer\n15https://github.com/MikulasZelinka/pyﬁction\n16http://aka.ms/textworld\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n19\n(sub)quests and reaching new locations (exploring). This score could be used as the reward signal\nif it is available, otherwise positive reward signals can be assigned when the agent ﬁnishes the\ngame. The agent’s policy maps the state of the environment at any time and words generated in\nthe command so far to the next word, which needs to be added to the command to maximise the\nreward received.\n3.5.2\nBenchmark tasks\nTextWorld was introduced with two diﬀerent sets of benchmark tasks [18] and a third task was\nadded in the form of a competition that was available until 31 May 2019.\nTask 1: A preliminary set of 50 hand-authored benchmark games are described in the original\nTextWorld paper [18]. These games were manually analysed to ensure validity.\nTask 2: This benchmark task is inspired by a treasure hunter task which takes place in a 3D\nenvironment [64] and was adapted for TextWorld. The agent is randomly placed in a randomly\ngenerated map of rooms with two objects on the map. The goal object (the object which the\nagent should locate) is randomly selected and is mentioned in the welcome message. In order to\nnavigate the map and locate the goal object, the agent may need to complete other tasks, for\nexample ﬁnding a key to unlock a cabinet.\nThis task assesses the agent’s skills of aﬀordance extraction, eﬃcient navigation and memory.\nThere are diﬀerent levels for the benchmark, ranging from level 1 to 30, with diﬀerent diﬃculty\nmodes, number of rooms and quest length.\nTask 3: The TextWorld environment is still very new: TextWorld was only released to the\npublic in July 2018. A competition – First TextWorld Problems: A Reinforcement and Language\nLearning Challenge16, which ran until 31 May 2019, was launched by Microsoft Research Montreal\nto challenge researchers to develop agents that can solve these text-based games. The challenge is\ngathering ingredients to cook a recipe.\nAgents must determine the necessary ingredients from a recipe book, explore the house to\ngather ingredients, and return to the kitchen to cook up a delicious meal.\n3.5.3\nPublished benchmark results\nCˆot´e et al. [18] evaluate three baseline agents on the benchmark set in Task 1: BYU, Golovin and\nSimple. The BYU17 agent [65] utilises a variant of Q-learning [66] where word embeddings are\ntrained to be aware of verb-noun aﬀordances. The agent won the IEEE CIG Text-based adventure\nAI Competition in 2016. The Golovin18 agent [67] was developed speciﬁcally for classic text-based\ngames and uses a language model pre-trained on fantasy books to extract important keywords\nfrom scene descriptions. The Simple19 agent uniformly samples a command from a predeﬁned set\nat every step. Results indicated that all three baseline agents achieved low scores in the games.\nThis indicates that there is signiﬁcant scope for algorithms to improve on these results.\n17https://github.com/danielricks/BYU-Agent-2016\n18https://github.com/Kostero/text rpg ai\n19https://github.com/Microsoft/TextWorld/tree/master/notebooks\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n20\nCˆot´e et al. [18] also provide average performance results of three agents (BYU, Golovin and a\nrandom agent) on 100 treasure hunter games (task 2) at diﬀerent levels of diﬃculty. On diﬃculty\nlevel 1 the Golovin agents had the best average score, but the Random agent completed the game\nin the least number of steps. As the level of diﬃculty increase, the Random agent achieved the\nbest score and also completed the game in the least number of steps. These results can be used as\na baseline for evaluating improved algorithms.\nIt is evident that there is still enormous scope for research in the environment of text-based\ngames, and that the generative functionality of the TextWorld sandbox environment is a signiﬁcant\ncontribution in the endeavour of researchers trying to solve these problems.\n3.6\nSummary\nFor the reader’s convenience a summary of the discussed frameworks and algorithms that were\nshown to be eﬀective are presented in Table 1. It should be noted that since the ﬁeld moves at a\nrapid pace, the current state of the art will change (it may also be problem instance dependent\nwithin the benchmark class), however the listed algorithms can serve as a reasonable baseline for\nfuture research.\nFramework\nBenchmark class\nRecent eﬀective RL algorithm(s)\nOpenAI Gym\nAlgorithmic\nUREX [68]\nBox2D\nREINFORCE [69]\nClassic control\nTNPG and TRPO [13]\nMuJoCo\nPPO [21]\nRoboschool\nPPO [21]\nRobotics\nHER [23]7\nToy text\nBIRL [70]\nThe ALE\nAtari 2600\nA2C, ACER and PPO [21]; A3C [19];\nDistribution DQN, Dueling DDQN, Pri-\noritized DDQN and Rainbow [71] 20\nGarage\nBasic tasks\nTNPG and TRPO [13]\nLocomotion tasks\nPPO [21]\nPartially observable tasks\nTNPG and TRPO [13]\nHierarchical tasks\nHIRO [72]\nKeepaway soccer\nKeepaway\nEpisodic SMDP SARSA(λ) [46, 1]\nHalf-Field Oﬀence\nSARSA [63]\nTextWorld\nOriginal tasks 1, 2 and 3\nBYU and Golovin [18]\nGeneralisation tasks\nGATA [73]\nTable 1: A summary of recent algorithms that performed well in diﬀerent benchmark sets.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n21\n4\nDISCUSSION\nThis section focuses on the ways that the diﬀerent RL benchmarks discussed in Section 3 deal\nwith or facilitate research in addressing the challenges for RL discussed in Section 2.2.\n4.1\nPartially observable environment\nIn many of the benchmark tasks, such as the classic control tasks in OpenAI Gym, the agent is\nprovided with full information of the environment. The environment in TextWorld games, however,\nis partially observable since only local information and the player’s inventory are available. The\nagent might also not be able to distinguish between some states based on observations if only the\nlatest observation is taken into account, i.e. knowledge of past observations are important. In\nTextWorld games the environment might provide the same feedback for diﬀerent commands and\nsome important information about certain aspects of the environment might not be available by a\nsingle observation. Additionally, the agent might encounter observations that are time-sensitive,\nsuch as only being rewarded when it ﬁrst examines a clue but not any other time. Controlling the\npartial observability of the state is also part of TextWorld’s generative functionality. This is done\nby augmenting the agent’s observations, where the agent can be provided with a list of present\nobjects or even all game state information can be provided.\nThe partially observable tasks introduced in rllab (see Section 3.3.1), provide environments to\ninvestigate agents developed for dealing with environments where not all the information is known.\nIn RoboCup, a player can by default only observe objects in a 90-degree cone in front of them.\nIn works from Kuhlmann and Stone [74] and Stone et al. [46] it was shown that it is possible for\nlearning to occur in this limited vision scenario, however players do not perform at an adequate\nlevel. For this reason, players in the standardised Keepaway task [58] operate with 360-vision.\n4.2\nDelayed or sparse rewards\nThe tasks in the ALE and TextWorld are interesting when considering reward structure. In the\nALE, reward or feedback may only be seen after thousands of actions. In TextWorld, the agent\nhas to generate a sequence of actions before any change in the environment might occur or a\nreward is received. This results in sparse and delayed rewards in the games, in cases where an\nagent could receive a positive reward only after many steps when following an optimal strategy.\nIn Keepaway, there is immediate reward, since the learners receive a positive reward after each\naction they execute.\n4.3\nUnspeciﬁed or multi-objective reward functions\nIn HFO (Section 3.4.4) success not only includes maintaining possession of the ball (the main\nobjective in Keepaway), but the oﬀense players also need to learn to pass or dribble to move\n20A table summarising the best performance per game can be found at https://github.com/cshenton/atari-\nleaderboard.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n22\ntowards the goal and shoot when an angle is open. Moreover, success is only evaluated based on a\nscored goal at the end of an episode, which is rare initially. This aspect of HFO could serve as an\nideal environment for investigation into the challenge of problems with multi-objectives.\nDue the deﬁnition of a quest in TextWorld, i.e. a sequence of actions where each action depends\non the outcomes of the previous action, quests in TextWorld are limited to simple quests. However,\nin text adventure games, quests are often more complicated, involving multiple sub-quests. Cˆot´e\net al. [18] remark that this limitation could be overcome by treating a quest as a directed graph\nof dependent actions rather than a linear chain. If this can be incorporated in TextWorld in the\nfuture, the platform can also be used to study problems with multi-objectives and rewards of\nvarying diﬃculty.\n4.4\nSize of the state and action spaces\nThe benchmark tasks that are considered in this paper are ideal to investigate how the size of the\nstate and/or action space challenge can be addressed. The tasks considered all have continuous or\nlarge discrete state spaces.\nIn the ALE the number of states in the games are very large and in TextWorld the state space\nis combinatorially enormous; since the number of possible states increases exponentially with the\nnumber of rooms and objects [18]. In most of the tasks in OpenAI Gym, rllab, and in Keepaway,\nthe state space is continuous. In Keepaway, the size of the Keepaway region can be varied along\nwith the number of keepers and takers. This allows for investigation into a problem with various\ndiﬃculties due to the size of the state space.\nIn TextWorld, the action space is large and sparse because the set of all possible word strings\nis much larger than the subset of valid commands. TextWorld’s generative functionality also\nallows control over the size of the state space, i.e. the number of rooms, objects and commands.\nDiﬀerent problem diﬃculties can therefore arise in terms of the size of the state space and this\ncan aid in the investigation of algorithm behaviour with increasing state and action spaces.\n4.5\nThe trade-oﬀbetween exploration and exploitation\nIn the ALE the challenge of exploration vs. exploitation is diﬃcult due to the large state spaces\nof games and delayed reward. Simple agents sometimes even learn that staying put is the best\npolicy, since exploration can in some cases lead to negative rewards. Recently there has been some\neﬀort to address the exploration problem in the ALE, but these eﬀorts are mostly successful only\nin individual games.\nExploration is fundamental to TextWorld games as solving them can not be done by learning\na purely exploitative or reactive agent. The agent must use directed exploration as its strategy,\nwhere it collects information about objects it encounters along the way. This information will\nprovide knowledge about the goal of the game and provide insight into the environment and what\nmight be useful later in the game. Due to this, exploration by curiosity driven agents might fair\nwell in these types of problems.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n23\nOverall, there is still much work to be done to try and overcome this diﬃcult challenge.\nMachado et al. [15] suggest a few approaches for the ALE, such as agents capable of exploring in a\nmore abstract manner (akin to humans) and agents not exploring joystick movements, but rather\nexploring object conﬁgurations and game levels. Agents with some form of intrinsic motivation\nmight also be needed in order to continue playing even though achieving any reward might seem\nimpossible.\n4.6\nRepresentation learning\nThe original goal of the ALE was to develop agents capable of generalising over many games\nmaking it desirable to automatically learn representations instead of hand crafting features. Deep\nQ-Networks (DQN) [4] and DQN-like approaches are currently the best overall performing methods,\ndespite high sample complexity. However, additional tuning is often required to obtain better\nperformance [75], which suggest that there is still work to be done to improve performance\nby learning better representation in the ALE. Other diﬀerent approaches and directions for\nrepresentation learning that have been used in the literature are also mentioned in [15] and should\nstill be explored more in the ALE.\n4.7\nTransfer learning\nRegarding the ALE, many of the Atari 2600 games have similar game dynamics and knowledge\ntransfer should reduce the number of samples that are required to learn to play games that are\nsimilar. Even more challenging would be determining how to use general video game experience\nand share that knowledge across games that are not necessarily similar. Current approaches in\nthe literature that apply transfer learning in the ALE are restricted to only a limited subset of\ngames that share similarities and the approaches are based on using neural networks to perform\ntransfer, combining representations and policy transfer. Machado et al. [15] point out that it\nmight be interesting to determine whether transferring each of these entities independently could\nbe helpful. To help with the topic of transfer learning in the ALE, the new version includes\ndiﬀerent game modes and diﬃculty settings called ﬂavours (see Section 3.2), which introduces\nmany new environments that are very similar.\nSome of the tasks in rllab and environments in OpenAI Gym have been used in studying the\ntransferring of system dynamics from simulation to robots [76, 77, 78]. These simulation tasks are\nan ideal way to safely study the transferring of policies for robotic domains.\nTransfer learning has also been studied in the Keepaway soccer domain [49], which is a ﬁtting\nsetting since the number of players as well as the size of the action and state spaces can diﬀer.\nTextWorld’s generative functionality (described in full in [18]) allows for control of the size and\nthe partial observability of the state space, and therefore a large number of games with shared\ncharacteristics can be generated. This could be used for studying transfer learning in text-based\ngames, since agents can be trained on simpler tasks and behaviour transferred to harder problems.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n24\n4.8\nModel learning\nPlanning and model learning in complex domains are challenging problems and little research has\nbeen conducted on this topic compared to traditional RL techniques to learn policies or value\nfunctions.\nIn the ALE, the Stella emulator provides a generative model that can be used in planning and\nthe agent has an exact model of the environment. However, there has not been any success with\nplanning using a learned generative model in the ALE, which is a challenging task since errors\nstart to compound after only a few time steps. A few relatively successful approaches [79, 80]\nare available, but the models are slower than the emulator. A challenging open problem is to\nlearn a fast and accurate model for the ALE. On the other hand, related to this, is the problem of\nplanning using an imperfect model.\nOn tasks in OpenAI Gym and rllab some research has also been conducted in model learning\n[81, 82], but the main focus in the literature is on model-free learning techniques. Therefore there\nis still scope for substantial research to address this problem.\nWang et al. [82] attempted to address the lack of a standardised benchmarking framework\nfor model-based RL. They benchmarked 11 model-based RL algorithms and four model-free RL\nalgorithms across 18 environments from OpenAI Gym and have shared the code in an online\nrepository21. They evaluated the eﬃciency, performance and robustness of three diﬀerent categories\nof model-based RL algorithms (Dyna style algorithms, policy search with backpropagation through\ntime and shooting algorithms) and four model-free algorithms (TRPO, PPO, TD3, and SAC –\nrefer to Section 2.1.8 for these algorithms). They also propose three key research challenges for\nmodel-based methods, namely the dynamics bottleneck, the planning horizon dilemma, and the\nearly termination dilemma and show that even with substantial benchmarking, there is no clear\nconsistent best model-based RL algorithm. This again suggests that there is substantial scope\nand many opportunities for further research in model-based RL methods.\n4.9\nOﬀ-policy learning\nDeep neural networks have become extremely popular in modern RL literature, and the break-\nthrough work of Mnih et al. [3, 4] demonstrates DQN having human-level performance on Atari\n2600 games. However, when using deep neural networks for function approximation for oﬀ-policy\nalgorithms, new and complex challenges arise, such as instability and slow convergence. While\ndiscussing oﬀ-policy methods using function approximation, Sutton and Barto [1] conclude the\nfollowing: “The potential for oﬀ-policy learning remains tantalizing, the best way to achieve it\nstill a mystery.” Nevertheless, oﬀ-policy learning has become an active research ﬁeld in RL.\nThe use of oﬀ-policy learning algorithms in the ALE in current literature varies with most\napproaches using experience replay and target networks. This is an attempt at reducing divergence\nin oﬀ-policy learning, but these methods are very complex. New proposed algorithms such as\nGQ(λ) [83] are theoretically sound, but there is still a need for a thorough empirical evaluation or\ndemonstration of these theoretically sound oﬀ-policy learning RL algorithms. Other contributions\n21http://www.cs.toronto.edu/∼tingwuwang/mbrl.html\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n25\nof using oﬀ-policy learning in the ALE includes double Q-learning [84] and Q(λ) with oﬀ-policy\ncorrections [85].\nSome of the tasks in rllab and OpenAI Gym have also been used in studying oﬀ-policy\nalgorithms, for example introducing the soft actor-critic (SAC) algorithm [24] and using the\nrobotics environments from OpenAI Gym to learn grasping [86]. This area of research is still new\nand there is signiﬁcant scope for further research in this domain.\n4.10\nReinforcement learning in real-world settings\nThe robotics environments in the OpenAI Gym toolkit can be used to train models which work on\nphysical robots. This can be used to develop agents to safely execute realistic tasks. A request for\nresearch from OpenAI7 indicates that work in this area is an active research ﬁeld with promising\nresults.\nThe Keepaway and HFO soccer tasks are ideal settings to study multi-agent RL [87], an\nimportant research area for real-world problems since humans act in an environment where\nobjectives are shared with others.\nChallenges for RL that are unique to TextWorld games are related to natural language\nunderstanding: observation modality, understanding the parser feedback, common-sense reasoning\nand aﬀordance extraction, and language acquisition. These challenges are explained in more detail\nin Cˆot´e et al. [18]. Natural language understanding is an important aspect of artiﬁcial intelligence,\nin order for communication to take place between humans and AI. TextWorld can be used to\naddress many of the challenges described in Section 2.2 in simpler settings and to focus on testing\nand debugging agents on subsets of these challenges.\nIn addition to the frameworks covered in this survey, there are two further contributions that\nare focused on multi-agent and distributed RL. The MAgent research platform [88] facilitates\nresearch in many-agent RL, speciﬁcally in artiﬁcial collective intelligence. The platform aims at\nsupporting RL research that scales up from hundreds to millions of agents and is maintained\nin an online repository22. MAgent also provides a visual interface presenting the state of the\nenvironment and agents.\nA research team from Stanford has introduced the open-source framework SURREAL (Scalable\nRobotic REinforcementlearning ALgorithms) and the SURREAL Robotics Suite [89], to facil-\nitae research in RL in robotics and distributed RL. SURREAL eliminates the need for global\nsynchronization and improves scalability by decoupling a distributed RL algorithm into four\ncomponents. The four-layer computing infrastructure can easily be deployed on commercial cloud\nproviders or personal computers, and is also fully replicable from scratch, contributing to the\nreproducibility of results. The Robotics Suite is developed in the MuJoCo physics engine and\nprovides OpenAI gym-style interfaces in Python. Detailed API documentation and tutorials on\nimporting new robots and the creation of new environments and tasks are also provided, furthering\nthe contribution to research in this ﬁeld. The Robotics Suite is actively maintained in an online\n22https://github.com/geek-ai/MAgent\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n26\nrepository23. The diﬀerent robotics tasks include block lifting and stacking, bimanual peg-in-hole\nplacing and bimanual lifting, bin picking, and nut-and-peg assembly. Variants of PPO and DDPG\ncalled SURREAL-PPO and SURREAL-DDPG were developed and examined on the Robotics\nSuite tasks, and experiments indicate that these SURREAL algorithms can achieve good results.\n4.11\nA standard methodology for benchmarking\nThe ALE consists of games with similar structure in terms of of inputs, action movements, etc.\nThis makes the ALE an ideal benchmark for comparative studies. A standard methodology is\nhowever needed and this is proposed by Machado et al. [15]:\n• Episode termination can be standardised by using the game over signal than lives lost.\n• Hyperparameter tuning needs to be consistently applied on the training set only.\n• Training time should be consistently applied across diﬀerent problems.\n• There is a need for standard ways of reporting learning performance.\nThese same principles apply to groups of similar tasks in OpenAI Gym and rllab, and to\nTextWorld and Keepaway soccer.\n4.12\nTrends in benchmarking of RL\nIt is clear from Section 3 that the number of well thought-out frameworks designed for RL\nbenchmarks has rapidly expanded in recent years, with a general move to fully open source\nimplementations being evident. A notable example is OpenAI Gym re-implementing, to an extent,\nopen source variants of the benchmarks previously provided in the MuJoCo simulation environment.\nThe move to fully open source implementations has had two primary beneﬁts: reproducibility and\naccessibility.\nThe variety of RL frameworks and benchmark sets may present a challenge to a novice in the\nﬁeld, as there is no clear standard benchmark set or framework to use. This is not a surprising\nsituation as the array of RL application areas has become relatively diverse and so diﬀerent\ntypes of problems and their corresponding challenges will naturally be more interesting to certain\nsub-communities within the ﬁeld.\nOne aspect of modern RL benchmarks that is relatively striking is the increase in problem\ncomplexity. While it is not immediately clear how to precisely deﬁne problem diﬃculty, it is clear\nthat more and more problem features that are challenging for RL algorithms are being included\nin proposed benchmarks. Many established benchmark sets have been explicitly expanded to\nincrease the challenge of a given problem instance. Some notable examples include the addition of\nsticky actions in the ALE and the addition of the partially observable variants of rllab’s continuous\ncontrol tasks.\n23https://github.com/SurrealAI/surreal\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n27\nIt is also clear that the advancements made in the ﬁeld of deep learning has allowed for certain\ntypes of RL tasks to be more readily solvable. Two notable examples are the use of convolution\nneural networks [90] to assist in the vision problem present in Atari 2600 games of the ALE, and\nthe use of modern neutral network based approaches to natural language processing in Microsoft’s\nTextWorld.\n5\nCONCLUSION\nThis paper provides a survey of some of the most used and recent contributions to RL benchmarking.\nA number of benchmarking frameworks are described in terms of their characteristics, technical\nimplementation details and the tasks provided. A summary is also provided of published results on\nthe performance of algorithms used to solve these benchmark tasks. Challenges that occur when\nsolving RL problems are also discussed, including the various ways the diﬀerent benchmarking\ntasks address or facilitate research in addressing these challenges.\nThe survey reveals that there has been substantial progress in the endeavour of standardising\nbenchmarking tasks for RL. The research community has started to acknowledge the importance\nof reproducible results and research has been published to encourage the community to address\nthis problem. However, there is still a lot to be done in ensuring the reproducibility of results for\nfair comparison.\nThere are many approaches when solving RL problems and proper benchmarks are important\nwhen comparing old and new approaches. This survey indicates that the tasks currently used for\nbenchmarking RL encompass a wide range of problems and can even be used to develop algorithms\nfor training agents in real-world systems such as robots.\nReferences\n[1] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2018.\nISBN 978-0262039246.\n[2] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.\nContinuous control with deep reinforcement learning. arXiv:1509.02971, 2015.\n[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, A. Antonoglou, A. Wierstra, and M. Riedmiller.\nPlaying Atari with deep reinforcement learning. arXiv:1312.5602, 2013.\n[4] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou,\nH. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through\ndeep reinforcement learning. Nature, 518:529, 2015. doi: https://doi.org/10.1038/nature14236.\n[5] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A. Makhzani,\nH. K¨uttler, J. Agapiou, J. Schrittwieser, J. Quan, S. Gaﬀney, S. Petersen, K. Simonyan,\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n28\nT. Schaul, H. van Hasselt, D. Silver, T. Timothy Lillicrap, K. Kevin Calderone, P. Paul Keet,\nA. Brunasso, D. Lawrence, A. Ekermo, J. Repp, and R. Tsing. StarCraft II: A new challenge\nfor reinforcement learning. arXiv:1708.04782, 2017.\n[6] V. d. N. Silva and L. Chaimowicz. MOBA: A new arena for game AI. arXiv:1705.10443,\n2017.\n[7] I. Arel, C. Liu, T. Urbanik, and A. Kohls. Reinforcement learning-based multi-agent system\nfor network traﬃc signal control. IET Intelligent Transport Systems, 4(2):128–135, 2010. doi:\nhttps://doi.org/10.1049/iet-its.2009.0070.\n[8] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch-\nbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.\nMastering the game of Go with deep neural networks and tree search. Nature, 529:484–489,\n2016. doi: https://doi.org/10.1038/nature16961.\n[9] M. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated corpus of\nEnglish: The Penn Treebank. Computational Linguistics, 19(2):313–330, jun 1993. ISSN\n0891-2017.\n[10] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition\nchallenge. International Journal of Computer Vision, 115(3):211–252, Dec 2015. ISSN\n1573-1405. doi: https://doi.org/10.1007/s11263-015-0816-y.\n[11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal\nVisual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88(2):\n303–338, Jun 2010. ISSN 1573-1405. doi: https://doi.org/10.1007/s11263-009-0275-4.\n[12] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment:\nAn evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:\n253–279, 2013. ISSN 1076-9757. doi: https://doi.org/10.1613/jair.3912.\n[13] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforce-\nment learning for continuous control. In Proceedings of the 33rd International Conference on\nInternational Conference on Machine Learning, ICML’16, pages 1329–1338, 2016.\n[14] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement\nlearning that matters. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, pages 3207–3214, 2018.\n[15] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling.\nRevisiting the Arcade Learning Environment: Evaluation protocols and open problems for\ngeneral agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018. ISSN 1076-9757.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n29\n[16] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.\nOpenAI Gym. arXiv:1606.01540, 2016.\n[17] P. Stone and R. S. Sutton. Keepaway soccer: A machine learning test bed. In Robot Soccer\nWorld Cup, pages 214–223. Springer, 2001. doi: https://doi.org/10.1007/11780519 9.\n[18] M. A. Cˆot´e, ´A K´ad´ar, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, M. J. Hausknecht,\nL. E. Asri, M. Adada, W. Tay, and A. Trischler. TextWorld: A learning environment for\ntext-based games. arXiv:1806.11532, 2018.\n[19] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the\n33rd International Conference on International Conference on Machine Learning, ICML’16,\npages 1928–1937, 2016.\n[20] J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel. Trust region policy optimization.\nIn Proceedings of the 32nd International Conference on Machine Learning, ICML’15, pages\n1889–1897, 2015.\n[21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv:1707.06347, 2017.\n[22] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement\nlearning. In Proceedings of the 34th International Conference on Machine Learning, ICML’17,\npages 449–458, 2017.\n[23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,\nP. Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural Information\nProcessing Systems 30, pages 5048–5058, 2017.\n[24] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Oﬀ-policy maximum\nentropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th\nInternational Conference on Machine Learning, ICML’18, pages 1861–1870, 2018.\n[25] S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-\ncritic methods. In Proceedings of the 35th International Conference on Machine Learning,\nICML’18, pages 1587–1596, 2018.\n[26] D. Silver, R. S. Sutton, and M. M¨uller. Temporal-diﬀerence search in computer Go. Machine\nLearning, 87(2):183–219, 2012. ISSN 1573-0565. doi: https://doi.org/10.1007/s10994-012-\n5280-0.\n[27] M. Minsky. Steps toward artiﬁcial intelligence. Proceedings of the IRE, 49(1):8–30, 1961. doi:\nhttps://doi.org/10.1109/JRPROC.1961.287775.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n30\n[28] R. E. Bellman. Dynamic Programming. Princeton University Press, Princeton, 1957. ISBN\n978-0486428093.\n[29] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and\nData Engineering, 22(10):1345–1359, 2010. ISSN 1041-4347. doi: https://doi.org/10.1109/\nTKDE.2009.191.\n[30] K. Weiss, T. M. Khoshgoftaar, and D. Wang. A survey of transfer learning. Journal of Big\ndata, 3(9), 2016. ISSN 2196-1115. doi: https://doi.org/10.1186/s40537-016-0043-6.\n[31] M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey.\nJournal of Machine Learning Research, 10(Jul):1633–1685, 2009. ISSN 1532-4435.\n[32] G. Dulac-Arnold, D. Mankowitz, and T. Hester. Challenges of real-world reinforcement\nlearning. In Reinforcement Learning for Real Life (RL4RealLife) Workshop in the 36th\nInternational Conference on Machine Learning, 2019. arXiv:1904.12901.\n[33] E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033,\n2012. doi: https://doi.org/10.1109/IROS.2012.6386109.\n[34] Y. Liang, M. C. Machado, E. Talvitie, and M. Bowling. State of the art control of Atari games\nusing shallow reinforcement learning. In Proceedings of the 2016 International Conference on\nAutonomous Agents and Multiagent Systems, pages 485–493. International Foundation for\nAutonomous Agents and Multiagent Systems, 2016.\n[35] C. Dann, G. Neumann, and J. Peters. Policy evaluation with temporal diﬀerences: A survey\nand comparison. Journal of Machine Learning Research, 15:809–883, 2014. ISSN 1532-4435.\n[36] B. Papis and P. Wawrzy´nski. dotRL: A platform for rapid reinforcement learning meth-\nods development and validation. In 2013 Federated Conference on Computer Science and\nInformation Systems, pages 129–136. IEEE, 2013.\n[37] B. G. Woolley and K. O. Stanley. Evolving a single scalable controller for an octopus arm with\na variable number of segments. In Parallel Problem Solving from Nature, PPSN XI, pages 270–\n279, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. doi: https://doi.org/10.1007/978-\n3-642-15871-1 28.\n[38] T. Schaul, J. Bayer, D. Wierstra, Y. Sun, M. Felder, F. Sehnke, T. R¨uckstieß, and J. Schmid-\nhuber. PyBrain. Journal of Machine Learning Research, 11(Feb):743–746, 2010. ISSN\n1532-4435.\n[39] A. Yamaguchi and T. Ogasawara. SkyAI: Highly modularized reinforcement learning library.\nIn 2010 10th IEEE-RAS International Conference on Humanoid Robots, pages 118–123. IEEE,\n2010. doi: https://doi.org/10.1109/ICHR.2010.5686285.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n31\n[40] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine Learning, 8:229, 1992. ISSN 0885-6125. doi: https://doi.org/10.1007/\nBF00992696.\n[41] J. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational\nspace control. In Proceedings of the 24th International Conference on Machine Learning,\nICML ’07, pages 745–750, 2007. doi: https://doi.org/10.1145/1273496.1273590.\n[42] J. Peters, K. Mulling, and Y. Altun. Relative entropy policy search. In Proceedings of the\nTwenty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI’10, pages 1607–1612, 2010.\n[43] R. Rubinstein. The cross-entropy method for combinatorial and continuous optimization.\nMethodology And Computing In Applied Probability, 1(2):127–190, 1999. ISSN 1573-7713. doi:\nhttps://doi.org/10.1023/A:1010091220143.\n[44] N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies.\nEvolutionary Computation, 9(2):159–195, 2001. ISSN 1063-6560. doi: https://doi.org/10.\n1162/106365601750190398.\n[45] H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, and E. Osawa. RoboCup: The Robot World\nCup Initiative. In Proceedings of the First International Conference on Autonomous Agents,\nAGENTS ’97, pages 340–347, 1997. doi: https://doi.org/10.1145/267658.267738.\n[46] P. Stone, R. S. Sutton, and G. Kuhlmann. Reinforcement learning for RoboCup soccer\nKeepaway. Adaptive Behavior, 13(3):165–188, 2005. ISSN 1059-7123. doi: https://doi.org/10.\n1177/105971230501300301.\n[47] A. D. Pietro, L. While, and L. Barone. Learning in RoboCup keepaway using evolutionary\nalgorithms.\nIn Proceedings of the 4th Annual Conference on Genetic and Evolutionary\nComputation, GECCO’02, pages 1065–1072, 2002.\n[48] T. Walker, J. Shavlik, and R. Maclin. Relational reinforcement learning via sampling the\nspace of ﬁrst-order conjunctive features. In Proceedings of the ICML Workshop on Relational\nReinforcement Learning, 2004.\n[49] M. E. Taylor and P. Stone. Behavior transfer for value-function-based reinforcement learning.\nIn Proceedings of the fourth international joint conference on Autonomous agents and multia-\ngent systems, AAMAS ’05, pages 53–59, 2005. doi: https://doi.org/10.1145/1082473.1082482.\n[50] S. Didi and G. Nitschke. Multi-agent behavior-based policy transfer. In European Conference\non the Applications of Evolutionary Computation, EvoApplications, pages 181–197, 2016. doi:\nhttps://doi.org/10.1007/978-3-319-31153-1 13.\n[51] S. Didi and G. Nitschke. Hybridizing novelty search for transfer learning. In IEEE Symposium\nSeries on Computational Intelligence (SSCI), pages 1–8, 2016. doi: https://doi.org/10.1109/\nSSCI.2016.7850180.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n32\n[52] G. Nitschke and S. Didi. Evolutionary policy transfer and search methods for boosting\nbehavior quality: Robocup keep-away case study. Frontiers in Robotics and AI, 4:62, 2017.\nISSN 2296-9144. doi: https://doi.org/10.3389/frobt.2017.00062.\n[53] S. Didi and G. Nitschke. Policy transfer methods in RoboCup keep-away. In Proceedings\nof the Genetic and Evolutionary Computation Conference Companion, GECCO ’18, pages\n117–118, 2018. ISBN 978-1-4503-5764-7. doi: https://doi.org/10.1145/3205651.3205710.\n[54] D. Schwab, Y. Zhu, and M. Veloso.\nZero shot transfer learning for robot soccer.\nIn\nProceedings of the 17th International Conference on Autonomous Agents and MultiAgent\nSystems, AAMAS’18, pages 2070–2072, 2018.\n[55] Q. Cheng, X. Wang, Y. Niu, and L. Shen. Reusing source task knowledge via transfer\napproximator in reinforcement transfer learning. Symmetry, 11(1), 2018. ISSN 2073-8994.\ndoi: https://doi.org/10.3390/sym11010025.\n[56] M. Riedmiller, T. Gabel, R. Hafner, and S. Lange. Reinforcement learning for robot soccer.\nAutonomous Robots, 27(1):55–73, 2009. doi: https://doi.org/10.1007/s10514-009-9120-4.\n[57] A. Bai and S. Russell. Eﬃcient reinforcement learning with hierarchies of machines by leverag-\ning internal transitions. In Proceedings of the Twenty-Sixth International Joint Conference on\nArtiﬁcial Intelligence, pages 1418–1424, 2017. doi: https://doi.org/10.24963/ijcai.2017/196.\n[58] P. Stone, G. Kuhlmann, M. E. Taylor, and Y. Liu.\nKeepaway soccer: From machine\nlearning testbed to benchmark.\nIn Robot Soccer World Cup, pages 93–105, 2005.\ndoi:\nhttps://doi.org/10.1007/11780519 9.\n[59] P. Stone. Layered learning in multiagent systems: A winning approach to robotic soccer. MIT\nPress, 2000. ISBN 978-0819428448.\n[60] J. S. Albus. A new approach to manipulator control: The cerebellar model articulation\ncontroller (CMAC). Journal of Dynamic Systems, Measurement, and Control, 97(3):220–227,\n1975. doi: https://doi.org/10.1115/1.3426922.\n[61] J. S. Albus. Brains, Behavior and Robotics. McGraw-Hill, Inc., 1981. ISBN 0070009759.\n[62] S. Kalyanakrishnan, Y. Liu, and P. Stone. Half Field Oﬀense in RoboCup Soccer: A Multiagent\nReinforcement Learning Case Study. In RoboCup 2006: Robot Soccer World Cup X, pages\n72–85, 2007. doi: https://doi.org/10.1007/978-3-540-74024-7 7.\n[63] M. Hausknecht, P. Mupparaju, S. Subramanian, S. Kalyanakrishnan, and P. Stone. Half ﬁeld\noﬀense: An environment for multiagent learning and ad hoc teamwork. In AAMAS Adaptive\nLearning Agents (ALA) Workshop, 2016.\n[64] E. Parisotto and R. Salakhutdinov. Neural map: Structured memory for deep reinforcement\nlearning. arXiv:1702.08360, 2017.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n33\n[65] N. Fulda, D. Ricks, B. Murdoch, and D. Wingate. What can you do with a rock? Aﬀordance\nextraction via word embeddings. In Proceedings of the 26th International Joint Conference\non Artiﬁcial Intelligence, IJCAI’17, pages 1039–1045, 2017. doi: https://doi.org/10.24963/\nijcai.2017/144.\n[66] J. C. H. Watkins and P Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992. doi:\nhttps://doi.org/10.1007/BF00992698.\n[67] B. Kostka, J. Kwiecieli, J. Kowalski, and P. Rychlikowski. Text-based adventures of the\nGolovin AI agent. In 2017 IEEE Conference on Computational Intelligence and Games (CIG),\npages 181–188, 2017. doi: https://doi.org/10.1109/CIG.2017.8080433.\n[68] O Nachum, M. Norouzi, and D. Schuurmans. Improving policy gradient by exploring under-\nappreciated rewards. In 5th International Conference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. arXiv:1611.09321.\n[69] D. Ha. Reinforcement learning for improving agent design. Artiﬁcial Life, 25(4):352–365,\n2019. doi: https://doi.org/10.1162/artl a 00301.\n[70] C. Cundy and D. Filan.\nExploring hierarchy-aware inverse reinforcement learning.\narXiv:1807.05037, 2018.\n[71] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan,\nB. Piot, M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement\nlearning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[72] O. Nachum, S. Gu, H. Lee, and S. Levine. Data-eﬃcient hierarchical reinforcement learning. In\nAdvances in Neural Information Processing Systems 31, pages 3303–3313. Curran Associates,\nInc., 2018. URL http://papers.nips.cc/paper/7591-data-eﬃcient-hierarchical-reinforcement-\nlearning.pdf.\n[73] A. Adhikari, X. Yuan, M. A. Cˆot´e, M. Zelinka, M. A. Rondeau, R. Laroche, P. Poupart,\nJ. Tang, A. Trischler, and W. L. Hamilton. Learning dynamic knowledge graphs to generalize\non text-based games. arXiv preprint arXiv:2002.09127, 2020.\n[74] G. Kuhlmann and P. Stone. Progress in learning 3 vs. 2 Keepaway. In IEEE International\nConference on Systems, Man and Cybernetics, SMC’03, pages 52–59, 2003. doi: https:\n//doi.org/10.1109/ICSMC.2003.1243791.\n[75] R. Islam, P. Henderson, M. Gomrokchi, and D. Precup. Reproducibility of benchmarked deep\nreinforcement learning tasks for continuous control. In ICML Workshop on Reproducibility in\nMachine Learning, ICML’17, 2017. arXiv:1708.04133.\n[76] D. Held, Z. McCarthy, M. Zhang, F. Shentu, and P. Abbeel. Probabilistically safe policy\ntransfer. In IEEE International Conference on Robotics and Automation (ICRA), pages\n5798–5805, 2017. doi: https://doi.org/10.1109/ICRA.2017.7989680.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n34\n[77] M. Wulfmeier, I. Posner, and P. Abbeel. Mutual alignment transfer learning. In Proceedings\nof the 1st Annual Conference on Robot Learning, 2017.\n[78] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic\ncontrol with dynamics randomization. In IEEE International Conference on Robotics and\nAutomation (ICRA), pages 1–8, 2018. doi: https://doi.org/10.1109/ICRA.2018.8460528.\n[79] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using\ndeep networks in Atari games. In Proceedings of the 28th International Conference on Neural\nInformation Processing Systems - Volume 2, NIPS’15, pages 2863–2871, 2015.\n[80] S. Chiappa, S. Racaniere, D. Wierstra, and S. Mohamed. Recurrent environment simulators.\narXiv:1704.02254, 2017.\n[81] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-\nbased deep reinforcement learning with model-free ﬁne-tuning.\nIn IEEE International\nConference on Robotics and Automation (ICRA), pages 7559–7566, 2018. doi: https://doi.\norg/10.1109/ICRA.2018.8463189.\n[82] T. Wang, X. Bao, I. Clavera, J. Hoang, Y. Wen, E. Langlois, S. Zhang, G. Zhang, P. Abbeel,\nand J. Ba. Benchmarking model-based reinforcement learning. arXiv:1907.02057, 2019.\n[83] H. R. Maei and R. S. Sutton. GQ(lambda): A general gradient algorithm for temporal-\ndiﬀerence prediction learning with eligibility traces. In 3rd Conference on Artiﬁcial General\nIntelligence (AGI-2010). Atlantis Press, 2010. ISBN 978-90-78677-36-9. doi: https://doi.org/\n10.2991/agi.2010.22.\n[84] H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double Q-learning.\nIn Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, AAAI’16, pages\n2094–2100, 2016.\n[85] A. Harutyunyan, M. G. Bellemare, T. Stepleton, and R. Munos.\nQ(λ) with oﬀ-policy\ncorrections. In Algorithmic Learning Theory, pages 305–320, 2016. ISBN 978-3-319-46379-7.\ndoi: https://doi.org/10.1007/978-3-319-46379-7 21.\n[86] D. Quillen, E. Jang, O. Nachum, C. Finn, J. Ibarz, and S. Levine. Deep reinforcement\nlearning for vision-based robotic grasping: A simulated comparative evaluation of oﬀ-policy\nmethods. In IEEE International Conference on Robotics and Automation (ICRA), pages\n6284–6291, 2018. doi: https://doi.org/10.1109/ICRA.2018.8461039.\n[87] L. Bu¸soniu, R. Babuˇska, and B. De Schutter. A comprehensive survey of multiagent reinforce-\nment learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications\nand Reviews), 38(2):156–172, 2008. ISSN 1094-6977. doi: https://doi.org/10.1109/TSMCC.\n2007.913919.\nDOI TBC\nStapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms\n35\n[88] L. Zheng, J. Yang, H. Cai, M. Zhou, W. Zhang, J. Wang, and Y. Yu. MAgent: A many-agent\nreinforcement learning platform for artiﬁcial collective intelligence. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[89] L. Fan, Y. Zhu, J. Zhu, Z. Liu, O. Zeng, A. Gupta, J. Creus-Costa, S. Savarese, and L. Fei-\nFei. SURREAL: Open-source reinforcement learning framework and robot manipulation\nbenchmark. In Proceedings of The 2nd Conference on Robot Learning, volume 87 of PMLR,\npages 767–782, 2018.\n[90] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: https://doi.org/10.1109/\n5.726791.\nDOI TBC\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-11-27",
  "updated": "2020-11-27"
}