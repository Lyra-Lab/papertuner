{
  "id": "http://arxiv.org/abs/2011.13776v1",
  "title": "Enhancing Diversity in Teacher-Student Networks via Asymmetric branches for Unsupervised Person Re-identification",
  "authors": [
    "Hao Chen",
    "Benoit Lagadec",
    "Francois Bremond"
  ],
  "abstract": "The objective of unsupervised person re-identification (Re-ID) is to learn\ndiscriminative features without labor-intensive identity annotations.\nState-of-the-art unsupervised Re-ID methods assign pseudo labels to unlabeled\nimages in the target domain and learn from these noisy pseudo labels. Recently\nintroduced Mean Teacher Model is a promising way to mitigate the label noise.\nHowever, during the training, self-ensembled teacher-student networks quickly\nconverge to a consensus which leads to a local minimum. We explore the\npossibility of using an asymmetric structure inside neural network to address\nthis problem. First, asymmetric branches are proposed to extract features in\ndifferent manners, which enhances the feature diversity in appearance\nsignatures. Then, our proposed cross-branch supervision allows one branch to\nget supervision from the other branch, which transfers distinct knowledge and\nenhances the weight diversity between teacher and student networks. Extensive\nexperiments show that our proposed method can significantly surpass the\nperformance of previous work on both unsupervised domain adaptation and fully\nunsupervised Re-ID tasks.",
  "text": "Enhancing Diversity in Teacher-Student Networks via Asymmetric branches for\nUnsupervised Person Re-identiﬁcation\nHao Chen1,2, Benoit Lagadec2, and Francois Bremond1\n1University of Cˆote d’Azur, Inria, Stars Project-Team, France\n{hao.chen,francois.bremond}@inria.fr\n2European Systems Integration, France\nbenoit.lagadec@esifrance.net\nAbstract\nThe objective of unsupervised person re-identiﬁcation\n(Re-ID) is to learn discriminative features without labor-\nintensive identity annotations.\nState-of-the-art unsuper-\nvised Re-ID methods assign pseudo labels to unlabeled\nimages in the target domain and learn from these noisy\npseudo labels. Recently introduced Mean Teacher Model is\na promising way to mitigate the label noise. However, dur-\ning the training, self-ensembled teacher-student networks\nquickly converge to a consensus which leads to a local min-\nimum. We explore the possibility of using an asymmetric\nstructure inside neural network to address this problem.\nFirst, asymmetric branches are proposed to extract features\nin different manners, which enhances the feature diversity\nin appearance signatures. Then, our proposed cross-branch\nsupervision allows one branch to get supervision from the\nother branch, which transfers distinct knowledge and en-\nhances the weight diversity between teacher and student\nnetworks. Extensive experiments show that our proposed\nmethod can signiﬁcantly surpass the performance of previ-\nous work on both unsupervised domain adaptation and fully\nunsupervised Re-ID tasks. 1\n1. Introduction\nPerson re-identiﬁcation (Re-ID) targets at retrieving a\nperson of interest across non-overlapping cameras. Since\nthere are domain gaps resulting from illumination condi-\ntion, camera property and view-point variation, a Re-ID\nmodel trained on a source domain usually shows a huge per-\nformance drop on other domains.\nUnsupervised domain adaptation (UDA) targets at shift-\ning the model trained from a source domain with identity\nannotation to a target domain via learning from unlabeled\ntarget images. In the real world, unlabeled images in a target\ndomain can be easily recorded, which is almost labor-free.\nIt is intuitive to use these images to adapt a pretrained Re-\nID model to the desired domain. Fully unsupervised Re-ID\n1Code at https://github.com/chenhao2345/ABMT.\nfurther minimises the supervision by removing pre-training\non the labelled source domain.\nState-of-the-art UDA Person Re-ID methods [8, 27] and\nunsupervised methods [17] assign pseudo labels to unla-\nbeled target images. The generated pseudo labels are gener-\nally very noisy. The noise is mainly from several inevitable\nfactors, such as the strong domain gaps and the imperfection\nof clustering. In this way, an unsupervised Re-ID problem\nis naturally transferred into Generating pseudo labels and\nLearning from noisy labels problems.\nTo generate pseudo labels, the most intuitive way is to\nuse a clustering algorithm, which gives a good starting point\nfor clustering based UDA Re-ID [29, 6]. Recently, Ge et al.\n[8] propose to add a Mean Teacher [23] model as online\nsoft pseudo label generator, which effectively reduces the\nerror ampliﬁcation during the training with noisy labels. In\nthis paper, we also use both clustering-based hard labels and\nteacher-based soft labels in our baseline.\nTo handle noisy labels, one of the most popular ap-\nproaches is to train paired networks so that each network\nhelps to correct its peer, e.g., two-student networks in Co-\nteaching [9] and two-teacher-two-student networks in MMT\n[8].\nHowever, these paired models with identical struc-\nture are prone to converge to each other and get stuck in\na local minimum. There are several attempts to alleviate\nthis problem, such as Co-teaching+ [28], ACT [27] and\nMMT [8]. These attempts of keeping divergence between\npaired models are mainly based on either different train-\ning sample selection [28, 27] or different initialization and\ndata augmentation[8]. In this paper, we propose a strong\nalternative by designing asymmetric neural network struc-\nture in the Mean Teacher Model. We use two independent\nbranches with different depth and global pooling methods\nas last layers of a neural network. Features extracted from\nboth branches are concatenated as the appearance signa-\nture, which enhances the feature diversity in the appearance\nsignature and allows to get better clustering-based hard la-\nbels. Each branch gets supervision from its peer branch of\ndifferent structure, which enhances the divergence between\npaired teacher-student networks. Our proposed decoupling\nmethod does not rely on different source domain initializa-\narXiv:2011.13776v1  [cs.CV]  27 Nov 2020\ntions, which makes it more effective in the fully unsuper-\nvised scenario where the source domain is not available.\nIn summary, our contributions are:\n1. We propose to enhance the feature diversity inside\nperson Re-ID appearance signatures by splitting last\nlayers of a backbone network into two asymmetric\nbranches, which increases the quality of clustering-\nbased hard labels.\n2. We propose a novel decoupling method where asym-\nmetric branches get cross-branch supervision, which\navoids weights in paired teacher-student networks con-\nverging to each other and increases the quality of\nteacher-based soft labels.\n3. Extensive experiments and ablation study are con-\nducted to validate the effectiveness of each proposed\ncomponent and the whole framework.\n2. Related Work\nUnsupervised domain adaptive Re-ID.\nRecent unsuper-\nvised cross-domain Re-ID methods can be roughly catego-\nrized into distribution alignment and pseudo label based\nadaptation. The objective of distribution alignment is to\nlearn domain invariant features. Several attempts [24, 15]\nleverage semantic attributes to align the feature distribution\nin the latent space.\nHowever, these approaches strongly\nrely on extra attribute annotation, which requires extra la-\nbor.\nAnother possibility is to align the feature distribu-\ntion by transferring labeled source domain images into the\nstyle of target domain with generative adversarial networks\n[25, 33, 2]. Style transferred images are usually combined\nwith pseudo label based adaptation to get a better perfor-\nmance. Pseudo label based adaptation is a more straight-\nforward approach for unsupervised cross-domain Re-ID,\nwhich directly assigns pseudo labels to unlabelled target im-\nages and allows to ﬁne-tune a pre-trained model in a super-\nvised manner. Clustering algorithms are widely used in pre-\nvious unsupervised cross-domain Re-ID methods. UDAP\n[22] provides a good analysis on clustering based adaptation\nand use a k-reciprocal encoding [31] to improve the quality\nof clusters. PCB-PAST [29] simultaneously learns from a\nranking-based and clustering-based triplet losses. SSG [6]\nassigns clustering-based pseudo labels to both global and\nlocal features. To mitigate the clustering-based label noise,\nresearchers borrow ideas from how unlabeled data is used\nin Semi-supervised learning and Learning from noisy la-\nbels. ECN [34] uses an exemplar memory to save averaged\nfeatures to assign soft labels. ACT [27] splits the training\ndata into inliers/outliers to enhance the divergence of paired\nnetworks in Co-teaching [9]. MMT [8] adopts two student\nand two Mean Teacher networks. Two students are initial-\nized differently from source pre-training in order to enhance\nthe divergence of paired teacher-student networks. Each\nmean teacher network provides soft labels to supervise peer\nstudent network. However, despite different initializations\nand different data augmentations used in peer networks, the\ndecoupling is not encouraged enough during the training.\nWe directly use asymmetric neural network structure inside\nteacher-student networks, which encourages the decoupling\nat all epochs.\nTeacher-Student Network for Semi-Supervised Learn-\ning.\nUnsupervised domain adaptation can be regarded to\nsome extent as Semi-Supervised Learning (SSL), since both\nof them utilize labeled data (source domain for UDA) and\nlarge amount of unlabeled data (target domain for UDA).\nA teacher-student structure is commonly used in SSL. This\nstructure allows student network to gradually exploit data\nwith perturbations under consistency constraints.\nIn Π\nmodel and Temporal ensembling [14], the student learns\nfrom either samples forwarded twice with different noise\nor exponential moving averaged (EMA) predictions un-\nder consistency constraints. Instead of EMA predictions,\nMean-teacher model [23] uses directly the EMA weights\nfrom the student to supervise the student under a consis-\ntency constraint. Authors of Dual student [13] point out that\nthe Mean Teacher converging to student along with training\n(coupling problem) prevents the teacher-student from ex-\nploiting more meaningful information from data. Inspired\nby Deep Co-training [20], they propose to train two inde-\npendent students on stable samples which have same pre-\ndictions and enough large feature difference. However, in\nunsupervised cross-domain Re-ID, labeled source domain\nand unlabeled target domain do not share the same iden-\ntity classes, which makes traditional close-set SSL methods\nhard to use.\nFully unsupervised Re-ID.\nRecently, several fully unsu-\npervised Re-ID methods are proposed to further minimize\nthe supervision, which does not require any Re-ID anno-\ntation. A bottom-up clustering framework is proposed in\nBUC [16], which trains a network based on the clustering-\nbased pseudo labels in an iterative way.\n[17] replaces\nclustering-based pseudo labels with similarity-based soft-\nened labels.\nDifferent to image-based unsupervised Re-\nID, [26] learns tacklet information with clustering-based\npseudo labels. MMT [8] can be transferred into an unsu-\npervised method by removing the pre-training in source do-\nmain. However, without different source domain initializa-\ntions, divergence between peer networks can not be enough\nencouraged in MMT. Instead of different source domain ini-\ntializations, divergence is encouraged by asymmetric net-\nwork structures, which is more suitable for fully unsuper-\nvised Re-ID.\nFigure 1. Source domain pre-training for asymmetric branched\nnetwork. One ResNet bottleneck block corresponds to three con-\nvolutional layers. For UDA setting, inputs are labelled images\nfrom source training set. GAP refers Global Average Pooling,\nwhile GMP refers to Global Max Pooling. FC refers to Fully Con-\nnected layer.\n3. Proposed Method\n3.1. Overview\nGiven two datasets: one labeled source dataset Ds and\none unlabeled target dataset Dt, the objective of UDA is to\nadapt a source pretrained model Mpre to the target dataset\nwith unlabeled target data. To achieve this goal, we propose\na two-staged adaptation approach based on Mean Teacher\nModel. We focus on the coupling problem (teacher and\nstudent converge to each other) existing inside the original\nMean Teacher. Asymmetric branches and cross-branch su-\npervision are proposed in this paper to address this problem\nand to enhance the diversity in the network, which show\ngreat effectiveness for UDA Re-ID.\n3.2. Asymmetric branches\nA multi-branch structure is widely used in the fully su-\npervised Re-ID methods, especially in global-local feature\nbased methods [7, 3, 1]. Such structure keeps independence\nbetween branches, which makes features extracted from dif-\nferent branches diversiﬁed. In the unsupervised Re-ID, we\nconduct clustering on appearance signatures computed from\nperson images to generate pseudo labels. The quality of\nappearance signatures can be improved by extracting dis-\ntinct meaningful features from different branches. Thus,\nwe duplicate last layers of a backbone network and make\nthem different in the structure, which we call Asymmetric\nBranches.\nAsymmetric branches are illustrated in Figure 1. For a\nResNet-based [10] backbone, the layer 4 is duplicated. The\nﬁrst branch is kept unchanged as the one used in the original\nbackbone: 3 bottlenecks and global average pooling (GAP).\nThe second branch is composed of 4 bottlenecks and global\nmax pooling (GMP). The GAP perceives global informa-\ntion, while the GMP focuses on the most discriminative in-\nformation (most distinguishable identity information, such\nas a red bag or a yellow t-shirt). Asymmetric branches im-\nprove appearance signature quality by enhancing the feature\ndiversity, which is validated by source pre-training perfor-\nmance boost in Table 3 as well as examples in Figure 5.\nThey further improve the quality of pseudo labels during\nthe adaptation, which is validated by target adaptation per-\nformance in Table 3.\n3.3. Asymmetric Branched Mean Teaching\nWe call our proposed adaptation method Asymmetric\nBranched Mean Teaching (ABMT). Our proposed ABMT\ncontains two stages: Source pre-training and Target adapta-\ntion.\n3.3.1\nSource domain supervised pre-training\nIn the ﬁrst stage, we train a network in the fully supervised\nway on the source domain. Thanks to this stage, the model\nused for adaptation obtains a basic Re-ID capacity, which\nhelps to alleviate pseudo label noise. Given a source sam-\nple xs\ni and its ground truth identity y′\ni, the network (with\nweight θ) encodes xs\ni into average Fa(xs\ni|θ) and max fea-\ntures Fm(xs\ni|θ) and then gets two predictions Pa(xs\ni|θ) and\nPm(xs\ni|θ). Cross-entropy Lce and batch hard triplet [11]\nLtri losses are used in this stage as shown in Figure 1.\nThe whole network is trained with a combination of both\nlosses:\nLscr =λs\nceLce(Pa(xs\ni|θ), y′\ni) + λs\nceLce(Pm(xs\ni|θ), y′\ni)+\nλs\ntriLtri(Fa(xs\ni|θ), y′\ni) + λs\ntriLtri(Fm(xs\ni|θ), y′\ni)\n(1)\n3.3.2\nTarget domain unsupervised adaptation\nThe adaptation procedure is illustrated in Figure 2. It con-\ntains two components: Clustering-based hard label genera-\ntion and Cross-branch teacher-based soft label training. Af-\nter adaptation, only teacher network is used during the in-\nference.\nClustering-based hard label generation.\nIn previous\nUDA Re-ID methods, distance-based K-Means [8] and\ndensity-based clustering DBSCAN [27, 22] are main ap-\nproaches to generate pseudo labels.\nWe follow the state-of-the-art DBSCAN based cluster-\ning method presented in [22]. To adapt it to our proposed\nasymmetric branches, we concatenate the average and max\nfeatures from asymmetric branches in the teacher network\nas appearance signatures. Images belonging to the same\nidentity should have the same nearest neighbors in the fea-\nture space. Distance metric for DBSCAN are obtained by k-\nreciprocal re-ranking encoding [31] between target domain\nand source domain samples.\nFigure 2. ABMT adaptation. For UDA setting, inputs are training set images from both source and target domains. For fully unsupervised\nsetting, inputs are unlabeled images from target training set.\nThe density-based clustering generates unﬁxed cluster\nnumbers at different epochs, which means old classiﬁers\nfrom the last epoch can not be reused after a new clustering.\nThus, we simply create new classiﬁers depending on the\nnumber of clusters at the beginning of each epoch. We take\nnormalized mean features of each cluster from the average\nbranch to initialize the average branch classiﬁers and simi-\nlarly normalized max features from max branch to initialize\nthe max branch classiﬁers. We call these classiﬁers with\nﬂexible dimension ”Dynamic Classiﬁers”. With the help of\nthese Dynamic Classiﬁers, the student is trained on clus-\nter components (outliers are discarded) with cross-entropy\nloss:\nLce = −\nX\ni\n(y′\ni log(Pm(xt\ni|θ))) −\nX\ni\n(y′\ni log(Pa(xt\ni|θ)))\n(2)\nwhere y′\ni is the clustering based hard label and Pa(xt\ni|θ)\nand Pm(xt\ni|θ) are student predictions from both asymmetric\nbranches.\nCross-branch teacher-based soft label training.\nClus-\ntering algorithms generate hard pseudo labels whose con-\nﬁdences are 100%. Since Re-ID is a ﬁne-grained recogni-\ntion problem, people with similar clothes are not rare in the\ndataset. Hard pseudo labels of these similar samples can\nbe extremely noisy. In this case, soft pseudo labels (conﬁ-\ndences < 100%) are more reliable. Learning with both hard\nand soft pseudo labels can effectively alleviate label noise.\nThe Mean Teacher Model [23] (teacher weights θ′) uses\nthe EMA weights of the student model (student weights θ).\nThe Mean Teacher Model shows strong capacity to handle\nlabel noise and avoids error ampliﬁcation along with train-\ning. We deﬁne θ′\nt at training step t as the EMA of successive\nweights:\nθ′\nt =\n(\nθt,\nif t = 0\nαθ′\nt−1 + (1 −α)θt,\notherwise\n(3)\nwhere α is a smoothing coefﬁcient that controls the self-\nensembling speed of the Mean Teacher.\nDespite these advantages of Mean Teacher, such self-\nensembling teacher-student networks (the teacher is formed\nby EMA weights of the student, and the student is super-\nvised by the teacher) face the coupling problem. We use\nthe Mean Teacher soft label generator as in [8] and ad-\ndress the coupling problem by cross-branch supervision.\nEach branch in the student is supervised by a teacher\nbranch which has different structure. Weight diversity be-\ntween the paired teacher-student can be better kept. Given\none target domain sample xt\ni, the teacher (teacher weights\nθ′) encodes it into two feature vectors from two asym-\nmetric branches, average features Fa(xt\ni|θ′) and max fea-\ntures Fm(xt\ni|θ′). The dynamic classiﬁers then transform\nthese two feature vectors into two predictions respectively\nPa(xt\ni|θ′) and Pm(xt\ni|θ′). Similarly, features of the student\n(student weights θ) are Fa(xt\ni|θ) and Fm(xt\ni|θ), while pre-\ndictions are Pa(xt\ni|θ) and Pm(xt\ni|θ). The predictions from\nthe teacher supervise those from the student with a soft\ncross-entropy loss [12] in a cross-branch manner, which can\nbe formulated as\nLa→m\nsce\n= −\nX\ni\n(Pa(xt\ni|θ′) log(Pm(xt\ni|θ)))\n(4)\nLm→a\nsce\n= −\nX\ni\n(Pm(xt\ni|θ′) log(Pa(xt\ni|θ)))\n(5)\nTo further enhance the teacher-student networks’ discrimi-\nnative capacity, the features in the teacher supervise those\nof the student with a soft triplet loss [8]:\nLa→m\nstri\n= −\nX\ni\n(Ta(xt\ni|θ′) log(Tm(xt\ni|θ)))\n(6)\nLm→a\nstri\n= −\nX\ni\n(Tm(xt\ni|θ′) log(Ta(xt\ni|θ)))\n(7)\nwhere T(xt\ni|θ) =\nexp(∥F (xt\ni|θ)−F (xt\np|θ)∥2)\nexp(∥F (xt\ni|θ)−F (xtp|θ)∥2)+exp(∥F (xt\ni|θ)−F (xtn|θ)∥2)\nis the softmax triplet distance of the sample xt\ni, its hardest\npositive xt\np and its hardest negative xt\nn in a mini-batch.\nBy minimizing the soft triplet loss, the softmax triplet\ndistance in a mini-batch from the student is encouraged to\nget as close as possible to the distance from the teacher.\nThe positive and negative samples within a mini-batch\nare decided by clustering-based hard pseudo labels.\nIt\ncan effectively improve the UDA Re-ID performance.\nThe teacher-student networks are trained end-to-end with\nEquation (2), (4), (5), (6), (7).\nLtarget =λt\nceLce + λt\nsce(La→m\nsce\n+ Lm→a\nsce\n)\n+ λt\nstri(La→m\nstri\n+ Lm→a\nstri )\n(8)\n4. Coupling Problem in Mean Teacher Based\nMethods\nThe Mean Teacher Baseline is illustrated in Figure 3\n(a) where the student gets supervision from its own EMA\nweights. In the Mean Teacher Baseline, the student and the\nteacher quickly converge to each other (coupling problem),\nwhich prevents them from exploring more diversiﬁed infor-\nmation. Authors of MMT [8] propose to pre-train 2 student\nnetworks with different seeds. As illustrated in Figure 3\n(b), two Mean Teacher networks are formed separately from\ntwo students, which alleviates the coupling problem. How-\never, different initializations decouple both teacher peers\nonly at ﬁrst epochs. Without a diversity encouragement dur-\ning the adaptation, both teachers still converge to each other\nalong with training. In Figure 3 (c), our proposed asym-\nmetric branches provide a diversity encouragement during\nthe adaptation, which decouples both teacher peers at all\nepochs.\nTo validate our idea, we propose to measure Euclidean\ndistance of appearance signature features between two\nteacher networks or two teacher branches. We extract fea-\nture vectors after global pooling on all images in the tar-\nget training set. Then, we calculate the Euclidean distance\nbetween feature vectors of both teachers and sum up the\ndistance of every image as the ﬁnal feature distance.\nIf\nthe feature distance is large, we can say that both teacher\npeers extract diversiﬁed features. Otherwise, the teacher\npeers converge to each other. As we can see from the left\ncurves in Figure 4, the feature distance between two teach-\ners in MMT is large at the beginning, but it decreases and\nthen stabilizes. Differently, the feature distance between\ntwo branches in our proposed method remains large during\nthe training. Moreover, we visualize the Euclidean distance\nof appearance signature features on all target training sam-\nples between teacher and student networks in Figure 4 right\ncurves. Our method can maintain a larger distance, which\nshows that it can better decouple teacher-student networks.\n5. Experiments\n5.1. Datasets and Evaluation Protocols\nOur proposed adaptation method is evaluated on 3 Re-ID\ndatasets: Market-1501, DukeMTMC-reID and MSMT17.\nMarket-1501 [30] dataset is collected in front of a super-\nmarket in Tsinghua University from 6 cameras.\nIt con-\ntains 12,936 images of 751 identities in the training set\nand 19,732 images of 750 identities in the testing set.\nDukeMTMC-reID [21] is a subset of the DukeMTMC\ndataset. It contains 16,522 images of 702 persons in the\ntraining set, 2,228 query images and 17,661 gallery images\nof 702 persons for testing from 8 cameras. MSMT17 [25]\nis a large-scale Re-ID dataset, which contains 32,621 train-\ning images of 1,041 identities and 93,820 testing images of\n3,060 identities collected from 15 cameras. Both Cumu-\nlative Matching Characteristics (CMC) and mean Average\nPrecisions (mAP) are used in our experiments.\n5.2. Implementation details\nHyper-parameters used in our proposed method are\nsearched empirically from the Market→Duke task and kept\nthe same for the other tasks. To conduct fair comparison\nwith state-of-the-arts, we use a ImageNet [4] pre-trained\nResNet-50 [10] as our backbone network. The backbone\ncan be extended to ResNet-based networks designed for\ncross domain tasks, e.g., IBN-ResNet-50 [18]. An Adam\noptimizer with a weight decay rate of 0.0005 is used to op-\ntimize our networks. Our networks are trained on 4 Nvidia\n1080Ti GPUs under Pytorch [19] framework. Detailed con-\nﬁgurations are given in the following paragraphs.\nStage1: Source domain supervised pre-training.\nWe\nset λs\nce = 0.5 and λs\ntri = 0.5 in Equation 1. The max\nepoch Epre is set to 80. For each epoch, the networks are\ntrained Rpre = 200 iterations.The initial learning rate is\nset to 0.00035 and is multiplied by 0.1 at the 40th and 70th\nepoch. For each iteration, 64 images of 16 identities are\nresized to 256*128 and fed into networks.\nStage2: Target domain unsupervised adaptation.\nFor\nthe clustering, we set the minimum cluster samples to 4 and\nthe density radius r=0.002. Re-ranking parameters for cal-\nculating distances are kept the same as in [22] for UDA\nsetting. Re-ranking between source and target domain is\nFigure 3. Comparison between (a) Mean Teacher Baseline (b) Mutual Mean Teaching [8] and (c) our Mean Teacher with cross-branch\nsupervised asymmetric branches. Teacher network is formed by exponential moving average (EMA) values of student network.\nFigure 4. Distance comparison between features extracted from a ResNet50 backbone on all samples in DukeMTMC-reid training set for\nMarket →Duke task. Left: Feature distance between two teacher models in MMT and between two teacher branches in our proposed\nmethod. Right: Feature distance between teacher and student networks.\nnot considered for fully unsupervised setting. The Mean\nTeacher network is initialized and updated in the way of\nEquation 3 with a smoothing coefﬁcient α = 0.999. We\nset λt\nce = 0.5, λt\nsce = 0.5 and λt\nstri = 1 in Equation 8.\nThe adaptation epoch Eada is set to 40. For each epoch,\nthe networks are trained Rada = 400 iterations with a ﬁxed\nlearning rate 0.00035. For each iteration, 64 images of 16\nclustering-based pseudo identities are resized to 256*128\nand fed into networks with Random erasing [32] data aug-\nmentation.\n5.3. Comparison with State-of-the-Art Methods\nWe compare our proposed methods with state-of-the-\nart UDA methods in Table 1 for 4 cross-dataset Re-ID\ntasks: Market→Duke, Duke →Market, Market →MSMT\nand Duke →MSMT. Post-processing techniques (e.g., Re-\nranking [31]) are not used in the comparison. Our proposed\nmethod outperforms MMT [8] (cluster number is set to 500,\n700 and 1500 respectively). We can also adjust the density\nradius in DBSCAN depending on target domain size to get\na better performance, but we think it is hard to know the tar-\nget domain size in the real world. With an IBN-ResNet50\n[18] backbone, the performance on 4 tasks can be further\nimproved. Examples of retrieved images are illustrated in\nFigure 5. Compared to MMT, embeddings from our pro-\nposed method contain more discriminative appearance in-\nformation (e.g., shoulder bag in the ﬁrst row), which are ro-\nbust to noisy information (e.g., pose variation in the second\nrow, occlusion in the third row and background variation in\nthe fourth row). This qualitative comparison conﬁrms that\nappearance signatures of our proposed method are of im-\nproved quality.\nWe compare unsupervised Re-ID methods in Table 2.\nSince the Mean Teacher is designed for handling label\nnoise, it is interesting to see the performance without source\npre-training, which introduces more label noise during the\nadaptation. This setting corresponds to an unsupervised Re-\nID. We use ImageNet pretained weights as initialization.\nOur proposed method outperforms previous unsupervised\nRe-ID by a large margin, which shows that ImageNet ini-\ntialization can provide basic discriminative capacity for Re-\nID.\nMMT [8] is the ﬁrst Mean Teacher based UDA Re-ID\nmethod. Authors of MMT propose to use 2 students and\n2 teachers with different initialization and stochastic data\naugmentation to address the coupling problem. We also\nuse Mean Teacher soft pseudo labels but propose a differ-\nent decoupling solution. Features in asymmetric branches\nare always extracted in different manners during the adap-\ntation. Compared to MMT, our proposed method has less\nparameters (approximately 10% less parameters and 20%\nless operations) but achieves better performance. Moreover,\nin the unsupervised scenario, we can not pre-train MMT\nwith different seeds to obtain different Re-ID initializations.\nUDA Methods\nMarket →Duke\nDuke →Market\nMarket →MSMT\nDuke →MSMT\nmAP\nRank1\nmAP\nRank1\nmAP\nRank1\nmAP\nRank1\nHHL (ECCV’18)[33]\n27.2\n46.9\n31.4\n62.2\n-\n-\n-\n-\nECN (CVPR’19)[34]\n40.4\n63.3\n43.0\n75.1\n8.5\n25.3\n10.2\n30.2\nPCB-PAST (ICCV’19)[29]\n54.3\n72.4\n54.6\n78.4\n-\n-\n-\n-\nSSG (ICCV’19)[6]\n53.4\n73.0\n58.3\n80.0\n13.2\n31.6\n13.3\n32.2\nUDAP (PR’20)[22]\n49.0\n68.4\n53.7\n75.8\n-\n-\n-\n-\nACT (AAAI’20)[27]\n54.5\n72.4\n60.6\n80.5\n-\n-\n-\n-\nECN+ (PAMI’20) [35]\n54.4\n74.0\n63.8\n84.1\n15.2\n40.4\n16.0\n42.5\nMMT500 (ICLR’20)(ResNet50)[8]\n63.1\n76.8\n71.2\n87.7\n16.6\n37.5\n17.9\n41.3\nMMT700 (ICLR’20)(ResNet50)[8]\n65.1\n78.0\n69.0\n86.8\n-\n-\n-\n-\nMMT1500 (ICLR’20)(ResNet50)[8]\n-\n-\n-\n-\n22.9\n49.2\n23.3\n50.1\nours (ResNet50)\n69.1\n82.0\n78.3\n92.5\n23.2\n49.2\n26.5\n54.3\nMMT500 (ICLR’20)(IBN-ResNet50)[8]\n65.7\n79.3\n76.5\n90.9\n19.6\n43.3\n23.3\n50.0\nMMT700 (ICLR’20)(IBN-ResNet50)[8]\n68.7\n81.8\n74.5\n91.1\n-\n-\n-\n-\nMMT1500 (ICLR’20)(IBN-ResNet50)[8]\n-\n-\n-\n-\n26.6\n54.4\n29.3\n58.2\nours (IBN-ResNet50)\n70.8\n83.3\n80.4\n93.0\n27.8\n55.5\n33.0\n61.8\nTable 1. Comparison of unsupervised domain adaptation (UDA) Re-ID methods (%) on medium-to-medium datasets (Market→Duke and\nDuke →Market) and medium-to-large datasets (Market →MSMT and Duke →MSMT).\nUnsupervised methods\nMarket\nDuke\nmAP\nRank1\nmAP\nRank1\nMMT500*(ICLR’20)[8]\n26.9\n48.0\n7.3\n12.7\nBUC (AAAI’19)[16]\n30.6\n61.0\n21.9\n40.2\nSoftSim (CVPR’20)[17]\n37.8\n71.7\n28.6\n52.5\nTSSL (AAAI’20)[26]\n43.3\n71.2\n38.5\n62.2\nMMT*+DBSCAN (ICLR’20)[8]\n53.5\n73.1\n54.5\n69.5\nours w/o Source pre-training\n65.1\n82.6\n63.1\n77.7\nTable 2. Comparison of unsupervised Re-ID methods (%) with\na ResNet50 backbone on Market and Duke datasets. * refers to\nour implementation where we remove the source pre-training step.\nDBSCAN refers to a DBSCAN clustering based on re-ranked dis-\ntance.\nFigure 5. Examples of retrieved most similar 5 images in Market\n→Duke task from MMT [8] and our proposed method. Given\na query image, different identity images are highlighted by red\nbounding boxes, while same identity images are highlighted by\ngreen bounding boxes.\nThis decoupling strategy becomes inappropriate. Our de-\ncoupling strategy relies on structural asymmetry instead of\ndifferent initializations, which is much more effective in the\nunsupervised scenario.\nACT [27] uses 2 networks, in which each network learns\nfrom its peer. Input data are split into inliers and ouliers af-\nter DBSCAN. Then, the ﬁrst network selects small entropy\ninliers to train the second network, while the second selects\nsmall entropy outliers to train the ﬁrst. This method en-\nhances input asymmetry by data split. Differently, our pro-\nposed method focuses on neural network structure asymme-\ntry.\n5.4. Ablation Studies\nEffectiveness of each component in ABMT.\nCompared\nwith traditional clustering-based Re-ID methods, the perfor-\nmance improvement mainly comes from DBSCAN on re-\nranked distance, asymmetric branches and cross-branch su-\npervision. We use a Mean Teacher Baseline where original\nResNet-50 and a K-Means++ clustering of 500 clusters are\nadopted. We conduct ablation studies by gradually adding\none component at each time. Results are shown in Table\n3. We can observe: (1) Our proposed asymmetric branches\nbring the most signiﬁcant performance improvement dur-\ning the adaptation. Moreover, as we can see from ﬁrst two\nrows in Table 3, they can directly improve the domain gen-\neralizability of appearance signatures without target adapta-\ntion. (2) DBSCAN on re-ranked distance works better than\na K-Means++ clustering of 500 clusters during the adap-\ntation. (3) Cross-branch supervision works on asymmetric\nbranches, which can further improve the adaptation perfor-\nmance.\nSource pre-training\nMarket →Duke\nDuke →Market\nmAP\nRank1\nmAP\nRank1\nResNet50\n29.6\n46.0\n31.8\n61.9\nResNet50+AB\n31.5\n49.7\n33.2\n63.2\nTarget adaptation\nMarket →Duke\nDuke →Market\nmAP\nRank1\nmAP\nRank1\nMT-Baseline+K-Means\n59.9\n74.8\n68.9\n88.2\nMT-Baseline+DBSCAN\n61.9\n77.3\n69.9\n88.3\nMT-Baseline+K-Means+AB\n64.7\n78.1\n74.8\n90.5\nMT-Baseline+K-Means+AB+Cross-branch\n66.4\n79.9\n76.8\n91.7\nMT-Baseline+DBSCAN+AB\n67.8\n81.1\n77.3\n92.0\nABMT(MT-Baseline+DBSCAN+AB+Cross-branch)\n69.1\n82.0\n78.3\n92.5\nABMT+Stochastic data augmentation\n68.8\n81.2\n77.6\n91.7\nABMT+Drop out\n68.3\n81.8\n77.9\n92.0\nTable 3. Ablation studies with ResNet50 backbone. MT-Baseline corresponds to the Mean Teacher Baseline in Figure 3 (a) with a ResNet-\n50. K-Means refers to a K-Means++ clustering whose cluster number is set to 500. AB refers to asymmetric branches. DBSCAN refers to\na DBSCAN clustering [5].\nStructure\nMarket →Duke\nDuke →Market\nmAP\nRank1\nmAP\nRank1\nABMT\n69.1\n82.0\n78.3\n92.5\nABMT w/o different pooling\n65.2\n79.7\n74.2\n90.1\nABMT w/o extra bottleneck\n67.5\n80.6\n77.6\n92.4\nABMT + one more branch\n68.1\n80.7\n76.2\n90.4\nTable 4. Ablation studies on structure of asymmetric branches.\nLoss\nMarket →Duke\nDuke →Market\nmAP\nRank1\nmAP\nRank1\nABMT\n69.1\n82.0\n78.3\n92.5\nABMT w/o Lce\n52.5\n69.6\n57.5\n79.8\nABMT w/o Lsce\n66.7\n79.8\n77.7\n92.2\nABMT w/o Lstri\n64.7\n78.5\n75.5\n91.2\nTable 5. Ablation studies on loss functions.\nEffectiveness of asymmetric branch structure.\nTo vali-\ndate the effectiveness of our proposed asymmetric branch\nstructure, we compare several possible structures:\n(1)\n2 branches with different pooling methods and different\ndepths, (2) 2 branches with same pooling methods (global\naverage pooling) but different depths, (3) 2 branches with\ndifferent methods but same depths, (4) 3 branches where the\nnew branch is composed of 5 bottleneck blocks and global\naverage pooling. From results given in Table 4, we can con-\nclude that different pooling methods play a more important\nrole in asymmetric branches.\nEffectiveness of loss functions.\nWe conduct ablation\nstudies on loss functions used in our proposed method and\nreport results in Table 5. The degree of inﬂuence of 3 loss\nfunctions used in our proposed method: Lce > Lsce >\nLstri.\nCan traditional decoupling methods further improve the\nperformance?\nStochastic data augmentation (teacher in-\nputs and student inputs are under stochastic data augmen-\ntation methods) and drop out (teacher feature vectors and\nstudent feature vectors are under independent drop out op-\nerations before classiﬁers) are 2 widely-used methods to\nprovide random noise, which also helps to decouple the\nweights between the teacher and the student. We conduct\nexperiments with stochastic data augmentation . The re-\nsults in Table 3 show that they can not further improve the\nUDA Re-ID performance. These methods are not designed\nfor ﬁne-grained Re-ID task. As UDA Re-ID performance is\nalready very high, they can not contribute anymore.\n6. Conclusion\nIn this paper, we propose a novel unsupervised cross-\ndomain Re-ID framework. Our proposed method is mainly\nbased on learning from noisy pseudo labels generated by\nclustering and Mean Teacher.\nA self-ensembled Mean\nTeacher is robust to label noise, but the coupling problem\ninside paired teacher-student networks leads to a perfor-\nmance bottleneck. To address this problem, we propose\nasymmetric branches and cross-branch supervision, which\ncan effectively enhance the diversity in two aspects: appear-\nance signature features and teacher-student weights. By en-\nhancing the diversity in the teacher-student networks, our\nproposed method achieves better performance on both un-\nsupervised domain adaptation and fully unsupervised Re-ID\ntasks.\nReferences\n[1] Hao Chen, Benoit Lagadec, and Francois Bremond. Learn-\ning discriminative and generalizable representations by\nspatial-channel partition for person re-identiﬁcation. In The\nIEEE Winter Conference on Applications of Computer Vision\n(WACV), March 2020.\n[2] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Instance-\nguided context rendering for cross-domain person re-\nidentiﬁcation.\nIn Proceedings of the IEEE International\nConference on Computer Vision, pages 232–242, 2019.\n[3] Zuozhuo Dai, Mingqiang Chen, Siyu Zhu, and Ping Tan.\nBatch dropblock network for person re-identiﬁcation and be-\nyond. 2019 IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 3690–3700, 2018.\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009.\n[5] Martin Ester, Hans-Peter Kriegel, J¨org Sander, and Xiaowei\nXu. A density-based algorithm for discovering clusters in\nlarge spatial databases with noise. In KDD, 1996.\n[6] Yang Fu, Yunchao Wei, Guanshuo Wang, Xi Zhou, Honghui\nShi, and Thomas S. Huang. Self-similarity grouping: A sim-\nple unsupervised cross domain adaptation approach for per-\nson re-identiﬁcation. 2019 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 6111–6120, 2018.\n[7] Yang Fu, Yunchao Wei, Yuqian Zhou, Honghui Shi, Gao\nHuang, Xinchao Wang, Zhiqiang Yao, and Thomas S.\nHuang.\nHorizontal pyramid matching for person re-\nidentiﬁcation. In AAAI, 2018.\n[8] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\nteaching: Pseudo label reﬁnery for unsupervised domain\nadaptation on person re-identiﬁcation. In International Con-\nference on Learning Representations, 2020.\n[9] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu,\nWeihua Hu, Ivor Wai-Hung Tsang, and Masashi Sugiyama.\nCo-teaching: Robust training of deep neural networks with\nextremely noisy labels. In NeurIPS, 2018.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. 2016 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 770–778, 2015.\n[11] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\nfense of the triplet loss for person re-identiﬁcation. arXiv\npreprint arXiv:1703.07737, 2017.\n[12] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\nDistilling the knowledge in a neural network.\nArXiv,\nabs/1503.02531, 2015.\n[13] Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, and\nRynson W. H. Lau. Dual student: Breaking the limits of\nthe teacher in semi-supervised learning. 2019 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n6727–6735, 2019.\n[14] Samuli Laine and Timo Aila. Temporal ensembling for semi-\nsupervised learning. ArXiv, abs/1610.02242, 2016.\n[15] Shan Lin, Haoliang Li, Chang-Tsun Li, and Alex Chichung\nKot. Multi-task mid-level feature alignment network for un-\nsupervised cross-dataset person re-identiﬁcation. In BMVC,\n2018.\n[16] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\nYang. A bottom-up clustering approach to unsupervised per-\nson re-identiﬁcation. In AAAI, 2019.\n[17] Yutian Lin, Lingxi Xie, Yu Wu, Chenggang Yan, and Qi\nTian.\nUnsupervised person re-identiﬁcation via softened\nsimilarity learning. ArXiv, abs/2004.03547, 2020.\n[18] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two\nat once: Enhancing learning and generalization capacities\nvia ibn-net. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 464–479, 2018.\n[19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas K¨opf, Edward Yang, Zach DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\nFang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\nperative style, high-performance deep learning library.\nIn\nNeurIPS, 2019.\n[20] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan\nYuille. Deep co-training for semi-supervised image recogni-\ntion. In Proceedings of the european conference on computer\nvision (eccv), pages 135–152, 2018.\n[21] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking.\nIn European Confer-\nence on Computer Vision workshop on Benchmarking Multi-\nTarget Tracking, 2016.\n[22] Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian\nZhang, Chang Huang, and Xinggang Wang. Unsupervised\ndomain adaptive re-identiﬁcation: Theory and practice. Pat-\ntern Recognition, 102:107173, 2020.\n[23] Antti Tarvainen and Harri Valpola. Mean teachers are better\nrole models: Weight-averaged consistency targets improve\nsemi-supervised deep learning results. In NIPS, 2017.\n[24] Jingya Wang, Xiatian Zhu, Shaogang Gong, and Wei Li.\nTransferable joint attribute-identity deep learning for unsu-\npervised person re-identiﬁcation. 2018 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n2275–2284, 2018.\n[25] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nPerson transfer gan to bridge domain gap for person re-\nidentiﬁcation.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 79–88, 2018.\n[26] Guile Wu, Xiatian Zhu, and Shaogang Gong.\nTrack-\nlet self-supervised learning for unsupervised person re-\nidentiﬁcation. In AAAI 2020, 2020.\n[27] Fengxiang Yang, Ke Li, Zhun Zhong, Zhiming Luo, Xing\nSun, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong\nJi, and Shaozi Li. Asymmetric co-teaching for unsupervised\ncross domain person re-identiﬁcation. 2020.\n[28] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Wai-\nHung Tsang, and Masashi Sugiyama. How does disagree-\nment help generalization against label corruption? In ICML,\n2019.\n[29] Xinyu Zhang, Jiewei Cao, Chunhua Shen, and Mingyu You.\nSelf-training with progressive augmentation for unsuper-\nvised cross-domain person re-identiﬁcation. 2019 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n8221–8230, 2019.\n[30] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\ndong Wang, and Qi Tian. Scalable person re-identiﬁcation:\nA benchmark. 2015 IEEE International Conference on Com-\nputer Vision (ICCV), pages 1116–1124, 2015.\n[31] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-\nranking person re-identiﬁcation with k-reciprocal encoding.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 1318–1327, 2017.\n[32] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In AAAI, 2020.\n[33] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Gener-\nalizing a person retrieval model hetero- and homogeneously.\nIn The European Conference on Computer Vision (ECCV),\nSeptember 2018.\n[34] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and\nYi Yang.\nInvariance matters: Exemplar memory for do-\nmain adaptive person re-identiﬁcation.\nIn Proceedings of\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2019.\n[35] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi\nYang. Learning to adapt invariance in memory for person re-\nidentiﬁcation. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2020.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-11-27",
  "updated": "2020-11-27"
}