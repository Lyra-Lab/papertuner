{
  "id": "http://arxiv.org/abs/2011.13974v1",
  "title": "Trends in deep learning for medical hyperspectral image analysis",
  "authors": [
    "Uzair Khan",
    "Paheding Sidike",
    "Colin Elkin",
    "Vijay Devabhaktuni"
  ],
  "abstract": "Deep learning algorithms have seen acute growth of interest in their\napplications throughout several fields of interest in the last decade, with\nmedical hyperspectral imaging being a particularly promising domain. So far, to\nthe best of our knowledge, there is no review paper that discusses the\nimplementation of deep learning for medical hyperspectral imaging, which is\nwhat this review paper aims to accomplish by examining publications that\ncurrently utilize deep learning to perform effective analysis of medical\nhyperspectral imagery. This paper discusses deep learning concepts that are\nrelevant and applicable to medical hyperspectral imaging analysis, several of\nwhich have been implemented since the boom in deep learning. This will comprise\nof reviewing the use of deep learning for classification, segmentation, and\ndetection in order to investigate the analysis of medical hyperspectral\nimaging. Lastly, we discuss the current and future challenges pertaining to\nthis discipline and the possible efforts to overcome such trials.",
  "text": "1 \n \nTrends in deep learning for medical hyperspectral \nimage analysis \nUzair Khan 1, Paheding Sidike 2, Colin Elkin 1 , and Vijay Devabhaktuni 1  \n1 Department of Electrical and Computer Engineering, Purdue University Northwest  \n2 Department of Applied Computing, Michigan Technological University \n \nAbstract: Deep learning algorithms have seen acute growth of interest in their applications \nthroughout several fields of interest in the last decade, with medical hyperspectral imaging being a \nparticularly promising domain. So far, to best of our knowledge, there is no review paper that \ndiscusses the implementation of deep learning for medical hyperspectral imaging, which is what \nthis review paper aims to accomplish by examining publications that currently utilize deep learning \nto perform effective analysis of medical hyperspectral imagery. This paper discusses deep learning \nconcepts that are relevant and applicable to medical hyperspectral imaging analysis, several of \nwhich have been implemented since the boom in deep learning. This will comprise of reviewing the \nuse of deep learning for classification, segmentation, and detection in order to investigate the \nanalysis of medical hyperspectral imaging. Lastly, we discuss the current and future challenges \npertinent to this discipline and the possible efforts to overcome such trials.  \nKeywords: Deep learning, neural networks, machine learning, medical image analysis, medical \nhyperspectral imaging, COVID-19 \n \n1. Introduction \nMedical imaging refers to images used to aid in clinical work relative to the human body such \nas surgical procedures, diagnoses for impeding diseases, or simply to analyze and study body \nfunctions and is primarily based on radiological research. For the last couple of decades in particular, \nmodern imaging techniques such as X-rays, magnetic resonance imaging (MRI), and ultrasound have \nhad significant impacts on not only medical symptoms analysis but also on the spawning of more \nimaging techniques for improvised examination. Computed tomography (CT) scan, for example, is \nan X-ray procedure that displays the cross-sectional image of the body, which now also helps to \nassess brain or head-related injuries [1]. Another circumstance in which X-rays have been influential \nfor further progress in research and application is Mammography, where a low energy x-ray photon \nbeam is implemented to diagnose breast cancer, which is presently a common medical problem \nworldwide [2]. These implementations of x-rays are more commonly known as computer-aided \ndiagnosis (CAD) [3]. More than often for medical imaging, image fusion is utilized, in which the final \noutput image consists of more meaningful information, which is obtained from multiple input images \n[4]. An emerging imaging method in the discipline of biomedicine is Terahertz (THz) imaging, which \nis currently working to overcome its limitations with the aid of nanotechnology by finding its roots \nin the highly promising medical imaging methodology and becoming safer than traditional \nmethodologies [5]. The primary goals for any effort to obtain medical images for diagnosis is via a \nnon-invasive and inexpensive methodology. While the aforementioned techniques provide the \ndesired output, each method is either invasive in some form, or not economical and in some cases, \nneither. This leads to professionals in the medical field and in academia alike to look towards \nalternative methods for imaging that are better suited for the required criteria of a non-invasive and \na low-cost method. \nHyperspectral imaging (HSI) is a developing imaging technique amid the medical imaging \nmodality and offers noninvasive disease diagnosis. HSI comprises various images aligned in adjacent \nnarrow wavelengths or spectral bands (often in the range of hundreds) and recreates a reflectance \n2 \n \n \nspectrum of all the pixels in the band [6]. This is done by separating light using a spectral separator \nconsisting of bandpass filter(s) and accumulating it on a focal plane detector (typically a \ncomplementary metal oxide semiconductor (CMOS) sensor) to form the image. Hyperspectral \nimaging has traditionally been used for remote sensing [7] [8], agriculture [9], food safety & quality \nassessment [10] [11], image enhancement [12], disaster monitoring [13] [14], feature extraction [15], \nclassification [16], object detection [17] [18] [19], and recently even for conservation of art [20]. For \nmedical imaging, it is principally obtained by targeting tissue samples by transmission of light and \nused to diagnose and detect various types of cancers [21] and other medical applications. With cancer \nbeing the second leading cause of death in the US, this significantly impacts the medical society and \nits research to eliminate cancer. Medical Hyperspectral Imaging (MHSI) has previously aided in \nsuccessfully distinguishing between tumors and normal tissues from a rat breast tumor model, \nproviding a clear indication of how influential MHSI can be for future research on breast cancer [22]. \nIt has also proven its significance in detection of cancerous tissue cell detection from normal tissue \nspecimen for neck and head cancer [23]. Gastric cancer is the most common cause of cancer in the \nUnited States, and its future diagnosis will also be significantly simplified thanks to MHSI [24]. \nFurthermore, it has also facilitated cancer detection of head and neck in surgical specimens [25]. All \nthese MHSI applications for several medical diagnosis have led to the development of multiple \nalgorithms to more accurately and efficiently classify the cancerous tissues from a sample [26], which \nfurther instigates on the different techniques for the processing and analysis of MHSI.  \nArtificial intelligence, and its most common subset known as Machine Learning, is a highly \npopular approach to process hyperspectral images and extract meaningful data from it. Machine \nlearning (ML) algorithms make use of data and statistical models to learn and identify patterns to \ncomplete specific tasks and make decisions with or without human supervision. Several such ML \nalgorithms are utilized when examination of hyperspectral images is considered and consequently \nin identifying and classifying differences in a tissue specimen when studying MHSI. While ML \nalgorithms can be rudimentarily divided into supervised and unsupervised learning models, the \nalgorithms are prone to develop into increasingly complex models as we delve further into deep \nlearning (DL) [27], a branch of ML that is influenced by the structure and function of the human brain. \nIn supervised learning, the model is fashioned from a dataset containing a number of input features \nand outputs, or labels. This model is formed by finding the optimal model parameters from a training \nsample of the dataset, which is subsequently used to predict the outcome based on the minimized \ncost function. Unsupervised learning processes data without any particular structure and is trained \nto find patterns and typically create groupings based on clusters. \nTraditional ML algorithms typically used for MHSI applications lean towards classification \nmodels for identification and diagnosis, all of which include k-nearest neighbors (kNN) [28], linear \ndiscriminant analysis (LDA) [29], and support vector machines (SVM) [30] [31] [32] [33], with the \nlatter being most prominently applied. With ML already heavily facilitating the processing of MHSI, \nthe next logical step is to apply deep learning to achieve a more cost-effective and more accurate \nprediction model, which will provide a more comprehensive diagnosis to this pertaining medical \nissue. Deep learning in general has started taking off extraordinarily as early as 2012 [34] [35], and \ndeep learning for MHSI has been no different in this regard. This subset of ML has already proven to \noutperform traditional ML techniques in the principle of head and neck surgery [36] and looks \npromising for MHSI in a broader aspect. Deep learning has also been applied for classification of a \npreviously listed example related to head and neck cancer and shows significant improvement in \naccuracy over SVM and kNN [37]. \nOur end goal from this paper will be to highlight the key DL methods being used for MHSI and \nthe challenges for successful application of DL in MHSI. This is because the DL methods to be \nimplemented for MHSI in the coming decade will have a significant implication for future studies in \nseveral key areas of the medical discipline, including cancer research. The rest of the survey will be \ndiscussed as follows: Section 2 will introduce the fundamentals of DL methods that are currently \nbeing used as well as several emerging algorithms. Section 3 will discuss which DL methods are \n3 \n \n \npresently implemented for MHSI as well as additional algorithms that could be applied in the coming \ndecade. Section 4 will ultimately discuss the current challenges faced by DL for MHSI. \n \n \nFigure 1. Chart representing the number of papers published in the last decade The bars depict the quantity of papers \npublished spread apart evenly for every two consecutive years for each category of DL methodology. \nAs previously mentioned, the trend in research for DL methods being used for MHSI has seen a \nsudden surge since as recently as 2012. This can easily be traced back to the ImageNet Large Scale \nVisual Recognition Challenge (ILSVRC) that was conducted in 2012, in which a deep learning \nmethod, which we will discuss later in Section 2, broke prior records by providing accuracy results \nclose to 41% better than the best previous attempt [38]. Since then, more and more researchers have \nexplored DL methodologies in a wide array of disciplines. Our survey covers papers published that \nspecifically target the confrontation of medical hyperspectral imaging with deep learning techniques. \nFor papers to be considered valid for our purpose, we examined keywords only with major \npublishers comprising of IEEE, Elsevier, Springer, SPIE, MDPI, International Journal of Biomedical \nImaging, Journal of Biomedical Optics, and a handful of small publishers via Google Scholar as well \nas the publisher‚Äôs respective search engines, with the latest being published in December 2019. While \nnumerous publications are already present for deep learning being implemented for medical image \nanalysis, with our topic of discussion being so highly specific, we cross-checked all sections within \nthe publications that matched the subject matter of deep learning being implemented for medical \nhyperspectral imaging to verify papers for this survey. For certain situations in which published work \ncorresponded with multiple papers, we only considered papers with greater significance in regard \nto their contributions. The goal for these papers being included in this survey, as mentioned earlier, \nencompassed the benefits of deep learning methodologies, how they are contributing towards \nmedical hyperspectral imaging, and the challenges being faced for effectively applying these deep \nlearning techniques to medical hyperspectral imaging.  \n4 \n \n \nWith the rise of a global pandemic in the shape of coronavirus known as COVID-19 in late \nDecember of 2019, the race to find the solutions to its discovery and eventually its termination in form \nof a vaccine has severely heated up during early 2020. COVID-19 cases arise from cases of pneumonia, \nand as a result, affected humans display severe respiratory diseases and eventually death. In terms \nof applications of DL for medical imaging in regard to COVID-19, it is currently in the phase of being \nutilized heavily to detect the presence of COVID-19 amongst the screening of potential patients. The \npredominant testing technique currently being employed to detect COVID-19 is transcription-\npolymerase chain reaction (RT-PCR). X-ray procedures such CT scans are showing a pivotal role in \nearly diagnosis, and it is in this domain that DL is providing significant results [39]. With several DL \nmethodologies [40] [41] already cropping up to assist in the detection of COVID-19, it will not be long \nbefore DL also starts supplementing the eradication of the virus. For the purposes of this paper, \nhowever, we could not find any paper discussing DL in MHSI for the COVID-19 within our search \ncriteria. \n2. Deep Learning Methods \nMachine Learning methodologies are typically divided into either supervised or unsupervised \nlearning algorithms. A learning algorithm that makes use of a dataset comprising of a set of input \nfeatures and an output label to obtain a predicting model is known as a supervised learning \nalgorithm. The output label in the dataset for supervised learning could be categorized as either a \nclassification or regression. A classification problem categorizes the output into discrete values such \nas type A and type B. A regression problem provides the output as continuous value, which could \nrepresent a real value such as dollars or a more symbolic numerical value, such as a normalization of \na different number. The learning part in supervised learning refers to finding the optimal weights or \nparameters that minimize the cost (or loss) function. This would mean that the no further loss can be \nachieved and that the weights cannot be further improvised, which results in the best-fit model with \nthe given inputs, bias, and learning rate. After successfully creating the model, a portion of the data \nis used to test the accuracy and/or the efficiency of the model. This is simply done by comparing the \noutput from the model against the actual value from the dataset. While this is also implemented in \nthe process of minimizing the cost function, the goal of testing the model is to perceive the \nvulnerability of the model.  \nAn unsupervised learning algorithm is used to figure out the underlying structure of a dataset, \nwhich does not consist of a definite output. The algorithm does this by either clustering the output \ninto categories or by finding the associative properties among the input parameters. Since the initial \nweights and parameters are selected ambiguously, the resulting output is not always similar every \ntime, as a different model is obtained during every training process. In the following sections, we will \ndelve upon various deep learning methods, which are built upon the ML fundamentals previously \nstated.  \n \n2.1 Supervised Learning Methods \n2.1.1 Neural Networks \nNeural Networks (also known as multilayer perceptron) are a category of learning algorithms \nbuilt upon the idea and structure of a human brain, as the name suggests, and it lays the foundation \nfor the majority of the deep learning methods. A neural network contains neurons (mathematically \nreferred to as a perceptron) as the base unit which comprises of an activation number, and other sets \nof parameters such as weights W and bias b. This can be simplified to an activation function, which \ncan be expressed as \n       \n \n \n \n \n          ùí∂(1) = ùúé(ùëäùëé(0) + ùëè), \n \n \n \n \n \n \n \n  (1) \nwhere a(0) represents the initial activation numbers, or the input features, and a(1) refers to the \nactivation numbers for the next layer. The ùúé refers to the transfer function, which is traditionally \ndenoted as either a sigmoid function or a step function. To form a fully constructed layer at each \ninterval, we obtain the dot product between the weight vector and the input features vector. This \n5 \n \n \nrepresent a single layer of neurons, which has a simple feedforward mechanism, i.e. the neuron takes \nin a single input, performs the operation on it, and then passes it on to the next layer. A multi-layer \nperceptron (MLP) consists of two or more layers of neurons or perceptrons, which are also known as \nhidden layers. All of these layers from MLP combine together to form the basis of what is more \ncommonly known as a Deep Neural Network (DNN). While there are different additional DNN \ntechniques on which we will be expanding next, MLP is one of the most basic DNN architectures.  \n \nFigure 2. Visual representation of (a) simple neural network: a machine learning algorithm modelled from the \nbiological neural network of a human brain, (b) deep neural network: a neural network comprising of more than one hidden \nlayer of neurons for its architecture. \n2.1.2 Convolutional Neural Networks \nConvolution Neural Network (CNN) is an exceedingly popular deep learning algorithm that is used \nto classify an input image by recognizing patterns and features in order to differentiate between \nobjects. In fact, one of the earliest perceptron concepts in 1957 was designed with the purpose of \nclassifying the input image as man or woman [42]. The weights in a CNN architecture are designed \nto perform convolution operations on the input image rather than the features. In a convolution layer, \nthe spatial structure is preserved along with any temporal dependencies with the usage of filters or \nkernels. This means that instead of appropriating a weight for an input feature via the combined \nstretched matrix from the input features, a small filter with appropriate dimensions relative to the \ninput image is applied to perform the element-wise dot product between the filter and a chunk of the \nimage otherwise ‚Äòconvolved‚Äô in order to obtain the activation. Note that the depth of the filter should \nmatch the depth of the image to make this convolution possible. This process is repeated over all the \nspatial locations of the image, thereby providing us with a final activation map over the spatial \nregion. \nCNNs are predominantly part of typical classification architecture widely used in medical image \nanalysis. One example for simpler architectures would be LeNet [43], which was introduced over two \ndecades ago, which despite being a rather shallow network, displayed the basic concept of a CNN in \na simple and elegant fashion, using the tangent function as the activation function. Later on, Alexnet \n[38] shattered expectations at the ILSVRC in 2012. Bearing similar characteristics to LeNet, AlexNet \n[38] also made use of kernels with a larger field of layers closer to the input and smaller kernels closer \nto the output, with the key difference being in the assimilation of rectified linear units (ReLU) for the \nactivation function, which has since become the activation function of choice for modern CNNs due \nto higher classification accuracy. It can be duly noted that the rise in popularity of deep learning \ntechniques also coincides after this series of events. This has led to the exploration of several \narchitectures with farther reaching hidden layers. Building upon the base of deeper networks, more \nintricate layers are introduced which further reduces the error rate in a successful classification while \n(a)                                               (b) \n6 \n \n \nalso doing so more efficiently. In ILSVRC 2014, GoogleNet [44], used ‚Äúinception‚Äù blocks that \nessentially reduced the number of operations performed at each layer by making use of smaller sets \nof convolutions. This inception module used the different sizes of convolution layers together, \nthereby allowing the final filter concatenation to stack the output together. Later, ResNet [45] was \nalso introduced, which comprised of ResNet blocks. The residual block, as the name suggests, learns \nresidual features, which in turn provide a shortcut that skips one or more layers. This further \nextended the efficiency of deeper models, thus enabling the more common vanishing gradient \nproblem for deep learning models to be solved. \n \n \n \n2.1.3 U-net \nSegmentation is a popular architecture not only among the medical image analysis community \nbut also for the field of computer vision in general. While CNNs classify the pixels of an image, \nsegmentation provides inference by making prediction labels for each pixel that facilitates the \nenclosing of core object locations, thus separating the image into sections of fields. While this provides \nthe unfortunate overlap of neighboring pixels over regions with the same convolutions being \ncomputed multiple times, several solutions have been proposed to overcome this ordeal, the most \nprominent of which is known as a Fully Convolution Network (FCN). The core idea behind FCN is \nto take the original CNN with arbitrarily sized input images and use the fully connected layer as \nconvolutions to produce the segmented output. However, this still results in a degraded feature map \ndue to the propagation through several pooling layers.  \nU-net architecture [46] provides a solution for this issue built upon the foundation of the FCN. \nThis architecture consists of the basic FCN supported by an upsampling layer as opposed to a pooling \nlayer, which concludes in an increase in resolution for the final output image. It uses stochastic \ngradient descent to train the network and calculate the energy equation with the aid of softmax, \nimplemented pixel-wide across final feature map in which the softmax layer is defined as \n \nùëùùëò(ùë•) =\nùëíùëéùëò(ùë•)\n‚àë\nùëíùëéùëò(ùë•)\nùêæ\nùëò‚Ä≤=1\n \n,                     \n \n \n  (2) \n \nwhere ak(x) is the activation function. This is then applied to an energy equation as \n \nùê∏= ‚àëùë§(ùë•)log‚Å°(ùëùùëò(ùë•)(ùë•)),  \n \n \n \n \n \n \n  (3) \n \nCONVOLUTION+\nRELU \nPOOLING\nCONVOLUTION+\nRELU \nPOOLING \nFLATEN \n- \n-\nINPUT\nFULLY  \nSOFTMAX \nFEATURE LEARNING \nCLASSIFICATION \nDOG\nCAT\nDog \nEAGLE\nCONNECTED \nFigure 3. Convolutional Neural Network (CNN): a deep learning architecture containing several convolutional and \npooling layers, which are then connected at the output. This output in turn is used to classify the input image provided. \nCNN has proved exceeding popular in computer vision and visual image analysis as of late due to its high accuracy and \nprecision. \n7 \n \n \nwhere w is the weight matrix for the model. This energy function is in short, a combination of the \nsoft-max layer over the final feature map with the cross-entropy loss function. \n \n \n \n \n2.1.4 Recurrent Neural Networks \nRecurrent Neural Networks (RNNs) were developed with the thought of tackling progression \nof vectors over time, which is something CNNs cannot accomplish, as they are restricted to handling \nfixed vector size to provide fixed-size outputs. In addition, CNNs operate on a fixed number of layers. \nRNNs, by contrast, can have input and output vector of varying lengths, which make them invaluable \nfor undertaking problems in the Natural Language Processing (NLP) domain [47], as the input matrix \nin such an application is constantly evolving. Another way in which the RNNs differ from traditional \nCNNs is that they are not feedforward systems and instead loop the outputs of each hidden layer \nback to itself. For a classification problem, the output from the hidden layer is used as the inputs \nalong with the normal input for the hidden layer. This can be represented as \n‚Ñéùë°= ùúé(ùëäùë•ùë°+ ùëâ‚Ñéùë°‚àí1 + ùëè), \n                              (4) \nwhere ùë•ùë° is the input vector, ‚Ñéùë° is the hidden layer vector, W and V are the weight matrices, and b \nis the bias vector. \nThe RNN, however, also experiences the same vanishing gradient problem during training as \nregular DNNs. Several solutions involving special memory units have been proposed for RNNs, with \nthe Long Short Term Memory (LSTM) cell [34] being one of the most popular ones as well as having \nthe distinction of being of the earliest.  \n572 x 572\n570 x 570 \n568 x 568 \n1 \n64 64 \n2842 \n2822 \n2802 \n128 128 \n256 256\n512 \n512 \n512 \n256 \n256 128 \n128 64 \n64 \n2 \n512 \n1024 \n1362 \n1382 \n1402 \n642 \n662\n682 \n282 \n302 \n322\n522 \n542 \n562 \n1002 \n1022 \n1042\n1962 \n1982 \n2002 \n392 x 392 \n390 x 390\n388 x 388 \n388 x 388 \nInput \nImage \nFile \nOutput \nSegmentation \nMap \nLegend \nConvolution \nMax-pooling \nUp-convolution \nFinal \nConcatenate \nFigure 3. U-net architecture: a deep learning algorithm based upon Fully Convolutional Network (FCN) which in \nitself is built upon the foundation of CNN. Majority of the segmentation techniques involve u-net in one way or another, \nwith it forming the core component of a proposed model for segmentation. \n8 \n \n \n \n \n \n2.2 Unsupervised Learning Methods \n2.2.1 Auto-encoders \nAutoencoder is a type of neural network that is primarily trained to provide a learned \nrepresentation of the input. In other words, an autoencoder generates a replicate for the provided \ninput after undergoing a handful of operations. These operations are performed over a single \nhidden layer in which the model ‚Äòencodes‚Äô and then ‚Äòdecodes‚Äô the input to provide the mapped \noutput. Although this process might seem meaningless on the surface, this gives us the \nopportunity to map how data is projected for a lower dimension. This is because the hidden \nlayer has the smallest dimension in this network, and it also encompasses all the information to \nreconstruct the output for a same class of input. This is particularly useful in the case of anomaly \ndetection, in which the autoencoder is fed the inputs from same class of data. Since the mapping \nfeatures would produce an incoherent result with respect to the autoencoder hidden layer, the \nanomalies would be discovered easily.   \n \nFigure 5. A simple Recurrent Neural Network (RNN): a neural network in which the previous output is also used as \ninputs to modify the state of the hidden layer neuron. RNN has the capability of processing inputs of varying lengths \nwithout change in the model size, which is invaluable for constantly evolving datasets. \n9 \n \n \n \nFigure 4. Comprehensive layout of an autoencoder: a type of neural network that is used to understand the \nefficient data coding, which is particularly useful for dimension reduction for a given dataset. \n2.2.2. Restricted Boltzmann Machines \nRestricted Boltzmann Machines (RBMs) are a relatively simpler deep learning system \ncomprising of two layers: input and hidden. This forms the basis of deep belief networks. The \nnodes (neurons) from each layer form inter-layer connections while ‚Äòrestricting‚Äô intra-layer \nconnections, which helps derive its name. The RBM comprises of bidirectional communications \nbetween layers and is thus a generative model that uses the hidden layer to fashion new data \npoints. It does so by defining an energy function for a state of input and visible units of (y,z) as \n \nùê∏(ùë¶, ùëß) = ‚àëùëéùëñùë¶ùëñ\nùëñ\n‚àí‚àëùëèùëóùëßùëó\nùëó\n‚àí‚àë‚àëùë¶ùëñùë§ùëñ,ùëóùëßùëó\nùëó\nùëñ\n,  \n \n \n \n  (5) \nwhere a and b are the biases, w is the weight matrix, and y and z are the states for hidden unit j \nand visible unit i. The pair of possible hidden and visible vector is computed by finding the \nprobability as \nùëù(ùë¶, ùëß) =\n1\nùëßùëí‚àíùê∏(ùë¶,ùëß),  \n \n \n \n \n \n \n  (6) \nwhere Z is known as a partition function. The RBMs are primarily used to pre-train a NN to \ngenerate the initial weights and then are used to form the foundation for other deep learning \nmethods such as a Deep Belief Network (DBN). These DBNs are then used for many different \napplications, including cyber security [48], NLP [49], and of course medical image analysis [50]. \n \n10 \n \n \n \n \n \n2.2.3. Generative Adversarial Networks (GANs) \nGANs, used widely in image, video and audio generative scenarios [51], are a generative \narchitecture (as the name suggests) based largely on probability-related setups for unlabeled \ndatasets, which provide a better substitute to maximum likelihood estimators. A GAN \narchitecture pits two neural networks against one another, with the purpose of generating \nsynthetic labels that are comparable to actual data. In each iteration run, the two neural networks \nkeep improving repeatedly at the required task. This procedure continues until the output from \nthe generator resembles the actual sample data as closely as possible.  \nThe generator network and the discriminator network competing against each other, as \ndisplayed in Fig. 8, can be considered the two supposed neural networks that highlight the \nconcept of GAN as an example. The example can be considered as a min-max situation, in which \nthe function V (D,G) can be described as \n \nùëöùëñùëõùê∫ùëöùëéùë•ùê∑ùëâ(ùê∑, ùê∫) = ùê∏ùë•~ùëÉùëëùëéùë°ùëé(ùë•)[log(ùê∑(ùë•))] + ùê∏ùëß~ùëÉùëëùëéùë°ùëé(ùëß)[log(1 ‚àíùê∑(ùê∫(ùëß)))].    (7) \n \n \n \n \n \nh3 \nh2 \nh1 \nx \nFigure 5. RBM block diagram: A stochastic neural network that uses bidirectional layers (and consequently \ngenerative), comprising of hidden layers (h1, h2, & h3) and input layer x to effectively learn probability distributions\nfor the given set of inputs.  \nActual \nSample \nDiscriminator \nLabel \nRandomizer \nVariable \nGenerator \nSample \nFigure 6. Generative Adversarial Network block diagram: an architecture that pits two networks against each other,\nwhich is used to eventually learn the patterns of the input data in order to produce an output as closely as possible to an \nactual sample of the input. \n11 \n \n \n3. DL Methods for MHSI \n3.1 Classification \n Classification for pathological images has been one of the earliest fields of not just MHSI, but \nmedical analysis in general, in which deep learning techniques have been a significant influence. \nTypical procedures for processing the MHSI using classification include applications such as cell \nclassification [52] in order to identify and classify cancerous cells. This is also sometimes synonymous \nwith detection techniques on the surface, in which the architecture is designed to detect traces of \ncancer from samples, as opposed to just classifying the cells themselves, which we will discuss in the \nnext section. Classification techniques for MHSI often make use of transfer learning, which has \nproven to be quite useful in these scenarios. MHSI typically utilize small datasets comprising of \nmedical diagnosis images (typically hundreds or thousands), as opposed to the field of computer \nvision in which the datasets could consist of millions of sample images, which is one of the reasons \nwhy transfer learning has been feasible for this discipline. \nTransfer learning is fundamentally the use of a pre-trained network to classify input images \nfrom testing samples, and as mentioned earlier, the smaller datasets allow greater practicality and \nease of use, as opposed to training the network to obtain new input features every time for virtually \nsimilar input image datasets. The implementation of deep learning for MHSI took some time to catch \non, compared to deep learning being used for other areas of research. Earlier implementations of \ndeep learning in MHSI began with classification of benign & malignant tissues or cell samples using \nANNs, typically MLPs [21]. A particular study that explores HSI for characterizing kidney stones [53] \nmade use of Principal Component Analysis (PCA) to determine appropriate variables to be used for \na simple ANN model comprising of a hidden layer with four nodes, which was used to classify the \ntype of kidney stone from the HIS, whereas a similar approach was also undertaken to classify \ndifferent cancer types [54] [55]. There have been situations in which ANNs provided inferior results, \nwhich in turn highlights the problems related to deep learning, namely the absence of larger datasets. \nThis particular study assessed the performance between four supervised algorithms that pitted ANN \nagainst random forest, SVM and k-nearest neighbors [56]. The paper found that from the 11 patient \nsamples analyzed, SVM produced the best results, although they did remark that a larger training \ndataset may lead to better performance in general. \nIn more recent years, however, CNNs have become the prevalent choice for the task of tissue/cell \nclassification. For instance, one paper used PCA for transfer learning for CNNs with kernel fusion \n[57] to complete the task of classification in MHSI. This publication made use of the Gabor kernel \n(which is implemented to obtain spatial features) and the CNN kernel to improvise conventional \nCNN execution for MHSI classification, with the proposed model showing improved performance. \nCNN has also been implemented to classify blood cell MHSI in a similar fashion [52] [58] [59], where \nincreased pixel size for the MHSI produced better results with respect to classification accuracy, thus \nfurther proving the potential for CNN in MHSI. The CNN model showed promising prospects while \nclassifying head and neck cancer [37], even for an animal model [60]. A proposed CNN model also \nproduced successful results for classification of oral cancer diagnosis [61], the performance of which \nwas verified by implementing the same dataset for SVM and DBN. The majority of the proposed \nCNN models discussed in this section all build upon the traditional architecture for CNN, which \nbetter suits the requirements for the medical diagnosis under examination.  \n \n12 \n \n \n \n \n \n3.2 Detection \nDetection techniques in general refer to object detection for a given input image. For MHSI, \ndetection techniques are habitually used for the purpose of detection of malignant cancerous samples, \nalthough they are also used for reconstruction of tissue samples. While ANN has been used to detect \nsuch cancerous samples [62], the majority of detection techniques implemented for MHSI use CNNs \nfor classification of the pixels of the image, which is then used to detect the malign cancer cells/tissues \n[63] [64]. From this, after the pixel-wide classification is obtained from CNNs, it can also be \nconsidered as object classification, which is then used for post-processing to detect the presence of \ncancer or other purposes for the given input sample. One similar study implemented a CNN-based \nmodel to reconstruct tissue surface using an endoscopic probe, which displayed potential for \npractical applications [65]. One study also applied FCN to investigate tissue surface samples using \nan endoscopic probe in a similar fashion [66].  \nIn recent years, CNN has been particularly useful for MHSI in head and neck squamous cell \ncarcinoma detection [67] [68], although AEN has also been applied for a similar scenario [69]. \nImplementation of CNN was also observed for aiding brain tumor resection surgeries in real-time \namongst the considered papers [70] and similarly towards determining skin cancer detection [71]. \nSuch publications heavily suggest that although CNN is finely suited for classification techniques, as \ndiscussed in prior subsection, it also possesses potential for detection techniques that support a \npromising outlook for future research of detection techniques in MHSI. \n \n \n \n \n \n \n \n \nC1\nM1\nC2\nM2\nInput \nImage Map \nConvolution \nLayer \nConvolution \nLayer \nPooling \nLayer \nPooling \nLayer \nFully Connected \nLayer \nSoftmax \nOutput\nBenign \nTissue \nCancerous \nTissue \n. . . \nFigure 7. Visual representation of classification of a cancerous tissue sample. \n13 \n \n \n \n \n3.3 Segmentation \nSegmentation provides outlines for certain parts of images that dictate the position, volume, and \nsize of the relative objects in an image, as discussed in Section 2 of this study. For medical image \ndiagnosis, this is particularly useful, as it could clearly outline certain organs, or noteworthy parts of \na medical image. This is significant for medical diagnosis in which brain, liver or other important \norgans need to be distinguished from a medical image. While segmentation has been extensively \nused for medical diagnosis over the years [72] [73], in the case of MHSI, we could only find one \nnotable paper that used a segmentation technique, specifically for the purpose of retinal image \nanalysis [74].  \nThis paper implemented a dense-FCN (FCN being the foundation for U-net [46]) to segment the \nretinal image. With the usage of k-means clustering on the input data, the study was able to lessen \nthe complexity in order to aid the segmentation process and ultimately complete validation against \nalternate approaches involving other ML algorithms such as SVM and random forest. Its findings \nsuggests that spectral data may provide the opportunity for improvised segmentation results for \noptic disc and macula segmentation, which is important for retinal imaging analysis, thereby \nprompting the urge for further research for segmentation techniques for MHSI. \n \n \n \nPre-\nprocessing \nPost-\nprocessing \nDeep Features Learning \nInput \nCube\nCancer \nDetection\nFigure 8. Standard workflow for cancer detection using deep learning on medical image analysis. \nPixel-wide Prediction \nSegmentation \nInput Image \n96 \n256 \n384 \n384 \n256 \n4096 \n4096 \n21 \n21 \nForward/inference \nBackward/learning \nFigure 9. Classical representation of an FCN for the purposes of segmentation. \n14 \n \n \n \n3.4 Other \nDL applications of MHSI go beyond the normal implementation of traditional domains of \nmachine learning. This can be observed in a handful of papers considered in this study. For example, \none study utilized GAN with the aid of an autoencoder using MATLAB ML tools in order to \ndetermine the tissue oxygen saturation using hyperspectral input data, the results of which are useful \nfor the detection of ischaemia, a fatal disease that affects the blood supply to different organs of the \nbody, particularly the heart muscles [75]. \nAnother similar case in which blood oxygenation is determined using MATLAB tools was \ndiscussed in a 2017 study [76]. In this circumstance, the NN fitting tool was used to retrieve the \nparameters necessary to determine the oxygen saturation levels. Lastly, in another 2017 study, GAN \nwas employed again to assist in the task of staining lung histology imaging, which was then used to \nfurther study the necessary tissue samples [77]. Overall, these studies suggest a broader field of \napplications of DL for MHSI in the near future. Tables 1 and 2 below consist of the relevant \npublications we obtained for this paper. Table 1 details the titles of the papers and the category of DL \nthe paper pertains to, while Table 2 lists the application area of the papers. \n \nTable 1. Papers using deep learning techniques for MHSI. \nPublication Title \nCategory \nCell classification using convolutional neural networks in medical \nhyperspectral imagery [52] \nclassification \nTowards virtual H&E staining of hyperspectral lung histology images using \nconditional generative adversarial networks [77] \nother \nHyperspectral image segmentation of retinal vasculature, optic disc and macula \n[74] \nsegmentation \nHyperspectral imaging for cancer detection and classification [54] \nclassification \nDual-modality endoscopic probe for tissue surface shape reconstruction and \nhyperspectral imaging enabled by deep neural networks [65] \ndetection \nProbe-based rapid hybrid hyperspectral and tissue surface imaging aided by \nfully convolutional networks [66] \ndetection \nHyperspectral system for imaging of skin chromophores and blood \noxygenation [76] \nother \nMedical hyperspectral imaging: a review [21] \nclassification \nDeep convolutional neural networks for classifying head and neck cancer using \nhyperspectral imaging [37] \nclassification \nDeep learning based classification for head and neck cancer detection with \nhyperspectral imaging in an animal model [60] \nclassification \nConvolutional neural network for medical hyperspectral image classification \nwith kernel fusion [57] \nclassification \nBlood cell classification based on hyperspectral imaging with modulated Gabor \nand CNN [58] \nclassification \nMedical hyperspectral image classification based on end-to-end fusion deep \nneural network [59] \nclassification \n15 \n \n \nTissue classification of oncologic esophageal resectates based on hyperspectral \ndata [56] \nclassification \nTable 1 (continued) \nPublication Title \nCategory \nA dual stream network for tumor detection in hyperspectral images [62] \ndetection \nAdaptive deep learning for head and neck cancer detection using hyperspectral \nimaging [69] \ndetection \nHyperspectral imaging of head and neck squamous cell carcinoma for cancer \nmargin detection in surgical specimens from 102 patients using deep learning \n[67] \ndetection \nComputer-assisted medical image classification for early diagnosis of oral \ncancer employing deep learning algorithm [61] \nclassification \nHyperspectral imaging for head and neck cancer detection: specular glare and \nvariance of the tumor margin in surgical specimens [68] \ndetection \nCancer detection using hyperspectral imaging and evaluation of the superficial \ntumor margin variance with depth [64] \ndetection \nSurgical aid visualization system for glioblastoma tumor identification based \non deep learning and in-vivo hyperspectral images of human patients [70] \ndetection \nOptical biopsy of head and neck cancer using hyperspectral imaging and \nconvolutional neural networks [63] \ndetection \nConvolutional neural networks in skin cancer detection using spatial and \nspectral domain [71]  \ndetection \nEstimation of tissue oxygen saturation from RGB images and sparse \nhyperspectral signals based on conditional generative adversarial network [75] \nother \nDesign of a multilayer neural network for the classification of skin ulcers‚Äô \nhyperspectral images: a proof of concept [55] \nclassification \nHyperspectral imaging based method for fast characterization of kidney stone \ntypes [53] \nclassification \n \nTable 2. Deep learning methodologies used in MHSI and their applications. \nMethodology \nApplication \nCNN \nApplied CNN with comparison to SVM for blood cell discrimination [52] \nGAN \nVirtual staining for lung histology images using GANs [77] \nFCN \nSegmentation for the automatic retinal image analysis [74] \nANN \nImplementation of ANN and SVM for cancerous cell HSI [54] \nCNN \nCNN-based model used for endoscopic image reconstruction to enhance \nsurgical guidance [65] \nFCN \nSpot detection for surgical endoscopic tissue sample aided by FCN [66] \nNN \nMatlab NN toolkit used to characterize of skin using 2d mapping of skin \nchromophore distribution [76] \n16 \n \n \nANN \nThis review paper discusses the use of ANN for classification for MHSI [21] \nCNN \nClassifying cancerous tissue samples from neck and head regions using CNN \n[37] \n \n \nTable 2 (continued) \nMethodology \nApplication \nCNN \nDetection of neck and head cancerous cells via classification using CNN [60] \nImprovisation of CNN using kernel fusion implemented for cell classification \n[57] \nImplementation of CNN for blood cell classification [58] \nTwo-channel CNN for solving limited-samples problem for CNN models [59] \nMLP \nEvaluation of different supervised algorithms, including MLP, to analyze \ntissue samples of esophago-gastric resectates [56] \nNN \nNN with 2 hidden layers used as a classifier to combine structural and \nspectral data for tumor detection in tongue [62] \nAEN \nUse of AEN to detect cancer in a tissue sample [69] \nCNN \nUse of CNN to detect squamous cell carcinoma between samples from \ndifferent patients [67] \nCNN used for detection of oral cancer [61] \nUsing specular glare in MHSI along with CNN to detect squamous cell \ncarcinoma [68] \nAnother study for CNN to detect squamous cell carcinoma [64] \nDetection of brain tumor with the aid of CNN [70] \nDetecting carcinoma thyroid sample with the aid of CNN [63] \nDifferent CNN models compared to one another for classifying skin cancer \nfrom patient data HIS [71] \nGAN \nGAN-inspired model used to estimate the tissue oxygen saturation [75] \nANN \nClassifying cutaneous ulcer HIS using feedforward ANN [55] \nANN \nANN used to classify kidney stone tissue samples [53] \n4. Challenges in and Future of DL MHSI \nThe boom in deep learning research following the ILSVRC in 2012 has had substantial effects on \na variety of disciplines [35], particularly in medical image analysis and diagnosis, as evident from the \nsurge in papers published [78] [79]. While some domains in the medical community have already felt \nthe impact of the applications of deep learning techniques, particularly the domain of radiology, \nwhich has severely diminished effects for the profession of practicing radiologists [80], MHSI has yet \nto experience a similar fate. This may simply be due to lack of broader researchers of MHSI in general. \nHowever, examining the trend observed in the number of publications for deep learning \nimplemented in MHSI, there may be a similar outcome over the coming years. With the \nunprecedented advancement where general medical image analysis seemed to have reached an \nimpasse, as deep learning methodologies augment and improvise efficiency with more accurate \nresults [35], MHSI profession is also likely to also be impacted. Ultimately, further research for MHSI \n17 \n \n \nwould provide the essential requirement of large available datasets, which was also an obstacle for \ndeep learning as a whole prior to the last couple of decades.    \nWith such deep rooted research in deep learning techniques, it is clearly evident that there is not \none technique that trumps all others, as the needs and requirements vary from one situation to \nanother. However, CNNs appear to be the prevalent choice, as the bulk of publications considered in \nthis paper utilize it in some way or another. Meanwhile, there are still variants of architectures of \nCNNs implemented for a particular circumstance, such as in [37] and [68]. Although both investigate \nthe same subject of neck and head cancer, they each use different approaches to tackle the challenge \nusing CNNs. Issues for deep learning for MHSI also stem from several existing DL woes, one of which \nrelates to the broader problems for classification and detection, particularly imbalance of \nclassification for the purpose of object detection. The detection systems first perform a pixel-wide \nclassification, as discussed in the earlier section, which typically causes the class imbalance to be \nbiased towards non-object classes during the training process, which are easier to discern amongst \nthe samples and may cause distortion in the overall detection process. \n5. Conclusions \nDespite the boom in deep learning, its rigorous applications in medical image analysis, and the \nbenefit of hyperspectral imaging for medical analysis, there have not yet been any papers published \nthat review the publications dedicated towards the implementation of deep learning for medical \nhyperspectral image analysis. In this paper, we discussed the deep learning techniques for medical \nhyperspectral imaging and relevant papers published relative to the topic within the year range of \n2012-2019. In short, similar to the trend observed for deep learning since the iconic ILSVRC 2012, \nthere was a definite boost in research and papers published for deep learning for MHSI and medical \nimage analysis in general. We also discussed the different methodologies discussed in all the papers \nwe found as well as what the future in DL for MHSI may look like. The trend for the majority of \npapers implementing DL for MHSI seems to be paved by the use of CNNs to classify the blood \ncells/tissue sample in order to aid cancer detection and analysis. We could only find one paper that \ndiscussed the use of FCN for segmentation rationales of retinal imaging. Lastly, there were also \nisolated uses of GAN and Matlab tools to determine tissue oxygenation levels that could potentially \nbe used for detecting ischaemia (disease directly dependent on blood supply to vital organs) or even \nin staining of lung histology imaging. \nConflicts of Interest: The authors declare no potential conflict of interest and have no relevant financial motives \nfor this article. \nAppendix A (Search Criteria) \nGoogle Scholar was implemented to find the relative publications considered for this paper. The \nkey search words included during our search are ‚Äúdeep learning‚Äù and ‚Äúmedical hyperspectral \nimaging‚Äù; we found that word searches for topics such as specific techniques yielded more inaccurate \nresults relative to our purpose. In addition to Google Scholar, we also searched for the same search \nwords on the publisher‚Äôs own search engines for the papers considered for this article, which resulted \nin the discovery of papers that previously did not appear in google scholar. \n \nAppendix B (Datasets) \nThe majority of datasets utilized by the publications were obtained either directly through \nexperimentation using various imaging systems or via contribution from hospitals, laboratories, or \nclinical storage. Some examples of testing systems implemented include CRI Maestro for vivo \nimaging system [69], more commonly used Liquid Crystal Tunable Filters (LCTF) [58] [57] [75] [59], \nand Hybrid endoscopic apparatus (ICL SLHSI) [65]. Several papers also used actual medical samples, \nsuch as patients undergoing surgical cancer resection [64] [37] and other contributions of local patient \n18 \n \n \nsamples from hospitals, medical centers, and laboratories. The only largely available repository in \nuse by a paper was BioGPS UCI repository [61]. \n \n \nAppendix C (Software) \nVarious software packages were implemented across all the publications discussed in this paper, \nthe majority of which implemented Tensorflow (wherever stated). Tensorflow is an open-source \nmachine learning platform based on Python, which is supported by its vast number of libraries, tools, \nand community resources. The complete list of software and the papers utilizing those software can \nbe found below- \n \nSoftware \nUtilizing Publications \nStatistica \n[53] \nUnscrambler (for PCA/ ANN) \n[53] \nMatlab (data processing/implementing NNs) \n[58] [52] [57] [53] [54] [76] \nTensorflow (Python) \n[58] [64] [57] [37] [65] [68] [67] [59] [63] [70] \nKeras (Python) \n[71] [59] \nScikit (Python) \n[56] \nCaffe \n[52] \nReferences \n \n[1]  I. G. Stiell, G. A. Wells, K. L. Vandemheen, C. M. Clement, H. . Lesiuk, A. . Laupacis, \nR. D. McKnight, R. . Verbeek, R. J. Brison, D. . Cass, M. A. Eisenhauer, G. H. \nGreenberg and J. . Worthington, \"The Canadian CT Head Rule for patients with minor \nhead injury,\" The Lancet, vol. 357, no. 9266, pp. 1391-1396, 2001.  \nFigure 10. Sample dataset from BioGPS repository: collection of genome signatures of ovarian tissue that can be \nutilized to identify ovarian cancer associated fibroblasts (CAFs) gene signatures [82]. \n19 \n \n \n[2]  S. Procza, C. Avilab, J. Feya, G. Roqueb, M. Schuetza and E. Hamannc, \"X-ray and \ngamma imaging with Medipix and Timepix detectors in medical research,\" Radiation \nMeasurements, vol. 106104, no. 127, 2019.  \n[3]  K. Doi, \"Computer-aided diagnosis in medical imaging: historical review, current \nstatus and future potential,\" Computerized Medical Imaging and Graphics, no. 31, p. \n198‚Äì211, 2007.  \n[4]  F. E.-Z. A. El-Gamal, M. Elmogy and A. Atwan, \"Current trends in medical image \nregistration and fusion,\" Egyptian Informatics Journal, pp. 99-124, 2016.  \n[5]  A. . Stylianou and M. A. Talias, \"Nanotechnology-supported THz medical imaging,\" \nF1000Research, vol. 2, no. , pp. 100-100, 2013.  \n[6]  M. A. Calin, S. V. Parasca, D. . Savastru and D. . Manea, \"Hyperspectral Imaging in \nthe Medical Field: Present and Future,\" Applied Spectroscopy Reviews, vol. 49, no. 6, \npp. 435-447, 2014.  \n[7]  S. Paheding, V. K. Ansari and V. Sagan, \"Progressively Expanded Neural Network \n(PEN Net) for hyperspectral image classification: A new neural network paradigm for \nremote sensing image analysis,\" ISPRS Journal of Photogrammetry and Remote \nSensing, vol. 146, pp. 161-181, 2018.  \n[8]  J. Ren, J. Zabalza, S. Marshall and J. Zheng, \"Effective Feature Extraction and Data \nReduction in Remote Sensing using Hyperspectral Imaging,\" IEEE Signal Processing \nMagazine, vol. 31, no. 4, pp. 149-154, 2014.  \n[9]  D. Lorente, N. Aleixos, J. Gomez-Sanchis, S. Cubero, O. L. Garcia-Navarrete and J. \nBlasco, \"Recent Advances and Applications of Hyperspectral Imaging for Fruit and \nVegetable Quality Assessment,\" Food and Bioprocess Technology, vol. 5, no. 4, pp. \n1121-1142, 2012.  \n[10] D.-W. S. Yao-Ze Feng, \"Application of Hyperspetral Imaging in Food Safety \nInspection and Control: A Review,\" Critical Reviews in Food Science and Nutrition, \nvol. 52, no. 11, pp. 1039-1058, 2012.  \n[11] D. Wu and D.-W. Sun, \"Advanced applications of hyperspectral imaging technology \nfor food quality and safety analysis and assessment: A review - Part I: Fundamentals,\" \nInnovation Food Science & Emerging Technologies, vol. 19, pp. 1-14, 2013.  \n[12] S. Paheding, Y. Diskin, S. Arigela and V. K. Asari, \"Visibility improvement of shadow \nregions using hyperspectral band integration,\" in Proc. SPIE 9244, Image and Signal \nProcessing for Remote Sensing XX, Amsterdam, 2014.  \n[13] M. S. Alam, R. P. Gollapalli and S. Paheding, \"Identification and detection of oil and \noil-derived substances at the surface and subsurface levels via hyperspectral imaging,\" \nin Optical Pattern Recognition XXIII. Vol. 8398. International Society for Optics and \nPhotonics, Baltimore, 2012.  \n20 \n \n \n[14] M. S. Alam and S. Paheding, \"Trends in Oil Spill Detection via Hyperspectral \nImaging,\" in 2012 7th International Conferrence on Electrical and Computer \nEngineering, Dhaka, 2012.  \n[15] A. Essa, P. Sidike and V. Asari, \"Volumetric Directional Pattern for Spatial Feature \nExtraction in Hyperspectral Imagery,\" IEEE GEOSCIENCE AND REMOTE SENSING \nLETTERS, vol. 14, no. 7, pp. 1056-1060, 2017.  \n[16] S. Paheding, C. Chen, V. Asari, Y. Xu and W. Li, \"Classification of hyperspectral \nimage using multiscale spatial texture features,\" in IEEE 8th Workshop on \nHyperspectral Image and Signal Processing: Evolution in Remote Sensing, 2016.  \n[17] S. Paheding, V. K. Asari and M. S. Alam, \"Multiclass Object Detection With Single \nQuery in Hyperspectral Imagery Using Class-Associative Spectral Fringe-Adjusted \nJoint Transform Correlation,\" IEEE Transactions on Geoscience and Remote Sensing, \nvol. 54, no. 2, pp. 1196-1208, 2016.  \n[18] D. Manolakis and G. Shaw, \"Detection Algorithms for Hyperspectral Imaging \nApplications,\" IEEE Signal Processing Magazine, vol. 19, no. 1, pp. 29-43, 2002.  \n[19] S. Paheding, V. K. Asari and M. S. Alam, \"Multiple object detection in hyperspectral \nimagery using spectral fringe-adjusted joint transform correlator,\" in Proc. SPIE 9405, \nImage Processing: Machine Vision, San Francisco, 2015.  \n[20] C. Fischer and I. Kakoulli, \"Multispectral and hyperspectral imaging technologies in \nconservation: current research and potential applications,\" Studies in Conservation, vol. \n51, pp. 3-16, 2006.  \n[21] G. Lu and B. Fei, \"Medical hyperspectral imaging: a review,\" Journal of Biomedical \nOptics, vol. 010901, no. 19, 2014.  \n[22] S. V. Panasyuk, S. Yang, D. V. Faller, D. Ngo, R. A. Lew, J. E. Freeman and A. E. \nRogers, \"Medical Hyperspectral Imaging to Facilitate Residual Tumor Identification \nDuring Surgery,\" Cancer Bioloy & Therapy, vol. 6, no. 3, pp. 439-446, 2007.  \n[23] G. Lu, J. V. Little, X. Wang, H. Zhang, M. R. Patel, C. C. Griffith, M. W. El-Deiry, A. \nY. Chen and B. Fei, \"Detection of Head and Neck Cancer in Surgical Specimens Using \nQuantitative Hyperspectral Imaging,\" Clinical Cancer Research, vol. 23, no. 18, pp. \n5426-5436, 2017.  \n[24] S. Kiyotoki, J. Nishikawa, T. Okamoto, K. Hamabe, M. Saito, A. Goto, Y. Fujita, Y. \nHamamoto, Y. Takeuchi, S. Satori and I. Sakaidaa, \"New method for detection of \ngastric cancer by hyperspectral imaging: a pilot study,\" Journal of Biomedical Optics, \nvol. 18, no. 2, 2013.  \n[25] G. Lu, J. V. Little, X. Wang, H. Zhang, M. R. Patel, C. C. Griffith, M. W. El-Deiry, A. \nY. Chen and B. Fei, \"Detection of Head and Neck Cancer in Surgical Specimens Using \nQuantitative Hyperspectral Imaging,\" Clinical Cancer Research, vol. 23, no. 18, pp. \n5426-5436, 2017.  \n21 \n \n \n[26] R. Pike, S. K. Patton, G. Lu, L. V. Halig, D. Wang, Z. G. Chen and B. Fei, \"A minimum \nspanning forest based hyperspectral image classification method for cancerous tissue \ndetection,\" SPIE Medical Imaging, vol. 9034, pp. 15-20, 2014.  \n[27] Y. LeCun, Y. Bengio and G. Hinton, \"Deep learning,\" Nature, vol. 521, p. 436‚Äì444, \n2015.  \n[28] G. Lu, X. Qin, D. Wang, S. Muller, H. Zhang, A. Chen, Z. G. Chen and B. Fei, \n\"Hyperspectral imaging of neoplastic progression in a mouse model of oral \ncarcinogenesis,\" in Proc. SPIE, Medical Imaging 2016: Biomedical Applications in \nMolecular, Structural, and Functional Imaging, San Diego, 2016.  \n[29] A. Madooei, R. M. Abdlaty, L. Doerwald-Munoz, J. Hayward, M. S. Drew, Q. Fang \nand J. Zerubia, \"Hyperspectral image processing for detection and grading of skin \nerythema,\" Proc. SPIE, Medical Imaging 2017: Image Processing, vol. 10133, pp. 1-\n7, 2017.  \n[30] H. Akbari, Y. Kosugi, K. Kojima and N. Tanaka, \"Blood Vessel Detection and Artery-\nVein Differentiation Using Hyperspectral Imaging,\" in 31st Annual International \nConference of the IEEE EMBS, Minneapolis, 2009.  \n[31] Z. Liu, D. Z. J.-q. Yan, Q.-L. Li and Q.-l. Tang, \"Classification of hyperspectral medical \ntongue images for tongue diagnosis,\" Computerized Medical Imaging and Graphics, \nvol. 31, pp. 672-678, 2007.  \n[32] R. Archibald and G. Fann, \"Feature Selection and Classification of Hyperspectral \nImages With Support Vector Machines,\" IEEE Geosience and Remote Sensing Letters, \nvol. 4, no. 4, pp. 674-677, 2007.  \n[33] S. Vyas, A. Banerjee, L. Garza, S. Kang and P. Burlina, \"Hyperspectral signature \nanalysis of skin parameters,\" in Proc. SPIE, Medical Imaging 2013: Computer-Aided \nDiagnosis, Lake Buena Vista, 2013.  \n[34] M. Z. Alom, T. M. Taha, C. Yakopcic, S. Westberg, S. Paheding, M. S. Nasrin, B. C. \nV. Essen, A. A. S. Awwal and V. K. Asari, \"The History Began from AlexNet: A \nComprehensive Survey on Deep Learning Approaches,\" 2018.  \n[35] M. Z. Alom, T. M. Taha, C. Yakopcic, S. Westberg, P. Sidike, M. S. Nasrin, M. Hasan, \nB. C. V. Essen, A. A. S. Awwal and V. K. Asari, \"A State-of-the-Art Survey on Deep \nLearning Theory and Architectures,\" Electronics, vol. 8, no. 3, p. 292, 2019.  \n[36] M. G. Crowson, J. Ranisau, A. Eskander, A. Babier, B. Xu, R. R. Kahmke, J. M. Chen \nand T. C. Y. Chan, \"A Contemporary Review of Machine Learning in Otolaryngology‚Äì\nHead and Neck Surgery,\" The Laryngoscope, vol. 0, pp. 1-7, 2019.  \n[37] M. Halicek, G. Lu, J. V. Little, X. Wang, M. Patel, C. C. Griffith, M. W. El-Deiry, A. \nY. Chen and B. Fei, \"Deep convolutional neural networks for classifying head and neck \ncancer using hyperspectral imaging,\" Journal of Biomedical optics, vol. 22, no. 6, 2017. \n22 \n \n \n[38] A. Krizhevsky, I. Sutskever and G. E. Hinton, \"Imagenet classification with deep \nconvolutional neural networks,\" in Advances in Neural Information Processing \nSystems, 2012.  \n[39] L. Brunese, F. Mercaldo, A. Reginelli and A. Santone, \"Explainable Deep Learning for \nPulmonary Disease and Coronavirus COVID-19 Detection from X-rays,\" Computer \nMethods and Programs in Biomedicine, vol. 196, 2020.  \n[40] S. Minaee, R. Kafieh, M. Sonka, S. Yazdani and G. J. Soufi, \"Deep-COVID: Predicting \nCOVID-19 From Chest X-Ray Images Using Deep Transfer Learning,\" Medical Image \nAnalysis, vol. 65, 2020.  \n[41] A. I. Khan, J. L. Shah and M. M. Bhat, \"CoroNet: A deep neural network for detection \nand diagnosis of COVID-19 from chest x-ray images,\" Computer Methods and \nPrograms in Biomedicine, vol. 196, 2020.  \n[42] H. Wang and B. Raj, \"On the Origin of Deep Learning,\" Carnegie Mellon University, \n2017.  \n[43] Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, \"Gradient-based learning applied to \ndocument recognition,\" Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998. \n[44] C. Szegedy, W. L. Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke \nand A. Rabinovich, \"Going deeper with convolutions,\" in 2015 IEEE Conference on \nComputer Vision and Pattern Recognition (CVPR), Boston, 2015.  \n[45] K. He, X. Zhang, S. Ren and J. Sun, \"Deep Residual Learning for Image Recognition,\" \nin 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las \nVegas, 2016.  \n[46] O. Ronneberger, P. Fischer and T. Brox, \"U-Net: Convolutional Networks for \nBiomedical Image Segmentation,\" in Medical Image Computing and Computer-\nAssisted Intervention -- MICCAI 2015, Cham, Springer International Publishing, 2015, \npp. 234--241. \n[47] W. Yin, K. Kann, M. Yu and H. Schutze, \"Comparative Study of CNN and RNN for \nNatural Language Processing,\" 7 February 2017.  \n[48] M. Z. Alom, V. Bontupalli and T. M. Taha, \"Intrusion detection using deep belief \nnetworks,\" in National Aerospace and Electronics Conference (NAECON), Dayton, \nOH, 2015.  \n[49] R. Sarikaya, G. E. Hinton and A. Deoras, \"Application of Deep Belief Networks for \nNatural Language Understanding,\" IEEE/ACM Transactions on Audio, Speech, and \nLanguage Processing, pp. 778-784, 19 April 2014.  \n[50] A. Khatami, A. Khosravi, T. Nguyen, C. P. Lim and S. Nahavandi, \"Medical image \nanalysis using wavelet transform and deep belief networks,\" Expert Systems with \nApplications, vol. 86, pp. 190-198, 15 November 2017.  \n23 \n \n \n[51] S. A. Jalalifar, H. Hasani and H. Aghajan, \"Speech-Driven Facial Reenactment Using \nConditional Generative Adversarial Networks,\" 20 March 2018.  \n[52] X. Li, W. Li, X. Xu and W. Hu, \"Cell classification using convolutional neural networks \nin medical hyperspectral imagery,\" in 2017 2nd International Conference on Image, \nVision and Computing (ICIVC), Chengdu, 2017.  \n[53] F. Blanco, M. L√≥pez-Mesas, S. Serranti, G. Bonifazi, J. Havel and M. Valiente, \n\"Hyperspectral imaging based method for fast characterization of kidney stone types,\" \nJournal of Biomedical Optics, vol. 17, no. 7, p. 076027, 2012.  \n[54] M. Nathan, A. Kabatznik and A. Mahmood, \"Hyperspectral imaging for cancer \ndetection and classification,\" in 2018 3rd Biennial South African Biomedical \nEngineering Conference (SAIBMEC), Stellenbosch, 2018.  \n[55] D. C. Jaramillo, J. E. Escobar, J. Galeano and M. C. Torres-Madronero, \"Design of a \nmultilayer neural network for the classification of skin ulcers‚Äô hyperspectral images: a \nproof of concept,\" in 15th International Symposium on Medical Information Processing \nand Analysis, Medelin, 2019.  \n[56] M. Maktabi, H. K√∂hler, M. Ivanova, B. Jansen-Winkeln, J. Takoh, S. Niebisch, S. M. \nRabe, T. Neumuth, I. Gockel and C. Chalopin, \"Tissue classification of oncologic \nesophageal resectates based on hyperspectral data,\" International Journal of Computer \nAssisted Radiology and Surgery, vol. 14, no. 10, p. 1651‚Äì1661, 2019.  \n[57] Q. Huang, W. Li and X. Xie, \"Convolutional neural network for medical hyperspectral \nimage classification with kernel fusion,\" in International Conference on Biological \nInformation and Biomedical Engineering, Shanghai, 2018.  \n[58] Q. Huang, W. Li, B. Zhang, Q. Li, R. Tao and N. H. Lovell, \"Blood Cell Classification \nBased on Hyperspectral Imaging With Modulated Gabor and CNN,\" IEEE Journal of \nBiomedical and Health Informatics, vol. 24, no. 1, pp. 160 - 170, 2019.  \n[59] X. Wei, W. Li, M. Zhang and Q. Li, \"Medical Hyperspectral Image Classification \nBased on End-to-End Fusion Deep Neural Network,\" IEEE Transactions on \nInstrumentation and Measurement, vol. 68, no. 11, pp. 4481 - 4492, 2019.  \n[60] L. Ma, G. Lu, D. Wang, X. Wang, Z. G. Chen, S. Muller, A. Chen and B. Fei, \"Deep \nlearning based classification for head and neck cancer detection with hyperspectral \nimaging in an animal model,\" in SPIE Medical Imaging: Biomedical Applications in \nMolecular, Structural, and Functional Imaging, Orlando, 2017.  \n[61] P. R. Jeyaraj and E. R. S. Nadar, \"Computer-assisted medical image classification for \nearly diagnosis of oral cancer employing deep learning algorithm,\" Journal of Cancer \nResearch and Clinical Oncology, vol. 145, p. 829‚Äì837, 2019.  \n[62] P. Weijtmans, C. Shan, T. Tan, S. B. d. Koning and T. Ruers, \"A Dual Stream Network \nfor Tumor Detection in Hyperspectral Images,\" in 2019 IEEE 16th International \nSymposium on Biomedical Imaging, Venice, 2019.  \n24 \n \n \n[63] M. Halicek, J. V. Little, X. Wang, A. Y. Chen and B. Fei, \"Optical biopsy of head and \nneck cancer using hyperspectral imaging and convolutional neural networks,\" Journal \nof Biomedical Optics, vol. 24, no. 3, p. 036007, 2019.  \n[64] M. Halicek, H. Fabelo, S. Ortega, J. V. Little, X. Wang, A. Y. Chen, G. M. Callico, L. \nL. Myers, B. D. Sumer and B. Fei, \"Cancer detection using hyperspectral imaging and \nevaluation of the superficial tumor margin variance with depth,\" in SPIE Medical \nImaging, San Diego, 2019.  \n[65] J. Lin, N. T. Clancy, Y. H. Ji Qi, T. Tatla, D. Stoyanov, L. Maier-Hein and D. S. Elson, \n\"Dual-modality endoscopic probe for tissue surface shape reconstruction and \nhyperspectral imaging enabled by deep neural networks,\" Medical Image Analysis, vol. \n48, pp. 162-176, 2018.  \n[66] J. Lin, N. T. Clancy, X. Sun, J. Qi, M. Janatka, D. Stoyanov and D. S. Elson, \"Probe-\nBased Rapid Hybrid Hyperspectral and Tissue Surface Imaging Aided by Fully \nConvolutional Networks,\" in Medical Image Computing and Computer-Assisted \nIntervention - MICCAI 2016, 2016.  \n[67] M. Halicek, J. D. Dormer, J. V. Little, A. Y. Chen, L. Myers, B. D. Sumer and B. Fei, \n\"Hyperspectral Imaging of Head and Neck Squamous Cell Carcinoma for Cancer \nMargin Detection in Surgical Specimens from 102 Patients Using Deep Learning,\" \nCancers, vol. 11, no. 9, p. 1367, 2019.  \n[68] M. Halicek, H. Fabelo, S. Ortega, J. V. Little, X. Wang, A. Y. Chen, G. M. Callico, L. \nMyers, B. D. Sumer and B. Fei, \"Hyperspectral imaging for head and neck cancer \ndetection: specular glare and variance of the tumor margin in surgical specimens,\" \nJournal of Medical Imaging, vol. 6, no. 3, p. 035004, 2019.  \n[69] L. Ma, G. Lu, D. Wang, X. Qin, Z. G. Chen and B. Fei, \"Adaptive deep learning for \nhead and neck cancer detection using hyperspectral imaging,\" Visual Computing for \nIndustry, Biomedicine, and Art, vol. 2, no. 18, 2019.  \n[70] H. Fabelo, M. Halicek, S. Ortega, A. Szolna, J. Morera, R. Sarmiento, G. M. Callico \nand B. Fei, \"Surgical aid visualization system for glioblastoma tumor identification \nbased on deep learning and in-vivo hyperspectral images of human patients,\" in \nProceedings Volume 10951, Medical Imaging 2019: Image-Guided Procedures, \nRobotic Interventions, and Modeling, San Diego, 2019.  \n[71] I. P√∂l√∂nen, S. Rahkonen, L. Annala and N. N. , \"Convolutional neural networks in skin \ncancer detection using spatial and spectral domain,\" in Proceedings Volume 10851, \nPhotonics in Dermatology and Plastic Surgery 2019, San Francisco, 2019.  \n[72] L. Bi, D. Feng and J. Kim, \"Dual-Path Adversarial Learning for Fully Convolutional \nNetwork (FCN)-Based Medical Image Segmentation,\" The Visual Computer, vol. 32, \np. 1043‚Äì1052, 2018.  \n25 \n \n \n[73] A. Jesson and T. Arbel, \"Coronary Arteries Segmentation Based on 3D FCN With \nAttention Gate and Level Set Function,\" IEEE Access, pp. 42826 - 42835, 28 March \n2019.  \n[74] A. Garifullin, P. Koobi, P. Ylitepsa, K. Adjers, M. Hauta-Kasari, H. Uusitalo and L. \nLensu, \"Hyperspectral Image Segmentation of Retinal Vasculature, Optic Disc and \nMacula,\" in Digital Image Computing: Techniques and Applications (DICTA), \nCanberra, 2018.  \n[75] Q. Li, J. Lin, N. T. Clancy and D. S. Elson, \"Estimation of tissue oxygen saturation \nfrom RGB images and sparse hyperspectral signals based on conditional generative \nadversarial network,\" InternationalJournalofComputerAssistedRadiologyandSurgery, \nvol. 14, p. 987‚Äì995, 2019.  \n[76] E. Zherebtsov, A. Popov, A. Doronin, I. Meglinski and A. Bykov, \"Hyperspectral \nsystem for Imaging of skin chromophores and blood oxygenation,\" in European \nConferences on Biomedical Optics, Munich, 2017.  \n[77] N. Bayramoglu, M. Kaakinen, L. Eklund and J. Heikkila, \"Towards Virtual H&E \nStaining of Hyperspectral Lung Histology Images Using Conditional Generative \nAdversarial Networks,\" in IEEE International Conference on Computer Vision (ICCV), \nVenice, 2017.  \n[78] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. v. \nd. Laak, B. v. Ginneken and C. I. Sanchez, \"A survey on deep learning in medical image \nanalysis,\" Medical Image Analysis, vol. 42, pp. 60-88, 2017.  \n[79] N. Siddique, S. Paheding, C. Elkin and V. Devabhaktuni, \"U-Net and its variants for \nmedical image segmentation: theory and applications,\" arXiv:2011.01118, 2020.  \n[80] C. Liew, \"The future of radiology augmented with Artificial Intelligence: A strategy \nfor success,\" European Journal of Radiology, vol. 102, pp. 152-156, 2018.  \n[81] D. Manolakis and G. Shaw, \"Detection Algorithms for Hyperspectral Imaging \nApplications,\" IEEE Signal Processing Magazine, vol. 19, no. 1, pp. 29-43, 2002.  \n[82] T.-L. Yeung, K.-K. Wong, M. J. Birrer and S. C. Mok, \"Dataset: A cancer associated \nfibroblasts (CAFs) specific gene signature in high grade serous ovarian cancer,\" 19 \nSeptember 2014. [Online]. Available: http://biogps.org/dataset/E-GEOD-40595/a-\ncancer-associated-fibroblasts-cafs-specific-gene/. \n \n \n",
  "categories": [
    "eess.IV",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2020-11-27",
  "updated": "2020-11-27"
}