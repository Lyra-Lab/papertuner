{
  "id": "http://arxiv.org/abs/2011.14597v1",
  "title": "A Survey on Deep Learning for Software Engineering",
  "authors": [
    "Yanming Yang",
    "Xin Xia",
    "David Lo",
    "John Grundy"
  ],
  "abstract": "In 2006, Geoffrey Hinton proposed the concept of training ''Deep Neural\nNetworks (DNNs)'' and an improved model training method to break the bottleneck\nof neural network development. More recently, the introduction of AlphaGo in\n2016 demonstrated the powerful learning ability of deep learning and its\nenormous potential. Deep learning has been increasingly used to develop\nstate-of-the-art software engineering (SE) research tools due to its ability to\nboost performance for various SE tasks. There are many factors, e.g., deep\nlearning model selection, internal structure differences, and model\noptimization techniques, that may have an impact on the performance of DNNs\napplied in SE. Few works to date focus on summarizing, classifying, and\nanalyzing the application of deep learning techniques in SE. To fill this gap,\nwe performed a survey to analyse the relevant studies published since 2006. We\nfirst provide an example to illustrate how deep learning techniques are used in\nSE. We then summarize and classify different deep learning techniques used in\nSE. We analyzed key optimization technologies used in these deep learning\nmodels, and finally describe a range of key research topics using DNNs in SE.\nBased on our findings, we present a set of current challenges remaining to be\ninvestigated and outline a proposed research road map highlighting key\nopportunities for future work.",
  "text": "1\nA Survey on Deep Learning for Software Engineering\nYANMING YANG, Faculty of Information Technology, Monash University, Australia\nXIN XIA, Faculty of Information Technology, Monash University, Australia\nDAVID LO, School of Information Systems, Singapore Management University, Singapore\nJOHN GRUNDY, Faculty of Information Technology, Monash University, Australia\nIn 2006, Geoffrey Hinton proposed the concept of training “Deep Neural Networks (DNNs)” and an improved model training\nmethod to break the bottleneck of neural network development. More recently, the introduction of AlphaGo in 2016 demonstrated\nthe powerful learning ability of deep learning and its enormous potential. Deep learning has been increasingly used to develop\nstate-of-the-art software engineering (SE) research tools due to its ability to boost performance for various SE tasks. There\nare many factors, e.g., deep learning model selection, internal structure differences, and model optimization techniques, that\nmay have an impact on the performance of DNNs applied in SE. Few works to date focus on summarizing, classifying, and\nanalyzing the application of deep learning techniques in SE. To fill this gap, we performed a survey to analyse the relevant\nstudies published since 2006. We first provide an example to illustrate how deep learning techniques are used in SE. We then\nsummarize and classify different deep learning techniques used in SE. We analyzed key optimization technologies used in these\ndeep learning models, and finally describe a range of key research topics using DNNs in SE. Based on our findings, we present\na set of current challenges remaining to be investigated and outline a proposed research road map highlighting key opportunities\nfor future work.\nAdditional Key Words and Phrases: Deep learning, neural network, machine learning, software engineering, survey\nACM Reference Format:\nYanming Yang, Xin Xia, David Lo, and John Grundy. 2020. A Survey on Deep Learning for Software Engineering. ACM\nComput. Surv. 1, 1, Article 1 (January 2020), 35 pages. https://doi.org/10.1145/1122445.1122456\n1\nINTRODUCTION\nIn 1943, Warren Mcculloch and Walter Pitts first introduced the concept of the Artificial Neural Network (ANN)\nand proposed a mathematical model of an artificial neuron [84]. This pioneered a new era of research on artificial\nintelligence (AI). In 2006, Hinton et al. [40] proposed the concept of “Deep Learning (DL)”. They believed\nthat an ANN with multiple layers possessed extraordinary feature learning ability, which allows the feature data\nlearned to represent the essence of the original data. In 2009, they proposed Deep Belief Networks(DBN) and\nan unsupervised greedy layer-wise pre-training algorithm [87], showing great ability to solve complex problems.\nDL has since attracted attention of academics and industry practioners for many tasks. Development of Nvidia’s\ngraphics processing units (GPUs) significantly reduced the computation time of DL-based algorithms. DL has\nnow entered a period of great development. In 2012 Hinton’s research group participated in an image recognition\ncompetition for the first time and won the championship in a landslide victory by training a CNN model called\nAuthors’ addresses: Yanming Yang, Yanming.Yang@monash.edu, Faculty of Information Technology, Monash University, Melbourne, Australia;\nXin Xia, Faculty of Information Technology, Monash University, Melbourne, Australia, Xin.Xia@monash.edu; David Lo, School of Information\nSystems, Singapore Management University, Singapore, davidlo@smu.edu.sg; John Grundy, Faculty of Information Technology, Monash\nUniversity, Melbourne, Australia, John.Grundy@monash.edu.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for\ncomponents of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to\npost on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2020 Association for Computing Machinery.\n0360-0300/2020/1-ART1 $15.00\nhttps://doi.org/10.1145/1122445.1122456\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\narXiv:2011.14597v1  [cs.SE]  30 Nov 2020\n1:2\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nAlexNet on the ImageNet dataset. AlexNet outperformed the second best classifier (SVM) by a substantial margin.\nIn March 2016, AlphaGo was developed by DeepMind, a subsidiary of Google, which defeated the world champion\nof Go by a big score. With continuous improvements in DL’s network structures, training methods and hardware\ndevices, DL has been widely used to solve a wide variety of research problems in various fields.\nDriven by the success of DL techniques in image recognition and data mining, industrial practitioners and\nacademic researchers have shown great enthusiasm for exploring and applying DL algorithms in diverse software\nengineering (SE) tasks, including requirements, software design and modeling, software implementation, testing and\ndebugging, and maintenance. In requirements engineering, various DL algorithms have been employed to extract\nkey features for requirement analysis, and automatically identify actors and actions (i.e., user cases) in natural\nlanguage-based requirement descriptions [1, 100, 127]. In software design and modeling, DL has been leveraged\nfor design pattern detection [108], UI design search [14], and software design mining [81]. During software\nimplementation, researchers and developers have used DL for source code generation [26], source code modeling\n[49], software effort/cost estimation [7], etc. In software testing and debugging, various DL algorithms have been\ndesigned for detecting and fixing defects and bugs existed in software products, e.g., defect prediction [118], bug\nlocalization [63], vulnerability prediction [37]. It has been used for a varietu of software testing applications, such\nas test case generation [73], and automatic testing [144]. Researchers have applied DL to SE tasks to facilitate\nsoftware maintenance and evolution, such as code clone detection [90], feature envy detection [69], code change\nrecommendation [106], user review classification [28], etc.\nHowever, there is a lack of a comprehensive survey of deep learning usage to date in SE. This study performs a\ndetailed survey to review, summarize, classify, and analyze relevant papers in the field of SE that apply DL models.\nWe collected, reviewed, and analyzed 142 papers published in 20 major SE conferences and journals since “deep\nlearning” was introduced in 2006. We then analyzed the development trends of DL in SE, classified various DL\ntechniques used in diverse SE tasks, analyzed DL’s construction process, and summarized the research topics\ntackled by relevant papers. This study makes the following contributions:\n(1) We conducted a detailed analysis on 142 relevant studies that used DL techniques in terms of publication\ntrend, distribution of publication venues, and types of contributions. We analyzed an example in detail to\ndescribe the basic framework and the usage of DL techniques in SE.\n(2) We provide a classification of DL models used in SE based on their architectures and an analysis of DL\ntechnique selection strategy.\n(3) We performed a comprehensive analysis on the key factors that impact the performance of DL models in SE,\nincluding dataset, model optimization, and model evaluation.\n(4) We provide a description of each primary study according to six different SE activities and conducted an\nanalysis on these studies based on their task types. These include regression, classification, ranking, and\ngeneration tasks.\n(5) We discuss distinct technical challenges of using DL in software engineering and outline key future avenues\nfor research on using DL in software engineering.\nSection 2 introduces the workflow of a DL model through an example. Section 3 presents our Systematic\nLiterature Review methodology. Section 4 investigates the distribution and evolution of DL studies for SE tasks,\nand Section 5 gives an overall analysis on various DL techniques used in primary studies, including classifying\n30 DNN models based on their architectures and summarizing the model selection strategies adopted by studies.\nSection 6 analyzes a set of key techniques from four perspectives – datasets, model optimization, model evaluation,\nand the accessibility of source code. Section 7 lists research topics involved in primary studies and makes a briefly\ndescription of each work. Section 8 presents limitations of this study and its main threats to validity. Section\n9 discusses the challenges that still need to be solved in future work and outlines a clear research road-map of\nresearch opportunities. Section 10 concludes this paper.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:3\nFig. 1. The basic structure of a deep learning model.\n2\nDEEP LEARNING\n2.1\nBasic Structure of DL\nMost learning algorithms are shallow models with one or two non-linear feature representation layers, such as\nGMM, HMM, SVM, and MLP. The limitation of such a shallow model is the lack of ability to express complex\nfunctions. Their generalization ability is restricted for the complexity of problems, resulting in decreased learning\nability.\nDeep learning allows computational models composed of multiple layers to learn data representations with\nmultiple higher levels of abstraction [61]. This builds a neural network that simulates the human brain for analysis\nand learning. Similar to traditional ML, DL is suitable for various types of problems, such as regression, clustering,\nclassification, ranking, and generation problems. We present the basic structure of a DNN in Fig. 1.\nBased on the position and function of different layers, layers in a DNN can be classified into three categories, i.e.,\nthe input layer, the hidden layer, and the output layer. Generally, the first layer denotes the input layer, where the\npreprocessed data can be fed into DNNs; the last layer denotes the output layer, from which the results of a model\ncan be achieved, e.g., classification results, regression results, generation results, etc. The middle layers between\nthe input layer and the output layer are hidden layers. DNNs usually contain multiple hidden layers for enhancing\nthe expressive ability of DNNs and learning high-level feature representation. Besides, the way to connect between\ndifferent layers may vary, and as shown in Fig. 1, adjacent layers are full-connected (aka., full-connected layer),\nmeaning that any neuron in the 𝑖𝑡ℎlayer are connected to any neuron in the 𝑖+ 1𝑡ℎlayer. In some DNNs with\ncomplex structures, for tackling different SE issues, not only the fully connected layer can be used as a hidden\nlayer, but also a layer composed of other types of neurons can also be used as the hidden layer of a DNN, such as\nconvolution layer, pooling layer, LSTM layers, etc..\nCurrently, diverse DNNs and learning methods are used for SE tasks, such as Feedback Neural Network (FNN),\nConvolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), AutoEncoders, Generative Adversarial\nNetworks (GAN), and Deep Reinforcement Learning [31].\n2.2\nWorkflow when Using DL in SE\nWe present an example of using DL for a representative SE task. A novel sequence-to-sequence neural network\nis used to automatically perform comment updates with code changes [77]. Code comments are a vital source of\nsoftware documentation, helping developers to have a better understanding of source code and are beneficial to the\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:4\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\ncommunication between developers. However, developers sometimes neglect to update comments after changing\nsource code, leading to obsolete or inconsistent comments. It is necessary to perform Just-In-Time (JIT) code\ncomments updating, which aims to reduce and even avoid bad comments. To achieve this, a new approach called\nCUP builds a novel neural seq2seq model. Using DL techniques for such an SE task can be broken into six steps:\n(1) data collection, (2) data processing, (3) input construction, (4) model training, (5) model optimization, (6) model\ntesting, and (6) model application.\nData collection: Collecting data is a key step when building and training a DL model. Different SE tasks need to\nprocess and analyze different data types, such as requirements documents for requirements analysis, bug reports for\nbug report summarization, source code for code clone detection or code search, etc. In this example, for updating\nJIT code comments, the commonly available data are method-level code change datasets with comments. Each\nqualified instance contains old code snippets, new code snippets, old code comments, and new code comments.\nData processing: Processing raw data involves a number of steps, including data filtering, data splitting, data\ncleaning, data augmentation, and data segmentation for eliminating noise in data. For JIT code comments updating,\nsome instances are removed with unqualified comments, and no differences or empty between old and new\ncomments. Instances containing abstract methods are deleted to reduce method mismatching.\nInput construction: Since most DL models have strict requirements on the input-form, such as the input\nrequiring to be numeric and fixed size, it is necessary to transform SE data into multi-dimensional vectors in order\nto use DL models. In this JIT code comment updating example, code comments and code changes can be viewed\nas text-based data, processed by using a token-based method. Code changes and code comments are converted\ninto sequences so that they can be fed into their seq2seq model through data flattening and tokenization. After\ntokenization, old comments are converted into a token sequence. While to better represent code changes, each\nchange is aligned into two token sequences and construct a triple < 𝑡𝑖,𝑡\n′\n𝑖,𝑎𝑖> as an edit sequence to respectively\nrecord old source code, new source code, and an edit action, i.e., insert, delete, equal or replace.\nModel training: DL users need to select suitable DL techniques and different datasets, construct the structure of\na model, and decide model configuration, e.g., the number of layers and neural units of each layers. In our example,\na seq2seq DL model was built by training an encoder-decoder LSTM, since it is good at process nature language\ntext and token-based source code. In this model, the edit sequence of a code change and a token sequence of its old\ncomment were fed into the input layer. To capture the relationship between the code change and the old comment,\nthe encoder was composed of 4 layers: an embedding layer, a contextual embed layer, a co-attention layer, and a\nmodeling layer, where each layer had its role. The decoder included 2 layers: a LSTM layer and a dense layer. The\noutput of decoder was a new comment based on the corresponding captured relationship.\nModel optimization: After model design and construction, the designed model will be trained with the training\nset for achieving an effective DL model. Whether a model can work depends on thousands of parameters (aka.,\nweights), connecting neural units adjacent layers. Hence, model training is to fine-tune these weights to minimize\nthe loss of the model. For the seq2seq model in the example, the weights in the seq2seq neural network are trained\nby minimizing the difference between the real new comment and the generated new comment in a supervised way.\nModel testing: Generally, a training set is usually divided into two subsets of unequal sizes. The big subset is\nused for training the DL model, while the small one will be used for validating and testing the performance of the\nmodel when meeting new data. In this example, 20% of samples in the training set are put into the validation and\ntest set to ensure the effectiveness of CUP.\nModel application: Finally, the trained DL model can be applied to tackle practical SE tasks. In this example,\nthe trained model leverages two distinct encoders and a co-attention layer to learn the relationships between the\ncode change and the old comment. The LSTM-based decoder is used to generate new comments whose tokens are\ncopied from both the new code and the old comments.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:5\n3\nMETHODOLOGY\nWe performed a systematic literature review (SLR) following Kitchenham and Charters [52] and Petersen et al.\n[98]. In this section, we present details of our SLR methodology.\n3.1\nResearch Questions\nWe want to analyse the history of using DL models in SE by summarizing and analyzing the relevant studies, and\nproviding the guidelines on how to select and apply the DL techniques. To achieve this, we wanted to answer the\nfollowing research questions:\n(1) RQ1: What are the trends in the primary studies on the use of DL in SE?\n(2) RQ2: What DL techniques have been applied to support SE tasks?\n(3) RQ3: What key factors contribute to difficulties in training DNNs for SE tasks?\n(4) RQ4: What types of SE tasks and which SE phases have been facilitated by DL-based approaches?\nRQ1 analyzes the distribution of relevant publications that used DL in their studies since 2006 to give an overview\nof the trend of DL in SE. RQ2 provides a classification of different DL techniques supporting SE tasks and analyze\ntheir popularity based on their frequency of use in SE. RQ3 explores key technologies and factors that may affect\nthe efficiency of the DNN training phase. RQ4 investigates what types of SE tasks and which SE phases have been\nfacilitated by DNNs.\n3.2\nLiterature search and selection\nTo collect DL related papers in SE, we identified a search string including several DL related terms frequently\nappeared in SE papers that make use of DL. We then refined the search string by checking the title and abstract of a\nsmall number of relevant papers. After that, we used logical ORs to combine these terms, and the search string is:\n(\"deep\" OR \"neural\" OR \"Intelligence\" OR \"reinforcement\")\nWe specified the range the papers are published later: 2006-July 2020. Following previous studies [43, 47, 67],\nwe selected 22 widely read SE journals (10) and conferences (12) listed in Table 1 to conduct a comprehensive\nliterature review. We run the search string on three databases (i.e., ACM digital library 1, IEEE Explore 2, and Web\nof Science 3) looking for publications in the 22 publication venues whose meta data (including title, abstract and\nkeywords) satisfies the search string. Our search returns 655 relevant papers. After discarding duplicate papers, we\napplied some inclusion/exclusion criteria (presented in Section 3.3) by reading their title, abstract and keywords,\nand narrow the candidate set to 146 studies. After reading these 146 studies in full to ensure their relevance, we\nretained 142 studies.\n3.3\nInclusion and Exclusion Criteria\nAfter retrieving studies that match our search string, it is necessary to filter unqualified studies, such as studies with\ninsufficient contents or missing information. To achieve this, we applied our inclusion and exclusion criteria to\ndetermine the quality of candidate studies for ensuring that every study we kept implemented and evaluated a full\nDL approaches to tackle SE tasks.\nThe following inclusion and exclusion criteria are used:\n✔The paper must be written in English.\n✔The paper must adopt DL techniques to address SE problems.\n✔The length of paper must not be less than 6 pages.\n✘Books, keynote records, non-published manuscripts, and grey literature are dropped.\n1https://dl.acm.org\n2https://ieeexplore.ieee.org\n3http://apps.webofknowledge.com\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:6\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nTable 1. Publication venues for manual search\nNo.\nAcronym\nFull name\n1.\nICSE\nACM/IEEE International Conference on Software Engineering\n2.\nASE\nIEEE/ACM International Conference Automated Software Engineering\n3.\nESEC/FSE\nACM SIGSOFT Symposium on the Foundation of Software Engineering/European Soft-\nware Engineering Conference\n4.\nICSME\nIEEE International Conference on Software Maintenance and Evolution\n5.\nICPC\nIEEE International Conference on Program Comprehension\n6.\nESEM\nInternational Symposium on Empirical Software Engineering and Measurement\n7.\nRE\nIEEE International Conference on Requirements Engineering\n8.\nMSR\nIEEE Working Conference on Mining Software Repositories\n9.\nISSTA\nInternational Symposium on Testing and Analysis Working Conference on Mining Soft-\nware Repositories\n10.\nSANER\nIEEE International Conference on Software Analysis, Evolution and Reengineering\n11.\nICST\nIEEE International Conference on Software Testing, Verification and Validation\n12.\nISSRE\nIEEE International Symposium on Software Reliability Engineering\n13.\nTSE\nIEEE Transactions on Software Engineering\n14.\nTOSEM\nACM Transactions on Software Engineering and Methodology\n15.\nESE\nEmpirical Software Engineering\n16.\nJSS\nJournal of Systems and Software\n17.\nIST\nInformation and Software Systems\n18.\nASEJ\nAutomated Software Engineering\n19.\nIETS\nIET Software\n20.\nSTVR\nSoftware Testing, Verification and Reliability\n21.\nJSEP\nJournal of Software: Evolution and Process\n22.\nSQJ\nSoftware Quality Journal\n✘If a conference paper has an extended journal version, the conference version is excluded.\n3.4\nData Extraction and Collection\nAfter removing the irrelevant and duplicated papers, we extracted and recorded the essential data and performed\noverall analysis for answering our four RQs. Table 2 described the detailed information being extracted and\ncollected from 142 primary studies, where the column ′𝐸𝑥𝑡𝑟𝑎𝑐𝑡𝑒𝑑𝐷𝑎𝑡𝑎𝐼𝑡𝑒𝑚𝑠′ lists the related data items that would\nbe extracted from each primary study, and the column ‘𝑅𝑄′ denotes the related research questions to be answered\nby the extracted data items on the right. To avoid making mistakes in data collection, two researchers extracted\nthese data items from primary studies together and then another researcher double checked the results to make sure\nof the correctness of the extracted data.\n4\nRQ1: WHAT ARE THE TRENDS IN THE PRIMARY STUDIES ON USE OF DL IN SE?\nWe analyzed the basic information of primary studies to comprehend the trend of DL techniques used in SE in\nterms of the publication date, publication venues, and main contribution types of primary studies.\n4.1\nPublication trends of DL techniques for SE\nWe analyzed the publication trends of DL-based primary studies published between 2006 and the middle of 2020.\nAlthough the concept of “Deep Learning” has been proposed in 2006 and DL techniques had been widely used\nin many other fields in 2009, we did not find any studies using DL to address SE tasks before 2015. Fig. 2(a)\nshows the number of relevant studies published in predefined publication venues since the middle of 2020. It can\nbe observed that the number of publications from 2015 to 2019 shows a significant increase, with the number\nreaching 58 papers in 2019. In data collection, we only collect papers whose initial publication date is on July 2020\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:7\nTable 2. Data Collection for Research Questions\nRQs\nExtracted data items\nRQ1\nBasic information of each primary study (i.e., title, publication year, authors, publication venue)\nRQ1\nThe type of main contribution in each study (e.g., empirical study, case study, survey, or algorithm)\nRQ2\nDL techniques used in each study\nRQ2\nWhether and how the authors describe the rationale behind techniques selection\nRQ3\nDataset source (e.g., industry data, open source data, or collected data)\nRQ3\nData type (e.g., source code, nature language text, and pictures)\nRQ3\nThe process that datasets are transformed into input sets suitable for DNNs\nRQ3\nWhether and what optimization techniques are used\nRQ3\nWhat measures are used to evaluate the DL model\nRQ3\nPresence / absence of replication package\nRQ4\nThe practical problem that a SE task tries to solve\nRQ4\nThe SE activity in which each SE task belongs\nRQ4\nThe approach used for each SE task (e.g., regression, classification, ranking, and generation)\n2\n8\n13\n34\n58\n27\nNumber of Publications\n0\n20\n40\n60\nYear\n2015\n2016\n2017\n2018\n2019\n2020\n(a) Number of publications per year.\n2\n10\n23\n57\n115\n142\nNumber of Publications\n0\n50\n100\n150\nYear\n2015\n2016\n2017\n2018\n2019\n2020\n(b) Cumulative number of publications per year.\nFig. 2. Publication trends of DL-based primary studies in SE.\nor earlier; thus, the number of relevant studies in 2020 cannot reveal the overall trend of DL in 2020. However,\nextrapolating from the number of primary studies in previous years, we can estimate that there may be over 65\nrelevant publications using various DL techniques to solve SE issues by the end of 2020.\nWe also performed an analysis of the cumulative number of publications as shown in Fig. 2(b). We fit the\ncumulative number of publications as a power function, showing the publication trend in the last five years. We\ncan notice that the slope of the curve fitting the distribution increases substantially between 2015 and 2019, and\nthe coefficient of determination (𝑅2) attains the peak value (0.99447), which indicates that the number of relevant\nstudies using DL in SE intends to experience a strong rise in the future. Therefore, after analyzing Fig. 2, it can be\nforeseen that using DL techniques to address various SE tasks has become a prevalent trend since 2015, and huge\nnumbers of studies will adopt DL to address further challenges of SE.\n4.2\nDistribution of publication venues\nWe reviewed 142 studies published in various publication venues, including 12 conference proceedings and\nsymposiums as well as 10 journals, which covers most research areas in SE. Table 3 lists the number of relevant\npapers published in each publication venue. 69% of publications appeared in conferences and symposiums, while\nonly 31% of journal papers leveraged DL techniques for SE tasks. Among all conference papers, only 4 different\nconferences include over 10 studies using DL in SE in the last five years, i.e., SANER, ASE, ICSE, and MSR.\nCompared with other conference proceedings, SANER is the most popular one containing the highest number\nof primary study papers (22), followed by ASE (16). There are 13 and 6 relevant papers published in ICSE and\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:8\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nTable 3. Publication Venues with DL-based Studies.\nConference venue\n#Studies\nJournal venue\n#Studies\nSANER\n22\nTSE\n11\nASE\n16\nIST\n11\nICSE\n13\nJSS\n10\nMSR\n10\nTOSEM\n5\nICSME\n8\nESE\n2\nISSTA\n7\nASEJ\n2\nFSE\n6\nIETS\n1\nICST\n5\nSTVR\n1\nICPC\n4\nSQJ\n1\nRE\n3\nISSRE\n3\nESEM\n1\nESEM\nISSRE\nRE\nICPC\nICST\nFSE\nISSTA\nICSME\nMSR\nICSE\nASE\nSANER\nNumber of Publications\n0\n10\n20\n30\n40\n50\nYear\n2015\n2016\n2017\n2018\n2019\n2020\n(a) Number of primary studies published in various conference pro-\nceedings.\nSQJ\nSTVR\nIETS\nASEJ\nESE\nTOSEM\nJSS\nIST\nTSE\nNumber of Publications\n0\n5\n10\n15\nYear\n2015\n2016\n2017\n2018\n2019\n2020\n(b) Number of primary studies published in various journals.\nFig. 3. Distribution of different publication venues.\nFSE, respectively. Meanwhile, in all journals, TSE and IST include the highest number of relevant papers (11).\nTen studies related to DL techniques were published in JSS, and 5 were published in TOSEM. Almost half of the\npublication venues only published not more than 5 relevant papers.\nWe also checked the distribution of primary studies published in conferences and journals between 2015 and 2020,\nshown in Fig. 3. Fig 3(a) illustrates that the publication trend of various conference proceedings and symposiums\nhas a noticeable increase from 2015 to 2019. 70.4% of conference papers were published in 2018 and 2019, while\nonly a few different conferences or symposium venues included relevant papers between 2015 and 2017, which\ndemonstrates a booming trend in the last few years.\nFig. 3(b) shows the number of primary study papers published in different journal venues. It can be seen that\nthere is an increasing trend in the last five years, especially between 2018 and 2020. Furthermore, the relevant\npapers published in TSE, as one of the most popular journals, accounts for the largest proportion in 2018 and 2019;\nwhile another popular journal, IST, also makes up a large percentage in 2019 and 2020.\n4.3\nTypes of main contributions\nWe summarized the main contribution of each primary study and then categorized these studies according to\ntheir main contributions into five categories, i.e., New technique or methodology, Tool, Empirical study, Case\nstudy, and User study. We gave the definition of each main contribution in Table 4. The main contribution of 76%\nprimary studies was to build a novel DNN as their proposed new technique or methodology for dealing with various\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:9\nTable 4. The definition of five main contributions in primary studies.\nMain contribution\nDefinition\nNew technique or methodology\nThe study provided a solid solution or developed a novel framework to\naddress specific SE issues.\nTool\nThe study implemented and published a new tool or tool demo targeting\nSE issues.\nEmpirical study\nThe study collected primary data and performed a quantitative and quali-\ntative analysis on the data to explore interesting findings.\nCase study\nThe study analyzed certain SE issues based on one or more specific\ncases.\nUser study\nThe study conducted a survey to investigate the attitudes of different\npeople (e.g., developers, practitioners, users, etc) towards SE issues.\nproblems in different SE activities. 10% of relevant studies concentrated on performing assessment and empirical\nstudies for exploring the benefits of DL towards different SE aspects, such as research on the differences between\nML and DL to solve certain SE tasks, the performance of using DL to mine software repositories, applying DL\nin testing, etc. The main contribution of 9% was case studies. 2 primary studies (1%) that both proposed a novel\nmethodology and evaluated the novel methodology via a user study. Therefore, the main contribution of these two\nstudies spans across two categories, i.e., New technique or methodology and user study.\nSummary\n(1) DL has shown a booming trend in recent years\n(2) Most of primary study papers were published between 2018 and 2020\n(3) The number of conference papers employing DNNs for SE significantly exceeds that of journal papers.\n(4) SANER is the conference venue publishing the most DL-based papers (22), while TSE and IST include the\nhighest number of relevant papers among all journals (11).\n(5) Most DL-based studies were only published in a few conference proceedings (e.g., SANER, ASE, ICSE, MSR)\nand journals (e.g., TSE, IST, JSS, and TOSEM).\n(6) The main contribution of 75% primary studies is to propose a novel methodology by applying various DL\ntechniques, while only two primary studies performed a user study to better understand users’ attitudes and\nexperience toward various DNNs used for solving specific SE tasks.\n5\nRQ2: WHAT DL TECHNIQUES ARE APPLIED TO SUPPORT SE TASKS?\n5.1\nClassification of DNNs in SE\nMany sorts of DNNs have been proposed, and certain neural network architectures contain diverse DNNs with\ndifferent implementations. For instance, although LSTM and GRU are considered two different DNNs, they are both\nRNNs. We categorized DL-based models according to their architecture and different DNNs used. We classified the\narchitecture of various DNNs into 3 categories: the layered architecture, AutoEncoder (AE), and Encoder-Decoder\n[20, 91]. We provided a detailed classification of DNNs into five categories, i.e., RNN, CNN, FNN, GNN, and\ntailored DNN models, where tailored DNNs include the DNNs not often used in SE, e.g., DBN, HAN, etc. Table 5\nshows the variety of different DNNs, and also lists the number of times these models have been applied in SE.\nAs can be seen from Table 5 that compared Encoder-Decoder and AutoEncoder (AE) architectures, layered\nbased DNNs are the most popular and widely used architecture. In the layered architecture, 72 primary studies used\nnine different kinds of RNN-based models to solve practical SE issues, where LSTM is the most often applied\nRNN-based model (35), followed by standard RNN (23). The variants of LSTM, such as GRU and Bi-LSTM, are\noften adopted by researchers in multiple research directions. 48 primary studies employed CNN-based models,\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:10\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nTable 5. The number of various DNNs applied in per year.\nArchitecture\nFamily\nModel Name\n2015\n2016\n2017\n2018\n2019\n2020\nTotal\nLayered\narchitecture\n(157)\nRNN-based\nmodel (72)\nRNN\n1\n3\n7\n10\n2\n23\nRtNN\n1\n1\n2\nBidirectional RNN (BRNN)\n1\n1\nLSTM\n3\n10\n16\n6\n35\nBi-LSTM\n1\n1\n2\n4\nSiamese LSTM\n1\n1\nGRU\n1\n3\n4\nBidirectional GRU\n1\n1\nRecurrent Highway Network\n1\n1\nCNN-based\nmodel (48)\nCNN\n2\n2\n13\n20\n6\n43\nTree-based CNN (TBCNN)\n2\n1\n3\nRCNN\n1\n1\nDeep Residual Network\n1\n1\nFNN-based\nmodel (25)\nFNN\n3\n1\n8\n7\n3\n22\nRBFNN\n1\n1\nDeep Sparse FNN\n1\n1\nDeep MLP\n1\n1\nGNN-based\nmodel (6)\nGGNN\n4\n1\n5\nGraph Matching Network\n(GMN)\n1\n1\nTailored\nmodel (4)\nDeep\nBeliefe\nNetwork\n(DBN)\n1\n1\n2\nHAN\n1\n1\nDeep Forest\n1\n1\nEncoder-Decoder\n(15)\nRNN-based\nmodel (12)\nRNN\n1\n1\n6\n8\nLSTM\n2\n2\n4\nCNN-based\nmodel (1)\nCNN\n1\n1\nFNN-based\nmodel (2)\nFNN\n1\n1\n2\nAutoEncoder\n(7)\nRNN-based\nmodel (1)\nGRU\n1\n1\nCNN-based\nmodel (1)\nCNN\n1\n1\nFNN-based\nmodel (5)\nFNN\n2\n1\n2\n5\nwhere almost 90% of studies employed CNN. FNN-based model is the third most frequently used family with 25\nstudies using FNNs, followed by GNN-based models and tailored models. There are 24 combined DNNs were\nproposed in tailored models.\n15 primary studies leveraged different types of DNNs following the Encoder-Decoder architecture, where RNNs\nwere used in 12 studies, which is much higher than the number of other models used, i.e., CNN and FNN. In the\nlast architecture, over 70% of studies used FNN-based AEs as their proposed novel approaches; only 2 studies\nselected GRU and CNN to construct AEs respectively.\n5.2\nDL technique selection strategy\nSince heterogeneous DNNs have been used for SE tasks, selecting and employing the most suitable network is a\ncrucial factor. We scanned the relevant sections of DL technique selection in all of the selected primary studies and\nclassified the extracted rationale into three categories.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:11\nCharacteristic-based selection strategy (𝑆1): The studies justified the selected techniques based on their\ncharacteristics to overcome the obstacles associated with a specific SE issue [12, 23, 33, 145]. For instance, most\nof the seq2seq models were built by using RNN-based models thanks to their strong ability to analyze the sequence\ndata.\nSelection based on prior studies (𝑆2): Some researchers determined the most suitable DNN used in their\nstudies by referring to the relevant DL techniques in the related work [10, 49, 137]. For instance, due to the good\nperformance of CNN in the field of image processing, most studies selected CNN as the first option when the\ndataset contains images.\nUsing multiple feasible DNNs (𝑆3): Though not providing any explicit rationale, some studies designed\nexperiments for technique comparisons that demonstrated that the selected algorithms performed better than\nother methods. For example, some studies often selected a set of DNNs in the same SE tasks to compare their\nperformance and picked up the best one [2, 22, 110].\nWe noticed that the most commonly selection strategy is 𝑆1 (i.e., Characteristic-based selection strategy),\naccounting for 69%, nearly 3 times that of 𝑆2 (25%). Only 6% of primary studies adopt 𝑆3 to select their suitable\nDL algorithms.\nSummary\n(1) There are 30 different DNNs used in the selected primary studies.\n(2) We used a classification of DL-based algorithms from two perspectives, i.e., their architectures and the families\nto which they belong. The architecture can be classified into three types: Layered architecture, Encoder-Decoder,\nand AutoEncoder (AE); the family can be classified into five categories: RNN-based, CNN-based, FNN-based,\nGNN-based, and Tailored models.\n(3) Compared with Encoder-Decoder and AE, the layered architecture of DNNs is by far the most popular option in\nSE.\n(4) Four specific DNNs are used in more than 20 primary studies, i.e., CNN (43), LSTM (35), RNN (23), and FNN\n(22), and each of them has several variants that are also often used in many SE tasks.\n(5) We summarized three types of DNN-based model selection strategies. The majority of studies adopted 𝑆1 to\nselect suitable DL algorithms. Only 6% of primary studies used 𝑆3 as the model selection strategy due to the\nheavy workload brought by 𝑆3.\n6\nRQ3: WHAT KEY FACTORS CONTRIBUTE TO DIFFICULTY WHEN TRAINING DNNS IN\nSE?\nSince analyzing a DL architecture can provide a lot of insight, we investigated the construction process of a\nDL framework from three aspects: techniques used in data processing, model optimization, evaluation, and the\naccessibility of primary studies.\n6.1\nHow were datasets collected, processed, and used?\nData is one of the most important roles in the training phase. Unsuitable datasets can result in failed approaches or\ntools with the low performance. We focused on the data used in primary studies and conducted a comprehensive\nanalysis on the steps of data collection, data processing, and data application.\n6.1.1\nWhat were the sources of datasets used for training DNNs? Fig. 4 shows the sources of datasets in\nthe selected primary studies. It can be seen that 45% of primary studies trained DNNs by using an open-source\ndataset. One reason for choosing an open-source dataset is that studies are willing to pick up these datasets to\nevaluate the effectiveness of proposed DL-based approaches due to the existence of widely accepted datasets in\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:12\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\n(a) The sources of DL-related dataets.\n(b) The sources of collected datasets.\nFig. 4. The source of datasets used in primary study papers\ncertain SE issues (e.g., code clone detection, software effort/cost prediction, etc). Others are used because the\ndatasets were applied in related previous work. Due to the lack of available and suitable datasets, 18% of primary\nstudies constructed new datasets. Real-world datasets from industry are only used by 4% of studies.\n33% of studies performed a series of experiments on large-scale datasets so as to verify the scalability and\nrobustness of their models. To achieve this, many studies collected multiple small datasets from different sources.\nFig. 4(b) describes the sources of collected datasets. As tens of thousands of developers contribute to GitHub\ncommunity by uploading source code of their software artifacts, GitHub has become the most frequently used\nsource of collected data (51%). 26% of studies collected their datasets from different systems and projects. For\ninstance, Deshmukh et al. [21] collected bug reports from several bug tracking systems, i.e., issue tracking systems\nof Open Office, Eclipse, and Net Beans projects, as datasets to build a DL-based model for duplicated bug detection.\nThe app store is the third-largest source (11%), followed by Stack Overflow (SO) and Youtube.\n6.1.2\nWhat are the types of SE datasets used in prior DL studies? The datasets used in primary studies are\nof various data types. It is essential to analyze data types of datasets since the relationship between the type of\nimplicit feature being extracted and the architecture has a dominating influence on model selection. We summarize\nthe data types in primary studies and interpreted how data types determine the choice of DNNs.\nWe classified the data types of used datasets into four categories – code-based, text-based, software repository-\nbased, and user-based data types. Table 6 describes specific data in each data type. 104 primary studies collected\ndata from source code, and most of these studies used source code directly in some important SE activities, such\nas software testing and maintenance. Datasets containing various metrics were employed in 8 relevant studies,\nfollowed by code comments, defects (7), and test cases (6). Whereas few studies focused on analyzing the code\nannotation, pull-request, and patches. 8 primary studies used a multitude of screencasts as their datasets, where\n4 studies selected program screencasts to analyze developers’ behavior and 4 studies researched UI images for\nimproving the quality of APPs.\nText-based data types were the second most popular, including 13 different kinds of documentation. Bug report\nand requirements documentation are the two most commonly applied text-based data types in primary studies.\nSome types rarely appeared, such as logs, certifications, design documentation, etc.\nSince software repositories, especially SO and GitHub, contain a lot of useful patterns or information, we\nclassified the type of the information collected from these repositories into ′𝑆𝑜𝑓𝑡𝑤𝑎𝑟𝑒𝑟𝑒𝑝𝑜𝑠𝑖𝑡𝑜𝑟𝑦−𝑏𝑎𝑠𝑒𝑑𝑑𝑎𝑡𝑎\n𝑡𝑦𝑝𝑒𝑠′. 12% of studies concentrated on obtaining and learning useful information and patterns by crawling\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:13\nTable 6. Data types of datasets involved in primary studies.\nFamily\nData types\n#Studies\nTotal\nCode-based data types\nSource code\n61\nSoftware/code metric\n8\nCode comment\n7\nDefects\n7\nTest case\n6\nprogram screencasts\n4\nUI images\n4\nCode change\n2\nCode annotation\n2\nPull-requests\n2\nPatch\n1\n104\nText-based data types\nBug report\n9\nRequirement documentation\n4\nconfiguration documentation\n2\nAPP description\n2\nSoftware version information\n2\nDesign documentation\n1\nLog information\n1\nCertification\n1\nProtocol message\n1\nPatch\n1\n23\nSoftware repository-based data types\nQ&A in SO\n6\nTags in SO\n5\nIssues and commits\n4\nPull-requests\n2\n17\nUser-based data types\nUser behavior\n3\nUser review\n1\nInteraction traces\n1\n5\nrelated contents from SO (e.g., Q&A (questions and answers) and tags) and GitHub (e.g., issues, commits and,\npull-requests).\nUser-based data generally contains a great deal of user information, which can promote developers to better\ncomprehend user needs and behavior targeting different applications. Only 5 studies adopted user-based data types\n(i.e., user behavior, review, and interactions) to solve relevant SE tasks.\n6.1.3\nWhat input forms were datasets transformed into when training DNNs? The inputs of DNNs need to\nbe various forms of vectors. We found two techniques were often used to transform different source data types\ninto vectors: One-hot encoding and Word2vec. Only 5 studies produced the input of their models by adopting the\nOne-hot technique. We described input forms using 5 categories referring to their data types.\nToken-based input: Since some studies treated source code as text, they used a simple program analysis\ntechnique to generate code tokens into sequences and transformed tokens into vectors as the input of their DL-based\nmodels. A token-based input form can be applied to source code and text-based data when processing related\ndatasets.\nTree/graph-based input: To better comprehend the structure of source code, several studies convert source\ncode into Abstract Syntax Trees (AST) or Control Flow Graphs (CFGs), and then generate vector sequences by\ntraversing the nodes in each tree or graph.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:14\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nTable 7. The various input forms of DL-based models proposed in primary studies.\nFamily\nInput forms\n#Studies\nTotal\nToken-based input\nCode in tokens\n17\nText in tokens\n34\nCode and text in tokens\n13\n64\nTree/graph-based input\nCode in tree structure\n25\nCode in graph structure\n4\n29\nFeature-based input\nfeature/metric\n33\n33\nPixel-based input\npixel\n9\n9\nHybrid input\nCode in tree structure + text in token\n4\nCode features + text in token\n2\nCode in tree structure + features\n1\n7\nFeature/metric-based input: For analyzing the characteristics of software artifacts, some studies applied\ndatasets consisting of features or metrics extracted from different products, and thus the input form of the models\nproposed in these studies is software feature/metric-based vectors.\nPixel-based input: Some studies used datasets containing a large number of images and screencasts, e.g.,\nprogram screencasts, UI images, etc. When preprocessing these datasets, they often broke down screencasts into\npixels as an effective input form, for analyzing graph-based datasets in different SE tasks, such as bug detection,\ncode extraction, etc.\nCombined input: Many studies combined two or more data types extracted from software products to build\ncomprehensive datasets with more information for enhancing the quality and accuracy of proposed models. For\ninstance, Leclair et al. [60] proposed a novel approach for generating summaries of programs not only by analyzing\ntheir source code but also their code comments.\nTable 7 depicts the input formats of DL-based models. We can see that over 45% of studies transformed data (i.e.,\nsource code and various documentations) into the token-based input form (64), where 17 studies considered source\ncode as texts and thus converted code into token sequences as the input of models. 13 studies used both source\ncode and text-based materials and also constructed a token-based data structure as the input form of their proposed\nmodels. 25 studies utilize tree-based input form to analyze the source code, and only 4 studies transform source\ncode into a graph-based structure for extracting essential information. 33 studies adopted embedding techniques to\ngenerate feature-based vectors. Furthermore, 8 studies using image-based datasets split screencasts into pixels as\nthe basic unit of the input form. Only 7 studies processed datasets into multiple forms.\n6.2\nWhat techniques were used to optimize and evaluate DL-based models in SE?\nIn the training phase, developers attempt to optimize the models in different ways for achieving good performance.\nIn this section, we summarized the information describing the optimization methods and evaluation process, and\nperformed an analysis on key techniques.\n6.2.1\nWhat learning algorithms are used in order to optimize the models? The performance of DL-based\nmodels is dependent on selected optimization methods, which can systematically adjust the parameters of the DNN\nas training progresses.\nOut of the 142 studies analyzed, 131 identified the specific optimization method, but 11 studies did not mention\nwhat optimizers were used to adjust parameters in their work. Fig. 5 illustrated the frequency of the use of 20\noptimization methods used in all primary studies. We see that 6 optimizers were used in no less than 5 studies,\nwhere Adam optimizer is the most commonly used optimization method. Stochastic gradient descent (SGD) and\ngradient descent (GD) are also popular optimizers, which were used by 21 and 15 studies respectively, followed\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:15\n32\n21\n15\n14\n11\n8\n5\n3\n3\n3\n3\n2\n2\n2\n2\n1\n1\n1\n1\n1\nOptimization algorithms\nAdam\nSGD\nGD\nBackpropagation\nFine-tuning\nHyperparameter optimiser\nAdam update\nRMSprop\nMini-batch SGD\nPSO\nPolicy gradient\nAdagrad\nAdadelta\nMini-batch Adam\nL-BFGS\nAdaMax\nAdam stochastic optimiser\nNegative sampling\nRe-sampling\nSCG\nNumber of primary studies\n0\n5\n10\n15\n20\n25\n30\n35\nFig. 5. Various optimization algorithms used in primary studies.\nby back-propagation (14) and fine-tuning (11). Besides, some optimization methods are not often used, such as\nAdagrad and Adadelta.\n6.2.2\nWhat methods were used to alleviate the impact of Overfitting? One major problem associated with\napplying any type of learning algorithm is overfitting. Overfitting is the phenomenon of a DNN learning to fit\nthe noise in the training data extremely well, yet not being able to generalize to unseen data, which is not a good\napproximation of the true target function that the algorithm is looking forward to learn. We describe 9 general ways\nto combat overfitting [38, 107] by considering 3 aspects: data processing, model construction as well as model\ntraining, and then analyzed the usage distribution of these methods in relevant studies.\nCross-validation: A cross-validation algorithm can split a dataset into k groups (k-fold cross-validation).\nResearchers often leave one group to be the validation set and the others as the training set. This process will be\nrepeated until each group has been used as the validation set. Since a remaining subset of data is new towards the\ntraining process of DL-based models, the algorithm can only rely on the learned knowledge from other groups of\ndata to predict the results of the remaining subset, preventing overfitting.\nFeature selection: Overfitting can be prevented by selecting several of the most essential features for training\nDL-based models can effectively avoid overfitting. Therefore, researchers can pick up some key features by using\nfeature selection methods, train individual models for these features, and evaluate the generalization capabilities of\nmodels. For instance, Pooling is a typical technique to prevent overfitting since pooling can reserve main features\nwhile reducing the number of parameters and the amount of computation, and improve the generalization ability of\nthe model.\nRegularization: Regularization is a technique to constrain the network from learning a model that is too complex,\nwhich therefore can avoid overfitting. A penalty term would be added in the cost function to push the estimated\ncoefficients towards zero by applying L1 or L2 regularization.\nDropout: By applying dropout, a form of regularization, to the layers of DNNs, a part of neurons were ignored\nwith a set probability. Therefore, dropout can reduce interdependent learning among units to avoid overfitting.\nData augmentation: A larger dataset can reduce the chance of overfitting. Data augmentation is a good way to\nartificially increase the size of our dataset for improving the performance of a DNN when the scale of data was\nconstrained due to difficult to gather more data. For example, many studies performed various image transformations\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:16\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\n40\n25\n20\n20\n8\n6\n5\n1\n4\nDifferent overﬁ\u0000tting techniques\nCross-validation\nPooling\nRegularization\nDropout\nData augment\nEarly stopping\nData balancing\nEnsembling\nOthers\nThe times of techniques used for combating overﬁ\u0000tting\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nFig. 6. The distribution of various overfitting techniques used in primary studies\nto the image dataset (e.g., flipping, rotating, rescaling, shifting) for enlarging data size in the image classification\ntask.\nEarly stopping: Early stopping is an effective method for avoiding overfitting by truncating the number of\niterations, that is, stopping iterations before DL-based models converge on the training dataset to eliminate the\nimpact on overfitting.\nData balancing: With imbalanced data, DL-based models are likely to occur the overfitting problem since\nmodels will learn imbalanced knowledge with a disproportionate ratio of observations in each class. Using some\ndata balancing techniques can effectively alleviate the impact on models’ performance caused by overfitting.\nEnsembling: Ensembles are a set of machine learning methods for combining predictions from multiple separate\nmodels. For instance, Bagging as an ensemble learner can reduce the chance of overfitting complex models by\ntraining a large number of \"strong\" learners in parallel without restriction.\nFig. 6 illustrates the distribution of the techniques used for combating overfitting problems. Cross-validation has\nbeen used frequently among the selected studies to prevent overfitting; it is used in 40 studies, followed by pooling\n(25). Regularization and dropout are the third most popular techniques used in 20 studies. There are 8 studies that\nprevent overfitting by enlarging the scale of data, such as using a large-scale dataset, combining multiple datasets,\nand using different data augmentation techniques. 6 studies used early stopping and 5 selected data balancing to\ncombat the overfitting problem. Ensembling is the least frequently used one (1) compared with other techniques\n(1). Furthermore, among all primary studies, 4 studies used several new algorithms proposed by some studies\nto solve overfitting. We analyzed which factors may have an impact on the overfitting technique selection. We\nnoticed that the techniques used for combating overfitting have no strong association with either data types or input\nforms. However, there is a special relationship between model selection and these techniques. Most of the studies\nthat adopted CNNs to address specific SE tasks selected pooling as their first choice for preventing the overfitting\nproblem.\n6.2.3\nWhat measures are used to evaluate DL-based models? Accessing appropriate benchmarks is a\ncrucial part of evaluating any DL-based models in SE. We also explored the frequent metrics used to measure the\nperformance of DL-based models applied to respective SE tasks.\nTable 8 summarizes the commonly used evaluation metrics in the primary studies, used in no less than 3 studies.\nPrecision, recall, F1-measure, and accuracy are widely accepted metrics for evaluating the performance of DL-based\nmodels. Some studies adopted MRR and BLEU as evaluation metrics in their work, potentially indicating that\nmany studies focused on addressing ranking and translation tasks by training various DNNs. Another interesting\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:17\nTable 8. Metrics used for evaluation.\nMetrics\n#Studies\nPrecision@k\n69\nRecall@k\n59\nF1@k\n53\nAccuracy\n26\nMean Reciprocal Rank (MRR)\n15\nBLEU\n13\nRunning time\n13\nAUC\n11\nMean Average Precision (MAP)\n7\nMatthews Correlation Coefficient (MCC)\n5\nP-value\n4\nMETEOR\n4\nROC\n4\nROUGE\n4\nMean Absolute Error (MAE)\n4\nSuccessRate@k\n3\nCliff’s Delta\n3\nCoverage\n3\nBal (Balance)\n3\nStandardized Accuracy (SA)\n3\nOthers\n11\nobservation from Table 8 is that running time is selected as a performance indicator by a set of studies, which\ndoes not occur frequently when using non-learning techniques. This is because that learning algorithms, especially\nDNNs, require more time during their construction, training, and testing phases due to the high complexity of these\nnetworks (e.g., numerous types of layers, a great many neurons, and different optimization methods). Also, almost\nhalf of metrics are not commonly used in relevant studies, which are only used in 3 or 4 studies (e.g., P-value, ROC,\nROUGE, Coverage, Balance, etc.) and thus these metrics can reflect their respective characteristics of different SE\ntasks.\n6.3\nAccessibility of DL-based models used in primary studies.\nWe checked whether the source code of DL-based models is accessible for supporting replicability and repro-\nducibility. 53 studies provided the replication packages of their DL-based models, only accounting for 37.3% of\nall primary studies. 89 studies proposed novel DL-based models without publicly available source code, making it\ndifficult for other researchers to reproduce their results; some of these studies only disclosed their datasets. Based\non this observation, obtaining open-source code of DNNs is still one of the challenges in SE because many factors\nmay result in never realizing the replicability and reproducibility of DL application, e.g., data accessibility, source\ncode accessibility, different data preprocessing techniques, optimization methods, and model selection methods.\nTherefore, we recommend future DL studies to release replication packages.\nSummary\n(1) Most datasets are available online and 33% of datasets consist of multiple small-scale ones collected from\nGitHub, software systems and projects, and some software repositories.\n(2) 5 different data types are used in the primary studies, i.e., code-based, text-based, software repository-based,\ngraph-based, and user-based data types, where code-based and text-based types are the two main data types\nbeing used in 82.3% of primary studies.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:18\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nFig. 7. The distribution of DL techniques in\nDifferent SE activities.\nFig. 8. The classification of primary studies.\n(3) Most studies parse source code into token, tree, or graph structures, or extract features from programs. When the\nraw datasets are documentation, studies would convert them into token-based vectors as the input form of their\nmodels.\n(4) We observed that Adam is the most popular optimization algorithm being used in 32 studies, followed by\nSGD and GD, and several variants of Adam are still commonly used in SE. There are also some well-known\noptimization algorithms used in primary studies, such as back propagation, fin-tuning, and hyperparameter\noptimizer.\n(5) 9 different ways are used for combating the overfitting problem. 4 techniques were widely used – cross-validation,\nfeature selection, regularization, and dropout. We found that studies applying CNNs would choose the pooling\nmethod to prevent overfitting with high probability, and data type and input form does not influence technique\nselection.\n(6) There are over 20 different metrics used to verify the performance of DL-based models. Precision, recall,\nF1-measure, and accuracy were used commonly in primary studies.\n(7) Only 53 studies provided a public link of their models in their papers and yet 62.7% of proposed models are\ndifficult to be reproduced since the source code is unavailable.\n7\nRQ4: WHAT TYPES OF SE TASKS AND WHICH SE PHASES HAVE BEEN FACILITATED\nBY DL-BASED APPROACHES?\nIn this section, we first categorise a variety of SE tasks into six SE activities referring to Software Engineering\nBody of Knowledge [8], i.e., software requirements, software design, software implementation, software testing\nand debugging, software maintenance, and software management. We then analyze the distribution of DL-based\nstudies for different SE activities. We present a short description of each primary study, including the specific SE\nissue each study focused on, which and how DL techniques are used, and the performance of each DL model used.\n7.1\nDistribution of DL techniques in different SE activities\nWe analysed which SE activities and specific SE tasks each selected primary study tried to solve. As shown in\nFig. 7, the largest number of primary studies focused on addressing SE issues in software maintenance (39%).\n36% of studies researched software testing and debugging. Software management was the topic of 11% of primary\nstudies, followed by software implementation (9%). Software design and modeling (3%) and software requirements\n(2%) are addressed in very few studies.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:19\nTable 9. The specific research topics where DL techniques are often applied.\nSE activities\nspecific research topics\n#Studies\nTotal\nSoftware design\nSource code representation\n5\n5\nSoftware implementation\nCode search\n5\nCode programming\n4\n9\nSoftware testing and debugging\nDefect prediction\n11\nBug localization\n7\nApplication testing\n7\nProgram analysis\n5\nTest case generation\n4\nReverse execution\n3\n37\nSoftware maintenance\nCode clone detection\n11\nProgram repair\n6\nCode comment generation\n4\nSoftware quality evaluation\n4\nSource code representation\n4\n28\nSoftware management\nSoftware repository mining\n19\nEffort cost prediction\n6\n25\nWe classified all primary studies into four categories based on the types of their SE tasks, i.e., the regression\ntask, classification task, ranking task, and generation task. Fig. 8 describes the distribution of different task types\nwhere DL techniques were applied. Classification and generation tasks account for almost 80% of primary studies,\nwhere classification is the most frequent task (54%). 13% of studies belong to the regression task and the output\nof their proposed models is a prediction value, such as effort cost prediction. In SE, some studies adopted DL to\nconcentrate on a ranking task, accounting for 11% of all studies.\nWe summarized a set of research topics in which DL was engaged. Table 9 lists the research topics containing no\nless than three related studies. Software testing and debugging, as the most prevalent SE activity, has 37 primary\nstudies in six topics. The most popular study is defect prediction (11 studies), followed by bug localization (7) and\napplication testing (7). Software maintenance, as the second most popular activities, involves five research topics\nwith 28 relevant studies, where code clone detection is the most popular research topic. Two SE activities, software\nimplementation and software management, both contain two important research topics, where 19 primary studies\nmined software repositories by training DNNs, 6 studies estimated development cost, and 5 studies applied DL for\ncode search. Software design and modeling only involve one popular topic, i.e., source code representation/modeling.\nThere are no topics with more than three studies using DL techniques in software requirements.\n7.2\nSoftware requirements\n7.2.1\nRequirements analysis. A number of natural language-based documents that describe users’ specific\nneeds or services of a software product can be referred to as user requirements (aka, use cases, or actions) [127].\nExtracting use cases of a product from a large volume of textual requirement documentation is a common but labor-\nintensive task. Since the manual mapping system states between requirements and simulation is a time-consuming\ntask, Pudlitz et al. [100] proposed a self-trained Named-entity Recognition model combined with Bi-LSTM and\nCNN to extract the system states from requirements specification, working to reduce labor cost when linking the\nstate extracted from requirements to the simulation signal.\n7.2.2\nrequirement validation. The requirements specification may be subject to validation and verification\nprocedures, ensuring that developers have understood the requirements and the requirements conform to company\nstandards. Winkler et al. [128] present an automatic approach to identify and determine the method for requirement\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:20\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nvalidation. They predefined six possible verification methods and trained a CNN model as a multiclass and\nmultilabel classifier to classify requirements with respect to their potential verification methods. The mixed results\nrevealed that the imperfect training data impacted the performance of their classifier, but it still achieved good\nresults on the testing data.\n7.3\nSoftware design\n7.3.1\nSoftware design patterns detection. UI design is an essential component of software development, yet\nprevious studies cannot reliably identify relevant high-fidelity UI designs from large-scale datasets. Martín et al. [78]\nproposed a DL-based search engine to detect UI designs in various software products. The core idea of this search\nengine is to build a CNN-based wireframe image autoencoder to automatically generate labels on a large-scale\ndataset of Android UI designs. After manual evaluation of experimental results, they confirmed that their search\nengine achieved superior performance compared with image-similarity-based and component-matching-based\nmethods. Thaller et al. [108] proposed a flexible human- and machine-comprehensible software representation\nalgorithm, namely Feature Maps. They first extracted subtrees from the system’s abstract semantic graph (ASG).\nThen their algorithm pressed the high-dimensional and inhomogeneous vector space of these micro-structures into\na feature map. Finally, they adopted a classical machine learning model and a DL model (i.e., Random Forest and\nCNN) to identify instances of design patterns in source code. Their evaluation suggested that Feature Map is an\neffective software representation method, revealing important information hidden in the source code.\n7.3.2\nGUI modeling. Chen et al. [12] proposed a neural machine translator to learn a crowd-scale knowledge\nof user interfaces (UI). Their generative tool encoded the spatial layouts of visual features learned from a UI\nimage and learned to generate its graphical user interface (GUI) skeleton by combining RNN and CNN models. Its\nperformance had been verified on the large-scale UI data from real-world applications. Moran et al. [88] proposed\na strategy to facilitate developers automate the process of prototyping of GUIs in 3 steps: detection, classification,\nand assembly. First, they used computer vision techniques to detect logical components of a GUI from mock-up\nmetadata. They then trained CNNs to category GUI-components into domain-specific types. Finally, a KNN\nalgorithm was applied to generate a suitable hierarchical GUI structure to assemble prototype applications. Their\nevaluation achieved an average GUI-component classification accuracy of 91%.\n7.4\nSoftware implementation\n7.4.1\nCode search. Gu et al. [33] proposed DeepAPI, a DL-based approach to generate functional API usage\nsequences for a given natural language-based user query by using an attention-based GRU Encoder-Decoder.\nDeepAPI first encoded the user query into a fixed-length context vector and produced the API sequence according\nto the context vector. It also enhanced their model by considering the importance of individual APIs. To evaluate\nits effectiveness, they empirically evaluated their approach on 7 million code snippets. Gu et al. [32] proposed a\ncode search tool, DeepCS by using a novel DNN model. They considered code snippets as well as natural language\ndescriptions, and then embedded them into a high-dimensional unified vector representation. Thus, DeepCS gave\nthe relevant code snippets by retrieving the vector of the corresponding natural language query. They evaluated\nDeepCS with a large-scale dataset collected from GitHub.\nRecently, several proposals use DL techniques for code search by embedding source code and given queries\ninto vector space and calculating their semantic correlation [3]. Cambronero et al. [10] noticed that multiple\napproaches existed for searching related code snippets applied unsupervised techniques, while some adopted\nsupervised ones to embed source code and queries for code search. They defined 3 RQs to investigate whether\nusing supervised techniques is an effective way for code search and what types of DNNs and training corpus\nto use for this supervision. To understand these tradeoffs quantitatively, They selected and implemented four\nstate-of-the-art code search techniques. They found that UNIF outperformed CODEnn and SCS models based on\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:21\ntheir benchmarks and suggested evaluating simple components first before integrating a complicated one. Wan et\nal. [115] addressed the lack of analysis of structured features and inability to interpret search results in existing\ncode search works. To address these issues, they presented a new approach MMAN, which adopted three different\nDNNs (i.e., LSTM, Tree-LSTM, and GGNN (Gated Graph Neural Network)) to analyze both shallow features and\nthe semantic features in ASTs, and control-flow graphs (CFGs) of source code. The final results on a large-scale\nreal-world dataset demonstrated that MMAN accurately provided code snippets. Huang et al. [45] proposed an\nattention-based code-description representation learning model (CDRL) to refine the general DL-based code search\napproaches. They only picked up description terms and semantically related code tokens to embed a given query\nand its code snippet into a shared vector space.\n7.4.2\nProgramming. Gao et al. [26] introduced an attention-based Encoder-Decoder framework to directly\ngenerate sensible method names by considering the relationship between the functional descriptions and method\nnames. To evaluate their model, experiments were performed on large-scale datasets for handling the cold-start\nproblem, and the model achieved significant improvement over baselines. Alahmadi et al. [2] applied a CNN model\nto automatically identify the exact location of code in images for reducing the noise. They extracted 450 screencasts\ncovering C#, Java, and Python programming languages to evaluate their model, and the final result showed that\nthe accuracy of their model achieved 94%. Wang et al. [121] proposed a Neural Network-based translation model\nto address the domain-specific rare word problem when carrying out software localization. They trained an RNN\nencoder-decoder framework and enhanced it by adding linguistic information. Nguyen et al. [92] proposed a\nDL-based language model, Dnn4C, which augmented the local context of lexical code elements with both syntactic\nand type contexts by using an FNN model. Empirical evaluation on code completion showed that Dnn4C improved\naccuracy by 24.9% on average over four baseline approaches.\n7.5\nSoftware testing and debugging\n7.5.1\nDefect prediction. Defect prediction is the most extensive and active research topic in use of DL techniques\nin software maintenance. Almost 30% of primary studies focused on identifying defects [4, 18, 74, 111, 123, 133,\n147].\nMetrics-based defect prediction. Metrics or features extracted from a software product can give a vivid\ndescription of its running state, and thus it is easy for researchers and participants to use these software metrics\nfor defect prediction. Tong et al. [111] proposed a novel two-stage approach, SDAEsT, which is build based on\nstacked denoising autoencoders (SDAEs) and a two-stage ensemble (TSE) learning strategy. Specifically, in the\nfirst stage, they used SDAEs to extract more robust and representative features. To mitigate the impact on the class\nimbalance problem and eliminate the overfitting problem, they propose TSE learning strategy as the second phase.\nThey evaluated their work using 12 open-source defect datasets. Xu et al. [133] built an FNN model with a new\nhybrid loss function to learn the intrinsic structure and more discriminative features hidden behind the programs.\nPrevious studies obtained process metrics throughout analyzing change histories manually and often ignored the\nsequence information of changes during software evaluation. For better utilization of such sequence data, Wen et\nal. [123] built an RNN model to encode features from change sequences. They considered defect prediction as\nto the sequence labeling problem and performed fine-grained change analysis to extract six categories of change\nsequences, covering different aspects of software changes. Their evaluation on 10 public datasets showed that their\napproach achieved high performance in terms of F1-measure and AUC. To address the same problem, Liu et al.\n[74] proposed to obtain the Historical Version Sequence of Metrics (HVSM) from various software versions as\ndefect predictors and leveraged RNN to detect defects. Barbez et al. [4] analyzed and mined the version control\nsystem to achieve historical values of structural code metrics. They then trained a CNN based classifier, CAME, to\ninfer the anti-patterns in the software products.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:22\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nSemantic-based defect prediction. Wang et al. [117, 118] leveraged Deep Belief Network (DBN) to automati-\ncally learn semantic features from token vectors extracted from programs’ ASTs, compared to most previous works\nthat use manual feature specification. They evaluated their approach on file-level defect prediction tasks (within-\nproject and cross-project) and change-level defect prediction tasks (within-project and cross-project) respectively.\nThe evaluation results confirmed that DBN-based semantic features significantly outperformed the previous defect\nprediction based on traditional features in terms of F1-measure. Similarly, Dam et al. [18] used a tree-based LSTM\nnetwork, which can directly match with the AST of programs for capturing multiple levels of the semantics of\nsource code.\nJust-In-Time (JIT) defect prediction. Hoang et al. [41] presented an end-to-end DL-based framework, DeepJIT,\nfor change-level defect prediction, or Just-In-Time (JIT) defect prediction. DeepJIT automatically extracted features\nfrom code changes and commit messages, and trained a CNN model to analyze them for defect prediction. The\nevaluation experiments on two popular projects showed that DeepJIT achieved improvements over 10% for two\nopen-source datasets in terms of AUC.\n7.5.2\nBug detection. Wan et al. [114] implemented a Supervised Representation Learning Approach (SRLA)\nbased on an autoencoder with double encoding-layers to conduct cross-project Aging-Related Bugs (ARBs)\nprediction. They compared SRLA with the state-of-the-art approach, TLAP, to prove the effectiveness of SRLA.\nWang et al. [122] present a novel framework, Textout, for detecting text-layout bugs in mobile apps. They formulated\nlayout bug prediction as a classification issue and addressed this problem with image processing and deep learning\ntechniques. Thus, they designed a specifically-tailored text detection method and trained a CNN classifier to identify\ntext-layout bugs automatically. Textout achieved an AUC of 95.6% on the dataset with 33,102 text-region images\nfrom real-world apps. Source code is composed of different terms and identifiers written in natural language with\nrich semantic information. Based on this intuition, Li et al. [63] trained a DL-based model to detect suspicious\nreturn statements. They used a CNN to determine whether a given return statement in source code matched its\nmethod signature. To reduce the impact of the lack of negative training data, they converted the correct return\nstatements in real-world projects to incorrect ones. Li et al. [66] proposed an AIOps solution for identifying node\nfailures for an ultra-large-scale cloud computing platform at Alibaba.\n7.5.3\nVulnerability detection. Dam et al. [19] described a novel approach for vulnerability detection, which\nautomatically captured both syntactic and semantic features of source code. The experiments on 18 Android\napplications and Firefox applications indicated that the effectiveness of their approach for within-project prediction\nand cross-project prediction. Tian et al. [110] proposed to learn the fine-grained representation of binary programs\nand trained a Gated Recurrent Unit (BGRU) network model for intelligent vulnerability detection. Han et al. [37]\ntrained a shallow CNN model to capture discriminative features of vulnerability description and exploit these\nfeatures for predicting the multi-class severity level of software vulnerabilities. They collected large-scale data\nfrom the Common Vulnerabilities and Exposures (CVE) database to test their approach.\n7.5.4\nBug localization. To locate buggy files, Lam et al. [57] built an autoencoder in combination with Informa-\ntion Retrieval (IR) technique, rVSM, which learned the relationship between the terms used in bug reports and\ncode tokens in software projects. Some studies proposed to exploit CNN in the bug localization task [48, 129, 139].\nZhang et al. [139] proposed CNNFL, which localized suspicious statements in source code responsible for failures\nbased on CNN. They trained this model with test cases and tested it by evaluating the suspiciousness of statements.\nHuo et al. [48] present a deep transform learning algorithm, TRANP-CNN, for cross-project bug localization by\ntraining a CNN model to extract transferable semantic features from source code. Xiao et al. [129] used the word-\nembedding technique to retain the semantic information of the bug report and source code and enhanced CNN to\nconsider bug-fixing frequency and recency in company with feature detection techniques for bug localization. Li et\nal. [65] proposed a novel approach, DeepFL, to learn latent features for precise fault localization, adopting an RNN\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:23\nmodel. The evaluation on the benchmark dataset, Defects4J, described that DeepFL significantly outperformed\nstate-of-the-art approaches, i.e., TraPT/FLUCCS. Standard code parsers are of little help, typically resolving syntax\nerrors and their precise location poorly. Santos et al. [105] proposed a new methodology for locating syntax errors\nand provided some suggestions for possible changes for fixing these errors. Their methodology was of practical use\nto all developers but especially useful to novices frustrated with incomprehensible syntax errors.\n7.5.5\nTest case generation. Liu et al. [73] proposed a novel approach to automatically generate the most\nrelevant text of test cases based on the context of use cases for mobile testing. Koo et al. [54] implemented a\nnovel approach, PySE1, to generate the test case. PySE1 tackled the limitations of symbolic execution schemes\nby proposing a DL-based reinforcement learning algorithm to improve the branch policy for exploring the worse\ncase program execution. Zhao et al. [142] trained a DL-based model that combines LSTM and FNN to learn the\nstructures of protocol frames and deal with the temporal features of stateful protocols for carrying out security\nchecks on industrial network and generating fake but plausible messages as test cases. Liu et al. [72] proposed a\ndeep natural language processing tool, DeepSQLi, to produce test cases used to detect SQLi vulnerabilities. They\ntrained an encoder-decoder based seq2seq model to capture the semantic knowledge of SQLi attacks and used it to\ntransform user inputs into new test cases.\n7.5.6\nProgram analysis. Program analysis refers to any examination of source code or program executions that\nattempt to find patterns or anomalies thought to reveal specific behaviors of the software.\nStatic analysis. In Android mobile operating systems, applications communicate with each other a message-\npassing system, namely, Inter-Component Communication (ICC). Many serious security vulnerabilities may occur\nowing to misuse and abuse of communication links, i.e., ICCs. Zhao et al. [143] presented a new approach to\ndetermine communication links between Android applications. They augmented static analysis with DL techniques\nby encoding data types of the links and calculating the probability of the link existence. To reduce the number of\nfalse alarms, Lee et al. [62] trained a CNN model as an automated classifier to learn the lexical patterns in the\nparts of source code for detecting and classifying false alarms. Due to the impact of high false-positive rates on\nstatic analysis tools, Koc et al. [53] performed a comparative empirical study of 4 learning techniques (i.e., hand-\nengineered features, a bag of words, RNNs, and GNNs) for classifying false positives, using multiple ground-truth\nprogram sets. Their results suggest that RNNs outperform the other studied techniques with high accuracy.\nType inference. Helledoorn et al. [39] developed an automated framework, DeepTyper, a DL-based model to\nanalyze JavaScript language and learn types that naturally occurred in certain contexts. It then provided a type of\nsuggestion when the type checker cannot infer the types of code elements, such as variables and functions. Malik et\nal. [82] formulated the problem of inferring Javascript function types as a classification task. Thus, they trained a\nLSTM-based neural model to learn patterns and features from code annotated programs collected from real-world\nprojects, and then predicted the function types of unannotated code by leveraging the learned knowledge.\n7.5.7\nTesting techniques. Many studies focus on new methods to perform testing, such as for apps [96], games\n[144], and other software systems [5, 11]. There are also some studies using well-known testing techniques (e.g.,\nfuzzing [17, 30] and mutation testing [83]) for improving the quality of software artifacts. Zheng et al. [144]\nconducted a comprehensive analysis of 1,349 real bugs and proposed Wuji, a game testing framework, which\nused an FNN model to perform automatic game testing. Ben et al. [5] also used the FNN to test Advanced Driver\nAssistance Systems (ADAS). They leveraged a multi-objective search to guide testing towards the most critical\nbehaviors of ADAS. Pan et al. [96] present Q-testing, a reinforcement learning-based approach, benefiting from\nboth random and model-based approaches to automated testing of Android applications. Mao et al. [83] performed\nan extensive study on the effectiveness and efficiency of the promising PMT technique. They also complemented the\noriginal PMT work by considering more features and the powerful deep learning models to speed up this process of\ngenerating the huge number of mutants. Godefroid et al. [30] used DL-based statistical machine-learning techniques\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:24\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nto automatically generate input syntax suitable for input fuzzing. Cummins et al. [17] introduced DeepSmith, a\nnovel LSTM-based approach, for reducing the development task when using Fuzzers to discover bugs in compilers.\nThey accelerated compiler validation through the inference of generative models for compiler inputs, and then\napplied DeepSmith to automatically generate tens of thousands of realistic programs. Finally, they constructed\ndifferential testing methodologies on these generated programs for exposing bugs in compilers.\n7.5.8\nReverse execution. A decompiler is a tool to reverse the compilation process for examining binaries.\nLacomis et al. [56] introduced a novel probabilistic technique, namely Decompiled Identifier Renaming Engine\n(DIRE), which utilized both lexical and structural information recovered by the decompiler for variable name\nrecovery. They also present a technique for generating corpora suitable for training and evaluating models of\ndecompiled code renaming. Although reverse execution is an effective method to diagnose the root cause of\nsoftware crashes, some inherent challenges may influence its performance. To address this issue, Mu et al. [89]\npresent a novel DNN, which significantly increased the burden of doing hypothesis testing to track down non-alias\nrelation in binary code and improved memory alias resolution. To achieve this, they first employed an RNN to learn\nthe binary code pattern pertaining to memory access and then inferred the memory region accessed by memory\nreferences. Katz et al. [51] noticed that the source code generated by decompilation techniques are difficult for\ndevelopers to read and understand. To narrow the differences between human-written code and decompiled code,\nthey trained a non-language-specific RNN model to learn properties and patterns in source code for decompiling\nbinary code.\n7.6\nSoftware maintenance\nThere are a lot of studies contributing to increasing maintenance efficiency, such as improving source code, logging\ninformation, software energy consumption, etc. [36, 42, 75, 80, 103].\n7.6.1\nCode clone detection. Code clone detection is a very popular SE task in software maintenance using DL,\nwith around 20% of primary studies concentrating on this research topic.\nRNN-based code clone detection. Most studies use RNNs including RtNN [27], RvNN [125], and LSTM\n[9, 97, 112] to identify clones in source code. White et al. [125] proposed a novel code clone detector by combining\ntwo different RNNs, i.e., RtNN and RvNN, for automatically linking patterns mined at the lexical level with patterns\nmined at the syntactic level. They evaluated their DL-based approach based on file- and function-level. Gao et al.\n[27] first transformed source code into AST by parsing programs and then adopted a skip-gram language model to\ngenerate vector representation of ASTs. After that, they used the standard RNN model to find code clones from\njava projects. Buch et al. [9] introduced a tree-based code clone detection approach, and traversed ASTs to form\ndata sequences as the input of LSTM. Perez et al. [97] also used LSTM to learn from ASTs, and then calculated the\nsimilarities between ASTs written in Java and Python for identifying cross-language clones. Since source code\ncan be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and\nBytecode, Tufano et al. [112] conducted a series of experiments to demonstrate how DL can automatically learn\ncode similarities from different representations.\nFNN-based code clone detection. Some studies adopted FNNs for the code clone detection task [64, 90, 141].\nLi et al. [64] implemented a DL-based classifier, CClearner, for detecting function-level code clones by training an\nFNN. Compared with the approaches not using DL, CClearner achieved competitive clone detection effectiveness\nwith a low time cost. Zhao et al. [141] introduced a novel clone detection approach, which encoded data flow and\ncontrol flow and into a semantic matrix and designed an FNN structure to measure the functional similarity between\nsemantic representation of each code segment. Nafi et al. [90] proposed a cross-language clone detector without\nextensive processing of the source code and without the need to generate an intermediate representation. They\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:25\ntrained an FNN model, which can learn different syntactic features of source code across programming languages\nand identified clones by comparing the similarity of features.\nOthers. For detecting Type-4 code clones, Yu et al. [135] present a new approach that uses tree-based convolution\nto detect semantic clones, by capturing both the structural information of a code fragment from its AST and lexical\ninformation from code tokens. They also addressed the limitation of an unlimited vocabulary of tokens and models.\nWang et al. [120] developed a novel graph-based program representation method, flow-augmented abstract syntax\ntree (FA-AST), to better capture and leverage control and data flow information. FA-AST augmented original\nASTs with explicit control and data flow edges and then adopted two different GNN models (i.e., gated graph\nneural network (GGNN) and graph matching network (GMN)) to measure the similarity of various code pairs.\nTo effectively capture syntax and semantic information from programs to detect semantic clones, Fang et al. [25]\nadopted fusion embedding techniques to learn hidden syntactic and semantic features by building a novel joint\ncode representation. They also proposed a new granularity for functional code clone detection called caller-callee\nmethod relationships. Finally, they trained a supervised deep learning model to find semantic clones.\n7.6.2\nCode comment generation. Hu et al. [44] present a new approach that can automatically generate code\ncomments for Java code to help developers better understand the functionality of code segments. They trained a\nLSTM-based framework to learn the program structure for better comments generation. The context information of\nthe source code was not used and analyzed in previous automated comment summarization techniques. Ciurumelea\net al. [16] proposed a semi-automated system to generate code comments by using LSTM. Zhou et al. [148]\ncombined program analysis and natural language processing to build a Dl-based seq2seq model to generate Java\ncode comments. To generate code summarization, Leclair et al. [60] proposed a DL-based model combining texts\nfrom code with code structure from an AST. They processed data source as a separate input to reduce the entire\ndependence on internal documentation of code. Wan et al. [116] noticed that most of the previous work used the\nEncoder-Decoder architecture to generate code summaries, which omitted the tree structure of source code and\nintroduced some bias when decoding code sequences. To solve these problems, they trained a deep reinforcement\nlearning framework that incorporated an abstract syntax tree structure as well as sequential content of code snippets.\nThey trained this DNN by adopting an advantage reward composed of BLEU metric.\n7.6.3\nProgram repair. Bhatia et al. [6] proposed a novel neuro-symbolic approach combining DL techniques\nwith constraint-based reasoning for automatically correcting programs with errors. Specifically, they trained an RNN\nmodel to perform syntax repairs for the buggy programs and ensured functional correctness by using constraint-\nbased techniques. Through evaluation, their approach was able to repair syntax errors in 60% of submissions\nand identified functionally correct repairs for 24% submissions. Tufano et al. [113] proposed to leverage the\nproliferation of software development histories to fix common programming bugs. They used the Encoder-Decoder\nframework to translate buggy code into its fixed version after generating the abstract representation of buggy\nprograms and their fixed code. White et al. [124] trained an autoencoder framework to reason about the repair\ningredients (i.e., the code reused to craft a patch). They prioritized and transformed suspicious statements and\nelements in the code repository for patch generation by calculating code similarities. Lutellier et al. [79] present\na new automated generate-and-validate program repair approach, CoCoNuT, which trained multiple models to\nextract hierarchical features and model source code at different granularity levels (e.g., statement and function\nlevel) and then constructed a CNN model to fix different program bugs. Liu et al. [71] proposed an automated\napproach for detecting and refactoring inconsistent method names by using Paragraph Vector and a CNN. Ni et al.\n[93] exploited the bug fixes of historical bugs to classify bugs into their cause categories based on the intuition that\nhistorical information may reflect the bug causes. They first defined the code-related bug classification criterion\nfrom the perspective of the cause of bugs and generated ASTs from diff source code to construct fixed trees. Then,\nthey trained Tree-based Convolutional Neural Network (TBCNN) to represent each fixed tree and classified bugs\ninto their cause categories according to the relationship between bug fixes and bug causes.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:26\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\n7.6.4\nSource code representation. White et al. [126] conducted an empirical study to adopt DL in software\nlanguage modeling and highlight the fundamental differences between state-of-the-practice software language\nmodels and DL models. Their intuition is that the representation power of the abstractions is the key element of\nimproving the quality of software language models. Therefore, the goal of this study was to improve the quality of\nthe underlying abstractions by using Neural Network Language Models (i.e., Feed-forward neural networks (FNN),\nRNN) for numerous SE issues. They pinpointed that DL had a strong capability to model semantics and consider\nrich contexts, allowing it performed better at source code modeling. They evaluated these DL-based language\nmodels at a real SE task (i.e., code suggestion). Their evaluation results suggested that their model significantly\noutperformed the traditional language models on 16,221 Java projects.\nHussain et al. [49] introduced a gated recurrent unit-based model, namely CodeGRU, to model source code by\ncapturing its contextual, syntactical, and structural dependencies. The key innovation of their approach was to\nperforming simple program analysis for capturing the source code context and further employed GRU to learn\nvariable size context while modeling source code. They evaluated CodeGRU on several open-source java projects,\nand the experimental results verified that the approach alleviated the out of vocabulary issue. Using abstract\nsyntax tree (AST)-based DNNs may induce a long-term dependency problem due to the large size of ASTs. To\naddress this problem, Zhang et al. [137] present an advanced AST-based Neural Network (ASTNN) for source\ncode representation. The advanced ASTNN cut each entire AST into a set of small statement trees, and transform\nthese subtrees into vectors by capturing the lexical and syntactical knowledge of statement trees. They applied a\nbidirectional RNN model to produce the vector representation of a code snippet. They used ASTNN to detect code\nclones and classify source code for evaluating its performance. Gill et al. [29] introduced a lightweight framework,\nThermoSim, to simulate the thermal behavior of computing nodes in the Cloud Data Center (CDC) and measure the\neffect of temperatures on key performance parameters. They extended the previous framework, i.e., the CloudSim\ntoolkit by presenting an RNN-based temperature predictor for helping to analyze the performance of some key\nparameters. The final results demonstrated that ThermoSim accurately modeled and simulated the thermal behavior\nof a CDC in terms of energy consumption, time, cost, and memory usage.\n7.6.5\nCode classification. Bui et al. [22] described a framework of Bi-NN that built a neural network on top\nof two underlying sub-networks, each of which encoded syntax and semantics of code in a language. Bi-NN was\ntrained with bilateral programs that implement the same algorithms and/or data structures in different languages and\nthen be applied to recognize algorithm classes across languages. Software categorization is the task of organizing\nsoftware into groups that broadly describe the behavior of the software. However, previous studies suffered very\nlarge performance penalties when classifying source code and code comments only. Leclair et al. [59] proposed a\nset of adaptations to a state-of-the-art neural classification algorithm and conducted two evaluations.\n7.6.6\nCode smell detection. Fakhoury et al. [24] reported their experience in building an automatic linguistic\nanti-pattern detection using DNNs. They trained several traditional machine learning and DNNs to identify linguistic\nanti-patterns. A big challenge for DL-based code smell detection is the lack of a large number of labeled datasets,\nand thus Liu et al. [68] present a DL-based approach to automatically generating labeled training data for DL-based\nclassifiers. They applied their approach to detecting four common and well-known code smells, i.e., feature envy,\nlong method, large class, and misplaced class.\n7.6.7\nSelf-Admitted Technical Debt (SATD) detection. Technical debt (TD) is a metaphor to reflect the\ntradeoff developers make between short term benefits and long term stability. Self-admitted technical debt (SATD),\na variant of TD, has been proposed to identify debt that is intentionally introduced during SDLC. Ren et al.\n[102] proposed a CNN-based approach to determine code comments as SATD or non-SATD. They exploited the\ncomputational structure of CNNs to identify key phrases and patterns in code comments that are most relevant\nto SATD for improving the explainability of our model’s prediction results. Zampetti et al. [136] proposed to\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:27\nautomatically recommend SATD removal strategies by building a multi-level classifier on a curated dataset of\nSATD removal patterns. Their strategy was capable of recommending six SATD removal patterns, i.e., changing\nAPI calls, conditionals, method signatures, exception handling, return statements, or telling that a more complex\nchange is needed.\n7.6.8\nCode review. Siow et al. [106] believed that the hinge of the accurate code review suggestion is to\nlearn good representations for both code changes and reviews. Therefore, they designed a multi-level embedding\nframework to represent the semantics provided by code changes and reviews and then well trained through an\nattention-based deep learning model, CORE. Guo et al. [34] proposed Deep Review Sharing, a new technique\nbased on code clone detection for accurate review sharing among similar software projects, and optimized their\ntechnique by a series of operations such as heuristic filtering and review deduplication. They evaluated Deep Review\nSharing on hundreds of real code segments and it won considerable positive approvals by experts, illustrating its\neffectiveness.\n7.6.9\nSoftware quality evaluation. Variety evaluation metrics can be used to describe the quality of software\nproducts [101].\nSoftware trustworthiness. It is essential and necessary to evaluate software trustworthiness based on the\ninfluence degrees of different software behaviors for minimizing the interference of human factors. Tian et al. [109]\nconstructed behaviour trajectory matrices to represent the behaviour trajectory and then trained the deep residual\nnetwork (ResNet) as a software trustworthiness evaluation model to classify the current software behaviors. After\nthat, they used the cosine similarity algorithm to calculate the deviation degree of the software behavior trajectory.\nReadability. Mi et al. [86] proposed to leverage CNN to improve code readability classification. First, they\npresent a transformation strategy to generate integer matrices as the input of ConvNets. Then they trained Deep\nCRM, a DL-based model, which was made up of three separate ConvNets with identical architectures for code\nreadability classification.\nMaintainability. Kumar et al. [55] performed two case studies and applied three DNNs i.e., FLANN-Genetic\n(FGA and AFGA), FLANN-PSO (FPSO and MFPSO), FLANN-CSA (FCSA), to design a model for predicting\nmaintainability. They also evaluated the effectiveness of feature reduction techniques for predicting maintainability.\nThe experimental result showed that feature reduction techniques can achieve better results compared with using\nDNNs.\n7.7\nSoftware management\n7.7.1\nEffort estimation. Since only 39% of software projects are finished and published on time relative to the\nduration planned originally [7, 78], it is necessary to assess the development cost to achieve reliable software within\ndevelopment schedule and budget. Lopez et al. [78] compared three different neural network models for effort\nestimation. The experimental result demonstrated that MLP and RBFNN can achieve higher accuracy than the\nMLR model. Choetkiertiku et al. [15] observed that few studies focused on estimating effort cost in agile projects,\nand thus they proposed a DL-based model for predicting development cost based on combining two powerful DL\ntechniques: LSTM and recurrent highway network (RHN). Phannachitta et al. [99] conducted an empirical study to\nrevisit the systematic comparison of heuristics-based and learning-based software effort estimators on 13 standard\nbenchmark datasets. Ochodek et al.[94] employed several DNNs (i.e., CNN, RNN, Convolutional + Recurrent\nNeural Network (CRNN)) to design a novel prediction model, and compared the performance of the DL-based\nmodel with three state-of-the-art approaches: AUC, AUCG, and BN-UCGAIN. They noticed that CNN obtained\nthe best prediction accuracy among all software effort adaptors.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:28\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\n7.7.2\nSoftware repository mining. Some primary studies use DL techniques to mine the contents in different\nsoftware repositories [35, 58, 81]. In this section, we introduce three most widely used repositories, i.e., Stack\nOverflow (SO) [70, 146], GitHub [50], and Youtube [95].\nMining Stack Overflow (SO). The questions and answers in SO contain a great deal of useful information that\nis beneficial for programmers to address some tough problems when implementing software products. Considering\na question and its answers in Stack Overflow as a knowledge unit, Xu et al. [131, 132] extracted the knowledge units\nand analyzed the potential semantic relationship between Q and A in each unit. They formulated the problem of\npredicting semantically linkable knowledge units as a multi-class classification problem and adopted a CNN model\ncombining with word-embedding to capture and classify document-level semantics of knowledge units. Chen et al.\n[13] also applied word embeddings and the CNN model to mine SO for retrieving cross-lingual questions. They\ncompared their approach with other translation-based methods, and the final results showed that their approach can\nsignificantly outperform baselines and can also be extended to dual-language document retrieval from different\nsources. Yin et al. [134] proposed a new approach to pair the title of a question with the code in the accepted answer\nby mining high-quality aligned data from SO using two sets of features. To validate whether DL was the best\nsolution in all research topics, Menzies et al. [85] conducted a case study to explore faster approaches for text mining\nSO. They compared nine different solutions including traditional machine learning algorithms and DL algorithms\nand noticed that a tuned SVM performs similarly to a deep learner and was over 500 times faster than DL-based\nmodels. Zhang et al. [138] performed an empirical study to mine the posts in SO for investigating what potential\nchallenges developers face when using DL. They also built a classification model to quantify the distribution of\ndifferent Sort of DL related questions. They summarized the three most frequently asked questions and provided\nfive future research directions. Since answers to a question may contain extensive irrelevant information, Wang\net al. [119] proposed a DL-based approach, DeepTip, using different CNN architectures to extract short practical\nand useful tips and filtered useless information from developer answers. They conducted a user study to prove the\neffectiveness of their approach and the extensive empirical experiments demonstrated that DeepTip can extract\nuseful information from answers with high precision and coverage, significantly outperforming two state-of-the-art\napproaches.\nMining GitHub. Huang et al. [46] proposed a new model to classify sentences from issue reports of four projects\nin GitHub. They constructed a dataset collecting 5,408 sentences and refined previous categories (i.e., feature\nrequest, problem discovery, information seeking, information giving, and others). They then trained a CNN-based\nmodel to automatically classify sentences into different categories of intentions and used the batch normalization\nand automatic hyperparameter tuning approach to optimize their model. Xie et al. [130] proposed a new approach\nto recover issue-commit links. They constructed the code knowledge graph of a code repository and captured\nthe semantics of issue- or commit-related text by generating embeddings of source code files. Then they trained\na DNN model to calculate code similarity and semantic similarity using additional features. Ruan et al.[104]\npropose a novel semantically-enhanced link recovery method, DeepLink, using DL techniques. They applied word\nembedding and RNN to implement a DNN architecture to learn the semantic representation of code and texts as\nwell as the semantic correlation between issues and commits. They compared DeepLink with the state-of-the-art\napproach on 10 GitHub Java projects to evaluate the effectiveness of DeepLink. Liu et al. [76] proposed a DL-based\napproach to automatically generate pull request descriptions based on the commit messages and the added source\ncode comments in pull requests. They formulated this problem as a text summarization problem and solved it,\nconstructing an attentional encoder-decoder model with a pointer generator.\nMining Youtube. Ott et al. [95] employed CNN and AutoEncoder to identify Java code from videos on Youtube.\nTheir approach was able to identify the presence of typeset and handwritten source code in thousands of video\nimages with 85.6%-98.6% accuracy based on syntactic and contextual features learned through DL techniques.\nZhao et al. [140] present a new technique for recognizing workflow actions in programming screencasts. They\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:29\ncollected programming screencasts from Youtube and trained a CNN model to identify nine classes of frequent\ndeveloper actions by employing the image differencing technique and training a CNN model.\nSummary\n(1) We grouped six SE activities based on the body knowledge of SE – Software requirements, Software design\nand modeling, Software implementation, Software testing and debugging, Software maintenance, and Software\nmanagement – and provided an outline of the application trend of DL techniques among these SE activities.\n(2) We summarized various SE tasks into four categories – regression task, classification task, ranking task, and\ngeneration task – and classified all primary studies based on the task types. Most studies can be mapped to\nclassification tasks, and only 11% of primary studies are mapped to ranking tasks.\n(3) Software testing and software maintenance are the two SE activities containing the most related studies and\ninclude 17 specific SE research topics in which DL techniques were used.\n8\nLIMITATIONS\nData Extraction. There are several potential limitations to our work. One limitation is the potential bias in data\ncollection. Although we have listed the data items used for analysis in RQs in Section 3.4, some disagreements still\nappeared inevitably when extracting related content and classifying these data items. Two researchers first recorded\nthe relevant descriptions in each primary study and then discussed and generalized temporary categories based on\nall data in one item by comparing the objectives and contributions with related studies. If they were unable to reach\nan agreement on classification, another researcher would join in and resolve differences, which can mitigate the\nbias in data extraction to study validity.\nStudy Selection Bias. Another threat might be the possibility of missing DL related studies during the literature\nsearch and selection phase. We are unable to identify and retrieve all relevant publications considering the many\npublication venues publishing SE relevant papers. Therefore, we carefully selected 21 publication venues, including\nconference proceedings, symposiums, and journals, which can cover many prior works in SE. Besides, we identified\nour search terms and formed a search string by combining and amending the search strings used in other literature\nreviews on DL. These could keep the number of missed primary studies as small as possible.\n9\nCHALLENGES AND OPPORTUNITIES\nUsing DL in more SE activities. We found that DL has been widely used in certain SE topics, such as defect\nprediction, code clone detection, software repository mining, etc. However, few studies used DL for some SE\nresearch topics compared with other techniques or other learning algorithms. Although Software requirements and\nsoftware design are the most two important documentations during SDLC, not many studies focus on these two\nSE activities. Therefore, one potential opportunity is that researchers can utilize DL techniques to explore new\nresearch topics or pay attention to classical topics in software requirements and design.\nThe transparency of DL. In this study, we discussed 142 studies that used DL to address various SE issues. We\nnoticed that few studies declared the reason for the architecture they chose and explained the necessity and value\nof each layer in DNN, which leads to low transparency of the proposed DL solutions. Because it is inherently\ndifficult to comprehend what drives the decisions of the DNN due to the black-box nature of DL. Humans only pay\nattention to the output of DNNs since they can provide wise and actionable suggestions for humans. Furthermore,\nDL algorithms sift through millions of data points to find patterns and correlations that often go unnoticed by\nhuman experts. The decision they make based on these findings often confound even the engineers who created\nthem. New methods and studies on explaining the decision-making process of DNNs should be an active research\ndirection, which facilitates software engineers to design more effective DNN architectures for specific SE problems.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:30\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\nDL in real scenarios. We analyzed the source of datasets used for training DNNs in RQ3 and noticed that only\n4% studies used industry datasets to train and evaluate their proposed models. In fact, most studies contribute to\naddressing real-world SE issues, but the novel solutions or DL-based models have not been evaluated on industry\ndata. There is a room for more industry-academia collaboration so that the proposed models can be validated on\nreal industry data (which can be of very different nature than open-source data.\nData hungry. When analyzing the studies related to code clone detection, we found that several open-source public\ndata sets are often used repeatedly in these studies to evaluate their proposed models. A similar situation also\nexists in other research topics. These highlight the dependence on some studies on large publicly available labelled\ndatasets. One reason is that training a DNN requires a massive volume of data, but copious amounts of training data\nare rarely available in most SE tasks. Besides, it is impossible to give every possible labeled sample of a problem\nspace to a DL algorithm. Therefore, it will have to generalize or interpolate between its previous samples in order\nto classify data it has never seen before. It is a challenge to tackle the problem that DL techniques currently lack a\nmechanism for learning abstractions through explicit, verbal definition and only work best with thousands, millions,\nor even billions of training examples. One solution is to construct widely accepted datasets by using industrial\nlabeled data or crawling software repositories to collect related data samples and label them as public datasets.\nAnother is to develop new DL techniques, which can learn how to learn and be trained as an effective model with\nas little data size as possible, such as meta-learning.\nPerformance of DL and traditional techniques. DL has been gradually used in more and more SE tasks, replacing\nthe status of traditional algorithms. However, are DL algorithms really more efficient than traditional algorithms?\nWhat SE tasks are suitable for DL algorithms? What factors determine whether DL algorithms are better or worse\nthan traditional algorithms? These questions are almost unanswered and neglected by most researchers. A potential\nopportunity is to answer these questions. researchers can conduct empirical studies to investigate what SE tasks or\nenvironments are suitable for DL and compare the performance between DL and traditional techniques in some\nimportant SE research topics where most of Dl algorithms were applied.\n10\nCONCLUSION\nThis work performed a SLR on 142 primary studies related to DL for SE from 21 publication venues, including\nconference proceedings, symposiums, and journals. We established four research questions to comprehensively\ninvestigate various aspects pertaining to applications of DL models to SE tasks. Our SLR showed that there was a\nrapid growth of research interest in the use of DL for SE. Through an elaborated investigation and analysis, three\nDL architectures containing 30 different DNNs were used in primary studies, where RNN, CNN, and FNN are the\nthree most widely used neural networks compared with other DNNs. We also generalized three different model\nselection strategies and analyzed the popularity of each one. To comprehensively understand the DNN training and\ntesting process, we provided a detailed overview of key techniques in terms of data collection, data processing,\nmodel optimization, and model evaluation in RQ3. In RQ4, we analyzed the distribution of DL techniques used\nin different SE activities, classified primary studies according to specific SE tasks they solved and gave a brief\nsummary of each work. We observed that DL techniques were applied in 23 SE topics, covering 6 SE activities.\nFinally, we identified a set of current challenges that still need to be addressed in future work on using DLs in SE.\nREFERENCES\n[1] Aysh Al-Hroob, Ayad Tareq Imam, and Rawan Al-Heisa. 2018. The use of artificial neural networks for extracting actions and actors\nfrom requirements document. IST 101 (2018), 1–15.\n[2] Mohammad Alahmadi, Abdulkarim Khormi, Biswas Parajuli, Jonathan Hassel, Sonia Haiduc, and Piyush Kumar. 2020. Code Localization\nin Programming Screencasts. ESE 25, 2 (2020), 1536–1572.\n[3] Lingfeng Bao, Zhenchang Xing, Xin Xia, David Lo, Minghui Wu, and Xiaohu Yang. 2020. psc2code: Denoising Code Extraction from\nProgramming Screencasts. TOSEM 29, 3 (2020), 1–38.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:31\n[4] Antoine Barbez, Foutse Khomh, and Yann-Gaël Guéhéneuc. 2019. Deep Learning Anti-patterns from Code Metrics History. In ICSME.\nIEEE, 114–124.\n[5] Raja Ben Abdessalem, Shiva Nejati, Lionel C Briand, and Thomas Stifter. 2016. Testing advanced driver assistance systems using\nmulti-objective search and neural networks. In ASE. 63–74.\n[6] Sahil Bhatia, Pushmeet Kohli, and Rishabh Singh. 2018. Neuro-symbolic program corrector for introductory programming assignments.\nIn ICSE. IEEE, 60–70.\n[7] Manjubala Bisi and Neeraj Kumar Goyal. 2016. Software development efforts prediction using artificial neural network. IETS 10, 3\n(2016), 63–71.\n[8] Pierre Bourque, Richard E Fairley, et al. 2014. Guide to the software engineering body of knowledge (SWEBOK (R)): Version 3.0. IEEE\nComputer Society Press.\n[9] Lutz Büch and Artur Andrzejak. 2019. Learning-based recursive aggregation of abstract syntax trees for code clone detection. In SANER.\nIEEE, 95–104.\n[10] Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When deep learning met code search. In FSE.\n964–974.\n[11] Chao Chen, Wenrui Diao, Yingpei Zeng, Shanqing Guo, and Chengyu Hu. 2018. DRLgencert: Deep learning-based automated testing of\ncertificate verification in SSL/TLS implementations. In ICSME. IEEE, 48–58.\n[12] Chunyang Chen, Ting Su, Guozhu Meng, Zhenchang Xing, and Yang Liu. 2018. From ui design image to gui skeleton: a neural machine\ntranslator to bootstrap mobile gui implementation. In ICSE. 665–676.\n[13] Guibin Chen, Chunyang Chen, Zhenchang Xing, and Bowen Xu. 2016. Learning a dual-language vector space for domain-specific\ncross-lingual question retrieval. In ASE. IEEE, 744–755.\n[14] Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xin Xia, Liming Zhu, John Grundy, and Jinshui Wang. 2020. Wireframe-based UI\ndesign search through image autoencoder. TOSEM 29, 3 (2020), 1–31.\n[15] Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya Ghose, and Tim Menzies. 2018. A deep learning model for\nestimating story points. TSE 45, 7 (2018), 637–656.\n[16] Adelina Ciurumelea, Sebastian Proksch, and Harald C Gall. 2020. Suggesting Comment Completions for Python using Neural Language\nModels. In SANER. IEEE, 456–467.\n[17] Chris Cummins, Pavlos Petoumenos, Alastair Murray, and Hugh Leather. 2018. Compiler fuzzing through deep learning. In ISSTA.\n95–105.\n[18] Hoa Khanh Dam, Trang Pham, Shien Wee Ng, Truyen Tran, John Grundy, Aditya Ghose, Taeksu Kim, and Chul-Joo Kim. 2019. Lessons\nlearned from using a deep tree-based model for software defect prediction in practice. In MSR. IEEE, 46–57.\n[19] Hoa Khanh Dam, Truyen Tran, Trang Thi Minh Pham, Shien Wee Ng, John Grundy, and Aditya Ghose. 2018. Automatic feature learning\nfor predicting vulnerable software components. TSE (2018).\n[20] Li Deng. 2014. A tutorial survey of architectures, algorithms, and applications for deep learning. APSIPA Transactions on Signal and\nInformation Processing 3 (2014).\n[21] Jayati Deshmukh, KM Annervaz, Sanjay Podder, Shubhashis Sengupta, and Neville Dubash. 2017. Towards accurate duplicate bug\nretrieval using deep learning techniques. In ICSME. IEEE, 115–124.\n[22] Bui Nghi DQ, Yijun Yu, and Lingxiao Jiang. 2019. Bilateral dependency neural networks for cross-language algorithm classification. In\nSANER. IEEE, 422–433.\n[23] Hasan Ferit Eni¸ser and Alper Sen. 2020. Virtualization of stateful services via machine learning. SQJ 28, 1 (2020), 283–306.\n[24] Sarah Fakhoury, Venera Arnaoudova, Cedric Noiseux, Foutse Khomh, and Giuliano Antoniol. 2018. Keep it simple: Is deep learning\ngood for linguistic smell detection?. In SANER. IEEE, 602–611.\n[25] Chunrong Fang, Zixi Liu, Yangyang Shi, Jeff Huang, and Qingkai Shi. 2020. Functional code clone detection with syntax and semantics\nfusion learning. In ISSTA. 516–527.\n[26] Sa Gao, Chunyang Chen, Zhenchang Xing, Yukun Ma, Wen Song, and Shang-Wei Lin. 2019. A neural model for method name generation\nfrom functional description. In SANER. IEEE, 414–421.\n[27] Yi Gao, Zan Wang, Shuang Liu, Lin Yang, Wei Sang, and Yuanfang Cai. 2019. TECCD: A Tree Embedding Approach for Code Clone\nDetection. In ICSME. IEEE, 145–156.\n[28] Necmiye Genc-Nayebi and Alain Abran. 2017. A systematic literature review: Opinion mining studies from mobile app store user\nreviews. JSS 125 (2017), 207–219.\n[29] Sukhpal Singh Gill, Shreshth Tuli, Adel Nadjaran Toosi, Felix Cuadrado, Peter Garraghan, Rami Bahsoon, Hanan Lutfiyya, Rizos\nSakellariou, Omer Rana, Schahram Dustdar, et al. 2020. ThermoSim: Deep learning based framework for modeling and simulation of\nthermal-aware resource management for cloud computing environments. JSS (2020), 110596.\n[30] Patrice Godefroid, Hila Peleg, and Rishabh Singh. 2017. Learn&fuzz: Machine learning for input fuzzing. In ASE. IEEE, 50–59.\n[31] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT press.\n[32] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In ICSE. IEEE, 933–944.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:32\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\n[33] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep API learning. In FSE. 631–642.\n[34] Chenkai Guo, Dengrong Huang, Naipeng Dong, Quanqi Ye, Jing Xu, Yaqing Fan, Hui Yang, and Yifan Xu. 2019. Deep review sharing.\nIn SANER. IEEE, 61–72.\n[35] Chenkai Guo, Weijing Wang, Yanfeng Wu, Naipeng Dong, Quanqi Ye, Jing Xu, and Sen Zhang. 2019. Systematic comprehension for\ndeveloper reply in mobile system forum. In SANER. IEEE, 242–252.\n[36] Huong Ha and Hongyu Zhang. 2019. Deepperf: performance prediction for configurable software with deep sparse neural network. In\nICSE. IEEE, 1095–1106.\n[37] Zhuobing Han, Xiaohong Li, Zhenchang Xing, Hongtao Liu, and Zhiyong Feng. 2017. Learning to predict severity of software\nvulnerability using only vulnerability description. In ICSME. IEEE, 125–136.\n[38] Douglas M Hawkins. 2004. The problem of overfitting. Journal of chemical information and computer sciences 44, 1 (2004), 1–12.\n[39] Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis. 2018. Deep learning type inference. In FSE. 152–162.\n[40] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation 18, 7\n(2006), 1527–1554.\n[41] Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu Ubayashi. 2019. DeepJIT: an end-to-end deep learning\nframework for just-in-time defect prediction. In MSR. IEEE, 34–45.\n[42] Thong Hoang, Julia Lawall, Yuan Tian, Richard J Oentaryo, and David Lo. 2019. PatchNet: Hierarchical Deep Learning-Based Stable\nPatch Identification for the Linux Kernel. TSE (2019).\n[43] Seyedrebvar Hosseini, Burak Turhan, and Dimuthu Gunarathna. 2017. A systematic literature review and meta-analysis on cross project\ndefect prediction. TSE 45, 2 (2017), 111–147.\n[44] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In ICPC. IEEE, 200–20010.\n[45] Qing Huang, An Qiu, Maosheng Zhong, and Yuan Wang. 2020. A Code-Description Representation Learning Model Based on Attention.\nIn SANER. IEEE, 447–455.\n[46] Qiao Huang, Xin Xia, David Lo, and Gail C Murphy. 2018. Automating intention mining. TSE (2018).\n[47] Rubing Huang, Weifeng Sun, Yinyin Xu, Haibo Chen, Dave Towey, and Xin Xia. 2019. A survey on adaptive random testing. TSE\n(2019).\n[48] Xuan Huo, Ferdian Thung, Ming Li, David Lo, and Shu-Ting Shi. 2019. Deep transfer bug localization. TSE (2019).\n[49] Yasir Hussain, Zhiqiu Huang, Yu Zhou, and Senzhang Wang. 2020. CodeGRU: Context-aware deep learning with gated recurrent unit for\nsource code modeling. IST (2020), 106309.\n[50] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generating commit messages from diffs using neural machine\ntranslation. In ASE. IEEE, 135–146.\n[51] Deborah S Katz, Jason Ruchti, and Eric Schulte. 2018. Using recurrent neural networks for decompilation. In SANER. IEEE, 346–356.\n[52] Staffs Keele et al. 2007. Guidelines for performing systematic literature reviews in software engineering. Technical Report. Technical\nreport, Ver. 2.3 EBSE Technical Report. EBSE.\n[53] Ugur Koc, Shiyi Wei, Jeffrey S Foster, Marine Carpuat, and Adam A Porter. 2019. An empirical assessment of machine learning\napproaches for triaging reports of a java static analysis tool. In ICST. IEEE, 288–299.\n[54] Jinkyu Koo, Charitha Saumya, Milind Kulkarni, and Saurabh Bagchi. 2019. Pyse: Automatic worst-case test generation by reinforcement\nlearning. In ICST. IEEE, 136–147.\n[55] Lov Kumar and Santanu Ku Rath. 2016. Hybrid functional link artificial neural network approach for predicting maintainability of\nobject-oriented software. JSS 121 (2016), 170–190.\n[56] Jeremy Lacomis, Pengcheng Yin, Edward Schwartz, Miltiadis Allamanis, Claire Le Goues, Graham Neubig, and Bogdan Vasilescu.\n2019. Dire: A neural approach to decompiled identifier naming. In ASE. IEEE, 628–639.\n[57] An Ngoc Lam, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen. 2017. Bug localization with combination of deep learning\nand information retrieval. In ICPC. IEEE, 218–229.\n[58] Tien-Duy B Le and David Lo. 2018. Deep specification mining. In Proceedings of the 27th ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis. 106–117.\n[59] Alexander LeClair, Zachary Eberhart, and Collin McMillan. 2018. Adapting neural text classification for improved software categorization.\nIn ICSME. IEEE, 461–472.\n[60] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for generating natural language summaries of program\nsubroutines. In ICSE. IEEE, 795–806.\n[61] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436–444.\n[62] Seongmin Lee, Shin Hong, Jungbae Yi, Taeksu Kim, Chul-Joo Kim, and Shin Yoo. 2019. Classifying false positive static checker alarms\nin continuous integration using convolutional neural networks. In ICST. IEEE, 391–401.\n[63] Guangjie Li, Hui Liu, Jiahao Jin, and Qasim Umer. 2020. Deep Learning Based Identification of Suspicious Return Statements. In\nSANER. IEEE, 480–491.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:33\n[64] Liuqing Li, He Feng, Wenjie Zhuang, Na Meng, and Barbara Ryder. 2017. Cclearner: A deep learning-based clone detection approach.\nIn ICSME. IEEE, 249–260.\n[65] Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. 2019. Deepfl: Integrating multiple fault diagnosis dimensions for deep fault\nlocalization. In ISSTA. 169–180.\n[66] Yangguang Li, Zhen Ming Jiang, Heng Li, Ahmed E Hassan, Cheng He, Ruirui Huang, Zhengda Zeng, Mian Wang, and Pinan Chen.\n2020. Predicting Node Failures in an Ultra-large-scale Cloud Computing Platform: an AIOps Solution. TOSEM 29, 2 (2020), 1–24.\n[67] Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John Grundy, and Xiaohu Yang. 2020. On the Replicability and Reproducibility of Deep\nLearning in Software Engineering. arXiv preprint arXiv:2006.14244 (2020).\n[68] Hui Liu, Jiahao Jin, Zhifeng Xu, Yifan Bu, Yanzhen Zou, and Lu Zhang. 2019. Deep learning based code smell detection. TSE (2019).\n[69] Hui Liu, Zhifeng Xu, and Yanzhen Zou. 2018. Deep learning based feature envy detection. In ASE. 385–396.\n[70] Jin Liu, Pingyi Zhou, Zijiang Yang, Xiao Liu, and John Grundy. 2018. FastTagRec: fast tag recommendation for software information\nsites. ASEJ 25, 4 (2018), 675–701.\n[71] Kui Liu, Dongsun Kim, Tegawendé F Bissyandé, Taeyoung Kim, Kisub Kim, Anil Koyuncu, Suntae Kim, and Yves Le Traon. 2019.\nLearning to spot and refactor inconsistent method names. In ICSE. IEEE, 1–12.\n[72] Muyang Liu, Ke Li, and Tao Chen. 2020. DeepSQLi: Deep Semantic Learning for Testing SQL Injection. arXiv preprint arXiv:2005.11728\n(2020).\n[73] Peng Liu, Xiangyu Zhang, Marco Pistoia, Yunhui Zheng, Manoel Marques, and Lingfei Zeng. 2017. Automatic text input generation for\nmobile testing. In ICSE. IEEE, 643–653.\n[74] Yibin Liu, Yanhui Li, Jianbo Guo, Yuming Zhou, and Baowen Xu. 2018. Connecting software metrics across versions to predict defects.\nIn SANER. IEEE, 232–243.\n[75] Zhongxin Liu, Xin Xia, David Lo, Zhenchang Xing, Ahmed E Hassan, and Shanping Li. 2019. Which variables should i log? TSE\n(2019).\n[76] Zhongxin Liu, Xin Xia, Christoph Treude, David Lo, and Shanping Li. 2019. Automatic generation of pull request descriptions. In ASE.\nIEEE, 176–188.\n[77] Zhongxin Liu, Xin Xia, Meng Yan, and Shanping Li. [n. d.]. Automating Just-In-Time Comment Updating. In ASE.\n[78] Cuauhtémoc López-Martín and Alain Abran. 2015. Neural networks for predicting the duration of new software projects. JSS 101 (2015),\n127–135.\n[79] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. 2020. CoCoNuT: combining context-aware\nneural translation models using ensemble for program repair. In ISSTA. 101–114.\n[80] Suyu Ma, Zhenchang Xing, Chunyang Chen, Cheng Chen, Lizhen Qu, and Guoqiang Li. 2019. Easy-to-Deploy API Extraction by\nMulti-Level Feature Embedding and Transfer Learning. TSE (2019).\n[81] Alvi Mahadi, Karan Tongay, and Neil A Ernst. 2020. Cross-Dataset Design Discussion Mining. In SANER. IEEE, 149–160.\n[82] Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. Nl2type: inferring javascript function types from natural language\ninformation. In ICSE. IEEE, 304–315.\n[83] Dongyu Mao, Lingchao Chen, and Lingming Zhang. 2019. An extensive study on cross-project predictive mutation testing. In ICST.\nIEEE, 160–171.\n[84] Warren S McCulloch and Walter Pitts. 1943. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical\nbiophysics 5, 4 (1943), 115–133.\n[85] Tim Menzies, Suvodeep Majumder, Nikhila Balaji, Katie Brey, and Wei Fu. 2018. 500+ times faster than deep learning:(a case study\nexploring faster methods for text mining stackoverflow). In MSR. IEEE, 554–563.\n[86] Qing Mi, Jacky Keung, Yan Xiao, Solomon Mensah, and Yujin Gao. 2018. Improving code readability classification using convolutional\nneural networks. IST 104 (2018), 60–71.\n[87] Abdel-rahman Mohamed, George Dahl, and Geoffrey Hinton. 2009. Deep belief networks for phone recognition. In Nips workshop on\ndeep learning for speech recognition and related applications, Vol. 1. Vancouver, Canada, 39.\n[88] Kevin Patrick Moran, Carlos Bernal-Cárdenas, Michael Curcio, Richard Bonett, and Denys Poshyvanyk. 2018. Machine learning-based\nprototyping of graphical user interfaces for mobile apps. TSE (2018).\n[89] Dongliang Mu, Wenbo Guo, Alejandro Cuevas, Yueqi Chen, Jinxuan Gai, Xinyu Xing, Bing Mao, and Chengyu Song. 2019. RENN:\nefficient reverse execution with neural-network-assisted alias analysis. In ASE. IEEE, 924–935.\n[90] Kawser Wazed Nafi, Tonny Shekha Kar, Banani Roy, Chanchal K Roy, and Kevin A Schneider. 2019. CLCDSA: cross language code\nclone detection using syntactical features and API documentation. In ASE. IEEE, 1026–1037.\n[91] Tapas Nayak and Hwee Tou Ng. 2020. Effective modeling of encoder-decoder architecture for joint entity and relation extraction. In\nProceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8528–8535.\n[92] Anh Tuan Nguyen, Trong Duc Nguyen, Hung Dang Phan, and Tien N Nguyen. 2018. A deep neural network language model with\ncontexts for source code. In SANER. IEEE, 323–334.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n1:34\n•\nYanming Yang, Xin Xia, David Lo, and John Grundy\n[93] Zhen Ni, Bin Li, Xiaobing Sun, Tianhao Chen, Ben Tang, and Xinchen Shi. 2020. Analyzing bug fix for automatic bug cause classification.\nJSS 163 (2020), 110538.\n[94] Mirosław Ochodek, Sylwia Kopczy´nska, and Miroslaw Staron. 2020. Deep learning model for end-to-end approximation of COSMIC\nfunctional size based on use-case names. IST (2020), 106310.\n[95] Jordan Ott, Abigail Atchison, Paul Harnack, Adrienne Bergh, and Erik Linstead. 2018. A deep learning approach to identifying source\ncode in images and video. In MSR. IEEE, 376–386.\n[96] Minxue Pan, An Huang, Guoxin Wang, Tian Zhang, and Xuandong Li. 2020. Reinforcement learning based curiosity-driven testing of\nAndroid applications. In ISSTA. 153–164.\n[97] Daniel Perez and Shigeru Chiba. 2019. Cross-language clone detection by learning over abstract syntax trees. In MSR. IEEE, 518–528.\n[98] Kai Petersen, Sairam Vakkalanka, and Ludwik Kuzniarz. 2015. Guidelines for conducting systematic mapping studies in software\nengineering: An update. IST 64 (2015), 1–18.\n[99] Passakorn Phannachitta. 2020. On an optimal analogy-based software effort estimation. IST (2020), 106330.\n[100] Florian Pudlitz, Florian Brokhausen, and Andreas Vogelsang. 2019. Extraction of system states from natural language requirements. In\nRE. IEEE, 211–222.\n[101] Pooja Rani and GS Mahapatra. 2018. Neural network for software reliability analysis of dynamically weighted NHPP growth models\nwith imperfect debugging. STVR 28, 5 (2018), e1663.\n[102] Xiaoxue Ren, Zhenchang Xing, Xin Xia, David Lo, Xinyu Wang, and John Grundy. 2019. Neural network-based detection of self-admitted\ntechnical debt: from performance to explainability. TOSEM 28, 3 (2019), 1–45.\n[103] Stephen Romansky, Neil C Borle, Shaiful Chowdhury, Abram Hindle, and Russ Greiner. 2017. Deep green: Modelling time-series of\nsoftware energy consumption. In ICSME. IEEE, 273–283.\n[104] Hang Ruan, Bihuan Chen, Xin Peng, and Wenyun Zhao. 2019. DeepLink: Recovering issue-commit links based on deep learning. JSS\n158 (2019), 110406.\n[105] Eddie Antonio Santos, Joshua Charles Campbell, Dhvani Patel, Abram Hindle, and José Nelson Amaral. 2018. Syntax and sensibility:\nUsing language models to detect and correct syntax errors. In SANER. IEEE, 311–322.\n[106] Jing Kai Siow, Cuiyun Gao, Lingling Fan, Sen Chen, and Yang Liu. 2020. CORE: Automating Review Recommendation for Code\nChanges. In SANER. IEEE, 284–295.\n[107] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent\nneural networks from overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.\n[108] Hannes Thaller, Lukas Linsbauer, and Alexander Egyed. 2019. Feature maps: A comprehensible software representation for design\npattern detection. In SANER. IEEE, 207–217.\n[109] Junfeng Tian and Yuhui Guo. 2020. Software trustworthiness evaluation model based on a behaviour trajectory matrix. IST 119 (2020),\n106233.\n[110] Junfeng Tian, Wenjing Xing, and Zhen Li. 2020. BVDetector: A program slice-based binary code vulnerability intelligent detection\nsystem. IST 123 (2020), 106289.\n[111] Haonan Tong, Bin Liu, and Shihai Wang. 2018. Software defect prediction using stacked denoising autoencoders and two-stage ensemble\nlearning. IST 96 (2018), 94–111.\n[112] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2018. Deep learning\nsimilarities from different representations of source code. In MSR. IEEE, 542–553.\n[113] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2019. An empirical\nstudy on learning bug-fixing patches in the wild via neural machine translation. TOSEM 28, 4 (2019), 1–29.\n[114] Xiaohui Wan, Zheng Zheng, Fangyun Qin, Yu Qiao, and Kishor S Trivedi. 2019. Supervised Representation Learning Approach for\nCross-Project Aging-Related Bug Prediction. In ISSRE. IEEE, 163–172.\n[115] Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip Yu. 2019. Multi-modal attention network learning for\nsemantic source code retrieval. In ASE. IEEE, 13–25.\n[116] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. 2018. Improving automatic source code\nsummarization via deep reinforcement learning. In ASE. 397–407.\n[117] Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2018. Deep semantic feature learning for software defect prediction. TSE (2018).\n[118] Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic features for defect prediction. In ICSE. IEEE, 297–308.\n[119] Shaohua Wang, NhatHai Phan, Yan Wang, and Yong Zhao. 2019. Extracting API tips from developer question and answer websites. In\nMSR. IEEE, 321–332.\n[120] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting Code Clones with Graph Neural Network and Flow-Augmented\nAbstract Syntax Tree. In SANER. IEEE, 261–271.\n[121] Xu Wang, Chunyang Chen, and Zhenchang Xing. 2019. Domain-specific machine translation with recurrent neural network for software\nlocalization. ESE 24, 6 (2019), 3514–3545.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\nA Survey on Deep Learning for Software Engineering\n•\n1:35\n[122] Yaohui Wang, Hui Xu, Yangfan Zhou, Michael R Lyu, and Xin Wang. 2019. Textout: Detecting Text-Layout Bugs in Mobile Apps via\nVisualization-Oriented Learning. In ISSRE. IEEE, 239–249.\n[123] Ming Wen, Rongxin Wu, and Shing-Chi Cheung. 2018. How well do change sequences predict defects? sequence learning from software\nchanges. TSE (2018).\n[124] Martin White, Michele Tufano, Matias Martinez, Martin Monperrus, and Denys Poshyvanyk. 2019. Sorting and transforming program\nrepair ingredients via deep learning code similarities. In SANER. IEEE, 479–490.\n[125] Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk. 2016. Deep learning code fragments for code clone\ndetection. In ASE. IEEE, 87–98.\n[126] Martin White, Christopher Vendome, Mario Linares-Vásquez, and Denys Poshyvanyk. 2015. Toward deep learning software repositories.\nIn MSR. IEEE, 334–345.\n[127] Karl Wiegers and Joy Beatty. 2013. Software requirements. Pearson Education.\n[128] Jonas Paul Winkler, Jannis Grönberg, and Andreas Vogelsang. 2019. Predicting How to Test Requirements: An Automated Approach. In\nRE. IEEE, 120–130.\n[129] Yan Xiao, Jacky Keung, Kwabena E Bennin, and Qing Mi. 2019. Improving bug localization with word embedding and enhanced\nconvolutional neural networks. IST 105 (2019), 17–29.\n[130] Rui Xie, Long Chen, Wei Ye, Zhiyu Li, Tianxiang Hu, Dongdong Du, and Shikun Zhang. 2019. DeepLink: A code knowledge graph\nbased deep learning approach for issue-commit link recovery. In SANER. IEEE, 434–444.\n[131] Bowen Xu, Amirreza Shirani, David Lo, and Mohammad Amin Alipour. 2018. Prediction of relatedness in stack overflow: deep learning\nvs. SVM: a reproducibility study. In ESEM. 1–10.\n[132] Bowen Xu, Deheng Ye, Zhenchang Xing, Xin Xia, Guibin Chen, and Shanping Li. 2016. Predicting semantically linkable knowledge in\ndeveloper online forums via convolutional neural network. In ASE. IEEE, 51–62.\n[133] Zhou Xu, Shuai Li, Jun Xu, Jin Liu, Xiapu Luo, Yifeng Zhang, Tao Zhang, Jacky Keung, and Yutian Tang. 2019. LDFR: Learning deep\nfeature representation for software defect prediction. JSS 158 (2019), 110402.\n[134] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and natural\nlanguage pairs from stack overflow. In MSR. IEEE, 476–486.\n[135] Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, and Qianxiang Wang. 2019. Neural detection of semantic code clones via tree-based\nconvolution. In ICPC. IEEE, 70–80.\n[136] Fiorella Zampetti, Alexander Serebrenik, and Massimiliano Di Penta. 2020. Automatically learning patterns for self-admitted technical\ndebt removal. In SANER. IEEE, 355–366.\n[137] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. A novel neural source code representation\nbased on abstract syntax tree. In ICSE. IEEE, 783–794.\n[138] Tianyi Zhang, Cuiyun Gao, Lei Ma, Michael Lyu, and Miryung Kim. 2019. An empirical study of common challenges in developing\ndeep learning applications. In ISSRE. IEEE, 104–115.\n[139] Zhuo Zhang, Yan Lei, Xiaoguang Mao, and Panpan Li. 2019. CNN-FL: An effective approach for localizing faults using convolutional\nneural networks. In SANER. IEEE, 445–455.\n[140] Dehai Zhao, Zhenchang Xing, Chunyang Chen, Xin Xia, and Guoqiang Li. 2019. ActionNet: vision-based workflow action recognition\nfrom programming screencasts. In ICSE. IEEE, 350–361.\n[141] Gang Zhao and Jeff Huang. 2018. Deepsim: deep learning code functional similarity. In FSE. 141–151.\n[142] Hui Zhao, Zhihui Li, Hansheng Wei, Jianqi Shi, and Yanhong Huang. 2019. SeqFuzzer: An industrial protocol fuzzing framework from a\ndeep learning perspective. In ICST. IEEE, 59–67.\n[143] Jinman Zhao, Aws Albarghouthi, Vaibhav Rastogi, Somesh Jha, and Damien Octeau. 2018. Neural-augmented static analysis of Android\ncommunication. In FSE. 342–353.\n[144] Yan Zheng, Xiaofei Xie, Ting Su, Lei Ma, Jianye Hao, Zhaopeng Meng, Yang Liu, Ruimin Shen, Yingfeng Chen, and Changjie Fan.\n2019. Wuji: Automatic online combat game testing using evolutionary deep reinforcement learning. In ASE. IEEE, 772–784.\n[145] Cheng Zhou, Bin Li, and Xiaobing Sun. 2020. Improving software bug-specific named entity recognition with deep neural network. JSS\n(2020), 110572.\n[146] Pingyi Zhou, Jin Liu, Xiao Liu, Zijiang Yang, and John Grundy. 2019. Is deep learning better than traditional approaches in tag\nrecommendation for software information sites? IST 109 (2019), 1–13.\n[147] Tianchi Zhou, Xiaobing Sun, Xin Xia, Bin Li, and Xiang Chen. 2019. Improving defect prediction with deep forest. IST 114 (2019),\n204–216.\n[148] Yu Zhou, Xin Yan, Wenhua Yang, Taolue Chen, and Zhiqiu Huang. 2019. Augmenting Java method comments generation with context\ninformation based on neural networks. JSS 156 (2019), 328–340.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2020.\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2020-11-30",
  "updated": "2020-11-30"
}