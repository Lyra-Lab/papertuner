{
  "id": "http://arxiv.org/abs/2011.14808v1",
  "title": "Bringing AI To Edge: From Deep Learning's Perspective",
  "authors": [
    "Di Liu",
    "Hao Kong",
    "Xiangzhong Luo",
    "Weichen Liu",
    "Ravi Subramaniam"
  ],
  "abstract": "Edge computing and artificial intelligence (AI), especially deep learning for\nnowadays, are gradually intersecting to build a novel system, called edge\nintelligence. However, the development of edge intelligence systems encounters\nsome challenges, and one of these challenges is the \\textit{computational gap}\nbetween computation-intensive deep learning algorithms and less-capable edge\nsystems. Due to the computational gap, many edge intelligence systems cannot\nmeet the expected performance requirements. To bridge the gap, a plethora of\ndeep learning techniques and optimization methods are proposed in the past\nyears: light-weight deep learning models, network compression, and efficient\nneural architecture search. Although some reviews or surveys have partially\ncovered this large body of literature, we lack a systematic and comprehensive\nreview to discuss all aspects of these deep learning techniques which are\ncritical for edge intelligence implementation. As various and diverse methods\nwhich are applicable to edge systems are proposed intensively, a holistic\nreview would enable edge computing engineers and community to know the\nstate-of-the-art deep learning techniques which are instrumental for edge\nintelligence and to facilitate the development of edge intelligence systems.\nThis paper surveys the representative and latest deep learning techniques that\nare useful for edge intelligence systems, including hand-crafted models, model\ncompression, hardware-aware neural architecture search and adaptive deep\nlearning models. Finally, based on observations and simple experiments we\nconducted, we discuss some future directions.",
  "text": "1\nBringing AI To Edge: From Deep Learning’s\nPerspective\nDi Liu, Hao Kong, Xiangzhong Luo, Weichen Liu, Ravi Subramaniam\nAbstract—Edge computing and artiﬁcial intelligence (AI), es-\npecially deep learning for nowadays, are gradually intersecting\nto build a novel system, called edge intelligence. However,\nthe development of edge intelligence systems encounters some\nchallenges, and one of these challenges is the computational\ngap between computation-intensive deep learning algorithms and\nless-capable edge systems. Due to the computational gap, many\nedge intelligence systems cannot meet the expected performance\nrequirements. To bridge the gap, a plethora of deep learning\ntechniques and optimization methods are proposed in the past\nyears: light-weight deep learning models, network compression,\nand efﬁcient neural architecture search. Although some reviews\nor surveys have partially covered this large body of literature,\nwe lack a systematic and comprehensive review to discuss all\naspects of these deep learning techniques which are critical for\nedge intelligence implementation. As various and diverse methods\nwhich are applicable to edge systems are proposed intensively,\na holistic review would enable edge computing engineers and\ncommunity to know the state-of-the-art deep learning techniques\nwhich are instrumental for edge intelligence and to facilitate the\ndevelopment of edge intelligence systems. This paper surveys the\nrepresentative and latest deep learning techniques that are useful\nfor edge intelligence systems, including hand-crafted models,\nmodel compression, hardware-aware neural architecture search\nand adaptive deep learning models. Finally, based on observations\nand simple experiments we conducted, we discuss some future\ndirections.\nIndex Terms—Deep learning, model optimization, edge com-\nputing, neural architecture search\nI. INTRODUCTION\nIn 2012, AlexNet [1] broke the record of ImageNet LSVRC\ncontest [2], improving the prediction accuracy by a large\nmargin of 10%, which marked the milestone for the booming\ndevelopment of deep learning (DL) techniques, or more specif-\nically deep neural network (DNN) [3]. In the subsequent years,\nas the increasing number of DNN applications [4] emerges,\ndesigning DNN-based systems attracts a lot of attention and\nDi Liu, Hao Kong, and Xiangzhong Luo contribute equally to this work.\nThis research was conducted in collaboration with HP Inc. and supported by\nNational Research Foundation (NRF) Singapore and the Singapore Govern-\nment through the Industry Alignment Fund-Industry Collaboration Projects\nGrant (I1801E0028). This work is also partially supported by NTU NAP\nM4082282 and SUG M4082087, Singapore.\nDi Liu is with HP-NTU Digital Manufacturing Corporate Lab, Nanyang\nTechnological University, Singapore. (Email: liu.di@ntu.edu.sg)\nHao Kong and Weichen Liu are with HP-NTU Digital Manufacturing\nCorporate Lab, Nanyang Technological University, Singapore, and School\nof Computer Science and Engineering, Nanyang Technological University,\nSingapore. (Email: kong.hao@ntu.edu.sg, liu@ntu.edu.sg)\nXiangzhong\nLuo\nis\nwith\nSchool\nof\nComputer\nScience\nand\nEn-\ngineering,\nNanyang\nTechnological\nUniversity,\nSingapore.\n(Email:\nxi-\nangzho001@e.ntu.edu.sg)\nRavi Subramaniam is with Innovations and Experiences–Business Personal\nSystems, HP Inc., USA. (Email: ravi.subramaniam@hp.com)\nefforts from academia and industry, spanning from advanced\nimage manipulation and enhancement and convenient voice\nassistant on mobile phones to Level-4 autonomous driving\nsystems on automotives [5]. AlphaGo [6] is another prominent\nexample which defeated the top professional player in the\nancient Chinese game GO, deemed impossible before due to\nits extremely high complexity.\nThe successful application of DNN models relies on two\nstages: training and inference/deployment. Training indicates\nthe procedure of learning a predictive model from a huge\namount of labeled data, while inference denotes the procedure\nof using the trained model upon new data to make an accurate\nprediction. The training procedure involves a complex learning\nprocess requiring powerful computational units and a huge\namount of data and spending considerable time. For instance,\ntraining ResNet50 [7] with ImageNet on 8 Nvidia Tesla P100\ntakes 29 hours [8], where ImageNet dataset [2] has 1.28 M\nimages of 1000 classes for training. Recently, researchers also\nmeasure the environmental effect of the training procedure and\nindicate that training DNN consumes excessive energy and\nemits substantial CO2 [9] and environment-sustainable DNN\nmodels should be considered.\nThe trained DNN model is exploited to make prediction on\nnew data, and this procedure is usually called DNN inference.\nMany companies implement their DNN inference on servers to\nprovide convenient services for their customers such as voice-\nassistant, machine translation, image retrieval, etc [10]. How-\never, inference from cloud suffers from responsiveness (e.g.,\nlatency) and privacy issues which are critical in some scenar-\nios. For instance, DNNs are seen to be a pivotal technique\nfor self-driving cars, in which most of executions are subject\nto highly rigorous real-time constraints [5], [11], but the high\nand uncertain communication overhead makes it difﬁcult to\nsatisfy the temporal requirement. In addition, some inference\nis conducted on conﬁdential data, e.g., manufacturing and\nproduction data, and uploading these data to cloud may risk\ndata leakage which will severely hurt their business. To address\nthese concerns, edge computing is proposed [12].\nEdge computing systems are placed at the proximity of data\nproducer, like sensors, end-users, etc, to rapidly and locally\nprocess data. The emergence of edge computing paves the way\nfor pervasive intelligent systems [13]. IEEE Computer Society\nidentiﬁes ‘Artiﬁcial Intelligence at the edge’ as one of the top\n12 technology trends to reach adoption in 2020 [14]. Edge\nsystems have a broad spectrum of hardware features, from\npowerful computation units in communication stations and\nself-driving cars to small and battery-supplied smart phones\nand wearable devices. Fig. 1 plots the overview of edge\narXiv:2011.14808v1  [cs.LG]  25 Nov 2020\n2\nCloud\nCommunication \nstation\nIntelligent \nManufacturs\nSelf-driving \ncars\nwearables\nRobots\nUnmanned \nAerial \nVehicles  \nMobile \nphones\nPrinters\n3D Printers\nSmart \nSurvelliance\nVirtual Reality\nSensors\nEdge\nEnd\nFig. 1. The wide spectrum of Edge Computing\ncomputing. In different scenarios, edge systems are subject\nto diverse performance and physical constraints, e.g., latency\nand power constraints for self-driving cars, computational ca-\npability and power constraints for UAVs, etc. However, DNN\nmodels, the current core component of artiﬁcial intelligence,\nare highly computation-intensive and are even growing ‘wider’\n(more ﬁlters within a layer) and ‘deeper’ (more layers) with\nbillions of parameters and millions of ﬂoat point operations\n(FLOPs) (Some preliminaries about DNN are given in Section\nII). Such ‘deeper’ models provide high accuracy at the cost\nof highly computational complexity. This unavoidably leads to\n‘computational gap’ between DNN models and less-capable\nedge systems.\nMany approaches are proposed to bridge the ‘computational\ngap’ between the complex models and resource-constrained\nedge systems. From hardware perspective, different accelera-\ntors are proposed and many companies are dedicated to de-\nvelop speciﬁc hardware architectures to speed up the execution\nof DNN models, such as application-speciﬁc integrated circuits\n(ASICs) Tensor Processing Unit (TPU) [15] and DianNao\nfamily [16], and FPGA-based accelerators ESE [17] [18].\nHowever, developing chips is a costly process, 30-80 million\ndollars and 2-3 years to develop [19]. Besides, the beneﬁt\nof porting DNN applications to these new computing units\nis still under doubt. The research from Facebook shows the\nperformance gain by porting DNN applications to speciﬁc\nhardware accelerators may not be able to compensate the\nsigniﬁcant engineering effort spent due to the poor ecosystems\nand program-ability [20]. In addition, some customized edge\naccelerators achieve high efﬁciency at the cost of generality,\ne.g., edge TPU only supports a limited number of opera-\ntors [21] and some new DNN architectures cannot be well-\nsupported.\nOn the other hand, software is known to be ﬂexible and less\ncostly to develop in comparison with hardware. For efﬁcient\nedge intelligence systems, from software perspective, the goal\nis to design proper DNN models which can be ﬁt on edge\nsystems while guaranteeing the required performance and\nmaximizing accuracy. Different methodologies are devised to\napproach this goal, like novel light-weight DNN model design,\nmodel compression, and the emergent neural architecture\nsearch (NAS) [22], all of which facilitate the development\nand application of DNN models on edge systems. In the\npast decade, many DNN breakthroughs have signiﬁcantly\nimproved DNNs’ accuracy and performance and we believe\nthe innovation from deep learning techniques will still be the\nkey ingredient for the emergent edge intelligence.\nA. Motivation Behind This Paper\nThere exist several excellent reviews about edge computing\nand edge intelligence. Zhou et al. [13] discuss the great po-\ntential of edge intelligence and point out some future research\ndirections. Wang et al. [23] mainly review the edge intel-\nligence systems from communication systems’ perspective.\nChen and Ran in [24] discuss the diverse applications when\ndeep learning techniques are integrated into edge computing.\nAll of these reviews only have a small fraction to discuss\nthe edge DNN design which is the key element of edge\nintelligence systems.\nThe vast DNN design methods applicable to edge systems\ncan be broadly classiﬁed into three categories: hand-crafted\nmodels, model compression and hardware-aware NAS. Some\nprior DNN reviews cover one of the three categories, like\n[25] [26] for model compression, [27] for early works on\nefﬁcient DNN processing and [28] for NAS1 but there is no\nreview discussing all three categories which in future will be\nintegrated seamlessly to design effective and efﬁcient DNNs\nin the new edge intelligence era. Considering the anticipated\nemergence of numerous edge intelligence systems in the next\nyears [13], this motivates us to comprehensively review the\nrelevant works about edge DNN design in a holistic fashion.\nIn addition, except the static DNN design for edge systems,\nwe also discuss adaptive DNN models which are also not\nreviewed systematically in previous articles. Edge systems\ninstalled in real world operate in highly dynamic environment,\nwhile it still needs to guarantee a certain degree of quality of\nservice (QoS) and performance, like real-time requirement.\nHowever, static DNN models are unable to achieve such\nguarantee under dynamic environment [20]. Hence, we need\nmodels which are able to adapt their computation to achieve\na good trade-off between accuracy and efﬁciency.\n1This survey is for the general NAS, not the hardware-aware NAS.\n3\nB. Structure of This Paper\nIn general, we consider this paper as a good complementary\nfor other related surveys or reviews in the ﬁeld of edge com-\nputing and hope this paper facilitates a profound understanding\nfrom deep learning’s perspective, which plays a pivotal role\nin implementing edge intelligence systems. As shown in [29],\naround 50 papers were uploaded to Arxiv per day in 2018,\ndiscussing DNN related topics, i.e., in total more than 15000\npapers per year. It is supposed to have more now. AlexNet\nwhich broke the record in 2012 now is rarely deemed as\nan important reference approach for experimental comparison.\nTherefore, it is impossible to review all related papers. In this\npaper, due to the space limitation, we strive to cover the most\nrelevant, representative, and latest works2 which, we believe,\nare adequate for readers to fully understand the development,\nmotivation, and latest trend of these DL techniques applicable\nfor edge systems and to have a full view of these techniques.\nFinally, we provide some of our thoughts about designing\nDNNs for edge devices based on some observations and\nexperiments we have conducted.\nScope of this paper: Edge computing is an emergent\nresearch topic, consisting of many interesting and challenging\nresearch problems, like edge caching [30] and computation\nofﬂoading [31], [32]. Prior review or surveys have profoundly\ndiscussed these topics. In this paper, we only focus on deep\nlearning techniques which aid in developing edge intelligence\nsystems. For other related topics, we refer interesting readers\nto other excellent literature [23], [30], [31].\nThe reminder of this paper is organized as follows:\n• Section II introduces the preliminaries of DNN models\nto facilitate the understanding of techniques presented in\nsubsequent sections.\n• Section III discusses works designing novel DNN archi-\ntectures for light-weight DNN models.\n• Section IV reviews network compression methods, in-\ncluding network pruning, quantization, and knowledge\ndistillation.\n• Section V discusses NAS techniques aiming to design and\ncustomize efﬁcient DNN models for resource-constrained\nsystems, like some edge systems.\n• Section VI discusses adaptive DNN models which may\nadapt the model computation under dynamic environ-\nment.\n• Section VII demonstrates our thoughts on future edge\nDNN designs based on our observation and experimental\nresults.\n• Section VIII concludes this paper.\nFig 2 shows the detailed structure of this paper.\nII. PRELIMINARIES\nIn this section, we introduce some basic knowledge of\nDNNs for better understanding the techniques and approaches\ndiscussed in subsequent sections. In this paper, we mainly\nreview the models and techniques proposed for computer\nvision applications and there are two reasons for this. First,\n2We include peer-reviewed papers until 2020\nSection II\nPreliminaries of\nDNN\nSection III\nHandcrafted\nmodels\nSection IV\nModel\nCompression\nSection V\nHardware-aware\nNAS\nSection VI\nAdaptive Models\nSection III-A\nClassiﬁcation\nSection III-B\nObject Detection\nSection IV-A\nPruning\nSection IV-B\nQuantization\nSection IV-C\nKnowledge\nDistillation\nFig. 2. The schematic structure of this paper\ncurrently computer vision models are the major application\nfor edge intelligence systems; Second, although some recent\nstudies start to investigate how to design lightweight natural\nlanguage processing (NLP) models for edge devices like, [33]\n[34] [35], they are far from mature like models and techniques\nfor computer vision tasks. Hence, in this paper, we use DNN\nmodels of computer vision as the major example for our\npresentation.\nFor computer vision applications, when referring to DNNs,\nwe mean convolutional neural networks (CNN). Throughout\nthis paper, we may use DNN and CNN interchangeably.\nCNNs usually have many convolutional layers, pooling layers,\nactivation layers, and a couple of fully connected layers [3].\nFigure 3 shows a simple CNN with two convolutional layers\nand one fully connected layer. The convolutional layer is the\ncore ingredient in CNNs, which extracts the patterns/features\nfrom input data at different granularity. Convolutional layers\naccount for the major resource and time cost of a CNN model.\nTo better illustrate how the convolutional layer works, we give\nsome terminologies about convolutional layers as follows:\n• Kernel – A kernel is a 2D square matrix with size like\n3 × 3, 5 × 5, 7 × 7 and a convolutional operand.\n• Filter – A ﬁlter is a collection of kernels with size of\nk × k × c, where k is the kernel size and c is equal\nto the number of input channels. A ﬁlter is convolved\nwith all input channels to derive a new channel/feature\nmap/activation map.\n• Feature map/channel/activation – Feature map, chan-\nnel, and activation have the same meaning in CNNs. A\nfeature map is a 2D feature matrix generated by a ﬁlter.\nAs demonstrated in Figure 3, the input of the ﬁrst convolu-\ntional layer is an image with n1 channels, and each channel\nis a 2D array with height h1 and width w1. Correspondingly,\nthere are n1 kernels in each ﬁlter, and the kernel size is k1×k1.\nThere are n2 ﬁlters, and thus n2 feature maps will be generated\nafter the ﬁrst convolution operation and the size of the ﬁrst\nfeature maps is (h2 × w2 × n2), where h2 and w2 are the\nheight and weight of the ﬁrst feature maps, respectively. If\nthe padding is used during convolution, then h1 = h2 and\nw1 = w2. Otherwise, h1 > h2 and w1 > w2. For more details,\ninteresting readers are referred to [3].\nUsually, a convolutional layer is followed by a pool layer\nand an activation layer. In Fig. 3 , we omit activation layers.\nActivation functions are used to introduce non-linearity into\nneural network, and the common activation functions are\nrectify linear units (ReLu) [36], sigmoid, etc. The pooling\nlayer down-samples the feature map to reduce the spatial size\n4\nInput\nFilters\n…\nFeature maps\n𝒏𝟑channels\n𝒏𝟑channels\nOutput\n𝒏𝟐channels\nConvolution\nPooling\nConvolution\nPooling\nFull connection\n(ℎ1 ∗𝑤1 ∗𝑛1) (𝑘1 ∗𝑘1 ∗𝑛1 ∗𝑛2) (ℎ2 ∗𝑤2 ∗𝑛2)\n(ℎ3 ∗𝑤3 ∗𝑛2)\n(ℎ4 ∗𝑤4 ∗𝑛3) (ℎ5 ∗𝑤5 ∗𝑛3)\nFig. 3. The overview of a DNN/CNN model\nof the feature map and increase its receptive ﬁeld. The max\npooling [37] and the average pooling are the two common\npooling methods. A DNN model usually ends up with a couple\nof fully connected layers, which is used to fuse the feature\ninformation from the last convolutional layer and predict the\nclassiﬁcation of the input image. To improve the performance\nand training speed, modern DNN models also have other\noperation layers, like batch normalization layer [38], squeeze-\nand-excitation layer [39], etc.\nIII. LIGHTWEIGHT NETWORK DESIGN\nAlexNet [1] marks the milestone of rapid development\nof DNN models while researchers ﬁnd that the bigger a\nDNN model is, the better accuracy the model can provide.\nAs a result, this incentivizes the emergence of increasingly\ncomplicated DNN models [40]. However, the highly compu-\ntational complexity of DNN models hinders them from being\nefﬁciently deployed on resource-constraint devices, e.g., edge\nor IoT devices. A promising way to solve this problem is\nto design novel neural architectures/operators which are more\nefﬁcient on edge systems while not compromising accuracy.\nIn this section, we review the works which target to manually\ndesign lightweight models. For these hand-craft models, we\nclassify them into two categories based on their applications:\nclassiﬁcation and object-detection. These two applications are\nthe major deployment scenarios for edge intelligence systems.\nA. Classiﬁcation\nClassiﬁcation is the fundamental and most critical task\nin computer vision and it is also the ﬁrst application in\nwhich DNNs demonstrated its huge potential [1]. Moreover,\nclassiﬁcation models play a core role in other computer vision\ntasks, such as semantic segmentation and object detection,\nwhere classiﬁcation models serve as the backbone to extract\nfeatures from images and provide essential feature information\nfor segmentation and detection.\nSqueezeNet [41] is one of the early works towards de-\nsigning DNNs for resource-constrained hardware. The core\nidea in SqueezeNet is a new computational module, Fire\nmodule, in which convolutional operations are split into the\nsqueeze layer and expand layer and some 3 × 3 convolutional\noperations are replace by low-complexity 1 × 1 convolution\noperations. SqueezeNet achieves AlexNet-level accuracy while\ngreatly reducing the model complexity. The applicability of\nSqueezeNet is validated on real FPGA with small on-chip\nmemory. Gschwend et al. [42] develop a variant of SqueezeNet\nand implement it on an FPGA. It shows the SqueezeNet-\nlike model can be entirely ﬁt within the FPGA on-chip\nmemory. As the result, it signiﬁcantly eliminates the off-chip\nmemory access overhead and in turn improves the inference\nperformance.\nMobileNet series [43]–[45] are another prominent DNN\nmodels targeting resource-limited devices. MobileNet [43]\nreplaces the conventional convolutional operation with more\nefﬁcient depth-wise separable convolution operation proposed\nin [46] to reduce the computational cost. Depth-wise separable\nconvolution operation factorizes a conventional k × k × n\nconvolution into a k × k × 1 depth-wise convolution and\na 1 × 1 × n point-wise convolution. Each input channel\nis convolved with the depth-wise convolution operator and\nthe point-wise convolution linearly combines all results from\ndepth-wise convolution to generate one channel/feature map.\nDepth-wise separable convolution can signiﬁcantly reduce the\ncomputational complexity, thus shortening inference time on\nedge devices. On the top of MobileNet, MobileNetV2 [44]\nadds linear bottleneck and inverted residual block to improve\nboth accuracy and performance. The latest MobileNetV3 [45]\ncombines NAS and NetAdapt [47] to design a more accurate\nand efﬁcient network architecture.\nTABLE I\nCOMPARISON OF HAND-CRAFTED MODELS. EACH MODEL HAS A SCALING\nFACTOR TO SCALE UP THE NUMBER OF CHANNELS. HERE WE ONLY\nREPORT THE RESULTS OF THE SCALING FACTOR=1.\nModel\nYear\nParameters\nFLOPs/MACs\nAccuracy\nSqueezeNet [41]\n2016\n1.24M\n60.4%\nMobilenet [43]\n2017\n4.2M\n/569M\n70.6%\nMobilenetV2 [44]\n2018\n3.4M\n/300M\n72%\nMobilenetV2-large [44]\n2018\n6.9M\n/585M\n74.7%\nMobilenetV3-small [45]\n2019\n2.5M\n/56M\n67.4%\nMobilenetV3-large [45]\n2019\n5.4M\n/300M\n75.2%\nShufﬂeNet [48]\n2018\n3.4M\n/292M\n71.5%\nShufﬂeNetV2 [49]\n2018\n2.3M\n146M/\n69.4%\nEfﬁcientNet-B0 [50]\n2019\n5.3M\n390M/\n77.1%\nGhostNet [51]\n2020\n5.2M\n141M/\n73.9%\n5\nShufﬂeNet [48] deploys group convolution and channel\nshufﬂe to reduce the computational complexity while retaining\nhigh accuracy. ShufﬂeNetV2 [49] empirically observes four\nprinciples for designing efﬁcient DNNs and proposes channel\nsplit to improve accuracy and performance. GhostNet [51]\nproposes a ghost module based on an observation that some\nfeatures in convolutional layers are highly correlated. Thus,\nit ﬁrst uses standard convolutional operation to obtain a few\nintrinsic features and then generates more features from the\nintrinsic features with cheap linear operations.\nEfﬁcientNet [50] investigates the inﬂuence of three scaling\ndimensions in DNN models, i.e., depth/layer scaling, channel\nscaling and resolution scaling, and proposes a compound\nscaling method such that given a DNN model and a target\ncomputational complexity (i.e., FLOPs) it can effectively ad-\njust the three dimensions of a model to improve its predictive\nperformance. The scaled models achieves a comparable or\nbetter accuracy while having fewer parameters.\nDiscussion: Table I summarizes the models discussed in this\nsection. Signiﬁcant progresses have been made in lightweight\nDNN models for classiﬁcation task, and classiﬁcation DNN\nmodels are also the key component for other CV tasks, like\nsegmentation, detection, etc. The manual design of lightweight\nDNN models is considerably dependent on experts’ knowledge\nand also needs a time-consuming hyperparameter exploration.\nAs automated machine learning (AutoML3) [52] techniques,\nlike NAS, hyperparameter optimization, emerge to aid in\ndesigning DNN models, designers can put their focus on\ndesigning effective and efﬁcient DNN modules or operations.\nThen the new operations can be used as the fundamental\nelements to generate new models for edge systems. For\ninstance, the latest MobileNetV3 exploits NAS with the novel\narchitectures from MobileNetV2 and MnasNet [53] to ﬁnd\naccurate and efﬁcient DNN model for mobile setting. We\nthink this is becoming a mainstream trend to adopt NAS\nwith novel lightweight operators to design edge DNN model.\nHowever, existing methods overlook the impact of deployed\nhardware, on which some operators cannot be supported or\neffectively executed. Thus, for the future DNN classiﬁcation\nmodels, especially for edge systems, it is important to design\nmodels in a hardware-aware fashion.\nB. Object Detection\nObject detection is another vital ﬁeld in DNN research.\nBesides predicting the category of an input image, object de-\ntection locates objects in the input image by drawing bounding\nboxes. Generally, we can classify object detection methods into\ntwo categories: one-stage method and two-stage method. One-\nstage methods predict object classiﬁcation and localization\nin one single forward pass. YOLO [64] and SSD [65] are\ntwo widely-used examples of one-stage methods; Two-stage\nmethods ﬁrst deploy a backbone CNN network (usually for\nclassiﬁcation) to extract features from input images and then\na detection part uses the features extracted from the backbone\nnetwork to localize objects. RCNN [66], Fast RCNN [67], and\n3https://https://www.automl.org/\nFaster RCNN [68] are representative examples of two-stage\nmethods.\nTo boost the efﬁciency of object detection on resource-\nlimited devices, some works strive to reduce the computational\ncomplexity of backbone part of object detection models, like\nTiny-dsod [58] for DSOD [69], Tiny-SSD [59] for SSD [65],\nand Tiny-YOLO [57] for YOLO. Other efforts combines SSD\nframework with lightweight CNNs, like MobileNet, Shuf-\nﬂeNet, SequeezeNet, etc to improve efﬁciency. SequeezeDet\n[56] implements an object detection model by using Se-\nqueezeNet [41] as the backbone to improve the efﬁciency\nwhile not signiﬁcantly compromising accuracy. Pelee [60]\ncombines an improved SSD framework with its optimized\nPeleeNet to improve efﬁciency of object detection. Since one-\nstage methods are less computational than two-stage methods,\nall of these methods mentioned above target one-stage method\nfor improving efﬁciency of object detection DNNs.\nFew efforts strive to improve efﬁciency of two-stage meth-\nods. Li et al. [61] replace the heavy-head in RCNN frame-\nwork to speed up execution. Qin et al. [62] observe a\ngood conﬁguration of input resolution, backbone network,\nand detection head can readuce the complexity of two-stage\nmethods while maintaining competitive accuracy. Thus, they\npropose ThunderNet in which a variant of ShufﬂeNet, dubbed\nSNet, is proposed to implement an efﬁcient DNN for object\ndetection. EfﬁcientDet [63] is the counterpart of EfﬁcientNet\nin object detection, where a new bi-directional feature pyramid\nnetwork is proposed to combine with EfﬁcientNet to generate\nan efﬁcient object detection detector.\nDiscussion: Table II summarizes all object detection works\ndiscussed in this section. Although object detection can be\nconsidered as an extension of classiﬁcation, there lack ade-\nquate efforts to address the efﬁciency issues of object detection\nfor resource-constrained systems. The two-stages methods\nprovide high accuracy at the cost of efﬁciency, and even on\npowerful GPUs they cannot guarantee real-time constraints\n(e.g., 25fps for video). The one-stage approaches can trade\noff accuracy for efﬁciency, but the practical deployment on\nedge devices is still behind the expected performance [70].\nIn addition, we notice that only two works evaluate their\ndesigns on edge settings (on ARM CPU, mobile GPU or low\npower accelerators) and as pointed in [49] using the indirect\nmetrics lke FLOPs or MACs cannot directly translate to\nthe relevant performance metric, i.e., latency and throughput.\nTherefore, for the future model design, evaluating models’\ndirect performance on targeting platforms will be a necessity.\nIn addition, for edge systems, the trade-off between speed and\naccuracy should be application or context dependent, so a\ngood benchmark and design guide should be developed for\npractitioners [71].\nIV. NETWORK COMPRESSION\nNovel lightweight models provide us a solution to efﬁciently\ndeploy DNN models on edge devices. However, designing\na novel architecture is really challenging due to its large\ndesign space and complicated parameter tuning. Unfortunately,\nthe majority of DNN models are designed to pursue better\n6\nTABLE II\nHAND-CRAFTED OBJECT DETECTION MODELS SUMMARY. ALL INFORMATION OF THIS TABLE IS FROM THE ORIGINAL PAPER OR THE PAPER WHICH\nCOMPARE THESE MODELS. EXPERIMENTAL RESULTS ARE UPON TWO WIDELY-USED BENCHMARK, PASCAL VOC2007 [54] AND MS COCO [55].\nModel\nYear\nMethods\nInput\nParameters\nFLOPs/MACs\nmAP(PASCAL)\nmAP(COCO)\nTested Platform\nSqueezeDet [56]\n2017\nOne-stage\n1242 × 375\n-\n9.7B/\n-\n-\nGPU\nTiny-YOLO [57]\n2018\nOne-stage\n416 × 416\n15.12M\n3.49B/\n57.1%\n-\nGPU\nTiny-DSOD [58]\n2018\nOne-stage\n300 × 300\n1.15M\n1.12B/\n72.1%\n23.2%\nGPU\nTiny-SSD [59]\n2018\nOne-stage\n300 × 300\n1.13M\n/571.09M\n61.3%\n-\nGPU\nPelee [60]\n2018\nOne-stage\n320 × 320\n5.98M\n1.21B/\n70.9%\n22.4%\nGPU/Edge GPU\nLight-Head RCNN [61]\n2017\nTwo-stage\n800 × 1200\n-\n5.65BM/\n75.1%\n-\nGPU\nThunderNet [62]\n2019\nTwo-stage\n320 × 320\n-\n4.61M/\n75.1%\n-\nGPU/Edge CPU\nEfﬁcientDet [63]\n2020\nTwo-stage\n-\n8.1M*\n11B*/\n-\n43.0%*\nGPU\n*EfﬁcientDet has models with different complexity. We report the results of EfﬁcientDet D2 which may be applicable to edge systems.\naccuracy without consideration of resource constraints of edge\nsystems, where complex DNN models have a few hundred\nlayers and several billions of parameters to achieve competitive\naccuracy. As indicated in [72], DNN models usually have\nsigniﬁcant redundancy in terms of weights and parameters.\nThen, an interesting question is raised:\nCan we reduce the complexity of DNN models by\nremoving these redundancy without greatly compro-\nmising their predictive performance?\nNetwork compression tackles this problem by removing\nthe redundancy of over-parameterized networks. Network\ncompression techniques generally fall into three categories:\nnetwork pruning which removes the redundant weights and\nchannels of over-parameterized DNNs, quantization which\nuses fewer bits to store DNN weights and intermediate results\n(e.g., ﬂoat point 32, FP32 to integer 8, INT8), and knowledge\ndistill which learns a small and compact (student) model from\na large and over-parameterized (teacher) model. It is worthy\nnoting that the three approaches are not mutual exclusive to\neach other and in many cases they are combined to maximally\ncompress the redundant models. In this section, we discuss\nthree approaches in details.\nA. Network Pruning\nThe main motivation behind network pruning is that DNN\nmodels are usually over-parameterized in terms of weights\nand channels [72], and eliminating these redundancy within\nthe model can hugely reduce the computational complexity\nand storage requirement. Many network pruning methods are\nproposed in the past 5 years, and we like to classify them\ninto two major branches based on the pruned structure: non-\nstructure pruning and structure pruning.\n1) Non-Structure Pruning: Non-structured pruning tech-\nnique, widely known as weight pruning, conducts a ﬁne-\ngrained operation by removing irrelevant weights in over-\nparameterized DNN models, as shown in Fig. 4. It removes in-\ndividual weights within a kernel or individual neurons within a\nfully connected layer. Non-structure pruning can signiﬁcantly\nreduce the number of parameters and memory footprint.\nWeight pruning on neural network can be traced back to\n1990s, [73], [74], where pruning on fully-connected neural\nnetworks was investigated. Deep Compression framework\n[75], [76] is one of the pioneers in DNN model compression.\n-0.4\n0\n0.98\n1.2\n0.21\n0\n0\n0.6\n1.93\n0\n0\n0.98\n1.2\n0\n0\n0\n0.6\n1.93\nKernel Weights\nFig. 4. Non-structure Pruning\nDeep Compression uses three steps, prunning, quantization,\nand Huffman Encoding, to compress over-parameterized DNN\nmodels, such as VGG [77], AlexNet [1]. The pruning step\nremoves the weights with magnitude lower than a threshold\nand corresponding connections. Then, quantization reduces\nthe bit-width of weights to reduce the model size (More\ndetails about quantization in the subsequent section). Finally,\nHuffman encoding further compresses the weight storage.\nExperimental results show deep compression can signiﬁcantly\nreduce the model size with no or negligible accuracy loss.\nInspired by the success of deep compression, many new\nmethods are proposed to further improve the efﬁcacy of non-\nstructure pruning. Molchanov et al. [78] extend variational\ndrop-rate to each weight of a DNN model, and weights with\nhigh drop-rate are deemed irrelevant and thus can be removed\nfor model compression. Different from above-mentioned ap-\nproaches which conduct non-structure pruning directly on\npre-trained networks, NeST [79], that is inspired by the\ndevelopment of human brain, adopts a grow-prune scheme,\nwhere NeST ﬁrst makes a sparse seed DNN model bigger and\nmore complex and then prunes some irrelevant weights from\nthe grown model to generate the ﬁnal compact model. Zhang\net al. [80] formulate the weight-pruning problem as a non-\nconvex problem which can be solved by alternating direction\nmethod of multipliers (ADMM) method [81].\nAll methods discussed above target to reduce the model size\nvia non-structure pruning. However, for edge devices, power\nand energy are also important metrics to consider. Yang et al.\n7\nTABLE III\nCOMPARISON OF NON-STRUCTURE PRUNING METHODS\nMethods\nPruning method\nMetrics\nyear\nDeep Compression [76]\nThreshold/Huffman Coding\nModel size\n2015\nMolchanov et al. [78]\nVariational Drop Rate\nModel size\n2017\nYang et al. [82]\nheuristic\nEnergy\n2017\nZhang et al. [80]\nADMM\nModel size\n2018\nNeST et al. [79]\nGrow-prune\nModel size\n2019\nFilters\nChannels\nFilters\nChannels\nFig. 5. A simple visualization of ﬁlter pruning\nin [82] propose an energy-aware pruning method, in which\nthey strive to reduce the energy consumption of DNN models\nby non-structure pruning. The core idea behind their approach\nis to order layers according to their energy consumption and\nthen it prunes weights according to that order.\nAlthough non-structure pruning can signiﬁcantly reduce\nmemory footprint and multiply-accumulate (MACs) of DNNs,\nsuch reduction does not directly translate to latency improve-\nment. This is because non-structured pruning generates sparse\nstructures which lead to irregular access pattern. The irregular\npattern of sparsiﬁed DNN models needs special formats, e.g.,\ncompressed sparse row and compressed sparse column, to\nstore sparse matrices. The off-the-shelf hardware and software\ncannot efﬁciently execute those compressed formats, so spe-\ncialized hardware and software libraries are required to execute\nsparsiﬁed DNN models [83], [84].\n2) Structure Pruning: Structure pruning, on the other hand,\nprunes network by maintaining its regular pattern. To keep reg-\nularity, structure pruning completely removes some channels\nand ﬁlters, that have least impact on the model’s prediction\nas shown in Fig. 5. Since structured pruning does not lead\nto irregular pattern, the compressed network pruned by struc-\nture pruning can directly accelerate its inference on off-the-\nshelf hardware platforms without specialized software library\nsupport. Therefore, it has been receiving growing attention in\nrecent years.\nThe common process of structure pruning is (1) deﬁning\na pruning criterion; (2) selecting pruned channels according\nto the criterion and goal, such as compression ratio and\nthe number of MACs or FLOPs; and (3) ﬁne-tuning the\npruned model, i.e., retraining the pruned network to retain\naccuracy. Works in structure pruning deﬁne different criteria\nand adopt different methods to select the pruned channels,\nwhile minimizing accuracy loss. In terms of pruning methods,\nwe can classify them into two categories: training-based vs\ninference-based.\n• Training-based: The pruning is conducted during the\ntraining procedure, where a sparsity constraint is exposed\nand a compact network is directly learned from a big and\nover-parameterized network;\nTABLE IV\nTHE CLASSIFICATION OF STRUCTURE PRUNING METHODS.\nMethodologies\nPruning methods\nScope\nProcess\nYear\nLi et al. [87]\nInference-based\nLayer\nRule-based\n2016\nHu et al. [88]\nInference-based\nGlobal\nRule-based\n2016\nSSL [89]\nTraining-based\nLayer\nRule-based\n2016\nHe et al. [90]\nInference-based\nLayer\nRule-based\n2017\nThiNet [91]\nInference-based\nLayer\nRule-based\n2017\nDeepIoT [92]\nTraining-based\nLayer\nLearing-based\n2017\nDCP et al. [93]\nInference-based\nLayer\nRule-based\n2018\nSFP [94]\nTraining-based\nLayer\nRule-based\n2018\nHuang et al. [95]\nTraining-based\nGlobal\nRule-based\n2018\nAMC [96]\nInference-based\nLayer\nLearning-based\n2018\nNetAdapt [47]\nInference-based\nLayer\nLearning-based\n2018\nGate Decorator [97]\nInference-based\nGlobal\nRule-based\n2019\nLFPC [98]\nInference-based\nLayer\nLearning-based\n2020\nHRank [99]\nInference-based\nLayer\nRule-based\n2020\n• Inference-based: The prune method reduces the redun-\ndant channels from a pre-trained model according to\ndeﬁned rules;\nIn terms of pruning scope, we have layer pruning vs global\npruning.\n• Layer pruning: The pruning process is applied to the\nnetwork layer by layer for ﬁnding the pruned network\nsatisfying a deﬁned target;\n• Global pruning: The pruning process is applied to the\nwhole network for ﬁnding the best pruned network while\nsatisfying a deﬁned target;\nThe pruning process is either rule-based or learning-based.\n• Rule-based: The pruning is conducted according to some\ndeﬁned rules, like heuristic algorithms;\n• Learning-based: The pruning is conducted by a learning\nalgorithm, such as reinforcement learning [85], evolution-\nary algorithms [86] and gradient-based optimization.\nLi et al. [87] adopt a global pruning method where all ﬁlters\nare sorted in terms of absolute weight sum and then ﬁlters\nwith low magnitude are pruned and related channels are all\nremoved. He et al. [90] present a layer pruning method using\nLASSO regression and reconstruction error to select pruning\nchannels. Hu et al. [88] observe that a fraction of activation\nweights in DNNs are zero and these zero weights imply\nthe corresponding ﬁlters are likely to be redundant and can\nbe pruned, and they thus propose using Average Percentage\nof Zeros (APoZ) of a ﬁlter as the criteria to select pruned\nchannels. Instead of using the information from the currently\npruned layer for selecting pruned channels, ThiNet [91] pro-\nposes to exploit the information from the output of the next\nlayer to determine pruned ﬁlters. Discriminate-aware channel\npruning (DCP) in [93] relies on the discrimination of each\nﬁlter to select the pruned channels. The main concept is that\nthe discriminated channels provides more relevant information\nor features to retain accuracy and then the channels which\nare inadequately discriminated can be pruned for complexity\nreduction. You et al. [97] propose gate decorator module to\nreplace the batch normalization module in DNNs to select\npruned ﬁlters globally. Most structure pruning methods adopt\na uniform pruning criterion for all layers, but different layers\nhave different functions, thereby likely beneﬁting from em-\nploying different pruning criteria at different layers. Recently,\nHe et al. [98] propose LFPC to learn an optimal pruning\n8\ncriterion for each layer by using a gradient-based method.\nHRank [99] empirically ﬁnds that ﬁlters with low-rank is\nless informative than those with high-rank, and thus uses this\nobservation to prune the unimportant channels.\nWen et al. [89] propose SSL to have a training-based\npruning method, learning a compact and sparse model from\na pre-trained model. Liu et al. [100] propose an approach\ncalled network slimming, which takes wide and large networks\nas input models, but during training, insigniﬁcant channels\nare automatically identiﬁed and pruned afterwards. Soft ﬁlter\npruning (SFP) [94] deploy a training-based pruning method\nto prune a complex network. Huang et al. [95] propose the\nconcept of sparsity scaling factor for each ﬁlter which is\nlearned during to training, and then ﬁlters with scaling factor 0\nare removed. Yao et al. [92] train an recurrent neural network\n(RNN) to determine the dropout probability of each ﬁlters and\nprune the network layer by layer according to drop probability.\nMost of the above-mentioned structure pruning methods are\nrule-based, i.e., some heuristic algorithms devised according\nto their own criterion. In contrary, some work employ learning\nalgorithm to automatically prune network model. AMC [96]\nproposes to use reinforcement learning to automatically prune\nchannels of each layer. Anwar et al. [101] prune a DNN\nmodel at feature level, kernel level, and intra-kernel level (i.e.,\nweight), where they deploy evolutionary algorithm to ﬁnd the\nbest combination of different pruning granularities. However,\nsearching in a discrete space using RL and EA is really costly,\nso the gradient-based method is recently proposed to ﬁnd an\noptimal pruning criterion for each layer [98].\nTo determine how many channels should be pruned, the\nabove-mentioned works use indirect metrics like FLOPs or\ncompression ratio to prune network. Nevertheless, the reduced\nFLOPs and compression ratio cannot directly translate to the\nperformance improvement. In addition, a diverse of hard-\nware accelerators have emerged for boosting the execution\nof DNNs, but these various systems demonstrate different\ncapability to handling network complication. Hence, some\npruning studies directly target the direct metric upon a speciﬁc\nhardware, e.g., latency. NetAdapt [47] proposes an automated\nframework to prune ﬁlters in different layers such that the\npruned model can be adapted to a target platform. To optimize\nthe latency on a target platform, NetAdapt builds up a look-up-\ntable (LUT) for different operations and layers, so instead of\nmeasuring latency on the real platform, it can quickly estimate\nthe latency based on the model architecture and LUT. Yu et\nal. [102] introduce SIMD-aware pruning framework which\nemploys different pruning strategies for different underlying\nhardware, like weight pruning for low-parallelism CPU and\nﬁlter pruning for high-parallelism.\nDiscussion: Table III and IV summarize methods discussed\nin this section. Pruning is the well-studied topic in model\ncompression and many methods have been proposed to ad-\nvance the network pruning [103]. These pruning methods show\npromising results, capable of compressing a DNN model,\nin some cases more than 40x, while only degrading the\nprediction accuracy slightly [76]. To effectively adopt model\npruning for edge intelligence systems, the existing works\nsuffer from two ﬂaws: 1) the theoretical foundation behind\nForward Pass\na(FP32)\nW(FP32)\nZ(FP32)\nConv(FP32)\na(FP32)\nW(FP32)\nZ(FP32)\nConv(INT8)\na(INT8)\nW(INT8)\nZ(INT8)\nquantizer\nde-quantizer\nFig. 6. An example of convolution quantization.\nTABLE V\nTHE CLASSIFICATION OF QUANTIZATION METHODS.\nMethodologies\nBits\nWeights\nActivations\nYear\nGupta et al. [109]\n16-bit\nYes\nNo\n2015\nQ-CNN [110]\n1-bit\nYes\nNo\n2016\nBinaryConnect [111]\n1-bit\nYes\nNo\n2016\nTernary [112]\n2-bit\nYes\nNo\n2016\nDoReFa-Net [113]\nArbitrary\nYes\nYes\n2016\nXNOR-Net [114]\n1-bit\nYes\nYes\n2016\nINQ [115]\n5/4-bit†\nYes\nYes\n2017\nABC-Net [116]\n1-bit\nYes\nYes\n2017\nZhu et al. [117]\n2-bit\nYes\nNo\n2017\nJacod [118]\n8-bit\nYes\nYes\n2018\nBi-Real-Net [119]\n1-bit\nYes\nYes\n2018\nLQ-Net [120]\nArbitrary\nYes\nYes\n2018\nTQT [121]\n8-bit\nYes\nYes\n2019\nHAQ [122]\n(1∼8)-bit§\nYes\nYes\n2019\nHAWQ [123]\n2/4-bit†\nYes\nYes\n2019\nJung et al. [124]\n(2∼4)-bit\nYes\nYes\n2019\nZhuang et al. [125]\n2-bit\nYes\nYes\n2020\nXOR-Net [126]\n2-bit\nYes\nYes\n2020\na/b-bit † denotes the method quantizes the network weights for a bits while\noperating intermediate activation feature maps for b bits.\n(a ∼b)-bit§ represents the method quantizes the network weights or\nactivations from a bits to b bits.\npruning lacks, and thus there is a debate whether we should\nprune a complex model or directly train a compact model\nfor resource constrained hardware platforms. Some recent\nresearch [104] [105] [106] [107] strives to empirically or\ntheoretically ﬁnd an answer for this question. Since pruning\nis a time-consuming procedure involving large model train-\ning and iterative pruning-retraining procedure, a theoretical\nfoundation or proof may drastically change the way we use\nmodel pruning to design edge DNN models; 2) Almost all\nof pruning methods are hardware-agnostic and depend on\nhardware-independent metrics, like MACs and FLOPs. Since\ndifferent hardware architectures demonstrate different degrees\nof parallelism, the state-of-the-art methods may unnecessarily\nprune models without performance improvement (e.g., latency\nreduction) while reducing the capacity of DNN models which\nis proven to have a signiﬁcant impact on model’s accuracy\n[108]. Therefore, we need to devise pruning method with\nhardware-awareness such that the model can be tailored for\nvarious hardware.\nB. Quantization\nNetwork pruning reduces the complexity of DNN models\nby removing redundant weights or channels. However, the\n9\nstate-of-the-art models have more than billions of parameters\nand at the same time during inference a model produces a\nlarge portion of intermediate results (activation/feature maps)\nwhich usually occupy a large memory space. As a result,\nthe huge memory requirement prohibits DNN models from\nimplementing on memory-limited edge devices [127], [128].\nFor example, ResNet-50 [7] has 26 million parameter weights,\ngenerates 16 million activations in one inference, requires\naround 168 MB memory space, and needs at lease 3GB/s\nmemory bandwidth. It is not difﬁcult to see that it is unlikely\nto deploy these state-of-the-art models to edge devices, which\nhave limited storage and computational resources.\nIn this case, quantization becomes a promising approach to\naddress the aforementioned issue, which encodes full-precision\n(FP32) weights and activations with low-precision ones (e.g.,\nFP16, INT8, binary) while preserving even the same level of\naccuracy. Some early work has shown that using FP16 to\ntrain DNN models can reduce the computational cost while\nretaining accuracy [109]. Quantization signiﬁcantly beneﬁts\nDNN models on resource-limited devices, and it is capable of\nﬁtting the whole model into on-chip memory of edge devices\nsuch that the high overhead occurred by off-chip memory\naccess can be mitigated. In addition, since operations with low-\nbit representation usually consume less energy and execute\nfaster, quantization reduces energy consumption and latency\nas well on some hardware platforms [129]. In this section, we\ndiscuss some state-of-the-art quantization studies.\nWu et al. [110] propose a uniﬁed quantization framework,\nwhich improves the quantization performance by minimiz-\ning the estimation error of each layer’s response. On this\nbasis, an error correction training strategy is included. Jain\net al. [121] propose a trained uniform quantization method\nfor accurate and efﬁcient neural network inference on ﬁxed-\npoint hardware. Jacob et al. [118] quantize models into 8-\nbit integer and propose a quantize-aware training method to\neliminate the accuracy loss caused by quantization. Instead of\nquantizing all weights in a DNN, IQN [115] adopts a group-\nwise quantization method to gradually quantize weights of a\nDNN model. The advantage of IQN is that it is able to derive\nthe quantized model without accuracy loss.\nThe above works uniformly convert all weights and activa-\ntion into the same low-bit representation, but many emergent\nhardware and accelerators support mixed precision operations,\ne.g., Nvidia Turing GPU architecture supports 1-bit, 4-bit, 8-\nbit and 16-bit arithmetic operations [130]. This provides a\nmore effective and ﬂexible way to quantize weights and activa-\ntions of a DNN model. Wang et al. [122] propose a hardware-\naware automated quantization approach, namely HAQ. HAQ\nexploits reinforcement learning to select different quantization\nwidth for each layer upon a target hardware. Additionally, the\nhardware architecture is involved into the learning loop, so\nthat it can directly reduce the inference latency, energy and\nstorage on the target hardware. HAWQ [123] also considers\nto quantize a full-precision model into a mix-precision model\nwhere the quantized precision is determined for each layer\nbased on Hessian matrix.\nSome works use binary or ternary quantization to maximally\ncompress DNN models. Then, binarized or ternarized network\ncan use cheap bit operation to boost the efﬁciency on dedicated\nhardware [131]. In [111], Courbariaux et al. propose the\nbinaryconnect, which targets to transform the full-precision\nweights into the binary format. A very straightforward bina-\nrization method would be based on the sign function:\nwb =\n(\n+1 if w ≥0\n−1 otherwise\n(1)\nwhere wb is the binary weight and w is the full-precision\nweight. Due to the fact that this is a deterministic operation,\naveraging the discretization over the many input weights of\na hidden unit could compensate for the loss of information.\nAn alternative that allows a ﬁner and more correct averaging\nprocess to take place is to binarize stochatically, which helps\nto improve the model generalization capability:\nwb =\n(\n+1 with probability p = δ(w)\n−1 with probability 1 −p\n(2)\nwhere δ is the hard sigmoid formula:\nδ(x) = clip(x + 1\n2\n, 0, 1) = max(0, min(1, x + 1\n2\n))\n(3)\nIntuitively, applying a binarized method is an easy way\nto quantize the full precision weights. However, this will be\nharder for the training process to converge due to the highly\ndiscrete parameter space, which drastically degrades the model\nperformance.\nLater, Rastegari et al. [114] present an efﬁcient approxima-\ntion strategy, which constrains the full precision weights to\n+1 and −1 instead of directly rounding them. With a scaling\nfactor α, an convolutional operation can be approximated as\nfollows:\nI ∗W ≈I ∗(αB) = αI ∗B\n(4)\nwhere I is the input image and B ∈{−1, +1} is the binarized\nweight. Other binarized methods are proposed to improve the\ndegraded accuracy caused by binarization like [113], [116],\n[119]. XOR-Net [126] takes the implementation of BNNs\ninto account and simpliﬁes the number of instructions used\nin binary dot product to accelerate BNN inference on edge\ndevices.\nInspired by [114], Li et al. [112] propose ternary weight\nnetworks (TWNs) with weights constrained to {−1, 0, +1},\nwhich minimizes the Euclidian distance between full precision\nweights W and the ternary weights Wt along with a scaling\nfactor α. Here the quantized weights are obtained with a\nthreshold-based ternary function:\nWt\ni =\n\n\n\n\n\n+1, if Wi > ∆\n+0, if |Wi| ≤∆\n+0, if Wi < −∆\n(5)\nwhere ∆is a positive threshold parameter. Thus, the optimiza-\ntion objective can be formulated as follows:\nα∗, Wt∗= arg min J(α, Wt) = ||W −αWt||2\n2\n(6)\n10\nBy addressing the above convex optimization problem, we can\nobtain the approximately optimal α∗and ∆∗:\n(\nα∗\n∆=\n1\nI∆\nP\ni∈I∆|Wi|,\n∆∗= arg max\n1\n|I∆|(P\ni∈I∆)|Wi|2\n(7)\nwhere I∆denotes the number of elements in {i||Wi| > ∆}.\nOn top of TWNs, Zhu et al. [117] further introduce two inde-\npendent quantization scaling factors for positive and negative\nweights in each layer, respectively, to improve accuracy of\nternary quantization.\nQuantization are usually achieved via post-training quanti-\nzation which is regarded as the most prevalent method cur-\nrently. The mainstream DNN frameworks like Pytorch [132]\nand Tensorﬂow [133] all support post-training quantization.\nFig. 6 illustrates the procedure of post-training quantization.\nFirst, we should ﬁnd a proper encoding algorithm, which quan-\ntize both full-precision (e.g., FP32) activation results a and\nnetworks weights w into the low-precision (e.g., INT8). Then,\nthe operation (e.g., Convolution) is performed as usual, where\nthe derived outputs will be further relaxed to the full-precision\nformat which can be conducted together with the scaling factor\n(e.g., α in Eq. 4) with respect the quantizer. Post-training\nquantization is ﬂexible and can be broadly applied to existing\nDNN models. However, since the weights and activations\nare quantized into discrete values, we cannot use stochastic\ngradient descent to update the weights. Consequently, the\nquantized models may suffer from signiﬁcant accuracy loss.\nTherefore, some studies [120], [124], [125] aim to effectively\ntrain a low-precision, compact model. Zhang et al. [125] adopt\nan auxiliary full-precision model to facilitate the training of\nits quantized counterparts.\nRecently, the robustness of DNN models, i.e., robust to\nadversarial examples, is a burgeoning topic in DNN research\n[134]. For edge systems, robustness is a critical metric espe-\ncially for some safety-critical systems, like self-driving cars,\nUAVs, robots, etc. The failure or mispredication may lead\nto catastrophic results. Recently, Lin et al. [135] study the\neffect of quantization on the DNN robustness and propose\nthe Defensive Quantization (DQ) that addresses the robust\nissue of quantized models, where DQ can maintain adversarial\nrobustness and model performance at the same time. Moreover,\nGong et al. [136] consider to quantize network with the\nobjective of reducing energy consumption.\nDiscussion: Table V summarizes the quantization approaches\ndiscussed in this section. Quantization has become a standard\nmeans to compress memory-hungry DNN models for resource-\nconstrained edge systems. Many vendors develop their own\ntools and hardware to effectively quantize models and support\nefﬁcient execution of quantized models, like Nvidia Ten-\nsorRT4, Tensorﬂow Lite5, OpenVino6, etc, and some real-\nworld applications based on quantized models are emerging\nsuch as [137]. For edge intelligence systems, we have seen\na new trend that quantization will work with other model\n4https://developer.nvidia.com/tensorrt\n5https://www.tensorﬂow.org/lite\n6https://software.intel.com/content/www/us/en/develop/tools/openvino-\ntoolkit.html\nTABLE VI\nTHE CLASSIFICATION OF KNOWLEDGE DISTILLATION METHODS\nMethodologies\nDistillation Transfer\nNumber of Teachers\nYear\nRemero et al. [139]\nFrom intermediates\nSingle\n2014\nHinton et al. [140]\nFrom logits†\nSingle\n2015\nZagoruyko et al. [141]\nFrom intermediates\nSingle\n2016\nTarvainen et al. [142]\nFrom logits\nMultiple\n2017\nPolino et al. [143]\nFrom logits\nSingle\n2018\nRavi et al. [144]\nFrom logits\nSingle\n2019\nLi et al. [145]\nFrom logits\nSingle\n2020\nChung et al. [146]\nFrom intermediates\nSingle\n2020\nLiu et al. [147]\nFrom intermediates\nMultiple\n2020\n† Following [140], we use logits to denote the knowledge from the output\nof the neural network, i.e., from the last layer.\ncompression techniques as well as the new NAS methods to\nderive the most efﬁcient and compact models, for example,\nAQP [138] which combines NAS, quantization, and pruning\nto design efﬁcient DNNs. Such method will become a new\nstandard to design edge DNN models.\nC. Knowledge Distillation\nKnowledge distillation is another technique to conduct\nmodel compression, where a more compact student model\ncan learn the knowledge from a complicated and powerful\nteacher model. Bucila et al. [148] ﬁrst propose the concept\nof knowledge distillation, and Hinton et al. [140] generalize\nknowledge distillation and apply it to DNNs.\nThe core idea of knowledge distillation is to train a compact\nmodel (student) with the assistant of a complicated, pre-trained\nmodel (teacher). During training, the student model exploit\nthe conventional method to train the model and obtain a loss\naccording to the one-hot class distribution, e.g., [0, 0, 1, 0],\nnamely hard targets and at the same time the knowledge\nfrom the teacher model is distilled and transferred to the\nstudent model by calculating a new loss in which the target\nis the probability distribution of predicted class P from the\nteacher model, e.g., [0.1, 0.21, 0.6, 0.09], namely soft target.\nNevertheless, the probability of the correct class dominates the\nprobability distribution generated by the teacher network (e.g.,\n[0.97, 0.1, 0.0, 0.2]), which signiﬁcantly limits the knowledge\ntransferring capability. To alleviate this issue, Hinton et al.\n[140] propose softmax temperature in which temperature T\nis to soften the generated probability distribution. Intuitively,\na larger T leads to a ‘softer’ probability distribution (e.g.,\n[0.4, 0.2, 0.2, 0.2]). Hence, we are able to formulate the soft-\nmax with temperature as follows:\nP = {pi|\nexp( zi\nT )\nP\nj exp( zj\nT )}\n(8)\nwhen T is set to 1, it is the original softmax function. Please\nnote that we refer the softmax function with temperature T\nas δT for simplicity. Therefore, we can formulate the overall\nloss function as:\nL(x; W) = α ∗F(y, δ1(zs)) + β ∗F(δT (zt), δT (zs))\n(9)\nwhere F denotes the cross-entropy function. α and β are two\nbalancing factors. zs and zt represent the output logits from\nstudent model and teacher model, respectively. y is the ground\n11\nFig. 7. Overview of Knowledge Distill.\ntruth. An illustration about how knowledge distillation works\nis shown in Fig. 7.\nAfter [140], many efforts are made towards improving\nthe performance of knowledge distillation. The work in et\nal. [142], [147] extend the number of teacher models from\none to multiple. However, there exist performance difference\namong those teacher models. To tackle this issue, they propose\nto assign different weights for each teacher models, and\nthen weighted-average probability distributions from different\nteachers are applied to supervise the student model. Combined\nwith quantization, Polino et al. [143] introduce the quantized\ndistillation, which leverages distillation during the training\nprocess by incorporating knowledge distillation loss. Ravi\n[144] introduces a neural projection approach to design and\ntrain efﬁcient on-device neural networks. Preceding to the\nprediction, input instances are transformed into binary repre-\nsentations, which signiﬁcantly reduces the memory consump-\ntion. Afterwards, the prediction weights are learned by knowl-\nedge distillation to achieve higher generalization capability.\nIn [145], Li et al. propose to use knowledge distillation to\nefﬁciently compress models, where the uncompressed model\nand compressed model are considered as a teacher-student pair.\nThis new method can avoid the time-consuming ﬁne-tuning\nafter pruning and achieve data efﬁciency.\nThe above works only use the knowledge from the outputs\nof of the last layer in the teacher model. Can the intermediate\nknowledge help to obtain a better model? Remero et al. [139]\nadopt knowledge distillation to train a compact model, namely\nFitNets. The main idea in FitNets is to train a deeper and\nthinner student with the knowledge transferred from the shal-\nlower and wider teacher model. Different from the previous\nworks, the knowledge in FitNets is not only from the ﬁnal\noutputs but also from intermediate feature representations of\nthe teacher model. By doing so, the student model in FitNets\nmimics or imitates the teacher model from different granularity\nlevels. Similarly, Zagoruyko et al. [141] introduce the attention\ntransfer strategy to mimic the attention maps of a powerful\nteacher network, which proves to improve the performance of\nthe student network.\nDiscussion: Table VI summarizes the works discussed in this\nsection. As identiﬁed in literature [149], knowledge distillation\nprovides several beneﬁts for small network models. Accuracy:\ndistilling knowledge from large networks can improve the\naccuracy of small models which may be directly applicable\nto edge systems. Transferability [150]: Knowledge distillation\ndemonstrates better transferability for the small models, i.e.,\nfor a given dataset, learning the small model from knowledge\ndistillation outperforms learning it from scratch. This feature\nis really instrumental to design lightweight models for edge\nsystems, because for some edge systems installed in special\ncontexts, there lacks a huge amount of high-quality data to\ntrain an accurate model. Thus, knowledge distillation facil-\nitates training of a competitive, compact model with small\ndataset. Moreover, knowledge distillation can also be used to\ndesign robust networks [151]. Knowledge distillation shows\nseveral beneﬁts for edge systems and we envision knowledge\ndistillation will be gradually integrated with other techniques\nlike NAS to derive accurate, compact networks.\nV. HARDWARE-AWARE NEURAL ARCHITECTURE SEARCH\nModel compression provides a ‘large-to-small’ method to\ngenerate an efﬁcient and compact model from a complex\nmodel for edge devices. Although it has shown its success\nin reducing latency and model size, the accuracy of the\npruned model is inherently upper bounded by the pre-trained\nmodel, i.e., the compressed model cannot have better accuracy\nthan the pre-trained model. Moreover, network compression\ninvolves a costly and time-consuming compression-retraining\nprocedure to retain the accuracy of the pruned model. At the\nsame time, as we are witnessing the shift of DNN designs\nfrom manual design to automatic search, i.e., neural architec-\nture search, which has demonstrated its capability to design\nmore accurate DNNs without tedious parameter tuning, this\nimmediately raises an appealing question:\nCan NAS directly design hardware-efﬁcient, accurate\nneural architectures?\nAn increasing attention is paid to hardware-aware NAS and\nsome works exploit NAS to design hardware-efﬁcient DNNs.\nTan et al. [53] propose a hardware-aware NAS framework,\ndubbed MnasNet, in which both latency and accuracy are\nformulated into the reward function of reinforcement learning\nalgorithm and the latency is directly measured from the target\nmobile device. The reward is shown in Eq. (10).\nACC(m) ×\n\"\nLAT(m)\nT\n#w\n(10)\nwhere m is the obtained network, T is the expected latency\nand w is a variable to adjust the weight of latency in this\nreward. ACC and LAT are the real accuracy and latency of\nnetwork m, respectively. MnasFPN [158] is an extension of\nMnasNet which, instead of searching classiﬁcation model,\ntargets directly searching a network for object detection. Dai et\nal. [165] present ChamNet which deploys evolution algorithm\n(EA) and three predictors, i.e., energy predictor, accuracy\npredictor and latency predictors, to effectively and efﬁciently\nsearch for a DNN model on a target platform.\nThe RL-based or EA-based NAS frameworks explore the\noptimal architecture in a large, discrete search space, so they\ndemand huge amount of computational resource and take thou-\nsands of GPU days [166] to search for a neural architecture.\n12\nTABLE VII\nCOMPARISON OF DIFFERENT HARDWARE-AWARE NAS APPROACHES. HERE, COST REFERS TO THE TIME SPENT TO SEARCH THE NETWORK, WHILE WE\nUSE GPU Hour AS UNIT.\nHardware Evaluation\nSearch method\nCost (GPU days)\nMetrics\nTarget\nEdge Systems\nMnasNet [53]\nMeasure\nRL\n40000\nLatency\nImage Classiﬁcation\nYes\nProxylessNAS [152]\nPredictor\ngradient\n200\nLatency\nImage Classiﬁcation\nYes\nEdgeNAS [153]\nPredictor\ngradient\n-\nLatency\nImage Classiﬁcation\nYes\nSPNAS [154]\nLUT\ngradient\n0.2 †\nLatency\nImage Classiﬁcation\nYes\nFBNet [155]\nLUT\ngradient\n-\nLatency\nImage Classiﬁcation\nYes\nDenseNAS [156]\nLUT\ngradient\n3\nLatency\nImage Classiﬁcation\nYes\nNAS-FPN [157]\n-\nRL\n-\n-\nObject Detection\nYes*\nMnasFPN [158]\nMeasure\nRL\n-\nLatency\nObject Detection\nYes\nNAS-FCOS [159]\n-\nRL\n-\n-\nObject Detection\nNo\nAuto-FPN [160]\n-\ngradient\n-\n-\nObject Detection\nNo**\nAdversarialNAS [161]\n-\ngradient\n24\n-\nGAN\nNo\nSpArSe [162]\n-\ngradient\n24\nMemory\nGAN\nYes\nOFA [163]\nMeasure***\n-\n-\nMem/Lat\nImage Classiﬁcation\nYes\nMCUNet [164]\nMeasure***\n-\n-\nMem/Lat\nImage Classiﬁca tion\nYes\n† This low search cost is due to that SPNAS only searches for 8 epochs on a subset of ImageNet.\n*Although NAS-FPN is a hardware-agnostic approach, NAS-FPN devises a lite version for resource constrained systems.\n** Auto-FPN incorporate hardware-agnostic resource constraint like FLOPs, which cannot translate the runtime complexity upon target edge systems.\n*** These two methods train an over-parameterized and large network which is used to sample different small networks, and the over-parameterized\nnetwork is designed without consideration of hardware. However, sampled networks can be directly measured on devices.\nAs estimated in [152], MnasNet needs 40,000 GPU hours to\nsearch for the optimal network. It hinders users with limited\nresource to search for a DNN upon their target hardware\ndevices. Thus, some studies aim to reduce the search cost for\nhardware-efﬁcient NAS. ProxylessNAS [152] uses a gradient-\nbased method to design hardware-aware neural architectures.\nInstead of measuring real latency on the target platform, it\npresents a latency-predictor to facilitate exploration of the op-\ntimal conﬁguration for neural architectures. It greatly reduces\nthe search cost to 200 GPU hrs while ﬁnding efﬁcient models\nwith competitive accuracy. Li et al. [167] consider both latency\nand accuracy as their NAS objectives and use the concept of\npartial order pruning to reduce the search space such that\nit can signiﬁcantly reduce the searching time and achieve a\ngood trade-off between accuracy and latency. RCNAS [168]\nformulates the resource-constrained neural architecture search\nproblem as a submodularity function problem which is known\nto be NP-Hard but has good heuristic algorithms to approx-\nimately solve this problem. FBNet et al. [155] introduces\nDNAS, which is based on the differential neural architecture\nsearch (DARTS) method [166], but instead of just searching\nfor an optimal cell in DARTS, DNAS searches for an optimal\nsetting for each layer within the network. DNAS also takes\nlatency as their goal, where a look-up-table is set up for\nlatency prediction. EdgeNAS [153] proposes a novel NAS\nmethod to search efﬁcient and competitive DNN model for\nresource constrained edge devices, where a latency predictor is\ntrained from data collected from various architectures on target\nhardware devices and the latency predictor is integrated into\nthe DARTS-similar NAS framework for efﬁciently designing\nDNN models. Besides, SPNAS [154] introduces a single-\npath paradigm, which formulates kernels with different sizes\ninto one big kernel. Similar to DNAS, SPNAS constructs\na LUT in terms of the runtime latency of different kernels\nupon target hardware, which will be incorporated into the\ndifferentiable loss function (similar to DNAS and EdgeNAS).\nAll of the above-mentioned approaches mainly search for\nthe best operators for cells or layers where their width and\ndepth are ﬁne-tuned manually. However, as shown in [50],\nthe width and depth of a DNN have a critical impact on its\naccuracy and latency. Fang et al. [156] propose DenseNAS\nwhich not only searches for the optimal architectures but also\ntheir width and depth conﬁguration. The successful application\nof NAS on image classiﬁcation inspires researchers to explore\nthe potential application of NAS on other CV tasks, like\nNAS-FPN [157] and NAS-FCOS [159] for object detection\nand AdversarialNAS [161] for GAN. However, these methods\nonly consider the accuracy and ignore the performance like\nlatency and power consumption, which are critical for edge\nintelligence systems. Besides, Auto-FPN [160] aims to search\nfor a compact FPN with low FLOPs count, but FLOPs cannot\nnecessarily reﬂect the runtime performance upon target hard-\nware (see Fig. 11). Similar to MnasNet [53], MnasFPN [158]\ndirectly measures the runtime latency on target hardware,\nthereby signiﬁcantly increasing the search cost.\nBesides latency, other metrics are also considered in\nhardware-aware NAS frameworks. SpArSe [162] targets net-\nworks which can be ﬁt into micro-controllers which have\nsmall memory footprint and less computation capability, where\nNAS and pruning techniques are combined to design small-\nmemory networks. Cai et al. [163] propose an once-for-all\n(OFA) framework to train a large super network and then\nsample different size of networks from the super network\nto ﬁt the different hardware platforms. The advantage of\nOFA is that it just needs to train once to generate many\ndifferent DNNs which can be directly applied on diverse\nhardware platforms, thus greatly reducing the training cost and\nCO2 emission. Lin et al. [164] propose MCUNet aiming to\ndesign DNN models for microcontrollers. To ﬁt computation-\nintensive DNN models on microcontollers, MCUNet consists\nof two key parts, TinyNAS, a NAS framework to search\nfor models satisfying different constraints, such as memory,\nlatency and TinyEngine, an efﬁcient inference library.\nDiscussion: Table VII summarizes the works discussed in this\n13\nBlock/Layer Adaptive\nChannel Adaptive\nEarly Exit\nMulti-Branch\nFig. 8. Visualization of four different methods for adaptive models\nsection. NAS deﬁnes a new paradigm to design DNN models,\nfrees DNN designers from tedious hyperparameter tuning and\nautomates the costly DNN design procedure. Especially, in\nfuture edge era, more and more customized DNN models for\nspeciﬁc tasks will be developed and implemented on different\nhardware platforms, e.g., edge systems with intermittent power\nsupply [169]. Nevertheless, for edge intelligence systems, the\nexisting NAS methods suffer from two ﬂaws. First, the ma-\njority of existing methods use platform-independent metrics,\ni.e., FLOPs and MACs as the constraint to design the model.\nHowever, the same model demonstrates signiﬁcant difference\non different hardware [170]. Therefore, such designs may\ngenerate inefﬁcient models for targeting hardware. Second,\nthe existing NAS methods mainly target the models with high\naccuracy while ignoring other important system metrics, e.g.,\nenergy and latency. This leads to the searched model with\nlow efﬁciency and high energy consumption. For edge intelli-\ngence systems with diverse tasks and hardware architectures,\nwe need hardware-aware NAS methods to efﬁciently tailor\ncompetitive DNN models for a speciﬁc hardware platform.\nMoreover, to achieve an optimal design for future edge intelli-\ngence systems, NAS may need to take advantage of other model\ncompression techniques like network pruning, quantization,\nand knowledge distillation, to have a holistic and efﬁcient\ndesign paradigm, e.g., [138].\nVI. ADAPTIVE MODELS\nIn previous sections, we discuss the different ways to design\nlightweight DNN models for edge systems, all of which\ngenerate a static DNN model, i.e., the model does not change\nits execution behavior during run-time. However, during the\nrun-time, DNN applications share the computational units\nand communication bandwidth with other applications and\nas a result the availability of computing and communication\nresources may signiﬁcantly affect DNNs’ inference time. In\naddition, some edge systems are energy-constrained, battery\npowered or supplied by sustainable energy, like solar energy\n(intermittent computing [171]), and in these cases energy\nvariance will change the system’s status, such as scaling down\nfrequency and power which affects the inference time of DNN\napplications as well.\nFig. 9.\nAttention-based methods only conduct convolution operations on\n‘important’ areas to reduce the computational cost, where the red line\nhighlights the area of interest.\nSuch variance may inﬂuence the quality of service (QoS)\nof applications without rigorously temporal requirement, like\nvoice recognition, face recognition, machine translation, etc\n[20] and on the other hand may lead to catastrophic conse-\nquence for those with rigorously temporal requirement, (i.e.,\nreal-time requirement), like autonomous driving, UAV, etc.\nExecution variance of DNN applications require DNN models\nto be adaptive to the different input data and system status\n(like low power mode) for guaranteeing certain QoS or real-\ntime performance. Adaptive DNN models will be useful and\npractical for edge systems under dynamic environments and\nprior review articles rarely discuss this topic in a systematical\nway.\nWe, in this section, review some important techniques of\nadaptive DNN models which are applicable to edge systems.\nSome early works identify that for different input images, not\nall channels or layers of a DNN model are needed to make\naccurate prediction [172]. This key observation serves the\ntechnical foundation for adaptive models, i.e., a network may\nselectively activate its channels and layers per input basis. The\npartially activated network can achieve higher efﬁciency than\nthe originally full network without accuracy loss. Some initial\nworks on this topic are called conditional computation [172],\n[173], where the main objective of conditional computation is\nto enhance the capacity of a network while not signiﬁcantly\nincreasing the computational cost. However, for edge systems,\nthe efﬁciency is our top priority, and we brieﬂy classify\nadaptive models into ﬁve categories:\n• Block/Layer Adaptive: This method selects a portion of\nblocks/layers to conduct DNN inference as shown in Fig\n8;\n• Channel Adaptive: This method deploys a portion of\nchannels of each layer to conduct DNN inference as\nshown in Fig 8;\n• Early Exit: This method uses the intermediate result from\nearly layers to conduct prediction and skips the left layers\nin the network as shown in Fig 8;\n• Multi-branch: This method has diverse kernels for ex-\ntracting features and uses a combination of kernels to\nconduct DNN inference as shown in Fig 8;\n• Attention: This method employs attention mechanism\n[174] to ﬁnd the importantly spatial locations of images\nand only conduct computation-expensive convolution on\nthese area as shown in Fig 9.\nRuntime Network Pruning (RNP) [175], a channel adaptive\n14\nmethod, is conceptually similar to structure pruning discussed\nin Section IV-A. However, different from the methods in\nSection IV-A which aim to produce a static model at design\ntime, RNP is a runtime and dynamic pruning method, which\nuses RL to learn a policy to determine which ﬁlters should\nbe used according to the input data. Feature Boosting and\nSuppression (FBS) [176] exploits channel saliency to judge\nwhich channels can be skipped, where a low overhead predic-\ntor is presented to predict channel saliency according to feature\nmaps of the previous layer. Bejnordi et al. [177] propose a\nchannel-gate module to partially activate channels according\nto input. SlimmableNet [178], [179], also a channel adaptive\nmethod, proposes a method to train a network with different\nchannel width conﬁgurations and the network can actively vary\nits channel conﬁguration on the ﬂy.\nSome works exploit attention mechanism to improve com-\nputational efﬁciency. Attention technique mimics human visual\nsystems to locate their focal point on the important area and\nup-weights the areas of interest to improve the model accuracy.\nAttention has shown its successful application in CV [174] and\nNLP [180]. From efﬁcient perspective, instead of up-weighting\nthe important areas, some works avoid the computation-\nexpensive convolution on the less important areas to improve\nthe efﬁciency of DNN inference. SBnet [181] divides image\ninto blocks with ﬁxed size, and only convolves with blocks\nof interest. SACT [182] exploits adaptive computation time\n(ACT) technique [183] to vary the computation per spatial\nlocation of input images. Verelst and Tuytelaars [184] use\nGumble-softmax [185] to train a pixel-wise gate to identify\nthe region of interest while Chen et al. [186] propose to\ncombine attention with multi-branch technique to improve the\ncapacity of lightweight models. CGNets [187] is conceptually\nsimilar to attention mechanism, where few feature maps are\nused to identify the regions of interest within intermediate\nactivations and the convolutional computation is only applied\nto the regions of interest. CGNets also provide a hardware\nimplementation for accelerating its design.\nSome works observe that for simple input data, the model\ncan use intermediate results from early layers to predict the\nclass without affecting the accuracy. This is called early-exit.\nAdaptive Neural Network [188] learns a policy to determine\nwhether given an input data the model can use intermedi-\nate results from early convolutional layers to make accurate\nprediction. Based on this strategy, they also extend their\npolicy to select a model from a set of models with different\naccuracy/latency trade-off. BranchyNet [189] uses early-exit to\nskip some layers for easy input images, where a threshold is\ngiven to evaluate the conﬁdence of predictions from early-exit.\nAnother common method to achieve adaptivity is multi-\nbranch which is conceptually analogous to mixture of ex-\nperts [190] [191], where each branch has some convo-\nlution kernels and the multi-branch network selects some\nbranches/convolution kernels per input to conduct inference.\nHydraNet [192] presents a multi-branch network which can\nselect the best k branches to conduct inference on per-input\nbasis. Condconv [193] proposes a conditional convolutional\nlayer which combines different convolution kernels based on\nthe input. Dynamic deep neural networks (D2NN) [194], a bit\nTABLE VIII\nCOMPARISON OF DIFFERENT ADAPTIVE DNN MODELS\nMethods\nAdaptive Scope\nTraining\nYear\nRNP [175]\nChannels\nRL\n2017\nBenjordi et al. [177]\nChannels\nGradient\n2019\nSlimmableNet [178]\nChannels\nGradient\n2019\nFBS [176]\nChannels\nGradient\n2018\nCGNets [187]\nChannels\nGradient\n2019\nAdaptiveNN [188]\nEarly-exit\nGradient\n2017\nBranchyNet [189]\nEarly-exit\nGradient\n2017\nD2NN [194]\nMulti-branch\nRL\n2018\nCondconv [193]\nMulti-branch\nGradient\n2019\nHydraNets [192]\nMulti-branch\nGradient\n2018\nSkipNet [196]\nLayers\nRNN/RL\n2018\nBlockDrop [197]\nLayers\nRL\n2018\nSBnet [181]\nAttention\nGradient\n2018\nVerelst and Tuytelaars [184]\nAttention\nGradient\n2020\nChen et al. [186]\nAttention\nGradient\n2020\ndifferent from [192], [193], formalize a network as a directly\nacyclic graph (DAG) which has different operations for each\nnode within DAG and the network selects an effective and\nefﬁcient execution path from DAG according to its input.\nD2NN is trained in an end-to-end manner with assistant of\nRL.\nResNet [7] is found to be tolerant to the removal of some\nblocks or layers without affecting the predictive accuracy\n[195]. Therefore, some works investigate how to bypass\nblocks/layers of a ResNet-similar network. SkipNet [196] uses\nprevious layer’s activation to determine whether the subse-\nquent layer is required for the inference, where RNN and RL\nare used to control the executed blocks . Similarly, BlockDrop\n[197] uses RL to train a policy network to determine the block\nconﬁgurations used for different input images. ConvNet-AIG\n[198] learns a gate module for each block of ResNets to select\nthe inference blocks conditioned on input images.\nDiscussion: Table VIII summarizes the works discussed in\nthis section. Adaptive models are attracting more attention\nfrom researchers due to the computation limit of resource\nconstrained systems and highly dynamic environments. The\nadvantages of adaptive models are twofold: 1) Adaptive mod-\nels provide a ﬂexible way to achieve trade-off between accu-\nracy and efﬁciency on the ﬂy; 2) for some lightweight models\nadaptive models can increase the capacity of models, thereby\nimproving the accuracy of the model without increasing the\ncomputational cost (because it only needs to partially activates\nthe network). The existing adaptive models mainly consider\nthe dynamics of input images, i.e., fewer channels or layers\nfor ‘easy’ images and more channels or layers for ‘complex’\nimages. However, on edge systems, some system dynamics,\nmemory contention, cache miss, bandwidth unavailability, etc,\nalso impact the execution of DNN application and this part\nis ignored in current research outcomes. For edge intelligence\nsystems, it is of importance to take into account both dynamics,\ngenerating an adaptive edge intelligence system which can\nguarantee a certain level of QoS or meet real-time requirement\nunder dynamic environments.\nVII. DISCUSSION AND ENVISIONS\nDNN-based AI applications are increasingly integrated into\nour life and work and will greatly revolutionize the way we\n15\n0\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy\nEnergy\nLatency\nMemory\nSecurity\nFig. 10. The real design space for Edge intelligent systems. There are various\nscenarios which have different design concerns.\nlive and work. Some AI applications rely on super computing\npower to complete complex tasks while others will operate\nin proximity to data and end-users to help us live ‘smarter’\nand work ‘intelligently’. Edge intelligence, deemed as one\nof the most important AI trends, will make the proximity\nAI possible and accessible. In the ﬁrst DNN decade (2011-\n2020), researchers around the world have designed many\ncompelling DNN models, applicable to various domains, like\nnature language processing [199], AI-assisted medicine [200],\nrobots, etc. As reported in [201], over the years to come, the\ntraining/inference ratio of DNN models will increase to 1:5\nfrom current 1:1 and enterprises will gradually add AI services\nto their core business so that they can proﬁt from AI research\nand in turns AI applications can beneﬁt the whole society. We\ncan envision that the next important development for DNNs\nwill be the practical deployment, especially like edge devices.\nTo achieve this, we may need new DNN design methods, novel\nhardware [129] and the seamless cooperation between software\nand hardware.\nIn this section, based on our observations, we identify some\nimportant topics which we believe will play a pivotal role in\npushing DNNs to edge.\nA. Different metrics oriented DNN models\nThe accuracy improvement has been the highest priority in\nDNN research, where different models are justiﬁed by their\naccuracy increase, likewise a couple of percentiles. However\nwhen they are deployed on real systems, more metrics such\nas latency and power also matter. Moreover, DNN models are\nfound to be vulnerable to adversary attack [202], so security\nis emerging as another new design concern for ML systems.\nTherefore, when designing DNN models for the emerging edge\nintelligent systems, the objective should be focused not only\non accuracy but also on other critical metrics to have an overall\nconsideration. More speciﬁcally, edge DNN models are really\napplication-dependent or context-dependent, and it needs to\nﬁnd a good trade-off within the multi-dimension design space\nas shown in Fig. 10. As seen in Fig. 1, edge computing spans a\nwide spectrum. Some scenarios like UAVs, self-driving cars,\nrobots, etc, have restrict requirements for accuracy, latency,\nand security, hence we may need a design point which is able\nFig. 11.\nLatency vs FLOPs: The latency measurements of 10000 different\narchitectures with different FLOPs count. The experimental platform is Nvidia\nJetson Xavier, a widely used edge platform for robots and autonomous driving.\nto strike the balance within the design space. To do so, we need\nto correctly use metrics or deﬁne new combination metrics for\nedge DNN systems.\nFor example, DNN research mainly adopts FLOPs as the\nindicator of model complexity, while FLOPs are used as\nconstraints for network design and compression. However, the\nnumber of FLOPs may not directly translate to its latency,\nbecause DNN models have diverse architectures which may\ndemonstrate different effectiveness on different hardware plat-\nforms. Fig. 11 shows the latency of 100000 network archi-\ntectures generated using DARTS [166] which are measured\non Nvidia Jetson Xavier [203], where we can see that the\nmodels with the same latency may differ in the number of\nFLOPs by up to 26% (from 461 FLOPs to 623 FLOPs with\nlatency 120ms) and the models with the same FLOPs perform\ndifferent latency ranging from below 80ms up to 120ms.\nTherefore, we should carefully use the indirect metric to guide\nedge DNN design. In addition, design space for DNN is too\nlarge to have an exhaustive search. More design concerns will\nexacerbate this design complex issue. However, we still lack\nthe measurement of the trade-off between each metric and this\nwill lead to either high searching cost or sub-optimal design\nresult. Thus, it will be necessary to deﬁne some combined\nmetrics which can quantitatively evaluate the trade-off between\ndifferent metrics, like energy-delay-product for conventional\napplications on CPU.\nB. Hardware-software co-design\nThe high complexity of DNN models has spurred hardware\narchitecture innovation to boost DNN training and inference\nover the past ﬁve years. The academia and industry have\na consensus that the breakthroughs in hardware architecture\nwill bring DNNs to a new level and boost DNN adoption\n[15] [205]. However, different hardware accelerators features\ndiverse underlying characteristics, and the majority of DNNs\nwere designed without consideration of underlying hardware\nfeatures. We would like to call it hardware-agnostic design.\nIt is known that accuracy of DNN models are highly related\nto its width (channels or feature maps) and depth (layers\nor blocks) [50], DNN models designed without hardware\n16\nFig. 12.\nEffectiveness of Hardware-Software Co-Design Paradigm as shown in FNAS [204]. FNAS-loose, FNAS-medium, and FNAS-tight denote three\ndifferent design patterns in terms of the resource constraint.\nconsideration may not fully utilize the underlying hardware.\nFig 13 shows the latency of four state-of-the-art DNN mod-\nels, Inception v3 [206], ResNet50 [7], MnasNet [53] and\nMobileNet V3 [45] on three edge devices, Intel Neural\nComputing Stick 2 [207], Nvidia Jetson TX2 [203], Google\nEdge TPU [21] and one desktop GPU, Nvidia Quadro GV100.\nFor MobileNet V3 on Edge TPU, we use optimized models\nprovided by Google, which has more MACs than the original\nmodels (990M vs 210M). From the experimental results, we\nsee that MnasNet and MobileNet V3 on Edge TPU perform\nvery low latency, even lower than the high-end GPU, because\nthese two models are speciﬁcally optimized for Edge TPU.\nThese two models on Edge TPU also consume higher power\n(5W) than Inception V3 and ResNet50 (4.6W). Based on the\nobservations, we conjecture that since these two models are\ndesigned and optimized for Edge TPU, they are able to better\nutilizes the parallelism of underlying hardware, thereby having\nlower latency and higher power consumption. In addition, as\nshown in [170], the same neural architecture demonstrates\nup to 62x difference on modern mobile devices in terms of\ninference time.\nSome research strives to have a hardware-aware NAS, like\nFNAS [204], which directly incorporates resource constraints\nduring implementing DNNs on FPGAs. The experimental re-\nsults of FNAS are illustrated in Fig. 12. 7Z020 and 7A50T de-\nnote the low-end and high-end FPGAs, respectively. FNAS can\ndesign models according to different resource constraints, i.e.,\nFNAS-loose, FNAS-medium, and FNAS-tight. Thank to its\nhardware-aware method, FNAS can achieve the same accuracy\nlevel under different resource constraints, while signiﬁcantly\nreducing the search cost and inference latency.\nThese together signal the importance of system-level\nhardware-software co-design. The ﬁrst decade of 21th cen-\ntury witnessed the emergence of system-level design method-\nologies [208] for multicore system design. To alleviate the\nincreasing complexity of multicore systems, different software\nand hardware co-design methods were proposed to elevate\nthe design level to system-level by modeling software and\nhardware so that some tedious and error prone procedures can\nbe avoided. The history may repeat for the emergent edge\nintelligent systems. The large and complex design space of\nedge intelligent systems need hardware-software co-design to\nfacilitate the effective and efﬁcient design and implementation\nof edge intelligent systems.\nTo achieve co-design, we need to determine an effective\ndesign space for DNN models which will be helpful to reduce\nthe costly design time. We have seen some recent efforts\ntowards deﬁning the effective design space for DNN models\n[209], [210]. At the same time, effective hardware modeling\ntechniques are needed. We need metrics like Rooﬂine [211]\nto guide the direction in ﬁnding efﬁcient network upon a\ntarget platform and a standard benchmark which can fairly\nand quantitatively evaluate various DNN models on new\nhardware accelerators, like MLPerf [212] and ParaDNN [213].\nIn addition, to better utilize the underlying hardware, some\nDL compilers may need to be integrated into the co-design\nframework, like TVM [214] and patDNN [215].\nC. Lightweight models for other applications\nCurrently, the majority of works regarding deep learning on\nedge systems target computer vision tasks, i.e., image classiﬁ-\ncation and object detection. We have seen some successful CV-\nbased edge intelligent systems, such as face recognition, object\ntracing on UAVs, navigation on robots, video analytic systems\n[216], etc. However, we also see the success of DNN models\nin other domain, like NLP, machine translation, etc. These\nmodels, like BERT [217], are known to be highly complex,\neven more complicated than DNN models for image classi-\nﬁcation, and a recent study in [9] raises a concern regarding\nthe environment effect of training complex NLP models. As\ndiverse applications will be increasingly implemented on edge\nsystems, we need new methods or frameworks to design light-\nweight DNN models for domains other than computer vision.\nLike [218], an efﬁcient point-voxel CNN is proposed for\nefﬁcient 3D learning, and this model can help to implement 3D\nAR/VR applications and SLAM [219] of autonomous driving\non edge systems. Li et al. [220] recently present a method to\ncompress generative adversarial nets (GAN) [221]. By means\nof compressed GAN, some GAN-based applications, e.g., style\ntransfer, image synthesis, etc, can be efﬁciently implemented\non edge systems. Some recent works study to compress the\ncomplex NLP models such that they can be deployed on\nresource-constrained edge systems [33] [34] [35]. Only few\nefforts are made towards designing lightweight DNN models\n17\nFig. 13. Latency of four state-of-the-art DNN models on four devices.\nof other domains for edge systems, but there is a huge potential\nto exploit such models on edge systems.\nD. Learning on The Edge\nIn this paper, we review many techniques aiming to design\nlightweight models for edge intelligence systems, where the\nmodels are assumed to be trained on powerful servers but\nare deployed on edge devices for inference. However, due\nto high communication overhead, on-time model update and\npossible leakage of conﬁdential data, some edge systems prefer\nto train the model locally, i.e., on-device learning [222]. On-\ndevice learning is a challenging task, because training is a\nmore computation-intensive and memory-hungry procedure\ncompared to inference, whereas edge systems are resource-\nconstrained in many settings. Moreover, limited energy supply\nof some edge systems will make this issue even more difﬁ-\ncult. Although few efforts have been made towards efﬁcient\nlearning at the edge, such as [223], [224], learning at the\nedge is still at its early stage. There still remain a lot of\nissues to be addressed in this topic. Some new methods,\nsoftware frameworks and underlying libraries are needed to\nfacilitate the effective and efﬁcient training on edge systems\nwith consideration of limitation and constraints imposed by\nedge systems, e.g., [128].\nE. The data challenge at edge\nThe success of DNN models heavily relies on high-quality\nand large-scale datasets, such as ImageNet, but for some\nedge applications, e.g., edge surveillance systems in wild life,\ndefect detection in manufacturing process, etc, it is difﬁcult or\nexpensive to collect massive amount of data and label them\nto train a good model. Thus, the majority of edge intelligence\nsystems without large-scale dataset exploit transfer learning\n[225] to learn a competitive model, where a model trained with\nlarge-scale dataset is provided to extract features and then the\nclassiﬁcation layer (fully-connected layer) is further ﬁne-tuned\naccording to domain-speciﬁc dataset such that the model can\nbe adapted for the new domain.\nHowever, since, during the long-term operation, edge sys-\ntems are likely to collect data with different distributions\nfrom the original training data or data pertaining to a new\nclass which is not included in the original training data, we\nmay need to update the model on edge systems in order to\nprovide better prediction performance or infer a new class.\nOn one hand, edge systems can update models locally by\nusing incremental learning [226] [227], where techniques in\n[228] are deployed to improve the accuracy for new data and\ninfer unknown classes. On the other hand, a group of edge\nsystems is able to help each other to improve models by using\nfederated learning [229]. As discussed several times in this\npaper, data privacy is one motivation of edge systems, so it\nmay be impossible to collect data from different edge systems\nand share them with each other. Federated learning (FL) [229]\nis proposed to attack this issue, where instead of sharing the\ndata with a centralized server clients (e.g., an edge system)\nin FL only share the learned gradient with others which will\nnot leak data. Federated learning is a promising solution to\nshare the information among several models or data sources\nwhile still keeping the conﬁdential of data. Thus, FL can be\nused as a powerful tool to connect edge intelligence systems\nto improve their intelligent ability. Recently, few works study\nto employ incremental learning [227] and federated learning\n[230] [231] with edge systems, but the research in this context\nis still in its infancy. The breakthrough in this area will pave\nthe way of ubiquitously adoption of edge intelligence systems\nin our life.\nVIII. CONCLUSION\nThe convergence of edge computing and artiﬁcial intelli-\ngence leads to the concept of edge intelligence. Edge intel-\nligence is in its early stage and needs sustainable efforts.\nThis paper mainly surveys DL techniques which will facilitate\nthe efﬁcient deployment of DNN models on edge systems,\ni.e., lightweight models, network compression, hardware-\naware NAS and adaptive models. We provide some of our\nthoughts about edge intelligence systems and hope this paper\ncan help researchers from edge computing community to\nunderstand the state-of-the-art DL techniques and to explore\nnew opportunities in edge intelligence era. As stated in [40],\nDL algorithms are approaching the computational limits of\ncomputing systems and this probably indicates that designing\nefﬁcient DNN models will soon become a standard not only\nfor edge systems but also all AI systems. Then, designing\nefﬁcient DNN models will become a mainstream.\nREFERENCES\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural\ninformation processing systems, 2012, pp. 1097–1105.\n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in 2009 IEEE conference\non computer vision and pattern recognition.\nIeee, 2009, pp. 248–255.\n[3] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning.\nMIT\nPress, 2016, http://www.deeplearningbook.org.\n[4] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[5] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,\nP. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., “End to\nend learning for self-driving cars,” arXiv preprint arXiv:1604.07316,\n2016.\n[6] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot et al., “Mastering the game of go with deep neural networks\nand tree search,” nature, vol. 529, no. 7587, p. 484, 2016.\n18\n[7] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[8] P. Goyal, P. Doll´ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Ky-\nrola, A. Tulloch, Y. Jia, and K. He, “Accurate, large minibatch sgd:\nTraining imagenet in 1 hour,” arXiv preprint arXiv:1706.02677, 2017.\n[9] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, 2019, pp.\n3645–3650.\n[10] V. Kepuska and G. Bohouta, “Next-generation of virtual personal\nassistants (microsoft cortana, apple siri, amazon alexa and google\nhome),” in 2018 IEEE 8th Annual Computing and Communication\nWorkshop and Conference (CCWC).\nIEEE, 2018, pp. 99–103.\n[11] S. Baruah, P. Lee, P. Sarathy, and M. Wolf, “Achieving resiliency and\nbehavior assurance in autonomous navigation: An industry perspec-\ntive,” Proceedings of the IEEE, 2020.\n[12] W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge computing: Vision\nand challenges,” IEEE internet of things journal, vol. 3, no. 5, pp.\n637–646, 2016.\n[13] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge\nintelligence: Paving the last mile of artiﬁcial intelligence with edge\ncomputing,” Proceedings of the IEEE, vol. 107, no. 8, pp. 1738–1762,\n2019.\n[14] “Ieee\ncomputer\nsociety’s\ntop\n12\ntechnology\ntrends\nfor\n2020,”\nhttps://www.computer.org/press-room/2019-news/\nieee-computer-societys-top-12-technology-trends-for-2020,\naccess:\n2020-5-14.\n[15] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers et al., “In-datacenter\nperformance analysis of a tensor processing unit,” in Proceedings of\nthe 44th Annual International Symposium on Computer Architecture,\n2017, pp. 1–12.\n[16] Y. Chen, T. Chen, Z. Xu, N. Sun, and O. Temam, “Diannao family:\nenergy-efﬁcient hardware accelerators for machine learning,” Commu-\nnications of the ACM, vol. 59, no. 11, pp. 105–112, 2016.\n[17] S. Han, J. Kang, H. Mao, Y. Hu, X. Li, Y. Li, D. Xie, H. Luo, S. Yao,\nY. Wang et al., “Ese: Efﬁcient speech recognition engine with sparse\nlstm on fpga,” in Proceedings of the 2017 ACM/SIGDA International\nSymposium on Field-Programmable Gate Arrays, 2017, pp. 75–84.\n[18] K. Guo, S. Zeng, J. Yu, Y. Wang, and H. Yang, “[dl] a survey of\nfpga-based neural network inference accelerators,” ACM Transactions\non Reconﬁgurable Technology and Systems (TRETS), vol. 12, no. 1,\npp. 1–26, 2019.\n[19] M. Feldman, “The era of general purpose computers is ending,” The\nNext Platform, 2019.\n[20] C.-J. Wu, D. Brooks, K. Chen, D. Chen, S. Choudhury, M. Dukhan,\nK. Hazelwood, E. Isaac, Y. Jia, B. Jia et al., “Machine learning\nat facebook: Understanding inference at the edge,” in 2019 IEEE\nInternational Symposium on High Performance Computer Architecture\n(HPCA).\nIEEE, 2019, pp. 331–344.\n[21] “Google\nedge\ntpu,”\nhttps://cloud.google.com/edge-tpu/,\naccessed:\n2020-06-25.\n[22] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement\nlearning,” 2017. [Online]. Available: https://arxiv.org/abs/1611.01578\n[23] X. Wang, Y. Han, V. C. Leung, D. Niyato, X. Yan, and X. Chen,\n“Convergence of edge computing and deep learning: A comprehensive\nsurvey,” IEEE Communications Surveys & Tutorials, 2020.\n[24] J. Chen and X. Ran, “Deep learning with edge computing: A review,”\nProceedings of the IEEE, vol. 107, no. 8, pp. 1655–1674, 2019.\n[25] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “Model compression and\nacceleration for deep neural networks: The principles, progress, and\nchallenges,” IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 126–\n136, 2018.\n[26] L. Deng, G. Li, S. Han, L. Shi, and Y. Xie, “Model compression and\nhardware acceleration for neural networks: A comprehensive survey,”\nProceedings of the IEEE, vol. 108, no. 4, pp. 485–532, 2020.\n[27] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, “Efﬁcient processing\nof deep neural networks: A tutorial and survey,” Proceedings of the\nIEEE, vol. 105, no. 12, pp. 2295–2329, 2017.\n[28] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A\nsurvey,” Journal of Machine Learning Research, vol. 20, no. 55, pp.\n1–21, 2019.\n[29] J. Dean, D. Patterson, and C. Young, “A new golden age in computer\narchitecture: Empowering the machine-learning revolution,” IEEE Mi-\ncro, vol. 38, no. 2, pp. 21–29, 2018.\n[30] D. Liu, B. Chen, C. Yang, and A. F. Molisch, “Caching at the\nwireless edge: design aspects, challenges, and future directions,” IEEE\nCommunications Magazine, vol. 54, no. 9, pp. 22–28, 2016.\n[31] P. Mach and Z. Becvar, “Mobile edge computing: A survey on archi-\ntecture and computation ofﬂoading,” IEEE Communications Surveys &\nTutorials, vol. 19, no. 3, pp. 1628–1656, 2017.\n[32] J.-J. Yu, M. Zhao, W.-T. Li, D. Liu, S. Yao, and W. Feng, “Joint\nofﬂoading and resource allocation for time-sensitive multi-access edge\ncomputing network,” in 2020 IEEE Wireless Communications and\nNetworking Conference (WCNC).\nIEEE, 2020, pp. 1–6.\n[33] Z. Wu*, Z. Liu*, J. Lin, Y. Lin, and S. Han, “Lite transformer with\nlong-short range attention,” in International Conference on Learning\nRepresentations, 2020. [Online]. Available: https://openreview.net/\nforum?id=ByeMPlHKPH\n[34] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and\nQ. Liu, “Tinybert: Distilling bert for natural language understanding,”\narXiv preprint arXiv:1909.10351, 2019.\n[35] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, “Mobilebert: a\ncompact task-agnostic bert for resource-limited devices,” arXiv preprint\narXiv:2004.02984, 2020.\n[36] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural\nnetworks,” in Proceedings of the fourteenth international conference\non artiﬁcial intelligence and statistics, 2011, pp. 315–323.\n[37] J. Nagi, F. Ducatelle, G. A. Di Caro, D. Cires¸an, U. Meier, A. Giusti,\nF. Nagi, J. Schmidhuber, and L. M. Gambardella, “Max-pooling convo-\nlutional neural networks for vision-based hand gesture recognition,” in\n2011 IEEE International Conference on Signal and Image Processing\nApplications (ICSIPA).\nIEEE, 2011, pp. 342–347.\n[38] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” arXiv preprint\narXiv:1502.03167, 2015.\n[39] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 7132–7141.\n[40] N. C. Thompson, K. Greenewald, K. Lee, and G. F. Manso, “The\ncomputational limits of deep learning,” 2020.\n[41] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,\nand K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer\nparameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,\n2016.\n[42] D. Gschwend, “Zynqnet: An fpga-accelerated embedded convolutional\nneural network,” Swiss Federal Institute of Technology Zurich: Z¨urich,\nSwitzerland, 2016.\n[43] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-\nlutional neural networks for mobile vision applications,” arXiv preprint\narXiv:1704.04861, 2017.\n[44] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-\nbilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2018,\npp. 4510–4520.\n[45] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan,\nW. Wang, Y. Zhu, R. Pang, V. Vasudevan et al., “Searching for\nmobilenetv3,” in Proceedings of the IEEE International Conference\non Computer Vision, 2019, pp. 1314–1324.\n[46] F. Chollet, “Xception: Deep learning with depthwise separable convo-\nlutions,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017, pp. 1251–1258.\n[47] T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze,\nand H. Adam, “Netadapt: Platform-aware neural network adaptation\nfor mobile applications,” in Proceedings of the European Conference\non Computer Vision (ECCV), 2018, pp. 285–300.\n[48] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufﬂenet: An extremely efﬁ-\ncient convolutional neural network for mobile devices,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition,\n2018, pp. 6848–6856.\n[49] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufﬂenet v2: Practical\nguidelines for efﬁcient cnn architecture design,” in Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2018, pp. 116–131.\n[50] M. Tan and Q. Le, “EfﬁcientNet: Rethinking model scaling for con-\nvolutional neural networks,” in Proceedings of the 36th International\nConference on Machine Learning, ser. Proceedings of Machine Learn-\ning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. Long\nBeach, California, USA: PMLR, 09–15 Jun 2019, pp. 6105–6114.\n[51] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu, and C. Xu, “Ghostnet: More\nfeatures from cheap operations,” arXiv preprint arXiv:1911.11907,\n2019.\n19\n[52] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum,\nand F. Hutter, “Efﬁcient and robust automated machine learning,” in\nAdvances in neural information processing systems, 2015, pp. 2962–\n2970.\n[53] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,\nand Q. V. Le, “Mnasnet: Platform-aware neural architecture search\nfor mobile,” in 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2019, pp. 2815–2823.\n[54] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-\nman, “The pascal visual object classes (voc) challenge,” International\njournal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.\n[55] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in European conference on computer vision. Springer, 2014,\npp. 740–755.\n[56] B. Wu, F. Iandola, P. H. Jin, and K. Keutzer, “Squeezedet: Uniﬁed,\nsmall, low power fully convolutional neural networks for real-time\nobject detection for autonomous driving,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops,\n2017, pp. 129–137.\n[57] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”\narXiv, 2018.\n[58] J. L. Yuxi Li, Jianguo Li and W. Lin, “Tiny-DSOD: Lightweight object\ndetection for resource-restricted usage,” in BMVC, 2018.\n[59] A. Womg, M. J. Shaﬁee, F. Li, and B. Chwyl, “Tiny ssd: A tiny\nsingle-shot detection deep convolutional neural network for real-time\nembedded object detection,” in 2018 15th Conference on Computer\nand Robot Vision (CRV).\nIEEE, 2018, pp. 95–101.\n[60] R.\nJ.\nWang,\nX.\nLi,\nand\nC.\nX.\nLing,\n“Pelee:\nA\nreal-time\nobject\ndetection\nsystem\non\nmobile\ndevices,”\nin\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n31,\nS.\nBengio,\nH.\nWallach,\nH.\nLarochelle,\nK.\nGrauman,\nN.\nCesa-Bianchi,\nand\nR.\nGarnett,\nEds.\nCurran\nAssociates,\nInc.,\n2018,\npp.\n1963–1972.\n[Online].\nAvailable:\nhttp://papers.nips.cc/paper/\n7466-pelee-a-real-time-object-detection-system-on-mobile-devices.\npdf\n[61] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Light-\nhead r-cnn: In defense of two-stage object detector,” arXiv preprint\narXiv:1711.07264, 2017.\n[62] Z. Qin, Z. Li, Z. Zhang, Y. Bao, G. Yu, Y. Peng, and J. Sun,\n“Thundernet: Towards real-time generic object detection on mobile\ndevices,” in The IEEE International Conference on Computer Vision\n(ICCV), October 2019.\n[63] M. Tan, R. Pang, and Q. V. Le, “Efﬁcientdet: Scalable and efﬁcient\nobject detection,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 10 781–10 790.\n[64] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\nonce: Uniﬁed, real-time object detection,” in The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2016.\n[65] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu,\nand A. C. Berg, “Ssd: Single shot multibox detector,” in European\nconference on computer vision.\nSpringer, 2016, pp. 21–37.\n[66] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\nhierarchies for accurate object detection and semantic segmentation,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2014, pp. 580–587.\n[67] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international\nconference on computer vision, 2015, pp. 1440–1448.\n[68] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\ntime object detection with region proposal networks,” in Advances in\nneural information processing systems, 2015, pp. 91–99.\n[69] Z. Shen, Z. Liu, J. Li, Y.-G. Jiang, Y. Chen, and X. Xue, “Dsod: Learn-\ning deeply supervised object detectors from scratch,” in Proceedings\nof the IEEE international conference on computer vision, 2017, pp.\n1919–1927.\n[70] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with\ndeep learning: A review,” IEEE transactions on neural networks and\nlearning systems, vol. 30, no. 11, pp. 3212–3232, 2019.\n[71] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi,\nI. Fischer, Z. Wojna, Y. Song, S. Guadarrama et al., “Speed/accuracy\ntrade-offs for modern convolutional object detectors,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition,\n2017, pp. 7310–7311.\n[72] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. De Freitas, “Pre-\ndicting parameters in deep learning,” in Advances in neural information\nprocessing systems, 2013, pp. 2148–2156.\n[73] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in\nAdvances in neural information processing systems, 1990, pp. 598–605.\n[74] B. Hassibi, D. G. Stork, and G. J. Wolff, “Optimal brain surgeon and\ngeneral network pruning,” in IEEE international conference on neural\nnetworks.\nIEEE, 1993, pp. 293–299.\n[75] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights\nand connections for efﬁcient neural network,” in Advances in neural\ninformation processing systems, 2015, pp. 1135–1143.\n[76] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing\ndeep neural networks with pruning, trained quantization and huff-\nman coding,” International Conference on Learning Representations\n(ICLR), 2016.\n[77] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[78] D. Molchanov, A. Ashukha, and D. Vetrov, “Variational dropout spar-\nsiﬁes deep neural networks,” in Proceedings of the 34th International\nConference on Machine Learning-Volume 70.\nJMLR. org, 2017, pp.\n2498–2507.\n[79] X. Dai, H. Yin, and N. K. Jha, “Nest: A neural network synthesis\ntool based on a grow-and-prune paradigm,” IEEE Transactions on\nComputers, vol. 68, no. 10, pp. 1487–1497, 2019.\n[80] T. Zhang, S. Ye, K. Zhang, J. Tang, W. Wen, M. Fardad, and Y. Wang,\n“A systematic dnn weight pruning framework using alternating direc-\ntion method of multipliers,” in The European Conference on Computer\nVision (ECCV), September 2018.\n[81] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein et al., “Distributed\noptimization and statistical learning via the alternating direction method\nof multipliers,” Foundations and Trends® in Machine learning, vol. 3,\nno. 1, pp. 1–122, 2011.\n[82] T.-J. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convo-\nlutional neural networks using energy-aware pruning,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition,\n2017, pp. 5687–5695.\n[83] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze, “Eyeriss: An energy-\nefﬁcient reconﬁgurable accelerator for deep convolutional neural net-\nworks,” IEEE journal of solid-state circuits, vol. 52, no. 1, pp. 127–138,\n2016.\n[84] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and\nW. J. Dally, “Eie: efﬁcient inference engine on compressed deep neural\nnetwork,” ACM SIGARCH Computer Architecture News, vol. 44, no. 3,\npp. 243–254, 2016.\n[85] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[86] K. Deb, Multi-objective optimization using evolutionary algorithms.\nJohn Wiley & Sons, 2001, vol. 16.\n[87] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning\nﬁlters for efﬁcient convnets,” in International Conference on Learning\nRepresentations, 2016.\n[88] H. Hu, R. Peng, Y.-W. Tai, and C.-K. Tang, “Network trimming: A data-\ndriven neuron pruning approach towards efﬁcient deep architectures,”\narXiv preprint arXiv:1607.03250, 2016.\n[89] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, “Learning structured\nsparsity in deep neural networks,” in Advances in neural information\nprocessing systems, 2016, pp. 2074–2082.\n[90] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating\nvery deep neural networks,” in Proceedings of the IEEE International\nConference on Computer Vision, 2017, pp. 1389–1397.\n[91] J.-H. Luo, J. Wu, and W. Lin, “Thinet: A ﬁlter level pruning method\nfor deep neural network compression,” in Proceedings of the IEEE\ninternational conference on computer vision, 2017, pp. 5058–5066.\n[92] S. Yao, Y. Zhao, A. Zhang, L. Su, and T. Abdelzaher, “Deepiot:\nCompressing deep neural network structures for sensing systems with\na compressor-critic framework,” in Proceedings of the 15th ACM\nConference on Embedded Network Sensor Systems, 2017, pp. 1–14.\n[93] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y. Guo, Q. Wu, J. Huang,\nand J. Zhu, “Discrimination-aware channel pruning for deep neural\nnetworks,” in Advances in Neural Information Processing Systems,\n2018, pp. 875–886.\n[94] Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang, “Soft ﬁlter pruning\nfor accelerating deep convolutional neural networks,” in Proceedings\nof the 27th International Joint Conference on Artiﬁcial Intelligence,\n2018, pp. 2234–2240.\n[95] Z. Huang and N. Wang, “Data-driven sparse structure selection for\ndeep neural networks,” in Proceedings of the European conference on\ncomputer vision (ECCV), 2018, pp. 304–320.\n20\n[96] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, “Amc: Automl\nfor model compression and acceleration on mobile devices,” in The\nEuropean Conference on Computer Vision (ECCV), September 2018.\n[97] Z. You, K. Yan, J. Ye, M. Ma, and P. Wang, “Gate decorator:\nGlobal ﬁlter pruning method for accelerating deep convolutional neural\nnetworks,” in Advances in Neural Information Processing Systems,\n2019, pp. 2130–2141.\n[98] Y. He, Y. Ding, P. Liu, L. Zhu, H. Zhang, and Y. Yang, “Learning ﬁlter\npruning criteria for deep convolutional neural networks acceleration,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 2009–2018.\n[99] M. Lin, R. Ji, Y. Wang, Y. Zhang, B. Zhang, Y. Tian, and L. Shao,\n“Hrank: Filter pruning using high-rank feature map,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, 2020, pp. 1529–1538.\n[100] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning\nefﬁcient convolutional networks through network slimming,” in Pro-\nceedings of the IEEE International Conference on Computer Vision,\n2017, pp. 2736–2744.\n[101] S. Anwar, K. Hwang, and W. Sung, “Structured pruning of deep con-\nvolutional neural networks,” ACM Journal on Emerging Technologies\nin Computing Systems (JETC), vol. 13, no. 3, pp. 1–18, 2017.\n[102] J. Yu, A. Lukefahr, D. J. Palframan, G. S. Dasika, R. Das, and S. A.\nMahlke, “Scalpel: Customizing dnn pruning to the underlying hardware\nparallelism,” 2017 ACM/IEEE 44th Annual International Symposium\non Computer Architecture (ISCA), pp. 548–560, 2017.\n[103] D. Blalock, J. J. G. Ortiz, J. Frankle, and J. Guttag, “What is the state\nof neural network pruning?” arXiv preprint arXiv:2003.03033, 2020.\n[104] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the\nvalue of network pruning,” in International Conference on Learning\nRepresentations, 2019. [Online]. Available: https://openreview.net/\nforum?id=rJlnB3C5Ym\n[105] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding\nsparse,\ntrainable\nneural\nnetworks,”\nin\nInternational\nConference\non\nLearning\nRepresentations,\n2019.\n[Online].\nAvailable:\nhttps:\n//openreview.net/forum?id=rJl-b3RcF7\n[106] Y. Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, and X. Hu,\n“Pruning from scratch,” arXiv preprint arXiv:1909.12579, 2019.\n[107] E. Malach, G. Yehudai, S. Shalev-Shwartz, and O. Shamir, “Proving\nthe lottery ticket hypothesis: Pruning is all you need,” arXiv preprint\narXiv:2002.00585, 2020.\n[108] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv\npreprint arXiv:1605.07146, 2016.\n[109] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep\nlearning with limited numerical precision,” in International Conference\non Machine Learning, 2015, pp. 1737–1746.\n[110] J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng, “Quantized convo-\nlutional neural networks for mobile devices,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2016,\npp. 4820–4828.\n[111] M. Courbariaux, Y. Bengio, and J.-P. David, “Binaryconnect: Training\ndeep neural networks with binary weights during propagations,” in\nAdvances in neural information processing systems, 2015, pp. 3123–\n3131.\n[112] F. Li, B. Zhang, and B. Liu, “Ternary weight networks,” arXiv preprint\narXiv:1605.04711, 2016.\n[113] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, “Dorefa-net:\nTraining low bitwidth convolutional neural networks with low bitwidth\ngradients,” arXiv preprint arXiv:1606.06160, 2016.\n[114] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “Xnor-net:\nImagenet classiﬁcation using binary convolutional neural networks,” in\nEuropean conference on computer vision.\nSpringer, 2016, pp. 525–\n542.\n[115] A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen, “Incremental network\nquantization: Towards lossless cnns with low-precision weights,” arXiv\npreprint arXiv:1702.03044, 2017.\n[116] X. Lin, C. Zhao, and W. Pan, “Towards accurate binary convolutional\nneural network,” in Advances in Neural Information Processing Sys-\ntems, 2017, pp. 345–353.\n[117] C. Zhu, S. Han, H. Mao, and W. J. Dally, “Trained ternary quantiza-\ntion,” arXiv preprint arXiv:1612.01064, 2016.\n[118] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam,\nand D. Kalenichenko, “Quantization and training of neural networks\nfor efﬁcient integer-arithmetic-only inference,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\npp. 2704–2713.\n[119] Z. Liu, B. Wu, W. Luo, X. Yang, W. Liu, and K.-T. Cheng, “Bi-real\nnet: Enhancing the performance of 1-bit cnns with improved represen-\ntational capability and advanced training algorithm,” in Proceedings of\nthe European conference on computer vision (ECCV), 2018, pp. 722–\n737.\n[120] D. Zhang, J. Yang, D. Ye, and G. Hua, “Lq-nets: Learned quantization\nfor highly accurate and compact deep neural networks,” in Proceedings\nof the European conference on computer vision (ECCV), 2018, pp.\n365–382.\n[121] S. R. Jain, A. Gural, M. Wu, and C. Dick, “Trained uniform quantiza-\ntion for accurate and efﬁcient neural network inference on ﬁxed-point\nhardware,” arXiv preprint arXiv:1903.08066, 2019.\n[122] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “Haq: Hardware-aware\nautomated quantization with mixed precision,” in The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2019.\n[123] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer, “Hawq:\nHessian aware quantization of neural networks with mixed-precision,”\nin Proceedings of the IEEE International Conference on Computer\nVision, 2019, pp. 293–302.\n[124] S. Jung, C. Son, S. Lee, J. Son, J.-J. Han, Y. Kwak, S. J. Hwang,\nand C. Choi, “Learning to quantize deep networks by optimizing\nquantization intervals with task loss,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n4350–4359.\n[125] B. Zhuang, L. Liu, M. Tan, C. Shen, and I. Reid, “Training quantized\nneural networks with a full-precision auxiliary module,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2020, pp. 1488–1497.\n[126] L. H. K. D. Shien Zhu and W. Liu, “Xor-net: An efﬁcient computation\npipeline of binary neural network inference on edge devices,” in\nThe 26th IEEE International Conference on Parallel and Distributed\nSystems (ICPADS).\nIEEE, 2020.\n[127] N. D. Lane, S. Bhattacharya, P. Georgiev, C. Forlivesi, and F. Kawsar,\n“An early resource characterization of deep learning on wearables,\nsmartphones and internet-of-things devices,” in Proceedings of the\n2015 international workshop on internet of things towards applications,\n2015, pp. 7–12.\n[128] H. Cai, C. Gan, L. Zhu, and S. Han, “Tiny transfer learn-\ning: Towards memory-efﬁcient on-device learning,” arXiv preprint\narXiv:2007.11622, 2020.\n[129] W. J. Dally, Y. Turakhia, and S. Han, “Domain-speciﬁc hardware\naccelerators,” Communications of the ACM, vol. 63, no. 7, pp. 48–57,\n2020.\n[130] J. Burgess, “Rtx on—the nvidia turing gpu,” IEEE Micro, vol. 40, no. 2,\npp. 36–44, 2020.\n[131] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong,\nM. Jahre, and K. Vissers, “Finn: A framework for fast, scalable\nbinarized neural network inference,” in Proceedings of the 2017\nACM/SIGDA International Symposium on Field-Programmable Gate\nArrays, 2017, pp. 65–74.\n[132] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,\nZ. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differenti-\nation in pytorch,” 2017.\n[133] M. Abadi et al., “TensorFlow: Large-scale machine learning on\nheterogeneous systems,” 2015, software available from tensorﬂow.org.\n[Online]. Available: http://tensorﬂow.org/\n[134] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards\ndeep learning models resistant to adversarial attacks,” arXiv preprint\narXiv:1706.06083, 2017.\n[135] J. Lin, C. Gan, and S. Han, “Defensive quantization: When efﬁciency\nmeets robustness,” arXiv preprint arXiv:1904.08444, 2019.\n[136] C. Gong, Z. Jiang, D. Wang, Y. Lin, Q. Liu, and D. Z. Pan, “Mixed\nprecision neural architecture search for energy efﬁcient deep learning.”\nin ICCAD, 2019, pp. 1–7.\n[137] G. Chen, H. Meng, Y. Liang, and K. Huang, “Gpu-accelerated real-\ntime stereo estimation with binary neural network,” IEEE Transactions\non Parallel and Distributed Systems, vol. 31, no. 12, pp. 2896–2907,\n2020.\n[138] T. Wang, K. Wang, H. Cai, J. Lin, Z. Liu, H. Wang, Y. Lin, and S. Han,\n“Apq: Joint search for network architecture, pruning and quantization\npolicy,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 2078–2087.\n[139] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Ben-\ngio, “Fitnets: Hints for thin deep nets,” arXiv preprint arXiv:1412.6550,\n2014.\n[140] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” arXiv preprint arXiv:1503.02531, 2015.\n21\n[141] S. Zagoruyko and N. Komodakis, “Paying more attention to attention:\nImproving the performance of convolutional neural networks via atten-\ntion transfer,” arXiv preprint arXiv:1612.03928, 2016.\n[142] A. Tarvainen and H. Valpola, “Mean teachers are better role models:\nWeight-averaged consistency targets improve semi-supervised deep\nlearning results,” in Advances in neural information processing systems,\n2017, pp. 1195–1204.\n[143] A. Polino, R. Pascanu, and D. Alistarh, “Model compression via\ndistillation and quantization,” arXiv preprint arXiv:1802.05668, 2018.\n[144] S. Ravi, “Efﬁcient on-device models using neural projections,” in\nInternational Conference on Machine Learning, 2019, pp. 5370–5379.\n[145] T. Li, J. Li, Z. Liu, and C. Zhang, “Few sample knowledge distillation\nfor efﬁcient network compression,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n14 639–14 647.\n[146] I. Chung, S. Park, J. Kim, and N. Kwak, “Feature-map-level online\nadversarial knowledge distillation,” arXiv preprint arXiv:2002.01775,\n2020.\n[147] Y. Liu, W. Zhang, and J. Wang, “Adaptive multi-teacher multi-level\nknowledge distillation,” Neurocomputing, vol. 415, pp. 106–113, 2020.\n[148] C. Buciluˇa, R. Caruana, and A. Niculescu-Mizil, “Model compression,”\nin Proceedings of the 12th ACM SIGKDD international conference on\nKnowledge discovery and data mining, 2006, pp. 535–541.\n[149] J. Yim, D. Joo, J. Bae, and J. Kim, “A gift from knowledge distillation:\nFast optimization, network minimization and transfer learning,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017, pp. 4133–4141.\n[150] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transac-\ntions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–\n1359, 2009.\n[151] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation\nas a defense to adversarial perturbations against deep neural networks,”\nin 2016 IEEE Symposium on Security and Privacy (SP).\nIEEE, 2016,\npp. 582–597.\n[152] H.\nCai,\nL.\nZhu,\nand\nS.\nHan,\n“ProxylessNAS:\nDirect\nneural\narchitecture search on target task and hardware,” in International\nConference on Learning Representations, 2019. [Online]. Available:\nhttps://arxiv.org/pdf/1812.00332.pdf\n[153] X. Luo, D. Liu, H. Kong, and W. Liu, “Edgenas: Discovering efﬁcient\nneural architectures for edge systems,” in International Conference on\nComputer Design, 2020.\n[154] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha,\nJ. Liu, and D. Marculescu, “Single-path nas: Designing hardware-\nefﬁcient convnets in less than 4 hours,” in Joint European Confer-\nence on Machine Learning and Knowledge Discovery in Databases.\nSpringer, 2019, pp. 481–497.\n[155] B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda,\nY. Jia, and K. Keutzer, “Fbnet: Hardware-aware efﬁcient convnet design\nvia differentiable neural architecture search,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2019,\npp. 10 734–10 742.\n[156] J. Fang, Y. Sun, Q. Zhang, Y. Li, W. Liu, and X. Wang, “Densely\nconnected search space for more ﬂexible neural architecture search,”\narXiv preprint arXiv:1906.09607, 2019.\n[157] G. Ghiasi, T.-Y. Lin, and Q. V. Le, “Nas-fpn: Learning scalable feature\npyramid architecture for object detection,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2019, pp.\n7036–7045.\n[158] B. Chen, G. Ghiasi, H. Liu, T.-Y. Lin, D. Kalenichenko, H. Adams, and\nQ. V. Le, “Mnasfpn: Learning latency-aware pyramid architecture for\nobject detection on mobile devices,” arXiv preprint arXiv:1912.01106,\n2019.\n[159] N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, and Y. Zhang,\n“Nas-fcos: Fast neural architecture search for object detection,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 11 943–11 951.\n[160] H. Xu, L. Yao, W. Zhang, X. Liang, and Z. Li, “Auto-fpn: Automatic\nnetwork architecture adaptation for object detection beyond classi-\nﬁcation,” in Proceedings of the IEEE International Conference on\nComputer Vision, 2019, pp. 6649–6658.\n[161] C. Gao, Y. Chen, S. Liu, Z. Tan, and S. Yan, “Adversarialnas:\nAdversarial neural architecture search for gans,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 5680–5689.\n[162] I. Fedorov, R. P. Adams, M. Mattina, and P. Whatmough, “Sparse:\nSparse architecture search for cnns on resource-constrained microcon-\ntrollers,” in Advances in Neural Information Processing Systems, 2019,\npp. 4978–4990.\n[163] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once for all:\nTrain one network and specialize it for efﬁcient deployment,” in\nInternational Conference on Learning Representations, 2020. [Online].\nAvailable: https://openreview.net/forum?id=HylxE1HKwS\n[164] J. Lin, W.-M. Chen, Y. Lin, J. Cohn, C. Gan, and S. Han, “Mcunet:\nTiny deep learning on iot devices,” arXiv preprint arXiv:2007.10319,\n2020.\n[165] X. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y. Wang, M. Dukhan,\nY. Hu, Y. Wu, Y. Jia et al., “Chamnet: Towards efﬁcient network design\nthrough platform-aware model adaptation,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n11 398–11 407.\n[166] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture\nsearch,” arXiv preprint arXiv:1806.09055, 2018.\n[167] X. Li, Y. Zhou, Z. Pan, and J. Feng, “Partial order pruning: For best\nspeed/accuracy trade-off in neural architecture search,” in The IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), June\n2019.\n[168] Y. Xiong, R. Mehta, and V. Singh, “Resource constrained neural\nnetwork architecture search: Will a submodularity assumption help?” in\nProceedings of the IEEE International Conference on Computer Vision,\n2019, pp. 1901–1910.\n[169] G. Gobieski, B. Lucia, and N. Beckmann, “Intelligence beyond the\nedge: Inference on intermittent embedded systems,” in Proceedings of\nthe Twenty-Fourth International Conference on Architectural Support\nfor Programming Languages and Operating Systems, 2019, pp. 199–\n213.\n[170] A. Ignatov, R. Timofte, W. Chou, K. Wang, M. Wu, T. Hartley, and\nL. Van Gool, “Ai benchmark: Running deep neural networks on android\nsmartphones,” in Proceedings of the European conference on computer\nvision (ECCV), 2018, pp. 0–0.\n[171] B. Lucia, V. Balaji, A. Colin, K. Maeng, and E. Ruppel, “Intermittent\ncomputing: Challenges and opportunities,” in 2nd Summit on Advances\nin Programming Languages (SNAPL 2017). Schloss Dagstuhl-Leibniz-\nZentrum fuer Informatik, 2017.\n[172] Y. Bengio, N. L´eonard, and A. Courville, “Estimating or propagating\ngradients through stochastic neurons for conditional computation,”\narXiv preprint arXiv:1308.3432, 2013.\n[173] A. Davis and I. Arel, “Low-rank approximations for conditional\nfeedforward computation in deep neural networks,” arXiv preprint\narXiv:1312.4461, 2013.\n[174] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,\nR. Zemel, and Y. Bengio, “Show, attend and tell: Neural image\ncaption generation with visual attention,” in International conference\non machine learning, 2015, pp. 2048–2057.\n[175] J. Lin, Y. Rao, J. Lu, and J. Zhou, “Runtime neural pruning,” in\nAdvances in neural information processing systems, 2017, pp. 2181–\n2191.\n[176] X. Gao, Y. Zhao, Ł. Dudziak, R. Mullins, and C.-z. Xu, “Dynamic\nchannel pruning: Feature boosting and suppression,” arXiv preprint\narXiv:1810.05331, 2018.\n[177] B. Ehteshami Bejnordi, T. Blankevoort, and M. Welling, “Batch-\nshaping for learning conditional channel gated networks,” arXiv, pp.\narXiv–1907, 2019.\n[178] J.\nYu,\nL.\nYang,\nN.\nXu,\nJ.\nYang,\nand\nT.\nHuang,\n“Slimmable\nneural\nnetworks,”\nin\nInternational\nConference\non\nLearning\nRepresentations,\n2019.\n[Online].\nAvailable:\nhttps://openreview.net/forum?id=H1gMCsAqY7\n[179] J. Yu and T. S. Huang, “Universally slimmable networks and improved\ntraining techniques,” in Proceedings of the IEEE International Confer-\nence on Computer Vision, 2019, pp. 1803–1811.\n[180] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nAdvances in neural information processing systems, 2017, pp. 5998–\n6008.\n[181] M. Ren, A. Pokrovsky, B. Yang, and R. Urtasun, “Sbnet: Sparse blocks\nnetwork for fast inference,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2018, pp. 8711–8720.\n[182] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang, D. Vetrov,\nand R. Salakhutdinov, “Spatially adaptive computation time for residual\nnetworks,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2017, pp. 1039–1048.\n[183] A. Graves, “Adaptive computation time for recurrent neural networks,”\narXiv preprint arXiv:1603.08983, 2016.\n22\n[184] T. Verelst and T. Tuytelaars, “Dynamic convolutions: Exploiting spatial\nsparsity for faster inference,” in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2020, pp. 2320–\n2329.\n[185] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with\ngumbel-softmax,” arXiv preprint arXiv:1611.01144, 2016.\n[186] Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, “Dynamic\nconvolution: Attention over convolution kernels,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 11 030–11 039.\n[187] W. Hua, Y. Zhou, C. M. De Sa, Z. Zhang, and G. E. Suh, “Channel\ngating neural networks,” in Advances in Neural Information Processing\nSystems, 2019, pp. 1886–1896.\n[188] T. Bolukbasi, J. Wang, O. Dekel, and V. Saligrama, “Adaptive neural\nnetworks for efﬁcient inference,” in Proceedings of the 34th Interna-\ntional Conference on Machine Learning - Volume 70, 2017, p. 527–536.\n[189] S. Teerapittayanon, B. McDanel, and H.-T. Kung, “Branchynet: Fast\ninference via early exiting from deep neural networks,” in 2016 23rd\nInternational Conference on Pattern Recognition (ICPR). IEEE, 2016,\npp. 2464–2469.\n[190] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, “Adaptive\nmixtures of local experts,” Neural computation, vol. 3, no. 1, pp. 79–87,\n1991.\n[191] S. E. Yuksel, J. N. Wilson, and P. D. Gader, “Twenty years of mixture of\nexperts,” IEEE transactions on neural networks and learning systems,\nvol. 23, no. 8, pp. 1177–1193, 2012.\n[192] R. Teja Mullapudi, W. R. Mark, N. Shazeer, and K. Fatahalian,\n“Hydranets: Specialized dynamic architectures for efﬁcient inference,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 8080–8089.\n[193] B. Yang, G. Bender, Q. V. Le, and J. Ngiam, “Condconv: Conditionally\nparameterized convolutions for efﬁcient inference,” in Advances in\nNeural Information Processing Systems, 2019, pp. 1307–1318.\n[194] L. Liu and J. Deng, “Dynamic deep neural networks: Optimizing\naccuracy-efﬁciency trade-offs by selective execution,” in 32nd AAAI\nConference on Artiﬁcial Intelligence, AAAI 2018.\nAAAI press, 2018,\npp. 3675–3682.\n[195] A. Veit, M. J. Wilber, and S. Belongie, “Residual networks behave\nlike ensembles of relatively shallow networks,” in Advances in neural\ninformation processing systems, 2016, pp. 550–558.\n[196] X. Wang, F. Yu, Z.-Y. Dou, T. Darrell, and J. E. Gonzalez, “Skipnet:\nLearning dynamic routing in convolutional networks,” in Proceedings\nof the European Conference on Computer Vision (ECCV), 2018, pp.\n409–424.\n[197] Z. Wu, T. Nagarajan, A. Kumar, S. Rennie, L. S. Davis, K. Grauman,\nand R. Feris, “Blockdrop: Dynamic inference paths in residual net-\nworks,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2018, pp. 8817–8826.\n[198] A. Veit and S. Belongie, “Convolutional networks with adaptive infer-\nence graphs,” in Proceedings of the European Conference on Computer\nVision (ECCV), 2018, pp. 3–18.\n[199] D. W. Otter, J. R. Medina, and J. K. Kalita, “A survey of the usages\nof deep learning for natural language processing,” IEEE Transactions\non Neural Networks and Learning Systems, 2020.\n[200] E. J. Topol, “High-performance medicine: the convergence of human\nand artiﬁcial intelligence,” Nature medicine, vol. 25, no. 1, pp. 44–56,\n2019.\n[201] I.\nP.\nNetwork,\n“Monetizing\nai:\nHow\nto\nget\nready\nfor\n‘inference at scale’.” [Online]. Available: https://itpeernetwork.intel.\ncom/ai-inference-at-scale/#gs.6ojv36\n[202] X. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial examples: Attacks and\ndefenses for deep learning,” IEEE transactions on neural networks and\nlearning systems, vol. 30, no. 9, pp. 2805–2824, 2019.\n[203] “Nvidia jetson systems,” 2020. [Online]. Available: https://developer.\nnvidia.com/embedded/develop/hardware#family\n[204] W. Jiang, X. Zhang, E. H.-M. Sha, L. Yang, Q. Zhuge, Y. Shi, and J. Hu,\n“Accuracy vs. efﬁciency: Achieving both through fpga-implementation\naware neural architecture search,” in Proceedings of the 56th Annual\nDesign Automation Conference 2019, 2019, pp. 1–6.\n[205] J. L. Hennessy and D. A. Patterson, “A new golden age for computer\narchitecture,” Communications of the ACM, vol. 62, no. 2, pp. 48–60,\n2019.\n[206] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-\ning the inception architecture for computer vision,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2016,\npp. 2818–2826.\n[207] “Intel neural compute stick 2,” 2020. [Online]. Available: https:\n//software.intel.com/en-us/neural-compute-stick\n[208] A. Gerstlauer, C. Haubelt, A. D. Pimentel, T. P. Stefanov, D. D. Gajski,\nand J. Teich, “Electronic system-level synthesis methodologies,” IEEE\nTransactions on Computer-Aided Design of Integrated Circuits and\nSystems, vol. 28, no. 10, pp. 1517–1530, 2009.\n[209] I. Radosavovic, J. Johnson, S. Xie, W.-Y. Lo, and P. Doll´ar, “On\nnetwork design spaces for visual recognition,” in Proceedings of the\nIEEE International Conference on Computer Vision, 2019, pp. 1882–\n1890.\n[210] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Doll´ar,\n“Designing network design spaces,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n10 428–10 436.\n[211] S. Williams, A. Waterman, and D. Patterson, “Rooﬂine: An insightful\nvisual performance model for multicore architectures,” Commun.\nACM, vol. 52, no. 4, p. 65–76, Apr. 2009. [Online]. Available:\nhttps://doi.org/10.1145/1498765.1498785\n[212] P. Mattson, V. J. Reddi, C. Cheng, C. Coleman, G. Diamos, D. Kanter,\nP. Micikevicius, D. Patterson, G. Schmuelling, H. Tang et al., “Mlperf:\nAn industry standard benchmark suite for machine learning perfor-\nmance,” IEEE Micro, vol. 40, no. 2, pp. 8–16, 2020.\n[213] Y. E. Wang, G.-Y. Wei, and D. Brooks, “A systematic methodology\nfor analysis of deep learning hardware and software platforms,” in The\n3rd Conference on Machine Learning and Systems (MLSys), 2020.\n[214] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan,\nL. Wang, Y. Hu, L. Ceze et al., “{TVM}: An automated end-to-end\noptimizing compiler for deep learning,” in 13th {USENIX} Symposium\non Operating Systems Design and Implementation ({OSDI} 18), 2018,\npp. 578–594.\n[215] W. Niu, X. Ma, S. Lin, S. Wang, X. Qian, X. Lin, Y. Wang, and\nB. Ren, “Patdnn: Achieving real-time dnn execution on mobile devices\nwith pattern-based weight pruning,” in Proceedings of the Twenty-Fifth\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems, 2020, p. 907–922.\n[216] T. Hussain, K. Muhammad, J. Del Ser, S. W. Baik, and V. H. C.\nde Albuquerque, “Intelligent embedded vision for summarization of\nmultiview videos in iiot,” IEEE Transactions on Industrial Informatics,\nvol. 16, no. 4, pp. 2592–2602, 2019.\n[217] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[218] Z. Liu, H. Tang, Y. Lin, and S. Han, “Point-voxel cnn for efﬁcient 3d\ndeep learning,” in Advances in Neural Information Processing Systems,\n2019, pp. 965–975.\n[219] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and map-\nping: part i,” IEEE robotics & automation magazine, vol. 13, no. 2,\npp. 99–110, 2006.\n[220] M. Li, J. Lin, Y. Ding, Z. Liu, J.-Y. Zhu, and S. Han, “Gan compression:\nEfﬁcient architectures for interactive conditional gans,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, 2020, pp. 5284–5294.\n[221] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in\nAdvances in neural information processing systems, 2014, pp. 2672–\n2680.\n[222] S. Dhar, J. Guo, J. Liu, S. Tripathi, U. Kurup, and M. Shah, “On-device\nmachine learning: An algorithms and learning theory perspective,”\narXiv preprint arXiv:1911.00623, 2019.\n[223] Y. Wang, Z. Jiang, X. Chen, P. Xu, Y. Zhao, Y. Lin, and Z. Wang, “E2-\ntrain: Training state-of-the-art cnns with over 80% energy savings,” in\nAdvances in Neural Information Processing Systems, 2019, pp. 5138–\n5150.\n[224] Y. Wu, Z. Wang, Y. Shi, and J. Hu, “Enabling on-device cnn training by\nself-supervised instance ﬁltering and error map pruning,” arXiv preprint\narXiv:2007.03213, 2020.\n[225] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transac-\ntions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–\n1359, 2009.\n[226] T. Xiao, J. Zhang, K. Yang, Y. Peng, and Z. Zhang, “Error-driven in-\ncremental learning in deep convolutional neural network for large-scale\nimage classiﬁcation,” in Proceedings of the 22nd ACM international\nconference on Multimedia, 2014, pp. 177–186.\n[227] J. Shin, S. Choi, Y. Choi, and L.-S. Kim, “A pragmatic approach to on-\ndevice incremental learning system with selective weight updates,” in\nProceedings of the 57th Annual Design Automation Conference 2020,\n2020, pp. 1–6.\n23\n[228] Z. Chen and B. Liu, “Lifelong machine learning,” Synthesis Lectures\non Artiﬁcial Intelligence and Machine Learning, vol. 12, no. 3, pp.\n1–207, 2018.\n[229] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning:\nConcept and applications,” ACM Transactions on Intelligent Systems\nand Technology (TIST), vol. 10, no. 2, pp. 1–19, 2019.\n[230] X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen, “In-edge\nai: Intelligentizing mobile edge computing, caching and communication\nby federated learning,” IEEE Network, vol. 33, no. 5, pp. 156–165,\n2019.\n[231] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and\nK. Chan, “Adaptive federated learning in resource constrained edge\ncomputing systems,” IEEE Journal on Selected Areas in Communica-\ntions, vol. 37, no. 6, pp. 1205–1221, 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-11-25",
  "updated": "2020-11-25"
}