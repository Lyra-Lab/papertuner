{
  "id": "http://arxiv.org/abs/2012.00152v1",
  "title": "Every Model Learned by Gradient Descent Is Approximately a Kernel Machine",
  "authors": [
    "Pedro Domingos"
  ],
  "abstract": "Deep learning's successes are often attributed to its ability to\nautomatically discover new representations of the data, rather than relying on\nhandcrafted features like other learning methods. We show, however, that deep\nnetworks learned by the standard gradient descent algorithm are in fact\nmathematically approximately equivalent to kernel machines, a learning method\nthat simply memorizes the data and uses it directly for prediction via a\nsimilarity function (the kernel). This greatly enhances the interpretability of\ndeep network weights, by elucidating that they are effectively a superposition\nof the training examples. The network architecture incorporates knowledge of\nthe target function into the kernel. This improved understanding should lead to\nbetter learning algorithms.",
  "text": "Every Model Learned by Gradient Descent Is Approximately\na Kernel Machine\nPedro Domingos\npedrod@cs.washington.edu\nPaul G. Allen School of Computer Science & Engineering\nUniversity of Washington\nSeattle, WA 98195-2350, USA\nAbstract\nDeep learning’s successes are often attributed to its ability to automatically discover new\nrepresentations of the data, rather than relying on handcrafted features like other learning\nmethods. We show, however, that deep networks learned by the standard gradient de-\nscent algorithm are in fact mathematically approximately equivalent to kernel machines, a\nlearning method that simply memorizes the data and uses it directly for prediction via a\nsimilarity function (the kernel). This greatly enhances the interpretability of deep network\nweights, by elucidating that they are eﬀectively a superposition of the training examples.\nThe network architecture incorporates knowledge of the target function into the kernel.\nThis improved understanding should lead to better learning algorithms.\nKeywords:\ngradient descent, kernel machines, deep learning, representation learning,\nneural tangent kernel\n1. Introduction\nDespite its many successes, deep learning remains poorly understood (Goodfellow et al.,\n2016). In contrast, kernel machines are based on a well-developed mathematical theory,\nbut their empirical performance generally lags behind that of deep networks (Sch¨olkopf\nand Smola, 2002). The standard algorithm for learning deep networks, and many other\nmodels, is gradient descent (Rumelhart et al., 1986).\nHere we show that every model\nlearned by this method, regardless of architecture, is approximately equivalent to a kernel\nmachine with a particular type of kernel. This kernel measures the similarity of the model\nat two data points in the neighborhood of the path taken by the model parameters during\nlearning. Kernel machines store a subset of the training data points and match them to\nthe query using the kernel.\nDeep network weights can thus be seen as a superposition\nof the training data points in the kernel’s feature space, enabling their eﬃcient storage\nand matching. This contrasts with the standard view of deep learning as a method for\ndiscovering representations from data, with the attendant lack of interpretability (Bengio\net al., 2013). Our result also has signiﬁcant implications for boosting algorithms (Freund\nand Schapire, 1997), probabilistic graphical models (Koller and Friedman, 2009), and convex\noptimization (Boyd and Vandenberghe, 2004).\narXiv:2012.00152v1  [cs.LG]  30 Nov 2020\nDomingos\n2. Path Kernels\nA kernel machine is a model of the form\ny = g\n X\ni\naiK(x, xi) + b\n!\n,\nwhere x is the query data point, the sum is over training data points xi, g is an optional\nnonlinearity, the ai’s and b are learned parameters, and the kernel K measures the similarity\nof its arguments (Sch¨olkopf and Smola, 2002). In supervised learning, ai is typically a linear\nfunction of y∗\ni , the known output for xi. Kernels may be predeﬁned or learned (Cortes\net al., 2009).\nKernel machines, also known as support vector machines, are one of the\nmost developed and widely used machine learning methods. In the last decade, however,\nthey have been eclipsed by deep networks, also known as neural networks and multilayer\nperceptrons, which are composed of multiple layers of nonlinear functions. Kernel machines\ncan be viewed as neural networks with one hidden layer, with the kernel as the nonlinearity.\nFor example, a Gaussian kernel machine is a radial basis function network (Poggio and\nGirosi, 1990). But a deep network would seem to be irreducible to a kernel machine, since\nit can represent some functions exponentially more compactly than a shallow one (Delalleau\nand Bengio, 2011; Cohen et al., 2016).\nWhether a representable function is actually learned, however, depends on the learning\nalgorithm. Most deep networks, and indeed most machine learning models, are trained\nusing variants of gradient descent (Rumelhart et al., 1986). Given an initial parameter\nvector w0 and a loss function L = P\ni L(y∗\ni , yi), gradient descent repeatedly modiﬁes the\nmodel’s parameters w by subtracting the loss’s gradient from them, scaled by the learning\nrate ϵ:\nws+1 = ws −ϵ∇wL(ws).\nThe process terminates when the gradient is zero and the loss is therefore at an optimum\n(or saddle point). Remarkably, we have found that learning by gradient descent is a strong\nenough constraint that the end result is guaranteed to be approximately a kernel machine,\nregardless of the number of layers or other architectural features of the model.\nSpeciﬁcally, the kernel machines that result from gradient descent use what we term a\npath kernel. If we take the learning rate to be inﬁnitesimally small, the path kernel between\ntwo data points is simply the integral of the dot product of the model’s gradients at the\ntwo points over the path taken by the parameters during gradient descent:\nK(x, x′) =\nZ\nc(t)\n∇wy(x) · ∇wy(x′) dt,\nwhere c(t) is the path. Intuitively, the path kernel measures how similarly the model at the\ntwo data points varies during learning. The more similar the variation for x and x′, the\nhigher the weight of x′ in predicting y. Fig. 1 illustrates this graphically.\nOur result builds on the concept of neural tangent kernel, recently introduced to analyze\nthe behavior of deep networks (Jacot et al., 2018). The neural tangent kernel is the integrand\nof the path kernel when the model is a multilayer perceptron. Because of this, and since\na sum of positive deﬁnite kernels is also a positive deﬁnite kernel (Sch¨olkopf and Smola,\n2\nDeep Networks Are Kernel Machines\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥2)\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥1)\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥)\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥2)\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥1)\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥)\n𝑤𝑤𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓\n𝑤𝑤0\n𝑤𝑤1\n𝑤𝑤2\n𝑤𝑤3\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥1)\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥)\n𝛻𝛻𝑤𝑤𝑦𝑦(𝑥𝑥2)\nFigure 1: How the path kernel measures similarity between examples.\nIn this two-\ndimensional illustration, as the weights follow a path on the plane during training,\nthe model’s gradients (vectors on the weight plane) for x, x1 and x2 vary along\nit. The kernel K(x, x1) is the integral of the dot product of the gradients ∇wy(x)\nand ∇wy(x1) over the path, and similarly for K(x, x2). Because on average over\nthe weight path ∇wy(x) · ∇wy(x1) is greater than ∇wy(x) · ∇wy(x2), y1 has more\ninﬂuence than y2 in predicting y, all else being equal.\n3\nDomingos\n2002), the known conditions for positive deﬁniteness of neural tangent kernels extend to\npath kernels (Jacot et al., 2018). A positive deﬁnite kernel is equivalent to a dot product in\na derived feature space, which greatly simpliﬁes its analysis (Sch¨olkopf and Smola, 2002).\nWe now present our main result. For simplicity, in the derivations below we assume\nthat y is a (real-valued) scalar, but it can be made a vector with only minor changes. The\ndata points xi can be arbitrary structures.\nDeﬁnition 1 The tangent kernel associated with function fw(x) and parameter vector v is\nKg\nf,v(x, x′) = ∇wfw(x) · ∇wfw(x′), with the gradients taken at v.\nDeﬁnition 2 The path kernel associated with function fw(x) and curve c(t) in parameter\nspace is Kp\nf,c(x, x′) =\nZ\nc(t)\nKg\nf,w(t)(x, x′) dt.\nTheorem 1 Suppose the model y = fw(x), with f a diﬀerentiable function of w, is learned\nfrom a training set {(xi, y∗\ni )}m\ni=1 by gradient descent with diﬀerentiable loss function L =\nP\ni L(y∗\ni , yi) and learning rate ϵ. Then\nlim\nϵ→0 y =\nm\nX\ni=1\naiK(x, xi) + b,\nwhere K(x, xi) is the path kernel associated with fw(x) and the path taken by the param-\neters during gradient descent, ai is the average −∂L/∂yi along the path weighted by the\ncorresponding tangent kernel, and b is the initial model.\nProof In the ϵ →0 limit, the gradient descent equation, which can also be written as\nws+1 −ws\nϵ\n= −∇wL(ws),\nwhere L is the loss function, becomes the diﬀerential equation\ndw(t)\ndt\n= −∇wL(w(t)).\n(This is known as a gradient ﬂow (Ambrosio et al., 2008).) Then for any diﬀerentiable\nfunction of the weights y,\ndy\ndt =\nd\nX\nj=1\n∂y\n∂wj\ndwj\ndt ,\nwhere d is the number of parameters. Replacing dwj/dt by its gradient descent expression:\ndy\ndt =\nd\nX\nj=1\n∂y\n∂wj\n\u0012\n−∂L\n∂wj\n\u0013\n.\nApplying the additivity of the loss and the chain rule of diﬀerentiation:\ndy\ndt =\nd\nX\nj=1\n∂y\n∂wj\n \n−\nm\nX\ni=1\n∂L\n∂yi\n∂yi\n∂wj\n!\n.\n4\nDeep Networks Are Kernel Machines\nRearranging terms:\ndy\ndt = −\nm\nX\ni=1\n∂L\n∂yi\nd\nX\nj=1\n∂y\n∂wj\n∂yi\n∂wj\n.\nLet L′(y∗\ni , yi) = ∂L/∂yi, the loss derivative for the ith output. Applying this and Deﬁni-\ntion 1:\ndy\ndt = −\nm\nX\ni=1\nL′(y∗\ni , yi)Kg\nf,w(t)(x, xi).\nLet y0 be the initial model, prior to gradient descent. Then for the ﬁnal model y:\nlim\nϵ→0 y = y0 −\nZ\nc(t)\nm\nX\ni=1\nL′(y∗\ni , yi)Kg\nf,w(t)(x, xi) dt,\nwhere c(t) is the path taken by the parameters during gradient descent. Multiplying and\ndividing by\nR\nc(t) Kg\nf,w(t)(x, xi) dt:\nlim\nϵ→0 y = y0 −\nm\nX\ni=1\n R\nc(t) Kg\nf,w(t)(x, xi)L′(y∗\ni , yi) dt\nR\nc(t) Kg\nf,w(t)(x, xi) dt\n! Z\nc(t)\nKg\nf,w(t)(x, xi) dt.\nLet L′(y∗\ni , yi) =\nR\nc(t) Kg\nf,w(t)(x, xi)L′(y∗\ni , yi) dt/\nR\nc(t) Kg\nf,w(t)(x, xi) dt, the average loss deriva-\ntive weighted by similarity to x. Applying this and Deﬁnition 2:\nlim\nϵ→0 y = y0 −\nm\nX\ni=1\nL′(y∗\ni , yi)Kp\nf,c(x, xi).\nThus\nlim\nϵ→0 y =\nm\nX\ni=1\naiK(x, xi) + b,\nwith K(x, xi) = Kp\nf,c(x, xi), ai = −L′(y∗\ni , yi), and b = y0.\nRemark 1 This diﬀers from typical kernel machines in that the ai’s and b depend on x.\nNevertheless, the ai’s play a role similar to the example weights in ordinary SVMs and the\nperceptron algorithm: examples that the loss is more sensitive to during learning have a\nhigher weight. b is simply the prior model, and the ﬁnal model is thus the sum of the prior\nmodel and the model learned by gradient descent, with the query point entering the latter\nonly through kernels. Since Theorem 1 applies to every yi as a query throughout gradient\ndescent, the training data points also enter the model only through kernels (initial model\naside).\nRemark 2 Theorem 1 can equally well be proved using the loss-weighted path kernel Klp\nf,c,L =\nR\nc(t) L′(y∗\ni , yi)Kg\nf,w(t)(x, xi) dt, in which case ai = −1 for all i.\n5\nDomingos\nRemark 3 In least-squares regression, L′(y∗\ni , yi) = yi −y∗\ni .\nWhen learning a classiﬁer\nby minimizing cross-entropy, the standard practice in deep learning, the function to be\nestimated is the conditional probability of the class, pi, the loss is −Pm\ni=1 ln pi, and the\nloss derivative for the ith output is −1/pi. Similar expressions hold for modeling a joint\ndistribution by minimizing negative log likelihood, with pi as the probability of the data point.\nRemark 4\nAdding a regularization term R(w) to the loss function simply adds\n−\nR\nc(t)\nPd\nj=1(∂y/∂wj)(∂R/∂wj) to b.\nRemark 5 The proof above is for batch gradient descent, which uses all training data points\nat each step. To extend it to stochastic gradient descent, which uses a subsample, it suﬃces\nto multiply each term in the summation over data points by an indicator function Ii(t) that\nis 1 if the ith data point is included in the subsample at time t and 0 otherwise. The only\nchange this causes in the result is that the path kernel and average loss derivative for a\ndata point are now stochastic integrals. Based on previous results (Scieur et al., 2017),\nTheorem 1 or a similar result seems likely to also apply to further variants of gradient\ndescent, but proving this remains an open problem.\nFor linear models, the path kernel reduces to the dot product of the data points. It\nis well known that a single-layer perceptron is a kernel machine, with the dot product as\nthe kernel (Aizerman et al., 1964). Our result can be viewed as a generalization of this to\nmultilayer perceptrons and other models. It is also related to Lippmann et al.’s proof that\nHopﬁeld networks, a predecessor of many current deep architectures, are equivalent to the\nnearest-neighbor algorithm, a predecessor of kernel machines, with Hamming distance as\nthe comparison function (Lippmann et al., 1987).\nThe result assumes that the learning rate is suﬃciently small for the trajectory of the\nweights during gradient descent to be well approximated by a smooth curve. This is standard\nin the analysis of gradient descent, and is also generally a good approximation in practice,\nsince the learning rate has to be quite small in order to avoid divergence (e.g., ϵ = 10−3)\n(Goodfellow et al., 2016). Nevertheless, it remains an open question to what extent models\nlearned by gradient descent can still be approximated by kernel machines outside of this\nregime.\n3. Discussion\nA notable disadvantage of deep networks is their lack of interpretability (Zhang and Zhu,\n2018). Knowing that they are eﬀectively path kernel machines greatly ameliorates this. In\nparticular, the weights of a deep network have a straightforward interpretation as a super-\nposition of the training examples in gradient space, where each example is represented by\nthe corresponding gradient of the model. Fig. 2 illustrates this. One well-studied approach\nto interpreting the output of deep networks involves looking for training instances that are\nclose to the query in Euclidean or some other simple space (Ribeiro et al., 2016). Path\nkernels tell us what the exact space for these comparisons should be, and how it relates to\nthe model’s predictions.\nExperimentally, deep networks and kernel machines often perform more similarly than\nwould be expected based on their mathematical formulation (Brendel and Bethge, 2019).\n6\nDeep Networks Are Kernel Machines\nQuery\nAnswer\nTraining example 1\n𝐾𝐾\n𝑎𝑎1\nTraining example 2\n𝐾𝐾\n𝑎𝑎2\nTraining example m\n𝐾𝐾\n𝑎𝑎𝑚𝑚\n+\nModel\nFigure 2: Deep network weights as superpositions of training examples.\nApplying the\nlearned model to a query example is equivalent to simultaneously matching the\nquery with each stored example using the path kernel and outputting a weighted\nsum of the results.\n7\nDomingos\nEven when they generalize well, deep networks often appear to memorize and replay whole\ntraining instances (Zhang et al., 2017; Devlin et al., 2015). The fact that deep networks are\nin fact kernel machines helps explain both of these observations. It also sheds light on the\nsurprising brittleness of deep models, whose performance can degrade rapidly as the query\npoint moves away from the nearest training instance (Szegedy et al., 2014), since this is\nwhat is expected of kernel estimators in high-dimensional spaces (Hardle et al., 2004).\nPerhaps the most signiﬁcant implication of our result for deep learning is that it casts\ndoubt on the common view that it works by automatically discovering new representations of\nthe data, in contrast with other machine learning methods, which rely on predeﬁned features\n(Bengio et al., 2013). As it turns out, deep learning also relies on such features, namely the\ngradients of a predeﬁned function, and uses them for prediction via dot products in feature\nspace, like other kernel machines. All that gradient descent does is select features from this\nspace for use in the kernel. If gradient descent is limited in its ability to learn representations,\nbetter methods for this purpose are a key research direction. Current nonlinear alternatives\ninclude predicate invention (Muggleton and Buntine, 1988) and latent variable discovery\nin graphical models (Elidan et al., 2000).\nTechniques like structure mapping (Gentner,\n1983), crossover (Holland, 1975) and predictive coding (Rao and Ballard, 1999) may also\nbe relevant. Ultimately, however, we may need entirely new approaches to solve this crucial\nbut extremely diﬃcult problem.\nOur result also has signiﬁcant consequences on the kernel machine side. Path kernels\nprovide a new and very ﬂexible way to incorporate knowledge of the target function into\nthe kernel. Previously, it was only possible to do so in a weak sense, via generic notions\nof what makes two data points similar. The extensive knowledge that has been encoded\ninto deep architectures by applied researchers, and is crucial to the success of deep learning,\ncan now be ported directly to kernel machines.\nFor example, kernels with translation\ninvariance or selective attention are directly obtainable from the architecture of, respectively,\nconvolutional neural networks (LeCun et al., 1998) or transformers (Vaswani et al., 2017).\nA key property of path kernels is that they combat the curse of dimensionality by incor-\nporating derivatives into the kernel: two data points are similar if the candidate function’s\nderivatives at them are similar, rather than if they are close in the input space. This can\ngreatly improve kernel machines’ ability to approximate highly variable functions (Bengio\net al., 2005). It also means that points that are far in Euclidean space can be close in gradi-\nent space, potentially improving the ability to model complex functions. (For example, the\nmaxima of a sine wave are all close in gradient space, even though they can be arbitrarily\nfar apart in the input space.)\nMost signiﬁcantly, however, learning path kernel machines via gradient descent largely\novercomes the scalability bottlenecks that have long limited the applicability of kernel meth-\nods to large data sets. Computing and storing the Gram matrix at learning time, with its\nquadratic cost in the number of examples, is no longer required. (The Gram matrix is the\nmatrix of applications of the kernel to all pairs of training examples.) Separately storing\nand matching (a subset of) the training examples at query time is also no longer necessary,\nsince they are eﬀectively all stored and matched simultaneously via their superposition in\nthe model parameters. The storage space and matching time are independent of the num-\nber of examples. (Interestingly, superposition has been hypothesized to play a key role in\ncombatting the combinatorial explosion in visual cognition (Arathorn, 2002), and is also\n8\nDeep Networks Are Kernel Machines\nessential to the eﬃciency of quantum computing (Nielsen and Chuang, 2000) and radio\ncommunication (Carlson and Grilly, 2009).) Further, the same specialized hardware that\nhas given deep learning a decisive edge in scaling up to large data (Raina et al., 2009) can\nnow be used for kernel machines as well.\nThe signiﬁcance of our result extends beyond deep networks and kernel machines. In its\nlight, gradient descent can be viewed as a boosting algorithm, with tangent kernel machines\nas the weak learner and path kernel machines as the strong learner obtained by boosting\nit (Freund and Schapire, 1997). In each round of boosting, the examples are weighted by\nthe corresponding loss derivatives. It is easily seen that each round (gradient descent step)\ndecreases the loss, as required. The weight of the model at a given round is the learning rate\nfor that step, which can be constant or the result of a line search (Boyd and Vandenberghe,\n2004). In the latter case gradient descent is similar to gradient boosting (Mason et al.,\n1999).\nAnother consequence of our result is that every probabilistic model learned by gradient\ndescent, including Bayesian networks (Koller and Friedman, 2009), is a form of kernel\ndensity estimation (Parzen, 1962). The result also implies that the solution of every convex\nlearning problem is a kernel machine, irrespective of the optimization method used, since,\nbeing unique, it is necessarily the solution obtained by gradient descent.\nIt is an open\nquestion whether the result can be extended to nonconvex models learned by non-gradient-\nbased techniques, including constrained (Bertsekas, 1982) and combinatorial optimization\n(Papadimitriou and Steiglitz, 1982).\nThe results in this paper suggest a number of research directions. For example, viewing\ngradient descent as a method for learning path kernel machines may provide new paths\nfor improving it.\nConversely, gradient descent is not necessarily the only way to form\nsuperpositions of examples that are useful for prediction.\nThe key question is how to\noptimize the tradeoﬀbetween accurately capturing the target function and minimizing the\ncomputational cost of storing and matching the examples in the superposition.\nAcknowledgments\nThis research was partly funded by ONR grant N00014-18-1-2826. Thanks to L´eon Bottou\nand Simon Du for feedback on a draft of this paper.\nReferences\nM. A. Aizerman, E. M. Braverman, and L. I. Rozonoer. Theoretical foundations of the\npotential function method in pattern recognition learning. Autom. & Remote Contr., 25:\n821–837, 1964.\nLuigi Ambrosio, Nicola Gigli, and Giuseppe Savar´e. Gradient Flows: In Metric Spaces and\nin the Space of Probability Measures. Birkh¨auser, Basel, 2nd edition, 2008.\nD. W. Arathorn. Map-Seeking Circuits in Visual Cognition: A Computational Mechanism\nfor Biological and Machine Vision. Stanford Univ. Press, Stanford, CA, 2002.\n9\nDomingos\nY. Bengio, O. Delalleau, and N. L. Roux. The curse of highly variable functions for local\nkernel machines. Adv. Neural Inf. Proc. Sys., 18:107–114, 2005.\nY. Bengio, A. Courville, and P. Vincent.\nRepresentation learning: A review and new\nperspectives. IEEE Trans. Patt. An. & Mach. Intell., 35:1798–1828, 2013.\nP. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Academic Press,\nCambridge, MA, 1982.\nS. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cam-\nbridge, UK, 2004.\nW. Brendel and M. Bethge. Approximating CNNs with bag-of-local-features models works\nsurprisingly well on ImageNet. In Proc. Int. Conf. Learn. Repr., 2019.\nA. B. Carlson and P. B. Grilly. Communication Systems: An Introduction to Signals and\nNoise in Electrical Communication. McGraw-Hill, New York, 5th edition, 2009.\nN. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor\nanalysis. In Proc. Ann. Conf. Learn. Th., pages 698–728, 2016.\nC. Cortes, M. Mohri, and A. Rostamizadeh. Learning non-linear combinations of kernels.\nAdv. Neural Inf. Proc. Sys., 22:396–404, 2009.\nO. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. Adv. Neural Inf. Proc.\nSys., 24:666–674, 2011.\nJ. Devlin, H. Cheng, H. Fang, S. Gupta, L. Deng, X. He, G. Zweig, and M. Mitchell.\nLanguage models for image captioning: The quirks and what works. In Proc. Ann. Meet.\nAssoc. Comp. Ling., pages 100–105, 2015.\nG. Elidan, N. Lotner, N. Friedman, and D. Koller.\nDiscovering hidden variables:\nA\nstructure-based approach. Adv. Neural Inf. Proc. Sys., 13:479–485, 2000.\nY. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an\napplication to boosting. J. Comp. & Sys. Sci., 55:119–139, 1997.\nD. Gentner. Structure-mapping: A theoretical framework for analogy. Cog. Sci., 7:155–170,\n1983.\nI. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, Cambridge, MA,\n2016.\nW. Hardle, M. M¨uller, and A. Werwatz.\nNonparametric and Semiparametric Models.\nSpringer, Berlin, 2004.\nJ. H. Holland. Adaptation in Natural and Artiﬁcial Systems. Univ. Michigan Press, Ann\nArbor, MI, 1975.\nA. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization\nin neural networks. Adv. Neural Inf. Proc. Sys., 31:8571–8580, 2018.\n10\nDeep Networks Are Kernel Machines\nD. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques.\nMIT Press, Cambridge, MA, 2009.\nY. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to docu-\nment recognition. Proc. IEEE, 86:2278–2324, 1998.\nR. Lippmann, B. Gold, and M. Malpass. A comparison of Hamming and Hopﬁeld neural\nnetworks for pattern classiﬁcation. Technical Report 769, MIT Lincoln Lab, Lexington,\nMA, 1987.\nL. Mason, J. Baxter, P. L. Bartlett, and M. R. Frean. Boosting algorithms as gradient\ndescent. Adv. Neural Inf. Proc. Sys,, 12:512–518, 1999.\nS. Muggleton and W. Buntine.\nMachine invention of ﬁrst-order predicates by inverting\nresolution. In Proc. Int. Conf. Mach. Learn., pages 339–352, 1988.\nM. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information. Cam-\nbridge Univ. Press, Cambridge, UK, 2000.\nC. Papadimitriou and K. Steiglitz. Combinatorial Optimization: Algorithms and Complex-\nity. Prentice-Hall, Upper Saddle River, NJ, 1982.\nE. Parzen. On estimation of a probability density function and mode. Ann. Math. Stat.,\n33:1065–1076, 1962.\nT. Poggio and F. Girosi.\nRegularization algorithms for learning that are equivalent to\nmultilayer networks. Science, 247:978–982, 1990.\nR. Raina, A. Madhavan, and A. Y. Ng.\nLarge-scale deep unsupervised learning using\ngraphics processors. In Proc. Int. Conf. Mach. Learn., pages 873–880, 2009.\nR. P. N. Rao and D. H. Ballard.\nPredictive coding in the visual cortex: A functional\ninterpretation of some extra-classical receptive ﬁeld eﬀects. Nature Neurosci., 2:79–87,\n1999.\nM. T. Ribeiro, S. Singh, and C. Guestrin. “Why should I trust you?”: Explaining the\npredictions of any classiﬁer. In Proc. Int. Conf. Knowl. Disc. & Data Mining, pages\n1135–1144, 2016.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-\npropagating errors. Nature, 323:533–536, 1986.\nB. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regular-\nization, Optimization, and Beyond. MIT Press, Cambridge, MA, 2002.\nD. Scieur, V. Roulet, F. Bach, and A. d’Aspremont. Integration methods and optimization\nalgorithms. Adv. Neural Inf. Proc. Sys., 30:1109–1118, 2017.\nC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.\nIntriguing properties of neural networks. In Proc. Int. Conf. Learn. Repr., 2014.\n11\nDomingos\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, N. Aidan, L. Kaiser, and\nI. Polosukhin. Attention is all you need. Adv. Neural Inf. Proc. Sys., 30:5998–6008, 2017.\nC. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning\nrequires rethinking generalization. In Proc. Int. Conf. Learn. Repr., 2017.\nQ. Zhang and S.-C. Zhu. Visual interpretability for deep learning: A survey. Front. Inf.\nTech. & Elec. Eng., 19:27–39, 2018.\n12\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML",
    "I.2.6; I.5.1"
  ],
  "published": "2020-11-30",
  "updated": "2020-11-30"
}