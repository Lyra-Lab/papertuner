{
  "id": "http://arxiv.org/abs/2012.00187v1",
  "title": "Statistical patterns of word frequency suggesting the probabilistic nature of human languages",
  "authors": [
    "Shuiyuan Yu",
    "Chunshan Xu",
    "Haitao Liu"
  ],
  "abstract": "Traditional linguistic theories have largely regard language as a formal\nsystem composed of rigid rules. However, their failures in processing real\nlanguage, the recent successes in statistical natural language processing, and\nthe findings of many psychological experiments have suggested that language may\nbe more a probabilistic system than a formal system, and thus cannot be\nfaithfully modeled with the either/or rules of formal linguistic theory. The\npresent study, based on authentic language data, confirmed that those important\nlinguistic issues, such as linguistic universal, diachronic drift, and language\nvariations can be translated into probability and frequency patterns in parole.\nThese findings suggest that human language may well be probabilistic systems by\nnature, and that statistical may well make inherent properties of human\nlanguages.",
  "text": " 1 / 18 \n \n \nStatistical patterns of word frequency suggesting \nprobabilistic nature of human languages \n \nShuiyuan Yua, Chunshan Xub, Haitao Liuc,d,a* \n \na. Institute of Quantitative Linguistics, Beijing Language and Culture University, 100083, Beijing, \nChina. \nb. School of Foreign Studies, Anhui Jianzhu University, 230601, Hefei, China.  \nc. Department of Linguistics, Zhejiang University, 310058, Hangzhou, China. \nd. Centre for Linguistics and Applied Linguistics, Guangdong University of Foreign Studies, \n510420, Guangzhou, China.  \n \n*Correspondence to: Haitao Liu, e-mail: lhtzju@gmail.com \n \n \nAbstract \nTraditional linguistic theories have largely regard language as a formal system composed of rigid \nrules. However, their failures in processing real language, the recent successes in statistical natural \nlanguage processing, and the findings of many psychological experiments have suggested that \nlanguage may be more a probabilistic system than a formal system, and thus cannot be faithfully \nmodeled with the either/or rules of formal linguistic theory. The present study, based on authentic \nlanguage data, confirmed that those important linguistic issues, such as linguistic universal, \ndiachronic drift, and language variations can be translated into probability and frequency patterns \nin parole. These findings suggest that human language may well be probabilistic systems by nature, \nand that statistical may well make inherent properties of human languages.  \n \nKey words: language; system; probabilistic system; statistical laws; cognitive science \n \n \n 2 / 18 \n \n1. Introduction \nLanguage makes a vital part of human intelligence and cognition, a medium of information \ncommunication [1] and culture [2], and a vehicle of human thinking and knowledge [3], which is \noften believed to differentiate human beings from animals [4, 5]. Hence, to decipher the mystery of \nhuman beings and human intelligence, we probably have to understand how language works and \nevolves. For this purpose, various linguistic theories have been formulated. Since the beginning of \n20th century, incessant efforts have been devoted to establishing various formal linguistic theories \nthat try to model human languages in terms of rigid either/or rules [6-8]. However, these theories, \nthough having considerably extended our knowledge of human languages, probably have not \nsufficiently revealed the secrets of language. For example, these theories often have difficult in \naccounting for the gradual changes of language. What is more, these theories have largely failed in \npractical applications like natural language processing [9]. These linguistic theories can be traced \nback to Saussure[10], the founder of modern linguistics, who drew the distinction between langue \n[the abstract formal language system] and parole[the authentic language use], contending that the \nonly mission of linguistics is to establish the langue, a formal system of strict rules that represents \nthe essential knowledge of human language. But the wild bushes of authentic language seem to \nalways present so many exceptions to the formal, neat, and clear-cut rules of linguistic theories that \nthese theories often face tremendous difficulties when it comes to process real languages [11]. In \ncontrast, the statistics-oriented approaches based on corpus have reaped stunning achievement in \nnatural language processing [12]. This fact suggests that, probably, those formal theories may \noversee a vital property of human language, the frequency and probability in authentic language use, \nand neglecting this probabilistic nature of human languages may be responsible for the failure of \nrigid formal linguistic theories to correctly model human language.  \nSo, language is probably more a probabilistic system than a formal system of rigid rule, a \nprobabilistic system grounded in actual linguistic behaviors, consisting not so much in langue as in \nparole. Linguistic units and their relations at different levels are distributed in probability spaces \nestablished through language use, and these probabilities make inherent properties of human \nlanguages. This revolutionarily new conception may explain why recent achievements of NLP are \n 3 / 18 \n \nlargely ascribed to computer scientists and mathematicians, not linguists: these NLP technologies \nmostly rely on probabilistic models extracted from large data of authentic language use, i.e. the \nparole, bearing very little on the formal linguistic theories [12]. These successes in NLP strongly \nsuggest that language is probably not, or at least not merely, a formal system, but a probabilistic \nsystem well-grounded in authentic language use [13-15].  \nIn fact, some linguistic studies have suggested important roles of frequency and probability in \nlanguage production, comprehension, and change. Experiments have affirmed frequency and \nprobability as a major factor regulating language comprehension [16-18], influencing language \nacquisition [19-21], shaping syntactic patterns [22], driving the emergence and evolution of \nlinguistic constructions [23]. Experiments also reported that high frequency words are easy to \nmemorize and recognize [24], that high frequency brings about ready and accurate word judgments \n[25], and that frequency has much effect on the variation of language in a community [26]. These \nfindings suggest that frequency and probability information of language use are stored in human \nminds, influencing language comprehension, language production, language acquisition, \nsociolinguistics, etc.  \nLanguage as a probabilistic system rooted in parole is a revolutionarily new idea, which may \nprovide a new perspective for us to understand human cognition, human intelligence, and human \nlanguages. But this new idea must be anchored to ample evidences. Cognitive and psychological \nstudies, mostly microscopic and isolated, have presented some empirical evidences. But they are \nnot sufficient: macroscopic and systemic evidences are also indispensable, which is what the present \nstudy seeks. If this new idea is right, it may be expected that the important linguistic issues are not \nqualitative matters of yes/no dichotomies but quantitative matters of probability or frequency \ndistributions. For example, linguistic universals [27-28] may manifest as universal patterns of \nprobability or frequency widely found human languages. In other words, if language is a \nprobabilistic system, some macroscopic statistical patterns might be found in almost all human \nlanguages, which are linguistically significant enough to stand as linguistic universals. Similarly, if \nlanguage is a probabilistic system, the changes of language must be changes in statistical pattern of \nlanguage use. In this sense, diachronic change[29] and personal styles[30], which are the pivotal \nconcerns of historical linguistics and social linguistics, can also be captured in terms of changes and \n 4 / 18 \n \nvariations in those overall statistical patterns found in language data. To recap, the statistical patterns \nof probability or frequency may well have quantifiable changes over time, across regions, and \namong persons, which may account for diachronic drift, regional varieties, and personal styles. If \nthese fundamental linguistic issues are actually issues of probability and frequency, we may have \nmacroscopic evidences that language is very likely to be a probabilistic system. This type of \nevidences is sought for the first time in the present study, which conducted the first large-scale \nstatistical investigations into word frequency distribution of multiple languages that spans several \nhundred years. This study concentrates on word frequency distribution because on one hand, words \nare probably the least controversial linguistic units, which play fundamental roles in language, and \non the other, their frequencies make the basis of virtually all statistical properties found at higher \nlinguistic levels. Our findings suggest that the statistical patterns of word frequency can reflect both \nlinguistic universals and linguistic variations mentioned above, which indicates that statistical \npatterns may make inherent properties of human languages, and thus supports the idea that language \nis a probabilistic system [31-32]. \n2. Material and methods \n2.1. Language materials \n105 languages from 12 language families in Leipzig Corpora [33] are used as language materials \nfor the investigations into word frequency distributions. Google Books Ngram Viewer[34] is used \nas language materials for the study of diachronic drifts of word frequency distributions. The \nlanguage materials for explorations into personal variations in word frequency distribution come \nfrom Project Gutenberg [35], of which the present study selects 2707 works by 316 authors born in \nthe 19th century. Most of these authors have less than 10 works, with very few of them having more \nthan 10 works. When collecting statistics, we exclude numbers and punctuations, which have little \nto do with essential properties of language.  \n2.2 Indices of language change \nCosine similarity and spearman rho are the measures of changes in the kernel lexicon (the set of \n3000 most frequent words). Cosine similarity indicates word set similarity, that is, the degree of \n 5 / 18 \n \noverlap between two word-sets by measuring the fraction of the intersection size of two sets divided \nby the geometric mean of the two set sizes, which ranges between 0 and 1. Spearman's rho measures \nthe degree of correlation between 2 word frequency rank orders, that is, the degree of similarity \nbetween two word sets in terms of their word frequency rank order. \ncosine similarity：O(A, B) =\n|A∩B|\n√|A|∙|B|\n2\n                                         (1) \nSpearman rho：rs = 1 −\n6 ∑di\n2\nn(n2−1]                                            (2) \nIn these two formulae, A and B represent two kernel lexicon, di is the order difference between \ntwo sequences (two word frequency rank orders), n is the number of observations (sequence length). \nThe value of Spearman rho ranges between - 1 and + 1, and the symbols + and - presents the \ndirection of correlation. \nActually, these two indices are closely related to each other. This study calculates the two indices \nof six languages in Google Book at time intervals of 1,2,4,8,16,32,64 years, and then calculates the \ncorrelation coefficients between these two groups of indices. The results show that the coefficients \nare very high: for American English, the coefficient is 0.9099, for British English, it is 0.9780, for \nFrench it is 0.9858, for German it is 0.9595, for Italian it is 0.9736, for Spanish it is 0.9837.Hence, \nwe can use either of them to measure linguistic similarity.  \n2.3. Personal styles indicated by the kernel lexicon ( the set of the 3000 most frequent \nwords) \nKernel lexicon may reflect some fundamental language properties. The individual differences \nin the kernel lexicon hence may mark personal language styles. This study collected, from \nGutenberg Project, 2707 works by 316 authors. Word frequency distributions of the 3000 most \nfrequent words (the kernel lexicon) are extracted from all these 2707 works to serve as the standard \nof comparison. For each word (wi) in these 3000 words, its standard frequency is fi. Thus, we have \na frequency set (f1, f2,……f3000). Then, we extracted from each work the frequencies of these 3000 \nmost frequent words (wi) , which is another frequency set(q1, q2, ……q3000). The frequency set \nfound in each work are then compared with this standard frequency set, and the differences between \nthem make a 3000-dimention vector (q1-f1, q2-f2, ……q3000-f3000) signifying this work. With works \n 6 / 18 \n \nof the same author in one group, ANOSIM (Analysis of Similarities) can be carried out to compare \ndifferences among authors (between group difference) with differences among works of the same \nauthor (in group difference). When ANOSIM yields large R value and small p value, it means that \nbetween-group (inter-author) differences are significantly greater than in-group (intra-author) \ndifference, and that vector (q1-f1, q2-f2,……q3000-f3000) can serve as indicators of personal styles. \nThe analysis method adopted in this study is Analysis of Similarities (ANOSIM), which, unlike \nANOVA (Analysis of Variance), does not require the data value distribution to meet specific \nconditions. \n3. Results \n3.1. Universal patterns in word frequency distribution across human languages \nAbout 80 years ago, Zipf reported that the frequency of a word in a text or a text-set is a power-law \nfunction of its frequency rank order, with an exponent around -1[36]. To verify the universality of \nZipf’s law, we investigated into the word frequency distributions in the language materials of 105 \nlanguages from Leipzig Corpus, with about 1 million words for each language. The results indicate \na universal power-law relation between word frequency and frequency rank, as can be seen in Figure \n1. This universal pattern of word frequency distribution may be an inherent property of language, \nreflecting some fundamental mechanisms of language. What is more, the curves of distributions all \npresent a downward bending, which starts, in English, roughly at the 3000th most frequent word, \nand the curves are thus divided into two parts (Fig. 1), the upper one consisting of frequently used \nwords, mostly function words, referred to as the “kernel lexicon” [37], the lower part, mostly \ninfrequently used words [37-38].  \nIn fact, the word frequency distributions curves in all these 105 languages universally present \nsimilar patterns (Fig. 1), with similar linguistic significance. The curves universally consist of two \nparts. The upper part of the curve covers mostly function words (i.e. segment 1 in Fig.1) and kernel \nnotional words (i.e. segment 2 in Fig.1), which are either the fundamental syntactic means in \nsentence organization and comprehension or the fundamental blocks in our knowledge of world and \nsemantic representation. The lower part consists of these infrequent content words. This universal \n 7 / 18 \n \npattern of word frequency is probably not a contingent coincidence, but a linguistic universal shaped \nby fundamental mechanisms like human cognition.  \nRandom languages (monkey typing models) can also present power law distributions of word \nfrequency [39], which, however, differ from those of natural languages in that the power law \ndistributions in random languages usually lack downward bending and present different word length \ndistributions [40]. \n \nFig. 1. Word frequency distributions of 105 languages. For these 105 languages from over 10 \nlanguage families, the word frequency invariably present similar power law distributions, with the \ndownward bending appearing at roughly similar positions on the curves, In fact, the upper part \nseems to consist of 2 segments37. The first one at the top is largely function words, and the second \nsegment below it, mostly the highly frequent content words generally used across different topics. \nThat is, most words in the upper part are function words and kernel notional words, which are \nprobably more inherent and fundamental in the language, reflecting the mutually shared “mean”—\nthe core—of that language, while the words in the lower part are more peripheral, very often \nmotivated by specific topics. \n3.2. Word Frequency distributions as Indicators of diachronic drift  \n \n 8 / 18 \n \nLanguage is always changing. The changes will accumulate over time and eventually lead to \nsignificant diachronic differences. Similarly, for language variants in different geographical \nlocations, their differences should also increase over time. If language is a probabilistic system, the \ngradual diachronic drift should manifest itself in the probability or frequency patterns extracted from \nlanguage use, that is, the parole. Hence, the present study hypothesized word frequency distributions, \nwhich play fundamental roles in language use, may manifest gradual change over time, making an \nimportant part of diachronic drift of language.   \nAs can be seen in Figure 1, the curve of word frequency distribution can be roughly divides \ninto two parts at the 3000th most frequent word. In the upper part are those high-frequency words, \nwhich make the core of vocabulary, stably used for almost all topics, and in the lower part are the \nlow-frequency words, which are somewhat peripheral, biased toward some specific topics [37]. That \nis, words in the upper part are probably more inherent and fundamental in the language, reflecting \nthe mutually shared core of that language, while the words in the lower part are more contingent, \nvery often motivated by specific topics. As a result, in comparison with the low-frequency words, \nthe high-frequency core words making the upper part are more suitable for the study of diachronic \ndrift of language because these words and their statistical patterns may reflect the properties of \nlanguages.  \nIn this study, the changes in languages are appraised with two indices which measure the \nchanges in this kernel lexicon of 3000 words. The first index is Cosine similarity [41], which \nmeasures word set similarity, indicating the degree of similarity (or difference) between two word-\nsets. Another index is Spearman's rho [41], indicating the degree of similarity (or difference)  \nbetween two word sets in their word frequency rank order, that is, the degree of similarity in how \nthe words are ordered in terms of their frequencies.  \n 9 / 18 \n \nThe language materials are taken from Google Book Corpora \n(https://books.google.com/ngrams), which have about 8 million books published from 1500 to \n2009 [42]. For the present study, we select 6 languages in the corpora: American English, British \nEnglish, French, German, Italian, and Spanish. From the Google n-gram corpora, these two \nindices of set similarity are extracted to measure the diachronic changes. \nFig. 2． The diachronic changes of the 3000 most frequent words in 6 languages(a), and the \ndiachronic decrease of similarity between British and American English in 150 years(b) (The curves \nin (b) are smoothed, using a moving average filter with a span of 5). Figure 2a shows gradual \ndiachronic decrease in similarity (cosine similarity) of 3000 high frequency words in six languages \nover 150 years, at different time intervals. Figure 2b shows the rather gradual diachronic decrease \n(spanning 150 years) of similarity between American English and British English in terms of the \n3000 most frequent words.  \nThe statistical study of 6 languages suggests that the similarity in the kernel lexicon, though \nstably high during 150 years, gradually drops over time (Fig.2a). To recapitulate, the 3000 most \nfrequent words in each language are relatively stable, with only minor changes, but the changes \naccumulate over time. The similarity between the two varieties of English in their 3000 most \nfrequent words also tends to decrease slowly (Fig. 2b). These findings suggest that the statistical \npatterns of word frequency can well reflect diachronic and regional variations of languages, \nprobably making fundamental properties of language. \n3.3. Personal styles as reflected by word frequency distributions \n(a) \n(b) \n 10 / 18 \n \nThere are, apart from diachronic drifts, individual variations in languages, or, different personal \nstyles. If language is a probabilistic system, the personal styles should register themselves in patterns \nof probability or frequency. In the present study, it may be expected that such individual differences \nmay be found in the frequency distribution of the kernel lexicon. According to Saussure, the \nautonomous language system is an idealized “mean” of the miscellaneous use of language, and \nthis system is stored in individuals as slightly different copies. The word frequency distribution \nextracted from the whole speech community is probably a “mean”, and the word frequency \ndistribution stored in an individual, which derive from his or her language experience, is a copy of \nthat mean, that is, the overall frequency distribution. But this copy slightly deviates from the mean \nand differs from one another, and these differences and deviations may mark personal stylistic \nfeatures. In other words, the linguistic style of a person may lie partly in the differences between his \nword frequency distribution and that of the entire speech community. To test this possibility, this \nstudy collected, from Gutenberg Project, 2707 works by 316 authors born in the 19th century, with \nat least 3 works from each author. ANOSIM (Analysis of similarities) [43] is used to test whether \nthe inter-author differences are significantly greater than intra-author differences.  \nAs can be seen in Fig. 3, ANOSIM indicated that intra-author language consistency outstrips \ninter-author consistency: works of different authors present greater differences in word frequency \ndistribution than works of the same author [R = 0.9346 and p = 0.0010]. In other words, individual \ndeviations from overall word frequency distribution may indicate stylistic differences among \nauthors. This finding suggests that word frequency distributions extracted from language use are \nprobably  important indicators of personal stylistic features, and thus cannot be overlooked in \nstylistics and social linguistics. \n 11 / 18 \n \n \nFig. 3. Intra-author language consistency outstrips inter-author consistency. In the figure is a \ncorrelation coefficients matrix of 180 works. The works of the same author are arranged \nconsecutively on X and Y axes, forming intervals marked by numbers representing those authors. \nThe details are available in supplementary materials. In this figure Red points represent high \ncorrelation coefficients, which are largely distributed in the squares along the diagonal line of the \nmatrix between authors, that is, the coefficients are much higher among the works of the same author. \nHere, we choose not to present all 2707 works because that will overcrowd the figure, rendering it \nunreadable.  \n4. Discussion and Conclusion \nThis study shows that word frequencies extracted from language use have some universal statistical \npatterns, and can capture language variations like diachronic drifts and personal styles. \nThe languages investigated all present power-law distributions, with the curves universally \ncomposed of two parts: the upper one of high-frequency words and the lower one of low-frequency \nwords. In fact, the upper part universally presents two segments respectively composed of function \nwords and basic-category notional words. This pattern of word frequency distribution is probably a \n 12 / 18 \n \nfundamental linguistic universal, shaped by the dual-process mechanism of human cognition.  \nDual-process model characterizes most high-level human cognitive activities: type-1 process \nfeatures promptness, automaticity, effortlessness, and freedom from conscious attention, in most \ncases due to frequent repetition (high frequency); type-2 process, in contrast, features slowness, \ncontrolled attention, conscious effort, etc. [44].  \nThe universal pattern of word frequency distribution may have much to do with Type-1 process, \nwhich automatizes language comprehension and organization to a considerable degree. The limited \nnumber of function words or basic category notional words are highly frequent, and high frequency \nrender them readily accessible, subject to Type-1 processing. These words may form some \nfundamental templates in language processing or represents some most important blocks of our \nknowledge that deserve automatic processing. Such a pattern of word frequency distribution is \nperhaps molded by the common human cognition mechanism under the restriction of principle of \nleast effort: a limited number vital to language are processed automatically owing to their high \nfrequency. Thus, this distribution of word frequency is a linguistic universal, shaped by fundamental \ncommon cognitive mechanism and reflecting overall tendency in language use. But it is not a rigid \nyes/no rule, flexibly allowing for deviations and exceptions at microscopic scales. In the past, such \nexceptions have afflicted many researchers of language universals, with Greenberg probably as an \nexception, who introduced probability into language universals based on authentic language data \n[45].  \nDiachronic drifts are the key concerns of historical linguistics. Our findings suggest that word \nfrequency distributions may quantitatively present the diachronic drifts of language system. This \ndrift is gradual [46], which may be reflected by the slow change in the kernel words and their \nfrequency distributions. Hence, the diachronic drift of language is a gradual change in the use of \nlanguage, or rather in the frequency of language use. In the present study, such variations are \nquantitatively captured and described by the frequency distribution of kernel words. We may well \nbelieve that the diachronic drift can also be captured by frequency of other linguistic units or \nrelations. No diachronic change in language is sudden and abrupt: it is always embodied in the \nchange of frequency, or, the probability of use. The change is at first unnoticeably slight, but the \nslight change may implicitly influence our knowledge of the language, leading to change in the \n 13 / 18 \n \nprobability of use, which, under some conditions, may accumulate, multiply, cause chain reaction, \nand eventually become much drastic and significant over time. In most cases, rigid rules cannot \nsatisfactorily explain such changes, which can only be found in terms of frequency and probability. \nSimilarly, the differences among social varieties of language may largely be the differences in \nfrequency and probability, which is also a matter of degree. According to Saussure, langue is an \nidealized “mean” of the parole in a society, a “mean” replicated in individuals as slightly different \ncopies, owing to different linguistic environments or different verbal habits. These linguistic \nenvironments and the verbal habits will decide how frequently or how probably a person will use a \nlinguistic unit or structure, and different environment will lead to different habits, different \nfrequencies or probabilities in language use, which make slight deviations from the “mean”. To \nrecap, these slight deviations of individual copies are mostly likely to be differences in frequency \nof some units or structures, not the differences in those units per se. Otherwise it is impossible for \ninter-personal communication if person use different linguistic units. So, personal styles probably \nconsist in, to a considerable degree, the differences in frequency and probability of words used by \nindividuals. This is supported by the present study, which indicates that differences in personal styles \nconsist at least partly in the variations in frequency distribution. Such differences are usually matter \nof degree, beyond the reach of the rules of formal linguistic theories.   \nThe present study confines itself to frequency of words, not involving the frequency or \nprobability of grammatical constructions of word combinations, and thus having little to do with the \nso called grammaticality. But it is highly probable that the statistical patterns at the syntactic level \nare also important properties of languages, closely bearing on our judgment of grammaticality, and \nunderlying our production and comprehension of sentences [47]. Anyway, grammaticality is a \nmatter of degree, a matter of probability, somehow based on the frequency of words in language use. \nThere are increasing studies suggesting that language is a probabilistic system driven by human \nusers [31]. The present one, based on large-scale multiple language data, provides for the first time \nmacroscopic and systemic evidences that human languages are probably probabilistic systems. \nLanguages are by nature probabilistic, and any linguistic theory neglecting statistical patterns in \nactual use will be divorced from the reality of language, failing to faithfully model the real language, \nto satisfactorily explain the language processing mechanism of human beings, and eventual to \n 14 / 18 \n \neffectively unveil the mysteries of human intelligence and knowledge. To model language solely in \nterms of rules of dichotomy probably neglects many possibilities between YES and NO, providing \nonly an over-simplified sketch of human language. In comparison, the conception that language is \na probabilistic system may provide a picture of language with higher resolution. To faithfully model \nthe real language, linguistic theories should pay enough attention to the probabilistic nature of \nlanguage, and incorporate statistical patterns of human languages. Such theories may provide \nreliable guide for practical applications like NLP, propel the scientific explorations in linguistics, \nand more importantly, help unveil the mysteries of human languages and human intelligence. \n \nReferences \n[1]  Newmeyer FJ. Language Form and Language Function Cambridge: MIT Press; \n1998. \n[2]  Foley WA. Anthropological Linguistics: An Introduction. Malden: Blackwell \nPublishers; 1997. \n[3]  Whorf B. Language, Thought, and Reality: Selected Writings of Benjamin Lee \nWhorf. Carroll JB, editor. Cambridge: MIT Press; 1956. \n[4]  Mehler J, Nespor M, Pena M Why is language unique to humans? In: Pomerantz \nJ, editor. Topics in Integrative Neuroscience: From Cells to Cognition. \nCambridge: Cambridge University Press; 2008, p. 206-237. \n[5]   Scott-Phillips TC, Blythe RA. Why is combinatorial communication rare in the \nnatural world, and why is language an exception to this trend? J R Soc Interface \n2013; 10(88); https://doi.org/10.1098/rsif.2013.0520. \n[6]  Jakobson R. Preliminaries to speech analysis: the distinctive features and their \ncorrelates. 2nd ed. Cambridge: MIT Press; 1963. \n 15 / 18 \n \n[7]  Firth JR. Papers in linguistics. 1934-1951. London: Oxford University Press; 1957. \n[8]  Chomsky N. Aspects of the theory of syntax. Cambridge: MIT Press; 1965. \n[9]  Bar-Hillel Y. The Present Status of Automatic Translation of Languages. Adv \nComput 1960; 1: 91-163. \n[10]  \nSaussure F. Course in General Linguistics. La Salle: Open Court, 1983. \n[11]  \nBresnan J. \"Linguistics: The Garden and the Bush.\" Write-up of ACL Lifetime \nAchievement Award acceptance speech for 2016. Comput Linguist 2016; 42(4): \n599-617.  \n[12]  \nBates MModels of natural language understanding. PNAS 1995; 92 (22): \n9977–9982. \n[13]  \nGries S, Ellis NC. Statistical measures for Usage-based linguistics. Lang  \nLearn 2015; 65: 228-255. \n[14]  \nPagel M, Atkinson QD, Meade A. Frequency of word-use predicts rates of \nlexical evolution throughout Indo-European history. Nature 2007; 449: 717–720. \n[15]  \nBybee JL. Language Change (Cambridge University Press, Cambridge,). \n[16]  \nMacDonald MC, et al. Lexical nature of syntactic ambiguity resolution. \nPsychol Rev 2 1994; 191: 676–703. \n[17]  \nJurafsky D. A probabilistic model of lexical and syntactic access and \ndisambiguation. Cogn Sci 1996; 20: 137–194. \n[18]  \nEllis NC. Frequency effects in language processing. Stud Second Lang Acquis \n2002; 24: 143–88. \n[19]  \nHowes D. On the relation between the intelligibility and frequency of \n 16 / 18 \n \noccurrence of English words. J Acoust Soc Am 1957; 29: 296–305. \n[20]  \nSavin HB. Word-frequency effects and errors in the perception of speech. J \nAcoust Soc Am 1963; 35: 200–6. \n[21]  \nTheakston AL, Lieven EM, Pine JM,  Rowland CF. The acquisition of \nauxiliary syntax: BE and HAVE. Cogn Linguist 2005; 16: 247–77. \n[22]  \nBybee J, Hopper P (Eds.). Frequency and the emergence of linguistic structure. \nAmsterdam: John Benjamins; 2001. \n[23]  \nDiessel H. Frequency effects in language acquisition, language use, and \ndiachronic change. New Ideas Psychol 2007; 25(2): 108-127. \n[24]  \nHulme C, Roodenrys S, Schweickert R, Brown GA, Martin M, Stuart G. Word-\nfrequency effects on short-term memory tasks: evidence for a redintegration \nprocess in immediate serial recall. J Exp Psychol -Learn Mem Cogn 1997; 23: \n1217–32. \n[25]  \nBalota DA, Cortese MJ, Sergent-Marshall SD, Spieler DH, Yap MJ. Visual \nword recognition of single-syllable words. J. Exp. Psychol.-Gen 2004; 133: 283–\n316. \n[26]  \nTagliamonte S, D’Arce A. Frequency and variation in the community grammar: \ntracking a new change through the generations. Lang Var Chang 2007; 19: 199-\n217. \n[27]  \nBach E, Harms RT (Eds.). Universals in linguistic theory New York: Holt, \nRinehart and Winston; 1968. \n[28]  \nGreenberg JH (Ed.) Universals of languages. Cambridge: MIT Press; 1963. \n 17 / 18 \n \n[29]  \nGreenhill SJ, Wu C-H, Hua X, Dunn M, Levinson SC, Gray RD. Evolutionary \ndynamics of language systems. Proc. Natl. Acad. Sci. U.S.A. 2017; 114 (42): \nE8822-E8829. \n[30]  \nLabov W. Field Methods of the Project in Linguistic Change and Variation. In: \nBaugh J, Sherzer J, editors. Language in Use: Readings in Sociolinguistics. \nEnglewood Cliffs: Prentice-Hall; 1984, p.28-53.  \n[31]  \nLiu HT. Language as a human-driven complex adaptive system. Phys Life Rev \n2018; https://doi.org/10.1016/j.plrev.2018.06.006. \n[32]  \nBod R, Hay J, Jannedy S. Probabilistic Linguistics. Boston: MIT Press; 2003. \n[33]  \nLeipzig Corpora Collection.  http://wortschatz.uni-leipzig.de/en/download, \n2007.  \n[34]  \nGoogle \nBooks \nNgram. \nViewer \nhttp://storage.googleapis.com/books/ngrams/books/datasetsv2.html, 2012. \n[35]  \nProject Gutenberg. https://www.gutenberg.org/, 2014.  \n[36]  \nZipf GK. Selected studies of the principle of relative frequency in language. \nCambridge: Harvard Univ. Press; 1932. \n[37]  \nFerrer i Cancho R, Solé RV. Two regimes in the frequency of words and the \norigins of complex lexicons: Zipf’s law revisited. J Quant Linguist 2001; 8(3):  \n165-173. \n[38]  \nPetersen AM, Tenenbaum JN, Havlin S, Stanley HE, Percb M. Languages cool \nas they expand: Allometric scaling and the decreasing need for new words. Sci Rep  \n2012; 2: 943. \n 18 / 18 \n \n[39]  \nFerrer-i-Cancho R, Elvevåg B. Random texts do not exhibit the real Zipf's law-\nlike rank distribution. PLoS ONE 2010; 5 (3): e9411. \n[40]  \nYu SY, Xu CS, Liu HT. Zipf’s law in 50 languages: its structural pattern, \nlinguistic \ninterpretation, \nand \ncognitive \nmotivation. \n2018 \nhttps://arxiv.org/ftp/arxiv/papers/1807/1807.01855.pdf. Posted 5 Jul 2018. \n[41]  \nMyers JL, Well AD. Research Design and Statistical Analysis. 2nd ed. \nMahwah: Lawrence Erlbaum; 2003. \n[42]  \nBrysbaert M, Mandera P, Keuleers E. The Word Frequency Effect in Word \nProcessing: An Updated Review. Curr Dir Psychol Sci 2018; 27(1): 45–50. \n[43]  \nClarke KR. Non-parametric multivariate analyses of changes in community \nstructure. Austral Ecol 1993; 18: 117–143. \n[44]  \nDaniel K. Thinking, fast and slow. 1st ed. New York: Farrar, Straus and Giroux; \n2011. \n[45]  \nGreenberg JH (Ed.) Universals of Language. Cambridge: MIT Press; 1963. \n[46]  \nBybee JL. Diachronic Linguistics. In: Geeraerts D, Cuyckens H, editors.The \nOxford Handbook of Cognitive Linguistics. Oxford: Oxford University Press; \n2007, p.945-987. \n[47]  \nLiu.HT, Xu CS, Liang JY. Dependency distance: A new perspective on \nsyntactic patterns in natural languages. Phys Life Rev 2017; 21: 171-193. \n \n \n(2018-10-30) \n",
  "categories": [
    "cs.CL",
    "physics.comp-ph"
  ],
  "published": "2020-12-01",
  "updated": "2020-12-01"
}