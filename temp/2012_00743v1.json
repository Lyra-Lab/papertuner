{
  "id": "http://arxiv.org/abs/2012.00743v1",
  "title": "Adaptive Neural Architectures for Recommender Systems",
  "authors": [
    "Dimitrios Rafailidis",
    "Stefanos Antaris"
  ],
  "abstract": "Deep learning has proved an effective means to capture the non-linear\nassociations of user preferences. However, the main drawback of existing deep\nlearning architectures is that they follow a fixed recommendation strategy,\nignoring users' real time-feedback. Recent advances of deep reinforcement\nstrategies showed that recommendation policies can be continuously updated\nwhile users interact with the system. In doing so, we can learn the optimal\npolicy that fits to users' preferences over the recommendation sessions. The\nmain drawback of deep reinforcement strategies is that are based on predefined\nand fixed neural architectures. To shed light on how to handle this issue, in\nthis study we first present deep reinforcement learning strategies for\nrecommendation and discuss the main limitations due to the fixed neural\narchitectures. Then, we detail how recent advances on progressive neural\narchitectures are used for consecutive tasks in other research domains.\nFinally, we present the key challenges to fill the gap between deep\nreinforcement learning and adaptive neural architectures. We provide guidelines\nfor searching for the best neural architecture based on each user feedback via\nreinforcement learning, while considering the prediction performance on\nreal-time recommendations and the model complexity.",
  "text": "arXiv:2012.00743v1  [cs.IR]  11 Nov 2020\nAdaptive Neural Architectures\nfor Recommender Systems\nDimitrios Rafailidis\nMaastricht University\nMaastricht, The Netherlands\ndimitrios.rafailidis@maastrichtuniversity.nl\nStefanos Antaris\nKTH Royal Institute of Technology\nStockholm, Sweden\nantaris@kth.se\nAbstract—Deep learning has proved an effective means to\ncapture the non-linear associations of user preferences. However,\nthe main drawback of existing deep learning architectures is that\nthey follow a ﬁxed recommendation strategy, ignoring users’ real\ntime-feedback. Recent advances of deep reinforcement strate-\ngies showed that recommendation policies can be continuously\nupdated while users interact with the system. In doing so,\nwe can learn the optimal policy that ﬁts to users’ preferences\nover the recommendation sessions. The main drawback of deep\nreinforcement strategies is that are based on predeﬁned and\nﬁxed neural architectures. To shed light on how to handle this\nissue, in this study we ﬁrst present deep reinforcement learning\nstrategies for recommendation and discuss the main limitations\ndue to the ﬁxed neural architectures. Then, we detail how\nrecent advances on progressive neural architectures are used for\nconsecutive tasks in other research domains. Finally, we present\nthe key challenges to ﬁll the gap between deep reinforcement\nlearning and adaptive neural architectures. We provide guidelines\nfor searching for the best neural architecture based on each\nuser feedback via reinforcement learning, while considering the\nprediction performance on real-time recommendations and the\nmodel complexity.\nIndex Terms—Adaptive neural models, recommender systems,\ndeep reinforcement learning\nI. INTRODUCTION\nFollowing the collaborative ﬁltering strategy, latent models\nsuch as matrix factorization [1] and factorization machines [2]\nhave been widely used to generate personalized recommen-\ndations. To capture the non-linearity in user preferences,\nseveral deep learning strategies have been introduced [3]–[6].\nHowever, these models follow a ﬁxed recommendation policy\nwhich does not correspond to the dynamic real-world scenario,\nas in practice users evolve their preferences while interacting\nwith recommender systems [7], [8]. Recommender systems,\ninstead of having a static recommendation strategy, should\ncontinously update their policies according to users’ real-\ntime feedback. For instance, in real-time recommendations a\nuser may want to seek for alternatives with diverse topics of\ninterest, requiring to adapt the recommendation policy in real-\ntime [9].\nTo capture users’ drift on their preferences several time-\naware latent models have been introduced [10], [11]. However,\nsuch models aim to learn users’ long-term preferences. Instead,\nsequential recommender systems try to maximize the imme-\ndiate reward for future recommendations, that is to predict the\nnext recommendations for the short-term sessions [8], [12]–\n[14]. However, sequential recommendations fail to accurately\npredict the long-term rewards in the future [15]. To handle this\nissue, some attempts have been made trying to search for both\nshort-term and long-term rewards via Reinforcement Learning\n(RL), by ﬁnding the best recommendation policy based on\nan action-value function [16], [17]. The main challenge in\nbaseline RL methods like Q-Learning [18] and Partially-\nObserved Markov Decision Process (POMDP) [19] is the\nextremely large number of possible items in recommender\nsystems, resulting in enormous state and action spaces. To face\nthis problem, more recently a few studies exploit deep RL to\napproximate the action-value function of baseline RL meth-\nods, thus supporting huge amount of items in recommender\nsystems [9], [15], [20]. Based on the neural architecture of the\nDeep Q-Network (DQN) model [21], [22], deep RL methods\ncan continuously update their recommendation policies based\non users’ real-time feedback. Although deep RL methods\nin recommender systems try to learn the optimal strategy\nthat ﬁts users’ preferences, they rely on predeﬁned and ﬁxed\nneural architectures with high complexity. In doing so, they\ncannot dynamically adjust their neural architectures over the\nuser sessions based on the achieved performance by the\nrecommendation strategy [23].\nAccounting for the importance of adaptive neural architec-\ntures in recommender systems, the main contribution of this\nstudy is summarized as follows:\n• We investigate the main limitation of recent state-of-\nthe-art reinforcement learning strategies in recommender\nsystems, which are built on ﬁxed neural architectures over\nthe recommendation sessions.\n• In addition, we present how adaptive neural architectures\nare developed for consecutive tasks in other research\ndomains.\n• To bridge this gap we discuss the key challenges to pro-\nduce recommendations by dynamically adapting neural\narchitectures via deep reinforcement learning.\nII. REINFORCEMENT LEARNING IN RECOMMENDER\nSYSTEMS\nSeveral attempts have been made to generate recommenda-\ntions in real-time via RL, assuming that users keep interacting\nwith the recommender system. For example, Taghipour et\nal. [18] formulate a Q-Learning problem and learn to generate\nrecommendations based on user feedback on the Web. This\napproach inherits the intrinsic characteristic of RL, performing\na constant learning process. Mahmood et al. [17] adopt the RL\ntechnique to observe the responses of users in conversational\nrecommender systems. The goal is to maximize a cumulative\nreward function which corresponds to the beneﬁt that users\nreceive from a recommendation session.\nRecently, Zheng et al. [9] propose a RL framework to\ngenerate online news recommendation. Bonner and Flavian [7]\nintroduce a RL algorithm to learn from logged data based on a\nbiased recommendation policy. This algorithm tries to compute\nthe best recommendation policy for maximizing the reward\nby taking into account the control recommendation policy for\neach user. Zhao et al. [15] propose a deep RL strategy for\ngenerating page-wise recommendations based on the actor-\ncritic framework [24]. This approach can optimize a page of\nitems with proper display based on real-time feedback from\nusers. Bai et al. [25] present a self-attentive recommendation\nmodel for capturing the evolving demands of users over time\nfocusing on long-term demands e.g., repeated purchasing with\na persistent interest and short-term demands e.g., buying the\ncomplementary purchasing in a short time period. However,\nthis study does not aim at generating long-term novel rec-\nommendations but repeated ones. Gui et al. [26] introduce\na cooperative multi-agent reinforcement learning model for\nmention recommendation on Twitter. Shang et al. [27] adopt\na multi-agent generative adversarial reinforcement learning\nframework for recommendations on a ride-hailing platform.\nZou et al. [28] introduce a deep RL framework for short\nand long-term click prediction. However, this framework does\nnot produce personalised recommendations by focusing on the\nclick prediction problem. Zhao et al [20] present DEERS, a\npairwise deep RL framework. In this work, the best strategies\nare computed via recommending trial-and-error items and\nreceiving reinforcements of these items from both positive\nand negative user feedback. Lei et al. [29] demonstate how\nto use a RL agent for a Graph Convolutional Q-network,\naiming to compute recommendation policies based on the\ngraph-structured representations. Xin et al. [30] propose a self-\nsupervised RL strategy for sequential recommendations. Hong\net al. [31] introduce a music recommendation framework for\nadapting to user’s current preference based on reinforcement\nlearning in real time during a listening session. Zhao et al. [32]\npresent a deep hierarchical reinforcement learning framework\nto capture the long-term sparse conversion interest at the high\nlevel and automatically set abstract goals. At the the low-level\nof the hierarchy, the learning strategy tries to meet the abstract\ngoals and model short-term click interest when interacting with\na real-time environment.\nIn addition, Zhou et al. [33] investigate how to exploit\nknowledge graphs (KG) for RL in recommender systems.\nInstead of learning RL policies from scratch, this approach ﬁrst\nexploits the prior knowledge of the item correlation from KG\nto guide the candidate selection, enriches the representation\nof items and user states, and then propagates user preferences\namong the correlated items to handle the sparsity in user\npreferences. In a similar spirit, Wang et al. [34] generate\nsequential recommendations based on RL and KG. Zhao et\nal. [35] supervise the path ﬁnding process in KG via an RL\nframework to produce explainable recommendations.\nThe main limitation of the aforementioned state-of-the-\nart RL strategies in recommender systems, is that the neural\narchitectures are ﬁxed during training. Consequently, they\ndo not focus on the recommendation performance based on\nthe user real-time feedback and do not adapt the neural\narchitectures over the recommendation sessions in real-time,\naccordingly. This might signiﬁcantly limit the performance\nof deep RL strategies when producing recommendations, as\nfor example occurs in the case of consecutive tasks in other\nresearch domains [36]–[40].\nIII. ADAPTIVE NEURAL NETWORKS\nInstead of having a ﬁxed neural architecture during training,\nrecent studies introduce models with the ability to learn\nconsecutive tasks, such as natural language processing [41],\nspeech synthesis [42], image segmentation [43], object detec-\ntion [44] and domain independent feature decomposition [45]\nin the cross-domain retrieval task [46]. The goal of such\nmodels is to solve new tasks, without forgetting the ac-\nquired knowledge from previous tasks, also known as lifelong\nlearning in neural architectures [47]. The main idea is for\nevery new task to dynamically augment the neural architecture\nwhile preserving the parameters of the previous architecture\nunchanged. In particular, provided that in the real-world setting\nwe have to train the neural network in consecutive tasks, in the\nlearning strategy of the neural network we have to introduce\nregularization terms to preserve the model parameters similar\nto the learned parameters of the previous sessions, by consid-\nering in each new session their importance for each task [36],\n[37], [48].\nA few studies explore neural networks capable of dy-\nnamically adjusting the “capacity” of the neural architecture,\nthat is the numbers of the hidden layers and units when\ntraining the neural network. For example, Zhou et al. [49]\nperform incremental learning for a denoising autoencoder by\nadding new neurons for a certain group of “difﬁcult” training\nexamples. This is achieved by setting high loss for each\ngroup of “difﬁcult” examples, and then combine them with\nother neurons to reduce unecessary complexity of the neural\nnetwork and avoid redundancy. Provided a ﬁxed size of hidden\nunits and layers, Progressive Neural Networks (PNN) [39],\n[40] expand the neural architecture. In practice though, this\nexpansion strategy in PNN results in a large network structure\nfor many sequential tasks, as for each new task the PNN\nstucture is signiﬁcantly augmented. This means that in the\nPNN model the network architecture might become extremely\nlarge. This problem becomes even more challenging in the\ncase of recommender systems where users continously interact\nto provide the system with feedback in different forms such as\nratings, views, clicks and so on. The deep learning strategy of\nDynamically Expandable Network (DEN) [38] tries to face\nthe problem of high complexity by setting group sparsity\nregularization terms to the newly added parameters in each\nnew session. Nonetheless, in practice this strategy introduces\nmany hyperparameters in DEN. As a consequence, this results\nin several regularization and thresholding hypeparameters,\nrequiring a lot of effort to tune the model and improve its\nperformance.\nXu et al. [48] propose a reinforced continual learning\nframework for the image classiﬁcation task. This framework\nadapts the neural architecture by adjusting the numbers of\nhidden units and layers, accordingly. The reinforced continual\nlearning framework consists of the following three networks:\nthe controller, the user-session network, and the value network.\nThe controller is a Long Short-Term Memory network (LSTM)\nfor generating policies and computing how many hidden units\nand layers will be added in each session. The user-session\nnetwork is an extension of the DQN model by adjusting the\nneural architecture according to the performance accuracy of\na reinforcement agent. The value network is designed as a\nfully-connected network, which approximates the value of the\nstate per session.\nDifferent from baseline strategies with dynamically adjusted\nneural architectures which mainly focus on classiﬁcation\ntasks, recommendation strategies have to adapt the underlying\nneural architecture by capturing users’ preference dynamics\nover the sessions to boost the recommendation accuracy in a\npersonalized manner.\nIV. KEY CHALLENGES\nRecently, deep RL strategies have been widely used in\nrecommender systems to capture user feedback in real time\nand generate recommendation strategies. However, there is a\nstill a technological gap between baseline deep RL strategies\nwith ﬁxed neural architectures and adaptive ones developed\nin other research domains. In addition, we have to account\nfor the fact that neural architectures are not always the most\nsuitable learning strategy in recommender systems, as pointed\nout in [50].\nThe key challenges to develop a deep RL strategy with an\nadaptive neural architecture are summarized as follows:\n• When new user feedback/data arrive in a recommen-\ndation session, how can we formulate an optimization\nproblem to decide the optimal number of hidden units\nand/or layers in a existing neural architecture? In par-\nticular, the main challenge is to ﬁnd the optimal way\nto conﬁgure the neural network in each recommendation\nsession, considering both users long-term and short-term\npreferences.\n• In deep RL strategies, the challenge is to ﬁnd how to\nmodel a reward signal by taking into account both the\nrecommendation accuracy and the neural architecture’s\ncomplexity over the user sessions.\n• Provided that deep RL strategies require the computa-\ntion of a high number of model parameters, how can\nwe develop an adaptive strategy to adjust the neural\narchitecture based on the recommendation performance\nover the user sessions, while at the same time preserving\nthe model complexity low? In the real-world setting, con-\ntinuous augmentation of an existing neural architecture\nmight result to prohibited training cost. This means that\nresearchers have to be careful in the expansion strategy\nof the neural architecture when new user data arrive in a\nrecommendation session, so as to preserve a medium-size\nnetwork.\n• Which are reliable indicators to evaluate the trade-off\nbetween quality of recommender systems based on deep\nRL strategies and the model complexity of an adaptive\nneural architecture?\nDesigning an adaptive neural architecture in an RL frame-\nwork can be beneﬁciary in recommender systems. However,\nthis is a challenging task where researcher have to take\ninto account several factors, such as preserving the model\ncomplexity low and adjusting the neural architecture according\nto users’ preference dynamics in the recommendation sessions.\nREFERENCES\n[1] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factoriza-\ntion. In NIPS, pages 1257–1264, 2007.\n[2] Steffen Rendle.\nFactorization machines.\nIn ICDM, pages 995–1000,\n2010.\n[3] Travis Ebesu, Bin Shen, and Yi Fang. Collaborative memory network\nfor recommendation systems. In SIGIR, pages 515–524, 2018.\n[4] Sheng Li, Jaya Kawale, and Yun Fu. Deep collaborative ﬁltering via\nmarginalized denoising auto-encoder. In CIKM, pages 811–820, 2015.\n[5] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-\nSeng Chua. Neural collaborative ﬁltering. In WWW, pages 173–182,\n2017.\n[6] Hao Wang, Naiyan Wang, and Dit-Yan Yeung.\nCollaborative deep\nlearning for recommender systems. In KDD, pages 1235–1244, 2015.\n[7] Stephen Bonner and Flavian Vasile. Causal embeddings for recommen-\ndation. In RecSys, pages 104–112, 2018.\n[8] Bal´azs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos\nTikk. Session-based recommendations with recurrent neural networks.\nIn ICLR, 2016.\n[9] Guanjie\nZheng,\nFuzheng\nZhang,\nZihan\nZheng,\nYang\nXiang,\nNicholas Jing Yuan, Xing Xie, and Zhenhui Li.\nDRN: A deep\nreinforcement learning framework for news recommendation. In WWW,\npages 167–176, 2018.\n[10] Yehuda Koren. Collaborative ﬁltering with temporal dynamics. Com-\nmun. ACM, 53(4):89–97, 2010.\n[11] Dimitrios Rafailidis and Alexandros Nanopoulos. Repeat consumption\nrecommendation based on users preference dynamics and side informa-\ntion. In WWW - Companion Volume, pages 99–100, 2015.\n[12] Bal´azs Hidasi and Alexandros Karatzoglou. Recurrent neural networks\nwith top-k gains for session-based recommendations. In CIKM, pages\n843–852, 2018.\n[13] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. STAMP:\nshort-term attention/memory priority model for session-based recom-\nmendation. In KDD, pages 1831–1839, 2018.\n[14] Zhitao Wang, Chengyao Chen, Ke Zhang, Yu Lei, and Wenjie Li.\nVariational recurrent model for session-based recommendation.\nIn\nCIKM, pages 1839–1842, 2018.\n[15] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and\nJiliang Tang. Deep reinforcement learning for page-wise recommenda-\ntions. In RecSys, pages 95–103, 2018.\n[16] Yujing Hu, Qing Da, Anxiang Zeng, Yang Yu, and Yinghui Xu. Rein-\nforcement learning to rank in e-commerce search engine: Formalization,\nanalysis, and application. In KDD, pages 368–377, 2018.\n[17] Tariq Mahmood and Francesco Ricci. Improving recommender systems\nwith adaptive conversational strategies. In ACM Conference on Hypertext\nand Hypermedia, pages 73–82, 2009.\n[18] Nima Taghipour and Ahmad A. Kardan. A hybrid web recommender\nsystem based on q-learning. In SAC, pages 1164–1168, 2008.\n[19] Michael J. Kearns, Yishay Mansour, and Andrew Y. Ng.\nA sparse\nsampling algorithm for near-optimal planning in large markov decision\nprocesses. Machine Learning, 49(2-3):193–208, 2002.\n[20] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and\nDawei Yin. Recommendations with negative feedback via pairwise deep\nreinforcement learning. In KDD, pages 1040–1048, 2018.\n[21] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,\nTom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous\ncontrol with deep reinforcement learning. In ICLR, 2016.\n[22] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,\nIoannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing\natari with deep reinforcement learning. CoRR, abs/1312.5602, 2013.\n[23] Yang Yang, Da-Wei Zhou, De-Chuan Zhan, Hui Xiong, and Yuan Jiang.\nAdaptive deep models for incremental learning: Considering capacity\nscalability and sustainability. In KDD, pages 74–82, 2019.\n[24] Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an\nintroduction. Adaptive computation and machine learning. MIT Press,\n1998.\n[25] Ting Bai, Lixin Zou, Wayne Xin Zhao, Pan Du, Weidong Liu, Jian-Yun\nNie, and Ji-Rong Wen. Ctrec: A long-short demands evolution model\nfor continuous-time recommendation. In SIGIR, pages 675–684, 2019.\n[26] Tao Gui, Peng Liu, Qi Zhang, Liang Zhu, Minlong Pengn, Yunhua\nZhou, and Xuanjing Huang. Mention recommendation in twitter with\ncooperative multi-agent reinforcement learning. In SIGIR, pages 535–\n544, 2019.\n[27] Wenjie Shang, Yang Yu, Qingyang Li, Zhiwei Qin, Yiping Meng, and\nJieping Ye.\nEnvironment reconstruction with hidden confounders for\nreinforcement learning based recommendation.\nIn KDD, pages 566–\n576, 2019.\n[28] Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu,\nand Dawei Yin.\nReinforcement learning to optimize long-term user\nengagement in recommender systems. In KDD, pages 2810–2818, 2019.\n[29] Yu Lei, Hongbin Pei, Hanqi Yan, and Wenjie Li. Reinforcement learning\nbased recommendation with graph convolutional q-network. In SIGIR,\npages 1757–1760, 2020.\n[30] Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M.\nJose. Self-supervised reinforcement learning for recommender systems.\nIn SIGIR, pages 931–940, 2020.\n[31] Daocheng Hong, Yang Li, and Qiwen Dong. Nonintrusive-sensing and\nreinforcement-learning based adaptive personalized music recommenda-\ntion. In SIGIR, pages 1721–1724, 2020.\n[32] Dongyang Zhao, Liang Zhang, Bo Zhang, Lizhou Zheng, Yongjun\nBao, and Weipeng Yan.\nMahrl: Multi-goals abstraction based deep\nhierarchical reinforcement learning for recommendations.\nIn SIGIR,\npages 871–880, 2020.\n[33] Sijin Zhou, Xinyi Dai, Haokun Chen, Weinan Zhang, Kan Ren, Ruiming\nTang, Xiuqiang He, and Yong Yu.\nInteractive recommender system\nvia knowledge graph-enhanced reinforcement learning. In SIGIR, pages\n179–188, 2020.\n[34] Pengfei Wang, Yu Fan, Long Xia, Wayne Xin Zhao, ShaoZhang Niu,\nand Jimmy Huang. KERL: A knowledge-guided reinforcement learning\nmodel for sequential recommendation. In SIGIR, pages 209–218, 2020.\n[35] Kangzhi Zhao, Xiting Wang, Yuren Zhang, Li Zhao, Zheng Liu, Chunx-\niao Xing, and Xing Xie. Leveraging demonstrations for reinforcement\nrecommendation reasoning over knowledge graphs.\nIn SIGIR, pages\n239–248, 2020.\n[36] Abel S. Zacarias and Lu´ıs A. Alexandre.\nSena-cnn: Overcoming\ncatastrophic forgetting in convolutional neural networks by selective\nnetwork augmentation. In ANNPR, pages 102–112, 2018.\n[37] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning\nthrough synaptic intelligence. In ICML, pages 3987–3995, 2017.\n[38] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong\nlearning with dynamically expandable networks. In ICLR, 2018.\n[39] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert\nSoyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and\nRaia Hadsell.\nProgressive neural networks.\nCoRR, abs/1606.04671,\n2016.\n[40] Yilin Shen, Xiangyu Zeng, Yu Wang, and Hongxia Jin. User information\naugmented semantic frame parsing using progressive neural networks.\nIn INTERSPEECH, pages 3464–3468, 2018.\n[41] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard\nSocher. A joint many-task model: Growing a neural network for multiple\nNLP tasks. In EMNLP, pages 1923–1933, 2017.\n[42] Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, and Simon\nKing. Deep neural networks employing multi-task learning and stacked\nbottleneck features for speech synthesis. In ICASSP, pages 4460–4464,\n2015.\n[43] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross B. Girshick. Mask\nR-CNN. In CVPR, pages 2980–2988, 2017.\n[44] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. In\nCVPR, pages 6517–6525, 2017.\n[45] Xinxun Xu, Muli Yang, Yanhua Yang, and Hao Wang.\nProgres-\nsive domain-independent feature decomposition network for zero-shot\nsketch-based image retrieval. In IJCAI, pages 984–990, 2020.\n[46] Dimitrios Rafailidis and Fabio Crestani.\nCluster-based joint matrix\nfactorization hashing for cross-modal retrieval. In SIGIR, pages 781–\n784, 2016.\n[47] German Ignacio Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan,\nand Stefan Wermter. Continual lifelong learning with neural networks:\nA review. Neural Networks, 113:54–71, 2019.\n[48] Ju Xu and Zhanxing Zhu. Reinforced continual learning. In NIPS, pages\n907–916, 2018.\n[49] Guanyu Zhou, Kihyuk Sohn, and Honglak Lee.\nOnline incremental\nfeature learning with denoising autoencoders. In AISTATS, pages 1453–\n1461, 2012.\n[50] Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. Are\nwe really making much progress? A worrying analysis of recent neural\nrecommendation approaches. In RecSys, pages 101–109, 2019.\n",
  "categories": [
    "cs.IR",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2020-11-11",
  "updated": "2020-11-11"
}