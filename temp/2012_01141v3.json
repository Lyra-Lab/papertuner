{
  "id": "http://arxiv.org/abs/2012.01141v3",
  "title": "Algebraically-Informed Deep Networks (AIDN): A Deep Learning Approach to Represent Algebraic Structures",
  "authors": [
    "Mustafa Hajij",
    "Ghada Zamzmi",
    "Matthew Dawson",
    "Greg Muller"
  ],
  "abstract": "One of the central problems in the interface of deep learning and mathematics\nis that of building learning systems that can automatically uncover underlying\nmathematical laws from observed data. In this work, we make one step towards\nbuilding a bridge between algebraic structures and deep learning, and introduce\n\\textbf{AIDN}, \\textit{Algebraically-Informed Deep Networks}. \\textbf{AIDN} is\na deep learning algorithm to represent any finitely-presented algebraic object\nwith a set of deep neural networks. The deep networks obtained via\n\\textbf{AIDN} are \\textit{algebraically-informed} in the sense that they\nsatisfy the algebraic relations of the presentation of the algebraic structure\nthat serves as the input to the algorithm. Our proposed network can robustly\ncompute linear and non-linear representations of most finitely-presented\nalgebraic structures such as groups, associative algebras, and Lie algebras. We\nevaluate our proposed approach and demonstrate its applicability to algebraic\nand geometric objects that are significant in low-dimensional topology. In\nparticular, we study solutions for the Yang-Baxter equations and their\napplications on braid groups. Further, we study the representations of the\nTemperley-Lieb algebra. Finally, we show, using the Reshetikhin-Turaev\nconstruction, how our proposed deep learning approach can be utilized to\nconstruct new link invariants. We believe the proposed approach would tread a\npath toward a promising future research in deep learning applied to algebraic\nand geometric structures.",
  "text": "ALGEBRAICALLY-INFORMED DEEP NETWORKS (AIDN):\nA DEEP LEARNING APPROACH TO REPRESENT ALGEBRAIC\nSTRUCTURES\nMUSTAFA HAJIJ, GHADA ZAMZMI, MATTHEW DAWSON, AND GREG MULLER\nAbstract. One of the central problems in the interface of deep learning and mathematics is that\nof building learning systems that can automatically uncover underlying mathematical laws from\nobserved data. In this work, we make one step towards building a bridge between algebraic structures\nand deep learning, and introduce AIDN, Algebraically-Informed Deep Networks. AIDN is a deep\nlearning algorithm to represent any ﬁnitely-presented algebraic object with a set of deep neural\nnetworks. The deep networks obtained via AIDN are algebraically-informed in the sense that they\nsatisfy the algebraic relations of the presentation of the algebraic structure that serves as the input\nto the algorithm. Our proposed network can robustly compute linear and non-linear representations\nof most ﬁnitely-presented algebraic structures such as groups, associative algebras, and Lie algebras.\nWe evaluate our proposed approach and demonstrate its applicability to algebraic and geometric\nobjects that are signiﬁcant in low-dimensional topology. In particular, we study solutions for the\nYang-Baxter equations and their applications on braid groups. Further, we study the representations\nof the Temperley-Lieb algebra. Finally, we show, using the Reshetikhin-Turaev construction, how\nour proposed deep learning approach can be utilized to construct new link invariants. We believe\nthe proposed approach would tread a path toward a promising future research in deep learning\napplied to algebraic and geometric structures.\n1. Introduction\nOver the past years, deep learning techniques have been used for solving partial diﬀerential\nequations [26,27,37,44], obtaining physics-informed surrogate models [38,40], computing the Fourier\ntransform with deep networks [28], ﬁnding roots of polynomials [17], and solving non-linear implicit\nsystem of equations [45]. In this work, we make one step towards building the bridge between\nalgebraic/geometric structures and deep learning, and aim to answer the following question: How\ncan deep learning be used to uncover the underlying solutions of an arbitrary system of algebraic\nequations?\nTo answer this question, we introduce Algebraically-Informed Deep Networks (AIDN), a deep\nlearning method to represent any ﬁnitely-presented algebraic object with a set of neural networks.\nOur method uses a set of deep neural networks to represent a set of formal algebraic symbols that\nsatisfy a system of algebraic relations. These deep neural networks are simultaneously trained\nto satisfy the relations between these symbols using an optimization paradigm such as stochastic\ngradient descent (SGD). The resulting neural networks are algebraically-informed in the sense that\nthey satisfy the algebraic relations. We show that a wide variety of mathematical problems can be\nsolved using this formulation.\nNext, we discuss a motivating example and present the applicability of our method on the\nwell-known Yang-Baxter equation [20], which has been extensively studied in mathematics and\nphysics.\n1.1. Motivating Example:\nThe Yang-Baxter Equation. To solve the set-theoretic Yang-\nBaxter equation, one seeks an invertible function R : A × A →A × A, where A is some set, that\nsatisﬁes the following equation:\n(1.1)\n(R × idA) ◦(idA × R) ◦(R × idA) = (idA × R) ◦(R × idA) ◦(idA × R).\n1\narXiv:2012.01141v3  [cs.LG]  12 Feb 2021\nFinding solutions of the above equation has a long history and proved to be a very diﬃcult problem.\nFor many decades, the Yang–Baxter equation1 has been studied in quantum ﬁeld theory and\nstatistical mechanics as the master equation in integrable models [20]. Later, this equation was\napplied to many problems in low-dimensional topology [21]. For example, solutions of the Yang-\nBaxter equation were found to induce representations of the braid groups and have been used\nto deﬁne knots and 3-manifold invariants [48]. Today, the Yang-Baxter equation is considered\na cornerstone in several areas of physics and mathematics [49] with applications to quantum\nmechanics [22], algebraic geometry [25], and quantum groups [47].\nAs an example application, we show how the proposed AIDN can be utilized to solve the\nYang-Baxter equation [8]. In particular, assuming A ⊆Rn is a subset of a Euclidean space2, AIDN\nrealizes the desired solution R in equation 1.1 as a neural network fR(θ), where θ ∈Rk. Using SGD,\nwe can ﬁnd the parameters θ by optimizing a loss function, which essentially satisﬁes equation 1.1.\nMore details are given in Section 4.1.\n1.2. Related Work. Our work can be viewed as a part of the quest to discover knowledge and a step\ntowards building learning systems that are capable of uncovering the underlying mathematical and\nphysical laws from data. Examples of current deep learning eﬀorts to solve problems in mathematics\nand physics include general methods to solve partial diﬀerential equations [26,27,37,44] or more\nparticular ones that are aimed at solving single equations such as the Schr¨odinger equation [34].\nAlso, deep learning has been used to solve equations related to ﬂuid mechanics [4,50], non-linear\nequations [33,45] and transcendental equations [19].\nThis work can also be viewed as a step towards advancing computational algebra [31,42]. Although\nthere is a large literature devoted to computing linear representations of ﬁnitely-presented algebraic\nobjects [16] and of ﬁnite groups in particular [7, 46] as well as few works about representation\nof algebras [9], we are not aware of any algorithm that computes non-linear representations of\nalgebraic structures. Further, existing works ﬁnd the representations of algebraic structures in\nspecial cases [1]; the majority of these algorithms utilize GAP [13], a system for computational\ndiscrete algebra. Our proposed AIDN can (1) compute both linear and non-linear representations\nof algebraic structures, (2) provide a general computational scheme that utilizes non-traditional\ntools, and (3) oﬀer a diﬀerent paradigm from the classical methods in this space.\n1.3. Summary of Contribution. The main contributions of this work can be summarized as\nfollows:\n(1) We propose AIDN, a deep learning algorithm that computes non-linear representations\nof algebraic structures. To the best of our knowledge, we are the ﬁrst to propose a deep\nlearning-based method for computing non-linear representations of any ﬁnitely presented\nalgebraic structure.\n(2) We demonstrate the applicability of AIDN in low-dimensional topology. Speciﬁcally, we\nstudy the applicability of AIDN to braid groups and Templery-Lieb algebras, two algebraic\nconstructions that are signiﬁcant in low-dimensional topology.\n(3) We utilize AIDN for knot invariants discovery using deep learning methods. Speciﬁcally,\nusing the Reshetikhin-Turaev construction we show that AIDN can be used to construct\nnew link invariants.\nThe rest of the paper is organized as follows. Section 2 reviews the basics of neural networks that\nare needed for our settings. In Section 3, we present our AIDN method. In Section 4, we study the\napplication of AIDN algorithm to the braid groups and Templery-Lieb algebras. In Section 5 we\n1Technically, the term Yang-Baxter equation is utilized whenever the map R is linear. When the map R is an arbitrary\nmap deﬁned on a set, the term set-theoretic Yang-Baxter is used instead.\n2We may consider real or complex Euclidean spaces, but for this example we will constrain our discussion on\nreal-Euclidean spaces.\n2\n(a)\n(b)\nNet2\nNet1\nNet1\nNet2\n◦\nNet2\nNet1\nNet2\nNet1\n×\nFigure 1.\n(a) Composition of two nets; (b) product of two nets.\nshow how AIDN can be utilized to obtain invariants of knots and links. Finally, we discuss the\nlimitations and conclude the paper in Section 6.\n2. Background: Algebraic Structures with Deep Neural Networks\nThis section provides a brief introduction to neural networks and shows examples of the algebraic\nstructures that can be deﬁned on them. We only focus on real-neural networks with domains and\nco-domains on real Euclidean spaces for the sake of clarity and brevity. However, AIDN can be\neasily extended to complex-neural networks [15,23].\nA neural network, or simply network, is a function Net : Rdin −→Rdout deﬁned by a composition\nof the form:\n(2.1)\nNet := fL ◦· · · ◦f1\nwhere the functions fi, 1 ≤i ≤L called the layer functions. A layer function fi : Rni −→Rmi\nis typically a continuous, piecewise smooth, or smooth function of the following form: fi(x) =\nαi(Wi(x) + bi), where Wi is an mi × ni matrix, bi is a vector in Rmi, and αi : R →R is an\nappropriately chosen nonlinear function applied coordinate-wise to an input vector (z1, · · · , zmi) to\nget a vector (α(z1), · · · , α(zmi)).\nWe will use N(Rn) to denote the set of networks of the form Net : Rn →Rn. Note that N(Rn)\nis closed under composition of functions. If Net1 ∈N(Rm) and Net2 ∈N(Rn), then we deﬁne\nNet1 × Net2 ∈N(Rm × Rn) via (Net1 × Net2)(x, y) := (Net1(x), Net2(y)) for x ∈Rn and y ∈Rm.\nWe often use the graphical notation illustrated in Figure 1 to denote the composition and the\nproduct operations on networks.\nThe set N(Rn), or a subset of it, admits natural algebraic structures as follows. For two network\nNet1, Net2 ∈N(Rn), we can deﬁne their addition Net1+Net2 simply by (Net1+Net2)(x) = Net1(x)+\nNet2(x) for every x ∈Rn. Similarly, if a ∈R then a ∗Net is deﬁned via (a ∗Net)(x) = a ∗Net(x).\nFunction composition, addition, and scalar multiplication deﬁnes an associative R-algebra structure\non N(Rn). We will denote the group inside N(Rn) of all invertible networks by G(Rn). Finally, we\ncan deﬁne, on the associative algebra N(Rn), a Lie algebra structure by deﬁning the Lie bracket\nas [Net1, Net2] := Net1 ◦Net2 −Net2 ◦Net1. Given the above setting, we can now represent many\ntypes of algebraic structures inside N(Rn).\n3. Method: Algebraically-Informed Deep Networks (AIDN)\nThe motivation example provided in Section 1.1 can be deﬁned formally and generally as follows.\nLet s1, . . . , sn be a collection of formal symbols (generators) that satisfy a system of equations\nr1, . . . , rk which describe a formal set of equations these generators satisfy. We are interested in\nﬁnding functions fs1, . . . fsn (deﬁned on some domain) that correspond to the formal generators\ns1, . . . sn and satisfy the same relations r1, . . . , rk.\nLet the sets {si}n\ni=1 and {ri}k\ni=1 be denoted by S and R, respectively. Such a system ⟨S | R⟩is\ncalled a presentation. Depending on the algebraic operations that we are willing to allow while\nsolving these algebraic equations, presentations can encode diﬀerent algebraic objects. For example,\nif we allow multiplication between the algebraic objects and require that this multiplication be\n3\nassociative, have a multiplicative identity, and admit multiplicative inverses for all objects, then the\nresulting algebraic structure induced by the presentation ⟨S | R⟩is a group, and the presentation\ncan be thought of as a compact deﬁnition of the group3. On the other hand, ﬁnding functions\n{fi}n\ni=1 that correspond to the generators S and satisfy the relations R is formally equivalent to\nﬁnding a homomorphism from the algebraic structure ⟨S | R⟩to another algebraic structure where\nthe functions {fi}n\ni=1 live.\nFrom this perspective, AIDN can be formally thought of as taking an algebraic structure given\nas a ﬁnite presentation; that is, a set of generators {si}n\ni=1 and a set of relations {ri}k\ni=1, and\nproducing a set of neural nets {fi(x; θi)}n\ni=1, where θi ∈Rki is the parameter vector of the network\nfi, such that these neural nets correspond to the generators {si}n\ni=1 and satisfy the relations {ri}k\ni=1.\nThe proposed AIDN ﬁnds the weights {θi}n\ni=1 of the networks {fi(x; θi)}n\ni=1 by deﬁning the loss\nfunction as follows:\n(3.1)\nL(f1, · · · , fn) :=\nk\nX\ni=1\n||F(ri)||2\n2,\nwhere F(ri) is the relation ri written in terms of the networks {fi(x; θi)} and ||.||2 is the L2 norm.\nThis loss can be minimized using one of the versions of SGD [3].\nFor instance, to solve the set-theoretic Yang-Baxter equation (Equation 1.1), AIDN treats the\nproblem of ﬁnding fR(θ) as an optimization problem with the following objective function:\n(3.2)\nL(fR) := ||(fR(θ)×idA)◦(idA×fR(θ)◦(fR(θ)×idA)−(idA×fR(θ))◦(fR(θ)×idA)◦(idA×fR(θ))||2\n2.\nThe invertibility of the map fR can be realized in multiple ways. For example, one may choose to\ntrain a neural network that is invertable. This can be done using multiple well-studied methods\n(e.g., [2,18]). Alternatively, one may choose to impose invertibility inside the loss function, deﬁne\nanother map gR(α), and change the loss function in Equation 3 to the following:\nL(fR, gR) := ||(fR(θ) × idA) ◦(idA × fR(θ) ◦(fR(θ) × idA) −(idA × fR(θ)) ◦(fR(θ) × idA) ◦(idA × R)||2\n2\n+ ||fR(θ) ◦gR(α) −idA×A||2\n2 + ||gR(α) ◦fR(θ) −idA×A||2\n2.\nWe observed that this second method yields more stable solutions in practice. A summary of\nAIDN algorithm is given in 1 below. Further details about the implementation are provided in\nAppendix A.1.\nAlgorithm 1: AIDN: Algebraically-Informed Deep Nets\n1 Function AIDN(⟨S | R⟩, k), S = {si}n\ni=1 is a set of generator and R = {ri}k\ni=1 is a set of\nrelations. Dimension of the representation k.\n2\nforeach Generator si in S do\n3\nDeﬁne the network fi(x; θi) ∈N(Rk);\n4\nL(f1, · · · , fn) := Pk\ni=1 ||F(ri)||2\n2, where F(ri) is the relation ri written in terms of the\nnetworks {fi(x; θi)}n\ni=1.\n5\nMinimize L(f1, · · · , fn) using stochastic gradient descent.\n6\nreturn {fi(θ)}n\ni=1\n7 End Function\nConceptually, Algorithm 1 is simple yet it is very general in its applicability to a large set of\nalgebraic objects. Although there is no theoretical guarantee that the neural networks {fi(θ)}n\ni=1\n3One should keep in mind that a group can in general have diﬀerent presentations. Furthermore, even the problem of\ndetermining which presentations give the trivial group is known to be algorithmically undecidable [32,36].\n4\nexist as there is no guarantee that the loss function deﬁned in the AIDN algorithm converges to a\nglobal minimum, we consistently found during our experiments that AIDN is capable of achieving\ngood results and ﬁnding the desired functions given enough expressive power [6,14,30] for {fi(θ)}n\ni=1\nand enough sample points from the domains of these networks. This observation is consistent with\nother open research questions in theoretical deep learning [43] concerning the loss landscape of a\ndeep net. Speciﬁcally, multiple research eﬀorts (e.g., [5,41]) consistently reported that the parameter\nlandscape of deep networks has a large number of local minima that reliably yield similar levels of\nperformance on multiple experiments. Moreover, the local minima of these landscapes are likely to\nbe close to the global one [5].\nNote that the choice of the architecture of the neural network determines the type of the algebraic\nobject representation (e.g. linear, non-linear, etc). In Section 4.1 and Section 4.2, we will study\nmultiple architectures of neural networks.\nIn what follows, we explore the applicability of AIDN on diﬀerent algebraic structures and\nhighlight its properties and performance. All code and data used in this manuscript are publicly\navailable at https://github.com/mhajij/Algebraically_Informed_Deep_Nets/.\n4. AIDN for Finitely-presented Algebraic Structures\nOur proposed method is a universal treatment for representing ﬁnitely-presented algebraic\nstructures. We demonstrate this generality by applying it to multiple structures that are signiﬁcant\nin both geometric topology and algebra including the braid groups (4.1) and the Temperley-Lieb\nalgebras (4.2).\n4.1. Braid Group. Let D3 denotes the cube [0, 1]3 in 3-dimensional space, and ﬁx m points on\nthe top face of D3 and m points on the bottom face. A braid on m strands is a curve βm embedded\nin D3 and decomposed into m arcs such that it meets D3 orthogonally in exactly 2m points and\nwhere no arc intersects any horizontal plane more than once. A braid is usually represented by\na planar projection or a braid diagram. In the braid diagram, we make sure that the over-strand\nis distinguishable from the under-strand at each crossing by creating a break in the under-strand.\nFigure 2 shows an example of a braid diagram on 3 strands.\nFigure 2. An example of a braid diagram on 3 strands.\nThe set of all braids Bm has a group structure with multiplication as follows. Given two m-strand\nbraids β1 and β2, the product of these braids (β1 ·β2) is the braid given by the vertical concatenation\nof β1 on top of β2 as shown in Figure 3(a).\nThe group structure of Bm follows directly from this. The braid group Bm on m strands can\nbe described algebraically in terms of generators and relations using Artin’s presentation. In this\npresentation, the group Bm is given by the generators:\nσ1, . . . , σm−1,\nsubject to the relations:\n(1) For |i −j| > 1: σiσj = σjσi.\n5\n(a)\n(b)\nβ1\nβ2\nβ1\nβ2\n.\nβ2\nβ1\nβ2\nβ1\n×\nFigure 3. The product of two braids.\n(2) For all i < m −1: σiσi+1σi = σi+1σiσi+1.\nThe correspondence between the pictorial deﬁnition of the braid group and the algebraic deﬁnition\nis given by sending the generator σi to the picture illustrated in Figure 4.\n1\ni\ni + 1\nm\nFigure 4. The braid group generator σi.\nLet β1 ∈Bm and β2 ∈Bn. The product of the braids β1 and β1, denoted by β1 × β2, is the braid\ngiven by horizontal concatenation of β1 to the left of β2 as indicated in Figure 3 (b). If we use id to\ndenote the lone strand and σ to denote the crossing appear in the i and i + 1 position in Figure 4,\nthen using the graphical notation given in Figure 3(b) we can write:\n(4.1)\nσi = (×i−1id) × σ × (×m−i+1id),\nwhere ×kid means taking the product of the identity strand k times. This notation will be used\nthroughout the paper. Note that using the graphical notation, the braid relations have intuitive\nmeaning which is illustrated in Figure 5. In particular, the so-called Reidemeister 3 corresponds to\nthe relation in Figure 5(a) and Reidemeister 2 corresponds to the relation in Figure 5(b). Finally,\nthe relation in Figure 5(c) means that the generators can commute as long as they are suﬃciently\nfar away from each other.\n(a)\n(b)\n \n(c)\nFigure 5. The braid relations.\n4.1.1. Representing the Braid Group Via Neural Networks. We now have enough background and\nnotations to deﬁne the neural networks that can be utilized to represent the braid group.\nLet Bm be the braid group with generators relations as deﬁned earlier. Using AIDN, one might\nthink that we need to train m −1 neural networks, since Bm has m −1 generators. However,\nEquation 4.1 allows us to write every generator in terms of the identity strand, σ, as well as the\nproduct operation given in Figure 3. This observation along with the deﬁnition of the braid group\nimplies the following Lemma.\n6\nLemma 4.1. Let m, n ≥1, f, g ∈N(Rn × Rn). Deﬁne fi ∈N((Rn)m) by\n(4.2)\nfi := (×i−1idRn) × f × (×m−i+1idRn),\nand deﬁne gi ∈N((In)m) using g similarly. If the functional relations\n(1) For all 1 ≤i < m −1: fifi+1fi = fi+1fifi+1\n(2) For all 1 ≤i < m: figi = idIn = gifi.\n(3) For |i −j| > 1: fifj = fjfi.\nare satisﬁed, then the map F : Bm →G((Rn)m) given by σi →fi and σ−1\ni\n→gi, and illustrated in\nFigure 6, deﬁnes a group homomorphism from Bm to G((Rn)m).\nRn\nRn\nRn × Rn\ng\nRn × Rn\nId\nRn × Rn\nf\nRn × Rn\nFigure 6. Converting the braid relations into relations between neural networks.\nNote that in the previous theorem we included the network g in the training process and we train\ntwo networks f and g such that they are inverses of each other instead of training a single invertible\nnetwork f. This is because, as we mentioned earlier for the case of the Yang-Baxter equation, the\nfunction f needs to be invertible. We found that AIDN performs better if we train two networks\nthat are inverses of each other.\nObserve that the map F not only preserves the group structure but also preserves the product\noperation on braids. Speciﬁcally, F(β1 × β2) = F(β1) × F(β2) as illustrated in Figure 7.\n(b)\n(a)\nβ1\nβ2\nfβ1\nfβ2\nβ2\nβ1\nfβ2\nfβ1\nFigure 7. (a) Mapping a product of braids to a composition of neural networks.\n(b) Mapping a product of braids to a product of neural networks.\nLemma 4.1 implies that we only need to train two functions f, g that satisfy the relations (a) and\n(b) given in Figure 8 in order to represent any braid group Bm. These relations (Figure 8(a–b))\ncorrespond to the braid relations given in Figure 5(a–b). Observe that the relation in Figure 8(c) is\nautomatically satisﬁed by our deﬁnition of the mapping F.\n4.1.2. Performance of AIDN on Braid Group Representations. Based on Lemma 4.1, we only need\nto train two neural networks f, g ∈N(In × In) that satisfy the braid relations described in Figure 8\nto obtain a representation of the braid group. In our experiments, we choose the same architectures\nfor f and g. Table 1 reports the results of using the following architecture for networks f and g:\n(4.3)\nRn →R2n+2 →R2n+2 →R100 →R50 →Rn.\nBecause the choice of the activation function determines the type of representation (linear, aﬃne,\nor non-linear), we ran three diﬀerent experiments with three types of networks:\n• Linear: In this case, the activation is chosen to be the identity, and we set the bias to be\nzero for all layers.\n• Aﬃne: In this case, the activation is set to the identity with non-zero bias.\n7\n(a)\n(b)\n(c)\nFigure 8. The orange box corresponds to the function f and the red box represents\nthe function g. In order for the functions f and g to give us a representation of the\nbraid group, they must satisfy the braid relations given in Figure 5. (a) This is the\nfunctional relation that f must satisfy and corresponds to (a) in Figure 5. (b) This\nis the functional relation that f and g must by inverses and corresponds to (b) in\nFigure 5. (c) The functional relation that f must satisfy corresponding to (c) in\nFigure 5\nn = 2\nn = 4\nn = 6\nbraid group relation\nLinear\nAﬃne\nNon-Linear\nLinear\nAﬃne\nNon-Linear\nLinear\nAﬃne\nNon-Linear\nf ◦g = idIn\n15 × 10−6\n12 × 10−6\n0.04\n30 × 10−6\n24 × 10−6\n0.02\n30 × 10−6\n31 × 10−6\n0.04\ng ◦f = idIn\n10 × 10−6\n11 × 10−6\n0.02\n25 × 10−6\n18 × 10−6\n0.02\n32 × 10−6\n30 × 10−6\n0.04\nset-theoretic Yang Baxter\n75 × 10−7\n70 × 10−7\n0.007\n32 × 10−6\n29 × 10−6\n0.01\n29 × 10−6\n27 × 10−6\n0.01\nTable 1. The table describes L2 error of the braid group relations reported after\ntraining the networks f and g.\n• Non-linear: In this case, we choose a non-linear activation. Since our choice must guarantee\nthe invariability of the neural network, we performed a hyperparameter search over the\npossible activation functions and found that the hyperbolic tangent tanh to give the best\nresults. We also used zero bias for all layers as we found it yields better results. Finally, we\nused the identity activation for the last layer.\nThe results of all three cases are reported in Table 1. The results (L2 error) reported in Table 1\nare obtained after training the networks for 2 epochs in case of linear and aﬃne representations.\nIn case of non-linear representation, the results (L2 error) are reported after 600 epochs. This,\nunsurprisingly, indicates the diﬃculty of training non-linear representations as compared to linear\nand aﬃne representations.\nSome solutions obtained on linear braid group representations are given in Table 2. We only show\nthe ﬁnal matrix that corresponds to the function f. We note that the solutions given in Table 2 are\nsolutions for the Yang-Baxter equation. These results are promising and prove the feasibility of\nusing the proposed AIDN for braid group representations. Figure 9 shows a visualisation of the\nnon-linear solution when n = 2 by projecting its components to the plane and visualize them as\nscalar functions.\nn = 2\nn = 4\n\u0014\n−0.7115346\n0.54249334\n−0.02051556\n−0.54249316\n\u0015\n\n\n0.27163547\n−0.08972287\n0.30026573\n−0.1708404\n0.2937515\n−0.2939771\n−0.57252926\n−0.4431777\n0.36519125\n0.82168174\n−0.30026567\n0.17084022\n0.23628233\n−0.10712388\n0.572529\n0.44317824]\n\n\nTable 2. Some of the solutions obtained by AIDN for linear representations of the\nbraid group. The displayed matrices are obtained from the trained function f after\nmultiplying its weight matrices.\n8\n(a)\n(b)\n(c)\n(d)\nFigure 9. (a) and (b) represent the projections of the trained function f ∈N(R2)\nto the plane. (b) and (c) represent the same function but the region of plotting is\nconstrained to the training region which is [−1, 1]2 in this case. Observe that the\nfunction is almost linear when constrained on this region.\n4.2. Temperley-Lieb algebra. In this second example, we will explore the performance of using\nAIDN for representing the Temperley-Lieb algebra. Let R be a ring with a unit, and let m ≥1 be\nan integer.\nSimilar to the braid group, the Temperley-Lieb algebra can be deﬁned via graphical diagrams as\nfollows. Let D2 be the rectangular disk [0, 1]×[0, 1]. Fix m designated points {xi}m\ni=1 on the top edge\nof D2, where xi = (1,\ni\nm+1) for 1 ≤i ≤m, and m designated points on the bottom edge {xi}2m\ni=m+1\nof D2, where xi = (0, i−m\nm+1) for m + 1 ≤i ≤2m. An n-diagram in D2 is a collection of noncrossing\nembedded curves in D2 drawn on 2m designated points such that every designated point in D2 is\nconnected to exactly one other designated point by a single curve. The mth Temperley-Lieb algebra\nTLm is the free R-module generated by all m-diagrams. This algebra admits a multiplication given\nby juxtaposition of two diagrams in [0, 1] × [0, 1]. More precisely, let D1 and D2 be two diagrams in\n[0, 1] × [0, 1] such that ∂Dj, where j = 1, 2, consists of the points {xi}2m\ni=1 speciﬁed above. Deﬁne\nD1.D2 to be the diagram in [0, 1] × [0, 1] obtained by attaching D1 on the top of D2 and then\ncompress the result to [0, 1] × [0, 1]. This extends by linearity to a multiplication on TLm. With\nthis multiplication, TLm is an associative algebra over R.\nThe mth Temperley-Lieb algebra admits an intuitive presentation in terms of generators and\nrelations as follows. In this presentation, the mth Temperley-Lieb is generated by m −1 generators:\nU1, · · · , Um−1,\nwhere generator Ui is the diagram given in Figure 10.\n1\ni\ni + 1\nm\nFigure 10. The Temperley-Lieb algebra generator Ui.\nThe generators Ui satisfy the following relations:\n(1) For all 1 ≤i ≤m −2: UiUi+1Ui = Ui.\n(2) For all 2 ≤i ≤m −1: UiUi−1Ui = Ui.\n(3) For all 1 ≤i < m: U2\ni = δUi for all 1 ≤i ≤m −1\n(4) For |i −j| > 1: UiUj = UjUi.\nOne may observe that with the diagrammatic deﬁnition of Ui, the relations above have a natural\ntopological interpretation as shown in Figure 11.\n4.2.1. Representing the Temperley-Lieb Algebra Via Neural Networks. AIDN can be utilized to\nﬁnd a representation of TLm by deﬁning m −1 neural networks that satisfy the TLm relations.\n9\n(a)\n(b)\n(c)\n(d)\nFigure 11. Temperley-Lieb algebra relations.\nSimilar to the braid group, we can reduce the number of neural networks that we need to train to\none as discussed below.\nObserve that the only diﬀerence between the generators Ui is the position of the hook diagram.\nSpeciﬁcally, the generator Ui can be built from simple blocks corresponding to the hook and the\nidentity strand. More precisely, deﬁne the product of m and n diagrams D1 and D1, denoted by\nD1 × D2, to be the n + m diagram given by horizontal concatenation of D1 to the left of D2. Just as\nwith the braid group, we use id to denote the lone strand and U to denote the hook diagram in the\ni and i + 1 position in Figure 10 and write Ui = (×i−1id) × U × (×m−i+1id). As before, ×kid means\ntaking the product of the identity strand k times. With the presentation of the Temperley-Lieb\nalgebra, we immediately arrive at the following lemma:\nLemma 4.2. Let m, n ≥1 and f ∈N(Rn × Rn). Deﬁne fi ∈N((Rn)m), for 1 ≤i ≤m −1, by\n(4.4)\nfi := (×i−1idRn) × f × (×m−i+1idRn).\nIf the functional relations\n(1) For all 1 ≤i ≤m −2: fifi+1fi = fi.\n(2) For all 2 ≤i ≤m −1: fifi−1fi = fi.\n(3) For all 1 ≤i < m: f2\ni = δfi.\n(4) For |i −j| > 1: fifj = fjfi.\nare satisﬁed, then the map H : TLm →N((Rn)m), given by Ui →fi, and illustrated in Figure 12,\ndeﬁnes an algebra homomorphism from TLm to N((Rn)m).\nRn\nRn\nRn × Rn\nfU\nRn × Rn\nid\nFigure 12. We associate the element U in TL2 with a trainable neural network\nfU : Rn × Rn →Rn × Rn and we associate to the single strand in TL1 the identity\nnet idRn.\nLemma 4.2 implies that in order to deﬁne the mapping H, we only need to train a function fU that\nsatisﬁes the relations (a), (b) and (c) given in Figure 13. These correspond to the Temperley-Lieb\nalgebra (a), (b), and (c) given in Figure 11. Note that relation 13 (d) is automatically satisﬁed by\nour deﬁnition of the mapping H.\n4.2.2. Performance of AIDN on Temperley-Lieb algebra Representations. Using Lemma 4.2, we\nare only required to train a single network denoted by f. Similar to the experiments of the braid\ngroup, we test the AIDN algorithm on the Temperley-Lieb algebra for linear, aﬃne, and non-linear\nrepresentations. The architecture for the function U is chosen as we did for the braid group\ngenerator network in equation 4.3. We also used the same cases deﬁned in Section 4.1.2. The\n10\n(a)\n(b)\n(c)\n(d)\nFigure 13. Converting the Temperley-Lieb algebra relations to relations that the\nneural network fU must satisfy. The purple box corresponds to the generator U.\nThe functional equations in the ﬁgure correspond to the relations given in Figure 11.\nerrors (L2) of all the three cases are reported in Table 3. The linear and aﬃne results reported\nin Table 1 were obtained by training the network f with only 6 epochs. Training the non-linear\nrepresentation is harder and required signiﬁcantly more epochs (600) as compared to the linear and\naﬃne representation.\nn = 2\nn = 4\nn = 6\nTL relation\nLinear\nAﬃne\nNon-linear\nLinear\nAﬃne\nNon-linear\nLinear\nAﬃne\nNon-linear\nU1U2U1 = U1\n81 × 10−7\n64 × 10−7\n0.001\n89 × 10−7\n16 × 10−6\n0.009\n15 × 10−6\n24 × 10−6\n0.005\nU2U1U2 = U2\n66 × 10−7\n36 × 10−7\n0.001\n10 × 10−6\n15 × 10−6\n0.009\n14 × 10−6\n23 × 10−6\n0.005\nU 2 = δU\n73 × 10−7\n85 × 10−7\n0.005\n12 × 10−6\n20 × 10−6\n0.01\n20 × 10−6\n30 × 10−6\n0.01\nTable 3. This table describes L2 error of the Temperley-Lieb algebra relations\nreported after training the network f.\nWe provide in Table 4 concrete examples of the linear representations of the Temperley-Lieb\nalgebra (δ = 1) obtained using AIDN. We also used a plot similar to Figure 9 and observed that the\nneural network f with non-linear activations converge to a linear function on the region of training.\nn = 2\nn = 4\n\u0014\n0.1648174\n0.16481737\n0.83518277\n0.83518262\n\u0015\n\n\n0.11085764\n0.1405094\n0.08143319\n0.1615925\n0.15471724\n0.19610043\n0.11365128\n0.22552472\n0.61665326\n0.05473394\n0.91856676\n−0.16159254\n0.16469169\n0.57503754\n−0.1136513\n0.77447534\n\n\nTable 4. Some solutions obtained by AIDN for linear representations of the\nTemperley-Lieb algebra. The displayed matrices are obtained from the trained\nfunction f after multiplying its weight matrices. Note that in this case the matrices\nare their own inverses.\n5. Knot Invariants\nThe study of braid groups is closely related to the study of knot invariants. A knot in the 3-sphere\nS3 is a smooth one-to-one mapping f : S1 →S3. A link in S3 is a ﬁnite collection of knots, called\nthe components of the link, that do not intersect with each other. Two links are considered to be\nequivalent if one can be deformed into the other without any of the knots intersecting itself or any\nother knots4. In practice, we usually work with a link diagram of a link L. A link diagram is a\nprojection of L onto R2 such that this projection has a ﬁnite number of non-tangentional intersection\npoints, called crossings. A link invariant is a quantity, deﬁned for each link in S3, that takes\n4This is called ambient isotopy.\n11\nthe same value for equivalent links5. Link invariants play a fundamental role in low-dimensional\ntopology.\nIn Section 4.1, we showed that braid groups can be deﬁned via neural networks by representing\nthe main building blocks of braids as neural networks and then realizing the braid relations as an\noptimization problem. Given the close relationship between knots and braids6, one may wonder if\nknot invariants can be deﬁned by associating a neural network to each primitive building block of\nknots and then force the “knot relations” on these networks. In this section, we provide an answer\nto this question.\nIn low-dimensional topology, the relation between equivalent knots are called Reidemeister\nmoves [29]. Reidemeister moves are similar to braid relations; namely, two link diagrams are\nequivalent if one diagram can be obtained from the other by a ﬁnite sequence of moves of type\nΩ1, Ω2 or Ω3. These moves are given in Figure 14 (a), (b) and (c), respectively. Note that braid\nrelations (a) and (b) are precisely the moves Ω2 and Ω3.\n(a)\n(b)\n \n(c)\n(d)\nFigure 14. Turaev moves.\nBuilding knots and links from simple objects requires more primitives than braids. Namely,\nTuraev [39] proved that any knot or link can be built with the following primitives: simple crossings,\nthe cup, cap curves, and the identity strand (See Figure 15). The operations that are needed to\nbuild arbitrary knots are similar to those deﬁned on braids in Figure 7. Moreover, we require four\nmoves between these building blocks. These moves, which we call the Turaev moves, are presented\nin Figure 14. We now state the following two lemmas which are due to Reshetikhin and Turaev [39].\nLemma 5.1. Every link can be realized as a composition of product of the basic building blocks\ngiven in Figure 15.\nThe previous Lemma asserts that we can build any link with the basic building blocks. A link\ndiagram built in this way will be called a sliced link diagram. The following lemma can be used to\nbuild link invariants from these blocks:\nLemma 5.2.\n[35, 39] Let V be a vector space over a ﬁeld F. Let L be a link and D a sliced\ndiagram of L. Let R be an invertible endomorphism on V ⊗V and n : V ⊗V →F a homomorphism\nthat satisfy (1) (idV ⊗n)(R ⊗idV ) = (n ⊗idV )(idV ⊗R), (2) n.R = n, and (3) the Yang-Baxter\nequation. Then the bracket function [D] deﬁned by maps R and n is an isotopy invariant.\nNote that the conditions that R and n must satisfy correspond precisely to the moves given in\nFigure 14. Using Lemma 5.2, the setup to build a knot invariant using AIDN should now be clear.\nSpeciﬁcally, we only need to build four neural networks that correspond to the building blocks given\nin Figure 15 and then force the relations speciﬁed in Figure 14 on these networks. In the quantum\ninvariant literature, the knot invariants obtained using Lemma 5.2 are called quantum invariants\nor Reshetikhin-Turaev invariants [35]. Hence, AIDN, which can obtain knot invariants, can be\nconsidered as a deep learning method to obtain quantum invariants.\n5The equivalence relation here is ambient isotopy.\n6Building braid representations is closely related to building link invariants.\n12\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 15. The building blocks of a link are (a) the cup u : F →V ⊗V , (b) the\ncap n : V × V →F, (c) the simple crossing R : V ⊗V →V ⊗V , (d) the inverse\nR−1 : V ⊗V →V ⊗V of the simple crossing, and (e) identity strand. To build a\nknot invariant using AIDN we build a neural network for each building block and\nthen we train them to satisfy the relations in Lemma 5.2.\nIn our setting, we must discuss multiple remarks about Lemma 5.2. First, note that unlike the\ncase for the general AIDN algorithm, by Lemma 5.2 we must choose the functions R, u, n to be\nlinear. Second, note that the function u is not strictly required to obtain an invariant using Lemma\n5.2. In that context, u can be easily computed using n via the equations (n ⊗idV )(idV ⊗u) =\nidV = (idV ⊗n)(u ⊗idV ) [35]. However, we found that adding u explicitly to the optimization\nobjective yields better results, so we include these two equations in the optimization process.\n5.1. Performance of AIDN applied to quantum invariants. Following Lemma 5.2, we use R,\nR−1 u and n to denote neural network operators that need to be trained to obtain a RT invariant\nusing AIDN. The equations given in Table 5 make the ﬁnal objective function we used to obtain a\nRT invariant using AIDN.\nTuraev Move\ndim(V ) = 2\ndim(V ) = 4\nn.R = n\n0.015\n0.044\nR ⊗R−1 = idV ⊗V\n0.002\n0.14\nR−1 ⊗R = idV ⊗V\n0.003\n0.15\nYang Baxter\n0.17\n0.68\n(idV ⊗n)(R ⊗idV ) = (n ⊗idV )(idV ⊗R)\n0.07\n0.12\n(idV ⊗n)(u ⊗idV ) = idV\n1.5 × 10−7\n0.044\n(n ⊗idV )(idV ⊗u) = idV\n1.5 × 10−7\n0.044\nTable 5. The table describes L2 error of the RT relations reported after training\nthe networks R, R−1, u, and n.\nWe make a few comments on Table 5 in Section 6.\n6. Conclusion and Future Works\nThis work presents AIDN, a novel method to compute representations of algebraic objects using\ndeep learning. We show how the proposed AIDN algorithm can be used to obtain representations\nof algebraic objects and report the performance of AIDN in ﬁnding the solutions for diﬀerent\nalgebraic structures including groups, associative algebras, and Lie algebras. We also show how to\nutilize AIDN along with the RT construction to obtain quantum knot invariants. Our experimental\nresults are promising, and open a new paradigm of research where deep learning is utilized to\nget insights about mathematical problems. We believe this work merely scratch the surface of\npossibilities of interaction between deep learning and mathematical sciences, and we hope it inspires\nfurther research in this direction.\nAn important remark about the performance of AIDN is the diﬃculty of training while having\nmany generators and relations. While modern optimization paradigms such as SGD allows one to\n13\ntrain a model for high-dimensional data, we found that training multiple networks associated with\nan algebraic structure with many generators and relations and high-dimensional data to be diﬃcult.\nThis is evident in Table 5 where the L2 error is relatively much higher than the errors obtained\nwhile training the braid group and the Temperley-Lieb Algebra networks. This can be potentially\naddressed using better hyperparameter search and a more suitable optimization scheme.\nIn the future, we plan to address the following limitations. First, it is not clear whether the neural\nnetworks found by AIDN satisfy additional relations that are not explicitly given in the presentation.\nIntuitively, we would like our searching strategy to learn the exact relations that we provide and be\nas far as possible from all other possible relations. In the terminology of representation theory, it\nis not clear how to guarantee that the algorithm converges to a faithful representation of a given\ndimension, if one does exist. In our experimentation, we empirically tested the trained neural\nnetworks for additional relations that they could potentially satisfy, and found that these networks\ndo not satisfy these potential relations. We did not include these results because this testing\napproach is not systematic. However, we plan to investigate a systematic test for such a limitation\nin the future.\nThe theory of ﬁnite-dimensional (linear) representations of semisimple Lie groups and Lie algebras\nis very mature, and there are many well-known combinatorial results in this area, including an\nexplicit classiﬁcation through the famous Highest Weight Theorem [10]. It would be interesting to\nsee whether it could be proven mathematically in this context (or a similar one) that AIDN can be\nmade to converge to a given irreducible representation with a given highest weight. More generally,\nthe theory of (linear) representations of associative algebras, Lie algebras, Jordan algebras, ﬁnite\ngroups, Lie groups, etc. has been thoroughly investigated over the last century, and many powerful\napplications have been found to physics, PDEs, harmonic analysis, and several other ﬁelds. Hence,\nit would be useful and interesting to see whether it is possible to guarantee that AIDN converges\nto a given irreducible representation under certain conditions.\nFinally, since AIDN utilizes SGD to ﬁnd a representation of a given dimension of some algebraic\nstructure, the obtained solution is not unique. This raises the following question: can we understand\nthe distribution of the local minima solutions obtained by AIDN when applied to a particular\nalgebraic structure? From this perspective, we can consider AIDN as our tool to sample from\nthis unknown distribution. Consequently, AIDN can be potentially utilized in studying general\nproperties of the solution distribution space using other tools available in deep learning such as\ngenerative adversarial networks [12].\n7. Acknowledgment\nThe authors would like to thank Masahico Saito and Mohamed Elhamdadi for their valuable\ncomments.\nReferences\n[1] Jeﬀrey Adams and Fokko du Cloux. Algorithms for representation theory of real reductive groups. arXiv preprint\narXiv:0807.3093, 2008.\n[2] Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and J¨orn-Henrik Jacobsen. Invertible residual\nnetworks. In International Conference on Machine Learning, pages 573–582, 2019.\n[3] L´eon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pages 421–436. Springer,\n2012.\n[4] Steven L Brunton, Bernd R Noack, and Petros Koumoutsakos. Machine learning for ﬂuid mechanics. Annual\nReview of Fluid Mechanics, 52:477–508, 2020.\n[5] Anna Choromanska, Mikael Henaﬀ, Michael Mathieu, G´erard Ben Arous, and Yann LeCun. The loss surfaces of\nmultilayer networks. In Artiﬁcial intelligence and statistics, pages 192–204, 2015.\n[6] George Cybenko. Approximations by superpositions of a sigmoidal function. Mathematics of Control, Signals and\nSystems, 2:183–192, 1989.\n[7] Vahid Dabbaghian-Abdoly. An algorithm for constructing representations of ﬁnite groups. Journal of Symbolic\nComputation, 39(6):671–688, 2005.\n14\n[8] Pavel Etingof, Travis Schedler, and Alexandre Soloviev. Set-theoretical solutions to the quantum yang-baxter\nequation. arXiv preprint math/9801047, 1998.\n[9] U Fischbacher and JA de la Pe˜na. Algorithms in representation theory of algebras. In Representation Theory I\nFinite Dimensional Algebras, pages 115–134. Springer, 1986.\n[10] William Fulton and Joe Harris. Representation theory: a ﬁrst course, volume 129. Springer Science & Business\nMedia, 2013.\n[11] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press\nCambridge, 2016.\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages\n2672–2680, 2014.\n[13] GAP Group et al. Gap system for computational discrete algebra, 2007.\n[14] Boris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width. arXiv preprint\narXiv:1710.11278, 2017.\n[15] Akira Hirose. Complex-valued neural networks: theories and applications, volume 5. World Scientiﬁc, 2003.\n[16] Derek F Holt, Bettina Eick, and Eamonn A O’Brien. Handbook of computational group theory. CRC Press, 2005.\n[17] De-Shuang Huang, Horace HS Ip, and Zheru Chi. A neural root ﬁnder of polynomials based on root moments.\nNeural Computation, 16(8):1721–1762, 2004.\n[18] J¨orn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks. arXiv preprint\narXiv:1802.07088, 2018.\n[19] SK Jeswal and Snehashish Chakraverty. Solving transcendental equation using artiﬁcial neural network. Applied\nSoft Computing, 73:562–571, 2018.\n[20] Michio Jimbo. Introduction to the yang-baxter equation. International Journal of Modern Physics A, 4(15):3759–\n3777, 1989.\n[21] Christian Kassel and Vladimir Turaev. Braid groups, volume 247. Springer Science & Business Media, 2008.\n[22] Louis H Kauﬀman and Samuel J Lomonaco Jr. Braiding operators are universal quantum gates. New Journal of\nPhysics, 6(1):134, 2004.\n[23] Taehwan Kim and T¨ulay Adali. Universal approximation of fully complex feed-forward neural networks. In 2002\nIEEE International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages I–973. IEEE, 2002.\n[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n[25] Igor Moiseevich Krichever. Baxter’s equations and algebraic geometry. Functional Analysis and Its Applications,\n15(2):92–103, 1981.\n[26] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artiﬁcial neural networks for solving ordinary and\npartial diﬀerential equations. IEEE transactions on neural networks, 9(5):987–1000, 1998.\n[27] Isaac E Lagaris, Aristidis C Likas, and Dimitris G Papageorgiou. Neural-network methods for boundary value\nproblems with irregular boundaries. IEEE Transactions on Neural Networks, 11(5):1041–1049, 2000.\n[28] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,\nand Anima Anandkumar. Fourier neural operator for parametric partial diﬀerential equations. arXiv preprint\narXiv:2010.08895, 2020.\n[29] WB Raymond Lickorish. An introduction to knot theory, volume 175. Springer Science & Business Media, 2012.\n[30] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks:\nA view from the width. In Advances in neural information processing systems, pages 6231–6239, 2017.\n[31] Klaus Lux and Herbert Pahlings. Representations of groups: a computational approach, volume 124. Cambridge\nUniversity Press, 2010.\n[32] R.C. Lyndon and P.E. Schupp. Combinatorial Group Theory. Number v. 89 in Classics in mathematics. Springer-\nVerlag, 1977. URL: https://books.google.com.mx/books?id=2WUPAQAAMAAJ.\n[33] Karl Mathia and Richard Saeks. Solving nonlinear equations using recurrent neural networks. In World congress\non neural networks, July, pages 17–21, 1995.\n[34] Kyle Mills, Michael Spanner, and Isaac Tamblyn. Deep learning and the schr¨odinger equation. Physical Review A,\n96(4):042113, 2017.\n[35] Tomotada Ohtsuki. Quantum invariants: A study of knots, 3-manifolds, and their sets, volume 29. World Scientiﬁc,\n2002.\n[36] Michael O. Rabin. Recursive unsolvability of group theoretic problems. Annals of Mathematics, 67(1):172–194,\n1958.\n[37] Maziar Raissi. Deep hidden physics models: Deep learning of nonlinear partial diﬀerential equations. The Journal\nof Machine Learning Research, 19(1):932–955, 2018.\n[38] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part i): Data-driven\nsolutions of nonlinear partial diﬀerential equations. arXiv preprint arXiv:1711.10561, 2017.\n15\n[39] Nicolai Reshetikhin and Vladimir G Turaev. Invariants of 3-manifolds via link polynomials and quantum groups.\nInventiones mathematicae, 103(1):547–597, 1991.\n[40] Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery of partial\ndiﬀerential equations. Science Advances, 3(4):e1602614, 2017.\n[41] Levent Sagun, V Ugur Guney, Gerard Ben Arous, and Yann LeCun. Explorations on high dimensional landscapes.\narXiv preprint arXiv:1412.6615, 2014.\n[42] Akos Seress. An introduction to computational group theory. Notices of the AMS, 44(6):671–679, 1997.\n[43] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv\npreprint arXiv:1703.00810, 2017.\n[44] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial diﬀerential\nequations. Journal of computational physics, 375:1339–1364, 2018.\n[45] Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Nonlinear equation solving: A faster alternative to\nfeedforward computation. arXiv preprint arXiv:2002.03629, 2020.\n[46] Allan Kenneth Steel. Construction of ordinary irreducible representations of ﬁnite groups. 2012.\n[47] Vladimir G Turaev. The yang-baxter equation and invariants of links. Inventiones mathematicae, 92(3):527–553,\n1988.\n[48] Vladimir G Turaev. Quantum invariants of knots and 3-manifolds, volume 18. Walter de Gruyter GmbH & Co\nKG, 2020.\n[49] RS Vieira. Solving and classifying the solutions of the yang-baxter equation through a diﬀerential approach.\ntwo-state systems. Journal of High Energy Physics, 2018(10):110, 2018.\n[50] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-informed deep\nlearning for turbulent ﬂow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 1457–1466, 2020.\nAppendix A. Appendix\nA.1. Note on Implementation. To highlight the simplicity of the implementation of the AIDN\nalgorithm, we brieﬂy discuss pieces of pseudocode for the study case of the braid group that we\ndiscussed in Sections 2.\nTo train a braid group representation using AIDN, we start by creating the generator neural\nnetworks f, g ∈N(Rn × Rn), where n > 1, as explained in Section 4.1.2. To train f, g we create an\nauxiliary neural network for the relations of the braid group. Speciﬁcally, the auxiliary network is\ntrained with the loss function:\n(A.1)\nMSE = MSER2 + MSER3,\nwhere\n(A.2)\nMSER2 =\nn\nX\ni=1,j=1\n||(f ◦g)(xi, yj) −(xi, yj)||2\n2 + ||(g ◦f)(xi, yj) −(xi, yj)||2\n2,\nand\n(A.3)\nMSER3 =\nn\nX\ni=1,j=1,k=1\n||(f ×id)◦(id×f)◦(f ×id)(xi, yj, zk)−(id×f)◦(f ×id)◦(id×f)(xi, yj, zk)||2\n2.\n16\nHere, {xi, yi, zi}n\ni=1 denote points that are sampled uniformly from Ωn × Ωn × Ωn where Ω⊂R,\ntypically the unit interval [0, 1]. When n is large, a mini-batch setting for stochastic gradient descent\nis employed for eﬃciency [11,24].\nDepartment of Mathematics and Computer Science, Santa Clara University, Santa Clara, CA USA\nEmail address: mhajij@scu.edu\nUniversity of South Florida\nEmail address: ghadh@mail.usf.edu\nCONACYT Research Fellow, Centro de Investigaci´on en Matem´aticas, M´erida Campus, M´erida,\nYucat´an, M´exico\nEmail address: matthew.dawson@cimat.mx\nDepartment of Mathematics, University of Oklahoma, Norman, OK USA\nEmail address: gmuller@ou.edu\n17\n",
  "categories": [
    "cs.LG",
    "math.AT",
    "math.GR",
    "math.GT",
    "math.RT"
  ],
  "published": "2020-12-02",
  "updated": "2021-02-12"
}