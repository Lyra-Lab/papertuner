{
  "id": "http://arxiv.org/abs/2012.01281v1",
  "title": "Are Gradient-based Saliency Maps Useful in Deep Reinforcement Learning?",
  "authors": [
    "Matthias Rosynski",
    "Frank Kirchner",
    "Matias Valdenegro-Toro"
  ],
  "abstract": "Deep Reinforcement Learning (DRL) connects the classic Reinforcement Learning\nalgorithms with Deep Neural Networks. A problem in DRL is that CNNs are\nblack-boxes and it is hard to understand the decision-making process of agents.\nIn order to be able to use RL agents in highly dangerous environments for\nhumans and machines, the developer needs a debugging tool to assure that the\nagent does what is expected. Currently, rewards are primarily used to interpret\nhow well an agent is learning. However, this can lead to deceptive conclusions\nif the agent receives more rewards by memorizing a policy and not learning to\nrespond to the environment. In this work, it is shown that this problem can be\nrecognized with the help of gradient visualization techniques. This work brings\nsome of the best-known visualization methods from the field of image\nclassification to the area of Deep Reinforcement Learning. Furthermore, two new\nvisualization techniques have been developed, one of which provides\nparticularly good results. It is being proven to what extent the algorithms can\nbe used in the area of Reinforcement learning. Also, the question arises on how\nwell the DRL algorithms can be visualized across different environments with\nvarying visualization techniques.",
  "text": "Are Gradient-based Saliency Maps Useful in Deep\nReinforcement Learning?\nMatthias Rosynski\nDepartment of Production Engineering\nUniversity of Bremen, Bremen, Germany\nm_rosynski@gmx.net\nFrank Kirchner\nDepartment of Computer Science\nUniversity of Bremen, Bremen, Germany\nfrank.kirchner@dfki.de\nMatias Valdenegro-Toro\nGerman Research Center for Artiﬁcial Intelligence\nBremen, Germany\nmatias.valdenegro@dfki.de\nAbstract\nDeep Reinforcement Learning (DRL) connects the classic Reinforcement Learning\nalgorithms with Deep Neural Networks. A problem in DRL is that CNNs are\nblack-boxes and it is hard to understand the decision-making process of agents. In\norder to be able to use RL agents in highly dangerous environments for humans and\nmachines, the developer needs a debugging tool to assure that the agent does what\nis expected. Currently, rewards are primarily used to interpret how well an agent is\nlearning. However, this can lead to deceptive conclusions if the agent receives more\nrewards by memorizing a policy and not learning to respond to the environment. In\nthis work, it is shown that this problem can be recognized with the help of gradient\nvisualization techniques. This work brings some of the best-known visualization\nmethods from the ﬁeld of image classiﬁcation to the area of Deep Reinforcement\nLearning. Furthermore, two new visualization techniques have been developed,\none of which provides particularly good results.\nIt is being proven to what extent the algorithms can be used in the area of Rein-\nforcement learning. Also, the question arises on how well the DRL algorithms can\nbe visualized across different environments with varying visualization techniques.\n1\nIntroduction\nDue to the success achieved in recent years by Deep Reinforcement Learning [7] [2], Industrial\napplications are becoming increasingly tangible [13]. Research is being conducted on 3D map\nreconstruction for autonomous cars where DRL can be one of the solutions [18] and also in the\nﬁeld of AUVs [3] as well as many other applications that interact with the real world. Once DRL\nalgorithms are implemented on physical systems and interact with the real world, these systems can\nbe dangerous for themselves and humans. For this reason, debugging tools are needed to understand\nwhy the agent behaves that way and whether the agent is making the right decision for the correct\nreason and not making a right decision for the wrong reason [11].\nDeep reinforcement learning algorithms are nowadays interpreted and measured by the rewards the\nagents can get. For this reason these agents are called black box algorithms and are criticized. This\nmakes them difﬁcult to use in critical real-world applications.\nThis paper reveals that visualization techniques are a powerful debugging tool, that provides much\nmore information than interpreting rewards. With the help of Guided Backpropagation it is even\n1st I Can’t Believe It’s Not Better Workshop (ICBINB@NeurIPS 2020), Vancouver, Canada.\narXiv:2012.01281v1  [cs.LG]  2 Dec 2020\npossible to locate the error in a certain layer or stream in a neural network. Moreover it is shown that\nGrad-Cam methods can deliver results very early in training in case of very badly trained networks.\nThis is a big advantage especially for off-policy algorithms that take a long time to explore at the\nbeginning of the learning process. With off-policy algorithms it can take a few days until the rewards\nstart to increase so that the developer can determine if the neural network is learning at all [14].\nContributions. For DRL, the claim made by Adebayo et al. [1], that Guided Backpropagation does\nnot visualize the desired regions, but through partial input recovery it works as a kind of edge detector,\nis shown not to be accurate. Also, the claim that gradient methods can be difﬁcult to interpret, because,\nwhen answering the question \"What perturbation to the input increases a particular output?\", gradient\nmethods can choose perturbations which lack physical meaning [5], is shown not always to be an\nissue.\nFurthermore two new visualization techniques were developed, one of which provides particularly\ngood results. These were compared and analysed with 4 other popular visualization techniques. Their\nadvantages and disadvantages are discussed and different ﬁelds of application for the respective\nvisualizations are suggested.\n2\nRelated Work\nIn this section, previous work is presented and discussed. As already mentioned, there is currently\nnot a lot of work dealing with the topic of visualization in the area of deep reinforcement learning\nalgorithms. First two works from the ﬁeld of deep reinforcement learning are presented and then and\nthen visualization techniques from the ﬁeld of image processing.\nSemi Aggregated Markov Decision Processes. The authors which introduced Semi Aggregated\nMarkov Decision Processes [17] used the Atari 2600 environments as interpretable testbeds, they\ndeveloped a method of approximating the behavior of deep RL policies via Semi Aggregated Markov\nDecision Processes (SAMDPs). They used the more interpretable SAMDPs to gain insights about the\nhigher-level temporal structure of the policy. From a user perspective, an issue with the explanations\nis that they emphasize t-SNE clusters and state-action statistics which are uninformative to those\nwithout a machine learning background.\nPerturbation-based saliency methods. Another recently published work shows Perturbation-based\nsaliency methods for the visualization of learned policies by Greydanus et al. [5]. Their approach\nis to answer the question, \"How much does removing information from the region around location\nchange the policy?\". The authors deﬁned a saliency metric for image location as:\nSπ(t, i, j) = 1\n2 ∥πu (I1:t) −πu (I′\n1:t)∥2\n(1)\nThe difference πu (I1:t) −πu (I′\n1:t) can be interpreted as a ﬁnite differences approximation of the\ndirectional gradient ∇ˆvπu (I1:t) where the directional unit vector ˆv denotes the gradient in the\ndirection of I′\n1:t.\nIn other words, is looking at how important were these pixels for the policy. By checking how\nstrong the policy changes after removing some informations from the image. They use the same\napproach to construct Saliency Maps for the value estimate V π too. Greydanus et al. [5] claims\nthat gradient-based saliency methods do not yield well interpretable results. When answering the\nquestion \"What perturbation to the input increases a particular output?\", gradient methods can choose\nperturbations which lack physical meaning.\nGradients. The basic idea of backpropagation-based visualizations is to highlight relevant pixels by\npropagating the network output back to the input image space. The intensity changes of these pixels\nwhich have the most signiﬁcant impact on network decisions. Despite its simplicity, the results of\nsaliency map are normally very noisy which makes the interpretation difﬁcult. [8].\nGuided Backpropagation. The idea behind guided backpropagation is that neurons act like detectors\nof particular image features. It is interesting in what image features the neuron detects and not in what\nkind of features it doesn’t detect. That means when propagating the gradient, the ReLu function set\nall the negative gradients to zero. [8] With other ways we only backpropagate positive error signals\nand we also restrict to only positive inputs.\n2\nGradient-weighted Class Activation Mapping. One of the problems when it comes to using CAM\nis that a modern neural network not only consists of convolution layers but of different layers such\nas LSTM, MaxPooling, etc. GradCAM is an extension of CAM and it is broadly applicable to any\nCNN-based architectures. In order to obtain the class-discriminative localization map Grad-CAM\nLc\nGrad−CAM ∈Ru×v of width u and height v for any class c. First, the gradients yc of class c are\ncalculated up to the desired convolution feature map activations Ak. An average pooling over the\nwidth and height dimensions is then carried out [9].\nαc\nk =\nglobal average pooling\nz\n}|\n{\n1\nZ\nX\ni\nX\nj\n∂yc\n∂Ak\nij\n| {z }\ngradients via backprop\n(2)\nWeight αc\nk represents a partial linearization of the deep network downstream from A, and captures\nthe \"importance\" of feature map k for a target class c. On the end it performs a weighted combination\nof forward activation maps with a ReLU fuction. [9]\nLc\nGrad−CAM = Re LU\n X\nk\nαc\nkAk\n!\n|\n{z\n}\nlinear combination\n(3)\nThe result is a heatmap of the same size as the convolutional feature map. This feature map must\nthen be enlarged to the size of the original image and place it over the image to get the ﬁnal result.\nIf Grad-Cam is multiplied by Guided Backpropagation with the Hadamard product we get Guided\nGrad-Cam [9].\nGradient Methods. Adebayo et al. [1] deals with the informative value of visualization techniques.\nIn this work, various visualization techniques were compared and checked whether the techniques\nactually depend on the model, the training data or whether it partially reconstructs the image. In\nother words, their work claims that Guided Backpropagation and Guided GradCam act like an edge\ndetector, which means that they show the desired positions without showing the learned model.\nIn their work they randomize the weights of a model starting from the top layer, successively, all\nthe way to the bottom layer. This procedure destroys the learned weights from the top layers to\nthe bottom ones. Their results indicate that Guided backpropagation and Guided GradCam do not\nvisualize the learned model, but instead partially reconstruct the image. They interpret their ﬁndings\nthrough an analogy with edge detection in images, a technique that requires neither training data nor\nmodel.\nLaplacian Operator in Image Processing. To understand one of the results we will derive the\nkernel of the Laplacian ﬁlter which is used for edge detection. One approach for the design of\ndirectionally invariant high-pass ﬁlters for image processing (also called edge detectors) is to use\nsecond-order derivation operators, e.g. the Laplace operator [16]. For continuous functions, the\nLaplace operator is deﬁned by:\nL = ∂2I[x, y]\n∂x2\n+ ∂2I[x, y]\n∂y2\n(4)\nWe approximate the partial derivatives by difference equations and thus obtain :\nd2I[x, y]\ndx2\n= {I[x + ∆x, y] −2I[x, y] + I[x −∆x, y]}/∆x2\n(5)\nd2I[x, y]\ndy2\n≈{I[x, y + ∆y] −2I[x, y] + I[x, y −∆y]}/∆y2\n(6)\nThus with ∆x, ∆y = 1 (except for the sign) the mask for the Laplace operator is :\nL1 =\n\"\n0\n−1\n0\n−1\n4\n−1\n0\n−1\n0\n#\n(7)\n3\nNumerous other approximations of the Lapace operator are possible. Examples are the following\nmasks (their transfer functions have the form known from the Gaussian and binomial distributions)[16]\nL2 =\n\"\n0\n−1\n−1\n−1\n8\n−1\n−1\n−1\n0\n#\nL3 =\n\"\n1\n−2\n1\n−2\n4\n−2\n1\n−2\n1\n#\nL4 =\n\" −1\n−2\n−1\n−2\n12\n−2\n−1\n−2\n−1\n#\n(8)\n3\nExperimental Setup\nExperiments were performed in two different environments a simple (Breakout-v0) and a complex one\n(Seaquest-v0) from OpenAI Gym [4]. Four different agents were implemented. DDDQN (4 frames\nas input) [10], Splitted Attention DDDRQN (with LSTM), and two on Policy gradient algorithms\nA3C (3 frames as input) [6] and an A3C Agent with LSTM.\nThe Splitted Attention DDDRQN was by far the best trained agent and receives most of the points\n(Seaquest-v0 9521 Points) that is why most of the results are referring to this agent [12].\nIn order to better examine the visualization methods and to enable better interpretation, the original\nframes of the games are placed over the gradients with a 50% opacity. In this way it is possible to\nassign the gradients to the features and to better interpret the results.\nWith the off policy algorithms not only the output is visualized but also the Q-value and the Advantage\nstream. To proof the results of Ziyu Wang et al. [15]. In order to maintain comparability between the\nvisualization techniques, the same state was always visualized in this work. This also applies to the\nvisualization of the actor and critic agents, as well as the visualization of the value and advantage\nstreams. Two new visualization techniques have been developed and the following visualization\nmethods have been implemented. Some features of the implementation are discussed below.\nGradient and Guided Backpropagation. Compared to image processing, where the gradients of\nthe guided backpropagation or gradient method are normalized over the image, the gradients in a\nvideo can also be normalized over the entire video. For this reason, both have been implemented and\ntested. First, the gradients output were normalized for each state. Then the gradient and guided back\npropagation method around the whole video was normalized and tested.\nIn the visualizations, the gradients were checked in relation to different output layers. In particular,\nthis work examines the advantage and the value stream in the off-policy algorithms in more detail, as\nwell as the last layer that delivered the best results. In the actor critic methods both outputs of the NN\nwere examined.\nGrad-Cam and Guided Grad-Cam. With the Grad-Cam and Guided Grad-Cam method, the\nvisualization can be applied to different convolutional layers. An example of what the differences\nlook like is also shown. The best results were achieved on the ﬁrst convolutional layer across all\nagents and across all environments. For this reason, the results of the Grad-Cam methods are usually\napplied to the ﬁrst convolutional layer.\nG1Grad-Cam and G2Grad-Cam.\nTwo new visualization techniques were developed and\nwill be presented in this work, which are also compared. Both visualization techniques are a further\ndevelopment of Grad-Cam and Guided Grad-Cam. The idea behind it is the same as for guided back\npropagation, that neurons act like detectors of particular image features. And we are interested in\nwhat image features the neuron detects and not in what kind of features it does not detect. That\nmeans when propagating the gradient, we set all the negative gradients to 0. In the further course,\nGrad-Cam with the Guided Model is called G1Grad-Cam and GradCam with the Guided Model\nmultiplied by Guided backpropagation is called G2Grad-Cam.\n4\nExperimental Results\nSince a state usually contains several frames, the results of the gradient methods (unless other-wise\nstated) are related to the last frame in the sequence. With the Grad-Cam methods the results (unless\notherwise stated) relate to the ﬁrst convolutional layer. Furthermore, the word stable is deﬁned in the\ncontext of visualization techniques in this paper as follows: gradients or visual highlights that can be\n4\nseen on most frames in a video. Gradients or visual highlights that can be seen on every 4th, 5th (or\neven less) frame in a video are called not stable.\nFirst we discuss the visualization techniques, then we look closer at the class discriminative visual-\nization algorithms and ﬁnally we make a hypothesis about the importance of negative gradients in\nbackpropagation algorithms. Detailed results are all provided in the appendix (Figures 2 to 78).\n4.1\nVisualization Algorithms\nGradient. Gradients method delivers a lot of noise which often mixed with the important features. It\nis only very poorly suited for debugging neural networks in the deep reinforcement learning area. We\nget similar results with the agent in all environments. This can be seen in Figures: 2, 13, 14, 21, 29,\n31, and 50.\nGuided Backpropagation. Guided backpropagation delivers the best and most stable results under\nall environments and among all agents. Also, that it shows negative gradients is a very good indication\nof how well an agent is trained. It can be seen that the better the agent has been trained, the stronger\nthe transition between negative and positive gradients.\nAnother interesting observation that guided back propagation (compared to the Grad-Cam methods)\ndid not visualize the agent so well during the breakout game. Even in the DDDQN network, where\nthe agent was well trained, only weak gradients could be recognized on the agent himself on Figures\n19 and 3. In contrast, on Seaquest, which is a much more complex environment and the agent was\nnot well trained, very strong gradients were displayed on the agent itself, as Figures 13 and 14 show.\nAnother comparison that has been made in this work is how the visualization of the guided backprop-\nagation algorithm changes with respect to normalization. In other words, how does the visualization\nchange when the normalization of the gradients is carried out over a frame compared to when the\nnormalization was created over the entire video.\nThe results showed that when the frame was normalized, the gradients in the video ﬂashed more\nstrongly. With normalization across the entire video, the transitions from the frames were smoother.\nIn this way, the less known / visited states can be recognized because the gradients on which are\nweaker to recognize. This method is also the most suitable for visualizing the differences between\nthe advantage and the value stream in the off-policy algorithms. Guided back propagation was able to\nidentify most of the features in any environment and these were also very stable across the video.\nDuring the development of the split attention DDDQN agent, the guided back propagation method for\ndebugging was also used. An interesting ﬁnding that was made during development was that although\nthe neural network had several errors, results were still very good (over 3000-3500 points, similar\nresults were achieved by the normal attention DDDRQN network). Only after the Advantage and\nValue Stream was visualized did it become apparent that the network only learned on the Advantage\nside and that the gradients on the value side were very strong but chaotic. Based on the points, the\nerrors that were made during programming would not have been noticed. Altogether two errors could\nbe discovered and they could even be localized in the value stream. One mistake was incorrectly\nlinking the layer and another when merging the two streams.\nWhen developing the A3C with LSTM network, guided back propagation was also used to debug the\nnetwork. Before a working A3C with LSTM was developed in this work, attempts were ﬁrst made to\ntrain the agent with bidirectional layers. This was unsuccessful. In the visualization, no gradients\nwere displayed when the gradients from the output layer to the input layer were calculated (the output\ngradients during printing were also 0). Then the network with guided backpropagation was examined\nmore precisely with this visualization technique, because in contrast to the Grad-Cam methods, the\ngradient methods calculate the gradients from each individual layer to the input, this means that the\nindividual layers can be examined. It turned out that the agent started to learn in the ﬁrst three layers\nand no longer from the bidirectional layers. When these layers were replaced by LSTMs, the agent\nimmediately began to learn even in the higher layers.\nGrad-Cam. This method also showed stable results (based on the ﬁrst convolutional layer), even if\nthe results were often issued in inventory, especially with less well-trained agents, as seen in Figure\n46.\n5\nLooking at the higher layers, some features could also be visualized, but here the visualization\nwas inverted more often, as shown in Figure 41, and the speciﬁed position of the features in the\nvisualization algorithm was less precise, shown in Figures 33 and 37.\nOne advantage over the guided backpropagation method which is noticeable, is that with less well\ntrained agents, the visualization (even if mostly often inventoried) was able to visualize the features\nfaster and without interference. However the G1Grad-Cam could beat these results.\nGrad-Cam was able to achieve better results in the Breakout environment (see Figure: 4), where the\nagent could be visualized relatively poorly with guided backpropagation. Another advantage of the\ngrad cam method (at least for neural networks without LSTM) compared to guided backpropagation\nis that, as can be seen in Figure 4, the time ﬂow can be visualized. Means how important the previous\nframes were.\nDue to the negative gradients that exist in guided backpropagation, a superimposition or averaging\nof the gradients from all input frames would not provide a similar result, since the gradients would\ncancel each other out with guided backpropagation.\nSince the Grad-Cam has a ReLU function, only positive results are displayed. However, this has\nthe disadvantage that we cannot differentiate as well if an agent is well trained. The visualization\ndifferences are minimal as can be seen in the Actor Critic between Figures 69 and 70 and different\nbetween the Splitted Attention DDDRQN (See Figure 23) and the DDDQN Agent . However, the\nresults with Grad-Cam are better with poorly trained agents and have less disruption than with\nguided backpropagation. One reason for this could be that the grad-cam carried an average pooling\nacross the height and width of the dimension are carried out. With average pooling, the tendencies\ncould be displayed better and there would be fewer disturbances due to averaging. This could be\nthe reason why by Grad-Cam by weakly trained networks achieve better results than with guided\nbackpropagation.\nA disadvantage compared to guided back propagation, which can be seen in all agents is that not all\nfeatures can be visualized. By the breakout environment the area of the image where the agent breaks\nthrough the last line and receives many points could be not visualized (see Figure 3 and Figure 4). In\nSeaquest, Grad-Cam was unable to visualize the oxygen bar, nor the divers collected.\nGuided Grad-Cam and G2Grad-Cam. The combination of guided back propagation and one of the\nGrad-Cam methods has proven to be very unstable. One reason is that not all features are displayed\nin the Grad-Cam method, as has already been mentioned. But the main reason is that the results are\nshown often inverted by the Grad-Cam method. Due to the inversion, the areas with the features have\na value of 0, which means that the multiplication with the guided back propagation also results in 0.\nBasically, with these two methods, no additional knowledge can be obtained that has already been\nobtained with guided back propagation or Grad-cam.\nG1Grad-Cam The G1Grad-cam method gave much better results for the DDDQN agent in the game\nbreakout and also the time ﬂow than with the Grad-Cam method, as shown in Figure 6 (G1Grad-Cam)\nversus Figure 4 (Grad-Cam). Interestingly, the G1Grad-Cam method has problems visualizing the\nfeatures in game Seaquest.\nThe G1Grad-Cam method shows less inverted results than the Grad-Cam method. However, especially\nwith less well-trained agents, the results are not stable or not exactly in position. Basically, the method\nonly convinced the DDDQN agent in the game Breakout. Here G1grad-Cam was able to display the\nball and the agent as well as their past positions very clearly than all other visualization techniques.\nHowever, the upper breakthrough area could not be visualized as the guided back propagation method\ndid.\nAn interesting aspect of the poorly trained splitted attention agent is that in Seaquest the\nG1Grad-Cam method gives the best interpretable results over all visualization methods, as shown\nin Figure 47 (in contrast to the well trained one, where it gives no results). The neural network\nwas saved after 325 episodes (for comparison, the well-trained neural network was saved after\n5300 episodes). After 325 episodes the neural network has done about 250 000 steps and 75% of\nthem have chosen a random action (because of exploration). After this short period of time, the\nG1Grad-Cam method was able to deliver well interpretable results.\n6\nFor comparison: After 1500 episodes it can be determined via the rewards that the neural network\nis learning. This means that with the G1Grad-Cam method it is possible to recognize up to 5 times\nfaster if the neural network is learning at all. This makes it a helpful debugging tool.\n4.2\nAction Discriminative Visualization Algorithms\nThe fact that the agent has learned to recognize features does not mean that the agent has learned how\nto behave. The agent recognizes e.g. the oxygen bar and understand when it ends that the episode will\nend, but it has not learned what to do about it. It is similar with the ﬁshes that the agent recognizes\nthem but does not necessarily mean that the agent will avoid the ﬁsh or knows what to do with it. For\nthis reason, action discriminative algorithms were examined more closely.\nThe investigations did not give any indication that Grad-Cam and G1Grad-Cam, which are action\ndiscriminative algorithms, have a visible visual relationship between the state and the action taken.\nThe expectation that Grad-Cam would emphasize a particular ﬁsh because the agent would swim\naway from a ﬁsh or target a ﬁsh, could not be conﬁrmed.\n4.3\nHypothesis about Negative Gradients in Guided Backpropagation and their Meaning\nApparently negative gradients also play a role. With the DDDQN agent, the negative gradients were\ntime-dependent. The newest frame has the most positive and the oldest most negative gradients and\nthe negative gradients are always shown where the ball was or will be. One reason for this could\nbe the different architecture of the neural network which only calculates the error of the selected\naction but not the error of the non-selected actions compared to the other developed CNN. With the\nother agents, it can be observed that the negative gradients did not form in a time-dependent but\nposition-dependent manner. The strong negative gradients have formed around the features. It can\nalso be seen that the stronger the positive and negative gradients, the better the agent.\nFigure 1: The agent is covered\nwith positive gradients fol-\nlowed by negative and positve\nagain.\nA possible explanation could be that with guided backpropagation\nthe gradients resemble a matrix or the kernel of an edge detector.\nAnd the Laplacian Operator has strong positive and negative values\nin the matrix to detect edges. Backpropagation could create a matrix\nthat has learned to better identify features with strong negative and\npositive values, just like with a Laplace operator. Figure 1 shows that\nstrong positive gradients are followed by strongly negative gradients\nand some small positve again (compare in Equations 7 and 8, Equa-\ntion L1 and L3). The Laplace operator has a similar structure. The\ndifference is that the Laplace operator detects edges and not more\ncomplex features. Guided backpropagation would create a kind of\nLaplace matrix for features based on this hypothesis. This could be\na possible explanation for the role of negative gradients.\n5\nConclusions and Future Work\nVisualisation techniques. First of all, the assumptions made in the paper \"Visualizing and Under-\nstanding Atari Agents\" [5], that guided backpropagation cannot be used for visualization techniques\nand the assertion made in the paper \"Sanity Checks for Saliency Maps\" [1], that guided backpropaga-\ntion and guided Grad-Cam (at least in image processing) do not visualize the learned model but work\nsimilarly to an edge detector do not seem apply to deep reinforcement learning policies.\nVisualization methods are very well suited as an additional, if not main debugging tool. Especially\nGuided Backpropagation was used during the development of Splitted Attention DDDQN with\nbidirectional LSTM agents and the A3C with LSTM agent to detect various errors in the neural\nnetwork. Not only can errors be detected but also an assignment could be made in which stream the\nerror was located and which layer caused problems. This kind of debugging is not possible with the\nusual reward evaluation. Especially since the faulty neural network could achieve better results than\nthe original one despite the errors it contains, these errors would not have been noticed.\nFurthermore, it was also shown with two agents (Figure 68 and Figure 16) that Guided backpropaga-\ntion can be used to detect if the agent has learned to avoid an area. The agent obtained a high reward,\n7\nbut this is not the expected behavior. The agent has learned to stay in the water at a certain height.\nThe agent always shoots to the left or to the right if something comes up there. This is an important\ninsight because it means that if the environment changes the agent will probably not be able to cope\nwith these changes as well as an agent that has a lower score, but swims speciﬁcally towards the\nﬁshes. Here a simple reward function could even lead to misinterpretations. Guided backpropagation\ncould show the different strength of the gradients on the ﬁsh that swam at different heights, which\ncould lead to the conclusion that the agent does not visit certain areas of the environment.\nThe Grad-Cam and especially the G1Grad-Cam method achieves very early results. This is useful\nduring the training process of the neural network. For example, if an agent has to be trained for 14\ndays in a HPC cluster, G1Grad-Cam can quickly provide initial information after a few minutes /\nhours as to whether the agent recognizes any features at all or whether there are indications of errors\nin the neural network. The rewards that are currently used as the main debugging tool are hardly\nusable at the beginning, especially with off-policy algorithms. Because the agent is exploring at the\nbeginning, the rewards are random, but the neural network learns to recognize the features even if it\ndoes not yet know what they mean or how the agent has to behave. This is a big time advantage over\nthe Reward function that took 2 days with the Split Attention Agent to show the ﬁrst signs that the\nRewards are increasing.\nThe error does not necessarily have to be in the neural network, but also a faulty implementations\ncan be detected in this way (during this work wrong stacking of the frames could be visualized on\nthe neural network and the error could be corrected by the DDDQN agent). The G1Grad-Cam and\nGrad-Cam method can save time when designing neural networks during debugging compared to a\npure interpretation by the Reward function.\nHowever, no evidence was found that using G1/Grad-Cam methods which are action discriminative\nmethods, can visualize the difference in importance between the features, that has the biggest impact\non the action (agent’s decision). No feature was highlighted that is more important for the decision of\nan action.\nGuided backpropagation was able to recognize most of the features. All features detected by the\nGrad-Cam methods and more. Even though the gradients are sometimes hardly visible and only after\nlong training the DDDQN agent could visualize itself in the game Breakout.\nThis method is especially suitable to evaluate the agent at the end (Does the agent recognize all\nimportant features? Are there differences in intensity between the same features in different states?\nHow strong are the gradients on the features?) or for debugging purposes. Guided backpropagation\nis better suited for debugging than the Grade-Cam method, because individual streams could be\nexamined and individual layers. In this way the error can be identiﬁed more quickly under certain\ncircumstances (as in the development of the Split Attention Agent).\nNegative gradients. A hypothesis about possible explanation for the importance of negative gradients\nthat forms around features were put forward in this work. The explanation was compared to a\nLaplacian ﬁlter which revealed the edges. However, edges were not revealed here, but entire features\nit was concluded that through backpropagation a kind of Laplacian kernel feature detector was\nvisualized. Where the negative gradients would be produced by a second derivative to better detect\nthe features.\nFuture work. After having shown in this work that visualization techniques are a basic important\ntool for debugging neural networks, there are still some important questions left. First of all the\nfeatures that are important for the agent could be visualized. What could not be shown in this paper\nis that in action discriminative methods, the features that have a greater importance for the selected\naction are more strongly emphasized. A possible reason for this could be that by averaging all\nfeature maps of the Grad-Cam methods the differences between the normal features and the features\nthat directly inﬂuenced the agent’s decision were very small. Here, a special implementation of\nGuided backpropagation might help to better identify the differences, by reprogramming the Guided\nbackpropagation to an action discriminative algorithm. This would give a better understanding of the\nadvantages shown in this work, that guided backpropagation can better evaluate the intensity of how\nwell a neural network recognizes the feature. In combination with a modiﬁed action discriminative\nGuided backpropagation algorithm, it might be possible to get more meaningful results if it is\npossible to visualize the feature that had the greatest inﬂuence on the current action.\n8\nReferences\n[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.\nSanity checks for saliency maps, 2018.\n[2] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep\nreinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26–38, Nov\n2017.\n[3] Shane Barratt. Active robotic mapping through deep reinforcement learning. arXiv preprint\narXiv:1712.10069, 2017.\n[4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[5] Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding\natari agents, 2017.\n[6] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lill-\nicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. CoRR, abs/1602.01783, 2016.\n[7] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,\nabs/1312.5602, 2013.\n[8] Weili Nie, Yang Zhang, and Ankit Patel. A theoretical explanation for perplexing behaviors of\nbackpropagation-based visualizations. In Proceedings of the 35th International Conference on\nMachine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3809–3818.\nPMLR, 10–15 Jul 2018.\n[9] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi\nParikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based\nlocalization. International Journal of Computer Vision, Oct 2019.\n[10] Mohit Sewak. Deep q network (dqn), double dqn, and dueling dqn. In Deep Reinforcement\nLearning, pages 95–108. Springer, 2019.\n[11] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:\nVisualising image classiﬁcation models and saliency maps, 2013.\n[12] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, and Anastasiia Ignateva.\nDeep attention recurrent q-network. CoRR, abs/1512.01693, 2015.\n[13] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT\nPress, second edition, 2018.\n[14] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient\nmethods for reinforcement learning with function approximation. In Proceedings of the 12th\nInternational Conference on Neural Information Processing Systems, NIPS’99, pages 1057–\n1063. MIT Press, 1999.\n[15] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.\nDueling network architectures for deep reinforcement learning, 2015.\n[16] Hanyue Liang Yilun Chen, Rui Zhu. Filter in der bildverarbeitung.\n[17] Tom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding dqns.\nIn International Conference on Machine Learning, pages 1899–1908, 2016.\n[18] Karel Zimmermann, Tomas Petricek, Vojtech Salansky, and Tomas Svoboda. Learning for\nactive 3d mapping, 2017.\n9\nA\nDetailed List of Experiments\nThe agent and the environment are shown on the left and the visualization techniques on the right.\nQ-Values\n1st conv. Layer\nAgent\nEnv.\nGradient\nGuid. back.\nGrad-C.\nGuid. Grad-C.\nG1Grad-C.\nG2Grad-C.\nDDDQN [10]\nBreakout-v0\n\b\n\b\n\b\n\b\n\b\n\b\nDDDQN [10]\nSeaquest-v0\n\b\n\b\n\b\n\b\n\b\n\b\nSplit. At. DDDQN\n(bi. LSTM)\nSeaquest-v0\n\b\n\b\n\b\n\b\n\b\n\b\nA3C [6]\nBreakout-v0\n\b\n\b\n\b\n\b\n\b\n\b\nA3C [6]\nSeaquest-v0\n\b\n\b\n\b\n\b\n\b\n\b\nA3C with LSTM\n[6]\nSeaquest-v0\n\b\n\b\n\b\n\b\n\b\n\b\nTable 1: Combinations of visual explanation experiments\nIn addition, there are the different settings of the visualization techniques, e.g. which layer or stream\nwas visualized with gradient methods or up to which convolutional layers were the Grad-Cam\nmethod applied:\nVal. Stream\nAdv. Stream\n1st conv. L.\n2st conv. L.\n3st conv. L.\nAgent\nEnv.\nGuided backpropagation\nGrad-Cam\nDDDQN [10]\nBreakout-v0\n\b\n\b\n\b\nSplit. At. DDDQN\n(bi. LSTM)\nSeaquest-v0\n\b\n\b\n\b\n\b\n\b\nTable 2: Combinations of different settings for visual explanation experiments\nThe past frames in the input of the neural network for the guided backpropagation algorithm are also\nexamined:\nAgent\nEnv.\nt-1\nt-2\nt-3\nt-9\nDDDQN [10]\nBreakout-v0\n\b\n\b\n\b\nSplit. At. DDDQN (bi. LSTM)\nSeaquest-v0\n\b\nTable 3: Combinations of time-dependent visual explanation experiments\nAn other question also arises: When does the neural network show ﬁrst signs of feature recognition?\nIs it possible to ﬁnd out faster if the neural network or the agent in general has been programmed\ncorrectly? Than about the interpretation of the reward function, which is only possible after a long\ntime because of the exploration of the agent in off-policy algorithms.\nFor this reason, a neural network that was after only 325 episodes / 250 000 steps saved, is also tested\n(for comparison: the well trained agent had trained over 5300 episodes and about 13 000 000 steps):\nAgent\nEnv.\nGradient\nGuid. back.\nGrad-C.\nGuid. Grad-C.\nG1Grad-C.\nG2Grad-C.\nSplit.\nAt.\nDDDQN\n(bi.\nLSTM)\nSeaquest-v0\n\b\n\b\n\b\n\b\n\b\n\b\nTable 4: Combinations of time-dependent visual explanation experiments\n10\nB\nDetailed Experimental Results\nThe visualized results are presented in red or green color. Meanwhile red is used for negative\ngradients, green has been used for positive ones. Since the Grad-Cam methods have a ReLu function,\nthe results are always positive. All Images are taken at the same time for better comparison.\nB.1\nDDDQN: Breakout-v0\nFigure 2: Gradient visualization method. In\nthe red oval you can see that the NN-Agent\nstarted to understand which region gives him\nhigher future rewards and where it has to shot.\nThis is because if the agent breaks through\nthe last line, the ball will bounce off the box\nand the ceiling and get a lot of reward.\nFigure 3: Guided backpropagation visualiza-\ntion method. The ball is with this method well\nheighlighted. In the red oval you can see that\nthe NN-Agent started to understand which re-\ngion gives him higher rewards and where it\nhas to shot\n11\nFigure 4: Grad-Cam. Some visual highlights\non the agent as well as on the ball are clearly\nvisible. Furthermore you can see the direction\nthe ball is coming from as all 4 frames are\nvisualized with the Grad-Cam method.\nFigure 5: Guided Grad-Cam. Since Guided\nGrade-Cam is a multiplication of the Grade-\nCam method and backpropagation, it is also\nobvious that we only visualize sub areas of\nthe original source. The negatives and posi-\ntive gradients of the current frame calculated\nby the guided backpropagation method are\nmultiplied by the degree-cam method, which\ndoes its job on all frames. And only if both\nhave positive high values this area will be\nhighlighted.\nFigure 6: G1Grad-Cam. Comaprad to the\nGrad-Cam method we can see much more sta-\nble visualized features. We can see clearly the\nold positions of the ball and when the agent\nmoves the old position of the Agent because\nwe have four frames as input. This is different\nto the Gradient methods where we cant ovalep\nthe positive and negative gradients, thats why\nwe are using the gradient methods on the last\nframe.\nFigure 7: G2Grad-Cam. Since Guided Grade-\nCam is a multiplication of the Grade-Cam\nmethod and backpropagation, it is also obvi-\nous that we only visualize sub areas of the\noriginal source. The negatives and positive\ngradients of the current frame calculated by\nthe guided backpropagation method are multi-\nplied by the degree-cam method, which does\nits job on all frames. And only if both have\npositive high values this area will be high-\nlighted.\n12\nFigure 8: Guided backpropagation (advantage\nstream). In the advatage straem we can see\nthat the NN is slightly more focusing on his\nown position than in the value stream\nFigure 9: Guided backpropagation (value\nstream) . In the value stream we see that the\ngradients are stronger on focused on the future\nrewards. This is because if the agent breaks\nthrough the last line, the ball will bounce off\nthe box and the ceiling and get a lot of reward.\nFigure 10: Guided backpropagation (t-1). We\ncan see here that the old position of the ball\n(frame t-1) has positive gradients, whereas\nthe current position of the ball is very much\nwrapped in negative gradients.\nFigure 11: Guided backpropagation (t-2). We\ncan see here that the old position of the ball\n(frame t-2) has positive gradients, whereas\nthe current position of the ball is very much\nwrapped in negative gradients.\n13\nFigure 12: Guided backpropagation (t-3). We\ncan see here that the old position of the ball\n(frame t-3) has positive gradients, whereas\nthe current position of the ball is very much\nwrapped in negative gradients.\nB.2\nDDDQN: Seaquest-v0\nFigure 13: Gradient (advantage stream). We\ncan see on the Agent some gradients more\nthan on the value Stream, but in general very\nnoisy results.\nFigure 14: Gradient (value stream). We can\nsee on the Oxygen bar some gradients more\nthan on the advantage Stream, but in general\nvery noisy results.\n14\nFigure 15: Guided backpropagation (advan-\ntage stream). In this picture we can see that\nthe NN envelops more gradients around ﬁsh C\nthan on ﬁsh B or A which have hardly any gra-\ndients. This is because the agent has learned\nthat if it stays on the height between the red\nlines and only shoots to the left or right when\nsomething comes up it survives longer and\npays no attention to the rest of the environ-\nment. The agent has learned his behaviour by\nheart and would not be able to react as well\nas an agent who makes less points but reacts\nbetter to the environment. If the environment\nwere to change slightly (e.g. the ﬁsh would\nsuddenly swim from the top right to the bot-\ntom left) the agent could hardly react to it\nbecause it ignores everything that is not in the\narea between the lines.\nFigure 16: Guided backpropagation (value\nstream). In the Value Stream there is a small\ndifference to the advatage stream. We see that\nthe agent has no oxygen and has to appear.\nThe DDDQN agent has never learned how to\nsurfaced in seaquest, however he realises that\nthe oxygen bar is an important feature that\nis related to the end of the episode, but the\nagent has not learned how to do what. And\nsince the agent has learned to leave the area\nbetween the lines, he will not be able to learn\nhow to do it.\nFigure 17: Grad-Cam\nFigure 18: Guided Grad-Cam\n15\nFigure 19:\nG1Grad-Cam.\nCompared to\nBreakout-v0, the DDDQN agent did not give\ngood results for Seaquest-v0.\nFigure 20: G2Grad-Cam\nB.3\nSplit At. DDDQN: Seaquest-v0\nFigure 21: Gradient. Very noisy results.\nFigure 22: Guided backpropagation. Since\nthis agent has a very well trained NN network\nand has achieved the best results in the game,\nyou can can see here very clearly how the pos-\nitive gradients on the features are surrounded\nby negative gradients.\n16\nFigure 23: Grad-Cam. Good visible high-\nlights of the most important features.\nFigure 24: Guided Grad-Cam\nFigure 25: G1Grad-Cam. No visible high-\nlights over the whole video.\nFigure 26: G2Grad-Cam. No visible high-\nlights over the whole video.\n17\nFigure 27: Guided backpropagation (advan-\ntage stream)\nFigure 28: Guided backpropagation (value\nstream). The agent has learned to pay more\nattention to the collected divers in the Value\nStream. Here he has collected all 6 divers,\nwhich means that when he shows up he will\nget extra reward. Furthermore you can see\nthat the ﬁsh in the value stream has more gra-\ndients than in the advantage stream.\nFigure 29: Gradient. In this frame we see\nthat the agent also recognizes the divers that\ncontain a long term reward when the agent\ncollects 6 divers and brings them to the sur-\nface.\nFigure 30: Guided backpropagation. In this\nframe we see that the agent also recognizes\nthe divers that contain a long term reward\nwhen the agent collects 6 divers and brings\nthem to the surface.\n18\nFigure 31: Gradient. Here we see that the\nagent with the gradient method also pays at-\ntention to the empty oxygen bar. This is also\nthe reason why the agent appeared.\nFigure 32: Guided backpropagation. In this\nﬁgure we can see that the agent with the\ngradient method also pays attention to the\nempty oxygen bar - in contrast to the gradient\nmethod the oxygen bar is completely wrapped\nin gradients.\nFigure 33: Grad-Cam 2nd convolutional layer.\nWhen we visualise the second layer, with the\nvery well trained NN we always see no con-\nnections that we can interpret but not as well\nas we saw with the ﬁrst layer\nFigure 34: Guided Grad-Cam 2nd convolu-\ntional layer\n19\nFigure 35: G1Grad-Cam 2nd convolutional\nlayer. As in the ﬁrst layer, this method does\nnot show well interpretable results\nFigure 36: G2Grad-Cam 2nd convolutional\nlayer\nFigure 37:\nGrad-Cam 3rd convolutional\nlayer. Hardly interpretable results. Partially\ninvented highlights of the features.\nFigure 38: Guided Grad-Cam 3rd convolu-\ntional layer\n20\nFigure 39: G1Grad-Cam 3rd convolutional\nlayer. Better results in terms of agent posi-\ntion than the Grad-Cam method.\nFigure 40: G2Grad-Cam 3rd convolutional\nlayer\nFigure 41: Grad-Cam 2nd convolution layer\n(inverted gradients).\nIn the 3rd layer we\ncan often observe inverted gradients with the\nGrad-Cam method as shown in this ﬁgure.\nFigure 42: Guided backpropagation t-9. In\nthis layer we see the visualization of the frame\nt-9 projected on the current frame.\n21\nFigure 43: Gradient; episodes: 325. First\ngradients form around the features. Although\nthe agent still plays randomly and does not\nknow what these features mean, he begins to\nunderstand that they have an inﬂuence on the\ndecisions of the agent. There is little differ-\nence between the gradient method and Guided\nbackpropagation.\nFigure 44: Guided backpropagation; episodes:\n325. First gradients form around the features.\nAlthough the agent still plays randomly and\ndoes not know what these features mean, he\nbegins to understand that they have an inﬂu-\nence on the decisions of the agent. There is\nlittle difference between the gradient method\nand Guided backpropagation.\nFigure 45: Grad-Cam; episodes: 325\nFigure 46: Guided Grad-Cam; episodes: 325.\nThe Grad-Cam method gives clearly better\nand more interpretable results than the gradi-\nent methods.\n22\nFigure 47: G1Grad-Cam; episodes: 325. The\nG1Grad-Cam method gives the best results\nwe can on the ﬁsh and the agent and also on\nthe number of lives of the agent see some\nhighlights.\nFigure 48: G2Grad-Cam; episodes: 325\nB.4\nA3C: Breakout-v0\nFigure 49: Actor: Gradient. Gredients so\nsmall that you can’t see them without a strong\nmultiplication factor.\nFigure 50: Critc: Gradient. You can see some\narbitrary gradients around the ball.\n23\nFigure 51: Actor: Guided backpropagation.\nGredients so small that you can’t see them\nwithout a strong multiplication factor.\nFigure 52: Critc: Guided backpropagation.\nPositive gradients are formed around the ball\nfollowed by negative gradients.\nFigure 53: Actor: Grad-Cam. Inverted gradi-\nents. Around the ball and the agent you can\nsee that the neural net recognizes the agent\nand the ball (invented). This is the only visu-\nalization method on the actor side that recog-\nnizes both the ball and the agent.\nFigure 54: Critic: Grad-Cam. The ball and\nthe agent are well highlighted by this visual-\nization technique.\n24\nFigure 55: Actor: Guided Grad-Cam. Due to\nthe inverted results of the Grad-Cam method\nno results will be shown here.\nFigure 56: Critic: Guided Grad-Cam\nFigure 57: Actor: G1grad-Cam. This method\nshows us a very strong highlighting of the ball\nwhich is also very stable. However, only the\nball is highlighted and not the agent.\nFigure 58: Critic: G1grad-Cam. This method\nshows us a very strong highlighting of the ball\nwhich is also very stable. However, only the\nball is highlighted and not the agent.\n25\nFigure 59: Actor: G2grad-Cam. As we have\nnot seen any gradients in the Guided Back-\npropagation method we do not see any high-\nlighting here either.\nFigure 60: Critic: G2grad-Cam. Slight gradi-\nents can be seen on the ball.\nFigure 61: Actor: Gradient. Visualization\nof the actor gradient with a multiplication of\n250.\nFigure 62: Critic: Gradient.\n26\nFigure 63: Actor: Guided backpropagation.\nVisualization of the actor gradient with a mul-\ntiplication of 250.\nFigure 64: Critic: Guided backpropagation\nB.5\nA3C: Seaquest-v0\nFigure 65: Actor: Gradient. Too small gra-\ndients to visualize them just like in the game\nBreakout-v0.\nFigure 66: Critic: Gradient. Here you can see\nvery clearly that the agent has learned to turn\nand shoot only left and right and never leaves\nthe red area. You can see that the ﬁsh in this\narea are very strongly highlighted and outside\nthis area there are almost no gradients to be\nseen. Sometimes gradients also appear on the\nleft and right sides of the agent, where no ﬁsh\ncan be seen (red circle).\n27\nFigure 67: Actor: Guided backpropagation.\nToo small gradients to visualize them just like\nin the game Breakout-v0.\nFigure 68: Critic: Guided backpropagation.\nHere you can see very clearly that the agent\nhas learned to turn and shoot only left and\nright and never leaves the red area. You can\nsee that the ﬁsh in this area are very strongly\nhighlighted and also the agent itself. Outside\nthis area there are almost no gradients to be\nseen.\nFigure 69: Actor: Grad-Cam. The ﬁsh and\nthe agent itself are clearly highlighted. This\nmethod gives better results for the actor as\nwell as for the Speil Breakout-v0 than the gra-\ndient methods, where the gradients are too\nsmall to visualize them without a multiplica-\ntion factor.\nFigure 70: Critic: Grad-Cam. The ﬁsh and the\nagent itself are clearly highlighted. The ﬁsh\nand the agent itself are clearly highlighted.\n28\nFigure 71: Actor: G1Grad-Cam. Close to the\nimportant features some gradients are visual-\nized.\nFigure 72: Critic: G1Grad-Cam. Close to the\nimportant features some gradients are visual-\nized.\nB.6\nA3C with LSTM: Seaquest-v0\nFigure 73: Actor: Gradient\nFigure 74: Critic: Gradient\n29\nFigure 75: Actor: guided backpropagation\nFigure 76: Critic: guided backpropagation\nFigure 77: Actor: Grad-Cam\nFigure 78: Critic: Grad-Cam\n30\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-12-02",
  "updated": "2020-12-02"
}