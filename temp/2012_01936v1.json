{
  "id": "http://arxiv.org/abs/2012.01936v1",
  "title": "CUT: Controllable Unsupervised Text Simplification",
  "authors": [
    "Oleg Kariuk",
    "Dima Karamshuk"
  ],
  "abstract": "In this paper, we focus on the challenge of learning controllable text\nsimplifications in unsupervised settings. While this problem has been\npreviously discussed for supervised learning algorithms, the literature on the\nanalogies in unsupervised methods is scarse. We propose two unsupervised\nmechanisms for controlling the output complexity of the generated texts,\nnamely, back translation with control tokens (a learning-based approach) and\nsimplicity-aware beam search (decoding-based approach). We show that by nudging\na back-translation algorithm to understand the relative simplicity of a text in\ncomparison to its noisy translation, the algorithm self-supervises itself to\nproduce the output of the desired complexity. This approach achieves\ncompetitive performance on well-established benchmarks: SARI score of 46.88%\nand FKGL of 3.65% on the Newsela dataset.",
  "text": "CUT: Controllable Unsupervised Text Simpliﬁcation\nOleg Kariuk\nUkrainian Catholic University\nShipHawk\nolegkariuk@gmail.com\nDima Karamshuk\nFacebook\nkaramshuk@fb.com\nAbstract\nIn this paper, we focus on the challenge of\nlearning controllable text simpliﬁcations in un-\nsupervised settings. While this problem has\nbeen previously discussed for supervised learn-\ning algorithms, the literature on the analogies\nin unsupervised methods is scarse.\nWe pro-\npose two unsupervised mechanisms for con-\ntrolling the output complexity of the gen-\nerated texts, namely, back translation with\ncontrol tokens (a learning-based approach)\nand simplicity-aware beam search (decoding-\nbased approach). We show that by nudging\na back-translation algorithm to understand the\nrelative simplicity of a text in comparison to its\nnoisy translation, the algorithm self-supervises\nitself to produce the output of the desired com-\nplexity. This approach achieves competitive\nperformance on well-established benchmarks:\nSARI score of 46.88% and FKGL of 3.65% on\nthe Newsela dataset.\n1\nIntroduction\nText simpliﬁcation deals with the problem of rewrit-\ning complex texts into a language which is easier to\nread and understand while preserving its original in-\nformation and meaning. Simpliﬁcation techniques\ncan improve reading comprehension for a broader\nrange of users, ranging from foreign language learn-\ners (Allen, 2009; Petersen and Ostendorf, 2007)\nand non-experts (Elhadad and Sutaria, 2007; Sid-\ndharthan and Katsos, 2010) to people with disabili-\nties (Canning et al., 2000; Carroll et al., 1999) or\nlow-literacy (De Belder and Moens, 2010).\nA variety of supervised and unsupervised ap-\nproaches have been recently applied to the text\nsimpliﬁcation problem. On the supervised side,\nit has been considered as a monolingual machine\ntranslation exercise where a number of dedicated\nsequence-to-sequence models have been proposed\n(Kajiwara and Komachi, 2016; Scarton et al., 2018;\nZhang and Lapata, 2017).\nUnfortunately, the\nscarcity of parallel datasets limits the scalability\nof these approaches in application to different lan-\nguages, domains, and output styles. Moreover, the\nParallel Wikipedia Simpliﬁcation corpus, which has\nbecome the benchmark dataset for training and eval-\nuating text simpliﬁcation systems, is (a) prone to\nautomatic sentence alignment errors, (b) contains\na lot of inadequate simpliﬁcations and (c) poorly\ngeneralizes to other text styles (Xu et al., 2015).\nInspired by the recent success (Lample et al.,\n2018) of unsupervised machine translation, (Zhao\net al., 2020) have applied unsupervised techniques\nof back-translation and noisy auto-encoders to the\nproblem of text simpliﬁcation. By adjusting these\nmechanisms to the particularities of mono-lingual\ntranslation, they managed to achieve performance\nresults competitive with the supervised models.\nWhile this surprising effectiveness of unsupervised\napproaches and their ability to learn from vastly\navailable non-parallel text corpora make them a\nsigniﬁcantly more attractive option than supervised\ntext simpliﬁcation, there yet remains many un-\nsolved challenges. One of such is the challenge of\ncontrolling the complexity of the output produced\nby a text simpliﬁcation algorithm. This problem\nhas been solved for supervised approaches by learn-\ning on the characteristics of the output texts (Scar-\nton and Specia, 2018; Martin et al., 2019), an idea\nthat does not immediately apply to unsupervised\nsettings.\nIn this work, we build on an idea of using ded-\nicated tokens (Martin et al., 2019) to control the\ncomplexity of the produced output texts and apply\nit to unsupervised text simpliﬁcation. We introduce\nthis mechanism to the back translation algorithm,\nwhich allows the model to self-supervise the pro-\ncess of learning inter-relations between a control se-\nquence and the complexity of the produced output.\nWe compare this technique with simplicity-aware\npenalties for beam-search generation of the output\ntexts, thus leveraging on both learning-based and\ndecoding-based mechanisms for controlled text\ngeneration (Kikuchi et al., 2016). Together these\ncontributions allow us to achieve and exceed the\npreviously reported SARI and FKGL results on the\nNewsela dataset.\narXiv:2012.01936v1  [cs.CL]  3 Dec 2020\n2\nMethodology\nWe propose two different approaches for control-\nling the output complexity of unsupervised text\nsimpliﬁcation algorithms, namely, back-translation\nwith control tokens (a learning-based approach)\nand beam search with simplicity-aware penalties\n(a decoding-based approach).\n2.1\nBack-translation with control tokens\nFor learning-based control, we get inspiration from\n(Martin et al., 2019) who introduced different types\nof dedicated tokens to control the output complex-\nity of the supervised text simpliﬁcation. We apply\nthis idea to a self-supervised back-translation algo-\nrithm (Sennrich et al., 2016) as follows.\nFirstly, we produce the noisy translation u∗(y)\nof the original phrase y and compute control tokens\nbased on the original text y and its noisy transla-\ntion, i.e., H(y, u∗(y)), where H is a sequence of\nfour tokens which represent the compression ratio,\nLevenshtein similarity, word rank and the depth of\ndependency tree as deﬁned in (Martin et al., 2019).\nThen\nwe\nconcatenate\nthe\ncomputed\ncontrol\ntokens\nwith\nthe\nnoisy\ntranslation\nH(y, u∗(y)) ⌢u∗(y) and translate the resulting\nsequence back, aiming at reproducing the original\ntext. We apply this procedure for both – simple\n(y ∼S) and complex sentences (x ∼C) – and\ntrain the algorithm to minimize the difference\nbetween the original and the output texts, i.e.,\nL =Ey∼S[−log Pc→s(y|H(y, u∗(y)) ⌢u∗(y))]+\nEx∼C[−log Ps→c(x|H(x, v∗(x)) ⌢v∗(x))]\n2.2\nSimplicity-aware beam search\nFor a decoding-based control, we introduce the\nsimplicit-aware penalties in the beam search when\ngenerating the output texts. Instead of decoding\nthe most probable words in a greedy fashion, the\nbeam search algorithm generates an output sen-\ntence by keeping a ﬁxed number (speciﬁed by beam\nsize parameter) of hypotheses with the highest log-\nprobability at each step.\nTo manage the exact matches ratio, length and\nsimplicity (FKGL-based) of each hypothesis in the\nbeam search iterations, we added three types of\nscore penalties:\nLength penalty (LP) favors shorter or longer\nhypothesis depending on λlength parameter:\nLP = eλlength×length(hypothesis)\nExact matches penalty (EMP) uses cosine sim-\nilarity between input and hypothesis to restrict the\ncopying of input:\nEMP = eλexact matches×cos(input,hypothesis)\nFKGL penalty (FKGLP) encourages hypothe-\nsis with lower FKGL score:\nFKGLP = eλF KGL×FKGL(hypothesis)\nWe ﬁnd an optimal combination of different\npenalties by running a grid search on already\ntrained models over a hold-out validation dataset\nand optimizing for the best trade-off between SARI\nand FKGL 1.\n2.3\nPre-trained language models\nWe use the pre-trained language models from XLM\nlibrary (Lample and Conneau, 2019) as the basis for\nour experiments. The XLM models were trained\non a large Wikipedia + Toronto Book Corpus and\nalready encapsulate powerful embeddings for En-\nglish texts. We also use the implementation of\nthe noisy auto-encoders (AE) and back-translation\n(BT) from the XLM library as the basis for our un-\nsupervised approach and use supervised machine\ntranslation (MT) step for comparing the perfor-\nmance of the supervised and semi-supervised set-\ntings. We denote vanilla unsupervised and semi-\nsupervised XLM models with XLM-U and XLM-S;\nour approach to back-translation with control to-\nkens with CUT-U (t) and CUT-S (t) and a variant\nwith simplicity-aware penalties with CUT-U (p)\nand CUT-S (p), correspondingly.\n3\nExperiments\n3.1\nDataset\nWe conducted our experiments on two different\nsimpliﬁcation datasets, the summary statistics of\nwhich are presented in Table 1.\nWikiLarge has become a benchmark for train-\ning and evaluating text simpliﬁcation models\n(Zhang and Lapata, 2017).\nOriginally it had\n296,402 sentence pairs, but we took 5,000 pairs\nfor machine translation step during our model train-\ning. For validations and tests, we used TurkCorpus\n(Xu et al., 2016).\nNewsela is a corpus of thousands of news ar-\nticles professionally leveled to different reading\n1LP=0.1, EMP=0.4, LFGKLP=0.4 and LP=0.4, EMP=1.3,\nLFGKLP=1.0 have been identiﬁed as optimal for CUT-S (p)\nand CUT-U (p) in range(0.1, 1.3, 0.3), correspondingly.\nWikiLarge\nNewsela\nSource\n291,402\n81,705\nTarget\n291,402\n76,073\nTrain\n5,000\n5,000\nValid\n2,000\n1,500\nTest\n359\n1,500\n-\nVocab (src)\n41,303\n33,316\nVocab (tgt)\n39,912\n22,405\nCompression\n0.98\n0.76\nFKGL (src)\n9.51\n8.51\nFKGL (tgt)\n6.33\n2.86\nTable 1: Statistical description of the datasets\ncomplexities (Xu et al., 2015). We used the most\ncontrast article versions. For the machine transla-\ntion step, for the test, and for the validation datasets,\nwe used parallel complex-simple pairs provided by\n(Xu et al., 2015).\n3.2\nMetrics\nWe use a variety of well established metrics from\nEasier Automatic Sentence Simpliﬁcation Evalua-\ntion (EASSE) framework (Alva-Manchego et al.,\n2019) to analyze the quality of the produced text\nsimpliﬁcations, including:\nBLEU (Bilingual Evaluation Understudy) - a\nprecision-oriented metric that estimates the propor-\ntion of n-gram matches between a system’s output\nand a reference (Papineni et al., 2002).\nSARI, introduced by (Xu et al., 2016), compares\nsystem output against the references and against\nthe input sentence. It measures how the simplicity\nof a sentence was improved based on the words\nadded, deleted, and kept by the system.\nFKGL (Flesch-Kincaid Grade Level) estimates\nthe readability of text using cognitively motivated\nfeatures (Kincaid et al., 1975). Commonly reported\nas measures of simplicity, FKGL relies on average\nsentence lengths and the number of syllables per\nword.\nWe complement this set with (a) the proportion\nof exact matches between simpliﬁed and origi-\nnal sentences, (b) the average proportion of added\nwords and (c) the average proportion of deleted\nwords to gain additional insights on the nature of\nthe transformations that a model is performing on\nthe input texts.\n3.3\nBaselines\nWe benchmarked our model against several well-\nknown baselines:\nBLEU\nSARI\nFKGL\nMatch\nAdd\nDel\nPBMT-R\n18.19\n15.77\n7.59\n-\n-\n-\nHybrid\n14.46\n30.00\n4.01\n-\n-\n-\nEncDecA\n21.7.0\n24.12\n5.11\n-\n-\n-\nDRESS\n23.21\n27.37\n4.13\n-\nDRESS-LS\n24.30\n26.63\n4.21\n-\n-\n-\nDMASS+DCSS\n-\n27.28\n5.17\n-\n-\n-\nXLM-U\n16.97\n19.32\n10.52\n0.46\n0.04\n0.05\nCUT-U (p)\n17.21\n16.51\n8.8\n0.27\n0.03\n0.04\nCUT-U (t)\n18.78\n37.87\n6.55\n0.05\n0.07\n0.41\nXLM-S\n19.44\n43.18\n4.18\n0.09\n0.12\n0.53\nCUT-S (p)\n21.33\n42.47\n3.65\n0.07\n0.1\n0.52\nCUT-S (t)\n21.70\n46.88\n3.92\n0.04\n0.16\n0.58\nOutput = Input\n18.52\n12.78\n10.36\n1.00\n0.00\n0.00\nOutput = Ref\n100.00\n100.00\n4.18\n0.00\n0.19\n0.61\nTable 2: Performance comparison of CUT models and\nbaselines on Newsela dataset.\nPBMT-R is phrase-based machine translation\nsystem with a re-ranking post-processing step pro-\nposed by (Wubben et al., 2012)\nHybrid is a simpliﬁcation model that includes a\nprobabilistic model for splitting and dropping and\na PBMT-R model for substitution and reordering\n(Narayan and Gardent, 2014)\nSBMT-SARI is a syntax-based translation\nmodel trained on PPDB (Ganitkevitch et al., 2013)\nand trained with SARI (Zhang and Lapata, 2017)\nEncDecA, a basic attention-based encoder-\ndecoder model, DRESS, a deep reinforcement\nlearning model, DRESS-LS, a linear combination\nof DRESS and the lexical simpliﬁcation model,\nall of them were introduced in (Zhang and Lapata,\n2017).\nDMASS+DCSS is a combination of DMASS\nand DCSS models from (Zhao et al., 2018).\nUNTS+10K is an unsupervised model based on\na shared encoder and two decoders with limited\nsupervision of 10K labeled examples (Surya et al.,\n2019).\nThe BLEU, SARI, and FKGL results for the\nabove-mentioned models were taken from (Zhang\nand Lapata, 2017) and (Zhao et al., 2018).\n3.4\nResults\nIn Table 2, we summarize the results of the ex-\nperiments on the Newsela dataset. Both control\nmechanisms – penalties and tokens – achieve su-\nperior performance in comparison to the unsuper-\nvised baseline deﬁned by the off the shelf XML-U\nmodel, however, a learning-based approach (CUT-\nU (t)) outpeforms a decoding-based one (CUT-U\n(p)). This ﬁnding is in-line with the previous re-\nsults achieved for supervised text summarization\n(Kikuchi et al., 2016).\nCUT-U (t) out-performs all baseline models from\nInput\nBack in 1950 , Eiji Toyoda visited a Ford plant to learn how Americans made cars .\nReference\nHe visited a Ford factory back in 1950 to learn how Americans made cars .\nNbChars1.0 + LevSim1.0\nBack in 1950 , Eiji Toyoda visited a Ford plant to learn how Americans made cars .\nNbChars1.0 + LevSim0.75\nIn 1950 , Eiji Toyoda visited a Ford factory to learn how Americans made cars .\nNbChars1.0 + LevSim0.5\nIn 1950 , Eiji Toyoda visited a Ford factory to learn how to make cars .\nNbChars1.0 + LevSim0.25\nIn 1950 , Eiji Toyoda visited a Ford factory .\nNbChars1.0 + WordRank1.0\nIn 1950 , Eiji Toyoda visited a Ford factory to learn how Americans made cars .\nNbChars1.0 + WordRank0.75\nIn 1950 , Eiji Toyoda visited a Ford factory to learn how to make cars .\nTable 3: Results of applying control tokens to an example in a Newsela test set: NbChars for compression ratio,\nLevSim for Levenshtein similarity, WordRank as deﬁned in (Martin et al., 2019).\nthe literature (including both supervised and unsu-\npervised) on the SARI metric. Moreover, when\nwe combine our unsupervised control mechanisms\nwith extra supervision provided by the MT step in\nXLM model, we achieve the highest SARI score\nof 46.88 (CUT-S (t)) and the lowest FKGL of 3.65\n(CUT-S (p)) across all compared methods.\nIn Table 3, we demonstrate an example of ap-\nplying control tokens to an input sentence in the\nNewsela test set. A combination of different tokens\nprovides ﬂexibility to adjust the sentence’s output\ncomplexity, which is in line with the previous re-\nsults in (Martin et al., 2019). What is remarkable,\nhowever, is that the model has managed to train it-\nself in a completely unsupervised way by gradually\nlearning from noisy outputs of the back-translation\niterations.\nOur results have been less striking on the other\npopular benchmark – the WikiLarge dataset Ta-\nble 4. Although our models achieved signiﬁcantly\nbetter results on SARI and FKGL than both XLM-U\nand XLM-S baselines, as well as signiﬁcantly de-\ncreased the exact match ratio, we have not observed\nequivalent improvements in the BLEU score and\nin comparison to the-state-of-the-art results. We\nattribute this to the fact that the WikiLarge dataset\nis a signiﬁcantly noisier dataset (Xu et al., 2015).\nWe will be looking in understanding and improving\nour performance in these settings in future work.\n4\nConclusions\nIn this paper, we looked at two unsupervised mecha-\nnisms – a learning-based and a decoding-based – to\ncontrol the output complexity of text simpliﬁcation\nalgorithms. We built on an idea of adding com-\nplexity control tokens in the input text and applied\nit to the back-translation algorithm. By iterating\nthe procedure of generating a noisy translation of a\nsentence and learning from its relative complexity\ncompared to the original, the model self-supervised\nits ability to produce a controllable output and im-\nBLEU\nSARI\nFKGL\nMatch\nAdd\nDel\nPBMT-R\n81.11\n38.56\n8.33\n-\n-\n-\nHybrid\n48.97\n31.40\n4.56\n-\n-\n-\nSBMT-SARI\n73.08\n39.96\n7.29\n-\n-\n-\nEncDecA\n88.85\n35.66\n8.41\n-\n-\n-\nDRESS\n77.18\n37.08\n6.58\n-\n-\n-\nDRESS-LS\n80.12\n37.27\n6.62\n-\n-\n-\nDMASS+DCSS\n-\n40.42\n7.18\n-\n-\n-\nUNTS+10K\n76.13\n35.29\n-\n-\n-\n-\nXLM-S\n92.66\n30.99\n9.68\n0.73\n0.02\n0.02\nCUT-S (t)\n41.33\n32.01\n8.73\n0.05\n0.16\n0.19\nCUT-S (t+p)\n78.01\n35.64\n8.01\n0.18\n0.04\n0.22\nXLM-U\n94.83\n28.30\n9.75\n0.76\n0.02\n0.01\nCUT-U (t)\n49.70\n23.89\n9.77\n0.30\n0.04\n0.06\nOutput = Input\n97.41\n27.32\n9.90\n1.00\n0.00\n0.00\nOutput = Ref\n68.87\n40.83\n8.33\n0.00\n0.19\n0.21\nTable 4: Performance comparison of CUT models and\nbaselines on Wikipedia Large.\nproved its simpliﬁcation performance overall. An\nalternative – decoding-based mechanism – has also\nimproved in comparison with the baseline but has\ndemonstrated inferior performance on SARI and\nBLEU metrics.\nWhile we ﬁnd our models’ ability to self-\nsupervise on noisy outputs rather striking, we think\nthere is even more potential for improving the per-\nformance of this mechanism by providing more\nguidance in the trial-and-error process of applying\ncontrol sequences and learning from them with re-\ninforcement learning. We also plan to explore the\ngeneralizability of the unsupervised approaches by\nconducting a cross-corpora validation, i.e., validate\nthe models on the corpora they have not seen dur-\ning training. We believe that together these will\nhelp to establish unsupervised text simpliﬁcation as\na viable alternative to the supervised methods and\nremove the constraints imposed by the necessity of\ncompiling parallel text corpora.\nReferences\nDavid Allen. 2009.\nA study of the role of relative\nclauses in the simpliﬁcation of news texts for learn-\ners of english. System, 37:585–599.\nFernando Alva-Manchego, Louis Martin, Carolina\nScarton, and Lucia Specia. 2019. EASSE: Easier au-\ntomatic sentence simpliﬁcation evaluation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP): System Demonstra-\ntions, pages 49–54, Hong Kong, China. Association\nfor Computational Linguistics.\nYvonne Canning, John Tait, Jackie Archibald, and Ros\nCrawley. 2000. Cohesive generation of syntactically\nsimpliﬁed newspaper text.\nIn Proceedings of the\nThird International Workshop on Text, Speech and\nDialogue, TDS ’00, pages 145–150, London, UK,\nUK. Springer-Verlag.\nJohn Carroll, Guido Minnen, Darren Pearce, Yvonne\nCanning, Siobhan Devlin, and John Tait. 1999. Sim-\nplifying text for language-impaired readers.\nIn\nNinth Conference of the European Chapter of the\nAssociation for Computational Linguistics, Bergen,\nNorway. Association for Computational Linguistics.\nJan De Belder and Marie-Francine Moens. 2010. Text\nsimpliﬁcation for children.\nNoemie Elhadad and Komal Sutaria. 2007. Mining a\nlexicon of technical terms and lay equivalents. In\nProceedings of the Workshop on BioNLP 2007: Bi-\nological, Translational, and Clinical Language Pro-\ncessing, BioNLP ’07, pages 49–56, Stroudsburg, PA,\nUSA. Association for Computational Linguistics.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013.\nPPDB: The paraphrase\ndatabase. In Proceedings of the 2013 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 758–764, Atlanta, Georgia. Associa-\ntion for Computational Linguistics.\nTomoyuki Kajiwara and Mamoru Komachi. 2016.\nBuilding a monolingual parallel corpus for text sim-\npliﬁcation using sentence similarity based on align-\nment between word embeddings.\nIn Proceedings\nof COLING 2016, the 26th International Confer-\nence on Computational Linguistics: Technical Pa-\npers, pages 1147–1158, Osaka, Japan. The COLING\n2016 Organizing Committee.\nYuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya\nTakamura, and Manabu Okumura. 2016.\nControl-\nling output length in neural encoder-decoders. arXiv\npreprint arXiv:1609.09552.\nJ.\nPeter\nKincaid,\nRobert\nP.\nFishburne,\nRichard Lawrence Rogers, and Brad S. Chissom.\n1975.\nDerivation of new readability formulas\n(automated readability index, fog count and ﬂesch\nreading ease formula) for navy enlisted personnel.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nAdvances in\nNeural Information Processing Systems (NeurIPS).\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018. Unsupervised ma-\nchine translation using monolingual corpora only. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nLouis Martin, Benoˆıt Sagot, ´Eric de la Clergerie, and\nAntoine Bordes. 2019. Controllable sentence sim-\npliﬁcation. arXiv preprint arXiv:1910.02677.\nShashi Narayan and Claire Gardent. 2014. Hybrid sim-\npliﬁcation using deep semantics and machine trans-\nlation.\nIn Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 435–445, Balti-\nmore, Maryland. Association for Computational Lin-\nguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation.\nIn Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nSarah E. Petersen and Mari Ostendorf. 2007. Text sim-\npliﬁcation for language learners: a corpus analysis.\nIn SLaTE.\nCarolina Scarton, Gustavo Paetzold, and Lucia Specia.\n2018. Text simpliﬁcation from professionally pro-\nduced corpora. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation (LREC 2018), Miyazaki, Japan. Euro-\npean Language Resources Association (ELRA).\nCarolina Scarton and Lucia Specia. 2018.\nLearning\nsimpliﬁcations for speciﬁc target audiences. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 712–718.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data.\nIn Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96.\nAdvaith Siddharthan and Napoleon Katsos. 2010. Re-\nformulating discourse connectives for non-expert\nreaders.\nIn Human Language Technologies: The\n2010 Annual Conference of the North American\nChapter of the Association for Computational Lin-\nguistics, HLT ’10, pages 1002–1010, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nSai Surya, Abhijit Mishra, Anirban Laha, Parag Jain,\nand Karthik Sankaranarayanan. 2019. Unsupervised\nneural text simpliﬁcation.\nIn Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2058–2068, Florence,\nItaly. Association for Computational Linguistics.\nSander Wubben, Antal van den Bosch, and Emiel Krah-\nmer. 2012. Sentence simpliﬁcation by monolingual\nmachine translation. In Proceedings of the 50th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1015–\n1024, Jeju Island, Korea. Association for Computa-\ntional Linguistics.\nWei Xu, Chris Callison-Burch, and Courtney Napoles.\n2015.\nProblems in current text simpliﬁcation re-\nsearch: New data can help. Transactions of the Asso-\nciation for Computational Linguistics, 3:283–297.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze\nChen, and Chris Callison-Burch. 2016. Optimizing\nstatistical machine translation for text simpliﬁcation.\nTransactions of the Association for Computational\nLinguistics, 4:401–415.\nXingxing Zhang and Mirella Lapata. 2017. Sentence\nsimpliﬁcation with deep reinforcement learning. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages\n584–594, Copenhagen, Denmark. Association for\nComputational Linguistics.\nSanqiang Zhao, Rui Meng, Daqing He, Saptono Andi,\nand Parmanto Bambang. 2018.\nIntegrating trans-\nformer and paraphrase rules for sentence simpliﬁca-\ntion. arXiv preprint arXiv:1810.11193.\nYanbin Zhao, Lu Chen, Zhi Chen, and Kai Yu.\n2020.\nSemi-supervised text simpliﬁcation with\nback-translation and asymmetric denoising autoen-\ncoders. arXiv preprint arXiv:2004.14693.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-12-03",
  "updated": "2020-12-03"
}