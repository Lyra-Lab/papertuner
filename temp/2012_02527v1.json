{
  "id": "http://arxiv.org/abs/2012.02527v1",
  "title": "Demonstration-efficient Inverse Reinforcement Learning in Procedurally Generated Environments",
  "authors": [
    "Alessandro Sestini",
    "Alexander Kuhnle",
    "Andrew D. Bagdanov"
  ],
  "abstract": "Deep Reinforcement Learning achieves very good results in domains where\nreward functions can be manually engineered. At the same time, there is growing\ninterest within the community in using games based on Procedurally Content\nGeneration (PCG) as benchmark environments since this type of environment is\nperfect for studying overfitting and generalization of agents under domain\nshift. Inverse Reinforcement Learning (IRL) can instead extrapolate reward\nfunctions from expert demonstrations, with good results even on\nhigh-dimensional problems, however there are no examples of applying these\ntechniques to procedurally-generated environments. This is mostly due to the\nnumber of demonstrations needed to find a good reward model. We propose a\ntechnique based on Adversarial Inverse Reinforcement Learning which can\nsignificantly decrease the need for expert demonstrations in PCG games. Through\nthe use of an environment with a limited set of initial seed levels, plus some\nmodifications to stabilize training, we show that our approach, DE-AIRL, is\ndemonstration-efficient and still able to extrapolate reward functions which\ngeneralize to the fully procedural domain. We demonstrate the effectiveness of\nour technique on two procedural environments, MiniGrid and DeepCrawl, for a\nvariety of tasks.",
  "text": "Demonstration-Efﬁcient Inverse Reinforcement Learning in Procedurally\nGenerated Environments\nAlessandro Sestini,1 Alexander Kuhnle,2 Andrew D. Bagdanov1\n1Dipartimento di Ingegneria dell’Informazione, Università degli Studi di Firenze, Florence, Italy\n2Department of Computer Science and Technology, University of Cambridge, United Kingdom\n{alessandro.sestini, andrew.bagdanov}@uniﬁ.it, alexander.kuhnle@cantab.net\nAbstract\nDeep Reinforcement Learning achieves very good results in\ndomains where reward functions can be manually engineered.\nAt the same time, there is growing interest within the commu-\nnity in using games based on Procedurally Content Generation\n(PCG) as benchmark environments since this type of environ-\nment is perfect for studying overﬁtting and generalization of\nagents under domain shift. Inverse Reinforcement Learning\n(IRL) can instead extrapolate reward functions from expert\ndemonstrations, with good results even on high-dimensional\nproblems, however there are no examples of applying these\ntechniques to procedurally-generated environments. This is\nmostly due to the number of demonstrations needed to ﬁnd a\ngood reward model. We propose a technique based on Adver-\nsarial Inverse Reinforcement Learning which can signiﬁcantly\ndecrease the need for expert demonstrations in PCG games.\nThrough the use of an environment with a limited set of initial\nseed levels, plus some modiﬁcations to stabilize training, we\nshow that our approach, DE-AIRL, is demonstration-efﬁcient\nand still able to extrapolate reward functions which generalize\nto the fully procedural domain. We demonstrate the effec-\ntiveness of our technique on two procedural environments,\nMiniGrid and DeepCrawl, for a variety of tasks.\n1\nIntroduction\nIn recent years Deep Reinforcement Learning (DRL) has\nyielded impressive results on problems with known reward\nfunctions in complex environments such as video games\n(Mnih et al. 2015; OpenAI et al. 2019; Vinyals et al. 2019)\nand continuous control (Levine et al. 2016). However, de-\nsigning and engineering good hard-coded reward functions is\ndifﬁcult in some domains. In other settings, a badly-designed\nreward function can lead to agents which receive high re-\nwards in unintended ways (Amodei et al. 2016).\nInverse Reinforcement Learning (IRL) algorithms attempt\nto infer a reward function from expert demonstrations (Ng\nand Russell 2000). This reward function can then be used to\ntrain agents which thus learn to mimic the policy implicitly\nexecuted by human experts. IRL offers the promise of solving\nmany of the problems entailed by reward engineering. These\napproaches have achieved good performance both in continu-\nous control tasks (Fu, Luo, and Levine 2018; Finn, Levine,\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nand Abbeel 2016) and in Atari games (Tucker, Gleave, and\nRussell 2018).\nAt the same time, there is increasing interest from the\nDRL community in procedurally-generated environments.\nIn the video game domain, Procedural Content Generation\n(PCG) refers to the programmatic generation of environments\nusing random processes that result in an unpredictable and\nnear-inﬁnite range of possible states. PCG controls the lay-\nout of game levels, the generation of entities and objects,\nand other game-speciﬁc details. Cobbe et al. noted that in\nclassical benchmarks like the Arcade Learning Environment\n(ALE) (Bellemare et al. 2013), agents can memorize speciﬁc\ntrajectories instead of learning relevant skills, since agents\nperpetually encounter near-identical states. Because of this,\nPCG environments are a promising path towards address-\ning the need for generalization in RL. For an agent to do\nwell in a PCG environment, it has to learn policies robust to\never-changing levels and a general representation of the state\nspace.\nMost IRL benchmarks focus on ﬁnding reward functions\nin simple and static environments like MuJoCo (Todorov,\nErez, and Tassa 2012) and comparatively simple video games\nlike Atari (Tucker, Gleave, and Russell 2018). None of these\nRL problems incorporate levels generated randomly at the\nbeginning of each new episode. The main challenges with\nprocedurally-generated games is the dependence of IRL ap-\nproaches on the number of demonstrations: due to the vari-\nability in the distribution of levels, if a not sufﬁciently large\nnumber of demonstrations is provided, the reward function\nwill overﬁt to the trajectories in the expert dataset. This leads\nto an unsuitable reward function and consequently poorly\nperforming RL agents. Moreover, in most domains, provid-\ning a large number of expert demonstrations is expensive in\nterms of human effort.\nTo mitigate the need for very many expert demonstrations\nin PCG games, we propose a novel Inverse Reinforcement\nLearning technique for such environments. Our work is based\non Adversarial Inverse Reinforcement Learning (AIRL) (Fu,\nLuo, and Levine 2018) and substantially reduces the required\nnumber of expert trajectories (see ﬁgure 1). We propose spe-\nciﬁc changes to AIRL in order to decrease overﬁtting in\nthe discriminator, to increase training stability, and to help\nachieve better performance in agents trained using the learned\nreward. Additionally, instead of using a fully procedural envi-\nFigure 1: Demonstration-efﬁcient AIRL. The left part of the image illustrates the AIRL baseline, which extrapolates a reward\nfunction from expert demonstrations directly on the fully procedural environment. This naive application of AIRL requires a\nlarge number of expert demonstrations. Our demonstration-efﬁcient AIRL approach is shown in the right part of the image.\nDE-AIRL extrapolates the reward function on a subset of all possible game levels, referred to as SeedEnv, and is applied in\nthe fully procedural environment, ProcEnv, only after training. This approach enables an RL policy to achieve near-expert\nperformance while requiring only a few expert demonstrations.\nronment for training, we “under-sample” the full distribution\nof levels into a small, ﬁxed set of seed levels, and experts\nneed only provide demonstrations for this reduced set of\nprocedurally-generated levels. We show that the disentangled\nreward functions learned by AIRL generalize enough such\nthat, subsequently, they enable us to ﬁnd near-expert policy\neven on the full distribution of all possible levels. We test\nour approach in two different PCG environments for various\ntasks.\n2\nRelated Work\nInverse Reinforcement Learning (IRL) refers to techniques\nthat infer a reward function from human demonstrations,\nwhich can subsequently be used to train an RL policy. It is\noften assumed that demonstrations come from an expert who\nis behaving near-optimally. IRL was ﬁrst described by Ng\nand Russell, and one of its ﬁrst successes was by Ziebart et al.\nwith Maximum Entropy IRL, a probabilistic approach based\non the principle of maximum entropy favoring rewards that\nlead to a high-entropy stochastic policy. However, this ap-\nproach assumes known transition dynamics and a ﬁnite state\nspace, and can retrieve only a linear reward function. Guided\nCost Learning (Finn, Levine, and Abbeel 2016) relaxed these\nlimitations and was one of the ﬁrst algorithm able to esti-\nmate non-linear reward functions over inﬁnite state spaces\nin environments with unknown dynamics. Recently, Finn,\nLevine, and Abbeel noticed that GCL is closely related to\nGAN training, and this idea led to the development of Adver-\nsarial Inverse Reinforcement Learning (AIRL) (Fu, Luo, and\nLevine 2018). This method is able to recover reward func-\ntions robust to changes in dynamics and can learn policies\neven under signiﬁcant variations in the environment.\nSimilarly to IRL, Imitation Learning (IL) aims to directly\nﬁnd a policy that mimics the expert behavior from a dataset of\ndemonstrations, instead of inferring a reward function which\ncan subsequently be used to train an RL policy. Standard\napproaches are based on Behavioral Cloning (Bain and Sam-\nmut 1995; Syed and Schapire 2008) that mainly use super-\nvised learning (Bain and Sammut 1995; Syed and Schapire\n2008; Ross, Gordon, and Bagnell 2011; Reddy, Dragan, and\nLevine 2019; Cai et al. 2019; Knox and Stone 2009). Genera-\ntive Adversarial Imitation Learning (GAIL) (Ho and Ermon\n2016) is a recent IL approach which is based on a generator-\ndiscriminator approach similar to AIRL. However, since our\ngoal is to operate in PCG environments, we require IRL\nmethods able to learn a reward function which generalizes to\ndifferent levels rather than a policy which tends to overﬁt to\nlevels seen in expert demonstrations.\nIbarz et al. combine IL and IRL: they ﬁrst do an itera-\ntion of Behavioral Cloning, and then apply active preference\nlearning (Christiano et al. 2017) in which they ask humans\nto choose the best of two trajectories generated by the policy.\nWith these preferences they obtain a reward function, which\nthe policy tries to optimize in an iterative process.\nProcedural Content Generation (PCG) refers to algorith-\nmic generation of level content, such as map layout or en-\ntity attributes in video games. There is a growing interest\nin PCG environments from the DRL community. As noted\nabove, Cobbe et al. created a suite of PCG benchmarks and\ndemonstrated that the ability to generalize becomes an inte-\ngral component of success when agents are faced with ever\nchanging levels. Similarly, Risi and Togelius state that often\nan algorithm will not learn a general policy, but instead a\npolicy that only works for a speciﬁc version of a speciﬁc\ntask with speciﬁc initial parameters. Justesen et al. explored\nhow procedurally-generated levels can increase generaliza-\ntion during training, showing that for some games procedural\nlevel generation enables generalization to new levels within\nthe same distribution. Other examples of PCG environments\nused as DRL benchmarks are (Küttler et al. 2020; Guss et al.\n2019; Chevalier-Boisvert, Willems, and Pal 2019; Juliani\net al. 2019; Sestini, Kuhnle, and Bagdanov 2019). Notably,\nGuss et al. applied Imitation Learning in the form of be-\nhavioral cloning over a large set of human demonstrations\nin order to improve the sample efﬁciency of DRL. To the\nbest of our knowledge, our work is the ﬁrst to apply IRL to\nprocedurally-generated environments.\n3\nAdversarial Inverse Reinforcement\nLearning\nOur approach is based on Adversarial Inverse Reinforce-\nment Learning (AIRL), which takes inspiration from GANs\n(Goodfellow et al. 2014) by alternating between training a dis-\ncriminator Dθ(s, a) to distinguish between policy and expert\ntrajectories and optimizing the trajectory-generating policy\nπ(a|s). The AIRL discriminator is given by:\nDθ(s, a) =\nexp{fθ,ω(s, a, s′)}\nexp{fθ,ω(s, a, s′)} + π(a|s),\n(1)\nwhere π(a|s) is the generator policy and fθ,ω(s, a, s′) =\nrφ(s, a)+γφω(s′)−φω(s) is a potential base reward function\nwhich combines a reward function approximator r(s, a) and\na reward shaping term φω. For deterministic environment\ndynamics, Fu, Luo, and Levine show that there is a state-\nonly reward approximator f ∗(s, a, s′) = r∗(s) + γV ∗(s′) −\nV ∗(s) = A∗(s, a), where the reward is invariant to transition\ndynamics and hence “disentangled”.\nThe objective of the discriminator is to minimize the cross-\nentropy between expert demonstrations τ E = (sE\n0 , aE\n0 , . . . )\nand generated trajectories τ π = (sπ\n0, aπ\n0, . . . ):\nL(θ) = −Eτ E\n\" T\nX\nt=0\nlog Dθ(sE\nt , aE\nt )\n#\n−Eτ π∼π\n\" T\nX\nt=0\nlog\n\u00001 −Dθ(sπ\nt , aπ\nt )\n\u0001\n#\n.\n(2)\nThe authors show that, at optimality, f ∗(s, a)\n=\nlog π∗(a|s) = A∗(s, a), which is the advantage function\nof the optimal policy. The learned reward function is based\non the discriminator:\nˆr(s, a) = log(Dθ(s, a)) −log(1 −Dθ(s, a)),\n(3)\nand the generator policy is optimized with respect to a maxi-\nmum entropy objective (using equations (3) and (1)):\nJ(π) = Eτ∼π\n\" T\nX\nt=0\nˆrt(st, at)\n#\n= Eτ∼π\n\" T\nX\nt=0\nfθ(st, at) −log(π(at|st))\n#\n.\n(4)\n4\nModiﬁcations to AIRL\nIn the following we present three extensions to the original\nAIRL algorithm which increase stability and performance,\nwhile decreasing the tendency of the discriminator to overﬁt\nto the expert demonstrations.\n• Reward standardization. Adversarial training alternates\nbetween discriminator training and policy optimization,\nand the latter is conditioned on the reward which is updated\nwith the discriminator. However, forward RL assumes a\nstationary reward function, which is not true in adversarial\nIRL training. Moreover, policy-based DRL algorithms usu-\nally learn a value function based on rewards from previous\niterations, which consequently may have a different scale\nfrom the currently observed rewards due to discriminator\nupdates. Generally, forward RL is very sensitive to reward\nscale which can affect the stability of training. For these\nreasons, as suggested in Tucker, Gleave, and Russell and\nIbarz et al., we standardize the reward to have zero mean\nand some standard deviation.\n• Policy dataset expansion. In the original AIRL algorithm,\neach discriminator training step is followed by only one\npolicy optimization step. The experience collected in this\npolicy step is then used for the subsequent discriminator\nupdate. However, a single trajectory may not offer enough\ndata diversity to prevent the discriminator from overﬁt-\nting. Hence, instead of just one policy step, we perform\nK iterations of forward RL for every discriminator step as\nsuggested by Tucker, Gleave, and Russell.\nMoreover, as already noted by Fu, Luo, and Levine and\nReddy, Dragan, and Levine, IRL methods tend to learn\nrewards which explain behavior locally for the current\npolicy, because the reward can forget the signal that it\ngave to an earlier policy. To mitigate this effect we follow\ntheir practice of using experience replay over the previous\niterations as policy experience dataset. For the same reason,\nwhen we apply the learned reward function, we do not re-\nuse the ﬁnal, possibly overﬁtted reward model, but rather\none from an earlier training iteration.\n• Fixed number of timesteps. Many environments have a\nterminal condition which can be triggered by agent behav-\nior. Christiano et al. observed that these conditions can\nencode information about the environment even when the\nreward function is not observable, thus making the policy\ntask easier. Moreover, since the range of the learned re-\nward model is arbitrary, rewards may be mostly negative\nin some situations, which encourages the agent to meet the\nterminal conditions as early as possible to avoid more neg-\native rewards (the so-called “RL suicide bug”). For these\nreasons we do not terminate an episode in a terminal state,\nbut artiﬁcially extend it to a ﬁxed number of timesteps by\nrepeating the last timestep.\n5\nDemonstration-efﬁcient AIRL in\nprocedural environments\nIn PCG game environments, the conﬁguration of the level as\nwell as its entities are determined algorithmically. Unless the\ngame is very simplistic, this means it is unlikely to see the\nexact same level conﬁguration twice. Forward RL beneﬁts\nfrom such environmental diversity by increasing the level of\ngeneralization and credibility of agent behavior. However, as\na consequence of this diversity, many expert demonstrations\nmay be required for IRL to learn useful behavior. This is es-\n(a) Minigrid\n(b) Potions task\n(c) Maze task\n(d) Ranged task\nFigure 2: Screenshots of the various environments and tasks.\npecially challenging for an adversarial techniques like AIRL\nas it is known that GANs require many of positive examples\n(Luˇci´c et al. 2019).\nIn the following, we call the fully procedural environment\nProcEnv. Levels Li ∼ProcEnv are sampled from this en-\nvironment, and sample trajectories τ Li ∼Li from each\nlevel, where trajectories τ Li = (s0, a0, . . . , aT −1, sT ) are\nsequences of alternating states and actions. Consequently, if\nwe have two trajectories τ Li and τ Lj, in most cases (unless\nLi = Lj) they differ not only in their state-action sequences,\ni.e. the behavior, but also in their level content Li vs Lj from\nwhich they are sampled.\nTo illustrate this, suppose we have a simple ProcEnv\nwith two generation parameters: the number of objects\no ∈[1, 10] and the number of enemies e ∈[1, 6], so overall\n|ProcEnv| = 10 · 6 = 60 level conﬁgurations. Sampling ex-\npert demonstrations is a two-stage process: ﬁrst, we sample\nlevels L1 = (3, 4), L2 = (5, 1), L3 = (7, 2) ∼ProcEnv,\nwhere (o, e) denotes the number of objects and enemies, re-\nspectively, and next we sample corresponding trajectories\nτ (3,4)\n1\n, τ (5,1)\n2\n, τ (7,2)\n3\n, which form our expert dataset. When\nfaced with another trajectory sample based on a random level,\nsay, τ (1,4), the discriminator can simply distinguish expert\nand non-expert trajectories by counting objects and enemies\nin the levels as observed in the states of the trajectories and\nignoring agent behavior entirely. Sampling more expert tra-\njectories increases the probability of levels being equal (or\nat least similar), and thus makes it harder to memorize level\nconﬁgurations. However, collecting a large number of demon-\nstrations can be very expensive, and cannot not ultimately\nsolve the problem for rich enough PCG environments.\nOur objective is to make AIRL effective and data-efﬁcient\nwhen working with PCG environments. Our main idea is to\nintroduce an artiﬁcially reduced environment, which we call\na SeedEnv, that consists of n ≪N levels sampled from the\nfully procedural ProcEnv. These levels are then used to obtain\nn randomly sampled expert demonstrations:\nSeedEnv(n) = {L1, ..., Ln | Li ∼ProcEnv}\nDemos = {τ Li | Li ∈SeedEnv(n)}\nUsing the simpliﬁed example from before, this would\nmean that SeedEnv(3) = {L1, L2, L3} and Demos =\n{τ L1\n1 , τ L2\n2 , τ L3\n3 }. In the following, we refer to each Li ∈\nSeedEnv(n) as seed level.\nThe reward function is learned via AIRL on the reduced\nSeedEnv environment instead of the fully-procedural Pro-\ncEnv. To distinguish expert from non-expert trajectories, the\ndiscriminator thus cannot rely on memorized level character-\nistics seen in expert demonstrations, but instead must consider\nthe behavior represented by the state-action sequence of the\ntrajectory.\nOnce the discriminator is trained on SeedEnv, the learned\nreward function can be used to train a new agent on the\nfull ProcEnv environment. The disentanglement property\nof AIRL encourages the reward function to be robust to\nthe change of dynamics between different levels, assuming\na minimum number of seed levels necessary to generalize\nacross level conﬁgurations.\nIn summary, we observe that there are two sources of\ndiscriminative features in expert trajectories: those related\nto the level, and those related to agent behavior. If AIRL is\napplied naively to PCG environments, the discriminator is\nprone to overﬁtting to level characteristics seen during expert\ndemonstrations instead of focusing on the expert behavior\nitself. On the one hand, by reducing discriminator training\nto the SeedEnv – the set of expert demonstration levels –\nwe force the discriminator to focus on trajectories and to\navoid overﬁtting to level characteristics. On the other hand,\nSeedEnv must contain enough levels to enable the resulting\nreward function to generalize beyond levels in the reduced\nProcEnv sample. We show empirically in the next section\nthat the number of levels required to generalize beyond levels\nsampled in ProcEnv is much smaller than the number required\nto avoid overﬁtting, which may be infeasibly large for PCG\nenvironments with many conﬁguration options.\n6\nExperimental results\nWe evaluate our method on two different PCG environments:\nMinigrid (Chevalier-Boisvert, Willems, and Pal 2019) and\nDeepCrawl (Sestini, Kuhnle, and Bagdanov 2019). For all\nexperiments, we train an agent with Proximal Policy Opti-\nmization (Schulman et al. 2017) on the ground-truth, hard-\ncoded reward function and then generate trajectories from\nthis trained expert policy to use as demonstrations for IRL.\nThe apprenticeship learning metric is used for IRL evaluation:\nagent performance is measured based on the ground-truth\nreward after having been trained on the learned IRL reward\nmodel.\nWe use the state-only AIRL algorithm with all modiﬁca-\ntions described in section 4 to learn a reward function in all\nexperiments. We also trained policies with state-only GAIL\nbut, as it is not an IRL method, we cannot re-optimize the\n(a)\n(b)\nFigure 3: Experimental results on MiniGrid. (a) Mean reward during training for: our DE-AIRL with different numbers of seed\nlevels on both SeedEnv and ProcEnv, naive AIRL with different numbers of demonstrations on ProcEnv, and expert performance\non ProcEnv. (b) Discriminator loss during training on either SeedEnv (our approach) or ProcEnv (naive AIRL).\nobtained model, so we instead transfer the learned policy\nfrom the SeedEnv to ProcEnv.\nTo highlight the importance of our SeedEnv approach for\nlearning good rewards in the context of PCG environments,\nwe perform the following ablations for each task:\n• DE-AIRL (ours): We train a reward function on See-\ndEnv and use it to train a PPO agent on ProcEnv. We\nshow results for a varying number n of seed levels in\nSeedEnv(n).\n• AIRL without disentanglement: We train a reward func-\ntion on SeedEnv, but without the shaping term φω(s)\nwhich encourages robustness to level variation.\n• Naive AIRL: We apply AIRL directly on ProcEnv and\nshow results for a varying number n of demonstrations.\n• GAIL: We train a policy with GAIL on SeedEnv and then\nevaluate it on ProcEnv.\nDetails on network architectures and hyperparameters, includ-\ning an ablation study for the AIRL modiﬁcations described\nin section 4, are given in Appendix A and Appendix B 1.\nPerformance on Minigrid\nMinigrid is a grid world environment with multiple variants.\nFor our experiments, we use the MultiRoom task: a PCG\nenvironment consisting of a 15 × 15 grid, where each tile\ncan contain either the agent, a door, a wall, or the goal. See\nﬁgure 2 for an example screenshot of the environment. The\naim of the agent is to explore the level and arrive at the goal\ntile by navigating through 2 or 3 rooms connected via doors.\nThe shape and position of the rooms, as well as the position\nof the goal and the initial location of the agent, are random.\nEach episode lasts a maximum of 30 steps. The ground-truth\nreward function gives +1.0 for each step the agent stays at the\ngoal location. The action space consist of 4 discrete actions:\nmove forward, turn left, turn right and open door.\nAs the results in ﬁgure 3 show, using a SeedEnv with only\n40 levels and the associated 40 demonstrations, AIRL is able\nto extrapolate a good reward function enabling the agent to\nachieve near-expert performance in ProcEnv. However, if we\n1Supplementary Material with code is available at http://tiny.cc/\nde_airl\ntrain a reward model with only 40 demonstrations directly on\nthe full PCG environment, we obtain an inadequate reward\nfunction and consequently a poor agent policy. This is also\ndemonstrated by the loss curves: the loss of the discriminator\nwith 40 demonstrations on ProcEnv converges to zero very\nquickly, indicating the overﬁtting to level characteristics we\ndiscussed in section 5. The results also show that 40 is a good\nnumber of seed levels for SeedEnv: whereas we ﬁnd a good\npolicy for SeedEnv with only 30 seed levels, the reward func-\ntion does not generalize beyond the expert levels to be useful\non ProcEnv. Moreover, the plots show that naive AIRL is not\nsuccessful on ProcEnv with even 100 – so more than twice as\nmany – expert trajectories. Only with 1000 demonstrations\ndoes naive AIRL achieve near-expert performance, showing\nthat our DE-AIRL is much more demonstration-efﬁcient.\nPerformance on DeepCrawl\nDeepCrawl is a Roguelike game built for studying the ap-\nplicability of DRL techniques in video game development.\nThe visible environment at any time is a grid of 10 × 10 tiles.\nEach tile can contain the agent, an opponent, an impassable\nobject, or collectible loot. The structure of the map and object\nlocations are procedurally generated at the beginning of each\nepisode. Collectible loot and actors have attributes whose val-\nues are randomly chosen as well. The action space consists\nof 8 movement actions: horizontal, vertical and diagonal.\nFor our experiments, we use three different tasks deﬁned\nin the DeepCrawl environment (see ﬁgure 2):\n• Potions: The agent must collect red potions while avoid-\ning all other collectible objects. The ground-truth reward\nfunction gives +1.0 for collecting a red potion and −0.5\nfor collecting any other item. An episode ends within 20\nsteps.\n• Maze: In this variant, the agent must reach a randomly\nlocated goal in an environment with many impassable\nobstacles forming a maze. The goal is a static enemy and\nthere are no collectible objects. The reward function gives\n+10.0 for each step the agent stays in proximity to the\ngoal. Episodes end after 20 timesteps.\n• Ranged Attack: For this task, the agent has two addi-\ntional actions: melee attack and ranged attack. The goal\nFigure 4: Experimental results on DeepCrawl tasks. Mean reward during training for: our DE-AIRL with different numbers\nof seed levels on both SeedEnv and ProcEnv, naive AIRL with different numbers of demonstrations on ProcEnv, and expert\nperformance on ProcEnv.\nFigure 5: Mean reward on SeedEnv throughout training for AIRL without the shaping term, and for GAIL, plus expert\nperformance on ProcEnv for comparison. SeedEnvs consist of 40 levels for Minigrid, and 20 levels for the DeepCrawl tasks.\nof the agent is to hit a static enemy with only ranged\nattacks until the enemy is defeated. The ground-truth re-\nward function gives +1.0 for each ranged attack made\nby the agent. The levels are the same as for the Potions\ntask, plus a randomly located enemy. Episodes end after\n20 timesteps.\nEven for the more complex DeepCrawl tasks, the results\nin ﬁgure 4 show that our demonstration-efﬁcient AIRL ap-\nproach allows agents to learn a near-expert policy for ProcEnv\nwith few demonstrations: in two of the three tasks only 20\ndemonstrations are necessary, while for the Ranged Attack\ntask 10 already sufﬁce. Similar to Minigrid, the naive AIRL\napproach directly applied on ProcEnv does not achieve good\nperformance even with 100 demonstrations – so with more\nthan ﬁve times as many demonstrations. With 1000 demon-\nstrations, naive AIRL reaches similar performance on Potions\nand Maze, but still not on the Ranged Attack task. In ﬁgure 2\nof the Supplementary Material, we provide more detailed re-\nsults for the DeepCrawl experiments, including the evolution\nof discriminator losses which behave consistently with what\nwe have observed for the Minigrid environment.\nImportance of disentanglement\nWe claimed above that the use of a disentangling IRL al-\ngorithm like AIRL is fundamental for PCG games. We test\nthis experimentally by training an AIRL reward function\nwithout the shaping term φ(s) on a SeedEnv. As the plots\nin ﬁgure 5 show, this modiﬁed version does not achieve the\nsame level of performance as the full disentangling AIRL on\nall tasks. We believe this is due to the variability of levels\nin SeedEnv: removing φ(s) takes away the disentanglement\nproperty, which results in the reward function no longer being\nable to generalize, even for the small set of ﬁxed seed levels.\nSimilar results were observed by Roa-Vicens et al..\nWe also train a state-only GAIL model on a SeedEnv. On\nMinigrid and Maze the policy reaches near-expert perfor-\nmance, while on Potions and Ranged Attack it resembles the\nperformance of AIRL without φ(s). We believe that this dis-\ncrepancy is caused by the different degree of “procedurality”\nof these tasks: for Potions and Ranged Attack, there are many\ndifferent collectible objects with procedural parameters – in\nfact, all entities and their attributes are chosen randomly at\nthe beginning of each episode. For the other two tasks, the\nnumber of procedural choices is smaller, consisting only of\nthe static obstacles and no attributes. The degree of proce-\ndurality presumably allows GAIL to achieve good results\non SeedEnv for Minigrid and Maze, but not for Potions and\nRanged Attack. However, on none of the tasks does GAIL\nreach the level of performance of our demonstration-efﬁcient\nAIRL approach when transferring policies from SeedEnv to\nProcEnv, as shown in table 1. Note that, as we have men-\ntioned before, GAIL is not an IRL method and hence cannot\nbe re-optimized on the ProcEnv environment, contrary to\nAIRL, so this shortcoming is not unexpected.\nTable 1: Average ground-truth episode reward over 100 episodes on ProcEnv. Our AIRL approach trains an agent directly on\nProcEnv using the reward model learned on SeedEnv, whereas this is not possible for GAIL, hence the GAIL policy is trained on\nSeedEnv and then transferred to ProcEnv.\nMinigrid\nDeepCrawl\nSeed levels\nMultiRoom\nSeed levels\nPotions\nMaze\nRanged Attack\nDE-AIRL (ours)\n40\n12.19\n20\n3.78\n141.87\n2.30\nGAIL\n40\n9.00\n20\n1.71\n33.77\n1.01\n100\n9.21\n100\n2.38\n66.00\n1.11\n7\nConclusion\nWe have presented an IRL approach, DE-AIRL, which is\nbased on AIRL with a few modiﬁcations to stabilize perfor-\nmance, and is able to ﬁnd a good reward function for PCG\nenvironments with only few demonstrations. Our method\nintroduces a SeedEnv which consists of only a few levels\nsampled from the PCG level distribution, and which is used\nto train the reward model instead of the full fully-procedural\nenvironment. In doing so, the learned reward model is able to\ngeneralize beyond the SeedEnv levels to the fully-procedural\nenvironment, while it simultaneously avoids overﬁtting to the\nexpert demonstration levels. We have shown that DE-AIRL\nsubstantially reduces the number of required expert demon-\nstrations as compared to AIRL when directly applied on the\nPCG environment. Moreover, the experiments illustrated that\nthe success of our approach derives from the disentanglement\nproperty of the reward function extrapolated by AIRL. Fi-\nnally, we compared to an imitation learning approach, GAIL,\nand observed that DE-AIRL generalizes better than the GAIL\npolicy when transferring from the expert demonstration levels\nto the fully-procedural environment.\nA disadvantage of our method is that we do not know\nthe required number of seed levels prior to training. In this\ndirection, an interesting next step would be to understand\nwhat minimum number of seed levels is required to obtain a\ngood reward function as well as a good policy. For instance,\nstarting with a small number of seed levels, how can we\nchoose additional seed levels optimally based on the training\nand learned reward function so far?\nReferences\nAmodei, D.; Olah, C.; Steinhardt, J.; Christiano, P.; Schul-\nman, J.; and Mané, D. 2016. Concrete problems in AI safety.\narXiv preprint arXiv:1606.06565 .\nBain, M.; and Sammut, C. 1995.\nA Framework for Be-\nhavioural Cloning. In Machine Intelligence 15, 103–129.\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The arcade learning environment: An evaluation plat-\nform for general agents. Journal of Artiﬁcial Intelligence\nResearch 47: 253–279.\nCai, X.-Q.; Ding, Y.-X.; Jiang, Y.; and Zhou, Z.-H. 2019.\nExpert-Level Atari Imitation Learning from Demonstrations\nOnly. arXiv preprint arXiv:1909.03773 .\nChevalier-Boisvert, M.; Willems, L.; and Pal, S. 2019.\nMinimalistic gridworld environment for openai gym.\nGithub Repository. URL https://github.com/maximecb/gym-\nminigrid.\nChristiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.;\nand Amodei, D. 2017. Deep reinforcement learning from\nhuman preferences. In Advances in Neural Information Pro-\ncessing Systems.\nCobbe, K.; Hesse, C.; Hilton, J.; and Schulman, J. 2019.\nLeveraging procedural generation to benchmark reinforce-\nment learning. arXiv preprint arXiv:1912.01588 .\nFinn, C.; Levine, S.; and Abbeel, P. 2016. Guided cost learn-\ning: Deep inverse optimal control via policy optimization. In\nInternational Conference on Machine Learning.\nFu, J.; Luo, K.; and Levine, S. 2018. Learning Robust Re-\nwards with Adverserial Inverse Reinforcement Learning. In\nInternational Conference on Learning Representations.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-\nFarley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014.\nGenerative adversarial nets. In Advances in Neural Informa-\ntion Processing Systems.\nGuss, W. H.; Houghton, B.; Topin, N.; Wang, P.; Codel, C.;\nVeloso, M.; and Salakhutdinov, R. 2019. MineRL: a large-\nscale dataset of minecraft demonstrations. In International\nJoint Conference on Artiﬁcial Intelligence.\nHo, J.; and Ermon, S. 2016. Generative adversarial imitation\nlearning. In Advances in Neural Information Processing\nSystems.\nIbarz, B.; Leike, J.; Pohlen, T.; Irving, G.; Legg, S.; and\nAmodei, D. 2018. Reward learning from human preferences\nand demonstrations in Atari. In Advances in Neural Informa-\ntion Processing Systems.\nJuliani, A.; Khalifa, A.; Berges, V.-P.; Harper, J.; Teng, E.;\nHenry, H.; Crespi, A.; Togelius, J.; and Lange, D. 2019. Ob-\nstacle Tower: A Generalization Challenge in Vision, Control,\nand Planning. In International Joint Conference on Artiﬁcial\nIntelligence.\nJustesen, N.; Torrado, R.; Bontrager, P.; Khalifa, A.; Togelius,\nJ.; and Risi, S. 2018. Illuminating Generalization in Deep\nReinforcement Learning through Procedural Level Genera-\ntion. Proceedings of NIPS Workshop on Deep Reinforcement\nLearning .\nKnox, W. B.; and Stone, P. 2009. Interactively shaping agents\nvia human reinforcement: The TAMER framework. In Inter-\nnational Conference on Knowledge Capture.\nKüttler, H.; Nardelli, N.; Miller, A. H.; Raileanu, R.; Selvatici,\nM.; Grefenstette, E.; and Rocktäschel, T. 2020. The NetHack\nLearning Environment. arXiv preprint arXiv:2006.13760 .\nLevine, S.; Finn, C.; Darrell, T.; and Abbeel, P. 2016. End-\nto-end training of deep visuomotor policies. The Journal of\nMachine Learning Research 17(1): 1334–1373.\nLuˇci´c, M.; Tschannen, M.; Ritter, M.; Zhai, X.; Bachem,\nO.; and Gelly, S. 2019. High-Fidelity Image Generation\nWith Fewer Labels. In International Conference on Machine\nLearning.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; et al. 2015. Human-level control through\ndeep reinforcement learning. Nature 518(7540): 529–533.\nNg, A. Y.; and Russell, S. 2000. Algorithms for Inverse\nReinforcement Learning. In International Conference on\nMachine Learning.\nOpenAI; Berner, C.; Brockman, G.; Chan, B.; Cheung, V.;\nDebiak, P.; Dennison, C.; Farhi, D.; Fischer, Q.; Hashme, S.;\nHesse, C.; Józefowicz, R.; Gray, S.; Olsson, C.; Pachocki, J.;\nPetrov, M.; de Oliveira Pinto, H. P.; Raiman, J.; Salimans,\nT.; Schlatter, J.; Schneider, J.; Sidor, S.; Sutskever, I.; Tang,\nJ.; Wolski, F.; and Zhang, S. 2019. Dota 2 with Large Scale\nDeep Reinforcement Learning. arXiv preprint 1912.06680 .\nRadford, A.; Metz, L.; and Chintala, S. 2016. Unsupervised\nrepresentation learning with deep convolutional generative\nadversarial networks. In International Conference on Learn-\ning Representations.\nReddy, S.; Dragan, A. D.; and Levine, S. 2019. SQIL: Im-\nitation Learning via Reinforcement Learning with Sparse\nRewards. In International Conference on Learning Represen-\ntations.\nRisi, S.; and Togelius, J. 2019. Procedural content generation:\nfrom automatically generating game levels to increasing gen-\nerality in machine learning. arXiv preprint arXiv:1911.13071\n.\nRoa-Vicens, J.; Wang, Y.; Mison, V.; Gal, Y.; and Silva, R.\n2019. Adversarial recovery of agent rewards from latent\nspaces of the limit order book. In Advances in Neural Infor-\nmation Processing Systems.\nRoss, S.; Gordon, G.; and Bagnell, D. 2011. A reduction\nof imitation learning and structured prediction to no-regret\nonline learning. In International Conference on Artiﬁcial\nIntelligence and Statistics.\nSchulman, J.; Levine, S.; Abbeel, P.; Jordan, M.; and Moritz,\nP. 2015. Trust region policy optimization. In International\nConference on Machine Learning.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347 .\nSestini, A.; Kuhnle, A.; and Bagdanov, A. D. 2019. Deep-\nCrawl: Deep Reinforcement Learning for Turn Based Strat-\negy Games. In Proceedings of AIIDE Workshop on Experi-\nmental AI in Games.\nSyed, U.; and Schapire, R. E. 2008. A game-theoretic ap-\nproach to apprenticeship learning. In Advances in Neural\nInformation Processing Systems.\nTodorov, E.; Erez, T.; and Tassa, Y. 2012. Mujoco: A physics\nengine for model-based control. In International Conference\non Intelligent Robots and Systems.\nTucker, A.; Gleave, A.; and Russell, S. 2018. Inverse rein-\nforcement learning for video games. In Proceedings of NIPS\nWorkshop on Deep Reinforcement Learning.\nVinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.;\nDudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds, T.;\nGeorgiev, P.; et al. 2019. Grandmaster level in StarCraft II\nusing multi-agent reinforcement learning. Nature 575(7782):\n350–354.\nZiebart, B. D.; Maas, A.; Bagnell, J. A.; and Dey, A. K.\n2008. Maximum entropy inverse reinforcement learning. In\nNational Conference on Artiﬁcial intelligence.\nA\nImplementation details\nIn this section we give additional details on the network archi-\ntectures used for DE-AIRL on the Minigrid and DeepCrawl\nenvironments.\nNetwork Structures\nFu, Luo, and Levine use a multilayer perceptron for reward\nand policy models, however we use Convolutional Neural\nNetworks (CNNs) like Tucker, Gleave, and Russell. More-\nover, we use Proximal Policy Optimization (PPO) (Schulman\net al. 2017) instead of Trust-Region Policy Optimization\n(TRPO) (Schulman et al. 2015) as in the original paper.\n• Minigrid. The policy architecture consists of two branches.\nThe ﬁrst branch takes the global view of the 15 × 15 grid,\nand each tile is represented by a categorical value that\ndescribes the type of element in that tile. This input is fed\nto an embedding layer and then to a convolutional layer\nwith 3 × 3 ﬁlters and 32 channels. The second branch is\nlike the ﬁrst, but receives as input the 7 × 7 categorical\nlocal view of what the agent sees in front of it. The outputs\nof the convolutional layers are ﬂattened and concatenated\ntogether before being passed through a fully-connected\nlayer of size 256. The last layer is a fully connected layer\nof size 4 that represents the probability distribution over\nactions.\nThe reward model and the shaping term φω have the same\narchitecture. Unlike the policy network, they take only\nthe global categorical map and pass it through an embed-\nding layer, two convolutional layers with 3 × 3 ﬁlters and\n32 channels followed by a maxpool, and then two fully-\nconnected layers of size 32 and a ﬁnal fully-connected\nlayer with a single output. All other layers except the last\none use leaky-ReLu activations.\n• Potions and Maze. The convolutional structure of the pol-\nicy of the Potions and Maze tasks are the same of Sestini,\nKuhnle, and Bagdanov without the “property module” and\nthe LSTM layer. The reward model takes as input only the\nglobal view, then it is followed by a convolutional layer\nwith 1 × 1 ﬁlters and size 32, by two convolutional layers\nwith 3 × 3 ﬁlters and 32 ﬁlters, two fully-connected layers\nof size 32, and a ﬁnal fully-connected layer with a single\noutput and no activation. The shaping term φω shares the\nsame architecture. We used leaky ReLu instead of simple\nReLu as used in DCGAN (Radford, Metz, and Chintala\n2016).\n• Ranged Attacks. In this case the policy has the complete\nstructure of Sestini, Kuhnle, and Bagdanov without LSTM,\nand the reward model is the same of the previous tasks with\nthe addition of other two input branches that take as input\ntwo lists of properties of the agent and the enemy. Both\nare followed by embedding layers and two fully connected\nlayers of size 32. The resulting outputs are concatenated\ntogether with the ﬂattened result of the convolutional layer\nof the ﬁrst branch. This vector is then passed to the same\n3 fully connected layers of the potion task. The shaping\nterm shares the same architecture.\nTable 2: Hyper-parameters for all the tasks. Most of the values\nwere chosen after many preliminary experiments made with\ndifferent conﬁgurations\nParameter\nMinigrid\nPotions\nMaze\nRanged Attack\nlrpolicy\n5e−5\n5e−5\n5e−6\n5e−5\nlrreward\n5e−6\n5e−4\n5e−4\n5e−4\nlrbaseline\n5e−4\n5e−4\n5e−4\n5e−4\nentropy coefﬁcient\n0.5\n0.1\n0.1\n0.1\nexploration rate\n0.5\n0.2\n0.2\n0.2\nK\n3\n3\n5\n3\nγ\n0.9\n0.9\n0.9\n0.9\nmax timesteps\n30\n20\n20\n20\nstdreward\n0.05\n0.05\n0.05\n0.05\nHyperparameters\nIn table 2 we detail the hyperparameters used for all tasks for\nboth policy and reward optimization.\nB\nEffects of modiﬁcations to AIRL\nIn ﬁgure 6 we give an ablation study on both SeedEnv and\nProcEnv for the modiﬁcations to AIRL proposed in section 4\nof the main text. The plots show how the use of both reward\nstandardization and policy dataset expansion yield more sta-\nble and better results for the majority of the tasks on both the\nenvironment types.\nC\nAdditional experimental results\nIn ﬁgure 7 we summarize all the experimental results de-\nscribed in section 6 of the main text. Included are the per-\nformance of our demonstration-efﬁcient AIRL for all tasks,\nthe evolution of discriminator losses, and plots showing the\nimportance of using a disentangling reward function.\nSeedEnv\nProcEnv\nFigure 6: Ablation study of the modiﬁcations described in section 4 of the main text. The ﬁrst row represents training in a\nSeedEnv, while the last row represents training in a ProcEnv. For all the DeepCrawl tasks we used 20 seed levels and 20\ndemonstrations, while for Minigrid we used 40 seed levels and 40 demonstrations.\n(a)\n(b)\n(c)\nFigure 7: Summary of experimental results. Column (a): reward evolution in SeedEnv and ProcEnv with different numbers\nof seed levels, and naive AIRL on ProcEnv. Column (b): the evolution of the loss function. Column (c): the training of AIRL\nwithout the shaping term and GAIL, both in the SeedEnv with 20 seed levels for DeepCrawl and 40 seed levels for Minigrid. The\ndotted horizontal line refers to expert performance in the ProcEnv.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-12-04",
  "updated": "2020-12-04"
}