{
  "id": "http://arxiv.org/abs/2012.02825v2",
  "title": "A Survey on Deep Learning for Human Mobility",
  "authors": [
    "Massimiliano Luca",
    "Gianni Barlacchi",
    "Bruno Lepri",
    "Luca Pappalardo"
  ],
  "abstract": "The study of human mobility is crucial due to its impact on several aspects\nof our society, such as disease spreading, urban planning, well-being,\npollution, and more. The proliferation of digital mobility data, such as phone\nrecords, GPS traces, and social media posts, combined with the predictive power\nof artificial intelligence, triggered the application of deep learning to human\nmobility. Existing surveys focus on single tasks, data sources, mechanistic or\ntraditional machine learning approaches, while a comprehensive description of\ndeep learning solutions is missing. This survey provides a taxonomy of mobility\ntasks, a discussion on the challenges related to each task and how deep\nlearning may overcome the limitations of traditional models, a description of\nthe most relevant solutions to the mobility tasks described above and the\nrelevant challenges for the future. Our survey is a guide to the leading deep\nlearning solutions to next-location prediction, crowd flow prediction,\ntrajectory generation, and flow generation. At the same time, it helps deep\nlearning scientists and practitioners understand the fundamental concepts and\nthe open challenges of the study of human mobility.",
  "text": "A Survey on Deep Learning for Human Mobility\nMASSIMILIANO LUCA, Fondazione Bruno Kessler (FBK), Italy and Free University of Bolzano, Italy\nGIANNI BARLACCHIâˆ—, Amazon Alexa, Germany\nBRUNO LEPRI, Fondazione Bruno Kessler (FBK), Italy\nLUCA PAPPALARDO, Institute of Information Science and Technologies, National Research Council (ISTI-\nCNR), Italy\nThe study of human mobility is crucial due to its impact on several aspects of our society, such as disease spreading, urban\nplanning, well-being, pollution, and more. The proliferation of digital mobility data, such as phone records, GPS traces, and\nsocial media posts, combined with the predictive power of artificial intelligence, triggered the application of deep learning to\nhuman mobility. Existing surveys focus on single tasks, data sources, mechanistic or traditional machine learning approaches,\nwhile a comprehensive description of deep learning solutions is missing. This survey provides a taxonomy of mobility tasks,\na discussion on the challenges related to each task and how deep learning may overcome the limitations of traditional\nmodels, a description of the most relevant solutions to the mobility tasks described above and the relevant challenges for\nthe future. Our survey is a guide to the leading deep learning solutions to next-location prediction, crowd flow prediction,\ntrajectory generation, and flow generation. At the same time, it helps deep learning scientists and practitioners understand\nthe fundamental concepts and the open challenges of the study of human mobility.\nCCS Concepts: â€¢ Computing methodologies â†’Artificial intelligence; Machine learning; â€¢ Applied computing â†’\nTransportation.\nAdditional Key Words and Phrases: Human Mobility, Deep Learning, Datasets, Next-location Prediction, Crowd Flow Prediction,\nTrajectory Generation, Trajectory, Mobility Flows, Artificial Intelligence\nACM Reference Format:\nMassimiliano Luca, Gianni Barlacchi, Bruno Lepri, and Luca Pappalardo. 2021. A Survey on Deep Learning for Human Mobility.\n1, 1 (August 2021), 42 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\nâˆ—Work done before joining Amazon.\nAuthorsâ€™ addresses: Massimiliano Luca, Fondazione Bruno Kessler (FBK), Via Sommarive, 19, Povo - Trento, Italy, Free University of Bolzano,\nPiazza Domenicani, 3, Bolzano, Italy, mluca@fbk.eu; Gianni Barlacchi, Amazon Alexa, , Berlin, Germany, gianni.barlacchi@gmail.com; Bruno\nLepri, Fondazione Bruno Kessler (FBK), Via Sommarive, 19, Povo - Trento, Italy, ; Luca Pappalardo, Institute of Information Science and\nTechnologies, National Research Council (ISTI-CNR), Via G. Moruzzi 1, 56124, Pisa, Italy, luca.pappalardo@isti.cnr.it.\n2021. XXXX-XXXX/2021/8-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: August 2021.\narXiv:2012.02825v2  [cs.LG]  27 Jun 2021\n2\nâ€¢\nLuca et al.\n1\nINTRODUCTION\nUrban population is increasing strikingly and human mobility is becoming more complex and bulky, affecting\ncrucial aspects of people lives such as the spreading of viral diseases (e.g., the COVID-19 pandemic) [101, 105,\n127, 130, 141, 156], the behavior of people in case of natural disasters [83, 175, 199], the public and private\ntransportation and the resulting traffic volumes [31, 58, 94, 153], the well-being of citizens [137, 176, 192], the\nseverity of air pollution, energy and water consumption [19, 126, 178]. Furthermore, crowdsâ€™ movement between\ncities is influenced by migrations from rural to urban areas, such as those induced by natural disasters, climate\nchange, and conflicts [2, 70, 117, 144, 149, 166, 169].\nFortunately, policymakers are not unarmed in facing these challenges. The rise of ubiquitous computing (e.g.,\nmobile phone, the Internet of Things, social media platforms) provides an always up-to-date and precise way to\nsense human movements at various temporal and spatial scales. Examples of mobility data include tracks from\nGPS devices embedded in smartphones [21, 106, 123, 143, 232], vehicles [13, 60, 133, 136] or boats [29, 57, 151, 210];\nrecords produced by the communication between phones and the cellular network [18, 65]; and geotagged posts\nfrom social media platforms [17, 41, 88, 111, 146]. This deluge of digital data fostered a vast scientific production\non various aspects of human mobility, such as the mining of trajectory data [84, 119, 193, 226, 230], the uncovering\nof the statistical patterns [9, 18, 65, 173], and the estimation of the privacy risk [45, 59, 139, 140, 142, 155]. The\ndevelopment of powerful Artificial Intelligence (AI) techniques and the availability of big mobility data offered\nunprecedented opportunities for researchers to use Deep Learning (DL) approaches to solve mobility-related\nchallenges. In this survey, we focus on DL solutions to predict or generate human movements and exclude\nother approaches solving other problems, such as semantic enrichment of mobility data (e.g., predicting the\npurpose of movement) [150], home location detection [132], and population inference [46]. In particular, we\nfocus on two categories of tasks: predictive and generative (see Figure 1). We discuss two predictive tasks, namely\nnext-location prediction and crowd flow prediction, and two generative tasks, namely trajectory generation and\nflow generation.\nNext-location prediction is about forecasting which location an individual will visit given historical data about\ntheir mobility. It is crucial in many applications such as travel recommendation, location-aware advertisements\nand geomarketing, early warning of potential public emergencies, and recommendation of friends in social\nnetwork platforms [23, 203, 227, 228, 235]. Crowd flow prediction, instead, is the task of forecasting the incoming\nand outgoing flows of people on a geographic region, which has an impact on public safety, the definition of\non-demand services, the management of land use, and traffic optimization [50, 81, 164, 204, 219]. Concerning\ngenerative tasks, trajectory generation deals with generating synthetic trajectories that can reproduce, realistically,\nthe individual statistical patterns of human mobility [9, 74, 171, 194]. Flow generation deals with generating\nrealistic flows among locations, given their characteristics and the distance among them, and without any\nknowledge about the real flows. Although approaches based on Machine Learning (ML) achieve good results\nin solving these four tasks [23, 50, 164, 203, 204, 219, 228], multiple reasons pushed researchers to adopt DL\ntechniques, such as the ability to automatically extract relevant patterns from (un)structured and heterogeneous\ndata, and the outstanding results obtained in other fields (e.g., computer vision, natural language processing).\nSeveral survey papers that provide interesting perspectives on single mobility tasks [9, 50, 74, 100, 164, 165,\n171, 204], mobility data sources [207, 228], or traditional ML approaches [74, 185, 194]. A few surveys discuss DL\napproaches to spatio-temporal data mining [197], tasks related to the smart city ecosystem [32] or traffic-related\nissues [86, 200], covering some aspects related to human mobility but without a specific focus on the challenges\nand solutions to mobility-related tasks.\nIn this survey, we discuss DL solutions to next-location prediction, crowd flow prediction, trajectory generation\nand flow generation, organizing them into a proper taxonomy and discussing why those solutions may overcome\nlimitations of existing traditional models. In order to find relevant papers for these tasks, we searched on Scopus\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n3\nPREDICTION\nMOBILITY TASKS\nPredicting an individual's \nfuture whereabouts \n \nNEXT-LOCATION \nPREDICTION\nCROWD FLOW \nPREDICTION\nTRAJECTORY \nGENERATION\nFLOW \nGENERATION\n \nGENERATION\n \nGenerating realistic \nmobility data\nPredicting future mobility \nat individual or collectiveÂ level\nGeneratingÂ realistic Origin \nDestination matrices\nGeneratingÂ realistic individual \nspatio-temporalÂ trajectories\nPredictingÂ  in/outÂ  \naggregated crowd ï¬‚ows\nFig. 1. A taxonomy of the mobility tasks we discuss in this survey. We classify mobility tasks in predictive, aiming at\nforecasting future mobility at an individual or collective level (Section 3), and generative, aiming at generating realistic\ntrajectories of mobility flows (Section 4). Among the predictive tasks, we cover i) next-location prediction, the problem\nof forecasting future whereabouts given the mobility history of individuals (Section 3.1), and ii) crowd flow prediction,\nwhose goal is to forecasting future aggregated flows given historical observations (Section 3.2). On the other hand, we\nhave two generative tasks: i) trajectory generation aims at generating realistic individual trajectories (Section 4.1), and ii)\nflow generation whose goal is to generate realistic flows among locations on a geographic region (Section 4.2). We use this\ntaxonomy to map relevant works to the task they solve and shape this surveyâ€™s structure.\nthe following keywords: \"crowd flow\", \"next-location\", \"flow prediction\", \"flow generation\", \"trajectory generation\",\n\"mobility generation\", and \"mobility prediction\". Among the obtained results, we selected those using DL, which\nare commonly used in subsequent papers as baselines, which are seminal works for the task they address, and\nwhich add some novelty in terms of DL pipelines, how DL modules are combined and data handled. For each task,\nwe also describe the open mobility datasets used by the papers and the evaluation metrics typically adopted. In\nthe Appendix, we provide a more detailed description of deep learning modules (Appendix B), datasets (Appendix\nC), evaluation metrics (Appendix D), and mobility patternsâ€™ (Appendix E). In summary, this survey provides the\nreader with:\nâ€¢ An introduction to the fundamental concepts and nomenclature of human mobility (Section 2.1) and the\nkey ideas behind the DL modules (Sections 2.2).\nâ€¢ A taxonomy of tasks related to predicting and generating human movements, with a comprehensive dis-\ncussion on next-location prediction (Section 3.1), crowd flow prediction (Section 3.2), trajectory generation\n(Section 4.1), and flow generation (Section 4.2). For each task, we define the problem, discuss the DL modules\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n4\nâ€¢\nLuca et al.\nused in the literature to address it, we highlight the advantages of using DL over traditional models and list\nthe public datasets and evaluation metrics commonly used for each task.\nâ€¢ A discussion of the most interesting open challenges about the four tasks (Section 5). We also provide a\nGitHub repository (bit.ly/DL4HM) where researchers can co-operate to update the list of relevant mobility\ndatasets and papers.\nâ€¢ An Appendix in which we discuss the characteristics of DL modules, the data sources, the public datasets,\nthe evaluation metrics used in the selected papers, and the mobility patterns used to assess the realism of\ngenerative models.\nThe survey is structured as follows. In Section 2, we provide the definition of spatio-temporal trajectories\nand spatial aggregations (Section 2.1), and we briefly discuss the key DL modules used by the models tackling\nhuman mobility challenges (Section 2.2). A detailed discussion of such modules and a more detailed overview of\nhuman-mobility well-known laws and patterns are discussed in Appendix E.\nIn Sections 3 and 4, we explore, respectively, the predictive and generative tasks summarized in Figure 1. For\neach task, after discussing why it is a relevant problem, we formally define it. Then, after briefly presenting the\ntraditional mechanistic and/or ML-based approaches, we describe in detail the aspects that are not captured or\nonly partially captured by such models. Then, we highlight how DL-models overtake these limitations and which\nDL modules are commonly used to succeed. Finally, In Section 5, we discuss some of the open challenges and we\nderive some conclusions.\n2\nBACKGROUND\nHere we introduce the notation used in the remainder of the paper. In Section 2.1, we define key mobility concepts,\nand in Section 2.2 we briefly introduce the DL concepts and notation used in this survey.\n2.1\nSpatio-temporal trajectories and spatial aggregations\nMobility data describe the movements of a set of individuals during a period of observation. They are typically\ncollected through electronic devices and stored in the form of spatio-temporal trajectories or mobility flows.\nThe trajectory of an individual is a sequence of records that allows for reconstructing their movements during\nthe period of observation [230, 231]. Typically, each record contains the individualâ€™s identifier, a geographic\nlocation expressed as a spatial point, and a timestamp indicating when the individual went through that location.\nDefinition 2.1. Let ð‘¢be an individual, a trajectory ð‘‡ð‘¢= âŸ¨ð‘1, ð‘2, ..., ð‘ð‘›ð‘¢âŸ©is a time-ordered sequence composed by\nð‘›ð‘¢spatio-temporal points visited by ð‘¢. A spatio-temporal point is a pair ð‘= (ð‘¡,ð‘™), where ð‘¡indicates the time\nwhen point ð‘™= (ð‘¥,ð‘¦) is visited by ð‘¢, and ð‘¥and ð‘¦are spatial coordinates in a given reference system, e.g., latitude\nand longitude. A semantic spatio-temporal point ð‘is a tuple ð‘= (ð‘œ,ð‘¡,ð‘™), where ð‘¡indicates the time when point\nð‘™= (ð‘¥,ð‘¦) is visited by ð‘¢, ð‘™is a pair of coordinates (ð‘¥,ð‘¦), and ð‘œis a parameter that brings some meaning to the\npoint (e.g., home, workplace, or some other categories), if any.\nTypically, stay points (or stops) are detected on spatio-temporal trajectories to find locations in which users\nspend a minimum amount of time [135, 230]. In some tasks, the geographic space is discretized by mapping the\ncoordinates to a spatial tessellation, i.e., a covering of the bi-dimensional space using a countable number of\ngeometric shapes called tiles, with no overlaps and no gaps. For instance, for crowd flow prediction, a spatial\ntessellation is used to aggregate the flows of people moving among the tiles.\nDefinition 2.2. Given an area ð´, a set of geographical polygons called tessellation, G, is defined with the\nfollowing properties: (1) G contains a finite number of polygons, ð‘”ð‘–, called tiles, G = {ð‘”ð‘–: ð‘–= 1, ...,ð‘›}; (2) the tiles\nare non-overlapping, ð‘”ð‘–âˆ©ð‘”ð‘—= âˆ…, âˆ€ð‘–â‰ ð‘—; (3) the union of all tiles completely covers ð´, Ãð‘›\nð‘–=1 ð‘”ð‘–= ð´.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n5\nThe tiling of the geographic space aims at creating the covering of the entire area of interest using regular\ntiles, such as equilateral triangular, squared, quadrilateral, hexagonal tiles, or irregular tiles that define the shape\nof buildings, census cells, or administrative units. A spatial join can then be used to associate each trajectoryâ€™s\npoint or stay point with the tile that contains it. Since the tessellation has no overlapping tiles and no gaps, each\npoint is assigned only to one tile. Further details may be found in Appendix A.\n2.2\nDeep Learning Modules\nHere we introduce the DL notation used in the remainder of the paper. For a detailed description of the DL\nmodules introduced here, see Appendix B.\nFCs: Fully Connected networks (FCs) consist of a series of fully-connected layers, in which all the neurons are\nconnected to those in the next layer. FCs are universal approximators (i.e., can learn any representation function)\n[66]. In human mobility tasks, FC networks are commonly used to capture the impact on individual or collective\nmobility of external features and/or preferences (e.g., weather conditions, presence of public events).\nRNNs, LSTMs and GRUs: Recurrent Neural Networks (RNNs) [157] can efficiently deal with sequential data,\nand they are used to capture spatial and temporal patterns in mobility tasks. For example, RNNs are used in\nnext-location prediction to find periodic trajectories patterns, represented as stay point sequences. In crowd flow\nprediction, RNNs are used to capture the flowsâ€™ temporal patterns. However, RNNs suffer from the vanishing\ngradient problem [98] and cannot propagate information found at early steps, losing relevant information at the\nbeginning of a sequence when it is time to analyze its end [98]. Long-short-term Memory networks (LSTMs) [75]\nand Gated Recurrent Units (GRUs) [35] are two gate implementations that mitigate this problem. Appendix B.1\nprovides further details and references on RNNs, LSTMs and GRUs.\nAttention mechanisms: These mechanisms are based on the idea that, when dealing with a large amount of\ninformation, our brains focus on the most significant parts and consider all the others as background information.\nIn attention mechanisms, the information in input is scored according to the context (e.g., attention maps), and\nthe model focuses more on the information with high scores. In human mobility, attention is widely used for\nnext-location prediction and crowd flow prediction to capture user preferences and highlight relevant historical\npatterns, respectively. Further details can be found in Appendix B.3.\nCNNs: Similarly to the visual cortex [78, 79], Convolutional Neural Networks (CNNs) are made of neurons that\nreact only to certain stimuli in a restricted region of the visual field [103]. They are effective in computer vision\napplications such as object recognition [147, 168], image classification and segmentation [53, 103], movement\nor event recognition [188], and more [95]. In human mobility tasks, CNNs are widely used to capture spatial\npatterns in the data, especially in crowd flow prediction where the distribution of people on a geographic region\nis represented as an image. Additional information on CNNs can be found in Appendix B.2.\nGenerative Models: in human mobility, Variational AutoEncoders (VAEs) and Generative Adversarial Net-\nworks (GANs) are used to generate realistic trajectories (i.e., synthetic trajectories that realistically reproduce mo-\nbility patterns). VAEs transform input data (e.g., trajectories) from a high-dimensional space to a low-dimensional\nspace, encoding samples as a distribution [96]. GANs [67] set up a game between a generator (e.g., a neural\nnetwork) and a discriminator (e.g., a classifier). The generatorâ€™s goal is to generate realistic data to fool the\ndiscriminator, whose purpose is to classify real and fake data and provide feedback to the generator to improve\nthe realism of the generated data. Details about VAEs and GANs can be found in Appendix B.4.\n3\nPREDICTIVE MODELS\nThe goal of predictive models for human mobility is to forecast future whereabouts, either at the individual or\ncollective level. At the individual level, next-location predictors forecast an individualâ€™s future whereabouts,\nbased on their historical observations (Section 3.1). At the collective level, crowd flow predictors forecast the\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n6\nâ€¢\nLuca et al.\namount of people moving from or to geographic locations given historical information about aggregated crowd\nflows (Section 3.2).\nIn this Section, we describe both tasks, discussing how DL brings significant improvements with respect to\ntraditional approaches, and describing the relevant state-of-the-art solutions to each task, with a reference to the\npublic datasets and the metrics used for training and testing the models.\n3.1\nNext-Location Prediction\nPredicting individualsâ€™ future locations is relevant in multiple applications such as monitoring public health\n[11, 26], well-being [137, 192], and traffic congestions [164], and to improve travel recommendation, geomarketing,\nand link prediction in social network platforms [23, 203, 227, 228, 235]. Next-location predictors may help\npolicymakers organize the public transportation network, urban planners decide a cityâ€™s future developments,\nand transportation companies provide citizens with a better service in terms of traffic reduction and ease of\nmobility. Predicting an individualâ€™s next location is challenging because it requires capturing the spatial and\ntemporal patterns that characterize human habits [9], and combining heterogeneous data sources to model\nmultiple factors influencing human displacements (e.g., weather, transportation mode, presence of POIs).\nProblem Definition. Next-location prediction consists of forecasting the next location (stay point) an indi-\nvidual will visit in the future, given their historical mobility data. Formally, let ð‘¢be a user, ð‘‡ð‘¢their trajectory, and\nð‘ð‘¡âˆˆð‘‡ð‘¢ð‘¢â€™s current location, next-location prediction aims at predicting ð‘¢â€™s next destination ð‘ð‘¡+1. This problem\nis treated in two ways: (i) as a multi-class classification task, in which we have as many classes as locations\nand we aim at predicting the next visited location ð‘ð‘¡+1; or (ii) as a regression task, predicting ð‘ð‘¡+1 = (ð‘¥ð‘¡+1,ð‘¦ð‘¡+1),\nwhere ð‘¥ð‘¡+1 and ð‘¦ð‘¡+1 are the next locationâ€™s geographic coordinates. A variant of next-location prediction aims at\nforecasting the next Point Of Interest (POI) ð‘ð‘¡+1 an individual ð‘¢will visit given their trajectory ð‘‡ð‘¢. Regardless of\nthe specific definition, next-location predictors output a ranking of the probability of each location to be ð‘¢â€™s next\ndestination.\nDL vs Traditional approaches. Next-location prediction has been widely explored prior to the DL explosion\nusing probabilistic or pattern-based approaches, which can work with a reasonably small amount of data [23, 228].\nIn a seminal work, Calabrese et al. [25] propose a probabilistic model combining peopleâ€™s trajectories and\ngeographical features such as land use, POIs, and distance of trips. Ashbrook et al. [6] cluster GPS data into\nmeaningful locations and incorporate them into a Markov model to predict individualsâ€™ future movements. Gambs\net al. [62] introduce a Mobility Markov Chain (MMC) in which states represent POIs and transitions between\nstates correspond to a movement between two POIs [62, 63]. Among the pattern-based approaches, Monreale et\nal. [121] develop a trajectory pattern mining algorithm to represent movement patterns as sequences of regions\nfrequently visited with a typical travel time. Although traditional approaches achieve good performance with a\nsmall amount of data, they have substantial limitations. Notably, they require a considerable effort in feature\nengineering and cannot capture long-range temporal and spatial dependencies [159].\nNext-location predictors should capture the spatial, temporal and social-geographic dimensions of human\nmobility (Figure 2). Regarding the spatial and temporal dimensions, predictors must capture at the same time\nspatial and temporal regularities hidden in human movements, as well as the tendency of people to get out of the\nroutine. At the same time, predictors should capture the impact of external factors and individual preferences on\nthe decision to move (e.g., weather conditions, preference for certain POIs, influence of friendships). Traditional\napproaches can only partially capture these aspects, and they particularly struggle in capturing complex sequential\npatterns in the data. DL approaches overtake these issues by using mechanisms such as RNNs, LSTMs, GRUs,\nFCs, attention mechanisms, and CNNs to capture temporal, spatial, and social-geographic patterns in the data.\nDatasets and Evaluation Metrics. Next-location predictors are mainly trained and tested on public datasets\nof check-ins, generally coming from geosocial network platforms (e.g., Gowalla, Foursquare), and GPS traces,\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n7\nNEXT-LOCATION\nPREDICTION\nIndividual Spatial\nPatterns\nExternal Factors (e.g.,\nweather conditions)\nIndividual Temporal\nPatterns\nUser Preferences (e.g.,\nPOIs, friendships)\nGeographic\nRepresentation\nSpatial Dimension\nTemporal Dimension\nOther Dimensions\nRegular Mobility\nOut-of-Routine\nMobility\nRegular Mobility\nOut-of-Routine\nMobility\nSpatial Dimension\nFC\n[1, 182, 44]\nRNN\n[214, 114, 49]\nLSTM\n[182, 153, 64, 99, 214]\nGRU\n[33, 54]\nCNN\n[8, 64, 118]\nAttention\n[1, 214, 153, 64]\nTemporal Dimension\nFC\n[1, 182, 44]\nRNN\n[214, 114, 49]\nLSTM\n[182, 153, 64, 99, 214, 8]\nGRU\n[33, 54]\nCNN\n[64, 118]\nAttention\n[1, 214, 153, 64]\nOther Dimensions \nFC\n[1, 182, 33, 49, 118]\nFig. 2. a) The aspects next-location predictors should capture regarding the spatial dimension (red), the temporal dimension\n(blue), and the social and geographic dimensions (yellow) of human mobility data. b) DL modules that allow to capture each\ndimension, with the reference to the selected papers in the literature that implement these modules.\ncollected with smartphones or vehicles on-board GPS devices. Examples of check-insâ€™ datasets widely used for\nnext-location prediction are Cho et al. [34], Feng et al. [54], and Yang et al. [213].\nExamples of commonly used trajectory datasets are taxi traces collected in Porto, Portugal [123] and San\nFrancisco, USA [143]. We refer to Appendix C.2 and C.3 for details on the check-in and trajectory datasets\nmentioned above.\nWhenever next-location prediction is intended as a regression task, predictors are evaluated using the Haversine\ndistance or the equirectangular distance between the actual location and the predicted one (see Appendix D.1 for\ndetails). When next-location prediction is intended as a multi-class classification task, predictors are evaluated\nwith accuracy (ACC/ACC@k), recall (Rec@k), F1-score (F1@k), Mean Average Percentage Error (MAPE) and/or\nArea Under the Curve (see Appendix D.2 for details).\nDL approaches. Table 1 contains the selected DL approaches to next-location prediction, with the corre-\nsponding DL modules, datasets and evaluation metrics used, and link to an implementation (if available).\nDe BrÃ©bisson et al. [44] use an FC to predict taxiâ€™s passenger drop-off locations. The input data consist\nof trajectories represented as a variable-length sequence of GPS points and other meta-information, such as\ndeparture time, driver identity, and client information. The model performance is evaluated on the dataset of\ntaxis in Porto [123] in terms of equirectangular distance to the actual visited location. However, it cannot take\ninto consideration the temporal dimension of the mobility data.\nST-RNN (Spatial Temporal Recurrent Neural Networks) [114] overtakes this issue by extending RNN with time-\nand spatial-specific transition matrices. Each RNNâ€™s layer learns an upper and lower bound for the temporal and\nspatial matrices through linear interpolation. The model is evaluated on the datasets of Gowalla [34] and the\nGlobal Terrorism Dataset (GTD) [177] using F1-score, Rec@k, MAPE and AUC.\nDeepMove [54] is an attentional recurrent network for mobility prediction from lengthy and sparse trajectories.\nFirst, historical and current trajectories are passed to a multi-modal embedding module to create a dense\nrepresentation of the spatio-temporal and individual-specific features. Historical trajectories are handled by an\nattention mechanism to extract mobility patterns, while a GRU handles current trajectories. The output of the\nmulti-modal embedding, the GRU, and the attention mechanism are concatenated and passed to an FC to predict\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n8\nâ€¢\nLuca et al.\nReference\nName\nYear\nDL Modules\nEvaluation\nDataset\nCode (https://bit.ly)\nNext-Location Prediction\nAbideen et al.[1]\nDWSTTN\n2021\nEncoder, Decoder, Attention, FC\nDistance\n[123]\n-\nTang et al.[182]\nCLNN\n2021\nLSTM, Embedding, FC\nDistance\n[123]\n-\nBao et al.[8]\nBiLSTM-CNN\n2020\nEmbedding, BiLSTM, CNN\nACC@k\n-\nChen et al.[33]\nDeepJMT\n2020\nGRU, FC, Encoder\nACC@k\n[213]\n-\nYang et al.[212]\nFlashback\n2020\nAttention, RNN\nACC@k\n[34]\nFlashback-1\nEbel et al.[49]\n-\n2020\nRNN, FC, Embedding\nDistance\n[123, 143]\n-\nRossi et al.[153]\n-\n2019\nAttention, LSTM\nDistance\n[123, 143, 184]\n-\nGao et al.[64]\nVANext\n2019\nCNN, GRU, Attention\nACC@k\n[34]\n-\nKong et al.[99]\nHST-LSTM\n2018\nLSTM\nACC\n-\nHST-LSTM\nLv et al.[118]\nT-CONV\n2018\nCNN, FC\nDistance\n[123]\nT-CONV\nFeng et al.[54]\nDeepMove\n2018\nAttention, GRU, FC\nACC\n[54]\nDeepMove\nYao et al.[214]\nSERM\n2017\nLSTM\nACC@k\n-\nSERM-Repo\nLiu et al.[114]\nST-RNN\n2016\nRNN\nRec@k, F1@k, MAPE, AUC\n[34, 177]\nSTRNN\nDe BrÃ©bisson et al.[44]\n-\n2015\nFC\nDistance\n[123]\nnext-loc-1\nTable 1. List of the selected papers tackling next-location prediction. For each paper, we describe the name of the corresponding\nproposed model (if any), the year of publication, the DL modules used in the proposed solution, the metrics used for\nperformance evaluation, the link to the public datasets used for training and testing the model, and the link to a repository\nwith the code (if any). The papers are sorted by year of publication in decreasing order.\nan individualâ€™s next-location. DeepMove is evaluated, using ACC@k, on Foursquare data [54], private mobile\nphone data, and a private dataset from a popular Chinese social network platform.\nHST-LSTM (Hierarchical Spatial-Temporal LSTM) [99] aims at predicting an individualâ€™s short-term next-\nlocation. First, the authors design an ST-LSTM (Spatial Temporal LSTM), which combines the spatial and temporal\ncharacteristics of a trajectory using an LSTM. Then, ST-LSTM is extended into HST-LSTM, which models periodic\npatterns using an encoder-decoder module. The encoder encodes the locations visited by a user in a given\ntime span and area of interest, while the decoder predicts the possible areas of interest the user will visit next.\nHST-LSTM is evaluated using ACC on private data from Baidu.\nT-Conv [118] treats trajectories as images and handle them using CNNs to capture the spatial patterns at\ndifferent scales. The output of the CNN, the trajectoryâ€™s starting date-time, and other personal information about\nthe user are passed to an FC that handles the prediction. T-Conv is evaluated using the datasets of taxis in Porto\n[123] and the Haversine distance.\nRossi et al. [153] propose an LSTM network equipped with a self-attention module to predict the coordinates\nof a taxiâ€™s next drop-off location. Locations are enriched with geographical data to describe the surrounding area\nof the location semantically. The model is tested using the Haversine distance and on the datasets of taxis in\nPorto [123], New York City [184], and San Francisco [143].\nAnother strand of research focuses on predicting the next POI an individual will visit using semantic trajectories.\nFor example, SERM (Semantics-Enriched Recurrent Model) [214] relies on an embedding layer to represent the\ntimestamp, the location, and the keywords of a social media post concisely. Both the userâ€™s trajectory and the\nembedding are fed into an LSTM responsible for predicting the next POI. SERM is evaluated using ACC@k on\nFoursquare check-ins in New York City [222] and tweets in Los Angeles [223].\nIn VANext (Variational Attention based Next Location) [64], the historical trajectories and the current one are\nembedded using two separate causal encoders to represent the semantic relationships among POIs. The encoded\nhistorical trajectories are passed to a CNN; the encoded current trajectory is passed to a GRU. The outputs of the\nCNN and the GRU are passed to an attention mechanism, which detects the most similar historical trajectory to\nthe current one and passes it to an FC that predicts the individualâ€™s next POI. VANext is evaluated using ACC@k\non the datasets of Gowalla [34] and Foursquare for Singapore and New York City.\nFlashback [212] is based on an RNN and the concept of flashback, a technique that uses a sparse semantic\ntrajectory to predict the next POI by looking for similar trajectories in terms of temporal characteristics. Flashback\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n9\nalso uses an embedding to model the preferences of individuals to visit specific POIs. The outputs of the RNN\nand the embedding are passed to an FC that predicts the next visited POI. Flashback is evaluated using ACC@k\non check-ins from Gowalla [34] and Foursquare.\nDeepJMT (Deep Model for Joint Mobility and Time) [33] can predict an individualâ€™s next POI as well as when\nthey will visit it. The model is based on four pipelines: a sequential dependency encoder, a spatial context encoder,\na periodicity context extractor, and a social-temporal context extractor. The sequential dependency encoder is\na hierarchical GRU that takes as input an embedding of a userâ€™s trajectories. The high-level GRU captures the\ntransitions between trajectories; while the low-level GRU models the transition within a trajectory. The spatial\ncontext extractor determines the dynamic influence of spatial neighbors, modeled as a graph in which nearest\npoints influence the final prediction. The periodicity context extractor is an attentional GRU aiming at extracting\nperiodicity patterns from an individualâ€™s historical trajectories. The social-temporal context extractor leverages\nsocial relationships using an FC and pooling functions to facilitate both next-POI and time prediction. Finally, the\noutputs of the four modules are concatenated to generate the prediction. DeepJMT is evaluated using ACC@k on\nFoursquare check-ins in New York City [213], Tokyo [213] and Istanbul.\nEbel et al. [49] propose a model to predict a vehicleâ€™s destinations and routes, given a partial trajectory and\ncontextual data (e.g., day, time, weather). First, the area is tessellated and GPS points are assigned to these tiles\nusing a k-d tree-based space partitioning method. The model is based on two main modules. The first module\nis an RNN that takes as input the mapped trajectory; while the second module is an FC that takes as input the\nembedded contextual data. The two modulesâ€™ outputs are merged and passed to an additional FC that produces\nthe individualâ€™s probabilities to end a trip into a specific tile. The model is evaluated with the mean Haversine\ndistance and the distance to the actual arrival point on taxisâ€™ traces in Porto [123] and San Francisco [143].\nBiLSTM-CNN [8] relies on a spatial clustering algorithm to derive areas of interest from POIs and uses bi-LSTMs\nand CNNs to predict the next area an individual will visit. The historical mobility data of people are passed into a\nbi-directional LSTM and then to a CNN to capture the overall spatial and temporal patterns. The CNNâ€™s output\nis then passed to an FC handling the prediction of the next location. The model is evaluated with ACC@1 and\nACC@5 on a private dataset of Weibo check-ins collected in Wuhan, China.\nCLNN (Classification Learning Neural Network) [182] relies on an information extraction module that extracts\ncoordinates, date, time and driver characteristics, POIs and historical information from mobility data (e.g., similar\ntrajectories). An LSTM is used to process the coordinates. Date, time, driver characteristics and POIs are embedded\ninto a dense representation. Two FCs process the embedded and historical information, respectively. The outputs\nof the two FCs and the LSTM are fused together with a weighted sum, whose output is fed into an FC to predict\nthe coordinates of the next taxiâ€™s destination. The model is evaluated using the mean Haversine distance on the\ndataset of taxis in Porto [123].\nDWSTTN (Deep Wide Spatio Temporal Transformer Network) [1] is a transformer-like architecture that\npredicts a taxiâ€™s next destination using historical pick-up and drop-off observations and taxisâ€™ features and\npreferences. The model consists of two identical parts, the encoder and the decoder. The encoder is responsible\nfor the learning phase, the decoder for predicting the coordinates of the taxiâ€™s next location. In both, information\nabout a taxi (e.g., identifier and stand identifier) and temporal information (e.g., weekday, hour, day type) are\npassed to an embedding layer (temporal transformer), while information about locations is embedded by a spatial\ntransformer. The outputs of the temporal and spatial transformers are passed to two attention mechanisms to\nextract further relevant spatial and temporal information. The outputs of the attention mechanisms are fused\ntogether using a weighted sum, whose output is passed to an FC. In the encoder, the FC outputs the learned\nfeaturesâ€™ representation; in the decoder, the FC outputs the next locationâ€™s coordinates. The model is evaluated\nusing the Haversine distance on the datasets of taxis in Porto [123] and New York [184].\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n10\nâ€¢\nLuca et al.\n3.2\nCrowd Flow Prediction\nCrowd flow prediction is the problem of forecasting the incoming and outgoing flows of locations in a geographic\nregion, usually split into tiles on a spatial tessellation [204, 231]. It is a crucial problem given its impact on several\naspects of society, from public safety [231] to the definition of on-demand services [234], the management of\nland use [81], and traffic optimization [231]. For example, crowd flow predictors may help city managers and\npolicymakers discover the traffic congestions in the city; people in business find potential areas of business\ninvestment; citizens improve travel plans and stagger the peaks of travel. These predictors may also help prevent\nor mitigate dangerous situations, such as creating massive crowds of people streamed into a strip region, by\nsending out warnings or evacuating people in advance.\nCrowd flow prediction is challenging because it requires dealing with both spatial and temporal dependencies.\nIndeed, a regionâ€™s out-flow may affect the in-flows of both near and far regions. At the same time, crowd flows\nare characterized by temporal closeness, trends, and periodicity. Temporal closeness marks the dependencies\nbetween events that are close in time; trends highlight patterns that repeat over time (e.g., weekends and working\ndays); periodicity captures the repetitive nature of relevant events (e.g., rush hours in the morning). Furthermore,\nexogenous factors such as weather conditions, holidays, and the presence of public city events may affect crowd\nflow patterns.\nProblem Definition. Given an individualâ€™s trajectory ð‘‡ð‘¢and a spatial tessellation G of the geographic space\n(generally a ð‘–Ã— ð‘—grid), the set of locations (tiles) the trajectory intersects in a time interval Î”ð‘¡is:\nð‘žð‘¡\nð‘‡ð‘¢= {(ð‘ð‘˜â†’ð‘¡) âˆˆÎ”ð‘¡âˆ§(ð‘ð‘˜â†’(ð‘¥,ð‘¦)) âˆˆ(ð‘–, ð‘—)|(ð‘–, ð‘—)},\n(1)\nwhere (ð‘–, ð‘—) indicates a location on G and ð‘ð‘˜is the user ð‘¢â€™s current location, identified by the coordinates (ð‘¥,ð‘¦).\nLet ð‘„be the set of locations covered by all the individual trajectories and let ð‘¡âˆ’1, ð‘¡, and ð‘¡+ 1 be three consecutive\ntime spans, the incoming flow in(ð‘–,ð‘—)\nð‘¡\nto a location (ð‘–, ð‘—) is the number of the individuals that are in (ð‘–, ð‘—) at time ð‘¡\nbut were not in (ð‘–, ð‘—) at time ð‘¡âˆ’1. Similarly, the outgoing flow out(ð‘–,ð‘—)\nð‘¡\nto location (ð‘–, ð‘—) is the number of individuals\nthat are in (ð‘–, ð‘—) at time ð‘¡and move to another location at time ð‘¡+ 1:\nin(ð‘–,ð‘—)\nð‘¡\n=\nâˆ‘ï¸\nð‘‡âˆˆð‘„\n|{ð‘¡> 1|(ð‘–, ð‘—) âˆ‰ð‘žð‘¡âˆ’1\nð‘‡\nâˆ§(ð‘–, ð‘—) âˆˆð‘žð‘¡\nð‘‡}|;\nout(ð‘–,ð‘—)\nð‘¡\n=\nâˆ‘ï¸\nð‘‡âˆˆð‘„\n|{ð‘¡> 1|(ð‘–, ð‘—) âˆˆð‘žð‘¡\nð‘‡âˆ§(ð‘–, ð‘—) âˆ‰ð‘žð‘¡+1\nð‘‡\n}|.\n(2)\nWe can represent the flows of a region as a tensor ð‘‹ð‘¡âˆˆð‘…2Ã—ð¼Ã—ð½, where one dimension is associated with the\nin-flow (ð‘‹ð‘¡)1,ð‘–,ð‘—= in(ð‘–,ð‘—)\nð‘¡\nand the other with the out-flow (ð‘‹ð‘¡)2,ð‘–,ð‘—= out(ð‘–,ð‘—)\nð‘¡\n. Therefore, crowd flow prediction is\nthe task of predicting ð‘‹ð‘¡+Î” given the historical flows {ð‘‹ð‘¡|1, . . . ,ð‘‹ð‘¡|ð‘¡}. In most of the selected papers, Î” = 1. When\nÎ” > 1 (e.g., in [196, 233]), the problem is named multi-step crowd flow prediction.\nA variant of crowd flow prediction aims at forecasting the entire origin-destination matrix (i.e., flows among\npairs of locations) given the historical observations of crowd flows [152].\nDL vs Traditional approaches. Crowd flow prediction may be tackled using classic time-series prediction\nmodels based on autoregression (AR), such as the AutoRegressive Moving Average (ARMA) [20], the Autore-\ngressive Integrated Moving Average (ARIMA) [107, 122], and variants like the Stationary and Seasonal ARIMA\n(SARIMA) [201], vector ARMA [89] and space-time ARIMA [90]. Since ARMA and ARIMA can make predictions\nonly out of stationary time-series that do not statistically change over time, they cannot accurately predict\nnew events. SARIMA, vector ARMA and space-time ARMA are designed to overtake this assumption, but they\npresent other issues. For instance, SARIMAâ€™s predictive performance decreases when dealing with patterns\nwith long seasonal periods. In general, the AR predictors have difficulties handling spatial dependencies and\nshort-term samples, capturing patterns way before in the time series, and including additional features (e.g.,\nweather conditions, presence of public city events), making them ineffective for crowd flow prediction.\nIn contrast, DL approaches can effectively model spatial and temporal dependencies and capture the influence\nof external factors (see Figure 3). Since spatial tessellations describe a bidimensional space, DL predictors represent\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n11\nCROWD FLOW \nPREDICTION\nSpatial Patterns\nExternal Factors (e.g.,\nweather conditions)\nTemporal Patterns\nUser Preferences (e.g.,\nPOIs, friendships)\nSpatial Dimension\nTemporal Dimension\nOther Dimensions\nClose (e.g., inï¬‚uence \nof near-by areas)\nDistant (e.g, inï¬‚uence\nof distant areas)\nPeriodic (e.g, patterns\nof previous weeks)\nTrends (e.g., patterns\nfrom previous month)\nRecent (e.g. patterns\nfrom previous hours)\nSpatial Dimension\nCNN\n[43, 196, 211, 148, 183, 221, 124,\n112, 109, 47, 215, 87, 224]\nConvLSTM \n[82, 113, 124, 3]\nConvGRU\n[237]\nAttention \n[85, 43, 196, 221, 233, 109, 215]\nGCN\n[180]\nFC\n[237]\nTemporal Dimension\nCNN\n[43, 211, 124, 112, 237, 224]\nLSTM\n[196, 148, 109, 47, 215, 87]\nConvLSTM\n[82, 183, 113, 124, 3]\nConvGRU\n[237]\nAttention \n[82, 43, 196, 183, 237, 109, 215]\nGCN\n[180]\nFC\n[237]\nOther Dimensions \nFC\n[82, 43, 211, 148, 221, 124, 109, \n47, 87, 237, 224] \nCNN\n[196, 112]\na)\n           \nb)\nFig. 3. a) The aspects crowd flow predictors should capture regarding the spatial dimension (red), the temporal dimension\n(blue), and the social and geographic dimensions (yellow) of human mobility data. b) DL modules that allow capturing each\ndimension, with reference to the selected papers in the literature that implement these modules.\ncrowd flows as matrices and exploit the effectiveness of CNNs in detecting near and far away spatial and temporal\ndependencies in matrices. At the same time, since gated RNNs such as LSTMs and GRUs may be used to model\ncomplex sequential patterns in the data, DL approaches may efficiently capture patterns in the temporal evolution\nof crowd flows. CNNs and RNNs may be combined in ConvLSTM models, which may capture spatial and temporal\npatterns at the same time. Finally, we should also consider other aspects that may affect crowd flows, such as\nweather conditions and the presence of public events in the city. Such external factors may be handled with FCs\nand combined with the output of the CNN/RNN modules.\nDatasets and Evaluation metrics. The most used datasets to train and evaluate crowd flow predictors are\nthe Citi Bike dataset [14], which describes the trips between bike-sharing stations in New York City, and Zhang\net al.â€™s dataset [224], which provides the aggregated incoming and outgoing flows for each tile of a squared\ntessellation of New York City and Beijing, extracted from raw taxisâ€™ GPS traces. Some papers use a dataset about\nthe bike-sharing system of Washington D.C. [15], and a dataset describing the pick-up and drop-off locations of\ntaxis in New York City [184]. We refer to Appendix C.2 and C.3 for details about these datasets.\nThe performance of crowd flow predictors is evaluated as the error between the empirical crowd flows and the\npredicted ones. Commonly used error metrics are Mean Absolute Error (MAE), Root Mean Squared Error (RMSE),\nand Mean Absolute Percent Error (MAPE) (see Appendix D.3 for definitions and details).\nDL approaches.\nTable 2 shows the selected papers on crowd flow prediction, highlighting the DL modules, evaluation metrics,\nand datasets they use, with a link to an implementation (if available).\nIn their seminal work, Zhang et al. propose ST-ResNet [224], which consists of three modules that rely on\nCNNs to capture trends, periodic patterns, and temporal closeness. The modulesâ€™ output is combined with the\noutput of an FC that deals with external factors such as weather conditions and presence of public events. The\nmodel is evaluated on the datasets of taxis in Beijing and bikes in New York City [224] using RMSE.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n12\nâ€¢\nLuca et al.\nReference\nName\nYear\nDL Modules\nEvaluation\nDataset\nCode (https://bit.ly)\nCrowd Flow Prediction\nJiang et al.[82]\nDeepCrowd\n2021\nConvLSTM, Attention, FC\nMSE, MAE, MAPE, RMSE\n[82]\nDeepCrowd\nDai et al.[43]\n-\n2021\nAttention, CNN, FC\nRMSE\n[224]\n-\nWang et al.[196]\nSeqST\n2020\nLSTM, CNN, Attention\nMAE, RMSE\n[14, 184]\n-\nYang et al.[211]\nST-ESNet\n2020\nCNN, FC\nRMSE\n[224]\n-\nRen et al.[148]\nHIDLST\n2020\nLSTM, CNN, FC\nRMSE\n[224]\n-\nTian et al.[183]\nLDRSN\n2020\nCNN, ConvLSTM, Attention\nRMSE, MAPE, MAE\n[14, 184]\n-\nYuan et al.[221]\nMV-RANet\n2020\nCNN, Attention, FC\nRMSE, MAPE\n[224]\n-\nLiu et al.[113]\nATFM\n2020\nConvLSTM\nRMSE\n[224]\nATFM-2\nSun et al.[180]\nMVGCN\n2020\nGCN, FC\nRMSE, MAE\n[14, 15, 184, 224]\n-\nMourad et al.[124]\nASTIR\n2019\nConvLSTM, CNN, FC\nRMSE\n[224]\nASTIR_Model\nZhou et al.[233]\nST-Attn\n2019\nAttention, FC\nRMSE\n[14, 224]\nST-Attn\nRong et al. [152]\n2019\nCNN, ResNet\nRMSE, MAE\n-\n-\nLi et al.[109]\nST-DCCNAL\n2019\nCNN, Attention, LSTM, FC\nRMSE\n[224]\nST-DCCNAL\nLin et al.[112]\nDeepSTN+\n2019\nCNN\nRMSE, MAE\n[14]\nDeepSTN\nDu et al.[47]\nDST-ICRL\n2019\nCNN, LSTM, FC\nRMSE, MAE\n[224]\nDST-ICRL\nAi et al.[3]\n-\n2018\nConvLSTM\nRMSE, MAE\n-\n-\nYao et al.[215]\nSTDN\n2018\nCNN, LSTM, Attention, FC\nRMSE, MAPE\n[14]\nSTDN-2\nJin et al. [87]\nSTRCN\n2018\nCNN, LSTM, FC\nRMSE\n[224]\n-\nZonoozi et al.[237]\nPCRN\n2018\nConvGRU, CNN, FC\nRMSE\n[224]\n-\nZhang et al.[224]\nST-ResNet\n2017\nCNN, FC\nRMSE\n[224]\nST-ResNet\nTable 2. List of selected papers tackling crowd flow prediction. For each paper, we describe the name of the corresponding\nproposed model (if any), the year of publication, the DL modules used in the proposed solution, the metrics used for\nperformance evaluation, the link to the public datasets used for training and testing the model, and the link to a repository\nwith the code (if any). The papers are sorted by year of publication in decreasing order.\nST-ESNet [211] extends STRes-Net [224] by adding convolutional layers to upsample and downsample the\nmatrices representing crowd flows. The model is evaluated with the datasets of taxis in Beijing [224] using RMSE.\nMany works in the literature combine CNNs with RNNs to exploit the latterâ€™s capability to deal with temporal\npatterns. STRCN (Spatio-Temporal Recurrent Convolutional Network) [87] uses three CNNs to capture close,\nshort-term (daily influence), and mid-term (the difference between workdays and weekends) spatial patterns.\nThe output of the CNNs is fed into three LSTMs, which handle temporal dynamics. STRCN also uses external\nfeatures, such as weather conditions and features to distinguish between workdays and holidays, through an FC.\nThe output of the FC and the LSTMs are combined. Modelâ€™s performances are evaluated on the datasets of taxis\nin Beijing and bikes in New York City [224] using RMSE.\nSTDN (Spatial-Temporal Dynamic Network) [215] consists of two CNNs: the first one captures the local spatial\ndependencies based on the similarities of historical traffic volumes to uncover the flows; the second CNN captures\nthe traffic flows. The outputs of the two CNNs are pairwise multiplied and summed with an FCâ€™s output that\nhandles external features (e.g., weather events). Moreover, STDN uses three LSTMs to capture the temporal\ndependencies of historical data describing the crowd flows. CNNs and LSTMs work on two separate pipelines,\nthe outputs of which are summed together and forwarded to another LSTM with an attention mechanism that\nanalyzes the temporal dynamics of the current day. Finally, the output of this latter LSTM is passed to an additional\nFC to perform crowd flow and traffic prediction. The model performance is evaluated on the New York Cityâ€™s\nbikes dataset [14] and on a dataset of taxisâ€™ flows in New York City [184] using MAPE.\nDST-ICRL (Deep Spatio-Temporal with Irregular Convolutional Residual Network) [47] combines convolutional\nresidual units with LSTMs to capture the irregular properties of traffic flows in different transportation lines.\nSimilarly to STDN [215] and ST-RESNET [224], DST-ICRL uses three pipelines to uncover daily, weekly, and\nrecent spatio-temporal dynamics and handles external features using an FC. The authors evaluate DST-ICRL on\ntwo private datasets of checkins from e-cards on buses and subways in Beijing, using MAE and RMSE.\nAnother category of crowd flow predictors exploits attention mechanisms. ST-DCCNAL (Spatio-Temporal\nDensely Connected Convolutional Networks and Attention LSTM) [109] combines a CNN with an attentional\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n13\nLSTM to simplify the selection of the inputs. The authors use DenseNet [77] to cope with the spatial patterns and\nFCs to deal with the external features. The outputs of DenseNet and the FCs are fed into an attentional LSTM to\nextract temporal patterns and make the prediction. The performances are evaluated on the datasets of taxis in\nBeijing and bikes in New York City [224] using RMSE.\nMV-RANet (Multi-View Residual Attention Network) [221] uses two pipelines to deal with spatio-temporal\npatterns and mobility patterns. The first pipeline models the closeness, the periods, and the trends of crowd\nflows using three attention residual networks, the outputs of which are fused and passed to a convolutional\nlayer. The output of the convolutional layer is then passed to an FC. The second pipeline captures the mobility\npatterns in the data, generating three graphs: one for the transition probability, one for the transition distance,\nand one that mimics the flow patterns. The three graphs are then encoded using Node2Vec [71], an algorithmic\nframework that learns a continuous feature representation of the nodes of a graph, and passed to an FC to extract\nrelevant mobility patterns. The outputs of the FC handling the graph representation and the FC that handles\nthe spatio-temporal patterns are fed into an additional FC responsible for making the prediction. MV-RANet is\nevaluated using RMSE, MAPE, and MAE on the flows of taxis in Beijing [224] and a private mobile phone data\n(CDRs) for the city of Sanya.\nLDRSN (Local-Dilated Region-Shifting Network) [183] consists of five modules. The first one is a CNN that\nhandles local spatial dependencies; the second one uses dilated units to deal with distant spatial dependencies. A ð‘˜-\ndilated unit is a convolution that captures the correlation of two regions with a distance of ð‘˜. The outputs of these\ntwo modules are fused and forwarded to two other modules: one handles long-term temporal dependencies, the\nother deals with short-term temporal patterns. The long-term module consists of three pipelines with consecutive\nconvolutional layers, each followed by attention LSTMs. A single ConvLSTM handles the short-term patterns.\nThe outputs of the two temporal modules are fed into a final module: as a first step, it sums the outputs of the\ntwo temporal modules and forwards the result to a convolutional layer to make the prediction. The model is\nevaluated on the dataset of bikesâ€™ [224] and taxisâ€™ in New York City [184] using MAE, MAPE, and RMSE.\nTo capture long-range spatial dependencies, DeepSTN+ (Deep Spatio Temporal Network Plus) [112] replaces\nthe traditional residual units and the convolutional layers with ResPlus and ConvPlus. ResPlus units employ a\nConvPlus layer and a standard convolutional layer to capture distant spatial dependencies. The idea beyond a\nConvPlus layer is to separate the channels of the input matrix and use an FC to capture the long-range spatial\ndependencies among each pair of regions. Finally, DeepSTN+ uses an average pooling layer before the FC to\nreduce the number of parameters. Another peculiarity of DeepSTN+ is that it uses temporal factors and the\ndistribution of POIs to gain prior knowledge of the crowd flows. The experiments are conducted on the dataset of\nbikes in New York City [14] and a private dataset from the most popular social network vendor in China using\nMAE and RMSE.\nST-Attn [233] is specifically designed for multi-step crowd flow prediction. It relies on an encoder and a decoder\nbased on two attention mechanisms (one capturing spatial patterns and one capturing temporal patterns) and\nseveral FCs. The encoder is a stack of a spatial and temporal attention mechanism and FC layers. The decoder\nhas two consecutive attention mechanisms to capture spatial and temporal patterns: the first one uses historical\nobservations about crowd flows, the second one uses the output of the first attention mechanism and the output\nof the encoder. The decoderâ€™s output is passed to an FC layer to predict the Î” next crowd flows. The model is\nevaluated for Î” âˆˆ1 . . . 6 on the datasets of taxis in Beijing [224], taxis in New York City [184], and bikes in New\nYork City [14] using RMSE.\nASTIR [124] consists of three pipelines each using ConvLSTMs to capture periodic, long, and short-terms spatial\nand temporal dependencies, respectively. The outputs of the three ConvLSTMs are passed to three attention\nmechanisms, whose outputs are passed to three ConvLSTMs. The outputs of the three pipelines are combined\nwith the results of an FC handling external factors and passed to a tanh activation function to perform the\nprediction. ASTIR is evaluated on the dataset of taxis in Beijing and bikes in New York City [224] using RMSE.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n14\nâ€¢\nLuca et al.\nSeqST [196] performs multi-step crowd flow prediction using a sequence to sequence GAN. The GANâ€™s\ngenerator and discriminator rely on CNNs and LSTMs to extract spatio-temporal patterns from the data. In the\ngenerator, they use further CNNs to consider the impact of external factors, while the discriminator drives the\ngenerator to predict realistic crowd flows. The model is evaluated on the dataset of taxis [184] and bikes in New\nYork City [14] using MAE and RMSE.\nDeepCrowd [82]uses three pipelines to capture patterns in previous hours, days, and weeks. It consists of\nthree ConvLSTMs that downsample and upsample the input data. Each convLSTM is followed by an attention\nmechanism to extract relevant local spatio-temporal patterns. The outputs of the three attention mechanisms are\nconcatenated and passed to an additional attention mechanism to further extract relevant patterns. The output of\nthe attention mechanism is passed to a CNN to predict crowd flows. DeepCrowd is evaluated on a public dataset\nof crowd flows in Tokyo and Osaka, Japan [82] using MSE, MAE, MAPE, and RMSE.\nIn contrast with the majority of solutions to crowd flow prediction, Dai et al. [43] uses only a single pipeline to\ncapture both near and far away temporal and spatial patterns. All the matrices representing crowd flows are\npassed to a temporal attention mechanism, whose output is passed to a spatial attention mechanism. The output\nof the spatial attention mechanism is fed into a CNN to extract spatio-temporal patterns. The output of this CNN\nis fused with the results of a FC which handles external features. The model is evaluated on the dataset of taxis in\nBeijing and bikes in New York City [224] using RMSE.\nSome other models address slightly different definition of crowd flow prediction [3, 180, 237]. PCRN (Periodic\nConvolutional Recurrent Networks) [237] solves the problem of predicting the distribution of presences in a city.\nThe authors use a pyramidal model made of three ConvGRUs (which has both the advantages of CNNs and GRUs)\nwith an external module that captures the patternsâ€™ periodicity by memorizing the periodic representations\nlearned by the stacked ConvGRUs. The authors propose three ways to retrieve and update the periodic patterns:\nusing a sequential periodic representation, using an estimated average of periodic representation, adopting a\ntemporally ordered representation. The outputs of the periodic module and the ConvGRUs are fused. The model\nis evaluated on the datasets of taxis in Beijing and bikes in New York City [224] using RMSE.\nAi et al. [3] aim to predict the short-term distribution of features that describe the movements of bikes of\na dockless bike-sharing system. They rely on ConvLSTMs to address the spatial and temporal dependencies.\nConvLSTMs take as input a spatio-temporal sequence composed of the number of bicycles in an area, the\ndistribution uniformity, the usage distribution, and the time of the day to predict their values in the near future.\nThe authors evaluate their model using private datasets from two bike-sharing companies in Chengdu, China.\nThe metrics used for the evaluation are MAE and RMSE.\nSome recent works solves the problem of flow prediction, a variant of crowd flow prediction in which they aim\nto use historical observations of crowd flows to forecast the entire origin-destination matrix, i.e., flows among\npairs of locations.\nMVGCN (Multi-View Graph Convolutional Network) [180] does so handling the external features (e.g., weather\ndata) with two FCs, which deals with weather information and meta-information such as time and day of the\nweek. The authors select key time snapshots to process graphs representing recent, daily, weekly, monthly and\nquarterly mobility flows. Each node in these graphs represents a region with time-varying flows. The graphs are\nforwarded to five graph convolutional networks and the outputs of the seven networks (five graph convolutional\nnetworks and two FCs for the external features) are fused using a multi-view fusion mechanism. The fusion\nmoduleâ€™s output is fed into an additional FC that outputs the predicted graph, in which each node is a region and\nthe links are the predicted flows. The experiments are conducted on the dataset of bikes [14] and taxis [184] in\nNew York City, bikes in Washington D.C. [15], and taxis in Beijing [224]. In both cases, RMSE and MAE are used\nto evaluate the modelâ€™s performance.\nRong et al. [152] use a distinct CNN for each location represented as an image-like matrix, to extract long\nand short-terms spatial and temporal features. Each matrix is made of several channels, each corresponding to\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n15\none time interval of the historical observations. The model is evaluated on a private dataset of a Chinese social\nnetwork referring to flows in Beijing using MAE and RMSE.\n4\nGENERATIVE MODELS\nGenerative models of human mobility aim at generating realistic spatio-temporal trajectories (trajectory genera-\ntion, Section 4.1) or mobility flows (flow generation, Section 4.2).\nIn this Section, we describe both tasks, discussing how DL brings significant improvements with respect to\ntraditional approaches, and describing the relevant state-of-the-art solutions to each task, with a reference to the\npublic datasets and the metrics used for training and testing the models.\n4.1\nTrajectory Generation\nThe goal of generative models of individual human mobility is to generate synthetic trajectories with realistic\nmobility patterns [9, 55, 74, 92, 165, 194]. The generated synthetic trajectories must reproduce a set of spatial and\ntemporal mobility patterns, such as the distribution of characteristic distances traveled and the predictability of\nhuman whereabouts (see Appendix E). The use of generative mobility models is crucial in many applications.\nFirst, synthetic trajectories are useful to the performance analysis of networking protocols such as mobile ad\nhoc networks, where the displacements of network users are exploited to route and deliver the messages [74, 92,\n186]. Second, synthetic trajectories are fundamental for urban planning, what-if analysis, and computational\nepidemiology, e.g., simulating changes in urban mobility in the presence of new infrastructures, epidemic diffusion,\nterrorist attacks, or international events [42, 190, 208, 209]. Furthermore, generative models are a viable solution\nto protect geo-privacy of trajectory data [59, 120, 140]: while disclosing real data requires a hard-to-control\ntrade-off between uncertainty and utility, synthetic records that preserve statistical properties may achieve in\nmultiple tasks comparable performance to real data.\nSolving trajectory generation requires capturing, simultaneously, the temporal and spatial patterns of individual\nhuman mobility. A realistic generative model should reproduce the temporal statistics observed empirically,\nincluding the number and sequence of visited locations together with the time and duration of the visits. In\nparticular, the biggest hurdle consists of the simultaneous description of an individualâ€™s routine and sporadic\nout-of-routine mobility patterns. Regarding spatial patterns, a generative model should reproduce the tendency\nof individuals to move preferably within short distances [65, 133], the heterogeneity of characteristic distances\n[65, 133] and their scales [4], the tendency of individuals to split into returners and explorers [136], the routinary\nand predictable nature of human displacements [173], and the fact that individuals visit a number of locations\nthat are constant in time [5].\nProblem Definition. A generative mobility model ð‘€is any algorithm able to generate a set of ð‘›synthetic\ntrajectories Tð‘€= {ð‘‡ð‘Ž1, . . . ,ð‘‡ð‘Žð‘›}, which describe the movements, during a certain period of time, of ð‘›independent\nagents ð‘Ž1, . . . ,ð‘Žð‘›. The synthetic trajectory generated for a single agent ð‘Žð‘–should be in the form of Definition 2.1,\ni.e., a time-ordered sequence ð‘‡ð‘Žð‘–= âŸ¨ð‘1, ð‘2, ..., ð‘ð‘˜âŸ©composed by spatio-temporal points, describing the ð‘˜locations\nvisited by ð‘Žð‘–. The realism of ð‘€is evaluated with respect to:\n(1) A set of spatial patterns (ð‘ 1, . . . ,ð‘ ð‘šð‘ ) and temporal patterns (ð‘¡1, . . . ,ð‘¡ð‘šð‘¡) K = {ð‘ 1, . . . ,ð‘ ð‘šð‘ ,ð‘¡1, . . . ,ð‘¡ð‘šð‘¡} (see\nAppendix E). The patterns refer to the distributions of individual measures, which quantify aspects related\nto the mobility of a single individual (e.g., radius of gyration, mobility entropy), or collective measures,\nwhich quantify aspects related to the mobility of a region as a whole (e.g., OD matrices). A realistic Tð‘€is\nexpected to reproduce as many mobility patterns as possible.\n(2) A set X = {ð‘‡ð‘¢1, . . . ,ð‘‡ð‘¢ð‘š} of real mobility trajectories corresponding to ð‘šreal individuals ð‘¢1 . . .ð‘¢ð‘šthat\nmove on the same region as the one on which synthetic trajectories are generated. Generally, a portion\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n16\nâ€¢\nLuca et al.\nXtrain âˆˆX is used to train ð‘€or to fit its parameters. The remaining part Xtest is used to compute the set K\nof patterns, which are compared with the patterns computed on Tð‘€.\n(3) A function ð·that computes the dissimilarity between two distributions, such as the KL divergence or\nthe JS (see Appendix D.4 for definitions). Specifically, for each measure in ð‘“âˆˆK, ð·(ð‘ƒ(ð‘“,Tð‘€)||ð‘ƒ(ð‘“,Xtest))\nindicates the dissimilarity between ð‘ƒ(ð‘“,Tð‘€), the distribution of the measures computed on the synthetic\ntrajectories in Tð‘€, and ð‘ƒ(ð‘“,Xtest), the distribution of the measures computed on the real trajectories in Xtest.\nThe lower ð·(ð‘ƒ(ð‘“,Tð‘€)||ð‘ƒ(ð‘“,Xtest)), the more realistic model ð‘€is with respect to ð‘“and Xtest.\nDL vs Traditional approaches. There is a vast literature on mechanistic generative models that reproduce\nsimple temporal, spatial, and social patterns of human mobility [9, 74, 92, 134, 194]. For example, in the Exploration\nand Preferential Return (EPR) model [172], an agent can choose between two competing mechanisms: exploration,\nduring which an agent chooses a new location never visited before, based on a random walk process with a\npower-law jump-size distribution; and preferential return, in which an agent returns to a previously visited\nlocation based on its frequency. Several studies subsequently improved the EPR model by adding increasingly\nsophisticated spatial or social mechanisms [5, 10, 39, 136, 187]. EPR and its extensions focus mainly on the spatial\naspects of human mobility, implementing unrealistic temporal mechanisms. TimeGeo [85] and DITRAS [134]\nimprove the temporal mechanism integrating into an EPR-like model a data-driven model that captures the\ncircadian propensity to travel and out-of-routine trips. Although mechanistic models have the advantage of being\ninterpretable by design, their realism is limited because of the simplicity of the implemented mechanisms.\nTRAJECTORY\nGENERATION\nIndividual Spatial\nPatterns\nExternal Factors (e.g.,\nweather conditions)\nIndividual Temporal\nPatterns\nUser Preferences (e.g.,\nPOIs, friendships)\nSpatial Dimension\nTemporal Dimension\nOther Dimensions\nRegular Mobility\nOut-of-Routine\nMobility\nRegular Mobility\nOut-of-Routine\nMobility\nSpatial Dimension\nCNN\n[198, 55, 218]\nLSTM\n[198, 76, 218]\nAttention\n[55]\nRNN\n[104]\nFC\n[115]\nTemporal Dimension\nCNN\n[198, 55, 218]\nLSTM\n[198, 55 76, 218]\nAttention \n[55]\nRNN\n[104]\nFC\n[115]\nOther Dimensions \nCNN\n[218, 55]\na)\n \nb)\nFig. 4. a) The aspects trajectory generators should capture regarding the spatial dimension (red), the temporal dimension\n(blue), and the social and geographic dimensions (yellow) of human mobility data. b) DL modules that allow to capture each\ndimension, with the reference to the selected papers in the literature that implement these modules.\nThe limitations mentioned above can be tackled using DL generative paradigms such as GANs and VAEs.\nIn both cases, models rely on DL modules to learn the distribution of data and generate mobility trajectories\ncoming from the same distributions (Figure 4). Thanks to the versatility of DL modules, GANs and VAEs can\ncapture different aspects simultaneously (e.g., spatial, temporal, social dimensions of mobility) while traditional\napproaches can capture peculiar aspects of mobility only (e.g., only the spatial dimension). Moreover, DL models\ncan capture complex and non-linear relationships in the data that traditional approaches may fail to capture.\nTherefore, such models can generate more realistic data than traditional models.\nDatasets and evaluation metrics. Trajectory generators are commonly trained and evaluated on GPS datasets\nsuch as Geolife [232], MDC [106], and taxi traces in San Francisco [143]. Additional information about these\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n17\nReference\nName\nYear\nDL Modules\nEvaluation\nDataset\nCode (https://bit.ly)\nTrajectory Generation\nWang et al. [198]\nTSG\n2021\nGAN, CNN, LSTM\ndistances, ð‘Ÿð‘”, ð‘(ð‘Ÿ,ð‘‘)\n[123]\nTSG_Model\nFeng et al. [55]\nMoveSim\n2020\nGAN, self-attention\nCNN\ndistances, ð‘Ÿð‘”, ð‘(ð‘Ÿ,ð‘‘),\nDailyLoc, G-rank, I-rank\n[232]\nHuang et al. [76]\nSVAE\n2019\nVAE, LSTM\nMDE\n-\n-\nOuyang et al. [128]\nOuyang GAN\n2018\nWGAN, CNN\nð‘(ð‘Ÿ), ð‘(ð‘Ÿ,ð‘¡), ð‘(ð‘Ÿ,ð‘‘),\nð‘(ð‘Ÿ,ð‘‘ð‘¡ð‘œð‘¡ð‘Žð‘™), ð‘ð‘‘ð‘¡ð‘œð‘¡ð‘Žð‘™,\nlocation frequency\n[106]\n-\nKulkarni et al. [104]\n-\n2018\nRNN, GAN\nvisitation frequency,\nstatistical similarity,\nprivacy tests\n[106]\n-\nYin et al. [218]\n-\n2018\nGAN, FC\nreconstruction error, utility loss\n[143]\n-\nLiu et al. [115]\ntrajGANs\n2018\nGANs\n-\n-\n-\nTable 3. List of selected papers tackling trajectory generation. For each paper, we describe the name of the corresponding\nproposed model (if any), the year of publication, the DL modules used in the proposed solution, the metrics used for\nperformance evaluation, the link to the public datasets used for training and testing the model, and the link to a repository\nwith the code (if any). Papers are sorted by year of publication in decreasing order.\ndatasets can be found in Appendix C.2. The distance between the distribution measures the performances of\ntrajectory generators (e.g., using KL or JS divergence, see Appendix D.4 for details) of standard mobility metrics\ncomputed on a real dataset and the generated dataset. Appendix D.1 describes a set of standard mobility metrics\ncommonly used to evaluate the realism of trajectory generators.\nDL approaches. Most of the DL approaches to trajectory generation are based on GANs, while a few are\nbased on VAEs. Table 3 reports a selection of the most relevant DL approaches to trajectory generation.\nIn their vision paper, Liu et al. [115] propose the trajGANs framework to address the potential and challenges\nof using GANs for trajectory generation. Similar to a typical GAN [67], a trajGAN consists of a generator ðº,\nwhich accepts a random vector ð‘§and generates a dense representation of synthetic trajectories samples, and a\ndiscriminator ð·, which classifies an input trajectory sample into â€œrealâ€ or â€œfakeâ€. Liu et al. [115] suggest the use\nof Recurrent Neural Networks (RNNs) to create dense representations of trajectories and transform between\ntrajectories and distributional representations.\nOuyang et al. [128] represent a trajectory as a sequence of stays, each with a geographic location, start time and\nduration. Specifically, a trajectory is an ð‘›1 Ã—ð‘›2 Ã—ð‘˜matrix, where ð‘›1 Ã—ð‘›2 is the size of a squared tessellation, and ð‘˜\nis the maximum number of stay repetitions for each location (set in the experiments to ð‘˜= 4). A Wasserstein GAN\n[72] is used to train a trajectory generator. Both the generator and the discriminator are based on a CNN. The\nexperiments are conducted on the MDC dataset [106] using a 64 Ã— 64 squared tessellation on the city of Lausanne,\nSwitzerland. The similarity between synthethic and real trajectories is evaluated using the JS divergence on the\npopularity ð‘(ð‘Ÿ) and temporal popularity ð‘(ð‘Ÿ,ð‘¡) of locations, the staying patterns ð‘(ð‘Ÿ,ð‘‘), the semantic importance\nð‘ð‘‘ð‘¡ð‘œð‘¡ð‘Žð‘™(ð‘Ÿ), the semantic distance, and G-rank.\nSong et al. [174] use a CNN with four layers within a GAN framework to generate trajectories represented as\n512 Ã— 512 matrices. In a data convolution process, the input 512 Ã— 512 matrices are resized into a 32 Ã— 32 matrix.\nGiven the small size of the available (private) mobile phone datasets, the experiments include synthetic ones\nobtained by randomly shuffling the real trajectories. In a deconvolution process, the GANâ€™s output (a 32 Ã— 32\nmatrix) is resized back to a 512 Ã— 512 matrix using the nearest neighbor function. No quantitative evaluation of\nthe modelâ€™s realism is provided.\nHuang et al. [76] present SVAE (Sequential Variational Autoencoder), a generative model based on a combination\nof a Variational Autoencoder (VAE) and an LSTM, combining the ability of VAEs to construct a latent space\nthat captures salient features of the training data with the ability of LSTMs to process sequential data. In a data\nprocessing phase, they force the input trajectories to have fixed timestamps, and in the experiments they evaluate\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n18\nâ€¢\nLuca et al.\nthe realism of SVAE through the Mean Distance Error (MDE) between each step of real and synthetic trajectory\npairs, finding that the reconstruction error of SVAE is smaller than 800 meters.\nKulkarni et al. [104] benchmark the performance of RNNs, SeqGAN [220], RGAN [52] and nonparametric\ncopulas to generate synthetic trajectories. They compare the generated trajectories with real ones extracted from\nthe MDC dataset [106] based on geographic and semantic similarity, statistical similarity, long-range dependencies\nand privacy tests. They find that copulas have an advantage over all other methods in terms of both model\nperformance and computational time.\nMoveSim [55] is a model-free GAN framework that integrates the domain knowledge of human mobility\nregularity. The generator consists of a self-attention-based sequential model to capture the temporal transitions\nin human mobility. The discriminator consists of a mobility regularity-aware loss to distinguish the generated\ntrajectory from a fake one. The mobility regularities of spatial continuity and temporal periodicity are used to\npre-train the generator and discriminator to accelerate the learning procedure. They conduct experiments on a\nprivate mobile phone dataset, using the base station as the spatial unit, and on GeoLife [232], projecting GPS\ncoordinates into a grid. As for the temporal granularity set the basic time slot of a trajectory as half an hour of\nthe day. The realism of MoveSim is evaluated based on the distribution of distances, radius of gyration, number\nof locations visited daily, G-rank, and I-rank using the JS divergence with respect to real trajectories.\nYin et al. [218] use a GAN-based framework to generate density distributions rather than trajectories, i.e., the\nnumber of users in each location at each time slot. Both the generator and the discriminator are implemented\nwith FCs. Experiments are conducted on a dataset extracted from MoMo (spatial resolution 2km, time slots of\n30 minutes) and on the dataset of taxis in San Francisco (squared tessellation of 50kmÃ—50km, time slots of two\nminutes) [143]. The proposed model is evaluated in terms of reconstruction error and utility loss outperforms the\ndifferential privacy approach in data utility and attack error.\nTSG (Two-Stage GAN) [198] consists of two GANs with different objectives. The first GAN captures spatio-\ntemporal patterns from the trajectories, mapped into a spatial tessellation leveraging CNNs (both for generator\nand discriminator). The generated matrices are then processed and the origin (enter point) and destination (exit\npoint) of the trips are extracted. The second GAN extracts road information from road maps using CNNs, and\nuses LSTMs to generate a trajectory between an origin road and a destination road on the road network. The\nmodel performance is evaluated using the dataset of taxis in Porto [123] by measuring the JS divergence between\nthe distribution of trajectory lengths and the frequencies of the top 50 visited places.\n4.2\nFlow Generation\nFlow generation consists of generating the flows between a set of geographic locations, given some locationsâ€™\ncharacteristics (e.g., population, POIs, land use, distance to other locations) and without any information about\nthe real flows, [9]. Flow generation is crucial to many aspects of our society, such as transport planning [51]\nand spatial economics [93, 138, 145] to reduce inequalities and to design more sustainable communities, and the\nmodeling of epidemic spreading patterns [7, 28, 110, 225]. Solving flow generation requires capturing the spatial\npatterns of close and distant flows, dependencies in the mobility network, and the characteristics of the locations.\nProblem definition. Given a tessellation G over a region ð´, the flow, ð‘¦(ð‘”ð‘–,ð‘”ð‘—), between locations ð‘”ð‘–and ð‘”ð‘—\nrepresents the number of people moving from ð‘”ð‘–to ð‘”ð‘—. The total outflow, ð‘‚ð‘–, from location ð‘”ð‘–is the total number\nof people originating from location ð‘”ð‘–, i.e., ð‘‚ð‘–= Ã\nð‘—ð‘¦(ð‘”ð‘–,ð‘”ð‘—). Flow generation aims to estimate ð‘¦(ð‘”ð‘–,ð‘”ð‘—)âˆ€ð‘–, ð‘—âˆˆG,\nð‘–â‰ ð‘—, i.e., the flow between each pair of locations (ð‘”ð‘–,ð‘”ð‘—)in given their total outflows ð‘‚ð‘–and ð‘‚ð‘—.\nDL vs Traditional approaches. Flow generation has attracted interest for a long time ago. Notably, in 1946\nGeorge K. Zipf proposed a model to estimate mobility flows, drawing an analogy with Newtonâ€™s law of universal\ngravitation [236]. This model, known as the gravity model, is based on the assumption that the number of\ntravelers between two locations (flow) increases with the locationsâ€™ populations while decreases with the distance\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n19\nbetween them [9]. Despite the significant results achieved by the gravity model, it suffers from several drawbacks,\nincluding the inability to capture the structure of the real flows accurately and the more significant variability of\nreal flows than expected [166]. Moreover, generations are done without considering other factors (e.g., POIs, street\nnetworks and others). Other mechanistic approaches aim to extend the gravity model by capturing additional\ninformation like the radiation model. All the models mentioned cannot capture non-linear relationships in the\ndata and rely on a limited set of parameters.\nFLOW\nGENERATION\nSpatial Patterns\nGeographic Characteristics\n(e.g., POIs)\nSpatial Dimension\nOther Dimensions\nClose (e.g., inï¬‚uence\nof near-by areas)\nDistant (e.g., inï¬‚uence\nof distant areas)\nSpatial Dimension\nFC\n[166]\nGCN\n[217,116]\nAttention\n[116]\nOther Dimensions \nFC\n[166]\na)\nb)\nFig. 5. a) The aspects flow generators should capture regarding the spatial dimension (red) and the social and geographic\ndimensions (yellow) of human mobility data. b) DL modules that allow to capture each dimension, with the reference to the\nselected papers in the literature that implement these modules.\nIn contrast, DL approaches can capture complex and non-linear relationships in the data and easily integrate\nadditional information about locations such as population, POIs, population (Figure 5). For example, an FC can be\nfed with data representing POIs and, similarly, POIs can be a node feature in a graph neural network. Finally,\nCNNs can capture spatial relationships that traditional approaches cannot capture. Similarly, FC networks can be\nfed with additional features representing areas near the origin and the destination of a flow to characterize its\nspatial patterns.\nDatasets and evaluation metrics. Flow generators are generally evaluated on commuting data from censuses\nfrom official statistics institutes [91, 166]. An alternative dataset is the set of traces by taxis in Beijing [217, 229]\naggregated into flows (see Appendix C for details). Flow generation is commonly evaluated as the CPC (Common\nPart Of Commuters) between real and generated flows. Other metrics commonly used for this purpose are MAE,\nRMSE and MAPE. Additional information about these metrics can be found in Appendix D.3.\nDL approaches. There is a limited literature tackling flow generation (Table 4).\nDeepGravity [166] use FCs to extend the original gravity model with the ability to capture non-linear relation-\nships and the possibility to integrate additional information to characterize the locations easily (e.g., population,\nPOIs). The model is evaluated on commuting data from official statistics in the UK and Italy and a dataset of flows\naggregated from individual trajectories in the US [91] using CPC and RMSE. The authors also provide meaningful\nexplanations of the generated flows in terms of features of the origin and destination locations.\nSI-GCN (Spatial Interaction GCN) [217] consists of three parts: (i) a layer for managing the spatial representation\nof the data (e.g., construct the local graph, negative sampling, features organization); followed by (ii) an encoder\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n20\nâ€¢\nLuca et al.\nReference\nName\nYear\nDL Modules\nEvaluation\nDataset\nCode (https://bit.ly)\nF. G.\nYao et al. [217]\nSI-GCN\n2020\nGCN\nRMSE, MAPE, CPC\n[229]\nSimini et al. [166]\nDeep Gravity\n2020\nFC\nCPC\n[91]\nDeepGravity\nLiu et al. [116]\nGMEL\n2020\nGNN, Attention\nRMSE, MAE, CPC\nGMEL-Code\nTable 4. List of selected papers tackling flow generation. For each paper, we describe the name of the corresponding proposed\nmodel (if any), the year of publication, the DL modules used in the proposed solution, the metrics used for performance\nevaluation, the link to the public datasets used for training and evaluating the model, and the link to a repository with the\ncode (if any). The papers are sorted by year of publication in decreasing order.\nthat uses graph convolutions to generate a representation in a latent space for all the geographical units; and (iii)\na decoder in charge of generating the missing flows starting from the latent representation. SI-GCN is evaluated\non the dataset of T-Drive [229] using RMSE, MAPE and CPC as evaluation metrics.\nGML (Geocontextual Multitask Embedding Learner) [116] captures the spatial correlation from geographic\ncontextual information and relies on two graph neural networks with attention (GAT). The role of the GATs is to\nlearn an embedding representation that is then passed to a gradient boosting that is in charge of generating the\nflows. The model is evaluated on commuting data regarding New York City using MAE, RMSE, and CPC.\n5\nCONCLUSIONS\nIn this survey, we proposed a perspective on DL approaches to human mobility, focusing on next-location\nprediction, crowd flow prediction, trajectory generation, and flow generation. For each task, we highlighted the\nchallenges related to solving it and how DL may help capture human mobility better than traditional models.\nWe described a selection of relevant state-of-the-art solutions to each task. As an additional contribution to the\ncommunity, we created a GitHub repository (bit.ly/DL4HM) where researchers can contribute to maintaining the\nlist of relevant papers always up-to-date. Our survey reveals that predictive tasks (next-location and crowd flow\nprediction) are well established in the community and addressed by a significant variety of DL approaches. In\ncontrast, the usage of DL for generative tasks (trajectory generation and flow generation) is more recent and\nshould require more attention in the future.\nOur overview of the state of the art of DL for human mobility reveals that existing solutions suffer from several\nlimitations, and many relevant aspects need to be addressed in the future. In particular, we identify the following\nopen challenges.\nGeographic Transferability. Although DL models can capture complex mobility patterns automatically, they\nstrictly depend on the data used for training and may not be geographically transferable, i.e., one model trained\non a specific region can be used to predict locations or crowd flows or to generate synthetic trajectories on a\ndistinct, non-overlapping region. Geographic transferability can be crucial in situations where there is a scarcity\nor even absence of mobility data for a region, and it poses several challenges related, for example, to the design\nof a suitable encoder of the mobility trajectories or flows. As a first tentative in this direction, RegionTrans [195]\nprovides insights on how transferability can be tackled for crowd flow prediction. Still, more work is needed to\naddress this open challenge.\nExplainability. DL models are by nature opaque, i.e., they are black-boxes from which it is hard to reconstruct\nthe reasoning that led to the generation of a trajectory or the prediction of a location or flow. Nonetheless,\nexplainability is crucial for gaining a deeper understanding of mobility patterns and highlighting the presence of\nbiases in the modelâ€™s reasoning. It is important to develop mobility-related explanations that provide examples\nand counter-examples to validate trajectories and crowd flows from different perspectives. While models rely on\nmany features, either external ones (e.g., weather data, POIs) or spatio-temporal ones, it is not clear what the role\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n21\nof each feature is to the modelâ€™s prediction or generation. Designing explainable DL models for human mobility\nis essential to gain knowledge that can be useful for possible users, such as policymakers and urban planners.\nPrivacy. DL models raise privacy issues both in the training and the prediction or generation phase. For example,\nin trajectory generation, evaluating the risk of re-identifying a real user from synthetic trajectories is crucial,\nespecially when there is a scarcity of data to train the models. A synthetic trajectory may resemble a real one and\na malicious adversary may use this information for re-identification. In the training phase, the risk of leaking\nprivate data is high regardless of the mobility task, as the portions of information used cannot be controlled\ndirectly. The extent training trajectories or crowd flows can be perturbed without degrading the realism of the\ngenerative models and the accuracy of predictive ones is an aspect that is barely investigated in the literature.\nTunability. Meaningful predictions and simulations require models that can be controlled along relevant\nmobility dimensions, such as geographic space, time granularity, presence of mobility restrictions or other events\nthat may alter mobility, predictability of trajectories or crowd flows, and more. Current DL models have a limited\ndegree of tunability, which limits their usability in practice. For example, it is not clear to what extent the modelsâ€™\nrealism or accuracy depends on the size or shape of the spatial tessellation, which may vary according to the\nuserâ€™s needs. For example, suppose the decision-maker is interested in identifying the most active areas of a city\nin terms of daily mobility. In that case, the usage of administrative tiles may provide more interpretable results.\nIn contrast, if they want to understand how the transportation and road networks affect and are affected by big\nevents in a city, the usage of fine-grained grid-like tessellations may be helpful.\nInteraction Dimension. Next-location predictors and trajectory generators assume the independence of the\nindividualsâ€™ mobility, even though social purposes or collective needs can explain a significant portion of human\nmovements. Peopleâ€™s movements are not entirely independent of each other and they can lead to situations like\ntraffic jams, accidents, or massive commuting patterns. Models that can include these aspects need to be designed\nto capture human mobilityâ€™s complexity more comprehensively.\nACKNOWLEDGMENTS\nLuca Pappalardo has been partially supported by EU H2020 SoBigData++ grant agreement #871042.\nREFERENCES\n[1] Zain Ul Abideen, Heli Sun, Zhou Yang, Rana Zeeshan Ahmad, Adnan Iftekhar, and Amir Ali. 2021. Deep Wide Spatial-Temporal Based\nTransformer Networks Modeling for the Next Destination According to the Taxi Driver Behavior Prediction. Applied Sciences 11, 1\n(2021), 17.\n[2] Mohammed N Ahmed, Gianni Barlacchi, Stefano Braghin, Francesco Calabrese, Michele Ferretti, Vincent PA Lonij, Rahul Nair, Rana\nNovack, Jurij Paraszczak, and Andeep S Toor. 2016. A Multi-Scale Approach to Data-Driven Mass Migration Analysis.. In SoGood@\nECML-PKDD.\n[3] Yi Ai, Zongping Li, Mi Gan, Yunpeng Zhang, Daben Yu, Wei Chen, and Yanni Ju. 2019. A deep learning approach on short-term\nspatiotemporal distribution forecasting of dockless bike-sharing system. Neural Computing and Applications 31, 5 (2019), 1665â€“1677.\n[4] Laura Alessandretti, Ulf Aslak, and Sune Lehmann. 2020. The scales of human mobility. Nature 587, 7834 (2020), 402â€“407. https:\n//doi.org/10.1038/s41586-020-2909-1\n[5] Laura Alessandretti, Piotr Sapiezynski, Vedran Sekara, Sune Lehmann, and Andrea Baronchelli. 2018. Evidence for a conserved quantity\nin human mobility. Nature Human Behaviour 2, 7 (2018), 485â€“491.\n[6] Daniel Ashbrook and Thad Starner. 2002. Learning significant locations and predicting user movement with GPS. In Proceedings. Sixth\nInternational Symposium on Wearable Computers,. IEEE, 101â€“108.\n[7] Duygu Balcan, Bruno GonÃ§alves, Hao Hu, JosÃ© J Ramasco, Vittoria Colizza, and Alessandro Vespignani. 2010. Modeling the spatial\nspread of infectious diseases: The GLobal Epidemic and Mobility computational model. Journal of computational science 1, 3 (2010),\n132â€“145.\n[8] Yi Bao, Zhou Huang, Linna Li, Yaoli Wang, and Yu Liu. 2020. A BiLSTM-CNN model for predicting usersâ€™ next locations based on\ngeotagged social media. International Journal of Geographical Information Science (2020), 1â€“22.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n22\nâ€¢\nLuca et al.\n[9] Hugo Barbosa, Marc Barthelemy, Gourab Ghoshal, Charlotte R James, Maxime Lenormand, Thomas Louail, Ronaldo Menezes, JosÃ© J\nRamasco, Filippo Simini, and Marcello Tomasini. 2018. Human mobility: Models and applications. Physics Reports 734 (2018), 1â€“74.\n[10] Hugo Barbosa, Fernando B de Lima-Neto, Alexandre Evsukoff, and Ronaldo Menezes. 2015. The effect of recency to human mobility.\nEPJ Data Science 4 (2015), 1â€“14.\n[11] Gianni Barlacchi, Christos Perentis, Abhinav Mehrotra, Mirco Musolesi, and Bruno Lepri. 2017. Are you getting sick? Predicting\ninfluenza-like symptoms using human mobility behaviors. EPJ Data Science 6, 1 (2017), 27.\n[12] Gianni Barlacchi, Alberto Rossi, Bruno Lepri, and Alessandro Moschitti. 2017. Structural semantic models for automatic analysis of\nurban areas. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 279â€“291.\n[13] Armando Bazzani, Bruno Giorgini, Sandro Rambaldi, Riccardo Gallotti, and Luca Giovannini. 2010. Statistical laws in urban mobility\nfrom microscopic GPS data in the area of Florence. Journal of Statistical Mechanics: Theory and Experiment 2010, 05 (2010), P05001.\n[14] Citi Bike. 2013. Citi Bike System Data - NYC. https://www.citibikenyc.com/system-data\n[15] Capital Bikeshare. 2011. Capital Bikeshare - Washington DC. https://www.capitalbikeshare.com/system-data\n[16] V. Bindschaedler and R. Shokri. 2016. Synthesizing Plausible Privacy-Preserving Location Traces. In 2016 IEEE Symposium on Security\nand Privacy (SP). 546â€“563.\n[17] Justine I. Blanford, Zhuojie Huang, Alexander Savelyev, and Alan M. MacEachren. 2015. Geo-Located Tweets. Enhancing Mobility\nMaps and Capturing Cross-Border Movement. PLOS ONE 10, 6 (2015), 1â€“16.\n[18] Vincent D. Blondel, Adeline Decuyper, and Gautier Krings. 2015. A survey of results on mobile phone datasets analysis. EPJ Data\nScience 4, 1 (2015), 10.\n[19] M Bohm, Mirco Nanni, and Luca Pappalardo. 2021. Quantifying the presence of air pollutants over a road network in high spatio-\ntemporal resolution. In Climate Change AI, NeurIPS Workshop.\n[20] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. 2015. Time series analysis: forecasting and control. John\nWiley & Sons.\n[21] Lorenzo Bracciale, Marco Bonola, Pierpaolo Loreti, Giuseppe Bianchi, Raul Amici, and Antonello Rabuffi. 2014. CRAWDAD dataset\nroma/taxi (v. 2014-07-17).\n[22] Dirk Brockmann, Lars Hufnagel, and Theo Geisel. 2006. The scaling laws of human travel. Nature 439, 7075 (2006), 462â€“465.\n[23] Ingrid Burbey and Thomas L Martin. 2012. A survey on predicting personal mobility. International Journal of Pervasive Computing and\nCommunications (2012).\n[24] F. Calabrese, G. Di Lorenzo, L. Liu, and C. Ratti. 2011. Estimating Origin-Destination Flows Using Mobile Phone Location Data. IEEE\nPervasive Computing (2011), 36â€“44.\n[25] Francesco Calabrese, Giusy Di Lorenzo, and Carlo Ratti. 2010. Human mobility prediction based on individual and collective geographical\npreferences. In 13th international IEEE conference on intelligent transportation systems. 312â€“317.\n[26] Luca Canzian and Mirco Musolesi. 2015. Trajectories of depression: unobtrusive monitoring of depressive states by means of smartphone\nmobility traces analysis. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing. 1293â€“1304.\n[27] Justin David Carlson. 2010. Mapping Large, Urban Environments with GPS-Aided SLAM. Ph.D. Dissertation. Carnegie Mellon University.\n[28] Serhan Cevik. 2020. Going Viral: A Gravity Model of Infectious Diseases and Tourism Flows. (2020).\n[29] Shwu-Jing Chang, Gong-Ying Hsu, Jia-Ao Yang, Kuan-Ning Chen, Yung-Fang Chiu, and Fu-Tong Chang. 2010. Vessel traffic analysis\nfor maritime Intelligent Transportation System. In 2010 IEEE 71st Vehicular Technology Conference. IEEE, 1â€“4.\n[30] Guangshuo Chen, Aline Carneiro Viana, Marco Fiore, and Carlos Sarraute. 2019. Complete trajectory reconstruction from sparse\nmobile phone data. EPJ Data Science 8, 1 (2019), 30.\n[31] J. Chen, Z. Xiao, D. Wang, W. Long, and V. Havyarimana. 2019. Stay of Interest: A Dynamic Spatiotemporal Stay Behavior Perception\nMethod for Private Car Users. In 2019 IEEE 21st International Conference on High Performance Computing and Communications.\n1526â€“1532.\n[32] Qi Chen, Wei Wang, Fangyu Wu, Suparna De, Ruili Wang, Bailing Zhang, and Xin Huang. 2019. A survey on an emerging area: Deep\nlearning for smart city data. IEEE Transactions on Emerging Topics in Computational Intelligence 3, 5 (2019), 392â€“410.\n[33] Yile Chen, Cheng Long, Gao Cong, and Chenliang Li. 2020. Context-aware Deep Model for Joint Mobility and Time Prediction. In\nProceedings of the 13th International Conference on Web Search and Data Mining. 106â€“114.\n[34] Eunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility: user movement in location-based social networks. In\nProceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 1082â€“1090.\n[35] Kyunghyun Cho, Bart van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.\n2014. Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 1724â€“1734.\n[36] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.\n2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078\n(2014).\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n23\n[37] Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter Stewart. 2016. Retain: An interpretable\npredictive model for healthcare using reverse time attention mechanism. In Advances in Neural Information Processing Systems.\n3504â€“3512.\n[38] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. 2015. Attention-based models for speech\nrecognition. In Advances in neural information processing systems. 577â€“585.\n[39] Giuliano Cornacchia and Luca Pappalardo. 2021. STS-EPR: Modelling individual mobility considering the spatial, temporal, and\nsocial dimensions together. Procedia Computer Science 184 (2021), 258â€“265.\nhttps://doi.org/10.1016/j.procs.2021.03.035 The 12th\nInternational Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging\nData and Industry 4.0 (EDI40) / Affiliated Workshops.\n[40] BalÃ¡zs Cs CsÃ¡ji, Arnaud Browet, Vincent A Traag, Jean-Charles Delvenne, Etienne Huens, Paul Van Dooren, Zbigniew Smoreda, and\nVincent D Blondel. 2013. Exploring the mobility of mobile phone users. Physica A: statistical mechanics and its applications 392, 6\n(2013), 1459â€“1473.\n[41] Yilan Cui, Xing Xie, and Yi Liu. 2018. Social media and mobility landscape: Uncovering spatial patterns of urban human mobility with\nmulti source data. Frontiers of Environmental Science & Engineering 12, 5 (2018), 7.\n[42] Susan L Cutter, Joseph A Ahearn, Bernard Amadei, Patrick Crawford, Elizabeth A Eide, Gerald E Galloway, Michael F Goodchild,\nHoward C Kunreuther, Meredith Li-Vollmer, Monica Schoch-Spana, et al. 2013. Disaster resilience: A national imperative. Environment:\nScience and Policy for Sustainable Development 55, 2 (2013), 25â€“29.\n[43] Genan Dai, Xiaoyang Hu, Youming Ge, Zhiqing Ning, and Yubao Liu. 2021. Attention based simplified deep residual network for\ncitywide crowd flows prediction. Frontiers of Computer Science 15, 2 (2021), 1â€“12.\n[44] Alexandre De BrÃ©bisson, Ã‰tienne Simon, Alex Auvolat, Pascal Vincent, and Yoshua Bengio. 2015. Artificial neural networks applied to\ntaxi destination prediction. In Proceedings of the 2015th International Conference on ECML PKDD Discovery Challenge. 40â€“51.\n[45] Yves-Alexandre de Montjoye, SÃ©bastien Gambs, Vincent Blondel, Geoffrey Canright, Nicolas de Cordes, SÃ©bastien Deletaille, Kenth\nEngÃ¸-Monsen, Manuel Garcia-Herranz, Jake Kendall, Cameron Kerry, Gautier Krings, Emmanuel LetouzÃ©, Miguel Luengo-Oroz, Nuria\nOliver, Luc Rocher, Alex Rutherford, Zbigniew Smoreda, Jessica Steele, Erik Wetter, Alex â€œSandyâ€ Pentland, and Linus Bengtsson. 2018.\nOn the privacy-conscientious use of mobile phone data. Scientific Data 5, 1 (2018), 180286.\n[46] Pierre Deville, Catherine Linard, Samuel Martin, Marius Gilbert, Forrest R Stevens, Andrea E Gaughan, Vincent D Blondel, and Andrew J\nTatem. 2014. Dynamic population mapping using mobile phone data. Proceedings of the National Academy of Sciences 111, 45 (2014),\n15888â€“15893.\n[47] Bowen Du, Hao Peng, Senzhang Wang, Md Zakirul Alam Bhuiyan, Lihong Wang, Qiran Gong, Lin Liu, and Jing Li. 2019. Deep irregular\nconvolutional residual LSTM for urban traffic passenger flows prediction. IEEE Transactions on Intelligent Transportation Systems 21, 3\n(2019), 972â€“985.\n[48] Nathan Eagle and Alex Sandy Pentland. 2009. Eigenbehaviors: identifying structure in routine. Behavioral Ecology and Sociobiology 63,\n11 (2009), 1689â€“1689.\n[49] Patrick Ebel, Ibrahim Emre GÃ¶l, Christoph Lingenfelder, and Andreas Vogelsang. 2020. Destination Prediction Based on Partial\nTrajectory Data. arXiv preprint arXiv:2004.07473 (2020).\n[50] Zeinab Ebrahimpour, Wanggen Wan, Ofelia Cervantes, Tianhang Luo, and Hidayat Ullah. 2019. Comparison of main approaches for\nextracting behavior features from crowd flow analysis. ISPRS International Journal of Geo-Information 8, 10 (2019), 440.\n[51] Sven Erlander and Neil F Stewart. 1990. The gravity model in transportation analysis: theory and extensions. Vol. 3. Vsp.\n[52] CristÃ³bal Esteban, Stephanie L Hyland, and Gunnar RÃ¤tsch. 2017. Real-valued (medical) time series generation with recurrent conditional\ngans. arXiv preprint arXiv:1706.02633 (2017).\n[53] Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. 2013. Learning Hierarchical Features for Scene Labeling. IEEE\nTrans. Pattern Anal. Mach. Intell. 35, 8 (2013), 1915â€“1929.\n[54] Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and Depeng Jin. 2018. Deepmove: Predicting human mobility\nwith attentional recurrent networks. In Proceedings of the 2018 world wide web conference. 1459â€“1468.\n[55] Jie Feng, Zeyu Yang, Fengli Xu, Haisu Yu, Mudan Wang, and Yong Li. 2020. Learning to Simulate Human Mobility. In Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Virtual Event, CA, USA) (KDD â€™20). Association for\nComputing Machinery, New York, NY, USA, 3426â€“3433. https://doi.org/10.1145/3394486.3412862\n[56] Z. Feng and Y. Zhu. 2016. A Survey on Trajectory Data Mining: Techniques and Applications. IEEE Access 4 (2016), 2056â€“2067.\n[57] V. Fernandez Arguedas, G. Pallotta, and M. Vespe. 2018. Maritime Traffic Networks: From Historical Positioning Data to Unsupervised\nMaritime Traffic Monitoring. IEEE Transactions on Intelligent Transportation Systems 19, 3 (2018), 722â€“732.\n[58] Michele Ferretti, Gianni Barlacchi, Luca Pappalardo, Lorenzo Lucchini, and Bruno Lepri. 2018. Weak nodes detection in urban transport\nsystems: Planning for resilience in Singapore. In 2018 IEEE 5th international conference on data science and advanced analytics (DSAA).\nIEEE, 472â€“480.\n[59] Marco Fiore, Panagiota Katsikouli, Elli Zavou, Mathieu Cunche, Franccoise Fessant, Dominique Le Hello, Ulrich Matchi Aivodji, Baptiste\nOlivier, Tony Quertier, and Razvan Stanica. 2019. Privacy in trajectory micro-data publishing : a survey. arXiv: Cryptography and\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n24\nâ€¢\nLuca et al.\nSecurity (2019).\n[60] Riccardo Gallotti, Armando Bazzani, Mirko Degli Esposti, and Sandro Rambaldi. 2013. Entropic measures of individual mobility\npatterns. Journal of Statistical Mechanics: Theory and Experiment 2013, 10 (2013), P10022.\n[61] Riccardo Gallotti, Armando Bazzani, Sandro Rambaldi, and Marc Barthelemy. 2016. A stochastic model of randomly accelerated walkers\nfor human mobility. Nature Communications 7, 1 (2016), 12600.\n[62] SÃ©bastien Gambs, Marc-Olivier Killijian, and Miguel NÃºÃ±ez del Prado Cortez. 2010. Show me how you move and I will tell you who\nyou are. In Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Security and Privacy in GIS and LBS. 34â€“41.\n[63] SÃ©bastien Gambs, Marc-Olivier Killijian, and Miguel NÃºÃ±ez del Prado Cortez. 2012. Next place prediction using mobility markov chains.\nIn Proceedings of the First Workshop on Measurement, Privacy, and Mobility. 1â€“6.\n[64] Qiang Gao, Fan Zhou, Goce Trajcevski, Kunpeng Zhang, Ting Zhong, and Fengli Zhang. 2019. Predicting human mobility via variational\nattention. In The World Wide Web Conference. 2750â€“2756.\n[65] Marta C Gonzalez, Cesar A Hidalgo, and Albert-Laszlo Barabasi. 2008. Understanding individual human mobility patterns. nature 453,\n7196 (2008), 779â€“782.\n[66] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep learning. Vol. 1. MIT press Cambridge.\n[67] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\n2014. Generative Adversarial Nets. In Proceedings of the 27th International Conference on Neural Information Processing Systems.\n2672â€“2680.\n[68] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. 2013. Hybrid speech recognition with deep bidirectional LSTM. In 2013\nIEEE workshop on automatic speech recognition and understanding. IEEE, 273â€“278.\n[69] Alex Graves and JÃ¼rgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM and other neural network\narchitectures. Neural networks 18, 5-6 (2005), 602â€“610.\n[70] Clark L Gray and Valerie Mueller. 2012. Natural disasters and population mobility in Bangladesh. Proceedings of the National Academy\nof Sciences 109, 16 (2012), 6000â€“6005.\n[71] Aditya Grover and Jure Leskovec. 2016. Node2vec: Scalable Feature Learning for Networks. In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining. Association for Computing Machinery, 855â€“864.\n[72] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. 2017. Improved Training of Wasserstein\nGANs. In Proceedings of the 31st International Conference on Neural Information Processing Systems. 5769â€“5779.\n[73] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition. 770â€“778.\n[74] Andrea Hess, Karin Anna Hummel, Wilfried N Gansterer, and GÃ¼nter Haring. 2015. Data-driven human mobility modeling: a survey\nand engineering guidance for mobile networking. ACM Computing Surveys (CSUR) 48, 3 (2015), 1â€“39.\n[75] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735â€“1780.\n[76] D. Huang, X. Song, Z. Fan, R. Jiang, R. Shibasaki, Y. Zhang, H. Wang, and Y. Kato. 2019. A Variational Autoencoder Based Generative\nModel of Urban Human Mobility. In 2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR). 425â€“430.\n[77] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. 2017. Densely Connected Convolutional Networks. In 2017 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR). 2261â€“2269.\n[78] D H HUBEL and T N WIESEL. 1959. Receptive fields of single neurones in the catâ€™s striate cortex. The Journal of physiology 148, 3\n(1959), 574â€“591.\n[79] D. H. Hubel and T. N. Wiesel. 1962. Receptive fields, binocular interaction and functional architecture in the catâ€™s visual cortex. The\nJournal of Physiology 160, 1 (1962), 106â€“154.\n[80] Tomoharu Iwata and Hitoshi Shimizu. 2019. Neural collective graphical models for estimating spatio-temporal population flow from\naggregated data. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 3935â€“3942.\n[81] Kasthuri Jayarajah, Andrew Tan, and Archan Misra. 2018. Understanding the Interdependency of Land Use and Mobility for Urban\nPlanning. In Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous\nComputing and Wearable Computers. Association for Computing Machinery, 1079â€“1087.\n[82] Renhe Jiang, Zekun Cai, Zhaonan Wang, Chuang Yang, Zipei Fan, Quanjun Chen, Kota Tsubouchi, Xuan Song, and Ryosuke Shibasaki.\n2021. DeepCrowd: A Deep Model for Large-Scale Citywide Crowd Density and Flow Prediction. IEEE Transactions on Knowledge and\nData Engineering (2021).\n[83] Renhe Jiang, Xuan Song, Zipei Fan, Tianqi Xia, Quanjun Chen, Satoshi Miyazawa, and Ryosuke Shibasaki. 2018. Deepurbanmomentum:\nAn online deep-learning system for short-term urban mobility prediction. In Thirty-Second AAAI Conference on Artificial Intelligence.\n[84] Shan Jiang, Gaston A. Fiore, Yingxiang Yang, Joseph Ferreira, Emilio Frazzoli, and Marta C. GonzÃ¡lez. 2013. A Review of Urban\nComputing for Mobile Phone Traces: Current Methods, Challenges and Opportunities. In Proceedings of the 2nd ACM SIGKDD\nInternational Workshop on Urban Computing.\n[85] Shan Jiang, Yingxiang Yang, Siddharth Gupta, Daniele Veneziano, Shounak Athavale, and Marta C. Gonzalez. 2016. The TimeGeo\nmodeling framework for urban mobility without travel surveys. Proceedings of the National Academy of Sciences 113 (2016), 201524261.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n25\n[86] Weiwei Jiang and Jiayun Luo. 2021. Graph Neural Network for Traffic Forecasting: A Survey. arXiv:cs.LG/2101.11174\n[87] Wenwei Jin, Youfang Lin, Zhihao Wu, and Huaiyu Wan. 2018. Spatio-Temporal Recurrent Convolutional Networks for Citywide\nShort-Term Crowd Flows Prediction.\n[88] Raja Jurdak, Kun Zhao, Jiajun Liu, Maurice AbouJaoude, Mark Cameron, and David Newth. 2015. Understanding Human Mobility\nfrom Twitter. PLOS ONE 10, 7 (2015), 1â€“16.\n[89] Yiannis Kamarianakis and Poulicos Prastacos. 2003. Forecasting traffic flow conditions in an urban network: Comparison of multivariate\nand univariate approaches. Transportation Research Record 1857, 1 (2003), 74â€“84.\n[90] Yiannis Kamarianakis and Poulicos Prastacos. 2005. Spaceâ€“time modeling of traffic flow. Computers & Geosciences 31, 2 (2005), 119â€“133.\n[91] Yuhao Kang, Song Gao, Yunlei Liang, Mingxiao Li, Jinmeng Rao, and Jake Kruse. 2020. Multiscale dynamic human mobility flow dataset\nin the US during the COVID-19 epidemic. Scientific data 7, 1 (2020), 1â€“13.\n[92] Dmytro Karamshuk, Chiara Boldrini, Marco Conti, and Andrea Passarella. 2011. Human mobility models for opportunistic networks.\nIEEE Communications Magazine 49, 12 (2011), 157â€“165.\n[93] David Karemera, Victor Iwuagwu Oguledo, and Bobby Davis. 2000. A gravity model analysis of international migration to North\nAmerica. Applied economics 32, 13 (2000), 1745â€“1755.\n[94] L. Khaidem, M. Luca, F. Yang, A. Anand, B. Lepri, and W. Dong. 2020. Optimizing Transportation Dynamics at a City-Scale Using a\nReinforcement Learning Framework. IEEE Access 8 (2020), 171528â€“171541.\n[95] Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed Qureshi. 2020. A survey of the recent architectures of deep convolutional\nneural networks. Artificial Intelligence Review (2020).\n[96] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).\n[97] Niko Kiukkonen, Jan Blom, Olivier Dousse, Daniel Gatica-Perez, and Juha Laurila. 2010. Towards rich mobile phone datasets: Lausanne\ndata collection campaign. Proc. ICPS, Berlin 68 (2010).\n[98] J. F. Kolen and S. C. Kremer. 2001. Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies. 237â€“243.\n[99] Dejiang Kong and Fei Wu. 2018. HST-LSTM: A Hierarchical Spatial-Temporal Long-Short Term Memory Network for Location\nPrediction.. In IJCAI. 2341â€“2347.\n[100] Vartika Koolwal and Krishna Kumar Mohbey. 2020. A comprehensive survey on trajectory-based location prediction. Iran Journal of\nComputer Science 3, 2 (2020), 65â€“91.\n[101] Moritz UG Kraemer, Chia-Hung Yang, Bernardo Gutierrez, Chieh-Hsi Wu, Brennan Klein, David M Pigott, Louis Du Plessis, Nuno R\nFaria, Ruoran Li, William P Hanage, et al. 2020. The effect of human mobility and control measures on the COVID-19 epidemic in\nChina. Science 368, 6490 (2020), 493â€“497.\n[102] Mark A Kramer. 1991. Nonlinear principal component analysis using autoassociative neural networks. AIChE journal 37, 2 (1991),\n233â€“243.\n[103] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks.\nIn Advances in Neural Information Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.). Curran\nAssociates, Inc., 1097â€“1105.\n[104] Vaibhav Kulkarni, Natasa Tagasovska, Thibault Vatter, and Benoit Garbinato. 2018. Generative models for simulating mobility\ntrajectories. arXiv preprint arXiv:1811.12801 (2018).\n[105] Shengjie Lai, Andrea Farnham, Nick W Ruktanonchai, and Andrew J Tatem. 2019. Measuring mobility, disease connectivity and\nindividual risk: a review of using mobile phone data and Health for travel medicine. Journal of travel medicine 26, 3 (2019).\n[106] Juha K Laurila, Daniel Gatica-Perez, Imad Aad, Olivier Bornet, Trinh-Minh-Tri Do, Olivier Dousse, Julien Eberle, Markus Miettinen,\net al. 2012. The mobile data challenge: Big data for mobile computing research. Technical Report.\n[107] Sangsoo Lee and Daniel B Fambro. 1999. Application of subset autoregressive integrated moving average model for short-term freeway\ntraffic volume forecasting. Transportation Research Record 1678, 1 (1999), 179â€“188.\n[108] Maxime Lenormand, Aleix Bassolas, and JosÃ© J Ramasco. 2016. Systematic comparison of trip distribution laws and models. Journal of\nTransport Geography 51 (2016), 158â€“169.\n[109] Wei Li, Wei Tao, Junyang Qiu, Xin Liu, Xingyu Zhou, and Zhisong Pan. 2019. Densely Connected Convolutional Networks With\nAttention LSTM for Crowd Flows Prediction. IEEE Access 7 (2019), 140488â€“140498.\n[110] Xinhai Li, Huidong Tian, Dejian Lai, and Zhibin Zhang. 2011. Validation of the gravity model in predicting the global spread of\ninfluenza. International journal of environmental research and public health 8, 8 (2011), 3134â€“3143.\n[111] Yuan Liao, Sonia Yeh, and Gustavo S Jeuken. 2019. From individual to collective behaviours: exploring population heterogeneity of\nhuman mobility based on social media data. EPJ Data Science 8, 1 (2019), 34.\n[112] Ziqian Lin, Jie Feng, Ziyang Lu, Yong Li, and Depeng Jin. 2019. Deepstn+: Context-aware spatial-temporal neural network for crowd\nflow prediction in metropolis. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 1020â€“1027.\n[113] Lingbo Liu, Jiajie Zhen, Guanbin Li, Geng Zhan, Zhaocheng He, Bowen Du, and Liang Lin. 2020.\nDynamic Spatial-Temporal\nRepresentation Learning for Traffic Flow Prediction. IEEE Transactions on Intelligent Transportation Systems (2020).\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n26\nâ€¢\nLuca et al.\n[114] Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. Predicting the next location: A recurrent model with spatial and temporal\ncontexts. In Thirtieth AAAI conference on artificial intelligence.\n[115] Xi Liu, Hanzhou Chen, and Clio Andris. 2018. trajGANs: Using generative adversarial networks for geo-privacy protection of trajectory\ndata (Vision paper). In Location Privacy and Security Workshop. 1â€“7.\n[116] Zhicheng Liu, Fabio Miranda, Weiting Xiong, Junyan Yang, Qiao Wang, and Claudio Silva. 2020. Learning geo-contextual embeddings\nfor commuting flow prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 808â€“816.\n[117] Massimiliano Luca, Gianni Barlacchi, Nuria Oliver, and Bruno Lepri. 2021. Leveraging Mobile Phone Data for Migration Flows.\narXiv:cs.CY/2105.14956\n[118] Jianming Lv, Qing Li, Qinghui Sun, and Xintong Wang. 2018. T-CONV: A convolutional neural network for multi-scale taxi trajectory\nprediction. In 2018 IEEE international conference on big data and smart computing (bigcomp). 82â€“89.\n[119] Jean DamascÃ¨ne Mazimpaka and Sabine Timpf. 2016. Trajectory data mining: A review of methods and applications. Journal of Spatial\nInformation Science 2016, 13 (2016), 61â€“99.\n[120] D. J. Mir, S. Isaacman, R. CÃ¡ceres, M. Martonosi, and R. N. Wright. 2013. DP-WHERE: Differentially private modeling of human mobility.\nIn 2013 IEEE International Conference on Big Data. 580â€“588.\n[121] Anna Monreale, Fabio Pinelli, Roberto Trasarti, and Fosca Giannotti. 2009. Wherenext: a location predictor on trajectory pattern\nmining. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 637â€“646.\n[122] CK Moorthy and BG Ratcliffe. 1988. Short term traffic forecasting using time series methods. Transportation planning and technology\n12, 1 (1988), 45â€“56.\n[123] Luis Moreira-Matias, Joao Gama, Michel Ferreira, Joao Mendes-Moreira, and Luis Damas. 2013. Predicting taxiâ€“passenger demand\nusing streaming data. IEEE Transactions on Intelligent Transportation Systems 14, 3 (2013), 1393â€“1402.\n[124] Lablack Mourad, Heng Qi, Yanming Shen, and Baocai Yin. 2019. ASTIR: Spatio-Temporal Data Mining for Crowd Flow Prediction. IEEE\nAccess 7 (2019), 175159â€“175165.\n[125] Anastasios Noulas, Salvatore Scellato, Renaud Lambiotte, Massimiliano Pontil, and Cecilia Mascolo. 2012. A Tale of Many Cities:\nUniversal Patterns in Human Urban Mobility. PLOS ONE 7, 5 (2012), 1â€“10.\n[126] MM Nyhan, I Kloog, R Britter, C Ratti, and P Koutrakis. 2019. Quantifying population exposure to air pollution using individual\nmobility patterns inferred from mobile phone data. Journal of exposure science & environmental epidemiology 29, 2 (2019), 238.\n[127] Nuria Oliver, Bruno Lepri, Harald Sterly, Renaud Lambiotte, SÃ©bastien Deletaille, Marco De Nadai, Emmanuel LetouzÃ©, Albert Ali Salah,\nRichard Benjamins, Ciro Cattuto, et al. 2020. Mobile phone data for informing public health actions across the COVID-19 pandemic life\ncycle.\n[128] Kun Ouyang, Reza Shokri, David S Rosenblum, and Wenzhuo Yang. 2018. A Non-Parametric Generative Model for Human Trajectories..\nIn IJCAI. 3812â€“3817.\n[129] Xi Ouyang, Chaoyun Zhang, Pan Zhou, Hao Jiang, and Shimin Gong. 2016. Deepspace: An online deep learning framework for mobile\nbig data to understand human mobility patterns. arXiv preprint arXiv:1610.07009 (2016).\n[130] Luca Pappalardo, Giuliano Cornacchia, Victor Navarro, Loreto Bravo, and Leo Ferres. 2020. A dataset to assess mobility changes in\nChile following local quarantines. arXiv:physics.soc-ph/2011.12162\n[131] Luca Pappalardo, Leo Ferres, Manuel Sacasa, Ciro Cattuto, and Loreto Bravo. 2020. An individual-level ground truth dataset for home\nlocation detection. arXiv:2010.08814\n[132] Luca Pappalardo, Leo Ferres, Manuel Sacasa, Ciro Cattuto, and Loreto Bravo. 2021. Evaluation of home detection algorithms on mobile\nphone data using individual-level ground truth. EPJ Data Science 10, 1 (2021), 29. https://doi.org/10.1140/epjds/s13688-021-00284-9\n[133] Luca Pappalardo, Salvatore Rinzivillo, Zehui Qu, Dino Pedreschi, and Fosca Giannotti. 2013. Understanding the patterns of car travel.\nThe European Physical Journal Special Topics 215, 1 (2013), 61â€“73.\n[134] Luca Pappalardo and Filippo Simini. 2018. Data-driven generation of spatio-temporal routines in human mobility. Data Mining and\nKnowledge Discovery 32, 3 (2018), 787â€“829.\n[135] Luca Pappalardo, F Simini, G Barlacchi, and R Pellungrini. 2019. scikit-mobility: A Python library for the analysis, generation and risk\nassessment of mobility data. arXiv preprint arXiv:1907.07062 (2019).\n[136] Luca Pappalardo, Filippo Simini, Salvatore Rinzivillo, Dino Pedreschi, Fosca Giannotti, and Albert-LÃ¡szlÃ³ BarabÃ¡si. 2015. Returners and\nexplorers dichotomy in human mobility. Nature Communications 6, 1 (2015), 8166.\n[137] Luca Pappalardo, Maarten Vanhoof, Lorenzo Gabrielli, Zbigniew Smoreda, Dino Pedreschi, and Fosca Giannotti. 2016. An analytical\nframework to nowcast well-being using mobile phone data. International Journal of Data Science and Analytics (2016), 75â€“92.\n[138] Roberto Patuelli, Aura Reggiani, Sean P Gorman, Peter Nijkamp, and Franz-Josef Bade. 2007. Network analysis of commuting flows: A\ncomparative static approach to German data. Networks and Spatial Economics 7, 4 (2007), 315â€“331.\n[139] Roberto Pellungrini, Luca Pappalardo, Francesca Pratesi, and Anna Monreale. 2017. A Data Mining Approach to Assess Privacy Risk in\nHuman Mobility Data. ACM Transactions on Intelligent Systems and Technologies 9, 3 (2017).\n[140] R. Pellungrini, L. Pappalardo, F. Simini, and A. Monreale. 2020. Modeling Adversarial Behavior Against Mobility Data Privacy. IEEE\nTransactions on Intelligent Transportation Systems (2020), 1â€“14.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n27\n[141] Emanuele Pepe, Paolo Bajardi, Laetitia Gauvin, Filippo Privitera, Brennan Lake, Ciro Cattuto, and Michele Tizzoni. 2020. COVID-19\noutbreak response, a dataset to assess mobility changes in Italy following national lockdown. Scientific data 7, 1 (2020), 1â€“7.\n[142] Christos Perentis, Michele Vescovi, Chiara Leonardi, Corrado Moiso, Mirco Musolesi, Fabio Pianesi, and Bruno Lepri. 2017. Anonymous\nor Not? Understanding the Factors Affecting Personal Mobile Data Disclosure. ACM Trans. Internet Technol. 17, 2 (2017).\n[143] Michal Piorkowski, Natasa Sarafijanovic-Djukic, and Matthias Grossglauser. 2009. CRAWDAD dataset epfl/mobility (v. 2009-02-24).\n[144] Rafael Prieto Curiel, Luca Pappalardo, Lorenzo Gabrielli, and Steven Richard Bishop. 2018. Gravity and scaling laws of city to city\nmigration. PLOS ONE 13, 7 (2018), 1â€“19.\n[145] Rafael Prieto Curiel, Luca Pappalardo, Lorenzo Gabrielli, and Steven Richard Bishop. 2018. Gravity and scaling laws of city to city\nmigration. PLOS ONE 13, 7 (07 2018), 1â€“19. https://doi.org/10.1371/journal.pone.0199892\n[146] F. Rebelo, C. Soares, and R. J. F. Rossetti. 2015. TwitterJam: Identification of mobility patterns in urban centers based on tweets. In 2015\nIEEE First International Smart Cities Conference (ISC2). 1â€“6.\n[147] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region\nProposal Networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems. 91â€“99.\n[148] Yibin Ren, Huanfa Chen, Yong Han, Tao Cheng, Yang Zhang, and Ge Chen. 2020. A hybrid integrated deep learning model for the\nprediction of citywide spatio-temporal flow volumes. International Journal of Geographical Information Science 34, 4 (2020), 802â€“823.\n[149] Rafael Reuveny. 2007. Climate change-induced migration and violent conflict. Political geography 26, 6 (2007), 656â€“673.\n[150] S. Rinzivillo, L. Gabrielli, M. Nanni, L. Pappalardo, D. Pedreschi, and F. Giannotti. 2014. The purpose of motion: Learning activities\nfrom Individual Mobility Networks. In 2014 International Conference on Data Science and Advanced Analytics (DSAA). 312â€“318.\n[151] Maria Riveiro, Giuliana Pallotta, and Michele Vespe. 2018. Maritime anomaly detection: A review. Wiley Interdisciplinary Reviews: Data\nMining and Knowledge Discovery 8, 5 (2018), e1266.\n[152] Can Rong, Jie Feng, and Yong Li. 2019. Deep learning models for population flow generation from aggregated mobility data. In Adjunct\nProceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM\nInternational Symposium on Wearable Computers. 1008â€“1013.\n[153] Alberto Rossi, Gianni Barlacchi, Monica Bianchini, and Bruno Lepri. 2019. Modelling Taxi Driversâ€™ Behaviour for the Next Destination\nPrediction. IEEE Transactions on Intelligent Transportation Systems (2019).\n[154] Alessio Rossi, Luca Pappalardo, Paolo Cintia, F. Marcello Iaia, Javier FernÃ¡ndez, and Daniel Medina. 2018. Effective injury forecasting\nin soccer with GPS training data and machine learning. PLOS ONE 13, 7 (2018), 1â€“15.\n[155] Luca Rossi, Matthew J. Williams, Christopher Stich, and Mirco Musolesi. 2015. Privacy and the City: User Identification and Location\nSemantics in Location-Based Social Networks. In Proceedings of the 9th International AAAI Conference on Web and Social Media.\n[156] N. W. Ruktanonchai, J. R. Floyd, S. Lai, C. W. Ruktanonchai, A. Sadilek, P. Rente-Lourenco, X. Ben, A. Carioli, J. Gwinn, J. E. Steele, O.\nProsper, A. Schneider, A. Oplinger, P. Eastham, and A. J. Tatem. 2020. Assessing the impact of coordinated COVID-19 exit strategies\nacross Europe. Science 369, 6510 (2020), 1465â€“1470.\n[157] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning Internal Representations by Error Propagation. MIT Press, 318â€“362.\n[158] T. Russo, L. Dâ€™Andrea, A. Parisi, M. Martinelli, A. Belardinelli, F. Boccoli, I. Cignini, M. Tordoni, and S. Cataudella. 2016. Assessing the\nfishing footprint using data integrated from different tracking devices: Issues and opportunities. Ecological Indicators 69 (2016), 818 â€“\n827.\n[159] BA Sabarish, R Karthi, and T Gireeshkumar. 2015. A survey of location prediction using trajectory mining. In Artificial Intelligence and\nEvolutionary Algorithms in Engineering Systems. Springer, 119â€“127.\n[160] Christian M. Schneider, Vitaly Belik, Thomas CouronnÃƒÂ©, Zbigniew Smoreda, and Marta C. GonzÃƒÂ¡lez. 2013. Unravelling daily human\nmobility motifs. Journal of The Royal Society Interface 10, 84 (2013), 20130246.\n[161] M. Schuster and K. K. Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing 45, 11 (1997),\n2673â€“2681.\n[162] Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu. 2017. Interpretable convolutional neural networks with dual local and global\nattention for review rating prediction. In Proceedings of the Eleventh ACM Conference on Recommender Systems. 297â€“305.\n[163] Alex Sherstinsky. 2020. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D:\nNonlinear Phenomena 404 (2020), 132306.\n[164] Yan Shi, Haoran Feng, Xiongfei Geng, Xingui Tang, and Yongcai Wang. 2019. A Survey of Hybrid Deep Learning Methods for Traffic\nFlow Prediction. In Proceedings of the 2019 3rd International Conference on Advances in Image Processing. Association for Computing\nMachinery, 133â€“138.\n[165] Seungjae Shin, Hongseok Jeon, Chunglae Cho, Seunghyun Yoon, and Taeyeon Kim. 2020. User Mobility Synthesis based on Generative\nAdversarial Networks: A Survey. In 2020 22nd International Conference on Advanced Communication Technology (ICACT). IEEE, 94â€“103.\n[166] Filippo Simini, Gianni Barlacchi, Massimiliano Luca, and Luca Pappalardo. 2020. Deep Gravity: enhancing mobility flows generation\nwith deep neural networks and geographic information. arXiv:cs.LG/2012.00489\n[167] Filippo Simini, Marta C. GonzÃ¡lez, Amos Maritan, and Albert-LÃ¡szlÃ³ BarabÃ¡si. 2012. A universal model for mobility and migration\npatterns. Nature 484, 7392 (2012), 96â€“100.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n28\nâ€¢\nLuca et al.\n[168] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. In Proceedings\nof the 3rd International Conference on Learning Representations, Yoshua Bengio and Yann LeCun (Eds.).\n[169] Alina SÃ®rbu, Gennady Andrienko, Natalia Andrienko, Chiara Boldrini, Marco Conti, Fosca Giannotti, Riccardo Guidotti, Simone Bertoli,\nJisu Kim, Cristina Ioana Muntean, et al. 2020. Human migration: the big data perspective. International Journal of Data Science and\nAnalytics (2020), 1â€“20.\n[170] B. H. Soleimani, E. N. De Souza, C. Hilliard, and S. Matwin. 2015. Anomaly detection in maritime data based on geometrical analysis of\ntrajectories. In 2015 18th International Conference on Information Fusion (Fusion). 1100â€“1105.\n[171] G. Solmaz and D. Turgut. 2019. A Survey of Human Mobility Models. IEEE Access 7 (2019), 125711â€“125731.\n[172] Chaoming Song, Tal Koren, Pu Wang, and Albert-LÃ¡szlÃ³ BarabÃ¡si. 2010. Modelling the scaling properties of human mobility. Nature\nPhysics 6, 10 (2010), 818â€“823.\n[173] Chaoming Song, Zehui Qu, Nicholas Blumm, and Albert-LÃ¡szlÃ³ BarabÃ¡si. 2010. Limits of predictability in human mobility. Science 327,\n5968 (2010), 1018â€“1021.\n[174] H. Y. Song, M. S. Baek, and M. Sung. 2019. Generating Human Mobility Route Based on Generative Adversarial Network. In 2019\nFederated Conference on Computer Science and Information Systems (FedCSIS). 91â€“99.\n[175] Xuan Song, Quanshi Zhang, Yoshihide Sekimoto, Ryosuke Shibasaki, Nicholas Jing Yuan, and Xing Xie. 2016. Prediction and Simulation\nof Human Mobility Following Natural Disasters. ACM Transactions on Intelligent Systems and Technologies 8, 2 (2016).\n[176] Victor Soto, Vanessa Frias-Martinez, Jesus Virseda, and Enrique Frias-Martinez. 2011. Prediction of socioeconomic levels using cell\nphone records. In International Conference on User Modeling, Adaptation, and Personalization. 377â€“388.\n[177] University of Maryland Start. 2009. Global Terrorism Database. http://www.start-dev.umd.edu/gtd/\n[178] Iain D Stewart, Chris A Kennedy, Angelo Facchini, and Renata Mele. 2018. The electric city as a solution to sustainable urban\ndevelopment. Journal of Urban Technology 25, 1 (2018), 3â€“20.\n[179] Arkadiusz Stopczynski, Vedran Sekara, Piotr Sapiezynski, Andrea Cuttone, Mette My Madsen, Jakob Eg Larsen, and Sune Lehmann.\n2014. Measuring Large-Scale Social Networks with High Resolution. PLOS ONE 9, 4 (2014), 1â€“24.\n[180] Junkai Sun, Junbo Zhang, Qiaofei Li, Xiuwen Yi, Yuxuan Liang, and Yu Zheng. 2020. Predicting citywide crowd flows in irregular\nregions using multi-view graph convolutional networks. IEEE Transactions on Knowledge and Data Engineering (2020).\n[181] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural\ninformation processing systems. 3104â€“3112.\n[182] Jinjun Tang, Jian Liang, Tianjian Yu, Yong Xiong, and Guoliang Zeng. 2021. Trip destination prediction based on a deep integration\nnetwork by fusing multiple features from taxi trajectories. IET Intelligent Transport Systems (2021).\n[183] Chujie Tian, Xinning Zhu, Zheng Hu, and Jian Ma. 2020. Deep spatial-temporal networks for crowd flows prediction by dilated\nconvolutions and region-shifting attention mechanism. Applied Intelligence (2020), 1â€“14.\n[184] New York City TLC. 2009. New York City Taxi & Limousine Commission. https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n[185] Eran Toch, Boaz Lerner, Eyal Ben-Zion, and Irad Ben-Gal. 2019. Analyzing large-scale human mobility data: a survey of machine\nlearning methods and applications. Knowledge and Information Systems 58, 3 (2019), 501â€“523.\n[186] Marcello Tomasini, Basim Mahmood, Franco Zambonelli, Angelo Brayner, and Ronaldo Menezes. 2017. On the effect of human mobility\nto the design of metropolitan mobile opportunistic networks of sensors. Pervasive and Mobile Computing 38 (2017), 215 â€“ 232.\n[187] Jameson Toole, Carlos Herrera-Yague, Christian Schneider, and Marta C. Gonzalez. 2015. Coupling Human Mobility and Social Ties.\nJournal of the Royal Society Interface 12 (2015).\n[188] Alexander Toshev and Christian Szegedy. 2014. DeepPose: Human Pose Estimation via Deep Neural Networks. In Proceedings of the\n2014 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 1653â€“1660.\n[189] International Telecommunication Union. 2019. Measuring digital development Facts and figures. Technical Report. International\nTelecommunication Union.\n[190] WMO UNISDR. 2012. Disaster risk and resilience. Thematic Think Piece, UN System Task Force on the Post-2015 UN Development Agenda\n(2012).\n[191] Michele Vespe, Maurizio Gibin, Alfredo Alessandrini, Fabrizio Natale, Fabio Mazzarella, and Giacomo C. Osio. 2016. Mapping EU\nfishing activities using ship tracking data. Journal of Maps 12, sup1 (2016), 520â€“525.\n[192] Vasiliki Voukelatou, Lorenzo Gabrielli, Ioanna Miliou, Stefano Cresci, Rajesh Sharma, Maurizio Tesconi, and Luca Pappalardo. 2020.\nMeasuring objective and subjective well-being: dimensions and data sources. International Journal of Data Science and Analytics (2020).\n[193] Di Wang, Tomio Miwa, and Takayuki Morikawa. 2020. Big Trajectory Data Mining: A Survey of Methods, Applications, and Services.\nSensors 20, 16 (2020), 4571.\n[194] Jinzhong Wang, Xiangjie Kong, Feng Xia, and Lijun Sun. 2019. Urban human mobility: Data-driven modeling and prediction. ACM\nSIGKDD Explorations Newsletter (2019), 1â€“19.\n[195] Leye Wang, Xu Geng, Xiaojuan Ma, Feng Liu, and Qiang Yang. 2018. Cross-city transfer learning for deep spatio-temporal prediction.\narXiv preprint arXiv:1802.00386 (2018).\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n29\n[196] Senzhang Wang, Jiannong Cao, Hao Chen, Hao Peng, and Zhiqiu Huang. 2020. SeqST-GAN: Seq2Seq Generative Adversarial Nets for\nMulti-Step Urban Crowd Flow Prediction. ACM Transactions on Spatial Algorithms and Systems (TSAS) 6, 4 (2020), 1â€“24.\n[197] Senzhang Wang, Jiannong Cao, and Philip Yu. 2020. Deep learning for spatio-temporal data mining: A survey. IEEE Transactions on\nKnowledge and Data Engineering (2020).\n[198] Xingrui Wang, Xinyu Liu, Ziteng Lu, and Hanfang Yang. 2021. Large Scale GPS Trajectory Generation Using Map Based on Two Stage\nGAN. Journal of Data Science 19, 1 (2021), 126â€“141.\n[199] Yan Wang and John E Taylor. 2018. Coupling sentiment and human mobility in natural disasters: a Twitter-based study of the 2014\nSouth Napa Earthquake. Natural hazards 92, 2 (2018), 907â€“925.\n[200] Yuan Wang, Dongxiang Zhang, Ying Liu, Bo Dai, and Loo Hay Lee. 2019. Enhancing transportation systems via deep learning: A\nsurvey. Transportation research part C: emerging technologies 99 (2019), 144â€“163.\n[201] Billy M Williams and Lester A Hoel. 2003. Modeling and forecasting vehicular traffic flow as a seasonal ARIMA process: Theoretical\nbasis and empirical results. Journal of transportation engineering 129, 6 (2003), 664â€“672.\n[202] Jianxin Wu. 2017. Introduction to convolutional neural networks. National Key Lab for Novel Software Technology. Nanjing University.\nChina 5 (2017), 23.\n[203] Ruizhi Wu, Guangchun Luo, Junming Shao, L. Tian, and Chengzong Peng. 2018. Location prediction on trajectory data: A review. Big\nData Min. Anal. 1 (2018), 108â€“127.\n[204] Peng Xie, Tianrui Li, Jia Liu, Shengdong Du, Xin Yang, and Junbo Zhang. 2020. Urban flow prediction from spatiotemporal data using\nmachine learning: A survey. Information Fusion 59 (2020), 1â€“12.\n[205] SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. 2015. Convolutional LSTM network:\nA machine learning approach for precipitation nowcasting. In Advances in neural information processing systems. 802â€“810.\n[206] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show,\nattend and tell: Neural image caption generation with visual attention. In International conference on machine learning. 2048â€“2057.\n[207] Shuai Xu, Xiaoming Fu, Jiuxin Cao, Bo Liu, and Zhixiao Wang. 2020. Survey on user location prediction based on geo-social networking\ndata. World Wide Web (2020), 1â€“44.\n[208] Takahiro Yabe, Kota Tsubouchi, and Yoshihide Sekimoto. 2017. CityFlowFragility: Measuring the Fragility of People Flow in Cities to\nDisasters Using GPS Data Collected from Smartphones. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 3, Article 117 (Sept.\n2017), 17 pages. https://doi.org/10.1145/3130982\n[209] Takahiro Yabe, Kota Tsubouchi, Akihito Sudo, and Yoshihide Sekimoto. 2016. A Framework for Evacuation Hotspot Detection after\nLarge Scale Disasters Using Location Data from Smartphones: Case Study of Kumamoto Earthquake. In Proceedings of the 24th\nACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (Burlingame, California) (SIGSPACIAL â€™16).\nAssociation for Computing Machinery, New York, NY, USA, Article 44, 10 pages. https://doi.org/10.1145/2996913.2997014\n[210] Zhaojin Yan, Yijia Xiao, Liang Cheng, Rong He, Xiaoguang Ruan, Xiao Zhou, Manchun Li, and Ran Bin. 2020. Exploring AIS data for\nintelligent maritime routes extraction. Applied Ocean Research 101 (2020), 102271.\n[211] Bing Yang, Yan Kang, Hao Li, Yachuan Zhang, Yan Yang, and Lan Zhang. 2020. Spatio-temporal expand-and-squeeze networks for\ncrowd flow prediction in metropolis. IET Intelligent Transport Systems 14, 5 (2020), 313â€“322.\n[212] Dingqi Yang, Benjamin Fankhauser, Paolo Rosso, and Philippe Cudre-Mauroux. 2020. Location Prediction over Sparse User Mobility\nTraces Using RNNs: Flashback in Hidden States!. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial\nIntelligence, IJCAI-20. 2184â€“2190.\n[213] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. 2019. Revisiting User Mobility and Social Relationships in LBSNs: A\nHypergraph Embedding Approach. 2147â€“2157.\n[214] Di Yao, Chao Zhang, Jianhui Huang, and Jingping Bi. 2017. Serm: A recurrent model for next location prediction in semantic trajectories.\nIn Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. 2411â€“2414.\n[215] Huaxiu Yao, Xianfeng Tang, Hua Wei, Guanjie Zheng, and Zhenhui Li. 2019. Revisiting spatial-temporal similarity: A deep learning\nframework for traffic prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5668â€“5675.\n[216] Huaxiu Yao, Xianfeng Tang, Hua Wei, Guanjie Zheng, Yanwei Yu, and Zhenhui Li. 2018. Modeling spatial-temporal dynamics for\ntraffic prediction. arXiv preprint arXiv:1803.01254 (2018).\n[217] Xin Yao, Yong Gao, Di Zhu, Ed Manley, Jiaoe Wang, and Yu Liu. 2020. Spatial Origin-Destination Flow Imputation Using Graph\nConvolutional Networks. IEEE Transactions on Intelligent Transportation Systems (2020).\n[218] Dan Yin and Qing Yang. 2018. GANs based density distribution privacy-preservation on mobility data. Security and Communication\nNetworks 2018 (2018).\n[219] Xueyan Yin, Genze Wu, Jinze Wei, Yanming Shen, Heng Qi, and Baocai Yin. 2020. A Comprehensive Survey on Traffic Prediction.\narXiv preprint arXiv:2004.08555 (2020).\n[220] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In\nThirty-first AAAI conference on artificial intelligence.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n30\nâ€¢\nLuca et al.\n[221] Hao Yuan, Xinning Zhu, Zheng Hu, and Chunhong Zhang. 2020. Deep Multi-View Residual Attention Network for Crowd Flows\nPrediction. Neurocomputing (2020).\n[222] Chao Zhang, Jiawei Han, Lidan Shou, Jiajun Lu, and Thomas LLa Porta. 2014. Splitter: Mining fine-grained sequential patterns in\nsemantic trajectories. Proceedings of the VLDb Endowment 7, 9 (2014), 769â€“780.\n[223] Chao Zhang, Keyang Zhang, Quan Yuan, Luming Zhang, Tim Hanratty, and Jiawei Han. 2016. Gmove: Group-level mobility modeling\nusing geo-tagged social media. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining. 1305â€“1314.\n[224] Junbo Zhang, Yu Zheng, and Dekang Qi. 2017. Deep spatio-temporal residual networks for citywide crowd flows prediction. In\nThirty-First AAAI Conference on Artificial Intelligence.\n[225] Yahua Zhang, Anming Zhang, and Jiaoe Wang. 2020. Exploring the roles of high-speed train, air and coach services in the spread of\nCOVID-19 in China. Transport Policy 94 (2020), 34 â€“ 42. https://doi.org/10.1016/j.tranpol.2020.05.012\n[226] K. Zhao, S. Tarkoma, S. Liu, and H. Vo. 2016. Urban human mobility data mining: An overview. In 2016 IEEE International Conference\non Big Data (Big Data). 1911â€“1920.\n[227] Liang Zhao. 2020. Event Prediction in Big Data Era: A Systematic Survey. arXiv preprint arXiv:2007.09815 (2020).\n[228] Xin Zheng, Jialong Han, and Aixin Sun. 2018. A survey of location prediction on twitter. IEEE Transactions on Knowledge and Data\nEngineering 30, 9 (2018), 1652â€“1671.\n[229] Yu Zheng. 2011. T-Drive trajectory data sample.\n[230] Yu Zheng. 2015. Trajectory data mining: an overview. ACM Transactions on Intelligent Systems and Technology (TIST) (2015), 1â€“41.\n[231] Yu Zheng, Licia Capra, Ouri Wolfson, and Hai Yang. 2014. Urban computing: concepts, methodologies, and applications. ACM\nTransactions on Intelligent Systems and Technology (TIST) 5, 3 (2014), 1â€“55.\n[232] Yu Zheng, Xing Xie, Wei-Ying Ma, et al. 2010. GeoLife: A collaborative social networking service among user, location and trajectory.\nIEEE Data Eng. Bull. 33, 2 (2010), 32â€“39.\n[233] Yirong Zhou, Hao Chen, Jun Li, Ye Wu, Jiangjiang Wu, and Luo Chen. 2019. ST-Attn: Spatial-Temporal Attention Mechanism for\nMulti-step Citywide Crowd Flow Prediction. In 2019 International Conference on Data Mining Workshops (ICDMW). IEEE, 609â€“614.\n[234] Yirong Zhou, Ye Wu, Jiangjiang Wu, Luo Chen, and Jun Li. 2018. Refined Taxi Demand Prediction with ST-Vec. In 2018 26th International\nConference on Geoinformatics. IEEE, 1â€“6.\n[235] Wen-Yuan Zhu, Wen-Chih Peng, Ling-Jyh Chen, Kai Zheng, and Xiaofang Zhou. 2015. Modeling User Mobility for Location Promotion\nin Location-Based Social Networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining. 1573â€“1582.\n[236] George Kingsley Zipf. 1946. The P 1 P 2/D hypothesis: on the intercity movement of persons. American sociological review 11, 6 (1946),\n677â€“686.\n[237] Ali Zonoozi, Jung-jae Kim, Xiao-Li Li, and Gao Cong. 2018. Periodic-CRN: A Convolutional Recurrent Model for Crowd Density\nPrediction with Recurring Periodic Patterns.. In IJCAI. 3732â€“3738.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n31\nA\nSPATIAL TESSELLATIONS\nSpatial tessellations may be computed using several open-source tools. Figure 6 shows examples of tessellations\nin New York City.\nLibrary scikit-mobility [135] allows for creating a squared spatial tessellation, given a reference geographic\narea and a tile size expressed in meters.\nS2 Geometry is an open-source project that represents spatial data on a three-dimensional sphere. It provides\nefficient and scalable spatial indexing techniques to carry out operations such as testing relationships among\nobjects, measuring centroids, distances, and more. S2 Geometry decomposes the unit sphere into a hierarchy of\ncells (tiles), each of which is a quadrilateral bounded by four geodesics. The top level of the hierarchy is obtained\nby projecting the six faces of a cube into the unit sphere, and lower levels are obtained by subdividing each cell\ninto four sub-cells recursively. Each cell in the hierarchy has a level, defined as the number of times the cell has\nbeen subdivided (starting with a face cell). Cellsâ€™ levels range from 0 to 30. The smallest cells at level 30 are called\nleaf cells; there are 6 Ã— 430 cells in total, each about 1cm across on the Earthâ€™s surface.\nThe H3 geospatial indexing system consists of a hexagonal tiling of the sphere with hierarchical indexes.\nThe hexagonal grid system is created on the planar faces of a sphere-circumscribed icosahedron, and the grid\ncells are then projected to the surface of the sphere using a specific projection. The H3 grid is constructed by\nrecursively creating increasingly higher precision hexagon grids until the desired resolution is achieved. The first\nH3 resolution (resolution 0) consists of 122 base cells, and each subsequent resolution is created splitting each\ncell into seven children recursively. H3 provides 15 finer grid resolutions in addition to resolution 0. The finest\nresolution, resolution 15, has cells with an area of less than 1ð‘š2.\nUber H3\nAdministrative\nSquared\nS2 Geometry\nFig. 6. Examples of spatial tessellations constructed over New York City. On the left, the city is tessellated in hexagons using\nH3 (resolution 6). In the second image, the area is split according to the administrative boundaries described in the GeoJSON\nfile downloaded from New York Cityâ€™s open data portal. In the third image, we use scikit-mobility to split an area into a\nsquared grid with tiles of 1kmÃ—1km. In the last image, we make a tessellation using S2 Geometry.\nB\nDEEP LEARNING MODULES\nB.1\nRecurrent Neural Networks\nA fully connected neural network (FC) consists of a series of fully-connected layers. All the neurons in one layer\nare connected to the neurons in the next layer. An FC layer, therefore, is a function that, given an input ð‘¥âˆˆRð‘š,\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n32\nâ€¢\nLuca et al.\nmap values in Rð‘šâ†’Rð‘›where ð‘šand ð‘›are the number of neurons in two consecutive layers. FCs are universal\napproximators (i.e., can learn any representation function) [66] but they are not able to deal with sequential data.\nRecurrent Neural Networks (RNNs) [157] can efficiently deal with sequential data such as time series, in which\nvalues are ordered by time, or sentences in natural language, in which the order of the words is crucial to shaping\nits meaning. An RNN consists of a sequence of gates ðº= {ðº0, . . . ,ðºð‘›âˆ’1}, each one producing an hidden state â„Žð‘–\nbased on the current input ð‘¥ð‘–and the output from the previous gate â„Žð‘–âˆ’1 (Figure 7). In Vanilla RNNs, a gate ðºð‘–\nis implemented by using a hyperbolic tangent function (tanh), which takes as input ð‘¥ð‘–and â„Žð‘–âˆ’1 and computes\nthe current state â„Žð‘–. RNNs suffer from the vanishing gradient problem [98] and cannot propagate information\nfound at early steps, losing relevant information at the beginning of a sequence when it is time to analyze its\nend [98]. Long-short-term Memory networks (LSTMs) [75] and Gated Recurrent Units (GRUs) [35] are two gate\nimplementations that mitigate this problem (Figure 7).\nGi\nxi\nhi\nhi-1\n=\nG0\nx0\nh0\nx2\nh2\nGn\nxn\nhn\nh0\nhn-1\nG1\nx1\nh1\nh1\nt\nt\ntanh\ns\ns\ns\ns\nt\nt\n+\nX\ns\nX\nX\nt\n+\nX\nX\n-1\nX\ns\nt\nX\n+\nSigmoidÂ \nTanh\nPointwise\nmultiplication\nPointwise sum\nVector\nconcatenation\nci-1\nhi-1\nxi\nxi\nhi-1\nri\nzi\nhi\nhi\n_\nci\nhi\noi\nini\nfi\nci\n_\nci-1\n*\nLSTM\nGRU\nhi-1\n*\ncell state\nforget gate\ninput gate\noutput gate\nreset gate\nupdate gate\nFig. 7. (Top) An example of a Recurrent Neural Network (RNN). The right term of the equation corresponds to the unrolled\nversion of the network on the left term. In general, an RNN takes as input a sequence ð‘‹= âŸ¨ð‘¥0,ð‘¥1, . . . ð‘¥ð‘›âŸ©and produces an\noutput â„Žð‘–that, at a certain moment ð‘–, is based on the current input ð‘¥ð‘–and the output of the previous gate â„Žð‘–âˆ’1. Usually, in\nVanilla RNNs, the tanh function (green circle) is used to combine the current input ð‘¥ð‘–and the previous hidden state â„Žð‘–âˆ’1.\n(Bottom) The structure of an LSTM gate (left) and a GRU gate (right). In the LSTM gate, we highlight with a dashed line the\ncell state (in blue), the forget gate (in orange), the input gate (in green), and the output gate (in purple). In the GRU gate, we\nhighlight with the dashed line the reset gate (in red) and the update gate (in green).\nIn LSTMs, the cell state ð‘ð‘–carries the data through the network, while the internal gates remove useless\ninformation from the flow or add relevant knowledge to the cell state. There are three internal gates: forget,\ninput, and output gate (Figure 7). The forget gate passes the information of the previous hidden state â„Žð‘–âˆ’1 and\nof the current input ð‘¥ð‘–to a sigmoid activation function which ranges in [0, 1]. The closer the sigmoidâ€™s output\n(ð‘“ð‘–) is to 0, the less likely the information is to be considered relevant. The sigmoidâ€™s output ð‘“ð‘–is then pointwise\nmultiplied with previous cell state ð‘ð‘–âˆ’1 forming ð‘âˆ—\nð‘–âˆ’1. Values of ð‘âˆ—\nð‘–âˆ’1 close to 0 are not taken into consideration by\nthe current cell state. The input gate passes â„Žð‘–âˆ’1 and ð‘¥ð‘–to a sigmoid activation function and to a tanh activation\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n33\nfunction. The sigmoid function outputs ð‘–ð‘›ð‘–, i.e., the relevance of the new data (0 is not relevant; 1 is relevant).\nThe tanh outputs, ð‘ð‘–âˆˆ[âˆ’1, 1], is multiplied with ð‘–ð‘›ð‘–, and the resulting product is pointwise summed with ð‘âˆ—\nð‘–âˆ’1\nto obtain the new cell state ð‘ð‘–. The output gate generates the hidden state â„Žð‘–. The previous hidden state â„Žð‘–âˆ’1\nand the current input ð‘¥ð‘–are passed to a sigmoid activation function, which generates ð‘œð‘–. This value is pointwise\nmultiplied with the output of a tanh applied on the new cell state ð‘ð‘–, to form the new hidden state â„Žð‘–. The hidden\nstate â„Žð‘–and the cell state ð‘ð‘–are passed to the next gate of the recurrent network. In some works, bidirectional\nLSTMs (Bi-LSTMs) [161] are adopted instead of LSTMs. Bi-LSTMs duplicate the recurrent layer: the sequence is\nprovided as input to one layer and its reverse as input to the other layer. Analyzing time dependencies in both\ndirections gives an advantage in scenarios like speech recognition, in which the context is essential to interpret\nthe meaning of a sentence [68, 69].\nIn GRUs, the relevant information is propagated throughout the network using hidden states only (Figure 7).\nGRUs have two types of internal gates. The reset gate decides how much to forget and consists of a sigmoid\nfunction that takes as input the hidden layer of the previous step â„Žð‘–âˆ’1 and the current input ð‘¥ð‘–. The output of the\nsigmoid, ð‘Ÿð‘–, is pairwise multiplied with â„Žð‘–âˆ’1, generating â„Žâˆ—\nð‘–âˆ’1. Depending on â„Žâˆ—\nð‘–âˆ’1, we can determine which past\ninformation to forget (low values) or keep (high values). The update gate establishes whether the past information\nis relevant for future predictions and, therefore, should be propagated to the next steps. It is composed of a\nsigmoid function that takes as input â„Žð‘–âˆ’1 and ð‘¥ð‘–and outputs ð‘§ð‘–. The next hidden state, â„Žð‘–, is obtained by: (i)\ncomputing Ë†â„Žð‘–, which is the output of a tanh function of â„Žâˆ—\nð‘–âˆ’1 and the current input ð‘¥ð‘–; (ii) multiplying 1 âˆ’ð‘§ð‘–with\nâ„Žð‘–âˆ’1; and (iii) adding the resulting sum to the product between ð‘§ð‘–and Ë†â„Žð‘–.\nFurther mathematical details and formalizations can be found in [163] for RNNs and LSTMs and [36] for GRUs.\nRNNs are widely used in next-location prediction (Section 3.1), often in combination with attention mechanisms,\nto capture the temporal relationships in individual trajectories. Moreover, RNNs are often combined with\nconvolutional neural networks in crowd flow prediction (Section 3.2) and trajectory generation (Section 4.1), to\ncapture temporal and spatial patterns at the same time.\nB.2\nConvolutional Neural Networks\nConvolutional Neural Networks (CNNs) are widely used in computer vision for their efficacy in object recognition\n[147, 168], image classification and segmentation [53, 103], movement or event recognition [188], and more [95].\nSimilarly to the visual cortex [78, 79], a CNN is a network made of neurons that react only to certain stimuli in a\nrestricted region of the visual field [103].\nCNNs alternate two types of layers: (i) the convolutional layers reduce the size of the matrix by applying a\nkernel function, or filter, that keeps all the relevant information; (ii) the pooling layers reduce the spatial size of\nthe convoluted features to decrease the computational power required to process the data. Figure 8 shows an\nexample of a CNN architecture with two convolutional layers and two pooling layers. Usually, at the end of the\nCNN, an FC is used to compute the output.\nThe convolutional layers apply one or more filters to the input matrix ð´to extract relevant features and\nsummarize the characteristics of an ð‘–Ã— ð‘—area of ð´into a single value. Specifically, given an input matrix ð´of size\nð‘›Ã— ð‘š, a filter is a mathematical operation between the original matrix ð´and another matrix ðµof size ð‘˜Ã— ð‘™, with\nð‘˜< ð‘›and ð‘™< ð‘š. The filter is generally applied with a sliding window mechanism called stride.\nThe pooling layers aim at downsampling the input matrix, replacing each portion of it with summary statistics.\nIn practice, pooling layers are either max pooling or average pooling. Max pooling returns the maximum value\nfrom the part of the matrix convoluted by a given size and stride filter. Average pooling returns the average\nof the values produced by the filter operations. Pooling layers create representations of the matrix that are\napproximately invariant to small translations of the input.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n34\nâ€¢\nLuca et al.\nInput\nConvolution\nPooling\nConvolution\nPooling\nScore\n0.14\n0.73\n...\n0.19\n0.42\nÂ \nFC\nFig. 8. Example of architecture of a CNN. The neural network consists of two convolutions and two pooling layers. After the\nconvolution/pooling layers, there is an FC that outputs the prediction. For instance, to classify a sample into one of ð‘›classes,\nthe last layer is usually fed into an FC that outputs a 1 Ã— ð‘›vector containing the probabilities of the sample to belong to a\nspecific class.\nA significant limitation of CNNs, especially relevant for networks with many layers, is the vanishing gradient\nissue [98]. The so-called residual units are a solution to this problem [73]. A residual unit implements the skip\nconnection also known as identity connection. Given a network with ð‘™1, . . . ,ð‘™ð‘˜layers, the output of a layer ð‘™ð‘–is\nadded to the output of the layer ð‘™ð‘–+ð‘—so to preserve the loss of information. The variable ð‘—represents the skip size,\nand it is usually smaller than 4.\nA mathematical formalization of CNNs can be found in [202].\nDifferently from RNNs, where the input is processed sequentially, CNNs are not designed to process sequences,\nand they are used mainly in crowd flow prediction (Section 3.2): the evolution of the incoming (outgoing) flows\nwithin a city is represented as a sequence of matrices. CNNs are used to capture the dynamics of the spatial\ndependencies among the areas of a city. CNNs are often combined with RNNs to capture temporal dependencies\n(e.g., ConvLSTM [205]). CNNs are also used in trajectory generation (Section 4.1): real trajectories are represented\nas images and used to train a Generative Adversarial Network (GAN), in which two CNNs are used as generator\nand discriminator to generate realistic trajectories.\nB.3\nAttention Mechanism\nAttention mechanisms are based on the idea that, when dealing with a large amount of information, our brains\nfocus on the most significant parts and consider all the others as background information. Initially introduced\nfor natural language processing (e.g., machine translation [36, 181] and speech recognition tasks [38]), the\nusage of attention mechanisms rapidly extended to computer vision, healthcare, and recommendation systems\n[37, 162, 206]. Given the current input ð‘¥and a context, an attention mechanism produces a score for each element\nof ð‘¥. These scores are usually computed using adequate activation functions (e.g., softmax) and organized in a\nvector ð‘ . A context vector Ë†ð‘is computed as the pairwise multiplication between ð‘ and ð‘¥. The context plays a\ncrucial role in the establishment of which features should be the most important to the model.\nB.4\nGenerative Models\nDimensionality reduction consists in transforming data from a high-dimensional space to a low-dimensional one\nretaining important properties and information of the original data [102]. In general, this is done by employing\na module called encoder (i.e., a neural network), which reduces the space of the original features generating\na latent space; and a decoder (i.e., a neural network), which transforms the latent space back to the original\nspace. AutoEncoders (AEs) [102] reduce the reconstruction error while decoding. AEs are designed to encode\na sample always in the same way and, therefore, cannot be considered a generative model as they are not\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n35\nData\nTraining\nData\nDiscriminator\nGenerator\nG (z; Î˜(G))\nD (x; Î˜(D))\nx ~ pdata\nz ~ pz\nx ~ pg\nSynthetic / Real ?\nObjective\nFunction\nBackpropagation\nFig. 9. Visual representation of a Generative Adversarial Network (GAN). A GAN is composed of a generator ðº, and a\ndiscriminator ð·. The generator is a differentiable function ðº(ð‘§; Î˜(ðº)) which outputs the new data according to a distribution\nð‘ð‘”, where Î˜(ðº) are the parameters of the generative model. The discriminator represents a differentiable function ð·(ð‘¥; Î˜(ð·)),\nwhere Î˜(ð·) are the parameters of the discriminative model, which produces the probability that ð‘¥comes from the distribution\nof training data ð‘ð‘‘ð‘Žð‘¡ð‘Ž. The aim is to obtain a generator that is a good estimator of ð‘ð‘‘ð‘Žð‘¡ð‘Ž. When this occurs, the discriminator\nis \"fooled\" and can no longer distinguish the samples from ð‘ð‘‘ð‘Žð‘¡ð‘Žfrom those from ð‘ð‘”.\ncapable of decoding (i.e., generate) data with different charateristics with repsect to the original data. Variational\nAutoEncoders (VAEs) [96] solve this problem by encoding a sample as a distribution instead of a set of fixed\nvalues. In this way, the decoder can be used to generate data that are similar to the original ones (e.g., realistic as\nthey should follow similar patterns) but diverse.\nGenerative adversarial networks (GANs) [67] are an example of generative models, i.e., any model that takes\na training set, consisting of samples drawn from a distribution, and learns to represent an estimate of that\ndistribution. A generative mobility model can generate synthetic spatio-temporal trajectories that realistically\nreproduce mobility patterns. Recently, GANs are being used to generate synthetic mobility trajectories [115, 128].\nThe basic idea of a GAN is to set up a game between a generator (e.g., a neural network) and a discriminator\n(e.g. a classifier) (Figure 9). The generator creates samples from the distribution ð‘ð‘”that are intended to come\nfrom the same distribution as the training data ð‘ð‘‘ð‘Žð‘¡ð‘Žand hence similar to the original ones. The discriminator\nexamines the generated samples to determine whether they are real or fake. In other words, the generator is\ntrained to fool the discriminator by generating samples that are indistinguishable from real ones.\nMathematically, the generator and the discriminator are two functions that are differentiable with respect to\nboth the inputs and the parameters. The discriminator is a function ð·that takes ð‘¥(e.g., a trajectory) as input\nand uses Î˜(ð·) as parameters. The generator is a function ðºthat takes ð‘§(e.g., set of trajectories) as input and\nuses Î˜(ðº) as parameters. ð·wishes to minimize a cost function ð½(ð·)(Î˜(ð·), Î˜(ðº)) while controlling only Î˜(ð·).\nSimilarly, ðºwishes to minimize a cost function ð½(ðº)(Î˜(ð·), Î˜(ðº)) while controlling only Î˜(ðº). This scenario is a\ngame the solution of which is a Nash equilibrium [66], i.e., a tuple (Î˜(ð·), Î˜(ðº)) that is a local minimum of ð½(ð·)\nwith respect to Î˜(ð·) and a local minimum of ð½(ðº) with respect to Î˜(ðº).\nThe training process consists of two simultaneous Stochastic Gradient Descent (SGD) [66]. SGD is a stochastic\napproximation of gradient descent optimization that replaces the actual gradient with an estimate, thus achieving\nfaster iterations in trade for a lower convergence rate. Two gradient steps are made simultaneously: one updating\nÎ˜(ð·) to reduce ð½(ð·) and one updating Î˜(ðº) to reduce ð½(ðº). Usually, the cost function used for the discriminator\nand the generator is the cross-entropy [66], defined as: ð»(ð‘ƒ,ð‘„) = âˆ’ð¸ð‘ƒ[ð‘™ð‘œð‘”ð‘„], where ð‘ƒ,ð‘„are two distributions\nand ð¸ð‘ƒis the expected value of ð‘™ð‘œð‘”ð‘„according to ð‘ƒ.\nA formalization of the concept of GAN can be found in [67]\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n36\nâ€¢\nLuca et al.\nC\nDATA\nThe last decade has witnessed the emergence of massive datasets of digital traces that portray human movements\nat an unprecedented scale and detail. Examples include tracks generated by GPS devices embedded in smartphones\n[232] or private vehicles [133]; mobile phone records [18]; and geotagged posts from social media platforms [125].\nUnfortunately, most of these datasets are proprietary and not publicly available, making research on human\nmobility hard to reproduce. In this Section, we discuss the peculiarities of various mobility data sources and, for\neach of them, provide a reference to a list of public datasets commonly used to train and test models presented in\nthis survey (Table 5).\nC.1\nMobile Phone Records\nMobile phones are ubiquitous, with coverage in most countries that reaches almost 100% of the population\n[18, 189]. Telco companies record the activity of mobile phone users for billing and operational purposes, hence\nstoring an enormous amount of information on where, when, and with whom users communicate [18]. Every\ntime a user engages a telecommunication interaction â€“ calls, text messages, data connections â€“ the operator\nassigns a Radio Base Station (RBS) to deliver the communication through the network. Since the position and the\ncoverage area of each RBS are known, a userâ€™s telecommunication interaction reflects their geographic location.\nEach interaction generates several mobile data formats [18, 131].\nEvery time a user makes/receives a call or sends/receives an SMS, a new Call Detail Record (CDR) is created.\nA CDR is a tuple (uð‘œ, uð‘–,ð‘¡,ð´ð‘œ,ð´ð‘–,ð‘‘), where uð‘œand uð‘–are the identifiers of the caller and the callee, and ð‘¡is a\ntimestamp of when the call starts. In turn, ð´ð‘œand ð´ð‘–are the RBSs that manage the outgoing call and the receiving\ncall, and ð‘‘is the call duration. An individualâ€™s mobility can be reconstructed from CDRs assuming a movement\nbetween the RBSs of any two consecutive records [18]. Aggregated mobility, such as flows, can be inferred by\ncounting the number of users that move, in a given time window, between two RBSs or spatial aggregations\nof them (e.g., neighborhoods or municipalities) [24]. CDRs are sparse in time, i.e., a userâ€™s position is known\nwhen they make or receive a call or a text message only, leading to sparse and incomplete mobility trajectories.\nNotwithstanding, they are the most common format in human mobility studies [10, 18, 40, 65, 136, 167].\nWhen a user uploads or downloads data from the Internet using their phoneâ€™s connection, they generate\nan eXtended Detail Record (XDR), a tuple (u,ð‘¡,ð´,ð‘˜) where ð´is the RBS that serves the connection, and ð‘˜the\namount of uploaded/downloaded information. XDRs are less common in the literature than CDRs, mainly because\nthe advent of the mobile data connection is relatively recent. XDRs partially overcome the problem of sparsity\npresent in CDRs [30], because mobile connections are more frequent than calls and text messages.\nFor both CDRs and XDRs, the spatial granularity is at the level of RBS, i.e., the user position is approximated\nwith the location of the RBS used for the telecommunication activity. This approximation implies that the userâ€™s\nposition within an RBSâ€™s coverage area is unknown and that user tracking depends on the spatial distribution of\nantennas on the territory, which depends on population density. Nevertheless, mobile phone data may cover a\nlarge sample size on a national scale. An advantage of mobile phone data is its multidimensionality: CDRs also\nprovide information about the social interactions between users; both CDRs and XDRs may be accompanied by\nsocio-demographic information about the users (e.g., age, sex).\nC.1.1\nAvailable datasets. Since mobile phone data contain sensitive information [45, 139], they are not typically\npublicly available, and any data collected by a specific group of researchers may not be shared with other groups,\nmaking reproducibility difficult.\nC.2\nGPS traces\nGlobal Navigation Satellite Systems (GNSS) use satellites to provide geo-spatial positioning, allowing electronic\nreceivers to determine their location (longitude, latitude, and altitude) and time, using signals transmitted along a\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n37\nline of sight from satellites. The US Global Positioning System (GPS) is the most popular one, and GPS receivers\nare ubiquitous in many tools of everyday life, such as mobile phones [5, 179, 232], vehicles [61, 133, 136], vessels\n[158, 170, 191], and wearable devices [154]. On mobile phones, GPS receivers are activated by apps that require\nthe userâ€™s position (e.g., Google Maps). On vehicles, GPS devices automatically turn on when the vehicle starts,\nsending positions to a server with a frequency of a few seconds. The precision of GPS receivers varies from a\nfew centimeters to meters, depending on the deviceâ€™s quality and the errors generated by the system [27]. GPS\ncannot track the devices in enclosed spaces, such as buildings and tunnels. A typical GPS trace is a set of tuples\n(ð‘¢,ð‘¡,ð‘™ð‘Žð‘¡,ð‘™ð‘›ð‘”) where ð‘¢is a user, ð‘¡is a timestamp of the measurement and ð‘™ð‘Žð‘¡,ð‘™ð‘›ð‘”are the positionâ€™s latitude and\nlongitude. GPS traces may require several preprocessing tasks aimed to mitigate errors and extract meaningful\nsemantics. For example, since GPS traces are dense sequences of spatio-temporal points, they do not explicitly\ndefine semantic locations, which must be inferred through specific preprocessing techniques [56, 230, 231].\nC.2.1\nAvailable datasets. GeoLife [232] is a publicly available dataset describing the GPS trajectories of 182 users\nover 4.5 years, collected using different devices (e.g., GPS receivers and mobile phones) every 1-5 seconds or 5-10\nmeters. GeoLife covers a broad range of usersâ€™ outdoor movements, from life routines like go home and go to\nwork to entertainment and sports activities. Each point in a GeoLifeâ€™s trajectory contains latitude, longitude,\naltitude, and the timestamp. Moreover, the userâ€™s transportation mode is also available for most of the trajectories.\nOther public datasets provide information about the trips of GPS-equipped taxis in several cities. Piorkowski\net al. [143] provide the trajectories, sampled on average every 10 seconds, of taxis in San Francisco, in May\n2008. Each point of a trajectory consists of the taxiâ€™s identifier, the latitude, the longitude, the timestamp, and\nthe occupancy. Moreira et al. [123] (ECML/PKDD Challenge) provide the trips of taxis in Porto, Portugal, in\nwhich points contain the latitude, the longitude and a timestamp indicating when the trip started. Data are\ncollected approximately every 15 seconds. For each trip, the dataset provides several auxiliary information, such\nas the tripâ€™s typology (e.g., dispatched from the central, demanded to the operator, requested to the driver), the\nstand from which the taxi departed, and an identifier of the passengerâ€™s phone number. The Taxi and Limousine\nCommission of New York City collected a dataset on yellow and green taxis operating in the city starting from\n2009 [184]. The dataset provides information on pick-up and drop-off dates/times and locations, trip distances,\nitemized fares, rate types, payment types, and driver-reported passenger counts.\nThe T-Drive dataset [229] describes the trajectories of about 10,000 taxis in Beijing, China, for one week. Points\nare sampled every 177 seconds and contain the taxiâ€™s identifier, the latitude, the longitude, and the timestamp.\nZhang et al. [224] create a squared tessellation on New York City and Beijing and, for each tile, provide the\nincoming and the outgoing flows. Beijingâ€™s flows are captured based on taxisâ€™ GPS signal; the New York City\nones are based on the cityâ€™s bike-sharing system.\nSimilarly, Jiang et al. [82] generated a tessellation of 450 Ã— 450 meters over Tokyo and Osaka, Japan and, for\neach tile, they provide people density and incoming and outgoing flows. The mobility is captured using GPS\ndevices.\nCiti Bike by Lyft [14] provides a dataset describing the trips between bike-sharing stations in New York\nCity. Each record describes the stations where the trip started and ended when it took place and the stationsâ€™\ncoordinates. There are similar open datasets for other cities, such as Washington DC [15].\nThe Mobile Data Challenge dataset (MDC) [106] describes the trajectories of 185 participants of a data collection\ncampaign in Lausanne, collected from mobile phones [97]. The dataset offers various information (e.g., calendar\nlogs, data from accelerometers), including two files describing the individualsâ€™ mobility through GPS receivers\nand Wireless Local Area Networks (WLANs). Each record in the first file describes the userâ€™s identifier, latitude,\nlongitude, altitude, timestamp, speed, heading direction, and accuracy. Records in the second file describe the\nuserâ€™s identifier, a timestamp, the first three bytes of the deviceâ€™s MAC address, and the access pointsâ€™ location.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n38\nâ€¢\nLuca et al.\nRecently, to take countermeasures for the COVID-19 pandemic, companies such as Cuebiq and SafeGraph are\nproviding free access upon request to their data.1\nC.3\nSocial Media Data\nUsersâ€™ posts on social media (e.g., photos, text, videos) can be associated with a geographic location and time\n(geotagged). The presence of spatial and temporal information allows the reconstruction of usersâ€™ trajectories\nfrom the sequence of published posts. Some platforms like Twitter provide either the postâ€™s precise geotag (i.e., a\nlatitude and longitude pair, a format recently removed) or the position of a predefined location suggested by the\nplatform (e.g., a city, an area, a restaurant). In other platforms like Foursquare and Facebook, users can check-in\nin predefined locations called venues, i.e., POIs that provide information about social, cultural, and infrastructural\ncomponents of a geographic area (e.g., cities, shops, museums). A venue is associated with a physical location\n(latitude and longitude pair) and textual information (a description of the place or the activities related to the\nplace) and can follow a hierarchical categorization that provides different levels of detail about the activities (e.g.,\nFood, Asian Restaurant, Chinese Restaurant) [12].\nIn general, a geotagged record describes the posting userâ€™s identifier, the resource identifier (e.g., post, tweet,\nphoto), the time of posting, and, depending on the platform, the venue identifier/category or a location as a string\n(e.g., Statue of Liberty, New York) or a latitude/longitude pair.\nFor most of the social media platforms, geotagged posts are downloadable through their APIs. APIs impose\nlimitations on the number of downloadable posts and queries per day or require authorization from the platformâ€™s\nusers to download the data. Usersâ€™ location is available only when they post something or check-in into a venue,\nleading to a data sparsity problem. Nevertheless, social media data bring the advantage that an objective definition\nof location is available, facilitating the data preprocessing phase [41].\nC.3.1\nAvailable datasets. Datasets about check-ins on social media platforms that are not active anymore, such\nas Gowalla and Brightkite, are freely available. Gowalla was a location-based social network platform in which,\nsimilarly to Foursquare, the users were allowed to check-in in the so-called spots (venues) through a website\nor the app. The related dataset [34] consists of more than six million check-ins over one year and a half from\nFebruary 2009 to October 2010. Each check-in describes the user identifier, the location identifier, the latitude\nand longitude pair, and a timestamp. The dataset also provides information about the usersâ€™ friendship network,\nwhich contains about 200,000 nodes and one million edges [34]. In Brightkite, another platform that is not active\nanymore, users were allowed to check-in in POIs, to specify who is nearby at the moment and who went to a\nPOI before. The dataset contains almost 4.5 million check-ins from April 2008 to October 2010 and the usersâ€™\nfriendship network with about 60,000 nodes and 220,000 edges [34].\nA dataset collected through Foursquare APIs is introduced in [54] and contains check-ins of 16,000 users over\none year in New York City.\nTwitter provides several open datasets, in which location is usually expressed as a semantic point of interest\neither suggested by the platform (e.g., Empire State Building, NYC) or typed by the users (e.g., Home), or as a\nlatitude and longitude pair. Geotagged tweets may be retrieved directly using Twitter APIs. For example, Zhang et\nal. [223] (GMove) provide a Twitter dataset describing 1.4 million tweets from August to November 2014 covering\nthe area of Los Angeles.\nA comprehensive list of Twitter datasets is available at github.com/shaypal5/awesome-twitter-data.\n1Cuebiq: cuebiq.com/visitation-insights-covid19/, SafeGraph: safegraph.com/covid-19-data-consortium. For instance, [91] is a recently\npublished dataset based on SafeGraph. It contains aggregated daily mobility flows at different spatial aggregations such as country to country,\nstate to state and census tract to census tract flows.\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n39\nRef.\nName\nItems\nTime span\nArea\nUsed By\nTask\nLink\n(https://bit.ly/)\nGPS traces\n[232]\nGeoLife\n182\n4.5 Years\nAsia\n[55]\nTraj. Gen.\nGeolife\n[229]\nT-Drive\n10k\n1 Week\nBeijing, China\n[80]\nFlow. Pred.\nT-Drive-Data\n[82]\nDeepCrowd\n-\n4 months\nTokyo\nand\nOsaka,\nJapan\n[82]\nCrowd Flow Pred.\nDeepCrowd\n[224]\nST-ResNet taxis\n-\n4, 6 months\nBeijing, China\n[47, 87, 109, 113, 148,\n180, 221, 224, 237]\nCrowd Flow Pred.\nST-ResNet\n[224]\nST-ResNet bikes\n-\n6 months\nNew York City, USA\n[47, 87, 109, 113, 148,\n180, 221, 224, 237]\nCrowd Flow Pred.\nST-ResNet\n[143]\nTaxi San Francisco\n500\n30 days\nSan Francisco, USA\n[49, 153, 218]\nNext-Loc., Traj. Gen.\nTaxiSF\n[123]\nECML-PKDD taxi\n441\n9 months\nPorto, Portugal\n[44, 49, 118, 153]\nNext-Loc.\nTaxiPorto\n[184]\nTaxi New York City\n-\nFrom 2009\nNew York City\n[153, 180, 183]\nNext-Loc.\nTaxiNYC-2\n[106]\nMDC\n185\n2 years\nLausanne,\nSwitzer-\nland\n[104, 129]\nTraj. Gen\nMDC-2\n[91]\nCOVID 19 US Flows\n-\nFrom 2019\nUnited States\n[166]\nFlow Gen.\nUSFlows\ncheck-ins\n[34]\nGowalla\n196k\n20 months\nCalifornia & Nevada,\nUSA\n[64, 114, 212]\nNext-Loc.\nGowallaData\n[34]\nBrightkite\n58k\n30 months\n-\n[64, 114, 212]\nNext-Loc.\nBrightkite\n[54]\nDeepMove\n16k\n1 Year\nNew York City\n[54]\nNext-Loc.\nDeepMove\n[223]\nGMove\n1.4M\n4 Months\nLos Angeles\n[214]\nNext-Loc.\nSERM-Repo\n[14]\nNew York City bikes\n-\nfrom 2013\nNew York City, USA\n[112, 180, 183, 216]\nCrowd Flow Pred.\nBikeNYCData\n[15]\nWashington\nDC\nbikes\n-\nfrom 2010\nWashington\nDC\n,\nUSA\n[180]\nCrowd Flow Pred.\nBikeWashington\nTable 5. Public mobility datasets used in the selected papers. For each dataset, we provide a reference to the paper introducing\nit, the number of items (users or points) in the dataset (symbol â€œ-â€ indicates that the dataset is aggregated, that the number\nis not available, or that the dataset is continuously updated), its time span, the geographic area covered, the list of selected\npapers that use it, the mobility tasks the dataset is used for, and the link to download it.\nD\nEVALUATION METRICS\nD.1\nDistance metrics\nThe Haversine distance is the distance on the spherical earth2 between two points ð‘1 and ð‘2:\nð‘‘â„Ž(ð‘1, ð‘2) = 2ð‘…\n âˆšï¸„\nð‘Ž(ð‘1, ð‘2)\nð‘Ž(ð‘1, ð‘2) âˆ’1\n!\n;\nð‘Ž(ð‘1, ð‘2) = ð‘ ð‘–ð‘›2\n\u0012ðœ™2 âˆ’ðœ™1\n2\n\u0013\n+ ð‘ð‘œð‘ (ðœ™1)ð‘ð‘œð‘ (ðœ™2)ð‘ ð‘–ð‘›2\n\u0012ðœ†2 âˆ’ðœ†1\n2\n\u0013\n(3)\nwhere ð‘…is the earth radius and ðœ†ð‘–and ðœ™ð‘–, with ð‘–= 1, 2, are the longitude and the latitude of ð‘ð‘–, respectively. The\nHaversine distance range in [0, âˆž] and lower values indicate better performance. The equirectangular distance is\ndefined as:\nð‘‘ð‘’ð‘ž(ð‘1, ð‘2) = ð‘…\nâˆšï¸„\u0012\n(ðœ†ð‘2 âˆ’ðœ†ð‘1)ð‘ð‘œð‘ \n\u0012ðœ™ð‘2 âˆ’ðœ™ð‘1\n2\n\u0013\u00132\n+ (ðœ™ð‘2 âˆ’ðœ™ð‘1)2\n(4)\nwhere ðœ†ð‘ð‘–and ðœ™ð‘ð‘–are the longitude and latitude of point ð‘ð‘–, respectively.\nD.2\nClassification metrics\nAccuracy (ACC) indicates how many of the locations an individual will visit are correctly predicted. The k-\naccuracy (ACC@k) is often used instead of ACC: predictors output a list of all possible locations an individual\ncan visit next, ranked from the most to the least likely, and ACC@k is the fraction of times the real location\n2Flat earthers can simply use the Euclidean distance, defined as follows: given two points ð‘1\n=\n(ð‘¥1, ð‘¦1) and ð‘2\n=\n(ð‘¥2, ð‘¦2),\nð‘‘flatearth=\nâˆšï¸\n(ð‘¥1 âˆ’ð‘¥2)2 + (ð‘¦1 âˆ’ð‘¦2)2\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n40\nâ€¢\nLuca et al.\nis among the ð‘˜most likely locations predicted by the model, i.e., the percentage that a list of predictions with\nlength ð‘˜covers the ground truth location (ACC = ACC@1).\nPrecision measures how accurate the predictor is on the positive class. Recall measures the True Positive\nRate (TPR), i.e., the fraction of positive instances correctly predicted by the model. F1-score summarizes the\nperformance of a model and it is computed as the harmonic mean of Precision and Recall. Given the number of\nTrue Positives (TPs), False Positives (FPs), and False Negatives (FNs), we define Precision, Recall, and F1-Score as:\nð‘ƒð‘Ÿð‘’ð‘ð‘–ð‘ ð‘–ð‘œð‘›=\nð‘‡ð‘ƒ\n(ð‘‡ð‘ƒ+ ð¹ð‘ƒ) ;\nð‘…ð‘’ð‘ð‘Žð‘™ð‘™=\nð‘‡ð‘ƒ\n(ð‘‡ð‘ƒ+ ð¹ð‘) ;\nð¹1 = 2ð‘ƒð‘Ÿð‘’ð‘ð‘–ð‘ ð‘–ð‘œð‘›Ã— ð‘…ð‘’ð‘ð‘Žð‘™ð‘™\nð‘ƒð‘Ÿð‘’ð‘ð‘–ð‘ ð‘–ð‘œð‘›+ ð‘…ð‘’ð‘ð‘Žð‘™ð‘™\n(5)\nThe Receiver Operating Characteristic Curve (ROC) visualizes a classifierâ€™s performance by plotting the Recall\nagainst the False Positive Rate (FPR). FPR is the ratio of FPs over the sum of FPs and True Negatives (TNs).\nAUC (âˆˆ[0, 1]) measures the area under the ROC. It is scale-invariant and provides an aggregate measure of\nperformance across all possible classification thresholds. The higher the AUC, the better the model, where\nAUC=0.50 indicates the performance of a model that makes predictions at random.\nD.3\nError metrics\nCommonly used error metrics are Mean Average Error (MAE), Mean Squared Error (MSE), Root Mean Squared\nError (RMSE), and MAPE (Mean Average Percentage Error), defined as follows:\nMAE = 1\nð‘›\nð‘›\nâˆ‘ï¸\nð‘–=1\n|ð‘¦ð‘–âˆ’Ë†ð‘¦ð‘–|;\nMSE = 1\nð‘›\nð‘›\nâˆ‘ï¸\nð‘–=1\n(ð‘¦ð‘–âˆ’Ë†ð‘¦ð‘–)2;\nRMSE =\nv\nt\n1\nð‘›\nð‘›\nâˆ‘ï¸\nð‘–=1\n(ð‘¦ð‘–âˆ’Ë†ð‘¦ð‘–)2;\nMAPE =\n \n1\nð‘›\nð‘›\nâˆ‘ï¸\nð‘–=1\n|ð‘¦ð‘–âˆ’Ë†ð‘¦ð‘–|\n|ð‘¦ð‘–|\n!\nâˆ—100\n(6)\nwhere Ë†ð‘¦ð‘–indicates the predicted value, ð‘¦ð‘–indicates the actual value, and ð‘›is the number of predictions. All metrics\nrange in [0, âˆž] and lower values indicate better performance. Since MAE uses the errorâ€™s absolute value, it does\nnot consider whether the model overestimates or underestimates the actual value. MSE weighs large errors more\nthan MAE, and it is sensitive to outliers. RMSE weighs the errors more than MAE, hence penalizing models that\nproduce large errors. As the values are squared, the RMSE is expressed in the same unit as the predicted one.\nThe SÃ¸rensen-Dice index, also called Common Part of Commuters (CPC) [9, 108]. It is a well-established\nmeasure to compute the similarity between real flows, ð‘¦ð‘Ÿ, and generated flows, ð‘¦ð‘”:\nð¶ð‘ƒð¶=\n2 Ã\nð‘–,ð‘—ð‘šð‘–ð‘›(ð‘¦ð‘”(ð‘™ð‘–,ð‘™ð‘—),ð‘¦ð‘Ÿ(ð‘™ð‘–,ð‘™ð‘—))\nÃ\nð‘–,ð‘—ð‘¦ð‘”(ð‘™ð‘–,ð‘™ð‘—) + Ã\nð‘–,ð‘—ð‘¦ð‘Ÿ(ð‘™ð‘–,ð‘™ð‘—)\n(7)\nCPC is always positive and contained in the closed interval (0, 1) with 1 indicating a perfect match between the\ngenerated flows and the ground truth and 0 highlighting bad performance with no overlap.\nD.4\nDivergence metrics\nThe Kullback-Leibler (KL) divergence measures how different a probability distribution is from a reference\nprobability distribution. It is used to assess the performance of a generative mobility model by calculating the\nextent to which synthetic trajectories and real trajectories are similar with respect to relevant mobility patterns.\nFormally, given two discrete probability distributions ð‘ƒand ð‘„, defined on the same probability space ð‘‹, the KL\ndivergence from ð‘ƒto ð‘„is defined as:\nð·KL(ð‘ƒ||ð‘„) =\nâˆ‘ï¸\nð‘¥âˆˆð‘‹\nð‘ƒ(ð‘¥)ð‘™ð‘œð‘”\n\u0012 ð‘ƒ(ð‘¥)\nð‘„(ð‘¥)\n\u0013\n.\n(8)\nFormally, given two probability distributions ð‘ƒand ð‘„, KL divergence is the expectation of the logarithmic\ndifference between the probabilities of ð‘ƒand ð‘„, where the expectation is taken using the probabilities of ð‘ƒ. KL\n, Vol. 1, No. 1, Article . Publication date: August 2021.\nA Survey on Deep Learning for Human Mobility\nâ€¢\n41\ndivergence is always non-negative (ð·KL(ð‘ƒ||ð‘„) â‰¥0) and not symmetric, i.e., ð·KL(ð‘ƒ||ð‘„) â‰ ð·KL(ð‘„||ð‘ƒ). ð‘ƒand ð‘„are\nthe same distribution if ð·KL(ð‘ƒ||ð‘„) = 0.\nThe Jensen-Shannon (JS) divergence is a measure to assess the similarity between two distributions. It is\nbased on the KL divergence but it is symmetric (ð½ð‘†(ð‘ƒ||ð‘„) = ð½ð‘†(ð‘„||ð‘ƒ)) and ranges in [0, 1]. Formally, given two\nprobability distributions ð‘ƒand ð‘„, and ð‘€= 1\n2 (ð‘ƒ||ð‘„), we define the JS divergence as:\nð·JS(ð‘ƒ||ð‘„) = 1\n2ð·KL(ð‘ƒ||ð‘€) + 1\n2ð·ð¾ð¿(ð‘„||ð‘€).\n(9)\nThe JS divergence is used, as an alternative to KL, to assess the performance of generative mobility models.\nE\nHUMAN MOBILITY PATTERNS\nHuman movements, far from being random, follows well-defined statistical patterns [9, 194]. In this Section, we\nrevise the most relevant spatial (Section E.1) and temporal (Section E.2) patterns of human mobility.\nE.1\nSpatial patterns\nDisplacements. The distance between two consecutive locations visited by an individual is called jump length\nor displacement [22, 65]. The term location usually indicates a spatial point in which an individual spent a\nminimum amount of time reflecting human behavioral tendencies that motivate people to move between two\nplaces. Formally, a jump length Î”ð‘Ÿ= ð‘‘(ð‘ ð‘–,ð‘ ð‘–+1) is the distance between two spatio-temporal points ð‘ ð‘–and ð‘ ð‘–+1 in a\ntrajectory ð‘‡ð‘¢= âŸ¨ð‘ 1,ð‘ 2, ...,ð‘ ð‘›âŸ©. A truncated power-law well approximates the empirical distribution ð‘ƒ(Î”ð‘Ÿ) within\na population of individuals, with the value of the exponent slightly varying based on the type of data and the\nspatial scale [22, 65].\nRadius of gyration. The characteristic distance traveled by an individual ð‘¢during a period of time can be\nquantified by their radius of gyration [65], defined as ð‘Ÿð‘”(ð‘¢) =\nâˆšï¸ƒ\n1\nð‘›ð‘¢\nÃð‘›ð‘¢\nð‘–=1 ð‘‘(ð‘ ð‘–,ð‘ ð‘ð‘š)2, where ð‘›ð‘¢is the number of\npoints in ð‘‡ð‘¢, ð‘ ð‘–âˆˆð‘‡ð‘¢and ð‘ ð‘ð‘š=\n1\nð‘›ð‘¢\nÃð‘›ð‘¢\nð‘–=1 ð‘ ð‘–is the position vector of the center of mass of the set of points in ð‘‡ð‘¢. A\ntruncated power-law well approximates the distribution of ð‘Ÿð‘”[65, 133]. At a collective level, the evolution in time\nof the average ð‘Ÿð‘”of individuals follows a logarithmic curve âŸ¨ð‘Ÿð‘”(ð‘¡)âŸ©âˆ¼ð›¼+ ð›½lnð‘¡[65, 172].\nThe ð‘˜-radius of gyration of an individual ð‘¢is defined as the radius over their ð‘˜most frequented locations [136],\nð‘Ÿ(ð‘˜)\nð‘”\n(ð‘¢) =\nâˆšï¸ƒ\n1\nð‘ð‘˜\nÃð‘˜\nð‘–=1 ð‘›ð‘–ð‘‘(ð‘ ð‘–âˆ’ð‘ (ð‘˜)\nð‘ð‘š)2, where ð‘ ð‘–âˆˆð‘‡ð‘¢, ð‘ð‘˜is the sum of the visits to ð‘¢â€™s ð‘˜most frequented locations,\nand ð‘ (ð‘˜)\nð‘ð‘š=\n1\nð‘ð‘˜\nÃð‘˜\nð‘–=1 ð‘ ð‘–is the center of mass computed on ð‘¢â€™s ð‘˜most frequented locations. The comparison of ð‘Ÿð‘”\nand ð‘Ÿ(ð‘˜)\nð‘”\nover an entire population revealed the existence of a returners and explorers dichotomy [136].\nMobility entropy. The temporal-uncorrelated entropy of an individual ð‘¢characterizes the predictability of\ntheir spatial movements, and it is defined as ð‘†ð‘¢ð‘›ð‘(ð‘¢) = âˆ’Ãð‘›ð‘¢\nð‘–=1 ð‘ð‘¢(ð‘–) log2 ð‘ð‘¢(ð‘–), where ð‘›ð‘¢is the number of distinct\nlocations visited byð‘¢and ð‘ð‘¢(ð‘–) is the probability thatð‘¢visits location ð‘–[48, 173]. The real entropy of an individual\nð‘¢considers also the order in which the places were visited and the time spent at each location, and it is defined\nas ð‘†(ð‘¢) = âˆ’Ã\nð‘‡â€²ð‘¢âŠ‚ð‘‡ð‘¢ð‘ƒ(ð‘‡â€²\nð‘¢) log2 ð‘ƒ(ð‘‡â€²\nð‘¢) [173], where ð‘ƒ(ð‘‡â€²\nð‘¢) is the probability of finding a particular time-ordered\nsubsequence ð‘‡â€²\nð‘¢in the trajectory ð‘‡ð‘¢. The distribution of both ð‘†ð‘¢ð‘›ð‘and ð‘†are peaked and in particular ð‘ƒ(ð‘†) peaks\naround ð‘†â‰ˆ0.8, indicating that the real spatio-temporal uncertainty in a typical userâ€™s whereabouts is 20.8 = 1.74,\ni.e., fewer than two locations [173].\nI-rank and G-rank. Location ranks identify the importance of a location to an individualâ€™s mobility: the most\nvisited location (likely home or work) has rank 1, the second most visited location (e.g., school or local shop) has\nrank 2, and so on. The visitation frequency of locations ð‘ƒ(ð¿), or I-rank, follows a Zipf law: the probability of\nfinding an individual at a location of rank ð¿is well approximated by ð‘ƒ(ð¿) âˆ¼1/ð¿[65, 133]. Similarly, the collective\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n42\nâ€¢\nLuca et al.\nvisitation frequency of a location ð‘ƒ(ð‘Ÿ), or G-rank, indicates the popularity of locations according to how people\nvisit them on the geographic space [128, 134].\nSemantic importance. For an individualâ€™s trajectory ð‘‡ð‘¢, we define ð‘ð‘‘total(ð‘Ÿ) =\nð‘‘ð‘¡ð‘œð‘¡ð‘Žð‘™(ð‘Ÿ)\nÃ\nð‘Ÿâ€² ð‘‘ð‘¡ð‘œð‘¡ð‘Žð‘™(ð‘Ÿâ€²) , where ð‘‘ð‘¡ð‘œð‘¡ð‘Žð‘™(ð‘Ÿ) is the\ntotal stay duration in location ð‘Ÿinterpreted as ð‘Ÿâ€™s semantic importance [16, 128]. The semantic distance between\ntwo trajectories ð‘‡ð‘¢and ð‘‡ð‘£is the distance between the distribution of the ð‘ð‘‘ð‘¡ð‘œð‘¡ð‘Žð‘™(ð‘Ÿ) for ð‘¢and ð‘£.\nMean Distance Error. Given two equal-sized sets of trajectories T = {ð‘‡1, . . . ,ð‘‡ð‘} and Ë†T = { Ë†ð‘‡1, . . . , Ë†ð‘‡ð‘}, the\nMean Distance Error (MDE) between T and Ë†T is defined as ð‘€ð·ð¸=\nÃð‘\nð‘–ð‘‘(ð‘‡ð‘–, Ë†ð‘‡ð‘–)\nð‘\n, where ð‘‘is the distance between\ntwo points [76].\nMobility networks. In an individual mobility network (IMN), nodes represent locations and directed edges\nrepresent an individualâ€™s trips between locations [150, 160]. The vast majority of individualsâ€™ trips can be described\nwith a limited number of daily motifs which represent the underlying regularities in daily movements [160].\nIndividual trajectories may be aggregated to study the flows of individuals between locations at different spatio-\ntemporal scales. Flows can typically be described by an Origin-Destination (OD) matrix, or mobility network,\nwhich has a specific structure and dynamics [167].\nE.2\nTemporal metrics\nWaiting time and circadian rhythm. The waiting time Î”ð‘¡is the elapsed time between two consecutive points in\nthe mobility trajectory of an individual ð‘¢, or equivalently as the time spent in a location: Î”ð‘¡= ð‘¡ð‘–âˆ’ð‘¡ð‘–âˆ’1. Empirically\nthe distribution of waiting times is well approximated by a truncated power-law [172] The movements of\nindividuals are not distributed uniformly during the hours of the day but follow a circadian rhythm [92, 134];\npeople tend to be stationary during the night hours while prefer moving at specific times of the day, for example,\nto reach the workplace or return home.\nTemporal location patterns. The temporal popularity ð‘(ð‘Ÿ,ð‘¡) measures the visiting probability for a location ð‘Ÿat\nany time ð‘¡[128]. The staying patterns ð‘(ð‘Ÿ,ð‘‘) measures the probability of visiting a location ð‘Ÿfor a duration ð‘‘\n[128].\n, Vol. 1, No. 1, Article . Publication date: August 2021.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.SI",
    "I.2"
  ],
  "published": "2020-12-04",
  "updated": "2021-06-27"
}