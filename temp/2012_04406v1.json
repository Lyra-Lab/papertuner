{
  "id": "http://arxiv.org/abs/2012.04406v1",
  "title": "NavRep: Unsupervised Representations for Reinforcement Learning of Robot Navigation in Dynamic Human Environments",
  "authors": [
    "Daniel Dugas",
    "Juan Nieto",
    "Roland Siegwart",
    "Jen Jen Chung"
  ],
  "abstract": "Robot navigation is a task where reinforcement learning approaches are still\nunable to compete with traditional path planning. State-of-the-art methods\ndiffer in small ways, and do not all provide reproducible, openly available\nimplementations. This makes comparing methods a challenge. Recent research has\nshown that unsupervised learning methods can scale impressively, and be\nleveraged to solve difficult problems. In this work, we design ways in which\nunsupervised learning can be used to assist reinforcement learning for robot\nnavigation. We train two end-to-end, and 18 unsupervised-learning-based\narchitectures, and compare them, along with existing approaches, in unseen test\ncases. We demonstrate our approach working on a real life robot. Our results\nshow that unsupervised learning methods are competitive with end-to-end\nmethods. We also highlight the importance of various components such as input\nrepresentation, predictive unsupervised learning, and latent features. We make\nall our models publicly available, as well as training and testing\nenvironments, and tools. This release also includes OpenAI-gym-compatible\nenvironments designed to emulate the training conditions described by other\npapers, with as much fidelity as possible. Our hope is that this helps in\nbringing together the field of RL for robot navigation, and allows meaningful\ncomparisons across state-of-the-art methods.",
  "text": "NavRep: Unsupervised Representations for Reinforcement Learning of\nRobot Navigation in Dynamic Human Environments\nDaniel Dugas, Juan Nieto, Roland Siegwart and Jen Jen Chung\nAbstract— Robot navigation is a task where reinforcement\nlearning approaches are still unable to compete with traditional\npath planning. State-of-the-art methods differ in small ways,\nand do not all provide reproducible, openly available imple-\nmentations. This makes comparing methods a challenge. Recent\nresearch has shown that unsupervised learning methods can\nscale impressively, and be leveraged to solve difﬁcult problems.\nIn this work, we design ways in which unsupervised learning\ncan be used to assist reinforcement learning for robot naviga-\ntion. We train two end-to-end, and 18 unsupervised-learning-\nbased architectures, and compare them, along with existing\napproaches, in unseen test cases. We demonstrate our approach\nworking on a real life robot. Our results show that unsupervised\nlearning methods are competitive with end-to-end methods. We\nalso highlight the importance of various components such as\ninput representation, predictive unsupervised learning, and la-\ntent features. We make all our models publicly available, as well\nas training and testing environments, and tools1. This release\nalso includes OpenAI-gym-compatible environments designed\nto emulate the training conditions described by other papers,\nwith as much ﬁdelity as possible. Our hope is that this helps\nin bringing together the ﬁeld of RL for robot navigation, and\nallows meaningful comparisons across state-of-the-art methods.\nI. INTRODUCTION\nRobot navigation in complex environments and in the pres-\nence of humans is a challenging problem due to complexity\nin human behavior and the unpredictability of unstructured\nenvironments (see Fig. 1). Though popular planning ap-\nproaches work well in many scenarios, edge cases complex\nenough to cause planning failures abound, and the engineer-\ning work which is necessary to address one edge case does\nnot necessarily translate to another. As a result, designing a\nplanner that is able to perform in any environment is still a\nmonumental engineering task.\nOutside of robot navigation, reinforcement learning (RL)\napproaches have led data-driven controllers to reach expert\nand even super-human performance at game-play tasks [1].\nData-driven methods, at least in theory, present a potential of\nunlimited scale, where a planner can be improved indeﬁnitely\nso long as more data can be gathered. For this reason,\nseveral research efforts have focused on the use of data\ndriven-methods for robot navigation [2–4]. Yet, the question\nremains of whether similar performance can be achieved for\nthe problem of navigation among humans, and how.\nThis work was supported by the EU H2020 project CROWDBOT under\ngrant nr. 779942\nThe authors are with the Autonomous Systems Lab, ETH Z¨urich,\nZ¨urich\n8092,\nSwitzerland. {dugasd; jnieto; rsiegwart;\nchungj}@ethz.ch\n1https://github.com/ethz-asl/navrep\nFig. 1.\nTimelapse of an example target environment for autonomous\nnavigation, snapshot of the corresponding LiDAR measurements on the right\nFig. 2.\nWe make NavRepSim openly available, a simulator which contains\nefﬁcient, gym-compatible environments for end-to-end navigation, allowing\nall to reproduce scenes from (a) [2, 3], (b) CADRL and SOADRL [5–7],\nCrowdMove [4] (not shown), (c) IAN [8], and (d) real data.\nWhen planning in the presence of humans, it is often\nnecessary to provide estimates for the position and state of\nhumans around the robot [5–7, 9, 10]. To make planning\ntractable, these estimates are typically reduced to position,\nvelocity, and radius of the whole human. Subtleties in\nhow individual legs move in LiDAR, or full body cues in\n3D/camera measurements, are therefore lost. However, with\nend-to-end learning, since the data-driven model has access\nto the raw sensor input, it could theoretically infer actions\nbased on such subtle information modalities.\nFor end-to-end learning, having to train the entire per-\nception component of a policy from sparse rewards can be\ninefﬁcient. An alternative is to use features from perception\nmodels which have been extensively trained with supervised\ntraining. This however, requires large quantities of labeled\ndata. Using unsupervised training, it is possible to learn rich\nencodings from unlabeled data. These rich encodings can be\nshared across tasks. In addition, these unsupervised models\ncan be used to generate synthetic reproductions of the world,\nwhich can assist or replace simulation. The question is, using\ncurrent methods, can we train unsupervised representations\nwhich are practical for robot navigation?\nWe address this through the following contributions:\n• We provide an open-source simulation environment,\ntrained models, and useful tools for future end-to-end\nnavigation benchmarking (see Fig. 2).\n• We design several unsupervised learning architectures\nfor the task of robot navigation among humans.\narXiv:2012.04406v1  [cs.RO]  8 Dec 2020\n• We compare existing approaches to each other and to\nours, in seen and unseen environments, and demonstrate\nour approach on a real robot.\nII. RELATED WORK\nSeveral state-of-the-art RL methods for robot navigation\nmake use of exact pedestrian positions in the input: CADRL\nand following works [5, 9, 10] trained an RL-based policy to\navoid simulated agents with no static obstacles, though real\nobstacles are handled out-of-policy in real testing. SARL [6]\ntarget the same domain as CADRL, with an architecture that\nfocuses on pairwise interactions in order to model a useful\nvalue function. SOADRL [7] extends the SARL architecture\nto also take into accounts static obstacles, in some cases with\na limited ﬁeld of view.\nEnd-to-end methods avoid the need for detection and\ntracking of humans by using sensor data as input directly.\nPfeiffer et al. [2] explore using end-to-end supervised learn-\ning for robot navigation given 2D LiDAR measurements.\nExample trajectories are taken from an expert planner.\nIn [3] imitation learning is used, followed by end-to-end\nRL, allowing for better performance and harder navigation\nproblems. Both [2, 3] have a static environment, with no\ndynamic agents. CrowdMove [4] shows a robot navigating in\na crowded space using an end-to-end RL policy. The learning\nagent is trained to solve a joint planning problem, where\nmultiple instances of itself all simultaneously navigate past\neach other and obstacles to reach their goal.\nHa and Schmidhuber [11] raise the question of whether\nunsupervised learning can assist end-to-end RL. Their world\nmodels approach is able to beat state-of-the-art performance\non the CarRacing openAI gym environment by splitting the\nend-to-end task into an unsupervised latent feature learning\ntask and a control policy learning from latent features RL\ntask. A stated potential of this approach is the ability\nto use the unsupervised model to replace simulation, and\nthey demonstrate a proof of concept for this idea on the\nDoom gym environment. Piergiovanni et al. [12] applies\nthis “dream” idea to real-world robot navigation, in the\nlimited scope of navigating to or avoiding an object based on\ncamera images. GameGAN [13] pushes the neural simulation\nconcept further by approximating the pac-man game through\nunsupervised learning of a generative adversarial network\n(GAN) architecture. Recent work [14–16], also demonstrated\nthat the unsupervised learning of sequences can deliver\nimpressive results simply through increase in scale. They\ndemonstrate state-of-the-art performance in various natural\nlanguage processing tasks, using their approach.\nIII. NAVIGATION SIMULATION ENVIRONMENT\nThe NavRepSim environment is designed with RL appli-\ncations in mind. It aims to apply to any range-based sensor\nnavigation problem. The goal of open-sourcing this simulator\nis to make it easier for anyone to reproduce state-of-the-art\nsolutions for learning-based navigation.\nThe simulator offers 23 ﬁxed maps as well as the pos-\nsibility to procedurally generate maps containing random\nFig. 3.\nThe state representation obtained from the world (left) combines\nboth the LiDAR observation (sl), and goal/velocity observation (sr). The\nLiDAR observation is either represented as a 64 × 64 matrix of occupancy\nprobability (rings) or a 1080 vector of distances (1D).\npolygons (see Fig. 2 for examples of the available maps).\nThe offered maps were designed to imitate those used\nin prior work. Note that pre-made gym environments are\nmade available to reproduce the speciﬁc training domains\nof other paper. We also created several new maps from real\nLiDAR data. Overall, we provide environments that range\nfrom simpliﬁed and highly synthetic to more realistic maps\ncontaining real-world sensor noise.\nIn addition to having different maps, the learning environ-\nments can also vary in terms of the simulated robot(s), the\nsimulated human agents as well as the learning curriculum.\nThe simulated robot can be either holonomic or differential-\ndrive. The inertial physics of the robot base can be simulated\nwith high ﬁdelity or simpliﬁed as instantaneous velocities.\nFurther, collisions can either lead to damage (reﬂected as a\nnegative reward) or instant episode termination. Users can\nalso specify the number of robots in the environment, since\njoint training of several robots in the same simulation is also\npossible. The number and behavior of the simulated human\nagents can also be adjusted. Agents in the environment may\nbe static or dynamic, they may be rendered as moving legs or\nellipses, and they can be controlled using different policies\n(constant velocity, ORCA [17], global planning). Finally,\nwe offer two learning curricula. The ﬁrst uses a constant\nepisode set-up, picking different maps and agent locations\nfor every episode, while the second varies the environment\ndifﬁculty (more human agents and more obstacles in the\ncase of procedurally generated maps) based on the policy’s\nsuccess.\nCare is taken to maintain simulation efﬁciency, with most\nenvironments running at more than 100 iterations per second\nin a single thread (on a laptop, Intel Core i7-6600U CPU\n@ 2.60GHz x 4). For comparison, the CarRacing gym\nenvironment used in [11] runs at the same speed on the same\nmachine.\nIV. LEARNING-BASED NAVIGATION\nA core purpose of this work is to study the possible\nvariants of learning-based robot navigation. The basic setup\ncommon across many state-of-the-art methods is a ground\nrobot equipped with a range sensor (often a 2D planar\nLiDAR). The state space used in the learning formulation is\nderived from the incoming range data. As discussed in Sec-\ntion II, several works also assume that live pedestrian tracks\nare available to the robot, however in our work we do not\nFig. 4.\nVisualization of the V auto-encoding module’s inputs and outputs.\nNote that the LiDAR observation sl can be either in the rings or 1D\nrepresentation.\nmake this assumption. The robot action space is continuous\nand is represented as a vector in R3, a = [vx, vy, ω]. Each of\nvx, vy, ω ∈[−1, 1] correspond to the robot’s translational and\nrotational velocity in its base-link frame. Here we consider\na holonomic robot, however the work can be extended to\ndiff-drive robots by simply requiring vy = 0.\nThe scalar reward function for time-step t is rt = rt\ns +\nrt\nc + rt\nd + rt\np. The ﬁrst term, rt\ns, is the success reward, which\nis 100 if the goal is reached and 0 otherwise. The collision\nreward, rt\nc, is -25 if the robot collides and 0 otherwise. The\ndanger reward, rt\nd, is equal to -1 if the robot is closer than\n0.2m to an obstacle or agent, and 0 otherwise. Finally, rt\np is\nthe progress reward, which is equal to the difference between\nthe previous distance to goal and new distance to goal.\nA. State space variants\nThe state representation st is a combination of the LiDAR\nobservations st\nl and the robot velocity and goal position in\nthe robot frame st\nr (see Fig. 3). In this work, we consider two\nmethods for representing LiDAR measurements and compare\ntheir effects when used as inputs to a learned navigation\npolicy. The ﬁrst representation is simply a 1-dimensional\nvector of the 1080 raw distance values from the LiDAR\nnormalized to [0, 1]. We refer to this as the 1D representation.\nWe also consider a probabilistic representation for the input\nLiDAR data. This representation maps the space around\nthe robot as a polar grid (R64×64), with evenly distributed\ndiscrete angular sections and exponentially distributed dis-\ncrete radius intervals, leading to a greater resolution closer\nto the robot compared to regions that are farther away.\nGiven the LiDAR scan, each grid cell is assigned a value\np ∈{0, 0.5, 1} for being unoccupied, unknown or occupied,\nrespectively. We refer to this two-dimensional probabilistic\nLiDAR representation as the rings representation.\nB. NavRep: Unsupervised representations for navigation\nIn contrast to end-to-end learning approaches, NavRep ex-\nploits modular network architecture and training. Following\nthe approach of [11], we make use of three main modules,\n• V (LiDAR encoding): a variational auto-encoder (VAE)\ntrained to reconstruct the LiDAR state,\n• M (prediction): a long short-term memory network\n(LSTM) trained to predict future state sequences, and\n• C (controller): a small fully connected network (FCN)\n(2-layer perceptron) trained to navigate using the latent\nrepresentations of V and M as input.\nFor brevity, we refer to the latent encoding of V as z, and\nthe latent encoding of M as h. These modules are illustrated\nFigs 4 and 5.\nFig. 5.\nVisualization of the M prediction module’s inputs and outputs. The\nz latent features for each observation in the sequence are ﬁrst encoded by\nthe V module. These are passed to M along with the sequence of actions\na. Each predicted ˆzt+1 in the sequence is then decoded by V.\nFig. 6.\nOverview of the architectures trained in this work. For all ap-\nproaches there are 2 LiDAR input variants. In addition, the 3 unsupervised-\nlearning-based approaches each have 3 latent feature variants. Simpliﬁed\nschema of the components are detailed for 3 abitrarily selected architectures.\n1) Transformer architecture: From this basic set of mod-\nules, we offer our ﬁrst NavRep learning variant. Rather than\nan LSTM as in [11], we instead make use of the attention-\nbased Transformer architecture [18] to generate predictions\nof the future LiDAR encodings.\n2) Joint vs modular training: Our second NavRep learn-\ning variant is designed to provide insight into the effect of\ntraining V and M jointly or separately. In [11], the authors\nﬁrst train V to minimize reconstruction loss before training\nM to minimize the z prediction loss while keeping the V\nnetwork weights ﬁxed. Here we compare their modular\ntraining regime with one that trains V and M jointly.\n3) Input features to C: The ﬁnal set of NavRep variants\nthat we consider are the inputs used by the controller to\ncompute the robot action. We compare between using z, h\nor [z, h], where the latter was proposed in [11]. An overview\nof all the learning architecture and training variants that we\nstudy is provided in Fig. 6.\nC. Network implementation and training\n1) Hyperparameters: All V modules are composed of a 4-\nlayer convolutional neural network (CNN) encoder, followed\nby 2-layer FCN, and 4-layer CNN decoding blocks. z latent\nfeatures are of size 32, h latent features are of size 64.\nThe LSTM-based M module has 512-cells. The Transformer-\nbased M module is composed of 8 causal-self-attention\nblocks with 8 attention heads each. The modular V and\nM modules have respectively 4.3 million and 1.3 million\ntrainable parameters. The V+M transformer model has 4.8\nmillion trainable parameters. More details available in the\nopen-source implementation.\n2) V: was trained using minibatch gradient descent, with\na typical VAE loss,\nLV = Lrec + KL(zt),\nwhere KL(zt) is the Kullblack-Leibler divergence between\nthe latent feature distribution parameterized by the encoder\nand the prior latent feature distribution. The reconstruction\nloss Lrec is the mean squared error between input and output,\nLrec = H(ˆst, st),\n(1)\nwhere H denotes the binary cross entropy, st denotes the\nobserved state (LiDAR representation) at time t in the\nsequence.\n3) M: was trained on batches of sequences. We deﬁne the\nloss function as the latent prediction loss,\nLM = H(ˆzt+1, zt+1) + H( ˆsr\nt+1, st+1\nr\n).\n(2)\nThis adds the binary cross entropy loss between the predicted\n(ˆzt+1) and the true (zt+1) latent features to that of the\npredicted ( ˆsr\nt+1) and the true (st+1\nr\n) goal-velocity states.\n4) Joint V+M: was trained with a loss function composed\nof the sum of the reconstruction and prediction losses from\n(1) and (2). However in this case, for the rings variant the\nprediction loss is\nLV +M = H(ˆst+1, st+1),\nthe binary cross entropy loss between the reconstructed state\nprediction ˆst+1 = V (ˆzt+1) and true next state st+1. For the\n1D LiDAR variant we used the mean squared error instead of\nthe cross-entropy loss as the predictions are not distributions,\nbut direct values.\n5) C: was trained using the proximal policy optimization\n(PPO) algorithm [19, 20]. Each C module was trained up\nto 3 times with varying random seeds in order to mitigate\ninitialization sensitivity in the results. Training was stopped\nafter 60 Million training steps.\n6) Training data: The V and M modules were trained\nwith data extracted from the same environment in which the\nC module was later trained. To generate training sequences,\nthe ORCA [17] policy was used to control the robot, and the\nnumber of agents in the scene was increased to ensure sufﬁ-\ncient observations of dynamic objects and prevent imbalance\nin the dataset. A simple model of leg movement was used\nto render dynamic agents as moving legs in the generated\nLiDAR data and a time-step of 0.2s was used for updating\nthe agents’ positions.\nThe C module itself was trained in the SOADRL-like\nenvironment (shown in Fig. 2), with randomly generated\npolygons and agents (every episode is unique). A bounding\nobstacle had to be added to prevent the policy from learning a\nsimple risk-averse heuristic which consists in going around\nthe entire area. During training, actions for the C module\nwere limited to x-y movement, with 0 rotation. However,\nthe robot’s initial angular position was randomized for each\nepisode.\nCurriculum learning was used to improve the training of\nthe policy: every failed episode lead to a lower number of\nFig. 7.\nComparison of the worldmodel error for the joint, modular and\ntransformer architectures, for their (top) rings LiDAR state representation\nvariant, and ((bottom) 1D variant. This error is calculated and shown\nseparately for the LiDAR state sl (left) and goal-velocity state sr (right).\npolygon obstacles and agents, conversely, every successful\nepisode lead to an increase. The maximum number of agents\nand polygons was capped to 5 agents and 10 obstacles.\nD. End-to-end learning baselines\nAs a baseline, we implement a typical end-to-end RL\napproach. This approach attempts to learn a navigation policy\nπ(a, s) predicting action probabilities directly from LiDAR\ninputs.\nTwo variants of end-to-end learning are implemented:\nthe ﬁrst taking the 1-dimensional LiDAR representation as\nthe state input, and the second taking 2-dimensional rings\nrepresentation as the state input. For the value and policy\nmodel, we use a CNN for feature extraction, followed by\nfully connected layers. To make comparisons fair, the same\nCNN architecture and dimensions are used as in the V and\nC models described in the following subsection.\nV. EXPERIMENTS AND RESULTS\nA. Worldmodel error for unsupervised learning architectures\nThe six joint, modular and transformer architectures\n(Fig. 6) have differing loss functions, which makes it difﬁcult\nto compare their losses during training. To this end, we deﬁne\nthe “worldmodel error”, a next-step prediction error term that\nallows meaningful comparison across architectures,\nEworldmodel = MSE(ˆst+1, st+1).\nHowever, this error function does not produce comparable\nresults for the rings vs. 1d LiDAR inputs, as they do not have\nthe same dimensions and scale. Instead, these comparisons\nare shown separately in Fig. 7. The worldmodel errors shown\nhere are calculated for the training sets, which compares each\narchitecture’s ability to learn in an unsupervised manner. It\ncan also be understood as the accuracy of ’Dreams’ generated\nby each architecture.\nB. Controller training performance\nThough the training environment features curriculum\nlearning through adaptive difﬁculty, a separate validation\nenvironment with ﬁxed difﬁculty is used to quantify the ab-\nsolute progress of each model every 100’000 training steps.\nThis validation is similar to the training environment, except\nthat it remains on the maximum difﬁculty, i.e. maximum\nFig. 8.\n3 training sessions were performed for each of the two end-to-\nend and 18 NavRep architectures to account for the inﬂuence of network\ninitialization on the results (legend as in Fig. 7). Horizontal lines mark the\nmaximum success rate of each curve. The SOADRL success rate allows for\ncomparison with methods that use exact human locations as input.\nnumber of agents and obstacles, and that it consists of\n100 episodes with arbitrary but deterministic initializations.\nSuccess rates (number of validation runs in which the robot\nreaches the goal without collision) are shown in Fig. 8.\n1) Discussion: Looking at the modular, joint and trans-\nformer models, it seems that the worldmodel error and\nlearned controller performance are correlated. This suggests\nthat better reconstruction and prediction performance in the\nV and M modules leads to latent features which are more\nuseful for the control task. Nevertheless, it can also be\nseen that the end-to-end approaches perform better in the\nvalidation tasks than the NavRep controllers. Finally, none of\nthe models are able to surpass the performance of SOADRL,\nwhich has access to exact positions of pedestrians.\nC. Test performance\nEach model was tested in 27 unseen scenarios (3 maps,\n9 scenarios each). At least 300 episodes were run in each\nscenario, for a total of 10’000 test episodes per model. These\nscenarios are designed to represent a wide range of possible\nreal-life situations that can help to identify speciﬁc issues\nwith the learned controller, such as global planning, agent\navoidance and crowd aversion. For each map, the ﬁrst 6\nscenarios are taken from [8], and the remaining 3 are (7)\na trivial scenario where the robot is close to the goal with\nno obstacle in between, (8) an easy scenario with a close\ngoal, and a single obstacle between robot and goal, and (9) a\ntypical dynamic avoidance task where all agents are placed\non a small circle with goals on the opposite side and no\nobstacles in between. The scenario deﬁnitions and selection\nwas ﬁxed before any testing took place to avoid bias.\n1) Discussion: In general it can be seen that the end-to-\nend models perform suffer a greater drop in performance\nfrom training to testing, whereas the NavRep models have\nmore consistent performance. In particular, the 1D LiDAR\nend-to-end model fails completely in the realistic map even\nthough it is a top performer during training. In addition,\nwhen investigating the low performance of SOADRL, we\nsaw that most failures were due to crashes into obstacles\nand not agents.\nIn the same test environment, a traditional planner based\nFig. 9.\nMean performance of each model in the testing environments.\nLegend as in Figs 7, 8, with error bars denoting the minimum and maximum\nscore across random seeds. Overall performance is shown (left), as well as\nper-map performance from the simple, complex, and realistic maps.\nFig. 10. Comparison of trajectories executed by the best performing models\nduring testing in unseen environment. From left to right, the scenarios are 1)\nsimplest scenario in realistic map, 2) global planning task in realistic map,\n3) full planning task (global, static, dynamic avoidance) in complex map,\n4) global planning in simple map. Orange trajectories are trajectories which\nresulted in failure to reach the goal, through collision or time-out. Blue\ntrajectories are successful trajectories. Grey circles indicate the positions of\ndynamic agents. Overlaid is the percentage of successful trajectories.\non the timed elastic band (TEB) approach from the widely-\nused ROS navigation stack achieves 76% success rate.\nThe low success rates of all models on long-range navi-\ngation scenarios (Fig. 10) indicates that global navigation is\na challenge for RL-based methods. We also see that several\nworks [5, 7, 10] avoid this problem by dealing with local\nnavigation only. Looking at other failures, we see that in\nseveral cases, the end-to-end and unsupervised models don’t\nlearn to proceed slowly in the presence of “danger” (other\ndynamic agents). Instead they move as fast as possible,\nor are stuck when available space becomes narrow. This\ncan perhaps be improved through reward design. Finally, in\nthe challenging dense static crowd scenario, though most\nattempts fail, a few are able to succeed by avoiding the\ndense area, which implies taking a signiﬁcantly longer route.\nAs shown in [8], this is a challenging situation even for\ntraditional planners.\nD. Analysis of learning variants\n1) Impact of rings: Rings encodes the space around the\nrobot as values between 0 and 1 in a pseudo-probabilistic rep-\nresentation of occupancy. It seems that allowing the NavRep\nmodel to infer from and predict occupancy uncertainties\nexplicitly makes its task easier than inferring directly from\nthe raw LiDAR scan. In addition, rings provides a higher-\nlevel abstraction for the CNN to operate in, meaning that\nit requires less depth, while scaling the spatial resolution\ninformation to focus on regions close to the robot. This is\nin line with how sensors typically work and also matches\nthe way that, in obstacle avoidance, more attention should\nusually be paid to close objects. The difference in resolution\nsensitivity between the 1D and rings representations could\nalso explain the dramatic performance drop observed in the\nrealistic map for the end-to-end model. Local features in the\nLiDAR representations for the simple vs. realistic maps vary\nmore in the 1D case than in the rings. Moreover, rings more\neasily allows using the same model with different sensors of\nvarying resolution.\n2) Joint vs modular training: Previous work [11] rec-\nommends modular training, that is, training V, M and C\nseparately, and to use both z and h as inputs to the C\nmodule. However, in this work we ﬁnd that jointly trained\nV+M models provide superior performance in almost all\ntasks. The computational cost of joint training is furthermore\nnot signiﬁcantly greater than for modular training, though\nmemory usage does increase due to having to store sequences\nof rich observations rather than compressed latent features.\n3) Latent feature usefulness: When looking at model per-\nformances both in seen and unseen environments, our results\ndo not support the idea that using both z and h latent features\nis advantageous. Though in some cases models trained with\nz + h features performed slightly better, this improvement\nis not signiﬁcant when compared to the variance between\nrandom seeds. In our experience, the performance cost of\nincluding h latent features in the C training regime is great:\ntraining a z +h C model takes approximately 9 times longer\nthan for an equivalent z-only C model. h-only models have\nit worst, with low test performance and slow training times.\nE. Real-robot trials\nWe took our Transformer architecture (rings, z-only) and\nimplemented it on a Pepper robot (Fig. 11). Our robot has\na quasi-holonomic base and two 270◦LiDARs merged to-\ngether to form a 1080-vector combined range scan. The robot\nwas given several waypoints in a space mixed with people\nand obstacles, and tasked to patrol between waypoints.\nThe learned controller achieved 100% success in the real\nenvironment, reaching its goal for all trajectories. However,\nwe observed issues which show a need for improvement.\n(i) Smoothness: the commands selected by the controller are\naggressive, leading to jerky motion. (ii) Reluctance to go into\ntight spaces: when faced with narrow but traversable areas,\nthe controller oscillates around the entrance but eventually\nmakes it through. (iii) Getting stuck: we observed the planner\nspend several seconds oscillating in front of an obstacle\nbefore ﬁnally passing on one side.\nThese issues appear to be in part due to the following\ncauses: (i) Inertia in the dynamics of the robot is not\nmodelled in the training environment, which causes sub-\noptimal control in the policy. (ii) Sensor noise is not present\nin the simulated environment. As a result, when it occurs\nFig. 11.\nWe test the proposed method on a real robot, where the learned\npolicy is repeatedly able to avoid obstacles and people and reach its goals.\nGoals are shown in red, successful trajectories in blue, images taken during\nthe experiment are displayed next to the locations they were taken in. Each\nsequence was run three times in a row.\nin the real scenario, the controller can react unpredictably.\nWe surmise that there are further unknown reasons. Such\n“unknown unknown” effects could possibly be addressed via\ncontinual learning by reﬁning the model in the real world.\nVI. CONCLUSION\nTraining robust sensor-to-control policies for robot navi-\ngation remains a challenging task. NavRep methods do not\ndominate over end-to-end methods, but we do see interesting\nproperties and trade-offs between the two. Though less\nsuited to learning speciﬁc tasks, unsupervised representations\ndisplay several beneﬁts, such as more consistent perfor-\nmance across general tasks, modularity, and the potential\nto generate “Dream” environments [11], which could assist\nor replace simulation. The popularity of these methods is\nadditionally propelled by the continued increase in compute\nand memory availability. Moving forward we expect to see\nlarger unsupervised models with richer modalities, that create\nmore convincing approximations of reality and provide more\nuseful and discerning features.\nNevertheless, navigation-RL still has a long way to go,\nshould one aim to replace traditional planners. Among the\ncurrent issues are global planning, sim-2-real constraints, and\nsafety. Potential avenues include more rigorous exploration\nof reward functions, more realistic training environments,\nand continual learning. Given that standardized gym-like\nenvironments could help harmonize future research, we hope\nthat by making NavRep and NavRepSim publicly available\nand easy to use, progress in this domain can be accelerated\nand to improve collaboration between researchers.\nREFERENCES\n[1] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi,\nD. Guo, and C. Blundell, “Agent57: Outperforming the atari human\nbenchmark,” arXiv preprint arXiv:2003.13350, 2020.\n[2] M. Pfeiffer, M. Schaeuble, J. Nieto, R. Siegwart, and C. Cadena,\n“From perception to decision: A data-driven approach to end-to-\nend motion planning for autonomous ground robots,” in 2017 ieee\ninternational conference on robotics and automation (icra).\nIEEE,\n2017, pp. 1527–1533.\n[3] M. Pfeiffer, S. Shukla, M. Turchetta, C. Cadena, A. Krause, R. Sieg-\nwart, and J. Nieto, “Reinforced imitation: Sample efﬁcient deep\nreinforcement learning for mapless navigation by leveraging prior\ndemonstrations,” IEEE Robotics and Automation Letters, vol. 3, no. 4,\npp. 4423–4430, 2018.\n[4] T. Fan, P. Long, W. Liu, and J. Pan, “Distributed multi-robot col-\nlision avoidance via deep reinforcement learning for navigation in\ncomplex scenarios,” The International Journal of Robotics Research,\np. 0278364920916531, 2020.\n[5] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decentralized non-\ncommunicating multiagent collision avoidance with deep reinforce-\nment learning,” in 2017 IEEE international conference on robotics\nand automation (ICRA).\nIEEE, 2017, pp. 285–292.\n[6] C. Chen, Y. Liu, S. Kreiss, and A. Alahi, “Crowd-robot interaction:\nCrowd-aware robot navigation with attention-based deep reinforce-\nment learning,” in 2019 International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2019, pp. 6015–6022.\n[7] L. Q. Liu, D. Dugas, G. Cesari, R. Siegwart, and R. Dub´e, “Robot nav-\nigation in crowded environments using deep reinforcement learning,”\nin 2020 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, in press.\n[8] D. Dugas, J. Nieto, R. Siegwart, and J. J. Chung, “Ian: Multi-behavior\nnavigation planning for robots in real, crowded environments,” in 2020\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS).\nIEEE, in press.\n[9] Y. F. Chen, M. Everett, M. Liu, and J. P. How, “Socially aware\nmotion planning with deep reinforcement learning,” in 2017 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2017, pp. 1343–1350.\n[10] M. Everett, Y. F. Chen, and J. P. How, “Motion planning among\ndynamic, decision-making agents with deep reinforcement learning,”\nin 2018 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2018, pp. 3052–3059.\n[11] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy\nevolution,” in Advances in Neural Information Processing Systems,\n2018, pp. 2450–2462.\n[12] A. Piergiovanni, A. Wu, and M. S. Ryoo, “Learning real-world robot\npolicies by dreaming,” arXiv preprint arXiv:1805.07813, 2018.\n[13] S. W. Kim, Y. Zhou, J. Philion, A. Torralba, and S. Fidler, “Learning to\nsimulate dynamic environments with gamegan,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 1231–1240.\n[14] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training.”\n[15] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[16] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language\nmodels are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.\n[17] J. Van den Berg, M. Lin, and D. Manocha, “Reciprocal velocity obsta-\ncles for real-time multi-agent navigation,” in 2008 IEEE International\nConference on Robotics and Automation. IEEE, 2008, pp. 1928–1935.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nAdvances in neural information processing systems, 2017, pp. 5998–\n6008.\n[19] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[20] A. Hill, A. Rafﬁn, M. Ernestus, A. Gleave, R. Traore, P. Dhariwal,\nC. Hesse, O. Klimov, A. Nichol, M. Plappert et al., “Stable baselines,”\nGitHub repository, 2018.\n",
  "categories": [
    "cs.RO",
    "cs.LG"
  ],
  "published": "2020-12-08",
  "updated": "2020-12-08"
}