{
  "id": "http://arxiv.org/abs/2012.06969v2",
  "title": "Predicting Generalization in Deep Learning via Local Measures of Distortion",
  "authors": [
    "Abhejit Rajagopal",
    "Vamshi C. Madala",
    "Shivkumar Chandrasekaran",
    "Peder E. Z. Larson"
  ],
  "abstract": "We study generalization in deep learning by appealing to complexity measures\noriginally developed in approximation and information theory. While these\nconcepts are challenged by the high-dimensional and data-defined nature of deep\nlearning, we show that simple vector quantization approaches such as PCA, GMMs,\nand SVMs capture their spirit when applied layer-wise to deep extracted\nfeatures giving rise to relatively inexpensive complexity measures that\ncorrelate well with generalization performance. We discuss our results in 2020\nNeurIPS PGDL challenge.",
  "text": "Predicting Generalization in Deep Learning via\nLocal Measures of Distortion\nAbhejit Rajagopal\nUniversity of California, San Francisco\nabhejit.rajagopal@ucsf.edu\nVamshi C. Madala\nUniverstiy of California, Santa Barbara\nvamshichowdary@ucsb.edu\nShivkumar Chandrasekaran\nUniversity of California, Santa Barbara\nshiv@ucsb.edu\nPeder E. Z. Larson\nUniverstiy of California, San Francisco\npeder.larson@ucsf.edu\nAbstract\nWe study generalization in deep learning by appealing to complexity measures orig-\ninally developed in approximation and information theory. While these concepts are\nchallenged by the high-dimensional and data-deﬁned nature of deep learning, we\nshow that simple vector quantization approaches such as PCA, GMMs, and SVMs\ncapture their spirit when applied layer-wise to deep extracted features giving rise to\nrelatively inexpensive complexity measures that correlate well with generalization\nperformance. We discuss our results in 2020 NeurIPS PGDL challenge.\n1\nBackground\nIn approximation theory, generalization error is measured by the maximum error between an approxi-\nmation f ∗(x) and the true target function f(x) in a domain D, as minx∈D ||f(x) −f ∗(x)||∞. For\nclassiﬁcation problems, this deﬁnition is often relaxed to estimate how often the target function is\nmatched over the whole domain, e.g. using a normalized 0-norm to measure train/test classiﬁcation\n“accuracy”, or a more-continuous p-norm to gauge average train/test error. In any case, measuring the\ntrue generalization performance requires knowledge of the target function over the whole domain,\nwhich presents an issue for deep learning problems with ﬁnite high-dimensional data.\nBased on recent works [1, 2], however, the 2020 NeurIPS PGDL Challenge seeks predictors of\ndeep neural network (DNN) generalization performance that are based solely on training data, DNN\nparameters, and training hyperparameters (e.g. learning rate, optimizer, training loss value, etc).\nHere, the true generalization is measured as the accuracy of the model on a large withheld set\nof test data. For each architecture (e.g. fully-convolutional or CNN + fully-connected) and task\n(e.g. image classiﬁcation on CIFAR10 or SVNH), the challenge provides several trained networks\nwith comparable training performance (≥90%) but signiﬁcantly different testing performance. The\ntask is then to develop a method for predicting the generalization error for a withheld set of networks\ntrained for the same or new tasks with the same or different hyperparameters.\nOur approach to this challenge is based on recent work in approximation theory [3, 4, 5], which\nsuggests that deep networks constructed using the true compositional structure of the target function\ncan not only escape the curse of dimensionality but also converge to the target with provably good\ngeneralization error by penalizing the roughness of the interpolant, even when only given point-data.\nFor approximation from point data, there is an inherent trade-off between (a) ﬁtting all the training\npoints, (b) the order or complexity of the approximation, and (c) the rate of convergence. Thus, for a\ngiven parameterization (DNN with weights), we seek to predict its generalization performance using\nmeasures of the model’s regularity or smoothness. While a comprehensive approach would count\noscillations at every node (e.g. neuron) in the feedforward directed acyclic graph G shared by the\ntarget function and its neural network approximation, due to the large nodal degree of typical DNNs\nand for the purpose of this challenge, we seek a less expensive measure of the network’s smoothness\nthat can operate within the challenge’s computational parameters (≤5min/model, ≤20GB RAM).\nPreprint. Under review.\narXiv:2012.06969v2  [stat.ML]  16 Dec 2020\n2\nMeasures of Distortion\nVector quantization (VQ) is intimately connected to both statistical learning theory and pattern\nrecognition. In communications, VQ is used at the receiver to determine which (possibly high-\ndimensional) codewords were transmitted. If we view image classiﬁcation through the same lens,\nwe see that deep networks are simply an elaborate decoder for the classiﬁcation labels (e.g. one-hot\nencoded or multi-level). For deterministic feedforward networks that are trained well, features\nextracted in the intermediate layers are drop-in replacements for the inputs. That is, the role of the\ndeep network is to degeneratively transform seemingly disparate, high dimensional image vectors to\nclustered representations that are ultimately quantized to the classiﬁcation labels.\nAlthough the ﬁnal quantization rule can be quite complicated due to the high nodal degree and\nnonlinear activation of many DNNs, [5] provides the insight that for valid compositional architectures\n(decoders), minimizing the Sobolev norm of the nodal functions yields provably good performance.\nThis implies that for two networks with the same training performance, the network with smoother\nnodes/layers will tend to have better generalization performance as the number of samples N →∞.\nWe postulate that a computationally efﬁcient alternative to computing the Sobolev norm at each node\nis to instead estimate the complexity of the required VQ rule at that layer. For example, if we view the\nfeature distribution of training data at an intermediate layer ℓas a symbol constellation, a complicated\nVQ rule would correspond to a highly oscillatory (rough) function at the next layer ℓ+ 1.\n2.1\nSymbol Distortion and Euclidean Metrics\nWe can use p-norms to measure the expected complexity or distortion of VQ at an intermediate\nlayer, starting with the input layer (prior to any network operation). This is commonly used in VQ\nto measure the mismatch between the source symbols and reconstructed symbols [6], and also has\nconnections to the mesh norm minx,y∈T,x̸=y ∥x −y∥that is central to approximation theory. As seen\nin Figure 1, the mean minimum intra-class distance of training data reduces deeper in the network.\nFigure 1: L2 distortion matrices as a function of layer (PGDL Model 668). The diagonal and off-\ndiagonal elements represent the mean minimum intra- and inter-class 2-norm distance, respectively.\n2.2\nLabel Distortion\nDue to the high nodal degree of current DNNs, p-norms may systematically over estimate the\ncomplexity of the required VQ. A simpler metric closer to the classiﬁcation task is the accuracy of a\nnearest-neighbor-type classiﬁer. Good networks will have diagonally dominant confusion matrices,\ncorresponding to simple classiﬁcation boundaries even if the distance between samples is large.\nHowever, such classiﬁers also depend on p norms. To better capture distances between the high\ndimensional feature vectors, we can use dimensionality reduction techniques, e.g. kernel PCA.\n2.2.1\nKernel Principal Component Analysis (kPCA) and Gaussian Mixture Models (GMMs)\nUnfortunately, computing the accuracy of a nearest-neighbor classiﬁer using transformed coordinates\nof the training data’s features does not correlate well with generalization (test) performance of models\nwith good training accuracy, although it can help identify networks with unwanted degeneracy.\nInstead, we compute the principal components and evaluate the “validation” accuracy of the resulting\nnearest neighbor classiﬁer using different mutually-exclusive subsets of the training data. Since\neach kernel PCA model is trained on only a subset of the training data’s features (e.g. at layer ℓ),\nthe resulting confusion matrix measures how disparate the features of training data are, or how\ngeneralizable the features are across different subsets.\n2\nTo enable faster computation during the competition, we additionally employ Gaussian mixture mod-\nels (GMMs) that are initialized and trained on each class independently with 3-5 components per class.\nIn addition to enabling a fast classiﬁcation computation (e.g. closest mixture centroid), GMMs can be\nused to measure the conﬁdence in each prediction with respect to the distribution of the chosen training\nsubset. That is, for each class i, we compute the distortion di = Exj{maxk∈GMMj p(Θk|xi)} ∀j\nwhere j represents each class in the dataset; k corresponds to each of the GMM components of class\nj represented by GMMj; and the expectation is approximated by calculating the mean of inner\nmaximum values for all data points in class j.\n2.3\nSupport Vector Machines (SVMs)\nTo provide additional supervision into the classiﬁer, we can employ support vector machines (SVMs).\nUnlike a nearest-neighbor or GMM-based classiﬁers, kernel SVMs can easily achieve arbitrary\ntraining accuracy, approaching the performance of neural network layers at the expense of more\nparameters. That is, in addition to computing confusion matrices and conﬁdence measures based on\nthe distance to the boundary (e.g. margin error), the complexity of the SVM can be directly measured\nusing the number of support vectors required for ϵ training error. The more complicated the decision\nfunction, the more support vectors that are required, and higher the expected Sobolev norm.\n3\nResults\nFor the PGDL competition, we evaluated how well the aforementioned measures of distortion\ncorrelate with test performance. With the exception of the direct SVM complexity metric (number\nof support vectors), for each model we computed the layer-wise symbol and label distortion using\nintermediate features (L2 distance), their projected (kPCA) coordinates relative to GMM centroids,\nand kernel (radial basis function) SVMs. The normalized trace (mean of the diagonal) of these\nmatrices was used as the ﬁnal complexity measure for these cases. The correlation with the test\naccuracy for each Task-1 and Task-2 DNN model in the competition’s public dataset is shown in\nFigures 2a-2b, while a comparison of correlation values is displayed in Table 1. As can be seen, the\nnumber of support vectors (#SVs) provides the best correlation for both Task-1 and Task-2 models.\n(a) Task-1 models.\n(b) Task-2 models.\nFigure 2: Distortion-test accuracy correlations for different distortion measures.\nTable 1: R2 values of distortion-test accuracy correlations\nDistortion measure\nTask-1\nTask-2\nTrainset\nTestset\nTrainset\nTestset\nIntermediate features\n0.53\n0.61\n0.56\n0.9\nGMM\n0.31\n0.39\n0.10\n0.43\nSVM\n0.50\n0.71\n0.44\n0.93\n#SVs\n0.59\n0.93\n0.81\n0.98\n3\nTo further demonstrate the properties of our complexity measures, we analyse the distortion matrices\nfor four representative models from the Task 1 public dataset, indicated by stars in the correlation plots\nof Figure 2a. Despite having similar training performance, these models differ in their generalization\naccuracy on the test set, as: model 668: 86.3%; model 542: 83.3%; model 152: 71.7% and model 90:\n66.8%. The corresponding distortion matrices for these models are shown in Figure 3. While the L2\ndistortion measure (Fig. 3a) is able to distinguish a well generalized model from a poorly generalized\none (by comparing the relative normalized trace), it is harder to draw a conclusion by just looking at a\nsingle model’s distortion matrix. This may be attributed to the limitation of the 2-norms in computing\nrepresentative distances between high dimensional features, as previously mentioned. The SVM\ndistortion measure overcomes this problem, as can be seen from Figure 3c. SVM distortions are able\nto quantify the distances between the points within a class and also between the point clusters of\ndifferent classes while maintaining a better correlation with model’s generalization.\n(a) L2 Distortion.\n(b) GMM label distortion.\n(c) SVM label distortion.\nFigure 3: Comparison of distortion measures.\nThe kPCA + GMM distortion measure (Figure 3b) tends to approach this but fails to quantify the\ndistortion for certain models. This might be due to the constraint imposed by the radial basis function\n(RBF) kernel used in kPCA, and a forced reduction in the dimensionality (from 100s to just 3). This\ncan be seen from Figure 4, which depicts the feature vectors for the above models after applying\nkPCA and ﬁtting GMMs. The ellipses of same color but with different gradients represent the clusters\nwithin a class. These plots provide a visual validation to our claim that better generalized models\nhave smoother intermediate feature representations and thus better separated class clusters.\n4\nFigure 4: Label wise GMM clusters for different models.\n4\nConclusion\nWe showed that simple measures of regularity can be used to predict generalization, laying the foun-\ndation for exploring generalization in deep learning from the point of view of approximation theory\nand computational harmonic analysis. We developed simple metrics to measure the smoothness of\nDNN layers by exploiting the similarity between intermediate feature representations extracted by the\nmodels. We evaluated these metrics against VGG-like CNN models and Network-in-network models\nand demonstrated that these metrics correlate well with the models’ generalization, independent of the\ntraining strategy. Currently our proposed metrics do not take into account the notion of adversarial\nperformance or architectural hyperparameters, and so in our future work we aim to incorporate these\nnotions to develop more accurate complexity measures that better match theoretical results.\nAcknowledgments and Disclosure of Funding\nThis research was supported by AFRL grant #FA8650-18-C-1137, NIH/NCI grant #R01CA229354,\nand NIH/NIBIB grant #1F32EB030411-01.\nReferences\n[1]\nYiding Jiang et al. “Predicting the generalization gap in deep networks with margin distributions”.\nIn: arXiv preprint arXiv:1810.00113 (2018).\n[2]\nYiding Jiang et al. “Fantastic generalization measures and where to ﬁnd them”. In: arXiv\npreprint arXiv:1912.02178 (2019).\n[3]\nHrushikesh N Mhaskar and Tomaso Poggio. “Deep vs. shallow networks: An approximation\ntheory perspective”. In: Analysis and Applications 14.06 (2016), pp. 829–848.\n[4]\nHN Mhaskar and Tomaso Poggio. “An analysis of training and generalization errors in shallow\nand deep networks”. In: Neural Networks 121 (2020), pp. 229–241.\n[5]\nAbhejit Rajagopal. “High-Dimensional Polynomial Approximation with Applications in Imag-\ning and Recognition”. PhD thesis. University of California, Santa Barbara, 2019.\n[6]\nSambu Seo and Klaus Obermayer. “Soft learning vector quantization”. In: Neural computation\n15.7 (2003), pp. 1589–1604.\n5\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2020-12-13",
  "updated": "2020-12-16"
}