{
  "id": "http://arxiv.org/abs/2012.07369v2",
  "title": "Learning for MPC with Stability & Safety Guarantees",
  "authors": [
    "Sébastien Gros",
    "Mario Zanon"
  ],
  "abstract": "The combination of learning methods with Model Predictive Control (MPC) has\nattracted a significant amount of attention in the recent literature. The hope\nof this combination is to reduce the reliance of MPC schemes on accurate\nmodels, and to tap into the fast developing machine learning and reinforcement\nlearning tools to exploit the growing amount of data available for many\nsystems. In particular, the combination of reinforcement learning and MPC has\nbeen proposed as a viable and theoretically justified approach to introduce\nexplainable, safe and stable policies in reinforcement learning. However, a\nformal theory detailing how the safety and stability of an MPC-based policy can\nbe maintained through the parameter updates delivered by the learning tools is\nstill lacking. This paper addresses this gap. The theory is developed for the\ngeneric Robust MPC case, and applied in simulation in the robust tube-based\nlinear MPC case, where the theory is fairly easy to deploy in practice. The\npaper focuses on Reinforcement Learning as a learning tool, but it applies to\nany learning method that updates the MPC parameters online.",
  "text": "Learning for MPC with Stability & Safety Guarantees ⋆\nSebastien Gros a, Mario Zanon b\naDept. of Cybernetics, Faculty of Information Technology, NTNU, Norway\nbIMT School for Advanced Studies Lucca, Piazza San Francesco 19, 55100, Lucca, Italy\nAbstract\nThe combination of learning methods with Model Predictive Control (MPC) has attracted a signiﬁcant amount of attention\nin the recent literature. The hope of this combination is to reduce the reliance of MPC schemes on accurate models, and to\ntap into the fast developing machine learning and reinforcement learning tools to exploit the growing amount of data available\nfor many systems. In particular, the combination of reinforcement learning and MPC has been proposed as a viable and\ntheoretically justiﬁed approach to introduce explainable, safe and stable policies in reinforcement learning. However, a formal\ntheory detailing how the safety and stability of an MPC-based policy can be maintained through the parameter updates\ndelivered by the learning tools is still lacking. This paper addresses this gap. The theory is developed for the generic Robust\nMPC case, and applied in simulation in the robust tube-based linear MPC case, where the theory is fairly easy to deploy in\npractice. The paper focuses on Reinforcement Learning as a learning tool, but it applies to any learning method that updates\nthe MPC parameters online.\nKey words: Safe MPC Learning, Safe MPC-based policies, Safe Reinforcement Learning, Robust MPC, Stability\n1\nIntroduction\nModel Predictive Control (MPC) is a very successful tool\nfor generating policies that minimize a certain cost un-\nder some state and input constraints. MPC uses model-\nbased predictions of the future system trajectories to\nproduce a control input proﬁle over a future time win-\ndow that satisﬁes the constraints while minimizing the\ncost. Closed-loop control policies are then obtained by\nupdating that control input proﬁle at every time instant,\nin a receding-horizon fashion, based on the latest state\nof the system and information on its environment. MPC\nheavily relies on a model of the system at hand to per-\nform well. However, accurate models are expensive to\ndevelop, and can be too complex to use in the context\nof MPC. As a result, while MPC can deliver a reason-\nable approximation of the optimal policy, it is usually\nsuboptimal.\n⋆This paper was not presented at any IFAC meeting.\nCorresponding author M. Zanon. This paper was partially\nsupported by the Italian Ministry of University and Re-\nsearch under the PRIN’17 project ”Data-driven learning of\nconstrained control systems” , contract no. 2017J89ARP;\nand by the Norwegian Research Council project “Safe\nReinforcement-Learning using MPC” (SARLEM).\nEmail addresses: sebastien.gros@ntnu.no (Sebastien\nGros), mario.zanon@imtlucca.it (Mario Zanon).\nIn the recent literature, various learning techniques, of-\nten borrowed from the ﬁeld of machine learning, have\nbeen investigated to address this problem. A number\nof them focus on using machine learning to improve\nthe ﬁtting of the MPC model to the data, see, e.g., [11]\nand references therein. Other approaches argue for ad-\njusting the MPC model, cost and constraints in view\nof maximizing directly the closed-loop performance.\nThe authors of [9] provide strong theoretical justiﬁ-\ncations for this approach, and propose to use Rein-\nforcement Learning (RL) techniques to perform that\nadjustment in practice. The use of learning techniques\nwithin control has been proposed in, e.g., [3,4,14–\n16,21,23,2,9,31,33,18,34,1,30,27].\nA prime motivation for using MPC-based policies is the\npossibility to enforce constraints on the system trajecto-\nries. This feature can be leveraged to ensure the safety of\nthe system at hand by deﬁning constraints that limit its\nevolution to a safe set, and including these constraints\nin the MPC formulation. When using inaccurate mod-\nels, or when the real system is stochastic, safety can be\nmaintained via robust MPC techniques, where the un-\ncertainties are taken into account in the MPC formula-\ntion. Robust MPC delivers policies that are safe by con-\nstruction, using a worst-case approach, and ensures that\nthe real system trajectories satisfy the constraints at all\nPreprint submitted to Automatica\n25 July 2022\narXiv:2012.07369v2  [cs.LG]  22 Jul 2022\ntime.\nThe combination of learning with robust MPC has been\ninvestigated in [31]. This combination arguably oﬀers\nthe most direct pathway to optimize the closed-loop per-\nformance of an MPC-based policy while maintaining its\nsafety. The authors of [31] additionally argue that ro-\nbust MPC oﬀers a direct pathway towards safe reinforce-\nment learning, providing strong certiﬁcates of safety. In-\ndeed, when using robust MPC, the policies produced via\nlearning can be made safe and stable by construction by\nimposing certain constraints on the parameter updates\nsuggested by the learning algorithm [31]. In contrast, in\nclassic reinforcement learning, e.g., based on Deep Neu-\nral Networks (DNN), enforcing the safety of the resulting\npolicy is typically done via extensive in silico validations\nusing Monte Carlo techniques. In that context, formal\ncertiﬁcates of strong safety require in principle an inﬁ-\nnite amount of simulations and are therefore diﬃcult to\nestablish in practice.\nWhile [31] details how to learn policies that are safe and\nstable by construction, an important gap remains to be\naddressed. Performing learning on a control policy re-\nquires that regular updates of the policy parameters are\nimplemented. Some learning methods implement the up-\ndates at every sampling time of the policy, while other\nmethods implement the updates less frequently. If the\nparameter updates are to be implemented while the sys-\ntem is being operated, then implementing safe and stable\npolicies at every parameter update does not necessarily\nyield an overall safe and stable closed-loop system. In-\ndeed, safety with parameter updates can only be guar-\nanteed if the update takes place when the system state is\nwithin speciﬁc sets. Stability is also not guaranteed when\nthe parameters are frequently updated, even if each pol-\nicy implemented is stable. Ensuring safety and stability\nthrough the parameter updates then requires additional\nconditions. This paper addresses that issue by detailing\nhow to maintain safety and stability through the param-\neter updates, and therefore provides a complete theo-\nretical framework to deploy safe and stable learning for\nMPC. The paper focuses on learning techniques based on\nreinforcement learning, but applies equally to all learn-\ning techniques that propose directions in the MPC pa-\nrameters space along which the MPC parameters ought\nto be updated.\nThe paper is organized as follows. Section 2 provides\nbackground material on MDPs. Section 3 proposes a def-\ninition of safe policies and Section 4 provides background\nmaterial on robust MPC. Section 5 presents conditions\nfor building a safe combination of RL and robust MPC,\nwhere the parameter updates provided by RL do not\njeopardize the safety of the robust MPC scheme. Sec-\ntion 6 presents conditions for RL to update the MPC\nparameters while maintaining the system stability, dis-\ncussed in an augmented parameter-state space. Section 7\nillustrates the theoretical results by means of two simple\nexamples and Section 8 concludes the paper.\n2\nBackground\nWe consider real systems that can be described as\ndiscrete-time Markov chains with continuous state and\naction spaces. We will label the underlying conditional\ntransition probability density over the states s and\nactions a as:\nϕ [ si+1 | si, ai] : Rn × Rn × Rm →R+,\n(1)\nwhere n and m are the state and input space sizes, re-\nspectively. Throughout the paper, index i will refer to\nthe physical time of the system. We will assume that (1)\nis only inaccurately known. We will assume in the fol-\nlowing that a stage cost\nL (si, ai) : Rn × Rm →R\n(2)\nis provided, and that our goal is to ﬁnd the parameters\nθ of a policy\nπθ : Rn →Rm\n(3)\ndelivering the actions, i.e., ai = πθ (si), so as to mini-\nmize the expected discounted cost:\nJ (πθ) = E\n\"\n∞\nX\ni=0\nγiL (si, ai)\n\f\f\f\f\f ai = πθ (si)\n#\n,\n(4)\nwhere E[·] is the expected value operator applying to the\nreal trajectories yielded by (1) in closed-loop with policy\nπθ, and γ ∈(0, 1] a discount factor. We will consider that\nthe actions delivered by the policy are possibly restricted\nto a subset of Rm, i.e., the minimization of J is subject\nto\nπθ (si) ∈P,\n∀si.\n(5)\nFinding the parameters θ that (locally) minimize the\nclosed-loop cost J (πθ), and therefore maximize the\nclosed-loop performance of the policy is arguably one\nof the main goals of any learning algorithms focusing\non improving a policy. E.g., many methods in Rein-\nforcement Learning (RL) deal with the evaluation of\nthe policy gradient ∇θJ (πθ), which is used to update\nthe policy parameters θ such that J (πθ) is sequentially\ndecreased. In this paper, we will focus for simplicity on\nparameter updates that reduce J (πθ) directly, albeit\na number of learning techniques compute updates that\nare not directly based on J (πθ). The theory presented\nhere applies to all learning methods for MPC with minor\nmodiﬁcations.\nWe consider in this paper that we seek policies that keep\nthe system safe in the sense of respecting some state\n2\nconstraints expressed as:\nsi ∈X,\n(6)\nfor any time i = 0, . . . , ∞, where s0,...,∞denotes the\nclosed-loop trajectories with policy πθ. Note that for\nthe sake of simplicity, we will not treat mixed state-\ninput constraints here, even though the proposed results\narguably readily extend to that case. Throughout the\npaper, we will assume that the real state transition (1)\nis imperfectly or only coarsely known, and diﬃcult to\ncapture via simple mathematical models.\n3\nSafe Policies\nIdeally, a policy is safe if (6) holds at all time i with\nprobability one. Unfortunately, guaranteeing this uni-\ntary probability is impossible without a perfect knowl-\nedge of the system dynamics (1). In that context, a more\nrealistic notion of safety can be deﬁned in the context\nof Bayesian inference, where safety is regarded as prob-\nabilistic, conditioned on our knowledge of the system\n(prior and actual data), labeled D. Such data can, e.g.,\nbe the set of all state transitions s, a, s+ observed so far,\nbut also include some prior knowledge of the system.\nThe strictest notion of safety hence becomes a proba-\nbilistic counterpart of (6), which can be deﬁned in terms\nof constraint satisfaction as:\nσ := P [ si ∈X\n∀i | D ] .\n(7)\nProbability (7) is epistemological, and to be understood\nin the context of Bayesian hypothesis testing. It under-\nlines that safety can only be a belief conditioned on our\ncurrent knowledge of the system at hand. In this paper,\nwe adopt that operational notion of knowledge-based\nsafety and label a policy satisfying (7) as σ-safe. Assess-\ning probability (7) in practice can be diﬃcult, but it can\narguably be done in several ways:\n1. Direct data-based: data D are collected on the real\nsystem, and used to infer an estimation of σ, with-\nout using a model of (1). The obvious diﬃculty here\nlies in the need for collecting an extremely large\ndata set if a policy with σ close to one is to be de-\nsigned. Real data are costly, especially for safety-\ncritical systems, and designing policies that achieve\na high σ can be unrealistic in that context.\n2. Direct model-based: a “pessimistic” simulation\nmodel of the real system is constructed (see Equa-\ntions (8)-(11) below) from D, and σ is estimated\nin silico via Monte Carlo methods. If the model is\npessimistic, the in-silico estimation of σ converges\n(as more in-silico data are generated) to a lower\nbound for the true σ.\n3. Indirect model-based: similarly to 2, a “pessimistic”\ncontrol model of the real system is constructed, and\nused to build policies that are safe by construction\nfor that model. Monte Carlo sampling is here re-\nplaced by an explicit or implicit propagation of the\nuncertainty set, based on an overestimation of the\nsupport of (1). The resulting σ is by construction\na lower bound for (7).\nExamples of approach 1. can be found in, e.g., [8,10];\nexamples of approach 2. can be found in [24,35]; and\nexamples of approach 3. include, e.g., [6,20,29]. While\napproach 2. is often used in the context of safe rein-\nforcement learning (providing only weak guarantees that\nhold upon convergence) and to certify control policies\nin practice, it is diﬃcult to provide strong safety guar-\nantees in that context. In this paper, we will consider\nthe third approach, based on robust MPC techniques.\nOur intentions are two-fold. First, we present a theory\nthat speciﬁes formally how learning can be deployed in\nthe MPC context without jeopardizing the closed-loop\nstability and safety of the resulting policy throughout\nthe learning process. Second, we propose this framework\nas one viable approach to safe reinforcement learning if\nstrong safety and stability guarantees are expected to be\nsatisﬁed throughout the learning process.\nThe models of the real system used in approaches 2.\nand 3. above are often built as structured models that\ncan, e.g., take the form:\nsi+1 = Fϑ (si, ai, wi) ,\n(8)\nwhere Fϑ : Rn × Rm × Rd →Rn, and ϑ is the set of\nmodel parameters. Moreover, variable\nwi ∈Wϑ ⊂Rd,\n(9)\nis an external, typically stochastic disturbance contained\nin a compact set Wϑ, modeling the stochasticity of the\nreal system. The notion of “pessimistic” model used\nabove then requires that for any state-action pair s, a,\nthe support of the real system dynamics density (1) is\n(almost entirely) included in the set:\nDϑ (s, a) = { Fϑ (s, a, w) | ∀w ∈Wϑ} ,\n(10)\ni.e.,\nZ\nDϑ(s,a)\nϕ [ s+ | s, a] ds+ = 1,\n∀s, a.\n(11)\nCondition (11) provides a formal deﬁnition of a “pes-\nsimistic” model, i.e., a model that includes the support\nof the real state transition (1). Note that this condition\ndoes not necessarily need to be conservative. Indeed (11)\ncan in principle hold tightly, with Dϑ covering the sup-\nport of ϕ but not more. Condition (11) further entails\nthat a policy guaranteeing that (6) is satisﬁed for all the\npossible trajectories resulting from (8)-(9) is safe by con-\nstruction. Robust MPC techniques can be used to build\nsuch policies.\n3\nSimilarly to (7), the validity of (11) is limited to our\nknowledge of the system supported by all data and prior\nknowledge available about it, i.e., D. Condition (11) then\nought to be regarded as probabilistic as well, in which\ncase it becomes:\n˜σ := P [ (11) | D ] .\n(12)\nOne can easily verify that if a policy (5) ensures that\nthe closed-loop trajectories of the model (8)-(11) respect\nthe state constraints (6) at all time, then the probability\nthat the policy is safe for the real system is at least ˜σ,\nand σ ≥˜σ holds. In practice, a minimum requirement\nfor ˜σ > 0 to hold is to ensure that:\ns+ ∈Dϑ (s, a)\n(13)\nholds for all observed state transition triplets (s, a, s+) ∈\nD. Condition (13) then deﬁnes a set\nΘD := { ϑ | s+ ∈Dϑ (s, a) , ∀(s, a, s+) ∈D }\nwhere the model parameters ϑ should be restricted,\nwhich is typically tackled via set-membership system\nidentiﬁcation methods [5]. In this paper we will consider\na set ΘD possibly formed from (13), and possibly fur-\nther restricted by prior or structural knowledge of the\nsystem. For a complete discussion on the deﬁnition of\nΘD based on (13) and its deployment within safe RL,\nwe refer to [31].\n4\nRobust MPC as a safe & stable policy\nWe will consider the use of robust MPC as a means to\ngenerate safe policies πθ (si) for system (1), subject to\nthe associated performance index (4) and safety restric-\ntion (6), with the limitation (12). Note that these poli-\ncies are parametrized by parameter θ, which deﬁnes the\nrobust MPC scheme as we will discuss in the following. A\nstrong point of robust MPC is that safety in the sense of\nSection 3 and stability can be enforced by construction.\nA non-trivial remaining question is then how to retain\nsafety and stability through the learning process, where\nthe parameters of the robust MPC scheme are regularly\nupdated.\nA generic robust MPC scheme is based on predicting the\nsystem evolution at future times based on actions given\nby a sequence of future, parametrized policies:\nak = ηk\nθ (v, sk) ,\n(14)\nwhere the indices k refer to future time instances, occur-\nring at the corresponding physical times i + k and v is\na set of variables used to shape the policy sequence for\nthe speciﬁc current state of the system si. In this paper,\nwe consider robust MPC schemes of the form:\nˆVθ(si) = min\nv\nϕθ(v, si)\n(15a)\ns.t.\nXk+1 = Fϑ\n\u0000Xk, ηk\nθ (v, Xk) , Wθ\n\u0001\n, (15b)\nX0 = si,\nXθ\nN ⊆X f\nθ,\n(15c)\nηk\nθ (v, Xk) ⊆U,\nXθ\nk ⊆X,\n(15d)\n∀k = 0, . . . , N −1,\n(15e)\nwhere the state propagation (15b), called tube, is the\nextension of (8) to set propagation, i.e., we read (15b)\nas:\nXθ\nk+1 =\n\b\nFϑ\n\u0000s, ηk\nθ (v, s) , w\n\u0001\n| ∀s ∈Xθ\nk, w ∈Wθ\n\t\n.\n(16)\nThe robust MPC scheme (15) deﬁnes a policy πθ (s)\ngiven by the ﬁrst control input of the policy sequence\nadopted in the robust MPC scheme, i.e.:\nπθ (si) = η0\nθ (v⋆, si) ,\n(17)\nwhere v⋆is the solution of (15). The robust MPC model\nused in (15b) is then an intrinsic and central component\nof the robust MPC scheme formulation. We will then\nconsider that the model parameters ϑ are part of the\npolicy parameters θ, as they can be used to shape the\npolicy πθ delivered by the robust MPC scheme.\nSet U in (15) represents the possible limitations on the\nfeasible actions (e.g., actuator limitations), and the ter-\nminal set X f\nθ must be constructed such that (15) being\nfeasible entails that the state constraint Xk ⊆X can be\nenforced at all future times. This is typically achieved\nby resorting to a terminal control law which makes X f\nθ\nforward invariant. The cost function (15a) is left unspec-\niﬁed here, as it can take diﬀerent forms such as, e.g., a\nworst-case cost (as in min-max robust MPC); a nomi-\nnal cost (as in tube MPC); an expected cost, or more\nelaborate risk-adverse costs. Function ˆVθ(s) receives an\ninﬁnite value for the states s for which problem (15) is\ninfeasible. For the sake of simplicity, we do not consider\nmixed state-input constraints here, although the pro-\nposed theory readily applies to that case.\nWe ought to stress that any Robust MPC formula-\ntion can be considered in our framework, e.g., robust\nMPC [20,6,32], possibly also combined with non-\nstandard formulations such as [17]. Note that while\nnonlinear robust MPC is in general hard to formulate\nand solve, recent research has provided methods that\ncan be applied in practice, see, e.g., [19,29,13]. Further-\nmore, it is arguably useful here to stress that the choice\nof using robust MPC to carry safe and stable policies,\nwhile not necessarily straightforward to implement, is\nnonetheless not restrictive. Indeed, the existence of a\nsafe policy for the model (8)-(9) entails the existence of\n4\na robust MPC scheme (15) delivering a safe policy for\nthe system model (8)-(9), and therefore a σ-safe policy\nfor the real system (1). While these observations are\nwell understood in the MPC community, we provide\nhereafter a Lemma that highlights their importance in\na learning context.\nLemma 1 Assume that there exists a feasible policy π\nsuch that the trajectories of the model (8)-(9): (i) are in X\nwith probability 1 for a set of initial conditions s0 ∈X 0;\nand (ii) are asymptotically stabilized to some set L with\nan associated Lyapunov function Vπ(s). Then there exists\na robust MPC scheme of the form (15) that is σ-safe for\nthe real system dynamics (1), with probability at least σ\nof being stable for the real system dynamics (1).\nRemark 1 Before delivering the proof of this simple\nLemma, it ought to be stressed here that its purpose is not\nto specify how the robust MPC scheme should be built, but\nrather to dismiss potential concerns that robust MPC is a\nlimited tool for producing safe and stable policies. Indeed,\nit shows that if a safe and stable policy exists, then a ro-\nbust MPC scheme that produces a safe and stable policy\nalso exists.\nPROOF. For the ﬁrst claim it is suﬃcient to prove that\nis it possible to setup a recursively feasible robust MPC\nscheme such that the model dynamics satisfy (6). Let us\nselect ηk\nθ as\nηk\nθ (v, s) = π (s) + ρk\nθ (v, s) ,\n(18)\nwhere ρk\nθ is an arbitrary policy such that ρk\nθ (0, s) = 0\nfor all s, and select\nX f\nθ = X 0.\n(19)\nThen constraints (15b) and (15c) ensure that the robust\nMPC policy maintains the trajectories of the model (8)-\n(9) in the set X and that the robust MPC scheme is re-\ncursively feasible. Moreover, the choice of policy (18) en-\nsures that the constraints are feasible because the trivial\nchoice v = 0 is feasible.\nConcerning the second claim, we exploit the fact that the\npolicy is stabilizing and there exists a Lyapunov function\nVπ(s) associated with set L. By selecting the cost as\nϕθ(v, s) = Vπ(s) + v⊤v,\nwe obtain that ηk\nθ (0, s) is optimal and the MPC value\nfunction is a Lyapunov function, i.e., it satisﬁes ˆVθ(s) =\nVπ(s). Since policy π is safe and stabilizing by assump-\ntion, the corresponding robust MPC scheme is safe and\nstable for the model dynamics (8)-(9). Then, using (12),\nwe conclude that the resulting policy is σ-safe for the\nreal system dynamics (1) and stabilizing with probabil-\nity σ.\n□\nLemma 1 is further discussed in Remark 2 below.\nIn practice, for simplicity, the parametrized policy (14)\nused in (15) is often selected as a nominal state-input\nreference sequence ¯u0,...,N−1, ¯x0,...,N−1 together with an\nadditional linear state feedback, i.e.,\nηk\nθ (v, s) = ¯uk −Kθ (s −¯xk) ,\n(20)\nv = {¯u0,...,N−1, ¯x0,...,N−1} .\n(21)\nIn that speciﬁc case, the policy (17) deﬁned by the robust\nMPC scheme becomes\nπθ (si) = η0\nθ (v⋆, si) = ¯u⋆\n0.\n(22)\nWe deﬁne next some important sets in the space of pa-\nrameters θ.\nDeﬁnition 1 Let us deﬁne the set ΘF of parameters\nθ such that the robust MPC scheme (15) is recursively\nfeasible for the dynamics (8)-(9) for some non-empty set\nX 0\nθ of initial conditions.\nDeﬁnition 2 Let us deﬁne the set ΘL of parameters θ\nsuch that the value function ˆVθ(s) deﬁned by (15) is a\nLyapunov function for the dynamics (8)-(9) with respect\nto a set Lθ.\nIn order to make Deﬁnition 2 more concrete, we mention\nas an example the following conditions for ˆVθ(s) to be a\nLyapunov function, which are classic in robust MPC:\n(1) it is lower and upper bounded by K∞functions;\n(2) ˆVθ(s+) ≤γ ˆVθ(s)+δθ holds for all s ∈X 0\nθ and where\ns+ ∈{ Fϑ (s, πθ (v, s) , w) | ∀w ∈Wθ} ,\nfor some positive constants δθ, and γ < 1.\nThen set Lθ is deﬁned as:\nLθ =\n\u001a\ns\n\f\f\f\f ˆVθ(s) ≤\nδθ\n1 −γ\n\u001b\n,\n(23)\nand the model (8)-(9) is stabilized to Lθ for any initial\ncondition s ∈X 0\nθ , see [25]. In Section 6, we will use this\ndeﬁnition of Lyapunov function for the sake of simplicity,\nthough our results hold in general, mutatis mutandis.\nIn the context of Deﬁnitions 1 and 2, the σ-safety of\nthe real system is guaranteed for θ ∈ΘD ∩ΘF, and\nthe stability of the real system is guaranteed for θ ∈\nΘD ∩ΘL with probability at least σ. We note that\n5\nestablishing conditions on θ such that θ ∈ΘF ∩ΘL\nis typically done in practice via min-max robust MPC\nor tube-based MPC [25]. We ought to stress that the\nconditions above are provided as an example which is\nfairly general, as it includes the case of [25, Section 3.4],\nas well as [20,32]. However, more generic conditions are\napplicable and can be used in our theory.\nRemark 2 We observe that Lemma 1 is of little practi-\ncal use since it constructs a speciﬁc MPC scheme using\na safe and stabilizing policy which is not known. Nev-\nertheless, it allows us to obtain an important theoretical\nconsideration. Provided that the MPC parametrization is\nrich enough, there does exist a parameter which yields a\nnon-conservative model (8)-(9), and a design of the MPC\ncost and feedback (14) such that the scheme we construct\nin the proof of the lemma can be captured by an adequate\nchoice of the parameter. This means that there exists a\nparameter θ ∈ΘD ∩ΘF ∩ΘL. Consequently, the inter-\nsection of these three sets is nonempty for a rich-enough\nparametrization of the robust MPC scheme.\nAssumption 1 In the remainder of the paper, we will\nuse the following assumptions:\n1. The set of feasible initial conditions X 0\nθ is compact\nand continuous in θ.\n2. The conditional density (1) underlying the real sys-\ntem is bounded for all state-action pairs, i.e.,\nϕ [ s+ | s, a ] ≤¯ϕ < ∞,\n∀s+, s, a.\n(24)\nThese technical assumptions will be required in some\nparts of the theory proposed in this paper. We ought to\nnote here that the compactness of set X 0\nθ is a mild as-\nsumption for a well-posed MPC scheme, and is standard\nin the robust MPC literature [6,20]. The continuity of\nthe set with respect to θ holds if, e.g., all functions in-\nvolved in the MPC scheme are suﬃciently diﬀerentiable,\nand if the MPC formulation satisﬁes standard regular-\nity assumptions for Nonlinear Programs [22,7]. Finally,\nwe stress that one speciﬁc setting of learning for robust\nMPC trivially satisﬁes this continuity assumption. In-\ndeed, if the learning is not allowed to change the model\nand safety constraints in the robust MPC scheme, then\nX 0\nθ is independent of θ and continuity trivially follows.\nWe should underline here that this case is in principle\nnot restrictive. Indeed, as argued in [9], optimality can be\nrecovered by adjusting the cost only, then such a choice\ndoes not introduce any theoretical restriction. Assump-\ntion 1.2 requires that all the states of the real system are\nsubject to some stochasticity. It is a technical assump-\ntion aimed at simplifying the subsequent discussions,\nand could arguably be relaxed at the cost of making the\nsubsequent argumentations signiﬁcantly more technical.\n4.1\nSafety & Stability Constraints in RL\nIn the following, we will consider the safety and stability\nconditions detailed in this section as constraints applied\nto the RL steps updating the policy parameters. More\nspeciﬁcally, similarly to [31], we consider that the RL\nsteps taken on the robust MPC scheme are feasible steps\n∆θ taken on the constrained optimization problem:\nmin\nθ\nJ (πθ) ,\n(25a)\ns.t.\nθ ∈ΘL ∩ΘF ∩ΘD,\n(25b)\nin the sense that each parameter update θp+1 = θp +\n∆θ, labelled by the index p, satisﬁes (25b) and reduces\nthe cost (25a). More speciﬁcally, the RL steps will be\ncomputed according to:\nmin\nθp+1\n1\n2 ∥θp+1 −θp∥2\nH + α∇θJ\n\u0000πθp\n\u0001⊤(θp+1 −θp) ,\n(26a)\ns.t.\nθp+1 ∈ΘL ∩ΘF ∩ΘD,\n(26b)\nfor some positive-deﬁnite matrix H ≈∇2\nθJ (πθ), and\nsome α ∈(0, 1]. We should recall here that for any H ≻0\nand α small enough, the sequence θ0,...,∞stemming from\n(26) converges to a (possibly local) solution of (25) [22].\nIt is useful to observe here that classic policy gradient\nmethods [26,28] in RL are typically based on (26a), with\nthe exclusion of the safety and stability constraint (26b).\nThe identity matrix is often used for H when the policy\nis based on very high dimensional approximators such\nas, e.g., Deep Neural Networks.\nWe will assume here that the gradient ∇θJ in (26a)\nis either evaluated directly via actor-critic or policy\nsearch techniques, or replaced by a surrogate based on\nQ-learning techniques, all formed using data collected\non the real system in closed-loop with policy πθ. The\nsafety and stability constraints (25b) will then be built\nbased on (6), (8), (9), and (13). In the remainder of the\npaper, a mild technical assumption on the Nonlinear\nProgram (26) will be very helpful.\nAssumption 2 The solution θp+1 of (26) is continuous\nwith respect to α in a neighborhood of α = 0.\nAssumption 2 follows from technical assumptions on the\nset ΘL ∩ΘF ∩ΘD, which we propose to not discuss\nextensively here for the sake of brevity. In particular,\nwe note that Assumption 2 naturally holds if the set\nΘL ∩ΘF ∩ΘD can be represented by a ﬁnite set of\ncontinuous inequality constraints, and if the resulting\nproblem (26) fulﬁls classical regularity assumptions and\nsuﬃcient second-order conditions (SOSC) [22].\n6\nAn important question that needs to be addressed is\nhow feasible parameter updates θp+1 = θp + ∆θ result-\ning from (26) can be implemented in the robust MPC\nscheme without jeopardizing the safety and stability of\nthe closed-loop system. We discuss the safety question\nin the next section using two diﬀerent approaches.\n5\nRecursive Feasibility with RL-Based Param-\neter Updates\nLet us consider a sequence of parameters θ0,...,∞result-\ning from (26), and consider that each parameter θp of\nthat sequence is applied for a certain amount of time (i.e.,\nat least one sampling time of the robust MPC scheme).\nThis section provides conditions such that this sequence\nof parameter updates does not jeopardize the safety of\nthe corresponding sequence of policies πθ0,...,∞result-\ning from the corresponding robust MPC schemes. Note\nthat a parameter update θp →θp+1 occurring at a time\nsample i means here that ai = πθp+1 (si) and the inputs\naj = πθp (sj) with j < i are used from the previous pa-\nrameter update. The next theorem provides a ﬁrst set\nof conditions for ensuring the safety of the parameter\nupdates.\nTheorem 1 Assume that for all p, parameter θp satis-\nﬁes θp ∈ΘF ∩ΘD, and that the initial conditions s0 are\nin the set X 0\nθ0. If each parameter update θp →θp+1 takes\nplace in a state si such that\nsi ∈X 0\nθp+1\n(27)\nholds, then the closed-loop trajectories s0,...,∞resulting\nfrom applying the sequence of policies πθ0,...,∞is σ-safe.\nPROOF. Assuming that (11) holds, then a standard\nresult for robust MPC is that if θp+1 ∈ΘF and if the\ninitial state at which policy πθp+1 is deployed satisﬁes\ncondition (27), then policy πθp+1 ensures that the state\ntrajectories are feasible at all time with unitary proba-\nbility. We then observe that if every parameter update\nθp →θp+1 is applied under condition (27), then each\npolicy keeps the state trajectories feasible. As a result, if\n(27) is ensured at every parameter update θp →θp+1,\nthen the entire state trajectory s0,...,∞resulting from the\npolicy sequence πθ0,...,∞remains feasible at all time.\nIf statement (11) does not hold, then the closed-loop\ntrajectories s0,...,∞may become infeasible, though not\nnecessarily. Hence if statement (11) holds with a prob-\nability σ, then the closed-loop trajectories s0,...,∞have\nprobability no smaller than σ to be feasible.\n□\nThe results of Theorem 1 can be leveraged in practice\nby solving the robust MPC schemes associated to both\nθp and θp+1 in parallel at every sampling instant and\nselecting the control input associated to θp+1 as soon as\nthe MPC scheme associated to θp+1 is feasible.\nThe theorem ensures the recursive feasibility of the se-\nquence of robust MPC schemes such that the closed-loop\nstate trajectories are contained in the set X, within the\nframework presented in Section 3. An important caveat,\nthough, is that there is no guarantee that condition (27)\ncan be met in ﬁnite time by the closed-loop trajectories\nunder policy πθp. As a result, it might be possible that a\nparameter update θp+1, though feasible for (25), yields\nan update condition (27) that never becomes satisﬁed,\nhence blocking the learning process. The remainder of\nthis section proposes two diﬀerent approaches to tackle\nthat issue, using either backtracking or additional con-\nstraints in (25).\nIn order to support and simplify the coming argumen-\ntation, it is useful to introduce a technical lemma. Let\nus consider the trivial locally compact measure on Rn,\nassociating to any compact set A ⊂Rn the bounded\npositive real number:\nµ (A) =\nZ\nA\nds.\n(28)\nLemma 2 Suppose that Assumption 1.2 holds and con-\nsider an arbitrary policy a = π (s), yielding a (continu-\nous) Markov Chain s0,...,∞. Then for any set A and ini-\ntial condition s0 ∈A, the following inequality holds:\nP[ s0,...,∞∈A ] ≤µ (A) ¯ϕ\n(29)\nwhere ¯ϕ is deﬁned by (24).\nPROOF. Denoting φ[sk] the density of the Markov\nchain at time k, we observe that:\nφ[sk] :=\nZ\nϕ [ sk | sk−1, π (sk−1) ] φ[sk−1]dsk−1 ≤¯ϕ\n(30)\nholds for any k > 0. It follows that\nP[sk ∈A] =\nZ\nA\nφ[sk]dsk ≤µ (A) ¯ϕ.\n(31)\nThen (29) follows from the Fr´echet inequalities, stating:\nP[s0,...,∞∈A] ≤inf\nk\nP[sk ∈A] ≤µ (A) ¯ϕ.\n(32)\n□\n7\n5.1\nParameter Update via Backtracking\nIn this subsection, we consider the use of backtracking\non the parameter updates computed according to (26)\nto ensure the feasibility of updating the parameters in\nﬁnite time. For the sake of simplicity in the following\ndevelopments, rather than a line-search strategy [22], we\nuse a gradient adaptation strategy in the cost (26a), by\niteratively reducing parameter α, therefore generating a\nstep ranging from a full step (α = 1) to ∆θ = 0 (with\nα = 0). The following theorem then guarantees that\nthere is some α > 0 such that the probability that the\nparameter update condition (27) is not met in ﬁnite time\nis less than 1 −σ.\nTheorem 2 Consider the closed loop trajectory si,...,∞\nunder policy πθp starting at the physical sampling time\ni with the initial state si. Consider the parameter up-\ndate θp+1(α) (where we highlight the dependency on α)\nresulting from (26) and suppose that Assumptions 1-2\nhold. Then the probability that the update condition (27)\nis not met in ﬁnite time can be made arbitrarily small by\nselecting α small enough, i.e., the following limit holds:\nlim\nα→0 P\nh\nsi,...,∞/∈X 0\nθp+1(α)\ni\n≤1 −σ.\n(33)\nA simple interpretation of Theorem 2 is that it is always\npossible to backtrack to a short-enough parameter up-\ndate (α small enough) such that the update condition\n(27) becomes satisﬁed in ﬁnite time with probability ar-\nbitrarily close to 1 −σ.\nThe intuition behind this result is that, by continuity\narguments, X 0\nθp+1(α) tends to X 0\nθp as α becomes small,\nsuch that the two sets match asymptotically. Moreover,\nwe observe that under policy πθp, the closed-loop tra-\njectories evolve in set X 0\nθp and the update is infeasible\nif they are outside of set X 0\nθp+1(α). It follows that for a\nparameter update to be blocked forever despite α being\narbitrarily small, if (11) holds, the closed-loop state tra-\njectories under policy πθp need to evolve on an inﬁnitely\nsmall set. This would require unbounded densities in the\nreal closed-loop dynamics (1), which is excluded by As-\nsumption 1, or that (11) does not hold. We formalize\nthese explanations in the next proof.\nPROOF. (of Theorem 2) If (11) holds, we ﬁrst observe\nthat θp+1(0) = θp trivially holds, such that\nsi,...,∞∈X 0\nθp+1(0)\n(34)\nholds by construction. Let us further deﬁne the set:\n∆X 0(θp, θp+1) =\nn\ns\n\f\f\f s ∈X 0\nθp\nand\ns /∈X 0\nθp+1\no\n,\n(35)\nAlgorithm 1: Safe and Stable learning - back-\ntracking\nInput: MPC parameter θ, and ϱ, n, H\n1 while Learning do\n2\nSet update = false, α = 1, fail = 0\n3\nCompute θ+ from (26)\n4\nwhile not update do\n5\nCompute MPC solution u0 from θ\n6\nCompute MPC solution u+\n0 from θ+\n7\nif MPC solution from θ+ is feasible then\n8\nSet u0 ←u+\n0 and θ ←θ+\n9\nupdate = true\n10\nelse\n11\nfail = fail + 1\n12\nApply input u0 to the system\n13\nif fail ≥n then\n14\nα = ϱα and recompute θ+ from (26)\nsuch that ∆X 0(θp, θp) = ∅. A trajectory sk,...,∞in\nclosed-loop under policy πθp that never satisﬁes the up-\ndate condition (27) must evolve in ∆X 0(θp, θp+1). We\nobserve that by Assumption 2 θp+1(α) is continuous in\na neighborhood of α = 0, and by Assumption 1.1 we\nhave that the set ∆X 0(θp, θp+1) is continuous in θp+1.\nIt follows that\nlim\nα→0 µ\n\u0000∆X 0(θp, θp+1(α))\n\u0001\n= 0.\n(36)\nWe can then conclude using Lemma 2:\nlim\nα→0 P\nh\nsi,...,∞/∈X 0\nθp+1(α)\ni\n=\n(37)\nlim\nα→0 P\n\u0002\nsi,...,∞∈∆X 0(θp, θp+1(α))\n\u0003\n= 0.\nSince (11) holds with probability σ, (33) readily follows.\n□\nA practical implementation of the backtracking ap-\nproach is detailed in Algorithm 1. The implementation\nconsists in reducing parameter α if the update condi-\ntion is not met for n time steps. We ought to stress here\nthat lines 3 and 14 of Algorithm 1 can be performed of-\nﬂine independently of the state of the system and of the\nrobust MPC schemes. It follows that the online compu-\ntational burden is limited to solving two independent\nrobust MPC schemes, possibly in parallel.\n5.2\nParameter Updates via Constrained Feasibility\nAs an alternative to backtracking, we propose next an\napproach imposing additional constraints in (26). We\nthen no longer rely on taking short-enough steps in θ\nto achieve the feasibility of the parameter updates, but\n8\nrather form an update that is feasible by construction.\nThis entails that updates can be performed at every time\nstep i such that p = i, hence we will use the notation θi\nthroughout this section.\nWe deﬁne as Xθi+1\n1\n(si, πθi(si)) the 1-step dispersion set\nstarting from state si, applying the input a = πθi(si),\nand using set Wθi+1 to model the stochasticity of the\nsystem. Note the subtle but important diﬀerence with\nXθi+1\n1\n= Xθi+1\n1\n(si, πθi+1(si)), where we apply action a =\nπθi+1(si) instead of a = πθi(si).\nTheorem 3 The parameter update θi+1 given by\nmin\nθi+1\n1\n2 ∥θi+1 −θi∥2\nH + ∇θJ (πθi)⊤(θi+1 −θi) ,\n(38a)\ns.t.\nθi+1 ∈ΘL ∩ΘF ∩ΘD,\n(38b)\nX 0\nθi+1 ⊇Xθi+1\n1\n(si, πθi(si)).\n(38c)\nsatisﬁes (27) by construction with probability at least σ.\nPROOF. Equation (11) entails si+1 ∈Xθi+1\n1\n(si, πθi(si)).\nUsing (38c), Xθi+1\n1\n(si, πθi(si)) ⊆X 0\nθi+1 holds, and ro-\nbust MPC is feasible for all possible realizations of si+1,\ni.e., si+1 ∈X 0\nθi+1. Since (11) holds with probability σ,\n(27) holds with probability at least σ.\n□\nWe elaborate next on how constraint (38c) can be formu-\nlated. We observe that recursive feasibility of MPC (15)\nimplies that, if a given state is feasible, then the tube\naround the predicted trajectory is also feasible, such that\nXθi+1\n1\n(si, πθi(si)) ⊆X 0\nθi+1\n⇐\nsi ∈X 0\nθi+1(πθi(si)),\n(39)\nwhere X 0\nθi+1(a) deﬁnes the set of states s for which the\nrobust MPC scheme (15) is feasible under the additional\nconstraint ¯u0 = a. The second condition in (39) is more\neasily written as a condition on the parameters and the\nnominal MPC trajectory: a detailed discussion on how\nthis is done is provided in [31] for linear tube MPC. The\nmain diﬀerence between that approach and the one used\nin this paper is that in that case the constraint takes the\nform h(v, θ) ≤0, while in this paper it takes the form\nh(v⋆(θ), θ) ≤0. Since the main idea is unchanged, we\ndo not provide further details for the sake of brevity.\nWe prove next that constraint (38) is non-blocking, i.e.,\nthat the parameter update yielded by (38) cannot be\nθi+1 = θi at all times, unless θi = θ⋆. Note that Theo-\nrem 3 does not require Assumption 1 nor Assumption 2.\nHowever, both assumptions are needed in order to be\nable to prove the non-blocking property.\nTheorem 4 Consider the closed loop trajectory si,...,∞\nunder policy πθi starting at the physical sampling time\ni with the initial state si. Suppose that Assumptions 1-\n2 hold. Assume further that Assumption 1.1 holds also\nfor X 0\nθi+1(a), i.e., if the ﬁrst action is ﬁxed in the MPC\nscheme, the set of feasible initial conditions is compact\nand continuous in θ. Then, the probability that (38) yields\nθi+1 = θi ̸= θ⋆for all times is zero, i.e.,\nP [θp+1 = θp ̸= θ⋆, p = i, . . . , ∞] = 0.\n(40)\nPROOF. In order to prove the result, we will prove\nthat there does exist a parameter update θi+1 ̸= θi\nwhich does decrease the cost (38a) and satisfy (38c). To\nthat end, we consider θi+1(α) resulting from (26), and\nwe prove next that\nlim\nα→0 P\nh\nX 0\nθi+1(α) ̸⊇X1(sj, πθi(sj)), j = i, . . . , ∞\ni\n(41)\n≤lim\nα→0 P\nh\nsi,...,∞/∈X 0\nθi+1(α)(πθi(si)))\ni\n= 0,\nwhere we used (39) to obtain the inequality above. Be-\ncause H ≻0, any update θi+1(α) reducing (26a) must\nalso be a descent direction for (38a). Consequently, if ad-\nditionally θi+1(α) is feasible for (38c), then (38) cannot\nyield a 0 update.\nSince by using α = 0, (26) yields θi+1(0) = θi, we have\nsi,...,∞∈X 0\nθi+1(0)(πθi(si)) = X 0\nθi(πθi(si)) = X 0\nθi.\nWe use (39) to deﬁne the set:\n∆X 0\nπθi(si)(θi, θi+1)\n(42)\n=\nn\ns\n\f\f\f s ∈X 0\nθi, and s /∈X 0\nθi+1(πθi(si))\no\n,\ni.e., the set for which θi+1 violates the constraint (38c).\nNote that ∆X 0\nπθi(si)(θi, θi) = ∅. Consider a trajec-\ntory si,...,∞in closed-loop under policy πθi such that\nparameter θi+1(α) solves (26) but never satisﬁes con-\nstraint (38c). By deﬁnition such trajectory must evolve\nin set ∆X 0\nπθi(si)(θi, θi+1(α)). We observe that, by As-\nsumption 2, θi+1(α) is continuous in a neighborhood\nof α = 0, and by assumption we have that the set\n∆X 0\nπθi(si)(θi, θi+1) is continuous in θi+1. It follows that\nlim\nα→0 µ\n\u0010\n∆X 0\nπθi(si)(θi, θi+1(α))\n\u0011\n= 0.\n(43)\nWe can then conclude using Lemma 2:\nlim\nα→0 P\nh\nsi,...,∞/∈X 0\nθi+1(α)(πθi(si))\ni\n=\n(44)\nlim\nα→0 P\nh\nsi,...,∞∈∆X 0\nπθi(si)(θi, θi+1(α))\ni\n= 0.\n9\n□\n6\nStability of MPC with Parameter Updates\nIn the previous section, we investigated the recursive fea-\nsibility of performing RL-based parameter updates on\nthe MPC scheme. In this section we will discuss the sta-\nbility of a sequence of MPC schemes satisfying the recur-\nsive feasibility conditions discussed above. We will ﬁrst\ndiscuss the joint stability of the state s and parameters\nθ; and then relax our assumptions, treat the updates\nof θ as a perturbation acting on the system, and prove\nInput-to-State Stability (ISS).\nIn order to discuss joint state and parameter stability,\nwe will show that, assuming that the sequence of pa-\nrameters converges linearly, the sequence of MPC poli-\ncies stabilizes the system in the state-parameter space.\nIf θp ∈ΘL ∩ΘF ∩ΘD for all p, the sequence of parame-\nters θ0,...,∞yields a sequence of Lyapunov functions ˆVθp\non their respective feasible sets X 0\nθp. Hence each MPC\nwith parameter θp is stabilizing the system trajectory to\nthe corresponding level set Lθp. The stability of the sys-\ntem trajectories when updating the parameters θp can\nthen be investigated by piecing together the individual\nLyapunov functions ˆVθ0,...,∞, and by assuming some reg-\nularity condition on the functions ˆVθ0,...,∞as well as a\nsuﬃciently fast convergence of the parameter sequence\nθ0,...,∞. This statement is formalized in the next theo-\nrem, for which we need the two following assumptions.\nAssumption 3 At every parameter update θp →θp+1,\nthe inequality:\n∥θp+1 −θ⋆∥≤r ∥θp −θ⋆∥\n(45)\nholds for some r ∈]0, 1[, where θ⋆is the solution of (25).\nAssumption 4 The condition\nsup\ns∈X 0\nθp∩X 0\nθp+1\n\f\f\f ˆVθp+1 (s) −ˆVθp (s)\n\f\f\f ≤αV ∥θp+1 −θp∥\n(46)\nholds for all p for some αV > 0.\nBefore providing the result, let us discuss these two as-\nsumptions. Assumption 3 essentially requires that the\nparameter updates converge at a linear rate. This type\nof convergence holds for exact gradient methods, i.e., if\nthe policy gradient ∇θJ(πθ) used to compute the pa-\nrameter updates in, e.g., (26) is evaluated exactly. We\nought to stress here that in a learning context, the gra-\ndient is typically evaluated from data and therefore is\nstochastic by nature. The amount of stochastic pertur-\nbation around the average value asymptotically decays\nas the amount of data used in the gradient evaluation\nincreases. As a result, Assumption 3 is to be taken as\nasymptotically valid, as well as the subsequent result\npresented in Theorem 5. A stability result requiring an\nassumption weaker than Assumption 3 is presented later\nin the text. That latter result is, however, weaker.\nIn order to discuss the conservatism of the regularity\nAssumption 4, we observe ﬁrst that in general both the\npolicy and the value function can be discontinuous [25].\nIndeed, the continuity of the value function of optimiza-\ntion problems is known to be intricate, with the notable\nexception of linear robust MPC with a quadratic cost\nand polyhedral uncertainty. Assuming that the param-\neter sequence θ0,...,∞belongs entirely to a bounded and\nconnected (BC) set Θ such that X 0\nθ is non-empty ev-\nerywhere in Θ, we observe that (46) holds if ˆVθ is Lips-\nchitz continuous on Θ with a bounded Lipschitz constant\nfor all s in the applicable domain of deﬁnition. This, in\nturn, holds if for all s in the domain of deﬁnition, ˆVθ\nis almost everywhere diﬀerentiable with respect to θ on\nΘ, with bounded and Lebesgue integrable derivatives.\nSince in the general case discontinuities cannot be ex-\ncluded, one might be tempted to conclude that (46) is\nin general violated. However, one should consider that,\nunder very mild regularity assumptions, such disconti-\nnuities can only occur on a set of zero measure, i.e.,\ncondition (46) holds for almost all s. Indeed, whenever\nthe optimal MPC solution satisﬁes the strong second or-\nder suﬃcient conditions for optimality, a suitable con-\nstraint qualiﬁcation, and strict complementarity holds,\nthen both the policy and the value function are contin-\nuous and diﬀerentiable with respect to the parameter\nθ [22,7]. In case strict complementarity does not hold,\nthen directional derivatives do exist and continuity is\nnot lost, such that (46) holds. A typical situation in\nwhich the policy and eventually also the value function\ncan become discontinuous is when one of the two other\nconditions is not met. We observe that essentially all\nNMPC solvers require these two conditions to hold, such\nthat one can conclude that any state-parameter com-\nbination which does not pose issues for NMPC solvers\nsatisﬁes (46). Since the set of problematic points is of\nzero measure for well-posed problems, the probability\nof visiting such a state-parameter pair is zero for non-\npathological systems (1).\nTheorem 5 Suppose that Assumptions 3-4 hold. Let us\nadditionally assume that each parameter is given by (26)\nor (38) such that θp ∈ΘL ∩ΘF ∩ΘD, and the pa-\nrameter updates satisfy the parameter update conditions\n(27) or (38c). Then the sequence of robust MPC schemes\nwith parameters θ0,...,∞is asymptotically stable in the\njoint state-parameter update space for any s0 ∈X0\nθ0, and\nsteers the system trajectory to the level set Lθ∞.\nPROOF. Consider an augmentation of the state s with\n10\nthe current parameters θp. Let us label the augmented\nstate S. We then propose the candidate Lyapunov func-\ntion:\nW(S) = ˆVθp (s) + ζ∆p\n(47)\nwhere we label ∆p := ∥θp −θ⋆∥, and ζ is a positive con-\nstant. Function (47) tackles the regular state space of\nthe system jointly with the parameter update space, and\nwill allow us to establish stability in that joint space for\nζ large enough. We ﬁrst observe that since ˆVθ0,...,∞(s)\nare Lyapunov functions, for any θp, W(S) is a Lyapunov\nfunction in between parameter updates, i.e., W is de-\ncreasing along the system trajectory. Moreover, since ∆p\nis a norm in the state-parameter space, W (S) is ade-\nquately lower and upper bounded if ˆVθp is. Finally, since\nby assumption ∆p →0, the system state s is eventu-\nally steered towards Lθ∞. We observe that between pa-\nrameter updates the decrease of W(S) under a speciﬁc\nparameter θp holds from:\nW(Si+1) = ˆVθp (si+1) + ζ∆p\n(48)\n≤γ ˆVθp (si) + δθp + ζ∆p < W(Si),\n(49)\nfor any si outside of Lθp.\nUpon updating the parameter θp →θp+1 at a speciﬁc\ntime instant i, we observe that:\nW(Si+1) −W(Si) = ˆVθp+1 (si+1) −ˆVθp (si)\n(50)\n+ ζ (∆p+1 −∆p) .\nIf the state at time si lies outside of the level set Lθp+1\nsuch that:\nˆVθp+1 (si+1) < ˆVθp+1 (si)\n(51)\nholds under the MPC with parameter θp+1, then W is\ndecreasing over the parameter updates at time i if:\nˆVθp+1 (si+1) −ˆVθp (si) + ζ (∆p+1 −∆p) <\nˆVθp+1 (si) −ˆVθp (si) + ζ (∆p+1 −∆p) ≤0.\n(52)\nUsing (46), condition (52) holds if:\nˆVθp+1 (si) −ˆVθp (si) + ζ (∆p+1 −∆p) ≤\nαV ∥θp+1 −θp∥+ ζ (∥θp+1 −θ⋆∥−∥θp −θ⋆∥) ≤0.\n(53)\nUsing (45) we observe that\n∥θp+1 −θp∥≤∥θp+1 −θ⋆∥+ ∥θp −θ⋆∥\n(54)\n≤(r + 1) ∥θp −θ⋆∥.\n(55)\nIt follows that (53) holds if\nαV ∥θp+1 −θp∥+ ζ (∥θp+1 −θ⋆∥−∥θp −θ⋆∥) ≤\n≤αV (r + 1) ∥θp −θ⋆∥+ ζ (r −1) ∥θp −θ⋆∥≤0.\n(56)\nHence W decreases when si lies outside of the level set\nLθp+1 if:\nαV (r + 1) + ζ (r −1) ≤0,\n(57)\nwhich can always be ensured by choosing:\nζ ≥αV (r + 1)\n1 −r\n.\n(58)\nAs a result at all time k whether a parameter update\ntakes place or not, either function W is decreasing or\nthe system trajectory is contained in the level set Lθp\ncorresponding to the MPC parameters in use.\n□\nWe elaborate in the next remarks on the assumptions\nand the stability claim made by Theorem 5.\nRemark 3 The decrease of function W at a time instant\ni with corresponding parameter index p is ensured for any\nsi outside the level sets Lθp, regardless of whether a pa-\nrameter update has occurred or not. However, if si ∈Lθp,\nthen W is not guaranteed to decrease, but the trajectory\nsi,i+1,... is guaranteed to remain in the level set Lθp until\nthe next parameter update occurs.\nTheorem 5 guarantees the stabilization of the system tra-\njectory in the state-parameter space, and under the Lya-\npunov function W, to the sequence of level sets Lθp con-\nverging to Lθ∞. Theorem 5 hence guarantees the stabil-\nity of the system trajectory under fast parameter updates\n(e.g., at every sampling time i), albeit the parameter up-\ndates entering in the Lyapunov function via the norm ∆p\ncan temporarily drive the system trajectory away from\nthe level sets (due to the fact that stability is guaranteed\nin the state-parameter update space, as opposed to the\nstate space alone).\nHence a case covered by Theorem 5 and expected in prac-\ntice is one where the parameter updates are fairly slow and\nsmall compared to the system dynamics, possibly yield-\ning a situation where the system trajectory is stabilized\nto Lθ∞mostly by moving from level set to level set, i.e.,\nLθp →Lθp+1 →. . . , without W systematically decreas-\ning. Theorem 5, however, guarantees that updating the\nparameters faster and more aggressively does not jeopar-\ndize the system stability.\nRemark 4 Let us further comment on Assumption 3,\ni.e., convergence of the parameters sequence θ0,...,∞de-\nlivered by the RL scheme. As previously discussed, many\n11\nRL methods deliver a sequence of parameters that is\nstochastic by nature, because they are based on measure-\nments taken from a stochastic system. We then observe\nthat Equation (45) is only satisﬁed asymptotically for\nlarge data sets. For RL methods based on very small\ndata sets, such as, e.g., to the extreme those using basic\nstochastic gradient methods, one could consider an ex-\ntension of Theorem 5 where the decrease of W holds only\nin a stochastic sense, or as practical stability or ISS. To\nthat end, one can, e.g., assume\n∥θp+1 −θ⋆∥≤r ∥θp −θ⋆∥+ q,\n(59)\nwhich implies that the parameter updates converge to a\nneighborhood of zero. The main conclusions of the Theo-\nrem still hold, mutatis mutandis, with asymptotic stabil-\nity replaced by practical stability. A formal discussion of\nthis extension is not provided here for the sake of brevity,\nand we rather discuss next an alternative relaxation of\nAssumptions 3-4.\nWhile the result of Theorem 5 is strong, it also requires\nsome assumptions that do not necessarily hold, and is\ntypically only asymptotically valid. We provide next an\nalternative result which does not require Assumption 3\nand only requires a weaker version of Assumption 4, but\nalso yields weaker conclusions.\nAssumption 5 It holds that\nˆVθp+1 (s) −ˆVθp (s) ≤β(∆p),\n(60)\nfor almost all s with β a K function.\nBefore stating the theorem, we observe that Assump-\ntion 5 can be seen as a slight relaxation of Assumption 4.\nTheorem 6 Suppose that Assumption 5 holds and as-\nsume that each parameter is given by (26) or (38) such\nthat θp ∈ΘL ∩ΘF ∩ΘD. Then the closed-loop system\nis ISS.\nPROOF. By assumption, we have that each ˆVθp is up-\nper and lower bounded by K∞functions. Moreover,\nˆVθp (s+) −ˆVθp (s) ≤−(1 −γ) ˆVθp (s) + δθp,\nwhich, using (60), entails that\nˆVθp+1 (s+) −ˆVθp (s) ≤−(1 −γ) ˆVθp (s) + δθp + β(∆p),\nwhich is the decrease condition for ISS Lyapunov func-\ntions [12]. Consequently, the closed-loop system is ISS\nwith respect to ∆and δθ.\n□\nThe result of Theorem 6 can be understood as follows:\nwhile parameter updates might perturb the closed-loop\nsystem and temporarily jeopardize asymptotic stabil-\nity, the destabilizing eﬀect is bounded and disappears as\nsoon as the parameters are not updated anymore. Conse-\nquently, as RL converges the possibly destabilizing per-\nturbations decrease in intensity and eventually vanish.\nThe original stability result of [25] is then recovered.\n7\nNumerical Examples\nIn this section we provide numerical examples which il-\nlustrate the theoretical developments.\n7.1\nRecursive Feasibility\nWe ﬁrst discuss a simple academic example which is con-\nstructed, but allows us to discuss the theory in simple\nterms. Consider the scalar linear system\ns+ = As + Ba + w,\nA = 1.1,\nB = 0.1,\nwith w ∈[w, w] := [−0.1, 0.1]. We construct MPC such\nthat it delivers a policy as close as possible to −Ks+as,\nwhere θ = {K, as} are parameters to be adjusted by RL\nand the state and input must satisfy\ns ≤s := 0.1,\na ∈[−10, 10 −0.5K].\nOne can verify that the robust MPC formulation\nmin\nu\n(u −(as −Ks))2\ns.t. As + Bu + w ≤s,\nu ∈[−10, 10 −0.5K],\nguarantees that the state constraint s ≤s is never vi-\nolated with the given dynamics and process noise. The\nstage cost ℓ(s, a) = (s −40)2 + 10−4a2 should be mini-\nmized by RL, and the MPC region of attraction at con-\nvergence must include the interval [s0\nb, s1\nb] = [0, 0.1].\nWe consider a discount factor γ = 0.9 and solve the\nproblem by applying constrained policy gradient to the\nexact total expected cost J. At each policy gradient it-\neration p we solve the problem\nθp+1 := arg min\nθ\n0.5∥θ −θp∥2\n2 + α∇θJ⊤(θ −θp)\ns.t. Asj\nb + Baj\nb + w ≤s,\nj = 0, 1,\naj\nb :=\nh\n−Ksj\nb + asi10−0.5K\n−10\n,\nj = 0, 1,\nA −BK ∈[−1 + ϵ, 1 −ϵ],\nwhere [·]b\na := max(a, min(·, b)), ϵ = 10−6, α = 1, and\nwe computed ∇θJ using the deterministic policy gradi-\nent theorem to compute the gradient in an actor-critic\nframework [26].\n12\n0\n100\n200\n300\n400\n500\n-10\n-5\n0\n0\n100\n200\n300\n400\n500\n0.07\n0.08\n0.09\n0.1\n0\n100\n200\n300\n400\n500\n0\n5\n10\nFig. 1. Closed-loop simulation with parameter updates rely-\ning on backtracking. Top plot: state trajectory. Middle plot:\nstate trajectory zoom. Bottom plot: control trajectory (blue\nline) and upper bound 10 −0.5K (dashed black line).\nWe initialize the problem with K = 2, as = 0. The\nproblem converges in one iterate to the optimal solu-\ntion K⋆= 11, as = 0.9, but, depending on the initial\nstate, the solution cannot be immediately applied to the\nsystem. Indeed, the region of attraction for the initial\nguess is the interval S0 := [−8.9, 0.1], while for the op-\ntimal solution the region of attraction is the interval\nS⋆:= [−4.4, 0.1]. We display in Figure 1 a simulation\nstarting from s = −8 and using a backtracking strategy\n(Algorithm 1) with n = 1, i.e., if the solution is not fea-\nsible, α is reduced with ρ = 0.9. One can observe that,\nin the beginning, parameter θ is not updated until: (a)\nα becomes smaller and, consequently, the region of at-\ntraction becomes larger; and (b) the state approaches\nthe region of attraction.\nWe also performed a simulation in which the update\nwas done by trajectory-constrained parameter updates,\nwhere the following problem was solved\nθi+1 := arg min\nθ\n0.5∥θ −θi∥2\n2 + α∇θJ⊤(θ −θi)\ns.t. Asj\nb + Baj\nb + w ≤s,\nj = 0, 1,\naj\nb :=\nh\n−Kisj\nb + as\ni\ni10−0.5K\n−10\n,\nj = 0, 1,\nA −BK ∈[−1 + ϵ, 1 −ϵ],\nsj\nwc ∈Sθ,\nj = 0, 1,\nwhere Sθ is the region of attraction given θ, and sj\nwc are\nthe one-step worst-case state realizations which, in this\nspeciﬁc case, are given by\ns0\nwc := Asi + B [−Kisi + as\ni]10−0.5Ki\n−10\n+ w,\ns1\nwc := Asi + B [−Kisi + as\ni]10−0.5Ki\n−10\n+ w.\nThe closed-loop trajectories are shown in Figure 2, where\none can see that the convergence is slower than with\n0\n100\n200\n300\n400\n500\n-10\n-5\n0\n0\n100\n200\n300\n400\n500\n0.07\n0.08\n0.09\n0.1\n0\n100\n200\n300\n400\n500\n0\n5\n10\nFig. 2. Closed-loop simulation with trajectory-constrained\nparameter updates. Top plot: state trajectory. Middle plot:\nstate trajectory zoom. Bottom plot: control trajectory (blue\nline) and upper bound 10 −0.5K (dashed black line).\nbacktracking, since the parameters are updated at each\ntime, which reduces the maximum implementable con-\ntrol and, therefore, makes the convergence to the opti-\nmal operating set slower. Nevertheless, also in this case\nwe recover the optimal solution K∞= K⋆, as\n∞= as\n⋆.\n7.2\nValue Function\nWe consider now the linear system with dynamics and\nstage cost\ns+ =\n\"\n1 0.1\n0 1\n#\ns +\n\"\n0.05\n0.1\n#\na + w,\nℓ(s, a) =\n\"\ns −sr\na −ar\n#⊤\ndiag\n\n\n\n\n\n\n1\n0.01\n0.01\n\n\n\n\n\n\n\"\ns −sr\na −ar\n#\n,\nwhere s = (p, v) and sr = (−3, 0), ar = 0. We formulate\na problem with prediction horizon N = 50 and introduce\nthe state and control constraints −1 ≤s ≤1, −10 ≤\na ≤10. The real noise set is selected as a regular octagon,\nand we parametrize Wω as a polytope with 4 facets.\n13\nWe formulate tube based MPC as\nQθ(s, a) :=\nmin\nz\nN−1\nX\nk=0\n\r\r\r\r\r\nxk −xr\nuk −ur\n\r\r\r\r\r\n2\nH\n+\n\r\r\rxN −xr\n\r\r\r\n2\nP\n+\n\r\r\rx0\n\r\r\r\n2\nΛ + λ⊤x0 + l\n(61a)\ns.t. x0 = s,\nu0 = a,\n(61b)\nxk+1 = Axk + Buk + b,\nk ∈IN−1\n0\n,\n(61c)\nCxk + Duk + ck ≤0,\nk ∈IN−1\n0\n,\n(61d)\nGxN + g ≤0,\n(61e)\nwhere one must enforce that the system dynamics (61c)\nand a parametrized compact uncertainty set Wω are\nsuch that s+ −(As+Ba+b) ∈Wω. This issue has been\ndiscussed in detail in [31], where the set is parametrized\nas the polyhedron Wω := { w | Mw ≤m } and the\nfollowing set membership constraint is imposed on ω =\n(M, m) for all past samples si+1, si, ai, i ∈I:\nM(si+1 −(Asi + Bai + b)) ≤m,\n∀i ∈I.\nThen, ck is computed by tightening the original con-\nstraints Cs + Da + ˆc ≤0, so as to guarantee that, for\nany process noise w ∈Wω, the constraints are satisﬁed.\nMoreover, parameters xr, ur must be a steady-state for\nthe system dynamics (61c), i.e.,\n(A −I)xr + Bur = 0.\nFinally, G and g must be selected such that they deﬁne a\nrobust positively invariant terminal set for the feedback\nlaw u = −K(x −xr) + ur, with K the solution to the\nLQR formulated with A, B, H, P. The vector of MPC\nparameters is then deﬁned as\nθ = {Λ, λ, l, H, xr, ur, M},\n(62)\nand we consider K, P, ck, G, g as functions of these\nparameters. Vector m can also be included in θ, but,\nas discussed in [31] this is not necessary. Matrices C, D\nand vector ¯c are assumed to be known. Finally, A, B,\nb could in principle also be included in the parameter\nvector θ. However, as discussed in [31] this makes the\nsafe RL problem much harder to formulate and solve,\nsince it obliges one to store very large amounts of data\nand formulate an equally large amount of constraints.\nThe set of parameters guaranteeing safety and stability\nthen becomes\nΘ := { θ | H ≻0,\nM(si+1 −(Asi + Bai + b)) ≤m, ∀i ∈I,\n(A −I)xr + Bur = 0,\n∃x s.t. Gx ≤g },\ni.e., the noise set must include all observed noise samples,\nthe reference must be a steady-state of the system and\nthe terminal set must be nonempty. This last condition\nalso entails that the MPC domain is nonempty.\nWe update θ using a batch Q learning approach with\nbatches of horizon Nb = 20 with learning rate α = 0.1,\nusing the backtracking strategy.\nWe simulated the system starting from state s0 =\n(0.8, 0). The backtracking strategy never rejected nor\nreduced any step. The resulting closed-loop trajectory\nis displayed in Figure 3, together with the reference,\nmaximum robust positive invariant (MRPI) and termi-\nnal sets at the beginning and end of the simulation, as\nwell as the minimum robust positive invariant (mRPI)\nsets throughtout the simulation. We display the noise\nset approximation at the end of the simulation in Fig-\nure 4, and the evolution throughout the RL epochs of\nthe parameter θ and the average TD error in each batch\nin Figure 5. We display the MPC Lyapunov functions\nˆVθ and W in time in Figure 6. One can see that in the\nbeginning ˆVθ sometimes increases upon parameter up-\ndates, but decreases inside each batch. Note that this\nresult is perfectly in line with Theorem 5 and Remark 3.\nAfter the displayed time interval, the Lyapunov func-\ntion ˆVθ was always 0, i.e., the state trajectory remained\ninside the mRPI set, even when this set was updated\nby a parameter change. Some words are due in order\nto discuss function W: as pointed out in Remark 4, in\npractice one can at best expect that the parameters\nconverge to a neighborhood of the optimal ones. There-\nfore, we selected θ⋆as the average of θ over the last 100\nepochs, when, as shown in Figure 5, the parameter is at\nconvergence. We observed that ζ = 0.1 was suﬃciently\nhigh to satisfy the conditions of Theorem 5. As one can\nsee in Figure 6, diﬀerently from ˆVθ, the obtained W is\ndecreasing also in the ﬁrst epochs. Finally, we show the\nperformance J in terms of total discounted cost over\neach batch in Figure 7. One can see that after a short\ntransient the performance reaches convergence and does\nnot improve anymore.\n8\nConclusions\nThis paper discusses how to implement Learning-based\nadaptations of a robust MPC scheme in order to im-\nprove its closed-loop performance while maintaining the\nstability and safety of the control policy. We show in\nparticular that these requirements can be treated via\nconstraints on the learning steps, and parameter update\nconditions that are fairly simple to verify, and that can\nbe implemented online in real time. We additionally es-\ntablish that the proposed approach ensures that the up-\ndate conditions are not blocking the learning process, in\nthe sense that they are met in ﬁnal time with probabil-\nity one. We ﬁnally show that under some conditions on\n14\nFig. 3. MRPI (red), terminal (cyan) sets and reference xr\n(black and grey circle) at the beginning and end of the learn-\ning process; state trajectoyr (black line) and mRPI sets (yel-\nlow) at each time instant.\nFig. 4. True process noise set (transparent octogon), noise\nsamples (black dots), their convex hull (red dots) and noise\nset parametrized by matrix M (cyan).\nthe learning process, a form of stability of the resulting\nlearning-based robust MPC scheme is guaranteed in the\nstate-parameter space. The proposed approaches are il-\nlustrated in two simulated examples.\nReferences\n[1]\nS. Abdufattokhov, M. Zanon, and A. Bemporad. Learning\nConvex Terminal Costs for Complexity Reduction in MPC.\nIn 2021 60th IEEE Conference on Decision and Control\n(CDC), pages 2163–2168. 2021.\n[2]\nB. Amos, I. D. J. Rodriguez, J. Sacks, B. Boots, and J. Z.\nKolter.\nDiﬀerentiable mpc for end-to-end planning and\ncontrol. In Proceedings of NIPS, NIPS’18, pages 8299–8310,\nUSA, 2018. Curran Associates Inc.\n0\n200\n400\n600\n800\n1000\n-50\n0\n50\n0\n200\n400\n600\n800\n1000\n10-2\n100\nFig. 5. Top plot: parameter evolution through the epochs.\nBottom plot: TD error through the epochs.\n0\n50\n100\n150\n200\n0\n2\n4\n6\n8\n10-3\n0\n50\n100\n150\n200\n3.5\n4\n4.5\n5\n5.5\nFig. 6. Top ﬁgure: Lyapunov function ˆVθ over the ﬁrst\nepochs. Bottom plot: Lyapunov function W over time.\n0\n5\n10\n15\n20\n40\n60\n80\n100\nFig. 7. Performance J in terms of total discounted cost for\neach batch. The dashed lines indicate the maximum and\nminimum of J over all future times.\n[3]\nA. Aswani, H.o Gonzalez, S. S. Sastry, and C. Tomlin.\nProvably safe and robust learning-based model predictive\ncontrol. Automatica, 49(5):1216 – 1226, 2013.\n[4]\nF. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause.\nSafe Model-based Reinforcement Learning with Stability\nGuarantees.\nIn I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\n15\neditors, Advances in Neural Information Processing Systems\n30, pages 908–918. Curran Associates, Inc., 2017.\n[5]\nD.P. Bertsekas and I.B. Rhodes. Recursive state estimation\nfor a set-membership description of uncertainty.\nIEEE\nTransactions on Automatic Control, 16:117–128, 1971.\n[6]\nL. Chisci, J.A. Rossiter, and G. Zappa.\nSystems with\npersistent disturbances: predictive control with restricted\nconstraints. Automatica, 37:1019–1028, 2001.\n[7]\nA.V. Fiacco. Introduction to sensitivity and stability analysis\nin nonlinear programming. Academic Press, New York, 1983.\n[8]\nChris Gaskett. Reinforcement learning under circumstances\nbeyond\nits\ncontrol.\nIn\nInternational\nconference\non\ncomputational\nintelligence,\nrobotics\nand\nautonomous\nsystems, 2003.\n[9]\nS. Gros and M. Zanon. Data-Driven Economic NMPC Using\nReinforcement Learning. IEEE Transactions on Automatic\nControl, 65(2):636–648, Feb 2020.\n[10] Matthias Heger.\nConsideration of Risk in Reinforcement\nLearning. In Machine Learning Proceedings 1994, pages 105–\n111. Elsevier, 1994.\n[11] Lukas Hewing, Kim P. Wabersich, Marcel Menner, and\nMelanie N. Zeilinger.\nLearning-Based Model Predictive\nControl: Toward Safe Learning in Control. Annual Review of\nControl, Robotics, and Autonomous Systems, 3(1):269–296,\n2020.\n[12] Zhong-Ping Jiang and Yuan Wang. Input-to-state stability\nfor discrete-time nonlinear systems. Automatica, 37(6):857–\n869, 2001.\n[13] Johannes K¨ohler, Peter K¨otting, Raﬀaele Soloperto, Frank\nAllg¨ower, and Matthias A. M¨uller.\nA Robust Adaptive\nModel Predictive Control Framework for Nonlinear Uncertain\nSystems.\nInternational Journal of Robust and Nonlinear\nControl, 31(18):8725–8749, 2021.\n[14] T.\nKoller,\nF.\nBerkenkamp,\nM.\nTurchetta,\nand A. Krause. Learning-based Model Predictive Control for\nSafe Exploration and Reinforcement Learning. Published on\nArxiv, 2018.\n[15] F. L. Lewis and D. Vrabie.\nReinforcement learning and\nadaptive dynamic programming for feedback control. IEEE\nCircuits and Systems Magazine, 9(3):32–50, 2009.\n[16] F.\nL.\nLewis,\nD.\nVrabie,\nand\nK.\nG.\nVamvoudakis.\nReinforcement learning and feedback control: Using natural\ndecision methods to design optimal adaptive controllers.\nIEEE Control Systems, 32(6):76–105, 2012.\n[17] D. Limon, I. Alvarado, T. Alamo, and E.F. Camacho. MPC\nfor Tracking Piecewise Constant References for Constrained\nLinear Systems. Automatica, 44(9):2382–2387, 2008.\n[18] D. Masti, M. Zanon, and A. Bemporad.\nTuning LQR\nControllers: a Sensitivity-Based Approach.\nIEEE Control\nSystems Letters, 6:932–937, 2022. Also in 60th IEEE Conf.\non Decision and Control, Austin, TX, December 13-15, 2021.\n[19] D.\nQ.\nMayne,\nE.\nC.\nKerrigan,\nE.\nJ.\nvan\nWyk,\nand\nP. Falugi.\nTube-based robust nonlinear model predictive\ncontrol.\nInternational Journal of Robust and Nonlinear\nControl, 21(11):1341–1353, 2011.\n[20] D.Q. Mayne, M.M. Seron, and S.V. Rakovic. Robust model\npredictive control of constrained linear systems with bounded\ndisturbances. Automatica, 41:219–224, 2005.\n[21] R. Murray and M. Palladino. A model for system uncertainty\nin reinforcement learning. Systems & Control Letters, 122:24\n– 31, 2018.\n[22] J. Nocedal and S.J. Wright.\nNumerical Optimization.\nSpringer\nSeries\nin\nOperations\nResearch\nand\nFinancial\nEngineering. Springer, 2 edition, 2006.\n[23] C. J. Ostafew, A. P. Schoellig, and T. D. Barfoot. Robust\nConstrained Learning-based NMPC enabling reliable mobile\nrobot path tracking. The International Journal of Robotics\nResearch, 35(13):1547–1563, 2016.\n[24] Iason Papaioannou, Wolfgang Betz, Kilian Zwirglmaier, and\nDaniel Straub.\nMCMC algorithms for Subset Simulation.\nProbabilistic Engineering Mechanics, 41:89–103, 2015.\n[25] J. B. Rawlings, D. Q. Mayne, and M. Diehl.\nModel\nPredictive Control: Theory, Computation, and Design. Nob\nHill Publishing, 2nd edition, 2017.\n[26] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and\nM. Riedmiller. Deterministic policy gradient algorithms. In\nProceedings of ICML, ICML’14, pages I–387–I–395, 2014.\n[27] R. Soloperto, M. M¨uller, and F. Allg¨ower.\nGuaranteed\nClosed-Loop Learning in Model Predictive Control.\n[28] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy\ngradient methods for reinforcement learning with function\napproximation.\nIn Proceedings of NIPS, pages 1057–1063,\nCambridge, MA, USA, 1999. MIT Press.\n[29] Mario E. Villanueva, Rien Quirynen, Moritz Diehl, Benoˆıt\nChachuat, and Boris Houska.\nRobust MPC via Min-Max\nDiﬀerential Inequalities. Automatica, 77:311–321, 2017.\n[30] K. Wabersich, L. Hewing, A. Carron, and M. Zeilinger.\nProbabilistic\nmodel\npredictive\nsafety\ncertiﬁcation\nfor\nlearning-based control.\narXiv:1906.10417v1, 25 Jun 2019,\n2019.\n[31] M.\nZanon\nand\nGros.\nSafe\nReinforcement\nLearning\nUsing Robust MPC.\nTransaction on Automatic Control,\n66(8):3638–3652, 2021.\n[32] M. Zanon and S. Gros.\nOn the Similarity Between Two\nPopular Tube MPC Formulations.\nIn Proceedings of the\nEuropean Control Conference, pages 651–656, 2021.\n[33] M.\nZanon,\nS.\nGros,\nand\nA.\nBemporad.\nPractical\nReinforcement Learning of Stabilizing Economic MPC. In\nProceedings of the European Control Conference, pages 2258–\n2263, 2019.\n[34] Mengjia Zhu, Dario Piga, and Alberto Bemporad. C-GLISp:\nPreference-Based\nGlobal\nOptimization\nUnder\nUnknown\nConstraints With Applications to Controller Calibration.\nIEEE Transactions on Control Systems Technology, 2022.\n(in press).\n[35] Konstantin M. Zuev.\nSubset Simulation Method for Rare\nEvent Estimation: An Introduction, pages 1–25.\nSpringer\nBerlin Heidelberg, Berlin, Heidelberg, 2021.\n16\n",
  "categories": [
    "cs.LG",
    "cs.SY",
    "eess.SY",
    "math.OC"
  ],
  "published": "2020-12-14",
  "updated": "2022-07-22"
}