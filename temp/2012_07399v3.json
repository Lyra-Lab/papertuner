{
  "id": "http://arxiv.org/abs/2012.07399v3",
  "title": "Robust Unsupervised Learning via L-Statistic Minimization",
  "authors": [
    "Andreas Maurer",
    "Daniela A. Parletta",
    "Andrea Paudice",
    "Massimiliano Pontil"
  ],
  "abstract": "Designing learning algorithms that are resistant to perturbations of the\nunderlying data distribution is a problem of wide practical and theoretical\nimportance. We present a general approach to this problem focusing on\nunsupervised learning. The key assumption is that the perturbing distribution\nis characterized by larger losses relative to a given class of admissible\nmodels. This is exploited by a general descent algorithm which minimizes an\n$L$-statistic criterion over the model class, weighting small losses more. Our\nanalysis characterizes the robustness of the method in terms of bounds on the\nreconstruction error relative to the underlying unperturbed distribution. As a\nbyproduct, we prove uniform convergence bounds with respect to the proposed\ncriterion for several popular models in unsupervised learning, a result which\nmay be of independent interest.Numerical experiments with kmeans clustering and\nprincipal subspace analysis demonstrate the effectiveness of our approach.",
  "text": "ROBUST UNSUPERVISED LEARNING VIA L-STATISTIC\nMINIMIZATION\nAndreas Maurer\nIstituto Italiano di Tecnologia\nam@andreas-maurer.eu\nDaniela A. Parletta\nIstituto Italiano di Tecnologia &\nUniversity of Genoa\ndaniela.parletta@iit.it\nAndrea Paudice\nIstituto Italiano di Tecnologia &\nUniversity of Milan\nandrea.paudice@iit.it\nMassimiliano Pontil\nIstituto Italiano di Tecnologia &\nUniversity College of London\nmassimiliano.pontil@iit.it\nFebruary 22, 2021\nABSTRACT\nDesigning learning algorithms that are resistant to perturbations of the underlying data distribution\nis a problem of wide practical and theoretical importance. We present a general approach to this\nproblem focusing on unsupervised learning. The key assumption is that the perturbing distribution is\ncharacterized by larger losses relative to a given class of admissible models. This is exploited by a\ngeneral descent algorithm which minimizes an L-statistic criterion over the model class, weighting\nsmall losses more. Our analysis characterizes the robustness of the method in terms of bounds on the\nreconstruction error relative to the underlying unperturbed distribution. As a byproduct, we prove\nuniform convergence bounds with respect to the proposed criterion for several popular models in\nunsupervised learning, a result which may be of independent interest.Numerical experiments with\nKMEANS clustering and principal subspace analysis demonstrate the effectiveness of our approach.\n1\nIntroduction\nMaking learning methods robust is a fundamental problem in machine learning and statistics. In this work we\nproposes an approach to unsupervised learning which is resistant to unstructured contaminations of the underlying data\ndistribution. As noted by Hampel [9], “outliers” are an ill-deﬁned concept, and an approach to robust learning, which\nrelies on rules for the rejection of outliers (see [20] and the references therein) prior to processing may be problematic,\nsince the hypothesis class of the learning process itself may determine which data is to be regarded as structured or\nunstructured. Instead of the elimination of outliers – quoting Hampel “data that don’t ﬁt the pattern set by the majority\nof the data” – in this paper we suggest to restrict attention to “a sufﬁcient portion of the data in good agreement with\none of the hypothesized models”.\nTo implement the above idea, we propose using L-estimators [21], which are formed by a weighted average of the\norder statistics. That is, given a candidate model, we ﬁrst rank its losses on the empirical data and than take a weighted\naverage which emphasizes small losses more. An important example of this construction is the average of a fraction of\nthe smallest losses. However, our observations apply to general classes of weight functions, which are only restricted to\nbe non-increasing and in some cases Lipschitz continuous.\nWe highlight that although L-statistics have a long tradition, a key novelty of this paper is to use them as objective\nfunctions based on which to search for a robust model. This approach is general in nature and can be applied to robustify\nany learning method, supervised or unsupervised, based on empirical risk minimization. In this paper we focus on\nunsupervised learning, and our analysis includes KMEANS clustering, principal subspace analysis and sparse coding,\namong others.\narXiv:2012.07399v3  [cs.LG]  18 Feb 2021\nA PREPRINT - FEBRUARY 22, 2021\nThis paper makes the following contributions:\n• A theoretical analysis of the robustness of the proposed method (Theorem 1). Under the assumption that\nthe data-distribution is a mixture of an unperturbed distribution adapted to our model class and a perturbing\ndistribution, we identify conditions under which we can bound the reconstruction error, when the minimizer of\nthe proposed objective trained from the perturbed distribution is tested on the unperturbed distribution.\n• An analysis of generalization (Theorems 4–6). We give dimension-free uniform bounds in terms of Rademacher\naverages as well as a dimension- and variance-dependent uniform bounds in terms of covering numbers which\ncan outperform the dimension-free bounds under favorable conditions.\n• A meta-algorithm operating on the empirical objective which can be used whenever there is a descent algorithm\nfor the underlying loss function (Theorem 9).\nThe paper is organized as follows. In Section 2 we give a brief overview of unsupervised (representation) learning.\nIn Sections 3 to 5 we present and analyze our method. In Section 6 we discuss an algorithm optimizing the proposed\nobjective and in Section 7 we present numerical experiments with this algorithm for KMEANS clustering and principal\nsubspace analysis, which indicate that the proposed method is promising. Proofs can be found in the supplementary\nmaterial.\nPrevious Work\nSome elements of our approach have a long tradition. For ﬁxed models the proposed empirical\nobjectives are called L-statistics or L-estimators. They have been used in robust statistics since the middle of the last\ncentury [14] and their asymptotic properties have been studied by many authors (see [21] and the references therein).\nAlthough inﬂuence functions play a certain role, our approach is somewhat different from the traditions of robust\nstatistics. Similar techniques to ours have been experimentally explored in the context of classiﬁcation [10] or latent\nvariable selection [11]. Finite sample bounds, uniform bounds, the minimization of L-statistics over model classes and\nthe so called risk based-objectives however are more recent developments [18, 19, 12], and we are not aware of any\nother general bounds on the reconstruction error of models trained from perturbed data. A very different line of work for\nrobust statistics are model-independent methods available in high dimensions [6, 7]. Although elegant and very general,\nthese depth-related pre-processing methods may perform sub-optimally in practice, as our numerical experiments\nindicate. Finally, we note that previous work on PAC learning (e.g. [2]) has addressed the problem of learning a good\nclassiﬁer with respect to a target, when the data comes from a perturbed distribution affected by unstructured noise.\nSimilarly to us, they consider that the target distribution is well adapted to the model class.\n2\nUnsupervised Learning\nLet S be a class of subsets of Rd, which we call the model class. For S ∈S deﬁne the distortion function dS : Rd →\n[0, ∞) by1\ndS (x) = min\ny∈S ∥x −y∥2 for x ∈Rd.\n(1)\nWe assume that the members of S are either compact sets or subspaces, so the minimum in (1) is always attained. For\ninstance S could be the class of singletons, a class of subsets of cardinality k, the class of subspaces of dimension k, or\na class of compact convex polytopes with k vertices2.\nWe write P (X) for the set of Borel probability measures on a locally compact Hausdorff space X. If µ ∈P\n\u0000Rd\u0001\n, deﬁne\nthe probability measure µS ∈P ([0, ∞)) as the push-forward of µ under dS, that is, µS (A) = µ ({x : dS (x) ∈A})\nfor A ⊆[0, ∞). Now consider the functional Φ : P ([0, ∞)) →[0, ∞) deﬁned by\nΦ (ρ) =\nZ ∞\n0\nrdρ (r) ,\nρ ∈P.\n(2)\nThen Φ (µS) = EX∼µ [dS (X)] is the expected reconstruction error, incurred when coding points by the nearest\nneighbors in S. The measures µS ∈P ([0, ∞)) and the functional Φ allow the compact and general description of\nseveral problems of unsupervised learning as\nmin\nS∈S Φ (µS) = min\nS∈S EX∼µ [dS (X)] .\n(3)\n1In most parts our analysis applies also to other distortion measures, for example omitting the square in (1). The chosen form is\nimportant for generalization bounds, when we want to bound the complexity of the class {x 7→dS (x) : S ∈S} for speciﬁc cases.\n2In these cases the set S is the image of a linear operator on a prescribed set of code vectors, see [17]. Our setting is more general,\ne.g. it includes non-linear manifolds.\n2\nA PREPRINT - FEBRUARY 22, 2021\nDenote with S∗= S∗(µ) a global minimizer of (3). Returning to the above examples, if S is the class of singleton sets,\nthen S∗(µ) is the mean of µ. If it is the class of subsets of cardinality k, then S∗(µ) is the optimal set of centers for\nKMEANS clustering. If S is the class of k-dimensional subspaces, then S∗(µ) is the principal k-dimensional subspace.\nAn important drawback of the above formulation is that the functional Φ is very sensitive to perturbing masses at\nlarge distortions R. In the tradition of robust statistics (see e.g. [8, 21]) this can be expressed in terms of the inﬂuence\nfunction, measuring the effect of an inﬁnitesimal point mass perturbation of the data. Let δR be the unit mass at R > 0,\nthen the inﬂuence function\nIF (R; ρ, Φ)\n:=\nd\ndtΦ\n\u0000(1 −t)ρ + tδR\n\u0001\f\f\f\f\nt=0\n= R −Φ (ρ) ,\ncan be arbitrarily large, indicating that even a single datapoint could already corrupt S∗(µ). To overcome this problem,\nin the next section we introduce a class of robust functional based on L-statistics.\n3\nProposed Method\nOur goal is to minimize the reconstruction error on unperturbed test data, from perturbed training data. Speciﬁcally, we\nassume that the data we observe comes from a perturbed distribution µ that is the mixture of an unperturbed distribution\nµ∗, which is locally concentrated on the minimizer S∗= S∗(µ∗), and a perturbing distribution ν which is unstructured\nin the sense that it does not concentrate on any of our models3. Figure 1 depicts such a situation, when S is the set of\nsingletons and d = 1.\nS∗\nFigure 1: Densities of the unperturbed distribution µ∗(light green) with high local concentration on the optimal\nmodel S∗, the perturbing distribution ν (light red) without signiﬁcant concentration, and the observable mixture\nµ = (1 −λ) µ∗+ λν∗(black) at λ = 0.6.\nWe wish to train from the available, perturbed data a model ˆS ∈S, which nearly minimizes the reconstruction error on\nthe unperturbed distribution µ∗. To this end we exploit the assumption that the unperturbed distribution µ∗is much\nmore strongly concentrated at S∗than the mixture µ = (1 −λ) µ∗+ λν is at models S away from S∗in terms of\nreconstruction error.\nThe key observation is that if the mixture parameter λ is not too large, the concentration of µ∗causes the cumulative\ndistribution function of the losses for the optimal model FµS∗: r 7→µS∗[0, r] to increase rapidly for small values of r,\nuntil it reaches the value ζ = FµS∗(r∗), where r∗is a critical distortion radius depending on S∗. Thus, when searching\nfor a model, we can consider as irrelevant the remaining mass 1 −ζ = µS∗(r∗, ∞), which can be attributed to ν and\nmay arise from outliers or other contaminating effects. To achieve this, we modify the functional (2) so as to consider\nonly the relevant portion of data, replacing Φ (µS) by\nζ−1\nZ F −1\nµS (ζ)\n0\nrdµS (r) .\n(4)\nIntuitively, the minimization of (4) forces the search towards models with the smallest truncated expected loss. Among\nsuch models there is also S∗, whose losses have the strongest concentration around a small value and then leading to a\nvery small value r∗for F −1\nµS∗(ζ).\nMore generally, since the choice of the hard quantile-thresholding at ζ is in many ways an ad hoc decision, we might\nwant a more gentle transition of the boundary between relevant and irrelevant data. Let W : [0, 1] →[0, ∞) be a\nbounded weight function and deﬁne, for every ρ ∈P [0, ∞),\nΦW (ρ) =\nZ ∞\n0\nrW (Fρ (r)) dρ (r) .\n3This is in contrast with the assumptions made in adversarial learning, where the goal is to increase robustness against adversarial\nworst-case perturbations (see e.g. [13]).\n3\nA PREPRINT - FEBRUARY 22, 2021\nWe require W to be non-increasing and zero on [ζ, 1] for some critical mass ζ < 1. The parameter ζ must be chosen on\nthe basis of an estimate of the amount λ of perturbing data. Note that if W is identically 1 then Φ1 = Φ in (2), while if\nW = ζ−11[0,ζ] then ΦW is the hard thresholding functional in (4).\nWe now propose to “robustify” unsupervised learning by replacing the original problem (3) by\nmin\nS∈S ΦW (µS) ,\n(5)\nand denote a global minimizer by S† ≡S† (µ).\nIn practice, µ is unknown and the search for the model S† has to rely on ﬁnite data. If ˆµ (X) = 1\nn\nPn\ni=1 δXi is the\nempirical measure induced by an i.i.d. sample X = (X1, ..., Xn) ∼µn, then the empirical objective is the plug-in\nestimate\nΦW (ˆµ (X)S) = 1\nn\nn\nX\ni=1\ndS (Xi) W\n\u0012 1\nn |{Xj : dS (Xj) ≤dS (Xi)}|\n\u0013\n= 1\nn\nn\nX\ni=1\ndS (X)(i) W\n\u0012 i\nn\n\u0013\n,\n(6)\nwhere dS (X)(i) is the i-th smallest member of {dS (X1) , ..., dS (Xn)}.\nThe empirical estimate ΦW (ˆµ (X)S) is an L-statistic [21]. We denote a minimizer of this objective by\nˆS (X) = arg min\nS∈S ΦW (ˆµ (X)S) .\n(7)\nIn the sequel we study three questions:\n1 If the underlying probability measure is a mixture µ = (1 −λ) µ∗+ λν of an unperturbed measure µ∗\nand a perturbing measure ν, and S† = S† (µ) is the minimizer of (5), under which assumptions will\nthe reconstruction error Φ\n\u0000µ∗\nS†\n\u0001\nincurred by S† on the unperturbed distribution approximate the minimal\nreconstruction error Φ (µ∗\nS∗)?\n2 When solving (5) for a ﬁnite amount of data X, under which conditions can we reproduce the behavior of S†\nby the empirical minimizer ˆS (X) in (7)?\n3 How can the method be implemented and how does it perform in practice?\n4\nResilience to Perturbations\nBefore we address the ﬁrst question we make a preliminary observation in the tradition of robust statistics and compare\nthe inﬂuence functions of the functional Φ1 to that one of the proposed ΦW with bounded W, and W (t) = 0 for\nζ ≤t < 1. While we saw in (4) that for any ρ ∈P ([0, ∞)) the inﬂuence function IF (R; ρ, Φ) = R −Φ (ρ) is\nunbounded in R, in the case of ΦW we have, for any R ∈Rd, that\nIF (R; ρ, ΦW )\n≤\nIFmax (ρ, W) :=\nZ F −1\nρ\n(ζ)\n0\nW (Fρ (r)) Fρ (r) dr.\nNotice that the right hand side is always bounded, which already indicates the improved robustness of ΦW [8]. The\nupper bound IFmax on the inﬂuence function plays also an important role in the subsequent analysis.\nReturning now to the data generating mixture µ = (1 −λ) µ∗+ λν, where µ∗∈P\n\u0000Rd\u0001\nis the the ideal, unperturbed\ndistribution and ν ∈P\n\u0000Rd\u0001\nthe perturbation, we make the following assumption.\nAssumption A. There exists S0 ∈S, δ > 0, β ∈(0, 1 −λ) and a scale parameter r∗> 0 (in units of squared euclidean\ndistance), such that for every model S ∈S satisfying Φ (µ∗\nS) > Φ\n\u0000µ∗\nS0\n\u0001\n+ δ we have FµS (r) < βFµ∗\nS0 (r) for all\nr ≤r∗.\nLoosely speaking this assumption prescribes that, under the perturbed distribution µ, any model S with a large\nreconstruction error on µ∗, should have its losses far less concentrated than the losses of S0 around a small value (any\nr ≤r∗). As an example, on a typical sample from µ any such S will have far more large losses than S0. For the sake of\nintuition, one should think of S0 as S∗(µ∗) and β as a very small number controlling the concentration of the losses.\nEquivalently, the assumption requires a perturbing distribution that is concentrated on no model S very different from\nS0 in terms of reconstruction error on the target. For concrete examples for the cases of K-MEANS clustering and\nprincipal subspace analysis are given in Figures 1 and 2.\nWe now state the main result of this section.\n4\nA PREPRINT - FEBRUARY 22, 2021\nTheorem 1. Let µ∗, ν ∈P\n\u0000Rd\u0001\n, µ = (1 −λ) µ∗+ λν, and λ ∈(0, 1) and suppose there are S0, r∗, δ > 0 and\nβ ∈(0, 1 −λ), satisfying Assumption A. Let W be nonzero on a set of positive Lebesgue measure, nonincreasing and\nW (t) = 0 for t ≥ζ = FµS0 (r∗). Then IFmax (µS0, W) > 0, and if any S ∈S satisﬁes\nΦW (µS) −ΦW (µS0) ≤\n\u0012\n1−β\n1−λ\n\u0013\nIFmax(µS0, W)\n(8)\nthen we have that Φ (µ∗\nS) ≤Φ\n\u0000µ∗\nS0\n\u0001\n+ δ. In particular we always have that Φ\n\u0000µ∗\nS†\n\u0001\n≤Φ\n\u0000µ∗\nS0\n\u0001\n+ δ.\nS∗\nS\nFigure 2: Illustration of Theorem 1 for d = 2 and k = 1 in the case of PSA. The target distribution (dark gray)\nis concentrated on the subspace S∗, while the perturbing distribution (light gray) does not concentrate well on any\nindividual subspace.\nWe close this section by stating some important conclusions of the above theorem.\n1. A simplifying illustration of Theorem 1 for principal subspace analysis is provided by Figure 2. The\ndistributions µ∗and ν are assumed to have uniform densities ρ (µ∗) and ρ (ν) supported on dark red and\nlight red areas of the unit disk respectively. Suppose β = ρ (ν) /ρ (µ∗) < 1 −λ, let r∗= sin2 (π/ρ (µ∗))\nand δ = 4r∗. If Φ (µ∗\nS) > Φ (µ∗\nS∗) + δ then the direction of the subspace S does not intersect the black\npart of the unit circle and therefore FµS (r) ≤βFµ∗\nS∗(r) for all r ≤r∗. Thus Assumption A is satisﬁed and\nconsequently, if W (t) = 0 for t ≥FµS∗(r∗), then S† must intersect the black part of the unit circle and\nΦ\n\u0000µ∗\nS†\n\u0001\n≤Φ (µ∗\nS∗) + δ.\n2. The generic application of this result assumes that S0 = S∗(µ∗), but this is not required. Suppose S is the\nset of singletons and µ∗is bimodal, say the mixture of distant standard normal distributions, and λ = 0 for\nsimplicity. Clearly there is no local concentration on the midpoint S∗(µ∗), but there is on each of the modes.\nIf S0 is the mean of the ﬁrst mode and ζ is sufﬁciently small, then S† can be near the mean of the other mode,\nbecause it has comparable reconstruction error. In this way the result also explains the astonishing behavior of\nour algorithm in clustering experiments with mis-speciﬁed number of clusters.\n3. The conditions on W prescribe an upper bound on the cutoff parameter ζ. If the cutoff parameter ζ is chosen\nsmaller (so that W (t) = 0 for t ≥ζ ≪FµS∗(r∗)), the required upper bound in (8) decreases and it becomes\nmore difﬁcult to ﬁnd S satisfying the upper bound. This problem becomes even worse in practice, because the\nbounds on the estimation error also increase with ζ, as we will see in the next section.\n5\nGeneralization Analysis\nUp to this point we were working with distributions and essentially inﬁnite data. In practice we only have samples\nX ∼µn and then it is important to understand to which extend we can obtain the conclusion of Theorem 1, when\nS is the minimizer of the empirical robust functional ΦW (ˆµ (X)S). This can be settled by a uniform bound on the\nestimation error for ΦW .\nProposition 2. Under the conditions of Theorem 1 with X ∼µn we have that\nPr\nn\nΦ\n\u0010\nµ∗\nˆS(X)\n\u0011\n≤Φ (µ∗\nS∗) + δ\no\n≥Pr\n\u001a\n2 sup\nS∈S\n|ΦW (µS) −ΦW (ˆµS (X))| ≤\n\u0012\n1 −\nβ\n1 −λ\n\u0013\nIFmax (µS∗, W)\n\u001b\n.\nThe left hand side is the probability that the minimization of our robust L-statistic objective returns a δ-optimal model\nfor the target distribution µ∗. The right hand side goes to 1 as n grows. As we show next, this is due to the fact that the\n5\nA PREPRINT - FEBRUARY 22, 2021\nclass {µS} enjoys a uniform convergence property with respect to the functional ΦW . Particularly, we present three\nuniform bounds that control the rate of decay of the same estimation error |ΦW (µS) −ΦW (ˆµS (X))|.\nThe ﬁrst two bounds are dimension-free and rely on Rademacher and Gaussian averages of the function class\n{x 7→d (x, S) : S ∈S}. Bounds for these complexity measures in the practical cases considered can be found\nin [17]. Our last bound is dimension dependent but may outperform the other two if the variance of the robust objective\nis small under its minimizer. All three bounds require special properties of the weight function W.\nFor this section we assume µ ∈P\n\u0000Rd\u0001\nto have compact support, write X =support(µ) and let F be the function class\nF = {x ∈X 7→d (x, S) : S ∈S} .\nWe also set Rmax = supf∈F ∥f∥∞.\nThe ﬁrst bound is tailored to the hard-threshold ζ−11[0,ζ]. It follows directly from the elegant recent results of [12]. For\nthe beneﬁt of the reader we give a proof in the appendix, without any claim of originality and only slightly improved\nconstants.\nTheorem 3. Let W = ζ−11[0,ζ] and η > 0. With probability at least 1 −η in X ∼µn we have that\nsup\nS∈S\n|ΦW (µS) −ΦW (ˆµS (X))| ≤2\nζnEXR (F, X) + Rmax\nζ√n\n \n2 +\nr\nln (2/η)\n2\n!\n,\nwhere R (F, X) is the Rademacher average\nR (F, X) = Eϵ\n\"\nsup\nS∈S\nn\nX\ni=1\nϵid (Xi, S)\n#\nwith independent Rademacher variables ϵ = (ϵ1, ..., ϵn).\nThe next bound requires boundedness and a Lipschitz property for the weight function W which can\notherwise be arbitrary.\nWe deﬁne the norm ∥W∥∞\n=\nsupt∈[0,1] |W (t)| and seminorm ∥W∥Lip\n=\ninf {L : ∀t, s ∈[0, 1] , W (t) −W (s) ≤L |t −s|} .\nTheorem 4. For any η > 0\nsup\nS∈S\n|ΦW (µS) −ΦW (ˆµS (X))| ≤\n2√π\n\u0010\nRmax ∥W∥∞+ ∥W∥Lip\n\u0011\nn\nEXG (F, X) + Rmax ∥W∥∞\nr\n2 ln (2/η)\nn\nwhere G (F, X) is the Gaussian average\nG (F, X) = Eγ\n\"\nsup\nS∈S\nn\nX\ni=1\nγid (Xi, S)\n#\n,\nwith independent standard normal variables γ1, ..., γn.\nOur last result also requires a Lipschitz property for W and uses a classical counting argument with covering numbers\nfor a variance-dependent bound.\nTheorem 5. Under the conditions of the previous theorem, with probability at least 1 −η in X ∼µn we have that for\nall S ∈S\n|ΦW (µS) −ΦW (ˆµS (X))| ≤\np\n2VSC +\n6Rmax\n\u0010\n∥W∥∞+ ∥W∥Lip\n\u0011\nC\nn\n+ ∥W∥∞Rmax\n√n\n,\nwhere VS is the variance of the random variable ΦW (ˆµS (X)), and C is the complexity term\nC = kd ln\n\u0010\n16n ∥S∥2 /η\n\u0011\nif S is the set of sets with k elements, or convex polytopes with k vertices and ∥S∥= supx∈S∈S ∥x∥, or\nC = kd ln\n\u000016nR2\nmax/η\n\u0001\nif S is the set of set of k-dimensional subspaces.\n6\nA PREPRINT - FEBRUARY 22, 2021\nWe state two important conclusion from the above theorems.\n1. Our bounds decrease at least as quickly as n−1/2 ln n. However, the bound in the last theorem may be\nconsiderably smaller than the previous two if n is large and the unperturbed distribution is very concentrated.\nThe last term, which is of order n−1/2 does not carry the burden of the complexity measure and decays quickly.\nThe second term contains the complexity, but it decreases as n−1. It can be shown from the Efron-Stein\ninequality (see e.g.[3] Theorem 3.1) that the variance VS of our L-statistic estimator is at most of order n−1,\nso the entire bound is at most of order n−1/2 ln n. On the other hand Vs can be very small. For example, if the\nunperturbed distribution is completely concentrated at S∗and ζ is chosen appropriately VS∗= 0 and, apart\nfrom the complexity-free last term the decay is as n−1 ln n.\n2. The above bounds implies that, by equating the estimation error to 1\n2\n\u00001−\nβ\n1−λ\n\u0001\nIFmax (µS∗, W) and solving\nfor η, our method recovers a δ-optimal (w.r.t. µ∗) model with probability at least equal to 1 −exp(−n).\nFinally, we highlight that the above uniform bounds may be of independent interest. For example, consider the case that\nthe test data also come from the perturbed distribution. In such a situation one might be interested in evaluating the\nperformance of the learned model only on data that ﬁt the model class, i.e. ΦW µS. These bounds guarantee that by\nminimizing the empirical robust functional, one also get good performances on future data from the same distribution.\n6\nAlgorithms\nIn this section we present our algorithm for (approximately) minimizing the robust L-statistic ΦW (ˆµ(X)S) w.r.t. model\nS ∈S. Throughout we assume W non-increasing and ﬁxed, and to simplify the notation we use the shorthand\nˆΦS(X) ≡ΦW (ˆµ(X)S).\n6.1\nGeneral Algorithm\nLet x = (x1, . . . , xn) be a realization of X ∼µn, consider the following function of S ∈S\nˆΦS(x) = 1\nn\nn\nX\ni=1\nW\n\u0012π(i)\nn\n\u0013\ndS(xi)\n(9)\nwhere π is the ascending ordering of the dS(x)(i) and notice that minimizing (9) is equivalent to minimize (6). Let p\nany ﬁxed element in Symn\n4 and let\nφS(x, p) = 1\nn\nn\nX\ni=1\nW\n\u0012p(i)\nn\n\u0013\ndS(xi).\nIn the following we will leverage the following property of φS.\nLemma 6. For any S ∈S and any p ∈Symn, if π is the ascending ordering of the dS(xi)s, then φS(x, p) ≥\nφS(x, π) = ˆΦS(x).\nWe need also the following deﬁnition.\nDeﬁnition 7. A mapping D : S × Sn →S is a Descent Oracle for φS iff for any S ∈S and any p ∈Symn,\nφD(S,p)(x, p) ≤φS(x, p).\nThe algorithm attempts to minimize (9) via alternating minimization of φS. At the beginning, it picks an initial model\nS0 and sort the induced losses in ascending order, i.e. pick the optimal permutation π0. Then it starts iterating this\ntwo steps by ﬁrst calling the descent oracle D(St, πt) and then sorting the induced losses. At each step either the\npermutation πt or the model St are ﬁxed. Pseudocode is given in Algorithm 1. Indeed, at each step the algorithm\nﬁrst ﬁnds a descending iteration St+1 of φSt(x, πt) and then sort the losses according to πt+1, an operation that by\nLemma 6 cannot increase the value of φSt+1. Thus the following holds.\nTheorem 8. Algorithm 1 is a descent algorithm for the problem of minimizing (9), i.e. for any t, ˆΦSt+1(x) ≤ˆΦSt(x).\nThis algorithm is general and to apply it to a speciﬁc learning problem an implementation of the descent oracle is\nneeded. The efﬁciency of Algorithm 1 depends upon such oracle. In the following we show two descent oracles for the\ncases of KMEANS and PSA. We complement these results with a computational lower bound showing that, in general,\nminimizing (9) is NP-Hard.\n4Here Symn denotes the set of all n! permutations over n objects.\n7\nA PREPRINT - FEBRUARY 22, 2021\nAlgorithm 1\n1: Pick any S0 ∈S\n2: π0 ←arg minp∈Symn φS0(x, p)\n3: for t = 1, . . . , T do\n4:\nSt ←D(St−1, πt−1)\n5:\nπt ←arg minp∈Sn φSt(x, p)\n6: end for\n7: return ST\n7\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nζ\n26\n28\n30\n32\n34\n36\nTest error\nRKM\nSD\nkmeans++\nFigure 3: Experiments for KMEANS on synthetic data and real data with k = 2 and k = 3.\nTheorem 9. Minimizing (9) for the case of KMEANS when k = 1 and W is the hard threshold is NP-Hard.\nNotice that in the case of KMEANS when W is the identity, the problem reduces to ﬁnding the optimal KMEANS solution,\na problem which is known to be hard. However, KMEANS admits a simple closed form solution when k = 1; in some\nsense minimizing the robust objective is even harder than standard KMEANS. The immediate consequence of this result\nis that approximate solutions to the problem of minimizing (9) are the best one can get; our algorithms, are a ﬁrst step\ntowards the design of methods with provable approximation guarantees.\nk-Means Clustering (KMEANS).\nIn this case S is the set of all possible k-tuples of centers in Rd and dS(x) =\nminc∈S ∥x −c∥2\n2. Keeping ﬁxed the permutation p, we consider as descent oracle the following Lloyd-like update for\nthe centers. Each center c ∈S induces a cluster formed by a subset of training points xi, i ∈I which are closer to c\nthan every other center (breaking ties arbitrarily). The overall loss of representing point in I with c is\nX\ni∈I\nW\n\u0012p(i)\nn\n\u0013\n∥xi −c∥2\n2.\nThis loss is minimized at\nˆc =\n1\nP\ni∈I W\n\u0010\np(i)\nn\n\u0011\nX\ni∈I\nW\n\u0012p(i)\nn\n\u0013\nxi,\nso the following holds.\nProposition 10. Given S and p, the mapping that for every c ∈S returns the ˆc deﬁned above is a descent oracle for\nKMEANS and its runtime is O(nkd).\nThe resulting algorithm can is a generalization of the method proposed in [4].\n8\nA PREPRINT - FEBRUARY 22, 2021\nPrincipal Subspace Analysis (PSA).\nIn this case S is the set of all possible d × k matrices U such that U ⊤U = Id,\ndS(x) = ∥x −UU ⊤x∥2\n2 and\nφU(x, p) =\nn\nX\ni=1\nW\n\u0012p(i)\nn\n\u0013\n∥x −UU ⊤xi∥2\n2.\nGiven p, it is easy to see that the above function is minimized at the matrix ˆU formed by stacking as columns the k\neigenvectors of Pn\ni W\n\u0010\np(i)\nn\n\u0011\nxix⊤\ni associated to the top k eigenvalues, so the following holds.\nProposition 11. Given U and p, the mapping that returns the ˆU deﬁned above is a descent oracle for PSA and its\nruntime is O(min{d3 + nd2, n3 + n2d}).\n7\nExperiments\nThe purpose of the numerical experiments is to show that:\n• Our algorithms for PSA and KMEANS outperform standard SVD, KMEANS++ and the Spherical Depth method\n(SD) in presence of outliers, while obtain similar performances on clean data.\n• Our algorithms on real data are not too sensitive to the parameters of the weight function. In particular, we\nshow that there exist a wide-range of ζ values such that using the hard-threshold function leads to good results.\n• In the case of KMEANS our method is able to accurately reconstruct some of the true centers even when the\nvalue of k is miss-speciﬁed. This matches the second remark after Theorem 1.\nImplemented Algorithms.\nFor KMEANS++ we used the sklearn implementation fed with the same parameters for\nthe maximum number of iterations T and the initializations r we used for our method. Notice that T is only an upper\nbound to the number of iterations, the algorithms stop when the difference between the current objective value and the\nprevious one is smaller than 10−7. To set r we used the largest value before diminishing returns were observed. For\nstandard PSA we compute the SVD of P\ni xix⊤\ni . The SD method is a general purpose pre-processing technique that is\napplied on the data before performing KMEANS and PSA (see e.g. [6, 7]). This method computes a score for each point\nin the dataset by counting in how many balls, whose antipodes are pairs of points in the data, it is contained. The 1 −ζn\npoints with the smallest scores are discarded. If the data contain n points, the methods needs to check O(n2) balls for\neach of the n point resulting in a runtime of O(n3). For scalability on real data, we implemented a randomized version\nof this method that for each point only check M balls picked uniformly at random from the set of all possible balls\nand used M = O(n); the resulting runtime is O(n2). In the following we refers to our methods as RKM and RPSA\nrespectively. All experiments have been run on an standard laptop equipped with an Intel i9 with 8 cores each working\nat 2,4 GHz and 16 GB of RAM DDR4 working at 2,6 GHz.\n7.1\nKMEANS Clustering\nSynthetic Data.\nWe run two experiments with artiﬁcial data in R2. In the ﬁrst experiment, we generated 300 inliers\nfrom 3 isotropic truncated Gaussians (100 points each) with variance 0.1 along both axis and mean (−3, 0), (0, 1) and\n(3, 0) respectively. We then corrupt the data adding 100 points from a fourth isotropic truncated Gaussian centered at\n(−1, −5) with variance 5 along both axis. For both RKM and KMEANS++ we T = 10 and r = 30. We initialized RKM\nwith uniform centers and set ζ = 0.75, the same ζ is used for SD. Results are shown in Figure 3 top left, where it is\npossible to see that while RKM recovers the true centers, SD and KMEANS++ both fail badly placing one centers in the\nmiddle of the two clusters and the other close to the mean of the perturbing distribution. In the second experiment, we\ngenerated 300 points from the same 3 inliers Gaussians and set the algorithms with k = 2 and ζ = 0.6, while T and\nr are as above. Results are shown in the top right of Figure 3, where it is possible to see that KMEANS++ and SD –\nalthough to a lesser extend – wasted a center to merge 2 clusters, while RKM correctly recovers 2 out of the 3 centers.\nReal Data.\nIn the synthetic experiments we choose ζ according to the exact fraction of outliers, a quantity which is\nusually unknown in practice. Here we show that there is a wide range of values for ζ such that RKM performs better\nthan KMEANS++. We used the Fashion-MNIST dataset which consists of about 70000 28 × 28 images of various types\nof clothes splitted in a training set of 60000 images and a test set of 10000 images. Speciﬁcally, there are 10 classes\nin the dataset: t-shirts, trousers, pullover, dresses, coats, sandals, shirts, sneakers, bags and ankle boots. The training\ndata were generated by sampling 1000 points, from the training set, each from the sneakers and the trousers classes\nas inliears, and 250 points from each other class as outliers. The resulting fraction of outliers is about 0.5. The test\ndata consist of all the sneakers and the trousers in the test set and has size of about 2000. We run the algorithms with\n9\nA PREPRINT - FEBRUARY 22, 2021\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nζ\n22\n24\n26\n28\nTest error\nRPSA\nSD\nPSA\nFigure 4: Experiments for PSA on synthetic data and real data with k = 1 and k = 2.\nT = 50, r = 30, M = 4000, k = 2 and ζ in the range [0.4, 1]. Results are shown in the bottom row of Figure 3. In\nthe lower left, it is possible to see that the centers learned by RKM at the optimal threshold value ζ = 0.5 look good,\nwhile the centers found by SD and KMEANS++ are affected by the outliers. Speciﬁcally, the such centers arise from the\noverlap of multiple classes. One center suffers from the effect of the other two shoes classes (sandald and boots) as\nwitnessed by the elongated background area, while the other is affected by the clothes classes (most noticeably, the\ncoats) as suggested by background shadow. As for the reconstruction error, RKM outperforms SD uniformly over the\nrange of considered values of ζ.\n7.2\nPrincipal Subspace Analysis\nSynthetic Data.\nWe run a synthetic experiment with artiﬁcial data in R2. We generate 50 points from the uniform\ndistribution over [−1, 1] × [−0.1, 0.1] as inliers and 50 points for the uniform distribution over R++ ∪R−−∩B(0, 1)5\nas outliers. We run RPSA with T = 50, r = 30, ζ = 0.5 and initialize U as a normalized Gaussian matrix. We set\nk = 1 for all algorithms. Results are shown in the left plot of Figure 4 where it is possible to see that the principal\nsubspace learned by RPSA is not affected by the outliers, as opposed to SD and PSA.\nReal Data.\nSimilarly to the case of KMEANS, we tested our method on real data for a range of values of ζ. We used\nagain the same setting as before on the Fashion-MNIST dataset. We run the algorithms we T = 50, r = 5, M = 4000,\nk = 2 and ζ in the range [0.4, 1]. Results are shown in the right plot of Figure 4, where it is possible our algorithm\noutperforms both PSA and does better than SD.\n8\nConclusions and Future Works\nIn this work, we address the important problem of designing robust methods for unsupervised learning. We proposed\na novel general framework, based on the minimization of an L-statistic, to design algorithms that are resilient to the\npresence of outliers and/or to model miss-speciﬁcation. Our method has strong statistical guarantees, is ﬂexible enough\nto incorporate many problems in unsupervised learning and is effective in practice as the experiments reveal. On the\nother hand, several extensions can be considered. First, here we studied in details KMEANS and PSA, but our theory also\ncovers the cases of KMEDIAN, sparse coding or non-negative matrix factorization. A related improvement also regards\nthe design of methods for the choice of ζ which do not require an estimate of the fraction of outliers. Second, we\nbelieve that this framework can be extended to supervised learning problems such us canonical correlation analysis and\npartial least squares. Third, our algorithm has only a descent property, and it would be interesting to design algorithms\nwith stronger guarantees such as provable approximation properties.\nReferences\n[1] Sara Ahmadian, Alessandro Epasto, Ravi Kumar, and Mohammad Mahdian.\nClustering without over-\nrepresentation. In Ankur Teredesai, Vipin Kumar, Ying Li, R´omer Rosales, Evimaria Terzi, and George Karypis,\neditors, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\nKDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 267–275. ACM, 2019.\n[2] Dana Angluin and Philip D. Laird. Learning from noisy examples. Mach. Learn., 2(4):343–370, 1987.\n[3] St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of\nindependence. Oxford university press, 2013.\n5Here with R++ and R−−we denote the top right and the bottom left orthant of R2.\n10\nA PREPRINT - FEBRUARY 22, 2021\n[4] Sanjay Chawla and Aristides Gionis. k-means-: A uniﬁed approach to clustering and outlier detection. In\nProceedings of the 13th SIAM International Conference on Data Mining, May 2-4, 2013. Austin, Texas, USA,\npages 189–197. SIAM, 2013.\n[5] F. Cucker and S. Smale. On the mathematical foundations of learning. American Mathematical Society, 39(1):1–49,\n2002.\n[6] Ryan T Elmore, Thomas P Hettmansperger, and Fengjuan Xuan. Spherical data depth and a multivariate median.\nDIMACS Series in Discrete Mathematics and Theoretical Computer Science, 72:87, 2006.\n[7] Ricardo Fraiman, Fabrice Gamboa, and Leonardo Moreno. Connecting pairwise geodesic spheres by depth:\nDCOPS. J. Multivar. Anal., 169:81–94, 2019.\n[8] Frank R Hampel. The inﬂuence curve and its role in robust estimation. Journal of the american statistical\nassociation, 69(346):383–393, 1974.\n[9] Frank R Hampel. Robust statistics: A brief introduction and overview. In Research report/Seminar f¨ur Statistik,\nEidgen¨ossische Technische Hochschule (ETH), volume 94. Seminar f¨ur Statistik, Eidgen¨ossische Technische\nHochschule, 2001.\n[10] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama.\nCo-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in neural\ninformation processing systems, pages 8527–8537, 2018.\n[11] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In\nAdvances in neural information processing systems, pages 1189–1197, 2010.\n[12] Jaeho Lee, Sejun Park, and Jinwoo Shin.\nLearning bounds for risk-sensitive learning.\narXiv preprint\narXiv:2006.08138, 2020.\n[13] Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In Advances in Neural\nInformation Processing Systems, volume 31, pages 2687–2696, 2018.\n[14] EH Lloyd. Least-squares estimation of location and scale parameters using order statistics. Biometrika, 39(1/2):88–\n95, 1952.\n[15] Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of Probability, pages\n1269–1283, 1990.\n[16] Andreas Maurer et al. A bernstein-type inequality for functions of bounded interaction. Bernoulli, 25(2):1451–\n1471, 2019.\n[17] Andreas Maurer and Massimiliano Pontil. k-dimensional coding schemes in Hilbert spaces. IEEE Transactions\non Information Theory, 56(11):5839–5846, 2010.\n[18] Andreas Maurer and Massimiliano Pontil. Empirical bounds for functions with weak interactions. arXiv preprint\narXiv:1803.03934, 2018.\n[19] Andreas Maurer and Massimiliano Pontil. Uniform concentration and symmetrization for weak interactions. arXiv\npreprint arXiv:1902.01911, 2019.\n[20] Keith Ord. Outliers in statistical data : V. Barnett and T. Lewis, 1994, 3rd edition, (John Wiley & Sons, Chichester),\n584 pp., [UK pound]55.00, ISBN 0-471-93094-6. International Journal of Forecasting, 12(1):175–176, March\n1996.\n[21] Robert J Serﬂing. Approximation theorems of mathematical statistics, volume 162. John Wiley & Sons, 1980.\n11\nA PREPRINT - FEBRUARY 22, 2021\nSupplementary Material\nThe supplementary material is organized as follows:\n• In Appendix A we prove the statistical properties of the proposed method; in particular we prove Theorems 1,\n3 and 5.\n• In Appendix B we give a proof of the hardness result described by Theorem 10.\n• Finally, in Appendix C we present additional experiments with the proposed method for the case of K-MEANS.\nA\nStatistical Properties of the Proposed Method\nWe ﬁrst analyze some basic properties of the functional ΦW . The following is easily seen to be an alternative deﬁnition\nof ΦW .\nKW (t) =\nZ t\n0\nW (u) du\nand\nΦW (ρ) =\nZ ∞\n0\nrdKW (Fρ (r)) for ρ ∈P ([0, ∞)) .\nFrom this we ﬁnd\nLemma 12. For ρ1, ρ2 ∈P and W bounded\nΦW (ρ1) −ΦW (ρ2) = −\nZ ∞\n0\n(KW (Fρ1 (r)) −KW (Fρ2 (r))) dr,\n(10)\nand\nd\ndtΦW ((1 −t) ρ1 + tρ2) =\nZ ∞\n0\nW\n\u0000F(1−t)ρ1+tρ2 (r)\n\u0001\n(Fρ1 (r) −Fρ2 (r)) dr.\nProof. Since members of P have ﬁnite ﬁrst moments we have for any ρ ∈P that rρ (r, ∞) →0 as r →∞, so\nlim\nr→∞r (KW (Fρ1 (r)) −KW (Fρ2 (r))) ≤∥W∥∞lim\nr→∞r |ρ2 (r, ∞) −ρ1 (r, ∞)| = 0,\nand the formula (10) follows from integration by parts. Thus for arbitrary ρ ∈P\nΦW ((1 −t) ρ1 + tρ2) −ΦW (ρ) = −\nZ ∞\n0\n(KW ((1 −t) Fρ1 (r) + tFρ2 (r)) −KW (Fρ (r))) dr.\nTaking the derivative w.r.t. t and using the chain rule and K′\nW = W gives the second identity.\nWe now analyze the inﬂuence function of the functional ΦW .\nLemma 13. Let R ∈[0, ∞), ρ ∈P ([0, ∞))\n(i) If W is nonnegative, bounded and W (t) = 0 for t ≥ζ and ζ < 1 then\nIF (R; ρ, ΦW ) ≤IFmax (ρ, W) :=\nZ F −1\nρ\n(ζ)\n0\nW (Fρ (r)) Fρ (r) dr.\n(ii) If ζ > 0, W = ζ−11[0,ζ], ρ is non-atomic and F −1\nρ\n(ζ) (ρ) = F −1\nρ\n(ζ) = F −1\nρ\n(ζ). Then\nIC (R; ρ, ΦW )\n=\n\u001a ζ−1 \u0000R + (ζ −1) F −1\nρ\n(ζ)\n\u0001\n−ΦW (ρ)\nif\n0 ≤R ≤F −1\nρ\n(ζ)\nF −1\nρ\n(ζ) −ΦW (ρ)\nif\nF −1\nρ\n(ζ) < R\n≤\nF −1\nρ\n(ζ) −ΦW (ρ) .\nProof. (i) In the second conclusion of Lemma 12, letting ρ2 = δR and taking the limit t →0 we obtain the inﬂuence\nfunction\nIF (R; ρ, ΦW ) =\nZ ∞\n0\nW (Fρ (r)) (Fρ (r) −FδR (r)) dr.\n12\nA PREPRINT - FEBRUARY 22, 2021\nPart (i) follows.\n(ii) From Lemma 12 we get\nd\ndtΦ ((1 −t) ρ + tδR) (t = 0)\n=\nζ−1\nZ F −1\nρ\n(ζ)\n0\n\u0000Fρ (r) −1[R,∞) (r)\n\u0001\ndr\n=\nζ−1\n Z F −1\nρ\n(ζ)\n0\nFρ (r) dr −\nZ F −1\nρ\n(ζ)\n0\n1[R,∞) (r) dr\n!\n.\nFrom integration by parts the ﬁrst term in parenthesis is ζ\n\u0000F −1\nρ\n(ζ) −ΦW (ρ)\n\u0001\n. The second term is zero if F −1\nρ\n(ζ) <\nR, otherwise it is F −1\nρ\n(ζ) −R. This gives the identity. For the inequality observe that R ≤F −1\nρ\n(ζ) implies\nζ−1 \u0000R + (ζ −1) F −1\nρ\n(ζ)\n\u0001\n≤F −1\nρ\n(ζ).\nA.1\nResilience to Perturbations\nWe prove Theorem 1.\nLemma 14. Let S, S∗∈S, µ ∈P\n\u0000Rd\u0001\n, and suppose that there exists r∗> 0 and α ∈(0, 1) such that\n∀r ∈(0, r∗) , FµS (r) ≤αFµS∗(r) .\n(11)\nIf W is nonzero on a set of positive Lebesgue measure, nonincreasing and W (t) = 0 for all t ≥FµS∗(r∗) then\nΦW (µS) −ΦW (µS∗) ≥(1 −α)\nZ r∗\n0\nW (FµS∗(r)) FµS∗(r) dr = (1 −α) IFmax (µS∗, W) > 0.\nProof. By Lemma 12 and the fundamental theorem of calculus\nΦW (µS) −ΦW (µS∗) =\nZ ∞\n0\n Z\n[0,1]\nW (sFµS (r) + (1 −s) FµS∗(r)) ds\n!\n(FµS∗(r) −FµS (r)) dr.\nSuppose ﬁrst r∗≤r. If W > 0 then sFµS (r) + (1 −s) FµS∗(r) < FµS∗(r∗) ≤FµS∗(r) and therefore FµS (r) <\nFµS∗(r∗), so the integrand is positive, or else W = 0. For a lower bound we can therefore restrict the integration in r\nto the interval [0, r∗).\nIf r < r∗then by (11) sFµS (r) + (1 −s) FµS∗(r) < FµS∗(r) ≤FµS∗(r∗) so W (sFµS (r) + (1 −s) FµS∗(r)) ≥\nW (FµS∗(r)), since W is nonincreasing. The conclusion follows from (11).\nWe restate Assumption A and Theorem 1.\nAssumption A. There exists S0 ∈S, δ > 0, β ∈(0, 1 −λ) and a scale parameter r∗∈(0, 1) (in units of squared\neuclidean distance), such that for every model S ∈S satisfying Φ (µ∗\nS) > Φ\n\u0000µ∗\nS0\n\u0001\n+ δ we have FµS (r) < βFµ∗\nS0 (r)\nfor all r ≤r∗.\nTheorem 15. Let µ∗, ν ∈P\n\u0000Rd\u0001\n, µ = (1 −λ) µ∗+ λν, and λ ∈(0, 1) and suppose there are S0, r∗, δ > 0\nand 0 < β < 1 −λ, satisfying Assumption A. Suppose that W is nonzero on a set of positive Lebesgue measure,\nnonincreasing and W (t) = 0 for t ≥ζ = FµS0 (r∗).\nProof. Let S, S0 ∈S and assume that Φ (µ∗\nS) > Φ\n\u0000µ∗\nS0\n\u0001\n+ δ. Then for r ≤r∗Assumption A implies FµS (r) ≤\nβFµ∗\nS0 (r) ≤\nβ\n1−λFµS0 (r), and the conditions on W also imply that W = 0 on [FµS∗(r∗) , 1]. Thus Lemma 14 with\na = β/ (1 −λ) < 1 gives\nΦW (µS) −ΦW (µS0) ≥\n\u0012\n1 −\nβ\n1 −λ\n\u0013\nIFmax (µS0, W) > 0.\nThus, if ΦW (µS) −ΦW (µS0) <\n\u0010\n1 −\nβ\n1−λ\n\u0011\nIFmax (µS0, W), we must have Φ (µ∗\nS) ≤Φ\n\u0000µ∗\nS0\n\u0001\n+ δ. The condition\n(11) is clearly always satisﬁed by the minimizer S† (µ) of ΦW (µS).\n13\nA PREPRINT - FEBRUARY 22, 2021\nA.2\nGeneralization\nA second application of Lemma 12 gives a Lipschitz property of ΦW relative to the Wasserstein and Kolmogorov\nmetrics for distributions with bounded support.\nLemma 16. For ρ1, ρ2 ∈P with support in [0, Rmax] and ∥W∥∞< ∞\nΦW (ρ2) −ΦW (ρ1) ≤∥W∥∞dW (ρ1, ρ2)\nand\nΦW (ρ2) −ΦW (ρ1) ≤Rmax ∥W∥∞dK (ρ1, ρ2) .\nHere dW (ρ1, ρ2) = ∥Fρ1 −Fρ2∥1 is the 1-Wasserstein distance and dK (ρ1, ρ2) = ∥Fρ1 −Fρ2∥∞the Kolmogorov-\nSmirnov distance.\nProof. From (10) and Hoelder’s inequality we get\nΦW (ρ1) −ΦW (ρ2) = −\nZ ∞\n0\n Z Fρ1(r)\nFρ2(r)\nW (u) du\n!\ndr ≤2 ∥W∥∞\nZ ∞\n0\n|Fρ1 (r) −Fρ2 (r)| dr.\nWe can bound the integral either by ∥Fρ1 −Fρ2∥1 = dW (ρ1, ρ2), which gives the ﬁrst inequality, or by\nZ Rmax\n0\n|Fρ1 (r) −Fρ2 (r)| dr ≤∥Fρ1 −Fρ2∥∞\nZ Rmax\n0\ndr = RmaxdK (ρ1, ρ2) ,\nwhich gives the second inequality.\nThe Lipschitz properties imply estimation and bias bounds for the plug-in estimator.\nCorollary 17. Let ρ ∈P with support in [0, Rmax] and ∥W∥∞< ∞and suppose that ˆρ is the empirical measure\ngenerated from n iid observations R = (R1, ..., Rn) ∼ρn\nˆρ (R) = 1\nn\nn\nX\ni=1\nδRi.\nThen (i)\nPr {|ΦW (ρ) −ΦW (ˆρ (R))| > t} ≤2 exp\n \n−2nt2\nR4max ∥W∥2\n∞\n!\n.\nand (ii)\nΦW (ρ) −E [ΦW (ˆρ (R))] ≤Rmax ∥W∥∞\n√\n2n\n.\nProof. (i) By Lemma 16 and the Dvoretzky-Kiefer-Wolfowitz Theorem in the version of Massart [15]\nPr {|ΦW (ρ) −ΦW (ˆρ (R))| > t} ≤Pr\n\u001a\ndK (ρ, ˆρ (R)) >\nt\nRmax ∥W∥∞\n\u001b\n≤2 exp\n \n−2nt2\nR2max ∥W∥2\n∞\n!\n.\n(ii) Let R′ = (R1, ..., Rn) be iid to R. Then\n14\nA PREPRINT - FEBRUARY 22, 2021\nΦW (ρ) −E [ΦW (ˆρ (R))]\n≤\n∥W∥∞E [dW (ρ1, ˆρ (R))]\n=\n∥W∥∞ER\nZ Rmax\n0\n\f\f\f\f\fER′\n\"\n1\nn\nX\ni\n1[R′\ni,∞) (t)\n#\n−\n\"\n1\nn\nX\ni\n1[Ri,∞) (t)\n#\f\f\f\f\f dt\n≤\n∥W∥∞\nn\nZ Rmax\n0\nERR′\n\f\f\f\f\f\nX\ni\n\u0010\n1[R′\ni,∞) (t) −1[Ri,∞) (t)\n\u0011\f\f\f\f\f dt\n≤\n∥W∥∞\nn\nZ Rmax\n0\n \nERR′\nX\ni\n\u0010\n1[R′\ni,∞) (t) −1[Ri,∞) (t)\n\u00112\n!1/2\ndt\n=\n∥W∥∞\n√n\nZ Rmax\n0\n\u0012\nER1R′\n1\n\u0010\n1[R′\n1,∞) (t) −1[R1,∞) (t)\n\u00112\u00131/2\ndt\nby Jensens inequality and independence. But the expectation is just twice the variance of the Bernoulli variable\n1[R1,∞) (t), and therefore at most 1/2. The result follows.\nRephrasing part (i) of this corollary in terms of conﬁdence windows we have, for any δ > 0 with probability at least\n1 −δ that\n|ΦW (ρ) −ΦW (ˆρ (R))| ≤Rmax ∥W∥∞\nr\nln (2/δ)\n2n\n.\nFor the weight function W = ζ−11[0,ζ] the bound on the estimation error scales with ζ−1, which is not surprising,\nsince we only consider a fraction ζ of the data. So for decreasing ζ the functional becomes more robust (because the\ninﬂuence Rζ decreases) but it becomes more difﬁcult to estimate.\nRestatement of Proposition 2.\nProposition 18. Assume the conditions of Theorem 1. Then\nPr\nn\nΦ\n\u0010\nµ∗\nˆS(X)\n\u0011\n≤Φ (µ∗\nS∗) + δ\no\n≥Pr\n\u001a\n2 sup\nS∈S\n|ΦW (µS) −ΦW (ˆµS (X))| ≤\n\u0012\n1 −\nβ\n1 −λ\n\u0013\nICmax (µS∗, W)\n\u001b\n.\nProof.\nΦW\n\u0010\nµ ˆS(X)\n\u0011\n−ΦW (µS∗)\n≤\n\u0010\nΦW\n\u0010\nµ ˆS(X)\n\u0011\n−ΦW\n\u0010\nˆµ ˆS(X) (X)\n\u0011\u0011\n+\n\u0010\nΦW\n\u0010\nˆµ ˆS(X) (X)\n\u0011\n−ΦW (ˆµS† (X))\n\u0011\n+ (ΦW (ˆµS† (X)) −ΦW (µS†)) + (ΦW (µS†) −ΦW (µS∗)) .\nThe second term and the last term are negative by the minimality properties of ˆS (X) and S†. The remaining terms are\nbounded by 2 supS∈S |ΦW (µS) −ΦW (ˆµS (X))|. Thus\nPr\n\u001a\n2 sup\nS∈S\n|ΦW (µS) −ΦW (ˆµS (X))| ≤\n\u0012\n1 −\nβ\n1 −λ\n\u0013\nICmax (µS∗, W)\n\u001b\n≤Pr\n\u001a\nΦW\n\u0010\nµ ˆS(X)\n\u0011\n−ΦW (µS∗) ≤\n\u0012\n1 −\nβ\n1 −λ\n\u0013\nICmax (µS∗, W)\n\u001b\n≤Pr\nn\nΦ\n\u0010\nµ∗\nˆS(X)\n\u0011\n≤Φ (µ∗\nS∗) + δ\no\n,\nwhere the last inequality follows from Theorem 1.\nLemma 19. If W = ζ−11[0,ζ] with ζ < 1, then for ρ ∈P ([0, Rmax))\nΦW (ρ) =\nsup\nλ∈[0,Rmax]\n\u001a\nλ −ζ−1\nZ ∞\n0\nmax {λ −t, 0} dρ (t)\n\u001b\n15\nA PREPRINT - FEBRUARY 22, 2021\nProof. Integration by parts gives\nZ ∞\n0\nmax {λ −t, 0} dρ (t) =\nZ λ\n0\nFρ (t) dt = λFρ (λ) −\nZ λ\n0\ntdρ (t) .\nThe maximum of λ −ζ−1 R λ\n0 Fρ (t) dt is attained at ζ = Fρ (λ), which shows λ ≤Rmax, and substitution gives\nsup\nλ∈R\n\u001a\nλ −ζ−1\nZ ∞\n0\nmax {λ −t, 0} dρ (t)\n\u001b\n=\nλ −ζ−1\n \nλFρ (λ) −\nZ λ\n0\ntdρ (t)\n!\n=\nζ−1\nZ F −1\nρ\n(ζ)\n0\ntdρ (t) =\nZ ∞\n0\ntζ−11[0,ζ] (Fρ (t)) dρ (t)\n=\nΦW (ρ) .\nRestatement of Theorem 3.\nTheorem 20. Let W = ζ−11[0,ζ] and η > 0. With probability at least 1 −η in X ∼µn we have that\nsup\nS∈S\n|ΦW (µS) −ΦW (ˆµS (X))| ≤2\nζnEXR (F, X) + Rmax\nζ√n\n \n2 +\nr\nln (2/η)\n2\n!\n,\nwhere R (F, X) is the Rademacher average\nR (F, X) = Eϵ\n\"\nsup\nS∈S\nn\nX\ni=1\nϵid (Xi, S)\n#\nwith independent Rademacher variables ϵ = (ϵ1, ..., ϵn).\nProof. Using Lemma 19 we get with independent Rademacher variables ϵ = (ϵ1, ..., ϵn)\nE\n\u0014\nsup\nS∈S\nΦW (µS) −ΦW (ˆµS (X))\n\u0015\n≤\nζ−1EX\n\"\nsup\nλ∈[0,Rmax],S∈S\nZ ∞\n0\nmax {λ −t, 0} dˆµS (X) (t) −\nZ ∞\n0\nmax {λ −t, 0} dµS (t)\n#\n=\nζ−1EX\n\"\nsup\nλ∈[0,Rmax],S∈S\n1\nn\nn\nX\ni=1\nmax {λ −d (Xi, S) , 0} −EX∼µ [max {λ −d (X, S) , 0}]\n#\n=\n1\nζnEXX′\n\"\nsup\nλ∈[0,Rmax],S∈S\nn\nX\ni=1\n(max {λ −d (Xi, S) , 0} −max {λ −d (X′\ni, S) , 0})\n#\n=\n1\nζnEXX′ϵ\n\"\nsup\nλ∈[0,Rmax],S∈S\nn\nX\ni=1\nϵi (max {λ −d (Xi, S) , 0} −max {λ −d (X′\ni, S) , 0})\n#\n≤\n2\nζnEXϵ\n\"\nsup\nλ∈[0,Rmax],S∈S\nn\nX\ni=1\nϵi max {λ −d (Xi, S) , 0}\n#\n≤\n2\nζnEXϵ\n\"\nsup\nλ∈[0,Rmax],S∈S\nn\nX\ni=1\nϵi (λ −d (Xi, S))\n#\n≤\n2\nζnEXϵ\n\"\nsup\nS∈S\nn\nX\ni=1\nϵid (Xi, S)\n#\n+ 2\nζnEϵ\n\"\nsup\nλ∈[0,Rmax]\nλ\nn\nX\ni=1\nϵi\n#\n≤\n2\nζnEXR (F, X) + 2Rmax\nζ√n .\n16\nA PREPRINT - FEBRUARY 22, 2021\nHere the third identity is a standard symmetrization argument, the second inequality the triangle inequality, followed\nby the contraction inequality for Rademacher averages, since t →max {t, 0} is a contraction. Then we used\nthe triangle inequality again. Now let Ψ (X) be the random variable supS∈S ΦW (µS) −ΦW (ˆµS (X)). It then\nfollows from Lemma 16 and the bounded difference inequality that with probability at least 1 −η we have Ψ (X) ≤\nEΨ (X) + ζ−1Rmax\np\nln (1/η) / (2n).\nCombined with above bound on EΨ (X) this completes the proof.\nTheorem 4 follows directly from Theorems 2 and 5 in [19] and from the bias bound, Corollary 17 (ii).\nRestatement of Theorem 5.\nTheorem 21. Under the conditions of the previous theorem, with probability at least 1 −η in X ∼µn we have that for\nall S ∈S\n|ΦW (µS) −ΦW (ˆµS (X))| ≤\np\n2VSC +\n6Rmax\n\u0010\n∥W∥∞+ ∥W∥Lip\n\u0011\nC\nn\n+ ∥W∥∞Rmax\n√n\n,\nwhere VS is the variance of the random variable ΦW (ˆµS (X)), and C is the complexity term\nC = kd ln\n\u0010\n16n ∥S∥2 /η\n\u0011\nif S is the set of sets with k elements, or convex polytopes with k vertices and ∥S∥= supx∈S∈S ∥x∥, or\nC = kd ln\n\u000016nR2\nmax/η\n\u0001\nif S is the set of set of k-dimensional subspaces.\nProof. For any ﬁxed S ∈S the L-statistic x ∈X n 7→fS (x) := ΦW (ˆµS (x)) is\n\u0010\nRmax ∥W∥∞, Rmax ∥W∥Lip\n\u0011\n-weakly interacting (see [18]) and therefore satisﬁes the following version of Bernstein’s inequality (see [16], [18]): For\nη ∈(0, 1/e) with probability at least 1 −η in X ∼µn we have\nE [fS] −fS (X) ≤\np\n2VS ln (1/η) + Rmax\n\u00122 ∥W∥∞\n3\n+\n3 ∥W∥Lip\n2\n\u0013 ln (1/η)\nn\n,\nwhere E [fS] and VS are expectation and variance of the random variable fS (X) = ΦW (ˆµS (X)) respectively. We\nwill make this bound uniform with a covering number argument.\nDeﬁne a pseudo metric dX on S by\ndX (S1, S2) = sup\nx∈X\n|d (x, S1) −d (x, S2)| .\nIt follows from Lemma 16 that for every x ∈X n we have\nfS1 (x) −fS2 (x) ≤∥W∥∞dW (ˆµS1 (x) , ˆµS2 (x)) ≤∥W∥∞dX (S1, S2) .\nIn particular |E [fS1] −E [fS2]| ≤∥W∥∞dX (S1, S2) and\np\nVS1 −\np\nVS2\n=\n∥fS1 −E [fS1]∥L2(µn) −∥fS2 −E [fS2]∥L2(µn)\n≤\n∥fS1 −fS2∥L2(µn) + |E [fS1] −E [fS2]| ≤2 ∥W∥∞dX (S1, S2) .\nNow let N = N (S, dX , ϵ) be the corresponding minimal covering number of S with dX -balls of radius ϵ, and\nlet S0 ⊆S be such that ∀S ∈S, ∃S′ ∈S0 with dR (S, S′) < 1/n and |S0| ≤N.\nThen, abbreviating\nRmax\n\u0010\n2 ∥W∥∞/3 + 3 ∥W∥Lip /2\n\u0011\nwith C, with probability at least 1 −η in X that for every S ∈S\n17\nA PREPRINT - FEBRUARY 22, 2021\nE [fS] −fS (X)\n≤\nE [fS′] −fS′ (X) + 2 ∥W∥∞\nn\n≤\np\n2VS′ ln (N/η) + C ln (N/η) + 2 ∥W∥∞\nn\n=\np\n2VS ln (N/η) + C ln (N/η) + 2 ∥W∥∞\nn\n+\n\u0010p\nVS′ −\np\nVS\n\u0011 p\n2 ln (N/η)\n≤\np\n2VS ln (N/η) + C ln (N/η) + 2 ∥W∥∞\np\n2 ln (N/η) + 2 ∥W∥∞\nn\n.\nIn the ﬁrst inequality we used uniform approximation of fS by fS′, where S′ is the nearest neighbour of S in S0. The\nnext line combines Bernstein’s inequality with a union bound over S0. Finally we again approximate √VS′ by √VS.\nNext we bound the covering numbers N (S, dX , 1/n), which we do separately for the case of uniformly bounded S\nand PSA. In case of the mean, k-means or sparse coding is easy to see that for S1, S2 ∈S and any two respective\nenumerations xi and yi or enumerations of the extreme points\ndX (S1, S2) ≤2 ∥S∥H (S1, S2) ≤2 ∥S∥max\ni\n∥xi −yi∥.\nIt follows that N (S, dX , 1/n) can be bounded by the covering number of a ball of radius ∥S∥2 in a kd-dimensional\nBanach space. Use the standard result of Cucker and Smale [5] we have\nN (S, dX , 1/n) ≤\n\u0010\n8n ∥S∥2\u0011kd\n.\nFor PSA we can use unit vectors spanning the subspaces and instead of ∥S∥2 we have the maximal squared norm in the\nsupport, so\nN (S, dX , 1/n) ≤\n\u0010\n8n ∥X∥2\u0011kd\n.\nkd ln\n\u0010\n8n ∥S∥2 /η\n\u0011\n.\nPutting it all together and adding the bias bound ΦW (µS) −E [ΦW (ˆµS (X))] ≤∥W∥∞Rmax/√n (Corollary 17 (ii))\nwe get\nΦW (µS) −ΦW (ˆµS (X))\n≤\nr\n2σ2 (ΦW (ˆµS (X))) kd ln\n\u0010\n8n ∥S∥2 /η\n\u0011\n+\nRmax\n\u0010\n6 ∥W∥∞+\n3∥W ∥Lip\n2\n\u0011\nkd ln\n\u0010\n8n ∥S∥2 /η\n\u0011\nn\n+ ∥W∥∞Rmax\n√n\nThe result follows from elementary estimates and algebraic simpliﬁcations.\nB\nAlgorithms\nRestatement of Lemma 7.\nLemma 22. For any S ∈S and any p ∈Symn, if π is the ascending ordering of the dS(xi)s, then φS(x, p) ≥\nφS(x, π) = ˆΦS(x).\nProof. Writing w (i) = W\n\u0010\nπ(i)\nn\n\u0011\nand zi = dS\n\u0000xπ(i)\n\u0001\nit is enough to show that the identity permutation is a minimizer\nof\nr (p) =\nn\nX\ni=1\nw (p (i)) zi for p ∈Symn\nThis follows from the following claim, which we prove by induction:\nFor k ∈{1, ..., n} there is for every p ∈Symn some p′ ∈Symn such that r (p′) ≤r (p) and p′ (j) = j for all\n1 ≤j < k. The case k = 1 holds trivially. If the claim holds for any k ≤n −1 then there is q ∈Symn such that\n18\nA PREPRINT - FEBRUARY 22, 2021\nr (q) ≤r (p) and q (j) = j for all 1 ≤j < k. If q (k) = π (k) then the claim for k + 1 clearly holds by deﬁning\np′ := q. If q (k) ̸= k note ﬁrst that both q (k) > k and q−1 (k) > k. Then deﬁne p′ (j) := q (j) except for p′ (k) := k\nand p′ \u0000q−1 (k)\n\u0001\n:= q (k). Then p′ (j) = j for all 1 ≤j < k + 1 and\nr (p′) −r (p) ≤r (p′) −r (q) = (w (k) −w (q (k)))\n\u0000zk −zq−1(k)\n\u0001\n≤0,\nbecause the ﬁrst term is non-negative (since w is non-increasing) and the second non-positive. So r (p′) ≤r (p) which\nproves the claim for the case k + 1 and completes the induction.\nRestatement of Theorem 9.\nTheorem 23. Minimizing ˆΦS(x) for the case of KMEANS when k = 1 and W is the hard threshold is NP-Hard.\nProof. Notice that minimizing the ˆΦS(x) in the case of KMEANS is equivalent to minimize the following function of a\nsubset C ⊆X of size ⌊zn⌋\nL(C) = 1\nn\nX\nx∈C\n∥xi −µC∥2\n2\nwhere µC = mean(C) and return µC. In what follow we will consider L(C) as actually L(C)n in order to remove the\nconstant factor outside the objective and simplify the notation. The following lemma enables us to rewrite L(C) in\nterms of pairwise distances.\nLemma 24. Let C ⊆X, then\nL(C) =\n1\n2|C|\nX\nx,y∈C\n∥x −y∥2\n2.\n(12)\nProof. Let X and Y two i.i.d. random variables supported on C, then\nE[∥X −Y ∥2\n2] = E[∥X∥2\n2] + E[∥Y ∥2\n2] −2E[⟨X, Y ⟩]\n= E[∥X∥2\n2] + E[∥X∥2\n2] −2E[∥E[X]∥2\n2]\n= 2E[∥X∥2\n2] −2E[∥E[X]∥2\n2] = 2E[∥X −E[X]∥2\n2].\nNow assume X and Y are independent samples from the uniform distribution on C, then\nE[∥X −E[X]∥2\n2] =\n1\n|C|\nX\nx∈C\n∥x −µC∥2\n2\n= E[∥X −Y ∥2\n2]/2 =\n1\n2|C|2\nX\nx,y∈C\n∥x −y∥2\n2\nfrom which the thesis follows.\nWe recall the deﬁnition of NP-hardness for optimization problems.\nDeﬁnition 25. A computational problem Π is said NP-hard (optimization) if and only if the related decision problem\nΠD is NP-hard. Assume Π is deﬁned as the problem of minimizing a function fX(µ) deﬁned by an input instance X\nif the minimum exists, then ΠD is deﬁned as the problem of determining, given in input X and a rational number c,\nwhether there exist an assignment to the variables µ such that fX(µ) ≤q.\nIn order to show hardness of an optimization problem Π, it is enough to show hardness of the related decision problem\nΠD. For this reason, the following will be useful.\nDeﬁnition 26. DECISION ROBUST 1-MEANS\nInput: Points X = {x1, . . . , xn} ⊂Rd, an integer h and a rational number c.\n19\nA PREPRINT - FEBRUARY 22, 2021\nDataset\nk\nRKM\nSD\nK-MEANS++\nFMNIST\n2\n25.98\n33.17\n34.39\nEMNIST\n2\n38.41\n37.78\n40.29\ncifar10\n4\n31.07 × 104\n96.42 × 105\n95.57 × 105\nVictorian\n5\n1.64\n1.66\n1.76\nIris\n1\n0.32\n3.71\n4.75\nTable 1: Experimental results for the case of K-MEANS clustering. In all the experiments ζ = 0.5. In each row, the\nperformance in bold corresponds to the winning algorithm.\nOutput: Yes if there exist a C ⊆X such that |C| = h and L(C) ≤c, No otherwise.\nTo prove the theorem we will reduce n/2-CLIQUE to the decision version ROBUST 1-MEANS via a polynomial time\nalgorithm. Since n/2-CLIQUE is NP-complete, hardness for ROBUST 1-MEANS will follow.\nDeﬁnition 27. n/2-CLIQUE\nInput: A simple undirected connected graph G = (V, E) with |V | = n.\nOutput: Yes if G contains a clique of size n/2, No otherwise.\nGiven an instance of n/2-CLIQUE in the form of a graph G = (V, E) with n vertices, we create an instance of ROBUST\n1-MEANS ΠD(G) which is equivalent to G. Let A denote the symmetric n × n adjacency matrix of G, i.e. Aij = 1 iff\n(i, j) ∈E otherwise Aij = 0. Consider the graph embedding given by the map φ : V →Rn such that φ(i) = Ai: +nei,\nwhere Ai: denotes the i-th row of A and ei denotes the i-th vector of the canonical basis of Rn. Given G we build\nan instance of ROBUST 1-MEANS by setting X = {φ(1), . . . , φ(n)}, h = n/2 and c = m(2n2 −3n), where we set\nm =\n\u0000n\n2\n\u0001\nas a shortcut. Notice that it takes O(n) to build such instance. The following lemma ﬁnishes the proof by\nshowing the aforementioned equivalence.\nLemma 28. G is a Yes instance iff ΠD(G) is a Yes instance.\nProof. Assume that G is a Yes instance, i.e. G contains at least clique of size n/2. Notice that for any (i, j) ∈E it holds\nthat\n∥φ(i) −φ(j)∥2\n2 ≤(n −1)2 + (n −1)2 = 2n2 −4n + 2 ≤2n2 −3n\nwhile for any (i, j) /∈E it holds\n∥φ(i) −φ(j)∥2\n2 ≥2n2.\nIf {c1, . . . , cn/2} are the vertices in the clique, the cost L(C), by Lemma 24, of the subset C = {φ(c1), . . . , φ(cn/2)} is\nat most c, since in such clique contains exactly m edges.\nNow suppose that ΠD(G) admits a cost of at most c. Lets denote by C the subsets of X achieving such cost, then the\nassociated vertices must form a clique otherwise at least one of the m distance will be larger than 2n2 leading to a cost\nlarger of c.\nThus if we could solve in polynomial time DECISION ROBUST 1-MEANS we could solve in polynomial time n/2-CLIQUE.\nC\nExperiments\nIn this section we discuss the additional experimental results we obtained with our method in the case of K-MEANS\nclustering. We tested RKM, SD and standard K-MEANS++ with the ζ = 0.5, r = 30, and T = 100. Due to its cubic\nruntime, SD is slow even on moderate-sized datasets. Thus, we considered the randomized version of SD with M\nequals to the size of the training set. For this method, we repeated each experiment 5 times and reported the average\nreconstruction error on the test data (standard deviations resulted to be negligible in all cases).\nIn the following we describe each dataset, but Fashion MNIST whose experiment has already been described in the\nmain body.\n20\nA PREPRINT - FEBRUARY 22, 2021\nEMNIST.\nThis dataset consists of about 814000 28 × 28 images of digits, lowercase and uppercase letters from the\nEnglish alphabet arranged in 62 classes. The training data were generated by sampling 1000 0s and 1000 1s as inliers\nand sampling 33 points from each other class as outliers. We used k = 2 clusters. The test data consist of all the 0s and\n1s in the test set and has a size of about 2000.\ncifar10.\nThe dataset consists of about 60000 100 × 100 images from 10 classes: airplanes, cars, trucks, ships, dogs,\ncats, frogs, horses, birds and deer. The training data were generated by sampling 1000 points from each of the vehicle\nclasses as inliers and 300 points from each of the animal classes as outliers. We used k = 4 clusters. The test data\nconsist of all the vehicle images from the test set and has size of about 4000.\nVictorian.\nThis dataset consists of 4500 texts from 45 authors of English language from Victorian Era, 100 texts from\neach author. The data have been processed as in [1] and is made of 10 features. The training data were generated by\nsampling 50 points from each of one of the ﬁrst 5 authors in the dataset as inliears and 5 points from each other class as\noutliers. We used k = 5. The test data consist of the remaining 50 points from each of the inlier authors and has a size\nof about 250.\nIris.\nThis dataset consists of 150 records of iris ﬂowers. Each record contains 4 features: sepal length, sepal width,\npetal length and petal width. There classes. The training data were generated by sampling 30 points from the iris-setosa\nclass as inliear and 15 points from each other class as outliers. We used k = 1. Since the training set is small sized, we\nused exact version for SD. The test data consist of all the remaining iris-setosa points and has a size of about 20.\n21\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-12-14",
  "updated": "2021-02-18"
}