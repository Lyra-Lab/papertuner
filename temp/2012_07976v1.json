{
  "id": "http://arxiv.org/abs/2012.07976v1",
  "title": "NeurIPS 2020 Competition: Predicting Generalization in Deep Learning",
  "authors": [
    "Yiding Jiang",
    "Pierre Foret",
    "Scott Yak",
    "Daniel M. Roy",
    "Hossein Mobahi",
    "Gintare Karolina Dziugaite",
    "Samy Bengio",
    "Suriya Gunasekar",
    "Isabelle Guyon",
    "Behnam Neyshabur"
  ],
  "abstract": "Understanding generalization in deep learning is arguably one of the most\nimportant questions in deep learning. Deep learning has been successfully\nadopted to a large number of problems ranging from pattern recognition to\ncomplex decision making, but many recent researchers have raised many concerns\nabout deep learning, among which the most important is generalization. Despite\nnumerous attempts, conventional statistical learning approaches have yet been\nable to provide a satisfactory explanation on why deep learning works. A recent\nline of works aims to address the problem by trying to predict the\ngeneralization performance through complexity measures. In this competition, we\ninvite the community to propose complexity measures that can accurately predict\ngeneralization of models. A robust and general complexity measure would\npotentially lead to a better understanding of deep learning's underlying\nmechanism and behavior of deep models on unseen data, or shed light on better\ngeneralization bounds. All these outcomes will be important for making deep\nlearning more robust and reliable.",
  "text": "NeurIPS 2020 Competition:\nPredicting Generalization in Deep Learning (Version 1.1)\nYiding Jiang ∗†\nPierre Foret†\nScott Yak†\nDaniel M. Roy‡§\nHossein Mobahi†§\nGintare Karolina Dziugaite¶§\nSamy Bengio†§\nSuriya Gunasekar‖§\nIsabelle Guyon ∗∗§\nBehnam Neyshabur†§\npgdl.neurips@gmail.com\nDecember 16, 2020\nAbstract\nUnderstanding generalization in deep learning is arguably one of the most impor-\ntant questions in deep learning. Deep learning has been successfully adopted to a large\nnumber of problems ranging from pattern recognition to complex decision making, but\nmany recent researchers have raised many concerns about deep learning, among which\nthe most important is generalization. Despite numerous attempts, conventional sta-\ntistical learning approaches have yet been able to provide a satisfactory explanation\non why deep learning works. A recent line of works aims to address the problem by\ntrying to predict the generalization performance through complexity measures. In this\ncompetition, we invite the community to propose complexity measures that can ac-\ncurately predict generalization of models. A robust and general complexity measure\nwould potentially lead to a better understanding of deep learning’s underlying mecha-\nnism and behavior of deep models on unseen data, or shed light on better generalization\nbounds. All these outcomes will be important for making deep learning more robust\nand reliable.\n∗Lead organizer: Yiding Jiang; Scott Yak and Pierre Foret help implement large portion of the infras-\ntructure and the remaining organizers’ order is randomized.\n†Aﬃliation: Google Research\n‡Aﬃliation: University of Toronto\n§Equal contribution: random order\n¶Aﬃliation: Element AI\n‖Aﬃliation: Microsoft Research\n∗∗Aﬃliation: University Paris-Saclay and ChaLearn\n1\narXiv:2012.07976v1  [cs.LG]  14 Dec 2020\nKeywords\nGeneralization, Deep Learning, Complexity Measures\nCompetition type\nRegular.\n1\nCompetition description\n1.1\nBackground and impact\nDeep learning has been successful in a wide variety of tasks, but a clear understanding\nof underlying root causes that control generalization of neural networks is still elusive. A\nrecent empirical study [14] looked into many popular complexity measures. By a carefully\ncontrolled analysis on hyperparameter choices being a confounder for both generalization\nand the complexity measure, they came to surprising ﬁndings about which complexity\nmeasures worked well and which did not. However, rigorously evaluating these complexity\nmeasures required training many neural networks, computing the complexity measures on\nthem, and analyzing statistics that condition over all variations in hyperparameters. Such\ncumbersome process makes it painstaking, error-prone, and computationally expensive.\nAs the results, this procedure is not accessible to members of the wider machine learning\ncommunity who do not have access to larger compute power.\nBy hosting this competition, we intend to provide a platform where participants only\nneed to write the code that computes the complexity measure for a trained neural network,\nand let the competition evaluation framework handle the rest. This way, participants can\nfocus their eﬀorts on coming up with the best complexity measure instead of replicating\nthe experimental set up. In addition, the ML community beneﬁts from results that are\ndirectly comparable with each other, which alleviates the need for having every researcher\nto reproduce all the benchmark results themselves.\nThis problem is likely to be of interest to machine learning researchers who study gener-\nalization of deep learning or experts in learning theory, and the neural architecture search\ncommunity. We hope that this competition would enable researchers to quickly test theories\nof generalization with a shorter feedback loop, thereby leading to stronger foundations for\ndesigning high-performance, eﬃcient, and reliable deep learning algorithms/architectures.\nIn addition, the fact that all proposed approaches are assessed within the same evaluation\nsystem can ensure a fair and transparent evaluation procedure.\nThis competition is unlike a typical supervised-learning competition – participants are\nnot given a large and representative training set for training a model to produce predictions\non an unlabelled test set.\nInstead, participants submit code, which would run on our\nevaluation server, and the results would be published on a public leaderboard updated on\na daily basis. They are given a small set of models for debugging their code, but this set\n2\nis not expected to be suﬃcient for training their model. Their code is expected to take a\ntraining dataset of image label pairs as well as a model fully trained on it as input, and\ngenerate a real number as output. The value of the output should ideally be larger for\nmodels that have larger generalization gaps.\nGeneralization is one the most fundamental question of machine learning. A principled\nunderstanding of generalization can provide theoretical guarantees for machine learning\nalgorithms, which makes deep learning more accountable and transparent, and is also de-\nsirable in safety critical applications. For example, generalization to diﬀerent environments\nand data distribution shifts is a critical aspect for deploying autonomous vehicles in real\nlife. The result of this competition could also have implications for more eﬃcient architec-\nture search which could reduce the carbon footprint of designing machine learning models\nand have environmental impact in the long run.\n1.2\nNovelty\nThis competition is quite unique, and there are no previous competitions similar to it. The\ncompetition is focused on achieving better theory and understanding of the observed phe-\nnomena. We hope the competition will allow the fair comparison of numerous theories the\ncommunity has proposed about generalization of deep learning; however, we also welcome\nthe broader data science community to ﬁnd practical solutions that are robust under the\ncovariate shift in data and architecture, but do not necessarily give rise statistical bounds\nor direct theoretical analysis.\n1.3\nData\nAs previously outlined, our competition diﬀers from the traditional supervised learning\nsetting, because our data-points are trained neural networks. As such, the dataset provided\nto competitors will be a collection of trained neural networks. We chose the Keras API\n(integrated with TensorFlow 2.x) for its ease of use and the familiarity that a large part\nof the community has developed with it. Furthermore, all our models will be sequential\n(i.e., no skip-connections), making it more intuitive for the competitors to explore the\narchitecture of each network. To remove a potential confounding factor, all models are\ntrained until they reach the interpolation regime (close to 100% accuracy on the training set\n1 or to a ﬁxed cross entropy value). Each model is represented by a JSON ﬁle describing its\narchitecture and a set of trained weights (HDF5 format). We provide the helper functions\nneeded to load a model from those ﬁles.\nThe ﬁrst architecture type we consider is derived from the parameterized architecture\ndescribed in Figure 1a. Due to its similarity to the VGG model [30], we will refer to it\nas VGG-like models. The second connectivity pattern we consider resembles [16] (referred\n1Alternative, we can also consider margin based stopping criterion (i.e. diﬀerence between the highest\nlogit and second highest logit).\n3\n(a) Architecture followed by VGG-like models.\n(b) Architectures for two sets of VGG-like hyper\nparameters.\nFigure 1: Example architectures of the networks in our dataset.\nlater as Network in network which is generally more parameter eﬃcient than VGG models\ndue to the usage of global average pooling yet yields competitive performance.\n• These types of models are well represented in the literature. Inspired by the seminal\nwork of [30], they made their way into an important body of theoretical research\nin deep learning, see for instance [8, 17, 12]; however, the exact nature of their\ngeneralization is still not well understood.\n• Empirically, these models reach the interpolating regime quite easily for a large num-\nber of optimizers / regularization, while exhibiting a large range of out of sample\naccuracy (sometime up to 20% of test accuracy diﬀerence, for two model with the\nsame architecture and regularization, both reaching 100% training accuracy, depend-\ning on the optimizer / learning rate / batch size). In other words, we select these\narchitectures as they often exhibit large generalization gaps and are thus well-suited\nfor this competition.\n• These architectures keep the number of hyper-parameters reasonable while exploring\n4\na large range of neural network types (fully connected or convolutional, deep or\nshallow, with convolutional / dense bottlenecks or not), although future iterations of\nthe competition may include arbitrary type of computational graph.\nIt is worth noting that the optimization method chosen will not be visible to the com-\npetitor (i.e. dropout layers will be removed and optimizer is not an attribute of the model),\nalthough certain aspects of the hyperparameters can be inferred from the models (e.g.\ndepth or width).\n1.3.1\nPublic and private tasks\nThe competition is composed of the the following phases:\n• Phase 0: Public data was given to the competitors: they were able to download Task 1\nand Task 2 to measure the performance of their metrics locally, before uploading\nsubmissions to our servers.\n• Phase 1: First online leaderboard, accessible at the beginning of the competion (also\ncalled public leaderboard). This leaderboard is composed of Task 4 and Task 5 and\nwas used to compute the scores displayed on the leaderboard for the ﬁrst phase of\nthe competition. There was no Task 3 released in this competition, but we keep the\noriginal numbering of the tasks to avoid any confusion.\n• Phase 2: Private leaderboard, only accessible in the last phase of the competition,\nwhere competitors can upload their very best metrics. Winners are determined only\non their score on this leaderboard (to prevent overﬁtting of the public leaderboard,\nas usual). This phase is composed of Task 6, Task 7, Task 8, and Task 9.\nTask 1\n• Model:\nVGG-like models, with 2 or 6 convolutional layers [conv-relu-conv-relu-\nmaxpool] x 1 or x 3. One or two dense layers of 128 units on top of the model.\nWhen dropout is used, it is added after each dense layer.\n• Dataset: CIFAR-10 [15](10 classes, 3 channels).\n• Training: Trained for at most 1200 epochs, learning rate is multiplied by 0.2 after 300,\n600 and 900 epochs. Cross entropy and SGD with momentum 0.9. Initial learning\nrate of 0.001\n• Hparams: Number of ﬁlters of the last convolutional layer in [256, 512]. Dropout\nprobability in [0, 0.5]. Number of convolutional blocks in [1, 3]. Number of dense\nlayers (excluding the output layer) in [1, 2]. Weight decay in [0.0, 0.001]. Batch size\nin [8, 32, 512].\n5\nTask 2\n• Model: Network in Network. When dropout is used, it is added at the end of each\nblock.\n• Dataset: SVHN [20] (10 classes, 3 channels)\n• Training: Trained for at most 1200 epochs, learning rate is multiplied by 0.2 after 300,\n600 and 900 epochs. Cross entropy and SGD with momentum 0.9. Initial learning\nrate of 0.01.\n• Hparams: Number of convolutional layers in [6, 9, 12], dropout probability in [0.0,\n0.25, 0.5], weight decay in [0.0, 0.001], batch size in [32, 512, 1024].\nTask 4\n• Model: Fully convolutional with no downsampling. Global average pooling at the\nend of the model. Batch normalization (pre-relu) on top of each convolutional layer.\n• Dataset: CINIC-10 [3] (random subset of 40% of the original dataset).\n• Training: Trained for at most 3000 epochs, learning rate is multiplied by 0.2 after\n1800, 2200 and 2400 epochs. Initial learning rate of 0.001.\n• Hparams: Number of parameters in [1M, 2.5M], Depth [4 or 6 conv layers], Reversed\n[True of False]. If False, deeper layers have more ﬁlters. If True, this is reversed and\nthe layers closer to the input have more ﬁlters. Weight decay in [0, 0.0005]. Learning\nrate in [0.01, 0.001]. Batch size in [32, 256].\nTask 5\nIdentical to Task 4 but without batch normalization.\nTask 6\n• Model: Network in networks.\n• Dataset: Oxford Flowers [28](102 classes, 3 channels), downsized to 32 pixels.\n• Training: Trained for 10000 epochs maximum, learning rate is multiplied by 0.2\nafter 4000, 6000, and 8000 epochs. Initial learning rate: 0.01. During training, we\nrandomly apply data augmentation (the cifar10 policy from AutoAugment) to half\nthe examples.\n• Hparams: Weight decay in [0.0, 0.001], batch size in [512, 1024], number of ﬁlters in\nconvolutional layer in [256, 512], number of convolutional layers in [6, 9, 12], dropout\nprobability in [0.0, 0.25], learning rate in [0.1, 0.01].\n6\nTask 7\n• Model: Network in networks, with dense layer added on top on the global average\npooling layer. Example: For a NiN with 256-256 dense, the global average pooling\nlayer will have an output size of 256, and another dense layer of 256 units is added\non top of it before the output layer.\n• Dataset: Oxford pets [29] (37 classes, downsized to 32 pixels, 4 channels). During\ntraining, we randomly apply data augmentation (the CIFAR-10 policy from Au-\ntoAugment) to half the examples.\n• Training: Trained for at most 5000 epochs, learning rate is multiplied by 0.2 after\n2000, 3000 and 4000 epochs. Initial learning rate of 0.1.\n• Hparams: Depth in [6, 9], dropout probability in [0.0, 0.25], weight decay in [0.0,\n0.001], batch size in [512, 1024], dense architecture in [128-128-128, 256-256, 512]\nTask 8\n• Model: VGG-like models (same as in task1) with one hidden layer of 128 units.\n• Dataset: Fashion MNIST [33] (28x28 pixels, one channel).\n• Training: Trained for at most 1800 epochs, learning rate is multiplied by 0.2 after\n300, 600 and 900 epochs.\nInitial learning rate of 0.001.\nWeight decay of 0.0001\napplied to all models.\n• Hparams: Number of ﬁlters of the last convolutional layer in [256, 512]. Dropout\nprobability in [0, 0.5]. Number of convolutional blocks in [1, 3]. Learning rate in\n[0.001, 0.01]. Batch size in [32, 512].\nTask 9\n• Model: Network in Network.\n• Dataset: CIFAR-10, with the standard data augmentation (random horizontal ﬂips\nand random crops after padding by 4 pixels.\n• Training: Trained for at most 1200 epochs, learning rate is multiplied by 0.2 after 300,\n600 and 900 epochs. Cross entropy and SGD with momentum 0.9. Initial learning\nrate of 0.01.\n• Hparams: Number of ﬁlters in the convolutional layers in [256, 512], Number of\nconvolutional layers in [9, 12], dropout probability in [0.0, 0.25], weight decay in [0.0,\n0.001], batch size in [32, 512].\n7\n1.4\nTasks and application scenarios\nIn this competition, each dataset D has a set of training data Dtrain = {(xi, yi)}Nt\ni=i and\na set of validation data Dval = {(xi, yi)}Nv\ni=i. Further, it has a set of parameterized mod-\nels uniquely identiﬁed by the models’ hyperparameters, Θ = {θ0, . . . , θn}. Each hyper-\nparameter produces one set of model weights2.\nThe set of weights produced by Θ is\nW = {w0, . . . , wn} where wi represents parameters of the model with θi trained on Dtrain.\nWe further denote the resulting model to be fwi and use fwi(xj) as the prediction of xj.\nThe generalization gap of the model is formally deﬁned as\ng(fw; D) =\n1\n|Dval|\nX\n(x,y)∈Dval\n1(fw(x) ̸= y) −\n1\n|Dtrain|\nX\n(x,y)∈Dtrain\n1(fw(x) ̸= y),\n(1)\nwhere 1 is the indicator function. A complexity measure µ : (fw, Dval) →R maps the\nmodel and dataset to a real number. The task is to ﬁnd a complexity measure µ such that:\nsgn(µ(w; Dtrain) −µ(w′; Dtrain)) = sgn(g(w; D) −g(w′; D))\n∀(w, w′) ∈W × W\n(2)\nInformally, the function µ should order the models in the same way that the generalization\ngap does. Further, for notation simplicity the dependency on Dval will be omitted unless\ndiscussing about multiple datasets.\nIn statistical learning theory, µ is often the upperbound of the generalization error\na function can make; however, such complexity measure is often much larger than the\nadmissible error made by deep neural networks with large number of parameters, rendering\nthem vacuous. In these cases, these complexity measures can still be informative so long\nas they provide comparison between diﬀerent models. Beyond the theoretical interests and\nusage in model selections, these complexity measures can also be instrumental in Neural\nArchitecture Search (NAS) by alleviating the need of having a validation dataset. This can\nbe critical in regimes where data are scarce and using valuable data for model selection\nis sub-optimal. Finally, if the complexity measure is fully diﬀerentiable, it may act as a\nregularizer to improve generalization.\nThe largest challenge of designing such complexity measure measure is making it robust\nto changes in the model architectures and the dataset used for training the model. Many\ncomplexity measures only work on a single dataset or only correlates with one particular\ntype of hyper-parameter change (e.g. depth). While such a generic complexity measure\nmay seem to be diﬃcult to obtain, recent work [14] has proposed rigorous protocols for\nidentifying promising complexity measure, and shows that it is possible to ﬁnd complexity\nmeasures that fulﬁll the above criteria.\n2Due to the stochasticity in SGD or in the initialization, the same set of hyperparameters can yield\nmodels that generalize diﬀerently. However, a good generalization measure should also be able to rank\nbetween these models too.\n8\n1.5\nMetrics\nIn this competition, each dataset has a set of hyperparameters that are adjusted to create\ndiﬀerent models. Formally, we denote each hyperparameter by θi taking values from the\nset Θi, for i = 1, . . . , nH and nH denoting the total number of hyperparameter types. Each\nvalue of hyperparameters θ ≜(θ1, θ2, . . . , θnH) ∈Θ is drawn from Θ ≜Θ1×Θ2×· · ·×ΘnH.\nFor any pair of (θ, θ′) ∈Θ × Θ, we deﬁne:\nVφ(θ, θ′) ≜sgn(φ(θ) −φ(θ′))\n(3)\nThen the Kendall’s ranking correlation between a measure and generalization gap is deﬁned\nas:\nτ(µ, Θ) ≜\n1\n|Θ|(|Θ| −1)\nX\nθ1\nX\nθ2̸=θ1\nVµ(θ1, θ2)Vg(θ1, θ2)\n(4)\n1.5.1\nMetric 1: Controlled Ranking Correlation\nTo ensure that the ranking correlation is good at capturing changes in every single hyper-\nparameter; therefore, we design the ﬁrst metric to reﬂect the measure ability to predict\nchanges in generalization gap as the result of changes in a single hyperparameter:\nmi ≜|Θ1 × · · · × Θi−1 × Θi+1 × · · · × ΘnH|\n(5)\nψ(µ, i) ≜1\nmi\nX\nθ1∈Θ1\n· · ·\nX\nθi−1∈Θi−1\nX\nθi+1∈Θi+1\n· · ·\nX\nθnH ∈ΘnH\nτ (µ, ∪θi∈Θi{(θ1, . . . , θi, . . . , θnH)})\n(6)\nThe inner τ reﬂects the ranking correlation between the generalization and the complexity measure\nfor a small group of models where the only diﬀerence among them is the variation along a single\nhyperparameter θi. We then average the value across all combinations of the other hyperparameter\naxis. Intuitively, if a measure is good at predicting the eﬀect of hyperparameter θi over the model\ndistribution, then its corresponding ψi should be high. Finally, we compute the average ψi across\nall hyperparamter axes, and name it Ψ\nΨ(µ) ≜\n1\nnH\nnH\nX\ni=1\nψ(µ, i)\n(7)\nΨ is implicitly dependent on the dataset so we will compute the average value across all\ndatasets:\nMetric1(µ) =\nX\nD\nΨ(µ; D)\n(8)\nNote: This metric is only provided for eﬃcient computation since the true metric can be\nexpensive to compute. All ranking will be done using Metric 2 below. In most cases we\nhave tested, metric 1 and metric 2 correlate highly with each other, but metric 2 is the\nmore principled measurement.\n9\n1.5.2\nMetric 2: Conditional Mutual Information\nTo ensure that a measure is causally informative of generalization, we use a special instance\nof the Inductive Causation (IC) algorithm to measure whether an edge exists between the\ncomplexity measure and observed generalization in a causal probabilistic graph. Speciﬁ-\ncally, we measure how informative is the complexity measure about generalization when\none or more hyper-parameters is observed.\nConcretely, we denote O as the set of hyper-parameters being conditioned on. For\ninstance, if O = {∅}, then |O| = 0 the conditional mutual information measures how\ninformative the measure is about generalization in general; if O = {learning rate, depth},\nthen |O| = 2 the conditional mutual information measures how informative the measure\nis about generalization when we already know the learning rate and the depth of the\nmodel. For a particular O, we can partition all the models into groups based on their\nvalues at the members of O. For O = {Θi}N\ni=1, the groups are models are QN\ni=1 Θi. As\na concrete example, suppose O = {learning rate, depth} and there are 2 possible learning\nrates, {0.1, 0.01}, and 2 depths, {8, 16}, then there will be 4 groups and models within\neach group will have the same learning rate and depth.\nWe further treat Vµ and Vg as Bernoulli random variables by counting over groups of\nmodels. On a particular group Ok, we can compute:\np(Vg|Ok),\np(Vµ|Ok),\np(Vg, Vµ|Ok)\n(9)\nThese probabilities be easily obtained by counting over the models within Ok, which further\nallows us to compute the mutual information between Vµ and Vg conditioned on Ok:\nI(Vg, Vµ | Ok) =\nX\nVg\nX\nVµ\np(Vg, Vµ | Ok) log\n\u0010\np(Vµ, Vg | Ok)\np(Vµ | Ok) p(Vg | Ok)\n\u0011\n(10)\nSince each Ok occurs with equal probability of pc = 1 / QN\ni=1 |Θi|, with slight abuse of\nnotation, we can compute the mutual information between Vµ and Vg conditioned on that\nvalues of O is observed as follows:\nI(Vg, Vµ | O) =\nX\nOk\npc I(Vg, Vµ | Ok)\n(11)\nSince here the conditional mutual information between a complexity measure and general-\nization is at most equal to the conditional entropy of generalization, we normalize it with\nthe conditional entropy to arrive at a criterion ranging between 0 and 1. The conditional\nentropy of generalization is computed as follows:\nH(Vg | O) =\nX\nOk\npc\nX\nVg\np(Vg | Ok) log\n\u0000p(Vg | Ok)\n\u0001\n(12)\n10\nˆI(Vg, Vµ | O) = I(Vg, Vµ | O)\nH(Vg | O)\n(13)\nFinally, by the IC algorithm3, we take the minimum over all possible O and our ﬁnal metric\nis the follows:\nJ (µ) ≜min\nO\nˆI(Vg, Vµ | O)\n(14)\nSimilar to Ψ, J (µ) is implicitly dependent on the dataset so we will compute the average\nvalue across all datasets:\nMetric2(µ) =\nX\nD\nJ (µ; D)\n(15)\nThis metric is more principled than metric 1 and it is the only metric for ranking the\nsubmissions.\n1.5.3\nHuman Evaluation\nSince the competition format is very new, we reserve the rights to inspect the submitted\ncode for abuse (e.g., using the provided compute for tasks unrelated to the competition or\ntempting with the competition server). We expect the need for human evaluation to be\nrare.\n1.6\nBaselines, code, and material provided\nWe will be providing baselines in the form of 2 diﬀerent measures. First measure is the\nVC-dimension of the models and the second measure is the true generalization gap of the\nmodels with added noises. The VC-dimension of convolutional neural networks can be\nfound in [14]. The former is meant to be a weak baseline from classical machine learning\nliterature, and the latter is meant to be a strong baseline, which we expect few solution to\nbeat since it is essentially a noisy version of the true quantity of interest.\nWe will be providing code providing an example measure as well as demonstrating how\nto access various attribute of the models and how to compute potential values of interest\nsuch as norms of the weights or the gradients. Depending on the the feedbacks from the\ncommunity, we may also add new baselines.\n1.7\nTutorial and documentation\nThe inspiration and much backbone of the this competition can be found in [14], which\nidentiﬁes various challenges of evaluating generalization and outlines the rigorous proce-\ndure to demonstrate the eﬀectiveness of certain generalization measure. The procedure\nis modiﬁed and reproduce in the metric shown above. We will also prepare a tutorial on\n3For computational practicality, we will often restrict the maximum number of hyper-parameters in O.\n11\ngeneralization in deep learning for participants who are not as familiar with the ﬁeld, and\nprovide a list of reference for further readings. We will also be providing the API of the\nsoftware used in the competition.\n2\nOrganizational aspects\n2.1\nProtocol\nThe competition will be separated into two phases. In development phase (phase 1)\nof the competition, the competitors will develop solutions on the public data that we\nprovide, and submit solutions which we will evaluate on the Phase 1 private dataset. After\nevaluation phase (phase 2) starts, the competitors are expected to submit their ﬁnal\nsolution. The ﬁnal solution will be evaluated on Phase 1 data ﬁrst to check if it ﬁnishes\nwithin time. If the solution ﬁnishes on Phase 1 within time then their code will be run\non the Phase 2 private dataset without any time limit. Both private datasets would be\nmodels trained on diﬀerent hyper-parameters from the public dataset. The competitors are\nexpected to submit code which we will evaluate on the cloud. We will be using Codalab\nto orchestrate the online submissions and provide a live leader board. At NeurIPS, we will\nbe organizing a workshop for top-performing teams to present their solutions. There will\nbe posters and also oral presentation. We also plan to invite guest speakers. The details\nof the workshop is still being ﬁnalized.\n2.2\nRules\n2.2.1\nDraft of Rules\n1. Participants are expected to form teams. There are no limits on number of partici-\npants on each team.\n2. Each participant can only be on one team. Violation may result in disqualiﬁcation.\n3. Since we may use publicly available dataset, including the datasets in the submission\nfor any form of direct lookup is not permitted. Violation may result in disqualiﬁca-\ntion.\n4. Each team need to submit a academic-paper-style write up that describes their solu-\ntion to be eligible for winning in the evaluation phase.\n5. Top eligible teams will be invited to give a presentation at NeurIPS 2020.\n6. All submission in Phase 1 are required to ﬁnish executing in a ﬁxed time limit;\nsubmissions that exceed the computational limit will receive minimum score (0.0).\nWe will announce the time limit soon, but a submission should on average be able\n12\nto process a model within 5 minutes wall-clock time on GPU. Hardwares specs are\ncurrently in preparation.\n7. Only one submission is allowed in Phase 2 for ﬁnal scoring.\n• The submission will be ﬁrst run on the Phase 1 data. If the submission times\nout or fails on the Phase 1 data, then it will not be evaluated on Phase 2 data;\nhowever, the competitors will be allowed to resubmit changed version of the\ncode.\n• If the submission ﬁnishes running within the time limit, the submission will be\nrun on Phase 2 datasets without time limit. This process can only be done\nonce, after which any further submission from the team is not evaluated.\n8. Computation resource is allocated on a ﬁrst-come-ﬁrst-serve basis.\nA maximum\nsubmissions of 5 is allowed per day for each team. This number is subject to change.\n9. Competitors aﬃliated with Alphabet and co-organizers are not eligible for winning\nbut participation is allowed.\n2.2.2\nDiscussion\nOur primary interest is to provide a platform for the community to test out new theories to\nimprove the understanding of why deep neural networks generalize, and rigorously analyze\nhow much these theories reﬂect the real models. By asking for a write-up on the theoretical\nmotivation behind the proposed complexity measures, we hope to motivate the competitors\nto build their solutions in a more principled manner. However, solutions that use paramet-\nric solutions or black-box methods are also valuable since they tend to be more expressive\nand powerful. In this case, a write-up will also help practitioners in the community.\nWe are providing the compute for the evaluation, which both prevent cheating since the\ncompetitors do not have access to the data and lower the barrier-to-entry for participants\nwho may not have access to large amounts of compute resources. The reason for limiting\nsubmissions per day is two-fold:\n• It is unclear if it is possible to overﬁt to the private dataset since the competition\nformat is extremely new.\n• We need to restrict the compute for practicality since we are providing free compute\nand we want to prevent abuse. We will also adjust the time budget based on user\nfeedback.\nCurrent proposal of the competition only includes a set of sequential feed-forward mod-\nels trained to small loss on several image classiﬁcation benchmarks. In the future iteration\nof this competition, we plan to support more classes of computational graph (e.g. ResNet)\nand at diﬀerent loss values. We are also considering including tracks for transfer learning.\n13\n2.2.3\nCheating Prevention\nThe private models and data that will be used in the competitions will be created from\nscratch so the competitors will not be able to ﬁnd them online. Further, the submission will\nnot be able to access the internet while the code is running, which prevents the information\nof the dataset from leaking. We also put compute time limits on the submissions. Solutions\nthat exceed the time limit will receive the minimum score of 0.0. We are only allowing a\nsmall number of submissions everyday to minimize the possibility of reverse engineering.\nFinally, as the last resort, if we observe unusual behavior, we will manually inspect the\ncompetitors’ source code.\n3\nTimeline\n• Jul 15: Phase 1 starts. Participants submit their solution to be evaluated on the\nPhase 1 private dataset.\n• Oct 08: Phase 2 begins. Participants’ code are evaluated on the Phase 2 private\ndataset.\n• Oct 24: Phase 2 ends. All computation ﬁnalized.\n• Oct 31: Results are announced.\n• Dec 11: Winning teams are invited to present at the conference\n4\nOrganizing team\n• Yiding Jiang: Yiding Jiang is an AI resident at Google Research. He previously\nreceived Bachelor of Science in Electrical Engineering and Computer Science from\nUniversity of California, Berkeley. He has worked on projects in deep learning, re-\ninforcement learning and robotics. He has published papers related to predicting\ngeneralization of neural networks [13, 31] and evaluating complexity measures [14].\n• Pierre Foret: Pierre Foret is an AI resident at Google Research. He previously\nreceived a Master of Financial Engineering from University of California, Berkeley\nand a Master in applied math from ENSAE Paristech. His research interests lie in\nthe intersection of optimization and generalization in deep learning.\n• Scott Yak: Scott Yak is a Software Engineer at Google Research. He has previ-\nously received a Bachelor of Science and Engineering at Princeton University. He is\ncurrently working on AutoML and Neural Architecture Search at Google. He has\npublished work on predicting generalization of neural networks [34].\n14\n• Behnam Neyshabur: Behnam Neyshabur is a senior research scientist at Google.\nBefore that, he was a postdoctoral researcher at New York University and a member\nof Theoretical Machine Learning program at Institute for Advanced Study (IAS)\nin Princeton.\nIn summer 2017, He received a PhD in computer science at TTI-\nChicago.\nHe is interested in machine learning and optimization and his primary\nresearch is on optimization and generalization in deep learning. He has co-organized\nICML 2019 workshops on “Understanding and Improving Generalization in Deep\nLearning” and “Identifying and Understanding Deep Learning Phenomen”. He has\npublished several papers related to complexity measures and generalization in deep\nlearning [14, 31, 2, 26, 27, 25, 22, 23, 21, 1, 24]\n• Hossein Mobahi: Hossein Mobahi is a research scientist at Google Research. His\nrecent eﬀorts covers the intersection of machine learning, generalization and opti-\nmization, with emphasis on deep learning. Prior to joining Google in 2016, he was\na postdoctoral researcher in the Computer Science and Artiﬁcial Intelligence Lab\n(CSAIL) at MIT. He obtained his PhD in Computer Science from the University of\nIllinois at Urbana-Champaign (UIUC). He is the recipient of Computational Science\n& Engineering Fellowship, Cognitive Science & AI Award, and Mavis Memorial Schol-\narship. He has published several works on generalization [14, 13, 7] and theoretical\nfoundations of self-distillation [18].\n• Gintare Karolina Dziugaite: Gintare Karolina Dziugaite is a Fundamental Re-\nsearch Scientist at Element AI. Dziugaite recently graduated from the University\nof Cambridge, where she completed her doctorate in Zoubin Ghahramani’s machine\nlearning group. The focus of her thesis was on constructing generalization bounds to\nunderstand existing learning algorithms in deep learning and propose new ones. She\ncontinues to work on explaining generalization phenomenon in deep learning using\nstatistical learning tools. She was a lead organizer for the 2019 ICML workshop on\nMachine Learning with Guarantees. She has published several works in generalization\nfor deep learning [5, 6, 19, 4].\n• Daniel M. Roy: Daniel M. Roy is an Assistant Professor in the Department of\nStatistical Sciences at the University of Toronto and Canada CIFAR AI Chair. Prior\nto joining Toronto, Roy was a Research Fellow of Emmanuel College and Newton\nInternational Fellow of the Royal Society and Royal Academy of Engineering, hosted\nby the University of Cambridge. Roy completed his doctorate in Computer Science\nat the Massachusetts Institute of Technology.\nRoy has co-organized a number of\nworkshops, including the 2008, 2012, and 2014 NeurIPS Workshops on Probabilistic\nProgramming, a 2016 Simons Institute Workshop on Uncertainty in Computation,\nand special sessions in 2019 at the Statistical Society of Canada meeting and 2016\nat the Mathematical Foundations of Programming Semantics conference. Last year\nat ICML, he organized a workshop on Machine Learning with Guarantees. He has\n15\npublished several works in generalization for deep learning [35, 5, 6, 19].\n• Suriya Gunasekar: Suriya Gunasekar is a senior researcher at the Machine Learning\nand Optimization (MLO) Group of Microsoft Research. Prior to joining MSR, she\nwas a Research Assistant Professor at Toyota Technological Institute at Chicago. She\nreceived my PhD in ECE from The University of Texas at Austin. She has published\nseveral works in optimization and implicit regularization [32, 11, 10, 9].\n• Isabelle Guyon: Isabelle Guyon is chaired professor in “big data” at the Universit´e\nParis-Saclay, specialized in statistical data analysis, pattern recognition and machine\nlearning. She is one of the cofounders of the ChaLearn Looking at People (LAP)\nchallenge series and she pioneered applications of the MIcrosoft Kinect to gesture\nrecognition. Her areas of expertise include computer vision and and bioinformat-\nics. Prior to joining ParisSaclay she worked as an independent consultant and was\na researcher at AT&T Bell Laboratories, where she pioneered applications of neural\nnetworks to pen computer interfaces (with collaborators including Yann LeCun and\nYoshua Bengio) and coinvented with Bernhard Boser and Vladimir Vapnik Support\nVector Machines (SVM), which became a textbook machine learning method. She\nworked on early applications of Convolutional Neural Networks (CNN) to handwrit-\ning recognition in the 1990’s. She is also the primary inventor of SVMRFE, a variable\nselection technique based on SVM. The SVMRFE paper has thousands of citations\nand is often used as a reference method against which new feature selection meth-\nods are benchmarked. She also authored a seminal paper on feature selection that\nreceived thousands of citations. She organized many challenges in Machine Learning\nsince 2003 supported by the EU network Pascal2, NSF, and DARPA, with prizes\nsponsored by Microsoft, Google, Facebook, Amazon, Disney Research, and Texas\nInstrument. Isabelle Guyon holds a Ph.D. degree in Physical Sciences of the Univer-\nsity Pierre and Marie Curie, Paris, France. She is president of Chalearn, a nonproﬁt\ndedicated to organizing challenges, vice president of the Unipen foundation, adjunct\nprofessor at NewYork University, action editor of the Journal of Machine Learning\nResearch, editor of the Challenges in Machine Learning book series of Microtome,\nand program chair of the upcoming NIPS 2016 conference.\n• Samy Bengio: Samy Bengio (PhD in computer science, University of Montreal,\n1993) is a research scientist at Google since 2007.\nHe currently leads a group of\nresearch scientists in the Google Brain team, conducting research in many areas of\nmachine learning such as deep architectures, representation learning, sequence pro-\ncessing, speech recognition, image understanding, large-scale problems, adversarial\nsettings, etc. He was the general chair for Neural Information Processing Systems\n(NeurIPS) 2018, the main conference venue for machine learning, was the program\nchair for NeurIPS in 2017, is action editor of the Journal of Machine Learning Re-\nsearch and on the editorial board of the Machine Learning Journal, was program\n16\nchair of the International Conference on Learning Representations (ICLR 2015, 2016),\ngeneral chair of BayLearn (2012-2015) and the Workshops on Machine Learning for\nMultimodal Interactions (MLMI’2004-2006), as well as the IEEE Workshop on Neu-\nral Networks for Signal Processing (NNSP’2002), and on the program committee of\nseveral international conferences such as NIPS, ICML, ICLR, ECML and IJCAI.\n5\nResources\n• Community We will facilitate a public forum where the organizers interact with\nthe participants and answer any potential questions. We hope the platform would\nreduce communication friction and also foster a community among the competitors.\n• Computing Resources We will provide all the computational resources needed for\nevaluating the complexity measures. We will also be providing learning resources for\nthose not as knowledgeable about statistical learning theory and generalization in\ndeep learning.\n• Prize None.\nReferences\n[1] Arora, S., Ge, R., Neyshabur, B., and Zhang (Alphabetical Order), Y.\nStronger generalization bounds for deep nets via a compression approach. In Pro-\nceedings of the 35th International Conference on Machine Learning (ICML) (2018),\npp. 254–263.\n[2] Chatterji, N. S., Neyshabur, B., and Sedghi, H. The intriguing role of module\ncriticality in the generalization of deep networks.\nIn International Conference on\nLearning Representations (ICLR) (2020 (spotlight)).\n[3] Darlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. Cinic-10\nis not imagenet or cifar-10, 2018.\n[4] Dziugaite, G. K. Revisiting Generalization for Deep Learning: PAC-Bayes, Flat\nMinima, and Generative Models. PhD thesis, University of Cambridge, 2020.\n[5] Dziugaite, G. K., and Roy, D. M. Computing nonvacuous generalization bounds\nfor deep (stochastic) neural networks with many more parameters than training data.\narXiv preprint arXiv:1703.11008 (2017).\n[6] Dziugaite, G. K., and Roy, D. M. Data-dependent pac-bayes priors via diﬀerential\nprivacy. In Advances in Neural Information Processing Systems (2018), pp. 8430–8441.\n17\n[7] Elsayed, G., Krishnan, D., Mobahi, H., Regan, K., and Bengio, S. Large\nmargin deep networks for classiﬁcation. In Advances in neural information processing\nsystems (2018), pp. 842–852.\n[8] Frankle, J., and Carbin, M. The Lottery Ticket Hypothesis: Finding Sparse,\nTrainable Neural Networks. arXiv e-prints (Mar 2018), arXiv:1803.03635.\n[9] Gunasekar, S., Lee, J., Soudry, D., and Srebro, N. Characterizing implicit\nbias in terms of optimization geometry. arXiv preprint arXiv:1802.08246 (2018).\n[10] Gunasekar, S., Lee, J. D., Soudry, D., and Srebro, N. Implicit bias of gra-\ndient descent on linear convolutional networks. In Advances in Neural Information\nProcessing Systems (2018), pp. 9461–9471.\n[11] Gunasekar, S., Woodworth, B. E., Bhojanapalli, S., Neyshabur, B., and\nSrebro, N. Implicit regularization in matrix factorization. In Advances in Neural\nInformation Processing Systems (2017), pp. 6151–6159.\n[12] Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both Weights and Con-\nnections for Eﬃcient Neural Networks. arXiv e-prints (Jun 2015), arXiv:1506.02626.\n[13] Jiang, Y., Krishnan, D., Mobahi, H., and Bengio, S. Predicting the generaliza-\ntion gap in deep networks with margin distributions. arXiv preprint arXiv:1810.00113\n(2018).\n[14] Jiang, Y., Neyshabur, B., Krishnan, D., Mobahi, H., and Bengio, S. Fan-\ntastic generalization measures and where to ﬁnd them. In International Conference\non Learning Representations (2019).\n[15] Krizhevsky, A. Learning multiple layers of features from tiny images. Tech. rep.,\n2009.\n[16] Lin,\nM.,\nChen,\nQ.,\nand Yan,\nS.\nNetwork in network.\narXiv preprint\narXiv:1312.4400 (2013).\n[17] Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Rethinking the Value\nof Network Pruning. arXiv e-prints (Oct 2018), arXiv:1810.05270.\n[18] Mobahi, H., Farajtabar, M., and Bartlett, P. L. Self-distillation ampliﬁes\nregularization in hilbert space. arXiv preprint arXiv:2002.05715 (2020).\n[19] Negrea, J., Haghifam, M., Dziugaite, G. K., Khisti, A., and Roy, D. M.\nInformation-theoretic generalization bounds for sgld via data-dependent estimates. In\nAdvances in Neural Information Processing Systems (2019), pp. 11013–11023.\n18\n[20] Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y.\nReading digits in natural images with unsupervised feature learning.\n[21] Neyshabur, B. Implicit Regularization in Deep Learning. PhD thesis, TTIC, 2017.\n[22] Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. Exploring\ngeneralization in deep learning. In Advances in Neural Information Processing Systems\n(NIPS) (2017), pp. 5947–5956.\n[23] Neyshabur, B., Bhojanapalli, S., and Srebro, N. A pac-bayesian approach to\nspectrally-normalized margin bounds for neural networks. In International Conference\non Learning Representations (ICLR) (2018).\n[24] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. Towards\nunderstanding the role of over-parametrization in generalization of neural networks.\nIn International Conference on Learning Representations (ICLR) (2019).\n[25] Neyshabur, B., Tomioka, R., Salakhutdinov, R., and Srebro, N. Geom-\netry of optimization and implicit regularization in deep learning.\narXiv preprint\narXiv:1705.03071 (2017).\n[26] Neyshabur, B., Tomioka, R., and Srebro, N. In search of the real inductive bias:\nOn the role of implicit regularization in deep learning. In International Conference on\nLearning Representations (ICLR) workshop (2015).\n[27] Neyshabur, B., Tomioka, R., and Srebro, N. Norm-based capacity control in\nneural networks. In Conference on Learning Theory (COLT) (2015), pp. 1376–1401.\n[28] Nilsback, M.-E., and Zisserman, A. Automated ﬂower classiﬁcation over a large\nnumber of classes.\nIn Proceedings of the Indian Conference on Computer Vision,\nGraphics and Image Processing (Dec 2008).\n[29] Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and\ndogs. In IEEE Conference on Computer Vision and Pattern Recognition (2012).\n[30] Simonyan, K., and Zisserman, A. Very deep convolutional networks for large-scale\nimage recognition. In International Conference on Learning Representations (2015).\n[31] Song, X., Jiang, Y., Du, Y., and Neyshabur, B.\nObservational overﬁtting\nin reinforcement learning. In International Conference on Learning Representations\n(ICLR) (2020).\n[32] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. The\nimplicit bias of gradient descent on separable data. The Journal of Machine Learning\nResearch 19, 1 (2018), 2822–2878.\n19\n[33] Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for\nbenchmarking machine learning algorithms. CoRR abs/1708.07747 (2017).\n[34] Yak, S., Gonzalvo, J., and Mazzawi, H.\nTowards task and architecture-\nindependent generalization gap predictors. arXiv preprint arXiv:1906.01550 (2019).\n[35] Yang, J., Sun, S., and Roy, D. M. Fast-rate pac-bayes generalization bounds via\nshifted rademacher processes. In Advances in Neural Information Processing Systems\n(2019), pp. 10802–10812.\n20\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-12-14",
  "updated": "2020-12-14"
}