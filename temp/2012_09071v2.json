{
  "id": "http://arxiv.org/abs/2012.09071v2",
  "title": "Joint Generative and Contrastive Learning for Unsupervised Person Re-identification",
  "authors": [
    "Hao Chen",
    "Yaohui Wang",
    "Benoit Lagadec",
    "Antitza Dantcheva",
    "Francois Bremond"
  ],
  "abstract": "Recent self-supervised contrastive learning provides an effective approach\nfor unsupervised person re-identification (ReID) by learning invariance from\ndifferent views (transformed versions) of an input. In this paper, we\nincorporate a Generative Adversarial Network (GAN) and a contrastive learning\nmodule into one joint training framework. While the GAN provides online data\naugmentation for contrastive learning, the contrastive module learns\nview-invariant features for generation. In this context, we propose a\nmesh-based view generator. Specifically, mesh projections serve as references\ntowards generating novel views of a person. In addition, we propose a\nview-invariant loss to facilitate contrastive learning between original and\ngenerated views. Deviating from previous GAN-based unsupervised ReID methods\ninvolving domain adaptation, we do not rely on a labeled source dataset, which\nmakes our method more flexible. Extensive experimental results show that our\nmethod significantly outperforms state-of-the-art methods under both, fully\nunsupervised and unsupervised domain adaptive settings on several large scale\nReID datsets.",
  "text": "Joint Generative and Contrastive Learning for Unsupervised Person\nRe-identiﬁcation\nHao Chen1,2,3*\nYaohui Wang1,2*\nBenoit Lagadec3\nAntitza Dantcheva1,2\nFrancois Bremond1,2\n1Inria\n2Universit´e Cˆote d’Azur\n3European Systems Integration\n{hao.chen, yaohui.wang, antitza.dantcheva, francois.bremond}@inria.fr\nbenoit.lagadec@esifrance.net\nAbstract\nRecent self-supervised contrastive learning provides an\neffective approach for unsupervised person re-identiﬁcation\n(ReID) by learning invariance from different views (trans-\nformed versions) of an input. In this paper, we incorporate\na Generative Adversarial Network (GAN) and a contrastive\nlearning module into one joint training framework. While\nthe GAN provides online data augmentation for contrastive\nlearning, the contrastive module learns view-invariant fea-\ntures for generation. In this context, we propose a mesh-\nbased view generator. Speciﬁcally, mesh projections serve\nas references towards generating novel views of a per-\nson. In addition, we propose a view-invariant loss to fa-\ncilitate contrastive learning between original and gener-\nated views. Deviating from previous GAN-based unsuper-\nvised ReID methods involving domain adaptation, we do\nnot rely on a labeled source dataset, which makes our\nmethod more ﬂexible. Extensive experimental results show\nthat our method signiﬁcantly outperforms state-of-the-art\nmethods under both, fully unsupervised and unsupervised\ndomain adaptive settings on several large scale ReID dat-\nsets. Source code and models are available under https:\n//github.com/chenhao2345/GCL.\n1. Introduction\nA person re-identiﬁcation (ReID) system is targeted at\nidentifying subjects across different camera views. In par-\nticular, given an image containing a person of interest (as\nquery) and a large set of images (gallery set), a ReID sys-\ntem ranks gallery-images based on visual similarity with\nthe query. Towards this, ReID systems are streamlined to\nbring to the fore discriminative representations, which al-\nlow for robust comparison of query and gallery images. In\nthis context, supervised ReID methods [4, 33] learn rep-\nresentations guided by human-annotated labels, which is\ntime-consuming and cumbersome. Towards omitting such\nhuman annotation, researchers increasingly place empha-\nsis on unsupervised person ReID algorithms [35, 24, 27],\nwhich learn directly from unlabeled images and thus allow\nfor scalability in real world deployments.\n*Equal contribution.\nFigure 1: Left: Traditional self-supervised contrastive learning\nmaximizes agreement between representations (f1 and f2) of aug-\nmented views from Data Augmentation (DA). Right: Joint gener-\native and contrastive learning maximizes agreement between orig-\ninal and generated views.\nRecently, self-supervised contrastive methods [16, 6]\nhave provided an effective retrieval-based approach for\nunsupervised representation learning.\nGiven an image,\nsuch methods maximize agreement between two augmented\nviews of one instance (see Fig. 1). Views refer to trans-\nformed versions of the same input. As shown in very re-\ncent works [6, 7], data augmentation enables a network\nto explore view-invariant features by providing augmented\nviews of a person, which are instrumental in building ro-\nbust representations. Such and similar methods considered\ntraditional data augmentation techniques, e.g., ‘random ﬂip-\nping’, ‘cropping’, and ‘color jittering’. Generative Adver-\nsarial Networks (GANs) [15] constitute a novel approach\nfor data augmentation. As opposed to traditional data aug-\nmentation, GANs are able to modify id-unrelated features\nsubstantially, while preserving id-related features, which is\nhighly beneﬁcial in contrastive ReID.\nPrevious GAN-based methods [1, 9, 55, 25, 41, 50]\nconsidered unsupervised ReID as an unsupervised domain\nadaptation (UDA) problem. Under the UDA setting, re-\nsearchers used both, a labeled source dataset, as well as an\nunlabeled target dataset to gradually adjust a model from a\nsource domain into a target domain. GANs can be used\nin cross-domain style transfer, where labeled source do-\nmain images are generated in the style of a target domain.\nHowever, the UDA setting necessitates a large-scale labeled\nsource dataset.\nScale and quality of the source dataset\n1\narXiv:2012.09071v2  [cs.CV]  30 Mar 2021\nstrongly affect the performance of UDA methods. Recent\nresearch has considered fully unsupervised ReID [35, 24],\nwhere under the fully unsupervised setting, a model directly\nlearns from unlabeled images without any identity labels.\nSelf-supervised contrastive methods [16, 6] belong to this\ncategory. In this work, we use a GAN as a novel view gen-\nerator for contrastive learning, which does not require a la-\nbeled source dataset.\nHere, we aim at enhancing view diversity for contrastive\nlearning via generation under the fully unsupervised set-\nting. Towards this, we introduce a mesh-based novel view\ngenerator.\nWe explore the possibility of disentangling a\nperson image into identity features (color distribution and\nbody shape) and structure features (pose and view-point)\nunder the fully unsupervised ReID setting.\nWe estimate\n3D meshes from unlabeled training images, then rotate\nthese 3D meshes to simulate new structures. Compared to\nskeleton-guided pose transfer [14, 25], which neglects body\nshape, mesh recovery [21] jointly estimates pose and body\nshape. Estimated meshes preserve body shape during the\ntraining, which facilitates the generation and provides more\nvisual clues for ﬁne-grained ReID. Novel views can be gen-\nerated by combining identity features with new structures.\nOnce we obtain the novel views, we design a pseudo la-\nbel based contrastive learning module. With the help of our\nproposed view-invariant loss, we maximize representation\nsimilarity between original and generated views of a same\nperson, whereas representation similarity of other persons\nis minimized.\nOur proposed method incorporates generative and con-\ntrastive modules into one framework, which are trained\njointly. Both modules share the same identity feature en-\ncoder.\nThe generative module disentangles identity and\nstructure features, then generates diversiﬁed novel views.\nThe novel views are then used in the contrastive module\nto improve the capacity of the shared identity feature en-\ncoder, which in turn improves the generation quality. Both\nmodules work in a mutual promotion way, which signiﬁ-\ncantly enhances the performance of the shared identity fea-\nture encoder in unsupervised ReID. Moreover, our method\nis compatible with both UDA and fully unsupervised set-\ntings. With a labeled source dataset, we obtain better per-\nformance by alleviating the pseudo label noise.\nOur contributions can be summarized as follows.\n1. We propose a joint generative and contrastive learn-\ning framework for unsupervised person ReID. Gener-\native and contrastive modules mutually promote each\nother’s performance.\n2. In the generative module, we introduce a 3D mesh\nbased novel view generator, which is more effective\nin body shape preservation than skeleton-guided gen-\nerators.\n3. In the contrastive module, a view-invariant loss is pro-\nposed to reduce intra-class variation between original\nand generated images, which is beneﬁcial in building\nview-invariant representations under a fully unsuper-\nvised ReID setting.\n4. We overcome the limitation of previous GAN-based\nunsupervised ReID methods that strongly rely on a la-\nbeled source dataset.\nOur method signiﬁcantly sur-\npasses the performance of state-of-the-art methods un-\nder both, fully unsupervised, as well as UDA settings.\n2. Related Work\nUnsupervised representation learning.\nRecent con-\ntrastive instance discrimination methods [44, 16, 6] have\nwitnessed a signiﬁcant progress in unsupervised represen-\ntation learning. The basic idea of instance discrimination\nhas to do with the assumption that each image is a single\nclass. Contrastive predictive coding (CPC) [30] included\nan InfoNCE loss to measure the ability of a model to clas-\nsify positive representation amongst a set of unrelated neg-\native samples, which has been commonly used in following\nworks on contrastive learning. Recent contrastive methods\ntreated unsupervised representation learning as a retrieval\ntask. Representations can be learnt by matching augmented\nviews of a same instance from a memory bank [44, 16] or a\nlarge mini-batch [6]. MoCoV2 [7] constitutes the improved\nversion of the MoCo [16] method, incorporating larger data\naugmentation. We note that data augmentation is pertinent\nin allowing a model to learn robust representations in con-\ntrastive learning. However, only traditional data augmenta-\ntion was used in aforementioned methods.\nData augmentation.\nMoCoV2 [7] used ‘random crop’,\n‘random color jittering’, ‘random horizontal ﬂip’, ‘ran-\ndom grayscale’ and ‘gaussian blur’.\nHowever, ‘random\ncolor jittering’ and ‘grayscale’ were not suitable for ﬁne-\ngrained person ReID, because such methods for data aug-\nmentation tend to change the color distribution of origi-\nnal images. In addition, ‘Random Erasing’ [49] has been\na commonly used technique in person ReID, which ran-\ndomly erases a small patch from an original image. Cross-\ndomain Mixup [29] interpolated source and target domain\nimages, which alleviated the domain gap in UDA ReID. Re-\ncently, Generative Adversarial Networks (GANs) [15] have\nshown great success in image [23, 22, 2] and video syn-\nthesis [34, 37, 3, 39, 38]. GAN-based methods can serve\nas a method for evolved data augmentation by condition-\nally modifying id-unrelated features (style and structure)\nfor supervised ReID. CamStyle [53] used the CycleGAN-\narchitecture [54] in order to transfer images from one cam-\nera into the style of another camera. FD-GAN [14] was tar-\ngeted to generate images in a pre-deﬁned pose, so that im-\nages could be compared in the same pose. IS-GAN [10] was\n2\nstreamlined to disentangle id-related and id-unrelated fea-\ntures by switching both local and global level identity fea-\ntures. DG-Net [47] recolored grayscale images with a color\ndistribution of other images, targeting to disentangle iden-\ntity features. Deviating from such supervised GAN-based\nmethods, our method generates novel views by rotating 3D\nmeshes in an unsupervised manner.\nUnsupervised person ReID.\nRecent unsupervised person\nReID methods were predominantly based on UDA. Among\nUDA-based methods, several works [36, 26] used seman-\ntic attributes to facilitate domain adaptation. Other works\n[43, 12, 5, 45, 13] assigned pseudo labels to unlabeled im-\nages and proceeded to learn representations with pseudo la-\nbels. Transferring source dataset images into the style of\na target dataset represents another line of research.\nSP-\nGAN [9] and PTGAN [41] used CycleGAN [54] as do-\nmain style transfer-backbone. HHL [50] aims at transfer-\nring cross-dataset camera styles. ECN [51, 52] exploited\ninvariance from camera style transferred images for UDA\nReID. CR-GAN [8] employed parsing-based masks to re-\nmove noisy backgrounds. PDA [25] included skeleton es-\ntimation to generate person images with different poses\nand cross-domain styles. DG-Net++ [55] jointly disentan-\ngled id-related/id-unrelated features and transferred domain\nstyles. While the latter is related to our our method, we aim\nat training jointly a GAN-based online data augmentation,\nas well as a contrastive discrimination, which renders the\nlabeled source dataset unnecessary, rather than transferring\nstyle.\nFully unsupervised methods do not require any identity\nlabels. BUC [27] represented each image as a single class\nand gradually merged classes. In addition, TSSL [42] con-\nsidered each tracklet as a single class to facilitate cluster\nmerging. SoftSim [28] utilized similarity-based soft labels\nto alleviate label noise. MMCL [35] assigned multiple bi-\nnary labels and trained a model in a multi-label classiﬁca-\ntion way. JVTC and JVTC+ [24] added temporal informa-\ntion to reﬁne visual similarity based pseudo labels. We note\nthat all aforementioned fully unsupervised methods learn\nfrom pseudo labels. We show in this work that disentan-\ngling view-invariant identity features is possible in fully un-\nsupervised ReID, which can be an add-on to boost the per-\nformance of previous pseudo label based methods.\n3. Proposed Method\nWe refer to our proposed method as joint Generative and\nContrastive Learning as GCL. The general architecture of\nGCL comprises of two modules, namely a View Genera-\ntor, as well as a View Contrast Module, see Fig. 2. Firstly,\nthe View Generator uses cycle-consistency on both, image\nand feature reconstructions in order to disentangle identity\nand structure features. It combines identity features and\nmesh-guided structure features to generate one person in\nnew view-points. Then, original and generated views are\nexploited as positive pairs in the View Contrast Module,\nwhich enables our network to learn view-invariant identity\nfeatures. We proceed to elaborate on both modules in the\nfollowing.\n3.1. View Generator (Generative Module)\nAs shown in Fig. 2, the proposed View Generator in-\ncorporates 4 networks: an identity encoder Eid, a struc-\nture encoder Estr, a decoder G and an image discrimi-\nnator D. Given an unlabeled person ReID dataset X =\n{x1, x2, ..., xN}, we generate corresponding 3D meshes\nwith a popular 3D mesh generator Human Mesh Recovery\n(HMR) [21], which simultaneously estimates body shape\nand pose from a single RGB image.\nHere, we denote\nthe 2D projection of a 3D mesh as original structure sori.\nThen, as depicted in Fig. 3, we rotate each 3D mesh by\n45°, 90°, 135°, 180°, 225°, 270° and 315°, respectively and\nproceed to randomly pick one 2D projection of these ro-\ntated meshes as a new structure snew.\nWe use the 3D\nmesh rotation to mimic view-point variance from differ-\nent cameras. Next, unlabeled images are encoded to iden-\ntity features by the identity encoder Eid : x →fid,\nwhile both original and new structures are encoded to struc-\nture features by the structure encoder Estr\n: sori\n→\nfstr(ori), snew →fstr(new). Combining both, identity and\nstructure features, the decoder generates synthesized im-\nages G : (fid, fstr(ori)) →x′\nori, (fid, fstr(new)) →x′\nnew,\nwhere a prime is used to represent generated images.\nGiven the lack of real images corresponding to the\nnew structures, we consider a cycle consistency [54] to\nreconstruct the original image by swapping the structure\nfeatures in the View Generator.\nWe encode and decode\nonce again to get synthesized images in original structures\nG(Eid(x′\nnew), sori) →x′′\nori. We calculate an image recon-\nstruction loss as follows.\nLimg = E[∥x −x′\nori∥1] + E[∥x −x′′\nori∥1]\n(1)\nIn addition, we compute a feature reconstruction loss\nLfeat = E[∥fid −Eid(x′\nnew)∥1]+\nE[∥fid −Eid(x′′\nori)∥1].\n(2)\nThe discriminator D attempts to distinguish between real\nand generated images with the adversarial loss\nLadv = E[log D(x) + log(1 −D(x′\nori))]+\nE[log D(x) + log(1 −D(x′\nnew))]+\nE[log D(x) + log(1 −D(x′′\nori))].\n(3)\nConsequently, the overall GAN loss combines the above\nnamed losses with weighting coefﬁcients λimg and λfeat\nLgan = λimgLimg + λfeatLfeat + Ladv.\n(4)\n3\nFigure 2: (a) General architecture of GCL: Generative and contrastive modules are coupled by the shared identity encoder Eid. (b)\nGenerative module: The decoder G combines the identity features encoded by Eid and structure features Estr to generate a novel view\nx′\nnew with a cycle consistency. (c) Contrastive module: View-invariance is enhanced by maximizing the agreement between original\nEid(x), synthesized Eid(x′\nnew) and memory fpos representations.\nFigure 3: Example images as generated by the View Generator via\n3D mesh rotation based on left input image.\n3.2. View Contrast (Contrastive Module)\nThe previous reconstruction and adversarial losses work\nin an unconditional manner. They only explore identity fea-\ntures within the original view-point, which renders appear-\nance representations view-variant. In rotating an original\nmesh to a different view-point, e.g., from front to side view-\npoint, the generation is prone to fail due to lack of informa-\ntion pertained to the side view. This issue can be alleviated\nby enhancing the view-invariance of representations.\nGiven an anchor image x, the ﬁrst step is to ﬁnd pos-\nitive images that belong to the same identity and negative\nimages that belong to different identities. Here, we store all\ninstance representations in a memory bank [44], which sta-\nbilizes pseudo labels and enlarges the number of negatives\nduring the training with mini-batches. The memory bank\nM is updated with a momentum coefﬁcient α.\nM[i]t = α · M[i]t−1 + (1 −α) · f t\n(5)\nwhere M[i]t and M[i]t−1 respectively refer to the identity\nfeature vector in the t and t −1 epochs.\nWe use a clustering algorithm DBSCAN [11] on all\nmemory bank feature vectors to generate pseudo identity\nlabels Y = {y1, y2, ..., yJ}, which are renewed at the be-\nginning of every epoch.\nGiven the obtained pseudo la-\nbels, we have Npos positive and Nneg negative instances\nfor each training instance. Npos and Nneg vary for differ-\nent instances. For simplicity in a mini-batch training, we\nﬁx common positive and negative numbers for every train-\ning instance. Given an image x, we randomly sample K\ninstances that have different pseudo identities and one in-\nstance representation fpos that has the same pseudo identity\nwith x from the memory bank. Note that fpos is from a ran-\ndom positive image that usually has a pose and camera style\ndifferent from x and x′\nnew. x and x′\nnew are encoded by Eid\ninto identity feature vectors f and f ′\nnew. Next, f, f ′\nnew and\nfpos are used in turn to form three positive pairs. The f ′\nnew\nand K different identity instances in the memory bank are\nused as K negative pairs. Towards learning robust view-\ninvariant representations, we extend the InfoNCE loss [30]\ninto a view-invariant loss between original and generated\nviews. We use sim(u, v) =\nu\n∥u∥2 ·\nv\n∥v∥2 to denote the cosine\nsimilarity. We deﬁne the view-invariant loss as a softmax\nlog loss of K + 1 pairs as following.\nLvi = E[log (1 +\nPK\ni=1 exp (sim(f ′\nnew, ki)/τ)\nexp (sim(f, fpos)/τ)\n)]\n(6)\nL′\nvi = E[log (1 +\nPK\ni=1 exp (sim(f ′\nnew, ki)/τ)\nexp (sim(f ′new, f)/τ)\n)]\n(7)\n4\nL′′\nvi = E[log (1 +\nPK\ni=1 exp (sim(f ′\nnew, ki)/τ)\nexp (sim(f ′new, fpos)/τ)\n)],\n(8)\nwhere τ indicates a temperature coefﬁcient that controls the\nscale of calculated similarities. Lvi maximizes the invari-\nance between original and memory positive views.\nL′\nvi\nmaximizes the invariance between synthesized and origi-\nnal views. L′′\nvi maximizes the invariance between synthe-\nsized and memory positive views. Meanwhile, the synthe-\nsized view is pushed away from K negative views in the\nlatent space. Replacing sim(f ′\nnew, ki) in Eq. 6, Eq. 7 and\nEq. 8 with sim(f, ki) is another possibility, which pushes\naway the original view from negative instances. After test-\ning, sim(f ′\nnew, ki) works better, because pushing away the\nsynthesized view from negative instances aid the generation\nof more accurate synthesized views that look different from\nthe K negative instances.\n3.3. Joint Training\nOur proposed GCL framework is trained in a joint train-\ning way. Both GAN and contrastive instance discrimination\ncan be trained in a self-supervised manner. While the GAN\nlearns a data distribution via adversarial learning on each in-\nstance, contrastive instance discrimination learns represen-\ntations by retrieving each instance from candidates. In our\ndesigned joint training, the two modules work as two col-\nlaborators with the same objective: enhancing the quality\nof representations built by the shared identity encoder Eid.\nWe formulate our GCL as an approach to augment contrast\nfor unsupervised ReID. Firstly, the generative module gen-\nerates online data augmentation, which enhances the pos-\nitive view diversity for contrastive module. Secondly, the\ncontrastive module, in turn, learns view-invariant represen-\ntations by matching original and generated views, which re-\nﬁne the generation quality. The joint training boosts both\nmodules simultaneously. Our joint training conducts for-\nward propagation initially on the generative module and\nsubsequently on the contrastive module. Back-propagation\nis then conducted with an overall loss that combines Eq. 4,\nEq. 6, Eq. 7 and Eq. 8.\nLall = Lgan + Lvi + L′\nvi + L′′\nvi\n(9)\nTo accelerate the training process and alleviate the noise\nfrom imperfect generation quality at beginning epochs, we\nneed to warm up the four modules used in the View Gen-\nerator Eid, Estr, G and D. We ﬁrstly use a state-of-the-art\nunsupervised ReID method to warm up Eid, which is then\nconsidered as a baseline in our ablation studies. Generally\nspeaking, any unsupervised ReID method can be used to\nwarm up Eid. Before conducting the View Contrast, we\nfreeze Eid and warm up Estr, G, and D only with GAN\nloss in Eq. 4 for 40 epochs. In the following, we bring in\nthe memory bank and the pseudo labels to jointly train the\nwhole framework with Lall for another 20 epochs. During\nthe joint training, pseudo labels are updated at the beginning\nof every epoch.\n4. Experiments\n4.1. Datasets and Evaluation Protocols\nThree mainstream person ReID datasets are consid-\nered in our experiments, including Market-1501 [46],\nDukeMTMC-reID [31] and MSMT17 [41]. Market-1501\nis composed of 12,936 images of 751 identities for training\nand 19,732 images of 750 identities for test captured from 6\ncameras. DukeMTMC-reID contains 16,522 images of 702\npersons for training, 2,228 query images and 17,661 gallery\nimages of 702 persons for test from 8 cameras. MSMT17 is\na larger dataset, which contains 32,621 training images of\n1,041 identities and 93,820 testing images of 3,060 identi-\nties collected from 15 cameras.\nFollowing state-of-the-art unsupervised ReID methods\n[35, 24], we evaluate our proposed method GCL under\nfully unsupervised setting on the three datasets and under\nfour UDA benchmark protocols, including Market→Duke,\nDuke→Market, Market→MSMT and Duke→MSMT. We\nreport both quantitative and qualitative results for unsuper-\nvised person ReID and view generation.\n4.2. Implementation Details\nWe ﬁrstly present network design details of Eid, Estr, G\nand D. In the following descriptions, we write the size of\nfeature maps in channel×height×width. Our model design\nis mainly inspired by [47, 55]. (1) Eid is a ImageNet [32]\npre-trained ResNet50 [17] with slight modiﬁcations. The\noriginal fully connected layer is replaced by a fully con-\nnected embedding layer, which outputs identity representa-\ntions f in 512×1×1 for the View Contrast. In parallel, we\nadd a part average pooling that outputs identity features fid\nin 2048×4×1 for the View Generator. (2) Estr is composed\nof four convolutional and four residual layers, which out-\nput structure features fstr in 128×64×32. (3) G contains\nfour residual and four convolutional layers. Every residual\nlayer contains two adaptive instance normalization layers\n[19] that transform fid into scale and bias parameters. (4)\nD is a multi-scale PatchGAN [20] discriminator at 64×32,\n128×64 and 256×128.\nThen, we present the training and testing conﬁguration\ndetails.\nOur framework is implemented in Pytorch and\ntrained with one Nvidia Titan RTX GPU. (1) For the Eid\nwarm-up, we consider JVTC [24], because it is a state-\nof-the-art ReID method that is compatible with both fully\nunsupervised and UDA settings. We also test other base-\nlines, e.g., MMCL [35] and ACT [45] to demonstrate the\ngeneralizability of our method. (2) For training, inputs are\nresized to 256×128.\nWe empirically set a large weight\nλimg = λfeat = 5 for reconstruction in Eq. 4. With a batch\nsize of 16, we use SGD to train Eid and Adam optimizer to\n5\nMethod\nReference\nMarket1501\nDukeMTMC-reID\nSource\nmAP\nRank1\nRank5\nRank10\nSource\nmAP\nRank1\nRank5\nRank10\nBUC [27]\nAAAI’19\nNone\n29.6\n61.9\n73.5\n78.2\nNone\n22.1\n40.4\n52.5\n58.2\nSoftSim [28]\nCVPR’20\nNone\n37.8\n71.7\n83.8\n87.4\nNone\n28.6\n52.5\n63.5\n68.9\nTSSL [42]\nAAAI’20\nNone\n43.3\n71.2\n-\n-\nNone\n38.5\n62.2\n-\n-\nMMCL [35]\nCVPR’20\nNone\n45.5\n80.3\n89.4\n92.3\nNone\n40.2\n65.2\n75.9\n80.0\nJVTC [24]\nECCV’20\nNone\n41.8\n72.9\n84.2\n88.7\nNone\n42.2\n67.6\n78.0\n81.6\nJVTC+ [24]\nECCV’20\nNone\n47.5\n79.5\n89.2\n91.9\nNone\n50.7\n74.6\n82.9\n85.3\nMMCL*\nThis paper\nNone\n45.1\n79.5\n89.0\n91.9\nNone\n40.9\n64.8\n75.2\n79.8\nJVTC*\nThis paper\nNone\n47.2\n75.4\n86.7\n90.5\nNone\n43.9\n66.8\n77.6\n81.0\nJVTC+*\nThis paper\nNone\n50.9\n79.1\n89.8\n92.9\nNone\n52.8\n74.9\n83.3\n85.8\nours(MMCL*)\nThis paper\nNone\n54.9\n83.7\n91.6\n94.0\nNone\n49.3\n69.7\n79.7\n82.8\nours(JVTC*)\nThis paper\nNone\n63.4\n83.7\n91.6\n94.3\nNone\n53.3\n72.4\n82.0\n84.9\nours(JVTC+*)\nThis paper\nNone\n66.8\n87.3\n93.5\n95.5\nNone\n62.8\n82.9\n87.1\n88.5\nECN [51]\nCVPR’19\nDuke\n43.0\n75.1\n87.6\n91.6\nMarket\n40.4\n63.3\n75.8\n80.4\nPDA [25]\nICCV’19\nDuke\n47.6\n75.2\n86.3\n90.2\nMarket\n45.1\n63.2\n77.0\n82.5\nCR-GAN [8]\nICCV’19\nDuke\n54.0\n77.7\n89.7\n92.7\nMarket\n48.6\n68.9\n80.2\n84.7\nSSG [12]\nICCV’19\nDuke\n58.3\n80.0\n90.0\n92.4\nMarket\n53.4\n73.0\n80.6\n83.2\nMMCL [35]\nCVPR’20\nDuke\n60.4\n84.4\n92.8\n95.0\nMarket\n51.4\n72.4\n82.9\n85.0\nACT [45]\nAAAI’20\nDuke\n60.6\n80.5\n-\n-\nMarket\n54.5\n72.4\n-\n-\nDG-Net++ [55]\nECCV’20\nDuke\n61.7\n82.1\n90.2\n92.7\nMarket\n63.8\n78.9\n87.8\n90.4\nJVTC [24]\nECCV’20\nDuke\n61.1\n83.8\n93.0\n95.2\nMarket\n56.2\n75.0\n85.1\n88.2\nECN+ [52]\nPAMI’20\nDuke\n63.8\n84.1\n92.8\n95.4\nMarket\n54.4\n74.0\n83.7\n87.4\nJVTC+ [24]\nECCV’20\nDuke\n67.2\n86.8\n95.2\n97.1\nMarket\n66.5\n80.4\n89.9\n92.2\nMMT [13]\nICLR’20\nDuke\n71.2\n87.7\n94.9\n96.9\nMarket\n65.1\n78.0\n88.8\n92.5\nCAIL [29]\nECCV’20\nDuke\n71.5\n88.1\n94.4\n96.2\nMarket\n65.2\n79.5\n88.3\n91.4\nACT*\nThis paper\nDuke\n59.1\n78.8\n88.9\n91.7\nMarket\n51.5\n70.9\n80.0\n83.4\nJVTC*\nThis paper\nDuke\n65.0\n85.7\n93.6\n95.6\nMarket\n56.5\n73.9\n84.5\n87.7\nJVTC+*\nThis paper\nDuke\n67.6\n87.0\n95.2\n97.0\nMarket\n66.7\n81.0\n89.9\n91.5\nours(ACT*)\nThis paper\nDuke\n66.7\n83.9\n91.4\n93.4\nMarket\n55.4\n71.9\n81.6\n84.6\nours(JVTC*)\nThis paper\nDuke\n73.4\n89.1\n95.0\n96.6\nMarket\n60.4\n77.2\n86.2\n88.4\nours(JVTC+*)\nThis paper\nDuke\n75.4\n90.5\n96.2\n97.1\nMarket\n67.6\n81.9\n88.9\n90.6\nTable 1: Comparison of unsupervised ReID methods (%) with a ResNet50 backbone on Market and Duke datasets. We test our proposed\nmethod on several baselines, whose names are in brackets. * refers to our implementation based on authors’ code.\ntrain Estr, G and D. Learning rate is set to 1×10−4 during\nthe warm-up. In the joint-training, learning rate in Adam is\nset to 1 × 10−4 and 3.5 × 10−4 in SGD and are multiplied\nby 0.1 after 10 epochs. (3) In the View Contrast module,\nwe set the momentum coefﬁcient α = 0.2 in Eq. 5 and the\ntemperature τ = 0.04 in Eq. 6. The number of negatives K\nis 8192. DBSCAN density radius is set to 2×10−3. (4) For\ntesting, only Eid is conserved and outputs representations f\nof dimension 512.\nImportant parameters are set by a grid search on the fully\nunsupervised Market-1501 benchmark. The temperature τ\nis searched from {0.03, 0.04, 0.05, 0.06, 0.07} and ﬁnally\nis set to 0.04. A smaller τ increases the scale of similarity\nscores in the Eq. 6, Eq. 7 and Eq. 8, which makes view-\ninvariant losses more sensitive to inter-instance difference.\nHowever, when τ is set to 0.03, these losses become too\nsensitive and make the training unstable. The number of\nnegatives K is searched from {2048, 4096, 8192}. A larger\nK pushes away more negatives in the view-invariant losses.\nSince the Market-1501 dataset has only 12936 training im-\nages, we set K = 8192.\n4.3. Unsupervised ReID Evaluation\nComparison with state-of-the-art methods.\nTab. 1\nshows the quantitative results on the Market-1501 and\nDukeMTMC-reID datasets. Tab. 2 shows the quantitative\nresults on the MSMT17 dataset. Our method is mainly de-\nsigned for fully unsupervised ReID. Under this setting, we\ntest the performance of GCL with three different baselines,\nincluding MMCL, JVTC and JVTC+. Our implementation\nof the three baselines provides results that are slightly dif-\nferent from those mentioned in the corresponding papers.\nThus, we ﬁrstly report results of our implementations and\nthen add our GCL on these baselines. Our method improves\nthe performance of the baselines by large margins. These\nimprovements show that GANs are not limited to cross-\ndomain style transfer for unsupervised ReID.\nUnder the UDA setting, we also evaluate the perfor-\nmance of GCL with three different baselines, including\nACT, JVTC and JVTC+. The labeled source dataset is only\nused to warm up our identity encoder Eid, but not used\nin our joint generative and contrastive training. Compared\nto fully unsupervised methods, the UDA warmed Eid is\nstronger and extracts improved identity features. Thus, the\nperformance of UDA methods is generally higher than fully\nunsupervised methods. With a strong baseline JVTC+, our\nGCL achieves state-of-the-art performance.\n6\nMethod\nReference\nMSMT17\nSource\nmAP\nR1\nR5\nR10\nMMCL [35]\nCVPR’20\nNone\n11.2\n35.4\n44.8\n49.8\nJVTC [24]\nECCV’20\nNone\n15.1\n39.0\n50.9\n56.8\nJVTC+ [24]\nECCV’20\nNone\n17.3\n43.1\n53.8\n59.4\nJVTC*\nThis paper\nNone\n13.4\n36.0\n48.8\n54.9\nJVTC+*\nThis paper\nNone\n16.3\n40.4\n55.6\n61.8\nours(JVTC*)\nThis paper\nNone\n18.0\n41.6\n53.2\n58.4\nours(JVTC+*)\nThis paper\nNone\n21.3\n45.7\n58.6\n64.5\nECN [51]\nCVPR’19\nMarket\n8.5\n25.3\n36.3\n42.1\nSSG [12]\nICCV’19\nMarket\n13.2\n31.6\n49.6\n-\nMMCL [35]\nCVPR’20\nMarket\n15.1\n40.8\n51.8\n56.7\nECN+ [52]\nPAMI’20\nMarket\n15.2\n40.4\n53.1\n58.7\nJVTC [24]\nECCV’20\nMarket\n19.0\n42.1\n53.4\n58.9\nDG-Net++ [55]\nECCV’20\nMarket\n22.1\n48.4\n60.9\n66.1\nCAIL [29]\nECCV’20\nMarket\n20.4\n43.7\n56.1\n61.9\nMMT [13]\nICLR’20\nMarket\n22.9\n49.2\n63.1\n68.8\nJVTC+ [24]\nECCV’20\nMarket\n25.1\n48.6\n65.3\n68.2\nJVTC*\nThis paper\nMarket\n17.1\n39.6\n53.3\n59.3\nJVTC+*\nThis paper\nMarket\n20.5\n44.0\n59.5\n71.1\nours(JVTC*)\nThis paper\nMarket\n21.5\n45.0\n57.1\n66.5\nours(JVTC+*)\nThis paper\nMarket\n27.0\n51.1\n63.9\n69.9\nECN [51]\nCVPR’19\nDuke\n10.2\n30.2\n41.5\n46.8\nSSG [12]\nICCV’19\nDuke\n13.3\n32.2\n51.2\n-\nMMCL [35]\nCVPR’20\nDuke\n16.2\n43.6\n54.3\n58.9\nECN+ [52]\nPAMI’20\nDuke\n16.0\n42.5\n55.9\n61.5\nJVTC [24]\nECCV’20\nDuke\n20.3\n45.4\n58.4\n64.3\nDG-Net++ [55]\nECCV’20\nDuke\n22.1\n48.8\n60.9\n65.9\nMMT [13]\nICLR’20\nDuke\n23.3\n50.1\n63.9\n69.8\nCAIL [29]\nECCV’20\nDuke\n24.3\n51.7\n64.0\n68.9\nJVTC+ [24]\nECCV’20\nDuke\n27.5\n52.9\n70.5\n75.9\nJVTC*\nThis paper\nDuke\n19.9\n45.4\n59.1\n64.9\nJVTC+*\nThis paper\nDuke\n23.6\n49.4\n65.2\n71.1\nours(JVTC*)\nThis paper\nDuke\n24.9\n50.8\n63.4\n68.9\nours(JVTC+*)\nThis paper\nDuke\n29.7\n54.4\n68.2\n74.2\nTable 2: Comparison of unsupervised Re-ID methods (%) with a\nResNet50 backbone on MSMT17. * refers to our implementation\nbased on authors’ code.\nAblation Study.\nTo better understand the contribution of\ngenerative and contrastive modules, we conduct ablation\nexperiments on the two fully unsupervised benchmarks:\nMarket-1501 and DukeMTMC-reID. Quantitative results\nwith a JVTC baseline are reported in Tab. 3. By gradually\nadding loss functions on the baseline, our ablation experi-\nments correspond to three scenarios. (1) Only Generation:\nwith only Lgan, our generation module disentangles iden-\ntity and structure features. Since there is no inter-view con-\nstraint, Eid tends to extract view-speciﬁc identity features,\nwhich decreases the ReID performance. (2) Only Contrast:\nwe use LwoGAN\nvi\n= E[log (1 +\nPK\ni=1 exp (sim(f,ki)/τ)\nexp (sim(f,fpos)/τ)\n)] to\ntrain our contrastive module without generation. We also\nadd a set of traditional data augmentation, including random\nﬂipping, cropping, jittering, erasing, to train our contrastive\nmodule like a traditional memory bank based contrastive\nmethod. (3) Joint Generation and Contrast: Lvi, L′\nvi and\nL′′\nvi enhance the view-invariance of identity representations\nbetween original, synthesized and memory-stored positive\nviews, while negative views are pushed away. We provide\nhow view-invariant representations learned from generated\nviews affect pseudo labels in Appendix C.\nLoss\nMarket-1501\nDukeMTMC-reID\nmAP\nRank1\nmAP\nRank1\nBaseline\n47.2\n75.4\n43.9\n66.8\n+Lgan\n41.6\n69.0\n25.8\n45.9\n+LwoGAN\nvi\n47.8\n75.2\n44.1\n67.8\n+LwoGAN\nvi\n+ TDA\n53.7\n78.7\n48.5\n70.0\n+Lgan + Lvi\n54.1\n79.4\n47.4\n68.4\n+Lgan + Lvi + L′\nvi\n59.2\n82.2\n50.5\n71.0\n+Lgan + Lvi + L′\nvi + L′′\nvi\n63.4\n83.7\n53.3\n72.4\nTable 3: Ablation study on loss functions used in two modules.\n(1). Lgan corresponds to generation w/o contrast. (2). LwoGAN\nvi\ncorresponds to contrast w/o generation. TDA denotes traditional\ndata augmentation. (3). Lgan + Lvi (L′\nvi and L′′\nvi) correspond to\njoint generative and contrastive learning.\nFigure 4: Qualitative ablation study on the view-invariant losses.\nFor simplicity, Lvi denotes three view-invariant losses Lvi+L′\nvi+\nL′′\nvi, which helps Eid to extract view-invariant features (red shirt).\nMethod\nFID(realism)\nSSIM(diversity)\nReal\n7.22\n0.350\nFD-GAN [14]\n216.88\n0.271\nIS-GAN [10]\n281.63\n0.165\nDG-Net [47]\n18.24\n0.360\nOurs(U)\n59.86\n0.367\nOurs(UDA)\n53.07\n0.369\nTable 4: Comparison of FID (lower is better) and SSIM (higher is\nbetter) on Market-1501 dataset. U denotes the fully unsupervised\nsetting. UDA denotes Duke→Market setting.\nWe also conduct a qualitative ablation study, where syn-\nthesized novel views without and with view-invariant losses\nare illustrated in Fig. 4. Results conﬁrm that Eid extracts\nview-speciﬁc identity features (black bag), in the case that\nview-invariant losses are not used. Given view-invariant\nlosses, Eid is able to extract view-invariant identity features\n(red shirt). Another example is provided in Appendix B.\n4.4. Generation Quality Evaluation\nComparison with state-of-the-art methods.\nWe com-\npare generated images between our proposed GCL under\nthe JVTC [24] warmed fully unsupervised setting and state-\nof-the-art GAN-based ReID methods in Fig. 5. FD-GAN\n[14], IS-GAN [10] and DG-Net [47] are supervised Re-ID\nmethods. Since the source code of these three methods is\navailable, we compare generated images of same identities.\nWe observe that there exists blur in images generated by\n7\nFigure 5: Comparison of the generated images on Market-1501 dataset. ⋆refers to methods without sharing source code, whose examples\nare cropped from their papers. Examples of FD-GAN, IS-GAN, DG-Net and GCL are generated from six real images shown in the ﬁgure.\nFigure 6: Generated novel views on the three datasets.\nFD-GAN and IS-GAN. DG-Net generates sharper images,\nbut different body shapes and some incoherent objects (bags\nand clothes) are observed. PDA [25] and DG-Net++ [55]\nare UDA methods, whose source code is not yet released.\nWe can only compare several generated images with un-\nknown identities as illustrated in their papers. PDA gener-\nates blurred cross-domain images, whose quality is similar\nto FD-GAN and IS-GAN. DG-Net++ extends DG-Net into\ncross-domain generation, which has same problems of body\nshape and incoherent objects. Our GCL preserves better\nbody shape information and does not generate incoherent\nobjects. Moreover, our GCL is a fully unsupervised method.\nWe use Fr´echet Inception Distance (FID) [18] to mea-\nsure visual quality, as well as Structural SIMilarity (SSIM)\n[40] to capture structure diversity of generated images. In\nTab. 4, we compare our method with FD-GAN [14], IS-\nGAN [10] and DG-Net [47], whose source code is avail-\nable. FID measures the distribution distance between gen-\nerated and real images, where a lower FID represents the\ncase, where generated images are similar to real ones. SSIM\nmeasures the intra-class structural similarity, where a larger\nSSIM represents a larger diversity. We note that DG-Net\nis outperforms our method w.r.t. FID, because the distribu-\ntion is better maintained with ground truth identities in the\nsupervised method DG-Net. However, our method is supe-\nrior to DG-Net w.r.t. SSIM, as DG-Net swaps intra-dataset\nstructures, whereas our rotated meshes build structures that\ndo not exist in the original dataset.\nFigure 7: Linear interpolation on identity features. Identity fea-\ntures are swapped between left and right persons.\nMore discussion.\nTo validate, whether identity and struc-\nture features can be really disentangled under a fully unsu-\npervised ReID setting, two experiments are conducted by\nchanging ﬁrstly only structure features and then only iden-\ntity features. Results in Fig. 6 show that changing structure\nfeatures only change structures and do not affect appear-\nances. We also ﬁx structure features and linearly interpolate\ntwo random identity feature vectors. Results in Fig. 7 show\nthat identity features only change appearances and do not\naffect structures in generated images. More examples are\nprovided in Appendix D.\n5. Conclusions\nIn this paper, we propose a joint generative and con-\ntrastive learning framework for unsupervised person ReID.\nDeviating from previous contrastive methods with tradi-\ntional data augmentation techniques, we generate diversi-\nﬁed views with a 3D mesh guided GAN. These generated\nnovel views are then combined with original images in\nmemory based contrastive learning, in order to learn view-\ninvariant representations, which in turn improve generation\nquality. Our generative and contrastive modules mutually\npromote each other’s performance in unsupervised ReID.\nMoreover, our framework does not rely on a source dataset,\nwhich is mandatory in style transfer based methods. Exten-\nsive experiments on three datasets validate the effectiveness\nof our framework in both unsupervised person ReID and\nmulti-view person image generation.\n8\nReferences\n[1] Slawomir Bak, Peter Carr, and Jean-Francois Lalonde. Do-\nmain adaptation through synthesis for unsupervised person\nre-identiﬁcation. In ECCV, 2018. 1\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale GAN training for high ﬁdelity natural image synthesis.\nIn ICLR, 2019. 2, 11\n[3] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A\nEfros. Everybody dance now. In ICCV, 2019. 2\n[4] Hao Chen, Benoit Lagadec, and Francois Bremond. Learn-\ning discriminative and generalizable representations by\nspatial-channel partition for person re-identiﬁcation.\nIn\nWACV, 2020. 1\n[5] Hao Chen, Benoit Lagadec, and Francois Bremond.\nEn-\nhancing diversity in teacher-student networks via asymmet-\nric branches for unsupervised person re-identiﬁcation.\nIn\nWACV, 2021. 3\n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020. 1, 2\n[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv preprint arXiv:2003.04297, 2020. 1, 2\n[8] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Instance-\nguided context rendering for cross-domain person re-\nidentiﬁcation. In ICCV, 2019. 3, 6\n[9] Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi\nYang, and Jianbin Jiao. Image-image domain adaptation with\npreserved self-similarity and domain-dissimilarity for person\nre-identiﬁcation. In CVPR, 2018. 1, 3\n[10] Chanho Eom and Bumsub Ham. Learning disentangled rep-\nresentation for robust person re-identiﬁcation. In NeurIPS,\n2019. 2, 7, 8\n[11] Martin Ester, Hans-Peter Kriegel, J¨org Sander, and Xiaowei\nXu. A density-based algorithm for discovering clusters in\nlarge spatial databases with noise. In KDD, 1996. 4\n[12] Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou,\nHonghui Shi, and Thomas S Huang. Self-similarity group-\ning: A simple unsupervised cross domain adaptation ap-\nproach for person re-identiﬁcation. In ICCV, 2019. 3, 6,\n7\n[13] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\nteaching: Pseudo label reﬁnery for unsupervised domain\nadaptation on person re-identiﬁcation. In ICLR, 2020. 3,\n6, 7\n[14] Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi,\nXiaogang Wang, and Hongsheng Li. Fd-gan: Pose-guided\nfeature distilling gan for robust person re-identiﬁcation. In\nNeurIPS, 2018. 2, 7, 8\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014. 1, 2\n[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020. 1, 2\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 5\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In NeurIPS, 2017. 8\n[19] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization.\nIn ICCV,\n2017. 5\n[20] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In CVPR, 2017. 5\n[21] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In CVPR, 2018. 2, 3\n[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019. 2\n[23] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of StyleGAN. In CVPR, 2020. 2, 11\n[24] Jianing Li and Shiliang Zhang.\nJoint visual and tempo-\nral consistency for unsupervised domain adaptive person re-\nidentiﬁcation. In ECCV, 2020. 1, 2, 3, 5, 6, 7, 11\n[25] Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank\nWang.\nCross-dataset person re-identiﬁcation via unsuper-\nvised pose disentanglement and adaptation. In ICCV, 2019.\n1, 2, 3, 6, 8\n[26] Shan Lin, Haoliang Li, Chang-Tsun Li, and Alex Chichung\nKot. Multi-task mid-level feature alignment network for un-\nsupervised cross-dataset person re-identiﬁcation. In BMVC,\n2018. 3\n[27] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\nYang. A bottom-up clustering approach to unsupervised per-\nson re-identiﬁcation. In AAAI, 2019. 1, 3, 6\n[28] Yutian Lin, Lingxi Xie, Yu Wu, Chenggang Yan, and Qi\nTian.\nUnsupervised person re-identiﬁcation via softened\nsimilarity learning. In CVPR, 2020. 3, 6\n[29] Chuanchen Luo, Chunfeng Song, and Zhaoxiang Zhang.\nGeneralizing person re-identiﬁcation by camera-aware in-\nvariance learning and cross-domain mixup. In ECCV, 2020.\n2, 6, 7\n[30] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 2, 4\n[31] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking. In ECCVW, 2016. 5\n[32] Olga Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Zhiheng Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, A. Berg, and Li Fei-Fei. Imagenet large scale visual\nrecognition challenge. IJCV, 2015. 5\n[33] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. Beyond part models: Person retrieval with reﬁned\npart pooling (and a strong convolutional baseline). In ECCV,\n2018. 1\n[34] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz. MoCoGAN: Decomposing motion and content for\nvideo generation. In CVPR, 2018. 2\n9\n[35] Dongkai Wang and Shiliang Zhang. Unsupervised person re-\nidentiﬁcation via multi-label classiﬁcation. In CVPR, 2020.\n1, 2, 3, 5, 6, 7\n[36] Jingya Wang, Xiatian Zhu, Shaogang Gong, and Wei Li.\nTransferable joint attribute-identity deep learning for unsu-\npervised person re-identiﬁcation. CVPR, 2018. 3\n[37] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. G3AN: Disentangling appearance and motion\nfor video generation. In CVPR, 2020. 2\n[38] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In WACV, 2020. 2\n[39] Yaohui Wang, Francois Bremond, and Antitza Dantcheva.\nInmodegan: Interpretable motion decomposition generative\nadversarial network for video generation.\narXiv preprint\narXiv:2101.03049, 2021. 2\n[40] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 8\n[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nPerson transfer gan to bridge domain gap for person re-\nidentiﬁcation. In CVPR, 2018. 1, 3, 5\n[42] Guile Wu, Xiatian Zhu, and Shaogang Gong.\nTrack-\nlet self-supervised learning for unsupervised person re-\nidentiﬁcation. In AAAI, 2020. 3, 6\n[43] Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wei Bian, and Yi\nYang. Progressive learning for person re-identiﬁcation with\none example. TIP, 2019. 3\n[44] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In CVPR, 2018. 2, 4\n[45] Fengxiang Yang, Ke Li, Zhun Zhong, Zhiming Luo, Xing\nSun, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong\nJi, and Shaozi Li. Asymmetric co-teaching for unsupervised\ncross-domain person re-identiﬁcation. In AAAI, 2020. 3, 5,\n6\n[46] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\ndong Wang, and Qi Tian. Scalable person re-identiﬁcation:\nA benchmark. ICCV, 2015. 5\n[47] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng,\nYi Yang, and Jan Kautz. Joint discriminative and generative\nlearning for person re-identiﬁcation. In CVPR, 2019. 3, 5, 7,\n8\n[48] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-\nranking person re-identiﬁcation with k-reciprocal encoding.\nIn CVPR, 2017. 11\n[49] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In AAAI, 2020.\n2\n[50] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Gener-\nalizing a person retrieval model hetero- and homogeneously.\nIn ECCV, 2018. 1, 3\n[51] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi\nYang.\nInvariance matters: Exemplar memory for domain\nadaptive person re-identiﬁcation. In CVPR, 2019. 3, 6, 7\n[52] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi\nYang. Learning to adapt invariance in memory for person\nre-identiﬁcation. PAMI, 2020. 3, 6, 7\n[53] Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li,\nand Yi Yang.\nCamera style adaptation for person re-\nidentiﬁcation. In CVPR, 2018. 2\n[54] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros.\nUnpaired image-to-image translation using cycle-\nconsistent adversarial networks. In ICCV, 2017. 2, 3, 11\n[55] Yang Zou, Xiaodong Yang, Zhiding Yu, B. V. K. Vijaya Ku-\nmar, and Jan Kautz. Joint disentangling and adaptation for\ncross-domain person re-identiﬁcation. In ECCV, 2020. 1, 3,\n5, 6, 7, 8\n10\nAppendices\nAppendix A. Cycle consistency\nTwo kinds of cycle consistency are used in our proposed\nmethod GCL. 1) Based on a popular assumption in ReID\nthat two images of a same identity should have same near-\nest neighbors in the dataset, we have calculated k-reciprocal\nJaccard distance [48] for the DBSCAN clustering. This op-\neration effectively makes pseudo labels more reliable for\ncontrastive learning.\n2) Since no paired data are avail-\nable, we have used the CycleGAN [54] structure to super-\nvise the generative module. By minimizing the image and\nfeature reconstruction losses in a cycle consistency, repre-\nsentations are disentangled into appearance and structure\nfeatures, which permits generating person images in novel\nview-points without changing identity information.\nAppendix B. View-invariant losses\nWe illustrate another example in Fig. 8 to conﬁrm the ef-\nfectiveness of the view-invariant losses in generation. When\nGCL is trained without the view-invariant losses, GCL de-\ngrades to a traditional CycleGAN, which is prone to af-\nfected by the noise inside the original image. The view-\ninvariant losses help the identity encoder to extract identity\nfeatures shared between different views, which are robust to\nthe noise inside the original image.\nAppendix C. Effects on pseudo labels\nWe minimize intra-class variance via contrasting gen-\nerated images, which leads to a larger inter-class distance\nin latent space.\nLearning view-invariant representations\nfrom diversiﬁed generated data helps clustering algorithms\nto generate more accurate pseudo labels. With a same DB-\nSCAN clustering, the cluster number of GCL is closer to\nreal identity number than that of contrastive learning with\ntraditional data augmentation. For example, Market-1501\ndataset has 751 real identities. DBSCAN in GCL catego-\nrizes unlabeled images into around 520 clusters, while the\ncontrastive learning with traditional data augmentation has\naround 460 clusters (see Fig. 9).\nAppendix D. Generated views\nWe illustrate more examples of generated views with\na JVTC [24] fully unuspervised baseline on Market-1501\nin Fig. 10, DukeMTCM-reID in Fig. 11 and MSMT17 in\nFig. 12. Here, we show generated examples from both train-\ning and test sets to conﬁrm the effectiveness of our GCL.\nGenerally, the generation quality is good enough to help our\nGCL learn view-invariant representations. However, there\nare still some limitations, e.g., some visual blurs still exist\nand detailed identity information is lost in some cases (in\nFigure 8: More qualitative ablation study on the view-invariant\nlosses.\nFor simplicity, Lvi denotes three view-invariant losses\nLvi+L′\nvi+L′′\nvi, which helps Eid to extract better identity features\n(white shirt).\nFigure 9: Cluster number curve on Market-1501. TDA denotes\ntraditional data augmentation, including random ﬂipping, crop-\nping, jittering, erasing.\nthe bottom row of Fig. 10, the red logo on the shorts disap-\npears in the generated images). In future work, we believe\nthat the visual blurs can be alleviated by leveraging the ar-\nchitectures from more recent GANs [2, 23] in our genera-\ntor and detailed identity information can be better preserved\nwhen better unsupervised baselines are available.\nGenerally, it is easier to generate novel views of 45°,\n180° and 315°. 45° and 315° are small rotations, in which\noriginal and synthesized images can share maximal identity\ninformation. 180° can be roughly regarded as a horizontal\nﬂipping. Results in Tab. 5 verify this supposition. Our gen-\nerated novel views on the three datasets will be released as\na new dataset to facilitate future research on view-invariant\nand unsupervised ReID.\n11\n45°\n90°\n135°\n180°\n225°\n270°\n315°\nMarket\n56.55\n75.83\n62.59\n51.22\n62.31\n70.79\n55.00\nDuke\n58.24\n72.21\n66.29\n57.41\n64.61\n68.08\n55.03\nMSMT\n54.14\n64.46\n60.75\n55.98\n59.78\n62.26\n48.40\nTable 5: FID score on different views.\nFigure 10: Examples of generated novel views on Market-1501\ntraining and test sets.\nFigure 11: Examples of generated novel views on DukeMTMC-\nreID training and test sets.\n12\nFigure 12: Examples of generated novel views on MSMT17 train-\ning and test sets.\n13\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-12-16",
  "updated": "2021-03-30"
}