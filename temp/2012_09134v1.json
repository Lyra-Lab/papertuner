{
  "id": "http://arxiv.org/abs/2012.09134v1",
  "title": "Multi-agent navigation based on deep reinforcement learning and traditional pathfinding algorithm",
  "authors": [
    "Hongda Qiu"
  ],
  "abstract": "We develop a new framework for multi-agent collision avoidance problem. The\nframework combined traditional pathfinding algorithm and reinforcement\nlearning. In our approach, the agents learn whether to be navigated or to take\nsimple actions to avoid their partners via a deep neural network trained by\nreinforcement learning at each time step. This framework makes it possible for\nagents to arrive terminal points in abstract new scenarios. In our experiments,\nwe use Unity3D and Tensorflow to build the model and environment for our\nscenarios. We analyze the results and modify the parameters to approach a\nwell-behaved strategy for our agents. Our strategy could be attached in\ndifferent environments under different cases, especially when the scale is\nlarge.",
  "text": "Multi-agent navigation based on deep reinforcement\nlearning and traditional pathﬁnding algorithm\nHongda Qiu\nDecember 17, 2020\nAbstract\nWe develop a new framework for multi-agent collision avoidance problem. The frame-\nwork combined traditional pathﬁnding algorithm and reinforcement learning. In our ap-\nproach, the agents learn whether to be navigated or to take simple actions to avoid their\npartners via a deep neural network trained by reinforcement learning at each time step.\nThis framework makes it possible for agents to arrive terminal points in abstract new sce-\nnarios.\nIn our experiments, we use Unity3D and Tensorﬂow to build the model and environ-\nment for our scenarios. We analyze the results and modify the parameters to approach a\nwell-behaved strategy for our agents. Our strategy could be attached in different environ-\nments under different cases, especially when the scale is large.\nKey words: Collision Avoidance Problem, Multi-agent Problem, Deep Reinforcement\nLearning\n1\nIntroduction\nMulti-agent navigation problem (also known as multi-agent pathﬁnding and collision avoid-\nance problem) has been a highly-concerned topic in control, robotics and optimization. The\nresearch of this problem has great potential in engineering applications. One of the major di-\nrections is to develop reliable and high efﬁcient navigating algorithm (or strategy) for agents\nto arrive their terminal points without any collisions.\nIn general, a pathﬁnding and collision avoiding mission requires the agent(s) to ﬁgure out\ncollision-free optimized paths γ from the set-off(s) to the target(s). We can deﬁne such a\nproblem rigorously as the following:\nLet p be an agent, α its set-off, β its target and S ⊂R2 its motion space with an obstacle set\nB = {Bk|Bk ⊂R2, ∂Bk ⊂C(R2)}, where ∂Bk means the boundary of Bk. These boundaries\n∂Bk can be regarded as closed, continuous loops on Euclidean plane R2. In our work, we\nfurther suppose that ∂Bk are (piecewise) smooth.\nSuppose the position and velocity of p at time step t are γ(t) = (x(t), y(t)), v(t) = ˙γ(x) =\n( ˙x(t), ˙y(t)) = (u(t), w(t)), respectively, and the maximum time T, the optimization goal of\nthe problem is:\nmin\nγ\nZ T\n0 |vi(t)|dt\n(1)\nsubject to γi(t) ̸∈S/ ∪Bk∈B Bk, |γi(t) −γj(t)| ≥dsa f e, γ(0) = s, γ(T) = t, t ∈[0, T]. Here\nγ(t) = (γ1(t), γ2(t))is the path, which is second-order differentiable with respect to t.\nTwo classes of methods have been developed to solve this problem for the single-agent case.\nThe ﬁrst class is usually referred to as continuous methods that employ calculus of variation\n[1]. In those works, the problem is described as an optimal control problem with constraints:\ngiven the set of obstacles {Bk}, the optimization target of some agent a is to minimize the\nfollowing functional along its path γ:\nJ(γ) =\nZ T\n0 L(t, γ(t), ˙γ(t))dt\n(2)\nsubject to d(a, ∂Bk) > dsa f e. Here L(t, γ(t), ˙γ(t)) is an energy functional of the time t, the\nposition γ(t) and the velocity ˙γ(t), usually deﬁned by speciﬁed circumstances; dsa f e is the\nminimized distance from the a to the bound of Pk, and dsa f e, the \"safe distance\" between p\nand any obstacles [1].\n1\narXiv:2012.09134v1  [cs.MA]  5 Dec 2020\nDiscretized methods, on the other hand, are also well developed. In early works, researchers\ndiscretize the environment of agent(s) into some grid world or graph and develop efﬁcient\nalgorithms to ﬁgure out global optimal solution for the agent which is allowed to take several\ndiscrete actions. Typical discretize algorithm include A∗search algorithm [2] and Dijkstra‘s\nalgorithm [3]. This line of thinking is still efﬁcacious in many ﬁelds including engineering,\nrobotics and commercial games, yet it does not work well in complicated cases, such as that\nwith a stochastic environment or large scale. Several methods have been developed to solve\nthis problem. One method related to our work is to use triangular mesh [4] instead of grid\nworld to decrease the computational cost [5]. Compared to continuous methods, discretized\nmethods typically require smaller computational cost and have higher efﬁciency.\nMulti-agent navigation [6], [7], [8], however, arouses new challenges for us. First of all,\nin such a system, each agent has to be considered with a group of independent constraints,\nwhich brings large cost for numerical calculation whether we adopt continuous or discontin-\nuous methods. Secondly, the model of the problem is NP-hard and the scale of the problem\nin practice can usually grow extremely large. In practice, every agent need to avoid not only\nstatic obstacles but also their partners who themselves are also dynamical. Getting precise\nsolutions numerically via traditional tools is thus expensive and time-costing.\nFortunately, some characteristics of the problem offer researchers new implication.\nThe\nMarkov property of the process of navigation implies the possibility of using reinforcement\nlearning tools. For each agent, the state of t st is completely determined by st−1 and its\ndecision (or action) at t −1, at−1.\ns0\na0→s1\na1→...\nat−1\n→st\nReinforcement Learning (RL) has immense advantage in dealing with robot navigation prob-\nlem. The advantage of reinforcement learning is that we do not have to precisely \"know\" ex-\nactly the model of the problem; instead, the algorithm itself could ﬁgure it out. Decisions of\nagents are completely obtained by a parameterized policy which can be optimized (trained)\nbased on training data of the systems. Such path thus circumvent the challenge of deﬁning\nand verifying a complicated model represented by some group of equations that are usually\nhard to solve.\nIn [9], the author introduces an example in which the agent (a Khepera robot) is required to\nexplore the environment and avoid collision with obstacles as long as possible. The problem\nis ﬁnally solved by mean-square policy iteration algorithm, where the task of the agent is\nspeciﬁed by a reward function. In the training process, the agent gathers local information\nof the environment via 8 distance sensors, and its navigation strategy, also known as behav-\nioral policy in reinforcement learning, is trained by a self-organizing map [10].\nReinforcement learning has also shown its power in solving multi-robot navigation prob-\nlems. Some recent works develop high performance navigating policy for agents via evo-\nlutionary reinforcement learning [11] and deep reinforcement learning [12]- [13], where de-\ncentralized and non-communicative method is used to train their agents. agents gain lo-\ncal information of the environment and make their decision independently while updating\nthe same policy. In [12], researchers study a small scale decentralized multi-agent collision\navoidance problem. In that work a bilateral (two-agent) value function is deﬁned based on\nthe state of a certain agent and its neighbor. However, agents in this work are supposed to\nbe omniscient to their surroundings, which is impossible in real world. As a progress, [13]\nfurther studies the problem in a larger scale, and develops a decentralized framework, in\nwhich the model of the agents is based on a real product. In the training, a group of 20\nagents detects surrounding environment through a series of distance sensors and share the\ncommon policy. In their tests, agents are able to ﬁnish tasks in many rich and complex sce-\nnarios.\nBoth of these works employ a class of deep reinforcement learning algorithm known as pol-\nicy gradient methods (or gradient-based methods) [14], [15]. Instead of estimating value\nfunction, this class of algorithms directly searches for the optimal parameter set in the pa-\nrameter set of the policy, allowing researchers to solve problems with continuous state and\naction space and large-scale inputs.\nThere are still some problems remained for us. First of all, deep reinforcement learning\nmerely guarantees collision-free paths, but not the shortest ones–these algorithms do not\nconsider the global structure of the maps (the activity area for the agents), which makes it\nfairly possible that the ﬁnal path is not globally optimized. Secondly, training and testing\nfor agents are only done in limited types of scenarios. This leads to weak robustness for\nvariance of environments. Agents may suffer much weaker performance, i.e., getting higher\ncollision rate or longer average path when the environment varies. For instance, a policy\ntrained using small group of agents may not work well for larger group anymore [13], [16].\n2\nRetraining the agents for every new environment, on the other hand, can be expensive and\ntypically require extra designing on the algorithm [17].\nThe advantages of reinforcement learning and traditional pathﬁnding methods inspire us to\ncombine them together—pathﬁnding problem has been efﬁcaciously solved by traditional\nmethods, and collision avoidance by reinforcement learning.\nIn this work, we explore the direction on combining traditional algorithms and machine\nlearning methods. We propose a framework that merge multi-agent reinforcement learning\nand classical pathﬁnding algorithm together. In our framework, the local decision policy\non pathﬁnding and collision avoidance is parameterized and optimized via reinforcement\nlearning using a policy-gradient algorithm [14], while the decisions of agents include an ac-\ntion that obtained by a determined and global pathﬁnding algorithm [2]. The complete logic\nof our framework is shown in (Figure 1). Our numerical results show that this framework\nachieve the following advantages: (1) Strong robustness to variance of environment, includ-\ning different maps, random starting and terminus points and different density of agents; (2)\nHigher efﬁciency of navigation compared to pure learning method [13] on the same path-\nprogramming problems; (3) Flexibility. This means that the policy trained from a funda-\nmental scenario can be used in other scenarios without losing too much accuracy; (4) Inde-\npendent decision. Unlike some previous work using complete centralized framework [23],\nwhile still adopting a centralized training process, our work enables for each agent a decen-\ntralized decision process.\nThis paper is organized as follows. In section 2 we introduce some related works in multi-\nFigure 1: An Overview of Our Approach\nagent reinforcement learning. The model of multi-agent navigation problem and our ap-\nproach are presented in section 3. The results and the main results of simulations are re-\nported in section 4, and our works are discussed and a conclusion is made in section 5.\n2\nRelated Work\nMulti-agent navigation problem has gained a large fragment of attention from researchers,\nMulti-agent navigation [6], [7], [8]. Several directions have been explored and many ap-\nproaches have been developed to attack this problem, including classical algorithm in com-\nputer science [6], constraint optimization [7] and conﬂict based searching [8], [18].\nThere has also been increasing focus on solving multi-agent navigation problem using ma-\nchine learning and reinforcement learning. The main inspiration of researchers is usually\nto make the decision policy highly adaptive to unknown or stochastic environments. Some\nlatest progresses have been made via evolutionary artiﬁcial neural networks (ANN) [19] or\nreinforcement learning implemented by evolutionary neural networks [11]. Other works\nusually aim at coping with very large scale problems in which agents can only gather partial\nand local information of environment. For instance, in [20], a deep neural work architecture\nis adopted to cope with the swarm coordinate problem in which agents could only get in-\nformation from nearby partners. Another related work is [21] in which deep reinforcement\n3\nlearning algorithm is used to build a mapless navigator for agents based on locally visual\nsensors.\nMulti-agent reinforcement learning (MARL) has gained great interest from researchers in\napplied math and computer science. The improvement and comparison in our work are\nbased on a recent study about MARL on R2 [13], where a policy-gradient method is adopted\nand several complex scenarios are tested.\nOne of the most important directions that related to our study stands on how to enhance\nthe performance of learning algorithms via attaching the learning algorithm with extra ap-\nproaches or tools. For instance, imitation learning–introducing expert knowledge to the\nmodel [22] can improve the robustness of learning and make the agents adaptive to differ-\nent environments. In another example [23], randomized decision process is introduced into\nthe framework to decrease the conﬂicts among agents and improve synergies of the whole\nsystem.\nSome prior works also focus on solving practical problems via combining MARL with other\nﬁelds of knowledge. For example, in [18], MARL is combined with conﬂict based search,\nenabling the agents to cope with pathﬁnding problems with sequential subtasks. In another\nwork [24], Mattia and his group use one-step Q-learning algorithm and wavelet adopted\nvortex method to build the dynamical model for schools of swimming ﬁsh.\nMerging RL or MARL with other ﬁelds of methods is also a direction gaining increasing at-\ntention. For instance, Hu proposes the Nash Q-learning algorithm by attaching the Lemke-\nHouson algorithm [25] to the iteration of policy updating to receive a Nash equilibrium for\nan N-agent game [26]. In another work [16], an adaptive algorithm that combines MARL\nand multi-agent game theory is developed to cope with MARL in highly crowded cases.\n3\nApproach\nThe main idea of our work is to combine traditional pathﬁnding algorithm and reinforce-\nment learning. In other words, we employ reinforcement learning to train the collision-\navoidance behavioral strategy for the agents while a pathﬁnding algorithm is attached to\nnavigate the agents to their target. The agents learn to decide whether to be navigated or to\navoid their partners at each time step. Every agent in the scenario acts independently, while\ntraining is centralized (Figure 2) – all the agents share the same strategy πθ and parameter\nset θ.\nWe begin this section with introducing the model of the problem; then we introduce the\nFigure 2: Reinforcement Learning\nframework of our approach, including the state, action reward design and algorithm of re-\ninforcement learning.\n3.1\nNotations\nBefore formal discussions, we introduce some basic notations in this paper.\n• Rd: d-dimensional Euclidean space;\n• C(X), Ck(X), C∞(X): continuous/k-degree differentiable/smooth functions on space\nX where X is usually a subspace of some Euclidean space;\n• ∆X: the space of distributions on some space X;\n4\n• EV: the expectation of some random variable V;\n• |v|: norm of vector v ∈Rd;\n• S: state space of agents in learning;\n• A: action space of agents in learning;\n3.2\nThe model of the problem\nThe model of N-agent navigation problem in our work is shown as the following: Let\ni = 1, . . . , N be the index of agents. For any agent i, let αi be its starting point and βi its\nterminal point. All agents are running on a motion space S = [0, l] × [0, l] ⊂R2 with an\nobstacle set B = {Bk|Bk ⊂R2, ∂Bk ⊂C∞(R2)}.\nSuppose that each agent i has a path γ and its the position and velocity at time step t is\nγi(t) = (xi, yi) ∈S ⊂R2, vi(t) = (ui, wi) ∈[0, 1] ∗[0, 2π) respectively. Furthermore, we\nnote the safe distance between each two agents as dsa f e and the terminal time as Ti ≤T for\neach agent i. All these variables are listed as in Table1.\nTable 1: Variables of Agent\nNotation\nDeﬁnition\nγi(t)\nPosition of i at t\nvi(t)\nVelocity of i at t\nTi\nTerminal time of i\ndsa f e\nThe safe distance between agents\nIn addition, we assume that:\na. l ≫dsa f e;\nb. Each agent i knows its corresponding αi, βi and the structure of the map, but there is\nno communication among agents.\nNow it is a fair point to propose the main objective of our approach.\nFirst of all, we introduce the goal of policy, that is, the goal of optimization that the optimal\npolicy of agents should achieve.\nLet π be any policy for agents and π∗be the optimal policy. Consider a discretized partition\nof the time interval [0, T], 0 < t1 < t2 < ... < t f inal = T. In our work, we take ta = a∆t\nwhere δt notes the time unit for simulation. For simplicity of notation and without loss of\ngenerality, we assume that T >> 1 and take δt = 1. Thus, we use can denote any time point\nby an integer t ∈Z+.\nAs a consequence, we can represent the dynamics of the system as the following:\n\n\n\n\n\n\n\n\n\nγi(t + 1) = γi(t) + vi(t)\nγi(0) = αi, γi(Ti) = βi\nγi(t) /∈S/ ∪Bk, |γi(t) −γj(t)| ≥dsa f e\ni = 0, 1, ..., N, t = 0, 1, ..., T\n(3)\nIn this system, the velocity vi(t) is chosen by some policy π for each agent i at each time step\nt.\nThus, given any set of αi, βi, at each time step t, a policy π can determine a series of paths\nγπ(t; αi, βi), i = 1, ..., N. To measure how well the policy works, we may compute the aver-\nage of all the paths, that is,\nS(π; αi, βi, i = 1, ..., N) = 1\nN\nN\n∑\ni=1\nLength(γπ(t; αi, βi)) = 1\nN\nN\n∑\ni=1\nTi\n∑\nt=0\n|vi(t)|\n(4)\nIn this work, we wish to optimize π, i.e., to obtain π∗among the set of all π by apply\nreinforcement learning. To achieve this, we parameterize π with a set of parameters θ =\n(θ1, ..., θM) ∈RM where M is the dimension. Then we rewrite π as πθ.\n5\nTo measure the performance of a policy πθ, we consider a dataset D of starting points and\nterminal points, i.e., D = {(αm\n1 , ..., αm\nN; βm\n1 , ..., βm\nN), m = 1, ..., M}. Then we can deﬁne a cost\nfunction R of θ by computing the average of S(π; αi, βi, i = 1, ..., N) among all data in D:\nR(θ; D) = 1\nM\nM\n∑\nm=1\nS(πθ; αi, βi, i = 1, ..., N)\n(5)\nThe optimum θ∗is deﬁned as:\nθ∗= argminθR(θ; D)\n(6)\nThe goal of learning is to optimize θ∗based on some dataset D. We will discuss our learning\nframework in the following sections.\n3.3\nMulti-Agent Reinforcement Learning\nIn this work, we adopt a class of policy gradient method [14], [15] to train the policy πθ. This\nmethod has shown its capability on solving machine-learning problems with continuous\nstate space in many other works ( [9], [13]). We discuss it with more detail in section 3.3.6.\nAgents are trained through interaction with the environment (scenarios). In the simulation,\neach agent learns whether to ﬁnd the path to its target, or to take some simple actions to\navoid approaching objects at each time step t. The maximum time step is T. The trajectory\nof each agent is then deﬁned as:\nh = {(st, at)|t = 1, 2, ..., T}\n(7)\nIn the following part of this section we omit the index i, since the deﬁnition of each agent is\nexactly the same. The position, velocity, starting point, terminal point of agent at time step t\nis then noted as γ(t), v(t), α, β. The terminal time of agent is noted as T.\nSpeciﬁc model of learning is introduced as the following.\n3.3.1\nState\nIn this work, each agent is modeled as disc with radius r = 1 physical unit, equipped with\n45 range sensors around (Figure 3). The sensors distribute uniformly around the agent, and\neach of them return the distance and type (that is, static or motile) of other objects. The ray\ndistance is noted as d.\nEach agent gains its state at time t st = (ot), where ot = (oj\nt), j = 1, 2, ..., 45\nFigure 3: The model of robot\n(t = 1, 2, ..., Ti). Here oj\nt = (dj\nt, typej\nt) is the information gained by the j the sensor of agent,\nincluding distance dj\nt ∈[0, d] and type of perceptible object typej\nt ∈{0, 1} (0 is referred to\nstatic obstacles and 1, other agents) near the agent.\n3.3.2\nAction\nThe action spaces consist of several actions {a0, a1, ..., a5}, where a0 means to be navigated\nby some pathﬁnding method M in the current time step, and a1, ..., a5, other simple actions\nallowed for avoiding collisions. We list all the actions as in 2.\n6\nTable 2: List of Actions\nNotation\nDeﬁnition\na0\ntake action accroding to method M\na1\nstay at the current point\na2\nmove forward: v = v0\na3\nturn left: v = 0, ∆ω = −ω0\na4\nturn right: v = 0, ∆ω = ω0\na5\nmove backward: v = v0\nAs a further remark, such construction of action space means that the policy π only need\nto make decision for agents among the six basic actions a0, ..., a5. The speciﬁc data of the\npathﬁnding task, i.e., the dataset of D and the motion space (in other words, the structure\nof the map) S/ ∪Bk∈B Bk are only received by the pathﬁnding method A. In other words,\nthe learning algorithm does not need to consider the speciﬁc structure of any map; it only\noptimize the parameter θ according to the reward which will be deﬁned in section 3.3.4.\n3.3.3\nPathﬁnding Method M\nNext, we introduce how the method M works to determine the action a0. We adopt the A∗\nalgorithm [2]. The map including the obstacle sets B is known to the algorithm before we\nbegin to train the agents by reinforcement learning.\nGiven the starting point α and terminus point β of a agent p, The main idea of A∗algorithm\nis to minimize the following object when p is at point x:\nf (x) = g(x) + h(x)\nwhere the cost function g(x) means the cost p has spent on its way from α to x, and the\nestimation function h(x), an estimate of the cost from x to t.\nIn practice we use the Delaunay Triangulation [4] to get a navigation mesh on the map and\nthen navigate the agents with A∗algorithm. The exact process of A∗algorithm could be\nseen in [2].\nBefore we deﬁne g(x) and h(x), we give the deﬁnition of cost g and estimation h of a triangle\n∆abc.\nFirst of all, we deﬁne the \"in-edge\" and \"out-edge\" of a triangle ∆abc which p passes through.\nThe in-edge is the edge that p go across when it enters ∆abc; the out-edge, on the opposite,\nthe edge that p go across when p leaves ∆abc. In Figure 4, suppose that γ is the path of p, ab\nthe in-edge and ac the out-edge.\nThe cost g of ∆abc is deﬁned as the distance between the middle point of in-edge and the\nmiddle point of out-edge. The estimation h is the straight-line distance from the barycenter\no of ∆abc to the terminus point tof p. In the following picture, for instance, g = |e f | and\nh = |ot|.\nNow we could deﬁne g(x), h(x) of p in a triangulated map. Suppose that p is currently at\nx and has passed through triangles ∆1, ∆2, ..., ∆n (Figure 5), where x ∈∆n, and the cost and\nestimation of ∆are gi, hi respectively, the cost function and estimation function are deﬁned\nas g(x) = ∑n\ni=1 gi, h(x) = hn. Here s, t is deﬁned as above.\n7\nFigure 4: Moving across a triangle: the blue curve γ notes the path of p through ∆abc; p\nenters ∆abc through edge ab and leave it through edge ac.\nFigure 5: Moving through several triangles\n3.3.4\nReward\nThe step reward for each agent is designed as the following:\nrt = rnavigation + rscenario + rpenalty\nEach agent gets a positive reward rnavigation > 0 when it decides action a0. The inspiration\nof setting so is to encourage the agent to get navigated as frequent as possible so that it can\narrive at its target faster. The value of\nπθ(ai\ntsi\nt)\nπθold(ai\ntsi\nt) is set as,\nrnavigation =\n(\nr0,\ni f decides a0\n0,\nelse\n(8)\nThe agent also gets a reward rscenario when interacting with the scenario. The idea is straight-\nfoward: if the agent arrives at its target, it would get a positive reward; if it coolides, it would\nget a negative reward; otherwise, it would get nothing.\nrscenario =\n\n\n\n\n\nrnegative,\ni f collides with other objects\nrpositive,\ni f arrives the target\n0,\nelse\n(9)\nMoreover, agent gets a time penalty rpenalty < 0 at each time step. The maximal possible\namount of time penalty that an agent can get in one single path shall be smaller than T. To\n8\nensure that the agent always gets a positive reward when it manages to arrive at its target,\nwe assume that rpenalty · T ≤rpositive.\nIn our experiments we set r0 = 0.00005, rnegative = −0.5, rpositive = 1, rpenalty = −0.0001.\n3.3.5\nNeural Network\nOur approach is implemented with a parameter set θ ∈RL with scale L for policy π. Here a\nlinear neural network with K full connection layers, each of U = 256 units is employed. The\ninput of this network is the observation gained by the sensors, and the output, the action of\nagent (Figure 6). The scale of parameter set θ is then L = 256 ∗K.\nFigure 6: The neural network in our approach\n3.3.6\nPolicy Gradient Method\nIn this work, we adopt a variation of the Policy Gradient Method [14].\nLet st ∈S be the state of the agent and at ∈A the action taken by the agent at time step t.\nLet V : ∫→R be the value function of learning deﬁned as\nV(s) = E[\n∞\n∑\nt=0\nηtrt|s0 = s]\n(10)\nwhere η is the discount rate of learning.\nThe policy gradient method can be regarded as a stochastic gradient algorithm aiming at\nmaximizing the following objective,\nL(θ) = E πθ(ai\ntsi\nt)\nπθold(ai\ntsi\nt)\nˆAt\n(11)\nwhere ˆAt is the advantage function [15] deﬁned as the following:\nˆAt = δt + (ηλ)δt+1 + ... + (ηλ)T−t+1δT−1\n(12)\nwhere δt = rt + ηV(st+1) −V(st). One can think of ˆAt as an analogy of the value function V\nin classical reinforcement learning framework.\nThen equation 11 leads to an estimator of policy gradient,\ng(θ) = E∇θ log πθ(at|st) ˆAt\n(13)\nwhere πθ : S × A →∆(A) becomes a randomized policy that determines the distribution\nof at+1 according to (st, at). In this work, the value function of policy π is noted as V(s; πθ).\nThe training process of each agent shares and updates the same parameter set θ. U is the\nbatch size for data collecting. η, λ, ϵ are hyper-parameters of the algorithm. Their values\nwill be introduced in section 4.\n4\nSimulation and Results\nWe begin this section by introducing the training scenario and hyper-parameters of our ex-\nperiments. Then we turn to compare our methods quantitively with pure reinforcement\nlearning methods [12], [13]. The section ends up with tests on some special scenarios.\n9\nAlgorithm 1 Proximal Policy Optimization (PPO, use clip surrogate objective)\n1: for each episode with Max Training Step do\n2:\nfor actor i= 1,2, ..., N do\n3:\nExecute πθold for U steps and gain si\nt, ai\nt, ri\nt respectively;\n4:\nCalculate advantage estimates Ai\n1... Ai\nt, according to formula:\nAi\nt =\nT−1\n∑\np=t\n{(ηλ)p−t[ri\np + ηV(si\np+1; πθ) −V(si\np; πθ)]}\n5:\nend for\n6:\nOptimize L(θ) = min(ui\nt(θ)Ai\nt, clip(ui\nt(θ)Ai\nt, 1 −ϵ, 1 + ϵ)Ai\nt) using the Adam opti-\nmizer, where ui\nt(θ) =\nπθ(ai\ntsi\nt)\nπθold(ai\ntsi\nt);\n7:\nθold ←θ (All agents are updated in parallel)\n8: end for\n4.1\nScenarios and Hyper-Parameters\nOur algorithm is implemented in Tensorﬂow 1.4.0 (Python 3.5.2). The models of our scenar-\nios and agents are built in Unity3D 2017.4.1f1. The policies are trained and simulated based\non an i7-7500CPU and a NIVIDA GTX 950M GPU.\nOur basic scenario is built in a square α of size 200 × 200 (Figure 7). Initially, αi, βi are set ran-\ndomly, and v0\ni = (0, 0), with i = 1, 2, ..., N. We set v0 = 1, ω0 = 1, T = 10000 and N = 80.\nEach agent will be reinitialized once it arrives its terminus point or its training exceeds the\nmax training time step T. If a collision happens, the involved agents will also be immediately\nreinitialized. In the simulation, all the space, angle and velocity in the experiment adopt the\nbasic units in Unity3D.\nFigure 7: The Training Scenario\nThe hyper-parameters in table 1.\nThe learning rate discounts as in 8.\nFigure 8: The Learning Rate Curve\n4.2\nComparison: The Baseline Algorithm\nTo better demonstrate the performance of our method, we introduce a baseline algorithm\n[13], [14] and compare the simulation results between our method and this baseline algo-\n10\nHyper-Parameters\nValue\nη\n0.99\nλ\n0.95\nϵ\n0.1\nHidden Units\n256\nNumber of Full-Connection Layers\n4 or 8\nMax Training Step\n5000000\nLearning Rate\n0.0003\nBatch Size U\n2048\nMemory Size\n2048\nTable 3: The hyper-parameter list\nrithm.\nThe idea of its construction is straightfoward. One can regard this baseline algorithm as a\n\"curtailed\" version of our own algorithm in which the action a0 is removed from the action\nset. In other words, this is an algorithm that involves nothing more than reinforcement learn-\ning. In the following content, we refer to it as \"Pure Reinforcement Learning (RL) Method\".\n4.3\nResults of Simulation\nThe result of training and test is shown as the following.\nSeveral measurements are employed to quantitively measure the effectiveness of our ap-\nproach in the training scenario and compare it with other methods in several scenarios. In\nour tests, a agent begins a trial once it is initialized or reinitialized. The trial ends if the agent\narrives or collides with other objects. In the tests, we run the model in each scenario for\n20000 trials and get the results.\n1. Success Rate: In our experiment, a trial of a agent is said to be “success” once it arrives\nits terminus point without colliding others or exceeding max time step T. The success\nrate is deﬁned as Number o f Sucess Trials\nTotal Number o f Trials .\n2. Extra Distance Proportion (EDP): Since the starting points and terminus points are de-\ntermined randomly, it is unreasonable to measure the circuitousness of the paths given\nby our method using average running distance or velocity of the agents. Hence, we\nadopt the Extra Distance Proportion (EDP). Given starting point a and terminus point\nb of a certain agent, suppose that l is the shortest distance given by the pathﬁnding\nalgorithm without considering avoiding its partner (l could be precalculated before\nnavigation, and in the training scenario we could obviously get l = |a −b|) and d (ob-\nviously d ≥l) is the actual total distance the agent run in practice, the extra distance\nproportion e(a, b) is deﬁned as:\ne(a, b) = d −l\nl\nWe use e to note the average EDP of all the agents during the test. We could easily see\nthat the more circuitously the agent runs, the larger e becomes. Note that EDP are only\nupdated when one agent arrives its terminus point successfully.\n3. Collision Rate: A trial ends and becomes an \"accident\" if the agent is involved in a\ncollision. The collision Rate is deﬁned as\nNumber o f\nAccidents\nTotal Number o f Trials during the test, reﬂecting\nthe reliability of avoiding collision of the policy.\nWe train the agents with N = 80, and test the policy when N = 40, 80, 120, 160, 200 in the\nfollowing scenarios. Our method (with 4 or 8 Full -Connection layers) are compared with the\n11\npure reinforcement learning approach in [12], [13]. In both cases, the models spend about 4\nhours before converging to a stable policy. The 4-FC-layer policy converges after 22000 steps,\nwhile the 8-FC-layer case, 66000 steps. The pure RL policy spends 24 hours on training.\nThe learning process is also shown below. Figure 9 and Figure 10 are screen shots from\nTensorboard. During the training, the Average Reward-Time (rt −t) curve is adopted here\nto show the learning effectiveness of our algorithm.\nFigure 9: The Average Reward-Time Curve of our policy (4 FC layers)\nFigure 10: The Average Reward-Time Curve of our policy (4 FC layers)\n1. Basic Scenario: This is our training scenario. During this test we also count the num-\nber of perceptible partners of each robot at each time step. We gain a sample of scale\n100000 in the case N = 80, 120, 160, 200 and draw the histograms of them to show the\ncrowdedness around individual agents. These statistics could partially reﬂect the vari-\nance of observation of agents while N changes (Figure 11). Through these histograms\nwe could see that what agents observe when N = 80 largely differs from that when\nN = 160 or 200. This explains why the performance of our method decrease while N\nincreases, especially when N exceeds 160;\n2. Cross Road Scenario: This scenario is almost the same as the basic scenario. The only\nthing different is that each agent starts randomly from one of the four edges of α and\nis supposed to arrive a random point on the opposite edge. For instance, the agent\nmay have starting point a ∈[x1, 100] ⊂Sand terminus point b ∈[x2, −100] ⊂Swhere\nx1, x2 ∈[−100, 100]. The results are shown in Table 2.\nOur policy performances better than pure reinforcement learning methods in the tests. The\nagents can arrive at their terminus points with higher accuracy and lower cost. Meanwhile,\nour method learns much quicker compared to pure RL method and it does not require the\nagents to be retrained. It also shows better robustness in different cases, including different\ndensity of agents or different maps, especially when the scale of agents increases.\nNext, we test the policy in different scenarios to show the adaptability for new scenarios of\nour approach. The results of these tests are shown in Table 3.\n1. Four-Wall Scenario;\n2. Random Obstacle Scenario;\n3. Circle Transport scenario.\nIn those tests, the agents are capable of coping with unfamiliar scenarios without any further\nassistance. The value of EDP shows that our policy is able to optimize the paths of agents\nwith lower computational cost, comparing to the pure RL method.\n12\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 11: The crowdness when N = 40, 80, 120, 160, 200\n13\nN\nMethod\nSuccess Rate\nEDP (mean/std)\nCollision Rate\n40\nPure RL Method\nOur Method (4 FC layers)\nOur Method (8 FC layers)\n0.983\n0.984\n0.983\n0.573/0.236\n0.516/0.218\n0.545/0.246\n0.016\n0.015\n0.016\n80\nPure RL Method\nOur Method (4 FC layers)\nOur Method (8 FC layers)\n0.963\n0.971\n0.961\n0.877/0.3252\n0.652/0.281\n0.701/0.2933\n0.036\n0.028\n0.038\n120\nPure RL Method\nOur Method (4 FC layers)\nOur Method (8 FC layers)\n0.898\n0951\n0.921\n0.987/0.477\n0.854/0.403\n0.866/0.425\n0.093\n0.047\n0.077\n160\nPure RL Method\nOur Method (4 FC layers)\nOur Method (8 FC layers)\n0.732\n0.862\n0.860\n1.132/0.532\n1.054/0.456\n0.991/0.481\n0.138\n0.136\n0.137\n200\nPure RL Method\nOur Method (4 FC layers)\nOur Method (8 FC layers)\n0.532\n0.831\n0.783\n1.232/0.738\n1.126/0.653\n1.103/0.689\n0.249\n0.167\n0.211\nTable 4: The results of tests in the basic scenario\nFigure 12: Cross Road Scenario\nFigure 13:\nThe Four-Wall(FW) Scenario:\nIn this scenario, the blue agents start from\n{(x, y)|x ∈80, 100, y ∈[80, 100]} and are supposed to run to {(x, y)|x ∈−80, −100, y ∈\n[−80, −100]}; the pink agents do the opposite. Here we set N = 30.\n14\nFigure 14: The Random Obstacle (RO) Scenario: This scenario contains several rectangle\nobstacles of random size. The starting and terminus points of agents are randomly set just\nlike that in the cross road scenario. Here we set N=80, 120.\nFigure 15: The Circle Transport (CT) Scenario: In this scenario, 20 agents are put uniformly\non a circle of radius=80. Each of them are supposed to arrive its antipodal point. This is\na difﬁcult mission in multi-agent navigation, but the agents successfully ﬁnish the mission\nunder the navigation.\n15\nScenario\nMethod\nSuccess\nRate\nEDP\nCollision Rate\nFW\nPure RL Method\nOur Method(4 FC layers)\n0.329\n0.969\n2.313\n1.177\n0.360\n0.030\nRO\nN=80\nPure RL Method\nOur Method(4 FC layers)\n0.623\n0.999\n1.951\n0.833\n0.268\n<0.001\nRO\nN=120\nPure RL Method\nOur Method(4 FC layers)\n0.432\n0.960\n2.003\n1.051\n0.377\n0.039\nCT\nPure RL Method\nOur Method(4 FC layers)\n0.999\n0.999\n1.310\n0.893\n<0.001\n<0.001\nTable 6: The performance of the policy that trained in our basic scenario in different scenarios\n5\nConclusion\nIn this work, we develop a new framework for the multi-agent pathﬁnding and collision\navoiding problem by combining a traditional pathﬁnding algorithm and a reinforcement\nlearning method together. Both parts are essential–the reinforcement learning training de-\ntermines the reliable behavior of the agents, while the pathﬁnding algorithm guarantees the\narrival of them.\nNumerical results demonstrate that our approach has a better accuracy and robustness com-\npared to pure learning method. It is also adaptive for different scale of agents and scenarios\nwithout retraining.\nOngoing work will concentrate on enhancing the efﬁciency of our method by decreasing\ncomputational cost using tools in probability theory. Meanwhile, we are also interested in\nmaking use of the cooperation among agents to reﬁne the framework of current algorithms.\nReferences\n[1] W. Li, S. N. Chow, M. Egerstedt, J. Lu, and H. Zho, “Method of evolving junctions: A\nnew approach to optimal path-planning in 2d environments with moving obstacles,”\nInternational Journal of Robotics Research, vol. 36, no. 4, p. 027836491770725, 2017.\n[2] P. E. Hart, N. J. Nilsson, and B. Raphael, “A formal basis for the heuristic determination\nof minimum cost paths,” IEEE Transactions on Systems Science and Cybernetics, vol. 4,\nno. 2, pp. 100–107, 2007.\n[3] R. B. Dial, “Algorithm 360:\nshortest-path forest with topological ordering [h],”\nCommunications of the Acm, vol. 12, no. 11, pp. 632–633, 1969.\n[4] I. Debled-Rennesson, E. Domenjoud, B. Kerautret, and P. Even, Discrete Geometry for\nComputer Imagery.\nSpringer, 2002.\n[5] D. Demyen and M. Buro, “Efﬁcient triangulation-based pathﬁnding.” Aaai, vol. 1338,\nno. 9, pp. 161–163, 2007.\n[6] S. Bhattacharya, R. Ghrist, and V. Kumar, “Multi-robot coverage and exploration on rie-\nmannian manifolds with boundaries,” The International Journal of Robotics Research,\nvol. 33, no. 1, pp. 113–137, 2014.\n[7] W. Hönig, T. S. Kumar, L. Cohen, H. Ma, H. Xu, N. Ayanian, and S. Koenig, “Multi-\nagent path ﬁnding with kinematic constraints.” in ICAPS, vol. 16, 2016, pp. 477–485.\n[8] G. Sharon, R. Stern, A. Felner, and N. R. Sturtevant, “Conﬂict-based search for optimal\nmulti-agent pathﬁnding,” Artiﬁcial Intelligence, vol. 219, pp. 40–66, 2015.\n[9] M. Sugiyama, H. Hachiya, and T. Morimura, Statistical Reinforcement Learning:\nModern Machine Learning Approaches.\nChapman & Hall/CRC, 2015.\n[10] T. Kohonen,\n“Self-organized formation of topologically correct feature maps,”\nBiological Cybernetics, vol. 43, no. 1, pp. 59–69, 1982.\n16\n[11] Z. Liu, B. Chen, H. Zhou, G. Koushik, M. Hebert, and D. Zhao, “Mapper: Multi-agent\npath planning with evolutionary reinforcement learning in mixed dynamic environ-\nments,” arXiv preprint arXiv:2007.15724, 2020.\n[12] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decentralized non-communicating mul-\ntiagent collision avoidance with deep reinforcement learning,” pp. 285–292, 2016.\n[13] P. Long, T. Fan, X. Liao, W. Liu, H. Zhang, and J. Pan, “Towards optimally decentralized\nmulti-robot collision avoidance via deep reinforcement learning,” 2017.\n[14] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy opti-\nmization algorithms,” 2017.\n[15] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” 2016.\n[16] J. E. Godoy, I. Karamouzas, S. J. Guy, and M. Gini, “Adaptive learning for multi-\nagent navigation,” in Proceedings of the 2015 International Conference on Autonomous\nAgents and Multiagent Systems, 2015, pp. 1577–1585.\n[17] T. George Karimpanal and R. Bouffanais, “Self-organizing maps for storage and\ntransfer of knowledge in reinforcement learning,” Adaptive Behavior, vol. 27, no. 2, p.\n111C126, Dec 2018. [Online]. Available: http://dx.doi.org/10.1177/1059712318818568\n[18] J. Motes, R. Sandstrm, H. Lee, S. Thomas, and N. M. Amato, “Multi-robot task and\nmotion planning with subtask dependencies,” IEEE Robotics and Automation Letters,\nvol. 5, no. 2, pp. 3338–3345, 2020.\n[19] F. Wang and E. Mckenzie, “A multi-agent based evolutionary artiﬁcial neural network\nfor general navigation in unknown environments,” in Proceedings of the third annual\nconference on Autonomous Agents, 1999, pp. 154–159.\n[20] M. Hüttenrauch, A. Šoši´c, and G. Neumann, “Guided deep reinforcement learning for\nswarm systems,” 2017.\n[21] L. Tai, G. Paolo, and M. Liu, “Virtual-to-real deep reinforcement learning: Continuous\ncontrol of mobile robots for mapless navigation,” in Ieee/rsj International Conference\non Intelligent Robots and Systems, 2017.\n[22] G. Sartoretti, J. Kerr, Y. Shi, G. Wagner, T. S. Kumar, S. Koenig, and H. Choset, “Primal:\nPathﬁnding via reinforcement and imitation multi-agent learning,” IEEE Robotics and\nAutomation Letters, vol. 4, no. 3, pp. 2378–2385, 2019.\n[23] Y. Jiang, H. Yedidsion, S. Zhang, G. Sharon, and P. Stone, “Multi robot planning with\nconﬂicts and synergies,” Autonomous Robots, vol. 43, no. 8, pp. 2011–2032, 2019.\n[24] M. Gazzola, B. Hejazialhosseini, and P. Koumoutsakos, “Reinforcement learning and\nwavelet adapted vortex methods for simulations of self-propelled swimmers,” Siam\nJournal on Scientiﬁc Computing, vol. 36, no. 3, 2016.\n[25] C. E. Lemke and J. T. Howson, “Equilibrium points of bimatrix games,” Journal of the\nSociety for Industrial & Applied Mathematics, vol. 12, no. 2, pp. 413–423, 1964.\n[26] Hu, Junling, Wellman, and P. Michael, “Nash q-learning for general-sum stochastic\ngames,” Journal of Machine Learning Research, vol. 4, no. 4, pp. 1039–1069, 2003.\n17\n",
  "categories": [
    "cs.MA",
    "cs.LG",
    "cs.RO"
  ],
  "published": "2020-12-05",
  "updated": "2020-12-05"
}