{
  "id": "http://arxiv.org/abs/2012.11663v1",
  "title": "Combining Deep Reinforcement Learning And Local Control For The Acrobot Swing-up And Balance Task",
  "authors": [
    "Sean Gillen",
    "Marco Molnar",
    "Katie Byl"
  ],
  "abstract": "In this work we present a novel extension of soft actor critic, a state of\nthe art deep reinforcement algorithm. Our method allows us to combine\ntraditional controllers with learned neural network policies. This combination\nallows us to leverage both our own domain knowledge and some of the advantages\nof model free reinforcement learning. We demonstrate our algorithm by combining\na hand designed linear quadratic regulator with a learned controller for the\nacrobot problem. We show that our technique outperforms other state of the art\nreinforcement learning algorithms in this setting.",
  "text": "Combining Deep Reinforcement Learning And Local Control\nFor The Acrobot Swing-up And Balance Task\nSean Gillen, Marco Molnar, and Katie Byl\nAbstract— In this work we present a novel extension of soft\nactor critic, a state of the art deep reinforcement algorithm.\nOur method allows us to combine traditional controllers with\nlearned neural network policies. This combination allows us to\nleverage both our own domain knowledge and some of the ad-\nvantages of model free reinforcement learning. We demonstrate\nour algorithm by combining a hand designed linear quadratic\nregulator with a learned controller for the acrobot problem.\nWe show that our technique outperforms other state of the art\nreinforcement learning algorithms in this setting.\nI. INTRODUCTION\nAdvances in machine learning have allowed researchers to\nleverage the massive amount of compute available today in\norder to better control robotic systems. The result is that\nmodern model-free reinforcement learning has been used\nto solve very difﬁcult problems. Recent examples include\ncontrolling a 47 DOF humanoid to navigate a variety of\nobstacles [1], dexterously manipulating objects with a 24\nDOF robotic hand [2], and allowing a physical quadruped\nrobot to run [3], and recover from falls [4].\nDespite this, these algorithms can struggle on certain low\ndimensional problems from the nonlinear control literature.\nNamely the acrobot [5] and the cart pole pendulum. These\nare both under-actuated mechanical systems that have unsta-\nble ﬁxed points in their unforced dynamics (see section II-B).\nTypically, the goal is to bring the system to this ﬁxed point\nand keep it there. In this paper we focus on the acrobot as\nwe found less examples of model free reinforcement learning\nperforming well on this task.\nIt is not uncommon to see some variation of these systems\ntackled in various reinforcement benchmarks, but we have\nfound these problems have usually been artiﬁcially modiﬁed\nto make them easier. For example the very popular OpenAI\nGym benchmarks [6] includes an acrobot task. But the\nobjective is only to get the system in the rough area of the\nunstable ﬁxed point, and the dynamics are integrated with\na ﬁxed time-step of .2 seconds, which makes the problem\nmuch easier and unrepresentative of a physical system. We\nhave found that almost universally, modern model free rein-\nforcement learning algorithms fail to solve a more realistic\nversion of the task. Notably, the Deep Mind control suite [7]\nincludes the full acrobot problem, and all but one algorithm\n*This work was funded in part by NSF NRI award 1526424.\nSean Gillen and Katie Byl are with the Electrical and Computer Engineer-\ning Department at the University of California, Santa Barbara CA 93106\nsgillen@ucsb.edu, katiebyl@ucsb.edu. Marco Molnar is\nwith TU Berlin marco.molnar@posteo.de.\n**Source\ncode\nfor\nthis\npaper\ncan\nbe\nfound\nhere:\nhttps://github.com/sgillen/ssac\nFig. 1: System diagram for the new technique proposed in\nthis paper. Rounded boxes represent learned neural networks,\nsquared boxes represent static, hand crafted functions. The\nlocal controller is a hand designed LQR, the swing-up\ncontroller is obtained via reinforcement learning, and the\ngating function is trained as a neural network classiﬁer\nthat they tested (the exception being [8]) learned nothing, the\naverage return after training was the same as before training.\nDespite this, there are many traditional model based solu-\ntions [5], [9], that can solve this problem well. In this work\nwe do not seek to improve upon the model based solutions\nto this problem, but to extend to the class of problems that\nmodel free reinforcement learning methods can be used to\nsolve. We believe the methods used here to solve the acrobot\ncan be extended to other problems, such as making robust\nwalking policies.\nOne of the primary reasons why this problem is difﬁcult\nfor RL is that the region of state space that can be brought\nto the unstable ﬁxed point is very small, even with generous\ntorque limits. An untrained RL agent explores by taking\nrandom actions in the environment. Reaching the region of\nattraction is rare, we found that for our system, random\nactions will reach the basin of attraction for a well designed\nLQR in about 1% of trials. However an RL agent doesn’t\nhave access to a well designed LQR at the start of training,\nin addition to reaching the region where stabilization is pos-\nsible, the agent must also stabilize the acrobot for the agent\nto receive a strong reward signal. This results in successful\ntrials in this environment being extremely rare, and therefore\ntraining is in-feasibly slow and sample inefﬁcient.\nOur solution to add a predesigned balancing controller into\nthe system, this is comparatively easy to design, and can be\ndone with a linear controller. Our contribution is a novel way\nto combine this balancing controller with an algorithm that\narXiv:2012.11663v1  [cs.RO]  21 Dec 2020\nis learning the swing-up behavior. We simultaneously learn\nthe swing-up controller, and a function that switches between\nthe two controllers.\nA. Related Work\nWork done by Randolov et. al. [10] is closely related to our\nown. In that work they construct a local controller, an LQR,\nand combine it with a learned controller to swing-up and\nbalance an inverted double pendulum (similar to the acrobot\nwe study but with actuators at both joints). The primary\ndifferences between our work and theirs is that they hard\ncode the transition between their two controllers. In contrast\nwe learn our transition function online and in parallel with\nour swing-up controller.\nWork done by Yoshimoto et. al. [11], like ours, learns the\ntransition function between controllers in order to swing-\nup and balance an acrobot. However, unlike our work they\nlimit the controllers they switch between to pre-computed\nlinear functions. In contrast our work simultaneously learns\na nonlinear swing-up controller and the transition between a\nlearned and pre-computed balance controller.\nWiklednt et. al [12] too swing-up and balance an acrobot\nusing a combined neural network and LQR. However they\nonly learn to swing-up from a single initial condition,\nwhereas our method learns to solve the task from any initial\nposition.\nDoya [13] also learns many controllers using reinforce-\nment learning, and adaptively switches between them. How-\never unlike our work, the switching function is not learned\nusing reinforcement learning, but is instead selected ac-\ncording to which of the controllers currently makes best\nprediction of the state at the current point in state space.\nWe believe our model free updates will avoid the model bias\nthat can be associated with such approaches. Furthermore\nour work allows for combining learned controllers with hand\ndesigned controllers, such as the LQR.\nII. BACKGROUND\nA. Nomenclature\nWe formulate our problem as a Markov decision process,\nM = (S, A, R). At each time step t, Our agent receives\nthe current state st ∈S and chooses an action at ∈A.\nIt then receives a reward according to the reward function\nrt = R(st, at, st+1). The goal is to ﬁnd a policy π : S →\np(A = a|s) that satisﬁes:\n(1)\nπ∗= arg max\nπ\nE\n\" ∞\nX\nt=0\nγtR(st, at, st+1)\n#\nB. Acrobot\nThe acrobot is described in Figure 2. It is a double inverted\npendulum with a motor only at the elbow. We use the\nparameters from Spong [5]:\nTABLE I: Mass and inertial parameters used in simulation\nParameter\nValue\nUnits\nm1, m2\n1\nKg\nl1, l2\n1\nm\nlc1, lc2\n.5\nm\nI1\n.2\nKg*m2\nI2\n1.0\nKg*m2\nFig. 2: Diagram for the acrobot system\nThe state of this system is st = [θ1, θ2, ˙θ1, ˙θ2]. The action\nat = τ, is the torque at the elbow joint. The goal we wish to\nachieve is to take this system from any random initial state,\nto the upright state gs = [π/2, 0, 0, 0], which we will refer to\nas the goal state. To achieve this goal, we seek the maximum\nof the following reward function:\n(2)\nrt = l1 sin(θ1) + l2 sin(θ1 + θ2)\nThis was motivated by the popular Acrobot-v1 environment\n[14], We found empirically that for our algorithm this reward\nsignal led to the same solutions as the more typical ∥st −\ngs∥. However we found that some of the other algorithms\nwe compared to perform better with the sinusoidal reward\nfunction.\nWe implement the system in python (all source code\nis provided, see footnote on page one), the dynamics are\nimplemented using Euler integration with a time-step of\n.01 seconds, and the control is updated every .2 seconds.\nWe experimented with smaller timesteps and higher order\nintegrators, generally we found these made the balancing\ntask easier, but made the wall clock time for the learning\nmuch slower.\nC. Soft Actor Critic\nSoft actor critic (SAC) is an off policy deep reinforcement\nlearning algorithm shown to do well on control tasks with\ncontinuous actions spaces [15]. To aid in exploration, rather\nthan directly optimize the discounted sum of future rewards,\nSAC attempt to ﬁnd a policy that optimizes a surrogate\nobjective:\n(3)\nJsoft = E\n\" ∞\nX\nt=0\nγt\n\u0012\nRt + αH(π(·|st))\n\u0013#\nWhere H is the entropy of the policy.\nSAC introduces several neural networks for the training.\nWe deﬁne a soft value function Vφ(st), a neural network\ndeﬁned by weights φ, which approximate Jsoft given the\ncurrent state. Next we deﬁne two soft Q functions Qρ1(st, at)\nand Qρ2(st, at) which approximate Jsoft given both the\ncurrent state and the current action. Using two Q networks\nis a trick that aids the training by avoiding overestimating\nthe Q function. We must also deﬁne a target soft value\nfunction Vφ(st), which follows the value function via polyak\naveraging:\n(4)\nVφ\n+(st) = cpyVφ(st) + (1 −cpy)Vφ\nWith cpy a ﬁxed hyper parameter. We also deﬁne Πθ, a\nneural network that outputs µθ(st) and log(σθ(st)) which\ndeﬁne the probability distribution of our policy πθ. The\naction is given by:\n(5)\nat = tanh(µθ(st) + σθ(st)ϵt)\nwhere ϵt is drawn from N(0, 1).\nSAC also make use of a replay buffer D which stores\nthe tuple (st, at, rt) after policy rollouts. When it is time to\nupdate we sample randomly from this buffer, and use those\nsamples to compute our losses and update our weights.\nWith this we can deﬁne the losses for each of these\nnetworks (originated from [15])\nThe loss for our two Q functions is:\n(6)\nLQ = Est,at∼D\n\u00141\n2\n\u0010\nQρ(st, at) −ˆQ(st, at)\n\u00112\u0015\nwhere\n(7)\nˆQ(st, at) = r(st, at) + γEst+1\nh\nVφ(st+1)\ni\nOur policy seeks to minimize:\n(8)\nLπ = Est∼D,ϵt∼N(0,1) [log πθ(fθ(ϵt, st)|st)\n−Qρ1(st, fθ(ϵt, st)]\nAnd our value function:\n(9)\nLV =\nE\nst∼D\n\u00141\n2\n\u0010\nVφ(st) −ˆVφ(st)\n\u00112\u0015\nWhere\n(10)\nˆVφ = Eat∼πθ\n\u0002\nQmin(st, at) −logπθ(at|st)\n\u0003\nAnd Qmin = min(Qρ1(st, at), Qρ2(st, at))\nSAC starts by doing policy roll outs, recording the state,\naction, reward, and the active controller at each time step. It\nstores these experiences in the replay buffer. After enough\ntrials have been run, we run our update step. We sample from\nthe replay buffer, and use these sampled states to compute\nthe losses above. We then run one step of Adam [16] to\nupdate our network weights. We repeat this update nu times\nwith different samples. Finally we copy our weights to our\ntarget network and repeat until convergence (or some other\nstopping metric).\nIII. SWITCHED SOFT ACTOR CRITIC\nOur primary contribution is to extend SAC in two key\nways, we call the modiﬁed algorithm switched soft actor\ncritic (SSAC). The ﬁrst modiﬁcation is a change to the\nstructure of the learned controller in order to inject our do-\nmain knowledge into the learning. Our controller consists of\nthree distinct components. The gate function, the balancing\ncontroller, and the swing-up controller. The gate, Gγ : S →\n[0, 1], is a neural network parameterized by weights γ which\ntakes the observations at each time step and outputs a number\ngt representing which controller it thinks should be active.\ngt ≈1 implies high conﬁdence that the balancing controller\nshould be active, and gt ≈0 implies the swing-up controller\nis active. This output is fed through a standard switching\nhysteris function, to avoid rapidly switching on the class\nboundary, parameters given in the appendix. The swing-up\ncontroller can be seen as the policy network from vanilla\nSAC, the action then is determined by equation (5). The\nparameters for these networks are given in the appendix.\nThe balancing controller is a linear quadratic regulator C :\nS →A about the acrobot’s unstable equilibrium. We use the\nLQR designed by Spong [5]:\nUsing\nQ =\n\n\n\n\n1000\n−500\n0\n0\n−500\n1000\n0\n0\n0\n0\n1000\n−500\n0\n0\n−500\n1000\n\n\n\n, R =\n\u0000.5\u0001\nThe resulting control law is:\nu = −Ks\nwith\nK = [−1649.8, −460.2, −716.1, −278.2]\nThese three functions together form our policy, πθ. Al-\ngorithm 1 demonstrates how the action is computed at each\ntimestep.\nWe learn the basin of attraction for the regulator by\nframing it as a classiﬁcation problem, our neural network\ntakes as input the current state, and outputs a class prediction\nbetween 0-1. A one implying that the LQR is able to stabilize\nthe system, and a zero implying that it cannot. We then deﬁne\na threshold function T(s), as a criteria for what we consider\na successful trial:\n(11)\nT(s) = ∥st −gs∥< ϵthr\n∀t ∈{Ne −b, ..., Ne}\nHere s is understood to be an entire trajectory of states,\nNe is the length of each episode, ethr and b hyper parameters\nwith values given in the appendix. We are following the\nconvention of a programming language here, (11) returns\none when the inequality holds, and zero otherwise. To gather\ndata, we sample a random initial condition, do a policy roll\nout using the LQR, and record the value of 11 as the class\nlabel.\nTo train the gating network we minimize the binary cross\nentropy loss:\nLG = E\nγ −[cwyi log(Gγ(si)) + (1 −yi) log(1 −Gγ(si))]\n(12)\nWhere yi is the class label for the ith sample, cw is a class\nweight for positive examples. we set cw =\nnt\nnp w where nt\nis the total number of samples, np is the number of positive\nexamples, and w is a manually chosen weighting parameter\nto encourage learning a conservative basin of attraction.\nWe found that the learned basin was very sensitive to this\nparameter, a value of .01 empirically works well. Note that\nunlike the other losses above, the data here is not computed\nover a sample but is instead computed over the entire replay\nbuffer. We found the gate was prone to ”forgetting” the basin\nof attraction early in the training otherwise. This also allows\nus to update the gate infrequently compared to the other\nnetworks, and so the total impact on wall clock time is\nmodest.\nThe second extension is a modiﬁcation of the replay\nbuffer D. We do this by constructing D from two separate\nbuffers, Dn and Dr. Only roll outs that ended in a successful\nbalance (as deﬁned by equation (11)) are stored in Dr. The\nother buffer stores all trials, the same as the unmodiﬁed\nreplay buffer. Whenever we draw experience from D, with\nprobability pd we sample from Dn, and with probability\n(1 −pd) we sample from Dr. We found this to speed up\nlearning dramatically, as even with the LQR and a decent\ngating function in place, the swing-up controller ﬁnds the\nbasin of attraction only in a tiny minority of trials.\nAlgorithm 1 Do-Rollout(Gγ, Πθ, K)\n1: s = r = a = g = r = {}\n2: Reset environment, collect s0\n3: for t ∈{0, ..., T} do\n4:\ngt = hyst(Gγ(st))\n5:\nif (gt) == 1 then\n6:\nat = −Kst\n7:\nelse\n8:\nSample ϵt from N(0, 1)\n9:\nat = β tanh(µθ(st) + σθ(st) ∗ϵt)\n10:\nTake one step using at, collect {st+1, rt}\n11:\ns = s S st, r = r S rt\n12:\na = a S at, g = g S gt\n13: return s, a, r, g\nIV. RESULTS\nA. Training\nTo train SSAC we ﬁrst start by training the gate exclu-\nsively, using the supervised learning procedure outlined in\nsection III This allows us to form an estimate of the basin of\nattraction before we try to learn to reach it. We trained the\ngate for 1e6 timesteps, and then trained both in parallel using\nalgorithm 2 for another 1e6 timesteps. The policy, value,\nand Q functions are updated every 10 episodes, and the gate\nevery 1000. The disparity is because, as mentioned earlier,\nAlgorithm 2 Switched Soft Actor Critic\n1: Initialize network weights θ, φ, γ, ρ1, ρ2 randomly\n2: set φ = φ\n3: for n ∈{0, ..., Ne} do\n4:\ns, r, a, g = Do-Rollout(Gγ, Πθ, K)\n5:\nif T(s) then\n6:\nStore s, r, a in Dn\n7:\nStore s, r, a in Dr\n8:\nStore s, g, T(s) in Dg\n9:\nif Time to update policy then\n10:\nsample sr, ar, rr from D\n11:\nˆQ ≈R + γVφ(S)\n12:\nQmin = min(Qρ1(sr, ar), Qρ2(sr, ar))\n13:\nˆV ≈Qmin −αH(πθ(A|S))\n14:\nRun one step of Adam on LQ(sr, qr, rr)\n15:\nRun one step of Adam on Lπ(sr)\n16:\nRun one step of Adam on LV (sr)\n17:\nφ = qφ + (1 −q)φ\n18:\nif Time to update gate then\n19:\nRun one step of Adam on LG using all samples\nin Dg\nthe gate is updated using the entire replay buffer, while all\nthe other losses are updated with one sample batch from the\nbuffer. Hyperparameters were selected by picking the best\nperforming values from a manual search, which are reported\nin the appendix.\nIn addition to training on our own version of SAC and\nSwitched SAC we also examined the performance of several\nalgorithms written by OpenAI and cleaned up by the com-\nmunity [17]. We examine PPO and TRPO, two popular trust\nregion methods. A2C was included to compare to a non trust\nregion, modern policy gradient algorithm. We also include\nTD3, which has been shown in the literature to do well on\nthe acrobot and cartpole problems [18]\nStable baselines includes hyperparameters that were al-\ngorithmically tuned for each environment. For algorithms\nwhere parameters for Acrobot-v1 were available we chose\nthose, some algorithms were missing tuned Acrobot-v1 ex-\namples, and for those we used parameters for Pendulum-v0,\nsimply because it is another continuous, low dimensional\ntask. Note we don’t expect the hyper-parameters to impact\nthe learned policy’s score in this case, only how fast learning\noccurs. Reported rewards are averaged over 4 random seeds.\nEvery algorithm makes 2e6 interactions with the environ-\nment. Also note that this project was largely inspired by\nspending a large amount of time manually tuning these\nparameters to work on this task (with no success better than\nwhat we see here). Figure 3 shows the reward curve for our\nalgorithm and the algorithms from stable baselines. Table II\nshows the mean and standard deviation for the ﬁnal rewards\nobtained by all algorithms.\nFig. 3: Reward curve for SSAC and the other algorithms we\ncompare to. the solid line is the smoothed average of episode\nreward, averaged over four random seeds. The shaded area\nindicates the best and worst rewards at each epoch across\nthe four seeds. SSAC is shown starting later to account for\nthe time training the gating function alone.\nAlgorithm\n(implementation)\nMean Reward ± Standard\nDeviation\nSSAC (Ours)\n92.12 ± 2.35\nSAC\n73.01 ± 11.41\nPPO\n0.43 ± 8.89\nTD3\n78.67 ± 61.85\nTRPO\n17.63 ± 3.39\nA2C\n2.57 ± 3.63\nTABLE II: Rewards after training for across learning algo-\nrithms. This table shows results after 2 million environment\ninteractions\nAs we can see, for this environment, with the number\nof steps we have allotted, our approach outperforms the\nalgorithms we compared to, with TD3 making it the closest\nto our performance. This is a necessarily ﬂawed comparison.\nThese algorithms are meant to be general purpose, so it is\nunfair to compare them to something designed for a partic-\nular problem. But that is part of the point we are making,\nthat adding just a small amount of domain knowledge can\nimprove performance dramatically.\nB. Analyzing performance\nTo qualitatively evaluate the performance of our learned\nagent we examine the behavior during individual episodes.\nSSAC gives us a deterministic controller (we can set ϵt\nfrom 5 to zero). We chose the initial condition s0\n=\n(−π/2, 0, 0, 0) and record a rollout. The actions are dis-\nplayed in ﬁgure 4, and the positions in 5.\nWe have also found that despite achieving relatively high\nrewards, the other algorithms we compare to often fail\nto meet the balance criteria (11). We often see solutions\nwhere the ﬁrst link is constantly rotating, with the second\nlink constantly vertical. To demonstrate this, as well as to\nFig. 4: Torque exerted during the sampled episode\nFig. 5: Observations during the sampled episode\ndemonstrate our algorithms robustness, we run roll outs\nwith the trained agents across a grid of initial conditions,\nrecording if the trajectory satisﬁes (11) or not. We compare\nour method with TD3, which was the best performing model\nfree method we could ﬁnd on this task. Figure 6 show the\nresults, when these initial conditions were run for SSAC,\nit satisﬁed (11) for every initial condition.\nV. CONCLUSIONS\nWe have presented a novel control design methodology\nthat allows engineers to leverage their domain knowledge,\nwhile also reaping many of the beneﬁts from recent ad-\nvances in deep reinforcement learning. In our case study\nwe constructed a policy to swing-up and balance an acrobot\nwhile only needing to manually design a linear controller for\nthe balancing task. We believe this method of control will\nbe straightforward to apply to the double or triple cartpole\nproblems, which to our knowledge no model free algorithm\nis reported as solving. We also think that this general\nmethodology can be extended to more complex problems,\nsuch as legged locomotion. In that case the linear controller\nhere could be a nominal walking controller obtained via\nFig. 6: Balance map for TD3, X and Y indicate the initial\nposition for the trial, a black dot indicates that the trial started\nfrom that point satisﬁes equation (11), and red indicates the\nconverse. when these initial conditions were run for SSAC,\nit satisﬁed (11) for every initial condition\ntrajectory optimization, and the learned controller could be\na recovery controller to return to the basin of attraction of\nthis nominal controller.\nREFERENCES\n[1] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa,\nT. Erez, Z. Wang, S. M. A. Eslami, M. Riedmiller, and D. Silver,\n“Emergence of Locomotion Behaviours in Rich Environments,”\narXiv:1707.02286 [cs], July 2017, arXiv: 1707.02286. [Online].\nAvailable: http://arxiv.org/abs/1707.02286\n[2] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz,\nB. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell,\nA. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng,\nand W. Zaremba, “Learning Dexterous In-Hand Manipulation,”\narXiv:1808.00177\n[cs,\nstat],\nAug.\n2018f,\narXiv:\n1808.00177.\n[Online]. Available: http://arxiv.org/abs/1808.00177\n[3] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis,\nV. Koltun, and M. Hutter, “Learning agile and dynamic motor skills\nfor legged robots,” Science Robotics, vol. 4, no. 26, p. eaau5872,\nJan. 2019. [Online]. Available: http://robotics.sciencemag.org/lookup/\ndoi/10.1126/scirobotics.aau5872\n[4] J. Lee, J. Hwangbo, and M. Hutter, “Robust Recovery Controller\nfor a Quadrupedal Robot using Deep Reinforcement Learning,”\narXiv:1901.07517 [cs], Jan. 2019, arXiv: 1901.07517. [Online].\nAvailable: http://arxiv.org/abs/1901.07517\n[5] M. W. Spong, “Swing up control of the acrobot using partial\nfeedback linearization *,” IFAC Proceedings Volumes, vol. 27,\nno.\n14,\npp.\n833–838,\nSept.\n1994.\n[Online].\nAvailable:\nhttps:\n//linkinghub.elsevier.com/retrieve/pii/S1474667017474040\n[6] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” 2016.\n[7] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. de Las Casas,\nD.\nBudden,\nA.\nAbdolmaleki,\nJ.\nMerel,\nA.\nLefrancq,\nT.\nLillicrap,\nand\nM.\nRiedmiller,\n“DeepMind\ncontrol\nsuite,”\nhttps://arxiv.org/abs/1801.00690, DeepMind, Tech. Rep., Jan. 2018.\n[Online]. Available: https://arxiv.org/abs/1801.00690\n[8] G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan,\nD.\nTB,\nA.\nMuldal,\nN.\nHeess,\nand\nT.\nLillicrap,\n“Distributed\nDistributional Deterministic Policy Gradients,” arXiv:1804.08617\n[cs,\nstat],\nApr.\n2018,\narXiv:\n1804.08617.\n[Online].\nAvailable:\nhttp://arxiv.org/abs/1804.08617\n[9] M. W. Spong, “Energy Based Control of a Class of Underactuated\nMechanical Systems,” IFAC Proceedings Volumes, vol. 29, no. 1,\npp. 2828–2832, June 1996. [Online]. Available: https://linkinghub.\nelsevier.com/retrieve/pii/S1474667017581057\n[10] J. Randl ˜Aˇzv, A. G. Barto, and M. T. Rosenstein, “Combining\nReinforcement\nLearning\nwith\na\nLocal\nControl\nAlgorithm,”\nin\nProceedings of the Seventeenth International Conference on Machine\nLearning, ser. ICML ’00.\nSan Francisco, CA, USA: Morgan\nKaufmann Publishers Inc., 2000, pp. 775–782. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=645529.657804\n[11] J. Yoshimoto, M. Nishimura, Y. Tokita, and S. Ishii, “Acrobot control\nby learning the switching of multiple controllers,” Artiﬁcial Life and\nRobotics, vol. 9, no. 2, pp. 67–71, May 2005. [Online]. Available:\nhttp://link.springer.com/10.1007/s10015-004-0340-6\n[12] L. Wiklendt, S. Chalup, and R. Middleton, “A small spiking neural\nnetwork with LQR control applied to the acrobot,” Neural Computing\nand Applications, vol. 18, no. 4, pp. 369–375, May 2009. [Online].\nAvailable: http://link.springer.com/10.1007/s00521-008-0187-1\n[13] K. Doya, K. Samejima, K.-i. Katagiri, and M. Kawato, “Multiple\nModel-Based Reinforcement Learning,” Neural Computation, vol. 14,\nno.\n6,\npp.\n1347–1369,\nJune\n2002.\n[Online].\nAvailable:\nhttp:\n//www.mitpressjournals.org/doi/10.1162/089976602753712972\n[14] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford,\nJ. Schulman, S. Sidor, Y. Wu, and P. Zhokhov, “Openai baselines,”\nhttps://github.com/openai/baselines, 2017.\n[15] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic:\nOff-Policy Maximum Entropy Deep Reinforcement Learning with a\nStochastic Actor,” arXiv:1801.01290 [cs, stat], Aug. 2018, arXiv:\n1801.01290. [Online]. Available: http://arxiv.org/abs/1801.01290\n[16] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic\nOptimization,” arXiv:1412.6980 [cs], Dec. 2014, arXiv: 1412.6980.\n[Online]. Available: http://arxiv.org/abs/1412.6980\n[17] A. Hill, A. Rafﬁn, M. Ernestus, A. Gleave, A. Kanervisto, R. Traore,\nP. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford,\nJ. Schulman, S. Sidor, and Y. Wu, “Stable baselines,” https://github.\ncom/hill-a/stable-baselines, 2018.\n[18] T.\nP.\nLillicrap,\nJ.\nJ.\nHunt,\nA.\nPritzel,\nN.\nHeess,\nT.\nErez,\nY. Tassa, D. Silver, and D. Wierstra, “Continuous control with\ndeep reinforcement learning,” arXiv:1509.02971 [cs, stat], Sept.\n2015, arXiv: 1509.02971. [Online]. Available: http://arxiv.org/abs/\n1509.02971\nAPPENDIX\nHyperparameters\nHyperparameter\nValue\nEpisode length (Ne)\n50\nExploration steps\n5e4\nInitial policy/value learning rate\n1e-3\nSteps per update\n500\nReplay batch size\n4096\nPolicy/value minibatch size\n128\nInitial gate learning rate\n1e-5\nWin criteria lookback (b)\n10\nWin criteria threshold (ϵthr)\n.1\nDiscount (γ)\n.95\nPolicy/value updates per epoch\n4\nGate update frequency\n5e4\nNeedle lookup probability pn\n.5\nEntropy coefﬁcient (α)\n.05\nPolyak constant (cpy)\n.995\nHysteresis on threshold\n.9\nHysteresis off threshold\n.5\nNetwork Architecture\nThe policy, value, and Q networks are all made of four\nfully connected layers, with 32 hidden nodes and Relu\nactivations. The gate network is composed of two hidden\nlayers with 32 nodes each, also with Relu activations, the last\noutput is fed through a sigmoid to keep the result between\n0-1.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2020-12-21",
  "updated": "2020-12-21"
}