{
  "id": "http://arxiv.org/abs/2012.13321v2",
  "title": "Unsupervised deep clustering and reinforcement learning can accurately segment MRI brain tumors with very small training sets",
  "authors": [
    "Joseph Stember",
    "Hrithwik Shalu"
  ],
  "abstract": "Purpose: Lesion segmentation in medical imaging is key to evaluating\ntreatment response. We have recently shown that reinforcement learning can be\napplied to radiological images for lesion localization. Furthermore, we\ndemonstrated that reinforcement learning addresses important limitations of\nsupervised deep learning; namely, it can eliminate the requirement for large\namounts of annotated training data and can provide valuable intuition lacking\nin supervised approaches. However, we did not address the fundamental task of\nlesion/structure-of-interest segmentation. Here we introduce a method combining\nunsupervised deep learning clustering with reinforcement learning to segment\nbrain lesions on MRI.\n  Materials and Methods: We initially clustered images using unsupervised deep\nlearning clustering to generate candidate lesion masks for each MRI image. The\nuser then selected the best mask for each of 10 training images. We then\ntrained a reinforcement learning algorithm to select the masks. We tested the\ncorresponding trained deep Q network on a separate testing set of 10 images.\nFor comparison, we also trained and tested a U-net supervised deep learning\nnetwork on the same set of training/testing images.\n  Results: Whereas the supervised approach quickly overfit the training data\nand predictably performed poorly on the testing set (16% average Dice score),\nthe unsupervised deep clustering and reinforcement learning achieved an average\nDice score of 83%.\n  Conclusion: We have demonstrated a proof-of-principle application of\nunsupervised deep clustering and reinforcement learning to segment brain\ntumors. The approach represents human-allied AI that requires minimal input\nfrom the radiologist without the need for hand-traced annotation.",
  "text": "Unsupervised deep clustering and reinforcement\nlearning can accurately segment MRI brain\ntumors with very small training sets\nJoseph Stember1\nHrithwik Shalu2\nMarch 22, 2021\n1Memorial Sloan Kettering Cancer Center, New York, NY, US, 10065\n2Indian Institute of Technology, Madras, Chennai, India, 600036\n1joestember@gmail.com\n2lucasprimesaiyan@gmail.com\nAbstract\nPurpose Lesion segmentation in medical imaging is key to evaluating\ntreatment response. We have recently shown that reinforcement learning\ncan be applied to radiological images for lesion localization. Furthermore,\nwe demonstrated that reinforcement learning addresses important limita-\ntions of supervised deep learning; namely, it can eliminate the requirement\nfor large amounts of annotated training data and can provide valuable in-\ntuition lacking in supervised approaches. However, we did not address\nthe fundamental task of lesion/structure-of-interest segmentation. Here\nwe introduce a method combining unsupervised deep learning clustering\nwith reinforcement learning to segment brain lesions on MRI.\nMaterials and Methods We initially clustered images using unsuper-\nvised deep learning clustering to generate candidate lesion masks for each\nMRI image. The user then selected the best mask for each of 10 training\nimages. We then trained a reinforcement learning algorithm to select the\nmasks. We tested the corresponding trained deep Q network on a separate\ntesting set of 10 images. For comparison, we also trained and tested a U-\nnet supervised deep learning network on the same set of training/testing\nimages.\nResults Whereas the supervised approach quickly overﬁt the training\ndata and predictably performed poorly on the testing set (16% average\nDice score), the unsupervised deep clustering and reinforcement learning\nachieved an average Dice score of 83%.\nConclusion We have demonstrated a proof-of-principle application of\nunsupervised deep clustering and reinforcement learning to segment brain\ntumors. The approach represents human-allied AI that requires minimal\ninput from the radiologist without the need for hand-traced annotation.\narXiv:2012.13321v2  [cs.CV]  19 Mar 2021\nIntroduction\nSegmentation of lesions, organs, or other structures-of-interest is an integral\ncomponent of artiﬁcial intelligence (AI) in radiology [11, 16, 10]. Essentially all\nAI segmentation, like the tasks of localization and classiﬁcation, falls within the\ncategory of supervised deep learning.\nA supervised deep learning research project focusing on segmentation typi-\ncally begins by accruing and pre-processing a large number of appropriate im-\nages. As a general rule, hundreds of images are required for successful training.\nNext, the process of annotation begins, which entails radiologists or other imag-\ning researchers tracing outer contours around each structure-of-interest, thereby\ncreating a mask. Once enough masks have been acquired, the data is typically\n\"augmented\" via a series of rotations, scaling, translations, and/or the addi-\ntion of random pixel noise to artiﬁcially produce a larger training set. At this\npoint, the data set is divided between a training/validation set, which comprises\nthe vast majority of the image data set, and a smaller separate testing set. A\nconvolutional neural network (CNN) is trained by repeated forward and back-\npropagation through the CNN with the training set images as input and output\ncompared to masks for the loss. The two most common CNN architectures for\nsegmentation are mask-regional-CNN (mask-RCNN) [7] and U-net [15].\nWe sought to address three key shortcomings in current supervised deep\nlearning approaches:\n1. Requirement of large amounts of expert-annotated data. This can be ex-\npensive, tedious, and time-consuming. For example, it has been estimated\nthat segmenting 1, 000 images may require a month of full-time work by\ntwo experts [3].\n2. Lack of generalizability, making the algorithm “brittle” and subject to\ngrossly incorrect predictions when even a small amount of variation is\nintroduced. This can occur when applying a trained network to images\nfrom a new scanner, institution, and/or patient population [24, 6].\n3. Lack of insight or intuition into the algorithm, which limits the conﬁdence\nneeded for clinical implementation and restricts potential contributions\nfrom non-AI experts with advanced domain knowledge (e.g., radiologists\nor pathologists) [2, 9].\nIn recent work [19, 20], we introduced the concept of radiological reinforce-\nment learning (RL): the application of reinforcement learning to analysis of\nmedical images. We showed that RL can address these challenges by applying\nRL to lesion localization.\nHowever, segmentation can provide additional information such as lesion vol-\nume, which is particularly useful for follow-up determining radiological response\nto treatment. Here we apply RL to the task of segmentation. In order to do so,\nwe leverage another important, though less prevalent, branch of AI in radiology:\nunsupervised deep learning. Unsupervised deep learning performs data cluster-\ning, grouping the data sets into diﬀerent classes. The identity of these classes is\nleft to human users to determine, thereby providing post-training \"supervision.\"\nHence we proceed in two major phases:\n• Use unsupervised clustering CNN or unsupervised deep clustering (UDC)\nto generate clusters that are candidate lesion masks. The user then selects\nthe cluster that serves as the best mask. Then for this proof-of-principle\napplication, we simplify by reducing the tiling of each image to two clus-\nters, the user-selected mask and its complement, for the next step in the\ntraining set.\n• Using a training set comprised of lesions and masks obtained from UDC,\ntrain a deep reinforcement learning (RL) deep Q network (DQN) in tan-\ndem with Q-learning to predict the best cluster to serve as lesion mask.\nUsing this method, we produce lesion segmentations with minimal input\nfrom the user. The user is spared from manually tracing out a single lesion\nborder. Additionally, given the data eﬃciency of RL, even the user eﬀort to\nselect masks from the UDC step can be minimized. This is due to the ability\nof RL to train eﬀectively on very small data sets, as we have seen in our recent\nwork [19, 20]. We will show in this paper that we were able to produce accurate\nlesion segmentations with only 10 user-directed mask selections.\nMethods\n0.1\nOverview\nWe can divide our approach into ﬁve key steps:\n• Collect 20 glioma MRI 2D images. The ﬁrst 10 will be used for the training\nset, and the remaining 10 will comprise the testing set.\n• Use an unsupervised deep clustering (UDC) network to generate a set of\npossible lesion masks.\n• User selects appropriate mask.\n• As above, obtain 10 training set images + masks.\n• Train a reinforcement learning (RL) algorithm using Q learning and a\ndeep Q network to select the correct lesion masks automatically.\n0.2\nData collection\nWe collected 20 publicly available T2 FLAIR images of gliomas as screen cap-\ntures using web sites accessed via Google Images [5, 13, 8, 17, 12] as well as\npublished articles [18, 14, 4, 27, 25]. As we used only publicly available images,\nIRB approval was deemed unnecessary for this study.\n0.3\nClustering\n0.3.1\nSuperpixel generation\nConsider for example one of the training images.\nWe start by generating a\ntiling of our image using the SLIC method [26, 1]. This approach separates\nthe image into Nsp superpixels using nearest-neighbors with the features of\nred-green-blue color channel pixel intensity, each in the range of 0 to 255 and\nx, y position.\nThis tends to cluster regions of the image that are of similar\ncolor and are spatially close. An example superpixel tiling is displayed on the\nright image in Figure 3.\nIn order to ensure small enough superpixels so as\nnot to artifactually group together disparate regions, we used a high number\nof superpixels Nsp = 1 × 104. The python function skimage.segmentation.slic\nproduces the tiling ultimately with a number of superpixels close to Nsp. We\nset a parameter called compactness equal to 100 to balance the inﬂuence of\ncolor proximity and space proximity. The SLIC tiling forms a scaﬀolding for\nsubsequent steps.\n0.3.2\nClustering CNN\nWe run a forward pass of the convolutional neural network (CNN) shown in\nFigure 2. Part of a sample output of the forward pass is displayed as the upper\nleft panel in Figure 3. The target is produced from this output by making the\nmost numerous features selected for each superpixel the only feature present for\neach pixel within that superpixel. The target obtained from the sample output\nof Figure 3 is shown as the lower left image of Figure 3.\nThen cross entropy between target and output provides pixel-wise classiﬁca-\ntion CNN loss, which we minimize via backpropagation. The CNN thus learns\nduring training to predict features for which all or as many as possible pixels in\neach superpixel have the same value. In other words, as the network trains, it\nbecome more conﬁdent about the categories to which small regions (superpixels)\nin the image belong. Then, grouping the superpixels together provides us with\nlarger image clusters. Ours is thus an agglomerative (\"bottom up\") hierarchical\nimage clustering.\nBecause the network in Figure 2 is unsupervised, it does not know the cat-\negory or label of the clusters. However, it does learn that certain tiny regions\nbelong together in the same cluster and class. Then it is up to the user to assign\nactual identities to these regions. A key step in our approach is to minimize the\nburden of user or expert oversight needed to inform the unsupervised network,\nthereby guiding it toward more meaningful predictions.\nOur network, as shown in Figure 2, consists of three convolutional layers.\nWe use 3×3 kernals with NC = 100 channels and zero padding to maintain the\nH × W dimensions of the convolutional layers. Hence these layers are of size\nH×W×NC = 240×240×100. Because the third layer also has these dimensions,\nthe NC channel represents a 100-CNN-feature embedding. This provides higher\norder features than the simpler red-green-blue and x, y coordinates, and the most\nimportant features for clustering are determined by backpropagation. As stated\nabove, loss is calculated as the cross entropy between CNN output and target.\nSince the target for each superpixel can be speciﬁed via one hot encoding vector,\nthe closer all the CNN output pixels in the superpixel are to being of the same\nclass and the class matching the one hot encoding vector, the higher the log and\nthe lower the negative sum of log values, thus the lower the loss.\nTraining the clustering CNN is somewhat unique in that it ends when the\nnumber of clusters/classes, Ncl, reaches 25. The clustering CNN trains to make\noutput pixel features more homogeneous within superpixels, but also across\nsuperpixels. This represents the agglomerative building up of increasingly larger\nand \"higher level\" clusters, starting from the tiny superpixels (roughly 240 ×\n240 pixels ÷ 10, 000 superpixels ≈6 pixels per superpixel) and building up to\nwhat would ultimately be Ncl = 1 (all of the image being in one class). Then it\nis up to the user to determine the optimal number of classes at which to stop the\ntraining. Based on trial and error, we found that Ncl = 25 is a good number of\nclasses that produces masks that encompass important structures, including the\nlesions. Hence, we set training to conclude as soon as Ncl reaches 25. Training\nto the goal of 25 classes typically takes on the order of 20 epochs, comprising a\nfew seconds of training time.\nSince we want the user to interactively oversee the selection of lesion masks\nand to reduce the need to wait around for very long, we want the network to\ntrain almost instantaneously. As such, candidate lesion masks can be provided\nby the CNN from Figure 2 and the user can select the correct masks in real\ntime.\nIn order to achieve fast training, we use the relatively large learning\nrate of 0.1.\nHowever, with this higher learning rate, we must mitigate the\nrisk that the tapering oﬀin Ncl during training could occur too quickly, or\n\"skip\" over intermediate values. Were this to occur, intermediate-sized features\nin the images might not be adequately factored into the ultimate clustering,\ncomprising a loss of valuable information. In order to counteract the possibility\nof skipping over intermediate-sized clusters, we apply batch normalization after\neach convolutional layer.\nThis keeps the network weights from growing too\nlarge.\nWe also use stochastic gradient descent as the optimization iterative\nmethod with the standard momentum value of 0.9. Unlike other optimization\nalgorithms, stochastic gradient descent has no acceleration, which could combine\nwith the large learning rate to make training unstable.\n0.3.3\nEvaluating clusters and selecting best masks\nFor each image, having generated Nclusters, we remove small candidate masks\nthat are likely artifactual. We do so by requiring that all candidate masks be\nover 1×103 pixels large. Furthermore, to avoid candidates that are overly large,\nwe include an upper size boundary, which is the total number of pixels in the\nimage minus 1 × 103 pixels. We then display all candidate masks for the viewer\nto select the best. This is displayed in Figure 4. In that case, for example, the\nuser would instantly recognize that the ﬁrst image contains the best mask. We\nimagine that in future implementations users will click on or dictate the best\nimage in a ﬂuid and eﬃcient process. However, for this proof of principle, the\nuser recorded the best image number by entering it into a python array. In our\nproof of principle, we use the center of the mass point for each mask as this\nﬁduciary point. Any point, even one selected at random from the image, could\nserve as this ﬁduciary point.\nWe can imagine the user clicking on the best cluster for lesion mask selection.\nThis would provide for the recording of a point within the lesion. We use this\nas a ﬁducial point pﬁd.\nProceeding as above for each of the 10 training set images, we obtain an\narray of length 10 containing indices of the best mask selections for each image.\nThen we are able to re-produce the 10 best masks to accompany the raw images\nby calling the array. Having obtained the 10 training masks, we now turn to the\ntask of training a separate network to detect and segment lesion masks without\nany user interaction.\n0.4\nReinforcement learning for lesion segmentation\n0.4.1\nReinforcement learning environment and deﬁnition of states,\nactions, and rewards\nState space\nHere the various states are the diﬀerent clusters as produced by\nthe unsupervised CNN, as in Figure 2. For example in Figure 4, the four possible\nstates are the four sub-ﬁgures. The state that corresponds to the correct lesion\nmask is the ﬁrst (upper left) sub-ﬁgure. In general, for an image tiling consisting\nof Ncl clusters, there are Ncl diﬀerent states. For the sake of simplicity in this\nproof-of-principle, we restrict the state space to two possible clusters with a\ncluster overlaying the mask M and its complement MC.\nAs constructed thus far, our scenario ﬁts the framework of a standard multi\n(two)-armed bandit problem [23]. This being the case, we would not have the\nmulti-step sequence of states si, which would be required to employ the Bellman\nEquation for updating Q. We thus allow for ﬁve subsequent states to be explored\nper episode of training. As such, ﬁve transitions can be stored in the replay\nmemory buﬀer of each episode.\nAs in Figure 4, we overlay color-coded clusters onto our grayscale images to\nrepresent states st. The initial input image s1 shows the lesion mask colored in\nred, as shown in Figure 5. For subsequent steps of environmental sampling for\nQ learning, we can in general at time t deﬁne the state st by:\nst =\n(\nred mask\nif at−1 = 1\ngreen mask\nif at−1 = 2,\n(1)\nwhere at−1 is the action taken in the previous step.\nAction space\nThe actions our agent can take consist of selecting the cluster\nin the image tiling that represents the lesion mask. In general, there are Ncl\npossible actions. In the simpliﬁed environment we evaluate in the present work,\nNcl = 2 so that the action represents selection of either M or MC.\nMore\nspeciﬁcally, the action selects a cluster as lesion mask by selecting the region to\nwhich pf belongs. For the training set images, since we selected the center of\nmass as the points of interest, these are all inside the lesion masks.\nIn other words, the action space is generally A ∈NNcl\n0\n, but in our simpliﬁed\ncase A ∈N2\n0 is deﬁned by:\nA =\n\u0012\n1\n2\n\u0013\n=\n\u0012 predict pf ∈M\npredict pf ∈MC\n\u0013\n.\n(2)\nReward structure\nWe seek to reward and incentivize choosing the correct\ncluster while penalizing incorrect cluster selection. The reward scheme we use\nis thus given by:\nrt =\n\n\n\n\n\n\n\n\n\n+1\nif at−1 = 1 and point is within the lesion\n+1\nif at−1 = 2 and point is outside the lesion\n−1\nif at−1 = 1 and point is outside the lesion\n−1\nif at−1 = 2 and point is within the lesion.\n(3)\n0.4.2\nTraining: Deep Q network\nAs in our recent work [19, 20], we use a convolutional neural network (CNN)\nthat we term a Deep Q network (DQN) to approximate the function for Qt(a).\nThe architecture of our DQN is displayed in Figure 5 and is very similar to\nDQNs used in our recent work. As before, it employs 3 × 3 kerns with stride\nof 2 and padding such that resulting ﬁlter sizes are unchanged.\nThere are\nfour convolutional layers, following up exponential linear unit (elu) activation\nfunctions. The last convolutional layer is followed by fully connected layers,\nultimately producing a two-node output. The two output nodes correspond to\nthe set of two Q(s, a) values, which depend on the state s and the two possible\nactions at = 1, 2. Q(s, a) is well-known from reinforcement learning as the action\nvalue function. We use mean squared error loss with batch size of nbatch = 16\nand learning rate of 1 × 10−4.\nOur DQN loss is the diﬀerence between output Q values, QDQN, and the\n“target” Q value, Qtarget. The former is computed by a forward pass, FDQN(st)\nof the DQN, Q(t)\nDQN = FDQN(st).\nThe latter is computed by sampling the\nenvironment via the Bellman equation / temporal diﬀerence learning, as below.\n0.4.3\nQ learning via TD(0) temporal diﬀerence learning\nAs training learns the functional approximation of Q, bringing QDQN closer to\nQtarget, we wish to optimize the latter toward the best possible Q value, Q⋆,\nby sampling from the environment. As in our recent work, we achieve this via\ntemporal diﬀerence Q learning in its simplest form: TD(0). With TD(0), we\ncan update Q(t)\ntarget by way of the Bellman Equation:\nQ(t)\ntarget = rt + γmaxaQ(st+1, a),\n(4)\nwhere γ is the discount factor and maxaQ(st+1, a) is equivalent to the state\nvalue function V (st+1). The most important part of the environment sampled\nis the reward value rt.\nOver time, with this sampling, Q(t)\ntarget converges toward the optimal Q func-\ntion, Q⋆. In our implementation, for each episode, the agent was allowed to\nsample the image for 20 steps. We set γ = 0.99, a frequently used value that,\nbeing close to 1, emphasizes current and next states but includes those further\nin the future.\nQ learning is oﬀ-policy in the sense that the policy for sampling state-action\nspace is not the same as the policy followed to select new actions. As in our\nrecent work [19, 20], we select each action at at time step t according to the oﬀ-\npolicy epsilon-greedy algorithm, which seeks to balance exploration of various\nstates by exploiting known best policy, according to:\nat =\n(\nmaxa∈A{Qt(a)}\nwith probability ϵ\nrandom action in A\nwith probability 1 −ϵ.\n(5)\nfor the parameter ϵ < 1.\nWe used an initial ϵ or 0.7 to allow for adequate\nexploration. As Q learning proceeds and we wish to increasingly favor exploita-\ntion of a better known and more optimal policy, we set ϵ to decrease by a rate\nof 1 × 10−4 per episode. The decrease continued down to a minimum value\nϵmin = 1 × 10−4, so that some amount of exploring would always take place.\n0.4.4\nReplay memory buﬀer\nAs in our previous work [19, 20], and as per standard Q learning [23], we store\nstates, actions taken, next states, and rewards from the actions as transition\nvalues. More formally, for time t, we store the given state st, action at, resulting\nin reward rt and bringing the agent to new state st+1. We collect these values in\na tuple, called a transition, Tt = (st, at, rt, st+1). For each successive time step,\nwe stack successive transitions as rows in a transition matrix T up to a maximum\nsize of Nmemory = 1, 800 rows. This is the replay memory buﬀer, which allows\nthe DQN to learn from past experience sampling from the environments of\nthe training images and states. After reaching Nmemory = 1, 800 rows, ∀t >\nNmemory, new rows Tt are added while earlier rows are removed from T.\nDuring DQN training, batches of batch size nbatch transitions are randomly\nsampled from T. This allows a thorough and well distributed sampling of envi-\nronments and states so that QDQN is as generalized as possible.\nIn all, we sample for Nepisodes = 300 episodes, each consisting of ﬁve succes-\nsive states, each of which begins with s1, which is randomly selected for each\nepisode among the 10 training sets.\n1\nResults\n1.1\nApplication of trained UDC and RL to testing set\nFor each of the 10 testing set images, we predict to which cluster (M or MC) our\nﬁdelity point pf belongs. In this case, for simplicity, we used the lesion mask\ncenter of mass points. For each image, we generate a state as we did for the\ntraining images. Then we run each state through the trained DQN and extract\nthe best action as the index of the larger of the two predicted Q values, i.e.,\nargmax(QDQN). This selects the predicted lesion mask.\n1.2\nTraining a U-net for comparison\nAs in our previous work, we seek to compare the performance of RL to that\nof supervised deep learning [19, 20]. As the task here is lesion mask genera-\ntion/segmentation, the supervised CNN we use is the U-net architecture that\nwe previously applied successfully to segment brain aneurysms [21] and menin-\ngiomas [22].\nWe trained the 16-layer U-net on our training set of 10 images and corre-\nsponding masks for 50 epochs. We use epoch in analogy to the episodes from\nRL. We used a batch size of four, Adam optimizer with learning rate of 1×10−5,\nand loss function given by the negative Dice similarity score between network\noutput and hand annotated mask.\nThese parameters and network architecture provided accurate segmentation\nin our prior work when provided with training set sizes in the hundreds, further\nincreasing by data augmentation [21, 22].\nHowever, in this case, given the\nextremely small training set size, the network began overﬁtting the training\ndata early in the training process and badly overﬁtted the training set, an\nunsurprising result. The result was not generalizable to the separate testing\nset, for which the Dice score was 16%.\n1.3\nComparison between unsupervised deep clustering and\nreinforcement learning segmentation versus supervised\ndeep learning/U-net\nThe average Dice similarity score of UDC and RL segmentation was 83%, while\nthat of the trained deep supervised network with U-net architecture was 16%.\nThe diﬀerence was statistically signiﬁcant with a p-value of 5.3×10−14. A visual\ncomparison of the performance of the two trained networks is shown as a box\nplot in Figure 9.\n2\nDiscussion\nWe have shown that a combination of unsupervised deep clustering and deep\nreinforcement learning can produce accurate lesion segmentations. Furthermore,\nit is able to do so with a very small training set, similar to results we saw in\nrecent lesion localization work with RL [20, 23]. Even with a small training\nset, the user was only required to select the best mask from the candidates\nproduced by the unsupervised clustering CNN. The user did perform a single\ncontour tracing annotation.\nThis was an initial proof-of-principle work with some important limitations\nand goals for future work. For simplicity, we restricted the number of clusters\nproduced by the clustering CNN to Ncl 2 clusters/classes, the lesion mask, and\nits complement. In general, the clustering CNN would tile the image into Ncl\nclusters. Through trial and error, we found that for these images, around 25\nclusters gave the best segmentation of important structures. Our general ap-\nproach would be a slight generalization of that presented here, using Ncl possible\nactions, which would select the cluster of interest. The cluster ultimately se-\nlected would be the predicted mask. The initial tiling into superpixels by SLIC\nwas just one of many initial schemes that can be incorporated into a CNN-based\nclustering algorithm. Others may prove more eﬀective for the types of images\nwe are analyzing, and a comparison between diﬀerent approaches is a topic for\nfuture work.\nFinally, our ultimate goal is extension of this approach to fully three-dimensional\nimage stacks.\nThis would provide lesion volumes, as opposed to the two-\ndimensional areas.\nConﬂicts of interest\nThe authors have pursued a provisional patent based on the work described\nhere.\nReferences\n[1]\nRadhakrishna Achanta et al. “SLIC superpixels compared to state-of-the-\nart superpixel methods”. In: IEEE transactions on pattern analysis and\nmachine intelligence 34.11 (2012), pp. 2274–2282.\n[2]\nVanessa Buhrmester, David Münch, and Michael Arens. “Analysis of ex-\nplainers of black box deep neural networks for computer vision: A survey”.\nIn: arXiv preprint arXiv:1911.12116 (2019).\n[3]\nGabriel Chartrand et al. “Deep learning: a primer for radiologists”. In:\nRadiographics 37.7 (2017), pp. 2113–2131.\n[4]\nLC Hygino Da Cruz et al. “Pseudoprogression and pseudoresponse: imag-\ning challenges in the assessment of posttreatment glioma”. In: American\nJournal of Neuroradiology 32.11 (2011), pp. 1978–1985.\n[5]\nFrank Gaillard. Cases. Last accessed November 10, 2020. 2020. url: https:\n//radiopaedia.org/cases/9015/studies/9774?lang=us.\nFigure 1: Overview of the training scheme.\n[6]\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. “Explaining\nand harnessing adversarial examples”. In: arXiv preprint arXiv:1412.6572\n(2014).\n[7]\nKaiming He et al. “Mask r-cnn”. In: Proceedings of the IEEE international\nconference on computer vision. 2017, pp. 2961–2969.\n[8]\nPublicly available web image. Glioma. Last accessed November 10, 2020.\n2020. url: https://www.slideshare.net/yashika54/fig-21b-axial-\nt1-weighted-wtd-mri.\n[9]\nXiaoxuan Liu et al. “A comparison of deep learning performance against\nhealth-care professionals in detecting diseases from medical imaging: a\nsystematic review and meta-analysis”. In: The lancet digital health 1.6\n(2019), e271–e297.\n[10]\nMaciej A Mazurowski et al. “Deep learning in radiology: An overview of\nthe concepts and a survey of the state of the art with focus on MRI”. In:\nJournal of magnetic resonance imaging 49.4 (2019), pp. 939–954.\n[11]\nMorgan P McBee et al. “Deep learning in radiology”. In: Academic radi-\nology 25.11 (2018), pp. 1472–1480.\n[12]\nSuyash Mohan. Advanced Neuroimaging of Brain Tumors Radiogenomics,\nBiomarkers & Response Assessment. Last accessed November 10, 2020.\n2018. url: https://www.pennmedicine.org/cancer.\nFigure 2: Schematic of the unsupervised clustering convolutional neural net-\nwork. The raw image is fed as input into the network with convolutional layers\nvia 3 × 3 kernals and zero padding to keep subsequent layers the same H × W\nsize. The three convolutional layers have 100 channels and each pixel has 100\nfeatures. Then argmax is obtained over the 100 features for each pixel, selecting\nthe most important feature over the image.\nFigure 3: Illustration of target vs. output prediction from the network illus-\ntrated in Figure 2. On the right is a tiling of the input image. The individual\ncells are the so-called superpixels, generated by the SLIC method. For the pur-\nposes of illustration, a small subset of the image is analyzed on the left panel.\nThe CNN from Figure 2 predicts most important features of the 100-feature\nembeddings in the penultimate layer of said ﬁgure. The goal is for each pixel\nin a given superpixel to be the one type of feature. Here we illustrate three\nfeatures as three shapes. The algorithm to generate the target is to make all\npixels in a given superpixel equal to the most numerous or common feature type\npredicted in the CNN output.\nFigure 4: Set of clusters produced by the unsupervised deep clustering CNN.\nThese serve as lesion mask candidates. The user can select the best image or\nmask number; here the best mask is clearly the ﬁrst one, in the top left image.\nThat cluster M and its complement MC are then used to train the reinforcement\nlearning network. The reinforcement learning network learns to select which is\nthe best cluster (mask or its complement) to serve as lesion mask.\nFigure 5: Architecture of deep Q learning CNN to select which region should\nbe the lesion mask.\nFigure 6: Sample mask predictions. (A) shows one of the testing set images,\n(B) displays the hand annotated mask overlaid in red, (C) shows the predicted\nmask from the unsupervised deep clustering reinforcement learning approach\noverlaid in green, and (D) shows both masks overlaid, noting that their region\nof overlap becomes yellow.\nFigure 7: Training set accuracy as a function of training time. A steady and\nmonotonic increase is manifest.\nFigure 8: Test set accuracy of reinforcement learning map selection as a function\nof training time. All 10 test set images have the correct mask selected by the\nfourth episode and there is no subsequent dip in accuracy to suggest over-ﬁtting.\nBest ﬁt sigmoid function curve is shown in red.\nFigure 9: Bar plot comparison between testing set accuracy (measured as Dice\nscore) for unsupervised deep clustering and reinforcement learning map selection\n(UDC + RL) and supervised deep learning/U-net (SDL). The average value\nis given by the height of the bars, and the standard deviation is represented by\nthe error bars.\n[13]\nDavid Preston. Glioma. Last accessed November 10, 2020. 2006. url:\nhttps://case.edu/med/neurology/NR/Glioma.\n[14]\nAlexander Radbruch et al. “Relevance of T2 signal changes in the assess-\nment of progression of glioblastoma according to the Response Assessment\nin Neurooncology criteria”. In: Neuro-oncology 14.2 (2011), pp. 222–229.\n[15]\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. “U-net: Convo-\nlutional networks for biomedical image segmentation”. In: International\nConference on Medical image computing and computer-assisted interven-\ntion. Springer. 2015, pp. 234–241.\n[16]\nLuca Saba et al. “The present and future of deep learning in radiology”.\nIn: European journal of radiology 114 (2019), pp. 14–24.\n[17]\nJan Buckner Sani Kizilbash. Cancer Therapy Advisor: Central Nervous\nSystem Malignancies. Last accessed November 10, 2020. 2020. url: https:\n/ / www . cancertherapyadvisor . com / home / decision - support - in -\nmedicine/oncology/central-nervous-system-malignancies/.\n[18]\nAnja Smits et al. “Neurological impairment linked with cortico-subcortical\ninﬁltration of diﬀuse low-grade gliomas at initial diagnosis supports early\nbrain plasticity”. In: Frontiers in neurology 6 (2015), p. 137.\n[19]\nJoseph Stember and Hrithwik Shalu. “Deep reinforcement learning to de-\ntect brain lesions on MRI: a proof-of-concept application of reinforcement\nlearning to medical images”. In: arXiv preprint arXiv:2008.02708 (2020).\n[20]\nJoseph N Stember and Hrithwik Shalu. “Reinforcement learning using\nDeep Q Networks and Q learning accurately localizes brain tumors on\nMRI with very small training sets”. In: arXiv preprint arXiv:2010.10763\n(2020).\n[21]\nJoseph N Stember et al. “Convolutional neural networks for the detection\nand measurement of cerebral aneurysms on magnetic resonance angiogra-\nphy”. In: Journal of digital imaging 32.5 (2019), pp. 808–815.\n[22]\nJoseph N Stember et al. “Eye Tracking for Deep Learning Segmentation\nUsing Convolutional Neural Networks”. In: Journal of digital imaging 32.4\n(2019), pp. 597–604.\n[23]\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An in-\ntroduction. MIT press, 2018.\n[24]\nXiaoqin Wang et al. “Inconsistent Performance of Deep Learning Models\non Mammogram Classiﬁcation”. In: Journal of the American College of\nRadiology (2020).\n[25]\nPatrick Y Wen et al. “Response assessment in neuro-oncology clinical tri-\nals”. In: Journal of Clinical Oncology 35.21 (2017), p. 2439.\n[26]\nJianwei Yang, Devi Parikh, and Dhruv Batra. “Joint unsupervised learning\nof deep representations and image clusters”. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 2016, pp. 5147–\n5156.\n[27]\nJunfeng Zhang et al. “Clinical applications of contrast-enhanced perfusion\nMRI techniques in gliomas: recent advances and current challenges”. In:\nContrast media & molecular imaging 2017 (2017).\n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2020-12-24",
  "updated": "2021-03-19"
}