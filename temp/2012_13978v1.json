{
  "id": "http://arxiv.org/abs/2012.13978v1",
  "title": "MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining",
  "authors": [
    "Zhi Wen",
    "Xing Han Lu",
    "Siva Reddy"
  ],
  "abstract": "One of the biggest challenges that prohibit the use of many current NLP\nmethods in clinical settings is the availability of public datasets. In this\nwork, we present MeDAL, a large medical text dataset curated for abbreviation\ndisambiguation, designed for natural language understanding pre-training in the\nmedical domain. We pre-trained several models of common architectures on this\ndataset and empirically showed that such pre-training leads to improved\nperformance and convergence speed when fine-tuning on downstream medical tasks.",
  "text": "MeDAL: Medical Abbreviation Disambiguation Dataset for Natural\nLanguage Understanding Pretraining\nZhi Wen1, Xing Han Lu1, Siva Reddy1,2,3\n1McGill University\n2Facebook CIFAR AI Chair\n3Mila – Quebec Artiﬁcial Intelligence Institute\n{zhi.wen,xing.han.lu}@mail.mcgill.ca\nsiva@cs.mcgill.ca\nAbstract\nOne of the biggest challenges that prohibit\nthe use of many current NLP methods in\nclinical settings is the availability of public\ndatasets.\nIn this work, we present MeDAL,\na large medical text dataset curated for ab-\nbreviation disambiguation, designed for natu-\nral language understanding pre-training in the\nmedical domain. We pre-trained several mod-\nels of common architectures on this dataset\nand empirically showed that such pre-training\nleads to improved performance and conver-\ngence speed when ﬁne-tuning on downstream\nmedical tasks.\n1\nIntroduction\nRecent work in mining medical texts focus on\nbuilding deep learning models for different medical\ntasks, such as mortality prediction (Grnarova et al.,\n2016) and diagnosis prediction (Li et al., 2020).\nHowever, because of the private nature of medical\nrecords, there are few large-scale, publicly avail-\nable medical text datasets that are suitable for pre-\ntraining models, and real-world, private datasets are\noften small-scale and imbalanced. As a result, one\nof the biggest challenge in building deep learning-\nbased NLP systems for biomedical corpora is the\navailability of public datasets (Wang et al., 2018).\nTo tackle this problem, we present Medical\nDataset for Abbreviation Disambiguation for Natu-\nral Language Understanding (MeDAL)1, a large\ndataset of medical texts curated for the task of\nmedical abbreviation disambiguation, which can be\nused for pre-training natural language understand-\ning models. Figure 1 shows an example of sample\nin the dataset, where the true meaning of the abbre-\nviation ‘DHF’ is inferred from its context, and Fig-\nure 2 shows the pretraining framework. Although\nthis dataset can be used for building abbreviation-\nexpansion systems, its main purpose is to enable\n1https://github.com/BruceWen120/medal\n... for obtaining bovine liver DHF reductase in high yield and ...\n... for obtaining bovine liver  dihydrofolate  reductase in high yield and ...\nOriginal text:\nSample in MeDAL:\nDisambiguate:\n... for obtaining bovine liver  dihydrofolate  reductase in high yield and ...\ndengue hemorrhagic fever\ndihydroxyfumarate\ndiastolic heart failure\nFigure 1: A sample in the MeDAL dataset.\neffective pre-training and improve performance on\ndownstream tasks during ﬁne-tuning.\nThe motivation behind using abbreviation disam-\nbiguation as the pre-training task is two-fold. First,\nabbreviations are widely used in medical records by\nhealthcare professionals and can often be ambigu-\nous (Xu et al., 2007; Islamaj Dogan et al., 2009).2\nThe ubiquitousness of abbreviations poses a restric-\ntion on building deep learning models for medical\ntasks, such as mortality prediction (Grnarova et al.,\n2016) and diagnosis prediction (Li et al., 2020).\nSecond, we believe that understanding natu-\nral language in a knowledge-rich domain such as\nmedicine requires understanding of domain knowl-\nedge at some level, similar to how humans can\nunderstand medical text only after receiving medi-\ncal training. The abbreviation disambiguation task\nenables models to use domain knowledge to un-\nderstand the global and local context, as well as\nthe possible meanings of the abbreviation in the\nmedical domain.\nMedical abbreviation disambiguation has long\nbeen studied (Skreta et al., 2019; Li et al., 2019;\nFinley et al., 2016; Liu et al., 2018; Joopudi et al.,\n2018; Jin et al., 2019) and our work builds upon\nmany of them. In particular, our data generation\nprocess is inspired by the reverse substitution tech-\n2For example, ‘MR’ is a commonly used abbreviation\nwhich has a number of possible meanings, including ‘morphi-\nnone reductase’, ‘magnetoresistance’ and ‘menstrual regula-\ntion’, depending on the context.\narXiv:2012.13978v1  [cs.CL]  27 Dec 2020\nPre-training:\nabbreviation\ndisambiguation\nPre-trained\nmodel\nMortality\nprediction\nDiagnosis\nprediction\nTask-speciﬁc\ndata\nFine-tuning\nPre-training\nFine-tuning\nMeDAL\nInitialized \nmodel\nFigure 2: Diagram of using MeDAL for pre-training NLU models in medical domain.\nnique (Skreta et al., 2019; Finley et al., 2016).\nOur work differs from them in mainly two as-\npects. First, instead of trying to improve perfor-\nmance on abbreviation disambiguation itself, we\npropose to use it as a pre-training task for transfer\nlearning on other clinical tasks. Second, existing\ndatasets for medical abbreviation disambiguation,\nfor instance CASI (Moon et al., 2014), are small\ncompared to datasets used for general language\nmodel pre-training, and as noted by Li et al. (2019)\nsome are erroneous. Thus, we chose to construct a\nnew dataset large enough for effective pre-training.\nOur main contributions are: a) we present a large\ndataset for pre-training on the task of medical ab-\nbreviation disambiguation. b) we provide empirical\nevidence of the beneﬁt of abbreviation pre-training\nfor a wide range of deep learning architectures.\n2\nAbbreviation Disambiguation\n2.1\nDataset Summary\nThe MeDAL dataset consists of 14,393,619 articles\nand on average 3 abbreviations per article. The\nstatistics of MeDAL are summarized in Table 1.\nThe distribution of number of words and the\ndistribution of number of abbreviations are shown\nin Figure 3a and Figure 3b, respectively.\n2.2\nDataset Creation\nThe MeDAL dataset is created from PubMed ab-\nstracts which are released in the 2019 annual base-\nline.3 PubMed is a search engine that indexes sci-\nentiﬁc publications in biomedical domain. The\nPubMed corpus contains 18,374,626 valid abstracts\nwith 80 words in each abstract on average.\nWe use reverse substitution (Skreta et al., 2019)\nto generate samples without human labeling. We\nidentify full terms in text that have known abbre-\nviations and replace them with their abbreviations.\n3https://www.nlm.nih.gov/databases/download/\npubmed medline.html\n0\n500\n1000\n1500\n2000\n2500\n3000\n# of words\n100\n101\n102\n103\n104\n105\n106\n# of articles\n(a) Word count distribution\n0\n10\n20\n30\n40\n# of abbreviations\n100\n101\n102\n103\n104\n105\n106\n107\n# of articles\n(b) Abbreviation count distribution\nFigure 3: Distributions of number of words and number\nof abbreviations.\nFor reverse substitution, mappings of abbreviations\nto expansions established by Zhou et al. (2006)\nare used. Mappings where the abbreviation maps\nto only one expansion or the expansion maps to\nmultiple abbreviations are discarded, resulting in\n24,005 valid pairs of mappings. Among the valid\nmappings are 5,886 abbreviations, which means\neach abbreviation maps to about 4 expansions on\naverage.\nTo avoid completely removing all expansions\nand making them unseen to models, the expansions\nare substituted with a pre-deﬁned probability. For\nour study, expansions are substituted with a proba-\nbility of 0.3, although our processing scripts allow\nfor other values for future use.\n2.3\nPretraining\nThe task of abbreviation disambiguation is treated\nas a classiﬁcation problem, where the classes are\nall possible expansions.\nConsidering the huge size of the dataset and the\nassociated computational cost, a subset of 5 million\ndata points are sampled from the complete corpus,\nwhich are split into 3 million training samples, 1\nmillion validation samples and 1 million test sam-\nples. This subset is used throughout this study.\nWhen creating this subset, because the distri-\nbution of true expansions is highly imbalanced,\na sampling strategy is adopted which essentially\nremoves classes in increasing order of frequency\nin an iterative manner.\nThe sampling strategy\nworks in the following way: from each class label,\nNC = min(FC, T) samples that have this label\nare randomly selected, where FC is the frequency\nof that class in the unsampled dataset, and T is a\nthreshold that is computed using Algorithm 1 such\nthat each class can have at most T samples, and\nP\nC NC is equal to the total number of samples N.\nThe strategy iteratively removes classes, and at\nevery iteration decreases N′ (which corresponds to\nthe number of remaining samples) and L (which\ncorresponds to the number of labels remaining).\nThen, the rate r is calculated based on how many\nclasses L can ﬁt in the remaining N′ if each re-\nmaining L has exactly r samples. In this way, it\nis ensured that the moment the current class fre-\nquency fC being iterated is greater than the desired\nrate r, the sampling stops.\nAlgorithm 1 Compute threshold T\nRequire: array of class frequency f, N > 0\nSort f in increasing order\nL ←length(f)\nN′ ←N\nfor each fC ∈f do\nN′ ←N′ −fC\nL ←L −1\nr = round(N′/L)\nif fC ≥r then\nreturn r + 1\nend if\nend for\n3\nEvaluation Tasks\nMortality Prediction\nAs a downstream task to\nevaluate models’ performance in clinical settings,\nmortality prediction aims at predicting the mor-\ntality of a patient at the end of a hospital admis-\nsion, using ICU patient notes. The mortality predic-\ntion dataset is generated from MIMIC-III (Johnson\ntotal # of articles\n14,393,619\nmedian # of words\n150\nmean # of words\n152.47\nmedian # of abbreviations\n2\nmean # of abbreviations\n3.04\nTable 1: Statistics of the MeDAL dataset\net al., 2016). Medical notes in this MIMIC-III com-\nprise of free-form text documents written by nurses,\ndoctors, and many types of specialists, and are writ-\nten throughout the patient’s stay. Only notes writ-\nten by physicians and nurses at least twenty-four\nhours before the end of the discharge time are used,\nfor the goal is to accurately predict whether a pa-\ntient is at risk of dying by the end of the admission.\nIn order to balance positive and negative samples\n(roughly 10% of patients expire at the end of an\nadmission) while keeping as much text diversity as\npossible, we sample at most four notes from each\nsurviving patient.\nThe dataset generated has a total of 137,607 neg-\native samples and 138,864 positively-labelled notes.\nThen, using stratiﬁed random splitting, we selected\n75%/10%/15% of the patients to be included in the\ntraining/validation/test splits. As an example of\nthe ubiquitousness of abbreviations, ‘MR’ appears\n1,612 times in 1,366 samples in the test set alone.\nDiagnosis Prediction\nSimilar to mortality pre-\ndiction, diagnosis prediction aims to predict the di-\nagnoses associated with a hospital admission from\nmedical notes written during the admission. The\nsame MIMIC-III medical notes and the same splits\nfrom mortality prediction are used, with seven\ntraining samples that have no diagnosis recorded\nremoved. In MIMIC-III, diagnoses are recorded\nwith International Classiﬁcation of Diseases (ICD)\ncodes, which are standardized codes designed for\nbilling purposes. We discard minor distinctions of\nICD codes under the same category by taking the\nﬁrst three digits (for codes that start with ‘E’ or ‘V’\nthe ﬁrst four digits) of ICD codes.4 After grouping,\nthere are 1,204 unique diagnosis codes.\nTop-k recall is used for evaluation of models\nbased on the similarities to real-life medical deci-\nsion making (Choi et al., 2015), which is deﬁned\nas the number of diagnosis codes in that admis-\nsion that are present in the top k predictions of the\n4For example, codes 4800 to 4809 represent viral pneu-\nmonia of different causes, and they are grouped into one ICD\ncode 480.\nFully-connected\nlayer\nprediction\nhidden 0\nhidden 1\nhidden N\n...\nAttention layer with \nlearnable query vector\naggregated\nhidden vector\nFigure 4: Attention output layer for mortality and diag-\nnosis prediction.\nmodel, divided by the number of diagnosis codes\nin that admission in total. Note that since most ad-\nmissions have multiple diagnoses, a small k would\nresult in a top-k recall less than 100% even if all of\nthe top k predictions are correct.5 On our dataset,\nthe highest possible top-5, top-10 and top-30 recalls\nare 50.17%, 79.48% and 99.88% on validation set,\nand 49.75%, 79.23% and 99.79% on test set.\n4\nModels\nThe models are ﬁrst pre-trained on the MeDAL\ndataset, then pre-trained weights are used to initial-\nize models for training on the downstream tasks.\nWe compared this training strategy with training re-\nspective models from scratch to validate the beneﬁt\nof pre-training.\nLSTM\nBiLSTM is used as a baseline model.\nSpeciﬁcally, the BiLSTM consists of three layers\nwith hidden size of 512. Pre-trained Fasttext model\nis used for word embeddings (Bojanowski et al.,\n2017).\nLSTM + Self Attention\nTo allow for leveraging\ninformation extracted by LSTM in a ﬂexible man-\nner, soft attention layers are added on top of LSTM.\nThe attention layer is largely based on the soft at-\ntention by Bahdanau et al. (2014). Its detailed\nformulation is included in Appendix A.\nTransformers\nWe\nused\nthe\npre-trained\nELECTRA-small discriminator (Clark et al.,\n2020) as an example of Transformer-based\n(Vaswani et al., 2017) model and, since it was\nnot pre-trained on medical text, we compared\nits performance with or without pre-training on\nabbreviation disambiguation.\n5For instance, if an admission has 10 diagnoses codes, the\nhighest possible top-5 recall for it would be 5/10 = 50%\nwhich is when all of the top 5 predictions are correct.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\ntraining epoch\n0.0\n0.2\n0.4\n0.6\n0.8\nvalidation accuracy\nvalidation accuracy on abbreviation disambiguation\nLSTM\nLSTM + SA\nELECTRA\nFigure 5: Validation accuracy on abbreviation disam-\nbiguation. ‘SA’ stands for self attention layer.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\ntraining epoch\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\n0.28\ntop-5 recall\ntop-5 recall on validation set\nLSTM + SA (s)\nLSTM max (s)\nELECTRA (s)\nLSTM + SA (p)\nLSTM max (p)\nELECTRA (p)\nFigure 6: Top-5 recall on diagnosis prediction valida-\ntion set. ‘SA’ stands for self attention layer. ‘max’ rep-\nresents max-pooling output layer. ‘(s)’ and ‘(p)’ indi-\ncates whether the model is trained from scratch or pre-\ntrained, respectively.\nTask-speciﬁc Output Layer\nDepending on the\ntask, the output layer can take various forms. For\nabbreviation disambiguation, the output layer is a\nfully-connected layer, whose input is the hidden\nvector at the location of the abbreviation from the\nprevious layers and output space is all possible\nexpansions. For mortality or diagnosis prediction\nwhich are not associated with any speciﬁc token,\nhidden vectors from the previous layers need to\nbe ﬁrst aggregated into one vector. This can be\nachieved by either a pooling layer or an additional\nattention layer with a learnable query vector. Then\nthe output layer is a fully connected layer that takes\nthe aggregated vector as input. The attention out-\nput layer is illustrated in Figure 4. In preliminary\nexperiments we found attention output layer gen-\nerally improves models’ performance compared to\nmax-pooling output layer, and therefore it is used\nthroughout the rest of the study unless otherwise\nnoted.\n5\nResults\nModels’ performance on the pre-training task, ab-\nbreviation disambiguation, is shown in Figure 5.\nAs the goal is not to optimize performance on this\nModel\nValidation accuracy\nPretrained\nFrom scratch\nLSTM\n82.67%\n82.17%\nLSTM+SA\n82.46%\n80.29%\nELECTRA\n84.19%\n83.92%\nTest accuracy\nLSTM\n82.80%\n82.61%\nLSTM+SA\n82.98%\n79.96%\nELECTRA\n84.43%\n83.25%\nTable 2: Results on mortality prediction. Bold\nfont indicates the training strategy (pre-trained or\nfrom scratch) that has higher accuracy.\ntask, Figure 5 serves to conﬁrm the models are\nproperly pre-trained.\nAfter pre-training, models are ﬁne-tuned on the\ntwo downstream tasks to evaluate the beneﬁt of pre-\ntraining. On the mortality prediction task, all three\nmodels that are pre-trained perform better than their\nfrom-scratch counterparts, shown in Table 2.\nThe beneﬁt of pre-training is more signiﬁcant\non diagnosis prediction, shown in Figure 6. Both\nLSTM and LSTM + self attention perform consid-\nerably better if they pre-trained. In fact, the two\nmodels’ performance increase by more than 70%\nrelatively. While for ELECTRA the gain is not as\nsigniﬁcant, pre-training leads to faster convergence\nduring ﬁne-tuning.\nOn the two downstream tasks, experiment re-\nsults show that pre-training improves ELECTRA’s\nperformance even when the model is already fully\npre-trained on non-medical texts and is among the\nstate-of-the-art, and bring the other models’ per-\nformance close to ELECTRA’s. This shows that\npre-training on the MeDAL dataset can generally\nimproves models capabilities of understanding lan-\nguage in medical domain. The complete results\ncan be found in Appendix C.\n6\nConclusion and Discussion\nIn this work, we present MeDAL, a large dataset\non abbreviation disambiguation, designed for pre-\ntraining natural language understanding models in\nthe medical domain. We pre-trained a variety of\nmodels using common architectures and empiri-\ncally showed that such pre-training leads to im-\nprovement in performance as well as faster conver-\ngence when ﬁne-tuning on two downstream clinical\ntasks.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural Machine Translation by Jointly\nLearning to Align and Translate.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching Word Vectors with\nSubword Information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nEdward Choi,\nMohammad Taha Bahadori,\nAndy\nSchuetz, Walter F. Stewart, and Jimeng Sun. 2015.\nDoctor AI: Predicting Clinical Events via Recurrent\nNeural Networks.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020.\nELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather\nThan Generators.\nIn International Conference on\nLearning Representations.\nGregory P. Finley, Serguei V.S. Pakhomov, Reed McE-\nwan, and Genevieve B. Melton. 2016.\nTowards\nComprehensive Clinical Abbreviation Disambigua-\ntion Using Machine-Labeled Training Data. AMIA\n... Annual Symposium proceedings. AMIA Sympo-\nsium, 2016:560–569.\nPaulina Grnarova, Florian Schmidt, Stephanie L. Hy-\nland, and Carsten Eickhoff. 2016.\nNeural Docu-\nment Embeddings for Intensive Care Patient Mortal-\nity Prediction.\nR. Islamaj Dogan, G. C. Murray, A. Neveol, and\nZ. Lu. 2009.\nUnderstanding PubMed(R) user\nsearch behavior through log analysis.\nDatabase,\n2009(0):bap018–bap018.\nQiao Jin, Jinling Liu, and Xinghua Lu. 2019. Deep\nContextualized Biomedical Abbreviation Expansion.\nIn BioNLP 2019, pages 88–96. Association for Com-\nputational Linguistics (ACL).\nAlistair E.W. Johnson, Tom J. Pollard, Lu Shen,\nLi Wei H. Lehman, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo\nAnthony Celi, and Roger G. Mark. 2016. MIMIC-\nIII, a freely accessible critical care database. Scien-\ntiﬁc Data, 3(1):1–9.\nVenkata Joopudi, Bharath Dandala, and Murthy De-\nvarakonda. 2018.\nA convolutional route to abbre-\nviation disambiguation in clinical text. Journal of\nBiomedical Informatics, 86:71–78.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nMethod for Stochastic Optimization.\nIrene Li, Michihiro Yasunaga, Muhammed Yavuz\nNuzumlalı, Cesar Caraballo, Shiwani Mahajan, Har-\nlan Krumholz, and Dragomir Radev. 2019. A Neural\nTopic-Attention Model for Medical Term Abbrevia-\ntion Disambiguation.\nYue Li, Pratheeksha Nair, Xing Han Lu, Zhi Wen,\nYuening Wang, Amir Ardalan Kalantari Dehaghi,\nYan Miao, Weiqi Liu, Tamas Ordog, Joanna M.\nBiernacka, Euijung Ryu, Janet E. Olson, Mark A.\nFrye, Aihua Liu, Liming Guo, Ariane Marelli, Yuri\nAhuja, Jose Davila-Velderrain, and Manolis Kel-\nlis. 2020. Inferring multimodal latent topics from\nelectronic health records. Nature communications,\n11(1):2536.\nYue Liu, Tao Ge, Kusum S. Mathews, Heng Ji, and\nDeborah L. McGuinness. 2018.\nExploiting Task-\nOriented Resources to Learn Word Embeddings for\nClinical Abbreviation Expansion. In BioNLP 2015.\nSungrim Moon,\nSerguei Pakhomov,\nNathan Liu,\nJames O Ryan, and Genevieve B Melton. 2014.\nA sense inventory for clinical abbreviations and\nacronyms created using clinical notes and medical\ndictionary resources. Journal of the American Med-\nical Informatics Association, 21(2):299–307.\nMarta Skreta,\nAryan Arbabi,\nJixuan Wang,\nand\nMichael Brudno. 2019.\nTraining without training\ndata: Improving the generalizability of automated\nmedical abbreviation disambiguation.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need.\nYanshan Wang, Liwei Wang, Majid Rastegar-Mojarad,\nSungrim Moon, Feichen Shen, Naveed Afzal, Sijia\nLiu, Yuqun Zeng, Saeed Mehrabi, Sunghwan Sohn,\nand Hongfang Liu. 2018. Clinical information ex-\ntraction applications: A literature review.\nHua Xu, Peter D. Stetson, and Carol Friedman. 2007.\nA study of abbreviations in clinical notes. AMIA ...\nAnnual Symposium proceedings / AMIA Symposium.\nAMIA Symposium, 2007:821–825.\nW. Zhou, V. I. Torvik, and N. R. Smalheiser. 2006.\nADAM: another database of abbreviations in MED-\nLINE. Bioinformatics, 22(22):2813–2818.\nA\nAttention Layer\nFollowing Vaswani et al. (2017), the attention layer\ncan be expressed in terms of key, query and value\nvectors, denoted as ki, qi and vi respectively,\nwhere the subscript i denotes the location in the\nsequence. Speciﬁcally, the attention layer in our\nmodels is deﬁned as Equation 1.\nwij =\nexp αij\nP\nn exp αin\n(1)\nαij in Equation 1 is computed with Equation 2,\nwhere Wa and b are learnable parameters.\nαij = tanh(qi · Wa · kjT + b)\n(2)\nHere wij is the weight assigned to location j for\nlocation i. Then the output of the attention layer at\nlocation i is computed by taking the weighted sum\nof value vectors at all locations, i.e. oi = P\nn win ·\nvn, where oi denotes the output of attention layer\nat location i. Unless otherwise noted, throughout\nthis paper ki, qi and vi are all equal to the hidden\nvector at position i from the previous layer hi.\nB\nExperiment Details\nExcept for ELECTRA, the rest of the models are\ntrained with Adam optimizer (Kingma and Ba,\n2014) with learning rate of 0.001. Text is tokenized\nusing pre-trained Fasttext embeddings (Bojanowski\net al., 2017). All LSTM modules are bi-directional\nand have 3 layers, with hidden size of 512. Batch\nsize is set to 64. We experimented with various\nchoices of batch sizes, including 32, 64, 96 and\n128, and noted only minimal differences. ELEC-\nTRA is trained with Adam optimizer with learning\nrate of 0.00002 and with batch size of 16.\nC\nAdditional Experiments Results\nFigure 7 to Figure 8 show the top-10, and top-30\nrecalls on diagnosis prediction, respectively. Table\n3 shows the complete performance of models on\ndiagnosis prediction.\nModel\nValidation performance\nTop-5 recall\nTop-10 recall\nTop-30 recall\nPre-trained\nFrom scratch\nPre-trained\nFrom scratch\nPre-trained\nFrom scratch\nLSTM\n26.20%\n15.49%\n40.00%\n26.33%\n63.57%\n45.78%\nLSTM+SA\n28.08%\n15.43%\n41.75%\n26.33%\n65.15%\n46.33%\nElectra\n28.63%\n28.08%\n42.35%\n41.74%\n65.64%\n65.37%\nTest performance\nLSTM\n26.94%\n15.67%\n40.59%\n25.97%\n65.49%\n45.15%\nLSTM+SA\n27.47%\n15.93%\n41.24%\n25.97%\n65.86%\n45.67%\nElectra\n27.88%\n27.90%\n41.76%\n41.82%\n66.23%\n66.49%\nTable 3: Performance on diagnosis predictionab\na Note that, as discussed in Section 3, on our dataset the highest possible top-5, top-10 and top-30 recalls are 50.17%, 79.48%\nand 99.88% on validation set, and 49.75%, 79.23% and 99.79% on test set.\nb Bold font indicates the training strategy (pre-trained or from scratch) that has higher accuracy.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\ntraining epoch\n0.250\n0.275\n0.300\n0.325\n0.350\n0.375\n0.400\n0.425\ntop-10 recall\ntop-10 recall on validation set\nLSTM + SA (s)\nLSTM max (s)\nELECTRA (s)\nLSTM + SA (p)\nLSTM max (p)\nELECTRA (p)\nFigure 7: Top-10 recall on diagnosis prediction vali-\ndation set. ‘SA’ stands for self attention layer. ‘max’\nrepresents max-pooling output layer. ‘(s)’ and ‘(p)’ in-\ndicates whether the model is trained from scratch or\npre-trained, respectively.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n# of training epochs\n0.45\n0.50\n0.55\n0.60\n0.65\nvalidation top-30 recall\nvalidation top-30 recall vs # of training epochs\nLSTM + SA (s)\nLSTM max (s)\nELECTRA (s)\nLSTM + SA (p)\nLSTM max (p)\nELECTRA (p)\nFigure 8: Top-30 recall on diagnosis prediction vali-\ndation set. ‘SA’ stands for self attention layer. ‘max’\nrepresents max-pooling output layer. ‘(s)’ and ‘(p)’ in-\ndicates whether the model is trained from scratch or\npre-trained, respectively.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2020-12-27",
  "updated": "2020-12-27"
}