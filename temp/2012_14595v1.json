{
  "id": "http://arxiv.org/abs/2012.14595v1",
  "title": "Sparse PCA via $l_{2,p}$-Norm Regularization for Unsupervised Feature Selection",
  "authors": [
    "Zhengxin Li",
    "Feiping Nie",
    "Jintang Bian",
    "Xuelong Li"
  ],
  "abstract": "In the field of data mining, how to deal with high-dimensional data is an\ninevitable problem. Unsupervised feature selection has attracted more and more\nattention because it does not rely on labels. The performance of spectral-based\nunsupervised methods depends on the quality of constructed similarity matrix,\nwhich is used to depict the intrinsic structure of data. However, real-world\ndata contain a large number of noise samples and features, making the\nsimilarity matrix constructed by original data cannot be completely reliable.\nWorse still, the size of similarity matrix expands rapidly as the number of\nsamples increases, making the computational cost increase significantly.\nInspired by principal component analysis, we propose a simple and efficient\nunsupervised feature selection method, by combining reconstruction error with\n$l_{2,p}$-norm regularization. The projection matrix, which is used for feature\nselection, is learned by minimizing the reconstruction error under the sparse\nconstraint. Then, we present an efficient optimization algorithm to solve the\nproposed unsupervised model, and analyse the convergence and computational\ncomplexity of the algorithm theoretically. Finally, extensive experiments on\nreal-world data sets demonstrate the effectiveness of our proposed method.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nSparse PCA via ℓ2,p-Norm Regularization\nfor Unsupervised Feature Selection\nZhengxin Li, Feiping Nie*, Jintang Bian, and Xuelong Li, Fellow, IEEE\nAbstract—In the ﬁeld of data mining, how to deal with high-dimensional data is an inevitable problem. Unsupervised feature selection\nhas attracted more and more attention because it does not rely on labels. The performance of spectral-based unsupervised methods\ndepends on the quality of constructed similarity matrix, which is used to depict the intrinsic structure of data. However, real-world data\ncontain a large number of noise samples and features, making the similarity matrix constructed by original data cannot be completely\nreliable. Worse still, the size of similarity matrix expands rapidly as the number of samples increases, making the computational cost\nincrease signiﬁcantly. Inspired by principal component analysis, we propose a simple and efﬁcient unsupervised feature selection\nmethod, by combining reconstruction error with ℓ2,p-norm regularization. The projection matrix, which is used for feature selection, is\nlearned by minimizing the reconstruction error under the sparse constraint. Then, we present an efﬁcient optimization algorithm to\nsolve the proposed unsupervised model, and analyse the convergence and computational complexity of the algorithm theoretically.\nFinally, extensive experiments on real-world data sets demonstrate the effectiveness of our proposed method.\nIndex Terms—Dimension Reduction, Principal Component Analysis, ℓ2,p-Norm, Unsupervised Feature Selection.\n!\n1\nINTRODUCTION\nW\nITH the rapid development of information technology,\nhigh-dimensional data exist almost everywhere in all\nwalks of life, such as weather forecast [1], ﬁnancial transaction\nanalysis [2], geological prospecting [3], image search [4], text\nmining [5], bioinformatics [6], etc. Unfortunately, the curse of\ndimensionality seriously restricts many practical applications. To\nsolve this problem, feature selection is used to reduce the di-\nmension by ﬁnding a relevant feature subset of data [7]. The\nadvantages of feature selection mainly include: improving the\nperformance of data mining tasks, reducing computational cost,\nimproving the interpretability of data. Therefore, feature selection\nhas become a necessary prerequisite for many data mining tasks,\nsuch as pattern recognition [8], clustering [9], classiﬁcation [10],\nsimilarity retrieval [11], etc.\nBased on whether data labels are available, feature selection\ncan be divided into supervised and unsupervised methods [12]. Su-\npervised feature selection utilizes the correlation between features\nand labels to ﬁnd discriminative features. However, obtaining la-\nbels is expensive, or even impractical in many applications. Thus,\nunsupervised feature selection has attracted a lot of attention,\nbecause it does not rely on labels. In the paper, we propose a new\nmethod for unsupervised feature selection. The main contributions\nare summarized as follows:\n•\nA new unsupervised model is proposed to perform fea-\nture selection. The sparse projection matrix is learned by\n•\nCorresponding author: Feiping Nie.\n•\nZ. Li was with the College of Equipment Management and UAV Engineer-\ning, Air Force Engineering University, Xi’an 710051, Shaanxi, P. R. China.\nHe is currently a postdoctor in the School of Computer Science and Center\nfor OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern\nPolytechnical University. E-mail: zhengxinli@nwpu.edu.cn\n•\nF. Nie, J. Bian and X. Li were with the School of Computer Sci-\nence and Center for OPTical IMagery Analysis and Learning (OPTI-\nMAL), Northwestern Polytechnical University, Xi’an 710072, Shaanxi,\nP. R. China.E-mail: feipingnie@gmail.com; bianjintang@gmail.com; xue-\nlong li@nwpu.edu.cn\nminimizing the reconstruction error of data.\n•\nAn optimization algorithm is presented to solve the pro-\nposed model. We prove the convergence of the algorithm,\nand evaluate its computational complexity, which is linear\nto the number of samples.\n•\nExtensive experiments on real-world data sets demonstrate\nthe effectiveness of our proposed method.\nThe rest paper is organized as follows. In Section 2, we give a\nbrief review of the related work and introduce some notations\nand deﬁnitions. In Section 3, we propose a new unsupervised\nfeature selection model. In Section 4, the optimization algorithm\nis presented to solve the proposed model. In Section 5, we discuss\nthe convergence and computational complexity of the optimization\nalgorithm. In Section 6, experiments are implemented to evaluate\nthe effectiveness of the proposed method. Finally, we provide the\nconclusion in Section 7.\n2\nBACKGROUND\n2.1\nRelated work\nThe techniques of unsupervised feature selection can be divided\ninto three types [13]: ﬁlter, wrapper and embedded methods.\nFilter methods [14] are independent of the data mining tasks.\nThey are usually intuitive and computationally efﬁcient. LapScore\n(Laplacian Score) [15] is one of the most classic ﬁlter methods. It\ncalculates the score for each feature independently, according to\nits ability to preserve the intrinsic structure of original data. Then,\nall the features are ranked by the scores. Because each feature\nis evaluated independently, it may work well on binary-cluster\nproblems, but are very likely to fail in multi-cluster cases [16].\nWrapper methods [17] combine feature selection with the data\nmining tasks. The mining algorithm is utilized to evaluate the\neffectiveness of selected features. The result of feature selection\nperforms well in the mining task. However, wrapper methods are\nusually computationally expensive and weak in generalization.\narXiv:2012.14595v1  [cs.LG]  29 Dec 2020\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\nEmbedded methods [18] integrate feature selection into model\nlearning. Since there is no need to evaluate feature subsets, they\nare more efﬁcient than wrapper methods [19]. Thus, embedded\nmethods have gradually become a hotspot, and many representa-\ntive methods keep emerging, such as MCFS (Multi-Cluster Fea-\nture Selection) [16], UDFS (Unsupervised Discriminative Feature\nSelection) [20], EUFS (Embedded Unsupervised Feature Selec-\ntion) [21], DGUFS (Dependence Guided Unsupervised Feature\nSelection) [22], SOGFS (Structured Optimal Graph Feature Se-\nlection) [23] and RNE (Robust Neighborhood Embedding Feature\nSelection) [24], etc.\nMCFS selects features by using spectral regression with ℓ1-\nnorm regularization, so that the multi-cluster structure of original\ndata can be preserved. UDFS selects the discriminative features by\njoint discriminative analysis and ℓ2,1-norm minimization. EUFS\nembeds unsupervised feature selection into a clustering algorithm\nvia sparse learning. ℓ2,1-norm is applied on the cost function to\nreduce the effects of noise. DGUFS enhances the interdependence\namong original data, cluster labels, and selected features. SOGFS\nconducts feature selection and local structure learning simultane-\nously, so that the similarity matrix can be determined adaptively.\nRNE selects features by calculating feature weight matrix through\nlocally linear embedding algorithm, and ultilizing ℓ1-norm to\nminimize its reconstruction error.\nMost embedded methods, even including LapScore, utilize\nspectral analysis and manifold learning to select discriminative\nfeatures. They usually build a similarity matrix to depict the intrin-\nsic structure of original data. However, real-world data contain a\nlarge number of noise samples and features, making the similarity\nmatrix constructed by original data cannot be completely reliable.\nWorse still, the size of similarity matrix expands rapidly as the\nnumber of samples increases, making the computational cost in-\ncrease signiﬁcantly. Inspired by principal component analysis [25],\nwe propose a simple and efﬁcient unsupervised feature selection\nmethod from a new perspective.\n2.2\nNotations and deﬁnitions\nWe ﬁrst introduce some notations and deﬁnitions that will be used\nthroughout the paper. Given a matrix M ∈Rn×d, the (i, j)-th\nelement of M is denoted by mij, its i-th row, j-th column are\ndenoted by mi, mj respectively. The transpose of M is denoted\nby M T . The trace of M is denoted by Tr(M). The ℓ2,p-norm is\ndeﬁned as:\n∥M∥2,p =\n\n\n\nn\nX\ni=1\n\n\nd\nX\nj=1\nm2\nij\n\n\np\n2 \n\n\n1\np\n=\n n\nX\ni=1\n\r\rmi\r\rp\n2\n! 1\np\n, p > 0\n(1)\nWhen p ≥1, since it satisﬁes the basic norm conditions, ℓ2,p-\nnorm is a valid norm. However, when 0 < p < 1, ℓ2,p is not a\nvalid norm. For convenience, we still call them norms in the paper.\n3\nUNSUPERVISED FEATURE SELECTION MODEL\nSupposing a data set {x1, x2, . . . , xn} contains n data points\nxi ∈Rd×1, X ∈Rn×d denotes the data matrix. Without loss\nof generality, we assume that all the data points are centralized:\nn\nX\ni=1\nxi = 0\n(2)\nSupposing we explore principal component analysis for di-\nmension reduction, the new coordinate system formed by principal\ncomponents is:\n{w1, w2, . . . , wd},\nwi ∈Rd×1\ns.t.\n∥wi∥= 1,\nwT\ni wj = 0 (i ̸= j)\n(3)\nIf we want to reduce the dimension of the data points from d\nto m (m < d), some coordinates in the coordinate system should\nbe discarded. Then, the new coordinate system W ∈Rd×m is:\nW = {w1, w2, . . . , wm},\nwi ∈Rd×1\ns.t.\n∥wi∥= 1,\nwT\ni wj = 0 (i ̸= j)\n(4)\nThus, the projection of the data point xi in the new coordinate\nsystem is:\nzi = {zi1, zi2, . . . , zim}T ,\nzij = wT\nj xi\n(5)\nwhere zij is the jth-dimension coordinate of xi in the low\ndimensional coordinate system. Eq. (5) can be rewritten as\nzi = W T xi\n(6)\nIf we reconstruct xi with zi, the original data point can be\nrecovered as:\nˆxi =\nm\nX\nj=1\nzijwj = Wzi\n(7)\nFor the entire data set, the sum of the error between each\noriginal data point xi and its reconstructed point ˆxi is:\nn\nX\ni=1\n∥ˆxi −xi∥2\n2\n(8)\nWe can substitute Eq. (7) into Eq. (8):\nn\nX\ni=1\n∥Wzi −xi∥2\n2\n(9)\nAccording to the property of ℓ2-norm, Eq. (9) can be further\nexpanded to:\nn\nX\ni=1\n(Wzi)T (Wzi) −2\nn\nX\ni=1\n(Wzi)T xi +\nn\nX\ni=1\nxT\ni xi\n(10)\nDue to wT\ni wj = 0 (i ̸= j), we can get W T W = I. Then,\nEq. (10) can be converted to the following equation:\nn\nX\ni=1\nzT\ni zi −2\nn\nX\ni=1\nzT\ni W T xi +\nn\nX\ni=1\nxT\ni xi\n(11)\nAccording to Eq. (6), the above equation can be rewritten as\nn\nX\ni=1\nzT\ni zi −2\nn\nX\ni=1\nzT\ni zi +\nn\nX\ni=1\nxT\ni xi\n= −\nn\nX\ni=1\nzT\ni zi +\nn\nX\ni=1\nxT\ni xi\n(12)\nAccording to Eq. (2), Eq. (6) and the properties of matrix trace,\nwe can get\nn\nX\ni=1\nzT\ni zi = Tr\n \nW T\n n\nX\ni=1\nxixT\ni\n!\nW\n!\n= Tr(W T XT XW)\n(13)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\nWe can further substitute Eq. (13) into Eq. (12). Thus, Eq. (8)\nis equivalent to\n−Tr(W T XT XW) +\nn\nX\ni=1\nxT\ni xi\n(14)\nPrincipal component analysis requires that the reconstruction\nerror should be minimal. Thus, the objective function is\nmin\nW T W =I −Tr(W T XT XW) +\nn\nX\ni=1\nxT\ni xi\n(15)\nFor a given data set, Pn\ni=1 xT\ni xi is a constant, which has\nno impact on the minimization of the objective function. Then,\nEq. (15) can be rewritten as\nmin\nW T W =I −Tr(W T XT XW)\n(16)\nFor the general case that the data points are not centralized,\nEq. (16) can be rewritten as\nmin\nW T W =I −Tr(W T StW)\n(17)\nwhere St = XT HX is the total scatter matrix. H is the centering\nmatrix:\nH = In −1\nn11T\n(18)\nAs we all known, ℓ2,0-norm is the most suitable for feature\nselection. For the sake of feature selection, we add a regularization\nterm to the objective function of Eq. (17):\nmin\nW T W =I −Tr(W T StW) + γ∥W∥2,0\n(19)\nwhere γ > 0 is a regularization parameter. The regularization term\ncan make the projection matrix W be sparse on the row vectors,\nso as to complete the task of feature selection. Unfortunately, it is\ndifﬁcult to solve ℓ2,0-norm problem directly. Because ℓ2,p-norm\n(0 < p ≤1) is a reasonable choice to approximate ℓ2,0-norm\nin the feature selection task [23], we can replace ℓ2,0-norm with\nℓ2,p-norm. Thus, Eq. (19) can be rewritten as\nmin\nW T W =I −Tr(W T StW) + γ∥W∥p\n2,p, 0 < p ≤1\n(20)\nIn the process of minimizing the objective function of Eq. (20),\nγ∥W∥p\n2,p favors a small number of nonzero row vector wi. The\nprojection matrix W should satisfy the following two constraints:\nit is sparse in the row vectors; the reconstruction error of all data\npoints should be as small as possible, which is just the optimiza-\ntion direction of principal component analysis. For simplicity,\nwe denote the proposed method as SPCAFS (Sparse Principal\nComponent Analysis for Feature Selection).\n4\nOPTIMIZATION ALGORITHM\nIn this section, we present the optimization algorithm to solve\nproblem (20). According to the deﬁnition of ℓ2,p-norm, prob-\nlem (20) can be rewritten as\nmin\nW T W =I −Tr(W T StW) + γ\nd\nX\ni=1\n\r\rwi\r\rp\n2\n(21)\nwhere wi ∈Rm×1 is the i-th row vector of W. Since\n\r\rwi\r\rp\n2 can\nbe zero in theory, Eq. (21) may be non-differentiable. To avoid this\ncase, we replace\n\r\rwi\r\rp\n2 with\n\u0000wiT wi\u0001 p\n2 . Further, it is regularized\nas\n\u0010\nwiT wi\u0011 p\n2 →\n\u0010\nwiT wi + ϵ\n\u0011 p\n2\n(22)\nwhere ϵ is a sufﬁciently small constant. Then, Eq. (21) can be\nequivalent to\nmin\nW T W =I −Tr(W T StW) + γ\nd\nX\ni=1\n\u0010\nwiT wi + ϵ\n\u0011 p\n2\n(23)\nTheorem 1. The solution to problem (23), i.e. W ∈Rd×m, will\ncontain at least m non-zero rows.\nProof. According to the constraint of problem (23), any feasible\nsolution W ∈Rd×m should satisfy W T W = Im. Since Im ∈\nRm×m, the rank of W is m. Therefore, W contains at least m\nnon-zero rows.\nThe Lagrangian function of problem (23) is\nL(W, Λ) = −Tr(W T StW) + γ\nd\nX\ni=1\n\u0010\nwiT wi + ϵ\n\u0011 p\n2\n+ Tr(Λ(W T W −I))\n(24)\nwhere Λ is the Lagrangian multiplier. We take the derivative of\nEq. (24) with respect to W, and set its value equal to zero. Then,\nwe can get\n∂L(W, Λ)\n∂W\n= −StW + γGW + WΛ = 0\n(25)\nwhere G ∈Rd×d is a diagonal matrix, and the i-th diagonal\nelement is deﬁned as\ngii = p\n2\n\u0010\nwiT wi + ϵ\n\u0011 p−2\n2\n(26)\nIt is worth noting that G still depends on W. That is, W cannot\nbe directly calculated from Eq. (25). Thus, we utilize the following\nalternate optimization method to calculate W, G iteratively.\nFix G update W.\nWhen G is ﬁxed, it is easily to prove that solving Eq. (25) is\nequivalent to solving\nmin\nW T W =I −Tr(W T StW) + γ Tr(W T GW)\n(27)\nThe optimal W of Eq. (27) is formed by the m eigenvectors\nof (−St + γG), corresponding to the m smallest eigenvalues.\nFix W update G.\nWhen W is ﬁxed, we can easily calculate G by Eq. (26).\nBased on the above analysis, the optimization algorithm to\nsolve problem (23) is summarized in Algorithm 1.\n5\nDISCUSSION\n5.1\nConvergence analysis\nThe convergence of Algorithm 1 can guarantee that we can ﬁnd a\nlocally optimal solution of problem (23). Obviously, the converged\nsolution satisﬁes KKT condition. To prove the convergence, we\nﬁrst introduce the following lemma. Please refer to [23] for the\ndetailed proof.\nLemma 1. When 0 < p ≤1, for any positive real number u and\nv, the following inequality holds:\nu\np\n2 −p\n2\nu\nv\n2−p\n2\n≤v\np\n2 −p\n2\nv\nv\n2−p\n2\n(28)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nAlgorithm 1 The algorithm to solve the problem (23).\nInput: Data matrix X ∈Rn×d, reduced dimension m, regular-\nization parameter γ, a sufﬁciently small constant ϵ.\nOutput: h features of the data set.\n1: Initialize St = XT HX and G = I.\n2: repeat\n3:\nUpdate W ∈Rd×m. The columns of W are the m eigen-\nvectors of (−St + γG), corresponding to the m smallest\neigenvalues.\n4:\nUpdate G. The i-th element of G is deﬁned by Eq. (26).\n5: until converge\n6: Sort ∥wi∥2 (i = 1, 2, . . . , d) in descending order, and select\nthe top h ranked features.\nTheorem 2. When we calculate W according to Algorithm 1,\nupdated W will decrease the objective value of problem (23) until\nconverge.\nProof. Supposing the current updated W is denoted by ˆW, we\ncan easily derive the following inequality\n−Tr( ˆW T St ˆW) + γ Tr( ˆW T G ˆW) ≤\n−Tr(W T StW) + γ Tr(W T GW)\n(29)\nWe add the following item to the both sides of Eq. (29).\nγ\nd\nX\ni=1\np\n2\nϵ\n(wiT wi + ϵ)\n2−p\n2\n(30)\nThen, we can get\n−Tr( ˆW T St ˆW) + γ Tr( ˆW T G ˆW) +\nd\nX\ni=1\nγpϵ\n2 (wiT wi + ϵ)\n2−p\n2\n≤\n−Tr(W T StW) + γ Tr(W T GW) +\nd\nX\ni=1\nγpϵ\n2 (wiT wi + ϵ)\n2−p\n2\n(31)\nAccording to Eq. (26), we substitute G into Eq. (31):\n−Tr( ˆW T St ˆW) + γ\nd\nX\ni=1\np\n2\nˆwiT ˆwi + ϵ\n(wiT wi + ϵ)\n2−p\n2\n≤\n−Tr(W T StW) + γ\nd\nX\ni=1\np\n2\nwiT wi + ϵ\n(wiT wi + ϵ)\n2−p\n2\n(32)\nSetting u =\nˆwiT ˆwi + ϵ, v = wiT wi + ϵ, according to\nLemma 1, we have the inequality:\n\u0010\nˆwiT ˆwi + ϵ\n\u0011 p\n2 −p\n2\nˆwiT ˆwi + ϵ\n(wiT wi + ϵ)\n2−p\n2\n≤\n\u0010\nwiT wi + ϵ\n\u0011 p\n2 −p\n2\nwiT wi + ϵ\n(wiT wi + ϵ)\n2−p\n2\n(33)\nDue to γ > 0, we can further get\nγ\nd\nX\ni=1\n\u0010\nˆwiT ˆwi + ϵ\n\u0011 p\n2 −γ\nd\nX\ni=1\np\n2\nˆwiT ˆwi + ϵ\n(wiT wi + ϵ)\n2−p\n2\n≤\nγ\nd\nX\ni=1\n\u0010\nwiT wi + ϵ\n\u0011 p\n2 −γ\nd\nX\ni=1\np\n2\nwiT wi + ϵ\n(wiT wi + ϵ)\n2−p\n2\n(34)\nFor Eq. (32) and Eq. (34), we add the left parts of the two\ninequalities. For the right parts, we perform the similar operation.\nThus, we have\n−Tr( ˆW T St ˆW) + γ\nd\nX\ni=1\n\u0010\nˆwiT ˆwi + ϵ\n\u0011 p\n2 ≤\n−Tr(W T StW) + γ\nd\nX\ni=1\n\u0010\nwiT wi + ϵ\n\u0011 p\n2\n(35)\nBy comparing Eq. (35) with problem (23), we can infer Theorem 2\nholds.\n5.2\nComputational complexity analysis\nSince normalization is a prerequisite for all data mining tasks, we\ndon’t count its computational cost into feature selection methods.\nThe computational complexity of Algorithm 1 can be decomposed\ninto the following aspects:\n•\nWe need O(d2n) to initialize St and G, based on the\nnormalized data.\n•\nFor one iteration, we need O(d3) to update W by per-\nforming eigen-decomposition of (−St + γG).\n•\nFor one iteration, we need O(dm) to update G according\nto Eq. (26).\n•\nWe need O(dm) to calculate ∥wi∥2 (i = 1, 2, . . . , d) and\nO(dlogd) to complete the sorting.\nThus, the overall computational complexity is O(d2n + d3t),\nwhere t is the number of iterations of Algorithm 1. Note that,\nAlgorithm 1 is efﬁcient and always converges within 30 iterations\nin our experiments. We further compare its computational com-\nplexity in one iteration with that of other competing methods. In\nTable 1, apart from some introduced notations, c is the number of\nclusters, p is the number of neighbours in graph construction, h is\nthe number of selected features. We can conclude that:\n•\nFor embedded methods, the computational complexity of\nsparse regression usually contains d3, which is produced\nby inverse operation or eigen-decomposition.\n•\nSPCAFS does not require the construction of a similarity\nmatrix by KNN, which will need at least O(dn2).\n•\nOf all the methods, only the computational complexity of\nSPCAFS is linear to n. It indicates that the computational\ncomplexity of SPCAFS will not expand rapidly, as the\nnumber of samples increases.\nTABLE 1: Comparison of computational complexity.\nMethods\nComputational complexity\nLapScore\nO(dn2 + dlog2d)\nMCFS\nO(d3 + n2m + d2n)\nUDFS\nO(d3 + n2c)\nEUFS\nO(dn2 + nc2)\nDGUFS\nO(n3 + dn)\nSOGFS\nO(d3 + n2p + n2c)\nRNE\nO(dn2 + d2n + d2h)\nSPCAFS\nO(d2n + d3)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\n6\nEXPERIMENTAL EVALUATION\n6.1\nExperimental setup\n6.1.1\nExperimental environment and data sets\nHardware is a workstation with 3.8 GHz CPU and 16 GB RAM.\nThe experimental environment is Windows 64-bit Operating Sys-\ntem, running Matlab R2018a. Our experiments were executed\non 6 publicly available data sets, including four image data sets\nPalmData25, Imm40, PIE and AR, two bioinformatics data sets\nSRBCTML and LEUML. More information of the data sets is\nshown in Table 2.\nTABLE 2: The details of the experimental data sets.\nData sets\n# of Feature\n# of Instance\n# of Class\nPalmData25\n256\n2000\n100\nImm40\n1024\n240\n40\nPIE\n1024\n1166\n53\nAR\n2200\n2600\n100\nSRBCTML\n2308\n83\n4\nLEUML\n3571\n72\n2\n6.1.2\nComparision methods and parameters setting\nTo verify the effectiveness of the proposed method, we compared\nit with several state-of-the-art methods in the ﬁeld of unsuper-\nvised feature selection, such as LapScore, MCFS, UDFS, EUFS,\nDGUFS, SOGFS and RNE. To get the baseline for analysis, all\nfeatures are selected as a special case of feature selection.\nTo ensure that the experiments are as fair as possible, we adopt\nthe same strategy to set parameters for all the unsupervised feature\nselection methods. For LapScore, MCFS, UDFS, EUFS, DGUFS,\nSOGFS and RNE, we set the neighborhood size to be 5. For\nEUFS, SOGFS and SPCAFS, the reduced dimension is ﬁxed as\nm = c −1. We tune all the parameters by grid search strategy\nfrom {10−6, 10−4, 10−2, 100, 102, 104, 106 }. Without loss of\ngenerality, we set p = 1 in ℓ2,p-norm regularization of SPCAFS.\n6.1.3\nEvaluation metrics\nTo verify the validity of SPCAFS, we execute K-means clustering\nby inputting the results of different unsupervised feature selection\nmethods. Clustering accuracy (ACC) and Normalized Mutual\nInformation (NMI) are utilized to evaluate the effectiveness of\nfeature selection indirectly. ACC is deﬁned as [26]\nACC =\nPn\ni=1 δ(ci, map(li))\nn\n(36)\nwhere n is the number of data points, ci is the given cluster label,\nli is the obtained cluster label, map(·) is the permutation mapping\nfunction that maps each obtained cluster label li to the equivalent\nlabel from the data set. The best mapping can be found by using\nKuhn–Munkres algorithm [27]. δ is a function deﬁned as\nδ(a, b) =\n(\n1,\nif a = b\n0,\notherwise\n(37)\nNMI is deﬁned as [26]\nNMI =\nMI(C, C′))\nmax(H(C), H(C′))\n(38)\nwhere C is the set of clusters obtained from the ground truth\nand C′ is the set of clusters computed by a clustering algorithm.\nMI(C, C) is the mutual information metric. H(C), H(C′) are\nthe entropies of C and C′ respectively.\nFor each unsupervised feature selection method, the best result\nof K-means clustering with the optimal parameters is recorded.\nSince the result of K-means clustering depends on initialization,\nwe repeated K-means clustering 20 times for all the methods, and\nreport their average results.\n6.2\nClustering results analysis with selected features\nWithout loss of generality, we set the number of selected features\nas {10, 20, 30, 40, 50, 60, 70, 80, 90, 100 } for each data set. The\nexperimental results of ACC, NMI are illustrated in Fig. 1, Fig. 2\nrespectively. We can get the following conclusions:\n1.\nFeature selection of SPCAFS is effective. Compared with\nthe baseline (AllFea), both ACC and NMI of SPCAFS\nhave been signiﬁcantly improved on almost all these data\nsets. On PalmData25 data set, although ACC and NMI of\nSPCAFS are lower than the baseline, they are still higher\nthan those of other methods. As the number of selected\nfeatures increases, both ACC and NMI of SPCAFS grad-\nually approach the baseline.\n2.\nIn general, with the increase of the number of selected\nfeatures, the curves of clustering results show a trend\nof rising ﬁrst and then falling. Because data sets from\npractical applications usually contain many redundancy\nfeatures and a few discriminative features. As the number\nof selected features increases, some redundancy features\nare selected, decreasing the clustering performance of\nfeature selection methods.\n3.\nThe performance of SPCAFS exceeds other competing\nmethods on all these data sets. In particular on Imm40\ndata set, compared with the second best method MCFS,\nSPCAFS has 5 percent improvement of ACC, 3 percent\nimprovement of NMI on average.\n6.3\nConvergence study\nIn Section 5.1, we have proven the convergence of Algorithm 1.\nWe further study the speed of its convergence by experiments.\nThe convergence curves of the objective value are demostrated in\nFig. 3. Due to space limitation, we only show the results on four\ndata sets. We can see that the speed of convergence of Algorithm 1\nis very fast, which ensures the efﬁciency of SPCAFS.\n6.4\nParameter sensitivity analysis\n6.4.1\nSensitivity analysis for the parameters γ, m\nWe further investigate the impact of parameters γ, m on SPCAFS.\nThe experimental results on all these data sets are similar. Due\nto space limitation, we only present the experimental results on\nImm40 data set. We ﬁrst adjust γ by ﬁxing m = c −1. There are\nsome small ﬂuctuations in clustering performance under different\nγ, as illustrated in Fig. 4(a)-(b). Because γ is used to control the\nrow sparsity of projection matrix W, its variation will affect the\nresult of feature selection. Then, we adjust m by ﬁxing γ = 104.\nWhen m changes from 10 to 70, the clustering performance does\nnot change signiﬁcantly, as illustrated in Fig. 4(c)-(d). The results\nindicate that SPCAFS is not sensitive to parameters γ and m with\nwide range, and it can be used in practical applications.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\nACC\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(a) Imm40\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nACC\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(b) PIE\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nACC\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(c) SRBCTML\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\n0.2\n0.22\nACC\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(d) AR\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nACC\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(e) LEUML\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nACC\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(f) PalmData25\nFig. 1: Clustering results (ACC) of different unsupervised feature selection methods.\n6.4.2\nThe effect of p in ℓ2,p-Norm regularization\nIn the above experiments, we set p = 1 in ℓ2,p-norm regularization\nby default. In this section, we discuss the effect of p on the results\nof feature selection for SPCAFS. Without loss of generality, we\nonly show the experimental results on Imm40, SRBCTML data\nsets, as illustrated in Fig. 5.\nOn Imm40 data set, when p decreases from 1 to 0.1, the result\nof feature selection of SPCAFS becomes worse. On SRBCTML\ndata set, as the value of p decreases from 1 to 0.01, the results\nof feature selection show a trend of rising ﬁrst and then falling.\nThe phenomenon suggests that the choice of p is not the smaller\nthe better. The parameter p is used to balance the sparsity and\nthe convexity of the regularization. Small p results in highly non-\nconvex problem, which will increase the difﬁculty of optimization.\n7\nCONCLUSIONS\nIn the paper, we propose a new method for unsupervised fea-\nture selection, by combining reconstruction error with ℓ2,p-norm\nregularization. The projection matrix is learned by minimizing\nthe reconstruction error under the sparse constraint. Then, we\npresent an efﬁcient optimization algorithm to solve the proposed\nunsupervised model, and analyse the convergence and computa-\ntional complexity of the proposed algorithm. Finally, extensive\nexperiments on real-world data sets demonstrate the effectiveness\nof our proposed method.\nACKNOWLEDGMENTS\nThanks to the donors who have made contributions to the bench-\nmark data sets.\nREFERENCES\n[1]\nB. Ma and A. Entezari, “An interactive framework for visualization of\nweather forecast ensembles,” IEEE Transactions on Visualization and\nComputer Graphics, vol. 25, no. 1, pp. 1091–1101, 2019.\n[2]\nH. He, Y. Hong, W. Liu, and S. A. Kim, “Data mining model for\nmultimedia ﬁnancial time series using information entropy,” Journal of\nIntelligent and Fuzzy Systems, no. 1, pp. 1–7, 2020.\n[3]\nM. Dentith, R. J. Enkin, W. Morris, C. Adams, and B. Bourne, “Petro-\nphysics and mineral exploration: a workﬂow for data analysis and a new\ninterpretation framework,” Geophysical Prospecting, vol. 68, no. 1, pp.\n178–199, 2020.\n[4]\nC. Deng, E. Yang, T. Liu, J. Li, W. Liu, and D. Tao, “Unsupervised\nsemantic-preserving adversarial hashing for image search,” IEEE Trans-\nactions on Image Processing, vol. 28, no. 8, pp. 4032–4044, 2019.\n[5]\nF. Ali, S. El-Sappagh, and D. Kwak, “Fuzzy ontology and lstm-based\ntext mining: A transportation network monitoring system for assisting\ntravel,” Sensors, vol. 19, no. 2, 2019.\n[6]\nP. Luo, L. Tian, J. Ruan, and F. Wu, “Disease gene prediction by inte-\ngrating ppi networks, clinical rna-seq data and omim data,” IEEE/ACM\nTransactions on Computational Biology and Bioinformatics, vol. 16,\nno. 1, pp. 222–232, 2019.\n[7]\nI. Guyon and A. Elisseeff, “An introduction to variable and feature\nselection,” Journal of Machine Learning Research, vol. 3, no. 6, pp.\n1157–1182, 2003.\n[8]\nR. Zhang, J. Tao, and H. Zhou, “Fuzzy optimal energy management for\nfuel cell and supercapacitor systems using neural network based driving\npattern recognition,” IEEE Transactions on Fuzzy Systems, vol. 27, no. 1,\npp. 45–57, 2019.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nNMI\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(a) Imm40\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nNMI\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(b) PIE\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nNMI\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(c) SRBCTML\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.32\n0.34\n0.36\n0.38\n0.4\n0.42\n0.44\n0.46\nNMI\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(d) AR\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nNMI\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(e) LEUML\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100\nNumber of selected features\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nNMI\nAllFea\nLapScore\nMCFS\nUDFS\nEUFS\nDGUFS\nSOGFS\nRNE\nSPCAFS\n(f) PalmData25\nFig. 2: Clustering results (NMI) of different unsupervised feature selection methods.\n1 \n3 \n5 \n7 \n9 \n11\n13\n15\nNumber of interations\n0.9\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nObjective function value\n106\n(a) PalmData25\n1 \n3 \n5 \n7 \n9 \n11\n13\n15\nNumber of interations\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\nObjective function value\n106\n(b) Imm40\n1 \n3 \n5 \n7 \n9 \n11\n13\n15\nNumber of interations\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nObjective function value\n(c) SRBCTML\n1 \n3 \n5 \n7 \n9 \n11\n13\n15\nNumber of interations\n-600\n-500\n-400\n-300\n-200\n-100\n0\n100\nObjective function value\n(d) LEUML\nFig. 3: Convergence curve of SPCAFS on PalmData25, Imm40,\nSRBCTML and LEUML data sets.\n0\n10 \n0.2\n20 \nACC\n0.4\n30 \n0.6\n40 50 \n# Feature\n60 \n6\n4 \n70 \n2 \nlg\n80 \n0 \n-2\n90 \n-4\n100\n-6\n(a) ACC (m=39)\n0.2\n10 \n0.4\n20 \nNMI\n0.6\n30 \n0.8\n40 50 \n# Feature\n60 \n6\n4 \n70 \n2 \nlg\n80 \n0 \n-2\n90 \n-4\n100\n-6\n(b) NMI (m=39)\n0\n10 \n0.2\n20 \nACC\n0.4\n30 \n0.6\n40 50 \n# Feature\n60 \n70\n60\n70 \n50\nm\n80 \n40\n30\n90 \n20\n100\n10\n(c) ACC (γ = 104)\n0.2\n10 \n0.4\n20 \nNMI\n0.6\n30 \n0.8\n40 50 \n# Feature\n60 \n70\n60\n70 \n50\nm\n80 \n40\n30\n90 \n20\n100\n10\n(d) NMI (γ = 104)\nFig. 4: ACC and NMI of SPCAFS under different γ, m on Imm40\ndata set.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\n10 20 30 40 50 60 70 80 90 100\nNumber of selected features\n0.45\n0.5\n0.55\n0.6\n0.65\nACC\nAllFea\np=0.01\np=0.1\np=0.5\np=1\n(a) Imm40 (ACC)\n10 20 30 40 50 60 70 80 90 100\nNumber of selected features\n0.7\n0.75\n0.8\nNMI\nAllFea\np=0.01\np=0.1\np=0.5\np=1\n(b) Imm40 (NMI)\n10 20 30 40 50 60 70 80 90 100\nNumber of selected features\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nACC\nAllFea\np=0.01\np=0.1\np=0.5\np=1\n(c) SRBCTML (ACC)\n10 20 30 40 50 60 70 80 90 100\nNumber of selected features\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nNMI\nAllFea\np=0.01\np=0.1\np=0.5\np=1\n(d) SRBCTML (NMI)\nFig. 5: ACC and NMI of SPCAFS under different p on Imm40,\nSRBCTML data sets.\n[9]\nA. Hoyos-Idrobo, G. Varoquaux, J. Kahn, and B. Thirion, “Recursive\nnearest agglomeration (rena): Fast clustering for approximation of struc-\ntured signals,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 41, no. 3, pp. 669–681, 2019.\n[10] K. Kayabol, “Approximate sparse multinomial logistic regression for\nclassiﬁcation,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 42, no. 2, pp. 490–493, 2020.\n[11] Y. Wu, S. Wang, and Q. Huang, “Online fast adaptive low-rank similarity\nlearning for cross-modal retrieval,” IEEE Transactions on Multimedia,\nvol. 22, no. 5, pp. 1310–1322, 2020.\n[12] L. Yu and H. Liu, “Eﬁcient feature selection via analysis of relevance\nand redundancy,” Journal of Machine Learning Research, vol. 5, no. 12,\npp. 1205–1224, 2004.\n[13] Z. Li, L. Jing, Y. Yi, X. Zhou, and H. Lu, “Clustering-guided sparse struc-\ntural learning for unsupervised feature selection,” IEEE Transactions on\nKnowledge and Data Engineering, vol. 26, no. 9, pp. 2138–2150, 2014.\n[14] C. Lazar, “A survey on ﬁlter techniques for feature selection in gene\nexpression microarray analysis,” IEEE/ACM Transactions on Computa-\ntional Biology and Bioinformatics, vol. 9, no. 4, pp. 1106–1119, 2012.\n[15] X. He, D. Cai, and P. Niyogi, “Laplacian score for feature selection,” in\nAdvances in Neural Information Processing Systems, 2005, pp. 507–514.\n[16] C. Deng, C. Zhang, and X. He, “Unsupervised feature selection for multi-\ncluster data,” in Acm Sigkdd International Conference on Knowledge\nDiscovery and Data Mining, 2010, pp. 333–342.\n[17] M. M. Kabir, M. M. Islam, and K. Murase, “A new wrapper feature\nselection approach using neural network,” Neurocomputing, vol. 73, no.\n16-18, pp. 3273–3283, 2008.\n[18] P. Chong, K. Zhao, Y. Ming, and C. Qiang, “Feature selection embedded\nsubspace clustering,” IEEE Signal Processing Letters, vol. 23, no. 7, pp.\n1018–1022, 2016.\n[19] J. Li, K. Cheng, S. Wang, F. Morstatter, R. P. Trevino, J. Tang, and H. Liu,\n“Feature selection: A data perspective,” Acm Computing Surveys, vol. 50,\nno. 6, pp. Article 39:1–45, 2016.\n[20] Y. Yang, H. Shen, Z. Ma, Z. Huang, and X. Zhou, “L21-norm regularized\ndiscriminative feature selection for unsupervised learning,” 07 2011, pp.\n1589–1594.\n[21] S. Wang, J. Tang, and H. Liu, “Embedded unsupervised feature selec-\ntion,” in Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015,\npp. 470–476.\n[22] J. Guo and W. Zhu, “Dependence guided unsupervised feature selection,”\nin Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, New Orleans, Louisiana, USA, February 2-7, 2018.\nAAAI\nPress, 2018, pp. 2232–2239.\n[23] F. Nie, W. Zhu, and X. Li, “Structured graph optimization for unsu-\npervised feature selection,” IEEE Transactions on Knowledge and Data\nEngineering, vol. PP, pp. 1–1, 08 2019.\n[24] Y. Liu, D. Ye, W. Li, and H. Wang, “Robust neighborhood embedding\nfor unsupervised feature selection,” Knowledge-Based Systems, vol. 193,\np. 105462, 04 2020.\n[25] I. T. Jolliffe, “Principal component analysis,” Journal of Marketing\nResearch, vol. 87, no. 4, p. 513, 2002.\n[26] X. Zhu, S. Zhang, Y. Li, J. Zhang, L. Yang, and Y. Fang, “Low-\nrank sparse subspace for spectral clustering,” IEEE Transactions on\nKnowledge and Data Engineering, vol. 31, no. 8, pp. 1532–1543, 2019.\n[27] A. Strehl and J. Ghosh, “Cluster ensembles: a knowledge reuse frame-\nwork for combining partitionings,” Journal of Machine Learning Re-\nsearch, vol. 3, no. 3, pp. 583–617, 2002.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-12-29",
  "updated": "2020-12-29"
}