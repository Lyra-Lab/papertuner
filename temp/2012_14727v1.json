{
  "id": "http://arxiv.org/abs/2012.14727v1",
  "title": "AttrE2vec: Unsupervised Attributed Edge Representation Learning",
  "authors": [
    "Piotr Bielak",
    "Tomasz Kajdanowicz",
    "Nitesh V. Chawla"
  ],
  "abstract": "Representation learning has overcome the often arduous and manual\nfeaturization of networks through (unsupervised) feature learning as it results\nin embeddings that can apply to a variety of downstream learning tasks. The\nfocus of representation learning on graphs has focused mainly on shallow\n(node-centric) or deep (graph-based) learning approaches. While there have been\napproaches that work on homogeneous and heterogeneous networks with multi-typed\nnodes and edges, there is a gap in learning edge representations. This paper\nproposes a novel unsupervised inductive method called AttrE2Vec, which learns a\nlow-dimensional vector representation for edges in attributed networks. It\nsystematically captures the topological proximity, attributes affinity, and\nfeature similarity of edges. Contrary to current advances in edge embedding\nresearch, our proposal extends the body of methods providing representations\nfor edges, capturing graph attributes in an inductive and unsupervised manner.\nExperimental results show that, compared to contemporary approaches, our method\nbuilds more powerful edge vector representations, reflected by higher quality\nmeasures (AUC, accuracy) in downstream tasks as edge classification and edge\nclustering. It is also confirmed by analyzing low-dimensional embedding\nprojections.",
  "text": "AttrE2vec: Unsupervised Attributed Edge Representation\nLearning\nPiotr Bielaka, Tomasz Kajdanowicza, Nitesh V. Chawlaa,b\naDepartment of Computational Intelligence, Wroclaw University of Science and Technology, Poland\nbDepartment of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA\nAbstract\nRepresentation learning has overcome the often arduous and manual featurization of net-\nworks through (unsupervised) feature learning as it results in embeddings that can apply\nto a variety of downstream learning tasks. The focus of representation learning on graphs\nhas focused mainly on shallow (node-centric) or deep (graph-based) learning approaches.\nWhile there have been approaches that work on homogeneous and heterogeneous net-\nworks with multi-typed nodes and edges, there is a gap in learning edge representations.\nThis paper proposes a novel unsupervised inductive method called AttrE2Vec, which\nlearns a low-dimensional vector representation for edges in attributed networks. It sys-\ntematically captures the topological proximity, attributes aﬃnity, and feature similarity\nof edges. Contrary to current advances in edge embedding research, our proposal extends\nthe body of methods providing representations for edges, capturing graph attributes in\nan inductive and unsupervised manner. Experimental results show that, compared to\ncontemporary approaches, our method builds more powerful edge vector representations,\nreﬂected by higher quality measures (AUC, accuracy) in downstream tasks as edge classi-\nﬁcation and edge clustering. It is also conﬁrmed by analyzing low-dimensional embedding\nprojections.\nKeywords:\nrepresentation learning, graphs, edge embedding, random walk, neural\nnetwork, attributed graph.\n1. Introduction\nComplex networks, included attributed and heterogeneous networks, are ubiquitous\n— from recommender systems to citation networks and biological systems [1]. These\nnetworks present a multitude of machine learning problem statements, including node\nclassiﬁcation, link prediction, and community detection. A fundamental aspect of any\nsuch machine learning (ML) task, transductive or inductive, is the availability of fea-\nturized data. Traditionally, researchers have identiﬁed several network characteristics\nsuited to speciﬁc ML tasks and used them for the learning algorithm. This practice is\narduous as it often entails customizing to each speciﬁc ML task, and also is limited to\nthe computable characteristics.\nThis has led to a surge in (unsupervised) algorithms and methods that learn embed-\ndings from the networks, such that these embeddings form the featurized representation\nPreprint submitted to Information Sciences\nJanuary 1, 2021\narXiv:2012.14727v1  [cs.LG]  29 Dec 2020\nFigure 1: Our proposed AttrE2vec model compared to other methods in the task of an attributed graph\nembedding. Colors denote edge features. On the left we can see a graph, where the features are aligned\nto substructures of the graph. On the right, the features were shuﬄed (ca. 50%). Traditional approaches\nfail to build robust representations, whereas our method includes features information to construct the\nembedding vectors.\nof the network for the ML tasks [2, 3, 4, 5, 6]. This area of research is generally no-\ntated as representation learning in networks. Generally, these embeddings generated by\nrepresentation learning methods are agnostic to the end use-case, as they are generated\nin an unsupervised fashion. Traditionally, the focus was on representation learning on\nhomogeneous networks, i.e. the networks that have singular type of nodes and edges,\nand also do not have attributes attached to the nodes and edges [4].\nExisting representation learning models mainly focus on transductive learning, where\na model can only be trained using the entire input graph.\nIt means that the model\nrequires all the nodes and a ﬁxed structure of the network in the training phase, e.g.,\nNode2vec [7], DeepWalk [8] and GCN [9], to some extent.\nBesides, there have been\nmethods focused on heterogeneous networks that incorporate diﬀerent typed nodes and\nedges in a network, as well as content at each node [10, 11].\nOn the other hand, a less explored and exploited approach is the inductive setting. In\nthis approach, only a part of the network is used to train the model to infer embeddings\nfor new nodes. Several attempts have been made in the inductive setting including EP-B\n[12], GraphSAGE [13], GAT [14], SDNE [15], TADW [16], AHNG[17] or PVECB [18].\nThere is also recent progress on heterogeneous graph embedding, e.g., MIFHNE [19] or\n2\nmodels based on graph neural networks [20].\nState-of-the-art network embedding techniques are mostly unsupervised, i.e., aim at\nlearning low-dimensional representations that preserve the structure of an input graph,\ne.g., GraphSAGE [13], DANE [21], line2vec [22], RCAN [23]. Nevertheless, semi-supervised\nor supervised methods can learn vector representations but for a speciﬁc downstream pre-\ndiction task, e.g., TADW [16] or FSCNMF [24]. Hence it has been shown in the literature\nthat not much supervision is required to learn the embeddings.\nIn recent years, proposed models mainly focus on the graphs that do not contain\nattributes related to nodes and edges [4]. It is especially noticeable for edge attributes.\nThe majority of proposed approaches consider node attributes only, omitting the richness\nof edge feature space while learning the representation. Nevertheless, there have been\nsuccessfully introduced such models as DANE [21], GraphSAGE [13], SDNE [15] or\nCAGE [25] which make use of node features and EGNN [26], NEWEE [27], EGAT [28]\nthat consume edge attributes.\nTable 1: Comparison of most representative graph embedding methods with their abilities to learn\nthe representation, with or without attributes, reasoning types and short characteristics.\nThe most\nprominent and appropriate methods selected to compare to AttrE2vec in experiments are marked with\nbold text.\nMethod\nRepresentation\nAttributed\nReasoning\nFamily\nNodes\nEdges\nNodes\nEdges\nTransduct.\nInduct.\nSupervised\nECN [29] (2016)\n✓\n✓\nneigh. aggr.\nGCN [9] (2017)\n✓\n✓\n✓\n✓\nGCN/GNN\nECC [30] (2017)\n✓\n✓\n✓\nGCN, DL\nFSCNMF [24] (2018)\n✓\n✓\n✓\nGCN\nGAT [14] (2018)\n✓\n✓\n✓\n✓\nAE, DL\nPlanetoid [31] (2018)\n✓\n✓\n✓\n✓\nGNN\nEGNN [26] (2019)\n✓\n✓\n✓\n✓\n✓\n✓\nGNN\nEdgeConv [32] (2019)\n✓\n✓\nGNN\nEGAT [28] (2019)\n✓\n✓\n✓\n✓\n✓\n✓\nGNN\nAttribute2vec [33] (2020)\n✓\n✓\n✓\nGCN\nUnsupervised\nDeepWalk [8] (2014)\n✓\n✓\nRW, skip-gram\nTADW [16] (2015)\n✓\n✓\n✓\nRW, MF\nLINE [34] (2015)\n✓\n✓\nRW, skip-gram\nNode2vec [7] (2016)\n✓\n✓\nRW, skip-gram\nSDNE [15] (2016)\n✓\n✓\n✓\n✓\nAE\nGraphSAGE [13] (2017)\n✓\n✓\n✓\n✓\nRW\nEP-B [12] (2017)\n✓\n✓\n✓\n✓\nAE\nStruc2vec [35] (2017)\n✓\n✓\nRW, skip-gram\nDANE [21] (2018)\n✓\n✓\n✓\n✓\nAE\nLine2vec [22] (2019)\n✓\n✓\nRW, skip-gram\nNEWEE [27] (2019)\n✓\n✓\n✓\n✓\nRW, skip-gram\nAttrE2vec (2020)\n✓\n✓\n✓\n✓\n✓\nRW, AE, DL\nBoth node-based embedding methods and graph neural network inspired methods do\nnot generalize eﬀectively to both transductive and inductive settings, especially when\nthere are attributes associated with edges. This work is motivated by the idea of un-\nsupervised learning on networks with attributed edges such that the embeddings are\ngeneralizable across tasks and are inductive.\nTo that end, we develop a novel AttrE2vec, an unsupervised learning model that\nadapts auto-encoder and self-attention network with the use of feature reconstruction and\ngraph structural loss. To learn edge representation, AttrE2vec splits edge neighborhood\ninto two parts, separately for each node endings of the edge, and then generates random\n3\nedge walks in both neighborhoods. All walks are then aggregated over the node and edge\nattributes using one of the proposed strategies (Avg, Exp, GRU, ConcatGRU). These\nare accumulated with the original nodes and edge features and then fed to attention\nand dense layer to encode the edge. The embeddings are subsequently inferred via a\ntwo-step loss function — for both feature reconstruction and graph structural loss. As a\nconsequence, AttrE2vec can explicitly incorporate feature information from nodes and\nedges at many hops away to eﬀectively produce the plausible edge embeddings for the\ninductive setting.\nIn summary, our main contributions are as follows:\n• we propose a novel unsupervised AttrE2vec method, which learns a low-dimensional\nvector representation for edges that are attributed\n• we exploit the concept of a graph-topology-driven edge feature aggregation, from\nsimple ones to learnable GRU based, that captures edge topological proximity and\nsimilarity of edge features\n• the proposed method is inductive and allows getting the representation for edges\nnot present in the training phase\n• we conduct various experiments and show that our AttrE2vec method has superior\nperformance over all of the baseline methods on edge classiﬁcation and clustering\ntasks.\n2. Related work and Research Gap\nEmbedding information networks has received signiﬁcant interest from the research\ncommunity. We refer the readers to the survey articles for a comprehensive overview of\nnetwork embedding [4, 5, 3, 2] and cite only some of the most prominent works that are\nrelevant.\nUnsupervised network embedding methods use only the network structure or\noriginal attributes of nodes and edges to construct embeddings.\nThe most common\nmethod is DeepWalk [8], which in two-phases constructs node neighborhoods by per-\nforming ﬁxed-length random walks and employs the skip-gram [7] model to preserve the\nco-occurrences between nodes and their neighbors. This two-phase framework was later\nan inspiration for learning network embeddings by proposing diﬀerent strategies for con-\nstructing node neighborhoods or modeling co-occurrences between nodes, e.g., node2vec\n[7], Struc2vec [35], GraphSAGE [13], line2vec [22] or NEWEE [27]. Another group of un-\nsupervised methods utilizes auto-encoder or graph neural networks to obtain embedding.\nSDNE [15] uses auto-encoder architecture to preserve ﬁrst and second-order proximities\nby jointly optimizing the loss in neighborhood reconstruction.\nAnother auto-encoder\nbased representatives are EP-B [12] and DANE [21].\nSupervised network embedding methods are constructed as an end-to-end meth-\nods for particular tasks like node classiﬁcation or link prediction. These methods require\nnetwork structure, attributes of nodes and edges (if method is capable of using) and\nsome annotated target like node class.\nThe representatives are ECN [29], ECC [30],\nFSCNMF [24], GAT [14], planetoid [31], EGNN [26], GCN [9], EdgeConv [32], EGAT\n[28], Attribute2vec [33].\n4\nEdge representation learning has been already tackled by several methods, i.e.\nECN [29], EGNN [26], line2vec [22], EdgeConv [32], EGAT [28]. However, non of these\nmethods was able to directly take into account attributes of edges as well as perform the\nlearning in an unsupervised manner.\nAll the characteristics of the representative node and edge representation learning\nmethods are grouped in Table 1.\n3. Method\n3.1. Motivation\nIn the following paragraphs, we explain our three-fold motivation to propose the\nAttrE2vec.\nEdge embeddings. For a decade, network processing approaches gather more and more\nattention as graph data is produced in an increasing number of systems. Network em-\nbedding traditionally provided the notion of vectorizing nodes that was used in node\nclassiﬁcation or clustering. However, the edge representation learning did not gather\nenough attention and was accomplished through node embedding transformation [36].\nNevertheless, such an approach is problematic. For instance, inferring edge type from\nneighboring nodes’ embeddings may not be the best choice for edge type classiﬁcation in\nheterogeneous social networks. We claim that eﬃcient edge clustering, edge attribute re-\ngression, or link prediction tasks require dedicated and speciﬁc edge representations. We\nexpect that the representation learning approach devoted strictly to edges provides more\npowerful vector representations than traditional methods that require node embeddings\ntrained upfront and transform nodes’ embedding to represent edges.\nInductive embedding methods. A vast majority of contemporary network representation\nlearning methods is transductive (see Table 1). It means that any change to the graph\nrequires the whole retraining of the method to provide predictions for unseen cases—such\nproperty limits the applicability of methods due to high computational costs. Contrary,\nthe inductive approach builds a predictive ability that can be applied to unseen cases\nand does not need retraining – in general, inductive methods have a lower computation\ncost. Considering these advantages, we expect modern edge embedding methods to be\ninductive.\nEncoding graph attributes in embeddings. Much of the real-world data exhibits rich at-\ntribute sets or meta-data that contain crucial information, e.g., about the similarity of\nnodes or edges. Traditionally, graph representation learning has been focused on ex-\nploiting the network structure, omitting the related content. Thus, we may expect to\nconsume attributes as a regularizer over the structure. It would allow overcoming the\nlimitation when the only edge discriminating ability is encoded in the edges’ attributes,\nnot in the graph’s structure. Relying only on the network would produce inconclusive\nembeddings.\n5\n3.2. Attributed graph edge embedding\nWe denote an attributed graph as G = (V, E), where V is a set of nodes and E =\n{(u, v) ∈V × V } a set of edges. Every node u and every edge e = (u, v) has associated\nfeatures: mu ∈RdV and fuv ∈RdE, where M ∈R|V |×dV and F ∈R|E|×dE are node\nand edge feature matrices, respectively. By dV we denote dimensionality of node feature\nspace and dE dimensionality of edge feature space. The edge embedding task is deﬁned\nas learning a function g : E →Rd, which takes an edge and outputs its low-dimensional\nvector representation. Note that the embedding dimension d should be much less than the\noriginal edge feature dimensionality dE, i.e.: d << dE. More speciﬁcally, we aim at using\nthe topological structure of the graph and node and edge attributes: f : (E, F, M) →Rd.\nFigure 2: Overview of the AttrE2vec model.\nThe model ﬁrst computes edge random walks on two\nneighborhoods of a given edge (u, v). Each neighbourhood walks are aggregated into Su, Sv. Both are\ncombined with the edge features fuv using an Encoder module, which results into the edge embedding\nvector huv. The loss function consists of two parts: structural loss (Lcos) and feature reconstruction loss\n(LMSE).\n3.3. AttrE2vec\nIn contrast to traditional node embedding methods, we shift the focus from nodes\nto edges and consider a graph from an edge perspective. Given any edge e = (u, v), we\ncan observe three natural sources of knowledge: the edge attributes itself and the two\nneighborhoods - Nu and Nv, located behind nodes u and v, respectively. In AttrE2vec,\nwe exploit all three sources jointly.\nFirst, we obtain aggregations (summaries) Su, Sv of the both neighborhoods Nu, Nv.\nWe want to capture the topological structure of the neighborhood, so we perform k edge\nrandom walks of length L, which start from node u (or v, respectively) and use a\nuniformly distributed neighbor sampling approach (DeepWalk-like) to obtain the next\nedge. Each ith walk wi\nu started from node u is hence a sequences of edges.\nRW(G, k, L, u) →{w1\nu, w2\nu, . . . , wk\nu}\nwi\nu ≡(u, u2), (u3, u4), . . . , (uL−1, uL)\n6\nNext, we take the attributes of the edges (and nodes, if applicable) in each random\nwalk and aggregate them into a single vector using the walk aggregation model Aggw.\nSi\nu = Aggw(wi\nu, F, M)\nLater, aggregated walks are combined using the neighborhood aggregation model\nAggn, which summarizes the neighborhood Su (and Sv, respectively). The proposed\nimplementations of these aggregation are given in Section 3.4.\nSu = Aggn({S1\nu, S2\nu, . . . , Sk\nu})\nFinally, we obtain the low dimensional edge embedding huv using an encoder Enc\nmodule. It combines the edge attributes fuv with the summarized neighborhood infor-\nmation Su, Sv. We employ a simple Multilayer Perceptron (MLP) with 3 inputs (each of\nsize equal to the edge features dimensionality) and an attention mechanism over these in-\nputs, to check how much of the information of each input is used to create the embedding\nvector (see Figure 3):\nhuv = Enc(fuv, Su, Sv)\nFigure 3: Encoder module architecture\nThe overall illustration of the method is contained in Figure 2 and the inference\nalgorithm is shown in Algorithm 1.\n3.4. Aggregation models\nFor the purpose of the neighborhood aggregation model Aggn, we use an average over\nvectors Si\nu, as there is no particular ordering of these vectors (each one was generated\nby an equally important random walk). In the case of walk aggregation, we propose the\nfollowing:\n7\nAlgorithm 1: AttrE2vec inference algorithm\nData: graph G, edge list xe, edge features F, node features M\nParams: number of random walks per node k, random walk length L\nResult: edge embedding vectors huv\nbegin\nforeach (u, v) in xe do\nforeach i in (1. . . k) do\nwi\nu = RW(G, L, u)\nSi\nu = Aggw(wi\nu, F, M)\nwi\nv = RW(G, L, v)\nSi\nv = Aggw(wi\nv, F, M)\nend\nSu = Aggn({S1\nu, . . . , Sk\nu})\nSv = Aggn({S1\nv, . . . , Sk\nv})\nhuv = Enc(fuv, Su, Sv)\nend\nend\n• average – that computes a simple average of the edge attribute vectors in the\nrandom walk;\nSi\nu = 1\nL\nL\nX\nn=1\nfunun+1\n• exponential – that computes a weighted average, where the weights are exponents\nof the ”minus” position in the random walk so that further away edges are less\nimportant than the near ones;\nSi\nu = 1\nL\nL\nX\nn=1\ne−nfunun+1\n• GRU – that uses a Gated Recurrent Unit [37] architecture, where hidden and input\ndimension is equal to the edge attribute dimension; the aggregated representation\nis the output of the last hidden vector; the aggregation process starts here at the\nend of the random walk and proceeds to the beginning;\nSi\nu = GRU({funun+1, fun−1un, . . . , fu1u2})\n• ConcatGRU – that is similar to the GRU-based aggregator, but here we also\nuse the node feature information by concatenating the node attributes with the\nedge attributes; hence the GRU input size is equal to the sum of the edge and\nnode dimensions; in case there are not any node features available, one could use\n8\nnetwork-speciﬁc features, like degree, betweenness or more advanced techniques like\nNode2vec; the hidden dimension size and the aggregation direction is unchanged;\nSi\nu = ConcatGRU({funun+1 ⊕mun, . . . , fu1u2 ⊕mu1})\n3.5. Learning AttrE2vec’s parameters\nAttrE2vec is designed to make the most use of edge attributes and information about\nthe structure of the network. Therefore we propose a loss function, which consists of two\nmain parts:\n• structural loss Lcos – computes a cosine embedding loss; such function tries to\nminimize the cosine distance between a given embedding h and embeddings of edges\nsampled from the random walks h+ (positive), and simultaneously to maximize a\ncosine distance between an embedding h and embeddings of edges sampled from a\nset of all edges in the graph h−(negative), except for these in the random walks:\nLcos =\n1\n|B|\nX\nhuv∈B\n\nX\nh+\nuv\n(1 −cos(huv, h+\nuv)) +\nX\nh−\nuv\ncos(huv, h−\nuv)\n\n\nwhere B denotes a minibatch of edges and |B| the minibatch size,\n• feature reconstruction loss LMSE – computes a mean squared error of the actual\nedge features and the outputs of a decoder (implemented as a 3-layer MLP – see\nFigure 4), that reconstruct the edge features based on the edge embeddings;\nLMSE =\n1\n|B|\nX\n(huv,fuv)∈B\n(DEC(huv) −fuv)2\nwhere B denotes a minibatch of edges and |B| the minibatch size.\nFigure 4: Decoder module architecture\nWe combine the values of the above loss functions using a mixing parameter λ ∈[0, 1].\nThe higher the value of this parameter is, the more structural information is preserved\nand less focus is one the feature reconstruction. The total loss of AttrE2vec is given as\nfollows:\nL = λ ∗Lcos + (1 −λ) ∗LMSE\n9\n4. Experiments\nTo evaluate the proposed model’s performance, we perform three tasks: edge classi-\nﬁcation, edge clustering, and embedding visualization on three real-world datasets. We\nﬁrst train our model on a small subset of edges (inductive setting). Then we use the\nmodel to infer embeddings for edges from the test set. Finally, we evaluate them in all\ndownstream tasks: by predicting the class of edges in citation graphs (edge classiﬁ-\ncation), by applying the K-means++ algorithm (edge clustering; as deﬁned in [22])\nand by the dimensionality reduction method T-SNE (embedding visualization). We\ncompare our model to several baselines and contemporary methods in all experiments,\nsee Table 1. Eventually, we check the inﬂuence of AttrE2vec’s hyperparameters and per-\nform an ablation study on artiﬁcially generated datasets. We implement our model in\nthe popular deep learning framework PyTorch. All experiments were performed on an\nNVIDIA GTX1080Ti. Upon acceptance in the journal, we will make our code available\nat https://github.com/attre2vec/attre2vec and include our DVC [38] pipeline so\nthat all experiments can be easily reproduced.\n4.1. Datasets\nTable 2: Datasets used in the experiments.\nName\nFeatures\nNumber of\nTraining instances\ninitial\npre-processed\nnode\nedge\nnode\nedge\nnodes\nedges\nclasses\ninductive\ntransductive\nCora\n1 433\n0\n32\n260\n2 485\n5 069\n7+1\n160\n5 069\nCiteseer\n3 703\n0\n32\n260\n2 110\n3 668\n6+1\n140\n3 668\nPubmed\n500\n0\n32\n260\n19 717\n44 324\n3+1\n80\n44 324\nIn order to compare gathered evaluation evidence we focused on well known datasets,\nthat appear in the literature, namely: Cora [39], Citeseer [39] and Pubmed [40]. These\nare citation networks of scientiﬁc papers in several research areas, where nodes are the\npapers and edges denote citations between papers. We summarize basic statistics about\nthe datasets before and after pre-processing steps in Table 2.\nRaw datasets contain\nnode features only in the form of high dimensional sparse bags of words. For Cora and\nCiteseer, these are binary vectors, showing which of the most popular words were used\nin a given paper, and for Pubmed, the features are in the form of TF-IDF vectors. To\nadjust the datasets to our problem setting, we apply the following pre-processing steps\nto obtain edge level features, which are used to train and evaluate our AttrE2vec model:\n• we create dense vector representations of the nodes’ features by applying Doc2vec\n[41] in the PV-DBOW variant with a target dimension size of 128;\n• for each edge (u, v) and its symmetrical version (v, u) (necessary to perform uni-\nform, undirected random walks) we extract the following features:\n– 1 feature – cosine similarity of raw node features for nodes u and v (binary\nBoW; for Pubmed transformed from TF-IDF to binary BoW),\n10\n– 2 features – the ratios of the number of used words (number of ones in the\nBoW) to all possible words in the document (length of BoW vector) in each\npaper u and v,\n– 256 features – concatenation of Doc2vec features for nodes u and v,\n– 1 feature – a binary indicator, which denotes whether this is an original edge\n(1) or its symmetrical counterpart (0),\n• we apply standardization (StandardScaler in Scikit-Learn [42]) of the edge feature\nmatrix.\nMoreover, we extracted new node features as 32-dimensional Node2vec embeddings\nto provide the evaluation possibility for one of our model versions (AttrE2vec with Con-\ncatGRU aggregator), which generalizes upon both edge and nodes attributes.\nRaw datasets provide each node labeled by the research area the paper comes from. To\napply this knowledge in the edge classiﬁcation problem setting, we applied the following\nrule: if an edge has two nodes from the same class (research area), the edge receives this\nclass; if two nodes have diﬀerent classes, the edge between these nodes is assigned with\na cross-domain citation class.\nTo ensure a fair comparison method, we follow the dataset preparation scheme from\nEP-B [12], i.e., for each dataset (Cora, Citeseer, Pubmed) we sample 10 train/validation/test\nsets, where the train set consists of 20 edges per class and the validation and test sets\nto contain 1 000 randomly chosen edges each. While reporting the resulting metrics, we\nshow the mean values over these ten sampled sets (together with the standard deviation).\n4.2. Baselines\nWe compare our method against several baseline methods. In the most simple case,\nwe use the edge features obtained during the pre-processing phase for all datasets (further\nreferred to as Doc2vec).\nMany standard approaches employ simple node embedding transformations to obtain\nedge embeddings. The authors of Node2vec [36] proposed binary operators like averaging,\nHadamard product, or L1 and L2 norms of vector diﬀerences. Here, we will use following\nmethods to obtain node embeddings: DeepWalk [8], Node2vec [36], SDNE [43] and\nStruc2vec [35]. In preliminary experiments, we evaluated these methods and checked\nthat the Average operator and an embedding size of 64 gives the best results. We will\nuse these models in 2 setups: (a) Avg(M,M) – using only the averaged node features,\n(b) Avg(M,M)⊕F – like previously but concatenated with the edge features from the\ndataset (in total 324-dim vectors).\nWe also checked a scheme to compute a 64-dim PCA reduction of the concatenated\nfeatures to have comparable vector sizes with the 64-dimensional embedding of our model,\nbut these turned out to perform poorly. Note that SDNE has the capability of inductive\nreasoning, but due to the non-availability of such implementation, we decided to evaluate\nthis method in the transductive scheme (which works in favor of the method).\n11\nFigure 5: Architecture of the MLP(M,M).\nFigure 6: Architecture of the MLP(M,M,F).\nWe also extend our body of baselines by more sophisticated approaches – two dense\nautoencoder architectures.\nIn the ﬁrst setting MLP(M,M), we train a model (see\nFigure 5), which reconstructs concatenated embeddings of connected nodes. In the second\nbaseline MLP(M,M,F), the autoencoder (see Figure 6) is extended by edge attributes.\nIn both settings, we employ the mean squared error as the model loss function. The\noutput of the encoders (embeddings) is used in the downstream tasks. The input node\nembeddings are obtained using the methods mentioned above, i.e., DeepWalk, Node2vec,\nSDNE, and Struc2vec.\nThe last baseline is Line2vec [22], which is directly dedicated for edges - we use an\nembedding size of 64.\n4.3. Edge classiﬁcation\nTo evaluate our model in an inductive setting, we need to make sure that test edges\nare unseen during the model training procedure – we remove them from the graph. Note\nthat all baselines (except for GraphSage, see 1) require all edges during the training\nphase (i.e., these are transductive methods).\nAfter each training epoch of AttrE2vec, we evaluate the embeddings using L2-\nregularized Logistic Regression (LR) classiﬁer and compute AUC. The regression model\nis trained on edge embeddings from the train set and evaluated on edge embeddings from\nthe validation set. We take the model with the highest AUC value on the validation set.\n12\nTable 3: AUC values for edge classiﬁcation. F denotes the edge attributes (also referred to as ”Doc2vec”),\nM – node attributes (e.g., embeddings computed using ”Node2vec”), ⊕– concatenation operator,\nAvg(M,M) – average operator on node embeddings, MLP(·) – encoder output of MLP autoencoder\ntrained on given attributes. AUC in bold shows the highest value and AUC in italic — the second\nhighest value.\nMethod group/name\nVector\nAUC\nsize\nCiteseer\nCora\nPubmed\nTransductive\nEdge features only; F\n(Doc2vec)\n260\n86.13 ± 0.95\n88.67 ± 0.51\n79.15 ± 1.41\nLine2vec\n64\n86.19 ± 0.28\n91.75 ± 1.07\n84.88 ± 1.19\nAvg(M,M)\nDeepWalk\n64\n58.40 ± 1.08\n59.98 ± 1.32\n51.04 ± 1.23\nNode2vec\n64\n58.26 ± 0.89\n59.59 ± 1.11\n51.03 ± 1.01\nSDNE\n64\n54.28 ± 1.57\n55.91 ± 1.11\n50.00 ± 0.00\nStruc2vec\n64\n61.29 ± 0.86\n61.30 ± 1.58\n54.67 ± 1.46\nMLP(M,M)\nDeepWalk\n64\n55.88 ± 1.68\n57.87 ± 1.53\n51.23 ± 0.77\nNode2vec\n64\n55.35 ± 2.26\n57.44 ± 0.87\n51.48 ± 1.55\nSDNE\n64\n55.56 ± 0.93\n56.02 ± 1.22\n50.00 ± 0.00\nStruc2vec\n64\n59.93 ± 1.43\n59.76 ± 1.80\n53.27 ± 1.32\nAvg(M,M)⊕F\nDeepWalk\n324\n86.13 ± 0.95\n88.67 ± 0.51\n79.15 ± 1.41\nNode2vec\n324\n86.13 ± 0.95\n88.67 ± 0.51\n79.15 ± 1.41\nSDNE\n324\n86.14 ± 1.03\n88.70 ± 0.51\n79.15 ± 1.41\nStruc2vec\n324\n86.21 ± 0.97\n88.73 ± 0.48\n79.24 ± 1.36\nMLP(M,M,F)\nDeepWalk\n64\n84.58 ± 1.11\n86.47 ± 0.87\n78.60 ± 1.84\nNode2vec\n64\n84.65 ± 1.05\n86.71 ± 0.68\n78.84 ± 1.71\nSDNE\n64\n84.32 ± 1.13\n85.99 ± 0.77\n78.34 ± 1.07\nStruc2vec\n64\n83.95 ± 1.16\n85.54 ± 0.96\n77.19 ± 1.42\nInductive\nAvg(M,M)\nGraphSage\n64\n54.84 ± 1.90\n55.16 ± 1.36\n51.14 ± 1.64\nMLP(M,M)\nGraphSage\n64\n55.19 ± 1.04\n55.47 ± 1.66\n50.36 ± 1.54\nAvg(M,M)⊕F\nGraphSage\n324\n86.14 ± 0.95\n88.68 ± 0.51\n79.16 ± 1.41\nMLP(M,M,F)\nGraphSage\n64\n84.63 ± 1.11\n86.14 ± 0.45\n78.00 ± 1.85\nAttrE2vec (our)\nAvg\n64\n88.97 ± 0.82\n93.43 ± 0.56\n87.68 ± 1.25\nExp\n64\n88.91 ± 1.10\n92.80 ± 0.38\n86.18 ± 1.41\nGRU\n64\n88.92 ± 1.13\n93.06 ± 0.63\n86.39 ± 1.21\nConcatGRU\n64\n88.56 ± 1.34\n92.93 ± 0.61\n86.34 ± 1.18\nMoreover, an early stopping strategy is implemented– if the validation AUC metric does\nnot improve for more than 15 epochs, the learning is terminated. Our approach to model\nselection is aligned with the schema proposed in [44] because this approach is more nat-\nural than relying on the loss function. This is repeated for all 10 data splits (see: Section\n4.1 for details). We report a mean and std AUC measures for 10 test sets (see Table 3)\nWe choose AdamW [45] with a learning rate of 0.001 to optimize our model’s pa-\nrameters. We also set the size of positive samples to |h+| = 5 and negative samples\nto |h−| = 10 in the cosine embedding loss. The mixing coeﬃcient is set to λ = 0.5,\nequally including the inﬂuence of features and topological graph structure. We choose\nan embedding size of 64 as a reasonable value while dealing with edge features of size\n260.\nIn Table 3, we summarize the AUC values for baseline methods and for our model.\nEven though vectors’ original dimensionality is relatively high (260), good results are\nalready yielded using only the edge features (Doc2vec).\nHowever, adding structural\ninformation about the graph could further improve the results.\nUsing representations from node embedding methods, which are transformed to edge\n13\nembeddings using the average operator Avg(M,M), achieve poor results of about 50-\n60% AUC. However, if these are combined with the edge features from the datasets\nAvg(M,M)⊕F, the AUC values increase signiﬁcantly to about 86%, 88% and 79% for\nCiteseer, Cora, and Pubmed, respectively. Unfortunately, this results in an even higher\nvector dimensionality (324).\nThe MLP-based approach results lead to similar conclusions. Using only node em-\nbeddings MLP(M,M) we achieve quite poor results of about 50% (on Pubmed) up to\n60% (on Cora). With MLP(M,M,F) approach we observe that edge features improve\nthe classiﬁcation results. The AUC values are still slightly worse than concatenation\noperator (Avg(M,M)⊕F), but we can reduce the edge embedding size to 64.\nThe Line2vec [22] algorithm achieves very good results, without considering edge\nfeatures information – we get about 86%, 92% and 85% AUC for Citeseer, Cora, and\nPubmed, respectively. These values are higher than for any other baseline approach.\nOur model performs the best among all evaluated methods. For Citeseer, we gain\nabout 3 percent points compared to the best baselines: Line2vec, Struc2vec (Avg(M,M)⊕F)\nor GraphSage (Avg(M,M)⊕F). Note that the algorithm is trained only on 140 edges\nin the inductive setting, whereas all transductive baselines require the whole graph for\ntraining. The gains on Cora are 2 pp, and on Pubmed we achieve up to 4pp (and up\nto 8pp compared only to GraphSage (Avg(M,M)⊕F)). Our model with the Average\n(Avg) aggregator works the best, whereas the Gated Recurrent Unit (GRU) aggregator\nachieves the second-best results.\n4.4. Edge clustering\nSimilarly to Line2vec [22], we apply the K-Means++ algorithm on the resulting em-\nbedding vectors and compute an unsupervised clustering accuracy [46]. We summarize\nthe results in Table 4. Our model performs the best in all but one case and achieves\nsigniﬁcantly better results than other baseline methods. The only exception is for the\nPubmed dataset, where Line2vec achieves the best clustering accuracy. Other baseline\nmethods perform similarly as in the edge classiﬁcation task. Hence, we will not discuss\nthe details, and we encourage the reader to go through the detailed results.\n4.5. Embedding visualization\nFor all tested baseline methods and our proposed AttrE2vec method, we compute\n2-dimensional projections of the produced embeddings using T-SNE [47] method. We\nvisualize them in Figure 7. In our subjective opinion, these plots correspond to the AUC\nscores reported in Table 3—the higher the AUC, the better the group separation. In\ndetails, for Doc2vec raw edge features seem to form groups, but unfortunately overlap\nto some degree. We cannot observe any pattern in the node embedding-based settings\n(Avg(M,M) and MLP(M,M)), they tempt to be quasi-random. When concatenated\nwith the edge attributes (Avg(M,M)⊕F and MLP(M,M,F)) we observe a slightly\nbetter grouping, but yet non satisfying. AttrE2vec model produces much more formed\ngroups, with only a little overlapping. To summarize, based on the observed groups’\nseparability and AUC metrics, our approach works the best among all methods.\n14\nFigure 7: 2-D T-SNE projections of embedding vectors for all evaluated methods. Columns denotes\naggregation approach, beside F that denotes the edge attributes and g(E) that is an edge embedding\nobtained with graph structure only. Rows gather particular methods.\n15\nTable 4: Accuracy on edge clustering. F denotes the edge attributes (also referred to as ”Doc2vec”),\nM – node attributes (e.g., embeddings computed using ”Node2vec”), ⊕– concatenation operator,\nAvg(M,M) – average operator on node embeddings, MLP(·) – encoder output of MLP autoencoder\ntrained on given attributes. AUC in bold shows the highest value and AUC in italic — the second\nhighest value.\nMethod group/name\nVector\nAccuracy\nsize\nCiteseer\nCora\nPubmed\nTransductive\nEdge features only; F (Doc2vec)\n260\n54.13 ± 2.73\n54.64 ± 5.86\n46.33 ± 1.53\nLine2vec\n64\n54.73 ± 2.56\n63.50 ± 1.92\n55.26 ± 1.36\nAvg(M,M)\nDeepWalk\n64\n28.89 ± 1.06\n21.93 ± 0.86\n27.24 ± 0.50\nNode2vec\n64\n26.82 ± 0.67\n21.32 ± 0.62\n27.17 ± 0.74\nSDNE\n64\n21.01 ± 0.50\n17.97 ± 0.47\n31.38 ± 0.69\nStruc2vec\n64\n25.21 ± 1.33\n20.15 ± 0.64\n32.02 ± 1.49\nMLP(M,M)\nDeepWalk\n64\n26.36 ± 1.37\n21.06 ± 0.57\n27.40 ± 0.93\nNode2vec\n64\n26.37 ± 1.64\n21.31 ± 0.98\n27.67 ± 0.78\nSDNE\n64\n22.27 ± 0.76\n17.15 ± 0.36\n28.44 ± 1.21\nStruc2vec\n64\n24.22 ± 0.83\n19.56 ± 0.49\n31.31 ± 1.70\nAvg(M,M)⊕F\nDeepWalk\n324\n54.13 ± 2.73\n54.70 ± 5.85\n46.33 ± 1.53\nNode2vec\n324\n54.13 ± 2.73\n54.70 ± 5.85\n46.33 ± 1.53\nSDNE\n324\n55.29 ± 2.06\n55.43 ± 4.63\n46.33 ± 1.53\nStruc2vec\n324\n55.59 ± 1.51\n52.47 ± 6.52\n46.32 ± 1.29\nMLP(M,M,F)\nDeepWalk\n64\n48.74 ± 4.03\n47.38 ± 4.72\n46.49 ± 1.20\nNode2vec\n64\n50.80 ± 2.30\n48.48 ± 3.38\n46.15 ± 1.43\nSDNE\n64\n46.17 ± 3.15\n44.87 ± 3.54\n45.74 ± 1.89\nStruc2vec\n64\n47.35 ± 3.73\n44.38 ± 3.04\n45.40 ± 1.72\nInductive\nAvg(M,M)\nGraphSage\n64\n18.79 ± 0.62\n17.70 ± 1.05\n27.04 ± 0.71\nMLP(M,M)\nGraphSage\n64\n18.92 ± 0.98\n17.89 ± 0.85\n27.09 ± 0.81\nAvg(M,M)⊕F\nGraphSage\n324\n54.06 ± 2.54\n54.82 ± 6.86\n46.49 ± 1.64\nMLP(M,M,F)\nGraphSage\n64\n48.79 ± 4.04\n47.49 ± 5.41\n45.15 ± 1.54\nAttrE2vec (our)\nAvg\n64\n59.82 ± 3.30\n65.42 ± 1.71\n48.86 ± 2.46\nExp\n64\n59.07 ± 4.65\n66.36 ± 3.62\n48.02 ± 2.55\nGRU\n64\n60.16 ± 2.25\n66.15 ± 3.71\n49.41 ± 1.49\nConcatGRU\n64\n60.71 ± 2.75\n66.00 ± 2.21\n50.27 ± 3.75\n5. Hyperparameter Sensitivity of AttrE2vec\nWe investigate hyperparameters’ eﬀect considering each of them independently, i.e.,\nsetting a given parameter and preserving default values for all other parameters. The\nevaluation is applied for our model’s two inductive variants: with the Average aggregator\nand with the GRU aggregator. We use all three datasets (Cora, Citeseer, Pubmed) and\nreport the AUC values. We choose following hyperparameter value sets (values with an\nasterisk denote the default value for that parameter):\n• length of random walk: L = {4, 8∗, 16},\n• number of random walks: k = {4, 8, 16∗},\n• embedding size: d = {16, 32, 64∗},\n• mixing parameter: λ = {0, 0.25, 0.5∗, 0.75, 1}.\n16\nFigure 8: Eﬀects of hyperparameters on Cora, Citeseer and Pubmed datasets.\nThe results of all experiments are summarized in Figure 8. We observe that for both\naggregation variants, Avg and GRU, the trends are similar, so we will include and discuss\nthem based only on the Average aggregator.\nIn general, the higher the number of random walks k and the length of a single\nrandom walk L, the better results are achieved. One may require higher values of these\nparameters, but it signiﬁcantly increases the random walk computation time and the\nmodel training itself.\nUnsurprisingly, the embedding size (embedding dimension) also follows the same\ntrend. With more dimensions, we can ﬁt more information into the created representa-\ntions. However, as an embedding goal is to ﬁnd low-dimensional vector representations,\nwe should keep reasonable dimensionality. Our chosen values (16, 32, 64) seem plausible\nwhile working with 260-dimensional edge features.\nAs for loss mixing parameter λ, we observe that too high values negatively inﬂuence\nthe model performance. The greater the value, the more critical the structural loss be-\ncomes. Simultaneously the feature loss becomes less relevant. Choosing λ = 0 causes\nthe loss function to consider feature reconstruction only and completely ignores the em-\nbedding loss. This yields signiﬁcantly worse results and conﬁrms that our approach of\ncombining both feature reconstruction and structural embedding loss is justiﬁed.\nIn\ngeneral, the best values are achieved for setting an equal inﬂuence of both loss factors\n(λ = 0.5).\n6. Ablation study\nWe performed an ablation study to check whether our method AttrE2vec is invariant\nto introduced noise in an artiﬁcially generated network. We use a barbell graph, which\n17\nFigure 9: AttrE2vec performance for various noise levels p and mixing parameter values λ ∈{0, 0.5, 1}.\nFigure 10: 2-D representations of ideal and noisy graph edges using AttrE2vec with λ ∈{0, 0.5, 1}.\n18\nconsists of two fully connected graphs and a path which connects them (see: Figure 1).\nThe graph has seven nodes in each full graph and seven nodes in the path – a total of\n50 edges. Next, we generate features from 3 clusters in a 200-dimensional space using\nisotropic Gaussian blobs. We assign the features to 3 parts of the graph: the ﬁrst to\nthe edges in one of the full graphs, the second to the edges in the path and the third\nto the edges in the other full graph. The edge classes are matching the feature clusters\n(i.e., three classes). Therefore, the structure is aligned with the features, so any good\nstructure based embedding method can ﬁt this data very well (see: Figure 1). A problem\noccurs when the features (and hence the classes) are shuﬄed within the graph structure.\nMethods that employ only a structural loss function will fail. We want to check how our\nmodel AttrE2vec, which includes both structural and feature-based loss, performs with\ndiﬀerent amount of such noise.\nWe will use the graph mentioned above and introduce noise by shuﬄing p% of all\nedge pairs, which are from diﬀerent classes, i.e., an edge with class 2 (originally lo-\ncated in the path) may be swapped with one from the full graphs (classes 1 or 3).\nWe use our AttrE2vec model with an Average aggregator in the transductive setting\n(due to the graph size) and report the edge classiﬁcation AUC for diﬀerent values of\np ∈{0, 0.1, . . . , 0.5, . . . , 0.9, 1} and λ ∈{0, 0.5, 1}. The values of the mixing parameter λ\nallow us to check how the model behaves when working only with a feature-based loss\n(λ = 0), only with a structural loss (λ = 1), and with both losses at equal importance\n(λ = 0.5). We train our model for ﬁve epochs and repeat the computations ten times for\nevery (p, λ) pair, due to the shuﬄing procedure’s randomness. We report the mean and\nstandard deviation of the AUC value in Figure 9.\nUsing only the feature loss or a combination of both losses allows us to achieve nearly\n100% AUC in the classiﬁcation task. The ﬂuctuations appear due to the low number\nof training epochs and the local optima problem. The performance of the model that\nuses only structural loss (λ = 1) decreases with higher shuﬄing probabilities, and from\na certain point, it starts improving slightly because shuﬄing results in a complete swap\nof two classes, i.e., all features and classes from one graph part are exchanged with all\nfeatures and classes from another part of the graph.\nWe also demonstrate how our method reacts on noisy data with various λ ∈{0, 0.5, 1}.\nThere are two graphs: one where the features are aligned to substructures of the graph\nand the second with shuﬄed features (ca. 50%), see Figure 10. Keeping AttrE2vec with\nλ = 0.5 allows to represent noisy graphs fairly.\n7. Conclusions and future work\nWe introduce AttrE2vec – the novel unsupervised and inductive embedding model to\nlearn attributed edge embeddings by leveraging on the self-attention network with auto-\nencoder over attribute space and structural loss on aggregated random walks. Attre2vec\ncan directly aggregate feature information from edges and nodes at many hops away\nto infer embeddings not only for present nodes, but also for new nodes.\nExtensive\nexperimental results show that AttrE2vec obtains the state-of-the-art results in edge\nclassiﬁcation and clustering on CORA, PUBMED and CITESEER.\n19\nAcknowledgments\nThe work was partially supported by the National Science Centre, Poland grant No.\n2016/21/D/ST6/02948, and 2016/23/B/ST6/01735, as well as by the Department of\nComputational Intelligence, Wroc law University of Science and Technology statutory\nfunds.\nReferences\n[1] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, J. Leskovec, R. Barzilay,\nP. Battaglia, Y. Bengio, M. Bronstein, S. G¨unnemann, W. Hamilton, T. Jaakkola, S. Jegelka,\nM. Nickel, C. Re, L. Song, J. Tang, M. Welling, R. Zemel, Open graph benchmark: Datasets for\nmachine learning on graphs (may 2020). arXiv:2005.00687.\nURL http://arxiv.org/abs/2005.00687\n[2] D. Zhang, J. Yin, X. Zhu, C. Zhang, Network Representation Learning: A Survey, IEEE Transac-\ntions on Big Data 6 (1) (2018) 3–28. doi:10.1109/tbdata.2018.2850013.\n[3] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, P. S. Yu, A Comprehensive Survey on Graph Neural\nNetworks, IEEE Transactions on Neural Networks and Learning Systems (2019) 1–21doi:10.1109/\nTNNLS.2020.2978386.\n[4] B. Li, D. Pi, Network representation learning: a systematic literature review, Neural Computing\nand Applications 32 (21) (2020) 16647–16679. doi:10.1007/s00521-020-04908-5.\n[5] I. Chami, S. Abu-El-Haija, B. Perozzi, C. R´e, K. Murphy, Machine Learning on Graphs: A Model\nand Comprehensive Taxonomy (2020).\nURL http://arxiv.org/abs/2005.03675\n[6] S. Bahrami, F. Dornaika, A. Bosaghzadeh, Joint auto-weighted graph fusion and scalable semi-\nsupervised learning, Information Fusion 66 (2021) 213–228.\nURL www.scopus.com\n[7] A. Grover, J. Leskovec, Node2vec: Scalable feature learning for networks, in: Proceedings of the\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Vol. 13-17-\nAugu, 2016, pp. 855–864. doi:10.1145/2939672.2939754.\n[8] B. Perozzi, R. Al-Rfou, S. Skiena, DeepWalk: Online Learning of Social Representations Bryan,\nin: Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and\ndata mining - KDD ’14, ACM Press, New York, New York, USA, 2014, pp. 701–710. doi:10.1145/\n2623330.2623732.\nURL http://dl.acm.org/citation.cfm?doid=2623330.2623732\n[9] T. N. Kipf, M. Welling, Semi-supervised classiﬁcation with graph convolutional networks, in: 5th\nInternational Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings,\nInternational Conference on Learning Representations, ICLR, 2017, pp. 1–14. arXiv:1609.02907.\nURL http://arxiv.org/abs/1609.02907\n[10] Y. Dong, N. V. Chawla, A. Swami, Metapath2vec: Scalable representation learning for hetero-\ngeneous networks, in: Proceedings of the ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, Vol. Part F1296, ACM, New York, NY, USA, 2017, pp. 135–144.\ndoi:10.1145/3097983.3098036.\nURL https://dl.acm.org/doi/10.1145/3097983.3098036\n[11] S. . Wang, V. V. Govindaraj, J. M. G´orriz, X. Zhang, Y. . Zhang, Covid-19 classiﬁcation by\nfgcnet with deep feature fusion from graph convolutional network and convolutional neural network,\nInformation Fusion 67 (2021) 208–229, cited By :1.\nURL www.scopus.com\n[12] A. Garc´ıa-Dur´an, M. Niepert, Learning graph representations with embedding propagation, in:\nAdvances in Neural Information Processing Systems, Vol. 2017-Decem, 2017, pp. 5120–5131.\n[13] W. L. Hamilton, R. Ying, J. Leskovec, Inductive representation learning on large graphs, in: Ad-\nvances in Neural Information Processing Systems, Vol. 2017-Decem, 2017, pp. 1025–1035.\n[14] P. Veliˇckovi´c, A. Casanova, P. Li`o, G. Cucurull, A. Romero, Y. Bengio, Graph attention networks,\nin:\n6th International Conference on Learning Representations, ICLR 2018 - Conference Track\nProceedings, International Conference on Learning Representations, ICLR, 2018, pp. 1–12. arXiv:\n1710.10903.\n20\n[15] D. Wang, P. Cui, W. Zhu, Structural deep network embedding, in:\nProceedings of the ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, Vol. 13-17-Augu,\n2016, pp. 1225–1234. doi:10.1145/2939672.2939753.\n[16] C. Yang, Z. Liu, D. Zhao, M. Sun, E. Y. Chang, Network representation learning with rich text\ninformation, in: IJCAI International Joint Conference on Artiﬁcial Intelligence, Vol. 2015-Janua,\n2015, pp. 2111–2117.\n[17] M. Liu, J. Liu, Y. Chen, M. Wang, H. Chen, Q. Zheng, Ahng: Representation learning on attributed\nheterogeneous network, Information Fusion 50 (2019) 221–230, cited By :3.\nURL www.scopus.com\n[18] L. Lan, P. Wang, J. Zhao, J. Tao, J. Lui, X. Guan, Improving network embedding with partially\navailable vertex and edge content, Information Sciences 512 (2020) 935–951. doi:10.1016/j.ins.\n2019.09.083.\n[19] B. Li, D. Pi, Y. Lin, I. Khan, L. Cui, Multi-source information fusion based heterogeneous network\nembedding, Information Sciences 534 (2020) 53–71. doi:10.1016/j.ins.2020.05.012.\n[20] C. Zhang, D. Song, C. Huang, A. Swami, N. V. Chawla, Heterogeneous graph neural network,\nin: Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, ACM, New York, NY, USA, 2019, pp. 793–803. doi:10.1145/3292500.3330961.\nURL https://dl.acm.org/doi/10.1145/3292500.3330961\n[21] H. Gao, H. Huang, Deep attributed network embedding, in: IJCAI International Joint Conference\non Artiﬁcial Intelligence, Vol. 2018-July, 2018, pp. 3364–3370. doi:10.24963/ijcai.2018/467.\n[22] S. Bandyopadhyay, A. Biswas, N. Murty, R. Narayanam, Beyond node embedding: A direct unsu-\npervised edge representation framework for homogeneous networks (2019). arXiv:1912.05140.\n[23] Y. Chen, T. Qian, Relation constrained attributed network embedding, Information Sciences 515\n(2020) 341–351. doi:10.1016/j.ins.2019.12.033.\n[24] S. Bandyopadhyay, H. Kara, A. Kannan, M. N. Murty, FSCNMF: Fusing structure and content via\nnon-negative matrix factorization for embedding information networks (2018). arXiv:1804.05313.\n[25] D. Nozza, E. Fersini, E. Messina, CAGE: Constrained deep Attributed Graph Embedding, Infor-\nmation Sciences 518 (2020) 56–70. doi:10.1016/j.ins.2019.12.082.\n[26] J. Kim, T. Kim, S. Kim, C. D. Yoo, Edge-labeling graph neural network for few-shot learning, in:\nProceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion, Vol. 2019-June, 2019, pp. 11–20. arXiv:1905.01436, doi:10.1109/CVPR.2019.00010.\n[27] Q. Li, Z. Cao, J. Zhong, Q. Li, Graph representation learning with encoding edges, Neurocomputing\n361 (2019) 29–39. doi:10.1016/j.neucom.2019.07.076.\n[28] L. Gong, Q. Cheng, Exploiting edge features for graph neural networks, in: Proceedings of the IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition, 2019, pp. 9203–9211.\ndoi:10.1109/CVPR.2019.00943.\n[29] C. Aggarwal, G. He, P. Zhao, Edge classiﬁcation in networks, in: 2016 IEEE 32nd International\nConference on Data Engineering, ICDE 2016, Institute of Electrical and Electronics Engineers Inc.,\n2016, pp. 1038–1049. doi:10.1109/ICDE.2016.7498311.\n[30] M. Simonovsky, N. Komodakis, Dynamic edge-conditioned ﬁlters in convolutional neural networks\non graphs, in: Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2017, Vol. 2017-Janua, 2017, pp. 29–38. doi:10.1109/CVPR.2017.11.\n[31] T. D. Bui, S. Ravi, V. Ramavajjala, Neural Graph Learning: Training Neural Networks Using\nGraphs, dl.acm.org 2018-Febua (2018) 64–71. doi:10.1145/3159652.3159731.\n[32] Y. Wang, Y. Sun, M. M. Bronstein, J. M. Solomon, Z. Liu, S. E. Sarma, Dynamic Graph CNN for\nLearning on Point Clouds, ACM Transactions on Graphics 38 (5) (2019) 146. doi:10.1145/3326362.\n[33] T. Wanyan, C. Zhang, A. Azad, X. Liang, D. Li, Y. Ding, Attribute2vec: Deep network embedding\nthrough multi-ﬁltering GCN (apr 2020). arXiv:2004.01375.\nURL http://arxiv.org/abs/2004.01375\n[34] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, Q. Mei, LINE: Large-scale information network\nembedding, in: WWW 2015 - Proceedings of the 24th International Conference on World Wide\nWeb, 2015, pp. 1067–1077. doi:10.1145/2736277.2741093.\n[35] L. F. Ribeiro, P. H. Saverese, D. R. Figueiredo, Struc2vec: Learning node representations from\nstructural identity, in: Proceedings of the ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, Vol. Part F1296, 2017, pp. 385–394. doi:10.1145/3097983.3098061.\n[36] A. Grover, J. Leskovec, node2vec: Scalable feature learning for networks, in: Proceedings of the\n22nd ACM SIGKDD international conference on Knowledge discovery and data mining, ACM,\n2016, pp. 855–864.\n[37] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical Evaluation of Gated Recurrent Neural Net-\n21\nworks on Sequence Modeling (dec 2014). arXiv:1412.3555.\nURL http://arxiv.org/abs/1412.3555\n[38] R. Kuprieiev, D. Petrov, R. Valles, P. Redzy´nski, C. da Costa-Luis, A. Schepanovski, I. Shcheklein,\nS. Pachhai, J. Orpinel, F. Santos, A. Sharma, Zhanibek, D. Hodovic, P. Rowlands, Earl, A. Grigorev,\nN. Dash, G. Vyshnya, maykulkarni, Vera, M. Hora, xliiv, W. Baranowski, S. Mangal, C. Wolﬀ,\nnik123, O. Yoktan, K. Benoy, A. Khamutov, A. Maslakov, Dvc: Data version control - git for data\n& models (May 2020). doi:10.5281/zenodo.3859749.\nURL https://doi.org/10.5281/zenodo.3859749\n[39] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, T. Eliassi-Rad, Collective classiﬁcation in\nnetwork data, AI Magazine 29 (3) (2008) 93. doi:10.1609/aimag.v29i3.2157.\nURL https://ojs.aaai.org/index.php/aimagazine/article/view/2157\n[40] G. Namata, B. London, L. Getoor, B. Huang, Query-driven Active Surveying for Collective Clas-\nsiﬁcation, in: Proceedings ofthe Workshop on Mining and Learn- ing with Graphs, Edinburgh,\nScotland, UK., 2012, pp. 1–8.\n[41] Q. Le, T. Mikolov, Distributed representations of sentences and documents, in: 31st International\nConference on Machine Learning, ICML 2014, Vol. 4, 2014, pp. 2931–2939. arXiv:1405.4053.\nURL http://arxiv.org/abs/1405.4053\n[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pret-\ntenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,\nE. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine Learning Research 12\n(2011) 2825–2830.\n[43] D. Wang, P. Cui, W. Zhu, Structural deep network embedding, in: Proceedings of the 22Nd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, ACM,\nNew York, NY, USA, 2016, pp. 1225–1234. doi:10.1145/2939672.2939753.\nURL http://doi.acm.org/10.1145/2939672.2939753\n[44] D. Q. Nguyen, T. D. Nguyen, D. Phung, A self-attention network based node embedding model\n(jun 2020). arXiv:2006.12100.\nURL http://arxiv.org/abs/2006.12100\n[45] I. Loshchilov, F. Hutter, Decoupled Weight Decay Regularization (nov 2017). arXiv:1711.05101.\nURL http://arxiv.org/abs/1711.05101\n[46] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clustering analysis, in: M. F.\nBalcan, K. Q. Weinberger (Eds.), Proceedings of The 33rd International Conference on Machine\nLearning, Vol. 48 of Proceedings of Machine Learning Research, PMLR, New York, New York,\nUSA, 2016, pp. 478–487.\nURL http://proceedings.mlr.press/v48/xieb16.html\n[47] L. van der Maaten, G. Hinton, Visualizing data using t-SNE, Journal of Machine Learning Research\n9 (2008) 2579–2605.\nURL http://www.jmlr.org/papers/v9/vandermaaten08a.html\n22\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-12-29",
  "updated": "2020-12-29"
}