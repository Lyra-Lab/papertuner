{
  "id": "http://arxiv.org/abs/2012.15754v1",
  "title": "Limitations of Deep Neural Networks: a discussion of G. Marcus' critical appraisal of deep learning",
  "authors": [
    "Stefanos Tsimenidis"
  ],
  "abstract": "Deep neural networks have triggered a revolution in artificial intelligence,\nhaving been applied with great results in medical imaging, semi-autonomous\nvehicles, ecommerce, genetics research, speech recognition, particle physics,\nexperimental art, economic forecasting, environmental science, industrial\nmanufacturing, and a wide variety of applications in nearly every field. This\nsudden success, though, may have intoxicated the research community and blinded\nthem to the potential pitfalls of assigning deep learning a higher status than\nwarranted. Also, research directed at alleviating the weaknesses of deep\nlearning may seem less attractive to scientists and engineers, who focus on the\nlow-hanging fruit of finding more and more applications for deep learning\nmodels, thus letting short-term benefits hamper long-term scientific progress.\nGary Marcus wrote a paper entitled Deep Learning: A Critical Appraisal, and\nhere we discuss Marcus' core ideas, as well as attempt a general assessment of\nthe subject. This study examines some of the limitations of deep neural\nnetworks, with the intention of pointing towards potential paths for future\nresearch, and of clearing up some metaphysical misconceptions, held by numerous\nresearchers, that may misdirect them.",
  "text": "Limitations of Deep Neural Networks:\na discussion of G. Marcus' critical appraisal of deep learning\nStefanos Tsimenidis\nAbstract\nDeep neural networks have triggered a revolution in artificial intelligence, having \nbeen applied with great results in medical imaging, semi-autonomous vehicles, e-\ncommerce, genetics research, speech recognition, particle physics, experimental \nart, economic forecasting, environmental science, industrial manufacturing, and a \nwide variety of applications in nearly every field. This sudden success, though, \nmay have intoxicated the research community and blinded them to the potential \npitfalls of assigning deep learning a higher status than warranted. Also, research \ndirected at alleviating the weaknesses of deep learning may seem less attractive to \nscientists and engineers, who focus on the low-hanging fruit of finding more and \nmore  applications  for  deep  learning  models,  thus  letting  short-term benefits \nhamper long-term scientific progress. Gary Marcus wrote a paper entitled Deep \nLearning: A Critical Appraisal, and here we discuss Marcus' core ideas, as well as \nattempt a general assessment of the subject. This study examines some of the \nlimitations  of deep  neural  networks,  with  the  intention  of pointing  towards \npotential  paths  for  future  research,  and  of  clearing  up  some  metaphysical \nmisconceptions, held by numerous researchers, that may misdirect them.\nI Introduction\nPattern classification with neural networks is nothing new, first proposed in 1947 \n[1]  and implemented during 1957-58 by Rosenblat  [2]. Although Minsky and \nPapert [3] brought to the fore the limitations of these perceptrons and the problem \nof linear separability, famously depicted in the XOR problem, it was always \nknown that these limitations could, in principle, be surpassed by adding hidden \nlayers. The issues that hampered the real-world implementation of such deep \nneural networks were solved much later, through (1) the re-invention of back-\npropagation [4], (2) the running of deep neural networks on GPUs, allowing for the \nscalability of the computationally expensive matrix calculations needed, and (3) \nthe collection of massive amounts of labeled data, indispensable for training deep neural \nnetworks [5]. \nDeep learning as we know it took off around 2010  [6] through the work of researchers \nGeoffrey Hinton, Yoshua Bengio, Yann LeCun, and the IDSIA group in Switzerland, and the \ntipping point came in 2012 with a series of publications showing how deep architectures \nwere  increasingly  achieving  state-of-the-art  performance  in  a  variety  of  pattern \nclassification  tasks,  most  notably  the  famous  work  of  Hinton  and  colleagues  on  the \nImageNet object recognition challenge [7]. This sparked a new machine learning revolution, \nwith an explosion of applications in technology, science, finance, industry, even art and \neducation. The previous AI winter gave way to a surge of activity and excitement, and in \ndirect  proportion  with  the  media  over-hype  surrounding  “artificial  intelligence,”  the \nexpectations of the public skyrocketed. People extrapolated into the future a wide range of \nfantastical scenarios,  from the Promethean storyline where AI helps humans achieve god-\nlike status[8], to the dystopian and paranoid themes of AI enslaving or destroying us[9].\nIn the present day, almost a decade after the “deep learning revolution”, what previously \nappeared like an exponential curve of progress towards AGI (the mythical Artificial General \nIntelligence) starts to resemble a sigmoid, an exponential that reaches a plateau. After the \nadvent of cars that stay inside highway lanes absent human steering, programs that beat \nhumans at chess, Go and video games, and chatbots with storytelling skills and word-play \ncapabilities, everyone braced oneself: something big was about to come. Now, self-driving \ncars haven't gotten any nearer full autonomy  in an urban environment, game-playing \nprograms keep playing games, and chatbot applications are being shut down after failing to \nrise to the expectations. Some of the pioneers of the Deep Learning revolution are now \nworried we're about to enter a new AI winter [6] [10].\nWhat  follows is a discussion  of some of the most severe limitations of deep neural \nnetworks, as identifying a problem is often the first step towards solving it.\nII Deep Neural Networks\nNeural networks, named as such because they loosely model the computational operations \nof biological neurons, consist of input units, one or more hidden layers of processing units, \nand a set of output units. These artificial neurons are interconnected, and information flows \nfrom the input units forward, through the hidden layers, and finally to the output.\nWhat the network does is it maps the inputs to the desired outputs. The information for this \nmapping  is  represented  in  the  neuronal  connection  weights,  which  determine  what \ncomputations will be performed on the input signal. Training the neural network entails \nadjusting these weights, using the back-propagation algorithm, to gradually make a structure \nthat will process the inputs and get them to approximate the desired outputs. \nWith enough training examples neural networks can, in principle, approximate any function, \nthat  is,  any  possible  input-output  mapping.  In  practice  they  never  reach  perfect \ngeneralization,  but  they  often  perform  well  enough  for  a  large  number  of  narrow \napplications, hence their rising popularity.\nWhen the number of hidden layers increases we get “deep” neural networks, which \nhave impressive capability in learning and representing input-output mappings.  \nDeep neural networks are mostly used for classification tasks, assigning an input \ninto a particular class from a set of possible classes. Many applications, from \ncomputer  vision  to  machine  translation,  can  be  formulated  as  classification \nproblems. In a sense, neural networks are used like a hammer in quest for nails: \nmachine  learning  engineers  are  on  a  constant  lookout  for  tasks  that  can  be \nexpressed as a class-assignment problem.\nIn principle deep neural networks can approximate every conceivable input-output \nmapping, and in principle a huge amount of cognitive tasks can potentially be \nformulated as a classification problem. In principle modeling the computations \nperformed by biological neurons could give rise to intelligence, and deep neural \nnetworks could, in principle, be the final invention humanity had to make.\nIII Limitations\nIntelligence, according to connectionism, emerges from computations in neurons \nand networks of neurons, and since humans exhibit high levels of intelligence, the \nconnectionist thesis is supposedly proven right. What follows from this is that the \nlimitations, if any, of deep neural networks, should be minor and unimportant. \nThese digital models of brain function are still crude and primitive, connectionism \ntells us, and with further progress in the field, with deeper and more complex \narchitectures, their shortcomings should gradually diminish to zero, at which point \nwe will have reached the “AI Singularity” [11] [12]. \nThe empirical data shows that the limitations of deep neural nets are neither \nunimportant nor diminishing over time.\nGeneralization\nNeural networks are infamous for their inability to generalize. Given plenty and \ngood training data, and with hyperparameter calibration, they may yield impressive \nresults in interpolation, but in terms of extrapolation their failure is absolute. \nFor humans generalization, transfer learning, and extrapolation comes easily and \nnaturally. As Fodor and Pylyshyn [13] say, ”the ability to entertain a given thought \nimplies the ability to entertain thoughts with semantically related contents.” Any \neffort towards artificial intelligence and applying machine learning to real-world \nsituations must take generalization seriously into account.\nAll  machine  learning  models  struggle  to  make  the  best  tradeoff  between \napproximation and generalization [14]. Approximating the training data too closely leads to \noverfitting, and settling for a loose fit for the sake of generalization leads to an unacceptably \nbad performance on the training data. In most practical applications, the training data \nrepresent the cases most commonly encountered. On an intuitive level, focusing on the \ncommon cases makes for a model that  fails spectacularly on unexpected situations, and \ncrafting a more flexible model renders it useless on the everyday, normal cases that the \nmodel was meant to tackle for in the first place.\nThere are good reasons why metrics, confusion matrices, ROC curves, and MSEs are used, \nand why we have the generalization\\overfitting tradeoff and the bias\\variance tradeoff [14] \n[15] [16]. Automating classification tasks will always entail tradeoffs.\nUnfortunately, in high-stake and in critical situations tradeoffs are unacceptable.\nWhen human lives are at risk, or when a wrong decision will have dire consequences, we \nencounter a paradox. In these high-stake situations we want our model both to generalize \nperfectly, but also to perfectly overfit the training data. You want a self driving car to both \nadapt to new situations, which  means underfitting the training data, but also to always hit \nthe brakes when pedestrians pass the street, which necessarily means overfitting the training \ndata.\nAs the industry is conspicuously silent on this point, perhaps it merits emphasizing. Let \nalone the failure of deep neural networks to extrapolate, even interpolation poses problems \nfor them. Either they will perform well in general cases and fail in edge cases, or they will \noverfit the training data and fail everywhere else. For certain applications this trade-off is \nprohibitive.  No  one  wants  to  pass  a  street  roamed  by  self-driving  cars  with  a  0,87 \nprobability of hitting the brakes. A car biased towards false negatives will be deadly, while a \ncar biased towards false positives will be useless.\nA direct consequence of these issues is that you can not use neural networks  to make \npredictions about policies and economic measures. Just like they don not know what to do \nwith data they've never seen before, neural networks can't predict the outcomes of new, \nnever-before-implemented, policies.\nIt has been argued [17] that the backpropagation learning algorithm may be responsible for \nthe inability of deep neural networks to generalize well. The output units are independent \nand compute error functions independently of each other, the argument goes, while a more \n“global” type of learning would allow the model to encode relationships among features. It \ndoes not seem obvious, though, how a more globalized training regime would solve \ngeneralization and succeed  where every single machine learning algorithm ever devised has \nfailed.\nData Dependent\nDeep neural networks are extremely data-dependent in that they require massive amounts of \nlabeled data to learn adequately [5]. \nIt has been known for decades that neural networks can not generalize well and \ncan not extrapolate at all [17]. The only thing that saves the day is exhaustive data, \nteaching  them  to  approximate  a  function  with  many,  and  in  theory  infinite, \ninstances. That is why deep learning took off during the past decade: massive \namounts of labeled data, and the newfound ability to harvest them, that is, more \ncomputational power to deal with them, and backpropagation to learn from them. \nIn some sense, the modern machine learning revolution is more due to big data \nthan it is to neural networks.\nWe can not use deep learning in situations where there are not massive amounts of \ndata in the first place. First we must generate and label a large volume of data, then \nuse a deep learning model to memorize them. In any new application, in any \nuncharted territory, humans must go first, map the territory, then train a model.\nHumans can learn from a few trials, even in a single trial (one-shot learning), and \ngeneralize  easily  from  that  one  experience.  A lot  of  hype  surrounds  some \naccomplishments of “AI” algorithms, with the implied thesis that these algorithms \nare somehow “intelligent,” but the truth is these models simply memorize billions \nof  data,  interpolate  at  the  short  gaps  between,  and  thus  might  appear  to \n“understand.” They yield seemingly impressive results only when they've already \nbeen fed almost every possible input\\output pair [18].\nBackpropagation, one of the great deep learning enablers, needs labeled data to \ncompute error functions and adjust synaptic weights. Without knowing the outputs \nthere  can  be  no  learning--a  vicious  circle  if  you  lack  big  data  and  human \nannotators.\nCan not Function in an Open World\nMachine extrapolation is impossible because machines must generalize from finite \ndata, and unless the function to be approximated is very simple, or periodic, they \nare bound to fail  outside  of  the  subspace enclosing the  instances they  have \nencountered.  This  doesn  not  necessarily  have  to  pose  a  problem  in  finite \nenvironments, and given big data that span all the input space, in which case the \nneural network only needs to interpolate. Unfortunately, systems in the real world \nare not isolated. They are complex and open-ended. And deep learning models will \nhave to extrapolate.\nA divide-and-conquer strategy could be attempted to break the domain into smaller \nproblems and train a separate neural network  for each, but most real world \nproblems are not easily compartmentalized, especially in fields like policy-making \nand economics. In these highly open-ended systems the number of variables and \nfactors affecting the outcome is vast, factors that are in turn affected by other \nfactors, on and on, a chain of causal relationships reaching to infinity [19].\nIn such a system no amount of training data will be adequate as it would only \nbroaden  the  range  of  interpolation.  An  infinite  problem  space  can  not  be \napproximated by the finite subspace of training instances, and this is true even in the era of \nbig data.\nCan not Adapt\nFor the same reason that neural networks ca not function in an open world, they also can not \nadapt to outlier inputs in closed systems. Exhaustive data that cover all possible situations \nare feasible not in finite worlds, but in worlds that are both finite and relatively static.\nDeep learning models fail in dynamical systems. As the environment changes, they must be \ntrained anew. In “black swan” situations [20] they fail completely, and should not be utilized \nin environments where such occurrences are expected.\nLack of Hierarchical Perception\nA sentence is not a series of words, but a series of phrases, each one with a unified meaning \n[20]. A picture is not a series of pixels, but an arrangement of objects, some of which are \narrangements of objects themselves (eg a car). Humans naturally discern such hierarchies \nand process them as unified wholes, variables in relationship with other variables, forming \nhierarchies in relationship with other hierarchies. Artificial neural networks can not detect \nsuch structures [22]. Their attention is uniform; every element of their inputs, every pixel of \nan image, every word of a sentence, carries equal weight.\nAccording to Marcus [17], neural networks “do not include any explicit representation of a \nrelationship  between  variables.  Instead,  the  mapping  between  input  and  output  is \nrepresented through the set of connection weights....They replace operations that work over \nvariables with local learning, changing connections between individual nodes without using \n'global information.'”\nTheir  oblivion to linguistic structure is dwarfed by their  incomprehension of abstract \nrelationships. The phrase “John is father to Mary” is meaningless to them and they can not \nregister the conceptual similarity with the phrase “George is father to John.” As known since \nthe 1960's, though, neural networks could appear to understand abstract relations “if they're \ntrained by an exhaustive procedure of taking in every possible datum where the relationship \nholds,” (Rosenblatt (1962) ) in other words, by being spoon-fed the entire input space, \nwhich is not unlike what takes place today with the highly publicized and much hyped \n“accomplishments” of modern “AI.”\nCausation vs Correlation\nSimilar to their inability to register hierarchical structure and abstract relationships, neural \nnetworks lack the capacity to differentiate between correlation and causation [23]. Their \nability to spot factors that correlate highly with each other within stupendous volumes of \ncomplex data is truly remarkable, and can be harnessed for many practical applications but \ninferring causal relationships eludes them.\nThis may lead to “dumb insights,” especially in cases of overfitting, where they're \nprone to confuse coincidences for meaningful associations (a well known example \nbeing the Google flu case [24]). The capacity of “AI” models, which usually \nmeans  deep  learning  models,  to  attribute  statistical  significance  to  spurious \ncorrelations has gained an almost legendary status.\nThe inability of neural nets to detect causation heavily aggravates their inability to \nmake predictions. A neural network only perceives a graph of points on an axis \nsystem. Only a human can “see behind” these points to what, and how, variables \naffect each other, and thus is able to anticipate events. Deep learning models can \nmake  adequate  short  term  predictions  in  simple  systems  or  with  exhaustive \ntraining, but in a complex system a capacity for causal inference is indispensable. \nWork on mathematically formulating causal inference has been attempted [25], but \nnot with deep learning. The causal-inference research project has focused on \nhigher level algorithms, on cognitive models instead of connectionist ones. It \nseems that if the task of automated causal reasoning is ever solved it will not be \nwith neural networks.\nBlackboxes\nIn engineering a black box signifies a system or device we can not know how it \nfunctions but can only see what inputs go in, and what outputs come out. In deep \nlearning we use feature extraction and vectorization to represent the objects we \nwant the model to process with numbers. The model only sees numbers and spots \nstatistical regularities among these numbers [6]. It can not register any qualitative \nrelationships  between  the  variables  these  numbers  represent,  like  causality, \nhierarchy, and other abstractions [17] [26]. It only detects quantitative relationships \namong the numbers themselves, and therefore cannot explain its decisions in any \nhuman-meaningful way. It's a black-box.\nIt has been shown that a machine learning model's interpretability  is inversely \nproportional to its flexibility [16], and neural networks, with their mimicking brain \nplasticity, are arguably the most flexible models of all [27].\nDebugging such an algorithm poses serious problems. Many applications use \ncascades of deep neural networks, one's output feeding the input of another to \nachieve complex tasks. The human brain is not merely a large neural network but it \nis a network of networks, and the quest for artificial general intelligence may take \nthe  direction  of  researching  hierarchies  of  deep  neural  networks  [28].  These \nsystems might well be impossible to debug.\nAs  long  as  they  remain  blackboxes  we  can't  trust  neural  networks  to  make \nimportant decisions in high-stake situations [26]. Netflix recommendations and \nautomatic captioning of photos on blogs might be fine, but terrorism detection and \nforensic procedures shouldn't be entrusted on systems that can not explain how \nthey reach their conclusions.\nIn Lipton [29] we learn that concerns over trust and other issues of interpretability may be \n“quasi-scientific.” We learn that a lot of disagreement has been going about what makes a \nmodel interpretable, with candidate definitions often contradicting each other, therefore, we \nlearn, concerns about interpretability are meaningless and “quasi-scientific”. \nWhile it is true that interpretability is not always well-defined and there is high-variance of \ncompeting definitions, all of these definitions share one thing in common. Neural networks \ndo not fulfill any one of them. No matter what standards for interpretability you set, neural \nnetworks do not meet them.\nWhen and if deep learning models start becoming easier to understand the time will be ripe \nfor a more rigorous examination of the issue, and the “quasi-scientific” discussions about AI \nexplainability will be a thing of the past.\nOntological Inference\nAmong the abilities to understand hierarchical structure, causal relationships, abstract ideas, \nand symbols, ontological inference may be the highest abstraction of all, and to the author's \nknowledge, has never been discussed in the artificial intelligence literature.\nOntological inference is the capacity to infer, from sense perceptions, the existence of sense-\nundetectable entities.  The proverbial apple falling on a robot's head will never cause it to \ndiscover gravity. Though the robot may register statistical regularities about objects falling \ntowards the ground when unsupported, it will never hypothesize an entity, an existence, that \nactively pulls bodies towards the center of the planet. Atoms and electrons have never been \nand will probably never be directly observed, but their existence can be inferred from other \nobservations, indirectly [30]. Even a natural process can be, and is, conceptualized as an \nentity,  an  abstract  being  of  sorts,  and  its  occurrence  can  be  expected  whenever  the \nconditions that trigger it are present. \nThis ontological-detection faculty that humans posses is perhaps the single most important \nfactor that enabled humanity's scientific progress and flourishing. To say that deep neural \nnetworks don't have access to this higher function would be an understatement. Ontological \ninference is so outside the grasp of any mathematical formulation and computational model \nthat it is never being discussed in the literature of any modern, reductionism-infused science \nthat supposedly  studies the human mind: cognitive science, neuroscience, psychology, \ncomputer science, even modern philosophy and epistemology. Everyone seems meticulous \nand rigorous in pretending that (1) such a faculty does not exist, and  (2) we are well on the \nway to create machine intelligence that far outweighs the intelligence of humans.\nTechnical Debt\nThe term “technical debt” refers to systems that seem attractive on the short-term but \nbecome unmanageable later on when they increase in complexity. \nTraditional systems using rules and logic can be modularized, debugged, and \npolished. As a system increases in complexity such tasks become increasingly \nimportant, not because they add further functionality, but because they clear the \npath for future improvements.\nThe reason why deep learning is utilized, though, is precisely because it excels in \ntasks where more logic-oriented techniques fail. Deep learning models may be \neasy to implement and may bring some impressive initial results to companies, but \nas they scale up, and since they lack the debugability of interpretable algorithms, \nsmall hindrances can compound to serious issues. \nAlso, their penchant to mistake correlation for causation rises proportionally with \nthe increase in all the data, variables, features, and quantities that the neural \nnetworks are summoned to inspect, until the noise overwhelms the signal.\nSheer Lack of Intelligence\nLately, a few popular treatments of artificial intelligence have commented of how \ndeep neural networks can produce outputs that seem, from a human perspective, \nmonumentally unintelligent [31] [32]. Algorithms that missclasify TV sets for \norangutans,  generate  dirty  words  and  racist  remarks  when  asked  to  generate \nchildren's stories, give ridiculous answers to simple common-sense reasoning \nquestions. None  of  that  comes  as a surprise to those  who  know  how these \nalgorithms work, but they serve as a reminder that the intelligence we're sometimes \ncarried away to attribute to them is an illussion.\nA lot  has  been  said  about  games-playing  AI  programs  that  use  deep  neural \nnetworks trained with reinforcement learning [33]. They beat humans and come up \nwith seemingly ingenious and creative solutions to problems but, again, to attribute \nsome sort of understanding to these algorithms would be premature. DeepMind's \nAtari system failed on minor perturbations from the training set, such as slightly \nmoving the Y coordinate of an object [34]. These neural-nets were trained day and \nnight for weeks, on more than a thousand processors and GPUs, using petabytes \nand petabytes of training data, then playing games against each other to further \n“hone their skills,” eventually achieving breathtaking results; and then a minor rule \nchanges slightly, or a hardly perceptible perturbation is applied on the pixels, and \neverything crumbles. The algorithms never really understood anything about what \nthey were doing, but simply overfitted the data and superficially acted upon arrays \nof pixels without awareness of what these pixels represented.\nAnother famous case is the adversarial attacks on deep neural networks trained for \nobject recognition from images [35] [36]. These networks can be easily tricked \ninto misclassifying images by applying slight perturbations, which can be found by \nmaximizing their prediction error [37]. No neurologically sound human would \nmisclassify an image of an ocean for a car. Artificial neural networks simply don't \nperceive things the way we do. \nIV The Rise and Fall of Connectionism\nThe lure of connectionism initially came from cognitive science and the doctrine that all \nparts of intelligence, including abstract thinking and symbol manipulation can be accounted \nfor by the computations of networks of neurons [38]. \nThis view initially seemed to be corroborated by the apparent success of deep neural \nnetworks, but has lately been challenged. There's growing evidence from neuroscience that \nknowledge of brains does not correlate with understanding of behavior [39].  Even if we \nsimulated the entirety of a human brain there are no guarantees, to put it mildly, that the \nsimulation would do what we would expect a human brain would do. The connectionist \napproach to general intelligence is being abandoned. \nSome final attempts to defend connectionism point to it as the biologically plausible way of \nexplaining  and  realizing  intelligence,  but  back-propagation,  the  fundamental  learning \nalgorithm that largely made connectionism implementable in the first place, is clearly not \nbiologically plausible [40], and the parts of deep neural networks that are biologically \nplausible have severe limitations.\nNew  approaches  are  being  proposed,  symbolic  representations  [22],  algorithmic \nrepresentational models [41], conceptual representations of higher cognitive functions [12], \na combination of approaches [42], etc. All these proposals attempt a higher-level approach \nto intelligence than the neural, as scientists increasingly acknowledge connectionism is not \nthe answer. Reproducing brain computation does not bring us nearly as close to general \nintelligence as previously hoped.\nDeep learning has tremendous potential for practical applications, most of it still untapped, \nand with the late massive inflow of talented engineers into the field the future looks \npromising. If we ever succeed in creating artificial general intelligence deep neural networks \nwill no doubt have played a role in it, however small that role might be.\nV Conclusion\nEver since Searle made his Chinese room argument [43] a lot has been said to refute it [44] \n[45] [46], even to the the point of mocking Searle for having made it. Perhaps Searle gets \nthe last laugh, though; nearly all the limitations of deep neural networks we examined, as \nwell  as  the  fact  that  these  limitations  are  neither  trivial  nor  diminishing  with  new \nbreakthroughs in computer science, are because Searle was right.\nNeural networks do not understand what they do.\nThey may be able to represent information, but they have no awareness of what this \ninformation means. It remains to be discerned to what degree is intelligence dependent on, \nand rendered possible by, consciousness.\nReferences\n[1] Pitts, W., &  McCulloch,W.S. (1947). How we know universals: the perception of\nauditory and visual forms. Bulletin of Mathematical Biology, 9(3):127–147,\n[2] Rosenblatt, F. (1958). The perceptron: a probabilistic model for information\nstorage and organization in the brain. Psychological Review, 65:386–408,\n1958.\n[3] Minsky, M., Papert, S. (1969). Perceptrons. MIT Press\n[4] Rumelhart, G. Hinton, and R. Williams. Learning representations by\nback-propagating errors. Nature, 323:533–536, October 1986.\n[5] Ng, A. (2016). What Artificial Intelligence Can and Can’t Do Right Now. Harvard \nBusiness\nReview.\n[6] Chollet, F. (2017). Deep Learning with Python. Manning Publications.\n[7] Krizhevsky, A., Sutskever, I., & Hinton, G.E. (2012). Imagenet classification with \ndeep convolutional neural networks. In (pp. 1097-1105)\n[8] Harari, N.Y., (2018). Homo Deus, Harper Perennial\n[9] Bostrom, N. (2014). Superintelligence, Oxford University Press\n[10] Sabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic Routing Between Capsules. \narXiv,\ncs.CV.\n[11] Kurzweil, R. (2006). The Singularity is Near, Penguin Books\n[12] Goertzel, B. (2016). The AGI Revolution,Humanity+ Press\n[13] Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: a \ncritical\nanalysis. Cognition, 28(1-2)(1-2), 3-71.\n[14] Mostafa, Y., Ismail, M., Lin, H. (2012). Learning From Data, AML\n[15]  Goodfellow,  I.,  Bengio,  Y.,  Courville,  A.  (2016).\n Deep  Learning, \nwww.deeplearningbook.org\n[16] James G., Witten, D., Hastie, T., Tibshirani, R.(2013)An Introduction to Statistical \nLearning, Springer\n [17]Marcus, G.F. (1998). Rethinking Eliminative Connectionism, COGNITIVE PSYCHOLOGY \n37, 243–282\n[18] Touretzky, D. S. (1991). Connectionism and compositional semantics. In J. A. Barnden & J. B. \nPollack (Eds.), High-level connectionist models (pp. 17–31). Hillsdale, NJ: Erlbaum.\n[19] Gleick, J. (1987). Chaos, Penguin Books\n[20] Taleb, N.N. (2010). The Black Swan, Random House\n[21] Chomsky, N. (1998). On Language, New Press\n[22] Marcus, G., (2017). Deep /learning:A Critical Appraisal, arxiv\n[23] Pearl, J. (2018). The book of why, Penguin Books\n[24] Lazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). Big data. The parable of Google \nFlu: traps in big data analysis. Science, 343(6176)(6176), 1203-1205\n[25] Pearl, J. (2000). Causality : models, reasoning, and inference /. Cambridge, U.K.; New York : \nCambridge University Press.\n[26] Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the\nPredictions of Any Classifier. arXiv, cs.LG.\n[27] Haykin, S. (2009), Neural Networks and Intelligent Machines, Pearson\n[28] Kurzweil, R. (2013). How to Create a Mind, Penguin Books\n[29] Lipton, Z. C. (2016). The Mythos of Model Interpretability. arXiv, cs.LG.\n[30] Ratzsch, D. (2000). Science & Its Limits, IVP Academic\n[31] Broussard, M. (2019), Artificial Unintelligence, The MIT Press\n[32] Shane, J. (2019). You Look Like a Thing and I Love You, Voracious\n[33] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game\nof go without human knowledge. Nature, 550(7676):354, 2017.\n[34] Kansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla, M., Lou, X. et al. (2017).\nSchema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive\nPhysics. arXIv, cs.AI.\n[35] Huang, S., Papernot, N., Goodfellow, I., Duan, Y., & Abbeel, P. (2017). Adversarial Attacks on\nNeural Network Policies. arXiv, cs.LG.\n[36] Nguyen, A., Yosinski, J., & Clune, J. (2014). Deep Neural Networks are Easily Fooled: High\nConfidence Predictions for Unrecognizable Images. arXiv, cs.CV.\n[37] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. et al. \n(2013). Intriguing properties of neural networks. arXiv, cs.CV.\n[38] Dupuy, J.P. (2009). On the Origins of Cognitive Science:The Mechanization of the \nMind, MIT Press\n[39] Krakauer, J.W., Ghazanfar, A.A. (2017) Neuroscience Needs Behavior: Correcting a \nReductionist Bias, Neuron 93\n[40]Smolensky, P. (1988). On the proper treatment of connectionism. Behavioral and \nBrain Sciences,\n11, 1–74.\n[41] Cooper, R.P., Peebles, D. (2015). Beyond Single Level Accounts, Topics in \nCognitive Science 7, 243-258\n[42] Domingos, P. (2015). The Master Algorithm, Basic Books\n[43] Searle, John. R. (1980) Minds, brains, and programs. Behavioral and Brain Sciences \n3 (3): 417-457\n[44] Pinker, S. (1997). How the Mind Works, Penguin Books\n[45] Preston, J., Bishop, M. , eds. (2002). Views Into the Chinese Room, Oxford \nUniversity Press\n[46] Warwick, K. (2012). Artificial Intelligence, Routledge\nSupplementary Materials\nReport on the current state of AI explainability\nSince lack of explainability is a big weakness of deep learning, and discussed in the main body \nbody of this paper, we include a short report that was commissioned for a grant proposal, for \nfunding to conduct  research  on an  experimental, explainable  machine  learning  model.  The \nconstraints were: a) the report should be about a page long, and b) it should contain references \nonly from 2019, since the grant proposal would be sent in the first month of 2020, and we needed \nto capture the current state of the field. The reference-numbers do not point to the references of \nthe main paper, but to the references at the end of this document.\nDe Graaf and Malle [1] argue that humans regard AI systems as intentional agents \nand  expect  explanations  similar  to  what  a  human  would  give,  thus  linguistic  and \npsychological analysis of human-level explanations is necessary. Rutjes et al [2] maintain \nthat the problem of AI interpretability lies outside the domain of computer science and \nshould be tackled by cognitive scientists. They even suggest seeking counsel from the \nsocial sciences, since concepts like fairness, accuracy, and explainability may be, according to the \nauthors, social constructs. Much focus has been given on these ideas and similar arguments have \nbeen made. These are important and nuanced issues, and when black-box models cease being black-\nboxes, these discussions will become relevant. \nThe class of inherently explainable machine learning algorithms is comprised by linear \nmodels, decision trees, and Bayesian classifiers [3]. Explanations entail providing visualizations and \ninformation about coefficient weights, decision paths, rules and probabilities. In one study with \nLogistic Regression  predicting diabetic patient re-admittance [4], the model displays probabilities \nof re-admittance, a confusion matrix with metrics, and simple statements such as “The probability \nof readmission increases as the patient's glucose serum levels exceed >200.” Another study uses \ngradient-boosted trees for diagnosis and applies techniques to explore counterfactuals, identify key \nfactors, and display visualizations [5]. With these interpretable models the cognitive and linguistic \nanalysis of  explanations mentioned above is not irrelevant. Hal et al [6] developed a systematic \ninterview method to identify the explainability requirements  of an AI system's stakeholders and \nthus be able to design a system that meets them.\nAt the other end of the explainability spectrum lie Deep Learning models that are, by their \nvery nature, blackboxes. Efforts to render them intelligible utilize tools to approximate deep neural \nnetworks with either linear or gradient-based models, or decision trees [3]. These techniques are \neffective only for a narrow domain of the model they approximate, and break down outside of it [7]. \nAn example of this in [8] focused on computer vision. The researchers performed  correlation \nanalysis  to detect patterns  between the factors of  input  data variations and test outputs. The \nalgorithm learns a dictionary of semantic concepts as an explanation of what it “sees” in the image. \nA similar correlation analysis was used in a classification task (determine whether MRI scan \nrequests should be approved) [9]. The goal was to find specific words in the MRI requests that \nmaximize probability of positive prediction.\nDespite such efforts deep neural networks refuse to yield. They remain blackboxes, and many \nresearchers have given up attempts to unlock them, searching for workarounds instead. In [10] it \nwas suggested that the model should augment classifications by providing additional information \nand predictions, thus appearing more rigorous and authoritative than it is (eg., a model that predicts \nthe likelihood of future dementia could also predict the results of future cognitive tests and brain \nscans). Additionally, they suggest that reproducibility might make up for the lack of explainability: \nIf a machine learning pipeline consistently yields high-quality results on a variety of datasets we \nfeel justified to trust it.\nAkula et al [11] experimented with an interactive game where a human sees a blurred image \nand, to infer what it depicts, asks the machine questions. The machine runs object recognition \nalgorithms on the unblurred image. The user asks questions that are meaningful to a human, and the \nmachine iteratively builds a model of what the human perceives\\thinks, learning to provide human-\nrelevant answers. Then the user hypothesizes which objects the machine will classify correctly in a \nset of unblurred images. The closer his expectations are to actual machine performance, the more \nthe user is said to trust the machine. It's unclear what the purpose of such studies is, but experiments \nshowed that over time humans trust the machine more, regardless of the neural network's remaining \nas much a blackbox as before. \nEhsan et al [12] worked on reinforcement learning, where future decisions depend on past \nones. Users train the model by acting on the specific domain\\task and providing natural language \nexplanations of their decision-making rationale. The system associates action with rationale, and \nduring  deployment  it  outputs,  for  each  action,  the  corresponding  explanation.  This  pseudo-\nexplainability method yields acceptable results only in narrow domains where the number of \npossible states and actions is severely limited, hence in the study the domain problem was a simple \nvideo game.\nSuch techniques that give the illusion of explainability may enhance the user's trust and \ncomfort, but in a plethora of applications, especially high-stake ones, we need the explainability to \nbe  actual.  When  the  scientific  community  gives  up  on  the  problem  and  experiments  with \nworkarounds, it is time to consider alternative AI models.\nReferences \n[1] Maartje M. A. de Graaf, Bertram F. Malle, How People Explain Action (and \nAutonomous Intelligent Systems Should Too), (2017). Artificial Intelligence for Human-\nRobot Interaction, AAAI Technical Report FS-17-01\n[2] Heleen Rutjes, Martijn C. Willemsen, and Wijnand A. IJsselsteijn. (2019). \nConsiderations on Explainable AI and Users’ Mental Models. In Where is the Human? \nBridging the Gap Between AI and HCI, Workshop at CHI’19, May 4–9, 2019, Glasgow, \nScotland Uk. ACM, New York, NY, USA. \n[3] Arun Rai, (2019).Explainable AI: from black box to glass box,  Journal of the \nAcademy of Marketing Science, Springer, https://doi.org/10.1007/s11747-019-00710-5\n[4] Sofia Meacham, Georgia Isaac, Detlef Nauck, Botond Virginas, (2019).Towards \nExplainable AI: Design and Development for Explanation of machine learning \npredictions for a patient readmittance medical application, CompCon 2019,AISC 997, pp. \n939-955\n[5] Danding Wang, Qian Yang, Ashraf Abdul, Brian Y. Lim. (2019). Designing Theory-\nDriven User-Centric Explainable AI. In 2019 CHI Conference on Human Factors in \nComputing Systems Proceedings (CHI 2019), May 4–9,2019, Glasgow, Scotland, UK. \nACM, New York, NY, USA. 15 pages. h  tt  ps://doi.org/10.1145/3290605.3300831\n \n \n[6] M. Hall, D. Harborne, R. Tomsett, V. Galetic, S. Quintana-Amate, A. Nottle, and A. \nPreece, (2019), “A systematic method to understand requirements for explainable ai \n(XAI) systems,” in Proceedings of the IJCAI 2019 Workshop on Explainable Artificial \nIntelligence (XAI).\n[7] Brent Mittelstadt, Chris Russell, and Sandra Wachter. (2019). Explaining Explanations \nin AI . In FAT* ’19: Conference on Fairness, Accountability, and Transparency (FAT* \n’19), January 29–31, 2019, Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages. \nhttps://doi.org/10.1145/3287560.3287574\n[8]  Chi Zhang, Biyao Shang, Ping Wei, Li Li, Yuehu Liu, Nanning Zheng, (2019), \nBuilding Explainable AI Evaluation for Autonomous  Perception,  CVPR Workshops, \nConference Proceedings.\n[9] Alwin Yaoxian Zhang, Sean Shao Wei Lam, Eng Hock Marcus Ong, Phua Hwee Tang, \nand Ling Ling Chan. (2019). Explainable AI: Classification of MRI Brain Scan Orders for \nQuality Improvement. In Proceedings of the IEEE/ACM 6th International Conference on \nBig Data Computing, Applications and Technologies (BDCAT’19), December 2–5, 2019, \nAuckland, New Zealand. ACM, New York, NY, USA. \nhttps://doi.og/10.1145/3365109.3368791\n[10] Olivier Colliot. Interpretable and reliable artificial intelligence systems for brain \ndiseases,(2019). ERCIM News, ERCIM,  118. hal-02178901\n[11] Arjun Akula, Changsong Liu, Sinisa Todorovic, Joyce Chai, Song-Chun Zhu, (2019). \nExplainable AI as Collaborative Task Solving, CVPR Workshops, Conference Proceedings.\n[12] Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, and Mark O. Riedl. (2019). \nAutomated Rationale Generation: A Technique for Explainable AI and its Effects on Human \nPerceptions. In 24th International Conference on Intelligent User Interfaces (IUI ’19), March 17–\n20,  2019,  Marina  del  Rey,  CA,  USA.\n ACM,  New  York,  NY,  USA. \nhttps://doi.org/10.1145/3301275.3302316\n",
  "categories": [
    "cs.AI",
    "cs.CY",
    "cs.LG"
  ],
  "published": "2020-12-22",
  "updated": "2020-12-22"
}