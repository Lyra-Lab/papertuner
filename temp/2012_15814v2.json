{
  "id": "http://arxiv.org/abs/2012.15814v2",
  "title": "Language-Mediated, Object-Centric Representation Learning",
  "authors": [
    "Ruocheng Wang",
    "Jiayuan Mao",
    "Samuel J. Gershman",
    "Jiajun Wu"
  ],
  "abstract": "We present Language-mediated, Object-centric Representation Learning (LORL),\na paradigm for learning disentangled, object-centric scene representations from\nvision and language. LORL builds upon recent advances in unsupervised object\ndiscovery and segmentation, notably MONet and Slot Attention. While these\nalgorithms learn an object-centric representation just by reconstructing the\ninput image, LORL enables them to further learn to associate the learned\nrepresentations to concepts, i.e., words for object categories, properties, and\nspatial relationships, from language input. These object-centric concepts\nderived from language facilitate the learning of object-centric\nrepresentations. LORL can be integrated with various unsupervised object\ndiscovery algorithms that are language-agnostic. Experiments show that the\nintegration of LORL consistently improves the performance of unsupervised\nobject discovery methods on two datasets via the help of language. We also show\nthat concepts learned by LORL, in conjunction with object discovery methods,\naid downstream tasks such as referring expression comprehension.",
  "text": "Language-Mediated, Object-Centric Representation Learning\nRuocheng Wang∗\nStanford University\nJiayuan Mao∗\nMIT CSAIL\nSamuel J. Gershman†\nHarvard University\nJiajun Wu†\nStanford University\nAbstract\nWe\npresent\nLanguage-mediated,\nObject-\ncentric Representation Learning (LORL), a\nparadigm for learning disentangled, object-\ncentric scene representations from vision and\nlanguage. LORL builds upon recent advances\nin unsupervised object discovery and segmen-\ntation, notably MONet and Slot Attention.\nWhile these algorithms learn an object-centric\nrepresentation just by reconstructing the input\nimage, LORL enables them to further learn\nto associate the learned representations to\nconcepts, i.e., words for object categories,\nproperties, and spatial relationships, from\nlanguage input. These object-centric concepts\nderived from language facilitate the learning\nof object-centric representations.\nLORL\ncan\nbe\nintegrated\nwith\nvarious\nunsuper-\nvised object discovery algorithms that are\nlanguage-agnostic. Experiments show that the\nintegration of LORL consistently improves\nthe\nperformance\nof\nunsupervised\nobject\ndiscovery methods on two datasets via the\nhelp of language. We also show that concepts\nlearned by LORL, in conjunction with object\ndiscovery methods,\naid downstream tasks\nsuch as referring expression comprehension.\n1\nIntroduction\nCognitive studies show that human infants develop\nobject individuation skill from diverse sources of\ninformation: spatial-temporal information, object\nproperty information, and language (Xu, 1999,\n2007; Westermann and Mareschal, 2014). Speciﬁ-\ncally, young infants develop object-based attention\nthat disentangles the motion and location of objects\nfrom their visual appearance features. Later on,\nthey can leverage the knowledge acquired through\nword learning to solve the problem of object indi-\nviduation: words provide clues about object iden-\ntity and type. The general picture from cognitive\nscience is that object perception and language co-\ndevelop in support of one another (Bloom, 2002).\n∗, †: indicates equal contribution\nInput Image\nSegmentation I\nSegmentation II\nQ: What is the black\nobject in the image?\nA: Plate. ☓\nQ: What is the black\nobject in the image?\nA: Pan. ✓\nInput Image\nSegmentation I\nSegmentation II\nQ: How many legs\nare visible?\nA: 3. ☓\nQ: How many legs\nare visible?\nA: 4. ✓\n(b)\n(a)\n(PartNet)\n(ShopVRB)\nFigure 1: Two illustrative cases of Language-mediated,\nObject-centric Representation Learning. Different col-\nors in the segmentation masks indicate individual ob-\njects recognized by the model. LORL can learn from vi-\nsual and language inputs to associate various concepts:\nblack, pan, leg, with the visual appearance of individ-\nual objects. Furthermore, language provides cues about\nhow an input scene should be segmented into individ-\nual objects: (a) segmenting the frying pan and its han-\ndle into two parts (Segmentation II) yields an incorrect\nanswer to the question ; (b) an incorrect parsing of the\nchair image makes the counting result wrong.\nOur long-term goal is to endow machines with\nsimilar abilities. In this paper, we focus on how\nlanguage may support object discovery and seg-\nmentation.\nRecent work has studied the prob-\nlem of unsupervised object representation learning,\nthough without language. As an example, factor-\nized, object-centric scene representations have been\nused in various kinds of prediction (Goel et al.,\n2018), reasoning (Yi et al., 2018), and planning\ntasks (Veerapaneni et al., 2020), but they have not\nconsidered the role of language and how it may\nhelp object representation learning.\nAs a concrete example, consider the input im-\nages shown in Fig. 1 and the paired questions.\nFrom language, we can learn to associate concepts,\narXiv:2012.15814v2  [cs.LG]  8 Jun 2021\nsuch as black, pan, and leg, with the referred ob-\nject’s visual appearance. Further, language pro-\nvides cues about how an input scene should be\nsegmented into individual objects: a wrong parsing\nof the input scene will lead to an incorrect answer\nto the question. We can learn from such failure that\nthe handle belongs to the frying pan (Fig. 1a) and\nthe chair has four legs (Fig. 1b).\nMotivated by these observations, we propose\na computational learning paradigm, Language-\nmediated, Object-centric Representation Learning\n(LORL), associating learned object-centric repre-\nsentations to their visual appearance (masks) in\nimages, and to concepts—words for object prop-\nerties such as color, shape, and material—as pro-\nvided in language. Here the language input can\nbe either descriptive sentences or question-answer\npairs. LORL requires no annotations on object\nmasks, categories, or properties during the learning\nprocess.\nIn LORL, four modules are jointly trained. The\nﬁrst is an image encoder, learning to encode an im-\nage into factorized, object-centric representations.\nThe second is an image decoder, learning to recon-\nstruct masks for individual objects from the learned\nrepresentations by reconstructing the input. These\ntwo modules share the same formulation as recent\nunsupervised object discovery research: learning to\ndecompose the image into a series of slot proﬁles,\ncomprised of pixel masks and latent embeddings.\nEach slot proﬁle is expected to represent a single\nobject in the image.\nThe third module in LORL is a pre-trained se-\nmantic parser that translates the input sentence into\na semantic, executable program, where each con-\ncept (i.e., words for object properties such as red)\nis associated with a vector space embedding. Fi-\nnally, the last module, a neural-symbolic program\nexecutor, takes the object-centric representation\nfrom Module 1, intermediate representations from\nModule 2, and concept embeddings and the seman-\ntic program from Module 3 as input, and outputs\nan answer if the language input is a question, or\nTRUE/FALSE if it’s a descriptive sentence. The\ncorrectness of the executor’s output and the quality\nof reconstructed images (as output of Module 2)\nare the two supervisory signals we use to jointly\ntrain Modules 1, 2, and 4.\nWe integrate the proposed LORL with state-\nof-the-art\nunsupervised\ndiscovery\nmethods,\nMONet (Burgess et al., 2019) and Slot Atten-\ntion (Locatello et al., 2020). The evaluation is\nbased on two datasets: ShopVRB (Nazarczuk\nand Mikolajczyk, 2020) contains images of daily\nobjects and question-answer pairs; PartNet (Mo\net al., 2019) contains images of furniture with\nhierarchical structure, supplemented by descriptive\nsentences we collected ourselves. We show that\nLORL consistently improves existing methods on\nunsupervised object discovery, much more likely\nto group different parts of a single object into a\nsingle mask.\nWe further analyze the object-centric represen-\ntations learned by LORL. In LORL, conceptually\nsimilar objects (e.g. objects of similar shapes) ap-\npear to be clustered in the embedding space. More-\nover, experiments demonstrate that the learned con-\ncepts can be used in new tasks, such as visual\ngrounding of referring expressions, without any\nadditional ﬁne-tuning.\n2\nRelated Work\nUnsupervised object representation learning.\nGiven an input image, unsupervised object rep-\nresentation learning methods segment objects in\nthe scene and build an object-centric representa-\ntion for them. A mainstream approach has focused\non using compositional generative scene models\nthat decompose the scene as a mixture of compo-\nnent images (Greff et al., 2016; Eslami et al., 2016;\nGreff et al., 2017; Burgess et al., 2019; Engelcke\net al., 2020; Greff et al., 2019; Locatello et al.,\n2020; Goyal et al., 2020). In general, these models\nuse an encoder-decoder architecture: the image en-\ncoder encodes the input image into a set of latent\nobject representations, which are fed into the im-\nage decoder to reconstruct the image. Speciﬁcally,\n(Greff et al., 2019; Burgess et al., 2019; Engelcke\net al., 2020) use recurrent encoders that iteratively\nlocalize and encode objects in the scene. Another\nline of research (Eslami et al., 2016; Crawford and\nPineau, 2019; Kosiorek et al., 2018; Stelzner et al.,\n2019; Lin et al., 2020) leverages object locality\nto attend to different local patches of the image.\nThese models often use a pixel-level reconstruc-\ntion loss. In contrast, we propose to explore how\nlanguage, in addition to visual observations, may\ncontribute to object-centric representation learn-\ning. There has also been work that uses other types\nof supervision, such as dynamic prediction (Kipf\net al., 2020; Bear et al., 2020) and multi-view con-\nsistency (Prabhudesai et al., 2020). In this paper,\nwe focus on unsupervised learning of object-centric\nrepresentations from static images and language.\nVisual concept learning.\nLearning visual con-\ncepts from language and other forms of supervision\nprovides useful representations for various down-\nstream tasks, such as image captioning (Yin and\nOrdonez, 2017; Wang et al., 2018), visual-question\nanswering (Yi et al., 2018; Huang et al., 2019),\nshape differentiation (Achlioptas et al., 2019), im-\nage classiﬁcation (Mu et al., 2020), and scene ma-\nnipulation (Prabhudesai et al., 2020). Previous\nwork has been focusing on various types of rep-\nresentations (Ren et al., 2016; Wu et al., 2017),\ntraining algorithms (Faghri et al., 2018; Morgado\net al., 2020) and supervision (Johnson et al., 2016;\nYang et al., 2018). In this paper, we focus on learn-\ning visual concepts that can be grounded in object-\ncentric representations. Recent work on object-\ncentric grounding of visual concepts (Wu et al.,\n2017; Mao et al., 2019; Hudson and Manning,\n2019; Prabhudesai et al., 2020) has shown great\nsuccess in achieving high performance in down-\nstream tasks and strong generalization from a small\namount of data. However, these methods assume\npre-trained object detectors to generate object pro-\nposals in the scene. In contrast, our LORL learns\nto individuate objects and associates concepts with\nthe learned object-centric representations without\nany annotations on object segmentation masks or\nproperties.\n3\nPreliminaries\nBefore delving into our language-mediated object-\ncentric representation learning paradigm, we ﬁrst\ndiscuss a general formulation that uniﬁes multi-\nple concurrent unsupervised object representation\nlearning methods and a neuro-symbolic framework\nfor learning visual concepts from language.\n3.1\nUnsupervised Object-Centric\nRepresentation Learning\nGiven an image I, a typical unsupervised ob-\nject representation learning model will decom-\npose the scene into a series of slot proﬁles\n{(z1, x1, m1), . . . , (zK, xK, mK)}, where each\nslot proﬁle is expected to represent an object (or\nnothing, as the number of slots may be greater than\nthe actual number of objects in the scene). Here\nzi is the object feature, xi is the object image, and\nmi is the object mask specifying its location in the\nscene.\nIn our paper, we focus on two recent models,\nMONet (Burgess et al., 2019) and Slot Attention\n(Locatello et al., 2020). MONet uses a recurrent\nspatial attention network (Ronneberger et al., 2015)\nto segment out objects in the scene, and adopts\na variational autoencoder (Kingma and Welling,\n2014) to encode objects as well as reconstruct ob-\nject images for self-supervision. At a very high\nlevel, its objective function is calculated as\nL =\n\r\r\r\r\r\n K\nX\nk=1\nmkxk\n!\n−I\n\r\r\r\r\r\n2\n2\n+β·\nK\nX\nk=1\nKL(zk), (1)\nwhere the ﬁrst term is a pixel-wise L2 reconstruc-\ntion loss and the second term computes the KL\ndivergence between the distribution of zk’s and a\nprior Gaussian distribution.\nSlot Attention uses a transformer-like attention\nnetwork (Vaswani et al., 2017) to extract object fea-\ntures, and decode them with convolutional neural\nnetworks to component images and object masks.\nThe model is trained by the same reconstruction\nloss in the form of the L2-norm:\nL =\n\r\r\r\r\r\n K\nX\nk=1\nmkxk\n!\n−I\n\r\r\r\r\r\n2\n2\n.\n(2)\n3.2\nNeuro-Symbolic Concept Learning\nThe neuro-symbolic concept learner (NS-CL; Mao\net al., 2019) learns visual concepts by looking at\nimages and reading paired questions and answers.\nNS-CL takes a set of segmented objects in a given\nimage as its input, extracts their visual features\nwith a ResNet (He et al., 2015), translates the input\nquestion into an executable program by a seman-\ntic parser, and executes the program based on the\nobject-centric representation to answer the ques-\ntion. The key idea of NS-CL is to explicitly repre-\nsent individual concepts in natural language (colors,\nshapes, spatial relationships, etc.) as vector space\nembeddings, and associate them with the object\nembeddings.\nNS-CL answers the input question by executing\nthe program based on the object-centric represen-\ntation. For example, in order to query the name\nof the white object in Fig. 2, NS-CL ﬁrst ﬁlters\nout the object by computing the cosine similarity\nbetween the concept white and individual object\nrepresentations, which produces a “mask” vector\nwhere each entry denotes the probability that an\nobject has been selected. The output “mask” on\nWhat is the name of \nthe white object? \nInput Image\nInput Question\n(a) Image\nEncoder\n(e.g., SlotAttention)\nObject-Centric\nRepresentations\n(b) Image\nDecoder\nObjectness\nScores\n0.9\n0.9\n0.1\nObject\nRecon.\nMask\nRecon.\nRecon. Image\n(c) Semantic\nParser\nScene\nFilter[White]\nQuery[Name]\n(d) Neuro-Symbolic Program Executor\nObjectness Scores\nmin(Objectness, Whiteness)\nAnswer\nPlate\nConcept Embeddings\nWhite\nPlate\n……\nFigure 2: Our LORL contains four modules. (a) An image encoder encodes the input image into a factorized,\nobject-centric representation. (b) An image decoder learns to reconstruct the image from the learned representa-\ntions. It also decodes an objectness score based on the representation. (c) A pre-trained semantic parser translates\nthe input sentence into an executable program and associates concepts with learnable vector embeddings. (d) A\nneuro-symbolic program executor takes the object-centric representation, the objectness scores, the parsed pro-\ngram, and concept embeddings as input to predict the answer (if the input is a question) or TRUE/FALSE (if the\ninput is a descriptive sentence).\nthe objects is fed into the next module and the ex-\necution will continue. The last query operation\nproduces the answer to the question. The vector\nembeddings of individual objects and the concepts\nare jointly trained based on language supervision.\n4\nLanguage-mediated, Object-centric\nRepresentation Learning\nMarrying the ideas of unsupervised object-centric\nrepresentation learning and neuro-symbolic con-\ncept learning, we are able to learn an object-centric\nrepresentation using both visual and language su-\npervision. Fig. 2 shows an overview of Language-\nmediated, Object-centric Representation Learning\n(LORL). In LORL, four modules are optimized\njointly: an image encoder, an image decoder, a\nsemantic parser, and a neuro-symbolic program\nexecutor.\nImage encoder.\nGiven an input image, we ﬁrst\nuse the image encoder (Fig. 2a) to individuate ob-\njects in the scene and extract an object-centric\nscene representation. It takes the input image as\nits input, individuates objects in the scene, and pro-\nduces a collection of latent slot embeddings {zi}.\nImage decoder.\nThe decoder (Fig. 2b) takes the\nobject-centric representation produced by the im-\nage encoder and produces a 3-tuple for each indi-\nvidual slot (xk, mk, sk), where xk reconstructs the\nRGB image of the slot, mk reconstructs the mask,\nand sk ∈[0, 1] is a scalar indicating the objectness\nof the slot. That is, whether k-th slot corresponds\nto a single object in the scene. Here, we have\nextended the general pipeline we described in Sec-\ntion 3.1 with an objectness indicator. It serves dual\npurposes. First, it weights each reconstructed com-\nponent image while generating the reconstructed\nimage. Mathematically, the reconstructed image I′\nis computed as: I′ = PK\nk=1 sk · (mkxk). Second,\nit mediates the output of all filter operations in\nthe program executor.\nIn this paper, we will experiment with two im-\nage encoder-decoder options: MONet (Burgess\net al., 2019) and Slot Attention (Locatello et al.,\n2020). They are both compatible with the learning\nparadigm described above. For both models, we\nuse a single linear layer to predict the objectness\nscore for each slot on top of the second-last layer\nof their image decoders.\nSemantic parser.\nA pre-trained semantic parser\n(Fig. 2c) will translate the input question into an\nexecutable program composed of primitive opera-\ntions, such as filter, which ﬁlters out objects\nwith certain concepts and query, which queries\nthe attribute of the input object. We use roughly\nthe same domain-speciﬁc language (DSL) for repre-\nsenting programs as CLEVR (Johnson et al., 2017a,\nsee also the appendix for details). All concepts that\nappear in the program, such as white, are associated\nwith distinct, learnable concept embedding vectors.\nQ: The small plate is \nwhat color?\nP:scene, \nfilter[small], \nfilter[plate], \nquery[color]\nA: purple\nQ: What number of \nceramic things are there?\nP:  scene, \nfilter[ceramic], \ncount\nA: 1\nD: There is a part \nleft of the cyan seat, \nits color is red.\nP:  scene, \nfilter[cyan, \nseat], \nrelate[left], \nequal[red]\nD: The number of \nlegs there is 4.\nP:  scene, \nfilter[leg], \ncount, \nequal[4]\nD: The color of \nthe back is brown. \nP:  scene, \nfilter[back], \nquery[color], \nequal[brown]\n(a) Examples on Shop-VRB-Simple\n(b) Examples on PartNet-Chairs\nQ: There is a plastic thing; \nare there any heavy metal \nthings behind it?\nP:  scene, \nfilter[plastic], \nrelate[behind], \nfilter[heavy, \nmetal], exist\nA: True\nFigure 3: In Shop-VRB-Simple, questions involve seven different attributes of objects in the scene: name, size,\nweight, material, color, shape, mobility. In PartNet-Chairs, images are paired with sentences describing the names\nand colors of the parts in the scene, and their relationships.\nNeuro-symbolic program executor.\nThe pro-\ngram executor (Fig. 2d) takes the object-centric\nrepresentation from the image encoder {zk}, the\nobjectness score {sk} from the image decoder, the\nconcept embeddings and the program generated\nby the semantic parser as its input. It executes the\nprogram based on the visual and concept represen-\ntations to answer the question.\nThe original program executor in NS-CL (Sec-\ntion 3.2) assumes a pre-trained object detector\nfor generating object proposals.\nIn LORL, we\nassociate each object representation with an ob-\njectness score sk. Recall that a filter opera-\ntion in NS-CL produces a mask vector indicating\nwhether an object has been selected. Here, we me-\ndiate the output of an filter(c) operation as\nmin(sk, filter(c)). Intuitively, a slot will be\nselected only if, ﬁrst, it has concept c and, second,\nit corresponds to a single object in the scene.\nTraining paradigm.\nDuring training, we jointly\noptimize the image encoder, the image decoder,\nand the concept embeddings. They are trained by\nminimizing the loss L:\nL = α · Lperception + β · Lreasoning.\nFor MONet-based image encoder-decoder, we use\nEquation 1 as the perception loss Lperception, while\nfor Slot Attention-based encoder-decoder, we use\nEquation 2. The neuro-symbolic program executor\nproduces a distribution over candidate answers to\nthe input question. We use the cross-entropy loss\nbetween the predicted answer and the ground truth\nanswer as Lreasoning.\nWe use a three-stage training paradigm in LORL.\nFirst, we train the model with only visual inputs\nwith Lperception for N1 epochs. Next, we ﬁx the\nimage encoder and the image decoder, and opti-\nmize the concept embeddings with the loss term\nLreasoning for N2 epochs. During this second stage,\nthe image encoder and the decoder can already\nproduce decent object segmentation results. Fi-\nnally, we jointly optimize all three modules for N3\nepochs. We provide detailed information about\nthe hyperparameters for different models in the\nappendix.\n5\nExperiments\nWe ﬁrst evaluate whether the representations\nlearned by LORL lead to better image segmen-\ntation with the help of language. We then evaluate\nhow these representations may be used for instance\nretrieval, visual reasoning, and referring expression\ncomprehension.\n5.1\nImage Segmentation\nData.\nWe use two datasets for image segmenta-\ntion evaluation. The ﬁrst, Shop-VRB-Simple, is\nbased on Shop-VRB (Nazarczuk and Mikolajczyk,\n2020), a dataset of complex household objects and\nquestion-answer pairs. The second is based on\nchairs in PartNet (Mo et al., 2019), a dataset where\nthe objects are different parts of a chair. Fig. 3\nshows some examples from the two datasets.\nShop-VRB is a visual reasoning dataset, similar\nto CLEVR (Johnson et al., 2017a), but with com-\nplex household objects of different sizes, weights,\nmaterials, colors, shapes, and mobility. Because\nthe original Shop-VRB dataset includes very small\nand highly transparent objects and complex back-\ngrounds, which current unsupervised representa-\ntion learning models cannot handle, we generate\n10K images with a clean background ourselves us-\nImage\nSlot Attention\nLORL + SA\nFigure 4: Visualization on Shop-VRB-Simple. Pixels with the same color represent a mask produced by the\nmodels. The Slot Attention model often fails to segment blenders, coffee makers, and toasters. LORL helps to\ngreatly improve its results.\ning large objects from the dataset. We also pair\nevery image with 9 questions, resulting in 90K\nquestions in total. The test split has 960 images\nand 8.6K questions. We name this variant Shop-\nVRB-Simple.\nWhile the previous literature on unsupervised ob-\nject segmentation mainly focuses on settings where\nobjects are spatially disentangled, we also explore\nhow language may help when objects of interest\nare different parts of a global shape. To this end, we\ncollect a new dataset, PartNet-Chairs, using chair\nshapes from PartNet. Every image here shows a\nchair, where each part of the chair (legs, seat, back,\narms) is randomly assigned a color. We select six\ndifferent chair shapes with one or four legs and zero\nor two arms. We generate 5K images for training.\nEach image is paired with 8 descriptive sentences\ngenerated from human-written templates, resulting\nin 40K examples in total. The test split has 960 im-\nages. Each sentence describes the name and color\nof parts. We provide all templates in the supple-\nmentary material. We are interested in whether\nobject-centric representation learning models may\nseparate these parts and whether and how language\nmay help in this scenario.\nBaselines.\nWe use MONet (Burgess et al., 2019)\nand Slot Attention (Locatello et al., 2020) as the im-\nage modules (Modules 1 and 2), and evaluate how\nthe incorporation of language may improve their\nperformance. Because MONet is color-sensitive,\nand on Shop-VRB-Simple, many objects have di-\nverse colors and specular reﬂection, it does not\nproduce meaningful results there. Thus we only\nshow results with MONet on PartNet-Chairs. We\n(a)\nARI↑\nGT Split↓\nPred Split↓\nSlot Attention\n83.51±2.3\n15.68±1.9\n13.19±1.5\nLORL + SA\n89.23±1.6\n9.95±1.6\n10.18±1.3\n(b)\nSlot Attention\nLORL + SA\nCoffee maker\n39.4±7.0\n21.9±8.3\nBlender\n38.9±11.5\n17.7±3.4\nToaster\n33.4±3.9\n16.4±7.2\nTable 1: (a) Results on Shop-VRB-Simple.\nLORL\nhelps to improve Slot Attention (SA; Locatello et al.,\n2020) in all metrics. (b) The Ground Truth split ra-\ntio for the three object categories where SA most com-\nmonly fail. LORL helps SA to reduce the ratio by 50%.\nshow results with Slot Attention on both datasets.\nMetrics.\nWe use three metrics for evaluation.\nFollowing (Greff et al., 2019), we ﬁrst use the Ad-\njusted Rand Index (ARI; Rand, 1971; Hubert and\nArabie, 1985). It treats segmentation as a clustering\nproblem: each mask is the cluster index that the\npixels within belong to. ARI is computed as the\nsimilarity between the predicted and ground truth\nclusters, and ranges from 0 (random) to 1 (perfect\nmatch).\nIn practice, we found this pixel-wise metric is\nsensitive to the size of objects: a model that infre-\nquently makes mistakes on large objects will have\nlower ARI than one that frequently mis-segments\nsmall objects. Thus, we in addition design two\nobject-centric metrics:\n• Ground Truth Split Ratio (GT Split) mea-\nsures the ratio of objects (GT masks) that are\ncovered by more than one prediction mask.\n• Prediction Split Ratio (Pred Split) measures\nthe ratio of prediction masks that cover more\nImage\nSlot\nAttention\nLORL \n+ SA\nImage\nMONet\nLORL \n+ MONet\nFigure 5: Visualization on PartNet-Chairs. Pixels with the same color represent a mask produced by the models.\nLORL successfully recognizes different parts in various situations.\nARI↑\nGT Split↓\nPred Split↓\nMONet\n91.41±3.7\n10.3±5.1\n14.09±5.2\nLORL + MONet\n94.91±2.1\n4.95±0.7\n4.02±2.5\nSlot Attention\n87.32±3.6\n12.54±6.6\n22.99±5.0\nLORL + SA\n95.81±1.0\n3.39±1.1\n2.92±1.0\nTable 2: Quantitative results on PartNet-Chairs. All\nnumbers are in percentage.\nLORL consistently im-\nproves MONet’s and Slot Attention’s performance on\nsegmentation.\nthan one object (GT mask).\nConcretely, we ﬁrst assign each pixel to the pre-\ndiction mask with the maximum value at the pixel.\nWe say a prediction mask covers an object if it cov-\ners at least 20% of the object’s pixels. The GT and\nPred Split ratios are thus deﬁned as:\nGTSplit = # of objects that are covered by > 1 masks\n# of objects that are covered by > 0 masks,\nPredSplit = # of masks that cover > 1 objects\n# of masks that cover > 0 objects.\nIdeally, there is a one-to-one correspondence be-\ntween objects and predicted masks, with both GT\nSplit and Pred Split being 0. Please refer to the\nappendix for detailed comparison of the proposed\nmetrics with other metrics.\nResults.\nThe quantitative results on SHOP-VRB-\nSimple are summarized in Table 1. We show the\nmean and standard error on each metric over 3 runs.\nSince our semantic parsing module is trained on\npaired question-program pairs, it achieves nearly\nperfect accuracy (>99.9%) on test questions. Thus,\nin later sections, we will focus on evaluating object\nsegmentation, concept grounding, and downstream\ntask performances. LORL helps Slot Attention\nachieve better segmentation results in all three met-\nrics. From visualizations in Fig. 4, we ﬁnd that\nthe original Slot Attention model struggles with\nmetallic objects; but with LORL, it performs much\nbetter in those cases.\nTo further explore how LORL helps Slot Atten-\ntion on failure cases, we calculate the Ground Truth\nSplit Ratio for each object category, and ﬁnd that\nSlot Attention most often fail to segment coffee\nmakers, blenders, and toasters as a whole. These\nobjects have complex sub-parts and their appear-\nance changes quickly when the viewpoint changes.\nWith the help of language, Slot Attention improves\nconsistently over its ablative variants across all the\nthree metrics, reducing the GT Split by 50% on\naverage (Table 1b). Furthermore, we include ab-\nlation studies on how different types of questions\nand different modules (the objectness score module\nand the concept learning module) contribute to the\nperformance improvement in the appendix.\nOn PartNet-Chairs, LORL also helps both\nMONet and Slot Attention improve with a large\nmargin, as shown in Table 2. The results are aver-\naged over 4 runs. MONet in general performs well\non this dataset, though it still sometimes merges\ndifferent parts with the same color into a single\nmask. An example can be found in Fig. 5, column\n3, where the blue arm and the blue bottom in the\ninput image are put into the same mask by MONet.\nSuch an issue is alleviated in LORL + MONet.\nFig. 5 also includes examples to show how LORL\nhelps Slot Attention.\nAs shown in Table 2, the improvement on Slot\nAttention is larger and more consistent, compared\nk = 1\nk = 3\nk = 5\nSlot Attention\n54.07±2.6\n43.70±1.7\n37.02±1.7\nLORL + SA\n94.03±0.6\n91.03±1.3\n87.71±1.7\nTable 3:\nThe percentage (%) of retrieved objects\nthat belong to the same category as the query object.\nWith LORL, objects within the same category are more\nlikely to be close to each other in the feature space. The\nresults are averaged over 3 runs and standard errors are\ngiven.\nQA Accuracy\nLORL + SA (No FT)\n62.79±1.6\nLORL + SA\n92.72±1.0\nTable 4: Our three-stage training paradigm improves\nvisual concept learning. Without ﬁne-tuning (i.e., the\nthird training stage), the question answering accuracy\ndrops by 30%. The results are averaged over 3 runs\nand standard errors are given.\nwith the improvement on MONet. We hypothesize\nthat this is because the two models adopt different\napproaches for aligning object features and masks.\nWhile MONet uses separate modules for segmen-\ntation and object representation learning, Slot At-\ntention obtains masks by directly decoding object\nrepresentations. Having a shared representation\nmight have allowed Slot Attention to gain more\nfrom language supervision.\n5.2\nInstance Retrieval\nWe now analyze the learned object representations\non Shop-VRB-Simple. We ﬁrst use them for in-\nstance retrieval: for each model, we randomly se-\nlect a segmented object and use its learned repre-\nsentation to search for its k nearest neighbors in\nthe feature space. Then, for each selected object,\nwe compute how many of the k nearest neighbors\nbelong to the same category. During searching,\nwe only consider object representations whose cor-\nresponding mask, after decoding, has at least an\nIntersection over Union (IoU) of at least 0.75 with a\nground truth object mask. We sample 1,000 object\nfeatures from each model for evaluation.\nTable 3 includes results with k = 1, 3, 5, sug-\ngesting that the object representations learned by\nLORL + Slot Attention are better for retrieval, com-\npared with features learned by Slot Attention alone\nwithout language. This is because Slot Attention\noften confuses categories that are visually similar\nbut conceptually different, such as baking tray and\nchopping board.\n5.3\nVisual Reasoning\nAs another analysis, we also evaluate how the\nlearned representation of LORL + Slot Attention\nperforms on visual question answering on the Shop-\nVRB-Simple dataset. Here we compare with an\nablated version of LORL, where we only train the\nmodel for the ﬁrst two stages, as stated at the end\nof Section 4. We do not train the model for the\nthird stage—jointly optimizing or ﬁne-tuning all\nthree trainable modules. We name this ablation\nLORL + SA (No FT). Through this analysis, we\nhope to understand the importance of joint training\nof the vision modules (Modules 1 and 2) and the\nreasoning module (Module 4).\nTable 4 shows that joint training is crucial for\nvisual reasoning. This resonates with the previous\nresult, where visually similar objects are clustered\ntogether in the latent space, impeding the useful-\nness of the information encoded.\n5.4\nReferring Expression Comprehension\nFinally we evaluate the representations learned\nby LORL on referring expression comprehension,\nwhere given an expression referring to a set of\nobjects in the scene, like “The white plates”, the\nmodel is expected to return the corresponding ob-\nject masks. After learning all needed concepts from\nquestion-answer pairs, LORL can naturally handle\nreferring expression without any further training, if\nwe assume a pre-trained semantic parser.\nWe choose the IEP-Ref model (Liu et al., 2019)\nas our baseline. It uses a module approach and\nreceives direct segmentation supervision. On Shop-\nVRB, we adapt the code provided by (Liu et al.,\n2019) to generate 17K training examples, only for\nIEP-Ref, and 1.7K testing referring expressions for\nevaluating both LORL and IEP-Ref. Measured in\nRecall@0.5 (the ratio of the recalled objects based\non an IoU threshold of 0.5), IEP-Ref performs bet-\nter than LORL, but the margin is small (90.1% vs.\n84.4%). Note that while IEP-Ref has been trained\non 17K training examples with ground truth object\nsegmentations, LORL does not require any train-\ning data on referring expression comprehension.\nThe relatively comparable results are strong evi-\ndence that the representations learned by LORL\nalso transfer to a new task.\n6\nConclusion\nWe have proposed Language-mediated, Object-\ncentric\nRepresentation\nLearning\n(LORL),\na\nparadigm for learning object-centric representa-\ntions from vision and language. Experiments on\nShop-VRB-Simple and PartNet-Chairs show that\nlanguage signiﬁcantly contributes to learning better\nrepresentations. This behavior is consistent across\ntwo unsupervised image segmentation models.\nThrough systematic studies, we have also shown\nhow LORL helps models to learn object represen-\ntations that encode conceptual information, and\nare useful for downstream tasks such as retrieval,\nvisual reasoning, and referring expression compre-\nhension.\nAcknowledgments\nWe thank Ruidong Wu for helpful discussions in\nthe early stage of the project. This work in part\nsupported by the Samsung Global Research Out-\nreach (GRO) program, Autodesk, IBM, an Amazon\nResearch Award (ARA), and Stanford HAI.\nReferences\nPanos Achlioptas, Judy Fan, Robert Hawkins, Noah\nGoodman, and Leonidas J Guibas. 2019.\nShape-\nglot: Learning language for shape differentiation. In\nCVPR.\nDaniel M Bear, Chaofei Fan, Damian Mrowca, Yun-\nzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz,\nLi Fei-Fei, Jiajun Wu, Joshua B Tenenbaum, et al.\n2020. Learning physical graph representations from\nvisual scenes. In NeurIPS.\nPaul Bloom. 2002. How Children Learn the Meanings\nof Words. MIT press.\nChristopher P Burgess, Loic Matthey, Nicholas Wat-\nters, Rishabh Kabra, Irina Higgins, Matt Botvinick,\nand Alexander Lerchner. 2019.\nMonet:\nUnsu-\npervised scene decomposition and representation.\narXiv:1901.11390.\nEric Crawford and Joelle Pineau. 2019. Spatially in-\nvariant unsupervised object detection with convolu-\ntional neural networks. In AAAI.\nMartin Engelcke, Adam R. Kosiorek, Oiwi Parker\nJones, and Ingmar Posner. 2020.\nGenesis: Gen-\nerative scene inference and sampling with object-\ncentric latent representations. In ICML.\nSM Eslami, Nicolas Heess, Theophane Weber, Yuval\nTassa, Koray Kavukcuoglu, and Geoffrey E Hinton.\n2016. Attend, infer, repeat: Fast scene understand-\ning with generative models. In NeurIPS.\nFartash Faghri, David J Fleet, Jamie Ryan Kiros, and\nSanja Fidler. 2018.\nVse++:\nImproving visual-\nsemantic embeddings with hard negatives.\nIn\nBMVC.\nVikash Goel, Jameson Weng, and Pascal Poupart. 2018.\nUnsupervised video object segmentation for deep re-\ninforcement learning. In NeurIPS.\nAnirudh Goyal,\nAlex Lamb,\nPhanideep Gampa,\nPhilippe Beaudoin, Sergey Levine, Charles Blun-\ndell, Yoshua Bengio, and Michael Mozer. 2020.\nObject ﬁles and schemata: Factorizing declarative\nand procedural knowledge in dynamical systems.\narXiv:2006.16225.\nKlaus Greff, Rapha¨el Lopez Kaufman, Rishabh Kabra,\nNick Watters, Christopher Burgess, Daniel Zoran,\nLoic Matthey, Matthew Botvinick, and Alexander\nLerchner. 2019. Multi-object representation learn-\ning with iterative variational inference. In ICML.\nKlaus Greff, Antti Rasmus, Mathias Berglund, Tele\nHao, Harri Valpola, and J¨urgen Schmidhuber. 2016.\nTagger: Deep unsupervised perceptual grouping. In\nNeurIPS.\nKlaus Greff, Sjoerd van Steenkiste, and J¨urgen Schmid-\nhuber. 2017. Neural expectation maximization. In\nNeurIPS.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Deep residual learning for image recog-\nnition. In CVPR.\nPingping Huang, Jianhui Huang, Yuqing Guo, Min\nQiao, and Yong Zhu. 2019. Multi-grained attention\nwith object-level grounding for visual question an-\nswering. In ACL.\nLawrence Hubert and Phipps Arabie. 1985. Compar-\ning partitions. Journal of classiﬁcation, 2(1):193–\n218.\nDrew Hudson and Christopher D Manning. 2019.\nLearning by abstraction: The neural state machine.\nIn NeurIPS.\nJustin Johnson, Bharath Hariharan, Laurens van der\nMaaten, Li Fei-Fei, C Lawrence Zitnick, and Ross\nGirshick. 2017a. CLEVR: A diagnostic dataset for\ncompositional language and elementary visual rea-\nsoning. In CVPR.\nJustin Johnson, Bharath Hariharan, Laurens van der\nMaaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-\nnick, and Ross Girshick. 2017b. Inferring and exe-\ncuting programs for visual reasoning. In ICCV.\nJustin Johnson, Andrej Karpathy, and Li Fei-Fei.\n2016.\nDensecap: Fully convolutional localization\nnetworks for dense captioning. In CVPR.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nDiederik P. Kingma and Max Welling. 2014.\nAuto-\nencoding variational bayes. In ICLR.\nThomas Kipf, Elise van der Pol, and Max Welling.\n2020. Contrastive learning of structured world mod-\nels. In ICLR.\nAdam R. Kosiorek, Hyunjik Kim, Ingmar Posner, and\nYee Whye Teh. 2018. Sequential attend, infer, re-\npeat: Generative modelling of moving objects. In\nNeurIPS.\nZhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Wei-\nhao Sun, Gautam Singh, Fei Deng, Jindong Jiang,\nand Sungjin Ahn. 2020.\nSpace:\nUnsupervised\nobject-oriented scene representation via spatial at-\ntention and decomposition. In ICLR.\nRuntao Liu, Chenxi Liu, Yutong Bai, and Alan L Yuille.\n2019. Clevr-ref+: Diagnosing visual reasoning with\nreferring expressions. In CVPR.\nFrancesco Locatello, Dirk Weissenborn, Thomas Un-\nterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas\nKipf. 2020. Object-centric learning with slot atten-\ntion. In NeurIPS.\nJiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B.\nTenenbaum, and Jiajun Wu. 2019.\nThe neuro-\nsymbolic concept learner:\nInterpreting scenes,\nwords, and sentences from natural supervision. In\nICLR.\nKaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Sub-\narna Tripathi, Leonidas J Guibas, and Hao Su. 2019.\nPartnet: A large-scale benchmark for ﬁne-grained\nand hierarchical part-level 3d object understanding.\nIn CVPR.\nPedro Morgado, Yi Li, and Nuno Nvasconcelos. 2020.\nLearning representations from audio-visual spatial\nalignment. In NeurIPS.\nJesse Mu, Percy Liang, and Noah Goodman. 2020.\nShaping visual representations with language for\nfew-shot classiﬁcation. In ACL.\nM. Nazarczuk and K. Mikolajczyk. 2020. Shop-vrb:\nA visual reasoning benchmark for object perception.\nIn ICRA.\nMihir Prabhudesai, H. F. Tung, Syed Ashar Javed, Max-\nimilian Sieb, Adam W. Harley, and K. Fragkiadaki.\n2020. Embodied language grounding with 3d visual\nfeature representations. In CVPR.\nWilliam M Rand. 1971. Objective criteria for the eval-\nuation of clustering methods. Journal of the Ameri-\ncan Statistical association, 66(336):846–850.\nZhou Ren, Hailin Jin, Zhe Lin, Chen Fang, and Alan\nYuille. 2016.\nJoint image-text representation by\ngaussian visual-semantic embedding. In ACM MM.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\n2015. U-net: Convolutional networks for biomedi-\ncal image segmentation. In MICCAI.\nKarl Stelzner, Robert Peharz, and Kristian Kersting.\n2019. Faster attend-infer-repeat with tractable prob-\nabilistic models. In ICML.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nRishi Veerapaneni, John D Co-Reyes, Michael Chang,\nMichael Janner, Chelsea Finn, Jiajun Wu, Joshua\nTenenbaum, and Sergey Levine. 2020.\nEntity ab-\nstraction in visual model-based reinforcement learn-\ning. In CoRL.\nJosiah Wang, Pranava Swaroop Madhyastha, and Lu-\ncia Specia. 2018. Object counts! bringing explicit\ndetections back into image captioning. In NAACL.\nGert Westermann and Denis Mareschal. 2014. From\nperceptual\nto\nlanguage-mediated\ncategorization.\nPhilosophical Transactions of the Royal Society B:\nBiological Sciences, 369(1634):20120391.\nJiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli.\n2017. Neural scene de-rendering. In CVPR.\nFei Xu. 1999. Object individuation and object identity\nin infancy: The role of spatiotemporal information,\nobject property information, and language. Acta psy-\nchologica, 102(2-3):113–136.\nFei Xu. 2007.\nSortal concepts, object individua-\ntion, and language.\nTrends in Cognitive Sciences,\n11(9):400–406.\nJianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and\nDevi Parikh. 2018. Visual curiosity: Learning to ask\nquestions to learn visual recognition. In CoRL.\nKexin Yi, Jiajun Wu, Chuang Gan, Antonio Tor-\nralba, Pushmeet Kohli, and Josh Tenenbaum. 2018.\nNeural-symbolic vqa: Disentangling reasoning from\nvision and language understanding. In NeurIPS.\nXuwang Yin and Vicente Ordonez. 2017. Obj2Text:\nGenerating visually descriptive language from ob-\nject layouts. In EMNLP.\nA\nDomain-Speciﬁc Language (DSL)\nLORL extends the domain-speciﬁc language of the\nCLEVR dataset (Johnson et al., 2017a) to accom-\nmodate descriptive sentences. Speciﬁcally, we add\nan extra primitive operation: Equal(X, y). It\ntakes two inputs. In our case, the ﬁrst argument X\nis the output of a Query, Exist, or Count op-\neration. All three operations output a distribution\nover possible answers. The second argument y is\neither a word or number, such as TRUE, white, or\n4. The Equal operation computes the probability\nof X=y. In LORL, models are trained to maximize\nthe output probability.\nB\nHyperparameters\nFor optimization hyperparameters, we largely\nadopt original settings in Burgess et al. (2019) and\nLocatello et al. (2020). Table 5 summarizes the\nhyperparameters for the loss weights (α and β), the\nnumber of training epochs of different stages (N1,\nN2, N3), and the batch size. We early-stop the train-\ning when QA performance converges. We skip the\nsecond training phase on PartNet-Chairs, because\nthe ﬁrst phase (vision-only) yields poor segmen-\ntation performance on this dataset. Establishing a\nmeaningful grounding of concepts could be hard\nin this case. If we keep the second training phase\nfor LORL + Slot Attention on PartNet-Chairs, the\nmodel converges slower in the third training phase\n(15 more epochs in our experiments), but the ﬁnal\nperformance remains the same.\nShop-VRB\nPartNet-Chairs\nSlot Attention\nMONet\nSlot Attention\nα\n1\n0.01\n1\nβ\n0.1\n1\n0.1\nBatch Size\n64\n64\n64\nN1\n90\n100\n50\nN2\n20\n0\n0\nN3\n80\n200\n200\nTable 5: Hyperparameters of LORL\nLearning rate scheduling.\nFor Slot Attention\nmodels, during the ﬁrst training stage (perception-\nonly), we use the learning rate schedule described\nin the original paper on both datasets. Initially,\nthe learning rate is linearly increased from zero to\n4 × 10−4 in the ﬁrst 10K iterations. After that, we\ndecay the learning rate by 0.5 for every 100K itera-\ntions. On PartNet-Chairs, after the ﬁrst stage, Slot\nAttention models continue to use the same learning\nrate scheduling. For Shop-VRB-Simple, we switch\nto a ﬁxed learning rate of 0.001 during N2 phase,\n(a) There are 4 objects in the scene; one of them is split into 3\nmasks. Thus, GT Split = 1/4 = 0.25.\n(b) There are 5 masks in the scene; one of them covers two\nobjects. Thus, Pred Split = 1/5 = 0.2.\nFigure 6: Two examples on how GT/Pred Split Ratios\nare computed.\nwhich takes 20 epochs. After 20 epochs, we de-\ncrease the learning rate to 2 × 10−4. We further\ndecrease the learning rate to 2 × 10−5 after another\n65 epochs. We use the Adam optimizer (Kingma\nand Ba, 2015) for Slot Attention models.\nFor MONet models, we use RMSProp with a\nlearning rate of 0.01 during the ﬁrst stage, and use\n0.001 for the second and the third stage.\nMeanwhile, we also follow NS-CL (Mao et al.,\n2019) to use curriculum learning. Speciﬁcally, in\nthe second training stage, we limit the number of\nobjects in the scene to be 3. In the third training\nstage, we gradually increase the number of objects\nin the scene and the complexity of the questions.\nC\nImplementation Details\nIn this section, we provide additional implementa-\ntion details of our experimental setups and metrics.\nGT and Pred split ratios.\nIn this paper, we have\nintroduced two new metrics for evaluating the per-\nformance of unsupervised object segmentation:\nnamely, the GT split and and the Pred split ratio.\nA simple example of how we can compute\nARI: 82.5\nPred Split: 0.0\nGT Split: 0.67 \nARI: 70.7\nPred Split: 0.0\nGT Split: 0.33\nARI: 96.0\nPred Split: 0.5\nGT Split: 0.0\nARI: 60.6\nPred Split: 0.2\nGT Split: 0.0\n(a.1)\n(a.2)\n(b.2)\n(b.1)\nFigure 7: Comparison of ARI and GT/Pred Split Ratios\non example images. Pixels with the same color repre-\nsent a mask produced by the models. We ﬁnd that ARI\nis very sensitive to the size of objects, while split ratios\ncapture object-level failures where an object is split or\nmultiple objects are merged.\nGT/Pred split ratios is shown in Fig. 6. At a high\nlevel, the GT split ratio computes the percentage of\nobjects that are split into multiple parts in model\nsegmentation. Meanwhile, Pred split ratio com-\nputes the percentage of objects that are merged into\na single object in model segmentation. We intro-\nduce these two new metrics because the ARI score\nis evaluated at the pixel level and does not account\nfor the variance of object sizes. By contrast, GT\nsplit and Pred split metrics are computed at the\nobject level. This difference is illustrated in Fig. 7.\nFor concrete examples, in the Fig. 6 (a.1), two\nobjects, the chopping board and the thermos, are\nwrongly segmented. In Fig. 6 (a.2), only one object\nmis-segmented. However, the ARI score of the ﬁrst\nimage is much higher because the coffee maker has\na large size. GT split ratio is evaluated on the ob-\nject level and thus favor the second one. Similarly,\nin the Fig. 6 (b.1), the four legs are merged into\ntwo masks, while in Fig. 6 (b.2), the seat and the\nback of the chair are merged into a single object.\nHowever, the ﬁrst segmentation result has a sig-\nniﬁcantly higher ARI score because the chair legs\nonly contribute to a small area in the image. In\nthis paper, we propose to jointly use ARI scores\nand the proposed GT/Pred split ratio to evaluate\nsegmentation masks.\nThroughout the paper, we have been using IoU =\n0.2 as the threshold while computing the GT/Pred\nsplit ratios. Table 6 summarizes the results with\ndifferent IoU thresholds. LORL consistently im-\nproves the baseline.\nReferring expression comprehension.\nIn this\nexperiment, the data is generated using the code\nadapted from Liu et al. (2019). It contains two\ntypes of expressions, the ﬁrst one directly refers\nto an object by its properties: for example, “the\nwhite plate”. The second type of sentences refers\nto the object by relating it with another object: for\nexample, “the object that is in front of the mug.”\nThe output of the model is the masks of all referred\nobjects. The dataset is composed of the same set\nof concepts and same DSL as in the Shop-VRB-\nSimple.\nWe use the IEP-Ref model proposed in Liu et al.\n(2019) as a baseline. It is adapted from its prior\nwork IEP (Johnson et al., 2017b). IEP-Ref ﬁrst\ntranslates the referring expression into a sequence\nof primitive operations, which are implemented as\ndifferent modular networks. The model takes the\nimage feature extracted by a CNN as input and exe-\ncutes the program by chaining these component net-\nworks. It outputs a segmentation mask on objects\nbeing referred to. During training, groundtruth seg-\nmentation masks are needed.\nFor all methods, including ours and the baseline,\nwe assume a pretrained semantic parser. Since\nthe neuro-symbolic program executor outputs a\ndistribution over all objects indicating whether they\nare selected, we directly multiply its output with the\nobject segmentation masks to get the ﬁnal output.\nD\nAdditional Results\nThe following section presents a collection of abla-\ntion studies on different modules of LORL, as well\nas a few extensions.\nObjectness score.\nTo validate the effectiveness\nof the proposed objectness score module, we\nhereby compare two models: the original Slot At-\ntention model and the Slot Attention model aug-\nmented with the proposed objectness score module.\nBoth models are trained using images only (there is\nno language in the loop), on the Shop-VRB-Simple\nThres = 0.1\nThres = 0.3\nGT Split↓\nPred Split↓\nGT Split↓\nPred Split↓\nSlot Attention\n23.71±1.5\n23.45±4.1\n8.91±0.7\n7.47±0.9\nLORL + SA\n17.74±1.4\n18.15±1.2\n5.86±1.3\n5.67±0.7\nTable 6: GT/Pred split ratios on Shop-VRB-Simple using different IoU thresholds. The results are averaged over\n3 runs and standard errors are given.\nARI↑\nGT Split↓\nPred Split↓\nSlot Attention\n83.51±2.3\n15.68±1.9\n13.19±1.5\nSA + Obj score\n83.4±1.8\n16.57±1.5\n12.71±0.8\nTable 7: Ablation study of the objectness score module\non Shop-VRB-Simple. The results are averaged over 3\nruns and standard errors are given.\nARI↑\nGT Split↓\nPred Split↓\nSA (image-only)\n83.51±2.3\n15.68±1.9\n13.19±1.5\nCount only\n85.52±1.7\n13.86±2.0\n12.56±1.4\nExist only\n86.29±2.2\n15.34±2.9\n10.33±1.1\nQuery only\n88.79±1.1\n10.64±1.7\n9.03±0.5\nAll types\n89.23±1.6\n9.95±1.6\n10.18±1.3\nTable 8: Ablation study of using different types of ques-\ntions to train LORL + SA on Shop-VRB-Simple. The\nstandard errors are computed based on 3 runs.\ndataset. The Table 7 summarizes the result. The\nobjectness module alone does not contribute to the\nsegmentation performance.\nQuestion type.\nIn this section, we investigate\nhow different types of questions affect LORL. We\nuse the Shop-VRB-Simple dataset for evaluation.\nThere are three types of questions in the dataset,\ncounting (e.g., how many plates are there?), ex-\nistence (e.g., is there a toaster?), and query (e.g.,\nwhat is the color of the mug?). We train LORL\n+SA with only one single type of questions (the\nnumber of total questions is the same).\nResults are summarized in Table 8. In general,\ntraining on all three types of questions improves\nthe segmentation accuracy. The largest gain comes\nfrom the query question. Interestingly, the best re-\nsult is achieved when trained on the original dataset,\nwhere the ratio of counting, existence, and query\nquestions is 1:1:7. Note that all these models are\ntrained with the same number of questions and thus\nthey are directly comparable with each other.\nData efﬁciency.\nIn addition, we provide another\nanalysis by comparing models trained with differ-\nent number of question-answer pairs. The results\nare shown in Table 9. Adding more language data\nconsistently improves the result. All results are\nbased on the LORL +SA model trained on the\nShop-VRB-Simple dataset.\nARI↑\nGT Split↓\nPred Split↓\n25% (22.5K)\n81.01\n14.31\n15.34\n50% (45K)\n84.39\n14.79\n9.37\n75% (67.5K)\n86.53\n11.67\n11.52\n100% (90K)\n89.23\n9.95\n10.18\nTable 9: Ablation study of using different number of\nquestions to train LORL + SA on Shop-VRB-Simple.\nVisual reasoning baselines.\nTo establish a base-\nline for visual reasoning tasks, we present the re-\nsults of two visual reasoning approaches IEP (John-\nson et al., 2017b) and NS-CL (Mao et al., 2019)\nfor reference on the Shop-VRB-Simple dataset, as\nshown in Table 12. All models are trained with\nthe same set of question-answer pairs. Note that\nNSCL has the access to a pretrained object detec-\ntion module, while LORL +SA and IEP do not.\nLORL +SA outperforms IEP, which is trained with\nexactly the same amount of supervision as ours. It\nalso achieves a comparable result as NS-CL.\nIntegration with SPACE.\nSPACE (Lin et al.,\n2020) is another popular method for unsupervised\nobject-centric representation learning. SPACE uses\nparallel spatial attention to decompose the input\nscene into a collection of objects, and it is also com-\npatible with the proposed learning paradigm LORL.\nWe include additional results of LORL +SPACE\non the CLEVR dataset. Shown in the Table 11,\nLORL +SPACE shows a signiﬁcant advantage over\nthe vanilla SPACE model. Additionally, we ﬁnd\nthat SPACE shows poor segmentation results on\nShop-VRB-Simple and ParNet-Chairs, no matter\nwhether it is integrated with LORL. For example,\nit frequently segments complex objects into too\nmany fragments on Shop-VRB-Simple. We conjec-\nture that this is because SPACE was designed for\nsegmenting objects of similar sizes.\nBaseline using language supervision.\nWe also\nconducted an additional baseline model that uses\nlanguage supervision in a different way. Speciﬁ-\ncally, based on the Slot Attention model, we use a\nGRU to directly encode question and answer, and\nconcatenate it with the image feature to obtain the\nobject representation. On Shop-VRB-Simple, this\nPrecision\nRecall\n@0.5\n@0.75\n@1\n@0.5\n@0.75\n@1\nLORL + SA (NO)\n59.52±0.7\n54.13±2.2\n36.36±3.6\n94.96±1.2\n86.4±3.6\n58.07±5.7\nLORL + SA\n89.47±0.7\n79.37±1.2\n52.48±1.6\n94.72±0.1\n84.02±0.6\n55.55±1.2\nTable 10: Concept quantiﬁcation evaluation. The number after @ indicates the IoU threshold. The results suggest\nthat objectness score improves the precision of concept quantiﬁcation.The results are averaged over 3 runs.\nARI↑\nGT Split↓\nPred Split↓\nSPACE (Lin et al., 2020)\n72.34\n29.2\n12.38\nLORL + SPACE\n97.82\n2.04\n2.17\nTable 11: Segmentation performance of SPACE and\nLORL +SPACE on CLEVR. The integration of LORL\nimproves the result.\nQA Accuracy\nLORL + SA (No FT)\n62.79±1.6\nIEP (Johnson et al., 2017b)\n78.3±0.1\nNSCL (Mao et al., 2019)\n97.9±0.0\nLORL + SA\n92.72±1.0\nTable 12: Question answering accuracy on the Shop-\nVRB-Simple dataset. The results are averaged over 3\nruns and standard errors are given.\nmodel does not show improvement over the Slot At-\ntention baseline: ARI = 76.4%, GT Split = 29.2%,\nPred Split = 13.6%. This suggests the effective-\nness of LORL.\nConcept quantiﬁcation.\nAlthough LORL with-\nout the objectness score can achieve a comparable\nresult in terms of QA accuracy, objectness score\nis crucial if we want to evaluate how models dis-\ncover objects in images. Here, we show that, on\nthe Shop-VRB-Simple dataset, LORL +SA shows\nsigniﬁcant improvement in recovering a holistic\nscene representation.\nSpeciﬁcally, we extract a scene graph for each\nscene, where each node corresponds to a detected\nobject. We represent each node i as a set of con-\ncepts Ci associated with the object (e.g., {large,\nbrown, wooden, chopping board}). We associate\na concept with a detected object if its cosine simi-\nlarity with the object representation is greater than\n0. We heuristically remove nodes that are not as-\nsociated with any concepts (by treating them as\n“background” objects) or have objectness scores\nthat are smaller than 0.5. This results in a scene\ngraph, where each node corresponds to a detected\nobject. In the following, we compare it against the\ngroundtruth scene graph.\nFor each pair of groundtruth node i and detected\nnode j, we compute the concept IoU score based\non their associated concepts Ci and Cj as:\nIoUij = |Ci ∩Cj|\n|Ci ∪Cj|.\nNext, we perform a maximum weight match-\ning between the detected scene graph and the\ngroundtruth scene graph with the Hungarian al-\ngorithm. We use the IoU score as the weight for\nevery edge and remove edges whose IoU score is\nsmaller than a given threshold. Finally, based on\nthe macthing, we can compute the precision and\nrecall of the detected scene graph. We show the\naverage precision and recall over the entire test set\nimages in Table 10. The results suggest that object-\nness score signiﬁcantly improves the precision of\nthe extracted concepts.\n",
  "categories": [
    "cs.LG",
    "cs.CL",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2020-12-31",
  "updated": "2021-06-08"
}