{
  "id": "http://arxiv.org/abs/2101.03963v1",
  "title": "Language Detection Engine for Multilingual Texting on Mobile Devices",
  "authors": [
    "Sourabh Vasant Gothe",
    "Sourav Ghosh",
    "Sharmila Mani",
    "Guggilla Bhanodai",
    "Ankur Agarwal",
    "Chandramouli Sanchi"
  ],
  "abstract": "More than 2 billion mobile users worldwide type in multiple languages in the\nsoft keyboard. On a monolingual keyboard, 38% of falsely auto-corrected words\nare valid in another language. This can be easily avoided by detecting the\nlanguage of typed words and then validating it in its respective language.\nLanguage detection is a well-known problem in natural language processing. In\nthis paper, we present a fast, light-weight and accurate Language Detection\nEngine (LDE) for multilingual typing that dynamically adapts to user intended\nlanguage in real-time. We propose a novel approach where the fusion of\ncharacter N-gram model and logistic regression based selector model is used to\nidentify the language. Additionally, we present a unique method of reducing the\ninference time significantly by parameter reduction technique. We also discuss\nvarious optimizations fabricated across LDE to resolve ambiguity in input text\namong the languages with the same character pattern. Our method demonstrates an\naverage accuracy of 94.5% for Indian languages in Latin script and that of 98%\nfor European languages on the code-switched data. This model outperforms\nfastText by 60.39% and ML-Kit by 23.67% in F1 score for European languages. LDE\nis faster on mobile device with an average inference time of 25.91\nmicroseconds.",
  "text": "Language Detection Engine for Multilingual\nTexting on Mobile Devices\nSourabh Vasant Gothe, Sourav Ghosh, Sharmila Mani, Guggilla Bhanodai, Ankur Agarwal, Chandramouli Sanchi\nSamsung R&D Institute Bangalore, Karnataka, India 560037\nEmail: {sourab.gothe, sourav.ghosh, sharmila.m, g.bhanodai, ankur.a, cm.sanchi}@samsung.com\nAbstract—More than 2 billion mobile users worldwide type\nin multiple languages in the soft keyboard. On a monolingual\nkeyboard, 38% of falsely auto-corrected words are valid in\nanother language. This can be easily avoided by detecting the\nlanguage of typed words and then validating it in its respective\nlanguage. Language detection is a well-known problem in natural\nlanguage processing. In this paper, we present a fast, light-weight\nand accurate Language Detection Engine (LDE) for multilingual\ntyping that dynamically adapts to user intended language in real-\ntime. We propose a novel approach where the fusion of character\nN-gram model [1] and logistic regression [2] based selector\nmodel is used to identify the language. Additionally, we present\na unique method of reducing the inference time signiﬁcantly\nby parameter reduction technique. We also discuss various\noptimizations fabricated across LDE to resolve ambiguity in\ninput text among the languages with the same character pattern.\nOur method demonstrates an average accuracy of 94.5% for\nIndian languages in Latin script and that of 98% for European\nlanguages on the code-switched data. This model outperforms\nfastText [3] by 60.39% and ML-Kit1 by 23.67% in F1 score [4]\nfor European languages. LDE is faster on mobile device with an\naverage inference time of 25.91µ seconds.\nIndex Terms—Language detection, multilingual, character N-\ngram, logistic regression, parameter reduction, mobile device,\nIndian macaronic languages, European languages, soft-keyboard\nI. INTRODUCTION\nIn the current era of social media, language detection\nis a much required intelligence in mobile device for many\napplications viz. translation, transliteration, recommendations,\netc. Language detection algorithms work almost accurately\nwhen the language scripts are distinct using simple script\ndetection method. In India, there are 22 ofﬁcial languages, and\nalmost every language has its own script but in general user\nprefers to type in Latin script. As per our statistical analysis,\n39.78% of words typed in QWERTY layout are from Indian\nlanguages. Hindi is a popular Indian language, 22.8% of Hindi\nlanguage users use QWERTY keyboard for typing, that implies\nthe need of support for languages written in Latin script.\nStandard languages written in Latin script i.e typed in\nQWERTY keyboard are referred to as Macaronic languages.\nThese languages can have the same character pattern with\nother languages, unlike standard ones. For example, when\nHindi language is written in Latin script (Hinglish), the word\n“somwar” which means Monday, shares the same text pattern\n1https://ﬁrebase.google.com/docs/ml-kit/android/identify-languages\nwith English word “Ransomware”, in such cases, character-\nbased probabilistic models alone fail to identify the exact\nlanguage as probability will be higher for multiple languages.\nAlso, the user may type based on phonetic sound of the word\nthat leads to variations like, “somwaar”, “somvar”, “somvaar”\netc. which are completely user dependent.\nThe soft keyboard provides next-word predictions, word\ncompletions, auto-correction, etc. while typing. Language\nModels (LMs) responsible for those are built using Long\nShort-Term Memory Recurrent Neural Networks (LSTM\nRNN) [5] based Deep Neural networks (DNN) model [6] with\na character-aware CNN embedding [7]. We use knowledge\ndistilling method proposed by Hinton et al. [8] to train the LM\n[9]. Along with LMs, adding another DNN based model for\ndetecting the language that executes on every character typed,\nwill increase the inference time and memory and leads to lag in\nmobile device. Additionally, in soft-keyboards extensibility is\na major concern. Adding one or more language in the keyboard\nbased on the locality or discontinuing the support of a language\nshould be effortless.\nConsidering the above constraints into account, we present\nthe Language Detection Engine (LDE), an amalgamation of\ncharacter N-gram models and a logistic regression based\nselector model. The engine is fast in inferencing on mobile\ndevice, light-weight in model size and accurate for both code-\nswitched (switching between the languages) and monolingual\ntext. This paper discusses various optimizations performed to\nincrease engine accuracy compared to DNN based solutions\nin ambiguous cases of the code-switched input text.\nWe also discuss how LDE performs on ﬁve Indian Mac-\naronic languages Hinglish (Hindi in English), Marathinglish\n(Marathi in English), Tenglish (Telugu in English), Tanglish\n(Tamil in English), Benglish (Bengali in English) and four\nEuropean languages- Spanish, French, Italian, and German.\nSince the typing layout is of English language (Latin script),\nwe term English as a primary language and all other languages\nas a secondary language.\nII. RELATED WORK\nIn this section, we discuss about work related to language\ndetection, both N-gram based and deep learning based ap-\nproaches.\n978-1-7281-6332-1/20/$31.00 © 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any\ncurrent or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or\nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nhttps://doi.org/10.1109/ICSC.2020.00057\narXiv:2101.03963v1  [cs.CL]  7 Jan 2021\n50\n100\n150\n200\n250\n60\n70\n80\n90\n100\nCorpus Size (×1000 lines)\nAccuracy (%)\nHinglish\nTenglish\nSpanish\nGerman\nFig. 1: Char N-gram accuracy over corpus size\nA. N-gram based models\nAhmed et al. [10] detail about language identiﬁcation using\nN-gram based cumulative frequency addition to increase the\nclassiﬁcation speed of a document. It is achieved by reducing\ncounting and sorting operations by cumulative frequency addi-\ntion method. In our problem, we detect the language, based on\nuser typed text rather than document with large information.\nVatanen et al. [1] compared the naive bayes classifer based\ncharater N-gram and ranking method for language detection\ntask. Their paper focuses on detecting short segments of length\n5 to 21 characters, and all the language models are constructed\nindependently of each other without considering the ﬁnal clas-\nsiﬁcation in the process. We have adopted similar methodology\nfor building char N-gram models in our approach.\nErik Tromp et al. [11] discuss Graph-Based N-gram Lan-\nguage Identiﬁcation (LIGA) for short and ill-written texts.\nLIGA outperforms other N-gram based models by capturing\nthe elements of language grammar as a graph. However LIGA\ndoes not handle the case of code-switched text.\nAll the above referred models do not prioritize recently\ntyped words. For seamless multilingual texting which involves\ncontinuous code-switching, more priority must be given to the\nrecent words so that the suggestions from currently detected\nlanguage model can be fetched from the soft keyboard and\nshown to the user.\nB. Deep learning based models\nLopez et al. propose a DNN based language detection for\nspoken utterance [12] motivated by the success in acoustic\nmodeling using DNN. They train a fully connected feed-\nforward neural network along with the logisic regression\ncalibration to identify the exact language. Javier Gonzalez-\nDominguez et al. [13] further extend to utilize LSTM RNNs\nfor the same task.\nZhang et al. [14] have recently presented CMX, a fast,\ncompact model for detection of code-mixed data. They address\nsame problem as ours but in a different environment. They\ntrain basic feed-forward network that predicts the language\nfor every token passed where multiple features are used to\nobtain the accurate results. However, such models require huge\ntraining data and are not feasible in terms of extensibility and\nmodel size for mobile devices.\nThis paper presents a novel method to resolve the ambiguity\nin input text and detect the language accurately in multilingual\nsoft-keyboard for ﬁve Indian macaronic languages and four\nEuropean languges.\nIII. PROPOSED METHOD\nWe propose the Language Detection Engine (LDE) that\nenhances user experience in multilingual typing by accurately\ndeducing the language of input text in real-time. LDE is a\nunion of (a) character N-gram model which gives emission\nprobability of input text originating from a particular language,\nand (b) a selector model which uses the emission probabilities\nto identify the most probable language for a given text using\nlogistic regression [15]. Unique architecture of independent\ncharacter N-gram models with selector model is able to detect\nthe code-mixed multilingual context accurately.\nA. Emission probability estimation using Character N-gram\nCharacter N-gram is a statistical model which estimates\nprobability distribution over the character set of a particular\nlanguage given its corpus.\n1) Train data: Training corpus is generated by crawling\nonline data from various websites. For Indian macaronic\nlanguages, we crawled the native script (Example: Devanagari\nscript for Hindi) data of the languages and reverse translit-\nerated to Latin script. This data is validated by the language\nexperts for quality purpose.\nWe experimented with various sizes of corpus to train\ncharacter N-gram model and found out that 100k sentences\nshow the best accuracy from the model on sample test set, as\ndetailed in Fig. 1.\n2) Model training: For every supported language li, we\ntrain Character N-gram model Cli independent of the other\nlanguages as shown in Fig. 2. Probability of a sequence of\nwords (t1..n) in language li is given by,\nPli (t1..n) =\nn\nY\nk=1\nPli(tk)rn−k , where r ∈(0, 1]\n(1)\nWe prioritize the probability of most recent word over the\nprevious words using a variable r, value ranging between\n0 and 1, that effectively reduces the impact of the leading\nwords probability by converging values closer to 1. To prevent\nthe underﬂow of values we use logarithmic probabilities.\nMathematically,\nlog Pli (t1..n) =\nn\nX\nk=1\nrn−k · log Pli(tk)\n(2)\nThe probability of sequence of characters in a word t,\nrepresented as c0..m, where c0 is considered as space character,\nw1\nw2\n.\n.\n.\n.\nwn\n \nWords list lis listt\n.\n.\n.\nCL1\nCL2\nCLm\nEP(w1)\n \nL1,  EP(w1)\n     \nL2, . . . , EP(w1)Lm\n   \nEP(w2)\n \nL1,  EP(w2)\n     \nL2, . . . , EP(w2)Lm\n.\n.\nEP(wn)\n \nL1,  EP(wn)\n     \nL2, . . . , EP(wn)Lm\n \n \nChar N-gram\n   Models list\n \n \n \nEmis lists listion Probabilities list of words words list\n \n \nLogis listtic Regression Regres lists listion\n    \nModel\ny\nw  b\n, \nw\n(w\nL  \n1 ,  bL1),\nw\n(w\nL  \n2 ,  bL2),\n.\n.\n.\nw\n(w\nLm ,  bLm)\n \nParameter\nReduc Regression tion\nTL1,\nTL2\n   .\n   .\nTLm\n \nFinal Thres listhold\n      values list\n \n                \n \n \n \nm\n- Number of words language s listupported\nL        \nn\n            - nth Language\n w  b            \n, \n-\n \n \n \nWeight and bias list vec Regression tors list\nC\n \nLm             -  Tr\n \n \n \n \n \nained Char N-gram Model of words mth language\n EP(wn)\n \nLm  \n \n \n \n \n- Emis lists listion probability of word w of words word w  \nn\n \n \n \nin language m\n[ ]\n[\n]\n \nN-gram Trainer\nCorpus list\n \n \nLanguage Detec Regression tion Engine\n  \n \nWeight , bias list vec Regression tors list\nFig. 2: Model Training\nis given by\nPli (c0..m) = Pli (c1|c0) ·\nm\nY\nk=2\nPli (ck|ck−2ck−1)\n(3)\nThese trained models Cli are used to estimate the emission\nprobability of character sequence during the inference for\nlanguage li. We have chosen n to be 3 in N-gram model,\ni.e character tri-gram model is trained on the corpus.\nB. Selector Model\nHere we brieﬂy discuss the motivation behind an addi-\ntional selector model. Firstly, input text originating from one\nlanguage may also have signiﬁcant emission probability in\nanother language that may belong to the same family. This\nis because of words sharing the similar roots and frequent\nusage of loan words.\nFor example, the Spanish word “vocabulario” shares lin-\nguistic root with its English counterpart “vocabulary”. Again,\n“jungle” which is a frequently used word in English, is\nactually a loan word via Hindi from Sanskrit. Presence of\nsuch words in the training corpus increases the preplexity of\nthe model, i.e emission probabilities will be higher for multiple\nlanguages which makes it difﬁcult to deduce the ultimate\nlanguage.\nSecondly, as character N-gram model gets trained based\non the frequency of characters in the corpus, this makes\nit dependent on the size of character set. So, the emission\nprobability values become incomparable as languages with\nsmaller character set will statistically get higher values. Hence,\nthese probabilities from character N-gram are not sufﬁcient\nenough to determine the source language accurately. To this\nend, we present a logistic regression based selector model\nwhich addresses the illustrated problems.\nSelector model S comprises of weight and bias vectors\nw and b respectively of size m, where m is the number\nof supported languages. This model transforms the emission\nprobability provided by char N-gram such that the new\nprobability value of word tn, P ′\nli (tn), given by,\nlog P ′\nli (tn) = wli · log Pli (tn) + bli\n(4)\nwhere li is deemed to be the origin language of word tn if,\nP ′\nli (tn) ≥0.5\n(5)\n1) Train data: Training data for the selector model is\nthe vector of emission probabilities of word tn for every\nlanguage li. Batch of 200k labeled words are used for training\nparameters of a particular language li. These 200k words\ncomprise of 100k vocabulary words belonging to language\nli, and another 100k words that are equally distributed among\nother languages.\nTrained character N-gram models Cl1..m provide the re-\nquired emission probabilities for every word from m different\nlanguages.\n2) Model Training: Weight and bias vectors of the selector\nmodel are trained such that for every input word, probability\nfor labeled language is greater than 0.5 as given in equation\n(5). These trained weight and bias vectors are used to ob-\ntain new probability values as given in equation (4), which\nnow become comparable among languages. Newly estimated\nprobabilities resolve the ambiguity in the input text among the\nlanguages which have same patterns, with clearly dominated\nprobability for the ﬁnal detected language.\nAs shown in Fig. 2, selector model takes emission probabil-\nities εp(wn)Lm for every word wn from each language (Lm)\nas input from pre-trained character N-gram models (Clm) and\nyields weight and bias vectors wli, bli respectively.\n3) Parameter Reduction: LDE performs a set of computa-\ntions to detect the language on mobile device, which we term\nas on-device inference. For every character typed by the user\non a soft-keyboard, on-device inferencing happens followed\nby the inference of DNN Language model to provide next\nword predictions, word completions, and auto-correction, etc.\nbased on the context.\nTo optimize on-device inference time, we propose a novel\nmethod of parameter reduction which reduces multiple com-\nputations during inference to a single arithmetic operation.\nEquation (4) is simpliﬁed to combine the weight and bias\nparameters as a single threshold value τl given by Equation (8)\nwhich effectively reduces the computation to constant time.\nFrom Equations (4) and (5),\nlog P ′\nli (tn) ≥log 0.5\nwli · log Pli (tn) + bli ≥log 0.5\n(6)\nThis can be further reduced to\nlog Pli (tn) −log 0.5 −bli\nwli\n≥0\n=⇒log Pli (tn) −log 0.5 −bli\nwli\n+ log 0.5 ≥log 0.5\n=⇒log Pli (tn) −(wli −1) · log 2 −bli\nwli\n≥log 0.5\n∴log Pli (tn) −τli ≥log 0.5\n(7)\nwhere τli is a parameter given by\nτli = (wli −1) · log 2 −bli\nwli\n(8)\n4) On-device inference: From equations (6) and (7), it is\nevident that we can obtain the ultimate probability log P ′\nli (tn)\njust by subtracting the threshold value τl from the logarithmic\nemission probability log Pl (tn) as given below,\nlog P ′\nli (tn) = log Pli (tn) −τli\n(9)\nwhere li is the language and tn is the character sequence. This\nmakes probabilities from different languages comparable.\nIV. ENGINE ARCHITECTURE\nLanguage Detection Engine constitutes of multiple compo-\nnents in various phases like preprocessor, optimizer, Char N-\ngram and Selector. The input text is ﬁrst pre-processed and\npassed to the optimization phase where multiple heuristics are\napplied to address the enigmatic cases and proceeds to char N-\ngram inference and ﬁnally language selector phase to obtain\nthe detected language. End-to-end architecture is represented\nin Figure 3. In this section, we explain each phase of the\nengine in detail.\n \n \n“Lingua deteccion”\nPre-processor\nOptimizers\nChar N-gram\nSelector Model\nTokenizer\nCaching\nSpecial  Symbol Handler\nShort-text Handler\nTypo handler\nPronoun Exclusion\nRecent word priority\nProbability Estimation\nThreshold computing\nInference\n“Spanish”\nModel loading\nModel loading\nFig. 3: Engine Architecture\nA. Pre-processor\nIn this phase, the input text is preprocessed to obtain the\nrequired information from large context.\n1) Special Symbol Handler: In soft keyboard, the input may\nnot be only text but can also include various ideograms like\nemojis, stickers, etc. This handler trims the input and provides\nthe data that is necessary to detect the language.\n2) Tokenizer: Engine tokenizes the input context into to-\nkens with whitespace as a delimiter. The last two tokens are\nconcatenated and processed for language detection, which is\nobserved to be most efﬁcient in terms of processing time and\naccuracy, compared to considering more than two tokens. For\nshort words with character length ≤2, tokenizing is left to\nshort-text handler.\n3) Caching: Based on the current detected language multi-\nple algorithms like, auto-correction, auto-capitalization, touch-\narea correction [16] [17] etc. tune the word suggestions\naccordingly in real-time. This leads to multiple calls to LDE\nfor the same input text, hence LDE caches the language of\npreviously typed text, to avoid the redundant task of detecting\nthe language again.\nB. Optimizers\nLDE addresses enigmatic cases in multilingual typing by\napplying additional optimizations that are discussed below.\n1) Short-text Handler: The context is an entire input that\nthe user has typed and engine uses the previous two tokens of\nthe context to detect the language. In the cases of short-words\nwith character length less than or equal to two, it becomes\nambiguous to detect the language. For example, “to me” is a\nvalid context in English as well as in Hinglish, in such cases\nwords before this context helps to deduce the exact source\nlanguage. So extending the context to prior words, when\ncontext word length is less than two resolves the ambiguity\nfor the engine to decide upon short words. We observed ∼5%\nimprovement in the accuracy of Indian macaronic languages\nwith this change.\n2) Typo Handler: When a user makes a typo, often its\nharder to decide from which language the suggestions or\ncorrection should be provided. To address this issue, LDE\nobtains a correction candidate word from non-current language\nLM [9] with an edit distance of one and effectively avoids\nthe decrease in False Negatives due to wrong auto-correction.\nBelow example illustrates the need for this heuristic,\n“[Hello]E [bhai suno]H [can we meet ]E [ksl]∗”\nwhere subscript []E indicates English text and []H as\nHinglish and []∗a typo. When this context is typed in English\nand Hinglish bilingual keyboard, the engine fetches one auto-\ncorrection candidate [kal] (meaning tomorrow) with an edit\ndistance of one. Though the previous two words are from\nEnglish, LDE manages to auto-correct the typo into valid word\nfrom non-current language Hinglish.\nTypo handler automatically adopts to the user behavior\nwhile typing and provides valid corrections from the LM.\nFor Indian macaronic languages, ∼22% improvement and for\nEuropean languages ∼15% improvement observed in the F1\nscore of auto-correction on a linguist written bi-lingual test\nset.\nIn a closed beta trial with 2000 soft keyboard users over a\nperiod of two months, 38% of the falsely auto-corrected words\nare valid in another language. LDE is able to suppress the false\nauto-corrections and improve the auto-correction performance\nby 43.71% in mono-lingual keyboard.\n3) Pronoun Exclusion: Practically, there is no particular\nlanguage associated with proper nouns alone, but it follows\nthe language of the entire context. To address this, the engine\nstores a linguist validated pronoun’s list as a TRIE data-\nstructure [18] for efﬁcient look-up. If the typed word is found\nin pronouns list, the cached language for the input excluding\npronoun is considered as the detected language.\nC. Char N-gram\n1) Model loading: As explained in section 3. a tri-gram\nmodel is used to obtain emission probabilities of the character\nsequence. So there are n+1P3 possible character sequences\nin a language with the character set of length n and an\nadditional character, i.e whitespace ‘ ’. These probabilities\nare pre-computed for every language and stored separately\nin a binary data ﬁle which is further compressed using zlib\n[19] compression to reduce the ROM size on mobile device.\nConsidering the whitespace [10] as an extra character for\ntraining the char N-gram model makes an impact when the\ncharacter pattern is same among multiple languages. Fig.\n4 depicts average gain of 10.13% achieved for European\nlanguages when whitespace is considered.\nFor loading model on device, data ﬁle is uncompressed and\nprobabilities are loaded to an array of every language. Due to\nthe modularity of model ﬁles, we can upgrade the model or\nFig. 4: Pictorial representation of Table III\nadd a new language and remove existing one just by training\nthe required language’s model. Such provision addresses the\nextensibility issue for soft-keyboard effectively.\n2) Recent word prioritization: In multilingual typing code-\nswitching happens continuously, input begins with one lan-\nguage and eventually switches to another. To detect the current\nlanguage in real-time, priority should be given to the recently\ntyped character sequence as mentioned before.\nIn below example,\n“[Our company is ]E [intentando ]ES”\nﬁrst three words are of English and the next is of Spanish.\nIdeally, current detected language should be Spanish as there\nis a code-switch. But if all characters are treated equally most\nprobable language will be English. From Equation (1) it can\nbe observed that our char N-gram prioritizes trailing words\nthan the leading words resulting in detecting the language\naccurately.\nD. Selector Model\n1) Threshold computing: As explained in section III. A,\nlogistic regression model is trained using library provided\nby sklearn [15] in python. For every language, the model is\ntrained to obtain the weight and bias, and further reduced to\ndetermine threshold value as explained in Equation (8). The\ncomplete processing is done on a 64-bit linux machine ofﬂine\nand threshold values are loaded to the model.\n2) Model loading: After parameter reduction every individ-\nual language has corresponding threshold value. The threshold\nvalues are stored in respective languages char n-gram data\nﬁle itself and unload to an array of thresholds corresponding\nto supported languages on device. In this way, we curtail\nthe effort of re-training all the models for any modiﬁcations\nand update only threshold values in respective data ﬁle. LDE\ndoes not require any large infrastructure to train and build the\nmodel, all our experiments were conducted on a linux machine\nof 4GB RAM.\nV. EXPERIMENTAL RESULTS\nWe compare the Language detection Engine performance\nwith various baseline solutions like fastText library [3],\nlangID.py [20] and Equilid a DNN model [21] and also with\nGoogle’s ML-Kit2. In this section, we brieﬂy explain the\nexperimental set-up conﬁgured for all of above mentioned\nmodels and discuss about the test set that we prepared\nfor the evaluation. Performance of LDE is compared with\nmonolingual models like fastText and langId.py, ML-Kit and\nmultilingual model such as Equilid.\nA. fastText\nJoulin et al. [3] have distributed the model3 which can\nidentify 176 languages. We used this model to compare\nthe performance of European languages with LDE. However\nfastText pre-trained model does not support Indian macaronic\nlanguages.\nCustom fastText model for Indian macaronic languages:\nWe trained a custom fastText supervised model. We used\nreverse transliterated corpus of all the Indian languages which\nis validated by linguists. The same corpus is used to train LDE\nso that evaluation is comparable. 2.5GB of corpus was used\nto train the fastText model for ﬁve Indian languages each of\nsize 500MB. Custom trained model size after quantization is\n900KB.\nB. ML-Kit\nML Kit supports total of 103 languages including one\nIndian macaronic language, Hinglish. ML-Kit doesn’t have a\nprovision to train custom models for other Indian macaronic\nlanguages. For the experiment purpose a sample android\napplication was developed that uses the API exposed by ML-\nKit to identify the language and calculate the F1 score for\ngiven test set. Complete evaluation was performed on Samsung\nGalaxy A50 device.\nC. Langid.py\nLangid.py is a standalone python tool by Lui and Bald-\nwin [20] [22] that can identify 97 languages. Langid.py is\nmonolingual model, i.e it can not identify code-switched text.\nTherefore we compare only on inter-sentential sentences where\nno code-switching is involved within a sentence.\nD. Equilid: Socially-Equitable Language Identiﬁcation\nJurgens et al. propose a sequence-to-sequence DNN model\n[21] for detecting the language. Equilid identiﬁes the code-\nswitched multilingual text and tags every word with the\ndetected language. An experiment was conducted on a GPU\nto evaluate the metric by loading the pre-trained models4.\nPre-trained model is of size 559MB which can identify 70\nlanguages but none of the Indian macaronic languages are\nsupported by Equilid.\n2https://developers.google.com/ml-kit\n3https://dl.fbaipublicﬁles.com/fasttext/supervised-models/lid.176.ftz\n4http://cs.stanford.edu/ jurgens/data/70lang.tar.gz\nE. Performance Evaluation\nWe evaluate the performance on two types of test sets\nbased on code-switching style (a) Intra-sentential and (b) Inter-\nsentential. These test sets are hand written by the language\nexperts involving natural code-switching. We evaluate above\ndescribed methodologies and compare with LDE.\nTABLE I: Description of the Intra-sentential test set\nLanguage\nWords\nCharacters\nCode-switch (%)\nFrench\n6430\n37968\n48.84\nItalian\n4403\n30016\n53.17\nGerman\n5499\n34380\n49.97\nSpanish\n6663\n41231\n47.89\nHinglish\n6332\n36656\n61.70\nBenglish\n6123\n34983\n59.25\nMarathinglish\n5520\n38580\n65.40\nTanglish\n6024\n35416\n57.29\nTenglish\n5958\n44676\n50.22\nTABLE II: Comparison on Intra-sentential test set\nLanguage\nF1 score\nfastText\nLDE\nML-Kit\nEquilid\nFrench\n0.6714\n0.9980\n0.813\n0.9722\nItalian\n0.7445\n0.9901\n0.7926\n0.9934\nGerman\n0.5456\n0.9960\n0.8008\n0.9535\nSpanish\n0.5144\n0.9870\n0.8044\n0.9912\nHinglish\n0.5232\n0.9920\n0.9120\n−\nBenglish\n0.7562\n0.9561\n−\n−\nMarathinglish\n0.6278\n0.9840\n−\n−\nTanglish\n0.7820\n0.9765\n−\n−\nTenglish\n0.7120\n0.9981\n−\n−\n1) Intra-sentential test set: In this type, the code-switching\ncan occur anywhere in the sentence, where there are again\ntwo possibilities, i) Test set 1: context written mainly in\nprimary language English and partly in secondary languages,\nfor example,\n“Can you believe midterms comienza next week”\nwhere Spanish word is used while typing in English.\nand ii) Test set 2: context written mainly in secondary\nlanguage and partly written in primary language. For example,\n“Justo thinking en ti”\nwhere English word is used while typing in Spanish.\nA uniformly distributed test set of these two types were\ntaken by picking 300 sentences from each one. Every word in a\ntest sentence is manually tagged with the source language. As\nthere will be multiple code-switching involved, context level\nlanguage detection is performed i.e based on previous two\nwords the current language is identiﬁed which is exactly the\nway as LDE identiﬁes the language for soft keyboard.\nStatistics for these test sets like the percentage of code-\nswitching involved, characters, words are shown in Table I.\nFig. 5: Pictorial representation of Table II\nF1 score: Table II shows the comparison of F1 score\nbetween fastText [3], ML-Kit, Equilid [21] with LDE for\nEuropean and Indian macaronic languages. For European\nlanguages LDE outperforms fastText by 60.39% and exceeds\nGoogle’s ML-Kit by 23.67% also surpasses Equilid DNN\nModel by 1.55%. For Indian macaronic languages LDE is\n44.29% better than fastText and exceeds by 7.6% for Hinglish\nwith ML-Kit. It can be observed that LDE performs better\nthan the DNN based models which are huge in model size.\nFig. (5) represents the visualization of the performance of\nvarious language detection models on intra-sentential test-\nset, where LDE is in par with the Equilid and signiﬁcantly\ndominates fastText and ML-Kit.\nTABLE III: Comparison on Inter-sentential test set\nLanguage\nF1 score\nfastText\nLDE\nML-Kit\nLangId.py\nFrench\n0.9874\n0.9872\n0.9962\n0.9590\nItalian\n0.9745\n0.9856\n0.9834\n0.8918\nGerman\n0.9895\n0.9901\n0.9899\n0.902\nSpanish\n0.9765\n0.9823\n0.9892\n0.9182\nHinglish\n0.9094\n0.9530\n0.912\n−\nBenglish\n0.8563\n0.9163\n−\n−\nMarathinglish\n0.6696\n0.8936\n−\n−\nTanglish\n0.7963\n0.8696\n−\n−\nTenglish\n0.8675\n0.9102\n−\n−\n2) Inter-sentential test set: In this type of data the code-\nswitching occurs only after a sentence in ﬁrst language is\ncompletely typed. Total of 500 test sentences from every\nlanguage combination are used to obtain the metric. Unlike\nin previous case, here we evaluate sentence level accuracy\nfor each model, as there is no code-switching involved within\nthe sentence. Additionally, we evaluated the same test set on\nLangID.py [20] which is a popular off-the-shelf model for this\ntype of data.\nFig. 6: Pictorial representation of Table III\nF1 score: On inter-sentential test-set all the models perform\naccurately as there is a long context to identify. Table III shows\nthe F1 score for fastText, ML-Kit and LangId for European\nand Indian languages. It is observed that LDE is in par with\nML-Kit and fastText and better than LangId.py for European\nlanguages. However, for Indian languages LDE dominates\nfastText by 10% and ML-Kit by 22.95% for Hinglish which\nshows that LDE performs as good as the DNN models. Figure\n6. shows the comparison of various models performance on\ninter-sentential test set.\nInference time: Table IV shows the inference time and\nmodel size for all 10 supported languages on a uniformly\ndistributed intra-sentential and inter-sentential test set. Average\ninference time is 25.91µ seconds and the model size of LDE\nfor all 10 languages combined is 166.65KB.\nTABLE IV: Average Inference time and Model size\nLanguage\nInference Time (µs)\nModel size (KB)\nFrench\n24.30\n22.23\nEnglish\n20.64\n20.28\nItalian\n20.04\n21.71\nGerman\n21.08\n18.44\nSpanish\n24.30\n16.66\nHinglish\n35.41\n13.54\nBenglish\n27.34\n13.46\nMarathinglish\n26.94\n14.31\nTanglish\n32.46\n13.74\nTenglish\n26.56\n12.28\nVI. CONCLUSION\nWe have proposed LDE a fast, light-weight, accurate engine\nfor multilingual typing with a novel approach, that unites char\nN-gram and logistic regression model for improved accuracy.\nLDE model size is 5X smaller than that of fastText custom\ntrained model and ∼60% better in accuracy. LDE being a\nshallow learning model, either surpasses or in par with state-\nof-the-art DNN models in performance. Though char N-gram\nis trained on monolingual data, LDE accurately detects code-\nswitching in a multilingual text with the help of uniquely\ndesigned selector model. LDE also improved the performance\nof auto-correction by 43.71% by suppressing correction of\nvalid foreign words. Furthermore, LDE is suitably designed\nfor supporting extensibility of languages.\nREFERENCES\n[1] T. Vatanen, J. J. V¨ayrynen, and S. Virpioja, “Language identiﬁcation of\nshort text segments with n-gram models.”\n[2] H.-F. Yu, F.-L. Huang, and C.-J. Lin, “Dual coordinate descent methods\nfor logistic regression and maximum entropy models,” Machine Learn-\ning, vol. 85, no. 1-2, pp. 41–75, 2011.\n[3] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, “Bag of tricks for\nefﬁcient text classiﬁcation,” arXiv preprint arXiv:1607.01759, 2016.\n[4] C. Goutte and E. Gaussier, “A probabilistic interpretation of precision,\nrecall and f-score, with implication for evaluation,” in European Con-\nference on Information Retrieval.\nSpringer, 2005, pp. 345–359.\n[5] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[6] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock`y, and S. Khudanpur,\n“Extensions of recurrent neural network language model,” in 2011 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2011, pp. 5528–5531.\n[7] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, “Character-aware\nneural language models,” in Thirtieth AAAI Conference on Artiﬁcial\nIntelligence, 2016.\n[8] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural\nnetwork,” arXiv preprint arXiv:1503.02531, 2015.\n[9] W. Chen, D. Grangier, and M. Auli, “Strategies for training large\nvocabulary neural language models,” arXiv preprint arXiv:1512.04906,\n2015.\n[10] B. Ahmed, S.-H. Cha, and C. Tappert, “Language identiﬁcation from\ntext using n-gram based cumulative frequency addition.”\n[11] E. Tromp and M. Pechenizkiy, “Graph-based n-gram language identi-\nﬁcation on short texts,” in Proc. 20th Machine Learning conference of\nBelgium and The Netherlands, 2011, pp. 27–34.\n[12] I. Lopez-Moreno, J. Gonzalez-Dominguez, O. Plchot, D. Martinez,\nJ. Gonzalez-Rodriguez, and P. Moreno, “Automatic language identiﬁca-\ntion using deep neural networks,” in 2014 IEEE international conference\non acoustics, speech and signal processing (ICASSP).\nIEEE, 2014, pp.\n5337–5341.\n[13] J. Gonzalez-Dominguez, I. Lopez-Moreno, H. Sak, J. Gonzalez-\nRodriguez, and P. J. Moreno, “Automatic language identiﬁcation using\nlong short-term memory recurrent neural networks,” in Fifteenth Annual\nConference of the International Speech Communication Association,\n2014.\n[14] Y. Zhang, J. Riesa, D. Gillick, A. Bakalov, J. Baldridge, and D. Weiss, “A\nfast, compact, accurate model for language identiﬁcation of codemixed\ntext,” arXiv preprint arXiv:1810.04142, 2018.\n[15] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin,\n“Liblinear: A library for large linear classiﬁcation,” Journal of machine\nlearning research, vol. 9, no. Aug, pp. 1871–1874, 2008.\n[16] S. Azenkot and S. Zhai, “Touch behavior with different postures on\nsoft smartphone keyboards,” in Proceedings of the 14th international\nconference on Human-computer interaction with mobile devices and\nservices.\nACM, 2012, pp. 251–260.\n[17] C. Thomas and B. Jennings, “Hand posture’s effect on touch screen\ntext input behaviors: A touch area based study,” arXiv preprint\narXiv:1504.02134, 2015.\n[18] S. Mani, S. V. Gothe, S. Ghosh, A. K. Mishra, P. Kulshreshtha,\nM. Bhargavi, and M. Kumaran, “Real-time optimized n-gram for mobile\ndevices,” in 2019 IEEE 13th International Conference on Semantic\nComputing (ICSC).\nIEEE, 2019, pp. 87–92.\n[19] J.-l. Gailly and M. Adler, “Zlib home site,” 2008.\n[20] M. Lui and T. Baldwin, “Langid.py: An off-the-shelf language\nidentiﬁcation\ntool,”\nin\nProceedings\nof\nthe\nACL\n2012\nSystem\nDemonstrations, ser. ACL ’12.\nStroudsburg, PA, USA: Association\nfor Computational Linguistics, 2012, pp. 25–30. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=2390470.2390475\n[21] D. Jurgens, Y. Tsvetkov, and D. Jurafsky, “Incorporating dialectal\nvariability for socially equitable language identiﬁcation,” in Proceedings\nof the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers).\nVancouver, Canada: Association\nfor Computational Linguistics, Jul. 2017, pp. 51–57.\n[22] M. Lui and T. Baldwin, “Cross-domain feature selection for language\nidentiﬁcation,” in Proceedings of 5th international joint conference on\nnatural language processing, 2011, pp. 553–561.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-01-07",
  "updated": "2021-01-07"
}