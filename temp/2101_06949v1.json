{
  "id": "http://arxiv.org/abs/2101.06949v1",
  "title": "HinFlair: pre-trained contextual string embeddings for pos tagging and text classification in the Hindi language",
  "authors": [
    "Harsh Patel"
  ],
  "abstract": "Recent advancements in language models based on recurrent neural networks and\ntransformers architecture have achieved state-of-the-art results on a wide\nrange of natural language processing tasks such as pos tagging, named entity\nrecognition, and text classification. However, most of these language models\nare pre-trained in high resource languages like English, German, Spanish.\nMulti-lingual language models include Indian languages like Hindi, Telugu,\nBengali in their training corpus, but they often fail to represent the\nlinguistic features of these languages as they are not the primary language of\nthe study. We introduce HinFlair, which is a language representation model\n(contextual string embeddings) pre-trained on a large monolingual Hindi corpus.\nExperiments were conducted on 6 text classification datasets and a Hindi\ndependency treebank to analyze the performance of these contextualized string\nembeddings for the Hindi language. Results show that HinFlair outperforms\nprevious state-of-the-art publicly available pre-trained embeddings for\ndownstream tasks like text classification and pos tagging. Also, HinFlair when\ncombined with FastText embeddings outperforms many transformers-based language\nmodels trained particularly for the Hindi language.",
  "text": "HinFlair: a pre-trained contextual string embeddings for pos tagging and text\nclassification in Hindi language\nHARSH PATEL, Medi-Caps University, India\nRecent advancements in language models based on recurrent neural networks and transformers architecture have\nachieved state-of-the-art results on a wide range of natural language processing tasks such as pos tagging, named entity\nrecognition, and text classification. However, most of these language models are pre-trained in high resource languages\nlike English, German, Spanish. Multi-lingual language models include Indian languages like Hindi, Telugu, Bengali in\ntheir training corpus, but they often fail to represent the linguistic features of these languages as they are not the\nprimary language of the study. We introduce HinFlair, which is a language representation model (contextual string\nembeddings) pre-trained on a large monolingual Hindi corpus. Experiments were conducted on 6 text classification\ndatasets and a Hindi dependency treebank to analyze the performance of these contextualized string embeddings for\nthe Hindi language. Results show that HinFlair outperforms previous state-of-the-art publicly available pre-trained\nembeddings for downstream tasks like text classification and pos tagging. Also, HinFlair when combined with FastText\nembeddings outperforms many transformers based language model trained particularly for Hindi language.\nThe datasets and other resources used for this study are publicly available at https://github.com/harshpatel1014/\nHinFlair\nAdditional Key Words and Phrases: Text Classification, POS tagging, Language modeling\nACM Reference Format:\nHarsh Patel. 2021. HinFlair: a pre-trained contextual string embeddings for pos tagging and text classification in\nHindi language. 1, 1 (January 2021), 7 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nDifferent NLP tasks like pos tagging, named entity recognition, question answering, sentiment analysis,\nmachine translation have seen significant improvements in recent years. Improved deep learning techniques\nand data availability has led to the development of many language representation model and embeddings.\nThese language models and embeddings capture deep semantic/syntactic features of the language resulting\nin less dependence on feature engineering. These advancements have engendered the NLP community to\ntransition from task-specific problems to fine-tuning models for downstream tasks. However, most of the\nmodels and embeddings are primarily trained in high resource languages, and few models are trained in low\nresource languages. Languages spoken in India are diverse. Around 1.3 billion people in India communicate\nwith these diverse languages, with Hindi being the most spoken (500 million). Therefore, there is a great\nAuthor’s address: Harsh Patel, patel.harsh1014@gmail.com, Medi-Caps University, AB Rd, Pigdamber, Rau, Indore, Madhya\nPradesh, India, 453331.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2021 Association for Computing Machinery.\nManuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2101.06949v1  [cs.CL]  18 Jan 2021\n2\nHarsh Patel\nneed for language models and embeddings representing these languages for social, cultural, and linguistic\nreasons[Ruder 2020].\nWord embeddings are important decisive components for general NLP tasks like sequence labeling, text\nclassification, entity extraction, etc. Typically, there are three types of distinctive word embeddings. First,\nclassic word embedding trained over large data that captures syntactic and semantic similarity like Word2Vec\n[Mikolov et al. 2013], GloVe [Pennington et al. 2014]. Second, embeddings that capture character level\nsub-word features like FastText [Bojanowski et al. 2017]. Third, embeddings that address context dependency\nand polysemy of words like ELMo [Peters et al. 2018].\nThe introduction of transformers based architecture replaced recurrent layers with multi-headed self\nattention [Vaswani et al. 2017]. Models based on RNN architecture are not effective in handling long-term\ndependencies and prevents parallelization even with attention mechanism [Hochreiter et al. 2001; Parikh\net al. 2016]. Transformers seems to address these issues effectively and are therefore ideal when dealing with\nNLP tasks like machine translation and question answering. Researchers have since then introduced many\ntransformers based architecture, with one capturing context beyond fixed length [Dai et al. 2019], other\ncapturing context in both directions [Devlin et al. 2018], while some incorporating the best of earlier models\n[Yang et al. 2019], each giving state-of-the-art results for various NLP tasks.\nHowever, there are few language representation models and pre-trained embeddings for low resource\nlanguages like Hindi. In this article, I introduce HinFlair, pre-trained embeddings for the Hindi language.\nHinFlair is based on Flair embeddings [Akbik et al. 2018], which achieves state-of-art of results on various\nsequence labeling tasks. Flair embeddings are pre-trained contextual string embeddings that combine the\nbest of different types of embeddings mentioned above. Same as flair embeddings, HinFlair is trained on a\nlarge monolingual Hindi corpus that captures character level contextualized features. I evaluated HinFlair\nembeddings on six text classification datasets and one pos tagging dataset, and results show that HinFlair\nsignificantly outperforms previous state-of-the-art embeddings and language models.\n2\nRELATED WORK\nThere are limited literature and research work on language representation models and pre-trained word\nembeddings when it comes to the Hindi language. AI4Bharat-IndicNLP Corpus have trained FastText\nembedding on their Hindi monolingual corpus containing around 63 million sentences [Kunchukuttan\net al. 2020]. Their pre-trained embeddings have given state-of-the-art results on text classification. Indic-\nTransformers have trained 4 different transformers based architecture for the Hindi language and have\nachieved the best results for various NLP tasks [Jain et al. 2020]. Likewise, there is a comprehensive study\nwhere English text classification datasets are converted to the Hindi language, and they tested performance\nof around 8 different neural architectures on these converted datasets [Joshi et al. 2019]. Models on iNLTK\nlibrary also gives state-of-the-art result for text classification [Arora 2020]. In this study, I have compared\nperformance of HinFlair with the results published from these researches stated above.\n3\nMATERIALS AND METHODS\nThe following sections describe the dataset used for training HinFlair embeddings, along with the details of\npos tagging and text classification datasets used for evaluation of the embedding. Furthermore, technical\ndetails of training the embedding and fine-tuning are presented.\nManuscript submitted to ACM\nHinFlair: a pre-trained contextual string embeddings for pos tagging and text classification in Hindi\nlanguage\n3\nTable 1. Statistics of Monolingual Hindi Data\nSource\nNo. of Sentences\nBBC-new\n18,098\nBBC-old\n135,171\nHindMonoCorp\n44,486,496\nHealth Domain\n8,001\nTourism Domain\n15,395\nWikipedia\n259,305\nJudicial Domain\n152,776\nTotal\n45,075,242\nTable 2. Statistics of Classification and POS tagging datasets\nDataset\nNo. of Classes\nTrain\nTest\nIITP Movie\n3\n2480\n310\nIITP Product\n3\n4182\n523\nBBC Articles\n14\n3468\n867\nTrec-6\n6\n5452\n500\nSST-1\n5\n8544\n2210\nSST-2\n2\n6920\n1821\nUD Hindi\n30\n13304\n1684\n3.1\nDatasets\nHinFlair embedding is trained on a large monolingual Hindi corpus produced by IIT Bombay [Kunchukuttan\net al. 2018]. The monolingual corpus is created by collecting Hindi text from various sources containing\ntotal of around 45 million sentences. For this study, I have used 80 percent of the data for training, while 10\npercent of data is used for validation and testing each. The statistics of the monolingual corpus are listed in\nTable 1.\nHinFlair embeddings performance is tested on 6 text classification datasets: IIT Patna movie review\ndataset [Akhtar et al. 2016], IIT Patna product review dataset [Akhtar et al. 2016], BBC articles for text\nclassification. All these datasets are in Hindi language. The other 3 datasets: Trec-6 question corpus [Voorhees\nand Harman 2000], Stanford Sentiment Datasets SST-1 and SST-2 are also used for evaluation of embeddings\n[Socher et al. 2013]. These datasets are translated from their original versions in English to Hindi language\nusing Google Translate. While Hindi Universal Dependency Treebank is used for testing HinFlair on pos\ntagging task [Bhat et al. 2017; Palmer et al. 2009]. Table 2 shows statistics about these datasets.\n3.2\nHinFlair Training Details\nFor this study, I have used state-of-the-art Flair embeddings architecture model to train HinFlair [Akbik\net al. 2018]. HinFlair is trained on large monolingual corpus. The language model embedding is trained in\nthe forward direction. Tokens from the corpus are fed as a sequence of characters into bidirectional LSTMs\n[Graves 2013; Hochreiter and Schmidhuber 1997]. Bidirectional LSTMs captures context from both direction\nflexibly encoding long-term dependencies better than typical RNNs [Jozefowicz et al. 2016]. The output\nManuscript submitted to ACM\n4\nHarsh Patel\nfrom both hidden states are concatenated after last character in the word, giving contextualized string\nembeddings for each word in a sequence [Akbik et al. 2019]. The overall approach is illustrated in Figure 1.\nFig. 1. Overall approach for training HinFlair embeddings\nHindi language is written in Devanagari script. Therefore, a character dictionary from a monolingual\ncorpus was created before training the model. Parameters for training the model are selected as per the\nrecommendation from the authors of Flair. The hidden size of 1024 is taken for both LSTMs. Sequence\nlength of 250 and a mini-batch size of 100 is selected. Model training is initialized with a learning rate of\n20, annealed by the factor of 4 for every 25 splits with no improvement. HinFlair embeddings model was\ntrained for 10 epochs for more than a week. The model reached the validation perplexity of 3.44 at end of\nthe training.\n3.3\nExperiments\nPOS tagging. POS tagging is a sequence labeling problem. Popular neural network architecture BiLSTMs-\nCRF is employed on top of HinFlair embeddings in this experiment [Huang et al. 2015]. Flair NLP library\nallows concatenating different word embeddings together. Experiments on high resource languages show\nthat the model gives better results when Flair embedding is combined with classical word embeddings\n[Akbik et al. 2018]. Therefore, for this experiment, I have concatenated HinFlair embeddings with FastText\nembeddings trained in the Hindi language. Model is trained with an initial learning rate of 0.1 for 200 epochs.\nLSTMs hidden size is set to 256 with a batch size equal to 32. F1 score metric is used to test performance of\nthe model.\nText Classification. The initial settings for text classification is same as pos tagging. Vector representations\nfor each word from HinFlair embeddings and FastText embedding are concatenated together. These\nrepresentations are taken as input to GRU network instead of BiLSTMs-CRF network [Chung et al. 2014].\nManuscript submitted to ACM\nHinFlair: a pre-trained contextual string embeddings for pos tagging and text classification in Hindi\nlanguage\n5\nTable 3. Test Results for POS tagging\nTags\nPrecision\nRecall\nF1-score\nPRP\n0.9903\n0.9896\n0.9900\nPSP\n0.9972\n0.9983\n0.9978\nNNPC\n0.9004\n0.9089\n0.9047\nNNP\n0.9395\n0.9269\n0.9331\nSYM\n1.0000\n1.0000\n1.0000\nCC\n0.9915\n0.9930\n0.9922\nRP\n0.9938\n0.9856\n0.9897\nJJ\n0.9450\n0.9695\n0.9571\nNN\n0.9733\n0.9702\n0.9718\nVM\n0.9951\n0.9948\n0.9949\nQF\n0.9576\n0.9679\n0.9627\nVAUX\n0.9941\n0.9964\n0.9953\nQC\n0.9899\n0.9933\n0.9916\nNST\n0.9940\n0.9940\n0.9940\nINTF\n0.8571\n0.9231\n0.8889\nNNC\n0.8450\n0.8390\n0.8420\nNEG\n0.9947\n0.9947\n0.9947\nDEM\n0.9764\n0.9892\n0.9828\nQO\n0.9815\n0.9298\n0.9550\nRB\n0.9683\n0.9037\n0.9349\nRDP\n0.8750\n0.8750\n0.8750\nJJC\n0.8235\n0.5833\n0.6829\nWQ\n0.9545\n1.0000\n0.9767\nQCC\n0.9694\n0.9596\n0.9645\nPRPC\n1.0000\n0.7500\n0.8571\nUNK\n0.1875\n0.4286\n0.2609\nNSTC\n1.0000\n1.0000\n1.000\nRBC\n1.0000\n0.0000\n0.0000\nQFC\n1.0000\n0.0000\n0.0000\nCCC\n1.0000\n1.0000\n1.0000\nF1-score\n97.44\nThe output of GRU network is single embedding for complete sentence. The hidden size of GRU is 256.\nOther parameters are same as used for model training of pos tagging. Accuracy metric is used for evaluation\nof HinFlair for text classification.\n4\nRESULTS\nThe results of HinFlair embeddings on pos tagging and 6 text classification datasets are listed in Table 3\nand Table 4 respectively.\nResults show that HinFlair achieves state-of-the-art results on 5 out of 6 text classification datasets.\nHinFlair achieves the best accuracy of 62.26 beyond 57.74 on IITP Movie, best accuracy of 77.25 beyond\n75.71 on IITP product review dataset. On BBC article dataset, HinFlair gets the second-best score of 77.6.\nResults here are compared with transformers based models available on iNLTK. When compared with other\nManuscript submitted to ACM\n6\nHarsh Patel\nTable 4. Test Results for Text Classification in Hindi\nDataset\nFT-W\nFT-WC\nINLP\niNLTK\nHinFlair\nIITP Movie\n41.61\n44.52\n45.81\n57.74\n62.26\nIITP Product\n58.32\n57.17\n63.48\n75.71\n77.25\nBBC Articles\n72.29\n67.44\n74.25\n78.75\n77.6\nTrec-6\n-\n-\n-\n-\n94.39\nSST-1\n-\n-\n-\n-\n40.7\nSST-2\n-\n-\n-\n-\n78.74\nword embeddings, HinFlair outperforms embeddings like INLP by at least 10 points on each dataset. For\nthis study, I have converted three text classification datasets to Hindi language. HinFlair achieves a score\nof 94.39 on trec dataset, 40.7 on SST-1, and 78.74 on SST-2 dataset. There aren’t any pre-trained word\nembedding in the Hindi language that is tested on these datasets.\nFor POS tagging task, HinFlair gets the best F1-score of 97.44 on Universal Dependency Hindi treebank\ncontaining 30 different tags on pos tagging whose results are listed in Table 3.\n5\nCONCLUSION AND FUTURE WORK\nThis article presents HinFlair, pre-trained contextualized string embeddings for the Hindi language. Results\nshow that HinFlair significantly outperforms previous word embeddings for NLP tasks like text classification\nand pos tagging. HinFlair embedding is trained in the forward direction. For future work, I plan to train\nHinFlair embedding on the same corpus but in a backward direction as combining embeddings trained in\nthe opposite direction further improves the performance on various NLP tasks. Also, there aren’t many\ntransformers models trained on a large Hindi corpus. Therefore, for future work, I would like to train\nlanguage models based on transformers architecture for Indic languages.\nACKNOWLEDGMENTS\nI would like to thank the Department of Computer Science and Engineering from Medi-Caps University for\nassistance. I also thank anonymous reviewers for their suggestions and comments.\nREFERENCES\nAlan Akbik, Tanja Bergmann, and Roland Vollgraf. 2019. Pooled Contextualized Embeddings for Named Entity Recognition.\nIn NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational\nLinguistics. 724–728.\nAlan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual String Embeddings for Sequence Labeling. In COLING\n2018, 27th International Conference on Computational Linguistics. 1638–1649.\nMd Shad Akhtar, Ayush Kumar, Asif Ekbal, and Pushpak Bhattacharyya. 2016. A hybrid deep learning architecture for\nsentiment analysis. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\nTechnical Papers. 482–493.\nGaurav Arora. 2020. iNLTK: Natural Language Toolkit for Indic Languages. In Proceedings of Second Workshop for NLP\nOpen Source Software (NLP-OSS). 66–71.\nRiyaz Ahmad Bhat, Rajesh Bhatt, Annahita Farudi, Prescott Klassen, Bhuvana Narasimhan, Martha Palmer, Owen Rambow,\nDipti Misra Sharma, Ashwini Vaidya, Sri Ramagurumurthy Vishnu, et al. 2017. The hindi/urdu treebank project. In\nHandbook of Linguistic Annotation. Springer, 659–697.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.\nEnriching word vectors with subword\ninformation. Transactions of the Association for Computational Linguistics 5 (2017), 135–146.\nManuscript submitted to ACM\nHinFlair: a pre-trained contextual string embeddings for pos tagging and text classification in Hindi\nlanguage\n7\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent\nneural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl:\nAttentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 (2019).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.\nBert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\nAlex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 (2013).\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jürgen Schmidhuber, et al. 2001. Gradient flow in recurrent nets: the\ndifficulty of learning long-term dependencies.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015.\nBidirectional LSTM-CRF models for sequence tagging.\narXiv preprint\narXiv:1508.01991 (2015).\nKushal Jain, Adwait Deshpande, Kumar Shridhar, Felix Laumann, and Ayushman Dash. 2020. Indic-Transformers: An\nAnalysis of Transformer Language Models for Indian Languages. arXiv preprint arXiv:2011.02323 (2020).\nRamchandra Joshi, Purvi Goel, and Raviraj Joshi. 2019. Deep Learning for Hindi Text Classification: A Comparison. In\nInternational Conference on Intelligent Human Computer Interaction. Springer, 94–101.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language\nmodeling. arXiv preprint arXiv:1602.02410 (2016).\nAnoop Kunchukuttan, Divyanshu Kakwani, Satish Golla, Avik Bhattacharyya, Mitesh M Khapra, Pratyush Kumar, et al.\n2020. AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages. arXiv preprint\narXiv:2005.00085 (2020).\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. 2018. The IIT Bombay English-Hindi Parallel Corpus. In\nProceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European\nLanguage Resources Association (ELRA), Miyazaki, Japan.\nhttps://www.aclweb.org/anthology/L18-1548\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and\nphrases and their compositionality. Advances in neural information processing systems 26 (2013), 3111–3119.\nMartha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009. Hindi syntax:\nAnnotating dependency, lexical predicate-argument structure, and phrase structure. In The 7th International Conference\non Natural Language Processing. 14–17.\nAnkur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural\nlanguage inference. arXiv preprint arXiv:1606.01933 (2016).\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In\nProceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 1532–1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.\nDeep contextualized word representations. arXiv preprint arXiv:1802.05365 (2018).\nSebastian Ruder. 2020. Why You Should Do NLP Beyond English. http://ruder.io/nlp-beyond-english.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013.\nRecursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing. 1631–1642.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998–6008.\nEllen M Voorhees and Donna Harman. 2000. Overview of the sixth text retrieval conference (TREC-6).\nInformation\nProcessing & Management 36, 1 (2000), 3–35.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized\nautoregressive pretraining for language understanding. In Advances in neural information processing systems. 5753–5763.\nManuscript submitted to ACM\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-01-18",
  "updated": "2021-01-18"
}