{
  "id": "http://arxiv.org/abs/2101.08387v6",
  "title": "A Survey on Ensemble Learning under the Era of Deep Learning",
  "authors": [
    "Yongquan Yang",
    "Haijun Lv",
    "Ning Chen"
  ],
  "abstract": "Due to the dominant position of deep learning (mostly deep neural networks)\nin various artificial intelligence applications, recently, ensemble learning\nbased on deep neural networks (ensemble deep learning) has shown significant\nperformances in improving the generalization of learning system. However, since\nmodern deep neural networks usually have millions to billions of parameters,\nthe time and space overheads for training multiple base deep learners and\ntesting with the ensemble deep learner are far greater than that of traditional\nensemble learning. Though several algorithms of fast ensemble deep learning\nhave been proposed to promote the deployment of ensemble deep learning in some\napplications, further advances still need to be made for many applications in\nspecific fields, where the developing time and computing resources are usually\nrestricted or the data to be processed is of large dimensionality. An urgent\nproblem needs to be solved is how to take the significant advantages of\nensemble deep learning while reduce the required expenses so that many more\napplications in specific fields can benefit from it. For the alleviation of\nthis problem, it is essential to know about how ensemble learning has developed\nunder the era of deep learning. Thus, in this article, we present fundamental\ndiscussions focusing on data analyses of published works, methodologies, recent\nadvances and unattainability of traditional ensemble learning and ensemble deep\nlearning. We hope this article will be helpful to realize the intrinsic\nproblems and technical challenges faced by future developments of ensemble\nlearning under the era of deep learning.",
  "text": "A Survey on Ensemble Learning under the Era of Deep Learning \n \nYongquan Yanga, Haijun Lvb, Ning Chenc \na Institution of Clinical Pathology, West China Hospital, Sichuan University, 37 Guo \nXue Road, 610041 Chengdu, China \nb AIDP, Baidu Co., Ltd, 701 Na Xian Road, 201210 Shanghai, China \nc School of Electronics and Information, Xi’an Polytechnic University, 19 Jin Hua Road, \n710048 Xi’an, China \n \n \nAbstract \nDue to the dominant position of deep learning (mostly deep neural networks) in \nvarious artificial intelligence applications, recently, ensemble learning based on deep \nneural networks (ensemble deep learning) has shown significant performances in \nimproving the generalization of learning system. However, since modern deep neural \nnetworks usually have millions to billions of parameters, the time and space overheads \nfor training multiple base deep learners and testing with the ensemble deep learner \nare far greater than that of traditional ensemble learning. Though several algorithms \nof fast ensemble deep learning have been proposed to promote the deployment of \nensemble deep learning in some applications, further advances still need to be made \nfor many applications in specific fields, where the developing time and computing \nresources are usually restricted or the data to be processed is of large dimensionality. \nAn urgent problem needs to be solved is how to take the significant advantages of \nensemble deep learning while reduce the required expenses so that many more \napplications in specific fields can benefit from it. For the alleviation of this problem, it \nis essential to know about how ensemble learning has developed under the era of \ndeep learning. Thus, in this article, we present fundamental discussions focusing on \ndata analyses of published works, methodologies, recent advances and unattainability \nof traditional ensemble learning and ensemble deep learning. We hope this article will \nbe helpful to realize the intrinsic problems and technical challenges faced by future \ndevelopments of ensemble learning under the era of deep learning. \n \n \n \n \n \n \n \n \n                           \nCorrespondence to: Yongquan Yang (remy_yang@foxmail.com) \n \n1 Introduction \nEnsemble learning (Dietterich 2000; Zhou 2009, 2012), a machine-learning \ntechnique that utilizes multiple base learners to form an ensemble learner for the \nachievement of better generalization of learning system, has achieved great success in \nvarious artificial intelligence applications and received extensive attention from the \nmachine learning community. However, with the increase in the number of base \nlearners, the costs for training multiple base learners and testing with the ensemble \nlearner also increases rapidly, which inevitably hinders the universality of the usage of \nensemble learning in many artificial intelligence applications. Especially when deep \nlearning (LeCun et al. 2015) (mostly deep neural networks (He et al. 2016; Szegedy et \nal. 2017; Huang et al. 2017b; Xie et al. 2017; Sandler et al. 2018; Zoph et al. 2018; Tan \nand Le 2021)) is dominating the development of various artificial intelligence \napplications, the usage of ensemble learning based on deep neural networks \n(ensemble deep learning) is facing severe obstacles.  \nSince modern deep neural networks (Rawat and Wang 2017; Liu et al. 2017a; Alam \net al. 2020; Khan et al. 2020) usually have millions to billions of parameters, the time \nand space overheads for training multiple base deep learners and testing with the \nensemble deep learner is far greater than that of traditional ensemble learning. Fast \nensemble deep learning algorithms like Snapshot (Huang et al. 2017a), fast geometric \nensembling (FGE) (Garipov et al. 2018) and stochastic weight averaging (SWA) \n(Izmailov et al. 2018; Maddox et al. 2019) have promoted the deployment of ensemble \ndeep learning in some artificial intelligence applications to some extent. However, due \nto the large expenses compared with traditional ensemble learning, the deployment \nof ensemble deep learning algorithms still needs further advances in some specific \nfields, where the developing time and computing resources are usually restricted (f. e. \nthe field of robotics vision (Yang et al. 2018, 2019)) or the data to be processed is of \nlarge dimensionality (f. e. the field of histopathology whole slide image analysis (Yang \net al. 2020c, b)). In these specific fields, it is sometimes still difficult to deploy the \ntraditional ensemble learning algorithms, which makes the deployment of ensemble \ndeep learning even more challenging. Thus, an urgent problem needs to be solved is \nhow to make full usage of the significant advantages of ensemble deep learning while \nreducing the expenses required for both training and testing so that many more \nartificial intelligence applications in specific fields can benefit from it. \nTo alleviate this problem, it is essential to know about how ensemble learning has \ndeveloped under the era of deep learning. In this article, we achieve this by presenting \ndiscussions with our observations of various existing ensemble learning algorithms \nand related applications. Different from existing review articles (Sagi and Rokach 2018; \nDong et al. 2020) that mostly discussed about traditional ensemble learning, reviews \n(Cao et al. 2020) that particularly discussed ensemble deep learning in bioinformatics, \nor specific technical innovations with ensemble learning introduced, such as GAN-\nEnsembles (Durugkar et al. 2017; Ghosh et al. 2018; Han et al. 2021), this article aims \nto reveal the intrinsic problems and technical challenges of deploying ensemble \nlearning under the era of deep learning to a wider range of specific fields, with more \nfundamental discussions about the developing routes of both traditional ensemble \nlearning and ensemble deep learning. First, through data analysis of published works, \nwe show the prosperity of ensemble learning and the gap between ensemble deep \nlearning and traditional ensemble learning. Second, we discuss the methodologies, \nrecent advances and unattainability of traditional ensemble learning by analyzing \nexisting well-known approaches, applications and giving some of our observations. \nThird, we discuss the methodologies, recent advances and unattainability of ensemble \ndeep learning with analyses and observations in the context of usual and fast solutions. \nFinally, we discuss the whole paper, pointing out some possible future research \ndirections. \n \n2 Data Analysis of Published Works \nIn this section, we present some data analysis of published works to show the \nprosperity of ensemble learning (EL) and the gap between ensemble deep learning \n(EDL) and traditional ensemble learning (TEL).  \n2.1 Prosperity of EL  \nThe research history of EL can be traced back to 1990 (Hansen and Salamon 1990; \nSchapire 1990). Due to its significant advantages in improving the generalization of \nlearning systems, the research on the theoretical algorithms and applications of EL has \nalways been a research hotspot. Dietterich (Dietterich 1997), an authoritative scholar \nin the field of machine learning, once listed EL as the first of four research directions \nin the field of machine learning in AI Magazine. In the past 30 years, the research of EL \nhas made remarkable progress. As shown in Figure 1, the number of papers related to \nthe topic of ‘ensemble learning’ and published in the core set of Web of Science from \n1990 to 2019 has been increasing steadily year by year, which reflects an exponentially \ngrowing prosperity of EL.  \n \n \nFigure. 1. The number of papers published in the core set of Web of Science from 1990 to 2019 for \nthe topic of ‘ensemble learning’.  \n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n 'ensemble learning'\nNumber of papers\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2.2 Gap between EDL and TEL \nThe research history of ensembles of neural networks can be traced back to the \nearly research stage of ensemble learning in 1990 (Hansen and Salamon 1990). \nHowever, the research progress of ensembles of deep neural networks (ensemble \ndeep learning, EDL) had been very slow before 2014. Later benefited from the great \nsuccess of deep neural networks with popularization of big data and high-performance \ncomputing resources, the research progress of EDL began to sprout rapidly. Figure 2 \nshows the comparison between the number of papers related to ‘ensemble deep \nlearning’ or ‘ensemble learning’ not ‘deep’ (traditional ensemble learning, TEL) and \npublished in the core set of Web of Science from 1990 to 2019. It can be noted from \nFigure 2 that a large gap exists between EDL and TEL. Compared with TEL, the \ndevelopment of EDL is severely lagging behind. The primary reason for this \nphenomenon is that modern deep neural networks (Rawat and Wang 2017; Liu et al. \n2017a; Alam et al. 2020; Khan et al. 2020) usually have parameters ranging from \nmillions to billions which makes the time and space overheads for the training and \ntesting stages of EDL much greater than that of TEL, which severely hinders the \ndeployment of EDL in various artificial intelligence applications in specifical fields. \n \n \nFigure 2. The comparison between the number of papers published in the core set of Web of \nScience from 1990 to 2019 for the topic of ‘traditional ensemble learning’ (’ensemble learning’ not \n‘deep’) and the topic of ‘ensemble deep learning’. \n \n3 Traditional Ensemble Learning \nTraditional ensemble learning (TEL) has been playing a major role in the research \nhistory of ensemble learning (EL). In this section, starting with the paradigm of usual \nmachine learning (UML), we respectively present the methodology of TEL, well-known \nimplementations for the methodology of TEL, recent advances of TEL and \nunattainability of TEL.  \n3.1 Preliminary \n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n 'ensemble learning' not 'deep'\n'ensemble deep learning'\nNumber of papers\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\nLet us first consider the paradigm of usual machine learning (UML), where a raw \ndata set 𝐷= {𝑑1, ⋯, 𝑑𝑛} and its corresponding target set 𝑇= {𝑡1, ⋯, 𝑡𝑛} are given. \nSpecifically, 𝑑𝑛 is a raw data point of 𝐷 and 𝑡𝑛 is the target corresponding to 𝑑𝑛. \nIn the UML paradigm, two components are normally essential: 1) feature extraction \nwhich converts raw data points into corresponding learnable representations; and 2) \nmodel development which evolves a learner that can map representations into \ncorresponding targets.  \nThe feature extraction consists of a number of extracting methods (Guyon and \nElisseeff 2006), and a converting procedure. An extracting method is responsible for \nextracting a type of representations from raw data points; and the converting \nprocedure is responsible for incorporating the results of extracting methods into \nlearnable representations for the whole raw data set. The model development \ncomponent consists of a learning algorithm, a learning strategy, and an evolving \nprocedure. The learning algorithm is responsible for the construction and optimization \nof a learner; the learning strategy is responsible for configurating rules to carry out the \nevolving procedure; and the evolving procedure is responsible for, under the learning \nstrategy, updating the parameters of the learner to be appropriate for mapping \nrepresentations into corresponding targets. The outline for the methodology of UML \nis shown as Figure. 3. \n \n \nFigure. 3. The methodology of usual machine learning (UML), which consists of feature extraction \nand model development. \n \nFormally, let 𝐸𝑀= {𝑒𝑚1(∗; 𝜃1\n𝑒𝑚), ⋯, 𝑒𝑚𝑚(∗; 𝜃𝑚\n𝑒𝑚)}  denote various extracting \nmethods. Particularly,  𝑒𝑚𝑚(∗; 𝜃𝑚\n𝑒𝑚)  signifies an extracting method 𝑒𝑚𝑚(∗) \nparameterized by 𝜃𝑚\n𝑒𝑚. The feature extraction component can be expressed as \n𝐹= 𝐹𝑒𝑎𝑡𝑢𝑟𝑒𝐸𝑥𝑡𝑟𝑎𝑐𝑡𝑖𝑜𝑛(𝐷, 𝐸𝑀 ) = {𝑓1, ⋯, 𝑓𝑛}. \nThereinto, 𝑓𝑛 , which signifies the representation of 𝑑𝑛 , is constructed by the \nconverting procedure that can be more specifically expressed as \n𝑓𝑛= 𝑐𝑜𝑛𝑣𝑒𝑟𝑡𝑖𝑛𝑔(𝑑𝑛, 𝐸𝑀) \n= {𝑒𝑚1(𝑑𝑛; 𝜃1\n𝑒𝑚), ⋯, 𝑒𝑚𝑚(𝑑𝑛; 𝜃𝑚\n𝑒𝑚)} \n= {𝑥𝑛,1, ⋯, 𝑥𝑛,𝑘}     𝑠. 𝑡.  𝑘≥𝑚. \nFormally, let 𝐿𝐴= {𝑙(∗; 𝜃𝑙), 𝑜𝑝𝑡(∗,∗; 𝜃𝑜𝑝𝑡)} denote the learning algorithm and \nevolving\nlearning algorithm\nfeature extraction\nmodel development\nfeatures\nlearner\nlearning strategy\ntraining data\nS\nS\nconverting\n…\nextracting methods\n𝐿𝑆= {𝑙𝑠(𝜃𝑙𝑠)}  denote the learning strategy. Specifically, 𝑙(∗; 𝜃𝑙)  signifies the \nconstruction of a learner 𝑙(∗) parameterized by 𝜃𝑙 and 𝑜𝑝𝑡(∗,∗; 𝜃𝑜𝑝𝑡) signifies the \nlearner’s optimization procedure 𝑜𝑝𝑡(∗,∗) parameterized by 𝜃𝑜𝑝𝑡 for updates of 𝜃𝑙, \nand 𝑙𝑠(𝜃𝑙𝑠)  signifies a learning strategy 𝑙𝑠  parameterized by 𝜃𝑙𝑠 . Note, here \n𝑜𝑝𝑡(∗,∗) implicitly consists of an objective function constructed for classification or \nregression and corresponding optimization of the objective function. The model \ndevelopment component can be expressed as \n𝐿= 𝑀𝑜𝑑𝑒𝑙𝐷𝑒𝑣𝑒𝑙𝑜𝑝𝑚𝑒𝑛𝑡(𝐹, 𝑇, 𝐿𝐴, 𝐿𝑆) = {𝑙(∗; 𝜃𝑢\n𝑙)}. \nThereinto, 𝑙(∗; 𝜃𝑢\n𝑙) signifies the learner 𝑙(∗) parameterized by 𝜃𝑢\n𝑙, which is updated \nby the evolving procedure that can be more specifically expressed as  \n𝜃𝑢\n𝑙= 𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐹, 𝑇, 𝑙(∗; 𝜃𝑙), 𝑜𝑝𝑡(∗,∗; 𝜃𝑜𝑝𝑡) | 𝑙𝑠(𝜃𝑙𝑠)) \n= 𝑎𝑟𝑔𝑜𝑝𝑡\n𝜃𝑙(𝑙(𝐹; 𝜃𝑙), 𝑇; 𝜃𝑜𝑝𝑡 | 𝑙𝑠(𝜃𝑙𝑠)). \nThe developed learner 𝑙(∗; 𝜃𝑢\n𝑙) forms the mapping between representations 𝐹 and \ncorresponding targets 𝑇. \n \nAt testing, given a test raw data point 𝑑𝑡𝑒𝑠𝑡 , the corresponding target 𝑡𝑡𝑒𝑠𝑡 \npredicted by the evolved learner can be formally expressed as follows \n𝑓𝑡𝑒𝑠𝑡= {𝑒𝑚1(𝑑𝑡𝑒𝑠𝑡; 𝜃1\n𝑒𝑚), ⋯, 𝑒𝑚𝑚(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑚\n𝑒𝑚)}, \n𝑡𝑡𝑒𝑠𝑡= 𝑙(𝑓𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑙). \n \nNote, in the expressions of this subsection, each 𝜃  denotes the parameters \ncorresponding to the implementation of respective expression. \n3.2 Methodology of TEL \nThe paradigm of traditional ensemble learning (TEL) and the paradigm of UML \nprimarily differ in the model development. TEL improves UML by replacing the model \ndevelopment of UML with generating base learners and forming ensemble learner. \nGenerating base learners is responsible for evolving multiple base learners that have \ndiversity in mapping the extracted representations into corresponding targets. \nForming ensemble learner is responsible for integrating the base leaners into an \nensemble leaner that can achieve better generalization. \nWith appropriate learning algorithms (𝐿𝐴) and learning strategies (𝐿𝑆), generating \nbase learners can be roughly divided into two categories: one is to use different types \nof learning algorithms to generate 'heterogeneous' base learners; and the other is to \nuse the same learning algorithm to generate 'homogeneous' base learners. Forming \nensemble learner consists of an ensembling criteria and an integrating procedure. An \nensembling criteria is responsible for the construction and configuration of an \nensemble learner. With the ensembling criteria, an integrating procedure is \nresponsible for forming the final ensemble learner which is appropriate for mapping \nfrom predictions of base learners into corresponding targets. The outline for the \nmethodology of TEL is shown as Figure. 4. \nFormally, generating base learners can be expressed as \n𝐿𝑏= 𝐵𝑎𝑠𝑒𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛(𝐹, 𝑇, 𝐿𝐴, 𝐿𝑆) = {𝑙1(∗; 𝜃𝑢\n𝑙1), ⋯, 𝑙𝑏(∗; 𝜃𝑢\n𝑙𝑏)}. \nThereinto, 𝑙𝑏(∗; 𝜃𝑢\n𝑙𝑏) signifies a base learner 𝑙𝑏(∗) parameterized by 𝜃𝑢\n𝑙𝑏, which is \nupdated by the evolving procedure. More specifically, the details of generating \n'heterogeneous' base learners can be expressed as follows \n𝐿𝐴= {{𝑙1(∗; 𝜃𝑙1), 𝑜𝑝𝑡1(∗,∗; 𝜃𝑜𝑝𝑡1)}, ⋯, {𝑙𝑏(∗; 𝜃𝑙𝑏), 𝑜𝑝𝑡𝑏(∗,∗; 𝜃𝑜𝑝𝑡𝑏)} }, \n𝐿𝑆= {𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,1(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,1), ⋯, 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)}, \n𝜃𝑢\n𝑙𝑏= 𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐹, 𝑇, 𝑙𝑏(∗; 𝜃𝑙𝑏), 𝑜𝑝𝑡𝑏(∗,∗; 𝜃𝑜𝑝𝑡𝑏) | 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)) \n= 𝑎𝑟𝑔𝑜𝑝𝑡\n𝜃𝑙𝑏\n(𝑙𝑏(𝐹; 𝜃𝑙𝑏), 𝑇; 𝜃𝑜𝑝𝑡𝑏 | 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)) . \nAnd, the details of generating 'homogeneous' base learners can be expressed as \nfollows \n𝐿𝐴= {{𝑙0(∗; 𝜃𝑙0), 𝑜𝑝𝑡0(∗,∗; 𝜃𝑜𝑝𝑡0)}}, \n𝐿𝑆= {𝑙𝑠ℎ𝑜𝑚𝑜,1(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,1), ⋯, 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)}, \n𝑙𝑏(∗) = 𝑙0(∗), \n𝜃𝑢\n𝑙𝑏= 𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐹, 𝑇, 𝑙0(∗; 𝜃𝑙0), 𝑜𝑝𝑡0(∗,∗; 𝜃𝑜𝑝𝑡0) | 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)) \n= 𝑎𝑟𝑔𝑜𝑝𝑡\n𝜃𝑙0\n(𝑙0(𝐹; 𝜃𝑙0), 𝑇; 𝜃𝑜𝑝𝑡0 | 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)). \n \n \nFigure. 4. The methodology of traditional ensemble learning (TEL), which consists of feature \nextraction, generating base learners and forming ensemble learner. In generating base learners, \nthe learning strategy for evolving heterogeneous base learners is usually simpler than the learning \nstrategy for evolving homogeneous base learners. In forming ensemble learner, building \nappropriate ensembling criteria is the key point to form the ensemble learner. \n \nLet 𝐸𝐶= {𝑙𝑒(∗; 𝜃𝑙𝑒), 𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑐𝑜𝑛𝑓)}  denote the ensembling criteria for \nforming ensemble learner, and 𝑇̃ = {𝑡1̃, ⋯, 𝑡𝑛̃ }, 𝑡𝑛̃ = {𝑙1(𝑓𝑛; 𝜃𝑢\n𝑙1), ⋯, 𝑙𝑏(𝑓𝑛; 𝜃𝑢\n𝑙𝑏)} \nsignify the predictions of the base learners. Specifically, 𝑙𝑒(∗; 𝜃𝑙𝑒)  signifies the \nconstruction of an ensemble learner 𝑙𝑒(∗)  parameterized by 𝜃𝑙𝑒  and 𝑐𝑜𝑛𝑓(∗,∗\n…\n…\n…\nor\nevolving\nevolving\nlearning\nalgorithms\nensemble learner\nS\nor\nfeature extraction\ngenerating base learners\nforming ensemble learner\nfeatures\nbase learners\nlearning\nstrategies\nS\ntraining data\nS\nS\n…\n…\nconverting\n…\nheterogeneously\nhomogeneously\nintegrating\nensembling\ncriterion\nextracting \nmethods\n; 𝜃𝑐𝑜𝑛𝑓) signifies the ensemble learner’s configuration 𝑐𝑜𝑛𝑓(∗,∗) parameterized by \n𝜃𝑐𝑜𝑛𝑓 for updates of 𝜃𝑙𝑒. Forming ensemble learning can be formally expressed as \n𝐿𝑒= 𝐸𝑛𝑠𝑒𝑚𝑏𝑙𝑒𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐹𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛(𝑇̃, 𝑇, 𝐸𝐶) = {𝑙𝑒(∗; 𝜃𝑢\n𝑙𝑒)}. \nThereinto, 𝑙𝑒(∗; 𝜃𝑢\n𝑙𝑒)  signifies an ensemble learner 𝑙𝑒(∗)  parameterized by 𝜃𝑢\n𝑙𝑒 , \nwhich is configured by the integrating procedure. More specifically, the integrating \nprocedure can be expressed as \n𝜃𝑢\n𝑙𝑒= 𝑖𝑛𝑡𝑒𝑔𝑟𝑎𝑡𝑖𝑛𝑔(𝑇̃, 𝑇, 𝑙𝑒(∗; 𝜃𝑙𝑒), 𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑐𝑜𝑛𝑓)) \n= 𝑎𝑟𝑔𝑐𝑜𝑛𝑓\n𝜃𝑙𝑒\n(𝑙𝑒(𝑇̃; 𝜃𝑙𝑒), 𝑇; 𝜃𝑐𝑜𝑛𝑓). \nAt testing, given a test raw data point 𝑑𝑡𝑒𝑠𝑡 , the corresponding target 𝑡𝑡𝑒𝑠𝑡 \npredicted by the formed ensemble learner can be expressed formally as follows \n𝑓𝑡𝑒𝑠𝑡= {𝑒𝑚1(𝑑𝑡𝑒𝑠𝑡; 𝜃1\n𝑒𝑚), ⋯, 𝑒𝑚𝑚(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑚\n𝑒𝑚)}, \n𝑡̃𝑡𝑒𝑠𝑡= {𝑙1(𝑓𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑙1), ⋯, 𝑙𝑏(𝑓𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑙𝑏)}, \n𝑡𝑡𝑒𝑠𝑡= 𝑙𝑒(𝑡̃𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑙𝑒). \n3.3 Well-known implementations for methodology of TEL \nSince most existing well-known TEL approaches primarily focus on generating base \nlearners and forming ensemble learner, here we weaken the descriptions for feature \nextraction (Guyon and Elisseeff 2006) since it is beyond the topic of the methodology \nof TEL. Particularly in discussion of generating base learners, we primarily elaborate \non various learning strategies, as discussion of machine learning algorithms is a topic \nmore appropriate in UML. The discussed well-known learning strategies and \nensembling criteria for generating base learners and integrating base learners are \nsummarized in Table. 1 and Table. 2. \n \nTable. 1. Various well-known learning strategies (LS) for generating base learners \nCategory \nSpecific LS \nManipulating at data level \nSampling data points \nBagging (Breiman 1996) \nDagging (Ting and Witten 1997)  \nWeighting data points \nBoosting (Freund and Schapire 1997)  \nWagging (Bauer and Kohavi 1999)  \nMultiboosting (Webb 2000) \nManipulating at feature level \nRandom Subspace (Ho 1998) \nManipulating at both data and feature levels \nRandom Forests (Breiman 2001)  \nRotation Forests (Rodríguez et al. 2006)  \nOthers \nNegative correlation learning (Liu and Yao 1999)  \nANN-based random initialization (Hansen and Salamon 1990)  \n \nTable. 2. Various well-known ensembling criteria (EC) for forming ensemble learner  \nCategory \nSpecific EC \nWeighting methods \nMajority voting / averaged summarization (Mendes-Moreira et al. \n2012) \nMinimizing the objective (Mao et al. 2015) \nMeta-learning methods  \nStacking (Wolpert 1992) \nMixture of expert (Masoudnia and Ebrahimpour 2014) \nWeighting-based learning (Omari and Figueiras-Vidal 2015) \nEnsemble selection (Zhou et al. 2002) \nmethods (many could be better than \nall) \nClustering based criteria (Bakker and Heskes 2003) \nRanking-based criteria (Martinez-Muñoz et al. 2009) \nSelection based criteria (Dos Santos et al. 2008; Hernández-Lobato et \nal. 2009) \n \n3.3.1 Base learner generation \nIt is generally believed that the base learners used for ensemble should have \nsufficient diversity and the prediction of a single base learner should be as accurate as \npossible (Granitto et al. 2005; Zhou 2009, 2012). While generating multiple base \nlearners, the learning algorithm and the learning strategy have to be considered. The \nlearning algorithm used to generate the base learners can be a decision tree (DT) \n(Quinlan 1986), artificial neural network (ANN) (Rumelhart et al. 1986), support vector \nmachine (SVM) (Chang and Lin 2011) or related variants. The used learning algorithm \nbasically determines the upper limit of the predictive performance of the base learners. \nGenerally, the predictive performance of the base learners is required to be at least \nbetter than random prediction, because, theoretically, a number of random \npredictions cannot be integrated to produce a well-regular prediction. The learning \nstrategy that ensures the diversity of the base learners plays a decisive role in \nconstructing the ensemble learner, especially when the accuracy of the base learners \nreaches certain limit. This is because, theoretically, a better ensemble learner cannot \nbe summarized from a number of identical base learners. Usually, the learning strategy \nfor generating base learners can be heterogeneous or homogeneous. When the \nlearning strategy employs different learning algorithms to generate ‘heterogeneous’ \nbase learners, the diversity of the base learners can be well ensured with very simple \nlearning strategy like independently learning, because good diversity can exist among \nthe base learners via the discrepancies of their diverse structures in addition to \ndifferent parameters. However, when the learning strategy uses the same learning \nalgorithm to generate ‘homogeneous’ base learners, the learning strategy to ensure \nthe diversity of the base learners is the key point to success, because the diversity of \nthe base learners is only kept in discrepancies of their parameters as they enjoy the \nsame structure. Theoretically, both heterogeneous learning strategy and \nhomogeneous learning strategy can be used at any time to generate base learners. \nHowever, the homogeneous learning strategy is the most commonly used, due to the \nfact that it only requires one learning algorithm to be implemented in practice. Thus, \nin the following contents of this subsection, we discuss various learning strategies for \nthe diversity of ‘homogeneous’ base learners, respectively in the context of \nmanipulating at data level, manipulating at feature level, manipulating at both data \nand feature levels, and others, which are summarized in Table 1.  \n(1) Manipulating at data level \nBreiman et al. (Breiman 1996) and Freund et al. (Freund and Schapire 1997), \nensured the diversity of the base learners by sampling or weighting the training set \nmultiple times to generate new training sets for training different base learners. The \ntwo learning strategies are known as Bagging (Breiman 1996) and Boosting (Freund \nand Schapire 1997). Bagging uses the simple random sampling to generate new data \nsets for the training of base learners. A known variant of Bagging is Dagging (Ting and \nWitten 1997), which generates disjoint data sets by random sampling without \nreplacement. Boosting starts with equal weights assigned to the samples of the \ntraining data and reweights the samples of the training data referring to the \nperformance of previous base learner to focus on difficult data points. A known variant \nof Bagging is Wagging (Bauer and Kohavi 1999), which assigns random weights to the \nsamples in the training data using Gaussian distribution or Poisson distribution. \nCombining Bagging and Wagging, Webb (Webb 2000) proposed Multiboosting, which \nfirstly uses Boosting to assign weights to the samples in the training data, and then \napplies Wagging to update the weights of the assigned samples.  \n(2) Manipulating at feature level \nDifferent from manipulating at the training data level (Breiman 1996; Freund and \nSchapire 1997; Ting and Witten 1997; Bauer and Kohavi 1999; Webb 2000), Ho (Ho \n1998) upgraded the manipulation at data level to feature level (aka, Random Subspace) \nby sampling the features extracted from the training data and then using the sampled \nfeatures as the input of the learning algorithm to generate various base learners.  \n(3) Manipulating at both data and feature levels \nThe two learning strategies of manipulating at data or feature level significantly \npromoted the development of ensemble learning. By combining the advantages of \nthem, Breiman et al. (Breiman 2001) constructed a learning strategy by sampling the \ntraining data as well as the features extracted from the training data at the same time, \nand proposed the famous random forests (RF) ensemble learning approach, which is \nstill widely used in various application fields. Later, Rodriguez et al. (Rodríguez et al. \n2006) proposed rotation forests (RoF), the learning strategy of which first randomly \nsplits the training features into several disjoint subsets and then principle component \nanalysis (PCA) (Wold et al. 1987) is applied to each subset and axis rotation is employed \nto form new features for the training of base learners.  \n(4) Others \nAnother known learning strategy that can effectively ensure the diversity of base \nlearners is to encourage negative correlation in the predictive error of base learners \nby complementary learning of the training data (Liu and Yao 1999). Besides, when \nartificial neural network (ANN) (Rumelhart et al. 1986) is employed as the learning \nalgorithm, a commonly used learning strategy to obtain diverse base learners is \ntraining the ANN with different random initializations, which tends to differentiate the \nerrors of the base learners (Hansen and Salamon 1990). \n3.3.2 Ensemble learner formation \nTo form the ensemble learner from the based learners, an appropriate ensembling \ncriterion is the key to success. Basically, known ensembling criteria for the ensemble \nlearner formation can be roughly divided into three categories: weighting methods, \nmeta-learning methods, and ensemble selection methods. In the following contents \nof this subsection, we respectively discuss these three categories, which are \nsummarized in Table 2, and summarize corresponding usage.  \n(1) Weighting methods \nThe essence of the weighting methods for ensembling criteria is to combine the \noutputs of the base learners by assigning different weights to obtain better predictions. \nThe majority voting rule commonly used for classification problem is the simplest \nimplementation of the weighting method. The class with the most votes from the base \nlearners will be taken as the final output of the ensemble learner. As for regression \nproblem, the outputs of the base learners can be averaged (Mendes-Moreira et al. \n2012) to produce the final output of the ensemble learner. Another commonly used \nweighting method is to assign the weight of a base learner in the ensemble learner \naccording to its predictive ability on a validation data set. In addition, Mao et al. (Mao \net al. 2015) used the error between the weighted output of multiple base learners and \nthe true value as the objective function, where the weights of the base learners are \nsubject to ∑𝜔𝑖= 1  and −1 < 𝜔𝑖< 1 , and minimized the objective function to \ndetermine the weights of the base learners for the ensemble learner. Weighting \nmethods are simple to use, but they may not be able to fully exploit the information \nof the base learners, especially when large amount of data is available. \n(2) Meta-learning methods  \nUnlike the weighting methods which simply assign different weights to the base \nlearners, the fundamental idea of meta-learning methods for ensembling criteria is by \nlearning from the meta-knowledge that has been learnt by the base learners (i.e., the \npredictions of the base learners) and corresponding targets to further reduce the \ngeneralization error. Via learning to map the predictions of the base learners into \ncorresponding targets, meta-learning methods can further reduce the generalization \nerror when large amount of data is available, by fully exploiting the information of the \nbase learners to overcome the limitation of weighting methods. As the essence of \nmeta-learning methods follows the paradigm of machine learning, they are \nappropriate to form ensemble learners for both classification and regression problems. \nIn 1992, Wolpert (Wolpert 1992) proposed the first meta-learning method Stacking to \nintegrate multiple base learners. A popular variant of the Stacking method is the \nmixture of experts (ME) (Masoudnia and Ebrahimpour 2014), which first uses a divide-\nand-conquer strategy to split the problem into multiple sub-problems, and \nrespectively trains an expert (base learner) for each sub-problem, and then use \nanother machine learning algorithm to learn from the outputs of the base learners \nand corresponding ground-truths to further reduce the generalization error. \nCombining the advantages of both the weighting method and the meta-learning \nmethod, Omari and FIgueiras-Vidal (Omari and Figueiras-Vidal 2015) first weighted the \noutputs of multiple base learners to obtain an initial ensemble, and then used another \nmachine learning algorithm to learn from the initial ensemble to further reduce the \ngeneralization error. Meta-learning methods can fully exploit the information from the \nbase learners, but it will take much more expenses to evolve the ensemble criteria, \nespecially when the number of base learners is relatively large. \n(3) Ensemble selection methods \nIn the early research stage of ensemble learning, most ensembling criteria used \nall the generated base learners to form an ensemble learner. Although this simple \nselection strategy can obtain an ensemble learner that is significantly better than a \nsingle base learner with appropriate ensembling criterion, the prediction time and \nstorage space of the ensemble learner increased rapidly with the increase in the \nnumber of base learners. To alleviate this problem, in 2002, Zhou et al (Zhou et al. \n2002) proposed the concept of ensemble selection for the first time, affirming that \nusing a small number of base learners can as well achieve better ensemble \nperformance. This caused a strong focus in the community of ensemble learning and \nopened a new research direction of selective ensemble learning (Zhang and Zhang \n2011). The strategies for selecting the base learner can be roughly divided into: \nclustering-based methods, ranking-based methods, and selection-based methods. The \nclustering-based method needs to consider how to measure the similarity between \nthe prediction results of two base learners (subsets), which clustering algorithm to \nchoose, and how to determine the number of subsets for the base learners (Bakker \nand Heskes 2003). The ranking-based method first uses a certain measurement (such \nas accuracy) to rank the base learners, and then uses a suitable stopping criterion to \nselect a number of base learners for ensemble (Martinez-Muñoz et al. 2009). The \nselection-based method leverages a certain selection strategy, such as greedy strategy \n(Jungnickel 1999) or dynamic programming (Kumar 2010), to select only a proportion \nof the base learners to participate in integrating (Dos Santos et al. 2008; Hernández-\nLobato et al. 2009). Ensemble selection methods can favorably reduce the prediction \ntime and storage space of the ensemble learner, but it may not achieve the optimal \nsolution by ignoring the information of some base learners.   \n(4) Summarization of usage \nIn summary, there are no stationary rules for deciding which ensembling criterion \nis better than others. One has to construct the appropriate ensembling criterion by \nreferring to specific situations. Theoretically, weighting methods can be used without \nany prerequisites. However, weighting methods may not be able to fully exploit the \ninformation of the base learners when large amount of collected data is available. In \nthis situation, meta-learning methods are better than weighting methods. In addition, \nwhen the efficiency of the ensemble learner is a key factor to be considered, ensemble \nselection methods should also be united with weighting methods or meta-learning \nmethods for a more appropriate ensembling criterion. \n3.4 Recent advances of TEL \n3.4.1 Material collection \nApart from the well-known implementation of TEL, in recent years, a lot of TEL \nadvances have been proposed for various artificial intelligence applications (Behera \nand Mohanty 2019; Mohapatra et al. 2021). As the number of proposed TEL advances \ncan reach up to hundreds or thousands in a single year, it is quite difficult for a survey \nto contain all of these advances. Thus, we propose to review a number of highly cited \nrecent TEL advances and summarize corresponding key innovations to show some \nunderlying trends of recent development of TEL. That highly cited recent advances are \nselected is based on one of the assumptions behind the PageRank (Page et al. 1998). \nIn the case of this paper, that is the more citations a paper gets the more people agree \nwith the approach presented in the paper. Thus, we consider summarizations from the \nreview of the selection of highly cited recent TEL advances can more appropriately and \nstably reflect the primary underlying trends in recent development of TEL than the \ncollection of all the recent TEL advances. The selected works for review of recent TEL \nadvances were suggested by searching on Web of Science (accessed at February 3, \n2021) with the filtering rules including being published in the last ten years and being \nhighly cited or being a hotspot. \n3.4.2 Review \nReferring to the methodology of TEL presented in Figure. 4, but different from the \ndiscussion in Section 3.1 that focused more on learning strategies for generating base \nlearners and ensembling criteria for forming ensemble learner, we add additional \ndescriptions of extracting methods for feature representations as well as depictions of \nlearning algorithms for generating base learners, since they also provide essential basis \nfor TEL approaches to be carried out in specific applications. \nFor the 3D human action recognition, Wang et al. (Wang et al. 2014) proposed an \nensemble learning approach called actionlet ensemble. Actionlet was defined as a \nconjunction of the features for a subset of the joints of the human skeleton. The 3D \njoint position feature (3D-JPF) and local occupancy patter (LOP) were extracted from \nthe training images, and further transformed into temporal patterns for training via \nthe proposed Fourier temporal pyramid (FTP) (Wang et al. 2014). Multiple actionlet \nclassifiers based on an support vector machine (SVM) (Chang and Lin 2011) were \ntrained with a proposed data mining strategy which aims to discover discriminative \nactionlet classifiers on the training dataset. A convex linear combination of the pre-\ntrained multiple actionlet classifiers were proposed to form the final actionlet \nensemble classifier. \nFor the prediction of protein-protein interactions, You et al. (You et al. 2013) \nemployed auto covariance (AC) (Guo et al. 2008), conjoint triad (CT) (Shen et al. 2007), \nlocal descriptor (LD) (Davies et al. 2008) and Moran autocorrelation (MA) (Xia et al. \n2009) to extract features from protein sequences. The extracted features were used \nto train multiple base learners based on a single hidden layer feed-forward neural \nnetworks, extreme learning machine (ELM) (Ding et al. 2015; Huang et al. 2015), with \ndifferent random initializations. Majority voting was employed to integrate the trained \nmultiple base learners into the final ensemble learner.  \nFor the identification of DNase I hypersensitive sites, Liu et al. (Liu et al. 2016a) \nproposed an ensemble learning algorithm entitled iDHS-EL. Multiple base learners \nbased on the random forests (RF) algorithm (Breiman 2001) were trained using three \ntypes of features including Kmer (Lee et al. 2011), reverse complement Kmer (RC-Kmer) \n(Gupta et al. 2008) and pseudo dinucleotide composition (PseDNC) (Chen et al. 2013) \nthat reflect the characteristics of a DNA sequence from different angles. The final \nensemble learner was formed by employing the grid search strategy to integrate the \npre-trained multiple base learners. For a similar task of identifying N6-\nmethyladenosine sites, Wei et al. (Wei et al. 2018) proposed an ensemble learning \nalgorithm named M6APred-EL. Three feature representation algorithms, including \nPS(kmer)NP, PCPs (Liu et al. 2016b), and RFHC-GACs (Chen et al. 2016), were used to \nextract diverse features from RNA sequences. The extracted features were respectively \nemployed to train SVM (Chang and Lin 2011) for multiple base predictors to identify \nN6-methyladenosine sites. The obtained multiple base predictors were integrated by \nmajority voting to form the ensemble predictor. \nFor the landslide susceptibility mapping, Chen et al. (Chen et al. 2017) proposed \nseveral ensembles using multiple base learners that were heterogeneously trained \nwith artificial neural network (ANN) (Rumelhart et al. 1986), maximum entropy \n(MaxEnt) (Phillips et al. 2004) and SVM (Chang and Lin 2011). Based on previous \nstudies, eleven landslide conditioning factors such as elevation, slope degree, aspect, \nprofile and plan curvatures, topographic wetness index (TWI), distance to roads, \ndistance to rivers, normalized difference vegetation index (NDVI), land use, and \nlithology were selected for training. The proposed ensembles include ANN-SVM, ANN-\nMaxEnt, ANN-MaxEnt-SVM, and SVM-MaxEnt which were integrated by testing \ndifferent basic mathematics (multiplication, division, addition and subtraction). For \nthe same task, Pham et al. (Pham et al. 2017) assessed the application of the ensemble \nlearning technique in the landslide problem, constructing several ensemble learners \nby combining the base classifier of multilayer perceptron (MLP) (Rosenblatt 1958) with \nBagging (Breiman 1996), Dagging (Ting and Witten 1997), Boosting (Freund and \nSchapire 1997), Multiboosting (Webb 2000) and Rotation Forests (Rodríguez et al. \n2006). The factors used for training were selected based on topography, physic-\nmechanical properties, locations and meteorology. The majority voting was employed \nto form the ensemble learner. In addition, Dou et al. (Dou et al. 2020) also constructed \nthree types of ensemble learners for this task, combining SVM (Chang and Lin 2011) \nrespectively with Bagging (Breiman 1996), Boosting (Freund and Schapire 1997) and \nStacking (Wolpert 1992) ensembling strategies. The factors used for training were \nselected by referring to previous literatures, and the ensembling criteria employed to \nform the ensemble learner include majority voting and meta-learning. \nFor the short-term load forecasting, Li et al. (Li et al. 2020) proposed an ensemble \nlearning algorithm based on wavelet transform (Daubechies and Bates 1993), extreme \nlearning machine (ELM) (Ding et al. 2015; Huang et al. 2015) and partial least squares \nregression (PLSR) (Geladi and Kowalski 1986). Various wavelet transform specifications \nwere used to generated different types of features from input load series. Each type \nof the generated features was individually leveraged to train multiple predictors based \non extreme learning machine (ELM) (Ding et al. 2015; Huang et al. 2015). PLSR, which \ntackle the high degree of correlation between the individual forecast, was used to \nweight the multiple predictors to establish an accurate ensemble forecast. \nFor the multi-class imbalance learning, Bi et al. (Bi and Zhang 2018) proposed an \nensemble learning algorithm named diversified error correcting output codes (DECOC) \nby introducing the error correcting output codes (ECOC) (Dietterich and Bakiri 1995) \ninto the ensemble learning algorithm diversified one-against-one (DOVO) (Kang et al. \n2015). ECOC is a decomposition strategy that builds a codeword for each class by \nmaximizing the distance (such as Hamming distance) between various classes and, by \nselecting bits from the built codewords, divides the original problem into multiple sub-\nproblems which can be further individually solved via machine learning. In original \nDOVO, one-vs-one (OVO) was employed to divide the original problem into multiple \nsub-problems which are further solved with a variety of learning algorithms. OVO is a \ndecomposition strategy that build a sub-problem by only selecting instances for each \npair of classes from the original data. DECOC is formed by replacing OVO of EOVO with \nECOC, and the diverse learning algorithms that DECOC employs include SVM (Chang \nand Lin 2011), k-nearest neighbor (KNN) (Altman 1992), Logistic Regression (LR) (Cox \n1959), C4.5 (Salzberg 1994), AdaBoost (AB) (Freund and Schapire 1996), Random \nForests (RF) (Breiman 2001), and multilayer perceptron (MLP) (Rosenblatt 1958). The \nobtained multiple based learners were weighted by minimizing the error in favor of \nthe minority classes to form the final ensemble learner. For experiments, 17 public \ndatasets were used, which had clearly defined features, the number of which ranges \nfrom 3 to 128. \nFor the soil moisture forecasting, Prasad et al. (Prasad et al. 2018) proposed and \nensemble learning approach based on ensemble empirical mode decomposition \n(EEMD) (WU and HUANG 2009) and complete ensemble empirical mode \ndecomposition with adaptive noise (CEEMDAN) (Torres et al. 2011). EEMD and \nCEEMDAN are two improved variants of empirical mode decomposition (EMD) (Huang \net al. 1998), which decomposes intact soil moisture time series into several intrinsic \nmode functions (IMFs) to extract instantaneous frequency features. Each of the IMFs \nand residual were separately processed by partial autocorrelation function (PACF) \n(Ramsey 1974) to help selecting features with statistically significant relationship. The \nprocessed IMFs and residual were used to train extreme learning machine (ELM) (Ding \net al. 2015; Huang et al. 2015) and Random Forests (RF) (Breiman 2001) to generate \nmultiple base learners. To form the ensemble learner, the generated multiple base \nlearners were integrated via averaged summation. \nFor the flood susceptibility mapping, Shahabi et al. (Shahabi et al. 2020) proposed \nan ensemble learning algorithm which employed four types of k-nearest neighbor \n(KNN) classifiers as the base learning algorithm and utilized Bagging (Breiman 1996) \nstrategy to generate sub-datasets for training multiple base learners. Ten task-related \nconditioning factors (Rahmati et al. 2016) were chosen for training, including distance \nto river, elevation, slope, lithology, curvature, rainfall, topographic wetness index (TWI), \nstream power index (SPI), land use/land cover, and river density. The employed four \ntypes of KNN classifiers include coarse KNN (Altman 1992) which defines the nearest \nneighbor among all classes as the classifier, cosine KNN (CoKNN) (Hu et al. 2016) which \nuses the cosine distance metric as the nearest neighbor classifier, cubic KNN (CuKNN) \n(Wu et al. 2008) which uses the cubic distance metric as the nearest neighbor classifier, \nand weighted KNN which uses the weighted Euclidean distance as the nearest \nneighbor classifier. The obtained multiple base learners were integrated by majority \nvoting to form the final ensemble learner. \nFor the automatic detection of lung cancer from biomedical dataset, Shakeel et al. \n(Shakeel et al. 2020) employed generalized neural network (GNN) (Behler and \nParrinello 2007) as the learning algorithm and trained multiple base learners using the \nensemble learning process proposed in (Xia et al. 2017), which generated multiple \nbase learners with subsets equally divided from the input features collected by \nexamining lung cancer effects on people’s genetic changes and other impacts. The \nobtained multiple base learners were integrated by majority voting to form the final \nensemble learner. \nFor the hyperspectral image classification, Su et al. (Su et al. 2020) proposed two \ntypes of ensemble learners which employed the tangent space collaborative \nrepresentation classifier (TCRC) (Su et al. 2016) as the learning algorithm and \nrespectively utilized Bagging (Breiman 1996) and Boosting (Freund and Schapire 1997) \nto train multiple base learners. The features used for training were the narrow spectral \nbands captured by hyperspectral images. TCRC is an improved collaborative \nrepresentation classifier (CRC) (Li et al. 2016), the principle of which is that a testing \nsample can be approximated by training samples. CRC classifies a testing sample by \nassigning it to the class whose labeled training samples provide the smallest \nrepresentation residual. TCRC improves CRC by using simplified tangent distance to \ntake advantage of the local manifold in the tangent space of the testing sample (Su et \nal. 2016). The majority voting was employed to form the two types of ensemble \nlearners. \nFor the wind power forecasting, Wang et al. (Wang et al. 2020) proposed a hybrid \napproach named BMA-EL based on Bayesian model averaging (BMA) (Hoeting et al. \n1999; Wasserman 2000) and ensemble learning (EL). For training, the task-related \nfeatures include wind speed, wind direction and ambient temperature. Artificial neural \nnetwork (ANN) (Rumelhart et al. 1986), radial basis function neural network (RBFNN) \n(Borş and Pitas 1996), and SVM were employed as the learning algorithms. For \ngenerating more diverse base learners, the training set were divided into three subsets \nusing clustering of self-organizing map (Vesanto and Alhoniemi 2000) and K-fold cross-\nvalidation (Bishop 2006). ANN, RBFNN and SVM were respectively optimized on the \nthree generated subsets to produce three heterogeneous base learners. BMA was \nutilized to integrate the three base learners and the parameters of the BMA model \nwere optimized on a validation dataset to form the final ensemble learner. \n3.4.3 Summary \n \nThe information of the reviewed recent works of TEL is listed in Table. 3. The \nfeature extraction methods adopted by the TEL approaches proposed in the reviewed \nworks are listed in Table. 4. The learning algorithms and learning strategies employed \nto generate base learners are respectively listed in Table. 5 and Table. 6. The \nensembling criteria utilized to integrate base learners are listed in Table. 7. Finally, \nreferring to Table. 3-7, the key innovations of the TEL approaches proposed in the \nreviewed works are summarized in Table. 8. \n \nTable. 3. The information of the applications in the reviewed recent TEL works (TA) \nApp. Description \nCitations Published Year \nTA1 \n3D human action recognition (Wang et al. 2014) \n433 \n2014 \nTA2 \nPrediction of protein-protein interactions (You et al. 2013) \n239 \n2013 \nTA3 \nIdentification of hypersensitive sites \n(Liu et al. 2016a) \n191 \n2016 \n(Wei et al. 2018) \n91 \n2018 \nTA4 \nLandslide susceptibility mapping \n(Chen et al. 2017) \n176 \n2017 \n(Pham et al. 2017) \n290 \n2017 \n(Dou et al. 2020) \n70 \n2020 \nTA5 \nShort-term load forecasting (Li et al. 2020) \n23 \n2020 \nTA6 \nMulti-class imbalance learning (Bi and Zhang 2018) \n97 \n2018 \nTA7 \nSoil moisture forecasting (Prasad et al. 2018) \n71 \n2018 \nTA8 \nFlood susceptibility mapping (Shahabi et al. 2020) \n45 \n2020 \nTA9 \nDetection of lung cancer (Shakeel et al. 2020) \n43 \n2020 \nTA10 \nHyperspectral image classification (Su et al. 2020) \n26 \n2020 \nTA11 \nWind power forecasting (Wang et al. 2020) \n24 \n2020 \n \nTable. 4. The methods for feature extraction (FE) of the TEL (TFE) approaches proposed in the \nreviewed works \nCode \nDescription \nTFE1 \nTemporal patterns transformed by Fourier temporal pyramid (FTP) (Wang et al. 2014) \nTFE2 \nAuto covariance (Guo et al. 2008) \nTFE3 \nConjoint triad (Shen et al. 2007) \nTFE4 \nLocal descriptor (Davies et al. 2008) \nTFE5 \nMoran autocorrelation (Xia et al. 2009) \nTFE6 \nKmer (Lee et al. 2011) \nTFE7 \nReverse complement Kmer (Gupta et al. 2008) \nTFE8 \nPseudo dinucleotide composition (Chen et al. 2013) \nTFE9 \nPS(kmer)NP (Liu et al. 2016b) \nTFE10 \nPCPs (Liu et al. 2016b) \nTFE11 \nRFHC-GACs (Chen et al. 2016) \nTFE12 \nLandslide related factors (Chen et al. 2017) \nTFE13 \nFactors selected based on various properties of landslide (Pham et al. 2017) \nTFE14 \nFactors selected by referring to previous literatures (Dou et al. 2020) \nTFE15 \nWavelet decompositions (Daubechies and Bates 1993) \nTFE16 \nFactors predefined on public datasets (Bi and Zhang 2018) \nTFE17 \nEEMD (WU and HUANG 2009) / CEEMDAN (Torres et al. 2011) followed with PACF (Ramsey 1974) \nTFE18 \nFlood related factors (Rahmati et al. 2016) \nTFE19 \nFactors of cancer effects (Shakeel et al. 2020) \nTFE20 \nNarrow spectral bands of hyperspectral images (Su et al. 2020) \nTFE21 \nWind power related factors (Wang et al. 2020) \n \nTable. 5. The learning algorithms (LA) of the TEL (TLA) approaches proposed in the reviewed works \nCode \nAbbr. \nDescription \nTLA1 \nSVM \nSupport vector machine (Chang and Lin 2011) \nTLA2 \nELM \nExtreme learning machine (Ding et al. 2015; Huang et al. 2015) \nTLA3 \nRF \nRandom forest (Breiman 2001) \nTLA4 \nANN \nArtificial neural network (Rumelhart et al. 1986) \nTLA5 \nMaxEnt \nMaximum entropy (Phillips et al. 2004) \nTLA6 \nMLP \nMultilayer perceptron (Rosenblatt 1958) \nTLA7 \nKNN \nK nearest neighbor (Altman 1992) \nTLA8 \nLR \nLogistic regression (Cox 1959) \nTLA9 \nC4.5 \nAn improved iterative Dichotomiser 3 (Salzberg 1994) \nTLA10 \nAB \nAdaBoost (Freund and Schapire 1996) \nTLA11 \nCoKNN \nKNN based on cosine distance (Hu et al. 2016) \nTLA12 \nCuKNN \nKNN based on cubic distance (Wu et al. 2008) \nTLA13 \nWeKNN \nKNN based on weighted Euclidean distance \nTLA14 \nGNN \nGeneralized neural network (Behler and Parrinello 2007) \nTLA15 \nTCRC \nTangent space collaborative representation classifier (Su et al. 2016) \nTLA16 \nRBFNN \nRadial basis function neural network (Borş and Pitas 1996) \n \nTable. 6. The learning strategies (LS) of the TEL approaches proposed in the reviewed works \nCode Description \nLS1 \nDiscovering discriminative learner on the training dataset (Wang et al. 2014) \nLS2 \nTraining with randomly initialized parameters on the same dataset \nLS3 \nManipulating at both the data and feature levels with random selections (Breiman 2001) \nLS4 \nTraining multiple learners on the same dataset \nLS5 \nManipulating at the feature level with division \nLS6 \nManipulating at the data level with divide-and-conquer strategy \nLS7 \nManipulating at the data level with Bagging strategy (Breiman 1996) \nLS8 \nManipulating at the data level with Dagging strategy (Ting and Witten 1997) \nLS9 \nManipulating at the data level with Boosting strategy (Freund and Schapire 1997) \nLS10 \nManipulating at the data level with Multioosting strategy (Webb 2000) \nLS11 \nManipulating at both the data and feature level with axis rotation (Rodríguez et al. 2006) \nLS12 \nManipulating at the data level with self-organizing map (Vesanto and Alhoniemi 2000) and cross-\nvalidation (Bishop 2006) \n \nTable. 7. The ensembling criteria (EC) of the TEL (TEC) approaches proposed in the reviewed works \nCode \nDescription \nTEC1 \nLinear combination \nTEC2 \nMajority voting \nTEC3 \nGrid search \nTEC4 \nBasic mathematics \nTEC5 \nStacking (Wolpert 1992) \nTEC6 \nPartial least squares regression (Geladi and Kowalski 1986) \nTEC7 \nMinimizing the error in favor of the minority classes \nTEC8 \nAveraged summation \nTEC9 \nBayesian model averaging (Hoeting et al. 1999; Wasserman 2000) \n \nTable. 8. The key innovations of the TEL approaches proposed in the reviewed works \nApp. FE \nLA&LS \nEC \nLA \nLS \nTA1 \nTFE1 \nTLA1 \nHomo. by LS1 \nW. by TEC1 \nTA2 \nTFE2-5 \nTLA2 \nHomo. by LS2 \nW. by TEC2 \nTA3 \nTFE6-8 \nTLA1 \nHomo. by LS3 \nW. by TEC3 \nTFE9-11 \nTLA3 \nHomo. by LS5 \nW. by TEC2 \nTA4 \nTFE12 \nTLA1,4,5 \nHetero. by LS4 \nW. by TEC4 \nTFE13 \nTLA6 \nHomo. by LS7-10 \nW. by TEC2 \nTFE14 \nTLA7 \nHomo. by LS7,9 \nW. by TEC2 / M. by TEC5 \nTA5 \nTFE24 \nTLA2 \nHomo. by LS5 \nW. by TEC6 \nTA6 \nTFE16 \nTLA1,3,7-10 \nHetero. by LS6 \nW. by TEC7 \nTA7 \nTFE17 \nTLA2,3 \nHomo. by LS5 \nW. by TEC8 \nTA8 \nTFE18 \nTLA7,11-13 \nHetero. / Homo. by LS7 \nW. by TEC2 \nTA9 \nTFE19 \nTLA14 \nHomo. by LS5 \nW. by TEC2 \nTA10 \nTFE20 \nTLA15 \nHomo. by LS7,9 \nW. by TEC2 \nTA11 \nTFE21 \nTLA1,4,16 \nHetero. by LS12 \nW. by TEC9 \n \nBased on these tables, we can summarize: The representations of data employed \nby the proposed TEL approaches of the reviewed works for training mostly were \nvarious hand-crafted features; The learning algorithms of these TEL approaches that \nwere frequently employed are classic SVM, KNN, and neural networks (ELM, ANN, MLP \nand GNN); Manipulating at the data level and the feature level by resampling were the \nfrequently learning strategies used by these proposed TEL approaches; While the \nmajority of these proposed TEL approaches trained homogenous (Homo.) base \nlearners, three of them trained heterogenous (Hetero.) base learners; While the \nmajority of these TEL approaches employed various weighting (W.) methods as \nensembling criteria to form the final ensemble learner, only one of them tried meta-\nlearning (M.) methods; And ensemble selection methods were rarely used, probably \nbecause the number of base learners was not massive enough to employ ensemble \nselection. These summarizations reflect the primary underlying trends for the recent \ndevelopment of TEL. \n3.5 Unattainability \nBased on the primary underlying trends of TEL summarized in the subsection 3.4.3, \nin this subsection, we discuss the unattainability of TEL. While most of recent advances \nof TEL focused on proposing solutions for specific applications based on combinatorial \ninnovations by leveraging existing learning strategies for base learner generation and \nexisting ensembling criteria for ensemble learner formation, few recent advances of \nTEL proposed new learning strategies for base learner generation or new ensembling \ncriteria for ensemble learner formation. It seems that recent advances for the intrinsic \nproblems of TEL have reached to a bottleneck, although TEL is still prosperously \ndeveloping in various applications. Besides, the primary issue associated with the \nmethodology of TEL comes from the nature of UML, which evolves a learner based on \nhand-crafted features which are usually difficult to design and not expressive enough.  \n \n4 Usual Ensemble Deep Learning \nWith the popularization of big data, computing resources, and deep learning (DL, \nmostly deep neural networks (He et al. 2016; Szegedy et al. 2017; Huang et al. 2017b; \nXie et al. 2017; Sandler et al. 2018; Zoph et al. 2018; Tan and Le 2021)), which improves \nthe paradigm of UML, has achieved unprecedented success in the field of machine \nlearning. This will give birth to a huge number of approaches for ensembles of deep \nneural networks (ensemble deep learning, EDL) and promote the research of \nensemble learning into a new era. The usual way to evolve the methodology of EDL \n(usual ensemble deep learning, UEDL) is directly applying DL to the methodology of \nTEL. In this section, starting with the paradigm of deep learning (DL), we respectively \ndiscuss the methodology, recent works and unattainability of UEDL. \n4.1 Preliminary \nDifferent from the paradigm of UML (Fig. 3), the paradigm of DL embeds the \nfeature extraction into model development to form an end-to-end framework, which \nis able to learn task-specifically oriented features that are more expressive when \nmassive training data is available. The paradigm of DL is shown as Fig. 5. \n \n \nFigure. 5. The methodology of deep learning (DL), which embeds the feature extraction into deep \nmodel development to form an end-to-end framework. \n \nFormally, let 𝐷𝐿𝐴= {𝑑𝑙(∗; 𝜃𝑑𝑙), 𝑑𝑜𝑝𝑡(∗,∗; 𝜃𝑑𝑜𝑝𝑡)}  denote the deep learning \nalgorithm and 𝐿𝑆= {𝑙𝑠(𝜃𝑙𝑠)}  denote the learning strategy. Specifically, 𝑑𝑙(∗; 𝜃𝑑𝑙) \nsignifies the construction of a deep learner 𝑑𝑙(∗)  parameterized by 𝜃𝑑𝑙  and \n𝑑𝑜𝑝𝑡(∗,∗; 𝜃𝑑𝑜𝑝𝑡)  signifies the deep learner’s optimization procedure 𝑑𝑜𝑝𝑡(∗,∗) \nparameterized by 𝜃𝑑𝑜𝑝𝑡  for updates of 𝜃𝑑𝑙 . Note, here 𝑑𝑜𝑝𝑡(∗,∗)  implicitly \nconsists of an objective function constructed for classification or regression and \ncorresponding optimization of the objective function. The deep model development \ncomponent can be expressed as \n𝐷𝐿= 𝐷𝑒𝑒𝑝𝑀𝑜𝑑𝑒𝑙𝐷𝑒𝑣𝑒𝑙𝑜𝑝𝑚𝑒𝑛𝑡(𝐷, 𝑇, 𝐷𝐿𝐴, 𝐿𝑆) = {𝑑𝑙(∗; 𝜃𝑢\n𝑑𝑙)}. \nThereinto, 𝑑𝑙(∗; 𝜃𝑢\n𝑑𝑙) signifies the deep learner 𝑑𝑙(∗) parameterized by 𝜃𝑢\n𝑑𝑙, which \nis updated by the deeply evolving procedure that can be more specifically expressed \nas  \n𝜃𝑢\n𝑑𝑙= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙(∗; 𝜃𝑑𝑙), 𝑑𝑜𝑝𝑡(∗,∗; 𝜃𝑑𝑜𝑝𝑡) | 𝑙𝑠(𝜃𝑙𝑠)) \n= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡\n𝜃𝑑𝑙(𝑙(𝐹; 𝜃𝑑𝑙), 𝑇; 𝜃𝑑𝑜𝑝𝑡 | 𝑙𝑠(𝜃𝑙𝑠)). \nThe developed deep learner 𝑑𝑙(∗; 𝜃𝑢\n𝑑𝑙)  forms the mapping between data 𝐷  and \ndeeply evolving\ndeep learning algorithm\ndeep model development\ndeep learner\nlearning strategy\ntraining data\nS\nS\ncorresponding targets 𝑇. Note the data 𝐷 here can be raw data or features extracted \nfrom raw data. \n \nAt testing, given a test data point 𝑑𝑡𝑒𝑠𝑡, the corresponding target 𝑡𝑡𝑒𝑠𝑡 predicted \nby the evolved deep learner can be expressed formally as follows \n𝑡𝑡𝑒𝑠𝑡= 𝑑𝑙(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑑𝑙). \nNote, in the expressions of this subsection, each 𝜃  denotes the parameters \ncorresponding to the implementation of respective expression. \n4.2 Methodology of UEDL \nIntroducing DL to TEL, three basic patterns have emerged to evolve the \nmethodology of UEDL. As shown in Fig. 6, the three basic patterns include: A) feature \nextraction based on DL; B) generating base learners based on DL; and C) forming \nensemble learner based on DL. \nLet 𝐷𝐸𝑀= {𝑑𝑒𝑚1(∗; 𝜃1\n𝑑𝑒𝑚), ⋯, 𝑑𝑒𝑚𝑚(∗; 𝜃𝑚\n𝑑𝑒𝑚)}  denote various DL-based \nextracting methods. Particularly,  𝑑𝑒𝑚𝑚(∗; 𝜃𝑚\n𝑑𝑒𝑚)  signifies a DL-based extracting \nmethod \n𝑑𝑒𝑚𝑚(∗)  parameterized by \n𝜃𝑚\n𝑑𝑒𝑚 . Specifically, \n𝜃𝑚\n𝑑𝑒𝑚= {𝑑𝑙𝑚(∗\n; 𝜃𝑢\n𝑑𝑙𝑚), 𝜃𝑚\n𝑒𝑚} , which signifies that 𝑑𝑒𝑚𝑚(∗)  consists of a developed deep learner \n𝑑𝑙𝑚(∗)  parameterized by 𝜃𝑢\n𝑑𝑙𝑚  and a hyperparameter 𝜃𝑚\n𝑒𝑚  indicating what \nfeatures to extract from 𝑑𝑙𝑚(∗; 𝜃𝑢\n𝑑𝑙𝑚). The pattern A can be formally expressed as \n𝐹= 𝐷𝑒𝑒𝑝𝐹𝑒𝑎𝑡𝑟𝑢𝑒𝐸𝑥𝑡𝑟𝑎𝑐𝑡𝑖𝑜𝑛(𝐷, 𝐷𝐸𝑀 ) = {𝑓1, ⋯, 𝑓𝑛}. \nThereinto, 𝑓𝑛 , which signifies the deep representation of 𝑑𝑛 , is constructed by a \ndeeply converting procedure that can be more specifically expressed as \n𝑓𝑛= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑐𝑜𝑛𝑣𝑒𝑟𝑡𝑖𝑛𝑔(𝑑𝑛, 𝐷𝐸𝑀) \n= {𝑑𝑒𝑚1(𝑑𝑛; 𝜃1\n𝑑𝑒𝑚), ⋯, 𝑑𝑒𝑚𝑚(𝑑𝑛; 𝜃𝑚\n𝑑𝑒𝑚)} \n= {𝑑𝑥𝑛,1, ⋯, 𝑑𝑥𝑛,𝑘}     𝑠. 𝑡.  𝑘≥𝑚. \n \n \nFigure. 6. Three basic patterns to evolve the methodology of usual ensemble deep learning (UEDL) \nby applying DL to TEL. Pattern A: feature extraction based on DL. Pattern B: generating base \nlearners based on DL. Pattern C: forming ensemble learner based on DL. \n \nWith appropriate deep learning algorithms (𝐷𝐿𝐴) and learning strategies (𝐿𝑆), the \npattern B can be formally expressed as \nfeature extraction \ngenerating base learners\n(hetero/homogeneously) \nforming ensemble learner \nfeature extraction \nwith deep learning \ngenerating base learners \nwith deep learning\n(hetero/homogeneously)  \nforming ensemble learner \nwith deep learning \ndeep learning\nTEL\nUEDL\n……\n……\n……\n……\nDL\n:pattern A\n:pattern B\n:pattern C\n𝐿𝑏= 𝐵𝑎𝑠𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛(𝐷, 𝑇, 𝐷𝐿𝐴, 𝐿𝑆)\n= {𝑑𝑙1(∗; 𝜃𝑢\n𝑑𝑙1), ⋯, 𝑑𝑙𝑏(∗; 𝜃𝑢\n𝑑𝑙𝑏)}. \nThereinto, 𝑑𝑙𝑏(∗; 𝜃𝑢\n𝑑𝑙𝑏) signifies a base deep learner 𝑑𝑙𝑏(∗) parameterized by 𝜃𝑢\n𝑑𝑙𝑏, \nwhich is updated by the deeply evolving procedure. Identically, the data 𝐷 here can \nbe raw data or features extracted from raw data. More specifically, the details of \ngenerating 'heterogeneous' base deep learners can be expressed as follows \n𝐷𝐿𝐴= {{𝑑𝑙1(∗; 𝜃𝑑𝑙1), 𝑑𝑜𝑝𝑡1(∗,∗; 𝜃𝑑𝑜𝑝𝑡1)}, ⋯, {𝑑𝑙𝑏(∗; 𝜃𝑑𝑙𝑏), 𝑑𝑜𝑝𝑡𝑏(∗,∗; 𝜃𝑑𝑜𝑝𝑡𝑏)} }, \n𝐿𝑆= {𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,1(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,1), ⋯, 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)}, \n𝜃𝑢\n𝑑𝑙𝑏= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙𝑏(∗; 𝜃𝑑𝑙𝑏), 𝑑𝑜𝑝𝑡𝑏(∗,\n∗; 𝜃𝑑𝑜𝑝𝑡𝑏) | 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑑𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)) \n= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡\n𝜃𝑑𝑙𝑏\n(𝑑𝑙𝑏(𝐷; 𝜃𝑑𝑙𝑏), 𝑇; 𝜃𝑑𝑜𝑝𝑡𝑏 | 𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏(𝜃𝑙𝑠ℎ𝑒𝑡𝑒𝑟𝑜,𝑏)). \nAnd, the details of generating 'homogeneous' deep base learners can be expressed as \nfollows \n𝐷𝐿𝐴= {{𝑑𝑙0(∗; 𝜃𝑑𝑙0), 𝑑𝑜𝑝𝑡0(∗,∗; 𝜃𝑑𝑜𝑝𝑡0)}}, \n𝐿𝑆= {𝑙𝑠ℎ𝑜𝑚𝑜,1(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,1), ⋯, 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)}, \n𝑑𝑙𝑏(∗) = 𝑑𝑙0(∗) \n𝜃𝑢\n𝑑𝑙𝑏= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙0(∗; 𝜃𝑑𝑙0), 𝑑𝑜𝑝𝑡0(∗,\n∗; 𝜃𝑑𝑜𝑝𝑡0) | 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)) \n= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡\n𝜃𝑑𝑙0\n(𝑑𝑙0(𝐷; 𝜃𝑑𝑙0), 𝑇; 𝜃𝑑𝑜𝑝𝑡0 | 𝑙𝑠ℎ𝑜𝑚𝑜,𝑏(𝜃𝑙𝑠ℎ𝑜𝑚𝑜,𝑏)). \nLet 𝐷𝐸𝐶= {𝑑𝑙𝑒(∗; 𝜃𝑑𝑙𝑒), 𝑑𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑑𝑐𝑜𝑛𝑓)} denote the ensemble criteria for \nensemble deep learner formation, and 𝑇̃ = {𝑡1̃, ⋯, 𝑡𝑛̃ } signify the predictions of the \nbase learners. Specifically, 𝑑𝑙𝑒(∗; 𝜃𝑑𝑙𝑒)  signifies the construction of a DL-based \nensemble learner 𝑑𝑙𝑒(∗) parameterized by 𝜃𝑑𝑙𝑒 and 𝑑𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑑𝑐𝑜𝑛𝑓) signifies \nthe DL-based ensemble learner’s configuration 𝑑𝑐𝑜𝑛𝑓(∗,∗)  parameterized by \n𝜃𝑑𝑐𝑜𝑛𝑓 for updates of 𝜃𝑑𝑙𝑒. The pattern C can be formally expressed as \n𝐿𝑒= 𝐸𝑛𝑠𝑒𝑚𝑏𝑙𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐹𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛(𝑇̃, 𝑇, 𝐷𝐸𝐶) = {𝑑𝑙𝑒(∗; 𝜃𝑢\n𝑑𝑙𝑒)}. \nThereinto, 𝑑𝑙𝑒(∗; 𝜃𝑢\n𝑑𝑙𝑒)  signifies an ensemble deep learner 𝑑𝑙𝑒(∗)  parameterized \nby 𝜃𝑢\n𝑑𝑙𝑒, which is configured by the deeply integrating procedure. More specifically, \nthe deeply integrating procedure can be expressed as \n𝜃𝑢\n𝑑𝑙𝑒= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑖𝑛𝑡𝑒𝑔𝑟𝑎𝑡𝑖𝑛𝑔(𝑇̃, 𝑇, 𝑑𝑙𝑒(∗; 𝜃𝑑𝑙𝑒), 𝑑𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑑𝑐𝑜𝑛𝑓)) \n= 𝑎𝑟𝑔𝑑𝑐𝑜𝑛𝑓\n𝜃𝑑𝑙𝑒\n(𝑑𝑙𝑒(𝑇̃; 𝜃𝑑𝑙𝑒), 𝑇; 𝜃𝑑𝑐𝑜𝑛𝑓). \nAt testing, given a test data point 𝑑𝑡𝑒𝑠𝑡 , the corresponding procedures for the \nthree individual patterns can be expressed formally as follows \n𝑓𝑡𝑒𝑠𝑡= {𝑑𝑒𝑚1(𝑑𝑡𝑒𝑠𝑡; 𝜃1\n𝑑𝑒𝑚), ⋯, 𝑑𝑒𝑚𝑚(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑚\n𝑑𝑒𝑚)}, \n𝑡̃𝑡𝑒𝑠𝑡= {𝑑𝑙1(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑑𝑙1), ⋯, 𝑑𝑙𝑏(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑑𝑙𝑏)}, \n𝑡𝑡𝑒𝑠𝑡= 𝑑𝑙𝑒(𝑡̃𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑑𝑙𝑒). \nNote, the three patterns of introducing DL into TEL can be further combined to \nproduce more patterns for the construction of UEDL. No matter which pattern is \nemployed to construct an UEDL approach, the final formed learner for testing is usually \nregarded as an ensemble deep learner. \n4.3 Recent advances of UEDL \n4.3.1 Material collection \nThough many recent advances (Panda et al. 2021; Mohammed and Kora 2021; Das \net al. 2022) have been proposed for UEDL, we follow the strategy presented for the \nmaterial collection of recent advances of TEL in section 3.4.1. Thus, we review a \nnumber of highly cited recent works that are related to the topic of UEDL and \nsummarize corresponding key innovations to more appropriately and stably reflect the \nprimary underlying trends in recent development of UEDL. Identically, the reviewed \nworks were suggested by searching on Web of Science (accessed at March 1, 2021) \nwith filtering rules including being published in last ten years, and being highly cited \nor being a hotspot. \n4.3.2 Review \n   Referring to the methodology of TEL presented in Figure. 6, in addition to learning \nstrategies for generating base learners and ensembling criteria for forming ensemble \nlearner, we also add additional descriptions of extracting methods for feature \nrepresentations as well as depictions of learning algorithms for generating base \nlearners whenever they are necessary to provide essential basis for UEDL approaches \nto be carried out in specific applications. \nFor the probabilistic wind power forecasting, Wang et al. (Wang et al. 2017) \nproposed an ensemble deep learning approach based on different deep convolutional \nneural network (DCNN) (LeCun et al. 1998) architectures. Multiple base deep learners \nare independently trained using features extracted from raw wind power data by \nMallat wavelet decomposition (Mallat 1989) and converted into 2D image. The \nemployed DCNN architectures were different in terms of the number of hidden layers, \nthe number of maps in each layer and the size of the input. For the training of each \nbase deep learner, separated wavelet features were used. To form the ensemble deep \nlearner, the predictions of the multiple base deep learners were fused by wavelet \nreconstruction (Daubechies and Bates 1993).  \nFor the electricity load demand forecasting, Qiu et al. (Qiu et al. 2017) proposed a \ndeep ensemble learning approach based on empirical mode decomposition (EMD) \n(Huang et al. 1998). First, via EMD, load demand time series were decomposed into \nseveral intrinsic mode functions (IMFs) and one residual which can be regarded as \ninstantaneous frequency features extracted from the non-stationary and nonlinear \nraw data. Each of the IMFs and residual was separately used to train a deep belief \nnetwork (DBN) (Hinton et al. 2006) which was composed of restricted Boltzmann \nmachines (RBMs) (Hinton and Salakhutdinov 2006; Hinton 2012) and multilayer \nperceptron (MLP) (Gardner and Dorling 1998), resulting in a series of base deep \nlearners. The base deep learners were integrated via averaged summation or weighted \nlinear combination to form the ensemble learner. \nFor the sentiment analysis in social applications, Araque et al. (Araque et al. 2017) \ntrained multiple base deep learners based on word2vec (Mikolov et al. 2013) / doc2vec \n(Le and Mikolov 2014) combined with logistic regression (word2vec/doc2vec-LR). \nWord2vec was used for representations of short texts and doc2vec was used for \nrepresentations of long texts. The obtained base deep learners were integrated to \nform the final ensemble deep learner through weighting by majority voting or meta-\nlearning by random forests (Da Silva et al. 2014; Zhang and He 2015). \nFor the melanoma recognition in dermoscopic images, Codella et al. (Codella et al. \n2017) employed convolutional architecture for fast feature embedding (Caffe) (Jia et \nal. 2014) and deep residual network (ResNet) (He et al. 2016) pretrained on ImageNet \n(Deng et al. 2009) and convolutional network of U-shape (U-Net) (Ronneberger et al. \n2015) pretrained on lesion segmentation task (Codella et al. 2018) for feature \nrepresentations, in addition to several types of traditional feature representations \nincluding color histogram (CH) (Barata et al. 2014), edge histogram (EH) (Barata et al. \n2014), multi-scale variant of color local binary patters (MSLBP) (Zhu et al. 2010) and a \ntype of sparse coding (SC) (Mairal 2014). The deep CNN based feature representations \nand the traditional feature representations were respectively employed to train SVM \n(Chang and Lin 2011) for multiple base learners. The predictions of the obtained base \nlearners were averaged to form the final ensemble deep learner.  \nFor the wind speed forecasting, Chen et al. (Chen et al. 2018) utilized long short-\nterm memory (LSTM) (Hochreiter and Schmidhuber 1997; Sainath et al. 2015) to \nexploit the implicit information of collected wind speed time series. A cluster of LSTM \nwith diverse number of hidden layers and neurons in each hidden layer were built to \ntrain multiple base deep learners on the collected raw data. A meta-learning strategy \nwas proposed to form the final ensemble deep learner. The meta-learning strategy was \ncomposed of a support vector regression machine (SVRM) (Xu et al. 2017; Zhang et al. \n2017) that took the forecasting results of the trained base deep learners as inputs and \nwas optimized by extremal optimization (EO) (Boettcher and Percus 2000, 2001, 2002).  \nFor the crude oil price forecasting, Li et al. (Zhao et al. 2017) formulated an \nensemble deep learning approach named stacked denoising autoencoder bagging \n(SDAE-B). Employing the bagging strategy (Breiman 1996), multiple base deep learners \nwere trained based on SDAE (Vincent et al. 2008, 2010). Referring to (Zagaglia 2010; \nNaser 2016), 198 factors, including price, flow, stock macroeconomic and financial \nseries, were selected as exogenous variables for training. The predictions of the \nobtained base learners were averaged to form the final ensemble deep learner. \nFor the cancer prediction based on gene expression, Xiao et al. (Xiao et al. 2018) \nproposed to integrate multiple base learners using deep learning. The DEseq (Anders \nand Huber 2010) was employed to select informative genes from a given sequence for \ntraining. K nearest neighbor (KNN) (Altman 1992), support vector machine (SVM) \n(Chang and Lin 2011), decision tree (DT) (Quinlan 1986), random forests (RF) (Breiman \n2001), and gradient boosting decision tree (GBDT) (Friedman 2001) were employed to \ntrain five base learners with model selection using K-fold cross-validation (Bishop \n2006). To form the ensemble learner, a five-layer neural network (5-NN) was built as \nan implementation of the meta-learning strategy, which took the predictions of the \nfive obtained base learners as inputs and output normal or tumor. \nFor the prediction of melt index (MI) in industrial polymerization processes, Liu et \nal. (Liu et al. 2018) proposed an ensemble deep kernel learning (EDKL) approach. A \ntotal of 11 factors correlated with MI were chosen for training, including the reactor \npressure, reactor temperature, liquid level, and flow rate of the main catalysts. The \ndeep belief network (DBN) (Hinton et al. 2006) comprising a series of individual \nrestricted Boltzmann machines (RBM) (Hinton and Salakhutdinov 2006; Hinton 2012) \nwas adopted to extracted features from the chosen factors in a unsupervised manner. \nWith the extracted features, multiple base learners were trained based on a kernel \nlearning soft-sensing model (KLSM) (Liu et al. 2013) by bagging (Breiman 1996). The \npredictions of the obtained base learners were averaged to form the final ensemble \ndeep learner. \nFor the brain disease diagnosis based on MRI image, Suk et al. (Suk et al. 2017) as \nwell proposed to integrate multiple base learners using deep learning. A series of \ntechniques (details can be found in the preprocessing section) were employed to \nextract 93-dimensional volumetric features from an MRI image. Based on the \nextracted features, sparse regression model (SRM) (Wang et al. 2011; Zhang and Shen \n2012) was utilized to train multiple base learners with different values of the \nregularization control parameter. To form the ensemble learner, a six-layer \nconvolutional neural network (6-CNN) was built as an implementation of the meta-\nlearning strategy, which took the predictions of the obtained base learners as inputs \nand output disease diagnosis. \nFor the red lesion detection in fundus images, Orlando et al. (Orlando et al. 2018) \nformed an ensemble learning approach enhanced by deep learning. An unsupervised \ncandidate lesion detection approach was proposed based on morphological \noperations to identify potential lesion areas. A five-layer convolutional neural network \n(5-CNN) was built to learn features from patches around the detected potential lesion \nareas. A 63-dimensional feature vector was as well extracted from lesion candidates \nusing descriptors explored in related literature (Niemeijer et al. 2005; SB and Singh \n2012; Seoud et al. 2016). Both convolutional and hand-crafted features are used by \nrandom forests (RF) (Breiman 2001) to form the ensemble learner.  \nFor bearing fault diagnosis, Xu et al. (Xu et al. 2019) prosed an ensemble learning \napproach based on deep learning and random forests (RF) (Breiman 2001). Morlet, a \ntype of continuous wavelet transform, was employed to convert raw vibration signals \nof bearing, which was one-dimensional in time-domain, into spectrum of abundant \ncondition information, which was two-dimensional in time-frequency domain (LIN and \nQU 2000). A built five-layer CNN model, which consists of two convolutional layers, \ntwo pooling layers and one fully connected layer, was trained with the converted \nspectrums for feature extraction. The features of the two pooling layers and the fully \nconnected layer were independently employed by RF (Breiman 2001) to produce three \nRF base learners. To form the ensemble learner, the outputs of the three base learners \nwere integrated by the winner-take-all strategy (Guo et al. 2016). Ma et al. (Ma and \nChu 2019) also proposed an ensemble learning approach for the same task. Employing \na multi-objective evolutionary algorithm (MOEA/DD) (Li et al. 2015), multiple base \ndeep learners were generated based on three deep learning architectures, including \nDBN (Hinton et al. 2006), ResNet (He et al. 2016) and deep auto-encoder (DAE) (Jia et \nal. 2016). The three architectures all have five layers (including three hidden layers). \nRegarding to the number of hidden neurons in each hidden layer and the learning rate \nfor back propogation, MOEA/DD was leveraged to evolve a population of base deep \nlearners with maximum accuracy and diversity simultaneously. Frequency spectrums \ncalculated from the in-situ monitoring signals were used as input features for the \ntraining. To form the ensemble deep learner, the ensemble weights for the based \nlearners were optimized by differential evolution (DE) (Storn and Price 1997) with the \nobjective of average training accuracy and selected by a designed selection strategy \nwith three constraints including: (1) prediction accuracy, (2) diversity and (3) training \nspeed. \nFor vehicle type classification, Liu et al., (Liu et al. 2017b) constructed an ensemble \ndeep learning approach based on three types of ResNet (He et al. 2016) architectures, \nincluding ResNet-50, ResNet-101, ResNet-152. With respective initializations \npretrained on ImageNet (Jia Deng et al. 2009), the three architectures were \nindependently trained on the same training set which had been augmented with \nbalance sampling to obtain diverse base learners. The majority voting was employed \nas the ensembling criterion to form the ensemble learner.  \nFor vehicle type classification, Qummar et al., (Qummar et al. 2019) constructed \nan ensemble deep learning approach based on five deep CNN architectures, including \nResNet-50 (He et al. 2016), Inception-v3 (Szegedy et al. 2016), Xception (Chollet 2017), \nDense-121 and Dense-169 (Huang et al. 2017b). With respective pretrained \ninitializations, the five architectures were independently trained on the same fundus \nimages of Kaggle (https://www.kaggle.com/c/diabetic-retinopathy-detection/data) to \ngenerate five different base learners. To form the ensemble learner, Stacking (Wolpert \n1992) was employed as the ensembling criterion. \nFor the prediction of neuromuscular disorders, Khamparia et al. (Khamparia et al. \n2020) also proposed to integrate multiple base learners using deep learning. \nBhattacharya coefficient was employed to select top gene features and the obtained \ncoefficient was given as input to generate gene features related to muscular disorder. \nBased on the extracted features, KNN (Altman 1992), DT (Quinlan 1986), linear \ndiscriminant analysis (LDA), quadratic discriminant analysis (QDA), RF (Breiman 2001), \nvariants of SVM (Chang and Lin 2011) was utilized to train multiple base learners. To \nform the ensemble learner, a five-layer convolutional neural network (5-CNN) was built \nas an implementation of the meta-learning strategy, which took the predictions of the \nobtained base learners as inputs and output disease diagnosis. \n4.3.3 Summary \nThe information of the reviewed recent works of UEDL is listed in Table. 9. In \naddition to Table. 4, the feature extraction methods adopted by the UDEL approaches \nproposed in the reviewed works are listed in Table. 10. In addition to Table. 5 and Table. \n6, the learning algorithms and learning strategies employed to generate base learners \nare respectively listed in Table. 11 and Table. 12. In addition to Table. 7, the ensembling \ncriteria utilized to integrate base learners are listed in Table. 13. Referring to Table. 9-\n13, the key innovations of the UEDL approaches proposed in the reviewed works are \nsummarized in Table. 14. Finally, referring to the paradigm of UEDL presented in Fig. 5, \nthe basic patterns of applying deep learning to TEL to evolve the UEDL approaches \nproposed in the reviewed works are listed in Table. 15. \n \nTable. 9. The information of the applications in the reviewed recent UEDL works (DA) \nCode Description \nCitations Published \nYear \nDA1 \nProbabilistic wind power forecasting (Wang et al. 2017) \n348 \n2017 \nDA2 \nElectricity load demand forecasting (Qiu et al. 2017) \n229 \n2017 \nDA3 \nSentiment analysis in social applications (Araque et al. 2017) \n299 \n2017 \nDA4 \nMelanoma recognition in dermoscopic images (Codella et al. 2017) \n295 \n2017 \nDA5 \nWind speed forecasting (Chen et al. 2018) \n144 \n2018 \nDA6 \nCrude oil price forecasting (Zhao et al. 2017) \n162 \n2017 \nDA7 \nCancer prediction based on gene expression (Xiao et al. 2018) \n176 \n2018 \nDA8 \nPrediction of melt index in industrial polymerization processes (Liu et \nal. 2018) \n113 \n2018 \nDA9 \nBrain disease diagnosis based on MRI image (Suk et al. 2017) \n148 \n2017 \nDA10 \nRed lesion detection in fundus images (Orlando et al. 2018) \n117 \n2018 \nDA11 \nBearing fault diagnosis \n(Xu et al. 2019) \n65 \n2019 \n(Ma and Chu 2019) \n49 \n2019 \nDA12 \nVehicle type classification (Liu et al. 2017b) \n52 \n2017 \nDA13 \nDiabetic retinopathy detection (Qummar et al. 2019) \n50 \n2019 \nDA14 \nPrediction of neuromuscular disorders (Khamparia et al. 2020) \n45 \n2020 \n \nTable. 10. In addition to Table. 4, the methods for feature extraction (FE) of the UEDL (TFE/DFE) \napproaches proposed in the reviewed works \nCode \nDescription \nTFE22 \nMallat wavelet decomposition (Mallat 1989) \nTFE23 \nEmpirical mode decomposition (EMD) (Huang et al. 1998) \nTFE24 \nColor histogram (Barata et al. 2014) \nTFE25 \nEdge histogram (Barata et al. 2014) \nTFE26 \nMulti-scale variant of color local binary patters (MSLBP) (Zhu et al. 2010) \nTFE27 \nSparse coding (SC) (Mairal 2014) \nTFE28 \nRaw wind speed series (Chen et al. 2018) \nTFE29 \nFactors including price, flow, stock, macroeconomic and financial series (Zagaglia 2010; Naser 2016) \nTFE30 \nDEseq (Anders and Huber 2010) \nTFE31 \nVolumetric features extracted from MRI image (Suk et al. 2017) \nTFE32 \nVolumetric features extracted from fundus images (Niemeijer et al. 2005; SB and Singh 2012; Seoud \net al. 2016) \nTFE33 \nMorlet wavelet transform (LIN and QU 2000) \nTFE34 \nFrequency spectrums calculated from the in-situ monitoring signals (Ma and Chu 2019) \nTFE35 \nRaw fundus images \nTFE36 \nRaw vehicle images \nTFE37 \ngene features related to muscular disorder (Qummar et al. 2019) \nDFE1 \nWord2vec (Mikolov et al. 2013) \nDFE2 \nDoc2vec (Le and Mikolov 2014) \nDFE3 \nCaffe (Jia et al. 2014) pretrained on ImageNet (Deng et al. 2009) \nDFE4 \nResNet (He et al. 2016) pretrained on ImageNet (Deng et al. 2009) \nDFE5 \nU-Net (Ronneberger et al. 2015) pretrained on lesion segmentation task (Codella et al. 2018) \nDFE6 \nUnsupervised learning features from chosen factors via DBN (Hinton and Salakhutdinov 2006; Hinton \n2012) \nDFE7 \nSupervised learning features via convolutional neural network (Orlando et al. 2018) \n \nTable. 11. In addition to Table. 5, the learning algorithms (LA) of the UEDL (TLA/DLA) approaches \nproposed in the reviewed works \nCode \nAbbr. \nDescription \nTLA17 \nDT \nDecision tree (Quinlan 1986) \nTLA18 \nGBDT \nGradient boosting decision tree (Friedman 2001) \nTLA19 \nKLSM \nkernel learning soft-sensing model (Liu et al. 2013) \nTLA20 \nSRM \nSparse regression model (Wang et al. 2011; Zhang and Shen 2012) \nTLA21 \nLDA \nLinear discriminant analysis \nTLA22 \nQDA \nQuadratic discriminant analysis \nDLA1 \nDCNN \nDeep convolutional neural network (LeCun et al. 1998) \nDLA2 \nDBN \nDeep belief network (Hinton et al. 2006) \nDLA3 \nLSTM \nLong short-term memory (Hochreiter and Schmidhuber 1997; Sainath et al. 2015) \nDLA4 \nSDAE \nStacked denoising autoencoder (Vincent et al. 2008, 2010) \nDLA5 \nResNet \nDeep residual network (He et al. 2016) \nDLA6 \nDAE \nDeep auto-encoder (Jia et al. 2016) \nDLA7 \nInception-v3 \nRethinking the Inception Architecture for Computer Vision (Szegedy et al. 2016) \nDLA8 \nXception \nDeep learning with depthwise separable convolutions (Chollet 2017) \nDLA9 \nDense \nDensely Connected Convolutional Networks (Huang et al. 2017b) \n \nTable. 12. In addition to Table. 6, the learning strategies (LS) of the UEDL approaches proposed in \nthe reviewed works \nCode \nDescription \nLS13 \nTraining with differently initialized values of regularization parameters (Suk et al. 2017) \nLS14 \nMulti-objective evolutionary algorithm (MOEA/DD) (Li et al. 2015) \n \nTable. 13. In addition to Table. 7, the ensembling criteria (EC) of the UEDL (TEC/DEC) approaches \nproposed in the reviewed works \nCode \nDescription \nTEC10 \nWavelet reconstruction (Daubechies and Bates 1993) \nTEC11 \nRandom forests (Araque et al. 2017) \nTEC12 \nSupport vector regression machine optimized by extremal optimization \n(Chen et al. 2018) \nTEC13 \nWinner-take-all (Guo et al. 2016) \nTEC14 \nDifferential evolution (Storn and Price 1997) \nTEC15 \nEnsemble selection with constraints (Ma and Chu 2019) \nDEC1 \nFive-layer neural network (Xiao et al. 2018) \nDEC2 \nSix-layer convolutional neural network (Suk et al. 2017) \nDEC3 \nFive-layer convolutional neural network (Khamparia et al. 2020) \n \nTable. 14. The key innovations of the UEDL approaches proposed in the reviewed works \nApp. FE \nLA&S \nEC \nLA \nLS \nDA1 \nTFE22 \nDLA1 \nHetero. by LS5 \nW. by TEC10 \nDA2 \nTFE23 \nDLA2 \nHomo. by LS5 \nW. by TEC1,8 \nDA3 \nDFE1,2 \nTLA8 \nHomo. by LS4 \nM. by TEC11 \nDA4 \nTFE24-27, DFE3-5 \nTLA1 \nHomo. by LS5 \nW. by TEC8 \nDA5 \nTFE28 \nDLA3 \nHetero. by LS4 \nM. by TEC12 \nDA6 \nTFE29 \nDLA4 \nHomo. by LS7 \nW. by TEC8 \nDA7 \nTFE30 \nTLA1,3,7,17,18 \nHomo. by LS4 \nM. by DEC1 \nDA8 \nDFE6 \nTLA19 \nHomo. by LS7 \nW. by TEC8 \nDA9 \nTFE31 \nTLA20 \nHomo. by LS13 \nM. by DEC2 \nDA10 \nTFE32, DFE7 \nTLA17 \nHomo. by LS3 \nW. by TEC2 \nDA11 \nTFE33, DFE7 \nTLA3 \nHomo. by LS3 \nW. by TEC13 \nTFE34 \nDLA2,5,6 \nHetero. by LS14 \nW. by TEC14 / S. by TEC15 \nDA12 \nTFE35 \nDLA5 \nHetero. by LS4 \nW. by TEC2 \nDA13 \nTFE36 \nDLA5,7-9 \nHetero. by LS4 \nW. by TEC2 \nDA14 \nTFE37 \nTLA7,17,21,22 \nHetero. by LS4 \nM. by DEC3 \n \nTable. 15. The basic patterns of applying deep learning to TEL to evolve the UEDL approaches \nproposed in the reviewed works \nApp.1-5 \nPattern App.6-10 Pattern App.11-14 Pattern \nDA1 \nB \nDA6 \nB \nDA11 \nA \nDA2 \nB \nDA7 \nC \nB \nDA3 \nA \nDA8 \nB \nDA12 \nDA13 \nB \nDA4 \nA \nDA9 \nC \nB \nDA5 \nB \nDA10 \nA \nDA14 \nC \n \nBased on these tables, we can summarize: Five of the reviewed works evolved \nUEDL approaches by employing pattern A, which utilized pretrained or unsupervised \ntrained deep models to extract more expressive features from raw data; Eight of the \nreviewed works evolved UEDL approaches by employing pattern B, which replaced the \ntraditional learning algorithms with deep learning algorithms to generate base deep \nlearners with hand-crafted features or raw data; Three of the reviewed works evolved \nUEDL approaches by employing pattern C, which constructed meta-learning based \nensembling criteria to form deep ensemble learner by deep learning algorithms; And \none work of DA11 proposed an ensemble selection (S.) based ensembling criterion to \nreduce the costs of ensemble deep learner at testing, as it was more expensive than \ntraditional ensemble learner. These summarizations reflect the primary underlying \ntrends for the recent development of UEDL. \n4.4 Unattainability \nBased on the primary underlying trends of UEDL summarized in subsection 4.3.3, \nin this subsection, we discuss the unattainability of UEDL. Although a few learning \nstrategies and ensemble criteria that are more appropriate for the methodology of \nUEDL have been proposed, the primary issue associated with the methodology of \nUEDL is that it still retains the paradigm of TEL by simply introducing DL to TEL. This \nnature of UEDL considerably increases the time and space demands of training \nmultiple base deep learners and testing the ensemble deep learner. To address this, \nthe concept of knowledge distillation (Frosst and Hinton 2018) has become popular in \nmany UEDL approaches. The key idea of knowledge distillation is using a student \nlearner, which is often simpler, to distil knowledge of multiple teacher learners \nselected from a pool of pre-optimized teacher learners which are often more complex \n(Parisotto et al. 2016; Shen et al. 2019). Though knowledge distillation can reduce the \ncosts at the testing stage of ensemble deep learning, it still requires large extra \nexpenses during the process of training which prevents adequate usage of UEDL in \nspecific fields. Thus, new methodology for fast ensemble deep learning needs to be \nfurther studied.  \n \n5. Fast Ensemble Deep Learning \nRetaining the paradigm of TEL, UEDL ignores the intrinsic characteristics of DL. To \naddress this, taking the inherent characteristics of DL into consideration, fast ensemble \ndeep learning (FEDL) emerges, which more intrinsically reduce the time and space \noverheads of EDL. \n5.1 Existing FEDL approaches \nTo reduce the time overhead of EDL in training multiple base deep learners, Huang \net al. (Huang et al. 2017a) proposed an FEDL approach named Snapshot, arguing that \nthe local minima found on the optimizing path of stochastic gradient descent (SGD) \n(Duchi et al. 2010; Kingma and Ba 2015) can benefit ensembles of deep neural \nnetworks. Snapshot utilizes the non-convex nature of deep neural networks and the \nability of SGD to converge and escape from local minima as needed. By allowing SGD \nto converge to local minima multiple times along the optimizing path via cyclic learning \nrates (Loshchilov and Hutter 2017; Smith 2017), instead of training multiple deep \nlearners independently from scratch, they effectively reduced the time cost of \nobtaining multiple base deep learners. Garipov et al. (Garipov et al. 2018) found that \npaths with lower loss values existing between the local minima in the loss plane of \ndeep neural networks, and proposed the fast geometric ensembling (FGE) algorithm \nto find local minima along these paths via cyclic learning rates. As a result, multiple \nbase deep learners can be obtained with less time overhead. Although Snapshot and \nFGE can effectively reduce the time of training multiple base deep learners, in order \nto make full use of Snapshot and FGE to obtain better generalization performance, we \nneed to store the obtained multiple base deep learners and average their predictions \nto form the final ensemble deep learner. This substantially increases the expenses for \nthe testing stage of the final ensemble deep learner. Observing that the local minima \nfound at the end of each learning rate cycle tend to accumulate at the boundary of the \nlow-value area on the loss plane of deep neural networks, the stochastic weight \naveraging (SWA) algorithm (Izmailov et al. 2018; Maddox et al. 2019) was proposed to \naverage the local minima at the boundary of low-value area to form the ensemble \ndeep learner. It has been proved that SWA can produce ensemble deep learner with \nbetter generalization. At the end of each learning rate cycle, SWA saves the weights of \nthe currently optimized deep neural network and averages them with the weights \nsaved at the end of the last learning rate cycle to form the current ensemble deep \nlearner. SWA can obtain multiple base deep learners under the time expense of \ntraining one deep learner and the space expense of additional one deep learner, and \neffectively reduces the time and space cost for the testing stage of the final ensemble \ndeep learner. \n5.2 Optional methodology of FEDL \nAlthough several FEDL approaches have been proposed, there still lacks a clear \ndefinition for FEDL. Alleviating this situation, Yang et al. (Yang et al. 2021) presented \nan optional definition and inferred evaluations of FEDL based on observations of \nexisting FEDL approaches. To present a clear definition, the problem of FEDL were \ndivided into three procedures, including: A) training a base deep learner; B) starting \nfrom the pre-trained base deep learner to find local minima in the entire parameter \nspace to obtain extra base deep learners; C) integrating the obtained multiple base \ndeep learners to form the ensemble deep learner. The methodology of this definition \nfor FEDL is shown in Fig. 7.  \nWith a deep learning algorithm (𝐷𝐿𝐴= {𝑑𝑙0(∗; 𝜃𝑑𝑙0), 𝑑𝑜𝑝𝑡0(∗,∗; 𝜃𝑑𝑜𝑝𝑡0)}) and a \nlearning strategy (𝐿𝑆1 = {𝑙𝑠0(𝜃𝑙𝑠0)} ), the procedure A) of FEDL can be formally \nexpressed as \n𝐷𝐿0 = 𝐵𝑎𝑠𝑒𝐷𝑒𝑒𝑝𝑀𝑜𝑑𝑒𝑙𝐷𝑒𝑣𝑒𝑙𝑜𝑝𝑚𝑒𝑛𝑡(𝐷, 𝑇, 𝐷𝐿𝐴, 𝐿𝑆1) = {𝑑𝑙0(∗; 𝜃𝑢\n𝑑𝑙0)}, \n𝜃𝑢\n𝑑𝑙0 = 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙0(∗; 𝜃𝑑𝑙0), 𝑑𝑜𝑝𝑡0(∗,∗; 𝜃𝑑𝑜𝑝𝑡0) | 𝑙𝑠0(𝜃𝑙𝑠0)) \n= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡0\n𝜃𝑑𝑙0\n(𝑑𝑙0(𝐷; 𝜃𝑑𝑙0), 𝑇; 𝜃𝑑𝑜𝑝𝑡0 | 𝑙𝑠0(𝜃𝑙𝑠0)). \nWith the pre-trained 𝐷𝐿0 , 𝐷𝐿𝐴  and additional learning strategies ( 𝐿𝑆2 =\n{𝑙𝑠1(𝜃𝑙𝑠1), ⋯, 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)}), the procedure B) of FEDL can be formally expressed as \n𝐷𝐿𝑏= 𝐸𝑥𝑡𝑟𝑎𝐵𝑎𝑠𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛(𝐷, 𝑇, 𝐷𝐿0, 𝐷𝐿𝐴, 𝐿𝑆2) =\n{𝑑𝑙1(∗; 𝜃𝑢\n𝑑𝑙1), ⋯, 𝑑𝑙𝑏(∗; 𝜃𝑢\n𝑑𝑙𝑏)}, \n𝑑𝑙𝑏(∗) = 𝑑𝑙0(∗), \n𝜃𝑢\n𝑑𝑙𝑏= 𝑑𝑒𝑒𝑝𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙0(∗; 𝜃𝑑𝑙0|𝜃𝑢\n𝑑𝑙𝑏−1), 𝑑𝑜𝑝𝑡0(∗,∗; 𝜃𝑑𝑜𝑝𝑡0) | 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)) \n= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡0\n𝜃𝑑𝑙0\n(𝑑𝑙0(𝐷; 𝜃𝑑𝑙0|𝜃𝑢\n𝑑𝑙𝑏−1), 𝑇; 𝜃𝑑𝑜𝑝𝑡0 | 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)). \nBoth ensemble criteria 𝐸𝐶= {𝑙𝑒(∗; 𝜃𝑙𝑒), 𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑐𝑜𝑛𝑓)} for TEL and 𝐷𝐸𝐶=\n{𝑑𝑙𝑒(∗; 𝜃𝑑𝑙𝑒), 𝑑𝑐𝑜𝑛𝑓(∗,∗; 𝜃𝑑𝑐𝑜𝑛𝑓)} for UEDL can be leveraged for fast ensemble deep \nlearner formation. In the context of FEDL, these two criteria are also named as \nensemble in model space. Another ensemble criterion that is used by FEDL is ensemble \nin parameter space, which fuses multiple base deep learners into a single deep learner. \nFormally, let 𝐸𝐶𝑃𝑆= {𝑓𝑢𝑠𝑖𝑛𝑔(∗, 𝜃𝑓𝑢𝑠𝑖𝑛𝑔)}  denote the ensemble criterion in \nparameter space, the procedure C) of FEDL in parameter space can be formally \nexpressed as follows \n𝐹𝐷𝐿𝑒= 𝐸𝑛𝑠𝑒𝑚𝑏𝑙𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐹𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛({𝐷𝐿0, 𝐷𝐿𝑏}, 𝐸𝐶𝑃𝑆)\n= {𝑓𝑑𝑙𝑒(∗; 𝜃𝑢\n𝑓𝑑𝑙𝑒)}, \n𝑓𝑑𝑙𝑒(∗) = 𝑑𝑙0(∗), \n𝜃𝑢\n𝑓𝑑𝑙𝑒= 𝑓𝑢𝑠𝑖𝑛𝑔({𝜃𝑢\n𝑑𝑙0, 𝜃𝑢\n𝑑𝑙1, ⋯, 𝜃𝑢\n𝑑𝑙𝑏}; 𝜃𝑓𝑢𝑠𝑖𝑛𝑔). \n \n \nFigure. 7. The methodology of fast ensemble deep learning (FEDL). Deep model development: pre-\ntraining a base deep learner. Generating base deep learners: starting from the pre-trained base \ndeep learner to find local minima in the entire parameter space to generate extra base deep \nlearners. Forming ensemble deep learner: integrating the obtained multiple base deep learners to \nform the final ensemble deep learner. \n \nAt testing, given a test data point 𝑑𝑡𝑒𝑠𝑡, the inference procedures of the ensemble \ndeep learner integrated in model space include computing the predictions of all base \ndeep learners which can be expressed as \n𝑡̃𝑡𝑒𝑠𝑡= {𝑑𝑙0(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑑𝑙0), 𝑑𝑙1(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑑𝑙1), ⋯, 𝑑𝑙𝑏(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑑𝑙𝑏)}, \nand mapping 𝑡̃𝑡𝑒𝑠𝑡 to the final prediction 𝑡𝑡𝑒𝑠𝑡 by the learner integrated in model \nspace. Whereas, the inference procedure of the ensemble deep learner fused in \nparameter space can be simply expressed as \n𝑡𝑡𝑒𝑠𝑡= 𝑓𝑑𝑙𝑒(𝑑𝑡𝑒𝑠𝑡; 𝜃𝑢\n𝑓𝑑𝑙𝑒). \nloss\n𝑝 \n…\nensembling criteria\nin model space\nor\nin parameter space\nstarting from the pre-trained base\ndeep learner to find local minima in\nthe entire parameter space (  ,   )\nO\n…\n𝑝 \ndeeply evolving\nS\n…\nor\n:deep model development\n:generating base deep learners \n:forming ensemble deep learner \n:training data\n…\n:deep learning algorithm\n:learning strategy\nS\n…\n:base deep learners\n:ensemble deep learner integrated in model or parameter space\n…\n5.3 Finding local minima in the sub-parameter space for FEDL \nExisting FEDL approaches (like Snapshot, FGE and SWA) have promoted the \ndeployment of ensemble deep learning in some artificial intelligence applications to \nsome extent. However, due to the large expenses compared with TEL, FEDL still needs \nfurther advances in some specific fields, where the developing time and computing \nresources are usually restricted (f. e. the field of robotics vision (Yang et al. 2018, 2019)) \nor the data to be processed is of large dimensionality (f. e. the field of histopathology \nwhole slide image analysis (Yang et al. 2020c, b)). In these specific fields, it is \nsometimes still difficult to deploy the TEL approaches, which makes the deployment \nof existing FEDL approaches still challenging.  \nTo alleviate this, Yang et al (Yang et al. 2020a, 2021) argued that local minima \nfound in the sub-parameter space (LoMiFoSS) can be effective for FEDL, and proposed \nthe concept of LoMiFoSS-based fast ensemble deep learning (LoMiFoSS-FEDL).  \nReferring to the optional methodology presented for FEDL, the problem of LoMiFoSS-\nFEDL were also divided into three procedures to present the methodology of \nLoMiFoSS-FEDL, including: A) pre-training a base deep learner; B) starting from the \npre-trained base deep learner to find local minima in the sub-parameter space; C) \nfusing the multiple base deep learners of the found local minima in the sub-parameter \nspace to form the final ensemble deep learner. Different from current state-of-the-art \nFEDL approaches (Huang et al. 2017a; Garipov et al. 2018; Izmailov et al. 2018; Maddox \net al. 2019) that optimize the entire parameters of a deep neural network architecture, \nLoMiFoSS-FEDL only optimizes partial parameters to further reduce the costs required \nfor training multiple base deep learners. LoMiFoSS-FEDL provides an addition to \ncurrent FEDL approaches. An optional methodology of LoMiFoSS-FEDL is shown in Fig. \n8. \nFormally, the procedure A) of LoMiFoSS-FEDL can be expressed exactly as the \nprocedure A) of FEDL. With the pre-trained 𝐷𝐿0 , 𝐷𝐿𝐴 , a sub-parameter space \n𝑆𝑃𝑆= {𝜃𝑑𝑙0,𝑠|𝜃𝑑𝑙0,𝑠∈𝜃𝑑𝑙0; 𝜃𝑑𝑙0,𝑓∈𝜃𝑑𝑙0; (𝜃𝑑𝑙0,𝑠∪𝜃𝑑𝑙0,𝑓) == 𝜃𝑑𝑙0} , and additional \nlearning strategies (𝐿𝑆2 = {𝑙𝑠1(𝜃𝑙𝑠1), ⋯, 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)}), the procedure B) of LoMiFoSS-\nFEDL can be formally expressed as \n𝐷𝐿𝑏= 𝐸𝑥𝑡𝑟𝑎𝐵𝑎𝑠𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐺𝑒𝑛𝑒𝑟𝑎𝑡𝑖𝑜𝑛(𝐷, 𝑇, 𝐷𝐿0, 𝐷𝐿𝐴, 𝑆𝑃𝑆, 𝐿𝑆2) =\n{𝑑𝑙1(∗; 𝜃𝑢\n𝑑𝑙1), ⋯, 𝑑𝑙𝑏(∗; 𝜃𝑢\n𝑑𝑙𝑏)}, \n𝑑𝑙𝑏(∗) = 𝑑𝑙0(∗), \n𝜃𝑢\n𝑑𝑙𝑏= 𝜃𝑢\n𝑑𝑙0,𝑓∪𝜃𝑢\n𝑑𝑙𝑏,𝑠, \n𝜃𝑢\n𝑑𝑙𝑏,𝑠= 𝑝𝑎𝑟𝑡𝑖𝑎𝑙𝑦_𝑒𝑣𝑜𝑙𝑣𝑖𝑛𝑔(𝐷, 𝑇, 𝑑𝑙0(∗; 𝜃𝑑𝑙0,𝑠|𝜃𝑢\n𝑑𝑙𝑏−1), 𝑑𝑜𝑝𝑡0(∗,\n∗; 𝜃𝑑𝑜𝑝𝑡0) | 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)) \n= 𝑎𝑟𝑔𝑑𝑜𝑝𝑡0\n𝜃𝑑𝑙0,𝑠\n(𝑑𝑙0(𝐷; 𝜃𝑑𝑙0,𝑠|𝜃𝑢\n𝑑𝑙𝑏−1), 𝑇; 𝜃𝑑𝑜𝑝𝑡0 | 𝑙𝑠𝑏(𝜃𝑙𝑠𝑏)). \nWith the ensemble criterion in parameter space 𝐸𝐶𝑃𝑆, the procedure C) of LoMiFoSS-\nFEDL in sub-parameter space can be formally expressed as \n𝐹𝐷𝐿𝑒= 𝐸𝑛𝑠𝑒𝑚𝑏𝑙𝑒𝐷𝑒𝑒𝑝𝐿𝑒𝑎𝑟𝑛𝑒𝑟𝐹𝑜𝑟𝑚𝑎𝑡𝑖𝑜𝑛({𝐷𝐿0, 𝐷𝐿𝑏}, 𝐸𝐶𝑃𝑆)\n= {𝑓𝑑𝑙𝑒(∗; 𝜃𝑢\n𝑓𝑑𝑙𝑒)}, \n𝑓𝑑𝑙𝑒(∗) = 𝑑𝑙0(∗), \n𝜃𝑢\n𝑓𝑑𝑙𝑒= 𝜃𝑢\n𝑑𝑙0,𝑓∪𝜃𝑢\n𝑑𝑙𝑒,𝑠, \n𝜃𝑢\n𝑑𝑙𝑒,𝑠= 𝑓𝑢𝑠𝑖𝑛𝑔({𝜃𝑢\n𝑑𝑙0,𝑠, 𝜃𝑢\n𝑑𝑙1,𝑠, ⋯, 𝜃𝑢\n𝑑𝑙𝑏,𝑠} ; 𝜃𝑓𝑢𝑠𝑖𝑛𝑔). \n \n \nFigure. 8. The methodology of LoMiFoSS-FDEL for fast ensemble deep learning. Deep model \ndevelopment: pre-training a base deep learner. Generating base deep learners: starting from the \npre-trained base deep learner to find local minima in the sub-parameter space to generate extra \nbase deep learners. Forming ensemble deep learner: integrating the obtained multiple base deep \nlearners in the sub-parameter space to form the final ensemble deep learner. \n \nAt testing, the inference procedure of the ensemble deep learner for LoMiFoSS-\nFDEL can be expressed the same as the inference procedure of the ensemble deep \nlearner fused in parameter space for FDEL. \n5.4 Remaining issues \nThe primary remaining issue associated with FEDL is the imperfection for its \ndefinition. From the existing methodologies (Fig. 7 and 8) presented for FEDL, we can \nnotice that the found extra base deep learners are tightly related to the pre-trained \nbase deep learner. However, we cannot be sure whether the found extra base deep \nlearners and the pre-trained base deep learner can be united to produce the optimal \nensemble solution. In addition, one may argue that finding multiple deep learners that \nloss\nstarting from the pre-trained base\ndeep learner to find local minima in\nthe subparameter space (  )\nO\n𝑝 \nevolving\nS\n…\nloss\n𝑝 \nO\nprojection\n𝑝 \n…\n…\n…\n…\nensembling criteria\nin parameter space\n…\n:deep model development\n:generating base deep learners \n:forming ensemble deep learner \n:training data\n:deep learning algorithm\n:learning strategy\nS\n:base deep learners\n:ensemble deep learner\n…\n𝑝 \n𝑝 ,0\n𝑝 ,1\n𝑝 , \n𝑝 ,𝑚\nare less tightly related to each other can achieve better ensemble performances. Thus, \nthe two presented methodologies can only provide some optional perspectives for the \ndefinition of FEDL, and new methodologies from different angles for ensemble deep \nlearning are still needed. \n \n6 Discussion \nThis article is different from existing review articles (Sagi and Rokach 2018; Dong \net al. 2020) that mostly discussed about traditional ensemble learning, reviews (Cao \net al. 2020) that particularly discussed ensemble deep learning in bioinformatics, or \nspecific technical innovations with ensemble learning introduced such as GAN-\nEnsembles (Durugkar et al. 2017; Ghosh et al. 2018; Han et al. 2021). With more \nfundamental discussions about the developing routes of both traditional ensemble \nlearning and ensemble deep learning, in this article, we aim to reveal the intrinsic \nproblems and technical challenges of deploying ensemble learning under the era of \ndeep learning to a wider range of specific fields.  \nData analyses of published works show that ensemble learning is still prosperously \ndeveloping while the research of ensemble deep learning (EDL) compared with the \nresearch of traditional ensemble learning (TEL) is severely lagging behind. The primary \nreason for this phenomenon lies in that the time and space overheads for the training \nand testing stages of EDL are much larger than that of TEL.  \nRecent advances of TEL have achieved remarkable progresses in various \napplications. However, most of these advances focused on proposing solutions for \nspecific applications, based on combinatorial innovations by leveraging existing \nlearning strategies for base learner generation and existing ensembling criteria for \nensemble learner formation. Few proposed new learning strategies for base learner \ngeneration or new ensembling criteria for ensemble learner formation. This \nphenomenon reflects that the advances for the intrinsic problems of TEL seem to have \nreached to a bottleneck, although TEL is still prosperously developing in various \napplications. Besides, the primary issue associated with the methodology of TEL comes \nfrom the nature of usual machine learning (UML), which evolves a learner based on \nhand-crafted features which are usually difficult to design and not expressive enough. \nDirectly introducing deep learning (DL) to enhance the methodology of TEL by \nthree basic patterns (introducing DL to feature extraction, introducing DL to base \nlearner generation or introducing DL to ensemble learner formation), recent advances \nof usual EDL (UEDL) have promoted the solutions for various applications. However, \nthe primary issue of the methodology of UEDL is that its nature still retains the \nparadigm of TEL, which considerably increases the costs of training multiple base deep \nlearners and testing the ensemble deep learner. This can also be confirmed by the fact \nthat knowledge distillation (Frosst and Hinton 2018) and ensemble selection based \nensembling criterion (Ma and Chu 2019) for ensemble deep learner formation \nappeared in some recent advances of UEDL to reduce the testing overhead. A \npromising future direction is to more intrinsically reduce the expenses of EDL by taking \ninto consideration the inherent characteristics of DL.  \nFast ensemble deep learning (FEDL) emerges to more intrinsically reduce the costs \nof EDL, considering some inherent characteristics of DL. Finding local minima along the \n(global or local) optimization path of SGD for multiple base deep learners, existing \nadvances of FEDL have provided preferrable solutions to promote the deployment of \nEDL in more artificial intelligence applications of specific fields. However, the primary \nremaining issue of FEDL is the imperfection for its definition, since currently existing \nalternative methodologies for FEDL can only provide uncomprehensive perspectives. \nA promising future direction is to comprehensively propose more appropriate \ndefinitions for FEDL.  \nNotably, many brilliant ideas have been proposed for TEL, another interesting \ndirection is to introduce some ideas of TEL into DL for FEDL, with the reverse direction \nof the development of UEDL that introduces DL into TEL. For example, recently, Zhang \net al (Zhang et al. 2021) introduced negative correlation learning (Liu and Yao 1999) to \nDL with a divide-and-conquer strategy and proposed a solution of deep negative \ncorrelation learning for FEDL. \n \nData Availability \nData sharing not applicable to this article as no datasets were generated or \nanalysed during the current study. \n \nCompeting interests \nThe authors declare that they have no competing interests. \n \nReference \nAlam M, Samad MD, Vidyaratne L, et al (2020) Survey on Deep Neural Networks in \nSpeech \nand \nVision \nSystems. \nNeurocomputing \n417:302–321. \nhttps://doi.org/10.1016/j.neucom.2020.07.053 \nAltman NS (1992) An introduction to kernel and nearest-neighbor nonparametric \nregression. Am Stat. https://doi.org/10.1080/00031305.1992.10475879 \nAnders S, Huber W (2010) Differential expression analysis for sequence count data. \nNat Preced. https://doi.org/10.1038/npre.2010.4282.1 \nAraque O, Corcuera-Platas I, Sánchez-Rada JF, Iglesias CA (2017) Enhancing deep \nlearning sentiment analysis with ensemble techniques in social applications. \nExpert Syst Appl. https://doi.org/10.1016/j.eswa.2017.02.002 \nBakker B, Heskes T (2003) Clustering ensembles of neural network models. Neural \nNetworks. https://doi.org/10.1016/S0893-6080(02)00187-9 \nBarata C, Ruela M, Francisco M, et al (2014) Two systems for the detection of \nmelanomas in dermoscopy images using texture and color features. IEEE Syst J. \nhttps://doi.org/10.1109/JSYST.2013.2271540 \nBauer E, Kohavi R (1999) Empirical comparison of voting classification algorithms: \nbagging, boosting, and variants. Mach Learn \nBehera S, Mohanty MN (2019) Detection of ocular artifacts using bagged tree \nensemble model. In: Proceedings - 2019 International Conference on Applied \nMachine Learning, ICAML 2019 \nBehler J, Parrinello M (2007) Generalized neural-network representation of high-\ndimensional \npotential-energy \nsurfaces. \nPhys \nRev \nLett. \nhttps://doi.org/10.1103/PhysRevLett.98.146401 \nBi J, Zhang C (2018) An empirical comparison on state-of-the-art multi-class imbalance \nlearning algorithms and a new diversified ensemble learning scheme. \nKnowledge-Based Syst. https://doi.org/10.1016/j.knosys.2018.05.037 \nBishop CM (2006) Pattern Recoginiton and Machine Learning \nBoettcher S, Percus A (2000) Nature’s way of optimizing. Artif Intell 119:275–286. \nhttps://doi.org/10.1016/S0004-3702(00)00007-2 \nBoettcher S, Percus AG (2001) Optimization with extremal dynamics. Phys Rev Lett. \nhttps://doi.org/10.1103/PhysRevLett.86.5211 \nBoettcher S, Percus AG (2002) Optimization with extremal dynamics. Complexity. \nhttps://doi.org/10.1002/cplx.10072 \nBorş AG, Pitas I (1996) Median radial basis function neural network. IEEE Trans Neural \nNetworks. https://doi.org/10.1109/72.548164 \nBreiman \nL \n(1996) \nBagging \npredictors. \nMach \nLearn. \nhttps://doi.org/10.1007/bf00058655 \nBreiman \nL \n(2001) \nRandom \nforests. \nMach \nLearn. \nhttps://doi.org/10.1023/A:1010933404324 \nCao Y, Geddes TA, Yang JYH, Yang P (2020) Ensemble deep learning in bioinformatics. \nNat. Mach. Intell. \nChang CC, Lin CJ (2011) LIBSVM: A Library for support vector machines. ACM Trans \nIntell Syst Technol. https://doi.org/10.1145/1961189.1961199 \nChen J, Zeng GQ, Zhou W, et al (2018) Wind speed forecasting using nonlinear-learning \nensemble of deep learning time series prediction and extremal optimization. \nEnergy Convers Manag. https://doi.org/10.1016/j.enconman.2018.03.098 \nChen W, Feng P, Ding H, Lin H (2016) Identifying N 6-methyladenosine sites in the \nArabidopsis \nthaliana \ntranscriptome. \nMol \nGenet \nGenomics. \nhttps://doi.org/10.1007/s00438-016-1243-7 \nChen W, Feng PM, Lin H, Chou KC (2013) IRSpot-PseDNC: Identify recombination spots \nwith \npseudo \ndinucleotide \ncomposition. \nNucleic \nAcids \nRes. \nhttps://doi.org/10.1093/nar/gks1450 \nChen W, Pourghasemi HR, Kornejady A, Zhang N (2017) Landslide spatial modeling: \nIntroducing new ensembles of ANN, MaxEnt, and SVM machine learning \ntechniques. Geoderma. https://doi.org/10.1016/j.geoderma.2017.06.020 \nChollet F (2017) Xception: Deep learning with depthwise separable convolutions. In: \nProceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, \nCVPR 2017 \nCodella NCF, Gutman D, Celebi ME, et al (2018) Skin lesion analysis toward melanoma \ndetection: A challenge at the 2017 International symposium on biomedical \nimaging (ISBI), hosted by the international skin imaging collaboration (ISIC). In: \n2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). IEEE, \npp 168–172 \nCodella NCF, Nguyen QB, Pankanti S, et al (2017) Deep learning ensembles for \nmelanoma \nrecognition \nin \ndermoscopy \nimages. \nIBM \nJ \nRes \nDev. \nhttps://doi.org/10.1147/JRD.2017.2708299 \nCox DR (1959) The Regression Analysis of Binary Sequences. J R Stat Soc Ser B. \nhttps://doi.org/10.1111/j.2517-6161.1959.tb00334.x \nDa Silva NFF, Hruschka ER, Hruschka ER (2014) Tweet sentiment analysis with classifier \nensembles. Decis Support Syst. https://doi.org/10.1016/j.dss.2014.07.003 \nDas A, Mohapatra SK, Mohanty MN (2022) Design of deep ensemble classifier with \nfuzzy decision method for biomedical image classification. Appl Soft Comput \n115:108178. https://doi.org/10.1016/J.ASOC.2021.108178 \nDaubechies I, Bates BJ (1993) Ten Lectures on Wavelets. J Acoust Soc Am 93:1671–\n1671. https://doi.org/10.1121/1.406784 \nDavies MN, Secker A, Freitas AA, et al (2008) Optimizing amino acid groupings for \nGPCR \nclassification. \nBioinformatics. \nhttps://doi.org/10.1093/bioinformatics/btn382 \nDeng J, Dong W, Socher R, et al (2009) ImageNet: A large-scale hierarchical image \ndatabase. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. \nIEEE, pp 248–255 \nDietterich TG (2000) Ensemble methods in machine learning. In: Lecture Notes in \nComputer Science (including subseries Lecture Notes in Artificial Intelligence and \nLecture Notes in Bioinformatics) \nDietterich TG (1997) Machine-learning research: Four current directions. AI Mag \nDietterich TG, Bakiri G (1995) Solving Multiclass Learning Problems via Error-\nCorrecting Output Codes. J Artif Intell Res. https://doi.org/10.1613/jair.105 \nDing S, Zhao H, Zhang Y, et al (2015) Extreme learning machine: algorithm, theory and \napplications. Artif Intell Rev. https://doi.org/10.1007/s10462-013-9405-z \nDong X, Yu Z, Cao W, et al (2020) A survey on ensemble learning. Front. Comput. Sci. \nDos Santos EM, Sabourin R, Maupin P (2008) A dynamic overproduce-and-choose \nstrategy for the selection of classifier ensembles. Pattern Recognit. \nhttps://doi.org/10.1016/j.patcog.2008.03.027 \nDou J, Yunus AP, Bui DT, et al (2020) Improved landslide assessment using support \nvector machine with bagging, boosting, and stacking ensemble machine learning \nframework in a mountainous watershed, Japan. Landslides 17:641–658. \nhttps://doi.org/10.1007/s10346-019-01286-5 \nDuchi J, Hazan E, Singer Y (2010) Adaptive subgradient methods for online learning \nand stochastic optimization. In: COLT 2010 - The 23rd Conference on Learning \nTheory \nDurugkar I, Gemp I, Mahadevan S (2017) Generative multi-adversarial networks. In: \n5th International Conference on Learning Representations, ICLR 2017 - \nConference Track Proceedings \nFreund Y, Schapire RE (1997) A Decision-Theoretic Generalization of On-Line Learning \nand \nan \nApplication \nto \nBoosting. \nJ \nComput \nSyst \nSci. \nhttps://doi.org/10.1006/jcss.1997.1504 \nFreund Y, Schapire RE (1996) Experiments with a New Boosting Algorithm. Proc 13th \nInt Conf Mach Learn. https://doi.org/10.1.1.133.1040 \nFriedman JH (2001) Greedy function approximation: A gradient boosting machine. Ann \nStat. https://doi.org/10.1214/aos/1013203451 \nFrosst N, Hinton G rey (2018) Distilling a neural network into a soft decision tree. In: \nCEUR Workshop Proceedings \nGardner MW, Dorling SR (1998) Artificial neural networks (the multilayer perceptron) \n- a review of applications in the atmospheric sciences. Atmos Environ. \nhttps://doi.org/10.1016/S1352-2310(97)00447-0 \nGaripov T, Izmailov P, Podoprikhin D, et al (2018) Loss surfaces, mode connectivity, \nand fast ensembling of DNNs. In: Advances in Neural Information Processing \nSystems \nGeladi P, Kowalski BR (1986) Partial least-squares regression: a tutorial. Anal Chim \nActa. https://doi.org/10.1016/0003-2670(86)80028-9 \nGhosh A, Kulharia V, Namboodiri V, et al (2018) Multi-agent Diverse Generative \nAdversarial Networks. In: Proceedings of the IEEE Computer Society Conference \non Computer Vision and Pattern Recognition \nGranitto PM, Verdes PF, Ceccatto HA (2005) Neural network ensembles: Evaluation of \naggregation algorithms. Artif Intell. https://doi.org/10.1016/j.artint.2004.09.006 \nGuo C, Yang Y, Pan H, et al (2016) Fault analysis of High Speed Train with DBN \nhierarchical ensemble. In: Proceedings of the International Joint Conference on \nNeural Networks \nGuo Y, Yu L, Wen Z, Li M (2008) Using support vector machine combined with auto \ncovariance to predict protein-protein interactions from protein sequences. \nNucleic Acids Res. https://doi.org/10.1093/nar/gkn159 \nGupta S, Dennis J, Thurman RE, et al (2008) Predicting human nucleosome occupancy \nfrom \nprimary \nsequence. \nPLoS \nComput \nBiol. \nhttps://doi.org/10.1371/journal.pcbi.1000134 \nGuyon I, Elisseeff A (2006) Feature Extraction, Foundations and Applications: An \nintroduction to feature extraction. Stud Fuzziness Soft Comput \nHan X, Chen X, Liu L-P (2021) GAN Ensemble for Anomaly Detection. Proc AAAI Conf \nArtif Intell 35:4090–4097. https://doi.org/10.1609/aaai.v35i5.16530 \nHansen LK, Salamon P (1990) Neural Network Ensembles. IEEE Trans Pattern Anal \nMach Intell. https://doi.org/10.1109/34.58871 \nHe K, Zhang X, Ren S, Sun J (2016) Deep Residual Learning for Image Recognition. In: \n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, \npp 770–778 \nHernández-Lobato D, Martinez-Muñoz G, Suárez A (2009) Statistical instance-based \npruning in ensembles of independent classifiers. IEEE Trans Pattern Anal Mach \nIntell. https://doi.org/10.1109/TPAMI.2008.204 \nHinton GE (2012) A Practical Guide to Training Restricted Boltzmann Machines. In: \nComputer. pp 599–619 \nHinton GE, Osindero S, Teh YW (2006) A fast learning algorithm for deep belief nets. \nNeural Comput. https://doi.org/10.1162/neco.2006.18.7.1527 \nHinton GE, Salakhutdinov RR (2006) Reducing the dimensionality of data with neural \nnetworks. Science (80- ). https://doi.org/10.1126/science.1127647 \nHo TK (1998) The random subspace method for constructing decision forests. IEEE \nTrans Pattern Anal Mach Intell. https://doi.org/10.1109/34.709601 \nHochreiter S, Schmidhuber J (1997) Long Short-Term Memory. Neural Comput. \nhttps://doi.org/10.1162/neco.1997.9.8.1735 \nHoeting JA, Madigan D, Raftery AE, Volinsky CT (1999) Bayesian model averaging: A \ntutorial. Stat Sci. https://doi.org/10.1214/ss/1009212519 \nHu LY, Huang MW, Ke SW, Tsai CF (2016) The distance function effect on k-nearest \nneighbor \nclassification \nfor \nmedical \ndatasets. \nSpringerplus. \nhttps://doi.org/10.1186/s40064-016-2941-7 \nHuang G, Huang G Bin, Song S, You K (2015) Trends in extreme learning machines: A \nreview. Neural Networks \nHuang G, Li Y, Pleiss G, et al (2017a) Snapshot Ensembles: Train 1, Get M for Free. In: \nInternational Conference on Learning Representations 2017 \nHuang G, Liu Z, Maaten L van der, Weinberger KQ (2017b) Densely Connected \nConvolutional Networks. In: 2017 IEEE Conference on Computer Vision and \nPattern Recognition (CVPR). IEEE, pp 2261–2269 \nHuang NE, Shen Z, Long SR, et al (1998) The empirical mode decomposition and the \nHubert spectrum for nonlinear and non-stationary time series analysis. Proc R Soc \nA Math Phys Eng Sci. https://doi.org/10.1098/rspa.1998.0193 \nIzmailov P, Podoprikhin D, Garipov T, et al (2018) Averaging weights leads to wider \noptima and better generalization. In: 34th Conference on Uncertainty in Artificial \nIntelligence 2018, UAI 2018 \nJia Deng, Wei Dong, Socher R, et al (2009) ImageNet: A large-scale hierarchical image \ndatabase. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. \nIEEE, pp 248–255 \nJia F, Lei Y, Lin J, et al (2016) Deep neural networks: A promising tool for fault \ncharacteristic mining and intelligent diagnosis of rotating machinery with massive \ndata. Mech Syst Signal Process. https://doi.org/10.1016/j.ymssp.2015.10.025 \nJia Y, Shelhamer E, Donahue J, et al (2014) Caffe: Convolutional architecture for fast \nfeature embedding. In: MM 2014 - Proceedings of the 2014 ACM Conference on \nMultimedia \nJungnickel D (1999) The Greedy Algorithm. pp 129–153 \nKang S, Cho S, Kang P (2015) Constructing a multi-class classifier using one-against-\none \napproach \nwith \ndifferent \nbinary \nclassifiers. \nNeurocomputing. \nhttps://doi.org/10.1016/j.neucom.2014.08.006 \nKhamparia A, Singh A, Anand D, et al (2020) A novel deep learning-based multi-model \nensemble method for the prediction of neuromuscular disorders. Neural Comput \nAppl. https://doi.org/10.1007/s00521-018-3896-0 \nKhan A, Sohail A, Zahoora U, Qureshi AS (2020) A survey of the recent architectures of \ndeep convolutional neural networks. Artif Intell Rev 53:5455–5516. \nhttps://doi.org/10.1007/s10462-020-09825-6 \nKingma DP, Ba JL (2015) Adam: A method for stochastic gradient descent. ICLR Int Conf \nLearn Represent \nKumar PR (2010) Dynamic programming. In: The Control Systems Handbook: Control \nSystem Advanced Methods, Second Edition \nLe Q, Mikolov T (2014) Distributed representations of sentences and documents. In: \n31st International Conference on Machine Learning, ICML 2014 \nLeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521:436–444. \nhttps://doi.org/10.1038/nature14539 \nLeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied to \ndocument recognition. Proc IEEE. https://doi.org/10.1109/5.726791 \nLee D, Karchin R, Beer MA (2011) Discriminative prediction of mammalian enhancers \nfrom DNA sequence. Genome Res. https://doi.org/10.1101/gr.121905.111 \nLi K, Deb K, Zhang Q, Kwong S (2015) An evolutionary many-objective optimization \nalgorithm based on dominance and decomposition. IEEE Trans Evol Comput. \nhttps://doi.org/10.1109/TEVC.2014.2373386 \nLi T, Qian Z, He T (2020) Short-Term Load Forecasting with Improved CEEMDAN and \nGWO-Based \nMultiple \nKernel \nELM. \nComplexity. \nhttps://doi.org/10.1155/2020/1209547 \nLi W, Du Q, Zhang F, Hu W (2016) Hyperspectral Image Classification by Fusing \nCollaborative and Sparse Representations. IEEE J Sel Top Appl Earth Obs Remote \nSens. https://doi.org/10.1109/JSTARS.2016.2542113 \nLIN J, QU L (2000) FEATURE EXTRACTION BASED ON MORLET WAVELET AND ITS \nAPPLICATION FOR MECHANICAL FAULT DIAGNOSIS. J Sound Vib 234:135–148. \nhttps://doi.org/10.1006/jsvi.2000.2864 \nLiu B, Long R, Chou KC (2016a) IDHS-EL: Identifying DNase i hypersensitive sites by \nfusing three different modes of pseudo nucleotide composition into an ensemble \nlearning \nframework. \nBioinformatics. \nhttps://doi.org/10.1093/bioinformatics/btw186 \nLiu W, Wang Z, Liu X, et al (2017a) A survey of deep neural network architectures and \ntheir \napplications. \nNeurocomputing \n234:11–26. \nhttps://doi.org/10.1016/j.neucom.2016.12.038 \nLiu W, Zhang M, Luo Z, Cai Y (2017b) An Ensemble Deep Learning Method for Vehicle \nType Classification on Visual Traffic Surveillance Sensors. IEEE Access 5:24417–\n24425. https://doi.org/10.1109/ACCESS.2017.2766203 \nLiu Y, Gao Z, Chen J (2013) Development of soft-sensors for online quality prediction \nof sequential-reactor-multi-grade industrial processes. Chem Eng Sci. \nhttps://doi.org/10.1016/j.ces.2013.07.002 \nLiu Y, Yang C, Gao Z, Yao Y (2018) Ensemble deep kernel learning with application to \nquality prediction in industrial polymerization processes. Chemom Intell Lab Syst. \nhttps://doi.org/10.1016/j.chemolab.2018.01.008 \nLiu Y, Yao X (1999) Ensemble learning via negative correlation. Neural Networks. \nhttps://doi.org/10.1016/S0893-6080(99)00073-8 \nLiu Z, Xiao X, Yu DJ, et al (2016b) pRNAm-PC: Predicting N6-methyladenosine sites in \nRNA \nsequences \nvia \nphysical-chemical \nproperties. \nAnal \nBiochem. \nhttps://doi.org/10.1016/j.ab.2015.12.017 \nLoshchilov I, Hutter F (2017) SGDR: Stochastic gradient descent with warm restarts. In: \n5th International Conference on Learning Representations, ICLR 2017 - \nConference Track Proceedings \nMa S, Chu F (2019) Ensemble deep learning-based fault diagnosis of rotor bearing \nsystems. Comput Ind. https://doi.org/10.1016/j.compind.2018.12.012 \nMaddox W, Garipov T, Izmailov P, et al (2019) A Simple Baseline for Bayesian \nUncertainty in Deep Learning. Adv Neural Inf Process Syst \nMairal J (2014) Sparse Modeling for Image and Vision Processing \nMallat SG (1989) A Theory for Multiresolution Signal Decomposition: The Wavelet \nRepresentation. \nIEEE \nTrans \nPattern \nAnal \nMach \nIntell. \nhttps://doi.org/10.1109/34.192463 \nMao S, Jiao L, Xiong L, et al (2015) Weighted classifier ensemble based on quadratic \nform. Pattern Recognit. https://doi.org/10.1016/j.patcog.2014.10.017 \nMartinez-Muñoz G, Hernández-Lobato D, Suarez A (2009) An analysis of ensemble \npruning techniques based on ordered aggregation. IEEE Trans Pattern Anal Mach \nIntell. https://doi.org/10.1109/TPAMI.2008.78 \nMasoudnia S, Ebrahimpour R (2014) Mixture of experts: A literature survey. Artif Intell \nRev. https://doi.org/10.1007/s10462-012-9338-y \nMendes-Moreira J, Soares C, Jorge AM, De Sousa JF (2012) Ensemble approaches for \nregression: A survey. ACM Comput. Surv. \nMikolov T, Chen K, Corrado G, Dean J (2013) Efficient estimation of word \nrepresentations in vector space. In: 1st International Conference on Learning \nRepresentations, ICLR 2013 - Workshop Track Proceedings \nMohammed A, Kora R (2021) An effective ensemble deep learning framework for text \nclassification. \nJ \nKing \nSaud \nUniv \n- \nComput \nInf \nSci. \nhttps://doi.org/10.1016/j.jksuci.2021.11.001 \nMohapatra SK, Khilar R, Das A, Mohanty MN (2021) Design of Gradient Boosting \nEnsemble Classifier with Variation of Learning Rate for Automated Cardiac Data \nClassification. In: 2021 8th International Conference on Signal Processing and \nIntegrated Networks (SPIN). IEEE, pp 11–14 \nNaser H (2016) Estimating and forecasting the real prices of crude oil: A data rich \nmodel using a dynamic model averaging (DMA) approach. Energy Econ. \nhttps://doi.org/10.1016/j.eneco.2016.02.017 \nNiemeijer M, Van Ginneken B, Staal J, et al (2005) Automatic detection of red lesions \nin \ndigital \ncolor \nfundus \nphotographs. \nIEEE \nTrans \nMed \nImaging. \nhttps://doi.org/10.1109/TMI.2005.843738 \nOmari A, Figueiras-Vidal AR (2015) Post-aggregation of classifier ensembles. Inf Fusion. \nhttps://doi.org/10.1016/j.inffus.2015.01.003 \nOrlando JI, Prokofyeva E, del Fresno M, Blaschko MB (2018) An ensemble deep \nlearning based approach for red lesion detection in fundus images. Comput \nMethods Programs Biomed. https://doi.org/10.1016/j.cmpb.2017.10.017 \nPage L, Brin S, Motwani R, Winograd T (1998) The PageRank Citation Ranking: Bringing \nOrder \nto \nthe \nWeb. \nWorld \nWide \nWeb \nInternet \nWeb \nInf \nSyst. \nhttps://doi.org/10.1.1.31.1768 \nPanda S, Das A, Mishra S, Mohanty MN (2021) Epileptic Seizure Detection using Deep \nEnsemble Network with Empirical Wavelet Transform. Meas Sci Rev 21:110–116. \nhttps://doi.org/10.2478/msr-2021-0016 \nParisotto E, Ba J, Salakhutdinov R (2016) Actor-mimic deep multitask and transfer \nreinforcement learning. In: 4th International Conference on Learning \nRepresentations, ICLR 2016 - Conference Track Proceedings \nPham BT, Tien Bui D, Prakash I, Dholakia MB (2017) Hybrid integration of Multilayer \nPerceptron Neural Networks and machine learning ensembles for landslide \nsusceptibility assessment at Himalayan area (India) using GIS. Catena. \nhttps://doi.org/10.1016/j.catena.2016.09.007 \nPhillips SJ, Dudík M, Schapire RE (2004) A maximum entropy approach to species \ndistribution modeling. In: Proceedings, Twenty-First International Conference on \nMachine Learning, ICML 2004 \nPrasad R, Deo RC, Li Y, Maraseni T (2018) Soil moisture forecasting by a hybrid machine \nlearning technique: ELM integrated with ensemble empirical mode \ndecomposition. Geoderma. https://doi.org/10.1016/j.geoderma.2018.05.035 \nQiu X, Ren Y, Suganthan PN, Amaratunga GAJ (2017) Empirical Mode Decomposition \nbased ensemble deep learning for load demand time series forecasting. Appl Soft \nComput J. https://doi.org/10.1016/j.asoc.2017.01.015 \nQuinlan \nJR \n(1986) \nInduction \nof \ndecision \ntrees. \nMach \nLearn. \nhttps://doi.org/10.1007/bf00116251 \nQummar S, Khan FG, Shah S, et al (2019) A Deep Learning Ensemble Approach for \nDiabetic \nRetinopathy \nDetection. \nIEEE \nAccess. \nhttps://doi.org/10.1109/ACCESS.2019.2947484 \nRahmati O, Pourghasemi HR, Zeinivand H (2016) Flood susceptibility mapping using \nfrequency ratio and weights-of-evidence models in the Golastan Province, Iran. \nGeocarto Int. https://doi.org/10.1080/10106049.2015.1041559 \nRamsey FL (1974) Characterization of the Partial Autocorrelation Function. Ann Stat \n2:. https://doi.org/10.1214/aos/1176342881 \nRawat W, Wang Z (2017) Deep convolutional neural networks for image classification: \nA comprehensive review. Neural Comput. \nRodríguez JJ, Kuncheva LI, Alonso CJ (2006) Rotation forest: A New classifier ensemble \nmethod. \nIEEE \nTrans \nPattern \nAnal \nMach \nIntell. \nhttps://doi.org/10.1109/TPAMI.2006.211 \nRonneberger O, Fischer P, Brox T (2015) U-Net: Convolutional Networks for \nBiomedical Image Segmentation. In: Lecture Notes in Computer Science \n(including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in \nBioinformatics). pp 234–241 \nRosenblatt (1958) The Perceptron: A Theory of Statistical Separability in Cognitive \nSystems \nRumelhart DE, Hinton GE, Williams RJ (1986) Learning representations by back-\npropagating errors. Nature. https://doi.org/10.1038/323533a0 \nSagi O, Rokach L (2018) Ensemble learning: A survey. Wiley Interdiscip. Rev. Data Min. \nKnowl. Discov. \nSainath TN, Vinyals O, Senior A, Sak H (2015) Convolutional, Long Short-Term Memory, \nfully connected Deep Neural Networks. In: 2015 IEEE International Conference \non Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp 4580–4584 \nSalzberg SL (1994) C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan \nKaufmann \nPublishers, \nInc., \n1993. \nMach \nLearn. \nhttps://doi.org/10.1007/bf00993309 \nSandler M, Howard A, Zhu M, et al (2018) MobileNetV2: Inverted Residuals and Linear \nBottlenecks. In: Proceedings of the IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition \nSB S, Singh V (2012) Automatic Detection of Diabetic Retinopathy in Non-dilated RGB \nRetinal Fundus Images. Int J Comput Appl. https://doi.org/10.5120/7297-0511 \nSchapire \nRE \n(1990) \nThe \nStrength \nof \nWeak \nLearnability. \nMach \nLearn. \nhttps://doi.org/10.1023/A:1022648800760 \nSeoud L, Hurtut T, Chelbi J, et al (2016) Red Lesion Detection Using Dynamic Shape \nFeatures for Diabetic Retinopathy Screening. IEEE Trans Med Imaging. \nhttps://doi.org/10.1109/TMI.2015.2509785 \nShahabi H, Shirzadi A, Ghaderi K, et al (2020) Flood detection and susceptibility \nmapping using Sentinel-1 remote sensing data and a machine learning approach: \nHybrid intelligence of bagging ensemble based on K-Nearest Neighbor classifier. \nRemote Sens. https://doi.org/10.3390/rs12020266 \nShakeel PM, Tolba A, Al-Makhadmeh Z, Jaber MM (2020) Automatic detection of lung \ncancer from biomedical data set using discrete AdaBoost optimized ensemble \nlearning \ngeneralized \nneural \nnetworks. \nNeural \nComput \nAppl. \nhttps://doi.org/10.1007/s00521-018-03972-2 \nShen J, Zhang J, Luo X, et al (2007) Predicting protein-protein interactions based only \non \nsequences \ninformation. \nProc \nNatl \nAcad \nSci \nU \nS \nA. \nhttps://doi.org/10.1073/pnas.0607879104 \nShen Z, He Z, Xue X (2019) MEAL: Multi-Model ensemble via adversarial learning. In: \n33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative \nApplications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI \nSymposium on Educational Advances in Artificial Intelligence, EAAI 2019 \nSmith LN (2017) Cyclical learning rates for training neural networks. In: Proceedings - \n2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017 \nStorn R, Price K (1997) Differential Evolution - A Simple and Efficient Heuristic for \nGlobal \nOptimization \nover \nContinuous \nSpaces. \nJ \nGlob \nOptim. \nhttps://doi.org/10.1023/A:1008202821328 \nSu H, Yu Y, Du Q, Du P (2020) Ensemble Learning for Hyperspectral Image Classification \nUsing Tangent Collaborative Representation. IEEE Trans Geosci Remote Sens. \nhttps://doi.org/10.1109/TGRS.2019.2957135 \nSu H, Zhao B, Du Q, Sheng Y (2016) Tangent Distance-Based Collaborative \nRepresentation for Hyperspectral Image Classification. IEEE Geosci Remote Sens \nLett. https://doi.org/10.1109/LGRS.2016.2578038 \nSuk H Il, Lee SW, Shen D (2017) Deep ensemble learning of sparse regression models \nfor \nbrain \ndisease \ndiagnosis. \nMed \nImage \nAnal. \nhttps://doi.org/10.1016/j.media.2017.01.008 \nSzegedy C, Ioffe S, Vanhoucke V, Alemi AA (2017) Inception-v4, inception-ResNet and \nthe impact of residual connections on learning. In: 31st AAAI Conference on \nArtificial Intelligence, AAAI 2017 \nSzegedy C, Vanhoucke V, Ioffe S, et al (2016) Rethinking the Inception Architecture for \nComputer Vision. In: 2016 IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR). IEEE, pp 2818–2826 \nTan M, Le Q V. (2021) EfficientNetV2: Smaller Models and Faster Training. \nhttps://doi.org/10.48550/arxiv.2104.00298 \nTing KM, Witten IH (1997) Stacking bagged and dagged models. Proc of ICML’97 \nTorres ME, Colominas MA, Schlotthauer G, Flandrin P (2011) A complete ensemble \nempirical mode decomposition with adaptive noise. In: 2011 IEEE International \nConference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp 4144–\n4147 \nVesanto J, Alhoniemi E (2000) Clustering of the self-organizing map. IEEE Trans Neural \nNetworks. https://doi.org/10.1109/72.846731 \nVincent P, Larochelle H, Bengio Y, Manzagol PA (2008) Extracting and composing \nrobust features with denoising autoencoders. In: Proceedings of the 25th \nInternational Conference on Machine Learning \nVincent P, Larochelle H, Lajoie I, et al (2010) Stacked denoising autoencoders: Learning \nUseful Representations in a Deep Network with a Local Denoising Criterion. J \nMach Learn Res \nWang G, Jia R, Liu J, Zhang H (2020) A hybrid wind power forecasting approach based \non Bayesian model averaging and ensemble learning. Renew Energy. \nhttps://doi.org/10.1016/j.renene.2019.07.166 \nWang H, Nie F, Huang H, et al (2011) Identifying AD-sensitive and cognition-relevant \nimaging biomarkers via joint classification and regression. In: Lecture Notes in \nComputer Science (including subseries Lecture Notes in Artificial Intelligence and \nLecture Notes in Bioinformatics) \nWang H zhi, Li G qiang, Wang G bing, et al (2017) Deep learning based ensemble \napproach \nfor \nprobabilistic \nwind \npower \nforecasting. \nAppl \nEnergy. \nhttps://doi.org/10.1016/j.apenergy.2016.11.111 \nWang J, Liu Z, Wu Y, Yuan J (2014) Learning actionlet ensemble for 3D human action \nrecognition. \nIEEE \nTrans \nPattern \nAnal \nMach \nIntell. \nhttps://doi.org/10.1109/TPAMI.2013.198 \nWasserman L (2000) Bayesian model selection and model averaging. J Math Psychol. \nhttps://doi.org/10.1006/jmps.1999.1278 \nWebb GI (2000) MultiBoosting: a technique for combining boosting and wagging. \nMach Learn. https://doi.org/10.1023/A:1007659514849 \nWei L, Chen H, Su R (2018) M6APred-EL: A Sequence-Based Predictor for Identifying \nN6-methyladenosine Sites Using Ensemble Learning. Mol Ther - Nucleic Acids. \nhttps://doi.org/10.1016/j.omtn.2018.07.004 \nWold S, Esbensen K, Geladi P (1987) Principal component analysis. Chemom Intell Lab \nSyst 2:37–52. https://doi.org/10.1016/0169-7439(87)80084-9 \nWolpert \nDH \n(1992) \nStacked \ngeneralization. \nNeural \nNetworks. \nhttps://doi.org/10.1016/S0893-6080(05)80023-1 \nWu X, Kumar V, Ross QJ, et al (2008) Top 10 algorithms in data mining. Knowl Inf Syst. \nhttps://doi.org/10.1007/s10115-007-0114-2 \nWU Z, HUANG NE (2009) ENSEMBLE EMPIRICAL MODE DECOMPOSITION: A NOISE-\nASSISTED DATA ANALYSIS METHOD. Adv Adapt Data Anal 01:1–41. \nhttps://doi.org/10.1142/S1793536909000047 \nXia J-F, Han K, Huang D-S (2009) Sequence-Based Prediction of Protein-Protein \nInteractions by Means of Rotation Forest and Autocorrelation Descriptor. Protein \nPept Lett. https://doi.org/10.2174/092986610789909403 \nXia J, Yokoya N, Iwasaki A (2017) A novel ensemble classifier of hyperspectral and \nLiDAR data using morphological features. In: ICASSP, IEEE International \nConference on Acoustics, Speech and Signal Processing - Proceedings \nXiao Y, Wu J, Lin Z, Zhao X (2018) A deep learning-based multi-model ensemble \nmethod for cancer prediction. Comput Methods Programs Biomed 153:1–9. \nhttps://doi.org/10.1016/j.cmpb.2017.09.005 \nXie S, Girshick R, Dollar P, et al (2017) Aggregated Residual Transformations for Deep \nNeural Networks. In: 2017 IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR). IEEE, pp 5987–5995 \nXu G, Liu M, Jiang Z, et al (2019) Bearing Fault Diagnosis Method Based on Deep \nConvolutional Neural Network and Random Forest Ensemble Learning. Sensors \n19:1088. https://doi.org/10.3390/s19051088 \nXu Y, Yang W, Wang J (2017) Air quality early-warning system for cities in China. Atmos \nEnviron. https://doi.org/10.1016/j.atmosenv.2016.10.046 \nYang Y, Chen N, Jiang S (2018) Collaborative strategy for visual object tracking. \nMultimed Tools Appl 77:7283–7303. https://doi.org/10.1007/s11042-017-4633-\nx \nYang Y, Lv H, Chen N, et al (2021) Local minima found in the subparameter space can \nbe effective for ensembles of deep convolutional neural networks. Pattern \nRecognit 109:107582. https://doi.org/10.1016/j.patcog.2020.107582 \nYang Y, Lv H, Chen N, et al (2020a) FTBME: feature transferring based multi-model \nensemble. \nMultimed \nTools \nAppl \n79:18767–18799. \nhttps://doi.org/10.1007/s11042-020-08746-4 \nYang Y, Wu Y, Chen N (2019) Explorations on visual localization from active to passive. \nMultimed Tools Appl 78:2269–2309. https://doi.org/10.1007/s11042-018-6347-\n0 \nYang Y, Yang Y, Chen J, et al (2020b) Handling Noisy Labels via One-Step Abductive \nMulti-Target Learning: An Application to Helicobacter Pylori Segmentation \nYang Y, Yang Y, Yuan Y, et al (2020c) Detecting helicobacter pylori in whole slide images \nvia weakly supervised multi-task learning. Multimed Tools Appl 79:26787–26815. \nhttps://doi.org/10.1007/s11042-020-09185-x \nYou ZH, Lei YK, Zhu L, et al (2013) Prediction of protein-protein interactions from \namino acid sequences with ensemble extreme learning machines and principal \ncomponent analysis. BMC Bioinformatics. https://doi.org/10.1186/1471-2105-\n14-S8-S10 \nZagaglia P (2010) Macroeconomic factors and oil futures prices: A data-rich model. \nEnergy Econ. https://doi.org/10.1016/j.eneco.2009.11.003 \nZhang CX, Zhang JS (2011) A survey of selective ensemble learning algorithms. Jisuanji \nXuebao/Chinese J. Comput. \nZhang D, Shen D (2012) Multi-modal multi-task learning for joint prediction of multiple \nregression and classification variables in Alzheimer’s disease. Neuroimage. \nhttps://doi.org/10.1016/j.neuroimage.2011.09.069 \nZhang L, Shi Z, Cheng MM, et al (2021) Nonlinear Regression via Deep Negative \nCorrelation \nLearning. \nIEEE \nTrans \nPattern \nAnal \nMach \nIntell. \nhttps://doi.org/10.1109/TPAMI.2019.2943860 \nZhang P, He Z (2015) Using data-driven feature enrichment of text representation and \nensemble technique for sentence-level polarity classification. J Inf Sci. \nhttps://doi.org/10.1177/0165551515585264 \nZhang X, Wang J, Zhang K (2017) Short-term electric load forecasting based on singular \nspectrum analysis and support vector machine optimized by Cuckoo search \nalgorithm. Electr Power Syst Res. https://doi.org/10.1016/j.epsr.2017.01.035 \nZhao Y, Li J, Yu L (2017) A deep learning ensemble approach for crude oil price \nforecasting. Energy Econ. https://doi.org/10.1016/j.eneco.2017.05.023 \nZhou ZH (2012) Ensemble methods: Foundations and algorithms \nZhou ZH (2009) Ensemble Learning. In: Encyclopedia of Biometrics \nZhou ZH, Wu J, Tang W (2002) Ensembling neural networks: Many could be better than \nall. Artif Intell. https://doi.org/10.1016/S0004-3702(02)00190-X \nZhu C, Bichot CE, Chen L (2010) Multi-scale color local binary patterns for visual object \nclasses recognition. In: Proceedings - International Conference on Pattern \nRecognition \nZoph B, Vasudevan V, Shlens J, Le Q V. (2018) Learning Transferable Architectures for \nScalable Image Recognition. In: 2018 IEEE/CVF Conference on Computer Vision \nand Pattern Recognition. IEEE, pp 8697–8710 \n \n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "A.1"
  ],
  "published": "2021-01-21",
  "updated": "2022-09-28"
}