{
  "id": "http://arxiv.org/abs/2101.11707v1",
  "title": "Knowledge-driven Natural Language Understanding of English Text and its Applications",
  "authors": [
    "Kinjal Basu",
    "Sarat Varanasi",
    "Farhad Shakerin",
    "Joaquin Arias",
    "Gopal Gupta"
  ],
  "abstract": "Understanding the meaning of a text is a fundamental challenge of natural\nlanguage understanding (NLU) research. An ideal NLU system should process a\nlanguage in a way that is not exclusive to a single task or a dataset. Keeping\nthis in mind, we have introduced a novel knowledge driven semantic\nrepresentation approach for English text. By leveraging the VerbNet lexicon, we\nare able to map syntax tree of the text to its commonsense meaning represented\nusing basic knowledge primitives. The general purpose knowledge represented\nfrom our approach can be used to build any reasoning based NLU system that can\nalso provide justification. We applied this approach to construct two NLU\napplications that we present here: SQuARE (Semantic-based Question Answering\nand Reasoning Engine) and StaCACK (Stateful Conversational Agent using\nCommonsense Knowledge). Both these systems work by \"truly understanding\" the\nnatural language text they process and both provide natural language\nexplanations for their responses while maintaining high accuracy.",
  "text": "Knowledge-driven Natural Language Understanding\nof English Text and its Applications∗\nKinjal Basu1, Sarat Varanasi1, Farhad Shakerin1, Joaquin Arias2 and Gopal Gupta1\n1Department of Computer Science\n2Artiﬁcial Intelligence Research Group\nThe University of Texas at Dallas, USA\nUniversidad Rey Juan Carlos, Madrid, Spain\nAbstract\nUnderstanding the meaning of a text is a funda-\nmental challenge of natural language understand-\ning (NLU) research. An ideal NLU system should\nprocess a language in a way that is not exclusive to\na single task or a dataset. Keeping this in mind, we\nhave introduced a novel knowledge driven seman-\ntic representation approach for English text. By\nleveraging the VerbNet lexicon, we are able to map\nsyntax tree of the text to its commonsense mean-\ning represented using basic knowledge primitives.\nThe general purpose knowledge represented from\nour approach can be used to build any reasoning\nbased NLU system that can also provide justiﬁca-\ntion. We applied this approach to construct two\nNLU applications that we present here: SQuARE\n(Semantic-based Question Answering and Rea-\nsoning Engine) and StaCACK (Stateful Conver-\nsational Agent using Commonsense Knowledge).\nBoth these systems work by “truly understanding”\nthe natural language text they process and both\nprovide natural language explanations for their re-\nsponses while maintaining high accuracy.\nIntroduction\nThe long term goal of natural language understanding\n(NLU) research is to make applications, e.g., chatbots\nand question answering (QA) systems, that act exactly\nlike a human assistant. A human assistant will under-\nstand the user’s intent and fulﬁll the task. The task\ncan be answering questions about a story, giving direc-\ntions to a place, or reserving a table in a restaurant by\nknowing user’s preferences. Human level understand-\ning of natural language is needed for an NLU applica-\ntion that aspires to act exactly like a human. To un-\nderstand the meaning of a natural language sentence,\nhumans ﬁrst process the syntactic structure of the sen-\ntence and then infer its meaning. Also, humans use com-\nmonsense knowledge to understand the often complex\nand ambiguous meaning of natural language sentences.\nHumans interpret a passage as a sequence of sentences\n∗Authors partially supported by NSF grants IIS 1718945,\nIIS 1910131, and IIP 1916206\nCopyright © 2021, Association for the Advancement of Ar-\ntiﬁcial Intelligence (www.aaai.org). All rights reserved.\nand will normally process the events in the story in the\nsame order as the sentences. Once humans understand\nthe meaning of a passage, they can answer questions\nposed, along with an explanation for the answer. More-\nover, by using commonsense, a human assistant under-\nstands the user’s intended task and asks questions to\nthe user about the required information to successfully\ncarry-out the task. Also, to hold a goal oriented con-\nversation, a human remembers all the details given in\nthe past and most of the time performs non-monotonic\nreasoning to accomplish the assigned task. We believe\nthat an automated QA system or a goal oriented closed\ndomain chatbot should work in a similar way.\nIf we want to build AI systems that emulate humans,\nthen understanding natural language sentences is the\nforemost priority for any NLU application. In an ideal\nscenario, an NLU application should map the sentence\nto the knowledge (semantics) it represents, augment it\nwith commonsense knowledge related to the concepts\ninvolved–just as humans do—then use the combined\nknowledge to do the required reasoning. In this pa-\nper, we introduce our novel algorithm for automatically\ngenerating the semantics corresponding to each English\nsentence using the comprehensive verb-lexicon for En-\nglish verbs - VerbNet (Kipper et al. 2008). For each\nEnglish verb, VerbNet gives the syntactic and seman-\ntic patterns. The algorithm employs partial syntactic\nmatching between parse-tree of a sentence and a verb’s\nframe syntax from VerbNet to obtain the meaning of\nthe sentence in terms of VerbNet’s primitive predicates.\nThis matching is motivated by denotational seman-\ntics of programming languages and can be thought of as\nmapping parse-trees of sentences to knowledge that is\nconstructed out of semantics provided by VerbNet. The\nVerbNet semantics is expressed using a set of primi-\ntive predicates that can be thought of as the semantic\nalgebra of the denotational semantics.\nWe also show two applications of our approach.\nSQuARE, a question answering system for reading com-\nprehension, is capable of answering various types of rea-\nsoning questions asked about a passage. SQuARE uses\nknowledge in the passage augmented with commonsense\nknowledge. Going a step further, we leverage SQuARE\nto build a general purpose closed-domain goal-oriented\narXiv:2101.11707v1  [cs.CL]  27 Jan 2021\nchatbot framework - StaCACK (pronounced as stack).\nOur work reported here builds upon our prior work in\nnatural language QA as well as visual QA (Pendharkar\nand Gupta 2019; Basu, Shakerin, and Gupta 2020;\nBasu et al. 2020).\nBackground and Contribution\nWe next describe some of the key technologies we em-\nploy. Our eﬀort is based on answer set programming\n(ASP) technology (Gelfond and Kahl 2014), speciﬁcally,\nits goal-directed implementation in the s(CASP) sys-\ntem. ASP supports nonmonotonic reasoning through\nnegation as failure that is crucial for modeling com-\nmonsense reasoning (via defaults, exceptions and pref-\nerences) (Gelfond and Kahl 2014). We assume that the\nreader is familiar with ASP (Gelfond and Kahl 2014),\ndenotational semantics (Schmidt 1986) and English lan-\nguage parsers (Stanford CoreNLP (Manning et al. 2014)\nand spaCy (Honnibal and Montani 2017)). We give a\nbrief overview of ASP and denotational semantics.\nASP: Answer Set Programming (ASP) is a declar-\native logic programming language that extends it\nwith negation-as-failure. ASP is a highly expressive\nparadigm that can elegantly express complex reason-\ning methods, including those used by humans, such as\ndefault reasoning, deductive and abductive reasoning,\ncounterfactual reasoning, constraint satisfaction (Baral\n2003; Gelfond and Kahl 2014). ASP supports better\nsemantics for negation (negation as failure) than does\nstandard logic programming and Prolog. An ASP pro-\ngram consists of rules that look like Prolog rules. The\nsemantics of an ASP program Π is given in terms\nof the answer sets of the program ground(Π), where\nground(Π) is the program obtained from the substitu-\ntion of elements of the Herbrand universe for variables\nin Π (Baral 2003). The rules in an ASP program are of\nthe form:\np :- q1, ..., qm, not r1, ..., not rn.\nwhere m ≥0 and n ≥0. Each of p and qi (∀i ≤m)\nis a literal, and each not rj (∀j ≤n) is a naf-literal\n(not is a logical connective called negation-as-failure or\ndefault negation). The literal not rj is true if proof of\nrj fails. Negation as failure allows us to take actions\nthat are predicated on failure of a proof. Thus, the rule\nr :- not s. states that r can be inferred if we fail to\nprove s. Note that in the rule above, the head literal p\nis optional. A headless rule is called a constraint which\nstates that conjunction of qi’s and not rj’s should yield\nfalse.\nThe declarative semantics of an Answer Set Program\nP is given via the Gelfond-Lifschitz transform (Baral\n2003; Gelfond and Kahl 2014) in terms of the answer\nsets of the program ground(Π). ASP also supports\nclassical negation. A classically negated predicate (de-\nnoted -p means that p is deﬁnitely false. Its deﬁni-\ntion is no diﬀerent from a positive predicate, in that\nexplicit rules have to be given to establish -p. More\nNP V NP\nExample\n“She grabbed the rail”\nSyntax\nAgent V Theme\nSemantics\nContinue(E,Theme), Cause(Agent, E)\nContact(During(E),Agent,Theme)\nFigure 1: VerbNet frame instance for the verb class grab\ndetails on ASP can be found elsewhere (Baral 2003;\nGelfond and Kahl 2014).\nThe goal in ASP is to compute an answer set given an\nanswer set program, i.e., compute the set that contains\nall propositions that if set to true will serve as a model\nof the program (those propositions that are not in the\nset are assumed to be false). Intuitively, the rule above\nsays that p is in the answer set if q1, ..., qm are in\nthe answer set and r1, ..., rn are not in the answer\nset. ASP can be thought of as Prolog extended with\na sound semantics of negation-as-failure that is based\non the stable model semantics (Gelfond and Lifschitz\n1988).\ns(CASP) System: s(CASP) (Arias et al. 2018) is a\nquery-driven, goal-directed implementation of ASP that\nincludes constraint solving over reals. Goal-directed ex-\necution of s(CASP) is indispensable for automating\ncommonsense reasoning, as traditional grounding and\nSAT-solver based implementations of ASP may not be\nscalable. There are three major advantages of using\nthe s(CASP) system: (i) s(CASP) does not ground the\nprogram, which makes our framework scalable, (ii) it\nonly explores the parts of the knowledge base that are\nneeded to answer a query, and (iii) it provides natural\nlanguage justiﬁcation (proof tree) for an answer (Arias\net al. 2020).\nDenotational Semantics: In programming language\nresearch, denotational semantics is a widely used ap-\nproach to formalize the meaning of a programming\nlanguage in terms of mathematical objects (called do-\nmains, such as integers, truth-values, tuple of values,\nand, mathematical functions) (Schmidt 1986). Denota-\ntional semantics of a programming language has three\ncomponents (Schmidt 1986):\n1. Syntax: speciﬁed as abstract syntax trees.\n2. Semantic Algebra: these are the basic domains\nalong with the associated operations; meaning of a\nprogram is expressed in terms of these basic opera-\ntions applied to the elements in the domain.\n3. Valuation Function: these are mappings from ab-\nstract syntax trees (and possibly the semantic alge-\nbra) to values in the semantic algebra.\nGiven a program P written in language L, P’s deno-\ntation (meaning), expressed in terms of the semantic\nalgebra, is obtained by applying the valuation function\nof L to program P’s syntax tree. Details can be found\nelsewhere (Schmidt 1986).\nVerbNet: Inspired by Beth Levin’s classiﬁcation of\nverbs and their syntactic alternations (Levin 1993),\nVerbNet (Kipper et al. 2008) is the largest online net-\nwork of English verbs. A verb class in VerbNet is mainly\nexpressed by syntactic frames, thematic roles, and se-\nmantic representation. The VerbNet lexicon identiﬁes\nthematic roles and syntactic patterns of each verb class\nand infers the common syntactic structure and seman-\ntic relations for all the member verbs. Figure 1 shows\nan example of a VerbNet frame of the verb class grab.\nThis paper makes the following novel contributions:\n(i) it presents a domain independent English text to\nanswer set program generator, (ii) demonstrates two\nrobust, scalable, and interpretable applications of our\nsemantic-driven approach, SQuARE and StaCACK,\nboth of which demonstrate improved performance over\nmachine learning (ML) based systems with regards to\naccuracy and explainability, and (iii) shows how the\ns(CASP) query-driven ASP system is crucial for com-\nmonsense reasoning as it guarantees a correct answer\nif the knowledge representation is accurate. Our work\nis based purely on reasoning and does not require any\nmanual intervention other than providing (reusable)\ncommonsense knowledge coded in ASP. It paves the\nway for developing advanced NLU systems based on\n“truly understanding” text or human dialog.\nSemantics driven ASP Code Generation\nSimilar to the denotational approach for meaning repre-\nsentation of a programming language, an ideal NLU sys-\ntem should use denotational semantics to composition-\nally map text syntax to its meaning. Knowledge primi-\ntives should be represented using the semantic algebra\n(Schmidt 1986) of well understood concepts. Then the\nsemantics along with the commonsense knowledge rep-\nresented using the same semantic algebra can be used\nto construct diﬀerent NLU applications, such as QA\nsystem, chatbot, information extraction system, text\nsummarization, etc. The ambiguous nature of natural\nlanguage is the main hurdle in treating it as a program-\nming language. English is no exception and the mean-\ning of an English word or sentence may depend on the\ncontext. The algorithm we present takes the syntactic\nparse tree of an English sentence and uses VerbNet to\nautomatically map the parse tree to its denotation, i.e.,\nthe knowledge it represents.\nStanford CoreNLP\nParser\nSemantic Generator\n(Valuation Function\ncontact(during(grab),agent(john),theme(the_apple)).\ncontinue(event(grab),theme(the_apple)).\ntransfer(during(grab),theme(the_apple)).\ncause(agent(john),event(grab)).\n...\n...\nSentence\n(John grabbed \nthe apple there)\nVerbNet\nVerbNet\nFrames\nVerb\n(grab)\nSentence Semantics\nRepresented in ASP\nParse Tree\nFigure 2: English to ASP translation process\nAn English sentence that consists of an action verb\n(i.e., not a be verb) always describes an event. The verb\nalso constrains the relation among the event partici-\npants. VerbNet encapsulates all of this information us-\ning verb classes that represent a verb set with similar\nmeanings. So each verb is a part of one or more classes.\nFor each class, it provides the skeletal parse tree (frame\nsyntax) for diﬀerent usage of the verb class and the re-\nspective semantics (frame semantic). The semantic deﬁ-\nnition of each frame uses pre-deﬁned predicates of Verb-\nNet that have thematic-roles (AGENT, THEME, etc.)\nas arguments. Thus, we can imagine VerbNet as a very\nlarge valuation (semantic) function that maps syntax\ntree patterns to their respective meanings. As we use\nASP to represent the knowledge, the algorithm gen-\nerates the sentence’s semantic deﬁnition in ASP. Our\ngoal is to ﬁnd the partial matching between the sentence\nparse tree and the VerbNet frame syntax and ground the\nthematic-role variables so that we can get the semantics\nof the sentence from the frame semantics and represent\nit in ASP.\nThe illustration of the process of semantic knowledge\ngeneration from a sentence is described in the Figure 2.\nWe have used Stanford’s CoreNLP parser (Manning et\nal. 2014) to generate the parse tree, pt, of an English\nsentence. The semantic generator component consists of\nthe valuation function to map the pt to its meaning. To\naccomplish this, we have introduced Semantic Knowl-\nedge Generation algorithm (Algorithm 1). First, the al-\ngorithm collects the list of verbs mentioned in the sen-\ntence and for each verb it accumulates all the syntactic\n(frame syntax) and corresponding semantic information\n(thematic roles and predicates) from VerbNet using the\nverb-class of the verb. The algorithm ﬁnds the grounded\nthematic-role variables by doing a partial tree match-\ning (described in Algorithm 2) between each gathered\nframe syntax and pt. From the verb node of pt, the\npartial tree matching algorithm performs a bottom-up\nsearch and, at each level through a depth-ﬁrst traversal,\nit tries to match the skeletal parse tree of the frame syn-\ntax. If the algorithm ﬁnds an exact match or a partial\nmatch (by skipping words, e.g., prepositions), it returns\nthe thematic roles to the parent Algorithm 1. Finally,\nAlgorithm 1 grounds the pre-deﬁned predicate with the\nvalues of thematic roles and generates the ASP code.\nThe ASP code generated by the above mentioned ap-\nproach represents the meaning of a sentence comprised\nof an action verb. Since VerbNet does not cover the se-\nmantics of the ‘be’ verbs (i.e., am, is, are, have, etc.), for\nsentences containing ‘be’ verbs, the semantic generator\nuses pre-deﬁned handcrafted mapping of the parsed in-\nformation (i.e., syntactic parse tree, dependency graph,\netc.) to its semantics. Also, this semantics is represented\nas ASP code. The generated ASP code can now be used\nin various applications, such as natural language QA,\nsummarization, information extraction, CA, etc.\nAlgorithm 1 Semantic Knowledge Generation\nInput: pt: constituency parse tree of a sentence\nOutput: semantics: sentence semantics\n1: procedure GetSentenceSemantics(pt)\n2:\nverbs ←getVerbs(pt)\n▷returns list of verbs\npresent in the sentence\n3:\nsemantics ←{}\n▷initialization\n4:\nfor each v ∈verbs do\n5:\nclasses ←getVNClasses(v)\n▷get the VerbNet\nclasses of the verb\n6:\nfor each c ∈classes do\n7:\nframes ←getVNFrames(c)\n▷get the\nVerbNet frames of the class\n8:\nfor each f ∈frames do\n9:\nthematicRoles ←\ngetThematicRoles(pt, f.syntax, v)\n▷see Algorithm 2\n10:\nsemantics ←semantics\n∪\ngetSemantics(thematicRoles, f.semantics)\n11:\n▷map the thematic roles into the frame semantics\n12:\nend for\n13:\nend for\n14:\nend for\n15:\nreturn semantics\n16: end procedure\nAlgorithm 2 Partial Tree Matching\nInput: pt: constituency parse tree of a sentence; s:\nframe syntax; v: verb\nOutput: tr: thematic role set or empty-set: {}\n1: procedure GetThematicRoles(pt, s, v)\n2:\nroot ←getSubTree(node(v), pt)\n▷returns the\nsub-tree from the parent of the verb node\n3:\nwhile root do\n4:\ntr ←getMatching(root, s)\n▷if s matches the\ntree return thematic-roles, else {}\n5:\nif tr ̸= {} then return tr\n6:\nend if\n7:\nroot ←getSubTree(root, pt)\n▷returns false if\nroot equals pt\n8:\nend while\n9:\nreturn {}\n10: end procedure\nThe SQuARE System\nQuestion answering system for reading comprehension\nis a challenging task for the NLU research community.\nIn recent times with the advancement of ML applied\nto NLU, researchers have created more advance QA\nsystems that show outstanding performance in QA for\nreading-comprehension tasks. However, for these high\nperforming neural-networks based agents, the question\nrises whether they really “understand” the text or not.\nThese systems are outstanding in learning data patterns\nand then predicting the answers that require shallow or\nno reasoning capabilities. Moreover, for some QA task,\nif a system claims that it performs equal or better than\na human in terms of accuracy, then the system must\nalso show human level intelligence in explaining its an-\nswers. Taking all this into account, we have created our\nSQuARE QA system that uses ML based parser to gen-\nerate the syntax tree and uses Algorithm 1 to translate a\nsentence into its knowledge in ASP. By using the ASP-\ncoded knowledge along with pre-deﬁned generic com-\nmonsense knowledge, SQuARE outperforms other ML\nbased systems by achieving 100% accuracy in 18 tasks\n(99.9% accuracy in all 20 tasks) of the bAbI QA dataset\n(note that the 0.01% inaccuracy is due to the dataset’s\nﬂaw, not of our system). SQuARE is also capable of\ngenerating English justiﬁcation of its answers.\nNatural Language\nProcessor\n(CoreNLP & spaCy)\nText\nQuestion\nSemantic\nGenerator\nASP Query\nGenerator\nValuation Function\ns(CASP) \nEngine\nSyntactic\nParse Tree\nSyntactic\nParse Tree\nSemantic\nKnowledge in\nASP\nASP Query\nAnswer\n(Text)\n(Question)\nCommonsense Knowledge\nFigure 3: SQuARE Framework\nArchitecture: SQuARE is composed of two main sub\nsystems: the semantic generator and the ASP query\ngenerator. Both subsystems inside the SQuARE archi-\ntecture (illustrated in Figure 3) share the common val-\nuation function. To parse the passage and the ques-\ntion asked, the CoreNLP and spaCy parsers are used\nto generate the syntactic parse tree as well as the nec-\nessary parsing information such as NER, POS tags,\nlemmas, etc. Our semantic generator employs the pro-\ncess described earlier for ASP code generation from En-\nglish sentences. The ASP query generator uses the same\nsemantic generation algorithm with minor changes to\nadapt to the fact that the sentence is a question. It\nidentiﬁes the query variable along with the question\ntype and other necessary details (e.g., NER, POS, de-\npendency graph, etc.) from the parsed question, and\nthen ASP query is formulated. For a given task, a gen-\neral query rule is deﬁned that is a collection of sub-\nqueries needed to answer the question. Such a rule can\nbe considered as the process-template that a human\nwill follow to achieve the same task, thus such process\ntemplates are a part of commonsense knowledge. To get\nthe answer, the s(CASP) executes the query against the\nknowledge generated from the semantic-generator and\nthe pre-deﬁned, reusable commonsense knowledge.\nSQuARE is also capable of reasoning over time using\nthe order of the events in the passage. A passage or a\nstory is a collection of sentences and the meaning of the\nwhole passage comprises of meanings of the individual\nsentences. Knowledge is represented using defaults, ex-\nceptions and preferences which permits non-monotonic\nreasoning, needed because conclusion drawn early in a\nstory may have to be revised later. SQuARE assumes\nthe events in the story occur sequentially unless it en-\ncounters an exception.\nCommonsense Concepts:\nJust\nlike\nhumans,\nSQuARE uses predeﬁned commonsense knowledge\nto understand the context. The dataset independent\ncommonsense\nknowledge\nis\nwritten\nusing\ngeneric\npredicate names so that it can be reused without any\nchanges. This knowledge is presented either as facts or\nas default rules with exceptions. The facts are concrete\nglobal truth of the world, such as property(color,\nwhite). Whereas, default rules capture the normal\nrelationship among two entities in the world or the\ngeneral laws of the world, such as the law-of-inertia.\nUsing negation as failure (NAF), these rules capture\nexceptions to defaults as well. NAF helps us to reason\neven if information is missing. Following default rule\n(used in task 20) illustrates that a thirsty person\nnormally drinks unless there is an exception (e.g., a\nmedical test needs to be performed which requires\nperson does not drink any ﬂuid in last 1 hour).\naction(X,drink) :- person(X),\nemotional_state(X,\nthirsty),\nnot ab_action(X,drink).\nSimilarly, to have rudimentary human reasoning capa-\nbilities, SQuARE employs basic commonsense compu-\ntational rules for counting objects, ﬁltering, searching,\netc. An example is given in the next section.\nExample: To demonstrate the power of the SQuARE\nsystem, we next discuss a full-ﬂedged example showing\nthe data-ﬂow and the intermediate results.\nStory: A customized segment of a story from the bAbI\nQA dataset about counting objects (Task-7) is taken.\n1 John moved to the bedroom.\n2 John got the football there.\n3 John grabbed the apple there.\n4 John picked up the milk there.\n5 John gave the apple to Mary.\n6 John left the football.\nParsed Output:\nCoreNLP and spaCy parsers parse\neach sentence of the story and passes the parsed infor-\nmation to the semantic generator. Details are omitted\ndue to lack of space, however, parsing can be easily done\nat https://corenlp.run/.\nSemantics: From the parsed information, the seman-\ntic generator generates the semantic knowledge in ASP.\nWe only give a snippet of knowledge (due to space con-\nstraint) generated from the third sentence of the story\n(VerbNet details of the verb - grab is given in Figure 1).\n1 contact(t3,during(grab),agent(john),\ntheme(the_apple)).\n2 cause(t3,agent(john),event(grab)).\n3 transfer(t3,during(grab),theme(the_apple)).\nQuestion and ASP Query:\nFor the question - “How\nmany objects is John carrying?”, the ASP query genera-\ntor generates a generic query-rule and the speciﬁc ASP\nquery (it uses the process template for counting).\ncount_object(T,Per,Count) :-\nfindall(O, property(possession,T,Per,O), Os),\nset(Os,Objects), list_length(Objects,Count).\n?- count_object(t6,john,Count).\nAnswer:\nThe s(CASP) ﬁnds the correct answer - 1\nJustiﬁcation:\nThe s(CASP) generated justiﬁcation\nfor this answer is shown below:\n The total count of all the objects that john is possessing at time t6 is 1, because\n[the_milk] is the list of all the objects that are possessed by john at time t6, \n                                                                                                            because\nthe_milk is possessed by john at time t6, because\ntime t6 comes after time t5, and\nthe_milk is possessed by john at time t5, because\ntime t5 comes after time t4, and\nthe_milk is possessed by john at time t4, and\nthere is no evidence that the_milk is not possessed by john at time t5.\nthere is no evidence that the_milk is not possessed by john at time t6.\nThe list [the_milk] is generated after removing duplicates from the list [the_milk],\n                                                                                                             because\nThe list [] is generated after removing duplicates from the list [].\n1 is the length of the list [the_milk], because\n0 is the length of the list [].\nStaCACK Framework\nConversational AI has been an active area of research,\nstarting from a rule-based system, such as ELIZA\n(Weizenbaum 1966) and PARRY (Colby, Weber, and\nHilf 1971), to the recent open domain, data-driven CAs\nlike Amazon’s Alexa, Google Assistant, or Apple’s Siri.\nEarly rule-based bots were based on just syntax analy-\nsis, while the main challenge of modern ML based chat-\nbots is the lack of “understanding” of the conversation.\nA realistic socialbot should be able to understand and\nreason like a human. In human to human conversations,\nwe do not always tell every detail, we expect the listener\nto ﬁll gaps through their commonsense knowledge. Also,\nour thinking process is ﬂexible and non-monotonic in\nnature, which means “what we believe today may become\nfalse in the future with new knowledge”. We can model\nthis human thinking process with (i) default rules, (ii)\nexceptions to defaults, and (iii) preferences over multi-\nple defaults (Gelfond and Kahl 2014).\nFollowing the discussion above, we have created Sta-\nCACK, a general closed-domain chatbot framework.\nStaCACK is a stateful framework that maintains states\nby remembering every past dialog between the user and\nitself. The main diﬀerence between StaCACK and the\nother stateful or stateless chatbot models is the use\nof commonsense knowledge for understanding user ut-\nterances and generating responses. Moreover, it is ca-\npable of doing non-monotonic reasoning by using de-\nfaults with exception and preferences in ASP. StaCACK\nachieves 100% accuracy on the Facebook bAbI dialog\ndataset suit (Bordes, Boureau, and Weston 2016) (in-\ncluding OOV: out-of-vocabulary datasets) of ﬁve tasks\ncreated for a restaurant reservation dialog system. In\naddition, StaCACK can answer questions that ML chat-\nbots cannot without proper training (details are given\nin following sections). We focus on agents that are de-\nsigned for a speciﬁc tasks (e.g., restaurant reservation).\nUser UnsaƟsﬁed\nStart\nUnderstand user intent\nAsk preferences based on the intent\nVerify and update query\nNoƟfy No Results\nThank you greeƟngs\nProvide result(s)\nExecute query\nComplete task and give details\nEnd\nIntent\nAsk other details\nComplete InformaƟon\nNo Updates\nResults\nUser SaƟsﬁed\nNo Results\nNo more \ndetails\nHas Preference \nUpdates\nNo-Updates\nNo\nMore details\nHas Updates\nIncomplete \nInformaƟon\nYes\nFigure 4: FSM for StaCACK framework\nFinite State Machine: Task-speciﬁc CAs follow a\ncertain scheme in their inquiry that can be modeled as\na ﬁnite state machine (FSM). The FSM is illustrated\nin Figure 4. However, the tasks in each state transi-\ntion are not simple as in every level it requires diﬀerent\ntypes of (commonsense) reasoning. We have tested our\nStaCACK framework on bAbI dialog dataset that deals\nwith only one user intent - restaurant table reservation.\nThe whole end-to-end conversation, starting from un-\nderstanding the user-intent to completing the reserva-\ntion, is divided into four diﬀerent tasks of the dataset\n- (a) issuing API calls (to actually make the reserva-\ntion), (b) updating API calls, (c) Displaying options,\nand, (d) providing extra information. Therefore, if a\nsystem can complete these tasks, then it should be able\nto hold the whole conversation, that is given in the ﬁfth\ndataset. However, using StaCACK, the implementation\nof the agent for these tasks become straightforward. We\nneed to add commonsense knowledge about the domain\n(discussed later). Parsing of the sentences is done with\nspaCy and CoreNLP parsers and the parse tree is trans-\nlated into knowledge using our Algorithm 1.\nCommonsense knowledge: Similar to the SQuARE\nsystem, StaCACK augments knowledge generated from\nuser utterances with commonsense knowledge. For a\ntask based, general purpose CA, the commonsense\nknowledge or the background knowledge helps in un-\nderstanding user-intent, missing information, and user\npreferences. For example, following rules illustrate how\nan agent can ﬁnd all the missing parameter’s value in\nthe knowledge base, which later on can be used to ask\ncounter-questions to the user to facilitate reasoning.\nmissing_parameter(X) :- query_parameter(X),\nnot query_parameter_value(X,_).\nall_missing_parameter(ParaList) :-\nfindall(P,missing_parameter(P),ParaList).\nDue to the basis of StaCACK in commonsense knowl-\nedge, its reasoning can be more sophisticated and it can\nhold conversation in a more natural manner (described\nin the next section) than ML based systems. Following\nis an example. Rule-1 depicts that normally a person\nwho has consumed a type of cuisine (e.g., Mexican) the\nday before, would not like to eat it again today. How-\never, there can be exceptions to this rule.\n1 cuisine_exception(P, C) :- property(person, P),\nproperty(cuisine, C), time(yesterday,P,C),\nnot ab_cusine_exception(P,C).\n2 cuisine_suggestion(P,C) :- property(cuisine,C),\nnot cuisine_exception(P, C),\nnot ab_cuisine_suggestion(P,C).\nExample: As we have already mentioned that the Sta-\nCACK framework shows excellent performance on the\nbAbI dialog dataset based on restaurant reservation\nconversation. Also, it is able to hold the conversation in\na more natural way by using commonsense knowledge,\nwhich may not be possible with a rule-based system\nbased on a monotonic logic. Following example shows\nhow StaCACK can understand the cuisine preference of\na user, just by performing reasoning over commonsense\ninformation about a cuisine (that curry is predominant\nin Indian and Thai cuisine).\nUser:\nGood morning\nStaCACK: Hello what can i help you with today\nUser:\nCan you make a restaurant reservation in\nLondon in a cheap price range\nStaCACK: I’m on it\nUser:\n<SILENCE>\nStaCACK: any preference on a type of cuisine\nUser:\nanything, except Lebanese food\nStaCACK: do you want to have Chinese food\nUser:\nI want to have curry\nStaCACK: Do you like Indian or Thai\nUser:\nThai\nPlease note that due to the space constraint, we have\nshown a conversation snippet here and how the end-\nto-end conversation looks like can be found elsewhere\n(Bordes, Boureau, and Weston 2016).\nExperiments and Results\nDatasets: The SQuARE and the StaCACK system\nhave been tested on the bAbI QA (Weston and others\n2015) and the bAbI dialog dataset respectively (Bordes,\nBoureau, and Weston 2016). With the aim of improving\nNLU research, Facebook researchers have created the\nbAbI datasets suit that comprise with diﬀerent NLU\napplication oriented simple task based datasets. The\ndatasets are designed in such a way that it becomes\neasy for human to reason and reach an answer with\nproper justiﬁcation whereas diﬃcult for machines due\nto the lack of understanding about the language.\nThese datasets are mainly created to train and test\ndeep-learning based NLU applications. The bAbI QA\ndataset not only expects the ML models to train and\ntest on the provided question answer pair for 20 reason-\ning based tasks but also presumes the model will learn\nthe pattern of supporting facts given with each answer.\nTaking this into account, we choose this dataset to test\nSQuARE system to show its “true understanding” by\ngenerating natural language justiﬁcation for each an-\nswer. Similarly, we have evaluated the StaCACK ap-\nproach on the bAbI dialog dataset to exhibit how we\ncan build a closed domain task based chatbot with com-\nmonsense knowledge. Another reason to choose these\ndatasets is its simplicity that helps us to concentrate\nmore on knowledge representation and modeling than\npre-processing or parsing of English sentences.\nExperiments: In the SQuARE system, the accuracy\nhas been calculated by matching the generated answer\nwith the actual answer given in the bAbI QA dataset.\nWhereas, StaCACK’s accuracy is calculated on the ba-\nsis of per-response as well as per-dialog. Table 1 sum-\nmarizes the testing statistics and performance metrics\nof the SQuARE system (benchmarks are tested on a\nintel i9-9900 CPU with 16G RAM ). Table 2 and table\n3 compares our results in terms of accuracy with the\nexisting state-of-the-art results for SQuARE and Sta-\nCACK system respectively.\nError Analysis: SQuARE is not able to reach 100%\naccuracy on two tasks - the three argument relations\n(Task 5) and the indeﬁnite knowledge (Task 10). We\nhave studied the error stories and the particular ques-\ntions where the system goes wrong. Thanks to the in-\nterpretable nature of the SQuARE system, we are able\nto identify the scenarios.\nTask 5: This error occurs because of the multiple cor-\nrect answers present in the story with no indication\nof which one is the preferred one. SQuARE system is\ncapable of ﬁnding all the answers and all are correct,\nthough bAbI (erroneously) assumes there is only one\ncorrect answer. Illustration of the scenario is given be-\nlow, where, clearly both milk and apple are answers,\nbut bAbI only accepts milk.\n1 Fred passed the milk to Bill.\n2 Fred went back to the bedroom.\n3 Fred took the apple there.\n4 Fred gave the apple to Bill.\n5 What did Fred give to Bill?\nmilk\nTask 10: The erroneous stories have two identical\nplaces with an or as a person’s location. So, for the\nquestion about that person’s location, SQuARE an-\nswers correctly, whereas the actual (erroneous) answer\nis ‘maybe’. SQuARE correctly infers that p ∨p equals\np. Following example illustrates the scenario.\n1 Fred is either in the cinema or the cinema.\n2 Is Fred in the cinema?\nmaybe\nThese errors show another advantage of our explainable\nlogic-based approach: SQuARE is able to identify errors\nin the dataset. On the contrary, the ML systems mimic\nthe dataset by learning the errors as well and show their\nshallow understanding.\nComparison of Results and Related Works: Us-\ning VerbNet to study the semantic relations of verbs\nand their semantic roles are not new. Text2DRS system\n(Ling 2018) uses VerbNet to understand the discourse\nof a passage and represent it in the Neo-Davidsonian\nform. Schmitz et al. have also used VerbNet to de-\nsign open information extraction system (Schmitz et\nal. 2012). Researchers have also used VerbNet to ex-\ntend the commonsense knowledge about verbs. For in-\nstance, McFate (McFate 2010) describes how VerbNet\ncan be used to expand the CYC ontology by adding\nverb semantic frames. However all these approaches are\nlimited to a particular domain or an application, and\nnot scalable as well, whereas our semantic driven En-\nglish sentence to ASP code generation approach is fully\nautomatic process and independent of any application.\nTable 2 compares our result in terms of accuracy\nwith other models for all the 20 bAbI tasks for the\nSQuARE system. Note that due to the space constraint,\nwe have shown comparison with two best performing\nTasks\nAttributes\nNo. of\nStories\nTot. No. of\nQuestions\nAvg. Questions\nper Story\nTot. No. of\nSentences\nAvg. Story Size\n(No. of Sentences)\nMax Story Size\n(No. of Sentences)\nAccuracy\n(%)\nAvg. Time\nper Question\n(Seconds)\nSingle Supporting Facts\n2200\n11000\n5\n22000\n10\n10\n100\n15.4\nTwo Supporting Facts\n2200\n11000\n5\n48390\n22\n88\n100\n42.8\nThree Supporting Facts\n2200\n11000\n5\n163496\n74\n320\n100\n147.1\nTwo Argument Relations\n11000\n11000\n1\n22000\n2\n2\n100\n0.6\nThree Argument Relations\n2200\n11000\n5\n59822\n27\n126\n99.8\n0.8\nYes/No Questions\n2200\n11000\n5\n22662\n10\n26\n100\n2.1\nCounting\n2200\n11000\n5\n28514\n13\n52\n100\n3\nLists/Sets\n2200\n11000\n5\n29394\n13\n58\n100\n5\nSimple Negation\n2200\n11000\n5\n22000\n10\n10\n100\n1.2\nIndeﬁnite Knowledge\n2200\n11000\n5\n22000\n10\n10\n98.2\n1.2\nBasic Coreference\n2200\n11000\n5\n22000\n10\n10\n100\n0.5\nConjunction\n2200\n11000\n5\n22000\n10\n10\n100\n0.6\nCompound Coreference\n2200\n11000\n5\n22000\n10\n10\n100\n0.6\nTime Reasoning\n2200\n11000\n5\n25756\n12\n14\n100\n0.6\nBasic Deduction\n2750\n11000\n4\n22000\n8\n8\n100\n0.4\nBasic Induction\n11000\n11000\n1\n99000\n9\n9\n100\n0.8\nPositional Reasoning\n1375\n11000\n8\n2750\n2\n2\n100\n0.3\nSize Reasoning\n2177\n11000\n5\n13655\n6\n20\n100\n0.4\nPath Finding\n11000\n11000\n1\n55000\n5\n5\n100\n0.8\nAgent’s Motivations\n1026\n11000\n11\n11000\n11\n12\n100\n0.4\nTotals\n68,928\n220,000\n735,439\nTable 1: Dataset Statistics and Performance Results\nsystems (memory-neural-network (MemNN) with adap-\ntive memory (AM), N-grams (NG), and non-linearity\n(NL); and the system by Mitra and Baral) and the ac-\ncuracies on other models (i.e., N-gram classiﬁer, LSTM,\nSVM, DMN, etc.) can be found elsewhere (Weston and\nothers 2015; Kumar and others 2016). The SQuARE\nsystem beats all these ML based systems in terms of\naccuracy and explainability.\nMitra’s system is the one closest to ours and moti-\nvated us to work on the bAbI QA dataset. Their work\nachieves high accuracy, however, the work is still a ML\nbased inductive logic programming system that requires\nmanual annotation of data, such as mode declaration,\nﬁnding group size, etc. Conversely, the SQuARE sys-\ntem relies fully on automatic reasoning with only man-\nual encoding of reusable commonsense knowledge. An\naction language based QA methodology using VerbNet\nhas been developed by Lierler et al (Lierler, Inclezan,\nand Gelfond 2017). The project aims to extend frame\nsemantics with ALM, an action language (Lierler, In-\nclezan, and Gelfond 2017), to provide interpretable se-\nmantic annotations. Unlike SQuARE, it is not an end-\nto-end automated QA system.\nTable 3 shows the accuracy of our proposal, Sta-\nCACK, against the best models on the bAbI dataset\nin terms of per-response and in parenthesis in terms of\nper-dialog: Mem2Seq (Madotto, Wu, and Fung 2018),\nand BoSsNET (Raghu, Gupta, and others 2018). Other\nresults can be found elsewhere (Bordes, Boureau, and\nWeston 2016). Unsurprisingly, similar to the rule-based\nsystem, StaCACK surpasses all the ML based models\nby showing 100% accuracy. Nevertheless, due to the\ncommonsense reasoning, StaCACK can hold better nat-\nural conversation (shown in the Example section of Sta-\nCACK) that is not possible with a standard rule-based\nsystem based on monotonic logic.\nTasks\nModel\nMemNN\n(AM+NG+NL)\nMitra\net al.\nSQuARE\nSingle Supporting Fact\n100\n100\n100\nTwo Supporting Facts\n98\n100\n100\nThree Supporting Facts\n95\n100\n100\nTwo Arg. Relation\n100\n100\n100\nThree Arg. Relation\n99\n100\n99.8\nYes/No Questions\n100\n100\n100\nCounting\n97\n100\n100\nLists/Sets\n97\n100\n100\nSimple Negation\n100\n100\n100\nIndeﬁnite Knowledge\n98\n100\n98.2\nBasic Coreference\n100\n100\n100\nConjunction\n100\n100\n100\nCompound Coreference\n100\n100\n100\nTime Reasoning\n100\n100\n100\nBasic Deduction\n100\n100\n100\nBasic Induction\n99\n93.6\n100\nPositional Reasoning\n60\n100\n100\nSize Reasoning\n95\n100\n100\nPath Finding\n35\n100\n100\nAgent’s Motivations\n100\n100\n100\nMEAN ACCURACY\n94\n100\n100\nTable 2: SQuARE accuracy (%) comparison\nMem2Seq\nBossNet\nStaCACK\nTask 1\n100 (100)\n100 (100)\n100 (100)\nTask 2\n100 (100)\n100 (100)\n100 (100)\nTask 3\n94.7 (62.1)\n95.2 (63.8)\n100 (100)\nTask 4\n100 (100)\n100 (100)\n100 (100)\nTask 5\n97.9 (69.6)\n97.3 (65.6)\n100 (100)\nTask 1 (OOV)\n94.0 (62.2)\n100 (100)\n100 (100)\nTask 2 (OOV)\n86.5 (12.4)\n100 (100)\n100 (100)\nTask 3 (OOV)\n90.3 (38.7)\n95.7 (66.6)\n100 (100)\nTask 4 (OOV)\n100 (100)\n100 (100)\n100 (100)\nTask 5 (OOV)\n84.5 (2.3)\n91.7 (18.5)\n100 (100)\nTable 3: Accuracy per response (per dialog) in %.\nDiscussion\nOur goal is to create NLU applications that mimics\nthe way human understand natural language. Humans\nunderstand a passage’s meaning and use commonsense\nknowledge to logically reason to ﬁnd a response to a\nquestion. We believe that this is the most eﬀective pro-\ncess to create an NLU application. Learning and reason-\ning both are integral parts of human intelligence. Today,\nML research dominates AI. Most state-of-the-art CA or\nQA systems have been developed using ML techniques\n(e.g., LSTM, GRU, Attention Network, Transformers,\netc.). The systems that are built with these techniques\nlearn the patterns of the training text remarkably well\nand shows promising results on test data. With the re-\ncent advancements in the language model research, the\npre-trained models such as BERT (Devlin et al. 2018)\nand GPT-3 (Brown et al. 2020) have outstanding ca-\npability of generating natural languages. These rapid\nevolutions in the NLU world showcase some extremely\nsophisticated text predictor. These are used to build a\nchatbot or a QA system that can generate correct re-\nsponses by exploiting the correlation among words and\nwithout properly understanding the content. These ML\ntechniques are extremely powerful in tasks where learn-\ning of hidden data patterns is needed, such as machine\ntranslation, sentiment analysis, syntactic parsing, etc.\nHowever, they fail to generate proper responses where\nreasoning is required and they mostly do not employ\ncommonsense knowledge. Also, the black-box nature of\ntheses models makes their response non-explainable. In\nother words, these models do not possess any internal\nmeaning representation of a sentence or a word and have\nno semantically-grounded model of the world. So, it will\nbe an injustice to say that they understand their in-\nputs and outputs in any meaningful way. Our semantic\nknowledge generation approach and its two applications\nare a step toward mimicking a human assistant. We be-\nlieve that, to obtain truly intelligent behavior, ML and\ncommonsense reasoning should work in tandem.\nCompared to ML-based QA systems and CAs, our\napproach has many advantages. It produces correct re-\nsponses by truly understanding the text and reasoning\nabout it, rather than by using patterns learned from\ntraining examples. Also, ML-based systems are more\nlikely to produce incorrect response, if not trained ap-\npropriately, resulting in vulnerability under adversarial\nattacks (Chan et al. 2018). We believe that our com-\nmonsense reasoning based systems are more resilient.\nScalability is also an issue due to the dependence on\ntraining data. Explainability is a necessary feature that\na truly intelligent system must possess. Both SQuARE\nand StaCACK are capable of generating natural lan-\nguage justiﬁcations for the responses they produce.\nFuture Work and Conclusion\nWe presented our novel semantics-driven English text\nto answer set program generator. Also, we showed how\ncommonsense reasoning coded in ASP can be lever-\naged to develop advanced NLU applications, such as\nSQuARE and StaCACK. We make use of the s(CASP)\nengine, a query-driven implementation of ASP, to per-\nform reasoning while generating an natural language\nexplanation for any computed answer. As part of fu-\nture work, we plan to extend the SQuARE system to\nhandle more complex sentences and eventually handle\ncomplex stories. Our goal is also to develop an open-\ndomain conversational AI chatbot based on automated\ncommonsense reasoning that can “converse” with a hu-\nman based on “truly understanding” that person’s dia-\nlog.\nReferences\n[Arias et al. 2018] Arias, J.; Carro, M.; Salazar, E.;\nMarple, K.; and Gupta, G. 2018. Constraint answer set\nprogramming without grounding. TPLP 18(3-4):337–\n354.\n[Arias et al. 2020] Arias, J.; Carro, M.; Chen, Z.; and\nGupta, G.\n2020.\nJustiﬁcations for goal-directed\nconstraint answer set programming.\narXiv preprint\narXiv:2009.10238.\n[Baral 2003] Baral, C. 2003. Knowledge representation,\nreasoning and declarative problem solving. Cambridge\nUniversity Press.\n[Basu et al. 2020] Basu, K.; Varanasi, S. C.; Shakerin,\nF.; and Gupta, G. 2020. SQuARE: Semantics-based\nQuestion Answering and Reasoning Engine. In Proc.\n36th ICLP (Technical Communications), volume 325 of\nEPTCS, 73–86.\n[Basu, Shakerin, and Gupta 2020] Basu, K.; Shakerin,\nF.; and Gupta, G.\n2020.\nAQuA: ASP-Based Visual\nQuestion Answering. In Practical Aspects of Declara-\ntive Languages, 57–72. Cham: Springer International\nPublishing.\n[Bordes, Boureau, and Weston 2016] Bordes,\nA.;\nBoureau, Y.-L.; and Weston, J.\n2016.\nLearning\nend-to-end\ngoal-oriented\ndialog.\narXiv\npreprint\narXiv:1605.07683.\n[Brown et al. 2020] Brown, T. B.; Mann, B.; Ryder, N.;\nSubbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.;\nShyam, P.; Sastry, G.; Askell, A.; et al.\n2020.\nLan-\nguage models are few-shot learners.\narXiv preprint\narXiv:2005.14165.\n[Chan et al. 2018] Chan, A.; Ma, L.; Juefei-Xu, F.; Xie,\nX.; Liu, Y.; and Ong, Y. S. 2018. Metamorphic rela-\ntion based adversarial attacks on diﬀerentiable neural\ncomputer. arXiv preprint arXiv:1809.02444.\n[Colby, Weber, and Hilf 1971] Colby, K. M.; Weber, S.;\nand Hilf, F. D.\n1971.\nArtiﬁcial paranoia.\nArtiﬁcial\nIntelligence 2(1):1–25.\n[Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.;\nand Toutanova, K. 2018. BERT: Pre-training of deep\nbidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805.\n[Gelfond and Kahl 2014] Gelfond, M., and Kahl, Y.\n2014. Knowledge representation, reasoning, and the de-\nsign of intelligent agents: The answer-set programming\napproach. Cambridge University Press.\n[Gelfond and Lifschitz 1988] Gelfond, M., and Lifschitz,\nV. 1988. The stable model semantics for logic program-\nming. In ICLP/SLP, volume 88, 1070–1080.\n[Honnibal and Montani 2017] Honnibal, M., and Mon-\ntani, I. 2017. spaCy 2: Natural language understanding\nwith Bloom embeddings, convolutional neural networks\nand incremental parsing. To appear.\n[Kipper et al. 2008] Kipper, K.; Korhonen, A.; Ryant,\nN.; and Palmer, M. 2008. A large-scale classiﬁcation\nof english verbs. Language Resources and Evaluation\n42(1):21–40.\n[Kumar and others 2016] Kumar, A., et al. 2016. Ask\nme anything: Dynamic memory networks for natural\nlanguage processing. In ICML, 1378–1387.\n[Levin 1993] Levin, B. 1993. English verb classes and\nalternations: A preliminary investigation. U. Chicago\nPress.\n[Lierler, Inclezan, and Gelfond 2017] Lierler,\nY.;\nIn-\nclezan, D.; and Gelfond, M. 2017. Action languages\nand question answering. In IWCS 2017—12th Interna-\ntional Conference on Computational Semantics—Short\npapers.\n[Ling 2018] Ling, G.\n2018.\nFrom Narrative Text to\nVerbNet-based DRSes: System Text2DRS.\n[Madotto, Wu, and Fung 2018] Madotto, A.; Wu, C.-S.;\nand Fung, P. 2018. Mem2Seq: Eﬀectively incorporat-\ning knowledge bases into end-to-end task-oriented dia-\nlog systems. arXiv preprint arXiv:1804.08217.\n[Manning et al. 2014] Manning, C. D.; Surdeanu, M.;\nBauer, J.; Finkel, J.; Bethard, S. J.; and McClosky, D.\n2014.\nThe Stanford CoreNLP NLP toolkit.\nIn ACL\nSystem Demonstrations, 55–60.\n[McFate 2010] McFate, C. 2010. Expanding verb cover-\nage in Cyc with VerbNet. In Proceedings of the ACL\n2010 Student Research Workshop, 61–66.\n[Pendharkar and Gupta 2019] Pendharkar,\nD.,\nand\nGupta, G. 2019. An ASP Based Approach to Answer-\ning Questions for Natural Language Text.\nIn Proc.\nPractical Aspects of Declarative Languages, Springer\nLNCS 11372, pp. 46-63, volume 11372 of Lecture Notes\nin Computer Science, 46–63. Springer.\n[Raghu, Gupta, and others 2018] Raghu, D.; Gupta, N.;\net al. 2018. Disentangling language and knowledge in\ntask-oriented dialogs. arXiv preprint arXiv:1805.01216.\n[Schmidt 1986] Schmidt, D. A.\n1986.\nDenotational\nsemantics: a methodology for language development,\nWilliam C. Brown Publishers, Dubuque, IA, USA.\n[Schmitz et al. 2012] Schmitz, M.; Soderland, S.; Bart,\nR.; Etzioni, O.; et al. 2012. Open language learning\nfor information extraction. In Proceedings of the 2012\nJoint Conference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Language\nLearning, 523–534.\n[Weizenbaum 1966] Weizenbaum, J.\n1966.\nELIZA—a\ncomputer program for the study of natural language\ncommunication between man and machine.\nCACM\n9(1):36–45.\n[Weston and others 2015] Weston, J., et al. 2015. To-\nwards AI-Complete Question Answering: A Set of Pre-\nrequisite Toy Tasks. arXiv preprint arXiv:1502.05698.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LO"
  ],
  "published": "2021-01-27",
  "updated": "2021-01-27"
}