{
  "id": "http://arxiv.org/abs/2101.11861v3",
  "title": "Symmetric equilibrium of multi-agent reinforcement learning in repeated prisoner's dilemma",
  "authors": [
    "Yuki Usui",
    "Masahiko Ueda"
  ],
  "abstract": "We investigate the repeated prisoner's dilemma game where both players\nalternately use reinforcement learning to obtain their optimal memory-one\nstrategies. We theoretically solve the simultaneous Bellman optimality\nequations of reinforcement learning. We find that the Win-stay Lose-shift\nstrategy, the Grim strategy, and the strategy which always defects can form\nsymmetric equilibrium of the mutual reinforcement learning process amongst all\ndeterministic memory-one strategies.",
  "text": "arXiv:2101.11861v3  [cs.GT]  21 May 2021\nSymmetric equilibrium of multi-agent reinforcement\nlearning in repeated prisoner’s dilemma\nYuki Usui\nFaculty of Science, Yamaguchi University, Yamaguchi 753-8511, Japan\nMasahiko Ueda\nGraduate School of Sciences and Technology for Innovation, Yamaguchi University,\nYamaguchi 753-8511, Japan\nAbstract\nWe investigate the repeated prisoner’s dilemma game where both players alter-\nnately use reinforcement learning to obtain their optimal memory-one strategies.\nWe theoretically solve the simultaneous Bellman optimality equations of rein-\nforcement learning. We ﬁnd that the Win-stay Lose-shift strategy, the Grim\nstrategy, and the strategy which always defects can form symmetric equilib-\nrium of the mutual reinforcement learning process amongst all deterministic\nmemory-one strategies.\nKeywords:\nRepeated prisoner’s dilemma game; Reinforcement learning\n1. Introduction\nThe prisoner’s dilemma game describes a dilemma where rational behavior of\neach player cannot achieve a favorable situation for both players [1]. In the game,\neach player chooses cooperation or defection. Each player can obtain more payoﬀ\nby taking defection than by taking cooperation regardless of the opponent’s\naction. Then, mutual defection is realized as a result of rational thought of\nboth players, while payoﬀs of both players increase when both players choose\ncooperation. Although the Nash equilibrium of the one-shot game is mutual\nEmail addresses: i007de@yamaguchi-u.ac.jp (Yuki Usui), m.ueda@yamaguchi-u.ac.jp\n(Masahiko Ueda)\nPreprint submitted to Elsevier\nMay 24, 2021\ndefection, when the game is inﬁnitely repeated, it has been known that mutual\ncooperation can be achieved as the Nash equilibrium. This fact is known as the\nfolk theorem. Because the repeated version of the prisoner’s dilemma game is\nalso simple, it has substantially been investigated [2].\nRecently, reinforcement learning technique attracts much attentions in the\ncontext of game theory [3, 4, 5, 6, 7, 8, 9, 10, 11]. In reinforcement learning,\na player gradually learns his/her optimal strategy against his/her opponents.\nBoth learning by a single player and learning by several players have been in-\nvestigated. Because rationality of players is bounded in reality, modeling of\nplayers as learning agents is crucial [12]. It is also signiﬁcant in the context\nof reinforcement learning, since the original reinforcement learning was formu-\nlated for Markov decision process with stationary environments [13]. Because\nthe existence of multiple agents in game theory leads to non-stationarity of envi-\nronments for each player, the standard application of reinforcement learning to\ngames breaks down [4, 14], and further theoretical understanding of reinforce-\nment learning in game theory is needed. Moreover, since the acquisition process\nof optimal strategies in reinforcement learning is generally diﬀerent from that\nin evolutionary game theory [15], accumulating knowledge about equilibrium in\neach learning dynamics is needed.\nIn this paper, we investigate the situation where both players alternately\nlearn their optimal strategies by using reinforcement learning in the repeated\nprisoner’s dilemma game. We theoretically derive equilibrium points of mutual\nreinforcement learning where both players take the same deterministic strategy.\nWe ﬁnd that the strategy which always defects (All-D), the Win-stay Lose-\nShift (WSLS) strategy [16], and the Grim strategy can form such symmetric\nequilibrium amongst all memory-one deterministic strategies.\nThis paper is organized as follows. In Section 2, we introduce the repeated\nprisoner’s dilemma game, and players using reinforcement learning. In Section\n3, we theoretically derive deterministic optimal strategies against the strategy\nof a learning opponent. In Section 4, we provide numerical results by using\nQ-learning which support our theoretical results. Section 5 is devoted to con-\n2\nclusion.\n2. Model\nWe consider the repeated prisoner’s dilemma game [3]. There are two players\nin the game, and each player is described as 1 and 2.\nEach player chooses\ncooperation (C) or defection (D) on every trial.\nThe action of player a is\nwritten as σa ∈{C, D}, and we collectively write σ := (σ1, σ2). The payoﬀof\nplayer a ∈{1, 2} when the state is σ is described as ra (σ). The payoﬀs in the\nprisoner’s dilemma game are deﬁned as\n\nr1 (C, C) , r2 (C, C)\nr1 (C, D) , r2 (C, D)\nr1 (D, C) , r2 (D, C)\nr1 (D, D) , r2 (D, D)\n\n\n=\n\nR, R\nS, T\nT, S\nP, P\n\n(1)\nwith T > R > P > S and 2R > T + S. We consider the situation where\nboth players use memory-one strategies. The memory-one strategy of player a\nis described as the conditional probability Ta (σa|σ′) of taking action σa when\nthe state in the previous round is σ′. (In this paper, we investigate only the\ngame with perfect monitoring, where players can perfectly observe the actions\nof both players in the previous round.) Then, when we deﬁne the probability\ndistribution of a state σ′ at time t by P (σ′, t), the time evolution of this system\nis described as the Markov chain\nP (σ, t + 1)\n=\nX\nσ′\nT (σ|σ′) P (σ′, t)\n(2)\nwith the transition probability\nT (σ|σ′)\n:=\n2\nY\na=1\nTa (σa|σ′) .\n(3)\nBelow we introduce the notation −a := {1, 2}\\a.\nWe consider the situation that both players learn their strategies by re-\ninforcement learning [13]. We assume that two players alternately learn and\nupdate their strategies [17], that is, player 1 ﬁrst learns her strategy against\na ﬁxed initial strategy of player 2, then player 2 learns his strategy against\n3\nthe strategy of player 1, then player 1 learns her strategy against the strategy\nof player 2, and so on. In other words, the two players inﬁnitely repeat the\ninﬁnitely repeated game, and their strategies are updated after each repeated\ngame is played and their long-term payoﬀs are calculated. We assume that the\nstrategy of player 1 is updated in n-th game with n = 2m −1 (m ∈N) and the\nstrategy of player 2 is updated in n-th game with n = 2m (m ∈N). We write\nthe strategies of player a at n-th game as T (n)\na\n(σa|σ′).\nIn reinforcement learning, each player learns mapping (called policy) from a\nstate to his/her action so as to maximize his/her expected future reward. In our\nmemory-one situation, a state and an action of player a are regarded as the state\nσ′ in the previous round and the action σa in the present round, respectively.\nWe deﬁne the action-value function of player a as\nQa\n\u0010\nσ(1)\na , σ(0)\u0011\n:=\nE\n\" ∞\nX\nk=0\nγkra(t + k + 1)\n\f\f\f\f\f σa(t + 1) = σ(1)\na , σ(t) = σ(0)\n#\n,\n(4)\nwhere γ is a discounting factor satisfying 0 ≤γ < 1. The action σa(t) represents\nthe action of player a at round t. Similarly, the payoﬀra(t) represents the payoﬀ\nof player a at round t, that is, ra(t) := ra (σ(t)). Due to the Markov property,\nthe action-value function Q obeys the Bellman equation against a ﬁxed strategy\nT−a of the opponent:\nQa\n\u0010\nσ(1)\na , σ(0)\u0011\n=\nX\nσ(1)\n−a\nT−a\n\u0010\nσ(1)\n−a|σ(0)\u0011\nra\n\u0010\nσ(1)\u0011\n+γ\nX\nσ(2)\na\nX\nσ(1)\n−a\nTa\n\u0010\nσ(2)\na |σ(1)\u0011\nT−a\n\u0010\nσ(1)\n−a|σ(0)\u0011\nQa\n\u0010\nσ(2)\na , σ(1)\u0011\n.\n(5)\nIt has been known that the optimal value of Q obeys the following Bellman\noptimality equation:\nQ∗\na\n\u0010\nσ(1)\na , σ(0)\u0011\n=\nX\nσ(1)\n−a\nT−a\n\u0010\nσ(1)\n−a|σ(0)\u0011\nra\n\u0010\nσ(1)\u0011\n+ γ\nX\nσ(1)\n−a\nT−a\n\u0010\nσ(1)\n−a|σ(0)\u0011\nmax\nσ(2)\na\nQ∗\na\n\u0010\nσ(2)\na , σ(1)\u0011\n(6)\n4\nwith the support\nsuppTa\n\u0010\n·| σ(0)\u0011\n=\narg max\nσ\nQ∗\na\n\u0010\nσ, σ(0)\u0011\n.\n(7)\nIn other words, in the optimal policy against T−a, player a takes the action σa\nwhich maximizes the value of Q∗\na\n\u0000·, σ(0)\u0001\nwhen the state at the previous round\nis σ(0).\nIn sum, in the (2m −1)-th game, player 1 learns T (2m−1)\n1\n(σ1|σ′) against\nT (2m−2)\n2\n(σ2|σ′) by calculating Q∗(2m−1)\n1\n\u0000σ, σ(0)\u0001\n, where Q∗(n)\na\n\u0000σ, σ(0)\u0001\nrepre-\nsents the optimal action-value function of player a in the n-th game. In the\n2m-th game, player 2 learns T (2m)\n2\n(σ2|σ′) against T (2m−1)\n1\n(σ1|σ′) by calculat-\ning Q∗(2m)\n2\n\u0000σ, σ(0)\u0001\n. We are interested in the ﬁxed points of the dynamics, that\nis, T (∞)\na\n(σa|σ′) and Q∗(∞)\na\n\u0000σ, σ(0)\u0001\n.\nIn this paper, we investigate only situations that the support (7) contains\nonly one action, that is, we investigate only deterministic strategies. Because\nthe number of deterministic memory-one strategies in the repeated prisoner’s\ndilemma game is sixteen, we check whether each deterministic strategy forms\nequilibrium or not.\n3. Results\nWe consider symmetric solutions of Eq. (6), that is,\nQ∗\n1\n\u0010\nσ(1)\n1 , σ(0)\u0011\n=\nX\nσ(1)\n2\nT2\n\u0010\nσ(1)\n2 |σ(0)\u0011\nr1\n\u0010\nσ(1)\u0011\n+ γ\nX\nσ(1)\n2\nT2\n\u0010\nσ(1)\n2 |σ(0)\u0011\nmax\nσ(2)\n1\nQ∗\n1\n\u0010\nσ(2)\n1 , σ(1)\u0011\n(8)\nwith\nT2 (C|C, C)\n=\nI (Q∗\n1(C, C, C) > Q∗\n1(D, C, C))\n(9)\nT2 (C|C, D)\n=\nI (Q∗\n1(C, D, C) > Q∗\n1(D, D, C))\n(10)\nT2 (C|D, C)\n=\nI (Q∗\n1(C, C, D) > Q∗\n1(D, C, D))\n(11)\nT2 (C|D, D)\n=\nI (Q∗\n1(C, D, D) > Q∗\n1(D, D, D))\n(12)\n5\nwhere I(· · · ) is the indicator function that returns 1 when · · · holds and 0\notherwise. Then, Eq. (6) becomes\nQ∗\n1 (C, C, C)\n=\nI (Q∗\n1(C, C, C) > Q∗\n1(D, C, C))\nn\nR + γ max\nσ\nQ∗\n1(σ, C, C)\no\n+I (Q∗\n1(C, C, C) < Q∗\n1(D, C, C))\nn\nS + γ max\nσ\nQ∗\n1(σ, C, D)\no\n(13)\nQ∗\n1 (C, C, D)\n=\nI (Q∗\n1(C, D, C) > Q∗\n1(D, D, C))\nn\nR + γ max\nσ\nQ∗\n1(σ, C, C)\no\n+I (Q∗\n1(C, D, C) < Q∗\n1(D, D, C))\nn\nS + γ max\nσ\nQ∗\n1(σ, C, D)\no\n(14)\nQ∗\n1 (C, D, C)\n=\nI (Q∗\n1(C, C, D) > Q∗\n1(D, C, D))\nn\nR + γ max\nσ\nQ∗\n1(σ, C, C)\no\n+I (Q∗\n1(C, C, D) < Q∗\n1(D, C, D))\nn\nS + γ max\nσ\nQ∗\n1(σ, C, D)\no\n(15)\nQ∗\n1 (C, D, D)\n=\nI (Q∗\n1(C, D, D) > Q∗\n1(D, D, D))\nn\nR + γ max\nσ\nQ∗\n1(σ, C, C)\no\n+I (Q∗\n1(C, D, D) < Q∗\n1(D, D, D))\nn\nS + γ max\nσ\nQ∗\n1(σ, C, D)\no\n(16)\nQ∗\n1 (D, C, C)\n=\nI (Q∗\n1(C, C, C) > Q∗\n1(D, C, C))\nn\nT + γ max\nσ\nQ∗\n1(σ, D, C)\no\n+I (Q∗\n1(C, C, C) < Q∗\n1(D, C, C))\nn\nP + γ max\nσ\nQ∗\n1(σ, D, D)\no\n(17)\nQ∗\n1 (D, C, D)\n=\nI (Q∗\n1(C, D, C) > Q∗\n1(D, D, C))\nn\nT + γ max\nσ\nQ∗\n1(σ, D, C)\no\n+I (Q∗\n1(C, D, C) < Q∗\n1(D, D, C))\nn\nP + γ max\nσ\nQ∗\n1(σ, D, D)\no\n(18)\nQ∗\n1 (D, D, C)\n=\nI (Q∗\n1(C, C, D) > Q∗\n1(D, C, D))\nn\nT + γ max\nσ\nQ∗\n1(σ, D, C)\no\n+I (Q∗\n1(C, C, D) < Q∗\n1(D, C, D))\nn\nP + γ max\nσ\nQ∗\n1(σ, D, D)\no\n(19)\n6\nQ∗\n1 (D, D, D)\n=\nI (Q∗\n1(C, D, D) > Q∗\n1(D, D, D))\nn\nT + γ max\nσ\nQ∗\n1(σ, D, C)\no\n+I (Q∗\n1(C, D, D) < Q∗\n1(D, D, D))\nn\nP + γ max\nσ\nQ∗\n1(σ, D, D)\no\n.\n(20)\nFor simplicity, we introduce the following notation:\nq1\n:=\nQ∗\n1 (C, C, C)\nq2\n:=\nQ∗\n1 (C, C, D)\nq3\n:=\nQ∗\n1 (C, D, C)\nq4\n:=\nQ∗\n1 (C, D, D)\nq5\n:=\nQ∗\n1 (D, C, C)\nq6\n:=\nQ∗\n1 (D, C, D)\nq7\n:=\nQ∗\n1 (D, D, C)\nq8\n:=\nQ∗\n1 (D, D, D) .\n(21)\nWe consider the following sixteen situations separately.\n3.1. Case 1: q1 > q5, q2 > q6, q3 > q7, and q4 > q8\nFor this case, the strategy obtained by reinforcement learning is the All-C\nstrategy. The solution of Eq. (6) is\nq1 = q2 = q3 = q4\n=\n1\n1 −γ R\n(22)\nq5 = q6 = q7 = q8\n=\nT +\nγ\n1 −γ R.\n(23)\nThis contradicts with the deﬁnition of the game T > R.\n3.2. Case 2: q1 > q5, q2 > q6, q3 > q7, and q4 < q8\nThe solution of Eq. (6) is\nq1 = q2 = q3\n=\n1\n1 −γ R\n(24)\nq4\n=\nS +\nγ\n1 −γ R\n(25)\nq5 = q6 = q7\n=\nT +\nγ\n1 −γ R\n(26)\nq8\n=\n1\n1 −γ P.\n(27)\n7\nThis contradicts with the deﬁnition of the game T > R.\n3.3. Case 3: q1 > q5, q2 > q6, q3 < q7, and q4 > q8\nThe solution of Eq. (6) is\nq1 = q3 = q4\n=\n1\n1 −γ R\n(28)\nq2\n=\n1\n1 −γ S\n(29)\nq5 = q7 = q8\n=\n1\n1 −γ T\n(30)\nq6\n=\nP +\nγ\n1 −γ R.\n(31)\nThis contradicts with the deﬁnition of the game T > R.\n3.4. Case 4: q1 > q5, q2 > q6, q3 < q7, and q4 < q8\nFor this case, the strategy obtained by reinforcement learning is “Repeat”\n[18]. The solution of Eq. (6) is\nq1 = q3\n=\n1\n1 −γ R\n(32)\nq2 = q4\n=\n1\n1 −γ S\n(33)\nq5 = q7\n=\n1\n1 −γ T\n(34)\nq6 = q8\n=\n1\n1 −γ P.\n(35)\nThis contradicts with the deﬁnition of the game T > R.\n3.5. Case 5: q1 > q5, q2 < q6, q3 > q7, and q4 > q8\nThe solution of Eq. (6) is\nq1 = q2 = q4\n=\n1\n1 −γ R\n(36)\nq3\n=\n1\n1 −γ2 S +\nγ\n1 −γ2 T\n(37)\nq5 = q6 = q8\n=\n1\n1 −γ2 T +\nγ\n1 −γ2 S\n(38)\nq7\n=\nP +\nγ\n1 −γ R.\n(39)\nThis contradicts with 2R > T + S.\n8\n3.6. Case 6: q1 > q5, q2 < q6, q3 > q7, and q4 < q8\nFor this case, the strategy obtained by reinforcement learning is Tit-for-Tat\n(TFT) [1, 19]. The solution of Eq. (6) is\nq1 = q2\n=\n1\n1 −γ R\n(40)\nq3 = q4\n=\n1\n1 −γ2 S +\nγ\n1 −γ2 T\n(41)\nq5 = q6\n=\n1\n1 −γ2 T +\nγ\n1 −γ2 S\n(42)\nq7 = q8\n=\n1\n1 −γ P.\n(43)\nThis solution becomes consistent with the condition of the case only when T +\nS = R + P and γ = T −R\nR−S .\n3.7. Case 7: q1 > q5, q2 < q6, q3 < q7, and q4 > q8\nFor this case, the strategy obtained by reinforcement learning is Win-stay-\nLose-Shift (WSLS) [16]. The solution of Eq. (6) is\nq1 = q4\n=\n1\n1 −γ R\n(44)\nq2 = q3\n=\nS + γP +\nγ2\n1 −γ R\n(45)\nq5 = q8\n=\nT + γP +\nγ2\n1 −γ R\n(46)\nq6 = q7\n=\nP +\nγ\n1 −γ R.\n(47)\nThis solution becomes consistent with the condition of the case when T +P < 2R\nand γ > T −R\nR−P .\n9\n3.8. Case 8: q1 > q5, q2 < q6, q3 < q7, and q4 < q8\nFor this case, the strategy obtained by reinforcement learning is the Grim\nstrategy. The solution of Eq. (6) is\nq1\n=\n1\n1 −γ R\n(48)\nq2 = q3 = q4\n=\nS +\nγ\n1 −γ P\n(49)\nq5\n=\nT +\nγ\n1 −γ P\n(50)\nq6 = q7 = q8\n=\n1\n1 −γ P.\n(51)\nThis solution becomes consistent with the condition of the case when γ > T −R\nT −P .\n3.9. Case 9: q1 < q5, q2 > q6, q3 > q7, and q4 > q8\nFor this case, the strategy obtained by reinforcement learning is the anti-\nGrim strategy. The solution of Eq. (6) is\nq1\n=\nS +\nγ\n1 −γ2 R +\nγ2\n1 −γ2 P\n(52)\nq2 = q3 = q4\n=\n1\n1 −γ2 R +\nγ\n1 −γ2 P\n(53)\nq5\n=\n1\n1 −γ2 P +\nγ\n1 −γ2 R\n(54)\nq6 = q7 = q8\n=\nT +\nγ\n1 −γ2 R +\nγ2\n1 −γ2 P.\n(55)\nThis contradicts with γ ≥0.\n3.10. Case 10: q1 < q5, q2 > q6, q3 > q7, and q4 < q8\nFor this case, the strategy obtained by reinforcement learning is anti-Win-\nstay-Lose-Shift (AWSLS). The solution of Eq. (6) is\nq1 = q4\n=\nS + γR +\nγ2\n1 −γ P\n(56)\nq2 = q3\n=\nR +\nγ\n1 −γ P\n(57)\nq5 = q8\n=\n1\n1 −γ P\n(58)\nq6 = q7\n=\nT + γR +\nγ2\n1 −γ P.\n(59)\nThis contradicts with γ ≥0.\n10\n3.11. Case 11: q1 < q5, q2 > q6, q3 < q7, and q4 > q8\nFor this case, the strategy obtained by reinforcement learning is anti-Tit-\nfor-Tat (ATFT). The solution of Eq. (6) is\nq1 = q2\n=\n1\n1 −γ S\n(60)\nq3 = q4\n=\n1\n1 −γ2 R +\nγ\n1 −γ2 P\n(61)\nq5 = q6\n=\n1\n1 −γ2 P +\nγ\n1 −γ2 R\n(62)\nq7 = q8\n=\n1\n1 −γ T.\n(63)\nThis contradicts with γ ≥0.\n3.12. Case 12: q1 < q5, q2 > q6, q3 < q7, and q4 < q8\nThe solution of Eq. (6) is\nq1 = q2 = q4\n=\n1\n1 −γ S\n(64)\nq3\n=\nR +\nγ\n1 −γ P\n(65)\nq5 = q6 = q8\n=\n1\n1 −γ P\n(66)\nq7\n=\n1\n1 −γ T.\n(67)\nThis contradicts with the deﬁnition of the game P > S.\n3.13. Case 13: q1 < q5, q2 < q6, q3 > q7, and q4 > q8\nFor this case, the strategy obtained by reinforcement learning is anti-Repeat.\nThe solution of Eq. (6) is\nq1 = q3\n=\n1\n1 −γ2 S +\nγ\n1 −γ2 T\n(68)\nq2 = q4\n=\n1\n1 −γ2 R +\nγ\n1 −γ2 P\n(69)\nq5 = q7\n=\n1\n1 −γ2 P +\nγ\n1 −γ2 R\n(70)\nq6 = q8\n=\n1\n1 −γ2 T +\nγ\n1 −γ2 S.\n(71)\nThis solution becomes consistent with the condition of the case only when T +\nS = R + P and γ = 1.\n11\n3.14. Case 14: q1 < q5, q2 < q6, q3 > q7, and q4 < q8\nThe solution of Eq. (6) is\nq1 = q3 = q4\n=\n1\n1 −γ2 S +\nγ\n1 −γ2 T\n(72)\nq2\n=\nR +\nγ\n1 −γ P\n(73)\nq5 = q7 = q8\n=\n1\n1 −γ P\n(74)\nq6\n=\n1\n1 −γ2 T +\nγ\n1 −γ2 S.\n(75)\nThis solution becomes consistent with the condition of the case only when T +\nS > 2P and γ = P −S\nT −S .\n3.15. Case 15: q1 < q5, q2 < q6, q3 < q7, and q4 > q8\nThe solution of Eq. (6) is\nq1 = q2 = q3\n=\nS +\nγ\n1 −γ2 P +\nγ2\n1 −γ2 R\n(76)\nq4\n=\n1\n1 −γ2 R +\nγ\n1 −γ2 P\n(77)\nq5 = q6 = q7\n=\n1\n1 −γ2 P +\nγ\n1 −γ2 R\n(78)\nq8\n=\nT +\nγ\n1 −γ2 P +\nγ2\n1 −γ2 R.\n(79)\nThis contradicts with the deﬁnition of the game T > R.\n3.16. Case 16: q1 < q5, q2 < q6, q3 < q7, and q4 < q8\nFor this case, the strategy obtained by reinforcement learning is the All-D\nstrategy. The solution of Eq. (6) is\nq1 = q2 = q3 = q4\n=\nS +\nγ\n1 −γ P\n(80)\nq5 = q6 = q7 = q8\n=\n1\n1 −γ P.\n(81)\nThis solution is always consistent with the condition of the case.\n12\nnumber\nq1 ≶q5\nq2 ≶q6\nq3 ≶q7\nq4 ≶q8\nstrategy T1(C)\nname\nEquilibrium?\nCase 1\n>\n>\n>\n>\n(1, 1, 1, 1)T\nAll-C\nNo\nCase 2\n>\n>\n>\n<\n(1, 1, 1, 0)T\nNo\nCase 3\n>\n>\n<\n>\n(1, 1, 0, 1)T\nNo\nCase 4\n>\n>\n<\n<\n(1, 1, 0, 0)T\nRepeat\nNo\nCase 5\n>\n<\n>\n>\n(1, 0, 1, 1)T\nNo\nCase 6\n>\n<\n>\n<\n(1, 0, 1, 0)T\nTFT\nNo in general\nCase 7\n>\n<\n<\n>\n(1, 0, 0, 1)T\nWSLS\nYes for γ > T −R\nR−P\nCase 8\n>\n<\n<\n<\n(1, 0, 0, 0)T\nGrim\nYes for γ > T −R\nT −P\nCase 9\n<\n>\n>\n>\n(0, 1, 1, 1)T\nanti-Grim\nNo\nCase 10\n<\n>\n>\n<\n(0, 1, 1, 0)T\nAWSLS\nNo\nCase 11\n<\n>\n<\n>\n(0, 1, 0, 1)T\nATFT\nNo\nCase 12\n<\n>\n<\n<\n(0, 1, 0, 0)T\nNo\nCase 13\n<\n<\n>\n>\n(0, 0, 1, 1)T\nanti-Repeat\nNo in general\nCase 14\n<\n<\n>\n<\n(0, 0, 1, 0)T\nNo in general\nCase 15\n<\n<\n<\n>\n(0, 0, 0, 1)T\nNo\nCase 16\n<\n<\n<\n<\n(0, 0, 0, 0)T\nAll-D\nYes\nTable 1: Summary of the results.\n3.17. Summary\nFrom the above subsections, we ﬁnd that the symmetric solution of the Bell-\nman optimality equation exists in ﬁnite regions of the parameter γ only for the\ncase 7, 8, and 16. In other words, only WSLS, the Grim strategy, and the All-D\nstrategy can form the symmetric equilibrium of mutual reinforcement learning.\nTFT does not form symmetric equilibrium. (The optimal strategy against TFT\nis investigated in detail in Appendix A.) The results are summarized in Table\n1, where the strategy vector of player 1 is deﬁned by\nT1(C)\n:=\n\n\n\n\n\n\n\n\nT1 (C|C, C)\nT1 (C|C, D)\nT1 (C|D, C)\nT1 (C|D, D)\n\n\n\n\n\n\n\n\n.\n(82)\nThe reason why TFT does not form equilibrium can be intuitively explained\nas follows. We consider the situation that player 2 uses TFT, and the previous\n13\nstate was (D, C). By deﬁnition, the action-value function (4) represents ex-\npected future reward when the previous state was σ(0). If the strategy of player\n1 is also TFT, the sequence\n(D, C) →(C, D) →(D, C) →(C, D) →· · ·\n(83)\nis realized. If the strategy of player 1 is All-C, the sequence\n(D, C) →(C, D) →(C, C) →(C, C) →· · ·\n(84)\nis realized. Because of T + S < 2R, the latter results in larger total payoﬀ\nthan the former. Therefore, as explained in Appendix A, the optimal strategy\nagainst TFT is not TFT.\nOne may recall that All-D, WSLS and Grim form subgame perfect equilibria\nin the repeated prisoner’s dilemma game, but TFT dose not [20]. Therefore,\nour reinforcement learning equilibrium seems to be similar to subgame perfect\nequilibrium. In fact, the above discussion that TFT does not form reinforce-\nment learning equilibrium is similar to the discussion that TFT does not form\nsubgame perfect equilibrium. However, we expect that reinforcement learning\nequilibrium is weaker than subgame perfect equilibrium, since, in the deﬁnition\nof subgame perfect equilibrium, arbitrary histories are considered.\nRelation\nbetween them should be clariﬁed in future.\n4. Numerical results\nIn this section, we check the theoretical results in the previous section by\nnumerical simulation.\nWe use Q-learning [13] as a method of reinforcement\nlearning. In Q-learning, the optimal action-value function of the agent a against\na ﬁxed strategy of the agent −a is learned through the following update rule:\nQ(t+1)\na\n\u0010\nσ(1)\na , σ(0)\u0011\n=\nQ(t)\na\n\u0010\nσ(1)\na , σ(0)\u0011\n+ η\n \nra + γ max\nσ(2)\na\nQ(t)\na\n\u0010\nσ(2)\na , σ(1)\u0011\n−Q(t)\na\n\u0010\nσ(1)\na , σ(0)\u0011!\n,\n(85)\nwhere ra is the reward by taking action σ(1)\na\nwhen the state is σ(0), and σ(1) is\nthe next state. The parameter η is called the learning rate. Here, we assume\n14\nthat, in each step, the agent a chooses the action σ(1)\na\nby using ǫ-greedy search,\nthat is, the agent a chooses an action uniformly randomly among all possible\nactions with probability ǫ, and chooses the best action with respect to the\ncurrent action-value function with probability 1 −ǫ. As before, we consider\nthe situation that two agents alternately learn their optimal strategies until Q\nvalues converge.\nWe set parameters (R, S, T, P) = (4, 0, 6, 1), η = 0.2, and ǫ = 0.01. In the\nnumarical calculation of Q, we take the statistical average over 103 realizations.\nThe initial condition of Q is Q\n\u0000σ(1), σ(0)\u0001\n= 0 for all σ(1) and σ(0).\nIn Figure 1, we display the time evolution of Q1 when the strategy of player\n2 is WSLS. According to Appendix A, the optimal strategy against WSLS is\nWSLS for γ > (T −R)/(R−P) and All-D for γ < (T −R)/(R−P). On the top\npanel of Figure 1, we provide the numerical results for γ = 0.9 > (T −R)/(R −\nP) = 2/3. The theoretical value of Q1 is also provided in Appendix A:\nq1 = q4\n=\n1\n1 −γ R = 40\n(86)\nq2 = q3\n=\nS + γP +\nγ2\n1 −γ R = 33.3\n(87)\nq5 = q8\n=\nT + γP +\nγ2\n1 −γ R = 39.3\n(88)\nq6 = q7\n=\nP +\nγ\n1 −γ R = 37.\n(89)\nWe can expect that the numerical results converge to the theoretical value in\nthe limit t →∞. We emphasize that the learned strategy by player 1 is also\nWSLS, which is consistent with the result in the previous section. We remark\nthat, as the learning proceeds, cooperation by player 1 after the state (C, D)\nbecomes diﬃcult to occur, which leads to the slow convergence of Q1(C, C, D).\nOn the bottom panel of Figure 1, we provide the numerical results for γ =\n0.2 < (T −R)/(R −P) = 2/3. The theoretical value of Q1 is also provided in\n15\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\n 0\n 20000\n 40000\n 60000\n 80000\n 100000\nQ\nt\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 0\n 20000\n 40000\n 60000\n 80000\n 100000\nQ\nt\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\nFigure 1: The time evolution of Q1 when the strategy of player 2 is WSLS. (Top) The value\nof the discounting factor γ is γ = 0.9. The straight dash lines correspond to 40, 39.3, 37, and\n33.3 from top to bottom. (Bottom) The value of the discounting factor γ is γ = 0.2. The\nstraight dash lines correspond to 6.46, 5.29, 2.29, and 0.458 from top to bottom.\n16\nAppendix A:\nq1 = q4\n=\nR +\nγ\n1 −γ2 T +\nγ2\n1 −γ2 P ≃5.29\n(90)\nq2 = q3\n=\nS +\nγ\n1 −γ2 P +\nγ2\n1 −γ2 T ≃0.458\n(91)\nq5 = q8\n=\n1\n1 −γ2 T +\nγ\n1 −γ2 P ≃6.46\n(92)\nq6 = q7\n=\n1\n1 −γ2 P +\nγ\n1 −γ2 T ≃2.29.\n(93)\nWe can expect that the numerical results also converge to the theoretical value\nin the limit t →∞. For this case, the learned strategy by player 1 is All-D.\nTherefore, we conclude that WSLS forms the equilibrium of mutual reinforce-\nment learning for suﬃciently large γ.\nIn Figure 2, we display the time evolution of Q1 when the strategy of player 2\nis Grim. According to Appendix A, the optimal strategy against Grim is Grim\nfor γ > (T −R)/(T −P) and All-D for γ < (T −R)/(T −P). On the top panel of\nFigure 2, we provide the numerical results for γ = 0.9 > (T −R)/(T −P) = 2/5.\nThe theoretical value of Q1 is also provided in Appendix A:\nq1\n=\n1\n1 −γ R = 40\n(94)\nq2 = q3 = q4\n=\nS +\nγ\n1 −γ P = 9\n(95)\nq5\n=\nT +\nγ\n1 −γ P = 15\n(96)\nq6 = q7 = q8\n=\n1\n1 −γ P = 10.\n(97)\nWe ﬁnd that, although the learned strategy by player 1 is Grim, there are dis-\ncrepancies between the theoretical values and the numerical results for Q1(C, C, C),\nQ1(C, D, C), Q1(D, C, C), and Q1(D, D, C). This is due to the property of the\nGrim strategy.\nIn our simulation, player 1 (a learning agent against Grim)\nstochastically chooses C or D.\nHowever, once player 1 chooses D, player 2\n(the agent with the Grim strategy) switches to a defector who always defects.\nTherefore, the state (D, C) occurs only once. Similarly, the state (C, C) occurs\nonly while player 1 keeps cooperating. Therefore, the number of times that\n17\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\n 0\n 20000\n 40000\n 60000\n 80000\n 100000\nQ\nt\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 0\n 20000\n 40000\n 60000\n 80000\n 100000\nQ\nt\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\nFigure 2: The time evolution of Q1 when the strategy of player 2 is Grim. (Top) The value\nof the discounting factor γ is γ = 0.9. The straight dash lines correspond to 40, 15, 10, and 9\nfrom top to bottom. (Bottom) The value of the discounting factor γ is γ = 0.2. The straight\ndash lines correspond to 6.25, 5.25, 1.25, and 0.25 from top to bottom.\n18\nthe states (C, C) and (D, C) occur in one trial of the inﬁnitely repeated game\ncannot be large enough for the Q values to converge to the theoretical values.\n(It has been known that the action-value function in Q-learning converges to\nthe true value if all state-action pairs are visited an inﬁnite number of times\n[13].) In addition, as the learning proceeds, cooperation by player 1 after the\nstate (C, D) becomes diﬃcult to occur, which leads to the slow convergence of\nQ1(C, C, D). On the bottom panel of Figure 2, we provide the numerical results\nfor γ = 0.2 < (T −R)/(T −P) = 2/5. The theoretical value of Q1 is also\nprovided in Appendix A:\nq1\n=\nR + γT +\nγ2\n1 −γ P = 5.25\n(98)\nq2 = q3 = q4\n=\nS +\nγ\n1 −γ P = 0.25\n(99)\nq5\n=\nT +\nγ\n1 −γ P = 6.25\n(100)\nq6 = q7 = q8\n=\n1\n1 −γ P = 1.25.\n(101)\nWe ﬁnd that the learned strategy by player 1 is Grim, although the theoretical\nprediction is All-D.\nDue to the same reason as above, there are discrepan-\ncies between the theoretical values and the numerical results for Q1(C, C, C),\nQ1(C, D, C), Q1(D, C, C), and Q1(D, D, C). In particular, although Q1(C, C, C)\nis updated as long as player 1 keeps cooperating, Q1(D, C, C) is updated only\nonce, that is, when player 1 ﬁrst defects. This fact leads to the discrepancy\nbetween the theoretical prediction Q1(C, C, C) < Q1(D, C, C) and the numer-\nical result Q1(C, C, C) > Q1(D, C, C). (In order to check this conjecture, we\nalso provide numerical results about the situation where implementation error\nexists in the action of player 2, in Appendix B. These results are consistent\nwith our conjecture.) In addition, due to the same reason as above, the con-\nvergence of Q1(C, C, D) is slow. Besides these facts, our numerical results are\nconsistent with the theoretical prediction, and we conclude that Grim can form\nthe equilibrium of mutual reinforcement learning.\n19\n5. Conclusion\nIn this paper, we theoretically investigated the situation where both play-\ners alternately use reinforcement learning to obtain their optimal memory-one\nstrategies in the repeated prisoner’s dilemma game. We derived the symmet-\nric solutions of the Bellman optimality equations. We found that WSLS, the\nGrim strategy, and the All-D strategy can form equilibrium of the mutual re-\ninforcement learning process amongst sixteen deterministic memory-one strate-\ngies. We checked this result by numerical simulation using Q-learning. The\nfollowing problems should be studied in future: (i) Whether asymmetric equi-\nlibrium points exist or not, (ii) analysis on non-deterministic strategies, and\n(iii) extension of our analysis to memory-two strategies. Furthermore, exten-\nsion of our analysis to the situations where the inequalities T > R > P > S\n[21, 22, 23, 24] or 2R > T + S [25, 26] do not hold is also a subject of future\nwork. In addition, elucidating the relation between equilibrium in the mutual\nreinforcement learning and equilibrium in evolutionary game theory [15] is a\nsigniﬁcant problem.\nAcknowledgement\nThis study was supported by JSPS KAKENHI Grant Number JP20K19884.\nAppendix A. Optimal strategy against ﬁxed strategies\nIn this appendix, we provide theoretical results on the deterministic optimal\nstrategy of a learning agent against the other agent with a ﬁxed strategy. We\nregard agent 1 and 2 as a learning agent and an agent with a ﬁxed strategy,\nrespectively.\nThe Bellman optimality equation of the agent 1 is Eq.\n(8) as\nbefore. We consider the situation where the agent 2 chooses the TFT strategy,\nthe WSLS strategy, and the Grim strategy. We introduce the notation (21) as\nbefore.\n20\nAppendix A.1. Optimal strategy against TFT\nHere we consider the situation that the strategy of the agent 2 is TFT:\nT2(C)\n=\n\n\n\n\n\n\n\n\n1\n1\n0\n0\n\n\n\n\n\n\n\n\n.\n(A.1)\nThen, the solution of Eq. (8) is as follows.\nAppendix A.1.1. The case T + S < R + P and γ > P −S\nR−S\nFor the case, the solution is\nq1 = q2\n=\n1\n1 −γ R\n(A.2)\nq3 = q4\n=\nS +\nγ\n1 −γ R\n(A.3)\nq5 = q6\n=\nT + γS +\nγ2\n1 −γ R\n(A.4)\nq7 = q8\n=\nP + γS +\nγ2\n1 −γ R\n(A.5)\nand because q1 > q5, q2 > q6, q3 > q7, and q4 > q8, the optimal strategy is\nAll-C.\nAppendix A.1.2. The case T + S < R + P and T −R\nT −P < γ < P −S\nR−S\nFor the case, the solution is\nq1 = q2\n=\n1\n1 −γ R\n(A.6)\nq3 = q4\n=\nS +\nγ\n1 −γ R\n(A.7)\nq5 = q6\n=\nT +\nγ\n1 −γ P\n(A.8)\nq7 = q8\n=\n1\n1 −γ P\n(A.9)\nand because q1 > q5, q2 > q6, q3 < q7, and q4 < q8, the optimal strategy is\nRepeat.\n21\nAppendix A.1.3. The case T + S < R + P and γ < T −R\nT −P\nFor the case, the solution is\nq1 = q2\n=\nR + γT +\nγ2\n1 −γ P\n(A.10)\nq3 = q4\n=\nS + γT +\nγ2\n1 −γ P\n(A.11)\nq5 = q6\n=\nT +\nγ\n1 −γ P\n(A.12)\nq7 = q8\n=\n1\n1 −γ P\n(A.13)\nand because q1 < q5, q2 < q6, q3 < q7, and q4 < q8, the optimal strategy is\nAll-D.\nAppendix A.1.4. The case T + S > R + P and γ > T −R\nR−S\nFor the case, the solution is\nq1 = q2\n=\n1\n1 −γ R\n(A.14)\nq3 = q4\n=\nS +\nγ\n1 −γ R\n(A.15)\nq5 = q6\n=\nT + γS +\nγ2\n1 −γ R\n(A.16)\nq7 = q8\n=\nP + γS +\nγ2\n1 −γ R\n(A.17)\nand because q1 > q5, q2 > q6, q3 > q7, and q4 > q8, the optimal strategy is\nAll-C.\nAppendix A.1.5. The case T + S > R + P and P −S\nT −P < γ < T −R\nR−S\nFor the case, the solution is\nq1 = q2\n=\nR +\nγ\n1 −γ2 T +\nγ2\n1 −γ2 S\n(A.18)\nq3 = q4\n=\n1\n1 −γ2 S +\nγ\n1 −γ2 T\n(A.19)\nq5 = q6\n=\n1\n1 −γ2 T +\nγ\n1 −γ2 S\n(A.20)\nq7 = q8\n=\nP +\nγ\n1 −γ2 S +\nγ2\n1 −γ2 T\n(A.21)\nand because q1 < q5, q2 < q6, q3 > q7, and q4 > q8, the optimal strategy is\nanti-Repeat.\n22\nAppendix A.1.6. The case T + S > R + P and γ < P −S\nT −P\nFor the case, the solution is\nq1 = q2\n=\nR + γT +\nγ2\n1 −γ P\n(A.22)\nq3 = q4\n=\nS + γT +\nγ2\n1 −γ P\n(A.23)\nq5 = q6\n=\nT +\nγ\n1 −γ P\n(A.24)\nq7 = q8\n=\n1\n1 −γ P\n(A.25)\nand because q1 < q5, q2 < q6, q3 < q7, and q4 < q8, the optimal strategy is\nAll-D.\nAppendix A.2. Optimal strategy against WSLS\nHere we consider the situation that the strategy of the agent 2 is WSLS:\nT2(C)\n=\n\n\n\n\n\n\n\n\n1\n0\n0\n1\n\n\n\n\n\n\n\n\n.\n(A.26)\nThen, the solution of Eq. (8) is as follows.\nAppendix A.2.1. The case T + P < 2R and γ > T −R\nR−P\nFor the case, the solution is\nq1 = q4\n=\n1\n1 −γ R\n(A.27)\nq2 = q3\n=\nS + γP +\nγ2\n1 −γ R\n(A.28)\nq5 = q8\n=\nT + γP +\nγ2\n1 −γ R\n(A.29)\nq6 = q7\n=\nP +\nγ\n1 −γ R\n(A.30)\nand because q1 > q5, q2 < q6, q3 < q7, and q4 > q8, the optimal strategy is\nWSLS.\n23\nAppendix A.2.2. The case T + P < 2R and γ < T −R\nR−P\nFor the case, the solution is\nq1 = q4\n=\nR +\nγ\n1 −γ2 T +\nγ2\n1 −γ2 P\n(A.31)\nq2 = q3\n=\nS +\nγ\n1 −γ2 P +\nγ2\n1 −γ2 T\n(A.32)\nq5 = q8\n=\n1\n1 −γ2 T +\nγ\n1 −γ2 P\n(A.33)\nq6 = q7\n=\n1\n1 −γ2 P +\nγ\n1 −γ2 T\n(A.34)\nand because q1 < q5, q2 < q6, q3 < q7, and q4 < q8, the optimal strategy is\nAll-D.\nAppendix A.2.3. The case T + P > 2R\nFor the case, the solution is\nq1 = q4\n=\nR +\nγ\n1 −γ2 T +\nγ2\n1 −γ2 P\n(A.35)\nq2 = q3\n=\nS +\nγ\n1 −γ2 P +\nγ2\n1 −γ2 T\n(A.36)\nq5 = q8\n=\n1\n1 −γ2 T +\nγ\n1 −γ2 P\n(A.37)\nq6 = q7\n=\n1\n1 −γ2 P +\nγ\n1 −γ2 T\n(A.38)\nand because q1 < q5, q2 < q6, q3 < q7, and q4 < q8, the optimal strategy is\nAll-D.\nAppendix A.3. Optimal strategy against Grim\nHere we consider the situation that the strategy of the agent 2 is Grim:\nT2(C)\n=\n\n\n\n\n\n\n\n\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n.\n(A.39)\nThen, the solution of Eq. (8) is as follows.\n24\nAppendix A.3.1. The case γ > T −R\nT −P\nFor the case, the solution is\nq1\n=\n1\n1 −γ R\n(A.40)\nq2 = q3 = q4\n=\nS +\nγ\n1 −γ P\n(A.41)\nq5\n=\nT +\nγ\n1 −γ P\n(A.42)\nq6 = q7 = q8\n=\n1\n1 −γ P\n(A.43)\nand because q1 > q5, q2 < q6, q3 < q7, and q4 < q8, the optimal strategy is\nGrim.\nAppendix A.3.2. The case γ < T −R\nT −P\nFor the case, the solution is\nq1\n=\nR + γT +\nγ2\n1 −γ P\n(A.44)\nq2 = q3 = q4\n=\nS +\nγ\n1 −γ P\n(A.45)\nq5\n=\nT +\nγ\n1 −γ P\n(A.46)\nq6 = q7 = q8\n=\n1\n1 −γ P\n(A.47)\nand because q1 < q5, q2 < q6, q3 < q7, and q4 < q8, the optimal strategy is\nAll-D.\nAppendix B. Numerical results under implementation error\nIn this appendix, we provide numerical results about the situation where\nimplementation error exists in the action of a player. The setup of numerical\nsimulation is the same as one in Section 4. We assume that the strategy of\nplayer 2 is Grim with implementation error, which takes wrong action with\nsmall probability 10−2. The strategy of player 1 is learned by Q-learning.\nIn Figure B.3, we display the time evolution of Q1 when the strategy of player\n2 is Grim with implementation error. We can see that the learned strategy of\n25\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\n 0\n 20000\n 40000\n 60000\n 80000\n 100000\nQ\nt\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 0\n 20000\n 40000\n 60000\n 80000\n 100000\nQ\nt\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\nFigure B.3: The time evolution of Q1 when the strategy of player 2 is Grim with implemen-\ntation error. (Top) The value of the discounting factor γ is γ = 0.9. The straight dash lines\ncorrespond to 40, 15, 10, and 9 from top to bottom. (Bottom) The value of the discounting\nfactor γ is γ = 0.2. The straight dash lines correspond to 6.25, 5.25, 1.25, and 0.25 from top\nto bottom.\n26\nplayer 1 is Grim for γ = 0.9, and All-D for γ = 0.2, in contrast to the case\nwithout implementation error in Figure 2, where the learned strategy is Grim\nfor both γ = 0.9 and γ = 0.2. This is because Grim with implementation error is\nnot irreversible, although Grim is irreversible, and Q1(C, C, C) and Q1(D, C, C)\nare updated suﬃciently many times.\nReferences\nReferences\n[1] A. Rapoport, A. M. Chammah, C. J. Orwant, Prisoner’s dilemma: A study\nin conﬂict and cooperation, Vol. 165, University of Michigan press, 1965.\n[2] C. Hilbe, K. Chatterjee, M. A. Nowak, Partners and rivals in direct reci-\nprocity, Nature Human Behaviour 2 (7) (2018) 469.\n[3] A. Rapoport, Optimal policies for the prisoner’s dilemma., Psychological\nReview 74 (2) (1967) 136.\n[4] T. W. Sandholm, R. H. Crites, Multiagent reinforcement learning in the\niterated prisoner’s dilemma, Biosystems 37 (1-2) (1996) 147–166.\n[5] Y. Sato, E. Akiyama, J. D. Farmer, Chaos in learning a simple two-person\ngame, Proceedings of the National Academy of Sciences 99 (7) (2002) 4748–\n4751.\n[6] J. Hu, M. P. Wellman, Nash q-learning for general-sum stochastic games,\nJournal of Machine Learning Research 4 (Nov) (2003) 1039–1069.\n[7] T. Galla, J. D. Farmer, Complex dynamics in learning complicated games,\nProceedings of the National Academy of Sciences 110 (4) (2013) 1232–1236.\n[8] S. Hidaka, T. Torii, A. Masumi, Which types of learning make a simple\ngame complex?, Complex Systems 24 (1) (2015) 49–74.\n27\n[9] M. Harper, V. Knight, M. Jones, G. Koutsovoulos, N. E. Glynatsi,\nO. Campbell, Reinforcement learning produces dominant strategies for the\niterated prisoner’s dilemma, PloS One 12 (12) (2017) e0188046.\n[10] W. Barfuss, J. F. Donges, J. Kurths, Deterministic limit of temporal diﬀer-\nence reinforcement learning for stochastic games, Physical Review E 99 (4)\n(2019) 043305.\n[11] Y. Fujimoto, K. Kaneko, Emergence of exploitation as symmetry break-\ning in iterated prisoner’s dilemma, Physical Review Research 1 (3) (2019)\n033077.\n[12] G. I. Bischi, A. Naimzada, Global analysis of a dynamic duopoly game\nwith bounded rationality, in: Advances in dynamic games and applications,\nSpringer, 2000, pp. 361–385.\n[13] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, MIT\npress, 2018.\n[14] L. Busoniu, R. Babuska, B. De Schutter, A comprehensive survey of mul-\ntiagent reinforcement learning, IEEE Transactions on Systems, Man, and\nCybernetics, Part C (Applications and Reviews) 38 (2) (2008) 156–172.\n[15] J. M. Smith, G. R. Price, The logic of animal conﬂict, Nature 246 (5427)\n(1973) 15.\n[16] M. Nowak, K. Sigmund, A strategy of win-stay, lose-shift that outperforms\ntit-for-tat in the prisoner’s dilemma game, Nature 364 (6432) (1993) 56–58.\n[17] S. Lan, Geometrical regret matching: A new dynamics to nash equilibrium,\nAIP Advances 10 (6) (2020) 065033.\n[18] E. Akin, The iterated prisoner’s dilemma: good strategies and their dy-\nnamics, Ergodic Theory, Advances in Dynamical Systems (2016) 77–107.\n[19] R. Axelrod, W. D. Hamilton, The evolution of cooperation, Science\n211 (4489) (1981) 1390–1396.\n28\n[20] L. A. Imhof, D. Fudenberg, M. A. Nowak, Tit-for-tat or win-stay, lose-\nshift?, Journal of Theoretical Biology 247 (3) (2007) 574–580.\n[21] J. Tanimoto, H. Sagara, Relationship between dilemma occurrence and the\nexistence of a weakly dominant strategy in a two-player symmetric game,\nBioSystems 90 (1) (2007) 105–114.\n[22] Z. Wang, S. Kokubo, M. Jusup, J. Tanimoto, Universal scaling for the\ndilemma strength in evolutionary games, Physics of Life Reviews 14 (2015)\n1–30.\n[23] H. Ito, J. Tanimoto, Scaling the phase-planes of social dilemma strengths\nshows game-class changes in the ﬁve rules governing the evolution of coop-\neration, Royal Society Open Science 5 (10) (2018) 181085.\n[24] M. R. Areﬁn, K. A. Kabir, M. Jusup, H. Ito, J. Tanimoto, Social eﬃciency\ndeﬁcit deciphers social dilemmas, Scientiﬁc Reports 10 (1) (2020) 1–9.\n[25] J. Tanimoto, H. Sagara, A study on emergence of alternating reciprocity\nin a 2× 2 game with 2-length memory strategy, BioSystems 90 (3) (2007)\n728–737.\n[26] M. Wakiyama, J. Tanimoto, Reciprocity phase in various 2× 2 games by\nagents equipped with two-memory length strategy encouraged by grouping\nfor interaction and adaptation, BioSystems 103 (1) (2011) 93–104.\n29\n",
  "categories": [
    "cs.GT",
    "physics.soc-ph"
  ],
  "published": "2021-01-28",
  "updated": "2021-05-21"
}