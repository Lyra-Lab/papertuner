{
  "id": "http://arxiv.org/abs/2101.11889v1",
  "title": "Explaining Natural Language Processing Classifiers with Occlusion and Language Modeling",
  "authors": [
    "David Harbecke"
  ],
  "abstract": "Deep neural networks are powerful statistical learners. However, their\npredictions do not come with an explanation of their process. To analyze these\nmodels, explanation methods are being developed. We present a novel explanation\nmethod, called OLM, for natural language processing classifiers. This method\ncombines occlusion and language modeling, which are techniques central to\nexplainability and NLP, respectively. OLM gives explanations that are\ntheoretically sound and easy to understand.\n  We make several contributions to the theory of explanation methods. Axioms\nfor explanation methods are an interesting theoretical concept to explore their\nbasics and deduce methods. We introduce a new axiom, give its intuition and\nshow it contradicts another existing axiom. Additionally, we point out\ntheoretical difficulties of existing gradient-based and some occlusion-based\nexplanation methods in natural language processing. We provide an extensive\nargument why evaluation of explanation methods is difficult. We compare OLM to\nother explanation methods and underline its uniqueness experimentally. Finally,\nwe investigate corner cases of OLM and discuss its validity and possible\nimprovements.",
  "text": "UNIVERSITY OF POTSDAM\nMASTER’S THESIS\nExplaining Natural Language\nProcessing Classiﬁers with Occlusion\nand Language Modeling\nAuthor:\nDavid HARBECKE\nSupervisors:\nProf. Dr. David SCHLANGEN\nProf. Dr. Manfred STEDE\nA thesis submitted in fulﬁllment of the requirements\nfor the degree of Master of Science in\nCognitive Systems: Language, Learning and Reasoning\nat the\nFaculty of Human Sciences\nSeptember 27, 2020\narXiv:2101.11889v1  [cs.CL]  28 Jan 2021\nDeclaration of Authorship\nI, David HARBECKE, declare that this thesis, titled “EXPLAINING NATURAL\nLANGUAGE PROCESSING CLASSIFIERS WITH OCCLUSION AND LANGUAGE\nMODELING”, and the work presented in it are my own. I conﬁrm that I\nworked independently and only with the sources and aids indicated.1 All\npassages of the work which I have taken from these sources and aids, either\nin wording or in meaning, are marked and listed in the bibliography. Parts\nof the presented research have been published with a co-author (Harbecke\nand Alt, 2020).2 Content contributions of the co-author that are mentioned\nare marked as such. I am familiar with “Richtlinie zur Sicherung guter\nwissenschaftlicher Praxis für Studierende an der Universität Potsdam (Pla-\ngiatsrichtlinie)”3.\nDate:\nSigned:\n1Nevertheless, this thesis is written in ﬁrst-person plural.\n2I hereby acknowledge the Association for Computational Linguistics for letting me\ndescribe ﬁndings of publications co-authored by me.\n3https://www.uni-potsdam.de/am-up/2011/ambek-2011-01-037-039.pdf\niii\nEnglish Abstract\nDavid HARBECKE\nEXPLAINING NATURAL LANGUAGE PROCESSING CLASSIFIERS WITH\nOCCLUSION AND LANGUAGE MODELING\nDeep neural networks are powerful statistical learners. However, their\npredictions do not come with an explanation of their process. To analyze\nthese models, explanation methods are being developed. We present a\nnovel explanation method, called OLM, for natural language processing\nclassiﬁers. This method combines occlusion and language modeling, which\nare techniques central to explainability and NLP, respectively. OLM gives\nexplanations that are theoretically sound and easy to understand.\nWe make several contributions to the theory of explanation methods. Axioms\nfor explanation methods are an interesting theoretical concept to explore their\nbasics and deduce methods. We introduce a new axiom, give its intuition\nand show it contradicts another existing axiom. Additionally, we point\nout theoretical difﬁculties of existing gradient-based and some occlusion-\nbased explanation methods in natural language processing. We provide an\nextensive argument why evaluation of explanation methods is difﬁcult. We\ncompare OLM to other explanation methods and underline its uniqueness\nexperimentally. Finally, we investigate corner cases of OLM and discuss its\nvalidity and possible improvements.\niv\nDeutsche Zusammenfassung\nDavid HARBECKE\nEXPLAINING NATURAL LANGUAGE PROCESSING CLASSIFIERS WITH\nOCCLUSION AND LANGUAGE MODELING\nTiefe neuronale Modelle sind gut im statistischen Lernen. Jedoch liefern\nderen Vorhersagen keine Erklärungen des Vorgangs. Um diese Modelle zu\nanalysieren, hat man Erklärungstechniken entwickelt. Wir präsentieren eine\nneue Erklärungstechnik, gennant OLM, für klassiﬁzerende Modelle linguis-\ntischer Datenverarbeitung. Diese Methode kombiniert das Maskieren von\nMerkmalen mit Sprachmodellierung, was jeweils grundlegende Methoden\nder Erklärungstechnik und linguistischer Datenverarbeitung sind. OLM\nliefert Erklärungen die theoretisch fundiert und einfach zu verstehen sind.\nWir machen mehrere theoretische Beiträge zu Erklärungstechniken. Axiome\nfür Erklärungstechniken sind ein interessantes theoretisches Konzept um\nderen Grundlagen auszuloten und Techniken abzuleiten. Wir führen ein\nneues Axiom ein, legen die Intuition dar und zeigen dass es einem existieren-\nden Axiom widerspricht. Zusätzlich zeigen wir theoretische Problematik\nin der Anwendung von gradienten- und manchen maskierungsbasierten\nErklärungstechniken bei linguistischer Datenverarbeitung auf. Wir argu-\nmentieren umfangreich, warum die Evaluation von Erklärungstechniken\nschwierig ist. Wir vergleichen OLM mit anderen Erklärungstechniken und\nheben dessen Alleinstellungsmerkmal hervor. Abschließend betrachten wir\nGrenzfälle der Anwendung von OLM und diskutieren dessen Validität und\nmögliche Verbesserungen.\nv\nContents\nDeclaration of Authorship . . . . . . . . . . . . . . . . . . . . . . . .\nii\nEnglish Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niii\nDeutsche Zusammenfassung\n. . . . . . . . . . . . . . . . . . . . . .\niv\nContents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nv\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nList of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nList of Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii\n1\nIntroduction\n1\n1.1\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1.2\nArchitecture . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.1.3\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.1.4\nCapabilities . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nNatural Language Processing . . . . . . . . . . . . . . . . . . .\n4\n1.2.1\nLanguage Representations for Deep Learning\n. . . . .\n6\n2\nExplainability of DNNs\n8\n2.1\nAxioms for Explanation Methods . . . . . . . . . . . . . . . . .\n11\n2.2\nGradient-based Explanation Methods in NLP . . . . . . . . . .\n13\n2.3\nOn the Incompleteness of Evaluating Explanations . . . . . . .\n15\n3\nMethods\n17\n3.1\nOcclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.2\nLanguage Modeling . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.3\nOLM\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.3.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.3.2\nFormula Derivation OLM . . . . . . . . . . . . . . . . .\n21\n3.3.3\nAxiomatic Analysis . . . . . . . . . . . . . . . . . . . . .\n23\n3.3.4\nOLM-S(ensitivity) . . . . . . . . . . . . . . . . . . . . . .\n24\nvi\n4\nExperiments\n26\n4.1\nCorrelation of Explanation Methods . . . . . . . . . . . . . . .\n29\n4.1.1\nBaseline Methods . . . . . . . . . . . . . . . . . . . . . .\n29\n4.1.2\nTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.1.3\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2\nCoLA Corner Case\n. . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.2.1\nStatistical Analysis of Explanations . . . . . . . . . . . .\n35\n4.3\nFAVA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5\nConclusion\n41\n5.1\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.2\nFuture Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nBibliography\n46\nvii\nList of Figures\n1.1\nSchematic view of a neural network\n. . . . . . . . . . . . . . .\n2\n2.1\nArtiﬁcial explanations\n. . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2\nSchematic display of data likelihood in NLP\n. . . . . . . . . .\n14\n4.1\nCorrelation of verb relevance for FAVA sentence pairs . . . . .\n40\nList of Tables\n4.1\nExample explanations for SST-2 . . . . . . . . . . . . . . . . . .\n26\n4.2\nResampling examples for SST-2 . . . . . . . . . . . . . . . . . .\n28\n4.3\nCorrelation of explanation methods on MNLI . . . . . . . . . .\n31\n4.4\nCorrelation of explanation methods on SST-2 . . . . . . . . . .\n32\n4.5\nCorrelation of explanation methods on CoLA . . . . . . . . . .\n32\n4.6\nSigniﬁcance test for explanations on CoLA\n. . . . . . . . . . .\n36\n4.7\nExample explanations for CoLA . . . . . . . . . . . . . . . . . .\n36\n4.8\nResampling examples for CoLA . . . . . . . . . . . . . . . . . .\n37\n4.9\nExample explanations for FAVA . . . . . . . . . . . . . . . . . .\n38\n4.10 Signiﬁcance test for explanations on FAVA . . . . . . . . . . . .\n39\nviii\nList of Deﬁnitions\npdata\nprobability distribution of data\npLM\nprobability distribution of a language model\nfθ\nneural network with parameters θ\nf\nneural network, or, in general, black-box function\nfc := projc ◦f\nprojection of function f to class c, i.e. output neuron c\n(X, Y)\nlabeled dataset\nX\na dataset or, more general, the whole input space\nY\nlabel set\nx ∈X\nan input element of the dataset; an input vector\nxi\nan indexed feature of the input x\nx\\i\nan input without the feature at i-th position\n(x\\i, ˆxi)\nan input with a replacement feature\nr f,c(xi)\nrelevance of an input feature xi regarding function f\nand class c\n1\n1 Introduction\nThe advent of deep learning has created an explanation gap as the models\nare considered hard to interpret (Guidotti et al., 2018; Adadi and Berrada,\n2018). Models without hand-engineered features are hard to understand in\ntheir decision making. This makes explanation methods highly relevant. In\nnatural language processing most state-of-the-art architectures are neural\nnetworks. To understand their decisions we point out gaps in existing\nexplanation methods and develop a novel method.\n1.1\nDeep Learning\nDeep learning describes the architectures of multi-layered neural networks\nand methods to train them. Deep neural networks (DNNs) learn features\nfrom data. The layers of a deep neural network learn increasingly higher\nlevel features (Deng and Yu, 2014).\nTo introduce deep neural networks we ﬁrst motivate interest in them. Then,\nwe introduce some of their theory by explaining a possible deep neural\nnetwork architecture and how DNNs can be trained. Lastly, we discuss\nspeciﬁc properties that show DNNs’ relevance to the presented work.\n1.1.1\nMotivation\nDeep neural networks have achieved state-of-the-art performance on a wide\nvariety of tasks, such as\n• image recognition (Ciregan et al., 2012; Krizhevsky et al., 2012),\n• text classiﬁcation (Kim, 2014),\n• perfect information games (Silver et al., 2016, 2018),\n2\nChapter 1. Introduction\nx1\nx2\nx3\nInput\nlayer\nh1\n1\nh1\n2\nh1\n3\nh1\n4\nHidden\nlayer 1\nh2\n1\nh2\n2\nh2\n3\nh2\n4\nHidden\nlayer 2\nˆy1\nˆy2\nOutput\nlayer\nFIGURE 1.1: Schematic view of a feed-forward neural network\nwith two hidden layers. Each node displays a neuron, the\narrows between nodes represent the weights. The biases and\nactivation functions are not depicted.\n• or estimating the underlying probability distribution of a sample space\n(Bengio et al., 2003; Goodfellow et al., 2014).\nIn addition to the wide variety of tasks they can perform well, the prediction\nprocess of a neural network can be seen as opaque. A neural network learns\nits parameters from data and does not need to have feature extractions\nengineered by humans. Thus, external techniques are required to explain the\ntraining and decisions of a neural network.\n1.1.2\nArchitecture\nA neural network consists of neurons in layers. Figure 1.1 gives a schematic\nview of a neural network. The input layer displays an input vector x =\n(x1, x2, x3)T with three input dimensions. For the ﬁrst hidden layer this input\nvector is multiplied by a weight matrix W1 with 3 × 4 dimensions, a bias\nvector b1 = (b1\n1, b1\n2, b1\n3, b1\n4)T is added and a non-linear activation function σ1 is\napplied component-wise. This is repeated for the next hidden layer and the\noutput layer with different weights and biases. Therefore, a step from a layer\nto the next is an afﬁne transformation followed by an activation function.\nThe activation function of the output layer is usually chosen such that the\noutput neurons represent a probability distribution or individual probability\nfunctions, depending on the formulation of the problem and the data. A\nmathematical formulation of the network would be\nˆy = σ3(W3σ2(W2σ1(W1x + b1) + b2) + b3).\n(1.1)\n1.1. Deep Learning\n3\nA neural network with several hidden layers is called deep neural network\n(DNN). The weights and biases are the trainable parameters θ of a neural\nnetwork. The prediction ˆy depends on these parameters, also referred to as\nweights. A simpler formulation if we are not interested in speciﬁc weights is\nˆy = fθ(x).\n(1.2)\n1.1.3\nTraining\nTo train a DNN we need an objective and a loss function. The objective is\nusually a labeled dataset (X, Y) and tells us what the network should predict\nfor each input. An element of the dataset x ∈X is usually a coordinate vector\nover R. The loss function is a function L( ˆy, y) of the prediction ˆy and true\nlabel y ∈Y. Sometimes a regularizer, which is a loss function on the network\nparameters θ, also usually coordinate vectors over R, is added to this loss\nfunction.\nWith all these ingredients, a neural network is usually trained with back-\npropagation (Linnainmaa, 1970; Rumelhart et al., 1986a,b) and a variation\nof gradient descent (Cauchy, 1847). Backpropagation is a method that prop-\nagates the error E calculated by the loss function to the parameters θ of\nthe DNN via the chain rule of differentiation. The need for differentiability\nexplains why often both the inputs and parameters are coordinate vectors\nover R. It ensures that the gradients are also real valued. This is important\nfor section Language Representations for Deep Learning (1.2.1).\nDifferentiation can be done in parallel for all parameters of a layer. Gradient\ndescent is an optimization technique of these parameters. Let us view the\nneural network as a high-dimensional function over all parameters θ. It\nchanges the parameter values by stepping proportionally to the size of the\npartial derivative of the loss over the whole dataset regarding the parameter\nin each direction. Gradient descent can be seen an optimization alternative\nto using Newton’s Method (Newton, 1736; Raphson, 1690; Simpson, 1740),\nwhich looks for zeros of a function, for the ﬁrst derivative of a function.\nGradient descent only updates the parameters once per iteration over the\ndataset. This is inefﬁcient as subsets of the dataset (mini-batch) can provide\na good estimation of the gradient (Wilson and Martinez, 2003; Bottou and\nBousquet, 2008). Stochastic gradient descent (Robbins and Monro, 1951)\n4\nChapter 1. Introduction\naverages the gradients over a mini-batch and does one optimization step for\nthis mini-batch. This is a better trade-off between update time and update\nquality. There are many popular and recent variants and alternatives to\nstochastic gradient descent such as Momentum (Qian, 1999) and ADAM\n(Kingma and Ba, 2014).\n1.1.4\nCapabilities\nThe universal approximation theorem states that with speciﬁc restrictions for\nthe width (Lu et al., 2017) or depth (Hanin, 2017) DNNs can approximate any\ncontinuous convex function. This gives an intuition on why they are state-of-\nthe-art for many prediction tasks. Furthermore, Choromanska et al. (2015)\nshow that deep networks have better loss surfaces than shallow networks for\ntraining. This means that the local optima that optimizers ﬁnd are closer to\nthe global optimum for deep networks. This is underlined by the information\nbottleneck principle (Tishby and Zaslavsky, 2015; Shwartz-Ziv and Tishby,\n2017) which states that training of a DNN is faster than that of similarly\ncapable shallow networks. The success of deep learning is also partly due to\nhardware with parallel computing capabilities (Strigl et al., 2010).\nFor this work the relevance of DNNs is two-fold. First, we try to explain their\nbehaviour when performing state-of-the-art prediction on natural language\nprocessing tasks. Second, we use neural language models to create these\nexplanations.\n1.2\nNatural Language Processing\nNatural language processing (NLP) encompasses the intersection between\nhuman language and the processing of it by computing machinery. The ﬁeld\nof NLP is nowadays mostly concerned with a statistical and quantitative\nprocessing and modeling of mass amounts of language data. Manning and\nSchütze (1999) state:\n“Increasingly, businesses, government agencies and individuals\nare confronted with large amounts of text that are critical for\nworking and living, but not well enough understood to get the\nenormous value out of them that they potentially hide.”\n1.2. Natural Language Processing\n5\nThis development has been ampliﬁed by the success of DNNs which are\ncurrently state of the art for many of the popular datasets of these tasks. This\nincludes\n• automatic speech recognition on the LibriSpeech corpus (Panayotov\net al., 2015) by Synnaeve et al. (2019) and the Wall Street Journal corpus\n(Paul and Baker, 1992) by Povey et al. (2016),\n• part-of-speech tagging on the Wall Street Journal part of the Penn\nTreebank (Marcus et al., 1999) by Bohnet et al. (2018),\n• sentiment analysis on the IMDB dataset (Maas et al., 2011), the Stanford\nSentiment Treebank (Socher et al., 2013) and the Yelp Review dataset\n(Zhang et al., 2015) by XLNET (Yang et al., 2019),\n• text classiﬁcation on the AG News corpus, the DBpedia onthology\n(Zhang et al., 2015) and the TREC dataset (Voorhees and Tice, 2000) by\nXLNET (Yang et al., 2019).\n• There are also multi-task benchmarks, such as GLUE (Wang et al.,\n2017) and SuperGLUE (Wang et al., 2019a), that combine several NLP\nclassiﬁcation tasks. Leading both benchmarks is T5 (Raffel et al., 2019).\nAt least three things are notable regarding the previous list. First, the variety\nof tasks is wide, ranging from speech recognition on audio data and generat-\ning text data in language modeling to classifying words, sentences or texts\nin part-of-speech tagging, relationship extraction, sentiment analysis and\ntext classiﬁcation. This makes the dominance of neural architectures in these\ntasks even more impressive.\nSecond, XLNet (Yang et al., 2019) and T5 (Raffel et al., 2019) appear frequently\non top of the leaderboard on many of these tasks. Both use a pre-trained\nlanguage model, i.e. they learn a representation of language by going over\nlarge text datasets with billions of words, such as the BooksCorpus (Zhu\net al., 2015) or English Wikipedia1. The training of language models will be\ndescribed in more detail in section Language Modeling (3.2). Raffel et al.\n(2019) even encode the problem formulation into the representation.\nThird, most of the datasets are classiﬁcation tasks in some sense where the\nneural model has a preselected set of outputs. These are the problems and\n1https://en.wikipedia.org\n6\nChapter 1. Introduction\nmodels we are interested in. The method presented in this thesis yields\nexplanations for NLP classiﬁcation tasks.\n1.2.1\nLanguage Representations for Deep Learning\nTime ﬂies like an arrow;\nfruit ﬂies like bananas\nAnthony Oettinger\nAn important component to utilize neural networks in NLP is the represen-\ntation of the input. This is nontrivial, as we saw in section Training (1.1.3).\nWe have to assign real valued coordinate vectors to our input. Furthermore,\nthe unit of language, called token or atomic parse element, from which to\nmap into our vector space is nonobvious. It is imperative to choose a token\nthat allows for an unambiguous mapping. For written language characters,\nwords, sentences and documents are among the candidates for this atomic\nparse element.\nA simple realization of this mapping is to count the words of an input and\ncreate a vector representation with vector indices corresponding to words.\nThis method is called bag-of-words (Harris, 1954). It can be used both on a\nsentence and a document level. A practical improvement on this is what\nis now called tf-idf (Salton et al., 1975). There, the word counts are scaled\nwith the logarithm of the inverse of the ratio of documents containing the\nword. Both methods do not preserve the order or contextual meaning of\nwords. Locally, the order and contextuality can be incorporated by choosing\nto use n-grams instead of or in addition to single words. However, this does\nnot retain information about longer contexts. It also yields exponentially\nmore possible n-grams and fewer counts of a speciﬁc n-gram for increasing\nn, which makes the representations less precise and efﬁcient.\nA variation to bag-of-words is one-hot-encoding where the words are not\ncounted but words get assigned pairwise distinct standard basis vectors.\nIf we ignore out-of-vocabulary words, this is an one-to-one function between\ninputs and the representation. This method can be used as underlying trans-\nformation for representing words with other methods. Compared to the\nprevious methods, it allows the order of words in the whole input to be kept.\n1.2. Natural Language Processing\n7\nHowever, none of these methods provide information about the similarity of\nwords.\nMikolov et al. (2013) introduce word2vec, a learned vector space represen-\ntation of words. It can be called an embedding because the dimensionality\nof this vector space is lower than the number of words that are represented.\nThis means that the vectors corresponding to words are not pairwise orthog-\nonal, generally. Words with similar meaning are supposed to have a small\nangle between their vector representations. Words are predicted from their\ncontext with the intuition that words that appear in similar contexts have\nsimilar meaning. At the time they achieved state of the art on semantic word\nsimilarity. GloVe (Pennington et al., 2014) explicitly learns vector representa-\ntions that are based on co-occurrence. These representations allow the DNNs\nthat employ them to use the order of words if they desire. Thus, context is\npreserved but does not inﬂuence representation itself.\nMany approaches use characters (Wieting et al., 2016; Bojanowski et al., 2017)\nor sub-words (Wu et al., 2016; Kudo and Richardson, 2018) as tokens to\nimprove on word representations. By choosing a smaller unit, similarities\nbetween words with similar spelling or the same root can be incorporated\ninto the representation. Still, it does not enable different representations of\nthe same word in different contexts.\nHoward and Ruder (2018) use the language modeling described in section\nLanguage Modeling (3.2) to create a vector embedding where the infor-\nmation of context is fed through several layers. This enables a different\nrepresentation for “ﬂies like” in the epigraph of this section, depending on\nthe context.\n8\n2 Explainability of DNNs\nIn section Capabilities (1.1.4) we indicated the performance abilities of\nDNNs. However, their state-of-the-art prediction performance is not the mea-\nsure of all things. Recital 71 of the European Union’s General Data Protection\nRegulation1 states:\n“[decision-making based solely on automated processing] should\nbe subject to suitable safeguards, which should include speciﬁc\ninformation to the data subject and the right [...] to obtain an\nexplanation of the decision reached after such assessment and to\nchallenge the decision.”\nIts potential legal ramiﬁcations are discussed in Goodman and Flaxman\n(2017). This regulation clearly states that for real-world applications of\nDNNs explanations are necessary. Explanations as support for a decision\nby a black box is detailed in Lombrozo (2006): “explanations are [...] the\ncurrency in which we exchanged beliefs”. We will be using the term ex-\nplainability (of DNNs) to refer to this ﬁeld, not the more frequently used\n“Explainable Artiﬁcial Intelligence (XAI)” or “Interpretable Machine Learn-\ning”. Both these terms indicate that there is something inherently explainable\nor interpretable about the models in deep learning. This may be the case in\nmachine learning but cannot be assumed in general. Furthermore, it is very\ncontroversial whether the “Artiﬁcial Intelligence” moniker is a precise or\nhelpful representation of the characteristics of deep learning (Jordan, 2019).\nSeveral popular surveys of explainability exist. We summarize some of\nthem to pigeonhole our work more precisely. Doshi-Velez and Kim (2017)\npoint out that explanations ﬁll an incompleteness in the problem that deep\nlearning models work on. E.g., the objective given to the model during\ntraining may not have measured generalization performance adequately.\nThis can be uncovered with explanations. Doshi-Velez and Kim (2017) divide\n1https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679\nChapter 2. Explainability of DNNs\n9\nexplanations into local and global explanations. Local explanations try to\nexplain the model at a speciﬁc input, whereas global explanations try to\nexplain the model as a whole or its general behaviour.\nAdadi and Berrada (2018) highlight key concepts in explainability. They\ngive four reasons for explanations: “explain to justify”, “explain to con-\ntrol”, “explain to improve” and “explain to discover”. Furthermore, they\nadd two categories to sort explanation methods into. They differentiate\nbetween model-speciﬁc interpretability and model-agnostic interpretability\nwith the latter denoting methods that can work on any black-box model. As\na subcategory to model-speciﬁc methods they discern intrinsic and post-hoc\nmethods.\nGuidotti et al. (2018) present a detailed taxonomy of problems that explana-\ntion methods can try to solve. They give formal deﬁnitions of these problems,\nincluding intrinsically explainable models and post-hoc black-box methods.\nIn addition to the previous methods, they describe black-box model expla-\nnation, which tries to build intrinsically explained models that approximate\nthe model to be explained.\nThere is an extended discussion of explanation and psychology in Miller\n(2019). They argue that many of the insights gained in psychological under-\nstanding of explanations can be used for explainability of DNNs. We will\ntry to psychologically motivate our method in section Motivation (3.3.1).\nHowever, we disagree that many insights from psychology can be easily\ntransferred. Two main questions for explanations of black boxes are how to\ngenerate explanations from them and how to evaluate the faithfulness of their\nexplanations. This removes assumptions generally held in psychological\ncontexts.\nThere exist a variety of NLP speciﬁc explanations. Li et al. (2016) present\nmethods to view and test heatmaps for different recurrent networks. They in-\nvestigate sentiment analysis by putting forward different investigative input.\nAlvarez-Melis and Jaakkola (2017) create an explanation graph between struc-\ntured input and output. This is especially useful for sequence to sequence\ntasks like machine translation. Many state-of-the-art models use attention\n(Bahdanau et al., 2015; Vaswani et al., 2017). Some papers argue whether\nattention weights as explanations are permissible. Jain and Wallace (2019)\nshow that attention weights do not necessarily agree with other explanations\n10\nChapter 2. Explainability of DNNs\nand can also be distorted while not changing the prediction. Wiegreffe and\nPinter (2019) disagree and show that under some circumstances these distor-\ntion lead to a signiﬁcant decrease in performance. Synthesized, these papers\nargue that attention weights can be used as explanation if and only if the\nattention is vital for the model. There is also a variety of approaches that\ndetermine the quality of models by linguistic analysis (Linzen et al., 2016;\nMcCoy et al., 2019).\nWe provide some examples of methods that that do not belong to our cat-\negory of explanations and will not be discussed further. Kim et al. (2018)\nanalyze neural models by determining which concepts were important to a\nclassiﬁcation decision. Intrinsic model-speciﬁc explanations can be found\nin older machine learning approaches, such as decision trees, or in model\narchitectures that provide explanations and predictions in parallel (Zhang\net al., 2018). Ribeiro et al. (2016) explain by learning a local explainable model\naround the prediction of a neural model.\nWe will now focus on local post-hoc input space explanations. They describe\nhow much a feature of one speciﬁc input contributed to a speciﬁc output\n(class). They give a real value called relevance for every input feature which\ncan be used to create a saliency map (Simonyan et al., 2013). An example can\nbe found in Table 4.1. The output can be the true label neuron, the predicted\nneuron or another output of choice. A positive relevance value indicates that\nthe feature contributed positively to the given output, whereas a negative\nrelevance value indicates that the feature distracted from the output. Input\nfeatures can be the input values for the ﬁrst layer, or a cluster of these. For\nNLP, e.g., all input values of a word (and punctuation mark) can be clustered.\nIf the explanation method gives relevances for every input dimension the\nrelevances are aggregated such that every word receives one relevance value.\nThis is often achieved by summing the relevances (Arras et al., 2017).\nAn important part of explainability is occlusion. It is either presented as an\nexplanation method (Robnik-Šikonja and Kononenko, 2008; Zintgraf et al.,\n2017) where the difference in prediction when removing an input feature is\nseen as indicator for the importance of this feature . Alternatively, it is used\nas evaluation of explanation methods (Zeiler and Fergus, 2014; Montavon\net al., 2018), where it is argued that (other) explanation methods should ﬁnd\nfeatures whose occlusion change the prediction signiﬁcantly.\n2.1. Axioms for Explanation Methods\n11\nThe use of a method both for generating and evaluating explanations points\nto a false dichotomy between explanation methods and their evaluation,\nwhich we discuss extensively in section On the Incompleteness of Evaluat-\ning Explanations (2.3). In general, most objective evaluation methods allow\nfor a derivation of an explanation method which satisﬁes this evaluation\nperfectly. This has already been done in Kindermans et al. (2018) and when\ndeveloping the theory for this thesis, it was originally intended as an evalu-\nation method. However, we argue the standard for an evaluation method\nshould be higher. It should not be considered as just an accessory to an\nexplanation method to justify the explanations.\nExplanations are a simpliﬁcation of the model’s decision process. They try to\npresent the process in a human-understandable way. For complex models\nthis is always an approximation. To strictly deﬁne some ground rules for this\napproximation, axioms for explanation methods are developed.\n2.1\nAxioms for Explanation Methods\nAxioms are a proposed method to develop and test explanation methods\nby Sundararajan et al. (2017) and extensively discussed in Lipton (2018).\nAdditional axioms are proposed in Bach et al. (2015), Ancona et al. (2018),\nKindermans et al. (2019) and Srinivas and Fleuret (2019). The advantage of\naxiomatic analysis is that objectives for explanation methods can be stated\nand discussed and each explanation method can be evaluated against these\nobjectives. This can also give context to tasks, models or settings where\nsome explanation methods might be suitable or unsuitable. We will discuss\nlater why axioms might be the most objective evaluation of explanation\nmethods. In the following, we brieﬂy discuss some important axioms and\ntheir intuition.2\nCompleteness The sum of the relevances of features of an input is equal to the\nprediction (Bach et al., 2015). This axiom stems from the intuition that a\nprediction of an input is a prediction of a composition of features.\nImplementation Invariance Two neural networks that are functionally equiv-\nalent, i.e. give the same output for all possible inputs, should receive the same\n2An extensive discussion of axioms and explanation methods was done in the Individual\nModule and will thus not be repeated.\n12\nChapter 2. Explainability of DNNs\nExplanation 1:\ngood ﬁlm , but very glum .\n(positive sentiment)\ngood ﬁlm , but very glum .\n(negative sentiment)\nExplanation 2:\ngood ﬁlm , but very glum .\n(positive sentiment)\ngood ﬁlm , but very glum .\n(negative sentiment)\nFIGURE 2.1: Artiﬁcial explanations from two methods. We\nare investigating the explanation of a sentiment classiﬁer for\nboth positive and negative sentiment. Red indicates a feature\nsupporting the prediction, blue indicates a detraction. The ﬁrst\nexplanations do not satisfy Class Zero-Sum as the explanations\nare equal for both classes. It is unclear which token actually\ncontributed to which classes. The second explanations do sat-\nisfy Class Zero-Sum. It is identiﬁable which token contributed\nto which class and by how much.\nrelevances for every input (Sundararajan et al., 2017). This seems trivial but\nis important to state as methods that work with internal weights of neural\nmodels do not necessarily comply.\nLinearity A network, which is a linear combination of other networks, should\nhave explanations which are the same linear combination of the original networks\nexplanations (Sundararajan et al., 2017). This axiom states that an explanation\nmethod should be a linear function of neural networks given the input.\nSensitivity An input feature should receive a non-zero relevance, if and only if the\nprediction of the network depends on the feature (Sundararajan et al., 2017). This is\na simple double check that features that are ignored by the network should\nnot receive relevance and features that are important to the model should\nreceive relevance.\nSensitivity-1 The relevance of an input variable should be the difference of predic-\ntion when the input variable is removed (Ancona et al., 2018). This could also be\nnamed the occlusion axiom. It basically restates the prediction difference\nformula (Robnik-Šikonja and Kononenko, 2008) which we will see later in\nEq. (3.1).\nWe introduce a new axiom:\nClass Zero-Sum The sum of relevances of a feature over all classes is zero (Harbecke\nand Alt, 2020). The intuition behind this axiom is guided by the normalization\nof most classiﬁers. If the sum of the predictions is ﬁxed, then every feature\n2.2. Gradient-based Explanation Methods in NLP\n13\nadds as much to the prediction of a set of classes as it takes away from all\nother classes. Thus, if we want to explain the feature for all classes we should\nﬁx the sum over all classes to zero. E.g., we do not want a token to contribute\npositively to positive and negative sentiment (see Figure 2.1).\nIf a relevance method fulﬁlls this axiom, it allows for an intuitive interpreta-\ntion of the relevances, that may be taken for granted. Since the relevances of\na feature are normalized to have a sum of zero over all classes, a feature with\npositive relevance to a class can be interpreted as contributing to that class,\nwhereas a feature with a negative relevance detracts from that class.\nThe Class Zero-Sum axiom is a contradiction to the Completeness axiom if the\nnormalization of the classiﬁer is not set to have a sum of zero. We argue that\nCompleteness forces a method to assign undue relevance, e.g. in cases where\nthere is no information detected in the input for any class, it does not make\nsense to distribute positive relevance over features.\n2.2\nGradient-based Explanation Methods in NLP\nGradient-based explanation methods were introduced with Sensitivity Analy-\nsis (Baehrens et al., 2010; Simonyan et al., 2013). The intuition is that the\ngradient of the prediction at the input tells us which input dimensions can\nchange the prediction the most and are thus most important for the predic-\ntion. As Sundararajan et al. (2017) showed, this can be misleading, as this\nonly gives information about the function in the local neighborhood of the\ninput.\nIn the following we call the distribution of data pdata, meaning pdata(x) is\nthe probability of x appearing as a data point for a speciﬁc task. This is not\nconstrained to a dataset but the evasive more general distribution, i.e. “data\nthat users would expect the systems to work well on” (Gorman and Bedrick,\n2019). As discussed in section Training (1.1.3), in general, an input x in this\ncase can be regarded as coordinate vector over R. In practice this is almost\nalways the case in NLP. Let us assume we have a practical limitation on the\nlength of input text and, therefore, a maximum embedding dimension of n.\nWe can make all other inputs have this dimensionality by padding them with\nzeros. Thus, we have a function f that maps text to a subset S of a coordinate\n14\nChapter 2. Explainability of DNNs\nspace Rn. We will now discuss which properties this subset S has and why\nthis is important.\nFIGURE 2.2: Schematic display of data likelihood in NLP. There\nare discrete inputs, i.e., combination of tokens, with a data\nlikelihood greater than zero. All other inputs in the embed-\nding space have likelihood zero because they have no corre-\nsponding tokens. Every input with a positive likelihood has a\nneighborhood that does not contain another input with positive\nlikelihood. Gradient-based explanation methods (red arrow)\nconsider inﬁnitesimal changes to the input and thus data with\nno likelihood.\n1. S is a discrete set in Rn.\nA discrete set is a set where every element s ∈S has a neighbourhood that\ndoes not contain any other point of S. There are a ﬁnite number of tokens\nwith pairwise distinct embeddings and a ﬁnite length. Thus, S is ﬁnite and\nthe global minimum of distances between two elements of S is positive.\nTherefore, S is discrete.\n2. pdata(x) for x ∈Rn is a discrete probability distribution.\nFor every point x ∈Rn/S we have pdata(x) = 0. We just saw that S ⊂Rn is\na discrete subset. Therefore, pdata is a discrete probability distribution, as can\nbe seen schematically in Figure 2.2. Note that this is particularly different\n2.3. On the Incompleteness of Evaluating Explanations\n15\nto vision. Every canonical image representation x in a coordinate space Rn\nentails a neighbourhood of small perturbations where the images still make\nsense. Consequently, the probability distribution of images embedded in Rn\nis continuous.\n3. This property of the probability distribution is fundamental to gradient-\nbased explanation methods.\nGradient-based explanation methods analyze the change of prediction with\nrespect to the the input dimensions. Analyzing inﬁnitesimal change in a\nfunction presumes that this change is meaningful. However, if the data\nprobability is zero everywhere in a small neighbourhood of an input vector,\nthese changes become meaningless, as the prediction function can never be\nconfronted with other vectors from this neighbourhood. These vectors are\nautomatically out of distribution, i.e. the prediction function is analyzed by\nevaluating a priori meaningless behaviour.\nWe thus argue that gradient-based explanation methods are not theoretically\njustiﬁed in NLP. Although, in some cases, especially if the function is well-\nregularized, local behaviour indicates global behaviour. This, however, can\nnot be assumed and needs to be investigated before using gradient methods.\n2.3\nOn the Incompleteness of Evaluating Explana-\ntions\nThe sole focus of an explanation method should be to relay information\nabout a model to a user. Explanations exist to help understand the decision\nprocess of the model (Doshi-Velez et al., 2017). Thus, the correct explanation\ncannot be independent of the model.\nEvery experimental evaluation of explanation methods relies on a ground\ntruth (property) that the explanation should have. For example, let us take the\nevaluation method where the explanation of a model is compared with some\nfeatures that are identiﬁed with the help of experts (Mohseni and Ragan,\n2018). These ground truths are independent of the model. First, neural\nmodels have achieved superhuman performance on several tasks, e.g. go\n(Silver et al., 2016) and chess (Silver et al., 2018), and grounding explanations\nby humans on these tasks cannot be considered helpful in understanding\nsuperhuman performance.\n16\nChapter 2. Explainability of DNNs\nSecond, it does not take into consideration that a model can be wrong and\nthe explanations correct. The explanations of a model that made a false\nclassiﬁcation can point to completely different features than an expert would\nselect.\nThird, a rigid scheme by humans does not take into account that predictive\nfeatures can be missed even by experts, as neural models are powerful statis-\ntical learners (Sarle, 1994; Geirhos et al., 2018). On the contrary, explanation\nmethods are especially useful in cases where the model uses artefacts, not\nhuman-intuitive features, for its decision.\nIt is unclear whether it is possible to distinguish between the theoretical foun-\ndations of explanation methods and their evaluations. The most prominent\nexample of a method that is used for both is occlusion, which is discussed\nin the following section Occlusion (3.1). Since the model is the only ground\ntruth and explanations are a simpliﬁcation, it is highly probable that there is\nmore than one sensible explanation for an input classiﬁed by a model. Thus,\nfor both explanations and evaluation, constraints are established that reduce\nthe number of explanations. If these constrains are explicitly stated they can\nbe regarded as axioms. Note that axioms can be used both to develop and to\ntest explanation methods. All in all, the need for somewhat subjective con-\nstraints makes the existence of a general evaluation of explanation methods\nunlikely.\nWe do not argue that it is impossible to evaluate an explanation method.\nThe performance in sensibly selected evaluations does probably correlate to\nthe quality of an explanation method. Measuring the correlation to sensible\nexplanation methods can also be seen as quality assessment. We will use\nand discuss this approach in section Tasks (4.1.2). Asserting which axioms\nan explanation method fulﬁlls is an important step towards evaluating its\nvalidity. We argue that the explanation method for one’s use case should\nbe motivated by the paradigms that the method fulﬁlls. Furthermore, there\nare sanity checks (Adebayo et al., 2018) that can determine whether an\nexplanation method has undesired properties. Experimental evaluation of\na method can provide guidance for selecting an explanation method. They\nbecome more valuable the closer the setting of the evaluation is to the setting\nwhere the explanation is needed.\n17\n3 Methods\nTo build a theoretically solid explanation method we combine a technique\nrelated to the Sensitivity-1 axiom with a method that considers likelihood in\nNLP. We introduce these techniques, occlusion and language modeling, before\nsynthesizing them to a new method.\n3.1\nOcclusion\nWe lay out the theory of occlusion and discuss its advantages and disadvan-\ntages. Occlusion was introduced under the name Occlusion Sensitivity (Zeiler\nand Fergus, 2014) as a method to detect whether an explanation method\ndetects important features of an input for a neural network, by evaluating\ninputs with occluded features detected by the explanation method.\nConversely, Robnik-Šikonja and Kononenko (2008) introduce the same tech-\nnique as a local post-hoc input space explanation method. It measures the\nimportance of features of an input for a classiﬁer. This is done by comparing\nthe predictions of the classiﬁer with and without the feature. We refer to this\nmethod as occlusion, too. A feature can be a word or sentence in NLP classi-\nfying or a pixel or larger patch of an image in image classifying. Occlusion is\na true black-box method. It only uses the predictions of a model, no internal\nrepresentations or even structural information about the model.\nFor an input x we take a feature xi. To determine the relevance of xi we\nconsider the input x\\i without this feature. This is an incomplete input as we\ndo not know which values to set as replacement for xi. A simple approach\nis to set all the values of xi to zero. In NLP, it is possible to delete words or\nreplace them with the <UNK> token. This will be done as baseline methods\nin the experiments. Zintgraf et al. (2017) propose sampling the values from\nthe overall distribution of the dataset. With this replacement we employ the\n18\nChapter 3. Methods\ndifference of probabilities formula (Robnik-Šikonja and Kononenko, 2008;\nZintgraf et al., 2017).\nThe relevance r given the prediction function f and class c is\nr f,c(xi) = fc(x) −fc(x\\i).\n(3.1)\nThere are various practical difﬁculties with occlusion. It is unclear which fea-\ntures to select, especially if the human understanding of features differs from\nthe feature space that was used to input the data. Furthermore, occlusion\ndoes not guarantee that the data still makes sense (to the model) after a part\nof the input is taken out. This is especially difﬁcult in NLP, where models\nhave increasing syntactic, hierarchical and other linguistic understanding\n(Liu et al., 2019a; Hewitt and Manning, 2019). Thus, models could misinter-\npret data with missing features in various ways, e.g. taking ungrammaticality\nas an indicator for the prediction.\n3.2\nLanguage Modeling\nWe offer a short introduction to language modeling. This is both valuable\nfor our approach and for understanding the foundation of current state-of-\nthe-art NLP classiﬁcation models. Language modeling can be seen as using\nlarge corpora of unannotated language data and creating a supervised task\nby reusing words as their own labels.\nA DNN takes a static word embedding like word2vec or GloVe, or a sub-word\nembedding like SentencePiece (Kudo and Richardson, 2018) as input and\npredicts one or several tokens that are missing. These missing tokens can\nbe following the original input or masked among the input. The labels of\nthese tokens are one-hot vectors. In a way, these models assign likelihood\nto a given text (Brown et al., 1992). Let us have a text T that is split into\ntokens (t1, . . . , tn) = T. If we have a language model pLM that is able to\nmake predictions of the form pLM(t1) and pLM(ti+1|(t1, . . . , ti)) then we get\n3.3. OLM\n19\na probability and score of the whole text.\npLM(T) = pLM((t1, . . . , tn)) = pLM(t1)\nn−1\n∏\ni=1\npLM(ti+1|(t1, . . . , ti))\nPPpLM(T) := pLM(T)−1\nn\n(3.2)\nPPpLM(T) is called perplexity of the corpus. The lower this score, the higher\nthe probability that the language model assigned to the corpus and thus,\nthrough a Bayesian argument, the better the language model.\nNot all language models are trained to predict text from scratch. E.g., Devlin\net al. (2019) mask 15% of words in a sentence and predict those. This does not\nlead to a model that can be measured by giving the perplexity of a corpus.\nNevertheless, all these models create an embedding of the input in every\nhidden layer that can be used for other tasks. It is a priori unclear whether\nthese representations are an improvement on the static embedding because\nthe primary goal of the DNN is not to create a better representation of the\ninput for other classiﬁcation tasks. However, experimental results from\nHoward and Ruder (2018), Peters et al. (2018) and Devlin et al. (2019) suggest\nthat using embeddings from a language model is an improvement on static\nembeddings. The success of these models in various tasks mentioned in sec-\ntion Natural Language Processing (1.2) has been coined “NLP’s ImageNet\nmoment” (Ruder, 2018). Devlin et al. (2019) argue “Recent empirical improve-\nments due to transfer learning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral part of many language\nunderstanding systems.”\nIn our experiments, we only use language models for the simple task of\npredicting one missing word or punctuation mark. This is an area where\nmasked language models should excel.\n3.3\nOLM\nThe main idea of this thesis is combining Occlusion with Language Modeling\n(OLM). Instead of leaving out tokens, we want to replace them with sensible\noptions that only remove information but not structure. This disallows the\nprediction network to consider a changed structure of the input. It is now\n20\nChapter 3. Methods\nforced to consider the change in information. To get a good overview of this\nchange we sample plenty of possible replacements and compute a weighted\naverage of the prediction results. Due to the law of large numbers (Bernoulli,\n1713) we consider this average an accurate approximation of all possibilities.\n3.3.1\nMotivation\nCounterfactual thinking is a psychological concept which states that humans\nthink of past and future events by looking at alternatives “What if [...] ?”\n(Kahneman and Tversky, 1981; Roese, 1997). This enables us to evaluate\npast actions in another way than just looking at the outcome. For statistical\nmodels we are able to actually evaluate counterfactuals without needing to\nguess about the outcome. Occlusion can be seen as an objective measure to\ndo this by changing small parts of the input and considering “What if” this\npart of the input is different.\nThese approaches are closely linked to perturbation-based explanation meth-\nods. In contrast, these methods do not presume a set of features of which\nat least one is occluded, but ﬁgure out which features create the largest\nchange in prediction if they are missing or different (Fong and Vedaldi, 2017;\nWachter et al., 2017). They highlight the synergy of local adversarials and\nexplanations. Ribeiro et al. (2016) propose a variant of this by using small\nperturbations to learn a local linear model that resembles the original model\nand is interpretable. We argue that all these methods fail to consider data\nlikelihood. Furthermore, Ilyas et al. (2019) show that uninterpretable adver-\nsarials are an inherent feature of almost all neural models, which questions\ntheir usefulness as explanations.\nNorm theory (Kahneman and Miller, 1986) states that we choose counter-\nfactuals depending on how easy they are to imagine. This is at least related\nto how likely these alternatives are. Rational imagination theory (Byrne,\n2007) argues explicitly that we choose probable alternatives to reality when\nevaluating outcomes. These alternatives can either lead to more positive\nor negative results (Roese and Olson, 2014). We consider these psycholog-\nical intuitions because they are important to our own understanding of\nexplanations. Although, as we argued in section On the Incompleteness\nof Evaluating Explanations (2.3), explanations should not be evaluated by\nhuman intuition, it should be clear how the results of the methods are to be\n3.3. OLM\n21\nunderstood. Furthermore, intuitions are closely related to axioms. The men-\ntioned psychological statements describe intuitive principles. The evaluation\nof likely alternatives is easily understood and communicable to laymen of\nDNNs who may examine explanations to judge a model’s decision process.\n3.3.2\nFormula Derivation OLM\nWe start from the difference of probabilities formula in Eq. (3.1). We reinter-\npret x\\i in the following way. Instead of considering removing the feature\nxi we rather remove the information provided by this feature to the model.\nThis gives a very intuitive information-theoretic question that our method\nanswers: What additional information does this feature give to the model for\nclassiﬁcation? For this we have to consider what inputs are how likely given\nthe rest of the input is preserved. In the abstract this gives\nfc(x\\i) = ∑\nˆxi\npdata( ˆxi|x\\i) fc(x\\i, ˆxi)\n(3.3)\nwith pdata being the data probability over a deﬁned space containing the\ninputs, as deﬁned in section Gradient-based Explanation Methods in NLP\n(2.2).\nTo use this formula in NLP we approximate the data distribution (pdata) with\na language model\npdata( ˆxi|x\\i) ≈pLM( ˆxi|x\\i).\n(3.4)\nIn practice this means we mask a word, which may consist of several tokens,\nor a punctuation mark and resample it with a given language model. Other\ninputs and features are possible but could increase the approximation error,\ne.g. by accumulating it over several words. The language model does not\nhave the information of the original word but all information of the context.\nThis can lead to cases where the original word is predicted with a very high\nprobability. We argue that in these cases the additional information provided\nby this word was negligent.\nThe big advantage of this approach is that the structure of the original input\nis preserved by only picking replacements that seem likely to a language\n22\nChapter 3. Methods\nmodel. It gives the formula\nfc(x\\i) :≈∑\nˆxi\npLM( ˆxi|x\\i) fc(x\\i, ˆxi)\n(3.5)\nfor x\\i. There are several practical difﬁculties that this approximation brings.\nFirst, it is only valid if all data is supposed to be grammatical. A language\nmodel assigns higher probability to replacement tokens that produce a gram-\nmatical sentence. We will investigate this effect with the Corpus of Linguistic\nAcceptability dataset, in section CoLA Corner Case (4.2).\nSecond, replacements sometimes make much more sense if another part\nof the input is changed. For example, a prepositional verb determines the\npreposition it appears alongside with and the preposition can be seen as\ninformation of the verb. This preposition, however, selects which verbs can\nbe sampled as replacement.\nThird, the length of the replacement is a limiting factor. There are cases when\nthe replacement should be allowed to contain more or fewer tokens than\nthe original. Additionally, some masked language models, which predict\nsub-word tokens frequently, may not sample a whole word. This distorts\nthe resampled input. The last two problems can be interpreted as not giv-\ning the language model enough freedom by forcing it to make exactly one\ntoken replacement. We elaborate possible alleviations of these difﬁculties in\nsection Future Work (5.2). In the following, we refer to the unit we want to\nresample as token because, disregarding practical concerns, it is arbitrary in\nour method.\nCombined with Eq. (3.1) we set\nr f,c(xi) := fc(x) −∑\nˆxi\npLM( ˆxi|x\\i) fc(x\\i, ˆxi).\n(3.6)\nThis establishes a new method that can determine what effect tokens have\non a prediction. They can either have a positive or negative relevance,\ndepending on whether the original prediction is greater than the averaged\nprediction after resampling.\n3.3. OLM\n23\n3.3.3\nAxiomatic Analysis\nWe will show that OLM satisﬁes Class Zero-Sum, Implementation Invariance,\nSensitivity-1 and Linearity. Let f be a neural network that takes an element of\nthe input space X and predicts a probability distribution over classes C, i.e,\nf : X →R|C|\nfc(x) ≥0\n∀c ∈C, x ∈X\n∑\nc∈C\nfc(x) = 1\n∀x ∈X.\n(3.7)\nLet xi be an indexed feature of an input x. We denote the relevance given to\nthis feature regarding model f and class c by our method OLM with r f,c(xi).\nOLM satisﬁes Class Zero-Sum. Intuitively, if the input with the resampled\ntoken increases the prediction for one class, it has to decrease the predictions\nof other classes, and vice-versa. We have\n∑\nc∈C\nr f,c(xi)\n(3.6)\n= ∑\nc∈C\n \nfc(x) −∑\nˆxi\npLM( ˆxi|x\\i) fc(x\\i, ˆxi)\n!\n= ∑\nc∈C\nfc(x) −∑\nˆxi\npLM( ˆxi|x\\i) ∑\nc∈C\nfc(x\\i, ˆxi)\n(3.7)\n= 1 −∑\nˆxi\npLM( ˆxi|x\\i) = 0.\n(3.8)\nThus, OLM satisﬁes Class Zero-Sum. From this follows that it does not satisfy\nCompleteness.\nOLM satisﬁes Implementation Invariance. OLM is a black-box method and\nonly evaluates the function of the neural network. It does not regard the\nparameters θ. Assume we have\nθ ̸= θ′\nand\nfθ(x) = fθ′(x)\n∀x ∈X.\n(3.9)\n24\nChapter 3. Methods\nThen we get\nr fθ,c(xi) = fθc(x) −∑\nˆxi\npLM( ˆxi|x\\i) fθc(x\\i, ˆxi)\n= fθ′c(x) −∑\nˆxi\npLM( ˆxi|x\\i) fθ′c(x\\i, ˆxi)\n= r fθ′,c(xi).\n(3.10)\nThus, OLM satisﬁes Implementation Invariance.\nOLM satisﬁes Sensitivity-1. OLM is deﬁned as an occlusion method, so it\nnecessarily provides the difference of prediction when an input variable is\noccluded. Equation (3.6) is based on Eq. (3.1).\nOLM satisﬁes Linearity. Let f = ∑n\nj=1 αjgj be a linear combination of models.\nThen we have\nr f,c(xi) = fc(x) −∑\nˆxi\npLM( ˆxi|x\\i) fc(x\\i, ˆxi)\n=\nn\n∑\nj=1\nαjgj\nc(x) −∑\nˆxi\npLM( ˆxi|x\\i)\nn\n∑\nj=1\nαjgj\nc(x\\i, ˆxi)\n=\nn\n∑\nj=1\nαjrgj,c(xi).\n(3.11)\n3.3.4\nOLM-S(ensitivity)\nIt can be of additional interest to determine to which input features the model\nis most sensitive. Previously, we measured the mean difference between the\nmodel prediction and the resampled predictions. As a measure for sensitivity\nwe suggest taking the standard deviation of the resampled predictions. This\nmeasures how varied the predictions are for one token position, given the\nrest of the input but regardless of the original token. With previous notation,\nwe suggest for sensitivity s:\ns f,c(xi) :=\ns\n∑\nˆxi\npLM( ˆxi|x\\i)\n\u0010\nfc(x\\i, ˆxi) −µ\n\u00112\n.\n(3.12)\nWe do not suggest this as a relevance measure because, as previously men-\ntioned, it is independent of the input feature xi. Rather, this measure suggests\n3.3. OLM\n25\nadditional information to the relevance method. For a neutral feature it may\nbe reassuring to know whether the model would have picked up on more\nclass-indicating possibilities. In combination OLM and OLM-S measure the\nmean and standard deviation of predictions with resampled tokens.\n26\n4 Experiments\nWe perform several experiments to investigate the explanations generated\nby OLM. Example explanations can be found in Table 4.1. These experiments\nhighlight some practical peculiarities of our explanation method. For com-\nparison with other methods we also display the results of OLM-S. These\nexperiments cannot be comprehensive (see section On the Incompleteness\nof Evaluating Explanations (2.3)) and there is no standard benchmark. Thus,\nwe compare to other explanation methods and conduct experiments in areas\nthat may present corner cases to OLM.\nMethod\nRelevances\nMax. value\nOLM\ngood ﬁlm , but very glum .\n0.57\nOLM-S\ngood ﬁlm , but very glum .\n0.48\nDelete\ngood ﬁlm , but very glum .\n0.98\nUNK\ngood ﬁlm , but very glum .\n0.98\nSensitivity Analysis\ngood ﬁlm , but very glum .\n35\nGradient*Input\ngood ﬁlm , but very glum .\n0.041\nIntegrated Gradients\ngood ﬁlm , but very glum .\n0.96\nTABLE 4.1: Example explanations for SST-2. Explanations\nare shown for positive sentiment which the input is correctly\npredicted as. Positive relevances are displayed in red, negative\nrelevances are displayed in blue. Color intensity is normalized\nfor every explanation method and is proportional to absolute\nvalue of relevance. The last column gives the maximum of\nthe absolute relevances. OLM and OLM-S give relevance or\nsensitivity to both clauses. The other occlusion-based methods\ngive almost all relevance to “good”. Gradient-based methods\ngive most relevance to the second clause. Resamples that OLM\nused multiple times can be found in Table 4.2. OLM gives\npositive relevance to “glum” because some alternatives are\npredicted with a much lower probability for positive sentiment.\nChapter 4. Experiments\n27\nUnderlying these experiments is mostly the same combination of explanation\nand prediction model. This is done for two reasons. We investigate the\nalgorithmic efﬁciency of OLM. For every token we resample k times. Let us\nassume the distribution of these tokens follows a variation of Zipf’s Law\n(Estoup, 1916; Zipf, 1949) with α > 1 (Piantadosi, 2014). Then, we have\nO(\nα√\nk) different samples per token. Furthermore, we resample each of n\ntokens in an input. Thus, for a single input we have O(n\nα√\nk) predictions. An\ninvestigation of the effect of different language models on the explanations\nof different classiﬁcation models should be done but requires vast resources.\nAdditionally, we ﬁx the models to compare results across different tasks\nand datasets. To this end, we also only investigate explanations of the true\nlabel neuron. Some explanation methods do not necessarily treat different\nclasses differently, as alluded to in Figure 2.1. We try to remove this effect by\nfocusing only on the most important class.\nFor OLM and OLM-S we use BERTBASE (Devlin et al., 2019) as a language\nmodel and choose words (and punctuation marks) as units for resampling.\nResampling is computationally expensive but the quality of the samples is\nvery important. We also want a language model that does not frequently\nproduce sub-word tokens. BERT uses WordPiece (Wu et al., 2016) which\ndoes have sub-word tokens but mostly predicts whole words. This is viable\nfor single word resampling, especially compared to many other masked\nlanguage models. Thus, we choose BERTBASE as a low-resource compromise\nof a well-ﬁtting state-of-the-art language model to analyze the method over\ndatasets. An example of the samples is shown in Table 4.2. We point out that\nin general our approach is language model agnostic. For generating single\ninput explanations, not analyzing a dataset, we suggest using a collection\nof the best well-ﬁtting language models available. For classiﬁcation we use\ndifferent variations of ROBERTA (Liu et al., 2019b)1 that we describe in\nsection Tasks (4.1.2).2\n1All models were originally published at https://github.com/pytorch/fairseq/tree/\nmaster/examples/roberta. We use the implementation and pre-trained models from\nhttps://github.com/huggingface/transformers.\n2Experiments are available at https://github.com/harbecke/xbert.\n28\nChapter 4. Experiments\ngood\nﬁlm\n,\nbut\nvery\nglum\n.\ngood\nlooking\n,\nbut\nnot\nbad\n.\n(34, 0.98)\n(11, 0.96)\n(84, 0.98)\n(87, 0.98)\n(22, 1)\n(26, 0.003)\n(100, 0.98)\nnice\nnews\nart\nand\nstill\nshort\n(10, 0.46)\n(5, 0.018)\n(2, 0.79)\n(3, 1)\n(10, 0.87)\n(11, 1)\ngreat\nidea\nquality\nnot\nvery\nold\n(3, 0.41)\n(4, 0.0011)\n(2, 1)\n(2, 1)\n(10, 0.98)\n(5, 1)\nﬁne\ntaste\nthough\nalso\nthin\n(3, 0.27)\n(4, 0.94)\n(2, 1)\n(6, 1)\n(5, 1)\nclassic\nmorning\ntoo\ndull\n(3, 0.93)\n(3, 0.0064)\n(5, 1)\n(5, 0.058)\ninteresting\ntry\nalways\nslow\n(3, 0.085)\n(3, 0.0026)\n(4, 0.89)\n(3, 1)\nlovely\njob\nnever\nboring\n(3, 0.99)\n(3, 0.99)\n(3, 1)\n(3, 0.0059)\nstrong\nwork\nsometimes\nsmall\n(2, 1)\n(2, 0.99)\n(2, 1)\n(2, 1)\nbad\nthing\nquite\ndark\n(2, 8e-05)\n(2, 0.0041)\n(2, 0.98)\n(2, 1)\nfun\nplan\nslightly\nexpensive\n(2, 0.87)\n(2, 0.0017)\n(2, 1)\n(2, 1)\nfunny\nlord\ndamn\n(2, 0.55)\n(2, 0.007)\n(2, 0.018)\nexcellent\nquestion\n(2, 0.88)\n(2, 0.0012)\nwonderful\nwalk\n(2, 0.93)\n(2, 0.22)\ndecent\nanswer\n(2, 0.48)\n(2, 0.011)\nscary\nthoughts\n(2, 0.001)\n(2, 0.07)\nadvice\n(2, 0.0052)\nmood\n(2, 0.99)\ntiming\n(2, 0.2)\nTABLE 4.2: Resampled words for the example explanations in\nTable 4.1. The header of a column indicates which word was\nreplaced. The word entry in the row shows the replacement.\nThe numbers in brackets are how often this word was the\nreplacement out of 100 samples (weight) and the prediction of\nthe positive sentiment neuron, which is the true label. Only\nwords which were sampled at least twice are presented, the\ncolumns are ordered by sampling count.\n4.1. Correlation of Explanation Methods\n29\n4.1\nCorrelation of Explanation Methods\nFirst, we compare the relevances produced by OLM to those of other ex-\nplanation methods. The main focus of this experiment is to evaluate how\nlarge the differences to other explanations are. It could be assumed that the\nexplanations of basic occlusion is very similar to OLM explanations. If this\nwere shown by experiments, it would make the theoretical beneﬁts of OLM\nsuperﬂuous. E.g., in theory, the language model of a state-of-the-art classiﬁer\ncould understand an obviously missing word. It could treat this similar to\nour method by internally representing it as missing and deriving a predic-\ntion from that. In the same vein, we investigate how much the explanation\nmethods that use gradients differ from occlusion-based methods to see if\ntheoretical difﬁculties manifest. 3\nWe calculate the correlation of explanation methods on tasks in the following\nway. Let r1\nx and r2\nx be the ordered relevances of an element x ∈X of dataset\nX for methods 1 and 2. With corr being the Pearson correlation coefﬁcient for\nsamples (Pearson, 1895), we set the correlation of two methods over dataset\nX to\n∑n\ni=1 corr(r1\ni , r2\ni )\nn\n.\n(4.1)\nThis means, two methods are perfectly positively correlated if and only if\nthey produce scaled relevances with possibly different positive scaling for\neach input.\n4.1.1\nBaseline Methods\nWe compare our explanations with two baselines based on occlusion (see\nEq. (3.1)). The simplest variation is removing the word of interest and not\nreplacing it. We call this method Delete, it was ﬁrst used in NLP by Li\net al. (2016). Similarly, we replace the word of interest with the unknown\ntoken <UNK>. This approach is more tailored to state-of-the-art classiﬁers\npre-trained with masked language modeling.\nFurthermore, we compare explanations to three gradient-based methods.\nAll these methods provide relevances for every dimension of the input. To\n3This experiment already appears in Harbecke and Alt (2020). The baseline methods\nwere selected by me. Christoph Alt selected and conducted the experiments. Phrasing and\nanalysis exceeding the publication is mine.\n30\nChapter 4. Experiments\nreceive relevances on word level we sum over the dimensions for each word\n(Arras et al., 2017). The simplest one is the absolute value of the gradients and\nis called Sensitivity Analysis (Simonyan et al., 2013). Note that this method\nonly provides non-negative relevances. It is especially comparable to OLM-S\nwhich also provides a non-negative sensitivity of the model. Input*Gradient\n(Shrikumar et al., 2017) is self-explaining. Every input gets multiplied with\nits gradient. Finally, we compare our explanations to Integrated Gradients\n(Sundararajan et al., 2017). This is the integration of the gradient of the\nprediction function along a straight path from a baseline, usually the zero\nvector, to the input vector multiplied by the path length.\n4.1.2\nTasks\nWe select three NLP classiﬁcation tasks. An input contains one or two\nsentences or phrases for all tasks. Each task focuses on one speciﬁc aspect of\nlanguage understanding. All tasks are part of the GLUE benchmark (Wang\net al., 2017) which does not publish test sets. Therefore, we report results on\nthe development set which we do not use for model optimization.\nMulti-Genre Natural Language Inference Corpus (MNLI) by Williams et al.\n(2018) is a natural language inference corpus. A data point consists of two\nsentences that may have a relation to each other.\n• If the second sentence is a sensible successor to the ﬁrst in content, this\npair gets the entailment label.\n• If the content of the sentences does not relate to each other, the label is\nneutral.\n• If the sentences have are in disagreement they get a contradiction label.\nThe corpus contains more than 400,000 samples.\nWe use a ROBERTALARGE model which is already ﬁne-tuned on MNLI. It\nachieves an accuracy of 90.2% on the development set which is two per-\ncentage points behind state-of-the-art T5 (Raffel et al., 2019). McCoy et al.\n(2019) show that even though these models perform around the human base-\nline for this task, they fail to generalize for a variety of rare constructions.\nCorrelations of the explanation methods for MNLI can be found in Table 4.3.\n4.1. Correlation of Explanation Methods\n31\nOcclusion\nGradient\nMethod\nOLM\nOLM-S\nDel\nUNK\nSen\nG*I\nIG\nOLM\n1.00\n0.61\n0.60\n0.58\n0.27\n-0.03\n0.28\nOLM-S\n0.61\n1.00\n0.32\n0.32\n0.35\n-0.01\n0.20\nDelete\n0.60\n0.32\n1.00\n0.73\n0.23\n-0.05\n0.34\nUNK\n0.58\n0.32\n0.73\n1.00\n0.22\n-0.03\n0.32\nSensitivity Analysis\n0.27\n0.35\n0.23\n0.22\n1.00\n0.03\n0.17\nGradient*Input\n-0.03\n-0.01\n-0.05\n-0.03\n0.03\n1.00\n0.00\nIntegrated Gradients\n0.28\n0.20\n0.34\n0.32\n0.17\n0.00\n1.00\nTABLE 4.3: Correlation between explanation methods on MNLI\ndevelopment set. The table is symmetrical. The ﬁrst two rows\nare our own methods. The next two rows are other occlu-\nsion methods. The last three rows are gradient-based explana-\ntion methods. The correlation of different methods is highest\nbetween the occlusion methods but never close to 1. Gradi-\nent*Input does not correlate with any method.\nStanford Sentiment Treebank (SST) by Socher et al. (2013) is a sentiment\nclassiﬁcation dataset. It contains 70,000 sentences from movies with either a\nnegative or positive connotation. The SST-2 version only consists of binary\nclassiﬁcation with positive and negative sentiment. Sentiment Analysis is an\neasy task to interpret explanations on if the explanation method assumes that\nfeatures cannot contribute to both classes (see Figure 2.1 and Eq. (3.8)). An\ninput feature contributes as much to the positive sentiment as it detracts from\nthe negative sentiment and vice versa. Therefore, the explanation method\nassigns each feature positive or negative sentiment.\nWe ﬁne-tune a pre-trained ROBERTABASE. This model achieves an accuracy\nof 94.5% on the development set which is 3 percentage points lower than mul-\ntiple state-of-the-art models, including T5 (Raffel et al., 2019). Correlations\nof the explanation methods for SST-2 can be found in Table 4.4.\nCorpus of Linguistic Acceptability (CoLA) by Warstadt et al. (2019) is a\ndataset with sentences labeled by their grammatical acceptability. It contains\nmore than 10,000 sentences which are annotated as either acceptable or\nunacceptable. We will elaborate on the speciﬁcs of this task for explanation\nmethods in section CoLA Corner Case (4.2).\n32\nChapter 4. Experiments\nOcclusion\nGradient\nMethod\nOLM\nOLM-S\nDel\nUNK\nSen\nG*I\nIG\nOLM\n1.00\n0.78\n0.52\n0.47\n0.30\n0.02\n0.35\nOLM-S\n0.78\n1.00\n0.39\n0.38\n0.37\n0.01\n0.30\nDelete\n0.52\n0.39\n1.00\n0.64\n0.21\n0.01\n0.37\nUNK\n0.47\n0.38\n0.64\n1.00\n0.18\n0.03\n0.36\nSensitivity Analysis\n0.30\n0.37\n0.21\n0.18\n1.00\n0.03\n0.13\nGradient*Input\n0.02\n0.01\n0.01\n0.03\n0.03\n1.00\n0.04\nIntegrated Gradients\n0.35\n0.30\n0.37\n0.36\n0.13\n0.04\n1.00\nTABLE 4.4: Correlation between explanation methods on SST-2\ndevelopment set. The results resemble the results from Table\n4.3. The correlation between OLM and other occlusion methods\nis a little lower. In contrast, the correlation between OLM-S and\nother occlusion methods is a little higher.\nAnalogous to SST-2 we ﬁne-tune ROBERTABASE and achieve a phi coefﬁ-\ncient4 (Yule, 1912) of 0.613 on the development set. STRUCTBERT (Wang\net al., 2019b) achieves a phi coefﬁcient of 0.753. Correlations of the explana-\ntion methods for CoLA can be found in Table 4.5.\n4also misnomered Matthews correlation coefﬁcient\nOcclusion\nGradient\nMethod\nOLM\nOLM-S\nDel\nUNK\nSen\nG*I\nIG\nOLM\n1.00\n0.56\n0.25\n0.21\n0.20\n0.02\n0.15\nOLM-S\n0.56\n1.00\n0.15\n0.12\n0.29\n0.03\n0.09\nDelete\n0.25\n0.15\n1.00\n0.35\n0.02\n0.04\n0.18\nUnk\n0.21\n0.12\n0.35\n1.00\n0.03\n0.03\n0.14\nSensitivity Analysis\n0.20\n0.29\n0.02\n0.03\n1.00\n-0.00\n0.07\nGradient*Input\n0.02\n0.03\n0.04\n0.03\n-0.00\n1.00\n0.12\nIntegrated Gradients\n0.15\n0.09\n0.18\n0.14\n0.07\n0.12\n1.00\nTABLE 4.5: Correlation between explanation methods on CoLA\ndevelopment set. The correlation between OLM and OLM-\nS and other occlusion methods (in bold) is much lower than\non the other two tasks. This indicates that this dataset could\nbe a corner case for our method. Correlation between other\nmethods is also lower but to a smaller extend.\n4.1. Correlation of Explanation Methods\n33\n4.1.3\nResults\nTables 4.3, 4.4 and 4.5 show correlation of all tested explanation methods.\nOverall, there is positive correlation between almost all methods. No meth-\nods produce equivalent relevances, even the correlation between OLM and\nOLM-S is never close to 1. The three occlusion-based relevance methods\nOLM, Delete and UNK have consistently high correlation on MNLI and SST-2\nbut much lower correlation on CoLA.\nWe draw the following conclusions. OLM produces signiﬁcantly different\nexplanations than other occlusion methods. Delete and UNK have a higher\ncorrelation with each other for all three tasks than with OLM which is ev-\nidence that it stands out from other occlusion methods. The theoretical\ndifferences between these methods seem to manifest experimentally.\nOLM-S has the about as much correlation to other occlusion methods than\nto Sensitivity Analysis. This can be seen as experimental validation as a\nsensitivity method with a somewhat different intent than a relevance method.\nThe correlation between gradient-based methods and other methods is low\nacross all tasks. This could indicate that gradient methods do not capture the\ndiscrete nature of NLP (see section Gradient-based Explanation Methods in\nNLP (2.2)). Note that we use correlation in our argument in two seemingly\nconﬂicting ways. If the correlation of explanations is close to 0, they are\nindependent of each other. It is unlikely that methods with independent\nresults can both give very good explanations. Furthermore, if the correlation\nof explanations is close to 1, the methods are redundant. They might have\ntheoretical differences but these at least did not manifest in the experiments.\nOn the whole, we think different sensible explanation methods should have\npositive correlation 0 ≪c ≪1. This appears to be the case for OLM and\nmost compared explanation methods.\nTable 4.1 shows example explanations for the input “good ﬁlm , but very\nglum .” from the SST-2 dataset. Table 4.2 shows the sampling examples that\nOLM used to give relevance to the words. The maximum value is easy to\ninterpret for occlusion-based methods, as it indicates the change in prediction\nif the feature with the maximum value is occluded. OLM and OLM-S give\nlower relevances to “good” because alternatives sometimes lead to a positive\nclassiﬁcation as well. Interestingly, alternatives to “ﬁlm” more often lead to a\n34\nChapter 4. Experiments\nnegative classiﬁcation. This indicates that the meaning or strength of “good”\ndepends on the following noun. Surprisingly, “glum” also receives a positive\nrelevance from OLM. There are alternatives, especially “bad”, that change\nthe prediction to negative. Furthermore, the classiﬁcation gives positive\nsentiment a probability of 98%, so no negative word had a big effect in the\noriginal sentence.\nFor two out of ﬁve words the original word was the most frequent sample\nfrom the language model. Additionally, the comma was resampled 84% of\nthe time and the period every time. This indicates that the information of\nthese was not completely lost as they are made very likely by the context.\nMany resampled words (prediction < 0.5) lead to a negative sentiment\nclassiﬁcation. Also, many resampled words lead to a classiﬁcation where the\nmodel assigns a high probability to one class, i.e. is very sure. Only the ﬁrst\nword has several replacements where the model is unsure.\nThis example also highlights a disadvantage of OLM. The structure “good\n[...] but [...] glum” can not be adequately evaluated by resampling only one\nword. It is possible to argue that two of these words already determine the\nsentiment of the third.\n4.2\nCoLA Corner Case\nCoLA is an interesting edge case for our method. In the CoLA dataset there\nare inputs that are grammatically acceptable and inputs that are unacceptable.\nThe task of a model is to ﬁnd out to which category the input belongs. The\nlanguage model in OLM mostly saw acceptable sentences during training,\nthus it can be assumed that OLM tries to resample such that grammatically\nacceptable input appears whenever possible. This can be seen as ﬂawed,\nbecause on average the resampling should lead to a similar class distribution\nas the original dataset.\nTherefore, in theory, OLM is able to identify important words if replacements\nmake the sentence acceptable, because it assigns relevance if the prediction\nof the model changes from ungrammatical to grammatical. However, it does\nnot identify the opposite case, where an input is grammatical only because of\na speciﬁc construction. OLM would not modify the input to be unacceptable.\n4.2. CoLA Corner Case\n35\nIn other words, CoLA is a task where the approximation pdata ≈pLM in\nEq. (3.4) is not necessarily true, the resampling is not faithful to the data.\n4.2.1\nStatistical Analysis of Explanations\nTo investigate this, we try to answer the following question. How much do\nthe explanations for grammatically acceptable and unacceptable sentences\ndiffer? We hypothesize that the explanations for unacceptable inputs have\nhigher values on average.\nFirst, we have to set the classes on equal footing. We only use explanations\nof inputs that were predicted correctly by the ROBERTABASE model from\nsection Correlation of Explanation Methods (4.1) and with a probability p\nof at least 0.9. The relevances produced by OLM are always in the range\n[p −1, p], as for all occlusion-based methods (see Eq. (3.1)). This ensures that\nall explanations fall into the range [−0.1, 1]. We have 165 sentences labeled\nas unacceptable and 678 labeled as acceptable with probability p ≥0.9.\nWe measure the sum, average and maximum of relevances over input sen-\ntences. For each of these measures we get results for acceptable and unaccept-\nable sentences, which can be compared with a statistical signiﬁcance test. We\nchoose Welch’s t-test (Welch, 1947) which enables samples sizes and variances\nto be different for both classes. We can ignore that the samples are not from a\nnormal distribution because of the large sample size (Kendall, 1951). Table\n4.6 shows results of these tests. The null hypothesis of the averages being\nequal can be rejected for all comparisons because all three tests are highly\nsigniﬁcant.\nWe feel obliged to mention that this is not a conclusive test. Features of\ninputs of different classes need not have equivalent properties. For accept-\nability datasets, there are many more possible constructions for unacceptable\nsentences than acceptable sentences. Thus, we must not expect a symmetry\nbetween the explanations of different classes.\nThe other two occlusion methods Delete and UNK ﬁnd more relevance in the\nacceptable sentences. This is likely due to them perturbing grammatically\nacceptable input such that it is not acceptable. The Delete method removes\nwords, the UNK method replaces them with the <UNK> token, which may\nnot share all syntactic properties with the original word. Thus, they are both\n36\nChapter 4. Experiments\nrelevance aggregation\nAvg.\nSum\nMax\nunacceptable sentence\n0.275\n1.89\n0.893\nacceptable sentence\n0.0384\n0.304\n0.172\np-value\n<0.001\n<0.001\n<0.001\nTABLE 4.6: Relevance values accumulated over inputs by the\nmethod in the column header, averaged over all sentences with\ncorrect classiﬁcation and probability p ≥0.9. For all accumu-\nlations the averages differ dramatically, which indicates that\nthe resamples changed the prediction a lot more for unaccept-\nable sentences. A Welch’s t-test was performed to compare the\nmeans and yielded a p-value < 0.001 for all three methods.\nlikely to break the syntax of a sentence. This conﬁrms that our method does\ndiffer signiﬁcantly from other occlusion-based methods.\nWe show some randomly selected examples of the explanations in Table\n4.7. The sentences correctly classiﬁed as unacceptable all have words that\ncan be replaced to make the sentence acceptable. The sentences correctly\nclassiﬁed as acceptable have few words with high relevance, so the language\nmodel rarely created sentences that the classiﬁer considered unacceptable.\nid\nrelevances\nmax value\n1\nJohn paid me against the book .\n0.99\n2\nThe person confessed responsible .\n1\n3\nMedea tried the nurse to poison her children .\n0.92\n4\nto die is no fun .\n0.49\n5\nThis teacher is a genius .\n0.056\n6\nSoaring temperatures are predicted for this weekend .\n0.08\nTABLE 4.7: Six randomly selected example sentences from the\nCoLA dataset with explanations. The upper three examples\nare grammatically unacceptable, the lower three examples are\nacceptable. All examples are correctly classiﬁed with a prob-\nability of at least 0.9. The upper examples have words with\nmuch higher relevance as the resampled words can make the\nsentence acceptable. In the lower examples the resampled\nwords change the prediction only once. Sentences with likely\nreplacement words that changed the prediction can be found\nin Table 4.8.\n4.3. FAVA\n37\nid\norig. word\nnew sentence\nprediction\n1\npaid\nJohn pushed me against the book .\n4e-4\n1\npaid\nJohn pressed me against the book .\n3.6e-4\n1\nme\nJohn paid damages against the book .\n4.3e-4\n1\nme\nJohn paid taxes against the book .\n7.9e-4\n1\nagainst\nJohn paid me for the book .\n3.7e-4\n2\nconfessed\nThe person was responsible .\n4.4e-4\n2\nconfessed\nThe person is responsible .\n4.4e-4\n2\nresponsible\nThe person confessed himself .\n4e-4\n3\ntried\nMedea orders the nurse to poison her children .\n3.5e-4\n3\nnurse\nMedea tried the same to poison her children .\n1.1e-3\n4\ndie\nto eat is no fun .\n0.089\nTABLE 4.8: Resampled sentences for Table 4.7. All resampled\nwords that changed the predicted class and were sampled at\nleast 10 out of 100 times are depicted. Most of the previously\nunacceptable sentences are now acceptable and thus correctly\nclassiﬁed by the model. In the last row we can see that the only\nfrequent sample for acceptable sentences that changed the pre-\ndiction was misclassiﬁed by the model. The upper rows of the\ntable show that the interval of predictions for acceptable sam-\nples is very narrow. They have a probability between 0.035%\nand 0.11% of being unacceptable, which is only a difference by\nfactor three.\nExamples of these resampled sentences can be found in Table 4.8.\nWe take this as evidence that the language model is able to construct gram-\nmatical sentences even under adverse circumstances. The inspection of this\ndataset with OLM can be a useful analysis. However, the approximation in\nEq. (3.4) of the language model distribution approximating the data distribu-\ntion does not hold up. The relevances of words in unacceptable sentences is\nampliﬁed because the language model tries to choose words that build an\nacceptable sentence. We consider the results of OLM on this task meaningful\nbut not unconditionally. The task shows investigating the approximation, or\nadapting the language model for the classiﬁcation task, can be pondered.\n4.3\nFAVA\nWe conclude with an experiment that resembles CoLA but with a speciﬁc\nlinguistic aspect and a possibility for more model introspection. Kann et al.\n(2019) introduce the Frames and Alternations of Verbs Acceptability (FAVA)\n38\nChapter 4. Experiments\nverb frame\nrelevances\nmax value\ncausative-\nthe chapter edited .\n0.49\ninchovative\ndavid edited the chapter .\n0.18\nspray-\nmichael poured the bucket with the soup .\n0.85\nload\nmichael poured the soup into the bucket .\n0.028\nthere-\nthere agreed with the politician a protester .\n0.082\nload\na protester agreed with the politician .\n0.15\nunderstood-\nkelly joked david .\n0.84\nobject\nkelly and david joked .\n0.42\ndative\nnicole proclaimed the greatest athlete to rebecca .\n0.43\nnicole proclaimed rebecca the greatest athlete .\n0.25\nTABLE 4.9: Randomly selected example explanations for sen-\ntence pairs in the FAVA dataset. The examples are ordered by\nverb frame, the unacceptable example is always the ﬁrst. All\nexamples are correctly classiﬁed with probability p ≥0.9. The\nverbs are frequently identiﬁed as the most important word in a\nsentence.\ndataset. It contains constructed sentences around verb properties that yield\nacceptable and unacceptable sentences. Most of these sentences come in\npairs of acceptable and unacceptable sentence, which are variations of each\nother and at least one uses the given verb frame. This allows a direct com-\nparison that exceeds the possibilities of CoLA. We only select these pairs for\nevaluation. Example sentences and frames can be found in Table 4.9.\nIn this case we do not ﬁne-tune a model on the dataset but use the\nROBERTABASE model from TextAttack ﬁne-tuned on CoLA.5 Since this model\nwas trained on another dataset, this allows us to evaluate on the train, devel-\nopment and test set. We do this to retrieve enough data. This dataset version\nencompasses 6466 single sentences or 3233 pairs. The accuracy for the model\non our class-balanced version of the dataset is 84.3%. The best model in the\noriginal paper (Kann et al., 2019) achieved an accuracy of 85.5% on the test\nset of the whole dataset.\nThis dataset is split into ﬁve types of syntactic verb frame alternations. Verbs\n5The model was originally published at https://github.com/QData/TextAttack. We\nused the version from the transformers package https://github.com/huggingface/\ntransformers.\n4.3. FAVA\n39\nunacceptable\nacceptable\naverage verb relevance\n0.400\n0.0960\naverage word relevance\n0.115\n0.0429\np-value\n<0.001\n<0.001\nTABLE 4.10: Relevance values averaged over all sentences\nwith correct classiﬁcation and probability p ≥0.9. For both\nunacceptable and acceptable sentences the verb is signiﬁcantly\nmore relevant than the average word. A Welch’s t-test was\nperformed to compare the means and yielded a p-value < 0.001\nfor both averages.\ncan be seen as the central part-of-speech to this task, as constructions are\nbuilt around the fact that they work for some verbs but not for all. Thus, we\nare interested whether verbs have a higher relevance than other words. We\nperform signiﬁcance tests as in section Statistical Analysis of Explanations\n(4.2.1). Table 4.10 shows statistical signiﬁcance for both acceptable and\nunacceptable sentences. For unacceptable sentences the relevance of verbs\nis more than three times as large as for the average word. For acceptable\nsentences the relevance is more than two times as large. This shows that\nOLM identiﬁes verbs as important words in this task which is centered on\nverbs.\nIn Table 4.9 we show an example of the explanation of sentence pairs for each\nof the syntactic verb frame alternations. For all verb frames the verb gets the\nhighest relevance in at least one of the two paired sentences. In only one of\nthe verb frames the verb has the highest relevance in both paired sentences.\nAs expected, the relevances of the unacceptable sentences are higher than\nthe relevances of acceptable sentences.\nWe also investigate how the relevances of verbs in sentence pairs are cor-\nrelated. We already saw that the relevances are higher for words of unac-\nceptable sentences. Figure 4.1 provides a visualization of the relevances of\nsentence pairs. The null hypothesis of the relevances not being correlated\nis rejected with p-value < 0.005. The relevances are weakly negatively cor-\nrelated with a Pearson’s correlation coefﬁcient of r = −0.134. We interpret\nthis as a sanity check (Adebayo et al., 2018) that our method does not simply\nassign high relevance to verbs.\nThe relevance of verbs in unacceptable sentences is higher on average, as is\n40\nChapter 4. Experiments\nFIGURE 4.1: Correlation of verb relevance for FAVA sentence\npairs. The x-axis denotes the relevance of the verb in the un-\nacceptable sentence of a sentence pair. The y-axis denotes the\nrelevance of the verb in the acceptable sentence of a sentence\npair. Only pairs are depicted where both sentences were cor-\nrectly predicted with probability p ≥0.9. Thus, the interval of\npossible values for both axes is [−0.1, 1].\nthe case for an average of all words in the CoLA experiments. Few verbs\nhave high relevance in both sentences of a sentence pair. This can have two\npossible reasons. If the classiﬁcation model does not contextualize a verb\ncompletely, the verb as a feature itself inﬂuences the model prediction. This\nwould mean an imbalance for the resampling predictions. Also, the verbs are\nunlikely to be special cases of both the unacceptable and the acceptable sen-\ntence. Resampling is more likely to change the prediction if the construction\naround the verb only does or does not work for few samples.\nAll in all, these experiments provide evidence that OLM is a useful tool for\nanalyzing black-box models. At the least, it provides easily interpretable\nexplanations. We identify language modeling as a standout feature of OLM\nthat makes its explanations vastly different from other methods. The intuitive\ntheory of OLM allows for evaluation and introspection of relevances even\nfor corner cases of the method.\n41\n5 Conclusion\nWe summarize the topics and central arguments of this thesis. Furthermore,\nwe point out weaknesses of our approach and how they could be alleviated.\nLast, we elaborate some possible future work.\n5.1\nSummary\nIn the ﬁrst chapter of this thesis we provide the necessary theoretical back-\nground. We introduce Deep Learning by motivating interest in the topic,\nmainly through its achieved results. Additionally, we discuss practical as-\npects, such as the architectures of deep neural networks and how to train\nthem. We round out this section by pointing to capabilities of neural net-\nworks and how they are important to this thesis.\nWe introduce the topic of explainability of neural networks. We summarize\ncentral papers and surveys of this topics. Focus is laid on theoretical aspects\nof explainability. This is done partly by showing work on axioms for expla-\nnation methods. In this context we provide our ﬁrst contribution, a novel\naxiom. It is guided by the principle that a feature of an input to a normalized\nprediction function contributes as much to a set of classes as it detracts from\nthe complementary classes.\nThe second contribution is a theoretical argument against gradient-based\nexplanation methods in natural language processing. We show that input\nin NLP is discrete and thus the data likelihood distribution is discrete. This\ninhibits the functionality of gradients when analyzing the prediction function.\nWe contrast this with the likelihood function in vision.\nFurthermore, we contribute another theoretical argument to explainability.\nWe discuss why a general evaluation of explanation methods is unlikely to\nexist. The main argument is that the ground truth for the explanation is only\nheld in the model, which is exactly what an explanation method is trying to\n42\nChapter 5. Conclusion\nextract. Since this can only be done by approximation, we point out that there\nis a false dichotomy between the evaluative rules for this approximation and\nthe explanation method that fulﬁlls them.\nThe central methods chapter introduces our main contribution, a novel ex-\nplanation method. It consists of the combination of two existing methods,\nOcclusion and Language Modeling and is coined OLM. Occlusion is a tech-\nnique in explainability that is used either to explain black-box models or\nevaluate explanations. It can be seen as incomplete because it does not de-\ntermine how to replace occluded features. For this replacement we propose\nusing language modeling in NLP. We show that language modeling is espe-\ncially suited to sample replacement. They excel at this task because language\nunderstanding by language models is the foundation of state-of-the-art mod-\nels in NLP.\nWe motivate OLM psychologically and through information theory. These\narguments lead us to selecting and evaluating likely alternatives to explain\nfeatures of the original input. The formula for OLM is achieved by inserting\nsampling with a language model into the difference of probabilities formula.\nWe analyze our method by going over the axioms that we introduced in the\npreceding chapter. We provide proofs of compliance. Since the formula for\nOLM is a weighted mean of predictions, we add the standard deviation as an\nadditional measure OLM-S. This measure is intended as a sensitivity method,\nnot a relevance method.\nWe provide experimental evaluation of our method. As alluded to in the\nsecond chapter, we do not consider a complete evaluation, like a single\nbenchmark, possible. We try to compare our method to existing methods\nby a correlation experiment. We work under the assumption that sensible\nmethods correlate, but when introducing a novel method it is also important\nto see that there is no perfect correlation. Both aspects are shown over three\ntasks. The results on one task, CoLA, point to a possible problem of OLM.\nThe second experiment is a deeper dive into the relevances of our method on\nCoLA. Linguistic acceptability is a context where it is wrong to assume that\nthe distribution of a language model is similar to the distribution of the data.\nWe show that this manifests in the explanations. We discuss how to interpret\nthese explanations and how they make sense once the context is understood.\nThe introspection with OLM is possibly deeper than with other relevance\n5.2. Future Work\n43\nmethods, as we can also provide the compared samples. Furthermore, we\nshow that the language modeling works as it is supposed to.\nFinally, we experiment on a dataset that is even more speciﬁc than CoLA.\nIt contains pairs of sentences with verb frames that are either acceptable\nor unacceptable. This provides the additional information that the verb\nhas to be very important for the model, as different verbs lead to different\nclassiﬁcations in the same structure. We show that our method is able to\nidentify verbs as very important. Furthermore, we show that relevances of\nverbs in sentence pairs is weakly negatively correlated, which indicates that\nour method does not blindly assign relevance to verbs.\n5.2\nFuture Work\nWe describe some of the shortcomings, unexplored areas and possible ideas to\nextend this work. All identiﬁed weaknesses concern the practical application\nof our method.\nThere are three main problems with our approach. The major problem\nwe identiﬁed during method development and experiments is the approx-\nimation of the data with a language model. While this is certainly valid\nfrequently, we saw that there are exceptions. A language model can always\nprovide syntactically correct data which does not make sense for the task. Of\nall possible sentences, most sentences are not suitable for a dataset because\nthey do not indicate a class. A language model alone has no mechanism to\ndetect this.\nOne possibility to improve selection of resampled tokens would be to train a\ngenerative adversarial network (Goodfellow et al., 2014) to detect whether\nan input is part of the given dataset. The discriminator of this model could\ndetermine whether proposed resampled tokens create a likely input and also\nassign a probability. This probability could be used or combined with the\nlanguage model probability to select resampled tokens. This would ensure\nthat the resampled data ﬁts the task.\nA second problem of our method is that we resample exactly one token in a\ntokens previous place. If we choose words as replacement units, this prevents\nthe use of language models that produce sub-word tokens frequently. It\nwould be possible to use beam search to allow them to build a word from\n44\nChapter 5. Conclusion\nthese sub-word tokens. Additionally, If we want to change the position, we\nwould not know where to position a replacement. This is problematic in\nseveral ways. In general, if we take out the information of one word then\nthere are other sensible replacements that do not consist of just one word.\nFurthermore, if the replaced word forces a syntactic structure in the input\nthat is not determined by any of the other inputs, it could be sensible to allow\nthis structure to change.\nIn an information theory sense, the removal of the information of one token\nplaces the sentence into another set of possibilities that contain the informa-\ntion of all other tokens but not necessarily in the exact phrasing as before.\nThis is the same phenomenon that humans have when thinking of formulat-\ning a sentence and changing the structure at the last moment because the last\nword ﬁts the new structure better. To allow this, we need an architecture that\nis invariant to rephrasing. It would be possible to use an encoder-decoder\narchitecture to encode the information of the sentence without the tokens of\ninterest. Then we have a representation of the sentence that can be used to\ngenerate the replacement sentence. However, it would be difﬁcult to detect\nwhether there is information missing that needs to be sampled in the encoded\nstate. E.g., if a strong adjective is removed in a sentiment classiﬁcation task,\nhow do we produce a resampled sentence with sentiment?\nAnother interesting aspect is choosing other features as replacement units.\nFor tasks with many sentences it could be interesting to measure the effect\nof sentences as features. However, we consider it unlikely that resampled\nsentences produce vastly different results than removed sentences. Syntax is\nno longer an issue, only context for other sentences may be important.\nThe third practical difﬁculty is the dependency of our approach on a language\nmodel. Even state-of-the-art language models differ from the true likelihood\nof language data. This problem is unlikely to be resolved practically, as\nlanguage models will presumably not become perfect in the future. Our\nmethod depends on automatic generation of replacements. This complicates\nresampling multiple features at the same time, as the approximation errors\nare likely to accumulate.\nAll mentioned practical points concern the possibilities and difﬁculties of\ngenerating good comparison samples. This thesis is an attempt towards\n5.2. Future Work\n45\nexplanation methods in natural language processing that do not use unjusti-\nﬁed inputs or methods. Therefore, a deeper analysis of sampling options to\nimprove the tailoring of OLM to NLP is an adequate future research direction.\n46\nBibliography\nAmina Adadi and Mohammed Berrada. Peeking inside the black-box: A\nsurvey on explainable artiﬁcial intelligence (xai). IEEE Access, 6:52138–\n52160, 2018.\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz\nHardt, and Been Kim. Sanity checks for saliency maps. In Advances in\nNeural Information Processing Systems, pages 9525–9536, 2018.\nDavid Alvarez-Melis and Tommi Jaakkola. A causal framework for ex-\nplaining the predictions of black-box sequence-to-sequence models. In\nProceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, pages 412–421, 2017.\nMarco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. Towards\nbetter understanding of gradient-based attribution methods for deep neu-\nral networks. International Conference on Learning Representations, 2018.\nLeila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and\nWojciech Samek. \"What is relevant in a text document?\": An interpretable\nmachine learning approach. PloS one, 12(8):e0181142, 2017.\nSebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen,\nKlaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for\nnon-linear classiﬁer decisions by layer-wise relevance propagation. PloS\none, 10(7):e0130140, 2015.\nDavid Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe,\nKatja Hansen, and Klaus-Robert Müller. How to explain individual classi-\nﬁcation decisions. Journal of Machine Learning Research, 11(Jun):1803–1831,\n2010.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine\ntranslation by jointly learning to align and translate. International Conference\non Learning Representations, 2015.\nBIBLIOGRAPHY\n47\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A\nneural probabilistic language model. Journal of machine learning research, 3:\n1137–1155, 2003.\nJakob Bernoulli. Ars conjectandi. Impensis Thurnisiorum, fratrum, 1713.\nBernd Bohnet, Ryan McDonald, Gonçalo Simões, Daniel Andor, Emily Pitler,\nand Joshua Maynez. Morphosyntactic tagging with a meta-bilstm model\nover context sensitive token encodings. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 2642–2652, 2018.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. En-\nriching word vectors with subword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146, 2017.\nLéon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In\nAdvances in neural information processing systems, pages 161–168, 2008.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Jennifer C.\nLai, and Robert L. Mercer. An estimate of an upper bound for the entropy\nof english. Computational Linguistics, 18(1):31–40, 1992.\nRuth M. J. Byrne. The rational imagination: How people create alternatives to\nreality. MIT press, 2007.\nAugustin Cauchy.\nMéthode générale pour la résolution des systemes\nd’équations simultanées.\nComptes rendus hebdomadaires des séances de\nl’Académie des Sciences, 25:536–538, 1847.\nAnna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous,\nand Yann LeCun. The loss surfaces of multilayer networks. In Artiﬁcial\nintelligence and statistics, pages 192–204, 2015.\nDan Ciregan, Ueli Meier, and Jürgen Schmidhuber. Multi-column deep neu-\nral networks for image classiﬁcation. In 2012 IEEE conference on computer\nvision and pattern recognition, pages 3642–3649. IEEE, 2012.\nLi Deng and Dong Yu. Deep learning: methods and applications. Foundations\nand Trends® in Signal Processing, 7(3–4):197–387, 2014.\n48\nBIBLIOGRAPHY\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-training of deep bidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–4186. Association for\nComputational Linguistics, 2019.\nFinale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable\nmachine learning. CoRR, arXiv:1702.08608, 2017.\nFinale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gersh-\nman, David O’Brien, Stuart Schieber, James Waldo, David Weinberger,\nand Alexandra Wood. Accountability of ai under the law: The role of\nexplanation. CoRR, arXiv:1711.01134, 2017.\nJean-Baptiste Estoup.\nGammes sténographiques: méthode et exercices pour\nl’acquisition de la vitesse. Institut sténographique, 1916.\nRuth C. Fong and Andrea Vedaldi.\nInterpretable explanations of black\nboxes by meaningful perturbation. In Proceedings of the IEEE International\nConference on Computer Vision, pages 3429–3437, 2017.\nRobert Geirhos, Carlos R. M. Temme, Jonas Rauber, Heiko H. Schütt, Matthias\nBethge, and Felix A. Wichmann. Generalisation in humans and deep\nneural networks. In Advances in neural information processing systems, pages\n7538–7550, 2018.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative\nadversarial nets. In Advances in neural information processing systems, pages\n2672–2680, 2014.\nBryce Goodman and Seth Flaxman. European union regulations on algo-\nrithmic decision-making and a “right to explanation”. AI magazine, 38(3):\n50–57, 2017.\nKyle Gorman and Steven Bedrick. We need to talk about standard splits.\nIn Proceedings of the 57th annual meeting of the association for computational\nlinguistics, pages 2786–2791, 2019.\nBIBLIOGRAPHY\n49\nRiccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca\nGiannotti, and Dino Pedreschi. A survey of methods for explaining black\nbox models. ACM computing surveys (CSUR), 51(5):1–42, 2018.\nBoris Hanin. Universal function approximation by deep neural nets with\nbounded width and relu activations. CoRR, arXiv:1708.02691, 2017.\nDavid Harbecke and Christoph Alt. Considering likelihood in NLP classiﬁ-\ncation explanations with occlusion and language modeling. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics:\nStudent Research Workshop, pages 111–117. Association for Computational\nLinguistics, 2020.\nZellig S. Harris. Distributional structure. Word, 10(2-3):146–162, 1954.\nJohn Hewitt and Christopher D. Manning. A structural probe for ﬁnding\nsyntax in word representations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 4129–\n4138, 2019.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning\nfor text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Papers), pages 328–339,\n2018.\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon\nTran, and Aleksander Madry. Adversarial examples are not bugs, they\nare features. In Advances in Neural Information Processing Systems, pages\n125–136, 2019.\nSarthak Jain and Byron C. Wallace. Attention is not Explanation. In Proceed-\nings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 3543–3556, 2019.\nMichael I. Jordan. Artiﬁcial intelligence—the revolution hasn’t happened\nyet. Harvard Data Science Review, 2019.\nDaniel Kahneman and Dale T. Miller. Norm theory: Comparing reality to its\nalternatives. Psychological review, 93(2):136, 1986.\n50\nBIBLIOGRAPHY\nDaniel Kahneman and Amos Tversky. The simulation heuristic. Technical\nreport, Stanford University, CA, Department of Psychology, 1981.\nKatharina Kann, Alex Warstadt, Adina Williams, and Samuel Bowman. Verb\nargument structure alternations in word and sentence embeddings. In\nProceedings of the Society for Computation in Linguistics (SCiL) 2019, pages\n287–297, 2019.\nMaurice George Kendall. The Advanced Theory of Statistics: Vol: II. Charles\nGrifﬁn and Co., Ltd., London, 1951.\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fer-\nnanda Viegas, and Rory Sayres. Interpretability beyond feature attribution:\nQuantitative testing with concept activation vectors (tcav). In International\nConference on Machine Learning, pages 2673–2682, 2018.\nYoon Kim. Convolutional neural networks for sentence classiﬁcation. In\nProceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1746–1751. Association for Computational Lin-\nguistics, 2014.\nPieter-Jan Kindermans, Kristof T. Schütt, Maximilian Alber, Klaus-Robert\nMüller, Dumitru Erhan, Been Kim, and Sven Dähne. Learning how to\nexplain neural networks: Patternnet and patternattribution. International\nConference on Learning Representations, 2018.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber,\nKristof T. Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. The (un)\nreliability of saliency methods. In Explainable AI: Interpreting, Explaining\nand Visualizing Deep Learning, pages 267–280. Springer, 2019.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimiza-\ntion. International Conference on Learning Representations, 2014.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi-\nﬁcation with deep convolutional neural networks. In Advances in Neural\nInformation Processing Systems, pages 1097–1105, 2012.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language\nindependent subword tokenizer and detokenizer for neural text processing.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 66–71, 2018.\nBIBLIOGRAPHY\n51\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and\nunderstanding neural models in nlp. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 681–691, 2016.\nSeppo Linnainmaa. The representation of the cumulative rounding error of\nan algorithm as a taylor expansion of the local rounding errors. Master’s\nThesis (in Finnish), University of Helsinki, 1970.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of\nlstms to learn syntax-sensitive dependencies. Transactions of the Association\nfor Computational Linguistics, 4:521–535, 2016.\nZachary C. Lipton. The mythos of model interpretability. Queue, 16(3):31–57,\n2018.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and\nNoah A. Smith. Linguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 1073–1094, 2019a.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:\nA robustly optimized BERT pretraining approach. CoRR, arXiv:1907.11692,\n2019b.\nTania Lombrozo. The structure and function of explanations. Trends in\ncognitive sciences, 10(10):464–470, 2006.\nZhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The\nexpressive power of neural networks: A view from the width. In Advances\nin Neural Information Processing Systems, pages 6231–6239, 2017.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y.\nNg, and Christopher Potts. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the Association for Computational\nLinguistics: Human Language Technologies, pages 142–150, 2011.\nChristopher D. Manning and Hinrich Schütze. Foundations of statistical natural\nlanguage processing. MIT press, 1999.\n52\nBIBLIOGRAPHY\nMitchell Marcus et al. Treebank-3 LDC99T42, 1999. Philadelphia: Linguistic\nData Consortium.\nTom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Di-\nagnosing syntactic heuristics in natural language inference. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics,\npages 3428–3448, 2019.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient esti-\nmation of word representations in vector space. CoRR, arXiv:1301.3781,\n2013.\nTim Miller. Explanation in artiﬁcial intelligence: Insights from the social\nsciences. Artiﬁcial Intelligence, 267:1–38, 2019.\nSina Mohseni and Eric D. Ragan. A human-grounded evaluation benchmark\nfor local explanations of machine learning. CoRR, arXiv:1801.05075, 2018.\nGrégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Methods\nfor Interpreting and Understanding Deep Neural Networks. Digital Signal\nProcessing, 73:1–15, 2018.\nIsaac Newton. The method of ﬂuxions and inﬁnite series. Henry Woodfall, 1736.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.\nLibrispeech: an asr corpus based on public domain audio books.\nIn\n2015 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5206–5210. IEEE, 2015.\nDouglas B. Paul and Janet M. Baker. The design for the wall street journal-\nbased csr corpus. In Proceedings of the workshop on Speech and Natural\nLanguage, pages 357–362. Association for Computational Linguistics, 1992.\nKarl Pearson. VII. Note on regression and inheritance in the case of two\nparents. Proceedings of the Royal Society of London, 58(347-352):240–242, 1895.\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove:\nGlobal vectors for word representation. In Proceedings of the 2014 conference\non empirical methods in natural language processing (EMNLP), pages 1532–\n1543, 2014.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word\nBIBLIOGRAPHY\n53\nrepresentations. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–2237, 2018.\nSteven T. Piantadosi. Zipf’s word frequency law in natural language: A\ncritical review and future directions. Psychonomic bulletin & review, 21(5):\n1112–1130, 2014.\nDaniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahremani, Vi-\nmal Manohar, Xingyu Na, Yiming Wang, and Sanjeev Khudanpur. Purely\nsequence-trained neural networks for asr based on lattice-free mmi. In\nInterspeech, pages 2751–2755, 2016.\nNing Qian. On the momentum term in gradient descent learning algorithms.\nNeural networks, 12(1):145–151, 1999.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text transformer. CoRR,\narXiv:1910.10683, 2019.\nJoseph Raphson. Analysis Aequationum Universalis. London, 1690.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust\nyou?: Explaining the predictions of any classiﬁer. In Proceedings of the 22nd\nACM SIGKDD international conference on knowledge discovery and data mining,\npages 1135–1144. ACM, 2016.\nHerbert Robbins and Sutton Monro. A stochastic approximation method.\nThe annals of mathematical statistics, pages 400–407, 1951.\nMarko Robnik-Šikonja and Igor Kononenko. Explaining classiﬁcations for\nindividual instances. IEEE Transactions on Knowledge and Data Engineering,\n20(5):589–600, 2008.\nNeal J. Roese. Counterfactual thinking. Psychological bulletin, 121(1):133, 1997.\nNeal J. Roese and James M. Olson. What might have been: The social psychology\nof counterfactual thinking. Psychology Press, 2014.\nSebastian Ruder. Nlp’s imagenet moment has arrived. The Gradient, 2018.\n54\nBIBLIOGRAPHY\nDavid E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning\nrepresentations by back-propagating errors. nature, 323(6088):533–536,\n1986a.\nDavid E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning\ninternal representations by error propagation. In D. E. Rumelhart and\nJ. L. McClelland, editors, Parallel distributed processing: explorations in the\nmicrostructure of cognition, volume 1, chapter 8, pages 318–362. MIT Press,\n1986b.\nGerard Salton, Anita Wong, and Chung-Shu Yang. A vector space model for\nautomatic indexing. Communications of the ACM, 18(11):613–620, 1975.\nWarren S. Sarle. Neural networks and statistical models. In Proceedings of the\nNineteenth Annual SAS Users Group International Conference, 1994.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning impor-\ntant features through propagating activation differences. In International\nConference on Machine Learning, pages 3145–3153, 2017.\nRavid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural\nnetworks via information. CoRR, arXiv:1703.00810, 2017.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre,\nGeorge Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda\nPanneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John\nNham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine\nLeach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Master-\ning the game of go with deep neural networks and tree search. nature, 529\n(7587):484, 2016.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Ku-\nmaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis\nHassabis. A general reinforcement learning algorithm that masters chess,\nshogi, and go through self-play. Science, 362(6419):1140–1144, 2018.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside con-\nvolutional networks: Visualising image classiﬁcation models and saliency\nmaps. CoRR, arXiv:1312.6034, 2013.\nBIBLIOGRAPHY\n55\nThomas Simpson. Essays on Several Curious and Useful Subjects, in Speculative\nand Mix’d Mathematicks. H. Woodfall, 1740.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.\nManning, Andrew Y. Ng, and Christopher Potts. Recursive deep models\nfor semantic compositionality over a sentiment treebank. In Proceedings of\nthe 2013 conference on empirical methods in natural language processing, pages\n1631–1642, 2013.\nSuraj Srinivas and François Fleuret. Full-gradient representation for neural\nnetwork visualization. In Advances in Neural Information Processing Systems,\npages 4126–4135, 2019.\nDaniel Strigl, Klaus Koﬂer, and Stefan Podlipnig. Performance and scalability\nof gpu-based convolutional neural networks. In 2010 18th Euromicro Con-\nference on Parallel, Distributed and Network-based Processing, pages 317–324.\nIEEE, 2010.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution\nfor Deep Networks. In International Conference on Machine Learning, pages\n3319–3328, 2017.\nGabriel Synnaeve, Qiantong Xu, Jacob Kahn, Edouard Grave, Tatiana\nLikhomanenko, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and\nRonan Collobert. End-to-end asr: from supervised to semi-supervised\nlearning with modern architectures. CoRR, arXiv:1911.08460, 2019.\nNaftali Tishby and Noga Zaslavsky. Deep learning and the information\nbottleneck principle. In 2015 IEEE Information Theory Workshop (ITW),\npages 1–5. IEEE, 2015.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Systems, pages 5998–6008,\n2017.\nEllen M. Voorhees and Dawn M. Tice. Building a question answering test col-\nlection. In Proceedings of the 23rd annual international ACM SIGIR conference\non Research and development in information retrieval, pages 200–207, 2000.\n56\nBIBLIOGRAPHY\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual expla-\nnations without opening the black box: Automated decisions and the gdpr.\nHarv. JL & Tech., 31:841, 2017.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel R. Bowman. GLUE: A multi-task benchmark and analysis platform\nfor natural language understanding. International Conference on Learning\nRepresentations, 2017.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian\nMichael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A\nstickier benchmark for general-purpose language understanding systems.\nIn Advances in Neural Information Processing Systems, pages 3266–3280,\n2019a.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng,\nand Luo Si. Structbert: Incorporating language structures into pre-training\nfor deep language understanding. In International Conference on Learning\nRepresentations, 2019b.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network\nacceptability judgments. Transactions of the Association for Computational\nLinguistics, 7:625–641, 2019.\nBernard L. Welch. The generalization of student’s’ problem when several\ndifferent population variances are involved. Biometrika, 34(1/2):28–35,\n1947.\nSarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 11–20, 2019.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Charagram:\nEmbedding words and sentences via character n-grams. In Proceedings\nof the 2016 Conference on Empirical Methods in Natural Language Processing,\npages 1504–1515, 2016.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage\nchallenge corpus for sentence understanding through inference. In Pro-\nceedings of the 2018 Conference of the North American Chapter of the Association\nBIBLIOGRAPHY\n57\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers), pages 1112–1122, 2018.\nD. Randall Wilson and Tony R. Martinez. The general inefﬁciency of batch\ntraining for gradient descent learning. Neural networks, 16(10):1429–1451,\n2003.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,\nJeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser,\nStephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,\nJason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. Google’s neural machine translation system: Bridging\nthe gap between human and machine translation. CoRR, arXiv:1609.08144,\n2016.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhut-\ndinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining\nfor language understanding. In Advances in neural information processing\nsystems, pages 5754–5764, 2019.\nG. Udny Yule.\nOn the methods of measuring association between two\nattributes. Journal of the Royal Statistical Society, 75(6):579–652, 1912.\nMatthew D. Zeiler and Rob Fergus. Visualizing and understanding convolu-\ntional networks. In European conference on computer vision, pages 818–833.\nSpringer, 2014.\nQuanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable convolu-\ntional neural networks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 8827–8836, 2018.\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional\nnetworks for text classiﬁcation. In Advances in neural information processing\nsystems, pages 649–657, 2015.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,\nAntonio Torralba, and Sanja Fidler. Aligning books and movies: Towards\nstory-like visual explanations by watching movies and reading books. In\n58\nBIBLIOGRAPHY\nProceedings of the IEEE international conference on computer vision, pages\n19–27, 2015.\nLuisa M. Zintgraf, Taco S. Cohen, Tameem Adel, and Max Welling. Vi-\nsualizing deep neural network decisions: Prediction difference analysis.\nInternational Conference on Learning Representations, 2017.\nGeorge Kingsley Zipf. Human behavior and the principle of least effort. Addison-\nWesley Press, 1949.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-01-28",
  "updated": "2021-01-28"
}