{
  "id": "http://arxiv.org/abs/2102.00405v2",
  "title": "BNLP: Natural language processing toolkit for Bengali language",
  "authors": [
    "Sagor Sarker"
  ],
  "abstract": "BNLP is an open source language processing toolkit for Bengali language\nconsisting with tokenization, word embedding, POS tagging, NER tagging\nfacilities. BNLP provides pre-trained model with high accuracy to do model\nbased tokenization, embedding, POS tagging, NER tagging task for Bengali\nlanguage. BNLP pre-trained model achieves significant results in Bengali text\ntokenization, word embedding, POS tagging and NER tagging task. BNLP is using\nwidely in the Bengali research communities with 16K downloads, 119 stars and 31\nforks. BNLP is available at https://github.com/sagorbrur/bnlp.",
  "text": "BNLP: Natural language processing toolkit for Bengali\nSagor Sarker\nBegum Rokeya University, Rangpur, Bangladesh\nbrursagor@gmail.com\nAbstract\nBNLP is an open-source language process-\ning toolkit for Bengali consisting of tokeniza-\ntion, word embedding, part of speech(POS)\ntagging, name entity recognition(NER) facili-\nties. BNLP provides pre-trained model with\nhigh accuracy to do model-based tokeniza-\ntion, embedding, POS, NER tasks for Bengali.\nBNLP pre-trained model achieves signiﬁcant\nresults in Bengali text tokenization, word em-\nbeddings, POS, and NER task. BNLP is be-\ning used widely by the Bengali research com-\nmunities with 25K downloads, 138 stars, and\n31 forks.\nBNLP is available at https://\ngithub.com/sagorbrur/bnlp.\n1\nIntroduction\nNatural language processing is one of the most im-\nportant ﬁelds in computation linguistics. Tokeniza-\ntion, embedding, POS, NER, text classiﬁcation,\nlanguage modeling are some of the sub-tasks of\nNLP. Any computational linguistics researcher or\ndeveloper needs hands-on tools to do these subtasks\nefﬁciently. Due to the recent advancement of NLP,\nthere are so many tools and methods to do word\ntokenization, word embedding, POS, NER in the\nEnglish language. NLTK (Loper and Bird, 2002),\ncoreNLP (Manning et al., 2014), spaCy (Honni-\nbal and Montani, 2017), AllenNLP (Gardner et al.,\n2018), Flair (Akbik et al., 2019), stanza (Qi et al.,\n2020) are few of the tools. These tools provide a\nvariety of methods to do tokenization, embedding,\nPOS, NER, language modeling for the English lan-\nguage. Support for other low resource languages\nlike Bengali is limited or no support at all. A re-\ncent tool like iNLTK (Arora, 2020) is an initial\napproach for different Indic languages including\nBengali. But as it groups with other indic lan-\nguages special monolingual support like easy pre-\nprocessing, tokenization, embedding, POS, NER\nfor Bengali is missing. Besides, iNLTK is mostly\nbased on deep learning(DL) language model based\npipeline, which needs DL based infrastructure to\ndo NLP tasks. And that makes iNLTK verbose\nand language model centric tool for Bengali lan-\nguage. On the other side, BNLP is totally machine\nlearning(ML) based toolkit that can do an instant\nprocess for Bengali NLP tasks. Table 1 provides\ndetailed feature comparison between BNLP and\nother tools.\nBNLP is an open-source language processing\ntoolkit for Bengali is build to address this problem\nand breaks the barrier to do different Bengali NLP\ntasks by:\n• Providing different tokenization methods to\ntokenize Bengali text efﬁciently\n• Providing different embedding method to em-\nbed Bengali word using the pre-trained model\nand also provides an option to train an embed-\nding model from scratch\n• Providing hands-on start option for POS or\nNER of Bengali sentences and also provides\nan option for training CRF based POS tagger\nor NER model from scratch.\nBNLP offers several widely used text preprocess-\ning techniques like removing stopwords, remov-\ning punctuations, removing foreign words. BNLP\nGithub repositories1 for source code of the pack-\nage, pre-trained model and documentation2. BNLP\nlibraries have a permissive MIT license. BNLP is\neasy to install via pip or by cloning repository, easy\nto plugin with any python projects.\n2\nRelated Works\nThere is a signiﬁcant number of open-source NLP\ntools for the English language. Tools like NLTK\n(Loper and Bird, 2002), coreNLP (Manning et al.,\n2014), spaCy (Honnibal and Montani, 2017), Al-\nlenNLP (Gardner et al., 2018), Flair(Akbik et al.,\n1https://github.com/sagorbrur/bnlp\n2https://bnlp.readthedocs.io/\narXiv:2102.00405v2  [cs.CL]  1 Dec 2021\nTool\nSupport\nBengali\nML Based\nPre-trained\nModel\nTokenizer\nEmbedding\nPOS\nNER\nLM\nNLTK\nspaCy\nFlair\nstanza\ninltk\nBNLP\nTable 1: BNLP feature comparison with other popular tools\nFigure 1: An example of doing basic tokenization using\nBNLP\n2019), stanza (Qi et al., 2020) are few of the tools.\nThese tools mostly build for the English language\nand have limited or no support for low resource\nlanguages. Especially in a low resource language\nlike Bengali, there is a huge scarcity of tools to pro-\ncess. iNLTK (Arora, 2020) is an initial approach to\nhelp process Bengali with tokenization, language\nmodel support. But as it’s a group with different\nIndic languages, a special monolingual concern\nfor Bengali is missing. Keeping that concern in\nmind we build BNLP to support especially Ben-\ngali and provides tokenization, embedding, POS,\nNER supports. Besides, iNLTK is mostly based on\nDL language model based pipeline, which needs\nDL based infrastructure to do NLP tasks. And that\nmakes iNLTK verbose and language model centric\ntool for Bengali language. On the other side, BNLP\nis a totally ML based toolkit that can do an instant\nprocess for Bengali NLP tasks.\n3\nBNLP API\nOur design principle was to make the tool easily\nusable with a few lines of code. The researcher or\ndeveloper can integrate this tool with installing a\nsimple python package. In this section, we are de-\nscribing how to do different NLP tasks for Bengali\ntext using BNLP toolkit.\nFigure 2: An example of generating word vector using\ntrained model and BNLP\n3.1\nTokenizers\nBNLP provides three different tokenization options\nto tokenize Bengali text. Under rule-based tok-\nenizer BNLP provides Basic Tokenizer a punctua-\ntion splitting tokenizer and NLTK3 tokenizer. As\nNLTK tokenizer is for the English language, we\nmodiﬁed nltk tokenize output to use it for Bengali\nkeeping in mind the difference between punctua-\ntion of English and Bengali. Under model-based\ntokenization BNLP provides sentencepice4 tok-\nenizer for Bengali text called Bengali Sentence-\npiece. Bengali sentencepiece API provides two\noptions, the pretrained sentencepiece model and\nthe training sentencepiece model. Anyone can tok-\nenize Bengali text using a pretrained sentencepiece\nmodel or can train their own Bengali sentencepiece\nmodel by calling train API. Figure 1 shows an ex-\nample of BNLP basic tokenizer.\n3.2\nEmbedding\nBNLP provides two different embedding option to\nembed Bengali words, one is word2vec (Mikolov\net al., 2013) and another is fasttext (Bojanowski\net al., 2016). Both Bengali word2vec and fasttext\nhas two option, one is embed Bengali word using\npre-trained model and another is training Bengali\nword2vec or fasttext model from scratch. For both\n3https://github.com/nltk/nltk\n4https://github.com/google/\nsentencepiece\nCorpus\nArticles\nSentences\nTokens\nWikipedia\n99139\n1818523\n32908419\nNews Articles\n127867\n4017940\n60526710\nTotal\n227006\n5836463\n93435129\nTable 2: Statistics of Datasets used for training sentencepiece, word2vec, fasttext Models\nSentences\nTrain\nTest\nPOS\n2997\n2247\n750\nNER\n67719\n64155\n3564\nTable 3: Statistics of POS and NER datasets\nTask\nPrecision\nRecall\nF1\nPOS\n81.74\n79.78\n80.75\nNER\n74.15\n60.91\n66.88\nTable 4: Evaluation results\nembedding model, we used gensim5 embedding\nAPI and trained with Bengali corpora. Figure 2\nshows an example of generating word vector using\npre-trained model and BNLP.\n3.3\nPOS Tagging\nBNLP provides a hands-on starting option for POS\nto Bengali by giving a method to tag part of speech\nfrom a given sentence using pre-trained CRF based\n(McCallum and Li, 2003) model. BNLP also pro-\nvides an option to train a CRF-based POS model\nwith custom POS datasets.\n3.4\nNER Tagging\nBNLP provides a hands-on starting option for NER\nto Bengali by giving a method to tag name entity\nfrom a given sentence using a pre-trained CRF-\nbased NER model. BNLP also provides an option\nto train a CRF-based NER model with custom NER\ndatasets. Figure 3 shows an example of name entity\ntagging using BNLP.\n5https://github.com/RaRe-Technologies/\ngensim\nFigure 3: An example of doing NER using BNLP\nApart from this BNLP provides some extra util-\nities methods like getting Bengali stopwords, let-\nters, punctuation from Corpus class.\n4\nPre-trained Models\nBNLP provides different pre-trained Bengali model\nincluding (i) sentencepiece (ii) word2vec (iii) fast-\ntext (iv) CRF-based POS tagging (v) CRF-based\nNER tagging model.\nSentencepiece: For training different language\nmodels we need subword level better vocabulary.\nWe build subword-based vocabulary by training\nsentencepice model on Bengali Wikipedia and\nnews articles datasets. We trained sentencepiece\nunigram language model (Kudo, 2018) with vocab\nsize of 50000.\nWord2Vec:\nWe trained Bengali word2vec\nmodel on Bengali Wikipedia and news articles\ndatasets using gensim word2vec pipeline.\nWe\ntrained our word2vec model with embedding di-\nmension 300, window size 5, the minimum number\nof word occurrences 1, and total workers number 8.\nWe train it for a total of 50000 iterations.\nFasttext: We trained Bengali Fasttext model on\nBengali Wikipedia and news articles datasets. For\ntraining fasttext we set embedding dimension 300,\nwindows size 5, number of minimum word occur-\nrences 1, model type skip-gram, learning rate 0.05.\nWe trained a total of 50 epochs and our loss is\n0.318668.\nCRF-Based POS Tagging Model: We trained\nour CRF-Based POS tagging model on nltr 6\ndatasets. We split data into 75% train and 25%\ntest. Our evaluation result for the POS tagging\nmodel is 80.75 F1 score.\nCRF-Based NER Model:\nWe trained our\nCRF-Based NER model on NER-Bengali-Datasets\n(Karim et al., 2019). We split data into 75% train\nand 25% test. Our evaluation result for the NER\nmodel is 68.88 F1 score.\nTable 3 provides detailed evaluation results of\nPOS tagging and NER model.\n6https://github.com/abhishekgupta92/\nbangla_pos_tagger\n4.1\nDatasets\nFor training sentencepiece, word2vec, fasttext we\nused Bengali raw text data from two sources. One\nis wikipedia7 dump dataset and another is crawl\nnews articles from different news portal sites. As\nshown in Table 2 our raw data contains a total\nof 99139 Wikipedia Bengali articles and 127867\nnews articles. Wikipedia corpus contains a total of\n1818523 sentences with 32908419 tokens. News ar-\nticles corpus contains a total of 4017940 sentences\nwith 60526710 tokens.\nFor POS tagging we used nltr datasets which\ncontains total of 2997 sentences. We split those\ndatasets into 2247 train and 750 test sets and train\nour POS tagging model. For NER we used NER-\nBengali-Datasets (Karim et al., 2019) which con-\ntains a total of 67719 data with 64155 train and\n3564 test. Table 3 provides details statistics of POS\nand NER datasets.\n5\nConclusion and Future Work\nBNLP language processing toolkit provides tok-\nenization, embedding, POS, NER facilities for Ben-\ngali. BNLP pre-trained model achieves signiﬁcant\nresults in Bengali text tokenizing, word embedding,\nPOS, and NER task. BNLP is being used widely\nby Bengali research communities and appreciated\nby the communities.\nWe are working on extending the support tools\nlike stemming, lemmatizing, corpus support for\nBNLP in the future. We are working on adding\nlanguage model-based support in BNLP so that re-\nsearchers can use it for different downstream tasks\nefﬁciently. While these tasks under development,\nwe are hoping that BNLP will accelerate Bengali\nNLP research and development.\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland Vollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-\nart NLP. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54–59, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nGaurav Arora. 2020. iNLTK: Natural language toolkit\nfor indic languages. In Proceedings of Second Work-\nshop for NLP Open Source Software (NLP-OSS),\n7https://dumps.wikimedia.org/bnwiki/\nlatest/\npages 66–71, Online. Association for Computational\nLinguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomás Mikolov. 2016. Enriching word vectors with\nsubword information. CoRR, abs/1607.04606.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew E.\nPeters, Michael Schmitz, and Luke Zettlemoyer.\n2018. Allennlp: A deep semantic natural language\nprocessing platform. CoRR, abs/1803.07640.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nRedwanul Karim, M. A. Islam, Sazid Simanto, Saif\nChowdhury, Kalyan Roy, Adnan Neon, Md Hasan,\nAdnan Firoze, and Mohammad Rahman. 2019. A\nstep towards information extraction: Named entity\nrecognition in bangla using deep learning. Journal\nof Intelligent and Fuzzy Systems, 37:1–13.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. CoRR, abs/1804.10959.\nEdward Loper and Steven Bird. 2002. Nltk: the natural\nlanguage toolkit. CoRR, cs.CL/0205028.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit.\nIn Proceedings of 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations, pages 55–60, Bal-\ntimore, Maryland. Association for Computational\nLinguistics.\nAndrew McCallum and Wei Li. 2003. Early results for\nnamed entity recognition with conditional random\nﬁelds, feature induction and web-enhanced lexicons.\nIn Proceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n188–191.\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space.\nIn 1st International Con-\nference on Learning Representations, ICLR 2013,\nScottsdale, Arizona, USA, May 2-4, 2013, Workshop\nTrack Proceedings.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D Manning. 2020.\nStanza:\nA\npython\nnatural\nlanguage\nprocessing\ntoolkit\nfor many human languages.\narXiv preprint\narXiv:2003.07082.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-01-31",
  "updated": "2021-12-01"
}