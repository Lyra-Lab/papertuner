{
  "id": "http://arxiv.org/abs/2102.02270v2",
  "title": "Confusion2vec 2.0: Enriching Ambiguous Spoken Language Representations with Subwords",
  "authors": [
    "Prashanth Gurunath Shivakumar",
    "Panayiotis Georgiou",
    "Shrikanth Narayanan"
  ],
  "abstract": "Word vector representations enable machines to encode human language for\nspoken language understanding and processing. Confusion2vec, motivated from\nhuman speech production and perception, is a word vector representation which\nencodes ambiguities present in human spoken language in addition to semantics\nand syntactic information. Confusion2vec provides a robust spoken language\nrepresentation by considering inherent human language ambiguities. In this\npaper, we propose a novel word vector space estimation by unsupervised learning\non lattices output by an automatic speech recognition (ASR) system. We encode\neach word in confusion2vec vector space by its constituent subword character\nn-grams. We show the subword encoding helps better represent the acoustic\nperceptual ambiguities in human spoken language via information modeled on\nlattice structured ASR output. The usefulness of the proposed Confusion2vec\nrepresentation is evaluated using semantic, syntactic and acoustic analogy and\nword similarity tasks. We also show the benefits of subword modeling for\nacoustic ambiguity representation on the task of spoken language intent\ndetection. The results significantly outperform existing word vector\nrepresentations when evaluated on erroneous ASR outputs. We demonstrate that\nConfusion2vec subword modeling eliminates the need for retraining/adapting the\nnatural language understanding models on ASR transcripts.",
  "text": "CONFUSION2VEC 2.0: ENRICHING AMBIGUOUS SPOKEN LANGUAGE\nREPRESENTATIONS WITH SUBWORDS\nPrashanth Gurunath Shivakumar, Panayiotis Georgiou, Shrikanth Narayanan\nUniversity of Southern California, Los Angeles, California, USA\nABSTRACT\nWord vector representations enable machines to encode hu-\nman language for spoken language understanding and pro-\ncessing. Confusion2vec, motivated from human speech pro-\nduction and perception, is a word vector representation which\nencodes ambiguities present in human spoken language in ad-\ndition to semantics and syntactic information. Confusion2vec\nprovides a robust spoken language representation by consid-\nering inherent human language ambiguities. In this paper,\nwe propose a novel word vector space estimation by unsu-\npervised learning on lattices output by an automatic speech\nrecognition (ASR) system. We encode each word in confu-\nsion2vec vector space by its constituent subword character\nn-grams. We show the subword encoding helps better rep-\nresent the acoustic perceptual ambiguities in human spoken\nlanguage via information modeled on lattice structured ASR\noutput. The usefulness of the proposed Confusion2vec repre-\nsentation is evaluated using semantic, syntactic and acoustic\nanalogy and word similarity tasks. We also show the beneﬁts\nof subword modeling for acoustic ambiguity representation\non the task of spoken language intent detection. The results\nsigniﬁcantly outperform existing word vector representations\nwhen evaluated on erroneous ASR outputs. We demonstrate\nthat Confusion2vec subword modeling eliminates the need for\nretraining/adapting the natural language understanding mod-\nels on ASR transcripts.\nIndex Terms— Confusion2Vec, Subword, Word Vector\nRepresentation, Word Embedding, Spoken Language Under-\nstanding\n1. INTRODUCTION\nSpeech is the primary and most natural mode of communi-\ncation for humans. This makes its use also attractive for hu-\nman computer interaction, which in turn requires decoding\nhuman language to enable spoken language understanding.\nHuman language is a complex construct involving multiple\ndimensions of information including semantics, syntax and\noften contain ambiguities which make it challenging for ma-\nchine inference of communication intent, emotions etc. Sev-\neral word vector representations have been proposed for ef-\nemail:\npgurunat@usc.edu\nfectively describing the human language in the natural lan-\nguage processing community.\nContextual modeling techniques like language modeling,\ni.e., predicting the next word in the sentence given a window\nof preceding context, have been shown to model meaningful\nword representations [2, 18]. Bag-of-word based contextual\nmodeling, where the current word is predicted given both its\nleft and right (local) contexts has shown to capture language\nsemantics and syntax [19]. Similarly, predicting local context\nfrom the current word, referred to as skip-gram modeling, is\nshown to better represent semantic and syntactic distances\nbetween words [20]. In [21] log bi-linear models combin-\ning global word co-occurrence information and local context\ninformation, termed as global vectors (GloVe), is shown to\nproduce meaningful structured vector space. Bi-directional\nlanguage models are proposed in [22], where internal states\nof deep neural networks are combined to model complex\ncharacteristics of word use and its variance over linguistic\ncontexts. The advantages of bi-directional modeling are fur-\nther exploited along with self-attention using transformer\nnetworks [31] to estimate a representation, termed as BERT\n(Bidirectional Encoder Representations from Transformers),\nthat has shown its utility on a multitude of natural language\nunderstanding tasks [6]. Models such as BERT, ELMo esti-\nmate word representations that vary depending on the context,\nwhereas the context-free representations including GloVe and\nWord2Vec generate a single representation irrespective of the\ncontext.\nHowever, most of the word vector representations infer\nthe knowledge through contextual modeling and many of the\ninherent ambiguities present in human language are often un-\nrecognized or ignored.\nFor instance, from the perspective\nof spoken language, the ambiguities can be associated with\nhow similar the words sound, i.e., for example, the words\n“see” and “sea” sound acoustically identical but have differ-\nent meanings. The ambiguities can also be associated with the\nunderlying speech signal itself due to wide range of acoustic\nenvironments involving noise, overlapped speech and chan-\nnel, room characteristics.\nThese ambiguities often project\nthemselves as errors through ASR systems. Most of the ex-\nisting word vector representations such as word2vec [20, 19],\nfasttext [3], GloVe [21], BERT [6], ELMo [22] do not account\nfor the ambiguities present in speech signals and thus degrade\narXiv:2102.02270v2  [cs.CL]  19 Feb 2021\nwhile processing the output of noisy ASR transcripts.\nConfusion2vec was recently proposed to handle repre-\nsentation ambiguity information present in human language\n[26]. Confusion2vec is estimated by unsupervised skip-gram\ntraining on the ASR output lattices and confusion networks.\nThe analysis of inherent acoustic ambiguity information of\nthe embeddings displayed meaningful interactions between\nthe semantic-syntactic subspace and acoustic similarity sub-\nspaces.\nIn [27], the usefulness of the Confusion2vec was\nconﬁrmed on the task of spoken language intent detection.\nThe Confusion2vec representation signiﬁcantly outperformed\ntypical word embeddings including word2vec and GloVe\nwhen evaluated on noisy ASR transcripts by reducing the\nclassiﬁcation error rate by approximately 20% relative.\nAlthough, there have been few attempts in leveraging\ninformation present in word lattices and word confusion net-\nworks for several tasks [29, 14, 30, 33, 28, 13], the main\ndownside with these works is that the word representation\nestimated by such techniques are task dependent and are\nrestricted to a particular domain and dataset.\nMoreover,\navailability of most of the task speciﬁc datasets are limited\nand task speciﬁc speech data are expensive to collect. The\nadvantage of Confusion2Vec is that it estimates a generic,\ntask-independent word vector representation via unsuper-\nvised learning on lattices or confusion networks generated by\nan ASR on any speech conversations.\nIn this paper, we incorporate subwords to represent each\nword for modeling both the acoustic ambiguity information\nand the contextual information. Each word is modeled as a\nsum of constituent n-gram characters. Our motivation behind\nthe use of subwords are the following: (i) they incorporates\nmorphological information of the words by encoding internal\nstructure of words [3], (ii) the bag of character n-grams often\nhave a high overlap between acoustically ambiguous words,\n(iii) subwords help model under-represented words more ef-\nﬁciently, thereby leading to more robust estimation with lim-\nited available data, which is the case since training Confu-\nsion2Vec is restricted to ASR lattice outputs, (iv) subwords\nenable representations for out-of-vocabulary words which are\ncommon-place with end-to-end ASR systems outputting char-\nacters.\nThe rest of the paper is organized as follows: Confu-\nsion2vec is introduced in Section 2. The proposed subword\nmodeling is presented in Section 3. Section 4 gives details\nof the evaluation techniques employed for assessing the word\nembedding models. The experimental setup and results of\nvarious analogy and similarity tasks are presented in sec-\ntion 5. Section 6 presents the application of the proposed\nword vector representation to the spoken language intent\ndetection task. Finally, the paper is concluded in section 7.\n2. CONFUSION2VEC\nIn psycho-acoustics, it is established that humans also re-\nlate words with how they sound [1] in addition to semantics\nand syntax. Inspired by principles of human speech produc-\ntion and perception, we previously proposed Confusion2vec\n[26]. The core idea is to estimate a hyper-space that not only\ncaptures the semantics and syntax of human language, but\nalso augments the vector space with acoustic ambiguity in-\nformation, i.e., word acoustic similarity information. In other\nwords, word2vec, GloVe can be viewed as a subspace of the\nconfusion2vec vector space.\nSeveral different methodologies are proposed for captur-\ning the ambiguity information.\nThe methodologies are an\nadaptation of the skip-gram modeling for word confusion net-\nworks or lattice-like structures. The word lattices are directed\nacyclic weighted graphs of all the word sequences that are\nlikely possible. A confusion network is a speciﬁc type of lat-\ntice with constraints that each word sequence passes through\neach node of graph. Such lattice-like structures can be de-\nrived from machine learning algorithms that output probabil-\nity measures, for example, an ASR. Figure 1, illustrates a con-\nfusion network that can possibly result from a speech recogni-\ntion system. Unlike typical simple sentences which are used\nfor training word embeddings like word2vec, GloVe, BERT,\nELMo etc., the information in the confusion network can be\nviewed along two dimensions: (i) contextual dimension, and\n(ii) acoustic ambiguity dimension.\nMore speciﬁcally, four different conﬁgurations of skip-\ngram modeling algorithms are proposed in our previous work\n[26], namely:\n(i) top-confusion, (ii) intra-confusion, (iii)\ninter-confusion, and (iv) hybrid model. The top-confusion\nversion considers only the most-probable path of the ASR\nconfusion network and applies the typical skip-gram model\non it. The intra-confusion version applies the skip-gram mod-\neling on the acoustic ambiguity dimension of the confusion\nnetwork and ignores the contextual information, i.e., each\nambiguous word alternative is predicted by the other over a\npre-deﬁned local context. The inter-confusion version applies\nthe skip-gram modeling on the contextual dimension but over\neach of the acoustic ambiguous words. The hybrid model is\na combination of both the intra and inter-confusion conﬁg-\nurations. More information on the training conﬁguration is\navailable in [26]. The present work builds upon this basic\nConfusion2vec framework.\n3. CONFUSION2VEC 2.0 SUBWORD MODEL\nSubword encoding of words has been popular in modeling\nsemantics and syntax of language using word vector rep-\nresentations [3, 6, 22].\nThe use of subwords are mainly\nmotivated by the fact that the subwords incorporate morpho-\nlogical information which can be helpful, for example, in\nrelating the preﬁxes, sufﬁxes and the word root. In this work,\nwt-1,1\nwt-1,2\nwt,2\nwt,1\nwt,3\nwt,4\nwt+1,2\nwt+1,1\nwt+1,3\nwt+2,2\nwt+2,1\nwt+2,3\nwt+2,4\nFig. 1: Example Confusion Network Output by ASR for the ground-truth phrase “I want to sit”\nwe apply subword representation for encoding the word am-\nbiguity information in the human language. We believe we\nhave a compelling case for the use of subwords for repre-\nsenting the acoustic similarities (ambiguities) between the\nwords in the language since more similarly sounding words\noften have highly overlapping subword representations. This\nhelps model the level of overlap and estimate the magnitude\nof acoustic similarity robustly. Moreover, use of subwords\nshould help in efﬁcient encoding of under-represented words\nin the language. This is crucial in the case of Confusion2vec\nbecause we are restricted to speech data and their corre-\nsponding decoded ASR lattices for training, thereby limiting\nword-word co-occurrence in contrast to typical word vec-\ntor representation which can be trained on large amounts of\neasily available plain text data. Another important aspect is\nthe ability to represent out-of-vocabulary words which are\na common place occurrence with end-to-end ASR systems\noutputting character sequences.\nIn the proposed model, each word w is represented as a\nsum of its constituent n-gram character subwords. This en-\nables the model to infer the internal structure of each word.\nFor example, a word “want“ is represented with the vector\nsum of the following subwords:\n<wa, wan, ant, nt>, <wan, want, ant>,\n<want, want>, <want>\nSymbols < and > are used to represent the beginning and end\nof the word. The n-grams are generated for n=3 upto n=6. It\nis apparent that an acoustically ambiguous, similar sounding\nword “wand” has a high degree of overlap with the set of\nn-gram characters.\nIn this paper, we consider two modeling variations: (i)\ninter-confusion, and (ii) intra-confusion versions of confu-\nsion2vec with the subword encoding.\n3.1. Intra-Confusion Model\nThe goal of the intra-confusion model is to estimate the inter-\nword relations between the acoustically ambiguous words\nthat appear in the ASR lattices. For this, we perform skip-\ngram modeling over the acoustic similarity dimension (see\nFigure 1) and ignore the contextual dimension of the ut-\nterance.\nThe objective of the intra-confusion model is to\nmaximize the following log-likelihood:\nT\nX\nt=1\nX\nˆa∈ˆ\nAt\nX\na∈At\nlog p(wt,a|wt,ˆa)\n(1)\nwhere T is the length of the utterance (confusion network) in\nterms of number of words, wi,j is the word in the confusion\nnetwork output by the ASR at time-step i and j is the index of\nthe word among the ambiguous alternatives. ˆ\nAt is the set of\nindices of all ambiguous words at time-step t, ˆa is the index\nof the current word along the acoustic ambiguity dimension,\nAt ⊆ˆ\nAt −ˆa is the subset of ambiguous words barring ˆa at\nthe current word t, i.e., for example from Figure 1, for the\ncurrent word, wt,ˆa, “want”, At ⊆{wand, won’t, what}.\nAdditionally, for subword encoding, each word input is rep-\nresented as:\nwi,j =\nX\ns∈Sw\nxs\n(2)\nwhere Sw is the set of all character n-grams ranging from n=3\nto n=6 and the word itself and xs is the vector representation\nfor n-gram subword s. Few training samples (input, target)\ngenerated for this conﬁguration pertaining to input confusion\nnetwork in Figure 1 are (I, eye), (eye, I), (want,\nwand), (want, won’t), (won’t, what), (wand,\nwhat) etc.\n3.2. Inter-Confusion Model\nThe aim of the inter-confusion model is to jointly model the\ncontextual co-occurrence information and the acoustic ambi-\nguity co-occurrence information along both the axis depicted\nin the confusion network. Here, the skip-gram modeling is\nperformed over time context and over all the possible acous-\ntic ambiguities. The objective of the inter-confusion model is\nto maximize the following log-likelihood:\nT\nX\nt=1\nX\nˆa∈ˆ\nAt\nX\nc∈Ct\nX\na∈Ac\nlog p(wc,a|wt,ˆa)\n(3)\nwhere Ct corresponds to set of indices of nodes of confu-\nsion network, i.e., words around the current word t along the\ntime-axis and c is the current context index. Ac is the set of\nindices of acoustically ambiguous words at a context c. For\nexample, for the current word, wt,ˆa, “want” in Figure 1,\nAc ⊆{I, eye, two, tees, to, seat, sit, seed,\neat} and At ⊆{wand, won’t, what, want}.\nNote,\neach word input is subword encoded as in equation 2. Few\ntraining samples (input, target) generated for this conﬁgu-\nration are\n(want, I), (want, eye), (want, two),\n(want, to), (want, tees), (what, I), (what,\neye), (what, to), (what, tees), (what, two),\n(won’t, eye) etc.\n3.3. Training Loss and Objective\nNegative sampling is employed for training the embedding\nmodel. Negative sampling was ﬁrst introduced for training\nword2vec representation [20]. It is a simpliﬁcation of the\nNoise Contrastive Estimation objective [9].\nThe negative\nsampling for training the embedding can be posed as a set of\nbinary classiﬁcation problems which operates on two classes:\npresence of signal or absence (noise). In the context of word\nembeddings the presence of the context words are treated\nas positive class and the negative class is randomly sampled\nfrom the unigram distribution of the vocabulary. The negative\nsampling for subword model can be expressed using binary\nlogistic loss as:\nlog σ(\nX\ns∈Swi\nxT\ns owt) +\nK\nX\nk=1\nEwk∼Pn(w)log σ(−\nX\ns∈Swi\nxT\ns owk)\n(4)\nwhere σ(x) =\n1\n1+e−x , wi is the input word, wt is the out-\nput word, Swi is the set of n-gram character subwords for the\nword wi, xs is the vector representation for the character n-\ngram subword s and owt is the output vector representation\nof target word wt. K is the number of negative samples to\nbe drawn from the negative sample, noise distribution Pn(w).\nThe noise distribution Pn(w) is chosen to be the unigram dis-\ntribution of words in the vocabulary raised to the 3/4th power\nas suggested in [20]. Note, for confusion2vec the input word\nwi and target word wt are derived according to equations 1\nand 3 for implementing the respective training conﬁgurations\n4. EVALUATIONS\nWe perform evaluations of the proposed word embeddings\nalong two aspects. One, in view of the assessing the useful,\nmeaningful information embedded in the word vector repre-\nsentation. Second, in its application to a realistic task of spo-\nken language intent detection.\n4.1. Analogy and Similarity Tasks\nFor evaluating the inherent semantic and syntactic knowledge\nof the word embeddings, we employ two tasks: (i) semantic-\nsyntactic analogy task, and (ii) word similarity task. The word\nanalogy task was ﬁrst proposed in [19] which comprises word\npair analogy questions of the form W1 is to W2 as W3 is to\nW4. The analogy is answered correct if vec(W1)−vec(W2)+\nvec(W3) is most similar to vec(W4). Another prominent ap-\nproach is the word similarity task, where rank-correlation be-\ntween cosine similarity of set of pair of word vectors and hu-\nman annotated word similarity scores are assessed [24]. For\nword similarity task, we use the WordSim-353 database [7]\nconsisting of 353 pairs of words annotated over a score of 1\nto 10 depending on the magnitude of word similarity as per-\nceived by humans.\nFor assessing the word acoustic ambiguity (similarity)\ninformation, we conduct the acoustic analogy task, Seman-\ntic&syntactic–acoustic analogy task and Acoustic similarity\ntasks proposed in [26]. The Acoustic analogy task comprises\nof word pair analogies compiled using homophones which\nanswer questions of the form: W1 sounds similar to W2 as W3\nsounds similar to W4. The acoustic analogy task is designed\nto assess the ambiguity information embedded in the word\nvector space [26]. The semantic&syntactic-acoustic analogy\ntask is designed to assess semantic, syntactic and acoustic\nambiguity information simultaneously.\nThe analogies are\nformed by replacing certain words by their homophone al-\nternatives in the original semantic and syntactic analogy task\n[26]. The acoustic word similarity task is analogous to the\nword similarity task, i.e., it contains of word pairs which are\nrated on their acoustic similarity based on the normalized\nphone edit distances.\nA value of 1.0 refers to two words\nsounding identical and 0.0 refers to the word pairs being\nacoustically dissimilar. More details regarding the evaluation\nmethodologies are available in [26]. The evaluation datasets\nare made available1.\n4.2. Spoken Language Intent Classiﬁcation\nWe also evaluate the efﬁcacy of the proposed word represen-\ntation models on the task of spoken language intent classi-\nﬁcation. A recurrent neural network (RNN) based classiﬁer\nis employed by initializing the embedding layer with the pro-\nposed word vectors. Classiﬁcation experiments are conducted\nby training the recurrent neural network on (i) clean manual\ntranscripts, and (ii) noisy ASR transcripts, with evaluations on\nboth manual and ASR transcripts. Classiﬁcation error rates of\n1https://github.com/pgurunath/confusion2vec_2.0\nModel\nAnalogy Tasks\nSimilarity Tasks\nS&S\nAcoustic\nS&S-Acoustic\nAverage Accuracy\nWord Similarity\nAcoustic Similarity\nGoogle W2V [20]\n61.42%\n0.9%\n16.99%\n26.44%\n0.6893\n-0.3489\nIn-domain W2V\n59.17%\n0.6%\n8.15%\n22.64%\n0.4417\n-0.4377\nfastText [3]\n75.93%\n0.46%\n17.40%\n31.26%\n0.7361\n-0.3659\nConfusion2Vec 1.0\n(word) [26]\nC2V-a\n63.97%\n16.92%\n43.34%\n41.41%\n0.5228\n0.6200\nC2V-c\n65.45%\n27.33%\n38.29%\n43.69%\n0.5798\n0.5825\nConfusion2Vec 2.0\nC2V-a\n56.74%\n50.79%\n44.67%\n50.73%\n0.3181\n0.8108\n(subword)\nC2V-c\n56.87%\n51.00%\n44.98%\n50.95%\n0.2893\n0.8106\nTable 1: Results: Different proposed models\nC2V-a: Intra-Confusion, C2V-c: Inter-Confusion, S&S: Semantic & Syntactic Analogy.\nFor the analogy tasks: the accuracies of baseline word2vec models are for top-1 evaluations, whereas of the other models are for top-2\nevaluations (as discussed in [26]). For the similarity tasks: all the correlations (Spearman’s) are statistically signiﬁcant with p < 0.001.\nthe intent detection is used to derive assessments of the word\nvector representations.\n5. ANALOGY & SIMILARITY TASKS\n5.1. Database\nThe Fisher English Training Part 1, Speech (LDC2004S13)\nand Fisher English Training Part 2, Speech (LDC2005S13)\ncorpora [5] are used for both training the ASR and the confu-\nsion2vec 2.0 embeddings. The choice of database is based on\n[26] for direct comparison purposes. The corpus consists of\nspontaneous telephonic conversations between 11,972 native\nEnglish speakers. The speech data amounts to approximately\n1,915 hours sampled at 8 kHz. The corpus is divided into 3\nparts for training (1,905 hours, 1,871,731 utterances), devel-\nopment (5 hours, 5000 utterances) and test (5 hours, 5000 ut-\nterances). Overall, the transcripts contain approximately 20.8\nmillion word tokens and vocabulary size of 42,150.\n5.2. Experimental Setup\nThe experimental setup is maintained identical to [26] for di-\nrect comparison. Brief detail of the setup is as follows:\n5.2.1. Automatic speech recognition\nA hybrid HMM-DNN based acoustic model is trained on the\ntrain subset of the speech corpus using the KALDI speech\nrecognition toolkit [23]. 40 dimensional mel frequency cep-\nstral coefﬁcients (MFCC) features are extracted along with\nthe i-vector features for training the acoustic model.\nThe\ni-vector features are used to provide speaker and channel\ncharacteristics to aid acoustic modeling. The DNN acoustic\nmodel, comprises 7 layers with P-norm non-linearity (p=2)\neach with 350 units [35]. The DNN is trained using 5 MFCC\nframe splices with left and right context of 2 to classify\namong 7979 Gaussian mixtures with stochastic gradient de-\nscent optimizer. The CMU pronunciation dictionary [32] is\nused as the word-pronunciation transcription lexicon. A tri-\ngram language model is trained on the training subset of the\nFisher English Speech Corpus. The ASR yields word error\nrates (WER) of 16.57% and 18.12% on the development and\nthe test datasets. Lattices are derived during the ASR decod-\ning with a decoding beam size of 11 and lattice beam size\nof 6. The lattices are converted to confusion networks with\nthe minimum Bayes risk criterion [34] for training the confu-\nsion2vec embeddings. The resulting confusion networks have\na vocabulary size of 41,274 and 69.5 million words, with an\naverage of 3.34 alternative (ambiguous) words for each edge\nin the graph.\n5.2.2. Confusion2Vec 2.0\nIn order to train the embedding, most frequent words are\nsub-sampled as suggested in [20], with the rejection thresh-\nold set to 10−4. Also, a minimum frequency threshold of\n5 is set and the rarely occurring words are pruned from the\nvocabulary. The context window size for both the acoustic\nambiguity and contextual dimensions are uniformly sam-\npled between 1 and 5.\nThe dimension of the word vec-\ntors are set to 300.\nThe number of negative samples for\nnegative sampling is chosen to be 64.\nThe learning rate\nis set to 0.01 and trained for a total of 15 epochs using\nstochastic gradient descent.\nAll the hyper-parameters are\nempirically chosen for optimal performance on the devel-\nopment set.\nWe implemented the confusion2vec 2.0 by\nmodifying the source code from fastText2 [3].\nWe make\nour source code and trained models available at https:\n//github.com/pgurunath/confusion2vec_2.0.\n2https://github.com/facebookresearch/fastText\nModel\nAnalogy Tasks\nSimilarity Tasks\nS&S\nAcoustic\nS&S-Acoustic\nAverage Accuracy\nWord Similarity\nAcoustic Similarity\nGoogle W2V [20]\n61.42%\n0.9%\n16.99%\n26.44%\n0.6893\n-0.3489\nIn-domain W2V\n59.17%\n0.6%\n8.15%\n22.64%\n0.4417\n-0.4377\nfastText [3]\n75.93%\n0.46%\n17.40%\n31.26%\n0.7361\n-0.3659\nConfusion2Vec 1.0\n(word) [26]\nC2V-1 + C2V-a\n67.03%\n25.43%\n40.36%\n44.27%\n0.5102\n0.7231\nC2V-1 + C2V-c\n70.84%\n35.25%\n35.18%\n47.09%\n0.5609\n0.6345\nC2V-1 + C2V-c (JT)\n65.88%\n49.4%\n41.51%\n52.26%\n0.5379\n0.7717\nConfusion2Vec 2.0\nfastText + C2V-a\n76.10%\n22.67%\n49.15%\n49.31%\n0.5744\n0.7577\n(subword)\nfastText + C2V-c\n76.16%\n22.56%\n49.12%\n49.12%\n0.5732\n0.7573\nTable 2: Results: Different proposed models\nC2V-a: Intra-Confusion, C2V-c: Inter-Confusion, S&S: Semantic & Syntactic Analogy.\nFor the analogy tasks: the accuracies of baseline word2vec models are for top-1 evaluations, whereas of the other models are for top-2\nevaluations (as discussed in [26]). For the similarity tasks: all the correlations (Spearman’s) are statistically signiﬁcant with p < 0.001.\n5.3. Results\nTable 1 lists the results in terms of accuracies for analogy\ntasks and rank-correlations for similarity tasks.\nThe ﬁrst\ntwo rows correspond to results with the original word2vec.\nGoogle W2V model is the open source model released by\nGoogle3, trained on 100 billion word Google News dataset.\nWe also train an in-domain version of original word2vec\non the Fisher English corpus for fair comparison with the\nconfusion2vec models, referred to as “In-domain W2V” in\nTable 1. The fastText model employed is the open source\nmodel trained on Wikipedia dumps with a vocabulary size\nof more than 2.5 million words released by Facebook4. The\nmiddle two rows of the table correspond to confusion2vec\nembeddings without subword encoding and they are taken\ndirectly from [26]. The bottom two rows correspond to the\nresults obtained with subword encoding. Note, the confu-\nsion2vec 1.0 is initialized on the Google word2vec model\nfor better convergence. The confusion2vec 2.0 model is ini-\ntialized on the fastText model to maintain compatibility with\nsubword encodings. We normalize the vocabulary for all the\nexperiments, meaning the same vocabulary is used to evaluate\nthe analogy and similarity tasks to allow for fair comparisons.\nComparing the baseline word2vec and fastText embed-\ndings to the confusion2vec, we observe the baseline embed-\ndings perform well on the semantic&syntactic analogy task\nand provide good positive correlation on the word similarity\ntask as expected. However, they perform poorly on the acous-\ntic analogy task, semantic&syntactic-acoustic analogy task\nand give small negative correlation on the acoustic analogy\ntask. All the confusion2vec models perform relatively well\non the semantic&syntactic analogy task and word similarity\ntask, but more importantly, yield high accuracies on acoustic\nanalogy task and semantic&syntactic-acoustic analogy tasks\n3https://code.google.com/archive/p/word2vec/\n4https://fasttext.cc/docs/en/pretrained-vectors.html\nand provide high positive correlation with the acoustic simi-\nlarity task.\nSpeciﬁcally with Confusion2Vec 2.0, among the analogy\ntasks, we observe the subword encoding enhances the acous-\ntic ambiguity modeling. For the acoustic analogy task we ﬁnd\nrelative improvement of upto 46.41% over its non-subword\ncounterpart.\nMoreover, even for the semantic&syntactic-\nacoustic analogy task, we observe improvements with sub-\nword encoding. However, we ﬁnd a small reduction in perfor-\nmance for the original semantic and syntactic analogy task.\nRegardless of the small dip in the performance, the accuracies\nremain acceptable in comparison to the in-domain word2vec\nmodel. Overall, taking the average accuracy of all the anal-\nogy tasks, we obtain an increase of approximately 16.62%\nrelative over the non-subword confusion2vec models.\nInvestigating the results for the similarity tasks, we ﬁnd\nsigniﬁcant and high correlation of 0.81 for acoustic similarity\ntask with the subword encoding. Again, a small degradation\nis observed with the word similarity task obtaining a corre-\nlation of 0.3181 against the 0.4417 of the in-domain base-\nline word2vec model. Overall, the results of the analogy and\nthe similarity tasks suggest the subword encoding greatly en-\nhances the ambiguity modeling of confusion2vec.\n5.4. Model Concatenation\nFurther, the confusion2vec model can be concatenated with\nthe other word embedding models to produce a new word\nvector space that can result in better representations as seen\nin [26]. Table 2 lists the results of the concatenated models.\nFor the previous, non-subword version of the confusion2vec,\nthe vector models are concatenated with the word2vec model\ntrained on the ASR output transcripts (C2V-1). The choice of\nusing the C2V-1 instead of the Google W2V for concatenation\nwas based on empirical ﬁndings. Where as to maintain com-\npatibility of subword encoding, the confusion2vec 2.0 models\n1\n0\n1\n2\n3\n2\n1\n0\n1\n2\nboy\ngirl\nprince\nprints\nprincess\nuncle\naunt\nant\npossible\nimpossible\nfortunate\nunfortunate\nwrite\nwright\nred\nread\nreading\nsee\nsea\nseeing\nprinz\n(a) fastText\n2\n1\n0\n1\n2\n3\n4\n5\n2\n1\n0\n1\n2\n3\n4\n5\nboy\ngirl\nprince\nprints\nprincess\nuncle\nant\npossible\nimpossible\nfortunate\nunfortunate\naunt\nwrite\nwright\nred\nread\nreading\nsee\nsea\nseeing\nprinz\n(b) Confusion2Vec 2.0: C2V-a\nFig. 2: 2-D plots of selected word vectors portraying semantic, syntactic and acoustic relationships after dimension reduction\nusing PCA\nThe blue lines indicate semantic relationships, blue ellipses indicate syntactic relationships, red lines indicate acoustic-semantic/syntactic\nrelations and red ellipses indicate acoustic ambiguity word relations.\nare concatenated with fastText models.\nFirst, comparisons between the non-concatenated ver-\nsions in Table 1 and the concatenated version in Table 2, of\nthe non-subword models, we observe an improvement of ap-\nproximately 7.22% relative in average analogy accuracy after\nconcatenation.\nWe don’t observe signiﬁcant improvement\nwith subword based models after concatenation in terms of\naverage analogy accuracy.\nHowever, we observe different\ndynamics between the acoustic ambiguity and the semantic\nand syntactic subspaces. Concatenation results in improved\nsemantic and syntactic evaluations with the expense of drop\nin accuracies of acoustic analogy task.\nWe also note im-\nprovements (9.27% relative) in semantic&syntactic-acoustic\nanalogy task after concatenation conﬁrming meaningful ex-\nistence of both ambiguity and semantic-syntactic relations.\nMoreover, the word similarity task also yields better correla-\ntion after concatenation.\nNext, comparisons of the confusion2vec 1.0 (non-subword)\nand the subword version, we observe signiﬁcant improve-\nments in the semantic&symantic analogy task (7.51% rela-\ntive) as well as the semantic&syntactic-acoustic analogy tasks\n(21.78% relative). Moreover, the subword models outperform\nthe non-subword version in both of the similarity tasks. The\nsubword models slightly under-perform in the acoustic anal-\nogy task, but more crucially outperform the Google W2V and\nFastText baselines signiﬁcantly.\nFurther, the concatenated models can be ﬁne-tuned and\noptimized to exploit additional gains as found in [26]. The\nrow corresponding to Confusion2Vec 1.0 - C2V + C2V-c (JT)\nis the best result obtained in [26] which involves 2-passes.\nThe Confusion2Vec 2.0 with the subword modeling with a\nsingle pass training gives comparable performance to the 2-\npass approach. Thus we skip the 2-pass approach with the\nsubword model in favor of ease of training and reproducibil-\nity.\n5.5. Embedding Visualization\nFigure 2 illustrates the word vector spaces of fastText embed-\ndings and the proposed C2V-a embeddings after dimension\nreduction using principal component analysis. We observe\nmeaningful interactions between the semantic&syntactic\nsubspace and the acoustic ambiguity subspace. For exam-\nple, in Figure 2b, vectors “boy”-“prince”, “see”-“seeing”,\n“read”-“write”, “uncle”-“aunt” are similar to acoustically\nambiguous vector “boy”-“prints”, “sea”-“seeing”, “read”-\n“write”, “uncle”-“ant” respectively which is not the case\nin Figure 2a with fastText embeddings.\nSuch vector rela-\ntionships can be exploited for downstream spoken language\napplications by providing crucial acoustic ambiguity infor-\nmation to recover from speech recognition errors. Also note,\nthe acoustically ambiguous words such as “prinz”, “prince”,\n“prints” are found clustered together.\nAnother important\nobservation is that the word “prinz”, out-of-vocabulary in\nEnglish, has an orphaned representation under fastText in\nFigure 2a. However, “prinz” ﬁnds a meaningful represen-\ntation on the basis of acoustic signature in the proposed\nConfusion2vec model as seen in Figure 2b, i.e., “prinz” is\nclustered together with acoustically similar words “prince”\n& “prints” and the vector “boy”-“prinz” is similar to vec-\ntor “boy”-“prince”. Occurrence of out-of-vocabulary words\nsuch as “prinz” is common place with end-to-end ASR sys-\ntems that output characters prone to errors.\nNote, out-of-\nvocabulary words such as “prinz” cannot be represented by\ntypical word embeddings such as word2vec, GloVe etc and\nhence sub-optimal for representation with many end-to-end\nASR systems.\n6. SPOKEN LANGUAGE INTENT DETECTION\nIn this section, we apply the proposed word vector embed-\nding to the task of spoken language intent detection. Spo-\nken language intent detection is the process of decoding the\nspeaker’s intent in contexts involving voice commands, call\nrouting and any human computer interactions. Many spoken\nlanguage technologies use an ASR to convert the speech sig-\nnal to text, a process prone to errors such as due to the varying\nspeaker and noise environments. The erroneous ASR outputs\nin turn result in degradation of the downstream intent classiﬁ-\ncation. Few efforts have focused on handling the errors of the\nASR to make the subsequent intent detection process more\nrobust to errors. These efforts often involve training the intent\nclassiﬁcation systems on noisy ASR transcripts. The down-\nsides of training the intent classiﬁers on the ASR is that the\nsystems are limited with the amount of speech data available.\nMoreover, varying speech signal conditions and use of differ-\nent ASR models make such classiﬁers non-optimal and less\npractical. In many scenarios, speech data is not available to\nenable adaptation on ASR transcripts.\nIn our previous work [27], we applied the non-subword\nversion of the Confusion2vec to the task of spoken language\nintent detection. We demonstrated the Confusion2vec is able\nto perform as efﬁciently as the popular word embeddings like\nword2vec and GloVe on clean manual transcripts giving com-\nparable classiﬁcation error rates. More importantly, we were\nable to illustrate the robustness of the confusion2vec em-\nbeddings when evaluated on the noisy ASR transcripts. The\nconfusion2vec gives signiﬁcantly better accuracies (upto rel-\native 20% improvements) when evaluated on ASR transcripts\ncompared to the word2vec, GloVe embeddings and state-of-\nthe-art models involving more complex neural network intent\nclassiﬁcation architectures. Moreover, we also showed that\nConfusion2vec suffers the least degradation between clean\nand ASR transcripts. We also found that the Confusion2vec\nconsistently provides the best classiﬁcation rates even when\nthe intent classiﬁer is trained on ASR transcripts. The exper-\niments indicated that the loss in accuracies between training\nthe intent classiﬁer on clean versus the ASR transcripts is\nreduced to 0.89% from 2.57% absolute. Overall, the results\nillustrate that confusion2vec has inherent knowledge of the\nacoustic ambiguity (similarity) word relations which corre-\nlates with the ASR errors using which the classiﬁer is able to\nrecover from certain errors more efﬁciently. In this section,\nwe incorporate the confusion2vec 2.0 embeddings with in-\nherent knowledge of acoustic ambiguity to allow robust intent\nclassiﬁcation.\n6.1. Database\nWe conduct experiments on the Airline Travel Information\nSystems (ATIS) benchmark dataset [12]. The dataset consists\nof humans making ﬂight related inquiries with an automated\nanswering machine with audio recorded and its transcripts\nmanually annotated. ATIS consists of 18 intent categories.\nThe dataset is divided into train (4478 samples), development\n(500 samples) and test (893 samples) consistent with previous\nworks [27, 11, 8]. For ASR evaluations, the audio recordings\nare down-sampled from 16kHz to 8kHz and then decoded us-\ning the ASR setup described in section 5.2.1 using the audio\nmappings5. The ASR achieves a WER of 18.54% on the ATIS\ntest set.\n6.2. Experimental Setup\nFor intent classiﬁcation we adopt a simple RNN architecture\nidentical to [27], to allow for direct comparison. The archi-\ntecture of the neural network is intentionally kept simple for\neffective inference of the efﬁcacy of the proposed embedding\nword features. The classiﬁer is comprised of an embedding\nlayer followed by a single layer of bi-directional recurrent\nneural network (RNN) with long short-term memory (LSTM)\nunits which is followed by a linear dense layer with soft-\nmax function to output a probability distribution across all the\nintent categories. The embedding layer is ﬁxed throughout\nthe training except for the randomly initialized embeddings\nwhere the embedding is estimated on the in-domain data spe-\nciﬁc to the task of intent detection.\nThe intent classiﬁcation models are trained on the 4478\nsamples of training subset and the hyper-parameters are tuned\non the development set. We choose the best set of hyper-\nparameters yielding the best results on the development set\nand then apply it on the unseen held-out test subset of both\nthe manual clean transcripts and the ASR transcripts and\nreport the results. For training we treat each utterance as a\nsingle sample (batch size = 1). The hyper-parameter space\nwe experiment are as follows: the hidden dimension size\nof the LSTM is tuned over {32, 64, 128, 256}, the learn-\ning rate over {0.0005, 0.001}, the dropout is tuned over\n{0.1, 0.15, 0.2, 0.25}. The Adam optimizer is employed for\noptimization and trained for a total of 50 epochs with early\n5https://github.com/pgurunath/slu confusion2vec\nstopping when the loss on the development set doesn’t im-\nprove for 5 consecutive epochs.\n6.3. Baselines\nWe include results from several baseline systems for pro-\nviding comparisons of Confusion2Vec 2.0 with the popular\ncontext-free word embeddings, contextual embeddings, pop-\nular established NLU systems and the current state-of-the-art.\n1. Context-Free Embeddings: GloVe6 [21], skip-gram\nword2vec7 [20] and fastText8 [3] word representations\nare employed. They are referred to as context-free em-\nbeddings since the word representations are static irre-\nspective of the context.\n2. ELMo: Peters et al. [22] proposed deep contextualized\nword representation based on character based deep\nbidirectional language model trained on large text cor-\npus. The models effectively model syntax and seman-\ntics of the language along varying linguistic contexts.\nUnlike context-free embeddings, ELMo embeddings\nhave varying representations for each word depending\non the word’s context. We employ the original model\ntrained on 1 Billion Word Benchmark with 93.6 million\nparameters9. For intent-classiﬁcation we add a single\nbi-directional LSTM layer with attention for multi-task\njoint intent and slot predictions.\n3. BERT: Devlin et al. [6] introduced BERT bidirec-\ntional contextual word representations based on self\nattention mechanism of Transformer models.\nBERT\nmodels make use of masked language modeling and\nnext sentence prediction to model language. Similar\nto ELMo, the word embeddings are contextual, i.e.,\nvary according to the context. We employ “bert-base-\nuncased” model10 with 12 layers of 768 dimensions\neach trained on BookCorpus and English Wikipedia\ncorpus.\nFor intent-classiﬁcation we add a single bi-\ndirectional LSTM layer with attention for multi-task\njoint intent and slot predictions.\n4. Joint SLU-LM: Liu and Lane [17] employed joint\nmodeling of the next word prediction along with intent\nand slot labeling. The unidirectional RNN model up-\ndates intent states for each word input and uses it as\ncontext for slot labeling and language modeling.\n5. Attn. RNN Joint SLU: Liu and Lane [16] proposed\nattention based encoder-decoder bidirectional RNN\n6https://nlp.stanford.edu/projects/glove/\n7https://code.google.com/archive/p/word2vec/\n8https://fasttext.cc/docs/en/pretrained-vectors.html\n9https://allennlp.org/elmo\n10https://github.com/google-research/bert\nmodel in a multi-task model for joint intent and slot-\nﬁlling tasks. A weighted average of the encoder bidi-\nrectional LSTM hidden states provides information\nfrom parts of the input word sequence which is used\ntogether with time aligned encoder hidden state for the\ndecoder to predict the slot labels and intent.\n6. Slot-Gated Attn.: Goo et al. [8] introduced a slot-\ngated mechanism which introduces additional gate\nto improve slot and intent prediction performance by\nleveraging intent context vector for slot ﬁlling task.\n7. Self Attn. SLU: Li et al. [15] proposed self-attention\nmodel with gate mechanism for joint learning of intent\nclassiﬁcation and slot ﬁlling by utilizing the semantic\ncorrelation between slots and intents. The model esti-\nmates embeddings augmented with intent information\nusing self attention mechanism which is utilized as a\ngate for slot ﬁlling task.\n8. Joint BERT: Chen et al. [4] proposed to use BERT em-\nbeddings for joint modeling of intent and slot-ﬁlling.\nThe pre-trained BERT embeddings are ﬁne-tuned for\n(i) sentence prediction task - intent detection, and (ii)\nsequence prediction task - slot ﬁlling. The Joint BERT\nmodel lacks the bi-directional LSTM layer in compari-\nson to the earlier baseline BERT based model.\n9. SF-ID Network: Haihong et al. [10] introduced a bi-\ndirectional interrelated model for joint modeling of in-\ntent detection and slot-ﬁlling. An iteration mechanism\nis proposed where the SF subnet introduces the intent\ninformation to slot-ﬁlling task while the ID-subnet ap-\nplies the slot information to intent detection task. For\nthe task of slot-ﬁlling a conditional random ﬁeld layer\nis used to derive the ﬁnal output.\n10. ASR Robust ELMo: Huang and Chen [13] proposed\nASR robust contextualized embeddings for intent de-\ntection. ELMo embeddings are ﬁne-tuned with a novel\nloss function which minimizes the cosine distance be-\ntween the acoustically confused words found in ASR\nconfusion networks. Two techniques based on super-\nvised and unsupervised extraction of word confusions\nare explored.\nThe ﬁne-tuned contextualized embed-\ndings are then utilized for spoken language intent de-\ntection.\n6.4. Results\nIn this section, we conduct experiments by training models on\n(i) clean human annotations and (ii) noisy ASR transcriptions.\nModel\nReference\nASR\n∆diff\nContext-Free Embeddings\nRandom\n2.69\n10.75\n8.06\nGloVe [21]\n1.90\n8.17\n6.27\nWord2Vec [20]\n2.69\n8.06\n5.37\nfastText [3]\n1.90\n8.40\n6.50\nJoint SLU-LM [17] †\n1.90\n9.41\n7.51\nAttn. RNN Joint SLU [16] †\n1.79\n8.06\n6.27\nSlot-Gated Attn. [8] †\n3.92\n10.64\n6.72\nSelf Attn. SLU [15] †\n2.02\n9.18\n7.16\nSF-ID Network [10] †\n3.14\n10.53\n7.39\nC2V 1.0 [26]\n2.46\n6.38\n3.92\nContextual Embeddings\nELMo [22] †\n1.46\n7.05\n5.59\nBERT [6] †\n1.12\n6.16\n5.04\nJoint BERT [4] †\n2.46\n7.73\n5.27\nASR Robust ELMo (unsup.) [13]\n3.24\n5.26\n2.02\nASR Robust ELMo (sup.) [13]\n3.46\n5.03\n1.57\nProposed Context-Free Embeddings\nC2V-c 2.0\n3.36\n5.82\n2.46\nC2V-a 2.0\n2.46\n4.37\n1.91\nfastText + C2V-c 2.0\n1.79\n4.70\n2.91\nfastText + C2V-a 2.0\n1.90\n5.04\n3.14\nTable 3: Results: Model trained on clean Reference: Classiﬁcation Error Rates (CER) for Reference and ASR Transcripts\n∆diff is the absolute degradation of model from clean to ASR. C2V 1.0 corresponds to C2V-1 + C2V-c (JT) in Table 1 and 2.\n† indicates joint modeling of intent and slot-ﬁlling.\n6.4.1. Training on Clean Transcripts\nTable 3 lists the results of the intent detection in terms of clas-\nsiﬁcation error rates (CER). The “Reference” column corre-\nsponds to results on human transcribed ATIS audio and the\n“ASR” corresponds to the evaluations on the noisy speech\nrecognition transcripts. Firstly, evaluating on the Reference\nclean transcripts, we observe the confusion2vec 2.0 with sub-\nword encoding is able to achieve the third best performance.\nThe best performing confusion2vec 2.0 achieves a CER of\n1.79%. Among the different versions of the proposed sub-\nword based confusion2vec, we ﬁnd that the concatenated ver-\nsions are slightly better. We believe this is because the con-\ncatenated models exhibit better semantic and syntactic rela-\ntions (see Table 1 and 2) compared to the non-concatenated\nones.\nAmong the baseline models, the contextual embed-\nding like BERT and ELMo gives the best CER. Note, the\nproposed confusion2vec embeddings are context-free and are\nable to outperform other context-free embedding models such\nas GloVe, word2vec and fastText.\nSecondly, evaluating the performance on the noisy ASR\ntranscripts, we ﬁnd that all the subword based confusion2vec\n2.0 models outperform the popular word vector embeddings\nby a big margin. The subword-confusion2vec gives an im-\nprovement of approximately 45.78% relative to the best per-\nforming context-free word embeddings. The proposed em-\nbeddings also improve over the contextual embeddings in-\ncluding BERT and ELMo (relative improvements of 29.06%).\nMoreover, the results are also an improvement over the non-\nsubword confusion2vec word vectors (31.50% improvement).\nComparisons between the different versions of the proposed\nconfusion2vec show the intra-confusion conﬁguration yields\nthe least CER. The best results with the proposed model out-\nperforms the state-of-the-art (ASR Robust ELMo [13]) by re-\nducing the CER by a relative of 13.12%. Inspecting the degra-\ndation, ∆diff (drop in performance between the clean and ASR\nevaluations), we ﬁnd that all the confusion2vec 2.0 with sub-\nword information undergo low degradation while giving the\nbest CER, thereby re-afﬁrming the robustness to noise in tran-\nscripts. This conﬁrms our initial hypothesis that the subword\nencoding is better able to represent the acoustic ambiguities\nin the human language.\n6.4.2. Training on Noisy ASR Transcripts\nTable 4 presents the results obtained by training models on the\nASR transcripts and evaluated on the ASR transcripts. Here\nwe omit all the joint intent-slot ﬁlling baseline models, since\ntraining on ASR transcripts need aligned set of slot labels due\nto insertion, substitution and deletion errors which is out-of-\nscope of this study. We note that the confusion2vec mod-\nels give signiﬁcantly lower CER. The subword based con-\nfusion2vec models also provide improvements over the non-\nsubword based confusion2vec model (21.28% improvement).\nModel\nWER %\nCER %\nRandom\n18.54\n5.15\nGloVe [21]\n18.54\n6.94\nWord2Vec [20]\n18.54\n5.49\nSchumann and Angkititrakul [25]\n10.55\n5.0411\nC2V 1.0\n18.54\n4.70\nC2V-c 2.0\n18.54\n4.82\nC2V-a 2.0\n18.54\n4.26\nfastText + C2V-c 2.0\n18.54\n3.70\nfastText + C2V-a 2.0\n18.54\n4.26\nTable 4: Results: Model trained and evaluated on ASR\ntranscripts.\nC2V 1.0 corresponds to C2V-1 + C2V-c (JT) in Table 1 and 2\nComparing the results in Table 3 and Table 4, we would like to\nhighlight the subword-confusion2vec model gives a minimum\nCER of 4.37% on model trained on clean transcripts which\nis much better than the CER obtained by popular word em-\nbeddings like word2vec, GloVe, fastText even when trained\non the ASR transcripts (15.15% better relatively). These re-\nsults prove the subword-confusion2vec models can eliminate\nthe need for re-training natural language understanding and\nprocessing algorithms on ASR transcripts for robust perfor-\nmance.\n7. CONCLUSION\nIn this paper, we proposed the use of subword encoding for\nmodeling the acoustic ambiguity information and augment\nword vector representations along with the semantic and syn-\ntax of the language.\nEach word in the language is repre-\nsented as a sum of its constituent character n-gram subwords.\nThe advantages of the subwords are conﬁrmed by evaluat-\ning the proposed models on various word analogy tasks and\nword similarity tasks designed to assess the effective acoustic\nambiguity, semantic and syntactic knowledge inherent in the\nmodels. Finally, the proposed subword models are applied\nto the task of spoken language intent detection. The results\nof intent classiﬁcation system suggest the proposed subword\nconfusion2vec models greatly enhance the classiﬁcation per-\nformance when evaluated on the noisy ASR transcripts. The\nresults highlight that subword-confusion2vec models are ro-\nbust and domain-independent and do not need re-training of\nthe classiﬁer on ASR transcript.\nIn the future, we plan to model ambiguity information us-\ning deep contextual modeling techniques such as BERT. We\nbelieve bidirectional information modeling with attention can\nfurther enhance ambiguity modeling. On the application side,\nwe plan to implement and assess the effect of using Confu-\n11We don’t domain-constrain, optimize or re-score our ASR, as in [25]\nsion2vec models for a wide range of natural language under-\nstanding and processing applications such as speech transla-\ntion, dialogue tracking etc.\nReferences\n[1] Jennifer Aydelott and Elizabeth Bates. Effects of acous-\ntic distortion and semantic context on lexical access.\nLanguage and cognitive processes, 19(1):29–56, 2004.\n[2] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. A neural probabilistic language model.\nJournal of machine learning research, 3(Feb):1137–\n1155, 2003.\n[3] Piotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. Enriching word vectors with subword\ninformation. Transactions of the Association for Com-\nputational Linguistics, 5:135–146, 2017.\n[4] Qian Chen, Zhu Zhuo, and Wen Wang. Bert for joint\nintent classiﬁcation and slot ﬁlling.\narXiv preprint\narXiv:1902.10909, 2019.\n[5] Christopher Cieri, David Miller, and Kevin Walker. The\nﬁsher corpus: a resource for the next generations of\nspeech-to-text. In LREC, volume 4, pages 69–71, 2004.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\n[7] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\nEhud Rivlin, Zach Solan, Gadi Wolfman, and Eytan\nRuppin. Placing search in context: The concept revis-\nited. In Proceedings of the 10th international conference\non World Wide Web, pages 406–414, 2001.\n[8] Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li\nHuo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung\nChen. Slot-gated modeling for joint slot ﬁlling and in-\ntent prediction.\nIn Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), volume 2, pages\n753–757, 2018.\n[9] Michael U Gutmann and Aapo Hyv¨arinen.\nNoise-\ncontrastive estimation of unnormalized statistical mod-\nels, with applications to natural image statistics. The\njournal of machine learning research, 13(1):307–361,\n2012.\n[10] E Haihong, Peiqing Niu, Zhongfu Chen, and Meina\nSong. A novel bi-directional interrelated model for joint\nintent detection and slot ﬁlling. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5467–5471, 2019.\n[11] Dilek Hakkani-T¨ur, G¨okhan T¨ur, Asli Celikyilmaz,\nYun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-Yi\nWang. Multi-domain joint semantic frame parsing using\nbi-directional rnn-lstm. In Interspeech, pages 715–719,\n2016.\n[12] Charles T Hemphill, John J Godfrey, and George R Dod-\ndington. The ATIS spoken language systems pilot cor-\npus. In Speech and Natural Language: Proceedings of\na Workshop Held at Hidden Valley, Pennsylvania, June\n24-27, 1990, 1990.\n[13] Chao-Wei Huang and Yun-Nung Chen. Learning asr-\nrobust contextualized embeddings for spoken language\nunderstanding.\nIn ICASSP 2020-2020 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 8009–8013. IEEE, 2020.\n[14] Faisal Ladhak, Ankur Gandhe, Markus Dreyer, Lambert\nMathias, Ariya Rastrow, and Bj¨orn Hoffmeister. Lat-\nticernn: Recurrent neural networks over lattices. In In-\nterspeech, pages 695–699, 2016.\n[15] Changliang Li, Liang Li, and Ji Qi.\nA self-attentive\nmodel with gate mechanism for spoken language under-\nstanding. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing, pages\n3824–3833, 2018.\n[16] Bing Liu and Ian Lane. Attention-based recurrent neural\nnetwork models for joint intent detection and slot ﬁlling.\nIn Interspeech 2016, pages 685–689, 2016.\ndoi: 10.\n21437/Interspeech.2016-1352.\n[17] Bing Liu and Ian Lane. Joint online spoken language un-\nderstanding and language modeling with recurrent neu-\nral networks. arXiv preprint arXiv:1609.01462, 2016.\n[18] Tom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur.\nRecurrent neural\nnetwork based language model.\nIn Eleventh Annual\nConference of the International Speech Communication\nAssociation, 2010.\n[19] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781, 2013.\n[20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean.\nDistributed representations of\nwords and phrases and their compositionality. In Ad-\nvances in neural information processing systems, pages\n3111–3119, 2013.\n[21] Jeffrey Pennington, Richard Socher, and Christopher D\nManning. Glove: Global vectors for word representa-\ntion. In Proceedings of the 2014 conference on empir-\nical methods in natural language processing (EMNLP),\npages 1532–1543, 2014.\n[22] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations.\narXiv preprint arXiv:1802.05365, 2018.\n[23] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas\nBurget, Ondrej Glembek, Nagendra Goel, Mirko Han-\nnemann, Petr Motlicek, Yanmin Qian, Petr Schwarz,\net al. The kaldi speech recognition toolkit. In IEEE 2011\nworkshop on automatic speech recognition and under-\nstanding, number CONF. IEEE Signal Processing Soci-\nety, 2011.\n[24] Tobias Schnabel, Igor Labutov, David Mimno, and\nThorsten Joachims.\nEvaluation methods for unsuper-\nvised word embeddings.\nIn Proceedings of the 2015\nconference on empirical methods in natural language\nprocessing, pages 298–307, 2015.\n[25] Raphael Schumann and Pongtep Angkititrakul. Incor-\nporating asr errors with attention-based, jointly trained\nrnn for intent detection and slot ﬁlling. In 2018 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 6059–6063. IEEE, 2018.\n[26] Prashanth Gurunath Shivakumar and Panayiotis Geor-\ngiou. Confusion2vec: Towards enriching vector space\nword representations with representational ambiguities.\nPeerJ Computer Science, 5:e195, 2019.\n[27] Prashanth Gurunath Shivakumar, Mu Yang, and Panayi-\notis Georgiou. Spoken language intent detection using\nconfusion2vec. arXiv preprint arXiv:1904.03576, 2019.\n[28] Matthias Sperber, Graham Neubig, Ngoc-Quan Pham,\nand Alex Waibel. Self-attentional models for lattice in-\nputs. arXiv preprint arXiv:1906.01617, 2019.\n[29] Kai Sheng Tai, Richard Socher, and Christopher D Man-\nning.\nImproved semantic representations from tree-\nstructured long short-term memory networks.\narXiv\npreprint arXiv:1503.00075, 2015.\n[30] Zhixing Tan, Jinsong Su, Boli Wang, Yidong Chen, and\nXiaodong Shi.\nLattice-to-sequence attentional neural\nmachine translation models. Neurocomputing, 284:138–\n147, 2018.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Ad-\nvances in neural information processing systems, pages\n5998–6008, 2017.\n[32] Robert Weide. The cmu pronunciation dictionary, re-\nlease 0.6, 1998.\n[33] Fengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang,\nand Kehai Chen.\nLattice-based transformer en-\ncoder for neural machine translation.\narXiv preprint\narXiv:1906.01282, 2019.\n[34] Haihua Xu, Daniel Povey, Lidia Mangu, and Jie Zhu.\nMinimum bayes risk decoding and system combination\nbased on a recursion for edit distance. Computer Speech\n& Language, 25(4):802–828, 2011.\n[35] Xiaohui Zhang, Jan Trmal, Daniel Povey, and Sanjeev\nKhudanpur.\nImproving deep neural network acoustic\nmodels using generalized maxout networks.\nIn 2014\nIEEE international conference on acoustics, speech\nand signal processing (ICASSP), pages 215–219. IEEE,\n2014.\n",
  "categories": [
    "cs.CL",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2021-02-03",
  "updated": "2021-02-19"
}