{
  "id": "http://arxiv.org/abs/2102.03022v1",
  "title": "Deceptive Reinforcement Learning for Privacy-Preserving Planning",
  "authors": [
    "Zhengshang Liu",
    "Yue Yang",
    "Tim Miller",
    "Peta Masters"
  ],
  "abstract": "In this paper, we study the problem of deceptive reinforcement learning to\npreserve the privacy of a reward function. Reinforcement learning is the\nproblem of finding a behaviour policy based on rewards received from\nexploratory behaviour. A key ingredient in reinforcement learning is a reward\nfunction, which determines how much reward (negative or positive) is given and\nwhen. However, in some situations, we may want to keep a reward function\nprivate; that is, to make it difficult for an observer to determine the reward\nfunction used. We define the problem of privacy-preserving reinforcement\nlearning, and present two models for solving it. These models are based on\ndissimulation -- a form of deception that `hides the truth'. We evaluate our\nmodels both computationally and via human behavioural experiments. Results show\nthat the resulting policies are indeed deceptive, and that participants can\ndetermine the true reward function less reliably than that of an honest agent.",
  "text": "Deceptive Reinforcement Learning for Privacy-Preserving\nPlanning\nZhengshang Liu, Yue Yang, Tim Miller, and Peta Masters\nSchool of Computing and Information Systems, The University of Melbourne\n{zhengshangl,yuey16}@student.unimelb.edu.au, {peta.masters,tmiller}@unimelb.edu.au\nABSTRACT\nIn this paper, we study the problem of deceptive reinforcement\nlearning to preserve the privacy of a reward function. Reinforce-\nment learning is the problem of finding a behaviour policy based on\nrewards received from exploratory behaviour. A key ingredient in\nreinforcement learning is a reward function, which determines how\nmuch reward (negative or positive) is given and when. However, in\nsome situations, we may want to keep a reward function private;\nthat is, to make it difficult for an observer to determine the reward\nfunction used. We define the problem of privacy-preserving rein-\nforcement learning, and present two models for solving it. These\nmodels are based on dissimulation ‚Äì a form of deception that ‚Äòhides\nthe truth‚Äô. We evaluate our models both computationally and via\nhuman behavioural experiments. Results show that the resulting\npolicies are indeed deceptive, and that participants can determine\nthe true reward function less reliably than that of an honest agent.\nKEYWORDS\nReinforcement Learning; Deception; Dissimulation\nACM Reference Format:\nZhengshang Liu, Yue Yang, Tim Miller, and Peta Masters. 2021. Deceptive\nReinforcement Learning for Privacy-Preserving Planning. In Proc. of the\n20th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2021), Online, May 3‚Äì7, 2021, IFAAMAS, 10 pages.\n1\nINTRODUCTION\nIn this paper, we study the problem of deceptive reinforcement\nlearning to preserve the privacy of reward functions. Reinforcement\nlearning is a framework within which an agent learns a behaviour\npolicy by interacting with its environment and responding to posi-\ntive and negative rewards [34]. Within this framework, the reward\nfunction, which determines when and how much reward (negative\nor positive) is given for each possible behaviour in a system, is\ncritical. It defines the goals of the agent.\nSituations frequently arise in which we do not want our goals to\nbe known. Consider a military commander needing to conceal the\npurpose of troop movements; a crime-writer who must avoid giving\naway the end of the story. In reinforcement learning, when we want\nto make it difficult for an observer to infer the final destination, we\nmust prevent or delay them from determining the reward function\nused to learn a policy.\nDeception involves fostering or maintaining false belief in the\nminds of others [8]. Bell defines two general types: dissimulation,\nProc. of the 20th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2021), U. Endriss, A. Now√©, F. Dignum, A. Lomuscio (eds.), May 3‚Äì7, 2021, Online.\n¬© 2021 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org). All rights reserved.\nFigure 1: Using dissimulation to deceive an observer about\nthe final destination. Taking the red path, what is the final\ndestination?\nwhich ‚Äòhides the truth‚Äô to avoid revealing information; and sim-\nulation, which ‚Äòshows the false‚Äô enticing an observer to believe\nsomething that is not true. Several models of deceptive planning\nhave been proposed in recent years [14, 15, 20, 23]. However, these\nare model-based and require reasoning about the model structure\nto inform the dissimulation, so are not applicable to model-free\nMDPs.\nIn this paper, we define a more general model of dissimulation\nfor preserving goal privacy. We present two methods: one based on\nambiguity, in which the agent selects actions that maximise the en-\ntropy from the observer‚Äôs point of view; and one based on Masters\nand Sardina [21]‚Äôs model for intention recognition using irrational-\nity, which takes action selection as a weighted sum of honest and\n‚Äòirrational‚Äô behaviour. These methods use pre-trained Q-functions\n(or policies). Since Q-functions provide a measure of expected fu-\nture reward for each action, they enable a general representation\nof the possibilities for action selection [34].\nFigure 1 shows an example of dissimulation. An escort driver\nin Paris has three potential destinations (in green), starting from\nthe blue point. The green routes are the optimal routes to each\narXiv:2102.03022v1  [cs.LG]  5 Feb 2021\ndestination. If the driver takes the red route and is where the car is\nlocated, as the observer, what do you think the final destination is,\nknowing that the driver may be deceiving you? The path makes\nsub-optimal progress to all three destinations. If the driver turns\nright and follows the blue route, destination 1 is probably elimi-\nnated from the set of potential goals, but the blue path is valid for\nboth destinations 2 and 3. Our ambiguity model would generate\na path corresponding to the red path, and the red+blue path for\ndestinations 2 and 3 once destination 1 is pruned as a possible path.\nSuch problems of deception are common and the use of AI for\ndeception is gaining recent traction [30] in domains such as path\nplanning [20], military tactical planning [26, 29], countering cyber-\nattacks [27] and conjuring tricks [33].\nWe evaluate our models by using a na√Øve intention recognition\nsystem and via a human subject experiment with 69 non-na√Øve\nparticipants. The intention recognition system and our participants\nwere required to estimate the likelihood of different destinations in\na path planning simulation. The results show that our agents are\neffective at hiding their reward compared to honest agents, but that,\nlike honest agents, the true reward function becomes clearer as\nmore actions are executed. The irrationality model deceives more\nthan the ambiguity model, but receives less discounted expected\nreward on its real reward function.\n2\nBACKGROUND AND RELATED WORK\n2.1\nTheory of Deception\nDeception, psychologists broadly agree, is a pejorative term for\nthe fostering or maintenance of false belief in the minds of oth-\ners [8]. Computer science has necessarily widened the definition,\nfirst, to accommodate mindless machines incapable of belief (as\nsuch) and second, to allow for the emerging realisation (particularly\nfrom the field of social robotics) that deception is a fundamental\naspect of intelligent behaviour, frequently beneficial not only to\nthe deceiver but to the deceived [32, 38]. Whereas deceptive AI\nhas tended to focus on detection [2], ethical implications [1], and\nthe qualities that make a deceptive act most likely to succeed [11],\nmilitary strategists Bell and Whaley [5, 6, 40] provide a general\ntheory focused on how to deceive. Their non-judgemental defini-\ntion is ‚Äúthe distortion of perceived reality‚Äù which they maintain\ncan only be achieved in one of two ways: by simulation (‚Äúshowing\nthe false‚Äù) or dissimulation (‚Äúhiding the true‚Äù). They propose three\nvariations on each method and suggest that a deceptive strategy\ntypically involves combinations of those six tactics in pursuit of\nsome strategic objective.\n2.2\nDeceptive Planning\nIn planning, deception is frequently associated with security and\nhas become almost synonymous with privacy-protection [9]. Dis-\nsimulation in this context becomes the task of obscuring intent\nby maximising a plan‚Äôs ambiguity [14, 16]. Obscuring intent as-\nsumes an observer engaged in intention recognition; and deceptive\nplanning is commonly (though not exclusively)1 conceived as an\ninversion of intention recognition [10, 14, 20].\n1See [15] for an argument against.\nIn this paper, we invert a type of cost-based goal recognition\n[25]. To generate a probability distribution over goals, they com-\npare each goal‚Äôs cost difference, that is, the difference between the\noptimal cost of a plan via observed actions and the optimal cost of\nany alternative plan. The lower the cost difference, the higher the\nprobability. Vered et al. [36] take a similar approach but instead of\ncost difference use the ratio between the optimal cost of reaching\neach goal via the observations and the optimal cost per se. They\npropose two heuristics to minimise the computational effort in the\ncontext of online recognition, one of which suggests pruning a goal\nfrom consideration if observations deviate too far from the optimal\nbehaviour. Masters and Sardina [20] apply Bell and Whaley‚Äôs the-\nory to path-planning. They assume a na√Øve observer, modelled as a\nprobabilistic intention recognition system. The inputs are observa-\ntions ¬Æùëúand the output is a probability distribution across potential\ngoals ùëÉ(ùê∫|¬Æùëú). An action is deceptive if, at that step, the probability\nof the real goal ùëîùëüdoes not dominate the probability of some other\ngoal: ùëÉ(ùëîùëü|¬Æùëú) ‚â§ùëÉ(ùëî)|¬Æùëú) for all ùëî‚ààùê∫\\{ùëîùëü}. They observe that every\npath has one last deceptive point (ùêøùê∑ùëÉ), even if it is the starting\npoint, and show that there is a radius around goal within which\ntrying to deceive is no longer valuable, and the agent should head\ndirectly to its true goal. At the path level, they define deceptive\ndensity as inversely proportional to the number of truthful steps it\ncontains; and deceptive extent by the distance remaining after the\nlast deceptive point has been reached, that is, the optimal cost from\nùêøùê∑ùëÉto ùëîùëü. Kulkarni et al. [16] extend a similar approach to classical\ntask planning, more general than path planning. Both approaches,\nhowever, are applicable only to model-based problems, so do not\ngeneralise to MDPs.\n2.3\nDeception in Markov Decision Processes\nOrnik and Topcu [23] present the comprehensive model of plan-\nning for deception in MDPs. Their model defines the notion of a\nbelief-induced reward, which is a reward that the agent receives, but\nthat is also affected by the belief of an observer. This includes cases\nwhen the observer has only partial visibility of the environment.\nFor example, the reward is received if the observer‚Äôs belief is that\nthe agent is not in the state that receives the reward, otherwise it\nreceives some negative reward. Ornik and Topcu then show how\nto define optimal policies for belief-induced rewards, and present\nsome examples of deceptive belief-induced reward functions. How-\never, their work is model-based, and further, they do not specify a\ndissimulative policy.\nKarabag et al. [13] present a model-based solution to a different\ndeceptive problem. In their problem, an agent is provided a policy\nto follow to achieve a goal, specified in linear temporal logic, but\ncan instead follow a different deceptive policy, modelled from an\nMDP, to achieve the goal. The aim is to try to achieve both policies\nwhile minimising the likelihood of the supervisor knowing.\nA closely related area of research is differential privacy for re-\ninforcement learning [18, 37, 39]. The general approach to this\nis to modify Q-learning and policy-based reinforcement learning\nalgorithms by e.g. adding Gaussian noise to the update rule [39].\nWhile the general idea is similar, there are a few major differences.\nFirst, our problem definition is motivated by strategic deception\nbased on theory of deception [6, 40], rather than on the idea of\nprivacy per se. This difference manifests itself in the problem def-\ninition: we assume that reward functions are fully observable to\nthe observer, but that the observer does not know which reward\nfunction the current policy is trained on. The work cited here, on\nthe other hand, assume that there is a single reward function and it\nis not observable. A privacy-preserving approach like this is not\nstrategic as it does not trade off against different goals. Further,\nit means that we explicitly measure the simulation of the policy,\nrather than the privacy of the reward function. While we could\nframe our problem in a similar way to Wang and Hegde, in strategic\ndeception, it is uncommon for an observer not to have a model of\nlikely goals for an actor. Second, we present a general model for\nMDPs, whereas Wang and Hegde [39], Vietri et al. [37], and Ma\net al. [18] are Q-learning and policy-gradient approaches. Finally,\nwe measure the strategic deception achieve both in computational\nand human studies.\nSome work in deceptive reinforcement learning investigates\ntechniques to counter the deceptive strategies of other agents, such\nas the agent being fed incorrect reward signals [12], deception in\ngames and multi-agent systems [4, 17, 28].\n2.4\nInverse Reinforcement Learning and\nImitation Learning\nOur definition of deceptive reinforcement learning is related to\ninverse reinforcement learning [22] and imitation learning [41].\nInverse reinforcement learning is the problem of inferring a reward\nfunction given traces of an agent‚Äôs behaviour in a variety of circum-\nstances and the sensory input to the agent. Imitation learning [41]\nis similar to inverse reinforcement learning, but instead of infer-\nring a reward function, the aim is to infer a policy. These methods\nlearn a reward function by observing e.g., a human complete the\nsame task many times. The problem that we define in this paper\ncould be framed as the problem of producing a policy that makes\nit difficult to perform inverse reinforcement or imitation learning.\nHowever, there are two key differences. First, in this paper, we aim\nto simply deceive for a single trace of behaviour, whereas these\ninverse learning problems require either a known optimal policy\nfrom which to generate traces, or a set of traces of behaviour. De-\nspite this, there is clearly a related problem that is of interest in\nstudying the problem of deception as obfuscating inverse reinforce-\nment learning. Second, we define a set of possible reward functions,\nwhereas inverse reinforcement learning starts with the set of all\nreward functions. The approach from Wang and Hegde [39] above\nproposes a Q-learning-based solution for such a problem.\n3\nMODELS\nIn this section, we define privacy-preserving reinforcement learning\nand present two solutions based on dissimulation.\n3.1\nProblem Formalism\nDefinition 1 (Markov Decision Process (MDP) [24]). An MDP is\na tuple Œ† = (ùëÜ,ùê¥,ùëá,ùëü,ùõæ), in which ùëÜis a set of states, ùê¥is a set of\nactions, ùëá(ùë†,ùëé,ùë†‚Ä≤) is a transition function from ùëÜ√ó ùê¥‚Üí2ùëÜ, which\ndefines the probability of action ùëégoing to state ùë†‚Ä≤ from state ùë†,\nùëü(ùë†,ùëé,ùë†‚Ä≤) is the reward received for the transition from executing\naction ùëéin state ùë†and ending up in state ùë†‚Ä≤, and ùõæis the discount\nfactor. The task is to synthesise a policy ùúã: ùëÜ‚Üíùê¥from states to\nactions that maximises expected reward over trajectories in ùúãfor\nproblem Œ†:\nE[\nùëá\n‚àëÔ∏Å\nùë°=0\nùõæùë°ùëÖ(ùë†, ùúã(ùë†),ùë†‚Ä≤)]\nA Q-function ùëÑ: ùëÜ√ó ùê¥‚ÜíR defines the value of selecting\nan action ùëéfrom state ùë†and then following the policy ùúã, writ-\nten ùëÑ(ùë†,ùëé). An optimal policy ùúãcan then be defined as ùúã(ùë†) =\narg maxùëé‚ààùê¥ùëÑ(ùë†,ùëé).\nDefinition 2 (Belief-induced reward). Ornik and Topcu [23] define\nbelief-induced rewards to model rewards that are dependent on the\nreward function and the beliefs of an observer. Formally, this is a\nfunction ùêø: ùëÜ√ó ùê¥√ó ùëÜ√ó B, in which B is a set of beliefs.\nOrnik and Topcu leave the actual instantiation of beliefs abstract,\nbut the concept is that ùêø(ùë†,ùëé,ùë†, ùêµ) is a reward that is some function\nof the belief of an observer and the real reward.\nUsing a belief-inducted reward, the task of solving an MDP is to\nsynthesise a policy ùúãthat maximises:\nE[\nùëá\n‚àëÔ∏Å\nùë°=0\nùõæùë°ùêø(ùë†, ùúã(ùë†),ùë†‚Ä≤, ùêµ)]\n(1)\nTo specify a deceptive reinforcement learning problem, we must\ninstantiate B and define ùêø. In our setting, B = R, as we aim to\ndeceive about the particular reward function. For ùêø, we need to\ndefine what is means to deceive about a reward function.\nFirst, we need to define the observer‚Äôs task. This is an intention\nrecognition task [3] in which the observer derives a probability\ndistribution over R that defines the probability ùëÉ(ùëüùëñ| ¬Æùëúùë°) that\nthe reward function ùëüùëñis the true reward function, given ¬Æùëúùë°, the\nsequence of observed state-action pairs up until time ùë°. For example,\nthe probability of the final destination of the each of the three\nlocations outlined in Figure 1. Our deceptive models later present\nsome ways to define this for an MDP.\nDefinition 3 (Deceptive reinforcement learning for Reward-Func-\ntion Privacy). A deceptive reinforcement learning problem is a tuple\nŒ† = (ùëÜ,ùê¥,ùëá,ùëü, R,ùõæ, ùêø), in which ùëÜ, ùê¥, ùëá, ùëü, and ùõæare as in Defini-\ntion 1, R is a set of possible reward functions such that ùëü‚ààR, which\nmodel the set of reward functions that an observer may believe\nare true, and ùêøis belief-inducted reward. The task is to synthesise\na policy ùúãthat maximises expected reward over trajectories in ùúã\nwhile also making it difficult for an observer to determine which\nreward function in R is the real reward function.\nDefining ùêøis not a straightforward task, and depends on the\nspecific domain being used. Typically, it would be defined as some\nweighted measure of the reward and the level of deception, such as:\nùêø( ¬Æùëúùë°, R) = (1 ‚àíùúî) ¬∑ ùëü(ùë†,ùëé,ùë†‚Ä≤) + ùúî¬∑ ùëë( ¬Æùëúùë°R)\n(2)\nin which ùë†, ùëé, and ùë†‚Ä≤ are the state-action-state values of the last\ntransition in ¬Æùëúùë°; that is, the latest transition; ùëë( ¬Æùëúùë°, R) is a measure\nof deception such as the simulation value defined by Masters and\nSardina [20], and ùúî‚àà[0, 1] is a weighting factor for deception\nthat determines how important the deception is. One difficulty in\ndefining ùúîis that the rewards and the deception are of different\nmagnitudes. Even if both are normalised, a policy using dissimula-\ntion (hides the truth) may use subtle deception, meaning that any\ndefinition of ùëë( ¬Æùëúùë°, R) has to capture that subtly between honest\nand deceptive behaviour.\nThe challenge of this problem is that it is difficult to model the\nintention recognition of the observer. For example, is the observer\nna√Øve, in that they do not believe that they are being deceived? Or\nare they aware that they are being deceived? Or somewhere in the\nmiddle? If they have some awareness, what model of deception\nare they using in their own intention recognition model. For this\nreason, the straightforward model of just solving for Equation 1 is\nonly optimal if our model of intention recognition is the same as\nthe observers, which is unlikely.\nIn this paper, we present two solutions to this problem that do\nnot have an explicit model of an observer: an ambiguity model\nand an irrationality model. Instead, the two models use only the\ninformation available to them by their given policy. We assume pre-\ntrained Q-functions for all reward functions in ùëÖùëõ; or alternatively,\npre-trained stochastic policies, but we only use Q-functions for the\nremainder of the paper. We use ùëÑùëüùëñto represent one trained on\nbogus reward function ùëüùëñ.\n3.2\nAmbiguity Model\nIn this model, an agent behaves ambiguously by selecting actions\nthat have high Q-values not only for the real reward function but\nalso for multiple bogus reward functions. As the trajectory pro-\ngresses, fewer reward functions remain sensible, so these are pruned\nfrom consideration. Eventually, the policy selects actions only opti-\nmal for the true reward function. The final point before this occurs\nconforms to Masters and Sardina‚Äôs last deceptive point [20].\nThe main idea is for our policy to generate sequences of actions\nthat have positive reward for several reward functions, including\nthe true reward function. For this, we need a measure of how far a\nsequence of observed behaviour diverges from optimal behaviour.\nObservations ¬Æùëúconsist of a sequence of tuples (ùë†,ùëé). We measure\nhow far observations diverge from the optimal solution by summing\nthe Q-differences:\nŒîùëüùëñ(¬Æùëú) =\n‚àëÔ∏Å\n(ùë†,ùëé) ‚àà¬Æùëú\n\u0012\nùëÑùëüùëñ(ùë†,ùëé) ‚àímax\nùëé‚Ä≤‚ààùê¥ùëÑùëüùëñ(ùë†,ùëé‚Ä≤)\n\u0013\n(3)\nThis formula is based on the definition by Ramirez and Geffner\n[25]. If ¬Æùëúfollows a sequence that is optimal for reward function\nùëüùëñ, then Œîùëüùëñ(¬Æùëú) = 0, and any sub-optimal behaviour has a negative\ndivergence. Other definitions are possible, such as using cost ratio\nlike Vered et al. [36]; what matters is that Œî allows us to compare\nbehaviour with respect to optimality.\nThe probability that reward function ùëüùëñis the true reward func-\ntion ùëü, from the perspective of an observer, is defined using a Boltz-\nmann distribution:\nùëÉ(ùëüùëñ| ¬Æùëú) =\nùëíùë•ùëù{Œîùëüùëñ(¬Æùëú)}\n√ç\nùëüùëó‚ààR ùëíùë•ùëù{Œîùëüùëó(¬Æùëú)} ¬∑ ùëÉ(ùëüùëñ),\n(4)\nin which ùëÉ(ùëüùëñ) is the prior probability that ùëüùëñis the true reward\nfunction, which can be uniform over R if this is unknown. If ¬Æùëúis\nfar from optimal for ùëüùëñcompared to other reward functions in R,\nits probability will be lower relative to the other reward functions.\nThis gives us a probability distribution over all reward functions in\nR. As with Œîùëüùëñ(¬Æùëú), other models could be used to define this, but\nwe use what is common in intention recognition models.\nOur model uses this probability distribution to minimise infor-\nmation gain by the observer using Shannon entropy2 [31] each\ntime an action is chosen.\nWe define the Q-gain of action ùëéfor reward function ùëüùëñas:\nùê∫ùëüùëñ(ùë†,ùëé) = ùëÑùëüùëñ(ùë†,ùëé) ‚àíùëÖùëüùëñ(¬Æùëú)\nin which ùëÖùëüùëñ(¬Æùëú) = ùëÑùëüùëñ(ùë†‚Ä≤,ùëé‚Ä≤) ‚àíùëÑùëüùëñ(ùë†0,ùëé0) is the residual expected\nreward received so far in sequence ¬Æùëúwhere (ùë†‚Ä≤,ùëé‚Ä≤) is the last pair\nin ¬Æùëúand (ùë†0,ùëé0) is the first pair in ¬ÆùëÇ. Thus, ùëÖùëüùëñ(¬Æùëú) represents the\nvalue of having arrived at state ùë†minus the reward of executing ùëé,\nwhile ùê∫ùëüùëñ(ùë†,ùëé) represents the gain that action ùëégives compared to\n‚Äòremaining‚Äô in state ùë†. Intuitively, ùê∫ùëüùëñ(ùë†,ùëé) < 0 implies that action\nùëéis moving ‚Äòaway‚Äô from the rewards given by ùëüùëñ, and ùê∫ùëüùëñ(ùë†,ùëé) > 0\nis moving ‚Äòtowards‚Äô the rewards.\nGiven a sequence of observations ¬Æùëú, our model chooses the action\nthat minimises the information gain for the observer:\nùúãùê∑(¬Æùëú,ùë†) = arg min\nùëé‚ààùê¥(ùë†)\n‚àíùúÖ\n‚àëÔ∏Å\nùëüùëñ‚ààR\nùëÉ(ùëüùëñ| ¬Æùëú¬∑ ùëé) √ó ùëôùëúùëî2(ùëÉ(ùëüùëñ| ¬Æùëú¬∑ ùëé))\n(5)\nin which ùê¥+(ùë†) is the set of actions with non-negative Q-gain for\nthe real reward function ùëü, and ùúÖis a normalising term. Thus, an\nagent following policy ùúãùê∑will move ambiguously between all of\nthe Q-functions to maximise entropy. Only evaluating actions in\nùê¥+(ùë†) ensure that progress is made towards the real goal.\nHowever, sometimes a particular reward can become so irrational\nthat it would be clear to an observer that this is no longer likely.\nWe exclude such reward functions from the entropy calculation by\nre-evaluating the bogus reward functions at each step of the plan,\nand excluding those would be irrational (negative Q-gain). This is\nsimilar to the pruning heuristic from Vered and Kaminka [35].\nA reward function is pruned from the entropy calculation (set R\nin Equation 5) if ùê∫ùëüùëñ(ùë†,ùëé) < ùõø, in which ùõøis a pruning parameter.\nIf ùõø= 0, a reward function is pruned because it offers no gain over\nthe current state. If ùõø< 0, the pruning would be less aggressive,\nallowing some actions that offer no gain. If ùõø= ‚àí‚àû, nothing would\nbe pruned. At each step, all reward functions are considered for all\nactions, so a pruned reward function can be re-considered later. This\nmay have the negative effect that all but the true one are pruned.\nIn implementation, a minimum number of policies can be specified.\nFigure 2 illustrates a path planning problem in which the agent\nmust navigate from the green start point to the orange destination.\nThe bogus destinations are red. In Figure 2a, the agent minimises\ninformation gain for all goals without pruning. It is difficult to see,\nbut the thicker line in Figure 2a compared to Figure 2b is the agent\nzigzagging repeatedly left-to-right. In Figure 2b with pruning, at\nthe first turn, labelled (a), the destination at the top left is pruned,\nwhile at turn (b), the destination on the right is pruned, and turn\n(c) prunes the destination at the bottom left. This delivers a shorter\npath than in Figure 2a because it avoids zigzagging behaviour from\ntrying to maximise the entropy of all destinations.\n2Shannon entropy measures information gain. Increasing uncertainty lowers informa-\ntion gain and increases entropy.\n(a) Without pruning\n(b) With pruning\nFigure 2: Examples of the ambiguity model. The agent nav-\nigates from the green starting point to the real destination\n(orange & marked), using bogus destinations (red).\n3.3\nIrrationality Model\nThe irrationality model is based on Masters and Sardina [21]. The\ndeceptive Q-value of an action is a weighted sum of its optimal\nQ-value and a irrationality measure. The higher the weight on the\noptimal Q-value, the less deceptive the behaviour.\nFirst, we define the irrationality measure for an observation se-\nquence, which is dependent on the history of a sequence of actions,\nrather than a single action. This is because an action may appear\nrational in a one state, but not in the context of a longer sequence.\nDefinition 4 (Irrationality Measure). For an observed sequence of\nstate-action pairs ¬Æùëú, the irrationality measure of ¬Æùëúwith respect to\nreward function ùëüùëñis:\nùêºùëÄ(¬Æùëú) = 1 ‚àímax\nùëüùëñ‚ààR Œîùëüùëñ(¬Æùëú)\n(6)\nin which Œîùëüùëñis a divergence function (Equation 3). This definition\nis similar to the definition of rationality for path planning outlined\nby Masters and Sardina [21].\nUnder this definition, a sequence ¬Æùëúthat has a low value for\nall reward functions has a high ùêºùëÄ‚Äî it is irrational not to make\nprogress towards at least one goal. We take the minimum of all\nreward functions: if the sequence is rational for any of the possible\nreward functions, then it is deemed rational by an observer who\ndoes not know the true reward function.\nThe goal of the agent is to maximise its expected reward as well\nas its irrationality. We use a parameter ùõº(0 ‚â§ùõº‚â§1) as the weight\nto define the importance of the Q-value versus the irrationality. The\ndeceptive policy ùúãùê∑is defined as the weighed sum of the optimal\nQ-value and the irrationality measure:\nùúãùê∑(¬Æùëú,ùë†) = arg max\nùëé‚ààùê¥\n(1‚àíùõº) ùëÑ‚Ä≤\nùëü(ùë†,ùëé)\n|   {z   }\nOptimal\n+ùõºùêºùëÄ(¬Æùëú¬∑ (ùë†,ùëé))\n|          {z          }\nIrrational\n(7)\nin whichùëÑ‚Ä≤ùëü(ùë†,ùëé) isùëÑùëü(ùë†,ùëé) normalised against other actionsùëé‚Ä≤ ‚ààùê¥\nto range [0,1]. The higher ùõº, the lower the weight given to the Q-\nvalue and the more irrational the behaviour.\nFigure 3 illustrates the irrationality model in a path planning\nsetting. When ùõº= 0, we get honest behaviour. As ùõºincreases,\nrationality decreases. For ùõº= 0.15 and 0.3, the agent moves away\nfrom both destinations. For ùõº= 0.3, the blue block in the bottom\ncorner reflects the agent‚Äôs excessive irrationality. As the agent\nmoves towards its true destination, its behaviour becomes more\nrational, capturing a similar idea to the last deception point in\nMasters and Sardina [20]: it becomes more difficult to deceive as\none ‚Äòapproaches‚Äô a goal.\n4\nCOMPUTATIONAL EVALUATION\nIn this section, we present a computational evaluation our two\nmodels from Section 3. The goals of the experiment are to quantify\nhow deceptive our models are compared to an honest baseline, and\nto determine the ‚Äòcost‚Äô of deception based on how far the deceptive\nbehaviour deviates from the honest optimal model.\n4.1\nExperiment Design\nWe implemented the ambiguity and irrationality models defined\nin Section 3 and applied in two domains: the P4 path planning\nframework3 and the Berkeley Pacman framework 4.\nIndependent variables We evaluated four different agents in\nour experiments:\n(1) an ‚Äòhonest‚Äô agent, which uses the optimal policy for the real\nreward function;\n(2) one deceptive agent that uses the ambiguity models; and\n(3-4) two deceptive agents using the irrationality model, with\nùõº= 0.3 and ùõº= 0.5 respectively.\nReward functions gave a reward of 10,000 for reaching the desti-\nnation state. Negative rewards model actions costs, with ‚àí1 for an\naction up, down, left, or right, and ‚àí\n‚àö\n2 for any diagonal move. For\nthe ambiguity model, we set pruning parameter ùõø= 0. Q-functions\nwere implemented as Q-tables for P4 and were learnt using value\niteration [34] until convergence, and implemented using linear func-\ntion approximation for Pacman and were learnt using Q-learning.\nMeasures We measured: (1) the total path cost, which is the\ninverse of the discounted reward; (2) the probability assigned to the\ntrue reward function, calculated using a na√Øve intention recognition\nalgorithm [19, 36]; and (3) the simulation value of the paths from\nMasters and Sardina [20]:\nùë†ùëñùëöùë¢ùëôùëéùë°ùëñùëúùëõ( ¬Æùëúùë°, R) =\n√çùë°\nùëó=1 ¬∑ maxùëüùëñ‚ààR\\{ùëü} ùëÉ(ùëüùëñ| ¬Æùëúùë°) ‚àíùëÉ(ùëü| ¬Æùëúùë°)\nùë°\nThis definition calculates, for each state-action pair, the deceptive-\nness of the step by using the observer‚Äôs intention recognition model\nand taking the difference between the most likely reward function\nand the actual reward function. The deceptiveness ùê∑( ¬Æùëúùë°, R) is then\nthe average deceptiveness over the observed trace\nThis intention recognition algorithm used for measures 2 and 3\nassigns the probability of each destination as being the real desti-\nnation by comparing how rational the observed behaviour is for\neach destination, where rationality is assigned based on how close\nthe observed behaviour is to the optimal behaviour.\nExperiment parameters We used five different layouts for\neach domain, varying in size and structure. For example, for P4 we\nvaried number and density of obstacles as follows:\n(1) 49 √ó 49 with no obstacles, such as in Figure 2;\n(2) 49 √ó 49 with some large obstacles, such as in Figure 3;\n(3) 49 √ó 49 map with random and high density obstacles;\n3See https://bitbucket.org/ssardina-research/p4-simulator/\n4See http://ai.berkeley.edu/\n(a) ùõº= 0\n(b) ùõº= 0.15\n(c) ùõº= 0.3\nFigure 3: Examples of the irrationality model in path planning. The agent navigates from the green starting point to the real\ndestination (orange & marked), using bogus destinations (red).\n(4) 100 √ó 100 with ‚Äòarchipelago‚Äô (a small number of large island\nobstacles);\n(5) 100 √ó 100 with many rooms and corridors.\nFor each layout type, we defined eight different variations by chang-\ning the number of goals (three or five in P4), distribution of rewards,\nand the position of the real reward, leading to a total of 40 layouts.\nFor the Pacman domain, we used 10 maps from the Berkley frame-\nwork. Each model was applied to all 50 maps for each domain. For\neach path generated, the intention recognition measure was taken\nat nine ‚Äòcheckpoints‚Äô: every 10% along the path.\n4.2\nResults\nFigure 4 plots the average probability of the real ‚Äògoal‚Äô at each point\nfor both domains. We separate the results into three-goal maps in\nP4, five-goal maps in P4, and the Pacman maps. In all cases, it is\neasier to identify the real reward function as more plan steps are\nrevealed, however, there is a clear trend that the deceptive models\nmake it more difficult. The irrationality models are more deceptive\nbut this must be considered alongside path costs.\nFigure 5 demonstrates the simulation measurements for different\nscenarios. The simulation level of the honest model is the lowest\namong all models in most of the time, which consists with the\nprobability results. This is perhaps the most interesting measure in\nour experiment, as it measures true ‚Äòdeceptiveness‚Äô.\nOn the simulation data, we performed a Kolmogorov-Smirnov\ntest of normality to confirm that our data matches the character-\nistics of a normal distribution. We then performed paired t-tests\nfor independent samples between the honest model and the three\ndeceptive models. The honest model showed the lowest level of sim-\nulation (ùëÄ=-0.24, ùëÜùê∑=0.17). In comparison, the ambiguity model\n(ùëÄ=-0.16, ùëÜùê∑=0.18) was more deceptive than the honest model\nùë°(98)=2.05, ùëù=.04. Similarly, the IR_0.3 model (ùëÄ=-0.13,ùëÜùê∑=0.24)\nwas more deceptive than the honest model ùë°(98)=2.64, p=.009, as\nwas the IR_0.6 model (ùëÄ=-0.009,ùëÜùê∑=0.21), ùë°(98)=6.06, ùëù< .001.\nFigure 6 shows the path costs as a proportion of the length of op-\ntimal (honest) path. The ambiguity model arrives at the destination\nwith fewer actions than either irrationality model. This is important\nbecause in addition to being deceptive, the objective of deceptive\nreinforcement learning is to maximise discounted expected rewards.\nIn some cases, irrationality model with ùõº=0.5 was more than four\ntimes as long. If we give higher priority to the expected reward for\nthe real reward function, we may prefer the ambiguity model or to\nuse the irrationality model with a lower value of ùõº. In some cases, if\ndeception is weighted low enough, the honest model would still be\npreferred because of the short paths, which results in higher actual\nreward for discount factor ùõæ< 1.\nAnalysis looking into individual maps, we see that the IR models\ngenerate longer paths due to the randomness in the paths. For the\nambiguity model, the paths are slightly longer for the five-goal\nmaps, because there are more bogus goals that ‚Äòpull‚Äô the agent\naway from the optimal honest path. This indicates that, in some\ndomains in which the real reward is strongly weighted, even if there\nare many possible bogus reward functions, it may still be more\nsuitable to select only a subset of the bogus goals for the entropy\ncalculations. Further results on individual maps are available in the\nsupplementary material.\nOverall, we see that it is easier to deceive in the Pacman game\nthan in path planning, which we attribute to the fact that there\nis just a single reward at the destination in path planning, and\neventually we end up with fewer and fewer goals until finally the\nonly likely goal from the observer‚Äôs view is the real goal.\n5\nHUMAN BEHAVIOURAL EVALUATION\nIn this section, we describe a human behavioural experiment to\nmeasure the ability to deceive people, rather than algorithms. Par-\nticipants were aware that they could be deceived, unlike the ‚Äòna√Øve‚Äô\nintention recognition model. There is only one intention recogni-\ntion model that detects deception for sequential decision-making:\nthe irrationality model by Masters and Sardina [21]. However, as\nour irrationality model uses this concept to generate behaviour, the\nMasters and Sardina model is not valid for us.\n5.1\nExperiment Design\nThe experiment design was similar to that used for the computa-\ntional evaluation, with three exceptions: (1) instead of the intention\nrecognition algorithm, we ask human participants to estimate the\ngoal distribution; (2) the human participants were provided with\nonly a random selection of the maps and methods; and (3) we as-\nsessed based only on the path-planning problem.\nOur experiment used 40 maps √ó four possible models producing\nbehaviour = 160 map-path pairs. We generated checkpoints at 25%,\nFigure 4: Intention recognition for P4 with three goals (left), P4 with 5 goals (middle) and Pacman (right)\nFigure 5: Intention recognition for P4 with three goals (left), P4 with 5 goals (middle) and Pacman (right)\nFigure 6: Path costs, proportional to honest path for P4 with three goals (left), P4 with 5 goals (middle) and Pacman (right)\n50%, and 75%, leading to 480 stimuli in total. Each participant re-\nceived 38 randomly-selected stimuli. Participants were ‚Äòaware‚Äô; that\nis, they were explicitly told that the agent may try to hide its true\ndestination, and that they should try to guess the true destination.\nWe recruited 69 participants via Amazon Mechanical Turk, a crowd-\nsourcing platform often used for human-subject experiments [7].\nParticipants were paid US$4 for completing all tasks, which took\non average 11.5 minutes. Participants were aged 20-55 (ùúá= 32). 15\nparticipants were female, 54 were male, and none chose to specify\ntheir gender manually.\n5.2\nResults\nFigures 7 and 8 summarise the results for the human subject eval-\nuation. We see similar outcomes to that of the na√Øve intention\nrecognition algorithm, except that human subjects were overall less\naccurate than the na√Øve model, even for honest behaviour. This is\nunderstandable as the optimal behaviour is straightforward for an\nalgorithm to calculate, but less so for a human. At the first check-\npoint, by which point participants have seen 25% of the path, the\naccuracy is close to random.\nFor the ambiguity model with three goals, participants were\nmore accurate than for the honest model at 75% density, but this\nis mostly accounted for by noisy data ‚Äì the difference is less than\n3%. The deceptive models were more effective at deceiving in the\nfive-goal model than the three-goal model, which is unsurprising\nas there are more bogus goals to use.\nAn interesting point is the effect of the participants being aware\nthat they are being deceived, which is not the case for the earlier\ncomputational experiments in which the observer model is na√Øve.\nIn the computational experiments, the honest model is never con-\nsidered deceptive. The simulation value is at most 0, meaning that\nthe real destination is judged to be as likely as others. However, in\nthe human subject experiments, the honest model is, on average,\nconsidered to be deceptive early in the experiment, presumably\n(a) Three goals\n(b) Five goals\nFigure 7: Average of experiment participant‚Äôs prediction of the true destination for the two scenarios\n(a) Three goals\n(b) Five goals\nFigure 8: Average simulation based on the experiment participants‚Äô prediction for the two scenarios\nbecause the participants were assuming that the model was using\ndeception as simulation (showing the false). Also interesting is that\nthe deceptive models were considered deceptive right up until the\n75% mark and presumably beyond. In the computational experi-\nments, the simulation value was, on average, below 0 at the 75%\nmark for all deceptive models. This is perhaps due to the fact that\nthe human participants are unable to make accurate judgements\nas quickly as the intention recognition algorithm. As such, results\nmay differ if we had a non-na√Øve intention recognition model.\nOverall, we see that our models deceived participants for the\npath planning task, but the effectiveness may not be sufficient if\nthe length of the plan is considered too high. This largely depends\non the weight ùúîin Equation 2.\n5.3\nLimitations\nThere are several limitations with our study. First, while path plan-\nning is a good application for human behavioural experiments\n(people are good at spatial reasoning), it is only one domain, so\nfurther experimentation on different types of domains is necessary.\nSecond, the na√Øve intention recognition model we used to evaluate\ndeception in the computational evaluation is not as sophisticated as\nour model of deception ‚Äì it does not mitigate for the fact that it is\nbeing deceived. This is difficult to mitigate because we need a level\nof separation between the methods and the evaluation metrics, and\nthe only suitable model of which we are aware is the irrationality\nmodel [21], on which our model is based. Third, there was only\nminimal incentive for our experimental participants, which is not\nreflective of some applications where failing to identify deception\ncan have devastating outcomes.\n6\nDISCUSSION AND FUTURE WORK\nIn this paper, we presented two models for preserving the privacy\nof reward functions in reinforcement learning. Through compu-\ntational and human evaluation in a path planning task, we have\nshown that the models can deceive both na√Øve intention recogni-\ntion algorithms and human subjects. However, often the length of\nplans is significantly higher, meaning that for domains in which\ndeception is weighted only lightly, an honest agent may be more\nsuitable. Clearly, this judgement depends on the domain and the\nmeasure of deception used.\nIn future work, we will apply this model to more tasks, and we\nwill investigate this model in policy-based reinforcement learning,\nin which we do not have Q-functions, but learn a policy directly.\nFurther, we aim to extend these models to models similar to that\nin Ornik and Topcu [23], in which the observer has only partial\nobservability of the agent and the environment.\nAcknowledgements This paper was supported by ARC Grant\nDP180101215 A Computational Theory of Strategic Deception. Ethics\napproval was obtained from The University of Melbourne Human\nEthics Committee; ethics approval number 1954358.1.\nREFERENCES\n[1] Ronald C. Arkin, Patrick Ulam, and Alan R. Wagner. 2012.\nMoral decision\nmaking in autonomous systems: enforcement, moral emotions, dignity, trust, and\ndeception. In Proceedings of the Institute of Electrical and Electronics Engineers\n(IEEE). 571‚Äì589.\n[2] Dorit Avrahami-Zilberbrand and Gal A Kaminka. 2014. Keyhole adversarial\nplan recognition for recognition of suspicious and anomalous behavior. In AAAI\nWorkshop on Plan, Activity, and Intent Recognition. 87‚Äì121.\n[3] Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. 2009. Action understand-\ning as inverse planning. Cognition 113, 3 (2009), 329‚Äì349.\n[4] Bikramjit Banerjee and Jing Peng. 2003. Countering deception in multiagent\nreinforcement learning. In Proceedings of the Workshop on Trust, Privacy, Deception\nand Fraud in Agent Societies at AAMAS-03, Melbourne, Australia. 1‚Äì5.\n[5] J. Bowyer Bell. 2003. Toward a theory of deception. International Journal of\nIntelligence and Counterintelligence 16, 2 (2003), 244‚Äì279.\n[6] J Bowyer Bell and Barton Whaley. 1982. Cheating: Deception in War & Magic,\nGames & Sports. St Martin‚Äôs Press.\n[7] Michael D Buhrmester, Sanaz Talaifar, and Samuel D Gosling. 2018. An evaluation\nof Amazon‚Äôs Mechanical Turk, its rapid rise, and its effective use. Perspectives on\nPsychological Science 13, 2 (2018), 149‚Äì154.\n[8] Thomas L Carson. 2010. Lying and deception: Theory and practice. Oxford\nUniversity Press.\n[9] Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David E Smith,\nand Subbarao Kambhampati. 2019. Explicability? legibility? predictability? trans-\nparency? privacy? security? The emerging landscape of interpretable agent\nbehavior. In ICAPS, Vol. 29. 86‚Äì96.\n[10] Anca D Dragan, Rachel M Holladay, and Siddhartha S Srinivasa. 2014. An Analysis\nof Deceptive Robot Motion.. In Robotics: science and systems. 10.\n[11] David Ettinger and Philippe Jehiel. 2010. A theory of deception. American\nEconomic Journal: Microeconomics 2, 1 (2010), 1‚Äì20.\n[12] Yunhan Huang and Quanyan Zhu. 2019. Deceptive reinforcement learning under\nadversarial manipulations on cost signals. In International Conference on Decision\nand Game Theory for Security. Springer, 217‚Äì237.\n[13] Mustafa O Karabag, Melkior Ornik, and Ufuk Topcu. 2019. Optimal Deceptive\nand Reference Policies for Supervisory Control. In 2019 IEEE 58th Conference on\nDecision and Control (CDC). IEEE, 1323‚Äì1330.\n[14] Sarah Keren, Avigdor Gal, and Erez Karpas. 2016. Privacy Preserving Plans in\nPartially Observable Environments. In Proceedings of IJCAI‚Äô16. 3170‚Äì3176.\n[15] Anagha Kulkarni, Matthew Klenk, Shantanu Rane, and Hamed Soroush. 2018. Re-\nsource Bounded Secure Goal Obfuscation. In AAAI Fall Symposium on Integrating\nPlanning, Diagnosis and Causal Reasoning.\n[16] Anagha Kulkarni, Siddharth Srivastava, and Subbarao Kambhampati. 2018. A\nunified framework for planning in adversarial and cooperative environments. In\nICAPS Workshop on Planning and Robotics.\n[17] Chunmao Li, Xuanguang Wei, Yinliang Zhao, and Xupeng Geng. 2020. An\nEffective Maximum Entropy Exploration Approach for Deceptive Game in Rein-\nforcement Learning. Neurocomputing (2020).\n[18] Pingchuan Ma, Zhiqiang Wang, Le Zhang, Ruming Wang, Xiaoxiang Zou, and\nTao Yang. 2019. Differentially Private Reinforcement Learning. In International\nConference on Information and Communications Security. Springer, 668‚Äì683.\n[19] Peta Masters and Sebastian Sardina. 2017. Cost-based goal recognition for path-\nplanning. In AAMAS. IFAAMAS, 750‚Äì758.\n[20] Peta Masters and Sebastian Sardina. 2017. Deceptive Path-Planning. In Proceed-\nings of IJCAI‚Äô17. 4368‚Äì4375.\n[21] Peta Masters and Sebastian Sardina. 2019. Goal recognition for rational and\nirrational agents. In Proceedings of AAMAS‚Äô19. IFAAMAS, 440‚Äì448.\n[22] Andrew Y Ng and Stuart J Russell. 2000. Algorithms for inverse reinforcement\nlearning.. In ICML, Vol. 1. 2.\n[23] Melkior Ornik and Ufuk Topcu. 2018. Deception in optimal control. In 2018 56th\nAnnual Allerton Conference on Communication, Control, and Computing (Allerton).\nIEEE, 821‚Äì828.\n[24] Martin L Puterman. 2014. Markov decision processes: discrete stochastic dynamic\nprogramming. John Wiley & Sons.\n[25] Miquel Ramirez and Hector Geffner. 2010. Probabilistic plan recognition using\noff-the-shelf classical planners. In Proceedings of AAAI‚Äô10. 1121‚Äì1126.\n[26] Miquel Ramirez, Michael Papasimeon, Nir Lipovetzky, Lyndon Benke, Tim Miller,\nAdrian R Pearce, Enrico Scala, and Mohammad Zamani. 2018. Integrated hybrid\nplanning and programmed control for real time UAV maneuvering. In Proceedings\nof the 17th International Conference on Autonomous Agents and MultiAgent Systems.\nInternational Foundation for Autonomous Agents and Multiagent Systems, 1318‚Äì\n1326.\n[27] Neil C Rowe. 2003. Counterplanning deceptions to foil cyber-attack plans. In\nIEEE Systems, Man and Cybernetics SocietyInformation Assurance Workshop, 2003.\nIEEE, 203‚Äì210.\n[28] Jun Sakuma, Shigenobu Kobayashi, and Rebecca N Wright. 2008.\nPrivacy-\npreserving reinforcement learning. In Proceedings of the 25th international con-\nference on Machine learning. 864‚Äì871.\n[29] Eugene Santos Jr, Deqing Li, and Xiuqing Yuan. 2008. On deception detection\nin multi-agent systems and deception intent. In Modeling and Simulation for\nMilitary Operations III, Vol. 6965. International Society for Optics and Photonics,\n696502.\n[30] ≈ûtefan Sarkadi, Alison R Panisson, Rafael H Bordini, Peter McBurney, Simon\nParsons, and Martin Chapman. 2019. Modelling deception using theory of mind\nin multi-agent systems. AI Communications 32, 4 (2019), 287‚Äì302.\n[31] Claude Elwood Shannon. 1948. A mathematical theory of communication. Bell\nsystem technical journal 27, 3 (1948), 379‚Äì423.\n[32] Jaeeun Shim and Ronald C. Arkin. 2013. A Taxonomy of Robot Deception and its\nBenefits in HRI. In IEEE International Conference on Systems, Man, and Cybernetics\n(SMC). 2328‚Äì2335.\n[33] Wally Smith, Frank Dignum, and Liz Sonenberg. 2016. The construction of\nimpossibility: a logic-based analysis of conjuring tricks. Frontiers in psychology 7\n(2016), 748.\n[34] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\nduction. MIT press.\n[35] Mor Vered and Gal A Kaminka. 2017. Heuristic online goal recognition in\ncontinuous domains. In IJCAI. AAAI Press, 4447‚Äì4454.\n[36] Mor Vered, Gal A. Kaminka, and Sivan Biham. 2016. Online goal recognition\nthrough mirroring: Humans and agents. In Conference on Advances in Cognitive\nSystems.\n[37] Giuseppe Vietri, Borja Balle, Akshay Krishnamurthy, and Zhiwei Steven Wu.\n2020. Private Reinforcement Learning with PAC and Regret Guarantees. arXiv\npreprint arXiv:2009.09052 (2020).\n[38] Alan R Wagner and Ronald C Arkin. 2011. Acting deceptively: Providing robots\nwith the capacity for deception. International Journal of Social Robotics 3, 1 (2011),\n5‚Äì26.\n[39] Baoxiang Wang and Nidhi Hegde. 2019. Privacy-preserving q-learning with func-\ntional noise in continuous spaces. In Advances in Neural Information Processing\nSystems. 11327‚Äì11337.\n[40] Barton Whaley. 1982. Toward a general theory of deception. The Journal of\nStrategic Studies 5, 1 (1982), 178‚Äì192.\n[41] Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. 2008. Maxi-\nmum entropy inverse reinforcement learning. In Proceedings of AAAI‚Äô08, Vol. 3.\n1433‚Äì1438.\nA\nSUPPLEMENTARY MATERIAL:\nNON-DECEPTIVE ACTIONS\nThis figure shows the percentage of non-deceptive actions, as mea-\nsured by whether the observer is deceived after the execution of the\naction, for each of the 50 cases in the computational experiments.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA"
  ],
  "published": "2021-02-05",
  "updated": "2021-02-05"
}