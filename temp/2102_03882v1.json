{
  "id": "http://arxiv.org/abs/2102.03882v1",
  "title": "Spoiler Alert: Using Natural Language Processing to Detect Spoilers in Book Reviews",
  "authors": [
    "Allen Bao",
    "Marshall Ho",
    "Saarthak Sangamnerkar"
  ],
  "abstract": "This paper presents an NLP (Natural Language Processing) approach to\ndetecting spoilers in book reviews, using the University of California San\nDiego (UCSD) Goodreads Spoiler dataset. We explored the use of LSTM, BERT, and\nRoBERTa language models to perform spoiler detection at the sentence-level.\nThis was contrasted with a UCSD paper which performed the same task, but using\nhandcrafted features in its data preparation. Despite eschewing the use of\nhandcrafted features, our results from the LSTM model were able to slightly\nexceed the UCSD team's performance in spoiler detection.",
  "text": "Spoiler Alert: Using Natural Language Processing to\nDetect Spoilers in Book Reviews\nAllen Bao\nMScAC (Computer Science)\nUniversity of Toronto\nMarshall Ho\nMScAC (Computer Science)\nUniversity of Toronto\nSaarthak Sangamnerkar\nMScAC (Data Science)\nUniversity of Toronto\nAbstract\nThis paper presents an NLP (Natural Language Processing) approach to detecting\nspoilers in book reviews, using the University of California San Diego (UCSD)\nGoodreads Spoiler dataset. We explored the use of LSTM, BERT, and RoBERTa\nlanguage models to perform spoiler detection at the sentence-level. This was con-\ntrasted with a UCSD paper which performed the same task, but using handcrafted\nfeatures in its data preparation. Despite eschewing the use of handcrafted features,\nour results from the LSTM model were able to slightly exceed the UCSD team’s\nperformance in spoiler detection.\n1\nIntroduction\nIn this report, we will explore the task of spoiler detection using the UCSD Goodreads Spoiler dataset\n[1]. A spoiler is a piece of information in a movie or book review which reveals important plot\nelements, such as the ending or a major plot twist. Our models are designed to ﬂag spoiler sentences\nautomatically.\n1.1\nRelated Work\nThere is little existing work done on spoiler detection. SpoilerNet, a custom network described in\nFine-grained spoiler detection from large-scale review corpora from UCSD, is one of the leading\nmodels in spoiler detection [1]. SpoilerNet is a bi-directional attention based network which features\na word encoder at the input, a word attention layer and ﬁnally a sentence encoder. It achieved an\nimpressive AUC (area under the curve) score of 0.889, indicating a relatively high true positive:false\npositive ratio.\nWan et al. introduced a handcrafted feature: DF-IIF - Document Frequency, Inverse Item Frequency -\nto provide their model with a clue of how speciﬁc a word is. This would allow them to detect words\nthat reveal speciﬁc plot information.\n1.2\nNovelty\nOur models rely less on handcrafted features compared to the UCSD team. We make use of an LSTM\nmodel and two pre-trained language models, BERT and RoBERTa, and hypothesize that we can\nhave our models learn these handcrafted features themselves, relying primarily on the composition\nand structure of each individual sentence. Through these methods, our models could match, or even\nexceed the performance of the UCSD team.\n2\nData Exploration and Preprocessing\n2.1\nDataset\nThe UCSD Goodreads Spoilers dataset was created as part of the UCSD Book Graph project [2]. The\nUCSD team scraped more than 15M records with detailed reviews on books from goodreads.com.\narXiv:2102.03882v1  [cs.CL]  7 Feb 2021\nFigure 1: UCSD SpoilerNet\nThis dataset is very skewed - only about 3% of review sentences contain spoilers. It is also context-\nsensitive: spoiler ﬂagging depends heavily upon the book being reviewed. For example, “the main\ncharacter died” spoils “Harry Potter” far more than the Bible.\n2.2\nExploratory Analysis\nThe Goodreads Spoiler dataset has 7 columns: book_id, user_id, review_id, rating, has_spoiler,\nreview_sentences, and timestamp. The most important feature is “review_sentences”, a list of all\nsentences in a particular review. Each sentence in the list is preceded by its own “spoiler” ﬂag – “1”\nfor spoiler, “0” otherwise. Spoiler tags are self-reported by the review authors [1]. The dataset has\nabout 1.3 million reviews. We extracted a subset of 275,607 reviews randomly and performed an\nexploratory analysis. One ﬁnding was that spoiler sentences were typically longer in character count,\nperhaps due to containing more plot information, and that this could be an interpretable parameter by\nour NLP models.\nTable 1: Exploratory data from the Goodreads Spoiler dataset\nUnique users\n17405 (6.3%)\nReviews per user\n15.8 (mean), 8 (median)\nUnique books\n24873 (9.0%)\nReviews per book\n11.1 (mean), 5 (median)\nTotal sentences\n3,534,334\nNo-spoiler sentences\n3,419,580 (96.8%)\nLength of spoiler sentences (chars)\n88.1 (mean), 74.0 (median)\nLength of non-spoiler sentences (chars)\n81.4 (mean), 70.0 (median)\n2.3\nDataset preparation\nOur ﬁnal dataset consists of the review_sentences feature, ﬂattened out so that each sample would\ncontain one review sentence. Spoiler ﬂags were separated into a separate label array corresponding to\neach sentence. We also explored other related UCSD Goodreads datasets, and decided that including\neach book’s title as a second feature could help each model learn the more human-like behaviour,\nhaving some basic context for the book ahead of time. This was important as the original dataset’s\nonly information about the book itself was through a non-descriptive book_id feature. For each of\nour models, the ﬁnal size of the dataset used was approximately 270,000 samples in the training set,\nand 15,000 samples in the validation and test sets each (used for validating results).\n2\n3\nModels\nWe found both LSTM [3] and BERT [4] to be capable candidates due to their success with sentiment\nanalysis in part 1 of our projects. We also aspired to include a model newer than BERT, and decided\nto go with the better optimized RoBERTa [5], after considering XLNet [6] as well.\nThe predictive task for our models is to ﬂag each spoiler sentence individually. We will compare our\nwork with that of Wan et al. at UCSD. Since the spoiler data is heavily skewed, AUC was used as a\nmetric. The UCSD team achieved a score of 0.889 [1] and that will be used as our baseline.\n3.1\nModel 1: LSTM\nLong short-term memory (LSTM) is a recurrent neural network (RNN) which excels in processing\nsequences [3]. A LSTM cell passes on a hidden state over time steps and the input, output and forget\ngates regulate the ﬂow of information into and out of the cell [3]. The LSTM’s major shortcoming\nis its size and complexity, taking a substantial amount of time to run compared with other methods.\nHowever, the nature of the input sequences as appended text features in a sentence (sequence) makes\nLSTM an excellent choice for the task.\nEach text entry was put through the Keras Tokenizer function, which maps each unique vocabulary\nword to a numeric entry before vectorizing each sentence to become compatible with the LSTM model.\nHyperparameters for the model included the maximum review length (600 characters, with shorter\nreviews being padded to 600), total vocabulary size (8000 words), two LSTM layers containing\n32 units, a dropout layer to handle overﬁtting by inputting blank inputs at a rate of 0.4, and the\nAdam optimizer with a learning rate of 0.003. The loss used was binary cross-entropy for the binary\nclassiﬁcation task.\n3.2\nModel 2: BERT\nIn sentiment analysis tasks (seen in part 1 of our course project) pre-trained language models such as\nBERT outperformed LSTMs. The attention-based nature of BERT means entire sentences can be\ntrained simultaneously, instead of having to iterate through time-steps as in LSTMs. The bi-directional\nnature of BERT also adds to its learning ability, because the “context” of a word can now come from\nboth before and after an input word.\nWe chose to use Hugging Face’s “bert-base-cased” model, which has 12 layers, and 109M parameters,\nproducing 768-dimensional embeddings with a model size of over 400MB [7]. We fed the same\ninput – concatenated “book title” and “review sentence” – into BERT. We used a dropout layer and\nthen a single output neuron to perform binary classiﬁcation. We did not use sigmoid activation for\nthe output layer, as we chose to use BCEWithLogitsLoss as our loss function which is quicker and\nprovides more mathematical stability.\n3.3\nModel 3: RoBERTa\nWe also wanted to try out a third model, and researched several candidates. We chose RoBERTa, which\nis a better-optimized BERT model developed by Facebook’s AI team [7]. We used “roberta-base”\nfrom Hugging Face [8], which has 12 layers and 125 million parameters, producing 768-dimensional\nembeddings with a model size of about 500MB. The setup of this model is similar to that of BERT\nabove.\n4\nExperiments and Results\nWe trained our 3 networks with the ﬂattened dataset of 1 million sentences. Metrics for true/false\npositives/negatives are collected in order to calculate AUC. We typically ran 5 epochs per training\nsession. The ﬁrst versions of our models trained on the review sentences only (without book titles);\nthe results were quite far from the UCSD AUC score of 0.889. Follow-up trials were performed\nafter tuning hyperparameters such as batch size, learning rate, and number of epochs, but none of\nthese led to substantial changes. After studying supplementary datasets related to the UCSD Book\nGraph project (as described in section 2.3), another preprocessing data optimization method was\nfound. Including book titles in the dataset alongside the review sentence could provide each model\n3\nFigure 2: Initial vs ﬁnal model AUC results.\nwith additional context. For instance, the model could capture the presence of shared words between\neach title and review sentence (ex. The word “Harry” appearing in a spoiler-sentence, given the title\n“Harry Potter”). The highest AUC score we achieved was approximately 0.91, from the LSTM model.\nEach of our 3 team members maintained his own code base. We used a mix of development envi-\nronments (Google Colab, vscode) and platforms (TensorFlow, PyTorch). Our BERT and RoBERTa\nmodels have subpar performance, both having AUC close to 0.5. LSTM was much more promising,\nand so this became our model of choice.\n5\nDiscussion\nThe AUC score of our LSTM model exceeded the lower end result of the original UCSD paper.\nWhile we had been conﬁdent with our innovation of adding book titles to the input data, beating\nthe original work in such a short period of time exceeded any reasonable expectation we had. This\nimplies that spoiler detection does not require handcrafted features such as DF-IIF, and that text\nsequences are mostly sufﬁcient for predicting labels with a relatively abstract relationship with the\ntext. Supplemental context (titles) help boost this accuracy even further.\nOur LSTM performed substantially better than our BERT and RoBERTa models, for which the AUC\nscores placed solidly in the 0.50’s. We are hesitant in drawing bold conclusions, as we would have\nspent more time debugging various parts of the model and the metric collection code, given more time.\nOne possible argument is that BERT, from which RoBERTa is derived, is not pre-trained to handle a\nhighly skewed dataset (only 3% of sentences contained spoilers). A model trained speciﬁcally for\nsentiment analysis or text classiﬁcation on more balanced data, such as BERT, may not have been the\nperfect candidate for spotting the intricacies of this dataset.\nThe deﬁnition of a spoiler is inherently subjective much of the time - and since this dataset was\nhuman-labelled, it is certainly subject to mislabeling. Thankfully, the sheer number of samples likely\ndilutes this effect, but the extent to which this occurs is unknown.\n5.1\nFuture Work\nThe ﬁrst priority for the future is to get the performance of our BERT and RoBERTa models to an\nacceptable level. For the scope of this investigation, our efforts leaned towards the winning LSTM\nmodel, but we believe that the BERT models could perform well with proper adjustments as well. We\nmay also test the inclusion of more features such as user information into the input of our models.\nWe are also looking forward to sharing our ﬁndings with the UCSD team.\n4\nAttributions and Code\nEach member of our team contributed equally. Allen Bao created our LSTM model and created our\nﬁrst dataset. He also optimized his network to match and exceed UCSD performance. Marshall Ho\nstudied the problem of our low initial AUC scores and created the second dataset which added book\ntitles. He also developed our model based on RoBERTa. Saarthak Sangamnerkar developed our\nBERT model and the metric reporting framework in pytorch. He is also our LaTeX expert.\nCode repository: https://github.com/allenbao64/csc2515-project\nReferences\n[1] Mengting Wan, Rishabh Misra, Ndapa Nakashole, and Julian McAuley. Fine-Grained Spoiler\nDetection from Large-Scale Review Corpora. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages 2605–2610, Florence, Italy, July 2019.\nAssociation for Computational Linguistics.\n[2] UCSD. UCSD Book Graph, 2019.\n[3] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation,\n9(8):1735–1780, November 1997.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May\n2019. arXiv: 1810.04805.\n[5] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT\nPretraining Approach. arXiv:1907.11692 [cs], July 2019. arXiv: 1907.11692.\n[6] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le.\nXLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv:1906.08237\n[cs], January 2020. arXiv: 1906.08237.\n[7] huggingface bert. BERT — transformers 4.0.0 documentation, 2020.\n[8] huggingface roberta. RoBERTa — transformers 4.0.0 documentation, 2020.\n5\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-02-07",
  "updated": "2021-02-07"
}