{
  "id": "http://arxiv.org/abs/2102.04148v1",
  "title": "Deep Reinforcement Learning for the Control of Robotic Manipulation: A Focussed Mini-Review",
  "authors": [
    "Rongrong Liu",
    "Florent Nageotte",
    "Philippe Zanne",
    "Michel de Mathelin",
    "Birgitta Dresp-Langley"
  ],
  "abstract": "Deep learning has provided new ways of manipulating, processing and analyzing\ndata. It sometimes may achieve results comparable to, or surpassing human\nexpert performance, and has become a source of inspiration in the era of\nartificial intelligence. Another subfield of machine learning named\nreinforcement learning, tries to find an optimal behavior strategy through\ninteractions with the environment. Combining deep learning and reinforcement\nlearning permits resolving critical issues relative to the dimensionality and\nscalability of data in tasks with sparse reward signals, such as robotic\nmanipulation and control tasks, that neither method permits resolving when\napplied on its own. In this paper, we present recent significant progress of\ndeep reinforcement learning algorithms, which try to tackle the problems for\nthe application in the domain of robotic manipulation control, such as sample\nefficiency and generalization. Despite these continuous improvements,\ncurrently, the challenges of learning robust and versatile manipulation skills\nfor robots with deep reinforcement learning are still far from being resolved\nfor real world applications.",
  "text": " \n \nArticle \n \nDeep Reinforcement Learning for the Control of \nRobotic Manipulation: A Focussed Mini-Review \n \nRongrong Liu, Florent Nageotte, Philippe Zanne, Michel de Mathelin and Birgitta Dresp-Langley \n \nICube Lab Robotics Department Strasbourg University UMR 7357 CNRS, 67085 Strasbourg, France; \n \n \nICube Lab UMR 7357 Centre National de la Recherche Scientifique CNRS, 67085 Strasbourg, \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAbstract: Deep learning has provided new ways of manipulating, processing and analyzing data. It \nsometimes may achieve results comparable to, or surpassing human expert performance, and has \nbecome a source of inspiration in the era of artificial intelligence. Another subfield of machine learning \nnamed reinforcement learning, tries to find an optimal behavior strategy through interactions with the \nenvironment. Combining deep learning and reinforcement learning permits resolving critical issues \nrelative to the dimensionality and scalability of data in tasks with sparse reward signals, such as robotic \nmanipulation and control tasks, that neither method permits resolving when applied on its own. In this \npaper, we present recent significant progress of deep reinforcement learning algorithms, which try to \ntackle the problems for the application in the domain of robotic manipulation control, such as sample \nefficiency and generalization. Despite these continuous improvements, currently, the challenges of \nlearning robust and versatile manipulation skills for robots with deep reinforcement learning are still far \nfrom being resolved for real-world applications. \n \nKeywords: deep learning; artificial intelligence; machine learning; reinforcement learning; deep \nreinforcement learning; robotic manipulation control; sample efficiency; generalization \n \n \n \n \n1. Introduction \n \nRobots were originally designed to assist or replace humans by performing repetitive \nand/or dangerous tasks which humans usually prefer not to do, or are unable to do because \nof physical limitations imposed by extreme environments. Those include the limited \naccessibility of narrow, long pipes underground, anatomical locations in specific minimally \ninvasive surgery procedure, objects at the bottom of the sea, for example. With the \ncontinuous developments in mechanics, sensing technology [1], intelligent control and other \nmodern technologies, robots have improved autonomy capabilities and are more dexterous. \nNowadays, commercial and industrial robots are in widespread use with lower long-term cost \nand greater accuracy and reliability, in the fields like manufacturing, assembly, packing, \ntransport, surgery, earth and space exploration, etc. \n \nThere are different types of robots available, which can be grouped into several \ncate-gories depending on their movement, Degrees of Freedom (DoF), and function. \nArticulated robots, are among the most common robots used today. They look like a \nhuman arm and that is why they are also called robotic arm or manipulator arm [2]. In \nsome contexts, a robotic arm may also refer to a part of a more complex robot. A robotic \narm can be described as a chain of links that are moved by joints which are actuated by \nmotors. We will start from a brief explanation of these mechanical components of a \ntypical robotic manipulator [3,4]. Figure 1 shows the schematic diagram of a simple two-\njoint robotic arm mounted on a stationary base on the floor . \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 1. Simplified schematic diagram of mechanical components of a two-joint robotic arm. \n \nJoints are similar to joints in the human body, which provide relative motion \nbetween two parts of the body. In robotic field, each joint is a different axis and \nprovides an additional DoF of controlled relative motion between adjacent links, as \nshown in Figure 1. In nearly all cases, the number of degrees of freedom is equal to \nthe number of joints [5]. \nAn end-effector, as the name suggests, is an accessory device or tool which is \nattached at the end of the chain, and actually accomplishes the intended task. The \nsimplest end-effector is the gripper, which is capable of opening and closing for \ngripping objects, but it can also be designed as a screw driver, a brush, a water jet, \na thruster, or any mechanical device, according to different applications. An end-\neffector can also be called a robotic hand. \nLinks are the rigid or nearly rigid components that connect either the base, joints or \nend effector, and bear the load of the chain. \nAn actuator is a device that converts electrical, hydraulic, or pneumatic energy into \nrobot motion. \n \nCurrently, the control sequence of a robotic manipulator is mainly achieved by \nsolving inverse kinematic equations to move or position the end effector with respect to \nthe fixed frame of reference [6,7]. The information is stored in memory by a \nprogrammable logic controller for fixed robotic tasks [8]. Robots can be controlled in \nopen-loop or with an exteroceptive feedback. The open-loop control does not have \nexternal sensors or environment sensing capability, but heavily relies on highly \nstructured environments that are very sensitively calibrated. If a component is slightly \nshifted, the control system may have to stop and to be recalibrated. Under this strategy, \nthe robot arm performs by following a series of positions in memory, and moving to them \nat various times in their programming sequence. In some more advanced robotic \nsystems, exteroceptive feedback control is employed, through the use of monitoring \nsensors, force sensors, even vision or depth sensors, that continually monitor the robot’s \naxes or end-effector, and associated components for position and velocity. The feedback \nis then compared to information stored to update the actuator command so as to achieve \nthe desired robot behavior. Either auxiliary computers or embedded microprocessors are \nneeded to perform interface with these associated sensors and the required \ncomputational functions. These two traditional control scenarios are both heavily \ndependent on hardware-based solutions. For example, conveyor belts and shaker \ntables, are commonly used in order to physically constrain the situation. \nWith the advancements in modern technologies in artificial intelligence, such as deep \nlearning, and recent developments in robotics and mechanics, both the research and \nindustrial communities have been seeking more software based control solutions using low-\ncost sensors, which has less requirements for the operating environment and calibration. The \nkey is to make minimal but effective hardware choices and focus on robust algorithms and \nsoftware. \nInstead \nof \nhard-coding \ndirections \nto \ncoordinate \nall \nthe \njoints,\n3 of 12 \n \nthe control policy could be obtained by learning and then be updated accordingly. Deep \nReinforcement Learning (DRL) is among the most promising algorithms for this purpose \nbecause no predefined training dataset is required, which ideally suits robotic \nmanipulation and control tasks, as illustrated in Table 1. A reinforcement learning \napproach might use input from a robotic arm experiment, with different sequences of \nmovements, or input from simulation models. Either type of dynamically generated \nexperiential data can be collected, and used to train a Deep Neural Network (DNN) by \niteratively updating specific policy parameters of a control policy network. \n \nTable 1. Comparison between traditional control and DRL based control expectation. \n  \nTraditional Control \nDRL Based Control Expectation \ncontrol solution \nhardware based \nsoftware based \nmonitoring sensor \nexpensive \nlow-cost \nenvironment requirement \nstructured \nunstructured situations \nhardware calibration \nsensitive to calibration \ntolerate to calibration \ncontrol algorithm \nhand coding required \ndata driven \n \nThis review paper tries to provide a brief and self-contained review of DRL in the \nresearch of robotic manipulation control. We will start with a brief introduction of deep \nlearning, reinforcement learning, and DRL in the second section. The recent progress of \nrobotic manipulation control with the DRL based methods will be then discussed in the \nthird section. What need to mention here is that, we can not cover all the brilliant \nalgorithms in detail in a short paper. For the algorithms mentioned here, one still need to \nrefer to those original papers for the detailed information. Finally, we follow the \ndiscussion and present other real-world challenges of utilizing DRL in robotic \nmanipulation control in the forth section, with a conclusion of our work in the last section. \n \n2. Deep Reinforcement Learning \n \nIn this part, we will start from deep learning and reinforcement learning, to better \nillustrate their combination version, DRL. \n \n2.1. Deep Learning \n \nDeep learning is quite popular in the family of machine learning, with its outstanding \nperformance in a variety of domains, not only in classical computer vision tasks, but also \nin many other practical applications; to just name a few, natural language processing, \nsocial network filtering, machine translation, bioinformatics, material inspection and \nboard games, where these deep-learning based methods have produced results \ncomparable to, and in some cases surpassing human expert performance. Deep \nlearning has changed the way we process, analyze and manipulate data. \nThe adjective “deep” in deep learning comes from the use of multiple layers in the \nnetwork. Figure 2 demonstrates a simple deep learning architecture with basic fully \nconnected strategy. A general comparison is conducted in Table 2 between deep \nlearning and traditional machine learning. With deep learning, the raw data like images \nare directly fed into a deep neural network multiple layers that progressively extract \nhigher-level features, while with traditional machine learning, the relevant features of \ninput data are manually extracted by experts. Besides, deep learning often requires a \nlarge amount of data to reach optimal results, thus it is also computationally intensive. \n4 of 12 \n \nTable 2. Comparison between traditional machine learning and deep learning. \n  \nTraditional Machine Learning \nDeep Learning \ndataset requirement \nperforms well with small dataset \nrequires large dataset \naccuracy \naccuracy plateaus \nexcellent performance potential \nfeature extraction \nselected manually \nlearned automatically \nalgorithm structure \nsimple model \nmulti-layer model \nmodel training time \nquick to train a model \ncomputationally intensive \nhardware requirement \nworks with not powerful hardware \nhigh-performance computer \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 2. A simple deep learning architecture. \n \nDeep models can be interpreted as artificial neural networks with deep structures. The \nidea of artificial neural networks is not something new, which can date back to 1940s [9]. In \nthe following years, the research community witnessed many important milestones with \nperceptrons [10], backpropagation algorithm [11,12], Rectified Linear Unit, or ReLU [13], Max-\npooling [14], dropout [15], batch normalization [16], etc. It is all these continuous algorithmic \nimprovements, together with the emergence of large-scale training data and the fast \ndevelopment of high-performance parallel computing systems, such as Graphics Processing \nUnits (GPUs) that allow deep learning to prosper nowadays [17]. \n \nThe first great success for deep learning is based on a convolutional neural network \nfor classification in 2012 [18]. It applies hundreds of thousands data-label pairs \niteratively to train the parameters with loss computation and backpropagation. Although \nthis technique has been improved continuously and rapidly since it took off, and is now \none of the most popular deep learning structures, it is not quite suitable for robotic \nmanipulation control, as it is too time-consuming to obtain large number of images of \njoints angles with labeled data to train the model. Indeed, there are some researches \nusing convolutional neural network to learn the motor torques needed to control the \nrobot with raw RGB video images [19]. However, a more promising and interesting idea \nis using DRL, as we will discuss hereafter. \n \n2.2. Reinforcement Learning \n \nReinforcement learning [20] is a subfield of machine learning, concerned with how \nto find an optimal behavior strategy to maximize the outcome though trial and error \ndynamically and autonomously, which is quite similar with the intelligence of human and \nanimals, as the general definition of intelligence is the ability to perceive or infer \ninformation, and to retain it as knowledge to be applied towards adaptive behaviors in \nthe environment. This autonomous self-teaching methodology is actively studied in many \n5 of 12 \n \ndomains, like game theory, control theory, operations research, information theory, \nsystem optimization, recommendation system and statistics [21]. \nFigure 3 illustrates the universal model of reinforcement learning, which is biologi-\ncally plausible, as it is inspired by learning through punishment or reward due to state \nchanges in the environment, which are either favorable (reinforcing) to certain behav-\niors/actions, or unfavourable (suppressing). Natural reinforcement learning is driven by \nthe evolutionary pressure of optimal behavioral adaptation to environmental constraints. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 3. Universal model of reinforcement learning. \n \nWhen an agent is in a state, it chooses an action according to its current policy and then \nit receives a reward from the environment for executing that action. By learning from this \nreward, it transitions to a new state, chooses a new action and then iterates through this \nprocess. In order to make it even easier to understand, one can compare reinforcement \nlearning to the structure of how we play a video game, in which the character, namely the \nagent, engages in a series of trials, or actions, to obtain the highest score, which is reward. \n \nReinforcement learning is different from supervised learning, where a training set of \nlabeled examples is available. In interactive problems like robot control domain using \nreinforcement learning, it is often impractical to obtain examples of desired behavior that \nare both correct and representative of all the situations in which the agent has to act. \nInstead of labels, we get rewards which in general are weaker signals. Reinforcement \nlearning is not a kind of unsupervised learning, which is typically about finding structure \nhidden in collections of unlabeled data. In reinforcement learning, the agent has to learn \nto behave in the environment based only on those sparse and time-delayed rewards, \ninstead of trying to find hidden structure. Therefore, reinforcement learning can be \nconsidered as a third machine learning paradigm, alongside supervised learning and \nunsupervised learning and perhaps other future paradigms as well [22]. \n \n2.3. Deep Reinforcement Learning \n \nAs the name suggests, DRL emerges from reinforcement learning and deep \nlearning, and can be regarded as the bridge between conventional machine learning and \ntrue artificial intelligence, as illustrated in Figure 4. It combines both the technique of \ngiving rewards based on actions from reinforcement learning, and the idea of using a \nneural network for learning feature representations from deep learning. Traditional \nreinforcement learning is limited to domains with simple state representations, while DRL \nmakes it possible for agents to make decisions from high-dimensional and unstructured \ninput data [23] using neural networks to represent policies. In the past few years, \nresearch in DRL has been highly active with a significant amount of progress, along with \nthe rising interest in deep learning. \n6 of 12 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 4. Deep reinforcement learning. \n \nFDRL has gained a lot of attraction, especially due to its well-known achievement in \ngames. Beginning around 2013, DeepMind showed impressive learning results in Atari \nvideo games at human level proficiency with no hand coded features using unprocessed \npixels for input [24,25], which can be regarded as the creation of this subfield. Another \nmile-stone was in 2016 when AlphaGo [26] first beat a human professional player of Go, \nwhich is a game from ancient China. This computer program was improved by \nAlphaZero [27] in 2017, together with its efficiency also in chess and shogi. In 2019, \nPluribus [28] showed its success over top human professionals in multiplayer poker, and \nOpenAI Five [29] beat the previous world champions in a Dota 2 demonstration match. \n \nApart from the field of games, it has large potential in other domains, including but \nnot limited to, robotics [30], natural language processing [31], computer vision [32], \ntransportation [33], finance [34] and healthcare [35]. Many exciting breakthroughs of this \nresearch have been published by both of giant companies, which include Google Brain, \nDeepMind and Facebook, and top academic labs such as in Berkeley, Stanford and \nCarnegie Mellon University, together with some independent non-profit research \norganizations like openAI and some other industrially focused companies. \nThe most commonly used DRL algorithms can be categorized in value-based \nmethods, policy gradient methods and model-based methods. The value-based methods \nconstruct a value function for defining a policy, which is based on the Q-learning \nalgorithm [36] using the Bellman equation [37] and its variant, the fitted Q-learning \n[38,39]. The Deep Q-Network (DQN) algorithm used with great success in [25] is the \nrepresentative of this class, followed by various extensions, such as double DQN [40], \nDistributional DQN [41,42], etc. A combination of these improvements has been studied \nin [43] with a state-of-the-art performance on the Atari 2600 benchmark, both in terms of \ndata efficiency and final perfor-mance. \nHowever, the DQN-based approaches are limited to problems with discrete and low-\ndimensional action spaces, and deterministic policies, while policy gradient methods are \nable to work with continuous action spaces and can also represent stochastic policies. \nThanks to variants of stochastic gradient ascent with respect to the policy parameters, \npolicy gradient methods are developed to find a neural network parameterized policy to \nmaximize the expected cumulative reward [44]. Like other policy-based methods, policy \ngradient methods typically require an estimate of a value function for the current policy \nand a sample efficient approach is to use an actor-critic architecture that can work with \noff-policy data. The Deep Deterministic Policy Gradient (DDPG) algorithm [45,46] is a \nrepresentation of this type of methods. There are also some researchers working on \ncombining policy gradient methods with Q-learning [47]. \n \nBoth value-based and policy-based methods do not make use of any model of the \nenvironment and are also called model-free methods, which limits their sample efficiency. On \nthe contrary, in the model-based methods, a model of the environment is either explicitly \n7 of 12 \n \ngiven or learned from experience by the function approximators [48,49] in conjunction \nwith a planning algorithm. In order to obtain advantages from both sides, there are many \nresearches available integrating model-free and model-based elements [50–52], which \nare among the key areas for the future development of DRL algorithms [53]. \n \n3. Deep Reinforcement Learning in Robotic Manipulation Control \n \nIn this section, the recent progress of DRL in the domain of robotic manipulation control \nwill be discussed. Two of the most important challenges here concern sample efficiency and \ngeneralization. The goal of DRL in the context of robotic manipulation control is to train a \ndeep policy neural network, like in Figure 2, to detect the optimal sequence of commands for \naccomplishing the task. As illustrated in Figure 5, the input is the current state, which can \ninclude the angles of joints of the manipulator, position of the end effector, and their derivative \ninformation, like velocity and acceleration. Moreover, the current pose of target objects can \nalso be counted in the current state, together with the state of corresponding sensors if there \nare some equipped in the environment. The output of this policy network is an action \nindicating control commands to be implemented to \n \n \nthe \n \n \n \n \nAgent \n \n \n \nState \nAction \n• Position \nControl commands \n• velocity \n• acceleration \n• torques \n• target pose \nReward \n• velocity \n• sensors \ncommands \ninformation \n \n \n \nFigure 5. A schematic diagram of robotic manipulation control using DRL. \n \n3.1. Sample Efficiency \n \nAs we know, in supervised deep learning, a training set of input-output pairs are fed \nto neutral networks to construct an approximation that maps an input to an output \n[54,55]. This learned target function can then be used for labeling new examples when a \ntest set is given. The study of sample efficiency for supervised deep learning tries to \nanswer the question of how large a training set is required in order to learn a good \napproximation to the target concept [56]. Accordingly, for DRL in robotic manipulation \ncontrol, the study of sample efficiency discusses how much data need to be collected in \norder to build an optimal policy to accomplish the designed task. \n \nThe sample efficiency for DRL is considerably more challenging than that for su-\npervised learning for various reasons [57]. First, the agent can not receive a training set \nprovided by the environment unilaterally, but information which is determined by both the \nactions it takes and dynamics of the environment. Second, although the agent desires to \nmaximize the long-term reward, the agent can only observe the immediate reward. \nAdditionally, there is no clear boundary between training and test phases. The time the \n8 of 12 \n \nagent spends trying to improve the policy often comes at the expense of utilizing this \npolicy, which is often referred to as the exploration–exploitation trade-off [53]. \nSince gathering experiences by interacting with the environment for robots is \nrelatively expensive, a number of approaches have been proposed in the literature to \naddress sample efficient learning. For example, in [58], which is the first demonstration \nof using DRL on a robot, trajectory optimization techniques and policy search methods \nwith neural networks were applied to achieve reasonable sample efficient learning. A \nrange of dynamic manipulation behaviors were learned, such as stacking large Lego \nblocks at a range of locations, threading wooden rings onto a tight-fitting peg, screwing \ncaps onto different bottles, assembling a toy airplane by inserting the wheels into a slot. \nVideos \nand \nother \nsupplementary \nmaterials \ncan \nbe \nfound \nat \nhttp://rll.berkeley.edu/icra2015gps/index.htm. Millimeter level precision can be achieved \nwith dozens of examples by this algorithm, but the knowledge of the explicit state of the \nworld at training time is required to enable sample efficiency. \nIn [59], a novel technique called Hindsight Experience Replay (HER) was proposed. \nEach episode was replayed but with a different goal than the one the agent was trying to \nachieve. With this clever strategy for augmenting samples, the policy for the pick-and-\nplace task, which was learned using only sparse binary rewards, performed well on the \nphysical robot without any finetuning. The video presenting their experiments is available \nat https://goo.gl/SMrQnI. But this algorithm relies on special value function \napproximators, which might not be trivially applicable to every problem. Besides, this \ntechnique can not be extended well with the use of reward shaping. \nSome other researchers try to achieve sample efficient learning through demonstra-\ntions in imitation learning [60,61], where a mentor provides demonstrations to replace the \nrandom neural network initialization. [62] was an extension of DDPG algorithm [46] for \ntasks with sparse rewards, where both demonstrations and actual interactions were used \nto fill a replay buffer. Experiments of four simulation tasks and a real robot clip insertion \nproblem were conducted. A video demonstrating the performance can be viewed at \nhttps://www.youtube.com/watch?v=WGJwLfeVN9w. However, the object location and \nthe explicit states of joints, such as position and velocity, must be provided to move from \nsimulation to real-world, which limits its application to high-dimensional data. \n \nBased on the work of generative adversarial imitation learning in [63,64] used \nGener-ative Adversarial Networks (GANs) [65] to generate an additional training data to \nsolve sample complexity problem, by proposing a multi-modal imitation learning \nframework that was able to handle unstructured demonstrations of different skills. The \nperformance of this framework was evaluated in simulation for several tasks, such as \nreacher and gripper-pusher. The video of simulated experiments is available at \nhttp://sites.google. com/view/nips17intentiongan. Like most GANs techniques, it is quite \ndifficult to train and many samples are required. \n \n3.2. Generalization \n \nGeneralization, refers to the capacity to use previous knowledge from a source envi-\nronment to achieve a good performance in a target environment. It is widely seen as a \nstep necessary to produce artificial intelligence that behaves similarly to humans. \nGeneraliza-tion may improve the characteristics of learning the target task by increasing \nthe starting reward on the target domain, the rate of learning for the target task, and the \nmaximum reward achievable [66]. \nIn [67], Google proposed a method to learn hand–eye coordination for robot grasp \ntask. In the experiment phrase, they collected about 800,000 grasp attempts over two \nmonths from multiple robots operating simultaneously, and then used these data to train \na controller that work across robots. These identical uncalibrated robots had differences \nin camera placement, gripper wear or tear. Besides, a second robotic platform with eight \nrobots collected a dataset consisting of over 900,000 grasp attempts, which was used to \ntest transfer between robots. The results of transfer experiment illustrated that data from \n9 of 12 \n \ndifferent robots can be combined to learn more reliable and effective grasping. One can \nrefer to the video at https://youtu.be/cXaic_k80uM for supplementary results. In contrast \nto many prior methods, there is no simulation data or explicit representation, but an end-\nto-end training directly from image pixels to gripper motion in task space by learning just \nfrom this high-dimensional representation. Despite of its attractive success, this method \nstill can not obtain satisfactory accuracy for real application, let alone it is very hardware \nand data intensive. \nTo develop generalization capacities, some researchers turn to meta learning [68], \nwhich is also known as learning to learn. The goal of meta learning is to train a model on \na variety of learning tasks, such that it can solve new learning tasks using only a small \nnumber of training samples [69]. In [70], meta learning was combined with \naforementioned imitation learning in order to learn to perform tasks quickly and efficiently \nin complex unstructured environments. The approach was evaluated on planar reaching \nand pushing tasks in simulation, and visual placing tasks on a real robot, where the goal \nis to learn to place a new object into a new container from a single demonstration. The \nvideo results are available at https://sites.google.com/view/one-shot-imitation. The \nproposed meta-imitation learning method allows a robot to acquire new skills from just a \nsingle visual demonstration, but the accuracy needs to be further improved. \nThere are also many other researches available to tackle other challenges in this \ndomain. For example, no matter whether a control policy is learned directly for a specific \ntask, or transferred from previous tasks, another important but understudied question is \nhow well will the policy performs, namely policy evaluation problem. A behavior policy \nsearch algorithm was proposed in [71] for more efficiently estimating the performance of \nlearned policies. \n \n4. Discussion \n \nAlthough algorithms of robotic manipulation control using DRL have been emerging in \nlarge numbers in the past few years, some even with demonstration videos showing how an \nexperimental robotic manipulator accomplishes a task with the policy learned, as we have \nillustrated above, the challenges of learning robust and versatile manipulation skills for robots \nwith DRL are still far from being resolved satisfactorily for real-world application. \n \nCurrently, robotic manipulation control with DRL may be suited to fault tolerant \ntasks, like picking up and placing objects, where a disaster will not be caused if the \noperation fails occasionally. It is quite attractive in situations, where there is enough \nvariation that the explicit modeling algorithm does not work. Potential applications can be \nfound in warehouse automation to replace human pickers for objects of different size and \nshape, clothes and textiles manufacturing, where cloth is difficult to manipulate by \nnature, and food preparation industry, where, for example, every chicken nugget looks \ndifferent, and it is not going to matter terribly if a single chicken nugget is destroyed. \nHowever, even in this kind of applications, DRL-based methods are not widely used \nin real-world robotic manipulation. The reasons are multiple, including the two concerns \nwe have discussed in the previous section, sample efficiency and generation, where \nmore progress is still required, as both gathering experiences by interacting with the \nenvironment and collecting expert demonstrations for imitation learning are expensive \nprocedures, especially in situations where robots are heavy, rigid and brittle, and it will \ncost too much if the robot is damaged in exploration. Another very important issue is \nsafety guarantee. Not like simulation tasks, we need to be very careful that learning \nalgorithms are safe, reliable and predictable in real scenarios, especially if we move to \nother applications that require safe and correct behaviors with high confidence, such as \nsurgery or household robots taking care of the elder or the disabled. There are also other \nchallenges including but not limited to the algorithm explainability, the learning speed, \nhigh-performance computational equipment requirements. \n10 of 12 \n \n5. Conclusions \n \nThe scalability of DRL, discussed and illustrated here, is well-suited for high-\ndimensional data problems in a variety of domains. In this paper, we have presented a \nbrief review of the potential of DRL for policy detection in robotic manipulation control \nand discussed the current research and development status of real-world applications. \nThrough a joint development of deep learning and reinforcement learning, with \ninspiration from other machine learning methods like imitation learning, GANs, or meta \nlearning, new algorithmic solutions can emerge, and are still needed, to meet challenges \nin robotic manipulation control for practical applications. \n  \nFunding: This research work is part of a project funded by the University of Strasbourg’s Initiative \nD’EXellence (IDEX). \n \nConflicts of Interest: The authors declare no conflict of interest. \n \nAbbreviations \n \nThe following abbreviations are used in this manuscript: \n \nDoF \n \nDRL \n \nDNN \n \nDQN \n \nDDPG \n \nHER \n \nGANs \n  \nDegrees of Freedom \n \nDeep Reinforcement Learning \n \nDeep Neural Network \n \nDeep Q-Network \n \nDeep Deterministic Policy Gradient \n \nHindsight Experience Replay \n \nGenerative Adversarial Networks \n \n \nReferences \n \n1. \nDresp-Langley, B.; Nageotte, F.; Zanne, P.; Mathelin, M.D.. Correlating grip force signals from multiple sensors highlights \nprehensile control strategies in a complex task-user system. Bioengineering 2020, 7, 143. \n \n2. \nEranki, V.K.P.; Reddy, Gurudu, R. Design and Structural Analysis of a Robotic Arm. 2017. \n \n3. \nChrist, R.D.; Wernli, R.L. Manipulators. The ROV Manual. 2014, 503–534. \n \n4. \nIvanescu, M. Control. In Mechanical Engineer’s Handbook; Publisher: City, Country, 2001; pp.611–714. \n \n5. \nSavatekar, R.D.; Dum, A.A. Design of control system for articulated robot using leap motion sensor. Int. Res. J. Eng. Technol. \n2016, 3, 1407–1417. \n \n6. \nWei, H.; Bu, Y.; Zhu, Z. Robotic arm controlling based on a spiking neural circuit and synaptic plasticity. Biomed. Signal \nProcess. Control 2020, 55, 101640. \n \n7. \nBožek, P.; Al Akkad, M.A.; Blištan, P.; Ibrahim N, I. Navigation control and stability investigation of a mobile robot based on a \nhexacopter equipped with an integrated manipulator. Int. J. Adv. Robot. Syst. 2017, 14, 1729881417738103. \n \n8. \nSafdar, B. Theory of Robotics Arm Control with PLC; Saimaa University of Applied Sciences: South Karelia, Finland, 2015. \n \n9. \nPitts, W.; McCulloch, W.S. How we know universals the perception of auditory and visual forms. Bull. Math. Biophys. 1947, 9, \n127–147. \n \n10. Rosenblatt, F. Perceptron simulation experiments. Proc. IRE 1960, 48, 301–309. \n \n11. Rumelhart, D.E.; Hinton, G.E.; Williams, R.J. Learning Internal Representations by Error Propagation; California Univ San \nDiego La Jolla Inst for Cognitive Science: San Diego, CA, USA, 1985. \n \n12. LeCun, Y.; Boser, B.; Denker, J.S.; Henderson, D.; Howard, R.E.; Hubbard, W.; Jackel, L.D. Backpropagation applied to \nhandwritten zip code recognition. Neural Comput. 1989, 1, 541–551. \n11 of 12 \n \n13. Jarrett, K.; Kavukcuoglu, K.; Ranzato, M.A.; LeCun, Y. What is the best multi-stage architecture for object recognition. In \nProceedings of the IEEE 12th International Conference on Computer Vision, Kyoto, Japan, 29 September–2 October 2009; \npp. 2146–2153. \n \n14. Ciresan, D.C.; Meier, U.; Masci, J.; Gambardella, L.M.; Schmidhuber, J. Flexible, high performance convolutional neural \nnetworks for image classification. In Proceedings of the Twenty-Second International Joint Conference on Artificial \nIntelligence, Barcelona, Spain, 6–12 July 2011. \n \n15. Hinton, G.E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R.R. Improving neural networks by preventing co-\nadaptation of feature detectors. arXiv 2012, arXiv:1207.0580. \n \n16. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv 2015, \narXiv:1502.03167. \n \n17. Liu R. Multispectral Images-Based Background Subtraction Using Codebook and Deep Learning Approaches. Ph.D. Thesis, \nUniversité Bourgogne Franche-Comté: Besançon, France, 2020. \n \n18. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classification with deep convolutional neural networks. Adv. Neural Inf. \nProcess. Syst. 2012, 1097–1105. \n \n19. \nLevine, S.; Finn, C.; Darrell, T.; Abbeel, P. End-to-end training of deep visuomotor policies. J. Mach. Learn. Res. 2016, 17, 1334–1373. \n \n20. Kaelbling, L.P.; Littman, M.L.; Moore, A.W. Reinforcement learning: A survey. J. Artif. Intell. Res. 1996, 4, 237–285. \n \n21. Kober, J.; Bagnell, J.A.; Peters, J. Reinforcement learning in robotics: A survey. Int. J. Robot. Res. 2013, 32, 1238–1274. \n \n22. Sutton, R.S.; Barto, A.G. Reinforcement Learning: An Introduction; MIT Press: Cambridge, MA, USA, 2018. \n \n23. Dresp-Langley, B.; Ekseth, O.K.; Fesl, J.; Gohshi, S.; Kurz, M.; Sehring, H.W. Occam’s Razor for Big Data? On detecting \nquality in large unstructured datasets. Appl. Sci. 2019, 9, 3065. \n \n24. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; Riedmiller, M. Playing atari with deep reinforce-\nment learning. arXiv 2013, arXiv:1312.5602. \n \n25. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.; Fidjeland, A.K.; \nOstrovski, G. Human-level control through deep reinforcement learning. Nature 2015, 518, 529–533. \n \n26. \nSilver, D.; Huang, A.; Maddison, C.J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, \nV.; Lanctot, M. Mastering the game of Go with deep neural networks and tree search. Nature 2016, 529, 484–489. \n27. Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai, M.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel, T.; et al. \nMastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv 2017, arXiv:1712.01815. \n \n28. Brown, N.; S.; holm, T. Superhuman AI for multiplayer poker. Science 2019, 365, 885–890. \n \n29. Berner, C.; Brockman, G.; Chan, B.; Cheung, V.; D˛ebiak, P.; Dennison, C.; Farhi, D.; Fischer, Q.; Hashme, S.; Hesse, C.; et \nal. Dota 2 with large scale deep reinforcement learning. arXiv 2019, arXiv:1912.06680. \n \n30. Gu, S.; Holly, E.; Lillicrap, T.; Levine, S. Deep reinforcement learning for robotic manipulation with asynchronous off-policy \nupdates. In Proceedings of the IEEE international conference on robotics and automation (ICRA), Singapore, 29 May–3 June \n2017; pp. 3389–3396. \n \n31. Sharma, A.R.; Kaushik, P. Literature survey of statistical, deep and reinforcement learning in natural language processing. In \nProceedings of the International Conference on Computing, Communication and Automation (ICCCA), Greater Noida, India, \n5–6 May 2017; pp. 350–354. \n \n32. Yun, S.; Choi, J.; Yoo, Y.; Yun, K.; Young, Choi, J. Action-decision networks for visual tracking with deep reinforcement \nlearning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21–26 July \n2017; pp. 2711–2720. \n \n33. Farazi, N.P.; Ahamed, T.; Barua, L.; Zou, B. Deep Reinforcement Learning and Transportation Research: A Comprehensive \nReview. arXiv 2015, arXiv:2010.06187. \n \n34. Mosavi, A.; Ghamisi, P.; Faghan, Y.; Duan, P. Comprehensive Review of Deep Reinforcement Learning Methods and \nApplications in Economics. arXiv 2015, arXiv:2004.01509. \n \n35. Liu, Y.; Logan, B.; Liu, N.; Xu, Z.; Tang, J.; Wang, Y. Deep reinforcement learning for dynamic treatment regimes on medical \nregistry data. In Proceedings of the IEEE International Conference on Healthcare Informatics (ICHI), Park City, UT, USA, 23–\n26 August 2017; pp. 380–385. \n \n36. Watkins, C.J.C.H.; Dayan, P. Q-learning. Mach. Learn. 1992, 8, 279–292. \n \n37. Bellman, R.E.; Dreyfus, S.E. Applied Dynamic Programming; Princeton University Press: Princeton, NJ, USA, 2015. \n \n38. Gordon, G.J. Stable fitted reinforcement learning. Adv. Neural Inf. Process. Syst. 1995, 8, 1052–1058. \n \n39. Riedmiller, M. Neural fitted Q iteration—First experiences with a data efficient neural reinforcement learning method. In \nProceedings of the European Conference on Machine Learning, Porto, Portugal, 3–7 October 2005; pp. 317–328. \n \n40. Hasselt, H.; Guez, A.; Silver, D. Deep reinforcement learning with double Q-Learning. In Proceedings of the Thirtieth AAAI \nConference on Artificial Intelligence, Phoenix, AZ, USA, 12–17 February 2016; pp. 2094–2100. \n \n41. Bellemare, M.G.; Dabney, W.; Munos, R. A distributional perspective on reinforcement learning. In Proceedings of the 34th \nInternational Conference on Machine Learning, Sydney, Australia, 6–11 August 2017; pp. 449–458. \n \n42. Dabney, W.; Rowl, M.; Bellemare, M.G.; Munos, R. Distributional reinforcement learning with quantile regression. In \nProceedings of the 32th AAAI Conference on Artificial Intelligence, New Orleans, LA, USA, 2–7 February 2018. \nRobotics 2021, 1, 0 \n12 of 12 \n \n43. Hessel, M.; Modayil, J.; Van Hasselt, H.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; Silver, D. \nRainbow: Combining improvements in deep reinforcement learning. arXiv 2017, arXiv:1710.02298. \n \n44. Salimans, T.; Ho, J.; Chen, X.; Sidor, S.; Sutskever, I. Evolution strategies as a scalable alternative to reinforcement learning. \narXiv 2017, arXiv:1703.03864. \n \n45. \nSilver, D.; Lever, G.; Heess, N.; Degris, T.; Wierstra, D.; Riedmiller, M. Deterministic policy gradient algorithms. In Proceedings of the \n31st International Conference on International Conference on Machine Learning, Beijing, China, 21–26 June 2014; pp. 387–395. \n46. Lillicrap, T.P.; Hunt, J.J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.; Silver, D.; Wierstra, D. Continuous control with deep \nreinforcement learning. arXiv 2015, arXiv:1509.02971. \n \n47. \nO’Donoghue, B.; Munos, R.; Kavukcuoglu, K.; Mnih, V. Combining policy gradient and Q-learning. arXiv 2016, arXiv:1611.01626. \n \n48. Oh, J.; Guo, X.; Lee, H.; Lewis, R.L.; Singh, S. Action-conditional video prediction using deep networks in atari games. Adv. \nNeural Inf. Process. Syst. 2015, 28, 2863–2871. \n \n49. Nagab, i A.; Kahn, G.; Fearing, R.S.; Levine, S. Neural network dynamics for model-based deep reinforcement learning with \nmodel-free fine-tuning. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Brisbane, \nQLD, Australia, 21–25 May 2018; pp.7559–7566. \n \n50. Silver, D.; Hasselt, H.; Hessel, M.; Schaul, T.; Guez, A.; Harley, T.; Dulac-Arnold, G.; Reichert, D.; Rabinowitz, N.; Barreto, A.; \net al. The predictron: End-to-end learning and planning. In Proceedings of the International Conference on Machine Learning, \nSydney, Australia, 6–11 August 2017; pp. 3191–3199. \n \n51. \nTamar, A.; Wu, Y.; Thomas, G.; Levine, S.; Abbeel, P. Value iteration networks. Adv. Neural Inf. Process. Syst. 2016, 29, 2154–2162. \n \n52. \nFrançois-Lavet, V.; Bengio, Y.; Precup, D.; Pineau, J. Combined reinforcement learning via abstract representations. In Proceedings \nof the AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 27 January–1 February 2019; pp. 3582–3589. \n53. François-Lavet, V.; Henderson, P.; Islam, R.; Bellemare, M.G.; Pineau, J. An introduction to deep reinforcement learning. \narXiv 2018, arXiv:1811.12560. \n \n54. Wandeto, J.M.; Dresp-Langley, B. The quantization error in a Self-Organizing Map as a contrast and colour specific indicator \nof single-pixel change in large random patterns. Neural Netw. 2019 , 119, 273–285. \n \n55. Dresp-Langley, B.; Wandeto, J.M. Pixel precise unsupervised detection of viral particle proliferation in cellular imaging data. \nInform. Med. Unlocked 2020, 20, 100433. \n \n56. \nAnthony, M.; Bartlett, P.L. Neural Network Learning: Theoretical Foundations; Cambridge University Press: Cambridge, UK, 2009. \n \n57. Kakade, S.M. On the Sample Complexity of Reinforcement Learning. Ph.D. Thesis, University of London, London, UK, 2003. \n \n58. Sergey, L.; Wagener, N.; Abbeel, P. Learning contact-rich manipulation skills with guided policy search. In Proceedings of the \n2015 IEEE International Conference on Robotics and Automation (ICRA), Seattle, WA, USA, 30 May 2015; pp.26–30. \n \n59. Andrychowicz, M.; Wolski, F.; Ray, A.; Schneider, J.; Fong, R.; Welinder, P.; McGrew, B.; Tobin, J.; Abbeel, O.P.; Zaremba, \nW. Hindsight experience replay. Adv. Neural Inf. Process. Syst. 2017, 30, 5048–5058. \n \n60. Tai, L.; Zhang, J.; Liu, M.; Burgard, W. A survey of deep network solutions for learning control in robotics: From reinforcement \nto imitation. arXiv 2016, arXiv:1612.07139. \n \n61. Bagnell, A.J. An Invitation to Imitation; Technical Report, Robotics Institute, Carnegie Mellon University: Pittsburgh, PA, USA, \n2015. \n \n62. \nVecerik, M.; Hester, T.; Scholz, J.; Wang, F.; Pietquin, O.; Piot, B.; Heess, N.; Rothörl, T.; Lampe, T.; Riedmiller, M. Leveraging \ndemonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv 2017, arXiv:1707.08817. \n63. Ho, J.; Ermon, S. Generative adversarial imitation learning. arXiv 2016, arXiv:1606.03476. \n \n64. Hausman, K.; Chebotar, Y.; Schaal, S.; Sukhatme, G.; Lim, J.J. Multi-modal imitation learning from unstructured \ndemonstrations using generative adversarial nets. In Proceedings of the Advances in Neural Information Processing Systems \n30 (NIPS 2017), Long Beach, CA, USA, 4–9 December 2017; pp. 1235–1245. \n \n65. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. Generative \nadversarial nets. Adv. Neural Inf. Process. Syst. 2014, 27, 2672–2680. \n \n66. Spector, B.; Belongie, S. Sample-efficient reinforcement learning through transfer and architectural priors. arXiv 2018, \narXiv:1801.02268. \n \n67. Levine, S.; Pastor, P.; Krizhevsky, A.; Ibarz, J.; Quillen, D. Learning hand-eye coordination for robotic grasping with deep \nlearning and large-scale data collection. Int. J. Robot. Res. 2018, 37, 421–436. \n \n68. Thrun, S.; Pratt, L. Learning to Learn; Springer Science & Business Media: Berlin/Heidelberg, Germany, 2012. \n \n69. \nFinn, C.; Abbeel, P.; Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv 2017, arXiv:1703.03400. \n \n70. \nFinn, C.; Yu, T.; Zhang, T.; Abbeel, P.; Levine, S. One-shot visual imitation learning via meta-learning. arXiv 2017, arXiv:1709.04905. \n \n71. Hanna, J.P.; Thomas, P.S.; Stone, P.; Niekum, S. Data-efficient policy evaluation through behavior policy search. ArXiv 2017, \nArXiv:1706.03469. \n",
  "categories": [
    "cs.RO"
  ],
  "published": "2021-02-08",
  "updated": "2021-02-08"
}