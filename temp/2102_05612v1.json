{
  "id": "http://arxiv.org/abs/2102.05612v1",
  "title": "Personalization for Web-based Services using Offline Reinforcement Learning",
  "authors": [
    "Pavlos Athanasios Apostolopoulos",
    "Zehui Wang",
    "Hanson Wang",
    "Chad Zhou",
    "Kittipat Virochsiri",
    "Norm Zhou",
    "Igor L. Markov"
  ],
  "abstract": "Large-scale Web-based services present opportunities for improving UI\npolicies based on observed user interactions. We address challenges of learning\nsuch policies through model-free offline Reinforcement Learning (RL) with\noff-policy training. Deployed in a production system for user authentication in\na major social network, it significantly improves long-term objectives. We\narticulate practical challenges, compare several ML techniques, provide\ninsights on training and evaluation of RL models, and discuss generalizations.",
  "text": "Personalization for Web-based Services\nusing Offline Reinforcement Learning\nPavlos Athanasios Apostolopoulos\npavlosapost@unm.edu\nThe University of New Mexico, ECE\nAlbuquerque, NM\nZehui Wang, Hanson Wang, Chad Zhou,\nKittipat Virochsiri, Norm Zhou, Igor L. Markov\n{wzehui,hansonw,yuzhoubrother,kittipat,nzhou,imarkov}@fb.com\nFacebook Inc., Menlo Park, CA\nABSTRACT\nLarge-scale Web-based services present opportunities for improv-\ning UI policies based on observed user interactions. We address\nchallenges of learning such policies through model-free offline Re-\ninforcement Learning (RL) with off-policy training. Deployed in a\nproduction system for user authentication in a major social network,\nit significantly improves long-term objectives. We articulate practi-\ncal challenges, compare several ML techniques, provide insights on\ntraining and evaluation of RL models, and discuss generalizations.\n1\nINTRODUCTION\nFor Web-based services with numerous customers, such as social\nnetworks, UI decisions impact top-line metrics, such as user engage-\nment, costs and revenues. Machine learning, especially Supervised\nLearning, can optimize these decisions but long-term cumulative\nobjectives makes it challenging to label each decision for training.\nAs an illustration, consider online user authentication. When a user\nmistypes or forgets her password, the service can loop back to the\nlogin prompt or offer another login channel, authorization code\nvia SMS (Short Message Service), etc. The user’s context is a part\nof a personalized configuration. The scale of the social network\nmakes monetary cost (service fees for authentication) significant.\nSome users get their password right on the second try or look it\nup but others give up, and this is reflected in daily/monthly user\nengagement. Deciding when to authenticate requires real-time per-\nsonalization with consideration for cost and engagement metrics,\nbut the impact on end metrics is delayed. Our work addresses these\nchallenges with ML, describes insights on model selection and train-\ning, and reports a production deployment. We also explain how\nour general approach and our infrastructure generalize to multiple\nchallenges beyond the didactic application.\nPrior efforts on personalized configuration systems covers a wide\nrange of content-serving applications, including recommendation\nsystems [7, 8], ad targeting [1, 35], and personalized medical health-\ncare [11]. Recent advances in Machine Learning brought us expres-\nsive deep neural networks with effective generalization, as well\nas infrastructure for building real-time systems powered by ML.\nA common theme is to provide a personalized configuration and\noptimize desired end metrics given user’s real-time context. Super-\nvised Learning methods have shown promising results [10, 38],\nbut they only focus on optimizing immediate metrics such as the\nclick-through rate [20] and conversion rate [24]. This limitation\nis important for long-term user engagement (daily/monthly active\nusers and monetary cost), as shown by the authentication example.\nReinforcement Learning (RL) [31] seeks an optimal policy to\nmaximize a long-term reward, and thus helps drive real-time per-\nsonalized decisions [33]. Here, the RL agent’s environment is an\nindividual online user, and the personalized serving procedure is\nmodeled via sequential agent-environment interactions. Therefore,\nthe RL agent alternates between policy improvement and inter-\naction with the environment. In practice, RL agents need many\ninteractions to learn good policies [6]. To this end, training RL\nagents online may undermine user experience and/or incur large\ncosts. Hence, policies are trained offline using logged interactions\nfrom any type of prior policies (Offline RL in Section 3).\nIn this work, we use Offline RL to improve personalized authen-\ntication for a Web-based service. We formalize the problem as a\nMarkov decision process (MDP) [26], where the RL agent learns a\npersonalized policy, i.e., when to send an authentication message to\nthe user after a failed login attempt. The training process optimizes\nlong-term user engagement and the service’s authentication costs.\nWe avoid the pitfalls of online trial-and-error by training on prior\nexperiences logged for different policies through off-policy learning.\nTo ensure effective offline training, we use several training heuris-\ntics, then evaluate performance of the learned candidate policies via\nan unbiased off-policy estimator. The best learned policy is chosen\nand evaluated in online A/B tests [15] on live data.\nThe remaining part of the paper is organized as follows. We\nreview Reinforcement Learning in Section 2 focusing on off-policy\nlearning. Section 3 discusses the need for offline learning and the\nchallenges of deploying such applications. A representative appli-\ncation amenable to reinforcement learning is covered in Section 4,\nand problem formalization in Section 5. Our contributions are in-\ntroduced in Section 6 where we describe an industry application of\nOffline RL to user authentication. Section 7 details how our Offline\nRL model was deployed to a major social network and compares\nit to a supervised-learning baseline. Section 8 puts our work in\nperspective and discusses personalization more broadly.\n2\nBACKGROUND\nIn this section, we review the background and establish notation\nthat will be used later. Section 2.1 reviews Reinforcement Learning\ntheory [31] and Section 2.2 focuses on off-policy learning.\n2.1\nPreliminaries on Reinforcement Learning\nReinforcement learning seeks to control an interactive dynamic\nsystem. At each discrete time step, 𝑡= 0, 1, · · · , the agent observes\nthe environment’s state 𝑠𝑡and responds with action 𝑎𝑡, while the\nenvironment responds with an associated reward 𝑟𝑡+1 and transi-\ntions into the next state 𝑠𝑡+1. To this end, the environment can be\ndefined by a Markov Decision Process (MDP) (S, A,𝑟, 𝑝,𝛾) [26],\nwhere S is the state space with 𝑠𝑡∈S, A is the action space with\narXiv:2102.05612v1  [cs.LG]  10 Feb 2021\nApostolopoulos et al.\n𝑎𝑡∈A, 𝑟𝑠𝑎is the reward function, 𝑝(·|𝑠,𝑎) is a conditional probabil-\nity distribution of the form 𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡) (environment’s dynamics),\nand 𝛾∈(0, 1] is a scalar discount factor.\nA Reinforcement Learning agent seeks a policy 𝜋∗(𝑎𝑡|𝑠𝑡) that for\neach state defines a distribution over possible actions. The policy\nshould maximize cumulative reward over time, i.e., an expectation\nunder the environment’s dynamics:\n𝐽(𝜋) = E\n∞\n∑︁\n𝑡=0\n𝛾𝑡𝑟𝑠𝑡𝑎𝑡\n(1)\n𝑠0 = 𝑠,𝑎0 = 𝑎,𝑠𝑡∼𝑝(·|𝑠𝑡−1,𝑎𝑡−1),𝑎𝑡∼𝜋(·|𝑠𝑡)\nBased on Equation 1, the Reinforcement Learning problem can be\nviewed as an optimization over the space of policies 𝜋:\n𝜋∗= arg max\n𝜋\nE\n∞\n∑︁\n𝑡=0\n𝛾𝑡𝑟𝑠𝑡𝑎𝑡\n(2)\nFor policies 𝜋𝜃smoothly parameterized by weights 𝜃, Equation 1\nis amenable to gradient descent in terms of ∇𝜃𝐽(𝜋𝜃), i.e., policy\ngradients [28, 32]. Functional dependency of 𝜋on 𝑠𝑡,𝑎𝑡makes it\neasier to work with stochastic policies.\nAdditionally, the Reinforcement Learning objective in Equation 1\ncan be optimized by accurately estimating a state-action value func-\ntion [31], and then using that value function to derive an optimal\npolicy 𝜋∗. The state-action value function of policy 𝜋is\n𝑄𝜋(𝑠𝑡,𝑎𝑡) = E𝑠𝑡∼𝑝(𝑠𝑡|𝑠𝑡−1,𝑎𝑡−1)\n𝑎𝑡∼𝜋(𝑎𝑡|𝑠𝑡)\n\" ∞\n∑︁\n𝑡=0\n𝛾𝑡𝑟𝑠𝑡𝑎𝑡\n#\n,\n(3)\nand we can express 𝑄𝜋(𝑠𝑡,𝑎𝑡) as follows:\n𝑄𝜋(𝑠𝑡,𝑎𝑡) = 𝑟𝑠𝑡𝑎𝑡+ 𝛾E𝑠𝑡+1∼𝑝(𝑠𝑡+1 |𝑠𝑡,𝑎𝑡)\n𝑎𝑡+1∼𝜋(𝑎𝑡+1 |𝑠𝑡+1)\n\u0002\n𝑄𝜋(𝑠𝑡+1,𝑎𝑡+1)\n\u0003\n(4)\nA policy can be defined for a state-action value function by\n𝜋(𝑎𝑡|𝑠𝑡) = 𝛿(𝑎𝑡= arg max\n𝑎𝑡∈A\n𝑄(𝑠𝑡,𝑎𝑡)),\n(5)\nand by substituting Equation 5 into Equation 4, we obtain the\nBellman optimality equations [5] that characterize optimal policies\nin terms of optimal Q-functions 𝑄∗= 𝑄𝜋∗:\n𝑄∗(𝑠𝑡,𝑎𝑡) = 𝑟𝑠𝑡𝑎𝑡+ 𝛾E𝑠𝑡+1∼𝑝(𝑠𝑡+1 |𝑠𝑡,𝑎𝑡)\n\u0014\nmax\n𝑎𝑡+1 𝑄∗(𝑠𝑡+1,𝑎𝑡+1)\n\u0015\n(6)\nTherefore, Reinforcement Learning (Equation 2) seeks an optimal\npolicy 𝜋∗such that 𝑄𝜋∗(𝑠𝑡,𝑎𝑡) ≥𝑄𝜋(𝑠𝑡,𝑎𝑡), ∀𝜋,𝑠𝑡∈S,𝑎𝑡∈A.\n2.2\nOff-Policy Learning\nIn the discussion below, we focus on policies defined by state-action\nvalue functions per Equation 5, and 𝜀-greedy policies [31] that\nperform random exploration with a small probability. This focus\nsimplifies the narrative, and we found such policies successful in\npractice. Sections 5 and 7 also discuss parameterized policies that\ndraw actions from state-conditional probability distributions and\nare trained to maximize cumulative rewards.\nStriving for an optimal Q-function 𝑄∗and therefore an opti-\nmal policy 𝜋∗= 𝛿(𝑎𝑡= arg max 𝑄∗(𝑠𝑡,𝑎𝑡)), Q-learning [41] al-\nternates between two phases. First, it improves an approximate\nestimate 𝑄(𝑠𝑡,𝑎𝑡;𝜃) of 𝑄∗by repeatedly regressing Equation 6\nwith respect to parameters 𝜃. In off-policy Q-learning, an action\nwith the best Q-value is considered Bellman-optimal per Equa-\ntion 6 and used in gradient calculations. In the second phase, the\nagent explores the environment, typically following a stochas-\ntic policy based on 𝑄(𝑠𝑡,𝑎𝑡;𝜃), e.g., an 𝜖-greedy [31] version of\n𝜋(𝑠𝑡) = 𝛿(𝑎𝑡= arg max 𝑄(𝑠𝑡,𝑎𝑡;𝜃)). Q-learning takes a single\ngradient step per training iteration to minimize the difference be-\ntween the left-hand and right-hand side of Equation 6. Among the\nmany variants of this learning procedure, the most common variant\ncalled DQN [22] utilizes a replay buffer D = {(𝑠𝑖\n𝑡,𝑎𝑖\n𝑡,𝑠𝑖\n𝑡+1,𝑟𝑖\n𝑡+1)}\nfor storing the agent’s interaction with the environment and alter-\nnates between data collection and gradient steps with respect to\nthe Temporal Difference loss function L𝑖(𝜃𝑖), defined as follows\nvia Equation 6:\nL𝑖(𝜃𝑖) = E𝑠𝑡,𝑎𝑡∼𝐷\n\u0002\n(𝑦𝑖−𝑄(𝑠𝑡,𝑎𝑡;𝜃𝑖))2\u0003\n(7)\nHere 𝑦𝑖= E𝑠𝑡+1∼𝐷\nh\n𝑟(𝑠𝑡,𝑎𝑡) + 𝛾max\n𝑎\n𝑄(𝑠𝑡+1,𝑎;𝜃−\n𝑖))\ni\n, 𝜃𝑖are the pa-\nrameters of the approximate Q-function at the iteration 𝑖of the\nlearning procedure, and for better stability in the learning proce-\ndure the 𝜃−\n𝑖are frozen parameters that are used for estimating the\ntarget values, i.e., 𝑦𝑖. The 𝜃−parameters are periodically updated to\nthe current parameters 𝜃of the parametric Q-function. Variants of\nDQN decouple the selection from action evaluation (Double DQN\n[36]), or approximate the Q-function through dueling network ar-\nchitectures (Dueling DQN [40]) to address the overestimation of\nQ-values in the DQN setting.\nQ-learning and its aforementioned deep reinforcement learning\nvariants are characterized as off-policy algorithms [31], since the\nlearning target values 𝑦𝑖can be computed without any considera-\ntion of the policy that was used to generate the experiences of the\nreplay buffer i.e., D = {(𝑠𝑖\n𝑡,𝑎𝑖\n𝑡,𝑠𝑖\n𝑡+1,𝑟𝑖\n𝑡+1)}.\n3\nOFFLINE REINFORCEMENT LEARNING\nOffline Reinforcement Learning seeks policy 𝜋∗to maximize cu-\nmulative reward (Equation 1), but avoids live interactions with the\nenvironment during training. It is used when such interactions can\nbe harmful [9]. Hence, a static dataset of state-action transitions\nD𝜋𝛽= {(𝑠𝑖\n𝑡,𝑎𝑖\n𝑡,𝑠𝑖\n𝑡+1,𝑟𝑖\n𝑡+1)} logged via behavioral policy 𝜋𝛽is used.\nWeb-based services routinely log each user session, but offline RL\nbased on Equation 7 requires redundant logs, where each row in-\ncludes the previous state 𝑠𝑡and next state 𝑠𝑡+1 [9]. Such rows can\nbe used independently and batch-sampled for training.\nOffline RL trains on state-action transitions batch-sampled from\ntraining set D𝜋𝛽through off-policy learning (Section 2.2). Learning\nrelies on RL mechanisms such as optimal state-action values (e.g.,\nEquation 7), but generalization results from supervised learning\nshould apply. After the learned policy 𝜋∗is evaluated offline and\ntuned, online evaluation uses live interaction with the environment.\nOff-policy Reinforcement Learning algorithms (DQN, Double\nDQN, and Dueling DQN) that estimate the state-action value func-\ntion 𝑄(𝑠𝑡,𝑎𝑡;𝜃) can be directly used offline. However, the online\nregime has the benefit of additional data collection via environment\ninteraction (exploration) that helps the agent refine its 𝑄(𝑠𝑡,𝑎𝑡;𝜃)\nestimates for high-reward actions. Although off-policy learning has\nshown promising results in offline RL [2, 14], the lack of exploration\nlimits agent’s learning [17] due to distributional shift phenomena.\nPersonalization for Web-based Services\nusing Offline Reinforcement Learning\nState distributional shift affects offline RL algorithms during\nthe agent’s exposure on the real-world environment (deployment\nphase) [17]. The latter occurs as the agent’s learned policy 𝜋∗may\nfollow a systematically different state visitation frequency com-\npared to the one of the training set, i.e., D𝜋𝛽, that is induced by\nthe behavior policy 𝜋𝛽. In other words, since the agent’s goal is to\nfind the best policy 𝜋∗offline by utilizing the static dataset D𝜋𝛽, its\nlearned policy can diverge from the behavior policy 𝜋𝛽, invoking\nunreasonable actions in out-of-distribution/unseen states.\nAction distributional shift affects off-policy learning algorithms,\nwhich are estimating the Q-function, i.e., 𝑄(𝑠𝑡,𝑎𝑡;𝜃), during the\ntraining process as well [42]. In principle, the accuracy of the re-\ngression in Equation 7 depends on the estimate of the Q-function\nfor actions that may be outside of the distribution of actions that the\nQ-function was ever trained on, i.e., right-hand side of Eq. 6 . The ac-\ntion distribution shift is exacerbated by the differences between the\nagent’s learned policy and the behavioral policy 𝜋𝛽during training.\nIf the agent’s parameterized Q-function produces large, erroneous\nvalues for out-of-distribution actions, it will further learn to do so.\nStandard RL methods address the distributional shift via ongoing\nexploration by revising the state-action value function 𝑄(𝑠𝑡,𝑎𝑡;𝜃).\nHowever, offline RL lacks such a feedback loop, and inaccuracies in\nthe state-action value function accumulate during learning.\nPrior research [28, 30] focuses on mitigating distributional shift\nin offline RL by limiting how much the learned optimal policy 𝜋∗\nmay deviate from the behavior policy 𝜋𝛽. This forces the agent’s\nlearned policy to stay close to the behavior policy and reduces\nstate distributional shift. Additionally, the latter reduces action\ndistributional shift during offline training as well, as most of the\nstates and actions fed into the right-hand side of Equation 7 are\nin-distribution with respect to the training set. In that case, the\naction distributional shift errors should not accumulate. As another\nexample, the work in [12] regularizes the agent’s learned policy\ntowards the behavioral policy by using the Kullback-Leibler (KL)\ndivergence with a fixed regularization weight, while the Maximum\nMean Discrepancy with an adaptively trained regularization weight\nis used in [17]. Moreover, the work in [12, 42] defines target values\n(𝑦𝑖) to regularize the agent’s learned policy towards the behavioral\npolicy and avoid actions inconsistent with the behavioral policy.\nTo guide our design decisions for the application in Section 4,\nin Section 5.1 we introduce a simplified, intuitive problem environ-\nment. In Section 6.1, we use this environment to illustrate important\ninsights on distributional shift in Offline RL and the impact of be-\nhavioral policy’s exploration on the quality of trained models.\n4\nAPPLICATION: USER AUTHENTICATION\nWe illustrate personalization in Web-based services with a self-\ncontained application that is representative of Web-based software\nand widespread. It optimizes long-term rewards and favors rein-\nforcement learning. We formalize the problem in Section 5.\nUser authentication starts a typical Web-based session; thus fail-\nures can limit user engagement. Due to forgotten passwords, such\nfailures are common when different passwords are used for each\nWeb-based service. The authentication UI may respond to failures\nby offering password recovery (Action A) or by prompting another\nlog-in attempt (Action B), see Figure 1. Password recovery verifies\nthe user’s identity by sending a code to a pre-registered mobile\nphone or another user-owned device. Common options include\n• One-Time Password (OTP) sent via SMS and email\n• Time-based One-Time Password (TOTP) authentication [19].\nSuch authentication is considered safe unless the user’s device\nis compromised or the phone number is hijacked [29].1 In our\napplication, we authenticate via OTP.\nAction A\nAction B\nFigure 1: User authentication UI.\nThe decision mechanism that choses between available actions\nmay affect long-term objectives, for example user engagement cap-\ntured by how many service users are active per day or per month.\nTo this end, Action A may typically succeed and improve user\nengagement but incurs OTP costs. Action B may succeed without\nsuch costs, and if it fails again, Action A can still be invoked. A\nnascent Web-based service may strive to grow user engagement at\ngreater cost, but mature services may lean towards lower cost.\nTraining the decision mechanism on user activity assumes\ntrends or patterns, as well as sufficient features to learn them. When\ndifferent user cohorts develop different trends, separating them\nsimplifies the learning task. For example, new users may exhibit\ndifferent behaviors than long-term users or have less historical data\nto use for inferences. This paper focuses on users with prior login\nhistory and additional user features.\nAdditional aspects of the problem are worth noting. Compared\nto multiplayer games, the lack of typical-case adversarial behavior\nsimplifies successful policies and makes them easier to learn from\nlogged data without simulated interactions. On the other hand,\ntraining a policy from scratch on user interactions in a live pro-\nduction system may adversely affect user engagement until the\npolicy is optimized. Compared to the (contextual) bandit setup, this\nproblem exhibits sequential depth.\nExtensions of this problem include state-dependent and larger\naction spaces, and substantial generalizations covered in Section 8.\n5\nPROBLEM FORMALIZATION\nThe application introduced in Section 4 deals with sequences of\nactions, e.g., Action B can be tried several times before falling back\non Action A. The sequential nature of this application calls for\nmodels based on states and state transitions. First, we introduce\na simplified, intuitive example used later in the paper and then\noutline our full-fledged state model.\n1Such authentication can also support two-factor authentication (2FA) [3, 27].\nApostolopoulos et al.\n5.1\nA motivational example\nConsider the user-authentication application with two possible ac-\ntions at a time, introduced in Section 4. It is easy to check whether\nAction A (password recovery) is more beneficial than Action B\n(another log-in attempt) on average, marginalizing all other infor-\nmation. This generalizes to short sequences, e.g., which action is\npreferable after one Action B? After two of them? (marginalizing\neverything else). By capturing such sequences up to some depth,\none can optimize per-action rewards using either supervised or\nreinforcement learning. Figure 2 illustrates possible system actions\nfrom the initial failed-login state and subsequent states, as well as\nimplied state transitions. The feedback to system actions is cap-\ntured by rewards only (which include OTP costs as per Equation\n8), and successful logins bring higher rewards than failures. State\ntransitions depend entirely on the actions chosen by the system.\nIn particular, after a successful login, the sequence can continue\nupon a future login failure. This self-contained state-reward model\ncan capture the most basic sequential preferences but lacks user\npersonalization, which is key to our work.\nFigure 2: State transitions during user authentication. All\nstates are distinct and indexed with depth (subscripts) and\nthe history of actions (encoded in binary in superscripts).\nA user can be characterized by a number of features that cover,\ne.g., time since account creation and local time, the amount of online\nactivity, etc. In other words, each user at each step is represented\nby a multidimensional vector such that proximal vectors represent\nsimilar users. Compared to the earlier few-state model that repre-\nsents an average user, personalization requires states that cannot\nbe limited to a small predefined set. We now interpret Figure 2 as\nshowing possible transitions and traces for one user, wherein each\nuser state 𝑠𝑡in each trace is represented by a finite-dimensional\nfeature vector (user state vector).\nThe semantics of features are not important for our motivational\nexample. Therefore, for each user we represent the initial state 𝑠0\n(the first failed log-in attempt) with a state vector by drawing it\nat random from a personalized multivariate Gaussian distribution\nwith mean and covariance selected randomly per user. Subsequent\nuser states 𝑠𝑡with 𝑡= 1, . . . that correspond to future failed login\nattempts are generated randomly and as part of state transitions.\nSpecifically, a base (user) feature vector is drawn from a multivariate\nGaussian that is defined per user, per discrete time step 𝑡, and per\naction 𝑎𝑡∈{𝐴, 𝐵} that was prompted by the authentication UI on\nuser state 𝑠𝑡−1. This base vector and the vector representing user\nstate 𝑠𝑡−1 are then aggregated to construct the vector for the next\nuser state 𝑠𝑡. The user engagement and corresponding action costs\non each user transition (𝑠𝑡,𝑎𝑡,𝑠𝑡+1) are represented by a single\nnormalized scalar value-reward 𝑟𝑡+1. The latter is drawn from a\nnormal distribution with randomly selected mean and variance,\ndefined per user, per time step, per action.\nThe simulated environment we introduced for system actions\nduring user authentication is used as a testbed for evaluating ML\ntechniques in Section 6. Under our formulation, the trained deci-\nsion mechanism that chooses an action for each transition aims\nto optimize and/or balance long-term objectives, i.e., cumulative\nrewards over a time horizon (Section 2.1).\n5.2\nOur practical state model and rewards\nThe problem formalization we use in practice differs from the moti-\nvational example in two aspects: (1) user state vectors are based on\nactual user features rather than drawn at random, (2) rewards are\ncrafted to approximate long-term objectives.\nUser features for state representation include real-time (contextual)\nand historical features. On-device real-time features capture the\napp platform, local time of day, day of the week, login time, etc.\nHistorical features focus on past events, such as the total number\nof user password recoveries sent in the past, time since last login,\netc. Historical features may also aggregate user data specific to the\nauthentication process, e.g., the number of authentication attempts\nin the last 30 days and their outcomes, which of Action A and\nAction B were used, and how the user reacted.\nRewards are formulated to help optimize long-term objectives that\nare affected by the authentication decision mechanism. Specifically,\nwe aim to optimize user engagement and monetary cost. In our\nWeb-based service, user engagement is reported and expressed as a\nbinary value that determines if the user was active during the day.\nMonetary cost represents the amount of money spent on password\nrecovery (Action A) after failed login attempts. Ideally, we would\nlike to avoid harming user engagement and not block users from\nlogging in, while reducing charges for Action A. To this end, we\nformulate the reward as a linear combination of user engagement\n(UE) and monetary cost (Cost), thus\n𝑟= 𝑤𝑢· UE −𝑤𝑐· Cost, where 𝑤𝑢,𝑤𝑐∈R\n(8)\nVarying the weights helps explore the Pareto curve of multiob-\njective tradeoffs relevant to the application. We assign rewards\nto the last action on each day (Figure 3) because system metrics\nfor user engagement are tallied at the end of each day. During RL\ntraining, rewards are propagated back via the Bellman equation.\n6\nAPPLYING REINFORCEMENT LEARNING\nThe problem formalization in Section 5 facilitates established ML\ntechniques, including supervised and reinforcement learning. In this\nsection we discuss the pros and cons of competing ML approaches,\nintroduce our proposed RL model, then explain how training data\nis extracted and augmented.\nFigure 3: Rewards are set daily per user, to the last action.\nPersonalization for Web-based Services\nusing Offline Reinforcement Learning\nFigure 4: Exploration diversity improves state coverage. To\nvisualize distributions of visited states in the feature space,\nwe use two principal components via PCA. P(𝐴) = 0.5 ex-\nhibits the greatest state coverage (blue), whereas P(𝐴) = 0.9\nexhibits limited coverage (red). The blue pointset is cloned.\n6.1\nCompeting ML approaches\nFor a given state vector in the feature space, an ML model can (𝑖) pre-\ndict rewards for each action via Supervised Learning or (𝑖𝑖) estimate\nQ-values via offline Reinforcement Learning (Section 3). Generally,\nReinforcement Learning offers two important advantages:\n• RL exploration policies are linked to exploitation and opti-\nmized to improve state coverage.\n• Long-term cumulative objectives, e.g., user engagement, can\nbe optimized by RL even when they are not faithfully repre-\nsented by immediate rewards for each action (Figure 3).\nThe impact of the exploration level of behavioral policies in offline\nRL is illustrated in Figure 4 that is produced for the motivational ex-\nample from Section 5.1. Here we compare (behavioral) exploration\npolicies defined by the complementary probabilities of Actions A\nand B. Specifically, using the induced state transitions and corre-\nsponding rewards as a static dataset, we train offline an RL model\nfor each behavioral policy (Section 3) through an established off-\npolicy algorithm (DQN). Intuitively, balanced exploration with equal\nprobabilities should provide better state coverage. Indeed, this is\nobserved in our simulation. In the figure, we project state vectors\nfrom the feature space onto two principal components (via PCA).\nThe yellow (70:30) and red (90:10) point clouds are narrower than\nthe blue (50:50) point cloud. Furthermore, Figure 5 shows that explo-\nration diversity in the behavioral policy helps train a better model.\nIn other words, during the evaluation phase, we can expect higher\nperceived returns (cumulative rewards) for the RL model trained\noffline with balanced exploration. Such evaluation is performed on\nusers not seen in the training dataset and by using the trained RL\nmodel as a decision mechanism that drives the state transitions.\nThe impact of behavioral policies’ exploration is related to the dis-\ntributional shift phenomemon discussed in Section 3. The higher\nthe state coverage in the training dataset, the less significant the\nstate distributional shift during evaluation and action distributional\nshift during training for an Offline RL model.\n20\n25\n30\n35\n40\n45\nNum. State Transitions\n10\n15\n20\n25\n30\nAverage Cumulative Reward\nP(a) = 0.1\nP(a) = 0.3\nP(a) = 0.5\nP(a) = 0.7\nP(a) = 0.9\nFigure 5: The impact of exploration diversity on learning\nreturns. Between Actions\nA and B, the former is chosen\nwith probability P(𝐴). The plot shows average cumulative re-\nwards over trajectories of different length. The best results\nare observed for balanced exploration P(𝐴) = 0.5.\nModeling forward sequential depth (i.e., the future impact of cur-\nrent actions) is difficult for supervised learning. Modeling backward\nsequential depth (the impact of past actions) is somewhat easier,\nusing additional features that summarize past states and actions.\nFurthermore, reinforcement learning often suffers from high vari-\nance in rewards and statistical bias in explored states.\nDespite obvious limitations, supervised learning (SL) remains a\nviable competitor to RL because\n• SL often facilitates more mature optimization methods\n• SL can train higher-quality models faster due to smaller\nvariances and greater learning rates.\nTherefore, to empirically evaluate our proposed RL method, Sec-\ntion 7 compares it to a production system trained with supervised\nlearning. Among different RL techniques, model-based RL (tree\nsearch, etc), runs into difficulties predicting state transitions based\non actions and corresponding rewards (environment dynamics).\n6.2\nRL decision models\nGiven the large state space in our application, we model the Q-\nfunction by a neural net. The values of the trained 𝑄(𝑠𝑡,𝑎𝑡;𝜃)\nexpress accumulated rewards (Equation 8) for Actions A, B based\non user features for a given state. We perform training offline on\na static dataset of state transitions using the DQN algorithm. As\na result, the task of learning a near-optimal Q-function is trans-\nformed into a familiar regression problem for Equation 7. A learned\nnear-optimal Q-function supports a decision mechanism that se-\nlects Action A or B based on current user features to maximize\naccumulated rewards. The use of Temporal Difference (Equation\n7) as the loss function is a key distinction from supervised learn-\ning. The implied use of the Bellman equation is what allows this\napproach to track long-term rewards.\nOur implementation (Section 7) makes it easy to try more sophis-\nticated RL models and algorithms with the same state model and\nrewards (Section 5.2). In fact, we have tried several improvements\nto DQN (such as DDQN, Dueling DQN) and concluded that DQN\nApostolopoulos et al.\ndoes not leave much room for improvement despite being simpler.\nHowever, more techniques that parameterize both the policy and\nthe Q-function are generally worth trying. Whether or not this\nimproves long-term objectives is unclear a priori, and we therefore\nimplement a recent technique in this category.\nCritic-Regularized Regression (CRR) [39] is an off-policy method\nthat discourages sampling low-quality actions from the training\ndataset. As it is typical for actor-critic algorithms, CRR parameter-\nizes both the policy and the Q-function [16]. It additionally trans-\nforms Q-values in the objective (Equation 1) with a monotonically\nincreasing function, such as exp(·), to emphasize higher Q-values.\nThis way, during policy update, 𝜋more often samples high-quality\nactions within the training distribution. The critic is trained using\ndistributional Q-learning [4, 39]. The distributional representation\nof the returns translates well into stochastic behavioral policies.\n6.3\nPreparing data for training\nEach row of our training data includes user features, actual system\nactions, and rewards.\nData extraction. Motivated by the impact of the exploration level\nof behavioral policies in Offline RL (Section 6.1) we choose equal\nprobabilities for Actions A and B. Thus, upon a failed login attempt,\nan action is chosen at random. Another handler starts extracting\nand computing user features simultaneously. The two asynchro-\nnous events are tagged with the same unique ID and joined by a\nstream processing system for logging in the training table. The\nuser engagement metric and the sum of OTP fees are computed\nby the end of the day. These rewards are joined with the training\ntable based on an anonymized user ID column, where rewards are\nattributed to the last login failure event of the day (see Figure 3).\nWe logged data using the described exploratory behavioral policy\nfor a time period of three weeks.\nData augmentation. Following the standard Markov Decision\nProcess (MDP) framework, RL models are trained on consecutive\npairs of state/action tuples that correspond to state transitions in\nuser sequences (Figure 2). We use the open-source applied Rein-\nforcement Learning platform ReAgent (Horizon) [9] to transform\nlogged state-action-reward data to the following row format:\n• MDP ID: a unique id, e.g., anonymized user ID, for the Markov\nDecision Process chain.\n• Sequence Number: a number representing the depth of the\nstate in the MDP, e.g., the timestamp of the state.\n• State Features: the user features for state representation\nof the current step.\n• Action: the actual system action.\n• Action Probability: the probability that the current sys-\ntem took the action logged.\n• Metrics: user engagement metric and the sum of OTP fees.\n• Possible Actions: an array of possible actions at the cur-\nrent step.\n• Next State Features: the user features for state represen-\ntation of the subsequent step.\n• Next Action: the actual system action at the next step.\n• Sequence Number Ordinal: a number representing the\ndepth of the state in the MDP after converting the Sequence\nNumber to an ordinal number.\n• Time Diff: a number representing the time difference be-\ntween the current and next state.\n• Possible Next Actions: an array of actions that were\npossible at the next step.\nThe metrics map enables reward shaping for Equation 8 by tuning\n𝑤𝑢,𝑤𝑐during training. Offline RL training is enabled by prepro-\ncessing logic for the row format above.\n7\nEMPIRICAL EVALUATION\nTo evaluate ML techniques from Section 6, our workflow first as-\nsesses quality of RL models trained offline. Hyperparameter tuning\nis performed based on offline metrics. For the best seen RL model,\nonline evaluation on live data helps us compare to a baseline pro-\nduction solution that uses SL. Our evaluation methodology and\nespecially the metrics are applicable beyond this work.\n7.1\nOffline training and evaluation for RL\nTo support decision models from Section 6.2, we train our DQN\nmodel using the ReAgent (Horizon) platform [9] and its default\nneural network architecture. We set the discount factor to 𝛾= 1.0\nbecause most MDP sequences are short (80% of them have a single\nstep). Equation 8 is used for the reward function.\nOffline RL training minimizes Temporal Difference loss (Equation\n7) over the provided static dataset of transitions (Section 6.3). Figure\n6 shows how loss values for the DQN RL model (Section 6.2) change\nover training epochs that consist of mini-batches (iterations). To\navoid overfitting, we split the static dataset of transitions into a\ntraining and test set, then compare the average Q-values of the\nsame Actions A, B between the training and test sets, and keep\nthe differences below 10%.\nOffline evaluation avoids the dangers of poorly trained policies\nin production. Additionally, it uses a static dataset for consistent\nexperimentation and performance tuning. To watch out for defi-\ncient training and distributional shifts (Section 3), our evaluation\nmethodology pays particular attention to training quality metrics.\n• The Temporal Difference Ratio addresses a pitfall in using\nraw Temporal Difference (TD) loss (Equation 7), which is\nsensitive to the magnitude of Q-values. A poor RL model\nwill exhibit low TD values when Q-values are low. Instead,\nwe evaluate the metric\nTD\nmin{ ¯𝑄𝐴, ¯𝑄𝐵} , where ¯𝑄𝐴, ¯𝑄𝐵are the\naverage Q-values of Actions A, B over the training dataset.\nFigure 6: Temporal Difference loss (MSE) with respect to\nlearning epochs for the DQN RL model (Section 6.2).\nPersonalization for Web-based Services\nusing Offline Reinforcement Learning\n• Distributional stability during training is vital to effective\nlearning. The action distribution of the trained offline RL\nmodel should not shift too far from the behavioral policy,\nwhich we measure as the KL-divergence [12] between the RL\nmodel’s learned policy and the behavioral policy function.\nThe action distribution of our DQN model in Figure 7 sta-\nbilizes over training. The change of the action distribution\nover a time window (25 training iterations) is a good proxy\nfor the distributional stability for offline RL models.\nCounterfactual Policy Evaluation (CPE) with respect to per-\nceived accumulated reward is the second tier of our evaluation\nmethodology. CPE is performed offline (because our RL model is\ntrained offline) and off-policy because it uses a static dataset of\ntransitions D𝜋𝛽, logged by the behavioral policy 𝜋𝛽. The key idea\nis to answer counterfactual questions of the form: “how probable\nwould it be for the offline policy to encounter/perceive same state\ntransitions/rewards as the logged ones?\". To this end, CPE provides\na safe way of ranking different offline RL models before they are\ntested in production. Off-policy evaluation can be based on ordinary\nimportance sampling [25], but at risk of high variance which grows\nexponentially with the number of state transitions. More sophisti-\ncated methods [34] provide controlled bias-variance tradeoffs by\nutilizing additional information, e.g., the learned Q-function. In our\napplication, relatively short MDP sequences make it practical to\nuse the sequential doubly robust estimator [13] to produce unbiased\nperformance estimates for policies trained offline.\nHyperparameter optimization for Offline RL is challenging [23],\nas there is no access to the environment. Hyperparameters can be\noptimized toward the metrics in our two-tier offline evaluation\nabove, after which the best seen models can be evaluated online.\nFor example, parameters for training, such as the learning rate,\ncan be optimized via the first tier, whereas the second tier can\ncompare different RL algorithms. Our DQN model (Figures 6 and 7)\nexhibited the best results in offline evaluation, was deployed online\nand evaluated vs. SL baseline production system (Section 7.2).\n7.2\nEmpirical comparisons\nOur baseline using supervised learning is a prior production\nmodel with a tree-based meta-learner that estimates the Condi-\ntional Average Treatment Effects [18]. The treatment effects are\nrepresented by a linear combination of the rewards defined in Equa-\ntion 8. The meta-learner decomposes the decision process into\ntwo steps by first estimating the conditional reward expectations,\nFigure 7: Changes in action distributions for Actions A, B\nover learning epochs for the DQN RL model (Section 6.2).\nTable 1: SL and RL policies are compared online to fixed poli-\ncies that always perform Action A or Action B. RL provides\nthe most attractive tradeoffs, shown in bold.\nUser engagement\nOTP cost\nPolicy\nDaily\nMonthly\nB\n-\n-\n-\nA\n+1.49% ± 0.273%\n+2.81% ± 0.279%\n+120% ± 4.16%\nSL\n+1.35% ± 0.212%\n+2.58% ± 0.116%\n+81.1% ± 3.13%\nRL\n+1.50% ± 0.213%\n+2.55% ± 0.183%\n+70.3% ± 2.96%\n𝑈= E[UE | 𝑎𝑡,𝑠= 𝑠𝑡], 𝐶= E[Cost | 𝑎𝑡,𝑠= 𝑠𝑡], where 𝑎𝑡∈𝐴. 𝑈\nand 𝐶are computed using Gradient Boosted Decision Trees whose\nparameters, including learning rate, tree depth and tree leaves, are\nswept in a large range to minimize mean squared errors. Then the\nlearner takes the differences between the estimates and chooses\nthe action that gives the highest reward:\n𝑎𝑡= arg max\n𝑎𝑡∈𝐴\nE[𝑟𝑡|𝑎𝑡,𝑠= 𝑠𝑡]\n(9)\nAfter deriving 𝑟𝑡using Equation 8, the final action is given by\n𝑎𝑡= arg max\n𝑎𝑡∈𝐴\n(𝑤𝑢· 𝑈−𝑤𝑐· 𝐶)\n(10)\nThe reward weights 𝑤𝑢and 𝑤𝑐are chosen through online experi-\nments to ensure compelling multiobjective tradeoffs.\nOnline evaluation is performed using one-month’s data. Table 1\ndemonstrates the first round of results at a 95% confidence level,\nwhere our RL model is trained on randomized data as per Section\n6.3. The RL model is compared to fixed policies (the same action\nrepeated always) and a prior production model based on SL. The\nRL model significantly reduces OTP costs while exhibiting com-\npetitive daily and monthly user engagement results. We estimate\nReturn On Investment (ROI) by dividing total cost by total monthly\nengagement. RL outperforms SL by 5.93% ± 0.782%. To collect\ndata for recurrent training, we deploy our RL agent by extending\nthe deterministic policy to an 𝜖-greedy policy with 𝜖= 0.1.\nWhen ML is applied to practical problems, it commonly opti-\nmizes surrogate objectives, and one has to empirically check that\npractical objectives are improved as a result. Moreover, sophisti-\ncated applications often track multiple performance metrics whose\nregressions may block new optimization. In our case, one such\nmetric counts notification disavow events (NDEs) for password reset,\nwhere a user turns off notifications, perhaps because there were\ntoo many. Comparing to SL, we observe that RL reduces NDEs by\n50%. To this end, Table 2 shows that the RL agent trades off Action\nA for Action B and thus reduces authentication messages, while\nmaintaining neutral engagement metrics. We also group users into\ncohorts — single-login users and multiple-login users (within the\nTable 2: Action distributions (% of Action A) for behavioral\npolicies trained with SL and RL by user cohort.\nSL\nRL\nAll users\n55.88%\n22.75%\nSingle-login users\n60.16%\n36.41%\nMultiple-login users\n49.40%\n11.19%\nApostolopoulos et al.\nFigure 8: Daily user engagement of a recurrent CRR model\nevaluation period). Intuitively, multiple-login users are more likely\nto enter their password correctly without help from our authenti-\ncation messages. From Table 2, we see that both RL and SL make\nuse of this user attribute but RL exploits it more efficiently\nRecurrent training and stability evaluation. When user behav-\niors shift over time, ML models must be refreshed using the most\nrecent data (recurrent training). The initial RL model trained on\nrandomized data shows sizable improvements. However, continued\ncollection of randomized data is risky in terms of direct costs and\nuser experience. The \"Rand\" policy (Section 6.1) in Table 3 suggests\nthat its deployment limits the benefits of RL models. Thus, we re-\nfresh the model using behavioral data collected from our RL agent.\nAdditionally, to check if recurrent training stabilizes in the long\nterm compared with the initial RL model, we perform a second\nround of online evaluation to study recurrent training using RL\nbehavioral data and model stability in the long term.\nWe train a DQN and a CRR model (Section 6.2) with the same\nparameters and rewards as the initial RL model, but collect their\ntraining data via policies from the deployed RL agent. Results sum-\nmarized in Table 3 use the same settings as before (a 95% confidence\nlevel, one-month’s testing window, and an 𝜖-greedy policy on DQN\nand CRR with 10% exploration rate). Both recurrent DQN and CRR\ngive neutral results compared with the initial RL model, which indi-\ncates stable performance in recurrent training. Figure 8 additionally\nshows daily user engagement for the CRR recurrent model vs. the\ninitial RL model, and this also shows consistent performance.\n8\nCONCLUSIONS\nIn this work we show how to apply reinforcement learning (RL) to\npersonalize user authentication in a Web-based system and compare\nRL to a competing approach based on supervised learning. Working\nwith industry data, using offline RL avoids releasing poorly trained\nTable 3: Recurrent training results compared with the ini-\ntial RL agent trained on randomized data (the \"Rand\" policy\nissues Action A and Action B with equal probability).\nUser engagement\nOTP cost\nPolicy\nDaily\nMonthly\nDQN\n+0.0023% ±0.106%\n+0.0576% ±0.0841%\n-0.166% ±0.439%\nCRR\n+0.116% ±0.240%\n+0.107% ±0.140%\n+1.27% ±2.87%\nRand\n-0.336% ±0.266%\n-0.693% ±0.162%\n+3.78% ±3.29%\nagents in production. However, this approach requires careful ex-\ntraction and augmentation of training data to ensure that off-policy\nlearning does not succumb to distributional shift. In practice, RL\nis often susceptible to high variance and high bias at several of its\nstages, especially when operating on large-scale live data. Fortu-\nnately, our method is sufficiently robust for production deployment\nusing the ReAgent (Horizon) platform [9]. Starting from state mod-\neling and data collection, we articulate obstacles and milestones\ntoward model training and demonstrate practical improvement\nin end-to-end system-level metrics at a large-scale deployment\nat Facebook. During development, we use a simplified problem\nenvironment to test intuition without sensitive data.\nBroader applications. Our self-contained didactic application is\nnot only critical to many Web-based systems, but also generalizes\nwell (e.g., to more than two actions at a time) and illustrates how\nother aspects of Web-based systems can be personalized and en-\nhanced via ML. Relevant optimizations explored previously include\n• personalized individual user notifications [9],\n• page prefetching to optimize user experience [37].\nUser notifications and product delivery must balance utility with\ndistraction, while page prefetches improve access latency at the cost\nof increased network bandwidth. These applications bear structural\nsimilarity to our work and share several salient aspects.\n(1) At each step, the system chooses from several actions. Future\ndecisions are improved based on user feedback.\n(2) One must optimize and/or balance long-term cumulative\nobjectives, some of which do not reduce to rewards for indi-\nvidual actions that can be handled by supervised learning.\n(3) Personalization is based on a number of user features and\ncan be supported by ML models in the feature space.\nMore general RL methods. Our straightforward RL model (DQN)\nis facilitated by the small action space, which (𝑖) is less demanding\nin terms of learning robustness, and (𝑖𝑖) allows for simpler neural-\nnet representations where separate outputs produce 𝑄(𝑠,𝑎) values\nfor different actions 𝑎. However, the overall approach generalizes\nto more sophisticated settings and can be extrapolated to other\npersonalized enhancements for Web-based systems as follows.\n• We use model-free RL without predicting future states and\naction rewards, but such predictions can be leveraged.\n• Our extraction and handling of user features, as well as\noffline model-free training and the evaluation methodology\nare not tied to specific RL models.\n• Our use of the open-source ReAgent (Horizon) platform [9]\nmakes it easy to employ models such as Double DQN [36]\n(DDQN) and Dueling DQN [40] that could provide a more\nstable learning for larger action spaces.\nFor very large or continuous action spaces, 𝑄(𝑠,𝑎) can be modeled\nas a function of both 𝑠and 𝑎[21]. Continuous action spaces can be\naddressed using policy gradient methods [32], available in ReAgent.\nPersonalization for Web-based Services\nusing Offline Reinforcement Learning\nREFERENCES\n[1] Deepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, and Liang Zhang.\n2014. LASER: A Scalable Response Prediction Platform for Online Advertising.\nIn Proc. 7th ACM International Conference on Web Search and Data Mining (New\nYork, NY) (WSDM ’14). ACM, New York, NY, 173–182. https://doi.org/10.1145/\n2556195.2556252\n[2] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. 2020. An Op-\ntimistic Perspective on Offline Reinforcement Learning. In Proc. 37th Inter-\nnational Conference on Machine Learning (Proc. Machine Learning Research,\nVol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, Virtual, 104–114. http:\n//proceedings.mlr.press/v119/agarwal20c.html\n[3] Akshay Awasthi. 2015. Reducing Identity Theft Using One-Time Passwords and\nSMS. EDPACS 52, 5 (2015), 9–19.\n[4] Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan\nHorgan, TB Dhruva, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. 2018.\nDistributed Distributional Deterministic Policy Gradients.\n[5] Richard Bellman. 1957. Dynamic Programming (1 ed.). Princeton University\nPress, Princeton, NJ, USA.\n[6] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. 2019. Challenges of\nReal-World Reinforcement Learning. arXiv:1904.12901 [cs.LG]\n[7] Florent Garcin, Christos Dimitrakakis, and Boi Faltings. 2013. Personalized\nNews Recommendation with Context Trees. In Proc. 7th ACM Conference on\nRecommender Systems (Hong Kong, China) (RecSys ’13). ACM, New York, NY,\n105–112. https://doi.org/10.1145/2507157.2507166\n[8] Florent Garcin and Boi Faltings. 2013. PEN Recsys: A Personalized News Rec-\nommender Systems Framework. In Proc. 2013 International News Recommender\nSystems Workshop and Challenge (Kowloon, Hong Kong) (NRS ’13). ACM, New\nYork, NY, 3–9. https://doi.org/10.1145/2516641.2516642\n[9] Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He, Zachary\nKaden, Vivek Narayanan, and Xiaohui Ye. 2018. Horizon: Facebook’s Open Source\nApplied Reinforcement Learning Platform. CoRR abs/1811.00260 (2018), 10pp.\narXiv:1811.00260 http://arxiv.org/abs/1811.00260\n[10] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2016.\nSession-based Recommendations with Recurrent Neural Networks.\narXiv:1511.06939 [cs.LG]\n[11] Daniel Sik Wai Ho, William Schierding, Melissa Wake, Richard Saffery, and Justin\nO’Sullivan. 2019. Machine learning SNP based prediction for precision medicine.\nFrontiers in Genetics 10 (2019), 267.\n[12] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson,\nAgata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2019. Way\nOff-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences\nin Dialog. arXiv:1907.00456 [cs.LG]\n[13] Nan Jiang and Lihong Li. 2016. Doubly Robust Off-policy Value Evaluation for\nReinforcement Learning. In Proceedings of The 33rd International Conference on\nMachine Learning (Proceedings of Machine Learning Research, Vol. 48), Maria Flo-\nrina Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York, NYA, 652–661.\nhttp://proceedings.mlr.press/v48/jiang16.html\n[14] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric\nJang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,\nand Sergey Levine. 2018. Scalable Deep Reinforcement Learning for Vision-\nBased Robotic Manipulation. In Proc. 2nd Conference on Robot Learning (Proc.\nMachine Learning Research, Vol. 87), Aude Billard, Anca Dragan, Jan Peters, and\nJun Morimoto (Eds.). PMLR, Virtual, 651–673. http://proceedings.mlr.press/v87/\nkalashnikov18a.html\n[15] Ron Kohavi and Roger Longbotham. 2017. Online Controlled Experiments and\nA/B Testing. Encyclopedia of Machine Learning and Data Mining 7, 8 (2017),\n922–929.\n[16] Vijay Konda and John Tsitsiklis. 2000. Actor-Critic Algorithms. In Advances\nin Neural Information Processing Systems, S. Solla, T. Leen, and K. Müller (Eds.),\nVol. 12. MIT Press, Denver, CO, 1008–1014. https://proceedings.neurips.cc/paper/\n1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf\n[17] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. 2019.\nStabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. In Ad-\nvances in Neural Information Processing Systems, H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran\nAssociates, Inc., Virtual, 11784–11794.\nhttps://proceedings.neurips.cc/paper/\n2019/file/c2073ffa77b5357a498057413bb09d3a-Paper.pdf\n[18] Sören R Künzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. Metalearners for\nestimating heterogeneous treatment effects using machine learning. Proceedings\nof the National Academy of Sciences 116, 10 (2019), 4156–4165.\n[19] Ricardo Margarito Ledesma. 2020. Systems and methods for one-time password\nauthentication. US Patent App. 16/918,742.\n[20] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.\nNeural Attentive Session-Based Recommendation. In Proc. 2017 ACM on Confer-\nence on Information and Knowledge Management (Singapore, Singapore) (CIKM\n’17). ACM, New York, NY, 1419–1428. https://doi.org/10.1145/3132847.3132926\n[21] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom\nErez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. Continuous control\nwith deep reinforcement learning. arXiv:1509.02971 [cs.LG]\n[22] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing Atari with\nDeep Reinforcement Learning. arXiv:1312.5602 [cs.LG]\n[23] Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna,\nAlexander Novikov, Ziyu Wang, and Nando de Freitas. 2020. Hyperparameter\nSelection for Offline Reinforcement Learning. arXiv:2007.09055 [cs.LG]\n[24] Bruno Pradel, Savaneary Sean, Julien Delporte, Sébastien Guérif, Céline Rouveirol,\nNicolas Usunier, Françoise Fogelman-Soulié, and Frédéric Dufau-Joel. 2011. A\nCase Study in a Recommender System Based on Purchase Data. In Proc. 17th\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n(San Diego, California, USA) (KDD ’11). ACM, New York, NY, 377–385. https:\n//doi.org/10.1145/2020408.2020470\n[25] Doina Precup, Richard S. Sutton, and Satinder P. Singh. 2000. Eligibility Traces\nfor Off-Policy Policy Evaluation. In Proceedings of the Seventeenth International\nConference on Machine Learning (ICML ’00). Morgan Kaufmann Publishers Inc.,\nSan Francisco, CA, USA, 759–766.\n[26] Martin L Puterman. 2014. Markov decision processes: discrete stochastic dynamic\nprogramming. John Wiley & Sons, Virtual.\n[27] Paul Rockwell. 2016. Two factor authentication using a one-time password. US\nPatent 9,378,356.\n[28] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.\n2015. Trust Region Policy Optimization. In Proc. 32nd International Conference on\nMachine Learning (Proc. Machine Learning Research, Vol. 37), Francis Bach and\nDavid Blei (Eds.). PMLR, Lille, France, 1889–1897. http://proceedings.mlr.press/\nv37/schulman15.html\n[29] Mohit Kr Sharma and Manisha J Nene. 2020. Two-factor authentication using\nbiometric based quantum operations. Security and Privacy 3, 3 (2020), e102.\n[30] Noah Y. Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,\nMichael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin\nRiedmiller. 2020. Keep Doing What Worked: Behavioral Modelling Priors for\nOffline Reinforcement Learning. arXiv:2002.08396 [cs.LG]\n[31] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Intro-\nduction. A Bradford Book, Cambridge, MA, USA.\n[32] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999.\nPolicy Gradient Methods for Reinforcement Learning with Function Approxi-\nmation. In Proc. 12th International Conference on Neural Information Processing\nSystems (Denver, CO) (NIPS’99). MIT Press, Cambridge, MA, USA, 1057–1063.\n[33] Xueying Tang, Yunxiao Chen, Xiaoou Li, Jingchen Liu, and Zhiliang Ying. 2019.\nA reinforcement learning approach to personalized learning recommendation\nsystems. Brit. J. Math. Statist. Psych. 72, 1 (2019), 108–135.\n[34] Philip Thomas and Emma Brunskill. 2016. Data-Efficient Off-Policy Policy Evalu-\nation for Reinforcement Learning. In Proceedings of The 33rd International Con-\nference on Machine Learning (Proceedings of Machine Learning Research, Vol. 48),\nMaria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York, NY,\n2139–2148. http://proceedings.mlr.press/v48/thomasa16.html\n[35] H. Toğuç and R. S. Kuzu. 2020. Hybrid Models of Factorization Machines with\nNeural Networks and Their Ensembles for Click-through Rate Prediction. In 2020\n5th International Conf. on Computer Science and Engineering (UBMK). ICEECS,\nHangzhou, China, 31–36. https://doi.org/10.1109/UBMK50275.2020.9219371\n[36] Hado van Hasselt, Arthur Guez, and David Silver. 2015. Deep Reinforcement\nLearning with Double Q-learning. arXiv:1509.06461 [cs.LG]\n[37] Hanson Wang, Zehui Wang, and Yuanyuan Ma. 2020.\nPredictive Precom-\npute with Recurrent Neural Networks. In Proc. Machine Learning and Sys-\ntems, I. Dhillon, D. Papailiopoulos, and V. Sze (Eds.), Vol. 2. mlsys.org,\nSan Jose, CA, 470–480.\nhttps://proceedings.mlsys.org/paper/2020/file/\n8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf\n[38] Zihan Wang, Ziheng Jiang, Zhaochun Ren, Jiliang Tang, and Dawei Yin. 2018. A\nPath-Constrained Framework for Discriminating Substitutable and Complemen-\ntary Products in E-Commerce. In Proc. Eleventh ACM International Conference on\nWeb Search and Data Mining (Marina Del Rey, CA, USA) (WSDM ’18). ACM, New\nYork, NY, 619–627. https://doi.org/10.1145/3159652.3159710\n[39] Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg,\nScott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nico-\nlas Heess, and Nando de Freitas. 2020.\nCritic Regularized Regression.\narXiv:2006.15134 [cs.LG]\n[40] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando\nFreitas. 2016. Dueling Network Architectures for Deep Reinforcement Learning.\nIn Proc. 33rd International Conference on Machine Learning (Proc. Machine Learning\nResearch, Vol. 48), Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR,\nNew York, NY, 1995–2003. http://proceedings.mlr.press/v48/wangf16.html\n[41] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning\n8, 3-4 (1992), 279–292.\n[42] Yifan Wu, George Tucker, and Ofir Nachum. 2019. Behavior Regularized Offline\nReinforcement Learning. arXiv:1911.11361 [cs.LG]\n",
  "categories": [
    "cs.LG",
    "cs.HC",
    "cs.SE"
  ],
  "published": "2021-02-10",
  "updated": "2021-02-10"
}