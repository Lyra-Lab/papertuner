{
  "id": "http://arxiv.org/abs/2102.05710v1",
  "title": "Derivative-Free Reinforcement Learning: A Review",
  "authors": [
    "Hong Qian",
    "Yang Yu"
  ],
  "abstract": "Reinforcement learning is about learning agent models that make the best\nsequential decisions in unknown environments. In an unknown environment, the\nagent needs to explore the environment while exploiting the collected\ninformation, which usually forms a sophisticated problem to solve.\nDerivative-free optimization, meanwhile, is capable of solving sophisticated\nproblems. It commonly uses a sampling-and-updating framework to iteratively\nimprove the solution, where exploration and exploitation are also needed to be\nwell balanced. Therefore, derivative-free optimization deals with a similar\ncore issue as reinforcement learning, and has been introduced in reinforcement\nlearning approaches, under the names of learning classifier systems and\nneuroevolution/evolutionary reinforcement learning. Although such methods have\nbeen developed for decades, recently, derivative-free reinforcement learning\nexhibits attracting increasing attention. However, recent survey on this topic\nis still lacking. In this article, we summarize methods of derivative-free\nreinforcement learning to date, and organize the methods in aspects including\nparameter updating, model selection, exploration, and parallel/distributed\nmethods. Moreover, we discuss some current limitations and possible future\ndirections, hoping that this article could bring more attentions to this topic\nand serve as a catalyst for developing novel and efficient approaches.",
  "text": "Derivative-Free Reinforcement Learning: A Review\nHong Qian, Yang Yu∗\nNational Key Laboratory for Novel Software Technology, Nanjing University, China\nqianh@lamda.nju.edu.cn, yuy@nju.edu.cn\n∗Corresponding author.\nAbstract\nReinforcement learning is about learning agent models that make the best sequential deci-\nsions in unknown environments. In an unknown environment, the agent needs to explore\nthe environment while exploiting the collected information, which usually forms a sophis-\nticated problem to solve. Derivative-free optimization, meanwhile, is capable of solving so-\nphisticated problems. It commonly uses a sampling-and-updating framework to iteratively\nimprove the solution, where exploration and exploitation are also needed to be well bal-\nanced. Therefore, derivative-free optimization deals with a similar core issue as reinforce-\nment learning, and has been introduced in reinforcement learning approaches, under the\nnames of learning classiﬁer systems and neuroevolution/evolutionary reinforcement learn-\ning. Although such methods have been developed for decades, recently, derivative-free re-\ninforcement learning exhibits attracting increasing attention. However, recent survey on\nthis topic is still lacking. In this article, we summarize methods of derivative-free reinforce-\nment learning to date, and organize the methods in aspects including parameter updating,\nmodel selection, exploration, and parallel/distributed methods. Moreover, we discuss some\ncurrent limitations and possible future directions, hoping that this article could bring more\nattentions to this topic and serve as a catalyst for developing novel and efﬁcient approaches.\n1\nIntroduction\nReinforcement learning [Sutton and Barto, 1998; Wiering and van Otterlo, 2012] aims to enable\nagents to automatically learn the policy with the maximum long-term reward via interactions\nwith environments. It has been listed as one of the four research directions of machine learning\nby Professor T. G. Dietterich [Dietterich, 1997]. In recent years, with the fusion of deep learn-\ning and reinforcement learning, deep reinforcement learning has made remarkable progress\nand attracted more and more attention from both the academic and industrial community. To\nname a few, the deep Q-network (DQN) [Mnih et al., 2015] proposed by DeepMind reaches\n1\narXiv:2102.05710v1  [cs.LG]  10 Feb 2021\nthe human-level control in Atari games, AlphaGo [Silver et al., 2016] proposed also by Deep-\nMind defeats the top human experts in the Go game, and AlphaZero [Silver et al., 2018] defeats\na world champion program in the games of chess, shogi and Go without the domain knowl-\nedge except the game rules. Due to the strong ability of reinforcement learning, it has been\napplied in automatic control [Abbeel et al., 2006], automatic machine learning [Zoph and Le,\n2017a], computer vision [Huang et al., 2017], natural language processing [Yu et al., 2017],\nscheduling [Wang and Usher, 2005], ﬁnance [Choi et al., 2009], commodity search [Shi et al.,\n2019], and network communication [Boyan and Littman, 1993], etc. In the ﬁeld of cognition\nand neuroscience, reinforcement learning also has important research value [Frank et al., 2004;\nSamejima et al., 2005].\nHowever, as reinforcement learning is being applied to more realistic problems, the com-\nplexity of ﬁnding out an optimal or satisfactory policy is also increasing. The optimization\nproblems encountered in reinforcement learning, especially deep reinforcement learning, are\noften quite sophisticated. For such optimization problems, standard gradient-based optimiza-\ntion methods may suffer from the difﬁculties of stationary point issues (e.g., a plethora of sad-\ndle points or spurious local optima), bad condition number, or ﬂatness in the activations that\ncould lead to the gradient vanishing problem [Shalev-Shwartz et al., 2017]. These difﬁculties\ncannot be neglected since they could result in the unsatisfactory performance of gradient-based\noptimization methods. Therefore, more effective policy learning methods that could make up\nfor the shortcomings of gradient-based ones are quite appealing.\nDerivative-free optimization [Conn et al., 2009; Kolda et al., 2003; Rios and Sahinidis,\n2013], also termed as zeroth-order or black-box optimization, involves a kind of optimization\nalgorithms that do not rely on the gradient information. Given a function f deﬁned over a contin-\nuous, discrete, or mixed search space X, it only relies on the objective function value (or ﬁtness\nvalue) f(x) on the sampled solution x. Since the conditions of using derivative-free algorithms\nare relaxed, they are easy to use and suitable for dealing with the sophisticated optimization\ntasks, e.g., non-convex and non-differentiable objective functions. Moreover, derivative-free\noptimization commonly uses a sampling-and-updating framework to iteratively improve the\nquality of solutions. One of the key issues is to balance the exploration and exploitation in the\nsearch space. This key issue is in a quite similar situation of reinforcement learning. Therefore,\nthe fusion of derivative-free optimization and reinforcement learning, termed as derivative-free\nreinforcement learning in this paper, has many potentialities.\nDerivative-free reinforcement learning has been developed for decades, under the names of\nlearning classiﬁer systems (e.g., [Sigaud and Wilson, 2007]) and neuroevolution/evolutionary\nreinforcement learning (e.g., [Moriarty et al., 1999; Whiteson, 2012]). Although it has some\nhistory, we have noticed that derivative-free reinforcement learning attracts increasing attentions\nrecently. This article reviews some recent advances of derivative-free reinforcement learning in\n2\noptimization, exploration and computation. In optimization, the article organizes the aspects\ninto parameter updating and model selection. In exploration, the article presents the recently\nproposed derivative-free exploration methods in reinforcement learning. In computation, the\narticle reviews the parallel and distributed derivative-free reinforcement learning approaches.\nWe also discuss some limitations and potential future directions of derivative-free reinforcement\nlearning.\nThe rest of this article is organized as follows. In Section 2, the article presents the back-\nground and organizes the aspects into introducing the basic concepts of reinforcement learning\nas well as derivative-free optimization. In Section 3, the article presents the relationship between\nreinforcement learning and derivative-free optimization from the aspects of optimization, explo-\nration, and computation. This section also discusses why they should be considered together and\nhow to combine them. The recent progress in derivative-free reinforcement learning from the\naspect of optimization is reviewed in Section 4 and 5. Speciﬁcally, in Section 4, the derivative-\nfree model parameter updating in reinforcement learning is reviewed according to the different\noptimization methods. And in Section 5, the article presents the derivative-free model selection\nin reinforcement learning. In Section 6, the derivative-free exploration in reinforcement learn-\ning is reviewed. In Section 7, the article presents the parallel and distributed derivative-free\nreinforcement learning. At last, in Section 8, we conclude the paper, and discuss some current\nlimitations and possible future concerns of this direction.\n2\nBackground\nThis section introduces the background of reinforcement learning and derivative-free optimiza-\ntion. The fusion of them could be an effective way of tackling the hard policy search problems\nin reinforcement learning.\n2.1\nReinforcement learning\nReinforcement learning (RL) [Sutton and Barto, 1998; Wiering and van Otterlo, 2012] is an\nimportant direction in machine learning [Dietterich, 1997]. It aims to enable an agent to auto-\nmatically learn the policy with the maximum long-term reward via interactions with an envi-\nronment. An illustration of interaction structure of reinforcement learning is shown in Figure 1.\nWhen an agent is put in an unknown environment, it is told of the action space A in which there\nare actions it can choose to take, and the state space S in which its observation is contained in.\nBoth S and A can be discrete or continuous. The agent has a policy π to determine which action\na ∈A is chosen at a state s ∈S. Generally, a deterministic policy π is a mapping from the state\nspace S to the action space A, i.e., a = π(s); while a stochastic policy π chooses the action a\n3\nAgent\nEnvironment\nAction\nState\nReward\nFigure 1: Interaction structure of reinforcement learning.\naccording to the probability distribution π(a|s) at the state s with the constraints\n∀a ∈A, π(a|s) ≥0\nand\nX\na∈A\nπ(a|s) = 1.\n(1)\nWhen the agent takes one action at at a state st in time step t, the unknown environment typically\nresponds by transiting to the next state st+1 and feeds back a reward signal. The (unknown)\ntransition function can be represented as a distribution P(st+1|st, at), and the (unknown) reward\nfunction can be represented as rt = R(st, at).\nThe agent explores the environment according to a policy π by interactions with the envi-\nronment. Starting from the initial state s0 ∼ρ0(·), where ρ0(·) is the start state distribution, the\ninteraction trajectory τ (also frequently termed as episode or roll-out) is\ns0, a0 ∼π(·|s0), s1 ∼P(·|s0, a0), r0 = R(s0, a0),\na1 ∼π(·|s1), . . .\n(2)\nThe quality of the policy π is then evaluated from interaction trajectories, as the expected sum\nup of the reward along the trajectories. Commonly, the expected total reward, or expected return,\nof the policy π starting from the state s is the expectation over the policy distribution and the\ntransition distribution, e.g.,\nV π(s) = E\n\u0002 +∞\nX\nt=0\nγtrt|s0 = s\n\u0003\n,\n(3)\nwhere γ ∈[0, 1) is the discount factor.\nNote that reinforcement learning setting shares the key components < A, S, P, R, γ > with\nMarkov decision process (MDP) [Bellman, 1957], except that all the functions are known un-\nder the deﬁnition of an MDP. Also note that dynamic programming is a classical and effective\nmethod to solve the best policy in MDPs. There are thus two branches of approaches, model-\nbased ones that recover the transition distribution and reward function to form up a complete\nMDP for using classical solvers, and model-free ones that learn the policy without recover-\ning the MDP. In this paper, we mainly focus on model-free approaches. Nevertheless, model-\nassisted model-free reinforcement learning is a promising direction.\n4\nIn model-free reinforcement learning, the policy can be derived from well learned value\nfunctions, or can be learned directly. In either ways, the general steps of the learning approaches\nare the same, i.e., iterating between exploring the environment and updating the policy. The\nexploration is typically implemented by executing the policy with added noise, such as ϵ-greedy\nand Gibbs sampling. In this way, the probability of executing every action and visiting every\nstate is non-zero, and thus diverse trajectory data can be generated for the next step of policy\nupdate.\nTo learn the value function, the most popular way might be the temporal difference up-\ndate [Sutton and Barto, 1998], which is essentially an incremental update rule of arithmetic\nsum\nV π(st) ←V π(st) + δ ·\n\u0000rt + V π(st+1) −V π(st)\n\u0001\n,\n(4)\nwhere δ > 0 denotes the step size. After obtaining the value function, the policy is derived\nsimply as that taking the action of the largest value, i.e., π(s) = arg maxa Qπ(s, a), where\nQπ(s, a) = R(s, a) + Es′∼P(·|s,a)V π(s′). V π(s) and Qπ(s, a) are called value functions.\nThe value function based methods may face the problem of policy degradation [Bartlett\nand Baxter, 2001]. That is to say, a more accurate estimation of the value function may not\nguarantee a better policy. Another kind of approach is to learn the policy directly, i.e., policy\nsearch. In policy search, the policy model is ﬁrstly parameterized, such as softmax policy for\ndiscrete action space\nπθ(a|s) =\nexp\n\u0000hθ(s, a)\n\u0001\nP\na′∈A exp\n\u0000hθ(s, a′)\n\u0001,\n(5)\nand Gaussian policy for continuous action space\nπθ(a|s) =\n1\n√\n2πσ exp\n\u0012\n−(a −hθ(s))2\nσ2\n\u0013\n,\n(6)\nwhere hθ(s, a) in Eq. (5) and hθ(s) in Eq. (6) with parameter θ can be linear, and are nowadays\noften (deep) neural network models. Note that this parametric policy model is fully differen-\ntiable with respect to (w.r.t.) θ, if hθ is differentiable. Then, consider the objective of learning\nan optimal policy π∗= πθ∗that can maximize the total reward J(θ), i.e.,\nθ∗= arg max\nθ∈Θ\nJ(θ),\n(7)\nwhere Θ denotes the parameter space. We suppose that both the environment transitions and the\npolicy are stochastic. In episodic environments, J(θ) can be trajectory-wise total reward\nJ(θ) = Eτ∼πθ[R(τ)] =\nZ\nT\npθ(τ)R(τ) dτ,\n(8)\n5\nwhere R(τ) = PT−1\nt=0 rt is the total reward (or return) over a trajectory τ, T is a valid trajectory\nspace, and pθ(τ) = ρ0(s0) QT−1\nt=0 P(st+1|st, at)πθ(at|st) is the probability of generating a T-\nstep trajectory τ according to πθ in the environment. In continuing environments, J(θ) can be\naverage reward per time-step for one-step MDPs\nJ(θ) =\nZ\nS\ndπθ(s)\nZ\nA\nπθ(a|s)R(s, a) da ds,\n(9)\nwhere dπθ(s) is the stationary probability of visiting the state s following the policy πθ. We can\nﬁnd that the total reward objective J(θ) is also differentiable w.r.t. θ. A straightforward idea\nof solving the parameter is to follow the gradient ∇θJ(θ), which is generally called as policy\ngradient method.\nAffected by the deep learning, ﬂexible and capable deep neural network models are intro-\nduced in reinforcement learning. It can be noticed that, although the objective J(θ) is differen-\ntiable, the objective function is not simple, particularly when deep neural network models are\nemployed. How to best optimize the objective function is still an open problem with continual\nprogress.\n2.2\nDerivative-free optimization\nOptimization, x∗= arg maxx∈X f(x) as a general representative, plays a fundamental role in\nmachine learning. With the rapid development of machine learning, it has deeper applications\nin a broader and more complex scenario. Due to the increasing complexity of machine learning\napplication problems, optimization methods for complex and hard problems are attracting more\nand more attention from researchers. For complex optimization problems (e.g., non-convex,\nnon-smooth, non-differential, discontinuous, and NP-hard, etc.), gradient-based methods may\nfall into the local optima or even lose their power, and result in the unsatisfactory performance.\nIn a search space, the objective of optimization is to ﬁnd the solution with the extreme\nfunction value. A general principle of optimization is simply that, given the currently accessed\nsolutions, which can be randomly sampled at the initialization, ﬁnd the next solution with better\nfunction value. For example, gradient ascent method ﬁnds the next solution follows the gradient\ndirection of the objective function. This procedure requires ﬁrstly that the objective function is\ndifferentiable, and more importantly that there are few local optima and saddle points, so that\nthe gradient direction is informative and can lead to better solutions.\nDerivative-free optimization (DFO) [Conn et al., 2009; Kolda et al., 2003; Rios and Sahini-\ndis, 2013], also termed as zeroth-order or black-box optimization, ﬁnds the next solutions in\nanother way. It covers many families of optimization algorithms that do not rely on the gradi-\nent. It only relies on the function values (or ﬁtness values) f(x) on the sampled solution x. Most\nderivative-free optimization algorithms share a common structure. They ﬁrstly initialize from\n6\nsome random solutions in the search space. From the accessed solutions, derivative-free opti-\nmization algorithms build a model, either explicitly or implicitly, about the underlying objective\nfunction. The model could imply an area that contains some potential better solutions. They then\nsample new solutions from that model and update the model. Derivative-free optimization meth-\nods repeat this sampling-and-updating procedure to iteratively improve the quality of solutions.\nTo sum up, the general framework of derivative-free optimization methods involve some key\nsteps below:\n1. Randomly sample solutions;\n2. Evaluate the objective function value of the sampled solutions;\n3. Update the model from the sampled solutions and their function values;\n4. Sample new solutions according to the model with a designed mechanism;\n5. Repeat from step 2 until some termination conditions are satisﬁed;\n6. Return the best found solution and its function value.\nThe termination conditions usually include that the function evaluation budget is exhausted or\nthe goal optimal function value has been reached. Representative derivative-free algorithms in-\nclude evolutionary algorithms [Holland, 1975; Hansen et al., 2003], Bayesian optimization [Shahri-\nari et al., 2016], cross-entropy method [de Boer et al., 2005], deterministic or stochastic opti-\nmistic optimization [Munos, 2014], and classiﬁcation-based optimization [Yu et al., 2016], etc.\nSo far, the substantial progress has been made in both the theoretical analysis tools and theo-\nretical guarantees of derivative-free optimization [He and Yao, 2001; Yu and Zhou, 2008; Bull,\n2011; Jamieson et al., 2012; Munos, 2014; Yu and Qian, 2014; Duchi et al., 2015; Yu et al.,\n2015; Kawaguchi et al., 2015; Yu et al., 2016; Kawaguchi et al., 2016]. Some fundamental and\ncrucial concerns in theory, such as the global convergence rate and the function class that can\nbe optimized efﬁciently, are disclosed progressively.\nIn order to take a deeper insight of the key step 3 and step 4 in the above procedure, we\npresent a snapshot of a canonical genetic algorithm [Holland, 1975; Mitchell, 1998] as an exam-\nple. Genetic algorithms belong to the family of evolutionary algorithms. Consider maximizing a\npseudo-Boolean function arg maxx∈{0,1}d f(x). Here the solutions are represented as bit strings\nof length d. A canonical genetic algorithm maintains a population which includes n solutions,\nand the model in each iteration/generation is represented by the population (i.e., n solutions\nin each population). In each iteration, n children (or offspring) are produced on the basis of n\nparent solutions via the designed mechanism called crossover operator from the parents, e.g.,\n7\nsingle-point crossover which is shown below. We set d = 5 for illustration.\nxparent\n1\n= (0, 1, 0, 0, 1)\nxparent\n2\n= (1, 0, 1, 0, 0)\ncrossover\n−−−−→xchild\n1\n= (0, 1, 1, 0, 0)\nxchild\n2\n= (1, 0, 0, 0, 1)\n(10)\nIn the above single-point crossover, a point (or bit) on both parents’ bit strings is picked ran-\ndomly, and bits to the right of that point are swapped between the two parents, which results\nin two children. Besides, mutation is another variation operator in the designed mechanism. If\nmutation takes place, it can be applied to each child that has been produced by crossover. One\nway of realizing mutation could be ﬂipping each bit in a solution xchild with some probability\nand producing a new solution xchild\nnew , which is shown below. We set d = 5 for illustration.\nxchild = (0, 1, 1, 0, 0)\nmutation\n−−−−→xchild\nnew = (0, 1, 1, 1, 0)\n(11)\nAfter the crossover and mutation operators (step 4), the objective function values of these newly\nproduced solutions are evaluated (step 2). Then, the model, which is represented by the solu-\ntions, is updated via selecting the best n solutions from both the parents and children to form\nthe next generation of population (step 3).\nFrom the above algorithmic procedure, some key characteristics of derivative-free optimiza-\ntion algorithms can be observed. Firstly, they can be utilized as long as the quality of solutions\ncan be evaluated. Secondly, the designed mechanisms for sampling solutions and rules for up-\ndating model always consider the balance between exploration and exploitation. In optimiza-\ntion, exploration intuitively means gathering more information about objective functions and\nreducing some uncertainty, while exploitation means choosing the best solutions under current\ninformation. In the instance of canonical genetic algorithm, exploration is realized via crossover\nand mutation operators. The combination of exploration and exploitation could help optimiza-\ntion procedures maintain global and local search, and jump out of the local optima in order\nto ﬁnd out the (approximately) global optima with high probability. Thirdly, many derivative-\nfree optimization algorithms are population-based. A population of solutions is maintained and\nimproved iteratively, and the algorithms could share and leverage the information across a pop-\nulation. These key characteristics make derivative-free optimization methods have a low barrier\nto use as well as the ability of search globally.\nSince the conditions of using derivative-free optimization methods are relaxed, they are easy\nto use and general in the continuous, discrete, or mixed search space. Their ability could guaran-\ntee the effectiveness of global optimization. Therefore, derivative-free optimization methods are\nsuitable for dealing with the complex and hard optimization problems. They have been applied\nin the complex learning tasks and achieved impressive empirical results, such as policy search\nin reinforcement learning [Taylor et al., 2006; Abdolmaleki et al., 2015; Hu et al., 2017; Sali-\nmans et al., 2017], automatic machine learning and hyper-parameter tuning [Snoek et al., 2012;\n8\nThornton et al., 2013; Real et al., 2017, 2018], objective detection in computer vision [Zhang et\nal., 2015], subset selection [Qian et al., 2015, 2017], and security games [Brown et al., 2014],\netc.\n3\nWhen RL meets DFO\nAfter a brief recap of reinforcement learning (RL) and derivative-free optimization (DFO), this\nsection explains and emphasizes that the fusion of them, termed as derivative-free reinforcement\nlearning, is necessary and attractive. We explain it from the aspects of optimization, exploration,\nand computation, respectively.\nOptimization. A learning task typically involves three components, and they are represen-\ntation, evaluation, and optimization [Domingos, 2012]. The ability of optimization method has\na signiﬁcant impact on the complexity of model representation and the type of evaluation func-\ntion that we can choose for learning. Nowadays, in reinforcement learning, the policy or value\nfunction models are often represented by deep neural networks, i.e., deep reinforcement learn-\ning. When injecting this sophisticated deep representation into the evaluation function in RL\n(e.g., Eq. (8) or Eq. (9) in Section 2.1), the resulting optimization problems (e.g., Eq. (7) in\nSection 2.1) could be non-convex.\nFor such optimization problems, standard gradient-based methods may suffer from the dif-\nﬁculties of stationary point issues (e.g., a plethora of saddle points or spurious local optima),\nbad condition number, or ﬂatness in the activations that could lead to the gradient vanishing\nproblem [Shalev-Shwartz et al., 2017]. And these difﬁculties could result in the unsatisfactory\nresults.\nAt the same time, derivative-free methods that conduct optimization from samples provide\nanother way of policy learning, and can be complementary with gradient-based ones in rein-\nforcement learning. One straightforward way of applying derivative-free optimization methods\nis to deﬁne the search space as the policy parameter space and the objective function as the\nexpected long-term reward. Namely, X\ndef\n= Θ and f(x)\ndef\n= J(θ). For policy learning, derivative-\nfree optimization methods have their own merits of being able to search parameters globally\nand being easy to train. They do not perform gradient back-propagation, do not care whether\nrewards are sparse or dense, do not care the length of time horizons, and do not need value\nfunction approximation [Salimans et al., 2017]. In Section 4 and 5, we will discuss the recent\nadvances dedicated to the derivative-free model parameter updating as well as model selection\nin reinforcement learning, respectively.\nExploration. Almost all reinforcement learning methods share the exploration-learning\nframework [Yu, 2018]. Namely, an agent explores and interacts with an unknown environ-\nment to learn the optimal policy that maximizes the total reward from the exploration sam-\n9\nples. Generally speaking, the exploration samples involve states, state transitions, actions and\nrewards. From the exploration samples, the quality of a policy can be evaluated by rewards,\nand the learning step updates the policy or value function models from the evaluations. This\nexploration-learning procedure is repeated until some termination conditions are met. Explo-\nration is necessary in reinforcement learning. Because achieving the best total reward on the\ncurrent trajectory samples is not the ultimate goal, and the agent should visit the states that\nhave not been visited before so as to collect better trajectory samples. This means that the agent\nshould not follow the current policy tightly, and thus the exploration mechanisms need to en-\ncourage the agent to deviate from the previous trajectory paths properly.\nMost existing exploration mechanism mainly suffer from being memoryless and blind, e.g.,\naction space noise or parameter space noise [Plappert et al., 2018], or being difﬁcult to use in\nreal state/action spaces, e.g., curiosity-driven exploration [Pathak et al., 2017]. On the other\nhand, many mainstream policy gradient methods, such as truncated natural policy gradient\n(TNPG) [Duan et al., 2016a] and trust region policy optimization (TRPO) [Schulman et al.,\n2015], seldom touch the exploration.\nMeanwhile, derivative-free reinforcement learning is naturally equipped with the explo-\nration strategies. Because in the search process of derivative-free optimization methods, the\ndesigned mechanisms for sampling solutions and rules for updating model always consider the\nexploration. Therefore, derivative-free optimization methods can take part of the duty of ex-\nploration for reinforcement learning when updating the policy or value function models from\nsamples. Recently, some problem-dependent derivative-free exploration methods that could im-\nprove the sample efﬁciency have been proposed. In Section 6, we will discuss these works\ndedicated to the derivative-free exploration in reinforcement learning.\nComputation. Although derivative-free methods could bring some good news to reinforce-\nment learning with respect to optimization and exploration, they mostly suffer from low con-\nvergence rate. Derivative-free optimization methods often require to sample a large amount of\nsolutions before convergence, even if the objective function is convex or smooth [Jamieson et\nal., 2012; Duchi et al., 2015; Bach and Perchet, 2016]. And the issue of slow convergence be-\ncomes more serious as the dimensionality of a search space increases [Duchi et al., 2015; Qian\nand Yu, 2016]. Obviously, this issue will block the further application of derivative-free meth-\nods to reinforcement learning. They sample a lot of policy parameters before ﬁnding out an\noptimal or satisfactory one, and the quality of policy parameters is evaluated by the trajectory\nsamples. This makes reinforcement learning more sample inefﬁcient.\nFortunately, many derivative-free optimization methods are population-based. That is to\nsay, a population of solutions is maintained and improved iteratively. This characteristic makes\nthem highly parallel. Thus, derivative-free optimization methods can be accelerated by parallel\nor distributed computation, which alleviates their slow convergence. Furthermore, for paral-\n10\nlel/distributed derivative-free methods, the data communication cost is lower compared with\ngradient-based ones, since only scalars (ﬁtness values) instead of gradient vectors or Hes-\nsian matrices need to be conveyed. This merit further compensates for the low convergence\nrate partly. In Section 7, we will discuss the recent works dedicated to the parallel/distributed\nderivative-free reinforcement learning.\nDFRL\nOptimization\n(Sec. 4&5)\nExploration\n(Sec. 6)\nComputation\n(Sec. 7)\nParameter\nUpdating (Sec. 4)\nModel Selection\n(Sec. 5)\nFigure 2: The organization of the works reviewed in the article.\nTo sum up, derivative-free reinforcement learning could hopefully result in more effective\nand powerful algorithms for complex control tasks, more sample efﬁcient exploration in envi-\nronments, and more time efﬁcient global optimization for better policy. The organization of the\nfollowing reviewed works is depicted in Figure 2. We should stress that derivative-free optimiza-\ntion methods are not proposed to replace gradient-based ones, e.g, policy gradient algorithms,\nin reinforcement learning, and they are complementary with each other. In fact, some impres-\nsive works that will be discussed in this article are hybrids of derivative-free and gradient-based\nones.\n4\nDerivative-free model parameter updating in reinforcement\nlearning\nThe generic framework of using derivative-free optimization algorithms to update the model\nparameters in reinforcement learning is quite straightforward. After parameterizing the policy\nor value function models, the quality of a parameter θ is evaluated via the total reward J(θ)\nprovided by an environment. For policy search methods in deep reinforcement learning, the\nparameters θ of a policy model πθ are the weights of a deep neural network. Here the policy\nmodel can be softmax policy for discrete action space or Gaussian policy for continuous action\nspace, and this section only considers that the architecture/topology of a (deep) neural network\nis ﬁxed. Derivative-free algorithms regard θ and J(θ) as solutions and objective function values\n11\n(or ﬁtness values) respectively, and search the optimal solution θ∗. They sample different policy\nparameters, and learn where to sample in the next iteration.\nThe works dedicated to applying derivative-free methods to optimize the weights of neural\nnetworks have been developed for decades, e.g., neuroevolution. In neuroevolution, evolution-\nary algorithms, inspired from natural evolution, can not only optimize the weights, but also\nsearch the topology of neural networks (discussed in Section 5). In each generation of a neu-\nroevolutionary algorithm, each neural network in the population is evaluated by the task, and\nthe best ones are selected. The crossover and mutation operators are used to reproduce the new\nnetworks from the selected ones in order to form a new population, and the process iterates.\nA comprehensive discussion of neuroevolution can be found from the last century review pa-\nper [Yao, 1999] or the recent one [Stanley et al., 2019].\nThe methods of applying neuroevolution for reinforcement learning tasks have been devel-\noped for decades. Recently, more and more works have shown that, compared with gradient-\nbased methods, neuroevolution is competitive for not only policy search in RL [Salimans et\nal., 2017; Such et al., 2017], but also supervised learning tasks [Morse and Stanley, 2016;\nZhang et al., 2017]. This conﬁrms the power of neuroevolution and renews the increasing in-\nterests in it. In addition to the success of neuroevolution RL [Koutn´ık et al., 2013; Hausknecht\net al., 2014; Salimans et al., 2017; Risi and Togelius, 2017; Chrabaszcz et al., 2018], other\nderivative-free optimization methods for RL could also be promising. For instance, stochastic\nzeroth-order search could rival the gradient-based methods for the static linear policy optimiza-\ntion on the MuJoCo locomotion tasks [Mania et al., 2018]. And its convergence behavior is\nanalyzed in [Malik et al., 2019].\nIn this section, we mainly review the recent progress in derivative-free model parameter\nupdating in reinforcement learning, and the involved works include but are not only restricted\nto neuroevolution RL. For the early works on evolutionary RL, the survey of them can be found\nin [Moriarty et al., 1999; Whiteson, 2012].\n4.1\nEvolution strategies based model parameter updating\nEvolution strategies (ESs) [Hansen et al., 2015] belong to the family of evolutionary algorithms\ninspired from natural evolution. In ESs, a parameterized search distribution (e.g., Gaussian or\nCauchy distribution) is maintained, and solutions are sampled from this search distribution and\nthen are evaluated by an objective function. The search distribution is iteratively updated ac-\ncording to the evaluated solutions. Mutation is a commonly used variation operator in ESs,\nand it perturbs solutions in order to generate the new ones. Mutation introduces the diversity\nof solutions in the population, and may alleviate the problem that optimization algorithms are\ntrapped into the local optima. Due to the different algorithmic implementations, ESs have many\n12\nvariants, and one of the most popular among them might be the covariance matrix adaptation\nevolution strategy (CMA-ES) [Hansen and Ostermeier, 1996, 2001; Hansen et al., 2003]. In\nCMA-ES, the search distribution is a multivariate Gaussian, and its covariance matrix adapts\nover time. The mutation ellipsoids of CMA-ES are not restricted to be axis-parallel, and the\noverall step size is controlled with the cumulative step size adaptation.\nHeidrich-Meisner and Igel [Heidrich-Meisner and Igel, 2008, 2009b] proposed to optimize\nthe parameterized policy in RL via CMA-ES. They consider to apply CMA-ES because it is\nrobust (ranking policies based), can detect the correlations among parameters, and can infer the\nsearch direction from the scalar signals in RL. In [Heidrich-Meisner and Igel, 2008], CMA-\nES is used to optimize linear policies (e.g., πθ(s) = θ⊤s for the deterministic policies) on\nthe double cart-pole balancing task. Empirical results show that, compared with the episodic\nnatural actor-critic algorithm (NAC) [Peters and Schaal, 2008] which is a gradient-based one,\nCMA-ES is more robust with respect to noise, policy initialization, and hyper-parameters. On\nthe other hand, NAC could surpass CMA-ES with respect to learning speed under appropriate\npolicy initialization and hyper-parameters. In [Heidrich-Meisner and Igel, 2009b], CMA-ES is\nused to search neural network policies on the Markovian and non-Markovian variants of the\npole balancing task. Compared with the policy gradient methods, value function based meth-\nods, random search, and several neuroevolution methods, overall, the empirical performance of\nCMA-ES is superior.\nAlso, Heidrich-Meisner and Igel [Heidrich-Meisner and Igel, 2009a] enhanced CMA-ES\nfor direct parameterized policy search with an adaptive uncertainty handling mechanism. On\nthe basis of this mechanism, the resulting Race-CMA-ES can dynamically adapt the number\nof roll-outs for evaluating the parameterized policies such that the ranking of solutions is just\nreliable enough to push the learning process. Empirical results on the mountain car and swim-\nmer control tasks with linear policies show that, compared with CMA-ES and NAC [Peters and\nSchaal, 2008], Race-CMA-ES can accelerate the learning process and improve the algorithmic\nrobustness. Stulp and Sigaud [Stulp and Sigaud, 2012] were the ﬁrst to make the relationship\nbetween CMA-ES and the cross-entropy (CE) method [de Boer et al., 2005] explicit under the\nview of probability-weighted averaging. The efﬁcacy of CE for playing Tetris has been studied\nin [Szita and L¨orincz, 2006]. They claim that CE can be regarded as a special case of CMA-\nES through setting some CMA-ES parameters to extreme values. Besides, they also inject the\ncovariance matrix adaptation into the policy improvement with path integrals, and result in PI2-\nCMA. PI2-CMA shares the similar way of performing the parameter updating with CMA-ES\nand CE, but has the more consistent convergence behavior under varying initial conditions.\nWierstra et al. [Wierstra et al., 2008, 2014] presented the natural evolution strategies (NESs)\nwhere the natural gradient was used to update a parameterized search distribution in the direc-\ntion of higher expected ﬁtness. The effectiveness of NESs is empirically veriﬁed on a neu-\n13\nroevolutionary control policy design for the non-Markovian double pole balancing task. And\nthe result shows that NESs possess the merits of alleviating oscillations and premature conver-\ngence.\nSalimans et al. [Salimans et al., 2017] from OpenAI proposed to use a simple evolution\nstrategy, based on a simpliﬁcation of NESs, to directly optimize the weights θ of policy neural\nnetworks. After initializing the policy parameters θ0, OpenAI ES stochastically mutates the pa-\nrameters θ of the policy with Gaussian distribution, and evaluates the resulting mutated parame-\nters via the total reward/return J(·). Then, it combines these returns and updates the parameters\nθ. This process iterates till the termination condition is met. Let α > 0 denote the learning rate,\nand σ denote the noise standard deviation. The core steps are summarized as follows:\n1. Randomly sample ϵ1, . . . , ϵn from the standard Gaussian distribution N(0, I);\n2. Compute returns Ji = J(θt + σϵi) for i = 1, . . . , n;\n3. Update parameters θt+1 ←θt + α 1\nnσ\nPn\ni=1 Jiϵi;\n4. Repeat from step 1 till the termination condition is met.\nFurthermore, some techniques are integrated to facilitate the success of OpenAI ES for learning\npolicy neural networks. To name a few, virtual batch normalization [Salimans et al., 2016] is\nadopted to enhance the exploration ability, antithetic sampling [Geweke, 1988] (also known as\nmirrored sampling [Brockhoff et al., 2010]) is adopted to reduce variance, and ﬁtness shap-\ning [Wierstra et al., 2014] is adopted to decrease the trend of trapping into the local optima\nin the early training phase. This simple and generic OpenAI ES is very easy to parallelize and\nonly takes low communication cost, which will be discussed in Section 7. The effectiveness\nof OpenAI ES is empirically investigated on some simulated robotics tasks in MuJoCo and\nAtari games in OpenAI Gym [Todorov et al., 2012; Bellemare et al., 2013; Brockman et al.,\n2016]. And the results surprisingly show that OpenAI ES is comparable to some state-of-the-\nart gradient-based algorithms, e.g., trust region policy optimization (TRPO) [Schulman et al.,\n2015] and asynchronous advantage actor-critic (A3C) [Mnih et al., 2016], on both simple and\nhard environments, but OpenAI ES needs more samples. Notably, this work [Salimans et al.,\n2017] seems to renew the increasing interests in (deep) neuroevolution RL.\nEncouraged and inspired by [Salimans et al., 2017], new ideas, discussions and approaches\nare blooming out. Lehman et al. [Lehman et al., 2018a] claimed that the simple OpenAI ES\nin [Salimans et al., 2017] was more than just a ﬁnite-difference approximation of the reward\ngradient. Because OpenAI ES optimizes the average return of the whole solutions in the popu-\nlation instead of a single solution, it searches parameters that are robust to perturbation in the\nparameter space and yields more stable policies. Chrabaszcz et al. [Chrabaszcz et al., 2018]\n14\ncompared a simper and basic canonical ES algorithm with OpenAI ES [Salimans et al., 2017].\nThe empirical results on a subset of 8 Atari games [Bellemare et al., 2013; Brockman et al.,\n2016] show that this simpler canonical ES is able to match or even surpass the performance\nof OpenAI ES. This work [Chrabaszcz et al., 2018], on the one hand, further conﬁrms that the\npower of ES-based model parameter updating for policy search may rival that of the gradient-\nbased algorithms. On the other hand, it indicates that the ES-based RL algorithms could be\nfurther ameliorated in many aspects by integrating the recent advances made in the ﬁeld of ES.\nChoromanski et al. [Choromanski et al., 2018] enhanced ES-based RL model parameter\nupdating by structured evolution and compact policy networks. The resulting algorithm needs\nless policy neural network parameters, and thus could speed up training and inference. Chen\net al. [Chen et al., 2019b] improved ES for training policy neural networks in terms of the\nmutation strength adaptation and principal search direction update rules. And the number of\nelitists adaptation and restart procedure are also integrated to tackle the local optima. The pro-\nposed algorithm is of linear time complexity and low space complexity, and thus possesses the\nmerit of scalability. Choromanski et al. [Choromanski et al., 2019] applied the ideas of ac-\ntive subspaces [Constantine, 2015], which is one of the popular approaches to dimensionality\nreduction, to yield the sample-efﬁcient and scalable ES for policy optimization in RL. Liu et\nal. [Liu et al., 2019] boosted the sample efﬁciency of ES for RL by reusing the existing sam-\npled data, and proposed the trust region evolution strategies (TRES) algorithm. TRES realizes\nsample reuse for multiple epochs of updates in the way of iteratively optimizing a surrogate\nobjective function. Tang et al. [Tang et al., 2019] proposed a variance reduction technique for\nES-based RL model parameter updating. It utilizes the underlying MDP structure of RL through\nproblem re-parameterization and control variable construction. Fuks et al. [Fuks et al., 2019]\nproposed a technique called progressive episode lengths (PEL), and injected it into a canonical\nES [Chrabaszcz et al., 2018] to result in PEL-ES. Inspired from transfer learning and curricu-\nlum learning, PEL-ES ﬁrstly lets an agent play the easy and short tasks, and then applies the\ngathered knowledge to further deal with the harder and longer tasks. Compared with the canon-\nical ES [Chrabaszcz et al., 2018], PEL-ES is superior in optimization speed, total rewards, and\nstability.\nHouthooft et al. [Houthooft et al., 2018] proposed a meta-learning method for policy learn-\ning across different tasks called evolved policy gradient (EPG). EPG encodes the prior knowl-\nedge implicitly through a parametrized loss function, and parametrization is realized by tempo-\nral convolutions over the agent’s experience. The agent can use the learned loss function to learn\non a new task quickly. Thus, the main procedure is to evolve a parametrized and differentiable\nloss function. The agent optimizes its policy to minimize this loss function so as to gain high\nreturns. To implement this procedure, EPG involves two optimization loops:\n• In the inner loop, the agent learns to solve a task through minimizing a loss function\n15\nprovided by the outer loop.\n• In the outer loop, the parameters of a loss function are optimized in order to maximize\nthe returns gained after the inner loop.\nFigure 3: Figure from [Houthooft et al., 2018] that illustrates the high-level procedure of EPG.\nThe inner loop is optimized by the stochastic gradient descent (SGD) method. For the outer\nloop, since the returns are not explicit functions of the loss function parameters, ES can be ap-\nplied to optimize the parameters of this loss function. Figure 3 illustrates the main procedure of\nEPG. Empirical results on several continuous control tasks in MuJoCo [Todorov et al., 2012;\nBrockman et al., 2016] show that, compared with proximal policy optimization (PPO) [Schul-\nman et al., 2017] that is an off-the-shelf policy gradient algorithm, EPG can generate a learned\nloss function which trains the agent faster. Besides, EPG also exhibits generalization properties\nfor out-of-distribution test time tasks that surpass other meta-learning algorithms RL2 [Duan et\nal., 2016b] and MAML [Finn et al., 2017]. Notably, in EPG, ES is used to optimize the param-\neters of a loss function instead of policy parameters. And the success of EPG largely owes to\nthe hybrid of derivative-free and gradient-based algorithms. The similar scenario also happens\nin [Ha and Schmidhuber, 2018; Yu et al., 2019], where CMA-ES and gradient-based algorithms\nare mixed for policy transfer from simulation to reality.\n4.2\nGenetic algorithms based model parameter updating\nSimilar to evolution strategies (ESs), genetic algorithms (GAs) [Mitchell, 1998] also belong\nto the family of evolutionary algorithms inspired from natural evolution. Genetic algorithms\nmaintain a population of solutions. Via the operators of mutation, crossover (also called recom-\nbination) and selection, a population of solutions can be evolved. One of the main differences\nbetween GAs and ESs is that the crossover operator is often used in GAs. Crossover combines\ntwo parents to generate new offspring, and the newly generated solutions are usually mutated\nbefore adding them to the population. Notably, the crossover operator in GAs could further\nimprove the diversity of solutions in the population.\n16\nTo verify whether GAs can also be effectively applied to the RL tasks, Such et al. [Such\net al., 2017] used a very simple GA to update the weights of deep policy neural networks. It\niteratively maintains a population of parameter vectors θ, and the mutation operator is realized\nby the additive Gaussian noise to θ. The elitism technique is employed during evolution. For\nsimplicity, it does not consider crossover. Due to the huge amount of parameters in deep neural\nnetworks and the relatively unsatisﬁed scalability of GA, the work proposes an approach to\nstoring large parameter vectors θ compactly via representing each θ as an initialization seed plus\nthe list of random seeds. This compact representation approach is reversible (i.e., each parameter\nvector θ can be reconstructed from it), and substantially improves the scalability and efﬁciency\nof deep GA. The policy neural networks with more than four million free parameters can be\nsuccessfully evolved. Besides, novelty search (NS) [Lehman and Stanley, 2011] is integrated\ninto this deep GA to avoid the local optima and encourage exploration. And a distributed version\nof the deep GA on many CPUs across many machines is implemented for acceleration. The\nexperimental results on the Atari and MuJoCo humanoid locomotion tasks [Bellemare et al.,\n2013; Todorov et al., 2012; Brockman et al., 2016] indicate that the deep GA is effective and\ncompetitive. Overall, it performs roughly as well as DQN [Mnih et al., 2015], A3C [Mnih et\nal., 2016], and OpenAI ES [Salimans et al., 2017].\nGangwani and Peng [Gangwani and Peng, 2018] proposed a genetic policy optimization\n(GPO) method for sample-efﬁcient deep policy optimization. GPO involves crossover, muta-\ntion, and selection. For crossover, instead of directly exchanging the parameter representations\nof two parents (parameter crossover) that may destroy the hierarchical relationship of the neural\nnetworks and lead to a catastrophic drop in performance, GPO employs imitation learning to\nrealize policy crossover in the state space. The state-space crossover operator combines two par-\nent policies πx and πy to produce an offspring (or child) policy πc that shares the same network\narchitecture as parents via two steps, as shown in Figure 4. Firstly, it trains a two-level policy\nπH, and\nπH(a|s) = πS(parent = x|s) · πx(a|s) + πS(parent = y|s) · πy(a|s),\n(12)\nwhere πS is a binary policy that trains from the parents’ trajectories. Given a state s, πH selects\nbetween the parent policies πx and πy, and then outputs the action distribution of the selected\nparent. Secondly, the trajectories generated from the expert policy πH is used as the supervised\ndata, and the offspring policy πc is trained from the supervised data by imitation learning. To\navert the compounding errors introduced by the state distribution mismatch between expert and\noffspring, the dataset aggregation (DAgger) algorithm [Ross et al., 2011] is used for imitation\nlearning. This sample-efﬁcient crossover can distill the knowledge from parents to produce a\nchild that aims to imitate its best parent in generating similar state visiting distributions. For\nmutation, instead of utilizing the commonly-used Gaussian perturbation, GPO mutates the pol-\nicy network weights by randomly rolling out trajectories and performing several iterations of\n17\nParent-x\nParent-y\n!\"($|&)\n!(($|&)\n!) $ & = !+ parent = x & !\" $ & +\n4!+ parent = 5 & !(($|&)\n?\nBinary Policy\n!+(parent = 6|&)\nOffspring\n!7\nImitation Learning\nFigure 4: An illustration of combining the parent policies to produce an offspring policy in\nGPO [Gangwani and Peng, 2018].\na policy gradient algorithm using these roll-out trajectories. GPO chooses PPO [Schulman et\nal., 2017] and advantage actor-critic (A2C) algorithm [Sutton and Barto, 1998; Mnih et al.,\n2016] to fulﬁll the policy gradient mutation. This mutation operator possesses the merits of\nhigh efﬁciency, sufﬁcient genetic diversity, and good exploration of state space. For selection,\nGPO takes both performance and diversity into consideration. The experimental results on some\ncontinuous control tasks in MuJoCo [Todorov et al., 2012; Brockman et al., 2016] show that,\ncompared with PPO and A2C which are state-of-the-art policy gradient methods, GPO could\nbe superior in terms of episode reward and sample efﬁciency.\nBodnar et al. [Bodnar et al., 2020] also made progress in developing the variation operators\nin GAs tailored to policy optimization. They observe that, when the direct genetic encoding\nfor deep neural networks (i.e., the weights of a network are recorded as a list of real num-\nbers) meets the traditional variation operators in GAs, the negative side-effects may occur in\nRL since deep neural networks are sensitive to the small variation of weights. For instance, the\nrecently proposed evolutionary reinforcement learning (ERL) framework [Khadka and Tumer,\n2018], which combines both of them, has the risks of catastrophic forgetting and destructive\nbehaviors, and does not fully address the scalability problem of GAs for RL. Therefore, they\npropose the learning-based Q-ﬁltered distillation crossover, the safe mutation [Lehman et al.,\n2018b] based proximal mutation , and then integrate them into ERL [Khadka and Tumer, 2018]\nin a hierarchical manner to result in the proximal distilled evolutionary reinforcement learning\n(PDERL) framework. PDERL could compensate for the simplicity of the direct genetic en-\n18\ncoding, satisfy the functional requirements of the genetic variation operators when applied to\nthe directly encoded deep neural networks, and prevent the catastrophic forgetting of parental\nbehaviors. Compared with PPO [Schulman et al., 2017] and twin delayed deep deterministic\npolicy gradients (TD3) algorithm [Fujimoto et al., 2018], as well as ERL, PDERL is superior\nwhen evaluated on ﬁve robot locomotion tasks in OpenAI Gym [Todorov et al., 2012; Bellemare\net al., 2013; Brockman et al., 2016]. Notably, in GPO and PDERL, the gradient-based update is\ninjected into GAs to enhance the genetic variation operators. The success of them implies that,\nby fusing derivative-free and gradient-based algorithms in a clever way, the strengths of both\nsides could be absorbed, and more powerful policy search algorithms would be expected.\n4.3\nBayesian optimization based model parameter updating\nBayesian optimization (BO) [Shahriari et al., 2016] is also a kind of widely-used black-box\nderivative-free approaches aimed at optimizing the difﬁcult functions with relatively few evalu-\nations. BO constructs a probabilistic model for the function being optimized, and then exploits\nthis model to make decisions about the next solution point of the function to evaluate. The\nnewly evaluated solution and the previously evaluated ones are all used to update the proba-\nbilistic model so as to improve its accuracy, and this procedure iterates to sample the solutions\nwith increasing quality. There are two ingredients that need to be speciﬁed in BO, i.e., the proba-\nbilistic prior over functions that expresses the assumptions of the function being optimized, and\nthe acquisition function that determines the next solution to evaluate. For the probabilistic prior,\nBO usually uses the Gaussian process (GP) [Rasmussen and Williams, 2006] prior because of\nits ﬂexibility and analytical tractability. GP deﬁnes a distribution over functions speciﬁed by its\nmean function and covariance function. The function being optimized is assumed to be drawn\nfrom a GP prior, and this prior as well as the sampled data induce a posterior over functions.\nThe acquisition function a(·), that is constructed from the model posterior, determines which\nsolution should be evaluated next via a proxy optimization θnext = arg maxθ a(θ). The popular\nchoices of acquisition function are probability of improvement [Kushner, 1964], expected im-\nprovement [Moˇckus et al., 1978], GP upper conﬁdence bound [Srinivas et al., 2010; de Freitas\net al., 2012] and so on. As BO can exploit the prior information about the expected return and\nutilize this knowledge to select new policies to execute, more and more works focus on applying\nBO to RL, and early works on this direction is already reviewed in [Brochu et al., 2010].\nWilson et al. [Wilson et al., 2014] proposed a novel Gaussian process covariance function\nto measure the similarity between policies using the trajectory data generated from policy ex-\necutions. Compared with the traditional covariance functions that relate the policy parameters,\nthe proposed covariance function that relates the policy behavior is more reasonable and ef-\nfective. Furthermore, they also introduce a novel Gaussian process mean function that exploits\n19\nthe learned transition and reward functions to approximate the landscape of the expected re-\nturn. The developed Bayesian optimization approach tailored to reinforcement learning could\nrecover from model inaccuracies when good transition and reward models cannot be learned.\nEmpirical results on a set of classic control benchmarks verify its effectiveness and efﬁciency.\nCalandra et al. [Calandra et al., 2014] applied Bayesian optimization to design the gaits\nand the corresponding control policies of a bipedal robot. Three popular acquisition functions,\nas well as the effect of ﬁxed versus automatic hyper-parameter selection, are analyzed therein.\nIn [Calandra et al., 2016], they additionally formalize the problem of automatic gait optimiza-\ntion, discuss some widely-used optimization methods, and extensively evaluate Bayesian opti-\nmization on both simulated tasks and real robots. The evaluation demonstrates that BO is very\nsuitable for robotic applications since it could search a good set of gait parameters with only\na few amount of experimental trials. By comparing the different variants of BO algorithms,\nthey observe that the GP upper conﬁdence bound acquisition function performs the best among\nthem.\nMarco et al. [Marco et al., 2017] proposed a Bayesian optimization algorithm that can adap-\ntively select among multiple information sources with different accuracies and evaluation costs,\ne.g., experiments on a real-world robot and simulators. This BO algorithm exploits the prior\nknowledge from simulations via maximizing the information gain from each experiment, and\nautomatically integrates the cheap but inaccurate information from simulations with the accu-\nrate but expensive real-world experiments in a cost-effective way. The empirical results show\nthat using the prior model information from simulators can reduce the amount of data required\nto ﬁnd out the desirable control policies. Letham and Bakshy [Letham and Bakshy, 2019] aug-\nmented the on-line experiments with the off-line simulator observations to tune the live machine\nlearning systems on the basis of the multi-task Bayesian optimization [Swersky et al., 2013].\nThe empirical study on the live machine learning systems indicates that, by directly utilizing a\nsimple and biased off-line simulator together with a small number of on-line experiments, the\nproposed method can accurately predict the on-line outcomes and achieve the substantial gains.\nAnd the empirical result is consistent with the theoretical ﬁndings of the multi-task Gaussian\nprocess generalization.\nThe work termed as the Bayesian functional optimization (BFO) [Vien et al., 2018] extends\nthe BO methods to the functional policy representations, and its motivation is similar to [Vien et\nal., 2017]. BFO models the function space as a reproducing kernel Hilbert space (RKHS) [Ras-\nmussen and Williams, 2006], and introduces an efﬁcient update of the functional GP as well\nas a simple optimization of the acquisition functional. BFO bypasses the problem of manually\nselecting the features used in the function approximations to deﬁne the parameter space, and\nthus relaxes the performance reliance on the selected parameter space. The experimental result\non the RL task whose policies are modeled in RKHS shows that BFO is able to compactly\n20\nrepresent the complex solution functions.\nEriksson et al. [Eriksson et al., 2019] proposed the trust region Bayesian optimization\n(TuRBO) method in order to tackle the high-dimensional RL problems. They notice that the\ndifﬁculty of high-dimensional optimization in RL comes from the plentiful local optima and the\nheterogeneity of the objective function (e.g., the sparse rewards in RL may lead to the objective\nfunction being nearly constant in a large region of the search space). This difﬁculty makes the\ntask of learning a global surrogate model challenging. Thus, TuRBO maintains a collection of\nsimultaneous local probabilistic models. Each local surrogate model shares the same beneﬁts\nwith Bayesian probabilistic modeling, and at the same time enables the heterogeneous modeling\nof the objective function. In TuRBO, a multi-armed bandit strategy is adopted to globally allo-\ncate the samples among the trust regions. The empirical results show that TuRBO outperforms\nthe compared BO, EAs, and stochastic optimization on the RL benchmarks.\n4.4\nClassiﬁcation based model parameter updating\nThe classiﬁcation-based derivative-free optimization methods [Lozano et al., 2006; Yu et al.,\n2016; Hashimoto et al., 2018; Zhou et al., 2019] are recently proposed for non-convex func-\ntions with sample-complexity guarantees. The researchers notice that many derivative-free op-\ntimization methods are model-based, e.g., BO employs GP to model the function. The model-\nbased optimization approaches learn a model from the evaluated solutions, and the model is\nthen utilized to guide the sampling of solutions in the next iteration. The classiﬁcation-based\nderivative-free optimization methods use a particular type of model, classiﬁcation model from\nmachine learning, to model the objective function. A classiﬁcation model learns to classify the\nsolutions in the search space into two categories, the good/positive and the bad/negative ones,\naccording to the quality of the sampled solutions. The learned classiﬁer partitions the search\nspace into the good and bad regions. And then the solutions are sampled from the good regions\nwith high probability.\nYu et al. [Yu et al., 2016] attempted to answer the crucial questions of classiﬁcation-based\nderivative-free optimization, including which factors of a classiﬁer effect the optimization per-\nformance, and which function class can be efﬁciently solved. They identify the critical fac-\ntors, the error-target dependence and the shrinking rate, and propose the randomized coordinate\nshrinking (RACOS) classiﬁcation algorithm to efﬁciently learn the classiﬁer for both continu-\nous and discrete search space. Given a set of good/positive and bad/negative solutions, RACOS\nlearns an axis-aligned hyper-rectangle to cover all the positive but no negative solutions, and\nat the same time, the learning process is randomized and the hyper-rectangle is highly shrunk\nto meet the critical factors disclosed. However, RACOS needs to sample a batch of solutions\nin each iteration to update the classiﬁcation model, while in RL, the environment often only\n21\noffers the sequential policy evaluation. This means RACOS cannot be directly applied to policy\nsearch, where solutions are sampled sequentially. To address this issue, Hu et al. [Hu et al.,\n2017] further proposed a sequential version of RACOS called SRACOS, which is tailored to\ndirect policy search in RL. SRACOS adapts the classiﬁcation-based optimization for sequen-\ntial sampled solutions by forming the sample batch via reusing the historical solutions. In each\niteration, it only samples one solution, replaces a historical solution with the newly sampled\none, and then updates the classiﬁer as RACOS. They introduce three replacing strategies for\nmaintaining the historical solutions, i.e., replacing the worst one, replacing by a randomized\nmechanism, and replacing the one that has the largest margin from the best-so-far solution.\nThe empirical effectiveness is veriﬁed on the helicopter hovering task and controlling tasks in\nOpenAI Gym [Todorov et al., 2012; Bellemare et al., 2013; Brockman et al., 2016], and the\nneural network is used to represent the policy. The results indicate that SRACOS can surpass\nCMA-ES, CE, BO with respect to the total reward.\n5\nDerivative-free model selection in reinforcement learning\nDeep reinforcement learning uses the deep neural networks to model the policy or the value\nfunction. The ﬁnal determination of model depends on two aspects: the weights of connections\nin a neural network and the topology/architecture of a neural network. In the fourth section,\nwe mainly review the cases when the topology of a neural network is ﬁxed. Namely, the num-\nber of hidden layers, the number of hidden layer neurons, the edge set of connected neurons\nand the like are all ﬁxed in advance. Under the condition of ﬁxed neural network topology,\nthe derivative-free methods are used to optimize the weights of connections to search for the\noptimal policy. However, relying on the ﬁxed topology representation imposes some limita-\ntions, since it requires the user to correctly specify a good topology representation in advance.\nUnfortunately, in most tasks, the user cannot correctly guess the appropriate topology represen-\ntation. Selecting an overly simple topology representation could result in the insufﬁcient model\nexpression ability. Even if the optimization algorithm can ﬁnd out the optimal solution under\nthis simple representation, the policy corresponding to the found optimal solution may still be\nunsatisfactory. On the other hand, selecting an overly complex topology representation could\nsigniﬁcantly improves the expressive power of the model, but the huge search space makes the\noptimization algorithm inefﬁcient, and thus it is difﬁcult to ﬁnd out the optimal policy. There-\nfore, many works are devoted to developing the methods that can automatically ﬁnding out the\nappropriate topology representation of neural network. And the review of neuroevolution for\nlearning a neural network architecture can be found from [Yao, 1999; Stanley et al., 2019].\nThis section mainly reviews the methods for optimizing neural network topologies using\nderivative-free optimization, where neural networks can represent the policy or the value func-\n22\ntion in reinforcement learning. The network topology and connection weights are variable at\nthe same time, which further improves the expressiveness and ﬂexibility of the reinforcement\nlearning model. However, it challenges the derivative-free optimization algorithms. The search\nspace becomes larger and more complex, and the evaluation of model quality becomes more\ntime consuming and labor intensive.\nThe earliest and simplest work to optimize the neural network topologies using evolutionary\nalgorithms might be traced back to the structured genetic algorithm (sGA) [Dasgupta and Mc-\nGregor, 1992]. sGA uses a two-part representation to describe each neural network. The ﬁrst\npart represents the connectivity of the neural network in the form of a binary matrix. In this\nbinary matrix, the rows and columns correspond to the neuron nodes in the network, and the\nvalue of each element in the matrix indicates whether there is an edge connecting a given pair of\nnodes. The second part represents the weight of each edge in the neural network. Via evolving\nthese binary matrices with the connection weights, sGA can automatically discover the appro-\npriate the network topology. However, sGA still has some restrictions. When a new topology is\nintroduced (e.g., adding a new edge), the quality of the corresponding solution may be poor due\nto the fact that the weight associated with the new topology has not been optimized. Even if the\ntopology ultimately corresponds to a better policy.\nStanley and Miikkulainen [Stanley and Miikkulainen, 2002b,a] proposed the neuroevolution\nof augmenting topologies (NEAT), which is a well-known approach of topology and weight\nevolving artiﬁcial neural networks (TWEANNs), and achieved a signiﬁcant performance gains\nin RL. To represent networks of different topologies, NEAT uses a ﬂexible genetic coding. Each\nnetwork is described by an edge list, and each edge describes the connection between two neu-\nron nodes and the weight. In NEAT, the architecture is evolved in a way of incremental growth\nfrom minimal structure. New architecture is added incrementally as structural mutations occur,\nand only those architectures survive that are found to be useful. During mutation, one can add\nInput\nLayer\nHidden\nLayer\nOutput\nLayer\nAdd a new link\nFigure 5: A new link (edge) is added between two existing nodes in NEAT [Stanley and Mi-\nikkulainen, 2002b,a].\n23\nnew nodes or new connections to the network, as shown in Figure 5. To avoid the catastrophic\ncrossovers, NEAT introduces the innovation numbers to record and track the historical origin\nof each individual. Whenever a new individual emerges through mutation, it receives a unique\ninnovation number that belongs to itself. Therefore, the innovation numbers can be regarded as\nthe chronology of all individuals produced during the evolutionary process. Experimental result\non the pole-balancing benchmark shows that NEAT can be faster and better than the compared\nﬁxed-topology methods therein.\nTaylor et al. [Taylor et al., 2006] conducted a detailed empirical comparison between NEAT\nand Sarsa [Singh and Sutton, 1996] in the Keepaway RL benchmark based on robot soccer.\nSarsa is a kind of temporal difference [Sutton and Barto, 1998] methods that learn a value\nfunction to estimate the expected total reward for taking a particular action given a state. The\nresults show that NEAT can learn better policies than Sarsa, but NEAT requires more ﬁtness\nevaluations to achieve so. The results on two variations of Keepaway show that Sarsa can learn\nbetter policies when the task is fully observable, and NEAT can learn faster when the ﬁtness\nfunction is deterministic. Whiteson and Stone [Whiteson and Stone, 2006b,a] enhanced the\nsample efﬁciency of evolutionary value function approximation via combining NEAT and a\ntemporal difference method. The proposed algorithm can automatically discover the appropriate\ntopologies for pre-trained neural network function approximators, and exploit the off-policy\nnature of a temporal difference method.\nKohl and Miikkulainen [Kohl and Miikkulainen, 2009] noticed that NEAT could perform\npoorly on the fractured problems, where the correct action varies discontinuously as the agent\nmoves from state to state. They introduce a method to measure the degree of fracture by uti-\nlizing the concept of function variation, and propose RBF-NEAT and Cascade-NEAT to im-\nprove the performance on the fractured problems through biasing or constraining the search for\nnetwork topologies towards the local solutions. Gauci and Stanley [Gauci and Stanley, 2008]\nproposed the hypercube-based neuroevolution of augmenting topologies (HyperNEAT) that is\nable to exploit the geometric regularities in the two-dimensional game screen. Hausknecht et\nal. [Hausknecht et al., 2012] introduced a HyperNEAT-based general game playing (HyperNEAT-\nGGP) method to play Atari games. HyperNEAT-GGP reduces the learning complexity from the\nraw game screen by using a game-independent visual processing hierarchy aimed to identify\nthe objects and entities on the screen. And the identiﬁed ones are input of HyperNEAT. The\neffectiveness of HyperNEAT-GGP is veriﬁed on Asterix and Freeway.\nEbrahimi et al. [Ebrahimi et al., 2017] introduced an approach to learn the control policy for\nthe autonomous driving task aimed to minimize crashes and safety violations during training.\nThe proposed approach learns to generate an optimal network topology from demonstrations\nby utilizing a new reward function that simultaneously optimizes model size and accuracy.\nThey use a recurrent neural network to sequentially generate the description of layers of an\n24\narchitecture from a given design space, which is inspired by [Zoph and Le, 2017b]. The variable\nlength architectures are searched by an enhanced evolution strategy with a modiﬁcation in noise\ngeneration. Finally, by combining this derivative-free policy search with demonstrations, the\nproposed approach can learn a policy that adapts to the new environment based on the rewards\nin the target domain. The experimental result shows that, when the agent learns to drive in a\nreal simulation environment, this approach can learn more safely than the compared baseline\nmethod and has fewer cumulative crash times in the life cycle of the agent. Gaier and Ha [Gaier\nand Ha, 2019] attempted to answer the question of how important are the weight parameters\nof a neural network compared to its architecture. To deemphasize the importance of weights in\nthe neural networks, they assign a single shared weight parameter to every network connection\nfrom a uniform random distribution, and only search for the architectures that perform well on a\nwide range of weights without explicit weight training. The proposed search method is inspired\nby NEAT [Stanley and Miikkulainen, 2002b,a]. The empirical result shows that this method is\nable to ﬁnd the minimal neural network architectures that perform well on a set of continuous\ncontrol tasks only with a random weight parameter.\n6\nDerivative-free exploration in reinforcement learning\nIn most cases, RL algorithms share the exploration-learning framework [Yu, 2018]. An agent\nexplores and interacts with an unknown environment to learn the optimal policy that maximizes\nthe total reward from the exploration samples. The exploration samples involve states, state tran-\nsitions, actions and rewards. Exploration is necessary in RL. Because achieving the best total\nreward on the current trajectory samples is not the ultimate goal, and the agent should visit the\nstates that have not been visited before so as to collect better trajectory samples. This means that\nthe agent should not follow the current policy tightly, and thus the exploration mechanisms need\nto encourage the agent to deviate from the previous trajectory paths properly and drive it to the\nregions with uncertainty. Derivative-free RL is naturally equipped with the exploration strate-\ngies. In the search process of derivative-free optimization methods, the designed mechanisms\nfor sampling solutions and rules for updating model always consider the exploration. To name\na few, the mutation and crossover operators in GAs help to explore the solution space, the GP\nupper conﬁdence bound in BO uses variance to drive the search direction towards the regions\nwith uncertainty, and the classiﬁcation-based optimization algorithms usually adopt the global\nsampling to realize the exploration. Thus, derivative-free optimization methods can take part of\nthe duty of exploration for RL when updating the policy or value function models from samples.\nRecently, some problem-dependent derivative-free exploration methods that could improve the\nsample efﬁciency have been proposed.\nConti et al. [Conti et al., 2018] proposed the NSR-ES method, which attempted to enhance\n25\nthe ability of exploration in evolution strategies for deep RL. NSR-ES uses the novelty search\n(NS) [Lehman and Stanley, 2011] to explore. NS encourages policies to engage in different\nbehaviors than those previously seen. The encouragement of different behaviors is realized\nthrough computing the novelty of the current policy with respect to the previously generated\npolicies and then pushing the population distribution towards the regions in parameter space\nwith high novelty. NS is hybridized with ES to improve its performance on sparse or deceptive\ndeep reinforcement learning tasks. The proposed NSR-ES algorithm has been tested in the\nMuJoCo and Atari [Todorov et al., 2012; Bellemare et al., 2013; Brockman et al., 2016], and\nthe overall performance is superior to the classic evolution strategies. Lehman et al. [Lehman et\nal., 2018b] noticed that the simple mutation operators, e.g., Gaussian mutation, may lead to the\ncatastrophic consequences on the behavior of an agent in deep RL due to the drastic differences\nin the sensitivity of parameters. Therefore, they propose a set of safe mutation operators that\nfacilitate exploration without dramatically varying the network behaviors. The safe mutation\noperators scale the degree of mutation of each individual weight according to the sensitivity of\nthe outputs of network to that weight. And it is realized by computing the gradient of outputs\nwith respect to the weights.\nChen and Yu [Chen and Yu, 2019] considered the exploration as an extra optimization prob-\nlem in reinforcement learning, and realized the exploration component guided by a state-of-\nthe-art classiﬁcation-based derivative-free optimization algorithm, rather than directly applying\nderivative-free optimization to both exploration and learning. The proposed method searches for\nthe high-quality exploration policy globally by setting the performance of the sampled trajec-\ntories as the ﬁtness value of that exploration policy. The target policy is optimized by the deep\ndeterministic policy gradient (DDPG) algorithm [Lillicrap et al., 2016] on those explored trajec-\ntories. This method could overcome the sample inefﬁciency in derivative-free optimization and\nthe exploration locality in gradient-based one. Vemula et al. [Vemula et al., 2019] conducted a\ntheoretical study on when exploring in the parameter space is better/worse than exploring in the\naction space by the black-box optimizer. They reveal that, when the dimensionality of action\nspace and the horizon length are small, as well as the dimensionality of parameter space is large,\nexploration in the action space is preferred. Otherwise, when the horizon length is long and the\ndimensionality of policy parameter space is low, exploration in the parameter space is preferred.\nKhadka and Tumer [Khadka and Tumer, 2018] proposed the evolutionary reinforcement\nlearning (ERL) method. The core idea behind ERL is to combine the genetic algorithm (GA)\nand DDPG algorithm [Lillicrap et al., 2016]. The algorithmic ﬂow is illustrated in Figure 6.\nThe algorithm can be divided into the genetic algorithm module and the reinforcement learning\nmodule. In the genetic algorithm module, ERL generates n actors for parallel sampling, accu-\nmulates the cumulative reward of the sampled trajectories as the ﬁtness of these n actors, and\nthen constructs n new actors according to the mutation process of the genetic algorithm for the\n26\nFigure 6: Figure from [Khadka and Tumer, 2018] that illustrates the structure of ERL.\nnext sampling. In the reinforcement learning module, ERL collects samples of n actors into the\nexperience replay buffer, uses the policy of reinforcement learning algorithm itself to perform\nadditional sampling, and adds the sampling results to the experience replay buffer. Then, ac-\ncording to the DDPG algorithm which is a gradient-based one, a mini-batch sample with a ﬁxed\nnumber of rounds is selected from the experience replay buffer to optimize the policy. At last,\nin order to inject the gradient information of policy into the genetic algorithm process, ERL\nperiodically uses the policy to replace the policy with the worst ﬁtness value among n actors\nbefore the genetic algorithm performs further sampling. ERL is tested on multiple environments\nin MuJoCo [Todorov et al., 2012; Brockman et al., 2016], and the result shows that ERL is su-\nperior to the compared derivative-free algorithms and gradient-based ones with respect to both\nalgorithmic effectiveness and sample efﬁciency. For instance, as shown in Figure 7, we can ob-\nserve that DDPG can easily solve the standard inverted double pendulum task, while fails on\nthe hard one. Both tasks are similar for the evolutionary algorithm (EA). ERL is able to inherit\nthe merits of both DDPG and EA, and successfully solves both standard and hard tasks similar\nto EA while utilizing the gradients for better sample efﬁciency similar to DDPG.\nColas et al. [Colas et al., 2018] proposed the GEP-PG algorithm that splits reinforcement\nlearning into two parts: global exploration and policy gradient. In the global exploration process\n(GEP), it uses a method similar to novelty search to generate a variety of exploration policies\nbased on the policy behavior space. Speciﬁcally, the GEP process samples a batch of policy\nbehavior features from the behavior space. The behavior space is an artiﬁcially deﬁned mapping\nfrom the policy trajectory to a vector, which is used to represent the type of a policy. It then\n27\nCumulative*Reward\nCumulative*Reward\nEpisodes\nEpisodes\nFigure 7: Figure from [Khadka and Tumer, 2018] that compares the performance (measured\nby the cumulative reward) of the proposed ERL, evolutionary algorithm, and DDPG on the\nstandard (left sub-ﬁgure) and hard (right sub-ﬁgure) inverted double pendulum task.\nuses the neighborhood method to ﬁnd the clustering center for each policy behavior. After that,\neach clustering center is perturbed to generate a new set of exploration policies. After the new\nexploration policy is sampled, the behavior features of the exploration policy are marked, and\naccording to the newly added marks, the cluster centers are re-generated by the neighborhood\nmethod. This process is repeated so that the generated exploration policies can be placed in\ndifferent goals as much as possible, which could ensure the diversity of the generated policies.\nIn the policy gradient (PG) process, it combines with the deep deterministic policy gradient\n(DDPG) algorithm [Lillicrap et al., 2016], and adds the samples collected by the GEP process\nto the experience replay buffer. PG updates the policy by the ﬁxed mini-batch. The GEP-PG\napproach enhances the diversity of exploration policies and is tested on the continuous mountain\ncar and half-cheetah environments in MuJoCo [Todorov et al., 2012; Brockman et al., 2016].\nGEP-PG shows the better performance than the compared algorithms. Notably, some of the\naforementioned methods, e.g., GEP-PG and ERL, are essentially a combination of derivative-\nfree and gradient-based optimization. The derivative-free ones are used to better explore, and\nthe gradient-based ones are used to better exploit.\n7\nParallel and distributed derivative-free reinforcement learn-\ning\nAlthough derivative-free methods could bring some good news to RL with respect to optimiza-\ntion and exploration, they mostly suffer from low convergence rate. Derivative-free optimiza-\ntion methods often require to sample a large amount of solutions before convergence, even if\nthe objective function is convex or smooth [Jamieson et al., 2012; Duchi et al., 2015; Bach and\nPerchet, 2016]. And the issue of slow convergence becomes more serious as the dimensionality\nof a search space increases [Duchi et al., 2015; Qian and Yu, 2016]. Obviously, this issue will\nblock the further application of derivative-free methods to RL. Fortunately, many derivative-\n28\nfree optimization methods are population-based. A population of solutions is maintained and\nimproved iteratively. This characteristic makes them highly parallel. Thus, derivative-free op-\ntimization methods can be accelerated by parallel or distributed computation, which alleviates\ntheir slow convergence. Furthermore, for parallel/distributed derivative-free methods, the data\ncommunication cost is lower compared with gradient-based ones, since only one-dimensional\nscalars (ﬁtness values) instead of gradient vectors or Hessian matrices need to be conveyed.\nThis merit could further compensates for the low convergence rate.\nThe ES method [Salimans et al., 2017] mentioned in the fourth section is a typical paral-\nlel case of the derivative-free method. Moreover, by the virtue of high parallelization property\nof ES, a novel communication strategy based on common random numbers is introduced to\nfurther reduce the communication cost, and it can make the algorithm well scaled to a large\nnumber of parallel workers. It takes only 10 minutes to reach 6000 long-term reward in the\n3D Humanoid environment with 1440 cores. And with the increasing of core amount, the con-\nsumption of training time shows a relatively stable linear decline. SRACOS [Hu et al., 2017]\nis a classiﬁcation-based derivative-free optimization algorithm for direct policy search. Its dis-\ntributed version ZOOpt [Liu et al., 2017], an open-source toolkit, is implemented by the Julia\nlanguage. And the experimental result shows that the algorithm can support more than 100\nprocesses for parallel computing. The parallel strategies of ES and SRACOS are similar. Both\nare based on a certain generation module to construct multiple policies simultaneously. ES is\nbased on Gaussian perturbation for current parameters, and SRACOS is based on a randomized\ncoordinate shrinking classiﬁer. Policies are run and their ﬁtness values are evaluated in paral-\nlel. The generation module is updated by these ﬁtness values, and the next batch of policies is\nconstructed.\nReinforcement learning algorithms sometimes are sensitive to the hyper-parameters, such\nas learning rate and entropy penalty coefﬁcient. Finely tuned hyper-parameters can accelerate\nlearning and improve the performance of policy. Previous hyper-parameter adjustment methods\nare parallel search algorithms, which use the performance of ﬁnal policy under different hyper-\nparameters to adjust hyper-parameters. Such methods require a large amount of computational\nresources to simultaneously optimize multiple models. Population-based training (PBT) [Jader-\nberg et al., 2017] is a framework to simultaneously optimize model parameters and adjust hyper-\nparameters with a derivative-free method. The method is shown in Algorithm 1. The step(θ | h)\nis a function to conduct model optimization according to the current hyper-parameters h and the\nmodel parameters θ. The optimization method depends on tasks (e.g., SGD for supervised learn-\ning and DDPG for reinforcement learning). The eval(h) is a function to realize performance\nevaluation. The evaluation method also depends on tasks (e.g., the loss for supervised learning\nand the total reward of a policy for reinforcement learning). If the model evaluation result p\nis better than a threshold, population-based evolution will start to adjust the hyper-parameters\n29\nAlgorithm 1 Population Based Training (PBT) [Jaderberg et al., 2017]\nProcedure: TRAIN (P)\n1: initial population P\n2: for (θ, h, p, t) ∈P (asynchronously in parallel) do\n3:\nwhile not end of training do\n4:\none step of optimization: θ ←step(θ | h)\n5:\ncurrent model evaluation: p ←eval(h)\n6:\nif ready(p, t, P) then\n7:\nuse the rest of population to ﬁnd better solution: h′, θ′ ←exploit(h, θ, p, P)\n8:\nif θ ̸= θ′ then\n9:\nproduce new h: h, θ ←explore(h′, θ′, P)\n10:\nnew model evaluation: p ←eval(θ)\n11:\nend if\n12:\nend if\n13:\nupdate P with new (θ, h, p, t + 1)\n14:\nend while\n15: end for\n16: return θ with the highest p in P\nh. There are two main functions in the population-based evolution, i.e., exploit(h, θ, p, P) and\nexplore(h′, θ′, P). The exploit(h, θ, p, P) function exploits the best hyper-parameters h and its\nmodel parameters θ to conduct evolution. For example, replacing the worst solution in P with\nthe best one. The explore(h′, θ′, P) function explores the new unknown hyper-parameters h\nbased on the current population P. For example, perturbing h′ with Gaussian noise to generate\nthe new hyper-parameters h. After population evolution, the new hyper-parameters h as well as\nits model parameters θ will be added to P. The main characteristic of PBT is that the process of\nparameter optimization and hyper-parameters adjustment is hybrid, which can not only reduce\nthe computational cost but also help algorithm adjust hyper-parameters in dynamic environ-\nments. PBT has succeeded in RL when combining with A3C [Mnih et al., 2016]. The empirical\nresults in the DeepMind Lab 3D environment [Beattie et al., 2016], Atari games [Bellemare et\nal., 2013; Brockman et al., 2016], and the StarCraft II environment tasks [Vinyals et al., 2017]\nshow that PBT increases the ﬁnal performance of the agents when trained with the same number\nof episodes.\nRay [Moritz et al., 2018], a distributed framework proposed by the researchers from UC\nBerkeley, integrates the distributed implementation of the PBT algorithm. Online meta-learning\nby parallel algorithm competition (OMPAC) [Elfwing et al., 2018] is also a derivative-free\ndistributed hybrid optimization algorithm in RL. This algorithm can be regarded as an improve-\nment based on PBT. The main difference between OMPAC and PBT is the evolution mecha-\n30\nnism. In OMPAC, all solutions are evaluated in a synchronized manner after a ﬁxed number\nof samples, and OMPAC uses the stochastic universal sampling [Baker, 1987] to select solu-\ntions for continual learning. In PBT, solutions are evaluated asynchronously (e.g., after a ﬁxed\nnumber of training steps). Jaderberg et al. [Jaderberg et al., 2019] demonstrated that the agent\nusing only pixels and game points as input could learn to play highly competitively in a popular\nmulti-player ﬁrst-person video game. The ingredients in the proposed method include PBT of\nagents, internal reward optimization, and hierarchical RL with scalable computational archi-\ntectures. Jung ea al. [Jung et al., 2020] proposed the population-guided parallel policy search\n(P3S) to improve the performance of RL. P3S employs a population to search a better policy\nby exploiting the best policy information which is similar to PBT. However, the way of using\nthe best policy information is different. In P3S, instead of copying the parameter of the best\nlearner, it introduces a soft manner to guide the population for better search in the policy space.\nP3S maintains a batch of identical learners with their own value-functions and policies sharing\na common experience replay buffer, and searches a good policy cooperated by the guidance of\nthe best policy information. In a nutshell, parallel and distributed computation is necessary for\nderivative-free reinforcement learning.\n8\nDiscussion and Conclusion\nIn this article, we summarize some recent progress in applying derivative-free optimization to\nreinforcement learning. Since derivative-free optimization is a generally applicable tool, it can\nbe utilized in different levels and aspects of reinforcement learning. Here, we focus on the\naspects of parameter updating, model selection, exploration, and parallel/distributed derivative-\nfree methods. Due to the complexity of policy search in reinforcement learning, successful\nresearch studies of using derivative-free optimization are noticeable in all of these aspects.\nDerivative-free reinforcement learning approaches have their own advantages. Firstly, dur-\ning the optimization process, they do not perform gradient back-propagation, are easy to train,\npossess the ability of search globally, do not care whether the reward function is sparse or dense,\ndo not care the length of time horizons, do not require to carefully tune the discount factor, and\ncan be applied to optimize the non-differentiable policy functions. Secondly, they could provide\nbetter exploration tailored to the problems. Thirdly, they are highly suitable for parallelization\nand only require the low communication cost. Usually, the amount of information that needs to\nbe exchanged among workers does not rely on the size of neural networks.\nAt the same time, we also notice that the combination of derivative-free optimization and\nreinforcement learning has several limitations. These are mainly from the issues of current\nderivative-free optimization methods. The foremost ones could be the issues of sample com-\nplexity and scalability.\n31\nDerivative-free optimization algorithms could be comparable to some state-of-the-art gradient-\nbased ones in reinforcement learning tasks, but usually they require more samples [Salimans et\nal., 2017]. The low sample efﬁciency may block the further application of derivative-free op-\ntimization algorithms to reinforcement learning. Recently, some emerging works focusing on\nimproving sample efﬁciency by the way of introducing sample reuse [Pourchot et al., 2018; Liu\net al., 2019], surrogate models [Stork et al., 2019], importance sampling [Bibi et al., 2020], and\nmomentum [Chen et al., 2019a; Gorbunov et al., 2020]. However, these advances are far from\nenough and more future works on this direction are urgently needed.\nDue to the sampling mechanism, derivative-free optimization methods have limitation of\nscaling up to the search space with very high dimensionality. This is a crucial issue for deep re-\ninforcement learning. Deep neural network models often have more than a million parameters,\nfor which parameter optimization directly by derivative-free optimization can be inefﬁcient.\nWhile some recent works dedicated to tackling the scalability issue [Kandasamy et al., 2015;\nWang et al., 2016; Qian and Yu, 2016; Qian et al., 2016; Yang et al., 2018; Mutny and Krause,\n2018; M¨uller and Glasmachers, 2018; Eriksson et al., 2019; Li et al., 2020], more future works\non developing the scalable derivative-free optimization methods tailored to reinforcement learn-\ning are appealing.\nOther issues include noise-sensitivity as well as structure-insensitivity. Since derivative-free\noptimization relies on solution evaluations, noisy evaluation can badly affect the policy search in\nreinforcement learning [Wang et al., 2018]. Reinforcement learning usually has inner structures\nthat can help better solve the learning, such as hierarchical policy models and curiosity-driven\nexploration, while derivative-free methods commonly ignore the inner structure of the problem.\nHow to make the derivative-free optimization well aware of the inner structure and utilizing the\nstructure is also an important direction and remains under explored.\nFor the above issues, the hybrid of derivative-free and gradient-based algorithms could be a\npotential and promising way. Some existing successful attempts reviewed in this article indicate\nthat, by fusing derivative-free and gradient-based algorithms in a clever manner, the strengths\nof both sides could be absorbed, and more powerful reinforcement learning algorithms would\nbe expected.\nWith the successful cases and fast progress, we expect in a near future that derivative-free\nmethods will play an even more important role in developing novel and efﬁcient reinforcement\nlearning approaches, and hope that this review article will serve as a catalyst for this goal.\nAcknowledgments\nThis article has been accepted by Frontiers of Computer Science with DOI: 10.1007/s11704-\n020-0241-4 in 2020. This work is supported by the Program A for Outstanding Ph.D. Candi-\n32\ndate of Nanjing University, National Science Foundation of China (61876077), Jiangsu Science\nFoundation (BK20170013), and Collaborative Innovation Center of Novel Software Technol-\nogy and Industrialization. The authors would like to thank Xiong-Hui Chen and Zhao-Hua Li\nfor improving the article. The authors also would like to thank the anonymous reviewers for\ntheir detailed comments that have led to a signiﬁcant enhancement of this article.\nReferences\nPieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y. Ng. An application of reinforce-\nment learning to aerobatic helicopter ﬂight. In Advances in Neural Information Processing\nSystems 19, pages 1–8, Vancouver, Canada, 2006.\nAbbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, Nuno Lau, Lu´ıs Paulo Reis, and Gerhard\nNeumann. Model-based relative entropy stochastic search. In Advances in Neural Informa-\ntion Processing Systems 28, pages 3537–3545, Montr´eal, Canada, 2015.\nFrancis R. Bach and Vianney Perchet. Highly-smooth zero-th order online optimization. In\nProceedings of the 29th Conference on Learning Theory, pages 257–283, New York, NY,\n2016.\nJames E. Baker. Reducing bias and inefﬁciency in the selection algorithm. In Proceedings of\nthe 2nd International Conference on Genetic Algorithms, pages 14–21, Hillsdale, NJ, 1987.\nP. L. Bartlett and J. Baxter. Inﬁnite-horizon policy gradient estimation. Journal of Artiﬁcial\nIntelligence Research, 15:319–350, 2001.\nCharles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich\nK¨uttler, Andrew Lefrancq, Simon Green, V´ıctor Vald´es, Amir Sadik, Julian Schrittwieser,\nKeith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen\nKing, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv:1612.03801,\n2016.\nMarc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\nRichard Bellman. A Markovian decision process. Journal of Mathematics and Mechanics,\npages 679–684, 1957.\n33\nAdel Bibi, El Houcine Bergou, Ozan Sener, Bernard Ghanem, and Peter Richt´arik. A stochastic\nderivative-free optimization method with importance sampling: Theory and learning to con-\ntrol. In Proceedings of the 34th AAAI Conference on Artiﬁcial Intelligence, pages 3275–3282,\nNew York, NY, 2020.\nCristian Bodnar, Ben Day, and Pietro Li´o. Proximal distilled evolutionary reinforcement learn-\ning. In Proceedings of the 34th AAAI Conference on Artiﬁcial Intelligence, pages 3283–3290,\nNew York, NY, 2020.\nJustin A. Boyan and Michael L. Littman. Packet routing in dynamically changing networks: A\nreinforcement learning approach. In Advances in Neural Information Processing Systems 6,\npages 671–678, Denver, Colorado, 1993.\nEric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian optimization of\nexpensive cost functions, with application to active user modeling and hierarchical reinforce-\nment learning. arXiv:1012.2599, 2010.\nDimo Brockhoff, Anne Auger, Nikolaus Hansen, Dirk V. Arnold, and Tim Hohm. Mirrored\nsampling and sequential selection for evolution strategies. In Proceedings of the 11th Inter-\nnational Conference on Parallel Problem Solving from Nature, pages 11–21, Krak´ow, Poland,\n2010.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. OpenAI Gym. arXiv:1606.01540, 2016.\nMatthew Brown, Bo An, Christopher Kiekintveld, Fernando Ord´o˜nez, and Milind Tambe. An\nextended study on multi-objective security games. Autonomous Agents and Multi-Agent Sys-\ntems, 28(1):31–71, 2014.\nAdam D. Bull. Convergence rates of efﬁcient global optimization algorithms. Journal of Ma-\nchine Learning Research, 12:2879–2904, 2011.\nRoberto Calandra, Andr´e Seyfarth, Jan Peters, and Marc Peter Deisenroth. An experimental\ncomparison of Bayesian optimization for bipedal locomotion. In Proceedings of the 2014\nIEEE International Conference on Robotics and Automation, pages 1951–1958, Hong Kong,\nChina, 2014.\nRoberto Calandra, Andr´e Seyfarth, Jan Peters, and Marc Peter Deisenroth. Bayesian optimiza-\ntion for learning gaits under uncertainty - An experimental comparison on a dynamic bipedal\nwalker. Annals of Mathematics and Artiﬁcial Intelligence, 76(1-2):5–23, 2016.\n34\nXiong-Hui Chen and Yang Yu. Reinforcement learning with derivative-free exploration. In\nProceedings of the 18th International Conference on Autonomous Agents and MultiAgent\nSystems, pages 1880–1882, Montr´eal, Canada, 2019.\nXiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, and David D. Cox. ZO-\nAdaMM: Zeroth-order adaptive momentum method for black-box optimization. In Advances\nin Neural Information Processing Systems 32, pages 7202–7213, Vancouver, Canada, 2019.\nZefeng Chen, Yuren Zhou, Xiaoyu He, and Siyu Jiang. A restart-based rank-1 evolution strategy\nfor reinforcement learning. In Proceedings of the 28th International Joint Conference on\nArtiﬁcial Intelligence, pages 2130–2136, Macao, China, 2019.\nJames J. Choi, David Laibson, Brigitte C. Madrian, and Andrew Metrick. Reinforcement learn-\ning and savings behavior. The Journal of Finance, 64(6):2515–2534, 2009.\nKrzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, and Adrian\nWeller. Structured evolution with compact architectures for scalable policy optimization.\nIn Proceedings of the 35th International Conference on Machine Learning, pages 969–977,\nStockholm, Sweden, 2018.\nKrzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, and Vikas Sind-\nhwani. From complexity to simplicity: Adaptive ES-active subspaces for blackbox optimiza-\ntion. In Advances in Neural Information Processing Systems 32, Vancouver, Canada, 2019.\nPatryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. Back to basics: Benchmarking canonical\nevolution strategies for playing atari. In Proceedings of the 27th International Joint Confer-\nence on Artiﬁcial Intelligence, pages 1419–1426, Stockholm, Sweden, 2018.\nC´edric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: Decoupling exploration and\nexploitation in deep reinforcement learning algorithms. In Proceedings of the 35th Interna-\ntional Conference on Machine Learning, pages 1038–1047, Stockholm, Sweden, 2018.\nAndrew R. Conn, Katya Scheinberg, and Lu´ıs N. Vicente.\nIntroduction to Derivative-Free\nOptimization. SIAM, Philadelphia, PA, 2009.\nPaul G. Constantine. Active Subspaces - Emerging Ideas for Dimension Reduction in Parameter\nStudies, volume 2 of SIAM spotlights. SIAM, Philadelphia, PA, 2015.\nEdoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley,\nand Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning\nvia a population of novelty-seeking agents. In Advances in Neural Information Processing\nSystems 31, pages 5032–5043, Montr´eal, Canada, 2018.\n35\nD. Dasgupta and D. McGregor. Designing application-speciﬁc neural networks using the struc-\ntured genetic algorithm. In Proceedings of the International Conference on Combinations of\nGenetic Algorithms and Neural Networks, pages 87–96, Baltimore, Maryland, 1992.\nPieter-Tjerk de Boer, Dirk P. Kroese, Shie Mannor, and Reuven Y. Rubinstein. A tutorial on the\ncross-entropy method. Annals of Operations Research, 134(1):19–67, 2005.\nNando de Freitas, Alexander J. Smola, and Masrour Zoghi. Exponential regret bounds for Gaus-\nsian process bandits with deterministic observations. In Proceedings of the 29th International\nConference on Machine Learning, Edinburgh, UK, 2012.\nThomas G. Dietterich. Machine learning research: Four current directions. Artiﬁcial Intelli-\ngence Magazine, 18(4):97–136, 1997.\nPedro M. Domingos. A few useful things to know about machine learning. Communications of\nthe ACM, 55(10):78–87, 2012.\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep\nreinforcement learning for continuous control.\nIn Proceedings of the 33nd International\nConference on Machine Learning, pages 1329–1338, New York, NY, 2016.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2:\nFast reinforcement learning via slow reinforcement learning. arXiv:1611.02779, 2016.\nJohn C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for\nzero-order convex optimization: The power of two function evaluations. IEEE Transactions\non Information Theory, 61(5):2788–2806, 2015.\nSayna Ebrahimi, Anna Rohrbach, and Trevor Darrell. Gradient-free policy architecture search\nand adaptation. In Proceedings of the 1st Conference on Robot Learning, pages 505–514,\nMountain View, CA, 2017.\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Online meta-learning by parallel algorithm com-\npetition. In Proceedings of the 2018 Conference on Genetic and Evolutionary Computation,\npages 426–433, New York, NY, 2018.\nDavid Eriksson, Michael Pearce, Jacob R. Gardner, Ryan Turner, and Matthias Poloczek. Scal-\nable global optimization via local Bayesian optimization. In Advances in Neural Information\nProcessing Systems 32, pages 5497–5508, Vancouver, Canada, 2019.\n36\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In Proceedings of the 34th International Conference on Machine\nLearning, pages 1126–1135, Sydney, Australia, 2017.\nMichael J. Frank, Lauren C. Seeberger, and Randall C. O’reilly. By carrot or by stick: Cognitive\nreinforcement learning in parkinsonism. Science, 306(5703):1940–1943, 2004.\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error\nin actor-critic methods. In Proceedings of the 35th International Conference on Machine\nLearning, pages 1582–1591, Stockholm, Sweden, 2018.\nLior Fuks, Noor Awad, Frank Hutter, and Marius Lindauer. An evolution strategy with pro-\ngressive episode lengths for playing games. In Proceedings of the 28th International Joint\nConference on Artiﬁcial Intelligence, pages 1234–1240, Macao, China, 2019.\nAdam Gaier and David Ha. Weight agnostic neural networks. In Advances in Neural Informa-\ntion Processing Systems 32, pages 5365–5379, Vancouver, Canada, 2019.\nTanmay Gangwani and Jian Peng. Policy optimization by genetic distillation. In Proceedings\nof the 6th International Conference on Learning Representations, Vancouver, Canada, 2018.\nJason Gauci and Kenneth O. Stanley. A case study on the critical role of geometric regularity\nin machine learning. In Proceedings of the 23rd AAAI Conference on Artiﬁcial Intelligence,\npages 628–633, Chicago, IL, 2008.\nJohn Geweke. Antithetic acceleration of Monte Carlo integration in Bayesian inference. Journal\nof Econometrics, 38(1-2):73–89, 1988.\nEduard A. Gorbunov, Adel Bibi, Ozan Sener, El Houcine Bergou, and Peter Richt´arik.\nA\nstochastic derivative free optimization method with momentum. In Proceedings of the 8th\nInternational Conference on Learning Representations, Addis Ababa, Ethiopia, 2020.\nDavid Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Ad-\nvances in Neural Information Processing Systems 31, pages 2455–2467, Montr´eal, Canada,\n2018.\nNikolaus Hansen and Andreas Ostermeier. Adapting arbitrary normal mutation distributions\nin evolution strategies: The covariance matrix adaptation.\nIn Proceedings of 1996 IEEE\nInternational Conference on Evolutionary Computation, pages 312–317, Nayoya University,\nJapan, 1996.\n37\nNikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolu-\ntion strategies. Evolutionary Computation, 9(2):159–195, 2001.\nNikolaus Hansen, Sibylle D. M¨uller, and Petros Koumoutsakos. Reducing the time complex-\nity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES).\nEvolutionary Computation, 11(1):1–18, 2003.\nNikolaus Hansen, Dirk V. Arnold, and Anne Auger. Evolution strategies. In Janusz Kacprzyk\nand Witold Pedrycz, editors, Springer Handbook of Computational Intelligence, pages 871–\n898. Springer, Berlin, Heidelberg, 2015.\nTatsunori Hashimoto, Steve Yadlowsky, and John C. Duchi. Derivative free optimization via\nrepeated classiﬁcation. In Proceedings of the 2018 International Conference on Artiﬁcial\nIntelligence and Statistics, pages 2027–2036, Playa Blanca, Spain, 2018.\nMatthew\nJ.\nHausknecht,\nPiyush\nKhandelwal,\nRisto\nMiikkulainen,\nand\nPeter\nStone.\nHyperNEAT-GGP: A hyperNEAT-based Atari general game player. In Proceedings of the\n2012 Conference on Genetic and Evolutionary Computation, pages 217–224, Philadelphia,\nPA, 2012.\nMatthew J. Hausknecht, Joel Lehman, Risto Miikkulainen, and Peter Stone. A neuroevolution\napproach to general Atari game playing. IEEE Transactions on Computational Intelligence\nand AI in Games, 6(4):355–366, 2014.\nJun He and Xin Yao. Drift analysis and average time complexity of evolutionary algorithms.\nArtiﬁcial Intelligence, 127(1):57–85, 2001.\nVerena Heidrich-Meisner and Christian Igel. Evolution strategies for direct policy search. In\nProceedings of the 10th International Conference on Parallel Problem Solving from Nature,\npages 428–437, Dortmund, Germany, 2008.\nVerena Heidrich-Meisner and Christian Igel. Hoeffding and Bernstein races for selecting poli-\ncies in evolutionary direct policy search. In Proceedings of the 26th International Conference\non Machine Learning, pages 401–408, Montr´eal, Canada, 2009.\nVerena Heidrich-Meisner and Christian Igel. Neuroevolution strategies for episodic reinforce-\nment learning. Journal of Algorithms, 64(4):152–168, 2009.\nJ. H. Holland. Adaptation in Natural and Artiﬁcial Systems. The University of Michigan Press,\nAnn Arbor, MI, 1975.\n38\nRein Houthooft, Yuhua Chen, Phillip Isola, Bradly C. Stadie, Filip Wolski, Jonathan Ho, and\nPieter Abbeel. Evolved policy gradients. In Advances in Neural Information Processing\nSystems 31, pages 5405–5414, Montr´eal, Canada, 2018.\nYi-Qi Hu, Hong Qian, and Yang Yu. Sequential classiﬁcation-based optimization for direct\npolicy search. In Proceedings of the 31st AAAI Conference on Artiﬁcial Intelligence, pages\n2029–2035, San Francisco, CA, 2017.\nChen Huang, Simon Lucey, and Deva Ramanan. Learning policies for adaptive tracking with\ndeep feature cascades. In IEEE International Conference on Computer Vision, pages 105–\n114, Venice, Italy, 2017.\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali\nRazavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and\nKoray Kavukcuoglu. Population based training of neural networks. arXiv:1711.09846, 2017.\nMax Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia\nCastaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas\nSonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray\nKavukcuoglu, and Thore Graepel. Human-level performance in 3d multiplayer games with\npopulation-based reinforcement learning. Science, 364(6443):859–865, 2019.\nKevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query complexity of derivative-\nfree optimization. In Advances in Neural Information Processing Systems 25, pages 2681–\n2689, Lake Tahoe, NV, 2012.\nWhiyoung Jung, Giseung Park, and Youngchul Sung. Population-guided parallel policy search\nfor reinforcement learning. In Proceedings of the 8th International Conference on Learning\nRepresentations, Addis Ababa, Ethiopia, 2020.\nK. Kandasamy, J. Schneider, and B. Poczos.\nHigh dimensional Bayesian optimisation and\nbandits via additive models. In Proceedings of the 32nd International Conference on Machine\nLearning, pages 295–304, Lille, France, 2015.\nK. Kawaguchi, L. P. Kaelbling, and T. Lozano-Perez. Bayesian optimization with exponential\nconvergence. In Advances in Neural Information Processing Systems 28, pages 2809–2817,\nMontr´eal, Canada, 2015.\nKenji Kawaguchi, Yu Maruyama, and Xiaoyu Zheng. Global continuous optimization with\nerror bound and fast convergence. Journal of Artiﬁcial Intelligence Research, 56:153–195,\n2016.\n39\nShauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learn-\ning. In Advances in Neural Information Processing Systems 31, pages 1196–1208, Montr´eal,\nCanada, 2018.\nNate Kohl and Risto Miikkulainen. Evolving neural networks for strategic decision-making\nproblems. Neural Networks, 22(3):326–337, 2009.\nTamara G. Kolda, Robert Michael Lewis, and Virginia Torczon. Optimization by direct search:\nNew perspectives on some classical and modern methods. SIAM Review, 45(3):385–482,\n2003.\nJan Koutn´ık, Giuseppe Cuccu, J¨urgen Schmidhuber, and Faustino J. Gomez. Evolving large-\nscale neural networks for vision-based reinforcement learning. In Proceedings of the 2013\nConference on Genetic and Evolutionary Computation, pages 1061–1068, Amsterdam, The\nNetherlands, 2013.\nH. J. Kushner. A new method of locating the maximum of an arbitrary multipeak curve in the\npresence of noise. Journal of Basic Engineering, 86:97–106, 1964.\nJoel Lehman and Kenneth O. Stanley. Abandoning objectives: Evolution through the search for\nnovelty alone. Evolutionary Computation, 19(2):189–223, 2011.\nJoel Lehman, Jay Chen, Jeff Clune, and Kenneth O. Stanley. ES is more than just a tradi-\ntional ﬁnite-difference approximator. In Proceedings of the 2018 Conference on Genetic and\nEvolutionary Computation, pages 450–457, Kyoto, Japan, 2018.\nJoel Lehman, Jay Chen, Jeff Clune, and Kenneth O. Stanley. Safe mutations for deep and\nrecurrent neural networks through output gradients. In Proceedings of the 2018 Conference\non Genetic and Evolutionary Computation, pages 117–124, Kyoto, Japan, 2018.\nBenjamin Letham and Eytan Bakshy. Bayesian optimization for policy search via online-ofﬂine\nexperimentation. arXiv:1904.01049, 2019.\nZhenhua Li, Qingfu Zhang, Xi Lin, and Hui-Ling Zhen. Fast covariance matrix adaptation\nfor large-scale black-box optimization. IEEE Transaction on Cybernetics, 50(5):2073–2083,\n2020.\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval\nTassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learn-\ning. In Proceedings of the 4th International Conference on Learning Representations, San\nJuan, Puerto Rico, 2016.\n40\nYu-Ren Liu, Yi-Qi Hu, Hong Qian, Yang Yu, and Chao Qian. ZOOpt: Toolbox for derivative-\nfree optimization. arXiv:1801.00329, 2017.\nGuoqing Liu, Li Zhao, Feidiao Yang, Jiang Bian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Trust\nregion evolution strategies. In Proceedings of the 33rd AAAI Conference on Artiﬁcial Intelli-\ngence, pages 4352–4359, Honolulu, HI, 2019.\nJose A. Lozano, Pedro Larranaga, Inaki Inza, and Endika Bengoetxea. Towards a New Evolu-\ntionary Computation: Advances on Estimation of Distribution Algorithms. Springer-Verlag,\nBerlin, Germany, 2006.\nDhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter Bartlett, and Martin J.\nWainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic\nsystems. In Proceedings of the 22nd International Conference on Artiﬁcial Intelligence and\nStatistics, pages 2916–2925, Naha, Japan, 2019.\nHoria Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies\nis competitive for reinforcement learning. In Advances in Neural Information Processing\nSystems 31, pages 1805–1814, Montr´eal, Canada, 2018.\nAlonso Marco, Felix Berkenkamp, Philipp Hennig, Angela P. Schoellig, Andreas Krause, Stefan\nSchaal, and Sebastian Trimpe. Virtual vs. real: Trading off simulations and physical exper-\niments in reinforcement learning with Bayesian optimization. In Proceedings of the 2017\nIEEE International Conference on Robotics and Automation, pages 1557–1563, Singapore,\n2017.\nMelanie Mitchell. An Introduction to Genetic Algorithms. MIT Press, Cambridge, MA, 1998.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, and Georg Ostrovski.\nHuman-level control through deep reinforcement learning.\nNature, 518(7540):529–533,\n2015.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep rein-\nforcement learning. In Proceedings of the 33rd International Conference on Machine Learn-\ning, pages 1928–1937, New York, NY, 2016.\nJ. Moˇckus, V. Tiesis, and A. ˇZilinskas. Toward global optimization. In L. C. W. Dixon and\nG. P. Szego, editors, The Application of Bayesian Methods for Seeking the Extremum, pages\n117–128. Elsevier, Amsterdam, Netherlands, 1978.\n41\nDavid E. Moriarty, Alan C. Schultz, and John J. Grefenstette. Evolutionary algorithms for\nreinforcement learning. Journal of Artiﬁcial Intelligence Research, 11:241–276, 1999.\nPhilipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,\nWilliam Paul, Michael I. Jordan, and Ion Stoica. Ray: A distributed framework for emerg-\ning AI applications. In The 13th USENIX Symposium on Operating Systems Design and\nImplementation, pages 561–577, Carlsbad, CA, 2018.\nGregory Morse and Kenneth O. Stanley. Simple evolutionary optimization can rival stochastic\ngradient descent in neural networks. In Proceedings of the 2016 Conference on Genetic and\nEvolutionary Computation, pages 477–484, Denver, CO, 2016.\nNils M¨uller and Tobias Glasmachers. Challenges in high-dimensional reinforcement learning\nwith evolution strategies. In Proceedings of the 15th International Conference on Parallel\nProblem Solving from Nature, pages 411–423, Coimbra, Portugal, 2018.\nR´emi Munos. From bandits to Monte-Carlo tree search: The optimistic principle applied to\noptimization and planning. Foundations and Trends in Machine Learning, 7(1):1–129, 2014.\nMojmir Mutny and Andreas Krause. Efﬁcient high dimensional Bayesian optimization with\nadditivity and quadrature fourier features. In Advances in Neural Information Processing\nSystems 31, pages 9019–9030, Montr´eal, Canada, 2018.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven explo-\nration by self-supervised prediction. In Proceedings of the 34th International Conference on\nMachine Learning, pages 2778–2787, Sydney, Australia, 2017.\nJan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–1190, 2008.\nMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen,\nXi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise\nfor exploration. In Proceedings of the 6th International Conference on Learning Represen-\ntations, Vancouver, Canada, 2018.\nAlo¨ıs Pourchot, Nicolas Perrin, and Olivier Sigaud. Importance mixing: Improving sample\nreuse in evolutionary policy search methods. arXiv:1808.05832, 2018.\nHong Qian and Yang Yu. Scaling simultaneous optimistic optimization for high-dimensional\nnon-convex functions with low effective dimensions. In Proceedings of the 30th AAAI Con-\nference on Artiﬁcial Intelligence, pages 2000–2006, Phoenix, AZ, 2016.\n42\nChao Qian, Yang Yu, and Zhi-Hua Zhou. Subset selection by pareto optimization. In Advances\nin Neural Information Processing Systems 28, pages 1765–1773, Montr´eal, Canada, 2015.\nH. Qian, Y.-Q. Hu, and Y. Yu. Derivative-free optimization of high-dimensional non-convex\nfunctions by sequential random embeddings. In Proceedings of the 25th International Joint\nConference on Artiﬁcial Intelligence, pages 1946–1952, New York, NY, 2016.\nChao Qian, Jing-Cheng Shi, Yang Yu, and Ke Tang. On subset selection with general cost con-\nstraints. In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence,\npages 2613–2619, Melbourne, Australia, 2017.\nCarl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine\nLearning. MIT Press, Cambridge, Massachusetts, 2006.\nE. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin. Large-\nscale evolution of image classiﬁers. In Proceedings of the 34th International Conference on\nMachine Learning, pages 2902–2911, Sydney, Australia, 2017.\nEsteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for\nimage classiﬁer architecture search. arXiv:1802.01548, 2018.\nLuis Miguel Rios and Nikolaos V. Sahinidis. Derivative-free optimization: A review of al-\ngorithms and comparison of software implementations.\nJournal of Global Optimization,\n56(3):1247–1293, 2013.\nSebastian Risi and Julian Togelius. Neuroevolution in games: State of the art and open chal-\nlenges. IEEE Transactions on Computational Intelligence and AI in Games, 9(1):25–41,\n2017.\nSt´ephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and\nstructured prediction to no-regret online learning. In Proceedings of the 14th International\nConference on Artiﬁcial Intelligence and Statistics, pages 627–635, Fort Lauderdale, FL,\n2011.\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and\nXi Chen. Improved techniques for training GANs. In Advances in Neural Information Pro-\ncessing Systems 29, pages 2226–2234, Barcelona, Spain, 2016.\nTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies\nas a scalable alternative to reinforcement learning. arXiv:1703.03864, 2017.\n43\nKazuyuki Samejima, Yasumasa Ueda, Kenji Doya, and Minoru Kimura. Representation of\naction-speciﬁc reward values in the striatum. Science, 310(5752):1337–1340, 2005.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust\nregion policy optimization. In Proceedings of the 32nd International Conference on Machine\nLearning, pages 1889–1897, Lille, France, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv:1707.06347, 2017.\nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking\nthe human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE,\n104(1):148–175, 2016.\nShai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep\nlearning. In Proceedings of the 34th International Conference on Machine Learning, pages\n3067–3075, Sydney, Australia, 2017.\nJing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and Anxiang Zeng. Virtual-Taobao: Vir-\ntualizing real-world online retail environment for reinforcement learning. In Proceedings of\nthe 33rd AAAI Conference on Artiﬁcial Intelligence, pages 4902–4909, Honolulu, HI, 2019.\nO. Sigaud and S. W. Wilson.\nLearning classiﬁer systems: A survey.\nSoft Computing,\n11(11):1065–1078, 2007.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den\nDriessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanc-\ntot.\nMastering the game of Go with deep neural networks and tree search.\nNature,\n529(7587):484–489, 2016.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,\nKaren Simonyan, and Demis Hassabis.\nA general reinforcement learning algorithm that\nmasters chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.\nSatinder P. Singh and Richard S. Sutton.\nReinforcement learning with replacing eligibility\ntraces. Machine Learning, 22(1-3):123–158, 1996.\nJasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of ma-\nchine learning algorithms. In Advances in Neural Information Processing Systems 25, pages\n2960–2968, Lake Tahoe, NV, 2012.\n44\nNiranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian pro-\ncess optimization in the bandit setting: No regret and experimental design. In Proceedings\nof the 27th International Conference on Machine Learning, pages 1015–1022, Haifa, Israel,\n2010.\nK. O. Stanley and R. Miikkulainen. Evolving neural networks through augmenting topologies.\nEvolutionary Computation, 10(2):99–127, 2002.\nKenneth O. Stanley and Risto Miikkulainen. Efﬁcient reinforcement learning through evolving\nneural network topologies. In Proceedings of the 2002 Conference on Genetic and Evolu-\ntionary Computation, pages 569–577, New York, NY, 2002.\nKenneth O. Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural net-\nworks through neuroevolution. Nature Machine Intelligence, 1(1):24–35, 2019.\nJ¨org Stork, Martin Zaefferer, Thomas Bartz-Beielstein, and A. E. Eiben. Surrogate models for\nenhancing the efﬁciency of neuroevolution in reinforcement learning. In Proceedings of the\n2019 Conference on Genetic and Evolutionary Computation, pages 934–942, Prague, Czech\nRepublic, 2019.\nFreek Stulp and Olivier Sigaud. Path integral policy improvement with covariance matrix adap-\ntation. In Proceedings of the 29th International Conference on Machine Learning, Edinburgh,\nUK, 2012.\nFelipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley,\nand Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for\ntraining deep neural networks for reinforcement learning. arXiv:1712.06567, 2017.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,\nCambridge, Massachusetts, 1998.\nKevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task Bayesian optimization. In\nAdvances in Neural Information Processing Systems 26, pages 2004–2012, Lake Tahoe, NV,\n2013.\nIstvan Szita and Andr´as L¨orincz. Learning Tetris using the noisy cross-entropy method. Neural\nComputation, 18(12):2936–2941, 2006.\nYunhao Tang, Krzysztof Choromanski, and Alp Kucukelbir. Variance reduction for evolution\nstrategies via structured control variates. arXiv:1906.08868, 2019.\n45\nMatthew E. Taylor, Shimon Whiteson, and Peter Stone. Comparing evolutionary and temporal\ndifference methods in a reinforcement learning domain. In Proceedings of the 2006 Confer-\nence on Genetic and Evolutionary Computation, pages 1321–1328, Seattle, WA, 2006.\nC. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-WEKA: Combined selection\nand hyperparameter optimization of classiﬁcation algorithms. In Proceedings of the 19th\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages\n847–855, Chicago, IL, 2013.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based\ncontrol.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,\npages 5026–5033, Vilamoura, Portugal, 2012.\nAnirudh Vemula, Wen Sun, and J. Andrew Bagnell. Contrasting exploration in parameter and\naction space: A zeroth-order optimization perspective. In Proceedings of the 22nd Interna-\ntional Conference on Artiﬁcial Intelligence and Statistics, pages 2926–2935, Naha, Japan,\n2019.\nNgo Anh Vien, Viet-Hung Dang, and TaeChoong Chung. A covariance matrix adaptation evo-\nlution strategy for direct policy search in reproducing kernel Hilbert space. In Proceedings\nof The 9th Asian Conference on Machine Learning, pages 606–621, Seoul, Korea, 2017.\nNgo Anh Vien, Heiko Zimmermann, and Marc Toussaint. Bayesian functional optimization. In\nProceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence, pages 4171–4178, New\nOrleans, LA, 2018.\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\nMichelle Yeo, Alireza Makhzani, Heinrich K¨uttler, John P. Agapiou, Julian Schrittwieser,\nJohn Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt,\nDavid Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David\nLawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. StarCraft II: A new challenge\nfor reinforcement learning. arXiv:1708.04782, 2017.\nYi-Chi Wang and John M. Usher. Application of reinforcement learning for agent-based pro-\nduction scheduling. Engineering Applications of Artiﬁcial Intelligence, 18(1):73–82, 2005.\nZ. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. D. Freitas. Bayesian optimization in a billion\ndimensions via random embeddings. Journal of Artiﬁcial Intelligence Research, 55:361–387,\n2016.\n46\nHong Wang, Hong Qian, and Yang Yu. Noisy derivative-free optimization with value suppres-\nsion. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence, pages 1447–\n1454, New Orleans, LA, 2018.\nShimon Whiteson and Peter Stone. Evolutionary function approximation for reinforcement\nlearning. Journal of Machine Learning Research, 7:877–917, 2006.\nShimon Whiteson and Peter Stone. Sample-efﬁcient evolutionary function approximation for\nreinforcement learning. In Proceedings of the 21st AAAI Conference on Artiﬁcial Intelli-\ngence, pages 518–523, Boston, MA, 2006.\nShimon Whiteson. Evolutionary computation for reinforcement learning. In Marco Wiering\nand Martijn van Otterlo, editors, Reinforcement Learning: State-of-the-Art, pages 325–355.\nSpringer, Berlin, Heidelberg, 2012.\nMarco Wiering and Martijn van Otterlo. Reinforcement Learning: State-of-the-Art. Springer,\nBerlin, Heidelberg, 2012.\nDaan Wierstra, Tom Schaul, Jan Peters, and J¨urgen Schmidhuber. Natural evolution strategies.\nIn Proceedings of the 2008 IEEE Congress on Evolutionary Computation, pages 3381–3387,\nHong Kong, China, 2008.\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J¨urgen Schmidhuber.\nNatural evolution strategies. Journal of Machine Learning Research, 15(1):949–980, 2014.\nAaron Wilson, Alan Fern, and Prasad Tadepalli. Using trajectory data to improve Bayesian\noptimization for reinforcement learning. Journal of Machine Learning Research, 15(1):253–\n282, 2014.\nPeng Yang, Ke Tang, and Xin Yao. Turning high-dimensional optimization into computationally\nexpensive optimization. IEEE Transactions on Evolutionary Computation, 22(1):143–156,\n2018.\nXin Yao. Evolving artiﬁcial neural networks. Proceedings of the IEEE, 87(9):1423–1447, 1999.\nYang Yu and Hong Qian. The sampling-and-learning framework: A statistical view of evolu-\ntionary algorithms. In Proceedings of the 2014 IEEE Congress on Evolutionary Computation,\npages 149–158, Beijing, China, 2014.\nY. Yu and Z.-H. Zhou. A new approach to estimating the expected ﬁrst hitting time of evolu-\ntionary algorithms. Artiﬁcial Intelligence, 172(15):1809–1832, 2008.\n47\nYang Yu, Chao Qian, and Zhi-Hua Zhou. Switch analysis for running time analysis of evolu-\ntionary algorithms. IEEE Transactions on Evolutionary Computation, 19(6):777–792, 2015.\nYang Yu, Hong Qian, and Yi-Qi Hu. Derivative-free optimization via classiﬁcation. In Pro-\nceedings of the 30th AAAI Conference on Artiﬁcial Intelligence, pages 2286–2292, Phoenix,\nAZ, 2016.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. SeqGAN: Sequence generative adver-\nsarial nets with policy gradient. In Proceedings of the 31st AAAI Conference on Artiﬁcial\nIntelligence, pages 2852–2858, San Francisco, CA, 2017.\nWenhao Yu, C. Karen Liu, and Greg Turk.\nPolicy transfer with strategy optimization.\nIn\nProceedings of the 7th International Conference on Learning Representations, New Orleans,\nLA, 2019.\nYang Yu. Towards sample efﬁcient reinforcement learning. In Proceedings of the 27th Inter-\nnational Joint Conference on Artiﬁcial Intelligence, pages 5739–5743, Stockholm, Sweden,\n2018.\nYuting Zhang, Kihyuk Sohn, Ruben Villegas, Gang Pan, and Honglak Lee. Improving object\ndetection with deep convolutional networks via Bayesian optimization and structured pre-\ndiction. In IEEE Conference on Computer Vision and Pattern Recognition, pages 249–258,\nBoston, MA, 2015.\nXingwen Zhang, Jeff Clune, and Kenneth O. Stanley. On the relationship between the OpenAI\nevolution strategy and stochastic gradient descent. arXiv:1712.06564, 2017.\nAimin Zhou, Jinyuan Zhang, Jianyong Sun, and Guixu Zhang. Fuzzy-classiﬁcation assisted so-\nlution preselection in evolutionary optimization. In Proceedings of the 33rd AAAI Conference\non Artiﬁcial Intelligence, pages 2403–2410, Honolulu, HI, 2019.\nBarret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Pro-\nceedings of the 5th International Conference on Learning Representations, Toulon, France,\n2017.\nBarret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Pro-\nceedings of the 5th International Conference on Learning Representations, Toulon, France,\n2017.\n48\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-02-10",
  "updated": "2021-02-10"
}