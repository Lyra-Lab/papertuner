{
  "id": "http://arxiv.org/abs/2102.06800v1",
  "title": "Reinforcement Learning For Data Poisoning on Graph Neural Networks",
  "authors": [
    "Jacob Dineen",
    "A S M Ahsan-Ul Haque",
    "Matthew Bielskas"
  ],
  "abstract": "Adversarial Machine Learning has emerged as a substantial subfield of\nComputer Science due to a lack of robustness in the models we train along with\ncrowdsourcing practices that enable attackers to tamper with data. In the last\ntwo years, interest has surged in adversarial attacks on graphs yet the Graph\nClassification setting remains nearly untouched. Since a Graph Classification\ndataset consists of discrete graphs with class labels, related work has forgone\ndirect gradient optimization in favor of an indirect Reinforcement Learning\napproach. We will study the novel problem of Data Poisoning (training time)\nattack on Neural Networks for Graph Classification using Reinforcement Learning\nAgents.",
  "text": "Reinforcement Learning For Data Poisoning on\nGraph Neural Networks\nJacob Dineen,\nA S M Ahsan-Ul Haque,\nMatthew Bielskas\nDepartment of Computer Science, University of Virginia, Charlottesville, VA 22904\n[jd5ed, ah3wj, mb6xn]@virginia.edu\nAbstract—Adversarial Machine Learning has emerged\nas a substantial subﬁeld of Computer Science due to a\nlack of robustness in the models we train along with\ncrowdsourcing practices that enable attackers to tamper\nwith data. In the last two years, interest has surged in\nadversarial attacks on graphs yet the Graph Classiﬁcation\nsetting remains nearly untouched. Since a Graph Classiﬁ-\ncation dataset consists of discrete graphs with class labels,\nrelated work has forgone direct gradient optimization in\nfavor of an indirect Reinforcement Learning approach. We\nwill study the novel problem of Data Poisoning (training-\ntime) attack on Neural Networks for Graph Classiﬁcation\nusing Reinforcement Learning Agents.\nI. INTRODUCTION\nThe success of Deep Learning algorithms in appli-\ncations to tasks in computer vision, natural language\nprocessing, and reinforcement learning has induced cau-\ntious deployment behavior, particularly in safety-critical\napplications, due to their ability to be inﬂuenced by ad-\nversarial examples, or attacks [1], [2]. For real-world de-\nployment, a level of trust in the robustness of algorithms\nto adversarial attacks is not only desired but required.\nResearch by Goodfellow et al. in the areas of Generative\nAdversarial Networks (GANs) and Adversarial Training\n[3], [4] has inﬂuenced a sub-ﬁeld of Deep Learning\nfocused on exactly the above, where the synthesis of\nviable attacks on neural network architectures, under\nWhite-Box, Black-Box or Grey-Box Attacks, can pro-\nvide meaningful insight into both the models themselves\nand the preventative measures to be taken to ensure\ncorrectness (Adversarial Defenses [5]).\nHere, we study a less saturated version of the adver-\nsarial setting: The effect of inducing a Poison attack\non a Graph Neural Network under the task of graph\nclassiﬁcation [6], [7]. The agent is trained to ﬁnd an\noptimal policy, under Reinforcement Learning (RL) prin-\nciples, such that they can inject, modify, or delete graph\nstructure, or features under Black-Box Attacks. Such\nattacks assume that the agent has little to no underlying\nknowledge of the information regarding the target neural\nnetwork (architecture, parameters, etc.), as is most often\nthe case in real-world applications of machine learning\nalgorithms, although ML-as-a-service systems provide\ninﬁltrators with alternative methods to attack [8]. We\nstructure this problem as a Markov Decision Process, in\nwhich the agent must interact with their environment(s)\n(a set of graphs) to fool the underlying learning algorithm\ninto graph misclassiﬁcation.\nII. BACKGROUND\nA. Preliminaries\nWe introduce, with brevity, necessary background\ninformation regarding concepts and notation spanning\nGraph Mining and Reinforcement Learning in this sec-\ntion.\n1) Adversarial Attacks: In this paper, we consider\nan adversarial example to be an intentionally perturbed\ninstance of data, whose purpose is to fool the learning al-\ngorithm [9]. The attacker’s main goal is to induce model\nmisspeciﬁcation given a particular task such that imposed\nmodiﬁcations to the original data are unbeknownst to\nthe model or a human observer. Perturbations in the\ncontext of Graph Machine Learning are generally of the\nform of structure modiﬁcation, i.e. node, edge, or sub-\ngraph modiﬁcation, or feature modiﬁcations concerning\nparticular nodes in the graph. Such attacks fall under a\ntaxonomy noted in [2]: Evasion attacks and Poisoning\nattacks. While our focus here will be on the latter, we\nintroduce Evasion Attacks for completeness.\nEvasion attacks: Evasion attacks are adversarial\nattacks where the underlying attack occurs after the\nspeciﬁed learning algorithm is fully trained, e.g., the\narchitecture and the learnable parameters are ﬁxed and\nimmutable.\nPoison attacks: Poison attacks occur before or during\nthe model training phase. In this way, we induce model\nmisspeciﬁcation during the parameter estimation phase\nof our learning paradigm.\narXiv:2102.06800v1  [cs.LG]  12 Feb 2021\n2) Reinforcement Learning:\nThe purpose of our\nwork is to solve a Markov Decision Process (MDP) in\nwhich the agent(s) of the system is to solve sequential\ndecision-making processes. MDPs are popular frame-\nworks for superposing agent-environment interaction and\nsimulation. Formally, an MDP can be represented by the\ntuple (S, A, Pa, Ra), where S is the state space, A is\nthe action space, Pa, or T deﬁnes the dynamics of the\nstate space (the probability that an action at a given state\nwill lead to a subsequent state, the transition probability\nfunction), and Ra is the reward function. Given the\nintroduced context, attacker(s) of our learning algorithm\nis considered as agent(s). We establish common notation\n[10] but extend the basic deﬁnition of a Markov Decision\nProcess via augmentation (Universal Markov Decision\nProcesses).\n3) Graph Convolutional Neural Networks (GNN):\nGCNN is based on Convolutional Neural Network and\ngraph embedding. Graph Neural Networks embed infor-\nmation from graph structures [11]. A GNN is essentially\na CNN where the input is a graph (usually in the\nadjacency matrix form) and the feature vectors of every\nnode of that graph.\nIn GNNs every node is represented by their feature\nvectors. Then Neighborhood aggregation is performed\nfor each node. Summing over the embedding of the\nnodes gives representation of the graph. An important\napplication of GNN is node classiﬁcation. For Node\nClassiﬁcation, the GNN maps this input to the output\nfeature vector of every node [12]. DeepWalk [13] is such\nan algorithm for unsupervised node embedding.\n4) Graph Classiﬁcation: Graph classiﬁcation is the\ntask of taking an instance of a graph and predict-\ning its label(s). GNNs in this setting need to map\nnode outputs to a graph-level output (”readout layer”),\nand then typically a Multi-Layer Perceptron (MLP)\nis employed to assign graph embeddings to class la-\nbels. [14] have used 2D CNN for Reddit, Collab and\nIMDB datasets in a supervised setting. [15] have used\nGNN in a semi-supervised setting for Citeseer, Cora,\nPubmed and NELL datasets. Formally, Graph Classi-\nﬁcation is employed over a set of attributed graphs,\nD = {(G1, ℓ1) , (G2, ℓ2) , · · · , (Gn, ℓn)}, each containing\na graph structure and a label, and learning a function\nf : G →L where G is the input space of graphs and L\nis the set of graph labels.\n5) Notation: Each graph Gi ∈G is comprised of\nV vertices, and E edges. The cardinality of V and E\ncorresponds to the size and the connectedness (edges)\nof the graph, respectively. We let din(G, i) represent the\nin-degree of the ith node of G.\nB. Related Work\nAttacks (on GNNs) in the Graph Classiﬁcation setting\nare unique in that so far, they do not take the form of a\ngradient or meta-learning optimization problem directly\non the model hyperparameters. The ﬁrst attack intro-\nduced by [16] is RL-S2V, a Hierarchical Reinforcement\nLearning (HRL) approach for both Node and Graph\nClassiﬁcation at test-time. They propose this because\nHRL agents are better suited for attacking graph data\nwhich is both discrete and combinatorial. The authors\nformulate a Hierarchical Q Learning algorithm that relies\non GNN parameterization, and they successfully train it\nto attack by adding or removing edges in test graphs.\nUltimately this paper treats Graph Classiﬁcation as an\nafterthought and does not provide an insightful way to\nalter RL-S2V in this setting.\nAn evasion attack designed for Graph Classiﬁcation is\nRewatt ([17]), which relies on standard Actor-Critic to\nperturb individual graphs. This is done via ”rewiring”\noperations that simultaneously delete and add edges\nbetween nodes near each other. Rewiring is meant to\nbe subtle so that graph instances maintain similar met-\nrics such as degree centrality. The authors theoretically\njustify Rewatt by showing that rewiring preserves top\neigenvalues in the graph’s Laplacian matrix. While Re-\nwatt is useful at test time, it is ill-suited for a Poisoning\nattack. This is because it is designed to attack a single\ngraph with minimal perturbation. Meanwhile, we have\nthe burden of selecting training graphs that would be\nbest for poisoning, but we can get away with signiﬁcantly\naltering graphs if we limit ourselves to a small portion\nof the training data.\nThe newest entry to the Graph Classiﬁcation literature\ndetails a backdoor attack on Graph Neural Networks\n([18]). The purpose of a backdoor attack is to perturb\ndata points in a particular manner (e.g. wearing an\naccessory in security camera images) so that they are\nmislabeled as a class of your choice. For Graph Classi-\nﬁcation, this is done by generating a random subgraph\nas a trigger and injecting it a portion of training graphs.\nThe reasoning is that random subgraph generation (with\ngood parameters) can produce graphs that aren’t ”too\nanomalous” yet are assigned to the attacker’s class of\nchoice by the tuned GNN. This paper is interesting\nbecause they manipulate subgraphs instead of edges; an\nRL agent that perturbs subgraphs has potential to learn\nquickly due to the reduction in its action space.\nIII. MOTIVATION\nAttacks\non\nalgorithms\ndesigned\nfor\nclassiﬁca-\ntion/regression, detection, generative models, recurrent\nneural networks and deep reinforcement learning, and\neven graph neural networks, are well covered in litera-\nture [19], [20], [21], [1]. The generation of adversarial\nattacks in these spaces generally leads to correspond-\ning defenses, such as Adversarial Training, Perturbation\nDetection [22], and Graph Attention Mechanisms [23]\namong others. Uncovering the root cause of the model\nmalfunction is an equally important part of the process\ntowards widespread industrial adoption of Deep Neural\nNetworks (DNNs).\nAs a practical example, we consider Wikipedia as\na graphical representation - a network. Each node in\nthe overarching network, composed of sub-graphs, could\nrepresent an article. Under the guise of a graph ma-\nchine learning problem, a certain task for the learning\nalgorithm could be to classify nodes as being ’real’\nor ’fake’, e.g. ”Is this article, that is linked to these\nother authentic articles, malicious?”. A viable Graph\nMachine Learning algorithm would be able to detect\nor classify each observation to its correct class. A\nsuccessful adversarial attack on the algorithm would be\none that causes it, through perturbation of its underlying\nstructure, to misclassify the instance. Extending this to\na graph-level classiﬁcation task, in chemoinformatics,\nmolecules are represented as graph-structured objects\n[24], [25], and analysis, testing, and approval of chemical\ncompounds can be detrimentally disturbed via attacks.\nDrug Discovery is a common application that directly\nimpacts our well-being. Thus an adversary can thwart\nthe automation of scientiﬁc progress if they can send\ntraining data e.g. through a crowdsourcing project. We\nsee that many sociological and societal harms can result\nfrom misinformation propagation or malicious modiﬁ-\ncation of data, which is why it’s always important to\nexpose vulnerabilities and defenses in deployed machine\nlearning models.\nTo the best of our knowledge, we present a novel\nframework to the setting of adversarial data poisoning\nin graph classiﬁcation tasks [26]).\nIV. METHODOLOGY\nThe framework for initiating a Poison attack on a\ngraph neural network classiﬁer (GNN) can be decom-\nposed into a multi-step process involving 1) Data Pro-\ncurement, 2) Graph Classiﬁcation, and 3) Reinforcement\nLearning.\nA. Data Procurement\nWe require a graph dataset composed of a set of\ngraph/label pairs. Each graph in the dataset is repre-\nsented by a set of nodes and edges - G(V, E). This\ndataset is instantiated or partitioned into two distinct sets,\nforming training and testing data. The training set has\ntwo purposes: 1) to train a GNN for the task of label\nclassiﬁcation over a set of graphs, and 2) to represent\nthe state of the environment, whereby the reinforcement\nlearning agent can act to perturb the state via a ﬁnite\nset of actions A. As noted in Section II-A, this is a\nPoison attack where we perturb the training set, rather\nthan an evasion attack where the training data and GNN\nare untouched.\nExperimenting with Graph Neural Networks has be-\ncome more accessible with the release of Deep Graph Li-\nbrary [27]. It includes benchmark datasets, popular GNN\nalgorithms across several settings, and easy compatibility\nwith Pytorch and other Deep Learning libraries. Thus we\nwill rely on DGL for graph models and data.\nWe will ﬁrst experiment with a toy dataset class that\nDGL includes for Graph Classiﬁcation. It features eight\ngraph types such as circles and cubes. DGL also includes\nGraph Classiﬁcation datasets that are baselines for Graph\nKernel algorithms, such as PROTEINS, but our main\nfocus of this work will be on the synthetic dataset.\nB. Graph Classiﬁcation\nThe GNN, f, is a mapping from a raw graph g\nto a discrete label associated with the respective class\nof the graph. We visualize the MiniGCDataset from\nDeepGraphLibary in Figure 1 to conceptualize the task\nof graph classiﬁcation. As a comprehensive survey of\nGNNs is beyond the scope of this work, we use a simple\nmulti-layer Graph Convolutional Neural Network (GNN)\nfor all experiments which are described below.\nC. Graph Neural Networks\nHere, f features two convolutional layers, Relu activa-\ntion functions, a readout function, and a fully connected\nlinear layer. We use the node in degrees of G ∈G,\nxG = din(i, G), as input into the GNN and ﬁx the\nreadout function to hg =\n1\n|Y|\nP\nv∈V hv, averaging over\nnode features for each graph in the batch. A batch\nof graphs G ∈V, where V\n⊂G are fed through\nthe GNN, where a graph representation hg is learned\nthrough message passing and graph convolutions over\nall nodes before being passed through to the linear layer\nof the network for classiﬁcation [27]. As most graph\nclassiﬁcation tasks that we explore with are multi-class,\nFigure 1. Our Poison attack features a multi-step process, where the unperturbed model is used to generate a baseline generalization accuracy,\nand the RL algorithm is responsible for perturbing the original training data such that testing accuracy is degraded. Top: Running the original,\nunperturbed dataset through a GNN. Bottom: Applying REINFORCE to the training set to create a new dataset minimally perturbed by a\nthreshold. Ideally, we see a case where these minimal perturbations lead to a large accuracy decrease come testing time.\nthe outputs of the linear layer are passed through a\nsoftmax activation function: σ(z)i =\nezi\nPK\nj=1 ezj .\nFigure 2. Toy Classiﬁcation Task: MiniGCDataset from DGL. Each\nattributed item is a graph, label pair. The graph generation process is\nparameterized by MinNodes, MaxNodes, and the probability of\neach class is uniform.\nWe employ categorical cross-entropy as our loss func-\ntion J throughout, and iteratively learn θ via backprop-\nagation, seen in Equation 1.\n∆wij = −η ∂J\n∂wij\nwij ←wij −η ∂J\n∂wij\n(1)\nWe make note that we treat the hyper-parameterization\nof the GNN as ﬁxed to reduce the elasticity of our\nexperiments. Finely-tuned, and extensively trained neural\nnetworks may be less prone to poison attacks.\nD. Reinforcement Learning\n1) Algorithm: We use a Monte Carlo variant of\na Deep Policy Gradient algorithm: REINFORCE [28],\nwith additional algorithmic validation saved for future\nwork and use a simple random policy π(s) =\n1\n|A| as our\nbaseline. Intuitively, we conceptualize this as: How much\nbetter does a Deep Reinforcement Learning Algorithm\nchoose a sequence of poison points (graphs), such that\ntesting accuracy is degraded, against a fully random\nsearch of the action space?\n2) Reward: We craft a custom Reward function,\nEquation 2, to pass through p (poison points) rewards\nduring a single episode. Let θ be the original, learned\nparameters from training on the unperturbed set, and θp\nbe the perturbed variant. Xtest is the original testing\nset generated a priori. For simplicity, we assume that\nthe application of the actual or perturbed parameters to\nthe testing set yields an accuracy. The Reward function\nR is a signal that measures the difference between the\nunperturbed model’s application to Xtest against the\nperturbed model’s application to Xtest. In other words,\nif the reward is positive, it means that the Poison attack\nwas successful.\nR = θ(X) −θp(X)\n(2)\n3) Environment: Utilizing a form of Deep Reinforce-\nment Learning, a set of inputs, representing the state\nof the system, are required at t to pass through to a\nfunction approximator. We represent this state S (Gt) as\nthe mean, max, min over all Gi ∈G at t. Intuitively,\nthe RL algorithm sees as input a tensor containing each\ngraph’s representation (deﬁned by summary statistics) in\nthe training set, chooses an action, and transitions into a\nnew state based on the perturbations to the training set.\nWe deﬁne the action space A in Table I. At any t, the\nagent can alter the state by performing any a ∈A. To\ndiscretize the action space, we specify that the action of\nadding a subgraph into an existing graph’s structure has\na ﬁxed set of parameters n and p, corresponding to the\nnumber of nodes, and the probability of an edge between\nnodes, respectively. We also constrain the action space\nby permitting node or edge modiﬁcations to be random\nprocesses, e.g, the agent has no control over which node\nor edge to perturb, only that they wish to perform that\naction.\nAction\nName\nDeﬁnition\na1\nsubgraph add\nA random gnp graph is inserted\ninto the existing graph struc-\nture. We ﬁx n = 10 and p =\n0.75 for fairness.\nTable I\nENUMERATING THE ACTION SPACE OF OUR ENVIRONMENT.\nAn optimal policy would learn which sequence of\nactions to take, and their applications to a subset of G, to\nmaximize the episodic degradation in the GNN’s ability\nto generalize over the testing set.\nE. Poison attack\nThe culmination of the multi-step framework results\nin a poisoning attack on the original graph training set,\nre: Appendix 1. Given a set of graph, label pairs, we ﬁrst\ncompute a benchmark accuracy via the application of θ\nto the training set and perform inference on the testing\nset. Given a scalar value for p representing the number\nof poison points that are selected by the RL algorithm at\neach episode, we begin. After each poison point, decided\nby the RL algorithm, we perform retraining of θ on\nthe perturbed training set, generating θp, and applying\nit to the testing set. The retraining stage is conducted by\ntransfer learning: we take the original set of parameters\nand warm-start on them, and then retrain the network for\na single additional epoch.\nThe end-to-end system is designed to take as input\na full dataset composed of graphs and have an agent\nlearn to manipulate those graphs, using available actions\nin A, such that they maximize their episodic reward.\nThe reward is measured as the difference between the\nbaseline accuracy (the GNN’s accuracy on the testing set\nbefore perturbation on the training set) and the perturbed\naccuracy (the GNN’s accuracy on the testing set after the\ntraining set has been perturbed).\nV. EXPERIMENTS\nA. MiniGCDataset\nWe ﬁrst create partitioned sets from the MiniGC-\nDataset, with the size of the training set = 150, and the\ntesting set = 30. We run the original GNN for 70 epochs\nto generate θ and our benchmark accuracy on the testing\nset. During the Poison attack, we allow the RL algorithm\nto perturb p = 10 poison points during each episode.\nThe reward is passed back intra-episode, between poison\npoint selection, to reduce vanishing gradient issues and to\nencourage learning. This process is conducted 175 times\nas we ﬁx NEpisodes = 175. We also ﬁx NRuns = 10.\nThe Graph Neural Network architecture is deﬁned in\nSection IV-B. The RL algorithm procedure can be found\nin Algorithm ??, with the speciﬁc architecture released\nin our source code, here. Again, we use π(s) =\n1\n|A| as\na baseline to measure RL’s efﬁcacy at this speciﬁc task.\nFigure 3 shows reward over the episode, averaged over\nNRuns to reduce stochasticity.\nFigure 3. Simulation Results on MiniGCDataset: —Train— = 180,\n—Test— = 30, GNN epochs = 70, RL epochs = 175. The blue line\nrepresents the accuracy degradation over a random policy, averaged\nout over 10 runs. The orange line is the accuracy degradation over\nREINFORCE, averaged out over NRuns = 10\nWe see a modest improvement over random search,\nas well as lesser extremes in the incorrect direction,\nmeaning we rarely see cases where our RL agent perturbs\na set of graphs during an episode and increases the\ntesting accuracy.\nWe also create a way to visualize the efﬁcacy of\nindividual graphs perturbations in Figure 4. There, the\nx-axis represents the action id, i.e., the graph id within\nthe training set. The y-axis is the linear regression\ncoefﬁcient found when mapping the summed episodic\nactions counts to the mean episodic reward over Nruns.\nWe see that star graphs appear most often amongst the\ntop ten high valued coefﬁcients and perceive that they\nare the most likely class within this dataset to have an\nattack result in net positive reward on the system.\nFigure 4.\nLinear Regression: f: summed episodic actions counts -\n¿ mean episodic reward. Visualizing the coefﬁcients to show which\naction selections have a higher impact on accuracy degradation over\nthe testing set. the x-axis represents the graph id of the graph being\nperturbed, and the y-axis is the linear regression coefﬁcient. *The\nlinear regression model was ﬁt with an r2 of 0.85.\nB. MiniGC - Larger\nNext, we increase the size of the graphs by an order\nof magnitude. All other parameters are kept the same,\nincluding the size of the subgraph inserted into the\noriginal graph. In the GNN initial training stage, we\nsee that the size of the graphs has a profound effect\non test-time accuracy, meaning higher baselines for our\nRL algorithms. We visualize these results in Figure 5,\nnoting that the distribution plots of episodic reward\ncomparing REINFORCE and random selection show that\nREINFORCE has a laterally shifted distribution and a\nhigher peak. What we do notice is that central tendency\nmeasures are dramatically shifted downward, meaning\nthat the increase in network sizes allowed for the GNN\nto draw more clear decision boundaries and consequently\nincrease its own robustness.\nVI. CONCLUSION\nIn conclusion, we present a novel end-to-end poi-\nsoning attack on a GNN Classiﬁer. The results from\nour analysis show that our method presents a lift over\na random, brute force search of the graph space, and\nprovides additional insight into the vulnerabilities of\nspeciﬁc graph structures, e.g., which graph classes have\nFigure 5. Simulation Results on MiniGCDataset: —Train— = 180,\n—Test— = 30, GNN epochs = 70, RL epochs = 175. We increase\nthe size of the networks by an order of magnitude, such that\nMinNodes = 1500 and MaxNodes = 2000. These are normalized\ndensity plots. The x-axis represents the average episodic reward over\neach policy, while the y-axis is the density of observing that statistic.\nBlue corresponds to a random policy, while orange is the utilized\nREINFORCE algorithm.\nFigure 6.\nWe show the results on MiniGCDataset - Larger here,\nsumming over the coefﬁcients grouped by each graph class label.\nWith the enlarged graph sizes, we see that cycle, wheel, grid and\ncomplete graphs have net positive coefﬁcient values, which signiﬁes\nrelationships between the decision boundaries being draw by the\nGNN, as the overall structure of these four graph types makes it\ndifﬁcult for the model to distinguish them post-attack.\na higher propensity to affect test-time performance given\na perturbation. We also experiment with various ways of\nrepresenting the state of our graph dynamic system. Such\nresearch could be important in practice, particularly in\nthe healthcare industry, and has unexplored theoretical\nties to the robustness and expressive power of Graph\nDeep Learning.\nVII. FUTURE WORK\n1) State Representation: We intend to explore further\nmethods for encoding the state of the graphical system\nat t. Here, we use a ﬂattened representation of summary\nstatistics over each graph in the training set, but we\nbelieve that there may be other descriptive methods from\nthe area of network science, such as centrality measures,\nthat may better represent a graph’s characteristics in a\nlow-dimensional space.\n2) Action Space: One of the main things that we\nintend to explore in the future is the action space.\nWe reduced the action space from containing granular\nlevel actions, such as node or edge edits, but believe\nthat translating this to a Hierarchical Reinforcement\nLearning problem opens the door for us to operate on\na larger action space without sacriﬁcing much in the\nway of compute. In this way, we could subject the RL\nalgorithm to a further customized reward function that\nrewards more heavily when they choose to alter the\ngraphs in a small way, such that perturbations would be\nobfuscated from a human detector. Initially, our action\nspace resembled Table II.\nAction\nName\nDeﬁnition\na1\nsubgraph add\nA random gnp graph is inserted\ninto the existing graph struc-\nture.\na2\nnode delete\na random node with g is re-\nmoved\na3\nnode add\na node is added to g\na4\nedge delete\na random edge is deleted from\ng\na5\nedge add\na random edge is added to g\nTable II\nA HYPOTHESIZED ACTION SPACE EXPANSION.\n3) Baselines: The bulk of our work was completely\nexperimental, and as such, there lack academically-\ngrounded baselines as to which algorithm would perform\nbetter, or what that would mean for our results. In the\nfuture, we would like to add to our work an increasing\nvariety of ’ﬂat’ RL methods to validate our choice, or\nto ﬁnd a new winner. We would also like to explore the\nuse of pretrained models, where applicable.\n4) Datasets: Most of our experimental process was\ncentered around using the MiniGCDataset from Deep-\nGraphLibrary. Other datasets available feature vastly\ndifferent APIs, and require more work to integrate into\nour end to end attack. For future work, we intend to\nexpand the testing suite to include real-world datasets\ne.g PROTEINS (with known consequences faced by a\nsuccessful attack), and to make our overall pipeline more\nextensible.\n5) Compute: All work noted above was completed on\nlocal machines. Some bottlenecks were seen, particularly\nin the GNN training/retraining stage that could beneﬁt\nfrom parallel/distributed computing across nodes. For\nreal-world datasets, with thousands to millions of nodes\nand edges, this is a worthwhile endeavor. For experi-\nmental testing on a synthetic dataset, there was not an\nexplicit need for such support.\n6) Hierarchical Reinforcement Learning (HRL):\nWe propose an algorithmic conversion on the RL side of\nour Poison attack, trading a ﬂat method for a hierarchical\none. HRL splits an RL problem into a hierarchy of\nsubproblems in a way that higher-level problems use\nlower-level problems as subroutine [29]. It is similar\nto the optimal subproblem property found in dynamic\nprogramming problems or possibly similar to divide\nand conquer methods; and similar to those, each of the\nsubproblems can be reinforcement learning problems on\ntheir own. Usually, the lower-level problems only require\na short sequence of actions. We can use parallel pro-\ncessing if the subproblems are non-overlapping, which\nmeans that HRL can reduce computational complexity,\nand allow for the agent to have granular control over\nthe graph perturbation process extending beyond our\nwork here. But parallel learning is hard since changes\nin a policy at one level may cause changes in higher\nlevels. Hierarchical Actor-Critic (HAC) [30] claims to\novercome this. In HAC each level of the hierarchy is\ntrained independently of the lower levels by treating as\nif the lower level policies are already optimal. HAC has\nbeen shown to learn 3-level hierarchies in continuous\nstate and action spaces in parallel, and is our algorithm\nof choice as we move toward a hierarchical approach.\nREFERENCES\n[1] W. Jin, Y. Li, H. Xu, Y. Wang, and J. Tang, “Adversarial attacks\nand defenses on graphs: A review and empirical study,” arXiv\npreprint arXiv:2003.00653, 2020.\n[2] H. Xu, Y. Ma, H. Liu, D. Deb, H. Liu, J. Tang, and A. Jain,\n“Adversarial attacks and defenses in images, graphs and text: A\nreview,” International Journal of Automation and Computing,\nvol. 17, pp. 151–178, 2020.\n[3] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative\nadversarial networks,” 2014.\n[4] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and\nharnessing adversarial examples,” 2015.\n[5] K. Ren, T. Zheng, Z. Qin, and X. Liu, “Adversarial attacks\nand defenses in deep learning,” Engineering, vol. 6, no. 3, p.\n346–360, 2020.\n[6] B. Knyazev, X. Lin, M. R. Amer, and G. W. Taylor,\n“Spectral multigraph networks for discovering and fusing\nrelationships\nin\nmolecules,”\narXiv:1811.09595\n[cs,\nstat],\nNov 2018, arXiv: 1811.09595. [Online]. Available: http:\n//arxiv.org/abs/1811.09595\n[7] R. Ying, J. You, C. Morris, X. Ren, W. L. Hamilton,\nand J. Leskovec, “Hierarchical graph representation learning\nwith\ndifferentiable\npooling,”\narXiv:1806.08804\n[cs,\nstat],\nFeb\n2019,\narXiv:\n1806.08804.\n[Online].\nAvailable:\nhttp:\n//arxiv.org/abs/1806.08804\n[8] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart,\n“Stealing machine learning models via prediction apis,” in 25th\n{USENIX} Security Symposium ({USENIX} Security 16), 2016,\npp. 601–618.\n[9] W. E. Zhang, Q. Z. Sheng, A. Alhazmi, and C. Li, “Adversarial\nattacks on deep-learning models in natural language processing:\nA survey,” ACM Trans. Intell. Syst. Technol., vol. 11, no. 3,\nApr. 2020. [Online]. Available: https://doi.org/10.1145/3374217\n[10] R. S. Sutton and A. G. Barto, Reinforcement Learning:\nAn Introduction, 2nd ed.\nThe MIT Press, 2018. [Online].\nAvailable: http://incompleteideas.net/book/the-book-2nd.html\n[11] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li,\nand M. Sun, “Graph neural networks: A review of methods and\napplications,” arXiv preprint arXiv:1812.08434, 2018.\n[12] T.\nN.\nKipf,\n“Graph\nconvolutional\nnetworks.”\n[Online].\nAvailable: https://tkipf.github.io/graph-convolutional-networks/\n[13] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online\nlearning of social representations,” in Proceedings of the 20th\nACM SIGKDD international conference on Knowledge discov-\nery and data mining, 2014, pp. 701–710.\n[14] A. J.-P. Tixier, G. Nikolentzos, P. Meladianos, and M. Vazir-\ngiannis, “Graph classiﬁcation with 2d convolutional neural\nnetworks,” in International Conference on Artiﬁcial Neural\nNetworks.\nSpringer, 2019, pp. 578–593.\n[15] T. N. Kipf and M. Welling, “Semi-supervised classiﬁca-\ntion\nwith\ngraph\nconvolutional\nnetworks,”\narXiv\npreprint\narXiv:1609.02907, 2016.\n[16] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu,\nand L. Song, “Adversarial attack on graph structured data,”\narXiv:1806.02371 [cs, stat], Jun 2018, arXiv: 1806.02371.\n[Online]. Available: http://arxiv.org/abs/1806.02371\n[17] Y. Ma, S. Wang, T. Derr, L. Wu, and J. Tang, “Attacking\ngraph convolutional networks via rewiring,” arXiv:1906.03750\n[cs, stat], Sep 2019, arXiv: 1906.03750. [Online]. Available:\nhttp://arxiv.org/abs/1906.03750\n[18] Z. Zhang, J. Jia, B. Wang, and N. Z. Gong, “Backdoor\nattacks to graph neural networks,” arXiv:2006.11165 [cs],\nJun\n2020,\narXiv:\n2006.11165.\n[Online].\nAvailable:\nhttp:\n//arxiv.org/abs/2006.11165\n[19] N. Papernot, P. McDaniel, A. Swami, and R. Harang, “Crafting\nadversarial input sequences for recurrent neural networks,”\n2016.\n[20] N. Akhtar and A. Mian, “Threat of adversarial attacks on\ndeep learning in computer vision: A survey,” arXiv:1801.00553\n[cs],\nFeb\n2018,\narXiv:\n1801.00553.\n[Online].\nAvailable:\nhttp://arxiv.org/abs/1801.00553\n[21] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, and\nM. Sun, “Tactics of adversarial attack on deep reinforcement\nlearning agents,” 2019.\n[22] X. Xu, Y. Yu, B. Li, L. Song, C. Liu, and C. Gunter,\n“Characterizing malicious edges targeting on graph neural\nnetworks,” 2019. [Online]. Available: https://openreview.net/\nforum?id=HJxdAoCcYX\n[23] D.\nZhu,\nZ.\nZhang,\nP.\nCui,\nand\nW.\nZhu,\n“Robust\ngraph convolutional networks against adversarial attacks,”\nin Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery &amp; Data Mining,\nser.\nKDD\n’19.\nNew\nYork,\nNY,\nUSA:\nAssociation\nfor Computing Machinery, 2019, p. 1399–1407. [Online].\nAvailable: https://doi.org/10.1145/3292500.3330851\n[24] J. B. Lee, R. Rossi, and X. Kong, “Graph classiﬁcation using\nstructural attention,” in Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data\nMining.\nACM, Jul 2018, p. 1666–1674. [Online]. Available:\nhttps://dl.acm.org/doi/10.1145/3219819.3219980\n[25] I. Takigawa and H. Mamitsuka, “Graph mining: procedure,\napplication to drug discovery and recent advances,” Drug Dis-\ncovery Today, vol. 18, no. 1, p. 50–57, 2013.\n[26] K.\nXu,\nW.\nHu,\nJ.\nLeskovec,\nand\nS.\nJegelka,\n“How\npowerful are graph neural networks?” arXiv:1810.00826 [cs,\nstat], Feb 2019, arXiv: 1810.00826. [Online]. Available:\nhttp://arxiv.org/abs/1810.00826\n[27] M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou,\nC. Ma, L. Yu, Y. Gai, T. Xiao, T. He, G. Karypis, J. Li,\nand Z. Zhang, “Deep graph library: A graph-centric, highly-\nperformant package for graph neural networks,” arXiv preprint\narXiv:1909.01315, 2019.\n[28] R. J. Williams, “Simple statistical gradient-following algorithms\nfor connectionist reinforcement learning,” Machine Learning,\nvol. 8, no. 3, p. 229–256, May 1992.\n[29] B. Hengst, Hierarchical Reinforcement Learning.\nBoston,\nMA: Springer US, 2010, pp. 495–502. [Online]. Available:\nhttps://doi.org/10.1007/978-0-387-30164-8 363\n[30] A. Levy, G. Konidaris, R. Platt, and K. Saenko, “Learn-\ning multi-level hierarchies with hindsight,” arXiv preprint\narXiv:1712.00948, 2017.\nAlgorithm 1: Poison attack(G, π, T, p)\nX, Y = G ;\n// Extract graph label\npairs\nf = GNN(X, Y ) ; // Train Model on X,\nY training set\nt = 0\nwhile t ̸= T do\n//Outer loop-begin episode\nt = t + 1\npoison = 0\nX′ = X;\nf′ = f ;\n// Clone graphs\nand model\ns = state representations of X\nepisode reward = 0\nacc prev = validation accuracy of f\nwhile poison ̸= p do\n//Inner Loop\npoison = poison + 1\nChoose action a by policy\nX′ = a ∗X ;\n// Perturb graphs\nwith a\ns′ = state representations of X′\nf′ = f′ retrained one epoch on X′\nacc after = validation accuracy of X′\nreward = acc prev - acc after\nepisode reward = episode reward +\nreward\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CR"
  ],
  "published": "2021-02-12",
  "updated": "2021-02-12"
}