{
  "id": "http://arxiv.org/abs/2102.06991v2",
  "title": "The first large scale collection of diverse Hausa language datasets",
  "authors": [
    "Isa Inuwa-Dutse"
  ],
  "abstract": "Hausa language belongs to the Afroasiatic phylum, and with more\nfirst-language speakers than any other sub-Saharan African language. With a\nmajority of its speakers residing in the Northern and Southern areas of Nigeria\nand the Republic of Niger, respectively, it is estimated that over 100 million\npeople speak the language. Hence, making it one of the most spoken Chadic\nlanguage. While Hausa is considered well-studied and documented language among\nthe sub-Saharan African languages, it is viewed as a low resource language from\nthe perspective of natural language processing (NLP) due to limited resources\nto utilise in NLP-related tasks. This is common to most languages in Africa;\nthus, it is crucial to enrich such languages with resources that will support\nand speed the pace of conducting various downstream tasks to meet the demand of\nthe modern society. While there exist useful datasets, notably from news sites\nand religious texts, more diversity is needed in the corpus.\n  We provide an expansive collection of curated datasets consisting of both\nformal and informal forms of the language from refutable websites and online\nsocial media networks, respectively. The collection is large and more diverse\nthan the existing corpora by providing the first and largest set of Hausa\nsocial media data posts to capture the peculiarities in the language. The\ncollection also consists of a parallel dataset, which can be used for tasks\nsuch as machine translation with applications in areas such as the detection of\nspurious or inciteful online content. We describe the curation process -- from\nthe collection, preprocessing and how to obtain the data -- and proffer some\nresearch problems that could be addressed using the data.",
  "text": "The ﬁrst large scale collection of diverse Hausa language datasets\nIsa Inuwa-Dutse\nSchool of Computer Science\nUniversity of St Andrews, UK\niid1@st-andrews.ac.uk\nAbstract\nHausa language belongs to the Afroasiatic phylum, and with more ﬁrst-language speakers than\nany other sub-Saharan African language. With a majority of its speakers residing in the North-\nern and Southern areas of Nigeria and the Republic of Niger, respectively, it is estimated that\nover 100 million people speak the language. Hence, making it one of the most spoken Chadic\nlanguage. While Hausa is considered well-studied and documented language among the sub-\nSaharan African languages, it is viewed as a low resource language from the perspective of\nnatural language processing (NLP) due to limited resources to utilise in NLP-related tasks. This\nis common to most languages in Africa; thus, it is crucial to enrich such languages with resources\nthat will support and speed the pace of conducting various downstream tasks to meet the demand\nof the modern society. While there exist useful datasets, notably from news sites and religious\ntexts, more diversity is needed in the corpus.\nWe provide an expansive collection of curated datasets consisting of both formal and informal\nforms of the language from refutable websites and online social media networks, respectively.\nThe collection is large and more diverse than the existing corpora by providing the ﬁrst and\nlargest set of Hausa social media data posts to capture the peculiarities in the language. The\ncollection also consists of a parallel dataset, which can be used for tasks such as machine trans-\nlation with applications in areas such as the detection of spurious or inciteful online content. We\ndescribe the curation process – from the collection, preprocessing and how to obtain the data –\nand proﬀer some research problems that could be addressed using the data.\nKeywords: Hausa Language, African Language, Hausa Dataset, Low Resource Language,\nNatural Language Processing\n1. Introduction\nHausa language is one of the most widely spoken Chadic language, which is part of the\nAfroasiatic phylum. With its majority speakers residing in the Northern and Southern areas\nof Nigeria and the Republic of Niger, respectively, it is estimated that over 100 million people\nspeak the language. Many countries in West Africa and sub-Saharan countries adopt Hausa as the\nlingua franca, hence making it one of the dominant Chadic language spoken across the continent\n[1]. Major Hausa regions in Nigeria include Kano, Sokoto, Jigawa, Zaria, and Katsina. Figure 1\nshows a visual depiction of the phylogenetic evolution of Hausa language. The ﬁgure shows\nthe linguistic family and orthographic origin of the language. Hausa language relies on Arabic\nand Latin alphabets (both consonants and vowels). Ajami is the Arabic version of written Hausa\nPreprint submitted to Elsevier\nFebruary 18, 2021\narXiv:2102.06991v2  [cs.CL]  16 Feb 2021\nFigure 1: Linguistic family and orthographic origin of Hausa language. The ajami is a form of written Hausa text that is\nbased on Arabic alphabets and the boko script is based on Latin alphabets.\nthat has been widely in existence in pre-colonisation Hausa land, and the Boko script is based on\nLatin alphabet, which has been popularised since the down of colonisation in Hausa land.\nMotivation. In the quest to make computers more amenable to natural language through use-\nful computational tools, relevant datasets are being collected to support various downstream\ntasks. As noted earlier, Hausa is considered a well-studied and documented language among the\nsub-Saharan African languages [1], but a low resource language from NLP perspective due to\nlimited resources to undertake NLP-related tasks. Because this is common to most languages\nin Africa [2], it is crucial to enrich such languages with resources that will support and speed\nthe pace of conducting various downstream tasks to meet the demand of the modern society.\nAlthough Google LLC1 has access to tones of diverse data, anecdotal evidence suggests that its\ntranscription system embedded in the video-communication (Meet2) performs poorly in recog-\nnising Hausa speech.\nFor a basic assessment of the eﬃcacy of Google’s translation engine3 on Hausa language,\nFigures 2 and 3 show some examples of translated Hausa texts using the translation system. The\ndiscrepancies in the ﬁgures (2 and 3) can be attributed to the lack of suﬃcient and appropriate\ndata to train relevant language models. Some of the informal or day to day terms used in the lan-\nguage can only be obtained via online social media, not news websites or religious texts. Among\nthe implications of the incorrect translation is misinterpretation or out of context interpretation\nof events. One of the reasons of the above challenges is the lack of diverse datasets from vari-\nous sources in larger quantities that will enable the development of relevant resources and tools\nwith a wide range of applications in uncovering meanings in data. The following constitutes the\nmotivations for undertaking this study:\n- the existing NLP resources for downstream tasks in Hausa language relies mostly on for-\nmal text collections notably from news sites, such as BBC Hausa Service4 and VOA Hausa\n1https://www.google.com/\n2https://meet.google.com/\n3See https://translate.google.co.uk/\n4https://www.bbc.com/hausa/\n2\nFigure 2: Some examples of translated Hausa texts using Google’s translation engine. The texts in panels A, B, C and D\nhave been incorrectly translated. The use of an exclamation mark (!), motsin rai in Hausa, in panel D drastically changed\nthe message.\nFigure 3: More examples of Hausa texts that have been incorrectly translated. The texts, mostly proverbial, in panels A,\nB, C and D are examples of common conversational setting.\nService5. Others include parallel datasets from religious texts such as [3] and Tanzil6 for\ntasks such as automatic speech recognition, named entity recognition (NER) and part of\nspeech tagging. Thus, the existing Hausa corpora suﬀer from limited diversity and avail-\nability of curated ground-truth data for large scale downstream tasks.\n- no known existing collection of curated Hausa datasets from social media platforms, hence,\nthe need for diverse datasets to support various NLP-related tasks.\n- Other languages have received substantial contributions when compared to the most domi-\nnant Chadic language (Hausa). For instance, the HLT for African Languages [4] is focused\non notably Yoruba and Igbo.\nThis paper aims at addressing the aforementioned gap by making available relevant datasets that\nwill facilitate downstream tasks.\n5https://www.voahausa.com/\n6http://tanzil.net/docs/tanzil_project\n3\nContributions. The main focus of this study is to contribute diverse datasets to support NLP\nresearch that relies on Hausa language. Noting the above issues, the study oﬀers the following\ncontributions:\n- to enrich the language with useful resources that will facilitate downstream tasks, we pro-\nvide an expansive collection of curated datasets7 consisting of both formal and informal\nforms of the language from refutable websites and online social media networks, respec-\ntively. Based on the collection sources, the corpus consists of two broad categories: Social\nStream Category (Section 3.1) and Website and Blog Category (Section 3.2). The Social\nStream category consists of Twitter data from users interacting in Hausa language, and\nthe Website and Blog category is obtained through the scrapping of relevant websites and\nblogs. The collection also consists of parallel data, which can be used for natural language\nprocessing and machine translation tasks with a wide area of applications.\n- because the data is from various sources – news and government web sites, blog posts and\nsocial media platforms – the collection spans many aspects of social life. Moreover, the\ninclusion of social media data will help in capturing the peculiarities in the language and\nhow colloquial expressions manifest in the language.\n- we make available additional parallel dataset to support downstream tasks such as machine\ntranslation and automatic speech recognition systems.\n- we present a process to streamline the process of obtaining more diverse datasets in Hausa\nfrom social media platforms. We describe how to retrieve the hydrated version of the data\nand proﬀer some research problems that could be addressed using the data.\nThe remaining of this paper is structured as follows. Section 2 presents a summary of rele-\nvant studies and Section 3 presents a detailed description of the data collection and processing.\nSection 4 concludes the study and discusses some future work.\n2. Related Work\nAs the major spoken language among the sub-Saharan African language, Hausa language is\nestimated to have over 100 million speakers. Geographically, the major dialects of the language\ninclude Eastern Hausa (e.g. Kano), Western Hausa (e.g. Sokoto) and the Aderanci dialects from\nNiger. Hausa is considered a well-studied and documented language among the sub-Saharan\nAfrican languages [1], however, from NLP perspective, it is viewed as a low resource language\ndue to limited resources for computational linguistic. This is common to most languages in\nAfrica. There is a growing body of studies exploring various aspects of the language. Of interest\nto this study is the prevailing eﬀorts to enrich the language along the following dimensions.\nExisting Corpus. From a computational linguistic point of view, Hausa is categorised as low-\nresource language owing to the limited resources to computationally analyse the language and\nsupport downstream tasks. Other languages have received substantial contribution when com-\npared to the most dominant Chadic language (Hausa) widely spoken in Africa. Generally, there\n7See https://github.com/ijdutse/hausa-corpus for details.\n4\nare limited linguistic resources to support NLP tasks involving African languages. The com-\nmonly used parallel datasets for NLP tasks, such as Machine Translation, include (1) Tanzil\ndataset8, which is a translation of the Holy Qur’an in various languages including Hausa. (2)\nJW300 dataset [3] consisting of multilingual parallel sentences, and is used in the Masakhane\nProject [2] as the default dataset for open implementation. So far, the Masakhane Project9 is one\nof the biggest collaborative attempts to support African language developments through the de-\nvelopment and support for open-source linguistic resources and tools for African languages [2].\nThe work of [5] noted the lack of adequate parallel and diverse dataset for language translation.\nThis led the authors to embark on a project aimed at improving machine translation capability for\nlow-resource language. The data consists of Hausa–English parallel corpus, which is useful for\ntranslation and as evaluation benchmark for English–Hausa Neural Machine Translation (NMT).\nLanguage Models. The work of [6] contributes a Hausa-based words embedding model that is\ntrained on formalised texts from news web sites and religious texts to support downstream tasks.\nNoting the inadequacy of models trained using high-resource languages (such as English) to be\ntransferred for training low resource languages (such as Hausa), the work of [7] is based on\nmultilingual transformer models to improve accuracy and eﬀectiveness of low resource language\nmodels. The use of more diverse datasets from various sources such as social networks will help\nin capturing the peculiarities in the language, thus improving the language models.\nDownstream tasks in low resource languages. There are relevant parallel datasets in place to\nsupport downstream tasks such as machine translation and automatic speech recognition (ASR)\nsystems. Noting how the limited resources available to facilitate Human Language Technology\n(HLT), the work of [4] is focused on speech technology for some dominant indigenous languages\nin Nigeria [4]. Similarly, the work of [8] presents a database of text and speech, transcribed\nspeech resources, in Hausa language. The speech collection is based on the GlobalPhone project\n([9]), and the textual part collected from Hausa-based websites, mostly from news sites. Using\nthe collected datasets, the authors develop an ASR system to improve pronunciation and acoustic\nmodelling in Hausa language.\nTaking the aforementioned resources into account, the diversity of the datasets utilised can be\nimproved by incorporating various sources that capture more dialects and social media data texts.\nThus, the datasets in this study can be used for developing and improving relevant resources for\ndownstream tasks.\n3. Datasets\nIn the quest to make computers more amenable to natural language through useful computa-\ntional tools, numerous collection of diverse datasets are being collected. Based on the collection\nsources, the corpus is categorised into the following: Social Stream and Website and Blog cat-\negories. Accordingly, the collection consists of two sub-categories of Twitter data, from users\ninteracting in Hausa language, and two sub-categories of web scraping data from various Hausa-\nbased language sites and historic online ﬁles. The collection also consists of parallel data, which\ncan be used for downstream tasks such as machine translation. Figure 410 shows a summary of\nthe data collection pipeline.\n8http://tanzil.net/docs/tanzil_project\n9https://www.masakhane.io/\n10the ﬁgure is ﬁrst used in [10]\n5\nFigure 4: A summary of the data collection pipeline.\n3.1. Social Stream Category\nThe proliferation of various online information on social media platforms such as Twitter and\nFacebook makes it possible to harvest relevant posts. With the evolving nature of communication\nin which majority of the populace obtain useful information from the web and social media\nplatforms, it is pertinent to retrieve as much as possible relevant online datasets on low resource\nlanguages. In view of this, the Social Stream collection contains tweet objects11 retrieved from\nTwitter using the platform’s application programming interface (API). The collection activity\nstarts on 11/12/20 to date (13/02/21) and consists of the following sub-categories.\n- account-based collection: this relies on speciﬁc usernames, mostly of news channels, to\ncollect data posted in Hausa language. Because of the presence of news channels, the\ncategory usually consists of formal posts on news items that attract other users to engage\nthrough comment, reply, retweet or like. This allows for the retrieval of more data on\nusers’ engagements using slang, memes and other colloquial or informal posting styles in\nthe local language.\n- miscellaneous collection: the miscellaneous set mostly consists of users who engage or\ninteract with the content posted by users in the account-based collection. For a more di-\nverse dataset, the miscellaneous set also consists of post collected using relevant hashtags\nand keywords (see Table 1). The use of hashtags and keywords oﬀers a generic collection\nof daily tweets spanning numerous topics posted in Hausa language.\nWe deﬁne main tweet, as a post or tweet that attracts engagements by other users via replies. For\nannotation and sharing purpose, each main tweet (preﬁxed with MT) is followed by its corre-\nsponding reply, preﬁxed with MTR. Under this setting, the main tweet is not unique but the reply\nis.\nGetting the hydrated tweets. The full version of the social stream category cannot be made\navailable due to Twitter’s policy on content redistribution12. Instead, we provide relevant IDs\n11a tweet object is a complex data object composed of numerous descriptive ﬁelds, which enables the extraction of\nvarious features related to a post\n12see https://developer.twitter.com/en/developer-terms/agreement-and-policy\n6\nTable 1: Summary of data collection sources and corresponding descriptions\nSocial Stream\naccount-based collection from Twitter\nDescription\ntweets from speciﬁc account handles collected via Twitter’s API\nHandles\n@bbchausa, @voahausa, @HausaDw, @RFI Ha, @freedomradionig, @aminiya-\ntrust, @HausaRadio, @hausaonline, @rariyajarida, @CRIHausaRadio, @TRTHausa,\n@HausaTranslator, @HausaDictionary, @PTimesHausa,\nSocial Stream\nmiscellaneous collection from Twitter\nDescription\ntweets collected from Twitter using diverse keywords\nKeywords\nHausa language, Yaran Hausa, Dan Hausa, Hausa day, Hausa week, barkan mu da rana,\nsannu da zuwa, lokaci, gobe da rana, sanyi, yan siyasa, majalisa, shugabanni, labarai,\n#HausaDay, yan sanda, sojoji, wasan kwallon kafa, #hausaday\nWebsite & Blog\nMajor websites for non-tweet-based collection\nDescription\nmore formal texts from major news channels and a chronicle of posts\nSome urls\nhttps://www.bbc.com/hausa,https://www.voahausa.com/,https:\n//freedomradionig.com/,http://indigenousblogs.com/ha/,https:\n//www.rumbunilimi.com.ng/,http://fagenmarubuta.blogspot.com/\nof the tweet-object to be used in retrieving the complete data, including the meta-information\nabout each tweet13. Moreover, we include a short Jupyter notebook with a description of how\nto retrieve the data, especially the hydrated data, and render it usable. Alternatively, interested\nusers may use the hydrator package [11].\n3.2. Website and Blog Category\nThis category comprises data from Hausa-based news sources, blog posts and from historic\nﬁles from various sources. Using a custom web scrapping14 crawler, we scrap useful data from\nthe relevant websites; see Table 1 for some of the visited sites and Appendix 4 for more details.\nBecause the content in this category mostly consists of well-written texts, it is possible to examine\nand compare the formal written form of the language with its informal counterpart from social\nmedia platforms.\n3.3. Parallel Data\nAs a ﬁrst step in creating a parallel corpus (in English) for the corpus (in Hausa), we leverage\nGoogle’s translation API, Googletrans15, to eﬃciently create a parallel English corpus for the\ncollected Hausa corpus (using the Social Stream category). We sample from the Social Stream\ncategory for the translation, and subset of the translated segments or sentences is manually val-\nidated for correctness. Similar to the Social Stream category, the parallel collection consists of\nHausa main tweet (HMT), Hausa main tweet reply (HMTR), English main tweet (EMT) for the\ntranslated HMT, and English main tweet reply (EMTR) for the translated HMTR.\n3.4. Preprocessing and meta-analysis\nDue to the multifaceted nature of the datasets, we applied the following transformation so\nthat all the collections conform to a uniﬁed format for redistribution. This is to improve usability\nand suitability for sharing and further analysis.\n13The data presented in this paper are available at https://github.com/ijdutse/hausa-corpus\n14using the BeautifulSoup package, available at www.crummy.com/software/BeautifulSoup\n15https://translate.google.co.uk/\n7\nTable 2: A summary of basic statistics in the collected datasets. #DS and #TS denote dataset size and tokens size the\ncollection, respectively; µsent. refers to the average sentence size in each data category\nCategory\n#DS\n#TS\nµsentence size\nDescription\nDatasets\nMain Tweets\n3389\n34,035\n10\nTweets from social stream\nReply Tweets\n8649\n61,441\n7\nTweets from social stream\nNews and Blogposts\n29,371\n932,523\n32\nData collection from websites and blogs\nWeb Documents\n12,277\n10507117\n856\nData collection from websites and blogs\nParallel Main Tweets\n18\n3671\n13\nParallel data from social stream\nParallel Reply Tweets\n292\n3671\n7\nParallel data from social stream\nFigure 5: Some common terms in the tweets collection.\nPreprocessing. Noting how messy the Social Stream category could be and diﬃcult to use di-\nrectly without cleaning [12], we applied the following rudimentary cleaning process involving\ntokenisation, stopwords removal and text formatting. For normalisation, we remove urls, user\nmentions, emojis and non-Hausa posts from the collection. Both the raw and the cleaned version\nof the applicable datasets are made available because some of the raw tweets or replies contain\nonly user mention or emojis, which we designate as stopwords in the cleaned version of the\ndata. After the removal of spaces and special symbols in the Website and Blog category, each\nparagraph or chunk of texts is split into smaller unit punctuated by a full stop.\nMeta-analysis. Because the datasets in the corpus have been collected over time, we conducted\nlongitudinal and exploratory analyses for better understanding. Table 2 shows basic statistics\nabout the collected data, including the number of unique main tweets, replies, mean token size,\nand the average length of sentences in each category. For the Social Stream category, we de-\ntermine the most common terms through a high-level visualisation of samples from the tweets\ncollection. Accordingly, we provide a summary of the relevant themes in Figure 5 using word\ncloud16.\n3.5. Utility of the datasets\nBecause this is the ﬁrst large scale diverse collection of datasets in Hausa language, many\nrelevant studies can be conducted using the corpus to complement existing resources. Thus, we\nidentify relevant areas and research problems that could harness the curated datasets.\n- detection of online spurious content: the increasing number of uncensored posts is pos-\ning many challenges to online socialisation, especially the problem of identifying gen-\nuine information in an ecosystem cluttered with spurious content. Despite the prevention\n16the word cloud visualisation is based on the implementation at: https://github.com/amueller/word_cloud\n8\nmeasures taken by social media platforms, many sources of misleading information and\nrumours still exist, especially in low-resource languages such as Hausa. The collected cor-\npus, especially the Social Stream category, could be used to develop a prediction system\nto ascertain content veracity.\n- propagation of fake news: noting the prevalence of online fake news, it will be crucial\nto understand how such fake news and unfounded claims propagate in low resource lan-\nguages, especially since the existing methods are based on high resource languages. The\ncontributed datasets will be useful in investigating the problem.\n- low resource language model: the diverse nature of the corpus will improve the train-\ning of useful state-of-the-art language models, such as BERT [13]. While variants of such\nmodels have been used for multilingual setting, the data collection used for training is shal-\nlow and lacks diversity. Among other applications, the corpus can be used for downstream\ntasks such as machine translation, automatic speech recognition system, and sentiment\nanalysis in the low resource language.\nOverall, the contribution of this study will signiﬁcantly help in reducing the amount of time or\neﬀort in building tool-kits for downstream tasks in the low resource language.\n4. Conclusion\nHausa language is one of the dominant Chadic languages that is widely spoken in Africa.\nFrom the computational linguistic point of view, Hausa, among other African languages, is cat-\negorised as low-resource language owing to the limited resources to handle various NLP-related\ntasks. This limitation slows down the pace of development of the language in terms of computa-\ntional linguistic and for downstream tasks. In line with the existing studies, the study contributes\na diverse Hausa corpus that will signiﬁcantly help in reducing the amount of time in building\ntool-kits for various NLP downstream tasks in the language. We contributed the ﬁrst compre-\nhensive collection of curated datasets to support various NLP-related tasks in Hausa language.\nEssentially, the corpus consists of two categories according to collection sources: online social\nnetwork and websites and blogs. Due to the collection sources, the themes in the datasets span\nmany aspects of social life. Moreover, the inclusion of social media data will help in capturing\nthe peculiarities in the language and how colloquial expressions manifest in the language. We\nprovide a basic but useful analysis of the datasets and recommend some relevant problems that\ncould leverage the corpus. We also presented a process to streamline the activity of obtaining\nmore diverse datasets in Hausa from social media platforms, and how to retrieve the hydrated\nversion of the data.\nFuture work. It is pertinent to have a comprehensive collection and streamline the process to\nharness in enriching the language with the relevant resources. For future study, we hope this\nwill attract research curiosity to further explore the language in-depth for supporting downstream\ntasks. Future focus will be on the development of a language model using the contributed datasets\nand other existing ones. The diverse nature of the corpus will improve the training of useful state-\nof-the-art language models, such as BERT [13]. As pointed earlier in Section 3.5, the corpus can\nbe used for downstream tasks such as machine translation, automatic speech recognition system,\nand sentiment analysis in the low resource language. Thus, it will be useful to develop a more\ndiverse translation system that is equipped to support the informal Hausa content commonly\nfound in online social media platforms such as Facebook and Twitter.\n9\nAppendix\nDetails about the visited urls used in the Website and Blog category (Section 3.2) can be found\nat the project’s Github repository available at https://github.com/ijdutse/hausa-corpus/\ntree/master/data/web-sites. Other relevant resources include the following:\n- HugginFace: https://huggingface.co/datasets/hausa_voa_ner#curation-rationale\n- Published Hausa Corpora: https://github.com/besacier/ALFFA_PUBLIC/tree/master/\nASR/HAUSA\n- Library of Congress: https://www.loc.gov/rr/pdf/requestingmaterials.pdf\nReferences\n[1] Philip J Jaggar. Hausa, volume 7. John Benjamins Publishing, 2001.\n[2] Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel Whitenack, Kathleen Siminyu, Laura Martinus, Jamiil Toure\nAli, Jade Abbott, Vukosi Marivate, Salomon Kabongo, et al. Masakhane–machine translation for africa. arXiv\npreprint arXiv:2003.11529, 2020.\n[3] ˇZeljko Agic and Ivan Vulic. Jw300: A wide-coverage parallel corpus for low-resource languages. 2020.\n[4] Tunde Adegbola. Building capacities in human language technology for african languages. In Proceedings of the\nFirst Workshop on Language Technologies for African Languages, pages 53–58, 2009.\n[5] Adewale Akinfaderin.\nHausamt v1. 0: Towards english-hausa neural machine translation.\narXiv preprint\narXiv:2006.05014, 2020.\n[6] Idris Abdulmumin and Bashir Shehu Galadanci. hauwe: Hausa words embedding for natural language processing.\nIn 2019 2nd International Conference of the IEEE Nigeria Computer Chapter (NigeriaComputConf), pages 1–6.\nIEEE, 2019.\n[7] Michael A Hedderich, David Adelani, Dawei Zhu, Jesujoba Alabi, Udia Markus, and Dietrich Klakow. Transfer\nlearning and distant supervision for multilingual transformer models: A study on african languages. arXiv preprint\narXiv:2010.03179, 2020.\n[8] Tim Schlippe, Edy Guevara Komgang Djomgang, Ngoc Thang Vu, Sebastian Ochs, and Tanja Schultz. Hausa large\nvocabulary continuous speech recognition. In Spoken Language Technologies for Under-Resourced Languages,\n2012.\n[9] Tanja Schultz. Globalphone: a multilingual speech and text database developed at karlsruhe university. In Seventh\nInternational Conference on Spoken Language Processing, 2002.\n[10] Isa Inuwa-Dutse.\nTowards combating pandemic-related misinformation in social media.\narXiv preprint\narXiv:2011.14146, 2020.\n[11] Hydrator [Computer Software]. Documenting the now. https: // github. com/ docnow/ hydrator , 2020.\n[12] Isa Inuwa-Dutse, Mark Liptrott, and Ioannis Korkontzelos. Detection of spam-posting accounts on twitter. Neuro-\ncomputing, 315:496–511, 2018.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n10\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-02-13",
  "updated": "2021-02-16"
}