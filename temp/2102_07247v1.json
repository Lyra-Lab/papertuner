{
  "id": "http://arxiv.org/abs/2102.07247v1",
  "title": "Reinforcement Learning for IoT Security: A Comprehensive Survey",
  "authors": [
    "Aashma Uprety",
    "Danda B. Rawat"
  ],
  "abstract": "The number of connected smart devices has been increasing exponentially for\ndifferent Internet-of-Things (IoT) applications. Security has been a long run\nchallenge in the IoT systems which has many attack vectors, security flaws and\nvulnerabilities. Securing billions of B connected devices in IoT is a must task\nto realize the full potential of IoT applications. Recently, researchers have\nproposed many security solutions for IoT. Machine learning has been proposed as\none of the emerging solutions for IoT security and Reinforcement learning is\ngaining more popularity for securing IoT systems. Reinforcement learning,\nunlike other machine learning techniques, can learn the environment by having\nminimum information about the parameters to be learned. It solves the\noptimization problem by interacting with the environment adapting the\nparameters on the fly. In this paper, we present an comprehensive survey of\ndifferent types of cyber-attacks against different IoT systems and then we\npresent reinforcement learning and deep reinforcement learning based security\nsolutions to combat those different types of attacks in different IoT systems.\nFurthermore, we present the Reinforcement learning for securing CPS systems\n(i.e., IoT with feedback and control) such as smart grid and smart\ntransportation system. The recent important attacks and countermeasures using\nreinforcement learning B in IoT are also summarized in the form of tables. With\nthis paper, readers can have a more thorough understanding of IoT security\nattacks and countermeasures using Reinforcement Learning, as well as research\ntrends in this area.",
  "text": "ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\n1\nReinforcement Learning for IoT Security:\nA Comprehensive Survey\nAashma Uprety and Danda B. Rawat, Senior Member, IEEE\nAbstract—The number of connected smart devices has been\nincreasing exponentially for different Internet-of-Things (IoT)\napplications. Security has been a long run challenge in the\nIoT systems which has many attack vectors, security ﬂaws and\nvulnerabilities. Securing billions of\nconnected devices in IoT\nis a must task to realize the full potential of IoT applications.\nRecently, researchers have proposed many security solutions\nfor IoT. Machine learning has been proposed as one of the\nemerging solutions for IoT security and Reinforcement learning is\ngaining more popularity for securing IoT systems. Reinforcement\nlearning, unlike other machine learning techniques, can learn\nthe environment by having minimum information about the\nparameters to be learned. It solves the optimization problem\nby interacting with the environment adapting the parameters\non the ﬂy. In this paper, we present an comprehensive survey of\ndifferent types of cyber-attacks against different IoT systems and\nthen we present reinforcement learning and deep reinforcement\nlearning based security solutions to combat those different types\nof attacks in different IoT systems. Furthermore, we present the\nReinforcement learning for securing CPS systems (i.e., IoT with\nfeedback and control) such as smart grid and smart transporta-\ntion system. The recent important attacks and countermeasures\nusing reinforcement learning in IoT are also summarized in the\nform of tables. With this paper, readers can have a more thorough\nunderstanding of IoT security attacks and countermeasures using\nReinforcement Learning, as well as research trends in this area.\nfelix2020sur Index Terms—Reinforcement Learning, IoT, Se-\ncurity\nI. INTRODUCTION\nInternet of Things (IoT) connects the physical world to\nthe digital world. It is a revolutionary technology in which\nmachines talk to other machines to solve trivial to complex\ntasks [1–4]. Sensors and actuators are the resources from\nwhich data is exchanged between the physical world and the\ndigital world. The sensors collect data that are to be stored and\nprocessed to provide service to the user. It has brought a drastic\nchange in the lifestyle of humans by bringing smartness to\nthe devices and will eventually increase the quality of human\nlife. IoT has tremendously increased the use of the internet by\nbringing all the physical devices together in the network. Any\nManuscript received Day Month Year.\nAuthors are with the Department of Electrical Engineering and Com-\nputer Science at Howard University, Washington, DC 20059, USA. E-mail:\ndb.rawat@ieee.org.\nThis work was supported in part by the US NSF under grants CNS/SaTC\n2039583, CNS 1650831 and 1828811, by the DoD Center of Excellence in\nAI and Machine Learning (CoE-AIML) at Howard University under Contract\nNumber W911NF-20-2-0277 with the US Army Research Laboratory, the\nDoE’s National Nuclear Security Administration (NNSA) Award # DE-\nNA0003946. and the US Department of Homeland Security (DHS) under\ngrant award number, 2017-ST-062-000003. However, any opinion, ﬁnding,\nand conclusions or recommendations expressed in this material are those of\nthe author and do not necessarily reﬂect the views of these funding agencies.\nphysical device brought to internet connection that can interact\nwith human can be an IoT device. For example, when cars are\nconnected to each other through the internet and communicate\nwith each other, this is called internet of cars.\nIoT collects and processes human day to day data and brings\nautomation to the task. With all the easiness provided by IoT,\nthere also exist some pitfalls in using IoT. The major challenge\nis securing the system from attackers, maintaining the privacy\nof the user of IoT and making sure that certain IoT devices\ncan be trusted. More the number of connected devices, more\nis the chance of the vulnerabilities to attack. Security in IoT\noperation is the major challenge to be faced by IoT designers.\nThe dynamic environment of IoT and runtime communication\nadds additional security requirements on the IoT design. IoT\nbrings ﬂexibility and intelligence to the devices providing us\nusability but at the same time, it is also fearsome to use it.\nIoT is gaining a status for insecurity. Researchers divulge the\ndangerous ﬂaws in IoT which poses a major challenge in IoT\nsuccess [2, 5, 6]. We are sharing our every personal information\nthrough IoT devices and it is very important that our data are\nconﬁdential.\nReinforcement learning is a machine learning approach in\nwhich the agent interacts with the environment and tries to\nmaximize the numerical reward [7]. Human brain interacts\nwith the outer environment and uses that interaction to un-\nderstand and sustain in that environment [8]. Reinforcement\nLearning uses the human brain and sensory processing system\n[9] as an analogy to learning the environment. It is a process in\nwhich an agent has to explore all the system to understand it.\nConsidering the time it takes to converge and get an optimal\npolicy, it is not feasible in many scenarios. Traditional RL\nsuffers a curse of dimensionality. As the environment becomes\ncomplex, there is exponential growth in the parameters to\nbe learned by RL agent [10]. As a solution, we have deep\nreinforcement learning (DRL) which is a combination of deep\nnetwork and reinforcement learning (RL) [11]. RL has been\napplied in securing IoT technology in various domains which\nis the main scope of this paper. IoT is a highly mobile\ntechnology and is very vulnerable to many cyber attacks. The\nsensors and actuators are one point of attack. Network for\ncommunication is again another major point of attacks in IoT.\nMuch research work has been done to provide security to IoT\nsystem using RL technology.\nThe main scope of this paper is to provide a literature review\nof research done on securing IoT devices using RL. Along\nwith this, we also provide a background of reinforcement\nlearning. The paper is organized as follows. In Section II, we\ncompare reinforcement learning with other machine learning\narXiv:2102.07247v1  [cs.LG]  14 Feb 2021\n2\nACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\ntechniques. Further, we discuss why RL is suitable for IoT\nscenarios. Section III is about research works related to RL on\nsecuring IoT from several threats. We present the application\nof RL in speciﬁc CPS system in Section IV. At the end some\ndiscussion and future research challenge is presented. Table I\nis the list of abbreviations we used throughout the paper.\nII. BRIEF OVERVIEW OF REINFORCEMENT LEARNING\nIn this section, we brieﬂy introduce reinforcement learning\nand talk about deep reinforcement learning. The comparison\nof RL and other machine learning techniques is presented. In\nthe end, we answer why the use of RL is effective in the IoT\nscenario.\nA. Reinforcement Learning\nReinforcement learning is a kind of machine learning in\nwhich AI agent aims to accomplish a task by taking the best\nnext step which can give them overall higher ﬁnal reward,\nas shown in Fig. 1. In RL setting, the agent goes through\nmany trial and error steps and tries to maximize the reward\nit gets from the environment [7]. An agent interacts with an\nenvironment, which can be a simulator, a game, the real world\netc. Each time step the agent observes the state st from the\nenvironment, selects an action at, and then receives a reward\nrt and the environment changes to st+1. Therefore, each time\nstep the agent gathers experiences (st, at, rt, st+1) from which\nit can learn. If the action taken was favorable for the given\nenvironment, it will get a positive reward. If not it gets a\nnegative reward. The agent continues to collect the reward\naiming to maximize expected return from each state [12].\nReinforcement learning is a Markov Decision Process (MDP)\nin which the output of taking an action from a state depends\nonly on the present state irrespective of past states and actions.\nMDP is a tuple consisting of ﬁve elements as (S,A,P,R,γ).\nIt uses discount factor γ which is a scalar value between\n0 and 1. The discount factor is considered to maximize the\nfuture rewards that the agent gets from the environment. Value\nfunction in RL is a mapping from states to real numbers, where\nthe value of a state represents the long-term reward achieved\nstarting from that state and executing a particular policy. Value\nfunction v(s) is a representation of how good it is for an agent\nto be in the state s. Bellman equation[13] is the foundation\nmathematics behind reinforcement learning.\nv(s) = E[Rt+1 + γv(St+1)|St = s]\nIn the given Bellman equation, the value function is decom-\nposed as an immediate reward plus the value at the next\nsuccessor state with discount factor(γ).\nB. Deep Reinforcement Learning\nDeep Reinforcement Learning (DRL) is a combination of\ndeep learning and RL. DRL is revolutionary research in RL\nwhich is capable to solve complex computational tasks [14].\nFor the complex environment, an approximation of value\nfunction and policy gradient is a complex task. For this,\ndeep network is used to approximate these values. Consider\nFigure 1. Agent-Environment Interaction in Reinforcement Learning.\nthe set of actions taken by agents that results in a positive\nreward. In this case, a normal gradient is used to increase\nthe probability of again taking these sets of actions. The\ndeep network adds intelligence to RL agents and hence it\naccelerates the agent’s capability to optimize the policy. RL is\nthe only machine learning technique that can learn without any\ndataset. However, as the agent interacts with the environment,\nit generates the dataset. These datasets are used to train the\ndeep network in DRL. Researchers have proposed many DRL\napproaches with its application ranging from control [15],\nresource management [16, 17], robotics [18, 19] and many\nmore. In 2015, Google DeepMind introduced deep Q-network\n(DQN) [20], delivering results exceeding human in playing\nAtari games. Deep neural network was used in DQN as\nthe function approximator. In Go games, AlphaGo [21] and\nAlphaGo Zero [22] also showed an excellent result. Following\nthat, DeepMind team made additional improvements based on\nDQN which builds a target DQN which calculates the maxi-\nmum Q value and they named it Double DQN [23]. Dueling\nDQN [24] is another signiﬁcant development. In situations\nwith the exponentially vast environment and continuous action\nspace, DDPG [22] was proposed which uses the actor-critic\nmethod. Other approaches are still the center research topic\nworldwide.\nC. Comparison of RL with other Machine Learning\nMachine learning can be classiﬁed as Supervised, Unsuper-\nvised and Reinforcement Learning. In supervised learning, the\nML model tries to predict the dependencies between training\ndata and actual answer about a problem asked about that data\n[25]. Basically, the input is given and we know what the model\nshould predict in this kind of learning. It learns based on\nexample. While reinforcement learning is about learning the\nenvironment without example. RL is more human like learning\napproach in which learning does not require large data. Here\nthe agent do not know the target labels. Unsupervised Learning\nuses unlabeled data to understand the pattern. On the other\nhand, RL learn through interaction with environment without\nany prior data.\nD. Why Reinforcement Learning in IoT\nIoT connects millions of devices over the network. IoT\ndevices are extremely dynamic and they make a complex\n3\nTable I\nABBREVIATION TABLE\nAbbreviation\nDeﬁnition\nRL\nReinforcement Learning\nDRL\nDeep\nReinforcement\nLearning\nIoT\nInternet of Things\nCNN\nConvolutional Neural Net-\nwork\nMDP\nMarkov Decision Process\nCPS\nCyber Physical System\nDoS\nDenial of Service\nDDoS\nDistributed Denial of Ser-\nvice\nDQN\nDeep Q-Network\nML\nMachine Learning\nSINR\nSignal-to-inference-plus-\nnoise ratio\nSDN\nSoftware\nDeﬁned\nNetwork\nCRN\nCognitive Radio Network\nWACR\nWideband\nAutonomous\nCR\nVANET\nVehicular\nAd-Hoc\nNet-\nwork\nUAV\nUnmanned Aerial Vehicle\nPOMDP\nPartially Observable MDP\nICMP\nInternet Control Message\nProtocol\nnetwork [26]. Supervised and unsupervised learning technique\nhave been used in security for intrusion detection [27–32],\ndetection of malware [30, 33–35], CPS attack detection[36]\n[37] and also in privacy maintenance task of IoT [38]. How-\never, these techniques can not perform dynamic responses\nfor security in IoT environment [39]. For example for any\nnew and constantly evolving cyber attacks, supervised and\nunsupervised learning method ﬁrst need to get the dataset of\nthose attacks and then only ﬁnd a solution by learning the\ndata. Reinforcement learning is applicable in IoT environment\nfor many reasons. The real-time dynamic environment can be\nmonitored efﬁciently in a favorable way. RL can continuously\nlearn new information to accommodate to different advanced\nsettings [40, 41]. Some IoT environments are so complex that it\nis difﬁcult to model it. RL minimizes the effort associated with\nsimulating and solving such complex environment. Consider\na complex IoT scenario and we have to come up with a\nmodel that can solve a problem in that environment. For using\nsupervised and unsupervised method, ﬁrst simulation is to be\nperformed to generate dataset and then only dataset is used\nto train the model. However, reinforcement learning algorithm\nperforms trial and error in the environment and learns a model.\nThis minimizes the complexity involved in simulating and\nsolving a problem in a complex environment. Data collection\nfor some IoT environment is extremely difﬁcult. In such a\nscenario, there are no datasets to train the model using other\nmachine learning techniques. RL is the only machine learning\ntechnique that can learn without prior datasets.\nE. Reinforcement Learning for Securing IoT Against Adver-\nsarial Learning environment\nReinforcement learning is regarded as one of the best\nsolutions for securing IoT against adversarial learning en-\nvironment that incorporates the environment’s behavior into\nthe learning process concurrently [42]. This salient feature of\nReinforcement Learning offers IoT security against adversarial\nlearning environment where large number of diverse IoT\ndevices produce huge amount of bursty data or continuous\ndata stream.\nIII. THREATS AND RL BASED SOLUTIONS IN IOT\nSECURITY\nThe rapid development of smart and mobile devices has\nmade signiﬁcant growth in IoT usage in many areas. Nowa-\ndays, IoT is incorporated in many domains. Industrial, power,\nagriculture, vehicles, battleﬁeld, homes [43] are the common\napplication domain of IoT. However, IoT is facing security\nproblems with growth in its usage. Privacy and security main-\ntenance is crucial for IoT systems. IoT uses advanced tech-\nnologies like radio-frequency identiﬁcations (RFIDs), wireless\nsensor networks, Bluetooth, Zigbee, and cloud computing.\nPrivacy protection and securing the system from cyber attacks\nlike DoS attacks, jamming, eavesdropping, malware, and virus\ninjection [44] is a most and at the same time a very challenging\ntask. Privacy leakage is another challenge for security in IoT\n[45]. For instance, devices that collect and report the actions\nof elderly people in a smart old care home must have to avoid\nprivate information leakage to prevent any harm to elderly\npeople from attackers. IoT system is susceptible to attacks\nlike network, software and physical attack. In this paper, we\nmainly look at the following IoT threats.\nA. Denial of Service Attack\nIoT systems face cyber-attack like Denial of Service (DoS)\nattack resulting in selective forwarding and eavesdropping\n[46]. In IoT system attacks, DoS is the most common\nattack [47]. These attacks on IoT networks place serious\nthreats to human life and direct or indirect ﬁnancial losses. A\nDenial-of-Service attack is a serious and most prevalent attack\nin which the attacker modiﬁes the connection of network\nin such a way that it becomes unavailable to its expected\nusers. DoS attack is achieved by ﬂooding the communication\nnetwork with unnecessary trafﬁc. A denial of service attack\ndisables the service in the victim’s side by sending notably\nhuge sizes of packets. The attack trafﬁc can use the large\nportion of available bandwidth resulting in the services not\nreachable to legitimate users. Another critical challenge for\nIoT security in the current scenario is protecting the system\nagainst distributed denial of service(DDoS) attack. DDoS is\ntypically a DoS attack but is of distributed nature. This results\n4\nACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\nin a compromise of a tremendous number of IoT devices at a\ntime. In 2016, a DDoS attack performed by Mirai botnet [48]\nhad affected around 65,000 IoT devices just within the early\n20 hours [49]. DoS attack obstructs the usage for the genuine\nuser resulting in the unavailability of network resources.\nDDoS is the same kind of attack but the only difference is it\nis drilled from distributed sources. IoT devices have limited\npower capacity to leverage mechanisms to detect these denial\nattacks. Network entrance for IoT can be the place to apply\ndetection and protection mechanism from such attacks.\n1)\nIoT layers: IoT architecture is mainly 3-layered [50].\nThey are perception layer, network layer and application layer,\nas shown in Fig. 2.\nFigure 2. Three Layered IoT Architecture.\n• Perception Layer: The perception layer is all about\nsensing the physical characteristics of objects using\nsensors, actuators and other devices. The process of this\nperception is reliant on sensing technologies like RFID,\nGPS, 2-D barcode labels and readers [51]. This layer\nalso takes control of converting sensed information to\ndigital signals. Chips to sense are to be designed and\nmade as small as possible to implant it inside the tiny\nIoT devices. The main task of this layer is to gather the\ninformation by sensing the objects.\n• Network Layer: The network layer can be visualized\nas the neural network or brain of IoT. This layer is\naccountable for processing the information gathered\nfrom the Perception layer [52]. Also, it is responsible for\ntransmitting the information to the application layer using\nwired/wireless networking technologies. Technologies\nlike Wiﬁ, Bluetooth, Zigbee, WirelessHART, Ethernet,\n3G and so on are used to transmit the information.\nBecause IoT sensors collect a massive amount of data,\nit is necessary to have a middleware that can handle this\nhuge amount of data. For this, cloud computing is the\nmain technology used in this layer.\n• Application Layer: Application Layer is the topmost layer\nin IoT architecture which is the frontend of the IoT archi-\ntecture. This layer realizes the application of the overall\nIoT system. It supports by providing the demanded tools\nfor developers to practicalize IoT vision. The application\nlayer uses the data transmitted to them from previous IoT\nlayers. Automatic sensing device management and node\nmanagement are handled by this layer [53].\n2) DoS Attack in IoT layers:\n• DoS in Perception Layer: RFID is the main sensing\ntechnology used in the perception layer. Several at-\ntacks like Jamming[54], Kill Command Attack[55], De-\nsynchronizing attack[56] are common in this layer.\n• DoS in Network Layer: Attackers perform ﬂooding at-\ntacks like ICMP ﬂood attack, Ampliﬁcation based ﬂood-\ning, Reﬂection based ﬂooding and many more[57]. For\ninstance, Wiﬁ, which is the major technology in this layer\nsuffers ICMP ﬂooding attack.\n• DoS in Application Layer: A common attack in the\nApplication layer is Path based DoS attack[58], Repro-\ngramming attacks and so on.\n3) Reinforcement Learning against IoT DoS attack: The\nauthors in[59] proposed an approach to protect against DDoS\nattacks by using a Multiagent Router Throttling. They pro-\nposed a model where multiple reinforcement learning agents\nare involved. These agents are installed on routers. The agents\nlearn to rate-limit or throttle trafﬁc towards a victim server.\nIt has been illustrated to work ﬁne against DDoS attacks in\nsmall-scale network topologies. But this method suffered from\nscalability problems. To eliminate this issue, they proposed\nCoordinated Team Learning design on their multi-agent router\nthrottling method [60]. This paper is centered on resolving\nthe scalability issues as mentioned earlier. Here they have\nproposed an approach that combines mechanisms like hier-\narchical team-based communication, task decomposition, and\nteam rewards to minimize the DDoS attack trafﬁc. They\nreferenced a network model as used by authors in [61] to\ndevelop emulator for throttling approaches. By using up to 100\nreinforcement learning agents, the scalability of the proposed\napproach is evaluated. This method is applicable in highly\nscalable IoT environment. Simulation results showed that the\nadaptability of the proposed model is highly improved. Rl\nagents throttles the attacker trafﬁc from ﬂooding the server.\nServer is an important component in IoT mechanism. This\napproach minimizes the DoS attacks in the server.\nSoftware Deﬁned Network (SDN) is a well known ar-\nchitecture for controlling large network space. SDN allows\nnetwork administrators to have more control of the network\nand facilitate the efﬁcient use of network resources [62]. SDN\nsupports the separation of data plane and control plane in\nswitches and routers [63]. The combination of IoT and SDN,\ncommonly known as software deﬁned internet of things, has\na potential solution to managing IoT network trafﬁc.The work\nin [64] tried to mitigate DDoS attack using DDPG method\nwhich is more scalable than the work proposed in [60]. In this\napproach, the DRL agents are placed in the central Software\nDeﬁned Network (SDN) instead of distributed router locations.\nThe DRL agent proposed here takes control of the trafﬁc that\nreaches the server and prevents over ﬂooding of trafﬁc in the\nserver. The mitigating agent is trained using DDPG algorithm\nand its state space are features of each port of switch and ﬂow\nstatistics. The authors have taken eight features in this paper.\nAction taken by an agent is throttling of trafﬁc based on the\nmaximum bandwidth allowed for a speciﬁc host. The DDoS\nattack mitigating agent gets a negative reward if it overloads\n5\nthe server with massive trafﬁc. Also, it gets a reward based on\nthe percentage of benign trafﬁc and attack trafﬁc reaching the\nserver. The agent learns continuously and can take control of\nthe trafﬁc ﬂowing to the server. Hence, it achieves the goal of\nmitigating DDoS attack on the server. The proposed agent can\nmitigate DDoS ﬂooding attacks of different protocols such as\nTCP SYN, UDP and ICMP.\nB. Jamming Attack\nJamming is an attack in which an attacker contaminates\nthe original content of information by assigning interruption\nsignals in the network or by barring the original content of\nthe information [65]. This results in the original content not\nreachable to the desired destination. Jamming is similar to\na DoS attack. In wireless networks, the Jamming attack is\nachieved by decreasing the signal-to-noise ratio at the receiver\nside. This is achieved by passing interfering wireless signals to\nthe network. The jamming attack can hinder the transmission\nof information between sender and receiver. Jammers use\nintentional radio interference to create disturbance in the\nnetwork. This keeps the communicating medium busy not\nallowing the transmitter to transfer messages. The jamming\nattack can be proactive and reactive [66]. In proactive\njamming, jammers send the interference signal all the time\nwithout taking care of whether there is communication going\non in the network. On the other hand, reactive jammers\nonly attack when they sense communication in the network.\nIntelligent technologies like RL can be the potential research\nsolution to jamming attacks in such IoT networks.\n1) Jamming\nAttack\nin\nIoT:\nIoT\nis\nthe\nlarge\nscale\ninterconnected system that is vulnerable to numerous attacks\ndue to its large attack surfaces. People are dependent on\nIoT devices more than ever and any attack on this system\nis serious to human life in some way. The jamming attack\nis another serious attack in IoT that can severely disrupt\nthe normal working of the IoT system. Jamming is one of\nthe most dangerous attacks that can interfere in wireless\ncommunication channels in the network by injecting false\npackets and interrupting the radio communication frequencies.\nConsidering this, the jamming attack is a major challenge\nand threat to IoT networks having nodes with conﬁned\nenergy and power [67]. Reactive jamming is a challenging\nattack faced by IoT networks compared to another jamming.\nReactive jamming consumes the energy of low power devices\nunnecessarily. Thus, IoT devices being low power are mostly\naffected by this kind of attack. There are many antijamming\ntechniques proposed for general wireless network [68–71].\nThe anti-jamming solution proposed for this traditional\nnetwork is not applicable in the IoT network. The reasons\nare IoT network is highly dynamic, heterogeneous and more\ndemanding. Also, IoT has limited memory, power, and\ntransmission resources [72]. More robust technologies like\nmachine learning can be an effective antijamming solutions\nin IoT environment.\n2) Reinforcement Learning against IoT Jamming attack:\nIoT technology can perform well only if the communication\nof information is secure and efﬁcient. So, the demand of\nwireless medium to support IoT functioning is high. It is\nchallenging to properly assure the management and availability\nof spectrum resources. Unavailability of spectrum resources\ncan impose a challenge to the sustaining of IoT technology.\nCognitive Radio Network (CRN) in IoT somehow manages\nthe spectrum utilization process. But the jamming attack is a\nserious security threat faced in CNR based IoT devices. Due\nto limited powered devices, wireless based IoT systems are\nmore suffered by the jamming attack. Several anti-jamming\nalgorithms has been proposed [71, 73–79]. In our paper, we\nbrieﬂy discuss the anti-jamming technique implemented using\nRL. The work in [80] proposed a deep reinforcement learning\nbased power control scheme for IoT transmission against\njamming. Convolution Neural Network is used as a deep\nlearning algorithm. The DQN-based power control scheme\nis implemented over the universal software radio peripherals\n(USRPs). Depending on the present IoT transmission status\nand strength of the jammer, the agents determine the transmit\npower unaware of the IoT topology. This approach showed\nenhanced signal-to-interference-plus-noise (SINR) of the IoT\nsignals compared with anti-jamming using Q-learning. They\nhave used DQN as an RL algorithm. Agent is the transmitter\nwhose action is to choose and set the transmit power. On taking\naction, the SINR at a time slot is measured and calculated at\nthe end.\nThe authors in [81] have proposed a two dimensional anti-\njamming communication using DRL. CRN is the network\nmodel used that has multiple Primary Users (PU) and jammers\nand a single Secondary User (SU). In this scheme, SU, without\ninterfering with PUs, utilizes both spread spectrum and user\nmobility to perceive jamming attacks. The authors proposed\nDQN based scheme to suggest the SU to take one of the two\npossible actions. First is to leave an area of heavy jamming\nand reconnect to another base station. Second, use one of the\nchannel to send signals (frequency hopping) to beat the smart\njammers. SU obtains an optimal anti-jamming communication\npolicy by using DQN algorithm without having information\nabout the jamming model and radio channel model. They\nused Convolution Neural Network to accelerate the learning\nrate. SINR and utility of the SU against cooperative jamming\nare improved compared to other learning approaches. SU is\nthe agent that choose action based on the system state. State\nspace includes the availability of the number of Primary Users\nand the discrete SINR value of the SU at that time slot.\nDQN approach followed in this work converged faster than\nQ-learning approach. The proposed DQN teaches the SU to\nchoose optimal frequency hopping policy and hence mitigate\na jamming attack. The CRN model has many application in\nIoT [82]. Thus the proposed RL based anti-jamming technique\ncan be applicable in antijamming in CRN based IoT devices.\nAlternatively, the authors in [83] have proposed a model\nin which the receiver of SUs can decide to stay or leave the\ncurrent location to combat jamming attacks. This mobility can\ncause some overhead, so it should ﬁnd an optimal policy either\nto stay at the current location or move. Here, DQN based on\n6\nACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\nCNN have been used by the receiver to choose the action that\nmaximizes its utility. RL state space is the discrete measure\nof SINR of the signal sensed by the receiver at that time slot.\nAction by the receiver is whether to leave the location or\nstay there. They concluded that the proposed method achieved\nfaster convergence and higher SINR as compared to Q-learning\napproach.\nBoth the works in [81] and [83] took account of the\ndiscrete SINR value as RL state. But in a scenario of inﬁnitely\nlarge SINR, these approach is not suitable. Also, the SINR\nconsidered in these approaches may be noisy and false. To\naddress this issue, a Recursive Convolutional Neural Network\n(RCNN) that handles inﬁnite state problem was proposed\nby authors in [84]. An optimal anti-jamming strategy was\nachieved by the proposed DRL model. The proposed algorithm\nimproved the anti-jamming strategies against dynamic and\nintelligent jammers. Spectrum waterfall is deﬁned as a state\nspace of the RL environment. Spectrum waterfall utilizes\nthe spectrum information with temporal features. It does not\nrequire jamming pattern information so it is applicable against\nsmart jammers who continuously change their jamming pat-\ntern. The preprocessing layer in RCNN can remove excess\nnoise from the environment and hence reduce complexity.\nIt ﬁlters out the SINR with the help of a noise threshold.\nAnd recursive convolution layer handles the recursive input\nstate. The simulation result validates the proposed algorithm\nby showing that the user can avoid jamming even if the\njammers change jamming pattern intelligently. The proposed\nalgorithm of DQL with RCNN shows faster convergence\nthat Q-learning against ﬁxed jamming attacks. The proposed\nmethod converged in the presence of dynamic jammers but Q-\nlearning could not converge in this case. However, the work in\n[85] provided a theoretical proof of a condition in which the\nmethod proposed by [84] cannot converge. Here the authors\nraise a question against the previous DRL-based anti-jamming\nstrategy. When the jammer is intelligent enough that can learn\nthe communication pattern of the user and modify its jamming\npattern accordingly, the previous model fails to converge. Here\nthey design an RL agent against DRL anti-jamming. RL agent\nobserves the frequency spectrum and based on that it chooses\nthe frequency band to jam. They have opened a research\nchallenge against intelligent jamming attacks.\nWideband autonomous cognitive radio (WACR) based anti-\njamming using RL was proposed by authors in [86]. WACR\nmakes the use of its spectrum sensing ability to locate sweep-\ning jammers. WACR not only senses the active signal but\ncan also classify the signal properties which aids in ﬁnding\nsuch signals [87]. They deﬁne three steps wideband knowledge\nspectrum acquisition framework. They are wideband spectrum\nscanning, spectral activity detection, and signal classiﬁcation.\nA reinforcement learning based decision policy is proposed\nin which a WACR learn an optimal policy to pick the sub-\nbands for sensing and transmission. The selection of sub-band\nis based on the desired contiguous length of idle bandwidth\nfor a sub-band. For the sensed sub-band, Neyman-Pearson\n(NP) detector is used which allows the WACR to ﬁnd the\nfrequencies of all active signals in that sub-band. In this Q-\nlearning based RL setting, the action of WACR is either to\nremain in a sub-band or to switch to other sub-band. WACR\non taking each action updates its Q-table on the basis of\nreward it gets from the environment. Reward, in this case,\nis dependent on the amount of time WACR can avoid the\njammer. Experimental results from the simulation showed that\nthe Q-learning can learn the sweeping jammer pattern and can\noptimally switch the sub-bands to avoid jamming. A similar\nQ-learning approach is proposed for the WACR network in\nthe work [88]. The only difference is that the later one uses\na multi-agent Rl approach. They considered multiple WACRs\nand proposed a similar Q-learning approach to achieve anti-\njamming against sweeping jammers and interference from\nother WACRs. When multiple WACRs are operating in the\nsame spectrum range and there is sweeper jamming, the\nproposed multi-agent approach avoids sweeping jammers and\ninterference from other WACR. However, both [86] and [88]\nassumed ﬁxed jammer. Both did not cover a scenario in which\nsweeping jammers can also be cognitive and smart enough to\nadjust its jamming strategy accordingly.\nC. Spooﬁng Attack\nA spooﬁng attack is a case in which a malicious node\nimpersonates to be another person or device over the network.\nThe main aim of this attack is to get trust from nodes and\naccess the legitimate node to steal information or spread\nmalware [89]. Spooﬁng attackers trick the user or a node\nto believe that they are trustworthy and falsely access the\ninformation. A spooﬁng attack is of different types and we\nwill discuss some of them in brief. IP spooﬁng is done by\nimpersonating the IP address, sending information through that\naddress and trick the receiver to believe that information [90].\nARP spooﬁng is about sending the falsiﬁed ARP messages\nin the network [91]. The target of this attack is to falsify\nthe victim node to send the information to a malicious node\ninstead of sending it to a legitimate one. Other spooﬁng attacks\nlike DNS spooﬁng, web spooﬁng, email spooﬁng, etc are\ncommon.\n1) Spooﬁng Attack in IoT: IoT devices are interconnected\nand they share the information which is privacy critical. The\ntaxonomy of security attacks in the work [92] showed different\ncyber-attacks in IoT. Spooﬁng attack is a serious attack which\nmay even lead to DDoS attack and Man-in-the-Middle attack.\nLet us look at a typical example of how spooﬁng attacks can\ndisrupt IoT setting. Suppose an IoT scenario of connected\nmultiple UAVs that are deployed in monitoring and controlling\nbattleﬁeld information. Spooﬁng attackers can be any unknown\nUAV that tries to join the network. On gaining trust from\nthe network, the attacker can fake themselves to be genuine\nhowever it is malicious. The attacker UAV on joining the\nnetwork can sense all the critical information of the battleﬁeld.\nIt can also pass false information in the network which will\ncause a serious disruption of the battleﬁeld.\n2) Reinforcement Learning against IoT Spooﬁng Attack:\nReinforcement learning is like a game in which the agent\nplays with the opponent and learn the strategy followed\nby the opponent. In case of IoT communication, physical\nlayer information like received signal strength, channel state\n7\ninformation and channel impulse response can be useful\nin authenticating the transmitter [93]. Active authentication\nbased on ambient radio signal is one way to authenticate\nthe device and prevent spooﬁng attack. However, it is hard\nto obtain the dynamic time-variant channel mode in a real\nenvironment. Reinforcement learning was used to obtain this\ntime-variant channel information in the work [94]. Here the\nauthors proposed an active authentication of mobile devices in\nthe indoor environment using reinforcement learning. Here au-\nthors considered the PHY-layer information to detect spooﬁng\nattacks. The received signal strength at the receiver, which is\ntrust authority, was considered to detect spooﬁng attack. The\nreceiver formulates a hypothesis test to determine whether\na packet is sent from the particular address or not. The\nreceiver on getting a packet computes the test statistics of\nthe hypothesis test. If it is below a threshold, the receiver\naccepts the packet as authentic otherwise detects the packet\nas a spoofed packet. The test threshold of hypothesis test in\na dynamic environment is chosen by reinforcement learning.\nQ-learning was used to ﬁnd the optimal threshold strategy\nwithout knowing the model of the arriving packet. State space\nin this environment is the false alarm rate and miss rate.\nBased on the observation of states, the receiver chooses a test\nthreshold from L levels. State-action function in Q-learning\nis updated and utility calculated by the receiver is the reward\nfunction. Here the agent uses epsilon-greedy policy to get the\noptimal test threshold. The simulation result in experimenting\nwith a legitimate user and three spoofers showed that the\nproposed Q-learning based test threshold strategy gave better\nutility. Also, the result showed that the proposed approach\nminimized the convergence of the false alarm rate and miss\nrate as contrasted to the ﬁxed threshold approach. Compared\nto the ﬁxed threshold approach, the proposed Q-learning-\nbased threshold can efﬁciently detect the spooﬁng attacks in\na dynamic environment.\nThe work in [95] followed a similar approach as done\nby authors in [94] to detect spooﬁng attack. Reinforcement\nlearning was implemented to ﬁnd the optimal test threshold.\nBut here the authors have compared the performance of the RL\nagent on the following two algorithms. They have compared\nthe performance of Q-learning and Dyna-Q algorithm. The\nsimulation was implemented in an indoor environment in\nUSRPs. False alarm rate and miss rate are the states for\nthe agent and utility at receiver is given as reward function.\nThe simulation result showed that the error rate with Dyna-\nQ is lower than with Q-learning. The detection rate using\nboth algorithms is better than the ﬁxed threshold approach.\nSpooﬁng attack detection in an indoor environment is covered\nby previous papers. Authors in [96] proposed a rogue edge\ndetection scheme for VANETs (Vehicular Ad Hoc Network)\nobserving the ambient radio signals. Similar to the previous\napproach, here also authors used Q-learning to allow mobile\ndevices to reach optimal rogue edge attack detection policy\nwithout being aware of the dynamic VANET model.\nMost of the security approaches are reactive i.e. they try\nto detect the security breach and then only recover from the\nattack. However, the work in [97] tried to predict the intention\nof attack. They considered sensor spooﬁng attacks in one or\nmore sensors of an autonomous vehicle with multiple sensors.\nThe attackers try to take the vehicle in the undesired state by\nspooﬁng and hiding inside the sensors. Here authors came up\nwith a Reachability-based approach and Inverse RL to predict\nthe intention of the attacker and detect the compromised sen-\nsors. First reachability analysis was used, as done by authors in\n[98], to ﬁnd the set of possible states the vehicle can reach on\na certain time slot. Inverse RL was used to infer the maximum\nreward function expected by the attacker. The approach used\nwas to ﬁnd the group of sensors that deviate the vehicle\ntowards the undesired state. Bayesian Inverse RL (BIRL) can\nlearn the reward function in Markov Decision Process (MDP)\nif they are given the behavior and dynamics of the system\n[99]. Given the set of observations, they calculate the posterior\nprobability of all reachable goals. If any of the sensors return\na state value such that the variance of the posterior probability\nis within the user selected threshold, the recovery procedure is\ninitiated. The simulation result showed that when the variance\nof the posterior probability of goal of attacker reached below\nthe threshold value, the spoofed sensor was detected and\nomitted from the state estimation and a recovery process is\ninitiated. Similar to this work, BIRL was used in the work\n[100] to detect the spoofed sensor in an autonomous vehicle\nenvironment. In this work, authors have used active exploration\npolicy in which the vehicle explores the environment to reach\nsensitive states. Active exploration prevented the vehicle to\nreach states very near to the undesirable state.\nIV. REINFORCEMENT LEARNING IN CYBER PHYSICAL\nSYSTEMS\nA. Security in Smart Grid\nSmart Grid is an intelligent system to generate and distribute\nenergy in a distributed manner. It is the combination of the\ntraditional power grid and information systems that allows\nefﬁcient energy generation and consumption. Digital process-\ning in the traditional power grid leads to Smart Grid which\ngives the capability to control, communicate and monitoring\nof available energy sources. However, smart grids being online\nand connected is vulnerable to various cyber attacks. The\nintegration of cyber component exposes it to critical cyber-\nattacks and unauthorized penetration. Cyber Physical Attack\nalso called a blended attack imposes a threat to both the\ncyber and physical systems of the grid thus causing negative\nconsequences than by the individual attack (cyber attack or\nphysical attack) [101]. Attacks like information tampering and\neavesdropping throw a big threat to the security of Smart Grid\n[102]. Researchers around the world are concerned about the\nsecurity in this area of CPS and have proposed several security\napproaches. Here, we will talk about security approaches taken\nto protect Smart Grid using RL.\nA sequential attack on the network topology of a smart grid\nis a serious attack in which the attacker can determine the\nnumber and time to attack the component to cause maximum\ndamage. A sequential attack imposes more damage than by a\nsimultaneous attack when attacked on the same victim links\n[103]. The authors in [104] proposed a Q-learning based\nvulnerability analysis of smart grid under sequential attack\n8\nACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\nadmitting the physical system behaviors. Here the authors\ndeﬁned sequential attack as a sequence of coordinated in-\nterdiction such that it changes an in-service line into out-\nof-service. By manipulating the control commands or false\nline status data such an attack can be performed resulting in\ncascading blackout. The authors proposed a Q-learning based\nvulnerability analysis in a smart grid under sequential attack.\nIn this RL environment, the agent is the attacker who tries\nto identify the more vulnerable point in the grid under a\nsequential attack. State space is either in-service line or out-\nof-service line at a time. Action taken by the attacker is to\nmaliciously turn the in-service line to out of service. The goal\nof the attacker using Q-learning is to mind the optimal policy\nto fail the system with the least number of lines attacked.\nIf the attacker can reach blackout by turning of lines equal\nto or more than a threshold line value and with less action\nthan the threshold value, it will get a positive reward. It gets\na negative reward on taking more action than the threshold.\nOtherwise, the reward given is zero. The experimental results\nof this approach successfully identiﬁed the critical sequential\ntopology attacks. The blackout sizes in the proposed method\nshowed that with the increase in load, sequential attacks caused\nmore line outages and attack intentions were accomplished\nquicker. The proposed Q-learning approach tried to learn\nand ﬁnd out vulnerable sequences that directed to severe\nblackouts in the system. Using this vulnerability analysis of\nthe sequential attack, the defender side can follow the security\nmeasures to better the situational awareness cyber-security\napproaches.\nFalse Data Injection (FDI) is proven to be a challenging\nattack to the smart grid in which the attacker injects mali-\ncious data to the Supervisory Control and Data Acquisition\n(SCADA) system resulting in cascading failure of a smart\npower system. In the work [105], an intelligent FDI attack\non a smart grid with automatic voltage control was studied.\nThe authors considered a smart attacker that uses Q-learning\napproach to ﬁnd the optimal attack strategy stealthily to ma-\nnipulate the control system in a compromised substation. In the\ngiven paper, the state space is the voltage angle, the amplitude\nof the buses, the active and reactive power of the generator\nand, the active and reactive power of the load. An attacker\ncan perform FDI only based on local observation so authors\nconsidered the attack as Partially Observable MDP (POMDP).\nAttacker action is to compromise many measurements of the\nattacked substation. The reward function is deﬁned such that\nthe attacker’s action makes the bus voltage in compromised\nsubstation lower than the desired operational voltage. By\ngiving rewards for no action, the attacker stops injecting FDI\nand avoids giving a ﬁxed action pattern. Using Q-learning\nmethod with the nearest memory sequence results showed that\nthe proposed FDI, with little knowledge of the complete power\nsystem, could generate voltage breakdown events. Online\nlearning helps the attacker to choose probable attack times\nautomatically to make the attack silent. The test result shown\nin the results section validated the bad data detection and\ncorrection method presented against the proposed FDI attack.\nSome advantages and disadvantages of the some approaches\nfor securing IoT with reinforcement learning is tabulated in\nTable II.\nThe work in [106] tried to design a defender system against\ncyber attack in a smart grid using reinforcement learning. The\nauthors proposed a model free RL algorithm that can defend\ncyber attacks on the ﬂy without knowing any attack model. A\ndefender is proposed that can detect the low magnitude attack\nwhich will be the worst-case scenario for the defender. This\nmakes the defender sensitive to even a very slight deviation\nof measurement from a normal measurement. The proposed\ndefender system also limits the action space of the attacker.\nAn attacker can only make a lower magnitude attack to be\nnot detected. Such a lower magnitude attack, however, can\nnot make damage to the system. The agent does not know\nthe attacker attack time, so they considered two state i.e.\npreattack and postattack state. State space is the status of\ntransmission lines in the power system. After observing the\nmeasurement, agent (defender) can take two actions. Either\nthey can stop and declare an attack or they can continue to\nobtain more measurements. The goal of the agent to lessen\nthe detection delays and false alarm rates. Here the reward\nis the cost associated with the detection delay compared with\nthe false alarm rate. If the agent in preattack state takes action\nto stop, it gets a unitary reward. While if in postattack state\nit takes continue action, a cost is given as a reward which\nis due to the detection delay. SARSA algorithm was used to\ntrain the agent and update the Q- table. SARSA is a model\nfree RL algorithm that is shown to have better performance\nin POMDP environment [107]. Using this learned Q-table,\nthe agent performs online attack detection by choosing the\naction that leads to the minimum expected future cost. The\nagent continues to take action until it takes stop action on\nwhich it declares that there is attack in the system. They\nhave shown the simulation result of the proposed RL based\ndefender in the presence of different kinds of attacks and\ncompared with Euclidean detector and Cos-Sim detector. The\nresult showed that RL based detector detected the attack with\nvery low detection delay as compared to other approaches.\nHowever, here they consider single agent defender which can\nbe extended to multi agent and they have not considered a\nsmart attacker.\nNi and Paul [108] proposed a dynamic game between the\nattacker and the defender to ﬁnd the optimal attack strategies\nusing reinforcement learning. The attacker learns the attack\nsequence to be applied in the transmission lines. On the\nother hand, the defender learns to protect the lines selected.\nAn attacker takes generation loss and line outages as the\nreward and based on which it plans the next action. Here\nthe attacker ﬁnds the critical transmission lines in the smart\ngrid based on the action taken by the defender. This learned\nattack sequence is used by the defender so that it minimizes\nthe action set for the attacker. In this multistage game, they\nﬁrst assumed defender to be passive and attacker to be smart\nlearner. Defender policy was predeﬁned so using that policy\ninformation, the attacker performs trial and error action using\nQ-learning and conducts an attack on the transmission line.\nCalculation of generation loss and cascade are done and after\ngetting a reward, the Q-table is updated. Later at the end of\nthis multistage game, the defender aligns its action based on\n9\nTable II\nADVANTAGES AND DISADVANTAGES OF THE SOME OF THE RESEARCH WORK ASSOCIATED WITH SECURING IOT WITH REINFORCEMENT LEARNING\nApproach\nGoal\nSpecialities(+) and Limitations(-)\nMultiagent Router Throttling [59]\nMultiple Agent Learn to throttle the trafﬁc to\nvictim server\n+ Solves Stability issue\n-Not Scalable\nCoordinated Team Learning in Multiagent\nRouter Throttling [60]\nHierarchical team based communication to\nthrottle excessive trafﬁc reaching server\n+ Scalability is achieved\n+ Improved Adaptability\n- Consideration of less statistical feature\n-Lower Data Efﬁciency\nSmart mitigation Agent in Software Deﬁned\nNetwork [64]\nMitigation Agent throttles the trafﬁc by\nevaluating the controller of SDN\n+Highly Scalable\n+Improved Data Efﬁciency\n+Reduces overhead on SDN switches\nPower Control for IoT against Jamming [80]\nIoT device decides transmit power in a way to\nimprove SINR and utility in the presence of\njammers\n+More realistic approach\n(experimented under hardware constraint)\n+Improved Communication Efﬁciency\n- Cost overhead\nTwo-dimensional anti-jamming communication\n[81]\nAvoid jamming attack smartly without\ninterfering primary user\n+Faster Convergence\n- Cannot handle smart jammers\nAntijamming in underwater acoustic network\n[83]\nTo control transmit power against jamming in\nacoustic network for underwater robots and\nvehicles.\n+ Higher learning speed\n- Not scalable\n- Cannot handle smart jammer\nAntijamming communication using Spectrum\nWaterfall [84]\nTo achieve antijamming in dynamic environe-\nment in the presence of smart jammer\n+ Less information loss\n+ Reduced complexity\n- Cannot converge in the presence of\nRL-based jammer\nAntijamming with Wideband Autonomous\nCognitive Radio [86]\nOptimal sub-band selection against jammers\n+Reduced Complexity\n- Not practical\nActive Authentication of mobile devices [94]\nAutheticate mobile devices against spooﬁng\nattack\n+ Privacy Protection\n+ Reduced overhead cost\n- Not Scalable\nPhysical Layer Rogue edge detection in\nVANET [96]\nTo ﬁnd rogue edge node based on physical\nproperties of ambient radio signals\n+Handle dynamic environment\nPredicting malicious intention under cyber\nattack [97]\nTo predict the goal of sensor spooﬁng attack\nand determine the compromised sensor\n+More realistic approach\n-Higher Computation Complexity\n-Slower convergence speed\nRL approach for attack intention prediction\n[100]\nTo predict the intention of attacker and detect\nthe set of compromised sensor\n+Faster convergence speed\n- Complex computation\nthe observation and using the sequence learned by the attacker.\nHere the game was proposed as a zero-sum game in which the\nreward given to attacker and to defender are opposite to each\nother. The experimental result showed that total line outages\ncaused by the multistage attack are more consequential than a\nsingle stage attack. Also, the result here showed the decreased\nnumber of successful attacks and average generation loss on\nadjusting with the strategy of the defender. The shown case\nstudies in the paper imply that learned information of the\nattacker can ultimately assist the defenders to plan for better\ndefense policies.\nB. Security in Smart Transportation System\nSmart Transportation System (STS) is a CPS system that\nconsists of sensors technology, control, and communication\nin vehicles and any other transportation infrastructure. The\ngoal is to provide real-time road and other vehicle information\nfor users to improve safety and comfort in transportation.\nIt achieves the smartness in transportation by establishing\nthe connection between vehicle to vehicle (V2V), vehicle\nto other infrastructures (V2I), vehicles to pedestrian and so\non [109]. However, security challenges in STS are posing a\nthreat to this system. It should properly handle issues like\nprivacy protection, authorization, data integrity, data storage,\nand management [110]. Security is always perceived as one of\nthe most important considerations in realizing STS usecases\n[111]. Cybersecurity researchers around the world have pro-\nposed several methods to secure this transportation. Machine\nLearning is an emerging technology that have added more\nsmartness to the STS system and also it has been used in\nsecuring the system intelligently. VANET and FANET (Flight\nAd-Hoc Network) can both be considered as STS.\nNext we discuss the application of RL for security in STS.\nThe work in [112] presented deep reinforcement learning\napproach for Unmanned Aerial Vehicle (UAV) against smart\nattacks with no information on the attack model and accuracy\nof the system to detect the attack. DQN was used to ﬁnd\nthe optimal power allocation strategy against a smart attacker.\nAuthors ﬁrst formulated a prospect theory based smart attack\ngame to ﬁnd the attack on UAV transmission by a subjective\n10\nACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\nTable III\nREINFORCEMENT LEARNING PARAMETERS FOLLOWED BY SOME OF THE RESEARCH APPROACHES.\nReferences\nAlgorithm\nAgent\nState-Space\nAction-Space\nReward\n[59]\nSARSA\nRouter\nTrafﬁc ﬂowing\ntowards server\nProbabilistic throttle\nof trafﬁc coming from\nhost\nNegative reward if\nserver is overloaded\notherwise reward\ndependent on the rate\nof legitimate trafﬁc\n[64]\nDDPG\nMitigating Router\nFlow statistics and\nfeatures of each port\nGenerate a vector\nrepresenting\nmaximum bandwidth\nof speciﬁc host\nNegative reward\nif load on server\nexceeds an upper\nbound otherwise\ndeﬁned by reward\nfunction\n[80]\nDQN with CNN\nIoT devices\nSINR and utility\nvalue\nDecide transmit\npower at a time slot\nPositive reward if\nSINR and utility is\nimproved\n[81]\nDQN with CNN\nMobile device\nPresence of PUs and\nSINR of the signal at\nprevious timeslot\nDecide to leave or\nstay at an area and\nchoose channel\nSINR and utility\n[83]\nDQN with CNN\nSensor\nSINR and RSSI at\nprevious timeslot\nChoose trasmit power\nand decide to stay or\nmove to another area\nUtility of the signal\n[84]\nDRL with RCNN\nSensor\nRaw frequency\nspectrum information\nChoose signal\nfrequency\nDeﬁned by function\nbased on utility and\ncost of frequency\nswitching\n[86]\nQ- learning\nRadio\nRequired bandwidth\nlength\nSelect a new sub-\nband\nTime taken by\njammer to interfere\nthe transmission after\nswitching to a sub\nband\n[94]\nQ-Learning\nRadio Device\nFalse rate and miss\nrate of authenticated\npackets\nSelect test threshold\nvalue\nUtility\nattacker. Then DQN is proposed to ﬁnd the optimal power\nallocation strategy in multiple frequency channels. They com-\npared the convergence rate achieved by using Q-learning, DQN\nand WoLF-PHC (Win or Learn Fast- Policy Hill Climbing)\nfor power allocation against the attacker. UAV sends a signal\nin each time slot with certain power using DQN approach\nand observing the state of the network. Observation is the\nSINR value and utility of the received signal. The results\nshown in the paper depicts that DQN based power allocation\nis applicable for UAV having enough resources. On the other\nhand, WoLF-PHC based strategy can choose a transmission\nstrategy with a lower computational cost. Here UAV can\naddress the Q-learning based smart attack by learning the\noptimal transmission strategy. VANETs in a large network\ntopology bring high mobility in the onboard units (OBUs).\nDue to this large scale and dynamic nature, an antijamming\nstrategy like frequency hopping is not efﬁcient. The work\nin [113] presented a UAV based antijamming approach in\nVANET using reinforcement learning. This is a follow up\nresearch for the proposed UAV relay strategy in the work [114]\nconsidering more practical aspects. Here authors proposed a\nrelay game in which UAV learns whether or not to relay the\nOBUs data to another roadside unit (RSU) and smart jammer\ndecides its jamming power. The authors presented the Nash\nequilibrium (NE) to show the dependence between the trans-\nmission cost and channel model with the UAV relay strategy.\nHere hotbooting-PHC based strategy was presented for faster\nUAV relay decisions. Hot booting uses the experimental data\ngenerated in advance to update the Q-table. This initializes\nthe Q-value and probability of action-state and hence learning\nspeed is signiﬁcantly higher. Here UAV decides relay action\nbased on the observed Bit Error Rate (BER) of the data send\nby UAV and the channel quality. The experimental result\nshowed the decreased BER of OBU data and increased utility\nof VANET using the proposed algorithm than by using the\nQ-learning approach. Thus if the serving RSU for an OBU in\nVANET is severely jammed, the proposed UAV based relay\nstrategy can transfer that information to another RSU and\nprevents the VANET from potential jamming affects.\nConnected and autonomous vehicles (CAVs) and UAVs in\nIoT can be misused by attackers which directly impose a\nthreat to the STS. The authors in [115] proposed an anti-\njamming V2V communication in an integrated UAV-CAV\nnetwork with hybrid attackers. They assumed a malicious CAV\nthat can perform smart jamming and a malicious UAV without\nsmartness. Inspired by the predictive-adaptation feature of the\nhuman brain, they proposed a research tool called CDS to lead\nthe idea of an anti-jamming technique. The process of channel\n11\nselection is based on the risk level evaluation by task-switch\ncontrol and following the process of power control completion.\nReinforcement Learning is used for power control and channel\nselection. Here, the channel selection task is viewed as multi-\narmed bandit (MAB) problem [116] and the upper conﬁdence\nbound (UCB1) algorithm (index based policy) [117] is used as\nits solution. Experimental results showed a better transmission\npower and channel allocation strategy against hybrid attackers\nby the proposed method. Typical parameters for reinforcement\nlearning for different research works are listed in Table III.\nV. RESEARCH TRENDS AND OPEN RESEARCH\nCHALLENGES\nIoT is a highly dynamic environment generating a massive\namount of transactions. Connected devices in the network\ncan be millions in numbers making the security approaches\nmore challenging. Reinforcement Learning is proven to be\napplicable in securing the IoT technology from various attacks.\nWe presented research works done to secure the IoT system\nin our paper. It has been proven a powerful technology in IoT\nsecurity. However, there are some challenges to be considered.\nSome of the challenges and open issues are discussed below.\nA. Discretizing of action-state space and minimizing curse of\ndimensionality\nMost of the work done addresses ﬁnite action and state\nspace i.e. a discrete set of action spaces or ﬁnite MDP prob-\nlems. However, in a real IoT environment, the RL algorithm\nshould take care of continuous action and state spaces. Several\nworks have considered discretizing the action-state spaces.\nBut discretizing is a very expensive learning process and\nnot suitable in extremely non-linear problems like in IoT\nenvironment. The actor-critic approach has been proven to\nbe an effective solution to address continuous action space\n[118]. Application of actor-critic algorithms in solving IoT\nsecurity issues can be a further research approach in this\nﬁeld. Also, there is another approach called hierarchical deep\nreinforcement learning which decomposes the problem states\ninto smaller parts [119]. This can reduce the curse of dimen-\nsionality as faced by traditional RL approaches. This approach\nminimizes scaling problem by sub tasking any task and hence\nminimizing action and state space at a time. In such a dynamic\nand huge IoT environment, a hierarchical RL approach can be\napplied for better and quick optimization.\nB. Learn with partially observable environment\nReinforcement learning is an optimization problem that\nconsiders MDP environment. However, in IoT scenario, most\nof the environment are only partially observable. The rein-\nforcement learning agent in this IoT scenario, can not have\nthe complete perception of the environment. The reasons are\nbecause sensors are of limited sensing capacity and there is\ntransmission loss due to limited transmission capacity in IoT.\nDRL approaches have been used in POMDP environment.\nBut it is only applicable is small scale IoT environment. The\npotential solution to this problem could be the integration\nof recurrent neural network and RL to ﬁnd the policies in\nPOMDP environment.\nC. Joint reward from multiple agents\nWe have discussed the research work that considered mul-\ntiple RL agent located at different devices or sensors in\na distributed manner. Each agent can have speciﬁc task or\nsimilar task to perform. In most of the works, multiple agents\nare thought to perform similar task. The design of multi agent\nRL system with different agents performing different task is to\nbe studied. The challenge could be the collaborative method\nof considering the rewards from all the agents. This makes\nthe application of multiple agents a complex problem. IoT\nenvironment is highly dynamic and the proper control between\ndifferent agents is a demanding task to be worked on.\nD. Robustness against Adversarial RL\nOne of the challenge and active research area to apply re-\ninforcement learning in IoT is the consideration of adversarial\nenvironment. Very few works in literature have looked into\nthe problem of applying RL against adversarial conditions.\nThe environment can be adversary which continuously tries\nto win over the agent trying to learn the environment. In\nan multi agent RL problem, one of the agent can be an\nadversary. Therefore there should be a way to ensure that\nthe learned policy is robust against any uncertain changes\nin the environment. Methods to make the agents trained by\nreinforcement algorithm robust against any adversarial attacks\nis an open research challenge.\nVI. CONCLUSION\nIn this paper, we have presented a comprehensive survey\non the application of Reinforcement learning for IoT security.\nFirst, we have given a brief introduction about Reinforcement\nlearning and background information about several attacks in\nIoT. Following that, we have presented a survey on various re-\ninforcement learning techniques proposed against IoT attacks\nsuch as jamming attack, spooﬁng attack and denial of service\nattack. Furthermore, we have presented the Reinforcement\nlearning for securing CPS systems (i.e., IoT with feedback\nand control) such as smart grid and smart transportation\nsystem. Moreover, we have presented some open research\nchallenges and some research direction for IoT security using\nreinforcement learning.\nREFERENCES\n[1] L. Atzori, A. Iera, and G. Morabito, “The internet of things: A survey,”\nComputer networks, vol. 54, no. 15, pp. 2787–2805, 2010.\n[2] F. Olowononi, D. B. Rawat, and C. Liu, “Resilient Machine Learn-\ning for Networked Cyber Physical Systems: A Survey for Machine\nLearning Security to Securing Machine Learning for CPS,” IEEE\nCommunications Surveys and Tutorials, 2020.\nEarly Access, DoI:\nhttps://doi.org/10.1109/COMST.2020.3036778.\n[3] D. B. Rawat, R. Alsabet, C. Bajracharya, and M. Song, “On the\nperformance of cognitive internet-of-vehicles with unlicensed user-\nmobility and licensed user-activity,” Computer Networks, vol. 137,\npp. 98–106, 2018.\n[4] D. B. Rawat and C. Bajracharya, “Vehicular Cyber Physical Systems:\nAdaptive Connectivity and Security,” tech. rep., Springer, 2017.\n[5] S. Mansﬁeld-Devine, “Open source and the internet of things,” Network\nSecurity, vol. 2018, no. 2, pp. 14–19, 2018.\n[6] M. Min, X. Wan, L. Xiao, Y. Chen, M. Xia, D. Wu, and H. Dai,\n“Learning-based privacy-aware ofﬂoading for healthcare IoT with\nenergy harvesting,” IEEE Internet of Things Journal, vol. 6, no. 3,\npp. 4307–4316, 2018.\n12\nACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\n[7] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[8] F. L. Lewis and D. Vrabie, “Reinforcement learning and adaptive dy-\nnamic programming for feedback control,” IEEE Circuits and Systems\nMagazine, vol. 9, no. 3, pp. 32–50, 2009.\n[9] W. Schultz, P. Dayan, and P. R. Montague, “A neural substrate of\nprediction and reward,” Science, vol. 275, no. 5306, pp. 1593–1599,\n1997.\n[10] A. G. Barto and S. Mahadevan, “Recent advances in hierarchical\nreinforcement learning,” Discrete event dynamic systems, vol. 13, no. 1-\n2, pp. 41–77, 2003.\n[11] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\ndata with neural networks,” science, vol. 313, no. 5786, pp. 504–507,\n2006.\n[12] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning,\npp. 1928–1937, 2016.\n[13] S. Dreyfus, “Richard bellman on the birth of dynamic programming,”\nOperations Research, vol. 50, no. 1, pp. 48–51, 2002.\n[14] V. Franc¸ois-Lavet, P. Henderson, R. Islam, M. G. Bellemare, J. Pineau,\net al., “An introduction to deep reinforcement learning,” Foundations\nand Trends® in Machine Learning, vol. 11, no. 3-4, pp. 219–354, 2018.\n[15] S. P. K. Spielberg, R. B. Gopaluni, and P. D. Loewen, “Deep re-\ninforcement learning approaches for process control,” in 2017 6th\nInternational Symposium on Advanced Control of Industrial Processes\n(AdCONIP), pp. 201–206, 2017.\n[16] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, “Resource man-\nagement with deep reinforcement learning,” in Proceedings of the 15th\nACM Workshop on Hot Topics in Networks, pp. 50–56, 2016.\n[17] Y. Zhang, J. Yao, and H. Guan, “Intelligent cloud resource management\nwith deep reinforcement learning,” IEEE Cloud Computing, vol. 4,\nno. 6, pp. 60–69, 2017.\n[18] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot,\nN. Heess, T. Roth¨orl, T. Lampe, and M. Riedmiller, “Leveraging\ndemonstrations for deep reinforcement learning on robotics problems\nwith sparse rewards,” arXiv preprint arXiv:1707.08817, 2017.\n[19] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learn-\ning for robotic manipulation with asynchronous off-policy updates,”\nin 2017 IEEE international conference on robotics and automation\n(ICRA), pp. 3389–3396, IEEE, 2017.\n[20] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, pp. 529–533, 2015.\n[21] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot, et al., “Mastering the game of go with deep neural\nnetworks and tree search,” nature, vol. 529, no. 7587, p. 484, 2016.\n[22] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforce-\nment learning,” arXiv preprint arXiv:1509.02971, 2015.\n[23] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning,” in Thirtieth AAAI conference on artiﬁcial\nintelligence, 2016.\n[24] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and\nN. De Freitas, “Dueling network architectures for deep reinforcement\nlearning,” arXiv preprint arXiv:1511.06581, 2015.\n[25] I. Kachalsky, I. Zakirzyanov, and V. Ulyantsev, “Applying reinforce-\nment learning and supervised learning techniques to play hearthstone,”\nin 2017 16th IEEE International Conference on Machine Learning and\nApplications (ICMLA), pp. 1145–1148, 2017.\n[26] M. H. Ling, K.-L. A. Yau, J. Qadir, G. S. Poh, and Q. Ni, “Application\nof reinforcement learning for security enhancement in cognitive radio\nnetworks,” Applied Soft Computing, vol. 37, pp. 809–829, 2015.\n[27] P.\nGarcia-Teodoro,\nJ.\nDiaz-Verdejo,\nG.\nMaci´a-Fern´andez,\nand\nE. V´azquez, “Anomaly-based network intrusion detection: Techniques,\nsystems and challenges,” computers & security, vol. 28, no. 1-2, pp. 18–\n28, 2009.\n[28] S. Dua and X. Du, Data mining and machine learning in cybersecurity.\nCRC press, 2016.\n[29] A. L. Buczak and E. Guven, “A survey of data mining and machine\nlearning methods for cyber security intrusion detection,” IEEE Com-\nmunications surveys & tutorials, vol. 18, no. 2, pp. 1153–1176, 2015.\n[30] D. S. Berman, A. L. Buczak, J. S. Chavis, and C. L. Corbett, “A survey\nof deep learning methods for cyber security,” Information, vol. 10,\nno. 4, p. 122, 2019.\n[31] S. K. Biswas, “Intrusion detection using machine learning: A compar-\nison study,” International Journal of Pure and Applied Mathematics,\nvol. 118, no. 19, pp. 101–114, 2018.\n[32] Y. Xin, L. Kong, Z. Liu, Y. Chen, Y. Li, H. Zhu, M. Gao, H. Hou,\nand C. Wang, “Machine learning and deep learning methods for\ncybersecurity,” IEEE Access, vol. 6, pp. 35365–35381, 2018.\n[33] N. Milosevic, A. Dehghantanha, and K.-K. R. Choo, “Machine learning\naided android malware classiﬁcation,” Computers & Electrical Engi-\nneering, vol. 61, pp. 266–274, 2017.\n[34] S. KP et al., “A short review on applications of deep learning for cyber\nsecurity,” arXiv preprint arXiv:1812.06292, 2018.\n[35] M. Rege and R. B. K. Mbah, “Machine learning for cyber defense and\nattack,” DATA ANALYTICS 2018, p. 83, 2018.\n[36] D. Ding, Q.-L. Han, Y. Xiang, X. Ge, and X.-M. Zhang, “A survey\non security control and attack detection for industrial cyber-physical\nsystems,” Neurocomputing, vol. 275, pp. 1674–1683, 2018.\n[37] M. Wu, Z. Song, and Y. B. Moon, “Detecting cyber-physical attacks in\ncybermanufacturing systems with machine learning methods,” Journal\nof intelligent manufacturing, vol. 30, no. 3, pp. 1111–1123, 2019.\n[38] L. Xiao, X. Wan, X. Lu, Y. Zhang, and D. Wu, “Iot security techniques\nbased on machine learning: How do iot devices use ai to enhance\nsecurity?,” IEEE Signal Processing Magazine, vol. 35, no. 5, pp. 41–\n49, 2018.\n[39] T. T. Nguyen and V. J. Reddi, “Deep reinforcement learning for cyber\nsecurity,” arXiv preprint arXiv:1906.05799, 2019.\n[40] W. Wang, A. Kwasinski, D. Niyato, and Z. Han, “A survey on applica-\ntions of model-free strategy learning in cognitive wireless networks,”\nIEEE Communications Surveys & Tutorials, vol. 18, no. 3, pp. 1717–\n1757, 2016.\n[41] Y. Wang, Z. Ye, P. Wan, and J. Zhao, “A survey of dynamic spectrum\nallocation based on reinforcement learning algorithms in cognitive\nradio networks,” Artiﬁcial Intelligence Review, vol. 51, no. 3, pp. 493–\n506, 2019.\n[42] G. Caminero, M. Lopez-Martin, and B. Carro, “Adversarial envi-\nronment reinforcement learning algorithm for intrusion detection,”\nComputer Networks, vol. 159, pp. 96–109, 2019.\n[43] X. Li, R. Lu, X. Liang, X. Shen, J. Chen, and X. Lin, “Smart\ncommunity: an internet of things application,” IEEE Communications\nmagazine, vol. 49, no. 11, pp. 68–75, 2011.\n[44] I. Andrea, C. Chrysostomou, and G. Hadjichristoﬁ, “Internet of things:\nSecurity vulnerabilities and challenges,” in 2015 IEEE Symposium on\nComputers and Communication (ISCC), pp. 180–187, IEEE, 2015.\n[45] R. Roman, J. Zhou, and J. Lopez, “On the features and challenges\nof security and privacy in distributed internet of things,” Computer\nNetworks, vol. 57, no. 10, pp. 2266–2279, 2013.\n[46] F. A. Alaba, M. Othman, I. A. T. Hashem, and F. Alotaibi, “Internet\nof things security: A survey,” Journal of Network and Computer\nApplications, vol. 88, pp. 10–28, 2017.\n[47] S. Alanazi, J. Al-Muhtadi, A. Derhab, K. Saleem, A. N. AlRomi,\nH. S. Alholaibah, and J. J. Rodrigues, “On resilience of wireless\nmesh routing protocol against dos attacks in iot-based ambient assisted\nliving applications,” in 2015 17th International Conference on E-health\nNetworking, Application & Services (HealthCom), pp. 205–210, IEEE,\n2015.\n[48] M. Antonakakis, T. April, M. Bailey, M. Bernhard, E. Bursztein,\nJ. Cochran, Z. Durumeric, J. A. Halderman, L. Invernizzi, M. Kallitsis,\net al., “Understanding the mirai botnet,” in 26th {USENIX} Security\nSymposium ({USENIX} Security 17), pp. 1093–1110, 2017.\n[49] N. Woolf, “Ddos attack that disrupted internet was largest of its kind\nin history, experts say.”\n[50] I. Romdhani, R. Abdmeziem, and D. Tandjaoui, Architecting the\nInternet of Things: State of the Art. 07 2015.\n[51] M. Wu, T.-J. Lu, F.-Y. Ling, J. Sun, and H.-Y. Du, “Research on the\narchitecture of internet of things,” in 2010 3rd International Conference\non Advanced Computer Theory and Engineering (ICACTE), vol. 5,\npp. V5–484, IEEE, 2010.\n[52] P. Wang, S. Chaudhry, L. Li, S. Li, T. Tryfonas, and H. Li, “The internet\nof things: a security point of view,” Internet Research, 2016.\n[53] C.-L. Zhong, Z. Zhu, and R.-G. Huang, “Study on the iot architecture\nand gateway technology,” in 2015 14th International Symposium on\nDistributed Computing and Applications for Business Engineering and\nScience (DCABES), pp. 196–199, IEEE, 2015.\n[54] K. Finkenzeller, “Known attacks on rﬁd systems, possible counter-\nmeasures and upcoming standardisation activities,” in 5th European\nWorkshop on RFID Systems and Technologies, pp. 1–31, 2009.\n[55] A. Mitrokotsa, M. R. Rieback, and A. S. Tanenbaum, “Classiﬁcation\nof rﬁd attacks,” Gen, vol. 15693, no. 14443, p. 14, 2010.\n13\n[56] H.-Y. Chien and C.-W. Huang, “Security of ultra-lightweight rﬁd au-\nthentication protocols and its improvements,” ACM SIGOPS Operating\nSystems Review, vol. 41, no. 4, pp. 83–86, 2007.\n[57] K. Sonar and H. Upadhyay, “A survey: Ddos attack on internet of\nthings,” International Journal of Engineering Research and Develop-\nment, vol. 10, no. 11, pp. 58–63, 2014.\n[58] J. Deng, R. Han, and S. Mishra, “Defending against path-based dos\nattacks in wireless sensor networks,” in Proceedings of the 3rd ACM\nworkshop on Security of ad hoc and sensor networks, pp. 89–96, 2005.\n[59] K. Malialis and D. Kudenko, “Multiagent router throttling: Decentral-\nized coordinated response against ddos attacks,” in Twenty-Fifth IAAI\nConference, 2013.\n[60] K. Malialis and D. Kudenko, “Distributed response to network intru-\nsions using multiagent reinforcement learning,” Engineering Applica-\ntions of Artiﬁcial Intelligence, vol. 41, pp. 270–284, 2015.\n[61] D. K. Yau, J. C. Lui, F. Liang, and Y. Yam, “Defending against\ndistributed denial-of-service attacks with max-min fair server-centric\nrouter throttles,” IEEE/ACM Transactions on Networking, vol. 13,\nno. 1, pp. 29–42, 2005.\n[62] F. Hu, Q. Hao, and K. Bao, “A survey on software-deﬁned network and\nopenﬂow: From concept to implementation,” IEEE Communications\nSurveys & Tutorials, vol. 16, no. 4, pp. 2181–2206, 2014.\n[63] S. Fang, Y. Yu, C. H. Foh, and K. M. M. Aung, “A loss-free\nmultipathing solution for data center network using software-deﬁned\nnetworking approach,” IEEE transactions on magnetics, vol. 49, no. 6,\npp. 2723–2730, 2013.\n[64] Y. Liu, M. Dong, K. Ota, J. Li, and J. Wu, “Deep reinforcement\nlearning based smart mitigation of ddos ﬂooding in software-deﬁned\nnetworks,” in 2018 IEEE 23rd International Workshop on Computer\nAided Modeling and Design of Communication Links and Networks\n(CAMAD), pp. 1–6, IEEE, 2018.\n[65] D. G. Bhoyar and U. Yadav, “Review of jamming attack using\ngame theory,” in 2017 International Conference on Innovations in\nInformation, Embedded and Communication Systems (ICIIECS), pp. 1–\n4, 2017.\n[66] K. Grover, A. Lim, and Q. Yang, “Jamming and anti-jamming tech-\nniques in wireless networks: a survey,” International Journal of Ad Hoc\nand Ubiquitous Computing, vol. 17, no. 4, pp. 197–215, 2014.\n[67] S. G. Weber, L. Martucci, S. Ries, and M. M¨uhlh¨auser, “Towards\ntrustworthy identity and access management for the future internet,” in\n4th International Workshop on Trustworthy Internet of People, Things\n& Services, 2010.\n[68] Y. Wu, B. Wang, K. R. Liu, and T. C. Clancy, “Anti-jamming games\nin multi-channel cognitive radio networks,” IEEE journal on selected\nareas in communications, vol. 30, no. 1, pp. 4–15, 2011.\n[69] R. El-Bardan, S. Brahma, and P. K. Varshney, “Power control with\njammer location uncertainty: A game theoretic perspective,” in 2014\n48th Annual Conference on Information Sciences and Systems (CISS),\npp. 1–6, IEEE, 2014.\n[70] M. Cagalj, S. Capkun, and J.-P. Hubaux, “Wormhole-based antijam-\nming techniques in sensor networks,” IEEE transactions on Mobile\nComputing, vol. 6, no. 1, pp. 100–114, 2006.\n[71] B. Wang, Y. Wu, K. R. Liu, and T. C. Clancy, “An anti-jamming\nstochastic game for cognitive radio networks,” IEEE journal on selected\nareas in communications, vol. 29, no. 4, pp. 877–889, 2011.\n[72] N. Namvar, W. Saad, N. Bahadori, and B. Kelley, “Jamming in the\ninternet of things: A game-theoretic perspective,” in 2016 IEEE Global\nCommunications Conference (GLOBECOM), pp. 1–6, 2016.\n[73] R. D. Pietro and G. Oligeri, “Jamming mitigation in cognitive radio\nnetworks,” IEEE Network, vol. 27, no. 3, pp. 10–15, 2013.\n[74] H. A. Bany Salameh, S. Almajali, M. Ayyash, and H. Elgala, “Spectrum\nassignment in cognitive radio networks for internet-of-things delay-\nsensitive applications under jamming attacks,” IEEE Internet of Things\nJournal, vol. 5, no. 3, pp. 1904–1913, 2018.\n[75] J. Heo, J.-J. Kim, S. Bahk, and J. Paek, “Dodge-jam: Anti-jamming\ntechnique for low-power and lossy wireless networks,” in 2017 14th\nAnnual IEEE International Conference on Sensing, Communication,\nand Networking (SECON), pp. 1–9, IEEE, 2017.\n[76] S. Kim, “Cognitive radio anti-jamming scheme for security provision-\ning iot communications.,” KSII Transactions on Internet & Information\nSystems, vol. 9, no. 10, 2015.\n[77] D. B. Rawat and M. Song, “Securing space communication systems\nagainst reactive cognitive jammer,” in 2015 IEEE Wireless Communi-\ncations and Networking Conference (WCNC), pp. 1428–1433, IEEE,\n2015.\n[78] S. Djuraev, J.-G. Choi, K.-S. Sohn, and S. Y. Nam, “Channel hopping\nscheme to mitigate jamming attacks in wireless lans,” EURASIP\nJournal on Wireless Communications and Networking, vol. 2017, no. 1,\np. 11, 2017.\n[79] J. Becker, “Dynamic beamforming optimization for anti-jamming and\nhardware fault recovery,” 2014.\n[80] Y. Chen, Y. Li, D. Xu, and L. Xiao, “Dqn-based power control\nfor iot transmission against jamming,” in 2018 IEEE 87th Vehicular\nTechnology Conference (VTC Spring), pp. 1–5, 2018.\n[81] G. Han, L. Xiao, and H. V. Poor, “Two-dimensional anti-jamming\ncommunication based on deep reinforcement learning,” in 2017 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 2087–2091, 2017.\n[82] A. A. Khan, M. H. Rehmani, and A. Rachedi, “Cognitive-radio-\nbased internet of things: Applications, architectures, spectrum related\nfunctionalities, and future research directions,” IEEE wireless commu-\nnications, vol. 24, no. 3, pp. 17–25, 2017.\n[83] L. Xiao, X. Wan, W. Su, Y. Tang, et al., “Anti-jamming underwater\ntransmission with mobility and learning,” IEEE Communications Let-\nters, vol. 22, no. 3, pp. 542–545, 2018.\n[84] X. Liu, Y. Xu, L. Jia, Q. Wu, and A. Anpalagan, “Anti-jamming com-\nmunications using spectrum waterfall: A deep reinforcement learning\napproach,” IEEE Communications Letters, vol. 22, no. 5, pp. 998–1001,\n2018.\n[85] Y. Li, X. Wang, D. Liu, Q. Guo, X. Liu, J. Zhang, and Y. Xu, “On\nthe performance of deep reinforcement learning-based anti-jamming\nmethod confronting intelligent jammer,” Applied Sciences, vol. 9, no. 7,\np. 1361, 2019.\n[86] S. Machuzak and S. K. Jayaweera, “Reinforcement learning based\nanti-jamming with wideband autonomous cognitive radios,” in 2016\nIEEE/CIC International Conference on Communications in China\n(ICCC), pp. 1–5, 2016.\n[87] M. Bkassiny, S. K. Jayaweera, Y. Li, and K. A. Avery, “Wideband spec-\ntrum sensing and non-parametric signal classiﬁcation for autonomous\nself-learning cognitive radios,” IEEE Transactions on Wireless Com-\nmunications, vol. 11, no. 7, pp. 2596–2605, 2012.\n[88] M. A. Aref, S. K. Jayaweera, and S. Machuzak, “Multi-agent reinforce-\nment learning based cognitive anti-jamming,” in 2017 IEEE Wireless\nCommunications and Networking Conference (WCNC), pp. 1–6, 2017.\n[89] P. R. Babu, D. L. Bhaskari, and C. Satyanarayana, “A comprehensive\nanalysis of spooﬁng,” International Journal of Advanced Computer\nScience and Applications, vol. 1, no. 6, pp. 157–62, 2010.\n[90] Z. Duan, X. Yuan, and J. Chandrashekar, “Controlling ip spooﬁng\nthrough interdomain packet ﬁlters,” IEEE Transactions on Dependable\nand Secure Computing, vol. 5, no. 1, pp. 22–36, 2008.\n[91] S. Whalen, “An introduction to arp spooﬁng,” Node99 [Online Docu-\nment], April, 2001.\n[92] M. Nawir, A. Amir, N. Yaakob, and O. B. Lynn, “Internet of things\n(iot): Taxonomy of security attacks,” in 2016 3rd International Con-\nference on Electronic Design (ICED), pp. 321–326, 2016.\n[93] F. J. Liu, X. Wang, and H. Tang, “Robust physical layer authentication\nusing inherent properties of channel impulse response,” in 2011-\nMILCOM 2011 Military Communications Conference, pp. 538–542,\nIEEE, 2011.\n[94] J. Liu, L. Xiao, G. Liu, and Y. Zhao, “Active authentication with\nreinforcement learning based on ambient radio signals,” Multimedia\nTools and Applications, vol. 76, no. 3, pp. 3979–3998, 2017.\n[95] L. Xiao, Y. Li, G. Han, G. Liu, and W. Zhuang, “Phy-layer spooﬁng\ndetection with reinforcement learning in wireless networks,” IEEE\nTransactions on Vehicular Technology, vol. 65, no. 12, pp. 10037–\n10047, 2016.\n[96] L. Xiao, W. Zhuang, S. Zhou, and C. Chen, “Learning-based rogue\nedge detection in vanets with ambient radio signals,” in Learning-based\nVANET Communication and Security Techniques, pp. 13–47, Springer,\n2019.\n[97] N. Bezzo, “Predicting malicious intention in cps under cyber-attack,”\nin 2018 ACM/IEEE 9th International Conference on Cyber-Physical\nSystems (ICCPS), pp. 351–352, IEEE, 2018.\n[98] E. Yel, T. X. Lin, and N. Bezzo, “Reachability-based self-triggered\nscheduling and replanning of uav operations,” in 2017 NASA/ESA\nConference on Adaptive Hardware and Systems (AHS), pp. 221–228,\n2017.\n[99] D. Ramachandran and E. Amir, “Bayesian inverse reinforcement learn-\ning.,” in IJCAI, vol. 7, pp. 2586–2591, 2007.\n[100] M. Elnaggar and N. Bezzo, “An irl approach for cyber-physical attack\nintention prediction and recovery,” in 2018 Annual American Control\nConference (ACC), pp. 222–227, IEEE, 2018.\n[101] V. Y. Pillitteri and T. L. Brewer, “Guidelines for smart grid cybersecu-\nrity,” tech. rep., 2014.\n14\nACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\n[102] D. Wang, X. Guan, T. Liu, Y. Gu, Y. Sun, and Y. Liu, “A survey on\nbad data injection attack in smart grid,” in 2013 IEEE PES Asia-Paciﬁc\nPower and Energy Engineering Conference (APPEEC), pp. 1–6, IEEE,\n2013.\n[103] Y. Zhu, J. Yan, Y. Tang, Y. Sun, and H. He, “The sequential attack\nagainst power grid networks,” in 2014 IEEE International Conference\non Communications (ICC), pp. 616–621, IEEE, 2014.\n[104] J. Yan, H. He, X. Zhong, and Y. Tang, “Q-learning-based vulnerability\nanalysis of smart grid against sequential topology attacks,” IEEE\nTransactions on Information Forensics and Security, vol. 12, no. 1,\npp. 200–210, 2016.\n[105] Y. Chen, S. Huang, F. Liu, Z. Wang, and X. Sun, “Evaluation of\nreinforcement learning-based false data injection attack to automatic\nvoltage control,” IEEE Transactions on Smart Grid, vol. 10, no. 2,\npp. 2158–2169, 2018.\n[106] M. N. Kurt, O. Ogundijo, C. Li, and X. Wang, “Online cyber-attack\ndetection in smart grid: A reinforcement learning approach,” IEEE\nTransactions on Smart Grid, vol. 10, no. 5, pp. 5174–5185, 2019.\n[107] J. Loch and S. P. Singh, “Using eligibility traces to ﬁnd the best\nmemoryless policy in partially observable markov decision processes.,”\nin ICML, pp. 323–331, 1998.\n[108] Z. Ni and S. Paul, “A multistage game in smart grid security: A rein-\nforcement learning solution,” IEEE Transactions on Neural Networks\nand Learning Systems, vol. 30, no. 9, pp. 2684–2695, 2019.\n[109] G. Karagiannis, O. Altintas, E. Ekici, G. Heijenk, B. Jarupan, K. Lin,\nand T. Weil, “Vehicular networking: A survey and tutorial on re-\nquirements, architectures, challenges, standards and solutions,” IEEE\ncommunications surveys & tutorials, vol. 13, no. 4, pp. 584–616, 2011.\n[110] N. Alsaffar, H. Ali, and W. Elmedany, “Smart transportation system: A\nreview of security and privacy issues,” in 2018 International Confer-\nence on Innovation and Intelligence for Informatics, Computing, and\nTechnologies (3ICT), pp. 1–4, 2018.\n[111] M. A. Javed, E. Ben Hamida, and W. Znaidi, “Security in intelligent\ntransport systems for smart cities: From theory to practice,” Sensors,\nvol. 16, no. 6, p. 879, 2016.\n[112] L. Xiao, C. Xie, M. Min, and W. Zhuang, “User-centric view of\nunmanned aerial vehicle transmission against smart attacks,” IEEE\nTransactions on Vehicular Technology, vol. 67, no. 4, pp. 3420–3430,\n2018.\n[113] L. Xiao, X. Lu, D. Xu, Y. Tang, L. Wang, and W. Zhuang, “Uav relay\nin vanets against smart jamming with reinforcement learning,” IEEE\nTransactions on Vehicular Technology, vol. 67, no. 5, pp. 4087–4097,\n2018.\n[114] X. Lu, D. Xu, L. Xiao, L. Wang, and W. Zhuang, “Anti-jamming\ncommunication game for uav-aided vanets,” in GLOBECOM 2017-\n2017 IEEE Global Communications Conference, pp. 1–6, IEEE, 2017.\n[115] S. Feng and S. Haykin, “Anti-jamming v2v communication in an\nintegrated uav-cav network with hybrid attackers,” in ICC 2019 - 2019\nIEEE International Conference on Communications (ICC), pp. 1–6,\n2019.\n[116] L. Weng, “The multi-armed bandit problem and its solutions,”\nlilianweng.github.io/lil-log, 2018.\n[117] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of\nthe multiarmed bandit problem,” Machine learning, vol. 47, no. 2-3,\npp. 235–256, 2002.\n[118] A. Lazaric, M. Restelli, and A. Bonarini, “Reinforcement learning in\ncontinuous action spaces through sequential monte carlo methods,” in\nAdvances in neural information processing systems, pp. 833–840, 2008.\n[119] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, “Hier-\narchical deep reinforcement learning: Integrating temporal abstraction\nand intrinsic motivation,” in Advances in neural information processing\nsystems, pp. 3675–3683, 2016.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-02-14",
  "updated": "2021-02-14"
}