{
  "id": "http://arxiv.org/abs/2102.08655v2",
  "title": "Decoding EEG Brain Activity for Multi-Modal Natural Language Processing",
  "authors": [
    "Nora Hollenstein",
    "Cedric Renggli",
    "Benjamin Glaus",
    "Maria Barrett",
    "Marius Troendle",
    "Nicolas Langer",
    "Ce Zhang"
  ],
  "abstract": "Until recently, human behavioral data from reading has mainly been of\ninterest to researchers to understand human cognition. However, these human\nlanguage processing signals can also be beneficial in machine learning-based\nnatural language processing tasks. Using EEG brain activity to this purpose is\nlargely unexplored as of yet. In this paper, we present the first large-scale\nstudy of systematically analyzing the potential of EEG brain activity data for\nimproving natural language processing tasks, with a special focus on which\nfeatures of the signal are most beneficial. We present a multi-modal machine\nlearning architecture that learns jointly from textual input as well as from\nEEG features. We find that filtering the EEG signals into frequency bands is\nmore beneficial than using the broadband signal. Moreover, for a range of word\nembedding types, EEG data improves binary and ternary sentiment classification\nand outperforms multiple baselines. For more complex tasks such as relation\ndetection, further research is needed. Finally, EEG data shows to be\nparticularly promising when limited training data is available.",
  "text": "Decoding EEG Brain Activity for Multi-Modal Natural Language\nProcessing\nNora Hollenstein 1,∗, Cedric Renggli 2, Benjamin Glaus 2, Maria Barrett 3,\nMarius Troendle 4, Nicolas Langer 4 and Ce Zhang 2\n1Department of Nordic Studies and Linguistics, University of Copenhagen\n2Department of Computer Science, ETH Zurich, Switzerland\n3Department of Computer Science, IT University of Copenhagen, Denmark\n4Department of Psychology, University of Zurich, Switzerland\n∗Corresponding author: Nora Hollenstein; nora.hollenstein@hum.ku.dk\nAbstract\nUntil recently, human behavioral data from reading has mainly been of interest to researchers to\nunderstand human cognition. However, these human language processing signals can also be beneﬁcial\nin machine learning-based natural language processing tasks. Using EEG brain activity for this purpose\nis largely unexplored as of yet. In this paper, we present the ﬁrst large-scale study of systematically\nanalyzing the potential of EEG brain activity data for improving natural language processing tasks,\nwith a special focus on which features of the signal are most beneﬁcial. We present a multi-modal\nmachine learning architecture that learns jointly from textual input as well as from EEG features. We\nﬁnd that ﬁltering the EEG signals into frequency bands is more beneﬁcial than using the broadband\nsignal. Moreover, for a range of word embedding types, EEG data improves binary and ternary\nsentiment classiﬁcation and outperforms multiple baselines. For more complex tasks such as relation\ndetection, only the contextualized BERT embeddings outperform the baselines in our experiments,\nwhich raises the need for further research. Finally, EEG data shows to be particularly promising when\nlimited training data is available.\nKeywords: EEG, frequency bands, brain activity, physiological data, natural language processing, machine\nlearning, multimodal learning, neural networks\n1\nIntroduction\nRecordings of brain activity play an important role in furthering our understanding of how human language works\n(Ling, Lee, Armstrong, & Nestor, 2019; Murphy, Wehbe, & Fyshe, 2018). The appeal and added value of using\nbrain activity signals in linguistic research are intelligible (Stemmer & Connolly, 2012). Computational language\nprocessing models still struggle with basic linguistic phenomena that humans perform eﬀortlessly (Ettinger,\n2020). Combining insights from neuroscience and artiﬁcial intelligence will take us closer to human-level language\nunderstanding (McClelland, Hill, Rudolph, Baldridge, & Sch¨utze, 2020). Moreover, numerous datasets of cognitive\nprocessing signals in naturalistic experiment paradigms with real-world language understanding tasks are becoming\navailable (Alday, 2019; Kandylaki & Bornkessel-Schlesewsky, 2019).\nLinzen (2020) advocates for the grounding of NLP models in multi-modal settings to compare the generalization\nabilities of the models to human language learning. Multi-modal learning in machine learning refers to algorithms\nlearning from multiple input modalities encompassing various aspects of communication. Developing models that\n1\narXiv:2102.08655v2  [cs.CL]  13 Jul 2021\nlearn from such multi-modal inputs eﬃciently is crucial to advance the generalization capabilities of state-of-the-art\nNLP models. Bisk et al. (2020) posit that text-only training seems to be reaching the point of diminishing\nreturns and the next step in the development of NLP is leveraging multi-modal sources of information. Leveraging\nelectroencephalography (EEG) and other physiological and behavioral signals seem especially appealing to model\nmulti-modal human-like learning processes. Although combining diﬀerent modalities or types of information for\nimproving performance seems intuitively appealing, in practice, it is challenging to combine the varying level of\nnoise and conﬂicts between modalities (Morency & Baltruˇsaitis, 2017). Therefore, we investigate if and how we\ntake advantage of electrical brain activity signals to provide a human inductive bias for these natural language\nprocessing (NLP) models.\nTwo popular NLP tasks are sentiment analysis and relation detection. The goal of both tasks is to automatically\nextract information from text. Sentiment analysis is the task of identifying and categorizing subjective information\nin text. For example, the sentence “This movie is great fun.” contains a positive sentiment, while the sentence\n“This movie is terribly boring.” contains a negative sentiment. Relation detection is the task of identifying semantic\nrelationships between entities in the text. In the sentence “Albert Einstein was born in Ulm.”, the relation Birth\nPlace holds between the entities “Albert Einstein” and “Ulm”. NLP researchers have made great progress in\nbuilding computational models for these tasks (Barnes, Klinger, & im Walde, 2017; Rotsztejn, Hollenstein, &\nZhang, 2018). However, these machine learning (ML) models still lack core human language understanding skills\nthat humans perform eﬀortlessly (Barnes, Velldal, & Øvrelid, 2020; Poria, Hazarika, Majumder, & Mihalcea, 2020).\nBarnes, Øvrelid, and Velldal (2019) ﬁnd that sentiment models struggle with diﬀerent linguistic elements such as\nnegations or sentences containing mixed sentiment towards several target aspects.\nLeveraging Physiological Data for Natural Language Processing\nThe increasing wealth of literature on the cognitive neuroscience of language (see reviews by Friederici, 2000;\nPoeppel, 2014; Poeppel, Emmorey, Hickok, & Pylkk¨anen, 2012) enables the use of cognitive signals in applied\nﬁelds of language processing (e.g., Armeni, Willems, & Frank, 2017). In recent years, natural language processing\nresearchers have increasingly leveraged human language processing signals from physiological and neuroimaging\nrecordings for both augmenting and evaluating machine learning-based NLP models (e.g., Artemova, Bakarov,\nArtemov, Burnaev, & Sharaev, 2020; Hollenstein, de la Torre, Langer, & Zhang, 2019; Toneva & Wehbe, 2019).\nThe approaches taken in those studies can be categorized as encoding or decoding cognitive processing signals.\nEncoding and decoding are complementary operations: encoding uses stimuli to predict brain activity, while\ndecoding uses the brain activity to predict information about the stimuli (Naselaris, Kay, Nishimoto, & Gallant,\n2011). In the present study, we focus on the decoding process for predicting information about the text input from\nhuman brain activity.\nUntil now, mostly eye tracking and functional magnetic resonance imaging (fMRI) signals have been leveraged\nfor this purpose (e.g., Fyshe, Talukdar, Murphy, & Mitchell, 2014). On the one hand, fMRI recordings provide\ninsights into the brain activity with a high spatial resolution, which furthers the research of localization of language-\nrelated cognitive processes. FMRI features are most often extracted over full sentences or longer text spans, since\nthe extraction of word-level signals is highly complex due to the lower temporal resolution and hemodynamic\ndelay. The number of cognitive processes and noise included in brain activity signals make feature engineering\nchallenging. Machine learning studies leveraging brain activity data rely on standard preprocessing steps such as\nmotion correction and spatial smoothing, and then use data-driven approaches to reduce the number of features,\ne.g., principal component analysis (Beinborn, Abnar, & Choenni, 2019). Schwartz, Toneva, and Wehbe (2019), for\ninstance, ﬁne-tuned a contextualized language model with brain activity data, which yields better predictions of\nbrain activity and does not harm the model’s performance on downstream NLP tasks. On the other hand, eye\ntracking enables us to objectively and accurately record visual behavior with high temporal resolution at low cost.\nEye tracking is widely used in psycholinguistic studies and it is common to extract well-established theory-driven\nfeatures (Barrett, Bingel, Keller, & Søgaard, 2016; Hollenstein, Barrett, & Beinborn, 2020; Mathias, Kanojia,\nMishra, & Bhattacharyya, 2020). These established metrics are derived from a large body of psycholinguistic\nresearch.\nEEG is a non-invasive method to measure electrical brain surface activity. The synchronized activity of\n2\nneurons in the brain produces electrical currents. The resulting voltage ﬂuctuations can be recorded with external\nelectrodes on the scalp. Compared to fMRI and other neuroimaging techniques, EEG can be recorded with a\nvery high temporal resolution. This allows for more ﬁne-grained language understanding experiments on the\nword-level, which is crucial for applications in NLP (Beres, 2017). To isolate certain cognitive functions, EEG\nsignals can be split into frequency bands. For instance, eﬀects related to semantic violations can be found within\nthe gamma frequency range (∼30 −100 Hz), with well-formed sentences showing higher gamma levels than\nsentences containing violations (Penolazzi, Angrilli, & Job, 2009). Due to the wide extent of cognitive processes\nand the low signal-to-noise ratio in the EEG data, it is very challenging to isolate speciﬁc cognitive processes, so\nthat more and more researchers are relying on machine learning techniques to decode the EEG signals (Aﬀolter,\nEgressy, Pascual, & Wattenhofer, 2020; Pfeiﬀer, Hollenstein, Zhang, & Langer, 2020; P. Sun, Anumanchipalli, &\nChang, 2020). These challenges are the decisive factors why EEG has not yet been used for NLP tasks. Data-driven\napproaches combined with the possibility of naturalistic reading experiments are now bypassing these challenges.\nReading times of words in a sentence depend on the amount of information the words convey. This correlation\ncan be observed in eye tracking data, but also in EEG data (Frank, Otten, Galli, & Vigliocco, 2015). Thus,\neye tracking and EEG are complementary measures of cognitive load. Compared to eye tracking, EEG may be\nmore cumbersome to record and requires more expertise. Nevertheless, while eye movements indirectly reﬂect\nthe cognitive load of text processing, EEG contains more direct and comprehensive information about language\nprocessing in the human brain. As we show below, this is beneﬁcial for the higher level semantic NLP tasks\ntargeted in this work. For instance, word predictability and semantic similarity show distinct patterns of brain\nactivity during language comprehension (Ettinger, 2020; Frank & Willems, 2017). The word representations used\nby neural networks and brain activity observed via the process of subjects reading a story can be aligned (Wehbe,\nVaswani, Knight, & Mitchell, 2014). Moreover, EEG eﬀects that reﬂect syntactical processes can also be found in\ncomputational models of grammar (Hale, Dyer, Kuncoro, & Brennan, 2018).\nThe co-registration of EEG and eye-tracking has become an important tool for studying the temporal dynamics\nof naturalistic reading (Dimigen, Sommer, Hohlfeld, Jacobs, & Kliegl, 2011; Hollenstein et al., 2018; Sato &\nMizuhara, 2018). This methodology has been increasingly and successfully used to study EEG correlates in the\ntime domain (i.e., event-related potentials, ERPs) of cognitive processing in free viewing situations such as reading\n(Degno, Loberg, & Liversedge, 2021). In this context, ﬁxation-related potentials (FRPs), which are the evoked\nelectrical responses time-locked to the onset of ﬁxations, have been studied and have received broad interest\nby naturalistic imaging researchers for free viewing studies. In naturalistic reading paradigms, FRPs allow the\nstudy of the neural dynamics of how novel information from currently ﬁxated text aﬀects the ongoing language\ncomprehension process.\nAs of yet, the related work relying on EEG signals for NLP is very limited. Sassenhagen and Fiebach (2020)\nﬁnd that word embeddings can successfully predict the pattern of neural activation. However, their experiment\ndesign does not include natural reading, but reading isolated words. Hollenstein, de la Torre, et al. (2019) similarly\nﬁnd that various embedding type are able to predict aggregate word-level activations from natural reading, where\ncontextualized embeddings perform best. Moreover, Murphy and Poesio (2010) showed that semantic categories\ncan be detected in simultaneous EEG recordings. Muttenthaler, Hollenstein, and Barrett (2020) used EEG signals\nto train an attention mechanism, similar to Barrett, Bingel, Hollenstein, Rei, and Søgaard (2018), who used eye\ntracking signals to induce machine attention with human attention. However, EEG has not yet been leveraged\nfor higher-level semantic tasks such as sentiment analysis or relation detection. Deep learning techniques have\nbeen applied to decode EEG signals (Craik, He, & Contreras-Vidal, 2019), especially for brain-computer interface\ntechnologies, e.g., Nurse et al. (2016). However, this avenue has not yet been explored when leveraging EEG\nsignals to enhance NLP models. Through decoding EEG signals occurring during language understanding, more\nspeciﬁcally, during English sentence comprehension, we aim to explore their impact on computational language\nunderstanding tasks.\nContributions\nMore than a practical application of improving real-world NLP tasks, our main goal is to explore to what extent\nthere is additional linguistic processing information in the EEG signal to complement the text input. In this\n3\npresent study, we investigate for the ﬁrst time the potential of leveraging EEG signals for augmenting NLP\nmodels. For the purpose of making language decoding studies from brain activity more interpretable, we follow\nthe recommendations of Gauthier and Ivanova (2018): (1) We commit to a speciﬁc mechanism and task, and (2)\nsubdivide the input feature space including theoretically founded preprocessing steps. We investigate the impact of\nenhancing a neural network architecture for two common NLP tasks with a range of EEG features. We propose a\nmulti-modal network capable of processing textual features and brain activity features simultaneously. We employ\ntwo diﬀerent well-established types of neural network architectures for decoding the EEG signals throughout the\nentire study. To analyze the impact of diﬀerent EEG features, we perform experiments on sentiment analysis as a\nbinary or ternary sentence classiﬁcation task, and relation detection as a multi-class and multi-label classiﬁcation\ntask. We investigate the eﬀect of augmenting NLP models with neurophysiolgical data in an extensive study while\naccounting for various dimensions:\n1. We present a comparison of a purely data-driven approach of feature extraction for machine learning, using\nfull broadband EEG signals, to a more theoretically motivated approach, splitting the word-level EEG\nfeatures into frequency bands.\n2. We develop two EEG decoding components for our multi-modal ML architecture: A recurrent and a\nconvolutional component.\n3. We contrast the eﬀects of these EEG features on multiple word representation types commonly used in NLP.\nWe compare the improvements of EEG features as a function of various training data sizes.\n4. We analyze the impact of the EEG features on varying classiﬁcation complexity: from binary classiﬁcation\nto multi-class and multi-label tasks.\nThis comprehensive study is completed by comparing the impact of the decoded EEG signals not only to a text-only\nbaseline, but also to baselines augmented with eye tracking data as well as random noise. In the next section,\nwe describe the materials used in this study and the multi-modal machine learning architecture. Thereafter, we\npresent the results of the NLP tasks and discuss the dimensions deﬁned above.\n2\nMaterials and Methods\n2.1\nData\nFor the purpose of augmenting natural language processing tasks with brain activity signals, we leverage the\nZurich Cognitive Language Processing Corpus (ZuCo; Hollenstein et al., 2018; Hollenstein, Troendle, Zhang, &\nLanger, 2020). ZuCo is an openly available dataset of simultaneous EEG and eye tracking data from subjects\nreading naturally occurring English sentences. This corpus consists of two datasets, ZuCo 1.0 and ZuCo 2.0,\nwhich contain the same type of recordings. We select the normal reading paradigms from both datasets, in which\nparticipants were instructed to read English sentences in their own pace with no speciﬁc task beyond reading\ncomprehension. The participants read one sentence at a time, using a control pad to move to the next sentence.\nThis setup facilitated the naturalistic reading paradigm. Descriptive statistics about the datasets used in this work\nare presented in Table 1.\nA detailed description of the entire ZuCo dataset, including individual reading speed, lexical performance,\naverage word length, average number of words per sentence, skipping proportion on word level, and eﬀect of word\nlength on skipping proportion, can be found in Hollenstein et al. (2018). In the following section, we will describe\nthe methods relevant to the subset of the ZuCo data used in the present study.\n2.1.1\nParticipants\nFor ZuCo 1.0, data were recorded from 12 healthy adults (between 22 and 54 years old; all right-handed; 5 female\nsubjects). For ZuCo 2.0, data were recorded from 18 healthy adults (between 23 and 52 years old; 2 left-handed; 10\nfemale subjects). The native language of all participants is English, originating from Australia, Canada, UK, USA\nor South Africa. In addition, all subjects completed the standardized LexTALE test to assess their vocabulary and\nlanguage proﬁciency (Lexical Test for Advanced Learners of English; Lemh¨ofer & Broersma, 2012). All participants\n4\nZuCo 1.0\nZuCo 1.0\nZuCo 2.0\nTask SR\nTask NR\nTask NR\nParticipants\n12\n12\n18\nSentences\n400\n300\n349\nWords\n7,079\n6,386\n6,828\nUnique word types\n3,080\n2,657\n2,412\nSentiment Analysis\n✓\n-\n-\nRelation Detection\n-\n✓\n✓\nTable 1: Details about the ZuCo tasks used in this paper. In Task SR participants read sentences from\nmovie reviews, and in Task NR sentences from Wikipedia articles.\ngave written consent for their participation and the re-use of the data prior to the start of the experiments. The\nstudy was approved by the Ethics Commission of the University of Zurich.\n2.1.2\nReading Materials & Experimental Design\nThe reading materials recorded for the ZuCo corpus contain sentences from movie reviews from the Stanford\nSentiment Treebank (Socher et al., 2013) and Wikipedia articles from a dataset provided by Culotta, McCallum,\nand Betz (2006). These resources were chosen since they provide ground truth labels for the machine learning\ntasks in this work. Table 2 presents a few examples of the sentences read during the experiments.\nFor the recording sessions, the sentences were presented one at a time at the same position on the screen. Text\nwas presented in black with font size 20-point Arial on a light grey background resulting in a letter height of 0.8\nmm or 0.674°. The lines were triple-spaced, and the words double-spaced. A maximum of 80 letters or 13 words\nwere presented per line in all three tasks. Long sentences spanned multiple lines. A maximum of 7 lines for Task 1,\n5 lines for Task 2 and 7 lines for Task 3 were presented simultaneously on the screen.\nDuring the normal reading tasks included in the ZuCo corpus, the participants were instructed to read the sentences\nnaturally, without any speciﬁc task other than comprehension. Participants were told to read the sentences\nnormally without any special instructions. Participants were equipped with a control to trigger the onset of the\nnext sentence. The task was explained to the subjects orally, followed by instructions on the screen.\nThe control condition for this task consisted of single-choice reading comprehension questions about the content\nof the previous sentence. 12% of randomly selected sentences were followed by a control question on a separate\nscreen. To test the participants’ reading comprehension, these questions ask about the content of the previous\nsentence. The questions are presented with three answer options, out of which only one is correct.\n2.1.3\nEEG Data\nIn this section, we present the EEG data extracted from the ZuCo corpus for this work. We describe the acquisition\nand preprocessing procedures as well as the feature extraction.\nEEG Acquisition and Preprocessing\nHigh-density EEG data were recorded using a 128-channel EEG\nGeodesic Hydrocel system (Electrical Geodesics, Eugene, Oregon) with a sampling rate of 500 Hz. The recording\nreference was at Cz (vertex of the head), and the impedances were kept below 40 kΩ. All analyses were performed\nusing MATLAB 2018b (The MathWorks, Inc., Natick, Massachusetts, United States). EEG data was automatically\npreprocessed using the current version (2.4.3) of Automagic (Pedroni, Bahreini, & Langer, 2019). Automagic is an\nopen-source MATLAB toolbox that acts as a wrapper to run currently available EEG preprocessing methods and\noﬀers objective standardized quality assessment for large studies. The code for the preprocessing can be found\nonline.1\nOur preprocessing pipeline consisted of the following steps. First, 13 of the 128 electrodes in the outermost\ncircumference (chin and neck) were excluded from further processing as they capture little brain activity and\nmainly record muscular activity. Additionally, 10 EOG electrodes were used for blink and eye movement detection\n1https://github.com/methlabUZH/automagic\n5\n(and subsequent rejection) during ICA. The EOG electrodes were removed from the data after the preprocessing,\nyielding a total number of 105 EEG electrodes. Subsequently, bad channels were detected by the algorithms\nimplemented in the EEGLAB plugin clean rawdata2, which removes ﬂatline, low-frequency, and noisy channels.\nA channel was deﬁned as a bad electrode when recorded data from that electrode was correlated at less than 0.85\nto an estimate based on neighboring channels. Furthermore, a channel was deﬁned as bad if it had more line noise\nrelative to its signal than all other channels (4 standard deviations). Finally, if a channel had a longer ﬂat-line than\n5 seconds, it was considered bad. These bad channels were automatically removed and later interpolated using a\nspherical spline interpolation (EEGLAB function eeg interp.m). The interpolation was performed as a ﬁnal step\nbefore the automatic quality assessment of the EEG ﬁles (see below). Next, data was ﬁltered using a high-pass\nﬁlter (-6dB cut-oﬀ: 0.5 Hz) and a 60 Hz notch ﬁlter was applied to remove line noise artifacts. Thereafter, an\nindependent component analysis (ICA) was performed. Components reﬂecting artifactual activity were classiﬁed\nby the pre-trained classiﬁer MARA (Winkler, Haufe, & Tangermann, 2011). MARA is a supervised machine\nlearning algorithm that learns from expert ratings. Therefore, MARA is not limited to a speciﬁc type of artifact,\nand should be able to handle eye artifacts, muscular artifacts and loose electrodes equally well. Each component\nbeing classiﬁed with a probability rating > 0.5 for any class of artifacts was removed from the data. Finally,\nresidual bad channels were excluded if their standard deviation exceeded a threshold of 25 µV. After this, the\npipeline automatically assessed the quality of the resulting EEG ﬁles based on four criteria: A data ﬁle was\nmarked as bad-quality EEG and not included in the analysis if (1) the proportion of high-amplitude data points in\nthe signals (> 30 µV) was larger than 0.20; (2) more than 20% of time points showed a variance larger than 15\nmicrovolts across channels; (3) 30% of the channels showed high variance (> 15 µV); (4) the ratio of bad channels\nwas higher than 0.3.\nFree viewing in reading is an important characteristic of naturalistic behavior and imposes challenges for\nthe analysis of electrical brain activity data. Free viewing in the context of our study refers to the participant’s\nability to perform self-paced reading given the experimental requirement to keep the head still during data\nrecording. In the case of EEG recordings during naturalistic reading, the self-paced timing of eye ﬁxations leads to\na temporal overlap between successive ﬁxation-related events (Dimigen et al., 2011). In order to isolate the signals\nof interest and correct for temporal overlap in the continuous EEG, several methods using linear-regression-based\ndeconvolution modeling have been proposed for estimating the overlap-corrected underlying neural responses to\nevents of diﬀerent types (e.g., Ehinger and Dimigen 2019; Smith and Kutas 2015a, 2015b). Here, we used the unfold\ntoolbox for MATLAB (Ehinger & Dimigen, 2019).3 Deconvolution modeling is based on the assumption that in\neach channel the recorded signal consists of a combination of time-varying and partially overlapping event-related\nresponses and random noise. Thus, the model estimates the latent event-related responses to each type of event\nbased on repeated occurrences of the event over time, in our case eye ﬁxations.\nEEG Features\nThe fact that ZuCo provides simultaneous EEG and eye tracking data highly facilitates the\nextraction of word-level brain activity signals. Dimigen et al. (2011) demonstrated that EEG indices of semantic\nprocessing can be obtained in natural reading and compared to eye movement behavior. The eye tracking data\nprovides millisecond-accurate ﬁxation times for each word. Therefore, we were able to obtain the brain activity\nduring all ﬁxations of a word by computing ﬁxation-related potentials aligned to the onsets of the ﬁxation on a\ngiven word.\nIn this work, we select a range of EEG features with a varying degree of theory-driven and data-driven feature\nextraction. We deﬁne the broadband EEG signal, i.e., the full EEG signal from 0.5-50 Hz as the averaged brain\nactivity over all ﬁxations of a word, i.e., its total reading time. We compare the full EEG features, a data-driven\nfeature extraction approach, to frequency band features, a more theoretically motivated approach. Diﬀerent\nneurocognitive aspects of language processing during reading are associated with brain oscillations at various\nfrequencies. These frequency ranges are known to be associated with certain cognitive functions. We split the\nEEG signal into four frequency bands to limit the bandwidth of the EEG signals to be analyzed. The frequency\nbands are ﬁxed ranges of wave frequencies and amplitudes over a time scale: theta (4-8 Hz), alpha (8.5-13 Hz),\nbeta (13.5-30 Hz), and gamma (30.5-49.5 Hz). We elaborate on cognitive and linguistic functions of each of these\n2http://sccn.ucsd.edu/wiki/Plugin list process\n3https://github.com/unfoldtoolbox/unfold/\n6\nTask\nExample Sentence\nLabel(s)\nBinary/ternary sentiment analysis\n“The ﬁlm often achieves a mesmerizing poetry.”\nPositive\nBinary/ternary sentiment analysis\n“Flaccid drama and exasperatingly slow journey.”\nNegative\nTernary sentiment analysis\n“A portrait of an artist.”\nNeutral\nRelation detection\n“He attended Wake Forest University.”\nEducation\nRelation detection\n“She attended Beverly Hills High School, but\nleft to become an actress.”\nEducation,\nJob Title\nTable 2: Example sentences for all three NLP tasks used in this study.\nfrequency bands in Section 4.1.\nWe then applied a Hilbert transform to each of these time-series, resulting in a complex time-series. The Hilbert\nphase and amplitude estimation method yields results equivalent to sliding window FFT and wavelet approaches\n(Bruns, 2004). We speciﬁcally chose the Hilbert transformation to maintain temporal information for the amplitude\nof the frequency bands to enable the power of the diﬀerent frequencies for time segments deﬁned through ﬁxations\nfrom the eye-tracking recording. Thus, for each eye-tracking feature we computed the corresponding EEG feature\nin each frequency band. For each EEG eye-tracking feature, all channels were subject to an artifact rejection\ncriterion of ±90 µV to exclude trials with transient noise.\nIn spite of the high inter-subject variability in EEG data, it has been shown in previous research of machine\nlearning applications (Foster, Dharmaretnam, Xu, Fyshe, & Tzanetakis, 2018; Hollenstein, Barrett, et al., 2019),\nthat averaging over the EEG features of all subjects yields results almost as good as the single best-performing\nsubjects. Hence, we also average the EEG features over all subjects to obtain more robust features. Finally, for\neach word in each sentence, the EEG features consist of a vector of 105 dimensions (one value for each EEG\nchannel). For training the ML models, we split all available sentences into sets of 80% for training and 20% for\ntesting to ensure that the test data is unseen during training.\n2.1.4\nEye Tracking Data\nIn the following, we describe the eye tracking data recorded for the Zurich Cognitive Language Processing Corpus.\nIn this study, we focus on decoding EEG data, but we use eye movement data to compute an additional baseline.\nAs mentioned previously, augmenting ML models with eye tracking yields consistent improvements across a range\nof NLP tasks, including sentiment analysis and relation extraction (Hollenstein, Barrett, et al., 2019; Long, Lu,\nXiang, Li, & Huang, 2017; Mishra, Kanojia, Nagar, Dey, & Bhattacharyya, 2017). Since the ZuCo datasets provide\nsimultaneous EEG and eye tracking recordings, we leverage the available eye tracking data to augment all NLP\ntasks with eye tracking features as an additional multi-modal baseline based on cognitive processing features.\nEye Tracking Acquisition and Preprocessing\nEye movements were recorded with an infrared video-\nbased eye tracker (EyeLink 1000 Plus, SR Research) at a sampling rate of 500 Hz. The EyeLink 1000 tracker\nprocesses eye position data, identifying saccades, ﬁxations and blinks. Fixations were deﬁned as time periods\nwithout saccades during which the visual gaze is ﬁxed on a single location. The data therefore consists of (x,y) gaze\nlocation entries for individual ﬁxations mapped to word boundaries. A ﬁxation lasts around 200–250ms (with large\nvariations). Fixations shorter than 100 ms were excluded, since these are unlikely to reﬂect language processing\n(Sereno & Rayner, 2003). Fixation duration depends on various linguistic eﬀects, such as word frequency, word\nfamiliarity and syntactic category (Clifton, Staub, & Rayner, 2007).\nEye Tracking Features\nThe following features were extracted from the raw data: (1) gaze duration (GD),\nthe sum of all ﬁxations on the current word in the ﬁrst-pass reading before the eye moves out of the word; (2)\ntotal reading time (TRT), the sum of all ﬁxation durations on the current word, including regressions; (3) ﬁrst\nﬁxation duration (FFD), the duration of the ﬁrst ﬁxation on the prevailing word; (4) go-past time (GPT), the sum\n7\nof all ﬁxations prior to progressing to the right of the current word, including regressions to previous words that\noriginated from the current word; (5) number of ﬁxations (nFix), the total amount of ﬁxations on the current word.\nWe use these ﬁve features provided in the ZuCo dataset, which cover the extent of the human reading process.\nTo increase the robustness of the signal, analogously to the EEG features, the eye tracking features are averaged\nover all subjects (Barrett & Hollenstein, 2020). This results in a feature vector of ﬁve dimensions for each word in\na sentence. Training and test data were split in the same fashion as the EEG data.\n2.2\nNatural Language Processing Tasks\nIn this section, we describe the natural language processing tasks we use to evaluate the multi-modal ML models.\nAs usual in supervised machine learning, the goal is to learn a mapping from given input features to an output\nspace to predict the labels as accurately as possible. The tasks we consider in our work do not diﬀer much in the\ninput deﬁnition as they consist of three sequence classiﬁcation tasks for information extraction from text. The goal\nof a sequence classiﬁcation task is to assign the correct label(s) to a given sentence. The input for all tasks consists\nof tokenized sentences, which we augment with additional features, i.e., EEG or eye tracking. The labels to predict\nvary across the three chosen tasks resulting in varying task diﬃculty. Table 2 provides examples for all three tasks.\n2.2.1\nTask 1 and 2: Sentiment Analysis\nThe objective of sentiment analysis is to interpret subjective information in text. More speciﬁcally, we deﬁne\nsentiment analysis as a sentence-level classiﬁcation task. We run our experiments on both binary (positive/negative)\nand ternary (+ neutral) sentiment classiﬁcation. For this task, we leverage only the sentences recorded in the ﬁrst\ntask of ZuCo 1.0, since they are part of the Stanford Sentiment Treebank (Socher et al., 2013), and thus directly\nprovide annotated sentiment labels for training the ML models. For the ﬁrst task, binary sentiment analysis, we\nuse the 263 positive and negative sentences. For the second task, ternary sentiment analysis, we additionally use\nthe neutral sentences, resulting in a total of 400 sentences.\n2.2.2\nTask 3: Relation Detection\nRelation classiﬁcation is the task of identifying the semantic relation holding between two entities in text. The\nZuCo corpus also contains Wikipedia sentences with relation types such as Job Title, Nationality and Political\nAﬃliation. The sentences in ZuCo 1.0 and ZuCo 2.0, from the normal reading experiment paradigms, include\n11 relation types (Figure 1). In order to further increase the task complexity, we treat this task diﬀerently than\nthe sentiment analysis tasks. Since any sentence can include zero, one or more of the relevant semantic relations\n(see example in Table 2, we treat relation detection as a multi-class and multi-label sequence classiﬁcation task.\nConcretely, every sample can be assigned to any possible combination out of the 11 classes including none of\nthem. Removing duplicates between ZuCo 1.0 and ZuCo 2.0 resulted in 594 sentences used for training the models.\nFigure 1 illustrates the label and relation distribution of the sentences used to train the relation detection task.\n2.3\nMulti-Modal Machine Learning Architecture\nWe present a multi-modal neural architecture to augment the NLP sequence classiﬁcation tasks with any other\ntype of data. Although combining diﬀerent modalities or types of information for improving performance seems\nan intuitively appealing task, it is often challenging to combine the varying levels of noise and conﬂicts between\nmodalities in practice.\nPrevious works using physiological data for improving NLP tasks mostly implement early fusion multi-modal\nmethods, i.e., directly concatenating the textual and cognitive embeddings before inputting them into the network.\nFor example, Hollenstein and Zhang (2019), Barrett, Gonz´alez-Gardu˜no, Frermann, and Søgaard (2018) and\nMishra et al. (2017) concatenate textual input features with eye-tracking features to improve NLP tasks such as\nentity recognition, part-of-speech tagging and sentiment analysis, respectively. Concatenating the input features at\nthe beginning in only one joint decoder component aims at learning a joint decoder across all modalities at risk of\nimplicitly learning diﬀerent weights for each modality. However, recent multi-modal machine learning work has\nshown the beneﬁts of late fusion mechanisms (Ramachandram & Taylor, 2017). Do, Nguyen, Tsiligianni, Cornelis,\n8\n0\n20\n40\n60\n80\n100\n120\nNatoinality\nJob Title\nDeath\nWife\nPlotical Affiliation\nBirth Place\nEmployer\nVisited\nFounder\nAwarded\nEducation\nNumber of sentences\n0\n50\n100\n150\n5\n4\n3\n2\n1\n0\nNumber of sentences\nNumber of relations\nFigure 1: (left) Label distribution of the 11 relation types in the relation detection dataset. (right) Number\nof relation types per sentence in the relation detection dataset.\nand Deligiannis (2017) argument in favor of concatenating the hidden layers instead of concatenating the features\nat input time. Such multi-modal models have been successfully applied in other areas, mostly combining inputs\nacross diﬀerent domains, for instance, learning speech reconstruction from silent videos (Ephrat, Halperin, & Peleg,\n2017), or for text classiﬁcation using images (Kiela, Grave, Joulin, & Mikolov, 2018). Tsai et al. (2019) train a\nmulti-modal sentiment analysis model from natural language, facial gestures, and acoustic behaviors.\nHence, we adopted the late fusion strategy in our work. We present multi-modal models for various NLP tasks,\ncombining the learned representations of all input types (i.e., text and EEG features) in a late fusion mechanism\nbefore conducting the ﬁnal classiﬁcation. Purposefully, this enables the model to learn independent decoders for\neach modality before fusing the hidden representations together. In the present study, we investigate the proposed\nmulti-modal machine learning architecture, which learns simultaneously from text and from cognitive data such as\neye tracking and EEG signals.\nIn the following, we ﬁrst describe the uni-modal and multi-modal baseline models we use to evaluate the results.\nThereafter, we present the multi-modal NLP models that jointly learn from text and brain activity data.\n2.3.1\nUni-Modal Text Baselines\nFor each of the tasks presented above, we train uni-modal models on textual features only. To represent the word\nnumerically, we use word embeddings. Word embeddings are vector representations of words, computed so that\nwords with similar meaning have a similar representation. To analyze the interplay between various types of word\nembeddings and EEG data, we use the following three embedding types typically used in practice: (1) randomly\ninitialized embeddings trained at run time on the sentences provided, (2) GloVe pre-trained embeddings based\non word co-occurrence statistics (Pennington, Socher, & Manning, 2014)4, and (3) BERT pre-trained contextual\nembeddings (Devlin, Chang, Lee, & Toutanova, 2019)5.\nThe randomly initialized word representations deﬁne word embeddings as n-by-d matrices, where n is the\nvocabulary size, i.e., the number of unique words in our dataset, and d is the embedding dimension. Each value in\nthat matrix is randomly initialized and will then be trained together with the neural network parameters. We set\nd = 32. This type of embeddings does not beneﬁt from pre-training on large text collections and hence is known to\nperform worse than GloVe or BERT embeddings. We include them in our study to better isolate the impact of the\nEEG features and to limit the learning of the model on the text it is trained on. Non-contextual word embeddings\nsuch as GloVe encode each word in a ﬁxed vocabulary as a vector. The purpose of these vectors is to encode\nsemantic information about a word, such that similar words result in similar embedding vectors. We use the GloVe\nembeddings of d = 300 dimensions that are trained on 6 billion words. The contextualized BERT embeddings were\npre-trained on multiple layers of transformer models with self-attention (Vaswani et al., 2017). Given a sentence,\nBERT encodes each word into a feature vector of dimension d = 768, which incorporates information from the\n4https://nlp.stanford.edu/projects/glove/\n5https://huggingface.co/bert-base-uncased\n9\nInput Layer\nword embeddings\nd1\nd1\nd1\nd1\nword1\nword2\nwords\n…\ne1\ne1\ne1\ne1\nbiLSTMs\nDense Layers\nconcatenate hidden states\nprediction\nDense \nLayer + \nSoftmax\n…\n…\nEEG features\nword1\nword2\nwords\n…\nEEG \nDecoding \nComponent\n…\nFigure 2: The multi-modal machine learning architecture for the EEG-augmented models. Word em-\nbeddings of dimension d are the input for the textual component (yellow); EEG features of dimension\ne for the cognitive component (blue). The text component consists of recurrent layers followed by two\ndense layers with dropout. We test multiple architectures for the EEG component (see Figure 3). Finally,\nthe hidden states of both components are concatenated and followed by a ﬁnal dense layer with softmax\nactivation for classiﬁcation (green).\nword’s context in the sentence.\nThe uni-modal text baseline model consists of a ﬁrst layer taking the embeddings as an input, followed by\na bidirectional Long-Short Term Memory network (LSTM; Hochreiter & Schmidhuber, 1997), then two fully-\nconnected dense layers with dropout between them, and ﬁnally a prediction layer using softmax activation. This\ncorresponds to a single component of the multi-modal architecture, i.e., the top component in Figure 2. Following\nbest practices (e.g., C. Sun, Qiu, Xu, & Huang, 2019) , we set the weights of BERT to be trainable similarly to the\nrandomly initialized embeddings. This process of adjusting the initialized weights of a pre-trained feature extractor\nduring the training process, in our case BERT, is commonly known as ﬁne-tuning in the literature (Howard &\nRuder, 2018). In contrast, the parameters of the GloVe embeddings are ﬁxed to the pre-trained weights and thus\ndo not change during training.\n2.3.2\nMulti-Modal Baselines\nTo analyze the eﬀectiveness of our multi-modal architecture with EEG signals properly, we not only compare it\nto uni-modal text baselines, but also to multi-modal baselines using the same architecture described in the next\nsection for the EEG models, but replacing the features of the second modality with the following alternatives:\n(1) We implement a gaze-augmented baseline, where the ﬁve eye tracking features described in Section 2.1.4 are\ncombined with the word embeddings by adding them to the multi-modal model in the same manner as the EEG\nfeatures, as vectors with dimension = 5. The purpose of this baseline is to allow a comparison of multi-modal\nmodels learning from two diﬀerent types of physiological features. Since the beneﬁts of eye tracking data in ML\nmodels are well established (Barrett & Hollenstein, 2020; Mathias et al., 2020), this is a strong baseline. (2)\nWe further implement a random noise-augmented baseline, where we add uniformly sampled vectors of random\nnumbers as the second input data type to the multi-modal model. These random vectors are of the same dimension\nas the EEG vectors (i.e., d = 105). It is well known that the addition of noise to the input data of a neural network\nduring training can lead to improvements in generalization performance as a form of regularization (Bishop, 1995).\nThus, this baseline is relevant because we want to analyze whether the improvements from the EEG signals on the\nNLP tasks are due to its capability of extracting linguistic information and not merely due to additional noise.\n10\nEEG Decoding \nComponent\nInput Layer\ne1\ne1\ne1\ne1\nbiLSTMs\nDense Layers\n…\nEEG features\nword1\nword2\nwords\n…\nEEG Decoding Component\nInput Layer\ne1\ne1\ne1\ne1\nEEG features\nword1\nword2\nwords\n…\nConv\nConv\nMax \nPool\nConv\nConv\nConv\nConv\nInception Module\nconcatenate\nDense Layers\n…\nflatten\nFigure 3: EEG decoding components: (left) The recurrent model component is analogous to the text\ncomponent and consists of recurrent layers followed by two dense layers with dropout. (right) The\nconvolutional inception component consists of an ensemble of convolution ﬁlters of varying lengths which\nare concatenated and ﬂattened before the subsequent dense layers.\n2.3.3\nEEG Models\nTo fully understand the impact of the EEG data on the NLP models, we build a model that is able to deal with\nmultiple inputs and mixed data. We present a multi-modal model with late decision-level fusion to learn joint\nrepresentations of textual and cognitive input features. We test both a recurrent and a convolutional neural\narchitecture for decoding the EEG signals. Figure 2 depicts the main structure of our model and we describe the\nindividual components below.\nAll input sentences are padded to the maximum sentence length to provide ﬁxed-length text inputs to the model.\nWord embeddings of dimension d are the input for the textual component, where d ∈{32, 300, 768} for randomly\ninitialized embeddings, GloVe embeddings and BERT embeddings, respectively. EEG features of dimension e are\nthe input for the cognitive component, where e = 105. As described, the text component consists of bidirectional\nLSTM layers followed by two dense layers with dropout. Text and EEG features are given as independent inputs to\ntheir own respective component of the network. The hidden representations of these are then concatenated before\nbeing fed to a ﬁnal dense classiﬁcation layer.We also experimented with diﬀerent merging mechanisms to join the\ntext and EEG layers of our two-tower model (concatenation, addition, subtraction, maximum). Concatenation\noverall achieved the best results, so we report only these. Although the goal of each network is to learn feature\ntransformations for their own modality, the relevant extracted information should be complementary. This is\nachieved, as commonly done in deep learning, through alternatively running inference and back-propagation of\nthe data through the entire network enabling information to ﬂow from the component responsible for one input\nmodality to the other via the fully connected output layers. To learn a non-linear transformation function for each\ncomponent, we employ the rectiﬁed linear units (ReLu) as activation functions after each hidden layer.\nFor the EEG component, we test a recurrent and a convolutional architecture since both have proven useful\nin learning features from time series data for language processing (e.g., Fawaz et al., 2020; Lipton, Berkowitz,\n& Elkan, 2015; Yin, Kann, Yu, & Sch¨utze, 2017). For the recurrent architecture (Figure 3, left), the model\ncomponent is analogous to the text component: it consists of bidirectional LSTM layers followed by two dense\nlayers with dropout and ReLu activation functions. For the convolutional architecture (Figure 3, right), we build a\nmodel component based on the Inception module ﬁrst introduced by Szegedy et al. (2015). An inception module\nis an ensemble of convolutions that applies multiple ﬁlters of varying lengths simultaneously to an input time\nseries. This allows the network to automatically extract relevant features from both long and short time series. As\nsuggested by Schirrmeister et al. (2017) we used exponential linear unit activations (ELUs; Clevert, Unterthiner,\n& Hochreiter, 2015) in the convolutional EEG decoding model component.\nFor binary and ternary sentiment analysis, the ﬁnal dense layer has a softmax activation in order to use the\nmaximal output for the classiﬁcation. For the multi-label classiﬁcation case of relation detection, we replace the\nsoftmax function in the last dense layer of the model with a sigmoid activation to produce independent scores\nfor each class. If the score for any class surpasses a certain threshold, the sentence is labeled to contain that\n11\nParameter\nRange\nLSTM layer dimension\n64, 128, 256, 512\nNumber of LSTM layers\n1, 2, 3, 4\nCNN ﬁlters\n14, 16, 18\nCNN kernel sizes\n[1,4,7]\nCNN pool sizes\n3, 5, 7\nDense layer dimension\n8, 16, 32, 64, 128, 256, 512\nDropout\n0.1, 0.3, 0.5\nBatch size\n20, 40, 60\nLearning rate\n10−1, 10−2, 10−3, 10−4, 10−5\nRandom seeds\n13, 22, 42, 66, 78\nThreshold\n0.3, 0.5, 0.7\nTable 3: Tested value ranges included in the hyper-parameter search for our multi-modal machine learning\narchitecture. Threshold only applies to relation detection.\nrelation type (opposite to simply taking the max score as the label of the sentence). The threshold is tuned as an\nadditional hyper-parameter.\nThis multi-modal model with separate components learned for each input data type has several advantages: It\nallows for separate pre-processing of each type of data. For instance, it is able to deal with diﬀering tokenization\nstrategies, which is useful in our case since it is challenging to map linguistic tokenization to the word boundaries\npresented to participants during the recordings of eye tracking and brain activity. Moreover, this approach is\nscalable to any number of input types. The generalizability of our model enables the integration of multiple data\nrepresentations, e.g., learning from brain activity, eye movements, and other cognitive modalities simultaneously.\n2.3.4\nTraining Setup\nTo assess the impact of the EEG signals under fair modelling conditions, the hyper-parameters are tuned individually\nfor all baseline models as well as for all eye tracking and EEG augmented models. The ranges of the hyper-\nparameters are presented in Table 3. All results are reported as means over ﬁve independent runs with diﬀerent\nrandom seeds. In each run, 5-fold cross-validation is performed on a 80% training and 20% test split. The best\nparameters were selected according to the model’s accuracy on the validation set (10% of the training set) across\nall 5 folds. We implemented early stopping with a patience of 80 epochs and a minimum diﬀerence in validation\naccuracy of 10−7. The validation set is used for both parameter tuning and early stopping.\n3\nResults\nIn this study, we assess the potential of EEG brain activity data to enhance NLP tasks in a multi-modal architecture.\nWe present the results of all augmented models compared to the baseline results. As described above, we select\nthe hyper-parameters based on the best validation accuracy achieved for each setting.\nThe performance of our models is evaluated based on the comparison between the predicted labels (i.e., positive,\nneutral or negative sentiment for a sentence; or the relation type(s) in a sentence) and the true labels of the test set\nresulting in the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)\nacross the classiﬁed samples. The terms positive and negative refer to the classiﬁer’s prediction, and the terms\ntrue and false refer to whether that prediction corresponds to the ground truth label. The following decoding\nperformance metrics were computed:\nPrecision is the fraction of relevant instances among the retrieved instances, and is deﬁned as\nPrecision =\nTP\nTP + FP\n(1)\n12\nRandomly initialized\nGloVe\nBERT\nModel\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nBaseline\n0.572\n0.573\n0.552 (0.07)\n0.751\n0.738\n0.728 (0.08)\n0.900\n0.899\n0.893 (0.04)\n+ noise\n0.599\n0.574\n0.541 (0.08)\n0.721\n0.715\n0.709 (0.09)\n0.914\n0.916\n0.913 (0.03)\n+ ET\n0.615 0.605 0.586 (0.06)\n0.795 0.786\n0.781 (0.06)\n0.913\n0.907\n0.904 (0.05)\n+ EEG full\n0.540\n0.538\n0.525 (0.06)\n0.738\n0.729\n0.725 (0.07)\n0.913\n0.909\n0.906 (0.04)\n+ EEG θ\n0.602\n0.599 0.584* (0.08)\n0.789\n0.785 0.783+ (0.05)\n0.917\n0.916\n0.913* (0.04)\n+ EEG α\n0.610\n0.590\n0.565 (0.05)\n0.763\n0.758\n0.753 (0.05)\n0.912\n0.908\n0.906 (0.03)\n+ EEG β\n0.587\n0.578\n0.555 (0.07)\n0.781\n0.777\n0.774+ (0.06)\n0.911\n0.911\n0.907* (0.04)\n+ EEG γ\n0.614\n0.591\n0.553 (0.08)\n0.777\n0.773\n0.769* (0.07)\n0.917 0.917 0.915* (0.04)\n+θ+α+β+γ\n0.597\n0.597\n0.569 (0.08)\n0.766\n0.764\n0.760* (0.07)\n0.913\n0.913\n0.911* (0.04)\nTable 4: Binary sentiment analysis results of the multi-modal model using the recurrent\nEEG decoding component. We report precision (P), recall (R), F1-score and the standard deviation\n(std) between ﬁve runs. The best results per column are marked in bold . Signiﬁcance is indicated on\nthe F1-score with asterisks: * denotes p < 0.05 (uncorrected), + denotes p < 0.003 (Bonferroni corrected\np-value).\nRandomly initialized\nGloVe\nBERT\nModel\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nBaseline\n0.408\n0.384\n0.351 (0.07)\n0.510\n0.507\n0.496 (0.06)\n0.722\n0.714\n0.710 (0.05)\n+ noise\n0.373\n0.399\n0.344 (0.10)\n0.531\n0.519\n0.504 (0.04)\n0.711\n0.706\n0.700 (0.06)\n+ ET\n0.424 0.413 0.388 (0.06)\n0.539 0.528 0.513 (0.04)\n0.728\n0.717\n0.714 (0.05)\n+ EEG full\n0.391\n0.387\n0.353 (0.07)\n0.505\n0.505\n0.488 (0.07)\n0.724\n0.715\n0.711 (0.06)\n+ EEG θ\n0.397\n0.409\n0.360 (0.07)\n0.516\n0.510\n0.498 (0.06)\n0.715\n0.708\n0.704 (0.05)\n+ EEG α\n0.390\n0.390\n0.347 (0.08)\n0.520\n0.516\n0.506 (0.05)\n0.720\n0.712\n0.707 (0.05)\n+ EEG β\n0.350\n0.370\n0.302 (0.09)\n0.523\n0.519\n0.509 (0.05)\n0.732 0.720 0.717 (0.07)\n+ EEG γ\n0.409\n0.397\n0.359 (0.07)\n0.517.\n0.513\n0.502 (0.04)\n0.709\n0.705\n0.697 (0.06)\n+θ+α+β+γ\n0.401\n0.400\n0.368 (0.06)\n0.522\n0.516\n0.505 (0.05)\n0.722\n0.717\n0.713 (0.05)\nTable 5: Ternary sentiment analysis results of the multi-modal model using the recurrent\nEEG decoding component. We report precision (P), recall (R), F1-score and the standard deviation\n(std) between ﬁve runs. The best results per column are marked in bold.\nRecall is the fraction of the relevant instances that are successfully retrieved:\nRecall =\nTP\nTP + FN\n(2)\nThe F1-score is the harmonic mean combining precision and recall:\nF1 score = 2 · Precision · Recall\nPrecision + Recall\n(3)\nFor analyzing the results, we report macro-averaged precision (P), recall (R), and F1-score, i.e., the metrics are\ncalculated for each label to counteract the label imbalance in the datasets.\nThe results for the multi-modal architecture using the recurrent EEG decoding component are presented in\nTable 4 for binary sentiment analysis, Table 5 for ternary sentiment analysis, and Table 6 for relation detection.\nThe ﬁrst three rows in each table represent the uni-modal text baseline, the multi-modal noise and eye-tracking\nbaselines. This is followed by the multi-modal models augmented with the full broadband EEG signals and each\nof the four frequency bands. Finally, in the last row, we also present the results of a multi-modal model with\nﬁve components, where text and each frequency band extractors are learned separately and concatenated at the\nend. In both sentiment tasks, the EEG data yields modest but consistent improvements over the text baseline\nfor all word embeddings types. However, in the case of relation detection, the addition of either eye tracking or\n13\nRandomly initialized\nGloVe\nBERT\nModel\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nBaseline\n0.404 0.525 0.452 (0.04)\n0.501 0.609 0.539 (0.05)\n0.522 0.788\n0.623 (0.05)\n+ noise\n0.420\n0.424\n0.408 (0.07)\n0.577 0.497 0.532 (0.03)\n0.675 0.585\n0.625 (0.03)\n+ ET\n0.421\n0.404\n0.402 (0.06)\n0.547 0.476 0.506 (0.04)\n0.661\n0.631\n0.644 (0.03)\n+ EEG full\n0.345\n0.343\n0.334 (0.05)\n0.511 0.387 0.432 (0.09)\n0.652\n0.690\n0.668* (0.10)\n+ EEG θ\n0.430 0.421\n0.414 (0.07)\n0.582 0.508 0.539 (0.07)\n0.646\n0.736\n0.684* (0.08)\n+ EEG α\n0.368\n0.373\n0.358 (0.12)\n0.582 0.515 0.542 (0.06)\n0.652\n0.715\n0.679* (0.07)\n+ EEG β\n0.349\n0.340\n0.329 (0.09)\n0.581 0.497 0.532 (0.10)\n0.674\n0.726 0.696+ (0.06)\n+ EEG γ\n0.410\n0.399\n0.397 (0.05)\n0.554 0.488 0.514 (0.09)\n0.666.\n0.715\n0.686* (0.07)\n+ θ+α+β+γ\n0.370\n0.376\n0.363 (0.09)\n0.554 0.488 0.514 (0.09)\n0.675\n0.646\n0.659 (0.04)\nTable 6: Relation detection results of the multi-modal model using the recurrent EEG decod-\ning component. We report precision (P), recall (R), F1-score and the standard deviation (std) between\nﬁve runs. The best results per column are marked in bold. Signiﬁcance is indicated on the F1-score with\nasterisks: * denotes p < 0.05 (uncorrected), + denotes p < 0.003 (Bonferroni corrected p-value).\nRandomly initialized\nGloVe\nBERT\nModel\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nBaseline\n0.572\n0.573\n0.552 (0.07)\n0.751\n0.738\n0.728 (0.08)\n0.900\n0.899\n0.893 (0.04)\n+ noise\n0.558\n0.584\n0.528 (0.11)\n0.780\n0.767\n0.762 (0.06)\n0.895\n0.887\n0.883 (0.05)\n+ ET\n0.617\n0.623\n0.610 (0.07)\n0.790\n0.790\n0.783 (0.06)\n0.896\n0.887\n0.881 (0.05)\n+ EEG full\n0.588\n0.583\n0.572 (0.04)\n0.778\n0.774\n0.772+ (0.05)\n0.928 0.927\n0.926* 0.03\n+ EEG θ\n0.564\n0.569\n0.535 (0.08)\n0.805 0.792\n0.791+ (0.04)\n0.922\n0.919 0.917* (0.03)\n+ EEG α\n0.596\n0.593\n0.563 (0.08)\n0.775\n0.781\n0.772* (0.08)\n0.920\n0.917 0.916* (0.03)\n+ EEG β\n0.605\n0.597\n0.580 (0.08)\n0.802 0.797 0.792+ (0.05)\n0.920\n0.914 0.914* (0.04)\n+ EEG γ\n0.640 0.625 0.611+ (0.09)\n0.787\n0.780\n0.776+ (0.05)\n0.905\n0.905\n0.901 (0.04)\n+θ+α+β+γ\n0.599\n0.579\n0.558 (0.07)\n0.800\n0.794\n0.786+ (0.05)\n0.909\n0.910\n0.907 (0.04)\nTable 7: Binary sentiment analysis results of the multi-modal model using the convolutional\nEEG decoding component. We report precision (P), recall (R), F1-score and the standard deviation\n(std) between ﬁve runs. The best results per column are marked in bold. Signiﬁcance is indicated on the\nF1-score with asterisks: * denotes p < 0.05 (uncorrected), + denotes p < 0.003 (Bonferroni corrected\np-value).\nbrain activity data is not helpful for randomly initialized embeddings and only beneﬁcial in some settings using\nGloVe embeddings. Nevertheless, the combination of BERT embeddings and EEG data does improve the relation\ndetection models. Generally, the results show a decreasing maximal performance per task with increasing task\ncomplexity measured in terms of the number of classes (see Section 4.5 for a detailed analysis).\nFurthermore, the results for the multi-modal architecture using the convolutional EEG decoding component\nare presented in Table 7 for binary sentiment analysis, Table 8 for ternary sentiment analysis, and Table 9 for\nrelation detection. The results of this model architecture yield higher overall results, whereas the trend across\ntasks is similar to the models using the recurrent EEG decoding component, i.e., considerable improvements for\nboth sentiment analysis tasks, but for relation detection the most notable improvements are achieved with the\nBERT embeddings. This validates the popular choice of convolutional neural networks for EEG classiﬁcation\ntasks (Craik et al., 2019; Schirrmeister et al., 2017). While recurrent neural networks are often used in NLP and\nlinguistic modelling (due to the left-to-right processing mechanism), CNNs have shown better performance at\nlearning feature weights from noisy data (e.g., Kvist & Lockvall Rhodin, 2019). Hence, our convolutional EEG\ndecoding component is able to better extract the task-relevant linguistic processing information from the input\ndata.\nTo assess the results, we perform statistical signiﬁcance testing with respect to the text baseline in a bootstrap\n14\nRandomly initialized\nGloVe\nBERT\nModel\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nBaseline\n0.408\n0.384\n0.351 (0.07)\n0.510\n0.507\n0.496 (0.06)\n0.722\n0.714\n0.710 (0.05)\n+ noise\n0.359\n0.388\n0.334 (0.09)\n0.494\n0.484\n0.476 (0.07)\n0.715\n0.683\n0.684 (0.05)\n+ ET\n0.417\n0.399\n0.372 (0.05)\n0.509\n0.512\n0.500 (0.07)\n0.721\n0.687\n0.670 (0.05)\n+ EEG full\n0.365\n0.384\n0.333 (0.08)\n0.488\n0.484\n0.476 (0.06)\n0.738\n0.724 0.723+ (0.04)\n+ EEG θ\n0.389\n0.372\n0.330 (0.06)\n0.511\n0.495\n0.477 (0.06)\n0.727\n0.718\n0.716+ (0.05)\n+ EEG α\n0.357\n0.382\n0.331 (0.11)\n0.534\n0.525\n0.515+ (0.06)\n0.732\n0.715\n0.713+ (0.04)\n+ EEG β\n0.425 0.418 0.378 (0.08)\n0.534 0.529 0.520+ (0.05)\n0.727\n0.717\n0.715 (0.04)\n+ EEG γ\n0.404\n0.406\n0.360 (0.08)\n0.539 0.521\n0.514 (0.06)\n0.733 0.725 0.721+ (0.04)\n+θ+α+β+γ\n0.384\n0.402\n0.354 (0.10)\n0.517\n0.504\n0.488 (0.05)\n0.733 0.717\n0.715 (0.06)\nTable 8: Ternary sentiment analysis results of the multi-modal model using the convolutional\nEEG decoding component. We report precision (P), recall (R), F1-score and the standard deviation\n(std) between ﬁve runs. The best results per column are marked in bold. Signiﬁcance is indicated on the\nF1-score with asterisks: * denotes p < 0.05 (uncorrected), + denotes p < 0.003 (Bonferroni corrected\np-value).\nRandomly initialized\nGloVe\nBERT\nModel\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nP\nR\nF1 (std)\nBaseline\n0.404 0.525 0.452 (0.04)\n0.501 0.609 0.539 (0.05)\n0.522 0.788\n0.623 (0.05)\n+ noise\n0.424\n0.299\n0.342 (0.06)\n0.547\n0.441\n0.486 (0.06)\n0.532\n0.493\n0.511 (0.07)\n+ ET\n0.415\n0.307\n0.345 (0.08)\n0.447\n0.413.\n0.428 (0.07)\n0.558\n0.665\n0.593 (0.13)\n+ EEG full\n0.225\n0.225\n0.225 (0.06)\n0.548\n0.408\n0.464 (0.07)\n0.647\n0.664\n0.650 (0.09)\n+ EEG θ\n0.437 0.380\n0.400 (0.05)\n0.620\n0.493\n0.547 (0.05)\n0.721 0.698 0.707+ (0.03)\n+ EEG α\n0.372\n0.366\n0.352 (0.12)\n0.509\n0.433\n0.461 (0.12)\n0.661\n0.697\n0.675+ (0.08)\n+ EEG β\n0.394\n0.328\n0.338 (0.09)\n0.627\n0.479\n0.541 (0.05)\n0.643\n0.646\n0.640 (0.11)\n+ EEG γ\n0.405\n0.363\n0.366 (0.09)\n0.646 0.490 0.555 (0.04)\n0.667\n0.699\n0.679+ (0.06)\n+ θ+α+β+γ\n0.324\n0.227\n0.257 (0.11)\n0.460\n0.436\n0.437 (0.14)\n0.610\n0.562\n0.584 (0.05)\nTable 9: Relation detection results of the multi-modal model using the convolutional EEG\ndecoding component. We report precision (P), recall (R), F1-score and the standard deviation (std)\nbetween ﬁve runs. The best results per column are marked in bold. Signiﬁcance is indicated on the\nF1-score with asterisks: * denotes p < 0.05 (uncorrected), + denotes p < 0.003 (Bonferroni corrected\np-value).\ntest as described in Dror, Baumer, Shlomov, and Reichart (2018) over the F1-scores of the ﬁve runs of all tasks.\nWe compare the results of the multi-modal models using text and EEG data to the uni-modal text baseline. In\naddition, we apply the Bonferroni correction to counteract the problem of multiple comparisons. We choose this\nconservative correction because of the dependencies between the datasets used (Dror, Baumer, Bogomolov, &\nReichart, 2017). Under the Bonferroni correction, the global null hypothesis is rejected if p < α/N, where N is the\nnumber of hypotheses (Bonferroni, 1936). In our setting, α = 0.05 and N = 18, accounting for the combination of\nthe 3 embedding types and 6 EEG feature sets, namely broadband EEG; θ, α, β and γ frequency bands; and all\nfour frequency bands jointly. For instance, in Table 7 the improvements in 6 conﬁgurations out of 18 are also still\nstatistically signiﬁcant under the Bonferroni correction (i.e., p < 0.003), showing that EEG signals bring signiﬁcant\nimprovements in the sentiment analysis task. In the results tables, we mark signiﬁcant results under both the\nuncorrected and the Bonferroni corrected p-value.\n15\n4\nDiscussion\nThe results show consistent improvements on both sentiment analysis tasks, whereas the beneﬁts of using EEG\ndata are only visible in speciﬁc settings for the relation detection task. EEG performs better than, or at least\ncomparable to, eye tracking in many scenarios. This study shows the potential of decoding EEG for NLP and\nprovides a good basis for future studies. Despite the limited amount of data, these results suggest that augmenting\nNLP systems with EEG features is a generalizable approach.\nIn the following sections, we discuss these results from diﬀerent angles. We contrast the performance of\ndiﬀerent EEG features, we compare the EEG results to the text baseline and multi-modal baselines (as described\nin Section 2.3.2), and we analyze the eﬀect of diﬀerent word embedding types. Additionally, we explore the impact\nof varying training set sizes in a data ablation study. Finally, we investigate the possible reasons for the decrease\nin performance for the relation detection task, which we associate with the task complexity. We run all analyses\nwith both the recurrent and the convolutional EEG components.\n4.1\nEEG Feature Analysis\nWe start by investigating the impact of the various EEG features included in our multi-modal models. Diﬀerent\nneurocognitive aspects of language processing during reading are associated with brain oscillations at various\nfrequencies. We ﬁrst give a short overview of the cognitive functions related to EEG frequency bands that are\nfound in literature before discussing the insights of our results.\nTheta activity reﬂects cognitive control and working memory (Williams, Kappen, Hassall, Wright, & Krigolson,\n2019), and increases when processing semantic anomalies (Prystauka & Lewis, 2019). Moreover, M. C. Bastiaansen,\nVan Berkum, and Hagoort (2002) showed a frequency-speciﬁc increase in theta power as a sentence unfolds,\npossibly related to the formation of an episodic memory trace, or to incremental verbal working memory load. High\ntheta power is also prominent during the eﬀective semantic processing of language (M. C. Bastiaansen, Linden,\nKeurs, Dijkstra, & Hagoort, 2005). Alpha activity has been related to attentiveness (Klimesch, 2012). Both\ntheta and alpha ranges are sensitive to the lexical–semantic processes involved in language translation (Grabner,\nBrunner, Leeb, Neuper, & Pfurtscheller, 2007). Beta activity has been involved in higher-order linguistic functions\nsuch as the discrimination of word categories and the retrieval of action semantics as well as semantic memory,\nand syntactic processes, which support meaning construction during sentence processing. There is evidence that\nsuggests that beta frequencies are important for linking past and present inputs and the detection of novelty of\nstimuli, which are essential processes for language perception as well as production (Weiss & Mueller, 2012). Beta\nfrequencies also aﬀect decisions regarding relevance (Eugster et al., 2014). In reading, a stronger power-decrease in\nlower beta frequencies has been found for neutral compared to negative words (Scaltritti, Suitner, & Peressotti,\n2020). Contrarily, emotional processing of pictures enhances gamma band power (M¨uller, Keil, Gruber, & Elbert,\n1999). Gamma-band activity has been used to detect emotions (M. Li & Lu, 2009), and increases during syntactic\nand semantic structure building (Prystauka & Lewis, 2019). In the gamma frequency band, a power increase was\nobserved during the processing of correct sentences in multiple languages, but this eﬀect was absent following\nsemantic violations (Hald, Bastiaansen, & Hagoort, 2006; Penolazzi et al., 2009). Frequency band features have\noften been used in deep learning methods for decoding EEG in other domains, such as mental workload and sleep\nstage classiﬁcation (Craik et al., 2019).\nThe results show that our multi-modal models yield better results with ﬁltered EEG frequency bands than\nusing the broadband EEG signal on almost all tasks and embedding types, as well as on both EEG decoding\ncomponents. Although all frequency band features show promising results on some embedding types and tasks\n(e.g., BERT embeddings and gamma features for binary sentiment analysis reported in Table 4), the results show\nno clear sign of a single frequency band outperforming the others (neither across tasks for a ﬁxed embedding\ntype, nor for a ﬁxed task and across all embedding types). For the sentiment analysis tasks, where both EEG\ndecoding components achieve signiﬁcant improvements, theta and beta features most often achieve the highest\nresults. As described above, brain activity in each frequency band reﬂects speciﬁc cognitive functions. The positive\nresults achieved using theta band EEG features might be explained by the importance of this frequency band\nfor successful semantic processing. Theta power is expected to rise with increasing language processing activity\n(Kosch, Schmidt, Thanheiser, & Chuang, 2020). Various studies have shown that theta oscillations are related to\n16\nsemantic memory retrieval and can be task-speciﬁc (e.g., M. C. Bastiaansen et al., 2005; Giraud & Poeppel, 2012;\nMarko, Cimrov´a, & Rieˇcansk`y, 2019). Overall, previous research shows how theta correlates with the cognitive\nprocessing involved in encoding and retrieving verbal stimuli (see Kahana (2006) for a review), which supports our\nresults. The good performance of the beta EEG features might on one hand be explained by the eﬀect of the\nemotional connotation of words on the beta response (Scaltritti et al., 2020). On the other hand, the role of beta\noscillations in syntactic and semantic uniﬁcation operations during language comprehension (M. Bastiaansen &\nHagoort, 2006; Meyer, 2018) is also supportive of our results.\nBased on the complexity and extent of our results, it is unclear at this point whether a single frequency band\nis more informative for solving NLP tasks. Data-driven methods can help us to tease more information from the\nrecordings by allowing us to test broader theories and task-speciﬁc language representations (Murphy et al., 2018),\nbut our results also clearly show that restricting the EEG signal to a given frequency band is beneﬁcial. More\nresearch is required in this area to speciﬁcally isolate the linguistic processing from the ﬁltered EEG signals.\n4.2\nComparison to Multi-Modal Baselines\nThe multi-modal EEG models often outperform the text baselines (at least for the sentiment analysis tasks). We\nnow analyze how the EEG models compare to the two augmented baselines described in Section 2.3.2 (i.e., eye\ntracking and models augmented with random noise). We ﬁnd that EEG always performs better than or equal to\nthe multi-modal text + eye tracking models. This shows how promising EEG is as a data source for multi-modal\ncognitive NLP. Although eye tracking requires less recording eﬀorts, these results corroborate that EEG data\ncontain more information about the cognitive processes occurring in the brain during language understanding.\nAs expected, the baselines augmented with random noise perform worse than the pure text baselines in all\ncases except for binary sentiment analysis with BERT embeddings. This model seems to deal exceptionally well\nwith added noise. In the case of relation detection, when no improvement is achieved (e.g., for randomly initialized\nembeddings), the added noise harms the models similarly to adding EEG signals. It becomes clear for this task\nthat adding the full broadband EEG features is worse than adding random noise (except with BERT embeddings),\nbut some of the frequency band features clearly outperform the augmented noise baseline.\n4.3\nComparison of Embedding Types\nOur baseline results show that contextual embeddings outperform the non-contextual methods across all tasks.\nArora, May, Zhang, and R´e (2020) also compared randomly initialized, GloVe and BERT embeddings and found\nthat with smaller training sets, the diﬀerence in performance between these three embedding types is larger. This\nis in accordance with our results, which show that the type of embedding has a large impact on the baseline\nperformance on all three tasks. The improvements of adding EEG data in all three tasks are especially noteworthy\nwhen using BERT embeddings. In combination with the EEG data, these embeddings achieve improvements\nacross all settings, including the full EEG broadband data as well as all individual and combined frequency bands.\nThis shows that state-of-the-art contextualized word representations such as BERT are able to interact positively\nwith human language processing data in a multi-modal learning scenario.\nAugmenting our baseline with EEG data on the binary sentiment analysis tasks results in approximately +3%\nF1-score across all the diﬀerent embeddings with the recurrent EEG component. The gain is slightly lower at +1%\nfor all the embeddings in the ternary sentiment classiﬁcation task. While there is no signiﬁcant gain for relation\ndetection with random and GloVe embeddings, the improvements with BERT embeddings reach up to +7%. This\nshows that the improvements gained by adding EEG signals are not only dependent on the task, but also on the\nembedding type. In foresight, this ﬁnding might be useful in the future, when new embeddings will improve the\nbaseline performance even further while possibly also increasing the gain from the EEG signals.\n4.4\nData Ablation\nOne of the challenges of NLP is to learn as much as possible from limited resources. Unlike most machine learning\nmodels, one of the most striking aspects of human learning is the ability to learn new words or concepts from\nlimited numbers of examples (Lake, Salakhutdinov, & Tenenbaum, 2015). Using cognitive language processing\n17\nFigure 4: Data ablation for all three word embedding types for the binary sentiment analysis task using\nthe recurrent EEG decoding component. The shaded areas represent the standard deviations.\nFigure 5: Data ablation for all three word embedding types for the binary sentiment analysis task using\nthe convolutional EEG decoding component. The shaded areas represent the standard deviations.\ndata may allow us to take a step towards meta-learning, the process of discovering the cognitive processes that are\nused to tackle a task in the human brain (Griﬃths et al., 2019), and in turn be able to improve the generalization\nabilities of NLP models. Humans can learn from very few examples, while machines, particularly deep learning\nmodels, typically need many examples. Perhaps this advantage in humans is due to their multi-modal learning\nmechanisms (Linzen, 2020).\nTherefore, we analyze the impact of adding EEG features to our NLP models with less training data. We\nperformed data ablation experiments for all three tasks. The most conclusive results were achieved on binary\nsentiment analysis. Randomly initialised embeddings unsurprisingly suﬀer a lot when reducing training data.\nThe results are shown in Figure 4 and 5, for both EEG decoding components. We present the results for the\nbest-performing frequency bands only. The largest gain from EEG data is obtained with only 50% of the training\ndata with GloVe and BERT embeddings, which is as little as 105 training sentences. These experiments emphasize\nthe potential of EEG signals for NLP especially when dealing with very small amounts of training data and using\npopular word embedding types.\n4.5\nTask Complexity Ablation\nFrom the previously described results, one hypothesis on the reason why augmenting the baseline with EEG data\nlowers the performance in the relation detection task with randomly initialized and GloVe embeddings lies in the\ncomplexity of the task. More concretely, we measure the complexity by counting the number of classes the model\nneeds to learn. Generally, more complex tasks (in terms of number of classes) require more data to generalize (see\nfor instance J. Li et al., 2018). Therefore, it is clear that with a ﬁxed amount of data, the impact of augmenting\nthe feature space with additional information (in this case EEG data) is also less visible for the more complex tasks.\nWe see a decrease in performance with increasing complexity over the three evaluated tasks with all embeddings\nexcept for BERT. Therefore, we validate this hypothesis by simplifying the relation detection task by reducing the\nnumber of classes from 11 to 2. We create binary relation detection tasks for the two most frequent relation types\nJob Title and Visited (see Figure 1). For example, we classify all the samples containing the relation Job Title\n(184 samples) against all samples with no relation (219 samples).\nWe train these additional models with GloVe embeddings, since these did not show any signiﬁcant improvements\nwhen augmented with EEG data on the full relation detection task. The results for the full broadband EEG\nfeatures and the best frequency band from the previous convolutional results (gamma) are shown in Table 10. It is\n18\nRecurrent EEG Decoding\nConvolutional EEG Decoding\nJob Title vs. None\nPrecision\nRecall\nF1-score\nPrecision\nRecall\nF1-score\nGloVe\n0.789\n0.776\n0.767 (0.05)\n0.789\n0.776\n0.767 (0.05)\nGloVe + EEG full\n0.792\n0.782\n0.773 (0.06)\n0.796\n0.793\n0.789 (0.05)\nGloVe + EEG γ\n0.780\n0.788\n0.774 (0.1)\n0.817\n0.811\n0.808 (0.03)\nVisited vs. None\nPrecision\nRecall\nF1-score\nPrecision\nRecall\nF1-score\nGloVe\n0.762\n0.756\n0.734 (0.1)\n0.762\n0.756\n0.734 (0.1)\nGloVe + EEG full\n0.756\n0.759\n0.745 (0.1)\n0.766\n0.758\n0.750 (0.09)\nGloVe + EEG γ\n0.773\n0.768\n0.754 (0.1)\n0.819\n0.795\n0.795 (0.09)\nTable 10: Binary relation detection results for both EEG decoding components for the relation types Job\nTitle and Visited using GloVe embeddings. The best result in each column is marked in bold.\nevident that with the simpliﬁcation of the relation detection task into binary classiﬁcation tasks, EEG signals are\nable to boost the performance of the non-contextualized Glove embeddings and achieve considerable improvements\nover the text baseline. The gains are similar as for binary sentiment analysis for both EEG decoding components.\nThis conﬁrms our hypothesis that the EEG features tested yield good results on simple tasks, but more research is\nneeded to achieve improvements on more complex tasks. Note that, as mentioned previously, this is not the case\nfor BERT embeddings, which outperform the baselines on all NLP tasks.\n4.6\nConclusion\nWe presented a large-scale study about leveraging electrical brain activity signals during reading comprehension\nfor augmenting machine learning models of semantic language understanding tasks, namely, sentiment analysis\nand relation detection. We analyzed the eﬀects of diﬀerent EEG features and compared the multi-modal models\nto multiple baselines. Moreover, we compared the improvements gained from the EEG signals on three diﬀerent\ntypes of word embeddings. Not only did we test the eﬀect of varying training set sizes, but also tasks of various\ndiﬃculty levels (in terms of number of classes).\nWe achieve consistent improvements with EEG across all three embedding types. The models trained with\nBERT embeddings yield signiﬁcant performance increases on all NLP tasks. However, for randomly initialized\nand GloVe embeddings the improvement magnitude decreases for more diﬃcult tasks. For these two types of\nembedding, the improvement for the binary and ternary sentiment analysis tasks ranges between 1-4% F1-score.\nFor relation detection, a multi-class and multi-label sequence classiﬁcation task, it was not possible to achieve\nany improvements unless the task complexity is substantially reduced. Therefore, our experiments show that\nstate-of-the-art contextualized word embeddings combined with careful EEG feature selection achieve good results\nin multi-modal learning. Moreover, we ﬁnd that in the tasks where the multi-modal architecture does achieve\nconsiderable improvements, the convolutional EEG decoding component yields even higher results than the\nrecurrent component.\nTo sum up, we capitalize on the advantages of electroencephalography data to examine if and which EEG\nfeatures can serve to augment language understanding models. While our results show that there is linguistic\ninformation in the EEG signal complementing the text features, more research is needed to isolate language-speciﬁc\nbrain activity features. More generally, this work paves the way for more in-depth EEG-based NLP studies.\nReferences\nAﬀolter, N., Egressy, B., Pascual, D., & Wattenhofer, R. (2020). Brain2word: Decoding brain activity for\nlanguage generation. arXiv preprint arXiv:2009.04765.\nAlday, P. M. (2019). M/EEG analysis of naturalistic stories: A review from speech to language processing.\nLanguage, Cognition and Neuroscience, 34(4), 457–473.\n19\nArmeni, K., Willems, R. M., & Frank, S. (2017). Probabilistic language models in cognitive neuroscience:\npromises and pitfalls. Neuroscience & Biobehavioral Reviews.\nArora, S., May, A., Zhang, J., & R´e, C. (2020). Contextual embeddings: When are they worth it? arXiv\npreprint arXiv:2005.09117.\nArtemova, E., Bakarov, A., Artemov, A., Burnaev, E., & Sharaev, M. (2020). Data-driven models and\ncomputational tools for neurolinguistics: a language technology perspective. Journal of Cognitive\nScience, 21(1), 15–52.\nBarnes, J., Klinger, R., & im Walde, S. S. (2017). Assessing state-of-the-art sentiment models on\nstate-of-the-art sentiment datasets. In Proceedings of the 8th workshop on computational approaches\nto subjectivity, sentiment and social media analysis (pp. 2–12).\nBarnes, J., Øvrelid, L., & Velldal, E. (2019, August). Sentiment analysis is not solved! assessing and probing\nsentiment classiﬁcation. In Proceedings of the 2019 acl workshop blackboxnlp: Analyzing and interpret-\ning neural networks for nlp (pp. 12–23). Florence, Italy: Association for Computational Linguistics.\nRetrieved from https://www.aclweb.org/anthology/W19-4802\nDOI: 10.18653/v1/W19-4802\nBarnes, J., Velldal, E., & Øvrelid, L. (2020). Improving sentiment analysis with multi-task learning of\nnegation. Natural Language Engineering, 1–21. DOI: 10.1017/S1351324920000510\nBarrett, M., Bingel, J., Hollenstein, N., Rei, M., & Søgaard, A. (2018). Sequence classiﬁcation with\nhuman attention. In Proceedings of the 22nd conference on computational natural language learning\n(pp. 302–312).\nBarrett, M., Bingel, J., Keller, F., & Søgaard, A. (2016). Weakly supervised part-of-speech tagging using\neye-tracking data. In Proceedings of the 54th annual meeting of the association for computational\nlinguistics (Vol. 2, pp. 579–584).\nBarrett, M., Gonz´alez-Gardu˜no, A. V., Frermann, L., & Søgaard, A. (2018). Unsupervised induction\nof linguistic categories with records of reading, speaking, and writing. In Proceedings of the 2018\nconference of the north american chapter of the association for computational linguistics: Human\nlanguage technologies, volume 1 (long papers) (pp. 2028–2038).\nBarrett, M., & Hollenstein, N. (2020). Sequence labelling and sequence classiﬁcation with gaze: Novel uses\nof eye-tracking data for natural language processing. Language and Linguistics Compass, 14(11),\n1–16.\nBastiaansen, M., & Hagoort, P. (2006). Oscillatory neuronal dynamics during language comprehension.\nProgress in brain research, 159, 179–196.\nBastiaansen, M. C., Linden, M. v. d., Keurs, M. t., Dijkstra, T., & Hagoort, P. (2005). Theta responses are\ninvolved in lexical—semantic retrieval during language processing. Journal of cognitive neuroscience,\n17(3), 530–541.\nBastiaansen, M. C., Van Berkum, J. J., & Hagoort, P. (2002). Event-related theta power increases in the\nhuman EEG during online sentence processing. Neuroscience Letters, 323(1), 13–16.\nBeinborn, L., Abnar, S., & Choenni, R. (2019). Robust evaluation of language-brain encoding experiments.\nInternational Journal of Computational Linguistics and Applications.\nBeres, A. M. (2017). Time is of the essence: A review of electroencephalography (eeg) and event-related\nbrain potentials (erps) in language research. Applied psychophysiology and biofeedback, 42(4),\n247–255.\nBishop, C. M. (1995). Training with noise is equivalent to tikhonov regularization. Neural computation,\n7(1), 108–116.\nBisk, Y., Holtzman, A., Thomason, J., Andreas, J., Bengio, Y., Chai, J., . . . others (2020). Experience\ngrounds language. In Proceedings of the 2020 conference on empirical methods in natural language\nprocessing (emnlp) (pp. 8718–8735).\nBonferroni, C. (1936). Teoria statistica delle classi e calcolo delle probabilita. Pubblicazioni del R Istituto\n20\nSuperiore di Scienze Economiche e Commericiali di Firenze, 8, 3–62.\nBruns, A. (2004). Fourier-, Hilbert-and wavelet-based signal analysis: Are they really diﬀerent approaches?\nJournal of neuroscience methods, 137(2), 321–332.\nClevert, D.-A., Unterthiner, T., & Hochreiter, S. (2015). Fast and accurate deep network learning by\nexponential linear units (elus). arXiv preprint arXiv:1511.07289.\nClifton, C., Staub, A., & Rayner, K. (2007). Eye movements in reading words and sentences. In Eye\nmovements (pp. 341–371). Elsevier.\nCraik, A., He, Y., & Contreras-Vidal, J. L. (2019). Deep learning for electroencephalogram (EEG)\nclassiﬁcation tasks: a review. Journal of Neural Engineering, 16(3), 031001.\nCulotta, A., McCallum, A., & Betz, J. (2006). Integrating probabilistic extraction models and data mining\nto discover relations and patterns in text. In Proceedings of the human language technology conference\nof the north american chapter of the association of computational linguistics (pp. 296–303).\nDegno, F., Loberg, O., & Liversedge, S. P. (2021). Co-registration of eye movements and ﬁxation—related\npotentials in natural reading: Practical issues of experimental design and data analysis. Collabra:\nPsychology, 7(1).\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 conference of the north american\nchapter of the association for computational linguistics: Human language technologies, volume 1\n(long and short papers) (pp. 4171–4186).\nDimigen, O., Sommer, W., Hohlfeld, A., Jacobs, A. M., & Kliegl, R. (2011). Coregistration of eye\nmovements and EEG in natural reading: analyses and review. Journal of Experimental Psychology:\nGeneral, 140(4), 552.\nDo, T. H., Nguyen, D. M., Tsiligianni, E., Cornelis, B., & Deligiannis, N. (2017). Multiview deep learning\nfor predicting Twitter users’ location. arXiv preprint arXiv:1712.08091.\nDror, R., Baumer, G., Bogomolov, M., & Reichart, R.\n(2017).\nReplicability analysis for natural\nlanguage processing: Testing signiﬁcance with multiple datasets. Transactions of the Association\nfor Computational Linguistics, 5, 471–486.\nDror, R., Baumer, G., Shlomov, S., & Reichart, R. (2018). The hitchhiker’s guide to testing statistical\nsigniﬁcance in natural language processing.\nIn Proceedings of the 56th annual meeting of the\nassociation for computational linguistics (volume 1: Long papers) (pp. 1383–1392).\nEhinger, B. V., & Dimigen, O. (2019). Unfold: an integrated toolbox for overlap correction, non-linear\nmodeling, and regression-based EEG analysis. PeerJ, 7, e7838.\nEphrat, A., Halperin, T., & Peleg, S. (2017). Improved speech reconstruction from silent video. In\nProceedings of the ieee international conference on computer vision workshops (pp. 455–462).\nEttinger, A. (2020). What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for Computational Linguistics, 8, 34-48.\nEugster, M. J., Ruotsalo, T., Spap´e, M. M., Kosunen, I., Barral, O., Ravaja, N., . . . Kaski, S. (2014).\nPredicting term-relevance from brain signals. In Proceedings of the 37th international acm sigir\nconference on research & development in information retrieval (pp. 425–434).\nFawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., . . . Petitjean, F. (2020).\nInceptiontime: Finding alexnet for time series classiﬁcation. Data Mining and Knowledge Discovery,\n34(6), 1936–1962.\nFoster, C., Dharmaretnam, D., Xu, H., Fyshe, A., & Tzanetakis, G. (2018). Decoding music in the human\nbrain using eeg data. In 2018 ieee 20th international workshop on multimedia signal processing\n(mmsp) (pp. 1–6).\nFrank, S. L., Otten, L. J., Galli, G., & Vigliocco, G. (2015). The ERP response to the amount of\ninformation conveyed by words in sentences. Brain and Language, 140, 1–11.\n21\nFrank, S. L., & Willems, R. M. (2017). Word predictability and semantic similarity show distinct patterns\nof brain activity during language comprehension. Language, Cognition and Neuroscience, 32(9),\n1192–1203.\nFriederici, A. D. (2000). The developmental cognitive neuroscience of language: a new research domain.\nBrain and language, 71(1), 65–68.\nFyshe, A., Talukdar, P. P., Murphy, B., & Mitchell, T. M. (2014). Interpretable semantic vectors from\na joint model of brain-and text-based meaning. In Proceedings of the 52nd annual meeting of the\nassociation for computational linguistics (volume 1: Long papers) (p. 489-499).\nGauthier, J., & Ivanova, A. (2018). Does the brain represent words? An evaluation of brain decoding\nstudies of language understanding. arXiv preprint arXiv:1806.00591.\nGiraud, A.-L., & Poeppel, D. (2012). Cortical oscillations and speech processing: emerging computational\nprinciples and operations. Nature neuroscience, 15(4), 511.\nGrabner, R. H., Brunner, C., Leeb, R., Neuper, C., & Pfurtscheller, G. (2007). Event-related eeg theta\nand alpha band oscillatory responses during language translation. Brain Research Bulletin, 72(1),\n57–65.\nGriﬃths, T. L., Callaway, F., Chang, M. B., Grant, E., Krueger, P. M., & Lieder, F. (2019). Doing\nmore with less: meta-reasoning and meta-learning in humans and machines. Current Opinion in\nBehavioral Sciences, 29, 24–30.\nHald, L. A., Bastiaansen, M. C., & Hagoort, P. (2006). Eeg theta and gamma responses to semantic\nviolations in online sentence processing. Brain and Language, 96(1), 90–105.\nHale, J., Dyer, C., Kuncoro, A., & Brennan, J. R. (2018). Finding syntax in human encephalography\nwith beam search. In Proceedings of the 56th annual meeting of the association for computational\nlinguistics (volume 1: Long papers) (pp. 2727–2736).\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.\nHollenstein, N., Barrett, M., & Beinborn, L. (2020). Towards best practices for leveraging human\nlanguage processing signals for natural language processing. In Proceedings of the second workshop\non linguistic and neurocognitive resources (pp. 15–27).\nHollenstein, N., Barrett, M., Troendle, M., Bigiolli, F., Langer, N., & Zhang, C. (2019). Advancing NLP\nwith cognitive language processing signals. arXiv preprint arXiv:1904.02682.\nHollenstein, N., de la Torre, A., Langer, N., & Zhang, C. (2019). CogniVal: A framework for cognitive\nword embedding evaluation. In Proceedings of the 23nd conference on computational natural language\nlearning.\nHollenstein, N., Rotsztejn, J., Troendle, M., Pedroni, A., Zhang, C., & Langer, N. (2018). ZuCo, a\nsimultaneous EEG and eye-tracking resource for natural sentence reading. Scientiﬁc Data.\nHollenstein, N., Troendle, M., Zhang, C., & Langer, N. (2020). ZuCo 2.0: A dataset of physiological\nrecordings during natural reading and annotation. In Proceedings of the 12th language resources\nand evaluation conference (pp. 138–146).\nHollenstein, N., & Zhang, C. (2019). Entity recognition at ﬁrst sight: Improving NER with eye movement\ninformation. In Proceedings of the 2018 conference of the north american chapter of the association\nfor computational linguistics: Human language technologies, volume 1 (long papers).\nHoward, J., & Ruder, S. (2018). Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings\nof the 56th annual meeting of the association for computational linguistics (volume 1: Long papers)\n(pp. 328–339).\nKahana, M. J. (2006). The cognitive correlates of human brain oscillations. Journal of Neuroscience,\n26(6), 1669–1672.\nKandylaki, K. D., & Bornkessel-Schlesewsky, I. (2019). From story comprehension to the neurobiology of\nlanguage. Language, Cognition and Neuroscience, 34(4), 405-410.\n22\nKiela, D., Grave, E., Joulin, A., & Mikolov, T. (2018). Eﬃcient large-scale multi-modal classiﬁcation. In\nThirty-second aaai conference on artiﬁcial intelligence.\nKlimesch, W. (2012). Alpha-band oscillations, attention, and controlled access to stored information.\nTrends in Cognitive Sciences, 16(12), 606–617.\nKosch, T., Schmidt, A., Thanheiser, S., & Chuang, L. L. (2020). One does not simply rsvp: Mental\nworkload to select speed reading parameters using electroencephalography. In Proceedings of the\n2020 chi conference on human factors in computing systems (pp. 1–13).\nKvist, E., & Lockvall Rhodin, S. (2019). A comparative study between mlp and cnn for noise reduction\non images: The impact of diﬀerent input dataset sizes and the impact of diﬀerent types of noise on\nperformance.\nLake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through\nprobabilistic program induction. Science, 350(6266), 1332–1338.\nLemh¨ofer, K., & Broersma, M. (2012). Introducing LexTALE: A quick and valid lexical test for advanced\nlearners of english. Behavior Research Methods, 44(2), 325–343.\nLi, J., Liu, Y., Yin, R., Zhang, H., Ding, L., & Wang, W. (2018). Multi-class learning: From theory to\nalgorithm. In Advances in neural information processing systems (pp. 1586–1595).\nLi, M., & Lu, B.-L. (2009). Emotion classiﬁcation based on gamma-band EEG. In Engineering in medicine\nand biology society, 2009. embc 2009. annual international conference of the ieee (pp. 1223–1226).\nLing, S., Lee, A. C., Armstrong, B. C., & Nestor, A. (2019). How are visual words represented? Insights\nfrom EEG-based visual word decoding, feature derivation and image reconstruction. Human Brain\nMapping, 40(17), 5056–5068.\nLinzen, T. (2020). How can we accelerate progress towards human-like linguistic generalization? arXiv\npreprint arXiv:2005.00955.\nLipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of recurrent neural networks for\nsequence learning. arXiv preprint arXiv:1506.00019.\nLong, Y., Lu, Q., Xiang, R., Li, M., & Huang, C.-R. (2017). A cognition based attention model for\nsentiment analysis. In Proceedings of the 2017 conference on empirical methods in natural language\nprocessing (pp. 462–471).\nMarko, M., Cimrov´a, B., & Rieˇcansk`y, I. (2019). Neural theta oscillations support semantic memory\nretrieval. Scientiﬁc reports, 9(1), 1–10.\nMathias, S., Kanojia, D., Mishra, A., & Bhattacharyya, P. (2020). A survey on using gaze behaviour for\nnatural language processing. Proceedings of IJCAI .\nMcClelland, J. L., Hill, F., Rudolph, M., Baldridge, J., & Sch¨utze, H. (2020). Placing language in an\nintegrated understanding system: Next steps toward human-level performance in neural language\nmodels. Proceedings of the National Academy of Sciences.\nMeyer, L. (2018). The neural oscillations of speech processing and language comprehension: state of the\nart and emerging mechanisms. European Journal of Neuroscience, 48(7), 2609–2621.\nMishra, A., Kanojia, D., Nagar, S., Dey, K., & Bhattacharyya, P. (2017). Leveraging cognitive features\nfor sentiment analysis. Proceedings of The 20th Conference on Computational Natural Language\nLearning, 156–166.\nMorency, L.-P., & Baltruˇsaitis, T. (2017). Multimodal machine learning: integrating language, vision and\nspeech. In Proceedings of the 55th annual meeting of the association for computational linguistics:\nTutorial abstracts (pp. 3–5).\nM¨uller, M. M., Keil, A., Gruber, T., & Elbert, T. (1999). Processing of aﬀective pictures modulates\nright-hemispheric gamma band eeg activity. Clinical Neurophysiology, 110(11), 1913–1920.\nMurphy, B., & Poesio, M. (2010). Detecting semantic category in simultaneous EEG/MEG recordings. In\nProceedings of the naacl hlt 2010 ﬁrst workshop on computational neurolinguistics (pp. 36–44).\n23\nMurphy, B., Wehbe, L., & Fyshe, A. (2018). Decoding language from the brain. Language, Cognition,\nand Computational Models, 53.\nMuttenthaler, L., Hollenstein, N., & Barrett, M. (2020). Human brain activity for machine attention.\narXiv preprint arXiv:2006.05113.\nNaselaris, T., Kay, K. N., Nishimoto, S., & Gallant, J. L. (2011). Encoding and decoding in fMRI.\nNeuroImage, 56(2), 400–410.\nNurse, E., Mashford, B. S., Yepes, A. J., Kiral-Kornek, I., Harrer, S., & Freestone, D. R.\n(2016).\nDecoding EEG and LFP signals using deep learning: Heading TrueNorth. In Proceedings of the acm\ninternational conference on computing frontiers (pp. 259–266).\nPedroni, A., Bahreini, A., & Langer, N. (2019). Automagic: Standardized preprocessing of big EEG data.\nNeuroImage.\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation.\nIn Proceedings of the 2014 conference on empirical methods in natural language processing (pp.\n1532–1543).\nPenolazzi, B., Angrilli, A., & Job, R. (2009). Gamma EEG activity induced by semantic violation during\nsentence reading. Neuroscience Letters, 465(1), 74–78.\nPfeiﬀer, C., Hollenstein, N., Zhang, C., & Langer, N. (2020). Neural dynamics of sentiment processing\nduring naturalistic sentence reading. NeuroImage, 116934.\nPoeppel, D. (2014). The neuroanatomic and neurophysiological infrastructure for speech and language.\nCurrent Opinion in Neurobiology, 28, 142–149.\nPoeppel, D., Emmorey, K., Hickok, G., & Pylkk¨anen, L. (2012). Towards a new neurobiology of language.\nJournal of Neuroscience, 32(41), 14125–14131.\nPoria, S., Hazarika, D., Majumder, N., & Mihalcea, R. (2020). Beneath the tip of the iceberg: Current\nchallenges and new directions in sentiment analysis research. IEEE Transactions on Aﬀective\nComputing, 1-1. DOI: 10.1109/TAFFC.2020.3038167\nPrystauka, Y., & Lewis, A. G. (2019). The power of neural oscillations to inform sentence comprehension:\nA linguistic perspective. Language and Linguistics Compass, 13(9), e12347.\nRamachandram, D., & Taylor, G. W. (2017). Deep multimodal learning: A survey on recent advances\nand trends. IEEE Signal Processing Magazine, 34(6), 96–108.\nRotsztejn, J., Hollenstein, N., & Zhang, C. (2018). ETH-DS3Lab at SemEval-2018 Task 7: Eﬀectively\ncombining recurrent and convolutional neural networks for relation classiﬁcation and extraction. In\nProceedings of the 12th international workshop on semantic evaluation (p. 689-696).\nSassenhagen, J., & Fiebach, C. J. (2020). Traces of meaning itself: Encoding distributional word vectors\nin brain activity. Neurobiology of Language, 1(1), 54–76.\nSato, N., & Mizuhara, H. (2018). Successful encoding during natural reading is associated with ﬁxation-\nrelated potentials and large-scale network deactivation. Eneuro, 5(5).\nScaltritti, M., Suitner, C., & Peressotti, F. (2020). Language and motor processing in reading and typing:\nInsights from beta-frequency band power modulations. Brain and language, 204, 104758.\nSchirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J., Glasstetter, M., Eggensperger, K., Tangermann,\nM., . . . Ball, T. (2017). Deep learning with convolutional neural networks for EEG decoding and\nvisualization. Human brain mapping, 38(11), 5391–5420.\nSchwartz, D., Toneva, M., & Wehbe, L. (2019). Inducing brain-relevant bias in natural language processing\nmodels. In Advances in neural information processing systems (pp. 14100–14110).\nSereno, S. C., & Rayner, K. (2003). Measuring word recognition in reading: Eye movements and\nevent-related potentials. Trends in Cognitive Sciences, 7(11), 489–493.\nSmith, N. J., & Kutas, M. (2015a). Regression-based estimation of erp waveforms: Ii. nonlinear eﬀects,\noverlap correction, and practical considerations. Psychophysiology, 52(2), 169–181.\n24\nSmith, N. J., & Kutas, M. (2015b). Regression-based estimation of erp waveforms: I. the rerp framework.\nPsychophysiology, 52(2), 157–168.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., & Potts, C. (2013). Recursive\ndeep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013\nconference on empirical methods in natural language processing (pp. 1631–1642).\nStemmer, B., & Connolly, J. F. (2012). The EEG/ERP technologies in linguistic research. Methodological\nand Analytic Frontiers in Lexical Research, 47, 337.\nSun, C., Qiu, X., Xu, Y., & Huang, X. (2019). How to ﬁne-tune BERT for text classiﬁcation? In China\nnational conference on chinese computational linguistics (pp. 194–206).\nSun, P., Anumanchipalli, G. K., & Chang, E. F. (2020). Brain2char: A deep architecture for decoding\ntext from brain recordings. Journal of Neural Engineering.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., . . . Rabinovich, A. (2015). Going\ndeeper with convolutions. In Proceedings of the ieee conference on computer vision and pattern\nrecognition (pp. 1–9).\nToneva, M., & Wehbe, L. (2019). Interpreting and improving natural-language processing (in machines)\nwith natural language-processing (in the brain). In Advances in neural information processing\nsystems (pp. 14928–14938).\nTsai, Y.-H. H., Bai, S., Liang, P. P., Kolter, J. Z., Morency, L.-P., & Salakhutdinov, R. (2019). Multimodal\ntransformer for unaligned multimodal language sequences. In Proceedings of the 57th annual meeting\nof the association for computational linguistics (pp. 6558–6569).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017).\nAttention is all you need. In Advances in neural information processing systems (pp. 5998–6008).\nWehbe, L., Vaswani, A., Knight, K., & Mitchell, T. (2014). Aligning context-based statistical models of\nlanguage with brain activity during reading. In Proceedings of the 2014 conference on empirical\nmethods in natural language processing (emnlp) (pp. 233–243).\nWeiss, S., & Mueller, H. M. (2012). “Too many betas do not spoil the broth”: the role of beta brain\noscillations in language processing. Frontiers in psychology, 3, 201.\nWilliams, C. C., Kappen, M., Hassall, C. D., Wright, B., & Krigolson, O. E. (2019). Thinking theta and\nalpha: Mechanisms of intuitive and analytical reasoning. NeuroImage, 189, 574–580.\nWinkler, I., Haufe, S., & Tangermann, M. (2011). Automatic classiﬁcation of artifactual ICA-components\nfor artifact removal in EEG signals. Behavioral and Brain Functions, 7(1), 30.\nYin, W., Kann, K., Yu, M., & Sch¨utze, H. (2017). Comparative study of CNN and RNN for natural\nlanguage processing. arXiv preprint arXiv:1702.01923.\n25\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-02-17",
  "updated": "2021-07-13"
}