{
  "id": "http://arxiv.org/abs/2102.10303v2",
  "title": "Towards Building A Group-based Unsupervised Representation Disentanglement Framework",
  "authors": [
    "Tao Yang",
    "Xuanchi Ren",
    "Yuwang Wang",
    "Wenjun Zeng",
    "Nanning Zheng"
  ],
  "abstract": "Disentangled representation learning is one of the major goals of deep\nlearning, and is a key step for achieving explainable and generalizable models.\nA well-defined theoretical guarantee still lacks for the VAE-based unsupervised\nmethods, which are a set of popular methods to achieve unsupervised\ndisentanglement. The Group Theory based definition of representation\ndisentanglement mathematically connects the data transformations to the\nrepresentations using the formalism of group. In this paper, built on the\ngroup-based definition and inspired by the n-th dihedral group, we first\npropose a theoretical framework towards achieving unsupervised representation\ndisentanglement. We then propose a model, based on existing VAE-based methods,\nto tackle the unsupervised learning problem of the framework. In the\ntheoretical framework, we prove three sufficient conditions on model, group\nstructure, and data respectively in an effort to achieve, in an unsupervised\nway, disentangled representation per group-based definition. With the first two\nof the conditions satisfied and a necessary condition derived for the third\none, we offer additional constraints, from the perspective of the group-based\ndefinition, for the existing VAE-based models. Experimentally, we train 1800\nmodels covering the most prominent VAE-based methods on five datasets to verify\nthe effectiveness of our theoretical framework. Compared to the original\nVAE-based methods, these Groupified VAEs consistently achieve better mean\nperformance with smaller variances.",
  "text": "Published as a conference paper at ICLR 2022\nTOWARDS BUILDING A GROUP-BASED UNSUPER-\nVISED REPRESENTATION DISENTANGLEMENT FRAME-\nWORK\nYang Tao1∗, Xuanchi Ren2 , Yuwang Wang3†, Wenjun Zeng4 , Nanning Zheng1\n1Xi’an Jiaotong University, 2HKUST, , 3Microsoft Research Asia, 4EIT\nABSTRACT\nDisentangled representation learning is one of the major goals of deep learning, and\nis a key step for achieving explainable and generalizable models. A well-deﬁned\ntheoretical guarantee still lacks for the VAE-based unsupervised methods, which\nare a set of popular methods to achieve unsupervised disentanglement. The Group\nTheory based deﬁnition of representation disentanglement mathematically connects\nthe data transformations to the representations using the formalism of group. In\nthis paper, built on the group-based deﬁnition and inspired by the n-th dihedral\ngroup, we ﬁrst propose a theoretical framework towards achieving unsupervised\nrepresentation disentanglement. We then propose a model, based on existing VAE-\nbased methods, to tackle the unsupervised learning problem of the framework. In\nthe theoretical framework, we prove three sufﬁcient conditions on model, group\nstructure, and data respectively in an effort to achieve, in an unsupervised way,\ndisentangled representation per group-based deﬁnition. With the ﬁrst two of the\nconditions satisﬁed and a necessary condition derived for the third one, we offer\nadditional constraints, from the perspective of the group-based deﬁnition, for the\nexisting VAE-based models. Experimentally, we train 1800 models covering the\nmost prominent VAE-based methods on ﬁve datasets to verify the effectiveness\nof our theoretical framework. Compared to the original VAE-based methods,\nthese Groupiﬁed VAEs consistently achieve better mean performance with smaller\nvariances.\n1\nINTRODUCTION\nLearning independent and semantic representations of which individual dimension has interpretable\nmeaning, usually referred to as disentangled representations learning, is critical for artiﬁcial intelli-\ngence research (Bengio et al., 2013). Such disentangled representations are useful for many tasks:\ndomain adaptation (Li et al., 2019; Zou et al., 2020), zero-shot learning (Lake et al., 2017), and\nadversarial attacks (Alemi et al., 2016), etc. Intuitively, a disentangled representation should reﬂect\nthe factors of variations behind the observed data of the world, and one latent unit is only sensitive to\nchanges of an individual factor.\nDue to the facts that obtaining the ground-truth labels requires signiﬁcant human effort and humans\ncan learn those factors unsupervisedly, unsupervised representation disentanglement draws much\nattention from researchers recently. A lot of methods are proposed base on some intuitions. Most of\nthe state-of-the-art methods (Higgins et al., 2017; Burgess et al., 2018; Kim & Mnih, 2018; Chen et al.,\n2018; Kumar et al., 2017) are based on Variational Autoencoder (VAE) (Kingma & Welling, 2013).\nThese methods are fully unsupervised and can be applied to a variety of complex datasets (Lee et al.,\n2020). However, these methods suffer from the unidentiﬁability problem (Locatello et al., 2019b)\ndue to a lack of theoretical guarantee. Another stream of works (Chen et al., 2016; Lin et al., 2020;\nKhrulkov et al., 2021; Lee et al., 2020) leverage generative adversarial network (GAN) (Goodfellow\net al., 2014) to achieve disentanglement but are not interpretable. In general, a well-deﬁned theoretical\nguarantee is needed for those methods.\n∗Work done during internships at Microsoft Research Asia.\n†Corresponding author\n1\narXiv:2102.10303v2  [cs.LG]  15 Mar 2022\nPublished as a conference paper at ICLR 2022\nThe research of symmetry in physics demonstrates that inﬁnitesimal transformations that conform\nto some symmetry groups on physical objects can reﬂect their nature (Anderson, 1972; Noether,\n1915). Recently, inspired by this research on symmetry, Higgins et al. (2018) proposed a group-based\ndeﬁnition of disentangled representation. They argue that the symmetries, i.e., the transformations that\nchange certain aspects of data and keep other aspects unchanged, ideally reﬂect the underlying data\nstructure. The group-based deﬁnition is a formal and rigorous mathematical deﬁnition of faithful and,\nideally, interpretable representation of the generative factors of data, which is widely accepted (Greff\net al., 2019; Mathieu et al., 2019; Khemakhem et al., 2020). Subsequently, due to the fact that the\ndeﬁnition is deﬁned by the world state (i.e., Ground Truth) and based on the assumption (Caselles-\nDupr´e et al., 2019) that this deﬁnition should be useful for downstream tasks such as a Reinforcement\nLearning, Caselles-Dupr´e et al. (2019), Quessard et al. (2020), Painter et al. (2020) propose\nenvironment-based (to provide world state) methods to learn such disentangled representations in\nReinforcement Learning settings. These inspire us to ask the following question: how would the\ndeﬁnition beneﬁt unsupervised representation disentanglement, and how to learn such a disentangled\nrepresentation conforming to the deﬁnition in the setting of unsupervised representation learning?\nIn Group Theory1, the n-th dihedral group (Judson, 2020) is a set of all permutations of polygons\nvertices, forming a permutation group under the operation of composition (Miller, 1973). The\ngenerators in an n-th dihedral group, i.e., ﬂip and rotation, can be regarded as the disentangled factors\nand also transformations. In this paper, inspired by the n-th dihedral group, we answer the above\nquestions and address the challenge by proposing a theoretical framework to make the deﬁnition\npractically applicable for unsupervised representation disentanglement. We then propose a model to\ntackle the learning problem of the framework and verify its effectiveness. We theoretically prove in\nSection 3.2 the three sufﬁcient conditions towards achieving disentangled representation per group-\nbased deﬁnition, which are referred to as model, group structure, and data constraint, respectively.\nWith these conditions, we offer additional constraints from the perspective of the deﬁnition. The\nadditional constraints encourage existing VAE-based models to satisfy the symmetry requirement\nthat comes from the nature of factors. Finally, we provide a learning model based on the existing\nVAE-based methods in an effort to fulﬁll the three conditions (with the model and group structure\nconstraint and a necessary condition for the data constraint satisﬁed). As an intuitive understanding,\nwe introduce the additional constraints to reorganize the latent space to restrict its symmetry in an\nunsupervised way. These additional constraints indeed narrow down the solution space of VAE-\nbased models. Detailed discussion in Sec. 5.4. Our model consistently achieves statistically better\nperformance in prominent metrics (higher means and lower variances) than corresponding existing\nVAE-based models on ﬁve datasets, demonstrating that the group-based deﬁnition together with our\nproposed framework further encourages disentanglement.\nOur main contributions are summarized as follows:\n• To our best knowledge, we are the ﬁrst to provide a theoretical framework to make the\nformal group-based mathematical deﬁnition of disentanglement practically applicable to\nunsupervised representation disentanglement.\n• Our theoretical framework provides additional constraints from the perspective of group-\nbased deﬁnition for the existing VAE-based methods.\n• We propose a learning model of the framework by deriving and integrating additional loss\ninto existing VAE-based models, in an effort to make the learned representation conform to\nthe group-based deﬁnition without relying on the environment (as done in Caselles-Dupr´e\net al. (2019); Quessard et al. (2020); Painter et al. (2020)).\n2\nRELATED WORKS\nDifferent deﬁnitions have been proposed for disentangled representation (Bengio et al., 2013; Higgins\net al., 2018; Suter et al., 2019). However, only the group-based deﬁnition proposed by Higgins et al.\n(2018) focuses on the disentangled representation itself and is mathematically rigorous, which is well\naccepted (Caselles-Dupr´e et al., 2019; Quessard et al., 2020; Painter et al., 2020; Diane Bouchacourt,\n2021). Nevertheless, Higgins et al. (2018) do not propose a speciﬁc learning method based on their\n1We assume some basic familiarity with the fundamentals of Group Theory and Group Representation\nTheory. Please refer to Appendix A for some basic concepts.\n2\nPublished as a conference paper at ICLR 2022\ndeﬁnition. Before this rigorous deﬁnition was proposed, there had been some success in identifying\ngenerative factors in static datasets (without interaction with environment), e.g., β-VAE (Higgins et al.,\n2017), Anneal-VAE (Burgess et al., 2018), β-TCVAE (Chen et al., 2018), and FactorVAE (Kim &\nMnih, 2018). More recent works (Srivastava et al., 2020; Shao et al., 2020; Kim et al., 2019; Lezama,\n2018; Rezende & Viola, 2018) also do not consider the group-based deﬁnition. Therefore, how\ngroup-based deﬁnition will facilitate these methods is still an open question. Besides, all these works\nsuffer from the unidentiﬁability problem (Locatello et al., 2019b), which is a challenging problem\nin this literature. From group-based deﬁnition, our framework points out that, the unidentiﬁability\nproblem could be solved once the data constraint is satisﬁed. However, in this work, we can only get\na necessary condition for data constraint, and we still can not solve this challenging problem.\nAs pointed out in Quessard et al. (2020), it is not straightforward to reconcile the probabilistic infer-\nence methods with the group-based deﬁnition framework. Caselles-Dupr´e et al. (2019), Quessard\net al. (2020), Painter et al. (2020) leverage the interaction with the environment (assuming it is\navailable) as supervision instead of minimizing the total correlation as the VAE-based methods do.\nConsequently, the effectiveness of these methods is limited to the datasets with the environment\navailable. Our framework learns a representation conforming to the group-based deﬁnition without\nrelying on the environment. Pfau et al. (2020) propose a non-parametric method to unsupervisedly\nlearn linear disentangled planes in data manifold under a metric. However, as pointed out by the au-\nthors, the method does not generalize to held-out data and performs poorly when trying to disentangle\ndirectly from pixels.\nTo summarize, the existing probabilistic inference methods lack theoretical support, while the\napplication scope of existing methods based on the group-based mathematical deﬁnition Higgins\net al. (2018) is very limited. To the best of our knowledge, our work is the ﬁrst to reconcile the\nprobabilistic generative methods with the inherently deterministic group-based deﬁnition framework\nof Higgins et al. (2018).\n3\nTHE GROUP-BASED FRAMEWORK FOR UNSUPERVISED REPRESENTATION\nDISENTANGLEMENT\nOur goal is to explore the beneﬁt of the group-based deﬁnition for unsupervised representation\ndisentanglement and learn such a disentangled representation. The background of the group-based\ndeﬁnition is provided in Section 3.1. Section 3.2 presents the theoretical framework towards achieving\nunsupervised disentanglement, in which we derive three sufﬁcient conditions on the model, group\nstructure, and data, respectively. The conditions on the model and group structure provide additional\nconstraints for the existing VAE-based models.\n3.1\nGROUP-BASED DEFINITION\nWe brieﬂy review the group-based deﬁnition of disentangled representation Higgins et al. (2018).\nConsidering a group G acting on world state space W (can be understood as ground-truth) of data\nspace O and representation space Z via group action ·W and group action ·Z respectively. For a\nmapping f = b ◦h, where b and h denote the data generative process and encoding, we state: the\nmapping f is equivariant between the actions on W and Z if\ng · f(w) = f(g · w), ∀g ∈G, ∀w ∈W.\n(1)\nDeﬁnition 1 Assume G can be decomposed as G = G1 × G2 × · · · × Gm. The set Z is disentangled\nwith respect to G if: (i) group action of G on Z exits. (ii) the mapping f is equivariant between the\nactions on W and Z. (iii) There is a decomposition Z = Z1 × Z2 × · · · × Zm such that each Zi is\naffected only by the corresponding Gi.\nIt is challenging to apply the group-based deﬁnition to an unsupervised disentanglement setting in\npractice because the deﬁnition refers to the world state space W, the group action of G on W, and\nmapping b which are typically inaccessible in practice. We tackle the challenge by re-framing the\ndeﬁnition in a new framework in the following section.\n3\nPublished as a conference paper at ICLR 2022\n3.2\nPROPOSED THEORETICAL FRAMEWORK\nSince when the representation is disentangled, one latent unit in the representation space is only\nsensitive to changes of an individual generative factor, we make the following assumptions: G\nis a direct product of m cyclic groups (as suggested by Higgins et al. (2018) and for simplicity):\nG = (Z/nZ)m = Z/nZ × Z/nZ × · · · × Z/nZ, where n is the assumed total number of possible\nvalues for a factor and m is the total number of factors; we further assume Z is a set with the same\nelements in G. Therefore, the group actions of G on Z can be set to be element-wise addition, i.e.,\ng · z = g + z, ∀z ∈Z, g ∈G. For the generator of dimension i of G, gi = (0, . . . , 1, . . . , 0), gi\nonly affects the i-th dimension of z by gi + z. In addition, the action of each generator gi on w only\naffects a single dimension of w.\nAs we can seen from Equation 1 above, the group action is deﬁned on w, which is often not accessible,\nmaking it difﬁcult to apply the deﬁnition in practice. Therefore, for the unsupervised setting, we\nwould like to use permutations on the data space O (which only provides data without labels) to\nsubstitute the group actions on W. Speciﬁcally, inspired by the n-th dihedral group (Dummit &\nFoote, 1991), we construct a permutation group Φ, serving the role of an “agent” of G. The actions\nof G on W can be performed by ϕg ∈Φ on O, which can be formulated as\nf(g · w) = h(ϕg · b(w)) = h(ϕg · o), ∀w ∈W, g ∈G,\n(2)\nwhere o denotes the data (e.g., image) corresponding to the world state w through the mapping\nfunction b. If the above equation holds, we state that the “agent” permutation group Φ exists. We ﬁrst\ngive the conditions for the existence of this “agent” permutation group Φ, then derive the additional\ncondition to achieve such disentanglement. We accomplish these two objectives in Theorem 1 with\nthe proof provided in Appendix B. Theorem 1 states that a general permutation group Φ on O can\nserve as an agent group (agent group exists) if and only if both (i) and (ii) are satisﬁed. If the agent\ngroup exists, and its permutations (actions on O) can be deﬁned by an autoencoder-like model as\nshown in the equation in (iii), then Z is disentangled with respect to G.\nTheorem 1 For the group G = (Z/nZ)m, a permutation group Φ on O, a representation space Z,\na World State space W, and mapping b and h, Equation 2 holds if and only if (i) Φ is isomorphic to\nG, and (ii) For each generator of dimension i of G, gi, there exists a ϕi ∈Φ, i = 1, . . . , m, such\nthat ϕi · b(w) = b(gi · w), ∀w ∈W, and ϕi is a generator of Φ; Further, if Equation 2 holds and\n(iii) ϕg · b(w) = h−1(g · f(w)) ∀w ∈W, ϕg ∈Φ, then Z is disentangled with respect to G, where\nϕg is the corresponding element in Φ of g under the isomorphism.\nIn Theorem 1, (i) states that the relation between the elements (i.e., group structure) is preserved\nbetween Φ and G, and we denote it as the group structure constraint; (ii) actually indicates a data\nconstraint that all variations in the data can be generated by compositions of some basic permutation\ngenerators {ϕi}i=1,...,m. We denote it as the data constraint; (iii) states that the permutations in\nthe agent group Φ are deﬁned by encoding, action, and decoding, which is referred to as the model\nconstraint. Note that in Theorem 1, only the data constraint refers to the world state w.\nHere is a sketch of the proof: data constraint is a special case of Equation 2 for a generator, and\ngroup structure constraint is a relation-preserving constraint on compositions of generators, and\nsatisfying both constraints will thus result in that Equation 2 holds for any general element in Φ, and\nvice versa. Moreover, we can derive Equation 1 for disentanglement when combining the model\nconstraint and Equation 2.\nThe model constraint speciﬁes the way to permute the data. When the data is permuted, its world\nstate changes. Therefore, how the world states transit between each other is modeled by the model\nconstraint applied on the data. The isomorphism between Φ and G ensures that the world state space\nW and data space O have the same symmetry. In this way, the model applied on the data learns the\ntransition of the world states. Note that we aim to bring this group-based deﬁnition, which requires\nground truth by default, into the unsupervised setting. Now only the data constraint refers to the\nworld states, and it seems almost impossible to derive a sufﬁcient condition for it without the labels.\nWe thus make a trade-off in which we use a necessary condition in the next section.\n4\nGROUPIFIED VAE: A LEARNING METHOD OF THE FRAMEWORK\nLet’s look closer into the three constraints, respectively. Firstly, we consider the model constraint,\nϕg · o = h−1(g · h(o)) ∀o ∈O, ϕg ∈Φ, which suggests that the action of Φ on O can be\n4\nPublished as a conference paper at ICLR 2022\nDecoder\n(a)\n(b)\nEncoder\nDecoder\nFigure 1: Overview of the implementation (Groupiﬁed VAE). (a) Illustration of permutation group\nΦ = {ϕg|g ∈G} deﬁned on a VAE-based model, where G = (Z/nZ)m. The generators ϕi, ϕj ∈\nΦ are permutations on O. Speciﬁcally, when optimized, ϕi and ϕj are horizontal and vertical\nmovements. ϕi is deﬁned as the solid orange arrows illustrate: encode an image o to representation z,\nperform η on z to get z, add gi to z, and decode back to the image. This process can be regarded as an\nexchange of images in dataset (permutation), as the dashed orange arrow shows. These permutations\nform a group Φ. (b) The Isomorphism Loss, which guarantees that Φ is isomorphic to G, includes\nAbel Loss La constraining the commutativity, and Order Loss Lo constraining the cyclicity.\nimplemented using an autoencoder-like network that performs encoding, action on its representation\nspace, and decoding. Given an autoencoder-like network with an encoder h and a decoder d, since d\nis approximately the inversion of h, the model constraint can be formulated as\nϕg · o = h−1(g · h(o)) =\n∆d(g · h(o)), ∀o ∈O, g ∈G,\n(3)\ntogether with further implementation of Φ as described in Section 4.1, the model constraint can be\nfulﬁlled. Secondly, The data constraint requires that all variations in the data can be generated by\ncompositions of some basic permutations generators. Previous VAE-based works (Higgins et al.,\n2017; Burgess et al., 2018; Kim & Mnih, 2018; Chen et al., 2018; Kumar et al., 2017) aim to generate\nthe data with independent generative factors, which is in line with the data constraint. Intuitively, if\nthe VAE-based model can generate the data from statistical independent basic latent units and each\nunit corresponds to the basic permutation generator, the data constraint may be satisﬁed. Based\non the intuition above, we prove that if the world state is independently sampled per dimension, the\nminimization of total correlation is a necessary condition for the data constraint (see Appendix E).\nTherefore, we can leverage existing VAE-based models to fulﬁll the data constraint to some extent for\nthe unsupervised setting. Lastly, to satisfy the group structure constraint, we derive a self-supervised\nIsomorphism Loss which can be incorporated into the VAE-based model as described in Section 4.2.\n4.1\nIMPLEMENTATION OF GROUP Φ\nThe key is to implement the group actions of G on Z into the VAE-based models, we need to map\nthe representation z to a group that is isomorphic to G (cyclic representation space). Therefore, we\nconstruct a function η to achieve this mapping. Moreover, this mapping is required to be differentiable,\nin order for back-propagation to be adopted for optimization. According to Group Theory, there\nis an isomorphism between G and the n-th root unity group: {exp((2πiz)/n)|z ∈Zm}, where\nn, m are the same as in G. Therefore, the representation z can be mapped to z by the function η as\nz = η(z) = exp((2πiz)/n) (see Figure 1 (a)). However, z can not be mapped to directly as it has\ncomplex numbers, but we can use Euler’s formula: exp((2πiz)/n) = sin((2πz)/n)+i cos((2πz)/n)\nto map z to its real and imaginary part, i.e., vector sin((2πz)/n) and cos((2πz)/n). The two vectors\nare concatenated and fed to the decoder.\nFor ease of implementation, the permutation group Φ can be approximately generated by compositions\nof generators, i.e., Φ =< ϕ1, ϕ2, . . . , ϕm >. Recall that the generator ϕi of group Φ is deﬁned as\nϕi · o = d(gi · h(o)) = d(h(o) + gi), ∀o ∈O, where gi is generator of dimension i in G, as shown\nin Figure 1 (a). For ϕi, we implement gi · h(o) by adding 1 (without loss of generality) to the i-th\ndimension of h(o), then make it cyclic by function η. Similarly, for ϕ−1\ni , we add the value of n −1.\n5\nPublished as a conference paper at ICLR 2022\n4.2\nIMPLEMENTATION OF THE ISOMORPHISM\nIn this section, to satisfy the group structure constraint (isomorphism), we derive two equivalent\nconstraints, which are then converted into an Isomorphism Loss LI. Many groups are uniquely\ndetermined by the properties of the generators, e.g., group G =< a, b|a2 = b2 = e, ab = ba >. In\naddition, since the group Φ is isomorphic to G, Φ is also expected to be commutative and cyclic. In\nlight of this, we derive two constraints on generators that are equivalent to the isomorphism condition,\nas described in Theorem 2. Please refer to Appendix C for the proof.\nTheorem 2 The deﬁned permutation group Φ =< ϕ1, ϕ2, . . . , ϕm > is isomorphic to G =\n(Z/nZ)m if and only if: (i) for ∀generators ϕi, ϕj ∈Φ, 1 ≤i, j ≤m, we have ϕiϕj = ϕjϕi, and\n(ii) ∀ϕi ∈Φ, 1 ≤i ≤m, we have ϕn\ni = e, where e is the identity element of group Φ.\nThe ﬁrst constraint requires the group Φ to be an abelian group (Judson, 2020). Therefore, we denote\nit as Abel constraint and the loss derived from it as the Abel Loss La. The second is a constraint on\nthe order of elements. We thus denote it as the Order constraint and the loss derived from it as the\nOrder Loss Lo. See Appendix F for a more detailed implementation.\nAbel Loss. For the Abel constraint: ∀ϕi, ϕj ∈Φ, 1 ≤i, j ≤m, we have ϕiϕj = ϕjϕi. We\nminimize ∥ϕi · (ϕj · o) −ϕj · (ϕi · o)∥, ∀o ∈O to meet the Abel constraint, as shown in Figure 1\n(b). The Abel Loss is the sum of the losses of all combinations of two generators. Denote the set of\ncombinations as C = {(i, j)|1 ≤i, j ≤m}. The Abel Loss is deﬁned as follows\nLa =\nX\no∈O\nX\n(i,j)∈C\n∥ϕi · (ϕj · o) −ϕj · (ϕi · o)∥.\n(4)\nOrder Loss. For the Order constraint: ∀ϕi ∈Φ, 1 ≤i ≤m, we have ϕn\ni = e, where e is the identity\nelement in group Φ (identity mapping). Note that with n times composition of ϕi, it is difﬁcult for the\ngradient to back-propagate. We thus use an approximation that uses 2 times of composition instead.\nWhen the autoencoder can do the reconstruction well, this approximation holds, see appendix E\nfor details. Similar to Abel Loss, we minimize ∥ϕi · (ϕn−1\ni\n· o) −o∥, ∀o ∈O to satisfy the Order\nconstraint. The whole process is illustrated in Figure 1 (b). However, the equation is not symmetrical\nand leads to bias. Therefore, we use the following symmetrical form instead:\nLo =\nX\no∈O\nX\n1≤i≤m\n\u0000∥ϕi · (ϕn−1\ni\n· o) −o∥+ ∥ϕn−1\ni\n· (ϕi · o) −o∥\n\u0001\n.\n(5)\nWith the above two loss functions optimized, the isomorphism condition is satisﬁed, which can be\nillustrated by Theorem 3. Please refer to Appendix D for the proof.\nTheorem 3 The following two conditions are equivalent: (i) ∀ϕi, ϕj ∈Φ, 1 ≤i, j ≤m, we have\nϕiϕj = ϕjϕi and ∀ϕi ∈Φ, 1 ≤i ≤m, we have ϕn\ni = e (ii) the Abel Loss function (Equation 4)\nand the Order Loss function (Equation 5) are optimized.\nSince the Abel Loss and Order Loss are equally important for satisfying the isomorphism condition,\nwe assign equal weight to them. Thus, the Isomorphism Loss is LI = Lo + La. With the\nimplementation of group Φ, the model constraint is satisﬁed. We optimize the Isomorphism Loss to\nsatisfy the group structure constraint. To further satisfy the data constraint to some extent as described\nin Section 4, we leverage VAE-based models and optimize their original loss (that minimizes the total\ncorrelation), denoted as LV AE. Therefore, the Total Loss is L = LV AE + γILI, where γI is the\nweight of Isomorphism Loss. We denote the above VAE-based implementation as Groupiﬁed VAE.\n5\nEXPERIMENTS\nWe ﬁrst verify the effectiveness of Groupiﬁed VAE quantitatively in learning disentangled representa-\ntions on several datasets and several VAE-based models. Then, we show its effectiveness qualitatively\non two typical datasets. After that, we perform a case study on the dSprites dataset to analyze the\neffectiveness, and conduct ablation studies on the losses and hyperparameters. For the performance\ncomparison of two downstream tasks (abstract reasoning Van Steenkiste et al. (2019) and fairness\nevaluation Locatello et al. (2019a)), and more comprehensive results, please see Appendix I.\n6\nPublished as a conference paper at ICLR 2022\n5.1\nDATASETS AND BASELINE METHODS\nTo evaluate our method, we consider several datasets: dSprites (Higgins et al., 2017), Shapes3D (Kim\n& Mnih, 2018), Cars3D (Reed et al., 2015), and the variants of dSprites introduced by Locatello et\nal. (Locatello et al., 2019b): Color-dSprites and Noisy-dSprites. Please refer to Appendix G for the\ndetails of the datasets.\nWe choose the following four baseline methods as representatives of the existing VAE-based models,\nwhich are denoted as Original VAEs. We verify the effectiveness of our implementation based on\nthose methods. β-VAE (Higgins et al., 2017) introduces a hyperparameter β in front of the KL\nregularizer of the VAE loss. It constrains the VAE information capacity to learn the most efﬁcient\nrepresentation. AnnealVAE (Burgess et al., 2018) progressively increases the bottleneck capacity\nso that the encoder learns new factors of variation while retaining disentanglement in previously\nlearned factors. FactorVAE (Burgess et al., 2018) and β-TCVAE (Chen et al., 2018) both penalize\nthe total correlation (Watanabe, 1960), but estimate it with adversarial training (Nguyen et al., 2010;\nSugiyama et al., 2012) and Monte-Carlo estimator respectively.\ndSprits\nDCI\nBetaVAE\nMIG\nFactorVAE\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nβ-VAE\n0.23 ± 0.10\n0.46 ± 0.085\n0.75 ± 0.083\n0.86 ± 0.051\n0.14 ± 0.097\n0.37 ± 0.089\n0.51 ± 0.098\n0.63 ± 0.089\nAnnealVAE\n0.28 ± 0.10\n0.39 ± 0.056\n0.84 ± 0.050\n0.87 ± 0.0067\n0.23 ± 0.10\n0.34 ± 0.061\n0.70 ± 0.094\n0.68 ± 0.058\nFactorVAE\n0.38 ± 0.10\n0.41 ± 0.074\n0.89 ± 0.040\n0.89 ± 0.020\n0.27 ± 0.092\n0.31 ± 0.061\n0.74 ± 0.068\n0.75 ± 0.075\nβ-TCVAE\n0.35 ± 0.065\n0.36 ± 0.11\n0.86 ± 0.026\n0.861 ± 0.038\n0.17 ± 0.067\n0.24 ± 0.093\n0.68 ± 0.098\n0.70 ± 0.098\nCars3d\nDCI\nBetaVAE\nMIG\nFactorVAE\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nβ-VAE\n0.18 ± 0.059\n0.24 ± 0.041\n0.99 ± 1.6e −3\n1.0 ± 0.0\n0.071 ± 0.032\n0.11 ± 0.032\n0.81 ± 0.066\n0.93 ± 0.034\nAnnealVAE\n0.22 ± 0.046\n0.25 ± 0.046\n0.99 ± 4e −4\n0.99 ± 1.5e −4\n0.074 ± 0.016\n0.10 ± 0.014\n0.82 ± 0.062\n0.87 ± 0.028\nFactorVAE\n0.21 ± 0.054\n0.25 ± 0.040\n0.99 ± 1e −4\n1.0 ± 0.0\n0.098 ± 0.027\n0.11 ± 0.033\n0.90 ± 0.039\n0.93 ± 0.034\nβ-TCVAE\n0.24 ± 0.049\n0.26 ± 0.046\n1.0 ± 0.0\n1.0 ± 0.0\n0.10 ± 0.021\n0.11 ± 0.033\n0.88 ± 0.040\n0.93 ± 0.034\nShapes3d\nDCI\nBetaVAE\nMIG\nFactorVAE\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nOriginal\nGroupiﬁed\nβ-VAE\n0.44 ± 0.176\n0.56 ± 0.10\n0.91 ± 0.072\n0.90 ± 0.045\n0.28 ± 0.18\n0.42 ± 0.15\n0.82 ± 0.098\n0.82 ± 0.043\nAnnealVAE\n0.52 ± 0.051\n0.60 ± 0.078\n0.82 ± 0.076\n0.89 ± 0.086\n0.48 ± 0.047\n0.50 ± 0.052\n0.75 ± 0.074\n0.83 ± 0.066\nFactorVAE\n0.47 ± 0.10\n0.49 ± 0.065\n0.86 ± 0.055\n0.80 ± 0.075\n0.33 ± 0.13\n0.43 ± 0.11\n0.81 ± 0.056\n0.79 ± 0.066\nβ-TCVAE\n0.66 ± 0.10\n0.72 ± 0.061\n0.97 ± 0.039\n0.96 ± 0.042\n0.40 ± 0.18\n0.47 ± 0.090\n0.89 ± 0.064\n0.90 ± 0.046\nTable 1: Performance (mean ± std) on different datasets and different models with different metrics.\nWe evaluate β-VAE, AnnealVAE, FactorVAE, and β-TCVAE on dSprites, Cars3d, Shapes3d, Noisy-\ndSprites, and Color-dSprites for 1800 settings. These settings include different random seeds and\nhyperparameters, refer to Appendix G for the details. We only show the ﬁrst three datasets here. For\nmore results, please refer to Appendix I.\n5.2\nQUANTITATIVE EVALUATIONS\nThis section performs quantitative evaluations on the datasets and models introduced with different\nrandom seeds and different hyperparameters. Then, we evaluate the performance of the Original\nand Groupiﬁed VAEs in terms of several popular metrics: BetaVAE score (Higgins et al., 2017),\nDCI disentanglement Eastwood & Williams (2018) (DCI in short), MIG (Chen et al., 2018), and\nFactorVAE score (Kim & Mnih, 2018). We assign three or four hyperparameter settings for each\nmodel on each dataset. We run it with ten random seeds for each hyperparameter setting to minimize\nthe inﬂuence of random seeds. Therefore, we totally run ((3 × 10 × 3 + 10 × 3 × 3) × 2) × 5 = 1800\nmodels. We evaluate each metric’s mean and variance for each model on each dataset to demonstrate\nthe effectiveness of our method. As shown in Table 1, these Groupiﬁed VAEs have better performance\n(numbers marked bold in Table 1) than the Original VAEs on almost all the cases.\nOn Shapes3d, the Groupiﬁed VAEs outperform the Original ones on all the metrics except for\nBetaVAE scores, suggesting some disagreement between BetaVAE scores and other metrics. Similar\ndisagreement is also observed between the variances of MIG and other metrics on Cars3d. Note that\nthe qualitative evaluation in Appendix J shows that the disentanglement ability of Groupiﬁed VAEs is\nbetter on Shapes3d and Cars3d.\n7\nPublished as a conference paper at ICLR 2022\n(a) Original\n(b) Groupiﬁed\nFigure 2: Visual traversal comparison between\nOriginal and Groupiﬁed β-TCVAE. The traversal\nresults of Groupiﬁed VAEs are less entangled.\nOriginal\nGroupiﬁed\nFigure 3: Traversal results of two factors (ﬂoor\ncolor, scale) of Original and Groupiﬁed β-\nTCVAE. The traversal results of Groupiﬁed\nVAEs are cyclic.\n5.3\nQUALITATIVE EVALUATIONS\nWe qualitatively show the Groupiﬁed VAEs achieve better disentanglement than the Original ones.\nAs shown in Figure 2, the traversal results of Groupiﬁed β-TCVAE on Shape3d and Car3d are less\nentangled. For more qualitative evaluation, please refer to Appendix J. To verify that the Groupiﬁed\nVAEs learn a cyclic representation space (where n = 10), we provide the traversal results of [0,18]\nwith a step of 2 for both the Groupiﬁed and Original β-TCVAE on Shape3d in Figure 3. We observe\nthat the traversal results of Groupiﬁed VAEs are of high quality with a period of 10 (equal to n).\nHowever, the Original VAEs generate low-quality images without cyclicity. For the comparison of\nthe results on CelebA (real-world datasets), please see appendix J.\n5.4\nVISUALIZATION OF THE LEARNED REPRESENTATION SPACE\nTo understand how our theoretical framework helps the existing VAE-based models to improve the\ndisentanglement ability, we take dSprites as an example, visualize the learned representation space,\nand show the typical score distributions of the metrics. First, we visualize the space spanned by the\nthree most dominant factors (x position, y position, and scale).\nAs shown in Figure 5 (for more results, please refers to Appendix L), the spaces learned by the Original\nVAEs collapse, while the spaces of the Groupiﬁed VAEs only bend a little bit. The main reason\nis that the Isomorphism Loss, serving as a self-supervision signal, suppresses the representation\nspace distortion and encourages the disentanglement of the learned factors. As Figure 4 shows,\nthe Groupiﬁed VAEs consistently achieve better mean performance with smaller variances. The\nisomorphism reduces the search space of the network so that the Groupiﬁed VAEs converge to the\nideal disentanglement solution.\n(a) BetaVAE score\n(b) DCI disentanglement\n(c) MIG\n(d) FactorVAE score\nFigure 4: Performance distribution of Original and Groupiﬁed AnnealVAE on dSprites (demonstrated\nby the Violin Plot (Hintze & Nelson, 1998)). Variance is due to different hyperparameters and random\nseeds. We observe that Groupiﬁed AnnealVAE improves the average performance with smaller\nvariance in terms of BetaVAE score (a), DCI disentanglement (b), and MIG (c), and has a comparable\nmean performance with smaller variance in terms of FactorVAE score (d).\nOriginal\nGroupiﬁed\nFactor Size n = 10\nn = 5\nn = 10\nn = 15\nw/o Abel\nw/o Order\nGroupiﬁed\nDCI\n0.27 ± 0.10\n0.34 ± 0.062\n0.38 ± 0.055\n0.38 ± 0.064\n0.28 ± 0.11\n0.34 ± 0.056\n0.38 ± 0.055\nTable 2: Ablation study on the factor size n and Isomorphism Loss. DCI disentanglement is listed\n(mean ± std).\n8\nPublished as a conference paper at ICLR 2022\n(a) C = 10, Groupiﬁed\n(b) C = 20, Groupiﬁed\n(c) C = 25, Groupiﬁed\n(d) C = 30, Groupiﬁed\n(e) C = 10, Original\n(f) C = 20, Original\n(g) C = 25, Original\n(h) C = 30, Original\nFigure 5: The representation space spanned by the learned factors by Original (bottom row) and\nGroupiﬁed AnnealVAE (top row). The position of each point is the disentangled representation of the\ncorresponding image. An ideal result is all the points form a cube and color variation is continuous.\nThe increase of C (a hyperparameter of AnnealVAE) results in a collapse of representation space\nof the Original VAE. The collapse is suppressed by the Isomorphism Loss, which leads to better\ndisentanglement.\n5.5\nABLATION STUDY\nWe perform an ablation study on the assumed total number of possible values for a factor (factor size)\nn, Abel Loss La, and Order Loss Lo. We take the AnnealVAE trained on dSprites as an example.\nWe only consider the DCI disentanglement metric here. We investigate the inﬂuence of factor size n.\nBesides, to evaluate the effectiveness of the two constraints, the models with the Abel Loss alone or\nOrder Loss alone added are also evaluated. In this setting, we ﬁx n to 10. We compute the mean and\nvariance of the performance for 30 settings of hyperparameters and random seeds. Table 2 shows\nthat the isomorphism plays a role of cycle consistency in the representation space, leading to better\ndisentanglement. The performance is robust to the factor size n, as the models learn to adapt to\ndifferent n in the training process. The models with only the Abel Loss or Order Loss applied have\nimproved performance compared to the originals. The former (Abel Loss) performs better than the\nlatter, suggesting that commutativity plays a more important role. Note that the number of factors m\ncan be learned and is not a hyperparameter. See Appendix F for details. γI is empirically set to 1.\n6\nCONCLUSION\nIn this paper, we have opened the possibility of applying group-based deﬁnition to unsupervised\ndisentanglement by proposing a theoretical framework. The group structure and model constraint\nin the framework are effective for existing VAE-based unsupervised disentanglement methods. In\naddition, by establishing the feasibility of learning the representation conforming to the deﬁnition\nin unsupervised settings, we have exhibited the consistently better mean performance with lower\nvariance attributed to the deﬁnition. We believe our work constitutes a promising step towards\nunsupervised disentanglement with theoretical guarantee. As to the limitation, we only provide\na necessary condition for the data constraint, as a result, we can not address the unidentiﬁability\nproblem. Tackling the unidentiﬁability problem with the group-based deﬁnition is beyond the scope\nof this work, we will leave it as future work. In addition, a natural extension of our framework is to\nuse lie group Hall (2015) (which is also a manifold) to extend our framework.\n9\nPublished as a conference paper at ICLR 2022\nREFERENCES\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information\nbottleneck. arXiv preprint arXiv:1612.00410, 2016.\nPhilip W Anderson. More is different. Science, 177(4047):393–396, 1972.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. TPAMI, 35(8):1798–1828, 2013.\nChristopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-\njardins, and Alexander Lerchner. Understanding disentangling in beta -vae. arXiv preprint\narXiv:1804.03599, 2018.\nHugo Caselles-Dupr´e, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled\nrepresentation learning requires interaction with environments. In NeurIPS, pp. 4606–4615, 2019.\nRicky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of\ndisentanglement in variational autoencoders. In NeurIPS, pp. 2610–2620, 2018.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:\ninterpretable representation learning by information maximizing generative adversarial nets. In\nNeurPIS, 2016.\nStephane Deny Diane Bouchacourt, Mark Ibrahim. Addressing the topological defects of disentan-\nglement, 2021. URL https://openreview.net/forum?id=cbdp6RLk2r7.\nDavid S Dummit and Richard M Foote. Abstract algebra, volume 1999. Prentice Hall Englewood\nCliffs, NJ, 1991.\nCian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of\ndisentangled representations. In ICLR, 2018.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, abs/1406.2661,\n2014. URL http://arxiv.org/abs/1406.2661.\nKlaus Greff, Rapha¨el Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess, Daniel Zoran,\nLoic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning\nwith iterative variational inference. arXiv preprint arXiv:1903.00450, 2019.\nBrian Hall. Lie groups, Lie algebras, and representations: an elementary introduction, volume 222.\nSpringer, 2015.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. ICLR, 2017.\nIrina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,\nand Alexander Lerchner. Towards a deﬁnition of disentangled representations. arXiv preprint\narXiv:1812.02230, 2018.\nJerry L Hintze and Ray D Nelson. Violin plots: a box plot-density trace synergism. The American\nStatistician, 52(2):181–184, 1998.\nThomas W Judson. Abstract algebra: theory and applications. Virginia Commonwealth University\nMathematics, 2020.\nIlyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders\nand nonlinear ica: A unifying framework. In AISTATS, pp. 2207–2217, 2020.\nValentin Khrulkov, Leyla Mirvakhabova, Ivan Oseledets, and Artem Babenko. On disentangled\nrepresentations extracted from pretrained gans, 2021. URL https://openreview.net/\nforum?id=VCAXR34cp59.\n10\nPublished as a conference paper at ICLR 2022\nHyunjik Kim and Andriy Mnih. Disentangling by factorising. In ICML, 2018.\nMinyoung Kim, Yuting Wang, Pritish Sahu, and Vladimir Pavlovic. Bayes-factor-vae: Hierarchical\nbayesian deep auto-encoder models for factor disentanglement. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 2979–2987, 2019.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nAbhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled\nlatent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\nmachines that learn and think like people. Behavioral and brain sciences, 40, 2017.\nWonkwang Lee, Donggyun Kim, Seunghoon Hong, and Honglak Lee. High-ﬁdelity synthesis with\ndisentangled representation. In ECCV, 2020.\nJos´e Lezama. Overcoming the disentanglement vs reconstruction trade-off via jacobian supervision.\nIn International Conference on Learning Representations, 2018.\nYu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank Wang.\nCross-dataset person re-\nidentiﬁcation via unsupervised pose disentanglement and adaptation. In ICCV, 2019.\nZinan Lin, Kiran Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr and modelcentrality:\nSelf-supervised model training and selection for disentangling gans. In ICML, 2020.\nFrancesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Sch¨olkopf, and\nOlivier Bachem. On the fairness of disentangled representations. In NeurIPS, pp. 14611–14624,\n2019a.\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch¨olkopf,\nand Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentan-\ngled representations. In ICML, pp. 4114–4124, 2019b.\nEmile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. Disentangling disentanglement in\nvariational autoencoders. In ICML, pp. 4402–4412, 2019.\nWillard Miller. Symmetry groups and their applications. Academic Press, 1973.\nXuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals\nand the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,\n56(11):5847–5861, 2010.\nEmmy Noether. Der endlichkeitssatz der invarianten endlicher gruppen. Mathematische Annalen, 77\n(1):89–92, 1915.\nMatthew Painter, Adam Prugel-Bennett, and Jonathon Hare. Linear disentangled representations and\nunsupervised action estimation. NeurIPS, 33, 2020.\nDavid Pfau, Irina Higgins, Alex Botev, and S´ebastien Racani`ere. Disentangling by subspace diffusion.\nNeurIPS, 33, 2020.\nRobin Quessard, Thomas D Barrett, and William R Clements. Learning group structure and disentan-\ngled representations of dynamical environments. arXiv preprint arXiv:2002.06991, 2020.\nScott E. Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In NeurIPS,\n2015.\nDanilo Jimenez Rezende and Fabio Viola. Taming vaes. arXiv preprint arXiv:1810.00597, 2018.\nHuajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang,\nand Tarek Abdelzaher. Controlvae: Controllable variational autoencoder. In International Confer-\nence on Machine Learning, pp. 8655–8664. PMLR, 2020.\n11\nPublished as a conference paper at ICLR 2022\nAkash Srivastava, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu, Bernhard Egger, Prasanna\nSattigeri, Josh Tenenbaum, David D Cox, and Dan Gutfreund. Improving the reconstruction of\ndisentangled representation learners via multi-stage modelling. arXiv preprint arXiv:2010.13187,\n2020.\nMasashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density-ratio matching under the bregman\ndivergence: a uniﬁed framework of density-ratio estimation. Annals of the Institute of Statistical\nMathematics, 64(5):1009–1044, 2012.\nRaphael Suter, Djordje Miladinovic, Bernhard Sch¨olkopf, and Stefan Bauer. Robustly disentangled\ncausal mechanisms: Validating deep representations for interventional robustness. In ICML, pp.\n6056–6065. PMLR, 2019.\nSjoerd Van Steenkiste, Francesco Locatello, J¨urgen Schmidhuber, and Olivier Bachem. Are dis-\nentangled representations helpful for abstract visual reasoning? In NeurIPS, pp. 14245–14258,\n2019.\nSatosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research\nand development, 4(1):66–82, 1960.\nYang Zou, Xiaodong Yang, Zhiding Yu, BVK Kumar, and Jan Kautz. Joint disentangling and\nadaptation for cross-domain person re-identiﬁcation. In ECCV, 2020.\nREFERENCES\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information\nbottleneck. arXiv preprint arXiv:1612.00410, 2016.\nPhilip W Anderson. More is different. Science, 177(4047):393–396, 1972.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. TPAMI, 35(8):1798–1828, 2013.\nChristopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-\njardins, and Alexander Lerchner. Understanding disentangling in beta -vae. arXiv preprint\narXiv:1804.03599, 2018.\nHugo Caselles-Dupr´e, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled\nrepresentation learning requires interaction with environments. In NeurIPS, pp. 4606–4615, 2019.\nRicky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of\ndisentanglement in variational autoencoders. In NeurIPS, pp. 2610–2620, 2018.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:\ninterpretable representation learning by information maximizing generative adversarial nets. In\nNeurPIS, 2016.\nStephane Deny Diane Bouchacourt, Mark Ibrahim. Addressing the topological defects of disentan-\nglement, 2021. URL https://openreview.net/forum?id=cbdp6RLk2r7.\nDavid S Dummit and Richard M Foote. Abstract algebra, volume 1999. Prentice Hall Englewood\nCliffs, NJ, 1991.\nCian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of\ndisentangled representations. In ICLR, 2018.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, abs/1406.2661,\n2014. URL http://arxiv.org/abs/1406.2661.\nKlaus Greff, Rapha¨el Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess, Daniel Zoran,\nLoic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning\nwith iterative variational inference. arXiv preprint arXiv:1903.00450, 2019.\n12\nPublished as a conference paper at ICLR 2022\nBrian Hall. Lie groups, Lie algebras, and representations: an elementary introduction, volume 222.\nSpringer, 2015.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. ICLR, 2017.\nIrina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,\nand Alexander Lerchner. Towards a deﬁnition of disentangled representations. arXiv preprint\narXiv:1812.02230, 2018.\nJerry L Hintze and Ray D Nelson. Violin plots: a box plot-density trace synergism. The American\nStatistician, 52(2):181–184, 1998.\nThomas W Judson. Abstract algebra: theory and applications. Virginia Commonwealth University\nMathematics, 2020.\nIlyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders\nand nonlinear ica: A unifying framework. In AISTATS, pp. 2207–2217, 2020.\nValentin Khrulkov, Leyla Mirvakhabova, Ivan Oseledets, and Artem Babenko. On disentangled\nrepresentations extracted from pretrained gans, 2021. URL https://openreview.net/\nforum?id=VCAXR34cp59.\nHyunjik Kim and Andriy Mnih. Disentangling by factorising. In ICML, 2018.\nMinyoung Kim, Yuting Wang, Pritish Sahu, and Vladimir Pavlovic. Bayes-factor-vae: Hierarchical\nbayesian deep auto-encoder models for factor disentanglement. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 2979–2987, 2019.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nAbhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled\nlatent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\nmachines that learn and think like people. Behavioral and brain sciences, 40, 2017.\nWonkwang Lee, Donggyun Kim, Seunghoon Hong, and Honglak Lee. High-ﬁdelity synthesis with\ndisentangled representation. In ECCV, 2020.\nJos´e Lezama. Overcoming the disentanglement vs reconstruction trade-off via jacobian supervision.\nIn International Conference on Learning Representations, 2018.\nYu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank Wang.\nCross-dataset person re-\nidentiﬁcation via unsupervised pose disentanglement and adaptation. In ICCV, 2019.\nZinan Lin, Kiran Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr and modelcentrality:\nSelf-supervised model training and selection for disentangling gans. In ICML, 2020.\nFrancesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Sch¨olkopf, and\nOlivier Bachem. On the fairness of disentangled representations. In NeurIPS, pp. 14611–14624,\n2019a.\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch¨olkopf,\nand Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentan-\ngled representations. In ICML, pp. 4114–4124, 2019b.\nEmile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. Disentangling disentanglement in\nvariational autoencoders. In ICML, pp. 4402–4412, 2019.\nWillard Miller. Symmetry groups and their applications. Academic Press, 1973.\n13\nPublished as a conference paper at ICLR 2022\nXuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals\nand the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,\n56(11):5847–5861, 2010.\nEmmy Noether. Der endlichkeitssatz der invarianten endlicher gruppen. Mathematische Annalen, 77\n(1):89–92, 1915.\nMatthew Painter, Adam Prugel-Bennett, and Jonathon Hare. Linear disentangled representations and\nunsupervised action estimation. NeurIPS, 33, 2020.\nDavid Pfau, Irina Higgins, Alex Botev, and S´ebastien Racani`ere. Disentangling by subspace diffusion.\nNeurIPS, 33, 2020.\nRobin Quessard, Thomas D Barrett, and William R Clements. Learning group structure and disentan-\ngled representations of dynamical environments. arXiv preprint arXiv:2002.06991, 2020.\nScott E. Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In NeurIPS,\n2015.\nDanilo Jimenez Rezende and Fabio Viola. Taming vaes. arXiv preprint arXiv:1810.00597, 2018.\nHuajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang,\nand Tarek Abdelzaher. Controlvae: Controllable variational autoencoder. In International Confer-\nence on Machine Learning, pp. 8655–8664. PMLR, 2020.\nAkash Srivastava, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu, Bernhard Egger, Prasanna\nSattigeri, Josh Tenenbaum, David D Cox, and Dan Gutfreund. Improving the reconstruction of\ndisentangled representation learners via multi-stage modelling. arXiv preprint arXiv:2010.13187,\n2020.\nMasashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density-ratio matching under the bregman\ndivergence: a uniﬁed framework of density-ratio estimation. Annals of the Institute of Statistical\nMathematics, 64(5):1009–1044, 2012.\nRaphael Suter, Djordje Miladinovic, Bernhard Sch¨olkopf, and Stefan Bauer. Robustly disentangled\ncausal mechanisms: Validating deep representations for interventional robustness. In ICML, pp.\n6056–6065. PMLR, 2019.\nSjoerd Van Steenkiste, Francesco Locatello, J¨urgen Schmidhuber, and Olivier Bachem. Are dis-\nentangled representations helpful for abstract visual reasoning? In NeurIPS, pp. 14245–14258,\n2019.\nSatosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research\nand development, 4(1):66–82, 1960.\nYang Zou, Xiaodong Yang, Zhiding Yu, BVK Kumar, and Jan Kautz. Joint disentangling and\nadaptation for cross-domain person re-identiﬁcation. In ECCV, 2020.\n14\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2021-02-20",
  "updated": "2022-03-15"
}