{
  "id": "http://arxiv.org/abs/2102.10535v1",
  "title": "Automatic Code Generation using Pre-Trained Language Models",
  "authors": [
    "Luis Perez",
    "Lizi Ottens",
    "Sudharshan Viswanathan"
  ],
  "abstract": "Recent advancements in natural language processing \\cite{gpt2} \\cite{BERT}\nhave led to near-human performance in multiple natural language tasks. In this\npaper, we seek to understand whether similar techniques can be applied to a\nhighly structured environment with strict syntax rules. Specifically, we\npropose an end-to-end machine learning model for code generation in the Python\nlanguage built on-top of pre-trained language models. We demonstrate that a\nfine-tuned model can perform well in code generation tasks, achieving a BLEU\nscore of 0.22, an improvement of 46\\% over a reasonable sequence-to-sequence\nbaseline. All results and related code used for training and data processing\nare available on GitHub.",
  "text": "Automatic Code Generation using Pre-Trained Language Models\nLuis Perez\nDepartment of Computer Science\nStanford University\nluis0@stanford.edu\nLizi Ottens\nDepartment of Computer Science\nStanford University\nlottens@stanford.edu\nSudharshan Viswanathan\nDepartment of Computer Science\nStanford University\nviswans@stanford.edu\nAbstract\nRecent advancements in natural language processing\n[1] [2] have led to near-human performance in multiple\nnatural language tasks. In this paper, we seek to under-\nstand whether similar techniques can be applied to a highly\nstructured environment with strict syntax rules.\nSpeciﬁ-\ncally, we propose an end-to-end machine learning model for\ncode generation in the Python language built on-top of pre-\ntrained language models. We demonstrate that a ﬁne-tuned\nmodel can perform well in code generation tasks, achieving\na BLEU score of 0.22, an improvement of 46% over a rea-\nsonable sequence-to-sequence baseline. All results and re-\nlated code used for training and data processing are avail-\nable on GitHub. 1\n1. Introduction\nAutomating even small parts of software development is\nan active research area [3], with multiple approaches pro-\nposed methods (See Section 1). Succeeding in the automa-\ntion of even small tasks can save time for countless software\nengineers, which translates to saved resources across mul-\ntiple industries. Furthermore, as software continues to eat\nthe world 2 and demand for experienced software develop-\ners continues to outpace supply, automatic code generation\nwill become increasingly important.\nIn this paper, we propose a machine learning model to\nautomate the task of writing code by assisting developers\nin writing individual units of functionality (or “functions”).\nAutomating code generation can take on many forms, from\n1See shared repository located at https://github.com/kandluis/code-gen.\n2See article by Marc Andreessen.\nauto-completing lines of source code to generating lines of\nsource code from comments, generating source code from\nUI images, or generating unit tests from source code. In this\nproject, we aim to take the initial lines of code (a function\nsignature) along with a doc-string (function documentation)\nand generate the corresponding function body. In order to\ndo this, we use a pre-trained language model and ﬁne-tune it\non a canonical corpus of Python code scraped from GitHub\n[4].\n2. Background\nA primary challenge in code generation is that it is still\nan active area of research, with many possible solutions and\nongoing investigation [5]. State of the art solutions have not\nyet come close to automating basic tasks software engineers\nperform on a daily basis.\n2.1. Traditional Code Completion\nThe most traditional and well-known approach used by\nmultiple IDEs across a range of languages simply consists\nof token completion based on structured information ob-\ntained from static analysis of code. For example, when a\ndeveloper types a sequence of characters, the system will\nattempt to ﬁnd near-matching strings corresponding to func-\ntion deﬁnitions and propose completing these function calls.\nSimilarly, for object methods, on the typing of the acces-\nsor token (such as “-¿” or “.”), the IDE will propose auto-\ncompleting different methods belonging to the object.\nThe biggest drawback of these approaches is that they\nlack true understanding of the programmers intent, and also\nlack context relating to the surrounding code other than that\nfrom heuristics by the tool’s developers.\n1\narXiv:2102.10535v1  [cs.CL]  21 Feb 2021\n2.2. Using Machine Learning for Code Search\nAnother approach taken in multiple papers in the liter-\nature [4] involves framing the problem as a code search\nproblem. Rather than trying to generate code or complete\nthe code that the developer is making, we can re-frame the\nproblem as one of searching for relevant pre-existing snip-\npets. This is the primary approach we take in three of our\nbaseline models.\n2.3. Using Machine Learning for Code Generation\nOther more novel approaches from literature [5] are typ-\nically applied to restricted language domains, and have\nmassive complexity in evaluation results, etc. Speciﬁcally,\nwhile pre-trained models are trained on free-form language\ndata, programming languages often utilize non-natural vari-\nable names, function names, and syntax with more structure\n[5]. Work in this area has focused on creating more struc-\ntured models that take advantage of speciﬁc architectures\n[6]. In [7], the authors work to ﬁrst decompose the input\nsequence of text tokens for the context into a tree-like struc-\nture. Other approaches involve restricting the output of the\nmodel to a context-free grammar (CFG) or domain-speciﬁc\nlanguage (DSL) [8]. A code generation model’s output must\nadhere to a very speciﬁc form in order to be syntactically\ncorrect.\nIn this paper, we instead focus on taking a different ap-\nproach. As has been demonstrated by ever-increasing sizes\nof language models, we focus on improving the perfor-\nmance on the code prediction task by making use of pre-\ntrained language models that are then ﬁne-tuned on code.\n2.4. Dataset and Feature\nIn this project, we are leveraging the CodeSearchNet\ndataset [4]. The dataset consists of 2 million (comment,\ncode) pairs from open source libraries, ranging in languages\nfrom Python to Javascript, PHP, Java, Go and Ruby. Median\ncode-length consists of 60-100 text tokens, with 95% code-\nlength of up to 350 tokens. Median documentation length\nconsists of 10 text tokens. The distributions of methods and\n(comment, code) pairs across programming language are vi-\nsualized in Figure 3.\nWe restrict our dataset to samples in the Python program-\nming language rather than the others available. Focusing\non Python, there are over 1M methods and approximately\n500k (comment, code) pairs that make up our dataset. We\nmake this decision both for practical and modeling reasons.\nFrom a practical perspective, restricting to a reasonably-\nsized dataset focused on a single-language domains permits\nfor more thorough ablation studies. From a modeling per-\nspective, we belief that transfer learning from natural lan-\nguage to a programming language such as Python is an eas-\nier task to accomplish.\n3. Methodology\nIn this section, we explain our methodology for multiple\nexperiments and baselines proposed as well as details on the\ntraining data and distribution.\n3.1. CodeSearchNet Models\nFigure 4 explains the general architecture of the base-\nline models from the CodeSearchNet task. We successfully\ntrained and evaluated two baselines: Neural-Bag-Of-Words\nand an RNN-based baseline. See Section 4.\nGenerally speaking, the baselines models take as input\nexamples of (comments, code) pairs and learn to retrieve a\nspeciﬁc code snippet. Each programming language has its\nown encoder network (see three columns to the right in Fig-\nure 4), which are tasked with encoding a set of candidate\ncode snippets. They are then combined through a dot prod-\nuct operation with the embedding generated by the query\n(docstring) encoder to produce a matrix comparison.\nThe matrix diagonal serves as the scores of each query\ndoc string/code snippet. Through this methodology, these\nbaseline models are able to extract meaningful information\nand learn a joint distribution over the query and comment\npairs. We train these models as a baseline since we believe\nthey will be useful in the downstream task of code genera-\ntion. The models are trained on the following loss function:\n−1\nN\nX\ni\nlog\n \nexp(Ec(cT\ni )Eq(di))\nP\nj exp(Ec(cT\nj )Eq(dj))\n!\n(1)\n3.2. From Scratch RNN Models\nThe above baseline is useful only in the sense that it\nwould allow our system to ﬁnd pre-existing code snippets\nwhich might be relevant to the developer. Since our goal is\nrather to make novel code, we propose a different baseline\nbased on a more traditional sequence-to-sequence model.\nIn this case, we use a traditional RNN architecture which\ntakes as input individual characters. The reason we take this\napproach is to circumvent the need to learn word-level em-\nbeddings. Furthermore, we hypothesize that making use of\nentire words, from NLP models, will actually harm the per-\nformance of the model for code generation. The primary\nreason for this being that most of the syntax involved in\nwriting code does not generally map directly to the English\nlanguage. Concretely, we encode each character present in\nthe training data as a 1-of-k encoding (one-hot encoding)\nand feed them into an RNN one at a time. Our output will\nbe a k-dimensional output vector corresponding to a proba-\nbility distribution over the entire set of characters.\nFor the model architecture, we sweep over multiple types\nof RNN cells, including LSTM, RNN, and GRU. We ﬁnd\nthe best performing model to consists of an LSTM-based\nmodel using a hidden state size of 128 with two hidden lay-\ners in the internal RNN cell. Our training takes place using\nsequences of 50 characters, sampled at random from our in-\nput code. Given a sequence from i to i + 50, the model is\ntrained to predict the sequence from i + 1 to i + 51. This\nmeans we have a many-to-many sequence model (See Fig-\nure 6.2.1). We use batch size of 50 and train for a total of\n50 epochs.\nTo avoid issues with gradient explosion and stabilize\ntraining, we make liberal use of gradient-clipping. In par-\nticular, we clip all gradients to an absolute size of 5.\nWe sweep over learning rates and ﬁnd that a started\nlearning rate of 0.002 with an exponentially decaying\nschedule appears to perform best as measured by a held-\nout validation set. We use a decay rate of 0.97 per epoch.\nWe also experiment with the use of dropout, but ﬁnd little\nimpact on ﬁnal performance.\n3.3. Fine-Tuned Pre-Trained Large Language Mod-\nels\nOur ﬁnal approach relies on the use of pre-trained lan-\nguage models. We ﬁne tune our code generator using the\nsmall GPT-2 model with 117 million parameters. Using\nsuch a large backbone and continuing to ﬁne tune allows us\nto generate synthetic code samples with even higher quality,\ntreating programming languages as another speciﬁc domain\nalongside encyclopedia articles, news or books.\nFigure 1. Decoder-Only Architecture used by GPT-2.\nThe general architecture of the GPT-2 model consists of\na sequence-to-sequence predictive task based on the trans-\nformer architecture [9] [1]. However, it consists solely of\nthe 12-layer decoder-only, as visualized in Figure 1. Each\nlayer has 12 independent attention heads, leading to 144 dis-\ntinct attention patterns. By making use of an attention-based\nframework, the model is more adept at dealing with long-\nrange dependencies. This is because the attention mecha-\nnism allows the model to focus on the encoding of any of\nthe input sequence tokens.\n4. Results\nCodeSearchNet provides a good starting point as we are\nable to train different models on the input code streams. We\ntrained a simple LSTM model as well as a neural bag of\nwords model on a combination of all the available (code,\ndocumentation) pairs. For details on these simple baselines,\nplease see Appendix Section 6.1.\n4.1. Code Generation with Char-RNN\nAs both of the above baselines focus on understanding\nand extracting useful embeddings for our overall task, our\nprimary baseline consists of a straight-forward sequence-to-\nsequence model. Given that code typically does not consist\nof English words and can instead have quite a varied syntax,\nour baseline model is a model which uses character level\nembedding, so it is character aware [10].\nDue to computational constraints, we train only on the\nPython subset of the data and only on 10% of the total data\navailable. For the char-rnn model [10], this corresponds to\naround 50MB of raw text, or 78,357,395 characters with\n1,618 distinct symbols. Figure 9 shows the training and\nvalidation losses on the model. The loss is simply a soft-\nmax loss on the 1,618 characters for a sequence of length\n128 (the model is trained on sequences of length 128 by\ndefault). Figure 10 shows the perplexity, or the amount of\nmeaningful information encoded.\nWe include a sample generated from the best performing\nmodel for reference (See Section 2 in Appendix). A hyper-\nparameter tuning of learning rate and batch side for a total\nof 20 epochs has ﬁnal measured performance as shown in\nTable 4.1.\nBatch\nSize\nStarter\nLearning\nRate\nRegularization\nWeight\nBLEU Score\non Train\nBLEU Score\non Eval\n64\n0.02\n0.1\n0.022\n0.012\n64\n0.02\n0.01\n0.023\n0.019\n64\n0.002\n0.1\n0.034\n0.028\n64\n0.002\n0.01\n0.037\n0.012\n64\n0.0002\n0.1\n0.09\n0.073\n64\n0.0002\n0.01\n0.094\n0.014\n128\n0.02\n0.1\n0.024\n0.021\n128\n0.02\n0.01\n0.021\n0.013\n128\n0.002\n0.1\n0.033\n0.029\n128\n0.002\n0.01\n0.038\n0.011\n128\n0.0002\n0.1\n0.117\n0.093\n128\n0.0002\n0.01\n0.113\n0.034\n4.2. Code Generation with GPT-2\nWe have been working with the publicly available small\nGPT-2 model with 117 million parameters. We trained us-\ning the small GPT-2 model for 100,000 mini-batch itera-\ntions with a batch size of 2. We have included some sample\ncode that our model generated directly in the report. Qual-\nitatively, our model generates code which is far more rea-\nsonable than our baseline. The generated code is novel,\nas veriﬁed by doing n-gram overlap analysis between the\ngenerated code and the training dataset. We also note that\nthe model learns appropriate understanding of Python syn-\ntax, with uses of if-statements, function and method calls,\nas well as regularly commented code. For full output, see\nAppendix Section 6.2.\nWe observed that the idea of using Byte Pair encod-\ning as used in GPT-2 is a much better strategy to generate\ncode than just using characters, while of course the size of\nthe models itself has a very observable effect in generating\nPython-like code.\nOverall, the GPT-2 model quickly achieves performance\nthat’s much better than the baseline.\nContinued training\nof the model shows that our BLEU score performance will\ncontinue to increase, as seen in Figure 2\nFigure 2. BLEU Score During Training of GPT-2 Based Model for\nPython Code Generation\n5. Conclusions\nIn this paper, we explore the problem of automatically\ncompleting a function from the given function signature and\nhuman-readable documentation. We ﬁnd the best perform-\ning model to be a ﬁne-tuned version GPT-2, a transformer-\nbased NLP model which is trained to generate natural text\non an extremely large dataset.\nDespite the fact that our\nmodel focuses speciﬁcally on code rather than natural lan-\nguage, we hypothesize that it is able to treat programming\nlanguage as another speciﬁc domain alongside the encyclo-\npedia articles, news or books that its backbone has been\ntrained on. We are able to achieve a BLEU score of 0.22,\nimproving our baseline by ¿40%.\n6. Contributions\nAll team member contributed equally to this project.\nBaselines from the CodeSearchNet models for code search\nwere trained and tuned by Luis Perez and Sudharshan\nViswanathan. Data analysis and understanding of the fea-\ntures (including histograms, distribution of tokens, and\nother data insights) was primarily performed by Lizi Ottens.\nTraining of the baseline char-rnn model, as well as anal-\nysis of results and discussion was contributed primarily by\nLuis Perez. Fine-tuning and training with the small and\nmedium GPT-2 models was primarily explored and ana-\nlyzed by Lizi Ottens and Sudharshan Viswanathan.\nAll written submissions were co-written by all three au-\nthors.\nReferences\n[1] Alec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners. None, 2019.\n1, 3\n[2] Kenton Lee Kristina Toutanova Jacob Devlin, Ming-\nWei Chang. BERT: pre-training of deep bidirectional\ntransformers for language understanding.\nCoRR,\nabs/1810.04805, 2018. 1\n[3] Miltiadis Allamanis, Earl T. Barr, Premkumar T. De-\nvanbu, and Charles A. Sutton.\nA survey of ma-\nchine learning for big code and naturalness. CoRR,\nabs/1709.06182, 2017. 1\n[4] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Mil-\ntiadis Allamanis, and Marc Brockschmidt.\nCode-\nsearchnet challenge: Evaluating the state of semantic\ncode search, 2019. 1, 2, 6\n[5] Yasir Hussain, Zhiqiu Huang, Senzhang Wang, and\nYu Zhou. Codegru: Context-aware deep learning with\ngated recurrent unit for source code modeling. CoRR,\nabs/1903.00884, 2019. 1, 2\n[6] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili\nMou, and Lu Zhang.\nTreegen: A tree-based trans-\nformer architecture for code generation, 2019. 2\n[7] Xinyue Liu, Xiangnan Kong, Lei Liu, and Kuorong\nChiang.\nTreegan: Syntax-aware sequence genera-\ntion with generative adversarial networks.\nCoRR,\nabs/1808.07582, 2018. 2\n[8] Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong,\nGe Li, and Lu Zhang.\nA grammar-based struc-\ntural CNN decoder for code generation.\nCoRR,\nabs/1811.06837, 2018. 2\n[9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nCoRR, abs/1706.03762, 2017. 3\n[10] Yoon Kim, Yacine Jernite, David A. Sontag, and\nAlexander M. Rush. Character-aware neural language\nmodels. CoRR, abs/1508.06615, 2015. 3\n[11] KyungHyun Cho, Bart van Merrienboer, Dzmitry\nBahdanau, and Yoshua Bengio.\nOn the properties\nof neural machine translation: Encoder-decoder ap-\nproaches. CoRR, abs/1409.1259, 2014. 6\nAppendix and Figures\n6.1. CodeSearchNet Results\nThe Neural Bag of Words and LSTM CodeSearchNet baselines both report metrics in the same fashion. Below, we show\nthe training curves, which correspond to the loss in Equation (1).\nAdditionally, given that the baselines models for CodeSearchNet focus on code snippet retrieval, we also report the\nachieved mean reciprocal rank. The MRR is a statistic measure for evaluating any process that produces a list of possible\nresponses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the\nmultiplicative inverse of the rank of the ﬁrst correct answer: 1 for ﬁrst place, 1\n2 for second place, 1\n3 for third place and so on.\nThe mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries, as in Equation (2).\nMRR =\n1\n|Q|\n|Q|\nX\ni=1\n1\nranki\n(2)\n6.1.1\nNeural Bag of Words Baselines\nThis baseline consists of a simple encoder architecture which takes as input bag-of-words representation of the code and\nusing a single neural network encodes these token representation into an embedding [4]. This baseline actually performs the\nbest, achieving the lowest overall training and validation losses (see Figure 5) as well as the highest MRR on the validation\nset (See Figure 6).\n6.1.2\nBi-directional RNN Model\nIn this model, we employ the GRU cell [11] to summarize the input sequence. This baseline performs signiﬁcantly worse,\nsuffering from what appears to be obvious over-ﬁtting. In Figure 7, we can see that while the training loss appears to plateau,\nthe validation loss begins quickly climbing. While this behavior does not appear to affect the overall MRR achieved on the\nvalidation set, it is still clear that the model performs worse than the bag of words baseline as per Figure 8.\n6.2. Example Code\nListing 1. Sample generated using Char-RNN model.\nDownloads\nDailymotion\nvideos\nby URL.\n( x or\np i l l a r e s\n( i f\nmacks ) ,\ns t y l e\nas\na bool\nto\nyou\ne x t n e r\nto\noio\ni n s t a n c e\nof\nt h a t\nS e i r\nto\nbe two\nc a t e g o r i c a l\nS t r i n g\nto\nmandation\n: a t t r : ‘ Columnserv\nzr ,\nj )\ndimensed source\na l s o =\n‘ ‘ axis ‘ ‘ .\nElement .\nThis\nr e p r d u r e s\na\ns u b s t i m c l e\nof\n”””\ncode = item\nr e t u r n\ns e l f . fi l en a me\nr e s t r o x i g ,\ns e l f . g e t c h a n n e l s ( ) :\n”””Get\nthe\nf u n c t i o n\nas\nf i r m a p l i s t\n{1} and a\n: a t t r a c t e d c o o r d i n t :\nvalue\nof Time\nend aes :\nt a r g e t e x t\n= i n t ( cmd dict ) :\n# In\ndata\na p p l i c a t e\nhorinad ( s t r i n g ) :\nthe\nchannel\nf o r\ndecoded\nc o l l f i l e\nRunnee network number\nof\nelement .\nWhat you ’ s\na s s o c i a t e s\npy ch\nof\nL i s t\ndoes\nr e q u e s t\n‘ ‘ index ‘ ‘ .\ndef\nv e r t e v e n t n o t i f y c h a n n e l ( s e l f ,\ni n f n a =1)\ne l i f\nt r y i n g . format (\ncomps + ’ , ’ % ( random ) + 1 ,\nH. data :\ni f\nl i s t\n+ lookbing . get ( ’ g e t d i c t . get ’ ]}\nListing 2. Sample generated using small (117M) GPT-2 model after 40000 iterations.\ni n p u t s n i p p e t = def\ns e m i l o c a l l i n e a r t r e n d t r a n s i t i o n m a t r i x ( a u t o r e g r e s s i v e c o e f ) :\n””” Build\nthe\nt r a n s i t i o n\nmatrix\nf o r\na semi −l o c a l\nl i n e a r\ntrend\nmodel . ””” ,\nout p ut c od e =\n# Create\nthe\nmatrix\n# ( a u t o r e g r e s s i v e l i n e a r t r e n d m a t r i x )\nwhere\n: param\ns e m i l o c a l l i n e a r t r e n d m a t r i x :\n#\n( n samples )\ni s\nl e n g t h\nof\nthe\nsemi −sparse\nl i n e a r\nr e g r e s s i o n\nmatrix ;\n#\ni ,\nj = s e m i l o c a l l i n e a r t r e n d\n# Create\nthe\ncolumns\ncolumns = np . a s a r r a y ( columns )\ni f\na u t o c o r r e l a t e d t r e n d m a t r i x :\ncolumn = np . a s a r r a y ( columns )\n# Create\nthe\ni n d i c e s\nf o r\nthe\ncolumns\ni = column . min ( )\nj = column . max ( )\n# Ensure\nthe\ncolumns\nhave\nu n i f o r m i t y\nand\nthe\na x i s\np o s i t i o n\na s s e r t\nhasattr (\ncolumn ,\n” sigma ” )\na s s e r t\nhasattr ( axes ,\n” a x i s ” )\n# Create\nthe\ncolumns\ni = columns . copy ( )\nj = columns . copy ( )\n# Ensure\nthe\ni n d i c e s\nf o r\nthe\ni n d i c e s\nf o r\nthe\ncolumns\nax = ColumnSetAxes ( i ,\nj )\naxes . s e t a x e s ( ax )\n# Create\nthe\ncolumns\nax . s e t x l a b e l ( s e l f . d e f a u l t l a b e l )\nax . s e t y l a b e l ( s e l f . d e f a u l t y l a b e l )\naxes . s e t\nt i t l e ( s e l f .\nt i t l e )\naxes . t e x t ( u” Subsim ” ]\n# Create\nthe\ncolumn\nindex\ns e l f . g e t i n d e x ( a x i s = columns ,\nc o l s = c o l s )\n# Create\nthe\ncolumns\ni f\nnot\nalready\nprovided\nby\nt h i s\nf u n c t i o n\ni f\nnot\ncolumns :\ni f\na u t o c o r r e l a t e d t r e n d m a t r i x :\nax . columns = columns\ne l s e :\ni f\ni < 0 or\nj < 0 or\ni +1 <= i\n6.2.1\nFigures\nFigure 3. Histogram of the the number of (comment, code) pairs available in our dataset, as well as the number of unique function methods\nfor each language.\nFigure 4. General CodeSearchNet architecture for all of our baselines. Each language is processed through different encoder mechanisms.\nThe query encoder is shared (an NLP encoder), and the purpose of the CodeSearchNet tasks is to retrieve the most relevant code snippets\nsubject to the natural language query.\nFigure 5. Training and Validation losses for the Neural Bag of Words model in CodeSearchNet.\nFigure 6. MRR on validation set for the baseline neural bag of words model in the CodeSearchNet Challenge.\nFigure 7. Training and Validation losses for the RNN model in CodeSearchNet.\nFigure 8. MRR on validation set for the baseline RNN in the CodeSearchNet Challenge.\nFigure 9. Training and Validation Losses on the Baseline Char-RNN Model. This is the cross-entropy loss over 128 predicted character\nsequence.\nFigure 10. Training and Validation Perplexity on the Baseline Char-RNN Model. This is the cross-entropy loss over 128 predicted character\nsequence.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-02-21",
  "updated": "2021-02-21"
}