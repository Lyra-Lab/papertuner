{
  "id": "http://arxiv.org/abs/2102.10957v1",
  "title": "Co-occurrences using Fasttext embeddings for word similarity tasks in Urdu",
  "authors": [
    "Usama Khalid",
    "Aizaz Hussain",
    "Muhammad Umair Arshad",
    "Waseem Shahzad",
    "Mirza Omer Beg"
  ],
  "abstract": "Urdu is a widely spoken language in South Asia. Though immoderate literature\nexists for the Urdu language still the data isn't enough to naturally process\nthe language by NLP techniques. Very efficient language models exist for the\nEnglish language, a high resource language, but Urdu and other under-resourced\nlanguages have been neglected for a long time. To create efficient language\nmodels for these languages we must have good word embedding models. For Urdu,\nwe can only find word embeddings trained and developed using the skip-gram\nmodel. In this paper, we have built a corpus for Urdu by scraping and\nintegrating data from various sources and compiled a vocabulary for the Urdu\nlanguage. We also modify fasttext embeddings and N-Grams models to enable\ntraining them on our built corpus. We have used these trained embeddings for a\nword similarity task and compared the results with existing techniques.",
  "text": "Co-occurrences using Fasttext embeddings for word\nsimilarity tasks in Urdu\nUsama Khalid\nDepartment of Computer Science\nAIM Lab, NUCES (FAST)\nIslamabad, Pakistan\nusama.khalid@nu.edu.pk\nAizaz Hussain\nDepartment of Computer Science\nAIM Lab, NUCES (FAST)\nIslamabad, Pakistan\naizaz.hussain@nu.edu.pk\nMuhammad Umair Arshad\nDepartment of Computer Science\nAIM Lab, NUCES (FAST)\nIslamabad, Pakistan\numair.arshad@nu.edu.pk\nWaseem Shahzad\nDepartment of Computer Science\nAIM Lab, NUCES (FAST)\nIslamabad, Pakistan\nwaseem.shahzad@nu.edu.pk\nMirza Omer Beg\nDepartment of Computer Science\nAIM Lab, NUCES (FAST)\nIslamabad, Pakistan\nomer.beg@nu.edu.pk\nAbstract—Urdu is a widely spoken language in South Asia.\nThough immoderate literature exists for the Urdu language still\nthe data isn’t enough to naturally process the language by NLP\ntechniques. Very efﬁcient language models exist for the English\nlanguage, a high resource language, but Urdu and other under-\nresourced languages have been neglected for a long time. To\ncreate efﬁcient language models for these languages we must have\ngood word embedding models. For Urdu, we can only ﬁnd word\nembeddings trained and developed using the skip-gram model.\nIn this paper, we have built a corpus for Urdu by scraping and\nintegrating data from various sources and compiled a vocabulary\nfor Urdu language. We also modify fasttext embeddings and N-\nGrams models to enable training them on our built corpus. We\nhave used these trained embeddings for a word similarity task\nand compared the results with existing techniques. The datasets\nand code is made freely available on GitHub.1.\nIndex Terms—Word Embeddings, Ngrams, Fasttext, Urdu,\nWord2Vec, Skip-Gram, Low Resource\nI. INTRODUCTION\nUrdu language originated back in 12th with an Indo-Aryan\nvocabulary [1] base and is a mixture of Arabic and Persian.\nUrdu language is widely spoken and written in the South\nAsian region with more than 170 million speakers speciﬁcally\nin Pakistan and India. Despite this Urdu [2] is considered\na low resourced language because of insufﬁcient data [3]\nas compared to English and other widely spoken languages.\nIn recent times the paradigm has been shifted towards the\ndevelopment of efﬁcient models for low resource languages\n[4]. Many deep learning and machine learning techniques are\nused to train language models for the derivation of semantics\nfrom given textual data [5]. To derive meaningful information\nfrom the text it is useful to ﬁnd out the relation between\nwords. For example, as shown in Fig. 2, words are clustered\ntogether based on their similarity. Language models store this\ninformation which can then be used for many downstream\n1https://github.com/usamakh20/wordEmbeddingsUrdu\nFig. 1. N dimensional visualization of word vectors in 3D space.\ntasks.\nMachines cannot understand the language [6] in the way\nwe do so data cannot be used as it is passed into the network,\ninstead, each word is converted into a N dimensional vector.\nThese representations are known as word embeddings. An\nexample representation of these embeddings are shown in\nFig. 1. The words are projected from an n dimension to 3D\n[7], [8]. The words with related meanings tend to appear\nclose together. The word embeddings are the baseline for\nany natural language processing task e.g., transliteration,\nnatural language generation, understanding user inputs, etc\narXiv:2102.10957v1  [cs.CL]  22 Feb 2021\nFig. 2. An overview of the Fasttext architecture.\n[9]. All these vectors together combined show how much a\nword is similar to others in a given vector space [10]. For\nUrdu, a lot of work has been done on semantic analysis\nand sentence classiﬁcation however there are no studies that\nshow the performance analysis of word embeddings models\non the Urdu language [11]. Unlike the studies conducted for\nwidely spoken languages, in this paper, we use different word\nembedding models to compute similarity scores for words in\nthe Urdu language.\nIn this paper, we used a freely available Urdu news text\ncorpus COUNTER [12], [13], which contains data from\n1200 documents collected from different news agencies\nof Pakistan. We have then modiﬁed existing Fasttext and\nn-grams approaches to be applied to Urdu data and we\ntrain and provide embeddings. Additionally we compare\nour trained model and embeddings to previously available\nskip-gram [14], [15] technique on a word similarity task [15],\n[16].\nThis paper is organized in multiple sections which are as\nfollows: In section 2, we will look into the related research.\nIn section 3, we discuss methodologies and the experimental\nhypothesis. In section 4, we will look at the experimentation\nresults. Finally, in section 5 we will summarize our work and\ndiscuss the possible contributions and future directions.\nII. LITERATURE REVIEW\nA lot of work have been done in Urdu language in terms\nof POS tagging, Sentimental Analysis, NER, Stemmer,\nMT, Topic Modeling [1], [6], [11], [17]–[26] but not much\nwork has been done in word embeddings for Urdu. These\nwords embeddings play a major role in natural language\nunderstanding.\nMultiple\nlanguage\nembedding\ntraining\narchitectures have been introduced i.e. BERT [27], Word2Vec\netc.\nThere are many ways in which words vectors can be\nrepresented among them one is one hot encoding vector repre-\nsentation [28]. In one hot encoding the vectors are represented\nas long binary vector representations of words [24].\nTo formulate one hot vectors for a corpus, they can be\naggregated to form the BoW (Bag of Words) representation\n[29], [30]. The bag of words maintain a dictionary of all\npossible words in the language and keep track of the frequency\nof the word encountered in the particular corpus.\nThe problem with BoW is that it doesn’t keep track of\nwords similarity and contextual meaning. So to solve the\nwords similarity problem TF-IDF (Term Frequency - Inverse\nDocument Frequency) [31], [32] has been introduced. It\nassociates each word in a document with a number which\nis a measure of how relevant that word is. Based on this\nsimilarity of words, one can compare the similarity of\nmultiple documents together.\nWord2Vec is a fusion of two architectures i.e. CBOW and\nSkip-Gram [33]. These architectures are designed to be mirror\nimages of one another [34]. The CBOW the model tries to\npredict the closest context to the input word while the Skip-\nGram model tries to predict the closest words to the input\nword.\nWord embeddings help us considerably improve NLP\ntechniques for low resourced languages. In context of Urdu,\nthe only words embeddings present in literature are that of\nSkip-Gram [14]. To create a large sample word embeddings\nfor urdu, 140 million sentences in Urdu were used. To check\nthe accuracy of learned embeddings, the closest neighboured\nwords were analyzed w.r.t different words in the vector space\n[35], context window sizes and their performance on Urdu\ntransliteration.\nThe basic idea behind N-Gram language model is to assign\nprobability to each word in a given sequence of words [36],\n[37]. In word embeddings the words are dissected into multiple\nN number of chunks and then these chunks are assigned\nprobabilities. Using these probabilities the closest context of a\nword can be calculated in the vector space. By the calculation\nof probabilities, this model is very helpful in Natural Language\nGenerations, sentence completion [38], sentence correction\netc. The main issue of N-Gram model is that it is sensitive\nto the training corpus. Many models have been introduced\nFig. 3. An overview of the N-Gram model used in this research.\nwhich combine N-Grams and neural networks to overcome\nthe problems of N-Gram and generate more accurate results.\nFasttext is primarily an architecture developed by facebook\nfor text classiﬁcation [39]. Fasttext works on the principal of\nWord2Vec and n-grams technique. In word2vec the text is feed\ninto the Neural Network individually. However in Fasttext the\nwords are divided into several sub words and then feed into\nthe Neural Networks. Consider the word apple and we have\nto dissect this word into tri-grams then the resultant output\nwould be app, ppl, and ple [40]. The word vector for apple\nwill be the sum of all these tri-grams. After training the Neural\nNetwork on the training data, we get the word vector for each\nn-grams and later these n-grams can be used to relate other\nwords. For rare words can be mapped as there will be many\noverlapping n-grams which appeared in other words.\nIII. METHODOLOGY\nWe used two methods to train our models on word embed-\ndings, Fasttext and N-Grams [41]. Fig. 3 shows the working\nof modiﬁed N-Gram model used in this research. The n-gram\nmodel converts the document into tokens and stores these\ntokens in a dictionary based on the co-occurrences of words.\nThat is the number of times a token ti appears next to a\ntoken tj is stored in a co-occurrence dictionary. Against each\nkey there is a are multiple word vectors with the probability\nscore of its occurrence. In fasttext a document is tokenized and\npassed through a network. The network learns weights which\ncan be extracted as word embeddings. Fig. 2 shows how words\nare propagated through the network to extract embeddings for\nUrdu. In next sections we will discuss in detail about the\ndataset, experimentation and results.\nA. Corpus\nWe have used the Urdu Monolingual corpus [42] containing\n54 million sentences, 90 million tokens and 129K unique\ntokens. In the preprocessing step we removed all special\ncharacters such as brackets, single/double quotes and spaces\n[43]. All these special characters are replaced by spaces. As a\nsecond step consecutive occurring spaces of two or more are\nmatched and replaced by a single space character.\nB. Techniques\nWe have used two techniques for t [44]raining, namely\nngrams and Fasttext [45]. The ngram technique requires data\nto be separated sentence by sentence where each sentence\nis broken down into a list of words [46]. After separating\ninto list of words we remove common stop words. Similar\nprepossessing is applied for Fasttext, however the fasttext\npython package has the tokenizer and stop word removal tool\nbuiltin. The complete architecture is given in ﬁgure 2 and 3.\nC. Hyper Parameters\nThe Fasttext technique has four main hyper parameters [47]\nthat we can tune. Vector dimension represents the length of the\nvector size to represent a single word. Larger vectors capture\nmore information [48] but are harder to train and cost more\ndata [49]. Epochs is the number of times the model trains on\na batch of data. The larger the corpus the lesser number of\ntimes it may have to be iterated. Learning rate is a measure of\nhow quickly the model should converge to a solution [50]. Sub\nwords length speciﬁes the length of substrings [51] to consider\nfor different processing tasks like resolving out of vocabulary\nwords.\nFor the current study we have used the default parameters\nfor Fasttext which are\n• Vector dimension : 100\n• Epochs : 5\n• Learning rate : 0.05\n• Sub words length : min=3 & max=6\nThe ngrams technique only has a single hyper-parameter\nnamely the number of consecutive words or grams to train.\nIV. RESULTS AND DISCUSSION\nFor the evaluating the similarity of learned word representa-\ntions We use Urdu translated version of corpora SimLex-999\n[52] and WordSim-353 [53]. SimLex-999 is a gold standard\ndataset for evaluating word embeddings. It contains 999 noun,\nadjective and verb triplets in a concrete [54] and abstract form.\nThe dataset is designed to evaluate similarity of words rather\nthan relatedness and contains similarity score for words. The\nWordSim-353 dataset [55] contains relatedness scores for 353\nword pairs.\nFig. 4. In this ﬁgure, some word embeddings produced by fasttext are mapped\nto a 2D space which shows how words are related and how they appear in\nclose proximity to each other.\nThese datasets have been translated to urdu using ijunoon’s\ntranslation service2 and made available. For calculating the\nsimilarity and relatedness of words we use the Spearman cor-\nrelation coefﬁcient [56]. The difference between the predicted\nscore and actual score is d and n is the number of examples.\nrs = 1 −\n6Σnid2i\nn(n2 −1)\n(1)\nWordSim-353\nSimLex-999\nFasttext\n0.462\n0.743\nbigrams\n0.188\n0.156\nskip-gram [14]\n0.492\n0.293\nFasttext English [57]\n0.84\n0.76\nFig. 5. We compare our results with skip-gram [14] embeddings that were\nevaluated on the same dataset. The Fasttext embeddings are trained for 100\ndimensional vectors so the results of skip-gram are also for 100 dimensional\nvectors for a fair comparison.\nThe bigrams similarity measure as expected produces a\nlow correlation score, this is also because correlation is only\ncomputed for exact word matches from the corpus which are\ncomparatively very less as compared to Fasttext for WordSim-\n353 and SimLex-999. The Fasttext technique outperforms\nskip-gram based technique [14] for the SimLex-999 task\nhowever slightly under-performs in WordSim-353.\nV. CONCLUSION\nThe advent of Word Embedding techniques [58] was no\nless than a revolution in the ﬁeld of NLP. It enabled the repre-\n2https://translate.ijunoon.com/\nsentation of words in a digital form (vectors) that computers\ncan understand and perform mathematical calculations on, like\nthe famous example King - Man + Woman = Queen. It also\nestablished the ground work for modern Deep attention based\nmodels and Transformers in the ﬁeld of NLP.\nUrdu has for long remained an Under resourced language\nwhich has caused many proposed state-of-the-art techniques\nto under perform when being applied to Urdu corpora. It can\nalso be seen in Fig. 5 that performance of Fasttext on Urdu\ncorpora is nowhere near to that of English. In this research we\nhave proposed Word co-ocurrences using bigrams and Fasttext\nword embeddings trained using the COUNTER corpus and\nhave evaluated our approach on WordSim-353 and SimLex-\n999 similarity tasks and compared that to previously proposed\nskip-gram technique.\nIn the future work can be done on training these techniques\non larger Urdu corpora and evaluate on various tasks like POS\nTagging, NER, Machine Translation, sentiment analysis and\ndependency parsing. In addition to this large corpora have\nto be proposed for Urdu if we want to at least match the\nperformance of techniques that have been proposed for High\nresource Languages such as English. We hope that this work\nwill help researchers to produce better techniques in the area\nof Urdu NLP.\nREFERENCES\n[1] Mirza Beg and Mike Dahlin. A memory accounting interface for the\njava programming language.\n[2] Bilal Naeem, Aymen Khan, Mirza Omer Beg, and Hasan Mujtaba. A\ndeep learning framework for clickbait detection on social area network\nusing natural language cues. Journal of Computational Social Science,\npages 1–13, 2020.\n[3] Abdul Rehman Javed, Muhammad Usman Sarwar, Mirza Omer Beg,\nMuhammad Asim, Thar Baker, and Hissam Tawﬁk.\nA collaborative\nhealthcare framework for shared healthcare plan with ambient intelli-\ngence. Human-centric Computing and Information Sciences, 10(1):1–\n21, 2020.\n[4] Mirza Beg and Peter Van Beek. A graph theoretic approach to cache-\nconscious placement of data for direct mapped caches. In Proceedings\nof the 2010 international symposium on Memory management, pages\n113–120, 2010.\n[5] Haﬁz Tayyeb Javed, Mirza Omer Beg, Hasan Mujtaba, Hammad Majeed,\nand Muhammad Asim. Fairness in real-time energy pricing for smart\ngrid using unsupervised learning. The Computer Journal, 62(3):414–\n429, 2019.\n[6] Rabail\nZahid,\nMuhammad\nOwais\nIdrees,\nHasan\nMujtaba,\nand\nMirza Omer Beg.\nRoman urdu reviews dataset for aspect based\nopinion mining. In 2020 35th IEEE/ACM International Conference on\nAutomated Software Engineering Workshops (ASEW), pages 138–143.\nIEEE, 2020.\n[7] Mirza Beg, Laurent Charlin, and Joel So. Maxsm: A multi-heuristic\napproach to xml schema matching. 2006.\n[8] Aaditeshwar Seth and Mirza Beg. Achieving privacy and security in\nradio frequency identiﬁcation. In Proceedings of the 2006 International\nConference on Privacy, Security and Trust: Bridge the Gap Between\nPST Technologies and Business Services, pages 1–1, 2006.\n[9] Abdul Ali Bangash, Hareem Sahar, and Mirza Omer Beg. A method-\nology for relating software structure with energy consumption. In 2017\nIEEE 17th International Working Conference on Source Code Analysis\nand Manipulation (SCAM), pages 111–120. IEEE, 2017.\n[10] Mirza O Beg, Mubashar Nazar Awan, and Syed Shahzaib Ali. Algo-\nrithmic machine learning for prediction of stock prices. In FinTech as\na Disruptive Technology for Financial Institutions, pages 142–169. IGI\nGlobal, 2019.\n[11] Hussain S Khawaja, Mirza O Beg, and Saira Qamar. Domain speciﬁc\nemotion lexicon expansion. In 2018 14th International Conference on\nEmerging Technologies (ICET), pages 1–5. IEEE, 2018.\n[12] Muhammad Sharjeel, Rao Muhammad Adeel Nawab, and Paul Rayson.\nCounter: corpus of urdu news text reuse.\nLanguage resources and\nevaluation, 51(3):777–803, 2017.\n[13] Adeel Zafar, Hasan Mujtaba, Sohrab Ashiq, and Mirza Omer Beg. A\nconstructive approach for general video game level generation. In 2019\n11th Computer Science and Electronic Engineering (CEEC), pages 102–\n107. IEEE, 2019.\n[14] Samar Haider. Urdu word embeddings. In Proceedings of the Eleventh\nInternational Conference on Language Resources and Evaluation (LREC\n2018), 2018.\n[15] Saira Qamar, Hasan Mujtaba, Hammad Majeed, and Mirza Omer Beg.\nRelationship identiﬁcation between conversational agents using emotion\nanalysis. Cognitive Computation, pages 1–15.\n[16] Talha Imtiaz Baig, Nazish Banaras, Ebad Banissi, Raﬁa Bashir,\nMirza Omer Beg, Junaid Bilal, Ahmad Hassan Butt, Waseem Chishti,\nChristos Chrysoulas, Anum Dastgir, et al. Awan, shahid mahmood 245\nayubi, salah-u-din 192.\n[17] Wahab Khan, Ali Daud, Khairullah Khan, Jamal Abdul Nasir, Mo-\nhammed Basheri, Naif Aljohani, and Fahd Saleh Alotaibi. Part of speech\ntagging in urdu: Comparison of machine and deep learning approaches.\nIEEE Access, 7:38918–38936, 2019.\n[18] Neelam Mukhtar and Mohammad Abid Khan. Urdu sentiment analysis\nusing supervised machine learning approach. International Journal of\nPattern Recognition and Artiﬁcial Intelligence, 32(02):1851001, 2018.\n[19] Muhammad Kamran Malik. Urdu named entity recognition and clas-\nsiﬁcation system using artiﬁcial neural network. ACM Transactions on\nAsian and Low-Resource Language Information Processing (TALLIP),\n17(1):1–13, 2017.\n[20] Vaishali Gupta, Nisheeth Joshi, and Iti Mathur. Design & development\nof rule based inﬂectional and derivational urdu stemmer ‘usal’. In 2015\nInternational conference on futuristic trends on computational analysis\nand knowledge management (ABLAZE), pages 7–12. IEEE, 2015.\n[21] Khadija Shakeel, Ghulam Rasool Tahir, Irsha Tehseen, and Mubashir\nAli. A framework of urdu topic modeling using latent dirichlet allocation\n(lda).\nIn 2018 IEEE 8th Annual Computing and Communication\nWorkshop and Conference (CCWC), pages 117–123. IEEE, 2018.\n[22] Muhammad Umair Arshad, Muhammad Farrukh Bashir, Adil Majeed,\nWaseem Shahzad, and Mirza Omer Beg. Corpus for emotion detection\non roman urdu.\nIn 2019 22nd International Multitopic Conference\n(INMIC), pages 1–6. IEEE, 2019.\n[23] Saad Nacem, Majid Iqbal, Muhammad Saqib, Muhammad Saad,\nMuhammad Soban Raza, Zaid Ali, Naveed Akhtar, Mirza Omer Beg,\nWaseem Shahzad, and Muhhamad Umair Arshad. Subspace gaussian\nmixture model for continuous urdu speech recognition using kaldi.\nIn 2020 14th International Conference on Open Source Systems and\nTechnologies (ICOSST), pages 1–7. IEEE, 2020.\n[24] Adil Majeed, Hasan Mujtaba, and Mirza Omer Beg. Emotion detection\nin roman urdu text using machine learning. In Proceedings of the 35th\nIEEE/ACM International Conference on Automated Software Engineer-\ning Workshops, pages 125–130, 2020.\n[25] Uzma Rani, Aamer Imdad, and Mirza Beg. Case 2: Recurrent anemia\nin a 10-year-old girl. Pediatrics in review, 36(12):548–550, 2015.\n[26] Zubair Baig, Mirza Omer Beg, Baber Majid Bhatti, Farzana Ahamed\nBhuiyan, Tegawend´e F Bissyand´e, Shizhan Chen, Mohan Baruwal\nChhetri, Marco Couto, Jo˜ao de Macedo, Randy de Vries, et al. Ahmed,\nsanam 124 aleti, aldeida 105 alo´ısio, jo˜ao 151 arachchilage, nalin asanka\ngamagedara 7.\n[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805, 2018.\n[28] John T Hancock and Taghi M Khoshgoftaar. Survey on categorical data\nfor neural networks. Journal of Big Data, 7:1–41, 2020.\n[29] Yin Zhang, Rong Jin, and Zhi-Hua Zhou.\nUnderstanding bag-of-\nwords model: a statistical framework. International Journal of Machine\nLearning and Cybernetics, 1(1-4):43–52, 2010.\n[30] Mirza Omer Beg. Performance analysis of packet forwarding on ixp2400\nnetwork processor. 2006.\n[31] Bijoyan Das and Sarit Chakraborty.\nAn improved text sentiment\nclassiﬁcation model using tf-idf and next word negation. arXiv preprint\narXiv:1806.06407, 2018.\n[32] Muhammad Umer Farooq, Mirza Omer Beg, et al. Bigdata analysis of\nstack overﬂow for energy consumption of android framework. In 2019\nInternational Conference on Innovative Computing (ICIC), pages 1–9.\nIEEE, 2019.\n[33] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff\nDean.\nDistributed representations of words and phrases and their\ncompositionality. In Advances in neural information processing systems,\npages 3111–3119, 2013.\n[34] Adeel Zafar, Hasan Mujtaba, and Mirza Omer Beg.\nSearch-based\nprocedural content generation for gvg-lg.\nApplied Soft Computing,\n86:105909, 2020.\n[35] M Beg. Critical path heuristic for automatic parallelization. 2008.\n[36] Adam Pauls and Dan Klein.\nFaster and smaller n-gram language\nmodels. In Proceedings of the 49th annual meeting of the Association\nfor Computational Linguistics: Human Language Technologies, pages\n258–267, 2011.\n[37] Mirza Omer Beg. Flecs: A data-driven framework for rapid protocol\nprototyping. Master’s thesis, University of Waterloo, 2007.\n[38] Adeel Zafar, Hasan Mujtaba, Mirza Tauseef Baig, and Mirza Omer Beg.\nUsing patterns as objectives for general video game level generation.\nICGA Journal, 41(2):66–77, 2019.\n[39] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze,\nH´erve J´egou, and Tomas Mikolov.\nFasttext. zip: Compressing text\nclassiﬁcation models. arXiv preprint arXiv:1612.03651, 2016.\n[40] Muhammad Umer Farooq, Saif Ur Rehman Khan, and Mirza Omer Beg.\nMelta: A method level energy estimation technique for android devel-\nopment.\nIn 2019 International Conference on Innovative Computing\n(ICIC), pages 1–10. IEEE, 2019.\n[41] Hamza M Alvi, Hareem Sahar, Abdul A Bangash, and Mirza O Beg.\nEnsights: A tool for energy aware software development. In 2017 13th\nInternational Conference on Emerging Technologies (ICET), pages 1–6.\nIEEE, 2017.\n[42] Bushra Jawaid, Amir Kamran, and Ondrej Bojar. A tagged corpus and\na tagger for urdu. In LREC, pages 2938–2943, 2014.\n[43] Danyal Thaver and Mirza Beg.\nPulmonary crohn’s disease in down\nsyndrome: A link or linkage problem. Case reports in gastroenterology,\n10(2):206–211, 2016.\n[44] Ahmed Uzair, Mirza O Beg, Hasan Mujtaba, and Hammad Majeed.\nWeec: Web energy efﬁcient computing: A machine learning approach.\nSustainable Computing: Informatics and Systems, 22:230–243, 2019.\n[45] Mirza Beg and Peter van Beek. A constraint programming approach for\nintegrated spatial and temporal scheduling for clustered architectures.\nACM Transactions on Embedded Computing Systems (TECS), 13(1):1–\n23, 2013.\n[46] Muhammad Tariq, Hammad Majeed, Mirza Omer Beg, Farrukh Aslam\nKhan, and Abdelouahid Derhab. Accurate detection of sitting posture\nactivities in a secure iot based assisted living environment.\nFuture\nGeneration Computer Systems, 92:745–757, 2019.\n[47] Adeel Zafar, Hasan Mujtaba, Mirza Omer Beg, and Sajid Ali. Deceptive\nlevel generator. 2018.\n[48] Hareem Sahar, Abdul A Bangash, and Mirza O Beg. Towards energy\naware object-oriented development of android applications. Sustainable\nComputing: Informatics and Systems, 21:28–46, 2019.\n[49] Walid Koleilat, Joel So, and Mirza Beg.\nWatagent: A fresh look at\ntac-scm agent design. 2006.\n[50] Mirza Beg. Flecs: A framework for rapidly implementing forwarding\nprotocols.\nIn International Conference on Complex Sciences, pages\n1761–1773. Springer, 2009.\n[51] Muhammad Asad, Muhammad Asim, Talha Javed, Mirza O Beg, Hasan\nMujtaba, and Sohail Abbas.\nDeepdetect: detection of distributed de-\nnial of service attacks using deep learning.\nThe Computer Journal,\n63(7):983–994, 2020.\n[52] Felix Hill, Roi Reichart, and Anna Korhonen. Simlex-999: Evaluating\nsemantic models with (genuine) similarity estimation. Computational\nLinguistics, 41(4):665–695, 2015.\n[53] Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius\nPasca, and Aitor Soroa. A study on similarity and relatedness using\ndistributional and wordnet-based approaches. 2009.\n[54] Noman Dilawar, Hammad Majeed, Mirza Omer Beg, Naveed Ejaz,\nKhan Muhammad, Irfan Mehmood, and Yunyoung Nam. Understanding\ncitizen issues through reviews: A step towards data informed planning\nin smart cities. Applied Sciences, 8(9):1589, 2018.\n[55] Abdul Rehman Javed, Mirza Omer Beg, Muhammad Asim, Thar Baker,\nand Ali Hilal Al-Bayatti. Alphalogger: Detecting motion-based side-\nchannel attack using smartphone keystrokes.\nJournal of Ambient\nIntelligence and Humanized Computing, pages 1–14, 2020.\n[56] Ch Spearman. The proof and measurement of association between two\nthings. International journal of epidemiology, 39(5):1137–1150, 2010.\n[57] Vitalii Zhelezniak, Aleksandar Savkov, April Shen, and Nils Y Ham-\nmerla. Correlation coefﬁcients and semantic textual similarity. arXiv\npreprint arXiv:1905.07790, 2019.\n[58] Martin Karsten, Srinivasan Keshav, Sanjiva Prasad, and Mirza Beg.\nAn axiomatic basis for communication.\nACM SIGCOMM Computer\nCommunication Review, 37(4):217–228, 2007.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-02-22",
  "updated": "2021-02-22"
}