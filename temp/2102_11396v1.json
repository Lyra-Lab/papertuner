{
  "id": "http://arxiv.org/abs/2102.11396v1",
  "title": "Learning Low-dimensional Manifolds for Scoring of Tissue Microarray Images",
  "authors": [
    "Donghui Yan",
    "Jian Zou",
    "Zhenpeng Li"
  ],
  "abstract": "Tissue microarray (TMA) images have emerged as an important high-throughput\ntool for cancer study and the validation of biomarkers. Efforts have been\ndedicated to further improve the accuracy of TACOMA, a cutting-edge automatic\nscoring algorithm for TMA images. One major advance is due to deepTacoma, an\nalgorithm that incorporates suitable deep representations of a group nature.\nInspired by the recent advance in semi-supervised learning and deep learning,\nwe propose mfTacoma to learn alternative deep representations in the context of\nTMA image scoring. In particular, mfTacoma learns the low-dimensional\nmanifolds, a common latent structure in high dimensional data. Deep\nrepresentation learning and manifold learning typically requires large data. By\nencoding deep representation of the manifolds as regularizing features,\nmfTacoma effectively leverages the manifold information that is potentially\ncrude due to small data. Our experiments show that deep features by manifolds\noutperforms two alternatives -- deep features by linear manifolds with\nprincipal component analysis or by leveraging the group property.",
  "text": "arXiv:2102.11396v1  [cs.CV]  22 Feb 2021\nLearning Low-dimensional Manifolds for\nScoring of Tissue Microarray Images\nDonghui Yan†, Jian Zou$, Zhenpeng Li‡\n†Mathematics and Data Science, UMass Dartmouth, MA, USA\n$Mathematical Sciences, Worcester Polytechnic Institute, MA, USA\n‡Statistics, Dali University, Yunnan, China\nFebruary 24, 2021\nAbstract\nTissue microarray (TMA) images have emerged as an important high-\nthroughput tool for cancer study and the validation of biomarkers. Ef-\nforts have been dedicated to further improve the accuracy of TACOMA,\na cutting-edge automatic scoring algorithm for TMA images. One major\nadvance is due to deepTacoma, an algorithm that incorporates suitable\ndeep representations of a group nature. Inspired by the recent advance in\nsemi-supervised learning and deep learning, we propose mfTacoma to learn\nalternative deep representations in the context of TMA image scoring. In\nparticular, mfTacoma learns the low-dimensional manifolds, a common\nlatent structure in high dimensional data. Deep representation learning\nand manifold learning typically requires large data.\nBy encoding deep\nrepresentation of the manifolds as regularizing features, mfTacoma eﬀec-\ntively leverages the manifold information that is potentially crude due to\nsmall data. Our experiments show that deep features by manifolds outper-\nforms two alternatives—deep features by linear manifolds with principal\ncomponent analysis or by leveraging the group property.\nIndex terms— Deep representation learning; small data; manifold\nlearning; tissue microarray images\n1\nIntroduction\nTissue microarray (TMA) images [53, 33, 9] have emerged as an impor-\ntant high-throughput tool for the evaluation of histology-based laboratory\ntests. They are used extensively in cancer studies [20, 9, 25, 50], includ-\ning clinical outcome analysis [25], tumor progression analysis [38, 1], the\nidentiﬁcation of diagnostic or prognostic factors [19, 1] etc. TMA images\nhave also been used in the development and validation of tumor-speciﬁc\nbiomarkers [25]. Additionally, they are used in imaging genetics [11, 27]\nfor the study of genetics alterations. TMA images are produced from thin\nslices of tissue sections cut from small tissue cores (less than 1 mm in\ndiameter) which are extracted from tumor blocks. Many slices, typically\nseveral hundred (possibly from diﬀerent patients), are arranged as an ar-\nray and mounted on a TMA slide, and then stained with a tumor-speciﬁc\n1\nbiomarker. A TMA image can be produced for each tissue section (i.e.,\na cell in the TMA slide) when viewed with a high-resolution microscope.\nFigure 1 is an illustration of the TMA technology.\nFigure 1: An illustration of the TMA technology (image courtesy [56]). Small\ntissue cores are ﬁrst extracted from tumor blocks, and stored in archives. Then\nthin slices of tissue sections are taken from the tissue core. Hundreds of tissue\nsections are mounted, in the form of an array, on a tissue slide, and stained\nwith tumor-speciﬁc biomarkers. A TMA image is captured for each tissue section\nwhen viewed from a high-resolution microscope.\nSpots in a TMA image measure tumor-speciﬁc protein expression level.\nThe readings of a TMA image are quantiﬁed by its staining pattern, which\nis typically summarized as a numerical score by the pathologist. A protein\nmarker that is highly expressed in tumor cells will exhibit a qualitatively\ndiﬀerent pattern (e.g., in darker color) from otherwise. Such scores serve\nas a convenient proxy to study the tissue images. To liberate pathologists\nfrom intensive labors, and also to reduce the inherent variability and sub-\njectivity with manual scoring [48, 31, 14, 51, 20, 5, 16, 52, 4, 9] of TMA im-\nages, a number of commercial tools and algorithms have been developed.\nThis includes ACIS, Ariol, TMAx and TMALab II for IHC, and AQUA\n[8] for ﬂuorescent images. However, these typically require background\nsubtraction, segmentation or landmark selection etc, and are sensitive to\nfactors such as IHC staining quality, background antibody binding, hema-\ntoxylin counterstaining, and chromogenic reaction products used to detect\nantibody binding [56]. The primary diﬃculty in TMA image analysis is\nthe lack of easily-quantiﬁed criteria for scoring—the staining patterns are\nnot localized in position, shape or size.\nA major breakthrough was achieved with TACOMA [56], a scoring al-\ngorithm that is comparable to pathologists, in terms of accuracy and re-\npeatability, and is robust against variability in image intensity and stain-\ning patterns etc. The key insight underlying TACOMA is that, despite sig-\nniﬁcant heterogeneity among TMA images, they exhibit strong statistical\nregularity in the form of visually observable textures or staining patterns.\nSuch patterns are captured by a highly eﬀective image statistics—the gray\nlevel co-occurrence matrix (GLCM). Inspired by the success of deep learn-\ning [28, 34], deepTacoma [55] incorporates deep representations to meet\nmajor challenges—heterogeneity and label noise—in the analysis of TMA\nimages. The deep features explored by deepTacoma are of a group nature,\naiming at giving more concrete information than implied by the labels or\nto borrow information from “similar” instances, in analogy to how the\ncluster assumption would help in semi-supervised learning [13, 59].\n2\nHow to further advance the state-of-the-art? Motivated by progress made\nwith deepTacoma, we will further explore deep representations derived\nfrom latent structures in the data. While deepTacoma makes use of clus-\nters in the data, we pursue the low-dimensional manifolds in the present\nwork. For high dimensional data, often the data or part of it lie on some\nlow-dimensional manifolds [47, 43, 17, 26, 12, 6, 41]. Eﬀectively leverag-\ning the manifold information can improve many tasks, such as dimension\nreduction or model ﬁtting etc. Indeed, a recent work on the geometry of\ndeep learning [35] attributes the success of deep learning to the eﬀective-\nness of the deep neural networks in learning such structures in the data.\nAs our method is built upon TACOMA and uses manifold information,\nwe term it mfTacoma.\nGiven the overwhelming popularity of deep learning in image recogni-\ntion, it is worthwhile to remark the challenges in applying deep learning\nto TMA images [55]. The availability of large training sample, essential\nfor the success of deep learning, is severely limited for TMA images. TMA\nimages are much harder to acquire than the usual natural images as they\nhave to be taken from the human body and captured by high-end micro-\nscopes and imaging devices. Their labelling requires substantial expertise\nfrom pathologists. Additionally, the natural and TMA images are of a\ndiﬀerent nature in terms of classiﬁcation. Natural images are typically\nformed by a visually sensible image hierarchy, which leads to the neces-\nsary sparsity for deep neural networks to succeed [44]. In contrast, the\nscoring of TMA images is not about the shape of the staining pattern,\nrather the “severity and spread” of staining matters. A further limiting\nfact is that TMA images are scored by biomarkers or cancer types; there\nare over 100 cancer types according to the US National Cancer Institute\n[49].\nOur main contributions are as follows. First, we propose an eﬀective ap-\nproach to learn the low dimensional manifolds in high dimensional data,\nwhich allows us to advance the state-of-the-art in the scoring of TMA im-\nages. The approach is conceptually simple and easy to implement thanks\nto progress in deep neural networks during the last decades. Second, our\napproach demonstrates that representing low dimensional manifolds as\nregularizing features is a fruitful way of leveraging manifold information,\neﬀectively overcoming the diﬃculty that shadows many manifold learning\nalgorithms under small sample. Given the prevalence of low dimensional\nmanifolds in high dimensional data, our approach may be potentially ap-\nplicable to many problems involving high dimensional data.\nThe remainder of this paper is organized as follows.\nWe describe the\nmfTacoma algorithm in Section 2. In Section 3, we present our experi-\nmental results. Section 4 provides a summary of the methods and results.\nGiven the\n2\nThe mfTacoma algorithm\nIn this section, we will describe the mfTacoma algorithm. The scoring sys-\ntems adopted in practice typically use a small number of discrete values,\nsuch as {0, 1, 2, 3}, as the score (or label) for TMA images. The scoring\n3\ncriteria are: ‘0’ indicates a deﬁnite negative (no staining of tumor cells),\n‘3’ a deﬁnitive positive (majority cancer cells show dark staining), ‘2’ for\npositive (small portion of tumor cells show staining or a majority show\nweak staining), and ‘1’ indicates weak staining in small part of tumor\ncells, or image in discardable quality [37]. We formulate the scoring of\nTMA images as a classiﬁcation problem, following ([56]; [55]).\nFigure 2: Size of top 100 principal components of TMA images (in GLCM).\nThe representations we explore are derived from the low-dimensional man-\nifold structure in the data. The existence of low-dimensional manifolds in\nhigh dimensional data is well established [47, 43, 17, 32, 26, 12, 6]. For\nTMA images, as the dimension of the ambient space (i.e., the image size)\nis 1504 x 1440, it is hard to “see” the low-dimensional manifolds. We will\ninstead carry out a principal component analysis (PCA) [30, 45, 58, 46]\non the GLCM of all the TMA images to get a crude sense. Figure 2 shows\na sharp decay of the principal components and their vanishing beyond the\n50th component, and this clearly implies the existence of low-dimensional\nmanifolds.\nNote that while PCA extracts linear manifolds, the actual\nmanifolds maybe be highly nonlinear. Nevertheless, a simple PCA anal-\nysis should be fairly suggestive of their existence. The low-dimensional\nmanifolds are a global property of the data and is beyond what may be\nrevealed by features derived from individual data points alone. Therefore\nwe expect such information help in the scoring of TMA images. One could\nview the information revealed by manifolds as regularization by problem\nstructures in model ﬁtting.\nThus a better model (e.g., more stable or\nmore accurate) would be expected.\nOne technical challenge is how to extract or make use of the manifold\ninformation from the space formed by the TMA images. Manifold learn-\ning has been an active research area during the last two decades [32, 12].\nMany work deal with dimension reduction or data visualization, for ex-\nample, Isomap [47], locally linear embedding [43], Laplacian eigenmaps\n[2], Hessian eigenmaps [17], local tangent space alignment [57], diﬀusion\nmaps [39], metric manifold maps [41]. Some of these were also used as\na data-driven distance metric for image similarity [42]. A more fruitful\n4\nline of work seems to be the use of manifolds for regularization in model\nﬁtting. This is likely due to the diﬃculty in estimating the manifolds—the\nmanifolds may be highly nonlinear and the sample size is often dispropor-\ntionally small, thus using manifolds as auxiliary information may be more\nproductive. Indeed Belkin et al [3] successfully used the graph Laplacian\nas a regularizer in semi-supervised learning, and Osher and his colleagues\n[40] use the dimension of the manifold as a regularizer in image denoising.\nWe follow a similar line as [3, 40] but with representations derived from the\nlow-dimensional manifolds as regularizing features to be appended to the\ninput. This is particularly easy to implement, without having to solve a\ncomplicated optimization problem, and is fairly general. The eﬀectiveness\nof using regularizing features has been demonstrated in [55]. Thanks to\nthe availability and easy implementation of autoencoder [22], we will use\nit to extract the low-dimensional manifold representation corresponding\nto the TMA images. We term such deep representations as M-features.\nFigure 3 is an illustration of the feature hierarchy in mfTacoma.\nFigure 3: Illustration of feature hierarchy in mfTacoma.\nThe mfTacoma algorithm is fairly simple to describe.\nFirst, all TMA\nimages are converted to their GLCM representations. Then the GLCMs\nare input to the autoencoder to extract the deep features, to be con-\ncatenated with the GLCMs. The manifold-augmented features and their\nrespective scores are fed to a training algorithm. The trained classiﬁer\nwill be applied to get scores for TMA images in the test set.\nTo give\nan algorithmic description to mfTacoma, assume there are n training in-\nstances, m test instances, and N = n + m. Denote the training sample\nby (I1, Y1), ..., (In, Yn) where Ii’s are images and Yi’s are scores (thus\nYi ∈{0, 1, 2, 3}). Let In+1, ..., In+m be new TMA images that one wish\nto score (i.e., the test set has a size of m). mfTacoma is described as\nAlgorithm 1.\nFor the rest of this section, we will brieﬂy describe the GLCM and au-\ntoencoder.\n2.1\nThe gray level co-occurrence matrix\nThe GLCM is one of the most widely used image statistics for textured\nimages, such as satellite images and high-resolution tissue images.\nIt\n5\nAlgorithm 1 The mfTacoma algorithm\n1: for i = 1 to N\n2: Compute GLCM of image Ii;\n3: Denote the resulting GLCM by Xi;\n4: endfor\n5: Find manifold representation ∪N\ni=1{Zi} for ∪N\ni=1{Xi} with an autoencoder;\n6: Concatenate Xi and Zi and get XM\ni , i = 1, ..., N;\n7: Feed ∪n\ni=1{(XM\ni , Yi)} to RF to obtain a classiﬁcation rule ˆf;\n8: Apply ˆf to XM\ni\nto obtain scores for images Ii for i = n + 1, ..., n + m.\ncan be crudely viewed as a “spatial histogram” of neighboring pixels in\nan image.\nIt has been successfully applied in a variety of applications\n[24, 21, 36, 56, 55]. We follow notations used in [56, 54].\nThe GLCM is deﬁned with respect to a particular spatial relationship\nof two neighboring pixels. The spatial relationship entails two aspects—a\nspatial direction, in set {ր,\nց,\nտ,\nւ,\n↓,\n↑,\n→,\n←}, and the\ndistance between the pair of pixels along the direction. For a given spa-\ntial relationship, the GLCM for an image is deﬁned as a Ng × Ng matrix\nwith its (a, b)-entry being the number of times two pixels with gray val-\nues a, b ∈{1, 2, ..., Ng} are spatial neighbors; here Ng is the number of\ngray levels or quantization levels in the image. Note that, for each spatial\nrelationship, one can deﬁne a GLCM thus one image can correspond to\nmultiple GLCMs. The deﬁnition of GLCM is illustrated in Figure 4 with\na toy image (taken from [55]). For a balance of computational eﬃciency\nand discriminative power, we take Ng = 51 and uniform quantization [23]\nis applied over the 256 gray levels.\nFigure 4: Illustration of GLCM via a toy image (image taken from [55]).\nIn the 4 × 4 toy image, there are three gray lavels, {1, 2, 3}; the resulting GLCM\nfor spatial relationship (ր, 1) is a 3 × 3 matrix.\n2.2\nAutoencoder with deep networks\nAn autoencoder is a special type of deep neural network with the inputs\nand outputs being the same. A neural network is a layered network that\ntries to emulate the network of connected biological neurons in the human\nbrain; it can be used for tasks such as classiﬁcation or regression. The ﬁrst\nlayer of a neural network accepts the input signals, and the last layer for\noutputs. Here each node (unit) in the input or output layer corresponding\n6\nto one component of the inputs or outputs when these are treated as a vec-\ntor; note for simplicity here we omit the nodes corresponding to the bias\nterms. Each node in the intermediate layers (called hidden layers) takes\ninputs from all the connecting nodes in the previous layer, then performs\na nonlinear activation operation and outputs the resulting signals to those\nconnecting nodes in the next layer. As the data ﬂows from one layer to\nthe next, a weight is applied at all the connecting links. An illustration\nof the deep neural network is given in Figure 5. In the following, we will\nformally describe the details.\nFigure 5: Illustration of deep neural network and autoencoder. X1, ..., X4 are\ncomponents of the inputs and Z1, Z2 are components of the codewords.\nAssume the input data is represented by a matrix XN×p, where N is\nthe number of data instances and p is the data dimension or the number\nof nodes in the input layer. Let the weights to the links connecting the\n(i-1)-th and i-th hidden layer be denoted by W (i), with its dimension de-\ntermined by the number of nodes involved. Let the output signal from the\ni-th hidden layer be denoted by Z(i). Let σ be the nonlinear activation\nfunction (assume all hidden layers have the same activation function for\nsimplicity). Assume there are K hidden layers. Then the output signals\nat the ﬁrst hidden layer are given by\nZ(1) = σ\n\u0010\nW (1)X + b(1)\u0011\n.\nWith the convention Z(0) = X, the output signals at the i-th hidden layer\ncan be expressed as\nZ(i) = σ\n\u0010\nW (i)Z(i−1) + b(i)\u0011\n, i = 1, 2, ...K.\nOften the training of the neural network is formulated as solving\narg min\nW ,b J(W , b, X, Y ) = l\n\u0010\nZ(K), Y\n\u0011\n−λ\n2 · ||W ||,\n(1)\nwhere l(.) is a loss function (e.g., squared error for regression or cross\nentropy for classiﬁcation), and ||.|| is a norm such as the L2 norm. The\nsolution to (1) is often obtained by the back-propagation algorithm. Z(K)\nis obtained from X by the composition of a series of functions\nσ ◦W (K) · · · σ ◦W (2)σ ◦W (1),\n7\nthus it can be written as Z(K) = fW ,b(X) for some function fW ,b, which\ncan be viewed as a smoothing function (or transformation) of the data.\nWhen the output Y is the same as the input X, the neural network is\ncalled an autoencoder. The deep network in Figure 5 becomes an autoen-\ncoder when all X′\ni = Xi; Z1, Z2 are components of the codewords, that is,\n(X1, ..., X4) →(Z1, Z2).\nAlthough not necessary but typically an autoencoder can be divided as\nthe encoder part and the decoder part which are mirror-symmetric (i.e.,\ncorresponding layers have the same number of units or connecting weights)\nw.r.t. the layer in the center. If some hidden layer has less units than\nthat of the inputs, that means the data X, at certain stage of its transfor-\nmation, has a dimension less than the original dimension thus achieving\na dimension reduction eﬀect. For example, in Figure 5, the original data\ndimension is 4 and the transformed data has a dimension of 2. If one is\nwilling to assume that the activation function is smooth, then the data can\nbe viewed as lying on a low dimensional manifold. The ﬁtting of the neu-\nral network can thus be viewed as a way of learning the low-dimensional\nmanifold. As the activation is nonlinear, the resulting manifold is also\nnonlinear. As individual hidden layers in a neural network can be viewed\nas extracting features of the original data, we will use such features as\nrepresentation of the low dimensional manifold. These features are called\nM-features, to be appended to the existing GLCM features in the scoring\nof TMA images.\nNote that we state in Section 1 that for TMA images, the training sample\nsize is often far less than that required by a typical deep neural network.\nHowever, we could still use the autoencoder, for two reasons. First the\ndeep representation we will extract with an autoencoder is to be used\nfor regularization, thus it would be suﬃcient as long as the representation\ncaptures main features of the manifolds. Second, we can control the size of\nthe deep network according to the training sample size; indeed we will be\nusing the simplest autoencoder, that is, with only one hidden layer in this\nwork. The algorithm for manifolds extraction is simply to extract infor-\nmation of the trained hidden layer, when using some deep neural network\npackage (The deepnet package is used in this work). Let nHidden be the\ndimension of the low-dimensional manifold. An algorithmic description is\ngiven as Algorithm 2.\nAlgorithm 2 mfLearner(X, nHidden)\n1: dnn ←dbn.dnn.train(X, X, nHidden);\n2: Extract from dnn a low-dimensional representation ∪N\ni=1{Zi};\n3: Return(∪N\ni=1{Zi});\n3\nExperiments\nWe conduct experiments on TMA images, the data at which our meth-\nods are primarily targeting. The TMA images are taken from the Stan-\nford Tissue Microarray Database (http://tma.stanford.edu/, see [37]).\nTMAs corresponding to the biomarker, estrogen receptor (ER), for breast\ncancer tissues are used since ER is a known well-studied biomarker. There\n8\nare a total of 695 such TMA images in the database, and each image is\nscored at four levels (i.e., label), from {0, 1, 2, 3}.\nThe GLCM corresponding to (ր, 3) is used, which, according to [56],\nis the spatial relationship that leads to the greatest discriminating power\nfor ER/breast cancer. The pathological interpretation is that, the staining\npattern is approximately rotationally invariant (thus the choice of direc-\ntion is no longer important) and ‘3’ is related to the size of the staining\npattern for ER/breast cancer.\nFor autoencoder, we use the R package deepnet. Random Forests (RF)\n[7] is chosen as the classiﬁer due to its superior performance compared to\npopular methods such as support vector machines (SVM) [15] and boost-\ning [18], according to large scale simulation studies [10]. This is also true\nfor the scoring ([56]; [55]) and the segmentation of TMA images [29]. This\nis likely due to the high dimensionality (2601 when using GLCM) and the\nremarkable feature selection as well as noise-resistance ability of RF, while\nSVM and boosting methods are typically prune to those.\nFor simplicity, the test set error rate is used as our performance met-\nric. In all experiments, a random selection of half of the data are used\nfor training and the rest for test, and results are averaged over 100 runs.\nWe conduct two types of experiments.\nOne is to use linear manifolds\nextracted by PCA. The other is to use nonlinear manifolds extracted by\nautoencoder.\n3.1\nExperiments with manifolds by PCA\nWe mention in Section 2 that one can extract linear manifolds with PCA.\nHow eﬀective are those linear manifolds in the scoring of TMA images?\nWe distinguish between two cases. One is to use the leading principal\ncomponents as sole features to be input the classiﬁer, the other is to use\nthe leading principal components as regularizing features appended to the\nGLCMs. The results are shown in Figure 6 where we produce the test set\nerror rates when the number of leading principal components increases\nup to 100. It can be seen that the PCA features as regularizing features\nclearly outperform that using those as sole features with a performance\ngap close to about 6%.\n3.2\nExperiments with manifolds by autoencoder\nFor nonlinear manifolds with autoencoder, we conduct experiments on\nthree diﬀerent cases: 1) Use all the original image pixels as features along\nwith M-features extracted from the space of the original images; 2) GLCM\nfeatures along with M-features extracted from the space of the original im-\nages; 3) GLCM features along with M-features extracted from the space\nof GLCMs. We also tried using M-features extracted from the space of\noriginal images alone and that of GLCMs alone, but none gave satisfac-\ntory results. Table 1 shows results obtained in the three cases where the\ndimension of the low-dimensional manifold varies.\nUsing original images pixels as features along with manifolds features\nextracted from the space of original images does not lead to satisfactory\nresults, while using GLCMs with manifolds features from the space of\n9\nFigure 6: Test set error rate when including more leading principal components.\nLeft: using only principal components. Right: using both GLCM features and\nprincipal components.\noriginal images improves marginally over using GLCMs alone. The best\nresult is achieved at an error rate of 22.85% when using GLCM features\nalong with M-features extracted from the space of GLCMs with the di-\nmension of the manifold being 25. Compared to an error rate of 23.28%\nachieved by deepTacoma [55] and 24.79% without using any deep features\nby TACOMA [56], the improvement is substantial given that the perfor-\nmance by TACOMA [56] already rivals pathologists, and that progress in\nthis area is typically incremental in nature.\n4\nConclusions\nInspired by the recent success of semi-supervised learning and deep learn-\ning, we explore deep representation learning under small data, in the\ncontext of TMA image scoring. In particular, we propose the mfTacoma\nalgorithm to extract the low-dimensional manifolds in high dimensional\ndata. Under mfTacoma, deep representations about the manifolds, possi-\nbly crude due to small data, are conveniently used as regularizing features\nto be appended to the original data features. This turns out to be a simple\nand eﬀective way of making use of the low dimensional manifold informa-\ntion. Our experiments show that mfTacoma outperforms linear manifold\nfeatures extracted by PCA or deep features of a group nature. We con-\nsider this a notable improvement over TACOMA and deepTacoma given\nthat those already rival trained pathologists in the scoring of TMA images\nand progress in this area is typically incremental in nature.\nGiven the prevalence of low dimensional manifolds in high dimensional\ndata, we expect that deep features derived from low dimensional mani-\nfolds would help in many applications. Our approach of leveraging the\nmanifold information as regularizing features will be useful in small data\nsetting, where one may incorporate feature weights by accounting for the\nsample size or data quality.\n10\nFeatures\nDim. of manifold\nError rate\nGLCM\n—\n24.79%\nImage + AE image features\n100\n29.89%\n50\n29.88%\n25\n29.90%\nGLCM + AE image features\n400\n24.51%\n300\n24.64%\n200\n24.03%\n100\n24.63%\n50\n24.42%\n25\n24.36%\n10\n24.59%\nGLCM + AE GLCM features\n200\n24.72%\n100\n24.66%\n50\n24.59%\n40\n23.84%\n30\n23.23%\n25\n22.85%\n20\n23.08%\n10\n23.75%\nTable 1: Error rate in scoring TMA images when using diﬀerent deep features\nunder diﬀerent manifold dimensions. Note that the ﬁrst row corresponds to re-\nsults obtained by RF on the GLCM features alone (i.e., without deep features).\nHere ‘Image’ indicates when the original image pixels are used as features, ‘AE\nimage features’ indicates manifold features extracted from the space of the orig-\ninal TMA images, ‘AE GLCM features’ indicates manifold features extracted\nfrom the space of the GLCM of the TMA images.\nReferences\n[1] A. Beck, A. Sangoi, S. Leung, R. Marinelli, T. Nielsen TO, M. van de\nVijver, R. West, M. van de Rijn, and D. Koller. Systematic analysis of\nbreast cancer morphology uncovers stromal features associated with\nsurvival. Science Translational Medicine, 3(108):108–113, 2011.\n[2] M. Belkin and P. Niyogi.\nLaplacian eigenmaps for dimensionality\nreduction and data representation.\nNeural Computation, 15:1373–\n1396, 2003.\n[3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A\ngeometric framework for learning from labeled and unlabeled exam-\nples. Journal of Machine Learning Research, 7:2399–2434, 2006.\n[4] S. Bentzen, F. Buﬀa, and G. Wilson.\nMultiple biomarker tissue\nmicroarrays: bioinformatics and practical approaches. Cancer and\nMetastasis Reviews, 27(3):481–494, 2008.\n[5] A. Berger, D. Davis, C. Tellez, V. Prieto, J. Gershenwald, M. John-\nson, D. Rimm, and M. Bar-Eli.\nAutomated quantitative analysis\nof activator protein-2 α subcellular expression in melanoma tissue\nmicroarrays correlates with survival prediction.\nCancer research,\n65(23):11185, 2005.\n11\n[6] P. J. Bickel and D. Yan. Sparsity and the possibility of inference.\nSankhya: The Indian Journal of Statistics, Series A (2008-), 70(1):1–\n24, 2008.\n[7] L. Breiman. Random Forests. Machine Learning, 45(1):5–32, 2001.\n[8] R. Camp, G. Chung, D. Rimm, et al. Automated subcellular local-\nization and quantiﬁcation of protein expression in tissue microarrays.\nNature medicine, 8(11):1323–1327, 2002.\n[9] R. Camp, V. Neumeister, and D. Rimm. A decade of tissue microar-\nrays: progress in the discovery and validation of cancer biomarkers.\nJournal of Clinical Oncology, 26(34):5630–5637, 2008.\n[10] R. Caruana, N. Karampatziakis, and A. Yessenalina. An empirical\nevaluation of supervised learning in high dimensions. In Proceedings\nof the Twenty-Fifth International Conference on Machine Learning\n(ICML), pages 96–103, 2008.\n[11] B. J. Casey, F. Soliman, K. G. Bath, and C. E. Glatt.\nImaging\ngenetics and development: challenges and promises. Human Brain\nMapping, 31(6):838–851, 2010.\n[12] L. Cayton.\nAlgorithms for manifold learning.\nTechnical Report\nCS2008-0923, Department of Computer Science, UC San Diego,\n2008.\n[13] O. Chapelle, J. Weston, and B. Sch¨olkopf. Cluster kernels for semi-\nsupervised learning. In Advances in Neural Information Processing\nSystems 15, pages 601–608, 2003.\n[14] G. Chung, E. Kielhorn, and D. Rimm. Subjective diﬀerences in out-\ncome are seen as a function of the immunohistochemical method used\non a colorectal cancer tissue microarray. Clinical Colorectal Cancer,\n1(4):237–242, 2002.\n[15] C. Cortes and V. N. Vapnik.\nSupport-vector networks.\nMachine\nLearning, 20(3):273–297, 1995.\n[16] K. DiVito and R. Camp. Tissue microarrays–automated analysis and\nfuture directions. Breast Cancer Online, 8(7), 2005.\n[17] D. Donoho and C. Grimes. Hessian eigenmaps: Locally linear em-\nbedding techniques for high-dimensional data.\nProceedings of the\nNational Academy of Sciences, U. S. A., 100(10):5591–5596, 2003.\n[18] Y. Freund and R. Schapire. Experiments with a new boosting algo-\nrithm. In International Conference on Machine Learning (ICML),\n1996.\n[19] G. Fromont, M. Roupret, N. Amira, M. Sibony, G. Vallancien, P. Va-\nlidire, and O. Cussenot. Tissue microarray analysis of the prognostic\nvalue of E-Cadherin, Ki67, p53, p27, Survivin and MSH2 expression\nin upper urinary tract transitional cell Carcinoma. European Urology,\n48(5):764–770, 2005.\n[20] J. Giltnane and D. Rimm.\nTechnology insight:\nidentiﬁcation of\nbiomarkers with tissue microarray technology. Nature Clinical Prac-\ntice Oncology, 1(2):104–111, 2004.\n[21] P. Gong, D. Marceau, and P. J. Howarth. A comparison of spatial\nfeature extraction algorithms for land-use classiﬁcation with SPOT\nHRV data. Remote Sensing of Environment, 40:137–151, 1992.\n12\n[22] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. The MIT\nPress, 2016.\n[23] R. M. Gray and D. L. Neuhoﬀ. Quantization. IEEE Transactions of\nInformation Theory, 44(6):2325–2383, 1998.\n[24] R. M. Haralick.\nStatistical and structural approaches to texture.\nProceedings of IEEE, 67(5):786–803, 1979.\n[25] S. Hassan, C. Ferrario, A. Mamo, and M. Basik.\nTissue microar-\nrays: emerging standard for biomarker validation. Current Opinion\nin Biotechnology, 19(1):19–25, 2008.\n[26] C. Hegde, M. Wakin, and R. Baraniuk.\nRandom projections for\nmanifold learning. In Neural Information Processing Systems (NIPS),\nvolume 20, 2007.\n[27] D. Hibar, O. Kohannim, J. Stein, M.-C. Chiang, and P. Thompson.\nMultilocus genetic analysis of brain images. Frontiers in Genetics,\n2(73):1–11, 2011.\n[28] G. Hinton and R. Salakhutdinov.\nReducing the dimensionality of\ndata with neural networks. Science, 313:504–507, 2006.\n[29] S. Holmes, A. Kapelner, and P. Lee. An interactive Java statistical\nimage segmentation system: Gemident. Journal of Statistical Soft-\nware, 30(10):1–20, 2009.\n[30] H. Hotelling. Analysis of a complex of statistical variables into prin-\ncipal components. Journal of Educational Psychology, 24:417–441,\n1933.\n[31] C. Hsu, D. Ho, C. Yang, C. Lai, I. Yu, and H. Chiang. Interobserver\nreproducibility of Her-2/neu protein overexpression in invasive breast\ncarcinoma using the DAKO HercepTest. American journal of clinical\npathology, 118(5):693–698, 2002.\n[32] X. Huo, X. Ni, and A. Smith. A survey of manifold-based learning\nmethods. Recent Advances in Data Mining of Enterprise Data, pages\n691–745, 2007.\n[33] J. Kononen, L. Bubendorf, A. Kallionimeni, M. B¨arlund, P. Schraml,\nS. Leighton, J. Torhorst, M. Mihatsch, G. Sauter, and O. Kallioni-\nmeni. Tissue microarrays for high-throughput molecular proﬁling of\ntumor specimens. Nature Medicine, 4(7):844–847, 1998.\n[34] Y. LeCun, Y. Bengio, and G. Hinton.\nDeep learning.\nNature,\n521:436–444, 2015.\n[35] N. Lei, Z. Luo, S.-T. Yau, and D. X. Gu. Geometric understanding\nof deep learning. arXiv:1805.10451, 2018.\n[36] C. D. Lloyd, S. Berberoglu, P. J. Curran, and P. M. Atkinson.\nA comparison of texture measures for the per-ﬁeld classiﬁcation of\nMediterranean land cover. International Journal of Remote Sensing,\n25(19):3943–3965, 2004.\n[37] R. Marinelli, K. Montgomery, C. Liu, N. Shah, W. Prapong,\nM. Nitzberg, Z. Zachariah, G. Sherlock, Y. Natkunam, R. West,\net al. The Stanford tissue microarray database. Nucleic Acids Re-\nsearch, 36:D871–877, 2007.\n[38] S. Mousses, L. Bubendorf, U. Wagner, G. Hostetter, J. Kononen,\nR. Cornelison, N. Goldberger, A. Elkahloun, N. Willi, P. Koivisto,\n13\nW. Ferhle, M. Raﬀeld, G. Sauter, and O. Kallioniemi. Clinical valida-\ntion of candidate genes associated with prostate cancer progression in\nthe cwr22 model system using tissue microarrays. Cancer Research,\n62(5):1256–1260, 2002.\n[39] B. Nadler, S. Lafon, R. Coifman, and I. G. Kevrekidis.\nDiﬀusion\nmaps - a probabilistic interpretation for spectral embedding and clus-\ntering algorithms.\nPrincipal Manifolds for Data Visualization and\nDimension Reduction (Lecture Notes in Computational Science and\nEngineering), 58:238–260, 2007.\n[40] S. Osher, Z. Shi, and W. Zhu. Low dimensional manifold model for\nimage processing. SIAM Journal on Imaging Sciences, 10(4):1669–\n1690, 2017.\n[41] D. Perraul-Joncas and M. Meila. Non-linear dimensionality reduc-\ntion: Riemannian metric estimation and the problem of geometric\ndiscovery. arXiv:1305.7255, 2013.\n[42] R. Pless and R. Souvenir. A survey of manifold learning for images.\nIPSJ Transactions on Computer Vision and Applications, pages 83–\n94, 2009.\n[43] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally\nlinear embedding. Science, 290(5500):2323–2326, 2000.\n[44] J. Schmidt-Hieber. Nonparametric regression using deep neural net-\nworks with ReLU activation function. arXiv:1708.06633, 2017.\n[45] C. Shahabi and D. Yan. Real-time pattern isolation and recognition\nover immersive sensor data streams. In Proceedings of the 9th Inter-\nnational conference on multi-media modeling, pages 93–113, 2003.\n[46] J.\nShlens.\nA\ntutorial\non\nprincipal\ncomponent\nanalysis.\narXiv:1404.1100, 2014.\n[47] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geo-\nmetric framework for nonlinear dimensionality reduction. Science,\n290(5500):2319–2323, 2000.\n[48] T. Thomson, M. Hayes, J. Spinelli, E. Hilland, C. Sawrenko,\nD. Phillips, B. Dupuis, and R. Parker. HER-2/neu in breast cancer:\ninterobserver variability and performance of immunohistochemistry\nwith 4 antibodies compared with ﬂuorescent in situ hybridization.\nModern Pathology, 14(11):1079–1086, 2001.\n[49] US National Cancer Institute. https://www.cancer.gov/about-cancer/understanding/what-is-cancer.\n[50] D. Voduc, C. Kenney, and T. Nielsen. Tissue microarrays in clinical\noncology. Seminars in radiation oncology, 18(2):89–97, 2008.\n[51] H. Vrolijk, W. Sloos, W. Mesker, P. Franken, R. Fodde, H. Morreau,\nand H. Tanke. Automated Acquisition of Stained Tissue Microarrays\nfor High-Throughput Evaluation of Molecular Targets. Journal of\nMolecular Diagnostics, 5(3):160–167, 2003.\n[52] R. Walker. Quantiﬁcation of immunohistochemistry - issues concern-\ning methods, utility and semiquantitative assessment I. Histopathol-\nogy, 49(4):406–410, 2006.\n[53] W. H. Wan, M. B. Fortuna, and P. Furmanski. A rapid and eﬃcient\nmethod for testing immunohistochemical reactivity of monoclonal\nantibodies against multiple tissue samples simultaneously. Journal\nof Immunological Methods, 103:121–129, 1987.\n14\n[54] D. Yan, P. Bickel, and P. Gong. A bottom-up approach for texture\nmodeling with application to Ikonos image classiﬁcation. Submitted,\n2018.\n[55] D. Yan, T. W. Randolph, J. Zou, and P. Gong. Incorporating deep\nfeatures in the analysis of tissue microarray images. Statistics and\nIts Interface, 12(2):283–293, 2019.\n[56] D. Yan, P. Wang, B. S. Knudsen, M. Linden, and T. W. Randolph.\nStatistical methods for tissue microarray images–algorithmic scoring\nand co-training. The Annals of Applied Statistics, 6(3):1280–1305,\n2012.\n[57] Z. Zhang and H. Zha. Principal manifolds and nonlinear dimension\nreduction via local tangent space alignment. SIAM Journal on Sci-\nentiﬁc Computing, 26(1):313–338, 2004.\n[58] H. Zhao, P. C. Yuen, and J. T. Kwok. A novel incremental principal\ncomponent analysis and its application for face recognition. IEEE\nTransactions on Systems, Man, and Cybernetics–Part B: Cybernet-\nics, 36(4):873–886, 2006.\n[59] X. Zhu. Semi-supervised learning literature survey. TR 1530, Depart-\nment of Computer Science, University of Wisconsin-Madison, 2008.\n15\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-02-22",
  "updated": "2021-02-22"
}