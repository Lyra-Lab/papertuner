{
  "id": "http://arxiv.org/abs/2102.11914v1",
  "title": "A Robotic Model of Hippocampal Reverse Replay for Reinforcement Learning",
  "authors": [
    "Matthew T. Whelan",
    "Tony J. Prescott",
    "Eleni Vasilaki"
  ],
  "abstract": "Hippocampal reverse replay is thought to contribute to learning, and\nparticularly reinforcement learning, in animals. We present a computational\nmodel of learning in the hippocampus that builds on a previous model of the\nhippocampal-striatal network viewed as implementing a three-factor\nreinforcement learning rule. To augment this model with hippocampal reverse\nreplay, a novel policy gradient learning rule is derived that associates place\ncell activity with responses in cells representing actions. This new model is\nevaluated using a simulated robot spatial navigation task inspired by the\nMorris water maze. Results show that reverse replay can accelerate learning\nfrom reinforcement, whilst improving stability and robustness over multiple\ntrials. As implied by the neurobiological data, our study implies that reverse\nreplay can make a significant positive contribution to reinforcement learning,\nalthough learning that is less efficient and less stable is possible in its\nabsence. We conclude that reverse replay may enhance reinforcement learning in\nthe mammalian hippocampal-striatal system rather than provide its core\nmechanism.",
  "text": "A Robotic Model of Hippocampal Reverse Replay for\nReinforcement Learning\nMatthew T. Whelan,1,2 Tony J. Prescott,1,2 Eleni Vasilaki1,2,∗\n1Department of Computer Science, The University of Shefﬁeld, Shefﬁeld, UK\n2Shefﬁeld Robotics, Shefﬁeld, UK\n∗Corresponding author: e.vasilaki@shefﬁeld.ac.uk\nKeywords: hippocampal reply, reinforcement learning, robotics, computational neuroscience\nAbstract\nHippocampal reverse replay is thought to contribute to learning, and particu-\nlarly reinforcement learning, in animals. We present a computational model of\nlearning in the hippocampus that builds on a previous model of the hippocampal-\nstriatal network viewed as implementing a three-factor reinforcement learning rule.\nTo augment this model with hippocampal reverse replay, a novel policy gradient\nlearning rule is derived that associates place cell activity with responses in cells\nrepresenting actions. This new model is evaluated using a simulated robot spatial\nnavigation task inspired by the Morris water maze. Results show that reverse replay\ncan accelerate learning from reinforcement, whilst improving stability and robust-\nness over multiple trials. As implied by the neurobiological data, our study implies\nthat reverse replay can make a signiﬁcant positive contribution to reinforcement\nlearning, although learning that is less efﬁcient and less stable is possible in its ab-\nsence. We conclude that reverse replay may enhance reinforcement learning in the\nmammalian hippocampal-striatal system rather than provide its core mechanism.\n1\narXiv:2102.11914v1  [q-bio.NC]  23 Feb 2021\n1\nIntroduction\nMany of the challenges in the development of effective and adaptable robots can be posed as\nreinforcement learning (RL) problems; consequently there has been no shortage of attempts\nto apply RL methods to robotics (26, 45). However, robotics also poses signiﬁcant challenges\nfor RL systems. These include factors such as continuous state and action spaces, real-time\nand end-to-end learning, reward signalling, behavioural traps, computational efﬁciency, lim-\nited training examples, non-episodic resetting, and lack of convergence due to non-stationary\nenvironments (26,27,53).\nMuch of RL theory has been inspired by early behavioural studies in animals (45), and for\ngood reason, since biology has found many of the solutions to the control problems we are\nsearching for in our engineered systems. As such, with continued developments in biology, and\nparticularly in neuroscience, it would be wise to continue transferring insights from biology\ninto robotics (38). Yet equally important is its inverse, the use our computational and robotic\nmodels to inform our understanding of the biology (30,49). Robots offer a valuable real-world\ntesting opportunity to validate computational neuroscience models (24,36,43).\nThough the neurobiology of RL has largely centred on the role of dopamine as a reward-\nprediction error signal (39, 42), there are still questions surrounding how brain regions might\ncoordinate with dopamine release for effective learning. Behavioural timescales evolve over\nseconds, perhaps longer, whilst the timescales for synaptic plasticity in mechanisms such as\nspike-timing dependent plasticity (STDP) evolve over milliseconds (2) – how does the nervous\nsystem bridge these time differentials so that rewarded behaviour is reinforced at the level of\nsynaptic plasticities?\nOne recent hypothesis offering an explanation to this problem has been in three-factor learn-\ning rules (9,11,40,47). In the three-factor learning rule hypothesis, learning at synapses occurs\n2\nonly in the presence of a third factor, with the ﬁrst and second factors being the typical pre- and\npost-synaptic activities. This can be stated in a general form as follows,\nd\ndtwij = ηf(xj)g(yi)M(t)\n(1)\nwhere η is the learning rate, xj represents a pre-synaptic neuron with index j, yi a post-synaptic\nneuron with index i, and f(·) and g(·) being functions mapping respectively the pre- and post-\nsynaptic neuron activities. M(t) represents the third factor, which here is not speciﬁc to the\nneuron indices i and j and is therefore a global term. This third factor is speculated to represent\na neuromodulatory signal, which in this case is best thought of as dopamine, or more generally\nas a reward signal. Equation 1 appears to possess the problem stated above, of how learning can\noccur for neurons that were co-active prior to the introduction of the third factor. To solve this,\na synaptic-speciﬁc eligibility trace is introduced, which is a time-decaying form of the pre- and\npost-synaptic activities (11),\nd\ndteij = −eij\nτe\n+ ηf(xj)g(yi)\nd\ndtwij = eijM(t)\n(2)\nThe eligibility trace time constant, τe, modulates how far back in time two neurons were co-\nactive in order for learning to occur – the larger τe is, the more of the behavioural time history\nwill be learned and therefore reinforced. To effectively learn behavioural sequences over the\ntime course of seconds τe is set to be in the range of a few seconds (11). Work conducted by\nVasilaki et al. (47) successfully applied such a learning mechanism in a spiking network model\nfor a simulated agent learning to navigate in a Morris water maze task (47), in which they used\na value of 5s for τe, a value that was optimised to that speciﬁc setting.\nHippocampal replay however suggests an alternative approach, building on the three-factor\nlearning rule but avoiding the need for synapse-speciﬁc eligibility traces. Hippocampal replay\nwas originally shown in rodents as the reactivation during sleep states of hippocampal place\n3\ncells that were active during a prior awake behavioural episode (44,52). During replay events,\nthe place cells retain the temporal ordering experienced during the awake behavioural state,\nbut do so on a compressed timescale – replays typically replay cell activities over the course\nof a few tenths of a second, as opposed to the few seconds it took during awake behaviour.\nFurthermore, experimental results presented later to these original results showed that replays\ncan occur in the reverse direction too, and that these reverse replays occurred when the rodent\nhad just reached a reward location (5,8). Interestingly, these replays would repeat the rodent’s\nimmediate behavioural sequence that had led up to the reward, which led Foster and Wilson (8)\nto speculate that hippocampal reverse replays, coupled with phasic dopamine release, might be\nsuch a mechanism to reinforce behavioural sequences leading to rewards.\nWhilst it has been well established that hippocampal neurons project to the nucleus accum-\nbens (22), the proposal that reverse replays may play an important role in RL has since received\nfurther support. For instance, there are experimental results showing that reverse replays often\nco-occur with replays of the ventral striatum (35) as well as there being increased activity in the\nventral tegmental area during awake replays (14), which is an important region for dopamine\nrelease. Furthermore, rewards have been shown to modulate the frequency with which reverse\nreplays occur, such that increased rewards promotes more reverse replays, whilst decreased\nrewards suppresses reverse replays (1).\nTo help better understand the role of hippocampal reverse replays in the RL process, we\npresent here a neural RL network model that has been augmented with a hippocampal CA3\ninspired network capable of producing reverse replays. The network has been implemented\non a simulation of the biomimetic robot MiRo-e (31) to show its effectiveness in a robotic\nsetting. The RL model is an adapted hippocampal-striatal inspired spiking network by (47)\nderived using a policy gradient method, but modiﬁed here for continuous-rate valued neurons –\nthis modiﬁcation leads to a novel learning rule, though similar to previous learning rules (51),\n4\nfor this particular network architecture. The hippocampal reverse replay network meanwhile is\ntaken from Whelan et al. (50), who implemented the network on the same MiRo robot, itself\nbased on earlier work by Haga and Fukai (18) and Pang and Fairhall (34). We demonstrate in\nrobotic simulations that activity replay improves the stability and robustness of the RL algorithm\nby providing an additional signal to the eligibility trace.\n2\nMethodology\n2.1\nMiRo Robot and the Testing Environment\nWe implemented the model using a simulation of the biomimetic robot MiRo-e. The MiRo\nrobot is a commercially available biomimetic robot developed by Consequential Robotics Ltd\nin partnership with the University of Shefﬁeld. MiRo’s physical design and control system\narchitecture are inspired by biology, psychology and neuroscience (31) making it a useful plat-\nform for embedded testing of brain-inspired models of perception, memory and learning (28).\nFor mobility it is differentially driven, whilst for sensing we make use of its front facing sonar\nfor the detection of approaching walls and objects. The Gazebo physics engine is used to per-\nform simulations, where we take advantage of the readily available open-arena (Figure 1C).\nThe simulator uses the Kinetic Kame distribution of the Robot Operating System (ROS). Full\nspeciﬁcations for the MiRo robot, including instructions for simulator setup, can be found on\nthe MiRo documentation web page (4).\n2.2\nNetwork Architecture\nThe network is composed of a layer of 100 bidirectionally connected place cells, which con-\nnects feedforwardly to a layer of 72 action cells via a weight matrix of size 100×72 (Figure\n1B). In this model, activity in each of the place cells is set to encode a speciﬁc location in the\nenvironment (32, 33). Place cell activities are generated heuristically using two dimensional\n5\nnormal distributions of activity inputs, determined as a function of MiRo’s position from each\nplace ﬁeld’s centre point (Figure 1A). This is similar to other approaches of place cell activity\ngeneration (18,47). The action cells are driven by the place cells, with each action cell encod-\ning for a speciﬁc with 5 degree increments, thus 72 action cells encode 360 degrees of possible\nheading directions. By computing a population vector of the action cell activities, these dis-\ncreet heading directions can be transformed into continuous headings. For simplicity, MiRo’s\nforward velocity is kept constant at 0.2m/s. We now describe the details of the network in full.\n2.2.1\nHippocampal Place Cells\nThe network model of place cells represents a simpliﬁed hippocampal CA3 network that is\ncapable of generating reverse replays of recent place cell sequence trajectories. This model of\nreverse replays was ﬁrst presented in (50), but with one minor modiﬁcation. Whereas the reverse\nreplay model in (50) has a global inhibitory term acting on all place cells, here the place cells\nhave those inhibitory inputs removed from their dynamics. This modiﬁcation does not affect\nthe ability of the network to produce reverse replays, which is shown in the Supplementary\nMaterial, where reverse replays both with and without global inhibition are compared.\nIn more detail, the place cells consist of a network of 100 neurons, each of which is bidi-\nrectionally connected to its eight nearest neighbours as determined by the positioning of their\nplace ﬁelds. Hence, place cells with neighbouring place ﬁelds are bidirectionally connected to\none another (Figure 1B), whereas place cells whose place ﬁelds are further than one place ﬁeld\napart are not. In this manner, the connectivity of the network represents a map of the environ-\nment. This network approach is similar to the network approach taken by Haga and Fukai (18)\nin their model of reverse replay, except their weights are plastic whilst we keep ours static. The\nstatic weights for each cell, represented by wplace\njk\nindicating the weight projecting from neuron\nk onto neuron j, are all set to 1, with no cells self-projecting to themselves. Figure 1B displays\n6\nFigure 1: The testing environment, showing the simulated MiRo robot in a circular arena. A)\nPlace ﬁelds are spread evenly across the environment, with some overlap, and place cell rates\nare determined by the normally distributed input computed as a function of MiRo’s distance\nfrom the place ﬁeld’s centre. B) Place cells (blue, bottom set of neurons) are bidirectionally\nconnected to their eight nearest neighbours. These synapses have no long-term plasticity, but\ndo have short-term plasticity. Each place cell connects feedforwardly via long-term plastic\nsynapses to a network of action cells (red, top set of neurons). In total there are 100 place cells\nand 72 action cells.\nthe full connectivity schema for the bidirectionally connected place cell network.\nThe rate for each place cell neuron, represented by xj, is given as a linearly rectiﬁed rate\n7\nwith upper and lower bounds,\nxj =\n\n\n\n\n\n0\nif x\n′\nj < 0\n100\nif x\n′\nj > 100\nx\n′\nj\notherwise.\n(3)\nThe variable x\n′\nj is deﬁned as,\nx\n′\nj = α (Ij −ϵ)\nwhere α, ϵ are constants and Ij is the cell’s activity, which evolves according to time decaying\nﬁrst order dynamics,\nτI\nd\ndtIj = −Ij + ψjIsyn\nj\n+ Iplace\nj\n(4)\nwhere τI is the time constant, Isyn\nj\nis the synaptic inputs from the cell’s neighbouring neurons,\nand Iplace\nj\nis the place speciﬁc input calculated as per a normal distribution of MiRo’s position\nfrom the place ﬁeld’s centre point. ψj represents the place cell’s intrinsic plasticity, detailed\nfurther below.\nEach place cell has associated with it a place ﬁeld in the environment deﬁned by its centre\npoint and width, with place ﬁelds distributed evenly across the environment (100 in total). As\nstated, the place speciﬁc input, Iplace\nj\n, is computed from a two-dimensional normal distribution\ndetermined by MiRo’s distance from the place ﬁeld’s centre point,\nIplace\nj\n= Ip\nmaxexp\n\u0014\n−(xc\nMiRo −xc\nj)2 + (yc\nMiRo −yc\nj)2\n2d2\n\u0015\n(5)\nwhere Ip\nmax determines the max value for the place cell input. (xc\nMiRo, yc\nMiRo) represents MiRo’s\n(x, y) coordinate position in the environment, whilst (xc\nj, yc\nj) is the location of the place ﬁeld’s\ncentre point. The term d in the denominator is a constant that determines the width of the place\nﬁeld.\nThe synaptic inputs, Isyn\nj\n, are computed as a sum over neighbouring synaptic inputs modu-\n8\nlated by the effects of short-term depression and facilitation, Dk and Fk respectively,\nIsyn\nj\n= λ\n8\nX\nk=1\nwplace\njk\nxkDkFk\n(6)\nwhere wplace\njk\nis the weight projecting from place cell k onto place cell j. In this model, all these\nweights are ﬁxed at a value of 1. λ takes on a value of 0 or 1 depending on whether MiRo is\nexploring (λ = 0) or is at the reward (λ = 1). This prevents any synaptic transmissions during\nexploration, but not whilst MiRo is at the reward (the point at which reverse replays occur).\nThis two-stage approach can be found in similar models as a means to separate an encoding\nstage during exploration from a retrieval stage (41), and was a key feature of some of the early\nassociative memory models (21). Experimental evidence also supports this two-stage process\ndue to the effects of acetylcholine. Acetylcholine levels have been shown to be high during\nexploration but drop during rest (25), whilst acetlycholine itself has the effect of suppressing the\nrecurrent synaptic transmissions in the hippocampal CA3 region (20). It is for this reason that\nthe global inhibitory inputs found in (50) are not necessary, as the λ term here does functionally\nthe same operation as the inhibitory inputs (inhibition is decreased during reverse replays, thus\nincreasing synaptic transmission), yet is simpler to implement.\nDk and Fk in Equation 6 are respectively the short-term depression and short-term facilita-\ntion terms, and for each place cell these are computed as (as in (18), but see (7,46,48)),\nd\ndtDk = 1 −Dk\nτSTD\n−xkDkFk\n(7)\nd\ndtFk = U −Fk\nτSTF\n+ U (1 −Fk) xk\n(8)\nwhere τSTD and τSTF are the time constants, and U is a constant representing the steady-state\nvalue for short-term facilitation when there is no neuron activity (xk = 0). Dk and Fk each take\non values in the range [0, 1]. Notice that when xk > 0, short-term depression is driven steadily\ntowards 0, whereas short-term facilitation is driven steadily upwards towards 1. Modiﬁcation\n9\nof the time constants allows either short-term depression or short-term facilitation effects to\ndominate. In this model, the time constants are chosen so that depression is the primary short-\nterm effect. This ensures that during reverse replay events, activity propagating from one neuron\nto the next quickly dissipates, allowing for stable replays without activity exploding in the\nnetwork.\nWe turn ﬁnally to the intrinsic plasticity term in Equation 4, represented by ψj. Its be-\nhaviour, as observed in Equation 4, is to scale all incoming synaptic inputs. In (34), Pang and\nFairhall used a heuristically developed sigmoid whose output was determined as a function of\nthe neuron’s rate. Intrinsic plasticity in their model did not decay once it had been activated.\nSince our robot often travels across most of the environment, we needed a time decaying form\nof intrinsic plasticity to avoid potentiating all cells in the network. The simplest form of such\ntime decaying intrinsic plasticity is therefore,\nd\ndtψj = ψss −ψj\nτψ\n+\nψmax −1\n1 + exp [−β(xj −xψ)]\n(9)\nwith again, τψ being its time constant, and ψss being a constant that determines the steady state\nvalue for when the sigmoidal term on the right is 0. All of ψmax, β and xψ are constants that\ndetermine the shape of the sigmoid. Since ψj could potentially grow beyond the value of ψmax,\nwe restrict ψj so that if ψj > ψmax, then ψj is set to ψmax.\nIn order to initiate a replay event, place cell inputs, computed using Equation (5) with\nMiRo’s current location at the reward, are input into the place cell dynamics (Eqtn 4) one sec-\nond after MiRo reaches the reward, for a duration of 100ms. Intrinsic plasticity for those cells\nthat were most recently active during the trajectory is increased, whilst synaptic conductance in\nthe place cell network is turned on by setting λ = 1. This causes the place cell input to activate\nonly its adjacent cells that were recently active. This effects continues throughout all recently\nactive cells, thus resulting in a reverse replay. Short-term depression ensures that the activity\n10\ndissipates quickly as it propagates from one neuron to the next.\n2.2.2\nStriatal Action Cells\nThe action cell values determine how MiRo moves in the environment. All place cells project\nfeedforwardly through a set of plastic synapses to all action cells, as shown in Figure 1B. There\nare 72 action cells, the value of each drawn from a Gaussian distribution with mean ˜yi and\nvariance σ2,\nyi ∼N\n\u0000˜yi, σ2\u0001\n(10)\nThe mean value ˜yi is calculated as follows,\n˜yi =\n1\n1 + exp\nh\n−c1\nP100\nj=1 wPC-AC\nij\nxj −c2\ni\n(11)\nwith c1 and c2 determining the shape of the sigmoid. wPC-AC\nij\nrepresents the weight projecting\nfrom place cell j onto action cell i. The sigmoidal function is one possible choice which results\nin saturating terms in the RL learning rule (Section 5), an alternative option for instance could\nhave been a linear function. The action cells are restricted to take on values between 0 and 1,\ni.e. yi →[0, 1], and be interpreted as normalised ﬁring rates.\nMiRo moves at a constant forward velocity, whereas the output of the action cells sets a\ntarget heading for MiRo to move in. This target heading is allocentric, in that the heading is\nrelative to the arena. The activity for each action cell is denoted as yi and the target heading as\nθtarget. To ﬁnd the heading from the action cells, the population vector of the action cell values\nis computed as follows,\nθtarget = arctan\n\u0012 P\ni yi sin θi\nP\ni yi cos θi\n\u0013\n(12)\nwhere θi is the angle coded for by action cell i. It is also possible to compute the magnitude\nof the population vector, which denotes how strongly the action cell activities are promoting a\n11\nparticular heading,\nmtarget =\nv\nu\nu\nt\n X\ni\nyi sin θi\n!2\n+\n X\ni\nyi cos θi\n!2\n(13)\nFor practical reasons, the action cells are computed not only from place cell inputs, but\nalso by a separate module, termed a semi-random walk module. This is because the network,\nparticularly in the early stages of exploration when the weights are randomised, is often unable\nto make useable directional decisions. A simple implementation of a semi-random walk module\ntherefore allows MiRo to explore the environment sensibly, as opposed to erratically when the\nrandomised network weights are used. The details of the semi random walk implementation is\ngiven below.\nSemi-Random Walk Module – In cases where the signal provided by the action cells, as com-\nputed by Equation 13 is not strong enough (i.e. less than 1), then MiRo takes a random walk\nrather than following the direction selected by the action cells. To compute the heading, a small\nbut random value, θnoise, is added to MiRo’s current heading,\nθrandom walk = θcurrent + θnoise\n(14)\nwhere θnoise is a random variable taken from the uniform distribution θnoise ∼unif(−50o, 50o).\nThis ensures that MiRo generally keeps moving in its current direction, but is capable of chang-\ning slightly to the left or right, though by no more than 50o.\nTo convert this into action cell values, each action cell is computed as a function of its\nangular distance from θrandom walk, in a similar to manner to how the place cell activities were\ncomputed as the Cartesian distance of MiRo from the place cell centres,\nyrandom walk\ni\n= ymax\ni\nexp\n\"\n−(θrandom walk −θi)2\n2θ2\nd\n#\n(15)\nwhere ymax\ni\ndetermines the maximum value for yi, in this case 1, and θd determines the distri-\nbution width, and θi is the angle corresponding to action cell i.\n12\nTo state this more formally, let the magnitude of the place cell network proposal be (see\nEquation 13),\nmPC proposal =\nv\nu\nu\nt\n X\ni\n˜yi sin θi\n!2\n+\n X\ni\n˜yi cos θi\n!2\n(16)\nthen the ﬁnal action cell values are only changed to yi = yrandom walk\ni\nif mPC proposal < 1. Else\nthey stay as they are from Equation 10.\nComputing Action Cells During Reverse Replays – The computation for yi in Equation (10)\nis suitable for the exploration stage, but requires a minor modiﬁcation in order for the action\ncells to properly replay during reverse replay events. Thus far, yi is computed either by taking\nthe network’s output as determined by the place cell inputs or, if this output is weak, by using a\nsemi-random walk. In order for the yi term to compute properly in the reverse replay case then,\nwe perform the following,\nyreplay\ni\n=\n1\n1 + exp\nh\n−c1\nP100\nj=1\n\u0000wPC-AC\nij\n+ 0.1 sgn(er\nij)\n\u0001\nxj −c2\ni\n(17)\nwhich is the same computation as Equation (11), with the only difference being that the place\ncell to action cell weights have added to them the value 0.1 multiplied by the sign of the eligi-\nbility trace for that synapse. The term er\nij represents the value of eij, i.e. a trace of the potential\nsynaptic changes, at the moment of reward retrieval. This effectively stores the history of synap-\ntic activity and adds a transient weight increase to synapses that were recently active. How this\neligibility trace is computed is described in Section 5.\nModifying the action cells during replays is necessary so that a reverse replay of the place\ncells can appropriately reinstate the activity in the action cells (14). Without this change the\nreverse replays would offer no additional beneﬁts. This modiﬁcation acts like a synaptic tag that\nactivates at reward retrieval only and provides temporary synaptic modiﬁcations, according to\nthe sign of the eligibility trace, during the reverse replay stage. Despite making this assumption,\nthis temporary change in synaptic strengths is similar in nature to that of acetylcholine levels\n13\nmodifying synaptic conductances during replay events in the hippocampus (20). In other words,\nsynaptic weights (and their modiﬁcations) are suppressed during exploration, but are manifest\nduring the replay stage.\nWe also tested the rule using a weaker assumption, adding only a value of 0.1 for any\nsynapse in which eij > 0, whilst adding nothing for synapses where eij < 0. However, this was\nshown to perform worse than even the non-replay case. On closer inspection, the reason for this\nproved to be the loss of causality during a replay event. Since replays activate multiple cells\nsimultaneously, those synapses that may have had eij < 0 will be inﬂuenced by neighbouring\nplace and action cell activities without having the negative eligibility to counter those effects.\nThis inﬂuence caused them to increase their weights, as opposed to decreasing, which would be\nthe proper direction given a negative value for eij.\n2.2.3\nPlace Cell to Action Cell Synaptic Plasticity\nThe learning rule has been derived using a policy gradient reinforcement learning method (45).\nIts form is that of a three factor learning rule with an eligibility trace (11). The full derivation\nfor the learning rule is presented in the Appendix.\nWhen MiRo is exploring, a learning rule of the following is implemented\ndwPC-AC\nij\ndt\n= η\nσ2Reij\n(18)\nwhere R is a reward value, whilst the term eij represents the eligibility trace and is a time\ndecaying function of the potential weight changes, determined by,\ndeij\ndt = −eij\nτe\n+ (yi −˜yi) (1 −˜yi) ˜yixj.\n(19)\nDuring reverse replays however, the activity of the action cells are given by yreplay\ni\n. In\norder to retain mathematically accuracy in the derivation (see Appendix), we derived a similar\nlearning rule from a supervised learning approach having the following form\n14\ndwPC-AC\nij\ndt\n= η′eij,\n(20)\nwhere the eligibility trace is determined by,\ndeij\ndt = −eij\nτe\n+\n\u0010\nyreplay\ni\n−˜yi\n\u0011\n(1 −˜yi) ˜yixj,\n(21)\nWe have set η′ = η/σ2 and let R = 1 at the reward location in our simulations, which renders\nthe RL rule and the supervised learning rule equivalent.\n2.2.4\nPopulation weight vector for a single place cell\nThe population weight vector for a single place cell is computed as,\n(wx\nj , wy\nj ) =\n 72\nX\ni=1\nwPC-AC\nij\ncos θi ,\n72\nX\ni=1\nwPC-AC\nij\nsin θi\n!\n(22)\nwhere (wx\nj , wy\nj ) represents the x and y components for the weight population vector of the jth\nplace cell, wPC-AC\nij\nis the value of the weight from place cell j onto action cell i, and θi is the\nheading direction that action cell i codes for. The magnitude of the population weight vector\ncan then be computed as,\nMwj =\nq\u0000wx\nj\n\u00012 +\n\u0000wy\nj\n\u00012\n(23)\nThe population weight vector depicts the preferred direction of MiRo when placed at the center\nof the location of the place cell.\n2.2.5\nImplementation\nA description of the full implementation process is provided here, with an overview of the\nalgorithmic implementation presented in the Supplementary Material.\nInitialisation – At the start of a new experiment, the weights that connect the place cells to the\naction cells are randomised and then normalised,\nwPC-AC\nij\n←\nwPC-AC\nij\nP\ni wPC-AC\nij\n.\n(24)\n15\nAll the variables for the place cells are set to their steady state conditions for when no place\nspeciﬁc inputs are present, and the action cells are all set to zero. MiRo is then placed into a\nrandom location in the arena.\nTaking Actions – There are three main actions MiRo can make, depending on whether the\nreward it receives is positive (R = 1) and is therefore at the goal, negative (R = −1) such that\nMiRo has reached a wall, or zero (R = 0) for all other cases. If the reward is 0, the action cell\nvalues, yi is computed according to Equation 10, or yrandom walk\ni\nis computed from Equation\n15 if mPCproposal < 1, letting then yi = yrandom walk\ni\n. From this, a heading is computed using\nEquation 12. MiRo moves at a constant forward velocity with this heading, with a new heading\nbeing computed every 0.5s. If MiRo reaches a wall, a wall avoidance procedure is used, turning\nMiRo round 180o. Finally, if MiRo reaches the goal, it pauses there for 2s, after which it heads\nto a new random starting location.\nDetermining Reward Values – As described above, there are three reward values that MiRo\ncan collect. If MiRo has reached a wall, a reward of R = −1 is presented to MiRo for a period\nof 0.5s, which tends to occur during MiRo’s wall avoidance procedure. If MiRo has found the\ngoal, a reward of R = +1 is presented to it for a period of 2s. And if neither of these conditions\nare true, then MiRo receives no reward, i.e. R = 0.\nInitiating Reverse Replays – Reverse replays are only inititiated when MiRo reaches the goal\nlocation, but not for when MiRo is avoiding a wall. For the case in which reverse replays are\ninitiated, λ is set to 1 to allow hippocampal synaptic conductance, and the place speciﬁc input\nfor MiRo’s position whilst at the goal, Iplace\nj\n, is injected 1s after MiRo ﬁrst reaches the goal\nfor a total time of 100ms. With synaptic conductance enabled, and due to intrinsic plasticity,\nthis initiates reverse replay events initiating at the goal location and traveling back through the\nrecent trajectory in the place cell network. An example of a reverse replay can be found in\nthe Supplementary Material. Whilst learning is done as standard in the non-replay stage using\n16\nEquations 18 and 19 when MiRo ﬁrst reaches the goal, once the replays start learning is done\nusing the supervised learning rule of Equations 20 and 21.\nUpdating Network Variables – Regardless of whether MiRo is exploring, avoiding a wall, or\nis at the goal and is initiating replays, all the network variables, including the weight updates,\noccur for every time step of the simulation. It is only when MiRo has reached the goal, gone\nthrough the 2s of reward collection, and is making its way to a new random start location that\nall the variables are reset as in the Initialisation step above (though excluding the randomisation\nof the weights). This would then begin a new trial in the experiment.\n2.2.6\nModel parameter values\nAll parameter values used in the Hippocampal network are summarised in Table 1, and those\nfor the Striatal network in Table 2. Values for η and τe are speciﬁed appropriately in the results,\nsince they are modiﬁed across various experiments.\nParameter\nValue\nα\n1C−1\nϵ\n2A\nτI\n0.05s\nIp\nmax\n50A\nd\n0.1m\nλ\n0 or 1, see text\nτSTD\n1.5s\nτSTF\n1s\nU\n0.6\nψss\n0.1\nψmax\n4\nτψ\n10s\nβ\n1\nxψ\n10Hz\nTable 1: Summarising the model parameter values for the hippocampal network used in the\nexperiments. All these parameters are kept constant across all experiments.\n17\nParameter\nValue\nc1\n0.1\nc2\n20\nσ\n0.1\nθd\n10\nτe\nSee text\nη\nSee text\nTable 2: Summarising the model parameter values for the striatal network used in the exper-\niments. Except for the learning rate, η, and the eligibility trace time constant, τe, all other\nparameters are kept constant for all experiments.\n3\nResults\nThis results section is divided into two subsections. Presented ﬁrst are the results for when\nrunning the model without reverse replays, to demonstrate the functionality of the network and\nthe learning rule. Following this, the model is then run with reverse replays, with these results\nbeing compared to the non-replay case. All model parameters and the learning rule are kept\nequal between the two cases to facilitate the comparison, however when we compare the two\nmodels in terms of performance we optimise the key parameters deferentially for each model,\ncomparing best with best performance.\n3.1\nLearning Rule Without Reverse Replays\nWe ﬁrst demonstrate the functionality of the learning rule (Equations 18 and 19), without re-\nverse replays. Figure 2A shows the results for the time taken to reach the hidden goal as a\nfunction of trial number, averaged across 20 independent experiments. The time to reach the\ngoal approaches the asymptotic performance after around 5 trials. Note however that there ap-\npears to be larger variance towards the ﬁnal two trials. Further trials were later run in order to\ntest whether this increased variability in performance was signiﬁcant or not (see Section 3.2.4).\nFigure 2B displays the weight vector for the weights projecting from the place cells to the action\n18\nFigure 2: Results for the non-replay case in order to test that the derived learning rule performs\nwell. Parameters used were η = 0.01 and τe = 1s. A) Plot showing the average time to reach\ngoal (red line) and standard deviations (shaded area) over 20 trials. Averages and standard\ndeviations are computed from 20 independent experiments. B) Weight population vectors at the\nstart of trial 1 versus at the end of trial 20 in an example experiment. Magnitudes for the vectors\nare represented as a shade of colour; the darker the shade, the larger the magnitude. Red dots\nindicate the goal location.\ncells. We note that after 20 trials the arrows in general point towards the direction of the goal.\n3.2\nEffect of Reverse Replays on Performance\nWe then ran the model with reverse replays, implementing the learning rule of Equations 20 and\n21, using ﬁrst the same learning rate and eligibility trace time constant as in the non-replay case\n19\nabove. The performance average showed not to have any signiﬁcant difference (p > 0.05 across\n18 trials in a Wilcoxon Signed-Rank Test). Average time to reach goal over the last 10 trials is\n6.21s in the non-replay case and 6.92s in the replay case (data not shown, see Supplementary\nMaterial). This suggests in the ﬁrst instance that replays are at least as good when compared\nto the best case non-replay, which was also conﬁrmed when comparing individually optimised\nparameters (learning rate and eligibility time constant) for each network. Further results on\nperformance of varying the learning rate and eligibility trace time constant are presented next.\n3.2.1\nReducing the Eligibility Trace Time Constant\nGiven the standard, non-replay model requires the recent history to be stored in the eligibility\ntrace, it follows that having too small an eligibility trace time constant might negatively impact\nthe performance of the model. This is due to the time constant reﬂecting how far back the\ninformation about the Reward will be “transmitted”. Reverse replays however have the potential\nto compensate for this, since the recent history is also stored, and then replayed, in the place\ncell network. Figure 3 shows the effects on performance of signiﬁcantly reducing the eligibility\ntrace time constant (to τ = 0.04s). Both cases, with and without reverse replays, are compared.\nIf the learning rate is too small (η = 0.01) then for neither case is there any learning. But as the\nlearning rate is increased, having reverse replays shows to signiﬁcantly improve performance.\nSimilar results are found for a larger, but still small, eligibility trace time constant of τe = 0.2s\n(see Supplementary Material).\n3.2.2\nComparing Differences in Synaptic Weight Changes\nAn interesting comparison can be shown between the magnitudes of weight changes for the\nreplay case and non-replay cases. Figure 4 shows the population vectors of the weights after\nreward retrieval. Population vectors for the weights are computed according to Equations 22 and\n23. There are two observations to be made here. First is that the weight magnitudes are greater\n20\nFigure 3: Comparing the effects of a small eligibility trace time constant with and without\nreverse replays. τe = 0.04s across all ﬁgures. Thick lines are averages across 40 independent\nexperiments, with shaded areas representing one standard deviation. Moving averages, averaged\nacross 3 trials, are plotted here for smoothness.\nwhen reverse replays are implemented, which is expected since activity replay offers additional\ninformation to the synaptic changes. And second is that the direction of the population weight\nvectors themselves are slightly different, particularly in the location at the start of the trajectory.\nIn particular, the weight vectors point more towards the goal location in the replay case, whereas\nthe non-replay case has weight vectors pointing along the direction of the path taken by the\nrobot. Whilst only one case has been depicted here, this is representative of a number of cases\nfor various parameter values.\n21\nFigure 4: Population weight vectors after reward retrieval in the non-replay and replay cases.\nTop ﬁgure shows the path taken by MiRo, where S represents the starting location and G the\ngoal location. Plots show weight population vectors for the non-replay case (A) and standard\nreplay case (B) with τe = 1s; η = 0.1. The color scale represents the magnitudes for each\nweight vector.\n3.2.3\nPerformance Across Parameter Space\nWe investigated the robustness of the performance across various values of τe and η. Figure 5\ndisplays the average performance over the last 10 trials, comparing again with replays versus\nwithout replays. There are perhaps two noticeable observations to make here. Firstly, when\n22\nFigure 5: Comparing average performance across a range of values for τe and η. Bars show\nthe average time taken to reach the goal. These plots are found by ﬁrst averaging across 40\nindependent experiments (as shown in Figure 3 for instance), and then averaging over the last\n10 trials. Error bars indicate the 10 trial average of standard deviations.\nthe eligibility trace time constant is small, employing reverse replays shows considerable im-\nprovements in performance over the non-replay case across the various values of learning rates.\nLearning still exists in the non-replay case, however, it is noticeably diminished compared with\nthe replay case. Secondly, although this marked improvement in performance vanishes for\nlarger eligibility trace time constants, reverse replays do not at the very least hinder perfor-\nmance.\n23\n3.2.4\nComparison of Best Cases\nFigure 6 compares the results for the best cases with and without reverse replays. To achieve\nthese results we optimised τe and η independently for each case and run a total of 30 trials.\nThe reason for this was a suspected instability in the non-replay case when the amount of trials\nincreased as indicated in Figure 2. A Wilcoxin signed-rank test was run on all trials for the two\ncases, and for 8 of the last 12 trials (trials 19-30), there were signiﬁcant differences between the\ntwo (p < 0.05, full table of results can be found in the Supplementary material) despite there\nbeing no signiﬁcant differences in trials 0-18 (Section 3.2 above).\nThis instability in the non-replay case is not observed in the case with replays. We also\nFigure 6: Comparing the best cases with and without reverse replays. With reverse replays\nthe parameters are τe = 0.04s, η = 1. Without reverse replays the parameters are τe = 1s,\nη = 0.01. The means (solid lines) and standard deviations (shaded regions) are computed\nacross 40 independent experiments.\n24\nnote the difference in parameters for the best cases. With reverse replays the parameters are\nτe = 0.04s, η = 1, whereas without reverse replays they are τe = 1s, η = 0.01. We speculate\nthat the necessary choice in the eligibility time constant for the non-replay case (i.e. that it\nnecessarily needs to be large enough to store the trajectory history) is the cause of this instability.\n4\nDiscussion\nHippocampal reverse replay has long been implicated in reinforcement learning (8), but how\nthe dynamics of hippocampal replay produce behavioural changes, and why hippocampal re-\nplay could be important in learning, are ongoing questions. By embodying ﬁrst a hippocampal-\nstriatal inspired model (47) into a simulated MiRo robot, and then augmenting it with a model\nof hippocampal reverse replay (50), we have been able to examine the link between hippocam-\npal replay and behavioural changes in a spatial navigation task. We have shown that reverse\nreplays can enable quicker reinforced learning, as well as generating more robust behavioural\ntrajectories over repeated trials.\nIn the three-factor, synaptic eligibility trace hypothesis, the time constants for the traces\nhave been argued to be on the order of a few seconds, necessary for learning over behavioural\ntime scales (11). However, results here indicate that due to reverse replays, it is not necessary\nfor synaptic eligibility trace time constants to be on the order of seconds – a few milliseconds\nis sufﬁcient. The synaptic eligibility trace is still required here for storing the history; it just\ndoes not matter how much of the eligibility trace is stored, it is only important that enough is\nstored for effective reinstatement during a reverse replay. It has also been argued that neuronal,\nas opposed to synaptic, eligibility traces could be sufﬁcient for storing a memory trace, as in\nthe two-compartmental neuron model of (3). Intrinsic plasticity in this model is not unlike a\nneuronal eligibility trace, storing the memory trace within the place cells for reinstatement at\nthe end of a rewarding episode.\n25\nIt could be the case that reverse replays speed up learning by introducing an additional\nsource of information regarding past states, and the results shown here provide some support\nfor this. Experimental evidence does show for instance that disruption of hippocampal rip-\nples during awake states, when reverse replays occur, does disrupt but not completely diminish\nspatial learning in rats (23). Whilst the longer eligibility trace time constants in this model\n(τe = 1s, 5s) do not show diminished performance without reverse replays, the smaller time\nconstants (τe = 0.04s, 0.2s) do. Hence, these results support the view that reverse replays\nenhance, rather than provide entirely, the mechanism for learning. Beyond reverse replays\nhowever, forward replays have been known to occur on multiple occasions for up to 10 hours\npost-exploration (13), which could be more important for memory consolidation than awake\nreverse replays (6,12).\nIn the best case comparison (Figure 6), it is clear why a sufﬁciently large, yet not overly\nlarge, eligibility trace time constant for the non-replay case gives best performance – it must\nstore a suitable amount of the trajectory history for learning. If the eligibility trace time con-\nstant were too small, it would not store enough of the history, whereas too large and it stores\nsub-optimal or unnecessary trajectories that go too far back in time. Yet the non-replay model\nbecame more unstable as the number of trials increased, as shown in Figure 6. One expla-\nnation for this is that the eligibility trace time constant necessary for learning in non-replay\nhad to be large enough to store trajectory histories, but doing this increases the probability that\nsub-optimal paths may be learned. For the replay case however, since the trajectory was re-\nplayed during learning, it was not necessary to have such a large eligibility trace time constant.\nSub-optimal paths going further back in time are therefore no longer as strongly learned. Fur-\nthermore, replays are able to modify slightly the behavioural trajectories. By looking at the\neffects in the weight vectors of Figure 4, it is apparent that the weight vectors closer to the start\nlocation are shifted to point more towards the goal in the replay case. Reverse replays could\n26\nhelp in solving the exploration-exploitation problem in RL (45), since they could simulate more\noptimal trajectories that were not explicitly taken during behaviour.\nIn this model, there are two sets of competing behaviours during the exploratory stage – the\nmemory guided behaviour of the hippocampus and the semi-random walk behaviour – which\nare heuristically selected for based on the signal strength of the hippocampal output: If the\nhippocampal output does not express strongly for a particular action, the semi-random walk\nbehaviour is implemented instead. An interesting comparison with the basal ganglia, and its\ninput structure the striatum, could be made here, since these structures are thought to play a role\nin action selection (15, 29, 37, 39). A basic interpretation of this action selection mechanism\nis that the basal ganglia receives a variety of candidate motor behaviours, each of which are\nperhaps mutually incompatible, but from which the basal ganglia must select one (or more)\nof these behaviours for expressing (16, 17). Since the selection of an action in our model is\ndetermined from the striatal action cell outputs, it appears likely that this selection would occur\nwithin the basal ganglia.\nBut perhaps more interesting is that in the synaptic learning rule presented here, the differ-\nence between the action selected, yi, and the hippocampal output, ˜yi, is used to update synaptic\nstrengths. One interpretation for this could be that this difference behaves as an error signal,\nsignalling to the hippocampal-striatal synapses how “good”, or how “close”, their predictions\nwere in generating behaviours that led towards rewards. But how might this be implemented\nin the basal ganglia? Whilst the striatum acts as the input structure to the basal ganglia, neu-\nroanatomical evidence shows that the basal ganglia sub-regions loop back on one another (16),\nand that in particular the striatum sends inhibitory signals to the substantia nigra (SN), which\nin turn projects back both excitatory and inhibitory signals via dopamine (D1 and D2 receptors\nrespectively) to the striatum (10,19). There is therefore a potential mechanism for appropriate\nfeedback to the hippocampal-striatal synapses in order to provide this error signalling, and an\n27\nexploration of this error signal hypothesis could be a potentially interesting research endeavour.\n5\nConclusion\nThis work has explored the role that reverse replays may have in biological reinforcement learn-\ning. As a baseline, we have derived a policy gradient Reinforcement Learning rule, which we\nemployed to associate actions with place cell activities. This is a three factor learning rule with\nan eligibility trace, where the eligibility trace stores the pairwise co-activities of place and action\ncells. The learning rule was shown to perform successfully when applied to a simulated MiRo\nrobot for a Morris water-maze like task. We further augmented the network and learning rule\nwith reverse replays, which acted to reinstate recent place and action cell activities. The effect\nof these replays was that learning was signiﬁcantly improved for circumstances in which eligi-\nbility traces did not store sufﬁcient activity history. In addition, this had the effect of generating\nmore stable performance as the number of trials increased. Learning with reverse replays was\nimproved upon the case without replays, yet learning was still achievable without replays. Re-\nverse replay may therefore enhance reinforcement learning in the hippocampal-striatal network\nwhilst not necessarily providing its core mechanism.\nFunding\nThis work has been in part funded by the EU Horizon 2020 programme through the FET Flag-\nship Human Brain Project (HBP-SGA2: 785907; HBP-SGA3: 945539).\nAcknowledgments\nThe authors would like to thank Andy Philippides and Michael Mangan for their valuable input\nand useful discussions.\n28\nReferences\n1. R Ellen Ambrose, Brad E Pfeiffer, and David J Foster. Reverse replay of hippocampal place\ncells is uniquely modulated by changing reward. Neuron, 91(5):1124–1136, 2016.\n2. Guo-qiang Bi and Mu-ming Poo. Synaptic modiﬁcations in cultured hippocampal neu-\nrons: dependence on spike timing, synaptic strength, and postsynaptic cell type. Journal of\nneuroscience, 18(24):10464–10472, 1998.\n3. Johanni Brea, Alexisz Tam´as Ga´al, Robert Urbanczik, and Walter Senn. Prospective coding\nby spiking neurons. PLoS computational biology, 12(6), 2016.\n4. Consequential\nRobotics.\nDocumentation\nfor\nthe\nmiro-e\nrobot:\nhttp://labs.consequentialrobotics.com/miro-e/docs/, 2019.\n5. Kamran Diba and Gy¨orgy Buzs´aki. Forward and reverse hippocampal place-cell sequences\nduring ripples. Nature neuroscience, 10(10):1241–1242, 2007.\n6. Val´erie Ego-Stengel and Matthew A Wilson. Disruption of ripple-associated hippocampal\nactivity during rest impairs spatial learning in the rat. Hippocampus, 20(1):1–10, 2010.\n7. Umberto Esposito, Michele Giugliano, and Eleni Vasilaki. Adaptation of short-term plas-\nticity parameters via error-driven learning may explain the correlation between activity-\ndependent synaptic properties, connectivity motifs and target speciﬁcity. Frontiers in com-\nputational neuroscience, 8:175, 2015.\n8. David J Foster and Matthew A Wilson. Reverse replay of behavioural sequences in hip-\npocampal place cells during the awake state. Nature, 440(7084):680–683, 2006.\n9. Nicolas Fr´emaux and Wulfram Gerstner. Neuromodulated spike-timing-dependent plastic-\nity, and theory of three-factor learning rules. Frontiers in neural circuits, 9:85, 2016.\n29\n10. Charles R Gerfen, Thomas M Engber, Lawrence C Mahan, ZVI Susel, Thomas N Chase,\nFrederick J Monsma, and David R Sibley. D1 and d2 dopamine receptor-regulated gene ex-\npression of striatonigral and striatopallidal neurons. Science, 250(4986):1429–1432, 1990.\n11. Wulfram Gerstner, Marco Lehmann, Vasiliki Liakoni, Dane Corneil, and Johanni Brea. Eli-\ngibility traces and plasticity on behavioral time scales: experimental support of neohebbian\nthree-factor learning rules. Frontiers in neural circuits, 12:53, 2018.\n12. Gabrielle Girardeau, Karim Benchenane, Sidney I Wiener, Gy¨orgy Buzs´aki, and Micha¨el B\nZugaro. Selective suppression of hippocampal ripples impairs spatial memory. Nature\nneuroscience, 12(10):1222, 2009.\n13. Bapun Giri, Hiroyuki Miyawaki, Kenji Mizuseki, Sen Cheng, and Kamran Diba. Hip-\npocampal reactivation extends for several hours following novel experience. Journal of\nNeuroscience, 39(5):866–875, 2019.\n14. Stephen N Gomperts, Fabian Kloosterman, and Matthew A Wilson. Vta neurons coordinate\nwith the hippocampal reactivation of spatial experience. Elife, 4:e05360, 2015.\n15. Sten Grillner, Jeanette Hellgren, Ariane Menard, Kazuya Saitoh, and Martin A Wikstr¨om.\nMechanisms for selection of basic motor programs–roles for the striatum and pallidum.\nTrends in neurosciences, 28(7):364–370, 2005.\n16. Kevin Gurney, Tony J Prescott, and Peter Redgrave. A computational model of action se-\nlection in the basal ganglia. i. a new functional anatomy. Biological cybernetics, 84(6):401–\n410, 2001.\n17. Kevin Gurney, Tony J Prescott, and Peter Redgrave. A computational model of action se-\nlection in the basal ganglia. ii. analysis and simulation of behaviour. Biological cybernetics,\n84(6):411–423, 2001.\n30\n18. Tatsuya Haga and Tomoki Fukai. Recurrent network model for learning goal-directed se-\nquences through reverse replay. Elife, 7:e34171, 2018.\n19. LG Harsing Jr and MJ Zigmond. Inﬂuence of dopamine on gaba release in striatum: ev-\nidence for d1–d2 interactions and non-synaptic inﬂuences. Neuroscience, 77(2):419–429,\n1997.\n20. Michael E Hasselmo, Eric Schnell, and Edi Barkai. Dynamics of learning and recall at\nexcitatory recurrent synapses and cholinergic modulation in rat hippocampal region ca3.\nJournal of Neuroscience, 15(7):5249–5262, 1995.\n21. John J Hopﬁeld. Neural networks and physical systems with emergent collective com-\nputational abilities. Proceedings of the national academy of sciences, 79(8):2554–2558,\n1982.\n22. Mark D Humphries and Tony J Prescott. The ventral basal ganglia, a selection mechanism\nat the crossroads of space, strategy, and reward. Progress in neurobiology, 90(4):385–417,\n2010.\n23. Shantanu P Jadhav, Caleb Kemere, P Walter German, and Loren M Frank. Awake hip-\npocampal sharp-wave ripples support spatial memory.\nScience, 336(6087):1454–1458,\n2012.\n24. Adrien Jauffret, Nicolas Cuperlier, and Philippe Gaussier. From grid cells and visual place\ncells to multimodal place cell: a new robotic architecture. Frontiers in neurorobotics, 9:1,\n2015.\n25. Hideki Kametani and Hiroshi Kawamura. Alterations in acetylcholine release in the rat\nhippocampus during sleep-wakefulness detected by intracerebral dialysis. Life sciences,\n47(5):421–426, 1990.\n31\n26. Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A\nsurvey. The International Journal of Robotics Research, 32(11):1238–1274, 2013.\n27. Sampo Kuutti, Richard Bowden, Yaochu Jin, Phil Barber, and Saber Fallah. A survey of\ndeep learning applications to autonomous vehicle control. IEEE Transactions on Intelligent\nTransportation Systems, 2020.\n28. Fuhai Ling, Alejandro Jimenez-Rodriguez, and Tony J Prescott. Obstacle avoidance using\nstereo vision and deep reinforcement learning in an animal-like robot. In 2019 IEEE Inter-\nnational Conference on Robotics and Biomimetics (ROBIO), pages 71–76. IEEE, 2019.\n29. Jonathan W Mink. The basal ganglia: focused selection and inhibition of competing motor\nprograms. Progress in neurobiology, 50(4):381–425, 1996.\n30. Ben Mitchinson, M Pearson, T Pipe, and Tony J Prescott. Biomimetic robots as scientiﬁc\nmodels: a view from the whisker tip. Neuromorphic and brain-based robots, pages 23–57,\n2011.\n31. Ben Mitchinson and Tony J Prescott. Miro: a robot “mammal” with a biomimetic brain-\nbased control system. In Conference on Biomimetic and Biohybrid Systems, pages 179–191.\nSpringer, 2016.\n32. John O’Keefe. Place units in the hippocampus of the freely moving rat. Experimental\nneurology, 51(1):78–109, 1976.\n33. John O’Keefe and Jonathan Dostrovsky. The hippocampus as a spatial map: preliminary\nevidence from unit activity in the freely-moving rat. Brain research, 1971.\n34. Rich Pang and Adrienne L Fairhall. Fast and ﬂexible sequence induction in spiking neural\nnetworks via rapid excitability changes. eLife, 8:e44324, 2019.\n32\n35. CMA Pennartz, E Lee, J Verheul, P Lipa, Carol A Barnes, and BL McNaughton. The\nventral striatum in off-line processing: ensemble reactivation during sleep and modulation\nby hippocampal ripples. Journal of Neuroscience, 24(29):6446–6456, 2004.\n36. Tony J Prescott, Daniel Camilleri, Uriel Martinez-Hernandez, Andreas Damianou, and\nNeil D Lawrence. Memory and mental time travel in humans and social robots. Philo-\nsophical Transactions of the Royal Society B, 374(1771):20180025, 2019.\n37. Tony J Prescott, Fernando M Montes Gonz´alez, Kevin Gurney, Mark D Humphries, and\nPeter Redgrave. A robot model of the basal ganglia: behavior and intrinsic processing.\nNeural networks, 19(1):31–61, 2006.\n38. Tony J Prescott, Nathan Lepora, and Paul FM J Verschure. Living machines: A handbook\nof research in biomimetics and biohybrid systems. Oxford University Press, 2018.\n39. P Redgrave, N Vautrelle, PG Overton, and J Reynolds. Phasic dopamine signaling in action\nselection and reinforcement learning. In Handbook of Behavioral Neuroscience, volume 24,\npages 707–723. Elsevier, 2017.\n40. Paul Richmond, Lars Buesing, Michele Giugliano, and Eleni Vasilaki. Democratic pop-\nulation decisions result in robust policy-gradient learning: a parametric study with gpu\nsimulations. PLoS one, 6(5), 2011.\n41. Varun Saravanan, Danial Arabali, Arthur Jochems, Anja-Xiaoxing Cui, Luise Gootjes-\nDreesbach, Vassilis Cutsuridis, and Motoharu Yoshida. Transition between encoding and\nconsolidation/replay dynamics via cholinergic modulation of can current: a modeling study.\nHippocampus, 25(9):1052–1070, 2015.\n42. Wolfram Schultz. Predictive reward signal of dopamine neurons. Journal of neurophysiol-\nogy, 80(1):1–27, 1998.\n33\n43. Denis Sheynikhovich, Ricardo Chavarriaga, Thomas Str¨osslin, Angelo Arleo, and Wul-\nfram Gerstner. Is there a geometric module for spatial orientation? insights from a rodent\nnavigation model. Psychological review, 116(3):540, 2009.\n44. William E Skaggs and Bruce L McNaughton. Replay of neuronal ﬁring sequences in rat\nhippocampus during sleep following spatial experience. Science, 271(5257):1870–1873,\n1996.\n45. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT\npress, 2018.\n46. Misha Tsodyks, Klaus Pawelzik, and Henry Markram. Neural networks with dynamic\nsynapses. Neural computation, 10(4):821–835, 1998.\n47. Eleni Vasilaki, Nicolas Fr´emaux, Robert Urbanczik, Walter Senn, and Wulfram Gerstner.\nSpike-based reinforcement learning in continuous state and action space: when policy gra-\ndient methods fail. PLoS computational biology, 5(12), 2009.\n48. Eleni Vasilaki and Michele Giugliano. Emergence of connectivity motifs in networks of\nmodel neurons with short-and long-term plastic synapses. PloS one, 9(1), 2014.\n49. Barbara Webb. Can robots make good models of biological behaviour? Behavioral and\nbrain sciences, 24(6):1033–1050, 2001.\n50. Matthew T. Whelan, Eleni Vasilaki, and Tony J. Prescott. Fast reverse replays of recent\nspatiotemporal trajectories in a robotic hippocampal model. In Biomimetic and Biohybrid\nSystems, Cham, 2020. Springer International Publishing.\n51. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist rein-\nforcement learning. Machine learning, 8(3-4):229–256, 1992.\n34\n52. Matthew A Wilson and Bruce L McNaughton.\nReactivation of hippocampal ensemble\nmemories during sleep. Science, 265(5172):676–679, 1994.\n53. Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh,\nVikash Kumar, and Sergey Levine. The ingredients of real-world robotic reinforcement\nlearning. arXiv preprint arXiv:2004.12570, 2020.\n35\nAppendix - Mathematical Derivation of the Place-Action Cell\nSynaptic Learning Rule\nDerivation of the reinforcement learning rule – We derive a policy gradient rule (45) following\n(47), but here we use continuous valued neurons instead of spiking neurons. The expectation\nfor the rewards earned in an episode of duration T is given by,\n⟨R⟩T =\nZ\nX\nZ\nY\nR (x, y) Pw (x, y) dydx\n(25)\nwhere X is the space of the inputs of and Y the space of the output of the network, and Pw (x, y)\nthe probability that the network has input x and output y, parametrised by the weights.\nWe can decompose the probability, Pw (x, y) (see decomposition of the probability in (47))\nas,\nPw (x, y) =\nY\nj\ngj (x, y)\nY\ni\nhi (x, y) ,\n(26)\nwhere hi is the probability the i-th action cell generates output yj contained in y , when the\nnetwork receives input x. Similarly gj is the probability for the activity produced by the j-th\nplace cell given its input. We then wish to calculate the partial derivative over a weight wkl of\nthe expected reward,\n∂⟨RT⟩\n∂wkl\n=\nZ\nX\nZ\nY\nR (x, y) ∂Pw (x, y)\n∂wkl\ndydx.\n(27)\nTo do so, we take into account that Pw (x, y) =\nh\nPw(x,y)\nhk(x,y)\ni\nhk (x, y), where the term in square\nbrackets does not depend on wkl since we remove its contribution from Pw (x, y) by dividing\nwith hk (x, y). We can then write,\n∂Pw (x, y)\n∂wkl\n= Pw (x, y) ∂log hk (x, y)\n∂wkl\n.\n(28)\n36\nThis leads to,\n∂⟨RT⟩\n∂wkl\n=\nZ\nX\nZ\nY\nR (x, y) Pw (x, y) ∂log hk (x, y)\n∂wkl\ndydx.\n(29)\nTo proceed, we need to consider the distribution of the activities of the action cells hk. This\nwe choose to be a Gaussian function with mean ¯yk and variance σ2 (see also section “Striatal\nAction Cells”),\nhk (X, Y ) =\n1\nσ\n√\n2π exp\n \n−(yk −˜yk)2\n2σ2\n!\n.\n(30)\nThe mean of the distribution is calculated by ˜yk = fs\n\u0010\nc1\nP\nj wkjxj + c2\n\u0011\n, see also Equation\n11, where fs is a sigmoidal function. We note that a different choice of function would have\nresulted in a variant of this rule. Therefore,\n∂log hk (x, y)\n∂wkl\n= c1\nyk −˜yk\nσ2\n(1 −˜yk) ˜ykxl.\n(31)\nReplacing 31 in 29 we end up with,\n∂⟨RT⟩\n∂wkl\n=\nZ\nX\nZ\nY\nc1 R (x, y) Pw (x, y) yk −˜yk\nσ2\n(1 −˜yk) ˜ykxl dydx.\n(32)\nThen the batch update rule is given by,\ndwkl\ndt\n= η\nZ\nX\nZ\nY\nR (x, y) Pw (x, y) yk −˜yk\nσ2\n(1 −˜yk) ˜ykxl dydx.\n(33)\nThe batch rule indicates that we need to average the term R (x, y) yk−˜yk\nσ2\n(1 −˜yk) ˜ykxl across\nmany trials. When an on-line setting is considered, the average is naturally rising from sampling\nthroughout the episodes. Hence the on-line version of this rule is given by,\ndwkl\ndt\n= η R (x, y) yk −˜yk\nσ2\n(1 −˜yk) ˜ykxl,\n(34)\nwith the factor c1 absorbed in the learning rate. We note however that this rule is appropriate\nfor scenarios where reward is immediate. To deal with cases of distant rewards, such as ours\n37\nwhere reward comes at the end of a sequence of actions, we need to resort to eligibility traces.\nOur rule is similar to REINFORCE with multiparameter distribution (51); we differ by having a\ncontinuous time formulation and a different parametrisation of the neuronal probability density\nfunction. Further, in our case we do not learn the variance of the probability density function.\nWe introduce an eligibility trace by updating the weights connecting the place cells to the\naction cells, W PC-AC by\ndwPC-AC\nij\ndt\n= η\nσ2R (x, y) eij\n(35)\nThe term eij represents the eligibility trace, see also (45), and is a time decaying function of the\npotential weight changes, determined by,\ndeij\ndt = −eij\nτe\n+ (yi −˜yi) (1 −˜yi) ˜yixj.\n(36)\nDerivation of the supervised learning rule – During replays, we assume that synapses between\nplace and action cells change to minimise the function\nE = 1\n2\nX\ni\n\u0010\nyreplay\ni\n−˜yi\n\u00112\n,\n(37)\nin other words, we assume that during the replay Equation 17 provides a ﬁxed target value\nfor the mean of the Gaussian distribution of the action cells at time t. In what follows we\nconsider the target constant for the shape of the derivation and consistency with the form of the\nreinforcement learning rule, but in fact this target changes as time and consequently the weights\nfrom place to action cells change, making the rule unstable, but stabilising under a short, ﬁxed\nlength of reply time. Taking the gradient over the error function with respect to the weight wkl,\nwhen considering the “target” activity for the action cells ﬁxed, leads to the backpropagation\nupdate rule for a single layer network\ndwkl\ndt\n= η′ \u0010\nyreplay\nk\n−˜yk\n\u0011\n˜yk (1 −˜yk) xl,\n(38)\n38\nwhere η′ is the learning rule, in our simulations η′ = η/σ2 similar to the reinforcement learning\nrule. Also for consistency with the reinforcement learning rule formulation, we introduce an\neligibility trace by updating the weights connecting the place cells to the action cells, W PC-AC\nby\ndwPC-AC\nij\ndt\n= η′eij,\n(39)\nwhere the eligibility trace is determined by,\ndeij\ndt = −eij\nτe\n+\n\u0010\nyreplay\ni\n−˜yi\n\u0011\n(1 −˜yi) ˜yixj,\n(40)\nwhere again the time constant τe is the same as in the reinforcement learning rule.\nIn the case of replays then, when the robot has reached its target, it ﬁrst learns using the\nstandard learning rule as in Equations 35 and 36. After 1s, a replay event is initiated, and learn-\ning is then done using the supervised learning rule here, using Equations 39 and 40. By setting\nthe reward value to R = 1, we can ensure that both the RL learning rule and the supervised\nlearning rule are equal.\n39\n",
  "categories": [
    "q-bio.NC",
    "cs.RO"
  ],
  "published": "2021-02-23",
  "updated": "2021-02-23"
}