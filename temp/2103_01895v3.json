{
  "id": "http://arxiv.org/abs/2103.01895v3",
  "title": "Adversarial Examples can be Effective Data Augmentation for Unsupervised Machine Learning",
  "authors": [
    "Chia-Yi Hsu",
    "Pin-Yu Chen",
    "Songtao Lu",
    "Sijia Liu",
    "Chia-Mu Yu"
  ],
  "abstract": "Adversarial examples causing evasive predictions are widely used to evaluate\nand improve the robustness of machine learning models. However, current studies\nfocus on supervised learning tasks, relying on the ground-truth data label, a\ntargeted objective, or supervision from a trained classifier. In this paper, we\npropose a framework of generating adversarial examples for unsupervised models\nand demonstrate novel applications to data augmentation. Our framework exploits\na mutual information neural estimator as an information-theoretic similarity\nmeasure to generate adversarial examples without supervision. We propose a new\nMinMax algorithm with provable convergence guarantees for efficient generation\nof unsupervised adversarial examples. Our framework can also be extended to\nsupervised adversarial examples. When using unsupervised adversarial examples\nas a simple plug-in data augmentation tool for model retraining, significant\nimprovements are consistently observed across different unsupervised tasks and\ndatasets, including data reconstruction, representation learning, and\ncontrastive learning. Our results show novel methods and considerable\nadvantages in studying and improving unsupervised machine learning via\nadversarial examples.",
  "text": "Adversarial Examples can be Effective Data Augmentation\nfor Unsupervised Machine Learning\nChia-Yi Hsu1, Pin-Yu Chen2, Songtao Lu2, Sijia Liu3, Chia-Mu Yu1\n1National Yang Ming Chiao Tung University\n2 IBM Research\n3 Michigan State University\n{chiayihsu8315, chiamuyu}@gmail.com, {pin-yu.chen, songtao}@ibm.com, liusiji5@msu.edu\nAbstract\nAdversarial examples causing evasive predictions are widely\nused to evaluate and improve the robustness of machine learn-\ning models. However, current studies focus on supervised\nlearning tasks, relying on the ground-truth data label, a tar-\ngeted objective, or supervision from a trained classiﬁer. In\nthis paper, we propose a framework of generating adversarial\nexamples for unsupervised models and demonstrate novel ap-\nplications to data augmentation. Our framework exploits a mu-\ntual information neural estimator as an information-theoretic\nsimilarity measure to generate adversarial examples without\nsupervision. We propose a new MinMax algorithm with prov-\nable convergence guarantees for efﬁcient generation of unsu-\npervised adversarial examples. Our framework can also be\nextended to supervised adversarial examples. When using\nunsupervised adversarial examples as a simple plug-in data\naugmentation tool for model retraining, signiﬁcant improve-\nments are consistently observed across different unsupervised\ntasks and datasets, including data reconstruction, representa-\ntion learning, and contrastive learning. Our results show novel\nmethods and considerable advantages in studying and improv-\ning unsupervised machine learning via adversarial examples.\n1\nIntroduction\nAdversarial examples are known as prediction-evasive attacks\non state-of-the-art machine learning models (e.g., deep neural\nnetworks), which are often generated by manipulating native\ndata samples while maintaining high similarity measured\nby task-speciﬁc metrics such as Lp-norm bounded perturba-\ntions (Goodfellow, Shlens, and Szegedy 2015; Biggio and\nRoli 2018). Due to the implications and consequences on\nmission-critical and security-centric machine learning tasks,\nadversarial examples are widely used for robustness eval-\nuation of a trained model and for robustness enhancement\nduring training (i.e., adversarial training).\nDespite of a plethora of adversarial attacking algorithms,\nthe design principle of existing methods is primarily for su-\npervised learning models — requiring either the true label\nor a targeted objective (e.g., a speciﬁc class label or a ref-\nerence sample). Some recent works have extended to the\nsemi-supervised setting, by leveraging supervision from a\nclassiﬁer (trained on labeled data) and using the predicted\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n(I) Mathematical notation\nM sup/M unsup: trained supervised/unsupervised machine learning models\nx/xadv: original/adversarial data sample\nℓsup\nx /ℓunsup\nx\n: supervised/unsupervised loss function in reference to x\n(II) Supervised tasks\n(e.g. classiﬁcation)\n(III) Unsupervised tasks (our proposal)\n(e.g. data reconstruction, contrastive learning)\nxadv is similar to x but\nM sup(xadv) ̸= M sup(x)\nxadv is dissimilar to x but\nℓunsup\nx\n(xadv|M unsup) ≤ℓunsup\nx\n(x|M unsup)\nTable 1: Illustration of adversarial examples for super-\nvised/unsupervised machine learning tasks. Both settings use\na native data sample x as reference. For supervised setting,\nadversarial examples refer to similar samples of x causing\ninconsistent predictions. For unsupervised setting, adversar-\nial examples refer to dissimilar samples yielding smaller loss\nthan x, relating to generalization errors on low-loss samples.\nlabels on unlabeled data for generating (semi-supervised)\nadversarial examples (Miyato et al. 2018; Zhang et al. 2019;\nStanforth et al. 2019; Carmon et al. 2019). On the other hand,\nrecent advances in unsupervised and few-shot machine learn-\ning techniques show that task-invariant representations can\nbe learned and contribute to downstream tasks with limited\nor even without supervision (Ranzato et al. 2007; Zhu and\nGoldberg 2009; Zhai et al. 2019), which motivates this study\nregarding their robustness. Our goal is to provide efﬁcient\nrobustness evaluation and data augmentation techniques for\nunsupervised (and self-supervised) machine learning mod-\nels through unsupervised adversarial examples (UAEs). Ta-\nble 1 summarizes the fundamental difference between con-\nventional supervised adversarial examples and our UAEs.\nNotably, our UAE generation is supervision-free because it\nsolely uses an information-theoretic similarity measure and\nthe associated unsupervised learning objective function. It\ndoes not use any supervision such as label information or\nprediction from other supervised models.\nIn this paper, we aim to formalize the notion of UAE,\nestablish an efﬁcient framework for UAE generation, and\ndemonstrate the advantage of UAEs for improving a variety\nof unsupervised machine learning tasks. We summarize our\nmain contributions as follows.\n• We propose a new per-sample based mutual information\nneural estimator (MINE) between a pair of original and mod-\niﬁed data samples as an information-theoretic similarity mea-\nsure and a supervision-free approach for generating UAE. For\narXiv:2103.01895v3  [cs.LG]  8 Dec 2021\ninstance, see UAEs for data reconstruction in Figure 5 of sup-\nplementary material. While our primary interest is generating\nadversarial examples for unsupervised learning models, we\nalso demonstrate that our per-sample MINE can be used to\ngenerate adversarial examples for supervised learning models\nwith improved visual quality.\n• We formulate the generation of adversarial examples with\nMINE as a constrained optimization problem, which applies\nto both supervised and unsupervised machine learning tasks.\nWe then develop an efﬁcient MinMax optimization algorithm\n(Algorithm 1) and prove its convergence. We also demon-\nstrate the advantage of our MinMax algorithm over the con-\nventional penalty-based method.\n• We show a novel application of UAEs as a simple plug-in\ndata augmentation tool for several unsupervised machine\nlearning tasks, including data reconstruction, representa-\ntion learning, and contrastive learning on image and tabular\ndatasets. Our extensive experimental results show outstanding\nperformance gains (up to 73.5% performance improvement)\nby retraining the model with UAEs.\n2\nRelated Work and Background\n2.1\nAdversarial Attack and Defense\nFor supervised adversarial examples, the attack success cri-\nterion can be either untargeted (i.e. model prediction differs\nfrom the true label of the corresponding native data sample)\nor targeted (i.e. model prediction targeting a particular label\nor a reference sample). In addition, a similarity metric such as\nLp-norm bounded perturbation is often used when generating\nadversarial examples. The projected gradient descent (PGD)\nattack (Madry et al. 2018) is a widely used approach to ﬁnd\nLp-norm bounded supervised adversarial examples. Depend-\ning on the attack threat model, the attacks can be divided into\nwhite-box (Szegedy et al. 2013; Carlini and Wagner 2017b),\nblack-box (Chen et al. 2017; Brendel, Rauber, and Bethge\n2018; Liu et al. 2020), and transfer-based (Nitin Bhagoji et al.\n2018; Papernot et al. 2017) approaches.\nAlthough a plethora of defenses were proposed, many of\nthem failed to withstand advanced attacks (Carlini and Wag-\nner 2017a; Athalye, Carlini, and Wagner 2018). Adversarial\ntraining (Madry et al. 2018) and its variants aiming to gen-\nerate worst-case adversarial examples during training are so\nfar the most effective defenses. However, adversarial training\non supervised adversarial examples can suffer from unde-\nsirable tradeoff between robustness and accuracy (Su et al.\n2018; Tsipras et al. 2019). Following the formulation of un-\ntargeted supervised attacks, recent studies such as (Cemgil\net al. 2020) generate adversarial examples for unsupervised\ntasks by ﬁnding an adversarial example within an Lp-norm\nperturbation constraint that maximizes the training loss. In\ncontrast, our approach aims to ﬁnd adversarial examples that\nhave low training loss but are dissimilar to the native data\n(see Table 1), which plays a similar role to the category of\n“on-manifold” adversarial examples governing generalization\nerrors (Stutz, Hein, and Schiele 2019). In supervised set-\nting, (Stutz, Hein, and Schiele 2019) showed that adversarial\ntraining with Lp-norm constrained perturbations may ﬁnd\noff-manifold adversarial examples and hurt generalization.\n2.2\nMutual Information Neural Estimator\nMutual information (MI) measures the mutual dependence be-\ntween two random variables X and Z, deﬁned as I(X, Z) =\nH(X) −H(X|Z), where H(X) denotes the (Shannon) en-\ntropy of X and H(X|Z) denotes the conditional entropy of\nX given Z. Computing MI can be difﬁcult without know-\ning the marginal and joint probability distributions (PX,\nPZ, and PXZ). For efﬁcient computation, the mutual in-\nformation neural estimator (MINE) with consistency guar-\nantees is proposed in (Belghazi et al. 2018). Speciﬁcally,\nMINE aims to maximize the lower bound of the exact MI\nusing a model parameterized by a neural network θ, de-\nﬁned as IΘ(X, Z) ≤I(X, Z), where Θ is the space of\nfeasible parameters of a neural network, and IΘ(X, Z) is\nthe neural information quantity deﬁned as IΘ(X, Z) =\nsupθ∈Θ EPXZ[Tθ] −log(EPX⊗PZ[eTθ]). The function Tθ is\nparameterized by a neural network θ based on the Donsker-\nVaradhan representation theorem (Donsker and Varadhan\n1983). MINE estimates the expectation of the quantities\nabove by shufﬂing the samples from the joint distribution\nalong the batch axis or using empirical samples {xi, zi}n\ni=1\nfrom PXZ and PX ⊗PZ (the product of marginals).\nMINE has been successfully applied to improve represen-\ntation learning (Hjelm et al. 2019; Zhu, Zhang, and Evans\n2020) given a dataset. However, for the purpose of generating\nan adversarial example for a given data sample, the vanilla\nMINE is not applicable because it only applies to a batch of\ndata samples (so that empirical data distributions can be used\nfor computing MI estimates) but not to single data sample. To\nbridge this gap, we will propose two MINE-based sampling\nmethods for single data sample in Section 3.1.\n3\nMethodology\n3.1\nMINE of Single Data Sample\nGiven a data sample x and its perturbed sample x + δ, we\nconstruct an auxiliary distribution using their random sam-\nples or convolution outputs to compute MI via MINE as a\nsimilarity measure, which we denote as “per-sample MINE”.\nRandom Sampling Using compressive sampling (Candès\nand Wakin 2008), we perform independent Gaussian sam-\npling of a given sample x to obtain a batch of K compressed\nsamples {xk, (x + δ)k}K\nk=1 for computing IΘ(x, x + δ) via\nMINE. We refer the readers to the supplementary material\n(SuppMat 6.2, 6.3) for more details. We also note that ran-\ndom sampling is agnostic to the underlying machine learning\nmodel since it directly applies to the data sample.\nConvolution Layer Output When the underlying neural\nnetwork model uses a convolution layer to process the input\ndata (which is an almost granted setting for image data), we\npropose to use the output of the ﬁrst convolution layer of\na data input, denoted by conv(·), to obtain K feature maps\n{conv(x)k, conv(x + δ)k}K\nk=1 for computing IΘ(x, x + δ).\nWe provide the detailed algorithm for convolution-based per-\nsample MINE in SuppMat 6.2.\nEvaluation We use the CIFAR-10 dataset and the same\nneural network as in Section 4.2 to provide qualitative and\nquantitative evaluations on the two per-sample MINE meth-\nods for image classiﬁcation. Figure 1 shows their visual com-\nFigure 1: Visual comparison of MINE-based untargeted su-\npervised adversarial examples (with ϵ = 1) on CIFAR-10.\nPer-sample MINE Method\nFID\nKID\nRandom Sampling\n(10 runs, K = 96)\n339.47 ± 8.07\n14.86 ± 1.45\n1st Convolution\nLayer Output (K = 96)\n344.231\n10.78\nTable 2: Frechet and kernel inception distances (FID/KID)\nbetween the untargeted adversarial examples of 1000 test\nsamples and the training data in CIFAR-10.\nparisons, with the objective of ﬁnding the most similar per-\nturbed sample (measured by MINE with the maximal scaled\nL∞perturbation bound ϵ = 1) leading to misclassiﬁcation.\nBoth random sampling and convolution-based approaches\ncan generate high-similarity prediction-evasive adversarial\nexamples despite of large L∞perturbation.\nTable 2 compares the Frechet inception distance (FID)\n(Heusel et al. 2017) and the kernel inception distance (KID)\n(Bi´nkowski et al. 2018) between the generated adversarial\nexamples versus the training data (lower value is better).\nBoth per-sample MINE methods have comparable scores.\nThe convolution-based approach attains lower KID score and\nis observed to have better visual quality as shown in Figure 1.\nWe also tested the performance using the second convolution\nlayer output but found degraded performance. In this paper\nwe use convolution-based approach whenever applicable and\notherwise use random sampling.\n3.2\nMINE-based Attack Formulation\nWe formalize the objectives for supervised/unsupervised ad-\nversarial examples using per-sample MINE. As summarized\nin Table 1, the supervised setting aims to ﬁnd most similar\nexamples causing prediction evasion, leading to an MINE\nmaximization problem. The unsupervised setting aims to ﬁnd\nleast similar examples but having smaller training loss, lead-\ning to an MINE minimization problem. Both problems can\nbe solved efﬁciently using our uniﬁed MinMax algorithm.\nSupervised Adversarial Example Let (x, y) denote a\npair of a data sample x and its ground-truth label y. The\nobjective of supervised adversarial example is to ﬁnd a per-\nturbation δ to x such that the MI estimate IΘ(x, x + δ) is\nmaximized while the prediction of x + δ is different from y\n(or being a targeted class y′ ̸= y), which is formulated as\nMaximize\nδ\nIΘ(x, x + δ)\nsuch that x + δ ∈[0, 1]d , δ ∈[−ϵ, ϵ]d and fx(x + δ) ≤0.\nThe constraint x + δ ∈[0, 1]d ensures x + δ lies in the\n(normalized) data space of dimension d, and the constraint\nδ ∈[−ϵ, ϵ]d corresponds to the typical bounded L∞perturba-\ntion norm. We include this bounded-norm constraint to make\ndirect comparisons to other norm-bounded attacks. One can\nignore this constraint by setting ϵ = 1. Finally, the function\nf sup\nx (x + δ) is an attack success evaluation function, where\nf sup\nx (x + δ) ≤0 means x + δ is a prediction-evasive adver-\nsarial example. For untargeted attack one can use the attack\nfunction f sup\nx\ndesigned in (Carlini and Wagner 2017b), which\nis f sup\nx (x′) = logit(x′)y −maxj:j̸=y logit(x′)j + κ, where\nlogit(x′)j is the j-th class output of the logit (pre-softmax)\nlayer of a neural network, and κ ≥0 is a tunable gap be-\ntween the original prediction logit(x′)y and the top prediction\nmaxj:j̸=y logit(x′)j of all classes other than y. Similarly, the\nattack function for targeted attack with a class label y′ ̸= y is\nf sup\nx (x′) = maxj:j̸=y′ logit(x′)j −logit(x′)y′ + κ.\nUnsupervised Adversarial Example Many machine\nlearning tasks such as data reconstruction and unsupervised\nrepresentation learning do not use data labels, which pre-\nvents the use of aforementioned supervised attack functions.\nHere we use an autoencoder Φ(·) for data reconstruction to\nillustrate the unsupervised attack formulation. The design\nprinciple can naturally extend to other unsupervised tasks.\nThe autoencoder Φ takes a data sample x as an input and\noutputs a reconstructed data sample Φ(x). Different from\nthe rationale of supervised attack, for unsupervised attack\nwe propose to use MINE to ﬁnd the least similar perturbed\ndata sample x + δ with respect to x while ensuring the recon-\nstruction loss of Φ(x + δ) is no greater than Φ(x) (i.e., the\ncriterion of successful attack for data reconstruction). The\nunsupervised attack formulation is as follows:\nMinimize\nδ\nIΘ(x, x + δ)\nsuch that x + δ ∈[0, 1]d , δ ∈[−ϵ, ϵ]d and fx(x + δ) ≤0\nThe ﬁrst two constraints regulate the feasible data space and\nthe perturbation range. For the L2-norm reconstruction loss,\nthe unsupervised attack function is\nf unsup\nx\n(x + δ) = ∥x −Φ(x + δ)∥2 −∥x −Φ(x)∥2 + κ\nwhich means the attack is successful (i.e., f unsup\nx\n(x + δ) ≤0)\nif the reconstruction loss of x + δ relative to the original\nsample x is smaller than the native reconstruction loss mi-\nnus a nonnegative margin κ. That is, ∥x −Φ(x + δ)∥2 ≤\n∥x−Φ(x)∥2−κ. In other words, our unsupervised attack for-\nmulation aims to ﬁnd that most dissimilar perturbed sample\nx + δ to x measured by MINE while having smaller recon-\nstruction loss (in reference to x) than x. Such UAEs thus\nrelates to generalization errors on low-loss samples because\nthe model is biased toward these unseen samples.\n3.3\nMINE-based Attack Algorithm\nHere we propose a uniﬁed MinMax algorithm for solving\nthe aforementioned supervised and unsupervised attack for-\nmulations, and provide its convergence proof in Section 3.4.\nFor simplicity, we will use fx to denote the attack criterion\nfor f sup\nx\nor f unsup\nx\n. Without loss of generality, we will analyze\nthe supervised attack objective of maximizing IΘ with con-\nstraints. The analysis also holds for the unsupervised case\nsince minimizing IΘ is equivalent to maximizing I′\nΘ, where\nI′\nΘ = −IΘ. We will also discuss a penalty-based algorithm\nas a comparative method to our proposed approach.\nMinMax Algorithm (proposed) We reformulate the at-\ntack generation via MINE as the following MinMax opti-\nmization problem with simple convex set constraints:\nMin\nδ:x+δ∈[0,1]d, δ∈[−ϵ,ϵ]d Max\nc≥0\nF(δ, c) ≜c·f +\nx (x+δ)−IΘ(x, x+δ)\nThe outer minimization problem ﬁnds the best perturba-\ntion δ with data and perturbation feasibility constraints\nx + δ ∈[0, 1]d and δ ∈[−ϵ, ϵ]d, which are both con-\nvex sets with known analytical projection functions. The\ninner maximization associates a variable c ≥0 with the\noriginal attack criterion fx(x + δ) ≤0, where c is mul-\ntiplied to the ReLU activation function of fx, denoted as\nf +\nx (x + δ) = ReLU(fx(x + δ)) = max{fx(x + δ), 0}. The\nuse of f +\nx means when the attack criterion is not met (i.e.,\nfx(x + δ) > 0), the loss term c · fx(x + δ) will appear in\nthe objective function F. On the other hand, if the attack\ncriterion is met (i.e., fx(x + δ) ≤0), then c · f +\nx (x + δ) = 0\nand the objective function F only contains the similarity loss\nterm −IΘ(x, x + δ). Therefore, the design of f +\nx balances\nthe tradeoff between the two loss terms associated with at-\ntack success and MINE-based similarity. We propose to use\nalternative projected gradient descent between the inner and\nouter steps to solve the MinMax attack problem, which is\nsummarized in Algorithm 1. The parameters α and β denote\nthe step sizes of the minimization and maximization steps,\nrespectively. The gradient ∇f +\nx (x + δ) with respect to δ is\nset to be 0 when fx(x + δ) ≤0. Our MinMax algorithm\nreturns the successful adversarial example x + δ∗with the\nbest MINE value I∗\nΘ(x, x + δ∗) over T iterations.\nAlgorithm 1: MinMax Attack Algorithm\n1: Require: data sample x, attack criterion fx(·), step sizes\nα and β, perturbation bound ϵ, # of iterations T\n2: Initialize δ0 = 0, c0 = 0, δ∗= null, I∗\nΘ = −∞, t = 1\n3: for t in T iterations do\n4:\nδt+1 = δt −α · (c · ∇f +\nx (x + δt) −∇IΘ(x, x + δt))\n5:\nProject δt+1 to [−ϵ, ϵ] via clipping\n6:\nProject x + δt+1 to [0, 1] via clipping\n7:\nCompute IΘ(x, x + δt+1)\n8:\nPerform ct+1 = (1 −\nβ\nt1/4 ) · ct + β · f +\nx (x + δt+1)\n9:\nProject ct+1 to [0, ∞]\n10:\nif fx(x+δt+1) ≤0 and IΘ(x, x+δt+1) > I∗\nΘ then\n11:\nupdate δ∗= δt+1 and I∗\nΘ = IΘ(x, x + δt+1)\n12: Return δ∗, I∗\nΘ\nPenalty-based Algorithm (baseline) An alternative ap-\nproach to solving the MINE-based attack formulation is the\npenalty-based method with the objective:\nMinimize\nδ:x+δ∈[0,1]d, δ∈[−ϵ,ϵ]d\nc · f +\nx (x + δ) −IΘ(x, x + δ)\nwhere c is a ﬁxed regularization coefﬁcient instead of an\noptimization variable. Prior arts such as (Carlini and Wagner\n2017b) use a binary search strategy for tuning c and report\nthe best attack results among a set of c values. In contrast,\nour MinMax attack algorithm dynamically adjusts the c value\nin the inner maximization stage (step 8 in Algorithm 1). In\nSection 4.2, we will show that our MinMax algorithm is more\nefﬁcient in ﬁnding MINE-based adversarial examples than\nthe penalty-based algorithm. The details of the binary search\nprocess are given in SuppMat 6.5. Both methods have similar\ncomputation complexity involving T iterations of gradient\nand MINE computations.\n3.4\nConvergence Proof of MinMax Attack\nAs a theoretical justiﬁcation of our proposed MinMax attack\nalgorithm (Algorithm 1), we provide a convergence proof\nwith the following assumptions on the considered problem:\n• A.1: The feasible set ∆for δ is compact, and f +\nx (x + δ)\nhas (well-deﬁned) gradients and Lipschitz continuity (with\nrespect to δ) with constants Lf and lf. That is, |f +\nx (x + δ) −\nf +\nx (x + δ′)| ≤lf∥δ −δ′∥and ∥∇f +\nx (x + δ)) −∇f +\nx (x +\nδ′)∥≤Lf∥δ −δ′∥, ∀δ, δ′ ∈∆. Moreover, IΘ(x, x + δ)\nalso has gradient Lipschitz continuity with constant LI.\n• A.2: The per-sample MINE is η-stable over iterations for\nthe same input, |IΘt+1(x, x+δt+1)−IΘt(x, x+δt+1)| ≤η.\nA.1 holds in general for neural networks since the numeri-\ncal gradient of ReLU activation can be efﬁciently computed\nand the sensitivity (Lipschitz constant) against the input per-\nturbation can be bounded (Weng et al. 2018). The feasible\nperturbation set ∆is compact when the data space is bounded.\nA.2 holds by following the consistent estimation proof of the\nnative MINE in (Belghazi et al. 2018).\nTo state our main theoretical result, we ﬁrst deﬁne the\nproximal gradient of the objective function as L(δ, c) :=\n[δ −P∆[δ −∇δF(δ, c)], c −PC[c + ∇cF(δ, c)]],\nwhere\nPX\ndenotes the projection operator on convex set\nX, and ∥L(δ, c)∥is a commonly used measure for\nstationarity\nof\nthe\nobtained\nsolution.\nIn\nour\ncase,\n∆\n=\n{δ\n:\nx + δ\n∈\n[0, 1]d ∩δ\n∈\n[−ϵ, ϵ]d} and\nC = {c : 0 ≤c ≤¯c}, where ¯c can be an arbitrary\nlarge value. When ∥L(δ∗, c∗)∥= 0, then the point (δ∗, c∗) is\nrefereed as a game stationary point of the min-max problem\n(Razaviyayn et al. 2020). Next, we now present our main\ntheoretical result.\nTheorem 1. Suppose Assumptions A.1 and A.2 hold\nand the sequence {δt, ct, ∀t ≥1} is generated by the\nMinMax attack algorithm. For a given small constant\nε′ and positive constant β, let T(ε′) denote the ﬁrst\niteration index such that the following inequality is satisﬁed:\nT(ε′) := min{t|∥L(δt, ct)∥2 ≤ε′, t ≥1}. Then, when the\nstep-size and approximation error achieved by Algorithm 1\nsatisfy α ∼η ∼\np\n1/T(ε′), there exists some constant C\nsuch that ∥L(δT (ε′), cT (ε′))∥2 ≤C/\np\nT(ε′).\nProof. Please see the supplemental material (SuppMat 6.8).\nTheorem 1 states the rate of convergence of our proposed\nMinMax attack algorithm when provided with sufﬁcient sta-\nbility of MINE and proper selection of the step sizes. We\nalso remark that under the assumptions and conditions of\nstep-sizes, this convergence rate is standard in non-convex\nmin-max saddle point problems (Lu et al. 2020).\n3.5\nData Augmentation using UAE\nWith the proposed MinMax attack algorithm and per-sample\nMINE for similarity evaluation, we can generate MINE-based\nsupervised and unsupervised adversarial examples (UAEs).\nSection 4 will show novel applications of MINE-based UAEs\nas a simple plug-in data augmentation tool to boost the model\nperformance of several unsupervised machine learning tasks.\nWe observe signiﬁcant and consistent performance improve-\nment in data reconstruction (up to 73.5% improvement), rep-\nresentation learning (up to 1.39% increase in accuracy), and\ncontrastive learning (1.58% increase in accuracy). The ob-\nserved performance gain can be attributed to the fact that our\nUAEs correspond to “on-manifold” data samples having low\ntraining loss but are dissimilar to the training data, causing\ngeneralization errors. Therefore, data augmentation and re-\ntraining with UAEs can improve generalization (Stutz, Hein,\nand Schiele 2019).\n4\nPerformance Evaluation\nIn this section, we conduct extensive experiments on a va-\nriety of datasets and neural network models to demonstrate\nthe performance of our proposed MINE-based MinMax ad-\nversarial attack algorithm and the utility of its generated\nUAEs for data augmentation, where a high attack success\nrate using UAEs suggests rich space for data augmenta-\ntion to improve model performance. Codes are available at\nhttps://github.com/IBM/UAE.\n4.1\nExperiment Setup and Datasets\nDatasets We provide a brief summary of the datasets: •\nMNIST consists of grayscale images of hand-written digits.\nThe number of training/test samples are 60K/10K. • SVHN\nis a colar image dataset set of house numbers extracted from\nGoogle Street View images. The number of training/test sam-\nples are 73257/26302. • Fashion MNIST contains grayscale\nimages of 10 clothing items. The number of training/test sam-\nples are 60K/10K. • Isolet consists of preprocessed speech\ndata of people speaking the name of each letter of the English\nalphabet. The number of training/test samples are 6238/1559.\n• Coil-20 contains grayscale images of 20 multi-viewed ob-\njects. The number of training/test samples are 1152/288. •\nMice Protein consists of the expression levels (features) of\n77 protein modiﬁcations in the nuclear fraction of cortex.\nThe number of training/test samples are 864/216. • Human\nActivity Recognition consists of sensor data collected from\na smartphone for various human activities. The number of\ntraining/test samples are 4252/1492.\nSupervised Adversarial Example Setting Both data sam-\nples and their labels are used in the supervised setting. We\nselect 1000 test images classiﬁed correctly by the pretrained\nMNIST and CIFAR-10 deep neural network classiﬁers used\nin (Carlini and Wagner 2017b) and set the conﬁdence gap\nparameter κ = 0 for the designed attack function f sup\nx\ndeﬁned\nin Section 3.2. The attack success rate (ASR) is the fraction\nof the ﬁnal perturbed samples leading to misclassiﬁcation.\nUnsupervised Adversarial Example Setting Only the train-\ning data samples are used in the unsupervised setting. Their\ntrue labels are used in the post-hoc analysis for evaluating\nMNIST\nCIFAR-10\nASR\nMI\nASR\nMI\nPenalty-based\n100%\n28.28\n100%\n13.69\nMinMax\n100%\n51.29\n100%\n17.14\nTable 3: Comparison between MinMax and penality-based\nalgorithms on MNIST and CIFAR-10 datasets in terms of\nattack success rate (ASR) and mutual information (MI) value\naveraged over 1000 adversarial examples.\nthe quality of the associated unsupervised learning tasks. All\ntraining data are used for generating UAEs individually by\nsetting κ = 0. A perturbed data sample is considered as a suc-\ncessful attack if its loss (relative to the original sample) is no\ngreater than the original training loss (see Table 1). For data\naugmentation, if a training sample fails to ﬁnd a successful\nattack, we will replicate itself to maintain data balance. The\nASR is measured on the training data, whereas the reported\nmodel performance is evaluated on the test data. The training\nperformance is provided in SuppMat 6.10.\nMinMax Algorithm Parameters We use consistent parame-\nters by setting α = 0.01, β = 0.1, and T = 40 as the default\nvalues. The vanilla MINE model (Belghazi et al. 2018) is\nused in our per-sample MINE implementation. The parameter\nsensitivity analysis is reported in SuppMat 6.13.\nComputing Resource All experiments are conducted using\nan Intel Xeon E5-2620v4 CPU, 125 GB RAM and a NVIDIA\nTITAN Xp GPU with 12 GB RAM.\nModels and Codes We defer the summary of the considered\nmachine learning models to the corresponding sections. Our\ncodes are provided in SuppMat.\n4.2\nMinMax v.s. Penalty-based Algorithms\nWe use the same untargeted supervised attack formulation\nand a total of T = 9000 iterations to compare our proposed\nMinMax algorithm with the penalty-based algorithm using 9\nbinary search steps on MNIST and CIFAR-10. Table 3 shows\nthat while both methods can achieve 100% ASR, MinMax\nalgorithm attains much higher MI values than penalty-based\nalgorithm. The results show that the MinMax approach is\nmore efﬁcient in ﬁnding MINE-based adversarial examples,\nwhich can be explained by the dynamic update of the coefﬁ-\ncient c in Algorithm 1.\nFigure 2 compares the statistics of MI values over attack\niterations. One can ﬁnd that as iteration count increases, Min-\nMax algorithm can continue improving the MI value, whereas\npenalty-based algorithm saturates at a lower MI value due\nto the use of ﬁxed coefﬁcient c in the attack process. In the\nremaining experiments, we will report the results using Min-\nMax algorithm due to its efﬁciency.\n4.3\nQualitative Visual Comparison\nFigure 3 presents a visual comparison of MNIST supervised\nadversarial examples crafted by MinMax attack and the PGD\nattack with 100 iterations (Madry et al. 2018) given different\nϵ values governing the L∞perturbation bound. The main\ndifference is that MinMax attack uses MINE as an additional\nsimilarity regulation while PGD attack only uses L∞norm.\n1k 2k 3k 4k 5k 6k 7k 8k 9k\nnum iterations\n0\n10\n20\n30\n40\n50\n60\nMI\nBinary Search\nMinMax\n(a) MNIST\n1k 2k 3k 4k 5k 6k 7k 8k 9k\nnum iterations\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nMI\nBinary Search\nMinMax\n(b) CIFAR-10\nFigure 2: Mean and standard deviation of mutual information\n(MI) value versus attack iteration over 1000 samples.\n(a)\n(b) PGD attack\n(c) MinMax attack\nFigure 3: Comparison of untargeted supervised adversarial\nexamples on MNIST. The unsuccessful adversarial examples\nare marked with red crosses. Each column corresponds to\ndifferent ϵ values (L∞-norm perturbation bound) ranging\nfrom 0.1 to 1.0. Each row shows the adversarial examples\nof an original sample. MinMax attack using MINE yields\nadversarial examples with better visual quality than PGD\nattack, especially for large ϵ values.\nGiven the same ϵ value, MinMax attack yields adversarial\nexamples with better visual quality. The results validate the\nimportance of MINE as an effective similarity metric. In\ncontrast, PGD attack aims to make full use of the L∞pertur-\nbation bound and attempts to modify every data dimension,\ngiving rise to lower-quality adversarial examples. Similar\nresults are observed for adversarially robust models (Madry\net al. 2018; Zhang et al. 2019), as shown in SuppMat 6.16.\nMoreover, the results also suggest that for MINE-based\nattacks, the L∞norm constraint on the perturbation is not\ncritical for the resulting visual quality, which can be explained\nby the fact that MI is a fundamental information-theoretic\nsimilarity measure. When performing MINE-based attacks,\nwe suggest not using the L∞norm constraint (by setting\nϵ = 1) so that the algorithm can fully leverage the power of\nMI to ﬁnd a more diverse set of adversarial examples.\nNext, we study three different unsupervised learning tasks.\nWe only use the training samples and the associated training\nloss to generate UAEs. The post-hoc analysis reports the per-\nformance on the test data and the downstream classiﬁcation\naccuracy. We report their improved adversarial robustness\nafter data augmentation with MINE-UAEs in SuppMat 6.17.\n4.4\nUAE Improves Data Reconstruction\nData reconstruction using an autoencoder Φ(·) that learns\nto encode and decode the raw data through latent represen-\ntations is a standard unsupervised learning task. Here we\nuse the default implementation of the following four autoen-\ncoders to generate UAEs based on the training data samples\nof MNIST and SVHN for data augmentation, retrain the\nmodel from scratch on the augmented dataset, and report the\nresulting reconstruction error on the original test set. The re-\nsults of larger-scale datasets (CIFAR-10 and Tiny-ImageNet)\nare reported in SuppMat 6.11. All autoencoders use the L2\nreconstruction loss deﬁned as ∥x−Φ(x)∥2. We provide more\ndetails about the model retraining in SuppMat 6.10.\n• Dense Autoencoder (Cavallari, Ribeiro, and Ponti\n2018): The encoder and decoder have 1 dense layer sepa-\nrately and the latent dimension is 128/256 for MNIST/SVHN.\n• Sparse Autoencoder: It has a sparsity enforcer (L1 penalty\non the training loss) that directs a network with a single hid-\nden layer to learn the latent representations minimizing the\nerror in reproducing the input while limiting the number of\ncode words for reconstruction. We use the same architecture\nas Dense Autoencoder for MNIST and SVHN.\n• Convolutional Autoencoder1: The encoder uses convo-\nlution+relu+pooling layers. The decoder has reversed layer\norder with the pooling layer replaced by an upsampling layer.\n• Adversarial Autoencoder (Makhzani et al. 2016): It is\ncomposed of an encoder, a decoder and a discriminator. The\nrationale is to force the distribution of the encoded values to\nbe similar to the prior data distribution.\nWe also compare the performance of our proposed MINE-\nbased UAE (MINE-UAE) with two baselines: (i) L2-UAE\nthat replaces the objective of minimizing IΘ(x, x + δ) with\nmaximizing the L2 reconstruction loss ∥x −Φ(x + δ)∥2 in\nthe MinMax attack algorithm while keeping the same attack\nsuccess criterion; (ii) Gaussian augmentation (GA) that adds\nzero-mean Gaussian noise with a diagonal covariance matrix\nof the same constant σ2 to the training data.\nTable 4 shows the reconstruction loss and the ASR. The\nimprovement of reconstruction error is measured with respect\nto the reconstruction loss of the original model (i.e., with-\nout data augmentation). We ﬁnd that MINE-UAE can attain\nmuch higher ASR than L2-UAE and GA in most cases. More\nimportantly, data augmentation using MINE-UAE achieves\nconsistent and signiﬁcant reconstruction performance im-\nprovement across all models and datasets (up to 56.7% on\nMNIST and up to 73.5% on SVHN), validating the effective-\nness of MINE-UAE for data augmentation. On the other hand,\nin several cases L2-UAE and GA lead to notable performance\ndegradation. The results suggest that MINE-UAE can be an\neffective plug-in data augmentation tool for boosting the per-\nformance of unsupervised machine learning models. Table\n5 demonstrates UAEs can further improve data reconstruc-\ntion when the original model already involves conventional\naugmented training data such as ﬂip, rotation, and Gaussian\nnoise. The augmentation setup is given in SuppMat 6.12. We\nalso show the run time analysis of different augmentation\nmethod in SuppMat 6.7.\n4.5\nUAE Improves Representation Learning\nThe concrete autoencoder (Balın, Abid, and Zou 2019) is an\nunsupervised feature selection method which recognizes a\nsubset of the most informative features through an additional\n1We use https://github.com/shibuiwilliam/Keras_Autoencoder\nMNIST\nReconstruction Error (test set)\nASR (training set)\nAutoencoder\nOriginal\nMINE-UAE\nL2-UAE\nGA\n(σ = 0.01)\nGA\n(σ = 10−3)\nMINE-UAE\nL2-UAE\nGA\n(σ = 0.01)\nGA\n(σ = 10−3)\nSparse\n0.00561\n0.00243\n(↑56.7%)\n0.00348\n(↑38.0%)\n0.00280±2.60e-05\n(↑50.1%)\n0.00280±3.71e-05\n(↑50.1%)\n100%\n99.18%\n54.10%\n63.95%\nDense\n0.00258\n0.00228\n(↑11.6%)\n0.00286\n(↓6.0%)\n0.00244±0.00014\n(↑5.4%)\n0.00238±0.00012\n(↑7.8%)\n92.99%\n99.94%\n48.53%\n58.47%\nConvolutional\n0.00294\n0.00256\n(↑12.9%)\n0.00364\n(↓23.8%)\n0.00301±0.00011\n(↓2.4%)\n0.00304±0.00015\n(↓3.4%)\n99.86%\n99.61%\n68.71%\n99.61%\nAdversarial\n0.04785\n0.04581\n(↑4.3%)\n0.06098\n(↓27.4%)\n0.05793±0.00501\n(↓21%)\n0.05544±0.00567\n(↓15.86%)\n98.46%\n43.54%\n99.79%\n99.83%\nSVHN\nSparse\n0.00887\n0.00235\n(↑73.5%)\n0.00315\n(↑64.5%)\n0.00301±0.00137\n(↑66.1%)\n0.00293±0.00078\n(↑67.4%)\n100%\n72.16%\n72.42%\n79.92%\nDense\n0.00659\n0.00421\n(↑36.1%)\n0.00550\n(↑16.5%)\n0.00858±0.00232\n(↓30.2%)\n0.00860±0.00190\n(↓30.5%)\n99.99%\n82.65%\n92.3%\n93.92%\nConvolutional\n0.00128\n0.00095\n(↑25.8%)\n0.00121\n(↑5.5%)\n0.00098 ± 3.77e-05\n(↑25.4%)\n0.00104±7.41e-05\n(↑18.8%)\n100%\n56%\n96.40%\n99.24%\nAdversarial\n0.00173\n0.00129\n(↑25.4%)\n0.00181\n(↓27.4%)\n0.00161±0.00061\n(↑6.9%)\n0.00130±0.00037\n(↑24.9%)\n94.82%\n58.98%\n97.31%\n99.85%\nTable 4: Comparison of data reconstruction by retraining the autoencoder on UAE-augmented data. The error is the average L2\nreconstruction loss of the test set. The improvement (in green/red) is relative to the original model. The attack success rate (ASR)\nis the fraction of augmented training data having smaller reconstruction loss than the original loss (see Table 1 for deﬁnition).\nSVNH - Convolutional AE\nAugmentation\nAug. (test set)\nAug.+MINE-UAE (test set)\nFlip + Rotation\n0.00285\n0.00107 (↑62.46%)\nGaussian noise (σ = 0.01)\n0.00107\n0.00095 (↑11.21%)\nFlip + Rotation + Gaussian noise\n0.00307\n0.00099 (↑67.75%)\nTable 5: Performance of data reconstruction when retraining\nwith MINE-UAE and additional augmented training data.\nReconstruction Error (test set)\nAccuracy (test set)\nASR\nDataset\nOriginal\nMINE-UAE\nOriginal\nMINE-UAE\nMINE-UAE\nMNIST\n0.01170\n0.01142 (↑2.4%)\n94.97%\n95.41%\n99.98%\nFashion MMIST\n0.01307\n0.01254 (↑4.1%)\n84.92%\n85.24%\n99.99%\nIsolet\n0.01200\n0.01159 (↑3.4%)\n81.98%\n82.93%\n100%\nCoil-20\n0.00693\n0.01374 (↓98.3%)\n98.96%\n96.88%\n9.21%\nMice Protein\n0.00651\n0.00611 (↑6.1%)\n89.81%\n91.2%\n40.24%\nActivity\n0.00337\n0.00300 (↑11.0%)\n83.38%\n84.45%\n96.52%\nTable 6: Performance of representation learning by the con-\ncrete autoencoder and the resulting classiﬁcation accuracy.\nThe degradation on Coil-20 is explained in Section 4.5.\nconcrete select layer with M nodes in the encoder for data\nreconstruction. We apply MINE-UAE for data augmentation\nand use the same post-hoc classiﬁcation evaluation procedure\nas in (Balın, Abid, and Zou 2019).\nThe six datasets and the resulting classiﬁcation accuracy\nare reported in Table 6. We select M = 50 features for every\ndataset except for Mice Protein (we set M = 10) owing\nto its small data dimension. MINE-UAE can attain up to\n11% improvement for data reconstruction and up to 1.39%\nincrease in accuracy among 5 out of 6 datasets, corroborating\nthe utility of MINE-UAE in representation learning and fea-\nture selection. The exception is Coil-20. A closer inspection\nshows that MINE-UAE has low ASR (<10%) for Coil-20\nand the training loss after data augmentation is signiﬁcantly\nhigher than the original training loss (see SuppMat 6.10).\nTherefore, we conclude that the degraded performance in\nCoil-20 after data augmentation is likely due to the limitation\nof feature selection protocol and the model learning capacity.\n4.6\nUAE Improves Contrastive Learning\nThe SimCLR algorithm (Chen et al. 2018) is a popular con-\ntrastive learning framework for visual representations. It uses\nCIFAR-10\nModel\nLoss (test set)\nAccuracy (test set)\nASR\nOriginal\n0.29010\n91.30%\n-\nMINE-UAE\n0.26755 (↑7.8%)\n+1.58%\n100%\nCLAE (Ho and Vasconcelos 2020)\n-\n+0.05%\n-\nTable 7: Comparison of contrastive loss and the resulting\naccuracy on CIFAR-10 using SimCLR (Chen et al. 2018)\n(ResNet-18 with batch size = 512). The attack success rate\n(ASR) is the fraction of augmented training data having\nsmaller contrastive loss than original loss. For CLAE (Ho and\nVasconcelos 2020), we use the reported accuracy improve-\nment (it shows negative gain in our implementation), though\nits base SimCLR model only has 83.27% test accuracy.\nself-supervised data modiﬁcations to efﬁciently improve sev-\neral downstream image classiﬁcation tasks. We use the de-\nfault implementation of SimCLR on CIFAR-10 and generate\nMINE-UAEs using the training data and the deﬁned training\nloss for SimCLR. Table 7 shows the loss, ASR and the re-\nsulting classiﬁcation accuracy by taining a linear head on the\nlearned representations. We ﬁnd that using MINE-UAE for\nadditional data augmentation and model retraining can yield\n7.8% improvement in contrastive loss and 1.58% increase\nin classiﬁcation accuracy. Comparing to (Ho and Vasconce-\nlos 2020) using adversarial examples to improve SimCLR\n(named CLAE), the accuracy increase of MINE-UAE is 30x\nhigher. Moreover, MINE-UAE data augmentation also signif-\nicantly improves adversarial robustness (see SuppMat 6.17).\n5\nConclusion\nIn this paper, we propose a novel framework for studying ad-\nversarial examples in unsupervised learning tasks, based on\nour developed per-sample mutual information neural estima-\ntor as an information-theoretic similarity measure. We also\npropose a new MinMax algorithm for efﬁcient generation\nof MINE-based supervised and unsupervised adversarial ex-\namples and establish its convergence guarantees. As a novel\napplication, we show that MINE-based UAEs can be used\nas a simple yet effective plug-in data augmentation tool and\nachieve signiﬁcant performance gains in data reconstruction,\nrepresentation learning, and contrastive learning.\nAcknowledgments\nThis work was primarily done during Chia-Yi’s visit at IBM\nResearch. Chia-Yi Hsu and Chia-Mu Yu were supported by\nMOST 110-2636-E-009-018, and we also thank National\nCenter for High-performance Computing (NCHC) of Na-\ntional Applied Research Laboratories (NARLabs) in Taiwan\nfor providing computational and storage resources.\nReferences\nAthalye, A.; Carlini, N.; and Wagner, D. 2018. Obfuscated\ngradients give a false sense of security: Circumventing de-\nfenses to adversarial examples. International Coference on\nInternational Conference on Machine Learning.\nBalın, M. F.; Abid, A.; and Zou, J. 2019. Concrete autoen-\ncoders: Differentiable feature selection and reconstruction.\nIn International Conference on Machine Learning, 444–453.\nBelghazi, M. I.; Baratin, A.; Rajeshwar, S.; Ozair, S.; Bengio,\nY.; Courville, A.; and Hjelm, D. 2018. Mutual information\nneural estimation. In International Conference on Machine\nLearning, 531–540.\nBiggio, B.; and Roli, F. 2018. Wild patterns: Ten years after\nthe rise of adversarial machine learning. Pattern Recognition,\n84: 317–331.\nBi´nkowski, M.; Sutherland, D. J.; Arbel, M.; and Gretton, A.\n2018. Demystifying MMD GANs. In International Confer-\nence on Learning Representations.\nBrendel, W.; Rauber, J.; and Bethge, M. 2018. Decision-\nBased Adversarial Attacks: Reliable Attacks Against Black-\nBox Machine Learning Models. International Conference on\nLearning Representations.\nCandès, E. J.; and Wakin, M. B. 2008. An introduction to\ncompressive sampling. IEEE Signal Processing Magazine,\n25(2): 21–30.\nCarlini, N.; and Wagner, D. 2017a. Adversarial examples\nare not easily detected: Bypassing ten detection methods. In\nACM Workshop on Artiﬁcial Intelligence and Security, 3–14.\nCarlini, N.; and Wagner, D. 2017b. Towards evaluating the ro-\nbustness of neural networks. In IEEE Symposium on Security\nand Privacy, 39–57.\nCarmon, Y.; Raghunathan, A.; Schmidt, L.; Liang, P.; and\nDuchi, J. C. 2019.\nUnlabeled data improves adversarial\nrobustness. Neural Information Processing Systems.\nCavallari, G.; Ribeiro, L.; and Ponti, M. 2018. Unsupervised\nrepresentation learning using convolutional and stacked auto-\nencoders: a domain and cross-domain feature space analysis.\nIn IEEE SIBGRAPI Conference on Graphics, Patterns and\nImages (SIBGRAPI), 440–446.\nCemgil, T.; Ghaisas, S.; Dvijotham, K. D.; and Kohli, P. 2020.\nAdversarially Robust Representations with Smooth Encoders.\nIn International Conference on Learning Representations.\nChen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J.\n2017. ZOO: Zeroth Order Optimization Based Black-box\nAttacks to Deep Neural Networks Without Training Substi-\ntute Models. In ACM Workshop on Artiﬁcial Intelligence and\nSecurity, 15–26.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2018. A\nsimple framework for contrastive learning of visual represen-\ntations. In International Conference on Machine Learning.\nDonsker, M. D.; and Varadhan, S. S. 1983.\nAsymptotic\nevaluation of certain Markov process expectations for large\ntime. IV. Communications on Pure and Applied Mathematics,\n36(2): 183–212.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Ex-\nplaining and harnessing adversarial examples. International\nConference on Learning Representations.\nHeusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and\nHochreiter, S. 2017. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. In Advances in\nNeural Information Processing Systems, 6626–6637.\nHjelm, R. D.; Fedorov, A.; Lavoie-Marchildon, S.; Grewal,\nK.; Bachman, P.; Trischler, A.; and Bengio, Y. 2019. Learning\ndeep representations by mutual information estimation and\nmaximization.\nIn International Conference on Learning\nRepresentations.\nHo, C.-H.; and Vasconcelos, N. 2020. Contrastive learning\nwith adversarial examples. In Advances in Neural Informa-\ntion Processing Systems.\nLiu, S.; Lu, S.; Chen, X.; Feng, Y.; Xu, K.; Al-Dujaili, A.;\nHong, M.; and O’Reilly, U.-M. 2020. Min-max optimization\nwithout gradients: Convergence and applications to black-box\nevasion and poisoning attacks. In International Conference\non Machine Learning.\nLu, S.; Tsaknakis, I.; Hong, M.; and Chen, Y. 2020. Hybrid\nBlock Successive Approximation for One-Sided Non-Convex\nMin-Max Problems: Algorithms and Applications. IEEE\nTransactions on Signal Processing, 68: 3676–3691.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu,\nA. 2018. Towards Deep Learning Models Resistant to Ad-\nversarial Attacks. International Conference on Learning\nRepresentations.\nMakhzani, A.; Shlens, J.; Jaitly, N.; Goodfellow, I.; and Frey,\nB. 2016. Adversarial autoencoders. ICLR Workshop.\nMiyato, T.; Maeda, S.-i.; Koyama, M.; and Ishii, S. 2018.\nVirtual adversarial training: a regularization method for su-\npervised and semi-supervised learning. IEEE transactions on\npattern analysis and machine intelligence, 41(8): 1979–1993.\nNitin Bhagoji, A.; He, W.; Li, B.; and Song, D. 2018. Practi-\ncal Black-box Attacks on Deep Neural Networks using Efﬁ-\ncient Query Mechanisms. In Proceedings of the European\nConference on Computer Vision (ECCV), 154–169.\nPapernot, N.; McDaniel, P.; Goodfellow, I.; Jha, S.; Celik,\nZ. B.; and Swami, A. 2017. Practical black-box attacks\nagainst machine learning. In ACM Asia Conference on Com-\nputer and Communications Security, 506–519.\nRanzato, M.; Huang, F. J.; Boureau, Y.-L.; and LeCun, Y.\n2007. Unsupervised learning of invariant feature hierarchies\nwith applications to object recognition. In IEEE Conference\non Computer Vision and Pattern Recognition, 1–8.\nRazaviyayn, M.; Huang, T.; Lu, S.; Nouiehed, M.; Sanjabi,\nM.; and Hong, M. 2020. Nonconvex Min-Max Optimization:\nApplications, Challenges, and Recent Theoretical Advances.\nIEEE Signal Processing Magazine, 37(5): 55–66.\nStanforth, R.; Fawzi, A.; Kohli, P.; et al. 2019. Are Labels\nRequired for Improving Adversarial Robustness? Neural\nInformation Processing Systems.\nStutz, D.; Hein, M.; and Schiele, B. 2019. Disentangling\nadversarial robustness and generalization. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 6976–6987.\nSu, D.; Zhang, H.; Chen, H.; Yi, J.; Chen, P.-Y.; and Gao, Y.\n2018. Is robustness the cost of accuracy?–a comprehensive\nstudy on the robustness of 18 deep image classiﬁcation mod-\nels. In Proceedings of the European Conference on Computer\nVision (ECCV), 631–648.\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan, D.;\nGoodfellow, I.; and Fergus, R. 2013. Intriguing properties of\nneural networks. arXiv preprint arXiv:1312.6199.\nTsipras, D.; Santurkar, S.; Engstrom, L.; Turner, A.; and\nMadry, A. 2019. Robustness may be at odds with accuracy.\nIn International Conference on Learning Representations.\nWeng, T.-W.; Zhang, H.; Chen, P.-Y.; Yi, J.; Su, D.; Gao, Y.;\nHsieh, C.-J.; and Daniel, L. 2018. Evaluating the Robustness\nof Neural Networks: An Extreme Value Theory Approach.\nInternational Conference on Learning Representations.\nZhai, X.; Oliver, A.; Kolesnikov, A.; and Beyer, L. 2019. S4l:\nSelf-supervised semi-supervised learning. In Proceedings\nof the IEEE international Conference on Computer Vision,\n1476–1485.\nZhang, H.; and Wang, J. 2019. Defense Against Adversarial\nAttacks Using Feature Scattering-based Adversarial Training.\nNeural Information Processing Systems.\nZhang, H.; Yu, Y.; Jiao, J.; Xing, E.; El Ghaoui, L.; and\nJordan, M. 2019. Theoretically principled trade-off between\nrobustness and accuracy. In International Conference on\nMachine Learning, 7472–7482.\nZhu, S.; Zhang, X.; and Evans, D. 2020. Learning Adversari-\nally Robust Representations via Worst-Case Mutual Informa-\ntion Maximization. International Coference on International\nConference on Machine Learning.\nZhu, X.; and Goldberg, A. B. 2009. Introduction to semi-\nsupervised learning. Synthesis Lectures on Artiﬁcial Intelli-\ngence and Machine Learning, 3(1): 1–130.\n6\nSupplementary Material\n6.1\nCodes\nOur codes are provided as a zip ﬁle for review.\n6.2\nMore Details on Per-sample MINE\nRandom Sampling\nWe reshape an input data sample as a vector x ∈Rd and independently generate K Gaussian random\nmatrices {Mk}K\nk=1, where Mk ∈Rd′×d. Each entry in Mk is an i.i.d zero-mean Gaussian random variable with standard\ndeviation 1/d′. The compressed samples {xk}K\nk=1 of x is deﬁned as xk = Mkx. Similarly, the same random sampling procedure\nis used on x + δ to obtain its compressed samples {(x + δ)k}K\nk=1. In our implementation, we set d′ = 128 and K = 500.\nConvolution Layer Output\nGiven a data sample x, we fetch its output of the 1st convolutional layer, denoted by conv(x).\nThe data dimension is d′ × K, where K is the number of ﬁlters (feature maps) and d′ is the (ﬂattend) dimension of the feature\nmap. Each ﬁlter is regarded as a compressed sample denoted by conv(x)k. Algorithm 2 summarizes the proposed approach,\nwhere the function Tθ is parameterized by a neural network θ based on the Donsker-Varadhan representation theorem (Donsker\nand Varadhan 1983), and TI is the number of iterations for training the MI neural estimator I(θ).\nAlgorithm 2: Per-sample MINE via Convolution\n1: Require: input sample x, perturbed sample x + δ, 1st convolution layer output conv(·), MI neural estimator I(θ)\n2: Initialize neural network parameters θ\n3: Get {conv(x)k}K\nk=1 and {conv(x + δ)k}K\nk=1 via 1st convolution layer\n4: for t in TI iterations do\n5:\nTake K samples from the joint distribution: {conv(x)k, conv(x + δ)k}K\nk=1\n6:\nShufﬂe K samples from conv(x + δ) marginal distribution: {conv(x + δ)(k)}K\nk=1\n7:\nEvaluate\nthe\nmutual\ninformation\nestimate\nI(θ)\n←\n1\nK\nPK\nk=1 Tθ(conv(x)k, conv(x\n+\nδ)k)\n−\nlog\n\u0010\n1\nK\nPK\nk=1 exp[Tθ(conv(x)k, conv(x + δ)(k))]\n\u0011\n8:\nθ ←θ + ∇θI(θ)\n9: Return I(θ)\n6.3\nAblation Study of K for Random Sampling of Per-sample MINE\nWe follow the same setting as in Table 2 on CIFAR-10 and report the average persample-MINE, FID and KID results over 1000\nsamples when varying the value K in random sampling. The results in Table 8 show that both KID and FID scores decrease\n(meaning the generated adversarial examples are closer to the training data’s representations) as K increases, and they saturate\nwhen K is greater than 500. Similarly, the MI values become stable when K is greater than 500.\nCIFAR-10\nK\n50\n100\n200\n300\n400\n500\n600\n700\n800\nMI\n1.35\n1.53\n1.86\n2.05\n2.13\n2.32\n2.33\n2.35\n2.38\nFID\n226.63\n213.01\n207.81\n205.94\n203.73\n200.12\n200.02\n198.75\n196.57\nKID\n14.7\n12.20\n10.8\n10.02\n9.59\n8.78\n8.81\n8.22\n8.29\nTable 8: Comparison of MI, FID and KID when varying the value K in random sampling.\n6.4\nAdditional Visual Comparisons\nVisual Comparison of Supervised Adversarial Examples with ϵ = 0.03\nSimilar to the setting in Figure 1, we compare the\nMINE-based supervised adversarial examples on CIFAR-10 with the L∞constraint ϵ = 0.03 (instead of ϵ = 1) in Figure 4.\nVisual Comparison of Unsupervised Adversarial Examples\nFigure 5 shows the generated MINE-UAEs with ϵ = 1 on\nSVHN using the convolutional autoencoder. We pick the 10 images such that their reconstruction loss is no greater than that of\nthe original image, while they have the top-10 perturbation level measured by the L2 norm on the perturbation ∥δ∗∥2.\nFigure 4: Visual comparison of MINE-based supervised adversarial examples (untargeted attack with ϵ = 0.03) on CIFAR-10.\nBoth random sampling and convolution output can be used to craft adversarial examples with high similarity. The label below\neach data sample indicates the predicted label, where the predicted label of the original image is the ground-truth label.\nFigure 5: Visual comparison of MINE-based unsupervised adversarial examples on SVHN using convolutional autoencoder.\n6.5\nBinary Search for Penalty-based Attack Algorithm\nAlgorithm 3 summarizes the binary search strategy on the regularization coefﬁcient c in the penalty-based methods. The search\nprocedure follows the implementation in (Carlini and Wagner 2017b), which updates c per search step using a pair of pre-deﬁned\nupper and lower bounds. The reported MI value of penalty-based method in Figure 2 is that of the current best search in c, where\neach search step takes 1000 iterations (i.e., B = 9 and T ′ = 1000).\nAlgorithm 3: Binary Search\n1: Require: data sample x, attack criterion fx(·), mutual information IΘ(x, x + δ) and step sizes α, perturbation bound ϵ,\nnumber of iterations T ′ in each search, total number of binary search steps B\n2: Initialization: lb = 10−3 (lower bound on c), ub = 109 (upper bound on c), c = 10−3, I∗\nΘ = −∞, δ∗= null\n3: for b in B binary search steps do\n4:\nfor t in T ′ iterations do\n5:\nδt+1 = δt −α · (c · ∇f +\nx (x + δt) −∇IΘ(x, x + δt))\n6:\nProject δt+1 to [ϵ, −ϵ] via clipping\n7:\nProject x + δt+1 to [0, 1] via clipping\n8:\nCompute IΘ(x, x + δt+1)\n9:\nif fx(x + δt+1) ≤0 and IΘ(x, x + δt+1) > I∗\nΘ then\n10:\nδ∗= δt+1\n11:\nI∗\nΘ = IΘ(x, x + δt+1)\n12:\nif fx(x + δ∗) ≤0 then\n13:\nub = min{ub, c}\n14:\nif ub < 109 then\n15:\nUpdate c ←(lb + ub)/2\n16:\nelse\n17:\nlb= max{lb, c}\n18:\nif ub < 109 then\n19:\nUpdate c ←(lb + ub)/2\n20:\nelse\n21:\nUpdate c ←c · 10\n22: Return δ∗, I∗\nΘ\n6.6\nFixed-Iteration Analysis for Penalty-based Attack\nTo compare with the attack success of our MinMax Attack in Table 4, we ﬁx T=40 total iterations for penalty-based attack.\nSpeciﬁcally, for binary search with a times (each with b iterations), we ran a × b = 40 total iterations with (a,b)=(2,20)/(4,10)\non convolutional autoencoder and MNIST. The attack success rate of (2,20)/(4,10)/MinMax(ours) is 9.0/93.54/99.86 %,\ndemonstrating the effectiveness of our attack.\n6.7\nRun-time Analysis of Different Data Augmentation Methods\nWe perform additional run-time analysis for all methods using the same computing resources on the tasks in Tables 5 for MINST\nand CIFAR-10. Table 9 shows the average run-time for generating one sample for each method. Note that although MINE-UAE\nconsumes more time than others, its effectiveness in data augmentation is also the most prominent.\nMNIST\nMINE-UAE\nL2-UAE\nGA\nFlip or Rotation\navg. per sample\n0.059 sec.\n0.025 sec.\n0.012 sec.\n9.18 × 10−5 sec.\nCIFAR-10\nMINE-UAE\nL2-UAE\nGA\nFlip or Rotation\navg. per sample\n1.735 sec.\n0.37 sec.\n0.029 sec.\n0.00022 sec.\nTable 9: Run-time analysis of different data augmentation methods.\n6.8\nProof of Theorem 1\nThe proof incorporates the several special structures of this non-convex min-max problem, e.g., linear coupling between c and\nf +\nx (x + δ), the oracle of MINE, etc, which are not explored or considered adequately in prior arts. Therefore, the following\ntheoretical analysis for the proposed algorithm is sufﬁciently different from the existing works on convergence analysis of\nmin-max algorithms, although some steps/ideas of deviation are similar, e.g., descent lemma, perturbation term. For the purpose\nof the completeness, we provide the detailed proof as follows. To make the analysis more complete, we consider a slightly more\ngeneral version of Algorithm 1, where in step 7 we use the update rule ct+1 = (1 −β · γt) · ct + β · f +\nx (x + δt+1), where γt ≥0\nand γt = 1/t1/4 is a special case used in Algorithm 1.\nProof: First, we quantity the descent of the objective value after performing one round update of δ and c. Note that the\nδ-subproblem of the attack formulation is\nδt+1 = arg\nmin\nδ:x+δ∈[0,1]d, δ∈[−ϵ,ϵ]d\n\nct∇f +\nx (x + δt) −IΘt(x, x + δt), δ −δt\n\u000b\n+ 1\n2α∥δ −δt∥2.\n(1)\nFrom the optimality condition of δ, we have\n⟨∇ctf +\nx (x + δt) −∇IΘt(x, x + δt) + 1\nα(δt+1 −δt), δt+1 −δt⟩≤0.\n(2)\nAccording to the gradient Lipschitz continuity of function f +\nx (x) and IΘ(x, x + δ), it can be easily checked that F(δ, c) has\ngradient Lipschitz continuity with constant Lf ¯c + LI. Then, we are able to have\nctf +\nx (x + δt+1) −IΘt(x, x + δt+1)\n(3)\n≤ctf +\nx (x + δt) −IΘt(x, x + δt) + ⟨ct∇f +\nx (x + δt) + ∇IΘt(x, x + δt), δt+1 −δt⟩+ Lf ¯c + LI\n2\n∥δt+1 −δt∥2 .\n(4)\nSubstituting (2) into (4), we have\nctf +\nx (x + δt+1) −IΘt(x, x + δt+1) ≤ctf +\nx (x + δt) −IΘt(x, x + δt) −\n\u0012 1\nα −Lf ¯c + LI\n2\n\u0013\n∥δt+1 −δt∥2 .\n(5)\nFrom Assumption 2, we can further know that\nctf +\nx (x + δt+1) −IΘt+1(x, x + δt+1) ≤ctf +\nx (x + δt) −IΘt(x, x + δt) −\n\u0012 1\nα −Lf ¯c + LI\n2\n\u0013\n∥δt+1 −δt∥2 + η.\n(6)\nWhen αt ≤\n1\nLf ¯c+LI , we have\nctf +\nx (x + δt+1) −IΘt+1(x, x + δt+1) ≤ctf +\nx (x + δt) −IΘt(x, x + δt) −1\n2α ∥δt+1 −δt∥2 + η.\n(7)\nLet function f ′+\nx (ct, δt) = ctf +\nx (x + δt) −1(ct) and ξt denote the subgradient of 1(ct), where 1(·) denotes the indicator\nfunction. Since function cf +\nx (x + δ) is concave with respect to c, we have\nf ′+\nx (ct+1, δt+1) −f ′+\nx (ct, δt+1) ≤⟨f +\nx (x + δt+1), ct+1 −ct⟩−⟨ξt, ct+1 −ct⟩\n=⟨f +\nx (x + δt+1) −f +\nx (x, δt), ct+1 −ct⟩+ ⟨f +\nx (x + δt), ct+1 −ct⟩\n−⟨ξt+1, ct+1 −ct⟩−⟨ξt −ξt+1, ct+1 −ct⟩\n(a)\n= 1\nβ (ct+1 −ct)2 + γtct+1(ct+1 −ct) + (ξt+1 −ξt)(ct+1 −ct)\n(b)\n= 1\nβ (ct+1 −ct)2 + γt−1ct(ct+1 −ct) + (f +\nx (x + δt+1) −f +\nx (x + δt))(ct+1 −ct)\n(c)\n≤1\n2β (ct −ct−1)2 +\nβl2\nf\n2 ∥δt+1 −δt∥2 −(γt−1\n2\n−1\nβ )(ct+1 −ct)2\n+ γt\n2 c2\nt+1 −γt−1\n2\nc2\nt + γt−1 −γt\n2\nc2\nt+1\n(8)\nwhere in (a) we use the optimality condition of c-problem, i.e.,\nξt+1 −f +\nx (x + δt+1) + 1\nβ (ct+1 −ct) + γtct+1 = 0,\n(9)\nand in (b) we substitute\n⟨ξt+1 −ξt, ct+1 −ct⟩= ⟨f +\nx (x + δt+1) −f +\nx (x + δt), ct+1 −ct⟩\n−1\nβ ⟨ct+1 −ct −(ct −ct−1)\n|\n{z\n}\n:=vt\n, ct+1 −ct⟩−⟨γtct+1 −γt−1ct, ct+1 −ct⟩,\n(10)\nand in (c) we use the quadrilateral identity and according to the Lipschitz continuity function f +\nx (x + δ), we have\n\u0000f +\nx (x + δt+1) −f +\nx (x + δt)\n\u0001\n(ct+1 −ct) ≤\nβl2\nf\n2 ∥δt+1 −δt∥2 + 1\n2β (ct+1 −ct)2,\n(11)\nand also\nγt−1ct(ct+1 −ct) = γt−1\n2\n\u0000c2\nt+1 −c2\nt −(ct+1 −ct)2\u0001\n= γt\n2 c2\nt+1 −γt−1\n2\nc2\nt −γt−1\n2\n(ct+1 −ct)2 +\n\u0012γt−1 −γt\n2\n\u0013\nc2\nt+1.\n(12)\nCombining (7) and (8), we have the descent of the objective function, i.e.,\nct+1f +\nx (x + δt+1) + IΘt+1(x, x + δt+1) −\n\u0000ctf +\nx (x + δt) + IΘt(x, x + δt)\n\u0001\n≤1\n2β (ct −ct−1)2 −\n \n1\n2α −\nβl2\nf\n2\n!\n∥δt+1 −δt∥2 −\n\u0012γt−1\n2\n−1\nβ\n\u0013\n(ct+1 −ct)2\n+ γt\n2 c2\nt+1 −γt−1\n2\nc2\nt + γt−1 −γt\n2\nc2\nt+1 + η.\n(13)\nSecond, we need to obtain the recurrence of the size of the difference between two consecutive iterates. Note that the\nmaximization problem is\nct+1 = arg max\n0≤c≤¯c cf +\nx (x + δt+1) −1\n2β (c −ct)2 −γtc2,\n(14)\nand the update of sequence {ct} can be implemented very efﬁciently as stated in the algorithm description as\nct+1 = PC\n\u0000(1 −βγt)ct + βf +\nx (x + δt+1)\n\u0001\n.\n(15)\nFrom the optimality condition of c-subproblem at the t + 1th iteration, we have\n−⟨f +\nx (x + δt+1) −1\nβ (ct+1 −ct) −γtct+1, ct+1 −c⟩≤0, ∀c ∈C\n(16)\nalso, from the optimality condition of c-subproblem at the tth iteration, we have\n−⟨f +\nx (x + δt) −1\nβ (ct −ct−1) −γt−1ct, c −ct⟩≥0, ∀c ∈C.\n(17)\nPlugging in c = ct in (16), c = ct+1 in (17) and combining them together, we can get\n1\nβ vt+1(ct+1 −ct) + (γtct+1 −γt−1ct)(ct+1 −ct) ≤(f +\nx (x + δt+1) −f +\nx (x + δt))(ct+1 −ct).\n(18)\nIn the following, we will use this above inequality to analyze the recurrence of the size of the difference between two\nconsecutive iterates. Note that\n(γtct+1 −γt−1ct)(ct+1 −ct) = (γtct+1 −γtct + γtct −γt−1ct)(ct+1 −ct)\n=γt(ct+1 −ct)2 + (γt −γt−1)ct(ct+1 −ct)\n=γt(ct+1 −ct)2 + γt −γt−1\n2\n\u0000c2\nt+1 −c2\nt −(ct+1 −ct)2\u0001\n,\n=γt + γt−1\n2\n(ct+1 −ct)2 −γt−1 −γt\n2\n\u0000c2\nt+1 −c2\nt\n\u0001\n(19)\nand\nvt+1(ct+1 −ct) = 1\n2\n\u0000(ct+1 −ct)2 + v2\nt+1 −(ct −ct−1)2\u0001\n.\n(20)\nSubstituting (19) and (20) into (18), we have\n1\n2β (ct+1 −ct)2 −γt−1 −γt\n2\nc2\nt+1\n≤1\n2β (ct −ct−1)2 −1\n2β v2\nt+1 −γt−1 −γt\n2\nc2\nt −γt−1 + γt\n2\n(ct+1 −ct)2 + (f +\nx (x + δt+1) −f +\nx (x + δt))(ct+1 −ct)\n(21)\n(a)\n≤1\n2β (ct −ct−1)2 −γt(ct+1 −ct)2 −γt−1 −γt\n2\nc2\nt + (f +\nx (x + δt+1) −f +\nx (x + δt))(ct+1 −ct)\n(22)\n(b)\n≤1\n2β (ct −ct−1)2 −γt(ct+1 −ct)2 −γt−1 −γt\n2\nc2\nt +\nl2\nf\n2γt\n∥δt+1 −δt∥2 + γt\n2 (ct+1 −ct)2\n(23)\n≤1\n2β (ct −ct−1)2 −γt\n2 (ct+1 −ct)2 −γt−1 −γt\n2\nc2\nt +\nl2\nf\n2γt\n∥δt+1 −δt∥2\n(24)\nwhere (a) is true because 0 < γt < γt−1; in (b) we use Young’s inequality.\nMultiplying by 4 and dividing by βγt on the both sides of the above equation, we can get\n2\nβ2γt\n(ct+1 −ct)2 −2\nβ\n\u0012γt−1\nγt\n−1\n\u0013\nc2\nt+1\n≤\n2\nβ2γt\n(ct −ct−1)2 −2\nβ\n\u0012γt−1\nγt\n−1\n\u0013\nc2\nt −2\nβ (ct+1 −ct)2 +\n2l2\nf\nβγ2\nt\n∥δt+1 −δt∥2\n(25)\n≤\n2\nβ2γt−1\n(ct −ct−1)2 −2\nβ\n\u0012γt−2\nγt−1\n−1\n\u0013\nc2\nt + 2\nβ\n\u0012 1\nγt\n−\n1\nγt−1\n\u0013\n(ct −ct−1)2\n+ 2\nβ\n\u0012γt−2\nγt−1\n−γt−1\nγt\n\u0013\nc2\nt −2\nβ (ct+1 −ct)2 +\n2l2\nf\nβγ2\nt\n∥δt+1 −δt∥2.\n(26)\nCombining (13) and (26), we have\nct+1f +\nx (x + δt+1) + IΘt+1(x, x + δt+1) −γt\n2 c2\nt+1 +\n2\nβ2γt\n(ct+1 −ct)2 −2\nβ\n\u0012γt−1\nγt\n−1\n\u0013\nc2\nt+1\n≤ctf +\nx (x + δt) + IΘt(x, x + δt) −γt−1\n2\nc2\nt +\n2\nβ2γt−1\n(ct −ct−1)2 −2\nβ\n\u0012γt−2\nγt−1\n−1\n\u0013\nc2\nt\n+ 1\n2β (ct −ct−1)2 −1\nβ (ct+1 −ct)2 −\n \n1\n2αt\n−\n \nβL2\nf\n2\n+\n2L2\nf\nβγ2\nt\n!!\n∥δt+1 −δt∥2\n+ γt−1 −γt\n2\nc2\nt+1 + 2\nβ\n\u0012 1\nγt\n−\n1\nγt−1\n\u0013\n(ct −ct−1)2 + 2\nβ\n\u0012γt−2\nγt−1\n−γt−1\nγt\n\u0013\nc2\nt + η.\n(27)\nThird, we construct the potential function to measure the descent achieved by the sequence. From (27), we have\nPt+1 ≤Pt −1\n2β (ct+1 −ct)2 −\n \n1\n2α −\n \nβl2\nf\n2\n+\n2l2\nf\nβγ2\nt\n!!\n∥δt+1 −δt∥2\n+ γt−1 −γt\n2\nc2\nt+1 + 2\nβ\n\u0012\n1\nγt+1\n−1\nγt\n\u0013\n(ct+1 −ct)2 + 2\nβ\n\u0012γt−2\nγt−1\n−γt−1\nγt\n\u0013\nc2\nt + η\n(28)\nwhere we deﬁne\nPt := ctf(x+δt)+IΘt(x, x+δt)+\n\u0012 1\n2β +\n2\nβ2γt−1\n+ 2\nβ\n\u0012 1\nγt\n−\n1\nγt−1\n\u0013\u0013\n(ct −ct−1)2 −γt−1\n2\nc2\nt −2\nβ\n\u0012γt−2\nγt−1\n−1\n\u0013\nc2\nt (29)\nTo have the descent of the potential function, it is obvious that we need to require\n−1\n2β + 2\nβ\n\u0012\n1\nγt+1\n−1\nγt\n\u0013\n< 0,\n(30)\nwhich is equivalent to condition 1/γt+1 −1/γt ≤0.25 so that the sign in the front of term (ct+1 −ct)2 is negative.\nWhen 1/γt+1 −1/γt ≤0.2, we have\nPt+1 ≤Pt −\n \n1\n2α −\n \nβl2\nf\n2\n+\n2l2\nf\nβγ2\nt\n!!\n∥δt+1 −δt∥2 −\n1\n10β (ct+1 −ct)2 + γt−1 −γt\n2\nc2\nt+1 + 2\nβ\n\u0012γt−2\nγt−1\n−γt−1\nγt\n\u0013\nc2\nt +η, (31)\nwhich can be also rewritten as\n \n1\n2α −\n \nβl2\nf\n2\n+\n2l2\nf\nβγ2\nt\n!!\n∥δt+1 −δt∥2 +\n1\n10β (ct+1 −ct)2 ≤Pt −Pt+1 + γt−1 −γt\n2\nc2\nt+1 + 2\nβ\n\u0012γt−2\nγt−1\n−γt−1\nγt\n\u0013\nc2\nt +η. (32)\nFinally, we can provide the convergence rate of the MinMax algorithm as the following.\nFrom the deﬁnition that\nL(δ, c) :=\n\u0014\nδ −P∆[δ −∇δF(δ, c)]\nc −PC[c + ∇cF(δ, c)]\n\u0015\n,\nwe know that\n∥L(δt, ct)∥\n≤∥δt+1 −δt∥+ ∥δt+1 −P∆(δt −∇δF(δt, ct))∥+ |ct+1 −ct| + |ct+1 −PC(ct + ∇cF(δt, ct))|\n(a)\n≤∥δt+1 −δt∥+\n\r\r\r\rP∆\n\u0012\nδt+1 −(∇δF(δt, ct) + 1\nαt\n(δt+1 −δt))\n\u0013\n−P∆(δt −∇δF(δt, ct))\n\r\r\r\r\n+ |ct+1 −ct| +\n\f\f\f\fPC\n\u0012\nct+1 + f(δt+1) −1\nβ (ct+1 −ct) −γtct+1\n\u0013\n−PC(ct + f(δt))\n\f\f\f\f\n(b)\n≤\n\u0012\n2 + 1\nα\n\u0013\n∥δt+1 −δt∥+\n\u0012\n2 + 1\nβ\n\u0013\n|ct+1 −ct| + γtct+1 + |f(δt+1) −f(δt)|\n(c)\n≤\n\u0012\n2 + 1\nα + lf\n\u0013\n∥δt+1 −δt∥+\n\u0012\n2 + 1\nβ\n\u0013\n|ct+1 −ct| + γtct+1\n(33)\nwhere in (a) we the optimality condition of subproblems; in (b) we use the triangle inequality and non-expansiveness of the\nprojection operator; and (c) is true due to the Lipschitz continuity.\nThen, we have\n∥L(δt, ct)∥2 ≤3\n\u0012\n2 + 1\nα + lf\n\u00132\n∥δt+1 −δt∥2 + 3\n\u0012\n2 + 1\nβ\n\u00132\n(ct+1 −ct)2 + 3γ2\nt ¯c2.\n(34)\nWhen α ∼γ2\nt ∼η ∼O(1/\n√\nT) , and\n1\n2α >\n \nβl2\nf\n2\n+\n2l2\nf\nβγ2\nt\n!\n,\n(35)\nthen from (32) we can know that there exist constants C1 and C2 such that\nC1\n√\nT∥δt+1 −δt∥2 +\n1\n10β (ct+1 −ct)2 ≤Pt −Pt+1 + γt−1 −γt\n2\nc2\nt+1 + 2\nβ\n\u0012γt−2\nγt−1\n−γt−1\nγt\n\u0013\n¯c2 + C2\n√\nT\n,\n(36)\nand from (34) there exists constant C3 such that\n∥L(δt, ct)∥2 ≤C3T∥δt+1 −δt∥2 + 3\n\u0012\n2 + 1\nβ\n\u00132\n(ct+1 −ct)2 + 3γ2\nt ¯c2\n(37)\nCombining (36) and (37), we have\n∥L(δt, ct)∥2\n≤max\n(\nmax{C1, C3}\n√\nT, max\n( √\nT\n10β , 3\n\u0012\n2 + 1\nβ\n\u00132)) \u0012\nPt −Pt+1 + γt−1 −γt\n2\nc2\nt+1 + 2\nβ\n\u0012γt−2\nγt−1\n−γt−1\nγt\n\u0013\n¯c2\n\u0013\n+ 3γ2\nt ¯c2 + C2\n√\nT\n.\n(38)\nHence, there exist constant C4 such that\n∥L(δt, ct)∥2 ≤\n√\nTC4\n\u0012\nPt −Pt+1 + γt−1 −γt\n2\n¯c2 + 2\nβ\n\u0012γt−2\nγt−1\n−γt−1\nγt\n\u0013\n¯c2\n\u0013\n+ 3γ2\nt ¯c2 + C2\n√\nT\n.\n(39)\nApplying the telescoping sum, we have\n1\nT\nT\nX\nt=1\n∥L(δt, ct)∥2 ≤C4\n√\nT\n\u0012\n(P1 −PT +1) + γ0¯c2\n2\n+ 2γ0¯c2\nγ1β\n\u0013\n+ 3 ¯c2\n√\nT\n+ C2\n√\nT\n.\n(40)\nAccording to the deﬁnition of T(ε′), we can conclude that there exists constant C such that\n∥L(δT (ε′), cT (ε′))∥2 ≤\n1\nT(ε′)\nT (ε′)\nX\nt=1\n∥L(δt, ct)∥2 ≤\nC\np\nT(ε′)\n∼O\n \n1\np\nT(ε′)\n!\n,\n(41)\nwhich completes the proof.\n■\n6.9\nMore Discussion on UAEs and On-Manifold Adversarial Examples\nFollowing (Stutz, Hein, and Schiele 2019), adversarial examples can be either on or off the data manifold, and only the\non-manifold data samples are useful for model generalization. In the supervised setting, without the constraint on ﬁnding\non-manifold examples, it is shown that simply using norm-bounded adversarial examples for adversarial training will hurt model\ngeneralization2 (Stutz, Hein, and Schiele 2019).\nAs illustrated in Table 1, in the unsupervised setting we propose to use the training loss constraint to ﬁnd on-manifold\nadversarial examples and show they can improve model performance.\n6.10\nMore Details and Results on Data Augmentation and Model Retraining\nThe attack success rate (ASR) measures the effectiveness of ﬁnding UAEs. In Table 4, sparse autoencoder has low ASR due to\nfeature selection, making attacks more difﬁcult to succeed. MNIST’s ASR can be low because its low reconstruction loss limits\nthe feasible space of UAEs.\nTable 10 summarizes the training epochs and training loss of the models and datasets used in Section 4.4.\nMNIST\nTraining Epochs\nReconstruction Error (training set)\nAutoencoder\nOriginal\nMINE-UAE\nL2-UAE\nGA\n(σ = 10−2/10−3)\nOriginal\nMINE-UAE\nL2-UAE\nGA\n(σ = 10−2)\nGA\n(σ = 10−3)\nSparse\n50\n80\n80\n80\n0.00563\n0.00233\n0.00345\n0.00267±2.93e-05\n0.00265±3.60e-5\nDense\n20\n30\n30\n30\n0.00249\n0.00218\n0.00275\n0.00231±0.00013\n0.0023±0.00011\nConvolutional\n20\n30\n30\n30\n0.00301\n0.00260\n0.00371\n0.00309±0.00013\n0.00310±0.00015\nAdversarial\n50\n80\n80\n80\n0.044762\n0.04612\n0.06063\n0.058711±0.00659\n0.05551±0.00642\nSVHN\nSparse\n50\n80\n80\n80\n0.00729\n0.00221\n0.00290\n0.00283±0.00150\n0.00275 ± 0.00081\nDense\n30\n50\n50\n50\n0.00585\n0.00419\n0.00503\n0.00781±0.00223\n0.00781±0.00187\nConvolutional\nWe set 100 epochs, but the training loss converges\nafter 5 epochs\n0.00140\n0.00104\n0.00131\n0.00108±3.83e-05\n0.00113±6.76e-05\nAdversarial\nWe set 200 epochs and use the\nmodel with the lowest training loss\n0.00169\n0.00124\n0.02729\n0.00158±0.00059\n0.00130±0.00036\nTable 10: Details on the training epochs and losses for the datasets and models used in Section 4.4. The reconstruction error is\nthe average L2 reconstruction loss of the training set.\nIn Table 6, concrete autoencoder on Coil-20 has low ASR and the performance does not improve when using MINE-UAE\nfor data augmentation. The original training loss is 0.00683 while the MINE-UAE training loss is increased to 0.01368. With\ncareful parameter tuning, we were still unable to improve the MINE-UAE training loss. Therefore, we conclude that the degraded\nperformance in Coil-20 after data augmentation is likely due to the imitation of feature selection protocol and the model learning\ncapacity.\n6.11\nCIFAR-10 and Tiny-ImageNet Results\nTable 11 shows the results using convolutional autoencoders on CIFAR-10 and Tiny-ImageNet using the same setup as in Table 4.\nSimilar to the ﬁndings in Table 4 based on MNIST and SVHN, MINE-UAE attains the best loss compared to L2-UAE and GA.\nCIFAR-10 (original loss: 0.00669) / Tiny-ImageNet (original loss: 0.00979)\nReconstruction Error (test set) / ASR (training set)\nMINE-UAE\nL2-UAE\nGA (σ = 10−3)\n0.00562 (↑15.99%) / 99.98%\n0.00613 (↑8.37%) / 89.84%\n0.00564±1.31e-04 (↑15.70%) / 99.90%\n0.00958 (↑21.45%)/ 99.47%\n0.00969 (↑1.02%) / 88.02%\n0.00963±5.31e-05 (↑1.63%) / 99.83%\nTable 11: Comparison of data reconstruction by retraining the convolutional autoencoder on UAE-augmented data using CIFAR-\n10 (ﬁrst row) and Tiny-ImageNet (second row). The reconstruction error is the average L2 reconstruction loss of the test set. The\nimprovement (in green/red) is relative to the original model. The attack success rate (ASR) is the fraction of augmented training\ndata having smaller reconstruction loss than the original loss (see Table 1 for deﬁnition).\n2https://davidstutz.de/on-manifold-adversarial-training-for-boosting-generalization\n6.12\nMore Details on Table 5\nFor all convolutional autoencoders, we use 100 epochs and early stopping if training loss converges at early stage. For data\naugmentation of the training data, we set the rotation angle = 10 and use both horizontal and vertical ﬂip. For Gaussian noise,\nwe use zero-mean and set σ = 0.001.\n6.13\nHyperparameter Sensitivity Analysis\nAll of the aforementioned experiments were using β = 0.1 and α = 0.01. Here we show the results on MNIST with convolution\nautoencoder (Section 4.4) and the concrete autoencoder (Section 4.5) using different combinations of α and β values. The results\nare comparable, suggesting that our MINE-based data augmentation is robust to a wide range of hyperpameter values.\nMNIST\nConvolution AE\nConcrete AE\nα\nβ\n0.05\n0.1\n0.5\n0.05\n0.1\n0.5\n0.01\n0.00330\n0.00256\n0.00283\n0.01126\n0.01142\n0.01134\n0.05\n0.00296\n0.00278\n0.00285\n0.01129\n0.01133\n0.01138\nTable 12: Comparison of reconstruction error (test set) of convolution and concrete autoencoders with different combinations of\nα and β values on MNIST.\n6.14\nAblation Study on UAE’s Similarity Objective Function\nAs an ablation study to demonstrate the importance of using MINE for crafting UAEs, we replace the mutual information (MI)\nwith L2-norm and cosine distance between conv(x) and conv(x + δ) in our MinMAx attack algorithm. Table 13 compares\nthe performance of retraining convolutional autoencoders with UAEs generated by the L2-norm , cosine distances and feature\nscattering-based proposed by Zhang and Wang (2019). It is observed that using MINE is more effective that these two distances.\nMNIST (original reconstruction error: 0.00294)\nReconstruction Error (test set)\nASR (training set)\nMINE-UAE\n0.00256 (↑12.9%)\n99.86%\nL2 / cosine\n0.002983 (↓14.6%) / 0.002950 (↓0.3%)\n96.5% / 92%\nFeature scattering-based\n0.00268 (↑8.9%)\n99.96%\nSVHN (original reconstruction error: 0.00128)\nMINE-UAE\n0.00095 (↑25.8%)\n100%\nL2 / cosine\n0.00096 (↑18.75%) / 0.00100 (↑21.88%)\n94.52% / 96.79%\nFeature scattering-based\n0.001 (↑21.6%)\n99.76%\nTable 13: Ablation Study on UAE’s similarity objective function using MinMax algorithm. The reconstruction error is the average\nL2 reconstruction loss of the test set. The improvement (in green/red) is relative to the original model.\n6.15\nMore Data Augmentation Runs\nWhile the ﬁrst fun of UAE-based data augmentation is shown to improve model performance, here we explore the utility of more\ndata augmentation runs. We conducted two data augmentation runs for sparse autoencoder on SVHN. We re-train the model with\n1st-run UAEs, 2nd-run UAEs (generated by 1st-run augmented model) and original training data. The reconstruction error on the\ntest set of 2nd data augmentation is 0.00199, which slightly improves the 1st-run result (0.00235). In general, we ﬁnd that 1st-run\nUAE data augmentation has a much more signiﬁcant performance gain comparing to the 1st-run results.\n6.16\nMINE-based Supervised Adversarial Examples for Adversarially Robust Models\nVisual Comparison of Supervised Adversarial Examples for Adversarially Robust MNIST Models Figure 6 shows adver-\nsarial examples crafted by our attack against the released adversarially robust models trained using the Madry model (Madry\net al. 2018) and TRADES (Zhang et al. 2019). Similar to the conclusion in Figure 3, our MINE-based attack can generate\nhigh-similarity and high-quality adversarial examples for large ϵ values, while PGD attack fail to do.\n(a)\n(b) Madry model (our attack)\n(c) Madry model (PGD)\n(d) TRADES (our attack)\n(e) TRADES (PGD)\nFigure 6: Visual comparison of untargeted supervised adversarial examples on two released adversarial robust MNIST models:\nMadry model (Madry et al. 2018) and TRADES (Zhang et al. 2019). We mark the unsuccessful adversarial examples with the\nred cross. Each row shows the crafted adversarial examples of an original sample. Each column corresponds to different ϵ values\n(L∞-norm perturbation bound) ranging from 0.1 to 1.0.\nAttack Performance Figure 7 and 8 show the attack success rate (ASR) of released Madry and TRADES models against\nour attack and PGD attack on MNIST and CIFAR-10 with different L∞threshold ϵ. For all PGD attacks, we use 100 steps and\nset step size = 2.5 ∗\nϵ\nnumber of steps. For all our attacks against Madry model, we use learning rate = 0.01 and run 200 steps for\nMNIST, 100 steps for CIFAR-10. For all our attacks against TRADES model, we use learning rate = 0.01 and run 300 steps for\nMNIST, 200 steps for CIFAR-10. Both attacks have comparable attack performance. On MNIST, in the mid-range of the ϵ values,\nour attack ASR is observed to be lower than PGD, but it can generate higher-quality adversarial examples as shown in Figure 6.\n(a) MNIST\n(b) CIFAR-10\nFigure 7: Attack success rate (ASR) of the released Madry\nmodels (Madry et al. 2018) on MNIST and CIFAR-10.\n(a) MNIST\n(b) CIFAR-10\nFigure 8: Attack success rate (ASR) of the released\nTRADES models on MNIST and CIFAR-10.\nAdversarial Training with MINE-based Unsupervised Adversarial Examples We use the MNIST and CIFAR-10 models\nin Section 4.3 to compare the performances of standalone adversarial training (Adv. training) (Madry et al. 2018) and adversarial\ntraining plus data augmentation by MINE-based unsupervised adversarial examples (Adv. training-UAE) generated from\nconvolutional Autoencoder. Figure 9 shows the attack success rate (ASR) of Adv. training model and Adv training-UAE against\nPGD attack. For all PGD attacks, we use 100 steps and set step size = 2.5 ∗\nϵ\nnumber of steps. When ϵ = 0.4 , Adv. training-UAE\nmodel can still resist more than 60% of adversarial examples on MNIST. By contrast, ASR is 100% for Adv. training model. For\nCIFAR-10, ASR of Adv. training-UAE model is about 8% lower than Adv. training model when ϵ = 16. We therefore conclude\nthat data augmentation using UAE can improve adversarial training.\n(a) MNIST\n(b) CIFAR-10\nFigure 9: Performance evaluation of ASR for adv. training and adv. training-UAE against PGD attack with different ϵ value. Adv.\ntraining-UAE consistently shows lower or comparable ASR than adv. training, suggesting that data augmentation using UAE can\nimprove adversarial training.\n6.17\nImproved Adversarial Robustness after Data Augmentation with MINE-UAEs\nTo evaluate the adversarial robustness after data augmentation with MINE-UAEs, we use the MNIST and CIFAR-10 models in\nSection 4.3 and Section 4.6, respectively. We randomly select 1000 classiﬁed correctly images (test set) to generate adversarial\nexamples. For all PGD attacks, We set step size= 0.01 and use 100 steps.\nIn our ﬁrst experiment (Figure 10 (a)), we train the convolutional classiﬁer (Std) and Std with UAE (Std-UAE) generated from\nthe convolutional autoencoder on MNIST. The attack success rate (ASR) of the Std-UAE is consistently lower than the Std for\neach ϵ value.\nIn the second experiment (Figure 10 (b)), the ASR of SimCLR-UAE is signiﬁcantly lower than that of the original SimCLR\nmodel, especially for ϵ ≤0.02. When ϵ = 0.01, SimCLR-UAE on CIFAR-10 can still resist more than 40% of adversarial\nexamples, which is signiﬁcantly better than the original SimCLR model. Based on the empirical results, we therefore conclude\nthat data augmentation using UAE can improve adversarial robustness of unsupervised machine learning tasks.\n(a) MNIST (convolutional-autoencoder)\n(b) CIFAR-10 (SimCLR)\nFigure 10: Robustness evaluation of attack success rate (ASR) for the original model (Std/SimCLR) and the models trained with\nUAE augmentation (Std-UAE/ SimCLR-UAE) against PGD attack with different ϵ values. Models trained with UAE shows\nbetter robustness (lower ASR) than original models, implying that data augmentation with UAE can strengthen the adversarial\nrobustness.\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2021-03-02",
  "updated": "2021-12-08"
}