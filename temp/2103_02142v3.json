{
  "id": "http://arxiv.org/abs/2103.02142v3",
  "title": "Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control",
  "authors": [
    "Jacopo Panerati",
    "Hehui Zheng",
    "SiQi Zhou",
    "James Xu",
    "Amanda Prorok",
    "Angela P. Schoellig"
  ],
  "abstract": "Robotic simulators are crucial for academic research and education as well as\nthe development of safety-critical applications. Reinforcement learning\nenvironments -- simple simulations coupled with a problem specification in the\nform of a reward function -- are also important to standardize the development\n(and benchmarking) of learning algorithms. Yet, full-scale simulators typically\nlack portability and parallelizability. Vice versa, many reinforcement learning\nenvironments trade-off realism for high sample throughputs in toy-like\nproblems. While public data sets have greatly benefited deep learning and\ncomputer vision, we still lack the software tools to simultaneously develop --\nand fairly compare -- control theory and reinforcement learning approaches. In\nthis paper, we propose an open-source OpenAI Gym-like environment for multiple\nquadcopters based on the Bullet physics engine. Its multi-agent and vision\nbased reinforcement learning interfaces, as well as the support of realistic\ncollisions and aerodynamic effects, make it, to the best of our knowledge, a\nfirst of its kind. We demonstrate its use through several examples, either for\ncontrol (trajectory tracking with PID control, multi-robot flight with\ndownwash, etc.) or reinforcement learning (single and multi-agent stabilization\ntasks), hoping to inspire future research that combines control theory and\nmachine learning.",
  "text": "Learning to Fly—a Gym Environment with PyBullet Physics for\nReinforcement Learning of Multi-agent Quadcopter Control\nJacopo Panerati,1,2 Hehui Zheng,3 SiQi Zhou,1,2 James Xu,1 Amanda Prorok,3 and Angela P. Schoellig1,2\nAbstract— Robotic simulators are crucial for academic re-\nsearch and education as well as the development of safety-\ncritical applications. Reinforcement learning environments—\nsimple simulations coupled with a problem speciﬁcation in the\nform of a reward function—are also important to standardize\nthe development (and benchmarking) of learning algorithms.\nYet, full-scale simulators typically lack portability and paral-\nlelizability. Vice versa, many reinforcement learning environ-\nments trade-off realism for high sample throughputs in toy-\nlike problems. While public data sets have greatly beneﬁted\ndeep learning and computer vision, we still lack the software\ntools to simultaneously develop—and fairly compare—control\ntheory and reinforcement learning approaches. In this paper,\nwe propose an open-source OpenAI Gym-like environment for\nmultiple quadcopters based on the Bullet physics engine. Its\nmulti-agent and vision-based reinforcement learning interfaces,\nas well as the support of realistic collisions and aerodynamic\neffects, make it, to the best of our knowledge, a ﬁrst of its\nkind. We demonstrate its use through several examples, either\nfor control (trajectory tracking with PID control, multi-robot\nﬂight with downwash, etc.) or reinforcement learning (single\nand multi-agent stabilization tasks), hoping to inspire future\nresearch that combines control theory and machine learning.\nI. INTRODUCTION\nOver the last decade, the progress of machine learning—\nand deep learning speciﬁcally—has revolutionized computer\nscience by obtaining (or surpassing) human performance in\nseveral tasks, including image recognition and game play-\ning [1]. New algorithms, coupled with shared benchmarks\nand data sets have greatly contributed to the advancement of\nmultiple ﬁelds (e.g., as the KITTI suite [2] did for computer\nvision in robotics). While reinforcement learning (RL) looks\nas a very appealing solution to bridge the gap between\ncontrol theory and deep learning, we are still in the infancy\nof the creation of tools for the development of realistic\ncontinuous control applications through deep RL [3].\nAs automation becomes more pervasive —from healthcare\nto aerospace, from package delivery to disaster recovery—\nbetter tools to design robotic applications are also required.\nSimulations are an indispensable step in the design of both\nrobots and their control approaches [4], especially so when\nbuilding a prototype is expensive and/or safety (of the hard-\nware and its surroundings) is a concern. Besides platform-\nspeciﬁc and proprietary software, many of today’s open-\nsource robotic simulators are based on ROS’s plumbing and\nengines like Gazebo and Webots. While these solutions can\n1Jacopo Panerati, SiQi Zhou, James Xu, and Angela P. Schoellig are\nwith the Dynamic Systems Lab, Institute for Aerospace Studies, University\nof Toronto, Canada, e-mails: {name.lastname}@utoronto.ca; and\nthe 2Vector Institute for Artiﬁcial Intelligence in Toronto. 3Hehui Zheng\nand Amanda Prorok are with the the Prorok Lab and the Department of\nComputer Science and Technology, University of Cambridge, Cambridge,\nUnited Kingdom, e-mails: {hz337, asp45}@cam.ac.uk.\nFig. 1.\nRendering of a gym-pybullet-drones simulation with 10\nCrazyﬂie 2.x on a circular trajectory and a rubber duck for scale.\nleverage a host of existing plugins, their limited portability\ncan hinder typical machine learning workﬂows, based on\nremote, highly parallel, computing cluster execution.\nIn an attempt to standardize and foster deep RL research,\nover the last few years, RL environments have multiplied.\nOpenAI’s Gym [5] emerged as a standard that comprises\n(i) a suite of benchmark problems as well as (ii) an API\nfor the deﬁnition of new environments. Because of the\ninherent similarities between the decision-making loops of\ncontrol theory and RL, many popular environments are\ninspired by control tasks (e.g. the balancing of a pole on\na cart). However, because deep RL algorithms often rely on\nlarge amounts of data, some of these environments trade-off\nrealism for high sample throughputs. A reason for concern\nis that developing—and benchmarking—algorithms on envi-\nronments that are not necessarily representative of practical\nscenarios might curb the progress of RL in robotics [6].\nWith this work, we want to provide both the robotics and\nmachine learning (ML) communities with a compact, open-\nsource Gym-style environmenta that supports the deﬁnition\nof multiple learning tasks (multi-agent RL, vision-based RL,\netc.) on a practical robotic application: the control of one or\nmore nanoquadcopters. The softwarebc provided with this\npaper, gym-pybullet-drones, can help both roboticists\nand ML engineers to develop end-to-end quadcopter control\nwith model-free or model-based RL. The main features of\ngym-pybullet-drones are:\n1) Realism: support for realistic collisions, aerodynamics\neffects, and extensible dynamics via Bullet Physics [10].\naVideo: https://youtu.be/VdTsVu1HuYk\nbhttps://utiasdsl.github.io/gym-pybullet-drones\nchttps://github.com/utiasDSL/gym-pybullet-drones\narXiv:2103.02142v3  [cs.RO]  25 Jul 2021\nTABLE I\nFEATURE COMPARISON BETWEEN THIS WORK AND RECENT QUADCOPTER SIMULATORS WITH A FOCUS ON RL OR THE CRAZYFLIE 2.X\nPhysics\nRendering\nLanguage\nSynchro./Steppable\nRGB, Depth, and\nMultiple\nGym API\nMulti-agent\nEngine\nEngine\nPhysics & Rendering\nSegmentation Views\nVehicles\nGym-like API\nThis work\nPyBullet\nOpenGL3†\nPython\nYes\nYes\nYes\nYes\nYes\nFlightmare [7]\nAd hoc\nUnity\nC++\nYes\nYes\nYes\nW/o Vision\nNo\nAirSim [8]\nPhysX¶\nUE4\nC++\nNo\nYes\nYes\nNo\nNo\nCrazyS [9]\nGazebo§\nOGRE\nC++\nYes\nNo Segmentation\nNo\nNo\nNo\n† or TinyRenderer\n¶ or FastPhysicsEngine\n§ ODE, Bullet, DART, or Simbody\n2) RL Flexibility: availability of Gym-style environments\nfor\nboth\nvision-based\nRL\nand\nmulti-agent\nRL—\nsimultaneously, if desired.\n3) Parallelizability: multiple environments can be easily\nexecuted, with a GUI or headless, with or without a\nGPU, with minimal installation requirements.\n4) Ease-of-use: pre-implemented PID control, as well as\nStable Baselines3 [11] and RLlib workﬂows [12].\nSection II of this paper reviews similar simulation environ-\nments for RL, in general, and quadcopters, speciﬁcally. Sec-\ntion III details the inner working and programming interfaces\nof our Gym environment. In Sections IV and V, we analyze\nits computing performance and provide control and learning\nuse cases. Section VI suggests the possible extensions of this\nwork. Finally, Section VII concludes the paper.\nII. RELATED WORK\nSeveral reinforcement learning environments and quad-\ncopter simulators are already available to the community,\noffering different features and capabilities. Here, we brieﬂy\ndiscuss (i) the current landscape of RL environments, (ii) the\nlearning interfaces of existing quadcopter simulators, and (iii)\nhow they compare to gym-pybullet-drones.\nA. Reinforcement Learning Environments\nThe OpenAI Gym toolkit [5] was created in 2016 to\naddress the lack of standardization among the benchmark\nproblems used in reinforcement learning research and, within\nﬁve years, it was cited by over 2000 publications. Besides\nthe standard API adopted in this work, it comprises multiple\nproblem sets. Some of the simplest, “Classical control”\nand “Box2D”, are two-dimensional, often discrete action\nproblems—e.g., the swing-up of a pendulum. The more\ncomplex problems, “Robotics” and “MuJoCo”, include con-\ntinuous control of robotic arms and legged robots in three-\ndimensions (Swimmer, Hopper, HalfCheetah, etc.) that are\nbased on the proprietary MuJoCo physics engine [13].\nMuJoCo’s\nphysics\nengine\nalso\npowers\nDeepMind’s\ndm control [14]. While dm control’s environments do\nnot expose the same API as Gym, they are very similarly\nstructured and DeepMind’s suite includes many of the same\narticulated-body locomotion and manipulation tasks. How-\never, because of smoothing around the contacts and other\nsimpliﬁcations, even locomotion policies trained successfully\nwith these environments do not necessarily exhibit gaits that\nwould easily transfer to physical robots [3].\nThe need for MuJoCo’s licensing also led to the develop-\nment and adoption of open-source alternatives such as Geor-\ngia Tech/CMU’s DART and Google’s Bullet Physics [10]\n(with its Python binding, PyBullet). Open-source Bullet-\nbased re-implementations of the control and locomotion tasks\nin [5] are also provided in pybullet-gym. Community-\ncontributed Gym environments like gym-minigrid [15]—a\ncollection of 2D grid environments—were used by over 30\npublications between 2018 and 2021.\nBoth OpenAI and Google Research have made re-\ncent\nstrides\nto\ninclude\nsafety\nrequirements\nand\nreal-\nworld uncertainties in their control and legged loco-\nmotion\nRL\nbenchmarks\nwith\nsafety-gym\n[16]\nand\nrealworldrl suite [17], respectively.\nOne of the most popular Gym environment for quad-\ncopters is gymfc [18]. While having a strong focus on\nthe transferability to real hardware, the work in [18] only\naddresses the learning of an attitude control loop that exceeds\nthe performance of a PID implementation, using Gazebo\nsimulations. Work similar to [18], training a neural network\nin simulation for the sim2real stabilization of a Crazyﬂie 2.x\n(the same quadcopter model used here), is presented in [19].\nTo the best of our knowledge, gym-pybullet-drones is\nthe ﬁrst general purpose multi-agent Gym environment for\nquadcopters.\nB. Quadcopter Simulators\nRotorS [20] is a popular quadcopter simulator based on\nROS and Gazebo. It includes multiple AscTec multirotor\nmodels and simulated sensors (IMU, etc.). However, it does\nnot come with ready-to-use RL interfaces and its dependency\non Gazebo can make it ill-advised for parallel execution or\nvision-based learning applications. CrazyS [9] is an exten-\nsion of RotorS that is speciﬁcally targeted to the Bitcraze\nCrazyﬂie 2.x nanoquadcopter. Due to its accessibility and\npopularity in research, we also chose the Crazyﬂie to be\nthe default quadcopter model in gym-pybullet-drones.\nHowever, for RL applications, CrazyS suffers from the same\nlimitations as RotorS.\nMicrosoft’s AirSim [8] is one of the best known simu-\nlators supporting multiple vehicles—car and quadcopters—\nand photorealistic rendering through Unreal Engine 4. While\nbeing an excellent choice for the development of self-\ndriving applications, its elevated computational requirements\nand overly simpliﬁed collisions—using FastPhysicsEngine in\nmultirotor mode—make it less than ideal for learning control.\nAirSim also lacks a native Gym interface, yet wrappers for\nvelocity input control have been proposed [21].\nThe most recent and closely related work to ours is ETH’s\nUnity-based Flightmare [7]. This simulator was created to\nsimultaneously provide photorealistic rendering and very\nfast, highly parallel dynamics. Flightmare also implements\nGym’s API and it includes a single agent RL workﬂow. Un-\nlike gym-pybullet-drones, however, Flighmare does not\ninclude a Gym with vision-based observations nor one com-\npatible with multi-agent reinforcement learning (MARL).\ncf2x.urdf\ncf2p.urdf\nhb.urdf\nxG\nyG\nzG\nCF2.x\nxL\nyL\nzL\nT\n˙x\nD\nW\nF0\nF1\nF2\nF3\nG0\nG1\nG3\nG2\nFig. 2.\nThe three—1 in ×-conﬁguration and 2 in +-conﬁguration—\nquadcopter models in gym-pybullet-drones (top) and the forces and\ntorques acting on each vehicle, as modeled in Section III-C (bottom).\nTable I summarizes the main features of CrazyS, AirSim,\nFlightmare, and gym-pybullet-drones.\nIII. METHODS\nTo explain how gym-pybullet-drones works, one\nneeds to understand (i) how its dynamics evolve (Sub-\nsections III-A to III-C), (ii) which types of observations,\nincluding vision and multi-agent ones, can be extracted\nfrom it (Subsection III-D), (iii) what commands one can\nissue (Subsection III-E), and (iv) which learning and control\nworkﬂows we built on top of it (Subsections III-F and III-G).\nA. Gym Environment Classes\nOpenAI’s Gym toolkit was introduced to standardize the\ndevelopment of RL problems and algorithms in Python. As\nin a standard Markov decision process (MDP) framework, a\ngeneric environment class receives an action, uses it to update\n(step) its internal state, and returns a new state (observation)\npaired with a corresponding reward. This, of course, is\nakin to a simple feedback loop in which a controller feeds\nan input signal to a plant to receive a (possibly noisy)\noutput measurement. An OpenAI Gym environment also\nexposes additional information about whether its latest state\nis terminal (done) and other standard APIs to (i) reset it\nbetween episodes and (ii) query it about the domains (spaces)\nof its actions and observations.\nB. Bullet Physics\nPhysics engines are particularly appealing to researchers\nworking on both robotics and ML because they (i) expedite\nthe development and test of new applications for the former,\nwhile (ii) yielding large data sets for the latter [4]. The work\nin this paper is based on the open-source Bullet Physics\nengine [10]. Our choice was motivated by its collision\nmanagement system, the availability of both CPU- and GPU-\nbased rendering, the compatibility with the Uniﬁed Robot\nDescription Format (URDF), and its steppable physics—\nallowing to extract synchronized rendering and kinematics,\nas well as to control, at arbitrary frequencies.\nor\nHandwritten\nController\nPID, MPC, . . .\nTrained\nRL Policy\nPPO, A2C, . ..\ngym-pybullet-drones\nN ×\nMulti-agent Environment\nIntegrated PID (optional)\nBullet\nPhysics\nCollisions, Drag, Ground\nEffect, Downwash, . ..\naction:\nCommanded RPMs,\nDesired Velocities\nobs:\nKinematics,\nRGB, Depth, Segm.\nreward\ndone\ninfo\nFig. 3.\nSchematics of the handed over input parameters (action) and\nyielded return values (obs, reward, done, and info) by every call to the\nstep method of a gym-pybullet-drones environment.\nC. Quadcopter Dynamics\nWe use PyBullet to model the forces and torques acting on\neach quadcopter in our Gym and leverage the physics engine\nto compute and update the kinematics of all vehicles.\n1) Quadcopter models: The default quadcopter model in\ngym-pybullet-drones is the Bitcraze Crazyﬂie 2.x. Its\npopularity and availability meant we could leverage both\na wealth of system identiﬁcation work [22]–[24] and real-\nworld experiments to pick the parameters used in this section.\nIts arm length L, mass m, inertial properties J, physical\nconstants, and a convex collision shape are described through\nseparate URDF ﬁles for the × and + conﬁgurations (see\nFigure 2). We also provide the URDF for a generic, larger\nquadcopter based on the Hummingbird described in [25].\n2) PyBullet-based Physics Update: First, PyBullet let\nus set the gravity acceleration g and the physics stepping\nfrequency (which can be much ﬁner grained than the control\nfrequency at which the Gym steps). Besides the physical\nproperties and constants, PyBullet uses the URDF informa-\ntion to load a CAD model of the quadcopter. The forces Fi’s\napplied to each of the 4 motors and the torque T induced\naround the drone’s z-axis are proportional to the squared\nmotor speeds Pi’s in RPMs. These are linearly related to\nthe input PWMs and we assume we can control them near-\ninstantaneously [25]:\nFi = kF · P 2\ni ,\nT =\nX3\ni=0(−1)i+1kT · P 2\ni ,\n(1)\nwhere kF and kT are constants.\na) Explicit Python Dynamics Update: As an alternative\nimplementation, we also provide an explicit Python update\nthat is not based on Bullet. This can be used for comparison,\ndebugging, or the development of ad hoc dynamics [7]. In\nthis case, the linear acceleration in the global frame ¨x and\nchange in the turn rates in the local frame ˙ψ are computed\nas follows:\n¨x =\n\u0010\nR · [0, 0, kF\nP3\ni=0P 2\ni ] −[0, 0, mg]\n\u0011\nm−1,\n(2)\n˙ψ = J−1 \u0000κ×(L, kF , kT , [P 2\n0 , P 2\n1 , P 2\n2 , P 2\n3 ]) −ψ × (Jψ)\n\u0001\n,\n(3)\nwhere R is the rotation matrix of the quadcopter and κ×,\nκ+ are functions to compute the torques induced in the local\nframe by the motor speeds, for the × and + conﬁguration.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.85\n0.90\n0.95\n1.00\n1.00\nz (m)\nMotor Thrust / Actual Thrust\nExperimental\nFig. 4.\nThe motor thrust-to-actual thrust ratio corresponds to the percentage\nof thrust actually provided by the motors as a quadcopter hovers at different\naltitudes in z. We used the experimental data in blue to ﬁt the coefﬁcient\nkG in (5), yielding the ground effect proﬁle plotted in red.\n3) Aerodynamic Effects: While the model presented in (1)\ncaptures simple quadcopter dynamics, ﬂying in a medium,\nin the proximity of the ground, or near other vehicles can\nresult in additional aerodynamic effects (Figure 2). PyBullet\nallows to model these separately and use them jointly.\na) Drag: The spinning propellers of a quadcopter pro-\nduce drag D, a force acting in the direction opposite to\nthe one of motion. Our modeling is based on [23] and it\nstates that the air resistance is proportional to the quadcopter\nvelocity ˙x, the angular velocities of the rotors, and a matrix\nof coefﬁcients kD experimentally derived in [23]:\nD = −kD\n\u0012X3\ni=0\n2πPi\n60\n\u0013\n˙x.\n(4)\nb) Ground Effect: When hovering at a very low alti-\ntude, a quadcopter is subject to an increased thrust caused\nby the interaction of the propellers’ airﬂow with the surface,\ni.e., the ground effect. Based on [26] and real-world exper-\niments with Crazyﬂie hardware (see Figure 4), we model\ncontributions Gi’s for each motor that are proportional to\nthe propellers’ radius rP , speeds Pi’s, altitudes hi’s, and a\nconstant kG:\nGi = kGkF\n\u0012 rP\n4hi\n\u00132\nP 2\ni .\n(5)\nc) Downwash: When two quadcopters cross paths at\ndifferent altitudes, the downwash effect causes a reduction\nin the lift of the bottom one. For simplicity, we model it\nas a single contribution applied to the center of mass of the\nquadcopter whose module W depends on the distances in x,\ny, and z between the two vehicles (δx, δy, δz) and constants\nkD1, kD2, kD3 that we identiﬁed experimentally:\nW = kD1\n\u0012 rP\n4δz\n\u00132\nexp\n\n\n−1\n2\n\n\nq\nδ2x + δ2y\nkD2δz + kD3\n\n\n2\n\n.\n(6)\nFigure 9 compares a ﬂight simulation using this model with\ndata from a real-world ﬂight experiment.\nD. Observation Spaces\nEvery time we advance gym-pybullet-drones by one\nstep—which might include multiple steps of the physics\nengine—we receive an observation vector. In our code base,\nwe provide several implementations. Yet, they all include the\nfollowing kinematic information: a dictionary whose keys\nare drone indices n ∈[0..N] and values contain positions\nxn = [x, y, z]n’s, quaternions qn’s, rolls rn, pitches pn, and\nyaws jn’s, linear ˙xn, and angular velocities ωn’s, as well as\nthe motors’ speeds [P0, P1, P2, P3]n’s for all vehicles.\n{n : [xn, qn, rn, pn, jn, ˙xn, ωn, [P0, P1, P2, P3]n]}.\n(7)\n1) Adjacency Matrix of Multi-Robot Systems: In net-\nworked multi-robot systems, it is convenient to express the\nnotion of neighboring robots (those within a certain radius\nR) through adjacency matrix A ∈RN×N Aij. In this type\nof observation, the value of each drone’s key also includes\nthe drone’s corresponding row (of Booleans) in A:\n{n : {state : [. . . ], neighbors : [An0, . . . , AnN]}}.\n(8)\n2) Vision and Rendering: Furthermore, leveraging PyBul-\nlet’s bindings to TinyRenderer/OpenGL3 and inspired by\nBitcraze’s AI-deck, gym-pybullet-drones observations\ncan include video frames in each drone’s perspective (to-\nwards the positive direction of the local x-axis) for the RGB\n(Cn ∈R64×48×4), depth, and segmentation (Un, On ∈\nR64×48) views (Figure 5).\n{n : {. . . , rgb : Cn, dep : Un, seg : On}}.\n(9)\nE. Action Spaces\nAdvancing a gym-pybullet-drones environment by\none step requires to pass an action (or control input) to it.\nFor the sake of ﬂexibility, and acknowledging that different\nrobotic applications (e.g., stabilization vs. path planning)\nrequire different levels of abstractions, we provide multiple\nimplementations.\n1) Propellers’\nRPMs:\nThe\ndefault\naction\nspace\nof\ngym-pybullet-drones is a dictionary whose keys are\nthe drone indices n ∈[0..N] and the values contain the\ncorresponding 4 motor speeds, in RPMs, for each drone:\n{n : [P0, P1, P2, P3]n}.\n(10)\n2) Desired Velocity Input: Alternatively, drones can be\ncontrolled through a dictionary of desired velocity vectors,\nin the following format:\n{n : [vx, vy, vz, vM]n},\n(11)\nwhere vx, vy, vz are the components of a unit vector and\nvM is the desired velocity’s magnitude. In this case, the\ntranslation of the input into PWMs and motor speeds is\ndelegated to a PID controller comprising of position and\nattitude control subroutines [27].\n3) Other Control Modes: Developing additional RL and\nMARL applications will likely require to tweak and cus-\ntomize observation and action spaces. The modular structure\nof gym-pybullet-drones is meant to facilitate this. In\nSection V, we provide learning examples based on one-\ndimensional action spaces. The inputs of class DynAviary\nare the desired thrust and torques—from which it derives\nfeasible RPMs using non-negative least squares.\nF. Learning Workﬂows\nHaving understood how gym-pybullet-drones’s dy-\nnamics and observations/actions spaces work, using it in an\nRL workﬂow only requires a few more steps.\nRGB View\nDepth View\nSegmentation View\nFig. 5.\nRGB C, depth U, and segmentation O obs in (9). A Crazyﬂie 2.x can be given image processing capabilities by the AI-deck.\nTABLE II\nCPU AND GPU SPEED-UPS AS A FUNCTION OF THE NUMBER VEHICLES,\nENVIRONMENTS, AND THE USE OF VISION-BASED OBSERVATIONS\n# of Drones\n# of Env’s\nVision\nTinyRenderer‡\nOpenGL3∥\n1.0\n1.0\nNo\n16.8×\n15.5×\n1.0\n1.0\nYes\n1.3×\n10.8×\n5.0\n1.0\nYes\n0.2×\n2.5×\n10.0\n1.0\nNo\n2.3×\n2.1×\n80.0\n4.0\nNo\n0.95×\n0.8×\n‡ 2020 MacBook Pro (CPU: i7-1068NG7)\n∥Lenovo P52 (CPU: i7-8850H; GPU: Quadro P2000)\n1) Reward Functions and Episode Termination: Each step\nof an environment should return a reward value (or a dic-\ntionary of them, for multiple agents). Reward functions are\nvery much task-dependent and one must be implemented. As\nshown in Section V, it can be as simple as a squared distance.\nGym’s done and info return values are optional but can be\nused, e.g., to implement additional safety requirements [16].\n2) Stable Baselines3 Workﬂow: We provide a complete\ntraining workﬂow for single agent RL based on Stable\nBaselines3 [11]. This is a collection of RL algorithms—\nincluding A2C, DDPG, PPO, SAC, and TD3—implemented\nin PyTorch. As it supports both MLP and CNN policies,\nStable Baselines3 can be used with either kinematics or\nvision-based observations. In Section V, we show how to\nrun a training example and replay its best performing policy.\n3) RLlib Workﬂow: We also provide an example training\nworkﬂow for multi-agent RL based on RLlib [12]. RLlib is a\nlibrary built on top of Ray’s API for distributed applications,\nwhich includes TensorFlow and PyTorch implementations of\nmany popular RL (e.g., PPO, DDPG, DQN) and MARL (e.g.,\nMADDPG, QMIX) algorithms. In Section V, we show how\nto run a 2-agent centralized critic training example and replay\nits best performing policies.\nG. ROS2 Wrapper Node\nFinally, because of the signiﬁcance of ROS for the robotics\ncommunity, we also implemented a minimalist wrapper\nfor gym-pybullet-drones’s environments using a ROS2\nPython node. This node continuously steps an environment\nwhile (i) publishing its observations on a topic and (ii)\nreading actions from a separate topic it subscribed to.\nIV. COMPUTATIONAL PERFORMANCE\nTo demonstrate our proposal, we ﬁrst analyze its compu-\ntational efﬁciency. Being able to collect large data sets—\nthrough parallel execution and running faster than the wall-\nclock—is of particular importance for the development of re-\ninforcement learning applications. We chose to adopt Gym’s\nPython API [5], while leveraging Bullet’s C++ back end [10],\nto strike a balance between readability and portability, on one\nside, and computational performance, on the other.\nAs we believe that closed-loop performance is the better\ngauge of a simulation at work, we used a stripped-down\nversion of the PID control [27] example in Figure 6 to\ngenerate the data presented in Table II. The script—with no\nGUI, no debug information, fewer obstacles, and less front\nend reporting between physics steps—is available in folder:\ngym-pybullet-drones/experiments/performance/.\nUnlike simulations that rely on game engines like Unreal\nEngine 4 and Unity [7], [8], PyBullet has less demanding\nrendering requirements and can run with either the CPU-\nbased TinyRenderer or OpenGL3 when GPU acceleration\nis available. We collected the results in Table II using two\nseparate laptop workstations: (i) a 2020 MacBook Pro with\nan Intel i7-1068NG7 CPU and (ii) a Lenovo P52 with an\nIntel i7-8850H CPU and an Nvidia Quadro P2000 GPU.\nFor a single drone with physics updates at 240Hz, we\nachieved speed-ups of over 15× the wall-clock. Exploiting\nparallelism (i.e., multiple vehicles in multiple environments),\nwe could generate 80× the data of the elapsed time. This\nis slightly slower, but comparable, to Flightmare’s dynamics\nopen-loop performance. Although on simpler scenes, a visual\nobservations throughput of ∼750kB/s with TinyRenderer\nis also comparable to Flightmare, and 10× faster when\nOpenGL3 acceleration is available.\nV. EXAMPLES\nIn\npractice,\nwe\nshow\nhow\none\ncan\njointly\nuse\ngym-pybullet-drones with both control approaches and\nreinforcement learning algorithms. We do so through a set\nof six examples. All of our source code is available online\nand it can be installed using the following steps (please refer\nto the repository for a full list of requirements):\n$ git clone -b paper \\\ngit@github.com:utiasDSL/gym-pybullet-drones.git\n$ cd gym-pybullet-drones/\n$ pip3 install -e .\nA. Control\nThe ﬁrst four examples demonstrate how to command\nmultiple quadcopters using motor speeds or desired velocity\n−0.3\n0.0\n0.3\nx (m)\n−0.2\n0.0\n0.2\n˙x (m/s)\n−0.6\n−0.3\n0.0\ny (m)\n−0.2\n0.0\n0.2\n˙y (m/s)\n0.1\n0.2\nz (m)\n−4.0\n−2.0\n0.0\n·10−2\n˙z (m/s)\n0.0\n2.0\n4.0 ·10−2\nr (rad)\n−0.1\n0.1\nωx (rad/s)\n−4.0\n−1.0\n2.0 ·10−2\np (rad)\n−0.1\n0.1\nωy (rad/s)\n0.0\n0.5\n1.0\nj (rad)\n−0.1\n0.1\nωz (rad/s)\n1.44\n1.45\n1.46 ·104\nP0 (RPM)\n1.44\n1.45\n1.46 ·104\nP1 (RPM)\n0\n3\n6\n9\n12\n1.44\n1.45\n1.46 ·104\nt (s)\nP2 (RPM)\n0\n3\n6\n9\n12\n1.44\n1.45\n1.46 ·104\nt (s)\nP3 (RPM)\nCF0\nCF1\nCF2\nCF3\nFig. 6.\nPositions in x, y, z, linear velocities ˙x, ˙y, ˙z, roll r, pitch p, yaw j,\nangular velocities ω, and motors’ speeds P0, P1, P2, P3 of four Crazyﬂies\nCF0, CF1, CF2, CF3 tracking a circular trajectory, at different altitudes\nz’s, with different yaws j’s, via external PID control.\ncontrol inputs as well as two of the aerodynamic effects\ndiscussed in Section III: ground effect and downwash.\n$ cd gym-pybullet-drones/examples/\n1) Trajectory tracking with PID Control: The ﬁrst ex-\nample includes 4 Crazyﬂies in the × conﬁguration, using\nPyBullet’s physics update (1), and external PID control. The\ncontrollers receive the kinematics observations (7) and return\ncommanded motors’ speeds (10).\n$ python3 fly.py --num_drones 4\nFigure 6 plots position x, velocity ˙x = [ ˙x, ˙y, ˙z], roll r,\npitch p, yaw j, angular velocity ω, and motors’ speeds\n[P0, P1, P2, P3] for all vehicles, during a 12 seconds ﬂight\nalong a circular trajectory (the three top-left subplots).\n2) Desired Velocity Input: The second example also uses\nkinematics observations (7) but it is controlled by desired\nvelocity inputs (11). These are targeted by PID controllers\nembedded within the environment.\n$ python3 velocity.py --duration_sec 12\nFigure 7 shows the linear velocity response to step-wise\nchanges in the velocity inputs (the three top-right subplots).\n0.0\n0.5\n1.0\nx (m)\n−0.1\n0.2\n˙x (m/s)\n−2.0\n0.0\n2.0\ny (m)\n−0.4\n0.0\n0.4\n˙y (m/s)\n0.0\n0.5\n1.0\nz (m)\n−0.1\n0.0\n0.1\n˙z (m/s)\n−0.1\n0.2\nr (rad)\n−1.0\n1.0\nωx (rad/s)\n−0.1\n0.1\np (rad)\n−1.0\n1.0\nωy (rad/s)\n0.0\n1.5\nj (rad)\n−0.2\n0.2\nωz (rad/s)\n1.30\n1.40\n1.50\n·104\nP0 (RPM)\n1.30\n1.40\n1.50\n·104\nP1 (RPM)\n0\n3\n6\n9\n12\n1.30\n1.40\n1.50\n·104\nt (s)\nP2 (RPM)\n0\n3\n6\n9\n12\n1.30\n1.40\n1.50\n·104\nt (s)\nP3 (RPM)\nCF0\nCF1\nCF2\nCF3\nFig. 7.\nPositions in x, y, z, linear velocities ˙x, ˙y, ˙z, roll r, pitch p, yaw j,\nangular velocities ω, and motors’ speeds P0, P1, P2, P3 of four Crazyﬂies\nCF0, CF1, CF2, CF3 tracking step-wise desired velocity inputs via PID\ncontrol embedded within the Gym environment.\n3) Ground Effect: The third example compares the take-\noff of a Crazyﬂie with and without the ground effect contri-\nbution (5), using the coefﬁcient identiﬁed in Figure 4.\n$ python3 groundeffect.py\nFigure 8 compares positions and velocities along the global\nz-axis, during the ﬁrst half-second of simulation time, show-\ning a small but noticeable overshooting and the larger max-\nimum velocity of the drone experiencing the ground effect.\n4) Downwash: The last control example subjects two\nCrazyﬂies—moving in opposite directions along sinusoidal\ntrajectories in x and different altitudes of 0.5 and 1 meter—to\nthe downwash model in (6).\n$ python3 downwash.py --gui False\nFigure 9 compares the simulation results with the experi-\nmental data collected to identify the parameters used in (6).\nFigure 9 shows a very close match in the x and z positions\nbetween our simulation and the real-world.\nAs we would expect, however, our simpliﬁed single con-\ntribution modeling does not fully capture the impact of\ndownwash on the bottom drone—e.g., in the pitch p and\nits ramiﬁcations on velocity ˙x.\n0.0\n0.1\n0.2\nz (m)\n0.0\n0.2\n0.4\n˙z (m/s)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n1.2\n1.6\n2.0 ·104\nt (s)\nP0,1,2,3 (RPM)\nNo Ground Effect\nGround Effect\nFig. 8.\nPosition in z, linear velocity ˙z, and motors’ speeds P0 = P1 =\nP2 = P3 of a simulated Crazyﬂie taking-off with (red) and without (blue)\nthe modeled ground effect contribution in (5).\n−0.5\n0.0\n0.5\nx (m)\n−1.0\n0.0\n1.0\n˙x (m/s)\n0.4\n0.7\n1.0\nz (m)\n−0.4\n−0.1\n0.2\n˙z (m/s)\n−0.4\n0.0\n0.4\np (rad)\n−0.4\n0.0\n0.4\nωy (rad/s)\n0\n5\n10\n1.20\n1.60\n2.00 ·104\nt (s)\nP0,3 (RPM)\n0\n5\n10\n1.20\n1.60\n2.00 ·104\nt (s)\nP1,2 (RPM)\nCF0\nCF1\nExp0\nExp1\nFig. 9.\nPositions in x and z, linear velocities ˙x, ˙z, pitch p, angular velocity\nωy, and motors’ speeds P0 = P3, P1 = P2 of two Crazyﬂies CF0 (blue),\nCF1 (red) subject to the downwash model in (6), compared to the ﬂight\nlogs of a real-world experiment with two drones (grey and teal lines).\nB. Reinforcement Learning\nThe last two examples let one or more quadcopters learn\npolicies to reach a target altitude and hover in place. These\nare based on normalized kinematics observations (7) and a\nnormalized, one-dimensional RPMs action space (10).\n$ cd gym-pybullet-drones/experiments/learning/\n1) Single Agent Take-off and Hover: For a single agent,\nthe goal is to reach a predetermined altitude and stabilize.\nThe reward function is simply the negation of the squared\nEuclidean distance from the set point:\nr = −∥[0, 0, 1] −x∥2\n2.\n(12)\nWe use the default implementations of three popular RL\nalgorithms (PPO, A2C, and SAC) provided in Stable Base-\nlines3 [11]. We do not tune any of the hyperparameters.\nWe choose MLP models with ReLU activation and 4 hidden\nlayers with 512, 512, 256, and 128 units, respectively. For\nPPO, the training workﬂow can be executed as follows:\n0.2\n0.6\n1\n1.4\n1.8\n2.2\n2.6\n3\n·104\n−2\n−1\n0 ·102\n# timesteps\nEpisode Mean Reward\n0.0\n0.5\n1.0\nz (m)\n0.0\n0.2\n0.4\n˙z (m/s)\n0\n1\n2\n3\n4\n5\n6\n1.4\n1.5\n·104\nt (s)\nP0,1,2,3 (RPM)\nA2C\nPPO\nSAC\nFig. 10.\nAlgorithm’s learning curve (top) and best policy’s position in\nz, linear velocity ˙z, and motors’ speeds P0 = P1 = P2 = P3 (bottom)\nfor three single agent RL implementations from Stable Baselines3—A2C,\nPPO, and SAC—using the reward function in (12).\n$ python3 singleagent.py --algo ppo\nTo replay the best trained model, execute the following script:\n$ python3 test_singleagent.py --exp ./results/save\n-<env>-<algo>-<obs>-<act>-<time_date>\nFigure 10 compares (i) the three algorithms’ learning and\n(ii) the trained policies performance. While SAC performs\nbest, all algorithms succeed albeit with very different learn-\ning curves. These were not unexpected, as we deliberately\nomitted any parameter tuning to avoid cherry-picking.\n2) Multi-agent Leader-follower: The last example is a\nMARL problem in which a leader agent is trained as in (12)\nand a follower is rewarded by tracking its altitude:\nr0 = −∥[0, 0, 0.5] −x0∥2\n2,\nr1 = −0.5(z1 −z0)2. (13)\nThe workﬂow is built on top of RLlib [12] using a central\ncritic with 25 inputs and two action models with 12 inputs,\nall having two hidden layers of size 256 and tanh activations.\n$ python3 multiagent.py --act one_d_rpm\nTo replay the best trained model, execute the following script:\n$ python3 test_multiagent.py --exp ./results/save-<\nenv>-2-cc-<obs>-<act>-<time_date>\nFigure 11 shows a stable training leading to successfully\ntrained policies. The leader presents minor oscillations that\nare, expectedly, reﬂected by the follower. The RPMs com-\nmanded by these policies, however, appear to be erratic.\nVI. EXTENSIONS\nWe developed gym-pybullet-drones to provide a com-\npact and comprehensive set of features to kick-start RL\nin quadcopter control. Yet, we also structured its code\nbase for extensibility. Some of the enhancements in the\nworks include: (i) the support for heterogeneous quadcopter\nteams—this can be achieved by importing multiple URDF\n0.2\n0.4\n0.6\n0.8\n1.0\n·105\n−1.2\n−0.6\n0.0 ·102\n# timesteps\nEpisode Mean Reward\n0.0\n0.3\n0.6\nz (m)\n0.0\n0.1\n0.2\n˙z (m/s)\n0\n1\n2\n3\n4\n5\n6\n1.2\n1.5\n1.8 ·104\nt (s)\nP0,1,2,3 (RPM)\nCF0\nCF1\nFig. 11.\nLearning curve (top) and best policies’ positions in z, linear\nvelocities ˙z, and motors’ speeds P0 = P1 = P2 = P3 (bottom) for a 2-\nagent MARL implementation using RLlib and the reward functions in (13).\nﬁles, where the inertial properties are stored; (ii) the de-\nvelopment of more sophisticated aerodynamic effects—e.g.,\na downwash model made of multiple components instead\nof a single contribution applied to the center of mass; (iii)\nthe inclusion of symbolic dynamics—e.g., using CasADi\nto expose an analytical model that could be leveraged by\nmodel predictive control approaches; (iv) new workﬂows to\nsupport additional MARL frameworks beyond RLlib [12]—\ne.g., PyMARL; and ﬁnally, (v) Google Colaboratory support\nand Jupyter Notebook examples—to facilitate adoption by\nthose with limited access to computing resources.\nVII. CONCLUSIONS\nIn this paper, we presented an open-source, OpenAI Gym-\nlike [5] multi-quadcopter simulator written in Python on top\nof the Bullet Physics engine [10]. When compared to similar\nexisting tools, the distinguishing and innovative features of\nour proposal include (i) a more modular and sophisticated\nphysics implementation, (ii) vision-based Gym’s observa-\ntions, and (iii) a multi-agent reinforcement learning interface.\nWe showed how gym-pybullet-drones can be used for\nlow- and high-level control through trajectory tracking and\ntarget velocity input examples. We also demonstrated the\nuse of our work in separate workﬂows for single and multi-\nagent RL, based on state-of-the-art learning libraries [11],\n[12]. We believe our work will contribute to bridging the gap\nbetween reinforcement learning and control research, helping\nthe community to develop realistic MARL applications for\naerial robotics.\nACKNOWLEDGMENTS\nWe acknowledge the support of Mitacs’s Elevate Fellow-\nship program and General Dynamics Land Systems-Canada\n(GDLS-C)’s Innovation Cell. We also thank the Vector\nInstitute for providing access to its computing resources.\nREFERENCES\n[1] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi,\nZ. D. Guo, and C. Blundell, “Agent57: Outperforming the Atari human\nbenchmark,” in Proceedings of the 37th International Conference on\nMachine Learning, vol. 119.\nPMLR, 13–18 Jul 2020, pp. 507–517.\n[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\nThe kitti dataset,” International Journal of Robotics Research, 2013.\n[3] B. Recht, “A tour of reinforcement learning: The view from contin-\nuous control,” Annual Review of Control, Robotics, and Autonomous\nSystems, vol. 2, no. 1, pp. 253–279, 2019.\n[4] C. K. Liu and D. Negrut, “The role of physics-based simulators\nin robotics,” Annual Review of Control, Robotics, and Autonomous\nSystems, vol. 4, no. 1, 2021.\n[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” 2016.\n[6] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and\nD. Meger, “Deep reinforcement learning that matters,” in AAAI, 2018,\npp. 3207–3214.\n[7] Y. Song, S. Naji, E. Kaufmann, A. Loquercio, and D. Scaramuzza,\n“Flightmare: A ﬂexible quadrotor simulator,” in Proceedings of the\n4th Conference on Robot Learning.\nPMLR, 2020.\n[8] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity\nvisual and physical simulation for autonomous vehicles,” in Field and\nService Robotics.\nSpringer Int’l Publishing, 2018, pp. 621–635.\n[9] G. Silano and L. Iannelli, CrazyS: A Software-in-the-Loop Simulation\nPlatform for the Crazyﬂie 2.0 Nano-Quadcopter.\nSpringer Int’l\nPublishing, 2020, pp. 81–115.\n[10] E. Coumans and Y. Bai, “Pybullet, a python module for physics\nsimulation for games, robotics and machine learning,” 2016–2019.\n[11] A. Rafﬁn, A. Hill, M. Ernestus, A. Gleave, A. Kanervisto, and\nN. Dormann, “Stable baselines3,” 2019.\n[12] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg,\nJ. Gonzalez, M. Jordan, and I. Stoica, “RLlib: Abstractions for\ndistributed reinforcement learning,” in Proceedings of the 35th Inter-\nnational Conference on Machine Learning, vol. 80.\nPMLR, 10–15\nJul 2018, pp. 3053–3062.\n[13] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine\nfor model-based control,” in IEEE/RSJ International Conference on\nIntelligent Robots and Systems, 2012, pp. 5026–5033.\n[14] Y. Tassa, S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez,\nJ. Merel, T. Erez, T. Lillicrap, and N. Heess, “dm control: Software\nand tasks for continuous control,” 2020.\n[15] M. Chevalier-Boisvert, L. Willems, and S. Pal, “Minimalistic grid-\nworld environment for openai gym,” 2018.\n[16] A. Ray, J. Achiam, and D. Amodei, “Benchmarking Safe Exploration\nin Deep Reinforcement Learning,” 2019.\n[17] G. Dulac-Arnold, N. Levine, D. J. Mankowitz, J. Li, C. Paduraru,\nS. Gowal, and T. Hester, “An empirical investigation of the challenges\nof real-world reinforcement learning,” 2020.\n[18] W. Koch, R. Mancuso, R. West, and A. Bestavros, “Reinforcement\nlearning for uav attitude control,” ACM Transactions on Cyber-\nPhysical Systems, vol. 3, no. 2, p. 22, 2019.\n[19] A. Molchanov, T. Chen, W. H¨onig, J. A. Preiss, N. Ayanian, and G. S.\nSukhatme, “Sim-to-(multi)-real: Transfer of low-level robust control\npolicies to multiple quadrotors,” in IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2019, pp. 59–66.\n[20] F. Furrer, M. Burri, M. Achtelik, and R. Siegwart, Robot Operating\nSystem (ROS): The Complete Reference (Volume 1).\nSpringer Int’l\nPublishing, 2016, ch. RotorS—A Modular Gazebo MAV Simulator\nFramework, pp. 595–625.\n[21] S. Krishnan, B. Borojerdian, W. Fu, A. Faust, and V. J. Reddi, “Air\nlearning: An ai research platform for algorithm-hardware benchmark-\ning of autonomous aerial robots,” 2019.\n[22] C. Luis and J. Le Ny, “Design of a trajectory tracking controller for\na nanoquadcopter,” Polytechnique Montreal, Tech. Rep., 2016.\n[23] J. F¨orster, “ETH Zurich,” Master’s thesis, System Identiﬁcation of the\nCrazyﬂie 2.0 Nano Quadrocopter, 2015.\n[24] B. Landry, “Planning and control for quadrotor ﬂight through cluttered\nenvironments,” Master’s thesis, MIT, 2015.\n[25] C. Powers, D. Mellinger, and V. Kumar, Quadrotor Kinematics and\nDynamics.\nSpringer Netherlands, 2015, pp. 307–328.\n[26] G. Shi, X. Shi, M. O’Connell, R. Yu, K. Azizzadenesheli, A. Anand-\nkumar, Y. Yue, and S. Chung, “Neural lander: Stable drone landing\ncontrol using learned dynamics,” in International Conference on\nRobotics and Automation, 2019, pp. 9784–9790.\n[27] D. Mellinger and V. Kumar, “Minimum snap trajectory generation and\ncontrol for quadrotors,” in International Conference on Robotics and\nAutomation, 2011, pp. 2520–2525.\n",
  "categories": [
    "cs.RO",
    "cs.LG",
    "I.2.6; I.2.9"
  ],
  "published": "2021-03-03",
  "updated": "2021-07-25"
}