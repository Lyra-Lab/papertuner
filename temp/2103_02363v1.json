{
  "id": "http://arxiv.org/abs/2103.02363v1",
  "title": "Reinforcement Learning with External Knowledge by using Logical Neural Networks",
  "authors": [
    "Daiki Kimura",
    "Subhajit Chaudhury",
    "Akifumi Wachi",
    "Ryosuke Kohita",
    "Asim Munawar",
    "Michiaki Tatsubori",
    "Alexander Gray"
  ],
  "abstract": "Conventional deep reinforcement learning methods are sample-inefficient and\nusually require a large number of training trials before convergence. Since\nsuch methods operate on an unconstrained action set, they can lead to useless\nactions. A recent neuro-symbolic framework called the Logical Neural Networks\n(LNNs) can simultaneously provide key-properties of both neural networks and\nsymbolic logic. The LNNs functions as an end-to-end differentiable network that\nminimizes a novel contradiction loss to learn interpretable rules. In this\npaper, we utilize LNNs to define an inference graph using basic logical\noperations, such as AND and NOT, for faster convergence in reinforcement\nlearning. Specifically, we propose an integrated method that enables model-free\nreinforcement learning from external knowledge sources in an LNNs-based logical\nconstrained framework such as action shielding and guide. Our results\nempirically demonstrate that our method converges faster compared to a\nmodel-free reinforcement learning method that doesn't have such logical\nconstraints.",
  "text": "Reinforcement Learning with External Knowledge\nby using Logical Neural Networks\nDaiki Kimura , Subhajit Chaudhury , Akifumi Wachi , Ryosuke Kohita ,\nAsim Munawar , Michiaki Tatsubori , Alexander Gray\nIBM Research AI\ndaiki@jp.ibm.com\nAbstract\nConventional deep reinforcement learning methods\nare sample-inefﬁcient and usually require a large\nnumber of training trials before convergence. Since\nsuch methods operate on an unconstrained action\nset, they can lead to useless actions.\nA recent\nneuro-symbolic framework called the Logical Neu-\nral Networks (LNNs) can simultaneously provide\nkey-properties of both neural networks and sym-\nbolic logic.\nThe LNNs functions as an end-to-\nend differentiable network that minimizes a novel\ncontradiction loss to learn interpretable rules. In\nthis paper, we utilize LNNs to deﬁne an infer-\nence graph using basic logical operations, such as\nAND and NOT, for faster convergence in reinforce-\nment learning. Speciﬁcally, we propose an inte-\ngrated method that enables model-free reinforce-\nment learning from external knowledge sources\nin an LNNs-based logical constrained framework\nsuch as action shielding and guide.\nOur results\nempirically demonstrate that our method converges\nfaster compared to a model-free reinforcement\nlearning method that doesn’t have such logical con-\nstraints.\n1\nIntroduction\nDeep reinforcement learning methods have been success-\nfully applied to many applications, particularly computer\ngame, text-based game, and robot control applications [Mnih\net al., 2015; Narasimhan et al., 2015; Silver et al., 2017;\nKimura, 2018; Pathak et al., 2017; Yuan et al., 2018; Kimura\net al., 2018; Chaudhury et al., 2018]. Such methods require\na large number of training trials for converging to an optimal\naction policy. By default, due to a lack of external constraints,\nthey cannot avoid unsafe or useless actions. If an agent re-\nceives the proper action list (recommendations for action), it\ncan reduce the number of training trials. We believe we can\nprepare such an action list from external action constraints\npertaining to the environment. Another option is safe rein-\nforcement learning [Garcıa and Fern´andez, 2015], which can\navoid taking unsafe and useless actions via the action con-\nstraints.\nTo deﬁne such action constrains, there are various tech-\nniques in reinforcement learning that use symbolic logi-\ncal functions or graphs [Hasanbeig et al., 2018; Hasanbeig\net al., 2020].\nHowever, these techniques require all rules\nto be set manually, which is time-consuming.\nA recent\nneuro-symbolic framework called the Logical Neural Net-\nworks (LNNs) [Riegel et al., 2020] simultaneously provides\nkey-properties of both neural networks (learning) and sym-\nbolic logic (reasoning). It can train the constraints and rules\nwith logical functions in the neural networks, and since ev-\nery neuron in the LNNs has a component for a formula of\nweighted real-valued logics, it can calculate the probability\nand contradiction loss for each of the propositions. At the\nsame time, trained LNNs follow symbolic rules, which means\nthey yield a highly interpretable disentangled representation.\nIn this paper, we deﬁne the external knowledge for the re-\ninforcement learning within this LNNs structure, and then\nleverage the knowledge in the LNNs.\nWe propose an integrated reinforcement learning method\nwith external knowledge for action shielding (avoiding use-\nless action) and guiding (giving an action recommendation)\nthat is deﬁned in the LNNs. The performance of our proposed\nmethod is experimentally compared with a baseline method\nthat doesn’t use external knowledge. Our main contributions\nare (1) the proposal of an integrated method that uses logi-\ncal guides for fast training and safe reinforcement learning\nvia the trainable logical network and (2) the experimental\ndemonstration for the effectiveness of external knowledge in\nreinforcement learning by using LNNs.\n2\nProposed Method\nIn this paper, we proposed two methods: safe reinforcement\nlearning by LNNs (LNNs-Shielding) and action-guided rein-\nforcement learning by LNNs (LNNs-Guide). Figure 1 shows\nthe architectures of reinforcement learning with these meth-\nods, we provide a detailed explanation of each in the follow-\ning subsections. Since we apply these methods in a text-based\ngame environment that is speciﬁcally Coin-collector game of\nTextWorld [Cˆot´e et al., 2018], our explanations include some\nexamples from on the game.\nIn Coin-collector game, the\nagent needs to ﬁnd and take a coin in series of connected\nrooms and take the coin. Hence, the agent needs to travel\nthough various rooms while seeking the coin.\narXiv:2103.02363v1  [cs.AI]  3 Mar 2021\nLNNs\nRL\n𝑠!\n𝑎!\n𝑟𝑒𝑡𝑟𝑦_𝑠𝑖𝑔𝑛𝑎𝑙if 𝑎! = 𝑢𝑛𝑠𝑎𝑓𝑒\n𝑎!\nSemantic parser\n𝜙(𝑠!)\nLNNs-Shielding\n𝑎!, 𝑐𝑡𝑟𝑑(𝑎!, 𝑠!)\n𝑟!\ni. Reinforcement learning with LNNs-Shielding\nEnv\nii. Reinforcement learning with LNNs-Guide\nLNNs\nRL\n𝑠!\n𝑎!\nSemantic parser\n𝜙(𝑠!)\nLNNs-Guide\n𝑟!\nEnv\n∀𝑎∈𝐴, 𝑃\"##(𝑎|𝑠!)\nFigure 1: Architecture of reinforcement learning with proposed\nmethods.\n2.1\nLNNs-Shielding\nLNNs-Shielding avoids unsafe or useless actions that are\ndeﬁned by logical conditions in the LNNs for reinforce-\nment learning.\nFor example, the agent had better not to\ntake the action of “go to west room”, if it has already\nvisited the west room.\nAction shielding needs to be rep-\nresented in logical operations, hence the logical function\nfor this example is “visited west room” ∧“found\nwest room” ⇒¬ “go to west room” (∧: AND oper-\nator, ⇒: IMPLY operator, ¬: NOT operator). This shielding\nmeans that even if the agent has found the west room, it will\nnot go to the west room if the west room has already been vis-\nited. In the LNNs, the agent ﬁrst checks the current state val-\nues, then if it has already visited the west room, the agent set a\ntrue value for proposition of “visited west room”, that\nmeans “visited west room” = true. If the agent ﬁnd\nexit for the west room, it also set a true value for “found\nwest room”. Then the LNNs have a logical function for\nthis proposition, which is “found west room” ⇒“go\nto west room”.\nAt same time, the agent inputs current state values to\nRL method to obtain the action.\nIf the RL method out-\nputs “go to west room” as a selected action, the agent\nset a true value for a proposition of “go to west room”\naction in LNNs.\nIf the agent visited and found the\nwest room (“visited west room” = “found west\nroom” = true) and RL method outputs the “go to west\nroom” as a selected action, the “go to west room”\nproposition will observe a contradiction in the proposition.\nBecause the proposition was set true value from RL method\nby the selected action candidate, it was also set false from log-\nical function (“found west room” ⇒“go to west\nroom”). We assume such action restrictions will help lead\nto faster convergence in reinforcement learning.\nLet φ(st) be a logical propositional state input for cur-\nrent state st from a semantic parser algorithm.\nWe\ncan obtain these logical state values from raw text de-\nscriptions via the semantic parser such that the statement\n“found west room” is true when the state descrip-\ntion has “There is an unguarded exit to the\nwest.” 1. Let ⟨lowerni,st, upperni,st⟩= LNN(ni|φ(st))\nbe lower and upper bound values from the LNNs for a\nnode ni and input φ(st). All neurons in the LNNs return\npairs of values in the range [0, 1] representing lower and up-\nper bounds on the truth values of their corresponding subfor-\nmulae and propositions [Riegel et al., 2020]. These values\nare updated using an inference function from given propo-\nsitional inputs. Note that the weight and bias values in the\nconnections are updated during the back-propagation opera-\ntions. Normally, the upper bound is higher than the lower\nbound. However, if a neuron is observed to be contradicting\nthe logical rules, the lower bound will be higher than the up-\nper bound. Therefore, the contradiction value ctrd(at, st) for\nthe node ni is deﬁned as\nctrd(at, st) =\nX\nnj∈to(at)\nmax(0, lowernj,st −uppernj,st),\n(1)\nwhere at represents a node for an action value at t time\nstep from the model-free reinforcement learning method, and\nto(at) is all nodes connected to node at.\nIn reinforcement learning, the agent calculates this con-\ntradiction value from a given action at and state st from\nthe model-free LSTM-DQN++ [Yuan et al., 2018] reinforce-\nment learning method. The proposed LNNs-Shielding dis-\ntinguishes whether the given action is safe (useful) or un-\nsafe (useless) from the inference result in the LNNs. The\naction at will be discriminated as a safe action if the contra-\ndiction value ctrd(at, st) is α or higher. The action at will\nbe discriminated as an unsafe action if the condition value is\nless than α. When the action at is a safe action, the agent\nexecutes action at. Alternatively, when it is an unsafe ac-\ntion, the LNNs-Shielding returns the action at to the rein-\nforcement learning module, and the module then calculates\nthe next candidate for proper action. Note that the contra-\ndiction value of this next candidate will also be checked by\nthe LNNs-Shielding. The action policy training from the re-\nward signal is then performed by the reinforcement learning\nmethod.\n2.2\nLNNs-Guide\nLNNs-Guide recommends the suitable actions that are de-\nﬁned by the logical functions in LNNs for reinforcement\nlearning. For example, if the LNNs are trained similar rules\nto those in the previous LNNs-Shielding example, the LNNs-\nGuide can give a negative recommendation (similar to the\n1The semantic parser is not our focus in this paper.\nshielding) for the visited room. At the same time, the LNNs-\nGuide can recommend taking a “go to west room” ac-\ntion when the agent has found the west room, which is a pos-\nitive recommendation. For this example, we implement this\nrule on various logical operations such as “found west\nroom” ⇒“go to west room”. LNNs-Guide, therefore,\ncan additionally give positive recommendations compared to\nLNNs-Shielding. We assume such an action recommendation\nwill help lead to faster convergence in reinforcement learning,\nand it is more effective than the LNNs-Shielding since it also\nhas the positive recommendation function.\nLet φ(st), lowerni,st, upperni,st, ctrd(at, st) be the same\nas the deﬁnitions in the LNNs-Shielding method. The LNNs-\nGuide provides probabilities for recommendation of the ac-\ntion for each state input. The probability is calculated by\nPLNN(a|st) =\nev(a,st)\nP\naj∈A ev(aj,st) ,\n(2)\nv(a, st) = lowera,st + uppera,st\n2\n−ctrd(a, st),\n(3)\nwhere a is a targeted action for calculating the probability,\nand A is all actions. Value v(a, st) represents the level of\ntruth values for the propositions while discounting the con-\ntradiction value.\nIn reinforcement learning, the policy follows the probabil-\nities calculated with the epsilon greedy algorithm in LSTM-\nDQN++ [Yuan et al., 2018]. In this work, we select the ac-\ntion at by\nat =\n\u001a\nargmaxa∈APLNN(a|st) Q(st, a)\n: ζ ≥ϵ\nrandoma∈APLNN(a|st)\n: otherwise ,\n(4)\nwhere ζ is the current random value for epsilon greedy, and\nrandoma∈AP takes an action in accordance with probabili-\nties P. In this equation, an action is selected on the basis of\nq-value and action probabilities from LNNs-Guide. Training\nsteps by the reward signal are performed in the same way as\nin the reinforcement learning method.\n3\nEvaluation\n3.1\nExperiments\nWe evaluated the performance of the proposed method\nthrough experiments conducted in the TextWorld environ-\nment [Cˆot´e et al., 2018]. Our target was the Coin-collector\ngame. We built LNNs that has useful knowledge based on\nexternal data. We ﬁt the prepared data to the following rules\nand then trained the LNNs.\n• ¬ “visited all connected rooms” ∧“no\ncoin in east room” ⇒¬ “go east”\n• “found east room” ⇒“go east”\n• ¬ “visited all connected rooms” ∧“no\ncoin in west room” ⇒¬ “go west”\n• “found west room” ⇒“go west”\n0\n20\n40\n60\n80\n100\n120\n140\nTraining Episode\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nReward\nLSTM-DQN++\nLNNs-Shielding\nLNNs-Guide\nFigure 2: Reward [0-1] curves for proposed and baseline methods\nwith moving average (N = 5). Shaded area represents the standard\ndeviation value.\n• ¬ “visited all connected rooms” ∧“no\ncoin in south room” ⇒¬ “go south”\n• “found south room” ⇒“go south”\n• ¬ “visited all connected rooms” ∧“no\ncoin in north room” ⇒¬ “go north”\n• “found north room” ⇒“go north”\n• “found coin in the room” ⇒“take coin”\nThe reason we have ¬ “visited all connected\nrooms” as a rule is that the agent might have any preferred\naction at the dead-end of a path. The agent can go back to the\nvisited room with this proposition when it is at the dead-end.\nWe prepared the LSTM-DQN++ [Yuan et al., 2018] method\nas a baseline method and tested it along with the proposed\nmethods (LNNs-Shielding, LNNs-Guide). We set α = 1 for\nLNNs-Shielding.\n3.2\nResults\nFigure 2 shows the reward curves from the proposed and\nbaseline methods.\nThe LNNs-Shielding avoided previ-\nously visited rooms thanks to the rules in LNNs, such\nas “visited all connected rooms” ∧“no coin\nin east room” ⇒¬ “go east”, so it had a better\nlearning convergence than the baseline method. The LNNs-\nGuide converged the training extremely fast thanks to the ef-\nfect of the positive recommendation, such as “found east\nroom” ⇒“go east”.\nThe reason LNNs-Shielding was\nweaker than LNNs-Guide is that, while LNNs-Shielding only\nprohibited an action when the given action had a high contra-\ndiction value, the LNNs-Guide provided action recommen-\ndations at every time step. This result is in line with our ex-\npectation and leads us to conclude that LNNs-Guide is the\nsuperior method for utilizing external knowledge. However,\nwe also feel that LNNs-Guide may produce weaker results\ndue to incorrect or fuzzy rules in the LNNs. To alleviate such\nconcerns, we believe the interpretability of the rules in LNNs,\nwhich is the key beneﬁt of LNNs, would be helpful to conﬁrm\nthe correctness of the trained rules.\n4\nConclusion\nIn this work, we have proposed a method that utilizes external\nknowledge represented in trainable logical neural networks\nand demonstrated through experiments that is has better con-\nvergence compared to a baseline method. For future work, we\nplan to apply this method to other complex games and train\nthe policy directly in logical neural networks.\nReferences\n[Chaudhury et al., 2018] Subhajit\nChaudhury,\nDaiki\nKimura, Tu-Hoa Pham, Asim Munawar, and Ryuki\nTachibana. Video imitation gan: Learning control policies\nby imitating raw videos using generative adversarial\nreward estimation, 10 2018.\n[Cˆot´e et al., 2018] Marc-Alexandre\nCˆot´e,\n´Akos\nK´ad´ar,\nXingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine,\nJames Moore, Matthew Hausknecht, Layla El Asri, Mah-\nmoud Adada, et al. Textworld: A learning environment\nfor text-based games. In Workshop on Computer Games,\npages 41–75. Springer, 2018.\n[Garcıa and Fern´andez, 2015] Javier Garcıa and Fernando\nFern´andez.\nA comprehensive survey on safe reinforce-\nment learning.\nJournal of Machine Learning Research,\n16(1):1437–1480, 2015.\n[Hasanbeig et al., 2018] Mohammadhosein\nHasanbeig,\nAlessandro Abate, and Daniel Kroening.\nLogically-\nconstrained reinforcement learning.\narXiv preprint\narXiv:1801.08099, 2018.\n[Hasanbeig et al., 2020] Mohammadhosein\nHasanbeig,\nAlessandro Abate, and Daniel Kroening.\nCautious\nreinforcement learning with logical constraints.\narXiv\npreprint arXiv:2002.12156, 2020.\n[Kimura et al., 2018] Daiki Kimura, Subhajit Chaudhury,\nRyuki Tachibana, and Sakyasingha Dasgupta.\nInternal\nmodel from observations for reward shaping. 2018.\n[Kimura, 2018] Daiki Kimura.\nDaqn: Deep auto-encoder\nand q-network. arXiv preprint arXiv:1806.00630, 2018.\n[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin A. Riedmiller, Andreas Fid-\njeland, Georg Ostrovski, Stig Petersen, Charles Beattie,\nAmir Sadik, Ioannis Antonoglou, Helen King, Dharshan\nKumaran, Daan Wierstra, Shane Legg, and Demis Has-\nsabis.\nHuman-level control through deep reinforcement\nlearning. Nature, 518:529–533, 2015.\n[Narasimhan et al., 2015] Karthik Narasimhan, Tejas Kulka-\nrni, and Regina Barzilay. Language understanding for text-\nbased games using deep reinforcement learning. Associa-\ntion for Computational Linguistics, 2015.\n[Pathak et al., 2017] Deepak\nPathak,\nPulkit\nAgrawal,\nAlexei A. Efros, and Trevor Darrell.\nCuriosity-driven\nexploration by self-supervised prediction. In ICML, 2017.\n[Riegel et al., 2020] Ryan Riegel, Alexander Gray, Francois\nLuus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus\nAkhalwaya, Haifeng Qian, Ronald Fagin, Francisco Bara-\nhona, Udit Sharma, Shajith Ikbal, Hima Karanam, Sumit\nNeelam, Ankita Likhyani, and Santosh Srivastava. Logical\nneural networks, 2020.\n[Silver et al., 2017] David Silver, Thomas Hubert, Julian\nSchrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,\nThore Graepel, Timothy Lillicrap, Karen Simonyan, and\nDemis Hassabis. Mastering chess and shogi by self-play\nwith a general reinforcement learning algorithm. 2017.\n[Yuan et al., 2018] X. Yuan, Marc-Alexandre Cˆot´e, Alessan-\ndro Sordoni, R. Laroche, Remi Tachet des Combes,\nMatthew J. Hausknecht, and Adam Trischler. Counting\nto explore and generalize in text-based games.\nArXiv,\nabs/1806.11525, 2018.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2021-03-03",
  "updated": "2021-03-03"
}