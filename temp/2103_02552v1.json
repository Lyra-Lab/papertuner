{
  "id": "http://arxiv.org/abs/2103.02552v1",
  "title": "Multi-Channel and Multi-Microphone Acoustic Echo Cancellation Using A Deep Learning Based Approach",
  "authors": [
    "Hao Zhang",
    "DeLiang Wang"
  ],
  "abstract": "Building on the deep learning based acoustic echo cancellation (AEC) in the\nsingle-loudspeaker (single-channel) and single-microphone setup, this paper\ninvestigates multi-channel AEC (MCAEC) and multi-microphone AEC (MMAEC). We\ntrain a deep neural network (DNN) to predict the near-end speech from\nmicrophone signals with far-end signals used as additional information. We find\nthat the deep learning approach avoids the non-uniqueness problem in\ntraditional MCAEC algorithms. For the AEC setup with multiple microphones,\nrather than employing AEC for each microphone, a single DNN is trained to\nachieve echo removal for all microphones. Also, combining deep learning based\nAEC with deep learning based beamforming further improves the system\nperformance. Experimental results show the effectiveness of both bidirectional\nlong short-term memory (BLSTM) and convolutional recurrent network (CRN) based\nmethods for MCAEC and MMAEC. Furthermore, deep learning based methods are\ncapable of removing echo and noise simultaneously and work well in the presence\nof nonlinear distortions.",
  "text": "MULTI-CHANNEL AND MULTI-MICROPHONE ACOUSTIC ECHO CANCELLATION\nUSING A DEEP LEARNING BASED APPROACH\nHao Zhang1, DeLiang Wang1,2\n1Department of Computer Science and Engineering, The Ohio State University, USA\n2Center for Cognitive and Brain Sciences, The Ohio State University, USA\nABSTRACT\nBuilding on the deep learning based acoustic echo cancella-\ntion (AEC) in the single-loudspeaker (single-channel) and single-\nmicrophone setup, this paper investigates multi-channel AEC\n(MCAEC) and multi-microphone AEC (MMAEC). We train a deep\nneural network (DNN) to predict the near-end speech from micro-\nphone signals with far-end signals used as additional information.\nWe ï¬nd that the deep learning approach avoids the non-uniqueness\nproblem in traditional MCAEC algorithms.\nFor the AEC setup\nwith multiple microphones, rather than employing AEC for each\nmicrophone, a single DNN is trained to achieve echo removal for\nall microphones. Also, combining deep learning based AEC with\ndeep learning based beamforming further improves the system per-\nformance.\nExperimental results show the effectiveness of both\nbidirectional long short-term memory (BLSTM) and convolutional\nrecurrent network (CRN) based methods for MCAEC and MMAEC.\nFurthermore, deep learning based methods are capable of removing\necho and noise simultaneously and work well in the presence of\nnonlinear distortions.\nIndex Termsâ€” Acoustic echo cancellation, deep learning,\nmulti-channel AEC, multi-microphone AEC, nonlinearity\n1. INTRODUCTION\nAcoustic echo cancellation (AEC) is the task of removing undesired\nechoes that result from the coupling between a loudspeaker and a mi-\ncrophone in a communication system [1]. Modern hands-free com-\nmunication devices are usually equipped with multiple microphones\nand loudspeakers. The availability of additional devices also elevates\nthe need for enhanced sound quality and realism, which can hardly\nbe satisï¬ed with single-channel AEC. Therefore, it is necessary to\ndesign AEC for multiple loudspeakers and/or microphones, which\nleads to the study of MCAEC and MMAEC. MCAEC and MMAEC\npresent additional challenges and opportunities compared to single-\nchannel AEC and have received considerable attention recently.\nMulti-channel AEC refers to the setup with at least two loud-\nspeakers or channels (stereophonic sound). Although conceptually\nsimilar, MCAEC is fundamentally different from single-channel\nAEC and a straightforward generalization of single-channel AEC\ndoes not result in satisfactory performance because of the non-\nuniqueness problem [2].\nThis problem is due to the correlation\nbetween loudspeaker signals. As a result, the convergence of adap-\ntive technique could be degraded and the echo paths cannot be\ndetermined uniquely [2].\nMany methods have been proposed to\ncircumvent this problem [3, 4, 5, 6], among which coherence re-\nduction methods are most commonly used. Such methods, however,\ninevitably degrade sound quality, and a compromise must be made\nbetween enhanced convergence and sound quality corruption [2, 7].\nMMAEC is required for situations in which multiple micro-\nphones are present and beamforming techniques are usually com-\nbined with AEC for efï¬cient reduction of noise and acoustic echoes.\nThe most straightforward ways of combining these two processing\nmodules are applying AEC separately for each microphone signal\nbefore beamforming or applying a single-microphone AEC to the\noutput of a beamformer [8]. In general, the former scheme out-\nperforms the latter one [9]. Other algorithms employ relative echo\ntransfer functions [10, 11] or joint optimization strategies [12, 13]\nto improve the MMAEC performance. However, efï¬cient combina-\ntions of AEC and beamforming are still challenging and many of the\nstrategies exhibit convergence deï¬ciencies [7].\nRecently, deep learning based methods have been proposed for\nsolving AEC problems and have shown to be effective for echo and\nnoise removal, especially in situations with nonlinear distortions\n[14, 15, 16, 17]. On the basis of the deep learning based single-\nchannel AEC approach, we investigate AEC setups with multiple\nloudspeakers and microphones. The BLSTM based and CRN [18]\nbased methods are introduced to address MCAEC and MMAEC\nproblems. Evaluation results show that the proposed methods effec-\ntively remove acoustic echo and background noise in the presence\nof nonlinear distortions.\nThe proposed work has four major advantages over traditional\nmethods. First, although there are multiple acoustic paths in the\nMCAEC and MMAEC setups, the deep learning based approach\ncan naturally address the problem with model training, rather than\nemploying a separate AEC module for each echo path. Second, in-\nstead of estimating echo paths, deep learning based MCAEC works\nby directly estimating near-end speech, which intrinsically avoids\nthe non-uniqueness problem. Third, combining deep learning based\nAEC and deep learning based beamforming elevates AEC perfor-\nmance remarkably. Fourth, deep learning based methods can re-\nmove echo and noise simultaneously in the presence of nonlinear\ndistortions.\nThe remainder of this paper is organized as follows.\nSec-\ntion 2 presents the deep learning based approach for MCAEC and\nMMAEC. Experiments and evaluation results are given in Section\n3. Section 4 concludes the paper.\n2. METHOD DESCRIPTION\n2.1. Deep learning based AEC\nAs is shown in Fig. 1(a), the microphone signal y(n) in the single-\nchannel AEC setup is a mixture of echo d(n), near-end speech s(n),\nand background noise v(n):\ny(n) = d(n) + s(n) + v(n)\n(1)\narXiv:2103.02552v1  [eess.AS]  3 Mar 2021\nNear-End Room\nâ„!(ğ‘›)\nğ‘¥(ğ‘›)\nğ‘ \"(ğ‘›)\nğ‘‘\"(ğ‘›)\nğ‘‘! (ğ‘›)\nğ‘£\"(ğ‘›)\nğ‘¦\"(ğ‘›)\nFar-End Room\nğ‘ Ì‚(ğ‘›)\nMMAEC\nâ„\"(ğ‘›)\nğ‘ !(ğ‘›)\nğ‘£!(ğ‘›)\nâ€¦\nğ‘¦!(ğ‘›)\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nNear-End Room\nâ„#\"(ğ‘›)\nâ„\"\"(ğ‘›)\nğ‘¥#(ğ‘›)\nğ‘¥\"(ğ‘›)\nğ‘ \"(ğ‘›)\nğ‘‘\"\"(ğ‘›)\nğ‘‘#\"(ğ‘›)\nğ‘£\"(ğ‘›)\nğ‘¦\"(ğ‘›)\nFar-End Room\nğ‘”#(ğ‘›)\nğ‘”\"(ğ‘›)\nğ‘ Ì‚\"(ğ‘›)\nMCAEC\nNear-End Room\nâ„(ğ‘›)\nğ‘¥(ğ‘›)\nğ‘ (ğ‘›)\nğ‘‘(ğ‘›)\nğ‘£(ğ‘›)\nğ‘¦(ğ‘›)\nFar-End Room\nğ‘ Ì‚ (ğ‘›)\nAEC\n(a) Single-channel AEC\n(b) Multi-channel (Stereophonic) AEC\n(c) Multi-microphone AEC\nFig. 1. Diagrams of conventional (1) single-channel AEC setup, (2)\nMulti-channel (Stereophonic) AEC setup, and (c) Multi-microphone\nAEC setup.\nBLSTM\nğ‘¦(ğ‘›) ğ‘¥(ğ‘›)\nğ‘Œ!\nğ‘‹!\nğ‘ Ì‚(ğ‘›)\nSMM\nHidden layer 1\nHidden layer 2\nHidden layer 3\nHidden layer 4\nInput layer \nOutput layer\nğ‘†-!\nğ‘¦(ğ‘›) ğ‘¥(ğ‘›)\nğ‘Œ\"\nğ‘Œ#\nğ‘‹\"\nğ‘‹#\nğ‘†-\"\nğ‘†-#\nğ‘ Ì‚(ğ‘›)\nEncoder\nLSTM\nCRN\nSkip connections\nDecoder\n(a)\n(b)\nphase\nFig. 2. Deep learning for AEC: (a) BLSTM based method, (b) CRN\nbased method. Subscripts r, i, and m denote real, imaginary and\nmagnitude spectra of signals, respectively.\nwhere echo is generated by convolving a loudspeaker signal with\na room impulse response (RIR) (h(n)). We formulate AEC as a\nsupervised speech separation problem and the overall approach is to\nestimate the near-end speech from microphone signal with far-end\nsignal used as additional information. The diagrams of deep learning\nbased methods are shown in Fig. 2. The input signals, sampled at 16\nkHz, are windowed into 20 ms frames with a 10-ms overlap between\nconsecutive frames. Then a 320-point short time Fourier transform\n(STFT) is applied to each frame to extract the real, imaginary, and\nmagnitude spectra of signals, which are denoted as âˆ—r, âˆ—i, and âˆ—m,\nrespectively.\n2.1.1. BLSTM based AEC\nA BLSTM is trained to predict the spectral magnitude mask (SMM)\nof near-end speech from the magnitude spectrograms of input signals\n(Ym and Xm), as is shown in Fig. 2(a). The SMM is deï¬ned as:\nSMM(t, f) = min{1, |Sm(t, f)|/|Ym(t, f)|}\n(2)\nwhere |Sm(t, f)| and |Ym(t, f)| denote the spectral magnitudes of\nnear-end speech and microphone signal within a T-F unit at frame t\nand frequency f, respectively. Once the model is trained, the esti-\nmated spectral magnitude of near-end speech Ë†Sm is obtained from\npoint-wise multiplication of Ym and estimated SMM. Then Ë†Sm,\nalong with the phase of microphone signal, is sent to the inverse\nshort time Fourier transform to derive an estimated near-end signal\nË†s(n). A detailed description of the BLSTM based method is given\nin [14].\n2.1.2. CRN based AEC\nThe CRN based method trains the CRN, which is a causal system,\nfor complex spectral mapping [18]. As is shown in Fig. 2(b), it es-\ntimates the real and imaginary spectrograms of near-end speech ( Ë†Sr\nand Ë†Si) from the real and imaginary spectrograms of microphone\nsignal and far-end signal (Yr, Yi, Xr, and Xi). Hence, it is capable\nof enhancing both magnitude and phase responses simultaneously\nand Ë†s(n) resynthesized achieves better speech quality. A detailed\ndescription the CRN architecture is provided in [15].\n2.2. Deep learning for MCAEC\nWithout loss of generality, let us take stereophonic AEC as an ex-\nample to study deep learning based MCAEC. The signal model is\ngiven in Fig. 1(b) where the stereophonic signals, x1(n) and x2(n)\nare transmitted to loudspeakers and then coupled to one of the mi-\ncrophones. The signal picked up by microphone j is composed of\ntwo echo signals d1j(n), d2j(n), near-end speech sj(n), and back-\nground noise vj(n):\nyj(n) = P2\ni=1 dij(n) + sj(n) + vj(n),\nj = 1, 2.\n(3)\nwhere dij(n) = xi(n) âˆ—hij(n), i = 1, 2, hij(n) denotes the echo\npath from loudspeaker i to microphone j, and âˆ—denotes linear con-\nvolution.\nDeep learning based MCAEC works by estimating the target\nsj(n) given yj(n), x1(n), and x2(n) as inputs. Speciï¬cally, for\nBLSMT based MCAEC, we use [Yjm, X1m, X2m] as inputs and\ntrain the BLSTM to estimate the SMM of sj(n). For CRN based\nAEC, we use [Yjr, Yji, X1r, X1i, X2r, X2i] as inputs and train the\nnetwork to estimate [Sjr, Sji]. The training signals are generated by\nrandomly selecting j from {1, 2}, i.e. the model is exposed to sig-\nnals picked up by the two microphones in MCAEC during training.\nA model trained this way is able to achieve echo removal for both\nmicrophones in the system.\n2.3. Deep learning for MMAEC\nConsidering an MMAEC setup with one loudspeaker and M micro-\nphones, as is shown in Fig. 1(b). The signal picked up by micro-\nphone j is\nyj(n) = dj(n) + sj(n) + vj(n),\nj = 1, 2, Â· Â· Â· M\n(4)\nwhere dj(n) = x(n) âˆ—hj(n) is the echo received at microphone j.\nDifferent from traditional MMAEC methods that may need to\nemploy a separate AEC module for each microphone in the array,\ndeep learning based MMAEC can be trained to achieve echo removal\nfor all the microphones in the array with a single DNN. During train-\ning, we use yj(n) and x(n) as inputs and set the training target to\nthe corresponding near-end speech sj(n). The training signals are\ngenerated by randomly choosing j from {1, 2, Â· Â· Â· , M}.\nOnce the model is trained, the outputs of deep learning based\nMMAEC could be used for deep learning based minimum variance\ndistortion-less response (MVDR) beamforming [19]. Choosing the\nï¬rst microphone in the array as reference microphone, the MVDR\nbeamformer can be constructed as:\nË†\nw(f) =\nË†Î¦âˆ’1\nN (f)Ë†c(f)\nË†c(f)H Ë†Î¦âˆ’1\nN (f)Ë†c(f)\n(5)\nwhere (Â·)H denotes conjugate transpose, Ë†Î¦N(f) is the estimated\ncovariance matrix of overall interference (acoustic echo and back-\nground noise), Ë†c(f) is the estimated steering vector, which is esti-\nmated as the principal eigenvector of the estimated speech covari-\nance matrix Ë†Î¦s(f) [19, 20]. The covariance matrices of speech and\noverall interference are estimated from the output of deep learning\nbased MMAEC as\nË†Î¦S(f) = 1\nT\nP\nt Ë†S(t, f) Ë†S\nH(t, f)\n(6)\nË†Î¦N(f) = 1\nT\nP\nt Ë†\nN(t, f) Ë†\nN\nH(t, f)\n(7)\nwhere Ë†S(t, f) is the STFT representation of estimated speech sig-\nnals and Ë†\nN(t, f) is the estimated overall interference obtained as\nY (t, f) âˆ’Ë†S(t, f), T is the total number of frames used in the sum-\nmation.\nThe beamformer is usually applied on microphone signal\nY (t, f) and the enhancement results are calculated from\nYbf(t, f) = Ë†\nwH(f)Y (t, f)\n(8)\nConsidering that MVDR beamformer performs spatial ï¬ltering to\nmaintain signals from the desired direction while suppressing inter-\nferences from other directions. A trick we used is to employ the\nMVDR beamformer as a post-ï¬lter for further enhancement. It can\nbe implemented by feeding the output of DNN based MMAEC for-\nward to the deep learning based beamformer with the latter calcu-\nlated using the same DNN. The overall structure is shown in Fig. 3.\nThe further enhanced output is obtained using\nË†Sbf(t, f) = Ë†\nwH(f) Ë†S(t, f)\n(9)\n3. EXPERIMENTS\n3.1. Experiment setting\nThe simulation setups for evaluation are designed as follows.\nThe near-end and far-end speech signals are generated using the\nTIMIT dataset [21] by following the same way provided in [15].\nğ‘¦\"\nâ€¦\nğ‘¦#\nğ‘¦$\nâ€¦\nâ€¦\nOne DNN\nğ‘ Ì‚#\nDNN-AEC\nDNN-AEC\nDNN-AEC\nAcoustic \nBeamforming\nğ’˜\nğ‘ Ì‚\"\nğ‘ Ì‚$\nğ’”)\nğ’š\nğ‘ Ì‚+,\nFig. 3. Diagram of combining deep learning based AEC with deep\nlearning based beamforming for further enhancement.\nEcho signals are generated by convolving far-end signals with\nRIRs generated using the image method [22].\nTo investigate\nRIRs generalization, we simulate 20 different rooms of size a Ã—\nb Ã— c m (widthÃ—lengthÃ—height) for training mixtures, where\na = [4, 6, 8, 10], b = [5, 7, 9, 11, 13], c = 3. For MCAEC setup,\nthe two microphones and the two loudspeakers are positioned at\n(a, b + 0.05, c) m, (a, b âˆ’0.05, c) m, (a, b + 0.6, c + 0.5) m, and\n(a, b âˆ’0.6, c + 0.5) m, respectively. The near-end speaker is put\nat 20 random positions in each room with 1 meter apart from the\ncenter of the microphones.\nThe setup of MMAEC consists of a\nuniform linear array with four microphones and one loudspeaker.\nThe center of the microphone array is positioned at the center of the\nroom with 4 cm inter-microphone distance. Twenty pairs of posi-\ntions are simulated randomly for the loudspeaker and the near-end\nspeaker in each room, and the distance from the loudspeaker and\nthe near-end speaker to the center of the array are set to 0.6 m and 1\nm, respectively. The reverberation time (T60) is randomly selected\nfrom {0.2, 0.3, 0.4, 0.5, 0.6} s, and the length of RIR is set to 512.\nFor testing, we simulate three rooms of size 3 Ã— 4 Ã— 3 m (Room 1),\n5 Ã— 6 Ã— 3 m (Room 2), 11 Ã— 14 Ã— 3 m (Room3), and set T60 to 0.35\ns to generate test RIRs for both MCAEC and MMAEC setups.\nThe most common nonlinear distortion generated by a loud-\nspeaker is the saturation type nonlinearity, which is usually simu-\nlated using the scaled error function (SEF) [23, 24]:\nfSEF(x) =\nZ x\n0\ne\nâˆ’z2\n2Î·2 dz,\n(10)\nwhere x is the input to the loudspeaker, Î·2 represents the strength\nof nonlinearity. The SEF becomes linear as Î·2 tends to inï¬nity and\nbecomes a hard limiter as Î·2 tends to zero. To investigate the ro-\nbustness of the proposed method against nonlinear distortions, four\nloudspeaker functions are used during the training stage: Î·2 = 0.1\n(severe nonlinearity), Î·2 = 1 (moderate nonlinearity), Î·2 = 10 (soft\nnonlinearity), and Î·2 = âˆ(linear).\nBabble noise from NOISEX-92 dataset [25] is used as the back-\nground noise and the algorithm proposed in [26] is employed to\nmake the noise diffuse. The diffuse babble noise is then split into\ntwo parts, the ï¬rst 80% of it is used for training and the remaining is\nused for testing.\nWe create 20000 training mixtures and 100 test mixtures for\nboth MCAEC and MMAEC setups. Each training mixture is cre-\nated by ï¬rst convolving a loudspeaker signal (generated using ran-\ndomly selected far-end signal and loudspeaker function) with a ran-\ndomly chosen training RIR for loudspeaker to generate an echo. A\nrandomly chosen near-end utterance is convolved with an RIR for\nnear-end speaker and then mixed with the echo at a signal-to-echo\nratio (SER) randomly chosen from {âˆ’6, âˆ’3, 0, 3, 6} dB. Finally, the\ndiffuse babble noise is added to the mixture at a signal-to-noise ra-\ntio (SNR) randomly chosen from {8, 10, 12, 14} dB. The SER and\nSNR, which are evaluated during double-talk periods, are deï¬ned as:\nSER = 10 log10\n\u0002P\nn s2(n)/ P\nn d2(n)\n\u0003\n(11)\nTable 1.\nPerformance of MCAEC methods in the presence of\ndouble-talk, background noise with 3.5 dB SER, 10 dB SNR, Î·2 =\nâˆ(linear system).\nMicrophone 1\nERLE\nPESQ\nRIRs\nRoom1\nRoom2\nRoom3\nRoom1\nRoom2\nRoom3\nUnprocessed\n-\n-\n-\n2.12\n2.13\n2.17\nSJONLMS\n7.54\n7.63\n7.62\n2.41\n2.45\n2.47\nSJONLMS-PF\n19.04\n18.88\n18.67\n2.45\n2.48\n2.53\nBLSTM\n43.19\n67.17\n67.54\n2.57\n2.67\n2.76\nCRN\n32.68\n35.34\n41.31\n2.62\n2.83\n2.98\nMicrophone 2\nERLE\nPESQ\nRIRs\nRoom1\nRoom2\nRoom3\nRoom1\nRoom2\nRoom3\nUnprocessed\n-\n-\n-\n2.11\n2.14\n2.18\nSJONLMS\n7.51\n7.76\n7.63\n2.41\n2.45\n2.45\nSJONLMS-PF\n18.98\n18.89\n18.60\n2.45\n2.50\n2.50\nBLSTM\n49.12\n67.47\n67.67\n2.55\n2.68\n2.76\nCRN\n31.25\n36.61\n41.57\n2.60\n2.85\n2.99\nSNR = 10 log10\n\u0002P\nn s2(n)/ P\nn v2(n)\n\u0003\n(12)\nTest mixtures are created similarly but using different utterances,\nnoises, RIRs, SERs and SNRs.\nThe AMSGrad optimizer [27] and the mean squared error\n(MSE) cost function are used to train both BLSTM based and CRN\nbased methods.\nThe networks are trained for 30 epochs with a\nlearning rate of 0.001. The performance of MCAEC and MMAEC\nis evaluated in terms of utterance-level echo return loss enhance-\nment (ERLE) [1] for single-talk periods and perceptual evaluation\nof speech quality (PESQ) [28] for double-talk periods.\n3.2. Performance of MCAEC methods\nWe ï¬rst evaluate the performance of deep learning based MCAEC.\nThe proposed methods are compared with the stereophonic version\nof joint-optimized normalized least mean square algorithm [29]\nequipped with a coherence reduction technique proposed in [30]\n(SJONLMS). And post-ï¬ltering (PF) [31] is employed to further\nsuppress noises and residual echo (SJONLMS-PF). The parame-\nters for these methods are set accordingly to the values given in\n[29, 30, 31]. The comparison results are given in Table 1. In gen-\neral, the proposed BLSTM based and CRN based MCAEC methods\noutperform conventional methods and the performance generalizes\nwell to untrained RIRs.\nBLSTM based method achieves better\nERLE results while the CRN based method outperforms all other\nmethods in terms of PESQ.\n3.3. Performance of MMAEC methods\nThis part studies the performance of deep learning based MMAEC.\nWe employ single-channel JONLMS [29] for each microphone in\nthe array as a baseline and then combine the outputs with the ideal\nMVDR beamformer (JONLMS-IBF). The ideal MVDR beamformer\n(IBF) is calculated by substituting the true speech and interference\ncomponents of the microphone signal (S(t, f) and N(t, f)) into\n(6), (7), and (5). Therefore, it can be regarded as a stronger base-\nline compared to other MVDR beamformers. Three results are pro-\nvided for each deep learning based method, in which Ë†s is the output\nof the reference microphone, ybf and Ë†sbf are, respectively, the time-\ndomain beamformed microphone signal and beamformed enhanced\nsignal introduced in Section 2.3. The comparison results are given\nin Table 2. As can be seen from the table, deep learning based meth-\nods outperform traditional MMAEC methods in terms of ERLE and\nPESQ. Single-channel outputs of deep learning based methods (Ë†s)\nTable 2.\nPerformance of MMAEC methods in the presence of\ndouble-talk, background noise with 3.5 dB SER, 10 dB SNR, Î·2 =\nâˆ(linear system).\nERLE\nPESQ\nRIRs\nRoom1\nRoom2\nRoom3\nRoom1\nRoom2\nRoom3\nUnprocessed\n-\n-\n-\n2.04\n2.09\n2.10\nJONLMS\n6.94\n6.95\n6.93\n2.43\n2.45\n2.48\nJONLMS-IBF\n17.61\n16.76\n15.52\n2.70\n2.63\n2.66\nË†s\n62.03\n65.65\n66.84\n2.57\n2.71\n2.74\nybf\n2.65\n5.15\n2.06\n2.17\n2.39\n2.20\nBLSTM\nË†sbf\n60.34\n67.22\n66.11\n2.68\n2.87\n2.76\nË†s\n25.92\n32.94\n33.99\n2.66\n2.89\n2.94\nybf\n2.77\n5.48\n2.24\n2.18\n2.41\n2.21\nCRN\nË†sbf\n27.57\n36.92\n34.11\n2.75\n2.98\n2.89\nTable 3. Performance of deep learning based MCAEC and MMAEC\nin the presence of double-talk, background noise and nonlinear dis-\ntortions with Room2, 3.5 dB SER, 10 dB SNR, Î·2 = 0.1, Î·2 = 0.5.\nERLE\nPESQ\nNonlinearity\nÎ·2 = 0.1\nÎ·2 = 0.5\nÎ·2 = 0.1\nÎ·2 = 0.5\nMCAEC\n(Room2\nMicrophone 1)\nUnprocessed\n-\n-\n2.11\n2.13\nBLSTM\n67.32\n67.76\n2.67\n2.67\nCRN\n34.86\n34.72\n2.82\n2.83\nMMAEC\n(Room2)\nUnprocessed\n-\n-\n2.08\n2.08\nË†s\n65.81\n64.93\n2.70\n2.70\nybf\n5.11\n5.13\n2.38\n2.38\nBLSTM\nË†sbf\n67.18\n66.55\n2.87\n2.86\nË†s\n33.15\n33.05\n2.89\n2.88\nybf\n5.49\n5.46\n2.40\n2.40\nCRN\nË†sbf\n36.84\n36.83\n2.99\n2.99\nare good enough for echo and noise removal while combining deep\nlearning based beamformer as a post-ï¬lter (Ë†sbf) further improves the\noverall performance in most of the cases.\n3.4. Performance in situations with nonlinear distortions\nThis part tests the performance of BLSTM based and CRN based\nMCAEC and MMAEC in situations with nonlinear distortions intro-\nduced by loudspeaker. The results are shown in Table 3. Comparing\nthe results with those without nonlinear distortions shows that the\ndeep learning based methods can be trained to handle cases with and\nwithout nonlinear distortions and the performance generalizes well\nto untrained nonlinearity (Î·2 = 0.5).\n4. CONCLUSION\nWe have proposed a deep learning approach to MCAEC and\nMMAEC. Our approach overcomes the limitations of traditional\nmethods and produces remarkable performance in terms of ERLE\nand PESQ. Evaluation results show the effectiveness of both BLSTM\nand CRN based methods for removing echo and noise in cases with\nand without nonlinear distortions, and the performance generalizes\nwell to untrained RIRs. Moreover, the proposed methods can be\nextended to handle a general AEC setup with an arbitrary number of\nmicrophones and an arbitrary number of loudspeakers, which will\nbe demonstrated in future research.\n5. ACKNOWLEDGEMENTS\nThis research was supported in part by an NIDCD grant (R01\nDC012048) and the Ohio Supercomputer Center.\n6. REFERENCES\n[1] G. Enzner, H. Buchner, A. Favrot, and F. Kuech, â€œAcoustic\necho control,â€ in Academic press library in signal processing:\nimage, video processing and analysis, hardware, audio, acous-\ntic and speech Processing. Academic Press, 2014.\n[2] M. M. Sondhi, D. R. Morgan, and J. L. Hall, â€œStereophonic\nacoustic echo cancellation-an overview of the fundamental\nproblem,â€ IEEE Signal processing letters, vol. 2, no. 8, pp.\n148â€“151, 1995.\n[3] J. Benesty, F. Amand, A. Gilloire, and Y. Grenier, â€œAdaptive\nï¬ltering algorithms for stereophonic acoustic echo cancella-\ntion,â€ in 1995 ICASSP. IEEE, 1995, vol. 5, pp. 3099â€“3102.\n[4] S. Shimauchi, S. Makino, and J. Kojima,\nâ€œMethod and ap-\nparatus for multi-channel acoustic echo cancellation,â€ Aug. 26\n1997, US Patent 5,661,813.\n[5] M. Schneider and W. Kellermann, â€œMultichannel acoustic echo\ncancellation in the wave domain with increased robustness to\nnonuniqueness,â€ IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, vol. 24, no. 3, pp. 518â€“529, 2016.\n[6] J. Franzen and T. Fingscheidt, â€œAn efï¬cient residual echo sup-\npression for multi-channel acoustic echo cancellation based\non the frequency-domain adaptive Kalman ï¬lter,â€\nin 2018\nICASSP. IEEE, 2018, pp. 226â€“230.\n[7] M. Luis Valero, â€œAcoustic echo reduction for multiple loud-\nspeakers and microphones: Complexity reduction and conver-\ngence enhancement,â€ 2019.\n[8] W. Kellermann, â€œStrategies for combining acoustic echo can-\ncellation and adaptive beamforming microphone arrays,â€\nin\n1997 ICASSP. IEEE, 1997, vol. 1, pp. 219â€“222.\n[9] W. Herbordt and W. Kellermann, â€œLimits for generalized side-\nlobe cancellers with embedded acoustic echo cancellation,â€ in\n2001 ICASSP. IEEE, 2001, vol. 5, pp. 3241â€“3244.\n[10] G. Reuven, S. Gannot, and I. Cohen, â€œJoint noise reduction\nand acoustic echo cancellation using the transfer-function gen-\neralized sidelobe canceller,â€ Speech communication, vol. 49,\nno. 7-8, pp. 623â€“635, 2007.\n[11] M. L. Valero and E. A. Habets, â€œMulti-microphone acoustic\necho cancellation using relative echo transfer functions,â€ in\n2017 WASPAA. IEEE, 2017, pp. 229â€“233.\n[12] W. Herbordt, W. Kellermann, and S. Nakamura, â€œJoint opti-\nmization of LCMV beamforming and acoustic echo cancella-\ntion,â€ in 2004 12th European Signal Processing Conference.\nIEEE, 2004, pp. 2003â€“2006.\n[13] W. Herbordt and W. Kellermann, â€œGsaecâ€”acoustic echo can-\ncellation embedded into the generalized sidelobe canceller,â€\nin 2000 10th European Signal Processing Conference. IEEE,\n2000, pp. 1â€“4.\n[14] H. Zhang and D. L. Wang, â€œDeep learning for acoustic echo\ncancellation in noisy and double-talk scenarios,â€ in 2018 IN-\nTERSPEECH, 2018, pp. 3239â€“3243.\n[15] H. Zhang, K. Tan, and D. L. Wang, â€œDeep learning for joint\nacoustic echo and noise cancellation with nonlinear distor-\ntions.,â€ in 2019 INTERSPEECH, 2019, pp. 4255â€“4259.\n[16] G. Carbajal, R. Serizel, E. Vincent, and E. Humbert, â€œJoint\nDNN-based multichannel reduction of acoustic echo, reverber-\nation and noise,â€ arXiv preprint arXiv:1911.08934, 2019.\n[17] K. Sridhar, R. Cutler, A. Saabas, T. Parnamaa, H. Gamper,\nS. Braun, R. Aichner, and S. Srinivasan, â€œICASSP 2021 acous-\ntic echo cancellation challenge: Datasets and testing frame-\nwork,â€ arXiv preprint arXiv:2009.04972, 2020.\n[18] K. Tan and D. L. Wang,\nâ€œA convolutional recurrent neural\nnetwork for real-time speech enhancement,â€ in Interspeech,\n2018, pp. 3229â€“3233.\n[19] J. Heymann, L. Drude, A. Chinaev, and R. Haeb-Umbach,\nâ€œBLSTM supported GEV beamformer front-end for the 3rd\nCHiME challenge,â€\nin 2015 IEEE Workshop on Automatic\nSpeech Recognition and Understanding (ASRU). IEEE, 2015,\npp. 444â€“451.\n[20] X. Zhang, Z. Wang, and D. L. Wang, â€œA speech enhancement\nalgorithm by iterating single- and multi-microphone process-\ning and its application to robust ASR,â€ in 2017 ICASSP. IEEE,\n2017, pp. 276â€“280.\n[21] L. F. Lamel, R. H. Kassel, and S. Seneff, â€œSpeech database\ndevelopment: Design and analysis of the acoustic-phonetic\ncorpus,â€\nin Speech Input/Output Assessment and Speech\nDatabases, 1989.\n[22] J. B. Allen and D. A. Berkley, â€œImage method for efï¬ciently\nsimulating small-room acoustics,â€ The Journal of the Acousti-\ncal Society of America, vol. 65, no. 4, pp. 943â€“950, 1979.\n[23] F. Agerkvist, â€œModelling loudspeaker non-linearities,â€ in Au-\ndio Engineering Society Conference: 32nd International Con-\nference: DSP For Loudspeakers. Audio Engineering Society,\n2007.\n[24] H. Zhang and D. L. Wang, â€œA deep learning approach to active\nnoise control,â€ in 2020 INTERSPEECH in press, 2020.\n[25] A. Varga and H. J. Steeneken,\nâ€œAssessment for automatic\nspeech recognition: II. NOISEX-92: A database and an experi-\nment to study the effect of additive noise on speech recognition\nsystems,â€ Speech communication, vol. 12, no. 3, pp. 247â€“251,\n1993.\n[26] E. A. Habets, I. Cohen, and S. Gannot, â€œGenerating nonstation-\nary multisensor signals under a spatial coherence constraint,â€\nThe Journal of the Acoustical Society of America, vol. 124, no.\n5, pp. 2911â€“2917, 2008.\n[27] S. J. Reddi, S. Kale, and S. Kumar, â€œOn the convergence of\nAdam and beyond,â€ arXiv preprint arXiv:1904.09237, 2019.\n[28] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hek-\nstra, â€œPerceptual evaluation of speech quality (PESQ)-a new\nmethod for speech quality assessment of telephone networks\nand codecs,â€ in 2001 ICASSP. IEEE, 2001, vol. 2, pp. 749â€“\n752.\n[29] C. Paleologu, S. CiochinË˜a, J. Benesty, and S. L. Grant, â€œAn\noverview on optimized NLMS algorithms for acoustic echo\ncancellation,â€ EURASIP Journal on Advances in Signal Pro-\ncessing, vol. 2015, no. 1, pp. 97, 2015.\n[30] M. Djendi, â€œAn efï¬cient stabilized fast newton adaptive ï¬l-\ntering algorithm for stereophonic acoustic echo cancellation\nSAEC,â€ Computers & Electrical Engineering, vol. 38, no. 4,\npp. 938â€“952, 2012.\n[31] F. Ykhlef and H. Ykhlef, â€œA post-ï¬lter for acoustic echo can-\ncellation in frequency domain,â€ in Second World Conference\non Complex Systems. IEEE, 2014, pp. 446â€“450.\n",
  "categories": [
    "eess.AS",
    "cs.SD"
  ],
  "published": "2021-03-03",
  "updated": "2021-03-03"
}