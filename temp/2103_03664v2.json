{
  "id": "http://arxiv.org/abs/2103.03664v2",
  "title": "ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation",
  "authors": [
    "Raunak Dey",
    "Yi Hong"
  ],
  "abstract": "We introduce a neural network framework, utilizing adversarial learning to\npartition an image into two cuts, with one cut falling into a reference\ndistribution provided by the user. This concept tackles the task of\nunsupervised anomaly segmentation, which has attracted increasing attention in\nrecent years due to their broad applications in tasks with unlabelled data.\nThis Adversarial-based Selective Cutting network (ASC-Net) bridges the two\ndomains of cluster-based deep learning methods and adversarial-based\nanomaly/novelty detection algorithms. We evaluate this unsupervised learning\nmodel on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and\nMS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN\nfamily, our model demonstrates tremendous performance gains in unsupervised\nanomaly segmentation tasks. Although there is still room to further improve\nperformance compared to supervised learning algorithms, the promising\nexperimental results shed light on building an unsupervised learning algorithm\nusing user-defined knowledge.",
  "text": "ASC-Net: Adversarial-based Selective Network for\nUnsupervised Anomaly Segmentation\nRaunak Dey1 and Yi Hong2\n1 Department of Computer Science, University of Georgia\nraunak.dey@gmail.com\n2 Department of Computer Science and Engineering, Shanghai Jiao Tong University\nyi.hong@sjtu.edu.cn\nAbstract. We introduce a neural network framework, utilizing adversarial learn-\ning to partition an image into two cuts, with one cut falling into a reference\ndistribution provided by the user. This concept tackles the task of unsupervised\nanomaly segmentation, which has attracted increasing attention in recent years\ndue to their broad applications in tasks with unlabelled data. This Adversarial-\nbased Selective Cutting network (ASC-Net) bridges the two domains of cluster-\nbased deep learning methods and adversarial-based anomaly/novelty detection\nalgorithms. We evaluate this unsupervised learning model on BraTS brain tu-\nmor segmentation, LiTS liver lesion segmentation, and MS-SEG2015 segmenta-\ntion tasks. Compared to existing methods like the AnoGAN family, our model\ndemonstrates tremendous performance gains in unsupervised anomaly segmenta-\ntion tasks. Although there is still room to further improve performance compared\nto supervised learning algorithms, the promising experimental results shed light\non building an unsupervised learning algorithm using user-deﬁned knowledge.\n1\nIntroduction\nIn computer vision and medical image analysis, unsupervised image segmentation has\nbeen an active research topic for decades [14,17,19,20,26], due to its potential of apply-\ning to many applications without requiring the data to be manually labelled. Recently,\nadvances in GANs [15] have given rise to a class of anomaly detection algorithms,\nwhich are inspired by AnoGAN [24] to identify abnormal events, behaviors, or regions\nin images or videos [10,13,25]. The AnoGAN learns a manifold of normal images by\nmapping from image space to a latent space based on GANs. To detect the anomaly,\nAnoGAN needs iterative search in the latent space to ﬁnd the closest corresponding\nimages for a query image. The AnoGAN family, including f-AnoGAN [23] and other\nworks [4,5,16,27,28], focus on the reconstruction of the corresponding normal images\nfor a query image, but not directly working on the anomaly detection. As a result, their\nreconstruction quality heavily affects the performance of anomaly detection.\nTo center the focus on the anomaly without needing faithful reconstruction, we\npropose an adversarial-based selective cutting neural network (ASC-Net)3, shown in\nFigure 1. This network aims to decompose an image into two selective cuts based on\n3 Our source code is available on Github: https://github.com/raun1/ASC-NET.\narXiv:2103.03664v2  [cs.CV]  9 Jul 2021\n2\nRaunak Dey and Yi Hong\n32\n64\n128 256 512\n256 128\n64\n32\n256 128\n64\n32\n32\n64\n128\n256\n512\nFlatten\n-1/1 :: fake/real\nMain Module (M)\nDiscriminator (D)\n!\"#\n!$%\n…\n!&%\n!'(\nT\n!')\n*+,\n*-.\n*/.\n*01\n*+2\nFig. 1. Overview of our proposed ASC-Net for unsupervised anomaly segmentation.\na reference image distribution. Typically, the reference distribution is deﬁned by a set\nof images provided by users or experts who have vague knowledge and expectation of\nnormal cases. In this way, one cut will fall into the reference distribution, while other\nimage content outside of the reference image distribution will group into the other cut.\nThese two cuts allow to reconstruct the original input image semantically and perform a\nsimple intensity thresholding to cluster normal and abnormal regions. To consider these\ntwo cuts simultaneously, we extend U-Net [21] with two upsampling branches, as used\nin CompNet [11], a supervised image segmentation approach. Meanwhile, one branch\nconnects to a GAN’s discriminator network, which allows introducing the knowledge\ncontained in the reference image distribution. With the discriminator component aiding,\nthe network can separate images into softly disjoint regions; that is, the generation of\nour selective cuts is under the constraint of the reference image distribution. As a result,\nwe obtain a joint estimation of anomaly and the corresponding normal image, thus\nbypassing the need for perfect reconstruction. Furthermore, under the constraints of the\nGAN discriminator and the reconstruction of the original input, our ASC-Net becomes\nan unsupervised solution for anomaly detection, since we do not have any labels for the\nanomaly, with only a collection of normal images in the reference distribution.\nWe evaluate our proposed unsupervised anomaly segmentation network on three\npublic datasets, i.e., MS-SEG2015 [7], BraTS-2019 [1,2,18], and LiTS [6] datasets.\nFor the MS-SEG2015 dataset, an exhaustive study on comparing multiple existing\nautoencoder-based models, variational-autoencoder-based models, and GAN-based mod-\nels is performed in [3]. Compared to the best Dice scores reported in [3], we have sig-\nniﬁcant gains in performance, which are increased by 23.24% without post-processing\nand 20.40% with post-processing4. For BraTS dataset, our experiments show that f-\n4 Different from that in [3], we use a simple open-and-closed operation for post-processing.\nASC-Net\n3\nAnoGAN, the one performs the best after post-processing in [3], has difﬁculty recon-\nstructing the normal images required for anomaly segmentation. By constrast, we ob-\ntain a mean Dice score of 63.67% for the BraTS brain tumor segmentation and 32.24%\nfor the LiTS liver lesion segmentation, under the two-fold cross-validation settings for\nboth datasets. In addition, we improve the Dice score for the liver lesion segmentaiton\nto 50.23% using a simple post-processing scheme of open and closed sets.\nOverall, the contributions of our proposed method are summarized below:\n– Proposing an adversarial based framework for unsupervised anomaly segmentation,\nwhich bypasses the normal image reconstruction and works on anomaly detection\ndirectly. This framework presents a general clustering strategy to generate two se-\nlective cuts based on a reference image set with human knowledge.\n– To the best of our knowledge, our work is the ﬁrst one to apply an unsupervised\nsegmentation algorithm to the BraTS 2019 and LiTS liver lesion public datasets.\nBesides, our method outperforms the AnoGAN family and other popular methods\npresented in [3] on the publicly available MS-SEG2015 dataset.\n2\nAdversarial-based Selective Cutting Network (ASC-Net)\n2.1\nNetwork Framework\nFigure 1 describes the framework of our proposed ASC-Net, which includes two com-\nponents, i.e., the main module M and the discriminator D, and one simple clustering\nstep T based on thresholding. Overall, the main module includes normal and anomaly\nbranches to semantically reconstruct the original image for clustering, while the dis-\ncriminator brings user-deﬁned knowledge into the normal branch in the main module.\nMain Module M. The main module aims to generate two selective cuts, which guide\na follow-up simple reconstruction of an input image to cluster image pixels based on\nintensity thresholding. The M follows an encoder-decoder architecture like the U-Net,\nincluding one encoder and two decoders. The encoder E extracts features of an input\nimage Iin, which could be an image located within or outside of the reference distri-\nbution {Ird}, a collection of normal images. One decoder in green (the second branch)\nis designed to generate a “fence” cut Cf that is deﬁned by an image fence formed by\n{Ird}. The Cf aims to generate an image Ifc and tries to fool the discriminator D.\nThe other decoder in blue (the ﬁrst branch) is designed to generate another “wild” cut\nCw, which captures leftover image content that is not included in Ifc. As a result, the\nCw produces another images Iwc to complement the fence-cut output Ifc. The comple-\nmentary relation between these two cuts Cf and Cw is enforced by a positive Dice loss\ndiscussed later. Figure 2 demonstrates the “disjoincy” of Ifc and Iwc, like their com-\nplementary histogram distribution and different thresholded images at different peaks.\nThe reconstructor R consists of a 1 × 1 convolution layer with the Sigmoid as the\nactivation function, which is applied on the concatenation of the two-cut outputs Ifc\nand Iwc to regenerate the input image Iin back. This reconstructor R ensures that the\nCf does not generate an image Ifc far from the input image Iin and also ensures that\nthe Cw does not generate an empty image Iwc if the anomaly or novelty exists. Figure 3\nshows the histogram separation of the reconstructed images, compared to the original\n4\nRaunak Dey and Yi Hong\nFig. 2. Visualization of the “disjoincy” between images Ifc (top) and Iwc (bottom) generated by\ntwo cuts of ASC-Net. From left to right: the generated image, its histogram, and the following\nfour columns representing the histogram equalized images of the thresholded peaks with the ﬁrst\npeak being the ﬁrst image, etc. The ﬁrst peak of Ifc is disjoint with the last peak of Iwc, etc.\ninput images which present complex histogram peaks and have difﬁculty in separating\nthe brain tumor from backgorund and other tissues via a simple thresholding. The dis-\ncontinuous histogram distribution of Iro is inherited from the two generated sub-images\nIfc and Iwc through a simple weighted combination. As a result, the segmentation task\nbecomes relatively easy to be done on the reconstructed image Iro.\nFig. 3. Histogram comparison of two sample\nimages. From left to right: the input image, its\nhistogram, its reconstructed image using ASC-\nNet, and the histogram of the reconstructed im-\nage. The histograms of the input images vary\ngreatly, while the ones of their reconstructions\nshow peaks at similar ranges, which enables a\nthresholding based pixel-level separation.\nDiscriminator D. The GAN discrimina-\ntor tries to distinguish the generated im-\nage Ifc, according to a reference distribu-\ntion Rd deﬁned by a set of images {Ird},\nwhich are provided by the user or ex-\nperts. The Rd typically includes images\ncollected from the same group, for in-\nstance, normal brain scans, which share\nsimilar structures and lie on a manifold.\nIntroducing D allows us to incorporate\nour vague prior knowledge about a task\ninto a deep neural network. Typically, it\nis non-trivial to explicitly formulate such\nprior knowledge; however, it could be im-\nplicitly represented by a selected image\nset. The Rd is an essential component that\nmakes our ASC-Net possible to generate\nselective cuts according to the user’s in-\nput, without requiring other supervisions.\nThresholding T. To cluster the reconstructed image Iro into two groups at the pixel\nlevel, we choose the thresholding approach with the threshold values obtained using\nthe histogram of Iro. We observed that for an anomaly that is often brighter than the\nsurrounding tissues like the BraTS brain tumor, the intensity value at the rightmost peak\nof the histogram is a desired threshold; while an opposite case like darker LiTS liver\nlesions, the value at the leftmost peak would be the threshold. We also observed that\nASC-Net\n5\nthe histograms of the reconstructed images for different inputs reﬂect the same cut-off\npoint for the left or right peaks, which allows using one threshold for an entire dataset.\nLoss Functions. The main module M includes three loss functions: (i) the image gen-\neration loss for Cf (LossCf ), (ii) the “disjoincy” loss between Cf and Cw (LossCw),\nand (iii) the reconstruction loss (LossR). In particular, the Cf tries to generate an im-\nage Ifc that fools the discriminator D by minimizing LossCf = 1\nn\nPn\ni=1 |D(I(i)\nfc )−1|.\nHere, n is the number of samples in the training batch. The Cw tries to generate an\nimage Iwc that is complement to Ifc by minimizing the positive Dice score LossCw =\n2|Ifc∩Iwc|\n|Ifc|+|Iwc|. The last reconstruction takes an Mean-Squared-Error (MSE) loss between\nthe input image Iin and the reconstructed image Iro: LossR = 1\nn\nPn\ni=1 ∥I(i)\nin −I(i)\nro ∥2\n2.\nThe discriminator D tries to reject the the Cf output Ifc but accept the images from\nthe reference distribution Rd, by minimizing the following loss function: LossD =\n1\nn+m\n\u0010Pn\ni=1 |D(I(i)\nfc ) −(−1)| + Pm\ni=1 |D(I(i)\nRd) −1|\n\u0011\n. Here, m is the number of the\nimages in Rd. Even though D and Cfc are tied in an adversarial setup, here we do not\nuse the Earth Mover distance [22] in the loss function, since we would like D to iden-\ntify both positive samples and negative samples with equal precision. Therefore, we use\nMean Absolute Error (MAE) instead.\n2.2\nArchitecture Details and Training Scheme\nWe use the same network architecture for all of our experiments as shown in Fig. 1. The\nencoder E consists of four blocks of two convolution layers with a ﬁlter size of (3, 3)\nfollowed by a max pooling layer with a ﬁlter size of (2, 2) and batch normalization\nafter every convolution layer. After every pooling layer we also introduce a dropout of\n0.3. The number of feature maps in each of the convolution layer of a block are 32,\n64, 128, and 256. Following these blocks is a transition layer of two convolution layers\nwith feature maps of size 512 followed by batch normalization layers. The Cfc and Cwc\ndecoders are connected to the E and mirror the layers with the pooling layers replaced\nwith 2D transposed convolutional layers, which have the same number of feature maps\nas the blocks mirror those in the encoder. Similar to a U-Net, we also introduce skip\nconnections across similar levels in the encoder and decoders. The reconstructor R is\nsimply a Sigmoid layer applied to the concatenation of Ifc and Iwc, resulting in a sim-\npliﬁed CompNet [11]. The Discriminator D mimics the architecture of the E, except for\nthe last layer where a dense layer is used for classiﬁcation. All the intermediate layers\nhave ReLU activation function and the ﬁnal output layers have the Sigmoid activation.\nThe only exception is the output of the discriminator D, which has a Tanh activation\nfunction to separate Ifc and images from the Rd to the maximum extent.\nWe use Keras with Tensorﬂow backend and Adam optimizer with a learning rate of\n5e-5 to implement our architecture. We follow two distinct training stages:\n– In the ﬁrst stage, we train D and M in cycles. We start training D with {Rd}\nwith True labels and {Ifc} with False labels. These training samples are shufﬂed\nrandomly. Following D, we train M with {Iin} as input and the weights of D\nfrozen while preserving the connection between {Ifc} and D. The objective of\nthe M is to morph the appearance of {Iin} into {Ifc} to fool D with the frozen\n6\nRaunak Dey and Yi Hong\nweights. We call these two steps one cycle, and in each step there may be more than\none epochs of training for M or D.\n– In the second stage, M and D continue to be trained alternatively; however, the\ninput images to D are changed, since the training purpose at this stage is to focus\non the differences between the {Rd} and {Iin}, while ignoring the noisy biases\ncreated by the M in transforming {Iin} to {Ifc}. To achieve this, we augment the\nreference distribution {Rd} with its generated images via M, i.e., {Ifc(Rd)}. We\ntreat them as true images, and the union set {Rd ∪Ifc(Rd)} is used to update D.\nRuntime Analysis. We use two Nvidia TitanX GPUs and on average a discriminator\ncycle takes 2.5 ms to process a single 2D image slice with size of 240 × 240, while the\nmain module cycle takes 15.5 ms to process a single 2D image slice during training.\n3\nApplications\nWe evaluate our model on three unsupervised anomaly segmentation tasks: MS lesion\nsegmentation, brain tumor segmentation, and liver lesion segmentation. We use the MS-\nSEG2015 [7] training set, BraTS [1,2,18], and LiTS [6] datasets in these tasks.\nMS-SEG2015. The training set consists of 21 scans from 5 subjects with each scan\ndimensions of 181 × 217 × 181. We resize the axial slices to 160 × 160, so that we can\nshare the same network design as the rest of the experiments.\nBraTS 2019. This dataset consists of 335 T1-w MRI brain scans collected from 259\nsubjects with high grade Glioma and 76 subjects with low grade Gliomas in the training\nset. The 3D dimensions of the images are 240 × 240 × 155.\nLiTS. The training set of LiTS consists of 130 abdomen CT scans of patients with liver\nlesions, collected from multiple institutions. Each scan has a varying number of slices\nwith dimensions of 512×512. We resize these CT slices to 240 × 240 to share the same\nnetwork architecture with other tasks.\nFor all experiments, the image intensity is normalized to [0, 1] over the 3D volume;\nhowever, we perform the 3D segmentation task in the slice-by-slice manner using axial\nslices. To balance the sample size in Iin and Rd, we randomly sample and duplicate the\nnumber difference to the respective set.\nMS Lesion Segmentation. In this task, we randomly sample 2870 non-tumor, non-\nzero, Brats-2019 training set slices to make our reference distribution Rd as in [3], while\nthey use their own privately annotated healthy dataset. Meanwhile, the 2870 non zero\n2D slices of the MS-SEG2015 training set are used in the main module M. We train\nthis network using three cycles in the ﬁrst stage and one cycle in the second stage and\ntake the threshold at 254 intensity based on the right most peak of the image histogram.\nWe obtain an average Dice score of 32.94% without any post processing. By using\na simple post-processing with erosion and dilation5 with 5 × 5 ﬁlters, this number im-\nproves to 48.20% Dice score. In comparison, a similar study conducted by [3] consisting\nof a multitude of algorithms including AnoVAEGAN [4] and f-AnoGANS, obtained a\n5 We use this operator to improve the connectivity of the generated anomaly mask.\nASC-Net\n7\nIin\nIfc\nIwc\nIro\nMgt\nMest\nMest ∩Iin\nFig. 4. Sample results of MS-SEG2015, Brats-2019 and LiTS (top to bottom) obtained from the\nvarious branches of the network. The Ifc in the second row is contrast enhanced to present the\ncontent contained in the brain region. None of these include any of the post processed images.\nbest mean score of 27.8% Dice after post processing by f-AnoGANS. Before post pro-\ncessing the best method was Constrained AutoEncoder [8] with a score of 9.7% Dice.\nSample images of our method are included in Fig. 4\nBrain Tumor Segmentation. In this task, we perform patient-wise two-fold cross-\nvalidation on the Brats-2019 training set. In each training fold, we use a 90/10 split\nafter removing empty slices. The 2D slices from the 90% split without tumors are used\nto make our reference distribution Rd; while the 2D slices with tumors from the 90%\nsplit and all the slices from the 10% split are used for training our model. As a result,\nthe sample size of Rd for fold one and two amounts to 11,745 and 12,407 respectively,\nwhile the size of Iin amounts to 11,364 and 10,786, respectively. We train this network\nusing two cycles in the ﬁrst stage and one cycle in the second stage.\nWe obtain an average Dice score of 63.67% for the brain tumor segmentation. Fig-\nure 4 shows samples generated by our ASC-Net. Figure 5 shows our attempt to apply\nf-AnoGANs [23] by following their online instructions. The failure of AnoGANs in\nthe reconstruction brings to light the issue with the regeneration based methods and the\ncomplexity and stability of GAN based image reconstruction.\nLiver Lesion Segmentation. To generate the image data for this task, we remove the\nnon-liver region by using the liver mask generated by CompNet [11] and take all non-\nzero images. We have 11,926 2D slices without liver lesions used in the reference dis-\ntribution Rd. The remaining 6,991 images are then used for training the model. We\nperform slice-by-slice two-fold cross-validation and train the network using two cycles\nin both ﬁrst and second stages. To extract the liver lesions, we ﬁrst mask out the noises\nin the non-liver region of the reconstructed image Iro and then invert the image to take\na threshold value at 242, the rightmost peak of the inverted image.\nWe obtain an average Dice score of 32.24% for this liver lesion segmentation, which\nimproves to 50.23% by using a simple post processing scheme of erosion and dilation\n8\nRaunak Dey and Yi Hong\nFig. 5. Query images (top) and their reconstructions (bottom) using f-AnoGANs [23].\nFig. 6. Stability: The ﬁrst image is the input image, the second is the ground truth. The rest of\nimages are reconstruction from various re-runs of the framework with variable training cycles\nand stage. All runs are able to isolate the anomaly in question.\nwith 5×5 ﬁlter. Sampled results are shown in Fig. 4. In comparison, a recent study [12]\nreports a cross-validation result of 67.3% under a supervised setting. Note that the an-\nnotation in the LiTS lesion dataset is imperfect with missing small lesions [9,12]. Since\nwe use the imperfect annotation to select images for the reference distribution, some\nslices with small lesions may be included and treated as normal examples.\n4\nDiscussion and Future Work\nIn this paper we have presented a framework that performs two-cut split in an unsu-\npervised fashion guided by an reference distribution using GANs. Unlike the meth-\nods in the AnoGAN family which operate as a reconstruction-based method and needs\nfaithful reconstruction of normal images to function properly, we treat the anomaly\nsegmentation as a constrained two-cut problem that requires a semantical and reduced\nreconstruction for clustering. Our ASC-Net focuses on the anomaly detection with the\nnormal image reconstruction as a byproduct, thus still producing competitive results\nwhere reconstruction dependent methods such as f-AnoGAN fails to work on. The cur-\nrent version of our ASC-Net aims to solve the two-cut problem, which will be tasked\nto handle more than two selective cuts in the future. Theoretical understanding of the\nproposed network is also required, which is left as a future work.\nLimitations and Opportunities. One reason of our low Dice scores could be that we\nhad to select non-tumor or normal slices as our reference distribution, which does not\naccount for other co-morbidities. This affects the performance of the framework as it has\nno other guidance and would consider co-morbidities as an anomaly as well. However,\nthis provides possibility of bringing other anomalies into the users’ attention.\nASC-Net\n9\nFig. 7. Termination of network\ntraining affects the reconstruction\nresult. Left to right columns in\neach row: the input image, the im-\nage reconstructed via two cycles\nin the ﬁrst stage and one in the\nsecond stage, and the image re-\nconstructed via adding one cycle\nin the second stage.\nTermination and Stability. The termination point of\nthis network training is periodic. The general guide-\nline is that the peaks should be well separated and we\nterminate our algorithm at three or four peak separa-\ntion. However, continuing to train further may not al-\nways result in the improvement for the purpose of seg-\nmentation due to accumulation of holes as shown in\nFig. 7, even though visually the anomaly is captured in\nmore intricate detail. We however encourage training\nlonger as it reduces false positive and provide detailed\nanomaly reconstruction, though the Dice metric might\nnot account for it. In our experiments, we specify the\nnumber of cycles in each stage. However, due to the\nrandom nature of the algorithm and the lack of a par-\nticular purpose and guidance, the peak separation may\noccur much earlier, then training should be stopped ac-\ncordingly. The reported network in our Brats-2019 ex-\nperiments has an average Dice score of 6% over the\nnetwork trained longer as shown in Fig. 7. Regarding\nthe stability, Figure 6 demonstrates an anomaly estimated by different networks that are\ntrained with different number of training cycles. We observe that while the appearance\nof Iro changes, we still obtain the anomaly as a separate cut since our framework works\nwithout depending on the quality of reconstruction.\nAcknowledgements\nThis work was supported by NSF 1755970 and Shanghai Municipal Science and Tech-\nnology Major Project 2021SHZDZX0102.\nReferences\n1. Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S\nKirby, John B Freymann, Keyvan Farahani, and Christos Davatzikos. Advancing the cancer\ngenome atlas glioma mri collections with expert segmentation labels and radiomic features.\nScientiﬁc data, 4:170117, 2017.\n2. Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempﬂer, Alessan-\ndro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki,\net al. Identifying the best machine learning algorithms for brain tumor segmentation, pro-\ngression assessment, and overall survival prediction in the brats challenge. arXiv preprint\narXiv:1811.02629, 2018.\n3. Christoph Baur, Stefan Denner, Benedikt Wiestler, Nassir Navab, and Shadi Albarqouni.\nAutoencoders for unsupervised anomaly segmentation in brain mr images: A comparative\nstudy. Medical Image Analysis, page 101952, 2021.\n4. Christoph Baur, Benedikt Wiestler, Shadi Albarqouni, and Nassir Navab. Deep autoencod-\ning models for unsupervised anomaly segmentation in brain mr images. In International\nMICCAI Brainlesion Workshop, pages 161–169. Springer, 2018.\n10\nRaunak Dey and Yi Hong\n5. Amanda Berg, J¨orgen Ahlberg, and Michael Felsberg. Unsupervised learning of anomaly\ndetection from contaminated image data using simultaneous encoder training. arXiv preprint\narXiv:1905.11034, 2019.\n6. Patrick Bilic, Patrick Ferdinand Christ, Eugene Vorontsov, Grzegorz Chlebus, et al. The liver\ntumor segmentation benchmark (lits). arXiv:1901.04056, 2019.\n7. Aaron Carass, Snehashis Roy, Amod Jog, Jennifer L Cuzzocreo, Elizabeth Magrath, Adrian\nGherman, Julia Button, James Nguyen, Ferran Prados, Carole H Sudre, et al. Longitudinal\nmultiple sclerosis lesion segmentation: resource and challenge. NeuroImage, 148:77–102,\n2017.\n8. Xiaoran Chen and Ender Konukoglu. Unsupervised detection of lesions in brain mri using\nconstrained adversarial auto-encoders. arXiv preprint arXiv:1806.04972, 2018.\n9. Grzegorz Chlebus, Andrea Schenk, Jan Hendrik Moltz, Bram van Ginneken, Horst Karl\nHahn, and Hans Meine. Automatic liver tumor segmentation in ct with fully convolutional\nneural networks and object-based postprocessing. Scientiﬁc reports, 8(1):1–7, 2018.\n10. Allison Del Giorno, J Andrew Bagnell, and Martial Hebert. A discriminative framework\nfor anomaly detection in large videos. In European Conference on Computer Vision, pages\n334–349. Springer, 2016.\n11. Raunak Dey and Yi Hong. Compnet: Complementary segmentation network for brain mri ex-\ntraction. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pages 628–636. Springer, 2018.\n12. Raunak Dey and Yi Hong. Hybrid cascaded neural network for liver lesion segmentation. In\n2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pages 1173–1177.\nIEEE, 2020.\n13. Sarah M Erfani, Sutharshan Rajasegarar, Shanika Karunasekera, and Christopher Leckie.\nHigh-dimensional and large-scale anomaly detection using a linear one-class svm with deep\nlearning. Pattern Recognition, 58:121–134, 2016.\n14. Nathalie Giordana and Wojciech Pieczynski. Estimation of generalized multisensor hid-\nden markov chains and unsupervised image segmentation. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 19(5):465–475, 1997.\n15. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nneural information processing systems, pages 2672–2680, 2014.\n16. Masanari Kimura and Takashi Yanagihara. Anomaly detection using gans for visual in-\nspection in noisy training data. In Asian Conference on Computer Vision, pages 373–385.\nSpringer, 2018.\n17. Te-Won Lee and Michael S Lewicki. Unsupervised image classiﬁcation, segmentation, and\nenhancement using ica mixture models. IEEE Transactions on Image Processing, 11(3):270–\n279, 2002.\n18. Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Fara-\nhani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al.\nThe multimodal brain tumor image segmentation benchmark (brats). IEEE transactions on\nmedical imaging, 34(10):1993–2024, 2014.\n19. Robert J O’Callaghan and David R Bull. Combined morphological-spectral unsupervised\nimage segmentation. IEEE transactions on image processing, 14(1):49–62, 2004.\n20. Jan Puzicha, Thomas Hofmann, and Joachim M Buhmann. Histogram clustering for unsu-\npervised image segmentation. In Proceedings. 1999 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (Cat. No PR00149), volume 2, pages 602–608.\nIEEE, 1999.\n21. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In International Conference on Medical image computing\nand computer-assisted intervention, pages 234–241. Springer, 2015.\nASC-Net\n11\n22. Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric\nfor image retrieval. International journal of computer vision, 40(2):99–121, 2000.\n23. Thomas Schlegl, Philipp Seeb¨ock, Sebastian M Waldstein, Georg Langs, and Ursula\nSchmidt-Erfurth. f-anogan: Fast unsupervised anomaly detection with generative adversarial\nnetworks. Medical image analysis, 54:30–44, 2019.\n24. Thomas Schlegl, Philipp Seeb¨ock, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and\nGeorg Langs. Unsupervised anomaly detection with generative adversarial networks to guide\nmarker discovery. In International conference on information processing in medical imaging,\npages 146–157. Springer, 2017.\n25. Philipp Seeb¨ock, Sebastian Waldstein, Sophie Klimscha, Bianca S Gerendas, Ren´e Donner,\nThomas Schlegl, Ursula Schmidt-Erfurth, and Georg Langs. Identifying and categorizing\nanomalies in retinal imaging data. arXiv preprint arXiv:1612.00686, 2016.\n26. Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions\non pattern analysis and machine intelligence, 22(8):888–905, 2000.\n27. Houssam Zenati, Chuan Sheng Foo, Bruno Lecouat, Gaurav Manek, and Vijay Ramaseshan\nChandrasekhar. Efﬁcient gan-based anomaly detection. arXiv preprint arXiv:1802.06222,\n2018.\n28. Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chan-\ndrasekhar. Adversarially learned anomaly detection. In 2018 IEEE International Conference\non Data Mining (ICDM), pages 727–736. IEEE, 2018.\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "published": "2021-03-05",
  "updated": "2021-07-09"
}