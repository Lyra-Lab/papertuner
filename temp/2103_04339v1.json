{
  "id": "http://arxiv.org/abs/2103.04339v1",
  "title": "Network Representation Learning: From Traditional Feature Learning to Deep Learning",
  "authors": [
    "Ke Sun",
    "Lei Wang",
    "Bo Xu",
    "Wenhong Zhao",
    "Shyh Wei Teng",
    "Feng Xia"
  ],
  "abstract": "Network representation learning (NRL) is an effective graph analytics\ntechnique and promotes users to deeply understand the hidden characteristics of\ngraph data. It has been successfully applied in many real-world tasks related\nto network science, such as social network data processing, biological\ninformation processing, and recommender systems. Deep Learning is a powerful\ntool to learn data features. However, it is non-trivial to generalize deep\nlearning to graph-structured data since it is different from the regular data\nsuch as pictures having spatial information and sounds having temporal\ninformation. Recently, researchers proposed many deep learning-based methods in\nthe area of NRL. In this survey, we investigate classical NRL from traditional\nfeature learning method to the deep learning-based model, analyze relationships\nbetween them, and summarize the latest progress. Finally, we discuss open\nissues considering NRL and point out the future directions in this field.",
  "text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identiﬁer 10.1109/ACCESS.2020.DOI\nNetwork Representation Learning: From\nTraditional Feature Learning to Deep\nLearning\nKE SUN1, LEI WANG1, BO XU1, WENHONG ZHAO2, SHYH WEI TENG3, FENG XIA3, (Senior\nMember, IEEE)\n1Key Lab for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian 116620, China\n2Ultraprecison Machining Center, Zhejiang University of Technology, Hangzhou 310014, China\n3School of Engineering, IT and Physical Sciences, Federation University Australia, Ballarat, VIC 3353, Australia\nCorresponding author: Feng Xia (e-mail: f.xia@ieee.org).\nThis work is partially supported by Zhejiang Provincial Fundamental Public Welfare Research Program under Grant No. LGG18E050025.\nABSTRACT\nNetwork representation learning (NRL) is an effective graph analytics technique and\npromotes users to deeply understand the hidden characteristics of graph data. It has been successfully\napplied in many real-world tasks related to network science, such as social network data processing,\nbiological information processing, and recommender systems. Deep Learning is a powerful tool to learn\ndata features. However, it is non-trivial to generalize deep learning to graph-structured data since it is\ndifferent from the regular data such as pictures having spatial information and sounds having temporal\ninformation. Recently, researchers proposed many deep learning-based methods in the area of NRL. In this\nsurvey, we investigate classical NRL from traditional feature learning method to the deep learning-based\nmodel, analyze relationships between them, and summarize the latest progress. Finally, we discuss open\nissues considering NRL and point out the future directions in this ﬁeld.\nINDEX TERMS Traditional Feature Learning, Network Representation Learning, Deep Learning, Graph\nAnalytics.\nI. INTRODUCTION\nR\nEPRESENTATION learning is a new paradigm in the\nmachine learning ﬁeld aiming at representing infor-\nmation efﬁciently. For example, in the linguistic domain,\nword vectors generated from Word2vec framework [1]–[3]\nembed semantic information into low dimensional vectors\nso that machines can better understand the words after word\nembeddings. In the setting of network representation learning\n(NRL), a better representation method can make subsequent\nlearning tasks easier. In general, a better representation of a\nnetwork can preserve the graph topology and cluster similar\nnodes together in the embedding space. Additionally, the\nrepresentation learning is beneﬁcial for lots of downstream\ntasks, e.g., clustering [4], node classiﬁcation [5], and link-\nprediction [6]. They have been widely applied in bioinfor-\nmatics [7], linguistics [1], transport network [8], [9] and\nsocial sciences [10]–[13], etc. Many information process-\ning tasks mentioned above depend on how the data are\nrepresented. Meanwhile, with the development of big data,\nwhich presents Volume, Variety, and Velocity characteristics,\na more effective data representation method is required to\nachieve low cost and tractable computation.\nIn the past few decades, many traditional feature learn-\ning (TFL) algorithms e.g., principal component analysis\n(PCA) [14], isometric feature mapping (Isomap) [15], and\nlocal linear embedding (LLE) [16], have been proposed\nfor reducing dimensions of data. Instead, NRL focuses on\nlearning the vector representation of a node or a graph. In\ngeneral, these two kinds of algorithms are then combined to\ntake advantage of both of them.\nHowever, these traditional methods could not effectively\nextract complex and nonlinear structured relationships of\ndata. It is widely recognized that deep learning [17] has\nemerged as a powerful tool for extracting data features and\nhas been applied in many ﬁelds, such as image processing\nand speech recognition. Network science researchers have\napplied deep learning models, including convolutional neural\nnetwork [18], autoencoder neural network [19], recurrent\nneural network [20], and generative adversarial network [21],\ninto graph-structured data and proposed the graph neural net-\nVOLUME 4, 2020\n1\narXiv:2103.04339v1  [cs.SI]  7 Mar 2021\nK. Sun et al.: Network Representation Learning\nwork (GNN) models [22]–[26]. In recent, many researchers\ntry to introduce deep learning models, such as reinforcement\nlearning, adversarial methods to graph learning [27], [28].\nIn this survey, we provide a brief introduction to traditional\nrepresentation algorithms and then mainly review NRL algo-\nrithms associated with deep learning models.\nRepresentation learning models can be divided into “shal-\nlow” model and “deep” model. In this survey, we try to go\nthrough the development of data representation in graph-\nstructured data from TFL to recent NRL based on deep\nlearning. We do not intend to thoroughly summarize the\nvarious types of representation learning models in the liter-\nature. We will instead review TFL and the state-of-the-art\nrepresentation learning methods mainly focusing on NRL\ntechnologies and network embedding algorithms. For under-\nstanding related algorithms, we ﬁrst introduce the Word2vec\nframework as a basic model used by a large number of\nNRL algorithms to help understand what NRL is and the\nrelationship between NRL and deep learning. Furthermore,\nwe classify NRL into two categories, including TFL models\nand deep learning-based models. The overall organization of\nthe categories of network representation learning algorithms\nare shown as Fig 1.\nRelated Surveys. There are already a few papers that\nsummarized the algorithms about NRL. Our survey is dif-\nferent from all of them. We review several traditional feature\nlearning algorithms and classical deep learning-based NRL\nmodels. We present deep learning-based NRL models based\non the graph information they are keeping. A short introduc-\ntion to the other related surveys is given as follows.\nHamilton et al. [29] focus on methods and applications\nof NRL. They discussed NRL in two main parts: embed-\nding nodes and embedding sub-graphs. They proposed an\nencoder-decoder framework to organize various NRL mod-\nels. Goyal et al. [30] mainly reviewed applications and\nperformances of NRL, and gave detailed performance com-\nparisons of NRL. Zhang et al. [31] gave a comprehensive\ncategorization of NRL, including unsupervised methods,\nsemi-supervised methods, and methods preserving network\nstructure or vertex labels. The survey [32] did not review the\nwhole variety of NRL algorithms but focused on methods\nfor structure-preserving and property-preserving. In recent,\nmany researchers turn attentions to graph neural network\nbased models, and some surveys [33]–[36] specially investi-\ngated these works in the aspects of graph learning models and\naggregation methods. These models are basically generated\nfrom deep learning models to graph, such as graph atten-\ntion networks, graph autoencoders, and graph reinforcement\nlearning. There are some surveys focusing on some special\ncases of NRL. For example, Yang et al. [37] reviewed hetero-\ngeneous NRL with analysis over benchmark and evaluation;\nXie et al. [38] introduced dynamic network embedding from\naspects of models.\nThe rest of our survey is organized as follows. We ﬁrst\npresent notations and graph related concepts in Section II.\nThen, we review the models of TFL in Section III and NRL\nbased on deep learning models in Section IV. In the following\ntwo sections, we discuss the application of representation\nlearning, and list several open issues. We provide our conclu-\nsions, draw our prospects of network representation learning\nin future research in Section VII.\nII. NOTATIONS\nThis section presents TFL methods and NRL models based\non deep learning methods. In the following, we ﬁrst give the\ndeﬁnitions of a Graph, Network embedding, and Laplacian\nmatrix. Also, we list some terms and notations used in this\npaper in Table 1.\nDeﬁnition 1: Graph. A graph G = (V, E, Y) is a\ncollection consisting of nodes (vertices or points) V =\n{v1, v2, . . . , vn}, edges E = {ei,j}n\ni,j=1 and labels or side\ninformation associated to nodes Y. The edges between nodes\ncan be directed or undirected.\nDeﬁnition 2: Network embedding. Network embedding\nincludes node embedding and edge embedding. Given a\ngraph G, the embedding function f : V 7→U maps node\nvi ∈V to embedding vector ui ∈U, where V represents\nvectors in the original space and U represents vectors in\nthe projected space. Vector ui is the newly learned node\nrepresentation which often has low dimensions and preserves\nrelevant network properties.\nDeﬁnition 3: Laplacian matrix. In the graph theory,\nLaplacian matrix is a matrix representation of a graph.\nThe Laplacian matrix of a simple graph is represented as:\nL = D −A, where D = diag(P\nj:j̸=i Wij) is the degree\nmatrix, Wij is the weight between node i and j, and A is the\nadjacency matrix of a graph.\nTABLE 1: Terms and Notations\nG\nThe representation of a graph includes nodes and\nedges\nV\nThe set of vertices of a graph\nv\nA vertice of a graph\nE\nThe set of edges of a graph\nL\nLaplacian matrix of a graph\nD\nThe degree matrix of a graph\nA\nAdjacency matrix of graph\ni, j\nThe index of vertices\nu\nThe new representation vector\nW\nThe weight matrix of a graph\nL\nThe symbol of loss function\nRepresentation Learning relies on an essential assumption\nin the manifold hypothesis [39], which refers to the real-\nworld high dimensional data, such as images with two-\ndimensional manifolds embedding in the high dimensional\nspace. Graph-structured data often has high dimensions and\nvarious types. Based on the manifold hypothesis, NRL algo-\nrithm could reduce the dimensions of graph data but keeps\nthe internal relationship of nodes.\n2\nVOLUME 4, 2020\nK. Sun et al.: Network Representation Learning\nFIGURE 1: Categories of network representation learning algorithms.\nIII. TRADITIONAL FEATURE LEARNING MODELS\nLearning the intrinsic characteristics of data is always an\nenormous requirement for data science. In the past decade,\na great deal of traditional feature learning or representation\nlearning algorithms have been proposed in the machine learn-\ning domain such as kernel PCA [40] and kernel k-means [41].\nThey are a set of techniques allowing a system to automat-\nically learn the latent features from raw data. These tech-\nniques are different from feature engineering which manually\nsets feature parameters. In this section, we mainly focus on\nTFL on graphs, and we separate them into three parts: global\nfeature learning models, spectral learning based models, and\nmanifold learning models. Global feature learning mainly\nfocuses on preserving global information of data. Manifold\nlearning aims to preserve local features and information, i.e.,\npreserving property with the neighborhood of each node in a\nnetwork.\nA. GLOBAL FEATURE LEARNING\nAs mentioned above, global feature learning methods pri-\nmarily focus on preserving global information of raw data in\nthe learning feature space. In the following, we will present\nseveral algorithms belonging to this category.\n1) PCA Algorithms\nPCA algorithm [40], is one of the earliest and most popular\nmethods used to reduce the dimensions of the data. PCA is a\nlinear, unsupervised, generative, and global feature learning\nmethod. There are lots of global feature learning algorithms,\ne.g., the variants of PCA, including sparse PCA [42] and ro-\nbust PCA [43]. PCA can be used in NRL, such as dimension-\nreduced, and low-rank recovery of data. It can be applied for\nnetwork visualization and clustering in network science as\nwell.\nThe classical PCA algorithm has several weaknesses, e.g.,\nit lacks the ability to scale well to a number of data samples\nand is sensitive to outliers. In general, the reason that the\nclassical PCA algorithm is susceptible to outliers data is\ncaused by the quadratic term. Robust PCA can well overcome\nthese shortcomings of classical PCA algorithm mentioned\nabove and it is robust to occlusions and missing values\nby recovering the low-rank representation [43]. With the\ngraph smoothness assumption, Shahid et al. [44] incorpo-\nrated spectral graph regularization into robust PCA algorithm\nto improve the quality of clustering and dimension-reduced.\nThe normalized graph Laplacian is deﬁned as:\nL = D−1/2(D −A)D−1/2 = I −D−1/2AD−1/2,\n(1)\nwhere D = diag(di) is the degree matrix and A is an\nadjacency matrix. In general, graph-Laplacian [45] is often\ncombined with PCA methods [46]–[48] for feature extraction\nin the area of bioinformatics. The ﬁnal experiments showed\nthat the model can achieve better performance than the state-\nof-the-art models for the clustering and low-rank recovery\ntasks.\nTo accelerate the speed of robust PCA, Shahid et al. [49]\nproposed fast robust PCA (FRPCA) algorithms, which has\nthe same advantages as the previous robust PCA. FRPCA has\nlower computational complexity for processing large scale\ndatasets (O(nlog(n))) by using the FLANN library [50].\nMotivated by the emerging ﬁeld of signal processing on\ngraphs [51], FRPCA adopted the idea of graph similarity,\nincluding feature similarity and data similarity, to enhance\nthe clustering quality in the new representation space. One\nVOLUME 4, 2020\n3\nK. Sun et al.: Network Representation Learning\nproblem of PCA algorithms is its high computational com-\nplexity. Here, FRPCA utilized Fast Iterative Soft Threshold-\ning Algorithm (FISTA) [52] to solve this problem.\nThere are still many variants of PCA, such as sparse\nPCA (SPCA) [42] which extends classical PCA algorithm\nby adding sparsity constraint on input variables and can be\nwell applied for multivariate datasets. Megasthenis et al. [53]\nintroduced a variant of SPCA which accommodates the graph\nconstraints to analyze ﬁnancial data and the data in neuro-\nscience. Min et al. [54] proposed Edge-group Sparse PCA\n(ESPCA) which combines the prior gene network with the\nPCA method for dimension-reduced and feature extraction.\n2) ICA Algorithms\nIndependent component analysis (ICA) [55] is a classical and\npowerful tool in signal processing, and also has been used\nto analyze the structured graph data. It is widely used to\nbrain network analysis [56]–[58]. Park et al. [56] proposed a\nvariant ICA, Graph ICA, to explore the changes of cognitive\nnetworks in the brain after completing a task. The concepts\nof Graph-ICA can be shown as follows:\ng = [g1, . . . , gM]⊤= W[s1, . . . , sM]⊤= Ws,\n(2)\nwhere weight matrix W represents the relationship strength\nbetween source (graph) s to compose g. The main ideas\nof the algorithm are to decompose measured graphs into\ncommon source graphs and then ﬁnd these canonical network\ncomponents from limited sets of data in neuroimaging. Diana\net al. [57] directly utilized the ICA algorithm to extract\nfeatures of different brain networks on the fMRI data and\nfound the relationship among word learning with different\nparts of brain networks.\nZiegler et al. [58] designed a method combined with the\nICA algorithm and then applied it for analyzing the resting-\nstate fMRI. In short, the authors ﬁrst used ICA to deal\nwith neuronal components and then reconstructed them as\nweigh graphs. The method of calculating edge weighting\nin the graphs depends on the contribution to the speciﬁc\ncomponent.\nB. SPECTRAL LEARNING ON GRAPHS\nThe spectral learning, which is one kind of machine learning\nalgorithm based on spectral methods, utilizes information\nin the eigenvectors of the target matrix to extract hidden\nstructure. Most of the methods based on spectral learning just\nconsider the structural information, so they could not apply\nto networks with complex information. In the following, we\nwill discuss several common spectral learning frameworks,\nincluding spectral methods, singular value decomposition,\nand tensor factorization.\n1) Spectral Methods\nSpectral methods are fundamental for solving problems\nin engineering, applied mathematics, and statistics. More\nspeciﬁcally, network researchers have used spectral methods\nto solve problems in network science, such as analyzing and\nvisualizing networks.\nCommunity detection is a popular problem in social net-\nworks. To address this problem, Newman et al. [59] proposed\na spectral method as they ﬁnd that tradition community detec-\ntion methods based on maximum modularity and likelihood\nmethods can be treated as spectral algorithms. The main\nidea of the algorithm is to utilize the matrix eigenvectors\nto represent the networks. The proposed algorithm con-\ntains three parts: (1) modularity maximization, (2) degree-\ncorrected block model, and (3) normalized-cut graph parti-\ntioning. Zhang et al. [60] also applied the spectral method\nto detect communities while focused on overlapping com-\nmunities in social networks. They adopted the K-medians\nalgorithm to address the overlaps for graph clustering. In the\nreal world, data are collected in different forms with various\nfeatures and different structures. In order to deal with this\nsituation, Li et al. [61] designed a spectral clustering algo-\nrithm based on the bipartite graph. The solution is the ﬁrst\nk smallest eigenvectors. Moreover, the authors used a fast\napproximation algorithm to reduce the cost of computation\nof multi-view spectral clustering, so as to face the require-\nments of large-scale graph construction. Some researchers\ntry to combine spectral methods with deep learning [62]\nby adopting stochastic optimization, which is successfully\nused to ﬁnd meaningful subgoals in reinforcement learning\nenvironments.\n2) Tensor Factorization\nThe knowledge graph is a hot topic in network science, which\ncan express data or information in the form of a graph with\nedges representing relations and nodes representing entities.\nIt can be applied for recommender systems, search engines,\netc. A group of researchers have applied tensor factorization\nmethods to study NRL. Trouillon et al. [63] designed a\nmethod for link prediction called ComplEx, which is linear\nin both space and time. Besides, the algorithm exploited\ncomplex embeddings and utilized Hermitian dot product.\nTherefore, it is much simpler than neural tensor networks\nand holographic embeddings. Also, the algorithm is suitable\nfor large datasets. Trouillon et al. [64] extended previous\nwork [63] and utilized factorization models to study the\nknowledge graph. Moreover, the authors gave several proofs\nfor the proposed model and more experiments, especially\nrelated to the training time of the models. The main idea\nof the algorithm is decomposing tensors into a product of\nembedding matrices with much lower dimensions.\nC. MANIFOLD LEARNING\nManifold learning methods focus on preserving local similar-\nity among data when the new representations are learned. In\nrecent years, this algorithm is often used to deal with network\nanalysis tasks. We will present several manifold learning\nalgorithms, such as Isomap, Local Linear Embedding (LLE),\nand Laplacian Eigenmaps (LE). They are all based on the\ngraph construction by exploiting manifold learning.\n4\nVOLUME 4, 2020\nK. Sun et al.: Network Representation Learning\n1) Isomap\nSimilar to PCA, Isomap algorithm [15] is a classical-\ndimension reduced approach, which is building on classical\nMetric Multidimensional Scaling (MDS) [65]. It is more\npowerful than other classical reduction methods as it could\nkeep the nonlinear relations of original source. The main\nideas of Isomap contain three steps: (1) constructing a neigh-\nborhood graph by using connectivity algorithm (such as\nKNN) from adjacency matrix; (2) computing the shortest\npath of entries as the geodesic distance; (3) ﬁnally, using\nMDS algorithm to obtain coordinate vector. The objective\nfunction is shown as follows:\nmin\nΣi̸=j,...,N(di,j −||ui −uj||)2,\n(3)\nwhere d represents the shortest path obtained from step (2),\nand u is the new representation vector that can be learned\nwhen minimizing Eq. (3). From Eq. (3), we can see that the\noptimized objective function is to make the distance between\nnodes in the new learned space similar to the distance in the\noriginal space. In other words, new low dimensional vectors\napproximately preserve the geodesic distance of the original\ndata in the high dimensional space.\n2) Local Linear Embedding\nLocal Linear Embedding (LLE) [16] is an another classical\nnonlinear dimension-reduced approach. This algorithm relies\non the manifold hypothesis, and each node lies on its neigh-\nbors. Node features can be obtained from the summation of\nneighbor features, so that the algorithm has the ability to pre-\nserve the locally linear structure of neighborhood. Although\nLLE could preserve the structural information, it could only\nbe used to undirected graph. LLE includes three main steps:\n(1) selecting neighbors for each node; (2) computing the\nweight Wij which is the edge weight between the node and\nits neighbors:\nmin\nΣi||xi −ΣjWijxj||2,\n(4)\n(3) computing the new low dimensional representation from\nweights obtained from step (2), which is expressed as:\nmin\nΣi||ui −ΣjWijuj||2.\n(5)\nWhen minimizing (5), we can obtain the representation\nmatrix U. In summary, LLE encodes the local information\nat each point into the reconstruction weights of its neighbors\nand then uses these weights to compute the low dimensional\nembeddings.\n3) Laplacian Eigenmaps\nLaplacian Eigenmaps (LE) [45] is a popular approach to\nﬁnd the low dimensional representation. Similar to the ﬁrst\nstep of LLE, LE ﬁrst constructs a graph G by using the\nk nearest neighbors, and then uses the graph G to derive\nLaplacian matrix L = D −W, where the weight matrix W\nis generated by heat kernel method. The authors deﬁned an\nobjective function that makes connected points stay closer to\neach other, which is expressed as:\nΣ||ui −uj||2Wij = tr(U⊤LU).\n(6)\nWhen we minimize the above equation, the new represen-\ntation matrix U can be obtained. In addition, TFL methods,\nsuch as Isomap, LLE, and LE, are all just applied for the undi-\nrected graph without external node information and focus on\nlocal features of the graph. However, they are not suitable\nfor large-scale networks because obtaining eigenvector from\nlarge scale matrices has high computational complexity both\nin time and space.\nIV. DEEP LEARNING-BASED MODELS\nWe have witnessed the superior performance of deep learning\nin many ﬁelds, and they have been widely applied for image\nclassiﬁcation, speech recognition, and object detection, etc.\nThe deep architecture can extract latent information layer by\nlayer from data, which contributes to the performance of data\nprocessing. More precisely, the original data is transformed\nby a nonlinear model to a more higher-level feature represen-\ntation so as to achieve more abstract representation of data.\nThere are several deep learning-based NRL models [66]–[70]\nproposed in recent years. The timeline of some representative\nmethods of them are illustrated as Fig 2. Even though most\nof them are based on advanced models of deep learning,\nthere have some methods having connections with traditional\nfeature learning. We will introduce these in the related parts.\nIn the following, we will focus on reviewing them from three\nsubsections, as outlined in Table 2.\nA. A TAXONOMY OF DEEP LEARNING-BASED NRL\nMODELS\nThe deep learning-based NRL models have different cat-\negories. We assign them into three categories: (1) Edge-\nbased Modeling Methods, (2) Multi-source Based Modeling\nMethods, (3) Subgraphs Based Modeling Methods. As deep\nlearning-based NRL models are the major concern in this pa-\nper, we discuss and give a brief summary of them in Table 2.\nIn the following subsections, We detail the characteristics of\neach algorithm belonging to the listed categories and provide\na summary of them.\nB. EDGE-BASED MODELING METHODS\nGraph-structured data is a complex data type containing\nedges and nodes. In real world, graph edge can represent the\nlink between users and products or links of friends. Lots of\nNRL algorithms just consider the structure of the graph, such\nas ﬁrst-order proximity of nodes and second-order proximity\nof nodes. We cluster these NRL models by focusing on the\ngraph structure as edge-based modeling methods. In addition,\nthere are several NRL models based on Skip-Gram model [1],\nwhich is a powerful model in natural language processing.\nMoreover, the random walk approach [83] has been applied\nto capture graph structure. To understand these NRL algo-\nVOLUME 4, 2020\n5\nK. Sun et al.: Network Representation Learning\nFIGURE 2: A timeline of some representative network representation learning methods.\nTABLE 2: A summary of NRL algorithms according to the information they are preserving\nCategory\nAlgorithms\nNeural components\nProximities\nStrucutre Sequence Technology\nEdge-based Modeling Methods\nWord2vec [1]\nNeural Probabilistic Language Model\nNone\nRandom walk\nDeepWalk [71]\nSkip-Gram Model\nFirst-order, Second-order\nnode2vec [5]\nSecond-order, Higher-order\nAIDW [72]\nGANs\nGraphGAN [68]\nGANE [73]\nNone\nLINE [6]\nNone\nFirst-order, Second-order\nSDNE [67]\nAutoencoder\nAdjacency matricx\nDNGR [74]\nSecond-order, Higher-order\nSurﬁng model\nMulti-source Based Modeling Methods\nHNE [23]\n(Graph)Convolutional Neural Network\nNone\nLinear transformation matrices\nPATCH-SAN [66]\nGraph normaliztion\nSSC-GCN [22]\nFirst-order\nLayer-wise propagation rule\nPlanetoid [75]\nFeed-forward neural networks\nSecond-order, Higher-order\nNone\nTransNet [76]\nAutoencoder\nNone\nARGA [77]\nAutoencoder, GANs\nAdjacency matrix\nSubgraphs Based Modeling Methods\nGGS-NNs [78]\nGraph Neural Networks\nNone\nCNN-Graphs-FLSF [79]\nConvolutional Neural Network\nDIFFPOOL [80]\nGraph Neural Network,Pooling\nFirst-order\nLayer-wise propagation rule\nHGP-SL [81]\nSAGPool [82]\nrithms deduced from Skip-Gram model, we start with a brief\nintroduction of Word2vec model [1] in this subsection.\n1) Word2vec\nWord2vec model [1] [3] is recognized as a powerful tool in\nnatural language processing. It can reconstruct one-hot vector\nrepresentations of words (word embedding). Actually, the\nframework of the model is a variant of neural probabilistic\nlanguage model with three layers, and the word embedding\nrepresentations are the matrices between input layer and hid-\nden layer. After that, several literatures [84]–[86] have been\nproposed other variants to explain the principle of Word2vec.\nLevy et al. [85] pointed out that the neural word embedding\nis one kind of implicit matrix factorization. Given training\nwords set {w1, w2, wt, ..., wn}, where t is the position in a\ntext and the aim of Word2vec is to learn an estimated model:\nF(Θ, wt, wt−1, . . . , wt−n+1) = P(wt|wt−1, . . . , wt−n+1),\n(7)\nwhere function P is the conditional probability, function\nF is the function carried out by using a neural network\nand its free parameters, and Θ denotes the feature vector\nmatrix (neural network matrix). The weight matrices can be\nlearned by training the model when maximizing the empirical\nconditional probability of model F:\nmax\nX\nt\nF(Θ, wt, wt−1, . . . , wt−n+1).\n(8)\nThere are some variants of the model based on the way of\nnormalization, e.g., softmax normalization and hierarchical\nsoftmax. The word vector is attracting interest due to the\nfeature that semantic similarity words are located close to\neach other in the word vector space (representation space). In\naddition, the vectors can be computed by linear mathematical\noperations, for example, “king”-“queen”=“man”-“women”\nas shown in Fig. 3. These intriguing property of the word\nvector shows that it contains semantic information existing\nin the real world.\n6\nVOLUME 4, 2020\nK. Sun et al.: Network Representation Learning\nFIGURE 3: An example of word vectors embedding in two\ndimensional space.\n2) DeepWalk\nDeepWalk [71] is a NRL algorithm, and it can learn latent\nrepresentation of vertices in networks. The algorithm is the\nﬁrst generalization of Word2vec to networks. The truncated\nrandom walk approach is utilized to capture the graph struc-\nture, and then generate a sequence of vertices. However, the\nrandom walk approach is unbiased, which means it can not\nconduct breadth-ﬁrst search or depth-ﬁrst search on graph\nbased on preferences. That provides a chance to improve\nthe embedding performance by node2vec [5]. The frequency\nof the vertex appears in the sequences following power-law\ndistributions, which is similar to the distributions of words\nin natural language. This is the main reason that Word2vec\nalgorithm can be used to generalize the network-structured\ndata. Given a random walk sequence v1, v2, . . . , vl, l is\nthe length of word sequence. The training objective of\nDeepWalk is the same to the Word2vec algorithm. Given\na previous vertex vi, the likelihood of observing vertices\nvi−w, . . . , vi−1, vi+1, . . . , vi+w is expressed as\nP(vi−w, . . . , vi−1, vi+1, . . . , vi+w|vi).\n(9)\nNow, learning an effective vertex representation Φ(vi)\n(Φ(vi) = ΘT · vi) becomes an optimization problem:\nmax\nlog P(vi−w, . . . , vi−1, vi+1, . . . , vi+w|Φ(vi)). (10)\nDifferent from the form of the adjacency matrix, vertex\nvector representation can avoid the data sparse problem,\nwhich can achieve higher computational efﬁciency. Further-\nmore, the random walk approach is leveraged to generate\nsequences of vertices based on local information. This char-\nacteristic enables DeepWalk to run on the distributed systems\nso as to meet the requirement of large-scale data processing.\n3) Node2vec\nAnalogous to DeepWalk based on Word2vec, node2vec [5]\nalgorithm was proposed by extending the Skip-gram archi-\ntecture [1] to networks. The algorithm introduced a ﬂexible\nneighborhood sampling strategy than DeepWalk, which cap-\ntures network structure controlled by two hyperparameters\np and q. They are used to interpolate random walk with\nFIGURE 4: Node2vec random walk strategy.\nbreadth-ﬁrst sampling or depth-ﬁrst sampling. Given several\nnodes t, v, x1, x2, x3 as shown in Fig. 4, the unnormal-\nized transition probability between v and x is decided by\nαpq(t, x)·wvx. The piecewise function αpq(t, x) is expressed\nas\nαpq(t, x) =\n\n\n\n\n\n\n\n\n\n\n\n1\np\nif dtx = 0\n1\nif dtx = 1\n1\nq\nif dtx = 2,\n(11)\nwhere wvx is the static edge weight and dtx denotes the\nshortest path distance between nodes t and x. Actually, the\nunbiased random walk strategy of DeepWalk is a special\ncase of node2vec with p = 1 and q = 1. When tuning\nthese parameters, the model can trade off the preference of\nfocusing on the local structure or the global structure and\ntherefore learns high quality and more information embed-\ndings compared with DeepWalk.\n4) LINE\nLarge-scale Information Network Embedding (LINE) algo-\nrithm [6] is not a deep learning based model, but is often\ncompared with DeepWalk and node2vec [5]. Qiu et al. [87]\npointed out that LINE, Node2vec and LINE can be implicitly\ncategorized as matrix factorization frameworks. LINE is able\nto preserve the ﬁrst-order proximity and second-order prox-\nimity. But it could not preserve the high-order proximity like\nnode2vec. The ﬁrst-order proximity refers to the proximity of\ntwo nodes connected with one-hop, and it can be measured\nby the joint probability distribution:\np1(vi, vj) =\n1\n1 + exp(−⃗ui\n⊤· ⃗uj)\n,\n(12)\nwhere ⃗ui and ⃗uj stand for the vector representation of the\nnodes vi and vj, respectively. The second-order is similar to\nthe ﬁrst-order, but considers two nodes with a range of two-\nhop. Its proximity is the probability of the context node vj\ngenerated by node vi, i.e.,\np2(vj|vi) =\nexp( ⃗uj\n′⊤· ⃗ui)\nP\nk exp( ⃗uk\n′⊤· ⃗ui)\n.\n(13)\nVOLUME 4, 2020\n7\nK. Sun et al.: Network Representation Learning\nThe second-order proximity means that nodes with similar\ndistribution are similar to each other. To preserve the ﬁrst-\norder proximity or the second-order proximity, the optimiza-\ntion objective of the algorithm tries to minimize the loss\nfunctions derived from KL-divergence between probability\ndistribution and empirical distribution.\n5) SDNE\nMost NRL algorithms cannot extract the high nonlinear\nnetwork-structured feature. Wang et al. [67] designed a semi-\nsupervised model named Structure Deep Network Embed-\nding (SDNE), which is a representative NRL model based on\ndeep autoencoder approach [88]. The framework of SDNE is\nshown in Fig. 5. Similar to LINE which focuses on the graph-\nstructured proximity of nodes, SDNE also preserves the ﬁrst-\norder and second-order proximity of nodes. To address these\nstructure-preserving and sparsity problems, the basic ideas\nof the algorithm are stated as two parts: (1) utilizing un-\nsupervised component combining with deep autoencoder to\npreserve second-order proximity, which means that vertices\nwith similar neighborhood stay close in the latent representa-\ntion space; (2) using ﬁrst-order proximity as the supervised\ninformation to make similar vertices more similar in the\nembedding space, where the objective function is based on\nLaplacian eigenmaps [89]. The loss function for second-\norder proximity is given by:\nL2nd =\nn\nX\ni=1\n||(ˆri −ri) ⊙bi||2\n2,\n(14)\nwhere ˆri denotes the reconstructed representation, and ri\nis the input representation representing the neighborhood\nstructure of the vertex. Notation ⊙represents the Hadamard\nproduct and bi = {bi,j}n\nj=1 is used to impose more penalty\nto the reconstruction error of non-zero elements than zero\nelements, where\nbi,j =\n(\nβ > 1\nsi,j = 0,\n1\notherwise.\n(15)\nSDNE can preserve the local structure of network. The ﬁrst-\norder proximity is adopted to represent the local network\nstructure and the loss function is expressed as\nL1st =\nn\nX\ni,j=1\nsij||y(K)\ni\n−y(K)\nj\n||2\n2,\n(16)\nwhere sij is an instance from the the adjacency matrix S,\nand yi is the latent representation of node. As mentioned\nabove, the objective of the above loss function is to make\nsimilar vertices more similar in the embedding space by\nutilizing the supervised information. The two loss functions\nare all distance-based model similar to Isomap [15], LE [45],\nand LLE [16]. To preserve the ﬁrst-order and second-order\nproximity, the mix loss function combined L1st with L2nd is\ndeﬁned as\nL = L2nd + αL1st + νLreg,\n(17)\nFIGURE 5: The structure of deep network embedding.\nwhere Lreg is a regularizer term to avoid overﬁtting. The ﬁnal\nlatent representation of vertices can be achieved when the\nabove mix loss function is minimized.\n6) DNGR\nAnalogous to SDNE depending on the deep neural network\nmodel, Deep Neural Graph Learning (DNGR) [74] is an-\nother NRL algorithm incorporating deep autoencoders with\nnetwork features. In contrast to algorithms using a truncated\nrandom walk, such as DeepWalk and node2vec, the DNGR\nalgorithm utilizes random surﬁng model to overcome the\ndrawback that they cannot capture weighted graphs and cope\nwith evolved graphs. Two important contributions are stated\nin [74]: (1) designing a random surﬁng model motivated by\nPageRank mode, which can be directly applied for weighted\ngraphs and product the probabilistic co-occurrence (PCO)\nmatrix; (2) demonstrating a novel model for accurately learn-\ning vertex representation of weighted graphs. The main ideas\nof DNGR model are to transform the PCO matrix captured\nby random surﬁng model to positive pointwise mutual in-\nformation (PPMI) matrix and then feed them into stacked\ndenoising autoencoder [90] so as to learn the vertex latent\nrepresentation. The main processes are shown in Fig. 6.\n7) AIDW\nMost NRL algorithms ignore the robustness of representa-\ntion. To overcome this weakness, The authors [72] proposed\nan Adversarial Inductive DeepWalk (AIDW) model consist-\ning of structure preserving component and an adversarial\nlearning component. AIDW could well preserve structure\ninformation while having robustness to representation. The\ntrick behind AIDW is introducing the adversarial learning\nmodel including a discriminator and a generator (structure\npreserving component) to enhance the representation from\nthe structure preserving component of AIDW. However, the\nadversarial learning model always has high computation time\nbecause it conducts game playing between discriminator and\ngenerator. Here, the discriminator is trained to differentiate\nbetween feature vectors and prior samples. The loss function\nof discriminator is expressed as:\nOD(θ2) = Ez∼p(z)[log D(Z; θ2)]+\nEx[log(1 −D(G(x; θ1); θ2))],\n(18)\n8\nVOLUME 4, 2020\nK. Sun et al.: Network Representation Learning\nFIGURE 6: Deep Neural Graph Learning components.\nwhere G(∗; ∗) is a generator. Here, the two models improve\ntheir performance by using the minimax game mechanism.\nThe authors [25] adopted a similar policy with AIDW that\nuse GAN to enhance the performance of embedding, and pro-\nposed a novel NRL algorithm with adversarially regularized\nautoencoders (NetRA). Speciﬁcally, NetRA involves LSTM\nto product positive samples to feed generative model. How-\never, the regularization of NetRA is static. To further improve\nthe ability of adversarial training on graph, the authors [91]\ndeveloped a framework, which could dynamically regularize\nwith graph structure.\nC. MULTI-SOURCE BASED MODELING METHODS\nBesides graph-structured data, there are other types of infor-\nmation, including vertex attributes and vertex labels, etc. We\ncall this information as multi-source of nodes. It is no doubt\nthat efﬁciently using these data can enormously improve\nthe performance of network representation. In recent years,\nmany studies have focused on the multi-source embedding\nof graphs, such as considering labels information [92] and\nheterogeneous network embedding [23]. In this section, we\nwill illustrate the NRL algorithms, which consider both graph\nstructure and the features of nodes.\n1) HNE\nResearch in NRL focuses on homogeneous networks rather\nthan heterogeneous networks. Chang et al. [23] designed Het-\nerogeneous Network Embedding (HNE) algorithm to lever-\nage deep learning architectures. Besides, HNE has several\nkey advantages than traditional linear embedding models,\nsuch as being able to handle the dynamic networks, being\nsuitable for network-oriented data mining applications. To\ntransform different types of data into a uniform representa-\ntion space, a relatively linear transform matrix is introduced:\n˜x = U⊤x, and ˜z = V⊤z,\n(19)\nwhere U and V denote the linear transformation matrices,\nand ˜x and ˜z are the transformed samples. The way of linear\ntransformation is often used to transform embedding space\nby traditional feature learning based on linear projection like\nPCA [40], LLE [16]. Importantly, to represent the similarity\nbetween two data points, the inner product is used in the\nprojected space. Based on that, to denote relationship of\nheterogeneous linkages in networks, a decision function is\ndesigned\nd(xi, xj) = s(xi, xj) −tII,\n(20)\nwhere s(∗, ∗) denotes the inner product of two samples of x\nand z respectively., and tII is a bias-based value. Generally\nspeaking, most representation learning algorithms can be\nseen as mathematical optimization problems, and the loss\nfunction of HNE is deﬁned as:\nL(xi, xj) = log(1 + exp(−Ai,j · d(xi, xj))).\n(21)\nActually, the above equation can be regarded as a binary\nlogistic regression. Another fundamental characteristic of\nHNE is a deep structure including a CNN structure with\nfully connected layers to learn features of image and text,\nand thus it can model complex networks with heterogeneous\ncomponents.\nThere are various deep learning-based methods for deal-\ning with heterogeneous networks. For example, metap-\nath2vec [93] utilizes random walks method to capture graph\nstructure information and then feeds them to Heteroge-\nneousSkipGram to embed vectors. HAN [94] involves atten-\ntion mechanism to improve embedding performance. Zhang\net al. [95] proposed a GNN-based heterogeneous networks\nembedding algorithm, namely HetGNN. The authors con-\nsider that GNN could capture the rich neighborhood informa-\ntion. Existing approaches focus primarily on static networks,\nwhile a HIN in reality is usually changing with time. Zhang et\nal. [96] developed a dynamic heterogeneous network embed-\nding algorithm utilizing hierarchical attentions mechanism.\nThere are some traditional methods for heterogeneous net-\nwork embedding, such as TransN [97] based on dual-learning\nmechanism, RHINE [98] based on euclidean distance and\ntranslation-based distance.\nVOLUME 4, 2020\n9\nK. Sun et al.: Network Representation Learning\n2) Planetoid\nIn the real world, most datasets are composed of unlabeled\ndata. How to leverage a large amount of unlabeled data to\nimprove data analysis performance is still a considerable\nchallenge. To represent unlabeled data in the graph, Yang et\nal. [75] designed a novel semi-supervised learning algorithm\nfor graph embedding (Planetoid). The authors specially de-\nveloped two variants methods containing transductive graph\nembedding and inductive graph embedding. The transductive\ngraph embedding is applied for predicting class label and\ngraph context based on the input feature of observed labeled\ndata and embeddings extracted from graph structure. The loss\nfunction of transductive graph embedding is expressed as:\n−1\nLΣL\ni=1 log p(yi|xi, ei) −λE(i,c,γ)log σ(γw⊤\nc ei),\n(22)\nwhere the ﬁrst term is the probability of predicting labels,\nand the second term is the loss function for predicting graph\ncontext. To generalize unobserved instances, the inductive\nlearning relies on the input feature x and the embedding\nworks as a parameterized function of the feature of x. Similar\nto the loss function of transductive learning, the loss function\nhere is deﬁned as:\n−1\nLΣL\ni=1 log p(yi|xi) −λE(i,c,γ)log σ(γw⊤\nc hl1(xi)). (23)\nCompared with Eq. (22), Eq. (23) replaces embedding of\ninstance ei with embedding hl1(xi). In addition, Planetoid\nframework is based on feed-forward neural networks, of\nwhich the stochastic gradient descent (SGD) is adopted to\ntrain the model in mini-batch mode.\n3) PATHCHY-SAN\nFrom the view that arbitrary graph can be seen as an image,\nNiepert et al. [66] proposed a deep learning-based algorithm:\nPATHCHY-SAN, for learning arbitrary graph by integrating\nCNN. The algorithm opens up a novel perspective that deep\nlearning methods can be used to solve graph embedding\nproblems. The main idea of the algorithm is transforming\ngraph data to a special form combined with existing convolu-\ntional network components. PATCHY-SAN model contains\nfour steps: (1) node sequence selection; (2) neighborhood\ngraph construction; (3) normalizing the extracting neighbor-\nhood graph; (4) combining with existing CNN components,\nwhich is illustrated in Fig. 7. In step (3), in order to optimize\ngraph normalization, an optimal normalization problem is\ndeﬁned to ﬁnd the optimal labeling approach ˆl, which can\nassign similar structural nodes to the same relative position\nin the adjacency matrices for a given collection of graphs\nˆl = arg min\nl\nEg[|dA(Al(G), Al(G′))−dG(G, G′)|], (24)\nwhere g is a collection of unlabeled graphs, l denotes an\ninjective graph labeling procedure, Al(G) is a unique adja-\ncency matrix of graph G determined by labeling procedure l,\ndG denotes the distance between graphs based on nodes and\nFIGURE 7: CNN for graph architecture.\ndA based on matrices. From the equation, the optimal label-\ning produce ˆl can be obtained when the expected difference\nbetween the above two types of distance is minimized.\n4) GCNs\nDifferent from the above frameworks based on Word2vec,\nKipf and Welling [22] proposed a semi-supervised graph\nconvolutional networks (GCNs) considering graph structure\nand node label information. The authors reﬁned and op-\ntimized the previous GCN model proposed by Bruna et\nal. [99] and successfully draw attentions of researchers to\ngraph neural network. This previous GCNs model is based\non spectral graph regularization, which is often used by\ntraditional feature learning methods like robust PCA [44].\nGCNs is a scalable approach, which can be directly applied\nfor graph-structured data. The key innovation of the algo-\nrithm is introducing an effective neural network layer-wise\npropagation rule for graphs\nH(l+1) = σ( ˜D−1\n2 ˜A ˜D−1\n2 H(l)W(l)),\n(25)\nwhere σ(·) denotes the nonlinear activation function, such\nas function tanh, W(l) is the free weight matrix of a layer,\nand ˜A represents the adjacency matrix of an undirected graph\nwith added self-connections. H(l) is the matrix of activations\nin the lth layer, for example, H0 = X (feature matrix)\nand Hl\n= Z (The ﬁnal desired feature matrix). GCNs\nis a differentiable generalization of the Weisfeiler-Lehman\nalgorithm [100] as its propagation rule can be interpreted as a\nvariant of a hash function of that. To achieve semi-supervised\nlearning, graph-based regularization [101] is adopted to learn\ngraph feature, and it includes two steps: (1) getting embed-\nding of nodes; (2) training classiﬁer on the embeddings.\nThere are some GCNs-based variant methods for improv-\ning GCNs capability from different aspects. Li et al. [102]\nproved that GCNs is actually a special form of Lapla-\ncian smoothing and then proposed the co-training and self-\ntraining approaches to improve the learning efﬁciency of\nGCNs framework. Chen et al. [103] developed control variate\nbased algorithms to overcome the receptive ﬁeld size growing\nproblem of GCNs so as to arrive comparable convergence\nspeed. Hamilton et al. [7] designed an improved GCNs\nmodel: GraphSAGE with learnable aggregation functions\nrather than using graph Laplacian, which allows GCNs to\n10\nVOLUME 4, 2020\nK. Sun et al.: Network Representation Learning\napply to a large graph. Chen et al. [104] further improves\nthe sampling algorithm based on GraphSAGE and obtain a\nbetter computational efﬁciency to a large graph. There are\nmany other variant GCNs-based models future enhancing\nperformances like SGCN [105], mGCN [106], and Deep-\nGCNs [107].\nInspired by the success of GCNs, several researchers\ninvolved this model to knowledge graph representation.\nSchlichtkrull et al. [108] ﬁrst utilized the GCN and proposed\nthe R-GCN. The authors designed a matrix transform method\nto represent the relations between facts. This method could\naddress the embedding problem caused by too many types\nof relationships. Cai et al. [109] combined the TransE [110]\nwith GCN and proposed TransGCN, which could be directly\nused to link prediction of the heterogeneous relations knowl-\nedge graph, while has less parameters than R-GCN [108].\nWang et al. [111] further improved the propagation model,\nproposed the logical attention network (LAN). The model\nconsiders the disorder and inequality nature of entities, so\nas to well learn relations between the entities and the corre-\nsponding neighbors.\n5) TransNet\nRealizing that there are rich semantic information on edges,\nTu et al. [76] proposed TransNet-based NRL model to ex-\ntract social relationships from networks, and the interactions\nbetween nodes can be regarded as a translation operation.\nInstead of utilizing CNN, the algorithm designed an auto-\nencoder framework to learn edge latent representation. Au-\ntoencoder is utilized to reconstruct edge labels and vertex\nvectors. These vertex vectors of edges stay in a continued\nspace and we have\nu + l ≈v′,\n(26)\nwhere u and v′ denote the representations of vertices, and l is\nthe edge representation derived from label set l. To minimize\nthe distance d(∗, ∗) at the left and right side of (26), a hinge-\nloss is deﬁned as\nLtrans = max(γ + d(u + l, v′) −d(ˆu +ˆl, ˆv′), 0),\n(27)\nwhere L1 norm is adopted, γ\n> 0 is a margin hyper-\nparameter, and (ˆu, ˆv, ˆl) denotes a negative sample of orig-\ninal variables from the negative sampling set. Similar to\nSDNE framework, the deep model used by TransNet for\nlearning edge representation is deep autoencoder, and the\nreconstructed loss function is a distance-based model similar\nto LE [45], LLE [16]. The loss function is expressed as\nLrec = ||(s −ˆs) ⊙x||,\n(28)\nwhere s and ˆs denote input and output, respectively. Finally, a\njoint optimization objective is deﬁned by integrating the loss\nfunctions mentioned above\nL = Ltrans + α[Lae(l) + Lae(ˆl)] + ηLreg,\n(29)\nwhere λ and α are two hyper-parameters to regularize the\nimportance of different parts of the model.\n6) ARGA\nPrevious works have proved that GCNs is a powerful tool\nto represent graph-structured data. Pan et al. [77] proposed\na novel adversarial graph embedding framework, namely\nARGA leveraging GCNs as a graph encoder. Similar to\nAIDW, ARGA utilized GAN to enhance robustness of em-\nbedding while preserving structure and node label informa-\ntion. To keep both structure and node label information, the\nauthors developed a variant encoder based on GCNs, deﬁned\nas follows:\nZ1 = fRelu(X, A|W(0)),\nZ2 = flinear(Z, A|W(1)),\n(30)\nwhere X represents the node content, A is the graph-\nstructured information, such as adjacency matrix.\nThere are some other GAN-based network embedding\nalgorithms. For example, HeGAN [112] utilize a generator\nto produce negative samples so as to achieve better embed-\nding performance for heterogeneous information networks;\nGraphite [113] is variational autoencoders based embedding\nalgorithm while involving iterative message passing proce-\ndure to raise performance, etc.\nD. SUBGRAPHS-BASED MODELING METHODS\nThe NRL algorithms mentioned above just seek to address\nnode embedding. In addition, there are some requirements for\nlearning the representation of subgraphs or the whole graph,\nwhich refers to a set of nodes and edges, such as protein and\nmolecules. Subgraphs embedding can be applied for learn-\ning molecular ﬁngerprints [114] and predicting multicellular\nfunction [115], etc. In addition, the ﬁxed-size subgraphs can\nbe treated as motifs [116], [117] or graph kernel [118].\nHowever, we will not discuss them but primarily focus on\nthe deep learning-based NRL model.\n1) GGS-NNs\nTo deal with graph-structured data, Li et al. [78] proposed a\nnovel graph-based neural network model called Gated Graph\nSequence Neural Networks (GGS-NNs) by extending the\nprevious related GNNs model [119]. Being different from the\nfeature learning algorithm GNNs, the modiﬁcation specially\nused gated recurrent units [120] and modern optimization\ntechniques. In addition, GGS-NN can produce sequence\noutputs, e.g., paths on a graph, rather than a single output.\nThe basic component of GGS-NNs is GG-NNs containing\nthree main parts: (1) node annotation process initializes\nnode representation; (2) propagation model computes node\nrepresentation of each of them; (3) output model, i.e., the\nmodel ov = g(h(T )\nv\nxv) maps node representations with their\nlabels to outputs, where the notations hv and xv denote a\nrepresentation of node and node label, respectively. In graph\nlevel outputs, a graph level representation vector is deﬁned\nas:\nhg = tanh(\nX\nv∈V\nσ(i(h(T )\nv\n, xv)) ⊙tanh(j(h(T )\nv\n, xv))), (31)\nVOLUME 4, 2020\n11\nK. Sun et al.: Network Representation Learning\nFIGURE 8: GGS-NNs architecture.\nwhere σ(∗, ∗) works as a soft attention mechanism by de-\nciding the relevant nodes to the current graph level task.\ni(∗, ∗) and j(∗, ∗) are neural networks for computing in-\nputs hv, xv to real-valued vectors. The last two parts are\nthe core processes, which map the graph to the output. As\nmentioned above, GGS-NNs can produce sequence outputs,\ne.g., o(1), o(2), . . . , o(K), that is different from most graph\nrepresentation learning algorithms. The architecture shown\nin Fig. 8 contains several GG-NNs operating in sequence to\nproduce sequence outputs.\nThere are some new GNN-based models with innovations\nof involving new pooling strategies for graph-level represen-\ntation. For example, Ying et al. [80] proposed a differentiable\npooling strategy, which can learn hierarchical representations\nof graphs but it has the drawback of high computational com-\nplexity; Zhang et al. [81] also designed a hierarchical graph\npooling method without parameter. The proposed method has\ngood performance of keeping key substructures of graph. Lee\net al. [82] designed a pooling method on graph with involving\nself attention mechanism. This mechanism could pilot model\nfocusing on important features like a human.\n2) CNN-Graphs-FLSF\nGeneralizing CNN to graph-structured data is always a great\nchallenge. To overcome this problem, a novel convolutional\nneural network model was combined with fast localized\nspectral ﬁltering for graphs (CNNs-Graphs) in [79]. CNN is\na powerful tool in images, video tasks, and natural language\nprocessing. There are several contributions to generalize the\nclassical CNN from low dimensional data to high dimen-\nsional irregular domains, e.g., social networks and brain\nconnectomes. At ﬁrst, a novel convolutional ﬁlter named fast\nlocalized spectral ﬁlter on graphs was proposed by enhancing\nthe previous GCNs algorithm [99]. The fast localized spectral\nﬁlter is a spectral approach based on spectral graph the-\nory [121]. In general, the convolutional ﬁlter can be applied\nto recognize identical features of data. Spectral approaches\nare used here to offer a well-deﬁned localization operator on\ngraphs in spectral domain [51]. Here, the graph Laplacian\nmatrix [121] is leveraged as an essential operator in spectral\ngraph analysis, and the formula deﬁnition has been given in\nSection 2 Deﬁnition 3.\nIn order to improve the computational efﬁciency of the\nCNN-Graphs model, the polynomial ﬁlter is ﬁrst utilized\nto reduce the learning complexity, which can achieve the\nsame complexity as classical CNN. In addition, to further\nreduce the model complexity, Chebyshev expansion [122]\nwas utilized to overcome the high computational cost caused\nby the multiplication with the Fourier basis. Second, Graph\ncoarsening is a necessary operation because pooling oper-\nation needs a valid group of data (they are neighborhoods\nfor a graph) and coarsening phase of Graclus multilevel\nclustering algorithm [123] was employed to group similar\nvertices in graphs. Last, an efﬁcient and fast pooling strategy\nwas proposed by constructing a balanced binary tree and then\napplied pooling operation on the rearranged vertices of the\ngraph.\nV. APPLICATIONS\nThere are many NRL algorithms proposed based on deep\nlearning models. In practice, NRL and network embedding\nare commonly used in graph analytic tasks, such as node clas-\nsiﬁcation, link prediction, clustering, and graph visualization.\nIn this section, we will discuss the applications of these NRL\nalgorithms in the following.\nA. NODE CLASSIFICATION\nClassiﬁcation refers to dividing items into different cate-\ngories. In network science, the most common applications in\nthe graph analysis task include node classiﬁcation [124] and\ngraph classiﬁcation. In addition, node or graph classiﬁcation\nis often used as a benchmark to evaluate the performance\nof node embedding. In the node classiﬁcation application,\nlabels are able to indicate different information, such as\ninterest and afﬁliations. However, due to the limited amount\nof labeled data in reality and only a few available labels,\nsemi-supervised learning is often considered to enhance the\nperformance of the node classiﬁcation task [22] [125], or\ntext classiﬁcation [75]. The learned latent representation is\na real-value vector, which is convenient to be combined with\ntraditional classiﬁcation algorithm to address the problems,\nsuch as multi-label classiﬁcation [5] [71] [126]. Besides,\nWang et al. [67] proposed a semi-supervised deep embed-\nding algorithm, which can be used for link prediction and\nmulti-label classiﬁcation in blog-catalog networks. In het-\nerogeneous networks, e.g., social networks, various types of\nnetwork data take a great challenge for mining network data.\nMost representation learning algorithms just consider net-\nwork structure. However, network vertices contain rich text\ninformation, which can be incorporated with NRL through\nmatrix factorization so as to achieve high classiﬁcation accu-\nracy [92]. Another primary application of graph analysis task\nis graph classiﬁcation, which is assigning graphs to several\ncategories, such as classifying proteins based on biological\nfunction [7] [66] [127].\nIn order to present the general process of node classiﬁca-\ntion, and give comparisons of performance with some repre-\nsentative methods, we brieﬂy conduct a node classiﬁcation on\nwiki dataset1. The dataset is a webpage network consisting\nof 2,405 nodes, 17,981 links, and 20 classes. We utilize\nan open-source toolkit2, which packs common methods and\n1https://linqs.soe.ucsc.edu/data\n2https://github.com/thunlp/OpenNE\n12\nVOLUME 4, 2020\nK. Sun et al.: Network Representation Learning\nprovides ﬂexible parameters to control algorithms. Here,\nwe evaluate performance with metrics of time consumption,\nMicro-F1 score, and Macro-F1 score. The ratio of training\nis 0.5. The results are shown in Table 3. As shown in the\ntable, network embedding can be used to node classiﬁcation.\nDifferent methods have different performance in terms of\ncomputational efﬁciency and embedding accuracy.\nTABLE 3: Node classiﬁcation of wiki dataset by different\nnetwork embedding algorithms\nMethods\nTime\nMicro-F1\nMacro-F1\nLE [89]\n1.13s\n0.36\n0.15\nNode2vec [5]\n43.34s\n0.66\n0.53\nLINE [6]\n342s\n0.39\n0.28\nSDNE [67]\n2271s\n0.63\n0.51\nHOPE [128]\n1.62s\n0.60\n0.43\nB. LINK PREDICTION\nAnother popular application of node embedding is link pre-\ndiction by ﬁnding explicit or implicit links between nodes in\ngraphs, for example, mining the relationship links in social\nnetworks [6] [76] [129] [128]. Link prediction is widely used\nto predict unknown interactions between nodes to observe\nlinks and properties. In addition, it can be used to recommend\nitems via establishing links between users, such as predicting\nafﬁnities between users and movies [130]. Moreover, node\nembedding transforms a node into real-value vector, and\nsimilar node vectors tend to stay close in the latent learning\nspace. This intrinsic characteristic helps a lot in the predic-\ntion of missing edges [5] [67] as close nodes are likely to\nhave connections in the future. Recently, knowledge graph\nis becoming a hot topic in the network science domain, in\nwhich predicting missing relations of entities attracts lots\nof attention [131]. Predicting unknown interactions between\nproteins in computational biology is a fundamental problem,\nwhich can be treated as the link prediction problem in graphs.\nC. CLUSTERING\nClustering is a traditional problem of machine learning.\nGraph clustering refers to nodes or graphs having similar\nafﬁliations or interests which are densely grouped together in\na cluster. There are numerous applications of node clustering\nfor community detection [4], text categorization [74], [79],\ncomputational biology, and recommender systems [132], etc.\nBecause nodes after embedding process are real-valued vec-\ntors, density-based clustering is able to leverage the vector\nto perform node clustering tasks in graphs [133], [134].\nFurthermore, graph clustering is a powerful tool for chemical\nanalysis. For example, it can be used to divide certain wines\nbased on its chemical analysis information from different\ncategories [135]. Similar to classiﬁcation, clustering also can\nbe utilized to evaluate the performance of representation\nlearning. In NRL algorithms [6], the empirical experiment\nshowed that it has achieved high performance compared with\nthe methods by using the metric of matching authors to the\nbelonging communities.\nD. GRAPH VISUALIZATION\nGraph visualization is a great way to help human understand\nand analyze sophisticated networks. The basic form of graph\nvisualization is to project high dimensional network data\ninto a 2D picture, where the same group nodes have the\nsame color, and different node categories can be easily distin-\nguished. For example, LINE [6] is able to visualize the same\ngroup authors in the same ﬁeld, and the data come from co-\nauthorship networks. There are several beneﬁts of graph vi-\nsualization. When a graph is visualized as a 2D image, it will\nbe easy to reveal the real intrinsic structure of graphs, such\nas discovering the hidden structure or ﬁnding communities.\nIn real applications, graph visualization has many varieties of\napplications through social science [136] [137] and biology\nvisualization [66]. Furthermore, similar nodes stay close to\neach other in the 2D visualization ﬁgure as well as node\nclustering. Several researchers utilized graph visualization to\npresent document categories in the visual form [67] [124].\nBecause node embedding associates with real-value vector,\nit often connects with dimension-reduced methods, such as\nPCA and t-SNE [138], or other traditional methods.\nWe visualize 20Newsgroups dataset3 using different NRL\nmethods, i.e., LE [89], LLE [16], LINE [6], SDNE [67], and\nHOPE [128]. Different colors of nodes in pictures represent\ndifferent classes. The results are shown in Fig 9. We can see\nthat nodes with the same colors are clustering together. That\nmeans network embedding could keep the original structural\ninformation of the graph.\nE. OTHER APPLICATIONS\nBesides the general applications mentioned above, there are\nstill a number of speciﬁc applications. Herein, we brieﬂy give\nsome examples in the following.\nGGS-NNs [78] is a feature learning technique for graph-\nstructured data. The novel characteristic of the algorithm is\nthat it can produce a sequence of outputs rather than a single\noutput. Different from the mentioned applications, this algo-\nrithm can be applied for BABI TASK, which contains 20 test-\ning basic forms of reasoning tasks, such as deduction, induc-\ntion, counting, and path-ﬁnding, etc. Duvenaud et al. [114]\nextended the application of NRL to predict the properties of\nnew molecules. The algorithm is proposed based on circular\nﬁngerprints and has a better predictive performance on a\nseries of tasks. Predicting protein function is an interesting\ntopic of bioinformatics, such as OhmNet [115] leveraging\nmulti-layers tissue networks to predict multicellular function,\nwhich achieved remarkable prediction accuracy. NRL is also\nan important way to detect community. For example, Li et\nal. [139] proposed a novel embedding based method for\ncommunity detection leveraging both attributes and struc-\nture information of graphs, Tu et al. [140] proposed uniﬁed\n3http://qwone.com/ jason/20Newsgroups/\nVOLUME 4, 2020\n13\nK. Sun et al.: Network Representation Learning\nFIGURE 9: Graph visualization of 20Newsgroups dataset.\nframework for community detection considering NRL and\ntext modeling.\nVI. OPEN ISSUES\nEven though NRL is a powerful and general technique for\ngraph analysis. There still remain lots of concrete open\nresearch problems.\nLarge-scale graphs modeling. According to our knowl-\nedge, few NRL algorithms can better handle large-scale\nnetworks, and most of them are just suitable for small-\nscale networks rather than that having hundreds of millions\nof nodes and links. With large-scale networks, researchers\nusually care about computational efﬁciency and how to com-\nbine heterogeneous network structure and multi-type node\ninformation. In addition, with the scale of graph increasing,\nthe reconstruction vectors of graphs are becoming vague\nor inaccurate. That is a big problem when dealing with\nclassiﬁcation of large-scale graphs, and we believe that how\nto deal with large-scale network embedding still needs further\nexploration within the domain of NRL in the future.\nModel depth. Deep learning has achieved a great per-\nformance improvement for image classiﬁcation, handwriting\nrecognition, etc., and has been widely applied for network\nrepresentation learning in recent years, such as Graph Neu-\nral Networks, Graph Convolutional Networks, and Graph\nAutoencoders etc. Although there are many deep learning-\nbased methods proposed, most of them are shallow models.\nToo many layers could cause an over-smooth problem and\ncould not fully extract features of graphs. We still lack\nefﬁcient network embedding approaches to cope with these\nproblems. There are some NRL works adopting ideas from\ndeep learning models, such as DenseNet, graph convolution\nof different scales. Deep graph learning is still an open issue\nfor researchers to further study.\nInterpretability. We know that many deep learning mod-\nels behave as black-boxes, which causes the problem of\ninterpretability. However, lots of graph learning methods\nderived from deep learning methods need interpretability. For\nexample, graph learning methods are for recommend system\nor decision-making system. There are few researchers to ad-\ndress this problem with graph learning. So, interpretability is\ncrucial and even more challenging as complex characteristics\nof graph data.\nRobustness training and adversarial attacks. Most NRL\nalgorithms rely on ideal graph-structured data. However, in\nmost cases, data are often truncated, missing, fuzzy, lopsided\nand we cannot get ideal information. Few papers are dis-\ncussing how to well handle robustness with deep learning-\nbased NRL methods. Even though some works involve GAN\nmechanism to enhance the robustness of embedding, these\nalgorithms are inefﬁcient and only suitable for speciﬁc net-\nworks. In addition, deep learning model is sensitive to adver-\nsarial attacks. So, these deep learning-based NRL methods\nare inherently unable to overcome the attack problem. In\nsummary, the robust graph learning techniques still need to\nbe further explored.\nVII. CONCLUSION\nIn this paper, we review the NRL algorithms including TFL\nmodels and deep learning-based models. These NRL algo-\nrithms can learn to reconstruct the representation of graphs.\nWe ﬁrst give a brief introduction about TFL and then sepa-\nrately discuss the NRL algorithms by focusing on the graph\nsources (edges and node attributes). There are too many\ncategories of NRL, and we mainly pay attention to these\ndeep learning-based models. We propose three taxonomies\nof graphs embedding from the deep learning perspective as\nshown in Table 2. The most important contributions are the\nparts that introduce NRL algorithms. Also, we summarize the\napplication of graphs embedding in the aspects of classiﬁca-\ntion and semi-supervised learning, link prediction, clustering,\netc. Finally, we emphasize that NRL has great promising\nfuture research directions in the ﬁeld of network science.\nREFERENCES\n[1] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” in ICLR Workshop Papers, 2013.\n[2] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Dis-\ntributed representations of words and phrases and their compositionality,”\nin Advances in Neural Information Processing Systems, 2013, pp. 3111–\n3119.\n[3] F. Morin and Y. Bengio, “Hierarchical probabilistic neural network lan-\nguage model.” in Proceedings of the International Workshop on Artiﬁcial\nIntelligence and Statistics, 2005, pp. 246–252.\n[4] S. Fortunato, “Community detection in graphs,” Physics Reports, vol.\n486, no. 3-5, pp. 75–174, 2010.\n[5] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for\nnetworks,” in Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining.\nACM, 2016,\npp. 855–864.\n14\nVOLUME 4, 2020\nK. Sun et al.: Network Representation Learning\n[6] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line:\nLarge-scale information network embedding,” in Proceedings of the 24th\nInternational Conference on World Wide Web. International World Wide\nWeb Conferences Steering Committee, 2015, pp. 1067–1077.\n[7] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learn-\ning on large graphs,” in Advances in Neural Information Processing\nSystems, 2017, pp. 1025–1035.\n[8] F. Xia, J. Wang, X. Kong, D. Zhang, and Z. Wang, “Ranking station im-\nportance with human mobility patterns using subway network datasets,”\nIEEE Trans. Intell. Transp. Syst., vol. 21, no. 7, pp. 2840–2852, 2019.\n[9] W. Wang, F. Xia, H. Nie, Z. Chen, Z. Gong, X. Kong, and W. Wei,\n“Vehicle trajectory clustering based on dynamic representation learning\nof internet of vehicles,” IEEE Trans. Intell. Transp. Syst., 2020.\n[10] S. Yu, F. Xia, K. Zhang, Z. Ning, J. Zhong, and C. Liu, “Team recognition\nin big scholarly data: Exploring collaboration intensity,” in 2017 IEEE\n15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th\nIntl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big\nData Intelligence and Computing and Cyber Science and Technology\nCongress (DASC/PiCom/DataCom/CyberSciTech).\nIEEE, 2017, pp.\n925–932.\n[11] D. Zhang, T. Guo, H. Pan, J. Hou, Z. Feng, L. Yang, H. Lin, and F. Xia,\n“Judging a book by its cover: the effect of facial perception on centrality\nin social networks,” in The World Wide Web Conference, 2019, pp. 2290–\n2300.\n[12] X. Kong, Y. Shi, S. Yu, J. Liu, and F. Xia, “Academic social networks:\nModeling, analysis, mining and applications,” Journal of Network and\nComputer Applications, vol. 132, pp. 86–103, 2019.\n[13] J. Xu, S. Yu, K. Sun, J. Ren, I. Lee, S. Pan, and F. Xia, “Multivariate\nrelations aggregation learning in social networks,” in Proceedings of the\nACM/IEEE Joint Conference on Digital Libraries in 2020, 2020, pp. 77–\n86.\n[14] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”\nChemometrics and Intelligent Laboratory Systems, vol. 2, no. 1-3, pp.\n37–52, 1987.\n[15] J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric\nframework for nonlinear dimensionality reduction,” Science, vol. 290, no.\n5500, pp. 2319–2323, 2000.\n[16] S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by\nlocally linear embedding,” Science, vol. 290, no. 5500, pp. 2323–2326,\n2000.\n[17] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, p. 436, 2015.\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in Neural Infor-\nmation Processing Systems, 2012, pp. 1097–1105.\n[19] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\ndata with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,\n2006.\n[20] T. Mikolov, M. Karaﬁát, L. Burget, J. ˇCernock`y, and S. Khudanpur,\n“Recurrent neural network based language model,” in Eleventh Annual\nConference of the International Speech Communication Association,\n2010, pp. 1045––1048.\n[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in\nAdvances in Neural Information Processing Systems, 2014, pp. 2672–\n2680.\n[22] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\nconvolutional networks,” in International Conference on Learning Rep-\nresentations (ICLR), 2016.\n[23] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang,\n“Heterogeneous network embedding via deep architectures,” in Proceed-\nings of the 21th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining.\nACM, 2015, pp. 119–128.\n[24] D. Zügner, A. Akbarnejad, and S. Günnemann, “Adversarial attacks\non neural networks for graph data,” in Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery and Data\nMining.\nACM, 2018, pp. 2847–2856.\n[25] W. Yu, C. Zheng, W. Cheng, C. C. Aggarwal, D. Song, B. Zong, H. Chen,\nand W. Wang, “Learning deep network representations with adversarially\nregularized autoencoders,” in Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining.\nACM, 2018, pp. 2663–2671.\n[26] Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson, “Structured\nsequence modeling with graph convolutional recurrent networks,” in\nInternational Conference on Neural Information Processing.\nSpringer,\n2018, pp. 362–373.\n[27] A. Bojchevski and S. Günnemann, “Adversarial attacks on node em-\nbeddings via graph poisoning,” in International Conference on Machine\nLearning, 2019, pp. 695–704.\n[28] M. R. Mendonca, A. Ziviani, and A. M. Barreto, “Graph-based skill ac-\nquisition for reinforcement learning,” ACM Computing Surveys (CSUR),\nvol. 52, no. 1, pp. 1–26, 2019.\n[29] W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on\ngraphs: Methods and applications,” IEEE Data Engineering Bulletin,\nvol. 40, no. 3, pp. 52–74, 2017.\n[30] P. Goyal and E. Ferrara, “Graph embedding techniques, applications, and\nperformance: A survey,” Knowledge-Based Systems, vol. 151, pp. 78–94,\n2018.\n[31] D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Network representation\nlearning: A survey,” IEEE Trans. Big Data, 2018.\n[32] P. Cui, X. Wang, J. Pei, and W. Zhu, “A survey on network embedding,”\nIEEE Trans. Knowl. Data Eng, vol. 31, no. 5, pp. 833–852, 2018.\n[33] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A\ncomprehensive survey on graph neural networks,” IEEE Trans. Neural\nNetw. Learn. Syst., 2020.\n[34] Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs: A survey,” IEEE\nTrans. Knowl. Data Eng., 2020.\n[35] D. Bacciu, F. Errica, A. Micheli, and M. Podda, “A gentle introduction to\ndeep learning for graphs,” Neural Networks, 2020.\n[36] W. Cao, Z. Yan, Z. He, and Z. He, “A comprehensive survey on geometric\ndeep learning,” IEEE Access, vol. 8, pp. 35 929–35 949, 2020.\n[37] C. Yang, Y. Xiao, Y. Zhang, Y. Sun, and J. Han, “Heterogeneous network\nrepresentation learning: Survey, benchmark, evaluation, and beyond,”\narXiv preprint arXiv:2004.00216, 2020.\n[38] Y. Xie, C. Li, B. Yu, C. Zhang, and Z. Tang, “A survey on dynamic\nnetwork embedding,” arXiv preprint arXiv:2006.08093, 2020.\n[39] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A\nreview and new perspectives,” IEEE Trans. Pattern Anal. Mach. Intell,\nvol. 35, no. 8, pp. 1798–1828, 2013.\n[40] K. Pearson, “Liii. on lines and planes of closest ﬁt to systems of points\nin space,” The London, Edinburgh, and Dublin Philosophical Magazine\nand Journal of Science, vol. 2, no. 11, pp. 559–572, 1901.\n[41] I. S. Dhillon, Y. Guan, and B. Kulis, “Kernel k-means: spectral clustering\nand normalized cuts,” in Proceedings of the tenth ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery and Data mining.\nACM,\n2004, pp. 551–556.\n[42] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component\nanalysis,” Journal of Computational and Graphical Statistics, vol. 15,\nno. 2, pp. 265–286, 2006.\n[43] E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal component\nanalysis?” Journal of the ACM, vol. 58, no. 3, p. 11, 2011.\n[44] N. Shahid, V. Kalofolias, X. Bresson, M. Bronstein, and P. Van-\ndergheynst, “Robust principal component analysis on graphs,” in Pro-\nceedings of the IEEE International Conference on Computer Vision,\n2015, pp. 2812–2820.\n[45] M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques\nfor embedding and clustering,” in Advances in Neural Information Pro-\ncessing Systems, 2002, pp. 585–591.\n[46] C.-M. Feng, Y.-L. Gao, J.-X. Liu, J. Wang, D.-Q. Wang, and C.-G.\nWen, “Joint-norm constraint and graph-laplacian pca method for feature\nextraction,” BioMed Research International, vol. 2017, 2017.\n[47] C.-M. Feng, Y.-L. Gao, J.-X. Liu, C.-H. Zheng, and J. Yu, “Pca based\non graph laplacian regularization and p-norm for gene selection and\nclustering,” IEEE Trans. Nanobiosci, vol. 16, no. 4, pp. 257–265, 2017.\n[48] D. Sun, H. Liang, M. Ge, Z. Ding, W. Cai, and B. Luo, “Protein functional\nannotation reﬁnement based on graph regularized l1-norm pca,” Pattern\nRecognition Letters, vol. 87, pp. 212–221, 2017.\n[49] N. Shahid, N. Perraudin, V. Kalofolias, G. Puy, and P. Vandergheynst,\n“Fast robust pca on graphs,” IEEE J. Sel. Topics Signal Process, vol. 10,\nno. 4, pp. 740–756, 2016.\n[50] M. Muja and D. G. Lowe, “Scalable nearest neighbor algorithms for high\ndimensional data,” IEEE Trans. Pattern Anal. Mach. Intell, no. 11, pp.\n2227–2240, 2014.\n[51] D. Shuman, S. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,\n“The emerging ﬁeld of signal processing on graphs: Extending high-\ndimensional data analysis to networks and other irregular domains,” IEEE\nSignal Processing Magazine, vol. 3, no. 30, pp. 83–98, 2013.\nVOLUME 4, 2020\n15\nK. Sun et al.: Network Representation Learning\n[52] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algo-\nrithm for linear inverse problems,” SIAM Journal on Imaging Sciences,\nvol. 2, no. 1, pp. 183–202, 2009.\n[53] M. Asteris, A. Kyrillidis, A. Dimakis, H.-G. Yi, and B. Chandrasekaran,\n“Stay on path: Pca along graph paths,” in International Conference on\nMachine Learning, 2015, pp. 1728–1736.\n[54] W. Min, J. Liu, and S. Zhang, “Edge-group sparse pca for network-\nguided high dimensional data analysis,” Bioinformatics, vol. 34, no. 20,\npp. 3479–3487, 2018.\n[55] T.-W. Lee, “Independent component analysis,” in Independent Compo-\nnent Analysis.\nSpringer, 1998, pp. 27–66.\n[56] B. Park, D.-S. Kim, and H.-J. Park, “Graph independent component\nanalysis reveals repertoires of intrinsic network components in the human\nbrain,” PloS one, vol. 9, no. 1, p. e82873, 2014.\n[57] D. López-Barroso, P. Ripollés, J. Marco-Pallarés, B. Mohammadi, T. F.\nMünte, A.-C. Bachoud-Lévi, A. Rodriguez-Fornells, and R. de Diego-\nBalaguer, “Multiple brain networks underpinning word learning from\nﬂuent speech revealed by independent component analysis,” Neuroimage,\nvol. 110, pp. 182–193, 2015.\n[58] D. Ribeiro de Paula, E. Ziegler, P. M. Abeyasinghe, T. K. Das, C. Cav-\naliere, M. Aiello, L. Heine, C. Di Perri, A. Demertzi, Q. Noirhomme\net al., “A method for independent component graph analysis of resting-\nstate fmri,” Brain and Behavior, vol. 7, no. 3, p. e00626, 2017.\n[59] M. E. Newman, “Spectral methods for community detection and graph\npartitioning,” Physical Review E, vol. 88, no. 4, p. 042822, 2013.\n[60] Y. Zhang, E. Levina, and J. Zhu, “Detecting overlapping communities in\nnetworks using spectral methods,” arXiv preprint arXiv:1412.3432, 2014.\n[61] Y. Li, F. Nie, H. Huang, and J. Huang, “Large-scale multi-view spectral\nclustering via bipartite graph,” in Twenty-Ninth AAAI Conference on\nArtiﬁcial Intelligence, 2015, pp. 2750–2756.\n[62] D. Pfau, S. Petersen, A. Agarwal, D. Barrett, and K. Stachenfeld, “Spec-\ntral inference networks: Unifying spectral methods with deep learning,”\narXiv preprint arXiv:1806.02215, 2018.\n[63] T. Trouillon, J. Welbl, S. Riedel, É. Gaussier, and G. Bouchard, “Complex\nembeddings for simple link prediction,” in International Conference on\nMachine Learning, 2016, pp. 2071–2080.\n[64] T. Trouillon, C. R. Dance, É. Gaussier, J. Welbl, S. Riedel, and\nG. Bouchard, “Knowledge graph completion via complex tensor factor-\nization,” The Journal of Machine Learning Research, vol. 18, no. 1, pp.\n4735–4772, 2017.\n[65] I. Borg and P. Groenen, “Modern multidimensional scaling: Theory and\napplications,” Journal of Educational Measurement, vol. 40, no. 3, pp.\n277–280, 2003.\n[66] M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional neural\nnetworks for graphs,” in International Conference on Machine Learning,\n2016, pp. 2014–2023.\n[67] D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,”\nin Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining.\nACM, 2016, pp. 1225–1234.\n[68] H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, and\nM. Guo, “Graphgan: graph representation learning with generative adver-\nsarial nets,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence,\n2018.\n[69] S. Kumar, X. Zhang, and J. Leskovec, “Predicting dynamic embedding\ntrajectory in temporal interaction networks,” in Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery &\nData Mining, 2019, pp. 1269–1278.\n[70] J. Liu, F. Xia, L. Wang, B. Xu, X. Kong, H. Tong, and I. King, “Shifu2:\nA network representation learning based model for advisor-advisee rela-\ntionship mining,” IEEE Trans. Knowl. Data Eng., 2019.\n[71] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning\nof social representations,” in Proceedings of the 20th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining.\nACM, 2014, pp. 701–710.\n[72] Q. Dai, Q. Li, J. Tang, and D. Wang, “Adversarial network embedding,”\nin Thirty-second AAAI conference on artiﬁcial intelligence, 2018.\n[73] H. Hong, X. Li, and M. Wang, “Gane: A generative adversarial network\nembedding,” IEEE Trans. Neural Netw. Learn. Syst., 2019.\n[74] S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph\nrepresentations,” in Thirtieth AAAI Conference on Artiﬁcial Intelligence,\n2016.\n[75] Z. Yang, W. W. Cohen, and R. Salakhutdinov, “Revisiting semi-\nsupervised learning with graph embeddings,” in Proceedings of the\n33rd International Conference on International Conference on Machine\nLearning-Volume 48.\nJMLR. org, 2016, pp. 40–48.\n[76] C. Tu, Z. Zhang, Z. Liu, and M. Sun, “Transnet: translation-based net-\nwork representation learning for social relation extraction,” in Proceed-\nings of International Joint Conference on Artiﬁcial Intelligence (IJCAI),\nMelbourne, 2017.\n[77] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially\nregularized graph autoencoder for graph embedding,” in Proceedings of\nTwenty-Seventh International Joint Conference on Artiﬁcial Intelligence,\n2018, p. 2609–2615.\n[78] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph sequence\nneural networks,” in International Conference on Learning Representa-\ntions (ICLR), 2016.\n[79] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural\nnetworks on graphs with fast localized spectral ﬁltering,” in Advances in\nNeural Information Processing Systems, 2016, pp. 3844–3852.\n[80] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec,\n“Hierarchical graph representation learning with differentiable pooling,”\nin Advances in Neural Information Processing Systems, 2018, pp. 4800–\n4810.\n[81] Z. Zhang, J. Bu, M. Ester, J. Zhang, C. Yao, Z. Yu, and C. Wang,\n“Hierarchical graph pooling with structure learning,” arXiv preprint\narXiv:1911.05954, 2019.\n[82] J. Lee, I. Lee, and J. Kang, “Self-attention graph pooling,” arXiv preprint\narXiv:1904.08082, 2019.\n[83] F. Xia, J. Liu, H. Nie, Y. Fu, L. Wan, and X. Kong, “Random walks:\nA review of algorithms and applications,” IEEE Trans. Emerg. Topics\nComput. Intell., vol. 4, no. 2, pp. 95–107, 2019.\n[84] Y. Goldberg and O. Levy, “word2vec explained: Deriving mikolov\net al.’s negative-sampling word-embedding method,” arXiv preprint\narXiv:1402.3722, 2014.\n[85] O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix\nfactorization,” in Advances in Neural Information Processing Systems,\n2014, pp. 2177–2185.\n[86] X. Rong, “word2vec parameter learning explained,” arXiv preprint\narXiv:1411.2738, 2014.\n[87] J. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang, “Network embedding\nas matrix factorization: Unifying deepwalk, line, pte, and node2vec,” in\nProceedings of the 11th ACM International Conference on Web Search\nand Data Mining, 2018, pp. 459–467.\n[88] R. Salakhutdinov and G. Hinton, “Semantic hashing,” International\nJournal of Approximate Reasoning, vol. 50, no. 7, pp. 969–978, 2009.\n[89] M. Belkin and P. Niyogi, “Laplacian eigenmaps for dimensionality\nreduction and data representation,” Neural Computation, vol. 15, no. 6,\npp. 1373–1396, 2003.\n[90] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,\n“Stacked denoising autoencoders: Learning useful representations in a\ndeep network with a local denoising criterion,” Journal of Machine\nLearning Research, vol. 11, no. Dec, pp. 3371–3408, 2010.\n[91] F. Feng, X. He, J. Tang, and T.-S. Chua, “Graph adversarial training:\nDynamically regularizing based on graph structure,” IEEE Trans. Knowl.\nData Eng, 2019.\n[92] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representa-\ntion learning with rich text information,” in Proceedings of International\nJoint Conference on Artiﬁcial Intelligence (IJCAI), 2015, pp. 2111–2117.\n[93] Y. Dong, N. V. Chawla, and A. Swami, “metapath2vec: Scalable rep-\nresentation learning for heterogeneous networks,” in Proceedings of the\n23rd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining.\nACM, 2017, pp. 135–144.\n[94] X. Wang, H. Ji, C. Shi, B. Wang, P. Cui, P. Yu, and Y. Ye, “Heterogeneous\ngraph attention network,” arXiv preprint arXiv:1903.07293, 2019.\n[95] C. Zhang, D. Song, C. Huang, A. Swami, and N. V. Chawla, “Het-\nerogeneous graph neural network,” in Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discovery & Data\nMining, 2019, pp. 793–803.\n[96] X. Wang, Y. Lu, C. Shi, R. Wang, P. Cui, and S. Mou, “Dynamic\nheterogeneous information network embedding with meta-path based\nproximity,” IEEE Trans. Knowl. Data Eng, 2020.\n[97] Z. Li, W. Zheng, X. Lin, Z. Zhao, Z. Wang, Y. Wang, X. Jian, L. Chen,\nQ. Yan, and T. Mao, “Transn: Heterogeneous network representation\nlearning by translating node embeddings,” in 2020 IEEE 36th Interna-\ntional Conference on Data Engineering (ICDE).\nIEEE, 2020, pp. 589–\n600.\n16\nVOLUME 4, 2020\nK. Sun et al.: Network Representation Learning\n[98] C. Shi, Y. Lu, L. Hu, Z. Liu, and H. Ma, “Rhine: Relation structure-aware\nheterogeneous information network embedding,” IEEE Trans. Knowl.\nData Eng., 2020.\n[99] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and\nlocally connected networks on graphs,” in International Conference on\nLearning Representations (ICLR), 2014.\n[100] B. L. Douglas, “The weisfeiler-lehman method and graph isomorphism\ntesting,” arXiv preprint arXiv:1101.5211, 2011.\n[101] X. Zhu, Z. Ghahramani, and J. D. Lafferty, “Semi-supervised learning\nusing gaussian ﬁelds and harmonic functions,” in Proceedings of the 20th\nInternational Conference on Machine Learning (ICML-03), 2003, pp.\n912–919.\n[102] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional\nnetworks for semi-supervised learning,” in Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence, 2018.\n[103] J. Chen and J. Zhu, “Stochastic training of graph convolutional net-\nworks,” arXiv preprint arXiv:1710.10568, 2017.\n[104] J. Chen, T. Ma, and C. Xiao, “Fastgcn: fast learning with graph\nconvolutional networks via importance sampling,” arXiv preprint\narXiv:1801.10247, 2018.\n[105] T. Derr, Y. Ma, and J. Tang, “Signed graph convolutional networks,” in\n2018 IEEE International Conference on Data Mining (ICDM).\nIEEE,\n2018, pp. 929–934.\n[106] Y. Ma, S. Wang, C. C. Aggarwal, D. Yin, and J. Tang, “Multi-dimensional\ngraph convolutional networks,” in Proceedings of the 2019 SIAM Inter-\nnational Conference on Data Mining.\nSIAM, 2019, pp. 657–665.\n[107] G. Li, M. Müller, A. Thabet, and B. Ghanem, “Can gcns go as deep as\ncnns?” arXiv preprint arXiv:1904.03751, 2019.\n[108] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov,\nand M. Welling, “Modeling relational data with graph convolutional\nnetworks,” in European Semantic Web Conference.\nSpringer, 2018, pp.\n593–607.\n[109] L. Cai, B. Yan, G. Mai, K. Janowicz, and R. Zhu, “Transgcn: Coupling\ntransformation assumptions with graph convolutional networks for link\nprediction,” in Proceedings of the 10th International Conference on\nKnowledge Capture, 2019, pp. 131–138.\n[110] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko,\n“Translating embeddings for modeling multi-relational data,” in Ad-\nvances in Neural Information Processing Systems, 2013, pp. 2787–2795.\n[111] P. Wang, J. Han, C. Li, and R. Pan, “Logic attention based neighborhood\naggregation for inductive knowledge graph embedding,” in Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, vol. 33, 2019, pp.\n7152–7159.\n[112] B. Hu, Y. Fang, and C. Shi, “Adversarial learning on heterogeneous\ninformation networks,” in Proceedings of the 25th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery & Data Mining, 2019, pp.\n120–129.\n[113] A. Grover, A. Zweig, and S. Ermon, “Graphite: Iterative generative\nmodeling of graphs,” in International Conference on Machine Learning,\n2019, pp. 2434–2444.\n[114] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,\nA. Aspuru-Guzik, and R. P. Adams, “Convolutional networks on graphs\nfor learning molecular ﬁngerprints,” in Advances in Neural Information\nProcessing Systems, 2015, pp. 2224–2232.\n[115] M. Zitnik and J. Leskovec, “Predicting multicellular function through\nmulti-layer tissue networks,” Bioinformatics, vol. 33, no. 14, pp. i190–\ni198, 2017.\n[116] R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and\nU. Alon, “Network motifs: simple building blocks of complex networks,”\nScience, vol. 298, no. 5594, pp. 824–827, 2002.\n[117] L. Wang, J. Ren, B. Xu, J. Li, W. Luo, and F. Xia, “Model: Motif based\nnetwork embedding for link prediction,” IEEE Trans. Computat. Social\nSyst., vol. 7, no. 2, pp. 503–516, 2020.\n[118] S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M.\nBorgwardt, “Graph kernels,” Journal of Machine Learning Research,\nvol. 11, no. Apr, pp. 1201–1242, 2010.\n[119] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,\n“The graph neural network model,” IEEE Transactions on Neural Net-\nworks, vol. 20, no. 1, pp. 61–80, 2009.\n[120] K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y. Bengio, “Learning phrase representations using rnn\nencoder-decoder for statistical machine translation,” Computer Science,\n2014.\n[121] F. R. Chung and F. C. Graham, Spectral graph theory.\nAmerican\nMathematical Soc., 1997, no. 92.\n[122] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on\ngraphs via spectral graph theory,” Applied and Computational Harmonic\nAnalysis, vol. 30, no. 2, pp. 129–150, 2011.\n[123] I. S. Dhillon, Y. Guan, and B. Kulis, “Weighted graph cuts without\neigenvectors a multilevel approach,” IEEE Trans. Pattern Anal. Mach.\nIntell, vol. 29, no. 11, 2007.\n[124] S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network\nrepresentation,” in International Joint Conference on Artiﬁcial Intelli-\ngence, 2016, pp. 1895–1901.\n[125] X. Hu, X. Hu, and X. Hu, “Label informed attributed network embed-\nding,” in Tenth ACM International Conference on Web Search and Data\nMining, 2017, pp. 731–739.\n[126] T. D. Bui, S. Ravi, and V. Ramavajjala, “Neural graph machines: Learn-\ning neural networks using graphs,” arXiv preprint arXiv:1703.04818,\n2017.\n[127] P. Yanardag and S. Vishwanathan, “Deep graph kernels,” in Proceedings\nof the 21th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining.\nACM, 2015, pp. 1365–1374.\n[128] M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity\npreserving graph embedding,” in The ACM SIGKDD International Con-\nference, 2016, pp. 1105–1114.\n[129] W. Wang, J. Liu, F. Xia, I. King, and H. Tong, “Shifu: Deep learning\nbased advisor-advisee relationship mining in scholarly big data,” in\nProceedings of the 26th International Conference on World Wide Web\nCompanion.\nInternational World Wide Web Conferences Steering\nCommittee, 2017, pp. 303–310.\n[130] R. v. d. Berg, T. N. Kipf, and M. Welling, “Graph convolutional matrix\ncompletion,” arXiv preprint arXiv:1706.02263, 2017.\n[131] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich, “A review of\nrelational machine learning for knowledge graphs,” Proceedings of the\nIEEE, vol. 104, no. 1, pp. 11–33, 2016.\n[132] C. Shi, B. Hu, W. X. Zhao, and S. Y. Philip, “Heterogeneous information\nnetwork embedding for recommendation,” IEEE Trans. Knowl. Data\nEng, vol. 31, no. 2, pp. 357–370, 2019.\n[133] S. Cao, W. Lu, and Q. Xu, “Grarep: Learning graph representations with\nglobal structural information,” in Proceedings of the 24th ACM Inter-\nnational on Conference on Information and Knowledge Management.\nACM, 2015, pp. 891–900.\n[134] S. Wang, J. Tang, F. Morstatter, and H. Liu, “Paired restricted boltzmann\nmachine for linked data,” in ACM International on Conference on Infor-\nmation and Knowledge Management, 2016, pp. 1753–1762.\n[135] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu, “Learning deep repre-\nsentations for graph clustering,” in Twenty-Eighth AAAI Conference on\nArtiﬁcial Intelligence, 2014.\n[136] B. P. Chamberlain, J. Clough, and M. P. Deisenroth, “Neural embeddings\nof graphs in hyperbolic space,” arXiv preprint arXiv:1705.10359, 2017.\n[137] L. Freeman, “Visualizing social networks,” Social Network Data Analyt-\nics, vol. 6, no. 4, pp. 411–429, 2000.\n[138] V. D. M. Laurens and G. E. Hinton, “Visualizing data using t-SNE,”\nJournal of Machine Learning Research, vol. 9, no. 2605, pp. 2579–2605,\n2008.\n[139] Y. Li, C. Sha, X. Huang, and Y. Zhang, “Community detection in\nattributed graphs: An embedding approach,” in Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[140] C. Tu, X. Zeng, H. Wang, Z. Zhang, Z. Liu, M. Sun, B. Zhang, and\nL. Lin, “A uniﬁed framework for community detection and network\nrepresentation learning,” IEEE Trans. Knowl. Data Eng, vol. 31, 2019.\nVOLUME 4, 2020\n17\n",
  "categories": [
    "cs.SI",
    "cs.LG"
  ],
  "published": "2021-03-07",
  "updated": "2021-03-07"
}