{
  "id": "http://arxiv.org/abs/2103.04780v1",
  "title": "A Dual-Memory Architecture for Reinforcement Learning on Neuromorphic Platforms",
  "authors": [
    "Wilkie Olin-Ammentorp",
    "Yury Sokolov",
    "Maxim Bazhenov"
  ],
  "abstract": "Reinforcement learning (RL) is a foundation of learning in biological systems\nand provides a framework to address numerous challenges with real-world\nartificial intelligence applications. Efficient implementations of RL\ntechniques could allow for agents deployed in edge-use cases to gain novel\nabilities, such as improved navigation, understanding complex situations and\ncritical decision making. Towards this goal, we describe a flexible\narchitecture to carry out reinforcement learning on neuromorphic platforms.\nThis architecture was implemented using an Intel neuromorphic processor and\ndemonstrated solving a variety of tasks using spiking dynamics. Our study\nproposes a usable energy efficient solution for real-world RL applications and\ndemonstrates applicability of the neuromorphic platforms for RL problems.",
  "text": " \n \n \n \nA Dual-Memory Architecture for Reinforcement \nLearning on Neuromorphic Platforms \nWilkie Olin-Ammentorp1,2, Yury Sokolov1, Maxim Bazhenov1,2* \n1 Department of Medicine, University of California, San Diego \n2 Institute for Neural Computation, University of California, San Diego \n* Correspondence:  \nMaxim Bazhenov \nmbazhenov@health.ucsd.edu \nAbstract: \nReinforcement learning (RL) is a foundation of learning in biological systems and provides a \nframework to address numerous challenges with real-world artificial intelligence applications. \nEfficient implementations of RL techniques could allow for agents deployed in edge-use cases \nto gain novel abilities, such as improved navigation, understanding complex situations and \ncritical decision making. Towards this goal, we describe a flexible architecture to carry out \nreinforcement learning on neuromorphic platforms. This architecture was implemented using \nan Intel neuromorphic processor and demonstrated solving a variety of tasks using spiking \ndynamics. Our study proposes a usable energy efficient solution for real-world RL applications \nand demonstrates applicability of the neuromorphic platforms for RL problems. \nIntroduction: \nAs the number of data-collecting devices increases, so too does the need for efficient data \nprocessing. Rather than require all data collected from remote devices be processed at a \ncentral location, the need for data processing to be performed in-situ is becoming a priority; \nthis is especially true in situations where ‘agents’ collecting data may need to make critical \ndecisions based on these inputs with low latency (such as in self-driving cars or aerial drones). \nFor such use cases, efficiency of data processing becomes paramount, as energy sources and \nphysical space (‘size, weight, and power’) come at a premium1.  \nNeuromorphic architectures provide one path towards meeting this need. Although there is no \nuniversal definition on what constitutes a neuromorphic architecture, these systems generally \naim to provide efficient, massively-parallel processing schemes which often use binary ‘spikes’ \nto transmit information2. Given that a clear definition of a neuromorphic architecture is not yet \nuniversally agreed upon, it is difficult to design a single program which can be compiled to any \nneuromorphic system (as is the case with standard computer architectures). But, by restricting \nourselves to the massively-parallel operations which are a common feature of almost all \nneuromorphic systems, we can create a program which likely be adapted to any platform which \n \n \n \n \nmeets the emerging definition of what constitutes a neuromorphic system3. In this work, we \nutilize Intel’s neuromorphic processor codenamed ‘Loihi.’4 \nReinforcement learning (RL) represents a native way how biological systems learn. Instead of \nbeing trained before deployment by massive amounts of labeled data, humans and animals \nlearn continuously from experience by updating a policy based on continuously collected data. \nThis requires learning to occur in-situ rather than depending on slow and costly uploading of \nnew data to a central location where new information would be embedded to the previously \ntrained model, followed by downloading the new model to the agent.  \nTowards these objectives, we describe a high-level system for carrying out RL tasks which is \ninspired by several principles of biological computation, particularly by complementary learning \nsystems theory5, postulating that learning new memories in the brain depends on mutual \ninteraction between cortical and hippocampal networks. We show that such ‘dual-memory \nlearner’ (DML) can implement methods which can approach an optimal solution to an RL \nproblem. The DML architecture is then implemented in a spiking manner and executed on \nIntel’s Loihi processor. We demonstrate it solving the classic multi-arm bandit problem, as well \nas more advanced tasks such as navigation through a maze, and the card game Blackjack. To \nour knowledge, these advanced multi-step problems have not previously been demonstrated \nbeing solved solely by a neuromorphic system. We characterize the performance of its current \nimplementation, comment on its characteristics and limitations, and describe improvements \nwhich it can undergo in future work. \nResults: \nDual-memory learner (DML) framework \nMonte Carlo (MC) methods provide well-characterized RL techniques for learning optimal \npolicies via episodic experiences; the agent does not need to be equipped with a full model of \nhow the environment will react to its actions in order to learn. Instead, the agent tracks which \nstates it has entered, the actions it has taken, and once an episode concludes, updates its value \nestimates given its trajectory through the state space. This provides a simple but effective basis \nfor reinforcement learning, and we focus on implementing this method in our architecture \n(though it can also be extended to more modern temporal difference (TD) and n-step \nalgorithms).6 \n \n \n \n \n \nFigure 1: Biological inspiration for the dual-memory learner. \nWe propose a dual-memory learner (DML) framework, which mimics the high-level \norganization of learning function in the biological brain, so called complimentary learning \nsystems theory (Figure 1)5 to implement an MC learning technique using spiking networks. The \nproposed DML architecture contains four major sections which process and store the \ninformation required to carry out reinforcement learning on a neuromorphic platform (Figure \n2).  \n \nFigure 2: A high-level overview of the dual-memory learner. Each module is responsible for a sub-step of the evaluation required \nto choose an action which leads to the next step in a Markov chain process. \nThe fundamental epoch or ‘step’ in an RL agent modeling a Markov decision process requires \nseveral sub-steps: recognizing the agent’s current state, using this information to decide on an \nappropriate action given the current policy, returning this action to the environment in a \nmeaningful manner, and possibly applying reward signals to update internal value estimates \nand policy. We define specific modules and/or interactions to address each of these \nrequirements, forming the core structure of the DML and allowing it to be implemented via \n \n \n \n \nparallelized and local operations. The four modules we define are the decoder, short-term \nmemory (STM), long-term memory (LTM), and encoder (Figure 2).  \nArchitectural Implementation \nOne of the crucial aspects of a neuromorphic system is the question of how information is to be \nrepresented, particularly when all information must be encoded in ‘spikes,’ the binary all-or-\nnothing signal which is responsible for almost all information transfer between neurons in the \nhuman brain7. In this initial implementation, the convention that all information is rate-coded is \nused. While rate-coding can be costly compared to other encoding strategies and is likely not \nemployed in many regions of the brain8,9, we use it here due to its easy interpretability and \nfunctionality. Using this assumption and following the previously laid-out requirements, we \nindependently demonstrate the operation of each module constituting the DML. With the sole \nexception of the final encoder module, all modules are implemented entirely using spiking logic \nwhich run in the massively parallel ‘Neurocores’ of the Loihi architecture4. \n \nFigure 3: Overview of each module's purpose and implementation. A) The encoder provides an internal representation of the \nagent’s current state in the task (e.g. the current location of an agent in a maze). B) The short-term memory (STM) stores the \ncurrent episode’s trajectory through state space by storing successive state-action pairs (e.g. previous locations and moves in a \nmaze). C) The long-term memory (LTM) provides estimates of values for each state-action pair in the problem (e.g. the reward \nexpected after moving a certain direction in the maze). D) The encoder chooses an action given the current state and returns it \nto the environment by reading the appropriate value estimates from the LTM (e.g. finding and taking the cardinal direction in \nthe maze with the highest value).  \nDecoder \nIn the simplest case, the decoder provides a simple integer value which corresponds to a \nunique state in the problem. Signals from the environment indicating a change in state are \n \n \n \n \nassumed to be sparse in time, and the decoder must take these sparse signals and expand them \ninto a constant internal representation (Figure 3a). This is done by creating a series of bistable \nneurons which provide a one-hot representation of the current state (e.g. if an agent in a maze \nis in the fourth possible position, its fourth neuron in the encoder will be active).  \nShort-Term Memory (STM) \nWhen combined with a similar one-hot representation of the action taken following the current \nstate, the trajectory of the agent through each episode can be built by taking the outer-product \nof the decoder’s state representation and the consequent action from the encoder (Figure 3b). \nAgain, bistable neurons are used which are activated once a state-action pair has been \ntraversed, and are reset once an episode is complete (e.g. in a maze, an STM neuron becomes \nactive if the agent entered a location and made a specific movement). In all examples \npresented here, the end of an episode is indicated by a signal from the environment, optionally \naccompanied by a positive or negative reward. \nLong-Term Memory (LTM) \nWithin the LTM, the binary trajectory and reward signals must be converted into graded value \nrepresentations. This is currently done by maintaining a tabular array of value estimates for \neach state-action pair. This avoids requiring a function approximator, the training and \nimplementation of which is in itself currently an area of intense study within neuromorphic \ncomputing10,11.  \nTo represent a single value estimate, a circuit of several neurons is used. The dynamics in this \n‘value circuit’ (VC) are configured in a manner which allows the output compartment to \nconverge to a firing rate which corresponds to the proportion of reward signals the circuit \nreceives out of all reinforcement signals. Without new reinforcement signals, the VC maintains \nits current firing rate. The exact details of VC and a proof of its convergence are provided in the \nmaterials.  \nAn array of VCs represents the expected returns for all state-action pairs following the current \npolicy (Figure 3c). Agent policy is formed from a simple greedy or ε-greedy choice on action \nvalue estimates given the current state. At the end of an episode, these value expectations are \nupdated by using the trajectory stored in the STM to route reward signals to their \ncorresponding VCs. These signals then can incrementally adjust each VC as necessary, creating \nnew value estimates and allowing new policies to be derived. \nEncoder \nIn order to select an action appropriate to the current state, the encoder uses the information \nfrom the decoder to filter the output of the LTM (Figure 3d). Reading only the outputs of the \nLTM which correspond to actions possible at the current state, the decoder chooses the action \nwith the highest value (greedy), or optionally, may instead choose a random action with a set \nprobability (ε-greedy). Currently, the required argmax and random selection operations are \n \n \n \n \ndone via the x86 processor co-integrated on the Loihi chip, though they may be replaced via a \nwinner-take-all (WTA) or noisy WTA circuit for a purely spiking implementation of the DML12.  \nModular Integration \nHaving demonstrated the individual operation of each DML module, the remaining challenge is \nto integrate these modules into a system which works in concert to implement the full DML. \nAdditionally, this should be achieved by describing modules at a high level of abstraction, \nallowing the solution to automatically scale with the problem at hand and preventing it from \nbecoming burdensome to end users who may wish to deploy the program into new scenarios. \nThis remains a challenge in neuromorphic systems where ‘completeness’ is debatable and \ncompilation of arbitrary programs to end platforms may not always be feasible3.  \nWe maintain high levels of abstraction in our program integration by describing neuronal \ncircuits and hierarchies of circuits in terms of computational graphs. The elements of these \ngraphs are nodes with arbitrary dimensions, linked with stereotyped connectivity patterns and \npredefined excitability characteristics. These circuits can then be easily scaled to the given \nproblem at hand and compiled down into the individual compartments and synaptic \nconnections required to define an executable program on a platform such as Loihi. The block \ndiagrams representing these graphs are provided in the methods. \nThis high-level organization allows the RC-DML to address a variety of different problems while \nrequiring minimal amounts of code updates and maintaining executability on neuromorphic \nhardware. The only code changes required to allow the agent to address different problems are \nroutines which update the simulation of the environment and control communication between \nthe agent, environment, and host computer. The full source code for the RC-DML on Loihi is \nopenly available (see methods). \nProblem Solving \nMulti-Arm Bandit \nAs a basic demonstration of its capabilities, we first apply the rate-coded DML (RC-DML) to the \nmulti-arm bandit (MAB) problem. While it does not incorporate the concept of ‘state’ into a \nproblem, the MAB is itself a complex problem which addresses many fundamental aspects of \nRL6. In the MAB, a series of ‘arms’ is presented to the agent, each with a hidden true parameter \nwhich controls the probability a reward is obtained when that arm is ‘pulled13.’ Here, the \nfundamental learning problem is to find which arm gives the highest reward using the smallest \nnumber of interactions with the bandit in order to maximize cumulative reward. \nWe demonstrate RC-DML using an ε-greedy algorithm to solve the MAB. As the problem is \nstateless, it is simply indicated to the system that it remains in the same (singular) state after \nevery action. The value estimates learned for each action in this state are then used to estimate \nthe reward for each arm (Figure 4a). Using an ε-greedy policy, the RC-DML is forced to explore \neach arm and eventually converges on selecting the correct arm (Figure 4b, c).  \n \n \n \n \n \nFigure 4: Results from the RC-DML solution to the multi-arm bandit problem. A) The parameters of the Bernoulli distributions \nused to draw rewards after the pull of an arm. B) The choices of the RC-DML over 2000 epochs (arm pulls). C) The mean optimal \naction (MOA) of the RC-DML bandit, averaged over 100 epochs. D) The MOA of the bandit as the length of its rate coded \nrepresentations is varied. Bars indicate standard deviation (n=5). E) Comparison of the MOA between the RC-DML executing on \nLoihi and a traditional CPU-based ε-greedy algorithm. F) Further comparisons between the RC-DML and CPU ε-greedy \nalgorithms measuring MOA and cumulative reward as ε is varied. Bars indicate standard deviation (n=5). \nAs long the length of the period used to rate-code values lies above approximately 40 steps, the \nneuromorphic RC-DML demonstrates learning performance for the MAB which is comparable \nto that of a traditional, non-spiking ε-greedy algorithm running on a CPU (Figure 4d, e). The \nCPU-based algorithm maintains a small performance lead in proportion of optimal actions and \nthe mean average reward over the first 6400 epochs of learning over a variety of ε values \n(Figure 4f). This is due to the limited accuracy of value representation in the RC-DML due to its \nuse of rate-coding, which later limits its performance at Blackjack. No such obstacles are \npresent in the CPU-based algorithm which uses 64-bit floating point representations of value. \nDynamic Maze \nNavigation is a long-studied problem with many practical applications, such as robotic cleaners \nand self-driving vehicles. We focus on a dynamic maze task to evaluate the RC-DML’s ability to \nlearn navigation patterns. In this dynamic maze, a target location within a rectangular grid of \npoints provides a reward. If the agent can reach this reward within a set number of moves, it is \nrewarded. During this time, the only information provided to the agent is its current location \nwithin the maze. It cannot ‘see’ walls, but infers their presence by detecting its location remains \nunchanged after attempting to move in a certain direction. If the agent fails to reach the goal \n \n \n \n \nwithin a set number of steps, it is punished and a new episode begins. The multi-state nature of \nnavigation requires the RC-DML to demonstrate that it can accurately incorporate an episodic \nmemory into its learning process. Additionally, the RC-DML here employs a greedy policy \ncombined with a random starting location to force exploration.  \n \nFigure 5: Results from the RC-DML applied to a navigation task. A) First, the agent must learn to navigate to a reward at the \ncenter of the rectangular arena. Arrows indicate the direction in which the policy will guide the agent. B) After it has learned this \nsimple navigation task, walls are introduced which only allow the reward to be approached from the North; a new policy must \nbe learned to overcome this change. C) Finally, the reward is moved from its location at the center to the bottom of the arena. \nValue estimates across the entire arena must be re-learned to form an optimal policy which overcomes this change. D) The \naverage returns for the RC-DML agent over 100 epochs are plotted. This metric is calculated separately for each configuration of \nthe arena. \nGiven the consistent nature of rewards, the RC-DML can quickly learn the location of the \nreward within a 5 by 5 grid and converge on a policy which navigates to it from all starting \nlocations within the time limit of 8 steps (Figure 5a). When walls are introduced within the \nmaze to block off the reward from 3 sides, the previous navigation policy conflicts with the new \n \n \n \n \nconstraints imposed by the environment. But as the value estimates within the RC-DML are \neffectively created by a running average of previous returns, it gradually overcomes this faulty \npolicy as the new experiences gained from the environment update value estimates and allow a \nbetter policy to emerge which correctly navigates to the reward (Figure 5b).  \nFinally, the reward is placed at a new location within the maze. This requires a greater shift in \nthe navigational policy than when the previous reward location is blocked off, and the RC-DML \ntakes a longer time to adjust to this new environment, but once again the optimal policy is \nreached with experience (Figure 5c,d).  \nBlackjack \nThe card game Blackjack contains elements of random chance which makes it challenging to \nquickly converge to the optimal policy. In the variation of Blackjack we implement, the goal is to \nobtain a hand of cards with values summing as close as possible to 21 without going over. Each \npip card (2-9) has a value equivalent to its number, face cards are worth 10 points, and the ace \ncan count as either 1 or 11. To begin play, a card is dealt to both the dealer and the player. \nFrom there, the player chooses to either receive a new card from the dealer (‘hit’) or stay with \ntheir current sum (‘stick’). The dealer then follows a fixed policy to draw cards until their sum is \n17 or greater. If either the player is closer to 21 or the dealer goes over 21 during their turn, the \nplayer wins. If both player and dealer are equally close to 21, the game is a draw. Otherwise, \nthe dealer wins and the player loses. \nTo solve this problem, the RC-DML again uses a greedy policy combined with exploring starts. \nThe state of the player in blackjack is entirely determined by the current sum of the player’s \ncards (player sum), whether this sum can be modified by counting an in-hand ace as either 1 or \n11 without exceeding 21 (usable ace), and the value of the dealer’s visible card (dealer \nshowing). Cards are drawn from an infinite deck, therefore there is no hidden state within the \ndealing process to consider.  \n \n \n \n \n \nFigure 6: Results from applying the RC-DML to playing Blackjack. A) Difference in the value between sticking & hitting under an \noptimal policy evaluated on CPU and (B) the optimal policy formed by applying a greedy choice to these values. C) Difference in \nthe value between sticking & hitting after 1 million episodes of learning with the RC-DML on Loihi and (D) the policy formed from \nthese values. E) Jensen-Shannon (JS) divergence between each agent’s value representations and those under the optimal policy \n(taken after 2 billion episodes of learning on CPU). The coarse representation of values within the RC-DML limit its performance \nin this task and prevent it from reaching the optimal policy. However, its performance is still much better than random chance. \nF) The difference in policies between CPU and RC-DML corresponds to states where the difference in value between actions is \nsmall and requires fine representation. \nAlthough the RC-DML converges onto a policy which is quantifiably better than random chance \nand does not make obvious mistakes (such as drawing a new card when the player sum is \nalready close to 20), a gap remains between its performance and that of an equivalent MC \nalgorithm on CPU (Figure 6a-e). As previously mentioned, the limitations of rate-coding value \nestimates are the main source of this performance gap. Compared to the previous navigation \nproblem, learning an optimal Blackjack policy requires much more fine-grained representations \nof value (Figure 6f). \n \n \n \n \nDiscussion: \nWe have presented a novel framework for solving reinforcement learning (RL) problems using \nspiking neuromorphic hardware. This method implements critical elements and principles of \nthe complimentary learning systems theory which was proposed to explain declarative memory \nlearning in the biological brain. The method was successfully applied to three classic RL \nproblems - Multi-Arm Bandit, Dynamic Maze, and Blackjack - using Intel’s Loihi neuromorphic \nprocessor. We found that Loihi based implementation presents similar performance to the CPU \nbased algorithms unless high precision was required to successfully learn a policy. While the \nactive power consumption of a Loihi chip running the algorithm was much lower than that of a \ntraditional CPU, for more complex problems, such as Blackjack, the faster execution rate of an \nequivalent Monte Carlo program on a CPU gave it an advantage in the overall energy \nconsumption. The last limitation was found to be a result of rate-coding implementation of the \ninformation processing, that we used here due to its easy interpretability and functionality. \nPreliminary analysis revealed that using a different representation of information could succeed \nin making the proposed RL implementation competitive with traditional CPU solutions both in \nterms of learning capability and energy efficiency. \nReinforcement learning in machine learning solutions \nSince its formal emergence in the mid-20th century, reinforcement learning (RL) has grown to \nprovide a variety of techniques which can provably converge on optimal solutions for complex \nproblems6. With the addition of modern deep-learning techniques, RL has surpassed human \nperformance on a variety of problems, including the classic games Chess, Shogi, and Go14. Due \nto its ability to conquer these complex scenarios, RL has many possible applications in real-\nworld problems including autonomous driving, process control, and interpreting biological \ndata15–17. However, deploying RL in these and other edge-case situations requires that it be \nimplemented in an efficient manner1. The desire for low-power implementations of RL, \ncombined with the theory that neural circuits carry out some form of RL, has motivated several \nprevious efforts towards implementing RL on neuromorphic platforms 6,18–20. \nThe fundamental problem which RL solves is providing an ‘agent’ in an environment a way of \nupdating its internal models to choose an action at each state of the problem which leads to a \nmaximized cumulative reward, or ‘return.’ Generally, this is done by iteratively improving the \nagent’s value estimates, which are then applied to form a new policy to guide actions. Value \nestimates following the new policy are updated, and the process repeats. As long as this \nprocess satisfies the Bellman optimality principle, this process is guaranteed to converge to an \noptimal policy6. \nImplementing these mechanics on a neuromorphic platform gives rise to a number of \nchallenges, such as how to represent and update values, how to update these values using local \ninformation, and how this program can be expressed in a manner which may be implemented \non multiple architectures despite their underlying differences 2,3,10.  \n \n \n \n \nTo address these challenges, we created a flexible program carrying out an RL strategy \ndescribed by a high-level computational graph. In order to create this program, we focused on \nthe requirements of an RL program and how they can be satisfied by a biologically-inspired \nstructure. Once defined, we implemented this program on a neuromorphic platform in a \nmodular manner, allowing it to address different problems with minimal changes from the user. \nLearning in biological brain and complimentary learning systems theory \nThe mammalian brain contains analogs to the functions required for MC methods. Dedicated \nareas of the brain focus on maintaining robust internal representations of state, including \nphysical location and proprioception21,22. It is believed that the hippocampus can fuse these \ncomplex representations of state over time, allowing animals to ‘record’ the paths they have \ntaken in physical environment and play them back23,24. The short-term memories first formed in \nhippocampus can then be used to build more complex representations and strategies as they \nare integrated into the larger, long-term memory provided by the neocortex. The dual-memory \ntheory5 suggests that these two areas provide complementary functions to one another: the \nhippocampus provides a highly-plastic memory to learn from new experiences, and in order to \nprevent catastrophic forgetting the neocortex slowly integrates this new information into its \nalready-existing representations 5.  \nIn this work we implemented main elements of the complementary learning systems by \nutilizing separate short-term memory (STM) and long-term memory (LTM) modules \ncomplemented by decoder and encoder blocks. \nThe decoder’s role is to interpret and preserve information from the environment. Viewing \nsensory information from the environment as a coded signal, the decoder’s task is to extract \neverything from this signal relevant to the current state and store this internally for other \nmodules to use (e.g. given an image of a chess board, evaluate the placement of the pieces and \nrepresent this state information in a manner meaningful to the downstream modules).  \nThe STM is responsible for storing the agent’s episodic memory. Given internal representations \nstate, action, and reward the agent experiences during an episode, the STM must store this \ninformation and allow it to be ‘replayed’ when the episode has been completed and can be \nused to update value estimates. Consequently, the STM utilizes a highly plastic memory which \nupdates with every action the agent takes. \nOne of the central pieces of any RL agent is its value estimator, which here must provide the \nreturn expected given the current state, next action, and policy. This information must be \nconstructed via cumulative experiences and/or internal bootstrapping, with each additional \nexperience building upon previous knowledge rather than overwriting it. This requires the value \nestimator to have a stable, long-term memory. Cumulative experiences are integrated into the \nLTM by replaying the STM (akin to sharp-wave ripples within the brain24), and selective replay \nwithin the LTM can potentially be used for bootstrapping. Ultimately, the value estimates \n \n \n \n \nstored in this module are responsible for constructing the agent’s current policy, controlling its \ndecisions.  \nThe encoder carries out the final sub-step in an RL epoch, applying the current state and policy \nto choose an action. Additionally, this action may need to be transformed from an internal state \ninto one which can act on the environment (e.g. the intent to move a piece in Chess must be \ntranslated into the physical act of picking up and moving an object). These tasks are carried out \nby the encoder, closing the loop of the RL process. \nHaving defined these modules and how they act in concert to perform RL, the challenge was to \ntranslate them from abstract concepts into realizable implementations which can be \nimplemented on a real neuromorphic processor. In this work, we focused on demonstrating the \narchitecture with a simple proof-of-concept implementation which can be easily interpreted. \nHowever, more advanced representations and other advances in neuromorphic computation \ncan be integrated into the model in the future, and we posit this could lead to large gains in \nefficiency over the current implementation. \nPower Consumption \nOne of the key goals of neuromorphic hardware is to increase the efficiency of learning and \ninference in artificial systems2. However, the current implantation of the DML makes several \ntrade-offs which reduce its energy efficiency. First, information is represented through rate-\ncoding; this requires that each step of inference in a decision-making process be run over a \nseveral time steps in order to collect statistics on current spike rates25. Second, a dedicated \nspiking neural circuit exists in the LTM for every state-action value which must be estimated. \nThese circuits run continually and in parallel, even when their information is not needed. In a \nproblem such as Blackjack (with the largest state-action space tested of 400), this means much \nof the energy consumed to produce spikes is wasted, as only 0.5% of estimates being produced \nare relevant to the current state during each epoch. As a result, while the active power \nconsumption of a Loihi chip running the RC-DML algorithm to solve Blackjack is much lower \nthan that of a traditional CPU, the faster execution rate of an equivalent Monte Carlo program \non CPU gives it an advantage in the amount of energy consumed per RL epoch (67 μJ/epoch on \nCPU vs. 1728 μJ/epoch on Loihi when using 64 rate-coding steps).  \nCurrent Issues and Future Directions \nTwo key issues of the RC-DML (its uncompetitive power consumption and poor scalability) stem \nfrom the same underlying source: the rate-coded and tabular implementation of value \nestimates in the LTM. We argue that replacing this block with a different representation of \ninformation calculated through a function approximator which stores information passively in \nsynapses would succeed in making the DML competitive with traditional CPU implementations, \nboth in terms of learning capability and energy efficiency.  \nFor instance, vector-symbolic architectures (VSAs) offer an alternative basis of representing \ninformation which could be leveraged within the DML26. Binary Spatter Codes (BSC) in \n \n \n \n \nparticular provide a clear method towards replacing the rate-coding of information27. This \nwould both increase the number of values a single population of neurons can represent and \nreduce the amount of time required to produce a representation to a single time-step. \nPotentially, this single change could allow the DML to outperform the CPU in per-epoch energy \nconsumption, as running the current RC-DML architecture reduces its power consumption to 27 \nμJ/epoch when only a single computation step is executed per RL epoch.  \nA larger challenge facing neuromorphic implementations of the DML is an efficient and \neffective method for training function approximators such as deep neural networks. This \nadvance was a key requirement for recent progress in RL6, and equivalents within \nneuromorphic platforms must exist to enable state-of-the-art RL techniques on these \nplatforms. However, advances have been made on this front and should be evaluated to be \nincorporated into future revisions of the DML on neuromorphic platforms which support \nthem10,11,28,29. This would allow the architecture to scale to more complex problems with larger \nstate-spaces.  \nConclusion \nReinforcement learning provides unique learning capabilities and its development has provided \nmany landmark successes over the past decade. Therefore, it is crucial for neuromorphic \nsystems to show that they are capable of RL techniques and can demonstrate advantages for \nthese techniques over traditional hardware. In this work, we have demonstrated a flexible \narchitecture for RL on neuromorphic hardware which was implemented and fully executed on \nthe Intel Loihi platform. This rate-coded dual memory learner (RC-DML) was successfully able to \nlearn policies to maximize reward received from a multi-arm bandit, navigate through a \nchanging maze, and play the card game Blackjack. But while this shows that a neuromorphic \narchitecture is currently capable of RL techniques, the current implementation’s rate-coded and \ntabular approach to value representation makes it uncompetitive with traditional technologies. \nHowever, we believe that further advances from research into neuromorphic systems (such as \nvalue representation through vector-symbolic architectures and spike-based deep learning) can \novercome this obstacle in future work to create a system which can match the performance of \ntraditional approaches with greater energy efficiency. \n \n \n \n \n \n \nMaterials & Methods \nAll spiking networks were developed using Python 3.5.2, the Intel NxSDK v0.9.5 - v0.9.9, and were \nexecuted on the Intel Loihi processor through the Intel Neuromorphic Research Community (INRC) \ncloud. Code for the RC-DML and each task is available online (https://github.com/wilkieolin/loihi_rl). \nValue Circuit \nThe Value Circuit (VC) is a small spiking circuit which provides the long-term memory and learning \ncapability of the RC-DML. It accomplishes this task by taking an approximate moving average of sparse \nreward signals and representing this value with a continuous stream of rate-coded spikes.  \n \nSupplementary Figure 1: Illustration of the Value Circuit (VC). Reward signals have a chance to increase the firing rate of the \nsoft-reset neuron, and punishment signals, a chance to decrease the firing rate. Through the use of feedback, these probabilities \nare manipulated to be equal to one another when the firing rate of the soft-reset neuron equals the proportion of reward spikes \nreceived over a memory period. \nThe VC operates by using feedback to manipulate the probability its firing rate will change when a \nreward or punishment signal is received. The VC’s output is given by the firing rate of a soft-reset \nintegrate-and-fire (SRIF) neuron with no leakage; this 3-compartment neuron produces a regular firing \nrate when the charge in its memory compartment is held constant25. This firing rate is linearly \nproportional to the amount of charge in the memory compartment versus the integrator’s firing \nthreshold. In order to increase the firing rate, charge is added to the memory compartment through an \nexcitatory synapse, and to decrease it, charge is removed through an inhibitory synapse.  \nHowever, if we wish to use this neuron to track the expectation of a reward, charge should not be \nmodified with the arrival of every reward or punishment signal; this is clear if the neuron is already firing \ntonically or is quiescent. More generally, by using the spiking output of the SRIF in conjunction with an \n‘and’ gate which only fires when spikes are present on all its inputs in a single time-step, punishment \nsignals can be blocked or passed to make modifications to charge which are dependent on how close to \nthe quiescent state the SRIF is. To create a similar condition for the reward signals, an inverter is added \nto the signal path which fires whenever an input is not present. This restricts reward signals, making \nthem less likely to pass the closer the SRIF is to the tonic state. By combining these two boundaries, the \nprobabilities that the SRIF’s firing rate will increase or decrease become equal when the firing rate \nmatches the proportion of reward signals out of reinforcement signals received over a finite period. This \nduration of this ‘memory window’ is affected by increasing or reducing the amount of charge injected \n \n \n \n \ninto the SRIF’s memory compartment via synapses or changing the SRIF firing threshold. The larger the \nratio of injected charge to threshold, the shorter the effective memory window (Supplementary Figure \n2).   \n \nSupplementary Figure 2: Firing Rates of value circuits (VCs) with different integrate-and-fire thresholds in the soft-reset \nintegrate-and-fire (SRIF) neurons. A low threshold value leads to a shorter memory period (a) than a high threshold value (b), as \nmore larger changes in charge are required to significantly alter the firing rate of the high-threshold VC. \nThe convergence of the SRIF is shown formally by Eqns. 1-3 where +∆𝑞 and −∆𝑞 are increases or \ndecreases (respectively) in SRIF charge, 𝑠 is the state of the SRIF’s output, 𝑟̂ is the SRIF’s firing rate \nestimating the true parameter of Bernoulli-distributed reward 𝑟, and 𝑟𝑒𝑤𝑎𝑟𝑑/𝑝𝑢𝑛𝑖𝑠ℎ𝑚𝑒𝑛𝑡 indicates a \nsample of 1/0 from this distribution. \nEqn 1. \n𝑝(+∆𝑞) = 𝑝(𝑠= 0|𝑟̂) ⋅𝑝(𝑟𝑒𝑤𝑎𝑟𝑑= 1|𝑟) = (1 −𝑟̂)(𝑟) \nEqn 2. \n𝑝(−∆𝑞) = 𝑝(𝑠= 1|𝑟̂) ⋅𝑝(𝑝𝑢𝑛𝑖𝑠ℎ𝑚𝑒𝑛𝑡= 1|𝑟) = (𝑟̂)(1 −𝑟) \nEqn 3. \n𝑝(+∆𝑞) = 𝑝(−∆𝑞) 𝑖𝑓𝑓.  𝑟̂ = 𝑟 \nBlock Diagrams \nHere we detail structures used to carry out the tasks required in each module of the RC-DML. These \nmodules are implemented via spiking equivalents of common digital logic functions. Each arrow in the \nfigure represents a stereotyped connectivity pattern between blocks which transfers information via \nspikes. Each block automatically scales to the dimensions required by the given problem, reducing \nprogramming burden which switching the architecture to new tasks. These figures follow closely to the \nactual code which implements them. \n \n \n \n \n \nSupplementary Figure 3: Block diagrams of the RC-DML modules. A) The decoder receives sparse, one-hot signals. Upon \nreceiving a signal, it resets the memory bank and then sets the new current state, expanding it from a sparse signal in time to \none which has a continuous availability to the rest of the system. B) The short-term memory stores the state-action pairs which \nhave been encountered so far in an episode. This is done by forming an outer-product between current state and last action. \nWhen a new action arrives, its product is used to indicate this pair has been encountered. When a reinforcement signal arrives, \nstate-action pairs which were encountered are used to signal the LTM to possibly change its values via rewards and \npunishments. The memory buffer is then reset. C) The LTM consists of a large bank of VCs which are detailed above. It receives \nreward/punishment signals which are distributed to appropriate state/action pairs from the STM. D) The encoder reads from the \n \n \n \n \nLTM reward estimates corresponding to the current state. It accumulates these spikes in a buffer which is then read out by a C \nsubroutine on the onboard microprocessor to select the highest value. \nPower Estimates \nPower estimates for CPU tasks were estimated by utilizing the Intel SoC Watch Energy Analysis profiler \non a Windows 10-based system with an Intel i7-4710HQ processor (22 nm node). Active power used to \ncalculate the optimal Blackjack policy was measured by running the included RL program 5 times and \nsubtracting the system’s baseline power usage (also measured over 5 independent trials).  \nPower estimates on the Loihi system was measured by using energy probe tools included in the NxSDK \n0.9.9 toolkit. Programs were run on the ncl-ext-ghrd-01 system available on the Intel Research cloud \nplatform. Energy consumption for the RC-DML learning Blackjack was monitored over 128,000 \ncomputation time steps and averaged. \n \n \n \n \n \n \nFunding  \nThis work was supported by the Intel (00018020-001) and Lifelong Learning Machines program from \nDARPA/MTO (HR0011-18-2-0021). \nAcknowledgments \nWe would like to thank the members of the Intel Neuromorphic Research Lab for their support during \nour software development process. \nReferences \n1. \nLin, S. C. et al. The architectural implications of autonomous driving: Constraints and \nacceleration. ACM SIGPLAN Not. 53, 751–766 (2018). \n2. \nSchuman, C. D. et al. A Survey of Neuromorphic Computing and Neural Networks in \nHardware. 1–88 (2017). \n3. \nZhang, Y. et al. A system hierarchy for brain-inspired computing. 586, (2020). \n4. \nDavies, M. et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE \nMicro 38, 82–99 (2018). \n5. \nMcClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary \nlearning systems in the hippocampus and neocortex: Insights from the successes and \nfailures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 \n(1995). \n6. \nSutton, R. S. & Barto, A. G. Reinforcement Learning, An Introduction. (2020). \n7. \nGerstner, W. & Kistler, W. M. Spiking neuron models: Single neurons, populations, \nplasticity. (Cambridge university press, 2002). \n8. \nReinagel, P. & Reid, R. C. Temporal coding of visual information in the thalamus. J. \nNeurosci. 20, 5392–5400 (2000). \n9. \nDenève, S. & Machens, C. K. Efficient codes and balanced networks. Nature Neuroscience \nvol. 19 375–382 (2016). \n10. \nTavanaei, A., Ghodrati, M., Kheradpisheh, S. R., Masquelier, T. & Maida, A. Deep learning \nin spiking neural networks. Neural Networks vol. 111 47–63 (2019). \n11. \nNeftci, E. O., Mostafa, H. & Zenke, F. Surrogate Gradient Learning in Spiking Neural \nNetworks. 1–25 (2019). \n12. \nLynch, N., Musco, C. & Parter, M. Winner-Take-All Computation in Spiking Neural \nNetworks. 1–94 (2019). \n13. \nKaufmann, E., Cappé, O. & Garivier, A. On Bayesian Upper Confidence Bounds for Bandit \nProblems. (2012). \n14. \nSilver, D. et al. A general reinforcement learning algorithm that masters chess, shogi, and \n \n \n \n \nGo through self-play. 1144, 1140–1144 (2018). \n15. \nRavi Kiran, B. et al. Deep reinforcement learning for autonomous driving: A survey. arXiv \n1–18 (2020). \n16. \nHafner, R. & Riedmiller, M. Reinforcement learning in feedback control : Challenges and \nbenchmarks from technical process control. Mach. Learn. 84, 137–169 (2011). \n17. \nMahmud, M., Shamim Kaiser, M., Hussain, A. & Vassanelli, S. Applications of deep \nlearning and reinforcement learning to biological data. arXiv 29, 2063–2079 (2017). \n18. \nFriedmann, S., Frémaux, N., Schemmel, J., Gerstner, W. & Meier, K. Reward-based \nlearning under hardware constraints-using a RISC processor embedded in a \nneuromorphic substrate. Front. Neurosci. 7, 1–17 (2013). \n19. \nAmravati, A., Nasir, S. Bin, Thangadurai, S., Yoon, I. & Raychowdhury, A. Accelerator with \nStochastic Synapses and Embedded. 2018 IEEE Int. Solid - State Circuits Conf. - 124–126 \n(2018). \n20. \nWalter, F., Röhrbein, F. & Knoll, A. Neuromorphic implementations of neurobiological \nlearning algorithms for spiking neural networks. Neural Networks 72, 152–167 (2015). \n21. \nSolstad, T., Moser, E. I. & Einevoll, G. T. The Hippocampal Indexing Theory and Episodic \nMemory: Updating the Index. Hippocampus 1031, 1026–1031 (2006). \n22. \nMoser, M.-B., Rowland, D. C. & Moser, E. I. Place Cells, Grid Cells, and Memory. Cold \nSpring Harb. Perspect. Biol. 7, a021808 (2015). \n23. \nBuzsáki, G. The Brain from Inside Out. The Brain from Inside Out (Oxford University Press, \n2019). doi:10.1093/oso/9780190905385.001.0001. \n24. \nDenovellis, E. L. et al. Hippocampal replay of experience at real-world speeds. bioRxiv \n2020.10.20.347708 (2020). \n25. \nRueckauer, B., Lungu, I. A., Hu, Y., Pfeiffer, M. & Liu, S. C. Conversion of continuous-\nvalued deep networks to efficient event-driven networks for image classification. Front. \nNeurosci. 11, (2017). \n26. \nNeubert, P., Schubert, S. & Protzel, P. An Introduction to Hyperdimensional Computing \nfor Robotics. KI - Künstliche Intelligenz 33, 319–330 (2019). \n27. \nSchlegel, K., Neubert, P. & Protzel, P. A comparison of Vector Symbolic Architectures. \n(2020). \n28. \nBellec, G. et al. Eligibility traces provide a data-inspired alternative to backpropagation \nthrough time. Arxiv (2019). \n29. \nStewart, K., Orchard, G., Shrestha, S. B. & Neftci, E. Online few-shot Gesture learning on a \nNeuromorphic processor. arXiv 1–10 (2020). \n \n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "I.2"
  ],
  "published": "2021-03-05",
  "updated": "2021-03-05"
}