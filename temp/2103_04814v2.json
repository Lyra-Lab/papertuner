{
  "id": "http://arxiv.org/abs/2103.04814v2",
  "title": "Deeply Unsupervised Patch Re-Identification for Pre-training Object Detectors",
  "authors": [
    "Jian Ding",
    "Enze Xie",
    "Hang Xu",
    "Chenhan Jiang",
    "Zhenguo Li",
    "Ping Luo",
    "Gui-Song Xia"
  ],
  "abstract": "Unsupervised pre-training aims at learning transferable features that are\nbeneficial for downstream tasks. However, most state-of-the-art unsupervised\nmethods concentrate on learning global representations for image-level\nclassification tasks instead of discriminative local region representations,\nwhich limits their transferability to region-level downstream tasks, such as\nobject detection. To improve the transferability of pre-trained features to\nobject detection, we present Deeply Unsupervised Patch Re-ID (DUPR), a simple\nyet effective method for unsupervised visual representation learning. The patch\nRe-ID task treats individual patch as a pseudo-identity and contrastively\nlearns its correspondence in two views, enabling us to obtain discriminative\nlocal features for object detection. Then the proposed patch Re-ID is performed\nin a deeply unsupervised manner, appealing to object detection, which usually\nrequires multilevel feature maps. Extensive experiments demonstrate that DUPR\noutperforms state-of-the-art unsupervised pre-trainings and even the ImageNet\nsupervised pre-training on various downstream tasks related to object\ndetection.",
  "text": "1\nDeeply Unsupervised Patch Re-IdentiÔ¨Åcation for\nPre-training Object Detectors\nJian Ding, Enze Xie, Hang Xu, Chenhan Jiang, Zhenguo Li, Ping Luo, Gui-Song Xia\nAbstract‚ÄîUnsupervised pre-training aims at learning transferable features that are beneÔ¨Åcial for downstream tasks. However, most\nstate-of-the-art unsupervised methods concentrate on learning global representations for image-level classiÔ¨Åcation tasks instead of\ndiscriminative local region representations, which limits their transferability to region-level downstream tasks, such as object detection.\nTo improve the transferability of pre-trained features to object detection, we present Deeply Unsupervised Patch Re-ID (DUPR), a simple\nyet effective method for unsupervised visual representation learning. The patch Re-ID task treats individual patch as a pseudo-identity\nand contrastively learns its correspondence in two views, enabling us to obtain discriminative local features for object detection. Then\nthe proposed patch Re-ID is performed in a deeply unsupervised manner, appealing to object detection, which usually requires multi-\nlevel feature maps. Extensive experiments demonstrate that DUPR outperforms state-of-the-art unsupervised pre-trainings and even\nthe ImageNet supervised pre-training on various downstream tasks related to object detection.\nIndex Terms‚ÄîSelf-supervised learning, visual representation learning, contrastive learning, object detection.\n!\n1\nINTRODUCTION\nPre-training then Ô¨Åne-tuning has been a widely used\nparadigm when approaching computer vision problems\nwith deep models [1], [2], [3], [4]. In recent years, Ô¨Åne-\ntuning in object detection tasks has been dominated by the\nImageNet supervised pre-training [1], [3], [5], [6]. However,\nthere exists misalignment between image-level classiÔ¨Åcation\npre-training and object detection tasks, which make region-\nlevel predictions. One solution to eliminate the misalign-\nment [7] is to pre-train representations directly on a large-\nscale and high-quality object detection dataset; but annota-\ntions of such datasets are time-consuming, laborious, and\neven hard to obtain for some areas.\nAlternatively, unsupervised learning aims at pre-training\nrepresentations without human annotations, which allows\nus to pre-train representations with plenty of unlabeled\ndata for free [8], [9], [10], [11], [12], [13]. Among them,\ncontrastive learning methods [8], [14], [15] have achieved\ncomparable performance compared to ImageNet supervised\npre-training in many downstream tasks, such as image\nclassiÔ¨Åcation, object detection, and semantic segmentation.\nContrastive learning can learn view-invariant representations\nby maximizing the similarity between positive pairs of\n‚Ä¢\nThis work was supported by National Nature Science Foundation of China\nunder the grants 61922065, 41820104006 and 61871299. This work was\nalso partially supported by the General Research Fund of HK No.27208720\nand No.17212120.\n‚Ä¢\nJ. Ding is with the State Key Lab. LIESMARS, and also School of\nComputer Scienc Wuhan University, Wuhan, 430079, China. Email:\njian.ding@whu.edu.cn.\n‚Ä¢\nH. Xu, C. Jiang and Z. Li are with the Huawei Noah‚Äôs Ark Lab. Email:\nxbjxh@live.com, jchcyan@gmail.com, li.zhenguo@huawei.com.\n‚Ä¢\nE. Xie and P. Luo are with the University of Hong Kong. Email:\nxieenze@hku.hk and pluo@cs.hku.hk.\n‚Ä¢\nG.-S. Xia is with the National Engineering Research Center for Multi-\nmedia Software, School of Computer Science and Institute of ArtiÔ¨Åcial\nIntelligence, and also the State Key Lab. LIESMARS, Wuhan University,\nWuhan, 430072, China. Email: guisong.xia@whu.edu.cn.\n‚Ä¢\nJ. Ding and E. Xie are equally contributed to this work.\n‚Ä¢\nCorresponding author: Gui-Song Xia (guisong.xia@whu.edu.cn.)\nùëß\n‚Ä¶\n‚Ä¶\nùëß+\nPatch Re-identification\n‚Ä¶\n1\n1\n2\n2\n2\n1\nView 1\nView 2\nInput\npush\npull\nùëß‚àí\n3\n4\n5\n6\n7\n8\n9\n2\n1\n3\n6\n5\n4\n9\n8\n7\nPseudo ID\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFig. 1. Illustration of unsupervised pre-training for object detection by\npatch Re-ID, which follows the merits of person Re-ID that matches a\nhuman identity between cameras. In patch Re-ID, each patch within\nthe intersection area (orange bounding box) is treated as a pseudo-\nidentity. The unsupervised features are learned by matching the same\npatch identity (denoted by the same number) in two views generated\nby different transformations from the input image. In contrast to the\nprevious global view based contrastive learning methods, which learn\nglobally spatial invariant representations, the patch Re-ID learns spatial\nsensitive representations. Therefore, the translation, aspect ratio, and\nscale of objects in the input image will produce meaningful responses in\nthe feature maps for describing the location of objects inside an image,\nwhich is beneÔ¨Åcial for object detection.\nviews while minimizing the similarity between negative\npairs.\nRecent state-of-the-art contrastive learning methods take\nglobal views from the same image as positive pairs and\nviews from different images as negative pairs. The well-\ndesigned augmentations [14], [16], including random re-\nsized crop, are used to generate these views. Thus globally\nspatial invariant representations will be learned, which are\nbeneÔ¨Åcial for classiÔ¨Åcation. For example, classiÔ¨Åcation mod-\nels should predict the same category ‚Äùdog‚Äù for two views\ngenerated from the same image, containing a dog at the\narXiv:2103.04814v2  [cs.CV]  10 Apr 2022\n2\nbottom-left and top-left (see Fig. 1). So, these two views con-\ntaining the same object but at different locations should have\nsimilar global representations. The representations learned\nby contrasting global views can encode much information\nrelated to category and largely improve the performance\non ImageNet linear evaluation, approaching the accuracy of\nsupervised classiÔ¨Åcation [14], [17], [18], [19].\nHowever, there is a gap between pre-training global rep-\nresentations and region-level downstream tasks, such as ob-\nject detection. Being different from image-level classiÔ¨Åcation,\nwhich predicts the category for an entire image by a globally\nspatial invariant feature, object detection is a region-level\ntask which predicts categories and regression targets for\nmultiple regions by the region features. The region features\nat different locations should be discriminative since the pre-\ndiction targets for these regions are different. For example,\nregions of interest (RoIs) are assigned with foreground or\nbackground categories. Also, foreground RoIs overlapping\nobjects at different positions should predict different regres-\nsion targets. Based on the above reasons, features at different\nlocations in a feature map should be mapped to far apart\npoints in local feature space (see Fig. 3). Thus, the previous\nmethods that only optimize the single feature after globally\naveraged pooling are problematic for object detection since\nthey do not learn discriminative local representations in a\nfeature map. So the previous methods with higher perfor-\nmance on ImageNet classiÔ¨Åcation do not always lead to\nbetter transfer performance on object detection [17], [19],\n[20] (see also in Tab. 5).\nMoreover, previous works focus on learning discrimi-\nnative features at the Ô¨Ånal layer (e.g., on the 32√ó feature\nmap) [8], [14]. However, most deep learning based ob-\nject detectors need to extract features from the multi-level\nrepresentations (such as FPN [21] and PANet [22]). Thus,\nobject detection requires discriminative features at different\nfeature layers instead of only the Ô¨Ånal layer.\nTo address the aforementioned problems, we propose\nto pre-train region-level discriminative representations across\nmulti-level feature maps for object detection by Deeply Un-\nsupervised Patch Re-ID (DUPR). The task of patch Re-ID is\nto match the corresponding patch identity (denoted by the\nsame number in Fig. 1) of two views, which is inspired by\nthe person Re-ID that matches a human identity between\ncameras under different viewing conditions. By pre-training\nwith patch Re-ID, the features of matched patches should be\nmore similar than unmatched patches in local feature space\n(see Fig. 3), so the region representations at different loca-\ntions in a feature map are discriminative and beneÔ¨Åcial for\nregion-level tasks such as object detection. Furthermore, we\npropose a deeply unsupervised training strategy to learn multi-\nlevel representations. SpeciÔ¨Åcally, we extract features from\ndifferent intermediate layers to construct both image-level\nand patch-level contrastive loss. Our DUPR is independent of\nthe detailed self-supervised learning framework. We simply\nadopt the MoCo framework [8], [17] and InfoNCE [23] as\nloss function in this work, but patch Re-ID can also be used\nin other self-supervised learning frameworks [14], [19]. The\nwhole pipeline is shown in Fig. 3.\nOur contributions can be summarized as follows:\n‚Ä¢ We propose a self-supervised pretext task, named patch\nRe-ID that learns to match the same patch identity\n26\n28\n30\n32\n34\n36\n38\n40\n42\n12k\n18k\n36k\n90k\n180k\nBox mmAP\nIterations\nObject Detection on COCO\nDUPR\nMoco v2\n24\n26\n28\n30\n32\n34\n36\n38\n12k\n18k\n36k\n90k\n180k\nMask mmAP\nIterations\nInstance Segmentation on COCO\nsupervised\nrand\nFig. 2. Comparison with other pre-training methods on Ô¨Åne-tuning\nMask R-CNN R-50-FPN on COCO. The MoCo v2 baseline [17] and\nDUPR are pre-trained on the ImageNet-1M training set with 200 epochs.\nDUPR outperforms MoCo v2 and even the supervised counterpart at all\niterations. (a) shows the box mmAP and (b) shows the mask mmAP.\nWhen Ô¨Åne-tuning Mask R-CNN with 12k iterations, DUPR outperforms\nthe MoCo v2 by 2.9 points in box mAP and 2.4 points in mask mAP.\nbetween two views to get region-level discriminative\nfeature maps, which is tailored for object detection.\n‚Ä¢ We present a deeply unsupervised training strategy\nto improve the transferability of pre-trained models\nto object detection, which requires extracting features\nfrom multi-level feature maps for prediction.\n‚Ä¢ Our DUPR pre-training outperforms other unsuper-\nvised and supervised pre-training counterparts when\nserving as the initialization for Ô¨Åne-tuning. For exam-\nple, when Ô¨Åne-tuning Mask R-CNN R-50-FPN on MS\nCOCO [24], DUPR outperforms MoCo v2 [17] and su-\npervised pre-training at all different iterations as shown\nin Fig. 2. More importantly, it outperforms the strong\nbaseline MoCo v2 when serving as initialization for\nÔ¨Åne-tuning in other location-sensitive tasks, such as\nVOC [25] object detection (+2.4 APbb\n75), Cityscapes [26]\nsemantic segmentation (+1.0 mIoU), and LVIS [27] in-\nstance segmentation (+1.0 APmk).\n2\nRELATED WORK\n2.1\nObject Detection\nThe task of object detection is to locate objects in images and\nclassify their categories. Unlike the image-level classiÔ¨Åcation\nprediction task, object detection is a region-level prediction\ntask: it needs to regress the location and classify the category\nof an object simultaneously for each region. In two-stage\nobject detectors [3], [5], region features are extracted from\nthe proposals (generated by selective search [28] or RPN [5])\nby RoI Align [3]. For one-stage object detectors [6], [29], [30],\n[31], region features are extracted from sliding windows.\nThe prediction targets are different for different regions\nin an image. For each region, the classiÔ¨Åcation branch usually\npredicts the conÔ¨Ådence score of categories (foreground and\nbackground in RPN [5], or categories of objects in one-stage\ndetector [6] and R-CNN [1]). A region of interest (RoI) is\nassigned to be a positive example if it has an intersection-\nover-union (IoU) overlap with a ground-truth box above a\nthreshold; otherwise, it is assigned to be a negative example.\n3\nTherefore, the category prediction target is sensitive to the\nchange of regions (when the IoU changes from below the\nthreshold to above the threshold, or from one object to\nanother object with a different category). The localization\nbranch usually predicts the regression targets relative to the\nanchor (see deÔ¨Ånition of regression targets in [29], [1]), and\ndifferent positive RoIs should predict different regression\ntargets. For example, an RoI having high IoU overlap with\nan object should predict small regression targets, while an\nRoI having low IoU overlap with an object should predict\nlarge regression targets (see Fig. 5). So the local region\nrepresentations at different locations should be mapped to\nfar apart points in local feature space for object detection.\nBesides, object detection is also the combination of region-\nlevel classiÔ¨Åcation and localization. Features need to be\nsensitive to the location of a feature map for object local-\nization while maintaining the strong semantic information\nfor classiÔ¨Åcation at the same time.\nObject detection also requires multi-level representations,\nas predictions are directly made by using multi-level fea-\ntures [29], or the fusion of multi-level feature maps [21],\n[22]. For example, FPN [21] is a widely used structure\nin object detection to handle the scale variations, which\ncombines low-resolution, semantically strong features with\nhigh-resolution, semantically weak features via top-down\nconnections. Thus, our work will focus on learning discrimi-\nnative region-level features from multiple layers to pre-train\nrepresentations, which are tailored for object detection.\n2.2\nPre-training for Object Detection\nR-CNN [1] has shown that ImageNet supervised pre-\ntraining followed by domain-speciÔ¨Åc Ô¨Åne-tuning on a small\ndataset is an effective paradigm for learning high-capacity\nrepresentations. Pre-training for object detection largely im-\nproves the performance on small dataset [1], [32] and also\nspeeds up the convergence of object detectors [32]. How-\never, ImageNet supervised pre-training is weak at localiza-\ntion and helps less if the downstream task is localization-\nsensitive [1], [32]. To obtain better pre-trained representa-\ntions for object detection, Objects365 [7] is proposed. Pre-\ntraining on this large-scale and high-quality object detection\ndataset can signiÔ¨Åcantly surpass the ImageNet supervised\npre-training in convergence speed and mAP. Since annota-\ntions of object detection are expensive, weakly supervised\npre-training [33] has been explored for object detection.\nHowever, the weakly supervised pre-training pipeline [33]\nis complicated and still needs annotations. In contrast to\nthese works, our paper presents a method for unsupervised\npre-training for object detection, which has rarely been\nstudied before.\n2.3\nSelf-supervised Visual Representation Learning\nSelf-supervised visual representation learning leverages in-\nput data itself as supervision via pretext tasks. After pre-\ntraining, features are transferred to downstream tasks. Early\npretext tasks for self-supervised representation learning\ninclude rotation prediction [34], relative location predic-\ntion [35] and jigsaw [36], etc. These hand-crafted pretext\ntasks achieve promising results but still have a large gap\ncompared to ImageNet supervised pre-training. Recently,\nthe most successful methods in self-supervised learning are\ncontrastive learning [37], [8], [23], [14], [16], [38], [39], [18],\n[19], [20] via instance discrimination pretext task [15], [8],\n[40]. The core idea of contrastive learning is to pull together\npositive view pairs while pushing apart negative view pairs.\nThe success of contrastive learning relates to learning\nthe invariant representations to a family of similar views\n(positive pairs). The selection of positive pairs and data\naugmentations on views is important, and varies in different\nmethods. For example, CPC [23] and CPC v2 [39] take the\ncontext and future as positive pairs. Deep infomax [41]\nand AMIDIM [42] take the global and local features as\ntwo positive pairs. MoCo [8] and SimCLR [14] adopt the\ninstance discrimination task [40], [15] which takes the ran-\ndomly augmented global views from the same image as two\npositive pairs. SimCLR has also studied many data augmen-\ntation strategies to generate views. SwAV has additionally\nproposed a novel multi-crop data augmentation, which\nincreases the number of local views and maximizes the\nsimilarity between global and local views. The inÔ¨Çuence of\naugmentations and view selection for different downstream\ntasks has been studied in [16], [43]. Different downstream\ntasks have different optimal selections of positive view\npairs. It has been proven in [16] that the optimal views\nshould share the minimum information necessary to per-\nform well at the downstream tasks. However, InfoMin [16]\nstill takes the classiÔ¨Åcation as the downstream task and\ndesigns augmentations to improve the classiÔ¨Åcation perfor-\nmance.\nMost of the previous methods maximize the similarity\nbetween spatial misaligned views and focus on learning\nglobally spatial invariant representations for image-level clas-\nsiÔ¨Åcation, although the details are different. However, the\nbetter classiÔ¨Åcation accuracy of linear probing does not al-\nways lead to better transfer performance to object detection.\nFor example, SwAV [20] and BYOL [19] are much higher\nthan MoCo v2 [17] in ImageNet linear probing, but lower in\nthe transferring performance on object detection (see Tab. 5).\nBeing different form these works, our approach maximizes\nthe similarity between the spatially consistent local and local\nviews, and aims at learning spatial sensitive, multi-level, entire\nfeature maps for region-level tasks such as object detection.\nIn the way of extracting local views, most of the previous\nmethods send extra patches from the initial image to the\nnetwork, such as SwAV [20] and CPC [23], [39]. In contrast,\nour method directly extracts the local representation from\nthe feature map, which is more efÔ¨Åcient.\n2.4\nUnsupervised Dense Representation Learning\nThere are some early works related to pixel or region-level\nrepresentation learning. The auto-encoders [44], [45], [46]\nare trained by generating or modeling pixels in the input\nspace. However, pixel-level generation is computationally\nexpensive and needs extra heavy decoders which are not\nused by the downstream tasks. And the cycle-consistency of\ntime in video sequences [47], [48] use self-supervised track-\ning as a pretext task and learns the pixel-level correspon-\ndence in the video sequences. However, these works aim at\ndirectly being deployed in visual correspondence tasks in\nvideos without Ô¨Åne-tuning, rather than being transferred to\nother downstream tasks.\n4\nImage Contrastive loss\nPatch-level contrast: Patch Re-ID\nMomentum Backbone ùíáùüê\n4x\n8x\n16x\n32x\nùêµ1\nùêµ2\n+\nBackbone: ùíáùüè\n4x\n8x\n16x\n32x\n4x\nPatch Contrastive loss\nùëÖ1\nùëÖ2\nùêµ\nGAP\nMLP: ‚Ñé1\nGAP\nMLP: ‚Ñé2\n1 2 3\n4\n5 6\n7 8 9\n3\n2\n1\n6\n5\n4\n9\n8\n7\nImage-level contrast: Instance Discrimination  \nRoIAlign\nMLP: ùëî1\nRoIAlign\n1 2 3\n4 5 6\n7 8 9\n3 2 1\n6 5 4\n9 8 7\nMLP: ùëî2\nGlobal feature space\nLocal feature space\n‚Ä¶\n9\n9\nùêµ1\nùêµ2\n1\n1\nùíáùüê\n1 2 3\n4 5 6\n7 8 9\nùëî2\n‚Ñé2\nGAP\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFig. 3. The pipeline of Deeply Unsupervised Patch Re-ID (DUPR). The original image I is augmented into two views I1 and I2. The blue and\ngreen bounding boxes correspond to the original area of I1 and I2, respectively. The orange bounding box is the intersection area. I1 and I2 are\npassed to the backbone and momentum backbone, respectively. We take four feature levels to construct the contrastive loss. Each feature level\nincludes an image-level and a patch-level contrastive loss. The image-level contrastive learning aims at mapping similar global views to the nearby\npoints in global feature space but has no constraint on the local representations. Patch-level contrastive learning aims at mapping the corresponding\nlocal features of patch identity (denoted by the number in this Ô¨Ågure) to nearby points in local feature space. To obtain the corresponding patch\nidentity, the intersection area is numerically calculated then RoI Align is applied to the intersection area to extract region features with the shape of\n(C, S √ó S). Flipping is also applied to region features if the input image is Ô¨Çipped. The similarity between matched patch features are maximized\nby the patch-level contrastive loss, making representation discriminative at different locations. The parallel image-level and patch-level contrastive\nlearning makes the representation both semantic strong and location-sensitive, tailored for object detection.\nRecently, VADeR [49] has explored the pixel-level con-\ntrastive learning for transferring to multiple dense prediction\ntasks. But the VADeR requires the initialization of MoCo [8]\nand only optimizes the local representation; our model is\ntrained from scratch, and optimizes both global and local\nrepresentation. The VADeR [49] also does not perform well\nat object detection.\nBeing concurrent to our work, there are several self-\nsupervised learning methods [50], [51], [52], [53], [54] tar-\ngeting at object detection and semantic segmentation. In-\nstLoc [50] crops two spatially misaligned patches and pastes\nthem on two background images to form two positive views\nfor contrastive learning, which is quite different from our\nmethod. DetCo [52] focuses on the trade-off between classiÔ¨Å-\ncation and object detection tasks, and proposes to use global\nand local contrastive learning. Self-EMD [51], DenseCL [54],\nand PixPro [53] are three dense self-supervised learning\nmethods. Self-EMD [51] and DenseCL [54] Ô¨Ånd the positive\npairs of patch features by the similarities between patch\nfeatures, which are unstable if the initialization is not good.\nDifferent from them, DUPR Ô¨Ånds the positive patches by\ntheir locations in the original images, which is more accurate\nand stable.\nPixPro [53] is mostly related to our work since it also\nÔ¨Ånds the positive pairs of patches by locations in the\noriginal images; but we are different in the details for the\nconstruction of positive pairs, as shown in Fig. 4. Pix-\nPro [53] Ô¨Ånds the matched patches in two spatially misaligned\ngrids, and a threshold of the distance between misaligned\npatches is introduced. In contrast, we apply RoI Align [3]\non the intersection of two views and generate feature maps\nwith spatially aligned grids. So we can naturally obtain the\nmatched patches, and do not need an extra hyperparameter.\nPixPro [53] also introduces a pixel propagation module\n(PPM) to further improve the performance. Being simpler\nview #1\nview #2\nRoI Align \nin view #1\nRoI Align \nin view #2\nA positive pair \nin PixPro\nA positive pair \nin DUPR\nFig. 4. Comparison between PixPro [53] and DUPR in the construc-\ntion of positive pairs. PixPro [53] Ô¨Ånds positive pairs in two feature\nmaps with spatially misaligned grids: two patches are considered as\npositive pairs if the distance between their centers is below a threshold.\nDUPR generates two feature maps with spatially aligned grids, and can\nnaturally obtain the positive pairs in the aligned feature maps.\nthan PixPro [53], our method performs slightly better, as\nshown in Tab. 1 and Tab. 2.\n3\nMETHOD\n3.1\nPreliminary: Contrastive Learning\nThe main idea of contrastive learning is to pull together\npositive views while pushing apart negative views. Take\nMoCo [8] as an example, suppose I is the original image,\nI1 and I2 can be considered as two views of the same\nimage with different augmentations. Denote v1 and v2,+\nto be the normalized embeddings of I1 and I2. The target\nof contrastive learning is to pull together positive pairs\n(v1, v2,+) while pushing apart negative pairs (v1, v2,j). The\nconventional learning objective is the InfoNCE [23] loss:\nLv1,v2,+ = ‚àílog\nexp(v1¬∑v2,+/œÑ)\nPK\nj=0 exp(v1¬∑v2,j/œÑ)\n.\n(1)\n5\nHere œÑ is a temperature hyper-parameter. v1¬∑v2,j is the\ncosine similarity to measure the distance between two image\nfeatures. It can be considered as a non-parametric softmax-\nbased classiÔ¨Åer to identify v2,+ as v1.\nSelecting positive pairs is important [15], [41] for con-\ntrastive learning since they will learn invariant representa-\ntion against the transformation applied on positive pairs.\nWhat kind of transformation should the representation be\ninvariant to is determined by the downstream tasks, and\nvaries across different downstream tasks [16], [43]. For\nexample, suppose the downstream task is classiÔ¨Åcation;\nin such case, the representation should be invariant to the\nlocation of an object inside an image since the change\nof locations does not change the semantic category. And,\nsuppose the downstream task is to predict the location\nof an object. In this case, the representation should not be\ninvariant to the locations. But other factors (e.g., category\nand light condition) are irrelevant information, to which the\nrepresentation should be invariant.\nIn the previous works, v1 and v2,+ are pairs of global-\nlocal features [42], [41] or spatial misaligned global-global\nfeatures [8], [14]. On the one hand, these global averaged\nfeatures will lose spatial information. On the other hand, the\npositive pairs are not spatially aligned. Thus, these global\nview based methods tend to learn globally spatial invariant\nfeature, which is suitable for image-level classiÔ¨Åcation but\nnot for region-level and location-sensitive tasks such as object\ndetection. In contrast to the previous methods, our method\ncan be considered as spatially aligned local-local views\nselection, where the representations are sensitive to the\nlocations of objects, while invariant to other factors.\n3.2\nDeeply Unsupervised Patch Re-ID\nThe pipeline of DUPR is shown in Fig. 3, which consists\nof patch-level and image-level contrastive learning in paral-\nlel across multi-feature levels. The patch-level contrastive\nlearning directly optimizes an entire feature map before\naveraged pooling and maximizes the similarity between\nmatched patches (denoted by the same number in Fig. 3)\nto strengthen the spatial information for localization. Since\nobject detection is the combination of localization and classi-\nÔ¨Åcation, we also include the image-level contrastive learning\nto strengthen the semantic information for classiÔ¨Åcation.\nFor the contrastive learning framework, we simply choose\nthe MoCo v2 [17] as our strong baseline, although other\ncontrastive learning frameworks are also possible.\nFinally, we add the patch-level and image-level contrastive\nloss to multi-feature levels, as most object detectors need a\nmulti-level representation, such as FPN [21] and PANet [22].\nThe overall loss is deÔ¨Åned as:\nL =\nM\nX\nm=0\nŒ±mL(m)\nimage +\nM\nX\nm=0\nŒ≤mL(m)\npatch\n(2)\nwhere M is the number of feature maps. L(m)\nimage and L(m)\npatch\nare the image and patch contrastive loss for feature map\nof m-th level. Œ±m and Œ≤m are the weights to balance the\nimportance of different levels. We will describe the details of\npatch-level and image-level contrastive learning across multi-\nlevel feature maps in the following.\ncls. target: background\nreg. target: None\ncls. target: cow\nreg. target:\ncls. target: cow\nreg. target:\nFig. 5. The prediction targets varies across different regions. The\ncls. denotes the classiÔ¨Åcation target while the reg. denotes the regres-\nsion target. The (tx, ty, tw, th) is an instantiation of regression target,\nwhich follows the deÔ¨Ånition in [1], but it can also be any other kinds\nof regression targets, such as [55]. In this Ô¨Ågure, the absolute value of\n(tx1, ty1, tw1, th1) is larger than (tx2, ty2, tw2, th2).\n3.2.1\nPatch-Level Contrastive Loss.\nThe previous meth-\nods optimize global representations and do not learn dis-\ncriminative local representations. However, for location-\nsensitive tasks, features of different patches in a feature map\nshould be different since their prediction targets are differ-\nent. For example, different regions may represent different\ncategories of objects or the background. The regression\ntargets for different regions are also different (see Fig. 5 for\nillustration). So the local features of the matched patches\nshould be mapped to the nearby points in local feature space,\nwhile local features of different patches should be mapped\nto far apart points in local feature space as shown in Fig. 3.\nTo learn patch-level discriminative representations, we\ndesign the patch Re-ID pretext task to match the same patch\nidentities of two views (which are denoted by the same\nnumber in Fig. 3). First, we sample two augmentations\n(t ‚ààT and t\n‚Ä≤ ‚ààT ) from the same family of augmentations:\neach augmentation is a composition of multiple transforma-\ntions (e.g., cropping, resizing, Ô¨Çipping, and color distortion.)\nThen we apply augmentations to input image I and get\ntwo views: I1 = t(I) and I2 = t\n‚Ä≤(I). The corresponding\nrectangular regions of I1 and I2 in the original image I are\nrecorded and denoted as:\nR1 = (tl(1)\nx , tl(1)\ny , br(1)\nx , br(1)\ny )\nR2 = (tl(2)\nx , tl(2)\ny , br(2)\nx , br(2)\ny )\n(3)\nwhere the (tlx, tly) denotes the top-left vertex and (brx, bry)\ndenotes\nthe\nbottom-right\nvertex\nof\na\nrectangular\nre-\ngion. Then we can calculate the intersection area B =\n(tl(B)\nx\n, tl(B)\ny\n, br(B)\nx\n, br(B)\ny\n) from R1 and R2 as:\ntl(B)\nx\n= max(tl(1)\nx , tl(2)\nx )\ntl(B)\ny\n= max(tl(1)\ny , tl(2)\ny )\nbr(B)\nx\n= min(br(1)\nx , br(2)\nx )\nbr(B)\ny\n= min(br(1)\ny , br(2)\ny )\n(4)\nThe intersection B in the coordinate system of I1 and I2 can\nbe obtained by B1 = T1(B) and B2 = T2(B), where the T1\nand T2 are coordinate transformations from I to I1 and I to\nI2, respectively. Instead of using a global averaged feature,\nwhich loses spatial information, we split B1 and B2 of the\ntwo views into S √ó S patches and maximize the similarity\nbetween the corresponding patch features (denoted by the\nsame number). For the detailed implementation of m-th\nfeature map, we apply RoI Align [3] to extract region feature\n6\nfollowed by a pixel-wise MLP layer which is implemented\nby a 1 √ó 1 convolution as:\nr(m)\n1\n= g(m)\n1\n(RoIAlign(f (m)\n1\n(I1), B1))\n(5)\nr(m)\n2\n= g(m)\n2\n(RoIAlign(f (m)\n2\n(I2), B2))\n(6)\nwhere g(m)\n1\nand g(m)\n2\nare MLP layer and momentum MLP\nlayer, respectively. r(m)\n1\nand r(m)\n2\nare region features of a\nÔ¨Åxed shape (C, S √ó S). Then r(m)\n1,p\nand r(m)\n2,p\nis a positive\npair of normalized feature vectors, where the subscript\np ‚àà[0, S √ó S) denotes the position in the intersection area.\nAlthough our patch Re-ID pretext task is independent of\nloss functions, we simply adopt the InfoNCE [23] loss and\nfollow MoCo v2 [17] to use a dynamic memory bank to store\nfeatures from momentum updated encoder. We construct\nthe patch-level contrastive loss for the m-th feature map as:\nL(m)\npatch = ‚àí\nX\np\nlog\nexp(r(m)\n1,p ¬∑r(m)\n2,p /œÑ)\nexp(r(m)\n1,p ¬∑r(m)\n2,p /œÑ) + PK\nt=1 exp(r(m)\n1,p ¬∑r(m)\nt\n/œÑ)\n(7)\nwhere {r(m)\nt\n}t=1,...K are patch features of other images from\nmemory bank. m denotes the index of feature map. By\nminimizing the patch-level contrastive loss, our encoder can\nlearn patch-wise matching of identity between two views.\nSuch matching ability results in spatially sensitive feature\nmaps, which facilitates object detection. Note that if there\nis no overlap between R1 and R2, we will ignore the patch-\nlevel contrastive loss. The probability of such a case is very\nlow (e.g., 51 out of 1472 positive pairs), so it has not much\neffect on the results.\n3.2.2\nImage-Level Contrastive Loss.\nWe also optimize\nthe image-level contrastive loss as it is important to im-\nprove the classiÔ¨Åcation ability, which is also required by\nobject detection. Denote v(m)\n1\n= h1(GAP(f (m)\n1\n(I1))) and\nv(m)\n2,+ = h2(GAP(f (m)\n2\n(I2))) to be the normalized image\nfeatures of positive pairs. For simplicity, we neglect the\nnotation of normalization. The image-level contrastive loss\nfor the m-th feature map can be written as:\nL(m)\nimage = ‚àílog\nexp(v(m)\n1\n¬∑v(m)\n2,+ /œÑ)\nPK\nj=0 exp(v(m)\n1\n¬∑v(m)\n2,j /œÑ)\n(8)\n3.2.3\nImplementation Details.\nWe use unlabelled Ima-\ngeNet to pre-train our models for our experiments. For\nthe ablation experiments, we follow the data augmentation\nsettings in [17]. For the main experiments, we add the\nRand-Augmentation [56] following [16]. We choose ResNet\n50 [57] as our backbone and extract multi-level features from\nconv2 x, conv3 x, conv4 x and conv5 x. The stride of each\nfeature map is {4√ó, 8√ó, 16√ó, 32√ó}, respectively. By default,\nwe set Œ±0:3 = (0.1, 0.4, 0.7, 1.0) and Œ≤0:3 = (0, 0, 1, 1) for\nEq. (2). The RoI size S of patch features on conv5 x and\nconv4 x are 14 and 7. œÑ is 0.2 for ablation experiments and\n0.15 for main experiments. Unless other speciÔ¨Åed, we train\nwith a batch size of 256 for 200 epochs. We use a learning\nrate of 0.06 with a cosine decay schedule.\nWe maintain a unique memory bank for each image-level\nand patch-level contrastive loss in Eq. (2). For L(m)\npatch, the\nmemory bank stores the patch features of the m-th feature\nmap of other images. For L(m)\nimage, the memory bank stores\nthe image features of the m-th feature map of other images.\nWe store 65536 keys for each memory bank. For patch\nfeatures, there are S√óS features for a single level on a single\nimage, where S = 7 on conv5 x and S = 14 on conv4 x.\nIn a batch with 256 images, there are 256 √ó S √ó S patch\nfeatures. Since most patch features from a single image are\nsimilar, we sample 32 patch features per batch, when we\nenqueue and dequeue a batch of patch features.\nAll the algorithms of object detection, instance seg-\nmentation, and semantic segmentation are implemented in\ndetectron2 1. For Mask R-CNN R-50-FPN, Mask R-CNN\nR-50-C4, and Faster R-CNN-C4, we follow the settings in\nMoCo [8]. For RetinaNet R-50-FPN, we also add an extra\nnormalization layer similar to Mask R-CNN R-50-FPN in\nMoCo [8]. The pre-trained weights of PIRL [58], InsDis [15]\nare downloaded from Pycontrast 2, while the pre-trained\nweights of SwAV is downloaded from the ofÔ¨Åcial code 3.\n4\nEXPERIMENTAL ANALYSIS\nWe evaluate our DUPR and compare it with recent state-of-\nthe-art unsupervised and supervised counterparts in vari-\nous object detection related downstream tasks. The results of\nPASCAL VOC [25] object detection are reported in Sec. 4.1.\nThe results of COCO [24] object detection and instance\nsegmentation are presented in Sec. 4.2. Other localization-\nsensitive tasks (COCO keypoint detection, Cityscapes [26]\nsemantic segmentation, and instance segmentation, and\nLVIS instance segmentation [27]) are presented in Sec. 4.3.\nThe comparisons with other methods in object detection v.s.\nclassiÔ¨Åcation performance are reported in Sec. 4.4. Then we\nanalyze some ablation experiments in Sec. 4.5, and give the\nvisualization of features in Sec. 4.6.\n4.1\nPascal VOC Object Detection\n4.1.1\nExperimental Setup.\nPASCAL VOC [25] is a widely\nused small dataset for object detection, on which training\nfrom scratch can not catch up the performance compared to\nthe pre-trained counterparts even with longer training [32].\nWe Ô¨Åne-tune the Faster R-CNN with R-50-C4 backbone on\nPascal VOC trainval07+12 and evaluate the results on\ntest2007. All the settings are the same as MoCo [8]. The\nRPN is built on conv 4x feature map and R-CNN is built on\nthe conv 5x feature map in this detector. All the parameters\nof the network are Ô¨Åne-tuned end-to-end. The image size is\n[480, 800] in the training and 800 at inference. During Ô¨Åne-\ntuning, we train and synchronize all batch normalization\nlayers. The batch normalization is used in the newly ini-\ntialized RoI head layer. The Ô¨Åne-tuning takes a total of 24k\niterations.\n4.1.2\nResults Comparisons.\nThe results in Tab. 1 show\nthat DUPR outperforms other unsupervised methods and\nsupervised counterparts. Most unsupervised methods out-\nperform the supervised counterpart in AP75 (which requires\nhigh localization accuracy), which indicates that the represen-\ntations learned by supervised classiÔ¨Åcation may lose much\ninformation irrelevant to classiÔ¨Åcation but useful for local-\nization. However, the previous unsupervised pre-training\n1. https://github.com/facebookresearch/detectron2\n2. https://github.com/HobbitLong/PyContrast\n3. https://github.com/facebookresearch/swav\n7\nTABLE 1\nObject detection Ô¨Åne-tuned on PASCAL VOC. All the methods are\npre-trained on ImageNet-1M. We Ô¨Åne-tune the Faster R-CNN R-50-C4\non Pascal VOC trainval07+12 and evaluate it on test2007. We\nshow the gap compared to the ImageNet supervised counterpart in\nbrackets. The increases of at least 0.5 are in green. The results of\nrandom init, supervised, and MoCo v2 are from [8]. The results of\nBYOL [19] and SimCLR [14] are from [18]. ‚Äô*‚Äô means that results are\nimplemented by us. Other methods are from their original papers. Our\nmethod outperforms the supervised counterpart by 5.5 points and the\nMoCo v2 baseline by 2.0 points. 200‚Ä†: the model is trained with 100\nepochs but with symmetric loss [19], which makes one more network\nforward pass for each image; therefore, the number of network‚Äôs\nforward pass is the same as that of MoCo v2 [17] with 200 epochs.\npre-train\nepoch\nAP\nAP50\nAP75\nrandom init.\n-\n33.8\n60.2\n33.1\nsupervised\n100\n53.5\n81.3\n58.8\nInstDis* [15]\n200\n55.2 (+1.7)\n80.9 (‚àí0.4)\n61.2 (+2.4)\nBYOL [19]\n200\n55.3 (+1.8)\n81.4 (+0.1)\n61.1 (+2.3)\nPIRL* [58]\n200\n55.5 (+2.0)\n81.0 (‚àí0.3)\n61.3 (+2.5)\nSimCLR [14]\n200\n55.5 (+2.0)\n81.8 (+0.5)\n61.4 (+2.6)\nSwAV [20]\n800\n56.1 (+2.6)\n82.6 (+1.3)\n62.7 (+3.9)\nBoWNet [59]\n200\n55.8 (+2.3)\n81.3 (+0.0)\n61.1 (+2.3)\nSimSiam [18]\n200\n57.0 (+3.5)\n82.4 (+1.1)\n63.7 (+4.9)\nMoCo [8]\n200\n55.9 (+2.4)\n81.5 (+0.2)\n62.6 (+3.8)\nMoCo v2 [17]\n200\n57.0 (+3.5)\n82.4 (+1.1)\n63.6 (+4.8)\nMoCo v2 [17]\n800\n57.4 (+3.9)\n82.5 (+1.2)\n64.0 (+5.2)\nInfomin [16]\n200\n57.6 (+4.1)\n82.6 (+1.3)\n64.3 (+5.5)\nDetCo [52]\n200\n57.8 (+4.3)\n82.6 (+1.3)\n64.2 (+5.4)\nInstLoc [50]\n200\n57.9 (+4.4)\n82.9 (+1.6)\n64.9 (+6.1)\nDenseCL [54]\n200\n58.7 (+5.2)\n82.8 (+1.5)\n65.2 (+6.4)\nPixPro [53]\n200‚Ä†\n58.8 (+5.3)\n83.0 (+1.7)\n66.5 (+7.7)\nDUPR (ours)\n200\n59.0 (+5.5)\n83.2 (+1.9)\n66.0 (+7.2)\nmethods are still designed for classiÔ¨Åcation. In contrast,\nour DUPR is designed to encode the spatial information\nexplicitly. DUPR pre-training outperforms the MoCo v2\nstrong baseline by 2.4 points in AP75 and 0.8 points in\nAP50, and further signiÔ¨Åcantly improves the localization\naccuracy. DUPR also obtains state-of-the-art performance\nin AP and AP50. When compared to the pre-training by\nsupervised classiÔ¨Åcation, DUPR largely improves the AP75\nby 7.3 points. It veriÔ¨Åes that DUPR contains more spatial\ninformation than MoCo v2 and ImageNet supervised pre-\ntraining.\n4.2\nCOCO Object Detection and Segmentation\n4.2.1\nExperimental Setup.\nWe compare the Ô¨Åne-tuning\nresults of Mask R-CNN R-50-FPN, Mask R-CNN R-50-C4,\nand RetinaNet R-50-FPN with other unsupervised and Im-\nageNet supervised counterparts, including both one-stage\nand two-stage detectors with different backbones. We Ô¨Åne-\ntune these detectors on COCO train2017 with 118k im-\nages, and test on COCO val2017. For all detectors, input\nimages are randomly resized to a scale within [640, 800]\nduring training and Ô¨Åxed at 800 for inference. All the layers\nare trained end-to-end. For Mask R-CNN R-50-FPN and\nMask R-CNN R-50-C4, we strictly follow the settings in\nMoCo [8]. For Mask R-CNN R-50-FPN and RetinaNet R-50-\nFPN, the batch normalization is used in the newly initialized\nFPN. Other parameters of RetinaNet follow the default\nsetting of Detectron2 [60]. We explore the standard 1√ó and\n2√ó schedule for these detectors. For Mask R-CNN R-50-\nFPN, we also compare the Ô¨Åne-tuned results with the strong\nbaseline MoCo v2 [17] at fewer training iterations (12k, 18k,\nand 36k iterations) to study the convergence speed.\n4.2.2\nMask R-CNN R-50-FPN.\nThe results of Mask R-\nCNN, R-50-FPN, 1√ó schedule are shown in Tab. 2 (a). DUPR\noutperforms other unsupervised methods and the super-\nvised counterparts (e.g., surpasses MoCo v2 baseline by 1.1\npoints in mAP). In 2√ó schedule, DUPR outperforms MoCo\nv2 by 0.7 points in mAP and the supervised counterpart by\n1.0 point in mAP as shown in Tab. 2 (c).\n4.2.3\nMask R-CNN R-50-C4.\nAs shown in Tab. 2 (b),\nin 1√ó schedule, DUPR outperforms all other unsupervised\nand supervised counterparts (e.g., surpasses MoCo v2 by\n1.2 points in mAP). Compared to the ImageNet supervised\npre-training, the gain in AP75 is larger than AP50 (2.4 points\nv.s. 1.9 points), indicating that DUPR pre-training improves\nthe localization ability. In 2√ó schedule, where pre-training\nis less important, our method still outperforms the MoCo v2\nby 0.5 points and the ImageNet supervised pre-training by\n1.5 points in mAP as shown in Tab. 2 (d).\n4.2.4\nRetinaNet R-50-FPN.\nWe choose to Ô¨Åne-tune the\nRetinaNet [6] with R-50-FPN on COCO in 1√ó and 2√ó\nschedule. As shown in Tab. 3, in the 1√ó schedule, MoCo\nv2 has the same AP as the supervised counterpart. Our\nmethod outperforms MoCo v2 pre-training and supervised\ncounterpart by 0.7 points in AP. Our method generalizes\nwell on the one-stage object detector.\n4.2.5\nFine-tune with Fewer Iterations.\nPre-training can\nspeed up the convergence of the object detectors [32]. So,\nwe explore the performance of different unsupervised pre-\ntrained models, when serving as initialization for Ô¨Åne-\ntuning Mask R-CNN R-50-FPN at iterations of 12k, 18k,\n36k, 90k, and 180k in Fig. 2. Our DUPR outperforms MoCo\nv2 and even the ImageNet supervised pre-training at all\niterations. When Ô¨Åne-tuning with only 12k iterations, DUPR\nsigniÔ¨Åcantly outperforms the MoCo v2 by 2.9 points in\nmAP. It indicates that DUPR provides a better initialization\nand faster convergence speed than other methods. When\nÔ¨Åne-tuning with 90 iterations, DUPR still outperforms the\nsupervised counterpart by 1.1 points.\n4.3\nOther Localization-Sensitive Tasks\n4.3.1\nInstance Seg. on Cityscapes.\nCityscapes [26] is a\ndataset that focuses on semantic understanding of urban\nstreet scenes. We Ô¨Åne-tune the Mask R-CNN R-50-FPN\nfollowing the settings in MoCo [8]. Batch normalization\nis added before the FPN. All layers are trained end-to-\nend. Other hyperparameters follow the default settings of\nDetectron2 [60]. We Ô¨Åne-tune the model on the train_fine\nset (2975 images) for 90k iterations, and test on the val set.\nDUPR outperforms MoCo v2 by 0.5 points in mAP as shown\nin Tab. 4, which indicates the DUPR pre-training has a good\ngeneralization.\n4.3.2\nSemantic Seg. on Cityscapes.\nFor the semantic\nsegmentation on Cityscapes, we follow the FCN-based\nstructure and settings used in MoCo [8]. The FCN-based\nstructure consists of the convolutional layers in Resnet 50\nand the 3 √ó 3 convolutions with dilation 2 and stride 1\nin conv5 x stage. Then it is followed by two extra 3 √ó 3\nconvolutions of 256 channels with dilation 6, and a 1 √ó 1\nconvolution is added for per-pixel classiÔ¨Åcation. Since there\n8\nTABLE 2\nObject detection and instance segmentation Ô¨Åne-tuned on COCO. All the compared methods are pre-trained for 200 epochs on ImageNet.\nThe results of random init, supervised, and MoCo are from [8]. We implement other methods in the same standard Mask R-CNN with R-50-FPN\nsetting following MoCo [8]. Note in the original implementation of DenseCL [54], InstLoc [50], and PixPro [53], they evaluate results on a variant of\nMask R-CNN with R-50-FPN by using a different RoI box head, which is different from the settings in MoCo [8]. So we download their pre-trained\nmodels and evaluate them in the same standard-setting. The increases of at least 0.5 points are in green while opposites are in red.\npre-train\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\nrandom init.\n31.0\n49.5\n33.2\n28.5\n46.8\n30.4\nsupervised\n38.9\n59.6\n42.7\n35.4\n56.5\n38.1\nInstDis [15]\n37.5 (‚àí1.4) 57.6 (‚àí2.0) 40.6 (‚àí2.1) 34.1 (‚àí1.3) 54.7 (‚àí1.8) 36.5 (‚àí1.6)\nPIRL [58]\n37.6 (‚àí1.3) 57.7 (‚àí1.9) 41.1 (‚àí1.6) 34.1 (‚àí1.3) 54.7 (‚àí1.8) 36.2 (‚àí1.9)\nSwAV [20]\n38.6 (‚àí0.3) 60.5 (+0.9) 41.5 (‚àí1.2) 35.5 (+0.1) 57.1 (+0.6) 37.8 (‚àí0.3)\nMoCo [8]\n38.5 (‚àí0.4) 58.9 (‚àí0.7) 42.0 (‚àí0.7) 35.1 (‚àí0.3) 55.9 (‚àí0.6) 37.7 (‚àí0.4)\nMoCo v2 [17]\n38.9 (+0.0) 59.4 (‚àí0.2) 42.4 (‚àí0.3) 35.5 (+0.1) 56.5 (+0.0) 38.2 (+0.1)\nVADeR [49]\n39.2 (+0.3) 59.7 (+0.1) 42.7 (+0.0) 35.6 (+0.2) 56.7 (+0.2) 38.2 (+0.1)\nDenseCL [54]\n39.4 (+0.5) 59.9 (+0.3) 42.7 (+0.0) 35.6 (+0.2) 56.7 (+0.2) 38.2 (+0.1)\nInstLoc [50]\n39.3 (+0.4) 59.8 (+0.2) 42.9 (+0.2) 35.7 (+0.3) 56.9 (+0.4) 38.4 (+0.3)\nDetCo [52]\n40.1 (+1.2) 61.0 (+1.4) 43.9 (+1.2) 36.4 (+1.0) 58.0 (+1.5) 38.9 (+0.8)\nPixPro [53]\n39.7 (+0.8) 60.0 (+0.4) 43.5 (+0.8) 36.1 (+0.7) 57.1 (+0.6) 38.9 (+0.8)\nDUPR (ours)\n40.0 (+1.1) 60.4 (+0.8) 43.4 (+0.7) 36.2 (+0.8) 57.6 (+1.1) 38.9 (+0.8)\n(a) Mask R-CNN, R50-FPN, 1√ó schedule\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\n26.4\n44.0\n27.8\n29.3\n46.9\n30.8\n38.2\n58.2\n41.2\n33.3\n54.7\n35.2\n37.8 (‚àí0.4) 57.0 (‚àí1.2) 41.0 (‚àí0.2) 33.1 (‚àí0.2) 54.2 (‚àí0.5) 35.3 (+0.1)\n37.4 (‚àí0.8) 56.6 (‚àí1.6) 40.3 (‚àí0.9) 32.8 (‚àí0.5) 53.4 (‚àí1.3) 34.8 (‚àí0.4)\n33.0 (‚àí5.2) 54.3 (‚àí3.9) 34.6 (‚àí6.6) 29.5 (‚àí3.8) 50.4 (‚àí4.3) 30.4 (‚àí4.8)\n38.5 (+0.3) 58.3 (+0.1) 41.6 (+0.4) 33.6 (+0.3) 54.8 (+0.1) 35.6 (+0.4)\n38.9 (+0.7) 58.5 (+0.3) 42.1 (+0.9) 34.2 (+0.9) 55.2 (+0.5) 36.6 (+1.4)\n-\n-\n-\n-\n-\n-\n39.3 (+1.1) 58.8 (+0.6) 42.5 (+1.3) 34.3 (+1.0) 55.3 (+0.6) 36.7 (+1.5)\n39.5 (+1.3) 59.1 (+0.9) 42.7 (+1.5) 34.5 (+1.2) 56.0 (+1.3) 36.8 (+1.6)\n39.8 (+1.0) 59.7 (+1.5) 43.0 (+1.8) 34.7 (+1.4) 56.3 (+1.6) 36.7 (+1.5)\n40.0 (+1.8) 59.4 (+1.2) 43.2 (+2.0) 34.8 (+1.5) 56.1 (+1.4) 37.3 (+2.1)\n40.1 (+1.9) 59.8 (+1.6) 43.6 (+2.4) 34.9 (+1.6) 56.5 (+1.8) 37.3 (+2.1)\n(b) Mask R-CNN, R50-C4, 1√ó schedule\npre-train\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\nrandom init.\n36.7\n56.7\n40.0\n33.7\n53.8\n35.9\nsupervised\n40.6\n61.3\n44.4\n36.8\n58.1\n39.5\nMoCo v2 [17]\n40.9 (+0.3) 61.5 (+0.2) 44.7 (+0.3) 37.0 (+0.2) 58.7 (+0.6) 39.8 (+0.4)\nDUPR (ours)\n41.7 (+1.0) 62.3 (+1.0) 45.2 (+0.8) 37.5 (+0.7) 59.2 (+1.1) 40.2 (+0.7)\n(c) Mask R-CNN, R50-FPN, 2√ó schedule\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\n35.6\n54.6\n38.2\n31.4\n51.5\n33.5\n40.0\n59.9\n43.1\n34.7\n56.5\n36.9\n41.0 (+1.0) 60.5 (+0.6) 44.5 (+1.4) 35.7 (+1.0) 57.3 (+0.8) 38.1 (+1.2)\n41.5 (+1.5) 61.2 (+1.3) 45.1 (+2.0) 36.0 (+1.3) 58.0 (+1.5) 38.5 (+1.6)\n(d) Mask R-CNN, R50-C4, 2√ó schedule\nTABLE 3\nObject detection of RetinaNet Ô¨Åne-tuned on COCO. All the\nunsupervised models are pre-trained on ImageNet for 200 epochs. All\nthe results are implemented by us in the same settings.\npre-train\nAP\nAP50\nAP75\nrandom init.\n24.5\n39.0\n25.7\nsupervised\n37.4\n56.5\n39.7\nMoCo v2 [17]\n37.4 (+0.0)\n56.5 (+0.0)\n40.0 (+0.3)\nDUPR (ours)\n38.1 (+0.7)\n57.3 (+0.8)\n41.1 (+1.4)\n(a) RetinaNet, R50-FPN, 1√ó schedule\npre-train\nAP\nAP50\nAP75\nrandom init.\n32.2\n49.4\n34.2\nsupervised\n38.9\n58.5\n41.5\nMoCo v2 [17]\n39.4 (+0.5)\n59.0 (+0.5)\n42.2 (+0.7)\nDUPR (ours)\n40.0 (+1.1)\n59.6 (+1.1)\n43.0 (+1.5)\n(b) RetinaNet, R50-FPN, 2√ó schedule\nis no ofÔ¨Åcially released code, we reimplement the FCN-\nbased structure. DUPR outperforms MoCo v2 by 0.9 points\nas shown in Tab. 4. The results on Cityscapes indicate that\nDUPR pre-training transfers well to other region-level tasks.\n4.3.3\nKeypoint Detection on COCO.\nThe task of key-\npoint detection is to simultaneously detect people and locate\ntheir keypoints. We Ô¨Åne-tune the Mask R-CNN R-50-FPN\n(keypoint version) on COCO train2017 and evaluate it\non COCO val2017, following [8]. DUPR outperforms the\nsupervised counterpart by 1.3 points in mAP as shown in\nTab. 4.\n4.3.4\nInstance Seg. on LVIS-v1.0.\nLVIS [27] is an in-\nstance segmentation dataset, which has 1203 long tail dis-\ntributed categories and provides high-quality segmentation\nmasks. We Ô¨Åne-tune the Mask R-CNN R-50-FPN on LVIS\ntrain_v1.0 and test on LVIS val_v1.0. We train the\nTABLE 4\nComparison with other pre-training methods, Ô¨Åne-tuned on\nvarious localization-sensitive tasks. Our method outperforms MoCo\nv2 baseline and supervised counterparts. The seg. denotes the\nsemantic segmentation.\nCityscapes Instance seg.\nCityscapes Seg.\npre-train\nAPmk\nAPmk\n50\nmIoU\nrandom init.\n25.4\n51.1\n65.3\nsupervised\n32.9\n59.6\n74.6\nMoCo v2 [17]\n33.9 (+1.0)\n60.8 (+1.2)\n75.7 (+1.1)\nDUPR (ours)\n34.4 (+1.5)\n62.3 (+2.7)\n76.7 (+2.1)\nCOCO Keypoint Detection\npre-train\nAPkp\nAPkp\n50\nAPkp\n75\nrandom init.\n65.9\n86.5\n71.7\nsupervised\n65.8\n86.9\n71.9\nMoCo v2\n66.8 (+1.0)\n87.3 (+0.4)\n73.1 (+1.2)\nDUPR IN-1M\n67.1 (+1.3)\n87.4 (+0.5)\n72.9 (+1.0)\nLVIS-v1.0 Instance Seg.\npre-train\nAPmk\nAPmk\n50\nAPmk\n75\nrandom init.\n19.1\n29.8\n20.2\nsupervised\n22.3\n34.7\n23.5\nMoCo v2\n22.8 (+0.5)\n35.2 (+0.5)\n24.4 (+0.9)\nDUPR\n23.8 (+1.5)\n36.1 (+1.4)\n25.3 (+1.8)\nmodel for a total of 180k iterations. DUPR outperforms\nMoCo v2 by 1.0 point in APmk as shown in Tab. 4.\n4.4\nObject Detection v.s. ClassiÔ¨Åcation\nObject detection includes both classiÔ¨Åcation and localiza-\ntion. To better understand why DUPR improves object\ndetection, we also report the results of linear probing on\nImageNet for reference to obtain more insight. We compare\nthe classiÔ¨Åcation accuracy and detection mAP for various\nunsupervised pre-training methods in Tab. 5. We notice that\nunder these settings, with the compared models, there exists\n9\nTABLE 5\nObject detection v.s. ClassiÔ¨Åcation. The ImageNet cls. means linear\nevaluation on ImageNet. For COCO and VOC detection, we report AP\nof Mask R-CNN with R-50-C4 and Faster R-CNN with R-50-C4,\nrespectively. All the self-supervised models are pre-trained with 200\nepochs. The results of SimCLR [14], BYOL [19], SwAV [20], and\nSimSiam [18] are from [18]. Note that the DenseCL [54] is a\nconcurrent self-supervised learning method designed for object\ndetection. We can see from this table that the improvements in the\nImageNet linear evaluation do not always lead to the improvements in\nthe transfer performance to object detection.\npre-train\nImageNet cls.\nCOCO Det.\nVOC Det.\nSimCLR [14]\n68.3\n37.9\n55.5\nBYOL [19]\n70.6\n37.9\n55.3\nSwAV [20]\n69.1\n37.6\n55.4\nMoCo v2 [17]\n67.5\n38.9\n57.0\nSimSiam [18]\n70.0\n39.2\n57.0\nDenseCL [54]\n63.6\n39.3\n58.7\nDUPR\n63.6\n40.1\n59.0\nlittle correlation between ImageNet classiÔ¨Åcation and object\ndetection performance. For example, MoCo v2 [17] is lower\nthan BYOL [19] by 3.1 points in ImageNet accuracy but\nhigher than BYOL [19] by 1.7 points in VOC AP. DUPR and\nDenseCL [54] have a drop of 3.7 points in ImageNet linear\nevaluation compared to the MoCo v2 baseline. This drop\nis likely resulted from the joint optimization of both patch-\nlevel and image-level contrastive loss, which is more difÔ¨Åcult\nand will affect the optimization of image-level contrastive\nloss. A better balance between global representations for\nclassiÔ¨Åcation and local representations for localization is\npossible but is not the focus of this paper. We can conclude\nthat the improvements of DUPR in object detection are not\nfrom the ability of better classiÔ¨Åcation but the ability of\nbetter localization.\n4.5\nAblation Experiments\n4.5.1\nExperimental Setup.\nThe ablation experiments are\nconducted on PASCAL VOC with Faster R-CNN R-50-C4\nand COCO with R-50-FPN. We also report the SVM clas-\nsiÔ¨Åcation on PASCAL VOC, following the settings in [61],\nwhere the feature is Ô¨Åxed and used to train an SVM classiÔ¨Åer\non PASCAL VOC classiÔ¨Åcation task.\n4.5.2\nInÔ¨Çuence of Œ±0:3 for Image Contrastive Loss.\nIn\nthis ablation study, the Œ≤0:3 is set as (0, 0, 0, 0). The results\nin Tab. 6 (a) show: (1) all conÔ¨Ågurations of Œ±m can improve\nthe AP in VOC detection, especially for the high IoU metric\nAP75, suggesting that intermediate level contrastive loss can\nimprove the localization ability; (2) only the conÔ¨Åguration\nof (0.1, 0.4, 0.7, 1.0) has improvements on both VOC and\nCOCO detection. The reasons why large weights (i.e., con-\nÔ¨Åguration of (1, 1, 1, 1)) on shallow layers decrease the\nperformance on COCO detection are two aspects: (1) large\nweights on shallow layers will inÔ¨Çuence the optimization\nof deep layers, which is more important for classiÔ¨Åcation\nthan shallow layers; (2) COCO contains more classes than\nVOC, and the performance on COCO relies more on the\nclassiÔ¨Åcation ability. In fact, setting the deep layers with\nlarger weights is a common practice in [62], [63], [64], [65],\nalthough their tasks are different from ours.\nTABLE 6\nAblation of Œ±m for image contrastive loss. All models are\npre-trained with 200 epochs on ImageNet. Œ±m is the weight to balance\neach layer in Eq. (2). (a) Object detection is Ô¨Åne-tuned on Pascal VOC\ntrainval07+12 and tested on the Pascal VOC test 2007. (b) The\nSVM classiÔ¨Åcation is Ô¨Åne-tuned on Pascal VOC 2007. The results show\nthat there exists a correlation between the classiÔ¨Åcation ability for\nintermediate layers and transferring ability to object detection.\nVOC\nCOCO\nŒ±0:3\nAP\nAP50\nAP75\nAPbb\nAPmk\n0,0,0,1\n57.0\n82.4\n63.6\n38.9\n35.5\n0,0,1,1\n57.4 (+0.4) 82.7 (+0.3) 64.1 (+0.5) 38.9 (+0.0) 35.0 (‚àí0.5)\n1,1,1,1\n57.4 (+0.4) 82.3 (‚àí0.1) 63.9 (+0.3) 38.5 (‚àí0.4) 35.0 (‚àí0.5)\n0.1,0.4,0.7,1\n57.6 (+0.6) 82.4 (+0.0) 64.2 (+0.6) 39.2 (+0.3) 35.5 (+0.0)\n(a) Faster R-CNN, R50-C4\nŒ±0:3\nconv2 x\nconv3 x\nconv4 x\nconv5 x\n0,0,0,1\n47.3\n58.8\n74.0\n84.1\n0,0,1,1\n47.6 (+0.3)\n60.0 (+1.2)\n81.0 (+7.0)\n83.7 (‚àí0.4)\n1,1,1,1\n57.2 (+9.9)\n70.5 (+11.7)\n80.3 (+6.3)\n82.9 (‚àí1.2)\n0.1,0.4,0.7,1\n52.1 (+4.8)\n68.8 (+10.0)\n80.9 (+6.9)\n83.7 (‚àí0.4)\n(b) SVM ClassiÔ¨Åcation of Different Levels\nNow we study the classiÔ¨Åcation ability of intermediate\nlayers. It can be seen in Tab. 6 (b) that all three conÔ¨Åg-\nurations of Œ±0:3 largely improve the classiÔ¨Åcation perfor-\nmance of shallow layers. For example, when set Œ±0:3 =\n(0.1, 0.4, 0.7, 1.0), it improves the classiÔ¨Åcation performance\nof conv3 x by 10 points and conv4 x by 6.3 points. We also\nnotice a slight decrease in the classiÔ¨Åcation performance\nof the conv5 x, which makes sense as the optimization of\nmulti-level contrastive loss is more challenging than single-\nlevel one. Combining Tab. 6 (a) and Tab. 6 (b) we can con-\nclude: better representations of shallow layers can improve\ntransfer performance, especially for the localization aspect\nof object detection.\n4.5.3\nInÔ¨Çuence of RoI Size.\nIn this ablation study, we\nset Œ±0:3 = (0.1, 0.4, 0.7, 1.0) and Œ≤0:3 = (0, 0, 0, 1). The\nresults of different RoI sizes are presented in Tab. 7 (a).\nWe can see that Œ≤0:3 = (0, 0, 0, 1) with RoI size of 1 does\nnot have much difference compared to Œ≤0:3 = (0, 0, 0, 0)\nin object detection, where the patch Re-ID degenerates to\nglobal view contrastive learning and has no constraint on\nthe local representations, although it is spatially aligned.\nLarge RoI size improves the performance, which indicates\nthat larger RoI size can get more discriminative region-level\nrepresentations for object detection. But it is not always the\nlarger, the better: if the RoI size is larger than the size of a\nfeature map, it will not acquire more useful information. For\nexample, the improvements become saturated when the RoI\nsize is larger than 7, which is the size of conv5 x feature\nmap. So we set RoI size as 7 on conv5 x by default. Similarly,\nthe size of conv4 x is 14, so we set the RoI size as 14 on\nconv4 x by default.\n4.5.4\nInÔ¨Çuence of Œ≤0:3 for Patch Contrastive Loss.\nIn\nthis ablation study, we set Œ±0:3 = (0.1, 0.4, 0.7, 1.0). Simply\nadding patch loss to conv5 x (by setting Œ≤0:3 = (0, 0, 0, 1))\nimproves the AP by 0.7 points in VOC and 0.3 points in\nCOCO. And adding patch loss to both conv4 x and conv5 x\n(by setting Œ≤0:3 = (0, 0, 1, 1)) feature maps further improves\nthe AP by 0.5 points in COCO and 0.2 points in VOC\nas shown in Tab. 7 (b), which indicates that intermediate\nsupervision and patch contrastive loss are complementary.\n10\nTABLE 7\n(a) Ablation of RoI size. A larger size of RoI has better results than a\nsmaller one. We add patch loss to the conv5 x feature map to do the\nRoI size ablation study on Pascal VOC, Faster R-CNN, R-50-C4 and on\nCOCO, Mask R-CNN, R-50-FPN. (b) Ablation of Œ≤m. We use RoI\nsizes of 7 and 14 for conv5 x and conv4 x feature maps, respectively.\n(c) We compare single image-level and patch-level contrastive loss on\nconv5 x.\nVOC\nCOCO\nRoI Size\nAP\nAP50\nAP75\nAPbb\nAPmk\n1\n57.7\n82.4\n63.9\n39.1\n35.4\n3\n58.1\n82.5\n65.0\n39.3\n35.6\n7\n58.3\n82.7\n64.9\n39.5\n35.7\n9\n58.4\n82.8\n65.3\n39.4\n35.7\n11\n58.4\n82.9\n65.2\n39.5\n35.7\n(a) Ablation of RoI Size\nVOC\nCOCO\nŒ≤0:3\nAP\nAP50\nAP75\nAPbb\nAPmk\n0,0,0,0\n57.6\n82.4\n64.2\n39.2\n35.5\n0,0,0,1\n58.3\n82.7\n64.9\n39.5\n35.7\n0,0,1,1\n58.5\n83.2\n65.2\n40.0\n36.2\n1,1,1,1\n58.5\n82.8\n65.2\n39.7\n36.0\n0.1,0.4,0.7,1\n58.3\n83.5\n65.3\n39.9\n36.1\n(b) Ablation of Œ≤0:3\nVOC\nCOCO\nAP\nAP50\nAP75\nAPbb\nAPmk\nimage-level\n57.0\n82.4\n63.6\n38.9\n35.5\npatch-level\n57.9\n82.2\n64.6\n38.8\n35.3\n(c) Image and patch contrastive loss at conv5 x\n4.5.5\nImage-level v.s. Patch-level Contrastive Loss.\nWe\ncompare the single image-level and patch-level contrastive\nloss on conv5 x in Tab. 7 (c). Patch-level contrastive loss\nimproves the localization ability but slightly reduces the\nclassiÔ¨Åcation ability, compared to the image-level contrastive\nloss. For example, in VOC detection, patch-level contrastive\nloss signiÔ¨Åcantly improves the AP75 (which is more related\nto localization ability) by 1.0 point but slightly reduces\nthe AP50 by 0.2. It also slightly reduces the COCO AP by\n0.1 points, since classiÔ¨Åcation ability is more important to\nCOCO than VOC (note COCO has 80 categories while VOC\nhas 20 categories). When we combine both image-level and\npatch-level contrastive loss and achieve a balance between\nclassiÔ¨Åcation and localization as shown in the bottom line\nof Tab. 7 (b), we achieve the best performance.\n4.6\nVisualization\n4.6.1\nVisualization of Spatial Sensitivity.\nTo verify that\npatch Re-ID can learn spatial-sensitive features, we draw\nthe IoU-similarity curve: each point in the curve is obtained\nby calculating the IoU and similarity between a RoI and the\nground truth box. We randomly select 1000 images from the\nImageNet validation set. For each image, we sample 20 RoIs\nwith different IoUs (range from 0 to 1) between the ground\ntruth box and RoIs. We then use the RoI Align to extract\nregion features from conv4 x feature map, according to the\nground truth box and RoIs. Denote the feature extracted\nfrom a ground truth box as q and a feature extracted from a\nRoI as kŒ±, where the Œ± denotes the IoU between the ground\ntruth box and the RoI. Both q and kŒ± are of shape (C, S√óS).\nWe calculate an average of cosine similarity between q and\nkŒ± as:\nSim(q, kŒ±) =\n1\nS √ó S\nS√óS\nX\np=1\nqp¬∑kŒ±,p,\n(9)\nSim: 0.4\nSim: 0.8\nSim: 0.6\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.1\n0.3\n0.5\n0.7\n0.9\nSimilarity with GT\nIoU with ground truth box\nDUPR\nMoCo v2\n(a)\n(b)\nFig. 6. The visualization of spatial sensitivity. (a) We visualize the\nsimilarity between ground truth (Red bounding box) and RoIs (blue\nboxes with different IoUs) using the Eq. (9). (b) shows that DUPR learns\nmore spatial-sensitive features than MoCo v2: the curve of DUPR has a\nlarger slope than that of MoCo v2. More speciÔ¨Åcally, DUPR has a higher\nsimilarity between RoI and ground truth box when their IoU is larger than\n0.5, and has a smaller similarity than MoCo v2 when IoU is less than\n0.3. It means that features learned by DUPR are more discriminative\nthan MoCo v2 to suppress wrong predictions (boxes with small IoU).\nwhere qp and kŒ±,p are normalized features at position p of\nregion features. We compare DUPR with MoCo v2 in Fig. 6.\nThis visualization shows that DUPR is more sensitive to\nIoU: DUPR has a steeper slope than MoCo v2. When IoU\nbetween RoI and ground truth box is above 0.5 (usually\nassigned as positive samples in object detection [5]), DUPR\nhas a higher similarity; when IoU is below 0.3 (usually\nassigned as negative samples), DUPR has a lower similarity.\nSuch property makes the representations easy to suppress\nthe false-negative RoIs.\n4.6.2\nVisualization of Correspondence.\nWe use the\nlearned representations to visualize the correspondence be-\ntween two views. First, we create two views via random\ndata augmentation following the settings in [8]. We set the\nimage size to 448. We use the feature map of conv4 x (which\nis of shape (1024, 28, 28)) for matching. For each patch (one\nof 28 √ó 28) feature in one view, we Ô¨Ånd the patch with\nthe highest similarity in the other view. For visualization,\nwe use the point to represent a patch. The results shown\nin Fig. 7 demonstrate that patch Re-ID matches the corre-\nsponding patches better than MoCo v2.\n5\nCONCLUSION\nThis paper presents an unsupervised visual representation\nlearning method, named DUPR, to bridge the gap between\nthe unsupervised pre-training and downstream object de-\ntection tasks. Different from the previous methods that only\nlearn discriminative image-level representations in the Ô¨Ånal\nlayer, our method learns discriminative region-level multi-\nlevel representations. Therefore, our method outperforms\nother unsupervised models and even the supervised coun-\nterpart when transferred to downstream tasks related to\nobject detection. Moreover, our method is robust to various\nobject detectors, Ô¨Åne-tuning iterations. We hope our simple\nyet effective method could serve as a baseline of unsuper-\nvised pre-training for object detection tasks.\nIt is worth noticing that the DUPR presented in this\npaper learns representations for object detection from static\nimages. While, rather than static images, we humans learn\nmore from videos, which contain not only spatial conÔ¨Åg-\nurations but also rich temporal information. Thus, it is of\ngreat interest to extend the proposed DUPR for learning\n11\nPatch contrast \nImage contrast\nPatch contrast \nImage contrast\nPatch contrast \nImage contrast\nFig. 7. Visualization of Correspondence. We use pre-trained representations to match patches in two views, and compare the results of two self-\nsupervised tasks: (1) patch-level contrastive learning (patch Re-ID); (2) image-level contrastive learning (MoCov2). For fair comparison, both two\ntasks only add loss on the conv5 x with the same loss weight. For visualization, we use points to represent patches. It shows that representations\nof patch Re-ID can match the local patches in the other view better than MoCo v2.\nspatial-temporal representations from videos in the future.\nMoreover, our current implementation of DUPR is based\non MoCov2 [17] and needs multiple memory banks. To\nsave the memory occupation, we will further study how\nto apply DUPR to other self-supervised learning methods\n(e.g., BYOL [19] or SimSiam [18]). Beyond the pretext task,\nhow to design more suitable data augmentation pipeline for\ndense prediction tasks can also be studied in the future.\nREFERENCES\n[1]\nR. Girshick, J. Donahue, T. Darrell, and J. Malik, ‚ÄúRich feature hier-\narchies for accurate object detection and semantic segmentation,‚Äù\nin CVPR, 2014, pp. 580‚Äì587.\n[2]\nJ. Long, E. Shelhamer, and T. Darrell, ‚ÄúFully convolutional net-\nworks for semantic segmentation,‚Äù in CVPR, 2015, pp. 3431‚Äì3440.\n[3]\nK. He, G. Gkioxari, P. Doll¬¥ar, and R. Girshick, ‚ÄúMask r-cnn,‚Äù in\nICCV, 2017, pp. 2961‚Äì2969.\n[4]\nK. Simonyan and A. Zisserman, ‚ÄúTwo-stream convolutional net-\nworks for action recognition in videos,‚Äù arXiv:1406.2199, 2014.\n[5]\nS. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-\n12\ntime object detection with region proposal networks,‚Äù in NeurIPS,\n2015, pp. 91‚Äì99.\n[6]\nT.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll¬¥ar, ‚ÄúFocal loss for\ndense object detection,‚Äù in ICCV, 2017, pp. 2980‚Äì2988.\n[7]\nS. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and\nJ. Sun, ‚ÄúObjects365: A large-scale, high-quality dataset for object\ndetection,‚Äù in ICCV, 2019, pp. 8430‚Äì8439.\n[8]\nK. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, ‚ÄúMomentum contrast\nfor unsupervised visual representation learning,‚Äù in CVPR, 2020,\npp. 9729‚Äì9738.\n[9]\nM. Caron, P. Bojanowski, A. Joulin, and M. Douze, ‚ÄúDeep cluster-\ning for unsupervised learning of visual features,‚Äù in ECCV, 2018,\npp. 132‚Äì149.\n[10] J. Donahue and K. Simonyan, ‚ÄúLarge scale adversarial representa-\ntion learning,‚Äù in NeurIPS, 2019, pp. 10 542‚Äì10 552.\n[11] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, ‚ÄúGenerative adversarial\nnets,‚Äù in NeurIPS, 2014, pp. 2672‚Äì2680.\n[12] C. Zhuang, A. L. Zhai, and D. Yamins, ‚ÄúLocal aggregation for\nunsupervised learning of visual embeddings,‚Äù in ICCV, 2019, pp.\n6002‚Äì6012.\n[13] G. E. Hinton and R. S. Zemel, ‚ÄúAutoencoders, minimum descrip-\ntion length and helmholtz free energy,‚Äù in NeurIPS, 1994, pp. 3‚Äì10.\n[14] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, ‚ÄúA simple\nframework for contrastive learning of visual representations,‚Äù in\nICML, 2020, pp. 1597‚Äì1607.\n[15] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, ‚ÄúUnsupervised feature\nlearning via non-parametric instance discrimination,‚Äù in CVPR,\n2018, pp. 3733‚Äì3742.\n[16] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola,\n‚ÄúWhat makes for good views for contrastive learning?‚Äù in\nNeurIPS, 2020.\n[17] X. Chen, H. Fan, R. Girshick, and K. He, ‚ÄúImproved baselines with\nmomentum contrastive learning,‚Äù arXiv:2003.04297, 2020.\n[18] X. Chen and K. He, ‚ÄúExploring simple siamese representation\nlearning,‚Äù arXiv:2011.10566, 2020.\n[19] J.\nGrill,\nF.\nStrub,\nF.\nAltch¬¥e,\nC.\nTallec,\nP.\nH.\nRichemond,\nE. Buchatskaya, C. Doersch, B. ¬¥A. Pires, Z. Guo, M. G. Azar, B. Piot,\nK. Kavukcuoglu, R. Munos, and M. Valko, ‚ÄúBootstrap your own\nlatent - A new approach to self-supervised learning,‚Äù in NeurIPS,\n2020.\n[20] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,\n‚ÄúUnsupervised learning of visual features by contrasting cluster\nassignments,‚Äù NeurIPS, 2020.\n[21] T.-Y. Lin, P. Doll¬¥ar, R. Girshick, K. He, B. Hariharan, and S. Be-\nlongie, ‚ÄúFeature pyramid networks for object detection,‚Äù in CVPR,\n2017, pp. 2117‚Äì2125.\n[22] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, ‚ÄúPath aggregation network\nfor instance segmentation,‚Äù in CVPR, 2018, pp. 8759‚Äì8768.\n[23] A. v. d. Oord, Y. Li, and O. Vinyals, ‚ÄúRepresentation learning with\ncontrastive predictive coding,‚Äù arXiv:1807.03748, 2018.\n[24] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in\ncontext,‚Äù in ECCV.\nSpringer, 2014, pp. 740‚Äì755.\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman, ‚ÄúThe pascal visual object classes (voc) challenge,‚Äù IJCV,\nvol. 88, no. 2, pp. 303‚Äì338, 2010.\n[26] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-\nnenson, U. Franke, S. Roth, and B. Schiele, ‚ÄúThe cityscapes dataset\nfor semantic urban scene understanding,‚Äù in CVPR, 2016.\n[27] A. Gupta, P. Dollar, and R. Girshick, ‚ÄúLvis: A dataset for large\nvocabulary instance segmentation,‚Äù in CVPR, 2019, pp. 5356‚Äì5364.\n[28] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\n‚ÄúSelective search for object recognition,‚Äù IJCV, vol. 104, no. 2, pp.\n154‚Äì171, 2013.\n[29] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and\nA. C. Berg, ‚ÄúSsd: Single shot multibox detector,‚Äù in ECCV, 2016,\npp. 21‚Äì37.\n[30] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‚ÄúYou only look\nonce: UniÔ¨Åed, real-time object detection,‚Äù in CVPR, 2016, pp. 779‚Äì\n788.\n[31] Z. Tian, C. Shen, H. Chen, and T. He, ‚ÄúFcos: Fully convolutional\none-stage object detection,‚Äù in ICCV, 2019, pp. 9627‚Äì9636.\n[32] K. He, R. Girshick, and P. Doll¬¥ar, ‚ÄúRethinking imagenet pre-\ntraining,‚Äù in ICCV, 2019, pp. 4918‚Äì4927.\n[33] Y. Zhong, J. Wang, L. Wang, J. Peng, Y.-X. Wang, and L. Zhang,\n‚ÄúDap: Detection-aware pre-training with weak supervision,‚Äù\narXiv:2103.16651, 2021.\n[34] S. Gidaris, P. Singh, and N. Komodakis, ‚ÄúUnsupervised represen-\ntation learning by predicting image rotations,‚Äù in ICLR, 2018.\n[35] C. Doersch, A. Gupta, and A. A. Efros, ‚ÄúUnsupervised visual\nrepresentation learning by context prediction,‚Äù in ICCV, 2015, pp.\n1422‚Äì1430.\n[36] M. Noroozi and P. Favaro, ‚ÄúUnsupervised learning of visual\nrepresentations by solving jigsaw puzzles,‚Äù in ECCV, 2016, pp.\n69‚Äì84.\n[37] R. Hadsell, S. Chopra, and Y. LeCun, ‚ÄúDimensionality reduction\nby learning an invariant mapping,‚Äù in CVPR, 2006, pp. 1735‚Äì1742.\n[38] Y. Tian, D. Krishnan, and P. Isola, ‚ÄúContrastive multiview coding,‚Äù\narXiv:1906.05849, 2019.\n[39] O. J. H¬¥enaff, A. Srinivas, J. De Fauw, A. Razavi, C. Doersch,\nS. Eslami, and A. v. d. Oord, ‚ÄúData-efÔ¨Åcient image recognition\nwith contrastive predictive coding,‚Äù arXiv:1905.09272, 2019.\n[40] A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. A. Riedmiller,\nand T. Brox, ‚ÄúDiscriminative unsupervised feature learning with\nexemplar convolutional neural networks,‚Äù TPAMI, pp. 1734‚Äì1747,\n2016.\n[41] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal,\nP. Bachman, A. Trischler, and Y. Bengio, ‚ÄúLearning deep repre-\nsentations by mutual information estimation and maximization,‚Äù\narXiv:1808.06670, 2018.\n[42] P. Bachman, R. D. Hjelm, and W. Buchwalter, ‚ÄúLearning repre-\nsentations by maximizing mutual information across views,‚Äù in\nNeurIPS, 2019, pp. 15 535‚Äì15 545.\n[43] T. Xiao, X. Wang, A. A. Efros, and T. Darrell, ‚ÄúWhat should not be\ncontrastive in contrastive learning,‚Äù arXiv:2008.05659, 2020.\n[44] R. Zhang, P. Isola, and A. A. Efros, ‚ÄúColorful image colorization,‚Äù\nin ECCV.\nSpringer, 2016, pp. 649‚Äì666.\n[45] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, ‚ÄúExtract-\ning and composing robust features with denoising autoencoders,‚Äù\nin ICML, 2008, pp. 1096‚Äì1103.\n[46] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros,\n‚ÄúContext encoders: Feature learning by inpainting,‚Äù in CVPR,\n2016, pp. 2536‚Äì2544.\n[47] X. Wang, A. Jabri, and A. A. Efros, ‚ÄúLearning correspondence from\nthe cycle-consistency of time,‚Äù in CVPR, 2019, pp. 2566‚Äì2576.\n[48] N. Wang, Y. Song, C. Ma, W. Zhou, W. Liu, and H. Li, ‚ÄúUnsuper-\nvised deep tracking,‚Äù in CVPR, 2019, pp. 1308‚Äì1317.\n[49] P. O. Pinheiro, A. Almahairi, R. Y. Benmaleck, F. Golemo, and\nA. Courville, ‚ÄúUnsupervised learning of dense visual represen-\ntations,‚Äù arXiv:2011.05499, 2020.\n[50] C. Yang, Z. Wu, B. Zhou, and S. Lin, ‚ÄúInstance localization for\nself-supervised detection pretraining,‚Äù arXiv:2102.08318, 2021.\n[51] S. Liu, Z. Li, and J. Sun, ‚ÄúSelf-emd: Self-supervised object detection\nwithout imagenet,‚Äù arXiv:2011.13677, 2020.\n[52] E. Xie, J. Ding, W. Wang, X. Zhan, H. Xu, Z. Li, and P. Luo,\n‚ÄúDetco: Unsupervised contrastive learning for object detection,‚Äù\narXiv:2102.04803, 2021.\n[53] Z. Xie, Y. Lin, Z. Zhang, Y. Cao, S. Lin, and H. Hu, ‚ÄúPropagate\nyourself: Exploring pixel-level consistency for unsupervised vi-\nsual representation learning,‚Äù in CVPR, 2021, pp. 16 684‚Äì16 693.\n[54] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li, ‚ÄúDense contrastive\nlearning for self-supervised visual pre-training,‚Äù arXiv:2011.09157,\n2020.\n[55] Z. Tian, C. Shen, H. Chen, and T. He, ‚ÄúFcos: Fully convolutional\none-stage object detection,‚Äù in ICCV, 2019, pp. 9627‚Äì9636.\n[56] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, ‚ÄúRandaugment:\nPractical automated data augmentation with a reduced search\nspace,‚Äù in CVPRW, 2020, pp. 702‚Äì703.\n[57] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for\nimage recognition,‚Äù in CVPR, 2016, pp. 770‚Äì778.\n[58] I. Misra and L. v. d. Maaten, ‚ÄúSelf-supervised learning of pretext-\ninvariant representations,‚Äù in CVPR, 2020, pp. 6707‚Äì6717.\n[59] S. Gidaris, A. Bursuc, N. Komodakis, P. P¬¥erez, and M. Cord,\n‚ÄúLearning representations by predicting bags of visual words,‚Äù\nin CVPR, 2020, pp. 6928‚Äì6938.\n[60] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, and R. Girshick, ‚ÄúDetec-\ntron2,‚Äù https://github.com/facebookresearch/detectron2, 2019.\n[61] P. Goyal, D. Mahajan, A. Gupta, and I. Misra, ‚ÄúScaling and\nbenchmarking self-supervised visual representation learning,‚Äù in\nICCV, 2019, pp. 6391‚Äì6400.\n13\n[62] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, ‚ÄúPyramid scene parsing\nnetwork,‚Äù in CVPR, 2017, pp. 2881‚Äì2890.\n[63] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang, ‚ÄúOcnet:\nObject context for semantic segmentation,‚Äù IJCV, pp. 1‚Äì24, 2021.\n[64] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with\nconvolutions,‚Äù in CVPR, 2015, pp. 1‚Äì9.\n[65] M.\nContributors,\n‚ÄúMMSegmentation:\nOpenmmlab\nsemantic\nsegmentation toolbox and benchmark,‚Äù https://github.com/\nopen-mmlab/mmsegmentation, 2020.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-03-08",
  "updated": "2022-04-10"
}