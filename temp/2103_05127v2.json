{
  "id": "http://arxiv.org/abs/2103.05127v2",
  "title": "Model Complexity of Deep Learning: A Survey",
  "authors": [
    "Xia Hu",
    "Lingyang Chu",
    "Jian Pei",
    "Weiqing Liu",
    "Jiang Bian"
  ],
  "abstract": "Model complexity is a fundamental problem in deep learning. In this paper we\nconduct a systematic overview of the latest studies on model complexity in deep\nlearning. Model complexity of deep learning can be categorized into expressive\ncapacity and effective model complexity. We review the existing studies on\nthose two categories along four important factors, including model framework,\nmodel size, optimization process and data complexity. We also discuss the\napplications of deep learning model complexity including understanding model\ngeneralization, model optimization, and model selection and design. We conclude\nby proposing several interesting future directions.",
  "text": "Model Complexity of Deep Learning: A Survey\nXia Hu1, Lingyang Chu2, Jian Pei1, Weiqing Liu3, and\nJiang Bian3\n1School of Computing Science, Simon Fraser University, Burnaby,\nCanada, huxiah@sfu.ca, jpei@cs.sfu.ca\n2Department of Computing and Software, McMaster University,\nHamilton, Canada, chul9@mcmaster.ca\n3Microsoft Research, Beijing, China, {Weiqing.liu,\nJiang.bian}@microsoft.com\nAugust 4, 2021\nAbstract\nModel complexity is a fundamental problem in deep learning. In this\npaper we conduct a systematic overview of the latest studies on model\ncomplexity in deep learning.\nModel complexity of deep learning can\nbe categorized into expressive capacity and eﬀective model complexity.\nWe review the existing studies on those two categories along four impor-\ntant factors, including model framework, model size, optimization process\nand data complexity. We also discuss the applications of deep learning\nmodel complexity including understanding model generalization, model\noptimization, and model selection and design. We conclude by proposing\nseveral interesting future directions.\n1\nIntroduction\nMainly due to its superior performance, deep learning is disruptive in many\napplications, such as computer vision [40], natural language processing [55] and\ncomputational ﬁnance [91]. At the same time, however, a series of fundamental\nquestions about deep learning models remain, such as why deep learning can\nachieve substantially better expressive power comparing to classical machine\nlearning models, how to understand and quantify the generalization capability\nof deep models, and how to understand and improve the optimization process.\n1\narXiv:2103.05127v2  [cs.LG]  3 Aug 2021\nModel complexity of deep learning is a core problem and is related to many\nthose fundamental questions.\nModel complexity of deep learning is concerned about, for a certain deep\nlearning architecture, how complicated problems the deep learning model can\nexpress [15, 44, 70, 89]. Understanding the complexity of a deep model is a key\nto precisely understanding the capability and limitation of the model. Exploring\nmodel complexity is necessary not only for understanding a deep model itself,\nbut also for investigating many other related fundamental questions. For exam-\nple, from the statistical learning theory point of view, the expressive power of a\nmodel is used to bound the generalization error [69]. Some recent studies pro-\npose norm-based model complexity [60] and sensitivity-based model complex-\nity [76, 81] to explore the generalization capability of deep models. Moreover,\ndetecting changes of model complexity in a training process can provide insights\ninto understanding and improving the performance of model optimization and\nregularization [44, 74, 89].\nThe investigation of machine learning model complexity dates back to\ndecades ago. A series of early studies in 1990s discuss the complexity of classical\nmachine learning models [16, 20, 21, 98]. One representative model is decision\ntree [19], whose complexity is always measured by tree depth [20] and number\nof leaf nodes [16]. Another frequent subject in model complexity analysis is\nlogistic regression, which is the foundation of a large number of parameterized\nmodels. The model complexity of logistic regression is investigated from the per-\nspectives of Vapnik-Chervonenicks theory [26, 96], Rademacher complexity [46],\nFisher Information matrix [21], and the razor of model [6]. Here, the razor of\nmodel is a theoretical index of the complexity of a parametric family of models\ncomparing to the true distribution. However, deep learning models are dramat-\nically diﬀerent from those classical machine learning models discussed decades\nago [70]. The complexity analysis of classical machine learning models cannot\nbe directly applied or straightforwardly extended to deep models.\nRecently, model complexity in deep learning has attracted more and more\nattention [13, 60, 70, 78, 81, 89]. However, to the best of our knowledge, there\nis no existing survey on model complexity in deep learning. The lack of survey\non this emerging and important subject motivates us to conduct this survey of\nthe latest studies. In this article, we use the terms “deep learning model” and\n“deep neural network” interchangeably.\nThere are proliﬁc studies on complexity of classical machine learning models\ndecades ago, which are summarized in excellent surveys [20, 21, 61, 93].\nIn\nthe following we very brieﬂy review the complexity of several typical models,\nincluding decision tree, logistic regression and bayesian network models. We also\ndiscuss the diﬀerences between the model complexity of deep neural networks\nand that of those models.\n2\nThere are relatively well accepted standard measurements for complexity\nof decision trees. The complexity of a decision tree can be represented by the\nnumber of leaf nodes [16, 61] or the depth of the tree [20, 98]. Since a decision\ntree is constructed by recursively splitting the input space and proposing a\nlocal simple model for each resulting region [71], the number of leaf nodes in\nprinciple uses the number of resulting regions to represent the complexity of the\ntree. The depth of a decision tree as the complexity measure quantiﬁes in the\nworst case the number of queries needed to make a classiﬁcation [20]. A series of\nstudies [16, 61] investigate tree optimization based on the accuracy-complexity\ntradeoﬀ.\nFurthermore, Buhrman and De Wolf [20] associate the complexity\nof decision trees with the complexity of functions represented by certiﬁcate\ncomplexity [4], sensitivity complexity [28] and approximating polynomials [80],\nand use these complexity measures of functions to bound the complexity of\ndecision trees.\nLogistic regression is the foundation of a large number of parameterized\nmodels [21, 46].\nIn the early 1990s, the degree of eﬀective freedom is pro-\nposed as a complexity measure for linear models and penalized linear models,\nwhich is represented by Vapnik-Chervoneniks (VC) dimension [26].\nBesides,\nRademacher complexity and Gaussian complexity [8, 12] are also used to mea-\nsure model complexity of logistic regression models [46].\nComparing to VC\ndimension, Rademacher complexity takes data distribution into consideration\nand therefore reﬂects ﬁner-grained model complexity. Later, Bulso et al. [21]\nand Balasubramanian [6] suggest that model complexity of logistic regression\nmodels is related to the number of distinguishable distributions that can be rep-\nresented by the models. Bulso et al. [21] deﬁne a complexity measure of logistic\nregression models based on the determinant of the Fisher Information matrix.\nSpiegelhater et al. [93] systematically investigate the model complexity of\nBayesian hierarchical models. A unique challenge in measuring the complexity\nof Bayesian hierarchical models is that the number of model parameters is not\nclearly deﬁned. Spiegelhater et al. [93] deﬁne a model complexity measure by the\nnumber of eﬀective parameters. Using the information theoretic argument, they\nshow that this complexity measure can be estimated by the diﬀerence between\nthe posterior mean of the deviance and the deviance at the posterior estimates\nof the parameters of interest, and is approximately the trace of the product of\nFisher’s information [33] and the posterior covariance matrix. In addition, they\nsuggest that adding such a complexity measure to the posterior mean deviance\ngives a deviance information criterion for comparing models.\nComplexity measures are often model speciﬁc. The deﬁnition of model com-\nplexity largely depends on model structures. Diﬀerent model frameworks often\ncall for diﬀerent deﬁnitions of complexity measures according to their structural\ncharacteristics. The complexity measures of diﬀerent model frameworks usually\n3\ncannot be directly compared.\nDeep learning models are structurally diﬀerent from traditional machine\nlearning models and have dramatically more parameters. Deep learning models\nare always substantially more complex than traditional models. Therefore, the\nprevious methods of model complexity of traditional machine learning models\ncannot be directly applied to deep learning models to obtain valid complexity\nmeasures.\nFor example, measuring the complexity of a decision tree by the\ndepth of the tree [20, 98] and the number of leaf nodes [16, 61] is obviously not\napplicable to deep learning models. Measuring model complexity by the number\nof trainable parameters [46] has a very limited eﬀect on deep learning models\nsince deep learning models are often over-parameterized.\nThe rest of this survey is organized as follows.\nIn Section 2, we brieﬂy\nintroduce deep learning model complexity. In Section 3, we review the existing\nstudies on the expressive capacity of deep learning models. In Section 4, we\nsurvey the existing studies on the eﬀective complexity of deep learning models.\nIn Section 5, we discuss the applications of deep learning model complexity. In\nSection 6, we conclude this survey and discuss some future directions.\n2\nDeep Learning Model Complexity\nIn this section, we ﬁrst divide deep model complexity into two categories, ex-\npressive capacity and eﬀective model complexity. Then, we discuss a series of\nimportant factors of deep learning model complexity, and group the represen-\ntative existing studies accordingly.\n2.1\nWhat is Deep Learning Model Complexity?\nThe term “model complexity” may refer to two diﬀerent meanings in deep learn-\ning. First, model complexity may refer to capacity of deep models in expressing\nor approximating complicated distribution functions [13]. Second, it may de-\nscribe how complicated the distribution functions are with some parameterized\ndeep models [89]. These two meanings are captured by the notions of model\nexpressive capacity and model eﬀective complexity, respectively.\nExpressive capacity, also known as representation capacity, expressive\npower, and complexity capacity [60, 86], captures the capacity of deep learning\nmodels in approximating complex problems. Informally, the expressive capacity\ndescribes the upper bound of the complexity of any model in a family of models.\nThis notion is consistent with the description of hypothesis space complex-\nity [69, 96]. The hypothesis space is a family of hypotheses, such as the family\nof all neural networks with a ﬁxed model structure. Considering the hypothesis\nspace represented by a ﬁxed model structure, the model expressive capacity is\n4\nalso the hypothesis space complexity. In statistical learning theory, the complex-\nity of an inﬁnite hypothesis space is represented by its expressive power, that is,\nthe richness of the family of hypothesises [69]. A notion to capture hypothesis\nspace complexity is Rademacher complexity [12], which measures the degree to\nwhich a hypothesis space can ﬁt random noise. Another notion is VC dimen-\nsion [26], which reﬂects the size of the largest set that can be shattered by the\nhypothesis space. Exploring expressive capacity helps to obtain the guarantee\nof learnability of deep models and derive generalization bounds [69].\nEﬀective model complexity, also known as practical complexity, practical\nexpressivity, and usable capacity [37, 81], reﬂects the complexity of the functions\nrepresented by deep models with speciﬁc parameterization [89]. The eﬀective\nmodel complexity is for a model with parameters ﬁxed. The study of eﬀective\nmodel complexity helps the exploration of various aspects of deep models, such\nas understanding the optimization algorithms [81], improving model selection\nstrategies [72] and improving model compression [25].\nExpressive capacity and eﬀective model complexity are closely related but\nare two diﬀerent concepts. Expressive capacity describes the expressive power\nof the hypothesis space of a deep model. Eﬀective model complexity explores\nthe complexity of a speciﬁc hypothesis within the hypothesis space.\nLet us\nuse an example to demonstrate the distinction and relationship between model\nexpressive capacity and eﬀective model complexity.\nExample 1 (Diﬀerence between expressive capacity and eﬀective complexity).\nConsider unary polynomial function f(x) = ax2+bx+c. The expressive capacity\nof f(x) is unary quadratic.\nIn other words, f(x) cannot express a function\nmore complicated than a unary quadratic polynomial. When assigning diﬀerent\nvalues to the parameters a, b and c, the corresponding eﬀective complexity\nmay be diﬀerent.\nWith parameters a = 0, b = 1 and c = 1, for example,\nf(x) = x + 1, the eﬀective complexity becomes linear, which is obviously lower\nthan the expressive capacity.\nDenote by H the hypothesis space of a ﬁxed deep learning model structure,\nand by h ∈H a hypothesis (i.e., a deep learning model) in H. The eﬀective\nmodel complexity is the complexity of a speciﬁc hypothesis, written as EMC(h).\nThe expressive capacity of the deep model, denoted by MEC(H), can be written\nin the form of\nsup{EMC(h) : h ∈H}\nThis reﬂects the relationship between model expressive capacity and eﬀective\nmodel complexity.\nBecause of the complex, over-parameterized architectures, deep learning\nmodels usually have high expressive capacity [70, 87].\nHowever, a series of\nstudies [5, 37, 81] ﬁnd that the eﬀective complexity of a trained deep model\n5\nmay be much lower than the expressive capacity.\nInformally and intuitively, a deep learning model can be regarded as a “con-\ntainer” of knowledge learned from data.\nThe same model architecture as a\n“container” may contain diﬀerent amounts of knowledge by learning from diﬀer-\nent data and thus equipped with diﬀerent parameters. The expressive capacity\ncan be regarded as the upper bound of the amount of knowledge that a model\narchitecture can hold. The eﬀective model complexity is concerned about, for\na speciﬁc model, a speciﬁc training dataset, how much knowledge it actually\nholds.\n2.2\nImportant Factors of Deep Learning Model Complex-\nity\nBonaccorso [17] points out that a deep learning model consists of a static struc-\nture part and a dynamic parametric part. The static structure part is always\ndetermined before the learning process by the model selection principle, then\nstays immutable once decided. The parametric part is the objective of the opti-\nmization and is determined by a learning process. Both the static and dynamic\nparts contribute to model complexity. We reﬁne this division and summarize\nfour aspects aﬀecting model complexity, including both expressive capacity and\neﬀective complexity.\nModel framework The choice of model framework aﬀects model complexity.\nThe factor of model framework includes model type (e.g., feedforward\nneural network, convolutional neural network), activation function (e.g.,\nSigmoid [29], ReLU [73]), and others. Diﬀerent model frameworks may\nrequire diﬀerent complexity measure criteria and may not be directly com-\nparable to each other.\nModel size The size of deep model aﬀects model complexity. Some commonly\nused measures of model size include the number of parameters, the number\nof hidden layers, the width of hidden layers, the number of ﬁlters, and the\nﬁlter size. Under the same model framework, the complexities of models\nwith diﬀerent sizes can be quantiﬁed by the same complexity measure\ncriteria and thus become comparable [54].\nOptimization process The optimization process aﬀects model complexity, in-\ncluding the form of objective functions, the selection of learning algo-\nrithms, and the setting of hyperparameters.\nData complexity The data on which a model is trained aﬀect model com-\nplexity, too.\nThe major factors include the data dimensionality, data\ndistribution [69], information volume measured by Kolmogorov complex-\nity [22, 59], and some others.\n6\nTable 1: Summarize the aspects aﬀecting the expressive capacity and eﬀective\ncomplexity, respectively.\nModel\nModel\nLearning\nData\nFramework\nSize\nProcess\nComplexity\nExpressive Capacity\nDepth Eﬃciency\n✓\nWidth Eﬃciency\n✓\nExpressible Functional Space\n✓\n✓\n✓\n✓\nVC Dimension and Rademacher Complexity\n✓\n✓\n✓\nEﬀective Complexity\nEﬀective Complexity Measure\n✓\n✓\n✓\n✓\nHigh-capacity Low-reality Phenomenon\n✓\n✓\n✓\n✓\nAmong these four aspects, model framework and model size mainly aﬀect\nthe static structural part of a deep model, and optimization process and data\ncomplexity mainly aﬀect the dynamic parametric part.\nThe eﬀect of these four aspects on expressive capacity can be understood\nthrough the inﬂuence on the hypothesis space. A selected model framework\ncorresponds to a hypothesis space H. Each hypothesis h ∈H represents a model\nwith the given framework. Once the model size is determined, the hypothesis\nspace shrinks to a subset of H. For example, suppose we set up two models with\ndepth l1 and l2 (l1 ≤l2), respectively, and both with width m, the corresponding\nhypothesis spaces are H1 and H2, respectively. We have H1 ⊂H2. This is\nbecause, for each h ∈H1, where h is a hypothesis and thus a deep network\nmodel in this context, there exists h′ ∈H2 whose ﬁrst l1 layers are identical to\nthose in h and the subsequent layers are identical mappings. It is easy to show\nthat the expressive capacity of H1 will not exceed the expressive capacity of H2.\nRecently, model framework, model size and their eﬀects on expressive capacity\nof deep models have been well explored [13, 51, 64, 89].\nThe choice of data distribution and optimization algorithm will further re-\nduce the scope of the hypothesis space, thereby aﬀecting the expression capacity.\nFor example, Rademacher complexity is a data-dependent expressive capacity\nmeasure taking into account the eﬀect of data distribution [12, 69]. However,\nto the best of our knowledge, the eﬀect of data complexity and optimization\nprocess on expressive capacity of deep learning models is still rarely explored.\nThese four aspects also aﬀect the eﬀective model complexity. In general,\nmodel framework and model size decide the available range of eﬀective model\ncomplexity.\nThe eﬀective complexity of a model with ﬁxed parameters is a\nvalue in this range selected by optimization process and training data. The\noptimization process aﬀects the eﬀective complexity, for example, adding L1\nregularization constrains the degrees of freedom in a deep neural network, and\nthus constrains the eﬀective model complexity [44, 46]. The training data af-\nfects the eﬀecive complexity, for example, using the same model and the same\noptimization process, the eﬀective complexity of a model trained on linearly\n7\nclassiﬁable data is much lower than that trained on the ImageNet [31, 59].\nSpeciﬁcally, the eﬀect of training data complexity and optimization process on\neﬀective complexity are reﬂected in the values of model parameters.\nIn Table 1 we list the corresponding aspects aﬀecting expressive capacity and\neﬀective complexity, respectively. In Table 2 we list some representative studies\non the two major problems, expressive capacity and eﬀective complexity.\n2.3\nExisting Studies on Deep Learning Model Complex-\nity: An Overview\nThe literature on deep model complexity can be categorized from two diﬀerent\nangles. The ﬁrst angle focuses on whether an approach is model-speciﬁc or cross-\nmodel. The second angle focuses on whether an approach develops an explicit\ncomplexity measure. These two angles are applicable to both expressive capacity\nand eﬀective complexity.\n2.3.1\nModel-Speciﬁc versus Cross-Model\nBased on whether an approach focuses on one type of models or crossing multiple\ntypes of models, the existing studies on model complexity can be divided into\ntwo groups, model-speciﬁc approaches and cross-model approaches.\nA model-speciﬁc approach focuses on a certain type of models, and ex-\nplores complexity based on structural characteristics.\nFor example, Bian-\nchini et al. [14, 15] and Hanin et al. [37] study model complexity of fully-\nconnected feedforward neural networks (FCNNs for short), Bengio and Delal-\nleau [13, 30] focus on model complexity of sum-product networks. Moreover,\nsome studies further propose constraints on the activation functions to con-\nstrain the nonlinearity properties. For example, Liang et al. [60] assume that\nactivation functions σ(·) satisfy σ(z) = σ′(z)z.\nAn approach is cross-model when it covers multiple types of models rather\nthan one speciﬁc type of models, and thus can be applied to compare two or\nmore diﬀerent types of models. For example, Khrulkov et al. [51] compare the\ncomplexity of general recurrent neural networks (RNNs for short), convolutional\nneural networks (CNNs for short) and shallow FCNNs by building connections\namong these network architectures and tensor decompositions.\n2.3.2\nMeasure-Based versus Reduction-Based\nAccording to whether an explicit measure is designed, the state-of-the-art model\ncomplexity approaches can be divided into two groups, measure-based ap-\nproaches and reduction-based approaches.\nA deep model complexity study is measure-based if it deﬁnes an appropri-\nate quantitative representation of model complexity. For example, the number\n8\nTable 2: Overview of a collection of studies on model complexity of deep neural networks.\nReferences\nModel-speciﬁc\nCross-model\nMeasure-based\nReduction-based\nObjective Model\nMeasure Metric\nExpressive Capacity\nDepth Eﬃciency\n[13, 30]\n•\n•\nSPN\n-\n[67]\n•\n•\nBinary tree hierarchical network, deep\nGaussian network\n-\n[54]\n•\n•\nFCNN with σ(z) = {Signum, Heaviside}\n-\n[70]\n•\n•\nFCNN with σ(z) = {ReLU, Maxout}\n#linear regions\n[92]\n•\n•\nFCNN with σ(z) = {ReLU, Maxout}\n#linear regions\n[14, 15]\n•\n•\nFCNN\nSum of Betti number\nWidth Eﬃciency\n[64]\n•\n•\nFCNN with σ(z) = {ReLU}\n-\nExpressible Functional\nSpace\n[3]\n•\n•\nFCNN with σ(z) = {ReLU}\n-\n[36]\n•\n•\nFCNN with σ(z) = {ReLU}\n-\n[52]\n•\n•\nFCNN with σ(z) = {σ(z) = zr}\nDimension of functional variety\n[51]\n•\n•\nFCNN, CNN, RNN\nRank of tensor decomposition\nVC Dimension and\nRademacher Complexity\n[66]\n•\n•\nFCNN with linear threshold gates\nVC dimension\n[11]\n•\n•\nFCNN with σ = {piecewise polynomial}\nVC dimension\n[10]\n•\n•\nNetworks with σ = {piecewise linear}\nVC dimension\n[9]\n•\n•\nNetworks with σ = {ReLU}\nRademacher complexity\n[77]\n•\n•\nTwo-layer FCNN with σ = {ReLU}\nRademacher complexity\nEﬀective Complexity\nEﬀective Complexity\nMeasure\n[89]\n•\n•\nNetworks with σ = {piecewise linear}\n#linear regions, trajectory length\n[81]\n•\n•\nNetworks with σ = {piecewise linear}\nJacobian norm, trajectory length\n[44]\n•\n•\nFCNN with σ(z) = {curve activation}\n#approximated linear regions\n[60]\n•\n•\nFCNN with σ(z) = σ′(z)z\nFisher-Rao norm\n[74]\n•\n•\nAny deep models\nmax #samples to zero training error\nHigh-Capacity\nLow-Reality\nPhenomenon\n[37]\n•\n•\nFCNN with σ(z) = {ReLU}\n#linear regions, volumn of region\nboundaries\n9\nof linear regions is used to represent the complexity of FCNNs with piecewise\nlinear activation functions [37, 70, 81, 89].\nA complexity approach is reduction-based if it investigates a model com-\nplexity problem by reducing deep networks to some known problems and func-\ntions, and does not deﬁne any explicit complexity measure.\nFor example,\nArora et al. [3] build connections between deep networks with ReLU activa-\ntion functions and Lebesgue spaces. Khrulkov et al. [51] connect deep neural\nnetworks with several types of tensor decompositions.\n3\nExpressive Capacity of Deep Learning Models\nExpressive capacity of deep models is also known as representation capacity,\nexpressive power, and complexity capacity [60, 86]. It describes how well a deep\nlearning model can approximate complex problems. Informally, the expressive\ncapacity describes the upper bound of the complexity in a parametric family of\nmodels.\nExpressive capacity of deep learning models has been mainly explored from\nfour aspects.\n• Depth eﬃciency analyzes how deep learning models gain performance\n(e.g., accuracy) from the depth of architectures.\n• Width eﬃciency analyzes how the widths of layers in deep learning\nmodels aﬀect model expressive capacity.\n• Expressible functional space investigates the functions that can be\nexpressed by a deep model with a speciﬁc framework and speciﬁed size\nusing diﬀerent parameters.\n• VC Dimension and Rademacher Complexity are two classic mea-\nsures of expressive capacity in machine learning.\nIn this section, we review these four groups of studies.\n3.1\nDepth Eﬃciency\nA series of recent studies demonstrate that deep architectures signiﬁcantly out-\nperform shallow ones [13, 67]. Depth eﬃciency [64], which is the eﬀectiveness of\ndepth of deep models, has attracted a lot of interest. Speciﬁcally, the studies on\ndepth eﬃciency analyze why deep architectures can obtain good performance\nand measures the eﬀects of model depth on expressive capacity. We divide the\nstudies of depth eﬃciency into two subcategories: model reduction methods and\nexpressive capacity measures.\n10\n3.1.1\nModel Reduction\nOne way to study the expressive capacity of deep learning models is to reduce\ndeep learning models to understandable problems and functions for analysis.\nTo investigate depth eﬃciency, one intuitive idea is to compare the repre-\nsentation eﬃciency between deep networks and shallow ones. Bengio and Delal-\nleau [13, 30] investigate the depth eﬃciency problem on deep sum-product net-\nworks (SPN for short). An SPN consists of neurons computing either products\nor weighted sums of their inputs. They consider two families of functions built\nfrom deep sum-product networks. The ﬁrst family of functions is F = ∪n≥4Fn,\nwhere Fn is the family of functions represented by deep SPNs with n = 2k in-\nputs and depth k. The second family of functions is G = ∪n≥2,i≥0Gi,n, where\nGi,n is the family of functions represented by SPNs with n inputs and depth\n2i + 1.\nThen, Bengio and Delalleau establish the lower bounds on the number of\nhidden neurons required by a shallow sum-product network to represent those\nfamilies of functions. To approach a function f ∈F, a one-hidden-layer shallow\nSPN needs at least 2\n√n−1 hidden neurons and at least 2\n√n−1 product neurons.\nSimilarly, to approach a function g ∈G, a one-hidden-layer shallow SPN needs\nat least (n −1)i hidden neurons. The comparison of deep and shallow sum-\nproduct networks representing the same function indicates that, to represent\nthe same functions, the number of neurons in a shallow network has to grow\nexponentially but only a linear growth is needed for deep networks.\nMhaskar et al. [67] study functions representable by hierarchical binary tree\nnetworks, comparing with shallow networks. Fig. 1 demonstrates shallow net-\nworks and hierarchical binary tree networks. Denote by Sm the class of shallow\nnetworks with m neurons, in the form of\nx →\nm\nX\nk=1\nakσ(wk · x + bk)\nwhere wk ∈Rd, bk, ak ∈R are the parameters for the k-th hidden neuron, σ\nis the activation function. Denote by W NN\nr,d\nthe class of functions with con-\ntinuous partial derivatives of order ≤r with certain assumptions (see [67] for\ndetail). Correspondingly, a hierarchical binary tree network is a network with\nthe structure\nf(x1, . . . , xd) = hl1(h(l−1),1(. . . (h11(x1, x2), h12(x3, x4), . . .),\nh(l−1),2(. . .))\n(1)\nwhere each hij is in Sm, l is the depth of the network, hl1 corresponds to the\nroot node of the tree structure. See Fig. 1(b) as an example of the hierarchical\nbinary tree with depth= 3. Denote by Dm the class of hierarchical binary tree\n11\n(a)\n(b)\nFigure 1: 8 input dimensions fed into a shallow network (a) and a hierarchical\nbinary tree network (b), studied by Mhaskar et al. [67].\nnetworks. Let W NN\nH,r,d be the class of functions with the structure of Eq. (1),\nwhere each hij is in W NN\nr,2 . Deﬁne by infP ∈V ||f −P|| the approximation degree\nof V to function f, where V = {Sm, Dm}.\nMhaskar et al. [67] demonstrate that, to approximate a function f ∈W NN\nr,d\nto approximation degree ϵ, a shallow network in Sm requires O(ϵ−d/r) trainable\nparameters.\nMeanwhile, to approximate a function f ∈W NN\nH,r,d to the same\napproximation degree, a network in Dm requires only O(ϵ−2/r) trainable pa-\nrameters. Then, Mhaskar et al. [67] compare shallow Gaussian networks with\nhierarchical binary tree structures (Eq. (1)) where each hij computes a shal-\nlow Gaussian network, and obtain a similar conclusion. They demonstrate that\nfunctions with a designed compositional structure can be approximated by both\ndeep and shallow networks to the same degree of approximation. However, the\nnumbers of parameters of deep networks are much less than those of shallow\nnetworks.\nArora et al. [3] investigate the importance of depth on deep neural networks\nwith ReLU activation function. First, they investigate neural networks with\none-dimensional input and one-dimensional output. They prove that given any\nnatural numbers k ≥1 and w ≥2, there exists a family of functions that can\nbe represented by a ReLU neural network with k hidden layers each of width\nw. However, to represent this family of functions, a network with k′ < k hidden\nlayers requires at least\n1\n2k′w\nk\nk′ −1 hidden neurons.\nThen, they investigate\nReLU neural networks with d input dimensions. They prove that, given natural\nnumbers k, m, d ≥1 and w ≥2, there exists a family of Rd →R functions that\ncan be represented by a ReLU network with k + 1 hidden layers and 2m + wk\nneurons. This family of functions is constructed using the zonotope theory from\nthe polyhedral theory [102]. However, to represent this family of functions, the\nminimum number of hidden neurons that a ReLU neural network with k′ ≤k\n12\nhidden layers requires is\nmax{1\n2(k′w\nk\nk′d )(m −1)(1−1\nd ) 1\nk′ −1, k′(w\nk\nk′\nd\n1\nk′ )}.\nTo investigate when deep neural networks are provably more eﬃcient than\nshallow ones, Kuurkova [54] analyzes the limitation of expressive capacity of\nshallow neural networks with Signum activation function. The Signum activa-\ntion function is deﬁned as\nsgn(z) =\n\u001a −1\nfor\nz < 0\n1\nfor\nz ≥0\nKuurkova proves that there exist functions that cannot be L1 sparsely repre-\nsented by one-hidden-layer Signum neural networks, which have a limited num-\nber of neurons and a limited sum of absolute values of output weights (i.e.,\nL1-norm). Such functions should be nearly orthogonal to any function f from\nthe class of Signum perceptrons {sgn(vx + b) : X →{−1, 1}|v ∈Rd, b ∈R}.\nThe functions generated by Hadamard matrices are such examples. A Hadamard\nmatrix of order n is a n×n matrix with entries in {−1, 1} such that any two dis-\ntinct rows or columns are orthogonal. Kuurkova [54] proves that the functions\ninduced by n × n Hadamard matrices cannot be computed by shallow Signum\nnetworks with both the number of hidden neurons and the sum of absolute\nvalues of output weights smaller than\n√n\n⌈log2 n⌉.\nTo further illustrate the limitation of shallow networks, Kuurkova [54] com-\npares the representation capability of one and two hidden layer networks with\nHeaviside activation function. The Heaviside activation function is deﬁned as\nσ(z) =\n\u001a 0\nfor\nz < 0\n1\nfor\nz ≥0\n(2)\nLet S(k) be a 2k × 2k Sylvester-Hadamard matrix, which is constructed by\nstarting from S(2) =\n\u0012 1\n1\n1\n−1\n\u0013\nand then recursively iterating S(l + 1) =\nS(2)⊗S(l). Kuurkova [54] shows that, to represent a function induced by S(k),\na two-hidden-layer Heaviside network requires k neurons in each hidden layer.\nHowever, to represent such a function, a one-hidden-layer Heaviside network\nrequires at least 2k\nk hidden neurons, or the absolute value of some output weights\nis no less than 2k\nk .\nIn summary, the model reduction approaches reduce neural networks to some\nkind of functions [3, 13, 54, 67], and investigate the eﬀects of model depth on\nthe capacity to express function families.\n13\nFigure 2: An example from Montufar et al. [70] illustrating the advantage of\ndeep models. A deep ReLU network (the dotted line) captures the boundary\nmore accurately by approximating it with a larger number of linear regions than\na shallow one (the solid line).\n3.1.2\nExpressive Capacity Measures\nTo investigate depth eﬃciency, another idea is to develop an appropriate mea-\nsure of expressive capacity and study how expressive capacity changes when the\ndepth and layer width of a model increase.\nMontufar et al. [70] focus on fully-connected feed forward neural networks\n(FCNNs) with piecewise linear activation functions (e.g., ReLU [73] and Max-\nout [35]), and propose to use the number of linear regions as a representation\nof model complexity (Fig. 2). The basic idea is that an FCNN with piecewise\nlinear activation functions divides the input space into a large number of linear\nregions. Each linear region corresponds to a linear function. The number of\nlinear regions can reﬂect the ﬂexibility and complexity of the model.\nMontufar et al. [70] investigate FCNNs with two types of piecewise linear\nactivation functions: ReLU [73] and Maxout [35].\nThey prove that, under\ncertain parameter settings, the model expressive capacity of a ReLU network\nN, which is represented by the maximum number of linear regions and denoted\nby MEC(N), is bounded by\nMEC(N) ≥(\nl−1\nY\ni=1\n⌊mi\nm0\n⌋m0)\nm0\nX\nj=0\n\u0000ml\nj\n\u0001\n(3)\nwhere l is the number of hidden layers, mi is the width of i-th hidden layer,\nm0 = d is the dimensionality of the input. Based on this bound, they show\nthat a ReLU network with l hidden layers and layer width mi ≥m0 is able to\napproximate any piecewise linear function that has Ω(( m\nm0 )(l−1)m0mm0) linear\nregions.\nMontufar et al. [70] also prove that, for the rank-k Maxout activa-\ntion function, the expressive capacity of a one-hidden-layer Maxout network\n14\nwith m neurons is bounded by MEC(N) ≥kmin(d,m) and MEC(N) ≤\nmin{Pd\nj=1\n\u0000k2m\nj\n\u0001\n, km}. A rank-k Maxout network, which consists of l hidden\nlayers and the width of each layer equals m, can compute any piecewise linear\nfunction with Ω(kl−1km) linear regions. In conclusion, the maximum number of\nlinear regions generated by a FCNN with piecewise linear activation functions\nincreases exponentially with model depth.\nMontufar et al. [70] provide an explanation for the depth eﬃciency. They\nsuggest that the intermediary layer of a deep model is able to map several pieces\nof the inputs into the same output. As the number of layers increases, the layer-\nwise compositions of the functions re-use lower-level computation exponentially.\nThis allows deep models to compute highly complex functions, even with rela-\ntively fewer parameters.\nSerra et al. [92] improve the bounds of the maximum number of linear regions\nproposed by Montufar et al. [70] (Eq. (3)). Given a deep ReLU neural network\nwith l layers, let mi be the width of i-th hidden layer with mi ≥3d, d is the\ninput dimension.\nSerra et al. [92] prove that the maximal number of linear\nregions of this neural network is lower bounded by\nMEC(N) ≥(\nl−1\nY\ni=1\n(⌊mi\nd ⌋+ 1)d)\nd\nX\nj=0\n\u0000ml\nj\n\u0001\nand is upper bounded by\nMEC(N) ≤\nX\n(j1,...,jl+1)∈J\nl+1\nY\ni=1\n\u0000mi\nji\n\u0001\nwhere J = {(j1, . . . , jl+1) ∈Zl+1 : 0 ≤ji ≤min{d, m1 −j1, . . . , mi−1 −\nji−1, mi}}.\nBianchini and Scarselli [14, 15] design a topological measure of model com-\nplexity for deep neural networks. Given an FCNN with single output, denoted\nby N : Rd →R, they deﬁne S = {x ∈Rd|N(x) ≥0} the set of instances being\nclassiﬁed by N to the positive class. The model complexity of neural network\nN is measured by B(S), the sum of the Betti numbers of set S, that is,\nMEC(N) = B(S) =\nd−1\nX\ni=0\nbi(S)\nwhere bi(S) denotes the i-th Betti number that counts the number of (i + 1)-\ndimensional holes in S. The Betti number is generally used to distinguish be-\ntween spaces with diﬀerent characteristics in algebraic topology [18]. S contains\ninstances positively classiﬁed by the network N. Therefore, B(S) can be used\n15\nTable 3: Upper and lower bounds of B(S) given by Bianchini and Scarselli [14,\n15], for networks with a number of M hidden units, a number of d inputs and\na number of l hidden layers.\n#Inputs\n#Hidden Layers\nActivation Function\nBound of B(S)\nUpper Bounds of B(S)\nd\n1\nthreshold\nO(M d)\nd\n1\narctan\nO((d + M)d+2)\nd\n1\npolynomial, degree r\n1\n2(2 + r)(1 + r)d−1\n1\n1\narctan\nM\nd\nmany\narctan\n2M(2M−1)O((dl + d)d+2M)\nd\nmany\ntanh\n2M(M−1)/2O((dl + d)d+M)\nd\nmany\npolynomial, degree r\n1\n2(2 + rl)(1 + rl)d−1\nLower Bounds of B(S)\nd\n1\nany sigmoid\n( M−1\nd\n)d\nd\nmany\nany sigmoid\n2l−1\nd\nmany\npolynomial, degree r ≥2\n2l−1\nto investigate how S is aﬀected by the architecture of N and can represent the\nmodel complexity.\nBianchini and Scarselli [14, 15] report upper and lower bounds of B(S)\nfor a series of network architectures (Table 3).\nIn particular, Bianchini and\nScarselli [14, 15] demonstrate that B(S) of a single-hidden-layer network grows\npolynomially with respect to the hidden layer width. That is, B(S) ∈O(md),\nwhere m is the width of the hidden layer.\nThey also prove that B(S) of a\ndeep neural network grows exponentially in the total number of hidden neu-\nrons. That is, B(S) ∈O(2M), where M is the total number of hidden neurons.\nThis indicates that deep neural networks have higher expressive capacity, and\ntherefore are able to learn more complex functions than shallow ones.\nBianchini and Scarselli [14, 15] suggest that the layer-wise composition mech-\nanism of a deep model allows the model to replicate the same behavior in dif-\nferent regions of the input space, thus makes depth more eﬀective than width.\n3.2\nWidth Eﬃciency\nIn addition to depth eﬃciency, the eﬀect of width on expressive capacity, namely\nwidth eﬃciency, is also worth exploring. Width eﬃciency analyzes how width\naﬀects the expressive capacity of deep learning models [64]. Width eﬃciency is\nimportant for fully understanding expressive capacity and helps to validate the\ninsights obtained from depth eﬃciency [64].\nLu et al. [64] investigate the width eﬃciency of neural networks with ReLU\nactivation function. They extend the universal approximation theorem [7, 43] to\n16\nwidth-bounded deep ReLU neural networks. The classical universal approxima-\ntion theorem [7, 43] states that, one-hidden-layer neural networks with certain\nactivation functions (e.g., ReLU) can approximate any continuous functions on\na compact domain to any desired accuracy performance. Lu et al. [64] show\nthat, for any Lebesgue-integrable function f : Rd →R and any ϵ > 0, there\nexists a ReLU network N : Rd →R that can approximate f to ϵ L1-distance.\nThat is,\nZ\nRd |f(x) −FN (x)|dx < ϵ.\nHere FN is the function represented by the neural network N. The width mi\nof each hidden layer of N satisﬁes mi ≤d + 4.\nMoreover, to explore the role of layer width in expressive capacity quantita-\ntively, Lu et al. [64] raise the dual question of depth eﬃciency. That is, whether\nthere exists wide, shallow ReLU networks that cannot be approximated by any\nnarrow, deep neural network whose size is not substantially increased. Denote\nby FA : Rd →R the function represented by a ReLU neural network A whose\ndepth h = 3 and whose width of each layer is 2k2, where k is an integer such\nthat k ≥d + 4. Let FB : Rd →R be the function represented by a ReLU\nneural network B whose depth h ≤k + 2 and whose width of each hidden layer\nmi ≤k3/2, where the parameter values of B are bounded by [−b, b]. For any\nconstant b > 0, there is ϵ > 0 such that FA can never be approximated by FB\nto ϵ L1-distance. That is,\nZ\nRd |FA(x) −FB(x)|dx ≥ϵ.\nThis indicates that there exists a family of shallow ReLU neural networks\nwhich cannot be approximated by narrow networks whose depth is constrained\nby polynomial bounds.\nThe polynomial lower bound for width eﬃciency is smaller than the expo-\nnential lower bound for depth eﬃciency [13, 15, 70]. That is, to approximate\na deep model whose depth increases linearly, a shallow model requires at least\nan exponential increase in width. To approximate a shallow, wide model whose\nwidth increases linearly, a deep, narrow model requires at least a polynomial in-\ncrease in depth. However, Lu et al. [64] point out that depth cannot be strictly\nproved to be more eﬀective than width since a polynomial upper bound for\nwidth is still lacking. The polynomial upper bound ensures that to approxi-\nmate a shallow, wide model, a deep, narrow one requires at most a polynomial\nincrease in depth.\n17\n3.3\nExpressible Functional Space\nIn addition to the studies of depth eﬃciency and width eﬃciency, the third line\nof works explore the classes of functions that can be expressed by deep learning\nmodels with speciﬁc frameworks and speciﬁed size. This line of works explore the\nexpressible functional space of deep learning models, including model-speciﬁc\nand cross-model approaches.\n3.3.1\nModel-speciﬁc Approaches\nArora et al. [3] investigate the family of functions representable by deep neural\nnetworks with ReLU activation function. They prove that every piecewise linear\nfunction f : Rd →R can be represented by a ReLU neural network that consists\nof at most ⌈log2(d + 1)⌉hidden layers. The family of piecewise linear functions\nis dense in the family of compactly supported continuous functions, and the\nfamily of compactly supported continuous functions is dense in Lebesgue space\nLp(Rd) [3, 23].\nThe Lebesgue space Lp(Rd) is deﬁned as the the family of Lebesgue in-\ntergrable functions f for which\nR\nRd |f| < +∞[23].\nDeﬁne Lp norm [23] as\n||f||p = [\nR\nRd |f|p]1/p. Then, the above conclusion can be extended to the Lp(Rd)-\nspace. That is, every function f ∈Lp(Rd) can be approximated to arbitrary Lp\nnorm by a ReLU neural network which consists of at most ⌈log2(d + 1)⌉hidden\nlayers.\nG¨uhring et al. [36] study the expressive capacity of deep neural networks\nwith ReLU activation function in Sobolev space [1]. Given Ω⊂Rd, p ∈[1, ∞],\nn ∈N, Lp(Ω) is the Lebesgue space on Ω, the Sobolev space [84] is deﬁned as\nW n,p(Ω) = {f ∈Lp(Ω) : Dαf ∈Lp(Ω) for ∀α ∈Nd\n0, |α| ≤n},\nwhere Dαf is the α-th order derivative of f. Sobolev norm is deﬁned as\n||f||W n,p(Ω) = (\nX\n0≤|α|≤n\n||Dαf||p\nLp(Ω))1/p.\nG¨uhring et al. [36] analyze the eﬀect of ReLU neural networks in approxi-\nmating functions from Sobolev space, and establish upper and lower bounds on\nthe sizes of models to approximate functions in the Sobolev space. Speciﬁcally,\ndeﬁne a subset of Sobolev space f as\nFn,d,p,B = {f ∈W n,p((0, 1)d) : ||f||W n,p((0,1)d) ≤B}.\nThe upper bound shows that, for any d ∈N, n ∈N, n ≥2, ε > 0, 0 ≤s ≤1,\n1 ≤p ≤+∞, and B > 0, for any function f ∈Fn,d,p,B, there exists a neural\n18\nnetwork Nε and a choice of weights wf such that\n||Nε(·|wf) −f(·)||W s,p((0,1)d) ≤ε\nwhere Nε represents a ReLU neural network consisting of at most c log2(ε−\nn\nn−s )\nlayers and cε−\nd\nn−s log2(ε−\nn\nn−s ) neurons, and with non-zero parameters.\nThe\nvalue of constant c depends on the value of d, p, n, s and B.\nBesides, the lower bound [36] shows that, for any d ∈N, n ∈N, n ≥2, ε > 0,\nB > 0, k ∈{0, 1}, with p = +∞, for any function f ∈Fn,d,p,B, there exists a\nReLU neural network Nε such that\n||Nε(·|wf) −f(·)||W k,∞((0,1)d) ≤ε\nwhere Nε has at least c′ε−\nd\n2(n−1) non-zero weights.\nKileel et al. [52] explore the functional space of deep neural networks with\npolynomial activation functions. A polynomial activation function ρr(z) raises z\nto the power of r. They suggest that, with polynomial activation functions, the\nstudy of model complexity can be beneﬁtted from the application of powerful\nmathematical machinery of algebraic geometry. In addition, polynomials can\napproximate any continuous activation function, thus help to explore other deep\nlearning models.\nGiven a deep polynomial neural network N : Rd →Rc with depth h and\npolynomial degree r, let m = {m0, m1, . . . , mh} represent the architecture of N\nwith the width mi of layer i and m0 = d, mh = c. Let Fm,r be the functional\nspace of N.\nKileel et al. [52] deﬁne the functional variety Vm,r as Vm,r =\nFm,r, which is the Zariski closure of the functional space Fm,r. “The Zariski\nclosure of a set X is the smallest set containing X that can be described by\npolynomial equations.” [52] The functional variaty Vm,r can be much larger than\nthe functional space Fm,r. Kileel et al. [52] propose to use the dimensionality of\nfunctional variety, denoted by dim Vm,r, as the representation of the expressive\ncapacity of deep polynomial neural networks, written as\nMEC(Nm,r) = dim Vm,r.\nTo measure dim Vm,r, Kileel et al. [52] build connections between deep poly-\nnomial networks and tensor decompositions. Speciﬁcally, polynomial networks\nwith h = 2 are connected to CP tensor decompositions [56], and deep polyno-\nmial networks are connected to an iterated tensor decomposition [65]. Based on\ndecompositions, they prove that, for any ﬁxed m, there exists ˜r = r(m) such\n19\n(a) TT-decomposition\n(b) CP-decomposition\n(c) HT-decomposition\nFigure 3:\nExamples of networks corresponding to various tensor decompo-\nsitions [51], from the left are TT decomposition, CP-decomposition, HT-\ndecomposition.\nthat for any r > ˜r, dim Vm,r is bounded by\ndim Vm,r ≤min\n \nmh +\nh\nX\ni=1\n(mi−1 −1)mi, mh\n\u0012m0 + rh−1 + 1\nrh−1\n\u0013!\n.\nBesides, Kileel et al. [52] prove a bottleneck property of deep polynomial\nnetworks. That is, a too narrow layer is a bottleneck and may “chock” the\npolynomial network such that the network can never ﬁll the ambient space. The\nambient space Symr(Rd) is the space of homogeneous polynomials of degree r\nin d variables. A polynomial network ﬁlling the ambient space satisﬁes Fm,r =\nSymrh−1(Rd)c. They show that, network architectures ﬁlling the ambient space\ncan be helpful for optimization and training.\n3.3.2\nCross-Model Approaches\nIn addition to model-speciﬁc approaches, expressible functional space can be\ninvestigated in a cross-model manner. Speciﬁcally, Khrulkov et al. [51] study\nthe expressive capacity of recurrent neural networks (RNNs). They investigate\nthe connections between network architectures and tensor decompositions, then\nmake comparison between the expressive capacity of RNNs, CNNs, and shallow\nFCNNs.\nLet X ∈Rn1×n2×...×nd be a d-dimensional tensor. The Tensor Train (TT)\ndecomposition [83] of a tensor X is computed by\nX i1i2...id =\nr1\nX\nα1=1\n. . .\nrd−1\nX\nαd−1=1\nGi1α1\n1\nGα1i2α2\n2\n. . . Gαd−1id\nd\nwhere the tensors Gk ∈Rrk−1×nk×rk are called TT-cores. Khrulkov et al. [51]\nintroduce bilinear units to represent TT-cores. Given x ∈Rm, y ∈Rn and G ∈\nRm×n×k, a bilinear unit performs a map G : Rm×Rn →Rk, written as G(x, y) =\n20\nTable 4: Comparison of the expressive power of various network architectures\ngiven by Khrulkov et al. [51]. Each column represents a speciﬁc network with\nwidth r, rows show the upper bounds on the width of other types of networks\nto achieve equivalent expressive capacity.\nTT-Network\nHT-Network\nCP-Network\nTT-Network\nr\nrlog2(d)/2\nr\nHT-Network\nr2\nr\nr\nCP-Network\n≥rd/2\n≥rd/2\nr\nz. Based on the bilinear units, they show that a recurrent neural network realizes\nthe TT-decomposition of the weight tensor (Fig. 3(a)). Similarly, Khrulkov et\nal. [51] show that the Canonical (CP) decomposition [24], with the form of\nX i1i2...id =\nr\nX\nα=1\nvi1\n1,αvi2\n2,α . . . vid\nd,α,\ncorresponds to a one-hidden-layer FCNN (Fig. 3(b)).\nEach unit of the net-\nwork is denoted by Gα = vi1\n1,αvi2\n2,α . . . vid\nd,α, where vi,α ∈Rni. The Hierarchical\nTucker (HT) decomposition [27] corresponds to an CNN structure (Fig. 3(c)).\nKhrulkov et al. [51] propose the rank of tensor decomposition as a measure\nof neural network complexity, since the rank of decompositions corresponds\nto the width of networks. Based on the correspondence relationships between\nneural networks and tensor decompositions, they compare the model complexity\nof RNNs, CNNs, and shallow FCNNs. The main conclusions are summarized\nin Table 4.\nIn particular, given a random d-dimensional tensor whose TT-\ndecomposition is with rank r and mode size n, this tensor has exponentially large\nranks of CP-decomposition and HT-decomposition. That tells, to approximate\na recurrent neural network, a shallow FCNN or CNN requires an exponentially\nlarger width.\n3.4\nVC Dimension and Rademacher Complexity\nThe VC dimension and Rademacher complexity are widely used to analyze the\nexpressive capacity and generalization of classical parametric machine learning\nmodels [8, 12, 26, 69, 95]. A series of works investigate the VC dimension and\nRademacher complexity of deep learning models.\nThe VC dimension is an expressive capacity measure that reﬂects the largest\nnumber of samples that can be shattered by the hypothesis space [69]. A higher\nVC dimension means the model can shatter a larger number of samples and thus\nthe model has a higher expressive capacity. Maass [66] studies the VC dimension\nof feedforward neural networks with linear threshold gates. The linear threshold\n21\ngate means that each neuron is composed of a weighted sum function followed by\na Heaviside activation function (Eq. (2)). Let W be the number of parameters\nin the network and L be the depth of the network. Maass [66] proves that for\nL ≥3, the VC dimension of such networks is Θ(W log W).\nBartlett et al. [11] investigate the VC dimension of feedforward neural net-\nworks with piecewise polynomial activation functions. A piecewise polynimial\nactivation function with p pieces has the form σ(z) = φi(z), where z ∈[ti−1, ti),\ni ∈1, . . . , p + 1, and ti−1 < ti. Each φi is a polynomial function of degree no\nmore than r. Let W be the number of parameters in the network and L be the\ndepth of the network. Bartlett et al. [11] prove that the upper bound for the\nVC dimension of such networks is O(WL2 + WL log WL) and the lower bound\nfor the VC dimension is Ω(WL). Later, Bartlett et al. [10] improve this lower\nbound to Ω(WL log(W/L)).\nBartlett et al. [10] study the VC dimension for deep neural networks with\npiecewise linear activation functions (e.g., ReLU). Given a deep neural network\nwith L layers and W parameters, they prove that the lower bound for VC\ndimension of such networks is Ω(WL log(W/L)) and the upper bound for VC\ndimension is O(WL log W).\nRademacher complexity captures the capacity of a hypothesis space to ﬁt\nrandom labels as a measure of the expressive capacity [69]. A higher Rademacher\ncomplexity means that the model can ﬁt a larger number of random labels and\nthus the model has a higher expressive capacity. Bartlett et al. [9] investigate the\nRademacher complexity of deep neural networks with ReLU activation function.\nGiven a deep ReLU neural network with L layers, let Ai be the parameter matrix\nof layer i, and X ∈Rn×d the data matrix, where n is the number of instances and\nd is the input dimension. Bartlett et al. [9] prove that the lower bound for the\nRademacher complexity of such networks is Ω(||X||F\nQ\ni ||Ai||σ), where || · ||σ is\nthe Spectral norm, and ||·||F is the Frobenius norm. Neyshabur et al. [77] prove a\ntighter lower bound for two-layer ReLU neural networks. Suppose ||A1||σ ≤s1,\n||A2||σ ≤s2, and s1s2 is the Lipschitz bound of the function represented by\nthe network. They prove that the Rademacher complexity is lower bounded\nby Ω(s1s2\n√m||X||F /n), where m is the width of the hidden layer. This lower\nbound improves the bound [9] by a factor of √m.\nYin et al. [99] study the Rademacher complexity of adversarial trained neural\nnetworks. Given a feedforward neural network with ReLU activation function,\ndenoted by f, let L be the depth of the network, and Ai the parameter matrix\nof layer i. The function family represented by f with adversarial loss function\ncan be written as\nF = {fA : min\nx′∈B(ϵ) yfA(x),\nL\nY\ni=1\n||Ai||σ ≤r}\n22\nwhere B(ϵ) = {x′ ∈Rd : ||x′ −x|| ≤ϵ} is the set of samples perturbed around\nx with l∞distance ≤ϵ. Yin et al. [99] prove that the lower bound for the\nRademacher complexity of F is Ω(||X||F /n+ϵ\np\nd/n). This lower bound exhibits\nan explicit dependence on the input dimension d.\nSome studies [75, 100] suggest that deep learning models are often over-\nparameterized in practice and have signiﬁcantly more parameters than samples.\nIn this case, the VC dimension and Rademacher complexity of deep learning\nmodels are always too high, so the practical guidance they can provide is weak.\n4\nEﬀective Complexity of Deep Learning Mod-\nels\nEﬀective complexity of deep learning models is also known as practical complex-\nity, practical expressivity, and usable capacity [37, 81]. It reﬂects the complexity\nof the functions represented by deep models with speciﬁc parameterizations [89].\nEﬀective complexity of deep learning models has been mainly explored from\ntwo diﬀerent aspects.\n• General measures of eﬀective complexity design quantitative mea-\nsures for eﬀective complexity of deep learning models.\n• Investigations into the high-capacity low-reality phenomenon ﬁnd\nthat eﬀective complexity of deep learning models may be far lower than\ntheir expressive capacity.\nIn this section, we review these two groups of studies.\n4.1\nGeneral Measures of Eﬀective Complexity\nCompared to expressive capacity, the study of eﬀective model complexity has\nstronger requirements for sensitive and precise complexity measures. This is\nbecause the eﬀective complexity cannot be directly derived from the model\nstructure alone [44]. Diﬀerent parameter values of the same model structure\nmay lead to diﬀerent eﬀective complexity. An eﬀective complexity measure is\nexpected to be sensitive to diﬀerent parameter values used in models with the\nsame structure.\nA series of works devote to proposing feasible measures for eﬀective com-\nplexity of deep learning models. A major group of the complexity measures\ndepends on the linear region splitting of piecewise linear neural networks in the\ninput space. We start with those methods and then discuss the others.\n23\nFigure 4: A 2-hidden-layer ReLU network divides the 2-dimensional input space\ninto a number of linear regions, studied by Hanin and Rolnick [37]. Upon each\nhidden neuron shows the linear regions divided by the current neuron.\n4.1.1\nPiecewise Linear Property\nIt is well known that a neural network with piecewise linear activation functions\ngenerates a ﬁnite number of linear regions in the input space [44, 70, 89]. This\nproperty is called the piecewise linear property and is demonstrated in Fig. 4.\nThe number of linear regions as well as the density of such regions can usually\nreﬂect the eﬀective complexity. Therefore, a series of studies on eﬀective com-\nplexity starts from piecewise linear activation functions (e.g., ReLU, Maxout)\nor are based on the piecewise linear property.\nRaghu et al. [89] propose two interrelated eﬀective complexity measures for\ndeep neural networks with piecewise linear activation functions, such as ReLU\nand hard Tanh [82]. Speciﬁcally, they deﬁne trajectory x(t) between two input\npoints x1, x2 ∈Rd, which is a curve parametrized by a scalar t ∈[0, 1], with\nx(0) = x1 and x(1) = x2. The ﬁrst eﬀective complexity measure they propose\nis the number of linear regions in the input space when sweeping an input point\nthrough a trajectory path x(t), that is, the eﬀective complexity EMC(N) of\nmodel N is\nEMC(N) = T (N(x(t); W))\nwhere T is the number of linear regions passing through the trajectory x(t)\nand W is a speciﬁc set of model parameters. The second eﬀective complexity\nmeasure they propose is the trajectory length l(x(t)) deﬁned as\nl(x(t)) =\nZ\nt\n||dx(t)\ndt ||dt\nwhich is the standard arc length of the trajectory x(t). They prove the propor-\n24\nFigure 5: A circular trajectory (the left most) is fed into a Tanh neural network,\nthen the following images show the trajectory after the transformation of each\nhidden layer in turn [89]. It shows the increase of the length of the trajectory\nafter the layer transformation.\ntional relationship between these two complexity measures. To obtain the esti-\nmated value of eﬀective complexity, Raghu et al. [89] bound the expected value of\ntrajectory length of any layer in a deep ReLU neural network. Speciﬁcally, given\na deep ReLU neural network whose weights are initialized by N(0, σ2\nw/m) and\nthe biases are initialized by N(0, σ2\nb). Let z(i)(x(t)) denote the new trajectory\nobtained after the transformation of the ﬁrst i hidden layers of the trajectory\nx(t). The expected value of its trajectory length can be bounded by\nE[l(z(i)(x(t)))] ≥O( σw\n√m\n√m + 1)il(x(t))\nwhere m denotes the hidden layer width. Similarly, for a hard Tanh neural\nnetwork under the same initialization, the trajectory length is bounded by\nE[l(z(i)(x(t)))] ≥O\n\n\nσw\n√m\nq\nσ2w + σ2\nb + m\np\nσ2w + σ2\nb\n\n\ni\nl(x(t)).\nUsing these two complexity measures, Raghu et al. [89] explore the perfor-\nmance of deep neural networks and report some ﬁndings. First, the eﬀective\ncomplexity grows exponentially with respect to the depth of the model and\npolynomially with respect to the width. Fig. 5 is an example showing the evolu-\ntion of a trajectory after each hidden layer of a deep network. Second, how the\nparameters are initialized aﬀects the eﬀective complexity. Third, injecting per-\nturbations to a layer leads to exponentially larger perturbations at the remaining\nlayers. Last, regularization approaches, such as Batch Normalization [45], help\nto reduce trajectory length. This explains why Batch Normalization helps model\nstability and generalization.\nNovak et al. [81] empirically investigate the relationship between model com-\nplexity and generalization of neural networks with piecewise linear activation\nfunctions. They propose to use model sensitivity to measure eﬀective complex-\nity. Model sensitivity, also known as robustness, reﬂects the capacity of a model\n25\nto distinguish between diﬀerent inputs at small distances.\nNovak et al. [81]\nintroduce two sensitivity metrics, input-output Jacobian norm and trajectory\nlength [89].\nFirst, based on the piecewise linear property, Novak et al. [81] propose the\nJacobian norm to measure the local sensitivity under the assumption that the\ninput is perturbed within the same linear region.\nThis Jacobian norm can\nfurther represent the eﬀective model complexity. That is,\nEMC(N) = Ex∼D[||J(x)||F ]\nwhere J(x) = ∂fN (x)/∂xT is the Jacobian of class probabilities, fN is the\nfunction represented by the network N, D is the data distribution, and || · ||F\nis the Frobenius norm.\nSecond, they propose a trajectory length metric similar to what developed\nby Raghu et al. [89] as a sensitivity measure when the input is perturbed to\nother linear regions, further as a measure of eﬀective complexity, written as\nEMC(N) = Ex∼D[t(x)]\nwhere t(x) is a trajectory length deﬁned by the number of linear regions passing\nthrough a trajectory τ(x), that is,\nt(x) =\nk−1\nX\ni=0\n||c(zi) −c(z(i+1)%k)||1\n≈\nI\nz∈τ(x)\n|| ∂c(z)\n∂(dz)||1dz\nwhere z0, . . . , zk−1 are k equidistant points on the trajectory τ(x), c(z) is the\nstatus encoding of all hidden neurons at point z.\nUsing these two complexity measures, Novak et al. [81] study the correlation\nbetween complexity and generalization. They demonstrate that neural networks\nhave strong robustness in the vicinity of the training data manifold where deep\nmodels have good generalization capability. They also show that the factors\nassociated with poor generalization (e.g., full-batch training, random labels)\ncorrespond to weaker robustness, and the factors associated with good general-\nization (e.g., data augmentation, ReLU) correspond to stronger robustness.\nTo develop a measure of eﬀective complexity for general smooth activation\nfunctions, Hu et al. [44] propose an eﬀective complexity measure for deep neu-\nral networks with curve activation functions (e.g., Sigmoid [53], Tanh [48]).\nMotivated by the piecewise linear property, they suggest that, using a piece-\nwise linear function with a minimal number of linear regions to approximate\na given network, the number of linear regions of the approximation function\n26\ncan be a measure of eﬀective complexity of the given network. They learn a\npiecewise linear approximation of a deep neural network with curve activation\nfunctions, which is called the Linear Approximation Neural Network (LANN for\nshort). The LANN is constructed by learning a piecewise linear approximation\nfunction for the curve activation function on each hidden neuron. Speciﬁcally,\nHu et al. [44] deﬁne the approximation error of a LANN g to the target network\nf as E(g; f) = E[|g(x) −f(x)|]. They analyze how the approximation error at a\ncertain layer is propagated through the remaining hidden layers, and obtain the\ninﬂuence of the approximation error of each hidden neuron on E(g; f). That is,\nE(g; f) =\nX\ni,j\n1\nc\nX\n( |Vo|\ni+1\nY\nq=L\nE[|Jq|] )∗,j(E[ei,j] + E[|ˆϵi,j|])\nwhere Jq is the Jacobian matrix of layer q of the network f, ei,j is the approxi-\nmation error of g on a speciﬁc neuron {i, j}, Vo is the weight matrix of the output\nlayer, and ˆϵi,j is the negligible estimation error on neuron {i, j}. Given an ap-\nproximation degree λ, the LANN with the smallest number of linear regions is\nconstructed to meet the requirement E(g; f) ≤λ. The approximated number of\nlinear regions of the LANN is used as the eﬀective complexity measure, that is,\nEMC(f)λ = d\nL\nX\ni=1\nlog(\nmi\nX\nj=1\nki,j −mi + 1)\nwhere ki,j is the number of linear pieces of the approximation function on neuron\n{i, j}, mi is the width of layer i, and L is the depth of f.\nUsing the complexity measure, Hu et al. [44] investigate the trend of model\ncomplexity in the training process. They show that the eﬀective complexity in-\ncreases with respect to the number of training iterations. They also demonstrate\nthat the occurrence of overﬁtting is positively correlated with the increase of\neﬀective complexity, while regularization methods (e.g., L1, L2 regularizations)\nsuppress the increase of model complexity (see Fig. 6).\nThe piecewise linear property may provide novel opportunities for capturing\nmodel complexity in deep learning. In addition to the above studies on eﬀective\ncomplexity, piecewise linear neural networks or the piecewise linear property can\nalso help the exploration of expressive capacity (see Section 2, [4, 36, 64, 70, 92]).\nPiecewise linear activation functions, especially ReLU, are popular and eﬀective\nactivation functions in many tasks and applications [58, 70]. The local linear\ncharacteristic and a ﬁnite number of regional divisions facilitate quantifying\nand analyzing neural network model complexity with piecewise linear activation\nfunctions.\n27\nNM (30.64)\nL1 (26.74)\nL2 (26.99)\nFigure 6: Inﬂuence of regularization approaches on eﬀective complexity [44].\nThis ﬁgure shows that the decision boundaries of models trained on the Moon\ndataset. NM, L1, L2 are short for normal train, train with L1, and L2 regular-\nization, respectively. In brackets are the value of eﬀective complexity measure.\n4.1.2\nOther Measure Metrics\nThere are other eﬀective complexity measures based on ideas other than the\npiecewise linear property.\nNakkiran et al. [74] introduce an eﬀective complexity measure by investigat-\ning the double descent phenomenon. The double descent phenomenon of deep\nneural networks is that, as the model size, training epochs, or size of training\ndata increases, the test performance often ﬁrst decreases and then increases.\nThey suggest that to help capture the double descent eﬀect in training, a com-\nplexity measure should be sensitive to training processes, data distribution, and\nmodel architectures. They propose such an eﬀective complexity measure, that\nis, the maximum number of samples on which the model can be trained to close\nto zero training error, written as\nEMCD,ϵ(T ) = max {n|ES∼D[ErrorS(T (S))] ≤ϵ}\nwhere D is the data distribution, ϵ is the threshold of training error, T is the\ntraining procedure, ErrorS is the mean error of the model on the training set\nS, and n the size of training set S. Based on the eﬀective complexity mea-\nsure EMCD,ϵ(T ), they investigate the evolution of the training process and\nshow that, if EMCD,ϵ(T ) is suﬃciently smaller or suﬃciently larger than n,\nthe size of the training set, then any perturbation of T that increases its eﬀec-\ntive complexity EMCD,ϵ(T ) helps to improve the test performance. However,\nif EMCD,ϵ(T ) ≈n, then any perturbation of T that increases its eﬀective\ncomplexity EMCD,ϵ(T ) hurts the test performance.\nTo\napproach\nthe\ngeneralization\nproblem\nof\ndeep\nlearning\nmodels,\nLiang et al. [60] introduce a new notion of model complexity measure, the\nFisher-Rao norm. Their study focuses on deep fully connected neural networks\nwhose activation function σ(·) satisﬁes σ(z) = σ′(z)z. The model complexity\n28\nrepresented by Fisher-Rao norm is given by\nEMC(N) = ||θ||2\nfr = ⟨θ, I(θ)θ⟩\nwhere θ is the set of parameters (i.e., weights, bias) of neural network N, ⟨·, ·⟩\nis the inner product, and I(θ) is the Fisher information matrix, written as\nI(θ) = E[∇θℓ(N(X), Y ) ⊗∇θℓ(N(X), Y )]\nwhere ℓ(·, ·) is the loss function and ⊗is the tensor product. Liang et al. [60]\nintroduce the geometric invariance property that a complexity measure should\nsatisfy in order to study generalization capability.\nThe invariance property\nessentially says that, since many diﬀerent continuous operations may lead to\nexactly the same prediction, thus the generalization should only depend on\nthe equivalence classes obtained by these continuous transformations. Speciﬁc\nparameterization should not aﬀect the generalization. The complexity measure\nused to investigate generalization should satisfy the invariance property. They\ndemonstrate that this Fisher-Rao norm satisﬁes the invariance property. Let\nθ1, θ2 denote two parameter settings of a model f. If fθ1 = fθ2, then their Fisher-\nRao norms are equal, that is, ||θ1||fr = ||θ2||fr. In particular, the Fisher-Rao\nnorm remains invariant during the node-wise rescaling of a model.\nEﬀective complexity can be measured by the size of training samples that a\nmodel achieves zero training error [74], or by the Fisher-Rao metric [60]. More\neﬀective complexity measures along the line may be developed.\n4.2\nHigh-Capacity Low-Reality Phenomenon\nSeveral studies explore the gap between the eﬀective complexity and the expres-\nsive capacity of deep learning models.\nBa and Caruana [5] show that shallow fully connected neural networks can\nlearn complex functions previously learned by deep neural networks, sometimes\neven only requiring the same number of parameters as the deep networks. Specif-\nically, given a well-trained deep model, they propose to train a shallow model\nbased on the outputs of the deep model, to mimic the deep model. They show\nthat, the shallow mimic model can achieve an accuracy as high as the deep\nmodel. However, the shallow model cannot be trained directly on the original\nlabeled training data to achieve the same accuracy. This is also well recognized\nas knowledge distillation [41].\nBased on this phenomenon, Ba and Caruana [5] conjecture that the strength\nof deep learning may arise in part from a good match between deep architectures\nand current training algorithms. That is, compared with shallow architectures,\ndeep architectures may be easier to train by the current optimization techniques.\n29\nMoreover, they propose that when it is able to mimic the function learned by\na deep complex model using a shallow model, the function learned by the deep\nmodel is not really too complicated to learn. This study suggests that there\nmay be a big gap between the practical eﬀective complexity of a deep learning\nmodel and the theoretical bound of its expressive capacity. We call this the\nhigh-capacity low-reality phenomenon.\nHanin and Rolnick [37] investigate the high-capacity low-reality phenomenon\nin fully connected neural networks with piecewise linear activation functions, es-\npecially ReLU. They propose two eﬀective complexity measures using the num-\nber of linear regions in the input space and the volume of boundaries between\nthese linear regions.\nFirst, they investigate ReLU neural networks whose dimensionality of input\nand that of output are both equal to 1, and use the number of linear regions\nas the eﬀective complexity measure. They show that the average number of\nlinear regions grows linearly with respect to the total number of neurons, far\nbelow the exponential upper bound. Speciﬁcally, given a ReLU neural network\nN : R →R whose weights and biases of neurons z are randomly initialized and\nare bounded by E[||∇z(x)||] ≤C for some C > 0, the average number of linear\nregions is proportional to the product of the number of neurons and the size of\ntraining set, that is,\nE[#{linear regions in S}] ≈|S| · T · M\nwhere S is the training dataset, T is the number of breakpoints in the activation\nfunction (for ReLU, T = 1), and M is the total number of hidden neurons.\nSecond, they investigate ReLU neural networks whose input dimensionality\nexceeds 1, denoted by N : Rd →R (d > 1), and use the volume of boundaries\nbetween linear regions in the input space as an estimation of model complexity.\nThat is,\nEMC(N) = volumed−1(BN ∩K)\nvolumed(K)\nwhere BN = {x|∇N(x) is not continuous at x} represents the boundaries of\nthe linear regions formed by N, K ∈Rd is the data distribution. They prove\nthat, under the same parameter initialization assumption as d = 1, the expected\nvalue of the volume of linear region boundaries is approximately equal to\nE[EMC(N)] ≈T · M\nThis demonstrates that the average size of the region boundaries depends only\non the number of neurons, not on the depth of the model. They conclude that\nthe eﬀective complexity of deep neural networks may be much lower than the\ntheoretical bound. That is, the function learned by deep neural networks may\n30\nnot be more complex than that learned by shallow ones.\n4.3\nDiscussion\nEﬀective model complexity is a relatively new, promising and useful problem\nin deep learning. Detecting eﬀective model complexity during training helps to\ninvestigate the usefulness of optimization algorithms [47], the role of regular-\nizations [44, 89], and generalization capability [60, 81]. Furthermore, eﬀective\nmodel complexity can be used to describe model compression ratio, since ef-\nfective model complexity can be considered as a reﬂection of the information\nvolume in the model [32].\nEﬀective complexity can also be used for model\nselection and design to balance resource utilization and model performance.\nIn addition to the eﬀective complexity measures and the high-capacity low-\nreality phenomenon, there are a series of interesting problems about eﬀective\ncomplexity of deep learning models. For example, the cross-model comparison\nof eﬀective complexity is worth exploring. That is, how to compare the eﬀective\ncomplexity of multiple models with diﬀerent architectures, and how the eﬀective\ncomplexity is aﬀected by the choice of diﬀerent model architectures. Moreover,\ncan one specify the granularity of eﬀective complexity measures? Diﬀerent cases\nmay have diﬀerent requirements for eﬀective complexity. Correspondingly, the\napplication scopes and granularities of eﬀective complexity measures should be\nspeciﬁed and clariﬁed. Typically, eﬀective complexity measure by the number\nof non-zero parameters is obviously not suﬃcient to study the optimization\nprocess.\n5\nApplication\nExamples\nof\nDeep\nLearning\nModel Complexity\nModel complexity of deep learning has many applications. In this section, we\nreview three interesting applications of deep learning model complexity, namely\nunderstanding model generalization capability, model optimization, and model\nselection and design.\n5.1\nModel Complexity in Understanding Model General-\nization Capability\nDeep learning models are always over-parameterized, that is, they have far more\nmodel parameters than the optimal solutions and the number of training sam-\nples. However, it is often found that large over-parameterized neural networks\nexhibit good generalization capability [49, 76, 100]. Some studies even ﬁnd that\n31\nlarger and more complex networks usually generalize better [81]. This observa-\ntion is in contradiction with the classical notion of function complexity [81] and\nthe well-known Occam’s razor [90], which prefer simple models. What leads to\nthe good generalization capability of over-parameterized deep learning models?\nIn statistical learning theory, expressive capacity (i.e., hypothesis space com-\nplexity) is used to bound generalization error [69]. Speciﬁcally, let F be the set\nof functions representable by a certain model structure. Let fA(D) be a function\nf ∈F learned by algorithm A on training dataset D. Let ED(fA(D)) be the\nemperical error of fA(D) and E(fA(D)) the generalization error of fA(D). The\ngap between generalization error and emperical error is bounded by\nE(fA(D)) −ED(fA(D)) ≤sup\nf∈F\n{E(f) −ED(f)}\n(4)\nThe right-hand side can be quantiﬁed by analyzing the expressive capacity (e.g.,\nRademacher complexity) [49]. For example, Zheng et al. [101] analyze general-\nization error of deep ReLU neural networks using the basis-path norm, a norm\nmeasure based on the basis paths. Zheng et al. [101] suggest that there exist a\nsmall group of basis paths in a ReLU neural network, which are linearly inde-\npendent. Each input-output path of a ReLU neural network can be expressed\nin the form of multiplication and division of the basis paths. Therefore, the\nbasis paths can be used to explain the generalization behavior of ReLU neural\nnetworks. Zheng et al. [101] prove that the generalization error of ReLU neural\nnetworks (i.e. the right-hand side of Eq. (4)) can be bounded by a function of\nbasis-path norm.\nA series of studies investigate model complexity measures that can explain\ngeneralization capability of deep learning models [2, 60, 76, 81]. Neyshabur et\nal. [76] suggests that, from the perspective of generalization, a complexity mea-\nsure should satisfy the following property: a learned model with lower com-\nplexity generalizes better. In particular, they list several requirements which\nare summarized from observed empirical phenomena and are expected to be\nsatisﬁed by complexity measures.\n• With zero training error, a network trained on real labels, which leads to\ngood generalization, is expected to have much lower complexity than a\nnetwork trained on random labels.\n• Increasing the number of hidden units or the number of parameters, which\nleads to a decreased generalization error, is expected to decrease the com-\nplexity measure.\n• When training the same architecture on the same training dataset using\ntwo diﬀerent optimization algorithms, if both lead to zero training errors,\nthe model with better generalization is expected to have lower complexity.\n32\nBased on these desiderata, Neyshabur et al. [76] investigate several complex-\nity measures including norms [79], robustness [97], and sharpness [50]. They\nshow that, these measures can meet some of the above requirements, but not\nall.\nNovak et al. [81] deﬁne two complexity measures from the perspective of\nmodel sensitivity, and identify an empirical correlation between the complexity\nmeasures and model generalization capability. They show that operations that\nlead to poor generalization, such as full batch training, correspond to high sen-\nsitivity, and in turn imply high eﬀective model complexity. Similarly, operations\nthat lead to good generalization, such as data augmentation, correspond to low\nsensitivity, and thus imply low eﬀective model complexity.\nLiang et al. [60] deﬁne a complexity measure using the Fisher-Rao norm\nto investigate model generalization capability. They suggest that a complex-\nity measure used to study generalization should satisfy the invariance property.\nThe invariance property requires that the generalization capacity depends on\nthe equivalence classes obtained by deep models. In other words, many dif-\nferent parameterizations may lead to the same prediction. Thus, the speciﬁc\nparameterization of deep models should not aﬀect the generalization and the\ncomplexity measure. They show that the Fisher-Rao norm honors this invari-\nance property and thus is able to explain the generalization capability of deep\nlearning models.\n5.2\nModel Complexity in Optimization\nModel optimization is concerned about how and why a neural network model\ncan be successfully trained [86, 94].\nSpeciﬁcally, the optimization of a deep\nmodel is to determine model parameters to minimize a loss function in general\nnon-convex. The loss function is typically designed based on the understanding\nof a problem and the requirements of the model, and thus generally includes\na performance measure, which is evaluated on the training set, and other con-\nstraint terms [34].\nModel complexity is widely used to provide a metric to make optimization\ntraceable. For example, a measure metric of the eﬀective model complexity of\nneural networks helps to monitor the changes of a model during the optimization\nprocess and understand how the optimization process progresses [44, 47, 74, 89].\nSuch a metric also helps to verify the eﬀectiveness of new improvements of op-\ntimization algorithms [39]. For example, Nakkiran et al. [74] investigate the\ndouble descent phenomenon during training using eﬀective complexity mea-\nsured by the maximal size of the dataset on which zero training error can be\nachieved. They show that the double descent phenomenon can be represented\nas a function of the eﬀective complexity. Raghu et al. [89] and Hu et al. [44]\npropose new regularization methods and demonstrate the eﬀectiveness of these\n33\nregularizations through their impact on complexity.\nThe study of model complexity inspires explorations on the eﬀectiveness of\noptimization approaches. Hanin and Rolnick [37] use the boundary volumes of\nlinear regions as the complexity measure of ReLU neural networks, and ﬁnd\nthat during the training, the average boundary volume is always linearly pro-\nportional to the number of neurons, irrelevant to the depth of the model. This\ndemonstrates that deep models do not always learn more complex functions\nthan shallow ones, the success of deep learning may be related to optimization\nalgorithms. Ba and Caruana [5] suggest that the great performance of deep\nlearning may be due to the fact that deep models are easier to train than shal-\nlow architectures using the current optimization algorithms [37, 81]. This calls\nfor further exploration of the eﬀectiveness of optimization approaches and the\nrelationship with model structures.\n5.3\nModel Complexity in Model Selection and Design\nGiven a speciﬁc learning task, how can we determine a feasible model structure\nfor the task? Given a variety of models with diﬀerent architectures and diﬀerent\ncomplexity, how can we pick the best model from them? This is the model\nselection and design problem [71].\nIn general, model selection and design is based on the tradeoﬀbetween\nprediction performance and model complexity [61, 72]. On one hand, making\npredictions with high accuracy is the essential goal of learning a model [69].\nA model is expected to be able to capture the underlying patterns hidden in\nthe training data and achieve predictions of accuracy as high as possible. In\norder to represent a large amount of knowledge and obtain high accuracy, a\nmodel with a high expressive capacity, a large degree of freedom and a large\ntraining set is required [13]. To this extent, a model with more parameters and\nhigher complexity is favored. On the other hand, an overly complex model may\nbe diﬃcult to train and may incur unnecessary resource consumption, such as\nstorage, computation and time cost [72]. Unnecessary resource consumption\nshould be avoided particularly in practical large scale applications [42]. To this\nextent, a simpler model with comparable accuracy is preferred than a more\ncomplicated one.\nTo maintain a good tradeoﬀbetween accuracy and complexity, a model se-\nlected is expected to be complex enough to ﬁt the given data and achieve high\naccuracy. At the same time, the model should not be highly over-complicated.\nUnderstanding model complexity and developing an eﬀective complexity mea-\nsure are the premise for good model selection strategies. For instance, Michel\nand Nouy [68] propose a model selection strategy for tree tensor networks (i.e.,\nsum-product neural networks). Their method combines the empirical risk min-\nimization and model complexity penalty to select a model from a family of\n34\nmodels.\nNeural architecture search (NAS for short) is a popular solution to deep\nlearning model selection [57, 62, 63, 103], which automatically selects a good\nneural network architecture for a given learning task. Since an overly complex\nmodel may take too-long training time and thus may become a serious obstacle\nof neural architecture search [57, 62], the accuracy-complexity tradeoﬀis an\nimportant consideration in neural architecture search. Liu et al. [62] propose\nProgressive Neural Architecture Search, which searches for convolutional neural\nnetwork architectures in the increasing order of model complexity. Therefore,\nProgressive Neural Architecture Search favors low complexity models that meet\nthe requirement on prediction accuracy. Laredo et al. [57] propose Automatic\nModel Selection, which searches for fully connected neural networks that yield\na good balance between prediction accuracy and model complexity.\nRadosavovic et al. [88] investigate the network design spaces of model selec-\ntion and design approaches. They propose to compare design spaces by con-\ntrasting the estimated distributions of model complexity in the network design\nspaces. Using their proposed method of comparing the distributions of model\ncomplexity of network design space, they investigate several popular NAS ap-\nproaches, such as ENAS [85] and DARTS [63], and ﬁnd that there are signiﬁcant\ndiﬀerences between the network design spaces of these approaches.\n6\nConclusions and Future Directions\nIn this paper, we survey model complexity in deep learning. We summarize four\naspects aﬀecting deep learning model complexity, and two angles to overview\nexisting studies on deep learning model complexity. We discuss the two major\nproblems of deep learning model complexity, namely the model expressive ca-\npacity and eﬀective model complexity. We overview the state-of-the-art studies\non the expressive capacity from four aspects: depth eﬃciency, width eﬃciency,\nexpressible functional space, and VC dimension and Rademacher complexity.\nWe overview the state-of-the-art studies on the eﬀective complexity from two\naspects: general measures of eﬀective complexity and the high-capacity low-\nreality phenomenon. We discuss the application of deep learning model com-\nplexity, especially in generalization capability, optimization, model selection and\ndesign.\nModel complexity of deep learning is still in its infant stage. There are many\ninteresting challenges for future works.\nExpressive capacity of deep learning models is a challenging problem. For\nexample, in most cases, deep learning models are over-parameterized and have\nsuﬃcient expressive power for given tasks and data. A natural question is what\nexpressive capacity is suﬃcient for a given task. In other words, can we obtain\n35\na lower bound of expressive capacity of deep learning models that are suﬃcient\nfor a given task? Does a narrow layer limit the expressive capacity of a model\neven if the model itself has a large number of parameters?\nSeveral studies explore the bottleneck of model size (i.e., depth, width) in\nthe expressive capacity. That is, when the model size may become a bottleneck\nthat restricts the expressive capacity. For example, Hanin and Sellke [38] and\nLu et al. [64] discover that any deep ReLU network with width constrained by\nthe input dimensionality has very limited expressive power. Serra et al. [92]\nﬁnd that smaller widths in the ﬁrst few layers of a deep ReLU network cause\na bottleneck on the expressive power. Kileel et al. [52] identify a bottleneck of\nlayer width of deep polynomial networks. In a deep polynomial network, if there\nis a very narrow layer, no matter how wide the other layers are, the network\ncan never correspond to a convex functional space. A convex functional space\nbeneﬁts the optimization and makes the model easier to train. Research on the\nbottleneck of expressive capacity may help to tackle many other problems, such\nas model design, model selection, model compression, and pruning.\nThough some progress has been made, eﬀective complexity measures are still\na largely under-developed direction in deep learning. Comparing to expressive\ncapacity of deep learning models, measuring eﬀective model complexity is even\nmore challenging. Eﬀective complexity measures have to be capable of captur-\ning ﬁne granularity diﬀerences between two models, such as the same model\narchitecture with two diﬀerent optimization algorithms. Several previous stud-\nies [44, 60, 74, 81, 89] deﬁne and explore eﬀective complexity measures mainly\nfrom the perspective of piecewise linear property [44, 81, 89], Fisher-Rao met-\nric [60], or the size of trainable samples [74]. However, the eﬀective complexity\nmeasure is still a largely unexplored and valuable direction.\nLast, cross-model complexity comparison is a promising direction. Given\nseveral models with diﬀerent model frameworks and diﬀerent model sizes, how\ncan we compare their expressive capacity? After these models are trained suf-\nﬁciently on the same dataset, such as obtaining zero training error, how can\nwe compare their eﬀective complexity and further understand their generaliza-\ntion capability? In these cases, the cross-model complexity comparison is use-\nful. Comparing model complexity crossing diﬀerent deep models can in general\nhelp many problems of deep learning, in particular model selection and design.\nHowever, the exploration of cross-model comparison of expressive capacity or\neﬀective complexity of deep learning models is still very limited. Khrulkov et\nal. [51] compare expressive capacity between shallow FCNNs, CNNs, and RNNs\nby connecting network architectures to tensor decompositions. However, many\nmore sophisticated models are not involved and need to be further explored.\n36\nReferences\n[1] R. A. Adams and J. J. Fournier. Sobolev spaces. Elsevier, 2003.\n[2] Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in over-\nparameterized neural networks, going beyond two layers. In Advances in\nNeural Information Processing Systems, pages 6155–6166, 2019.\n[3] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding deep\nneural networks with rectiﬁed linear units. In International Conference\non Learning Representations, 2018.\n[4] S. Arora and B. Barak. Computational complexity: a modern approach.\nCambridge University Press, 2009.\n[5] J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances\nin Neural Information Processing Systems, pages 2654–2662, 2014.\n[6] V. Balasubramanian. Statistical inference, occam’s razor, and statistical\nmechanics on the space of probability distributions. Neural computation,\n9(2):349–368, 1997.\n[7] A. R. Barron. Universal approximation bounds for superpositions of a\nsigmoidal function. IEEE Transactions on Information theory, 39(3):930–\n945, 1993.\n[8] P. L. Bartlett, S. Boucheron, and G. Lugosi. Model selection and error\nestimation. Machine Learning, 48(1-3):85–113, 2002.\n[9] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized\nmargin bounds for neural networks. In Advances in Neural Information\nProcessing Systems, pages 6240–6249, 2017.\n[10] P. L. Bartlett, N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-\ndimension and pseudodimension bounds for piecewise linear neural net-\nworks. Journal of Machine Learning Research, 20(63):1–17, 2019.\n[11] P. L. Bartlett, V. Maiorov, and R. Meir.\nAlmost linear vc-dimension\nbounds for piecewise polynomial networks.\nNeural Computation,\n10(8):2159–2173, 1998.\n[12] P. L. Bartlett and S. Mendelson.\nRademacher and gaussian complexi-\nties: Risk bounds and structural results. Journal of Machine Learning\nResearch, 3(Nov):463–482, 2002.\n[13] Y. Bengio and O. Delalleau. On the expressive power of deep architectures.\nIn International Conference on Algorithmic Learning Theory, pages 18–\n36. Springer, 2011.\n37\n[14] M. Bianchini and F. Scarselli. On the complexity of neural network classi-\nﬁers: A comparison between shallow and deep architectures. IEEE Trans-\nactions on Neural Networks and Learning Systems, 25(8):1553–1565, 2014.\n[15] M. Bianchini and F. Scarselli. On the complexity of shallow and deep\nneural network classiﬁers. In ESANN, 2014.\n[16] M. Bohanec and I. Bratko. Trading accuracy for simplicity in decision\ntrees. Machine Learning, 15(3):223–250, 1994.\n[17] G. Bonaccorso. Machine learning algorithms. Packt Publishing Ltd, 2017.\n[18] G. E. Bredon. Topology and geometry, volume 139. Springer Science &\nBusiness Media, 2013.\n[19] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classiﬁcation\nand regression trees. CRC press, 1984.\n[20] H. Buhrman and R. De Wolf.\nComplexity measures and decision tree\ncomplexity: a survey. Theoretical Computer Science, 288(1):21–43, 2002.\n[21] N. Bulso, M. Marsili, and Y. Roudi. On the complexity of logistic regres-\nsion models. Neural computation, 31(8):1592–1623, 2019.\n[22] J.-R. Cano. Analysis of data complexity measures for classiﬁcation. Expert\nSystems with Applications, 40(12):4820–4831, 2013.\n[23] N. L. Carothers. Real analysis. Cambridge University Press, 2000.\n[24] J. D. Carroll and J.-J. Chang. Analysis of individual diﬀerences in multi-\ndimensional scaling via an n-way generalization of “eckart-young” decom-\nposition. Psychometrika, 35(3):283–319, 1970.\n[25] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model compression and accel-\neration for deep neural networks: The principles, progress, and challenges.\nIEEE Signal Processing Magazine, 35(1):126–136, 2018.\n[26] V. Cherkassky, X. Shao, F. M. Mulier, and V. N. Vapnik. Model complex-\nity control for regression using vc generalization bounds. IEEE Transac-\ntions on Neural Networks, 10(5):1075–1089, 1999.\n[27] N. Cohen and A. Shashua. Convolutional rectiﬁer networks as generalized\ntensor decompositions. In International Conference on Machine Learning,\npages 955–963, 2016.\n[28] S. Cook, C. Dwork, and R. Reischuk.\nUpper and lower time bounds\nfor parallel random access machines without simultaneous writes. SIAM\nJournal on Computing, 15(1):87–97, 1986.\n38\n[29] G. Cybenko. Approximation by superpositions of a sigmoidal function.\nMathematics of Control, Signals and Systems, 2(4):303–314, 1989.\n[30] O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In\nAdvances in Neural Information Processing Systems, pages 666–674, 2011.\n[31] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In IEEE Conference on Computer\nVision and Pattern Recognition, pages 248–255. IEEE, 2009.\n[32] J. Du. The “weight” of models and complexity. Complexity, 21(3):21–35,\n2016.\n[33] B. Frieden. Science from ﬁsher information: A uniﬁcation cambridge univ.\nPress, Cambridge, UK [Google Scholar], 2004.\n[34] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press,\n2016.\n[35] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Ben-\ngio. Maxout networks. In International Conference on Machine Learning,\npages 1319–1327. PMLR, 2013.\n[36] I. G¨uhring, G. Kutyniok, and P. Petersen. Complexity bounds for approx-\nimations with deep relu neural networks in sobolev norms. 2019.\n[37] B. Hanin and D. Rolnick. Complexity of linear regions in deep networks. In\nInternational Conference on Machine Learning, pages 2596–2604. PMLR,\n2019.\n[38] B. Hanin and M. Sellke. Approximating continuous functions by relu nets\nof minimal width. arXiv preprint arXiv:1710.11278, 2017.\n[39] S. Hayou, A. Doucet, and J. Rousseau. On the selection of initialization\nand activation function for deep neural networks. STAT, 1050:7, 2018.\n[40] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 770–778, 2016.\n[41] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural\nnetwork. STAT, 1050:9, 2015.\n[42] M. H¨oge, T. W¨ohling, and W. Nowak. A primer for model selection: The\ndecisive role of model complexity. Water Resources Research, 54(3):1688–\n1715, 2018.\n39\n[43] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward net-\nworks are universal approximators. Neural Networks, 2(5):359–366, 1989.\n[44] X. Hu, W. Liu, J. Bian, and J. Pei.\nMeasuring model complexity of\nneural networks with curve activation functions.\nIn Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining, pages 1521–1531, 2020.\n[45] S. Ioﬀe and C. Szegedy. Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift. In Proceedings of the 32nd\nInternational Conference on Machine Learning, Proceedings of Machine\nLearning Research, pages 448–456, 2015.\n[46] S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear\nprediction: Risk bounds, margin bounds, and regularization. In Advances\nin Neural Information Processing Systems, pages 793–800, 2009.\n[47] D. Kalimeris, G. Kaplun, P. Nakkiran, B. Edelman, T. Yang, B. Barak,\nand H. Zhang.\nSgd on neural networks learns functions of increasing\ncomplexity. In Advances in Neural Information Processing Systems, pages\n3491–3501, 2019.\n[48] B. L. Kalman and S. C. Kwasny. Why tanh: choosing a sigmoidal function.\nIn [Proceedings 1992] IJCNN International Joint Conference on Neural\nNetworks, volume 4, pages 578–581. IEEE, 1992.\n[49] K. Kawaguchi, L. P. Kaelbling, and Y. Bengio. Generalization in deep\nlearning. arXiv preprint arXiv:1710.05468, 2017.\n[50] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang.\nOn large-batch training for deep learning: Generalization gap and sharp\nminima. International Conference on Learning Representations, 2017.\n[51] V. Khrulkov, A. Novikov, and I. Oseledets. Expressive power of recurrent\nneural networks.\nIn International Conference on Learning Representa-\ntions, 2018.\n[52] J. Kileel, M. Trager, and J. Bruna. On the expressive power of deep poly-\nnomial neural networks. In Advances in Neural Information Processing\nSystems, pages 10310–10319, 2019.\n[53] J. Kilian and H. T. Siegelmann. On the power of sigmoid neural networks.\nIn Proceedings of the Sixth Annual Conference on Computational Learning\nTheory, pages 137–143, 1993.\n40\n[54] V. Kuurkova. Constructive lower bounds on model complexity of shallow\nperceptron networks. Neural Computing and Applications, 29(7):305–315,\n2018.\n[55] G. Lample, M. Ott, A. Conneau, L. Denoyer, and M. Ranzato. Phrase-\nbased & neural unsupervised machine translation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing,\npages 5039–5049, 2018.\n[56] J. M. Landsberg. Tensors: geometry and applications. Representation\nTheory, 381(402):3, 2012.\n[57] D. Laredo, S. F. Ma, G. Leylaz, O. Sch¨utze, and J.-Q. Sun. Automatic\nmodel selection for fully connected neural networks. International Journal\nof Dynamics and Control, 8(4):1063–1079, 2020.\n[58] Y. LeCun,\nY. Bengio,\nand G. Hinton.\nDeep learning.\nnature,\n521(7553):436–444, 2015.\n[59] L. Li. Data complexity in machine learning and novel classiﬁcation algo-\nrithms. PhD thesis, California Institute of Technology, 2006.\n[60] T. Liang, T. Poggio, A. Rakhlin, and J. Stokes. Fisher-rao metric, geom-\netry, and complexity of neural networks. In The 22nd International Con-\nference on Artiﬁcial Intelligence and Statistics, pages 888–896. PMLR,\n2019.\n[61] T.-S. Lim, W.-Y. Loh, and Y.-S. Shih. A comparison of prediction accu-\nracy, complexity, and training time of thirty-three old and new classiﬁca-\ntion algorithms. Machine learning, 40(3):203–228, 2000.\n[62] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,\nA. Yuille, J. Huang, and K. Murphy.\nProgressive neural architecture\nsearch. In Proceedings of the European Conference on Computer Vision\n(ECCV), pages 19–34, 2018.\n[63] H. Liu, K. Simonyan, and Y. Yang. Darts: Diﬀerentiable architecture\nsearch. International Conference on Learning Representations, 2019.\n[64] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neu-\nral networks: A view from the width. In Advances in Neural Information\nProcessing Systems, pages 6231–6239, 2017.\n[65] S. Lundqvist, A. Oneto, B. Reznick, and B. Shapiro.\nOn generic and\nmaximal k-ranks of binary forms. Journal of Pure and Applied Algebra,\n223(5):2062–2079, 2019.\n41\n[66] W. Maass. Neural nets with superlinear vc-dimension. Neural Computa-\ntion, 6(5):877–884, 1994.\n[67] H. Mhaskar, Q. Liao, and T. Poggio. When and why are deep networks\nbetter than shallow ones? In Proceedings of the Thirty-First AAAI Con-\nference on Artiﬁcial Intelligence, AAAI’17, page 2343–2349. AAAI Press,\n2017.\n[68] B. Michel and A. Nouy. Learning with tree tensor networks: complexity\nestimates and model selection. arXiv preprint arXiv:2007.01165, 2020.\n[69] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine\nlearning. MIT press, 2018.\n[70] G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of\nlinear regions of deep neural networks. In Advances in Neural Information\nProcessing Systems, pages 2924–2932, 2014.\n[71] K. P. Murphy. Machine learning: a probabilistic perspective. MIT press,\n2012.\n[72] I. J. Myung. The importance of complexity in model selection. Journal\nof Mathematical Psychology, 44(1):190–204, 2000.\n[73] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltz-\nmann machines. In Proceedings of the 27th International Conference on\nMachine Learning, pages 807–814, 2010.\n[74] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever.\nDeep double descent: Where bigger models and more data hurt. In Inter-\nnational Conference on Learning Representations, 2020.\n[75] B. Neyshabur. Implicit regularization in deep learning. arXiv preprint\narXiv:1709.01953, 2017.\n[76] B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Explor-\ning generalization in deep learning. In Advances in Neural Information\nProcessing Systems, pages 5947–5956, 2017.\n[77] B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro. The role of\nover-parametrization in generalization of neural networks. In International\nConference on Learning Representations, 2018.\n[78] B. Neyshabur, R. Tomioka, and N. Srebro. In search of the real inductive\nbias: On the role of implicit regularization in deep learning. In Interna-\ntional Conference on Learning Representations (Workshop), 2015.\n42\n[79] B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control\nin neural networks. In Conference on Learning Theory, pages 1376–1401,\n2015.\n[80] N. Nisan and M. Szegedy.\nOn the degree of boolean functions as real\npolynomials. Computational Complexity, 4(4):301–313, 1994.\n[81] R. Novak, Y. Bahri, D. A. Abolaﬁa, J. Pennington, and J. Sohl-Dickstein.\nSensitivity and generalization in neural networks: an empirical study. In\nInternational Conference on Learning Representations, 2018.\n[82] C. Nwankpa, W. Ijomah, A. Gachagan, and S. Marshall. Activation func-\ntions: comparison of trends in practice and research for deep learning.\nInternational Conference on Computational Sciences and Technology (IN-\nCCST), pages 124 – 133, Jan 2021.\n[83] I. V. Oseledets. Tensor-train decomposition. SIAM Journal on Scientiﬁc\nComputing, 33(5):2295–2317, 2011.\n[84] I. P´erez Arribas. Sobolev spaces and partial diﬀerential equations. 2017.\n[85] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean. Eﬃcient neural ar-\nchitecture search via parameters sharing. In Proceedings of the 35th In-\nternational Conference on Machine Learning, volume 80 of Proceedings of\nMachine Learning Research, pages 4095–4104, 2018.\n[86] T. Poggio, K. Kawaguchi, Q. Liao, B. Miranda, L. Rosasco, X. Boix,\nJ. Hidary, and H. Mhaskar. Theory of deep learning iii: explaining the non-\noverﬁtting puzzle. Massachusetts Institute of Technology CBMM Memo\nNo. 73, 2017.\n[87] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Ex-\nponential expressivity in deep neural networks through transient chaos.\nIn Advances in Neural Information Processing Systems, pages 3360–3368,\n2016.\n[88] I. Radosavovic, J. Johnson, S. Xie, W.-Y. Lo, and P. Doll´ar. On network\ndesign spaces for visual recognition. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pages 1882–1890, 2019.\n[89] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. S. Dickstein. On\nthe expressive power of deep neural networks. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pages 2847–\n2854. JMLR, 2017.\n[90] C. E. Rasmussen and Z. Ghahramani. Occam’s razor. In Advances in\nNeural Information Processing Systems, pages 294–300, 2001.\n43\n[91] P. Rebentrost, B. Gupt, and T. R. Bromley.\nQuantum computational\nﬁnance: Monte carlo pricing of ﬁnancial derivatives. Physical Review A,\n98(2):022321, 2018.\n[92] T. Serra, C. Tjandraatmadja, and S. Ramalingam. Bounding and counting\nlinear regions of deep neural networks. In International Conference on\nMachine Learning, pages 4558–4566. PMLR, 2018.\n[93] D. J. Spiegelhalter, N. G. Best, B. P. Carlin, and A. Van Der Linde.\nBayesian measures of model complexity and ﬁt. Journal of the Royal Sta-\ntistical Society: Series b (Statistical Methodology), 64(4):583–639, 2002.\n[94] R. Sun. Optimization for deep learning: theory and algorithms. Journal\nof Operations Research Society of China, 2020.\n[95] Y. Tan and J. Wang. A support vector machine with a hybrid kernel and\nminimal vapnik-chervonenkis dimension. IEEE Transactions on Knowl-\nedge and Data Engineering, 16(4):385–395, 2004.\n[96] V. Vapnik. The nature of statistical learning theory. Springer Science &\nBusiness Media, 2013.\n[97] H. Xu and S. Mannor. Robustness and generalization. Machine Learning,\n86(3):391–423, 2012.\n[98] A. C.-C. Yao. Decision tree complexity and betti numbers. Journal of\nComputer and System Sciences, 55(1):36–43, 1997.\n[99] D. Yin, R. Kannan, and P. Bartlett. Rademacher complexity for adver-\nsarially robust generalization. In International Conference on Machine\nLearning, pages 7085–7094. PMLR, 2019.\n[100] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding\ndeep learning requires rethinking generalization. International Conference\non Learning Representations, 2017.\n[101] S. Zheng, Q. Meng, H. Zhang, W. Chen, N. Yu, and T.-Y. Liu. Capacity\ncontrol of relu neural networks by basis-path norm. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 33, pages 5925–5932,\n2019.\n[102] G. M. Ziegler. Lectures on polytopes, volume 152. Springer Science &\nBusiness Media, 2012.\n[103] B. Zoph and Q. V. Le.\nNeural architecture search with reinforcement\nlearning. International Conference on Learning Representations, 2016.\n44\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-03-08",
  "updated": "2021-08-03"
}