{
  "id": "http://arxiv.org/abs/2103.05612v1",
  "title": "Challenges for Reinforcement Learning in Healthcare",
  "authors": [
    "Elsa Riachi",
    "Muhammad Mamdani",
    "Michael Fralick",
    "Frank Rudzicz"
  ],
  "abstract": "Many healthcare decisions involve navigating through a multitude of treatment\noptions in a sequential and iterative manner to find an optimal treatment\npathway with the goal of an optimal patient outcome. Such optimization problems\nmay be amenable to reinforcement learning. A reinforcement learning agent could\nbe trained to provide treatment recommendations for physicians, acting as a\ndecision support tool. However, a number of difficulties arise when using RL\nbeyond benchmark environments, such as specifying the reward function, choosing\nan appropriate state representation and evaluating the learned policy.",
  "text": "Challenges for Reinforcement Learning in Healthcare\nElsa Riachi ∗1,4, Muhammad Mamdani †2,5, Michael Fralick ‡3, and Frank Rudzicz\n§1,4,5\n1University of Toronto, Department of Computer Science\n2Li Ka Shing Centre for Healthcare Analytics Research and Training\n3Sinai Health System, Department of Medicine, University of Toronto\n4Vector Institute for Artiﬁcial Intelligence\n5Unity Health Toronto\nMarch 10, 2021\nAbstract\nMany healthcare decisions involve navigating through a multitude of treatment options in a\nsequential and iterative manner to ﬁnd an optimal treatment pathway with the goal of an optimal\npatient outcome.\nSuch optimization problems may be amenable to reinforcement learning.\nA reinforcement learning agent could be trained to provide treatment recommendations for\nphysicians, acting as a decision support tool.\nHowever, a number of diﬃculties arise when\nusing RL beyond benchmark environments, such as specifying the reward function, choosing an\nappropriate state representation and evaluating the learned policy.\n1\nIntroduction\nAlong with ongoing changes to regulatory approval processes for software Elenko et al. [2015], ma-\nchine learning is being increasingly used within healthcare. Machine learning can be broadly divided\ninto supervised learning, unsupervised learning, and reinforcement learning. Supervised learning re-\nquires a dataset where the outcome of interest is known, and results in a model which categorizes\ndata points, e.g., as images of skin cancer or CT scans, by ﬁnding correlative or discriminative\nrelationships in fully labelled data Esteva et al. [2017]. For example, supervised learning has been\napplied to identify hospitalized patients who are at an increased risk of death using collected data\n∗elsa.riachi@mail.utoronto.ca\n†muhammad.mamdani@unityhealth.to\n‡mike.fralick@mail.utoronto.ca\n§frank@cs.toronto.edu\n1\narXiv:2103.05612v1  [cs.LG]  9 Mar 2021\nof vital signs and patient outcomes Escobar et al. [2020]. Clinical data can also be leveraged to\nuncover hidden patterns with unsupervised learning. Unlike supervised learning which is used to\nobtain a predictive model, unsupervised learning can be used to better understand patient data. For\nexample, unsupervised learning was recently leveraged to identify whether there might be distinct\nclinical phenotypes of patients hospitalized with sepsis Seymour et al. [2019]. While predictions\nmade by a supervised learning model can be used to decide the next course of action, supervised\nlearning does not ﬁnd the optimal sequence of actions directly, since the eﬀect of an action taken at\na given time step is not independent of subsequent actions. Reinforcement learning therefore lends\nitself well to problems of sequential decision making where the eﬀect of actions may extend over an\nunknown time duration into the future. Reinforcement learning, the focus of this article, attempts\nto make a sequence of decisions to achieve a given goal. Unlike supervised or unsupervised methods,\nit is used to identify the optimal sequence of actions based on their subsequent eﬀect on the state\n(e.g., a patient’s current condition). In this survey, we highlight the key challenges that arise when\nusing RL to learn improved treatment strategies from medical records, and point the reader towards\nsalient directions for future research.\n2\nReinforcement learning\nA reinforcement learning (RL) agent is trained to take optimal actions with respect to a chosen\nreward function and assumed state of the world. The reward signal acts as a reinforcer of desired\nbehaviour and represents the task we want the agent to perform. The agent learns a policy that\nmaximizes cumulative reward by interacting with its environment. Formally, our goal is to solve a\nMarkov decision process, represented by a tuple (S, A, r, γ, P), where S denotes the state space, A\nthe action space, r the reward function, γ the discount factor, and P the environment’s transition\nprobabilities. The environment dynamics are assumed to satisfy the Markov property, i.e.:\nP(st+1|ht) ≈P(st+1|st, at),\n(1)\nwhere ht = ((s0, a0), . . . , (st, at)) is the history of all state-action pairs up to and including time\nt.\nThe Markov assumption is not restrictive – if the environment dynamics do not satisfy the\nMarkov property for a given state representation, another state representation can be used to act as\na summary of the environment’s history from which the next state can be predicted. The current\nstate can then be written as a function of past state-action pairs st = f((s0, a0)...(st−1, at−1)) or\nst = f(ht−1). The choice of state representation is discussed in Section 5. The goal of the RL\nagent is to maximize the expected return, or the discounted sum of rewards PT\nt=0 γtrt. The agent’s\npolicy π(a | s) assigns a probability to actions a ∈A, conditioned on the current state s. The state\nvalue function of a policy π, denoted as V π(s), is the expected return obtained after being in state\n2\ns and subsequently acting according to policy π. The action value function of a policy π, denoted\nas Qπ(a, s), is the expected return obtained after being in state s, taking action a and subsequently\nacting according to policy π.\nThe Bellman Equations 2 and 3 deﬁne the state value and action-value functions recursively.\nV π(st) = r(st, at) + γEst+1∼P (.|s,a)V π(st+1)\n(2)\nQπ(at, st) = r(st, at) + γEat+1∼π(.|s),st+1∼P (.|s,a)Qπ(at+1, st+1)\n(3)\nIt is clear that an optimal policy π∗will select the action a for which the action-value function\nQπ∗(a, s) is highest. The Bellman optimality Equations 4 and 5 use this fact to recursively deﬁne\nthe value functions for an optimal policy π∗.\nV π∗(st) = max\na\nQπ∗(a, st)\n(4)\nQπ∗(at, st) = r(st, at) + γEst+1∼P (.|st,at) max\nat+1 Qπ(at+1, st+1)\n(5)\nIf the environment dynamics, that is, the transition probabilities P(st+1|st, at) are known, and\nthe state and action spaces are ﬁnite, an optimal policy can be learned using an iterative procedure\ncalled policy iteration, which alternates between a policy evaluation step and a policy improvement\nstep Sutton and Barto [2018].\nGenerally, the transition dynamics are unknown. By interacting with the environment, value-\nbased RL agents observe state transitions and rewards and use those observations to build an approx-\nimation of Qπ∗(s, a) from which the optimal policy is obtained. Oﬀ-policy algorithms can learn an\noptimal policy from experience generated by a behavioural policy that is diﬀerent from the current\nestimate of the optimal policy. Q-learning Watkins and Dayan [1992] is an oﬀ-policy, value-based\nalgorithm. In the tabular case, the action-value function Q is updated according to Equation 6.\nAlthough the transition tuples used for the updates are generated by a suboptimal behavioural pol-\nicy, Q-learning converges to Qπ∗if all the state-action pairs are visited inﬁnitely often Watkins and\nDayan [1992].\nQ(st, at) = Q(st, at) + α(r(st, at) + γ max\na\nQ(st+1, a) −Q(st, at))\n(6)\nIn the case of a large or continuous action space or state space, a function approximator can be\nused to represent the Q-function termed the critic. For instance, the Q-function can be learned using\nkernel-based regression methods Ernst et al. [2005] or a neural network Riedmiller [2005]. Q-learning\nwith a function approximator is termed Fitted Q-Iteration (FQI) to distinguish it from Q-learning\n3\nin which state-action values are stored in a tabular structure. The parameters θ of the critic can be\nupdated using Equation 7. In practice the expectation over state transitions is estimated using a\nsample average from a batch of recorded trajectories.\nθ = θ + αE [(r(st, at) + γV (st+1) −Q(st, at))∇θQ(st, at) ]\n(7)\nThe policy can also be parameterized and updated to increase its return according to an advantage\nterm Qπ(St, At) −V π(St), which is computed using the estimated critic.\nAlternating a policy\nimprovement step with a policy evaluation step using gradient updates constitutes an actor-critic\nalgorithm Konda and Tsitsiklis [2000], and is seen as a generalization of policy iteration.\nIn the observational setting, the agent is considered to interact with the environment by sampling\nexperiences from previously collected records. Many applications of RL in healthcare Raghu et al.\n[2017b, 2018], Nemati et al. [2016], Weng et al. [2017] fall under this category because it is infeasible\nor even unethical to evaluate a policy on patients. Often, we would like to discover better policies\nthan those reﬂected in the records. For example, Raghu et al. Raghu et al. [2017b] used data from\nthe ICU to learn better treatment strategies for managing sepsis. However, this requires evaluating\na diﬀerent policy in the evaluation step than the one used to generate the recorded experience. In\nthis case, we resort to oﬀ-policy reinforcement learning algorithms. If the historical data is used to\nbuild a model of the environment, as was done by Raghu et al.\nRaghu et al. [2017b], the model\ncould be used to simulate trajectories using the policy network. The critic, or Q-function, can then\nbe updated using trajectories generated by the current estimate of the optimal policy.\n3\nReinforcement learning from observational data\nReinforcement learning leverages past experiences which involve interacting with the environment.\nUnlike supervised learning, where labels specify the required or desired output, past experience used\nfor training reinforcement learning algorithms may be suboptimal. Trajectories consist of sequences\nof state-action pairs which may result in an undesired outcome. In the case of dynamic, person-\nalized treatment recommendation systems, the data may consist of patient trajectories recorded\nin electronic health records (EHRs), which include information recorded over a patient’s course of\ntreatment, such as vital signs, lab measurements, administered medication or interventions. Se-\nquential data in EHRs is used to construct state-action pairs which are subsequently used to train a\nreinforcement learning algorithm. The key diﬀerence between RL using observational data and RL\nusing simulated environments is the lack of exploration during training in the former. Much of the\nsuccess of reinforcement learning has been in simulated robotics Andrychowicz et al. [2017] or games\nwhere exploration is fast and unconstrained Mnih et al. [2015], Silver et al. [2017]. Exploration is\neasy in a simulated environment, but is not always feasible or safe in the healthcare setting. Oﬀ-\n4\npolicy reinforcement learning algorithms rely on having good estimates of action-value functions,\nwhich require extensive samples of the state and action spaces. An exploration strategy is usually\nused to gather experience, such as occasionally taking a random action instead of the supposed\noptimal one. Batch reinforcement learning refers to learning a policy only from previously collected\nexperiences. Fujimoto et al. Fujimoto et al. [2018] showed how batch RL algorithms are susceptible\nto extrapolation errors, where the Q-function or critic overestimates the expected return for unseen\nstate-action pairs, even when the training set consists of trajectories generated by an optimal policy.\nObservational data such as medical records consist of limited coverage or limited samples of the\npossible state-action pairs. The observational data is also collected under diﬀerent policies than\nthe one we wish to evaluate. Updates to the parameters of the Q-function occur under a diﬀerent\ndistribution than the on-policy distribution. As a result the learned policy may be optimal for a\nbiased estimate of the Q-function. This can also lead to misleading evaluations when the learned\npolicy is compared to that of the clinicians’. For example, Raghu et al. Raghu et al. [2017a] used\na reward function for treating sepsis that included the SOFA score, which is a clinical score that\nquantiﬁes the degree of organ failure Vincent et al. [1996]. Their reward function penalized high\nSOFA scores as well as increases in SOFA scores. The learned policy recommended that patients\nwith severe sepsis (and, hence, high SOFA scores) receive less aggressive treatment. The authors\nreasoned that this is an eﬀect of having fewer severe cases in the dataset. However, Gottesman et\nal. Gottesman et al. [2018] remarked that during training, the agent experienced trajectories where\nsevere sepsis is aggressively treated with IV ﬂuids and vasopressors, however its observed reward is\ndiminished because high SOFA scores, which are correlated with severe cases, are penalized. The\nagent cannot learn that lack of treatment might result in lower returns because such trajectories are\nnot present in the collected records.\nEvaluating a policy using experience collected under a diﬀerent policy is termed ‘oﬀ-policy eval-\nuation’. The policy that generated the collected experience is called the ‘behavioural policy’, and\nthe policy we wish to evaluate is called the ‘target’ or ‘evaluation policy’. The goal is to estimate the\nexpected total reward that an agent would obtain if it were acting under the evaluation policy. One\napproach is to use the importance sampling (IS) estimator shown in Equation 8, which is unbiased,\nbut can suﬀer from high variance if the target and evaluation policies do not have a large common\nsupport Precup et al. [2000].\nThe IS estimator computes the expected return of the evaluation\npolicy πe using trajectories τ generated by a behavioural policy πb, by reweighing the return of each\nobserved trajectory G(τ) with its importance weight ρ(τ) shown in Equation 9.\nV πe =\nX\nτ\nρ(τ)G(τ)\n(8)\n5\nρ(τ) = ΠT\nt=0ρ(at|st) = ΠT\nt=0\nπe(at|st)\nπb(at|st)\n(9)\nG(τ) =\nT\nX\nt=0\nγtrt\n(10)\nAn important weakness of the IS estimator is its high variance. Trajectories which have a high\nchance of occurring under the evaluation policy but are rarely observed under the behavioural policy\nhave a high importance weight. Similarly, trajectories which have a low chance of occurring under\nthe target policy contribute little to the value estimate. As a result, the eﬀective sample size used to\nconstruct the value estimate is much smaller than the total number of samples used for evaluation.\nThe IS estimator is thus highly sensitive to diﬀerences in training samples. Variants of the importance\nsampling estimator trade unbiasedness for lower variance. To decrease the eﬀect of pathologically\nlarge or small importance weights, the weighted IS estimator normalizes the importance weights such\nthat P\na ∈A ρ(a | s) = 1. The WIS estimator is not unbiased, but it distributes the weights more\nsmoothly among samples from the training set. Komorowski et al.\nKomorowski et al. [2018] used\na WIS estimator to estimate the values of learned policies for the management of sepsis. Following\nthe guidelines of Gottesman et al.\nGottesman et al. [2018], the authors learned a policy for each\nof the 500 diﬀerent clustering solutions of the state space, and chose the policy that maximized the\n95% lower conﬁdence bound of the policy’s estimated value.\nDirect methods (DMs) learn the value of the target policy, either by learning the state transition\nprobabilities or by modeling the value V πe or action-value Qπe function of the evaluation policy.\nDirect methods suﬀer from bias due to the diﬀerence in state visitation probabilities under diﬀerent\npolicies, but are more consistent than IS estimators Farajtabar et al. [2018]. Doubly robust (DR)\nmethods combine importance sampling and direct method estimates to obtain an estimate with\nlower variance Jiang and Li [2015], Farajtabar et al. [2018].\nV πe\nDR(st) = ˆV (st) + ρt(rt + γVDR(st+1) −ˆQ(st, at))\n(11)\nThe DR estimator, shown in Equation 11 can be understood as applying a form of bias correction\nto the DM estimate ˆV (st), where the bias is estimated using the term ρt(rt+γVDR(st+1)−ˆQ(st, at)).\nThe importance weight ρt is used in this context since the transitions used to estimate the bias are\ngenerated from the behavioural policy. Generally the behavioural policy must be estimated from\nthe recorded experiences, the error in its estimate introduces bias in the DR estimator. Since the\nDR estimator uses an IS estimator, its performance also depends on the eﬀective sample size of the\ndata after applying importance weights. One approach to obtaining value estimates with higher\nconﬁdence is to constrain the learned policy to have common support with the observed policies. A\nsimilar approach was used by Fujimoto et al. Fujimoto et al. [2018] to avoid extrapolation errors in\n6\nbatch reinforcement learning, where the agent was constrained to choosing from a set of previously\ntaken actions at a given state.\n4\nReward design in healthcare\nAs mentioned in Section 2, a reinforcement learning agent is trained to maximize the sum of dis-\ncounted rewards. The reward function, speciﬁed in terms of the current state or state-action pair,\nshould reﬂect how favourable the state transition is with respect to the underlying goal. For ex-\nample, a bipedal robot learning to walk should be rewarded for standing upright while displacing\nin the forward direction. Ideally, by learning to maximize the total reward for a sequence of state\ntransitions, the robot learns a walking gait. Alternatively, the reward could be deﬁned in terms of\nforward displacement, in which case the robot may learn to achieve a high reward using the more\nstable means of crawling. Amodei et al. Amodei et al. [2016] describe unintended phenomena stem-\nming from poor reward design. Negative side eﬀects are the result of reward functions that do not\nfully capture our objectives or the restrictions on allowed behaviour. A form of mis-speciﬁed reward\nis using a metric for success that is correlated with successful actions but does not necessarily imply\nthat a successful action was taken. For example, in Dario Amodei [2017], an agent was rewarded for\ncollecting points along a racetrack while the goal of the game was to win the race. Targets could be\nrespawned at one location along the racetrack, so the agent learned to move in a circle, hitting the\ntargets as they respawned, collecting more points while never ﬁnishing the race. Reward hacking\nrefers to unexpected policies learned from mis-speciﬁed reward functions. Speciﬁcally, reward hack-\ning occurs when an agent ﬁnds a way to exploit its reward channel to obtain a much higher reward\nsignal than the true reward signal. Since the agent’s actions allow it to control what it observes,\nit may choose to blind itself to observations that provide a low reward. Current work on RL in\nhealthcare has restricted the agent’s action space to speciﬁc aspects of treatment, such as managing\nsepsis through ﬂuids and vasopressors Komorowski et al. [2018], Raghu et al. [2017b,a, 2018], or\ndetermining policies for weaning a patient from mechanical ventilation Prasad et al. [2017]. A more\ndiverse action space adds challenges of its own. For example, an agent that is additionally tasked\nwith determining which tests to perform to track a patient’s state might choose to avoid receiving\nnegative test results, in order to avoid negative reward. For example, an agent may avoid suggesting\nregular blood tests to avoid receiving evidence of a worsening infection. This is in contrast with\nnegative side eﬀects generally, in which the designed reward function does not fully express what\nthe designer intended.\nWhen a health practitioner makes decisions regarding the treatment of a patient, the patient\nusually trusts that the practitioner is motivated to improve the patient’s health. Progress is usually\ntracked by clinical measures, such as vital signs or test results. When choosing a reward function,\n7\na designer must translate high-level objectives to a function of the current state and action. The\ndesigner understands objectives expressed in natural language or in demonstrations, but cannot\nalways determine whether an agent trained with a given reward signal will achieve the desired goal.\nWhile domain knowledge is an integral part of reward design, it does not rule out the possibility of\nmis-speciﬁed rewards which can result in undesired behaviour. The learned behaviour of an agent\nis a result of training experience, the environment’s representation and dynamics, as well as the\nlearning algorithm and reward function. While the reward function might be inspired by how we\ntrack progress on a given task, its combination with the underlying dynamics and missing experience\nmight lead to unexpected behaviour. Reward design is a well-known challenge in the reinforcement\nlearning community, which needs to be addressed before reinforcement learning can be applied in\nsafety-critical settings such as healthcare.\n4.1\nReward shaping in healthcare\nIn healthcare, some goals can be expressed as binary reward functions, such as preventing patient\nmortality from sepsis Komorowski et al. [2018], where the reward is given at the end of a patient’s\ntrajectory, i.e., +100 for survival and −100 for mortality. Binary rewards, however, preclude the\nexpression of secondary objectives such as minimizing length of stay and total treatment costs.\nAdditionally, binary rewards increase the sample complexity of a reinforcement learning problem,\nand aggravate diﬃculties such as the credit assignment problem. It is possible to transform binary\nor sparse rewards, such that the optimal policy under the original reward function is the same as the\noptimal policy under the modiﬁed reward function. Ng et al. Ng et al. [1999] derive a general form\nfor intermediate rewards for which the optimal policy is the same as that of the corresponding binary\nor sparse reward function. The general form, termed potential-based reward shaping, is described\nin Equations 12 and 13, where φ(s) can be any chosen function of the state. Equation 12 describes\nthe form of the potential function F(s, a, s′) required to ensure that the reward transformation is\npolicy-invariant. Without further knowledge of the transition dynamics, Equation 13 is the only\ntype of transformation that can guarantee policy invariance.\nF(s, a, s′) = γΦ(s′) −Φ(s)\n(12)\nR′(s, a, s′) = R(s, a, s′) + F(s, a, s′)\n(13)\nWhile Equations 12 and 13 result in a policy-invariant reward transformation for any function φ(s),\ndomain knowledge is generally needed to obtain the beneﬁts of lower sample complexity Ng et al.\n[1999]. For instance, φ(s) could be an estimate of the risk of mortality given the patient’s current\nstate. Raghu et al. Raghu et al. [2017b,a] use intermediate rewards for sepsis management, based\n8\non clinical measures of severity, while the intended measure of success is mortality. Peng et al.\nPeng et al. [2018] use a mortality predictor to obtain intermediate rewards. However, both reward\nformulations do not adhere to the potential-based reward transformation shown in Equation 12. The\neﬀects of such miss-speciﬁed intermediate rewards are discussed in Section 6.\n4.2\nMulti-Objective Rewards\nWhile most applications of reinforcement learning in healthcare focus on treatment decisions, Cheng\net al. Cheng et al. [2019] use reinforcement learning to obtain a policy for ordering phlebotomy\ntests for patients suﬀering from sepsis or acute renal failure. Since phlebotomy tests are often costly\nand can result in eﬀects such as low hemoglobin levels Loftsgard and Kashyap [2016], a policy for\nordering tests should balance the need to track the patient’s state while minimizing healthcare costs\nand patient discomfort. Preferences for such a trade-oﬀare not easily expressed with a single reward\nfunction. The authors use ﬁtted Q-iteration (FQI) with a multi-objective or vector-valued reward\nas shown in Equation 14, where rSOF A penalizes increases in SOFA scores, rtreat denotes a positive\nreward if a treatment was started after a lab test was ordered, rinfo rewards test results that are\ninformative and rcost denotes the monetary cost of the lab test.\nrt =\nh\nrSOF A\nt\n, rtreat\nt\n, rinfo\nt\n, −rcost\nt\ni⊤\n(14)\nTo estimate the information gain needed for rinfo\nt\nthe authors train a multi-output Gaussian process\nto predict hourly values of future measurements such as vital signs and test results. rinfo is computed\nusing the absolute diﬀerence between predicted measurements according to the Gaussian process\nregression model, and measurements received from the lab results.\nIn the case of multiple objectives or vector-valued rewards, the notion of optimal policies is re-\nplaced by the set of Pareto-optimal or un-dominated policies. A Pareto-optimal policy is a policy\nwhich cannot be changed without incurring a lower return in at least one objective. Pareto-optimal\npolicies do not necessarily incur a high return for all objectives, especially in the case of conﬂicting\nobjectives where policies must account for a preferred trade-oﬀ. One can deﬁne a scalarization func-\ntion to transform the vector of rewards to a scalar, according to preferences or relative importance\nof each objective.\nFor example, scalar rewards can be formed from convex combinations of the\nobjectives. The optimal policy for a given scalarization function is a Pareto-optimal policy, however\nnot all Pareto-optimal policies correspond to optimal policies for some convex combination of reward\ncomponents. Cheng et al. Cheng et al. [2019], use a variation of Pareto-optimal ﬁtted Q-iteration\n(PO-FQI) Lizotte and Laber [2016] to ﬁnd an optimal policy that conforms to a clinician’s prefer-\nence. The method introduced by Lizotte et al. Lizotte and Laber [2016] ﬁnds a set of un-dominated\nactions Π(s) =\nn\na : ∄a′\u0010\n∀d, ˆQd(s, a) < ˆQd(s, a′)\n\u0011o\nfor each state in the state space at each time\n9\nstep of the horizon. The map Π : S →A describes a non-deterministic policy (NDP) 1 since each\nstate is mapped to a set of actions instead of a single action or a single distribution over actions.\nA NDP thus represents a set of possible policies. The algorithm for PO-FQI proceeds similarly to\nFQI but learns a set of approximations of a vector-valued Q-function, where each approximation\ncorresponds to a policy that is consistent with the NDP Π.\nIn order to ensure that PO-FQI is tractable, in addition to consistency with Π, the condition\nof representability is imposed to further limit the set of policies to be considered. For instance if\nthe Q-function approximator is a linear function with respect to state-action features φ(s, a), then\nπt(st) = argmaxaφ(st, a)T w for some real vector w. This requires a previously obtained state-action\nrepresentation φ(s, a), which can be hand-crafted or learned. Moreover, in the case of a binary action\nspace, such as whether to order a lab test, both actions might be Pareto-optimal and representable,\nwhich means that the set of policies learned by PO-FQI is not getting pruned. Cheng et al. Cheng\net al. [2019] use a stricter pruning scheme for un-dominated actions shown in Equation 15. A policy\nis learned for each lab separately, where the action space is binary, with a = 1 if the lab test should\nbe ordered. Speciﬁcally, a lab test is ordered if the expected return, in addition to a preference\nvariable ϵd, is greater than the null action along all dimensions of the reward vector.\nΠ(s) =\n\n\n\n\n\n1, ˆQd(s, a = 0) < bQd(s, a = 1) + εd,\n∀d\n0, otherwise\n(15)\nThe hyperparameters ϵd describe the relative priority of each objective. For instance if cost is not a\nprimary concern then ϵcost should be set to a positive value. For more critical objectives, ϵd could\nbe set to a negative value. Cheng et al.Cheng et al. [2019] tune ϵd so that the total number of orders\nfor each lab test under the new policy is close to the number of tests ordered by clinicians in the\ntraining set. To comply with clinical protocols, the authors introduce a budget that suggests taking\na lab test if the test has not been ordered in the last 24 hours.\nWhile multi-objective rewards can better represent the clinicians’ intents and preferences, they\nare still amenable to misspeciﬁcation. For instance, Cheng et al include the SOFA score in the set\nof objectives, however the action space only consists of lab orders, which do not directly aﬀect the\nSOFA score. Since treatment decisions, which inﬂuence the SOFA score, are not incorporated into\nthe action space nor the state space, they are considered unobserved confounders. For instance,\npatients with a high SOFA score might be monitored more frequently, thus clinicians might order\nmore lab tests and the SOFA score might remain high. Such trajectories in the training set wrongly\npenalize frequent testing for severely ill patients if the SOFA score is considered an outcome or a\npenalty for lab test orders.\nThe authors evaluate the learned policy using per-step weighted importance sampling (PS-WIS)\n1The term non-deterministic does not mean a stochastic policy in this context\n10\nand compare its estimated value with that of the behavioural (clinician’s) policy and three random\npolicies which order tests with probabilities 0.01, pemp and 0.5, where pemp was the empirical prob-\nability of ordering a lab test. PS-WIS often estimates higher values for the random policies than\nthat of the behavioural policies. For instance, the random policies, including that which orders tests\nwith probability 0.01, are estimated to result in a greater expected information gain than the be-\nhavioural policy. Additionally, they are estimated to result in a change of treatment more frequently\nthan the behavioural policy. These results are conspicuous since the behavioural policy orders tests\nmore frequently than the random policy with probability 0.01, and should therefore obtain greater\ninformation gain. Like any WIS evaluation method, PS-WIS requires an estimate of the behavioural\nor clinician’s policy which the authors learned using the training data. It is worth noting that lab\norders are sparse, and for most observed states, no lab was ordered. This dataset imbalance could\nlead to a poor estimate of the clinician’s policy, predicting a low probability of ordering a test at\nevery state, which could then result in misleading value estimates given by the PS-WIS method.\nIt is therefore imperative that models of the behavioural policy are properly validated before using\nthem in OPE methods.\n4.3\nLearning objectives from demonstrations\nAn alternative to handcrafting reward functions is to learn them from demonstrations.\nInverse\nreinforcement learning (IRL) enables an agent to learn a reward function from sample trajectories,\nassuming that the sample trajectories demonstrate the behaviour of an optimal policy with respect\nto the hidden reward function. IRL is especially useful when it is diﬃcult to express the reward\nfunction in terms of the many relevant features.\nFor example, Yu et al.\nYu et al. [2019] use\nBayesian IRL to learn a reward function for mechanical ventilation weaning and optimal sedative\ndosing in the ICU. The action space consists of a binary action indicating whether a patient should\nremain on mechanical ventilation, along with four levels of sedative dosage, for a total of 8 possible\naction combinations. The reward function is expressed as the sum of three reward criteria r =\nrvitals + rventon + rventoff. The ﬁrst term introduces a penalty if vital signs are outside the normal\nrange or if vital signs undergo sudden changes. The second term represents the cost of each additional\nhour spent on the ventilator, and the third term rewards successful extubations while penalizing\nunsuccessful extubations. The reward function is parameterized by seven parameters [C1, . . . , C7]\nwhich control the trade-oﬀbetween individual criteria of the reward.\nLearning the reward function for which the observed behaviour is optimal is an ill-posed problem,\nsince the observed policy may be optimal for many reward functions. However, one can compute a\nprobability distribution over the space of possible reward parameters. Let P(O | R; P) denote the\nprobability that a sequence of state-action pairs O is generated by an agent acting optimally in an\nenvironment with transition probabilities T, with respect to a reward function parameterized by\n11\nR. Using Bayes’ rule, the probability distribution over reward parameters can be written in terms\nof the likelihood of observed sequences P(R | O; T) = P (O | R;R)P (R)\nP (O)\n. An optimal policy, according\nto the recovered reward function, must induce the same expected return as the expert’s policy.\nThis fact provides a ﬁrst moment constraint on the likelihood function. The maximum entropy\ndistribution with a ﬁrst moment constraint is the Boltzmann distribution shown in Equation 16.\nFor interpretability, the function f(O, R) can be expressed as a linear combination of features of the\npatient’s trajectory O, such as total length of stay, vital signs and risk scores. The parameters of\nthe reward function R can be learned by maximizing the likelihood P(O | R; T) Ziebart et al. [2008]\nor a MAP estimate P(O | R; T)P(R) Ramachandran and Amir [2007].\nP(O|R; T) = ef(O;R)\nZ(R)\n(16)\nIn Bayesian IRL Ramachandran and Amir [2007], the likelihood of an expert trajectory Pr(O|R)\nfor a given reward function R is given by the Boltzmann distribution eα P\ni Q∗(si,ai,R). The probability\nof the reward function parameters is obtained from a MAP estimate shown in Equation 18\nP(O|R) = 1\n2eα P\ni Q∗(si,ai,R)\n(17)\nR∗= argmaxR P(O|R) P(R)\n(18)\nwhere the observed trajectory O consists of state-action pairs (si, ai), the prior Pr(R) is a uniform\ndistribution over the interval [0, 1] for each constant in [C1, ...C7] and Q∗(., ., R) is the action-value\nfunction for the optimal policy under reward parameters R. A form of MCMC algorithm termed\nPolicyWalk Ramachandran and Amir [2007] is used to estimate the mean of the posterior Pr(R|O)\nwhich proceeds as follows: at each iteration, a new hypothesis reward ˜R is uniformly sampled from\nthe neighbors of the current hypothesis R. Yu et al. Yu et al. [2019] use FQI to compute the Q-\nfunction and the optimal policy with respect to reward parameters R which is then used to obtain\nthe posterior Pr( ˜R|O) in accordance with Equation 17. The newly sampled reward ˜R is accepted\nwith probability min\nn\n1, Pr( ˜R|O)\nPr(R|O)\no\n. Thus the Bayesian IRL algorithm returns the parameters of the\ninferred reward as well as its optimal policy.\nYu et al Yu et al. [2019] compare the performance of the policy learned using Bayesian IRL πBIRL\nwith several baseline policies. The ﬁrst baseline policy πBL was learned using FQI with [C1, ...C7]\nset to [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7]. The remaining baseline policies πBL1 . . . πBL3, [C1, . . . , C7]\nwere set to uniformly sampled non-negative values that sum to 1.\nBoth πBIRL and πBL were\nconsistent with clinicians’ ventilation actions 99.6% and 99.7% of the time, respectively. This is\nunsurprising since ventilation actions in the training set are binary and sparse, thus precision and\nrecall are required to properly evaluate the performance of the learned reward parameters and poli-\ncies. The remaining baseline policies πBL1, πBL2, πBL3 achieve much lower rates of consistency since\n12\nthe constants pertaining to the reward terms rventoff and rventon are randomly set to high or low\nvalues in the [0, 1] interval which may disproportionately penalize ventilation costs or unsuccessful\nextubations. Both πBIRL and πBL also achieve similar consistencies with clinicians’ sedative dosing\nactions, with respective matching rates of 54.2% and 53.5%, while the remaining baseline policies\nπBL1, πBL2, πBL3 achieve much lower consistency rates. Despite achieving similar consistency rates\nwith πBL, the reward parameters for which πBIRL is optimized are very diﬀerent from the reward\nparameters of πBL, suggesting that similar policies may be obtained from a large range of reward\nparameters. In this case, it may be due to correlations between the objectives of the reward func-\ntion. For instance, the stability of vital signs is disrupted by unsuccessful extubations, which in\nturn might result in more hours on the ventilator. We reiterate that each iteration of IRL requires\nlearning the Q-function of the optimal policy with respect to the current estimate of the reward\nparameters, this makes IRL susceptible to extrapolation errors which are commonly encountered in\noﬀ-policy RL from observational data. To alleviate this problem, the PolicyWalk algorithm may\nbe modiﬁed by imposing a restriction on the policies of accepted reward hypotheses ˜R. For each\npolicy returned by the FQI step, the corresponding reward hypothesis ˜R is rejected if at certain\nstates the policy samples actions which are never or rarely taken by clinicians, regardless of the\nestimated Q-function. Moreover, if the environment dynamics are unknown, IRL suﬀers from an\ninherent ambiguity. If clinicians are treating a diﬃcult case, the patient’s trajectory may reﬂect\nlow returns due to unfavourable outcomes and higher costs. Such trajectories when used as expert\ndemonstrations may unduly inﬂuence the estimate of the reward function.\nLearning a reward function by treating medical records as demonstrations of optimal behaviour\nundermines the goal of learning better policies than current practices. A naive approach is to select\nfrom the training set only those trajectories which result in a favourable outcome, thus eliminating\nsample demonstrations which we may not want the RL agent to emulate. However this severely limits\nthe set of states present in the training set, and may restrict the set of sample demonstrations to less\nsevere cases of illness. Since the RL agent’s policy must not deviate too much from the clinician’s\npolicy, the reward function parameters for which an improved policy is optimal are expected to be\nclose to the parameters for which the clinicians’ policy is optimal. Hence IRL may still provide a\nuseful starting point for reward design.\nOne could also characterize diﬀerent approaches by their respective reward functions. If observed\nrecords are generated by diﬀerent physicians or hospitals, the records may reﬂect diﬀerent approaches\nor decision-making preferences. For example, a clinician who prescribes treatments earlier might\nprefer more preventative strategies than clinicians who choose to observe patients before taking\naction, and clinicians who prescribe more diagnostic tests might be more risk averse at the expense\nof higher costs.\nA learned reward function might reﬂect such hidden preferences.\nEquation 16\nsuggests that one can consider the parameters of a reward function as a latent variable that explains\n13\nthe state-action pairs generated by an agent. Diﬀerent approaches could be represented by diﬀerent\nparts of the latent space.\nThe learned reward functions could be examined to obtain a reward\nfunction that better captures our notion of desired behaviour.\n5\nState Representation and Model-Based RL\nReinforcement learning algorithms assume a Markov model of the agent’s environment. This as-\nsumption is not restrictive if the choice of state representation is such that the next state can be\npredicted only from the current state and action. Essentially, the chosen state representation must\nbe a relevant summary of the patient’s history. Recurrent neural networks can learn such a rep-\nresentation by being trained to predict the outcome or the next observation given a sequence of\nobservations and actions. The learned state representation can be considered as a patient embed-\nding, where patients of a similar phenotype (or having similar predicted outcomes) obtain similar\nstate representations. This idea was employed by Zhang et al. to learn a vector representation of a\npatient’s EMR that was then used to predict the risk of hospitalization Zhang et al. [2018].\nThe predictive model that learns a state representation can also be used to learn an optimal\npolicy. Model-based reinforcement learning makes use of a predictive model of the environment to\nchoose actions over several future time steps, and adjusts the plan if the outcome does not meet the\npredictions. However, the model can suﬀer from a distributional shift in state transitions when the\nagent is generating experiences under a diﬀerent policy. The agent can exploit vulnerabilities in the\npredictive model to come up with optimistic plans. For example, Oberst and Sontag Oberst and\nSontag [2019] showed that transition probabilities estimated from observational data with hidden\nconfounders led to wrong predictions of discharge under the RL policy.\nTheir experiments are\ndiscussed in Section 8.\nModeling the distribution of the next state instead of predicting point estimates is preferred since\nthe uncertainty of the predictive model can be used to make the RL agent risk-averse as was shown\nby Depeweg et al.Depeweg et al. [2018] on the Siemens industrial benchmark Hein et al. [2016]. The\nauthors trained a Bayesian neural network to predict state transitions, and learned an approximate\nposterior q(W) of the weights of the network, given the training data. The authors then extended\nthe reward function with a term quantifying the uncertainty of the expected reward as a result of the\nuncertainty over the weights W, and showed that a policy optimizing the extended reward is more\npredictive of performance at test time. A similar approach might help RL algorithms avoid learning\ndegenerate policies due to missing experiences in the training data, such as a lower proportion of\nsevere cases or a limited range of observed actions.\n14\n6\nApplication to Sepsis Management\nSeveral papers propose to use reinforcement learning to optimize sepsis management in the ICU.\nTraining data is obtained from MIMIC-III Johnson et al. [2016]. Treatment for sepsis may involve\nantibiotics, intravenous ﬂuids, vasopressors, and mechanical ventilation Rhodes et al. [2017]. How-\never, current work restricts the action space to the set of combinations of IV and vasopressor dosages,\nsince a large action space requires a larger training set to learn and evaluate a new policy. Observed\nIV and vasopressor dosages are discretized into 5 bins each, for a total of 25 distinct actions.\nKomorowski et al. Komorowski et al. [2018] deﬁned the reward function in terms of 90-day\nmortality. A trajectory that ended with mortality was given a reward of −100, with a discount factor\nof 0.99. A high discount factor penalizes late deaths almost as much as early deaths. A trajectory\nthat ended with survival after 90 days of the onset of sepsis was given a reward of 100. Raghu et\nal. Raghu et al. [2017b] also use a positive reward (+15) for survival and a penalty (−15) for death\nat the end of a patient’s trajectory. Compared to other work Raghu et al. [2017a,b] where the reward\nwas based on the SOFA score, binary rewards are simpler and less prone to misspeciﬁcation, however\ntheir sparsity decreases the sample eﬃciency of reinforcement learning algorithms and increases the\nvariance of oﬀ-policy value estimates.\nRaghu et al. Raghu et al. [2017b] Raghu et al. [2017a], deﬁne intermediate rewards based on the\nSOFA score, changes to the SOFA score and lactate levels, as shown in Equation 19 with a reward\nof +15 or −15 at the end of a patient’s trajectory.\nr(st, st+1) = C0⊮\n\u0000sSOFA\nt+1\n= sSOFA\nt\n&sSOFA\nt+1\n> 0\n\u0001\n+ C1\n\u0000sSOFA\nt+1\n−sSOFA\nt\n\u0001\n+ C2 tanh\n\u0000sLactate\nt+1\n−sLactate\nt\n\u0001\n(19)\nWhile the reward function was clinically motivated, Raghu et al.Raghu et al. [2017a] observed\nthat the learned policy recommended lower dosages of vasopressors and IV ﬂuids for severely ill\npatients. As mentioned by Gottesman et al. Gottesman et al. [2018] this might be due to ﬁtting a\nQ-function with missing state-action pairs. In a later work, Raghu et al. Raghu et al. [2018] used\na similar reward function but ensured that the learned policy did not diﬀer too much from the\nclinician’s policy by setting a small learning rate for the policy gradient.\nPeng et al. Peng et al. [2018] design an intermediate reward based on the likelihood of patient\nmortality at the end of the trajectory.\nThe authors train a regression model that predicts the\nprobability of mortality given a patient’s current observations. The reward function was deﬁned\nas shown in Equation 20 where f(o) is the probability of mortality, as predicted by the regression\nmodel, given current observations o. The reward function penalizes increases in the log-likelihood\nof mortality given the observations of the next time-step o′.\nOne problem with such a reward\nconstruction is the dependence of the probability of mortality on the behavioural or clinician’s\n15\npolicy. Therefore, the likelihood of mortality under the target policy may be diﬀerent. This might\ninﬂuence the learned policy since the reward function is not consistent with policy-invariant reward\ntransformations as discussed in Section 4.\nr(o, a, o′) = −log\nf(o′)\n1 −f(o′)f(o′) + log\nf(o)\n1 −f(o)\n(20)\nKomorowski et al. Komorowski et al. [2018] extracted vital signs, laboratory values, clinical scores\nand demographics of each patient. To reduce the size of the state space, the authors clustered the\npatient’s measurements using k-means++ to obtain 750 discrete states. A larger number of clusters\nprovides higher granularity but many states would be sparsely observed while training. Komorowski\net al. Komorowski et al. [2018] verify the validity of the learned state representation by examining\nthe distribution of ICD codes in the state clusters to determine whether past medical history and\ndiagnoses were included in the state representation. In contrast, Raghu et al. Raghu et al. [2017b]\nlearned a lower-dimensional continuous state representation using a sparse autoencoder, and learned\na policy using deep Q-learning. When using function approximators, it is important to ensure that\npatients with similar trajectories or demographics are assigned similar states, as the Q-function is\nexpected to interpolate between the observed trajectories. Additionally, the Markov property must\nbe veriﬁed for the learned state representation since it must be suﬃcient for predicting the next\nstate. Komorowski et al. Komorowski et al. [2018] used 80% of the development set to estimate the\ntransition probabilites using normalized transition frequencies. To verify the Markov property, the\nauthors simulated 500 random walks using the learned transition matrix, and counted the number of\ntimes an agent would remain in a given state for a given number of time-steps. If the state transition\nprocess is memoryless, the life-expectancy for each state must be exponentially distributed. The\nauthors verify the correlation coeﬃcient between the data and an exponential decay function ﬁtted\non the simulated data. Policy iteration was used to learn a new policy using the estimated transition\nprobabilites.\nThe learned policy recommended a higher dosage of vasopressors than clinicians.\nPatients from the test set would have received vasopressors 30% of the time under the learned\npolicy, but received vasopressors 17% of the time under the clinicians’ policy. Speciﬁcally, higher\nvasopressor dosages were recommended for patients who also received higher IV ﬂuid dosages. The\nauthors point to clinical literature which suggests that early administration of vasopressors might\nimprove outcomes Byrne and Van Haren [2017], Marik [2015]. Raghu et al. Raghu et al. [2017b]\nalso found that the learned policy recommended vasopressors more often than clinicians, but mostly\nin cases where the administered IV ﬂuid dosage was low. Careful examination of the trajectories\nthat recommended a change of dosage is required in order to understand whether the recommended\nchanges are due to extrapolation errors. Gottesman et al. describe a possible side eﬀect of limiting\nthe action space. When an agent was restricted to only consider when to intubate a septic patient,\nthe learned policy recommended intubation more frequently than physicians. This might be a case of\n16\nextrapolation errors common with oﬀ-policy batch RL, since the agent cannot recommend changes to\nother treatment strategies, and may not have observed the negative eﬀects of an aggressive intubation\npolicy.\nPeng et al Peng et al. [2018] use an LSTM autoencoder to obtain a 128-dimensional state rep-\nresentation based on patient history. The autoencoder is trained to minimize the reconstruction\nerror between the original measurements and the decoded measurements. However, the authors do\nnot examine the state representation for correlations with measurement time, clinical severity scores\nor demographics. The autoencoder was used to represent the state of all patients in the training\nset, at 4-hour intervals. For each state vector st, 300 Euclidean nearest neighbors were found. The\nkernel policy πk is the distribution of actions taken by clinicians among the survivors of the nearest\nneighbors. The authors found that the kernel policy exhibited a bias towards recommending no\naction, and postulate that this might be due to the fact that surviving neighbors might have been\nhealthier and required fewer interventions. The state representation is meant to cluster similar pa-\ntients, with similar measures of severity more closely so that actions for a more severely ill patient\nare not inﬂuenced by those taken for healthier patients. The bias of the kernel policy towards recom-\nmending no action suggests that the Euclidean distance between learned state representations does\nnot reﬂect patient similarities. Unlike the kernel policy, the DQN model exhibited a bias towards\nmore aggressive interventions. The authors use the kernel policy πk in a mixture-of-experts model,\nwhich selects between the action recommended by πk and that of a Deep Q-Network (DQN) model\nin order to maximize the expected return as estimated by a weighted doubly robust estimator.\nWhile the above proposed solutions to optimizing sepsis management report greater estimated\nreturns and lower expected rates of mortality, the authors also acknowledge the limitations of such\napproaches. Since design choices must be made when modeling the state space, action space, speci-\nfying the reward function and choosing the RL algorithm, several factors may inﬂuence the learned\npolicy. Oﬀ-policy evaluation methods are also inﬂuenced by the choices of state and action represen-\ntations and the speciﬁed reward function, and must therefore be supplemented by examinations of\nthe state representation, and restrictions on the set of actions that an agent may recommend. Addi-\ntionally, it is imperative to verify the eﬀective sample complexity when using WIS or DR evaluation\nmethods as discussed in Section 3.\n7\nApplication to Heparin Dosing\nNemati et al. Nemati et al. [2016] and Lin et al. Lin et al. [2018] use deep reinforcement learning to\nlearn an optimal policy for intravenous heparin dosing. Heparin is an intravenous (IV) anticoagulant\nused to treat blood clots among patients with severe renal failure because other anticoagulants are\ncontraindicated. One diﬃculty is that of sparse measurements and sparse rewards. Since a blood\n17\ntest is required to measure aPTT, observations are not frequently updated, and the response of\nactions is delayed.\nNemati et al.\nNemati et al. [2016] use a discriminative hidden Markov model (DHMM) to\nrepresent the patient’s state conditioned on a history of observations and actions. The output of\nthe DHMM as well as the current action taken are passed to a Q-network. Both the DHMM’s and\nthe Q-network’s parameters are updated by minimizing the TD-error. Thus, a state representation\nis learned from patient history alongside the Q-function. While such an approach provides an end-\nto-end solution, it precludes the veriﬁcation of the learned state representation before learning an\nimproved policy. Additionally, under this training scheme, the learned state representation is only\nused to predict the action value and may not be as informative as state representations which are\nlearned via clustering, autoencoding or next-state prediction methods.\nLin et al. Lin et al. [2018] state that unlike sepsis management with IV ﬂuids and vasopressors,\nheparin dosing policies cannot be learned with a discrete action space, since precise dosage control\nis required to maintain the activated partial thromboplastin time (aPTT) within the therapeutic\nrange for patients receiving heparin. The authors use the deep deterministic policy gradient (DDPG)\nalgorithm Silver et al. [2014] to learn an optimal policy over continuous state and action spaces. The\ndeterministic policy is parameterized by a neural network that outputs the recommended heparin\ndose given patient measurements at a given time. The DDPG algorithm is an instance of actor-critic\nalgorithms discussed in Section 2. The critic is parameterized by a neural network which estimates\nthe action-value or Q-function of the policy network. The training and test sets consist of electronic\npatient data from the MIMIC-III dataset Johnson et al. [2016] and the Emory hospital intensive\ncare unit. Both Nemati et al. Nemati et al. [2016] and Lin et al. Lin et al. [2018] used the reward\nfunction deﬁned in Equation 21 for the objective of maximizing the fraction of time a patient’s aPTT\nstays within the therapeutic range of heparin, which is between 60 and 100 seconds according to\nthe Beth Israel Deaconess Medical Center Ghassemi et al. [2014]. The reward value is 1 for aPTT\nvalues within the therapeutic range, and quickly diminishes to −1 as the aPTT values move further\nfrom the therapeutic range.\nrt =\n2\n1 + e−aP T Tt+60 −\n2\n1 + e−aP T Tt+100 −1\n(21)\nLin et al. Lin et al. [2018] use a direct method to evaluate the learned policy. The authors deﬁne\na distance between the RL policy and the clinicians’ policy over a patient’s trajectory as the mean\ndiﬀerence between the recommended dose and the administered dose.\nThe policy distances are\nbinned into 5 quantiles. For each quantile, the authors plot the observed return of the clinicians’\npolicy, showing decreasing returns as the policy distance increases and that the expected return is\ngreatest when the clinicians’ policy matches the RL policy. Since high return trajectories are more\nlikely to occur in less severe cases, it is diﬃcult to ascertain whether the learned policy is better\n18\nthan the clinicians’ policy. The authors perform a multiple linear regression analysis using patient\ncharacteristics, clinical severity scores and policy distance as features, and the expected return as\nthe outcome. The magnitude of coeﬃcients of patient characteristics and severity scores (SOFA and\npulmonary embolism) were close to 0 and had high p-values while the coeﬃcient for policy distance\nhad a greater magnitude and a low p-value (p < 0.05). While such an analysis appears to show\nthat the expected return is mainly determined by the policy distance and is independent of severity,\nstatistical tests cannot rule out the possibility of confounding Pearl [1998], crucially, such an analysis\ndoes not validate the recommendations made by the RL policy in areas where the policy distance\nis large. Additionally, the policy distances were binned into 5 quantiles, if the distribution of policy\ndistances has a long tail, then eﬀectively most policy distances would correspond to the 0-distance\nbin.\n8\nThe Role of Causality\nThe construction of better treatment strategies from observational data may be viewed as a problem\nof causal inference. Reinforcement learning involves choosing actions based on their subsequent eﬀect\non the state. In a sense, an agent must learn the eﬀect of its actions in an environment governed\nby unknown dynamics. While causal inference seems to be closely tied to reinforcement learning,\nfew papers propose ways to combine their strengths. In this section we review the basic principles\nof causal inference and describe its potential role in sequential decision support. A more thorough\ntreatment of causal inference can be found by Peters et al Peters et al. [2017].\n8.1\nStructural Causal Models\nStructural causal models are built on the assumption that data is generated by an underlying mech-\nanism which entails an observational distribution. Figure 1 shows a structural causal diagram or a\ncausal directed acyclic graph (DAG) with three observable variables, the covariates X, treatment T,\nand outcome Y . The latent variables UX, UY , and UT are unobserved independent noise variables.\nThe SCM, typically denoted by M, describes a data generating process as follows: ﬁrst the noise\nvariables UX, UY , and UT are sampled from their respective prior distributions. Since X is the root\nnode of the observable variables, it is generated ﬁrst using its structural equation X := fx(Ux).\nThe treatment T depends on X and its random noise variable UT . The outcome Y depends on the\ncovariates X, treatment T, and noise variable UY through its structural equation Y := fy(X, T, UY ).\nSuch an SCM entails an observational distribution P M(X, Y, T) = P(Y |X, T)P(T|X)P(X) which\ncan be factored according to the conditional independencies conveyed by the structural causal di-\nagram. In general, structural causal models describe how variables are generated in terms of their\nparents in the DAG, that is X := fx(PA(X)) where PA(X) denotes the set of parent nodes of\n19\n Ux\nX     \n UY\n    Y     \n UT\n   T\nFigure 1: A structural causal model of applying treatment T, with covariates X and outcome Y .\nUnobservable variables UX, UT , and UY are independent and sampled from a prior distribution\nP(U)\nX. Reinforcement learning involves modifying the policy which generated the observed trajectories\nthus changing the data-generating process. Structural causal models (SCM) provide a framework\nfor describing the data-generating process and the distributions entailed by interventions.\nAn intervention I = do\n\u0010\nXi := ˜f\n\u0010\nPAi, ˜Ui\n\u0011\u0011\nmodiﬁes the SCM by replacing the structural mech-\nanism which generates X by X := ˜f\n\u0010\nPAi, ˜Ui\n\u0011\n) for any observable variable X of the SCM. For in-\nstance, changing which features are considered when selecting a treatment T corresponds to changing\nthe set of parent nodes of T, and updating the parameters of a policy corresponds to changing the\nmechanism f. An intervention I, results in a diﬀerent SCM MI, and therefore can entail a diﬀerent\ndistribution P MI(X, Y, T), termed the interventional distribution. Computing the expected return\nof a new policy can therefore be seen as computing the expected outcome Y under an intervention.\nTo compute what would have happened to a particular patient under a diﬀerent policy or particu-\nlar intervention I requires computing the counterfactual distribution. Counterfactual distributions\nP M|X=x;I are evaluated as follows: the latent variables are ﬁrst sampled from the posterior distribu-\ntion U ∼P(U|X = x), then the interventional SCM MI is used to compute the value of observable\nrandom variables, starting with root nodes as described above.\nGenerally, diﬀerent SCM’s can entail the same interventional and observational distributions but\ndiﬀerent counterfactual distributions. As a result, even if we learn mechanisms that ﬁt the data\nwell, they might be inadequate at predicting counterfactuals.\nA new policy may have a higher\nexpected return, but it can be impossible to determine for which patients the outcome is improved.\nCertain types of SCMs do not have this problem; their counterfactual distributions can be uniquely\ndetermined. However, the assumptions required to form such an SCM are untestable. For instance,\nin the case of binary treatments and outcomes, the monotonicity condition is suﬃcient to identify\ncounterfactual distributions of a given SCM Pearl [2009].\nOberst and Sontag Oberst and Sontag [2019] propose a method of evaluating a policy based\non counterfactually generated trajectories. The authors generalize the monotonicity condition to\ncategorical treatments and outcomes, termed the counterfactual stability condition. To sample tra-\njectories with categorical treatments and outcomes, Oberst and Sontag Oberst and Sontag [2019]\n20\nuse a Gumbel-Max SCM and show that it satisﬁes counterfactual stability, hence its counterfactual\ndistributions are identiﬁable. In order to illustrate the use of counterfactual estimation for policy\nevaluation, the authors build a simpliﬁed sepsis simulator where state transitions are modeled using\na Gumbel-Max distribution, thus assuming that the true data-generating distribution is counter-\nfactually identiﬁable. Observations from the simulator consist of four vital signs, heart rate, blood\npressure, oxygen concentration and glucose levels, each is discretized into 3 bins; low, medium and\nhigh. The behaviour or clinician’s policy is set close to optimal by performing policy iteration us-\ning the simulator’s MDP and taking a random action with probability 0.05. The authors use 1000\nsampled trajectories from the simulator using the behaviour policy as the observational data used\nfor training. The simulator is not used to learn the target policy since the authors wish to illustrate\nthe potential pitfalls of training with observational data.\nTo mimic the situation of partial observability, the glucose and diabetes state are hidden when\nlearning the target policy.\nThe resulting training data is used to learn a model of sepsis using\nnormalized empirical counts of state transitions. Policy iteration is used to learn the target policy,\nwhile restricting the action space to the set of observed actions. Under this experimental setup, since\nthe true counterfactual trajectories can be computed using the simulator, the authors reveal how\noﬀ-policy evaluation and batch reinforcement learning can be optimistic. First, the target policy was\nevaluated using weighted importance sampling (WIS), and model-based policy evaluation (MB-PE)\nusing the parameters of the learned sepsis model. As shown in Figure 2, both methods estimate a\nhigh value for the target policy, with WIS having a noticeably high variance. Second, counterfactual\npolicy evaluation (CPE) was performed by sampling counterfactual trajectories from the Gumbel-\nMax SCM constructed from the learned transition probabilities. CPE also overestimates the value\nof the learned policy, reﬂecting the dangers of unobserved confounders.\nThe authors then generated trajectories under the target policy with the sepsis simulator, and\nobserved that the target policy performs poorly. Speciﬁc counterfactual trajectories can be examined\nto understand why oﬀ-policy evaluation overestimates the target policy. Using the learned SCM the\nauthors sample counterfactual trajectories for each individual patient in the validation set under the\ntarget policy and examine the trajectories of patients whose outcome is predicted to be discharge\nunder the target policy, but whose observed outcome under the clinician’s policy is mortality. For\ninstance, the target policy recommended stopping treatment for a speciﬁc patient whose vital signs\nwere normal except for glucose levels which were dangerously low.\nSince diabetes and glucose\nﬂuctuations are rare in the simulated data, and since they are not observed, the learned SCM predicts\nthat the patient has a high chance of recovery since the other vital signs are normal. When the\nfull state was observable to the RL algorithm, WIS on heldout data resulted in a median estimated\nvalue closer to the true value of the target policy. However model-based (MB) oﬀ-policy evaluation\ncontinues to overestimate the target policy as shown in Figure 3 which suggests that the learned\n21\ntransitions continue to suﬀer from extrapolation errors.\nWhile the experiments of Oberst and Sontag Oberst and Sontag [2019] are rather simpliﬁed, they\nillustrate the susceptibility of reinforcement learning to unobserved confounders and extrapolation\nerrors due to limited observational data. Accounting for the non-randomization of policies found in\nmedical records, and how it inﬂuences the learned policy, is a crucial step towards the applicability of\nreinforcement learning in health care. It is well known that supervised methods for risk stratiﬁcation\nare susceptible to confounding factors Paxton et al. [2013].\nFor example Caruana et al. [2015]\nremarked that since asthmatic patients are more likely to receive intensive care for pneumonia than\nnon-asthmatic patients, mortality predictors can learn that asthmatic patients are at a lower risk of\nmortality than non-asthmatic patients. Gottesman et al. Gottesman et al. [2019, 2018] reasoned that\nreinforcement learning is not immune to such confounding. Combining counterfactual models with\nreinforcement learning could be used to adjust for spurious correlations resulting from the clinicians’\npolicies.\nObs\nWIS\nMB\nCF\nTrue\n−0.5\n0.0\n0.5\n1.0\nAverage Reward\nFigure 2: Box plots of observed reward under the behavioural policy and the estimated reward for\nthe target policy using WIS, MB and CF policy evaluation methods. The true reward observed for\nnewly simulated trajectories under the target policy is much lower. Figure obtained from Oberst\nand Sontag [2019]\n9\nValidation of Decision Support\nSince the use of decision support tools include a practitioner in-the-loop, such tools do not exempt\nhealth practitioners from the responsibility for their decisions. While it is ultimately up to the health\npractitioner to decide whether to follow the treatment regime suggested by an RL agent, or ignore\nit if it isn’t a reasonable suggestion, this does not mean that machine learning or reinforcement\nlearning models can be treated as a black box. A model that produces a questionable suggestion\neven once will be diﬃcult to trust subsequently and clinicians may forgo its use altogether, thus\nnullifying the intended beneﬁt of the decision support tool.\n22\nObs\nWIS (train)\nWIS (heldout)\nMB\nTrue\n−1.0\n−0.5\n0.0\n0.5\n1.0\nAverage Reward\nFigure 3: Box plots of observed reward under the behavioural policy and the estimated reward\nfor the target policy using WIS on training and heldout data, and MB policy evaluation methods.\nThe target policy was learned with full access to the state. MB policy evaluation overestimates the\npolicy’s value.\nA commonly used demonstration of the validity of the learned policy is the plot of the frequency of\nmortality in the dataset against diﬀerences between administered dosages and recommended dosages\nby the learned policy. A characteristic U-shape is often observed, where the frequency of mortality\nis shown to increase as the diﬀerence between the administered and recommended dosages increases.\nWhile the mortality rate should be lower when the clinician’s and learned policies agree, the higher\nmortality rate of patients who were administered diﬀerent dosages than the recommended treatment\nshould not be interpreted as evidence for the validity of the learned policy. Additionally, as shown\nby Gottesman et al Gottesman et al. [2018] a similar U-curve can even be obtained by comparing the\nclinicians’ policy with a no treatment or random policy. The U-curve for a no-treatment policy simply\nshows the correlation between mortality and higher administered dosages of IV and vasopressors.\nSince the distribution of administered dosages has a long tail, discretizing the action space based on\nquantiles essentially places most observed actions into the ﬁrst bin, corresponding to zero-dosage. It\nis therefore no surprise that the learned policy and the no-action policy result in similar U-curves.\nGottesman et al. Gottesman et al. [2018] suggested that the learned policy can be sensitive to\nthe choice of state representation. Since oﬀ-policy evaluation is performed using the chosen state\nrepresentation, it can be diﬃcult to ascertain the validity of the learned policy. A suggested action\ncould be supported by showing which parts of the state representation or observation vector most\naﬀected the choice of action recommendation. For example, the suggestion to administer antibiotics\ncan be supported by pointing to the white blood cell count or the results of a culture test. Ribeiro et\nal. Ribeiro et al. [2016] provided indicators of feature inﬂuence by approximating a deep model locally\nto a more interpretable model of lower complexity. Similary, Chen et al. Chen et al. [2018] developed\na feature selector, trained to select features by maximizing the mutual information between the\nselected features and the model’s output. It is worth investigating whether incorporating solutions\nfor interpretability in decision support can be eﬀective at revealing how the model is inﬂuenced by\n23\ncorrelations between the behavioural policy and patient features.\nIn the case of discovering better treatment strategies, highlighting the inﬂuence of patient fea-\ntures may not be enough to explain a learned policy, since it does not reveal how the feature is\ninterpreted by the agent. When deciding on a course of action for a given patient, a physician can\nbe inﬂuenced by non-clinical factors such as the patient’s socioeconomic status, age, or gender Hajjaj\net al. [2010]. While age and gender could be considered as clinical factors, since they are related\nto co-morbidities Hajjaj et al. [2010], they can also have non-clinical inﬂuences. Factors such as a\npatient’s comfort and preferences could also lead to unnecessary or sub-optimal interventions, such\nas unnecessarily prescribing antibiotics due to pressure from patients Hajjaj et al. [2010]. Although\nthe state representation could exclude these non-clinical factors, they may still play a role. E.g., if\na patient’s comfort is correlated with compliance, then it may be beneﬁcial to consider a patient’s\npreferences when suggesting a course of treatment. Since some features can have a clinical and non-\nclinical inﬂuence on decisions, it can be challenging to understand the nature of their inﬂuence on\nthe learned policy. Another challenge arises from the need to limit treatment costs while improving\npatient outcomes. These objectives could come into conﬂict, and it might be unclear which objective\na chosen action favours.\n10\nConclusion\nWhile reinforcement learning provides a natural solution for learning improved policies for sequen-\ntial tasks, its application in healthcare introduces diﬃculties with regards to describing the state\nand action space, learning and evaluating policies from observational data, and designing reward\nfunctions. Since the chosen action and state representations aﬀect the learned policy as well as\nits estimated value, oﬀ-policy evaluation may not be enough to reveal shortcomings of the learned\npolicy. Additionally, when using observational data to learn a model of patient progression, one\nmust be careful about distributional shifts incurred by a change in policy which might invalidate\nthe model in under-explored areas of the state space. A potential area for future research is the\ndevelopment of standards or heuristics to validate and diagnose learned policies. Such methods\nshould be centered around how to account for rare or unseen states, and understanding the eﬀect\nof the chosen state and action representations. For instance, a continuous state representation re-\nquires a parameterized Q-function which is more susceptible to extrapolation errors; however, a\ndiscretized state space and a tabular Q-function may result in a loss of granularity. How should\ndesigners interpret diﬀerences in the learned policy under diﬀerent design choices? What kinds of\nimprovement should researchers reasonably expect given the limit of observed state-action pairs?\nGiven the safety-critical nature of recommending treatment actions, validating learned policies will\nrequire more than general guidelines if RL is to become a widely adopted approach for personalized\n24\ntreatment.\nAcknowledgements\nFR is a CIFAR Chair in Artiﬁcial Intelligence.\nReferences\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Mané.\nConcrete problems in AI safety. CoRR, abs/1606.06565, 2016. URL http://arxiv.org/abs/\n1606.06565.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.\nIn Advances in Neural Information Processing Systems, pages 5048–5058, 2017.\nLiam Byrne and Frank Van Haren. Fluid resuscitation in human sepsis: time to rewrite history?\nAnnals of intensive care, 7(1):4, 2017.\nRich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible\nmodels for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings\nof the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\nKDD ’15, pages 1721–1730, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3664-2. doi:\n10.1145/2783258.2788613. URL http://doi.acm.org/10.1145/2783258.2788613.\nJianbo Chen, Le Song, Martin Wainwright, and Michael Jordan.\nLearning to explain:\nAn\ninformation-theoretic perspective on model interpretation. In Jennifer Dy and Andreas Krause,\neditors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Pro-\nceedings of Machine Learning Research, pages 883–892, Stockholmsmässan, Stockholm Sweden,\n10–15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/chen18j.html.\nLi-Fang Cheng, Niranjani Prasad, and Barbara E Engelhardt. An optimal policy for patient labo-\nratory tests in intensive care units. In PSB, pages 320–331. World Scientiﬁc, 2019.\nJack Clark Dario Amodei.\nFaulty reward functions in the wild.\nhttps://blog.openai.com/\nfaulty-reward-functions/, 2017. Accessed: 2019-08-25.\nStefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steﬀen Udluft. Decomposi-\ntion of uncertainty in Bayesian deep learning for eﬃcient and risk-sensitive learning. In Jennifer Dy\nand Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learn-\ning, volume 80 of Proceedings of Machine Learning Research, pages 1184–1193, Stockholmsmäs-\n25\nsan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/\ndepeweg18a.html.\nEric Elenko, Austin Speier, and Daphne Zohar. A regulatory framework emerges for digital medicine.\nNature biotechnology, 33(7):697, 2015.\nDamien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.\nJ. Mach. Learn. Res., 6:503–556, 2005.\nGabriel J Escobar, Vincent X Liu, Alejandro Schuler, Brian Lawson, John D Greene, and Patricia\nKipnis.\nAutomated identiﬁcation of adults at risk for in-hospital clinical deterioration.\nNew\nEngland Journal of Medicine, 383(20):1951–1960, 2020.\nAndre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and\nSebastian Thrun.\nDermatologist-level classiﬁcation of skin cancer with deep neural networks.\nNature, 542:115, jan 2017. URL http://dx.doi.org/10.1038/nature21056http://10.0.4.14/\nnature21056.\nMehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust\noﬀ-policy evaluation. In ICML, 2018.\nScott Fujimoto, David Meger, and Doina Precup. Oﬀ-policy deep reinforcement learning without\nexploration. In ICML, 2018.\nMohammad M Ghassemi, Stefan E Richter, Ifeoma M Eche, Tszyi W Chen, John Danziger, and\nLeo A Celi. A data-driven approach to optimized medication dosing: a focus on heparin. Intensive\ncare medicine, 40(9):1332–1339, 2014.\nOmer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan,\nLinying Zhang, Yi Ding, David Wihl, Xuefeng Peng, et al. Evaluating reinforcement learning\nalgorithms in observational health settings. arXiv preprint arXiv:1805.12298, 2018.\nOmer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale\nDoshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nature\nmedicine, 25(1):16–18, 2019.\nFadi M Hajjaj, M Sam Salek, Mohammad KA Basra, and Andrew Y Finlay. Non-clinical inﬂuences\non clinical decision-making: a major challenge to evidence-based practice. Journal of the Royal\nSociety of Medicine, 103(5):178–187, 2010.\nDaniel Hein, Alexander Hentschel, Volkmar Sterzing, Michel Tokic, and Steﬀen Udluft. Introduction\nto the\" industrial benchmark\". arXiv preprint arXiv:1610.03793, 2016.\n26\nNan Jiang and Lihong Li. Doubly robust oﬀ-policy value evaluation for reinforcement learning.\narXiv preprint arXiv:1511.03722, 2015.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a\nfreely accessible critical care database. Scientiﬁc data, 3:160035, 2016.\nMatthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The arti-\nﬁcial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature\nMedicine, 24(11):1716, 2018.\nVijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information\nprocessing systems, pages 1008–1014, 2000.\nRongmei Lin, Matthew D Stanley, Mohammad M Ghassemi, and Shamim Nemati.\nA deep de-\nterministic policy gradient approach to medication dosing and surveillance in the ICU. In 2018\n40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society\n(EMBC), pages 4927–4931. IEEE, 2018.\nDaniel J Lizotte and Eric B Laber. Multi-objective Markov decision processes for data-driven decision\nsupport. The Journal of Machine Learning Research, 17(1):7378–7405, 2016.\nTO Loftsgard and R Kashyap. Clinicians role in reducing lab order frequency in ICU settings. J\nPerioper Crit Intensive Care Nurs, 2(112):2, 2016.\nPE Marik.\nThe demise of early goal-directed therapy for severe sepsis and septic shock.\nActa\nAnaesthesiologica Scandinavica, 59(5):561–567, 2015.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518:529–533, 2015.\nShamim Nemati, Mohammad M Ghassemi, and Gari D Cliﬀord. Optimal medication dosing from\nsuboptimal clinical examples: A deep reinforcement learning approach. In Engineering in Medicine\nand Biology Society (EMBC), 2016 IEEE 38th Annual International Conference of the, pages\n2978–2981. IEEE, 2016.\nAndrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:\nTheory and application to reward shaping. In ICML, volume 99, pages 278–287, 1999.\nMichael A. Oberst and David A Sontag. Counterfactual oﬀ-policy evaluation with Gumbel-Max\nstructural causal models. In ICML, 2019.\n27\nChris Paxton, Alexandru Niculescu-Mizil, and Suchi Saria.\nDeveloping predictive models using\nelectronic medical records: challenges and pitfalls.\nIn AMIA Annual Symposium Proceedings,\nvolume 2013, page 1109. American Medical Informatics Association, 2013.\nJudea Pearl. Why there is no statistical test for confounding, why many think there is, and why\nthey are almost right. https://ftp.cs.ucla.edu/pub/stat_ser/TEST/R256.ps, 1998. Accessed:\n2019-08-25.\nJudea Pearl. Causality. Cambridge university press, 2009.\nXuefeng Peng, Yi Ding, David Wihl, Omer Gottesman, Matthieu Komorowski, Li-wei H Lehman,\nAndrew Ross, Aldo Faisal, and Finale Doshi-Velez. Improving sepsis treatment strategies by com-\nbining deep and kernel-based reinforcement learning. In AMIA Annual Symposium Proceedings,\nvolume 2018, page 887. American Medical Informatics Association, 2018.\nJonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations\nand learning algorithms. MIT press, 2017.\nNiranjani Prasad, Li-Fang Cheng, Corey Chivers, Michael Draugelis, and Barbara E Engelhardt.\nA reinforcement learning approach to weaning of mechanical ventilation in intensive care units.\narXiv preprint arXiv:1704.06300, 2017.\nDoina Precup, Richard S. Sutton, and Satinder P. Singh.\nEligibility traces for oﬀ-policy policy\nevaluation. In ICML, 2000.\nAniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh\nGhassemi. Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602,\n2017a.\nAniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, and Marzyeh Ghas-\nsemi. Continuous state-space models for optimal sepsis treatment-a deep reinforcement learning\napproach. arXiv preprint arXiv:1705.08422, 2017b.\nAniruddh Raghu, Matthieu Komorowski, and Sumeetpal Singh. Model-based reinforcement learning\nfor sepsis treatment. arXiv preprint arXiv:1811.09602, 2018.\nDeepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In IJCAI, volume 7,\npages 2586–2591, 2007.\nAndrew Rhodes, Laura E Evans, Waleed Alhazzani, Mitchell M Levy, Massimo Antonelli, Ricard\nFerrer, Anand Kumar, Jonathan E Sevransky, Charles L Sprung, Mark E Nunnally, et al. Sur-\nviving sepsis campaign: international guidelines for management of sepsis and septic shock: 2016.\nIntensive care medicine, 43(3):304–377, 2017.\n28\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the\npredictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference\non knowledge discovery and data mining, pages 1135–1144. ACM, 2016.\nMartin Riedmiller. Neural ﬁtted Q iteration–ﬁrst experiences with a data eﬃcient neural reinforce-\nment learning method. In European Conference on Machine Learning, pages 317–328. Springer,\n2005.\nChristopher W Seymour, Jason N Kennedy, Shu Wang, Chung-Chou H Chang, Corrine F Elliott,\nZhongying Xu, Scott Berry, Gilles Clermont, Gregory Cooper, Hernando Gomez, et al. Derivation,\nvalidation, and potential treatment implications of novel clinical phenotypes for sepsis. Jama, 321\n(20):2003–2017, 2019.\nDavid Silver, Guy Lever, Nicolas Manfred Otto Heess, Thomas Degris, Daan Wierstra, and Martin A.\nRiedmiller. Deterministic policy gradient algorithms. In ICML, 2014.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go\nwithout human knowledge. Nature, 550(7676):354, 2017.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nJacqueline Vincent, Rosean Moreno, Jukka Takala, Sheila M. Willatts, Arnaldo de Mendonca, Hilgo\nBruining, Chantal Reinhart, Peter M. Suter, and Lambertius G. Thijs. The sofa (sepsis-related\norgan failure assessment) score to describe organ dysfunction/failure. Intensive Care Medicine,\n22:707–710, 1996.\nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.\nWei-Hung Weng, Mingwu Gao, Ze He, Susu Yan, and Peter Szolovits. Representation and reinforce-\nment learning for personalized glycemic control in septic patients. arXiv preprint arxiv:1712.00654,\n2017.\nChao Yu, Jiming Liu, and Hongyi Zhao. Inverse reinforcement learning for intelligent mechanical\nventilation and sedative dosing in intensive care units. BMC medical informatics and decision\nmaking, 19(2):57, 2019.\nJinghe Zhang, Kamran Kowsari, James H Harrison, Jennifer M Lobo, and Laura E Barnes. Pa-\ntient2vec: A personalized interpretable deep representation of the longitudinal electronic health\nrecord. IEEE Access, 6:65333–65346, 2018.\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse\nreinforcement learning. In AAAI, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.\n29\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "J.3; I.2"
  ],
  "published": "2021-03-09",
  "updated": "2021-03-09"
}