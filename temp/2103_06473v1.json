{
  "id": "http://arxiv.org/abs/2103.06473v1",
  "title": "Multi-Task Federated Reinforcement Learning with Adversaries",
  "authors": [
    "Aqeel Anwar",
    "Arijit Raychowdhury"
  ],
  "abstract": "Reinforcement learning algorithms, just like any other Machine learning\nalgorithm pose a serious threat from adversaries. The adversaries can\nmanipulate the learning algorithm resulting in non-optimal policies. In this\npaper, we analyze the Multi-task Federated Reinforcement Learning algorithms,\nwhere multiple collaborative agents in various environments are trying to\nmaximize the sum of discounted return, in the presence of adversarial agents.\nWe argue that the common attack methods are not guaranteed to carry out a\nsuccessful attack on Multi-task Federated Reinforcement Learning and propose an\nadaptive attack method with better attack performance. Furthermore, we modify\nthe conventional federated reinforcement learning algorithm to address the\nissue of adversaries that works equally well with and without the adversaries.\nExperimentation on different small to mid-size reinforcement learning problems\nshow that the proposed attack method outperforms other general attack methods\nand the proposed modification to federated reinforcement learning algorithm was\nable to achieve near-optimal policies in the presence of adversarial agents.",
  "text": "Multi-Task Federated Reinforcement Learning with\nAdversaries\nAqeel Anwar1, Arijit Raychowdhury2\nDepartment of Electrical and Computer Engineering\nGeorgia Institute of Technology, Atlanta, GA, USA\naqeel.anwar@gatech.edu1, arijit.raychowdhury@ece.gatech.edu2\nAbstract—Reinforcement learning algorithms, just like any\nother Machine learning algorithm pose a serious threat from\nadversaries. The adversaries can manipulate the learning algo-\nrithm resulting in non-optimal policies. In this paper, we analyze\nthe Multi-task Federated Reinforcement Learning algorithms,\nwhere multiple collaborative agents in various environments are\ntrying to maximize the sum of discounted return, in the presence\nof adversarial agents. We argue that the common attack\nmethods are not guaranteed to carry out a successful attack\non Multi-task Federated Reinforcement Learning and propose\nan adaptive attack method with better attack performance. Fur-\nthermore, we modify the conventional federated reinforcement\nlearning algorithm to address the issue of adversaries that\nworks equally well with and without the adversaries. Experi-\nmentation on different small to mid-size reinforcement learning\nproblems show that the proposed attack method outperforms\nother general attack methods and the proposed modiﬁcation to\nfederated reinforcement learning algorithm was able to achieve\nnear-optimal policies in the presence of adversarial agents.\nKeywords—Adversaries, MT-FedRL, FedRL, Attack\nI. INTRODUCTION\nIn the past decade, Reinforcement Learning (RL) has gained\nwide popularity in solving complex problems in an online\nfashion for various problem sets such as game playing [1],\nautonomous navigation [2], [3], robotics [4] and network\nsecurity [5]. In most of the real-life cases where we do not\nhave complete access to the system dynamics, conventional\ncontrol theory fails to provide optimum solutions. Model-free\nRL on the other hand uses heuristics to explore and exploit\nthe environment to achieve the underlying goal. With a boom\nin Internet of Things (IoT) devices [6], we have a lot of\ncompute power at our disposal. The problem, however, is\nthat the compute is distributed. Distributed algorithms have\nbeen studied to take advantage of these distributed compute\nagents. Conventional method consist of using these IoT as data-\ncollectors and then using a centralized server to train a network\non the collected data. Federated Learning, introduced by\nGoogle [7]–[9] is a distributed approach to machine learning\ntasks enabling model training on large sets of decentralized\ndata by individual agents. The key idea behind federated\nlearning is to preserve the privacy of the data to the local node\nresponsible for generating it. The training data is assumed\nto be local only, the agents however, can share the model\nparameter that is learned. This model sharing serves two\npurpose. Primarily it ensures the privacy of the data being\nServer\n…\nAgent 0\nEnvironment 0\nUpdate Policy\n(s, a, s’, r)\naction\nAgent 1\nEnvironment 1\nUpdate Policy\n(s, a, s’, r)\naction\nAgent n-1\nEnvironment n-1\nUpdate Policy\n(s, a, s’, r)\naction\nRL Return\nRL Return\nRL Return\nFig. 1: Federated RL - The idea is to learn a common uniﬁed policy\nwithout sharing the local training data that works good enough for\nall the environments\ngenerated locally, secondly in some of the cases the size\nof the model parameter might be much smaller than the\nsize of the local data, hence sharing the model parameter\ninstead of the data might save up on the communication\ncost involved. Federated learning has also been considered\nin the context of Reinforcement learning problem for both\nmulti-agent RL [10]–[13] and multi-task RL [14]–[16] where\nmultiple RL agents either in a single or multiple environments\ntry to jointly maximize the collective or sum of individual\ndiscounted returns, respectively.\nWhile ML algorithms have proven to provide superior\naccuracy over conventional methods, they pose a threat from\nadversarial manipulations. Adversaries can use a variety of\nattack models to manipulate the model either in the training\nor the inference phase leading to decreased accuracy or poor\npolicies. Common attack methods include data-poisoning and\nmodel poisoning where the adversary tries to manipulate the\ninput data or directly the learned model respectively. In this\npaper, we propose and analyze a model-poisoning attack for\nthe Multi-task Federated RL (MT-FedRL) problem and modify\nthe conventional Federated RL approach to provide protection\nfrom model poisoning attacks.\nThe contributions of this paper are as follows\n• We carry out a detailed study on the multi-task federated\nRL (MT-FedRL) with model-poisoning adversaries on\nmedium and large size problems of grid-world (Grid-\nWorld) and drone autonomous navigation(AutoNav).\n• We argue that the general adversarial methods are not\ngood enough to create an effective attack on MT-FedRL,\narXiv:2103.06473v1  [cs.LG]  11 Mar 2021\nand propose a model-poisoning attack methodology\nAdAMInG based on minimizing the information gain\nduring the MT-FedRL training.\n• Finally, we address the adversarial attack issue by\nproposing a modiﬁcation to the general FedRL algorithm,\nComA-FedRL, that works equally well with and without\nadversaries.\nThe rest of the paper is organized as follows. In Sec. II\nwe provide a background on the related work in the area of\nadversarial machine learning. Sec. III and IVformally deﬁnes\nthe MT-FedRL problem and the adversarial formulation\nrespectively. We then move on to some common threat models\nand propose an attack method in sec. V. Sec. VI proposes\nsome modiﬁcations to the conventional FedRL approach to\naddress the issues of adversaries. Finally in sec. VII we\nanalyze the proposed attack methods and address them on\na series of real-world problems before concluding it in sec.\nVIII.\nII. RELATED WORK\nThe effects of adversaries in Machine learning algorithms\nwere ﬁrst discovered in [17] where it was observed that a\nsmall lp norm perturbation to the input of a trained classiﬁer\nmodel resulted in conﬁdently misclassifying the input. These\nlp norm perturbations were visually imperceptible to humans.\nThe adversary here acts in the form of speciﬁcally creating\nadversarial inputs to produce erroneous outputs to a learned\nmodel [18]–[23]. For supervised learning problems, such as\na classiﬁcation task, where the network model has already\nbeen trained, attacking the input is the most probable choice\nfor an adversary to attack through. In RL, there is no clear\nboundary between the training and test phase. The adversary\ncan act either in the form of data-poisoning attacks, such\nas creating adversarial examples [24], [25], or can directly\nattack the underlying learned policy [26]–[29] either in terms\nof malicious falsiﬁcation of reward signals, or estimating the\nRL dynamics from a batch data set and poisoning the policy.\nAuthors in [30], try to attack an RL agent by selecting an\nadversarial policy acting in a multi-agent environment as a\nresult of creating observations that are adversarial in nature.\nTheir results on a two-player zero-sum game show that an\nadversarial agent can be trained to interact with the victim\nwinning reliably against it. In federated RL, alongside the\ndata-poisoning and policy-poisoning attacks, we also have\nto worry about the model-poisoning attacks. Since we have\nmore than one learning agents, a complete agent can take up\nthe role of an adversary. The adversarial agent can feed in\nfalse data to purposely corrupt the global model. In model\npoisoning attacks the adversary, instead of poisoning the\ninput, tries to adversely modify the learned model parameters\ndirectly by feeding false information purposely poisoning\nthe global model [31], [32]. Since federated learning uses\nan average operator to merge the local model parameters\nlearned by individual agents, such attacks can severely affect\nthe performance of the global model.\nAdversarial training can be used to mitigate the effects of\nsuch adversaries. [33] showed that the classiﬁcation model\ncan be made much robust against the adversarial examples by\nfeature de-noising. The robustness of RL policies has also been\nanalyzed by the adversarial training [34]–[37]. [34], [37] show\nthat the data-poisoning can be made a part of RL training to\nlearn more robust policies. They feed perturbed observations\nduring RL training for the trained policy to be more robust to\ndynamically changing conditions during test time. [38] shows\nthat the data-poisoning attacks in federated learning can be\nresolved by modifying the federated aggregation operator\nbased on induced ordered weighted averaging operators [39]\nand ﬁltering out possible adversaries. To the best of our\nknowledge, there is no detailed research carried out on MT-\nFedRL in the presence of adversaries. In this paper, we address\nthe effects of model poisoning attacks on the MT-FedRL\nproblem.\nIII. MULTI-TASK FEDERATED REINFORCEMENT\nLEARNING (MT-FEDRL)\nWe consider a Multi-task Federated Reinforcement Learn-\ning (MT-FedRL) problem with n number of agents. Each agent\noperates in its own environment which can be characterized\nby a different Markov decision process (MDP). Each agent\nonly acts and makes observations in its own environment.\nThe goal of MT-FedRL is to learn a uniﬁed policy, which is\njointly optimal across all of the n environments. Each agent\nshares its information with a centralized server. The state\nand action spaces do not need to be the same in each of\nthese n environments. If the state spaces are disjoint across\nenvironments, the joint problem decouples into a set of n\nindependent problems. Communicating information in the\ncase of N-independent problems does not help.\nWe consider policy gradient methods for RL. The MDP\nat each agent i can be described by the tuple Mi =\n(Si, Ai, Pi, Ri, γi) where Si is the state space, Ai is the\naction space, Pi is the MDP transition probabilities, Ri :\nSi × Ai →R is the reward function, and γi ∈(0, 1) is the\ndiscount factor.\nLet V π\ni\nbe the value function, induced by the policy π, at\nthe state s in the i-th environment, then we have\nV π\ni (s) = E\n\" ∞\nX\nk=0\nγk\ni Ri(sk\ni , ak\ni ) | s0\ni = s\n#\n,\nak\ni ∼π(·|sk\ni ).\n(1)\nSimilarly, we have the Q-function Qπ\ni\nand advantage\nfunction Aπ\ni for the i-th environment as follows\nQπ\ni (si, ai) = E\n\" ∞\nX\nk=0\nγk\ni R(sk\ni , ak\ni ) | s0\ni = si, a0\ni = ai\n#\n,\nAπ\ni (si, ai) = Qπ\ni (si, ai) −V π\ni (si).\n(2)\nWe denote by ρi the initial state distribution over the\naction space of i-th environment., The goal of the MT-FedRL\nproblem is to ﬁnd a uniﬁed policy π∗that maximizes the sum\nof long-term discounted return for all the environment i i.e.\n2\nmax\nπ\nV (π; ρ) ≜\nn−1\nX\ni=0\nEsi∼ρiV π\ni (si),\nρ =\n\n\nρ0\n...\nρn−1\n\n\n(3)\nSolving the above equation will yield a uniﬁed π∗resulting\nin a balanced performance across all the environments.\nWe use the parameter θ to model the family of policies\nπθ(a|s), considering both the tabular method (for simpler\nproblems) and neural network-based function approximation\n(for complex problems). The goal of the MT-FedRL problem\nthen is to ﬁnd θ∗satisfying\nθ∗= arg max\nθ\nV (θ; ρ) ≜\nn−1\nX\ni=0\nEsi∼ρiV πθ\ni\n(si).\n(4)\nIn tabular method, gradient ascent methods are utilized to\nsolve (3) over a set of randomized stationary policies {πθ :\nθ ∈R|S|×|A|}, where θ uses the softmax parameterization\nπθ(a | s) =\nexp (θs,a)\nP\na′∈A exp(θs,a′)·\n(5)\nFor a simpler problem where the size of state-space and action-\nspace is limited, this table is easier to maintain. For more\ncomplex problems with a larger state/action space, usually\nneural network-based function approximation {πθ : S →A}\nis used, where θ are the trainable weights of a pre-deﬁned\nneural network structure.\nOne approach to solving this uniﬁed-policy problem is by\nsharing the data Mi observed by each agent in its environment\nto a centralized server. The centralised server then can train a\nsingle policy parameter θ based on the collective data M =\n∪n−1\ni=0 Mi. This, however, comes with the cost of reduced\nprivacy as the agent needs to share its data with the server.\nIn MT-FedRL, however, the data tuple Mi is not shared with\nthe server due to privacy concerns. The data remains at the\nlocal agent and instead, the policy parameter θi is shared\nwith the server. Each agent i utilizes its locally available data\nDi to train the policy parameter θi by maximizing its own\nlocal value function V π\ni\nthrough SGD. We assume policy\ngradient methods for RL training. After the completion of\neach episode k, the agents share their policy parameter θk−\ni\nwith a centralized server. The server carries out a smoothing\naverage and generates N new sets of parameters θk+\ni\n, one\nfor each agent, using the following expression.\nθk+\ni\n= αkθk−\ni\n+ βk X\nj̸=i\nθk−\nj\n(7)\nwhere αk, βk = 1−α\nn−1 ∈(0, 1) are non-negative smoothing\naverage weights. The goal of this smoothing average is to\nachieve a consensus among the agents’ parameters, i.e.\nlim\nk→∞θk+\ni\n→θ∗\n∀i ∈{0, n −1}\n(8)\nAs the training proceeds, the smoothing average constants\nconverge to αk, βk →\n1\nn. The conditions on αk, βk to\nguarantee the convergence of Algorithm 1 can be found in\nAlgorithm 1: Multi-task Federated Reinforcement\nLearning (MT-FedRL) with smoothing average\nInitialization: θ0+\ni\n∈Rd, step sizes δk, smoothing\naverage threshold iteration t\n%Server Executes\nfor k=1,2,3,... do\nCalculate smoothing average parameters\nαk = 1\nn max(1, k/t),\nβk = 1 −αk\nn −1\nfor each agent i in parallel do\nReceive updated policy parameter from clients\nθ(k+1)−\ni\n←ClientUpdate\n\u0000i, θk+\ni\n\u0001\nend\nfor each agent i do\nPolicy update:\nθ(k+1)+\ni\n= αkθ(k+1)−\ni\n+ βk X\ni̸=j\nθ(k+1)−\nj\nSend updated policy parameter θ(k+1)+\ni\nback\nto client i\nend\nend\nFunction ClientUpdate(i, θ):\n1) Compute the gradient of the local value\nfunction ∂V\nπθ\ni\n(ρi)\n∂θsi,ai\nbased on the local data\n2) Update the policy parameter\nθ−= θ + δk ∂V πθ\ni\n(ρi)\n∂θsi,ai\n(6)\nreturn θ−\n[16]. The complete algorithm of multi-task federated RL can\nbe found in Alg. 1\nIV. MT-FEDRL WITH ADVERSARIES\nMT-FedRL has proven to converge to a uniﬁed policy that\nperforms jointly optimal on each environment [16]. This\njointly optimal policy yields near-optimal policies when\nevaluated on each environment if the agents’ goals are\npositively correlated. If the agent’s goals are not positively\ncorrelated, the uniﬁed policy might not result in a near-\noptimal policy for individual environments. This is exactly\nwhat happens to MT-FedRL in the presence of an adversary.\nLet L denote the set of adversarial agents in a n −\nagent MTFedRL problem. The smoothing average at the\nserver can be decomposed based on the adversarial and non-\nadversarial agent as follows\nθk+\ni\n= αkθk−\ni\n+ βk\nX\nj̸=i,j /∈L\nθk−\nj\n+ βk X\nl∈L\nθk−\nl\n(9)\nwhere i /∈L. θk+\ni\nis the updated policy parameter for\nagent i calculated by the server at iteration k. This update\nincorporates the knowledge from other environments and\nas the training proceeds, these updated policy parameters\nfor all the agents converge to a uniﬁed parameter θ∗. In\n3\na non-adversarial MT-FedRL problem, this uniﬁed policy\nparameter ends up achieving a policy that maximizes the\nsum of discounted returns for all the environments. In an\nadversarial MT-FedRL problem, the goal of the adversarial\nagent is to prevent the MT-FedRL from achieving this uniﬁed\nθ∗by purposely providing an adversarial policy parameter\nθk−\nl\n.\nParameters that effect learning: Using gradient ascent, each\nagent updates its own set of policy parameter locally according\nto the following equation,\nθk−\ni\n= θ(k−1)+\ni\n+ δi∇θiV\nπθi\ni\n(ρi)\n(10)\nwhere δi is the learning rate for agent i. Using Eq. 10 in\nthe smoothing average Eq. 7 yields\nθk+\ni\n=\n\nαkθ(k−1)+\ni\n+ βk\nX\nj̸=i,j /∈L\nθ(k−1)+\nj\n\n+\n\nαkδi∇θiV\nπθi\ni\n(ρi) + βk\nX\nj̸=i,j /∈L\nδj∇θjV\nπθj\nj\n(ρj)\n\n+\n \nβk X\nl∈L\nθk−\nl\n!\n(11)\nThe server update of the policy parameter can be decom-\nposed into three parts.\n• The weighted sum of the previous set of policy param-\neters θ(k−1)+\ni\nshared by the server with the respective\nagents.\n• The agent’s local update, which tries to shift the policy\nparameter distribution towards the goal direction.\n• The adversarial policy parameter which aims at shifting\nthe uniﬁed policy parameter away from achieving the\ngoal.\nIf the update carried out by the adversarial agent is larger\nthan the sum of each agent’s policy gradient update, the policy\nparameter will start moving away from the desired consensus\nθ∗. The success of the adversarial attack hence depends on,\n• The nature of adversarial policy parameter θk−\nl\n• Non-adversarial agent’s local learning rate δi\n• The number of non-adversarial agents n −|L|\nAn adversarial attack is more likely to be successful if the\nlocal learning rate of non-adversarial agents δi is small and\nthe number of adversarial agents |L| is large.\nThreat Model: For an adversarial agent to be successful in\nits attack, it needs to shift the convergence from θ∗in Eq. 8\nto θ′ such that the resultant policy π′ follows\nEsi∼ρiV π′\ni (si) << Esi∼ρiV π∗\ni\n(si),\n∀i /∈L\n(12)\nThe only way an adversarial agent can control this conver-\ngence is through the policy parameter θk−\nl\nthat it shares with\nthe server. The adversarial agent needs to share the policy\nparameter that moves the distribution of the smoothing average\nof non-adversarial agents either to a uniform distribution or\nServer\n…\nAgent 0\nEnvironment 0\nUpdate Policy\n(s, a, s’, r)\naction\nAgent l\nEnvironment l\nUpdate Policy\n(s, a, s’, r)\naction\nAgent n-1\nEnvironment n-1\nUpdate Policy\n(s, a, s’, r)\naction\nRL Return\nRL Return\nRL Return\nCommunicate\nevery n iterations\nFig. 2: Adversaries can negatively impact the uniﬁed policy by\nproviding adversarial policies to the server. This results in negatively\nimpacting the achieved discounted return on the environments\nin the direction that purposely yields bad actions (Fig. 3).\nGenerally shifting the distribution to uniform distribution will\nrequire less energy than to shift it to a non-optimal action\ndistribution. This requires that the adversary cancel out the\ninformation gained by all the other non-adversarial agents\nhence not being able to differentiate between good and bad\nactions, leaving all the actions equally likely.\nThreat Model: We will assume the following threat model.\nAt iteration k, each adversarial agent l shares the following\npolicy parameter with the server\nθk−\nl\n= λkθk\nadv\n(13)\nHence the threat model is deﬁned by the choice of the attack\nmodel θadv and λk ∈R which is a non-negative iteration-\ndependant scaling factor that will be used to control the norm\nof the adversarial attack. To make the scaling factor more\nmeaningful, we will assume that\n∥θadv∥2 ≈\n1\n(n −|L|)\nX\ni/∈L\n∥θi∥2\nThe relative difference in the norm of the policy parameter\nbetween the adversarial agent and non-adversarial agent will\nbe captured in the scaling factor term λk. One thing we need\nto be mindful of is the value of the scaling factor. If the\nscaling factor is large enough, almost any random (non-zero)\nvalues for the adversarial policy parameter θk\nl will result in a\nsuccessful attack. We will quantify the relative performance\nof the threat models by\n• How effective they are in attacking, either in terms of\nthe achieved discounted return under the threat induced\nuniﬁed policy or in terms of a more problem-speciﬁc\ngoal parameter (more in the experimentation section).\n• If two threat models achieve the same attacking perfor-\nmance, the one with a smaller scaling factor λk will be\nconsidered better. The smaller the norm of the adversarial\npolicy parameter, the better the chances for the threat\nmodel to go unnoticed by the server.\nV. COMMON ATTACK MODELS\nIn this section, we will discuss a few common attack models\nθadv and propose an adaptive attack model. For the rest of the\n4\nFig. 3: The objective of an adversarial agent is to shift the policy\ndistribution that yields poor actions\nsection, we will focus on the single-agent adversarial model\n|L| = 1. The extension of these threat models to multiple\nadversarial agents is straight forward.\nA. Random Policy Attack (Rand)\nThis attack will be used as a baseline for the other attack\nmethods. In a Random policy attack, the adversarial agent\nmaintains a set of random policy parameter sampled from\na Gaussian distribution with mean 0 and standard deviation\nσ ∈R i.e. for each element θadv,j of θadv\nθadv,j ∼N(0, σ)\n(14)\nThis attack assumes that the adversary has no knowledge to\nestimate the best attack method from. If the scaling factor λk\nis large enough, this attack method can shift the distribution\nof the policy towards a random distribution.\nB. Opposite Goal Policy Attack (OppositeGoal)\nThis attack method assumes that a sample environment\nis available for the agent to devise the attack. In this attack\nmethod, the adversary l learns a policy πOG\nθadv utilizing its\nlocal environment with the goal of minimizing (instead of\nmaximizing) the long term discounted return i.e. the objective\nfunction to maximize is\nJ(θadv) = −V\nπθadv\nl\n(ρl)\n(15)\nWith the completion of an episode k, the adversary updates\nits policy parameter θadv locally by maximizing Eq.15 and\nshares the scaled version of the updated policy parameter\nwith the server.\nThe OppositeGoal attack method can either shift the policy\nto a uniform distribution, or to a distribution which prefers\nactions that yield opposite goal. For the agent to shift the\ndistribution to uniform, the following constraints need to hold.\n1) All the N environments should be similar enough that\nthey generate policies that are close enough i.e.\n1\n|S|\nX\ns∈S\nKL(πθi(.|s), πθj(.|s)) ≤ϵ\n(16)\nor equivalently if the learning rate and initialization is\nthe same for each agent then\n\r\r\r\r\nθi\n∥θi∥2\n−\nθj\n∥θj∥2\n\r\r\r\r\n2\n≤ϵ\n(17)\n2) Action selection based on minimum probability action\nfor OppositeGoal policy should be close enough to action\nselection based on maximum probability for NormalGoal\nPolicy\n1\n|S|\nX\ns∈S\nKL(1 −πOG\nθi (.|s), πθj(.|s)) ≤ϵ\n(18)\nor equivalently if the learning rate is same initialization\nis zero θ0\ni = 0\n\r\r\r\r\nθi\n∥θi∥2\n+\nθj\n∥θj∥2\n\r\r\r\r\n2\n≤ϵ\n(19)\nIn short, all the N environments should be similar enough\nsuch that training on an opposite goal will yield a policy that\nwhen combined with a policy learned to maximize the actual\ngoal will yield a complete information loss.\nMost of the time, these assumptions will not hold as\nthey are too strict (the difference in environment dynamics,\ninitialization of policy parameter, the existence of multiple\nlocal minima, etc.). Instead, if the scaling factor is large, the\nOppositeGoal attack will shift the distribution of the consensus\nto an opposite goal policy. Since we are taking into account\nthe environment dynamics, this attack will however be better\nthan the random policy attack.\nC. Adversarial Attack by Minimizing Information Gain\n(AdAMInG)\nEven though the adversarial choice of opposite goal makes\nan intuitive sense as the best attack method, we will see\nin the results section that it’s not. Hence, we propose an\nattack method that takes into account the nature of MT-FedRL\nsmoothing averaging and devises the best attack given the\ninformation available locally. The goal of AdAMInG is to\ndevise an attack that uses a single adversarial agent with a\nsmall scaling factor by forcing the server to forget what it\nlearns from the non-adversarial agents.\nFor the smoothing average at the server to lose all the\ninformation gained by other non-adversarial agents we should\nhave\nθk−\nl\n= −\n1\nβk|L|\n\nαkθk−\ni\n+ βk X\nj̸=i,l\nθk−\nj\n\n\n(20)\nUsing the above equation in Eq. 9 will result θk+\ni\n= 0,\nhence losing the information gained by θk−\ni\n. The problem\nin the above equation is that the adversarial agents do not\nhave access to the policy parameter shared by non-adversarial\nagents θk−\ni\n, ∀i ̸= l and hence the quantity in the parenthesis\n(smoothing average of the non-adversarial agents) is unknown.\nThe attack model then is to estimate the smoothing average\nof the non-adversarial agents.\nThe adversarial agent has the following information avail-\nable to it\n• The last set of policy parameter shared by the adversarial\nagent to the server θ(k−1)−\nl\n• The federated policy parameter shared by the server to\nthe adversarial agent θ(k−1)+\nl\n5\nThe adversarial agent can estimate the smoothing average\nof the non-adversarial agents from these quantities. The\nAdAMInG attack shares the following policy parameter\nθk−\nl\n= λk\n \nαkθ(k−1)+\nl\n−θ(k−1)−\nl\nβk\n!\n(21)\nThe smoothing average at the server for i ∈{0, n−1}, i ̸= l\nbecomes\nθk+\ni\n= αkθk−\ni\n+ βk X\ni̸=j,l\nθk−\nj\n+ βkθk−\nl\n= αkθk−\ni\n+ βk X\ni̸=j,l\nθk−\nj\n−\nλk\nn −1αk \u0010\nθ(k−1)+\nl\n−θ(k−1)−\nl\n\u0011\n= αkθk−\ni\n+ βk X\ni̸=j,l\nθk−\nj\n−\nλk\nn −1βk X\nj̸=l\nθ(k−1)−\nj\n=\n\u0012\nαkθk−\ni\n−\nλk\nn −1βkθ(k−1)−\ni\n\u0013\n+\nX\nj̸=i,l\n\u0012\nβkθk−\nj\n−βkλk\nn −1θ(k−1)−\nj\n\u0013\nWe want θk+\ni\n→0, ∀i ∈{0, n−1}, i ̸= l. This means forcing\nthe two terms inside the parenthesis to 0. If the initialization\nof all the agents are same, i.e. θ0−\ni\n= θ0 = 0 ∀i and the\nlearning rate is small, we have ∥θk−\ni\n−θ(k−1)−\ni\n∥< ϵ. Hence\nθk+\ni\n→0 can be achieved by the following scaling factor\nλk∗= argminλk g(λk, n)\nwhere\ng(λk, n) =\n\f\f\f\fαk −βk\nλk\nn −1\n\f\f\f\f +\n\f\f\f\fβk\n\u0012\n1 −\nλk\nn −1\n\u0013\n(n −2)\n\f\f\f\f\nFor simplicity we have not shown the dependence of αk, βk in\nthe expression g(λk, n) as they directly depend on k. Solving\nthis optimization problem yields\nλ∗= n −1,\nn ≥3\n(22)\nThis means that the scaling factor should be equal to the\nnumber of non-adversarial agents and is independent of the\niteration k. For λk < λ∗we still can achieve a successful\nattack if the learning rate δ is not too high.\nAs the training proceeds, the values of the smoothing\nconstants αk, βk approach their steady-state value of\n1\nn.\nAt that point, the steady-state value of g(λk, n) deﬁned as\ngss(λk, n) is given by\ngss(λk, n) = n −1 −λ\nn\n(23)\nThe steady-state value gss(λk, n) signiﬁes how effec-\ntive/successful the AdAMInG attack will be for the selected\nparameters (λk, n). A steady-state value of 0 signiﬁes a perfect\nattack, where the policy parameter shared by the server loses\nall the information gained by the non-adversarial agents. On\nthe other hand, a steady-state value of 1 indicates a completely\n0\n20\n40\n60\n80\n100\nNumber of agents (n)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ngss(λ, n)\nFig. 4: g\n\u0000λk, n\n\u0001\nas a function of λk = 1 and n\n0\n20\n40\n60\n80\n100\nScaling factor (λ)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ngss(λ, n)\nFig. 5: g\n\u0000λk, n\n\u0001\nas a function of λk and n = 100\nunsuccessful attack. The smaller the gss(λk, n), the better the\nAdAMInG attack.\nFig. 4 plots g(λk, n) as a function of the number of agents\nn for a scaling factor of 1 (λk = 1). It can be seen that\nas the number of agents increases, the steady-state value\ngss becomes closer to 1 making it difﬁcult for AdAMInG to\nhave a successful attack with a scaling factor of 1. As the\nnumber of agents increases, the update carried out by the\nnon-adversarial agent has a more signiﬁcant impact on the\nsmoothing average than the adversarial agent making it harder\nfor the adversarial agent to attack. Fig. 5 plots g(λk, n) as\na function of the scaling factor λk for n = 100. It can be\nseen that the scaling factor has a linear impact on the success\nof the AdAMInG attack. The performance of the AdAMInG\nattack increases linearly with the increase in the scaling factor.\nThe best AdAMInG attack is achieved when λk = n−1 which\nis consistent with Eq. 22.\nIn the experimentation section, we will see that a non-zero\nsteady-state value can still result in a successful attack for a\nsmall learning rate δ.\nIt is safe to assume that if we do not change the learning rate\n(and it is small enough), we can ﬁnd the scaling factor required\nto achieve the same attacking performance by increasing the\nnumber of agents n. The steady-state relationship between\nn and λ in Eq. 23 lets us analyze the relative attacking\nperformances by varying the number of agents n. Let’s say\n6\nEvaluated Policy\nEnvironment\nCumulative reward\nNon-adv\nNon-adv\nHigh\nNon-adv\nAdv\nLow (Secondary Attack)\nAdv\nNon-adv\nLow\nAdv\nAdv\nLow (Secondary Attack)\nTABLE I: Cross-evaluation of policies in ComA-FedRL in terms\nof cumulative return\nthat for n1 number of agents and a given learning rate that\nis small, we were able to achieve a successful attack with λ1.\nNow to achieve the same successful attack for n2 number of\nagents we need\nλ2 = n2(1 + λ1)\nn1\n−1\n(24)\nUnlike the OppositeGoal attack, we can guarantee that the\nAdAMInG attack will yield a successful attack if the scaling\nfactor is equal to the number of non-adversarial agents.\nWe will see in the results section that the scaling factor\nneeds no to be this high if the learning rate δ is not high.\nWe will be able to achieve a good enough attack even if\nλk < n −1. The only down-side with the AdAMInG attack\nmethod is that it requires twice the amount of memory as\ncompared to that of OppositeGoal or Rand attack method.\nAdAMInG attack method needs to store the both the adversary\nshared policy parameter θ(k−1)−\nl\nand the server shared policy\nparameter θ(k−1)+\nl\nfrom the previous iteration to compute the\nnew set of policy parameters to be shared θk−\nl\nas shown in Eq.\n21. However, as opposed to the OppositeGoal attack method,\nthe AdAMInG attack method does not require to learn from\nthe data sampled from environment saving up much on the\ncompute cost.\nVI. DETECTING ATTACKS - COMA-FEDRL\nWe will see in Sec. VII that the FedRL algorithm under the\npresence of an adversary can severely affect the performance\nof the uniﬁed policy. Hence, we propose Communication\nAdaptive Federated RL (ComA-FedRL) to address the ad-\nversarial attacks on a Federated RL algorithm. Instead of\ncommunicating the policy parameter from all agents at a ﬁxed\ncommunication interval, we assign different communication\nintervals to agents based on the conﬁdence of them being\nan adversary. An agent, with higher conﬁdence of being an\nadversary, is assigned a large communication interval and\nvice-versa. Communicating less frequently with an adversary\nagent can greatly mitigate its effects on the learned uniﬁed\npolicy. Since we can’t guarantee that a certain agent is an\nadversary or not, we can’t just cut off the communication\nwith an agent we think would be an adversary. Moreover,\nan adversary can fake being a non-adversarial agent to get\naway with being marked as an adversarial agent. Hence, we\ndon’t mark agents as adversary or non-adversary, rather we\nadaptively vary the communication interval between the server\nand the agents based on how good, on average, does the policy\nof the agent performs in other environments. The complete\nalgorithm of ComA-FedRL can be seen in Algo. 2.\nAlgorithm 2: Communication Aware Federated RL\nInitialization: Initialize number agents n, θ0\ni ∈Rd, step\nsize δk, base_comm ∈R, comm[i] = base_comm ∀i ∈\n{0, n −1}, wait_comm ∈R\nfor k=1,2,3,... do\n% Pre-train phase\nif k ≤wait_comm then\nif k%base_comm = 0 then\nfor each agent i in parallel do\nθ(k+1)−\ni\n←ClientUpdate\n\u0000i, θk+\ni\n\u0001\nend\nr ←CrossEvalPolicies\n\u0000r, θ(k+1)−\u0001\nend\nend\nelse\nCalculate smoothing average parameters αk, βk\ncomm ←UpdateCommInt(r, comm)\nfor each agent i in parallel do\nif k%comm[i] = 0 then\nθ(k+1)−\ni\n←ClientUpdate\n\u0000i, θk+\ni\n\u0001\nend\nend\nnum_active_agents = 0\nfor each agent i do\nif k%comm[i] = 0 then\nnum_active_agents+=1\nθ(k+1)+\ni\n= αkθ(k+1)−\ni\n+ βk X\ni̸=j\nθ(k+1)−\nj\nSend θ(k+1)+\ni\nback to client i\nend\nend\nif num_active_agents = n then\nr ←CrossEvalPolicies\n\u0000r, θ(k+1)−\u0001\nend\nend\nend\nFunction CrossEvalPolicies(r, θ):\nfor each agent i in parallel do\nRandomly assign each agent i another agent j\nwithout replacement\nrj.append(ClientEvaluate(i, θj))\nend\nreturn r\nFunction ClientUpdate(i,θ):\n1) Compute the gradient of the local value\nfunction ∂V\nπθ\ni\n(ρi)\n∂θsi,ai\nbased on the local data;\n2) Update the policy parameter\nθ−= θ + δk ∂V πθ\ni\n(ρi)\n∂θsi,ai\nreturn θ−\nFunction ClientEvaluate(i, θj):\nEvaluate the policy θj on agent i and return the\ncumulative reward ret\nreturn ret\n7\nFig. 6: [GridWorld] The 12 environments used\nComA-FedRL begins with a pre-train phase, where each\nagent tries to learn a locally optimistic policy independent\nof others. These locally optimistic policies are expected\nto perform better than a random policy on other agents’\nenvironments. After every certain number of episodes, the\nserver randomly assigns a policy to all the environments\nwithout replacement for evaluation, and the cumulative reward\nachieved by this policy is recorded. Based on the nature\nof the policy and the environment it is cross-evaluated in,\nwe have four cases as shown in Table I. When the policy\nlocally learned by a non-adversarial agent is evaluated in the\nenvironment of a non-adversarial agent, it generally performs\nbetter than a random policy because of the correlation of the\nunderlying tasks. Hence we get a slightly higher cumulative\nreward compared to other cases. On the other hand, if an\nadversarial policy is cross-evaluated on a non-adversarial\nagent’s environment, it generally performs worse because of\nthe inherent nature of the adversary, giving a low cumulative\nreward. When the policies are evaluated on the adversarial\nagent’s environment, the adversary can present a secondary\nattack in faking the cumulative reward. It intentionally reports\na low cumulative return with the hopes of confusing the\nserver to mistake a non-adversarial agent with an adversarial\none. Since the adversarial agent has no way of knowing if\nthe policy shared by the server belongs to an adversarial or\na non-adversarial agent, it always shares a low cumulative\nreturn.\nAt the end of the pre-train phase, the cumulative rewards\nare averaged out for a given policy and are compared to a\nthreshold. If the averaged cumulative reward of the policy\nis below (above) this threshold, the policy is marked as\npossibly-adversarial (possibly-non-adversarial). The possibly-\nadversarial agents are assigned a higher communication\ninterval (less frequent communication), while possibly-non-\nadversarial agents are assigned a smaller communication\ninterval (more frequent communication). The agents are\nconstantly re-evaluated after a certain number of iterations and\nthe categories associated with the agents are updated. After\nre-evaluation, if an already marked possible-adversary agent is\nre-marked as possibly-adversary, the agent’s communication\ninterval is doubled, signifying a higher probability of it being\nan adversary and making it contribute even lesser towards\nthe server smoothing average. Hence as the training proceeds,\nthe adversarial agent’s contribution to the server smoothing\naverage becomes smaller and smaller.\nFurther details on the variables and functions used in Alg.\n2 can be found in the Appendix section A.\nVII. EXPERIMENTATION\nFor the entire experimentation section, we focus on single-\nadversary MT-FedRL and hence |L| = 1. We report the\nexperimental results from a simpler tabular-based RL problem\n(GridWorld) to a more complex neural network-based RL\nproblem (AutoNav). In both cases, we use policy gradient\nbased RL methods.\nA. GridWorld - Tabular RL\nProblem Description: We begin our experimentation with\na simple problem of GridWorld. The environments are\ngrid world mazes of size 10 × 10 as seen in Fig. 6.\nEach cell in the maze is characterized into one of the\nfollowing 4 categories: hell-cell (red), goal-cell (yellow),\nsource-cell (green), and free-cell (white). The agent is\ninitialized at the source-cell and is required to reach the\ngoal-cell avoiding getting into the hell-cell. The free-cells\nin the maze can be occupied without any consequences.\nThe agent can take one of the following 4 actions A =\n{move-up, move-down, move-right, move-left}\nwhich corresponds to the agent moving one cell in the\nrespective direction. At each iteration, the agent observes\na one-step SONAR-based state s ∈R4 which corresponds to\nthe nature of the four cells (up, down, right, left) surrounding\nthe agent. If the corresponding cell is a hell-cell, goal-cell,\nor free-cell, the corresponding state element is -1, 1, or 0\nrespectively. Hence, we have |S| = 81. Based on the nature of\nthe environment, only a subset of these states will be available\nfor each environment. At each iteration, the agent samples\nan action from the action space and based on the next state,\nobserves a reward. The reward is -1, 1, 0.1, or -0.1 if the\nagent crashed into hell-cell, reached the goal, moved closer\nto or away from the goal respectively. The effectiveness of\nthe MT-FedRL-achieved uniﬁed policy is quantiﬁed by the\nwin ratio (WR) deﬁned by\nWR =\n1\nn −1\nX\ni̸=l\n# of times agent i reached goal state\ntotal # of attempts in environment i\nIn this 12-agent MT-FedRL system, agent 0 is assigned the\nadversarial role (l = 0). The goal for agent 0 is to decrease\nthis win ratio. We will characterize the performance of the\n8\nλ = n −1\nλ = 1\nScaling factor λ\n0\n20\n40\n60\n80\n100\npsa(%)\nAdAMInG\nOppositeGoal\nRandom\nFig. 7: [GridWorld] Probability of successful attack psa(%) under\ndifferent attack models. The greater the psa the better the perfor-\nmance of the adversary.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nLearning rate δ\n0\n20\n40\n60\n80\n100\npsa(%)\nAdAMInG\nOppositeGoal\nRandom\nFig. 8: [GridWorld] Effect of learning rate (δ) on the performance\nof attack methods with λ = 1 and n = 12.\nadversarial attack by the probability of successful attack psa\ngiven by\npsa = 1 −\nWRadv\nWRno−adv\nwhere WRadv is the win ratio with an adversary, while\nWRno−adv is the win ratio without any adversary. An attack\nmethod is said to be successful with probability psa if it is\nable to defeat the system psa% of the time compared to a\nnon-adversarial MT-FedRL. The greater the psa the better the\nattack performance of the adversary.\nEffect of Adversaries We begin the experimentation by\nanalyzing the effect of the common attack models mentioned\nin Sec. V. Fig. 7 reports the psa for the three attack methods\nwith the scaling factor of n −1 and 1 (and a learning rate\nσ = 0.2). With the optimal scaling factor of n −1, it can be\nseen that all the three attack methods were able to achieve a\ngood enough attack (psa > 96%). For a scaling factor of 1,\nhowever, only AdAMInG attack method was able to achieve\na successful attack (psa = 98%). Both the random policy\nattack and OppositeGoal attack were only half as good as\nthe AdAMInG attack method with OppositeGoal being only\nslightly better than a random policy attack.\nAs mentioned in section IV, for a scaling factor of 1, the\nperformance of the attack method depends on the learning\nrate (δ) and the number of non-adversarial agents (n −|L|).\nFig. 8 reports psa of the attack methods with varying learning\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nLearning rate δ\n0\n20\n40\n60\n80\n100\npsa(%)\nAdAMInG (λ = 1)\nOppositeGoal (λ = 2)\nFig. 9: [GridWorld] Comparing attack performance for n = 12\nbetween AdAMInG with λ = 1 and OppositeGoal with λ = 2.\nn=4\nn=8\nn=12\nNumber of agents n\n0\n20\n40\n60\n80\n100\npsa(%)\nAdAMInG\nOppositeGoal\nRandom\nFig. 10: [GridWorld] Effect of number of agents (n) on the\nperformance of attack methods with λ = 1 and δ = 0.2\nrates. It can be seen that the greater the learning rate, the\npoorer the performance of the attack method in terms of\npsa. For a higher learning rate, the local update for each\nagent’s policy parameter has more effect than the update of\nthe server carried out with the adversary, and hence poorer\nthe performance of the attack. Another thing to observe is that\nas the learning rate increases the relative performance of the\nOppositeGoal attack compared to the Random policy attack\nbecomes poorer even becoming worse than the Random policy\nattack. The reason behind this is that the observable states\nacross environments are non-overlapping. The environment\navailable to the adversary for devising OppositeGoal attack\nfrom might not have access to the states observable in\nother environments. Hence the OppositeGoal policy attack\ncan not modify the policy parameter related to those states.\nOppositeGoal policy attack method either require a large\nscaling factor or more than one adversary to attack the MT-\nFedRL with performance similar to AdAMInG with single-\nadversary and unity scaling factor λ. This can be seen in Fig.\n9.\nA similar trend can be observed with varying the number\nof non-adversarial agents. It can be seen in Fig. 10 that for a\nsmaller number of non-adversarial agents (equivalently smaller\nnumber of total agents if the number of the adversarial agents\nis ﬁxed), it is easier for the adversary to attack with a high\npsa. The reason behind this is that the local update in Eq. 11\nis proportional to the number of non-adversarial agents. With\n9\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nNumber of episodes k\n0\n2\n4\n6\n8\n10\n12\n1\nn−1\nP\ni̸=l ∥θi∥2\n.\nδ\n.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFig. 11: [GridWorld] Based on the learning rate, the consensus gets\nconverged to an intermediate value\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nNumber of episodes k\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n1\nn−1\nP\ni̸=l\n\u0010P∞\nj rj\n\u0011\n.\nδ\n.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFig. 12: [GridWorld] Cumulative return (moving average of 60) for\ndifferent learning rate (δ) and n = 12\na smaller number of non-adversarial agents, the local update\nis smaller compared to the update by the adversary. Among\nthe three attack methods, AdAMInG is the most resilient to\nthese two parameters (λ, n), hence making it a better choice\nfor an adversarial attack in an MT-FedRL setting.\nAnalyzing AdAMInG Attack: We carry out a detailed\nanalysis of the AdAMInG attack method. The smoothing\naverage (Eq. 11) in the presence of an adversary carries\nout two updates - the local update which moves the policy\nparameter in a direction to maximize the collective goal,\nand the adversarial update which tries to move the policy\nparameter away from the consensus. When the training begins,\nthe initial set of policy parameters θi is farther away from\nthe consensus θ∗. Gradient descent ﬁnds a direction from\nthe current set of policy parameters to the consensus. This\ndirection has a higher magnitude when the distance between\nthe current policy parameter and the consensus is high. As\nthe system learns, the current policy parameter gets closer to\nthe consensus, and hence the magnitude of the direction of\nupdate decreases. So even if we have a static learning rate\nδ, the magnitude of local update δj∇θjV\nπθj\nj\n(ρj) in Eq. 11\nwill, in general, decrease as the system successfully learns.\nThere will be a point in training where the local update will\nbecome equal but opposite to the update being carried out\nby the AdAMInG adversary. From that point onwards, the\ncurrent policy parameter won’t change much. This can be\nseen in Fig. 11. The greater the learning rate δ, the earlier\nin training we will get to the equilibrium point, and hence\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nLearning rate δ\n0.0\n0.1\n0.2\n0.3\n0.4\nstd (θ∗)\nAdAMInG\nFig. 13: [GridWorld] Standard deviation of the consensus policy\nparameter\nConﬁguration\nLearning rate δ\npsa%\nstd\nλ = 1, n = 8\n0.2\n99.75%\n0.036\nλ = 2, n = 12\n0.2\n99.49%\n0.031\nTABLE II: [GridWorld] Relationship between λ and n for same\nattack performance with AdAMInG\npoorer the attack performance which can be seen in terms of\nthe achieved discounted return in Fig. 12. A greater standard\ndeviation of the consensus policy parameter indicates a better\ndifferentiation between good and bad actions for a given\nstate. Fig. 13 plots the standard deviation of the consensus\npolicy parameter for different learning rates δ. It can be seen\nthat for higher learning rates, the consensus has a higher\nstandard deviation hence being able to perform better than\nthe consensus achieved under lower learning rates.\nWe also compare the performance of the AdAMInG attack\nin relation to the scaling factor λ and the number of agents n.\nAccording to Eq. 24 an increase in the number of agents can\nbe compensated by increasing the scaling factor λ to achieve\nthe same attacking performance. We analyse the AdAMInG\nattack for the following two conﬁgurations: (λ = 1, n = 8)\nand (λ = 2, n = 12). Table II reports the psa and the standard\ndeviation of the consensus policy parameter θ∗. It can be seen\nthat both the conﬁgurations generate similar numbers. The\nsame trend can be observed temporally, in Fig. 14, for the\nachieved discounted return during each episode in training.\nResolving adversaries: We implement the N-agent single-\nadversary MT-FedRL problem using ComA-FedRL to address\nthe high psa of the conventional FedRL algorithm. Fig. 15\ncompares the performance of FedRL and ComA-FedRL for\ndifferent attack methods. By assigning a higher communica-\ntion interval to the probable adversary, ComA-FedRL was\nable to decrease the probability of successful attack psa in\nthe presence of adversary to as low as < 10%. The mean\ncommunication interval for adversarial and non-adversarial\nagents is plotted in Fig. 16. It can be seen that Random\npolicy attack has a slightly higher communication interval.\nThe reason behind this is one of the non-adversarial agents\nwas incorrectly marked as a probable adversarial agent at the\nbeginning of training, but later that was self-corrected to a\npossibly-non-adversarial agent.\n10\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nNumber of episodes k\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1\nn−1\nP\ni̸=l\n\u0010P∞\nj rj\n\u0011\nλ = 2, n = 12\nλ = 1, n = 8\nFig. 14: [GridWorld] Relationship between λ and n for same\nAdAMInG attack performance. (λ = 1, n = 8) and (λ = 2, n = 12)\nfollows the same discounted return across episodes which is in\naccordance with Eq. 24\nFedRL\nComa-FedRL\n0\n20\n40\n60\n80\n100\npsa(%)\nAdaming\nOppositeGoal\nRandom\nNo Adversary\nFig. 15: [GridWorld] Comparison of probability of successful\nattack psa(%) under different attack models for FedRL and\nComA-FedRL. The effect of adversarial agent is greatly reduced\nwith ComA-FedRL.\nB. AutoNav - NN based RL\nProblem Description: We also experiment on a more com-\nplex problem of drone autonomous navigation in 3D realistic\nenvironments. We use PEDRA [40] as the drone navigation\nplatform. The drone is initialized at a starting point and is\nrequired to navigate across the hallways of the environments.\nThere is no goal position, and the drone is required to ﬂy\navoiding the obstacles as long as it can. At each iteration t,\nthe drone captures an RGB monocular image from the front-\nfacing camera which is taken as the state st ∈R(320×180×3)\nof the RL problem. Based on the state st, the drone takes an\naction at ∈A. We consider a perception based probabilistic\naction space with 25 actions (|A| = 25). A depth-based\nreward function is used to encourage the drone to stay away\nfrom the obstacles. We use neural network-based function\napproximation to estimate the action probabilities based on\nstates. The C3F2 network used is shown in Fig. 17. We\nconsider 4 indoor environments (indoor-twist, indoor-frogeyes,\nindoor-pyramid, and indoor-complex) hence we have n = 4.\nThese environments can be seen in Fig. 18.\nThe effectiveness of MT-FedRL-achieved uniﬁed policy is\nAdAMInG\nOppositeGoal\nRandom\nNo Adversary\nAttack method\n0\n100\n200\n300\n400\n500\n600\nCommunication Intervals\nAdv agents\nNon-Adv agents\nFig. 16: [GridWorld] Average communication intervals for adver-\nsarial and non adversarial agents in ComA-FedRL\nConv Layer\nMaxPool\nFully Connected Layer\nInput Feature Map\n103\n103\n12\n12\n96\n7\n7\n3\n3\n5\n5\n2\n2\n3\n3\n3\n64\n1024\n1024\n25\n4\n4\nConv1\nConv2\nConv3\nFC1\nFC2\nFig. 17: [AutoNav] C3F2 neural network used to map states to\naction probabilities\nquantiﬁed by Mean Safe Flight (MSF) deﬁned as\nMSF =\n1\nn −1E\n\nX\ni̸=l\ndi\n\n\nwhere di is the distance traveled by the agent in the environ-\nment i before crashing. In this 4-agent MT-FedRL system,\nthe agent in the environment indoor-complex is assigned the\nadversarial role (l = 3). The goal for the adversarial agent is\nto decrease this MSF. We will characterize the performance of\nthe adversarial attack by the probability of successful attack\npsa given by\npsa = 1 −\nMSFadv\nMSFno−adv\nwhere MSFadv is the mean safe ﬂight of the MT-FedRL\nsystem in the presence of the adversary, while MSFno−adv\nis the mean safe ﬂight of the MT-FedRL system in the absence\nof the adversary. The greater the psa the better the attack\nmethod in achieving its goal.\nEffect of Adversaries: For each experiment, the MT-FedRL\nproblem is trained for 4000 episodes using the REINFORCE\nalgorithm with a learning rate of 1e-4 and γ = 0.99. Training\nhyper-parameters are listed in the appendix section A in detail.\nTable III reports the MSF achieved by the AutoNav problem\nfor various attack methods. It can be seen that except for\nthe AdAMInG attack, the rest of the attack methods achieve\nMSF comparable to the one achieved in the absence of an\nadversary (∼1000m). Fig. 19 plots the psa for the different\n11\nFig. 18: [AutoNav] Floor plan and screenshot of the four 3-D environments used\nFedRL\nComA-FedRL\n0\n20\n40\n60\n80\n100\npsa(%)\nAdaming\nOppositeGoal\nRandom\nFig. 19: [AutoNav] Comparison of probability of successful\nattack psa(%) under different attack models for FedRL and\nComA-FedRL. The effect of adversarial agent is greatly reduced\nwith ComA-FedRL.\nAdAMInG\nOpposite Goal\nRandom\nNo Adv\nFedRL\n6\n1076\n1098\n1137\nComA-FedRL\n1042\n1028\n1134\n1156\nTABLE III: [AutoNav] MSF (m) for different attack methods\nattack methods. It can be seen that AdAMInG achieves a psa\nof ∼99.5% while all the other attack methods achieve a psa\nof < 6%. The trend is similar to what was observed in the\nGridWorld task\nResolving Adversaries: We implement the N-agent single-\nadversary MT-FedRL problem using ComA-FedRL to address\nthe low MSF of FedRL. The results are reported in Table\nIII. It can be seen that the decrease in MSF due to adversary\nwas recovered using ComA-FedRL. Fig. 19 plots the psa for\nvarious attack methods with ComA-FedRL and compares it\nwith FedRL. It can be see that with ComA-FedRL we have\npsa < 10%. Hence ComA-FedRL was able to address the\nissue of adversaries in a MT-FedRL problem.\nVIII. CONCLUSION\nIn this paper we analyse Multi-task Federated Reinforce-\nment Learning algorithm with an adversarial perspective. We\nanalyze the attacking performance of some general attack\nmethods and propose an adaptive attack method AdAMInG that\ndevises an attack taking into account the aggregation operator\nof federated RL. The AdAMinG attack method is formulated\nand its effectiveness is studied. Furthermore, to address the\nissue of adversaries in MT-FedRL problem, we propose a com-\nmunication adaptive modiﬁcation to conventional federated\nRL algorithm, ComA-FedRL, that varies the communication\nfrequency for the agents based on their probability of being\nan adversary. Results on the problems of GridWorld (maze\nsolving) and AutoNav (drone autonomous navigation) show\nthat the AdAMInG attack method outperforms other attack\nmethods almost every time. Moreover, ComA-FedRL was\nable to recover from the adversarial attack resulting in near-\noptimal policies.\nIX. ACKNOWLEDGEMENTS\nThis work was supported in part by C-BRIC, one of six\ncenters in JUMP, a Semiconductor Research Corporation\n(SRC) program sponsored by DARPA.\nREFERENCES\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nnature, vol. 518, no. 7540, pp. 529–533, 2015.\n[2] M. A. Anwar and A. Raychowdhury, “Navren-rl: Learning to ﬂy in real\nenvironment via end-to-end deep reinforcement learning using monoc-\nular images,” in 2018 25th International Conference on Mechatronics\nand Machine Vision in Practice (M2VIP).\nIEEE, 2018, pp. 1–6.\n[3] C. Wang, J. Wang, Y. Shen, and X. Zhang, “Autonomous navigation\nof uavs in large-scale complex environments: A deep reinforcement\nlearning approach,” IEEE Transactions on Vehicular Technology, vol. 68,\nno. 3, pp. 2124–2136, 2019.\n[4] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in\nrobotics: A survey,” The International Journal of Robotics Research,\nvol. 32, no. 11, pp. 1238–1274, 2013.\n[5] L. Xiao, X. Wan, C. Dai, X. Du, X. Chen, and M. Guizani, “Security\nin mobile edge caching with reinforcement learning,” IEEE Wireless\nCommunications, vol. 25, no. 3, pp. 116–122, 2018.\n[6] S. Li, L. Da Xu, and S. Zhao, “The internet of things: a survey,”\nInformation Systems Frontiers, vol. 17, no. 2, pp. 243–259, 2015.\n[7] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,\nV. Ivanov, C. Kiddon, J. Koneˇcn`y, S. Mazzocchi, H. B. McMahan\net al., “Towards federated learning at scale: System design,” arXiv\npreprint arXiv:1902.01046, 2019.\n[8] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan,\nS. Patel, D. Ramage, A. Segal, and K. Seth, “Practical secure\naggregation for privacy-preserving machine learning,” in Proceedings of\nthe 2017 ACM SIGSAC Conference on Computer and Communications\nSecurity, 2017, pp. 1175–1191.\n12\n[9] J. Koneˇcn`y, H. B. McMahan, D. Ramage, and P. Richtárik, “Federated\noptimization: Distributed machine learning for on-device intelligence,”\narXiv preprint arXiv:1610.02527, 2016.\n[10] S. Kumar, P. Shah, D. Hakkani-Tur, and L. Heck, “Federated control\nwith hierarchical multi-agent deep reinforcement learning,” arXiv\npreprint arXiv:1712.08266, 2017.\n[11] H. H. Zhuo, W. Feng, Q. Xu, Q. Yang, and Y. Lin, “Federated\nreinforcement learning,” arXiv preprint arXiv:1901.08277, 2019.\n[12] G. Palmer, K. Tuyls, D. Bloembergen, and R. Savani, “Lenient multi-\nagent deep reinforcement learning,” arXiv preprint arXiv:1707.04402,\n2017.\n[13] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique\nof multiagent deep reinforcement learning,” Autonomous Agents and\nMulti-Agent Systems, vol. 33, no. 6, pp. 750–797, 2019.\n[14] H.-K. Lim, J.-B. Kim, J.-S. Heo, and Y.-H. Han, “Federated reinforce-\nment learning for training control policies on multiple iot devices,”\nSensors, vol. 20, no. 5, p. 1359, 2020.\n[15] B. Liu, L. Wang, and M. Liu, “Lifelong federated reinforcement\nlearning: a learning architecture for navigation in cloud robotic systems,”\nIEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 4555–4562,\n2019.\n[16] S. Zeng, A. Anwar, T. Doan, J. Romberg, and A. Raychowdhury,\n“A decentralized policy gradient approach to multi-task reinforcement\nlearning,” arXiv preprint arXiv:2006.04338, 2020.\n[17] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,\nand R. Fergus, “Intriguing properties of neural networks,” arXiv preprint\narXiv:1312.6199, 2013.\n[18] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” arXiv preprint arXiv:1412.6572, 2014.\n[19] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine\nlearning at scale,” arXiv preprint arXiv:1611.01236, 2016.\n[20] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and\nP. McDaniel, “Ensemble adversarial training: Attacks and defenses,”\narXiv preprint arXiv:1705.07204, 2017.\n[21] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in\nthe physical world,” arXiv preprint arXiv:1607.02533, 2016.\n[22] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple\nand accurate method to fool deep neural networks,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2016,\npp. 2574–2582.\n[23] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and\nA. Swami, “The limitations of deep learning in adversarial settings,” in\n2016 IEEE European symposium on security and privacy (EuroS&P).\nIEEE, 2016, pp. 372–387.\n[24] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel,\n“Adversarial attacks on neural network policies,” arXiv preprint\narXiv:1702.02284, 2017.\n[25] J. Kos and D. Song, “Delving into adversarial attacks on deep policies,”\narXiv preprint arXiv:1705.06452, 2017.\n[26] Y. Huang and Q. Zhu, “Deceptive reinforcement learning under\nadversarial manipulations on cost signals,” in International Conference\non Decision and Game Theory for Security.\nSpringer, 2019, pp.\n217–237.\n[27] Y. Ma, X. Zhang, W. Sun, and J. Zhu, “Policy poisoning in batch\nreinforcement learning and control,” in Advances in Neural Information\nProcessing Systems, 2019, pp. 14 570–14 580.\n[28] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, and M. Sun,\n“Tactics of adversarial attack on deep reinforcement learning agents,”\narXiv preprint arXiv:1703.06748, 2017.\n[29] V. Behzadan and A. Munir, “The faults in our pi stars: Security issues\nand open challenges in deep reinforcement learning,” arXiv preprint\narXiv:1810.10369, 2018.\n[30] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell,\n“Adversarial policies: Attacking deep reinforcement learning,” arXiv\npreprint arXiv:1905.10615, 2019.\n[31] P. Blanchard, R. Guerraoui, J. Stainer et al., “Machine learning with\nadversaries: Byzantine tolerant gradient descent,” in Advances in Neural\nInformation Processing Systems, 2017, pp. 119–129.\n[32] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing\nfederated learning through an adversarial lens,” in International\nConference on Machine Learning.\nPMLR, 2019, pp. 634–643.\n[33] C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He, “Feature\ndenoising for improving adversarial robustness,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2019,\npp. 501–509.\n[34] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, “Robust adversarial\nreinforcement learning,” arXiv preprint arXiv:1703.02702, 2017.\n[35] A. Mandlekar, Y. Zhu, A. Garg, L. Fei-Fei, and S. Savarese, “Ad-\nversarially robust policy learning: Active construction of physically-\nplausible perturbations,” in 2017 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS).\nIEEE, 2017, pp. 3932–3939.\n[36] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary,\n“Robust deep reinforcement learning with adversarial attacks,” arXiv\npreprint arXiv:1712.03632, 2017.\n[37] C. Tessler, Y. Efroni, and S. Mannor, “Action robust reinforcement\nlearning and applications in continuous control,” arXiv preprint\narXiv:1901.09184, 2019.\n[38] N. Rodríguez-Barroso, E. Martínez-Cámara, M. Luzón, G. G. Seco,\nM. Á. Veganzones, and F. Herrera, “Dynamic federated learning model\nfor identifying adversarial clients,” arXiv preprint arXiv:2007.15030,\n2020.\n[39] R. R. Yager and D. P. Filev, “Induced ordered weighted averaging\noperators,” IEEE Transactions on Systems, Man, and Cybernetics, Part\nB (Cybernetics), vol. 29, no. 2, pp. 141–150, 1999.\n[40] A. Anwar and A. Raychowdhury, “Autonomous navigation via deep\nreinforcement learning for resource constraint edge nodes using transfer\nlearning,” IEEE Access, vol. 8, pp. 26 549–26 560, 2020.\nAqeel Anwar received his Bachelor’s degree in\nElectrical Engineering from the University of Engi-\nneering and Technology (UET), Lahore, Pakistan,\nand Masters degree in Electrical and Computer\nEngineering from Georgia Institute of Technology,\nAtlanta, GA, USA in 2012 and 2017 respectively.\nCurrently, he is pursuing his Ph.D. in Electrical and\nComputer Engineering from the Georgia Institute\nof Technology under the supervision of Dr. Arijit\nRaychowdhury. His research interests lie at the\njunction of machine learning and hardware design.\nHe is working towards shifting Machine Learning (ML) from cloud to edge\nnodes by improving the energy efﬁciency of current state-of-the-art ML\nalgorithms and designing efﬁcient DNN accelerators.\nArijit Raychowdhury (Senior Member, IEEE)\nreceived the Ph.D. degree in electrical and computer\nengineering from Purdue University, West Lafayette,\nIN, USA, in 2007. His industry experience includes\nﬁve years as a Staff Scientist with the Circuits\nResearch Laboratory, Intel Corporation, Portland,\nOR, USA, and a year as an Analog Circuit Re-\nsearcher with Texas Instruments Inc., Bengaluru,\nIndia. He joined the Georgia Institute of Technology,\nAtlanta, GA, USA, in 2013, where he is currently an\nAssociate Professor with the School of Electrical\nand Computer Engineering and also holds an ON Semiconductor Junior\nProfessorship. He holds more than 25 U.S. and international patents and\nhas published over 100 papers in journals and refereed conferences. His\nresearch interests include low-power digital- and mixed-signal circuit design,\ndevice–circuit interactions, and novel computing models and hardware\nrealizations. Dr. Raychowdhury was a recipient of the Dimitris N. Chorafas\nAward for Outstanding Doctoral Research in 2007, the Intel Labs Technical\nContribution Award in 2011, the Best Thesis Award from the College of\nEngineering, Purdue University, in 2007, the Intel Early Faculty Award in\n2015, the NSF CISE Research Initiation Initiative Award (CRII) in 2015,\nand multiple best paper awards and fellowships.\n13\nAPPENDIX\nA. Training details\nPolicy gradient methods for RL is used to train both the\nGridWorld and AutoNav RL problems. For ComA-FedRL,\nwe use a base communication base_comm. In the pre-train\nphase, the communication interval for each agent is assigned\nthis base communication i.e.\ncomm[i] = base_comm\n∀i ∈{0, n −1}\nThis means that in the pre-train phase, the agents learn only on\nlocal data, and after every base_comm number of episodes,\nthe locally learned policies are shared with the server for cross-\nevaluation. This cross-evaluation runs n policies, each on a\nrandomly selected environment and the cumulative reward is\nrecorded. We also take into account the fact that the adversarial\nagent can present a secondary attack in terms of faking the\ncumulative reward that it return when evaluating a policy.\nIn the ComA-FedRL implementation, we assume that the\nadversarial agent returns a cumulative reward of −1, meaning\nthat it fakes the policy being evaluated as adversarial.\nAt the end of the pre-train phase, the cross evaluated\nrewards are used to assign communication intervals to all\nthe agents. There are various choices for the selection of\nthis mapping. The underlying goal is to assign a higher\ncommunication interval for agents whose policy performs\npoorly when cross-evaluated and vice versa. We use the\nmapping shown in Alg. 3. A reward threshold rth is used\nto assign agents different communication intervals. If the\ncumulative reward of a policy in an environment is below rth,\nit is assigned a high communication interval of high_comm\nepisodes (marked as a possible adversary), otherwise it\nis assigned a low communication interval of low_comm\nepisodes (marked as a possible non-adversary). The assigned\ncommunication interval also depends on the one-step history\nof communication intervals. If an agent was previously\nassigned a higher communication interval and is again marked\nas a possible adversary, the communication interval assigned to\nsuch an agent is doubled. The complete list of hyperparameters\nused for GridWorld and AutoNav can be seen in Table IV.\nAlgorithm 3: Update Communication Intervals\nFunction UpdateCommInt(rm×n, comm):\nInitialize low_comm, high_comm, rth\nfor each agent i do\nAverage the rewards across episodes\nravg ←1\nm\nm−1\nX\nj=0\nr[:, i]\nif ravg ≥rth then\ncomm[i] = low_comm\nend\nelse if ravg < rth then\nif comm[i] ̸= low_comm then\ncomm[i] = 2 ∗comm[i]\nend\nelse\ncomm[i] = high_comm\nend\nend\nend\nreturn comm\nHyperParameter\nGridWorld\nAutoNav\nFunctional Mapping\nTabular\nNeural Network\nNumber of agents\n4, 8, 12\n4\nAlgorithm\nREINFORCE\nREINFORCE\nMax Episodes\n1000\n4000\nGamma\n0.95\n0.99\nLearning rate\nVariable\n1e-4\nbase_comm\n8\n8\nwait_train\n600\n1000\nGradient clipping norm\nNone\n0.1\nOptimizer type\nADAM\nADAM\nEntropy Regularizer Scalar\nNone\n0.5\nTraining Workstation\nGTX1080\nGTX1080\nTraining Time\n9 hours\n35 hours\nTABLE IV: Training hyper-parameters for GridWorld and AutoNav\n14\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-03-11",
  "updated": "2021-03-11"
}