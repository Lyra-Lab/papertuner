{
  "id": "http://arxiv.org/abs/2103.07802v1",
  "title": "Hybrid computer approach to train a machine learning system",
  "authors": [
    "Mirko Holzer",
    "Bernd Ulmann"
  ],
  "abstract": "This book chapter describes a novel approach to training machine learning\nsystems by means of a hybrid computer setup i.e. a digital computer tightly\ncoupled with an analog computer. As an example a reinforcement learning system\nis trained to balance an inverted pendulum which is simulated on an analog\ncomputer, thus demonstrating a solution to the major challenge of adequately\nsimulating the environment for reinforcement learning.",
  "text": "March 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 1\nChapter 1\nHybrid computer approach to train a machine learning\nsystem\nMirko Holzer,1 Bernd Ulmann2\nThis chapter describes a novel approach to training machine learning\nsystems by means of a hybrid computer setup i. e. a digital computer\ntightly coupled with an analog computer. In this example, a reinforce-\nment learning system is trained to balance an inverted pendulum which\nis simulated on an analog computer, demonstrating a solution to the ma-\njor challenge of adequately simulating the environment for reinforcement\nlearning.3,4\n1. Introduction\nThe following sections introduce some basic concepts which underpin the\nremaining parts of this chapter.\n1.1. A brief introduction to artiﬁcial intelligence and ma-\nchine learning\nMachine learning is one of the most exciting technologies of our time. It\nboosts the range of tasks that computers can perform to a level which has\nbeen extremely diﬃcult, if not impossible, to achieve using conventional\nalgorithms.\nThanks to machine learning, computers understand spoken language,\noﬀer their services as virtual assistants such as Siri or Alexa, diagnose cancer\n1Mr. Holzer is co-founder and CEO of BrandMaker.\n2Dr. Ulmann is professor for business informatics at the FOM University of Applied\nSciences for Economics and Management in Frankfurt/Main (Germany) and a guest\nprofessor and lecturer at the Institute of Medical Systems Biology at Ulm University.\n3The analog/hybrid approach to this problem has also been described in [Ulmann (2020),\nsec. 6.24/7.4] with a focus on the analog computer part.\n4The authors would like to thank Dr. Chris Giles and Dr. David Farag´o for proof\nreading and making many invaluable suggestions and corrections which greatly enhanced\nthis chapter.\n1\narXiv:2103.07802v1  [cs.LG]  13 Mar 2021\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 2\n2\nM. Holzer, B. Ulmann\nfrom magnetic resonance imaging (MRI), drive cars, compose music, paint\nartistic pictures, and became world champion in the board game Go.\nThe latter feat is even more impressive when one considers that Go\nis probably one of the most complex game ever devised5; for example it\nis much more complex than chess. Many observers in 1997 thought that\nIBM’s Deep Blue, which defeated the then world chess champion Garry\nKasparov, was the ﬁrst proof-of-concept for Artiﬁcial Intelligence. How-\never, since chess is a much simpler game than Go, chess-playing computer\nalgorithms can use try-and-error methods to evaluate the best next move\nfrom the set of all possible sensible moves. In contrast, due to the 10360\npossible game paths of Go, which is far more than the number of atoms in\nthe universe, it is impossible to use such simple brute-force computer algo-\nrithms to calculate the best next move. This is why popular belief stated\nthat it required human intuition, as well as creative and strategic thinking,\nto master Go – until Google’s AlphaGo beat 18-time world champion Lee\nSedol in 2016.\nHow could AlphaGo beat Lee Sedol? Did Google develop human-like\nArtiﬁcial Intelligence with a masterly intuition for Go and the creative and\nstrategic thinking of a world champion? Far from it. Google’s AlphaGo\nrelied heavily on a machine learning technique called reinforcement learning,\nRL for short, which is at the center of the novel hybrid analog/digital\ncomputing approach introduced in this chapter.\nThe phrase Artiﬁcial Intelligence (AI ) on the other hand, is nowadays\nheavily overused and frequently misunderstood. At the time of writing in\nearly 2020, there exists no AI in the true sense of the word. Three ﬂavors\nof AI are commonly identiﬁed:\nArtiﬁcial Narrow Intelligence (ANI): Focused on one narrow task\nsuch as playing Go, performing face recognition, or deciding the\ncredit rating of a bank’s customer. Several ﬂavors of machine learn-\ning are used to implement ANI.\nArtiﬁcial General Intelligence (AGI): Computers with AGI would be\nequally intelligent to humans in every aspect and would be capa-\nble of performing the same kind of intellectual tasks that humans\nperform with the same level of success. Currently it is not clear\nif machine learning as we know it today, including Deep Learning,\ncan ever evolve into AGI or if new approaches are required for this.\n5We are referring here to games with complete information, whereas games with incom-\nplete information, such as Stratego, are yet to be conquered by machine learning.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 3\nHybrid computer approach to train a machine learning system\n3\nArtiﬁcial Super Intelligence (ASI): Sometimes\ncalled\n“humanity’s\nlast invention”, ASI is often deﬁned as an intellect that is supe-\nrior to the best human brains in practically every ﬁeld, including\nscientiﬁc creativity, general wisdom, and social skills. From today’s\nperspective, ASI will stay in the realm of science ﬁction for many\nyears to come.\nMachine learning is at the core of all of today’s AI/ANI eﬀorts. Two fun-\ndamentally diﬀerent categories of machine learning can be identiﬁed today:\nThe ﬁrst category, which consists of supervised learning and unsupervised\nlearning, performs its tasks on existing data sets. As described in [Domin-\ngos (2012), p. 85] in the context of Supervised Learning:\n“A dumb algorithm with lots and lots of data beats a\nclever one with modest amounts of it. (After all, machine\nlearning is all about letting data do the heavy lifting.)”\nReinforcement learning constitutes the second category. It is inspired by\nbehaviorist psychology and does not rely on data. Instead, it utilizes the\nconcept of a software agent that can learn to perform certain actions, which\ndepend on a given state of an environment, in order to maximize some\nkind of long-term (cumulative) reward. Reinforcement learning is very well\nsuited for all kinds of control tasks, as it does not need sub-optimal actions\nto be explicitly corrected. The focus is on ﬁnding a balance between explo-\nration (of uncharted territory) and exploitation (of current knowledge).\nHere are some typical applications of machine learning arranged by\nparadigm:\nSupervised learning: classiﬁcation (image classiﬁcation, customer re-\ntention, diagnostics) and regression (market forecasting, weather\nforecasting, advertising popularity prediction)\nUnsupervised learning: clustering\n(recommender systems,\ncustomer\nsegmentation, targeted marketing) and dimensionality reduction\n(structure discovery, compression, feature elicitation)\nReinforcement learning: robot navigation, real-time decisions, game\nAI, resource management, optimization problems\nFor the sake of completeness, (deep) neural networks need to be men-\ntioned, even though they are not used in this chapter. They are a very\nversatile family of algorithms that can be used to implement all three of su-\npervised learning, unsupervised learning and reinforcement learning. When\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 4\n4\nM. Holzer, B. Ulmann\nreinforcement learning is implemented using deep neural networks then the\nterm deep reinforcement learning is used. The general public often identiﬁes\nAI or machine learning with neural networks; this is a gross simpliﬁcation.\nIndeed there are plenty of other approaches for implementing the diﬀerent\nparadigms of machine learning as described in [Domingos (2015)].\n1.2. Analog vs. digital computing\nAnalog and digital computers are two fundamentally diﬀerent approaches\nto computation. A traditional digital computer, more precisely a stored pro-\ngram digital computer, has a ﬁxed internal structure, consisting of a control\nunit, arithmetic units etc., which are controlled by an algorithm stored as\na program in some kind of memory. This algorithm is then executed in\na basically stepwise fashion. An analog computer, in contrast, does not\nhave a memory at all and is not controlled by an algorithm. It consists of\na multitude of computing elements capable of executing basic operations\nsuch as addition, multiplication, integration (sic!) etc. These computing\nelements are then interconnected in a suitable way to form an analogue, a\nmodel, of the problem to be solved.\nSo while a digital computer has a ﬁxed internal structure and a vari-\nable program controlling its overall operation, an analog computer has a\nvariable structure and no program in the traditional sense at all. The big\nadvantage of the analog computing approach is that all computing elements\ninvolved in an actual program are working in full parallelism with no data\ndependencies, memory bottlenecks etc. slowing down the overall operation.\nAnother diﬀerence is that values within an analog computer are typ-\nically represented as voltages or currents and thus are as continuous as\npossible in the real world.6 Apart from continuous value representation,\nanalog computers feature integration over time-intervals as one of their\nbasic functions.\nWhen it comes to the solution or simulation of systems described by\ncoupled diﬀerential equations, analog computers are much more energy ef-\nﬁcient and typically also much faster than digital computers. On the other\nhand, the generation of arbitrary functions (e. g. functions, which can not\nbe obtained as the solution of some diﬀerential equation), complex logic\nexpressions, etc. are hard to implement on an analog computer. So the\n6There exist digital analog computers, which is not the contradiction it might appear to\nbe. They diﬀer from classical analog computers mainly by their use of a binary value\nrepresentation.\nMachines of this class are called DDAs, short for Digital Diﬀerential\nAnalyzers and are not covered here.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 5\nHybrid computer approach to train a machine learning system\n5\nidea of coupling a digital computer with an analog computer, yielding a\nhybrid computer, is pretty obvious. The analog computer basically forms\na high-performance co-processor for solving problems based on diﬀerential\nequations, while the digital computer supplies initial conditions, coeﬃcients\netc. to the analog computer, reads back values and controls the overall op-\neration of the hybrid system.7\n1.3. Balancing an inverse pendulum using reinforcement\nlearning\nReinforcement learning is particularly well suited for an analog/digital hy-\nbrid approach because the RL agent needs a simulated or real environment\nin which it can perform, and analog computers excel in simulating multi-\ntudes of scenarios. The inverse pendulum, as shown in ﬁgure 3, was chosen\nfor the validation of the approach discussed in this chapter because it is\none of the classical “Hello World” examples of reinforcement learning and\nit can be easily simulated on small analog computers.\nThe hybrid computer setup consists of the analog computer shown in\nﬁgure 1 that simulates the inverse pendulum and a digital computer running\na reinforcement learning algorithm written in Python.\nBoth systems communicate using serial communication over a USB con-\nnection. Figure 2 shows the overall setup: The link between the digital\ncomputer on the right hand side and the analog computer is a hybrid con-\ntroller, which controls all parts of the analog computer, such as the in-\ntegrators, digital potentiometers for setting coeﬃcients etc. This hybrid\ncontroller receives commands from the attached digital computer and re-\nturns values read from selected computing elements of the analog computer.\nThe simulation and the learning algorithm both run in real-time8.\nReinforcement learning takes place in episodes. One episode is deﬁned\nas “balance the pendulum until it falls over or until the cart moves outside\nthe boundaries of the environment”. The digital computer asks the analog\ncomputer for real-time simulation information such as the cart’s x position\nand the pendulum’s angle ϕ. The learning algorithm then decides if the\ncurrent episode, and therefore the current learning process, can continue\nor if the episode needs to be ended; ending the episode also resets the\nsimulation running on the analog computer.\n7More details on analog and hybrid computer programming can be found in [Ulmann\n(2020)].\n8As can be seen in this video: https://youtu.be/jDGLh8YWvNE\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 6\n6\nM. Holzer, B. Ulmann\nFig. 1.\nAnalog computer setup\nAnalog computer\nHybrid controller\nDigital computer\nmode control\nanalog readout\ndigital outputs\ndigital inputs\ndigital pot. ctrl.\nUSB\nFig. 2.\nBasic structure of the hybrid computer\n2. The analog simulation\nSimulating an inverted pendulum mounted on a cart with one degree of free-\ndom, as shown in ﬁgure 3, on an analog computer is quite straightforward.\nThe pendulum mass m is assumed to be mounted on top of a mass-less pole\nwhich in turn is mounted on a pivot on a cart which can be moved along\nthe horizontal axis. The cart’s movement is controlled by applying a force\nF to the left or right side of the cart for a certain time-interval δt. If the\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 7\nHybrid computer approach to train a machine learning system\n7\ncart couldn’t move, the pendulum would resemble a simple mathematical\npendulum described by\n¨ϕ −g\nl sin(ϕ) = 0\nwhere ¨ϕ is the second derivative of the pendulums angle ϕ with respect to\ntime.9\nM\nF\nm\nl\nmg\nx\ny\n0\nlcos(ϕ)\nϕ\nlsin(ϕ)\nFig. 3.\nConﬁguration of the inverted pendulum\nSince in this example the cart is non-stationary, the problem is much\nmore complex and another approach has to be taken.\nIn this case, the\nLagrangian10\nL = T −V\nis used, where T and V represent the total kinetic and potential energy of\nthe overall system. With g representing the gravitational acceleration the\npotential energy is\nV = mgl cos(ϕ),\nthe height of the pendulum’s mass above the cart’s upper surface.\n9In engineering notations like ˙ϕ =\ndϕ\ndt , ¨ϕ =\nd2ϕ\ndt2\netc.\nare commonly used to denote\nderivatives with respect to time.\n10This approach, which lies at the heart of Lagrangian mechanics, allows the use of\nso-called generalized coordinates like, in this case, the angle ϕ and the cart’s position\nx instead of the coordinates in classic Newtonian mechanics.\nMore information on\nLagrangian mechanics can be found in [Brizard (2008)].\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 8\n8\nM. Holzer, B. Ulmann\nThe kinetic energy is the sum of the kinetic energies of the pendulum\nbob with mass m and the moving cart with mass M:\nT = 1\n2\n\u0000Mv2\nc + mv2\np\n\u0001\n,\nwhere vc and vp represent the velocities of the cart and the pendulum,\nrespectively. With x denoting the horizontal position of the cart, the cart’s\nvelocity vc is just the ﬁrst derivative of its position x with respect to time:\nvc = ˙x\nThe pendulum’s velocity is a bit more convoluted since the velocity of\nthe pendulum mass has two components, one along the x- and one along\nthe y-axis forming a two-component vector. The scalar velocity is then the\nEuclidean norm of this vector, i. e.\nvp =\ns\u0012 d\ndt (x −l sin(ϕ))\n\u00132\n+\n\u0012 d\ndt (l cos(ϕ))\n\u00132\n.\nThe two components under the square root are\n\u0012 d\ndt (x −l sin(ϕ))\n\u00132\n= ( ˙x −l ˙ϕ cos(ϕ))2 = ˙x2 −2 ˙xl ˙ϕ cos(ϕ) + l2 ˙ϕ2 cos2(ϕ)\nand\n\u0012 d\ndt (l cos(ϕ))\n\u00132\n= (−l ˙ϕ sin(ϕ))2 = l2 ˙ϕ2 sin2(ϕ)\nresulting in\nvp = ˙x2 −2 ˙xl ˙ϕ cos(ϕ) + l2 ˙ϕ2 cos2(ϕ) + l2 ˙ϕ2 sin2(ϕ)\n= ˙x2 −2 ˙x ˙ϕl cos(ϕ) + l2 ˙ϕ2.\nThis ﬁnally yields the Lagrangian\nL = 1\n2M ˙x2 + 1\n2\n\u0000˙x2 −2 ˙x ˙ϕl cos(ϕ) + l2 ˙ϕ2\u0001\n−mgl cos ϕ\n= 1\n2(M + m) ˙x2 + m ˙x ˙ϕl cos(ϕ) + 1\n2ml2 ˙ϕ2 −mgl cos(ϕ).\nAs a next step, the Euler-Lagrange-equations\nd\ndt\n\u0012∂L\n∂˙x\n\u0013\n−∂L\n∂x = F and\n(1)\nd\ndt\n\u0012∂L\n∂˙ϕ\n\u0013\n−∂L\n∂ϕ = 0\n(2)\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 9\nHybrid computer approach to train a machine learning system\n9\nare applied.\nThe ﬁrst of these equations requires the following partial\nderivatives\n∂L\n∂x = 0\n∂L\n∂˙x = (M + m) ˙x −ml ˙ϕ cos(ϕ)\nd\ndt\n\u0012∂L\n∂˙x\n\u0013\n= (M + m)¨x −ml ¨ϕ cos(ϕ) + ml ˙ϕ2 sin(ϕ)\nwhile the second equations relies on these partial derivatives:\n∂L\n∂ϕ = ml ˙x ˙ϕ sin(ϕ) + mgl sin(ϕ)\n∂L\n∂˙ϕ = −ml ˙x cos(ϕ) + ml2 ˙ϕ\nd\ndt\n\u0012∂L\n∂˙ϕ\n\u0013\n= −ml¨x cos(ϕ) + ml ˙x ˙ϕ sin(ϕ) + ml2 ¨ϕ\nSubstituting these into equations (1) and (2) yields the following two\nEuler-Lagrange-equations:\nd\ndt\n\u0012∂L\n∂˙x\n\u0013\n= (M + m)¨x −ml ¨ϕ cos(ϕ) + ml ˙ϕ2 sin(ϕ) = F\n(3)\nand\nd\ndt\n\u0012∂L\n∂˙ϕ\n\u0013\n= −ml¨x cos(ϕ)+ml ˙x ˙ϕ sin(ϕ)+ml2 ¨ϕ−ml ˙x ˙ϕ sin(ϕ)−mgl sin(ϕ) = 0.\nDividing this last equation by ml and solving for ¨ϕ results in\n¨ϕ = 1\nl (¨x cos(ϕ) + g sin(ϕ))\nwhich can be further simpliﬁed to\n¨ϕ = ¨x cos(ϕ) + g sin(ϕ)\n(4)\nby assuming the pendulum having ﬁxed length l = 1.\nThe two ﬁnal equations of motion (3) and (4) now fully describe the\nbehaviour of the inverted pendulum mounted on its moving cart, to which\nan external force F may be applied in order to move the cart and therefore\nthe pendulum due to its inertia.\nTo simplify things further, it can be reasonably assumed that the mass\nm of the pendulum bob is negligible compared with the cart’s mass M, so\nthat (3) can be rewritten as\nM ¨x = F.\n(5)\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 10\n10\nM. Holzer, B. Ulmann\nThis simpliﬁcation comes at a cost: the movement of the pendulum\nbob no longer inﬂuences the cart, as it would have been the case with a\nnon-negligible pendulum mass m.\nNevertheless, this is not a signiﬁcant\nrestriction and is justiﬁed by the the resulting simpliﬁcation of the analog\ncomputer program shown in ﬁgure 4.\nAs stated before, an analog computer program is basically an inter-\nconnection scheme specifying how the various computing elements are to\nbe connected to each other. The schematic makes use of several standard\nsymbols:\n• Circles with inscribed values represent coeﬃcients.\nTechnically\nthese are basically variable voltage dividers, so a coeﬃcient must\nalways lie in the interval [0, 1]. The input value ¨x (the force applied\nto the cart in order to move it on the x-axis) is applied to two such\nvoltage dividers, each set to values γ1 and γ2.\n• Triangles denote summers, yielding the sum of all of its inputs at\nits output. It should be noted that summers perform an implicit\nsign inversion, so feeding two values −h and h sin(ωt) to a summer\nas shown in the right half of the schematic, yields an output signal\nof −(h sin(ωt) −h) = h(1 −sin(ωt)).\n• Symbols labelled with +Π denote multipliers while those labelled\nwith cos(. . . ) and sin(. . . ) respectively are function generators.\n• Last but not least, there are integrators denoted by triangles with a\nrectangle attached to one side. These computing elements yield the\ntime integral over the sum of their respective input values. Just as\nwith summers, integrators also perform an implicit sign-inversion.\nTransforming the equations of motion (4) and (5) into an analog com-\nputer program is typically done by means of the Kelvin feedback tech-\nnique.11\nThe tiny analog computer subprogram shown in ﬁgure 5 shows how the\nforce applied to the cart is generated in the hybrid computer setup. The\ncircuit is controlled by two digital output signals from the hybrid controller,\nthe interface between the analog computer and its digital counterpart. Ac-\ntivating the output D0 for a short time interval δt generates a force impulse\nresulting in an acceleration ¨x of the cart. The output D1 controls the direc-\ntion of ¨x, thus allowing the cart to be pushed to the left or the right. Both\nof these control signals are connected to electronic switches which are part\n11More information on this can be found in classic text books on analog computing or\nin [Ulmann (2020)].\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 11\nHybrid computer approach to train a machine learning system\n11\n¨x\nγ1\nγ1\nβx\n−˙x\nx\n−x\ncos(ϕ)\ng\nβϕ\nsin(ϕ)\n˙ϕ\n−ϕ\ncos(. . . )\nsin(. . . )\nh\nh\n−1\nsin(ωt)\ny\nx\nFig. 4.\nModiﬁed setup for the inverted pendulum\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 12\n12\nM. Holzer, B. Ulmann\nof the analog computer.\nThe outputs of these two switches are then fed to a summer which\nalso takes a second input signal from a manual operated SPDT (single\npole double throw) switch.\nThis switch is normally open and makes it\npossible for an operator to manually disturb the cart, thereby unbalancing\nthe pendulum. It is interesting to see how the RL system responds to such\nexternal disturbances.\npush the cart manually\n−1\n+1\nD0\nD1\n+1\n−1\n¨x\nFig. 5.\nControl circuit for the controlled inverted pendulum\n3. The reinforcement learning system\nReinforcement learning, as deﬁned in [Sutton (2018), sec. 1.1],\n“is learning what to do — how to map situations to\nactions — so as to maximize a numerical reward signal.\nThe learner (agent) is not told which actions to take, but\ninstead must discover which actions yield the most reward\nby trying them. In the most interesting and challenging\ncases, actions may aﬀect not only the immediate reward but\nalso the next situation and, through that, all subsequent re-\nwards. These two characteristics — trial-and-error search\nand delayed reward — are the two most important distin-\nguishing features of reinforcement learning.”\nIn other words, reinforcement learning utilizes the concept of an agent\nthat can learn to perform certain actions depending on a given state of an\nenivronment in order to maximize some kind of long-term (delayed) reward.\nFigure 612 illustrates the interplay of the components of a RL system: In\n12Source\nhttps://en.wikipedia.org/wiki/Reinforcement_learning,\nretrieved\nJan.\n9th, 2020.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 13\nHybrid computer approach to train a machine learning system\n13\neach episode13, the agent observes the state s ∈S of the system: position x\nof the cart, speed ˙x, angle ϕ of the pole and angular velocity ˙ϕ. Depending\non the state, the agent performs an action that modiﬁes the environment.\nThe outcome of the action determines the new state and the short-term\nreward, which is the main ingredient in ﬁnding a value function that is able\nto predict the long-term reward.\nEnvironment\nReward\nInterpreter\nAgent\nAction\nFig. 6.\nReinforcement learning\nFigure 7 translates these abstract concepts into the concrete use-case of\nthe inverted pendulum. The short-term reward is just a means to an end for\nﬁnding the value function: Focusing on the short-term reward would only\nlead to an ever-increasing bouncing of the pendulum, equal to the control\nalgorithm diverging. Instead, using the short-term reward to approximate\nthe value function that can predict the long-term reward leads to a robust\ncontrol algorithm (convergence).\n3.1. Value function\nRoughly speaking, the state value function V estimates “how benﬁcial”\nit is to be in a given state. The action value function Q speciﬁes “how\nbeneﬁcial” it is to take a certain action while being in a given state. When\nit comes for an agent to choose the next best action a, one straightforward\npolicy π is to evaluate the action value function Q(s, a) for all actions a that\n13See also section 1.3 for a deﬁnition of episode.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 14\n14\nM. Holzer, B. Ulmann\nAgent focuses on short term reward:\nlarge F < 0\n−−−−−−−−−−−→\nrshort(si,ai)=max\nF =0\n−−−−−−−−−−−−−−→\nrshort(si+1,ai+1)=max\n. . .\nThe pendulum bounces\nback and forth with ever\nincreasing angle\nAgent focuses on long term reward:\nsmall F < 0\n−−−−−−−−−−−→\nrshort(si,ai)<max\neven smaller F < 0\n−−−−−−−−−−−−−−→\nrshort(si+1,ai+1)<max\n. . .\nThe pendulum bounces\nback and forth with ever\ndecreasing angle\nFig. 7.\nRewards in the use-case of the inverted pendulum\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 15\nHybrid computer approach to train a machine learning system\n15\nare possible while being in state s and to choose the action abest yielding\nthe maximum value of the action value function:\nπ ←always take argmaxa(Q(s, a)) as next action\nA policy like this is also called a greedy policy.\nThe value function Q for a certain action a given state s is deﬁned\nas the expected future reward R14 that can be obtained when the agent\nis following the current policy π.\nThe future reward is the sum of the\ndiscounted rewards r for each step t beginning from “now” (s0 = s at\nt = 0) and ranging to the terminal state15. γ ∈[0, 1] is implemented as\na discount factor by exponentiating it by the number of elapsed steps t in\nthe current training episode.\nQ(s, a) = E[R(s, a)] = E\n\" ∞\nX\nt=0\nγtrt\n\f\f\f\ns0=s,a0=a\n#\nTwo important observations regarding the above-mentioned deﬁnition\nof Q are:\n(1) In step t = 0 given the state s0, the reward r0 only depends on the ac-\ntion a0. But all subsequent rewards rt\n\f\f\nt>0 are depending on all actions\nthat have been previously taken.\n(2) π and Q(s, a) are not deﬁned recursively, but iteratively.\nFollowing the policy π as deﬁned here is trivial if Q is given since all that\nneeds to be done in each step is to calculate argmaxa(Q(s, a)). So the main\nchallenge of reinforcement learning is to calculate the state value function\nVπ(s) and then derive the action value function Q from it or to determine\nQ directly. But often, V or Q cannot be calculated directly due to CPU or\nmemory constraints. For example the state space S = {(x, ˙x, ϕ, ˙ϕ) ∈R4}\nused here is inﬁnitely large. And even if it is discretized, the resulting com-\nbinatorial explosion still results in an enormous state space. So it cannot\nbe expected to ﬁnd an optimal value function in the limit of ﬁnite CPU\nand memory resources as shown in [Sutton (2018), Part II]. Instead, the\ngoal is to ﬁnd a good approximate solution that can be calculated relatively\nquickly.\n14The future reward R in this chapter equates the long-term reward in the previous\nchapter.\n15The reward in the terminal state is deﬁned to be zero, so that the inﬁnite sum over\nbounded rewards always yields a ﬁnite value.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 16\n16\nM. Holzer, B. Ulmann\n3.2. Q-learning algorithm\nThere are several approaches to ﬁnd the (action) value function Q. The\nimplementation described here uses the Q-learning algorithm, which is a\nmodel-free reinforcement learning algorithm. “Model-free” means, that it\nactually “does not matter for the algorithm”, what the semantics of the\nparameters/features of the environment are. It “does not know” the mean-\ning of a feature such as the cart position x or the pole’s angular velocity\n˙ϕ. For the Q-learning algorithm, the set of the features (x, ˙x, ϕ, ˙ϕ) is just\nwhat the current state within the environment comprises. And the state\nenables the agent to decide which action to perform next.\nChris Watkins introduced Q-learning in [Watkins (1989)] and in [Sutton\n(2018), ch. 6].\nQ-learning is deﬁned as a variant of temporal-diﬀerence\nlearning (TD):\n“TD learning is a combination of Monte Carlo ideas and dy-\nnamic programming (DP) ideas. Like Monte Carlo methods, TD\nmethods can learn directly from raw experience without a model of\nthe environment’s dynamics. Like DP, TD methods update esti-\nmates based in part on other learned estimates, without waiting for\na ﬁnal outcome (they bootstrap).”\nOne of the most important properties of Q-learning is the concept of\nexplore vs. exploit: The learning algorithm alternates between exploiting\nwhat it already knows and exploring unknown territory. Before taking a\nnew action, Q-learning evaluates a probability ε which decides, if the next\nstep is a random move (explore) or if the next move is chosen according to\nthe policy π (exploit). As described in section 3.1, in the context of this\nchapter the policy π is a greedy policy that always chooses the best possible\nnext action, i.e. argmaxa(Q(s, a)).\nAlgorithm 1 is a temporal diﬀerence learner based on Bellman’s equation\nand is inspired by [Sutton (2018), sec. 6.5]. The state space of the inverse\npendulum is denoted as S = {(x, ˙x, ϕ, ˙ϕ) ∈R4} and A : S →2A describes\nall valid16 actions for all valid states.\nIn plain english, the algorithm can be described as follows. For each step\nwithin an episode: Decide, if the next action shall explore new territory or\nexploit knowledge, that has already been learned. After this decision: Take\nthe appropriate action a, which means that the reward r for this action is\n16It is possible that not all actions that are theoretically possible in an environment are\nvalid in all given states.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 17\nHybrid computer approach to train a machine learning system\n17\nAlgorithm 1 Q-learning\n1: procedure Find Action Value Function Q\n2:\nα ←step size aka learning rate ∈(0, 1]\n3:\nγ ←discount rate ∈(0, 1]\n4:\nε ←probability to explore, small and > 0\n5:\nInitialize Q(s, a) ∀s ∈S, a ∈A(s), arbitrarily\n6:\nexcept that Q(terminal state, ·) = 0\n7: Loop for each episode:\n8:\nInitialize s\n9:\nwhile s ̸= terminal state do\n10:\nif random number < ε then\n11:\na ←random action ∈A(s)\n▷means: Explore\n12:\nelse\n13:\na ←argmaxa(Q(s, a))\n▷means: Exploit\n14:\nTake action a\n15:\nr ←reward for taking action a while in state s\n16:\ns′ ←state that follows due to taking action a\n17:\nabest ←argmaxa(Q(s′, a))\n18:\nQ(s, a) ←Q(s, a) + α[r + γ ∗Q(s′, abest) −Q(s, a)]\n19:\ns ←s′\ncollected and the system enters a new state s′ (see ﬁgure 6). After that:\nUpdate the action value function Q(s, a) with an estimated future reward\n(line 18 of algorithm 1), but discount the estimated future reward with the\nlearning rate α.\nIn machine learning in general the learning rate is a very important pa-\nrameter, because it makes sure, that new knowledge that has been obtained\nin the current episode does not completely “overwrite” past knowledge.\nThe calculation of the estimated future reward is obviously where the\nmagic of Q-learning is happening:\nQ(s, a) ←Q(s, a) + α[r + γ ∗Q(s′, abest) −Q(s, a)]\nQ-learning estimates the future reward by taking the short-term reward r\nfor the recently taken action a and adding the discounted delta between\nthe best possible outcome of a hypothetical next action abest (given the\nnew state s′) and the old status quo Q(s, a). It is worth mentioning that\nadding the best possible outcome of s′ aka abest = argmaxa(Q(s′, a)) can\nonly be a guess, because it is unclear at the time of this calculation if action\nabest is ever being taken due to the “explore vs. exploit” strategy. Still,\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 18\n18\nM. Holzer, B. Ulmann\nit seems logical that if Q(s, a) claims to be a measure of “how beneﬁcial”\na certain action is, that “the best possible future” abest from this starting\npoint onwards is then taken into consideration when estimating the future\nreward.\nThe discount rate γ takes into consideration that the future is not pre-\ndictable and therefore future rewards cannot be fully counted on. In other\nwords, γ is a parameter that balances the RL agent between the two oppo-\nsite poles “very greedy and short-sighted” and “decision making based on\nextremely long-term considerations.” As such, γ is one of many so called\nhyper parameters; see also section 3.3.4 et seq.\nGiven the size of S it becomes clear that many episodes are needed to\ntrain a system using Q-learning. A rigorous proof that Q-learning works\nand that the algorithm converges is given in [Watkins (1992)].\nWhen implementing Q-learning, a major challenge is how to represent\nthe function Q(s, a).\nFor small to medium sized problems that can be\ndiscretized with reasonable eﬀort, tabular representations are appropriate.\nBut as soon as the state space S is large, as in the case of the inverse\npendulum, other representations need to be found.\n3.3. Python implementation\nThe ideas of this chapter culminate in a hybrid analog/digital machine\nlearning implementation that uses reinforcement learning, more speciﬁcally\nthe Q-learning algorithm, in conjunction with linear regression, to solve the\nchallenge of balancing the inverse pendulum.\nThis somewhat arbitrary choice by the authors is not meant to claim\nthat using RL and Q-learning is the only or at least optimal way of balanc-\ning the inverse pendulum. On the contrary, it is actually more circuitous\nthan many other approaches. Experiments performed by the authors in\nPython have shown that a simple random search over a few thousand tries\nyields such ⃗θ = (θ1, θ2, θ3, θ4), that the result of the dot-multiplication with\nthe current state of the simulation ⃗s = (x, ˙x, ϕ, ˙ϕ) can reliably decide if the\ncart should be pushed to the left or to the right, depending, on whether or\nnot ⃗θ · ⃗s is larger than zero.\nSo the intention of this chapter is something else. The authors wanted\nto show that a general purpose machine learning algorithm like Q-learning,\nimplemented as described in this chapter, is suitable without change for\na plethora of other (and much more complex) real world applications and\nthat it can be eﬃciently trained using a hybrid analog/digital setup.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 19\nHybrid computer approach to train a machine learning system\n19\nPython oﬀers a vibrant Open Source ecosystem of building-blocks for\nmachine learning, from which scikit-learn, as described in [Pedregosa\n(2011)], was chosen as the foundation of our implementation17.\n3.3.1. States\nEach repetition of the While loop in line 9 of algorithm 1 represents one\nstep within the current iteration. The While loop runs until the current\nstate s ∈S of the inverse pendulum’s simulation reaches a terminal state.\nThis is when the simulation of one episode ends, the simulation is reset and\nthe next episode begins. The terminal state is deﬁned as the cart reaching a\ncertain position |x| which can be interpreted as “the cart moved too far and\nleaves the allowed range of movements to the left or to the right or bumps\ninto a wall”. Another trigger for the terminal state is that the angle |ϕ| of\nthe pole is greater than a certain predeﬁned value, which can be interpreted\nas “the pole falls over”.\nIn Python, the state s ∈S is represented as a standard Python Tuple of\nFloats containing the four elements (x, ˙x, ϕ, ˙ϕ) that constitute the state.\nDuring each step of each episode, the analog computer is queried for the\ncurrent state using the hc get sim state() function (see also sec. 3.4).\nOn the analog computer, the state is by the nature of analog computers a\ncontinuous function. As described in [Wiering (2012), pp. 3–42] an environ-\nment in reinforcement learning is typically stated in the form of a Markov\ndecision process (MDP), because Q-learning and many other reinforcement\nlearning algorithms utilize dynamic programming techniques. As an MDP\nis a discrete time stochastic control process, this means that for the Python\nimplementation, we need to discretize the continuous state representation\nof the analog computer.\nConveniently, this is happening automatically,\nbecause the repeated querying inside the While loop mentioned above is\nnothing else than sampling and therefore a discretization of the continuous\n(analog) state.18\n17Full source code on GitHub: https://git.io/Jve3j\n18Due to the partially non-deterministic nature of the Python code execution, the sam-\npling rate is not guaranteed to be constant. The slight jitter introduced by this phe-\nnomenon did not notably impede the Q-learning.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 20\n20\nM. Holzer, B. Ulmann\n3.3.2. Actions\nIn theory, one can think of an inﬁnite19 number of actions that can be\nperformed on the cart on which the inverted pendulum is mounted. As\nthis would complicate the implementation of the Q-learning algorithm, the\nfollowing two simpliﬁcations where chosen while implementing actions in\nPython:\n• There are only two possible actions a ∈{0, 1}: “Push the cart to the\nleft” and “push the cart to the right”.\n• Each action a is allowed in all states s ∈S\nThe actions a = 0 and a = 1 are translated to the analog computer simu-\nlation by applying a constant force from the right (to push the cart to the\nleft) or the other way round for a deﬁned constant period of time. The\nmagnitude of the force that is being applied is conﬁgured as a constant in-\nput for the analog computer using a potentiometer, as described in section\n2. The constant period of time for which the force is being applied can be\nconﬁgured in the Python software module using HC IMPULSE DURATION.\n3.3.3. Modeling the action value function Q(s, a)\nA straightforward way of modeling the action value function in Python\ncould be to store Q(s, a) in a Python Dictionary, so that the Python\nequivalent of line 18 in algorithm 1 would look like this:\ndictionary.py\n1\nQ_s_a = {} #create a dictionary to represent Q(s, a)\n2\n3\n[...]\n#perform the Q-learning algorithm\n4\n5\n#learn by updating Q(s, a) using learning rate alpha\n6\nQ_s_a[((x, xdot, phi, phidot), a)] = old_q_s_a +\n7\nalpha * predicted_reward\ndictionary.py\nThere are multiple problems with this approach, where even the ob-\nviously huge memory requirement for trying to store s ∈S in a tabular\nstructure is not the largest one. An even greater problem is that Python’s\nsemantics of Dictionaries are not designed to consider similarities. In-\nstead, they are designated as key-value pairs. For a Python dictionary\nthe two states s1 = (1.0, 1.0, 1.0, 1.0) and s2 = (1.0001, 1.0, 1.0, 1.0) are\n19Example: Push the cart from the left with force 1, force 2, force 3, force 4, . . .\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 21\nHybrid computer approach to train a machine learning system\n21\ncompletely diﬀerent and not at all similar. In contrast, for a control algo-\nrithm that balances a pendulum, a cart being at x-position 1.0 is in a very\nsimilar situation (state) to a cart being at x-position 1.0001. So Python’s\nDictionaries cannot be used to model Q(s, a). Also, trying to use other\ntabular methods such as Python’s Lists, creates many other challenges.\nThis is why linear regression has been chosen to represent and model\nthe function Q(s, a). As we only have two possible states a ∈{0, 1}, no\ngeneral purpose implementation of Q(s, a) has been done. Instead, two dis-\ncrete functions Qa=0(s) and Qa=1(s) are modeled using a linear regression\nalgorithm from scikit-learn called SGDRegressor20, which is capable of\nperforming online learning. In online machine learning, data becomes avail-\nable step by step and is used to update the best predictor for future data\nat each step as opposed to batch learning, where the best predictor is gen-\nerated by using the entire training data set at once.\nGiven m input features fi (i = 1 . . . m) and corresponding coeﬃcients\nθi (i = 0 . . . m), linear regression can predict ˆy as follows:\nˆy = θ0 + θ1f1 + θ2f2 + · · · + θmfm\nThe whole point of SGDRegressor is to iteratively reﬁne the values for all\nθi, as more and more pairs of [ˆy, fi (i = 1 . . . m)] are generated during\neach step of each episode of the Q-learning algorithm with more and more\naccuracy due to the policy iteration. Older pairs are likely less accurate\nestimates and are therefore discounted.\nA natural choice of features for linear regression would be to set m = 4\nand to use the elements of the state s ∈S as the input features f of the\nlinear regression:\nQa=n(s) = θ0 + θ1x + θ2 ˙x + θ3ϕ + θ4 ˙ϕ\nExperiments performed during the implementation have shown that this\nchoice of features does not produce optimal learning results, as it would\nlead to underﬁtting, i.e. the learned model makes too rough predictions\nfor the pendulum to be balanced.\nThe next section 3.3.4 explains this\nphenomenon.\nSGDRegressor oﬀers built-in mechanisms to handle the learning rate α.\nWhen constructing the SGDRegressor object, α can be directly speciﬁed as\na parameter. Therefore, line 18 of algorithm 1 on page 17 is simpliﬁed in\nthe Python implementation, as ”the α used in the Q-learning algorithm”\nand ”the α used inside the SGDRegressor” are semantically identical: The\n20SGD stands for Stochastic Gradient Descent\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 22\n22\nM. Holzer, B. Ulmann\npurpose of both of them is to act as the learning rate. Thus, the ”the α\nused in the Q-learning algorithm” can be omitted (means α can be set to\n1):\nQSGDReg(s, a) ←r + γ ∗Q(s′, abest)\nIt needs to be mentioned, that the SGDRegressor is not just overwriting the\nold value with the new value when the above-mentioned formula is executed.\nInstead, when its online learning function partial fit is called, it improves\nthe existing predictor for Q(s, a) by using the new value r + γ ∗Q(s′, abest)\ndiscounted by the learning rate α.\nIn plain english, calling partial fit is equivalent to “the linear re-\ngressor that is used to represent Q(s, a) in Python updating its knowledge\nabout the action value function Q for the state s in which action a has\nbeen taken by a new estimate without forgetting what has been previously\nlearned. The new estimate that is used to update Q consists of the short-\nterm reward r that came as a result of taking action a while being in step\ns plus the discounted estimated long-term reward γ ∗Q(s′, abest) that is\nobtained by acting as if after action a has been taken, in future always the\nbest possible future action will be taken.”\nanalog-cartpole.py\n1\n# List of possible actions that the RL agent can perform in\n2\n# the environment. For the algorithm, it doesn’t matter if 0\n3\n# means right and 1 left or vice versa or if there are more\n4\n# than two possible actions\n5\nenv_actions = [0, 1]\n6\n7\n[...]\n8\n9\n# we use one Linear Regression per possible action to model\n10\n# the Value Function for this action, so rbf_net is a list;\n11\n# SGDRegressor allows step-by-step regression using\n12\n# partial_fit, which is exactly what we need to learn\n13\nrbf_net = [SGDRegressor(eta0=ALPHA,\n14\npower_t=ALPHA_DECAY,\n15\nlearning_rate=’invscaling’,\n16\nmax_iter=5, tol=float(\"-inf\"))\n17\nfor i in range(len(env_actions))]\n18\n19\n[...]\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 23\nHybrid computer approach to train a machine learning system\n23\n20\n21\n# learn Value Function for action a in state s\n22\ndef rl_set_Q_s_a(s, a, val):\n23\nrbf_net[a].partial_fit(rl_transform_s(s),\n24\nrl_transform_val(val))\n25\n26\n[...]\n27\n28\n# Learn the new value for the Value Function\n29\nnew_value = r + GAMMA * max_q_s2a2\n30\nrl_set_Q_s_a(s, a, new_value)\nanalog-cartpole.py\nThe source snippet shows that there is a regular Python List which\ncontains two21 objects of type SGDRegressor.\nTherefore rbf net[n]\ncan be considered as the representation of Qa=n(s) in memory.\nFor\nincreasing the accuracy of Q during the learning process, the function\nrl set Q s a(s, a, val) which itself uses SGDRegressor’s partial fit\nfunction is called regularly at each step of each episode. Therefore lines\n29–30 are equivalent to line 18 of algorithm 1.\n3.3.4. Feature transformation to avoid underﬁtting\nUnderﬁtting occurs when a statistical model or machine learning algorithm\ncannot capture the underlying trend of the data. Intuitively, underﬁtting\noccurs when the model or the algorithm does not ﬁt the data well enough,\nbecause the complexity of the model is too low. As shown in section 2, the\ninverse pendulum is a non-linear function so one might think that this is\nthe reason that a simple linear regression which tries to match the com-\nplexity of a non-linear function using s ∈S as feature set might be prone\nto underﬁtting.\nIn general, this is not the case. As shown at the beginning of this section\non page 18 where a control algorithm is introduced, that is based on a\nrandomly found ⃗θ and that controls the cart via the linear dot multiplication\n⃗θ · ⃗s and where both vectors consist of mere four elements, simple linear\nfunctions can absolutely control a complex non-linear phenomenon.\nSo\nlinear control functions per se are not the reason for underﬁtting.\n21for i in range(len(env actions)) leads to two iterations as env actions contains\ntwo elements\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 24\n24\nM. Holzer, B. Ulmann\nInstead, the reason why underﬁtting occurs in the context of the Q-\nlearning algorithm is that the very concept of reinforcement learning’s value\nfunction, which is a general purpose concept for machine learning, creates\noverhead and complexity. And this complexity needs to be matched by the\nstatistical model - in our case linear regression - that is chosen to represent\nQ(s, a). The value function is more complex than a mere control policy like\nthe one that is described on page 18, because it not only contains the in-\nformation necessary to control the cart but it also contains the information\nabout the expected future reward. This surplus of information needs to be\nstored somewhere (i.e. needs to be ﬁtted by the model of choice).\nThe model complexity of linear regression is equivalent to the number\nm of input features fi (i = 1 . . . m) in the linear regression equation ˆy =\nθ0 + θ1f1 + θ2f2 + · · · + θmfm .\nFinding out the optimal threshold of the model complexity necessary to\navoid underﬁtting is a hard task that has not been solved in data science in\ngeneral at the time of writing. Therefore the model complexity is another\none of the many hyper parameters that need to be found and ﬁne-tuned be-\nfore the actual learning process begins. Finding the right hyper parameters\noften requires a combination of intuition and trial and error.\nConsequently the challenge that had to be solved by the authors was:\nWhen the model complexity of linear regression needs to be increased, more\nfeatures are needed. But how can more than four features be generated,\ngiven that s ∈S only consists of the four features x, ˙x, ϕ, ˙ϕ?\nThe solution is to perform a feature transformation, where the number\nof new features after transforming s ∈S is signiﬁcantly bigger than four:\n(x, ˙x, ϕ, ˙ϕ) →(f1, . . . , fm) (i = 1 · · · , m >> 4)\nSince the open question here is “how much means signiﬁcantly?”, one of\nthe requirements for a good feature transformation function is ﬂexibility\nin the sense that it must be as easy to adjust the hyperparamter “model\ncomplexity m” as it is to adjust some constants in the Python code (versus\nﬁnding a completely new feature transformation function each time m needs\nto be increased or decreased).\nIt is a best practice of feature engineering that a feature transformation\nintroduces a certain non-linearity. This can be done using many diﬀerent\noptions such as using a polynomial over the original features.\nRadial Basis Functions (RBF) are another option and have been chosen\nas a means of feature transformation that allows to increase the model\ncomplexity m of the linear regression. An RBF is a function that maps a\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 25\nHybrid computer approach to train a machine learning system\n25\nvector to a real number by calculating (usually) the Euclidian distance from\nthe function’s argument to a previously deﬁned center which is sometimes\nalso called an exemplar. This distance is then used inside a kernel function\nto obtain the output value of the RBF.\nBy using a large number of RBF transformations, where each RBF uses\na diﬀerent, randomly chosen, center, a high number m of input features fi\ncan be generated for the linear regression. In other words: A feature map\nof s is generated by applying m diﬀerent RBFs with m diﬀerent random\ncenters on s:\ns = (x, ˙x, ϕ, ˙ϕ)\ncenteri = random(xi, ˙xi, ϕi, ˙ϕi) for all i = 1 . . . m,\nδi = distanceeuclidi = ∥(s −centeri)∥\nRBFi : s ∈S →yi ∈R\nRBFi : e−(βδi)2 →yi\n(6)\nβ is a constant for all i = 1 . . . m that deﬁnes the shape of the bell curve\ndescribed by the Gaussian RBF kernel applied here. So in summary, the\nmethod described here is generating m features from the original four fea-\ntures of s where due to the fact that the Eucledian distance to the centers is\nused, similar states s are yielding similar transformation results y. As long\nas this similarity premise holds and as long as the feature transformation is\nnot just a linear combination of the original features but adds non-linearity,\nit actually does not matter for the purposes of adding more model complex-\nity to the linear regression, what kind of feature transformation is applied.\nRBFs are just one example.\nDue to the fact that it is beneﬁcial22 in the context of the analog com-\nputer simulation to achieve near real-time performance of the Python soft-\nware, the above-mentioned RBF transformation has not been implemented\nverbatim in Python. This would have been too slow. Instead, the scikit-\nlearn class RBFSampler has been used, which generates the feature map of\nan RBF kernel using a Monte Carlo approximation of its Fourier transform.\nThe experiments have shown that RBFSampler is fast enough and that the\napproximation is good enough23.\nanalog-cartpole.py\n1\n# The following four constants are tunable hyperparameters.\n2\n# Please note that the source code uses the term GAMMA for\n22High Python program execution performance is helpful to achieve a high sampling rate\nas described in section 3.3.1 and therefore a high accuracy\n23See also https://tinyurl.com/RBFSampler\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 26\n26\nM. Holzer, B. Ulmann\n3\n# denoting what we are calling BETA in this chapter: The\n4\n# shape of the bell curve.\n5\nRBF_EXEMPLARS\n= 250\n# amount of exemplars per \"gamma\n6\n# instance\" of the RBF network\n7\nRBF_GAMMA_COUNT = 10\n# amount of \"gamma instances\", i.e.\n8\n# RBF_EXEMPLARS*RBF_GAMMA_COUNT feat.\n9\nRBF_GAMMA_MIN\n= 0.05 # minimum gamma, linear interpolation\n10\n# between min and max\n11\nRBF_GAMMA_MAX\n= 4.0\n# maximum gamma\n12\n13\n[...]\n14\n15\n# create scaler and fit it to the sampled observation space\n16\nscaler = StandardScaler()\n17\nscaler.fit(clbr_res)\n18\n19\n[...]\n20\n21\n# the RBF network is built like this: create as many\n22\n# RBFSamplers as RBF_GAMMA_COUNT and do so by setting the\n23\n# \"width\" parameter GAMMA of the RBFs as a linear\n24\n# interpolation between RBF_GAMMA_MIN and RBF_GAMMA_MAX\n25\ngammas = np.linspace(RBF_GAMMA_MIN,\n26\nRBF_GAMMA_MAX,\n27\nRBF_GAMMA_COUNT)\n28\nmodels = [RBFSampler(n_components=RBF_EXEMPLARS,\n29\ngamma=g) for g in gammas]\n30\n31\n# we will put all these RBFSamplers into a FeatureUnion, so\n32\n# that our Linear Regression can regard them as one single\n33\n# feature space spanning over all \"Gammas\"\n34\ntransformer_list = []\n35\nfor model in models:\n36\n# RBFSampler just needs the dimensionality,\n37\n# not the data itself\n38\nmodel.fit([[1.0, 1.0, 1.0, 1.0]])\n39\ntransformer_list.append((str(model), model))\n40\n# union of all RBF exemplar’s output\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 27\nHybrid computer approach to train a machine learning system\n27\n41\nrbfs = FeatureUnion(transformer_list)\n42\n43\n[...]\n44\n45\n# transform the 4 features (Cart Position, Cart Velocity,\n46\n# Pole Angle and Pole Velocity At Tip) into\n47\n# RBF_EXEMPLARS*RBF_GAMMA_COUNT distances from the\n48\n# RBF centers (\"Exemplars\")\n49\ndef rl_transform_s(s):\n50\n# during calibration, we do not have a scaler, yet\n51\nif scaler == None:\n52\nreturn rbfs.transform(\n53\nnp.array(s).reshape(1, -1))\n54\nelse:\n55\nreturn rbfs.transform(\n56\nscaler.transform(\n57\nnp.array(s).reshape(1, -1)))\n58\n59\n# SGDRegressor expects a vector, so we need to transform\n60\n# our action, which is 0 or 1 into a vector\n61\ndef rl_transform_val(val):\n62\nreturn np.array([val]).ravel()\nanalog-cartpole.py\nSection\n3.3.3\nis\nmissing\nan\nexplanation\nfor\nthe\ntwo\nfunctions\nrl transform s and rl transform val used in lines 23 and 24 of the source\ncode snippet shown there. The latter one is just a technical necessity, be-\ncause SGDRegressor expects a vector. In our model, the action is not a\nvector but an integer, so rl transform val ensures, that we transform the\ninteger a into the vector ⃗a = (a).\nThe actual feature transformation is taking place in rl transform s.\nAs described above, the scikit-learn class RBFSampler is used for per-\nformance reasons. The model complexety m is deﬁned by two constants\nin Python: RBF EXEMPLARS and RBF GAMMA COUNT24 and m is the product\nof both of them. As an enhancement to the formula RBFi : e−(βδi)2 →yi\nexplained on page 25, where the shape parameter β of the Gaussian curve is\nkept constant, the solution here creates a linear interpolation between the\n24The source code calls β not BETA in this context but GAMMA. In contrast, γ is used to\ndenote the discount factor in this chapter.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 28\n28\nM. Holzer, B. Ulmann\nvalue RBF GAMMA MIN and RBF GAMMA MAX consisting of RBF GAMMA COUNT\ndistinct values for β. The overall result of m RBF transformations is united\nin a Python FeatureUnion so that the function rl transform s can con-\nveniently call the object rbfs to execute the transformation.\nThe experiments have shown, that the learning eﬃciency is higher, when\nthe values that are coming from the analog computer are processed as they\nare instead of being scaled before. Therefore, the full source code on GitHub\n(see footnote 17) contains a boolean switch called PERFORM CALIBRATION\nthat is set to False in the ﬁnal version. This means, that the if branch\nshown in lines 48 and 49 is the one performing the feature transformation.\n3.3.5. Decaying α and ε to improve learning and to avoid overﬁt-\nting\nOur experiments have shown that it makes sense to start with relatively\nhigh values for the learning rate α and the explore vs. exploit probability\nε and then to decay them over time. This is the equivalent of trying many\ndiﬀerent options at the beginning when there is still very little knowledge\navailable (high ε means to favor explore over exploit) and learning quickly\nfrom those explorations (high α). Later when knowledge has accumulated,\nit takes more eﬀort to modify facts that have been learned earlier (low α).\nSGDRegressor has a built in function to decay alpha over time. The ε\ndecay has been implemented manually. The following code snippet shows\nthe actual parameters used in the implementation.\nanalog-cartpole.py\n1\nGAMMA\n= 0.999 # discount factor for Q-learning\n2\nALPHA\n= 0.6\n# initial learning rate\n3\nALPHA_DECAY\n= 0.1\n# learning rate decay\n4\nEPSILON\n= 0.5\n# randomness for epsilon-greedy\n5\nEPSILON_DECAY_t\n= 0.1\n# decay parameter for epsilon\n6\nEPSILON_DECAY_m\n= 10\n# ditto\nanalog-cartpole.py\nDecaying α and ε is not only used to improve the learning itself, but also\nto avoid overﬁtting the model. Overﬁtting is described25 as the “produc-\ntion of an analysis which corresponds too closely or exactly to a particular\nset of data, and may therefore fail to ﬁt additional data or predict future\nobservations reliably”.\n25https://www.lexico.com/definition/overfitting\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 29\nHybrid computer approach to train a machine learning system\n29\nAnother mechanism that was used to avoid overﬁtting is to make sure\nthat the system is “not learning for too long”. The concrete meaning of “too\nlong” is - as many things in machine learning - another hyper parameter.\nFor being able to persistently access the (near to) optimal learning state,\nthe Python implementation supports a command line parameter, that forces\nthe current state of the “brain” to be saved every PROBE episodes, whereas\nPROBE is a constant (hyper parameter).\n3.4. Hybrid Interface\nThe Analog Paradigm Model-126 analog computer used by the authors\nincorporates a Hybrid Controller for connecting the Model-1 to digital\ncomputers. It uses an RS232 over USB mechanism that is compatible with\nmost PC operating systems, in the sense that the PC operating system is\nable to provide a virtual serial port that behaves exactly as if the analog\nand the digital computer where connected by an RS232 cable instead of an\nUSB cable. This is why, in Python, the communication uses the pySerial27\nlibrary. The following code snippet shows the setup parameters. Note that\nthe Hybrid Controller is communicating at 250,000 baud.\nanalog-cartpole.py\n1\n# Hybrid Controller serial setup\n2\nHC_PORT\n= \"/dev/cu.usbserial-DN050L1O\"\n3\nHC_BAUD\n= 250000\n4\nHC_BYTE\n= 8\n5\nHC_PARITY\n= serial.PARITY_NONE\n6\nHC_STOP\n= serial.STOPBITS_ONE\n7\nHC_RTSCTS\n= False\n8\nHC_TIMEOUT\n= 2\n9\n10\n[...]\n11\n12\nhc_ser = serial.Serial( port=HC_PORT,\n13\nbaudrate=HC_BAUD,\n14\nbytesize=HC_BYTE,\n15\nparity=HC_PARITY,\n16\nstopbits=HC_STOP,\n17\nrtscts=HC_RTSCTS,\n26http://analogparadigm.com/products.html\n27https://pypi.org/project/pyserial\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 30\n30\nM. Holzer, B. Ulmann\n18\ndsrdtr=False,\n19\ntimeout=HC_TIMEOUT)\nanalog-cartpole.py\nAnalog computers operate in diﬀerent modes. In this machine learning\nimplementation, the Python script controls the analog computer’s mode of\noperation according to the needs of the algorithm:\nInitial Condition marks the beginning of an episode. The pendulum is\nin an upright position.\nOperate is the standard mode of operation, where the analog computer\nis running the simulation in real-time.\nHalt means that the simulation is paused and that it can be resumed any\ntime by returning to the Operate mode.\nThe Hybrid Interface accepts certain mode-change commands via the serial\nline to put the analog computer into the appropriate mode. Moreover, there\nare several commands to read data from the various computing elements\nin the analog computer. All the computing elements can be referenced by\nunique addresses. For being able to inﬂuence the calculations of the analog\ncomputer, the Hybrid Controller provides analog and digital inputs and\noutputs. For the purposes of this problem, only two digital outputs are\nneeded to drive the model:28\nDigital Out #0 is used to set the direction from which the force is being\napplied when pushing the cart\nDigital Out #1 makes sure that a force is applied to the cart as long as\nit is being set to 1\nanalog-cartpole.py\n1\n# Addresses of the environment/simulation data\n2\nHC_SIM_X_POS\n= \"0223\" # address cart x\n3\nHC_SIM_X_VEL\n= \"0222\" # address cart x-velocity\n4\nHC_SIM_ANGLE\n= \"0161\" # address pendulum angle\n5\nHC_SIM_ANGLE_VEL\n= \"0160\" # address pend. angular vel.\n6\n7\nHC_SIM_DIRECTION_1\n= \"D0\"\n# dout: cart direct. = 1\n8\nHC_SIM_DIRECTION_0\n= \"d0\"\n# dout: cart direct. = 0\n9\nHC_SIM_IMPULSE_1\n= \"D1\"\n# dout: apply force\n10\nHC_SIM_IMPULSE_0\n= \"d1\"\n# dout: apply NO force\n11\n28See section 2.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 31\nHybrid computer approach to train a machine learning system\n31\n12\n# Model-1 Hybrid Controller: commands\n13\nHC_CMD_RESET\n= \"x\"\n# reset hybrid controller\n14\nHC_CMD_INIT\n= \"i\"\n# initial condition\n15\nHC_CMD_OP\n= \"o\"\n# start to operate\n16\nHC_CMD_HALT\n= \"h\"\n# halt/pause\n17\nHC_CMD_GETVAL\n= \"g\"\n# set address of analog\n18\n# computing element and\n19\n# return value and ID\n20\nHC_CMD_BULK_DEFINE\n= \"G\"\n# set addresses of multiple\n21\n# elements to be returned\n22\n# in a bulk transfer via \"f\"\n23\nHC_CMD_BULK_FETCH\n= \"f\"\n# fetch values of all\n24\n# addresses defined by \"G\"\n25\n[...]\n26\n27\ndef hc_send(cmd):\n28\nhc_ser.write(cmd.encode(\"ASCII\"))\n29\n30\ndef hc_receive():\n31\n# HC ends each communication with \"\\n\",\n32\n#so we can conveniently use readline\n33\nreturn hc_ser.readline().\n34\ndecode(\"ASCII\").\n35\nsplit(\"\\n\")[0]\nanalog-cartpole.py\nAs described in section 3.3.1, the digital computer samples the ana-\nlog computer’s continuous simulation by repeatedly reading the simulation\nstate variables, to generate the discretization needed for Q-learning. The\nHybrid Controller supports this eﬀort by providing a bulk readout func-\ntion that returns the simulation’s overall state with as little overhead as\npossible. This signiﬁcantly reduces latency during the learn and control\nloop, and thus - as experiments have shown - improves the eﬃciency and\nconvergence speed of the Q-learning algorithm.\nThe bulk mode is activated by sending a bulk deﬁnition command that\ndeﬁnes a readout group on the Hybrid Controller. After this has been done,\nthe bulk read is triggered by sending a short (single character) fetch com-\nmand. The following code snippet illustrates the concept and the process.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 32\n32\nM. Holzer, B. Ulmann\nanalog-cartpole.py\n1\n#define a readout group in the Hybrid Controller\n2\nhc_send(HC_CMD_BULK_DEFINE + HC_SIM_X_POS + ’;’\n3\n+ HC_SIM_X_VEL + ’;’\n4\n+ HC_SIM_ANGLE + ’;’\n5\n+ HC_SIM_ANGLE_VEL + ’.’)\n6\n7\n# when using HC_CMD_GETVAL,\n8\n#HC returns \"<value><space><id/type>\\n\"\n9\n# we ignore <type> but we expect a well formed response\n10\ndef hc_res2float(str):\n11\nf = 0\n12\ntry:\n13\nf = float(str.split(\" \")[0])\n14\nreturn f\n15\nexcept:\n16\n[...]\n17\n18\n# query the current state of the simulation, which consists\n19\n# of the x-pos and the the x-velocity of the cart, the angle\n20\n# and angle velocity of the pole/pendulum\n21\ndef hc_get_sim_state():\n22\n# bulk transfer: ask for all values that consitute the\n23\n# state in a bulk using a single fetch command\n24\nif HC_BULK:\n25\nhc_send(HC_CMD_BULK_FETCH)\n26\n(res_x_pos, res_x_vel,\n27\nres_angle, res_angle_vel) = hc_receive().split(’;’)\n28\nreturn (hc_res2float(res_x_pos),\n29\nhc_res2float(res_x_vel),\n30\nhc_res2float(res_angle),\n31\nhc_res2float(res_angle_vel))\n32\nelse:\n33\n[...]\nanalog-cartpole.py\nThe only way the environment in this example can be inﬂuenced by\nthe reinforcement learning agent is via the predeﬁned actions. In this case:\n“Push the cart to the left or push it to the right.” When looking at this\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 33\nHybrid computer approach to train a machine learning system\n33\nsituation from an analog computer’s viewpoint, this is a continuous opera-\ntion: “To push” means that a force F that does not necessarily need to be\nconstant over time is applied for a certain period of time. As described in\nsection 3.3.2 the implementation used here is heavily simpliﬁed: Two pos-\nsible actions, one ﬁxed and constant force F and a constant period of time\nwhere the force is applied form the two possible impulses. As the magnitude\n|F| of the force is conﬁgured directly at the analog computer, the Python\ncode can focus on triggering the impulse using the two above-mentioned\ndigital outputs.\nanalog-cartpole.py\n1\n# duration [ms] of the impulse, that influences the cart\n2\nHC_IMPULSE_DURATION = 20\n3\n4\n[...]\n5\n6\n# influence simulation by using an impulse to\n7\n# push the cart to the left or to the right;\n8\n# it does not matter if \"1\" means left or right\n9\n# as long as \"0\" means the opposite of \"1\"\n10\ndef hc_influence_sim(a, is_learning):\n11\n[...]\n12\n13\nif (a == 1):\n14\nhc_send(HC_SIM_DIRECTION_1)\n15\nelse:\n16\nhc_send(HC_SIM_DIRECTION_0)\n17\n18\nhc_send(HC_SIM_IMPULSE_1)\n19\nsleep(HC_IMPULSE_DURATION / 1000.0)\n20\nhc_send(HC_SIM_IMPULSE_0)\n21\n22\n[...]\nanalog-cartpole.py\nIn summary it can be stated, that working with an analog computer\nlike the Model-1 with its Hybrid Controller is quite straightforward. The\nserial communication can be encapsulated in some functions or objects and\nfrom that moment on, the only thing that one needs to keep in mind is the\ncompletely asynchronous and parallel nature of the setup. No assumptions\non certain sequence properties can be made and all the typical challenges\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 34\n34\nM. Holzer, B. Ulmann\nof asynchronous parallel setups like jitter, race conditions, latency, etc. can\noccur and need to be managed properly.\n4. Results\nAnalog computers outperform digital computers with respect to raw com-\nputational power as well as with respect to power eﬃciency for certain\nproblems, such as the simulation of dynamic systems that can be readily\ndescribed by systems of coupled diﬀerential equations. This is due to the\nfact that analog computers are inherently parallel in their operation as they\ndo not rely on some sort of an algorithm being executed in a step-wise fash-\nion. Other advantages are their use of a continuous value representation\nand that fact that integration is an intrinsic function.\nTypical analog computer setups tend to be extremely stable and are not\nprone to problems like numerical instabilities etc. Although the precision of\nan analog computer is quite limited compared with a stored-program digital\ncomputer employing single or even double precision ﬂoating point numbers\n(a precision analog computer is capable of value representation with about\n4 decimal places), solutions obtained by means of an analog computer will\nalways turn out to be realistic, something that cannot be said of numerical\nsimulations where the selection of a suitable integration scheme can have a\nbig eﬀect on the results obtained.\nIn reinforcement learning scenarios, the actual simulation of the envi-\nronment as shown in ﬁgure 6 on page 13 is one of the hardest and most\ntime consuming things to do.\nA simulation implemented on an analog\ncomputer behaves much more realistically than one performed on a digital\ncomputer. It yields unavoidable measurement errors as would be present\nin a real-world-scenario, it is immune to numerical problems etc.\nA plethora of RL algorithms such as (Deep) Q-learning are available\nand well understood, as for example shown in [Sutton (2018)]. Many Open\nSource implementations are available for free use.\nSo when it comes to\npractially apply RL to solve challenges in rapidly advancing ﬁelds such\nas autonomous cars, robot navigation, coupling of human nervous signals\nto artiﬁcial limbs and the creation of personalized medical treatments via\nprotein folding, a new project very often starts with the question “how can\nwe simulate the environment for our agent to learn from it?”\nThe authors have shown in this chapter that this challenge can be very\nsuccessfully tackled using analog computers, so that a next generation of\nanalog computers based on VLSI analog chips could help to overcome this\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 35\nHybrid computer approach to train a machine learning system\n35\nmainstream challenge of reinforcement learning by propelling the speed\nand accuracy of environment simulation for RL agents to a new level. This\nwould result in faster development cycles for RL enabled products and\ntherefore could be one of many catalysts in transforming reinforcement\nlearning from a research discipline to an economically viable building block\nfor the truly intelligent device of the future.\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 36\nMarch 16, 2021\n1:4\nws-rv9x6\nBook Title\nhybrid˙training\npage 37\nBibliography\nAlain J. Brizard, An Introduction to Lagrangian Mechanics, World Scientiﬁc\nPublishing Company, 2008\nPedro Domingos, “A Few Useful Things to Know About Machine Learning”,\nin Communications of the ACM, October 2012, pp. 78–87,\nPedro Domingos, The Master Algorithm:\nHow the Quest for the Ultimate\nLearning Machine Will Remake Our World, Basic Books, 2015\nFabian\nPedregosa\net\nal.\n“Machine\nLearning\nin\nPython”,\nin\nJour-\nnal\nof\nMachine\nLearning\nResearch\nVol.\n12,\n2011,\npp.\n2825–2830\ndoi:10.1145/2347736.2347755\nRichard S. Sutton et al., Reinforcement Learning: An Introduction, second\nedition, The MIT Press, 2018\nBernd Ulmann, Analog and Hybrid Computer Programming, DeGruyter, 2020\nChris Watkins, Learning from Delayed Rewards, Ph.D. thesis, University of\nCambridge, 1989\nChris Watkins, P. Dayan “Q-learning”, in Machine Learning 8(3-4) 1992,\npp. 279-292\nMarco Wiering et al., Reinforcement Learning, Springer, 2012\n37\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.ET",
    "68T05",
    "I.2.6; B.m"
  ],
  "published": "2021-03-13",
  "updated": "2021-03-13"
}