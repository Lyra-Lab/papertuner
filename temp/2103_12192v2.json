{
  "id": "http://arxiv.org/abs/2103.12192v2",
  "title": "Reward-Reinforced Reinforcement Learning for Multi-agent Systems",
  "authors": [
    "Changgang Zheng",
    "Shufan Yang",
    "Juan Parra-Ullauri",
    "Antonio Garcia-Dominguez",
    "Nelly Bencomo"
  ],
  "abstract": "Reinforcement learning algorithms in multi-agent systems deliver highly\nresilient and adaptable solutions for common problems in\ntelecommunications,aerospace, and industrial robotics. However, achieving an\noptimal global goal remains a persistent obstacle for collaborative multi-agent\nsystems, where learning affects the behaviour of more than one agent. A number\nof nonlinear function approximation methods have been proposed for solving the\nBellman equation, which describe a recursive format of an optimal policy.\nHowever, how to leverage the value distribution based on reinforcement\nlearning, and how to improve the efficiency and efficacy of such systems remain\na challenge. In this work, we developed a reward-reinforced generative\nadversarial network to represent the distribution of the value function,\nreplacing the approximation of Bellman updates. We demonstrated our method is\nresilient and outperforms other conventional reinforcement learning methods.\nThis method is also applied to a practical case study: maximising the number of\nuser connections to autonomous airborne base stations in a mobile communication\nnetwork. Our method maximises the data likelihood using a cost function under\nwhich agents have optimal learned behaviours. This reward-reinforced generative\nadversarial network can be used as ageneric framework for multi-agent learning\nat the system level",
  "text": "1\nReward-Reinforced Generative Adversarial\nNetworks for Multi-agent Systems\nChanggang Zheng, Shufan Yang∗, Juan Parra-Ullauri, Antonio Garcia-Dominguez, and Nelly Bencomo\nAbstract—Multi-agent systems deliver highly resilient and adaptable solutions for common problems in telecommunications,\naerospace, and industrial robotics. However, achieving an optimal global goal remains a persistent obstacle for collaborative\nmulti-agent systems, where learning affects the behaviour of more than one agent. A number of nonlinear function approximation\nmethods have been proposed for solving the Bellman equation, which describe a recursive format of an optimal policy. However, how to\nleverage the value distribution based on reinforcement learning, and how to improve the efﬁciency and efﬁcacy of such systems remain\na challenge. In this work, we developed a reward-reinforced generative adversarial network to represent the distribution of the value\nfunction, replacing the approximation of Bellman updates. We demonstrated our method is resilient and outperforms other conventional\nreinforcement learning methods. This method is also applied to a practical case study: maximising the number of user connections to\nautonomous airborne base stations in a mobile communication network. Our method maximises the data likelihood using a cost\nfunction under which agents have optimal learned behaviours. This reward-reinforced generative adversarial network can be used as a\ngeneric framework for multi-agent learning at the system level.\nIndex Terms—multi-agent, reinforcement learning, GAN, reward-reinforced GAN, airborne base station (ABS)\n!\n1\nINTRODUCTION\nP\nRactical applications of multi-agent systems, such as au-\ntonomous airborne base stations, need accurate real-time state\nestimation and learning capabilities to achieve optimised trajectory\nplanning with a maximised global goal (the maximum number of\nusers connected). The airborne base stations have a high degree\nof autonomy, serving their own users within the signal coverage\nrange. At the same time, however, multiple airborne stations need\nto be coordinated with each other to serve the users in a constantly\nchanging environment: users move around, and neighbouring base\nstations produce signal interference. It is challenging to create\noptimised trajectories for all agents (base stations) at the same\ntime, while considering the current state of neighbouring agents\nand their performed actions.\nTraditional centralised algorithms for multi-agent systems\nallowing agents to share their information with a central node\nare computationally expensive. Reinforcement learning-based dis-\ntributed algorithms can only explicitly share information with\ntheir neighbours, which is computationally efﬁcient compared\nwith centralised algorithms. Reinforcement learning has had great\nsuccesses in solving multi-agent collaborative tasks when three\nconstraints are met: i) having an environment, ii) having a reward\ngeneration process either through an approximated function or a\nsimple greedy method, and iii) having agents interacting with the\n•\nChanggang Zheng is at Department of Engineering Science, Univer-\nsity of Oxford, United Kingdom; Shufan Yang ∗is the corresponding\nauthor and currently is School of Computing, Edinburgh Napier Uni-\nversity, Edinburgh, United Kingdom and Center of Medical and In-\ndustrial Ultrasonics, University of Glasgow, Glasgow, United Kingdom\nE-mail:s.yang@napier.ac.uk; Juan Parra,\nAntonio Garcia-Dominguez,\nand Nelly Bencomo are with School of Engineering and Applied Science,\nAston University, United Kingdom. 2021 IEEE. Personal use of this\nmaterial is permitted. Permission from IEEE must be obtained for all other\nuses, in any current or future media, including reprinting/republishing this\nmaterial for advertising or promotional purposes, creating new collective\nworks, for resale or redistribution to servers or lists, or reuse of any\ncopyrighted component of this work in other works.\nenvironment. However, those methods often have high variance\nin their results. In addition, agents may start to compete instead\nof collaborating with each other due to scarce resources, such\nas communication channel capacities. To date, most solutions\nconcentrate on the learning of individual agents, but neglect\napproaches at the system level [1].\nA conventional reward mechanism in a reinforcement learning\nframework is designed to produce a cost-beneﬁt assessment of a\ngiven action, and subsequently apply a high or low reward in a\nheuristic search approach [2]. Hjeldm et al. proposed an approach\nto intelligent drone tracking using reinforcement learning [3].\nThis type of mechanism required continuous feedback from the\nenvironment, which means this technique scaled poorly to a\nlarge team with multi-agents; furthermore, traditional Bayesian\napproaches to reinforcement learning problems cannot model the\ninherent variability of the action of the state. Deep Q Network\n(DQN) reinforcement learning method used multiple hidden layers\nof a neural network to ﬁt a state value and a state-action value\ndistribution into a single agent [3].\nThe goal of classical reinforcement learning was to ﬁnd an op-\ntimal policy that could maximize rewards. Instead of searching for\na policy, researchers proposed a heuristic search which aimed to\nimprove an estimated state value function, in order to minimize the\nexpected distance between the value function’s output and agents’s\ntates [4]. Since heuristic search methods were impossible to\nemulate all states in an environment . Other researchers focused on\nﬁnding a reward function that ’experts’ are implicitly optimising\neither from states to actions, or from states to reward values, often\ncalled inverse reinforcement learning [5]. Two distinct approaches\nof inverse reinforcement learning were explored: the ﬁrst method\nwas proposed to seek a way for directly approximating the reward\nfunction by tuning inputs [5]; the second one was focused on\nlearning a policy that matched its action with demonstrated be-\nhaviour [6]. The ﬁrst approach depended on selecting a complete\nreward structure or set of feature functions which can be hard to\narXiv:2103.12192v2  [cs.MA]  14 May 2021\n2\ngeneralise since various environments may use different reward\nfeatures [7]. The second approach was sensitive for deterministic\nactions since the optimization became theoretically impossible if\nthe underlying reward function was a non-convex function [8]. The\ninverse reinforcement learning method had generalisation issues\nsince it used “experts” to demonstrate how the optimal behaviour\nshould be, which was unrealistic in real-life. Furthermore, agents\nbased on inverse reinforcement learning methods were only as-\nsumed to follow an optimal policy and were prone to only take\nsuboptimal actions by the agents.\nBy contrast, generative adversarial networks (GANs) have\nshown remarkable results at generating data that imitates a data\ndistribution in a multi-agent system [9]. For example, an extended\ngenerative adversarial imitation learning framework was proposed\nin [10] to train an action policy network for autonomous vehicles,\nwith only one action of the vehicle modelled. Although a large\nnumber of studies on image data augmentation tasks provided\npromising results, to our knowledge, far fewer research efforts\nhave been devoted to the application of adversarial training to\ncollaborative multi-agent systems.\nInspired by the study [11], we propose a deep generative model\nand an opponent discriminator model followed by a two-player\nmin-max game formula [3] as the single learning process for\nmodelling the behaviour of the entire team. We built on a reward\nmapping method that combines adversarial generative networks\nwith the use of reinforcement learning to produce domain-speciﬁc\nrewards. The generative model is used to examine the distribution\nof the value function for all agents, in order to reduce the training\nsteps while optimising the overall result. As aforementioned, we\napplied our model to a practical case study: the optimised deploy-\nment of airborne base stations in mobile communications. In our\napproach, the generator is trained to generate a predicted reward\nmap and trained adversely with discriminator, so that the networks\noptimise the properties of distributed multi-agent behaviours in\nan adversarial fashion. Our results show that Reward-Reinforced\nGenerative Adversarial Networks (RR-GANs) are able to achieve\nthe best global goal values, compared to other state-of-the-art\nreinforcement learning methods. This is thanks to how our model\ncan represent the distribution of the value function, replacing the\napproximation of Bellman updates via an adversarial learning\nmethod.\n2\nMETHODOLOGY\nThe modern ﬁeld of reinforcement learning is based on optimal\ncontrol [12], which came into use during the 1960’s to describe the\nproblem of designing a controller to minimize a measurement of\na dynamic system’s behaviour . The Bellman equation, as deﬁned\nin (1), states that the value of the initial state must equal the\nvalue of the expected next state, plus the expected reward along\nthe way. Instead of modelling the expectation of each value, we\ndesign a new method which applies Bellman’s equation to provide\nan approximated optimal value function. The distribution of Z is\ncharacterized by the interaction of two distributions: the reward R\ndistribution, the probability that the tuple (s, a) at time t will lead\nto the next distribution Z at time t + 1.\nZ(s, a) ≡D R(s, a) + Z( ´S, ´A)\n(1)\nTo solve the Bellman equation, we aim to recover a estimation\nprobability distribution over the reward function from “expert”\ndemonstrations using generative adversarial modelling. At the\ninitial stage, agents exploit the environment and use maximum\nlikelihood rewards to choose best actions. At the execution stage,\nthe generative modelling is used for generated reward mapping.\nOver time the agent is supposed to customize its actions to the\nenvironment so as to maximize the sum of this reward.\n2.1\nGeneric GAN\nThe generator G uses the original environment input o and a\nrandomly selected reward experience n to generate the predicted\nreward map p. Or, in mathematical notation, G : {o, n} →p. The\nadversarial discriminator D tries to classify the o concatenating\nwith p and the o concatenating with real reward map n.\nLGAN(G, D) =Eo,p[log D(o, p)] + Eo,n[log(1 −D(o, G(o, n))]\n(2)\nAs shown in (2), the G aims at minimizing the objective\nLGAN and the D tries to maximize the objective LGAN.\nLL1(G) = Eo,p,n [∥p −G(o, n)∥2]\n(3)\nWe substitute the deﬁnition given by (3) into (4). The generator\nobjective for optimising the divergence between generated data\nand the input real data is:\nGglobal = arg min\nG max\nD LGAN(G, D) + λLL1(G)\n(4)\n2.2\nRR-GAN\nThe data used for learning is obtained by a greedy method. An\nagent only takes the action when the number of connected users\nincreases, where a reward is given. At the learning stage, the user\ndistribution ( a part of the environment) is received as the input\nto the generator network: the target location is the one with the\nmaximum reward.\nThe GAN is a multi-agent system with N agents and the\nloss function LGAN(G, D) is denoted as f([θ1, θ2, . . . , θN], φ)\n, where θN is the parameter vector of the N th agent and φ is the\nparameter vector of the environment (i.e. the user distribution),\nand where the objective functions of generator are f(θN) and the\nobjective function of discriminator −f(θN). The optimum is a\nNash equilibrium deﬁned as in ( 5).\nΘ ∈argmaxf(Θ, φ∗), φ∗∈argmax −f(Θ∗, φ)\n(5)\nThe maximum likelihood estimator solves dL\ndx = 0, where\nx = (Θ, φ)T . The Nash-equilibrium points pose the following\nproperties:\ndLGAN\ndΘ\n=\n\u0014 df(x∗)\n−df(x∗)\n\u0015\n= 0\n(6)\nT(x∗) =\n\n\nd2f(x∗)\nd2θ\ndf(x∗)\ndθdΦ\n−d2f(x∗)\ndθdΦ\n−d2f(x∗)\nd2Φ\n\n≤0\n(7)\nWe substitute the deﬁnition of T(x) and gradient of dLGAN\ndθ\n, as\ngiven by (6) and (7) into (8). Using consensus optimization [13],\nthe objective function of the generator can add a 2-norm as a\nregularization term. The updated rule for the generator network\nbecomes:\nxk+1 = xk + αdLGAN\ndθ\n+ γT(x)T\n(8)\n3\nWhen γ and α are proper values using hypothesis testing,\nthe optimisation will be locally stable at the Nash equilibrium\nif T(x) is inevitable. Hence, a stationary distribution exists, and\nthe GAN architecture can converge to this stationary distribution.\nFurthermore, to avoid low convergence speed or even divergence,\nwe used the regulated RMSProp optimisation [14]. The empirical\nresults reported in the results section demonstrate the ability of\nour method to solve the reinforcement learning tasks. Further\nmathematical proof requires an argument similar to [14].\nRR-GAN is used to generate the predicted reward maps from\nthe current environment input and randomly selected past rewards\nfrom each agent. This method reduces the ﬁtting difﬁculty of the\nnetwork of the discriminator D. The network mainly needs to\nlearn from the data source (i.e. user distribution), and the time\nvarying environment (i.e. how many users are within range of\neach airborne base station). In the initial stage an agent is placed\nin a situation without knowledge of any goals or other information\nabout other agents. As an agent acts in the environment, a\nreinforcement reward governs its actions: increasing the number\nof connected users is rewarded. By only giving the agent a reward\nwhen connected users are reached, the agent learns to achieve\nits goals. However, since each agent competes (due to signal\ninterference), generative modelling is used to generate reward\nmapping at each execution stage. Over time, the agent customizes\nits actions to the environment to maximize the sum of the rewards.\nAs shown in Fig. 1, real-time information from the environ-\nment (in this case, the user distribution) allows the network to self-\nadjust. Agents will act according to the predicted reward map for\nmoving in the right direction. At the training stage, all agents will\nuse a greedy method to exploit the maximum users connections\nin their own signal coverage areas to generate a reward map (blue\narrows in Fig. 1). After several iterations, all agents adjust their\nbehaviours according to the generated reward maps (red arrows in\nFig. 1).\n[15].\nThe pseudo code of RR-GAN is shown below:\nAlgorithm 1: Reward-Reinforced GAN\nData: Environment E, Predicted Reward Map\nRpredicted, Reward Map R\nResult: Trained Reward-Reinforced GAN parameters θ\n1 Randomly generate environment E;\n2 Exploit rewards using greedy methods and store reward map R;\n3 for every epoch do\n4\nTrain Generator by using stored E and R, (Update θ);\n5\nTrain Discriminator by using stored R and Rpredicted;\n6\nfor every iteration do\n7\nif environment changed then\n8\nGenerator generate Rpredicted;\n9\nFind global optimal position;\n10\nend\n11\nDo action toward global optimal\n12\nend\n13 end\nTwo aspects can be modiﬁed to enhance the stability of GAN\ntraining: model setup, and optimization methods. In this work we\nused the adversarial learned kernels in a batch training.\nIn our experiment, the generative neural network is composed\nof both the generator model and discriminator model. Our net-\nwork structure follows Goodfellow’s published work [16]. The\ngenerator model consists of a U-net network, which is composed\nof two fully connected layers and eight deconvolutional layers.\nThe discriminator is composed of ﬁve convolution layers followed\nby two fully connected layers. The convolution and deconvolution\nlayer come with a batch normalization layer. The output layer of\nthe generator uses the sigmoid function as the activation function\nwhile all other and discriminator network are based on ReLu as\nactivation functions. Both generator and discriminator networks\nare trained under the Adam solver [17] with a learning rate of\n0.0001 [18], [19]. The parameters are selected with reference to\nsome other GANs [18], [19].\nThe speciﬁc structures of the generator and the discriminator\nare shown in Tables 1, 2, and 3.\nTABLE 1: Detailed model architecture —\nReward Map Prediction Network generator\nLayer name\nBlock type\nOutput resolution\nOutput depth\nDown 1\nConv Block\n100×100\n64\nDown 2\nBottleneck\n50×50\n128\nDown 3\nBottleneck\n25×25\n256\nDown 4\nBottleneck\n12×12\n512\nDown 5\nBottleneck\n6×6\n1024\nUp 1\nBottleneck\n12×12\n512\nUp 2\nBottleneck\n25×25\n256\nUp 3\nBottleneck\n50×50\n128\nUp 4\nBottleneck\n100×100\n64\nOut Conv\nConv Block\n100×100\nn\nTABLE 2: Bottleneck architecture\nLayer name\nOut Direction\nKernel size\nStride size\nConv 1+BatchNorm+ReLU\n3×3\n1\nConv 2+BatchNorm+ReLU\nUp\n3×3\n1\nMax Pool\nDown\n2×2\n2\nTABLE 3: Detailed model architecture —\nReward Map Prediction Network discriminator\nLayer name\nKernel size\nOutput depth\nStride size\nConv 1+BatchNorm+ReLU\n4×4\n64\n1\nConv 2+BatchNorm+ReLU\n4×4\n128\n1\nConv 3+BatchNorm+ReLU\n4×4\n256\n1\nConv 4+BatchNorm+ReLU\n4×4\n512\n1\nConv 5+BatchNorm+ReLU\n4×4\n512\n1\nFully Connected 1+Dropout\n—\n128\n—\nFully Connected 2+ReLU\n—\n1\n—\n3\nEXPERIMENTAL DESIGN AND RESULTS\nIn this section, we applied the proposed approach to control the\ntrajectory of airborne base stations in a mobile communication\nsystem. It is worth noting that this approach can solve many\nother tasks; for instance, trajectory prediction for industrial robotic\ncollaboration in warehouses [20]. More importantly, it can also be\napplied to common multi-agent problems: multi-agent learning\ncan exhibit unexpected interactions between agents as they gravi-\ntate toward an equilibrium [21].\n4\nG Loss\nMix Input\nD Loss\nConcatenate\nBlock\nConvolution\nBlock\nNormalization\nBlock\nFC\nLayer\nPooling \nLayer\nPredicted Reward\nMap\nReLU\nBlock\nReal Reward\nMap\nLabel\nEnvironment\nInput\nEnvironment\nReward Map\nData Pool\nExperimenting\nTraining\nAction\nEnvironment\nFig. 1: General diagram of the proposed RR-GAN\n3.1\nProblem Statement\nIn a mobile communication network, there are various scenarios\nwhere sudden spikes in connection demands can be generated,\nsuch as large social events (e.g. business campaigns, political\nrallies, university opening ceremonies or other unexpected events),\nor emergencies like the malfunction of the existing mobile base\nstations. Airborne base stations can provide a fast response to\nthese situations, as shown in Fig. 2. It is important to precisely\ncontrol the movement of the airborne base stations, in order to\nconnect the maximum number of users to the mobile network.\nFig. 2: Possible Airborne Base Stations using conditions: a static\nbase station (red) has malfunctioned; people gather at an area\nwith poor coverage from existing base stations\nWhile providing mobile communications with airborne base\nstations, interference and noise are two potential impairments to\nsignal quality [22]. Two parameters model communication chan-\nnel (signal-to-interference-plus-noise ratio (SNIR), and reference\nsignal received power (RSRP)) can be computed as the Power\nDensity multiplied by the Antenna Effective Area [22], [23]. SNIR\nand RSRP are used for evaluating the potential signal quality\nbetween the mobile station and a user device. A threshold is used\nto distinguish the connectivity between users and base stations.\nThe overall attenuation effects can be represented by path loss\n(free-space loss). The behaviour of the airborne base stations\ndepends on their proximity to the neighbouring airborne base\nstations, and how many users are being served. Our objective is\nto learn a joined distribution of the reward map of each airborne\nbase station for making long-term predictions about the optimised\nnumber of users staying connected to a mobile network.\nThe communication model follows the formula in (9). All\nparameters in Table 4 are remained the same in all experiments.\nLs =\n\u00124πd\nλ\n\u00132\n(9)\nIn (10), c is the speed of light in metres per second; EIRP is the\nEquivalent Isotropic Radiated Power (the drone transmit power)\nin watts; fc is the carrier frequency in hertz; d is the distance\nbetween the user and the airborne base station in metres and the\nwavelength is λ =\nc\nfc .\nRSRPn,u = EIRP\nLs\n= EIRP c2\n(4πfcd)2\n(10)\nIn (10), the RSRP for the link between the user u and the\nairborne base station n is calculated according to the EIRP, and\nthe free space path loss is given in (9).\nSINRn,u =\nRSRPn,u\nN + P\n∀i̸=n RSRPi,u\n(11)\nThe Signal to Interference plus Noise Ratio (SINR) is deﬁned\nin (11), where N is the noise power in Watts.\nAs an example, the distribution of users (and their mobile\nphones) can be modeled by a bivariate distribution consisting of\na mixture of distinct Gaussian clusters [24]. The probability of\nusers appearing can be treated as a mixture of two time-invariant\n2-dimensional Gaussian distributions varying with time [25], [26].\nEach airborne base station have many users (mobile phones) [24].\nFor each Gaussian user cluster, the “user appearing” probability\n5\nTABLE 4: Communication and environment parameters\nParameters\nValue\nCommunication Parameters\nLowest SINR requirment, θµ\n0dB\nUAV-base station antenna directivity angle, φap\n60◦\nCarrier frequency, fc\n2.4GHz\nDrone transmition power\n40dBm\nBandwidth\n200kHz\nNoise power spectral density\n10−20.4W/Hz\nEnvironment Parameters\nRandom seed\n19\nLength of the area\n100m\nWidth of the area\n100m\nDrone step size\n10m\nCluster number\n4\nRatio of users of each cluster\n4:5:6:6\nTotal number of users, Nu\n1050\nDrone numbers, Nd\n1,2,4,8\nUser Height, hu\n1.5m\nDrone Height, hd\n30m\nalso follows a 2-dimensional Gaussian distribution. The users in\neach mobile communication cell follows the distribution in (12).\n(Xpl, Ypl) ∼N\n\u0000Xgc, σ2\ngc1, YGc, σ2\ngc2, ρgc\n\u0001\n(12)\nThe distribution fpl(x, y) of user locations follows a 2-\ndimensional Gaussian distribution, where Xgc, YGc is the cluster\ncenter. The standard deviation, including gc1, gc2, pl1 and pl2,\nare random variables.\n(Xcn, Ycn) ∼N\n\u0000Xpl, σ2\npl1, Ypl, σ2\npl2, ρpl\n\u0001\n(13)\nThe user distribution fcluster(xcn, ycn) for cluster n, in (13),\nfollows a 2-dimensional Gaussian distribution [27].\nfMS(x, y) =\n1\nk + 1\n \nuniform +\nk\nX\nn=1\nclustern\n!\n(14)\nLayer 1: 2-dimensional \nGaussian distributions\nLayer 2: 2-dimensional \nGaussian distributions\nLayer 3: 2-dimensional \nGaussian distributions\nLayer 4: 2-dimensional \nUniform distributions\n……\nFig. 3: User distribution\nThe distribution of users is deﬁned in four layers in our\nsimulation, with three 2-dimensional Gaussian layers (layer 1,\nlayer 2 and layer 3) and one uniform distribution layer (layer 4),\nas shown in Fig. 3. The orange cluster in Fig. 3 shows the biggest\nsize of the cluster compared with the green and purple clusters,\nwhere these three clusters simulate an emergency scenario with\nmany users. The fourth layer is a uniform distribution layer, where\nthe normal distribution is used to simulate a general scenario. The\nnumber of users at each Gaussian-distributed layer are 300, 250,\nand 200 respectively. The standard deviations of each cluster are\n10, 7, and 6 (14).\nFig. 4: Reward analysis of Q-learning\n3.2\nResults\nEach airborne base station chooses one action among ﬁve options\nat each iteration: move “east”, “west”, “south”, or “north”, or stay\nat the same spot. All base stations start at the right bottom corner.\nIf an airborne base station moves in a direction which increases\nthe number of users, the reward for this airborne base station will\nincrease. However, if this airborne base station is too close to the\nneighbouring base station, the neighbouring base station will lose\nusers. The global goal is to provide connectivity for the maximum\nnumber of users.\nWe compare the performance of the proposed method with\nthe following baseline models: Q-learning, SARSA, DQN and k-\nmeans. All the baseline models use the same input features, and\nare trained with the same iterations and the same learning rate.\n•\nQ-learning is a model-free off-policy algorithm [28],\nwhich provides agents with the capability of learning to\nact optimally in Markovian domains [29]. The reward\nmap is exploited and updated through the Q-table [30].\nFig. 4 shows the dispersion of the global rewards over\neach training episode in the airborne base station system\nusing the Q-learning algorithm. As the graph describes, the\nglobal rewards stabilize around values between 250 and\n300, with a median of 269 connected users. The maximum\nvalue of the global reward is not within the ﬁrst and third\nquartiles.\n•\nSARSA applies the same policy in both data generation\nand evaluation. SARSA follows a Markovian (history-\nindependent) structure [30], [31], [32]. Under this struc-\nture, the value function can be expressed in an algorithmic\nform known as the Bellman equation, and used to update\nQ-values in the lookup tables.\n6\n•\nA Deep-Q Network (DQN) can directly use deep learning\nmethods to bridge the divide between high-dimensional\ninputs and agent actions [33]. The Deep Neural Network\npredicts the Q value of the current or potential states\nand actions. Two convolutional neural networks constantly\nupdate its parameters to learn the optimal option.\n•\nA clustering network divides data objects with high sim-\nilarity into clusters [34]. Currently popular clustering\nmethods are k-means and mean-shift. In this work, the\nk-means algorithm is used as one of the baseline methods.\nIt is interesting to test whether a clustering simulation of\nthe user distribution favours the clustering method. The\nposition of the ABS is randomly chosen, and their moving\ndirection is controlled using L2 distance followed by (15).\naj =\n1\n|ci|\nX\nx∈ci\nx\n(15)\nThe positions of the ABSs aj are updated with the number\nof users x in each center. As shown in Fig. 5, the k-means\nmethod actually performed very poorly, especially in the\nscenarios with 4 and 8 airborne base stations.\nFig. 5 compares the performance of RR-GAN against the four\nother baseline methods. As illustrated in Fig. 5, the RR-GAN\nmethod attains the highest average percentage of connected users\nover all airborne base stations, even with an increased number of\nairborne base stations. Since other methods do not consider the\npositions of the other base stations and the uncertainty during the\ninitial learning period, RR-GAN can achieve the highest number\nof user connections.The theoretical best performance is calculated\nusing a heuristic method. The performance of random position\nexperiments is simulated when Airborne base stations moves\nfollowing a random walk distribution. It is worth noting that the\nDQN method cannot minimize the divergence of the generated\nsample distribution and over-ﬁts the training set by maximizing\nthe likelihood, which reduces model generalisability.\n3.3\nRobustness Comparison\nThe potential application environments for airborne base stations\nare complex, real-world domains, which need to overcome the\nproblems of high sample complexity and brittle convergence prop-\nerties, requiring less meticulous hyperparameter tuning. Therefore,\nhere different learning rates, greedy factors and user distribution\nare all tested; to allow the ABS to be fully aware of the neigh-\nbouring ABS, we also tested on 100 rounds, 1,000 rounds, 10,000\nrounds and 100,000 rounds. The reward map is generated based on\nvarious amounts of exploration, stopping if there is no higher total\nreward for the last n rounds of stochastic exploring. The different\nnumber of n rounds is used to test network performance. Fig 7\nshows a testing environment with 2 ABSs (airborne base station),\n4 ABSs, and 8 ABSs.\nFor all rounds, the time taken to reach maximum performance\nfollows the same trend, particularly so for 8 airborne base stations.\nFor 2 or 4 ABSs, a shorter exploring time may introduce oscilla-\ntions in performance, but all achieve a similar plateau within a\nlonger time frame.\nA comparison of robustness with other baseline algorithms can\nbe found in Fig. 8. The greedy parameter is a key parameter in\nevaluating the algorithm’s performance to value their new actions.\nDuring the changing of the value of greedy parameters, we can\ninvestigate how each method responds with learning information\nand timing needed for training. With all else held equal, the greedy\npolicy is tested under the values 0.7, 0.8, and 0.9, with the learning\nrate and discount factor ﬁxed to 0.1 and 0.5, respectively. The user\nenvironment is generated with the same random seed.\nAs shown in Fig. 8, ﬁnal performance improves as the learning\nrate approaches 1. Additionally, the larger the greedy factor is,\nthe faster the algorithm will converge. Although initial efﬁcacy is\nlow, RR-GAN has a steeper convergence rate when compared to\nother baseline methods and, most importantly, is the only method\nto reach the global optimal. It is noted the DQN method is not\nchosen in the greedy method comparison. The DQN method hasn’t\nperformed well since state transition probability is approximated\nwith one hidden layer neural network.\n3.4\nScalability Test\nTo investigate the impact of the number of user clusters on the\nperformance of RR-GAN, we ran a scalability test to investigate\nhow much the number of user clusters inevitably affected the RR-\nGAN method. As shown in Fig. 9, even with an increased number\nof user clusters, RR-GAN performed consistently well with 2\nairborne base stations, 4 airborne base stations and 8 airborne\nbase stations.\n3.5\nLearning from the Neighbouring Base Station\nAirborne base stations will interfere with each other if they are\ntoo close. When airborne base stations move close to a user\ncluster, the reward increases; however, if a neighbouring airborne\nbase station has already moved towards that centre, the reward\ndecreases. This experiment uses an environment of two airborne\nbase stations to demonstrate that our RR-GAN can be made aware\nof the neighbouring base station via checking the history rewards\nof neighbouring ABSs. As shown in Fig. 6, the ﬁrst channel of\nthe reward map is generated for the ﬁrst ABS. This airborne base\nstation gains awareness of the fact that the left user cluster has\nthe second optimal reward. For that reason, the network predicts\nthat the second airborne base station will move to that place in\nnext epoch, which results in the ﬁrst ABS not moving into that\ndirection. Similarly, the second ABS, with the help of the second\nchannel of the reward map, will predict and ﬁnd the global optimal\nfor the ﬁrst ABS and automatically reduce the rewards. In this\nmanner, RR-GAN can successfully predict the neighbouring ABS\ntrajectory to achieve the best global goal.\n4\nCONCLUSION AND DISCUSSION\nIn this paper, a Reward-Reinforced GAN (RR-GAN)-based\ngeneric framework for multi-agent systems is proposed, which are\npotentially generalised to any multi-agent system. We propose to\nuse a generator network as an implicit indication of the distribution\nof users, in order to achieve the maximum global goal. Our case\nstudy demonstrates how RR-GAN has the best system perfor-\nmance compared with other baseline methods (the source code\ncan be accessed via GitHub link: https:\\\\github.com\\Changgang-\nZheng\\Reward-reinforced-Generative-Adversarial-Network \\). In\nterms of future developments, while this work demonstrated the\nadvantages of using RR-GAN for multi-agent learning, several\nchallenges still remain:\n•\nConvergence Failures: The convergence properties for\ngenerative neural networks constitute an open research\n7\n4 airborne base stations\n8 airborne base stations\n2 airborne base stations\nFig. 5: Comparison among Q-learning, SARSA, k-means, DQN and RR-GAN\nUser Distribution\nDrone 1 Reward Map\nDrone 2 Reward Map\nCan Learn Conflicting Data\nFig. 6: Learning from neighbour\nquestion. The theoretical condition for the adversarial\nmodel is when the loss function is a class of convex\noptimisation algorithms [15]. In that case, the adversar-\nial model can be guaranteed to ﬁnd a unique solution.\nHowever, when neural networks are used for the generator\nand the discriminator (as in this paper), it is not always\nguaranteed to converge to a unique solution. We used\nNash equilibriums from game theory to demonstrate the\npossibility of converging for an adversarial model when a\nsmall change in probabilities for discriminator leads to a\nsituation where two conditions hold: the generator did not\nchange and has no better strategy in the new environment.\nAt the training stage, it is a challenge to modify GAN\ndesign so that the discriminator can be trained optimally,\navoiding the issue of convergence failures. The current\nresearch is investigate how to move away from unimodal\ndistributions as a natural relaxation to solve potential\nconvergence failures [35].\n•\nUncertainty Quantiﬁcation: Accurately estimating user\nmovements is commonly based on Bayesian methods,\nwhich introduces epistemic uncertainty into the model pa-\nrameters (cluster centres and random seeds) [36]. Gaussian\nprocesses present issues of model inadequacy and param-\neter uncertainty when scaling to high-dimensional prob-\nlems. Our RR-GAN network creates out-of-distribution\nsamples so that classiﬁers can be explicitly taught about\nuncertain inputs. We evidence the complications in the\ntraining process from computer simulations, the impli-\ncations of which for real-life experimental data are still\nunknown.\n•\nExplanation of uncertainty:\nCreating explanations for\nthe causes of model uncertainty [37] is a relatively under-\nexplored area. In a GAN, uncertainty may arise because an\ninput is unlike the training data and has been constrained\nby a set of known features in a previous unseen combina-\ntion. This type of explanation has only been very recently\nexplored by Merric and Taly to calculate the variance of\nShapley values [38], despite the fact that it remains a\nchallenging research area which may produce important\nconsequences for assessing multi-agent learning systems.\n•\nInterpretability: Although we demonstrated that RR-\nGAN works as a stochastic estimation to improve system\nperformance, there remain issues with interpretability. One\nsolution is to use models that are intrinsically interpretable,\nsuch as logistic regression, so that an accurate explanation\ncan be produced. However, the increasing states of multi-\nagent will create the issue of retrieving historical samples:\nconsequently, in the future we will investigate attempts to\nprovide solutions for checking the state of every times-\ntamp: for instance, using queries on temporal graphs when\ndealing with increasing large state spaces [39].\nACKNOWLEDGEMENTS\nWe would like offer our sincerest gratitude to Paulo Valente Klaine\nand João P.B. Nadas, who provided an initial discussion for this\nproject.\n8\n4 airborne base stations\n8 airborne base stations\n2 airborne base stations\nFig. 7: Robustness comparison with various greedy methods\n4 airborne base stations\n8 airborne base stations\n2 airborne base stations\nFig. 8: Robustness comparison among Q-learning, SARSA, K-means, DQN and RR-GAN with various greedy methods\n4 airborne base stations\n8 airborne base stations\n2 airborne base stations\nFig. 9: Robustness comparison among environment with different amount center\n9\nREFERENCES\n[1]\nF. L. Da Silva, P. Hernandez-Leal, B. Kartal, and M. E. Taylor,\n“Uncertainty-aware action advising for deep reinforcement learning\nagents.” in AAAI, 2020, pp. 5792–5799.\n[2]\nN. K. Long, K. Sammut, D. Sgarioto, M. Garratt, and H. A. Abbass,\n“A comprehensive review of shepherding as a bio-inspired swarm-\nrobotics guidance approach,” IEEE Transactions on Emerging Topics in\nComputational Intelligence, 2020.\n[3]\nY. Liu, X. Wang, G. Boudreau, A. B. Sediq, and H. Abou-zeid, “Deep\nlearning based hotspot prediction and beam management for adaptive\nvirtual small cell in 5g networks,” IEEE Transactions on Emerging Topics\nin Computational Intelligence, vol. 4, pp. 83–94, 2020.\n[4]\nS. Russell, “Learning agents for uncertain environments,” in Proceedings\nof the eleventh annual conference on Computational learning theory,\n1998, pp. 101–103.\n[5]\nA. Y. Ng, S. J. Russell et al., “Algorithms for inverse reinforcement\nlearning.” in Icml, vol. 1, 2000, p. 2.\n[6]\nC. L. Baker, R. Saxe, and J. B. Tenenbaum, “Action understanding as\ninverse planning,” Cognition, vol. 113, no. 3, pp. 329–349, 2009.\n[7]\nA. Coates, P. Abbeel, and A. Y. Ng, “Apprenticeship learning for\nhelicopter control,” Communications of the ACM, vol. 52, no. 7, pp.\n97–105, 2009.\n[8]\nB. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of\nrobot learning from demonstration,” Robotics and autonomous systems,\nvol. 57, no. 5, pp. 469–483, 2009.\n[9]\nA. R. Shirazi and Y. Jin, “A strategy for self-organized coordinated mo-\ntion of a swarm of minimalist robots,” IEEE Transactions on Emerging\nTopics in Computational Intelligence, vol. 1, no. 5, pp. 326–338, 2017.\n[10] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in\nAdvances in neural information processing systems, 2016, pp. 4565–\n4573.\n[11] G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias,\nand A. Aspuru-Guzik, “Objective-reinforced generative adversarial\nnetworks (organ) for sequence generation models,” arXiv preprint\narXiv:1705.10843, 2017.\n[12] R. E. Bellman, “Dynamic programming,” Princeton University Press,\n1957.\n[13] L. Mescheder, S. Nowozin, and A. Geiger, “The numerics of gans,” arXiv\npreprint arXiv:1705.10461, 2017.\n[14] M. Arjovsky and L. Bottou., “Towards principled methods for training\ngenerative adversarial networks,” arXiv:1701.04862, 2017.\n[15] Z. Pan, W. Yu, B. Wang, H. Xie, V. S. Sheng, J. Lei, and S. Kwong, “Loss\nfunctions of generative adversarial networks (gans): Opportunities and\nchallenges,” IEEE Transactions on Emerging Topics in Computational\nIntelligence, 2020.\n[16] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” 3rd International Conference on Learning Repre-\nsentations, ICLR 2015 - Conference Track Proceedings, pp. 1–11, 2015.\n[17] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[18] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\nwith conditional adversarial networks,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 1125–\n1134.\n[19] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks,” in Proceedings\nof the IEEE international conference on computer vision, 2017, pp.\n2223–2232.\n[20] J. Li, H. Ma, and M. Tomizuka, “Interaction-aware multi-agent tracking\nand probabilistic behavior prediction via adversarial learning,” in 2019\nInternational Conference on Robotics and Automation (ICRA).\nIEEE,\n2019, pp. 6658–6664.\n[21] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang, “Generalization\nand equilibrium in generative adversarial nets (gans),” arXiv preprint\narXiv:1703.00573, 2017.\n[22] F. Afroz, R. Subramanian, R. Heidary, K. Sandrasegaran, and S. Ahmed,\n“Sinr, rsrp, rssi and rsrq measurements in long term evolution networks,”\nInternational Journal of Wireless & Mobile Networks, 2015.\n[23] M. La Rocca, “Rsrp and rsrq measurement in lte,” laroccasolutions\nTechnology & Services, Feb, vol. 2, p. 9, 2015.\n[24] F. Ricciato, P. Widhalm, M. Craglia, and F. Pantisano, Estimating\npopulation density distribution from network-based mobile phone data.\nPublications Ofﬁce of the European Union, 2015.\n[25] E. Cho, S. A. Myers, and J. Leskovec, “Friendship and mobility: user\nmovement in location-based social networks,” in Proceedings of the 17th\nACM SIGKDD international conference on Knowledge discovery and\ndata mining.\nACM, 2011, pp. 1082–1090.\n[26] H. Gao, J. Tang, and H. Liu, “Exploring social-historical ties on location-\nbased social networks,” in Sixth International AAAI Conference on\nWeblogs and Social Media, 2012.\n[27] K. Murphy, “Conjugate bayesian analysis of the gaussian distribution,”\npp. 1–28, 11 2007.\n[28] R. S. Sutton, “Generalization in reinforcement learning: Successful\nexamples using sparse coarse coding,” in Advances in neural information\nprocessing systems, 1996, pp. 1038–1044.\n[29] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Machine Learning,\nvol. 8, no. 3-4, pp. 279–292, 1992.\n[30] E. A. Petter, S. J. Gershman, and W. H. Meck, “Integrating models of\ninterval timing and reinforcement learning,” Trends in cognitive sciences,\nvol. 22, no. 10, pp. 911–922, 2018.\n[31] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[32] V. G. Lopez and F. L. Lewis, “Dynamic multiobjective control for\ncontinuous-time systems using reinforcement learning,” IEEE Transac-\ntions on Automatic Control, vol. 64, no. 7, pp. 2869–2874, 2019.\n[33] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al.,\n“Human-level control through deep reinforcement learning,” nature, vol.\n518, no. 7540, pp. 529–533, 2015.\n[34] A. Akarsu and T. Girici, “Fairness aware multiple drone base station\ndeployment,” IET Communications, vol. 12, no. 4, pp. 425–431, 2017.\n[35] J. Li, A. Madry, J. Peebles, and L. Schmidt, “On the limitations of ﬁrst-\norder approximation in gan dynamics,” in International Conference on\nMachine Learning.\nPMLR, 2018, pp. 3005–3013.\n[36] J. R. van Dorp, “A dependent project evaluation and review technique: A\nbayesian network approach,” European Journal of Operational Research,\nvol. 280, no. 2, pp. 689–706, 2020.\n[37] S. Yang, K. Wong-Lin, I. Rano, and A. Lindsay, “A single chip system for\nsensor data fusion based on a drift-diffusion model,” in 2017 Intelligent\nSystems Conference (IntelliSys).\nIEEE, 2017, pp. 198–201.\n[38] L. Merrick and A. Taly, “The explanation game: Explaining ma-\nchine learning models with cooperative game theory,” arXiv preprint\narXiv:1909.08128, 2019.\n[39] J. M. Parra-Ullauri, A. García-Domínguez, L. H. García-Paucar, and\nN. Bencomo, “Temporal models for history-aware explainability,” in\nProceedings of the 12th System Analysis and Modelling Conference,\n2020, pp. 155–164.\n",
  "categories": [
    "cs.MA"
  ],
  "published": "2021-03-22",
  "updated": "2021-05-14"
}