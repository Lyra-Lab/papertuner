{
  "id": "http://arxiv.org/abs/2103.12982v1",
  "title": "From Semantic Retrieval to Pairwise Ranking: Applying Deep Learning in E-commerce Search",
  "authors": [
    "Rui Li",
    "Yunjiang Jiang",
    "Wenyun Yang",
    "Guoyu Tang",
    "Songlin Wang",
    "Chaoyi Ma",
    "Wei He",
    "Xi Xiong",
    "Yun Xiao",
    "Eric Yihong Zhao"
  ],
  "abstract": "We introduce deep learning models to the two most important stages in product\nsearch at JD.com, one of the largest e-commerce platforms in the world.\nSpecifically, we outline the design of a deep learning system that retrieves\nsemantically relevant items to a query within milliseconds, and a pairwise deep\nre-ranking system, which learns subtle user preferences. Compared to\ntraditional search systems, the proposed approaches are better at semantic\nretrieval and personalized ranking, achieving significant improvements.",
  "text": "From Semantic Retrieval to Pairwise Ranking: Applying Deep\nLearning in E-commerce Search\nRui Li, Yunjiang Jiang, Wenyun Yang, Guoyu Tang, Songlin Wang, Chaoyi Ma, Wei He, Xi Xiong,\nYun Xiao, Eric Yihong Zhao\nJD.com, Mountain View, CA\n{rui.li, yunjiang.jiang, wenyun.yang, tangguoyu, wangsonglin3, machaoyi, hewei92, xiongxi, xiaoyun1, ericzhao}@jd.com\nABSTRACT\nWe introduce deep learning models to the two most important\nstages in product search at JD.com, one of the largest e-commerce\nplatforms in the world. Specifically, we outline the design of a deep\nlearning system that retrieves semantically relevant items to a query\nwithin milliseconds, and a pairwise deep re-ranking system, which\nlearns subtle user preferences. Compared to traditional search sys-\ntems, the proposed approaches are better at semantic retrieval and\npersonalized ranking, achieving significant improvements.\nCCS CONCEPTS\nâ€¢ Information systems â†’Information retrieval.\nKEYWORDS\nNeural networks; Semantic Search; Personalized Ranking\n1\nINTRODUCTION\nOver past decades, online shopping platforms have become ubiqui-\ntous in peopleâ€™s daily life. In a typical e-commerce search engine,\ntwo major stages are involved to answer a user query, namely\nCandidate Retrieval, which uses inverted indexes to efficiently\nretrieve candidates based on term matching, and Candidate Rank-\ning, which orders those candidates based on factors, such as rel-\nevance and predicted conversion ratio. Here we share our expe-\nriences and promising results of applying deep learning for both\nstages in e-commerce search at JD.com.\nFor candidate retrieval, we introduce the Deep Semantic Re-\ntrieval (DSR) system, in addition to traditional term matching based\nretrieval systems. DSR encodes queries and items into a semantic\nspace based on click logs and human supervision data, and uses an\nefficient K-nearest neighbor search algorithm to retrieve relevant\nitems from billions of candidates within milliseconds. For candidate\nranking, we introduce the Deep Pairwise Ranking (DPR) system\nto leverage both hand-crafted numerical features and raw sparse\nfeatures. DPR uses a Siamese architecture to effectively learn prefer-\nence between a pair of items under the same query from billions of\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR â€™19, July 21â€“25, 2019, Paris, France\nÂ© 2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-6172-9/19/07...$15.00\nhttps://doi.org/10.1145/3331184.3331434\nDOT\nProduct\nOffline Model Training\nTraining Data (Click Logs)\nItem Features\nNormalize\nMulti layer \nReLU\nItem Embedding Model\nItem Corpus\nQuery Features\nNormalize\nQuery Embedding Model\nItem  Embedding Model\nQuery Embedding Model\nEmbedding Indexes\nQuery\nItems\nOnline Serving\nQuery Embedding\nOffline Indexing\nIndexing\nMulti layer \nReLU\nFigure 1: Overview of deep semantic retrieval system.\nsearch log data and efficiently score each item individually (without\nenumerating all the pairs) online.\n2\nDEEP SEMANTIC RETRIEVAL\nThe most critical challenge for this stage is enabling semantic re-\ntrieval beyond exact term matches. Based on our analysis, this\nproblem impacts around 20% of our search traffic.\nSystem Overview Neural network models naturally address this\nproblem via semantic term embeddings. While many deep neural\nnetwork models (e.g., DSSM [2]) have been proposed for search\nrelevance ranking, our DSR system explores the rare territory of\ncandidate retrieval. As Figure 1 shows, DSR has three parts.\nOffline Training process learns a query, item two tower model. Each\ntower consists of 1) an input layer that concatenates input features\nof a query or an item, 2) an encoding ReLU multi-layer, and 3) a\nnormalization layer to ensure L2 normalized output embeddings.\nOffline Indexing module computes all the item embeddings from\nthe item corpus, then builds an embedding index offline for efficient\nonline embedding retrieval.\nOnline Serving module loads the query part of the two tower model\nand computes an embedding for any query, which then retrieves\nthe ğ¾nearest/most similar items from the item embedding index.\nModel Overview Now, we give a overview of our proposed model.\nGiven a training set D = {(ğ‘ğ‘–,ğ‘ +\nğ‘—,ğ‘ âˆ’\nğ‘˜) | ğ‘–, ğ‘—,ğ‘˜}, where ğ‘ğ‘–stands for\na query, ğ‘ +\nğ‘—stands for a positive item that is relevant to ğ‘ğ‘–, and ğ‘ âˆ’\nğ‘˜\nstands for a negative item that is irrelevant to ğ‘ğ‘–, we learn the query\nand item towers jointly by minimizing the following loss function\nğ¿(D) =\nâˆ‘ï¸\n(ğ‘ğ‘–,ğ‘ +\nğ‘—,ğ‘ âˆ’\nğ‘˜) âˆˆD\nmax\n\u0010\n0,ğ›¿âˆ’\n\u0010\nğ‘„(ğ‘ğ‘–)ğ‘‡ğ‘†(ğ‘ +\nğ‘—) âˆ’ğ‘„(ğ‘ğ‘–)ğ‘‡ğ‘†(ğ‘ âˆ’\nğ‘˜)\n\u0011\u0011\narXiv:2103.12982v1  [cs.IR]  24 Mar 2021\nUCTR\nUCVR\nGMV\nQRR\noverall\n+1.61%\n+0.98%\n+0.54%\nâˆ’4.34%\nlong tail\n+9.7%\n+10.03%\n+7.5%\nâˆ’9.99%\nTable 1: Online A/B test improvements.\nindexing (sec.)\nsearch (ms)\nQPS\nCPU\n3453\n9.92\n100\nGPU\n499\n0.74\n1422\nTable 2: Efficiency for indexing and serving.\nquery\nretrieved item\nå¥¶ç²‰å¤§ç«¥\nç¾èµè‡£å®‰å„¿å¥A+ 4æ®µ\n(milk powder big kid)\n(Enfamil A+ level-4 for 3-6 yr old )\nå­¦ä¹ è‡ªç”±æ³³å™¨æ\nè‹±å‘/yingfa åˆ’è‡‚\n\u0012learn free-style swim-\nming equipment\n\u0013\n(yingfa hand paddle )\nTable 3: Good cases from DSR\nwhere ğ›¿â‰¥0 is the margin used in hinge loss, ğ‘„(Â·) denotes the\nquery tower, ğ‘†(Â·) denotes the item tower and the superscript Â·ğ‘‡\ndenotes transpose operation. Intuitively, this function employs the\nhinge loss to encourage the dot product between a query ğ‘ğ‘–and a\npositive item ğ‘ +\nğ‘—to be larger than the product between the query ğ‘ğ‘–\nand a negative item ğ‘ âˆ’\nğ‘˜by a certain margin ğ›¿.\nExperimental Results Table 1 shows relative improvement of\nDSR in online A/B tests in terms of four core business metrics,\nincluding user click through rate (UCTR), user conversion rate\n(UCVR), and gross merchandise value (GMV), as well as query\nrewrite rate (QRR), which is believed to be a good indicator of\nsearch satisfaction. Table 2 reports the time consumed by DSR for\nindexing and searching 120 million items with Nvidia Tesla P40\nGPU and Intel 64-core CPU: GPU achieves ~10x speed up. Table 3\nshows a few sample queries to better illustrate the power of DSR at\nbridging semantic gaps between queries and relevant items.\n3\nDEEP PAIRWISE RANKING\nTraditional e-commerce search systems apply gradient boosted\ntrees (xgboost) models [1] to rank products based on predicted con-\nversion rates. Relying mainly on numeric features, those models can-\nnot effectively explore sparse features, such as userâ€™s click/purchase\nhistory. Deep learning(DL) naturally address this problem as it can\nleverage those raw features via embeddings. However, to apply DL\nin ranking, we need to address 1) big computation overhead and 2)\nsufficient amount of good training data required by DL.\nOverview To address those challenges, we first introduce a re-\nranking stage, which only scores the top K (100) items selected\nfrom earlier phases with a complex DL model, to reduce the re-\nquired computation, as we observe that 90% of clicks/orders are\nwithin top 75 positions in existing results. Second, we choose to\ntrain a pairwise ranking model with a large amount of noisy logs.\nWhile the top 100 results are usually all reasonably relevant, the\nfocus is personalized preference, which is hard to quantify. Logs,\nthough abundant, are only good for approximating theses subtle\npreferences among presented items. Thus, instead of learning con-\nversion rate, as most e-commerce ranking models do, we learn user\npreference between a pair of items for a given query.\nModel Details In order to effectively learn preference between\na pair of items under the same query offline and efficiently score\nFigure 2: Simplified Siamese ranking model.\nsession AUC\norder NDCG@5\nXgboost\n0.852\n0.522\nSiamese DL\n0.870\n0.573\nTable 4: Offline validation metrics.\nUCTR\nUCVR\nGMV\nAOP\noverall\n+1.16%\n+1.99%\n+1.04%\nâˆ’2.10%\nTable 5: Online A/B test improvements.\neach item individually (without enumerating all the pairs) online,\nwe design a pairwise Siamese model, as shown in Figure 2. The\ntraining data consists of triples (ğ‘,ğ‘,ğ‘) where ğ‘,ğ‘are two items\nco-occurring under the same session with query ğ‘. The Siamese\nstructure has two identical towers (sharing the same parameters)\nfor item ğ‘and item ğ‘. Each tower takes all the input features from\nuser, query, and item, goes through 3 ReLU layers, and outputs a\nlogit. The difference of the two logits is then combined with the\nbinary label in a cross-entropy loss function. During online serving,\nonly one of the two identical towers is used to score each item.\nThe model uses the following features: 1) hand picked numeric\nfeatures, such as userâ€™s purchasing power and past CTR/CVR/sale\nvolume, 2) raw text features for queries and items, tokenized into\nunigrams and bigrams, and 3) raw user action features, such as\nhistorical click streams. The latter two use sum-pooled embeddings.\nExperimental Results Our top-line validation metric is order-\nbased average session AUC. Our DPR model, which uses not only\nall the Xgboost features (about 130 total) but also sparse features,\nperforms significantly better than Xgboost methods (Table 4). Ta-\nble 5 shows results of online A/B tests. DPR significantly improves\nall the core business metrics, including UCTR, UCVR, GMV, and\naverage order position (AOP). As DPR uses orders as positive ex-\namples during our training, it achieves the most gain on UCVR.\nHowever, similar gains on UCTR/GMV are also observed.\nREFERENCES\n[1] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.\nIn KDD. ACM, 785â€“794.\n[2] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck.\n2013. Learning deep structured semantic models for web search using clickthrough\ndata. In CIKM. 2333â€“2338.\n",
  "categories": [
    "cs.IR",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-03-24",
  "updated": "2021-03-24"
}