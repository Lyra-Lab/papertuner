{
  "id": "http://arxiv.org/abs/2103.14580v1",
  "title": "Correcting Automated and Manual Speech Transcription Errors using Warped Language Models",
  "authors": [
    "Mahdi Namazifar",
    "John Malik",
    "Li Erran Li",
    "Gokhan Tur",
    "Dilek Hakkani Tür"
  ],
  "abstract": "Masked language models have revolutionized natural language processing\nsystems in the past few years. A recently introduced generalization of masked\nlanguage models called warped language models are trained to be more robust to\nthe types of errors that appear in automatic or manual transcriptions of spoken\nlanguage by exposing the language model to the same types of errors during\ntraining. In this work we propose a novel approach that takes advantage of the\nrobustness of warped language models to transcription noise for correcting\ntranscriptions of spoken language. We show that our proposed approach is able\nto achieve up to 10% reduction in word error rates of both automatic and manual\ntranscriptions of spoken language.",
  "text": "Correcting Automated and Manual Speech Transcription Errors using\nWarped Language Models\nMahdi Namazifar, John Malik, Li Erran Li, Gokhan Tur, Dilek Hakkani Tür\nAmazon Alexa AI\nmahdinam@amazon.com, jmmlik@amazon.com, lilimam@amazon.com, gokhatur@amazon.com,\nhakkanit@amazon.com\nAbstract\nMasked language models have revolutionized natural language\nprocessing systems in the past few years. A recently introduced\ngeneralization of masked language models called warped\nlanguage models are trained to be more robust to the types\nof errors that appear in automatic or manual transcriptions\nof spoken language by exposing the language model to the\nsame types of errors during training. In this work we propose\na novel approach that takes advantage of the robustness of\nwarped language models to transcription noise for correcting\ntranscriptions of spoken language. We show that our proposed\napproach is able to achieve up to 10% reduction in word error\nrates of both automatic and manual transcriptions of spoken\nlanguage.\n1. Introduction\nAutomatic Speech Recognition (ASR) has been an indispensable\ncomponent of human-machine interaction. Despite tremendous\nimprovements in the quality of ASR systems in recent years,\nthey are still imperfect [1, 2]. On the other hand, training ASR\nsystems requires large amounts of human transcribed spoken\nlanguage. These human transcriptions are expensive to acquire.\nAt the same time human transcriptions of spoken language\nare often not fully accurate due to human error. As a result,\nto reduce the number of mistakes in human transcriptions, it\nis common to have spoken sentences transcribed by multiple\ndifferent human transcribers, which signiﬁcantly adds to the cost\nof human transcription.\nWarped language models (WLMs) [3], variants of masked\nlanguage models (MLMs) [4] are designed to produce encoded\nrepresentations that are robust to the word-level errors commonly\nmade by ASR systems and human transcribers. That is due to\nthe fact that during the training of warped language models input\nsentences are corrupted by masking some of the tokens (MASK),\nreplacing some of the tokens with other random tokens (RAND),\ndropping some of the tokens (DROP), inserting random tokens\nin random positions in the sentences (INSERT), and keeping\nsome tokens intact (KEEP). Note that these operations (that are\nreferred to as warping operations) resemble the mistakes that are\nmade by ASR systems or by humans during the transcription\nof spoken language. Despite this resemblance, WLMs cannot\nbe directly used for ASR correction due to the lack distinction\nbetween different warping operations (more on this in section\n3). Inspired by the similarity between warping operations of\nWLMs and mistakes made by ASR systems, and in order to\nmake WLMs work for sentence correction, in this work, we\nﬁrst introduce a modiﬁcation of warped language models for\nautomatic sentence correction. We refer to this model as WLM\nsentence correction (WLM-SC). In this model, for each token\nof the input sentence, a warping operation as well as a token is\npredicted (Figure 1b) from which the corrected sentence could\nbe calculated. On the other hand, ASR systems often produce\nadditional hypotheses (referred to as ASR n-best list) for the\ntranscription of the spoken language, in addition to the single\nbest hypothesis. These additional hypotheses are rich sources\nof information on the spoken language. In this work, we also\nintroduce an approach to incorporate ASR n-best hypotheses\nin WLM-SC input as additional signals for sentence correction.\nOur proposed approach for each position in the input sentence\npredicts what changes should be made, which are independent\nof the language model of the ASR system. We then use this\napproach to correct ASR and human transcriptions of spoken\nlanguage.\nWe show through computational results that our\ncorrection approach reduces Word Error Rate (WER) of ASR\nand human transcriptions of spoken language by up to more than\n10%.\n2. Related Work\nWe brieﬂy review related works in correcting ASR and human\ntranscription of spoken language. Human transcription error\ncorrection makes use of either the acoustic model or the language\nmodel of an ASR system. Acoustic model based methods use the\nmismatch between the forced alignment of human transcription\nand posteriors of the acoustic classiﬁer to detect the phonetic\nerrors [5, 6]. There have also been several approaches to improve\nASR output including hypothesis re-scoring, post-processing and\nleveraging high level semantic information with joint ASR and\nSpoken Language Understanding (SLU) models [7, 8, 9, 10, 11].\nIn [12] the authors propose an ASR correction approach based\non maximum entropy language models to correct semantic and\nlexical errors. In [13, 14] ASR lattices are converted to word\nconfusion networks, and word error rates are improved by ﬁnding\nconsensus across alternative paths in the lattice. More recent\napproaches such as [15] use neural network based language\nmodels for ASR correction. The language model based sentence\ncorrection methods detect errors by ﬁnding the difference\nbetween ASR hypotheses and transcription [16, 6]. In contrast,\nin this work we introduce a modiﬁcation of warped language\nmodels trained with high quality ground truth transcription that\nfor each position in the input sentence predicts what changes\nshould be made, which are independent of the language model\nof the ASR system.\n3. Warped LM Sentence Correction\nMasked language models are trained by introducing certain types\nof noise to the input while the model is asked to recover the\noriginal input. More speciﬁcally, in training masked language\nmodels, ﬁrst a subset of tokens of the input sentences are\nrandomly selected. Some of these tokens go through the MASK\noperation, some go through the RAND operation, and some are\narXiv:2103.14580v1  [cs.CL]  26 Mar 2021\n(a) WLM architecture\n(b) WLM-SC architecture\nFigure 1: The original WLM and WLM-SC model architectures\nFigure 2: Example of sentence correction using modiﬁed WLM\nleft intact (KEEP operation) [4]. The model is then trained to\npredict the original token for each of the tokens in this subset,\ni.e. for a masked token, it should predict what was the token that\nwas masked, for a token that was replaced by another random\ntoken it should predict the original token, and for a token that\nwas kept intact it should predict the token itself.\nWarped language models [3] are identical to masked\nlanguage models except that they use two additional operations\nduring training to introduce noise to the input sentences. The\nﬁrst operation is DROP for which from the subset of the selected\ntokens some of the tokens are dropped and the model needs\nto predict the dropped tokens as the target for the token that\ncomes after the dropped token. The second new operation used\nin warped language models is INSERT for which some random\ntokens are inserted in the input sequence of tokens and for the\ninserted tokens the model needs to predict the special token\n[INSERT] as the target. Applying the set of MASK , DROP ,\nINSERT , RAND, and KEEP (referred to as warping operations)\nto a sentence constitutes what is referred to as warping a sentence.\nFigure 3 shows an example of such warping of a sentence.\nConsidering these warping operations, it is easy to show that\nany ASR or human transcription that includes mistakes could be\nachieved by applying a limited number of warping operations\non the correct transcription. As a result using warped language\nmodels for sentence correction intuitively seems like a viable\noption. One hurdle to do so however is the fact that although\nwarped language models can predict a token for every position\nin the input sentence, they cannot fully distinguish between all\nthe different warping operations. For instance if for a position\nthe model predicts a token that is different from the original\ntoken at that position, it would not be clear whether the model\nbelieves that there was a RAND operation at that position or a\nDROP operation at the previous position. In order to have a\nsentence correction model it is required to have not only the\ntokens for each position, but also the warping operation at every\nposition. Hence we introduced a modiﬁed version of warped\nFigure 3: Example of warping in warped language models\nborrowed from [3].\nlanguage models that does exactly that. Figure 1b depicts the\narchitecture of this model that we call WLM Sentence Correction\n(WLM-SC).\nThe warping operation prediction head in WLM-SC is a 5-\nway classiﬁer that predicts one of the warping operations for each\nposition that we predict a token for. To train this model, similar\nto the original warped language models, ﬁrst input sentences\nare warped, i.e. warping operations are applied to a subset\nof positions in the input sentences. Then the model is trained\nsuch that for each of these position it should predict a token\nand a warping operation that was applied at that position. The\ntraining loss is the sum of the token prediction loss and the\nwarping operation prediction loss. At evaluation time, the model\npredicts a token and a warping operation for every position\nof input sentences from which corrected sentences could be\nobtained. Figure 2 shows an example of how the proposed\nsentence correction algorithm works.\nFigure 4: WLM-SC with additional hypotheses\n4. WLM-SC with ASR Hypotheses\nIn the previous section, we outlined WLM-SC which is based on\na modiﬁcation of warped language models. In this framework,\nthe model receives a sentence as input and outputs a corrected\nsentence. One use case of sentence correction is when we deal\nwith transcriptions of spoken language. These transcriptions\ncould come from an ASR system or could be done by humans.\nIn both cases these transcriptions are noisy (i.e. contain mistakes)\nwhich makes the case for using sentence correction algorithms.\nFor this purpose these transcriptions could directly be fed into\nthe WLM-SC. However, in the case of ASR transcriptions, often\ntimes ASR output also includes ASR n-best hypotheses, which\ncould be seen as additional information that could be used for\ncorrecting the ASR transcription. On the other hand, for the case\nof human transcriptions, since running ASR on spoken language\nis relatively low cost, for correcting human transcriptions of\nspoken language we also get the ASR hypotheses of the spoken\nlanguage to use them in the correction process. In the rest of\nthis section, we discuss our proposed approach for correcting\na noisy transcription of spoken language, which could be ASR\ntop hypothesis or human transcription (referred to both as top\nhypothesis), with additional ASR hypotheses (referred to as\nadditional hypotheses)1.\nThe WLM-SC model that was introduced in the previous\nsection (Figure 1b) is ﬁne-tuned to perform error corrections\nfor the top hypothesis with the additional information that\nis provided by the additional hypotheses (we use up to 4\nadditional hypotheses). Each hypothesis is considered as a\nwarping of the ground truth of the spoken utterance. Additional\nhypotheses are ﬁrst aligned with the top hypothesis. Then for\neach hypothesis the minimum number of INSERT, DROP, and\nRAND warping operations needed to transform the hypothesis to\nthe ground truth transcription is calculated and are used as labels\nfor ﬁne tuning of the WLM-SC model. Note that instead of\nrandomly creating warping operations here we use the warping\noperations that exist in the noisy input. All the hypotheses (top\nand additional) and their labels are ﬁrst padded to a uniform\nlength and then are concatenated. The input then is embedded\nusing token and position embedding [17] as usual. The only\n1In case of human annotation these are ASR top hypothesis along\nwith additional ASR hypotheses\ndifference here is that the position embedding layer of the pre-\ntrained WLM-SC model is modiﬁed so that the i-th token of\neach tokenized hypothesis is given the same position vector\nfor transformers position embeddings, as the i-th token of the\ntokenized transcription (note that in the “Position Embeddings”\nlayer in Figure 4 the same sequence of vectors is repeated). The\nposition vectors are then allowed to be learned independently\nduring training. At training time the loss is calculated over all\ntokens (for all of the hypotheses). During inference however the\nprediction is done only for the top hypothesis. Figure 4 depicts\nhow ASR hypotheses are incorporated into the WLM-SC model.\n5. Computational Results\nAs was mentioned earlier, human transcriptions of spoken\nlanguage are noisy (include mistakes). In order to establish\na ground truth for our experiments, we use the consensus human\ntranscription of at least three independent transcribers. We refer\nto these consensus transcriptions as golden transcriptions and\nwe assume that golden transcriptions are error free. To evaluate\nthe effectiveness of WLM-SC with additional hypotheses for\nASR outputs and human transcriptions of spoken language, we\ncalculate the word error rate (WER) metric over a held out\ntest set of 100,000 utterances that have golden transcriptions.\nSpeciﬁcally, to calculate WER the sum of the Levenshtein\ndistances between all of the candidate corrections and their\ncorresponding golden transcriptions is divided by the sum of the\nlengths of all of the golden transcriptions. The ASR system used\nin this work is one of our internal ASR systems.\nWe ﬁrst train the modiﬁed WLM (WLM-SC) that was\nintroduced in Section 3 on the English Wikipedia corpus. Next\nwe ﬁne tune this model with the training data of spoken language\ntranscriptions (ASR or human transcriptions) in the architecture\nthat we introduced in Section 4 (Figure 4). We calculate the WER\nof the corrected test set sentences both with WLM-SC as well as\nwith WLM-SC with additional hypotheses, and compare them\nwith the original WER of transcriptions (ASR and human) before\nthe correction. The numbers are shown in Table 3. It should\nbe mentioned that oracle WER of the ASR n-best (i.e. if the\nhypothesis from the ASR n-best with the lowest WER is picked)\nis 11.17. From the table we can see that for ASR transcriptions\nthe WER is reduced from 15.11% to 14.32% for WLM-SC, and\nASR Transcription WER\nHuman Transcription WER\nASR Conﬁdence\nBin Size\nOriginal\nCorrected\nRel. Diff.\nOriginal\nCorrected\nRel. Diff.\nScore\n%\n%\n[0.0, 0.2]\n12074\n38.17\n37.26\n2.4\n29.01\n28.28\n2.5\n(0.2, 0.4]\n12280\n23.99\n22.49\n6.3\n19.11\n17.91\n6.3\n(0.4, 0.6]\n14208\n16.26\n14.66\n9.8\n14.03\n12.52\n10.8\n(0.6, 0.8]\n24463\n10.64\n9.40\n11.7\n10.15\n8.31\n18.1\n(0.8, 1.0]\n59050\n5.19\n4.95\n4.6\n5.56\n4.34\n21.9\nTable 1: Stratiﬁcation of the test set by ASR model conﬁdence score\nTranscription WER\nASR WER Bin\nBin Size\nASR\nASR WLM-SC\nHuman\nHuman WLM-SC\n+ Add. Hyp.\n+ Add. Hyp.\n0.00\n81185\n0.00\n0.35\n4.65\n2.36\n(0.00, 0.25)\n16151\n16.80\n14.87\n12.83\n12.04\n[0.25, 0.50)\n13205\n38.87\n34.98\n25.08\n25.37\n[0.50, 1.00)\n9764\n80.56\n78.32\n45.59\n47.03\n[1.00, ∞]\n2963\n238.46\n228.75\n132.36\n145.22\nTable 2: Word error rate (WER) percentage for ASR and human transcription, as well as for WLM-SC with additional hypotheses\nsentence correction broken down for ASR word error rates. The numbers show that improvements in ASR transcription WER are\napproximately uniform across ASR WER bins, while improvements in human transcription WER are observed primarily when the ASR\nhypothesis is correct and can be leveraged to correct word errors.\nOriginal\nWLM-SC\nWLM-SC\n+ Add. Hyp.\nASR\n15.11\n14.32\n14.24\nHuman\n12.38\n12.44\n11.07\nTable 3: Word error rate (WER) percentage between candidate\ntranscriptions in the test set and their golden transcriptions.\nOracle WER of the ASR n-best (which means if the hypothesis\nfrom the ASR n-best with the lowest WER is picked) is 11.17.\nto 14.24% for WLM-SC with additional hypotheses (a relative\nreduction of 4.6%). On the other hand, applying WLM-SC\non human transcriptions in fact slightly increases WER from\n12.38% to 12.44%. However when we apply WLM-SC with\nadditional hypotheses on human transcriptions WER is reduced\nfrom 12.38 to 11.07, which is a relative improvement of around\n10.6%. Another way to interpret these numbers is that when\nhumans make mistakes, the transcriptions are still ﬁne according\nto the WLM-SC model. However, these transcriptions may not\nbe ﬁne according to the acoustics information that is captured in\nthe additional ASR hypotheses (ASR n-best).\nIn Table 1, we show WER of the WLM-SC with additional\nhypotheses applied to ASR and human transcriptions partitioned\naccording to the ASR model conﬁdence score of the ASR\n1-best hypothesis. These conﬁdence scores (values between\n0 and 1) are partitioned into 5 bins of same length.\nWe\nobserve WER improvement across all groups of utterances.\nThe average relative improvement is 6.95% for ASR 1-best\ncorrection and 11.93% for human transcription correction. It is\nnotable that for ASR conﬁdence scores between 0.6 and 0.8 our\napproach reduces WER for ASR transcriptions by 11.7% which\nis highest among all bins. For human transcription, the highest\nimprovement is when the ASR conﬁdence scores are between\n0.8 and 1.0 where the overall WER drops from 5.56 to 4.34,\nwhich is 21.9% improvement.\nIn another set of results we study improvements made\nby WLM-SC with additional hypotheses for utterances with\ndifferent ASR WER ranges. Table 2 shows WER percentages\nfor ASR and human transcriptions as well as WER percentages\nfor ASR and human transcriptions corrections with WLM-SC\nwith additional hypotheses. The numbers in this table show that\nimprovements in ASR transcription WER are observed across all\nASR WER bins, while improvements in human transcription\nWER are observed primarily when the ASR hypothesis is\ncorrect and can be leveraged to correct word errors.\nThe\nnumbers of the last row of Table 1 suggest that improvements\nto human transcriptions could be attributed to ASR additional\nhypotheses. The results of Table 2 on the other hand suggest\nthat human transcriptions could be improved if the WER of\nASR transcriptions are low, i.e. ASR transcriptions are strong\nhypotheses.\n6. Conclusions\nTranscription of spoken language using Automatic Speech\nRecognition (ASR) plays a fundamental role in human-machine\ninteraction through natural language. Errors in ASR may result\nin further issues in natural language understanding and degrade\nthe user-perceived quality of interactions. These ASR systems\nthemselves are trained using manual transcriptions of spoken\nlanguage done by humans, which often include mistakes due\nto human errors. Therefore, improving the quality of human\ntranscriptions of spoken language could improve the quality of\nASR systems, which in turn positively impacts users’ experience\nwhen interacting with machines.\nIn this work we establish a strong connection between the\noperations that are used to introduce noise in the training of\nWLMs and the mistakes that are made in transcriptions of spoken\nlanguage done by humans or by ASR systems. Based upon this\nconnection we introduce WLM-SC as a modiﬁcation of WLM\nthat are language models that could be directly used for sentence\ncorrection. We then introduce a novel approach to incorporate\nASR hypotheses for correcting transcriptions (both human and\nASR) of spoken language using WLM-SC. In our experimental\nresults we show that our proposed approach not only is capable of\nimproving word error rate of ASR systems, it even reduces word\nerror rate of human transcriptions by up to 10%. Moreover our\nresults suggest that the gains on human transcription corrections\ngenerally come from the additional ASR hypotheses (which\ncould be interpreted as signals from the audio) and are only\nrealized when ASR hypotheses are of higher quality.\n7. References\n[1] L. Dong, S. Xu, and B. Xu, “Speech-transformer: A no-recurrence\nsequence-to-sequence model for speech recognition,” in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2018, pp. 5884–5888.\n[2] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang, C. Fuegen, G. Zweig,\nand M. L. Seltzer, “Transformer-based acoustic modeling for\nhybrid speech recognition,” in ICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2020, pp. 6874–6878.\n[3] M. Namazifar, G. Tur, and D. H. Tür, “Warped language models\nfor noise robust language understanding,” 2020.\n[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,” in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers).\nMinneapolis, Minnesota: Association for\nComputational Linguistics, Jun. 2019, pp. 4171–4186. [Online].\nAvailable: https://www.aclweb.org/anthology/N19-1423\n[5] J. Yang, L. Ondel, V. Manohar, and H. Hermansky, “Towards\nautomatic methods to detect errors in transcriptions of speech\nrecordings,”\nin ICASSP 2019 - 2019 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\n2019, pp. 3747–3751.\n[6] X. Wang, J. Yang, R. Li, S. Sadhu, and H. Hermansky,\n“Exploring methods for the automatic detection of errors in\nmanual transcription,” CoRR, vol. abs/1904.04294, 2019. [Online].\nAvailable: http://arxiv.org/abs/1904.04294\n[7] Y. Weng, S. S. Miryala, C. Khatri, R. Wang, H. Zheng, P. Molino,\nM. Namazifar, A. Papangelis, H. Williams, F. Bell, and G. Tur,\n“Joint contextual modeling for ASR correction and language\nunderstanding,” in ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\n2020, pp. 6349–6353.\n[8] P. Ponnusamy, A. R. Ghias, C. Guo, and R. Sarikaya, “Feedback-\nbased self-learning in large-scale conversational AI agents,” in\nThe Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artiﬁcial Intelligence,\nEAAI 2020,\nNew York,\nNY, USA, February 7-12,\n2020.\nAAAI Press, 2020, pp. 13 180–13 187. [Online]. Available:\nhttps://aaai.org/ojs/index.php/AAAI/article/view/7022\n[9] D. Zheng, Z. Chen, Y. Wu, and K. Yu, “Directed automatic\nspeech transcription error correction using bidirectional LSTM,” in\n2016 10th International Symposium on Chinese Spoken Language\nProcessing (ISCSLP), 2016, pp. 1–5.\n[10] Z. Chen, X. Fan, Y. Ling, and C. Guo, “Pre-training for query\nrewriting in a spoken language understanding system,” in ICASSP\n2020 - 2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2020, pp. 7969–7973.\n[11] Y. Song, D. Jiang, X. Zhao, Q. Xu, R. C. Wong, L. Fan, and\nQ. Yang, “L2RS: A learning-to-rescore mechanism for automatic\nspeech recognition,” CoRR, vol. abs/1910.11496, 2019. [Online].\nAvailable: http://arxiv.org/abs/1910.11496\n[12] S. Jung, M. Jeong, and G. Lee, “Speech recognition error correction\nusing maximum entropy language model,” in INTERSPEECH,\n2004.\n[13] L. Mangu, E. Brill, and A. Stolcke, “Finding consensus in speech\nrecognition: word error minimization and other applications of\nconfusion networks,” Computer Speech & Language, vol. 14,\nno. 4, pp. 373 – 400, 2000. [Online]. Available: http://www.\nsciencedirect.com/science/article/pii/S0885230800901529\n[14] D. Hakkani-Tur and G. Riccardi, “A general algorithm for\nword graph matrix decomposition,” in 2003 IEEE International\nConference on Acoustics, Speech, and Signal Processing, 2003.\nProceedings. (ICASSP ’03)., vol. 1, 2003, pp. I–I.\n[15] H. Xu, T. Chen, D. Gao, Y. Wang, K. Li, N. Goel, Y. Carmiel,\nD. Povey, and S. Khudanpur, “A pruned RNNLM lattice-\nrescoring algorithm for automatic speech recognition,” in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2018, pp. 5929–5933.\n[16] T. J. Hazen, “Automatic alignment and error correction of\nhuman generated transcripts for long speech recordings,” in\nINTERSPEECH 2006 - ICSLP, Ninth International Conference on\nSpoken Language Processing, Pittsburgh, PA, USA, September\n17-21, 2006.\nISCA, 2006. [Online]. Available: http://www.\nisca-speech.org/archive/interspeech_2006/i06_1258.html\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett, Eds., vol. 30.\nCurran Associates, Inc., 2017, pp.\n5998–6008. [Online]. Available: https://proceedings.neurips.cc/\npaper/2017/ﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-03-26",
  "updated": "2021-03-26"
}