{
  "id": "http://arxiv.org/abs/2103.14797v2",
  "title": "Unsupervised Self-Training for Sentiment Analysis of Code-Switched Data",
  "authors": [
    "Akshat Gupta",
    "Sargam Menghani",
    "Sai Krishna Rallabandi",
    "Alan W Black"
  ],
  "abstract": "Sentiment analysis is an important task in understanding social media content\nlike customer reviews, Twitter and Facebook feeds etc. In multilingual\ncommunities around the world, a large amount of social media text is\ncharacterized by the presence of Code-Switching. Thus, it has become important\nto build models that can handle code-switched data. However, annotated\ncode-switched data is scarce and there is a need for unsupervised models and\nalgorithms. We propose a general framework called Unsupervised Self-Training\nand show its applications for the specific use case of sentiment analysis of\ncode-switched data. We use the power of pre-trained BERT models for\ninitialization and fine-tune them in an unsupervised manner, only using pseudo\nlabels produced by zero-shot transfer. We test our algorithm on multiple\ncode-switched languages and provide a detailed analysis of the learning\ndynamics of the algorithm with the aim of answering the question - `Does our\nunsupervised model understand the Code-Switched languages or does it just learn\nits representations?'. Our unsupervised models compete well with their\nsupervised counterparts, with their performance reaching within 1-7\\% (weighted\nF1 scores) when compared to supervised models trained for a two class problem.",
  "text": "Unsupervised Self-Training for Sentiment Analysis of Code-Switched Data\nAkshat Gupta1, Sargam Menghani1, Sai Krishna Rallabandi2, Alan W Black2\n1Department of Electrical and Computer Engineering, Carnegie Mellon University\n2Language Technologies, Institute, Carnegie Mellon University\n{akshatgu, smenghan}@andrew.cmu.edu, {srallaba, awb}@cs.cmu.edu\nAbstract\nSentiment analysis is an important task in un-\nderstanding social media content like customer\nreviews, Twitter and Facebook feeds etc. In\nmultilingual communities around the world,\na large amount of social media text is char-\nacterized by the presence of code-switching.\nThus, it has become important to build mod-\nels that can handle code-switched data. How-\never, annotated code-switched data is scarce\nand there is a need for unsupervised mod-\nels and algorithms.\nWe propose a general\nframework called Unsupervised Self-Training\nand show its applications for the speciﬁc use\ncase of sentiment analysis of code-switched\ndata. We use the power of pre-trained BERT\nmodels for initialization and ﬁne-tune them in\nan unsupervised manner, only using pseudo\nlabels produced by zero-shot transfer.\nWe\ntest our algorithm on multiple code-switched\nlanguages and provide a detailed analysis of\nthe learning dynamics of the algorithm with\nthe aim of answering the question - ‘Does\nour unsupervised model understand the Code-\nSwitched languages or does it just learn its rep-\nresentations?’. Our unsupervised models com-\npete well with their supervised counterparts,\nwith their performance reaching within 1-7%\n(weighted F1 scores) when compared to super-\nvised models trained for a two class problem.\n1\nIntroduction\nSentiment analysis, sometimes also known as opin-\nion mining, aims to understand and classify the\nopinion, attitude and emotions of a user based on\na text query. Sentiment analysis has many appli-\ncations including understanding product reviews,\nsocial media monitoring, brand monitoring, reputa-\ntion management etc. Code switching is referred to\nas the phenomenon of alternation between multiple\nlanguages, usually two, within a single utterance.\nCode switching is very common in many bilin-\ngual and multilingual societies around the world\nincluding India (Hinglish, Tanglish etc.), Singapore\n(Chinglish) and various Spanish speaking areas of\nNorth America (Spanglish). A large amount of\nsocial media text in these regions is code-mixed,\nwhich is why it is essential to build systems that\nare able to handle code switching.\nVarious datasets have been released to aid ad-\nvancements in Sentiment Analysis of code-mixed\ndata. These datasets are usually much smaller and\nmore noisy when compared to their high-resource-\nlanguage-counterparts and are available for very\nfew languages. Thus, there is a need to come up\nwith both unsupervised and semi-supervised algo-\nrithms to deal with code-mixed data. In our work,\nwe present a general framework called Unsuper-\nvised Self-Training Algorithm for doing sentiment\nanalysis of code-mixed data in an unsupervised\nmanner. We present results for four code-mixed\nlanguages - Hinglish (Hindi-English), Spanglish\n(Spanish-English), Tanglish (Tamil-English) and\nMalayalam-English.\nIn this paper, we propose the Unsupervised Self-\nTraining framework and apply it to the problem\nof sentiment classiﬁcation. Our proposed frame-\nwork performs two tasks simultaneously - ﬁrstly,\nit gives sentiment labels to sentences of a code-\nmixed dataset in an unsupervised manner, and sec-\nondly, it trains a sentiment classiﬁcation model in\na purely unsupervised manner. The framework can\nbe extended to incorporate active learning almost\nseamlessly. We present a rigorous analysis of the\nlearning dynamics of our unsupervised model and\ntry to answer the question - ’Does the unsupervised\nmodel understand the code-switched languages or\ndoes it just recognize its representations?’. We also\nshow methods for optimizing performance of the\nUnsupervised Self-Training algorithm.\n2\nRelated Work\nIn this paper, we propose a framework called Unsu-\npervised Self-Training, which is an extension to the\nsemi-supervised machine learning algorithm called\narXiv:2103.14797v2  [cs.CL]  1 Oct 2021\nSelf-Training (Zhu, 2005). Self-training has pre-\nviously been used in natural language processing\nfor pre-training (Du et al., 2020a) and for tasks like\nword sense disambiguation (Yarowsky, 1995). It\nhas been shown to be very effective for natural lan-\nguage processing tasks (Du et al., 2020b) and better\nthan pre training in low resource scenarios both the-\noretically (Wei et al., 2020) and emperically(Zoph\net al., 2020a). Zoph et al. 2020b show that self-\ntraining can be a more useful than pre-training in\nhigh resourced scenarios for the task of object de-\ntection, and a combination of pre-training and self-\ntraining can improve performance when only 20%\nof the available dataset was used. However, our\nproposed framework differs from self-training such\nthat we only use the zero-shot predictions made\nby our initialization model to train our models and\nnever use actual labels.\nSentiment analysis is a popular task in industry\nas well as within the research community, used\nin analysing the markets (Nanli et al., 2012), elec-\ntion campaigns (Haselmayer and Jenny, 2017) etc.\nA large amount of social media text in most bilin-\ngual communities is code-mixed and many labelled\ndatasets have been released to perform sentiment\nanalysis. We will be working with four code-mixed\ndatasets for sentiment analysis, Malayalam-English\nand Tamil-English (Chakravarthi et al., 2020a,b,c)\nand Spanglish and Hinglish (Patwa et al., 2020).\nPrevious work has shown BERT based models\nto achieve state of the art performance for code-\nswitched languages in tasks like offensive language\nidentiﬁcation (Jayanthi and Gupta, 2021) and senti-\nment analysis (Gupta et al., 2021). We will build\nunsupervised models on top of BERT. BERT (De-\nvlin et al., 2018) based models have achieved state\nof the art performance in many downstream tasks\ndue to their superior contextualized representations\nof language, providing true bidirectional context\nto word embeddings. We will use the sentiment\nanalysis model from (Barbieri et al., 2020), trained\non a large corpus of English Tweets (60 million\nTweets) for initializing our algorithm. We will re-\nfer to the sentiment analysis model from (Barbieri\net al., 2020) as the TweetEval model in the remain-\nder of the paper. The TweetEval model is built\non top of an English RoBERTa (Liu et al., 2019)\nmodel.\n3\nProposed Approach: Unsupervised\nSelf-Training\nOur proposed algorithm is centred around the idea\nof creating an unsupervised learning algorithm that\nis able to harness the power of cross-lingual trans-\nfer in the most efﬁcient way possible, with the aim\nof producing unsupervised sentiment labels. In\nits most fundamental form, our proposed Unsuper-\nvised Self-Training algorithm1 is shown in Figure\n1 is shown in Figure 1.\nWe begin by producing zero-shot results for sen-\ntiment classiﬁcation using a selected pre-trained\nmodel trained for the same task. From the pre-\ndictions made, we select the top-N most conﬁdent\npredictions made by the model. The conﬁdence\nlevel is judged by the softmax scores. Making the\nzero-shot predictions and selecting sentences make\nup the Initialization block as shown in Figure 1.\nWe then use the pseduo-labels predicted by the\nzero-shot model to ﬁne tune our model. After that,\npredictions are made on the remaining dataset with\nthe ﬁne-tuned model. We again select sentences\nbased on their softmax scores for ﬁne-tuning the\nmodel in the next iteration. These steps are re-\npeated until we’ve gone through the entire dataset\nor until a stopping condition. At all ﬁne-tuning\nsteps, we only use the predicted pseduo-labels as\nground truth to train the model, which makes the\nalgorithm completely unsupervised.\nAs the ﬁrst set of predicted pseudo-labels are\nproduced by a zero-shot model, our framework\nis very sensitive to initialization. Care must be\ntaken to initialize the algorithm with a compatible\nmodel. For example, for the task of sentiment clas-\nsiﬁcation of Hinglish Twitter data, an example of a\ncompatible initial model would be a sentiment clas-\nsiﬁcation model trained on either English or Hindi\nsentiment data. It would be even more compatible\nif the model was trained on Twitter sentiment data,\nthe data thus being from the same domain.\n3.1\nOptimizing Performance\nThe most important blocks in the Unsupervised\nSelf-Training framework with respect to maximiz-\ning performance are the Initialization Block and the\nSelection Block (Figure 1). To improve initializa-\ntion, we must choose the most compatible model\nfor the chosen task. Additionally, to improve per-\nformance, we can use several training strategies\n1The code for the framework can be found here:\nhttps://github.com/akshat57/Unsupervised-Self-Training\nFigure 1: A visual representation of our proposed Unsupervised Self-Training framework.\nin the Selection Block. In this section we discuss\nseveral variants of the Selection Block.\nAs an example, instead of selecting a ﬁxed num-\nber of samples N from the dataset in the selection\nblock, we could be selecting a different but ﬁxed\nnumber Ni from each class i in the dataset. This\nwould need an understanding of the class distribu-\ntion of the dataset. We discuss this in later sections.\nAnother variable number of sentences in each it-\neration, rather than a ﬁxed number. This would\ngive us a selection schedule for the algorithm. We\nexplore some of these techniques in later sections.\nOther factors can be incorporated in the Selec-\ntion Block. Selection need not be based on just\nthe most conﬁdent predictions. We can have addi-\ntional selection criteria, for example, incorporating\nthe Token Ratio (deﬁned in section 8) of a partic-\nular language in the predicted sentences. Taking\nthe example of a Hinglish dataset, one way to do\nthis would be to select sentences that have a larger\namount of Hindi and are within selection thresh-\nold. In our experiments, we ﬁnd that knowing an\noptimal selection strategy is vital to achieving the\nmaximum performance.\n4\nDatasets\nWe test our proposed framework on four different\nlanguages - Hinglish (Patwa et al., 2020), Spanglish\n(Patwa et al., 2020), Tanglish (Chakravarthi et al.,\n2020b) and Malayalam-English (Chakravarthi\net al., 2020a). The statistics of the training sets\nare given in Table 1. We also use the test sets of\nthe above datasets, which have similar distribution\nas their respective training sets. The statistics of\nthe test sets are not shown for brevity. We ask the\nreader to refer to the respective papers for more\ndetails.\nThe choice of datasets, apart from covering three\nlanguage families, incorporate several other impor-\ntant features. We can see from Table 1 that the\nfour datasets have different sizes, the Malayalam-\nEnglish dataset being the smallest. Apart from the\nHinglish dataset, the other three datasets are highly\nimbalanced. This is an important distinction as we\ncannot expect an unknown set of sentences to have\na balanced class distributions. We will later see\nthat having a biased underlying distribution affects\nthe performance of our algorithm and how better\ntraining strategies can alleviate this problem.\nThe chosen datasets are also from two differ-\nent domains - the Hinglish and Spanglish datasets\nare a collection of Tweets whereas Tanglish and\nMalaylam-English are a collection of Youtube\nComments. The TweetEval model, which is used\nfor initialization is trained on a corpus of English\nTweets. Thus the Hinglish and Spangish datasets\nare in-domain datasets for our initialization model\n(Barbieri et al., 2020), whereas the Dravidian lan-\nguage (Tamil and Malayalam) datasets are out of\ndomain.\nThe datasets also differ in the amount of class-\nwise code-mixing. Figure 3 shows that for the\nHinglish dataset, a negative Tweet is more likely\nto contain large amounts of Hindi. This is not\nthe same for the other datasets. For Spanglish,\nboth positive and negative sentiment Tweets have\na tendency to use a larger amount of Spanish than\nEnglish.\nAn important thing to note here is that each of\nthe four code-mixed datasets selected are written\nin the latin script. Thus our choice of datasets does\nnot take into account mixing of different scripts.\n5\nModels\nModels built on top of BERT (Devlin et al., 2018)\nand its multilingual version like mBERT, XLM-\nLanguage\nDomain\nTotal\nPositive\nNegative\nNeutral\nHinglish\nTweets\n14000\n4634\n4102\n5264\nSpanglish\nTweets\n12002\n6005\n2023\n3974\nTanglish\nYoutube Comments\n9684\n7627\n1448\n609\nMalayalam-English\nYoutube Comments\n3915\n2022\n549\n1344\nTable 1: Training dataset statistics for chosen datasets.\nRoBERTa (Conneau et al., 2019) have recently\nproduced state-of-the-art results in many natural\nlanguage processing tasks. Various shared tasks\n(Patwa et al., 2020) (Chakravarthi et al., 2020c) in\nthe domain of code-switched sentiment analysis\nhave also seen their best performing systems build\non top of these BERT models.\nEnglish is a common language among all the\nfour code-mixed datasets being considered. This is\nwhy we use a RoBERTa based sentiment classiﬁca-\ntion model trained on a large corpus of 60 million\nEnglish Tweets (Barbieri et al., 2020) for initializa-\ntion. We refer to this sentiment classiﬁcation model\nas the TweetEval model for the rest of this paper.\nWe use the Hugging Face implementation of the\nTweetEval sentiment classiﬁcation model 2. The\nmodels are ﬁne-tuned with a batch size of 16 and\na learning rate of 2e-5. The TweetEval model pre-\nprocesses sentiment data to not include any URL’s.\nWe have done the same for for all the four datasets.\nWe compare our unsupervised model with a\nset of supervised models trained on each of the\nfour datasets. We train supervised models by ﬁne\ntuning the TweetEval model on each of the four\ndatasets. Our experiments have shown that the\nTweetEval model performs the better in compari-\nson to mBERT and XLM-RoBERTa based models\nfor code-switched data.\n6\nEvaluation\nWe evaluate our results based on weighted aver-\nage F1 and accuracy scores. When calculating the\nweighted average, the F1 scores are calculated for\neach class and a weighted average is taken based on\nthe number of samples in each class. This metric is\nchosen because three out of four datasets we work\nwith are highly imbalanced. We use the sklearn\nimplementation for calculating weighted average\nF1 scores3.\n2https://huggingface.co/cardiffnlp/twitter-roberta-base-\nsentiment\n3https://scikit-learn.org/stable/\nmodules/generated/sklearn.metrics.\nclassification_report.html\nLanguage\nF1\nAccuracy\nHinglish\n0.32\n0.36\nSpanglish\n0.31\n0.32\nTanglish\n0.15\n0.16\nMalayalam-English\n0.17\n0.14\nTable 2:\nZero-shot prediction performance for the\nTweetEval model for each of the four datasets, for a\ntwo-class classiﬁcation problem (positive and negative\nclasses). The F1 scores represent the weighted aver-\nage F1. These zero-shot predictions are for the training\ndatasets in each of the four languages.\nThere are two ways to evaluate the performance\nof our proposed method, corresponding to two dif-\nferent perspectives with which we look at the out-\ncome. One of the ways to evaluate the proposed\nmethod is to answer the question - ‘How good is the\nmodel when trained in the proposed, unsupervised\nmanner?’. We call this perspective of evaluation,\nhaving a model perspective. Here we’re evaluating\nthe strength of the unsupervised model. To evaluate\nthe method from a model perspective, we compare\nthe performance of best unsupervised model with\nthe performance of the supervised model on the\ntest set.\nThe second way to evaluate the proposed method\nis by looking at it from what we call an algorithmic\nperspective. The aim of proposing an unsupervised\nalgorithm is to be able to select sentences belong-\ning to a particular sentiment class from an unkown\ndataset. Hence, to evaluate from an algorithmic per-\nspective, we must look at the training set and check\nhow accurate the algorithm is in its annotations\nfor each class. To do this, we show performance\n(F1-scores) as a function of the number of selected\nsentences from the training set.\n7\nExperiments\nFor our experiments, we restrict the dataset to\nconsist of two sentiment classes - positive and\nnegative sentiments. In this section, we evaluate\nour proposed unsupervised self-training framework\nTrain Language\nVanilla\nRatio\nSupervised\nF1\nAccuracy\nF1\nAccuracy\nF1\nAccuracy\nHinglish\n0.84\n0.84\n0.84\n0.84\n0.91\n0.91\nSpanglish\n0.77\n0.76\n0.77\n0.77\n0.78\n0.79\nTamil\n0.68\n0.63\n0.79\n0.80\n0.83\n0.85\nMalayalam\n0.73\n0.71\n0.83\n0.85\n0.90\n0.90\nTable 3: Performance of best Unsupervised Self-Training models for Vanilla and Ratio selection strategies when\ncompared to performance of supervised models. The F1 scores represent the weighted average F1.\nfor four different code-swithced languages span-\nning across three language families, with different\ndataset sizes and different extents of imbalance and\ncode-mixing, and across two different domains. We\nalso present a comparison between supervised and\nunsupervised models.\nWe present two training strategies for our pro-\nposed Unsupervised Self-Training Algorithm. The\nﬁrst is a vanilla strategy where the same number\nof sentences are selected in the selection block for\neach class. The second strategy uses a selection\nratio - where we select sentences for ﬁne tuning\nin a particular ratio from each class. We evaluate\nthe algorithm based on the two evaluation criterion\ndescribed in section 6.\nIn the Fine-Tune Block in Figure 1, we ﬁne-tune\nthe TweetEval model on the selected sentences for\nonly 1 epoch. We do this because we do not want\nour model to overﬁt on the small amount of selected\nsentences. This means that when we go through\nthe entire training dataset, the model has seen every\nsentence in the train set exactly once. We see that\nif we ﬁne-tune the model for multiple epochs, the\nmodel overﬁts and its performance and capacity to\nlearn reduces with every iteration.\n7.1\nZero-Shot Results\nTable 2 shows the zero-shot results for the Tweet-\nEval model. We see that the zero-shot F1 scores\nare much higher for the Hinglish and Spanglish\ndatasets when compared to the results for the Dra-\nvidian languages. Part of the disparity in zero-shot\nperformance can be attributed to the differences in\ndomains. This means that the TweetEval model is\nnot as compatible to the Tanglish and Malayalam-\nEnglish dataset than it is to the Spanglish and\nHinglish datasets. Improved training strategies help\nincrease performance.\nThe zero-shot results in Table 2 use the TweetE-\nval model, which is a 3-class classiﬁcation model.\nDue to this, we get a prediction accuracy of less\nthan 50% for a binary classiﬁcation problem.\n7.2\nVanilla Selection\nIn the Vanilla Selection strategy, we select the same\nnumber of sentences for each class. We saw no im-\nprovement when selecting less than 5% sentences\nof the total dataset size in every iteration, equally\nsplit into the two classes. Table 3 shows the perfor-\nmance of the best unsupervised model trained in\ncomparison with a supervised model. For each of\nthese results, N = 0.05 * (dataset size), where N/2 is\nthe number of sentences selected from each class at\nevery iteration step. The best unsupervised model\nis achieved almost halfway through the dataset for\nall languages.\nThe unsupervised model performs surprisingly\nwell for Spanglish when compared to the su-\npervised counterpart.\nThe performance for the\nHinglish model is also comparable to the super-\nvised model. This can be attributed to the fact\nthat both datasets are in-domain for the TweetEval\nmodel and their zero-shot performances are bet-\nter than for the Dravidian languages, as shown in\nTable 2. Also, the fact that the Hinglish dataset\nis balanced helps improve performance. We ex-\npect the performance of the unsupervised models\nto increase with better training strategies.\nFor a balanced dataset like Hinglish, selecting\nN > 5% at every iteration provided similar perfor-\nmance whereas the performance deteriorates for\nthe three imbalanced datasets if a larger number\nof sentences were selected. This behaviour was\nsomewhat expected as when the dataset is imbal-\nanced, the model is likely to make more errors in\ngenerating pseduo-labels for one class more than\nthe other. Thus it helps to reduce the number of\nselections as that also reduces the number of errors.\n7.3\nSelection Ratio\nIn this selection strategy, we select unequal num-\nber of samples from each class, deciding on a ratio\nof positive samples to negative samples. The aim\nof selecting sentences with a particular ratio is to\nincorporate the underlying class distribution of the\ndataset for selection. When the underlying distribu-\ntion is biased, selecting equal number of sentences\nwould leave the algorithm to have lower accuracy\nin the produced pseudo-labels for the smaller class,\nand this error is propagated with every iteration\nstep.\nThe only way to truly estimate the correct selec-\ntion ratio is to sample from the given dataset. In an\nunsupervised scenario, we would need to annotate a\nselected sample of sentences to determine the selec-\ntion ratio empirically. We found that on sampling\naround 50 sentences from the dataset, we were ac-\ncurately able to predict the distribution of the class\nlabels with a standard deviation of approximately\n4-6%, depending on the dataset. The performance\nis not sensitive to estimation inaccuracies of that\namount.\nFinding the selection ratio serves a second pur-\npose - it also gives us an estimated stopping condi-\ntion. By knowing an approximate ratio of the class\nlabels and the size of the dataset, we now have an\napproximation for the total number of samples in\neach class. As soon as the total selections for a class\nacross all iteration reaches the predicted number\nof samples of that class, according to the sampled\nselection ratio, we should stop the algorithm.\nThe results for using the selection ratio are\nshown in Table 3. We see signiﬁcant improve-\nments in performance for the Dravidian languages,\nwith the performance reaching very close to the\nsupervised performance. The improvement in per-\nformance for the Hinglish and Spanglish datasets\nare minimal. This hints that a selection ratio strat-\negy was able to overcome the difference in domains\nand the affects of poor initialization as pointed out\nin Table 2.\nThe selection ratio strategy was also able to over-\ncome the problem of data imbalance. This can be\nseen in Figure 2 when we evaluate the framework\nfrom an algorithmic perspective. Figure 2 plots the\nclassiﬁcation F1 scores of the unsupervised algo-\nrithm as a function of the selected sentences. We\nﬁnd that using the selection ratio strategy improves\nthe performance on the training set signiﬁcantly.\nWe see improvements for the Dravidian languages,\nwhich were also reﬂected in Table 3.\nThis improvement is also seen for the Spanglish\ndataset, which is not reﬂected in Table 3. This\nmeans that for Spanglish, the improvement in the\nunsupervised model when trained with selection\nratio strategy does not generalize to a test set, but\nthe improvement is enough to select sentences in\nthe next iterations more accurately. This means\nthat we’re able to give better labels to our training\nset in an unsupervised manner, which was one of\nthe aims of developing an unsupervised algorithm.\n(Note : The evaluation from a model perspective is\ndone on the test set, whereas from an algorithmic\nperspective is done on the training set.)\nThis evaluation perspective also shows that if\nthe aim of the unsupervised endevour is to create\nlabels for an unlabelled set of sentences, one does\nnot have to process the entire dataset. For exam-\nple, if we are content with pseudo-labels or noisy\nlabels for 4000 Hinglish Tweets, the accuracy of\nthe produced labels would be close to 90%.\nFigure 2:\nPerformance of the Unsupervised Self-\nTraining algorithm as a function of selected sentences\nfrom the training set.\n8\nAnalysis\nIn this section we aim to understand the in-\nformation learnt by a model trained under the\nUnsupervised Self-Training. We take the example\nof the Hinglish dataset.\nTo do so, we deﬁne\na quantity called Token Ratio to quantify the\namount of code-mixing in a sentence.\nSince\nour initialization model is trained on an English\ndataset, the language of interest is Hindi and we\nwould like to understand how well our model\nhandles sentences with a large amount of Hindi.\nHence, for the Hinglish dataset, we deﬁne the\nHindi Token Ratio as:\nHindi Token Ratio = Number of Hindi Tokens\nTotal Number of Words\n(Patwa et al., 2020) provide three language la-\nbels for each token in the dataset - HIN, ENG, 0,\nwhere 0 usually corresponds to a symbol or other\nspecial characters in a Tweet. To quantify amount\nof code-mixing, we only use the tokens that have\nENG or HIN as labels. Words are deﬁned as tokens\nthat have either the label HIN or ENG. We deﬁne\nthe Token Ratio quantity with respect to Hindi, but\nour analysis can be generalized to any code-mixed\nlanguage. The distribution of the Hindi Token Ra-\ntio (HTR) in the Hinglish dataset is shown in Figure\n3. The ﬁgure clearly shows that the dataset is dom-\ninated by tweets that have a larger amount Hindi\nwords than English words. This is also true for the\nother three datasets.\nFigure 3: Distributions of Hindi Token Ratio for the\nHinglish Dataset.\n8.1\nLearning Dynamics of the Unsupervised\nModel\nTo study if the unsupervised model understands\nHinglish, we look at the performance of the model\nas a function of the Hindi Token Ratio. In Figure 4\n, the sentences in the Hinglish dataset are grouped\ninto buckets of Hindi Token Ratio. A bucket is of\nsize 0.1 and contains all the sentences that fall in\nits range. For example, when the x-axis says 0.1,\nthis means the bucket contains all sentences that\nhave a Hindi Token ratio between 0.1 and 0.2.\nFigure 4 shows that the zero shot model performs\nthe best for sentences that have very low amount\nof Hindi code-mixed with English. As the amount\nof Hindi in a sentence increases, the performance\nof the zero-shot predictions decreases drastically.\nOn training the model with our proposed Unsuper-\nvised Self-Training framework, we see a signiﬁcant\nrise in the performance for sentences with higher\nHTR (or sentences with a larger amount of Hindi\nthan English) as well as the overall performance\nof the model. This rise is gradual and the model\nimproves at classifying sentences with higher HTR\nwith every iteration.\nNext, we refer back to Figure 3. Figure 3 shows\nFigure 4: Model Performance for different Hindi Token\nRatio buckets. For example, a bucket labelled as 0.3\ncontains all sentences that have a Hindi Token ration\nbetween 0.3 and o.4.\nthe distribution of the Hindi Token Ratio for each of\nthe two sentiment classes. For the Hinglish dataset,\nwe see that tweets with negative sentiments are\nmore likely to contain more Hindi words than En-\nglish. The distribution for the Hindi Token Ratio\nfor positive sentiment is almost uniform, thus show-\ning no preference for English or Hindi words when\nexpressing a positive sentiment. If we look at the\ndistribution of the predictions made by the zero-\nshot unsupervised model, shown in Figure 5, we\nsee that majority of the sentences are predicted as\nbelonging to the positive sentiment class. There\nseems to be no resemblance with the original dis-\ntribution (Figure 3). As the model trains under\nour Unsupervised Self-Training framework, we see\nthat the predicted distribution becomes very similar\nto the original distribution.\nFigure 5: Comparison made between predictions made\nby the zero-shot and the best unsupervised model.\n8.2\nError Analysis\nIn this section, we look at the errors made by the\nunsupervised model. Table 4 shows the compar-\nison between the class-wise performance of the\nsupervised and the best unsupervised model. The\nunsupervised model is better at making correct pre-\ndictions for the negative sentiment class when com-\npared to the supervised model. Figure 6 shows the\nclasswise performance for the zero-shot and best\nunsupervised model for the different HTR buck-\nets. We see that the zero-shot models performs\npoorly for both the positive and negative classes.\nAs the unsupervised model improves with itera-\nModel\nType\nPositive\nAccuracy\nNegative\nAccuracy\nUnsupervised\n0.73\n0.94\nSupervised\n0.93\n0.83\nTable 4: Comparison between class-wise performance\nfor supervised and unsupervised models.\ntions through the dataset, we see the performance\nfor each class increase.\nFigure 6: Class-wise performance of the zero-shot and\nthe best unsupervised models for different Hindi Token\nRatio buckets.\n8.3\nDoes the Unsupervised Model\n‘Understand’ Hinglish?\nThe learning dynamics in section 8.1 show that\nas the TweetEval model is ﬁne-tuned under our\nproposed Unsupervised Training Framework, the\nperformance of the model increases for sentences\nthat have higher amounts of Hindi. In fact, the per-\nformance increase is seen across the board for all\nHindi Token Ratio buckets. We saw the distribu-\ntion of the predictions made by the zero-shot model,\nwhich preferred to classify almost all sentences as\npositive. But as the model was ﬁne-tuned, the pre-\ndicted distribution was able to replicate the original\ndata distribution. These experiments show that the\nmodel originally trained on an English dataset is\nbeginning to atleast recognize Hindi when trained\nwith our proposed framework, if not understand it.\nWe also see a bias in the Hinglish dataset where\nthe negative sentiments are more likely to contain\na larger number Hindi Tokens, which are unknown\ntokens from the perspective of the initial TweetE-\nval model. Thus the classiﬁcation task would be\naided by learning the difference in the underlying\ndistributions of the two classes. Note that we do\nexpect a supervised model to use this divide in the\ndistributions as well. Figure 6 shows a larger in-\ncrease in performance for the negative sentiment\nclass than the positive sentiment class, although the\nperformance is increased across the board for all\nHindi Token Ratio buckets. (This difference in per-\nformance can be remedied by selecting sentences\nwith high Hindi Token Ratio in the selection block.)\nThus, it does seem like that the model is able to\nunderstand Hindi and this understanding is aided\nby the differences in the class-wise distribution of\nthe two sentiments.\n9\nConclusion\nWe propose the Unsupervised Self-Training frame-\nwork and show results for unsupervised sentiment\nclassiﬁcation of code-switched data. The algo-\nrithm is comprehensively tested for four very dif-\nferent code-mixed languages - Hinglish, Spanglish,\nTanglish and Malayalam-English, covering many\nvariations including differences in language fami-\nlies, domains, dataset sizes and dataset imbalances.\nThe unsupervised models performed competitively\nwhen compared to supervised models. We also\npresent training strategies to optimize the perfor-\nmance of our proposed framework.\nAn extensive analysis is provided describing the\nlearning dynamics of the algorithm. The algorithm\nis initialized with a model trained on an English\ndataset and has poor zero-shot performance on sen-\ntence with large amounts of code-mixing. We show\nthat with every iteration, the performance on ﬁne-\ntuned model increases for sentences with a larger\namounts of code-mixing. Eventually, the model\nbegins to understand the code-mixed data.\n10\nFuture Work\nThe proposed Unsupervised Self-Training algo-\nrithm was tested with only two sentiment classes -\npositive and negative. An unsupervised sentiment\nclassiﬁcation algorithm is to be able to generate\nannotations for an unlabelled code-mixed dataset\nwithout going through the expensive annotation\nprocess. This can be done by including the neutral\nclass in the dataset, which is going to be a part of\nour future work.\nIn our work, we only used one initialization\nmodel trained on English Tweets for all four code-\nmixed datasets, as all of them were code-mixed\nwith English.\nFuture work can include testing\nthe framework with different and more compatible\nmodels for initialization. Further work can be done\non optimization strategies, including incorporating\nthe Token Ratio while selecting pseudo-labels, and\nactive learning.\nReferences\nFrancesco Barbieri, Jose Camacho-Collados, Leonardo\nNeves, and Luis Espinosa-Anke. 2020.\nTweet-\neval:\nUniﬁed benchmark and comparative eval-\nuation for tweet classiﬁcation.\narXiv preprint\narXiv:2010.12421.\nBharathi Raja Chakravarthi, Navya Jose, Shardul\nSuryawanshi, Elizabeth Sherly, and John P Mc-\nCrae. 2020a.\nA sentiment analysis dataset for\ncode-mixed malayalam-english.\narXiv preprint\narXiv:2006.00210.\nBharathi Raja Chakravarthi, Vigneshwaran Murali-\ndaran, Ruba Priyadharshini, and John P McCrae.\n2020b.\nCorpus creation for sentiment analysis\nin code-mixed tamil-english text.\narXiv preprint\narXiv:2006.00206.\nBR Chakravarthi, R Priyadharshini, V Muralidaran,\nS Suryawanshi, N Jose, E Sherly, and JP McCrae.\n2020c. Overview of the track on sentiment analysis\nfor dravidian languages in code-mixed text. In Work-\ning Notes of the Forum for Information Retrieval\nEvaluation (FIRE 2020). CEUR Workshop Proceed-\nings. In: CEUR-WS. org, Hyderabad, India.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJingfei Du, Edouard Grave, Beliz Gunel, Vishrav\nChaudhary, Onur Celebi, Michael Auli, Ves Stoy-\nanov, and Alexis Conneau. 2020a. Self-training im-\nproves pre-training for natural language understand-\ning. arXiv preprint arXiv:2010.02194.\nJingfei Du, Edouard Grave, Beliz Gunel, Vishrav\nChaudhary, Onur Celebi, Michael Auli, Ves Stoy-\nanov, and Alexis Conneau. 2020b. Self-training im-\nproves pre-training for natural language understand-\ning. arXiv preprint arXiv:2010.02194.\nAkshat Gupta, Sai Krishna Rallabandi, and Alan Black.\n2021. Task-speciﬁc pre-training and cross lingual\ntransfer for code-switched data.\narXiv preprint\narXiv:2102.12407.\nMartin Haselmayer and Marcelo Jenny. 2017. Senti-\nment analysis of political communication: combin-\ning a dictionary approach with crowdcoding. Qual-\nity & quantity, 51(6):2623–2646.\nSai Muralidhar Jayanthi and Akshat Gupta. 2021.\nSj_aj@ dravidianlangtech-eacl2021: Task-adaptive\npre-training of multilingual bert models for of-\nfensive language identiﬁcation.\narXiv preprint\narXiv:2102.01051.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhu Nanli, Zou Ping, Li Weiguo, and Cheng Meng.\n2012. Sentiment analysis: A literature review. In\n2012 International Symposium on Management of\nTechnology (ISMOT), pages 572–576. IEEE.\nParth Patwa, Gustavo Aguilar, Sudipta Kar, Suraj\nPandey, Srinivas PYKL, Björn Gambäck, Tanmoy\nChakraborty, Thamar Solorio, and Amitava Das.\n2020. Semeval-2020 task 9: Overview of sentiment\nanalysis of code-mixed tweets. arXiv e-prints, pages\narXiv–2008.\nColin Wei, Kendrick Shen, Yining Chen, and Tengyu\nMa. 2020. Theoretical analysis of self-training with\ndeep networks on unlabeled data.\narXiv preprint\narXiv:2010.03622.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nannual meeting of the association for computational\nlinguistics, pages 189–196.\nXiaojin Jerry Zhu. 2005. Semi-supervised learning lit-\nerature survey.\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui,\nHanxiao Liu, Ekin D Cubuk, and Quoc V Le. 2020a.\nRethinking pre-training and self-training.\narXiv\npreprint arXiv:2006.06882.\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui,\nHanxiao Liu, Ekin D Cubuk, and Quoc V Le. 2020b.\nRethinking pre-training and self-training.\narXiv\npreprint arXiv:2006.06882.\nA\nImplementation Details\nWe use the RoBERTa-base based model pre-trained\non a large English Twitter corpus for initialization,\nwhich has about 125M paramters. The model was\nﬁne-tuned using the NVIDIA GeForce GTX 1070\nGPU using python3.6. The Tanglish dataset was\nthe biggest dataset which required approximately 3\nminutes per iteration. One pass through the entire\ndataset required 20 iterations for the Vanilla selec-\ntion strategy and about 30 iterations for the Ratio\nselection strategy. The time required per iteration\nwas lower for the the other three datasets, with\nabout 100 seconds per iteration for the Malaylam-\nEnglish datasets.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-03-27",
  "updated": "2021-10-01"
}