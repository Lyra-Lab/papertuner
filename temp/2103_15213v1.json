{
  "id": "http://arxiv.org/abs/2103.15213v1",
  "title": "A Temporal Kernel Approach for Deep Learning with Continuous-time Information",
  "authors": [
    "Da Xu",
    "Chuanwei Ruan",
    "Evren Korpeoglu",
    "Sushant Kumar",
    "Kannan Achan"
  ],
  "abstract": "Sequential deep learning models such as RNN, causal CNN and attention\nmechanism do not readily consume continuous-time information. Discretizing the\ntemporal data, as we show, causes inconsistency even for simple continuous-time\nprocesses. Current approaches often handle time in a heuristic manner to be\nconsistent with the existing deep learning architectures and implementations.\nIn this paper, we provide a principled way to characterize continuous-time\nsystems using deep learning tools. Notably, the proposed approach applies to\nall the major deep learning architectures and requires little modifications to\nthe implementation. The critical insight is to represent the continuous-time\nsystem by composing neural networks with a temporal kernel, where we gain our\nintuition from the recent advancements in understanding deep learning with\nGaussian process and neural tangent kernel. To represent the temporal kernel,\nwe introduce the random feature approach and convert the kernel learning\nproblem to spectral density estimation under reparameterization. We further\nprove the convergence and consistency results even when the temporal kernel is\nnon-stationary, and the spectral density is misspecified. The simulations and\nreal-data experiments demonstrate the empirical effectiveness of our temporal\nkernel approach in a broad range of settings.",
  "text": "Published as a conference paper at ICLR 2021\nA TEMPORAL KERNEL APPROACH FOR DEEP LEARN-\nING WITH CONTINUOUS-TIME INFORMATION\nDa Xu\nWalmart Labs\nSunnyvale, CA 94086, USA\nDaXu5180@gmail.com\nChuanwei Ruan ∗\nInstacart\nSan Francisco, CA 94107, USA\nRuanchuanwei@gmail.com\nEvren Korpeoglu, Sushant Kuamr, Kannan Achan\nWalmart Labs\nSunnyvale, CA 94086, USA\n[EKorpeoglu, SKumar4, KAchan]@walmartlabs.com\nABSTRACT\nSequential deep learning models such as RNN, causal CNN and attention mech-\nanism do not readily consume continuous-time information. Discretizing the\ntemporal data, as we show, causes inconsistency even for simple continuous-time\nprocesses. Current approaches often handle time in a heuristic manner to be con-\nsistent with the existing deep learning architectures and implementations. In this\npaper, we provide a principled way to characterize continuous-time systems using\ndeep learning tools. Notably, the proposed approach applies to all the major deep\nlearning architectures and requires little modiﬁcations to the implementation. The\ncritical insight is to represent the continuous-time system by composing neural\nnetworks with a temporal kernel, where we gain our intuition from the recent\nadvancements in understanding deep learning with Gaussian process and neural\ntangent kernel. To represent the temporal kernel, we introduce the random feature\napproach and convert the kernel learning problem to spectral density estimation\nunder reparameterization. We further prove the convergence and consistency re-\nsults even when the temporal kernel is non-stationary, and the spectral density is\nmisspeciﬁed. The simulations and real-data experiments demonstrate the empirical\neffectiveness of our temporal kernel approach in a broad range of settings.\n1\nINTRODUCTION\nDeep learning models have achieved remarkable performances in sequence learning tasks leveraging\nthe powerful building blocks from recurrent neural networks (RNN) (Mikolov et al., 2010), long-\nshort term memory (LSTM) (Hochreiter & Schmidhuber, 1997), causal convolution neural network\n(CausalCNN/WaveNet) (Oord et al., 2016) and attention mechanism (Bahdanau et al., 2014; Vaswani\net al., 2017). Their applicability to the continuous-time data, on the other hand, is less explored due\nto the complication of incorporating time when the sequence is irregularly sampled (spaced). The\nwidely-adopted workaround is to study the discretized counterpart instead, e.g. the temporal data is\naggregated into bins and then treated as equally-spaced, with the hope to approximate the temporal\nsignal using the sequence information. It is perhaps without surprise, as we show in Claim 1, that\neven for regular temporal sequence the discretization modiﬁes the spectral structure. The gap can\nonly be ampliﬁed for irregular data, so discretizing the temporal information will almost always\n∗The work was done when the author was with Walmart Labs.\n1\narXiv:2103.15213v1  [cs.LG]  28 Mar 2021\nPublished as a conference paper at ICLR 2021\nintroduce intractable noise and perturbations, which emphasizes the importance to characterize the\ncontinuous-time information directly. Previous efforts to incorporate temporal information for deep\nlearning include concatenating the time or timespan to feature vector (Choi et al., 2016; Lipton\net al., 2016; Li et al., 2017b), learning the generative model of time series as missing data problems\n(Soleimani et al., 2017; Futoma et al., 2017), characterizing the representation of time (Xu et al., 2019;\n2020; Du et al., 2016) and using neural point process (Mei & Eisner, 2017; Li et al., 2018). While they\nprovide different tools to expand neural networks to coupe with time, the underlying continuous-time\nsystem and process are involved explicitly or implicitly. As a consequence, it remains unknown\nin what way and to what extend are the continuous-time signals interacting with the original deep\nlearning model. Explicitly characterizing the continuous-time system (via differential equations),\non the other hand, is the major pursuit of classical signal processing methods such as smoothing\nand ﬁltering (Doucet & Johansen, 2009; Särkkä, 2013). The lack of connections is partly due to\nthe compatibility issues between the signal processing methods and the auto-differential gradient\ncomputation framework of modern deep learning. Generally speaking, for continuous-time systems,\nmodel learning and parameter estimation often rely on the more complicated differential equation\nsolvers (Raissi & Karniadakis, 2018; Raissi et al., 2018a). Although the intersection of neural network\nand differential equations is gaining popularity in recent years, the combined neural differential\nmethods often require involved modiﬁcations to both the modelling part and implementation detail\n(Chen et al., 2018; Baydin et al., 2017).\nInspired by the recent advancement in understanding neural network with Gaussian process and the\nneural tangent kernel (Yang, 2019; Jacot et al., 2018), we discover a natural connection between the\ncontinuous-time system and the neural Gaussian process after composing with a temporal kernel.\nThe signiﬁcance of the temporal kernel is that it ﬁlls in the gap between signal processing and deep\nlearning: we can explicitly characterize the continuous-time systems while maintaining the usual\ndeep learning architectures and optimization procedures. While the kernel composition is also known\nfor integrating signals from various domains (Shawe-Taylor et al., 2004), we face the additional\ncomplication of characterizing and learning the unknown temporal kernel in a data-adaptive fashion.\nUnlike the existing kernel learning methods where at least the parametric form of the kernel is given\n(Wilson et al., 2016), we have little context on the temporal kernel, and aggressively assuming the\nparametric form will risk altering the temporal structures implicitly just like discretization. Instead,\nwe leverage the Bochner’s theorem and its extension (Bochner, 1948; Yaglom, 1987) to ﬁrst covert\nthe kernel learning problem to the more reasonable spectral domain where we can direct characterize\nthe spectral properties with random (Fourier) features. Representing the temporal kernel by random\nfeatures is favorable as we show they preserve the existing Gaussian process and NTK properties\nof neural networks. This is desired from the deep learning’s perspective since our approach will\nnot violate the current understandings of deep learning. Then we apply the reparametrization trick\n(Kingma & Welling, 2013), which is a standard tool for generative models and Bayesian deep learning,\nto jointly optimize the spectral density estimator. Furthermore, we provide theoretical guarantees for\nthe random-feature-based kernel learning approach when the temporal kernel is non-stationary, and\nthe spectral density estimator is misspeciﬁed. These two scenarios are essential for practical usage but\nhave not been studied in the previous literature. Finally, we conduct simulations and experiments on\nreal-world continuous-time sequence data to show the effectiveness of the temporal kernel approach,\nwhich signiﬁcantly improves the performance of both standard neural architectures and complicated\ndomain-speciﬁc models. We summarize our contributions as follow.\n• We study a novel connection between the continuous-time system and neural network via\nthe composition with a temporal kernel.\n• We propose an efﬁcient kernel learning method based on random feature representation,\nspectral density estimation and reparameterization, and provide strong theoretical guarantees\nwhen the kernel is nonstationary and the spectral density is misspeﬁcied.\n• We analyze the empirical performance of our temporal kernel approach for both the standard\nand domain-speciﬁc deep learning models through real-data simulation and experiments.\n2\nNOTATIONS AND BACKGROUND\nWe use bold-font letters to denote vectors and matrices. We use xt and (x, t) interchangeably to\ndenote a time-sensitive event occurred at time t, with t ∈T ≡[0, tmax]. Neural networks are denoted\n2\nPublished as a conference paper at ICLR 2021\nby f(θ, x), where x ∈X ⊂Rd is the input where diameter(X) ≤l, and the network parameters θ\nare sampled i.i.d from the standard normal distribution at initialization. Without loss of generality, we\nstudy the standard L-layer feedforward neural network with its output at the hth hidden layer given\nby f (h) ∈Rdh. We use ϵ and ϵ(t) to denote Gaussian noise and continuous-time Gaussian noise\nprocess. By convention, we use ⊗and ◦and to represent the tensor and outer product.\n2.1\nUNDERSTANDING THE STANDARD NEURAL NETWORK\nWe follow the settings from Jacot et al. (2018); Yang (2019) to brieﬂy illustrate the limiting Gaus-\nsian behavior of f(θ, x) at initialization, and its training trajectory under weak optimization. As\nd1, . . . , dL →∞, f (h) tend in law to i.i.d Gaussian processes with covariance Σh ∈Rdh×dh:\nf (h) ∼N(0, Σh), which we refer to as the neural network kernel to distinguish from the other kernel\nnotions. Also, given a training dataset {xi, yi}n\ni=1, let f\n\u0000θ(s)\u0001\n=\n\u0000f(θ(s), x1), . . . , f(θ(s), xn)\n\u0001\nbe the network outputs at the sth training step and y = (y1, . . . , yn). Using the squared loss\nfor example, when training with inﬁnitesimal learning rate, the outputs follows: df\n\u0000θ(s)\u0001\n/ds =\n−Θ(s) × (f\n\u0000θ(s)\u0001\n−y), where Θ(s) is the neural tangent kernel (NTK). The detailed formulation\nof Σh and Θ(s) are provided in Appendix A.2. We introduce the two concepts here because:\n1. instead of incorporating time to f(θ, x), which is then subject to its speciﬁc structures, can we\nalternatively consider an universal approach which expands the Σh to the temporal domain such as\nby composing it with a time-aware kernel?\n2. When jointly optimize the unknown temporal kernel and the model parameters, how can we\npreserve the results on the training trajectory with NTK?\nIn our paper, we show that both goals are achieved by representing a temporal kernel via random\nfeatures.\n2.2\nDIFFERENCE BETWEEN CONTINUOUS-TIME AND ITS DISCRETIZATION\nWe now discuss the gap between continuous-time process and its equally-spaced discretization. We\nstudy the simple univariate continuous-time system f(t):\nd2f(t)\ndt2\n+ a0\ndf(t)\ndt\n+ a1f(t) = b0ϵ(t).\n(1)\nA discretization with a ﬁxed interval is then given by: f[i] = f(i × interval) for i = 1, 2, . . .. Notice\nthat f(t) is a second-order auto-regressive process, so both f(t) and f[i] are stationary. Recall that\nthe covariance function for a stationary process is given by k(t) := cov\n\u0000f(t0), f(t0 + t)\n\u0001\n, and the\nspectral density function (SDF) is deﬁned as s(ω) =\nR ∞\n−∞exp(−iωt)k(t)dt.\nClaim 1. The spectral density function for f(t) and f[i] are different.\nThe proof is relegated to Appendix A.2.2. The key takeaway from the example is that the spectral\ndensity function, which characterizes the signal on the frequency domain, is altered implicitly even by\nregular discretization in this simple case. Hence, we should be cautious about the potential impact of\nthe modelling assumption, which eventually motivates us to explicitly model the spectral distribution.\n3\nMETHODOLOGY\nWe ﬁrst explain our intuition using the above example. If we take the Fourier transform on (1)\nand rearrange terms, it becomes: ˜f(iω) =\n\u0010\nb0\n(iω)2 + a0(iω) + a1\n\u0011\n˜ϵ(iω), where ˜f(iω) and ˜ϵ(iω)\nare the Fourier transform of f(t) and ϵ(t). Note that the spectral density of a Gaussian noise\nprocess is constant, i.e. |˜ϵ(iω)|2 = p0, so the spectral density of f(t) is given by: sθT (ω) =\np0\n\f\fb0/((iω)2 + a0(iω) + a1)\n\f\f2, where we use θT = [a0, a1, b0] to denote the parameters of the\nlinear dynamic system deﬁned in (1). The subscript T is added to distinguish from the parameters\nof the neural network. The classical Wiener-Khinchin theorem (Wiener et al., 1930) states that the\n3\nPublished as a conference paper at ICLR 2021\nFigure 1: (a). The relations between neural network kernel Σ(h), temporal kernel KT and the\nneural-temporal kernel Σ(h)\nT . (b). Composing a single-layer RNN with temporal kernel where the\nhidden out from GRU cells become f (h)(x, t) ≡f (h)(x) ◦φ(x, t). We use the RF-INN blocks to\ndenote the random feature representation parameterized by INN. The optional outputs y(t) can be\nobtained in a way similar to f (h)(x, t).\ncovariance function for f(t), which is a Gaussian process since the linear differential equation is a\nlinear operation on ϵ(t), is given by the inverse Fourier transform of the spectral density:\nKT (t, t′) := kθT (t′ −t) = 1\n2π\nZ\nsθT (ω) exp(iωt)dω,\n(2)\nWe defer the discussions on the inverse direction, that given a kernel kθT (t′ −t) we can also construct\na continuous-time system, to Appendix A.3.1. Consequently, there is a correspondence between\nthe parameterization of a stochastic ODE and the kernel of a Gaussian process. The mapping is not\nnecessarily one-to-one; however, it may lead to a more convenient way to parameterize a continuous-\ntime process alternatively using deep learning models, especially knowing the connections between\nneural network and Gaussian process (which we highlighted in Section 2.1).\nTo connect the neural network kernel Σ(h) (e.g. for the hth layer of the FFN) to a continuous-time\nsystem, the critical step is to understand the interplay between the neural network kernel and the\ntemporal kernel (e.g. the kernel in (2)):\n• the neural network kernel characterizes the covariance structures among the hidden repre-\nsentation of data (transformed by the neural network) at any ﬁxed time point;\n• the temporal kernel, which corresponds to some continuous-time system, tells how each static\nneural network kernel propagates forward in time. See Figure 1a for a visual illustration.\nContinuing with Section 2.2, it is straightforward construct the integrated continuous-time system as:\na2(x)d2f(x, t)\ndt2\n+a1(x)df(x, t)\ndt\n+a0(x)f(x, t) = b0(x)ϵ(x, t), ϵ(x, t = t0) ∼N(0, Σ(h), ∀t0 ∈T ,\n(3)\nwhere we use the neural network kernel Σ(h) to deﬁne the Gaussian process ϵ(x, t) on the feature\ndimension, so the ODE parameters are now functions of the data as well. To see that (3) generalizes the\nhth layer of a FFN to the temporal domain, we ﬁrst consider a2(x) = a1(x) = 0 and a0(x) = b0(x).\nThen the continuous-time process f(x, t) exactly follows f (h) at any ﬁxed time point t, and its\ntrajectory on the time axis is simply a Gaussian process. When a1(x), a2(x) ̸= 0, f(x, t) still\nmatches f (h) at the initial point, but its propagation on the time axis becomes nontrivial and is now\ncharacterized by the constructed continuous-time system. We can easily the setting to incorporate\nhigher-order terms:\nan(x)dnf(x, t)\ndtn\n+ · · · + a0(x)f(x, t) = bm(x)dmϵ(x, t)\ndtm\n+ · · · + b0(x)ϵ(x, t).\n(4)\nKeeping the heuristics in mind, an immediate question is what is the structure of the corresponding\nkernel function after we combine the continuous-time system with the neural network kernel?\n4\nPublished as a conference paper at ICLR 2021\nClaim 2. The kernel function for f(x, t) in (4) is given by: Σ(h)\nT (x, t; x′, t′) = kθT (x, t; x′, t′) ·\nΣ(h)(x, x′), where θT is the underlying parameterization of\n\b\nai(·)\n\tn\ni=1 and\n\b\nbi(·)\n\tm\ni=1 as functions\nof x. When\n\b\nai\n\tn\ni=1 and\n\b\nbi\n\tm\ni=1 are scalars, Σ(h)\nT (x, t; x′, t′) = kθT (t, t′) · Σ(h)(x, x′).\nWe defer the proof and the discussion on the inverse direction from temporal kernel to continuous-\ntime system to Appendix A.3.1. Claim 2 shows that it is possible to expand any layer of a standard\nneural network to the temporal domain, as part of a continuous-time system using kernel composition.\nThe composition is ﬂexible and can happen at any hidden layer. In particular, given the temporal\nkernel KT and neural network kernel Σ(h), we obtain the neural-temporal kernel on X × T :\nΣ(h)\nT\n= diag\n\u0000Σ(h) ⊗KT\n\u0001\n, where diag(·) is the partial diagonalization operation on X:\nΣ(h)\nT (x, t; x′, t′) = Σ(h)(x, x′) · KT (x, t; x′, t′).\n(5)\nThe above argument shows that instead of taking care of both the deep learning and continuous-time\nsystem, which remains challenging for general architectures, we can convert the problem to ﬁnding a\nsuitable temporal kernel. We further point out that when using neural networks, we are parameterizing\nthe hidden representation (feature lift) in the feature space rather than the kernel function in the kernel\nspace. Therefore, to give a consistent characterization, we should also study the feature representation\nof the temporal kernel and then combine it with the hidden representations of the neural network.\n3.1\nTHE RANDOM FEATURE REPRESENTATION FOR TEMPORAL KERNEL\nWe start by considering the simpler case where the temporal kernel is stationary and independent of\nfeatures: KT (t, t′) = k(t′ −t), for some properly scaled positive even function k(·). The classical\nBochner’s theorem (Bochner, 1948) states that:\nψ(t′ −t) =\nZ\nR\ne−i(t′−t)ωds(ω),\nfor some probability density function s on R,\n(6)\nwhere s(·) is the spectral density function we highlighted in Section 2.2. To compute the inte-\ngral, we may sample (ω1, . . . , ωm) from s(ω) and use the Monte Carlo method: ψ(t′ −t) ≈\n1\nm\nPm\ni=1 e−i(t′−t)ω. Since e−i(t′−t)⊺ω = cos\n\u0000(t′ −t)ω\n\u0001\n−i sin\n\u0000(t′ −t)ω\n\u0001\n, for the real part, we let:\nφ(t) =\n1\n√m\n\u0002\ncos(tω1), sin(tω1) . . . , cos(tωm), sin(tωm)\n\u0003\n,\n(7)\nand it is easy to check that ψ(t′ −t) ≈⟨φ(t), φ(t′)⟩. Since φ(t) is constructed from random samples,\nwe refer to it as the random feature representation of KT . Random feature has been extensive studied\nin the kernel machine literature, however, we propose a novel application for random features to\nparametrize an unknown kernel function. A straightforward idea is to parametrize the spectral density\nfunction s(ω), whose pivotal role has been highlighted in Section 2.2.\nSuppose θT is the distribution parameters for s(ω). Then φθT (t) is also (implicitly) parameterized\nby θT through the samples\n\b\nω(θT )i\n\tm\ni=1 from s(ω). The idea resembles the reparameterization\ntrick for training variational objectives (Kingma & Welling, 2013), which we formalize in the next\nsection. For now, it remains unknown if we can also obtain the random feature representation for non-\nstationary kernels where Bochner’s theorem is not applicable. Note that for a general temporal kernel\nKT (x, t; x′, t′), in practice, it is not reasonable to assume stationarity specially for the feature domain.\nIn Proposition 1, we provide a rigorous result that generalizes the random feature representation for\nnonstationary kernels with convergence guarantee.\nProposition 1. For any (scaled) continuous non-stationary PDS kernel KT on X × T , there exists a\njoint probability measure with spectral density function s(ω1, ω2), such that KT\n\u0000(x, t), (x′, t′)\n\u0001\n=\nEs(ω1,ω2)\n\u0002\nφ(x, t)⊺φ(x′, t′)\n\u0003\nwhere φ(x, t) is given by:\n1\n2√m\n\u0002\n. . . , cos\n\u0000[x, t]⊺ω1,i\n\u0001\n+ cos\n\u0000[x, t]⊺ω2,i\n\u0001\n, sin\n\u0000[x, t]⊺ω1,i\n\u0001\n+ sin\n\u0000[x, t]⊺ω2,i\n\u0001\n. . .\n\u0003\n,\n(8)\nHere,\n\b\n(ω1,i, ω2,i)\n\tm\ni=1 are the m samples from s(ω1, ω2).\nWhen the sample size m\n≥\n8(d+1)\nε2\nlog\n\u0010\nC(d)\n\u0000l2t2\nmaxσp/ε\n\u0001 2d+2\nd+3 /δ\n\u0011\n, with probability at least 1 −δ, for any ε > 0,\nsup\n(x,t),(x′,t′)\n\f\f\fKT\n\u0000(x, t), (x′, t′)\n\u0001\n−φ(x, t)⊺φ(x′, t′)\n\f\f\f ≤ε,\n(9)\n5\nPublished as a conference paper at ICLR 2021\nwhere σ2\np is the second moment of the spectral density function s(ω1, ω2) and C(d) is a constant.\nWe defer the proof to Appendix A.3.2. It is obvious that the new random feature representation in (8)\nis a generalization of the stationary setting. There are two advantages for using the random feature\nrepresentation:\n• the composition in the kernel space suggested by (5) is equivalent to the computationally\nefﬁcient operation f (h)(x) ◦φ(x, t) in the feature space (Shawe-Taylor et al., 2004);\n• we preserve a similar Gaussian process behavior and the neural tangent kernel results that\nwe discussed in Section 2.1, and we defer the discussion and proof to Appendix A.3.3.\nIn the forward-passing computations, we simply replace the original hidden representation f (h)(x) by\nthe time-aware representation f (h)(x) ◦φ(x, t). Also, the existing methods and results on analyzing\nneural networks though Gaussian process and NTK, though not emphasized in this paper, can be\ndirectly carried out to the temporal setting as well (see Appendix A.3.3).\n3.2\nREPARAMETERIZATION WITH THE SPECTRAL DENSITY FUNCTION\nWe now present the gradient computation for the parameters of the spectral distribution using only\ntheir samples. We start from the well-studied case where s(ω) is given by a normal distribution\nN(µ, Σ) with parameters θT = [µ, Σ]. When computing the gradients of θT , instead of sampling\nfrom the intractable distribution s(ω), we reparameterize each sample ωi via: Σ1/2ϵ + µ, where ϵ is\nsampled from a standard multivariate normal distribution. The gradient computations that relied on\nω is now replaced using the easy-to-sample ϵ and θT = [µ, Σ] now become tractable parameters in\nthe model given ϵ. We illustrate reparameterization in our setting in the following example.\nExample 1. Consider a single-dimension homogeneous linear model: f(θ, x) = f (0)(x) = θx.\nWithout loss of generality, we use only a single sample ω1 from the s(ω) which corresponds to the\nfeature-independent temporal kernel kθ(t, t′). Again, we assume s(ω) ∼N(µ, σ).\nThen the time-aware hidden representation for this layer for datapoint (x1, t1) is given by:\nf (0)\nθ,µ,σ(x1, t1) = 1/\n√\n2\n\u0002\nθx1 cos(t1ω1), θx1 sin(t1ω1)\n\u0003\n, ω1 ∼N(µ, σ).\nUsing the reparameterization, given a sample ϵ1 from the standard normal distribution, we have:\nf (0)\nθ,µ,σ(x1, t1) = 1/\n√\n2\n\u0002\nθx1 cos\n\u0000t1(σ1/2ϵ1 + µ)\n\u0001\n, θx1 sin\n\u0000t1(σ1/2ϵ1 + µ)\n\u0001\u0003\n,\n(10)\nso the gradients with respect to all the parameters (θ, µ, σ) can be computed in the usual way.\nDespite the computation advantage, the spectral density is now learnt from the data instead of being\ngiven so the convergence result in Proposition 1 does not provide sample-consistency guarantee. In\npractice, we may also misspecify the spectral distribution and bring extra intractable factors.\nTo provide practical guarantees, we ﬁrst introduce several notations: let KT (S) be the temporal\nkernel represented by random features such that KT (S) = E\n\u0002\nφ⊺φ\n\u0003\n, where the expectation is taken\nwith respect to the data distribution and the random feature vector φ has its samples {ωi}m\ni=1 drawn\nfrom the spectral distribution S. Without abuse of notation, we use φ ∼S to denote the dependency\nof the random feature vector φ on the spectral distribution S provided in (8). Given a neural network\nkernel Σ(h), the neural temporal kernel is then denoted by: Σ(h)\nT (S) = Σ(h) ⊗KT (S). So the\nsample version of Σ(h)\nT (S) for the dataset\n\b\n(xi, ti)\n\tn\ni=1 is given by:\nˆΣ(h)\nT (S) =\n1\nn(n −1)\nX\ni̸=j\nΣ(h)(xi, xj)φ(xi, ti)⊺φ(xj, tj), φ ∼S.\n(11)\nIf the spectral distribution S is ﬁxed and given, then using standard techniques and Theorem 1 it\nis straightforward to show limn→∞ˆΣ(h)\nT (S) →E\n\u0002 ˆΣ(h)\nT (S)\n\u0003\nso the proposed learning schema is\nsample-consistent.\nIn our case, the spectral distribution is learnt from the data, so we need some restrictions on the\nspectral distribution in order to obtain any consistency guarantee. The intuition is that if SθT does\nnot diverge from the true S, e.g. d(SθT ∥S) ≤δ for some divergence measure, the guarantee on S\ncan transfer to SθT with the rate only suffers a discount that does not depend on n.\n6\nPublished as a conference paper at ICLR 2021\nTheorem 1. Consider the f-divergence such that d(SθT ∥S) =\nR\nc\n\u0000dSθT /dS\n\u0001\ndS, with the generator\nfunction c(x) = xk −1 for any k > 0. Given the neural network kernel Σh, let M =\n\r\rΣh\r\r\n∞, then\nPr\n\u0010\nsup\nd(SθT ||S)≤δ\n\f\f ˆΣ(h)\nT (SθT ) →E\n\u0002 ˆΣ(h)\nT (SθT )\n\f\f ≥ε\n\u0011\n≤\n√\n2 exp\n\u0010\n−nε2\n64 max{4, M}(δ + 1)\n\u0011\n+ C(ε),\n(12)\nwhere C(ε) ∝\n\u0010 2l2t2\nmaxσSθT\nϵ/ max{4,M}\n\u0011 2d+2\nd+3 exp\n\u0010\n−\ndhϵ2\n32 max{16,M 2}(d+3)\n\u0011\nthat does not depend on δ.\nThe proof is provided in Appendix A.3.4. The key takeaway from (12) is that as long as the\ndivergence between the learnt SθT and the true spectral distribution is bounded, we still achieve\nsample consistency. Therefore, instead of specifying a distribution family which is more likely to\nsuffer from misspeciﬁcation, we are motivated to employ some universal distribution approximator\nsuch as the invertible neural network (INN) (Ardizzone et al., 2018). INN consists of a series of\ninvertible operations that transform samples from a known auxiliary distribution (such as normal\ndistribution) to arbitrarily complex distributions. The Jacobian that characterize the changes of\ndistributions are made invertible by the INN, so the gradient ﬂow is computationally tractable similar\nto the case in Example 1. We defer the detailed discussions to Appendix A.3.5.\nRemark 1. It is clear at this point that the temporal kernel approach applies to all the neural\nnetworks who have a Gaussian process behavior with a valid neural network kernel, which includes\nthe major architectures such as CNN, RNN and attention mechanism (Yang, 2019).\nFor implementation, at each forward and backward computation, we ﬁrst sample from the auxiliary\ndistribution to construct the random feature representation φ using reparameterization, and then\ncompose it with the selected hidden layer f (h) such as in (10). We illustrate the computation\narchitecture in Figure 2, where we adapt the vanilla RNN to the proposed framework. In Algorithm\n1, we provide the detailed forward and backward computations, using the L-layer FFN from the\nprevious sections as an example.\nFigure 2: The computation architecture of the proposed method using RNN as an example (following\nFigure 1b). Here, we use f (h)(θ, ·) to denote the standard GRU cell, and use g(ψ, ·) to denote the\ninvertible neural network. At the ith step, the GRU cell takes the feature vector xi, and the hidden\nstate of the previous steps ⃗hi, as input. The RF-INN module is called once for each batch.\n4\nRELATED WORK\nThe earliest work that discuss training continuous-time neural network dates back to LeCun et al.\n(1988); Pearlmutter (1995), but no feasible solution was proposed at that time. The proposed approach\nrelates to several ﬁelds that are under active research.\nODE and neural network. Certain neural architecture such as the residual network has been\ninterpreted as approximate ODE solvers (Lu et al., 2018). More direct approaches have been\nproposed to learn differential equations from data (Raissi & Karniadakis, 2018; Raissi et al., 2018a;\n7\nPublished as a conference paper at ICLR 2021\nLong et al., 2018), and signiﬁcant efforts have been spent on developing solvers that combine ODE\nand the back-propagation framework (Farrell et al., 2013; Carpenter et al., 2015; Chen et al., 2018).\nThe closest literature to our work is from Raissi et al. (2018b) who design numerical Gaussian process\nresulting from temporal discretization of time-dependent partial differential equations.\nRandom feature and kernel machine learning. In supervised learning, the kernel trick provides\na powerful tool to characterize non-linear data representations (Shawe-Taylor et al., 2004), but the\ncomputation complexity is overwhelming for large dataset. The random (Fourier) feature approach\nproposed by Rahimi & Recht (2008) provides substantial computation beneﬁts. The existing literature\non analyzing the random feature approach all assume the kernel function is ﬁxed and stationary (Yang\net al., 2012; Sutherland & Schneider, 2015; Sriperumbudur & Szabó, 2015; Avron et al., 2017).\nReparameterization and INN. Computing the gradient for intractable objectives using samples\nfrom auxiliary distribution dates back to the policy gradient method in reinforcement learning (Sutton\net al., 2000). In recent years, the approach gains popularity for training generative models (Kingma &\nWelling, 2013), other variational objectives (Blei et al., 2017) and Bayesian neural networks (Snoek\net al., 2015). INN are often employed to parameterize the normalizing ﬂow that transforms a simple\ndistribution into a complex one by applying a sequence of invertible transformation functions (Dinh\net al., 2014; Ardizzone et al., 2018; Kingma & Dhariwal, 2018; Dinh et al., 2016).\nOur approach characterizes the continuous-time ODE via the lens of kernel. It complements the\nexisting neural ODE methods which are often restricted to speciﬁc architectures, relying on ODE\nsolvers and lacking theoretical understandings. We also propose a novel deep kernel learning approach\nby parameterizing the spectral distribution under random feature representation, which is conceptually\ndifferent from using temporal kernel for time-series classiﬁcation (Li & Marlin, 2015). Our work is a\nextension of Xu et al. (2019; 2020), which study the case for self-attention mechanism.\nAlgorithm 1: Forward pass and parameter update, using the L-layer FFN as an example.\nInput: The FFN f(θ, ·) = {f (1)(θ, ·), . . . , f (L)(θ, ·)}; the invertible neural network g(ψ, ·);\nthe selected hidden layer h; the loss ℓi associated with each input (xi, ti); the auxilliary\ndistribution P.\nfor each mini-batch do\nSample\n\b\nϵ1,j, ϵ2,j\n\tm\nj=1 from the auxilliary distribution P;\nCompute the reparameterized samples ω using the INN g(ψ, ·), e.g. ω1,j(ψ) := g(ψ, ϵ1,j);\nfor sample i in the batch do\nConstruct the random feature representation φψ(xi, ti) using the reparameterized\nsamples (so φ is now explicitly parameterized by ψ) according to eq. (8);\nForward pass: get f (h)(θ, xi), let f (h)\u0000(θ, ψ), xi, ti\n\u0001\n:= f (h)(θ, xi) ◦φψ(xi, ti),\nthen pass it to the following feedforward layers to obtain the ﬁnal output ˆyi ;\nGradient computation: compute the gradients ∇θℓi( ˆyi)\n\f\f\nϵ, ∇ψℓi( ˆyi)\n\f\f\nϵ for the FFN and\nINN respectively, conditioned on the samples from the auxiliary distribution;\nend\nUpdate the parameters using the selected optimizer in a standard batch-wise fashion.\nend\nIt is straightforward from Figure 2 and Algorithm 1 that the proposed approach serves as a plug-in\nmodule and do not modify the original network structures of the RNN and FFN.\n5\nEXPERIMENTS AND RESULTS\nWe focus on revealing the two major advantages of the proposed temporal kernel approach:\n• the temporal kernel approach consistently improves the performance of deep learning models,\nboth for the general architectures such as RNN, CausalCNN and attention mechanism as\nwell as the domain-speciﬁc architectures, in the presence of continuous-time information;\n• the improvement is not at the cost of computation efﬁciency and stability, and we outperform\nthe alternative approaches who also applies to general deep learning models.\n8\nPublished as a conference paper at ICLR 2021\nWe point out that the neural point process and the ODE neural networks have only been shown to\nwork for certain model architectures so we are unable to compare with them for all the settings.\nTime series prediction with standard neural networks (real-data and simulation)\nWe conduct time series prediction task using the vanilla RNN, CausalCNN and self-attention mecha-\nnism with our temporal kernel approach (Figure A.1). We choose the classical Jena weather data for\ntemperature prediction, and the Wikipedia trafﬁc data to predict the number of visits of Wikipedia\npages. Both datasets have vectorized features and are regular-sampled. To illustrate the advantage\nof leveraging the temporal information compared with using only sequential information, we ﬁrst\nconduct the ordinary next-step prediction on the regular observations, which we refer to as Case1. To\nfully illustrate our capability of handling the irregular continuous-time information, we consider the\ntwo simulation setting that generate irregular continuous-time sequences for prediction:\nCase2. we sample irregularly from the history, i.e. xt1, . . . , xtq, q ≤k, to predict xtk+1;\nCase3. we use the full history to predict a dynamic future point, i.e. xtk+q for a random q.\nWe provide the complete data description, preprocessing, and implementation in Appendix B. We use\nthe following two widely-adopted time-aware modiﬁcations for neural networks (denote by NN) as\nbaselines, as well as the classical vectorized autoregression model (VAR).\nNN+time: we directly concatenate the timespan, e.g. tj −ti, to the feature vector. NN+trigo: we\nconcatenate the learnable sine and cosine features, e.g. [sin(π1t), . . . , sin(πkt)], to the feature vector,\nwhere {πi}k\ni=1 are free model parameters. We denote our temporal kernel approach by T-NN.\nFrom Figure 3, we see that the temporal kernel outperforms the baselines in all cases when the\ntime series is irregularly sampled (Case2 and Case3), suggesting the effectiveness of the temporal\nkernel approach in capturing and utilizing the continuous-time signals. Even for the regular Case1\nreported in Table A.1, the temporal kernel approach gives the best results, which again emphasizes\nthe advantage of directly characterize the temporal information over discretization. We also show\nin the ablation studies (Appendix B.6) that INN is necessary for achieving superior performance\ncompared with specifying a distribution family. To demonstrate the stability and robustness, we\nprovide sensitivity analysis in Appendix B.7 for model selection and INN structures.\nFigure 3: The mean absolute error on testing data for the standard neural networks: RNN, CausalCNN\n(denoted by CNN) and self-attention (denoted by Att), for the temporal kernel approach and the\nbaselines methods in Case2 and Case3. The numerical results are averaged over ﬁve repetitions.\nTemporal sequence learning with complex domain models\nNow we study the performance of our temporal kernel approach for the sequential recommendation\ntask with more complicated domain-speciﬁc two-tower architectures (Appendix B.3). Temporal\ninformation is known to be critical for understanding customer intentions, so we choose the two\npublic e-commerce dataset from Alibaba and Walmart.com, and examine the next-purchase recom-\nmendation. To illustrate our ﬂexibility, we select the GRU-based, CNN-based and attention-based\nrecommendation models from the recommender system domain (Hidasi et al., 2015; Li et al., 2017a)\nand equip them with the temporal kernel. The detailed settings, ablation studies and sensitivity analy-\nsis are all in Appendix B. The results are shown in Table A.2. We observe that the temporal kernel\napproach brings various degrees of improvements to the recommendation models by characterizing\n9\nPublished as a conference paper at ICLR 2021\nthe continuous-time information. The positives results from the recommendation task also suggests\nthe potential of our approach for making impact in boarder domains.\n6\nDISCUSSION\nIn this paper, we discuss the insufﬁciency of existing work on characterizing continuous-time data\nwith deep learning models and describe a principled temporal kernel approach that expands neural\nnetworks to characterize continuous-time data. The proposed learning approach has strong theoretical\nguarantees, and can be easily adapted to a broad range of applications such as deep spatial-temporal\nmodelling, outlier and burst detection, and generative modelling for time series data.\nScope and limitation. Although the temporal kernel approach is motivated by the limiting-width\nGaussian behavior of neural networks, in practice, it sufﬁces to use regular widths as we did in our\nexperiments (see Appendix B.3 for the conﬁgurations). Therefore, there are still gaps between our\ntheoretical understandings and the observed empirical performance, which require more dedicated\nanalysis. One possible direction is to apply the techniques in Daniely et al. (2016) to characterize the\ndual kernel view of ﬁnite-width neural networks. The technical detail, however, will be more involved.\nIt is also arguably true that we build the connection between the temporal kernel view and continuous-\ntime system in an indirect fashion, compared with the ODE neural networks. However, our approach\nis fully compatible with the deep learning subroutines while the end-to-end ODE neural networks\nrequire substantial modiﬁcations to the modelling and implementation. Nevertheless, ODE neural\nnetworks are (in theory) capable of modelling more complex systems where the continuous-time\nsetting is a special case. Our work, on the other hand, is dedicated to the temporal setting.\nREFERENCES\nZeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized\nneural networks, going beyond two layers. In Advances in neural information processing systems,\npp. 6155–6166, 2019.\nLynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W Pellegrini, Ralf S Klessen,\nLena Maier-Hein, Carsten Rother, and Ullrich Köthe. Analyzing inverse problems with invertible\nneural networks. arXiv preprint arXiv:1808.04730, 2018.\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On\nexact computation with an inﬁnitely wide neural net. In Advances in Neural Information Processing\nSystems, pp. 8139–8148, 2019a.\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of\noptimization and generalization for overparameterized two-layer neural networks. arXiv preprint\narXiv:1901.08584, 2019b.\nHaim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir\nZandieh. Random fourier features for kernel ridge regression: Approximation bounds and statistical\nguarantees. In International Conference on Machine Learning, pp. 253–262, 2017.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\nAtılım Günes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.\nAutomatic differentiation in machine learning: a survey. The Journal of Machine Learning\nResearch, 18(1):5595–5637, 2017.\nAharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.\nRobust solutions of optimization problems affected by uncertain probabilities. Management\nScience, 59(2):341–357, 2013.\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.\nJournal of the American statistical Association, 112(518):859–877, 2017.\nSalomon Bochner. Vorlesungen über Fouriersche integrale. Chelsea Publishing Company, 1948.\n10\nPublished as a conference paper at ICLR 2021\nStéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic\ntheory of independence. Oxford university press, 2013.\nPeter J Brockwell, Richard A Davis, and Stephen E Fienberg. Time series: theory and methods:\ntheory and methods. Springer Science & Business Media, 1991.\nBob Carpenter, Matthew D Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betan-\ncourt. The stan math library: Reverse-mode automatic differentiation in c++. arXiv preprint\narXiv:1509.07164, 2015.\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\ndifferential equations. In Advances in neural information processing systems, pp. 6571–6583,\n2018.\nEdward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Doctor\nai: Predicting clinical events via recurrent neural networks. In Machine Learning for Healthcare\nConference, pp. 301–318, 2016.\nAmit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:\nThe power of initialization and a dual view on expressivity. In Advances In Neural Information\nProcessing Systems, pp. 2253–2261, 2016.\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516, 2014.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv\npreprint arXiv:1605.08803, 2016.\nArnaud Doucet and Adam M Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years\nlater. Handbook of nonlinear ﬁltering, 12(656-704):3, 2009.\nNan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song.\nRecurrent marked temporal point processes: Embedding event history to vector. In Proceedings of\nthe 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.\n1555–1564, 2016.\nPatrick E Farrell, David A Ham, Simon W Funke, and Marie E Rognes. Automated derivation of the\nadjoint of high-level transient ﬁnite element programs. SIAM Journal on Scientiﬁc Computing, 35\n(4):C369–C393, 2013.\nJoseph Futoma, Sanjay Hariharan, and Katherine Heller. Learning to detect sepsis with a multitask\ngaussian process rnn classiﬁer. arXiv preprint arXiv:1706.04152, 2017.\nBalázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based\nrecommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. In Advances in neural information processing systems, pp.\n8571–8580, 2018.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nDurk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In\nAdvances in Neural Information Processing Systems, pp. 10215–10224, 2018.\nYann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for back-propagation.\nIn Proceedings of the 1988 connectionist models summer school, volume 1, pp. 21–28. CMU,\nPittsburgh, Pa: Morgan Kaufmann, 1988.\n11\nPublished as a conference paper at ICLR 2021\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models\nunder gradient descent. In Advances in neural information processing systems, pp. 8570–8581,\n2019.\nJing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. Neural attentive session-\nbased recommendation. In Proceedings of the 2017 ACM on Conference on Information and\nKnowledge Management, pp. 1419–1428, 2017a.\nShuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, and Le Song. Learning temporal point\nprocesses via reinforcement learning. In Advances in neural information processing systems, pp.\n10781–10791, 2018.\nSteven Cheng-Xian Li and Benjamin M Marlin. Classiﬁcation of sparse and irregularly sampled time\nseries with mixtures of expected gaussian kernels and random features. In UAI, pp. 484–493, 2015.\nYang Li, Nan Du, and Samy Bengio. Time-dependent representation for neural event sequence\nprediction. arXiv preprint arXiv:1708.00065, 2017b.\nZachary C Lipton, David Kale, and Randall Wetzel. Directly modeling missing data in sequences\nwith rnns: Improved classiﬁcation of clinical time series. In Machine Learning for Healthcare\nConference, pp. 253–270, 2016.\nZichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In\nInternational Conference on Machine Learning, pp. 3208–3216, 2018.\nYiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond ﬁnite layer neural networks:\nBridging deep architectures and numerical differential equations. In International Conference on\nMachine Learning, pp. 3276–3285. PMLR, 2018.\nHongyuan Mei and Jason M Eisner.\nThe neural hawkes process: A neurally self-modulating\nmultivariate point process. In Advances in Neural Information Processing Systems, pp. 6754–6764,\n2017.\nTomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan ˇCernock`y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Eleventh annual conference of the international speech\ncommunication association, 2010.\nRadford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business\nMedia, 2012.\nRoman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolaﬁa,\nJeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many\nchannels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499, 2016.\nBarak A Pearlmutter. Gradient calculations for dynamic recurrent neural networks: A survey. IEEE\nTransactions on Neural networks, 6(5):1212–1228, 1995.\nAli Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in\nneural information processing systems, pp. 1177–1184, 2008.\nMaziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear\npartial differential equations. Journal of Computational Physics, 357:125–141, 2018.\nMaziar Raissi, Paris Perdikaris, and George Em Karniadakis. Multistep neural networks for data-\ndriven discovery of nonlinear dynamical systems. arXiv preprint arXiv:1801.01236, 2018a.\nMaziar Raissi, Paris Perdikaris, and George Em Karniadakis. Numerical gaussian processes for\ntime-dependent and nonlinear partial differential equations. SIAM Journal on Scientiﬁc Computing,\n40(1):A172–A198, 2018b.\n12\nPublished as a conference paper at ICLR 2021\nSami Remes, Markus Heinonen, and Samuel Kaski. Non-stationary spectral kernels. In Advances in\nNeural Information Processing Systems, pp. 4642–4651, 2017.\nSimo Särkkä. Bayesian ﬁltering and smoothing, volume 3. Cambridge University Press, 2013.\nJohn Shawe-Taylor, Nello Cristianini, et al. Kernel methods for pattern analysis. Cambridge\nuniversity press, 2004.\nMikhail Aleksandrovich Shubin. Pseudodifferential operators and spectral theory, volume 200.\nSpringer, 1987.\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,\nMostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep neural\nnetworks. In International conference on machine learning, pp. 2171–2180, 2015.\nHossein Soleimani, James Hensman, and Suchi Saria. Scalable joint models for reliable uncertainty-\naware event prediction. IEEE transactions on pattern analysis and machine intelligence, 40(8):\n1948–1963, 2017.\nBharath Sriperumbudur and Zoltán Szabó. Optimal rates for random fourier features. In Advances in\nNeural Information Processing Systems, pp. 1144–1152, 2015.\nDougal J Sutherland and Jeff Schneider. On the error of random fourier features. arXiv preprint\narXiv:1506.02785, 2015.\nRichard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-\nods for reinforcement learning with function approximation. In Advances in neural information\nprocessing systems, pp. 1057–1063, 2000.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nNorbert Wiener et al. Generalized harmonic analysis. Acta mathematica, 55:117–258, 1930.\nAndrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.\nIn Artiﬁcial intelligence and statistics, pp. 370–378, 2016.\nDa Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Self-attention with\nfunctional time representation learning. In Advances in Neural Information Processing Systems,\npp. 15889–15899, 2019.\nDa Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive representa-\ntion learning on temporal graphs. arXiv preprint arXiv:2002.07962, 2020.\nAkira Moiseevich Yaglom. Correlation theory of stationary and related random functions. Volume I:\nBasic Results., 526, 1987.\nGreg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,\ngradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,\n2019.\nTianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nyström method\nvs random fourier features: A theoretical and empirical comparison. In Advances in neural\ninformation processing systems, pp. 476–484, 2012.\n13\nPublished as a conference paper at ICLR 2021\nA\nAPPENDIX\nWe provide the omitted proofs, detailed discussions, extensions and complete numerical results.\nA.1\nNUMERICAL RESULTS FOR SECTION 5\nFigure A.1: Visual illustrations on how we equip the standard neural architectures with the temporal\nkernel using the random Fourier feature with invertible neural network (the RFF-INN blocks).\nCase 1\nWeather\nWikipedia\nVAR\n0.2643\n2.31\nRNN\n0.2487/.002\n0.5142/.003\nRNN-time\n0.2629/.001\n0.4698/.004\nRNN-trigo\n0.2526/.003\n0.4542/.004\nT-RNN\n0.2386/.002\n0.4330/.002\nCNN\n0.3103/.002\n0.4998/.003\nCNN-time\n0.2933/.003\n0.4852/.001\nCNN-trigo\n0.2684/.004\n0.4556/.002\nT-CNN\n0.2662/.003\n0.4399/.003\nAttention\n0.4052/.003\n0.4795/.003\nAttention-time\n0.4298/.003\n0.4809/.002\nAttention-trigo\n0.2887/.002\n0.4445/.003\nT-Attention\n0.2674/.004\n0.4226/.002\nTable A.1: Mean absolute error for time series prediction of the regular scenario of Case 1. We\nunderline the best results for each neural architecture, and the overall best results are highlighted in\nbold-font. The reported results are averaged over ﬁve repetitions, with the standard errors provided.\nAlibaba\nWalmart\nMetric\nAccuracy\nDCG\nAccuracy\nDCG\nGRU-Rec\n77.81/.21\n47.12/.11\n18.09/.13\n3.44/.21\nGRU-Rec-time\n77.70/.24\n46.21/.13\n17.66/.16\n3.29/.23\nGRU-Rec-trigo\n78.95/.19\n49.01/.11\n21.54/.13\n6.67/.18\nT-GRU-Rec\n79.47/.35\n49.82/.40\n23.41/.11\n8.44/.21\nCNN-Rec\n74.89/.33\n43.91/.22\n15.98/.18\n1.97/.19\nCNN-Rec-time\n74.85/.31\n43.88/.21\n15.95/.18\n1.96/.17\nCNN-Rec-trigo\n75.97/.21\n45.86/.23\n17.74/.17\n3.80/.15\nT-CNN-Rec\n76.45/.16\n46.55/.38\n18.59/.33\n4.56/.31\nATTN-Rec\n51.82/.44\n30.41/.52\n20.41/.38\n7.52/.18\nATTN-Rec-time\n51.84/.43\n30.45/.50\n20.43/.36\n7.54/.19\nATTN-Rec-trigo\n53.05/.30\n33.10/.29\n24.49/.15\n8.93/.13\nT-ATTN-Rec\n53.49/.31\n33.58/.30\n25.51/.17\n9.22/.15\nTable A.2: Accuracy and discounted cumulative gain (DCG) for the domain-speciﬁc models on the\ntemporal recommendation tasks. See Appendix B for detail. We underline the best results for each\nneural architecture, and the overall best results are highlighted in bold-font.\n14\nPublished as a conference paper at ICLR 2021\nA.2\nSUPPLEMENTARY MATERIAL FOR SECTION 2\nWe discuss the detailed background for the Gaussian process behavior of neural network and the\ntraining trajectory under neural tangent kernel, as well as the proof for Claim 1.\nA.2.1\nGAUSSIAN PROCESS BEHAVIOR AND NEURAL TANGENT KERNEL FOR DEEP\nLEARNING MODELS\nThe Gaussian process (GP) view of neural networks at random initialization was originally discussed\nin (Neal, 2012). Recently, CNN and other standard neural architectures have all been recognized\nas functions drawn from GP in the limit of inﬁnite network width (Novak et al., 2018; Yang, 2019).\nWhen trained by gradient descent under inﬁnitesimal step schedule, the gradient ﬂow of the standard\nneural architectures can be described by the notion of Neural Tangent Kernel (NTK) whose asymptotic\nbehavior under inﬁnite network width is known (Jacot et al., 2018). The discovery of NTK has led\nto several papers studying the training and generalization properties of neural networks (Allen-Zhu\net al., 2019; Arora et al., 2019a;b).\nFor a L-layer FNN f(θ, x) = f (L) with hidden dimensions {dh}L\nh=1 and recursively deﬁned via:\nf (L) = W(L)f (L)(x) + b(L),\nf (h)(x) =\n1\n√dh\nW(h)σ\n\u0000f (h−1)(x)\n\u0001\n+ b(h),\nf (0)(x) = x,\n(A.1)\nfor h = 1, 2, . . . , L −1, where σ(.) is the activation function and the layer weights W(L) ∈RdL−1,\nW(h) ∈Rdh−1×dh and intercepts are initialized by sampling independently from N(0, 1) (without\nloss of generality). As d1, . . . , dL →∞, f (h) tend in law to i.i.d Gaussian processes with covariance\nΣh deﬁned recursively as shown by Neal (2012):\nΣ(1)(x, x′) = 1\nh1\nx⊺x′ + 1,\nΣ(h)(x, x′) = Ef∼N(0,Σ(h−1))\n\u0002\nσ\n\u0000f(x)\n\u0001\nσ\n\u0000f(x′)\n\u0001\u0003\n+ 1.\n(A.2)\nWe also refer to Σ(h) as the neural network kernel to distinguish from the other kernel notions. Given\na training dataset {xi, yi}n\ni=1, let f\n\u0000θ(s)\n\u0001\n=\n\u0000f(θ(s), x1), . . . , f(θ(s), xn)\n\u0001\nbe the network outputs\nat the sth training step and y = (y1, . . . , yn).\nWhen training the network by minimizing the squared loss ℓ(θ) with inﬁnitesimal learning rate, i.e.\ndθ(s)\nds\n= −∇ℓ(θ(s)), the network outputs at training step s follows the evolution (Jacot et al., 2018):\ndf\n\u0000θ(s)\n\u0001\nds\n= −Θ(s) × (f\n\u0000θ(s)\n\u0001\n−y),\n\u0002\nΘ(s)\n\u0003\nij =\nD∂f(θ(s), xi)\n∂θ\n, ∂f(θ(s), xj)\n∂θ\nE\n.\n(A.3)\nThe above Θ(s) is referred to as the NTK, and recent results shows that when the network widths\ngo to inﬁnity (or sufﬁciently large), Θ(s) converges to a ﬁxed Θ0 almost surely (or with high\nprobability).\nFor a standard L-layer FFN, the NTK Θ0 = Θ(L)\n0\nfor parameters {W(h), b(h)} on the hth-layer can\nalso be computed recursively:\nΘ(h)\n0 (xi, xj) = Σ(h)(xi, xj)\n˙Σ(k)(xi, xj) = Ef∼N(0,Σ(k−1))\n\u0002\n˙σ(f(xi)) ˙σ(f(xj))\n\u0003\n,\nand Θ(k)\n0 (xi, xj) = Θ(k−1)\n0\n(xi, xj) ˙Σ(k)(xi, xj) + Σ(k)(xi, xj),\nk = h + 1, . . . , L.\n(A.4)\nA number of optimization and generalization properties of neural networks can be studied using\nNTK, which we refer the interested readers to (Lee et al., 2019; Allen-Zhu et al., 2019; Arora et al.,\n2019a;b). We also point out that the above GP and NTK constructions can be carried out on all\nstandard neural architectures including CNN, RNN and the attention mechanism (Yang, 2019).\nA.2.2\nPROOF FOR CLAIM 1\nIn this part, we denote the continuous-time system by X(t) in order to introduce the full set notations\nthat are needed for our proof, where the length of the discretized interval is explicitly considered. Note\nthat we especially construct the example in Section 2 so that the derivations are not too cumbersome.\nHowever, the techniques that we use here can be extended to prove the more complicated settings.\n15\nPublished as a conference paper at ICLR 2021\nProof. Consider X(t) to be the second-order continuous-time autoregressive process with covariance\nfunction k(t) and spectral density function (SDF) s(ω) such that s(ω) =\nR ∞\n−∞exp(−iωt)k(t)dt.\nThe covariance function of the discretization Xa[n] = X(na) with any ﬁxed interval a > 0 is then\ngiven by ka[n] = k(na). According to standard results in time series, the SDF of X(t) is given by in\nthe form of:\ns(ω) =\na1\nω2 + b2\n1\n+\na2\nω2 + b2\n,\na1 + a2 = 0,\na1b2\n2 + a2b2\n1 ̸= 0.\n(A.5)\nWe assume without loss of generality that b1, b2 are positive numbers. Note that the kernel function\nfor Xa[i] can also be given by\nka[n] =\nZ ∞\n−∞\nexp(ianω)s(ω)dω\n= 1\na\n∞\nX\nk=−∞\nZ (2k+1)π\n(2k−1)π\nexp(inω)s(ω/a)dω\n= 1\na\nZ ∞\n−∞\nexp(inω)\n∞\nX\nk=−∞\ns\n\u0000ω + 2kπ\na\n\u0001\ndω,\n(A.6)\nwhich suggests that the SDF for the discretization Xa[n] can be given by:\nsa(ω) = 1\na\n∞\nX\nk=−∞\ns\n\u0000ω + 2kπ\nh\n\u0001\n= a1\n2\n\u0010\ne2ab1 −1\nb1|eab1 −eiω|2 −\ne2ab2 −1\nb1|eab2 −eiω|2\n\u0011\n=\na1(d1 −2d2 cos(ω))\n2b1b2|(eab1 −eiω)(eab2 −eiω)|2 ,\n(A.7)\nwhere d2 = b2eab2(e2ab2 −1) −b1eab1(e2ab2 −1). By the deﬁnition of discrete-time auto-regressive\nprocess, Xa[n] is a second-order AR process only if d2 = 0, which happens if and only if: b2/b1 =\n(eab2 −e−ab2)/(eab1 −e−ab1). However, the function g(x) = exp(ax) −exp(−ax) is concave on\n[0, ∞) (since the time interval a > 0) and g(0) = 0, the-above equality hold if b1 = b2. However,\nthis contradicts with (A.5), since a1 + a2 = 0 and a1b2\n2 + a2b2\n1 ̸= 0 suggests a1(b1 −b2)2 ̸= 0.\nHence, Xa[n] cannot be a second-order discrete-time auto-regressive process.\nA.3\nSUPPLEMENTARY MATERIAL FOR SECTION 3\nWe ﬁrst present the related discussions and proof for Claim 2 on the connection between continuous-\ntime system and temporal kernel. Then we prove the convergence result in Theorem 1 regarding the\nrandom feature representation for non-stationary kernel. In the sequel, we show the new results for\nthe Gaussian process behavior and neural tangent kernel under random feature representations, and\ndiscuss the potential usage of our results. Finally, we prove the sample-consistency result when the\nspectral distribution is misspeciﬁed.\nA.3.1\nPROOF AND DISCUSSIONS FOR CLAIM 2\nProof. Recall that we study the dynamic system given by:\nan(x)dnf(x, t)\ndtn\n+ · · · + a0(x)f(x, t) = bm(x)dmϵ(x, t)\ndtm\n+ · · · + b0ϵ(x, t),\n(A.8)\nwhere ϵ(x, t = t0) ∼N(0, Σ(h), ∀t0 ∈T . The solution process to the above continuous-time\nsystem is also a Gaussian process, since ϵ(x, t) is a Gaussian process and the solution of a linear\ndifferent equation is a linear operation on the input. For the sake of notation, we assume b0(x) = 1\nand b1(x) = 0, . . . , bm(x) = 0, which does not change the arguments in the proof. We apply the\nFourier transformation transformation on both sides and solve the for Fourier transform ˜f(iωx, iω):\n˜f(iωx, iω) =\n\u0010\n1\nan(x) · (iω)q + · · · + a1(x) · iω + a0(x)\n\u0011\nW(iω; ωx),\n(A.9)\n16\nPublished as a conference paper at ICLR 2021\nwhere W(iω; ωx) is the Fourier transform of ϵ(x, t).\nIf we do not make the assumption on\n{bj(x)}m\nj=1, they will simply show up on the numeration in the same fashion as {aj(x)}n\nj=1. Let\nGθT (iω; x) = aq(x) · (iω)q + · · · + a1(x) · iω + a0(x),\nand p(ωx) = |W(iω; ωx)|2 be the spectral density of the Gaussian process corresponding to ϵ\n(its spectral density does not depend on ω because ϵ is a Gaussian white noise process on the\ntime dimension). The dependency of G(· ; ·) on θT is because we deﬁned θT to the underlying\nparameterization of {aj(·)}n\nj=1 in the statement of Claim 2. Then the spectral density of the process\nf(x, t) is given by\np(ω, ωx) = C · p(ωx)|GθT (iω; x)|2 ∝p(ωx)pθT (ω; x),\nwhere C is constant that corresponds to the spectral density of the random Gaussian noise on the time\ndimension. Notice that the spectral density function obtained this way is regular, since it has the form\nof pθT (ω; x) = constant/(polynomial of ω2).\nTherefore, according to the classical Wiener-Khinchin theorem Brockwell et al. (1991), the covariance\nfunction of the solution process is given by the inverse Fourier transform of the spectral density:\nψ(x, t) = 1\n2π\nZ\np(ω, ωx) exp\n\u0000i[ω, ωx]⊺[t, x]\n\u0001\nd(ω, ωx)\n∝\nZ\npθT (ω; x) exp(iωt)dω ·\nZ\np(ωx) exp(iω⊺\nxx)dωx\n∝KθT\n\u0000(x, t), (x, t)\n\u0001\n· Σ(h)(x, x).\n(A.10)\nAnd therefore we reach the conclusion in Claim 2 by taking Σ(h)\nT (x, t; x′, t′) = ψ(x−x′, t−t′).\nThe inverse statement of Claim 2 may not be always true, since not all the neural-temporal kernel can\nﬁnd a exact corresponding continuous-time system in the form of (A.8). However, we may construct\nthe continuous-time system that approximates the kernel (arbitrarily well) in the following way, using\nthe polynomial approximation tools such as the Taylor expansion.\nFor a neural-temporal kernel Σ(h)\nT , we ﬁrst compute it Fourier transform to obtain the spec-\ntral density p(ωx, ω).\nNote that p(ωx, ω) should be a rational function in the form of\n(polynomial in ω2)/(polynomial in ω2), or otherwise it does not have stable spectral factorization\nthat leads to a linear dynamic system. To achieve the goal, we can always apply Taylor expansion or\nPade approximants that recovers the p(ωx, ω) arbitrarily well.\nThen we conduct a spectral factorization on p(ωx, ω) to ﬁnd G(iωx, iωt) and p(ωx) such that\np(ωx, ω) = G(iωx, iωt)p(ωx)G(−iωx, −iωt). Since p(ωx, ω) is now in a rational function form\nof ω2, we can ﬁnd G(iωx, iωt) as:\nbk(iωx) · (iω)k + · · · + b1(iωx) · (iω) + b0(iωx)\naq(iωx) · (iω)q + · · · + a1(iωx) · (iω) + a0(iωx).\nLet αj(x) and βj(x) be the pseudo-differential operators of aj(iωx) and bj(iωx) deﬁned in terms of\ntheir inverse Fourier transforms (Shubin, 1987), then the corresponding continuous-time system is\ngiven by:\nαq(x)dqf(x, t)\ndtq\n+ · · · + α0(x)f(x, t) = βk(x)dkϵ(t)\ndtk\n+ · · · + β0(x)ϵ(t).\n(A.11)\nFor a concrete end-to-end example, we consider the simpliﬁed setting where the temporal kernel\nfunction is given by:\nKθT (t, t′) := kθ1,θ2,θ3(t −t′) = θ2\n2\n21−θ1\nΓ(θ1)\n\u0000p\n2θ1\nt −t′\nθ3\n\u0001θ1Bθ1\n\u0000p\n2θ1\nt −t′\nθ3\n\u0001\n,\nwhere Bθ1(·) is the Bessel function so KθT (t, t′) belongs to the well-known Matern family. It is\nstraightforward to show that the spectral density function is given by:\ns(ω) ∝\n\u00102θ1\nθ2\n3\n+ ω2\u0011−(θ1+1/2)\n.\n17\nPublished as a conference paper at ICLR 2021\nAs a consequence, we see that s(ω) ∝\n\u0010p\n2θ1\nθ3\n+ iω\n\u0011−(θ1+1/2)\u0010p\n2θ1\nθ3\n−iω\n\u0011−(θ1+1/2)\n, so we\ndirectly have GθT (ω) =\n\u0010p\n2θ1\nθ3\n+ iω\n\u0011−(θ1+1/2)\ninstead of having to seek for polynomial approxi-\nmation. Now we can easily expand GθT (ω) using the binomial formula to ﬁnd the linear parameters\nfor the continuous-time system. For instance, when θ1 = 3/2, we have:\nd2f(t)\ndt2\n+ 2\n√2θ1\nθ3\ndf(t)\ndt\n+ 2θ1\nθ2\n3\nf(t) = ϵ(t).\nA.3.2\nPROOF FOR PROPOSITION 1\nProof. We ﬁrst need to show that the random Fourier features for the non-stationary kernel\nKT\n\u0000(x, t), (x′, t′)\n\u0001\ncan be given by (11), i.e.\nφ(x, t) =\n1\n2√m\n\u0002\n. . . , cos\n\u0000[x, t]⊺ω1,i\n\u0001\n+cos\n\u0000[x, t]⊺ω2,i\n\u0001\n, sin\n\u0000[x, t]⊺ω1,i\n\u0001\n+sin\n\u0000[x, t]⊺ω2,i\n\u0001\n. . .\n\u0003\n.\nTo simplify notations, we let z := [x, t] ∈Rd+1 and Z = X × T . For non-stationary kernels, their\ncorresponding Fourier transform can be characterized by the following lemma. Assume without loss\nof generality that KT is differentiable.\nLemma A.1 (Yaglom (1987)). A non-stationary kernel k(z1, z2) is positive deﬁnite in Rd if and\nonly if after scaling, it has the form:\nk(z1, z2) =\nZ\nexp\n\u0000i(ω⊺\n1z1 −ω⊺\n2z2)\n\u0001\nµ(dω1, dω2),\n(A.12)\nwhere µ(dω1, dω2) is some positive-semideﬁnite probability measure with bounded variation.\nThe above lemma can be think of as the extension of the classical Bochner’s theorem underlies the\nrandom Fourier feature for stationary kernels. Notice that when covariance function for the measure µ\nonly has non-zero diagonal elements and ω1 = ω2, then we recover the spectral representation stated\nin the Bochner’s theorem. Therefore, we can also approximate (A.12) with the Monte Carlo integral.\nHowever, we need to ensure the positive-semideﬁniteness of the spectral density for µ(dω1, dω2),\nwhich we denote by p(ω1, ω2). It has been suggested in Remes et al. (2017) that we consider another\ndensity function q(ω1, ω2) and let p be taken on the product space of q and then symmetrise:\np(ω1, ω2) = 1\n4\n\u0000q(ω1, ω2) + q(ω2, ω1) + q(ω1, ω1) + q(ω2, ω2)\n\u0001\n.\n(A.13)\nThen (A.12) suggests that\nk(z1, z2) = 1\n4Eq\nh\nexp\n\u0000i(ω⊺\n1z1−ω⊺\n2z2)\n\u0001\n+ exp\n\u0000i(ω⊺\n2z2 −ω⊺\n1z1)\n\u0001\n+ exp\n\u0000i(ω⊺\n1z1 −ω⊺\n1z1)\n\u0001\n+ exp\n\u0000i(ω⊺\n2z2 −ω⊺\n2z2)\n\u0001i\n.\nRecall that the real part of exp\n\u0000i(ω⊺\n1z1 −ω⊺\n2z2)\n\u0001\nis given by cos(ω⊺\n1z1 −ω⊺\n2z2). So with the\nTrigonometric equalities, it is straightforward to verify that k(z1, z2) = Eq\n\u0002\nφ(z)⊺φ(z)\n\u0003\n. Hence, the\nrandom Fourier features for non-stationary kernel can be given in the form of (11).\nThen we show the uniform convergence result as the number of samples goes to inﬁnity when comput-\ning Eq\n\u0002\nφ(z)⊺φ(z)\n\u0003\nby the Monte Carlo integral. Let ˜Z = Z × Z, so ˜Z = {(x, t, x′, t, )\n\r\r x, x′ ∈\nX; t, t′ ∈T }. Since diam(X) = l and T = [0, tmax], we have diam( ˜Z) = l2t2\nmax. Let the\napproximation error be\n∆(z, z′) = φ(z)⊺φ(z′) −KT (z.z′).\n(A.14)\nThe strategy is to use a ϵ-net covering for the input space ˜Z, which would require N\n=\n\u00002l2t2\nmax/r\n\u0001d+1 balls of radius r. Let C = {ci}N\ni=1 be the centers for each ϵ-ball. We ﬁrst show the\nbound for |∆(ci)| and the Lipschitz constant L∆of the error function ∆, and then combine them to\nget the desired result.\n18\nPublished as a conference paper at ICLR 2021\nSince ∆is continuous and differentiable w.r.t z, z′ according to the deﬁnition of φ, we have L∆=\n\r\r∇∆(c∗)\n\r\r, where c∗= arg maxc∈C\n\r\r∇∆(c)\n\r\r. Let c∗= (˜z, ˜z′). By checking the regularity\nconditions for exchanging the integral and differential operation, we verify that E\n\u0002\n∇φ(z)⊺φ(z′)\n\u0003\n=\n∇E\n\u0002\nφ(z)⊺φ(z′)\n\u0003\n= ∇E\n\u0002\nKT (z, z′)\n\u0003\n. We do not present the details here, since it is easy to check the\nregularity of φ(z)⊺φ(z′) as it consists of the sine and cosine functions who are continuous, bounded\nand have continuous bounded derivatives. Hence, we have:\nE\n\u0002\nL2\n∆\n\u0003\n= E˜z,˜z′\nh\r\r∇φ(˜z)⊺φ(˜z′) −∇KT (˜z, ˜z′)\n\r\r2i\n= E˜z,˜z′\nh\nE∥∇φ(˜z)⊺φ(˜z′)∥2 −2∥∇KT (˜z, ˜z′)∥· ∥∇φ(˜z)⊺φ(˜z′)∥+ ∥∇KT (˜z, ˜z′)∥2i\n≤E˜z,˜z′\nh\nE∥∇φ(˜z)⊺φ(˜z′)∥2 −∥∇KT (˜z, ˜z′)∥2i\n(by Jensen’s inequality)\n≤E∥∇φ(˜z)⊺φ(˜z′)∥2\n= E\n\r\r\r∇\n\u0000cos(˜z⊺ω1) + cos(˜z⊺ω2)\n\u0001\u0000cos((˜z′)⊺ω1) + cos((˜z′)⊺ω2)\n\u0001\n+\n\u0000sin(˜z⊺ω1) + sin(˜z⊺ω2)\n\u0001\u0000sin((˜z′)⊺ω1) + sin((˜z′)⊺ω2)\n\u0001\r\r\r\n2\n= 2E\n\r\r\rω1\n\u0000sin(˜z⊺ω1 −(˜z′)⊺ω1) + sin((˜z′)⊺ω2 −˜z⊺ω1)\n\u0001\n+ ω2\n\u0000sin(˜z⊺ω1 −(˜z′)⊺ω2) + sin((˜z′)⊺ω2 −˜z⊺ω2)\n\u0001\r\r\r\n2\n≤8E\n\r\r[ω1, ω2]\n\r\r2 = 8σ2\np.\n(A.15)\nHence, by the Markov’s inequality, we have\np\n\u0010\nL∆≥ϵ\n2r\n\u0011\n≤8σ2\np\n\u00102r\nϵ\n\u0011\n.\n(A.16)\nThen we notice that for all c ∈C, ∆(c) is the mean of m/2 terms bounded by [−1, 1], and the\nexpectation is 0. So applying a union bound and the Hoeffding’s inequality on bounded random\nvariables, we have:\np\n\u0010\n∪N\ni=1 |∆(ci)| ≥ϵ\n2\n\u0011\n≤2N exp\n\u0010\n−mϵ2\n16\n\u0011\n.\n(A.17)\nCombining the above results, we get\np\n\u0010\nsup\n(z,z′)∈C\n\f\f∆(z, z′)\n\f\f ≤ϵ\n\u0011\n≥1 −32σ2\npr2\nϵ2\n−2r−(d+1)\u00102l2tmax\nr\n\u0011d+1\nexp\n\u0010\n−mϵ2\n16\n\u0011\n≥C(d)\n\u0010l2t2\nmaxσp\nϵ\n\u00112(d+1)/(d+3)\nexp\n\u0010\n−\nmϵ2\n8(d + 3)\n\u0011\n,\n(A.18)\nwhere in the second inequality we optimize over r such that r∗=\n\u0010\n(d+1)k1\nk2\n\u00111/(d+3)\nwith\nk1 = 2(2l2t2\nmax)d+1 exp(−mϵ2/16) and k2 = 32σ2\npϵ−2. The constant term is given by C(d) =\n2\n7d+9\nd+3\n\u0010\u0000 d+1\n2\n\u0001 −d−1\nd+3 +\n\u0000 d\n2\n\u0001\n2\nd+3 \u0011\n.\nA.3.3\nTHE GAUSSIAN PROCESS BEHAVIOR AND NEURAL TANGENT KERNEL AFTER\nCOMPOSING WITH TEMPORAL KERNEL WITH THE RANDOM FEATURE\nREPRESENTATION\nThis section is dedicated to show the inﬁnite-width Gaussian process behavior and neural tangent\nkernel properties, similar to what we discussed in Appendix A.2, when composing neural networks\nin the feature space with the random feature representation of the temporal kernel.\nFor brevity, we still consider the standard L-layer FFN of (A.1). Suppose we compose the FFN with\nthe random feature representation φ(x, t) at the kth layer. It is easy to see that the neural network\n19\nPublished as a conference paper at ICLR 2021\nkernel for the ﬁrst k −1 layer are unchanged, so we compute them in the usual way as in (A.2). For\nthe kth layer, it is straightforward to verify that:\nlim\ndk→∞E\nh 1\ndk\nD\nW(k)f (k−1)(θ, x) ◦φ(x, t) , W(k)f (k−1)(θ, x′) ◦φ(x′, t′)\nE\f\f\f f (k−1)i\n→Σ(k)(x, x′) · KT\n\u0000(x, t), (x′, t′)\n\u0001\n.\nThe intuition is that the randomness in W (thus f(θ, .)) and φ(., .) are independent, i.e. the former\nis caused by network parameter initializations and the later is induced by the random features. The\ncovariance functions for the subsequent layers can be derived by induction, e.g. for the (k + 1)th\nlayer we have:\nΣ(k+1)\nT\n\u0000(x, t), (x′, t′)\n\u0001\n= Ef∼N\n\u00000,Σ(k)⊗KT\n\u0001\u0002\nσ\n\u0000f(x, t)\n\u0001\nσ\n\u0000f(x′, t′)\n\u0001\u0003\n.\nIn summary, composing the FNN, at any given layer, with the temporal kernel using its random\nfeature representation does not change the inﬁnite-width Gaussian process behavior. The statement is\ntrue for all the deep learning models who also have the Gaussian process behavior, which includes\nmost of the standard neural architectures including RNN, CNN and attention mechanism (Yang,\n2019).\nThe derivations for the NTK, however, is more involved since the gradient on all the layers are\naffected. We summarize the result for the L-layer FFN in the following proposition and provide the\nderivations afterwards.\nProposition A.1. Suppose f (k)\u0000θ, (x, t)\n\u0001\n= vec(f (k)(θ, x) ◦φ(x, t)) in the standard L-layer FFN.\nLet Σ(h)\nT\n= Σ(h) for h = 1, . . . , k, Σ(k)\nT\n= Σ(k)\nT\n⊗KT and Σ(h)\nT\n= Ef∼N(0,Σ(h)\nT\n)[σ(f)σ(f)] + 1\nfor h = k + 1, . . . , L. If the activation functions σ have polynomially bounded weak derivatives,\nas the network widths d1, . . . , dL →∞, the neural tangent kernel Θ(L) converges almost surely to\nΘ(L)\nT\nwhose partial application on parameters {W(h), b(h)} in the hth-layer is given recursively by:\nΘ(h)\nT\n= Σ(h)\nT ,\nΘ(k)\nT\n= Θ(k−1)\nT\n⊗˙Σ(k)\nT\n+ Σ(k)\nT ,\nk = h + 1, . . . , L.\n(A.19)\nProof. The strategies for deriving the NTK and show the convergence has been discussed in Jacot\net al. (2018); Yang (2019); Arora et al. (2019a). The key purpose for us presenting the derivations\nhere is to show how the convergence results for the neural-temporal Gaussian (Section 4.2) affects\nthe NTK. To avoid the cumbersome notations induced by the peripheral intercept terms, here we omit\nthe intercept terms b in the FFN without loss of generality. We let g(h) =\n1\n√dh σ\n\u0000f (h)(x, t)\n\u0001\n, so the\nFFN can be equivalently deﬁned via the recursion: f (h) = W(h)g(h−1)(x, t). For the ﬁnal output\nf\n\u0000θ, (x, t)\n\u0001\n:= W(L)f (L)(x, t), the partial derivative to W(h) can be given by:\n∂f\n\u0000θ, (x, t)\n\u0001\n∂W(h)\n= z(h)(x, t)\n\u0000g(h−1)(x, t)\n\u0001⊺,\n(A.20)\nwith z(h) deﬁned by;\nz(h)(x, t) =\n\u001a 1,\nh = L,\n1\n√dh D(h)(x, t)\n\u0000W(h+1)\u0001⊺z(h+1)(x, t),\nh = 1, . . . , L −1,\n(A.21)\nwhere\nD(h)(x, t) =\n\n\n\ndiag\n\u0010\n˙σ\n\u0000f (h)(x, t)\n\u0001\u0011\n,\nh = k, . . . , L −1,\ndiag\n\u0010\n˙σ\n\u0000f (h)(x)\n\u0001\u0011\n,\nh = 1, . . . , k −1.\nUsing the above deﬁnitions, we have:\nD∂f\n\u0000θ, (x, t)\n\u0001\n∂W(h)\n, ∂f\n\u0000θ, (x′, t′)\n\u0001\n∂W(h)\nE\n=\nD\nz(h)(x, t)\n\u0000g(h−1)(x, t)\n\u0001⊺, z(h)(x′, t′)\n\u0000g(h−1)(x′, t′)\n\u0001⊺E\n=\n\ng(h−1)(x, t), g(h−1)(x′, t′)\n\u000b\n·\n\nz(h)(x, t), z(h)(x′, t′)\n\u000b\n20\nPublished as a conference paper at ICLR 2021\nWe have established in Section 4.2 that\n\ng(h−1)(x, t), g(h−1)(x′, t′)\n\u000b\n→Σ(h−1)\nT\n\u0000(x, t), (x′, t′)\n\u0001\n,\nwhere\nΣ(h)\nT\n\u0000(x, t), (x′, t′)\n\u0001\n=\n\n\n\n\n\nΣ(h)(x, x′)\nh = 1, . . . , k\nΣ(h)(x, x′) · KT\n\u0000(x, t), (x′, t′)\n\u0001\nh = k\nEf∼N\n\u00000,Σ(h−1)\nT\n\u0001\nh\nσ\n\u0000f(x, t)\n\u0001\nσ\n\u0000f(x′, t′)\n\u0001i\nh = k + 1, . . . , L.\n(A.22)\nBy the deﬁnition of z(h), we get\n\nz(h)(x, t), z(h)(x′, t′)\n\u000b\n= 1\ndh\nD\nD(h)(x, t)\n\u0000W(h+1)\u0001⊺z(h+1)(x, t), D(h)(x′, t′)\n\u0000W(h+1)\u0001⊺z(h+1)(x′, t′)\nE\n≈1\ndh\nD\nD(h)(x, t)\n\u0000W(h+1)\u0001⊺z(h+1)(x, t), D(h)(x′, t′)\n\u0000 ˜\nW(h+1)\u0001⊺z(h+1)(x′, t′)\nE\n→1\ndh\ntr\n\u0010\nD(h)(x, t)D(h)(x′, t′)\n\u0011\nz(h+1)(x, t), z(h+1)(x′, t′)\n\u000b\n→˙Σh\nT\n\u0000(x, t), (x′, t′)\n\nz(h+1)(x, t), z(h+1)(x′, t′)\n\u000b\n.\n(A.23)\nThe approximation in the third line is made because the W(h+1) in the right half is replaced by its\ni.i.d copy under Gaussian initialization. This does not change the limit when dh →∞when the\nactionvation functions have polynomially bounded weak derivatives Yang (2019) such the ReLU\nactivation. Carrying out (A.23) recursively, we see that\n\nz(h)(x, t), z(h)(x′, t′)\n\u000b\n→\nL−1\nY\nj=h\n˙Σj\u0000(x, t), (x′, t′)\n\u0001\n.\nFinally, we have:\nD∂f\n\u0000θ, (x, t)\n\u0001\n∂θ\n, ∂f\n\u0000θ, (x′, t′)\n\u0001\n∂θ\nE\n=\nL\nX\nh=1\nD∂f\n\u0000θ, (x, t)\n\u0001\n∂W(h)\n, ∂f\n\u0000θ, (x′, t′)\n\u0001\n∂W(h)\nE\n=\nL\nX\nh=1\n\u0010\nΣ(h)\nT\n\u0000(x, t), (x′, t′)\n\u0001\n·\nL\nY\nj=h\n˙Σj\u0000(x, t), (x′, t′)\n\u0001\u0011\n.\n(A.24)\nNotice that we use a more compact recursive formulation to state the results in Proposition 1. It is\neasy to verify that after expansion, we reach the desired results.\nCompared with the original NTK before composing with the temporal kernel (given by (A.4)), the\nresults in Proposition A.1 shares a similar recursion structure. As a consequence, the previous results\nfor NTK can be directly adopted to our setting. We list two examples here.\n• Following Jacot et al. (2018), given a training dataset {xi, ti, yi(ti)}n\ni=1, let fT\n\u0000θ(s)\n\u0001\n=\n\u0000f(θ(s), x1, t1), . . . , f(θ(s), xn, tn)\n\u0001\nbe the network outputs at the sth training step and\nyT =\n\u0000y1(t1), . . . , yn(tn)\n\u0001\n. The analysis on the optimization trajectory under inﬁnitesimal\nlearning rate can be conducted via:\ndfT\n\u0000θ(s)\n\u0001\nds\n= −ΘT (s) × (fT\n\u0000θ(s)\n\u0001\n−yT ),\nwhere ΘT (s) converges almost surely to the NTK Θ(L)\nT\nin Proposition A.1.\n• Following Allen-Zhu et al. (2019) and Arora et al. (2019b), the generalization performance\nof the composed time-aware neural network can be explicitly characterized according to the\nproperties of Θ(L)\nT .\n21\nPublished as a conference paper at ICLR 2021\nA.3.4\nPROOF FOR THEOREM 1\nProof. We ﬁrst present a technical lemma that is crucial for establishing the duality result under the\ndistributional constraint df(SθT ∥S) ≤δ. Recall that the hidden dimension for the kth layer is dk.\nLemma A.2 (Ben-Tal et al. (2013)). Let f be any closed convex function with domain [0, +∞), and\nthis conjugate is given by f ∗(s) = supt≥0{ts −f(t)}. Then for any distribution S and any function\ng : Rdk+1 →R, we have\nsup\nSθT :df (SθT ∥S)≤δ\nZ\ng(ω)dSθT (ω) = inf\nλ≥0,η\nn\nλ\nZ\nf ∗\u0010g(ω) −η\nλ\n\u0011\ndS(ω) + δλ + η\no\n.\n(A.25)\nWe work with a scaled version of the f-divergence under f(t) = 1\nk(tk −1) (because its dual function\nhas a cleaner form), where the constraint set is now equivalent to {SθT : df(SθT ∥S) ≤δ/k}. It is\neasy to check that f ∗(s) = 1\nk′ [s]k′\n+ + 1\nk with 1\nk′ + 1\nk = 1.\nSimilar to the proof for Proposition 1, we let z := [x, t] ∈Rd+1 and Z = X × T to simplify the\nnotations. To explicitly annotate the dependency of the random Fourier features on Ω, which is\nthe random variable corresponding to ω, we deﬁne ˜φ(z, Ω) such that ˜φ(z, Ω) =\n\u0002\ncos(z⊺Ω1) +\ncos(z⊺Ω2), sin(z⊺Ω1) + sin(z⊺Ω2)\n\u0003\n, where Ω= [Ω1, Ω2]. Then the approximation error, when\nreplacing the sampled Fourier features φ by the original random variable ˜φ(z, Ω), is given by:\n∆n(Ω) :=\n1\nn(n −1)\nX\ni̸=j\nΣ(k)(xi, xj)˜φ(zi, Ω)⊺˜φ(zj, Ω)\n−E\n\u0002\nΣ(k)(Xi, Xj)KT,SθT\n\u0000(Xi, Ti), (Xj, Tj)\n\u0001\u0003\n=\n1\nn(n −1)\nX\ni̸=j\nΣ(k)(xi, xj)˜φ(zi, Ω)⊺˜φ(zj, Ω) −E\n\u0002\nΣ(k)(X, X′)˜φ(Z, Ω)⊺˜φ(Z′, Ω)\n\u0003\n.\n(A.26)\nWe ﬁrst show that sub-Gaussianity of ∆n(Ω) . Let {x′\ni}n\ni=1 be an i.i.d copy of the observations\nexcept for one element j such that xj ̸= x′\nj. Without loss of generality, we assume the last element\nis different, i.e. xn ̸= x′\nn. Let ∆′\nn(Ω) be computed by replacing x and z with the above x′ and its\ncorresponding z′. Note that\n|∆n(Ω) −∆′\nn(Ω)|\n=\n1\nn(n −1)\n\f\f X\ni̸=j\nΣ(k)(xi, xj)˜φ(zi, Ω)⊺˜φ(zj, Ω) −Σ(k)(x′\ni, x′\nj)˜φ(z′\ni, Ω)⊺˜φ(z′\nj, Ω)\n\f\f\n≤\n1\nn(n −1)\n\u0010 X\ni<n\n\f\fΣ(k)(xi, xn)˜φ(zi, Ω)⊺˜φ(zn, Ω) −Σ(k)(xi, x′\nn)˜φ(zi, Ω)⊺˜φ(z′\nn, Ω)\n\f\f\n+\nX\nj<n\n\f\fΣ(k)(xn, xj)˜φ(zn, Ω)⊺˜φ(zj, Ω) −Σ(k)(x′\nn, xj)˜φ(z′\nn, Ω)⊺˜φ(zj, Ω)\n\f\f\n\u0011\n≤4 max{1, M}\nn\n,\n(A.27)\nwhere the last inequality comes from the fact that the random Fourier features ˜φ are bounded by 1\nand the inﬁnity norm of Σ(k) is bounded by M. The above bounded difference property suggests\nthat ∆n(Ω) is a 4 max{1,M}\nn\n-sub-Gaussian random variable.\n22\nPublished as a conference paper at ICLR 2021\nTo bound ∆n(Ω), we use:\nsup\nSθT :df (SθT ||S)\n\f\f\f\nZ\n∆n(Ω)dSθT\n\f\f\f ≤\nsup\nSθT :df (SθT ||S)\nZ\n|∆n(Ω)|dSθT\n≤inf\nλ≥0\nnλ1−k′\nk′\nES\n\u0002\n|∆n(Ω)|k′\u0003\n+ λ(δ + 1)\nk\no\n(using Lemma 2)\n= (δ + 1)1/kES\n\u0002\n|∆n(Ω)|k′\u00031/k′\n(solving for λ∗from above)\n=\n√\nδ + 1ES\n\u0002\n|∆n(Ω)|2\u00031/2\n(let k = k′ = 1/2).\n(A.28)\nTherefore, to bound supSθT :df (SθT ||S)\n\f\f\f\nR\n∆n(Ω)dSθT\n\f\f\f we simply need to bound\n\u0002\n|∆n(Ω)|2\u0003\n. Us-\ning the classical results for sub-Gaussian random variables (Boucheron et al., 2013), for λ ≤n/8, we\nhave\nE\n\u0002\nexp\n\u0000λ∆n(Ω)\n\u00012\u0003\n≤exp\n\u0000−1\n2 log(1 −8 max{1, M}λ/n)\n\u0001\n.\nThen we take the integral over ω\np\n\u0010 Z\n∆n(ω)2dS(ω) ≥\nϵ2\nδ + 1\n\u0011\n≤E\nh\nexp\n\u0010\nλ\nZ\n∆n(ω)2dS(ω)\n\u0011i\nexp\n\u0010\n−λϵ2\nδ + 1\n\u0011\n(Chernoff bound)\n≤exp\n\u0010\n−1\n2 log\n\u00001 −8 max{1, M}λ\nn\n\u0001\n−λϵ2\nδ + 1\n\u0011\n(apply Jensen’s inequality).\n(A.29)\nFinally, let the true approximation error be ˆ∆n(ω) = ˆΣ(k)(SθT ) −Σ(k)(SθT ). Notice that\n\f\f ˆ∆n(ω)\n\f\f ≤\n\f\f∆n(Ω)\n\f\f +\n1\nn(n −1)\nX\ni̸=j\nΣ(k)(xi, xj)\n\f\f˜φ(zi, Ω)⊺˜φ(zj, Ω) −φ(zi)⊺φ(zj)\n\f\f.\nFrom (A.28) and (A.29), we are able to bound supSθT :df (SθT ||S) ∆n(Ω).\nFor the second\nterm, recall from Proposition 1 that we have shown the stochastic uniform convergence bound\nfor\n\f\f˜φ(zi, Ω)⊺˜φ(zj, Ω) −φ(zi)⊺φ(zj)\n\f\f under any distributions SθT .\nThe desired bound for\np\n\u0010\nsupSθT :df (SθT ||S)\n\f\f ˆ∆n(ω)\n\f\f ≥ϵ\n\u0011\nis obtained after combining all the above results.\nA.3.5\nREPARAMETRIZATION WITH INVERTIBLE NEURAL NETWORK\nIn this part, we discuss the idea of constructing and sampling from an arbitrarily complex distribution\nfrom a known auxiliary distribution by a sequence of invertible transformations. Given an auxiliary\nrandom variable z following some know distribution q(z), suppose another random variable x is\nconstructed via a one-to-one mapping from z: x = f(z), then the density function of x is given by:\np(x) = q(z)\n\f\f\f dz\ndx\n\f\f\f = q\n\u0000f\n−1(x)\n\u0001\f\f\fdf\n−1\ndx\n\f\f\f.\n(A.30)\nWe can parameterize the one-to-one function f(.) with free parameters θ and optimize them over the\nobserved evidence such as by maximizing the log-likelihood. By stacking a sequence of Q one-to-one\nmappings, i.e. x = fQ ◦fQ−1 ◦. . . f1(z), we can construct complicated density functions. It is easy\nto show by chaining that p(x) is given by:\nlog p(x) = log q(z) −\nQ\nX\ni=1\n\f\f\fdf\n−1\ndzi\n\f\f\f.\n(A.31)\nSamples from the auxiliary distribution can be transformed to the unknown target distribution in\nthe same manner, and the transformed samples are essentially parameterized by the transformation\nmappings.\n23\nPublished as a conference paper at ICLR 2021\nUnfortunately, most standard neural architectures are non-invertible, so we settle on a speciﬁc family\nof neural networks - the invertible neural network (INN) Ardizzone et al. (2018). A major component\nof INN is the afﬁne coupling layer. With z sampled from the auxiliary distribution, we ﬁrst divide z\ninto two halves [z1, z2] and then let:\nv1 = z1 ⊙exp\n\u0000s1(γ, z2)\n\u0001\n+ t1(γ, z2)\nv2 = z2 ⊙exp\n\u0000s2(γ, z1)\n\u0001\n+ t2(γ, z1),\n(A.32)\nwhere s1(γ, ·)\n\u0001\n, s1(γ, ·)\n\u0001\n, t1(γ, ·)\n\u0001\n, t1(γ, ·)\n\u0001\ncan be any function parameterized by different parts of\nγ. Here, ⊙denotes the element-wise product. Then the outcome is simply given by:\ng(γ, z) = [v1, v2].\nTo see that g(γ, ·) is invertible so the inverse transform mappings are tractable, it is straightforward\nto show that g\n−1(γ, [v1, v2]) is given by:\nz2 =\n\u0000v2 −t1(γ, v1)\n\u0001\n⊙exp\n\u0000−s1(γ, v1)\n\u0001\nz1 =\n\u0000v1 −t2(γ, v2)\n\u0001\n⊙exp\n\u0000−s2(γ, v2)\n\u0001\n.\n(A.33)\nBy stacking multiple afﬁne coupling layers, scalar multiplication and summation actions (which\nare all invertible), we are able to construct an INN with enough complexity to characterize any\nnon-degenerating distribution.\nB\nSUPPLEMENTARY MATERIAL FOR SECTION 5\nWe provide the detailed dataset description, experiment setup, model conﬁguration, parameter tuning,\ntraining procedure, validation, testing, sensitivity analysis and model analysis. The reported results\nare averaged over ﬁve iterations.\nB.1\nDATASETS\n• Jena weather dataset1. The dataset contains 14 different features such as air temperature,\natmospheric pressure, humidity, and other metrics that reﬂect certain aspect of the weather.\nThe data were collected between between 2009 and 2016 for every 10 minutes, so there are\n6 observations in each hour.\nA standard task on this dataset is to use 5 days of observations to predict the temperature 12\nhours in the future, which we refer to as the Case 1. We use a sliding window to obtain the\ntraining, validation and testing samples and make sure they have no overlaps (right panel of\nFigure A.2).\n• Wikipedia trafﬁc.2 The Wiki web page trafﬁc records the daily number of visits for 550\nWikipedia pages from 2015-07-01 to 2016-12-31. The features are decoded from the\nwebpage url, where we are able to obtain the project name, e.g. zh.wikipedia.org, the access,\ne.g. all-access, and the agent, e.g. spider. We use one-hot encoding to represent the features,\nand end up with a 14-dimension feature vector for each webpage.\nThe feature vectors do not change with time. We use the feature vectors and trafﬁc data from\nthe past 200 days to predict the trafﬁc of the next 14 days, which is also a standard task for\nthis dataset. The missing data are treated as zero.\n• Alibaba online shopping data.3. The dataset contains 4,136 online shopping sequences\nwith a total of 1,552 items. Each shopping sequence has a varying number of time-stamped\nuser-item interactions, from 11 to 157. We consider the standard next-item recommendation\ntask, where we make a recommendation based on the past interactions. No user or item\nfeatures are available.\n• Walmart.com e-commerce data.4\nThe session-based online shopping data contains\n∼12,000 shopping sessions made by 1,000 frequent users, with a total of 2,034 items.\n1https://www.bgc-jena.mpg.de/wetter/\n2https://www.kaggle.com/c/web-trafﬁc-time-series-forecasting/data\n3https://github.com/musically-ut/tf_rmtpp/tree/master/data/real/ali\n4https://github.com/StatsDLMathsRecomSys/Inductive-representation-learning-on-temporal-graphs\n24\nPublished as a conference paper at ICLR 2021\nThe lengths of the sessions vary from 14 to 87. In order to be consistent with the Alibaba\nonline shopping data, we do not use the provided item and user features. We also consider\nthe next-item recommendation task.\nB.2\nPREPROCESSING, TRAIN-VALIDATION-TEST SPLIT AND METRIC\nTo ensure fair comparisons across the various models originated from different setting, we minimize\nthe data-speciﬁc preprocessing steps, especially for the time series dataset.\nFigure A.2: Left: the walk-forward split. Notice that we do not to specify the train-validation\nproportions when using the walk-forward split. Right: side-by-side split with moving window.\n• Jena weather dataset. We do the train-validation-test split by 60%-20%-20% on the time\naxis (right panel of Figure A.2). We ﬁrst standardize the features on the training data, and\nthen use the mean and standard deviation computed on the training data to standardize the\nvalidation and testing data, so there is no information leak.\nFor Case 1, we use the observations made at each hour (one every six observations) in the\nmost recent 120 hours (5 days) to predict the temperature 12 hours in the future.\nFor Case 2, we randomly sample 120 observations from the most recent 120 hours (with a\ntotal of 720 observations), to predict the temperature 12 hours in the future.\nFor Case 3, we randomly sample 120 observations from the most recent 120 hours (with a\ntotal of 720 observations), to predict the temperature randomly sampled from 4 to 12 hours\n(with a total of 48 observations) in the future.\n• Wikipedia trafﬁc. We use the walk-forward split (illustrated in the left panel of Figure\nA.2) to test the performance of the proposed approaches under different train-validation-test\nsplit schema. The walk-forward split is helpful when the length of training time series is\nrelatively long compared with the full time series. For instance, on the Wiki trafﬁc data, the\nfull sequence length is 500, and the training sequence length is 200, so it is impossible to\nconduct the side-by-side split. The features are all one-hot encoding, so we do not carry out\nany preprocessing. The web trafﬁcs are standardized in the same fashion as the Jena weather\ndata.\nFor Case 1, we use the most recent 200 observations to predict the full web trafﬁcs for 14\ndays in the future.\nFor Case 2, we randomly sample 200 observations from the past 500 days (with a total of\n500 observations), to predict the full web trafﬁcs for 14 days in the future.\nFor Case 3, we randomly sample 200 observations from the past 500 days (with a total of\n500 observations), to predict 6 web trafﬁcs sampled from 0 to 14 days in the future.\n• Alibaba data. We conduct the standard preprocessing steps used in the recommender\nsystem literature. We ﬁrst ﬁlter out items that have less than 5 total occurrences, and then\nﬁlter out shopping sequences that has less then 10 interactions. Using the standard train-\nvalidation-test split in sequential recommendation, for each shopping sequence, we use all\nbut the last two records as training data, the second-to-last record as validation data, and the\nlast record as testing data.\n25\nPublished as a conference paper at ICLR 2021\nAll the training/validation/testing samples obtained from the real shopping sequences are\ntreated as the positive record. For each positive record\n\b\nx(t −k), . . . , x(t), x(t + 1)\n\t\n, we\nrandomly sample 100 items {x′\ni(t + 1)}100\ni=1, and treat each (x(t −k), . . . , x(t), x′\ni(t + 1)′)\nas a negative sample, which is again a standard implementation in the recommender systems\nwhere no negative labels are available.\n• Walmart.com data. We use the same preprocessing steps and train-validation-test split as\nthe Alibaba data.\nAs for the metrics, we use the standard Mean absolute error (MAE) for the time-series prediction\ntasks. For the item recommendation tasks, we use the information retrieval metrics accuracy and\ndiscounted cumulative gain (DCG). Recall that for each shopping sequence, there is one positive\nsample and 100 negative samples. We rank the candidates\n\b\nx(t + 1), x′\n1(t + 1), . . . , x′\n100(t + 1)\n\t\naccording to the model’s output score on each item. The accuracy checks whether the candidate with\nthe highest score is the positive x(t + 1), and the DCG is given by 1/ log\n\u0000rank\n\u0000x(t + 1)\n\u0001\u0001\nwhere\nrank\n\u0000x(t + 1)\n\u0001\nis the position at which the positive x(t + 1) is ranked.\nB.3\nMODEL CONFIGURATION AND IMPLEMENTATION DETAIL\nWe observe from the empirical results that using kernel addition, i.e. Σ(h)\nT (x, t; x′, t′) = Σ(h)(x, x′)+\nKT (x, t; x′, t′), and kernel multiplication give similar results in our experiments, but kernel addition\nis much faster in both training and inference. Therefore, we choose to use kernel addition as a trick\nto expedite the computation. Note that kernel addition corresponds to simply adding up the hidden\nrepresentations from neural network, and the random features from the temporal kernel.\nWe ﬁrst show the conﬁguration and implementation for the models we use in time-series prediction.\nAll the models take the same inputs for each experiment with the hyperparamters tuned on the\nvalidation data. Note that the VAR model do not have randomness in model initialization and training,\nso their outputs do not have standard deviations.\nFor the NN+time, NN+trigo and T-NN models, the temporal structures (time feature) are added to\nthe same part of the neural architectures (illustrated in Figure A.1), and are made to have the same\ndimension 32 (except for NN+time). We will conduct sensitivity analysis later on the dimension.\nFor the proposed T-NN models, we treat the number of INN (afﬁne coupling) blocks used in building\nthe spectral distribution as tuning parameters. All the remaining model conﬁgurations are the same\nacross all model variants. We do not experiment on using regularizations, dropouts or residual\nconnections. In terms of reparameterization, we draw samples from the auxiliary distribution once at\nthe beginning, so we do not have to resample during the training process.\n• VAR. In the vector autoregression model, each variable is modeled as a linear combination\nof past values of itself and the past values of other variables in the system. The order, i.e.\nthe number of past values used, is treated as the hyperparameter, which we select according\naccording to the AIC criteria on the validation data.\nFor\nexperiments\non\nthe\nJena\nweather\ndataset,\nwe\nchoose\nthe\norder\nfrom\n{20, 40, 60, 80, 100, 120}, since the maximum length of the history used for prediction\nis 120 for all three cases. Similarly, for experiments on the Wiki trafﬁc dataset, we choose\nthe order from {40, 60, 80, . . . , 200}.\n• RNN models. We use the one-layer RNN model the standard RNN cells. The hidden\ndimension for the RNN models is selected to be 32 after tuning on the orignal model. To\nmake time-series prediction, the output of the ﬁnal state is then passed to a two-layer fully-\nconnected multi-layer perceptron (MLP) with ReLU as the activation function. We adjust\nthe hidden dimensions of the MLP for each model variant such that they have approximately\nthe same number of parameters.\n• CausalCNN models. We adopt the CausalCNN (Wavenet) architecture from Oord et al.\n(2016). The original CausalCNN treats the number of ﬁlters, ﬁlter width, dilation rates and\nnumber of convolutional layers are hyperparameters. Here, to avoid the extensive parameter\ntuning for each model variant, we tune the above parameter of the plain CausalCNN model\nand adpot them to the other model variants. Speciﬁcally, we ﬁnd that using 16 ﬁlters where\neach ﬁlter has a width of 2, with 5 convolutional layers and the dilation rates given by\n26\nPublished as a conference paper at ICLR 2021\n{2l}5\nl=1, gives the best result for all three cases. Similar to the RNN models, we then pass\nthe output to a two-layer MLP to obtain the prediction.\n• Attention models. We use a single attention block in the self-attention architecture Vaswani\net al. (2017). Unlike the ordinary attention in sequence-to-sequence learning, here we\nneed to employ an extra causal mask (shown in Figure 1) to make sure that the model is\nnot using the future information to predict the past. Other than that, we adopot the same\nkey-query-value attention setting, with their dimension as the tuning parameter. We ﬁnd\nthat dimension=16 gives the best result in all cases for the original model and we adopt this\nsetting to the rest model variants. Also, we pass the output to a two-layer MLP to obtain the\nprediction.\nWe now discuss the conﬁgurations and implementations for the recommendation models. We ﬁnd out\nthat the two-tower architecture illustrated in Figure A.3 is widely used for sequential recommendation\n(Hidasi et al., 2015; Li et al., 2017a). Each item is ﬁrst passed through an embedding layer, and\nthe history is processed by some sequence processing model, while the target is transformed by\nsome feed-forward neural network (FFN) such as the MLP. The outputs from the two towers are\nthen combined together, often by concatenating, and then pass to the top-layer FFN to obtain a\nprediction score. Hence, we adopt this complex neural architecture to examine our approaches, with\nthe sequence processing model in Figure A.3 replaced by their time-aware counterparts given in\nFigure A.1.\nFigure A.3: A standard two-tower neural architecture for sequential recommendation. To incorporate\nthe continuous-time information, we adopt the model architectures in A.1 to replace the sequence\nprocessing model here.\nTo be consistent across the Alibaba and Walmart.com dataset, we use the same model conﬁgurations.\nSpeciﬁcally, the dimension of the embedding layer is chosen to be 100, the FNN on the sequence\nlearning layer is a single-layer MLP with ReLU as activation, and the prediction layer FNN a two-\nlayer MLP with ReLU as activation. We keep the modules to be as simple as possible to avoid\novercomplications in model selection. For the T-NN models, we treat the dimension of the random\nFourier features φ, and the number of INN (afﬁne coupling) blocks used in building the spectral\ndistribution as tuning parameters. We also do not experiment on using regularizations, dropouts or\nresidual connections.\n• GRU-Rec. We use a single layer RNN with GRU cells as the sequence processing model.\nWe treat the hidden dimension of GRU as a tuning parameter, and according to the validation\nperformance evaluated by the accuracy, dimension=32 gives the best outcome for the plain\nGRUforRec, and we adopt it for all the RNN variants.\n• CNN-Rec. Similar to the experiments for time-series prediction, we treat the number of\nconvolutional layers, dilation rates, number of ﬁlters and ﬁlter width as tuning parameters.\nWe select the best settings for the plain CNNforRec and adopt them to all the model variants.\n• ATTN-Rec. We also use the single-head single-block self-attention model as the sequence\nprocessing model. We treat the dimension of the key, query, value matrices (which are of\nthe same dimension) as the turning parameter. It turns out that dimension=20 gives the best\nvalidation performance for ATTNforRec, which we adopt to all the model variants.\n27\nPublished as a conference paper at ICLR 2021\nFigure A.4: The training and inference speed comparisons for standard neural architectures (RNN,\nCausalCNN, self-attention) equipped with the proposed temporal kernel approach, and concatenating\nthe time to feature vector.\nUnlike the time-series data where the all the samples have a equal sequence length, the shopping\nsequences have various lengths. Therefore, we set the maximum length to be 100 and use the masking\nlayer to take account of the missing entries.\nFinally, we mention the implementation to handle the different scales and formats of the timestamps.\nThe scale of the timespan between events can be very different across datasets, and the absolute\nvalue of the timestamps are often less informative, as they could be the linux epoch time or the\ncalendar date. Hence, for each sequence given by\n\b\n(x1, t1), . . . , (xk, tk), (xk+1, tk+1)\n\t\nwhere the\ntarget is to predict (xk+1, tk+1), we convert it to the representation under timespans:\n\b\n((x1, tk+1 −\nt1)), . . . , (xk, tk+1 −tk), (xk+1, 0)\n\t\n. We then transform the scale of the timespans to match with\nthe problem setting, for instance, in the online shopping data the timespan is measured by the minute,\nand in the weather prediction it is measured by the hour.\nB.4\nCOMPUTATION\nThe VAR is implemented using the Python module statsmodels5. The deep learning models are\nimplemented using Tensorﬂow 2.0 on a single Nvidia V100 GPU. We use the Adam optimizer and\nadopt the early-stopping training schema where the trainning is stopped if the validation metric stops\nimproving for 5 epochs. The loss function is mean absolute error for the time series prediction,\nand binary cross-entropy loss with the softmax function for the recommendation tasks. The ﬁnal\nmetrics are computed on the hold-out test data using model checkpoints saved during training that\nhas the best validation performance.\nWe brieﬂy discuss the computation efﬁciency of our approach. From Figure A.4 we see that the extra\ncomputation time for the proposed temporal kernel approach is almost negligible compared with\nconcatenating time to the feature vector. The advantage is partly due to the fact that we draw samples\nfrom the auxiliary distribution only once during training. Note that this does not interfere with our\ntheoretical results, and greatly speeds up the computation.\nB.5\nVISUAL RESULTS\nWe visualize the predictions of our approaches on the Jena weather data in Figure A.5. We see that\nT-RNN, T-CNN and T-ATTN all capture the temporal signal well. In general, T-RNN gives slightly\nbetter predictions on the Jena weather dataset.\n5https://www.statsmodels.org/dev/vector_ar.html\n28\nPublished as a conference paper at ICLR 2021\nFigure A.5: The predictions of the T-RNN, T-CNN and T-ATTN approaches. We use the models\ntrained for Case 1 that predict the temperature 12 hours in the future. The plot is made by using\nsliding windows. The timestamp reﬂects the hours.\nFigure A.6: The ablation study on parameterizing the spectral density with Gaussian distribution, or\nwith INN.\nB.6\nEXTRA ABLATION STUDY\nWe show the effectiveness of using INN to characterize the temporal kernel, compared with using\na known distribution family. We focus on the Gaussian distribution for illustration, where both the\nmean and (diagonal of) the covariance are treated as free parameters. In short, we now parameterize\nthe spectral distribution as Gaussian instead of using INN. We denote this approach by NN-RF, to\ndifferentiate with the temporal kernel approach T-NN.\nFirst, we compare the results between T-NN and NN-RF on the time-series prediction tasks (shown in\nFigure A.6). We see that T-NN uniformly outperforms NN-RF. The results for the recommendation\ntask is similar, as we show in Table A.3, where T-NN still achieves the best performance. The\nnumerical results are consistent with Theorem 1 where a more complex distribution family can lead\nto better outcome. It also justiﬁes our usage of the INN to parameterize the spectral distribution.\nB.7\nSENSITIVITY ANALYSIS\nWe conduct sensitivity analysis to reveal the stability of the proposed approaches with respect to\nthe model selections and our proposed learning schema. Speciﬁcally, we focus on the dimension\nof the random features φ and the number of INN (afﬁne coupling) blocks when parametrizing the\nspectral distribution. The results for the time-series prediction tasks are given in Figure A.7, A.8\n29\nPublished as a conference paper at ICLR 2021\nTable A.3: The ablation study on parameterizing the spectral density with Gaussian distribution, or\nwith INN, when composing the recommendation models with temporal kernel. The reported are\nthe accuracy and discounted cumulative gain (DCG) for the domain-speciﬁc models on temporal\nrecommendation tasks.\nAlibaba\nWalmart\nMetric\nAccuracy\nDCG\nAccuracy\nDCG\nGRU-Rec-RF\n78.05/.22\n47.30/.13\n19.96/.15\n4.82/.21\nT-GRU-Rec\n79.47/.35\n49.82/.40\n23.41/.11\n8.44/.21\nCNN-Rec-RF\n75.23/.35\n44.60/.26\n16.33/.20\n2.74/.19\nT-CNN-Rec\n76.45/.16\n46.55/.38\n18.59/.33\n4.56/.31\nATTN-Rec-RF\n52.30/.47\n31.51/.55\n22.73/.41\n7.95/.23\nT-ATTN-Rec\n53.49/.31\n33.58/.30\n25.51/.17\n9.22/.15\nand A.11. The sensitivity analysis results for the recommendation task on Alibaba and Walmart.com\ndatasets are provided in Figure A.9 and A.10. From Table A.3 and A.2, we see that the ATTN-Rec\nmodels do not perform well on the Aliababa dataset, and the CNN-Rec models do not perform well\non the Walmart.com dataset. Therefore, we do not provide sensitivty analysis for those models on the\ncorresponding dataset.\nIn general, the pattern is clear with respect to the dimension of φ, that the larger the dimension\n(within the range we consider), the better the performance. The result is also reasonable, since a\nlarger dimension for φ may express more temporal information and better characterize the temporal\nkernel On the other hand, the pattern for the number of INN (afﬁne coupling layers) blocks is less\nuniform, where in some cases too few INN blocks suffer from under-ﬁtting, while in some other\ncases too may INN blocks lead to over-ﬁtting. Therefore, we would recommend using the number of\nINN blocks as a hyperparameter, and keep the dimension of φ reasonably large.\n30\nPublished as a conference paper at ICLR 2021\nFigure A.7: Sensitivity analysis on the dimension of φγ for the Jena weather dataset. From top to\nbottom are the results for Case 1, Case 2 and Case 3 respectively.\n31\nPublished as a conference paper at ICLR 2021\nFigure A.8: Sensitivity analysis on the dimension of φγ for the Wikipedia web trafﬁc dataset. From\ntop to bottom are the results for Case 1, Case 2 and Case 3 respectively.\nFigure A.9: Sensitivity analysis on the number of INN (afﬁne coupling) layers for the recommendation\ntask on Aliaba and Walmart.com datasets.\n32\nPublished as a conference paper at ICLR 2021\nFigure A.10: Sensitivity analysis on the dimension of φγ for the recommendation task on the Alibaba\ndataset upper panel and the Walmart.com dataset lower panel.\nFigure A.11: Sensitivity analysis on the number of INN (afﬁne coupling) layers. The upper panel\ngives the results on the Jena weather dataset, the lower panel gives the results on the Wikipedia web\ntrafﬁc dataset.\n33\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-03-28",
  "updated": "2021-03-28"
}