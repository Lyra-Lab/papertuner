{
  "id": "http://arxiv.org/abs/2103.16364v2",
  "title": "ICE: Inter-instance Contrastive Encoding for Unsupervised Person Re-identification",
  "authors": [
    "Hao Chen",
    "Benoit Lagadec",
    "Francois Bremond"
  ],
  "abstract": "Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances has not been fully explored in previous contrastive\nmethods, especially for instance-level contrastive loss. To address this issue,\nwe propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\nCode is made available at https://github.com/chenhao2345/ICE.",
  "text": "ICE: Inter-instance Contrastive Encoding for Unsupervised Person\nRe-identiﬁcation\nHao Chen1,2,3\nBenoit Lagadec3\nFrancois Bremond1,2\n1Inria\n2Universit´e Cˆote d’Azur\n3European Systems Integration\n{hao.chen, francois.bremond}@inria.fr\nbenoit.lagadec@esifrance.net\nAbstract\nUnsupervised person re-identiﬁcation (ReID) aims at\nlearning discriminative identity features without annota-\ntions.\nRecently, self-supervised contrastive learning has\ngained increasing attention for its effectiveness in unsu-\npervised representation learning.\nThe main idea of in-\nstance contrastive learning is to match a same instance\nin different augmented views.\nHowever, the relationship\nbetween different instances has not been fully explored in\nprevious contrastive methods, especially for instance-level\ncontrastive loss. To address this issue, we propose Inter-\ninstance Contrastive Encoding (ICE) that leverages inter-\ninstance pairwise similarity scores to boost previous class-\nlevel contrastive ReID methods. We ﬁrst use pairwise sim-\nilarity ranking as one-hot hard pseudo labels for hard in-\nstance contrast, which aims at reducing intra-class vari-\nance. Then, we use similarity scores as soft pseudo labels\nto enhance the consistency between augmented and orig-\ninal views, which makes our model more robust to aug-\nmentation perturbations.\nExperiments on several large-\nscale person ReID datasets validate the effectiveness of our\nproposed unsupervised method ICE, which is competitive\nwith even supervised methods. Code is made available at\nhttps://github.com/chenhao2345/ICE.\n1. Introduction\nPerson re-identiﬁcation (ReID) targets at retrieving an\nperson of interest across non-overlapping cameras by com-\nparing the similarity of appearance representations. Super-\nvised ReID methods [29, 2, 23] use human-annotated labels\nto build discriminative appearance representations which\nare robust to pose, camera property and view-point varia-\ntion. However, annotating cross-camera identity labels is\na cumbersome task, which makes supervised methods less\nscalable in real-world deployments. Unsupervised methods\n[21, 22, 33] directly train a model on unlabeled data and\nthus have a better scalability.\nMost of previous unsupervised ReID methods [28, 11,\n42] are based on unsupervised domain adaptation (UDA).\nUDA methods adjust a model from a labeled source domain\nto an unlabeled target domain. The source domain provides\na good starting point that facilitates target domain adapta-\ntion. With the help of a large-scale source dataset, state-of-\nthe-art UDA methods [11, 42] signiﬁcantly enhance the per-\nformance of unsupervised ReID. However, the performance\nof UDA methods is strongly inﬂuenced by source dataset’s\nscale and quality. Moreover, a large-scale labeled dataset is\nnot always available in the real world. In this case, fully un-\nsupervised methods [21, 22] own more ﬂexibility, as they do\nnot require any identity annotation and directly learn from\nunlabeled data in a target domain.\nRecently, contrastive learning has shown excellent per-\nformance in unsupervised representation learning. State-of-\nthe-art contrastive methods [39, 5, 14] consider each image\ninstance as a class and learns representations by matching\naugmented views of a same instance. As a class is usu-\nally composed of multiple positive instances, it hurts the\nperformance of ﬁne-grained ReID tasks when different im-\nages of a same identity are considered as different classes.\nSelf-paced Contrastive Learning (SpCL) [13] alleviates this\nproblem by matching an instance with the centroid of the\nmultiple positives, where each positive converges to its cen-\ntroid at a uniform pace. Although SpCL has achieved im-\npressive performance, this method does not consider inter-\ninstance afﬁnities, which can be leveraged to reduce intra-\nclass variance and make clusters more compact. In super-\nvised ReID, state-of-the-art methods [2, 23] usually adopt\na hard triplet loss [16] to lay more emphasis on hard sam-\nples inside a class, so that hard samples can get closer to\nnormal samples. In this paper, we introduce Inter-instance\nContrastive Encoding (ICE), in which we match an instance\nwith its hardest positive in a mini-batch to make clusters\nmore compact and improve pseudo label quality. Matching\nthe hardest positive refers to using one-hot “hard” pseudo\nlabels.\nSince no ground truth is available, mining hardest pos-\nitives within clusters is likely to introduce false positives\ninto the training process. In addition, the one-hot label does\nnot take the complex inter-instance relationship into consid-\neration when multiple pseudo positives and negatives exist\narXiv:2103.16364v2  [cs.CV]  18 Aug 2021\nin a mini-batch. Contrastive methods usually use data aug-\nmentation to mimic real-world distortions, e.g., occlusion,\nview-point and resolution variance. After data augmenta-\ntion operations, certain pseudo positives may become less\nsimilar to an anchor, while certain pseudo negatives may\nbecome more similar. As a robust model should be invari-\nant to distortions from data augmentation, we propose to use\nthe inter-instance pairwise similarity as “soft” pseudo labels\nto enhance the consistency before and after augmentation.\nOur proposed ICE incorporates class-level label (cen-\ntroid contrast), instance pairwise hard label (hardest posi-\ntive contrast) and instance pairwise soft label (augmenta-\ntion consistency) into one fully unsupervised person ReID\nframework. Without any identity annotation, ICE signiﬁ-\ncantly outperforms state-of-the-art UDA and fully unsuper-\nvised methods on main-stream person ReID datasets.\nTo summarize, our contributions are: (1) We propose to\nuse pairwise similarity ranking to mine hardest samples as\none-hot hard pseudo labels for hard instance contrast, which\nreduces intra-class variance. (2) We propose to use pairwise\nsimilarity scores as soft pseudo labels to enhance the con-\nsistency between augmented and original instances, which\nalleviates label noise and makes our model more robust\nto augmentation perturbation. (3) Extensive experiments\nhighlight the importance of inter-instance pairwise similar-\nity in contrastive learning. Our proposed method ICE out-\nperforms state-of-the-art methods by a considerable margin,\nsigniﬁcantly pushing unsupervised ReID to real-world de-\nployment.\n2. Related Work\nUnsupervised person ReID.\nRecent unsupervised per-\nson ReID methods can be roughly categorized into un-\nsupervised domain adaptation (UDA) and fully unsuper-\nvised methods. Among UDA-based methods, several works\n[34, 20] leverage semantic attributes to reduce the domain\ngap between source and target domains.\nSeveral works\n[38, 49, 8, 50, 52, 4] use generative networks to transfer\nlabeled source domain images into the style of target do-\nmain. Another possibility is to assign pseudo labels to unla-\nbeled images, where pseudo labels are obtained from clus-\ntering [28, 10, 43, 3] or reference data [40]. Pseudo la-\nbel noise can be reduced by selecting credible samples [1]\nor using a teacher network to assign soft labels [11]. All\nthese UDA-based methods require a labeled source dataset.\nFully unsupervised methods have a better ﬂexibility for de-\nployment. BUC [21] ﬁrst treats each image as a cluster\nand progressively merge clusters. Lin et al. [22] replace\nclustering-based pseudo labels with similarity-based soft-\nened labels. Hierarchical Clustering is proposed in [41] to\nimprove the quality of pseudo labels. Since each identity\nusually has multiple positive instances, MMCL [33] intro-\nduces a memory-based multi-label classiﬁcation loss into\nunsupervised ReID. JVTC [19] and CycAs [36] explore\ntemporal information to reﬁne visual similarity. SpCL [13]\nconsiders each cluster and outlier as a single class and then\nconduct instance-to-centroid contrastive learning. CAP [35]\ncalculates identity centroids for each camera and conducts\nintra- and inter-camera centroid contrastive learning. Both\nSpCL and CAP focus on instance-to-centroid contrast, but\nneglect inter-instance afﬁnities.\nContrastive Learning.\nRecent contrastive learning meth-\nods [39, 14, 5] consider unsupervised representation learn-\ning as a dictionary look-up problem. Wu et al. [39] retrieve\na target representation from a memory bank that stores rep-\nresentations of all the images in a dataset. MoCo [14] in-\ntroduces a momentum encoder and a queue-like memory\nbank to dynamically update negatives for contrastive learn-\ning. In SimCLR [5], authors directly retrieve representa-\ntions within a large batch. However, all these methods con-\nsider different instances of a same class as different classes,\nwhich is not suitable in a ﬁne-grained ReID task. These\nmethods learn invariance from augmented views, which can\nbe regarded as a form of consistency regularization.\nConsistency regularization.\nConsistency regularization\nrefers to an assumption that model predictions should be\nconsistent when fed perturbed versions of the same im-\nage, which is widely considered in recent semi-supervised\nlearning [30, 27, 6].\nThe perturbation can come from\ndata augmentation [27], temporal ensembling [30, 18, 12]\nand shallow-deep features [46, 6]. Artiﬁcial perturbations\nare applied in contrastive learning as strong augmentation\n[7, 37] and momentum encoder [14] to make a model ro-\nbust to data variance. Based on temporal ensembling, Ge et\nal. [12] use inter-instance similarity to mitigate pseudo la-\nbel noise between different training epochs for image local-\nization. Wei et al. [37] propose to regularize inter-instance\nconsistency between two sets of augmented views, which\nneglects intra-class variance problem. We simultaneously\nreduce intra-class variance and regularize consistency be-\ntween augmented and original views, which is more suit-\nable for ﬁne-grained ReID tasks.\n3. Proposed Method\n3.1. Overview\nGiven a person ReID dataset X = {x1, x2, ..., xN}, our\nobjective is to train a robust model on X without annota-\ntion. For inference, representations of a same person are\nsupposed to be as close as possible. State-of-the-art con-\ntrastive methods [14, 5] consider each image as an indi-\nvidual class and maximize similarities between augmented\nviews of a same instance with InfoNCE loss [31]:\nLInfoNCE = E[−log\nexp (q · k+/τ)\nPK\ni=0 exp (q · ki/τ)\n]\n(1)\nwhere q and k+ are two augmented views of a same instance\nin a set of candidates ki. τ is a temperature hyper-parameter\nFigure 1: General architecture of ICE. We maximize the similarity\nbetween anchor and pseudo positives in both inter-class (proxy\nagreement between an instance representation f1 and its cluster\nproxy p1) and intra-class (instance agreement between f1 and its\npseudo positive m2) manners.\nthat controls the scale of similarities.\nFollowing MoCo [14], we design our proposed ICE with\nan online encoder and a momentum encoder as shown in\nFig. 1.\nThe online encoder is a regular network, e.g.,\nResNet50 [15], which is updated by back-propagation. The\nmomentum encoder (weights noted as θm) has the same\nstructure as the online encoder, but updated by accumulated\nweights of the online encoder (weights noted as θo):\nθt\nm = αθt−1\nm\n+ (1 −α)θt\no\n(2)\nwhere α is a momentum coefﬁcient that controls the up-\ndate speed of the momentum encoder. t and t −1 refer re-\nspectively to the current and last iteration. The momentum\nencoder builds momentum representations with the moving\naveraged weights, which are more stable to label noise.\nAt the beginning of each training epoch, we use the\nmomentum encoder to extract appearance representations\nM = {m1, m2, ..., mN} of all the samples in the train-\ning set X. We use a clustering algorithm DBSCAN [9] on\nthese appearance representations to generate pseudo iden-\ntity labels Y = {y1, y2, ..., yN}. We only consider clustered\ninliers for contrastive learning, while un-clustered outliers\nare discarded. We calculate proxy centroids p1, p2, ... and\nstore them in a memory for a proxy contrastive loss Lproxy\n(see Sec. 3.2). Note that this proxy memory can be camera-\nagnostic [13] or camera-aware [35].\nThen, we use a random identity sampler to split the train-\ning set into mini-batches where each mini-batch contains\nNP pseudo identities and each identity has NK instances.\nWe train the whole network by combining the Lproxy (with\nclass-level labels), a hard instance contrastive loss Lh ins\n(with hard instance pairwise labels, see Sec. 3.3) and a soft\ninstance consistency loss Ls ins (with soft instance pairwise\nlabels, see Sec. 3.4):\nLtotal = Lproxy + λhLh ins + λsLs ins\n(3)\nTo increase the consistency before and after data aug-\nmentation, we use different augmentation settings for pre-\ndiction and target representations in the three losses (see\nTab. 1).\nLoss\nPredictions (augmentation)\nTargets (augmentation)\nLproxy\nf (Strong)\np (None)\nLh ins\nf (Strong)\nm (Strong)\nLs ins\nP (Strong)\nQ (None)\nTable 1: Augmentation settings for 3 losses.\n3.2. Proxy Centroid Contrastive Baseline\nFor a camera-agnostic memory,\nthe proxy of cluster a\nis deﬁned as the averaged momentum representations of all\nthe instances belonging to this cluster:\npa = 1\nNa\nX\nmi∈ya\nmi\n(4)\nwhere Na is the number of instances belonging to the clus-\nter a.\nWe apply a set of data augmentation on X and feed them\nto the online encoder. For an online representation fa be-\nlonging to the cluster a, the camera-agnostic proxy con-\ntrastive loss is a softmax log loss with one positive proxy\npa and all the negatives in the memory:\nLagnostic = E[−log\nexp (fa · pa/τa)\nP|p|\ni=1 exp (fa · pi/τa)\n]\n(5)\nwhere |p| is the number of clusters in a training epoch and\nτa is a temperature hyper-parameter. Different from uniﬁed\ncontrastive loss [11], outliers are not considered as single\ninstance clusters. In such way, outliers are not pushed away\nfrom clustered instances, which allows us to mine more\nhard samples for our proposed hard instance contrast. As\nshown in Fig. 2, all the clustered instances converge to a\ncommon cluster proxy centroid. However, images inside a\ncluster are prone to be affected by camera styles, leading to\nhigh intra-class variance. This problem can be alleviated by\nadding a cross-camera proxy contrastive loss [35].\nFor a camera-aware memory,\nif we have C\n=\n{c1, c2, ...} cameras, a camera proxy pab is deﬁned as the\naveraged momentum representations of all the instances be-\nlonging to the cluster a in camera cb:\npab =\n1\nNab\nX\nmi∈ya∩mi∈cb\nmi\n(6)\nwhere Nab is the number of instances belonging to the clus-\nter a captured by camera cb.\nFigure 2: Proxy contrastive loss. Inside a cluster, an instance is\npulled to a cluster centroid by Lagnostic and to cross-camera cen-\ntroids by Lcross.\nGiven an online representation fab, the cross-camera\nproxy contrastive loss is a softmax log loss with one positive\ncross-camera proxy pai and Nneg nearest negative proxies\nin the memory:\nLcross = E[−1\n|P|\nX\ni̸=b∩i∈C\nlog\nexp (< fab · pai > /τc)\nPNneg+1\nj=1\nexp (< fab · pj > /τc)\n]\n(7)\nwhere < · > denotes cosine similarity and τc is a cross-\ncamera temperature hyper-parameter. |P| is the number of\ncross-camera positive proxies. Thanks to this cross-camera\nproxy contrastive loss, instances from one camera are pulled\ncloser to proxies of other cameras, which reduces intra-class\ncamera style variance.\nWe deﬁne a proxy contrastive loss by combining cluster\nand camera proxies with a weighting coefﬁcient 0.5 from\n[35]:\nLproxy = Lagnostic + 0.5Lcross\n(8)\n3.3. Hard Instance Contrastive Loss\nAlthough intra-class variance can be alleviated by cross-\ncamera contrastive loss, it has two drawbacks: 1) more\nmemory space is needed to store camera-aware proxies, 2)\nimpossible to use when camera ids are unavailable.\nWe\npropose a camera-agnostic alternative by exploring inter-\ninstance relationship instead of using camera labels. Along\nwith training, the encoders become more and more strong,\nwhich helps outliers progressively enter clusters and be-\ncome hard inliers. Pulling hard inliers closer to normal in-\nliers effectively increases the compactness of clusters.\nA mini-batch is composed of NP identities, where each\nidentity has NK positive instances. Given an anchor in-\nstance f i belonging to the ith class, we sample the hardest\npositive momentum representation mi\nk that has the lowest\ncosine similarity with f i, see Fig. 4. For the same anchor,\nwe have J = (NP −1) × NK negative instances that do\nnot belong to the ith class. The hard instance contrastive\nloss for f i is a softmax log loss of J + 1 (1 positive and J\nFigure 3: Comparison between triplet and hard instance con-\ntrastive loss.\nnegative) pairs, which is deﬁned as:\nLh ins = E[−log\nexp (< f i · mi\nk > /τh ins)\nPJ+1\nj=1 exp (< f i · mj > /τh ins)\n] (9)\nwhere k = arg mink=1,..,NK(< f i · mi\nk >) and τh ins is\nthe hard instance temperature hyper-parameter. By mini-\nmizing the distance between the anchor and the hardest pos-\nitive and maximizing the distance between the anchor and\nall negatives, Lh ins increases intra-class compactness and\ninter-class separability.\nRelation with triplet loss.\nBoth Lh ins and triplet loss\n[16] pull an anchor closer to positive instances and away\nfrom negative instances. As shown in Fig. 3, the traditional\ntriplet loss pushes away a negative pair from a positive pair\nby a margin. Differently, the proposed Lh ins pushes away\nall the negative instances as far as it could with a softmax.\nIf we select one negative instance, the Lh ins can be trans-\nformed into the triplet loss. If we calculate pairwise dis-\ntance within a mini-batch to select the hardest positive and\nthe hardest negative instances, the Lh ins is equivalent to\nthe batch-hard triplet loss[16]. We compare hard triplet loss\n(hardest negative) with the proposed Lh ins (all negatives).\nin Tab. 2.\nNegative in Lh ins\nMarket1501\nDukeMTMC-reID\nmAP\nRank1\nmAP\nRank1\nhardest\n80.1\n92.8\n68.2\n82.5\nall\n82.3\n93.8\n69.9\n83.3\nTable 2: Comparison between using the hardest negative and all\nnegatives in the denominator of Lh ins.\n3.4. Soft Instance Consistency Loss\nBoth proxy and hard instance contrastive losses are\ntrained with one-hot hard pseudo labels, which can not cap-\nture the complex inter-instance similarity relationship be-\ntween multiple pseudo positives and negatives. Especially,\ninter-instance similarity may change after data augmenta-\ntion. As shown in Fig. 4, the anchor A becomes less sim-\nilar to pseudo positives (P1, P2, P3), because of the visual\ndistortions. Meanwhile, the anchor A becomes more sim-\nilar to pseudo negatives (N1, N2), since both of them have\nred shirts. By maintaining the consistency before and after\nFigure 4: Based on inter-instance similarity ranking between anchor (A), pseudo positives (P) and pseudo negatives (N), Hard Instance\nContrastive Loss matches an anchor with its hardest positive in a mini-batch. Soft Instance Consistency Loss regularizes the inter-\ninstance similarity before and after data augmentation.\naugmentation, a model is supposed to be more invariant to\naugmentation perturbations. We use the inter-instance sim-\nilarity scores without augmentation as soft labels to rectify\nthose with augmentation.\nFor a batch of images after data augmentation, we mea-\nsure the inter-instance similarity between an anchor fA with\nall the mini-batch NK × NP instances, as shown in Fig. 4.\nThen, the inter-instance similarity is turned into a prediction\ndistribution P by a softmax:\nP =\nexp (< fA · m > /τs ins)\nPNP ×NK\nj=1\nexp (< fA · mj > /τs ins)\n(10)\nwhere τs ins is the soft instance temperature hyper-\nparameter. fA is an online representation of the anchor,\nwhile m is momentum representation of each instance in\na mini-batch.\nFor the same batch without data augmentation, we mea-\nsure the inter-instance similarity between momentum rep-\nresentations of the same anchor with all the mini-batch\nNK × NP instances, because the momentum encoder is\nmore stable. We get a target distribution Q:\nQ =\nexp (< mA · m > /τs ins)\nPNP ×NK\nj=1\nexp (< mA · mj > /τs ins)\n(11)\nThe soft instance consistency loss is Kullback-Leibler\nDivergence between two distributions:\nLs ins = DKL(P||Q)\n(12)\nIn previous methods, consistency is regularized between\nweakly augmented and strongly augmented images [27] or\ntwo sets of differently strong augmented images [37]. Some\nmethods [18, 30] also adopted mean square error (MSE)\nas their consistency loss function. We compare our setting\nwith other possible settings in Tab. 3.\nConsistency\nMarket1501\nDukeMTMC-reID\nmAP\nRank1\nmAP\nRank1\nMSE\n80.0\n92.7\n68.4\n82.1\nStrong-strong Aug\n80.4\n92.8\n68.2\n82.5\nours\n82.3\n93.8\n69.9\n83.3\nTable 3: Comparison of consistency loss. Ours refers to KL diver-\ngence between images with and without data augmentation.\n4. Experiments\n4.1. Datasets and Evaluation Protocols\nMarket-1501 [44], DukeMTMC-reID[25] and MSMT17\n[38] datasets are used to evaluate our proposed method.\nMarket-1501 dataset is collected in front of a supermarket\nin Tsinghua University from 6 cameras. It contains 12,936\nimages of 751 identities for training and 19,732 images of\n750 identities for test. DukeMTMC-reID is a subset of the\nDukeMTMC dataset. It contains 16,522 images of 702 per-\nsons for training, 2,228 query images and 17,661 gallery\nimages of 702 persons for test from 8 cameras. MSMT17\nis a large-scale Re-ID dataset, which contains 32,621 train-\ning images of 1,041 identities and 93,820 testing images\nof 3,060 identities collected from 15 cameras. Both Cu-\nmulative Matching Characteristics (CMC) Rank1, Rank5,\nRank10 accuracies and mean Average Precision (mAP) are\nused in our experiments.\n4.2. Implementation details\nGeneral training settings.\nTo conduct a fair comparison\nwith state-of-the-art methods, we use an ImageNet [26] pre-\ntrained ResNet50 [15] as our backbone network. We report\nresults of IBN-ResNet50 [24] in Appendix B. An Adam op-\ntimizer with a weight decay rate of 0.0005 is used to opti-\nmize our networks. The learning rate is set to 0.00035 with\na warm-up scheme in the ﬁrst 10 epochs. No learning rate\ndecay is used in the training. The momentum encoder is up-\nFigure 5: Parameter analysis on Market-1501 dataset.\ndated with a momentum coefﬁcient α = 0.999. We renew\npseudo labels every 400 iterations and repeat this process\nfor 40 epochs. We use a batchsize of 32 where NP = 8 and\nNK = 4. We set τa = 0.5, τc = 0.07 and Nneg = 50 in\nthe proxy contrastive baseline. Our network is trained on\n4 Nvidia 1080 GPUs under Pytorch framework. The total\ntraining time is around 2 hours on Market-1501. After train-\ning, only the momentum encoder is used for the inference.\nClustering settings.\nWe calculate k-reciprocal Jaccard\ndistance [47] for clustering, where k is set to 30. We set\na minimum cluster samples to 4 and a distance threshold\nto 0.55 for DBSCAN. We also report results of a smaller\nthreshold 0.5 (more appropriate for the smaller dataset Mar-\nket1501) and a larger threshold 0.6 (more appropriate for\nthe larger dataset MSMT17) in Appendix C.\nData augmentation.\nAll images are resized to 256×128.\nThe strong data augmentation refers to random horizontal\nﬂipping, cropping, Gaussian blurring and erasing [48].\n4.3. Parameter analysis\nCompared to the proxy contrastive baseline, ICE brings\nin four more hyper-parameters, including λh ins, τh ins for\nhard instance contrastive loss and λs ins, τs ins for soft in-\nstance consistency loss. We analyze the sensitivity of each\nhyper-parameter on the Market-1501 dataset.\nThe mAP\nresults are illustrated in Fig. 5. As hardest positives are\nlikely to be false positives, an overlarge λh ins or under-\nsized τh ins introduce more noise. λh ins and λs ins bal-\nance the weight of each loss in Eq. (3).\nGiven the re-\nsults, we set λh ins = 1 and λs ins = 10.\nτh ins and\nτs ins control the similarity scale in hard instance con-\ntrastive loss and soft instance consistency loss. We ﬁnally\nset τh ins = 0.1 and τs ins = 0.4. Our hyper-parameters are\ntuned on Market-1501 and kept same for DukeMTMC-reID\nand MSMT17. Achieving state-of-the-art results simultane-\nously on the three datasets can validate the generalizability\nof these hyper-parameters.\n4.4. Ablation study\nThe performance boost of ICE in unsupervised ReID\nmainly comes from the proposed hard instance contrastive\nloss and soft instance consistency loss. We conduct ablation\nexperiments to validate the effectiveness of each loss, which\nis reported in Tab. 4. We illustrate the number of clusters\nFigure 6: Dynamic cluster numbers during 40 training epochs on\nDukeMTMC-reID. “hard” and “soft” respectively denote Lh ins\nand Ls ins. A lower number denotes that clusters are more com-\npact.\nFigure 7: Dynamic KL divergence during 40 training epochs on\nDukeMTMC-reID. Lower KL divergence denotes that a model is\nmore robust to augmentation perturbation.\nduring the training in Fig. 6 and t-SNE [32] after training\nin Fig. 8 to evaluate the compactness of clusters. We also\nillustrate the dynamic KL divergence of Eq. (12) to mea-\nsure representation sensitivity to augmentation perturbation\nin Fig. 7 .\nHard instance contrastive loss.\nOur proposed Lh ins re-\nduces the intra-class variance in a camera-agnostic manner,\nwhich increases the quality of pseudo labels. By reducing\nintra-class variance, a cluster is supposed to be more com-\npact. With a same clustering algorithm, we expect to have\nless clusters when clusters are more compact. As shown in\nFig. 6, DBSCAN generated more clusters during the train-\ning without our proposed Lh ins. The full ICE framework\nhas less clusters, which are closer to the real number of\nidentities in the training set. On the other hand, as shown in\nFig. 8, the full ICE framework has a better intra-class com-\npactness and inter-class separability than the camera-aware\nbaseline in the test set. The compactness contributes to bet-\nCamera-aware memory\nMarket1501\nDukeMTMC-reID\nMSMT17\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nBaseline Lproxy\n79.3\n91.5\n96.8\n97.6\n67.3\n81.4\n90.8\n92.9\n36.4\n67.8\n78.7\n82.5\n+Lh ins\n80.5\n92.6\n97.3\n98.4\n68.8\n82.4\n90.4\n93.6\n38.0\n69.1\n79.9\n83.4\n+Ls ins\n81.1\n93.2\n97.5\n98.5\n68.4\n82.0\n91.0\n93.2\n38.1\n68.7\n79.8\n83.7\n+Lh ins + Ls ins\n82.3\n93.8\n97.6\n98.4\n69.9\n83.3\n91.5\n94.1\n38.9\n70.2\n80.5\n84.4\nCamera-agnostic memory\nMarket1501\nDukeMTMC-reID\nMSMT17\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nBaseline Lagnostic\n65.8\n85.3\n95.1\n96.6\n50.9\n67.9\n81.6\n86.6\n24.1\n52.3\n66.2\n71.6\n+Lh ins\n78.2\n91.3\n96.9\n98.0\n65.4\n79.6\n88.9\n91.9\n30.3\n60.8\n72.9\n77.6\n+Ls ins\n47.2\n66.7\n86.0\n91.6\n36.2\n50.4\n70.3\n76.3\n17.8\n38.8\n54.2\n60.9\n+Lh ins + Ls ins\n79.5\n92.0\n97.0\n98.1\n67.2\n81.3\n90.1\n93.0\n29.8\n59.0\n71.7\n77.0\nTable 4: Comparison of different losses. Camera-aware memory occupies up to 6, 8 and 15 times memory space than camera-agnostic\nmemory on Market1501, DukeMTMC-reID and MSMT17 datasets.\nter unsupervised ReID performance in Tab. 4.\nSoft instance consistency loss.\nHard instance contrastive\nloss reduces the intra-class variance between naturally cap-\ntured views, while soft instance consistency loss mainly\nreduces the variance from artiﬁcially augmented perturba-\ntion. If we compare the blue (ICE full) and yellow (w/o\nsoft) curves in Fig. 7, we can ﬁnd that the model trained\nwithout Ls ins is less robust to augmentation perturbation.\nThe quantitative results in Tab. 4 conﬁrms that the Ls ins\nimproves the performance of baseline.\nThe best perfor-\nmance can be obtained by applying Lh ins and Ls ins on\nthe camera-aware baseline.\nCamera-agnostic scenario.\nAbove results are obtained\nwith a camera-aware memory, which strongly relies on\nground truth camera ids.\nWe further validate the effec-\ntiveness of the two proposed losses with a camera-agnostic\nmemory, whose results are also reported in Tab. 4. Our\nproposed Lh ins signiﬁcantly improves the performance\nfrom the camera-agnostic baseline. However, Ls ins should\nbe used under low intra-class variance, which can be\nachieved by the variance constraints on camera styles\nLcross and hard samples Lh ins.\nLh ins reduces intra-\nclass variance, so that AA ≈AP1 ≈AP2 ≈AP3 ≈1\nbefore augmentation in Fig. 4.\nLs ins permits that we\nstill have AA ≈AP1 ≈AP2 ≈AP3 ≈1 after aug-\nmentation.\nHowever, when strong variance exists, e.g.,\nAA ̸≈AP1 ̸≈AP2 ̸≈AP3 ̸≈1, maintaining this rela-\ntionship equals maintaining intra-class variance, which de-\ncreases the ReID performance. On medium datasets (e.g.,\nMarket1501 and DukeMTMC-reID) without strong cam-\nera variance, our proposed camera-agnostic intra-class vari-\nance constraint Lh ins is enough to make Ls ins beneﬁcial\nto ReID. On large datasets (e.g., 15 cameras in MSMT17)\nwith strong camera variance, only camera-agnostic variance\nconstraint Lh ins is not enough. We provide the dynamic\ncluster numbers of camera-agnostic ICE in Appendix D.\n4.5. Comparison with state-of-the-art methods\nWe compare ICE with state-of-the-art ReID methods in\nTab. 5.\nFigure 8:\nT-SNE visualization of 10 random classes in\nDukeMTMC-reID test set between camera-aware baseline (Left)\nand ICE (Right).\nComparison with unsupervised method.\nPrevious un-\nsupervised methods can be categorized into unsupervised\ndomain adaptation (UDA) and fully unsupervised meth-\nods.\nWe ﬁrst list state-of-the-art UDA methods, includ-\ning MMCL [33], JVTC [19], DG-Net++ [52], ECN+\n[51], MMT [11], DCML [1], MEB [42], SpCL [13] and\nABMT [3]. UDA methods usually rely on source domain\nannotation to reduce the pseudo label noise. Without any\nidentity annotation, our proposed ICE outperforms all of\nthem on the three datasets.\nUnder the fully unsupervised setting, ICE also achieves\nbetter performance than state-of-the-art methods, including\nBUC [21], SSL [22], MMCL [33], JVTC [19], HCT [41],\nCycAs [36], GCL [4], SpCL [13] and CAP [35]. CycAs\nleveraged temporal information to assist visual matching,\nwhile our method only considers visual similarity. SpCL\nand CAP are based on proxy contrastive learning, which\nare considered respectively as camera-agnostic and camera-\naware baselines in our method.\nWith a camera-agnostic\nmemory, the performance of ICE(agnostic) remarkably sur-\npasses the camera-agnostic baseline SpCL, especially on\nMarket1501 and MSMT17 datasets. With a camera-aware\nmemory, ICE(aware) outperforms the camera-aware base-\nline CAP on all the three datasets. By mining hard positives\nto reduce intra-class variance, ICE is more robust to hard\nsamples. We illustrate some hard examples in Fig. 9, where\nICE succeeds to notice important visual clues, e.g., char-\nacters in the shirt (1st row), blonde hair (2nd row), brown\nshoulder bag (3rd row) and badge (4th row).\nMethod\nReference\nMarket1501\nDukeMTMC-reID\nMSMT17\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nUnsupervised Domain Adaptation\nMMCL [33]\nCVPR’20\n60.4\n84.4\n92.8\n95.0\n51.4\n72.4\n82.9\n85.0\n16.2\n43.6\n54.3\n58.9\nJVTC [19]\nECCV’20\n61.1\n83.8\n93.0\n95.2\n56.2\n75.0\n85.1\n88.2\n20.3\n45.4\n58.4\n64.3\nDG-Net++ [52]\nECCV’20\n61.7\n82.1\n90.2\n92.7\n63.8\n78.9\n87.8\n90.4\n22.1\n48.8\n60.9\n65.9\nECN+ [51]\nTPAMI’20\n63.8\n84.1\n92.8\n95.4\n54.4\n74.0\n83.7\n87.4\n16.0\n42.5\n55.9\n61.5\nMMT [11]\nICLR’20\n71.2\n87.7\n94.9\n96.9\n65.1\n78.0\n88.8\n92.5\n23.3\n50.1\n63.9\n69.8\nDCML [1]\nECCV’20\n72.6\n87.9\n95.0\n96.7\n63.3\n79.1\n87.2\n89.4\n-\n-\n-\n-\nMEB [42]\nECCV’20\n76.0\n89.9\n96.0\n97.5\n66.1\n79.6\n88.3\n92.2\n-\n-\n-\n-\nSpCL [13]\nNeurIPS’20\n76.7\n90.3\n96.2\n97.7\n68.8\n82.9\n90.1\n92.5\n26.8\n53.7\n65.0\n69.8\nABMT [3]\nWACV’21\n78.3\n92.5\n-\n-\n69.1\n82.0\n-\n-\n26.5\n54.3\n-\n-\nFully Unsupervised\nBUC [21]\nAAAI’19\n29.6\n61.9\n73.5\n78.2\n22.1\n40.4\n52.5\n58.2\n-\n-\n-\n-\nSSL [22]\nCVPR’20\n37.8\n71.7\n83.8\n87.4\n28.6\n52.5\n63.5\n68.9\n-\n-\n-\n-\nJVTC [19]\nECCV’20\n41.8\n72.9\n84.2\n88.7\n42.2\n67.6\n78.0\n81.6\n15.1\n39.0\n50.9\n56.8\nMMCL [33]\nCVPR’20\n45.5\n80.3\n89.4\n92.3\n40.2\n65.2\n75.9\n80.0\n11.2\n35.4\n44.8\n49.8\nHCT [41]\nCVPR’20\n56.4\n80.0\n91.6\n95.2\n50.7\n69.6\n83.4\n87.4\n-\n-\n-\n-\nCycAs [36]\nECCV’20\n64.8\n84.8\n-\n-\n60.1\n77.9\n-\n-\n26.7\n50.1\n-\n-\nGCL [4]\nCVPR’21\n66.8\n87.3\n93.5\n95.5\n62.8\n82.9\n87.1\n88.5\n21.3\n45.7\n58.6\n64.5\nSpCL(agnostic) [13]\nNeurIPS’20\n73.1\n88.1\n95.1\n97.0\n65.3\n81.2\n90.3\n92.2\n19.1\n42.3\n55.6\n61.2\nICE(agnostic)\nThis paper\n79.5\n92.0\n97.0\n98.1\n67.2\n81.3\n90.1\n93.0\n29.8\n59.0\n71.7\n77.0\nCAP(aware)[35]\nAAAI’21\n79.2\n91.4\n96.3\n97.7\n67.3\n81.1\n89.3\n91.8\n36.9\n67.4\n78.0\n81.4\nICE(aware)\nThis paper\n82.3\n93.8\n97.6\n98.4\n69.9\n83.3\n91.5\n94.1\n38.9\n70.2\n80.5\n84.4\nSupervised\nPCB [29]\nECCV’18\n81.6\n93.8\n97.5\n98.5\n69.2\n83.3\n90.5\n92.5\n40.4\n68.2\n-\n-\nDG-Net [45]\nCVPR’19\n86.0\n94.8\n-\n-\n74.8\n86.6\n-\n-\n52.3\n77.2\n-\n-\nICE (w/ ground truth)\nThis paper\n86.6\n95.1\n98.3\n98.9\n76.5\n88.2\n94.1\n95.7\n50.4\n76.4\n86.6\n90.0\nTable 5: Comparison of ReID methods on Market1501, DukeMTMC-reID and MSMT17 datasets. The best and second best unsupervised\nresults are marked in red and blue.\nComparison with supervised method.\nWe further pro-\nvide two well-known supervised methods for reference, in-\ncluding the Part-based Convolutional Baseline (PCB) [29]\nand the joint Discriminative and Generative Network (DG-\nNet) [45]. Unsupervised ICE achieves competitive perfor-\nmance with PCB. If we replace the clustering generated\npseudo labels with ground truth, our ICE can be trans-\nformed into a supervised method. The supervised ICE is\ncompetitive with state-of-the-art supervised ReID methods\n(e.g., DG-Net), which shows that the supervised contrastive\nlearning has a potential to be considered into future super-\nvised ReID.\n5. Conclusion\nIn this paper, we propose a novel inter-instance con-\ntrastive encoding method ICE to address unsupervised\nReID. Deviated from previous proxy based contrastive\nReID methods, we focus on inter-instance afﬁnities to make\na model more robust to data variance. We ﬁrst mine the\nhardest positive with mini-batch instance pairwise similar-\nity ranking to form a hard instance contrastive loss, which\neffectively reduces intra-class variance. Smaller intra-class\nvariance contributes to the compactness of clusters. Then,\nwe use mini-batch instance pairwise similarity scores as soft\nlabels to enhance the consistency before and after data aug-\nmentation, which makes a model robust to artiﬁcial aug-\nmentation variance. By combining the proposed hard in-\nstance contrastive loss and soft instance consistency loss,\nFigure 9: Comparison of top 5 retrieved images on Market1501\nbetween CAP [35] and ICE. Green boxes denote correct results,\nwhile red boxes denote false results. Important visual clues are\nmarked with red dashes.\nICE signiﬁcantly outperforms previous unsupervised ReID\nmethods on Market1501, DukeMTMC-reID and MSMT17\ndatasets.\nAcknowledgements.\nThis work has been supported by\nthe French government, through the 3IA Cˆote d’Azur In-\nvestments in the Future project managed by the National\nResearch Agency (ANR) with the reference number ANR-\n19-P3IA-0002. The authors are grateful to the OPAL in-\nfrastructure from Universit´e Cˆote d’Azur for providing re-\nsources and support.\nReferences\n[1] Guangyi Chen, Yuhao Lu, Jiwen Lu, and Jie Zhou. Deep\ncredible metric learning for unsupervised domain adaptation\nperson re-identiﬁcation. In ECCV, 2020. 2, 7, 8\n[2] Hao Chen, Benoit Lagadec, and Francois Bremond. Learn-\ning discriminative and generalizable representations by\nspatial-channel partition for person re-identiﬁcation.\nIn\nWACV, 2020. 1\n[3] Hao Chen, Benoit Lagadec, and Francois Bremond.\nEn-\nhancing diversity in teacher-student networks via asymmet-\nric branches for unsupervised person re-identiﬁcation.\nIn\nWACV, 2021. 2, 7, 8\n[4] Hao\nChen,\nYaohui\nWang,\nBenoit\nLagadec,\nAntitza\nDantcheva, and Francois Bremond. Joint generative and con-\ntrastive learning for unsupervised person re-identiﬁcation. In\nCVPR, 2021. 2, 7, 8\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020. 1, 2\n[6] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\nNorouzi, and Geoffrey Hinton. Big self-supervised models\nare strong semi-supervised learners. In NeurIPS, 2020. 2\n[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv preprint arXiv:2003.04297, 2020. 2\n[8] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Instance-\nguided context rendering for cross-domain person re-\nidentiﬁcation. In ICCV, 2019. 2\n[9] Martin Ester, Hans-Peter Kriegel, J¨org Sander, and Xiaowei\nXu. A density-based algorithm for discovering clusters in\nlarge spatial databases with noise. In KDD, 1996. 3, 10\n[10] Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou,\nHonghui Shi, and Thomas S Huang. Self-similarity group-\ning: A simple unsupervised cross domain adaptation ap-\nproach for person re-identiﬁcation. In ICCV, 2019. 2\n[11] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\nteaching: Pseudo label reﬁnery for unsupervised domain\nadaptation on person re-identiﬁcation. In ICLR, 2020. 1,\n2, 3, 7, 8\n[12] Yixiao Ge, Haibo Wang, Feng Zhu, Rui Zhao, and Hong-\nsheng Li. Self-supervising ﬁne-grained region similarities\nfor large-scale image localization. In ECCV, 2020. 2\n[13] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hong-\nsheng Li. Self-paced contrastive learning with hybrid mem-\nory for domain adaptive object re-id. In NeurIPS, 2020. 1,\n2, 3, 7, 8, 10, 11\n[14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020. 1, 2, 3\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 3, 5\n[16] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\nfense of the triplet loss for person re-identiﬁcation. arXiv\npreprint arXiv:1703.07737, 2017. 1, 4\n[17] Jieru Jia, Q. Ruan, and Timothy M. Hospedales. Frustrat-\ningly easy person re-identiﬁcation: Generalizing person re-\nid in practice. In BMVC, 2019. 10\n[18] Samuli Laine and Timo Aila. Temporal ensembling for semi-\nsupervised learning. In ICLR, 2017. 2, 5\n[19] Jianing Li and Shiliang Zhang.\nJoint visual and tempo-\nral consistency for unsupervised domain adaptive person re-\nidentiﬁcation. In ECCV, 2020. 2, 7, 8\n[20] Shan Lin, Haoliang Li, Chang-Tsun Li, and Alex Chichung\nKot. Multi-task mid-level feature alignment network for un-\nsupervised cross-dataset person re-identiﬁcation. In BMVC,\n2018. 2\n[21] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\nYang. A bottom-up clustering approach to unsupervised per-\nson re-identiﬁcation. In AAAI, 2019. 1, 2, 7, 8\n[22] Yutian Lin, Lingxi Xie, Yu Wu, Chenggang Yan, and Qi\nTian.\nUnsupervised person re-identiﬁcation via softened\nsimilarity learning. In CVPR, 2020. 1, 2, 7, 8\n[23] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei\nJiang. Bag of tricks and a strong baseline for deep person\nre-identiﬁcation. In CVPR Workshops, June 2019. 1\n[24] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two\nat once: Enhancing learning and generalization capacities\nvia ibn-net. In ECCV, 2018. 5, 10\n[25] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking.\nIn ECCV workshops,\n2016. 5\n[26] Olga Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Zhiheng Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, A. Berg, and Li Fei-Fei. Imagenet large scale visual\nrecognition challenge. IJCV, 2015. 5\n[27] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao\nZhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin,\nHan Zhang, and Colin Raffel. Fixmatch: Simplifying semi-\nsupervised learning with consistency and conﬁdence.\nIn\nNeurIPS, 2020. 2, 5\n[28] Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian\nZhang, Chang Huang, and Xinggang Wang. Unsupervised\ndomain adaptive re-identiﬁcation: Theory and practice. PR,\n2020. 1, 2\n[29] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. Beyond part models: Person retrieval with reﬁned\npart pooling (and a strong convolutional baseline). In ECCV,\n2018. 1, 8\n[30] Antti Tarvainen and Harri Valpola. Mean teachers are better\nrole models: Weight-averaged consistency targets improve\nsemi-supervised deep learning results. In NeurIPS, 2017. 2,\n5\n[31] A¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. ArXiv,\nabs/1807.03748, 2018. 2\n[32] Laurens van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-SNE. JMLR, 2008. 6\n[33] Dongkai Wang and Shiliang Zhang. Unsupervised person re-\nidentiﬁcation via multi-label classiﬁcation. In CVPR, 2020.\n1, 2, 7, 8\n[34] Jingya Wang, Xiatian Zhu, Shaogang Gong, and Wei Li.\nTransferable joint attribute-identity deep learning for unsu-\npervised person re-identiﬁcation. CVPR, 2018. 2\n[35] Menglin Wang, Baisheng Lai, Jianqiang Huang, Xiaojin\nGong, and Xian-Sheng Hua. Camera-aware proxies for un-\nsupervised person re-identiﬁcation. In AAAI, 2021. 2, 3, 4,\n7, 8, 11\n[36] Zhongdao Wang, Jingwei Zhang, Liang Zheng, Yixuan Liu,\nYifan Sun, Yali Li, and Shengjin Wang.\nCycas:\nSelf-\nsupervised cycle association for learning re-identiﬁable de-\nscriptions. In Andrea Vedaldi, Horst Bischof, Thomas Brox,\nand Jan-Michael Frahm, editors, ECCV, 2020. 2, 7, 8\n[37] Chen Wei, Huiyu Wang, Wei Shen, and Alan Yuille. Co2:\nConsistent contrast for unsupervised visual representation\nlearning. In ICLR, 2021. 2, 5\n[38] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nPerson transfer gan to bridge domain gap for person re-\nidentiﬁcation. In CVPR, 2018. 2, 5\n[39] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. CVPR, 2018. 1, 2\n[40] Hong-Xing Yu, W. Zheng, Ancong Wu, X. Guo, S. Gong,\nand J. Lai.\nUnsupervised person re-identiﬁcation by soft\nmultilabel learning. CVPR, 2019. 2\n[41] Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo.\nHierarchical clustering with hard-batch triplet loss for person\nre-identiﬁcation. In CVPR, 2020. 2, 7, 8\n[42] Yunpeng Zhai, Qixiang Ye, Shijian Lu, Mengxi Jia, Ron-\ngrong Ji, and Yonghong Tian. Multiple expert brainstorming\nfor domain adaptive person re-identiﬁcation. In ECCV, 2020.\n1, 7, 8\n[43] Xinyu Zhang, Jiewei Cao, Chunhua Shen, and Mingyu You.\nSelf-training with progressive augmentation for unsuper-\nvised cross-domain person re-identiﬁcation. In ICCV, 2019.\n2\n[44] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\ndong Wang, and Qi Tian. Scalable person re-identiﬁcation:\nA benchmark. ICCV, 2015. 5\n[45] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng,\nYi Yang, and Jan Kautz. Joint discriminative and generative\nlearning for person re-identiﬁcation. In CVPR, 2019. 8\n[46] Zhedong Zheng and Yi Yang. Rectifying pseudo label learn-\ning via uncertainty estimation for domain adaptive semantic\nsegmentation. IJCV, 2021. 2\n[47] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-\nranking person re-identiﬁcation with k-reciprocal encoding.\nIn CVPR, 2017. 6\n[48] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In AAAI, 2020.\n6\n[49] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Gener-\nalizing a person retrieval model hetero- and homogeneously.\nIn ECCV, 2018. 2\n[50] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi\nYang.\nInvariance matters: Exemplar memory for domain\nadaptive person re-identiﬁcation. In CVPR, 2019. 2\n[51] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi\nYang. Learning to adapt invariance in memory for person\nre-identiﬁcation. IEEE TPAMI, 2020. 7, 8\n[52] Yang Zou, Xiaodong Yang, Zhiding Yu, B. V. K. Vijaya Ku-\nmar, and Jan Kautz. Joint disentangling and adaptation for\ncross-domain person re-identiﬁcation. In ECCV, 2020. 2, 7,\n8\nAppendices\nAppendix A. Algorithm Details\nThe ICE algorithm details are provided in Algorithm 1.\nAlgorithm 1: Inter-instance Contrastive Encoding\n(ICE) for fully unsupervised ReID.\nInput : Unlabeled dataset X, ImageNet pre-trained\nonline encoder θo, ImageNet pre-trained\nmomentum encoder θm, maximal epoch Emax\nand maximal iteration Imax.\nOutput: Momentum encoder θm after training.\n1 for epoch = 1 to Emax do\n2\nEncode X to momentum representations M with the\nmomentum encoder θm;\n3\nRerank and Generate clustering pseudo labels Y on\nmomentum representations M with DBSCAN;\n4\nCalculate cluster proxies in Eq. (4) and camera\nproxies in Eq. (6) based on Y;\n5\nfor iter = 1 to Imax do\n6\nCalculate inter-instance similarities in a\nmini-batch;\n7\nTrain θo with the total loss in Eq. (3) which\ncombines proxy contrastive loss in Eq. (8), hard\ninstance contrastive loss in Eq. (9) and soft\ninstance consistency loss in Eq. (12);\n8\nUpdate θm by Eq. (2);\n9\nend\n10 end\nAppendix B. Backbone Network\nInstance-batch normalization (IBN) [24] has shown bet-\nter performance than regular batch normalization in unsu-\npervised domain adaptation [24, 13] and domain general-\nization [17].\nWe compare the performance of ICE with\nResNet50 and IBN-ResNet50 backbones in Tab. 6. The per-\nformance of our proposed ICE can be further improved with\nan IBN-ResNet50 backbone network.\nAppendix C. Threshold in clustering\nIn DBSCAN [9], the distance threshold is the maximum\ndistance between two samples for one to be considered as in\nthe neighborhood of the other. A smaller distance threshold\nis likely to make DBSCAN mark more hard positives as dif-\nferent classes. On the contrary, a larger distance threshold\nmakes DBSCAN mark more hard negatives as same class.\nIn the main paper, the distance threshold for DBSCAN\nbetween same cluster neighbors is set to 0.55, which is a\nBackbone\nMarket1501\nDukeMTMC-reID\nMSMT17\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nResNet50\n82.3\n93.8\n97.6\n98.4\n69.9\n83.3\n91.5\n94.1\n38.9\n70.2\n80.5\n84.4\nIBN-ResNet50\n82.5\n94.2\n97.6\n98.5\n70.7\n83.6\n91.9\n93.9\n40.6\n70.7\n81.0\n84.6\nTable 6: Comparison of ResNet50 and IBN-ResNet50 backbones on Market1501, DukeMTMC-reID and MSMT17 datasets.\nThreshold\nMarket1501\nDukeMTMC-reID\nMSMT17\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\n0.45\n82.5\n93.4\n97.5\n98.3\n68.0\n82.8\n91.5\n93.4\n36.6\n69.2\n79.3\n82.7\n0.5\n83.0\n94.1\n97.7\n98.3\n69.2\n82.9\n91.2\n93.2\n38.4\n69.9\n80.2\n83.8\n0.55\n82.3\n93.8\n97.6\n98.4\n69.9\n83.3\n91.5\n94.1\n38.9\n70.2\n80.5\n84.4\n0.6\n81.2\n93.0\n97.3\n98.5\n69.4\n83.5\n91.4\n94.0\n39.4\n70.9\n81.0\n84.5\nTable 7: Comparison of different distance thresholds on Market1501, DukeMTMC-reID and MSMT17 datasets.\nFigure 10: Dynamic cluster numbers of ICE(agnostic) during 40\ntraining epochs on DukeMTMC-reID. A lower number denotes\nthat clusters are more compact (less intra-cluster variance).\ntrade-off number for Market1501, DukeMTMC-reID and\nMSMT17 datasets. To get a better understanding of how\nICE is sensitive to the distance threshold, we vary the\nthreshold from 0.45 to 0.6. As shown in Tab. 7, a smaller\nthreshold 0.5 is more appreciate for the relatively smaller\ndataset Market1501, while a larger threshold 0.6 is more\nappreciate for the relatively larger dataset MSMT17. State-\nof-the-art unsupervised ReID methods SpCL [13] and CAP\n[35] respectively used 0.6 and 0.5 as their distance thresh-\nold. Our proposed ICE can always outperform SpCL and\nCAP on the three datasets with a threshold between 0.5 and\n0.6.\nAppendix D. Camera-agnostic scenario\nAs mentioned in the main paper, we provide the dynamic\ncluster numbers of camera-agnostic ICE during the training\nin Fig. 10. The red curve is trained without the hard instance\ncontrastive loss Lh ins as intra-class variance constraint. In\nthis case, the soft instance consistency loss Ls ins main-\ntains high intra-class variance, e.g., AA ̸≈AP1 ̸≈AP2 ̸≈\nAP3 ̸≈1, which leads to less compact clusters. The or-\nange curve is trained without Ls ins, which has less clusters\nat the beginning but more clusters at last epochs than the\nblue curve. The blue curve is trained with both Lh ins and\nLs ins, whose cluster number is most accurate among the\nthree curves at last epochs. Fig. 10 conﬁrms that combining\nLh ins and Ls ins reduces naturally captured and artiﬁcially\naugmented view variance at the same time, which gives op-\ntimal ReID performance.\nAppendix E. Future work\nOur proposed method is designed for traditional short-\nterm person ReID, in which persons do not change their\nclothes. For long-term person ReID, when persons take off\nor change their clothes, our method is prone to generate\nless robust pseudo labels, which relies on visual similarity\n(mainly based on cloth color). For future work, an interest-\ning direction is to consider how to generate robust pseudo\nlabels to tackle the cloth changing problem for long-term\nperson ReID.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-03-30",
  "updated": "2021-08-18"
}