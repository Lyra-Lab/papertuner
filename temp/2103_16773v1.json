{
  "id": "http://arxiv.org/abs/2103.16773v1",
  "title": "PAUL: Procrustean Autoencoder for Unsupervised Lifting",
  "authors": [
    "Chaoyang Wang",
    "Simon Lucey"
  ],
  "abstract": "Recent success in casting Non-rigid Structure from Motion (NRSfM) as an\nunsupervised deep learning problem has raised fundamental questions about what\nnovelty in NRSfM prior could the deep learning offer. In this paper we advocate\nfor a 3D deep auto-encoder framework to be used explicitly as the NRSfM prior.\nThe framework is unique as: (i) it learns the 3D auto-encoder weights solely\nfrom 2D projected measurements, and (ii) it is Procrustean in that it jointly\nresolves the unknown rigid pose for each shape instance. We refer to this\narchitecture as a Procustean Autoencoder for Unsupervised Lifting (PAUL), and\ndemonstrate state-of-the-art performance across a number of benchmarks in\ncomparison to recent innovations such as Deep NRSfM and C3PDO.",
  "text": "PAUL: Procrustean Autoencoder for Unsupervised Lifting\nChaoyang Wang1\nSimon Lucey1,2\n1Carnegie Mellon University 2University of Adelaide\n{chaoyanw, slucey}@cs.cmu.edu\nAbstract\nRecent success in casting Non-rigid Structure from Mo-\ntion (NRSfM) as an unsupervised deep learning problem\nhas raised fundamental questions about what novelty in\nNRSfM prior could the deep learning offer.\nIn this pa-\nper we advocate for a 3D deep auto-encoder framework to\nbe used explicitly as the NRSfM prior. The framework is\nunique as: (i) it learns the 3D auto-encoder weights solely\nfrom 2D projected measurements, and (ii) it is Procrustean\nin that it jointly resolves the unknown rigid pose for each\nshape instance.\nWe refer to this architecture as a Pro-\ncustean Autoencoder for Unsupervised Lifting (PAUL), and\ndemonstrate state-of-the-art performance across a number\nof benchmarks in comparison to recent innovations such as\nDeep NRSfM [21] and C3PDO [32].\n1. Introduction\nInferring non-rigid 3D structure from multiple unsyn-\nchronized 2D imaged observations is an ill-posed problem.\nNon-Rigid Structure from Motion (NRSfM) methods ap-\nproach the problem by introducing additional priors – of\nparticular note in this regards are low rank [10, 6, 3] and\nunion of subspaces [25, 45] methods.\nRecently, NRSfM has seen improvement in performance\nby recasting the problem as an unsupervised deep learning\nproblem [32, 8, 33]. These 2D-3D lifting networks have\ninherent advantages over classical NRSfM as: (i) they are\nmore easily scalable to larger datasets, and (ii) they al-\nlow fast feed-forward prediction once trained. These im-\nprovement, however, can largely be attributed to the end-\nto-end reframing of the learning problem rather than any\nfundamental shift in the prior/constraints being enforced\nwithin the NRSfM solution.\nFor example, both Cha et\nal. [8] and Park et al. [33] impose a classical low rank con-\nstraint on the recovered 3D shape. It is also well under-\nstood [10, 21, 45, 25] that such low rank priors have poor\nperformance when applied to more complex 3D shape vari-\nations.\nThe NRSfM ﬁeld has started to explore new non-rigid\nshape priors inspired by recent advances in deep learning.\nKong & Lucey [21] proposed the use of hierarchical spar-\nsity to have a more expressive shape model while ensuring\nthe inversion problem remains well conditioned. Although\nachieving signiﬁcant progress in several benchmarks, the\napproach is limited by the somewhat adhoc approximations\nit employs so as to make the entire NRSfM solution real-\nizable as a feed-forward lifting network. Such approxima-\ntions hamper the interpretability of the method as the ﬁnal\nnetwork is a substantial departure from the actually pro-\nposed objective. We further argue that this departure from\nthe true objective also comes at the cost of the overall effec-\ntiveness of the 2D-3D lifting solution.\nIn this paper we propose a prior that 3D shapes aligned\nto a common reference frame are compressible with an un-\ndercomplete auto-encoder. This is advantageous over pre-\nvious linear methods, because the deeper auto-encoder is\nnaturally capable of compressing more complicated non-\nrigid 3D shapes. What makes learning such an auto-encoder\nchallenging is: (i) it observes only 2D projected measure-\nments of the non-rigid shape; (ii) it must automatically\nresolve the unknown rigid transformation to align each\nprojected shape instance.\nWe refer to our solution as a\nProcustean Autoencoder for Unsupervised Lifting (PAUL).\nPAUL is considered unsupervised as it has to handle un-\nknown depth, shape pose, and occlusions.\nUnlike Deep\nNRSfM [21], the optimization process of PAUL does not\nhave to be realizable as a feed-forward network – allow-\ning for a solution that stays tightly coupled to the proposed\nmathematical objective.\nWe also explore other alternative deep shape priors such\nas: decoder only and decoder + low-rank. A somewhat sim-\nilar approach is recently explored by Sidhu et al. [34] for\ndense NRSfM. Our empirical results demonstrate the fun-\ndamental importance of the auto-encoder architecture for\n2D-3D lifting.\nContributions: We make the following contributions:\n• We present an optimization objective for joint learning\nthe 2D-3D lifting network and the Procrustean auto-\nencoder solely from 2D projected measurements.\n1\narXiv:2103.16773v1  [cs.CV]  31 Mar 2021\n3D shape at canonical frame\nNon-sequential 2D keypoints\nlearn\nchair\nH3.6M\naeroplane\nFigure 1: PAUL learns to reconstruct 3D shapes aligned to a\ncanonical frame, using 2D keypoint annotations only. Bot-\ntom rows show 3D shapes interpolated from the learned la-\ntent space of the Procrustean auto-encoder.\n• A naive implementation of PAUL through gradient de-\nscent would result in poor local minima, so instead we\nadvocate for a bilevel optimization, whose lower level\nproblem can be efﬁciently solved by orthographic-N-\npoint (OnP) algorithms.\n• Our method achieves state-of-the-art performance\nacross multiple benchmarks, and is empirically shown\nto be robust against the choice of hyper-parameters\nsuch as the dimension of the latent code space.\n2. Related Work\nNon-rigid structure from motion.\nNRSfM concerns the\nproblem of recovering 3D shapes from 2D point correspon-\ndences from multiple images, without the assumption of the\n3D shape being rigid. It is ill-posed by nature, and addi-\ntional priors are necessary to guarantee the uniqueness of\nthe solution. We focus our discussion on the type of priors\nimposed on shape/trajectory:\n(i) low-rank was advocated by Bregler et al. [6] based\non the insight that rigid 3D structure has a ﬁxed rank of\nthree [36].\nDai et al. [10] proved that the low-lank as-\nsumption is standalone sufﬁcient to solve NRSfM. It is\nalso applied temporally [12, 3] to constraint 3D trajecto-\nries. Kumar [24] recently revisited Dai’s approach [10] and\nshowed that by properly utilizing the assumptions that de-\nformation is smooth over frames, it is able to obtain com-\npetitive accuracy on benchmarks. However, since the rank\nis strictly limited by the minimum of the number of points\nand frames [10], it becomes infeasible to solve large-scale\nproblems with complex shape variations when the num-\nber of points is substantially smaller than the number of\nframes [21].\n(ii) union-of-subspaces is inspired by the intuition that com-\nplex non-rigid deformations could be clustered into a se-\nquence of simple motions [45]. It was extended to spatial-\ntemporal domain [25] and structure from category [2]. The\nmain limitation of using union-of-subspaces is how to effec-\ntively cluster deforming shapes from 2D measurements, and\nhow to compute afﬁnity matrix when the number of frames\nis huge.\n(iii) sparsity [22, 20, 44], is a more generic prior compared\nto union-of-subspaces. However, due to the sheer number\nof possible subspaces to choose, it is sensitive to noise.\n(iv) Procrustean normal distribution [27] assumes that the\n3D structure follows a normal distribution if aligned to a\ncommon reference frame.\nIt allows reconstruction with-\nout specifying ranks which are typically required by other\nmethods.\nIt was extended temporally as a Procrustean\nMarkov process [29]. Limited by assuming normal distri-\nbution, it is less favorable to model deformation which is\nnot Gaussian.\nUnsupervised 2D-3D lifting. NRSfM can be recast for un-\nsupervised learning 2D-3D lifting. Cha et al. [8] use low-\nrank loss as a learning objective to constraint the shape out-\nput of the 2D-3D lifting network. Park et al. [33] further\nmodiﬁes Cha’s approach by replacing the camera estima-\ntion network with an analytic least square solution which\naligns 3D structures to the mean shape of a sequence. Due\nto the inefﬁciency of low-rank to model complex shape vari-\nations, these methods are restricted to datasets with simpler\nshape variations, or requires temporal order so as to avoid\ndirectly handling global shape variations.\nInstead of using classical NRSfM priors, recent works\nexplore the use of deeper constraints. Generative Adver-\nsarial Networks (GANs) [14] are used to enforce realism\nof 2D reprojections across novel viewpoints [9, 39, 11, 23].\nThese methods are only applicable for large datasets due to\nthe requirement of learning GANs. It is also unclear how to\ndirectly learn GANs with training set existing missing data.\nNovotny et al. [32] instead enforces self-consistency on\nthe predicted canonicalization of the randomly perturbed\n3D shapes. Kong & Lucey [21] proposed the use of hi-\nerarchical sparsity as constraint, and approximate the op-\ntimization procedure of hierarchical sparse coding as a\nfeed-forward lifting network. It was recently extended by\nWang et al. [40] to handle missing data and perspective pro-\njection. These approaches use complicated network archi-\ntecture to enforce constraints as well as estimating camera\nmotion, while our method uses simpler constraint formula-\ntion, and realized with efﬁcient solution.\n2\nℎ\n𝑓!\n𝑓\"\n𝑓!\nOnP\nR∗\n*\n*\n2D-3D \nencoder\ndecoder\nencoder\n2D input\n3D recon.\nat canonical frame\nW\n𝑧∗$\nW\n𝑧∗$\nW\n𝑧∗$\n𝜑\n𝐿'()\n‖\n‖\n‖\n‖\n𝐿'(*+,. /0\n𝐿'(*+,. /1\n-\n-\nW\nℎ\n𝑓!\nS\nR∗\nOnP\nS%&'\n(a) NRSfM / Unsupervised training\n(b) Test on unseen data\nFigure 2: (a) In training, PAUL jointly optimizes the depth value z, camera rotation R together with the network weights. It\nis realized through a bilevel optimization strategy, which analytically computes R∗and z∗as the solution to an OnP problem.\nThe learning objective is formulated as a combination of reconstruction loss for the decoder-only stream (top row) and the\nauto-encoder (bottom row) together with regularizer Lreg applied on the code ϕ and decoder’s network weights; (b) in testing,\nonly 2D-3D encoder h and decoder fd are used. Camera rotation is directly estimated by OnP.\n3. Preliminary\nProblem setup. We are interested in the atemporal setup\nfor unsupervised 2D-3D lifting, which is a general setup\nthat not only works with single deforming objects, but also\nmultiple objects from the same object category. Speciﬁ-\ncally, given a non-sequential dataset consist of N frames\nof 2D keypoint locations {W(1), . . . , W(N)}, where each\nW ∈R2×P represents 2D location for P keypoints, and\nvisibility masks represented as diagonal binary matrices\n{M(1), . . . , M(N)}, we want to (i) recover the 3D locations\nfor every keypoints in the dataset, and (ii) train a 2D-3D lift-\ning network capable of making single frame prediction for\nunseen data.\nWeak perspective camera model. We assume weak per-\nspective projections, i.e. for a 3D structure S deﬁned at a\ncanonical frame, its 2D projection is approximated as:\nW ≈sRxyS + txy\n(1)\nwhere Rxy ∈R2×3, txy ∈R2 are the x-y component of\na rigid transformation, and s > 0 is the scaling factor in-\nversely proportional to the object depth if the true camera\nmodel is pin-hole. If all 2D points are visible and centered,\ntxy could be omitted by assuming the origin of the canoni-\ncal frame is at the center of the object. Due to the bilinear\nform of (1), s is ambiguous and becomes up-to-scale re-\ncoverable only when S is assumed to follow certain prior\nstatistics. A typical treatment to handle scale is to approxi-\nmate with orthogonal projection by normalizing the scale of\nW, setting s = 1 and leaving S to be scaled reconstruction.\nRegularized auto-encoder (RAE) for S. We assume that\nthe 3D shapes, if aligned to a canonical frame, are com-\npressible by an undercomplete auto-encoder with a low-\ndimensional bottleneck, i.e.\nS ≈fd ◦fe(S),\n(2)\nwhere fe is the encoder which maps S to a K-dimensional\nlatent code ϕ ∈RK , fd is the decoder function and ◦de-\nnotes function composition. In this work, we choose de-\nterministic RAE [13] instead of variational auto-encoder\n(VAE) [19] since RAE is easier to train and still leads to an\nequally smooth and meaningful latent space. The learning\nobjective for RAE is a combination of reconstruction loss\nand regularizers on the latent codes as well as the decoder’s\nweights,\nLRAE(x; θd, θe) = ∥fd ◦fe(x) −x∥F + Lreg.\n(3)\nwhere θd, θe are network weights for the auto-encode, x\ndenote data samples, and the regularizer Lreg is picked to\nbe ∥ϕ∥2\n2 and weight decay, which was shown to give com-\nparable performance to VAE when generating images and\nstructured objects [13].\n2D-3D lifting network. A 2D-3D lifting network is de-\nsigned to take input from 2D keypoints and visiblity mask,\nand outputs 3D keypoint locations. We assume the network\narchitecture is decomposed into two parts (i) a 2D-3D en-\ncoder h which maps 2D observations to latent code ϕ, and\n(ii) a decoder fd (reused from the auto-encoder) to generate\n3D shapes from ϕ. Thus this type of 2D-3D lifting network\ncan be expressed as fd ◦h(W, M), which is a general form\nfor network architectures used in literature [21, 30, 32].\n4. Learning Procrustean auto-encoder from 2D\nFor clarity, in this section we simplify the problem by as-\nsuming all points are visible, which allows removing trans-\n3\nlational component in (1). Description of handling occlu-\nsions are given in Sec. 4.3. Fig. 2 illustrates the proposed\napproach.\n4.1. Learning objective\nProcrustean auto-encoder.\nDirectly compressing 3D\nshapes Scam at camera frame is inefﬁcient due to the inclu-\nsion of the degrees of freedom from camera motion. There-\nfore, we choose to impose compressibility on S at canon-\nical frame as shown in (2). However, learning such auto-\nencoder from 2D observations requires overcoming sev-\neral obstacles: (i) due to the objects being non-rigid, the\ndeﬁnition of canonical frame is statistical and implicitly\nrepresented by the unknown rigid transformations to align\nScam’s; (ii) choosing canonical frame requires knowing the\nstatistics of Scam which we do not have complete informa-\ntion, since only the ﬁrst two rows of Scam are given as W\nrepresenting the x-y coordinates, while the 3rd row z repre-\nsenting depth values are unknown; (iii) reconstructing Scam\nin turn requires the estimation of the rigid transformation\nas well as the statistical model of the shape. To overcome\nthese, we propose a joint optimization scheme:\nmin\nθe,θd,\n{z(i)},{R(i)∈SO(3)}\nN\nP\ni=1\nLRAE(R(i)⊤\n\"\nW(i)\nz(i)⊤\n#\n; θe, θd),\n(4)\nwhere θe, θd are network weights for the auto-encoder,\nR⊤\n\u0014W\nz⊤\n\u0015\ncomputes S at the canonical frame.\nHowever, (4) is still steps away from being applicable\nto unsupervised 2D-3D lifting, since it misses the 2D-3D\nlifting network module in the objective function, and is dif-\nﬁcult to optimize due to the inclusion of unknown rotation\nmatrices in the input of the auto-encoder. In the following,\nwe address these by reparameterizing the learning objective\nand propose an efﬁcient optimization scheme.\nReparameterization for learning 2D-3D lifting. First we\nintroduce an auxiliary variable as the latent code ϕ, which\nsatisﬁes\nfd(ϕ) = R⊤\u0002\nW⊤\nz\n\u0003⊤.\n(5)\nThis leads to transforming (4) to a constrained optimization\nobjective with (5) as the constraint and the input to fd ◦fe\nreplaced by fd(ϕ),\nmin\nθe,θd,\n{R(i)∈SO(3)}\n{z(i), ϕ(i)}\nN\nX\ni=1\n∥fd ◦fe ◦fd(ϕ(i)) −R(i)⊤\n\"\nW(i)\nz(i)⊤\n#\n|\n{z\n}\nLrecon. AE(ϕ,R,z;θe,θd)\n∥F + Lreg,\ns.t. fd(ϕ) = R⊤\u0002\nW⊤\nz\n\u0003⊤.\n(6)\nDepending on the type of task, ϕ could be either treated\nas free variables to optimize if the task is to reconstruct\nthe ‘training’ set as a NRSfM problem, or ϕ could be the\nnetwork output of the 2D-3D encoder i.e. ϕ = h(W; θh).\nWe then relax the constrained optimization into an uncon-\nstrained one, which allows passing gradients to the weights\nof the 2D-3D encoder h,\nmin\nθh,θe,θd,\n{z(i)},{R(i)∈SO(3)}\nN\nX\ni=1\nLrecon. AE(h(W(i)), R(i), z(i))\n+ ∥fd ◦h(W(i)) −R(i)⊤\n\"\nW(i)\nz(i)⊤\n#\n∥F + Lreg.\n(7)\nThis loss function could be understood as the combination\nof the reconstruction losses for both an auto-encoder and an\nauto-decoder together with the regularizer from RAE, i.e.\nLrecon. AE + Lrecon. AD + Lreg.\nRelation to learning with auto-decoder. We note that an\nalternative auto-decoder approach with learning objective\nLrecon. AD + Lreg is applicable, the additional Lrecon. AE in\nour approach is to enforce the existence of a continuous in-\nverse mapping from 3D shape to latent code. This encour-\nages shapes with small variation to stay close in the latent\nspace, which is helpful to learn a meaningful and smoother\nlatent space. We investigate both approaches in Fig. 5 and\ncompare the learnt latent space visually in Fig. 3.\n4.2. Efﬁcient bilevel optimization\nDirectly optimizing (7) with gradient descent is inefﬁ-\ncient due to (i) the objective is non-convex and it is prone to\npoor local minima especially with respect to R. One could\nuse an off-the-shelf NRSfM method to provide initializa-\ntion for R [34]. However this would make the solution\nsensitive to the accuracy of the chosen NRSfM algorithm.\n(ii) when using SGD for large datasets, it is problematic to\nproperly update R(i) and z(i) if they are left as independent\nvariables. Alternatively, one could introduce additional net-\nworks to output R or z conditioned on 2D inputs [39, 21].\nWe ﬁnd this unnecessary because it introduces extra com-\nplexity to solve the problem but is still subject to the inefﬁ-\nciency of gradient descent.\nFor a more efﬁcient optimization strategy, we propose to\nﬁrst rearrange (7) to an equivalent bilevel objective:\nmin\nθh,θd,θe\nN\nX\ni=1\nmin\nR(i)∈SO(3), z(i) L(i)\nrecon. AE+L(i)\nrecon. AD+L(i)\nreg, (8)\nThe beniﬁt of this rearrangement is that the lower level\nproblem, i.e. minimizing the reconstruction losses with re-\nspect to R and z can be viewed as an extension of the\northographic-N-point (OnP) problem [35], which allows the\nuse of efﬁcient solvers [16, 5, 31]. In addition, if an OnP\nsolver reﬁned by geometric loss is able to converge to lo-\ncal minima, it is not required to be differentiable due to the\nfact that both lower-level and upper-level problems share\n4\ndrink\npickup\nshark\nADL(baseline)\nPAUL\n(a) 2D-latent space on short sequences with smooth\ncamera trajectories.\nADL (baseline)\nPAUL\n(b) 2D-latent space on long sequence (CMU-MoCap S70)\nperturbed with random cameras.\nFigure 3: Visualization of 2D latent space for ADL &\nPAUL. Each point represents the 2D latent code recovered\nfor each frame of a sequence. The color of points (from dark\nblue to bright yellow) indicates the temporal order of points.\nIdeally, the points should form trajectories in the temporal\norder. PAUL gives clearer trajectory-like structures in its\nlatent space, while ADL’s recovered codes are either more\nspread-out or form broken trajectories.\nthe same objective function, thus the gradient is zero at lo-\ncal minima [15]. This would lift the restrictions for the type\nof solvers we could use for the lower-level problem.\nDifferentiable fast solver for the lower-level problem.\nOn the other hand, we opt to use an algebraic solution\nwhich is computationally more light-weight compared to\nOnP solvers iteratively minimizing the geometric error. The\ncompromise of using an approximate (e.g. algebraic) so-\nlution is that, since it does not not necessarily reach local\nminima, it is required to be implemented as a differentiable\noperator, which could be easily accomplished via modern\nautograd packages. The solution we picked is:\n1. Find the closed-form least square solution ˜R∗for min-\nimizing the reprojection error:\nmin\n˜R\n∥˜R(fd ◦fe ◦fd)(ϕ) −W∥2\n2 + ∥˜Rfd(ϕ) −W∥2\n2.\n(9)\n2. Project ˜R∗to become a rotation matrix R∗∈SO(3)\nusing SVD.\n2\n4\n6\n8\n10\n12\nBottleneck dim\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nNormalized error (%)\ndance\nyoga\nstretch\nshark\ndrink\nFigure 4: 3D reconstruction error with different bottleneck\ndimensions. For each conﬁguration, PAUL is run 10 times\nand visualize with average accuracy (solid lines) together\nwith standard deviation (colored area).\n3. z∗= 1\n2((fd ◦fe ◦fd)⊤(ϕ)r∗\nz + fd(ϕ)⊤r∗\nz), which is\nthe closed-form least square solution for minimizing:\nmin\nz ∥(fd ◦fe ◦fd)⊤(ϕ)r∗\nz −z∥2\n2 + ∥fd(ϕ)⊤r∗\nz −z∥2\n2,\n(10)\nwhere r∗\nz\n⊤denotes the 3rd row of R∗.\nEnd-to-end training. Finally, with the approximate solu-\ntion R∗, z∗for the lower-level problem, the learning ob-\njective once again becomes a single level one, which is\nidentical to (7) except that R, z instead of being free vari-\nables, they are now replaced by R∗, z∗which are dif-\nferentiable functions conditioned on the network weights\nθh, θe, θd. This allows learning these weights end-to-end via\ngradient descent.\nPrediction on unseen data. To make 3D prediction of a\nsingle frame from unseen data, we ﬁrst use the learned 2D-\n3D encoder h and the decoder fd to compute S at the canon-\nical frame, and then run OnP algorithm [35] to align it to the\ncamera frame.\n4.3. Handling missing data\nIf there exists 2D keypoints missing from the observation\ndue to occlusions or out of image, the translational compo-\nnent in (1) is no longer removable simply by centering the\nvisible 2D points. To avoid reintroducing t which would\ncomplicate derivations, we choose to follow the object cen-\ntric trick to absorb translation through adaptively normaliz-\ning S according to the visibility mask M [40]. The normal-\nized ˜S is computed as:\n˜S = S + S(IP −M)1P 1⊤\nP .\n(11)\nwith this, the projection equation remains bilinear, i.e. ˜\nW =\nRxy˜S, where ˜\nW denotes the centered W by the average of\nvisible 2D points. This allows to adapt PAUL to handle\nmissing data with minimal changes. The detailed descrip-\ntion is provided in the supp. material.\n5\nADL\nADL + low rank\nPAUL(ours)\nS1\nS5\nS64\nS70\nS123\n0\n5\n10\n15\nNormalized error (%)\nDeep NRSfM\nADL\nADL + Low rank\nPAUL (ours)\nFigure 5:\nComparison with auto-decoder baseline (i.e.\nADL), and low rank constraint (ADL + low-rank) on CMU\nmotion capture dataset. PAUL gives signiﬁcantly more ac-\ncurate reconstruction compared to ADL and low-rank. Red\nline visualizes the difference between reconstructed and\ngroundtruth points.\n5. Experiments\n5.1. Implementation details\nNetwork architecture.\nThroughout our experiment, we\nuse the same auto-encoder architecture across datasets\nexcept the bottleneck dimension.\nThe number of neu-\nral units in each layer is decreased exponentially, i.e.\n{256, 128, 64, 32, 16}.\nIdeally, if validation set with 3D\ngroundtruth is provided, we could select optimal architec-\nture based on cross validation. However, due to the unsu-\npervised setting, we rather set the hyperparameters heuris-\ntically.\nWe pick a smaller bottleneck dimension, i.e. 4\nfor smaller datasets (e.g. synthetic NRSfM benchmarks)\nor datasets with mostly rigid objects (e.g. Pascal3D+), and\npick a larger dimension, i.e. 8 for articulated objects such as\nhuman skeleton (H3.6M, CMU motion capture dataset) and\nmeshes (UP3D). The robustness of our method against vari-\nations in hyperparameter settings is investigated in Sec. 5.3.\nFor the 2D-3D encoder, we experiment with both fully\nconnected residual network [30] and convolutional net-\nwork [21]. The only modiﬁcation we make to those archi-\ntecture is the dimension of their output so as to match the\npicked bottleneck dimension.\nTraining details. We keep the same weightings for Lreg\nacross all experiments, i.e.\nLreg = 0.01∥ϕ∥2\n2 + 10−4∥θd∥2\n2.\n(12)\nWe use the Adam optimizer [18] for training. The optimiza-\ntion parameters are tuned according to speciﬁc datasets so\nas to guarantee convergence.\nEvaluation metric. We follow two commonly used evalu-\nation protocols:\n(i) MPJPE evaluates the mean per-joint position error. To\naccount for the inherent ambiguity from weak perspective\ncameras, we ﬂip the depth values of the reconstruction if it\nleads to lower error. To account for the ambiguity in the\nobject distance, we either subtract the average depth values\nor subtract the depth value of a root keypoint. The latter is\nused only for H3.6M dataset due to the evaluation conven-\ntion in literature.\n(ii) Normalized error (NE) evalutes the relative error by:\n∥Spred −SGT∥F /∥SGT∥F .\n5.2. Baselines\nAuto-decoder lifting (ADL). As discussed in Sec. 4.1, an\nalternative approach for unsupervised lifting is only mini-\nmizing the loss Lrecon. AD + Lreg, without the term Lrecon. AE\nfor the auto-encoder. Hence for this baseline, we are only\ntraining with respect to the decoder, thus regarded as an\nauto-decoder approach.\nADL + low rank. In addition, we experiment with adding\nthe low rank constraint as another baseline. Similar to Cha\net al. [8] and Park et al. [33], we evaluate the nuclear norm\nof the output of the shape decoder as the approximate low\nrank loss, i.e. ∥S∥∗. We empirically pick the weighting for\nthe low rank loss as 0.01.\n5.3. NRSfM experiments\nIn the ﬁrst set of experiments, we evaluate the proposed\nmethod for the NRSfM task, where we report how well\nthe compared methods are able to reconstruct a dataset.\nThe goal is to evaluate the robustness of the proposed Pro-\ncrustean auto-encoder shape prior across different shape\nvariations, without being convoluted by the inductive bias\nfrom a 2D-3D lifting network, which is not the interest of\nthis work. To achieve this, on short sequences, instead of\nconditioning ϕ with a 2D-3D encoder, we treat ϕ as free\nvariable to optimize directly; and on long sequences, we\nuse the same 2D-3D encoder as in Deep NRSfM [21] to\nhave a fair comparison.\nNRSfM datasets. We report performance on two types of\ndatasets: (i) short sequences with simple object motions,\ne.g. drink, pickup, yoga, stretch, dance, shark which are\nstandard benchmarks used in NRSfM literature [4, 37].\n(ii) long sequences with large articulated motions, i.e. CMU\nmotion capture dataset [1]. We use the processed data from\nKong & Lucey [21] which is intentionally made more chal-\nlenging by inserting large random camera motions.\nRobustness against bottleneck dimension. As shown in\nFig. 4, we run the methods with varying bottleneck dimen-\n6\nshort sequences\nlong sequences (random cam. motion)\ndrink\npickup\nyoga\nstretch\ndance\nshark\nS1\nS5\nS64\nS70\nS123\n#frames\n1102\n357\n307\n370\n264\n240\n45025\n13773\n11621\n10788\n10788\nCNS [28]\n3.04\n9.18\n11.15\n7.97\n7.59\n8.32\n37.62\n40.02\n29.00\n26.26\n26.46\nPND [27]\n0.37\n3.72\n1.40\n1.56\n14.54\n1.35\n-\n-\n-\n-\n-\nBMM [10]\n2.66\n17.31\n11.50\n10.34\n18.64\n23.11\n16.45\n14.07\n18.13\n18.91\n19.32\nBMM-v2 [24]\n1.19\n1.98\n1.29\n1.44\n10.60\n5.51\n-\n-\n-\n-\n-\nDeep NRSfM [21]\n17.38\n0.53\n12.54\n21.63\n20.95\n21.83\n10.74\n13.40\n4.38\n2.17\n2.23\nPAUL\n0.47\n2.03\n1.71\n1.62\n10.22\n0.37\n4.97\n4.38\n0.39\n0.77\n0.59\nTable 1: Comparison with state-of-the-art NRSfM methods on both short sequences and long sequences, report with normal-\nized error. Long sequences are sampled from CMU motion capture dataset [1] with large random camera motion. Atemporal\nmethods are highlighted by orange, methods using temporal information are marked by green. Due to the code for PND and\nBMM-v2 is unavailable, they are excluded from evaluation on CMU motion capture sequences.\naero.\ncar\ntv.\nsofa\nmotor.\ndining.\nchair\nbus\nbottle\nboat\nbicycle\ntrain\nMean\n8 cls.\nC3DPO\n6.56\n8.21\n15.03\n7.30\n7.48\n3.77\n3.46\n20.41\n7.48\n7.58\n3.47\n33.70\n10.4\n7.58\nDeep NRSfM++\n7.51\n9.22\n17.43\n9.37\n6.18\n12.90\n3.97\n18.02\n2.08\n9.18\n4.03\n23.67\n10.3\n8.90\nPAUL\n3.99\n7.13\n9.88\n3.99\n3.74\n5.70\n2.19\n14.11\n1.03\n8.08\n1.74\n38.78\n8.4\n5.32\nTable 2: Per-category normalized error (%) on Pascal3D+ dataset. Follow the protocol of Agudo et al. [2], we further report\nthe average error of 8 object categories which are annotated with ≥8 keypoints.\nUP3D 79KP\nPascal3D+\navg occlusion %\n61.89\n37.68\nEM-SfM [37]\n0.107\n131.0\nGbNRSfM [12]\n0.093\n184.6\nDeep NRSfM [21]\n0.076\n51.3\nC3DPO [32]\n0.067\n36.6\nDeep NRSfM++ [40]\n0.062\n34.8\nPAUL\n0.058\n30.9\nTable 3: Comparison on datasets with high per-\ncentage of missing data. Test accuracy is reported\nwith MPJPE.\nGT pts.\nSH pts. [32]\nPose-GAN [23]\n130.9\n173.2\nC3DPO [32]\n95.6\n153.0\nPRN [33]\n86.4\n124.5\nPAUL\n88.3\n132.5\nTable 4: MPJPE on H3.6M valida-\ntion set. orange indicates atemporal\nmethod and green indicates methods\nuse temporal information.\nNE (%)\nC3DPO [32]\n35.09\nPRN [33]\n13.77\nPAUL (ours)\n12.36\nPAUL (train set)\n4.30\nTable 5: Test accuracy\non SURREAL synthetic\nsequences. Training er-\nror is also reported for\nPAUL (bottom row).\nsion from 2 to 12 on different datasets. To account for the\nstochastic behavior due to network initialization and gradi-\nent descent on small datasets, we run the methods 10 times\nand visualize with average accuracy (solid lines) together\nwith standard deviation (colored area). PAUL gives stable\nresults once the bottleneck dimension is sufﬁciently large.\nThis indicates that PAUL is practical for unseen datasets by\nusing an overestimated bottleneck dimension.\nComparison with ADL and low rank.\nAs shown in\nFig. 5, on sequences from CMU motion capture dataset,\nADL achieves lower error in most sequences when com-\nparing against Deep NRSfM, indicating it is indeed a strong\nbaseline. Augmenting ADL with low rank constraint is able\nto further decrease error for several sequences, but the im-\nprovement is not consistent across the whole dataset. In\ncomparison, PAUL gives signiﬁcant error reduction for all\nthe evaluated sequences, which demonstrates the effective-\nness of the proposed Procrustean anto-encoder prior.\nComparison with state-of-the-art NRSfM methods. Ta-\nble 1 collects results from some of the state-of-the-art\nNRSfM methods on the synthetic bechmarks, e.g. BMM-\nv2 [24], CNS [28] and PND [27]. All the well-performing\nmethods utilize temporal information while PAUL does not,\nbut still achieves competitive accuracy on short sequences.\nOn long sequences from CMU motion capture dataset, the\naccuracy of temporal-based methods e.g. CNS deteriorates\nsigniﬁcantly due to the data perturbed by large random cam-\nera motion. Atemporal methods on the other hand gives sta-\nble results and PAUL outcompetes all the compared meth-\nods by a wide margin.\n5.4. 2D-3D lifting on unseen data\nWe compare against recent unsupervised 2D-3D lifting\nmethods on the processed datasets by Novotny et al. [32]:\nDatasets. (i) Synthetic UP-3D is a large synthetic dataset\nwith dense human keypoints collected from the UP-3D\n7\ndataset [26].\nThe 2D keypoints are generated by ortho-\ngraphic projection of the SMPL body shape with the visi-\nbility computed from a ray traccer. Similar to C3DPO, we\nreport result for 79 representative vertices of the SMPL on\nthe test set;\n(ii) Pascal3D+ [41] consists of images from 12 object cat-\negories with sparse keypoint annotations. The 3D keypoint\ngroundtruth are created by selecting and aligning CAD\nmodels. To ensure consistency between 2D keypoint and\n3D groundtruth, the orthographic projections of the aligned\n3D CAD models are used as 2D keypoint annotations, and\nthe visibility mask are taken from the original 2D annota-\ntions. For a fair comparison against C3DPO, we use the\nsame fully-connected residual network as the 2D-3D en-\ncoder, and train a single model to account for all 12 object\ncategories.\n(iii) Human 3.6 Million dataset (H3.6M) [17] is a large-\nscale human pose dataset annotated by motion capture sys-\ntems. Following the commonly used evaluation protocol,\nthe ﬁrst 5 human subjects (1, 5, 6, 7, 8) are used for train-\ning and 2 subjects (9, 11) for testing. The 2D keypoint an-\nnotations of H3.6M preserves perspective effect, thus is a\nrealistic dataset for evaluating the practical usage of 2D-3D\nlifting.\nRobustness against occlusion. Both synthetic UP3D and\nPascal3D+ dataset simulate realistic occlusions with high\nocclusion percentage.\nWe focus our comparison against\nC3DPO and Deep NRSfM++ [40] which is a recent up-\ndate of Deep NRSfM for better handling missing data and\nperspective projections. As shown in Table 3, PAUL signif-\nicantly outperforms both of them. To account for the dis-\ntortion caused by the object scale, we switch the evaluation\nmetric from MPJPE to normalized error in Table 2 and re-\nport per-class error. PAUL leads with even bigger margin.\nRobustness against labeling noise. To work with in-the-\nwild data, 2D-3D lifting methods are required to be robust\nagainst annotation noise, which could be simulated by us-\ning 2D keypoints detected by a pretrained keypoint detec-\ntor. In addition, 2D annotation with perspective effect could\nalso be regarded as noise since it is not modeled by the as-\nsumed weak perspective camera model. We evaluate both\nscenarios on H3.6M dataset (see Table 4). PAUL outper-\nforms the compared atemporal methods (i.e. C3DPO and\nDeep NRSfM++) and is competitive to recently proposed\nPRN [33] which requires training data to be sequential.\n5.5. Dense reconstruction\nWe follow the comparison in Park et al. [33] on the syn-\nthetic SURREAL dataset [38], which consists of 5k frames\nwith 6890 points for training, and 2,401 frames for test-\ning. Unlike PRN [33] which subsamples a subset of points\nwhen evaluating the low rank shape prior due to the intense\ncomputational cost of evaluating nuclear norm, our auto-\ngroundtruth\nC3DPO\nPAUL (ours)\nDeepNRSfM ++\nFigure 6: Qualitative comparison on Pascal3D+ dataset.\nRed lines visualize the difference between groundtruth\npoints and predicted points. PAUL shows more accurate\nprediction in the compared samples.\nencoder shape prior is computationally cheaper when deal-\ning with dense inputs, thus we made no modiﬁcation when\napplying PAUL to SURREAL. As shown in Table 5, PAUL\nachieves lower test error compared to PRN, even though we\nuse no temporal information in training. It is worth to point\nout that the current bottleneck in achieving better test accu-\nracy is at the generalization ability of the 2D-3D encoder\nnetwork, not at the proposed unsupervised training frame-\nwork. As shown in the last row of Table 5, the reconstruc-\ntion error on the training set is already much lower than the\ntest error (i.e. 4.30% vs 12.36%).\n6. Conclusion\nWe propose learning a Procrustean auto-encoder for\nunsupervised 2D-3D lifting capable of learning from no-\nsequential 2D observations with large shape variations. We\ndemonstrate that having an auto-encoder performs favor-\nably compared to an alternative auto-decoder approach. The\nproposed method achieves state-of-the-art accuracy across\nNRSfM and 2D-3D lifting tasks. For future work, theo-\nretical analysis of the characterization of the solution (e.g.\nuniqueness) may help inspire further development. Inter-\npreting the approach as learning manifold may also help\nprovide guidance such as setting hyperparameters [7]. Fi-\nnally, it is straightforward to extend the method to model\nperspective projection using similar extensions outlined\nin [40, 43].\nAcknowledgement This work was partially supported by\nthe National Science Foundation under Grant No.1925281.\n8\nI. Additional discussion of latent space in\nPAUL\nPAUL uses the constraint that complex shape variation\nis compressible into a lower dimensional latent space with\nan auto-encoder. The use of latent space in our problem\nis different to generative modeling in that: (i) we focus\non the compressibility instead of compactness of the rep-\nresentation, i.e. we do not require that any linear interpo-\nlation between two latent codes still corresponds to a valid\n3D shape. (ii) due to the shape coverage from short se-\nquences in NRSfM is sparse and arbitrary, we do not im-\npose any prior distribution such as Gaussian on the latent\nspace [42, 19]. However, these do not prevent us from sam-\npling the learned latent space, which can be achieved with\nan ex-post density estimation step as shown by Ghosh et\nal. [13].\nauto-encoder v.s.\ndecoder-only lifting.\nIn the main\npaper, we showed that our auto-encoder-based approach\n(PAUL) empirically outperforms the decoder-only base-\nline (i.e. ADL). The theoretical difference between the\ntwo is that auto-encoders additionally enforce the existence\nof a continuous mapping from the 3D shape to the low-\ndimensional latent space through learning the encoder net-\nwork. The implication of this additional constraint is that it\nencourages shapes with small variation to stay close in the\nlatent space. Consequently it improves the uniqueness of\nthe 2D-3D lifting solution, as it implicitly forces each 3D\nshape to have a unique latent representation.\nWe analyze the difference by visualizing the latent space\nfor both PAUL and ADL. We set the bottleneck dimen-\nsion as 2 for both methods, and plot the 2D latent code\nfor each sample in the input sequence (see Fig. 3). The\ncolor (from dark blue to bright yellow) for each point rep-\nresents its temporal order in the sequence. Since the motion\nis by nature smooth, temporarily close frames have smaller\n3D shape variation, thus ideally their corresponding latent\ncodes should be close to each other. This implies that a con-\ntinuous code trajectory is expected in the visualization.\nIn Fig. 3(a), we ﬁrst compare PAUL and ADL on short\nsequences with smooth camera trajectory. As expected, we\nobserve that the reconstructed 2D codes from PAUL in-\ndeed forms one or a small number of continuous trajecto-\nries. We can also observe recurrent motion from the loop in\nthe code trajectory on the drink sequence. In comparison,\nwithout the constraint from the encoder, ADL produces a\nset of more spread-out latent codes, which forms a number\nof shorter trajectories in a less interpretable spatial order.\nOne may argue that since a 2D-3D encoder is also a con-\ntinuous mapping, it should partially fullﬁll the role of an\nencoder to encourage smoothness of the latent space. How-\never, the counterargument to rely on the inductive-bias from\na 2D-3D encoder is – since 2D projection is a combination\nof 3D shape and camera pose, small variation in 3D shape\ndoes not necessarily leads to small variation in 2D. Hitherto,\nthe continuity of the output of a 2D-3D encoder conditioned\non 2D inputs does not translates to the continuity of codes\nwith respect to 3D shapes. To support this counterargument,\nwe conduct experiment on a sequence with random cam-\nera motion (see Fig. 3(b)) which means temporal adjacent\nframes would have very different 2D projections. ADL with\na 2D-3D encoder (adopted from Deep NRSfM [21]) loses\nthe trajectory-like structure in its latent space, which PAUL\npreserves.\nII. Details for handling missing data\nAs shown in Sec. 4.3, we use the extension proposed\nby Wang et al. [40] to remove translation and rewrite the\nprojection equation into a bilinear form:\n˜\nWM = Rxy˜SM,\n(13)\nwhere ˜\nW, ˜S denotes the adaptively normalized W, S ac-\ncording to the visibility mask M. With this new projec-\ntion equation, we extend the expression of Lrecon. AE and\nLrecon. AD to account for missing data.\nFirst, due to missing data, the unknown values in Scam\nare no longer only the depth z, but also includes x-y coor-\ndinates of the missed keypoints. Thus we introduce a new\nfree variable S′\ncam ∈R3×P to optimize instead of z, and\n˜Scam could be expressed as a fusion between unknown S′\ncam\nand known W:\n˜Scam = ˜S′\ncam(1 −M) +\n\u0014 ˜\nW\n0⊤\n\u0015\nM.\n(14)\nWe note that the above formulation are expressed in adap-\ntive normalized form as in (13).\nThen, the loss functions are extended to be:\nLrecon. AE =∥\n^\nfd ◦fe ◦fd(ϕ) −R⊤˜Scam∥F\nLrecon. AD =∥^\nfd(ϕ) −R⊤˜Scam∥F\n(15)\nwhere the operator ∼denotes the same adaptive normaliza-\ntion as deﬁned in (11).\nIII. Additional results\nRobustness to noise.\nWe investigate the robustness of\nPAUL against noise by perturbed the groundtruth 2D key-\npoints using Gaussian noise with different standard devia-\ntion. We further investigate the implication of using dif-\nferent bottleneck dimension when learning from noisy data.\nAs shown in Fig. 8, on CMU motion capture dataset, PAUL\nlearned with clean data produces stable reconstruction ac-\ncuracy with bottleneck dimension from 4-12. When noise\n9\n(a) drink\n(b) pickup\n(c) yoga\n(d) stretch\n(e) dance\n(f) shark\nFigure 7: Results on NRSfM synthetic short sequences. blue: points reconstructed by PAUL; red: groundtruth points. Note\nthe recovered points mostly overlaps with the groundtruth points, indicating the good performance of PAUL.\n2\n4\n6\n8\n10\n12\nBottleneck dim\n0.0\n0.5\n1.0\n1.5\n2.0\nNormalized error (%)\nclean\nnoise std: 0.01\nnoise std: 0.05\nnoise std: 0.11\nnoise std: 0.17\nFigure 8: Results on CMU motion capture dataset S70 with\ndifferent level of noise and bottleneck dimension.\nis inserted to the training data, PAUL still keeps similar per-\nformance when the bottleneck dimension is relatively small\n(e.g. 4-8). The accuracy decrease becomes more noticeable\nonly when the bottleneck dimension is large (e.g. 12). This\nindicates that PAUL overall is robust against random Gaus-\nsian noise, and a good practise to apply PAUL to real data is\nto start from smaller bottleneck dimension, which provides\nstronger constraint to denoise the reconstruction.\nReferences\n[1] CMU Motion Capture Dataset.\navailable at http://\nmocap.cs.cmu.edu/. 6, 7\n[2] Antonio Agudo, Melcior Pijoan, and Francesc Moreno-\nNoguer.\nImage collection pop-up: 3d reconstruction and\nclustering of rigid and non-rigid categories. In The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2018. 2, 7\n[3] Ijaz Akhter, Yaser Sheikh, and Sohaib Khan. In defense of\northonormality constraints for nonrigid structure from mo-\ntion. In 2009 IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 1534–1541. IEEE, 2009. 1, 2\n[4] Ijaz Akhter, Yaser Sheikh, Sohaib Khan, and Takeo Kanade.\nNonrigid structure from motion in trajectory space. In Ad-\nvances in neural information processing systems, pages 41–\n48, 2009. 6\n[5] Adam W Bojanczyk and Adam Lutoborski. The procrustes\nproblem for orthogonal stiefel matrices. SIAM Journal on\nScientiﬁc Computing, 21(4):1291–1304, 1999. 4\n[6] Christoph Bregler. Recovering non-rigid 3d shape from im-\nage streams. Citeseer. 1, 2\n[7] Francesco Camastra and Antonino Staiano. Intrinsic dimen-\nsion estimation: Advances and open problems. Information\nSciences, 328:26–41, 2016. 8\n[8] Geonho Cha, Minsik Lee, and Songhwai Oh. Unsupervised\n3d reconstruction networks.\nIn Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 3849–\n3858, 2019. 1, 2, 6\n[9] Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dy-\nlan Drover, Rohith MV, Stefan Stojanov, and James M.\nRehg. Unsupervised 3d pose estimation with geometric self-\nsupervision. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2019. 2\n[10] Yuchao Dai, Hongdong Li, and Mingyi He. A simple prior-\nfree method for non-rigid structure-from-motion factoriza-\ntion. International Journal of Computer Vision, 107(2):101–\n122, 2014. 1, 2, 7\n[11] Dylan Drover,\nRohith MV, Ching-Hang Chen,\nAmit\nAgrawal, Ambrish Tyagi, and Cong Phuoc Huynh. Can 3d\npose be learned from 2d projections alone?\nIn The Euro-\npean Conference on Computer Vision (ECCV) Workshops,\nSeptember 2018. 2\n[12] Katerina Fragkiadaki, Marta Salas, Pablo Arbelaez, and Ji-\ntendra Malik. Grouping-based low-rank trajectory comple-\ntion and 3d reconstruction. In Advances in Neural Informa-\ntion Processing Systems, pages 55–63, 2014. 2, 7\n[13] P. Ghosh, M. S. M. Sajjadi, A. Vergari, M. J. Black, and B.\nSch¨olkopf. From variational to deterministic autoencoders.\nIn 8th International Conference on Learning Representa-\ntions (ICLR), Apr. 2020. 3, 9\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances\nin neural information processing systems, pages 2672–2680,\n2014. 2\n[15] Stephen Gould, Richard Hartley, and Dylan Campbell.\nDeep declarative networks: A new hope.\narXiv preprint\narXiv:1909.04866, 2019. 5\n[16] John C Gower, Garmt B Dijksterhuis, et al. Procrustes prob-\nlems, volume 30. Oxford University Press on Demand, 2004.\n4\n10\n[17] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3. 6m: Large scale datasets and pre-\ndictive methods for 3d human sensing in natural environ-\nments. IEEE transactions on pattern analysis and machine\nintelligence, 36(7):1325–1339, 2013. 8\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[19] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. stat, 1050:1, 2014. 3, 9\n[20] Chen Kong and Simon Lucey. Prior-less compressible struc-\nture from motion. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 4123–\n4131, 2016. 2\n[21] Chen Kong and Simon Lucey. Deep non-rigid structure from\nmotion. In The IEEE International Conference on Computer\nVision (ICCV), October 2019. 1, 2, 3, 4, 6, 7, 9\n[22] Chen Kong, Rui Zhu, Hamed Kiani, and Simon Lucey.\nStructure from category: A generic and prior-less approach.\nIn 2016 Fourth International Conference on 3D Vision\n(3DV), pages 296–304. IEEE, 2016. 2\n[23] Yasunori Kudo,\nKeisuke Ogaki,\nYusuke Matsui,\nand\nYuri Odagiri.\nUnsupervised adversarial learning of 3d\nhuman pose from 2d joint locations.\narXiv preprint\narXiv:1803.08244, 2018. 2, 7\n[24] Suryansh Kumar. Non-rigid structure from motion: Prior-\nfree factorization method revisited. In Winter Conference on\nApplications of Computer Vision (WACV 2020), 2020. 2, 7\n[25] Suryansh Kumar, Yuchao Dai, and Hongdong Li.\nMulti-\nbody non-rigid structure-from-motion. In 2016 Fourth In-\nternational Conference on 3D Vision (3DV), pages 148–156.\nIEEE, 2016. 1, 2\n[26] Christoph Lassner, Javier Romero, Martin Kiefel, Federica\nBogo, Michael J. Black, and Peter V. Gehler. Unite the peo-\nple: Closing the loop between 3d and 2d human representa-\ntions. In IEEE Conf. on Computer Vision and Pattern Recog-\nnition (CVPR), July 2017. 8\n[27] Minsik Lee, Jungchan Cho, Chong-Ho Choi, and Songhwai\nOh. Procrustean normal distribution for non-rigid structure\nfrom motion.\nIn Proceedings of the IEEE Conference on\ncomputer vision and pattern recognition, pages 1280–1287,\n2013. 2, 7\n[28] Minsik Lee, Jungchan Cho, and Songhwai Oh. Consensus of\nnon-rigid reconstructions. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n4670–4678, 2016. 7\n[29] Minsik Lee, Chong-Ho Choi, and Songhwai Oh.\nA pro-\ncrustean markov process for non-rigid structure recovery. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2014. 2\n[30] Julieta Martinez, Rayat Hossain, Javier Romero, and James J\nLittle. A simple yet effective baseline for 3d human pose es-\ntimation. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 2640–2649, 2017. 3, 6\n[31] Ab Mooijaart and Jacques JF Commandeur. A general solu-\ntion of the weighted orthonormal procrustes problem. Psy-\nchometrika, 55(4):657–663, 1990. 4\n[32] David Novotny, Nikhila Ravi, Benjamin Graham, Natalia\nNeverova, and Andrea Vedaldi. C3dpo: Canonical 3d pose\nnetworks for non-rigid structure from motion. In The IEEE\nInternational Conference on Computer Vision (ICCV), Octo-\nber 2019. 1, 2, 3, 7\n[33] Sungheon Park, Minsik Lee, and Nojun Kwak. Procrustean\nregression networks: Learning 3d structure of non-rigid ob-\njects from 2d annotations. In European Conference on Com-\nputer Vision, pages 1–18. Springer, 2020. 1, 2, 6, 7, 8\n[34] Vikramjit Sidhu, Edgar Tretschk, Vladislav Golyanik, Anto-\nnio Agudo, and Christian Theobalt. Neural dense non-rigid\nstructure from motion with latent space constraints. In Eu-\nropean Conference on Computer Vision (ECCV), 2020. 1,\n4\n[35] Carsten Steger.\nAlgorithms for the orthographic-n-point\nproblem.\nJournal of Mathematical Imaging and Vision,\n60(2):246–266, 2018. 4, 5\n[36] Carlo Tomasi and Takeo Kanade. Shape and motion from im-\nage streams under orthography: a factorization method. In-\nternational journal of computer vision, 9(2):137–154, 1992.\n2\n[37] Lorenzo Torresani, Aaron Hertzmann, and Chris Bregler.\nNonrigid structure-from-motion: Estimating shape and mo-\ntion with hierarchical priors. IEEE transactions on pattern\nanalysis and machine intelligence, 30(5):878–892, 2008. 6,\n7\n[38] G¨ul Varol, Javier Romero, Xavier Martin, Naureen Mah-\nmood, Michael J. Black, Ivan Laptev, and Cordelia Schmid.\nLearning from synthetic humans. In CVPR, 2017. 8\n[39] Bastian Wandt and Bodo Rosenhahn. Repnet: Weakly su-\npervised training of an adversarial reprojection network for\n3d human pose estimation. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n7782–7791, 2019. 2, 4\n[40] Chaoyang Wang, Chen-Hsuan Lin, and Simon Lucey. Deep\nnrsfm++: Towards 3d reconstruction in the wild.\narXiv\npreprint arXiv:2001.10090, 2020. 2, 5, 7, 8, 9\n[41] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond\npascal: A benchmark for 3d object detection in the wild. In\nIEEE Winter Conference on Applications of Computer Vi-\nsion, pages 75–82. IEEE, 2014. 8\n[42] Amir Zadeh, Yao-Chong Lim, Paul Pu Liang, and Louis-\nPhilippe Morency. Variational auto-decoder. arXiv preprint\narXiv:1903.00840, 2019. 9\n[43] Yinqiang Zheng, Yubin Kuang, Shigeki Sugimoto, Kalle As-\ntrom, and Masatoshi Okutomi. Revisiting the pnp problem:\nA fast, general and optimal solution. In Proceedings of the\nIEEE International Conference on Computer Vision, pages\n2344–2351, 2013. 8\n[44] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kon-\nstantinos G Derpanis, and Kostas Daniilidis.\nSparseness\nmeets deepness: 3d human pose estimation from monocular\nvideo. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4966–4975, 2016. 2\n[45] Yingying Zhu, Dong Huang, Fernando De La Torre, and Si-\nmon Lucey. Complex non-rigid motion 3d reconstruction by\nunion of subspaces. In Proceedings of the IEEE Conference\n11\non Computer Vision and Pattern Recognition, pages 1542–\n1549, 2014. 1, 2\n12\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-03-31",
  "updated": "2021-03-31"
}