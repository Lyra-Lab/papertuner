{
  "id": "http://arxiv.org/abs/2104.00106v1",
  "title": "Zero-Shot Language Transfer vs Iterative Back Translation for Unsupervised Machine Translation",
  "authors": [
    "Aviral Joshi",
    "Chengzhi Huang",
    "Har Simrat Singh"
  ],
  "abstract": "This work focuses on comparing different solutions for machine translation on\nlow resource language pairs, namely, with zero-shot transfer learning and\nunsupervised machine translation. We discuss how the data size affects the\nperformance of both unsupervised MT and transfer learning. Additionally we also\nlook at how the domain of the data affects the result of unsupervised MT. The\ncode to all the experiments performed in this project are accessible on Github.",
  "text": "Zero-Shot Language Transfer vs Iterative Back Translation for\nUnsupervised Machine Translation\nAviral Joshi, Chengzhi Huang, Har Simrat Singh\n(aviralj, chengzhh, harsimrs)@andrew.cmu.edu\nAbstract\nThis work focuses on comparing differ-\nent solutions for machine translation on\nlow resource language pairs, namely, with\nzero-shot transfer learning and unsuper-\nvised machine translation.\nWe discuss\nhow the data size affects the performance\nof both unsupervised MT and transfer\nlearning. Additionally we also look at how\nthe domain of the data affects the result of\nunsupervised MT. The code to all the ex-\nperiments performed in this project are ac-\ncessible on Github.\n1\nIntroduction\nUnsupervised Machine translation(UMT) is the\ntask of translating sentences from a source lan-\nguage to a target language without the help of par-\nallel data for training. UMT is especially useful\nwhen translating to and from low resource lan-\nguages for example English to Nepali or vice-\nversa. Since it is assumed that there is no access to\nparallel data between two languages, unsupervised\nMT techniques focus on leveraging monolingual\ndata to learn a translation model. More generally\nthere are three data availability scenarios that can\ndictate the choice of the training procedure and the\ntranslation model to be used. These scenarios are\nas follows:\n1. We have no parallel data for X-Z but have a\nlarge monolingual corpus. Unsupervised Ma-\nchine translation is very well suited for this\nscenario.\n2. We do not have parallel data for X-Z but\nwe have parallel data for Y-Z and a good\namount of monolingual data. Zero-Shot lan-\nguage transfer can be more effective in this\nscenario.\n3. Finally, we have some parallel data for\nX-Z and monolingual data for X and Z.\nIn this scenario semi-supervised approaches\nthat leverage both monolingual and parallel\ndata can outperform the 2 other approaches\nmentioned.\nHere X represents the source language (the lan-\nguage to transfer from), Z represents the target\nlanguage (the language to transfer to) and Y repre-\nsents a pivot language (an intermediate language)\nwhich is similar to the target language Z. In a low\nresource scenario there is little to no parallel data\nbetween X and Z.\nThe ﬁrst 2 data availability scenarios mentioned\nabove are generally more challenging due to lack\nof presence of parallel data between source and\ntarget languages and are explored in greater de-\ntail in this work. Futhermore, our report also dis-\ncuss the impact of mismatch in dataset domains\nfor training and evaluation and its effect on trans-\nlation. Our report is structured as follows. We\nﬁrst introduce related work in the domain of unsu-\npervised machine translation with back translation\nand language transfer while also discussing works\nthat detail the impact of domain mismatch in train-\ning and evaluation datasets for UMT in Section 2.\nAfter which we describe the experimental setup in\nSection 3. Moving on we describe the results of\nour experiments on low-resource language pairs\nwith varying training data sizes and also analyze\nthe impact of out of domain datasets on the qual-\nity of translation in Section 4. Finally, we con-\nclude with possible directions for future work in\nthe domain of UMT for low-resource languages.\n2\nRelated Work\n2.1\nUnsupervised Machine Translation Using\nMonolingual Data\nRecent advancement in Machine Translation can\nbe attributed to the development of training tech-\narXiv:2104.00106v1  [cs.CL]  31 Mar 2021\nniques that allow deep learning models to uti-\nlize large-scale monolingual data for improving\nperformance on translation while reducing de-\npendency on massive amounts of parallel data.\nOf the several attempts to leverage monolin-\ngual data three techniques – back-translation, lan-\nguage modelling and Auto-Encoding – in particu-\nlar standout as the most effective and widely ac-\ncepted strategies to do so.\nSennrich et. al. [20] ﬁrst introduced the idea\nof iteratively improving the quality of a machine\ntranslation model in a semi-supervised setting by\nusing “back-translation”, the idea revolved around\ntraining an initial sub-optimal translation system\nwith the help of available parallel data and then\nutilizing it to translate many sentences from a large\nmonolingual corpus on the target side to the source\nlanguage to generate augmented parallel-data for\ntraining the original translation system.\nGulcehre et.\nal [6] demonstrated an effec-\ntive strategy to integrate language modelling as\nan auxiliary task with Machine Translation im-\nproving the performance on translation quality be-\ntween both high resource languages and low re-\nsource language pairs.\nA more recent work by\nLample. et. al [14] introduced a Cross Lingual\nModel(XLM) that utilized a Cross Lingual Lan-\nguage modelling pretraining objective to improve\nperformance on translation.\nPrevious works have also looked at the task\nof translation from the perspective of an Auto-\nEncoder to utilize monolingual data. Cheng et.\nal.[3] introduced an approach which utilized an\nAuto-Encoder to reconstruct sentences in mono-\nlingual data where the encoder translates input\nfrom source to target language and the decoder\nworks to translate sentences back to the source lan-\nguage. Parallel data is utilized in conjunction to\nmonolingual data to learn the translation system.\nThis method was shown to outperform the origi-\nnal back-translation system proposed by Sennrich\net. al.\nUnlike\nthe\npreviously\ndescribed\nsemi-\nsupervised Lample et.\nal.\n[15] introduce an\neffective strategy to perform completely unsu-\npervised machine translation in the presence\nof a large monolingual corpus.\nTheir work\nleverages a Denoising Auto-Encoder (DAE) to\nreconstruct denoised sentences from the sentences\nin monolingual corpora which are systematically\ncorrupted to train encoder and decoders for both\nsource and target language.\nTo map the latent\nDAE representations of the two languages into\nthe same space.\nTo aid translation the authors\nalso introduce an additional adversarial loss term.\nThe DAE along with iterative back-translation\nis utilized to train a completely unsupervised\ntranslation system.\n2.2\nTransfer Learning - Zero shot\nTranslation\nEven though UMT has shown to perform well on\nsimilar language pairs with good amount of mono-\nlingual data, Artetxe et al. argued a scenario with-\nout any parallel data and abundant monolingual\ndata is unrealistic in practice. One common alter-\nnative in this scenario is to avoid using monolin-\ngual data with zero-shot transfer learning. Zero-\nshot translation concerns the case where direct\n(src-tgt) parallel data is lacking but there is paral-\nlel data between the target language and a similar\nlanguage to source language.\nTransfer Learning is ﬁrstly proposed for NMT\nby Zoph et al., who leverage a high resourced\nparent model to initialize the low-resourced child\nmodel. Nguyen and Chiang and Kocmi and Bo-\njar utilizes shared vocabularies for source and tar-\nget language to improve transfer learning, in ad-\ndition, mitigate the vocabulary mismatch by us-\ning cross-lingual word embedding. These meth-\nods show great performance in transfer learning\nfor low resource languages, they show limited ef-\nfects on zero-shot transfer learning.\nMultilinguality has been extensively studied\nand has been applied to the related problem of\nzero-shot translation (Johnson et al.; Firat et al.;\nArivazhagan et al.). Liu et al. showed some initial\nresults on multilingual unsupervised translation\nin the low-resource setting. Namely, Pretrained\nmBART25 is ﬁnetuned on transfer language to tar-\nget language pair, and is directly to the source to\ntarget pairs. mBART obtained up to 34.1 (Nl-En)\nwith zero-shot translation and works well when\nﬁne-tuning is also conducted in the same language\nfamily. Speciﬁcally, it acheives 23 BLEU points\nfor Ro-En using Cs as the transfer language, and\n18.9 BLUE points for Ne-En using Hi as the trans-\nfer language.\n2.3\nUnsupervised Machine Translation on\nOut-of-Domain Datasets\nOwing to increasing popularity of unsupervised\nMT and the techniques involved, adaptation of\nUMT to translate data from domains where a large\namount of parallel data is not obtainable is also\ngaining importance. One of the main issues as-\nsociated with this is the feasibility of using pre-\ntrained models.\nMost of the pre-trained mod-\nels, even though trained under supervised learn-\ning paradigm, fail to achieve signiﬁcant transla-\ntion accuracy on a domain which was non-existent\nin the training data. Unsupervised models also fol-\nlow the same pattern. Marchisio et al. [18] show\nsome signiﬁcant results and comment on the do-\nmain similarity of test and training data in State of\nthe Art models. The authors use United Nations\nParallel Corpus (UN) [22], News Crawl and Com-\nmonCrawl datasets for their work. The model was\ntrained solely on NewsCrawl while being evalu-\nated on all 3 datasets. A general trend of reduc-\ntion in performance was observed across various\nlanguage pairs.\nExtensive work has been done\nto address this problem. Dou et al. [4] propose\na Domain-Aware feature embeddings based ap-\nproach and achieve improvements over previous\nexisting methods. Luong and Manning [17] adopt\nan attention based approach to ﬁne tune a pre-\ntrained model on out of domain data. Sennrich\net al. [20] show that for out of domain data back\ntranslation helps the models learn a whole lot of\nwords previously non existent in the vocabulary\nwhich could be the reason why out of domain back\ntranslation based UMT fails. Our experiments fol-\nlow a similar line of work and show that ﬁne tun-\ning a pre-trained translation model does not effec-\ntively help with out of domain data.\n3\nExperiments\n3.1\nProposed Experiments\nIn our experiments, we focus on how the size of\ntraining data will affect the performance of both\nzero-shot transfer learning and Unsupervised Ma-\nchine Translation with iterative back translation.\nAdditionally, we conduct an extensive empirical\nevaluation of unsupervised machine translation us-\ning dissimilar domains for monolingual training\ndata between source and target languages and with\ndiverse datasets between training data and test\ndata.\n1. Unsupervised MT using iterative back\ntranslation: Our ﬁrst experiment involves\nUMT using back translation(BT) with the\nXLM model. Here we demonstrate the fea-\nsibility of UMT with BT for low-resource\nlanguage pairs and also how variation in the\nmonolingual data size effects the translation\nquality for similar and dissimilar language\npairs.\n2. Zero Shot Language Transfer: Our sec-\nond experiement involves zero-shot transla-\ntion using mBART model. We ﬁnetune on re-\nlated language (X →En) and apply it directly\nto the source-target (Y →En) language pair.\nHere we illustrate how the size of ﬁnetun-\ning data affect both ﬁnetuning scores (ﬁne-\ntuned on X →En and tested on X →En) and\ntransferring scores (ﬁnetuned on X →En and\ntested on Y →En).\n3. Domain Dissimilarity Our third experiment\nanalyses the performance of a model which is\ntrained on a different dataset and is tested on\nsentences from an entirely different domain.\nWe use XML model and perform back trans-\nlation on pre-trained models for all language\nparis X →En to ﬁne tune it on the required\ndataset. Back translation steps include X →\nEn and En →X. The model is then tested for\nthe previously mentioned steps on a dataset\nsourced from a completely different domain.\n3.2\nModels\nmBART [16] is used for experiments with zero-\nshot transfer learning, XLM [14] is used for ex-\nperiments with unsupervised machine translation.\n1. mBART is the ﬁrst method for pre-training a\ncomplete sequence to sequence model by de-\nnoising full texts in multiple languages. Pre-\ntraining is done on CC25 data set and pre-\ntrained model can be directly ﬁne-tuned for\nboth supervised and unsupervised MT. It also\nenables a smooth zero-shot transfer to lan-\nguage pairs with no bi-text data. In our ex-\nperiments, we use mBART25, which is pre-\ntrained on all 25 lagnauges. Ideally, for a fair\ncomparison unsupervised BT should be also\ndone using mBART25, however, the authors\ndid not provide their implementation for BT\nand so we shift to the XLM model for exper-\niments with BT since its implementation was\nmore accessible.\n2. XLM is a cross lingual language model\ndeveloped for both supervised and unsu-\npervised machine translation tasks.\nXLM\nmodel uses shared sub word vocabulary and\nis trained on 3 different language mod-\nelling objectives of Causal language model-\ning, Masked Language modeling and Trans-\nlation language modeling. The model is es-\nsentially Transformer based with 1024 sub\nunits and 8 heads.\nCross lingual XLM is\ntrained on 15 languages and is shown to\nachive SoTA on all the different languages.\nOur motivation behind using XLM was that a\npretrained XLM model was available for the\nlanguages of our choice. It supported 2 way\nbacktranslation and was comparatively easier\nto train and ﬁne tune than most other multi-\nlingual models.\n3.3\nDataset\nTwo language pairs were chosen for the experi-\nments where we conduct an investigation of the\nimpact of size of monolingual training data on\ntranslation quality between a similar and dissimi-\nlar language pair (Ro (Romanian) - En (English)\nand Ne (Nepali) - En respectively.\nFor Ro &\nEn, data from wikipedia was used. We perform\nrandom sampling to obtain monolingual sentences\nfrom the monolingual data provided by the authors\nof the XLM model (This was originally obtained\nby scraping articles from Wikipedia). For Nepali\nthe monolingual data was obtained from the Flores\ndata corpus.\nFor zero-shot transfer learning, we conducted\nexperiments on Ro to En with Cs (Czech) be-\ning the transfer language and Ne to En with Hi\n(Hindi) being the transfer language. Cs - En par-\nallel training data is taken from Europarl v7 [11]\nwith 668,595 sentence pairs, Ro - En test data is\ntaken from WMT16 test set for Romanian and En-\nglish, consisting of 1999 sentence pairs; Hi-En\nparallel training data is taken from IITB Bombay\nEnglish-Hindi corpus[13] with 1,609,682 sentence\npairs, Ne-En test data is taken from [7] with 2835\ntest sentences.\nTo analyse the effect of domain mismatch train-\ning and testing, we use Fr (French), Genrman\n(De) and Romanian (Ro). In all the experiments\nthe target language is English (En). Back trans-\nlation is performed both ways.\nThe choice for\nthese speciﬁc languages was motivated by the cri-\nterion on language similarity and availability of\ndata. French and German are related to English,\nalbeit not closely but still possess a similarity in\nterms of language family and closeness of vo-\ncabulary.\nRomanian was chosen as a low re-\nsource language, distantly related to English. To\nsimulate domain mismatch, the XLM model was\npretrained on WMT’16 which mostly has paral-\nlel data extracted from news articles. The model\nwas then ﬁne tuned on monolingual data of the\nfour languages obtained from the Tatoeba chal-\nlenge [21]. For French, English and German 50K\nmonolingual sentences were used, and for Roma-\nnian only 1999 sentences were available. Tatoeba\ndataset has sentences with minimum overlap with\nany other available machine translation dataset.\nWe used the sentences extracted from Wikipedia\npages present within the dataset. The validation\nand testing set was extracted from WMT’16.\n3.3.1\nExperiment Details\nWe use the following tokenizers for both prepro-\ncessing training data and evaluations for all three\nexperiments:\n• Ne, Hi:\nWe use Indic-NLP Library to\ntokenize both Indic language inputs and\noutputs.[12]\n• Ro, Cs, En: We apply Moses tokenization\nand special normalization for English, Czech\nand Romanian texts.\nWe conduct all the experiments for trans-\nfer learning on AWS g4dn.xlarge instance,\nwith max-tokens 512, total-num-update\n80000 and dropout 0.3. During the inference time,\nbeam size is set to 5.\nSimilar to experiments with transfer learning all\nexperiments for Back Translation were conducted\non a g4dn.xlarge instance the batch size was set to\n32 and multiple iterations of back translation were\nrun. The number of tokens per batch were set to\na maximum of 1500 and a embedding dimension\nof 1024 was used. Other hyper-parameters were\nkept the same as the implementation provided by\nthe authors of XLM.\nFor domain mismatch experiment we use XLM\nmodel as well. We follow the Byte Pair Encod-\ning technique as used by the authors of XLM, and\nuse fastBPE for the same. Tokenization is done\nusing Moses tokenization. The experiments were\nconducted on AWS p3.2xlarge instances. The\nmodel used 6 layers of transformers with 8 heads.\nThe embedding dimension was 1024.\n3.4\nEvaluation Metrics\nFor all of our tasks, we use BLEU score calcu-\nlated by sacrebleu as the automatic metric to\nevaluate the translation performance. Speciﬁcally,\nwe compute the BLEU scores over tokenized text\nfor both hypothesis outputs and references as men-\ntioned in Liu et al.\n4\nResults and Discussions\nIn this section we describe the results of the pro-\nposed experiments as mentioned previously in\nSection 3 and discuss our ﬁnding to illustrate the\nthe impact of dataset size and dataset domain for\nUnsupervised Machine Translation and data size\nfor Transfer Learning.\n4.1\nBack Translation\nAs expected with most deep learning approaches,\nthe trained model scales with increase in train-\ning data (monolingual) as the BLEU show a gen-\neral upward trend as more data is made accessi-\nble to the XLM model. This is evident in the re-\nsults shown in Table 1 especially for the high sim-\nilarity English-Romanian language pair. Whereas,\nthe performance on low similarity English-Nepali\npair is degenerate in all data availability scenarios.\nEven when the entire training corpus is provided\nto the model. This result indicates that it might not\nbe feasible to perform unsupervised MT when lan-\nguage pairs have little or no similarity using Back\nTranslation along. In which case other approaches\nfor Unsupervised MT such as language transfer\nare better suited. More about language transfer is\ndiscussed in Section 4.2.\nA big improvement in BLEU score is seen\nwhen training data size is increased from 10,000\nto 100,000 for the high similarity language pair.\nThe performance increases steadily as more data\nis provided to the model while the rate of in-\ncrease slows down. A reasonable BLEU for the\nEnglish-Nepali language pair was reached with\njust 600,000 sentences from each language.\n4.2\nTransfer Learning\nZero shot transfer learning for mBART [16] works\nas following: pretrained mBART is ﬁnetuned on\na related language Y to target En and is directly\napplied for testing on source X to target language\nEn.\nIn order to investigate on how the size of train-\ning data will affect the zero-shot BLEU points, we\nBLEU - Back Translation\nData Size\nHigh Similarity\nLow Similarity\nen\nro\nen\nne\n→\n←\n→\n←\n10K\n2.76\n3.93\n0.0\n0.0\n100K\n23.6\n23.6\n0.0\n0.0\n300K\n27.7\n26.9\n0.1\n0.1\n600K\n28.8\n28.3\n0.1\n0.2\n>10M\n33.3\n31.8\n0.1\n0.5\nTable 1: Comparison of Translation Quality be-\ntween Similar and Dissimilar language pairs and\nhow they scale with increase in monolingual data.\nrandomly sample 1K, 10K, 100K and 300K sen-\ntences from Europart v7 [11] of Cs - En parallel\ntraining corpus and test the ﬁnetuning model di-\nrectly on Ro - En test data; We also randomly sam-\nple 10K, 100K, 300K and 600K sentences from\nITTB Bombay-English corpus [13] and test the\nﬁnetuning model directly on Ne - En test data. We\npresent BLEU points of ﬁnetuning models on both\nsource-target language pairs (X - En) and related-\ntarget language pairs (Y - En) for better observing\nthe correlation between them. Each experiment is\ntrained using the same hyper-parameters.\nData Size\nTransfer\nRo →En\nFinetune\nCs →En\n0\n0.0\n0.0\n1K\n17.6\n16.2\n10K\n23.3\n21.9\n100K\n23.0\n25.7\n300K\n20.3\n26.5\n668K\n14.5\n27.5\nTable 2:\nBLEU points for Transfer Learning\nand directly ﬁnetuning of different data size, pre-\ntrained mBART25 is ﬁnetuned on Cs to En and di-\nrectly transferred to Ro to En, we choose the best\ncheckpoint weights validated on Cs to En dev set\nIt is worth noting that for the model ﬁnetuned\non Cs - En, we observed a generally negative cor-\nrelation between transferring scores and ﬁnetun-\ning scores, as well as a negative correlation be-\ntween the data size and transferring scores. In con-\ntrast, for the model ﬁnetuned on Hi - En there is\na positive correlation between transferring scores\nand ﬁnetuning scores and a positive correlation be-\ntween the data size and transferring scores. One\nhypothesis is that even though mBART achieves\n(a)\n(b)\nFigure 1: Figures depicting the variation of translation quality with the availability of monolingual data\nfor English and Romanian\nData Size\nTransfer\nNe →En\nFinetune\nHi →En\n0\n0.0\n0.0\n10K\n11.7\n12.8\n100K\n14.0\n18.7\n300K\n13.6\n19.1\n600K\n13.2\n19.3\n1.68M\n15.1\n23.4\nTable 3:\nBLEU points for Transfer Learning\nand directly ﬁnetuning of different data size, pre-\ntrained mBART25 is ﬁnetuned on Hi to En and di-\nrectly transferred to Ne to En, we choose the best\ncheckpoint weights validated on Hi to En dev set\nthe highest transferring scores for Ro - En when\nﬁnetuned on Cs - En and they both belong to Indo-\nEuropean language family, they are still vastly dif-\nferent languages using orthographic distance met-\nric Heeringa et al. developed, with Czech be-\ning in the Slavic Language sub-group and Roma-\nnian being in the Romance language sub-group.\nHence, ﬁnetuning on a larger data set would make\nmBART overﬁt towards the ﬁnetuned language,\nthus hurts the performance of zero-shot translation\non source-target pair due to the limited capacity\nof mBART model itself. For Ne and Hi are more\nclosely related to each other and have large over-\nlap in their vocabularies, so ﬁnetuning on Hi will\ngreatly beneﬁt the performance of zero-shot trans-\nlation.\n4.3\nComparison between Back Translation\nwith Zero-Shot Transfer Learning\nWhen does back translation work?\nIn Table\n3, we do not have much increase in performance\nwhen we increase the data size for monolingual\ndata when training on dissimilar language pairs\n(Ne and En). If there exists high quality similar\nlanguages bi-text data, language transfer is able to\nbeat the conventional methods with BT. For sim-\nilar language pairs (Ro and En), we do see the\ngain in BLEU points when data size increases and\ncan easily beat the zero-shot transfer learning with\n100K monolingual data.\nWhen does transfer learning work? Note that\nin Table 2, the performance for zero-shot trans-\nfer learning suffers from ”the curse of data size”,\nwhich means simply increasing the size of ﬁne-\ntuning data does not contribute to the performance\nof transferring scores, especially when ﬁnetuning\nlanguage (Cs) is not close enough to the target lan-\nguage (Ro). Romanian is closest to Catalan [8],\nhowever, not so much bi-text exists between Cata-\nlan and English and it is also a out of vocabu-\nlary language for mBART, thus does not beneﬁt\nmuch from multilingual pretraining. On the con-\ntrary, for languages in Indic Language family, in-\ncrease in data size does help in both transferring\nscores and direct ﬁnetuning scores.\nCompared\nwith Back Translation, zero-shot transfer learning\noutperforms it with just a small amount of bi-text\nfor Hi to En.\nLanguage Pair\nTraining Dataset\nWMT’16\nTatoeba(Wikipedia)\nFr →En\n32.8\n27.38\nDe →En\n34.3\n27.44\nRo →En\n18.3\n1.74\nEn →Fr\n33.4\n28.33\nEn →De\n26.4\n21.93\nEn →Ro\n18.9\n0.39\nTable 4: Domain Mismatch\n4.4\nDomain Dissimilarity\nWe present the results of domain mismatch exper-\niments in table 4. As expected there is a consistent\ndecrease in the performance of the model across\nall languages. The decrease is comparatively less\nfor Fr and De then Ro. French and German are\nborh high resource languages and the XLM model\nfor both these languages was pre trained as well\nas ﬁnetuned on 50K sentences. Whereas for Ro-\nmanian only 1999 sentences were available in the\nTatoeba challenge. The size of the validation and\ntest data was also smaller.\nUsing back transla-\ntion in this scenario did not prove to be ideal.\nThis also suggests that even for high resource lan-\nguages the domain of the dataset is a big factor\nto determine the overall translation efﬁcacy of the\nmodel.\nXLM uses shared sub word vocabulary\nwhich means there would be a lot of words which\nare differently used in different domains and are\nthus harder for the model to predict. The results\nfor Romanian could have been better if transfer\nlearning was used as the UMT technique.\nThe\nresults are particularly interesting as for low re-\nsource languages like Romanian, getting parallel\ndomain speciﬁc data is very tough and adapting a\npre-trained model does not help in the same way\nas it does in cases of high resource languages. We\nwish to explore this further with some extra exper-\niments as well.\n5\nConclusion and Future-Work\nIn this work, we compare two different solutions\nfor low resource language MT: unsupervised MT\nwith BT and zero-shot transfer learning. We focus\non how the size affects the performance for both\nunsupervised MT and transfer and how the domain\nmatters in unsupervised MT.\nFor transfer learning, we ﬁnd that if the model\nis transferred from a ”not so similar” language,\nlike Cs →En in our experiment, the performance\nfor transfer learning can not be scaled up by sim-\nply increasing the size of ﬁnetuning data. In con-\ntrast, for the closely related language, like lan-\nguages in Indic Language Family, we observe an\nopposite trend. However, the capacity of mBART\nmodel has not yet been well investigated, and we\nstill do not have a clear deﬁnition of the similar-\nity between transfer language and target language.\nThese could be the potential topics conducted in\nthe future resesarch.\nTalking about domain mismatch, we wish to ex-\nplore if transfer learning is a better UMT tech-\nnique for adapting a model to a different domain\nin case of low resource languages. We also want to\ntry more experiments to further solidify and gen-\neralize our ﬁndings. There have been numerous\nresearches done in this ﬁeld where attempts have\nbeen made to learn domain speciﬁc embeddings\nor even model based changes to generalize across\nmismatching domains. We hope to use some of the\nexisting methods and try to boost the performance\nof models speciﬁcally in case of low resource lan-\nguages.\nReferences\n[1]\nNaveen Arivazhagan, Ankur Bapna, Orhan\nFirat, Roee Aharoni, Melvin Johnson, and\nWolfgang Macherey.\nThe missing ingredi-\nent in zero-shot neural machine translation,\n2019.\n[2]\nMikel Artetxe, Sebastian Ruder, Dani Yo-\ngatama, Gorka Labaka, and Eneko Agirre.\nA call for more rigor in unsupervised cross-\nlingual learning, 2020.\n[3]\nYong Cheng. Semi-supervised learning for\nneural machine translation. In Joint Training\nfor Neural Machine Translation, pages 25–\n40. Springer, 2019.\n[4]\nZi-Yi Dou, Junjie Hu, Antonios Anasta-\nsopoulos, and Graham Neubig.\nUnsuper-\nvised domain adaptation for neural machine\ntranslation with domain-aware feature em-\nbeddings, 2019.\n[5]\nOrhan Firat,\nBaskaran Sankaran,\nYaser\nAl-Onaizan, Fatos T. Yarman Vural, and\nKyunghyun Cho. Zero-resource translation\nwith multi-lingual neural machine transla-\ntion, 2016.\n[6]\nCaglar Gulcehre, Orhan Firat, Kelvin Xu,\nKyunghyun Cho, Loic Barrault, Huei-Chi\nLin, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. On using monolingual cor-\npora in neural machine translation.\narXiv\npreprint arXiv:1503.03535, 2015.\n[7]\nFrancisco Guzm´an, Peng-Jen Chen, Myle\nOtt,\nJuan\nPino,\nGuillaume\nLample,\nPhilipp Koehn,\nVishrav Chaudhary,\nand\nMarc’Aurelio Ranzato. Two new evaluation\ndatasets for low-resource machine transla-\ntion:\nNepali-english and sinhala-english.\n2019.\n[8]\nWilbert Heeringa, Jelena Golubovic, Char-\nlotte Gooskens, Anja Sch¨uppert, Femke\nSwarte, and Stefanie Voigt. Lexical and or-\nthographic distances between Germanic, Ro-\nmance and Slavic languages and their rela-\ntionship to geographic distance, pages 99–\n137. P.I.E. - Peter Lang, 2013.\n[9]\nMelvin Johnson, Mike Schuster, Quoc V. Le,\nMaxim Krikun, Yonghui Wu, Zhifeng Chen,\nNikhil Thorat, Fernanda Vi´egas, Martin Wat-\ntenberg, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean.\nGoogle’s multilingual\nneural machine translation system: Enabling\nzero-shot translation.\nTransactions of the\nAssociation for Computational Linguistics,\n5:339–351, 2017.\ndoi:\n10.1162/tacl a\n00065.\nURL https://www.aclweb.\norg/anthology/Q17-1024.\n[10] Tom Kocmi and Ondˇrej Bojar. Trivial trans-\nfer learning for low-resource neural ma-\nchine translation. Proceedings of the Third\nConference on Machine Translation:\nRe-\nsearch Papers, 2018.\ndoi:\n10.18653/v1/\nw18-6325. URL http://dx.doi.org/\n10.18653/v1/W18-6325.\n[11] Philipp\nKoehn.\nEuroparl:\nA\nParal-\nlel Corpus for Statistical Machine Trans-\nlation.\nIn Conference Proceedings:\nthe\ntenth Machine Translation Summit, pages\n79–86, Phuket, Thailand, 2005. AAMT,\nAAMT.\nURL http://mt-archive.\ninfo/MTS-2005-Koehn.pdf.\n[12] Anoop\nKunchukuttan.\nThe\nIndicNLP\nLibrary.\nhttps://github.com/\nanoopkunchukuttan/indic_nlp_\nlibrary/blob/master/docs/\nindicnlp.pdf, 2020.\n[13] Anoop Kunchukuttan, Pratik Mehta, and\nPushpak Bhattacharyya.\nThe IIT bom-\nbay english-hindi parallel corpus.\nCoRR,\nabs/1710.02855, 2017.\nURL http://\narxiv.org/abs/1710.02855.\n[14] Guillaume Lample and Alexis Conneau.\nCross-lingual language model pretraining,\n2019.\n[15] Guillaume Lample, Alexis Conneau, Lu-\ndovic Denoyer,\nand Marc’Aurelio Ran-\nzato. Unsupervised machine translation us-\ning monolingual corpora only. arXiv preprint\narXiv:1711.00043, 2017.\n[16] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian\nLi, Sergey Edunov, Marjan Ghazvininejad,\nMike Lewis, and Luke Zettlemoyer. Multi-\nlingual denoising pre-training for neural ma-\nchine translation, 2020.\n[17] Minh-Thang Luong and Christopher D Man-\nning.\nStanford neural machine translation\nsystems for spoken language domains.\nIn\nProceedings of the International Workshop\non Spoken Language Translation, pages 76–\n79, 2015.\n[18] Kelly Marchisio, Kevin Duh, and Philipp\nKoehn.\nWhen does unsupervised machine\ntranslation work?, 2020.\n[19] Toan Q. Nguyen and David Chiang. Trans-\nfer learning across low-resource, related lan-\nguages for neural machine translation, 2017.\n[20] Rico Sennrich, Barry Haddow, and Alexan-\ndra Birch. Improving neural machine trans-\nlation models with monolingual data, 2016.\n[21] J¨org Tiedemann.\nThe tatoeba translation\nchallenge – realistic data sets for low re-\nsource and multilingual mt, 2020.\n[22] Michał Ziemski, Marcin Junczys-Dowmunt,\nand Bruno Pouliquen. The United Nations\nparallel corpus v1.0. In Proceedings of the\nTenth International Conference on Language\nResources and Evaluation (LREC’16), pages\n3530–3534, Portoroˇz, Slovenia, May 2016.\nEuropean Language Resources Association\n(ELRA). URL https://www.aclweb.\norg/anthology/L16-1561.\n[23] Barret Zoph, Deniz Yuret, Jonathan May,\nand Kevin Knight.\nTransfer learning\nfor low-resource neural machine transla-\ntion.\nIn Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Lan-\nguage Processing, pages 1568–1575, Austin,\nTexas,\nNovember 2016. Association for\nComputational Linguistics. doi: 10.18653/\nv1/D16-1163.\nURL\nhttps://www.\naclweb.org/anthology/D16-1163.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-03-31",
  "updated": "2021-03-31"
}