{
  "id": "http://arxiv.org/abs/2104.01506v2",
  "title": "Influencing Reinforcement Learning through Natural Language Guidance",
  "authors": [
    "Tasmia Tasrin",
    "Md Sultan Al Nahian",
    "Habarakadage Perera",
    "Brent Harrison"
  ],
  "abstract": "Interactive reinforcement learning agents use human feedback or instruction\nto help them learn in complex environments. Often, this feedback comes in the\nform of a discrete signal that is either positive or negative. While\ninformative, this information can be difficult to generalize on its own. In\nthis work, we explore how natural language advice can be used to provide a\nricher feedback signal to a reinforcement learning agent by extending policy\nshaping, a well-known Interactive reinforcement learning technique. Usually\npolicy shaping employs a human feedback policy to help an agent to learn more\nabout how to achieve its goal. In our case, we replace this human feedback\npolicy with policy generated based on natural language advice. We aim to\ninspect if the generated natural language reasoning provides support to a deep\nreinforcement learning agent to decide its actions successfully in any given\nenvironment. So, we design our model with three networks: first one is the\nexperience driven, next is the advice generator and third one is the advice\ndriven. While the experience driven reinforcement learning agent chooses its\nactions being influenced by the environmental reward, the advice driven neural\nnetwork with generated feedback by the advice generator for any new state\nselects its actions to assist the reinforcement learning agent to better policy\nshaping.",
  "text": "Inﬂuencing Reinforcement Learning through Natural Language Guidance\nTasmia Tasrin\nUniversity of Kentucky\ntta245@uky.edu\nMd Sultan Al Nahian\nUniversity of Kentucky\nmna245@uky.edu\nHabarakadage Perera\nUniversity of Kentucky\nhmpe228@uky.edu\nBrent Harrison\nUniversity of Kentucky\nbha286@g.uky.edu\nAbstract\nInteractive reinforcement learning (IRL) agents use hu-\nman feedback or instruction to help them learn in com-\nplex environments. Often, this feedback comes in the\nform of a discrete signal that’s either positive or neg-\native. While informative, this information can be difﬁ-\ncult to generalize on its own. In this work, we explore\nhow natural language advice can be used to provide a\nricher feedback signal to a reinforcement learning agent\nby extending policy shaping, a well-known IRL tech-\nnique. Usually policy shaping employs a human feed-\nback policy to help an agent to learn more about how\nto achieve its goal. In our case, we replace this human\nfeedback policy with policy generated based on natu-\nral language advice. We aim to inspect if the generated\nnatural language reasoning provides support to a deep\nRL agent to decide its actions successfully in any given\nenvironment. So, we design our model with three net-\nworks: ﬁrst one is the experience driven, next is the ad-\nvice generator and third one is the advice driven. While\nthe experience driven RL agent chooses its actions be-\ning inﬂuenced by the environmental reward, the advice\ndriven neural network with generated feedback by the\nadvice generator for any new state selects its actions to\nassist the RL agent to better policy shaping.\nIntroduction\nReinforcement learning (RL) is a machine learning approach\nthat teaches agents to exhibit behaviors that maximize a nu-\nmeric reward signal through trial-and-error. RL has proven\nthat it can train agents in complex environments with un-\nknown information. There are situations, however, where\nRL agents struggle to learn. For example, it is well known\nthat environments with sparse reward signals can prove dif-\nﬁcult for classic RL agents. In these situations, some re-\nsearchers have sought to augment classic RL approaches\nwith additional human knowledge in the way of direct feed-\nback or instruction. These approaches, called interactive re-\ninforcement learning (IRL) techniques, utilize this human\nknowledge to better enable agents to learn in especially com-\nplex environments. Typically, humans provide this knowl-\nedge by either providing demonstrations of positive behav-\nior or by providing numeric feedback on the quality of the\nCopyright © 2021by the authors. All rights reserved.\nactions taken by the agent during training. These, however,\ncan be difﬁcult for humans to provide in a way that is useful\nto the RL agent, especially when provided by teachers with\nlittle or no machine learning or artiﬁcial intelligence exper-\ntise.\nTo address this limitation of IRL, we explore the possi-\nbility of using natural human advice as a means to provide\nfeedback to an IRL system. Speciﬁcally, our approach in-\nvolves using this advice to train a computational advice gen-\nerator which the agent can then use to determine the qual-\nity of potential future actions. This will enable the agent to\nreceive targeted feedback commenting on the quality of ac-\ntions or suggestions for future actions to take while allow-\ning humans to provide feedback in a natural way. Our work\nextends previous work performed by Harrison et al. (Harri-\nson, Ehsan, and Riedl 2018) in which they show that nat-\nural language can be used to help simple agents general-\nize knowledge to unseen environments. However, their ap-\nproach utilizes highly structured language that was gener-\nated using a synthetic grammar, and they limited their study\nto grid world environments. In this work, we present a sys-\ntem for Automated Advice Aided Policy Shaping, or A3PS,\nan end-to-end system for training agents in complex envi-\nronments that combines deep reinforcement learning tech-\nniques with generated advice trained on human advice given\nin natural language. This is done by modifying the policy\nshaping algorithm, an IRL algorithm that learns from hu-\nman critique (Grifﬁth et al. 2013b).\nTo evaluate this system, we explore its effectiveness us-\ning the arcade game Frogger. In contrast to the work by\nHarrison et al., however, we evaluate our approach using a\nFrogger environment that utilizes a pixel state environment\nrather than a simpler grid-based one. We compare our ap-\nproach against state-of-the-art RL baselines and show that\nthe inclusion of natural language can signiﬁcantly enhance\nlearning even when rewards are sparse.\nRelated Works\nThe goal of IRL is to use human knowledge to help an\nautonomous agent learn in uncertain environments. One\nway that this can be done is by having a human teacher\ndirectly specify the reward function for an agent in var-\nious ways (Isbell et al. 2001; Hyeong Soo Chang 2006;\nTenorio-Gonz´alez, Morales, and Villase˜nor-Pineda 2010;\narXiv:2104.01506v2  [cs.AI]  11 Apr 2021\nKnox and Stone 2008; Thomaz and Breazeal 2006). While\nthese methods have proven effective, specifying a reward\nfunction directly can be difﬁcult for humans as often it is\nunclear how reward signals directly translate into behaviors.\nTo alleviate this, researchers developed methods for using\nhuman feedback to augment environmental reward. These\napproaches would use machine learning or deep learning\nmethods to merge various forms of human feedback with\nenvironmental reward in a way that often balances between\nthe two (Knox and Stone 2010; Knox and Stone 2012;\nArakawa et al. 2018). Ultimately, however, the goal of these\napproaches is to use human feedback to help an agent learn\nto maximize environmental reward.\nThese approaches still have limitations as they do not take\ninto account the fact that human feedback signals are often\ninconsistent. One explanation for this is that humans have\ntheir own policy for providing said feedback. One way to\naddress this limitation is to use human demonstrations of\npositive behavior to train an autonomous agent (Ng and Rus-\nsell 2000; Abbeel and Ng 2004; Taylor, Suay, and Chernova\n2011; Suay et al. 2016). This enables the agent to see exam-\nples of desirable behavior and learn from them. This can be\nproblematic for humans as it can be difﬁcult to specify what\ntypes of demonstrations will best help an agent learn.\nAnother option is to attempt to model how a human\nteacher provides feedback. This is the basis of the policy\nshaping algorithm (Grifﬁth et al. 2013a; Cederborg et al.\n2015), which seeks to model the feedback policy of a hu-\nman trainer and then combine it with a policy derived from\nan agent’s experience in an effort to guide exploration. The\nultimate goal is still to train an agent that maximizes en-\nvironmental reward, but this better enables it to understand\nhuman feedback. While this method has proven effective in\npractice, it is limited to working with discrete feedback. In\nthis work, we aim to extend it to better incorporate natural\nlanguage instructions.\nAs mentioned previously, our A3PS algorithm was in-\nspired by the work of Harrison et al. (Harrison, Ehsan, and\nRiedl 2018) in which they show that natural language can be\nused to help guide IRL agents in unknown environments.\nWhile they showed that their method was effective, their\nwork was limited in that the language that they investigated\nwas highly structured as it was generated by a synthetic\ngrammar. In addition, they limited their investigation to grid-\nbased environments. In this work, we explore how natural\nlanguage provided by humans can be used to improve learn-\ning in a complex environment that uses pixel information as\nstate.\nBackground\nReinforcement Learning\nReinforcement learning (RL) is a machine learning tech-\nnique where an agent’s target is to solve a Markov Decision\nProcess (MDP) by interacting with an environment through\na trial and error process. A MDP can be expressed as a tu-\nple of < S, A, T, R, γ > where S and A are sets of possi-\nble states in an environment and actions an agent can take\nrespectively. T describes how actions transition the agent\nfrom one state to another. R is a numeric reward function\nthat describes the quality of a state. γ is known as the dis-\ncount factor which determines how much emphasis an agent\nplaces on short-term versus long-term rewards. The goal of\nan agent in a MDP is to learn a policy π that speciﬁes the\naction that should be taken in each state that maximizes ex-\npected long-term reward.\nProximal Policy Optimization\nOur A3PS algorithm builds on Proximal Policy Optimiza-\ntion (PPO) (Schulman et al. 2017). PPO formulates vanilla\npolicy gradient in a way that provides more stable yet re-\nliable action probabilities for a RL agent. Instead of using\nlog probability to deﬁne the action’s impact on the agent’s\ncurrent policy, importance of the action from current policy\nover the previous policy’s action is measured (equation 1).\nrt(θ) differentiates between the old policy πθold(at|st) and\ncurrent policy πθ(at|st). Then clipping is done on the es-\ntimated advantage function ˆAt to avoid choosing the most\nexpected actions for current policy (equation 2).\nrt(θ) =\nπθ(at|st)\nπθold(at|st)\n(1)\nLClip(θ) = ˆEt[min(it(θ) ˆAt, clip(rt(θ), 1 −ϵ, 1 + ϵ) ˆAt)]\n(2)\nPolicy Shaping\nThe idea behind policy shaping is ﬁrst introduced by Grif-\nﬁth et al. (Grifﬁth et al. 2013b; Cederborg et al. 2015) where\nboth rewards and a numeric human feedback signal are\ncombined to ultimately determine the agent’s policy. Policy\nshaping does this by modeling the human’s feedback policy\nas a probability distribution over potential actions. By mod-\neling the human feedback policy in this way, it is possible\nto merge it with the agent’s action policy. This mixed pol-\nicy, thus, takes into account elements of both environmental\nreward and human feedback. In our work, we are employ-\ning a variant of policy shaping. Instead of human involve-\nment, the human’s feedback policy is determined through\nnatural language advice by an agent rather than a numeric\nfeedback signal. There are two primary advantages to this\ntechnique. The ﬁrst of these is that language is a more natu-\nral way for humans to provide guidance. The second is that\nlanguage should help the agent generalize this advice over\nmany states.\nMethods\nIn this paper, we propose a variant policy shaping method,\nAutomated Advice Aided Policy Shaping (A3PS) (see Fig-\nure 1). By combining human advice with RL, our method\nshould better enable agents to learn in complex environ-\nments and environments with sparse rewards. Our proposed\nnetwork A3PS is composed of three main modules: the Ex-\nperience Driven Agent (EDA), the Advice Generator, and\nthe Advice Driven Agent (ADA). The EDA is a Reinforce-\nment learning agent that uses PPO to learn a distribution\nover future actions that maximizes expected future rewards.\nFigure 1: Architecture of Automated Advice Aided Policy Shaping (A3PS)\nFigure 2: Network architecture of Experience Driven RL\nAgent\nThus, the EDA encapsulates knowledge learned through ex-\nperience and is primarily driven by environmental reward.\nThe ADA is a multi-modal deep neural network that uses\npixel-based game state and advice texts as inputs from the\npretrained advice generator and is responsible for producing\nthe action score vector. The action score vector is meant to\nrepresent the utility of actions based on human advice. The\noutput of these modules are then combined to produce a ﬁnal\ndistribution that combines the knowledge gained from both\nhuman advice and the agent’s own experience. This is then\nused to guide an agent during exploration in RL. We discuss\neach of these modules in greater detail below.\nExperience Driven RL Agent (EDA)\nThe Experience Driven Agent (EDA) is a reinforcement\nlearning agent implemented using the Proximal Policy Op-\ntimization (PPO) algorithm. The network architecture of the\nEDA is shown in Figure 2. At every time step, it takes\ntwo inputs: 1) Four consecutive frames of the environment\n(St1, ..St4) and 2) A vector Sg representing the status of\ncompletion of all the intermediate and ﬁnal goals in the envi-\nronment. We take 4 consecutive frames, so that the network\ncan better capture the motion and direction of the environ-\nment objects. Each frame of input is passed through a Con-\nvolutional Neural Network that encodes the image frame to\nan embedding vector labeled as emb in Figure 2. The goal\nstate vector Sg keeps the agent updated about its completed\ngoals and remaining goals. For each iteration, whenever a\ngoal position is explored by the PPO agent, ﬂag for the goal\nis set to 1. This information encourages the agent to remain\ntask oriented in situations where subgoals may be repeat-\nable.\nThe Actor-Critic model of EDA uses two deep neural net-\nworks: An Actor network gives action distribution from state\nand A Critic network tries to estimate the value for state-\naction combination. In our case, the state is composed of\nembedding vectors emb1..emb4. The four embedding vec-\ntors are sent to a LSTM (Hochreiter and Schmidhuber 1997)\niteratively in both Actor and Critic networks. The ﬁnal out-\nput of the LSTM is concatenated with the goal state vector\nSg before sending it to a fully connected(FC) layer. The FC\nlayer of Actor model populates action probabilities which\nassist the calculation of action ratio rt(θ) (equation 1) and\nthe Critic model generates a value which takes part in esti-\nmating the advantage function ˆAt of PPO algorithm. Later\nPPO with clipped objective is applied to the estimated ˆAt\nto avoid drastically changing the policy. Equation 2 refers\nto the mathematical representation of the clipping process.\nIn each iteration, this module generates action distribution\nvector Aexp as outcome.\nAdvice Generator\nOne of the issues present in (Harrison, Ehsan, and Riedl\n2018) is that it was difﬁcult to determine which advice has\nto be applied to a state at any given time. In our method, we\naddress this limitation by using the advice generator. The\nAdvice Generator module is a deep neural network which\naccepts the game state(current frame) as input and generates\nadvice regarding the state. As the task is similar to image\ncaptioning, we adopt the image captioning model utilized\nin (Xu et al. 2015) to implement the Advice generator. Our\nadvice generator is trained using a paired dataset of environ-\nment states and human generated advice utterances.\nAdvice Driven Agent (ADA)\nThe ADA agent utilizes the advice generated from the ad-\nvice generator combined with state information to learn\nan action distribution based on human feedback. Figure\n3 shows how we design this agent. In this work, we use\nResnet-101 module to extracts the features from each state\nFigure 3: Network architecture of Advice Driven Agent\nframe. Pretrained GloVe embeddings (Pennington, Socher,\nand Manning 2014) are used to convert the text generated by\nthe advice generator into vector representations. From these\nembedded words, LSTM cells capture advice context infor-\nmation in vector form. Then these two vectors of Resnet and\nLSTM features are concatenated and advanced through mul-\ntiple linear layers and ReLu activation layers to gather the\ndecoded action scores Aadv of the ADA module.\nAutomated Advice Aided Policy Shaping (A3PS)\nFigure 1 shows the overall architecture of A3PS. As you can\nsee in the ﬁgure, environmental state is used as an input to\nall three modules. These inputs are then processed via their\nrespective networks to produce the action distributions Aexp\nand Aadv which we have already discussed in the previous\nsegments of this section. From the two predicted action dis-\ntributions, the ﬁnal action distribution is calculated using the\nequation 3. Here α is a weight variable to control how much\nweight will be given to the results of individual network. We\ndecay the value of α over the training iterations. That means\nat starting point, the pretrained ADA network gets more im-\nportance with higher α value, but as the agent keeps explor-\ning, (1 −α) gets higher, hence EDA gains priority over the\nADA module. Thus, the agent will prioritize human advice\nearly during exploration and then rely on its own experience\nlater on.\na = softmax(α ∗Aadv + (1 −α) ∗Aexp)\n(3)\nAt each time step, the agent chooses the most probable\naction according to this combined action distribution to exe-\ncute. A note to mention, during training the A3PS architec-\nture, pretrained networks for both the advice generator and\nADA are utilized, so weights of the parameters of these two\nmodules remain unchanged. Only the EDA is updated dur-\ning RL as it is the only module that relies on environmental\nreward.\nExperimental Setup\nGame Environment\nWe test the A3PS system in the arcade game, Frogger (see\nFigure 4). We chose Frogger because the game is easy\nenough for humans to provide high quality feedback while\nstill being somewhat difﬁcult for a RL agent to solve. In\nFrogger, an agent must move from the bottom of the level\nto the top of the level while dodging car obstacles that ap-\nproach from the left and right of the screen. In total, there\nare eight rows in the environment before the agent reaches\nFigure 4: Frogger game state with generated advice “moved\nleft get better position next move forward get around tun-\nnel”.\nFigure 5: Average episode reward for EDA and A3PS in\ndense reward setting (smoothed with 100 episodes moving\nwindow).\nthe goal. The agent can move left, right, up, down, or choose\nto take no action at any time step. Also included in the en-\nvironment is a tunnel in the center row that blocks the agent\nfrom moving through it. Cars, however, approaching on that\nrow can move through it. For training purpose for all mod-\nules, the environmental state consists of the RGB values for\n100x100 resized images of the game environment.\nReward Function\nWe assign the highest reward of +400 if the agent reaches\nthe goal in the game environment. The agent also receives\nrewards if it reaches certain rows for the ﬁrst time. For ex-\nample, once the agent reaches the second row in the environ-\nment for the ﬁrst time, the agent will receive +10 reward. In\naddition, as there is an obstacle in level 5, a reward of +100\nhas given if the agent is able to reach the level by success-\nfully overcoming the tunnel. In addition, whenever an agent\ngoes one level up regardless of whether the agent has per-\nformed same action before, +1 is rewarded.\nPenalties are given to the agent for taking the wait action.\nIf the agent waits in the starting row, −5 reward is given to\nthe agent. The wait action results in −1 reward otherwise.\nIf the agent moves off the side of the environment or the\ntunnel, then −2 is rewarded. If the agent is hit by a car, the\nagent receives −20 reward and the episode ends.\nFigure 6: Average episode reward for EDA and A3PS in\nsparse reward setting (smoothed with 100 episodes moving\nwindow).\nDataset\nTo train the A3PS agent, we require a corpus of human\nadvice describing actions to take in various game states.\nIn this paper, we utilize the dataset used in (Ehsan et al.\n2019). This dataset contains examples of state/action in-\nformation paired with natural language explanations about\nwhy an action should be performed in a given state gath-\nered from users on Mechanical Turk. In total, the dataset\ncontains 1935 unique examples. To train the ADA, we split\nthis data into two parts using a ratio of 90 by 10. 90 percent\nof the dataset contains 1741 examples and is used for train-\ning the advice related modules. The remaining 10 percent\nis used for parameter tuning these models. Each piece of\nhuman advice contained in this dataset is preprocessed be-\nfore utilizing them in the training process. After discarding\nthe special characters from the natural language texts, the\nNLTK-tokenizer is applied to each line of the texts. Then\nstop words are removed before adding the texts in the vo-\ncabulary dictionary. This removal task is done to make sure\nthat our advice driven agent only focuses on the necessary\nelements of the natural language advice.\nNetwork Parameters\nAdam optimizer is used in all the networks as the optimiza-\ntion algorithm. However, as learning rate, for the EDA, 1e−4\nworks best. Similarly, the ADA module and advice genera-\ntor use learning rate of 1e−3 and 4e−4 respectively. For all\nnetworks, the LSTM size is ﬁxed to 512. And the value for\nimage embedding size for EDA network is set to 512. All\nthe experiments have been done using 2 Nvidia GTX 1080Ti\nGPUs.\nEvaluation & Discussion\nIn our experiments, we compare against the EDA network\nwith no access to language as a baseline. Thus, we are eval-\nuating if our A3PS algorithm can utilize human advice to the\npoint where it can outperform a baseline RL agent. We com-\npare in the Frogger environment described earlier under two\nconditions. The ﬁrst of these uses a dense reward function\nwhile the other uses a much more sparse reward function.\nThis will allow us to see if the presence of language can\nmake up for deﬁciencies in the environmental reward func-\ntion to enable an agent to learn.\nExperiment 1: Dense Reward\nFor this experiment, we have trained both standalone EDA\nnetwork and A3PS network for 10,500 episodes. Both of the\nmodels utilize the reward function that has been discussed\nin the Experimental Setup section. Also, the weight decay-\ning procedure takes place after each 2000 episodes from the\nstarting episode until around 6000 episodes and decay hap-\npens by 0.2 each time. As a result of this decay strategy, the\nA3PS agent solely uses a policy derived from environmental\nreward for the last 4500 episodes. Figure 5 depicts the visu-\nalization of experiment 1. As can be seen from the ﬁgure, the\nA3PS very quickly learns a decent policy and can quickly\nbegin optimizing that policy. This shows that the A3PS agent\nis able to successfully interpret the advice from human train-\ners and synthesize it into useful policy information. On the\ncontrary, the baseline EDA model struggles to build a pol-\nicy to reach the goal for the starting 3000 episodes. Though\nthe baseline model starts to learn from its experience which\nis visible at around episode 4000, it still takes some time\nbefore it matches the performance of the A3PS agent. This\nshows how powerful natural language as a source of human\nguidance can be. The agent in this situation was able to uti-\nlize this human advice to make up for its lack of experience.\nAs time went on, the agent was able to reﬁne this policy us-\ning its experience to learn an overall better policy.\nExperiment 2: Sparse Reward\nIn this experiment, we investigate the learning performance\nbetween the agents in the case of sparse or ill-deﬁned reward\nfunction. For this experiment, we deﬁne the reward function\ndifferently than the ﬁrst experiment. The agent will get a\npositive reward (+400) only when it reaches the goal and a\nnegative -20 reward when it dies. No other rewards are given\nor deducted from the agent. From Figure 6 we see that the\nbaseline agent EDA initially gets some negative rewards but\nlater it gets neither positive nor negative rewards. This im-\nplies that the agent tries to explore further into the environ-\nment, but soon dies and receives the negative reward. This\ncauses the agent to prefer to take no action as receiving no\nreward is preferable to the possibility of a negative reward.\nThis illustrates why environments with sparse rewards can\nbe difﬁcult for RL agents.\nIn contrast, the A3PS agent is able to supplement this\nsparse reward with a denser signal from the ADA mod-\nule. This encourages the agent to explore the environment\nand discover positive environmental reward. After 6000\nepisodes, the A3PS agent does not take guidance from the\nADA module and chooses actions only based on its experi-\nence. As we see from Figure 6, still the agent can reach goal\nwith its learned policy. Though the agent does not get any\nimmediate rewards for its actions, automated guidance helps\nit to stay on the direction and shape its policy to the optimal\nconvergence where it can take right policy after not being\nguided by the ADA module as well. In contrast, in baseline\nEDA network, without the guidance, the policy converges\ninto local optima and consequently agent does not explore\nto reach the goal.\nConclusion\nIn this paper we present A3PS, an IRL algorithm that uti-\nlizes natural language advice combined with environmental\nreward to train agents in complex environments. By using\npixel based game states and associative advice texts, we have\nshown its efﬁcacy in the arcade game, Frogger. We evalu-\nated A3PS system against a baseline deep-RL method and\nshowed that A3PS outperforms it for both dense and sparse\nreward functions, showing the effectiveness of our model\nand natural language as a source of instruction for IRL meth-\nods.\nAcknowledgements\nThis material is based upon work supported by the National\nScience Foundation under Grant No. 1849231.\nReferences\n[Abbeel and Ng 2004] Abbeel, P., and Ng, A. Y. 2004. Ap-\nprenticeship learning via inverse reinforcement learning. In\nProceedings of the Twenty-First International Conference\non Machine Learning, ICML ’04, 1. Association for Com-\nputing Machinery.\n[Arakawa et al. 2018] Arakawa, R.; Kobayashi, S.; Unno, Y.;\nTsuboi, Y.; and Maeda, S. 2018. DQN-TAMER: human-in-\nthe-loop reinforcement learning with intractable feedback.\nCoRR abs/1810.11748.\n[Cederborg et al. 2015] Cederborg, T.; Grover, I.; Isbell,\nC. L.; and Thomaz, A. L. 2015. Policy shaping with hu-\nman teachers. IJCAI’15, 3366–3372. AAAI Press.\n[Ehsan et al. 2019] Ehsan, U.; Tambwekar, P.; Chan, L.; Har-\nrison, B.; and Riedl, M. O. 2019. Automated rationale gen-\neration: A technique for explainable ai and its effects on hu-\nman perceptions. In Proceedings of the 24th International\nConference on Intelligent User Interfaces, IUI ’19, 263–274.\nNew York, NY, USA: Association for Computing Machin-\nery.\n[Grifﬁth et al. 2013a] Grifﬁth, S.; Subramanian, K.; Scholz,\nJ.; Isbell, C.; and Thomaz, A. L. 2013a. Policy shaping:\nIntegrating human feedback with reinforcement learning. In\nAdvances in Neural Information Processing Systems, 2625–\n2633.\n[Grifﬁth et al. 2013b] Grifﬁth, S.; Subramanian, K.; Scholz,\nJ.; Isbell, C. L.; and Thomaz, A. L. 2013b. Policy shaping:\nIntegrating human feedback with reinforcement learning. In\nBurges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.;\nand Weinberger, K. Q., eds., Advances in Neural Informa-\ntion Processing Systems 26. Curran Associates, Inc. 2625–\n2633.\n[Harrison, Ehsan, and Riedl 2018] Harrison, B.; Ehsan, U.;\nand Riedl, M. O.\n2018.\nGuiding reinforcement learning\nexploration using natural language. In Proceedings of the\n17th International Conference on Autonomous Agents and\nMultiAgent Systems, 1956–1958.\n[Hochreiter and Schmidhuber 1997] Hochreiter,\nS.,\nand\nSchmidhuber, J. 1997. Long short-term memory. Neural\nComput. 9(8):1735–1780.\n[Hyeong Soo Chang 2006] Hyeong Soo Chang. 2006. Re-\ninforcement learning with supervision by combining multi-\nple learnings and expert advices. In 2006 American Control\nConference, 6 pp.–.\n[Isbell et al. 2001] Isbell, C.; Shelton, C. R.; Kearns, M.;\nSingh, S.; and Stone, P. 2001. A social reinforcement learn-\ning agent. In Proceedings of the Fifth International Con-\nference on Autonomous Agents, 377–384. Association for\nComputing Machinery.\n[Knox and Stone 2008] Knox, W., and Stone, P.\n2008.\nTamer: Training an agent manually via evaluative reinforce-\nment. 292 – 297.\n[Knox and Stone 2010] Knox, W. B., and Stone, P.\n2010.\nCombining manual feedback with subsequent mdp reward\nsignals for reinforcement learning. In Proceedings of the 9th\nInternational Conference on Autonomous Agents and Multi-\nagent Systems: Volume 1 - Volume 1, AAMAS ’10, 5–12.\nInternational Foundation for Autonomous Agents and Mul-\ntiagent Systems.\n[Knox and Stone 2012] Knox, W. B., and Stone, P. 2012. Re-\ninforcement learning from simultaneous human and mdp re-\nward. In Proceedings of the 11th International Conference\non Autonomous Agents and Multiagent Systems - Volume 1,\nAAMAS ’12, 475–482. Richland, SC: International Foun-\ndation for Autonomous Agents and Multiagent Systems.\n[Ng and Russell 2000] Ng, A. Y., and Russell, S. 2000. Al-\ngorithms for inverse reinforcement learning. In in Proc. 17th\nInternational Conf. on Machine Learning, 663–670. Morgan\nKaufmann.\n[Pennington, Socher, and Manning 2014] Pennington,\nJ.;\nSocher, R.; and Manning, C.\n2014.\nGloVe: Global\nvectors for word representation.\nIn Proceedings of the\n2014 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 1532–1543.\nDoha, Qatar:\nAssociation for Computational Linguistics.\n[Schulman et al. 2017] Schulman, J.; Wolski, F.; Dhariwal,\nP.; Radford, A.; and Klimov, O. 2017. Proximal policy op-\ntimization algorithms. CoRR abs/1707.06347.\n[Suay et al. 2016] Suay, H. B.; Brys, T.; Taylor, M. E.; and\nChernova, S. 2016. Learning from demonstration for shap-\ning through inverse reinforcement learning. In Proceedings\nof the 2016 International Conference on Autonomous Agents\nand Multiagent Systems, AAMAS ’16, 429–437. Interna-\ntional Foundation for Autonomous Agents and Multiagent\nSystems.\n[Taylor, Suay, and Chernova 2011] Taylor,\nM.\nE.;\nSuay,\nH. B.; and Chernova, S. 2011. Integrating reinforcement\nlearning with human demonstrations of varying ability. In\nThe 10th International Conference on Autonomous Agents\nand Multiagent Systems - Volume 2, AAMAS ’11, 617–624.\nRichland, SC: International Foundation for Autonomous\nAgents and Multiagent Systems.\n[Tenorio-Gonz´alez, Morales, and Villase˜nor-Pineda 2010]\nTenorio-Gonz´alez, A.; Morales, E.; and Villase˜nor-Pineda,\nL.\n2010.\nDynamic reward shaping: Training a robot by\nvoice. 483–492.\n[Thomaz and Breazeal 2006] Thomaz, A. L., and Breazeal,\nC.\n2006.\nReinforcement learning with human teachers:\nEvidence of feedback and guidance with implications for\nlearning performance. In Proceedings of the 21st National\nConference on Artiﬁcial Intelligence - Volume 1, AAAI’06,\n1000–1005. AAAI Press.\n[Xu et al. 2015] Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville,\nA.; Salakhudinov, R.; Zemel, R.; and Bengio, Y.\n2015.\nShow, attend and tell: Neural image caption generation with\nvisual attention. In Bach, F., and Blei, D., eds., Proceedings\nof the 32nd International Conference on Machine Learning,\nvolume 37 of Proceedings of Machine Learning Research,\n2048–2057. Lille, France: PMLR.\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-04-04",
  "updated": "2021-04-11"
}