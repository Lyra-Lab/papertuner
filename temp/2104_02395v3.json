{
  "id": "http://arxiv.org/abs/2104.02395v3",
  "title": "Ensemble deep learning: A review",
  "authors": [
    "M. A. Ganaie",
    "Minghui Hu",
    "A. K. Malik",
    "M. Tanveer",
    "P. N. Suganthan"
  ],
  "abstract": "Ensemble learning combines several individual models to obtain better\ngeneralization performance. Currently, deep learning architectures are showing\nbetter performance compared to the shallow or traditional models. Deep ensemble\nlearning models combine the advantages of both the deep learning models as well\nas the ensemble learning such that the final model has better generalization\nperformance. This paper reviews the state-of-art deep ensemble models and hence\nserves as an extensive summary for the researchers. The ensemble models are\nbroadly categorised into bagging, boosting, stacking, negative correlation\nbased deep ensemble models, explicit/implicit ensembles,\nhomogeneous/heterogeneous ensemble, decision fusion strategies based deep\nensemble models. Applications of deep ensemble models in different domains are\nalso briefly discussed. Finally, we conclude this paper with some potential\nfuture research directions.",
  "text": "Ensemble deep learning: A review\nM.A. Ganaiea, Minghui Hub, A.K. Malika, M. Tanveera,∗, P.N. Suganthanb,c,∗\naDepartment of Mathematics, Indian Institute of Technology Indore, Simrol, Indore, 453552, India\nbSchool of Electrical & Electronic Engineering, Nanyang Technological University, Singapore\ncKINDI Center for Computing Research College of Engineering, Qatar University, Qatar\nAbstract\nEnsemble learning combines several individual models to obtain better generalization perfor-\nmance. Currently, deep learning architectures are showing better performance compared to the\nshallow or traditional models. Deep ensemble learning models combine the advantages of both\nthe deep learning models as well as the ensemble learning such that the ﬁnal model has bet-\nter generalization performance. This paper reviews the state-of-art deep ensemble models and\nhence serves as an extensive summary for the researchers. The ensemble models are broadly\ncategorised into bagging, boosting, stacking, negative correlation based deep ensemble models,\nexplicit/implicit ensembles, homogeneous/heterogeneous ensemble, decision fusion strategies\nbased deep ensemble models. Applications of deep ensemble models in diﬀerent domains are\nalso brieﬂy discussed. Finally, we conclude this paper with some potential future research direc-\ntions.\nKeywords: Ensemble Learning, Deep Learning.\n1. Introduction\nDeep learning architectures have been successfully employed across a wide range of applica-\ntions from image/video classiﬁcation to the health care. The success of these models is attributed\nto the better feature representation via multi layer processing architectures. The deep learning\nmodels have been mainly used for classiﬁcation, regression and clustering problems. Classiﬁ-\ncation problem is deﬁned as the categorization of the new observations based on the hypothesis\n∗Corresponding authors\nEmail addresses: phd1901141006@iiti.ac.in (M.A. Ganaie), minghui.hu@ntu.edu.sg (Minghui Hu),\nphd1801241003@iiti.ac.in (A.K. Malik), mtanveer@iiti.ac.in (M. Tanveer), epnsugan@ntu.edu.sg (P.N.\nSuganthan)\nPreprint submitted to Elsevier\nAugust 9, 2022\narXiv:2104.02395v3  [cs.LG]  8 Aug 2022\nh learned from the set of training data. The hypothesis h represents a mapping of input data\nfeatures to the appropriate target labels/classes. The main objective, while learning the hypothe-\nsis h, is that it should approximate the true unknown function as close as possible to reduce the\ngeneralization error. There exist several applications of these classiﬁcation algorithms ranging\nfrom medical diagnosis to remote sensing. Mathematically,\nOc = h(x, θc), Oc ∈Z,\n(1)\nwhere x is the input feature vector, Oc is the category of the sample x, θc is the set of learning\nparameters of the hypothesis h and Z is the set of class labels.\nRegression problems deal with the continuous decisions, instead of discrete categories. Math-\nematically,\nOr = h(x, θr), Or ∈R,\n(2)\nwhere x is the observation vector, Or is the output, and θr is the set of learning parameters of the\nhypothesis h.\nBroadly speaking, there are diﬀerent approaches of classiﬁcation like supervised, unsuper-\nvised classiﬁcation, few-shot, one-shot and so on. Here, we only discuss supervised and unsuper-\nvised classiﬁcation problems. In supervised learning, the building of hypothesis h is supervised\nbased on the known output labels provided in the training data samples, while as in unsupervised\nlearning hypothesis h is generated without any supervision as no known output values are avail-\nable with the training data. This approach, also known as clustering, generates the hypothesis h\nbased on the similarities and dissimilarities present in the training data.\nGenerally speaking, the goal of generating the hypothesis h in Machine learning area is that it\nshould perform better when applied to unknown data. The performance of the model is measured\nwith respect to the area in which the model is applied. Combining the predictions from several\nmodels has proven to be an elegant approach for increasing the performance of the models.\nCombination of several diﬀerent predictions from diﬀerent models to make the ﬁnal prediction is\nknown as ensemble learning or ensemble model. The ensemble learning involves multiple mod-\nels combined in some fashion like averaging, voting such that the ensemble model is better than\nany of the individual models. To prove that average voting in an ensemble is better than individ-\nual model, Marquis de Condorcet proposed a theorem wherein he proved that if the probability\nof each voter being correct is above 0.5 and the voters are independent, then addition of more\n2\nvoters increases the probability of majority vote being correct until it approaches 1 [1]. Although\nMarquis de Condorcet proposed this theorem in the ﬁeld of political science and had no idea of\nthe ﬁeld of Machine learning, but it is the similar mechanism that leads to better performance of\nthe ensemble models. Assumptions of Marquis de Condorcet theorem also holds true for ensem-\nbles [2]. The reasons for the success of ensemble learning include: statistical, computational and\nrepresentation learning [3], bias-variance decomposition [4] and strength-correlation [5].\nIn this era of machine learning, deep learning automates the extraction of high-level features\nvia hierarchical feature learning mechanism wherein the upper layer of features are generated\non the previous set of layer/layers. Deep learning has been successfully applied across diﬀer-\nent ﬁelds since the ImageNet Large Scale Recognition Challenge (ILSVRC) competitions [6, 7]\nand has achieved state-of-art performance. It has obtained promising results in object detection,\nsemantic segmentation, edge detection and number of other domains. However, given the com-\nputational cost, the training of deep ensemble models is an uphill task. Diﬀerent views have been\nprovided to understand how the deep learning models learn the features like learning through hi-\nerarchy of concepts via many levels of representation [8, 9, 10]. Given the advantages of deep\nlearning models from deep architectures, there are several bottlenecks like vanishing/exploding\ngradients [11, 12] and degradation problem [13] which prevent to reach this goal. Recently,\ntraining deep network’s has become feasible through the Highway networks [14] and Residual\nnetworks [13]. Both these networks enabled to train very deep networks. The ensemble learning\nhas been recently known to be strong reason for enhancing the performance of deep learning\nmodels [15]. Thus, the objective of deep ensemble models is to obtain a model that has best of\nboth the ensemble and deep models.\nThere exist multiple surveys in the literature which mainly focus on the review of ensemble\nlearning like learning of ensemble models in classiﬁcation problems [16, 17, 18, 19], regres-\nsion problems [20, 21] and clustering [22]. Review of both the classiﬁcation and regression\nmodels was given in [23]. Comprehensive review of the ensemble methods and the challenges\nwere given in [24]. Though [24] provided some insight about the deep ensemble models but\ncouldn’t give the comprehensive review of the deep ensemble learning while as [25] reviewed\nthe ensemble deep models in the context of bioinformatics. The past decade has successively\nevolved diﬀerent deep learning strategies which have lead to the exploration and innovation of\nthese models in multiple areas like health care, speech, image classiﬁcation, forecasting and other\n3\napplications. Broadly speaking, ensemble learning approaches have followed classical methods,\ngeneral methods and diﬀerent fusion strategies for improving the performance of the models.\nSince deep learning models are computation and data extensive, hence, ensemble deep learn-\ning models need special attention while exploring the complementary information of multiple\nalgorithms into a uniform framework. Ensemble deep learning models need to handle multiple\nquestions like how to induce diversity among the baseline models, how to keep the training time\nas well the models complexity lower for the practical applications, how to fuse the predictions of\nthe complementary algorithms. Multiple studies have handled these problems diﬀerently. In this\nreview paper, we comprehensively review the diﬀerent approaches used to handle the aforemen-\ntioned problems. In this paper, we give a comprehensive review of deep ensemble models. To\nthe best of our knowledge, this is the ﬁrst comprehensive review paper on deep ensemble\nmodels.\nThe rest of this paper is organised as follows: Section-3 discusses the theoretical aspects\nof deep ensemble learning, Section-4 discusses the diﬀerent approaches used in deep ensemble\nstrategies, applications of deep ensemble methods are given in Section-5 and ﬁnally conclusions\nand future directions are given in Section-6.\n4\nEnsemble Deep Learning \nTheory\nBias-Variance\nDecomposition\nStatistical Computational\nand Representational\nDiversity\nEnsemble Strategy\n  Boosting\n  Bagging\n  Stacking\nClassical\nMethods\nGeneral\nMethods\n  Explicit \\ Implicit \n  Negative correlation   \n    Learning\n  Homogeneous /           \n  Heterogeneous\nFusion Strategy\n  Majority Voting\n  Unweighted Model\nAveraging\n  Bayes Optimal\nClassifier\n  Stacked\nGeneralization\n  Super Learner\n  Consensus\n  Query-By-Committee\nApplications\n  Health Care\n  Speech\n  Image Classification\n  Forecasting\n  Others\nConclusion and\nFuture Work\nFigure 1: Layout of the paper.\n5\n2. Research Methodology\nThe studies in this review are searched from the Google Scholar and Scopus search engines.\nThe papers are the result of ensemble learning, ensemble deep learning, deep ensemble learning,\ndeep ensembles keywords. The articles where screened based on the title and abstract, followed\nby the screening of full-text version. The articles are elaborated based on the ensemble learning\nand deep learning approaches.\n3. Theory\nThe various reasons which have been justiﬁed for the success of ensemble learning can be\ndiscussed under the following subheadings:\n3.1. Bias-Variance Decomposition\nInitially, the success of ensemble methods was theoretically investigated for regression prob-\nlems. Krogh and Vedelsby [26], Brown et al. [27] proved via ambiguity decomposition that the\nproper ensemble classiﬁer guarantees a smaller squared error as compared to the individual pre-\ndictors of the classiﬁer. Ambiguity decomposition was given for single dataset based ensemble\nmethods, later on, multiple dataset bias-variance-covariance decomposition was introduced in\n[27, 28, 29, 30] and is given as:\nE[o −t]2 = bias2 + 1\nM var + (1 −1\nM )covar,\nbias = 1\nM\nX\ni\n(E[oi] −t),\nvar = 1\nM\nX\ni\nE[oi −E[oi]]2,\n(3)\ncovar =\n1\nM(M −1)\nX\ni\nX\nj,i\nE[oi −E[oi]][o j −E[oj]],\nwhere t is target, oi is the output of ith model and M is the ensemble size. Here, bias term\nmeasures the average diﬀerence between the base learner and the model output, var indicates\ntheir average variance, and covar is the covariance term measuring the pairwise diﬀerence of the\nbase learners.\n6\nEnsemble methods have been supported by several theories like bias-variance [4, 31], strength\ncorrelation [5], stochastic discrimination [32], and margin theory [33]. These theories provide\nthe equivalent of bias-variance-covariance decomposition [34].\nThe above given equations of decomposition error can’t be directly applied to the datasets\nwith discrete class labels due to their categorical nature. However, alternate ways to decompose\nthe error in classiﬁcation problems are given in [4, 35, 36, 37, 38].\nMultiple approaches like bagging, boosting have been proposed for generating the ensemble\nmethods. Bagging reduces the variance among the base classiﬁers [39] while as boosting based\nensembles lead to the bias and variance reduction [40, 41].\n3.2. Statistical, Computational and Representational Aspects\nDietterich provided Statistical, Computational and Representational reasons [3] for success\nof ensemble models. The learning model is viewed as the search of the optimal hypothesis h\namong the several hypothesis in the search space. When the amount of data available for the\ntraining is smaller compared to the size of the hypothesis space, the statistical problem arises.\nDue to this statistical problem, the learning algorithm identiﬁes the diﬀerent hypothesis which\ngives same performance on the training samples. Ensembling of these hypothesis results in an\nalgorithm which reduces the risk of being a wrong classiﬁer. The second reason is computational\nwherein a learning algorithm stucks in a local optima due to some form of local search. Ensem-\nble model overcomes this issue by performing some form of local search via diﬀerent starting\npoints which leads to better approximation of the true unknown function. Another reason is rep-\nresentational wherein none of the hypotheses among the set of hypothesis is able to represent the\ntrue unknown function. Hence, ensembling of these hypothesis via some weighting technique\nresults into the hypothesis which expands the representable function space.\n3.3. Diversity\nOne of the main reasons behind the success of ensemble methods is increasing the diversity\namong the base classiﬁers and the same thing was highlighted in [3]. Diﬀerent approaches\nhave been followed to generate diverse classiﬁers. Diﬀerent methods like bootstrap aggregation\n(bagging) [39], Adaptive Boosting (AdaBoost) [42], random subspace [43], and random forest\n[5] approaches are followed for generating the multiple datasets from the original dataset to train\nthe diﬀerent predictors such that the outputs of predictors are diverse. Attempts have been made\n7\nto increase diversity in the output data wherein multiple outputs are created instead of multiple\ndatasets for the supervision of the base learners. ‘Output smearing’ [44] is one of this kind which\ninduces random noise to introduce diversity in the output space.\n4. Ensemble Strategies:\nThe diﬀerent ensemble strategies have evolved over a period of time which results in bet-\nter generalization of the learning models. The ensemble strategies are broadly categorised as\nfollows:\n4.1. Bagging\nBagging [39], also known as bootstrap aggregating, is one of the standard techniques for\ngenerating the ensemble-based algorithms. Bagging is applied to enhance the performance of an\nensemble classiﬁer. The main idea in bagging is to generate a series of independent observations\nwith the same size, and distribution as that of the original data. Given the series of observa-\ntions, generate an ensemble predictor which is better than the single predictor generated on the\noriginal data. Bagging increases two steps in the original models: First, generating the bagging\nsamples and passing each bag of samples to the base models and second, strategy for combining\nthe predictions of the multiple predictors. Bagging samples may be generated with or without\nreplacement. Combining the output of the base predictors may vary as mostly majority voting is\nused for classiﬁcation problems while the averaging strategy is used in regression problems for\ngenerating the ensemble output. Figure 2 shows the diagram of the bagging technique. Here, Di\nrepresents the bagged datasets, Ci represents the algorithms and Fens calculates the ﬁnal outcome.\nRandom Forest [5] is an improved version of the decision trees that uses the bagging strategy\nfor improving the predictions of the base classiﬁer which is a decision tree. The fundamental\ndiﬀerence between these two methods is that at each tree split in Random Forest, only a subset\nof features is randomly selected and considered for splitting. The purpose of this method is to\ndecorrelate the trees and prevent over-ﬁtting. Breiman [5] showed heuristically that the variance\nof the bagged predictor is smaller than the original predictor and proposed that bagging is bet-\nter in higher dimensional data. However, the analysis of the smoothing eﬀect of bagging [45]\nrevealed that bagging doesn’t depend on the data dimensionality.\n8\nD\nD1\nD2\nDM\nC1\nC2\nCM\nFens\nFigure 2: Bagging\nD\nD1\nD2\nDM\nC1\nC2\nCM\nFens\nFigure 3: Boosting\nD\nC1\nC2\nCM\nC1\nC2\nCM\nC1\nC2\nCM\nFOutput\nLevel1\nLevel2\nLevelL\nFigure 4: Stacking\n9\nB¨uhlmann and Yu [46] gave theoretical explanation of how bagging gives smooth hard de-\ncisions, small variance, and mean squared error. Since bagging is computationally expensive,\nhence subbagging and half subbagging [46] were introduced. Half subbagging, being computa-\ntionally eﬃcient, is as accurate as the bagging.\nSeveral attempts tried to combine bagging with other machine learning algorithms. Kim et al.\n[47] used bagging method to generate multiple bags of the dataset and multiple support vector\nmachines were trained independently with each bag as the input. The output of the models is\ncombined via majority voting, least squares estimation weighting and double layer hierarchical\napproach. In the double layer hierarchical approach, another support vector machines (SVM) is\nused to combine the outcomes of the multiple SVM’s eﬃciently. Tao et al. [48] used asymmetric\nbagging strategy to generate the ensemble model to handle the class imbalance problems. A\ncase study of bagging, boosting and basic ensembles [49] revealed that at higher rejection rates\nof samples boosting is better as compared to bagging and basic ensembles. However, as the\nrejection rate increases the diﬀerence disappears among the boosting, bagging and basic ensem-\nbles. Bagging based multilayer perceptron [50] combined bagging to train multiple perceptrons\nwith the corresponding bag and showed that bagging based ensemble models perform better as\ncompared to individual multilayer perceptron. In [51], the analysis of the bagging approach and\nother regularisation techniques revealed that bagging regularized the neural networks and hence\nprovide better generalization. In [52], bagged neural networks (BNNs) was proposed wherein\neach neural network was trained over diﬀerent dataset sampled randomly with replacement from\noriginal dataset and was implemented for the short term load forecasting. Unlike Random forest\n[5] which uses majority voting for aggregating the ensemble of decision trees, bagging based\nsurvival trees [53] used Kaplan–Meier curve to predict the ensemble output for breast cancer\nand lymphoma patients. In [54], ensembles of stacked denoising autoencoders for classiﬁcation\nshowed that the bagging and switching technique in a general deep machine results in improved\ndiversity.\nBagging has also been applied to solve the problem of imbalanced data. Roughly Balanced\nBagging [55] tries to equalize each class’s sampling probability in binary class problems wherein\nthe negative class samples are sampled via negative binomial distribution, instead of keeping the\nsample size of each class the same number. Neighbourhood Balanced Bagging [56] incorpo-\nrated the neighbourhood information for generating the bagging samples for the class imbalance\n10\nYears\nAuthors\nContribution\n1996\nBreiman [39]\nProposed the idea of Bagging\n1998\nMao [49]\nCase study of bagging, boosting and basic ensembles\n2000\nBuja and Stuetzle [45]\nTheoretical analysis of bagging\n2001\nBreiman [5]\nBagging with random subspace Decision trees and ensembling outputs via majority voting\n2001\nGenc¸ay and Qi [51]\nStudy of Bayesian regularization, early stopping and Bagging\n2002\nKim et al. [47]\nBagging with SVM’s and ensembling outputs via SVM’s, majority voting and least squares estimation\n2002\nB¨uhlmann and Yu [46]\nTheoretical justiﬁcation of Bagging, proposed subbagging and half subagging\n2004\nHothorn et al. [53]\nBagging with decision trees and ensembling outputs via Kaplan–Meier curve\n2005\nOza [57]\nTheoretical and experimental analysis of online bagging and boosting\n2006\nTao et al. [48]\nProposed assymmetric bagging with SVM’s and ensembling outputs SVM’s\n2009\nHido et al. [55]\nRoughly balanced bagging on decision trees and ensembling outputs via majority voting\n2005, 2015\nHa et al. [50], Khwaja et al. [52]\nBagging with Neural networks and ensembling outputs via majority voting\n2015\nBłaszczy´nski and Stefanowski [56]\nNeighbourhood balanced bagging ensembling outputs via majority voting\nTable 1: Bagging based ensemble models\nproblems. Błaszczy´nski and Stefanowski [56] concluded that applying conventional diversiﬁ-\ncation is more eﬀective when applied at the last classiﬁcation methods. Both roughly balanced\nBagging and Neighbourhood Balanced Bagging have not been explored in deep learning archi-\ntectures. Thus, these approaches can be exploited to handle the class imbalance problems via\ndeep ensemble models.\nThe theoretical and experimental analysis of online bagging and boosting [57] showed that\nthe online bagging algorithm can achieve similar accuracy as the batch bagging algorithm with\nonly a little more training time. However, online bagging is an option when all training samples\ncan’t be loaded into the memory due to memory issues.\nAlthough ensembling may lead to increase in the computational complexity, but bagging\npossesses the property that it can be paralleled and can lead to eﬀective reduction in the training\ntime subject to the availability of hardware for running the parallel models. Since deep learning\nmodels have high training time, hence optimization of multiple deep models on diﬀerent training\nbags is not a feasible option.\n4.2. Boosting\nBoosting technique is used in ensemble models for converting a weak learning model into a\nlearning model with better generalization. Figure 3 shows the diagram of the boosting technique.\nThe techniques such as majority voting in case of classiﬁcation problems or a linear combina-\ntion of weak learners in the regression problems results in better prediction as compared to the\n11\nsingle weak learner. Boosting methods like AdaBoost [42] and Gradient Boosting [58] have\nbeen used across diﬀerent domains. Adaboost uses a greedy technique for minimizing a convex\nsurrogate function upper bounded by misclassiﬁcation loss via augmentation, at each iteration,\nthe current model with the appropriately weighted predictor. AdaBoost learns an eﬀective en-\nsemble classiﬁer as it leverages the incorrectly classiﬁed sample at each stage of the learning.\nAdaBoost minimizes the exponential loss function while as the Gradient boosting generalized\nthis framework to the arbitrary diﬀerential loss function.\nBoosting, also known as forward stagewise additive modelling, was originally proposed to\nimprove the performance of the classiﬁcation trees. It has been recently incorporated in the deep\nlearning models to further improve their performance.\nBoosted deep belief network (DBN) [59] for facial expression recognition uniﬁed the boost-\ning technique and multiple DBN’s via objective function which results in a strong classiﬁer. The\nmodel learns complex feature representation to build a strong classiﬁer in an iterative manner.\nDeep boosting [60] is an ensemble model that uses the deep decision trees. It can also be used in\ncombination with any other rich family classiﬁer to improve the generalization performance. In\neach stage of the deep boosting, the decisions of which classiﬁer to add and what weights should\nbe chosen depends on the (data-dependent) complexity of the classiﬁer to which it belongs. The\ninterpretation of the deep boosting classiﬁer is given via structural risk minimization principle at\neach stage of the learning. Multiclass Deep boosting [61] extended the Deep boosting [60] al-\ngorithm to theoretical, algorithmic, and empirical results to the multiclass problems. Due to the\nlimitation of the training data in each mini batch, Boosting CNN may overﬁt the data. To avoid\noverﬁtting, incremental Boosting CNN (IBCNN) [62] accumulated the information of multiple\nbatches of the training data samples. IBCNN uses decision stumps on the top of single neurons\nas the weak learners and learns weights via AdaBoost method in each mini batch. Unlike DBN\n[59] which uses image patch for learning the weak classiﬁers, IBCNN trains the weak classiﬁers\nfrom the fully connected layer i.e. the whole image is used for learning the weak classiﬁers. To\nmake the IBCNN model more eﬃcient, the weak learners loss functions are combined with the\nglobal loss function.\nBoosted CNN [63] used boosting for training the deep CNN. Instead of averaging, least\nsquares objective function was used to incorporate the boosting weights into CNN. Moghimi\net al. [63] also showed that CNN can be replaced by network structure within their boosting\n12\nframework for improving the performance of the base classiﬁer. Boosting increases the com-\nplexity of training the networks, hence the concept of dense connections was introduced in a\ndeep boosting framework to overcome the problem of vanishing gradient problem for image de-\nnoising [64]. Deep boosting framework was extended to image restoration in [65] wherein the\ndilated dense fusion network was used to boost the performance.\nThe convolutional channel features [66] generated the high level features via CNN and then\nused boosted forest for ﬁnal classiﬁcation. Since CNN has high number of hyperparameters\nthan the boosted forest, hence the model proved to be eﬃcient than end-to-end training of CNN\nmodels both in terms of performance and time. Yang et al. [66] showed its application in edge\ndetection, object proposal generation, pedestrian and face detection. A stagewise boosting deep\nCNN [67] trains several models of the CNNs within the oﬄine paradigm boosting framework.\nTo extend the concept of boosting in online scenario’s wherein only a chunk of data is available\nat given time, Boosting Independent Embeddings Robustly (BIER) [68] was proposed to cope\nup the online scenario’s. In BIER, a single CNN model is trained end-to-end with an online\nboosting technique. The training set in the BIER is reweighed via the negative gradient of the\nloss function to project the input spaces (images) into a collection of independent output spaces.\nTo make BIER more robust, Hierarchical Boosted deep metric learning [69] incorporated the\nhierarchical label information into the embedding ensemble which improves the performance of\nthe model on the large scale image retrieval application. Using deep boosting results in higher\ntraining time, to reduce the warm-up phase of training which trains the classiﬁer from scratch\ndeep incremental boosting [70] used transfer learning approach. This approach leveraged the\ninitial warm-up phase of each incremental base model of the ensemble during the training of\nthe network. To reduce the training time of boosting based ensembles, snapshot boosting [71]\ncombined the merits of snapshot ensembling and boosting to improve the generalization without\nincreasing the cost of training. Snapshot boosting trains each base network and combines the\noutputs via meta learner to combine the output of base learners more eﬃciently.\nLiterature shows that the boosting concept is the backbone behind well-known architectures\nlike Deep Residual networks [13, 72], AdaNet [73] . The theoretical background for the success\nof the Deep Residual networks (DeepResNet) [13] was explained in the context of boosting\ntheory [74]. The authors proposed multi-channel telescoping sum boosting learning framework,\nknown as BoostResNet, wherein each channel is a scalar value updated during rounds of boosting\n13\nYears\nAuthors\nContribution\n2014\nLiu et al. [59]\nBoosted deep belief network (DBN) as base classiﬁers for facial expression recognition.\n2014\nCortes et al. [60]\nDecision trees as base classiﬁers for binary class classiﬁcation problems.\n2014\nKuznetsov et al. [61]\nDecision trees as base classiﬁers for multiclass classiﬁcation problems.\n2015\nYang et al. [66]\nEnsemble of CNN and boosted forest for edge detection, object proposal generation, pedestrian and face detection.\n2016\nMoghimi et al. [63]\nBoosted CNN\n2016\nWalach and Wolf [67]\nCNN Boosting applied to bacterila cell images and crowd counting.\n2017\nOpitz et al. [68]\nBoosted deep independent embedding model for online scenarios.\n2017\nMosca and Magoulas [70]\nTransfer learning based deep incremental boosting.\n2017\nHan et al. [62]\nBoosting based CNN with incremental approach for facial action unit recognition.\n2018\nChen et al. [64]\nDeep boosting for image denoising with dense connections.\n2019\nChen et al. [65]\nDeep boosting for image restoration and image denoising.\n2019\nWaltner et al. [69]\nHierarchical boosted deep metric learning with hierarchical label embedding.\n2020\nZhang et al. [71]\nSnapshot boosting.\nTable 2: Boosting based ensemble models\nto minimize the multi-class error rate. The fundamental diﬀerence between the AdaNet and\nBoostResnet is that the former maps the feature vectors to classiﬁer space and boosts weak\nclassiﬁers while the latter used multi-channel representation boosting. Moreover, BoostResNet\nis more eﬃcient than DeepResnet in terms of computational time.\nThe theory of boosting was extended to online boosting in [75] and provided theoretical\nconvergence guarantees. Online boosting shows improved convergence guarantees for batch\nboosting algorithms.\nThe ensembles of bagging and boosting have been evaluated in [76]. The study evaluated the\ndiﬀerent algorithms based on the concept of bagging and boosting along with the availability of\nsoftware tools. The study highlighted the practical issues and opportunities of their feasibility in\nensemble modeling.\n4.3. Stacking\nEnsembling can be done either by combining outputs of multiple base models in some fashion\nor using some method to choose the “best” base model. Figure 4 shows the stacking technique.\nStacking is one of the integration techniques wherein the meta-learning model is used to integrate\nthe output of base models. If the ﬁnal decision part is a linear model, the staking is often referred\nto as “model blending” or simply “blending”. The concept of stacking or stacked regression was\ninitially given by [77]. In this technique, the dataset is randomly split into J equal parts. For the\njth-fold cross-validation one set is used for testing and the rest are used for training. With these\n14\ntraining testing pair subsets, we obtain the predictions of diﬀerent learning models which are\nused as the meta-data to build the meta-model. Meta-model makes the ﬁnal prediction, which is\nalso called the winner-takes-all strategy.\nStacking is a bias reducing technique [78]. Following [77], Deep convex net (DCN) [79]\nwas proposed which is a deep learning architecture composed of a variable number of modules\nstacked together to form the deep architecture. Each learning module in DCN is convex. DCN is\na stack of several modules consisting of linear input units, hidden layer non-linear units, and the\nsecond linear layer with the number of units as that of target classiﬁcation classes. The modules\nare connected layerwise as the output of the lower module is given as input to the adjacent higher\nmodule in addition to the original input data. The deep stacking network (DSN) enabling parallel\ntraining on very large scale datasets was proposed in [80], the network was named stacking\nbased as it shared the concept of “stacked generalization” [77]. The kernelized version of DCN,\nknown as kernel deep convex networks (K-DCN), was given in [81], here the number of hidden\nlayer approach inﬁnity via kernel trick. Deng et al. [81] showed that K-DCN performs better as\ncompared to the DCN. However, due to kernel trick the memory requirements increase and hence\nmay not be scalable to large scale datasets. Also, we need to optimize the hyperparameters like\nthe number of levels in the stacked network, the kernel parameters to get the optimal performance\nof the network. To leverage the memory requirements, random Fourier feature-based kernel deep\nconvex network [82] approximated the Gaussian kernel which reduces the training time and helps\nin the evaluation of K-DCN over large scale datasets. A framework for parameter estimation and\nmodel selection in kernel deep stacking networks [83] is based on the combination of model-\nbased optimization and hill-climbing approaches. Welchowski and Schmid [83] used data-driven\nframework for parameter estimation, hyperparameter tuning and model selection in kernel deep\nstacking networks. Another improvement over DSN was Tensor Deep Stacking Network (T-\nDSN) [84], here in each block of the stacked network, large single hidden layer was split into\ntwo smaller ones and then mapped bilinearly to capture the higher-order interactions among\nthe features. Comprehensive evaluation, the more detailed analysis of the learning algorithm\nand T-DSN implementation is given in [85]. Sparse coding is another popular method that is\nused in the deep learning area. The advantage of sparse representation is numerous, including\nrobust to noise, eﬀective for learning useful features, etc. Sparse Deep Stacking Network (S-\nDSN) is proposed for image classiﬁcation and abnormal detection [86, 87]. Li et al. [86], Sun\n15\net al. [87] stacked many sparse simpliﬁed neural network modules (SNNM) with mixed-norm\nregularization, in which weights are solved by using the convex optimization and the gradient\ndescent algorithm. In order to make sparse SNNM learning the local dependencies between\nhidden units, Li et al. [88] split the hidden units or representations into diﬀerent groups, which is\ntermed as group sparse DSN (GS-DSN). The DSN idea is also utilized in the Deep Reinforcement\nLearning ﬁeld. Zhang et al. [89] employed DSN method to integrate the observations from the\nformal network: Grasp network and Stacking network based on Q-learning algorithm to make\nan integrated robotic arm system do grasp and place actions. Wang et al. [90] stacked blocks\nmultiple times to increase the performance of the neural architecture search task. Zhang et al.\n[91] presents a deep hierarchical multi-patch network for image deblurring via stacking approach.\nSince there is no temporal representation of the data in DSNs, they are less eﬀective to the\nproblems where temporal dependencies exist in the input data. To embed the temporal informa-\ntion in DSNs, Recurrent Deep Stacking Networks (R-DSNs) [92] combined the advantages of\nDSNs and Recurrent neural networks (RNN). Unlike RNN which uses Back Propagation through\ntime for training the network, R-DSNs use Echo State Network (ESN) to initialize the weights\nand then ﬁne-tuning them via batch-mode gradient descent. A stacked extreme learning machine\nwas proposed in [93]. Here, at each level of the network ELM with the reduced number of hid-\nden nodes was used to solve the large scale problems. The number of hidden nodes was reduced\nvia the principal component analysis (PCA) reduction technique. Keeping in view the eﬃciency\nof stacked models, the number of stacked models based on support vector machine have been\nproposed [94, 95, 96]. Traditional models like Random Forests have also been extended to deep\narchitecture, known as deep forests [97], via stacking concept.\nIn addition to DSNs, there are some novel network architectures proposed based on the stack-\ning method, Low et al. [98] contributed a stacking-based deep neural network (S-DNN) which\nis trained without a backpropagation algorithm. Kang et al. [99] presented a model by stacking\nconditionally restricted Boltzmann machine and deep neural network, which achieved signiﬁcant\nsuperior performance with fewer parameters and fewer training samples.\n4.4. Negative Correlation Based Deep Ensemble Methods\nNegative correlation learning (NCL) [100] is an important technique for training the learning\nalgorithms. The main concept behind the NCL is to encourage diversity among the individual\n16\nmodels of the ensemble to learn the diverse aspects of the training data. NCL minimizes the em-\npirical risk function of the ensemble model via minimization of error functions of the individual\nnetworks. NCL [100] was evaluated for regression as well as classiﬁcation tasks. The evaluation\nused diﬀerent measures like simple averaging and winner-takes-all measures on classiﬁcation\ntasks and simple average combination methods for regression problems. The authors ﬁgured out\nthat winner-takes-all is better as compared to simple averaging in NCL ensemble models.\nShi et al. [101] proposed deep negative correlation learning architecture for crowd counting\nknown as D-ConvNet i.e. decorrelated convolutional networks. Here, counting is done based on\nregression-based ensemble learning from a pool of convolutional feature mapped weak regres-\nsors. The main idea behind this is to introduce the NCL concept in deep architectures. Robust\nregression via deep NCL [102] is an extension of [101] in which theoretical insights about the\nRademacher complexity are given and extended to more regression-based problems.\nBuschj¨ager et al. [103] formulated a generalized bias-variance decomposition method to con-\ntrol the diversity and smoothly interpolates. They present the Generalized Negative Correla-\ntion Learning (GNCL) algorithm, which can encapsulate many existing works in literature and\nachieve superior performance.\nThe NCL can also be employed for incremental learning tasks.\nMuhlbaier and Polikar\n[104] employed a dynamically modiﬁed weighted majority voting strategy to combine the sub-\nclassiﬁers. Tang et al. [105] proposed a negative correlation learning (NCL) based approach for\nensemble incremental learning.\n4.5. Explicit / Implicit Ensembles\nEnsembling of deep neural networks doesn’t seem to be an easy option as it may lead to in-\ncrease in computational cost heavily due to the training of multiple neural networks. High perfor-\nmance hardware’s with GPU acceleration may take weeks of weeks to train the deep networks.\nImplicit/Explicit ensembles obtain the contradictory goal wherein a single model is trained in\nsuch a manner that it behaves like ensemble of training multiple neural networks without incur-\nring additional cost or to keep the additional cost as minimum as possible. Here, the training time\nof an ensemble is same as the training time of a single model. In implicit ensembles, the model\nparameters are shared and the single unthinned network at test times approximates the model\naveraging of the ensemble models. However, in explicit ensembles model parameters are not\n17\nshared and the ensemble output is taken as the combination of the predictions of the ensemble\nmodels via diﬀerent approaches like majority voting, averaging and so on.\nDropout [106] creates an ensemble network by randomly dropping out hidden nodes from\nthe network during the training of the network. During the time of testing, all nodes are active.\nDropout provides regularization of the network to avoid overﬁtting and introduces sparsity in\nthe output vectors. Overﬁtting is reduced as it trains exponential number of models with shared\nweights and provides an implicit ensemble of networks during testing. Dropping the units ran-\ndomly avoids coadaptation of the units by making the presence of a particular unit unreliable.\nThe network with dropout takes 2 −3 times more time for training as compared to a standard\nneural network. Hence, a balance is to be set appropriately between the training time of the\nnetwork and the overﬁtting. Generalization of DropOut was given in DropConnect [107]. Un-\nlike DropOut which drops each output unit, DropConnect randomly drops each connection and\nhence, introduces sparsity in the weight parameters of the model. Similar to DropOut, Drop-\nConnect creates an implicit ensemble during test time by dropping out the connections (setting\nweights to zero) during training. Both DropOut and DropConnect suﬀer from high training time.\nTo alleviate this problem, deep networks with Stochastic depth [108] aimed to reduce the net-\nwork depth during training while keeping it unchanged during testing of the network. Stochastic\ndepth is an improvement on ResNet [13] wherein residual blocks are randomly dropped during\ntraining and bypassing these transformation blocks connections via skip connections. Swapout\n[109] is a generalization of DropOut and Stochastic depth. Swapout involves dropping of indi-\nvidual units or to skip the blocks randomly. Embarking on a distinctive approach of reducing\nthe test time, distilling the knowledge in a network [110] transferred the “knowledge” from en-\nsembles to a single model. Gradual DropIn or regularised DropIn [111] of layers starts from a\nshallow network wherein the layers are added gradually. DropIN trains the exponential number\nof thinner networks, similar to DropOut, and also shallower networks.\nAll the aforementioned methods provided an ensemble of networks by sharing the weights.\nThere have been attempts to explore explicit ensembles in which models do not share the weights.\nSnapshot ensembling [112] develops an explicit ensemble without sharing the weights. The au-\nthors exploited good and bad local minima and let the stochastic gradient descent (SGD) con-\nverge M-times to local minima along the optimization path and take the snapshots only when the\nmodel reaches the minimum. These snapshots are then ensembled by averaging at multiple local\n18\nYear\nAuthors\nContribution\n2013\nWan et al. [107]\nIntroduced DropConnect (Random skipping of connections)\n2014\nSrivastava et al. [106]\nIntroduced Dropout (Random skipping of units)\n2016\nHuang et al. [108]\nDeep networks with Stochastic depth (Random skipping of blocks)\n2016\nSingh et al. [109]\nIntroduced Swapout (Hybrid of Dropout and Stochastic depth approach)\nTable 3: Implicit / Explicit ensembles\nminima for object recognition. The training time of the ensemble is the same as that of the single\nmodel. The ensemble out is taken as the average of the output of the snapshot outputs at multiple\nlocal minimas. Random vector functional link network [113, 114] has also been explored for\ncreating the explicit ensembles [115] where diﬀerent random initialization of the hidden layer\nweights in a hierarchy diversiﬁes the ensemble predictions.\nExplicit/implicit produce ensembles out of a single network at the expense of base model\ndiversity [25] as the lower level features across the models are likely to be the same. To alleviate\nthis issue, branching based deep models [116] branch the network to induce more diversity.\nMotivated by diﬀerent initializations of the neural networks leads to diﬀerent local minima, Xue\net al. [117] proposed deep ensemble model wherein ensemble of fully convolution neural network\nover multiloss module with coarse ﬁne compensation module resulted in better segmentation of\ncentral serous chorioretinopathy lesion. Multiple neural networks with diﬀerent initializations,\nmultiple loss functions resulted in better diversity in an ensemble.\n4.6. Homogeneous & Heterogeneous ensembles\nHomogeneous ensemble (HOE) and heterogeneous ensemble (HEE) involve training a group\nof base learners either from the same family or diﬀerent families, as shown in Fig. 5 and Fig. 6,\nrespectively. Hence, each model of an ensemble must be as diverse as possible, and each base\nmodel must perform better than the random guess. The base learner can be a decision tree, neural\nnetwork, or any other learning model.\nIn homogeneous ensembles, the same base learner is used multiple times to generate the fam-\nily of base classiﬁers. However, the key issue is to train each base model such that the ensemble\nmodel is as diverse as possible, i.e. no two models are making the same error on a particular\ndata sample. The two most common ways of inducing randomness in a homogeneous ensemble\n19\nFigure 5: Homogeneous ensemble (HOE) has models based on the same algorithm, but each\nindividual model are fed with distinct datasets.\nFigure 6: The components in heterogeneous ensemble (HEE) share the same dataset but consists\nof various algorithms.\nare either sampling of the training set multiple times, thereby training each model on a diﬀerent\nbootstrapped sample of the training data or sampling the feature space of the training data and\ntrain each model on a diﬀerent feature subset of the training data. In some ensemble models\nlike Random forest [5] used both these techniques for introducing diversity in the ensemble of\ndecision trees. In neural networks, training models independently with diﬀerent initialization\nof the models also induces diversity. However, deep learning models have high training costs\n20\nand hence, training of multiple deep learning models is not a feasible option. Some attempts,\nlike horizontal vertical voting of deep ensembles [118] have been made to obtain ensembles of\ndeep models without independent training. Temporal ensemble [119] trains multiple models with\ndiﬀerent input augmentation, diﬀerent regularisation and diﬀerent training epochs. Training of\nmultiple deep neural networks for image classiﬁcation [120] and for disease prediction [121]\nshowed that better performance is achieved via an ensemble of multiple networks and averaging\nthe outputs. Despite these models, training multiple deep learning models for ensemble is an\nuphill task as millions or billions of parameters need to be optimized. Hence, some studies have\nused deep learning in combination with traditional models to build heterogeneous ensemble mod-\nels, enjoying the beneﬁts of lower computation and higher diversity. Heterogeneous ensemble\nfor default prediction [122] is an ensemble of the extreme gradient boosting, deep neural network\nand logistic regression. Heterogeneous ensemble for text classiﬁcation [123] is an ensemble of\nmultivariate Bernoulli na¨ıve Bayes (MVNB), multinomial na¨ıve Bayes (MNB), support vector\nmachine (SVM), random forest (RF), and convolutional neural network (CNN) learning algo-\nrithms. Using diﬀerent perspectives of data, model and decision fusion, heterogeneous deep\nnetwork fusion [124] showed that complex heterogeneous fusion architectures are more diverse\nand hence, show better generalization performance. Furthermore, Seijo-Pardo et al. [125] em-\nployed both homogeneous and heterogeneous ensembles for feature selection. Zhao et al. [126]\nsuggested that the heterogeneous bagging based ensemble strategy performs better than boost-\ning based Learn++ algorithms and some other NCL methods. Other examples that employed\nhomogeneous ensemble methods were used to deal with the presence of incremental tasks, such\nas concept drift [127], power load forecasting [128, 129], myoelectric prosthetic hands surface\nelectromyogram characteristics [130], etc. Das et al. [131] proposed an ensemble incremen-\ntal learning with pseudo-outer-product fuzzy neural network for traﬃc ﬂow prediction, real-life\nstock price, and volatility predictions, etc.\n4.7. Decision Fusion Strategies\nEnsemble learning trains several base learners and aggregates the outputs of base learners\nusing some rules. The rule used to combine the outputs determines the eﬀective performance\nof an ensemble. Most of the ensemble models focus on the ensemble architecture followed by\ntheir naive averaging to predict the ensemble output. However, naive averaging of the models,\nfollowed in most of the ensemble models, is not data adaptive and leads to less optimal perfor-\n21\nmance [132] as it is sensitive to the performance of the biased learners. As there are billions of\nhyperparameters in deep learning architecture, the issue of overﬁtting may lead to the failure of\nsome base learners. Hence, to overcome these issues, approaches like Bayes optimal classiﬁer\nand super learner have been followed [132].\nThe diﬀerent approaches followed in the literature for combining the outputs of the ensemble\nmodels are:\n4.7.1. Unweighted Model Averaging\nUnweighted averaging of the outputs of the base learners in an ensemble is the most followed\napproach for fusing the decisions in the literature. Here, the outcomes of the base learners are\naveraged to get the ﬁnal prediction of the ensemble model. Deep learning architectures have high\nvariance and low bias, thus, simple averaging of the ensemble models improve the generalization\nperformance due to the reduction of the variance among the models.\nThe averaging of the base learners is performed either on the outputs of the base learners\ndirectly or on the predicted probabilities of the classes via softmax function:\nPj\ni = softmax j(Oi) =\nOj\ni\nPK\nk=1 exp(Oj\nk)\n(4)\nwhere Pj\ni is the probability outcome of the ith unit on the jth base learner, Oj\ni is the output of the\nith unite of the jth base learner and K is the number of the classes.\nUnweighted averaging is a reasonable choice when the performance of the base learners is\ncomparable, as suggested in [13, 133, 134]. However, when the ensemble contains heterogeneous\nbase learners naive unweighted averaging may result in suboptimal performance as it is aﬀected\nby the performance of the weak learners and the overconﬁdent learners [132]. The adaptive\nmetalearner should be good enough to adaptively combine the strengths of the base learners as\nsome learners may have lower overall performance but maybe good at the classiﬁcation of certain\nsubclasses and hence, leading to better overall performance.\n4.7.2. Majority Voting\nSimilar to unweighted averaging, majority voting combines the outputs of the base learners.\nHowever, instead of taking the average of the probability outcomes, majority voting counts the\nvotes of the base learners and predicts the ﬁnal labels as the label with the majority of votes.\nIn comparison to unweighted averaging, majority voting is less biased towards the outcome of\n22\na particular base learner as the eﬀect is mitigated by majority vote count. However, favouring\nof a particular event by most of the similar base learners or dependent base learners leads to the\ndominance of the event in the ensemble model. In majority voting, the analysis by Kuncheva\net al. [135] showed that the pairwise dependence among the base learners plays an important\nrole and for the classiﬁcation of images, the prediction of shallow networks is more diverse as\ncompared to the deeper networks [136]. Hence, Ju et al. [132] hypothesised that the performance\nof the majority voting based shallows ensemble models is better as compared to the majority\nbased deep ensemble models.\nVoting methods have also started to be integrated with semi-supervised deep learning. Li\net al. [137] proposed an ensemble semi-supervised deep acoustic models for in automatic speech\nrecognition. Wang et al. [138] explored an ensemble self-learning method to enhance semi-\nsupervised performance and extract adverse drug events from social media in [139]. In the semi-\nsupervised classiﬁcation area, the author proposed a deep coupled ensemble learning method\nwhich is combined with complementary consistency regularization and gets the state of the art\nperformance in [140]. Some results have also been achieved with semi-supervised ensemble\nlearning on some datasets where the annotation is costly. Pio et al. [141] employed an ensemble\nmethod to improve the reliability of miRNA:miRNA predicted interactions.\nFurthermore, the multi-label classiﬁcation [142] problem is also a major point addressed by\nthe voting method, a typical application is the RAndom k-labELsets (RAKEL) algorithm [143].\nThe author trained several single-label classiﬁers using small random subsets of actual labels.\nThen the ﬁnal output is carried out by a voting scheme based on the predictions of these single\nclassiﬁers. There are also many variants of RAKEL proposed in recent years [144, 145, 146].\nShi et al. [147] proposed a solution for multi-label ensemble learning problem, which construct\nseveral accurate and diverse multi-label based basic classiﬁers and employ two objective func-\ntions to evaluate the accuracy and diversity of multi-label base learners. Another work [148]\nproposed an ensemble multi-label classiﬁcation framework based on variable pairwise constraint\nprojection. Xia et al. [149] proposed a weighted stacked ensemble scheme that employs the\nsparsity regularization to facilitate classiﬁer selection and ensemble construction. Besides, there\nare many applications of ensemble multi-label methods. Some publications employ multi-label\nensemble classiﬁers to explore the protein, such as protein subcellular localization [150], protein\nfunction prediction [151], etc. The Muli-label classiﬁer is also utilized in predicting the drug side\n23\neﬀects [152], predicting the gene prediction[153], etc. Moreover, there is another critical ensem-\nble multi-label algorithm called ensemble classiﬁer chains (ECC) [154]. This method involves\nbinary classiﬁers linked along a chain. The ﬁrst classiﬁer is trained using only the input data,\nand then each subsequent classiﬁer is trained on the input space and all previous classiﬁers in the\nchain. The ﬁnal prediction is obtained by the integration of the predictions and selection above a\nmanually set threshold. Chen et al. [155] propose an ensemble application of convolutional and\nrecurrent neural networks to capture both the global and local textual semantics and to model\nhigh-order label correlations.\n4.7.3. Bayes Optimal Classiﬁer\nIn Bayesian method, hypothesis hj of each base learner with the conditional distribution of\ntarget label t given x. Let hj be the hypothesis generated on the training data D evaluated on test\ndata (x, t), mathematically, hj(t|x) = P[y|x, hj, D]. With Bayes rule, we have\nP(t|x, D) ∝\nX\nhj\nP[t|h j, x, D]P[D|hj]P[hj]\n(5)\nand the Bayesian Optimal classiﬁer is given as:\nargmax\nt\nX\nhj\nP[t|h j, x, D]P[D|hj]P[hj],\n(6)\nwhere P[D|h j] = Π(t,x)∈Dhj(t|x) is the likelihood of the data under h j. However, due to overﬁtting\nissues this might be not a good measure. Hence, training data is divided into two sets-one for\ntraining the model and the other for evaluating the model. Usually validation set is used to tune\nthe hyperparameters of the model.\nChoosing prior probabilities in Bayes optimal classiﬁer is diﬃcult and hence, usually set\nto uniform distribution for simplicity. With a large sample size, one hypothesis tends to give\nlarger posterior probabilities than others and hence the weight vector is dominated by a single\nbase learner and hence Bayes optimal classiﬁer would behave as the discrete superlearner with a\nnegative likelihood loss function.\n4.7.4. Stacked Generalization\nStacked generalization [77] works by deducing the biases of the generalizer(s) with respect\nto a provided learning set. To obtain the good linear combination of the base learners in regres-\nsion, cross-validation data and least squares under non-negativity constraints was used to get the\n24\noptimal weights of combination [156]. Consider the linear combination of the predictions of the\nbase learners f1, f2, · · · , fm given as:\nfstacking(x) =\nm\nX\nj=1\nwj f j(x)\n(7)\nwhere w is the optimal weight vector learned by the meta learner.\n4.7.5. Super Learner\nInspired by the cross validation for choosing the optimal classiﬁer, Van der Laan et al. [157]\nproposed super learner which is weighted combination of the predictions of the base learner.\nUnlike the stacking approach, it uses cross validation approach to select the optimal weights for\ncombining the predictions of the base learners.\nWith smaller datasets, cross validation approach can be used to optimize the weights. How-\never, with the increase in the size of the data and the number of base learners in the model, it\nmay not be a feasible option. Instead of optimizing the V-fold cross validation, single split cross\nvalidation can also be used for optimizing the weights for optimal combination [158]. In deep\nlearning models, usually, a validation set is used to evaluate the performance instead of using the\ncross validation.\nAnother application ﬁeld for super learner is in Reinforcement Learning With the develop-\nment of Deep learning, some researchers have implemented deep reinforcement learning, which\ncombines deep learning with a Q-learning algorithm [159]. Ensemble methods in deep Q learn-\ning have decent performance. Chen et al. [160] proposed an ensemble network architecture\nfor deep reinforcement learning. The integrated network includes Temporal Ensemble and Tar-\nget Values Ensemble. Develop a human-like chat robot is a challenging job, by incorporat-\ning deep reinforcement learning and ensemble method, Cuay´ahuitl et al. [161] integrated 100\ndeep reinforcement learning agents, the agents are trained based on clustered dialogues. They\nalso demonstrate the ensemble of DRL agents has better performance than the single variant or\nSeq2Seq model. Stock trading is another topic where ensemble deep reinforcement learning has\nachieved a promising result. Carta et al. [162] found the single supervised classiﬁer is inade-\nquate to deal with the complex and volatile stock market. They employed hundreds of neural\nnetworks to pre-process the data, then they combined several reward-based meta learners as a\ntrading agency. Moreover, Yang et al. [163] trained an ensemble trading agency based on three\ndiﬀerent metrics: Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), and Deep\n25\nDeterministic Policy Gradient (DDPG). The ensemble strategy combines the advantages of the\nthree diﬀerent algorithms. Besides, some researchers try to use ensemble strategy to solve the\ndisease-prediction problem. The proposed model in [164] consists of several sub-models which\nare in response to diﬀerent anatomical parts.\n4.7.6. Consensus\nFigure 7: The process of consensus clustering. An ensemble of diﬀerent clustering results can\nbe combined by a consensus approach.\nUnsupervised learning is another group of machine learning techniques. The fundamental\ndiﬀerence between it and supervised learning is that unsupervised learning usually handles train-\ning samples without corresponding labels. Therefore, the primary usage of unsupervised learning\nis to do clustering. The reason why ensemble methods are employed is to combine some weak\nclusters into strong one. To create diverse clusters, several approaches can be applied: using\ndiﬀerent sampling data, using diﬀerent subsets of the original features, and employing diﬀer-\nent clustering methods [165]. Sometimes, even some random noise can be added to these base\nmodels to increase randomness, which is good for ensemble methods according to [166]. After\nreceiving all the outputs from each cluster, various consensus functions can be chosen to obtain\nthe ﬁnal output based on the user’s requirement [22]. The ensemble clustering is also known as\nconsensus clustering Fig. 7.\nZhou and Tang [167] explored ensemble methods for unsupervised learning and developed\nfour diﬀerent approaches to combine the outputs of these clusters. In recent years, some new en-\nsemble clustering methods have been proposed that illustrated the priority of ensemble learning\n[168, 169, 170]. Most of the clustering ensemble method is based on the co-association matrix\nsolution, which can be regarded as a graph partition problem. Besides, there is some research\n26\nfocus on integrating the deep structure and ensemble clustering method. Liu et al. [171, 172]\nﬁrstly showed that ensemble unsupervised representation learning with deep structure can be ap-\nplied in large scale data. Then the author combined the method with auto-encoder and extends\nit to the vision ﬁeld. Shaham et al. [173] ﬁrst demonstrated that some crowdsourcing algorithms\ncan be replaced by a Restricted Boltzmann Machine with a single hidden neuron, then propose\nan RBM-based Deep Neural Net (DNN) used for unsupervised ensemble learning. The unsu-\npervised ensemble method also makes some contribution to the ﬁeld of Natural Language Pro-\ncessing. Alami et al. [174] demonstrated that the ensemble of unsupervised deep neural network\nmodels that use Sentence2Vec representation as the input has the best performance according to\nthe experiments. Hassan et al. [175] proposed a module that includes four semantic similarity\nmeasures, which improves the performance on the semantic textual similarity (STS) task. The\nunsupervised ensemble method is also widely used for tasks that lack annotation, such as the\nmedical image. Ahn et al. [176] proposed an unsupervised feature learning method integrated\nensemble approach with a traditional convolutional neural network. Lahiri et al. [177] employed\nunsupervised hierarchical feature learning with ensemble sparsely autoencoder on retinal blood\nvessels segmentation task, meanwhile, Liu et al. [178] also propose an unsupervised ensemble\narchitecture to automatically segment retinal vessel. Besides, there are also some ensemble deep\nmethods working on localization predicting for long non-coding RNAs [179]. Hu and Suganthan\n[180] extended the ensemble random vector functional link to unsupervised tasks. The authors\nemploy manifold regularization to re-represent the original features, and then use the Kuhn-\nMunkre algorithm with consensus clustering to ensemble the clustering results from multiple\nhidden layers.\n4.7.7. Query-By-Committee\nActive Learning is another popular topic in the deep learning area, which is also often used\nin conjunction with semi-supervised learning and ensemble learning. The key sight of this is\nto make the algorithm learning from less annotated data. Some conventional active learning\nalgorithms, such as Query-By-Committee (as shown in Fig 8), have already adopted the idea of\nensemble learning. Melville and Mooney [181, 182] explored an ensemble method that builds\na diverse committee. Beluch et al. [183] discussed the power of ensembles for active learning\nis signiﬁcantly better than Monte-Carlo Dropout and geometric approaches. Sharma and Rani\n[184] show some applications in drug-target interaction prediction. Ensemble active learning is\n27\nFigure 8: Query-by-committee in Active Learning. Sampling with replacement is used to parti-\ntion the labeled training data set into training splits. The committee determines whether to label\nthe data based on the output of several algorithms.\nalso available to conquer the concept drift and class imbalance problem [185].\n5. Applications\nIn this section, we brieﬂy present the applications of deep ensemble models across diﬀerent\ndomains in a tabular form. Ensemble deep models have been implemented in several domains\nand therefore, in broad sense, we have classiﬁed the application domains into ﬁve categories,\ni.e., health care, speech, image classiﬁcation, forecasting and the rest models are listed in others\ncategory. Table 4 gives the information about the ensemble deep models that have been im-\nplemented in health care domain. Here, several papers are based on heterogeneous ensemble\ntechnique. It reveals that using diﬀerent family’s models into a single frame perform better in\nhealth care domain. Recently, ensemble deep techniques have been successful and have shown\ngood performance in health care domain. The models which have been implemented for speech\ntask have been given in Table 5 and most of the ensemble approaches are based on stacking\ntechnique. Table 6 contains the ensemble deep models that have been implemented in speech\nareas. Models that have been implemented in forecasting and other domains have been given\nin Table 7 and Table 8, respectively. Fig. 9 shows the percentages of the application domains.\n28\nOther Applications\n36.0%\nForecasting\n9.0%\nHealth Care\n27.0%\nSpeech\n5.6%\nImage Classification\n22.5%\nFigure 9: Ensemble-based approach in diﬀerent areas. Data from Tables 4 to 8.\nThe statistics reveals that diﬀerent ensemble deep techniques have been used in diﬀerent areas.\nA larger number of models, i.e. 27% of the ensemble deep models, have been implemented in\nhealth care domain and 5.6% percent of the models for speech application and 22.5% of the\nmodels for image classiﬁcation task. Moreover, 9% models have been used in forecasting and\n36% in other applications areas, i.e., information retrieval, emotion recognition, text categoriza-\ntion and so on. Fig. 10 shows the ensemble strategies in percentage. In ensemble learning,\nthere are several ways to integrate the outcomes of the models in an ensemble. In the literature,\nresearchers have proposed diﬀerent techniques of decision fusion according to diﬀerent areas\nof application. Bagging, boosting and stacking are the classical ensemble techniques. Based\non these three techniques, researchers have developed several other techniques also. Boosting\n(18.2%), stacking (12.5%) and bagging (4.5%) techniques have been implemented in ensemble\ndeep framework. Heterogeneous and implicit ensemble are also popular for making an eﬃcient\nensemble model and their contribution are as follows 11.4% and 10.2%, respectively. The rest\nensemble techniques are, i.e. unsupervised (3.4%), NCL (3.4%), reinforcement (1.1%), active\nlearning (1.1%), explicit ensemble (1.1%) and homogeneous ensemble (3.4%).\n6. Conclusions and future works\nIn this paper, we reviewed the recent developments of ensemble deep learning models. The\ntheoretical background of ensemble learning has been elaborated to understand the success of\nensemble learning. The various approaches ranging from traditional ones like bagging, boosting\nto the recent novel approaches like implicit/explicit ensembles, heterogeneous ensembles, have\n29\nYear\nAuthor\nTitle\nApproach\nArea\n2014\nZheng et al. [186]\nHIBAG—HLA genotype imputation with attribute bagging\nBagging\nGenotype Imputation\n2014\nCortes et al. [60]\nDeep Boosting\nBoosting\nClassiﬁcation\n2015\nZhang et al. [152]\nPredicting drug side eﬀects by multi-label learning and ensemble\nlearning\nDecision Fusion\nPredict the drug side eﬀects\n2016\nGuo et al. [150]\nHuman protein subcellular localization with integrated source and\nmulti-label ensemble classiﬁer.\nDecision Fusion\nProtein subcellular localization prediction\n2016\nLahiri et al. [177]\nDeep neural ensemble for retinal vessel segmentation in fundus im-\nages towards achieving label-free angiography\nDecision Fusion\nMedical image segmentation\n2017\nCabria and Gondra [187]\nMRI segmentation fusion for brain tumor detection\nHeterogeneous ensemble\nMRI segmentation\n2018\nGrassmann et al. [121]\nA deep learning algorithm for prediction of age-related eye disease\nstudy severity scale for age-related macular degeneration from color\nfundus photography\nHomogeneous ensemble\nDisease prediction\n2018\nCao et al. [179]\nThe lnclocator: a subcellular localization predictor for long non-\ncoding RNAs based on a stacked ensemble classiﬁer\nDecision Fusion\nSubcellular localization predictor\n2018\nSharma and Rani [184]\nBe-dti’: Ensemble framework for drug target interaction prediction\nusing dimensionality reduction and active learning\nActive learning\nDrug target interaction prediction\n2019\nAhn et al. [176]\nUnsupervised feature learning with k-means and an ensemble of\ndeep convolutional neural networks for medical image classiﬁcation\nDecision Fusion\nMedical image classiﬁcation\n2019\nLiu et al. [178]\nUnsupervised ensemble strategy for retinal vessel segmentation.\nUnsupervised\nMedical image classiﬁcation\n2020\nShalbaf and Vafaeezadeh [188]\nAutomated detection of COVID-19 using ensemble of transfer\nlearning with deep convolutional neural network based on CT scans\nHeterogeneous Ensemble\nDetection of COVID-19\n2020\nAli et al. [189]\nA smart healthcare monitoring system for heart disease prediction\nbased on ensemble deep learning and feature fusion\nBoosting\nHeart disease prediction\n2021\nZhou et al. [190]\nThe ensemble deep learning model for novel COVID-19 on CT im-\nages\nHeterogeneous Ensemble\nDetection of COVID-19\n2021\nLi et al. [191]\nIntelligent Fault Diagnosis by Fusing Domain Adversarial Training\nand Maximum Mean Discrepancy via Ensemble Learning\nHeterogeneous Ensemble\nFault diagnosis\n2021\nDas et al. [192]\nAutomatic COVID-19 detection from X-ray images using ensemble\nlearning with convolutional neural network\nHeterogeneous Ensemble\nDetection of COVID-19\n2022\nSukegawa et al. [193]\nIdentiﬁcation of osteoporosis using ensemble deep learning model\nwith panoramic radiographs and clinical covariates\nDecision Fusion\nIdentiﬁcation of osteoporosis\n2022\nGao et al. [194]\nVessel segmentation for X-ray coronary angiography using ensem-\nble methods with deep learning and ﬁlter-based features\nBoosting\nVessel segmentation\n2022\nRath et al. [195]\nImproved heart disease detection from ECG signal using deep learn-\ning based ensemble model\nHeterogeneous Ensemble\nHeart disease detection\n2022\nTanveer et al. [196]\nClassiﬁcation of Alzheimer’s Disease Using Ensemble of Deep\nNeural Networks Trained Through Transfer Learning\nHeterogeneous Ensemble\nClassiﬁcation of Alzheimer’s Disease\n2022\nRai and Chatterjee [197]\nHybrid CNN-LSTM deep learning model and ensemble technique\nfor automatic detection of myocardial infarction using big ECG data\nHeterogeneous Ensemble\nDetection of myocardial infarction\n2022\nGanaie and Tanveer [198]\nEnsemble deep random vector functional link network using privi-\nleged information for Alzheimer’s disease diagnosis\nImplicit ensemble\nDiagnosis of Alzheimer’s disease\nTable 4: Applications in health care\nYear\nAuthor\nTitle\nApproach\nArea\n2012\nTur et al. [199]\nTowards deeper understanding: Deep convex networks for semantic\nutterance classiﬁcation\nStacking\nSemantic Utterance Classiﬁcation\n2012\nDeng et al. [200]\nUse of kernel deep convex networks and end-to-end learning for\nspoken language understanding\nStacking\nSpoken Language Understanding\n2014\nDeng and Platt [201]\nEnsemble deep learning for speech recognition\nStacking\nSpeech Recognition\n2014\nPalangi et al. [92]\nRecurrent Deep-Stacking Networks for sequence classiﬁcation\nStacking\nSequence classiﬁcation\n2017\nLi et al. [137]\nSemi-supervised ensemble DNN acoustic model training\nDecision Fusion\nSpeech Recognition\nTable 5: Applications in speech\n30\nYear\nAuthor\nTitle\nApproach\nArea\n2012\nCiregan et al. [120]\nMulti-column deep neural networks for image classiﬁcation\nHomogeneous ensemble\nClassiﬁcation\n2013\nWan et al. [107]\nRegularization of Neural Networks using DropConnect\nImplicit ensemble\nImage recognition\n2014\nSrivastava et al. [106]\nDropout: a simple way to prevent neural networks from overﬁtting\nImplicit Ensemble\nComputer vision, speech recognition\ndocument classiﬁcation and computational biology\n2014\nLiu et al. [59]\nFacial expression recognition via a boosted deep belief network\nBoosting\nFacial expression recognition\n2015\nLi et al. [86]\nSparse deep stacking network for image classiﬁcation\nStacking\nImage Classiﬁcation\n2015\nYang et al. [66]\nConvolutional channel features\nBoosting\nPedestrian detection, face detection,\nedge detection and object proposal generation\n2016\nMoghimi et al. [63]\nBoosted Convolutional Neural Networks\nBoosting\nClassiﬁcation\n2016\nHuang et al. [108]\nDeep networks with stochastic depth\nImplicit ensemble\nClassiﬁcation\n2016\nHe et al. [13]\nDeep residual learning for image recognition\nImplicit ensemble\nclassiﬁcation, and object detection\n2016\nSingh et al. [109]\nSwapout: Learning an ensemble of deep architectures\nImplicit ensemble\nclassiﬁcation\n2016\nSmith et al. [111]\nGradual dropin of layers to train very deep neural networks\nImplicit ensemble\nClassiﬁcation\n2016\nLaine and Aila [119]\nTemporal ensembling for semi-supervised learning\nHomogeneous ensemble\nClassiﬁcation\n2016\nTang et al. [164]\nInquire and diagnose: Neural symptom checking ensemble using\ndeep reinforcement learning\nDecision Fusion\nInquire symptoms and diagnose diseases\n2017\nHuang et al. [112]\nSnapshot ensembles: train 1, get M for free\nExplicit ensemble\nClassiﬁcation\n2017\nMosca and Magoulas [70]\nDeep incremental boosting\nBoosting\nClassiﬁcation\n2018\nBeluch et al. [183]\nThe power of ensembles for active learning in image classiﬁcation\nDecision Fusion\nImage classiﬁcation\n2019\nAmin-Naji et al. [202]\nEnsemble of CNN for multi-focus image fusion\nDecision Fusion\nImage Classiﬁcation\n2019\nLi et al. [140]\nSemi-supervised deep coupled ensemble learning with classiﬁca-\ntion landmark exploration.\nDecision Fusion\nImage classiﬁcation\n2020\nWang et al. [90]\nParticle swarm optimisation for evolving deep neural networks for\nimage classiﬁcation by evolving and stacking transferable blocks.\nStacking\nImage Classiﬁcation\nTable 6: Applications in image classiﬁcation\nYear\nAuthor\nTitle\nApproach\nArea\n2014\nQiu et al. [203]\nEnsemble deep learning for regression and time series forecasting\nDecision Fusion\nRegression and Time Series Forecasting\n2016\nGrmanov´a et al. [129]\nIncremental ensemble learning for electricity load forecasting\nDecision Fusion\nElectricity load forecasting\n2017\nQiu et al. [204]\nEmpirical Mode Decomposition based ensemble deep learning for\nload demand time series forecasting\nDecision Fusion\nLoad demand forecasting\n2017\nLiu et al. [205]\nA Flood Forecasting Model Based on Deep Learning Algorithm via\nIntegrating Stacked Autoencoders with BP Neural Network\nStacking\nFlood Forecasting\n2018\nQiu et al. [128]\nEnsemble incremental learning random vector functional link net-\nwork for short-term electric load forecasting.\nDecision Fusion\nElectric load forecasting\n2020\nCarta et al. [162]\nA multi-layer and multi-ensemble stock trader using deep learning\nand deep reinforcement learning\nImplicit ensemble\nStock trader\n2020\nYang et al. [163]\nDeep reinforcement learning for automated stock trading: An en-\nsemble strategy\nDecision Fusion\nStock trading agency\n2021\nBhusal et al. [206]\nDeep ensemble learning-based approach to real-time power system\nstate estimation\nStacking\nElectric Power\n2022\nSingla et al. [207]\nAn ensemble method to forecast 24-h ahead solar irradiance using\nwavelet decomposition and BiLSTM deep learning network\nDecision Fusion\nForecasting\nTable 7: Applications in forecasting\n31\nYear\nAuthor\nTitle\nApproach\nArea\n2013\nDeng et al. [208]\nDeep stacking networks for information retrieval\nStacking\nInformation Retrieval\n2014\nKuznetsov et al. [61]\nMulti-class deep boosting\nBoosting\nClassiﬁcation\n2014\nWang et al. [209]\nSentiment classiﬁcation The contribution of ensemble learning\nBagging, Boosting\nSentiment classiﬁcation\n2015\nZareapoor and Shamsolmoali [210]\nApplication of Credit Card Fraud Detection: Based on Bagging En-\nsemble Classiﬁer\nBagging\nCredit Card Fraud Detection\n2016\nYin et al. [211]\nRecognition of emotions using multimodal physiological signals\nand an ensemble deep learning model\nDecision Fusion\nEmotions Recognition\n2016\nLiu et al. [172]\nA deep learning approach to unsupervised ensemble learning\nDecision Fusion\nClustering\n2016\nWalach and Wolf [67]\nLearning to count with CNN boosting\nBoosting\nObject counting in images\n2017\nHan et al. [62]\nIncremental boosting convolutional neural network for facial action\nunit recognition\nBoosting\nFacial action unit recognition\n2017\nChen et al. [155]\nEnsemble application of convolutional and recurrent neural net-\nworks for multi-label text categorization\nDecision Fusion\nText Categorization.\n2017\nOpitz et al. [68]\nBier-boosting independent embeddings robustly\nBoosting\nImage retrieval\n2018\nShi et al. [101]\nCrowd Counting with Deep Negative Correlation Learning\nNegative correlation learning\nCrowd Counting\n2018\nKazemi et al. [212]\nNovel genetic-based negative correlation learning for estimating\nsoil temperature\nNegative correlation learning\nSoil Temperature Estimation\n2018\nRandhawa et al. [213]\nCredit Card Fraud Detection Using AdaBoost and Majority Voting\nBoosting\nCredit Card Fraud Detection\n2018\nSun et al. [87]\nSparse Deep Stacking Network for Fault Diagnosis of Motor\nStacking\nFault Diagnosis\n2018\nChen et al. [64]\nDeep boosting for image denoising\nBoosting\nImage denoising\n2018\nLi et al. [122]\nHeterogeneous ensemble for default prediction of peer-to-peer lend-\ning in China\nHeterogeneous ensemble\nDefault prediction\n2018\nKilimci and Akyokus [123]\nDeep learning-and word embedding-based heterogeneous classiﬁer\nensembles for text classiﬁcation\nHeterogeneous ensemble\nClassiﬁcation\n2018\nLiu et al. [139]\nSSEL-ADE: a semi-supervised ensemble learning framework for\nextracting adverse drug events from social media\nDecision Fusion\nExtracting adverse drug events\n2019\nMart´ın et al. [214]\nAndroid malware detection through hybrid features fusion and\nensemble classiﬁers: The AndroPyTool framework and the Om-\nniDroid dataset\nDecision Fusion\nAndroid malware detection\n2019\nCuay´ahuitl et al. [161]\nEnsemble-based deep reinforcement learning for chatbots\nReinforcement\nChat robot\n2019\nChen et al. [65]\nReal-world image denoising with deep boosting\nBoosting\nImage denoising\n2019\nWaltner et al. [69]\nHiBsteR: Hierarchical Boosted Deep Metric Learning for Image\nRetrieval\nBoosting\nImage Retrieval\n2019\nWang et al. [215]\nAdaboost-based security level classiﬁcation of mobile intelligent\nterminals\nAdaboost\nSecurity Level Classiﬁcation\n2019\nChen et al. [216]\nNovel Hybrid Integration Approach of Bagging-Based Fisher’s Lin-\near Discriminant Function for Groundwater Potential Analysis\nBagging\nGroundwater Potential Analysis\n2019\nShi et al. [115]\nRandom vector functional link neural network based ensemble deep\nlearning\nImplicit ensemble\nClassiﬁcation\n2019\nZhang et al. [91]\nDeep stacked hierarchical multi-patch network for image deblurring\nStacking\nDeblurring Image\n2019\nAlami et al. [174]\nEnhancing unsupervised neural networks based text summarization\nwith word embedding and ensemble learning\nDecision Fusion\nText summarization\n2019\nHassan et al. [175]\nUests:\nAn unsupervised ensemble semantic textual similarity\nmethod\nDecision Fusion\nSemantic textual similarity\n2020\nZhang et al. [89]\nGrasp for stacking via deep reinforcement learning\nStacking\nRobotic arm control\n2020\nZhang et al. [71]\nSnapshot boosting: a fast ensemble framework for deep neural net-\nworks\nBoosting\nComputer vision (CV) and the natural language processing (NLP) tasks\n2021\nTsogbaatar et al. [217]\nDeL-IoT: A deep ensemble learning approach to uncover anomalies\nin IoT\nDecision Fusion\nIoT\n2022\nWen et al. [218]\nA new ensemble convolutional neural network with diversity regu-\nlarization for fault diagnosis\nSnapshot ensemble learning\nFault diagnosis\n2022\nHu and Suganthan [180]\nRepresentation Learning Using Deep Random Vector Functional\nLink Networks for Clustering\nDecision Fusion\nClustering\nTable 8: Other applications\n32\nHomogeneous Ensemble\n3.4%\nExplicit Ensemble\n1.1%\nActive Learning\n1.1%\nReinforcement\n1.1%\nNegative Correlation Learning\n3.4%\nUnsupervised\n3.4%\nImplicit Ensemble\n10.2%\nHeterogeneous Ensemble\n11.4%\nBagging\n4.5%\nStacking\n12.5%\nDecision Fusion\n29.5%\nBoosting\n18.2%\nFigure 10: Analysis of the applications of various ensemble methods. Data from Tables 4 to 8.\nled to better performance of deep ensemble models. We also reviewed the applications of the\ndeep ensemble models in diﬀerent domains.\nAlthough deep ensemble models have been applied across diﬀerent domains, there are several\nopen problems which can be explored in the future to ﬁll the gap. Big data [219] is still a\nchallenging problem, one can explore the beneﬁts of deep ensemble models for learning the\npatterns using the techniques like implicit deep ensemble to maximize the performance in both\ntime and generalization aspects.\nDeep learning models are diﬃcult to train than shallow models as large number of weights\ncorresponding to diﬀerent layers need to be tuned. Creating deep ensemble models may further\ncomplicate the problem. Hence, randomized models can be explored to overcome the training\ncost. Bagging based deep ensemble may incur heavy training time for optimizing the ensemble\nmodels. Hence, one can investigate the alternate ways of inducing diversity in the base mod-\nels with lesser training cost. Randomized learning modules like random vector functional link\nnetwork [113] are best suited for creating the ensemble models as randomized models lead to a\nsigniﬁcant variance reduction. Also, the hidden layers are randomly initialized, hence, can be\nused to create deep ensembles without incurring any additional cost of training [115]. Random-\n33\nized modules can be further explored using diﬀerent techniques like implicit / explicit ensembles\n[115], stacking based ensembles [220]. However, there are still open directions which can be\nworked upon like negative correlation learning, heterogeneous ensembles and so on.\nImplicit/explicit ensembles are faster compared to training of multiple deep models. How-\never, creating diversity within a single model is a big challenge. One can explore the methods\nto induce more diversity among the learners within these ensembles like branching based deep\nmodels [116]. Investigate the extension of explicit/implicit ensembles to traditional models.\nFollowing the stacking based approach, Deep convex net (DCN) [79], traditional methods\nlike random forest [5, 97], support vector machines [94, 95, 96] have been extended to deep\nlearning architectures which resulted in improved performance. One can investigate these tradi-\ntional models for creating the deep ensemble models.\nAnother big challenge of ensemble deep learning lies in model selection for building the\nensemble architecture, homogeneous and heterogeneous ensembles represent two diﬀerent ways\nfor choosing the model. However, to answer how many diﬀerent algorithms, and how many base\nlearners in the ensemble architecture, are still problem-dependent. Finding a criterion for model\nselection in ensemble deep learning should be an important target for researchers in the next few\nyears. Since most of the models focus on developing the architectures with little attention towards\nhow to combine the base learners prediction is still unanswered. Hence, one can investigate the\neﬀect of diﬀerent fusion strategies on the prediction of an ensemble output.\nFor unsupervised ensemble learning or consensus clustering, the ensemble approaches in-\nclude but are not limited to: Hyper-graph partitioning, Voting approach, Mutual information, etc.\nConsensus clustering is a powerful tool and it can improve performance in most cases. However,\nthere are many concerns remain to be tackled, it is exquisitely sensitive, which might assert as an\napparent structure without obvious demarcation or declared cluster stable without cluster resis-\ntance. Besides, current method cannot handle some complex but possible scenarios, such as the\nboundary samples are assigned to the single cluster, clusters do not intersect and the methods are\nnot able to represent outliers. These are the possible research directions for future work.\nThe problem of semi-supervised ensemble domains has not been extensively studied yet, and\nmost of the literature shows that semi-supervised ensemble methods are mainly used in cases\nwhere there is insuﬃcient labeling data. Also, combining the semi-supervision with some other\nmachine learning methods, such as active learning, is a direction for future research.\n34\nReinforcement learning is another popular topic recently. The idea of integrating model-\nbased reinforcement learning with ensemble learning has been used with promising results in\nmany applications, but there is little integration of planning & learning-based reinforcement\nlearning with ensemble learning methods.\nAcknowledgment\nThe funding for this work is provided by the National Supercomputing Mission under DST\nand Miety, Govt. of India under Grant No. DST/NSM/ R&D HPC Appl/2021/03.29, as well\nas the Department of Science and Technology under Interdisciplinary Cyber Physical Systems\n(ICPS) Scheme grant no. DST/ICPS/CPS-Individual/2018/276. Mr. Ashwani Kumar Malik\nacknowledges the ﬁnancial support (File no - 09/1022 (0075)/2019-EMR-I) given as scholarship\nby Council of Scientiﬁc and Industrial Research (CSIR), New Delhi, India. We are grateful to\nIIT Indore for the facilities and support being provided.\nReferences\n[1] M. d. Condorcet, Essay on the application of analysis to the probability of majority decisions, Paris: Imprimerie\nRoyale (1785).\n[2] L. K. Hansen, P. Salamon, Neural network ensembles, IEEE Transactions on Pattern Analysis and Machine\nIntelligence 12 (1990) 993–1001.\n[3] T. G. Dietterich, Ensemble methods in machine learning, in: International Workshop on Multiple Classiﬁer\nSystems, Springer, 2000, pp. 1–15.\n[4] R. Kohavi, D. H. Wolpert, Bias plus variance decomposition for zero-one loss functions, in: ICML, volume 96,\n1996, pp. 275–83.\n[5] L. Breiman, Random forests, Machine Learning 45 (2001) 5–32.\n[6] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nImagenet large scale visual recognition challenge, International Journal of Computer Vision 115 (2015) 211–252.\n[7] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with deep convolutional neural networks, in:\nAdvances in Neural Information Processing Systems, 2012, pp. 1097–1105.\n[8] L. Deng, D. Yu, Deep learning: methods and applications, Foundations and Trends® in Signal Processing 7\n(2014) 197–387.\n[9] I. Goodfellow, Y. Bengio, A. Courville, Deep learning. book in preparation for MIT press, URL: http://www.\ndeeplearningbook. org (2016).\n[10] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015) 436–444.\n[11] S. Hochreiter, Untersuchungen zu dynamischen neuronalen netzen, Diploma, Technische Universit¨at M¨unchen\n91 (1991).\n35\n[12] X. Glorot, Y. Bengio, Understanding the diﬃculty of training deep feedforward neural networks, in: Proceedings\nof the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, 2010, pp. 249–256.\n[13] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.\n[14] R. K. Srivastava, K. Greﬀ, J. Schmidhuber, Training very deep networks, in: Advances in Neural Information\nProcessing Systems, 2015, pp. 2377–2385.\n[15] A. Veit, M. J. Wilber, S. Belongie, Residual networks behave like ensembles of relatively shallow networks, in:\nAdvances in Neural Information Processing Systems, 2016, pp. 550–558.\n[16] Y. Zhao, J. Gao, X. Yang, A survey of neural network ensembles, in: 2005 International Conference on Neural\nNetworks and Brain, volume 1, IEEE, 2005, pp. 438–442.\n[17] L. Rokach, Ensemble-based classiﬁers, Artiﬁcial Intelligence Review 33 (2010) 1–39.\n[18] D. Gopika, B. Azhagusundari, An Analysis on Ensemble Methods In Classiﬁcation Tasks, International Journal\nof Advanced Research in Computer and Communication Engineering 3 (2014) 7423–7427.\n[19] P. Yang, Y. Hwa Yang, B. B Zhou, A. Y Zomaya, A review of ensemble methods in bioinformatics, Current\nBioinformatics 5 (2010) 296–308.\n[20] J. Mendes-Moreira, C. Soares, A. M. Jorge, J. F. D. Sousa, Ensemble approaches for regression: A survey, ACM\nComputing Surveys (csur) 45 (2012) 10.\n[21] Y. Ren, P. Suganthan, N. Srikanth, Ensemble methods for wind and solar power forecasting—a state-of-the-art\nreview, Renewable and Sustainable Energy Reviews 50 (2015) 82–91.\n[22] S. Vega-Pons, J. Ruiz-Shulcloper, A survey of clustering ensemble algorithms, International Journal of Pattern\nRecognition and Artiﬁcial Intelligence 25 (2011) 337–372.\n[23] Y. Ren, L. Zhang, P. N. Suganthan, Ensemble classiﬁcation and regression-recent developments, applications and\nfuture directions, IEEE Computational Intelligence Magazine 11 (2016) 41–53.\n[24] O. Sagi, L. Rokach, Ensemble learning: A survey, Wiley Interdisciplinary Reviews: Data Mining and Knowledge\nDiscovery 8 (2018) e1249.\n[25] Y. Cao, T. A. Geddes, J. Y. H. Yang, P. Yang,\nEnsemble deep learning in bioinformatics,\nNature Machine\nIntelligence 2 (2020) 500–508.\n[26] A. Krogh, J. Vedelsby, Neural network ensembles, cross validation, and active learning, in: Advances in Neural\nInformation Processing Systems, 1995, pp. 231–238.\n[27] G. Brown, J. Wyatt, R. Harris, X. Yao, Diversity creation methods: a survey and categorisation, Information\nFusion 6 (2005) 5–20.\n[28] S. Geman, E. Bienenstock, R. Doursat, Neural networks and the bias/variance dilemma, Neural Computation 4\n(1992) 1–58.\n[29] G. Brown, J. L. Wyatt, P. Tiˇno,\nManaging diversity in regression ensembles,\nJournal of Machine Learning\nResearch 6 (2005) 1621–1650.\n[30] D. Pedro, A uniﬁed bias-variance decomposition and its applications, in: 17th International Conference on\nMachine Learning, 2000, pp. 231–238.\n[31] D. H. Wolpert, On bias plus variance, Neural Computation 9 (1997) 1211–1243.\n[32] E. Kleinberg, Stochastic discrimination, Annals of Mathematics and Artiﬁcial Intelligence 1 (1990) 207–239.\n36\n[33] R. E. Schapire, Y. Freund, P. Bartlett, W. S. Lee, Boosting the margin: A new explanation for the eﬀectiveness of\nvoting methods, The annals of statistics 26 (1998) 1651–1686.\n[34] V. Pisetta, New Insights into Decision Trees Ensembles, Ph.D. thesis, Lyon 2, 2012.\n[35] E. B. Kong, T. G. Dietterich, Error-correcting output coding corrects bias and variance, in: Machine Learning\nProceedings 1995, Elsevier, 1995, pp. 313–321.\n[36] J. H. Friedman, On bias, variance, 0/1—loss, and the curse-of-dimensionality, Data Mining and Knowledge\nDiscovery 1 (1997) 55–77.\n[37] L. Breiman, Arcing classiﬁer (with discussion and a rejoinder by the author), The Annals of Statistics 26 (1998)\n801–849.\n[38] G. M. James, Variance and bias for general loss functions, Machine Learning 51 (2003) 115–135.\n[39] L. Breiman, Bagging predictors, Machine Learning 24 (1996) 123–140.\n[40] L. Breiman, Bias, variance, and arcing classiﬁers (1996).\n[41] C.-X. Zhang, J.-S. Zhang, RotBoost: a technique for combining rotation forest and AdaBoost, Pattern recognition\nletters 29 (2008) 1524–1536.\n[42] Y. Freund, R. E. Schapire, Experiments with a new boosting algorithm, in: icml, volume 96, Citeseer, 1996, pp.\n148–156.\n[43] I. Barandiaran, The random subspace method for constructing decision forests, IEEE Trans. Pattern Anal. Mach.\nIntell 20 (1998).\n[44] L. Breiman, Randomizing outputs to increase prediction accuracy, Machine Learning 40 (2000) 229–242.\n[45] A. Buja, W. Stuetzle, Smoothing eﬀects of bagging, Preprint. AT&T Labs-Research (2000).\n[46] P. B¨uhlmann, B. Yu, Analyzing bagging, The Annals of Statistics 30 (2002) 927–961.\n[47] H.-C. Kim, S. Pang, H.-M. Je, D. Kim, S.-Y. Bang, Support vector machine ensemble with bagging, in: Interna-\ntional Workshop on Support Vector Machines, Springer, 2002, pp. 397–408.\n[48] D. Tao, X. Tang, X. Li, X. Wu, Asymmetric bagging and random subspace for support vector machines-based\nrelevance feedback in image retrieval, IEEE Transactions on Pattern Analysis and Machine Intelligence 28 (2006)\n1088–1099.\n[49] J. Mao, A case study on bagging, boosting and basic ensembles of neural networks for ocr, in: 1998 IEEE Inter-\nnational Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence\n(Cat. No. 98CH36227), volume 3, IEEE, 1998, pp. 1828–1833.\n[50] K. Ha, S. Cho, D. MacLachlan, Response models based on bagging neural networks, Journal of Interactive\nMarketing 19 (2005) 17–30.\n[51] R. Genc¸ay, M. Qi, Pricing and hedging derivative securities with neural networks: Bayesian regularization, early\nstopping, and bagging, IEEE Transactions on Neural Networks 12 (2001) 726–734.\n[52] A. Khwaja, M. Naeem, A. Anpalagan, A. Venetsanopoulos, B. Venkatesh, Improved short-term load forecasting\nusing bagged neural networks, Electric Power Systems Research 125 (2015) 109–115.\n[53] T. Hothorn, B. Lausen, A. Benner, M. Radespiel-Tr¨oger, Bagging survival trees, Statistics in Medicine 23 (2004)\n77–91.\n[54] R. F. Alvear-Sandoval, A. R. Figueiras-Vidal, On building ensembles of stacked denoising auto-encoding classi-\nﬁers and their further improvement, Information Fusion 39 (2018) 41–52.\n37\n[55] S. Hido, H. Kashima, Y. Takahashi, Roughly balanced bagging for imbalanced data, Statistical Analysis and Data\nMining: The ASA Data Science Journal 2 (2009) 412–426.\n[56] J. Błaszczy´nski, J. Stefanowski, Neighbourhood sampling in bagging for imbalanced data, Neurocomputing 150\n(2015) 529–542.\n[57] N. C. Oza, Online bagging and boosting, in: 2005 IEEE International Conference on Systems, Man and Cyber-\nnetics, volume 3, Ieee, 2005, pp. 2340–2345.\n[58] J. H. Friedman, Greedy function approximation: a gradient boosting machine, Annals of Statistics (2001) 1189–\n1232.\n[59] P. Liu, S. Han, Z. Meng, Y. Tong, Facial expression recognition via a boosted deep belief network, in: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1805–1812.\n[60] C. Cortes, M. Mohri, U. Syed, Deep boosting, in: 31st International Conference on Machine Learning, ICML\n2014, 2014.\n[61] V. Kuznetsov, M. Mohri, U. Syed, Multi-class deep boosting, Advances in Neural Information Processing Systems\n3 (2014) 2501–2509.\n[62] S. Han, Z. Meng, A. S. Khan, Y. Tong, Incremental boosting convolutional neural network for facial action unit\nrecognition, Advances in Neural Information Processing Systems 29 (2016).\n[63] M. Moghimi, S. J. Belongie, M. J. Saberian, J. Yang, N. Vasconcelos, L.-J. Li, Boosted convolutional neural\nnetworks., in: BMVC, 2016, pp. 24–1.\n[64] C. Chen, Z. Xiong, X. Tian, F. Wu,\nDeep boosting for image denoising,\nin: Proceedings of the European\nConference on Computer Vision (ECCV), 2018, pp. 3–18.\n[65] C. Chen, Z. Xiong, X. Tian, Z.-J. Zha, F. Wu, Real-world image denoising with deep boosting, IEEE Transactions\non Pattern Analysis and Machine Intelligence (2019).\n[66] B. Yang, J. Yan, Z. Lei, S. Z. Li, Convolutional channel features, in: Proceedings of the IEEE International\nConference on Computer Vision, 2015, pp. 82–90.\n[67] E. Walach, L. Wolf, Learning to count with cnn boosting, in: European Conference on Computer Vision, Springer,\n2016, pp. 660–676. doi:10.1007/978-3-319-46475-6\\_41.\n[68] M. Opitz, G. Waltner, H. Possegger, H. Bischof, Bier-boosting independent embeddings robustly, in: Proceedings\nof the IEEE International Conference on Computer Vision, 2017, pp. 5189–5198.\n[69] G. Waltner, M. Opitz, H. Possegger, H. Bischof, Hibster: Hierarchical boosted deep metric learning for image\nretrieval, in: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), IEEE, 2019, pp.\n599–608.\n[70] A. Mosca, G. D. Magoulas, Deep incremental boosting, arXiv preprint arXiv:1708.03704 (2017).\n[71] W. Zhang, J. Jiang, Y. Shao, B. Cui, Snapshot boosting: a fast ensemble framework for deep neural networks,\nScience China Information Sciences 63 (2020) 112102.\n[72] C. Siu, Residual networks behave like boosting algorithms, in: 2019 IEEE International Conference on Data\nScience and Advanced Analytics (DSAA), IEEE, 2019, pp. 31–40.\n[73] C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, S. Yang, Adanet: Adaptive structural learning of artiﬁcial neural\nnetworks (2017) 874–883.\n[74] F. Huang, J. Ash, J. Langford, R. Schapire, Learning deep resnet blocks sequentially using boosting theory (2018)\n38\n2058–2067.\n[75] A. Beygelzimer, E. Hazan, S. Kale, H. Luo, Online gradient boosting, Advances in neural information processing\nsystems 28 (2015).\n[76] S. Gonz´alez, S. Garc´ıa, J. Del Ser, L. Rokach, F. Herrera, A practical tutorial on bagging and boosting based\nensembles for machine learning: Algorithms, software tools, performance study, practical perspectives and op-\nportunities, Information Fusion 64 (2020) 205–237.\n[77] D. H. Wolpert, Stacked generalization, Neural Networks 5 (1992) 241–259.\n[78] M. Leblanc, R. Tibshirani, Combining Estimates in Regression and Classiﬁcation, Journal of the American\nStatistical Association 91 (1996) 1641–1650. doi:10.1080/01621459.1996.10476733.\n[79] L. Deng, D. Yu, Deep convex net: A scalable architecture for speech pattern classiﬁcation, Proceedings of the\nAnnual Conference of the International Speech Communication Association, INTERSPEECH (2011) 2285–2288.\n[80] L. L. Deng, D. Yu, J. Platt,\nScalable stacking and learning for building deep architectures,\nICASSP, IEEE\nInternational Conference on Acoustics, Speech and Signal Processing - Proceedings (2012) 2133–2136. doi:10.\n1109/ICASSP.2012.6288333.\n[81] L. Deng, G. Tur, X. He, D. Hakkani-Tur, Use of kernel deep convex networks and end-to-end learning for spoken\nlanguage understanding, in: 2012 IEEE Workshop on Spoken Language Technology, SLT 2012 - Proceedings,\nIEEE, 2012, pp. 210–215. doi:10.1109/SLT.2012.6424224.\n[82] P.-S. Huang, L. Deng, M. Hasegawa-Johnson, X. He, Random features for Kernel Deep Convex Network, in:\n2013 IEEE International Conference on Acoustics, Speech and Signal Processing, 2, IEEE, 2013, pp. 3143–3147.\ndoi:10.1109/ICASSP.2013.6638237.\n[83] T. Welchowski, M. Schmid, A framework for parameter estimation and model selection in kernel deep stacking\nnetworks, Artiﬁcial Intelligence in Medicine 70 (2016) 31–40. doi:10.1016/j.artmed.2016.04.002.\n[84] B. Hutchinson, L. Deng, D. Yu, A deep architecture with bilinear modeling of hidden representations: Applica-\ntions to phonetic recognition, in: 2012 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), IEEE, 2012, pp. 4805–4808. doi:10.1109/ICASSP.2012.6288994.\n[85] B. Hutchinson, L. L. Deng, D. Yu, Tensor deep stacking networks, IEEE Transactions on Pattern Analysis and\nMachine Intelligence 35 (2013) 1944–1957. doi:10.1109/TPAMI.2012.268.\n[86] J. Li, H. Chang, J. Yang, Sparse deep stacking network for image classiﬁcation, in: Twenty-Ninth AAAI Confer-\nence on Artiﬁcial Intelligence, 2015.\n[87] C. Sun, M. Ma, Z. Zhao, X. Chen, Sparse deep stacking network for fault diagnosis of motor, IEEE Transactions\non Industrial Informatics 14 (2018) 3261–3270.\n[88] J. Li, H. Chang, J. Yang, W. Luo, Y. Fu, Visual representation and classiﬁcation by learning group sparse deep\nstacking network, IEEE Transactions on Image Processing 27 (2017) 464–476.\n[89] J. Zhang, W. Zhang, R. Song, L. Ma, Y. Li, Grasp for stacking via deep reinforcement learning (2020) 2543–2549.\n[90] B. Wang, B. Xue, M. Zhang, Particle swarm optimisation for evolving deep neural networks for image classiﬁca-\ntion by evolving and stacking transferable blocks, in: 2020 IEEE Congress on Evolutionary Computation (CEC),\nIEEE, 2020, pp. 1–8.\n[91] H. Zhang, Y. Dai, H. Li, P. Koniusz, Deep stacked hierarchical multi-patch network for image deblurring, in:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5978–5986.\n39\n[92] H. Palangi, L. Deng, R. K. Ward, Recurrent Deep-Stacking Networks for sequence classiﬁcation, 2014 IEEE\nChina Summit and International Conference on Signal and Information Processing, IEEE ChinaSIP 2014 - Pro-\nceedings (2014) 510–514. doi:10.1109/ChinaSIP.2014.6889295.\n[93] H. Zhou, G. B. Huang, Z. Lin, H. Wang, Y. C. Soh, Stacked extreme learning machines, IEEE Transactions on\nCybernetics 45 (2015) 2013–2025. doi:10.1109/TCYB.2014.2363492.\n[94] G. Wang, G. Zhang, K. S. Choi, J. Lu, Deep Additive Least Squares Support Vector Machines for Classiﬁcation\nwith Model Transfer, IEEE Transactions on Systems, Man, and Cybernetics: Systems 49 (2019) 1527–1540.\ndoi:10.1109/TSMC.2017.2759090.\n[95] J. Wang, K. Feng, J. Wu,\nSVM-Based Deep Stacking Networks,\nProceedings of the AAAI Conference on\nArtiﬁcial Intelligence 33 (2019) 5273–5280. doi:10.1609/aaai.v33i01.33015273.\n[96] X. Li, Y. Yang, H. Pan, J. Cheng, J. Cheng, A novel deep stacking least squares support vector machine for rolling\nbearing fault diagnosis, Computers in Industry 110 (2019) 36–47. doi:10.1016/j.compind.2019.05.005.\n[97] Z.-H. Zhou, J. Feng, Deep forest, arXiv preprint arXiv:1702.08835 (2017).\n[98] C.-Y. Low, J. Park, A. B.-J. Teoh, Stacking-based deep neural network: Deep analytic network for pattern classi-\nﬁcation, IEEE Transactions on Cybernetics 50 (2019) 5021–5034.\n[99] T. Kang, P. Chen, J. Quackenbush, W. Ding, A novel deep learning model by stacking conditional restricted boltz-\nmann machine and deep neural network, in: Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 2020, pp. 1316–1324.\n[100] Y. Liu, X. Yao, Ensemble learning via negative correlation, Neural Networks 12 (1999) 1399–1404. doi:10.\n1016/S0893-6080(99)00073-8.\n[101] Z. Shi, L. Zhang, Y. Liu, X. Cao, Y. Ye, M.-M. Cheng, G. Zheng, Crowd counting with deep negative correlation\nlearning, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 5382–\n5390.\n[102] L. Zhang, Z. Shi, M.-M. Cheng, Y. Liu, J.-W. Bian, J. T. Zhou, G. Zheng, Z. Zeng, Nonlinear Regression via\nDeep Negative Correlation Learning, IEEE Transactions on Pattern Analysis and Machine Intelligence PP (2019)\n1–1. doi:10.1109/tpami.2019.2943860.\n[103] S. Buschj¨ager, L. Pfahler, K. Morik, Generalized negative correlation learning for deep ensembling, arXiv preprint\narXiv:2011.02952 (2020).\n[104] M. D. Muhlbaier, R. Polikar, An ensemble approach for incremental learning in nonstationary environments, in:\nM. Haindl, J. Kittler, F. Roli (Eds.), Multiple Classiﬁer Systems, Springer Berlin Heidelberg, Berlin, Heidelberg,\n2007, pp. 490–500.\n[105] K. Tang, M. Lin, F. L. Minku, X. Yao, Selective negative correlation learning approach to incremental learning,\nNeurocomputing 72 (2009) 2796 – 2805. doi:https://doi.org/10.1016/j.neucom.2008.09.022, hybrid\nLearning Machines (HAIS 2007) / Recent Developments in Natural Computation (ICNC 2007).\n[106] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, B. Mele, G. Altarelli, Dropout: a simple\nway to prevent neural networks from overﬁtting, The journal of machine learning research 15 (2014) 1929–1958.\ndoi:10.1016/0370-2693(93)90272-J.\n[107] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, R. Fergus, Regularization of Neural Networks using DropConnect,\nin: S. Dasgupta, D. McAllester (Eds.), Proceedings of the 30th International Conference on Machine Learning,\n40\nvolume 28 of Proceedings of Machine Learning Research, PMLR, Atlanta, Georgia, USA, 2013, pp. 1058–1066.\ndoi:10.1109/TPAMI.2017.2703082.\n[108] G. Huang, Y. Sun, Z. Liu, D. Sedra, K. Q. Weinberger, Deep networks with stochastic depth, in: European\nConference on Computer Vision, Springer, 2016, pp. 646–661. doi:10.1007/978-3-319-46493-0\\_39.\n[109] S. Singh, D. Hoiem, D. Forsyth, Swapout: Learning an ensemble of deep architectures (2016) 28–36.\n[110] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural network, arXiv preprint arXiv:1503.02531\n(2015).\n[111] L. N. Smith, E. M. Hand, T. Doster, Gradual dropin of layers to train very deep neural networks, in: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 4763–4771.\n[112] G. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, K. Q. Weinberger, Snapshot ensembles: Train 1, get M for free,\narXiv preprint arXiv:1704.00109 (2017).\n[113] Y.-H. Pao, G.-H. Park, D. J. Sobajic, Learning and generalization characteristics of the random vector functional-\nlink net, Neurocomputing 6 (1994) 163–180.\n[114] A. K. Malik, R. Gao, M. A. Ganaie, M. Tanveer, P. N. Suganthan, Random vector functional link network: recent\ndevelopments, applications, and future directions, arXiv preprint arXiv:2203.11316 (2022).\n[115] Q. Shi, R. Katuwal, P. N. Suganthan, M. Tanveer, Random vector functional link neural network based ensemble\ndeep learning, Pattern Recognition 117 (2021) 107978.\n[116] B. Han, J. Sim, H. Adam, Branchout: Regularization for online ensemble tracking with convolutional neural\nnetworks,\nin: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.\n3356–3365.\n[117] J. Xue, Z. Wang, D. Kong, Y. Wang, X. Liu, W. Fan, S. Yuan, S. Niu, D. Li, Deep ensemble neural-like p systems\nfor segmentation of central serous chorioretinopathy lesion, Information Fusion 65 (2021) 84–94.\n[118] J. Xie, B. Xu, Z. Chuang, Horizontal and vertical ensemble with deep representation for classiﬁcation, arXiv\npreprint arXiv:1306.2759 (2013).\n[119] S. Laine, T. Aila, Temporal ensembling for semi-supervised learning, arXiv preprint arXiv:1610.02242 (2016).\n[120] D. Ciregan, U. Meier, J. Schmidhuber, Multi-column deep neural networks for image classiﬁcation, in: 2012\nIEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2012, pp. 3642–3649.\n[121] F. Grassmann, J. Mengelkamp, C. Brandl, S. Harsch, M. E. Zimmermann, B. Linkohr, A. Peters, I. M. Heid,\nC. Palm, B. H. Weber, A deep learning algorithm for prediction of age-related eye disease study severity scale for\nage-related macular degeneration from color fundus photography, Ophthalmology 125 (2018) 1410–1420.\n[122] W. Li, S. Ding, Y. Chen, S. Yang, Heterogeneous ensemble for default prediction of peer-to-peer lending in china,\nIEEE Access 6 (2018) 54396–54406.\n[123] Z. H. Kilimci, S. Akyokus, Deep learning-and word embedding-based heterogeneous classiﬁer ensembles for text\nclassiﬁcation, Complexity 2018 (2018).\n[124] S. Tabik, R. F. Alvear-Sandoval, M. M. Ruiz, J.-L. Sancho-G´omez, A. R. Figueiras-Vidal, F. Herrera, Mnist-\nnet10: A heterogeneous deep networks fusion based on the degree of certainty to reach 0.1% error rate. ensembles\noverview and proposal, Information Fusion 62 (2020) 73 – 80. doi:10.1016/j.inffus.2020.04.002.\n[125] B. Seijo-Pardo, I. Porto-D´ıaz, V. Bol´on-Canedo, A. Alonso-Betanzos, Ensemble feature selection: homogeneous\nand heterogeneous approaches, Knowledge-Based Systems 118 (2017) 124–139.\n41\n[126] Q. L. Zhao, Y. H. Jiang, M. Xu, Incremental learning by heterogeneous bagging ensemble, in: L. Cao, J. Zhong,\nY. Feng (Eds.), Advanced Data Mining and Applications, Springer Berlin Heidelberg, Berlin, Heidelberg, 2010,\npp. 1–12.\n[127] L. L. Minku, A. P. White, X. Yao, The impact of diversity on online ensemble learning in the presence of concept\ndrift, IEEE Transactions on knowledge and Data Engineering 22 (2009) 730–742.\n[128] X. Qiu, P. N. Suganthan, G. A. Amaratunga,\nEnsemble incremental learning random vector functional link\nnetwork for short-term electric load forecasting, Knowledge-Based Systems 145 (2018) 182 – 196. doi:https:\n//doi.org/10.1016/j.knosys.2018.01.015.\n[129] G. Grmanov´a, P. Laurinec, V. Rozinajov´a, A. B. Ezzeddine, M. Luck´a, P. Lacko, P. Vrablecov´a, P. N´avrat, Incre-\nmental ensemble learning for electricity load forecasting, volume 13, 2016, pp. 97–117.\n[130] F. Duan, L. Dai, Recognizing the gradual changes in sEMG characteristics based on incremental learning of\nwavelet neural network ensemble, IEEE Transactions on Industrial Electronics 64 (2017) 4276–4286. doi:10.\n1109/TIE.2016.2593693.\n[131] R. T. Das, K. K. Ang, C. Quek, ierspop: A novel incremental rough set-based pseudo outer-product with ensemble\nlearning, Applied Soft Computing 46 (2016) 170 – 186. doi:https://doi.org/10.1016/j.asoc.2016.04.\n015.\n[132] C. Ju, A. Bibaut, M. van der Laan, The relative performance of ensemble methods with deep convolutional neural\nnetworks for image classiﬁcation, Journal of Applied Statistics 45 (2018) 2800–2818.\n[133] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint\narXiv:1409.1556 (2014).\n[134] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, Going\ndeeper with convolutions, in: Proceedings of the IEEE conference on computer vision and pattern recognition,\n2015, pp. 1–9.\n[135] L. I. Kuncheva, C. J. Whitaker, C. A. Shipp, R. P. Duin, Limits on the majority vote accuracy in classiﬁer fusion,\nPattern Analysis & Applications 6 (2003) 22–31.\n[136] A. Choromanska, M. Henaﬀ, M. Mathieu, G. B. Arous, Y. LeCun, The loss surfaces of multilayer networks, in:\nArtiﬁcial Intelligence and Statistics, 2015, pp. 192–204.\n[137] S. Li, X. Lu, S. Sakai, M. Mimura, T. Kawahara, Semi-supervised ensemble DNN acoustic model training, in:\n2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2017, pp.\n5270–5274.\n[138] X. Wang, D. Kihara, J. Luo, G.-J. Qi, ENAET: self-trained ensemble autoencoding transformations for semi-\nsupervised learning, arXiv preprint arXiv:1911.09265 (2019).\n[139] J. Liu, S. Zhao, G. Wang, Ssel-ade: a semi-supervised ensemble learning framework for extracting adverse drug\nevents from social media, Artiﬁcial Intelligence in Medicine 84 (2018) 34–49.\n[140] J. Li, S. Wu, C. Liu, Z. Yu, H.-S. Wong, Semi-supervised deep coupled ensemble learning with classiﬁcation\nlandmark exploration, IEEE Transactions on Image Processing 29 (2019) 538–550.\n[141] G. Pio, D. Malerba, D. D’Elia, M. Ceci, Integrating microrna target predictions for the discovery of gene regula-\ntory networks: a semi-supervised ensemble learning approach, BMC Bioinformatics 15 (2014) S4.\n[142] G. Tsoumakas, I. Katakis, Multi-label classiﬁcation: An overview, International Journal of Data Warehousing\n42\nand Mining (IJDWM) 3 (2007) 1–13.\n[143] G. Tsoumakas, I. Vlahavas, Random k-labelsets: An ensemble method for multilabel classiﬁcation, in: European\nConference on Machine Learning, Springer, 2007, pp. 406–417.\n[144] J. M. Moyano, E. L. Gibaja, K. J. Cios, S. Ventura, An evolutionary approach to build ensembles of multi-label\nclassiﬁers, Information Fusion 50 (2019) 168–180.\n[145] K. Kimura, M. Kudo, L. Sun, S. Koujaku, Fast random k-labelsets for large-scale multi-label classiﬁcation, in:\n2016 23rd International Conference on Pattern Recognition (ICPR), IEEE, 2016, pp. 438–443.\n[146] R. Wang, S. Kwong, X. Wang, Y. Jia, Active k-labelsets ensemble for multi-label classiﬁcation, Pattern Recogni-\ntion 109 (2021) 107583.\n[147] C. Shi, X. Kong, P. S. Yu, B. Wang, Multi-label ensemble learning, in: D. Gunopulos, T. Hofmann, D. Malerba,\nM. Vazirgiannis (Eds.), Machine Learning and Knowledge Discovery in Databases, Springer Berlin Heidelberg,\nBerlin, Heidelberg, 2011, pp. 223–239.\n[148] P. Li, H. Li, M. Wu, Multi-label ensemble based on variable pairwise constraint projection, Information Sciences\n222 (2013) 269 – 281. doi:https://doi.org/10.1016/j.ins.2012.07.066, including Special Section on\nNew Trends in Ambient Intelligence and Bio-inspired Systems.\n[149] Y. Xia, K. Chen, Y. Yang, Multi-label classiﬁcation with weighted classiﬁer selection and stacked ensemble,\nInformation Sciences 557 (2021) 421–442.\n[150] X. Guo, F. Liu, Y. Ju, Z. Wang, C. Wang, Human protein subcellular localization with integrated source and\nmulti-label ensemble classiﬁer, Scientiﬁc Reports 6 (2016) 28087.\n[151] G. Yu, C. Domeniconi, H. Rangwala, G. Zhang, Z. Yu,\nTransductive multi-label ensemble classiﬁcation for\nprotein function prediction, in: Proceedings of the 18th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, 2012, pp. 1077–1085.\n[152] W. Zhang, F. Liu, L. Luo, J. Zhang, Predicting drug side eﬀects by multi-label learning and ensemble learning,\nBMC bioinformatics 16 (2015) 365.\n[153] L. Schietgat, C. Vens, J. Struyf, H. Blockeel, D. Kocev, S. Dˇzeroski, Predicting gene function using hierarchical\nmulti-label decision tree ensembles, BMC Bioinformatics 11 (2010) 2.\n[154] J. Read, B. Pfahringer, G. Holmes, E. Frank, Classiﬁer chains for multi-label classiﬁcation, Machine Learning 85\n(2011) 333.\n[155] G. Chen, D. Ye, Z. Xing, J. Chen, E. Cambria,\nEnsemble application of convolutional and recurrent neural\nnetworks for multi-label text categorization, in: 2017 international joint conference on neural networks (IJCNN),\nIEEE, 2017, pp. 2377–2383.\n[156] L. Breiman, Stacked regressions, Machine Learning 24 (1996) 49–64.\n[157] M. J. Van der Laan, E. C. Polley, A. E. Hubbard, Super learner, Statistical Applications in Genetics and Molecular\nBiology 6 (2007).\n[158] C. Ju, M. Combs, S. D. Lendle, J. M. Franklin, R. Wyss, S. Schneeweiss, M. J. van der Laan, Propensity score\nprediction for electronic healthcare databases using super learner and high-dimensional propensity score methods,\nJournal of Applied Statistics 46 (2019) 2216–2236.\n[159] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller, Playing atari with\ndeep reinforcement learning, arXiv preprint arXiv:1312.5602 (2013).\n43\n[160] X.-l. Chen, L. Cao, C.-x. Li, Z.-x. Xu, J. Lai, Ensemble network architecture for deep reinforcement learning,\nMathematical Problems in Engineering 2018 (2018).\n[161] H. Cuay´ahuitl, D. Lee, S. Ryu, Y. Cho, S. Choi, S. Indurthi, S. Yu, H. Choi, I. Hwang, J. Kim, Ensemble-based\ndeep reinforcement learning for chatbots, Neurocomputing 366 (2019) 118–130.\n[162] S. Carta, A. Corriga, A. Ferreira, A. S. Podda, D. R. Recupero, A multi-layer and multi-ensemble stock trader\nusing deep learning and deep reinforcement learning, Applied Intelligence (2020) 1–17.\n[163] H. Yang, X.-Y. Liu, S. Zhong, A. Walid, Deep reinforcement learning for automated stock trading: An ensemble\nstrategy (2020) 1–8.\n[164] K.-F. Tang, H.-C. Kao, C.-N. Chou, E. Y. Chang, Inquire and diagnose: Neural symptom checking ensemble\nusing deep reinforcement learning, in: Proceedings of NIPS Workshop on Deep Reinforcement Learning, 2016.\n[165] Y. S, enbabao˘glu, G. Michailidis, J. Z. Li, Critical limitations of consensus clustering in class discovery, Scientiﬁc\nReports 4 (2014) 1–13.\n[166] S. Bian, W. Wang,\nOn diversity and accuracy of homogeneous and heterogeneous ensembles,\nInternational\nJournal of Hybrid Intelligent Systems 4 (2007) 103–128.\n[167] Z.-H. Zhou, W. Tang, Clusterer ensemble, Knowledge-Based Systems 19 (2006) 77–83.\n[168] D. Huang, C.-D. Wang, J.-H. Lai, Locally weighted ensemble clustering, IEEE Transactions on Cybernetics 48\n(2017) 1460–1473.\n[169] L. Zheng, T. Li, C. Ding, Hierarchical ensemble clustering, in: 2010 IEEE International Conference on Data\nMining, IEEE, 2010, pp. 1199–1204.\n[170] D. Huang, J. Lai, C.-D. Wang, Ensemble clustering using factor graph, Pattern Recognition 50 (2016) 131–142.\n[171] H. Liu, T. Liu, J. Wu, D. Tao, Y. Fu, Spectral ensemble clustering, in: Proceedings of the 21th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, 2015, pp. 715–724.\n[172] H. Liu, M. Shao, S. Li, Y. Fu, Inﬁnite ensemble for image clustering, in: Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, 2016, pp. 1745–1754.\n[173] U. Shaham, X. Cheng, O. Dror, A. Jaﬀe, B. Nadler, J. Chang, Y. Kluger, A deep learning approach to unsupervised\nensemble learning, in: International Conference on Machine Learning, 2016, pp. 30–39.\n[174] N. Alami, M. Meknassi, N. En-nahnahi, Enhancing unsupervised neural networks based text summarization with\nword embedding and ensemble learning, Expert systems with applications 123 (2019) 195–211.\n[175] B. Hassan, S. E. Abdelrahman, R. Bahgat, I. Farag, Uests: An unsupervised ensemble semantic textual similarity\nmethod, IEEE Access 7 (2019) 85462–85482.\n[176] E. Ahn, A. Kumar, D. Feng, M. Fulham, J. Kim, Unsupervised feature learning with k-means and an ensemble of\ndeep convolutional neural networks for medical image classiﬁcation, arXiv preprint arXiv:1906.03359 (2019).\n[177] A. Lahiri, A. G. Roy, D. Sheet, P. K. Biswas, Deep neural ensemble for retinal vessel segmentation in fundus\nimages towards achieving label-free angiography, in: 2016 38th Annual International Conference of the IEEE\nEngineering in Medicine and Biology Society (EMBC), IEEE, 2016, pp. 1340–1343.\n[178] B. Liu, L. Gu, F. Lu, Unsupervised ensemble strategy for retinal vessel segmentation, in: International Conference\non Medical Image Computing and Computer-Assisted Intervention, Springer, 2019, pp. 111–119.\n[179] Z. Cao, X. Pan, Y. Yang, Y. Huang, H.-B. Shen, The lnclocator: a subcellular localization predictor for long\nnon-coding RNAs based on a stacked ensemble classiﬁer, Bioinformatics 34 (2018) 2185–2194.\n44\n[180] M. Hu, P. Suganthan, Representation learning using deep random vector functional link networks for clustering,\nPattern Recognition (2022) 108744.\n[181] P. Melville, R. J. Mooney, Constructing diverse classiﬁer ensembles using artiﬁcial training examples, in: IJCAI,\nvolume 3, 2003, pp. 505–510.\n[182] P. Melville, R. J. Mooney, Diverse ensembles for active learning, in: Proceedings of the twenty-ﬁrst international\nconference on Machine learning, 2004, p. 74.\n[183] W. H. Beluch, T. Genewein, A. N¨urnberger, J. M. K¨ohler, The power of ensembles for active learning in image\nclassiﬁcation, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.\n9368–9377.\n[184] A. Sharma, R. Rani, Be-dti’: Ensemble framework for drug target interaction prediction using dimensionality\nreduction and active learning, Computer Methods and Programs in Biomedicine 165 (2018) 151–162.\n[185] H. Zhang, W. Liu, J. Shan, Q. Liu, Online active learning paired ensemble for concept drift and class imbalance,\nIEEE Access 6 (2018) 73815–73828. doi:10.1109/ACCESS.2018.2882872.\n[186] X. Zheng, J. Shen, C. Cox, J. C. Wakeﬁeld, M. G. Ehm, M. R. Nelson, B. S. Weir, HIBAG—HLA genotype\nimputation with attribute bagging, The Pharmacogenomics Journal 14 (2014) 192–200.\n[187] I. Cabria, I. Gondra, Mri segmentation fusion for brain tumor detection, Information Fusion 36 (2017) 1–9.\n[188] A. Shalbaf, M. Vafaeezadeh, Automated detection of COVID-19 using ensemble of transfer learning with deep\nconvolutional neural network based on CT scans, International journal of computer assisted radiology and surgery\n16 (2021) 115–123.\n[189] F. Ali, S. El-Sappagh, S. R. Islam, D. Kwak, A. Ali, M. Imran, K.-S. Kwak, A smart healthcare monitoring system\nfor heart disease prediction based on ensemble deep learning and feature fusion, Information Fusion 63 (2020)\n208–222.\n[190] T. Zhou, H. Lu, Z. Yang, S. Qiu, B. Huo, Y. Dong, The ensemble deep learning model for novel COVID-19 on\nCT images, Applied Soft Computing 98 (2021) 106885.\n[191] Y. Li, Y. Song, L. Jia, S. Gao, Q. Li, M. Qiu, Intelligent fault diagnosis by fusing domain adversarial training\nand maximum mean discrepancy via ensemble learning, IEEE Transactions on Industrial Informatics 17 (2020)\n2833–2841.\n[192] A. K. Das, S. Ghosh, S. Thunder, R. Dutta, S. Agarwal, A. Chakrabarti, Automatic covid-19 detection from x-ray\nimages using ensemble learning with convolutional neural network, Pattern Analysis and Applications 24 (2021)\n1111–1124.\n[193] S. Sukegawa, A. Fujimura, A. Taguchi, N. Yamamoto, A. Kitamura, R. Goto, K. Nakano, K. Takabatake,\nH. Kawai, H. Nagatsuka, Y. Furuki, Identiﬁcation of osteoporosis using ensemble deep learning model with\npanoramic radiographs and clinical covariates, Scientiﬁc reports 12 (2022) 1–10.\n[194] Z. Gao, L. Wang, R. Soroushmehr, A. Wood, J. Gryak, B. Nallamothu, K. Najarian, Vessel segmentation for\nx-ray coronary angiography using ensemble methods with deep learning and ﬁlter-based features, BMC Medical\nImaging 22 (2022) 1–17.\n[195] A. Rath, D. Mishra, G. Panda, S. C. Satapathy, K. Xia, Improved heart disease detection from ECG signal using\ndeep learning based ensemble model, Sustainable Computing: Informatics and Systems 35 (2022) 100732.\n[196] M. Tanveer, A. Rashid, M. Ganaie, M. Reza, I. Razzak, K.-L. Hua, Classiﬁcation of Alzheimer’s disease using\n45\nensemble of deep neural networks trained through transfer learning, IEEE Journal of Biomedical and Health\nInformatics 26 (2022) 1453 – 1463. doi:10.1109/JBHI.2021.3083274.\n[197] H. M. Rai, K. Chatterjee, Hybrid cnn-lstm deep learning model and ensemble technique for automatic detection\nof myocardial infarction using big ecg data, Applied Intelligence 52 (2022) 5366–5384.\n[198] M. Ganaie, M. Tanveer, Ensemble deep random vector functional link network using privileged information for\nAlzheimer’s disease diagnosis, IEEE/ACM Transactions on Computational Biology and Bioinformatics (2022).\ndoi:10.1109/TCBB.2022.3170351.\n[199] G. Tur, L. Deng, D. Hakkani-T¨ur, X. He, Towards deeper understanding: Deep convex networks for seman-\ntic utterance classiﬁcation, in: 2012 IEEE international conference on acoustics, speech and signal processing\n(ICASSP), IEEE, 2012, pp. 5045–5048.\n[200] L. Deng, G. Tur, X. He, D. Hakkani-Tur, Use of kernel deep convex networks and end-to-end learning for spoken\nlanguage understanding, in: 2012 IEEE Spoken Language Technology Workshop (SLT), IEEE, 2012, pp. 210–\n215.\n[201] L. Deng, J. C. Platt, Ensemble deep learning for speech recognition, in: Fifteenth Annual Conference of the\nInternational Speech Communication Association, 2014.\n[202] M. Amin-Naji, A. Aghagolzadeh, M. Ezoji, Ensemble of cnn for multi-focus image fusion, Information Fusion\n51 (2019) 201–214.\n[203] X. Qiu, L. Zhang, Y. Ren, P. N. Suganthan, G. Amaratunga, Ensemble deep learning for regression and time\nseries forecasting, in: 2014 IEEE Symposium on Computational Intelligence in Ensemble Learning (CIEL),\nIEEE, 2014, pp. 1–6.\n[204] X. Qiu, Y. Ren, P. N. Suganthan, G. A. Amaratunga, Empirical mode decomposition based ensemble deep learning\nfor load demand time series forecasting, Applied Soft Computing 54 (2017) 246–255.\n[205] F. Liu, F. Xu, S. Yang,\nA ﬂood forecasting model based on deep learning algorithm via integrating stacked\nautoencoders with BP neural network, in: 2017 IEEE Third International Conference on Multimedia Big Data\n(BigMM), IEEE, 2017, pp. 58–61.\n[206] N. Bhusal, R. M. Shukla, M. Gautam, M. Benidris, S. Sengupta, Deep ensemble learning-based approach to\nreal-time power system state estimation, International Journal of Electrical Power & Energy Systems 129 (2021)\n106806.\n[207] P. Singla, M. Duhan, S. Saroha,\nAn ensemble method to forecast 24-h ahead solar irradiance using wavelet\ndecomposition and BiLSTM deep learning network, Earth Science Informatics (2021) 1–16.\n[208] L. Deng, X. He, J. Gao, Deep stacking networks for information retrieval, in: 2013 IEEE International Conference\non Acoustics, Speech and Signal Processing, IEEE, 2013, pp. 3153–3157.\n[209] G. Wang, J. Sun, J. Ma, K. Xu, J. Gu, Sentiment classiﬁcation: The contribution of ensemble learning, Decision\nSupport Systems 57 (2014) 77–93.\n[210] M. Zareapoor, P. Shamsolmoali, Application of credit card fraud detection: Based on bagging ensemble classiﬁer,\nProcedia Computer Science 48 (2015) 679–685.\n[211] Z. Yin, M. Zhao, Y. Wang, J. Yang, J. Zhang, Recognition of emotions using multimodal physiological signals\nand an ensemble deep learning model, Computer Methods and Programs in Biomedicine 140 (2017) 93–110.\n[212] S. Kazemi, B. Minaei Bidgoli, S. Shamshirband, S. M. Karimi, M. A. Ghorbani, K.-w. Chau, R. Kazem Pour,\n46\nNovel genetic-based negative correlation learning for estimating soil temperature, Engineering Applications of\nComputational Fluid Mechanics 12 (2018) 506–516.\n[213] K. Randhawa, C. K. Loo, M. Seera, C. P. Lim, A. K. Nandi, Credit card fraud detection using adaboost and\nmajority voting, IEEE Access 6 (2018) 14277–14284.\n[214] A. Mart´ın, R. Lara-Cabrera, D. Camacho, Android malware detection through hybrid features fusion and ensemble\nclassiﬁers: The andropytool framework and the omnidroid dataset, Information Fusion 52 (2019) 128–142.\n[215] F. Wang, D. Jiang, H. Wen, H. Song, Adaboost-based security level classiﬁcation of mobile intelligent terminals,\nThe Journal of Supercomputing 75 (2019) 7460–7478.\n[216] W. Chen, B. Pradhan, S. Li, H. Shahabi, H. M. Rizeei, E. Hou, S. Wang, Novel hybrid integration approach\nof bagging-based ﬁsher’s linear discriminant function for groundwater potential analysis,\nNatural Resources\nResearch 28 (2019) 1239–1258.\n[217] E. Tsogbaatar, M. H. Bhuyan, Y. Taenaka, D. Fall, K. Gonchigsumlaa, E. Elmroth, Y. Kadobayashi, Del-iot: A\ndeep ensemble learning approach to uncover anomalies in iot, Internet of Things 14 (2021) 100391.\n[218] L. Wen, X. Xie, X. Li, L. Gao, A new ensemble convolutional neural network with diversity regularization for\nfault diagnosis, Journal of Manufacturing Systems 62 (2020) 964 – 971. doi:10.1016/j.jmsy.2020.12.002.\n[219] Z.-H. Zhou, N. V. Chawla, Y. Jin, G. J. Williams, Big data opportunities and challenges: Discussions from data\nanalytics perspectives [discussion forum], IEEE Computational Intelligence Magazine 9 (2014) 62–74.\n[220] R. Katuwal, P. N. Suganthan, Stacked autoencoder based deep random vector functional link neural network for\nclassiﬁcation, Applied Soft Computing 85 (2019) 105854.\n47\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV"
  ],
  "published": "2021-04-06",
  "updated": "2022-08-08"
}