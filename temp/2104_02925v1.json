{
  "id": "http://arxiv.org/abs/2104.02925v1",
  "title": "Pretrained equivariant features improve unsupervised landmark discovery",
  "authors": [
    "Rahul Rahaman",
    "Atin Ghosh",
    "Alexandre H. Thiery"
  ],
  "abstract": "Locating semantically meaningful landmark points is a crucial component of a\nlarge number of computer vision pipelines. Because of the small number of\navailable datasets with ground truth landmark annotations, it is important to\ndesign robust unsupervised and semi-supervised methods for landmark detection.\n  Many of the recent unsupervised learning methods rely on the equivariance\nproperties of landmarks to synthetic image deformations. Our work focuses on\nsuch widely used methods and sheds light on its core problem, its inability to\nproduce equivariant intermediate convolutional features. This finding leads us\nto formulate a two-step unsupervised approach that overcomes this challenge by\nfirst learning powerful pixel-based features and then use the pre-trained\nfeatures to learn a landmark detector by the traditional equivariance method.\nOur method produces state-of-the-art results in several challenging landmark\ndetection datasets such as the BBC Pose dataset and the Cat-Head dataset. It\nperforms comparably on a range of other benchmarks.",
  "text": "Pretrained equivariant features improve unsupervised\nlandmark discovery\nRahul Rahaman∗, Atin Ghosh†, and Alexandre H. Thiery‡\nDepartment of Statistics and Applied Probability,\nNational University of Singapore\nApril 8, 2021\nAbstract\nLocating semantically meaningful landmark points is a crucial component of a large num-\nber of computer vision pipelines. Because of the small number of available datasets with\nground truth landmark annotations, it is important to design robust unsupervised and semi-\nsupervised methods for landmark detection. Many of the recent unsupervised learning meth-\nods rely on the equivariance properties of landmarks to synthetic image deformations. Our\nwork focuses on such widely used methods and sheds light on its core problem, its inability to\nproduce equivariant intermediate convolutional features. This ﬁnding leads us to formulate\na two-step unsupervised approach that overcomes this challenge by ﬁrst learning powerful\npixel-based features and then use the pre-trained features to learn a landmark detector by\nthe traditional equivariance method. Our method produces state-of-the-art results in sev-\neral challenging landmark detection datasets such as the BBC Pose dataset and the Cat-Head\ndataset. It performs comparably on a range of other benchmarks.\n1\nIntroduction\nProducing models that learn with minimal human supervision is crucial for tasks that are oth-\nerwise expensive to solve with manual supervision. Discovering landmarks within natural im-\nages falls into this category since manually annotating landmark points at the pixel level scales\npoorly with the number of annotations and the size of the dataset. Recent advancements in Deep\nLearning have brought signiﬁcant performance improvements to various computer vision tasks.\nThis work focuses on using deep-learning models for unsupervised discovery of landmarks, as\n∗rahul.rahaman@u.nus.edu\n†atin.ghosh@u.nus.edu\n‡a.h.thiery@nus.edu.sg\n1\narXiv:2104.02925v1  [cs.CV]  7 Apr 2021\nadopted in several recent articles [Jakab et al., 2018, Thewlis et al., 2017b, 2019, Zhang et al., 2018].\nEquivariance, the fundamental concept most modern unsupervised landmark discovery ap-\nproaches are based upon, formalizes the intuitive property that, when an image is deformed\nthrough the action of a geometric transformation, semantically meaningful landmarks associ-\nated with that image should also be deformed through the action of the same deformation. Most\nrecent approaches for unsupervised landmark discovery leverage this equivariance property of\nlandmark points.\nThe tasks of discovering and detecting semantically meaningful landmarks have been exten-\nsively studied. Developed before the advent of more recent deep-learning techniques and among\nthe large number of proposed methods for solving these problems, it is worth mentioning the su-\npervised and weakly-supervised learning approaches based on templates [Pedersoli et al., 2014,\nZhu and Ramanan, 2012], active appearance [Cootes et al., 1998, Cristinacce and Cootes, 2008,\nMatthews and Baker, 2004], as well as the regression-based models [Cao et al., 2014, Dantone\net al., 2012, Ren et al., 2014, Valstar et al., 2010]. Most of the recently proposed methodologies\nsuch as Cascade CNN [Sun et al., 2013], CFAN [Zhang et al., 2014a], coordinate regression [Nibali\net al., 2018] and several variants [Wu et al., 2017, Xiao et al., 2016, 2017, Yu et al., 2016, Zhang\net al., 2014b, 2015] leverage deep-convolutional feature extractors for solving the task of super-\nvised landmark detection.\nIn contrast, the approach described in this text is fully unsupervised and yet can perform\nmore robustly than several supervised approaches. Among other unsupervised landmark discov-\nery pipelines, the approach of Thewlis et al. [2017b] was one of the ﬁrst ones to leverage the\nequivariance property of landmarks. Equivariance was also used to discover 3D landmarks in the\nwork of Suwajanakorn et al. [2018]. Several other approaches are based on the creation of repre-\nsentations that disentangle geometric structures from appearance. Such approaches can be seen\nin the work of Rocco et al. [2017], Warpnet [Kanazawa et al., 2016], as well as Shu et al. [2018].\nIn the work of Thewlis et al. [2017a], objects are mapped to labels for capturing structures in the\nimage. Similar ideas can also be found [Thewlis et al., 2019].\nSeveral approaches attempt to disentangle the latent representation of images into visual and\nstructural content by comparing representation between related images. These related images\ncan be obtained through synthetic image deformations, or by exploiting the temporal continuity\nof video streams. Jakab et al. [2018] reconstructs images by combining the visual and the struc-\ntural content to discover landmarks. Simultaneous image generation and landmark discovery\ncan also be seen in [Reed et al., 2017, 2016, Zhang et al., 2018]. Although this set of methods has\nproven to be successful in some situations, the reliance on reconstructing high-resolution images\nis problematic in several aspects: these methods require high-capacity models, are often slow\nto train, and perform sub-optimally in situations where images are corrupted by a high level of\nnoise or in the presence of artifact, as is, for example, common in medical applications. The pro-\nposed approach does not rely on image reconstruction and the numerical experiments presented\n2\nin Section 5 demonstrate that it can still perform competitively on challenging tasks involving\nhuman-pose estimations, a task generally solved by leveraging the temporal continuity of adja-\ncent video frames.\nContributions. In this article, we take a closer look at the equivariance property of intermediate\nrepresentations within deep learning models for landmark discovery.\n• We empirically demonstrate that standard landmark discovery approaches are not eﬃcient\nat enforcing that the intermediate representations satisfy the equivariance property. To\nthis end, we deﬁne a metric similar to cumulative error distribution curves that we use to\nquantify the equivariance of convolutional features.\n• We propose a two-step approach for landmark discovery. In the ﬁrst step, powerful equiv-\nariant features are learned through a contrastive learning method. In a second step, these\nequivariant features are leveraged within more standard unsupervised landmark discovery\npipelines. Our method is able to ﬁnd semantically meaningful and consistent landmarks,\neven in challenging datasets with high inter-class variations.\n• Our numerical studies indicate that the proposed methodology outperforms previously pro-\nposed approaches in the diﬃcult task of ﬁnding human body landmarks in the BBC Pose\ndataset, facial landmarks in the Cat-head dataset. Our method performs comparably to\nother unsupervised methods in the CelebA-MAFL facial landmark dataset.\n2\nBackground\nLandmarks can be deﬁned as a ﬁnite set of points that deﬁne the structure of an object. A partic-\nular class of objects is expected to have the same set of landmarks across diﬀerent instances. In\nmany scenarios, the variations in the locations of the landmarks across diﬀerent instances of the\nsame object can be used for downstream tasks (e.g., facial recognition, shape analysis, tracking).\nConsider an image x ∈RH×W×C represented as a tensor with height H, width W and num-\nber of channels C. This image can also be thought of as (the discretization of) a mapping from\nΛ ≜[0, H] × [0, W] to RC. We are interested in designing a landmark extraction function\ny(x) = [y1(x), . . . , yK(x)] ∈ΛK that associates to the image x a set of K ≥1 landmarks\n{yk(x)}K\nk=1 where yk(x) ∈Λ for 1 ≤k ≤K. In this work, the mapping y : RH×W×C →ΛK\nis parametrized by a neural network, although it is not a requirement. Occasionally, in order to\navoid notational clutter, we omit the dependence of y on the input image x.\nConsider a bijective mapping g : Λ →Λ that describes a geometric deformation within\nof the domain Λ. Examples include translations, rotations, elastic deformations, with a proper\ntreatment of the boundary conditions. To any function ϕ : Λ →RC can be associated the\nfunction g♯◦ϕ : Λ →RC deﬁned as g♯◦ϕ(λ) = ϕ(g−1(λ)) for all λ ∈Λ. Since images can\nbe thought of as discretizations of functions from Λ to RC, the function g♯also deﬁnes, with a\n3\nslight abuse of notation, a deformation mapping on the set of images. With these notations, the\nfunction g describes the transformation of coordinates (i.e. elements of Λ), while the function\ng♯describes the action of the transformation g when applied to all pixels of a given image. A\nlandmark extraction function yk : RH×W×C →Λ is said to satisfy the equivariance property if\nthe condition\nyk[g♯(x)] = g[yk(x)]\n(1)\nholds for all images x ∈RH×W×C. Equation (1) translate the intuitive property that, when an\nimage is deformed under the action of a deformation mapping g♯, the associated landmarks are\ndeformed through the action of the same deformation.\nIn this article, we follow the standard approach [Jakab et al., 2018, Thewlis et al., 2017b] of\nimplementing the landmark extraction mapping yk : RH×W×C →Λ as a diﬀerentiable function\nby expressing it as the composition of a heatmap function hk : RH×W×C →RH×W with the\nstandard spatial soft-argmax [Chapelle and Wu, 2010] function σarg : RH×W →Λ,\nx −→\nhk\nhk(x) −→\nσarg yk(x)\nso that yk(x) = σarg◦hk(x) ∈Λ. In order to train a landmark extraction model, it is consequently\na natural choice to try to minimize the equivariance loss\nLeqv(x, g) = 1\nK\nK\nX\nk=1\n\r\ryk[g♯(x)] −g[yk(x)]\n\r\r2 .\n(2)\nThis formulation was ﬁrst used by Thewlis et al. [2017b], and then subsequently adopted by\nseveral other articles [Thewlis et al., 2019, Zhang et al., 2018]. Unfortunately, minimizing the\nequivariance loss alone generally leads to a trivial solution: all landmarks coincide. In practice,\nit is consequently necessary to consider an additional diversity loss\nLdiv(x) =\nX\nλ∈Λ\n X\nk∈K\n[σ ◦hk(x)]λ −max\n1≤k≤K[σ ◦hk(x)]λ\n!\n(3)\nwhere σ : RH×W →(0, 1)H×W is the usual spatial softmax function, although other choices are\ncertainly possible. The notation [σ ◦hk(x)]λ denotes the spatial softmax evaluated at the coor-\ndinate λ ∈Λ. The loss (3) was ﬁrst proposed in Thewlis et al. [2017b] in conjunction with the\nequivariance loss to penalize the concentration of diﬀerent heatmaps at the same coordinate. The\ndiversity loss is often computed for small image patches rather than individual coordinates. Land-\nmarks can be learned by directly minimizing the total loss Ldiv + Leqv on a dataset on unlabeled\nimages and randomly generated deformation mappings g.\n3\nEquivariance of intermediate representations\nthis section deﬁnes a measure of equivariance for convolutional features. This allows us to mea-\nsure the equivariance of intermediate convolutional features learned by the set of methods de-\n4\nscribed in Section 2.\nEquivariant features. Fully Convolutional Neural Networks (FCNN) are leveraged in most mod-\nern deep learning based landmark discovery methods [Jakab et al., 2018, Thewlis et al., 2017b,\n2019]. Let x ∈RH×W×C be an image and F(x) = f ∈RH×W×D be a convolutional feature ob-\ntained from x by passing it through a fully convolutional network F(·). One can always assume,\nusing a spatial change of resolution if necessary, that the convolutional features have the same\ndimensions as the input image x and can thus be parametrized by Λ ≡[0, H]×[0, W]. For any 2D\ncoordinate λ ≡(u, v) ∈Λ, the vector f(λ) ∈RD can be thought of as a D-dimensional represen-\ntation of a small region (i.e. receptive ﬁeld) of the input image x centred at λ ≡(u, v) ∈Λ. This\nremark motivates a natural extension of the notion of equivariance for convolutional features.\nConsider two images x and x′ related by a geometric deformation g : Λ →Λ in the sense that\nx′ = g♯(x), as well as the associated convolutional features deﬁned as f = F(x) and f ′ = F(x′).\nThe convolutional feature extractor would be perfectly equivariant if the extracted features f\nand f ′ were such that f(λ) = f ′(g[λ]) for any location λ ∈Λ. A landmark extraction function,\nas described in Section 2, that exploits equivariant convolutional features would directly inherit\nthe equivariance properties of these features. Note, though, that training a landmark extractor\nthrough the loss function described in Equation (2) only attempts to enforce the equivariance\nproperty of the ﬁnal landmarks. Consequently, it is natural to investigate whether (i) this equiv-\nariance property naturally percolates through the convolutional neural network and leads to the\ncreation of equivariant convolutional features (ii) there are more eﬃcient training methodologies\nfor enforcing the equivariance property throughout the neural network model.\nMeasure of equivariance: Let f, f ′ be the convolutional features associated to two images x\nand x′ = g♯(x) related through the deformation function g : Λ →Λ. As described previously,\none can always assume that the convolutional features and the images themselves have the same\ndimension and can, consequently, be parametrized by Λ ≡[0, H] × [0, W]. To quantify the\nequivariance of the convolutional features, it is computationally impractical to require the prop-\nerty f(λ) = f(g[λ]) to hold for all λ ∈Λ. Furthermore, it is desirable to focus on the equivariance\nproperty of a few well chosen locations.\nTo this end, in our study of the equivariance properties of intermediate convolutional features,\nwe exploit a labelled dataset D whose images have been annotated with J ≥1 semantically\nmeaningful landmark points: to each image x is associated y1, . . . , yJ ∈Λ landmarks. For such\nan image x and a geometric deformation g, consider the deformed image x′ = g♯(x) as well as\nthe associated convolutional features f and f ′. For each landmark yj associated to x ∈D, we\nconsider the location by′\nj ∈Λ in the deformed image x′ whose representation is the most similar\nto f ′(yj),\nby′\nj = argmax\nλ∈Λ\nsim\n\u0000f ′(λ), f(yj)\n\u0001\n.\n(4)\nIn (4) we have used the cosine similarity,\nsim(f1, f2) ≜\n\u001c f1\n∥f1∥,\nf2\n∥f2∥\n\u001d\n,\n(5)\n5\nalthough other choices are indeed possible. For discriminative and equivariant convolutional\nfeatures, we expect the Euclidean distance between by′\nj and yj to be small. Consequently, for a\ndistance threshold d > 0, the accuracy at threshold d is deﬁned as\nAcc(d) ≜\n1\n|D| · |J|\nX\nj∈J,xi∈D\nI\n\u0000∥by′\ni,j −y′\ni,j∥≤d\n\u0001\n.\n(6)\nwhere by′\ni,j and y′\ni,j denote the j-th landmark quantities associated to image xi ∈D. In the sequel,\nthe curve {(d, Acc(d)) : d≥0} will be referred to as the accuracy curve. An example is depicted\nin Figure 1.\nFeatures learned from landmark equivariance. The accuracy metric (6) can be used to in-\nvestigate whether the intermediate convolutional features learned by directly minimizing the\nequivariance loss (2) combined to the diversity regularization (3), i.e. Leqv + Ldiv, also exhibit\ngood equivariance properties. The approach consisting in directly minimizing the combined loss\nLeqv+Ldiv is referred to as the end-to-end learning approach, in contrast to our proposed two-step\napproach, described in Section 4. Furthermore, we use standard so-called hourglass convolutional\nneural architectures as recommended by most recent work on landmark discovery [Jakab et al.,\n2018, Thewlis et al., 2019, Zhang et al., 2018]. Such a model architecture T ◦F can be described\nas the composition of a convolutional feature extractor F : RH×W×C →RH×W×D that produces\nD-dimensional spatial features followed by another convolutional network T : RH×W×D →\nRH×W×K that transforms these features into K heatmaps (h1, . . . , hK) where hk ∈RH×W.\nAs described in Section 2, these K heatmaps are ﬁnally transformed into K landmark points\nσarg(h1), . . . , σarg(hK) ∈Λ through a spatial soft-argmax operation. For our experiments, we\nchose K = 50 landmark points and convolutional features of dimension D = 64. We trained\nin an end-to-end manner the hourglass network T ◦F on the BBC Pose dataset [Charles et al.,\n2013a] training dataset for landmark discovery. The accuracy metric (6) was then computed on\nthe BBC Pose dataset’s test set (i.e. not used during training). To this end, we chose a set of J = 7\nmanually annotated human body landmarks: head, two shoulders, two elbows, and two wrists.\nThe images were resized to a resolution 128 × 128 pixels.\nFigure 1 presents examples from the BBC Pose test dataset where random rotations and ran-\ndom elastic deformations were applied. The circles denote the locations by′\ni estimated through\nEquation (4) and the crosses indicate the ground truth locations ¯y′\ni; these locations were obtained\nby training a landmark extractor in an end-to-end manner. The right-most plots of Figure 1 report\nthe accuracy curves {(d, Acc(d)) : d > 0)} estimated from a set of J = 7 annotated landmarks.\nEach of the four accuracy curves is associated to one of four intermediate convolutional features\ncoined Layer 1 to Layer 4. Here, Layer 1 is the output of the feature extractor F(·) while Layer (2,\n3, 4) are representation situated in between Layer 1 and the ﬁnal heatmaps generated through the\ncomplete network T◦F. For comparison, the black dashed line shows the performance associated\nwith locations by′\ni generated uniformly at random within Λ = [0, H]×[0, W]. The results indicate\nthat, even though the intermediate convolutional features do possess some degree of equivariance\n(i.e. better than random), the performance is relatively poor with an accuracy Acc(d) = 57% for\na distance threshold of d = 20 pixels.\n6\n0\n5\n10\n15\n20\n0%\n25%\n50%\n20.0\n22.5\n25.0\n27.5\n30.0\n60%\n67%\n73%\n80%\n          Deformed samples                                                            Acc (y) vs distance in pixel (x)\nLayer #1\nLayer #2\nLayer #3\nLayer #4\nRandom\nFigure 1: Equivariance of intermediate convolutional features when trained by the end-to-end approach. The ﬁrst\ntwo images show deformed BBC Pose images with predicted (o) and ground truth (+) locations of J = 7 annotated\nlandmarks (head, two wrists, two elbows, two shoulders) found by maximizing similarity (4) from the features ex-\ntracted from Layer 1, i.e. created by the feature extractor F(·). The plots on the right show the accuracy curves\n(6) of several intermediate convolutional representations. The black dashed-line is a baseline consisting of random\npredictions. Even at a threshold distance d = 20 pixels, the accuracy of most intermediate representations is as low\nas Acc(d = 20) ≈57%. The rightmost zoomed-in plot shows that the feature of Layer 1, i.e. the output of the feature\nextractor F(·), has the worst performance among the four considered representations.\n4\nMethod\nThe previous section demonstrates that the direct minimization of the combined loss Leqv + Ldiv\nis not eﬃcient at inducing equivariance properties to the intermediate convolutional features. In\ncontrast, the ﬁrst step of the methodology described in this section aims at directly enforcing that\nthe intermediate features enjoy enhanced equivariance properties. In a second step, these pre-\ntrained features are leveraged within more standard landmark discovery pipelines. Although we\ncan choose to pre-train any of the convolutional features of the complete landmark extractor T◦F,\nour experiments indicate that it is most eﬃcient to concentrate on the ﬁnal convolutional features\n(i.e. layer 1 with the terminology of the previous section) generated by the feature extractor\nnetwork F(·). As depicted in the right-most plot of Figure 1, when end-to-end training is used,\nthe layer 1 is the worst in terms of equivariance among the four layers considered in the previous\nsection. At a heuristic level, this can be explained by the fact that the layer 1 is the further “away\"\nfrom the training signal among the four layers considered.\nOur proposed methodology proceeds in two steps. Step 1: train the feature extractor con-\nvolutional network F with the contrastive learning approach described in the remainder of this\nsection. Step 2: freeze the parameters of the feature extractor network F(·) and train the com-\nplete network T◦F(·) with a standard landmark discovery approach, i.e. minimize the combined\nobjective Leqv + Ldiv.\nStep 1: training the feature extractor F(·). We employ a contrastive learning framework\nsimilar to simCLR [Chen et al., 2020] to train the features extractor. To this end, we leverage\n(1) random geometric deformations g : Λ →Λ that only change the locations of the pixels\n(eg. rotations, dilatations, elastic deformations) (2) random appearance changes denoted by r :\nRH,W,D →RH,W,D that do not change the locations of the pixels (eg. noise addition, intensity\n7\nchange, contrast perturbation). For a batch of images (x1, . . . , xB) ≡B, we consider augmented\nversions deﬁned as x′\nb = (rb◦g♯\nb)(xb) augmented by random generated geometric and appearance\naugmentations gb and rb. Furthermore, for each image xb ∈B in the batch, we also consider\nK ≥1 randomly selected spatial locations λb,1, . . . , λb,K ∈Λ: the location λb,k is transform to\nλ′\nb,k ≡gb(λb,k) under the action of the geometric deformation gb. Finally, these pairs of images\nare passed through the feature extractor F : RH×W×C →RH×W×D and D-dimensional features\nare extracted at the locations λb,k and λ′\nb,k for 1 ≤b ≤|B| and 1 ≤k ≤K. We have fb,k ≜\nF(xb)[λb,k] ∈RD and f ′\nb,k ≜F(x′\nb)[λ′\nb,k] ∈RD. The feature extractor F is trained by minimizing\nthe contrastive loss Lcontrast deﬁned as\nLcontrast ≜−\n1\n|B| · K\nX\nb,k\nlog pb,k + log p′\nb,k\n(7)\nwhere the quantities pb,k and p′\nb,k are deﬁned as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npb,k ≜\nsτ(fb,k, f ′\nb,k)\nP\n(n,i)̸=(b,k)\nsτ(fb,k, fn,i) + P\nn,i\nsτ(fb,k, f ′\nn,i)\np′\nb,k ≜\nsτ(fb,k, f ′\nb,k)\nP\n(n,i)̸=(b,k)\nsτ(f ′\nb,k, f ′\nn,i) + P\nn,i\nsτ(f ′\nb,k, fn,i)\nand sτ(a, b) ≜exp{sim(a, b)/τ} is the exponential similarity between a, b ∈RD scaled by\ntemperature τ. The similarity sim(·, ·) is deﬁned in Equation (5). Minimizing the contrastive\nloss not only ensures equivariance, it also encourages dissimilarity between features belonging\nto diﬀerent parts of the image. The landmark detector trained in the second step described below\nbeneﬁts from both the consistency and contrast of these features.\nStep 2: training the landmark detector. After training the feature extractor F(·) as described\nin the previous section, its parameters are frozen. The entire network T ◦F is then unsupervis-\nedly trained in a standard fashion for landmark discovery. In this text, we follow the approach of\nThewlis et al. [2017b] although other approaches are possibles. We use a fully convolutional ar-\nchitecture with residual connections and minimize the combined loss Leqv+Ldiv. Furthermore, to\nencourage the heatmaps to be more concentrated, we also penalize the variance of the heatmaps.\nIn other words, we consider an additional regularization term expressed as\nLvariance ≜1\n|B|\n|B|\nX\nb=1\nTr(Cov [σ ◦T ◦F(xb)])\nIn the above Equation, σ(·) denotes the spatial softmax operation that transforms a heatmap\nT ◦F(xb) on Λ into a probability measure on Λ. The notation Cov designates the covariance\nmatrix. We defer additional details of implementation to the supplementary.\n8\n0\n5\n10\n15\n20\n0%\n25%\n50%\n75%\n0\n5\n10\n15\n20\n0%\n25%\n50%\n75%\n          Deformed samples                                                            Acc (y) vs distance in pixel (x)\nLayer #1\nLayer #2\nLayer #3\nLayer #4\nPre-trained Layer #1\nEnd-to-end Layer #1\nFigure 2: Performance gains oﬀered by the two-steps approach. The ﬁrst two images show deformed BBC Pose\nimages with predicted (o) and ground truth (+) locations of J = 7 annotated landmarks (head, two wrists, two\nelbows, two shoulders) found by maximizing similarity (4) from the features extracted from Layer 1, i.e. created by\nthe feature extractor F(·). In comparison to Figure 1, predictions are signiﬁcantly more accurate. The third plot\nshows the accuracy curves (6) of several intermediate convolutional representations. The right-most plot compares\nthe equivariance properties of Layer 1 when trained with an end-to-end method and with the proposed two-step\napproach.\n4.1\nImplementation Details\nWe choose the network F to be an hourglass encoder-decoder network. The encoder contains\nfour convolution-maxpool blocks, each of which halves the spatial dimension of its input. Sym-\nmetrically, the decoder block expands the spatial dimension to produce ﬁnal features of the same\nspatial dimension as that of the original input image. We use skip-connections to join the en-\ncoder and decoder. Each of the convolution blocks contains two convolution units separated by\nnon-linear activation ReLU and one max-pool/up-sample unit. The ﬁnal channel dimension of\nthe features F(x) is D = 64. The landmark network T is fully convolutional with the same inter-\nmediate spatial dimension as that of the image. For contrastive learning, we used a temperature\nof τ = 0.1.\nWe ﬁrst train the network F with the contrastive loss described in equation 6 of the main\ndraft on all the datasets. The image deformation set includes random aﬃne transformations, ran-\ndom crop and zoom operations, image augmentations related to intensity and contrast, as well\nas elastic deformations. Next, we freeze the parameters of the network F and train the landmark\ndetector. For this, we use the combined loss Leqv + Ldiv + Lvariance with the image deformation as\nrandom rotation and elastic deformation. Note that this combined loss was also used to train the\nend-to-end network presented in ablation.\nFor the diversity loss, we choose image patches of size 8 × 8 for images of size 128 × 128\nand 10 × 10 patches for 100 × 100 images. We use the Adam optimizer with a learning rate\nof 3.10−4 and LR decay of 0.9 every epoch for both training steps. For both steps, we train our\nmodel for 20 epochs. We also use L2 regularization of strength 5.10−4. To check the performance\nof unsupervisedly discovered landmarks, we use a linear regressor without bias to predict the\nground truth annotations. The regressor is always trained on the training set and then evaluated\non the test dataset. We keep the training and test split the same as previous works. We use Ridge\n9\nregression as our choice of regressor with regularization α = 0.1.\n5\nExperiments\nIn this section, we illustrate the performance gains in terms of equivariance of the intermedi-\nate features when adopting our proposed two-step method. Figure 2 shows some examples of\ndeformed images from the BBC Pose dataset with the ground truth and predicted locations of\nJ = 7 landmarks: the setting is similar to the one of Figure 1. The predicted locations were\nestimated through maximizing the similarity, as described in Equation (4). The third plot shows\nthe accuracy of the four intermediate representations layers (1,2,3,4) previously described at the\nend of Section 4: note that all layers beneﬁted in terms of equivariance from the two-step ap-\nproach, even though only layer 1 was pre-trained with contrastive learning. The right-most plot\ncompares the equivariance accuracy curve associated to Layer 1 when the model is trained in an\nend-to-end fashion and when trained with our proposed approach: there is a considerable boost\nin equivariance accuracy (6). At a distance threshold of d = 20 pixels, the end-to-end method\nleads to an accuracy of Acc(d = 20) = 57% while the proposed two-step approach leads to an\nequivariance accuracy of Acc(d = 20) = 87%.\n5.1\nHuman body landmarks.\nLearning human body landmarks in an unsupervised manner poses some unique challenges. Un-\nlike rigid and semi-rigid structures such as the human face, vehicles, or other non-deformable\nobjects, the human body allows ﬂexibility around diﬀerent body joints. The BBC Pose dataset is\ncomprised of 20 videos of sign language interpreters. The positional variability of the diﬀerent\nupper body parts during sign language communication leads to unique challenges. Following the\nsetup of Jakab et al. [2018], loose crops around the interpreters were resized to 128 × 128 and\nused for our experiments.\nQualitative results. We used the training part (ten videos) of the BBC Pose dataset to learn in\nan unsupervised manner K = 50 landmarks with our proposed two-step method. After training,\nthe discovered landmarks were computed on the validation dataset (ﬁve videos) and were used\nas features to train a linear regressor (without bias) to predict the actual location of the seven\nannotated landmarks. This regression model is then evaluated on the test dataset (ﬁve videos).\nFigure 3 shows some example of the test dataset with predicted (o) and ground truth (+) locations\nof the landmarks.\nQuantitative results. In table 1 we present our numerical results on the unsupervised landmark\ndetection. The ﬁrst few methods are fully supervised, and most of these methods leverage tem-\nporal continuity to track landmarks reliably across video frames. In contrast, Jakab et al. [2018]\nand ours are unsupervised and only use the ground truth annotations of the validation dataset\nto predict the locations of the annotated landmarks on the test dataset. Following the evaluation\nprocedure of Jakab et al. [2018], we report the proportion of landmarks predicted within a Eu-\nclidian distance of 6 pixels from the ground truth in an image of size 128 × 128.\n10\nFigure 3: Landmark prediction in BBC Pose dataset: Predicted (hollow circle: ‘◦’) vs. ground truth (‘+’) locations of\nthe seven annotated landmarks (head, two shoulders, two wrists, two elbows) in BBC Pose dataset. Fifty unsupervised\nlandmarks were used for this prediction. Visually our predictions and ground truth positions are quite close and are\nin fact coinciding in many examples.\nTable 1 shows the prediction score for diﬀerent classes of upper-body joints, as well as the\noverall score. When averaged all the landmarks, our method outperforms three of the supervised\nmethods and outperforms the unsupervised method of Jakab et al. [2018]. Furthermore, except\nfor the wrists, our method outperforms all the supervised methods for predicting individual land-\nmarks. The relatively poor performance for detecting the location of the wrists stems from the\nfact that our method uses elastic deformations of the same image frame to learn equivariance. In\ncontrast, most of the supervised methods exploit implicit temporal continuity while the method\nof Jakab et al. [2018] uses a diﬀerent frame from the same video to obtain original and deformed\nimage pair. Our methodology could certainly leverage recent image key-point matching methods\nin order to leverage more eﬃciently video modalities. Finally, table 2 reports the performance of\nour method as a function of the number of supervised samples from the validation dataset (total\n1000 samples) used to ﬁt the regression model. We can see that with just 100 supervised annota-\ntions, our method outperforms the approach of Jakab et al. [2018].\n5.2\nFacial landmarks\nFor the task of learning facial landmarks, we implement our methodologies on the CelebA [Liu\net al., 2015] dataset (200k human face images of 10k celebrity) as well as the Cat Head [Zhang\net al., 2008] dataset (9k images of cat faces). The celebA dataset contains 5 annotated facial land-\nmarks (two eyes, nose, two corners of mouth) whereas the cat dataset has 9 annotated landmark\npoints (six points in ears, two eyes, one mouth). The cat head dataset features more holistic\nvariations than human face images and contains a considerable amount of occlusions. Similar\nto human body landmarks, we train our two-step landmark detection network on a training set,\nﬁt a linear regressor on a validation set and ﬁnally evaluate the performance of the regressor on\na test dataset. In all our experiments related to facial landmarks, we adopt the popular metric\n11\nProportion (%) within 6 pixels\nSupervised\nHead\nWrst\nElbw\nShldr\nAvg.\nPﬁster et al. [2015]\n98.0\n88.4\n77.1\n93.5\n88.0\nCharles et al. [2013b]\n95.4\n72.9\n68.7\n90.3\n79.9\nChen and Yuille [2014]\n65.9\n47.9\n66.5\n76.8\n64.1\nPﬁster et al. [2014]\n74.9\n53.0\n46.0\n71.4\n59.4\nYang and Ramanan [2011]\n63.4\n53.7\n49.2\n46.1\n51.6\nUnsupervised\nJakab et al. [2018]\n76.1\n56.5\n70.7\n74.3\n68.4\nOurs\n99.4\n33.5\n78.3\n93.5\n72.9\nTable 1: Proportion of landmarks predicted within a Euclidean distance of 6 pixels from the ground truth on image\nof size 128 × 128 of the BBC-Pose dataset. Our method outperforms all the supervised methods in detecting head,\nelbow and shoulder. Our model does not perform well on locating ‘wrists’ mainly because unlike our method, the\nother methods including the unsupervised method leverages temporal continuity of video frames.\nOverall accuracy %\n# Training Samples\nJakab et al. [2018]\nOurs\n5\n-\n49.7 ± 5.0\n10\n-\n57.6 ± 3.8\n50\n-\n67.2 ± 0.8\n100\n-\n69.4 ± 0.6\n500\n-\n72.1 ± 0.5\n1000 (full dataset)\n68.4\n72.9\nTable 2: Proportion of landmarks predicted within a Euclidean distance of 6 pixels from the ground truth on image\nof size 128 × 128 of the BBC-Pose dataset. The table reports the perfmance as a function of the number of training\nsamples used during training. Our proposed method outperforms previous unsupervised approaches with only 100\nsamples.\ninter-ocular distance normalized MSE,\niod-mse ≜\n1\n|D| · |I|\nX\ni∈I,x∈D\n∥byi −¯yi∥\nIod(x) ,\n(8)\nwhere D denotes the test dataset, I is the set of manually annotated landmarks, byi and ¯yi are\nrespectively the predicted and ground truth location of the ith landmark in image x, and Iod(x)\ndenotes the distance between the ground truth eyes in an image x.\nCat head dataset. We keep the size of the cat images to their original 100 × 100 resolution\nand maintain the same train/test split as Zhang et al. [2018]. Furthermore, we discard the two\nannotations corresponding to the end of the ears [Thewlis et al., 2017b, Zhang et al., 2018].\nQualitative results. Figure 4 shows some examples of cat head test dataset. The top row\nshows images annotated with the ten unsupervised landmark locations discovered by our model.\nThe bottom row shows the result of predicting ground truth landmark locations of seven facial\nlandmark points. It is worth noting that the inter-example variation in this dataset is more than\n12\nFigure 4: Cat head dataset: the top row shows K = 10 unsupervised landmarks points (+) predicted by our model.\nThe bottom row shows seven facial landmark points (four in ears, two eyes, mouth) predicted by the ten unsupervised\nlandmarks through a linear regressor. Visibly, this dataset oﬀers a lot of inter-example variations due to diﬀerent\ncat breeds and colors. Our unsupervised landmarks are semantically meaningful, and our supervised predictions are\nvisibly accurate.\nPerformance on Cat head dataset\nLandmark count\nK = 10\nK = 20\nThewlis et al. [2017b]\n26.76\n26.94\nZhang et al. [2018]\n15.35\n14.84\nOurs (# Training Samples)\n5\n21.5 ± 2.4\n21.1 ± 1.6\n10\n18.9 ± 0.6\n18.8 ± 1.5\n100\n16.2 ± 0.3\n15.5 ± 0.3\n500\n15.2 ± 0.2\n14.6 ± 0.3\n1000\n14.9 ± 0.0\n14.3 ± 0.1\n7747 (full dataset)\n14.59\n13.80\nTable 3: Prediction performance (inter-ocular distance normalized MSE expressed in %) and sample eﬃciency of\nour method on Cat head dataset when K = 10 and K = 20 were unsupervisedly learned. Our method outperforms\nprevious unsupervised models with only 500 samples. The total number of training samples is more than 7000.\nany human face dataset as the images vary in pose, color, cat breed, occlusion.\nQuantitative results. Table 3 shows performance of our method. The performances of two\nunsupervised methods are listed at the top of the table. The bottom part of the table reports the\nperformance of our proposed approach as a function of the number of supervised samples. We\nreport both the mean and standard deviation in order to account for the variability in the regres-\nsion training set. Our method can outperform previous unsupervised methods when only 500\nsupervised samples out of the total available 7, 747 training samples are utilized.\nCelebA dataset. The CelebA images were resized to 128 × 128 as done by most works [Jakab\net al., 2018]. The MAFL dataset, a subset of the CelebA dataset, has a training set of 19k images\nand a test set of 1k images. Like previous works, we take our unsupervised model trained on\nCelebA and then train a linear regressor to predict the MAFL training set’s manual annotations.\nQualitative results. Some predictions generated by our approach are depicted in Figure 5. The\ntop row shows ten unsupervised landmarks learned for each image. The bottom row shows ﬁve\n13\nPerformance on MAFL dataset\nLandmark count\nNA\nK = 30\nK = 50\nSupervised\nCFAN [Zhang et al., 2014a]\n15.84\n-\n-\nCascaded CNN [Sun et al., 2013]\n9.73\n-\n-\nTCDCN [Zhang et al., 2015]\n7.95\n-\n-\nMTCNN [Zhang et al., 2014b]\n6.90\n-\n-\nUn/Self-supervised\n-\n-\n-\nThewlis et al. [2017b]\n-\n7.15\n6.67\nThewlis et al. [2017a]\n5.83\n-\n-\nShu et al. [2018]\n5.45\n-\n-\nSanchez and Tzimiropoulos [2019]\n3.99\n-\n-\nZhang et al. [2018]\n-\n3.16\n-\nThewlis et al. [2019]\n2.86\n-\n-\nJakab et al. [2018]\n-\n2.58\n2.54\nOurs\n-\n4.59\n4.31\nTable 4: Inter-ocular distance normalized MSE (expressed in %) dataset as deﬁned in Equation (8).\nNo. sample\nJakab et al. [2018]\nOurs\n1\n12.89 ± 3.2\n11.12 ± 2.1\n5\n8.16 ± 0.9\n8.82 ± 0.4\n10\n7.19 ± 0.4\n8.27 ± 0.3\nfull data\n2.58\n4.59\nTable 5: Sample eﬃciency on MAFL. When the sample size is extremely low, our model performs comparably. The\nfull training dataset contains 19000 images of human face.\nlandmarks as predicted by the unsupervised landmarks through a linear regressor without bias.\nQuantitative results. Table 4 reports the performance of several supervised, unsupervised, and\nsemi-supervised methods. Although our performance is not better than the previous state-of-the-\nart in terms of iod-mse%, for an image size of 128 × 128, the diﬀerence in actual MSE in terms\nof pixel distance is marginal. Table 5 presents the performance of the proposed approach as a\nfunction of the number of supervised samples utilized to train the regressor: our model performs\ncomparably with the SOTA.\n5.3\nAblation study\nWe perform our ablation table 6 that lists three methods. We compare our proposed method to\nthe one of Thewlis et al. [2017b] since it is the most similar. The second row reports the perfor-\nmance of the end-to-end learning framework of Thewlis et al. [2017b] when the architecture is\nreplaced by ours: the only diﬀerence when compared to the ﬁrst row is solely due to the eﬀect\nof our architecture. In the third row, we present our proposed two-step method. The improve-\n14\nFigure 5: MAFL facial landmarks: Examples from MAFL test dataset. The top row shows ten unsupervised land-\nmarks points (+) predicted by our model. The bottom row shows ﬁve predicted facial landmark points (eyes, nose,\nmouth) learned from the 10 unsupervised landmarks.\nBBC Pose Accuracy (%)\nMAFL mse (%)\nCat head mse (%)\nMethod\nHead\nWrst\nElbw\nShldr\nAvg.\n30\n50\n10\n20\nThewlis et al. [2017b]\n–\n–\n–\n–\n–\n7.15\n6.67\n26.76\n26.94\nEnd-to-end\n72.0\n15.5\n45.5\n57.0\n44.0\n6.57\n5.97\n22.97\n23.42\nPre-training\n99.4\n33.5\n78.3\n93.5\n72.9\n4.59\n4.31\n14.74\n13.80\nTable 6: Result of our ablation study. In the ﬁrst row we show the performance of the original work by Thewlis\net al. [2017b]. In the second row we adopt the end-to-end training method of equivariance training but replace the\narchitecture with our own model (S : T). The third row shows the result of our proposed two-stage method. The\narchitecture and the losses used in second and third row is same. Hence, the improvement from ﬁrst to second row\nis due to the architecture and the ﬁnal improvement from second to third row is due to our proposed pre-training\nmethod.\nment in performance from the second to the third row is only due to our pre-training method\napproach: we emphasize that the neural architectures and landmark losses are exactly the same. Our\napproach consistently leads to signiﬁcantly improved performances. In ﬁgure 6 we plot the ﬁnal\naccuracy curves of our pre-training method vs the end-to-end training method. As discussed be-\nfore, we ﬁt a linear regressor that predicts the location of the ground truth annotations by taking\nthe unsupervised landmark locations as input. The regressor is ﬁtted on a training dataset and\nthen used on a separate test dataset. The accuracy curves in ﬁgure 6 are obtained from the test\ndata predicted vs ground truth manual annotations of the three datasets BBCPose, Cat head, and\nMAFL.\n6\nConclusions\nOur study shows that the intermediate neural representations learned by standard end-to-end ap-\nproach leveraging equivariance losses enjoy poor equivariance properties. Instead, for unsuper-\nvised landmark discovery, we proposed to use contrastive learning for pre-training equivariant\nintermediate neural representations. The numerical experiments demonstrate that this simple\nstrategy, which can naturally be used within several other pipelines and tasks, can lead to a sig-\nniﬁcant performance boost even in challenging situations such as the one described in Section\n5.\n15\n0\n5\n10\n15\n20\n0%\n25%\n50%\n75%\n100%\nCat head distance vs accuracy\n0\n2\n4\n6\n8\n10\n0%\n25%\n50%\n75%\n100%\nMAFL distance vs accuracy\n0\n5\n10\n15\n20\n0%\n25%\n50%\n75%\n100%\nBBCPose distance vs accuracy\nOurs proposed\nEnd-to-end\nFigure 6: Accuracy curve of ﬁnal predicted landmarks that are obtained from linear regressor ﬁtted on top of un-\nsupervised landmarks. We compare the performance of our proposed pre-training vs. end-to-end training method.\nSimilar to previous accuracy curves, the x-axis denotes the distance d and y-axis denotes the corresponding accuracy\nAcc(d).\nReferences\nXudong Cao, Yichen Wei, Fang Wen, and Jian Sun. Face alignment by explicit shape regression.\nInternational Journal of Computer Vision, 107(2):177–190, 2014.\nOlivier Chapelle and Mingrui Wu. Gradient descent optimization of smoothed information re-\ntrieval metrics. Information retrieval, 13(3):216–235, 2010.\nJ. Charles, T. Pﬁster, M. Everingham, and A. Zisserman. Automatic and eﬃcient human pose\nestimation for sign language videos. International Journal of Computer Vision, 2013a.\nJ. Charles, T. Pﬁster, D. Magee, D. Hogg, and A. Zisserman. Domain adaptation for upper body\npose tracking in signed TV broadcasts. In British Machine Vision Conference, 2013b.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A Simple Framework for\nContrastive Learning of Visual Representations. 2020. URL http://arxiv.org/abs/\n2002.05709.\nXianjie Chen and Alan L. Yuille. Articulated pose estimation by a graphical model with image de-\npendent pairwise relations. CoRR, abs/1407.3399, 2014. URL http://arxiv.org/abs/\n1407.3399.\nTF Cootes, GJ Edwards, and CJ Taylor. Active appearance models. eccv’98: Proceedings of the\n5th european conference on computer vision-volume ii (pp. 484–498), 1998.\nDavid Cristinacce and Tim Cootes. Automatic feature localisation with constrained local models.\nPattern Recognition, 41(10):3054–3067, 2008.\nMatthias Dantone, Juergen Gall, Gabriele Fanelli, and Luc Van Gool. Real-time facial feature\ndetection using conditional regression forests. In 2012 IEEE Conference on Computer Vision and\nPattern Recognition, pages 2578–2585. IEEE, 2012.\n16\nTomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of object\nlandmarks through conditional image generation. Advances in Neural Information Processing\nSystems, (NeurIPS), 2018. ISSN 10495258.\nAngjoo Kanazawa, David W Jacobs, and Manmohan Chandraker. Warpnet: Weakly supervised\nmatching for single-view reconstruction. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3253–3261, 2016.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\nIn Proceedings of International Conference on Computer Vision (ICCV), December 2015.\nIain Matthews and Simon Baker. Active appearance models revisited. International journal of\ncomputer vision, 60(2):135–164, 2004.\nAiden Nibali, Zhen He, Stuart Morgan, and Luke Prendergast. Numerical Coordinate Regression\nwith Convolutional Neural Networks. 2018. URL http://arxiv.org/abs/1801.\n07372.\nMarco Pedersoli, Tinne Tuytelaars, and Luc Van Gool. Using a deformation ﬁeld model for local-\nizing faces and facial points under weak supervision. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 3694–3701, 2014.\nT. Pﬁster, K. Simonyan, J. Charles, and A. Zisserman. Deep convolutional neural networks for\neﬃcient pose estimation in gesture videos. In Asian Conference on Computer Vision, 2014.\nT. Pﬁster, J. Charles, and A. Zisserman. Flowing convnets for human pose estimation in videos.\nIn IEEE International Conference on Computer Vision, 2015.\nScott Reed, Aäron van den Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang,\nDan Belov, and Nando De Freitas. Parallel multiscale autoregressive density estimation. arXiv\npreprint arXiv:1703.03664, 2017.\nScott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee.\nLearning what and where to draw. In Advances in neural information processing systems, pages\n217–225, 2016.\nShaoqing Ren, Xudong Cao, Yichen Wei, and Jian Sun. Face alignment at 3000 fps via regressing\nlocal binary features. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1685–1692, 2014.\nIgnacio Rocco, Relja Arandjelovic, and Josef Sivic. Convolutional neural network architecture\nfor geometric matching. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 6148–6157, 2017.\nEnrique Sanchez and Georgios Tzimiropoulos. Object landmark discovery through unsupervised\nadaptation. CoRR, abs/1910.09469, 2019. URL http://arxiv.org/abs/1910.09469.\n17\nZhixin Shu, Mihir Sahasrabudhe, Rıza Alp Güler, Dimitris Samaras, Nikos Paragios, and Iasonas\nKokkinos. Deforming autoencoders: Unsupervised disentangling of shape and appearance.\nLecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and\nLecture Notes in Bioinformatics), 11214 LNCS:664–680, 2018. ISSN 16113349. doi: 10.1007/\n978-3-030-01249-6_40.\nYi Sun, Xiaogang Wang, and Xiaoou Tang. Deep convolutional network cascade for facial point\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n3476–3483, 2013.\nSupasorn Suwajanakorn, Noah Snavely, Jonathan Tompson, and Mohammad Norouzi. Discovery\nof latent 3d keypoints via end-to-end geometric reasoning. CoRR, abs/1807.03146, 2018. URL\nhttp://arxiv.org/abs/1807.03146.\nJames Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsupervised object learning from dense equiv-\nariant image labelling. CoRR, abs/1706.02932, 2017a. URL http://arxiv.org/abs/\n1706.02932.\nJames Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsupervised Learning of Object Landmarks\nby Factorized Spatial Embeddings. Proceedings of the IEEE International Conference on Computer\nVision, 2017-October:3229–3238, 2017b. ISSN 15505499. doi: 10.1109/ICCV.2017.348.\nJames Thewlis, Samuel Albanie, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of\nlandmarks by descriptor vector exchange. Proceedings of the IEEE International Conference on\nComputer Vision, 2019. ISSN 15505499. doi: 10.1109/ICCV.2019.00646.\nMichel Valstar, Brais Martinez, Xavier Binefa, and Maja Pantic.\nFacial point detection using\nboosted regression and graph models. In 2010 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, pages 2729–2736. IEEE, 2010.\nYue Wu, Chao Gou, and Qiang Ji. Simultaneous facial landmark detection, pose and deformation\nestimation under facial occlusion. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3471–3480, 2017.\nShengtao Xiao, Jiashi Feng, Junliang Xing, Hanjiang Lai, Shuicheng Yan, and Ashraf Kassim.\nRobust facial landmark detection via recurrent attentive-reﬁnement networks. In European\nconference on computer vision, pages 57–72. Springer, 2016.\nShengtao Xiao, Jiashi Feng, Luoqi Liu, Xuecheng Nie, Wei Wang, Shuicheng Yan, and Ashraf\nKassim. Recurrent 3d-2d dual learning for large-pose facial landmark detection. In Proceedings\nof the IEEE International Conference on Computer Vision, pages 1633–1642, 2017.\nY. Yang and D. Ramanan. Articulated pose estimation with ﬂexible mixtures-of-parts. In CVPR\n2011, pages 1385–1392, 2011. doi: 10.1109/CVPR.2011.5995741.\nXiang Yu, Feng Zhou, and Manmohan Chandraker. Deep deformation network for object land-\nmark localization. In European Conference on Computer Vision, pages 52–70. Springer, 2016.\n18\nJie Zhang, Shiguang Shan, Meina Kan, and Xilin Chen. Coarse-to-ﬁne auto-encoder networks\n(cfan) for real-time face alignment. In European conference on computer vision, pages 1–16.\nSpringer, 2014a.\nWeiwei Zhang, Jian Sun, and Xiaoou Tang. Cat head detection - how to eﬀectively exploit shape\nand texture features. volume 5305, pages 802–816, 10 2008. doi: 10.1007/978-3-540-88693-8_59.\nYuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He, and Honglak Lee. Unsupervised\nDiscovery of Object Landmarks as Structural Representations. Proceedings of the IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition, pages 2694–2703, 2018. ISSN\n10636919. doi: 10.1109/CVPR.2018.00285.\nZhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by\ndeep multi-task learning. In European conference on computer vision, pages 94–108. Springer,\n2014b.\nZhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Learning deep representation\nfor face alignment with auxiliary attributes. IEEE transactions on pattern analysis and machine\nintelligence, 38(5):918–930, 2015.\nXiangxin Zhu and Deva Ramanan. Face detection, pose estimation, and landmark localization in\nthe wild. In 2012 IEEE conference on computer vision and pattern recognition, pages 2879–2886.\nIEEE, 2012.\n19\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-04-07",
  "updated": "2021-04-07"
}