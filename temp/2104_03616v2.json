{
  "id": "http://arxiv.org/abs/2104.03616v2",
  "title": "Arena-Rosnav: Towards Deployment of Deep-Reinforcement-Learning-Based Obstacle Avoidance into Conventional Autonomous Navigation Systems",
  "authors": [
    "Linh Kästner",
    "Teham Buiyan",
    "Xinlin Zhao",
    "Lei Jiao",
    "Zhengcheng Shen",
    "Jens Lambrecht"
  ],
  "abstract": "Recently, mobile robots have become important tools in various industries,\nespecially in logistics. Deep reinforcement learning emerged as an alternative\nplanning method to replace overly conservative approaches and promises more\nefficient and flexible navigation. However, deep reinforcement learning\napproaches are not suitable for long-range navigation due to their proneness to\nlocal minima and lack of long term memory, which hinders its widespread\nintegration into industrial applications of mobile robotics. In this paper, we\npropose a navigation system incorporating deep-reinforcement-learning-based\nlocal planners into conventional navigation stacks for long-range navigation.\nTherefore, a framework for training and testing the deep reinforcement learning\nalgorithms along with classic approaches is presented. We evaluated our\ndeep-reinforcement-learning-enhanced navigation system against various\nconventional planners and found that our system outperforms them in terms of\nsafety, efficiency and robustness.",
  "text": "Arena-Rosnav: Towards Deployment of\nDeep-Reinforcement-Learning-Based Obstacle Avoidance into\nConventional Autonomous Navigation Systems\nLinh Kästner1, Teham Buiyan1, Lei Jiao1, Tuan Anh Le1, Xinlin Zhao1,\nZhengcheng Shen1 and Jens Lambrecht1\nAbstract—Recently, mobile robots have become important\ntools in various industries, especially in logistics. Deep rein-\nforcement learning emerged as an alternative planning method\nto replace overly conservative approaches and promises more\nefﬁcient and ﬂexible navigation. However, deep reinforcement\nlearning approaches are not suitable for long-range navigation\ndue to their proneness to local minima and lack of long term\nmemory, which hinders its widespread integration into indus-\ntrial applications of mobile robotics. In this paper, we propose\na navigation system incorporating deep-reinforcement-learning-\nbased local planners into conventional navigation stacks for\nlong-range navigation. Therefore, a framework for training\nand testing the deep reinforcement learning algorithms along\nwith classic approaches is presented. We evaluated our deep-\nreinforcement-learning-enhanced navigation system against var-\nious conventional planners and found that our system outper-\nforms them in terms of safety, efﬁciency and robustness.\nI. INTRODUCTION\nMobile robots have gained signiﬁcant importance due\nto their ﬂexibility and the variety of use cases they can\noperate in [1]. At the same time, the environments in which\nmobile robots operate have become increasingly complex,\nwith multiple static and dynamic obstacles like humans, fork\nlifts or robots. Reliable and safe navigation in these highly\ndynamic environments is essential in the operation of mobile\nrobotics [2]. Whereas classic planning approaches can cope\nwell with static environments, reliable obstacle avoidance in\ndynamic environments remains a big challenge. Typically,\ncurrent industrial approaches employ hand-engineered safety\nrestrictions and navigation measures [3], [4]. In particular,\nin environments that employ a variety of different actors,\nindustrial robots are often programmed to navigate overly\nconservatively, or to avoid such areas completely due to\nstringent restrictions. However, hand designing the navigation\nbehavior in dense environments is tedious and not always\nintuitive, due to not only the unpredictability of humans, but\nalso volatile behavior patterns of other actors.\nDeep Reinforcement Learning (DRL) emerged as an end-\nto-end learning approach, which directly maps raw sensor\noutputs to robot actions and has shown promising results\nin teaching complex behavior rules, increasing robustness\nto noise and generalizing new problem instances. Thus, a\n1Linh Kästner, Teham Buiyan, Lei Jiao, Tuan Anh Le, Xinlin Zhao,\nZhengcheng Shen and Jens Lambrecht are with the Chair Industry\nGrade Networks and Clouds, Faculty of Electrical Engineering, and\nComputer Science, Berlin Institute of Technology, Berlin, Germany\nlinhdoan@tu-berlin.de\nFig. 1: This work provides a platform to train and test learning-based\nobstacle avoidance approaches along with conventional global and\nlocal planners. This way, DRL-based obstacle avoidance approaches\nare made comparable against conventional approaches and feasible\nin industrial navigation systems\nvariety of literature incorporates DRL into robot navigation\n[5]-[6]. However, a main bottleneck is its limitation for local\nnavigation, due to a lack a long term memory and its myopic\nnature [7]. Efforts to integrate recurrent networks to mitigate\nthis issue result in tedious training and limited payoff. In\nthis paper, we address current limitations of integration of\nDRL into industrial robotic navigation systems for long-range\nnavigation in highly dynamic environments. We propose a\nnavigation framework, which is fully integrated into the robot\noperating system (ROS) and enables the deployment of DRL-\nbased planning as part of existing navigation stacks. As\nan interconnection entity, an intermediate way-point planner\nis provided, which connects DRL with conventional global\nplanning algorithms like RRT, A∗or Dikstra. Furthermore,\nthe platform serves as a both training and testing playground\nand provides a 2D simulator in which training a DRL-based\nlocal planner using robotic interfaces is made feasible. The\nmain contributions of this work are the following:\n• Proposal of a framework to train and integrate learning-\nbased approaches into conventional robot navigation\nstacks.\n• Proposal of an intermediate way-point generator for\ninterconnection between DRL and conventional global\nplanning modules.\n• Integration of state-of-the-art obstacle avoidance ap-\narXiv:2104.03616v2  [cs.RO]  23 Sep 2021\nproaches into the robot operating system (ROS) for\nextensive evaluation of navigation systems.\nThe paper is structured as follows. Sec. II begins with related\nworks followed by the methodology in Sec. III. Subsequently,\nthe results and evaluations are presented in Sec IV. Finally,\nSec. V provides a conclusion and outlook. We made our code\nopen-source at https://github.com/ignc-research/arena-rosnav.\nII. RELATED WORKS\nAutonomous navigation of mobile robots has been exten-\nsively studied in various research publications. While current\nrobot navigation planners work well in static environments or\nwith known obstacle positions, highly dynamic environments\nstill pose an open challenge. Current mobile navigation ap-\nproaches often employ hand-engineered measures and rules\n[8], [9], safety thresholds [2], dynamic zones contemplating\nto social conventions [10] or the complete avoidance of areas\nknown to be problematic [4]. These approaches are reaching\ntheir limits when the exact models are unknown or become\ncomplex for convoluted scenarios, which in term could lead\nto longer paths, waiting times or complete failure. Further-\nmore, in highly dynamic environments employing numerous\nbehavior classes, these hand-engineered rules could become\ntedious and computationally intensive. DRL has emerged as\nan end-to-end approach with potential to learn navigation\nin dynamic environments. Thus, various publications utilize\nDRL for path planning and obstacle avoidance [5], [11],\n[12], [13]. Shi et al. [13] proposed a DRL-based local\nplanner utilizing the Asynchronous Advantage Actor Critic\n(A3C) approach and achieved end-to-end navigation behavior\npurely from sensor observations. The researchers transfer the\napproach towards a real robot and demonstrate its feasibility.\nWorks from Sun et al. [14], Truong et al. [15] and Ferrer et\nal. [16] incorporate a human motion prediction algorithm into\nthe navigation to enhance environmental understanding and\nadjust the navigation for crowded environments. A socially\naware DRL-based planner (CADRL) was proposed by Chen\net al. [6]. The researchers introduce reward functions and\naim to teach the robot social norms like keeping to the right\nside of the corridor. Everett et al. [17] extend CADRL to\ninclude the handling of a dynamic amount of persons using an\nLong-Term-Short-Memory module and modify the approach\nto solve human randomness. Chen et al. [18] propose an\nobstacle avoidance approach using DRL by modeling human\nbehavior. Despite the success of DRL for local navigation\nand obstacle avoidance, integration of these algorithms into\nconventional robotic navigation systems is still an open\nfrontier, since most obstacle avoidance (OA) algorithms are\npurely developed as a numerical optimization problem. On\nthat account, our proposed framework aims to provide a\nplatform to employ learning-based OA approaches directly\nfor robot navigation. Similar to our work, Dugas et al. [7]\npropose a platform consisting of a 2D simulation environment\nto train and test different DRL algorithms. The researchers\nintegrate a variety of different DRL-based navigation ap-\nproaches into their platform and compare them against one\nanother. The work focused on the general navigation aspect\nof DRL-based planners and only tested on one scenario with\ndynamic obstacles. Our work extends this by incorporating\nthe global planner for long range navigation and speciﬁcally\ndeploy the planners on multiple challenging, highly dynamic\nenvironments. Most similar to our work, Gundelring et al.\n[19] ﬁrst combine a DRL-based local planner with a conven-\ntional global planner and demonstrate promising results. A\nlimitation of that work is that the researchers only employ\nsimple sub-sampling of the global path and no replanning\ncapability which leads to hindrance when there are multiple\nhumans blocking the way. In this work, we follow a similar\napproach but introduce an intermediate planner to spawn\nway-points more intelligent and add replanning functionality\nbased on spatial and time horizons.\nIII. METHODOLOGY\nIn this chapter we will present the methodology of our\nproposed framework. Since DRL-based methods are myopic\nand not suitable for long range navigation tasks [7], our work\nproposes a combination with traditional global planners to\nmake DRL feasible for industrial applications. The system\ndesign is described in Fig. 1. Part of our contribution is\nan interconnection way-point generator which connects the\ntwo planners. The platform serves as both a training and\ntesting environment and is integrated into the ubiquitously\nused ROS. In the following, each sub-module is described in\ndetail.\nA. Intermediate Planner - Waypoint Calculation\nIn order to accomplish long range navigation while utiliz-\ning a DRL-based local planner, a hierarchical path planning\nis implemented consisting of an intermediate planner as an\ninterconnection between the traditional global planner and\nthe DRL-based local planner. A∗search is used as a global\nplanner to ﬁnd a near optimal global path. Instead of a simple\nsub-sampling as proposed in Gundelring et al. [19], which\nis not ﬂexible for stuck situations, we introduce a spatial\nhorizon dependent on the robots position. The algorithm is\ndescribed in Algorithm 1.\nFig. 2: Subgoal selection. Given a global path, our intermediate\nplanner, will calculate a subgoal based on the spatial horizon\ndistahead.\nLet the global path πg be an array Y consisting of Nt\nposes from which we sample a subset Y subgoalsi ⊂X poses =\n{xi,...,xN} of local goals based on the robot’s position pr\nand a look-ahead distance dahead. Provided the global path\nAlgorithm 1: Subgoal calculation\nInput: Global path πg, Robot position pr\nOutput: Subgoal gsub\nParameter: dahead;\nFind the set of intersection points of R(pr,dahead) and\nglobal path πg as Φ;\nif Φ is not empty then\nselect the intersection point near to the global\ngoal as gsub\nelse\ncall GLOBAL REPLAN →new global path πnew\ng\n;\nSubgoal calculation(πnew\ng\n, pr);\nend\nand the current robot’s position, a sub-goal within the range\nof look-ahead distance is selected as the local goal for the\nDRL local planner. Thus, the subgoal is dynamic and moves\nwith the robot’s position. This makes the approach more\nﬂexible in situations with multiple unknown obstacles. Fig. 2\nillustrates our approach. The subset of goals can be observed\nas local sub-goals xi = pg and be given as input into the DRL\nagent. Additionally, we integrate a replanning mechanism to\nrework the global path when the robot is off-course for a\ncertain distance from the global path or if a time limit tlim\nis reached without movement, e.g. when the robot is stuck.\nIn that case, a global replanning will be triggered and a new\nsub-goal will be calculated based on the new global plan and\ncurrent robot position.\nB. Deep-Reinforcement-Learning-based Local Navigation\nWe formulate the robot navigation problem as a par-\ntially observable Markov decision problem (POMDP) that\nis formally describable by a 6-tuple (S,A,P,O,R,γ) where\nS are the robot states, A the action space, P the transition\nprobability from one state to another, R the reward, O the\nobservations and γ the discount factor which we keep at 0.9.\nNext, we formulate following task constraints for a collision\nfree navigation towards the goal:\nCj,i,t(s,a) =\n\n\n\n\n\n|pr −pg|2 < dgoal\ngoal constraint\nmax(Os,t) > dr\ncollision constraint\nargmin[t|s,π]\ntime constraint\n(1)\nWhere dgoal is the goals radius, pr is the robot’s position,\npg is the goal’s position , O is the robot’s observation and\ndr is the robot’s radius. To solve the POMDP, we employ\npolicy-based model-free actor critic reinforcement learning\nto learn a policy which maximizes a cumulative reward\ndependent on the state action pair. It does so by calculating\nthe policy gradients, which optimize both a value (critic) and\npolicy function (actor). The value function is calculated by\naccumulating the expected rewards at each time-step given\nan action performed while following policy π\nV ∗(s0) = argmaxE\n\"\nT\n∑\nt=0\nγR(st,π∗(st))\n#\n(2)\nSubsequently, the Bellman equation can be iterated over the\nvalue to calculate the optimal policy function π(st+1):\nπ∗(st+1) = argmaxR(st,a)+\n(3)\nZ\nst+1\nP(st,st+1|a)·V ∗(st+1)dst+1\n(4)\n1) Training Algorithm: To train our DRL agent, we utilize\nan A3C policy gradient method whose output is split into\na policy (actor) and a value (critic) function. Contrary to\nmost Deep-Q-Learning algorithms, the approach is an on-\npolicy training method in which the agent has to be trained\non the current status of the network. To avoid sample\ninefﬁciency, actor critic approaches employ asynchronous\ntraining of multiple actors simultaneously, each with its own\nenvironment N. The knowledge gained from all actors is\nthen concatenated into the ﬁnal output. For an agent with\na state st and an action at at a time step t, the policy\noutput µ(si,t), is a probability distribution of all actions,\nwhereas the advantage output Vθ(si) assesses the advantage\nof executing the actions proposed by the policy. For our\ncase, we forward the observations of N agents simultaneously\nas depicted in Fig. 3. The current observations are passed\ninto the actor-critic network, which returns the state value\nVθ(si) and the policy πθ upon which the simulation step is\nexecuted. The obtained reward ri is saved into the buffer R.\nThese values are used to calculate the value loss Lvalue and\nthe policy loss Lpolicy with a mean squared error. Softmax\nis applied to obtain the policy values into probabilities of\neach action. Finally, the policy-gradient (µ(si,t)) and value-\ngradient (Vθ(si)) are calculated using the respective losses.\nThe network parameters are updated accordingly. We use an\nAdam optimizer with a learning rate of 0.0001 and an ε of\n0.003. The discount factor is set to 0.9. The training algorithm\nis described in Algorithm 2.\n2) Reward System: We formulate our reward system as\nfollowing. First, we deﬁne pg as the goal position and\npr being the agent’s position. The total reward R(st,a) is\ncalculated as the sum of all sub-rewards.\nR(st,a) = [rt\ns +rt\nc +rt\nd +rt\np +rt\nm]\n(5)\nrt\ns =\n(\n15\n|pr −pg|2 < dg\n0\notherwise\nrt\nm =\n(\n0\notherwise\n−0.01\n∆r = 0\nrt\nd =\n(\n0\notherwise\n−0.15\ndr < dsafe\nrt\nc =\n(\n0\notherwise\n−10\nmax(Os,t) < dr\nrt\np =\n(\nwp ∗dt\ndt >= 0, dt = dt−1\nag −dt\nag\nwn ∗dt\nelse\nAlgorithm 2: Training Algorithm of A3C\nL ←new Array(Bsize);\nπθ,Vθ(si) ←net(observations);\nfor i = t −1...tstart do\nfor N do\nif episode ended then\nR = 0\nelse\nR = Vθ(si)\nend\nR ←rewards+γR Lvalue ←(R−Vθ(si))2\nLpolicy ←mse(softmax(πθ(si))(R−Vθ(si)))\n∂\n∂v ←∂\n∂v + ∂Lvalue(θv)\n∂θv\n∂\n∂θπ ←\n∂\n∂θπ +∇θlogπθ(ai|si)Lpolicy\nend\nend\nFig. 3: Neural network architecture. We utilize an on policy, actor\ncritic agent with a gated recurrent unit. As input we forward the\ndown-sampled LIDAR scan observations into the network. For both,\nactor and critic network, we use the same head.\nHere, rs is the success reward, rd is the distance reward, rp is\nthe progress reward, rm is the move reward, rd is the danger\nreward to keep the agent from a safety distance dsafe and rc\nis the collision reward. We set wp = 0.25,wn = 0.4. If the\nrobot moves away from the dynamic obstacle, it receives an\nadditional reward.\n3) Neural Network Architecture and Agent Design: Our\nneural network architecture is illustrated in Fig. 3. It consists\nof two networks, one for the value and one for the policy\nfunction. The input to our network is the 360 degree LIDAR\nscan, which we down-sampled to 344 values as well as the\ngoal position and angle with respect to the robot. After two\nfully connected layers, we add a gated recurrent unit (GRU)\nas memory module to also consider past observations that are\nstored in a buffer. Subsequently, we train on a continuous\naction state for more ﬂexibility and smoothness of actions\n[11]. We formalize the action space A as following:\na = {vlin,vang}\n(6)\nvlin ∈[0,0.3]m/s,\nvang ∈[−2.7,2.7]m/s\n(7)\n4) Training Setup: The agent is trained on randomized\nenvironments which are depicted in Fig. 1. Walls and static\nobstacles are spawned randomly after each episode. In the\nsame manner, dynamic obstacles are spawned at random\npositions and move randomly. This way, we mitigate over-\nﬁtting issues and enhance generalization. Application inter-\nfaces to include more complex obstacle models and shapes\nare provided. Curriculum training is adapted which spawns\nmore obstacles once a success threshold is reached and less\nobstacles if the agents success rate is low. the threshold\nis deﬁned as the sum over the mean reward. The training\nwas optimized with GPU usage and trained on a NVIDIA\nRTX 3090 GPU. Training time took between 8h and 20h to\nconverge. The hyperparamters are listed in Table II in the\nAppendix.\n5) Integration of Obstacle Avoidance and Alternative Ap-\nproaches: Our framework provides interfaces to integrate\nobstacle avoidance methods into our navigation system. For\ncomparison of our DRL-based local planner, we integrate a\nvariety of optimization-based obstacle avoidance algorithms\ninto ROS. This way, even purely numerical-optimization-\nbased obstacle avoidance algorithms are made feasible for\ndeployment on robotic navigation systems. Particularly, we\nincluded Timed Elastic Bands (TEB), Dynamic Windows\nApproach (DWA), Model Predictive Control (MPC), as well\nas learning-based obstacle avoidance methods CADRL and\nCrowdNav. In the next chapter we will use these approaches\nas baseline for our DRL-based local planner.\nIV. RESULTS AND EVALUATION\nIn the following chapter, we will present our experiments,\nthe results and evaluations. This includes an extensive com-\nparison with 3 state of the art model-based local planners\nTEB [20], DWA [21] and MPC [22] as well as a recent\nobstacle avoidance approache based on DRL: CADRL [17],\nwith our proposed DRL local planner which we denote as\nARENA.\nA. Qualitative evaluation\nWe conducted experiments on two different maps in which\nwe created a total of seven different test scenarios of increas-\ning difﬁculty. In each scenario, the obstacle velocities are\nset to 0.1 m/s. More extensive evaluations on the impact of\nobstacle velocities are presented in the quantitative evalua-\ntions. As a global planner, A∗is used for all approaches. For\nthe integrated OA approaches and the DRL-based planners,\nthe global path is sampled by the intermediate way-point\ngenerator using the time and location horizon of tlim = 4s and\ndahead = 1.55m. For the model-based approaches, we utilize\nthe ROS navigation stack interface. Localization is assumed\nto be perfectly known. For each planner, we conduct 100\ntest runs on each scenario. The qualitative trajectories of all\nplanners on each scenario are illustrated in Fig. 4 (a)-(f). The\nintensity of the trajectories indicates their frequency.\n1) Robot Trajectories: For scenarios that include more\nthan ﬁve obstacles, DWA and TEB struggle due to the\nslow reaction time of the local planners. Particularly, the\ntrajectories of the TEB planner (yellow) show stops and\nbackwards movements once the TEB planner perceives an\nobstacle or at direction changes and moves backwards which\nis inefﬁcient and sometimes too slow, especially if there\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 4: Trajectories of all planners on different test scenarios with obstacle velocity vobs = 0.3m/s. Upper row: ofﬁce map, lower row:\nplain test area. (a) 5 dynamic obstacles approaching robot from side, (b) 10 dynamic obstacles intersecting horizontally and blocking\ncorridor, (c) 20 dynamic obstacles intersecting horizontally, (d) 5 obstacles randomly intersecting the robots path on an empty map, (e)\n10 obstacles intersecting horizontally, (f) 20 obstacles intersecting horizontally.\nare multiple obstacles approaching from different directions.\nNevertheless, when the obstacles are not directly confronting\nthe agent, these approaches can attain robust and efﬁcient\npath planning which follows the global planner consistently\nas can be observed by the trajectories being consistent for\nmost test runs. Notable is the high performance of MPC in\nthe ofﬁce map where it accomplishes the best results in terms\nof obstacle avoidance and path robustness out of the model-\nbased approaches. The trajectories of the MPC planners are\nconsistent and do not include as many outliers. However,\nin the empty map, more outliers are observed in situations\nwhere the obstacle is directly approaching the agent (Fig.\n4(d)), whereas the TEB planner performs slightly better with\nless outliers. Our Arena planner as well as CADRL manages\nto keep a direct path towards the goal and still accomplishes\nless collisions compared to the model-based planners. The\ncollisions are visualized as circles with varying intensity\nbased on how frequently collisions occurred. CADRL avoids\ndynamic obstacles in a larger circular movement compared\nwith our ARENA planner which leads to longer trajectories\n(red). It is observed that our planner reacts to approaching\nobstacles already from a far distance while trying to follow\nthe global path once the obstacle is avoided to ensure\nefﬁciency. Contrarily, conventional planners react slowly to\napproaching obstacles and rely a more frequent replanning\nof the global path. Particularly, the TEB planner employs an\ninefﬁcient stop and backward motion and a large amount of\ninefﬁcient roundabout paths (Fig. 4 (a),(b),(f)). Our proposed\nARENA planner accomplishes a robust path to the goal while\naccomplishing a reliable collision avoidance. Additionally,\nour intermediate planner replans more efﬁciently in highly\ndynamic environments by considering the introduced spatial\nand time horizons which triggers a replanning only when\nthe robot becomes stuck for more than four seconds. In\ngeneral, it is concluded that the model-based approaches\nfollow the global path more consistently while learning-based\napproaches make decisions earlier thus more often got off-\ntrack when avoiding the obstacles.\nB. Quantitative Evaluations\nTable I lists the quantitative evaluations of our experiments\nfor all agents. In total, we conducted 100 test runs for each\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 5: Results from test-runs on the ofﬁce map. (a) Collisions over number of obstacles and vobs, (b) average time to reach goal over\nnumber of obstacles and vobs, (c) Average path length over number of obstacles and vobs. With vobs = (0.1,0.2,0.3)m/s, while a high\nopacity indicates a lower velocity.(d) Collisions over obstacle velocities, (e) average time to reach goal over obstacle velocities (lower is\nbetter), (f) Relative performance to ARENA planner.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 6: Results from test-runs on the empty map. (a) Collisions over number of obstacles and vobs, (b) Average time to reach goal over\nnumber of obstacles and vobs, (c) Average path length over number of obstacles and vobs. With vobs = (0.1,0.2,0.3)m/s, while a high\nopacity indicates a lower velocity. (d) Collisions over obstacle velocities, (e) Average time to reach goal over obstacle velocities (lower\nis better), (f) Relative performance to ARENA planner.\nplanner and each map. We evaluate the efﬁciency of path\nplanning by calculating the average distance traveled, the\naverage time to reach the goal and the collision rates with\nall objects. In addition, we deﬁne the success rate of all runs\nwhen the goal is reached with less than two collisions. We\nset the timeout to 3 minutes. To evaluate the capability of all\nplanners to cope within highly dynamic environments, we\ntest all planners on scenarios with 5, 10 and 20 dynamic\nobstacles, and three different obstacle velocities (0.1,0.3 and\n0.5 m/s). Subsequently, the success rate, collision rate, path\nlengths and time to reach goal over the number of obstacles\nare plotted in Fig. 5 and 6.\n1) Safety and Robustness: As expected, the learning-based\nplanners CADRL as well as our DRL agent can cope with\na higher number of dynamic obstacles and still manages to\nkeep a success rate of nearly 94 % with 10 obstacles whereas\nTABLE I: Quantitative evaluations\nTime [s]\nPath [m] Collisions Success\nTime [s]\nPath [m] Collisions Success\nTime [s]\nPath [m] Collisions Success\nvobs 0.1m/s\n5 dyn. Obstacles\n10 dyn. Obstacles\n20 dyn. Obstacles\nCADRL\n158.69\n31.07\n0\n100\n151.21\n29.97\n1\n100\n163.5\n31.30\n16\n95\nARENA (Ours)\n150.24\n29.13\n2\n100\n153.25\n29.76\n5.5\n100\n159.65\n30.96\n10.5\n96.5\nDWA\n185.1\n29.85\n15\n95.5\n187.6\n34.15\n53.5\n83.5\n195.60\n30.07\n83.5\n77\nMPC\n134.45\n26.95\n0\n100\n138.16\n26.28\n0\n100\n149.25\n29.33\n8.5\n97.5\nTEB\n171.95\n29.28\n0\n100\n171.83\n28.69\n2.5\n100\n183.86\n29.75\n5.5\n99\nvobs 0.2m/s\nCADRL\n150.46\n29.70\n11\n97.5\n184.03\n36.51\n44\n90\n181.14\n34.71\n53\n86.5\nARENA (Ours)\n151.2\n29.255\n3\n100\n163.25\n31.95\n18\n94\n161.60\n31.45\n30.5\n92.5\nDWA\n202.36\n30.16\n48\n85\n189\n34.68\n65\n80.5\n205.25\n31\n120\n58\nMPC\n147.97\n28.31\n2\n100\n144.75\n27.18\n28.5\n91\n169.28\n29.6\n20\n94\nTEB\n179.35\n29.44\n4.5\n98\n176.62\n29.2\n16\n93\n192.76\n30.62\n30\n90.5\nvobs 0.3m/s\nCADRL\n153.3835 29.9625\n43.5\n86.5\n185.96\n37.168\n73.5\n75\n206.195\n40.015\n130\n62\nARENA (Ours)\n152.2\n30.495\n15.5\n94.5\n165.5\n32.1\n29\n93\n170.275\n33.875\n65\n81\nDWA\n215.62\n31.195\n55.5\n82\n188.5\n34.65\n75.5\n76.5\n204.975\n31.985\n120\n56\nMPC\n149.05\n29.95\n24.5\n92\n169.8\n27.9\n40\n88\n178.745\n30.475\n91.5\n70.5\nTEB\n185.8\n28.965\n23.5\n91\n212.54\n33.43\n44\n85.5\n221.215\n31.65\n96\n68\nOverall Average\nEmpty Map\nOfﬁce Map\nCADRL\n170.511\n33.381\n41.33\n88.05\n181.34\n33.38\n49.1\n85.8\n159.68\n31.03\n33.5\n90.2\nARENA (Ours)\n158.57\n30.99\n19.88\n94.61\n166.96\n30.9\n23.7\n92.7\n150.18\n29.55\n16\n96.4\nDWA\n197.11\n31.972\n70.66\n77.1\n210.78\n31.97\n65.6\n79.1\n183.43\n32.48\n75.6\n75.1\nMPC\n153.49\n28.443\n23.8\n92.5\n162.7\n28.4\n26.7\n91.3\n144.29\n27.38\n21\n93.7\nTEB\n188.43\n30.115\n24.88\n91.83\n202.75\n30.11\n18.6\n94.1\n174.12\n29.57\n31.1\n89.5\nthe well-performing MPC planner and the DWA planner only\nattain 91% and 80.5% respectively for vobs = 0.2m/s. The gap\nis even more evident when increasing the obstacle velocities\nas can be observed in Fig. 5 and 6 and Table I. For the\ntest runs with a dynamic obstacle velocity of 0.3 m/s, our\nplanner outperforms all planners in terms of collision rate\nand success. In the most difﬁcult scenario with 20 obstacles,\nour planner still manages a 81 % success rate compared\nto 70.5 and 68 % for MPC and DWA respectively while\nalmost half of DWA’s runs failed (56 % success). Out of the\nconventional approaches, TEB and MPC achieve best overall\nresults with approximately 92 % success rate. MPC achieves\nthe best results among those 3 conventional approaches with\na success rate of over 88 and 70.5 % for 10 and 20 dynamic\nobstacles of 0.3 m/s respectively. The superior performance\nby our DRL-based planners in highly dynamic environments\nin terms of success rate and safety is clearly observed. It\nattains a success rate of over 81 % even with 20 obstacles,\nwhereas these scenarios cause a rapid performance drop for\nthe model-based approaches. CADRL accomplishes similar\nperformance in highly crowded environments but fail when\nobstacle velocities are increased. Whereas it attains 95 % and\n86.5 % success in environments with vobs is 0.1 and 0.2 m/s\nrespectively, success drops to 62 % when vobs = 0.3 m/s.\nThis is due to the fact that CADRL was trained on a speciﬁc\nobstacle velocity of 0.2 m/s, which makes it inﬂexible for\nhighly dynamic situations. Whereas conventional planners\ncope well in scenarios with slow moving obstacles, their\nperformance declines in environments with vobs < 0.2. With\nincreasing number and velocity of obstacles, the superiority\nof our ARENA planner in terms of success rate and safety\nbecomes more evident.\n2) Efﬁciency: In terms of efﬁciency, MPC is the most\nefﬁcient planner, taking less time and shorter trajectories\ncompared to the other approaches. As stated in Table I, MPC\nrequire an average of 153.49 s with an average path length\n28.44 meters, whereas CADRL and our ARENA planner\nrequire 170.5 and 158.57 s while the average path length\nis 33.38 and 30.99 meters respectively. However, similar\nto the success and collision rates, a decline in efﬁciency\nfor all model-based planners can be noticed for scenarios\nwith increased obstacle numbers and velocities. Our ARENA\nplanner, outperforms all planners in terms of efﬁciency in\nscenarios with 10 and 20 obstacles and a vos of 0.2 and 0.3\nm/s.\n3) Overall Performance: To compare our approach against\nall planners, we calculate the average performance for each\nmetric (efﬁciency, robustness and safety) by dividing the\nrespective overall values of the planners with the values\nfrom our DRL-based planner. The results are percentages\nindicating the relative performance to our ARENA planner.\nSubsequently, we added all percentages to attain the overall\nperformance metric. The results over vobs are plotted in Fig.\n5 (f) and 6 (f). Whereas, for vobs = 0.1m/s, MPC and TEB\nachieve a higher performance, our planner outperforms all\nplanners for higher obstacle velocities. Notable is the high\nperformance of the MPC planner for vobs at 0.1 m/s and\n0.2 m/s in the ofﬁce map but the rapid decline for vobs at\n0.3. On the empty map this decline already occurs at vobs at\n0.2 m/s. This is due to the fact that model-based planners\nperform worse on empty map due to their high dependence\non the global planner and a global map with static obstacle as\nmarker points. CADRL achieves similar performance to our\nARENA planner. However, one main limitation of CADRL\nis that it requires the exact positions of obstacles and can\nonly cope with circular obstacles. In contrast, our planner\nwas trained solely on laser scan observations and thus can\naccommodate any kind of obstacles without prior knowledge\nof its exact position. This makes our planner more ﬂexible\nfor real world application.\nV. CONCLUSION\nIn this paper, we proposed a framework to integrate\nDRL-based local planners into long range navigation. Our\nplatform can be used to directly train and deploy DRL-based\nalgorithms as local planners combined with classical global\nplanners. Our proposed intermediate planner will generate\nway-points and has shown success in being more efﬁcient\nat replanning than the existing ROS navigation stack and\nproviding reasonable way-points when the robot is stuck.\nWe integrate various conventional model-based as well as\nlearning-based obstacle avoidance algorithms into our system\nand propose a DRL-based local planner using a memory-\nenhanced A3C. Subsequently, we evaluate all planners in\nterms of safety, efﬁciency and robustness. Results demon-\nstrate the superiority of learning-based planners compared\nto conventional model-based ones on all metrics. Future\nwork includes the integration of semantics into the DRL\nfor behavior adaption and more sophisticated algorithms for\nour intermediate planner. Therefore, we aspire to extend\nour approach by including the intermediate planner into the\nreinforcement learning training process.\nVI. APPENDIX\nTABLE II: Hyperparameters for Training\nHyperparameter\nValue\nExplanation\nMean Success Bound\n1\nTraining considered done if mean\nsuccess rate reaches this value\nDiscount Factor\n0.9\nDiscount factor for reward estima-\ntion (often denoted by gamma)\nLearning Rate\n0.00025 Learning rate for optimizer\nEpsilon Max Steps\n105\nSteps until epsilon reaches mini-\nmum\nEpsilon End\n0.05\nMinimum epsilon value\nBatch Size\n64\nBatch size for training after every\nstep\nMemory Size\n1064\nLast X states will be stored in a\nbuffer (memory), from which the\nbatches are sampled\nTrade-off-factor λ\n0.95\nTrade-off-factor between bias and\nvariance\nClip-range\n0.2\nClipping range for the value func-\ntion to mitigate gradient exploding\nMaximum Gradient\n0.5\nMaximum value to clip the gradient\nREFERENCES\n[1] M. B. Alatise and G. P. Hancke, “A review on challenges of au-\ntonomous mobile robot and sensor fusion methods,” IEEE Access,\nvol. 8, pp. 39 830–39 846, 2020.\n[2] S. Robla-Gómez, V. M. Becerra, J. R. Llata, E. Gonzalez-Sarabia,\nC. Torre-Ferrero, and J. Perez-Oria, “Working together: A review\non safe human-robot collaboration in industrial environments,” IEEE\nAccess, vol. 5, pp. 26 754–26 773, 2017.\n[3] E. A. Sisbot, R. Alami, T. Siméon, K. Dautenhahn, M. Walters, and\nS. Woods, “Navigation in the presence of humans,” in 5th IEEE-RAS\nInternational Conference on Humanoid Robots, 2005.\nIEEE, 2005,\npp. 181–188.\n[4] K. Qian, X. Ma, X. Dai, and F. Fang, “Socially acceptable pre-collision\nsafety strategies for human-compliant navigation of service robots,”\nAdvanced Robotics, vol. 24, no. 13, pp. 1813–1840, 2010.\n[5] H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis, “Learning naviga-\ntion behaviors end-to-end with autorl,” IEEE Robotics and Automation\nLetters, vol. 4, no. 2, pp. 2007–2014, 2019.\n[6] Y. F. Chen, M. Everett, M. Liu, and J. P. How, “Socially aware\nmotion planning with deep reinforcement learning,” in 2017 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2017, pp. 1343–1350.\n[7] D. Dugas, J. Nieto, R. Siegwart, and J. J. Chung, “Navrep: Unsuper-\nvised representations for reinforcement learning of robot navigation\nin dynamic human environments,” arXiv preprint arXiv:2012.04406,\n2020.\n[8] C.-P. Lam, C.-T. Chou, K.-H. Chiang, and L.-C. Fu, “Human-centered\nrobot navigation—towards a harmoniously human–robot coexisting\nenvironment,” IEEE Transactions on Robotics, vol. 27, no. 1, pp. 99–\n112, 2010.\n[9] J. Guzzi, A. Giusti, L. M. Gambardella, G. Theraulaz, and G. A.\nDi Caro, “Human-friendly robot navigation in dynamic environments,”\nin 2013 IEEE International Conference on Robotics and Automation.\nIEEE, 2013, pp. 423–430.\n[10] X.-T. Truong and T.-D. Ngo, “Dynamic social zone based mobile\nrobot navigation for human comfortable safety in social environments,”\nInternational Journal of Social Robotics, vol. 8, no. 5, pp. 663–684,\n2016.\n[11] A. Faust, K. Oslund, O. Ramirez, A. Francis, L. Tapia, M. Fiser,\nand J. Davidson, “Prm-rl: Long-range robotic navigation tasks by\ncombining reinforcement learning and sampling-based planning,” in\n2018 IEEE International Conference on Robotics and Automation\n(ICRA).\nIEEE, 2018, pp. 5113–5120.\n[12] A. Francis, A. Faust, H.-T. Chiang, J. Hsu, J. C. Kew, M. Fiser,\nand T.-W. E. Lee, “Long-range indoor navigation with prm-rl,” IEEE\nTransactions on Robotics, 2020.\n[13] H. Shi, L. Shi, M. Xu, and K.-S. Hwang, “End-to-end navigation\nstrategy with deep reinforcement learning for mobile robots,” IEEE\nTransactions on Industrial Informatics, vol. 16, no. 4, pp. 2393–2402,\n2019.\n[14] S. Sun, X. Zhao, Q. Li, and M. Tan, “Inverse reinforcement learning-\nbased time-dependent a* planner for human-aware robot navigation\nwith local vision,” Advanced Robotics, pp. 1–14, 2020.\n[15] X.-T. Truong and T. D. Ngo, “Toward socially aware robot navigation\nin dynamic and crowded environments: A proactive social motion\nmodel,” IEEE Transactions on Automation Science and Engineering,\nvol. 14, no. 4, pp. 1743–1760, 2017.\n[16] G. Ferrer and A. Sanfeliu, “Anticipative kinodynamic planning: multi-\nobjective robot navigation in urban and dynamic environments,” Au-\ntonomous Robots, vol. 43, no. 6, pp. 1473–1488, 2019.\n[17] M. Everett, Y. F. Chen, and J. P. How, “Motion planning among\ndynamic, decision-making agents with deep reinforcement learning,”\nin 2018 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2018, pp. 3052–3059.\n[18] C. Chen, Y. Liu, S. Kreiss, and A. Alahi, “Crowd-robot interaction:\nCrowd-aware robot navigation with attention-based deep reinforcement\nlearning,” in 2019 International Conference on Robotics and Automa-\ntion (ICRA).\nIEEE, 2019, pp. 6015–6022.\n[19] R. Güldenring, M. Görner, N. Hendrich, N. J. Jacobsen, and J. Zhang,\n“Learning local planners for human-aware navigation in indoor envi-\nronments.”\n[20] C. Rösmann, F. Hoffmann, and T. Bertram, “Timed-elastic-bands for\ntime-optimal point-to-point nonlinear model predictive control,” in\n2015 european control conference (ECC).\nIEEE, 2015, pp. 3352–\n3357.\n[21] V. Kurtz and H. Lin, “Toward veriﬁable real-time obstacle motion\nprediction for dynamic collision avoidance,” in 2019 American Control\nConference (ACC).\nIEEE, 2019, pp. 2633–2638.\n[22] C. Rösmann, “Time-optimal nonlinear model predictive control,” Ph.D.\ndissertation, Dissertation, Technische Universität Dortmund, 2019.\n",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "published": "2021-04-08",
  "updated": "2021-09-23"
}