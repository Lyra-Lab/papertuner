{
  "id": "http://arxiv.org/abs/2104.04893v1",
  "title": "The Atari Data Scraper",
  "authors": [
    "Brittany Davis Pierson",
    "Justine Ventura",
    "Matthew E. Taylor"
  ],
  "abstract": "Reinforcement learning has made great strides in recent years due to the\nsuccess of methods using deep neural networks. However, such neural networks\nact as a black box, obscuring the inner workings. While reinforcement learning\nhas the potential to solve unique problems, a lack of trust and understanding\nof reinforcement learning algorithms could prevent their widespread adoption.\nHere, we present a library that attaches a \"data scraper\" to deep reinforcement\nlearning agents, acting as an observer, and then show how the data collected by\nthe Atari Data Scraper can be used to understand and interpret deep\nreinforcement learning agents. The code for the Atari Data Scraper can be found\nhere: https://github.com/IRLL/Atari-Data-Scraper",
  "text": "The Atari Data Scraper\nBrittany Davis Pierson*\nWashington State University\nbrittany.f.davis@wsu.edu\nJustine Ventura\nUniversity of Alberta\nlinnrose@ualberta.ca\nMatthew E. Taylor\nUniversity of Alberta\nmatthew.e.taylor@ualberta.ca\nAbstract\nReinforcement learning has made great strides in recent years due to the success of methods using deep\nneural networks. However, such neural networks act as a black box, obscuring the inner workings. While\nreinforcement learning has the potential to solve unique problems, a lack of trust and understanding of\nreinforcement learning algorithms could prevent their widespread adoption. Here, we present a library that\nattaches a “data scraper” to deep reinforcement learning agents, acting as an observer, and then show how\nthe data collected by the Atari Data Scraper can be used to understand and interpret deep reinforcement\nlearning agents. The code for the Atari Data Scraper can be found here: https: // github. com/ IRLL/\nAtari-Data-Scraper .\nI.\nIntroduction\nR\neinforcement learning allows an agent to\nlearn from interacting with an environ-\nment iteratively, learning sequences of\nactions in order to perform tasks and reach\ngoals [15].\nAs reinforcement learning algo-\nrithms have achieved new records on bench-\nmarks, they have also become more complex;\nmany of the top-performing reinforcement\nlearning algorithms today use deep neural net-\nworks, which are considered black-box algo-\nrithms. It is unlikely that signiﬁcantly less com-\nplex or more transparent reinforcement learn-\ning algorithms will be able to achieve the same\nperformance, in part because reinforcement\nlearning incorporates the temporal aspects of\nproblem solving. Additionally, reinforcement\nlearning algorithms perform best when they\nhave access to more information about the en-\nvironment. Consequently, top-performing al-\ngorithms to have the capability to represent\ncomplex features, often in non-linear ways.\nAs deep reinforcement learning algorithms\napproach and surpass human capabilities in\nsome domains, a new approach to interpret-\ning and explaining such agents may be needed.\n*Corresponding author\nSuppose we treat deep reinforcement learning\nalgorithms as if they were human-like subjects.\nIn doing so, we could use select methodologies\nfrom studies of human subjects in sociology,\nmarketing, psychology, environmental sciences\nand more. In biological ﬁeld research, for exam-\nple, animals under observation may be ﬁtted\nwith a device to record speciﬁc information. As\nﬁtness trackers have developed, some sociolog-\nical studies have similarly asked participants\nto wear devices to track information like daily\nactivity levels.\nCurrent\ncommonly-used\nreinforcement\nlearning libraries like OpenAI’s Baselines1,\nTensorforce2, keras-rl3, TF-Agents4 and more\nmake it difﬁcult and time-consuming to collect\nadditional data about deep reinforcement\nlearning agents beyond simply their scores.\nHowever, we argue, improving access to\ndata regarding the agent’s actions, rewards,\nlocation, etc. could help to make deep rein-\nforcement learning agents more interpretable.\nConsequently, users from novices to experts\nget more out of each time an agent interacts\n1https://github.com/openai/baselines\n2https://github.com/openai/baselines\n3https://github.com/keras-rl/keras-rl\n4https://www.tensorflow.org/agents\n1\narXiv:2104.04893v1  [cs.LG]  11 Apr 2021\nThe Atari Data Scraper • April 2021 • arXiv.com\nwith an environment. In order to facilitate a\nmovement towards more data-availability, we\nhave created a function that creates an Atari\nData Scraper. The Atari Data Scraper collects\ninformation about agents as they interact with\nvarious games from the Atari 2600 suite of\nOpenAI’s gym [1] and saves the data for later\nexamination.\nThe remainder of this article is structured as\nfollows: Section II covers relevant background\non interpretable deep reinforcement learning.\nSection III details the creation of a data scraper\nfor use with the popular Stable Baselines deep\nreinforcement learning library. Section IV fea-\ntures the outcome from running two agents\nwith an Atari Data Scraper attached. Section V\nconcludes with a discussion of the Atari Data\nScraper’s known limitations and the resulting\ndirections for future work.\nII.\nBackground\nDeep reinforcement learning’s use of deep neu-\nral networks as function approximators makes\nit inherently difﬁcult to interpret. The ﬁrst al-\ngorithm to successfully learn from pixel-only\ninput was the Deep Q-Network algorithm pro-\nposed by Mnih et al.\nin 2015 [10].\nThe al-\ngorithm, which is today referred to as DQN,\nconsists of the original DQN algorithm with\nthree improvements, all of which were pro-\nposed within a year of the groundbreaking\npaper’s publication. In the paper advocating\nfor one of those three improvements, Wang\net al.\nused saliency maps to visualize how\nDueling Q-Networks altered the functioning\nof the original DQN algorithm [16], showing\nthat from the very start of deep reinforcement\nlearning, visualizations were being used to in-\nterpret algorithms and their resulting agents.\nSaliency maps have played a large role in this,\nwith some researchers examining in detail how\nwell various existing saliency-creation meth-\nods work in deep reinforcement learning [7]\n[13]. Others are proposing entirely new meth-\nods tailored speciﬁcally to the challenges and\ncapabilities of deep reinforcement learning al-\ngorithms [4] [13] [11] [12].\nHowever, visual inspection of saliency maps\nis not the only way deep reinforcement learn-\ning algorithms have been interpreted. There\nhas long been a desire for more quantitative\nways to interpret deep reinforcement learning\nagents. In a 2013 preprint of a less-developed\nDQN algorithm, Mnih et al. included a dis-\ncussion about whether averaged reward or\nthe estimated action-value function should be\ngraphed to examine agent improvements and\ncompare different agents [9]. Since then, in\nmany published landmark deep reinforcement\nlearning papers, graphs are prominently fea-\ntured as ﬁgures designed to provide a sum-\nmary of the claims in the papers. Even saliency\nmaps have been compared using distance mea-\nsurements and other quantitative evaluation\nmetrics rather than pure visual inspection [2].\nAs deep reinforcement learning has devel-\noped, the search for quantitative metrics has\nsimilarly evolved. In 2020, Sequeira et al. pro-\nposed a framework using records from an\nagent’s interaction with an environment to\nmake a more interpretable agent [14]. They\nshowed how the framework might be used on\na traditional Q-learning agent, but plan to ex-\ntend the framework so that it can be used for\nDRL agents, too. The framework collected data\nsuch as how often a state was visited, current\nestimates of the values of states and actions,\nand the agent’s uncertainty in each state. By\ncollecting such data, the authors were able to\ndo things like ﬁnding sequences from bad sit-\nuations to good situations, or locate places in\nthe environment where the agent is generally\nuncertain.\nIn a similar vein, our Atari Data Scraper\nproject seeks to uncover what could be learned\nby collecting data on a deep reinforcement\nlearning agent, just as one might collect data\non an animal species of interest in the wild.\nAs a proof-of-concept, the ﬁrst iteration of a\ndata scraper was created as an additional class\ninserted into a fork of the OpenAI Baselines\nrepository [3]. We modiﬁed the implementa-\ntion of the Deep Q-Network (DQN) algorithm\nwithin the original repository to use this addi-\ntional class to collect a set of data. After each\n2\nThe Atari Data Scraper • April 2021 • arXiv.com\ntime step, the data scraper object was given the\nfollowing:\n• The number associated with the action\ntaken at that time step\n• The reward received from the environ-\nment\n• An array of the sum total of each action\nfor the current life\n• An array of the sum total of each action\nfor the current game\n• A boolean indicating if a life had been lost\non this step\n• The number of lives remaining\n• An array of the Q values for this step\n• The feature vector describing the current\ngame describing the current game screen\nThe ﬁrst iteration of the data scraper concept\nused this dataset to create a record for the cur-\nrent step. For all data passed in, besides the\ncurrent Q values and the feature vector, the\ndata was added as-is to the step record. Addi-\ntional data was also collected to aid in creating\nvisualizations. These include a ﬂag to denote\nif the step was the end of a game, the running\ntotal reward for the current life and the current\ngame, and the running total of steps taken for\nthe current life and the current game Addition-\nally, the feature vector was used to extract the\nx- and y-coordinates for Ms. Pacman and each\nof the four ghosts, which we will refer to as the\ncharacters in the environment. Then this data\nwas used to calculate the distance between Ms.\nPacman and each ghost, and the distance be-\ntween Ms. Pacman and each of the four Power\nPills. This data was in turn used to determine\nif Ms. Pacman had just eaten a Power Pill and,\nif so, mark that Power Pill as consumed for this\ngame.\nIII.\nImplementation Details\ni.\nThe Initial Concept\nIn the initial implementation, the characters’\nlocations were found by ﬁrst applying an edge\ndetector to each game frame and then search-\ning each discovered contour for a set of colors\nassociated with each of ﬁve characters: Ms.\nPacman and each of the four ghosts [8]. If a\ncolor in the speciﬁed set was found, the cen-\nter of that contour would be calculated and\nrecorded as that character’s position. The set\nof colors was created using an eyedropper tool\non screenshots of the environment to deter-\nmine as small a range of colors as possible for\neach character.\nii.\nA More Generic Atari Data Scraper\nThe initial proof-of-concept had some lim-\nitations.\nThe Atari Data Scraper is a re-\ndesign of the initial proof-of-concept, and\nis available here: https://github.com/IRLL/\nAtari-Data-Scraper. In the initial implemen-\ntation, a lot of computation was done on each\nstep to create a complete record before the next\nstep was taken. In the second iteration of the\nAtari Data Scraper, this initial data collection\nwas slimmed down to only the bare essentials\nto reduce computational “drag.”\nThen, the\nAtari Data Scraper can call a secondary pro-\ngram that takes that data and expands it from\nthe saved ﬁle to a larger dataset. Alternatively,\nthat processing can be attached to the end of\nan agent’s run by passing in an optional ﬂag.\nPart of reducing the work done by the Atari\nData Scraper during the agent’s training was\nmaking the choice to save a screenshot of the\ncurrent step rather than use the observed fea-\nture vector to immediately calculate the po-\nsitions of the characters in the environment.\nWhile this choice greatly increased the storage\nused by the Atari Data Scraper, it allowed the\ndata to be processed in two steps if needed. In\norder to handle the large number of images cre-\nated by multiple runs of the Atari Data Scraper,\nan option was added to have the images auto-\nmatically deleted after processing is completed.\nIn addition, some of the calculations done by\nthe ﬁrst iteration of the Atari Data Scraper were\nnot useful in understanding the agent. These\ncalculations can still be done manually using\nthe collected data, but we chose to scale back\nthe total amount of data automatically gener-\nated and calculated to only that which was\n3\nThe Atari Data Scraper • April 2021 • arXiv.com\nhelpful in understanding our agents. Items\nmarked with an asterisk in table 1 are collected\nat each step by the Atari Data Scraper, while\nall other items are calculated or pulled using\nimage processing by the secondary program.\nWe also used the redesign as an opportu-\nnity to improve the capabilities of the character\ntracking component of the Atari Data Scraper.\nFor one, due to the low resolution of the game\nscreen, the contours found by the edge detector\nwere not always crisp. Occasionally, one break\nin a detected edge would lead to the contour\ndetermined to be Ms. Pacman to merge with\na nearby wall into a single contour. When this\nhappened, the center of the contour, which was\nstored as the character’s location, would sud-\ndenly jump away and then return to a nearby\nlocation on the next step. To overcome this\nproblem in the more generic version of the\nAtari Data Scraper, the function which locates\ncharacters does not use contours. Instead, it\nsearches the entire image for the speciﬁed color\nranges. Since each of the ﬁve characters is a\ndifferent color when the ghosts are not toggled\nto dark-blue by a Power Pill, all ﬁve characters\ncould be distinguished most of the time. Since\nMs. Pacman never changes color, the agent’s\nlocation could almost always be tracked us-\ning this method. If the ghosts are not found,\nthen the contours are searched to try and locate\nfour dark-blue ghosts. Secondly, the use of a\ncolor picker was phased out, as color pickers\nshowed values which, despite being reported\nfor the same color, were different on different\ndevices. Instead, the matplotlib library was\nused to identify the color of speciﬁc pixels, and\nthe reported colors were used to deﬁne the\ncolor ranges for each character, resulting in\nmore accurate character tracking.\nThe ﬁrst iteration worked by inserting a func-\ntion call within the implementation of DQN,\nat each step. This approach requires copying\nthe repository so that the function call can be\ncalled after the agent takes each step in the\nenvironment. In order to allow the Atari Data\nScraper to be used without having to customize\na repository, we decided to choose a popular\ncode base of deep reinforcement learning im-\nplementations and then developed a method\nwhich can be passed in to that library’s exist-\ning functions. The OpenAI Baselines reposi-\ntory has little documentation, and has seen a\ndrop-off in activity in recent years. Therefore,\nwe chose a popular fork of OpenAI baselines\ncalled Stable Baselines [6], which today is often\nrecommended above Baselines.\nOne useful feature of Stable Baselines is the\nability to create custom callbacks. According\nto the Stable Baselines documentation, “A call-\nback is a set of functions that will be called\nat given stages of the training procedure [5].”\nThe custom callback class provided by Stable\nBaselines includes a function which is called on\neach step. Using this function, we collect the\nslimmed-down data on each step. A callback\ncan be used with an existing implementation of\na reinforcement learning algorithm by simply\nsetting the callback parameter in the learning\nfunction. This allows the improved second it-\neration of the Atari Data Scraper to be easily\nused with existing Stable Baselines implemen-\ntations.\nIV.\nResults\nThe initial iteration of the Atari Data Scraper\nonly worked with the DQN implementation\nand the Ms.Pacman environment. By using\nthe callback method, the improved Atari Data\nScraper could easily be passed into any re-\ninforcement learning algorithm’s implemen-\ntation in the Stable Baselines library. We have\ntested and veriﬁed that the callback works with\nthe Stable Baselines implementations of DQN,\nAdvantage Actor Critic (A2C), and Proximal\nPolicy Optimization (PPO2). We adjusted the\ncallback to ensure it would work, not only for\nthe Ms.\nPacman environment, but also for\nthe Pong environment. In the Pong environ-\nment, the two paddles and the ball are treated\nas characters, and their locations recorded at\neach time step along with the score and game\nnumber. In the Ms. Pacman environment, Ms.\nPacman and the four ghosts are treated as char-\nacters. For both the Ms.Pacman environment\nand the Pong environment, we used the fourth\n4\nThe Atari Data Scraper • April 2021 • arXiv.com\nTable 1: A listing of the data collected for DQN, A2C and PPO2 in the Ms. Pacman and Pong environments. An\nasterisk (*) marks data collected by default. All other data is collected and created in an optional second pass.\nData Collected by the Atari Data Scraper\nMs. Pacman\nPong\nA2C/PPO2\nDQN\nA2C/PPO2\nDQN\nstep number*\nstep number*\nstep number*\nstep number*\naction name*\naction name*\naction name*\naction name*\naction number*\naction number*\naction number*\naction number*\nstep reward*\nlife reward*\ngame reward*\ngame reward*\nlives*\nlives*\nball x-coordinate\nball x-coordinate\ncharacters’ x-coordinates\ncharacters’ x-coordinates\nball y-coordinate\nball y-coordinate\ncharacters’ y-coordinates\ncharacters’ y-coordinates\npaddles’ x-coordinates\npaddles’ x-coordinates\ndistances to ghosts\ndistances to ghosts\npaddles’ y-coordinates\npaddles’ y-coordinates\npill eaten statuses\npill eaten statuses\npaddle to ball distance\npaddle to ball distance\ndistance to pills\ndistance to pills\nstep reward\ncurrent life rewards\nstep reward\ncurrent game rewards\ncurrent life step\ncurrent life step\ncurrent game step\ncurrent game step\ngame number\ngame number\ntotal reward\ntotal reward\nlife number\nlife number\nreward at end of game\nend of game ﬂag\nversion available through OpenAI gym. For\neach of the two environments, all three algo-\nrithms were run: PPO2, A2C, and DQN. A\nnotebook with the visualizations made using\nthe data collected by the Atari Data Scraper can\nbe found in the Atari Data Scraper repository.\nThe DQN algorithm runs an agent through\nan environment, and data is collected about the\ncharacters in that environment. A2C and PPO2\ncan use multiple agents, each in its own en-\nvironment, to do batch training. In situations\nwith multiple environments running concur-\nrently, the Atari Data Scraper collects informa-\ntion for each environment. For an agent trained\nto play the game Ms.Pacman using DQN, the\ninformation collected can be used to create a\nsummary of all the games played by the agent,\nas shown in ﬁgure 1. In addition, we used the\nAtari Data Scraper with agents trained to play\nthe game Ms. Pacman with both the A2C and\nPPO2 algorithms. For the agent trained using\nthe A2C algorithm, the collected data was used\nto examine the quality of the games played in\neach of 4 concurrent environments over time\nas seen in ﬁgure 2. In ﬁgure 3 and ﬁgure 4, we\nshow how the collected data can be ﬁltered to\ndisplay the top 3 best and worst performing\ngames, with additional information about each\ngame.\nIn Pong, the resulting data allowed us to ex-\namine the change in the distance between the\nagent’s paddle and the ball each time the agent\nmissed the ball, allowing the opponent to score\na point, as seen in ﬁgure 5. A game of Pong\nends whenever the two players’ scores differ\nby 20 points. Using the Atari Data Scraper, we\nwere also able to visualize the agent’s learning\nprocess in a more traditional manner, via the\nagent’s score at the end of each game. The plot\nin ﬁgure 6 starts consistently negative, as the\nopponent continues to get 20 more points than\nthe agent quickly and easily, but gets into pos-\nitive numbers more often as the agent learns\nhow to score against the opponent.\n5\nThe Atari Data Scraper • April 2021 • arXiv.com\nFigure 1: A summary of an agent trained with DQN playing Ms. Pacman\nV.\nLimitations and Future Work\nIn an ideal world, we could ﬁnd a data col-\nlection mechanism that could be used across\nall existing or forthcoming deep reinforcement\nlearning algorithms. It would likewise be ap-\nplicable to all environments, be it part of the\nOpenAI gym framework or a custom environ-\nment. In reality, any mechanism of collecting\ndata from a deep reinforcement learning agent\nneeds an attachment into either the agent or\nthe environment by which it can access and\ncollect the data. Thus, we had to impose limi-\ntations on the Atari Data Scraper. Any one of\nthese imposed limitations is a potential avenue\nfor future work.\nFor one, there are some existing algorithm\nimplementations which learn faster and have\nadditional bells and whistles. In this project,\nwe wanted to create a data collection mech-\nanism that could be used easily by someone\nwho is just trying to get their ﬁrst deep re-\ninforcement learning agent running, and also\nby a researcher who has worked in the ﬁeld\nof deep reinforcement learning for years. For\nthis reason, we avoided single-algorithm deep\nreinforcement learning implementations. In-\nstead, we chose from among deep reinforce-\nment learning libraries which have many of the\nmost common algorithms implemented, any of\nwhich can be trained with a generic function\ncall. The Stable Baselines library was chosen in\npart because it has more documentation and\nfewer bugs than other options we explored, cre-\nating a shallower learning curve for those just\ngetting started in deep reinforcement learning.\nThe actual collection process is set in motion by\na function call speciﬁc to the Stable Baselines\nlibrary, the callback. A clear limitation of the\nexisting Atari Data Scraper is that is that this\nspeciﬁc function call would not work in other\ndeep reinforcement libraries.\nThe Atari Data Scraper was also speciﬁcally\ndesigned around Atari environments within\nthe OpenAI gym framework. We chose to focus\non this set of environments because it was less\nlimiting: the popular gym framework general-\nizes much of deep reinforcement learning, and\nso an Atari-focused Data Scraper could more\neasily be generalized to a multitude of other ex-\nisting environments which are registered in the\nOpenAI gym. In comparison, a data scraper\ndesigned around a single custom environment,\nespecially one not built around the gym frame-\nwork, would need signiﬁcant re-writing to be\nadapted to any other environment. To that end,\nthe data collection performed by the Atari Data\nScraper could also be achieved by an OpenAI\n6\nThe Atari Data Scraper • April 2021 • arXiv.com\nFigure 2: A summary of an agent trained with A2C playing Ms. Pacman\nFigure 3: The 3 bestgames played by an agent trained by\nthe A2C algorithm using 4 environments\ngym wrapper. Such wrappers also allow access\nto a function called on each step. Future work\nwhich implements the Atari Data Scraper as a\ngym wrapper would allow the Data Scraper to\nbe generalized across deep reinforcement learn-\ning libraries and potentially even to some of\nthe most cutting-edge algorithms. A data col-\nlection mechanism written as a wrapper would\nstill be limited to only environments registered\nin the OpenAI gym. Other future work could\nFigure 4: The 3 worst games played by an agent trained\nby the A2C algorithm using 4 environments\nchoose instead to write data collection mech-\nanisms for some of the most popular environ-\nments for deep reinforcement learning outside\nof the gym framework.\nHowever, this would still leave one of the\nmost restrictive limitations on the current Atari\nData Scraper in place. The biggest limitation\nis the method by which characters and items\nin the environment are located. In order to\nlocate characters, the Atari Data Scraper saves\n7\nThe Atari Data Scraper • April 2021 • arXiv.com\nFigure 5: The distance between the paddle and ball when\nthe agent misses and the opponent gains a\npoint. We can see it decreases on average as\ntraining increases.\nFigure 6: The increasing ability of an agent trained with\nDQN to play Pong\na screenshot of each step, requiring a lot of\nstorage space. The screenshot is then searched,\nusing image processing to ﬁnd pixels within\nhand-crafted color ranges. Thus, to be applied\nto a new environment, the Atari Data Scraper\nmust be equipped with painstakingly calcu-\nlated color ranges for individual objects in the\nenvironment.\nLogically, this also limits the\nAtari Data Scraper to environments where the\ncolor of important objects does not appear else-\nwhere in the environment. Future work could\ninclude developing a method of automatically\ntracking items in the environment, ideally in\na way that also allows broader generalization\nacross the gym framework and across various\nlibraries and implementations.\nFinally, the Atari Data Scraper uses the re-\nward signal to infer other information. For\nexample, in Ms. Pacman, eating a Power Pill\nearns the agent a reward of positive ﬁfty. The\nAtari Data Scraper uses this fact, along with\nthe agent’s coordinates, to record when each\nPower Pill is eaten in a game. Thus, the Atari\nData Scraper will not work as well for environ-\nments with clipped rewards or with rewards\nthat do not differentiate between events. For\nexample, in Ms. Pacman, eating one type of\nfruit and one type of ghost result in the same\nreward, making it much more difﬁcult to de-\ntermine which event occurred just by looking\nat the current reward signal. Additional future\nwork could seek other ways of accessing the in-\nformation currently carried in rewards signals.\nAlternatively, it could focus on transforming\nany rewards signal so that all important events\nprovide differing rewards.\nVI.\nAcknowledgements\nPart of this work has taken place in the Intelli-\ngent Robot Learning Laboratory at the Univer-\nsity of Alberta (IRLL), supported in part by re-\nsearch grants from the Alberta Machine Intelli-\ngence Institute (Amii), CIFAR, and NSERC. We\nwould like to acknowledge the help provided\nin this project by the members of IRLL. We\nare honored to be researching alongside such\nhard-working people and grateful for all their\nhelp in developing these ideas, proof-reading\nour writing, and testing our code. We would\nespecially like to thank Dr. Matthew Taylor for\nhis assistance in putting this team together and\nhis guidance throughout this project. In addi-\ntion, we would like to thank Dr. Dustin Arendt\nat Paciﬁc Northwest National Laboratory for\nhis advice and guidance on this project.\nReferences\n[1] Greg Brockman, Vicki Cheung, Ludwig\nPettersson, Jonas Schneider, John Schul-\nman, Jie Tang, and Wojciech Zaremba.\nOpenAI Gym. arXiv:1606.01540 [cs], June\n2016.\n8\nThe Atari Data Scraper • April 2021 • arXiv.com\n[2] Zoya\nBylinskii,\nTilke\nJudd,\nAude\nOliva,\nAntonio\nTorralba,\nand\nFrédo\nDurand.\nWhat do different evaluation\nmetrics tell us about saliency models?\narXiv:1604.03605 [cs], April 2017.\n[3] Prafulla Dhariwal, Christopher Hesse,\nOleg Klimov,\nAlex Nichol,\nMatthias\nPlappert, Alec Radford, John Schulman,\nSzymon Sidor, Yuhuai Wu, and Peter\nZhokhov. OpenAI Baselines, 2017.\n[4] Sam Greydanus, Anurag Koul, Jonathan\nDodge,\nand\nAlan\nFern.\nVisualiz-\ning and Understanding Atari Agents.\narXiv:1711.00138 [cs], September 2018.\n[5] Ashley Hill. Stable Baselines Docs: Call-\nbacks.\n[6] Ashley Hill, Christopher Hesse, Oleg\nKlimov, Alex Nichol, Matthias Plappert,\nAlec Radford, John Schulman, Szymon\nSidor, Yuhuai Wu, Peter Zhokhov, An-\ntonin Rafﬁn, Maximilian Ernestus, Adam\nGleave, Anssi Kanervisto, Rene Traore,\nand Prafrulla Dhariwal. Stable Baselines,\n2018.\n[7] Tobias Huber, Benedikt Limmer, and Elis-\nabeth André. Benchmarking Perturbation-\nbased\nSaliency\nMaps\nfor\nExplaining\nDeep Reinforcement Learning Agents.\narXiv:2101.07312 [cs], January 2021.\n[8] RD Milligan. Detecting objects in Pac-Man\n(Mark II), 2015.\n[9] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid\nSilver,\nAlex\nGraves,\nIoannis\nAntonoglou, Daan Wierstra, and Martin\nRiedmiller. Playing Atari with Deep Rein-\nforcement Learning. arXiv:1312.5602 [cs],\nDecember 2013.\n[10] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Andrei A. Rusu, Joel Veness,\nMarc G. Bellemare, Alex Graves, Martin\nRiedmiller, Andreas K. Fidjeland, Georg\nOstrovski, Stig Petersen, Charles Beattie,\nAmir Sadik, Ioannis Antonoglou, Helen\nKing, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis.\nHuman-level control through deep rein-\nforcement learning. Nature, 518(7540):529–\n533, February 2015.\n[11] Dmitry\nNikulin,\nAnastasia\nIanina,\nVladimir Aliev, and Sergey Nikolenko.\nFree-Lunch Saliency via Attention in\nAtari Agents.\narXiv:1908.02511 [cs],\nOctober 2019.\n[12] Nikaash Puri, Sukriti Verma, Piyush\nGupta, Dhruv Kayastha, Shripad Desh-\nmukh, Balaji Krishnamurthy, and Sameer\nSingh. Explain Your Move: Understand-\ning Agent Actions Using Speciﬁc and Rele-\nvant Feature Attribution. arXiv:1912.12191\n[cs], April 2020.\n[13] Matthias Rosynski, Frank Kirchner, and\nMatias Valdenegro-Toro. Are Gradient-\nbased Saliency Maps Useful in Deep Re-\ninforcement Learning? arXiv:2012.01281\n[cs], December 2020.\n[14] Pedro Sequeira and Melinda Gervasio.\nInterestingness Elements for Explainable\nReinforcement Learning: Understanding\nAgents’ Capabilities and Limitations. Ar-\ntiﬁcial Intelligence, 288:103367, November\n2020.\n[15] Richard S. Sutton and Andrew G. Barto.\nReinforcement Learning: An Introduction.\nAdaptive\nComputation\nand\nMachine\nLearning Series. The MIT Press, Cam-\nbridge, Massachusetts, second edition edi-\ntion, 2018.\n[16] Ziyu Wang, Tom Schaul, Matteo Hessel,\nHado van Hasselt, Marc Lanctot, and\nNando de Freitas. Dueling Network Archi-\ntectures for Deep Reinforcement Learning.\narXiv:1511.06581 [cs], April 2016.\n9\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2021-04-11",
  "updated": "2021-04-11"
}