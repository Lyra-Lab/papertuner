{
  "id": "http://arxiv.org/abs/2104.05000v1",
  "title": "Saddlepoints in Unsupervised Least Squares",
  "authors": [
    "Samuel Gerber"
  ],
  "abstract": "This paper sheds light on the risk landscape of unsupervised least squares in\nthe context of deep auto-encoding neural nets. We formally establish an\nequivalence between unsupervised least squares and principal manifolds. This\nlink provides insight into the risk landscape of auto--encoding under the mean\nsquared error, in particular all non-trivial critical points are saddlepoints.\nFinding saddlepoints is in itself difficult, overcomplete auto-encoding poses\nthe additional challenge that the saddlepoints are degenerate. Within this\ncontext we discuss regularization of auto-encoders, in particular bottleneck,\ndenoising and contraction auto-encoding and propose a new optimization strategy\nthat can be framed as particular form of contractive regularization.",
  "text": "Saddlepoints in Unsupervised Least Squares\nSamuel Gerber\nApril 13, 2021\nAbstract\nThis paper sheds light on the risk landscape of unsupervised least squares in the\ncontext of deep auto–encoding neural nets. We formally establish an equivalence be-\ntween unsupervised least squares and principal manifolds. This link provides insight\ninto the risk landscape of auto–encoding under the mean squared error, in particular\nall non-trivial critical points are saddlepoints. Finding saddlepoints is in itself difﬁcult,\novercomplete auto–encoding poses the additional challenge that the saddlepoints are\ndegenerate. Within this context we discuss regularization of auto–encoders, in particu-\nlar bottleneck, denoising and contraction auto–encoding and propose a new optimiza-\ntion strategy that can be framed as particular form of contractive regularization.\n1\nIntroduction\nThis papers examines the risk landscape of unsupervised least squares. Unsupervised least\nsquares aims to ﬁnd a reconstruction map r that minimizes the discrepancy of the output\nto it’s own input , i.e., minimizing ∥r(x) −x∥2. In the neural network literature this is\nreferred to as auto–encoding. Without any restrictions on r the identity mapping is an\noptimal, albeit uninformative, solution. An informative solution should provide a salient\nsummary of the data. To ﬁnd informative solutions, auto–encoding is typically formulated\nas a concatenation of an encoder map λ and a decoder map g with r = g◦λ with restrictions\non λ or g to avoid learning the identity function. Several regularization methods exist to\nenforce such restrictions. For example, the bottleneck auto–encoder forces the map λ\nto be of lower dimension than the input through adding a bottleneck layer. While such\nregularization avoid learning the identity map, they do not address the general problem of\noverﬁtting in unsupervised least squares.\nFor networks with linear activations, [Kawaguchi, 2016] show that for auto-encoding\nall minima are globally optimal and the solutions correpsonds to the maximal principal\nsubspace. This paper provides an analogous characterization of the critical points of the\nrisk function for deep auto-encoders in the non-linear case. In particular all critical points,\nbesides minima with zero risk, are saddle points. Zero risk solutions defy the purpose of\nautencoders, i.e. ﬁnding a more compact representation that characterizes the input data.\nIn the linear case solution with zero risk are identity functions. Zero In the non-linear case\nzero risk solutions include, besides the identity, space ﬁlling manifolds. This highlights\nan important distinction between the linear and non-linear case. In the linear case it is\n1\narXiv:2104.05000v1  [cs.LG]  11 Apr 2021\nsufﬁcent to restrict the dimensionality of the principal subspace, for example through the\nuse of a bottleneck layer. For non-linear auto-encoders additional regularization of the\nencoding and decoding functions is required, otherwise the optimzation will tend towards\nspace-ﬁlling solutions.\nWe establish a tight connection between unsupervised least squares and the more re-\nstrictive notion of a principal manifold. This connection illuminates the risk landscape of\nunsupervised leas squares, which in turn informs the behaviour of auto–encoding under var-\nious forms of regularization. Hastie and Stuetzle [1989] formally deﬁned principal curves\nas curves that pass through the middle of a distribution and showed that principal curves are\ncritical points of the mean squared reconstruction error. This indicates a tight connection\nto unsupervised least squares. Principal manifolds enforce that the reconstruction func-\ntion constitutes an orthogonal projection, while unsupervised least squares permit arbitrary\nreconstruction functions. To establish the equivalence between principal manifolds and\nunsupervised least squares we show that the critical points of the mean squared error risk\nfor arbitrary reconstruction functions, and thus the critical points of auto–encoders under\nsquared error loss, are principal manifolds. The connection of unsupervised least squares\nto principal manifolds illuminates the shape of the auto–encoder risk landscape. In par-\nticular Duchamp and Stuetzle [1996] showed that principal curves are saddlepoints of the\nmean squared error, this observation also holds for auto–encoding. Duchamp and Stuetzle\n[1996] and Gerber and Whitaker [2013] demonstrate that the saddlepoint nature of the risk\nlandscape leads to overﬁtting and renders traditional cross-validation techniques infeasible\nfor tuning of regularization parameters.\nThis difﬁculty persists for auto–encoders as Figure 1 illustrates in the case of bottleneck\nauto–encoding. The bottleneck layer enforces a hard constraint on the dimensionality of\nthe reconstruction function, while the network architecture implicitly regularizes the com-\nplexity of the reconstruction function. However, due to the saddlepoint nature of the risk\nlandscape the risk on test data keeps decreasing with increasing network complexity. Thus,\ncross-validation will fail to suggest an appropriate network complexity. In the limit, with\ninﬁnite training data and no restriction on the complexity of the reconstruction function a\nspace ﬁlling curve, with zero reconstruction error, is an optimal ﬁt. These global minima,\nspace-ﬁlling manifolds or the identity function, of the auto–encoding risk are not desirable\nsolutions and do not provide an adequate summary representation of the data. The hard\ndimensionality constraint in bottleneck auto–encoders avoids ﬁtting the identity function.\nHowever, the hard constraint requires careful initialization to ﬁnd an adequate parametriza-\ntion. As an alternative to the bottleneck layer several regularization schemes, such as de-\nnoising [Vincent et al., 2008] and contractive auto–encoding[Alain and Bengio, 2014], for\ncontrolling the dimensionality of the reconstruction function have been proposed. These\nregularization methods provide a soft constraints on dimensionality and solve or circum-\nvent the issue of inadequate parametrization. The soft dimensionality constraints do not\nalleviate the challenge of tuning the regularization or selecting an appropriate network ar-\nchitecture. Figure 2 shows that for a ﬁxed contractive penalty the test error for different\nnetwork architecture either selects an architecture that severely overﬁts or tends towards\nthe identity mapping. Fixing the network architecture while adjusting the regularization\ndoes not solve the issue as Figure 3 illustrates. A very ﬂexible network that permits severe\noverﬁtting leads to the smallest test error. If the network architecture is restrictive enough\n2\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n(a)\n(b)\n(c)\nFigure 1: Bottleneck auto–encoding enforces a hard dimensionality constrain through a\nbottleneck neural network layer. The images show (top row) the minima found after 20,000\niterations and (bottom row) the root mean square error on training (green solid line) and\ntest data (purple dashed line) using network architectures with hidden layers of (a) 50–100–\n200–100–50–1–50–100-200–100–50, (b) 50–100–50–1–50–100–50, (c) 200–1–200 units.\nDepending on the network architecture, auto–encoding does (a) overﬁt, (b) ﬁt well or (c)\nunderﬁt. The relatively small reach of the spiral example requires a fairly deep network\nto achieve an accurate ﬁt. However, the test error does not suggest that the ﬁt in (b) is\nadequate and suggests to use the most powerful network. In all cases the one-dimensional\nparametrization of the auto–encoder jumps across the spiral arms.\nthe test error decreases with decreasing penalty weight and the solution tends towards the\nidentity mapping. In practice the structure in the data is typically not know and it would\nbe difﬁcult to select an appropriate network architecture. Thus, a method that imposes a\ndimensionality constraint without reliance on the implicit regularization of the network ar-\nchitecture is desirable. We contend that the difﬁculties in regularization of auto–encoders\nare due to the saddlepoint structure of the unsupervised least squares risk.\nThe critical points of the unsupervised least squares risk are principal manifolds and\nprovide an arguably reasonable summary representation of the data. However, except for\nthe uninformative identity solution and space ﬁlling manifolds the critical points are saddle-\npoints. Figures Figures 1, 2 and 3 illustrate that the saddlepoint nature of the critical points\nrenders cross–validation useless, since it is possible to move away from a critical point\nwithout increasing the risk on test data. Furthermore steepest decent methods will typically\nnot even move towards desirable solutions. In the principal manifold setting, Gerber and\nWhitaker [2013] propose a solution to the saddlepoint challenge based on minimizing a\ndifferent risk. The novel risk has the property that principal manifolds are now minima\ninstead of saddlepoints. We show that this risk, derived from geometrical considerations,\n3\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n(a)\n(b)\n(c)\nFigure 2: Effect of network architecture in combination with contractive regularization\npenalty. The image show the (top row) minima found after 20,000 iterations and (bottom\nrow) the root mean square error on training (green solid line) and test data (purple dashed\nline) using a ﬁxed penalty of 0.02 and network architectures with hidden layers of (a)\n50–100–200–200–100–50, (b) 50–100–100–50, (c) 200–200 units. The gray lines show a\npolar grid on the input space and the black lines show the deformation of the polar grid after\nmapping it through the auto–encoder. The gray line shows the spiral the data is sample from\nwith noise. The black lines shows the spiral after mapping it through the auto–encoder.\nis a particular application of a more general approach based on minimizing the norm of the\nGradient of the risk. We apply this Gradient–Norm minimization to auto–encoders, which\nresults in a formulation that bears a close resemblance to contractive auto–encoding [Alain\nand Bengio, 2014].\n2\nThe Unsupervised Least Squares Risk\nThis section shows that the critical points of the unsupervised least squares risk are principal\nmanifolds and investigates the properties of those critical points in detail. Section 2.1\nrevisits the deﬁnition of a principal curves and manifolds and Section 2.2 establishes the\nconnection of the unsupervised least squares risk to principal manifolds by examination of\nthe ﬁrst variation of the unsupervised least squares risk.\n2.1\nConnection to Principal Manifolds\nLet X be a random variable with a smooth density p such that the support Ω= {x : p(x) >\n0} is a compact, connected region with smooth boundary. Denote by E the expectation\noperator, i.e., E [f(X)] =\nR\nΩf(x)p(x)dx.\nRecall the formal deﬁnition of principal curves.\nDeﬁnition 2.1 (Principal Curve [Hastie and Stuetzle, 1989]). Let g : Λ →Rn, Λ ⊂R and\n4\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n0\n20\n40\n60\n80\n100\nIterations (in 200)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nRoot mean square error\n(a)\n(b)\n(c)\nFigure 3: Effect of regularization penalty for ﬁxed network architecture. The image show\nthe (top row) minima found after 20,000 iterations and (bottom row) the root mean square\nerror on training (green solid line) and test data (purple dashed line) using a network ar-\nchitecture with hidden layers of 50–50–50–50 units and regularization weights (a) 0.04 (b)\n0.02 and (c) 0.005. The gray lines show a polar grid on the input space and the black lines\nshow the deformation of the polar grid after mapping it through the auto–encoder.\nλg : Ω→Λ with projection index λg(x) = maxz{s : ∥y −g(x)∥= inf˜z∥y −g(˜s)∥}. The\nprincipal curves of Y are the set G of smooth functions g that fulﬁll the self consistency\nproperty E[Y|λg(X) = z] = g(z).\nHastie and Stuetzle [1989] showed that principal curves are critical points of the risk\nd(g,X)2 = 1\n2E[∥g(λg(X))−X∥2]. The principal curve risk is closely related to the unsuper-\nvised learning risk, but imposes restriction on the form of the decoder λ, i.e., the encoder\nλ is forced to be the projection index given g\nThe typical approach to estimate principal curves optimizes over g and solver the non–\nlinear problem of ﬁnding λg. To circumvent the non–linear optimization problem of com-\nputing λg Gerber et al. [2009] proposed a new formulation that switches the optimization\nover the encoder λ while ﬁxing gλ(z) ≡E[X|λ(X) = z] to be the conditional expectation\ngiven λ. This yields the risk d(λ,X)2 = 1\n2E\nh\n∥gλ(λ(X))−X∥2i\n. Again closely related to\nthe unsupervised least squares risk but with restrictions imposed on the form of the decoder\ng.\nFor future reference we term the three different formulations as:\n1. Principal manifold risk (pm–risk): d(g,X)2 with λg constrained to be orthogonal to\ng\n2. Conditional expectation manifold risk (cem–risk): d(λ,X)2 with gλ constrained to\nthe conditional expectation given λ.\n3. Unsupervised least squares risk (uls–risk): d(g,λ,X)2 with g and λ unconstrained.\nDuchamp and Stuetzle [1996] showed that principal curves are saddlepoints of the pm–risk\nd(g,X)2 and Gerber and Whitaker [2013] showed that critical points of the cem–risk are\n5\nweak principal curves. Weak principal curves are, as the name implies, a slightly weaker\nversion of principal curves:\nDeﬁnition 2.2 (Weak Principal Curves [Gerber and Whitaker, 2013]). Let g : Λ →Rn and\nλ : Ω→Λ. The weak principal curves of Y are the set Gw of functions g that fulﬁll the self\nconsistency property E[Y|λ(Y) = s] = g(s) with λ satisfying\nD\ny−g(λ(y)), d\ndsg(s)\n\f\f\ns=λ(y)\nE\n=\n0∀y ∈Ω.\nFor principal curves which have no ambiguity points, i.e. all x ∈Ωhave a unique closest\npoint on the curve, the deﬁnition is equivalent to principal curves.\n2.2\nCritical Points are Principal Manifolds\nTo establish the connection of the unsupervised least squares risk\nd(g,λ,X)2 = 1\n2E\n\u0002\n∥g(λ(X))−X∥2\u0003\nto principal manifolds, we show that the critical points of d(g,λ,X)2 are weak principal\nmanifolds.\nFor the pm–risk the optimization is over decoder g only the encoder λ is deﬁned in\nterms of g. Vice versa, for the cem–risk the optimization is over the encoder λ only and\nthe decoder g is deﬁned in terms of λ. The following theorem establishes that the critical\npoints for the uls–risk which includes optimization over both g and λ are weak principal\nmanifolds.\nTheorem 2.1. The critical points of the unsupervised leasts squares risk d(g,λ,X)2 =\n1\n2E\n\u0002\n∥g(λ(X))−X∥2\u0003\nare weak principal curves.\nProof. The Gˆateaux derivative (or ﬁrst variation) of d(λ,g,X)2 with respect to λ is\nd\ndε d(λ +ετ,g,X)2 = E [(g(λ(X)−X))∇g (λ(X))τ(X)]\n(1)\nand with respect to g\nd\ndε d(λ,g+εh,X)2 = E [⟨g(λ(X))−X,h(λ(X))⟩] .\n(2)\nAt a critical point these have to be pointwise zero for any τ (variation of λ) and h (variation\nof g), respectively. This yields the conditions (g(λ(X)−X))∇g (λ(X)) = 0 from equa-\ntion (1) and g(z) = E[X|{x : λ(x) = z}] for any z in the image of λ from equation (2). This\nestablishes that critical points of the uls–risk are weak principal manifolds.\nThe critical points contain uninteresting minimal solutions with zero reconstruction\nresidual, i.e., space ﬁlling manifolds and the identity mapping. Critical points of the pm–\nrisk and cem–risk are saddlepoints, since the uls–risk is more ﬂexible, i.e., is a superset of\nboth the pm–risk and the cem-risk, the critical points of the uls–risk are saddlepoints as\nwell.\n6\nThe uls–risk poses additional challenges. If λ is permitted to be injective, i.e., no di-\nmensionality constraints, then the condition that g(z) = E[X|{x : λ(x) = z}] can only be\nsatisﬁed by the identity mapping, since the set {x : λ(x) = z} is a single point. Addition-\nally, in that setting the critical points have a discontinuous Gˆateaux derivative: Consider\na critical point with encoder λ∗and decoder g∗, with rank ∇λ less than n. Let πα be a\nset of maps such that λ + πα is injective and ∥πα(x)∥≤α. Now from the critical point\nconditions\nR\nλ −1({z}) (g(z)−x) p(x)dx = 0. Let s = (λ +πα)−1({z}), now for any α > 0 we\nhave\nR\n(λ+πα)−1({z}) (g(z)−x) p(x)dx = (g(s)−x) p(x) + α∥∇g(s)∥+ O(α2) which cannot\nbe made arbitrarily small. This discontinuity is expected since the expectation E changes\nabruptly when moving to an injective function, or in fact at any change in the rank of the\nJacobian ∇λ. Thus, in order to avoid the identity mapping it is necessary to constrain the\ndimensionality of the reconstruction function.\n3\nShaping the Unsupervised Least Squares Risk\nTo avoid ﬁtting overly curved solutions Rifai et al. [2011] propose an explicit penalty on\nthe shape of the solution by adding a penalty on the Hessian to contractive encoding. This\napproach combines a regularization on the dimensionality, the Jacobian, and a regulariza-\ntion on the curvature, the Hessian. Penalizing the Hessian is akin to the proposal by K´egl\net al. [2000] in the context of ﬁtting principal manifolds. However, the saddlepoint nature\nof the objective function makes it infeasible to use cross–validation for tuning the amount\nof regularization required.\nTo address the saddlepoint challenge we propose to change the objective function such\nthat all critical points are local minima. Gerber and Whitaker [2013] applied this approach\nto principal manifolds. They observed that principal curves are saddlepoints of the risk\nbecause curves with smaller risk can be achieved by either violating the conditional expec-\ntation constraint in the pm–risk or by violating the orthogonality constraint in the cem–risk\nformulation. This lead Gerber and Whitaker [2013] to minimize orthogonality using the\nrisk:\nq(λ,X)2 = E\n\n\n*\n(gλ(λ(X))−X), d\ndzg(z)\n\f\f\f\f\nz=λ(X)\n+2\n= 0.\n(3)\nGerber and Whitaker [2013] showed that all critical points of this risk are minima and\nprincipal curves.\nIn this section we show that there is a general principle underlying the derivation\nby Gerber and Whitaker [2013] that is not restricted to the principal manifold case. In Sec-\ntion 3.1 we derive the general principle from Newton’s method for optimization and show\nhow it leads to the orthogonal risk in the case of principal manifolds and in Section 3.3 we\napply it to auto–encoding which results in an orthogonal contractive penalty.\n3.1\nGradient–Norm Minimization\nNewton’s method for ﬁnding critical points of a function f : Rm →R is to ﬁnd the zero\ncrossings of ∇f. Newton’s method applied to the gradient of a multivariate function f :\n7\nRm →R yields updates of the form\nxk+1 = xk −αv\nwith v a solution to\n∇2 f(xk)v = ∇f(xk)\nwhere ∇2 f is the Hessian of f. For ∇2 f indeﬁnite the iterations moves—given an appro-\npriate step size—towards a saddlepoint.\nIf the Newton step is difﬁcult to solve, i.e., the inversion of the Hessian to expensive,\none can resort to minimizing the gradient norm ||∇f(x)||2. Since ||∇f(x)||2 ≥0, the critical\npoints of f are minima of the gradient–norm risk. However, depending on the structure\nof f, additional critical points are possible. At critical points of ||∇f(x)||2 the gradient\n∇2 f(x)∇f(x) has to be zero. Thus, either the gradient ∇f has to be zero, the Hessian\n∇2 f is zero or the gradient is a linear combination of directions with zero curvature, i.e.\n∇fv = 0 or ∇2 fv = 0 for all directions v. For functions with non-degenerate Hessian all\ncritical points require ∇f = 0, and the gradient–norm minimization ﬁnds a critical point of\nf. For functions with degenerate Hessians, inﬂection points in the direction of the gradient\nare additional critical points of ||∇f||2. Such critical points can pose a problem for ﬁnding\ncritical points of f through gradient–norm minimization and need to be evaluated.\nNewton’s method is a scaled steepest descent method with scaling by ∇2 f(x)−1. The\ngradient of ∇f(x) is ∇2 f(x)∇f(x), i.e., a steepest descent with scaling by ∇2 f(x). This\navoids having to invert the Hessian but worsen the condition number and results in slower\nconvergence [Boyd and Vandenberghe, 2004].\n3.2\nGradient–Norm Minimization for Principal Manifolds\nApplying the gradient-norm minimization procedure to the cem–risk recovers the geomet-\nrically derived formulation by Gerber and Whitaker [2013]. The gradient of d(λ,X)2 with\nrespect to λ is:\nE\n\"*\n(gλ(λ(X))−X), d\ndsg(s)\n\f\f\f\f\ns=λ(X)\n+#\n.\n(4)\nBy the calculus of variations this has to hold pointwise. Applying the gradient–norm mini-\nmization pointwise recovers the orthogonality risk in Equation 3.\n3.3\nGradient–Norm Minimization for Auto–encoding\nApplying the gradient-norm minimization to the uls–risk from Equations (1) and (2) results\nin the risk:\nE\nh\n(g(λ(X))−X)2i\n+αE\nh\n(g(λ(X))−X)2 ∇g (λ(X))2i\n(5)\nwith α = 1. The ﬁrst term is the squared residual and the second term can be seen as a\ndirectional contraction penalty; the Jacobian of g is penalized but only in the direction of\nthe residual vector. For principal manifolds the second term is zero since the Jacobian of\ng has to be zero in the direction of the residual, i.e. g · λ is an orthogonal projection to g.\n8\nTreating the second term of the derivative as a penalty we can weigh it differently through\nchanging α during optimization.\nFigure 4 illustrates this approach on a bottleneck neural network architecture and com-\npares it with no and contractive regularization.\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n(a)\n(b)\n(c)\nFigure 4: The image show the minima found after 20,000 iterations using a network archi-\ntecture with hidden layers of 50–100–200–1–200–100–50, units and (a) no regularization\n(b) contraction penalty and (c) orthogonal contractive penalty.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n||x −r(x)||\nPenalty\n||x −r(x)||2\n||(x −r(x))Jr(x)||2\nα||(x −r(x))Jr(x)||2 +||x −r(x)||2\n||(x −r(x))Jr(x)||2 ||x −r(x)||2\nα||(x −r(x))Jr(x)||2 ||x −r(x)||2 +||x −r(x)||2\nFigure 5: The effect of the orthogonal contractive penalty on an idealized example problem.\nThe idealized setting assumes that the Jacobian norm is perfectly anti–correlated with the\nresidual. That is, as the reconstruction functions moves away from the identity function (at\nx−r(x) = 0) towards a principal manifold (at x−r(x) = 1) the increase in error is matched\nby a decrease in the Jacobian. The orthogonal contraction has a minima at both 0 and 1, but\ncombined with the squared residual term, the effect is negligible. Increasing the orthogonal\npenalty creates a minima closer to the principal manifold, however, the identify function\nstill is minimal and has a larger region of attraction. The normalized orthogonal penalty\nactively pushes the solution away from the identity solution and combined with the residual\nterm leads to a minima close to the principal manifold.\nThe method for ﬁnding saddlepoints does not address the issue of discontinuous sad-\ndlepoints if the dimensionality is not restricted. The orthogonal contractive penalty does\ntypically not help to ﬁnd such degenerate solutions and results in the identity solution. This\nbehaviour is expected since moving towards the identity solution often also reduces the\northogonal contractive penalty as illustrated in an idealized setting in Figure 5. To remedy\n9\nthis problem we consider normalizing the contractive penalty to:\nαE\n\"\n(g(λ(X))−X)2 ∇g (λ(X))2\n(g(λ(X))−X)2\n#\n.\n(6)\nThis has the effect that the penalty increases quadratically towards the identity solutions,\nas illustrated in Figure 5. The normalized penalty counterweights the quadratic decrease in\nthe reconstruction error. Figure 6 demonstrates the desired effect of pushing the solution\naway from the identity.\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n(a)\n(b)\n(c)\nFigure 6: The image show the minima found after 20’000 iterations using a network ar-\nchitecture with hidden layers of 50–100–200–100–50, units and (a) 0.04 (b) 0.02 (c) 0.005\nweighted orthogonal penalty. The orthogonal penalty yields reasonable results for a range\nof penalty weights.\nThe orthogonal penalty works well for examples with a co–dimension of one. For\nmore realistic examples, with higher co–dimension, the orthogonal penalty reaches zero\nif the residual vectors are contained in any subspace of the normal bundle. In this case\nthe orthogonal contractive penalty has to be combined with a dimensionality constraint.\nFigure 7 shows the effect on denoising auto–encoding. The orthogonal contractive penalty\npreferentially selects directions that tend towards an orthogonal projection.\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n1.0\n0.5\n0.0\n0.5\n1.0\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n(a)\n(b)\n(c)\nFigure 7: The (a) illustration depicts the effect of orthogonal contraction with denoising\nauto–encoding. In the limit, points from the density (gray area) with noise added, x + ε\n(black dot) are preferentially mapped orthogonal to the contraction (purple) as compared to\ndenoising alone (orange). The (b) orthogonal contraction with denoising leads to sharper\nboundary as compared to (c) denoising alone. This indicates that orthogonal contraction\nwith densoing can yield solutions that generalize better to unseen data than denosing alone.\n10\n4\nConclusion\nThe connection of auto–encoding neural networks with principal manifolds casts regula-\nrization as an approach to ﬁnd saddlepoints of the mean squared error risk. The issue of\nsaddlepoints has been recently explored in the neural network literature. Pascanu et al.\n[2014] develop an optimization strategy to avoid saddlepoints, while Choromanska et al.\n[2015] argue that saddlepoints are desirable solutions in deep learning. The connection to\nprincipal manifolds makes the argument for saddlepoints as desirable solutions explicit in\nthe case of auto–encoding networks.\nWe proposed a new method, gradient–norm minimization for ﬁnding saddlepoints. The\ngradient–norm minimization is potentially converging slowly. Both Newton’s method and\ngradient–norm minimization are gradient descent schemes under scaling. Newton’s method\nscales by the inverse Hessian, while the proposed method corresponds to a scaling by the\nHessian. This scaling increases the condition number and decreases the convergence rate.\nReferences\nG. Alain and Y. Bengio. What regularized auto-encoders learn from the data-generating\ndistribution. The Journal of Machine Learning Research, 15(1):3563–3593, 2014.\nS. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.\nA. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surface\nof multilayer networks. In AI & Statistics (AIStats 2015). arXiv:1412.0233, 2015.\nT. Duchamp and W. Stuetzle. Extremal properties of principal curves in the plane. The\nAnnals of Statistics, 24(4):1511–1520, 1996.\nS. Gerber and R. Whitaker. Regularization-free principal curve estimation. The Journal of\nMachine Learning Research, 14(1):1285–1302, 2013.\nS. Gerber, T. Tasdizen, and R. Whitaker. Dimensionality reduction and principal surfaces\nvia kernel map manifolds. In IEEE 12th International Conference on Computer Vision,\npages 529–536, 2009.\nT. Hastie and W. Stuetzle. Principal curves. Journal of the American Statistical Association,\n84(406):502–516, 1989.\nK. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Informa-\ntion Processing Systems, pages 586–594, 2016.\nB. K´egl, A. Krzyzak, T. Linder, and K. Zeger. Learning and design of principal curves.\nIEEE Transaction On Pattern Analysis Machine Intelligence, 22(3):281–297, 2000.\nR. Pascanu, Y. N. Dauphin, S. Ganguli, and Y. Bengio. On the saddle point problem for\nnon-convex optimization. arXiv preprint arXiv:1405.4604, 2014.\n11\nS. Rifai, G. Mesnil, P. Vincent, X. Muller, Y. Bengio, Y. Dauphin, and X. Glorot.\nHigher order contractive auto-encoder. Machine Learning and Knowledge Discovery\nin Databases, pages 645–660, 2011.\nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol.\nExtracting and composing\nrobust features with denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning, pages 1096–1103. ACM, 2008.\n12\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2021-04-11",
  "updated": "2021-04-11"
}