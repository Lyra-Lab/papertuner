{
  "id": "http://arxiv.org/abs/2104.05224v1",
  "title": "Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation",
  "authors": [
    "Jakob Nyberg",
    "Ramesh Manuvinakurike",
    "Maike Paetzel-Prüsmann"
  ],
  "abstract": "Human ratings are one of the most prevalent methods to evaluate the\nperformance of natural language processing algorithms. Similarly, it is common\nto measure the quality of sentences generated by a natural language generation\nmodel using human raters. In this paper, we argue for exploring the use of\nsubjective evaluations within the process of training language generation\nmodels in a multi-task learning setting. As a case study, we use a\ncrowd-authored dialogue corpus to fine-tune six different language generation\nmodels. Two of these models incorporate multi-task learning and use subjective\nratings of lines as part of an explicit learning goal. A human evaluation of\nthe generated dialogue lines reveals that utterances generated by the\nmulti-tasking models were subjectively rated as the most typical, most moving\nthe conversation forward, and least offensive. Based on these promising first\nresults, we discuss future research directions for incorporating subjective\nhuman evaluations into language model training and to hence keep the human user\nin the loop during the development process.",
  "text": "arXiv:2104.05224v1  [cs.CL]  12 Apr 2021\nEstimating Subjective Crowd-Evaluations as an Additional Objective\nto Improve Natural Language Generation\nJakob Nyberg\nKTH Royal Institute\nof Technology, Sweden\njaknyb@KTH.se\nRamesh Manuvinakurike\nIntel Labs, USA\nramesh.manuvinakurike@\nintel.com\nMaike Paetzel-Pr¨usmann\nUniversity of Potsdam, Germany\npaetzel-pruesmann@\nuni-potsdam.de\nAbstract\nHuman ratings are one of the most prevalent\nmethods to evaluate the performance of natu-\nral language processing algorithms. Similarly,\nit is common to measure the quality of sen-\ntences generated by a natural language gener-\nation model using human raters. In this paper,\nwe argue for exploring the use of subjective\nevaluations within the process of training lan-\nguage generation models in a multi-task learn-\ning setting. As a case study, we use a crowd-\nauthored dialogue corpus to ﬁne-tune six dif-\nferent language generation models.\nTwo of\nthese models incorporate multi-task learning\nand use subjective ratings of lines as part of\nan explicit learning goal. A human evaluation\nof the generated dialogue lines reveals that ut-\nterances generated by the multi-tasking mod-\nels were subjectively rated as the most typ-\nical, most moving the conversation forward,\nand least offensive. Based on these promising\nﬁrst results, we discuss future research direc-\ntions for incorporating subjective human eval-\nuations into language model training and to\nhence keep the human user in the loop during\nthe development process.\n1\nIntroduction\nCreating spoken dialogue systems includes a mul-\ntitude of challenges as they involve various lan-\nguage processing (NLP) components. One such\nimportant component concerns natural language\ngeneration (NLG). Traditionally, the performance\nof a NLG unit has been evaluated using automatic\nmetrics, such as BLEU (Papineni et al., 2002) or\nMETEOR (Banerjee and Lavie, 2005).\nHuman\nevaluations of NLG (i.e., rating autonomously gen-\nerated dialogue responses) is still the most com-\nmon (see Li et al. (2016); Rashkin et al. (2019);\nHashimoto et al. (2019); Zhang et al. (2020) for\nmeasuring the performance of such approaches).\nComparing automatic metrics with human evalu-\nations, however, has shown little correlation be-\ntween the two (Liu et al., 2016; Lowe et al., 2017;\nBelz and Reiter, 2006; Novikova et al., 2017; Re-\niter, 2018), which stresses the importance of us-\ning human evaluations to rate the suitability of a\nsystem or part of a system that will ultimately be\nused by humans again. In recent times, apprecia-\nble advances have been made in developing auto-\nmated metrics showing correlation with the human\nratings (Zhang et al., 2019; Mehri and Eskenazi,\n2020). These approaches, however, do not provide\na method for measuring the affect and emotional\naspects of the generated content which is central\nto our approach.\nDespite human evaluations becoming increas-\ningly prevalent and affordable, they are usually\nonly seen as the ﬁnal stage of the system design\nprocess.\nEvaluations are hence performed after\nconcluding the implementation work and used to\ncompare the new approach to previous models or\ntechniques. The resulting feedback from the users\nis then discarded unless used for future compar-\nisons. In this paper, we argue for keeping the hu-\nman user in the loop by including human evalu-\nations in subsequent natural language generation\nprocesses. To keep the development of such a sys-\ntem at a fast pace and low overhead cost, human\nevaluations can not rely on a few experts but need\nto utilize online crowd-workers.\nWhile crowd-\nsourcing platforms allow us to gather ratings of\nseveral hundred dialogue lines within a few min-\nutes, such evaluations cannot rely on sophisticated\nmetrics requiring a high skill or long training pro-\ncess of the raters but need to utilize subjective rat-\nings of lines instead.\nTo the best of our knowledge, this paper is a ﬁrst\nproof-of-concept exploration to include subjective\nutterance ratings from crowd-workers collected at\na low cost and in a short time during the training\nof a system which is generating responses for a di-\nalogue agent. As a domain, we use the geography-\nthemed cooperative guessing game RDG-Map in\nwhich a human and an embodied conversational\nagent try to identify countries on a world map\n(Paetzel and Manuvinakurike, 2019). To enhance\nthe social component of the dialogue, the human-\nrobot team has a brief chat before and after each\ngame. Ultimately, we aim to increase people’s en-\ngagement playing with the agent by adapting its\nbehavior to the human player. Depending on the\nlearning and playing style of a person, the agent\nshould maximize the team’s performance by either\nencouraging or challenging the team mate during\nplay. As a ﬁrst step, the agent was given two affect\nstates based on Russell (1980) which inﬂuence its\ndialogue behavior: In addition to an indifferent be-\nhavior, utterances could be excited and encourag-\ning or impatient and provocative. Depending on\nthe team’s performance in the game and the hu-\nman responses to the agent, the affect state of the\nagent gradually changes over time.\nInitially, we used a crowd-authoring technique\nto gather potential responses for our dialogue\nagent (Mota et al., 2018). It has previously been\nshown that such crowd-authored content helps\nachieve variety in a dialogue system’s responses\n(Wang et al., 2012; Mitchell et al., 2014; Shah\net al., 2018).\nTo design the system described\nin this paper, we ﬁrst gathered subjective evalua-\ntions by a different set of crowd-workers, rating\nthe dialogue utterances on the dimensions typi-\ncality, offensiveness, and affect.\nWe then used\nthese utterances for training models for neural lan-\nguage generation. We trained six model variations\nto generate responses for different combinations\nof game scenario descriptions and affective state.\nTwo models were trained using multi-task learn-\ning goals, making the estimation of the subjective\naffect rating of the utterance a secondary goal of\nthe model. The main contribution of this paper is\nthe performance analysis of the multi-task models\ntrained on crowd-sourced evaluations compared\nto the models solely tasked with generating dia-\nlogue lines. To compare the different models, they\nwere used to generate utterances for scenarios both\nseen and unseen during training and resulting dia-\nlogue lines were then fed back into the evaluation\nsystem, acquiring the same human evaluations ob-\ntained for the original crowd-authored lines. In\naddition to analyzing differences in subjective rat-\nings of the dialogue lines, we compare the human\nevaluations to the BLEU score as an example of\na traditional automatic metric for evaluating lan-\nguage generation models. We conclude the paper\nby discussing advantages and challenges of our\nhuman-in-the-loop language generation pipeline\nand suggest future work to improve upon and fur-\nther evaluate the suitability of our proposal.\n2\nRelated Work\nThe role of crowd-workers in the development of\nNLG models can be two-folded: Sentences pro-\nvided by crowd-authors can be utilized as a source\nof the surface form of the sentences that the di-\nalogue system needs to generate or as feedback\nabout the performance of the NLG model. Meth-\nods for crowd-sourcing content include: (i) re-\nquesting the users to generate a sentence given\na context (Duˇsek and Jurˇc´ıˇcek, 2016), (ii) asking\nusers to generate surface forms using templates\n(Wang et al., 2012; Mitchell et al., 2014), and (iii)\nshowing the dialogue to crowd-workers and ask-\ning them to paraphrase a given dialogue (Shah\net al., 2018). Utterances collected using these ap-\nproaches have been shown to be diverse and have\nbeen used to train neural NLG models, some of\nwhich have achieved impressive results in recent\ntimes. Another method to utilize crowd-sourcing\nis to request crowd-workers to rate the generated\nsentences on various performance metrics (Deth-\nlefs et al., 2012; Rieser et al., 2014). Recent works\nhave studied utilizing human evaluations to train\nneural models directly (Ziegler et al., 2019). Hu-\nman judgments were shown to be particularly use-\nful for machine learning tasks where the loss func-\ntion for the intended learning goal is difﬁcult to ex-\npress with the data alone. The related work, how-\never, did not focus on dialogue generation but on\nother tasks that are difﬁcult to quantify objectively,\nlike summarization.\nWhile recent prominent neural NLG models\nhave been able to generate human-like sentences,\nthey are not only very large (in terms of the num-\nber of parameters), but also trained on enormous\ndata sets (in terms of the number of training sam-\nples) (Vaswani et al., 2017; Shirish Keskar et al.,\n2019; Radford et al., 2018, 2019; Brown et al.,\n2020; Li, 2020). Such models can respond well\neven in challenging dialogue tasks (Zhang et al.,\n2020; Adiwardana et al., 2020; Huang et al., 2020).\nDue to the hardware and data requirements of\nsuch models, ﬁne-tuning pre-trained models is a\npopular approach for obtaining well-performing\nlanguage generation models (Howard and Ruder,\n2018; Chen et al., 2020; Wolf et al., 2019a; He\net al., 2021). Lack of consistency is one of the\nmajor issues in neural dialogue generation, which\nhas been tackled by methods such as including per-\nsona or situation description to improve the consis-\ntency between generated sentences across multiple\nturns of dialogue. (Zhang et al., 2018; Liu et al.,\n2020; Wolf et al., 2019b).\nIn a similar fashion,\nthe question of how to incorporate information\nthat enables the consistent generation of affective,\nempathetic, or emotional dialogue has been exten-\nsively studied (Zandie and Mahoor, 2020; Shen\nand Feng, 2020; Zhou and Wang, 2018; Qian et al.,\n2018; Lubis et al., 2018; Rashkin et al., 2019; Lin\net al., 2019).\nIn this work, we extend the literature by explor-\ning an approach for developing an NLG pipeline\nusing crowd content and subjective evaluations\nfor a limited corpus of in-domain data.\nFol-\nlowing Pruksachatkun et al. (2020), we leverage\nthe EmpatheticDialogues (ED) corpus by Rashkin\net al. (2019) as an intermediate training step be-\nfore training on the domain-speciﬁc data. We ap-\nply models by Radford et al. (2019) and Zhang\net al. (2020) on the crowd-sourced content and\nhuman evaluations to generate utterances for the\ngiven domain. Like in the works of Wolf et al.\n(2019b), Zandie and Mahoor (2020) and Ziegler\net al. (2019), we use pre-trained models to reduce\nthe amount of hardware and crowd-sourced data\nneeded.\nHowever, we do not use human judg-\nments for reinforcement learning, like (Ziegler\net al., 2019) or (Nguyen et al., 2017), but for su-\npervised learning.\n3\nA Crowd-Sourced Human Evaluation\nPipeline\nOur pipeline to collect crowd-sourced ratings of\ndialogue lines follows the approach described by\nMota et al. (2018) with few alterations. In the ﬁrst\nevaluation stage, a set of untrained crowd-workers\nare asked to judge how typical and ordinary a sen-\ntence is given a situational description and how\noffensive it is on a ﬁve-point Likert scale. They\nare also asked if the utterance is nonsensical, in\nwhich case the relevancy and offensiveness ques-\ntions are skipped.\nThe second evaluation stage\nfocuses on the affect of utterances, and workers\nare asked to judge whether a sentence is excited,\nfrustrated or indifferent.\nIn case they perceived\nthe sentence as excited or frustrated, they need to\nmark the strength of the affect on a scale from 1\n(slightly) to 4 (extremely). For easier computation\ngoing forward, the affect rating is combined into a\nsingle scale ranging from -4 to +4, with negative\nvalues indicating frustration, 0 indicating indiffer-\nence, and positive values indicating excitement.\nThe pipeline runs fully automatically, given a\nset of input utterances. Each new task that is cre-\nated and uploaded to Amazon Mechanical Turk\nconsists of ﬁve utterances and is rated by ﬁve\ndifferent crowd-workers.\nCrowd-workers are al-\nlowed to take multiple tasks in a row, which re-\nsults in a varying level of familiarity with the task\nof individual raters. Once evaluations for the ﬁrst\nand second stage have been performed by ﬁve peo-\nple, their scores are automatically aggregated into\na single average rating per line. Figure 1 shows\na sample evaluation of a line written by a human\ncrowdworker and three language generation mod-\nels for a given scene.\nCrowd-workers were required to be based in the\nUS and have an approval rate of at least 80% to\ntake our HITs. They received $ 0.15 USD per task\nthey completed.\nParticipation was fully anony-\nmous and no personal data was collected. People\nwho responded randomly to our task (see Section\n7 for a discussion) were manually ﬂagged as unre-\nliable. Their ratings were consequently removed\nfrom the result aggregation, and a respective num-\nber of replacement tasks were uploaded.\n4\nModel Implementation and Training\n4.1\nTraining Corpora\nTwo sets of corpora were used in this project: The\nset of utterances collected and rated by crowd-\nworkers speciﬁcally for the RDG-Map game,\nand the EmpatheticDialogues (ED) corpus by\n(Rashkin et al., 2019). EmpatheticDialogues was\nused as an intermediary training step, with some\nmodels being trained for response generation on\nED before being ﬁne-tuned to the RDG-Map data\n(denoted as ED→RDG) to give the models time\nto learn the syntax of the task on a large dataset\nbefore applying them to the small domain-speciﬁc\ncorpus.\nEmpatheticDialogues Corpus\nEmpatheticDia-\nlogues is a corpus which consists of 24850 conver-\nsations that are connected to a textual description\nof a personal experience (Rashkin et al., 2019).\nCrowdworkers were asked to describe a situation\nin which they felt one of 32 given emotions. Two\ncrowdworkers then conversed about their experi-\nence for up to six dialog turns. Unlike the RDG-\nMap data, ED is not evaluated by human raters.\nInstead, the dialogue is assumed to match the des-\nignated situation.\nThe RDG-Map Corpus and Its Crowd-Sourced\nAffective Evaluations\nThe RDG-Map data was\ncollected using the crowd-sourcing process de-\nscribed previously. The aim of the dataset is to\nexpand the original behavior of the dialogue agent\nto make the interactive experience more engaging.\nThe dataset consists of 1512 utterances associated\nwith 61 different scenarios that occur in the RDG-\nMap game and the pre- and post-game social chat.\nEach scenario has a description of the situation\nthe human and robot are currently in and a direc-\ntion for the next utterance to be authored for the\nrobot (cf. Figure 1 for a sample). Each scenario\nincludes three different target affects: The robot is\ndescribed as either excited and encouraging, impa-\ntient and provocative, or indifferent.\nThe RDG-Map corpus resembles ED in its main\ncharacteristics: ED includes situational descrip-\ntions, emotional labels, at least one dialogue line\nper scenario, and comparable data ﬁelds. How-\never, several notable differences exist between the\ntwo corpora: For ED, the emotion label refers\nto an experience rather than the content of the\ndialogue line, and the description of the experi-\nence is narrated in ﬁrst-person instead of the third-\nperson format of the RDG-Map scenarios. More-\nover, the situational descriptions in ED refer to a\nprior event rather than a current situation. Perhaps\nthe most notable difference that for ED, the af-\nfect is recorded as textual emotion labels, whereas\nfor RDG-Map, it is recorded as a value.\nThis\nmeans that in order to perform emotion prediction\non both sets, either the task has to be changed be-\ntween the two sets, or the data has to be converted.\nThis is explained further in Section 4.4.\n4.2\nLanguage Generation Models\nThree variations of pre-trained transformer-based\nresponse generators were trained with the col-\nlected utterances: GPT-2 (Radford et al., 2019),\nDialoGPT (Zhang et al., 2020) and DialoGPT with\nmultitasking (further on referred to as “DialoGPT\nScenario: The human and the robot have ﬁnished\nplaying the game and talked about the game for a\nlittle while. If the robot is excited, how would it\nsay goodbye to the human player?\nHuman: I’ve got to go. Goodbye.\n(Typicality: 3.4, Offensiveness: 1.6, Affect: 0.0)\nRDG: Good to meet you, human. See you around.\n(Typ: 4.2, Off: 1.6, For: 3.8, Aff: -1.0)\nED→RDG: You did so well, you did so so well!\n(Typ: 4.2, Off: 2.2, For: 4.4, Aff: 3.4)\nFigure 1: Responses to a sample scenario, produced\nby a human crowdworker and DialoGPT (MT) trained\nwith different sets of data, with human evaluation\nscores shown underneath. Explanations of scores can\nbe found in Sections 3 and 5.\n(MT)”1) . These three models were in turn trained\nwith two levels of ﬁne-tuning, either being trained\nonly on RDG-Map data or ﬁrst on EmpatheticDia-\nlogues followed by RDG-Map data. This led to a\ntotal of six model variations. Worth noting is that\nGPT-2 and DialoGPT are architecturally the same\nmodel, both being decoder-only transformers but\ntrained on different sets of data. The only architec-\nturally different variant is DialoGPT (MT), which\nadds two parallel output layers.\nAll training was done using the ParlAI frame-\nwork (Miller et al., 2017). Implementations, con-\nﬁgurations, and pre-trained parameters for GPT-2\nand DialoGPT were sourced from HuggingFace’s\nTransformer library (Wolf et al., 2019a). All mod-\nels are “medium” sized models with 24 attention\nlayers, which amounts to about 345 million train-\nable parameters and a vocabulary of 50000 tokens.\n4.3\nDecoder Selection\nWe considered three decoding methods for our lan-\nguage model: greedy decoding, top-k sampling\nand nucleus sampling (Holtzman et al., 2019). Di-\naloGPT (MT), trained with ED→RDG, was used\nto generate utterances with the three decoding\nmethods, since it had the lowest perplexity on the\nevaluation data set. Scenario and affect combina-\ntions were selected in the same way as described in\nSection 5. Five sentences per scenario and affect\nwere generated for top-k and nucleus sampling (to-\n1MT in this scenario refers to “Multitasking”, and not\n“Machine Translation” which is also commonly abbreviated\nas “MT”\ntal: 90) and one utterance per context was evalu-\nated for the greedy decoding (total: 30) since it\nalways generates the same utterance for a given\ncontext.\nEvaluation of utterances were done using the\nquestions described in Section 3, measuring typi-\ncality, offensiveness and affect. A statistical anal-\nysis of the ratings found that top-k decoding pro-\nduced the most typical and least offensive output,\nby a slight margin compared to greedy decoding.\nAffect ratings did not differ signiﬁcantly between\nthe decoding methods. However, top-k produced\nthe widest range of affect, which led us to use it\nfor the main evaluation.\n4.4\nLearning Goals\nFor GPT-2 and DialoGPT without multi-task train-\ning, the only training goal was to predict the\nhuman-written utterance associated with the given\ncontext, i.e., the game situation with the affective\nstate. DialoGPT (MT) also does this, in addition\nto two further training goals that contribute to the\ntotal training loss. To include the affect score from\nthe human evaluations during training, an emotion\nclassiﬁcation task was included following the ex-\nample of Zandie and Mahoor (2020). The classi-\nﬁcation head consists of a single linear layer with\ndropout. The task varied slightly between the two\ndata sets. When training on RDG-Map data, the\nhead estimated the average affective evaluation\nscore of the utterance, which represents how ex-\ncited or frustrated it was perceived as. The eval-\nuation score is a decimal value in the range [-4,\n4]. When training on EmpatheticDialogues, the\nhead classiﬁed the input into one of 32 emotion\ncategories. Because of the different number and\ntypes of emotion labels between EmpatheticDia-\nlogues and the RDG-Map data, the prediction head\ncould not be preserved from one training phase to\nthe next. The layer was thus re-initialized when\nswitching data sets. A potential solution to this\nissue, not implemented in this work, would be to\npredict embedding vectors representing the emo-\ntion content in the input, similar to those in Felbo\net al. (2017).\nFollowing the works of Wolf et al. (2019b) and\nDevlin et al. (2019), next-sentence prediction, or\nmultiple choice, was also used as another learning\nobjective for DialoGPT (MT). The idea of next-\nsentence prediction is to train NLP models to as-\nsociate connected parts of the input, such as one\nturn of dialogue preceding another, to improve the\ncoherence of the generated text. In our implemen-\ntation, the task worked as follows: Along with\nthe true utterance written by a human for a spe-\nciﬁc scenario, a random utterance from another\nscenario was picked.\nThe model was then pre-\nsented with both utterances and tasked with de-\nciding which one is the actual response to the sce-\nnario.\n5\nAnalysis\nThe performance analysis of the two models utiliz-\ning multi-task learning in comparison to the four\nmodels trained with the sole task of generating di-\nalogue lines was based both on automated metrics\nas well as a second round of human evaluations.\nTo get a ﬁrst estimate of how well the models\npredict the RDG-Map data, the average per-token\nperplexities of the models on the test set were\nrecorded. We also calculated the average BLEU\nscore for utterances generated from scenarios in\nthe test set. For each generated utterance, all corre-\nsponding lines written by humans for that speciﬁc\ncombination of scenario and affect were used as\nreferences.\nFor the human evaluation of the different mod-\nels, a set of utterances to be evaluated was gener-\nated. All models used top-k decoding with k = 25.\nSix scenarios (three seen and three unseen dur-\ning training) were used for testing, with three af-\nfect categories each (excited, indifferent, and im-\npatient). Each model generated ﬁve utterances for\neach of the six scenarios with the three affects.\nEach model thus had 90 utterances evaluated, for\na total of 540 utterances across all models.\nThe evaluation pipeline described in Section 3\nwas used to gather human ratings of the utterances\ngenerated by the language models.\nOne addi-\ntional question was added to the ﬁrst stage, asking\ncrowdworkers to rate how much the given utter-\nance moves the conversation forward. 258 work-\ners participated in the evaluation.\nEach worker\nparticipated in 4 different tasks on average, with\na standard deviation of 10 tasks.\n6\nResults\n6.1\nPerformance of Multiple Training Goals\nSince the multitasking model implemented two\nadditional classiﬁers, the accuracy of these were\ntested.\nFor the multiple-choice task, the model\ntrained with ED→RDG picked the correct label\nTable 1: F1 scores on test set (242 utterances) for mul-\ntitasking models.\nData\nExcited\nNeutral\nFrustrated\nED→RDG\n0.96\n0.29\n0.99\nRDG\n0.93\n0.00\n0.96\nwith an accuracy of 82%, whereas the model only\ntrained on RDG-Map data had an accuracy of\n55%.\nTo calculate the accuracy of the emotion estima-\ntion head, the output was rounded to the closest\ninteger between -4 and 4.\nThis makes the out-\nput match the evaluation form shown to crowd\nworkers, where utterances are classiﬁed as either\nexcited, neutral or frustrated.\nThe F1 scores of\nED→RDG model were higher than those of RDG.\nFor both models, the F1 scores for classifying neu-\ntral utterances were lower than for the other labels.\nThis is to be expected given the proportions of the\ntraining data, as utterances evaluated as neutral are\nrare, and those rated as excited are the most fre-\nquent.\n6.2\nEvaluation of the Model Performance\nA two-way ANOVA with the model (DialoGPT,\nDialoGPT (MT) and GPT-2) and the training set\n(ED →RDG, RDG) as independent variable was\nperformed using both the BLEU score and the hu-\nman evaluation as dependent variables.\nAutomated Metrics\nThe data did not show a sig-\nniﬁcant inﬂuence of the model, F(2, 501) = 0.42,\np = .658, or the training data set, F(1, 501) =\n0.16, p = .692, or an interaction effect between\nthe two, F(2, 501) = 0.82, p = .441, on the gen-\nerated lines. The BLEU score of the utterances is,\nhowever, signiﬁcantly positively correlated with\nthe crowdworker rating of typicality, ρ = 0.137,\np = .002, and how much the lines advances the\nconversation, ρ = 0.106, p = .017.\nHuman\nEvaluation\nRatings\nfrom\ncrowd-\nworkers\nshowed\nthat\nboth\nthe\nmodel,\nF(2, 534) = 32.13, p < .001, and the train-\ning data, F(1, 534)\n=\n100.41, p\n<\n.001,\nsigniﬁcantly inﬂuenced how typical and ordinary\nthe generated lines were perceived.\nUsing a\nTukey’s PostHoc test, we found that the Di-\naloGPT (MT) model was rated as the most typical\n(M\n=\n3.27, SD\n=\n0.05) compared to both\nDialoGPT (M = 2.76, SD = 0.05), p < .001, and\nGPT-2 (M = 2.87, SD = 0.06), p < .001. The\ndifference between DialoGPT and GPT-2 was not\nsigniﬁcant, p = .218. There was also a signiﬁcant\ninteraction effect between the model and the\ndata set it was trained on, F(2, 534) = 16.35,\np < .001. A PostHoc test suggests the existence\nof two groups of models that perform almost\nidentical: If any of the models was only trained\non RDG-Map data, the performance between\nmodels was comparable.\nWhen including the\nEmpatheticDialogues data, only DialoGPT (MT)\nreached the same level of performance. DialoGPT\nand GPT-2 trained on ED→RDG both fell in\nthe low-performing group compared to the other\ncombinations.\nA similar result was obtained for the crowd-\nworker rating of how much each line moves the\nconversation forward.\nAgain, both the model,\nF(2, 534) = 9.789, p < .001, and the train-\ning data set, F(1, 534) = 112.515, p < .001,\nhad a signiﬁcant inﬂuence on the ratings.\nDi-\naloGPT (MT) was found to be the model that gen-\nerated the lines advancing the conversation most\n(M = 3.54, SD = 0.04) and the difference\nwas signiﬁcant in comparison to both DialoGPT\n(M = 3.33, SD = 0.04), p < .001, and GPT-2\n(M = 3.35, SD = 0.05), p < .001. The differ-\nence between DialoGPT and GPT-2 was not sig-\nniﬁcant, p = .925.\nUsing only the RDG-Map\ndata set for training (M = 3.64, SD = 0.03)\ngenerated lines that were perceived as advancing\nthe conversation more than when the models were\ntrained on the EmpatheticDialogues data in addi-\ntion (M = 3.18, SD = 0.03).\nAn interaction\neffect between the model and the training data\ncould be observed as well, F(2, 534) = 33.022,\np < .001, which showed a signiﬁcance between\nthe same two groups of well performing (all mod-\nels trained on the RDG-Map data set plus Di-\naloGPT (MT) trained on ED→RDG) and low per-\nforming variations (DialoGPT and GPT-2 trained\non ED→RDG).\nThe model, F(2, 534) = 12.46, p < .001, but\nnot the data set it was trained on, F(1, 534) =\n1.03, p = .31, signiﬁcantly inﬂuenced the rating\nof offensiveness of the utterances that were gen-\nerated. DialoGPT (MT) generated the least offen-\nsive lines (M = 2.43, SD = 0.05) in comparison\nto DialoGPT (M = 2.66, SD = 0.04), p < .001,\nand GPT-2 (M = 2.72, SD = 0.05), p < .001.\nThe ratings between DialoGPT and GPT-2 were\ncomparable, p = .639. The interaction effect be-\nModel\nData\nRating\nMax.\nMin.\nMean\nStd. Dev.\nDialoGPT (MT)\nED→RDG\nExcited\n3.6\n0.2\n1.6\n1.0\nDialoGPT (MT)\nED→RDG\nFrustrated\n3.8\n0.2\n1.5\n1.1\nDialoGPT (MT)\nRDG\nExcited\n3.6\n0.2\n1.2\n0.9\nDialoGPT (MT)\nRDG\nFrustrated\n3\n0.2\n1.0\n0.7\nHuman\nExcited\n3.8\n0.2\n1.5\n0.9\nHuman\nFrustrated\n4\n0.2\n1.4\n0.9\nTable 2: Affective ratings for utterances produced by multitasking model. Human ratings for comparison. Scores\nrange from 0 to 4, with 0 indicating indifference.\ntween the model and the data it was trained on was\nsigniﬁcant again, F(2, 534) = 16.01, p < .001.\nThis time, the best performing models were the\nDialoGPT (MT) trained on both RDG-Map alone\nand the ED→RDG combination, as well as Di-\naloGPT trained on ED→RDG.\nBoth the model, F(2, 534) = 12.548, p < .001,\nand the data set, F(1, 534) = 2.189, p = 0.14,\nhad a signiﬁcant inﬂuence on the affective ratings\nof the lines. DialoGPT (MT) produced lines that\nwere on average rated as more excited and en-\ncouraging, which is signiﬁcant compared to lines\ngenerated by DialoGPT, p < .001, and GPT-2,\np < .001. The DialoGPT (MT) was also the model\nthat generated lines that covered the most diverse\naffect in comparison to the other two. The models\ntrained on the ED→RDG combination were more\nfrustrated and provocative compared to the models\ntrained on the RDG-Map data alone. The combina-\ntion of model and data set was signiﬁcant as well,\nF(2, 534) = 13.224, p < .001. The three mod-\nels rated on the more excited end of the affective\nscale were the two DialoGPT (MT) models and the\nGPT-2 model trained on the RDG-Map data alone.\nThe most impatient lines were generated by GPT-\n2 trained on ED→RDG. A selection of affective\nratings is shown in Table 2.\nComparing Language Models and Crowd-\nAuthors\nEventually, we want to be able to use\nthe language models presented in this paper to gen-\nerate utterances that are comparable in their rating\nto the lines authored by crowd-workers. To un-\nderstand whether our models achieve human-level\nperformance, we combined the model and training\nset into a single independent variable and tested\nit against the ratings given to the crowd-authored\nlines. A one-way ANOVA with a Tukey’s PostHoc\nanalysis indeed showed that the ratings of the lines\ngenerated by all four models in the high perform-\ning group showed no signiﬁcant difference to the\nratings of the human lines, p ≥.948 for all four\nmodels. The two models in the low-performing\ngroup, however, were rated as signiﬁcantly less\ntypical than the lines written by crowd-authors,\np < .001 for both models. The affective rating\nand range of affect between ﬁve out of the six\ncombinations and the human model were compa-\nrable, p > .147 for all models except for GPT-\n2 trained on ED→RDG. This speciﬁc model and\ntraining data combination produced lines that were\non average much more frustrated and provocative\nthan the lines written by crowd-authors, p < .001.\nWhile the typicality of the lines and their affec-\ntive range was comparable, utterances generated\nby all six combinations of model and training data\nwere rated as signiﬁcantly more offensive than the\ncrowd-authored lines, p < .001 for all six mod-\nels. A comparison between DialoGPT (MT) and\nthe crowd-authored lines is summarized in Table\n3. All generated utterances and respective evalua-\ntion scores are available publicly on GitHub2.\n7\nDiscussion & Future Work\nWe trained six variations of neural language gen-\nerators on crowd-sourced content and evaluations.\nOur results suggest that DialoGPT (MT), the\nmodel additionally tasked with predicting the sub-\njective evaluations by crowd-workers, produced\nutterances that were perceived as the most typical,\nleast offensive, and most capable of moving the\nconversation forward. It also generated dialogue\nlines covering the widest range of affects, which\nmeets an important goal for the spoken dialogue\nsystem of the RDG-Map domain. Utterances gen-\nerated by DialoGPT (MT) reach scores compara-\nble to those given to human-authored lines in the\ndimensions relevance and affect for scenarios both\nseen and unseen during training; in real-time and\nat a lower cost than the crowd-sourced approach.\n2https://git.io/JYzq8\nAutomatic\nHuman Subjective Evaluation\nModel\nData\nBLEU\nForwardness\nOffensive\nTypical\nM\nSD\nM\nSD\nM\nSD\nM\nSD\nDialoGPT (MT)\nED→RDG\n0.41\n0.3\n3.6\n0.5\n2.5\n0.6\n3.2\n0.6\nDialoGPT (MT)\nRDG\n0.37\n0.3\n3.5\n0.5\n2.4\n0.6\n3.3\n0.6\nHuman\n-\n-\n-\n-\n1.9\n0.7\n3.4\n0.7\nTable 3: Average BLEU scores and ratings for forwardness (i.e., moving the conversation forward), offensiveness\nand typicality for multitasking model. Human ratings for comparison. Typicality ranges from 0 to 5, with 0\nrepresenting nonsensical content. Offensiveness and Forwardness range from 1 to 5.\nBased on these results, we consider the multitask\nlearning approach a success.\nUtilization of Subjective Ratings\nWhile our re-\nsults are promising when it comes to the success\nof using subjective ratings as a secondary goal in\nmulti-task learning to generate affective dialogue\nlines, further research is necessary to understand\nthe exact inﬂuence of this particular training objec-\ntive. In this work, we added two additional train-\ning goals in order to further utilize the collected\ndata: Multiple choice and emotion classiﬁcation.\nHence, it may be possible that the multiple-choice\ntask was more inﬂuential for the success of the Di-\naloGPT (MT) model. However, in observing the\ntraining process, it was noted that the training loss\nfor the multiple choice task decreased signiﬁcantly\nfaster than the loss of the emotion prediction task.\nThis indicates both that the emotion prediction\ntasks is a more difﬁcult task to train, and that it\nplays a larger role during the optimization as its\nloss term is present during a larger portion of the\ntraining process. While future work is necessary\nto determine the contribution of each task indi-\nvidually, our results show a strong indication that\nthe inclusion of the subjective ratings contributed\nmore to the performance improvements than dis-\ntinguishing between a real or fake response.\nKeeping the Human Rater in the Loop\nIn\nthis proof-of-concept, we only utilized the initial\ncrowd-evaluations of dialogue lines authored by\nother humans for training our NLG models. An\ninteresting topic for future exploration would be\nto further include the second round of evaluations\ncollected for the sentences generated by the NLG\nmodels. We could then envision natural language\ngeneration as an iterative process, deﬁned by a\nnumber of alternating training and evaluation ses-\nsions, where models can be adjusted based on the\nevaluations. This moves the process closer to rein-\nforcement learning, which is a topic that has been\ncovered in previous work (Li et al., 2016; Ziegler\net al., 2019; Nguyen et al., 2017). One of the chal-\nlenges with this approach is ﬁnding a reward func-\ntion which correlates the human evaluations with\nthe content and prevents the model from veering\noff topic, but with the beneﬁt that the model can\nbe trained on only evaluation data going forward.\nAddition of Further Tasks during Training\nGiven the performance improvements offered by\nmultitask learning, a potential subject of future\nwork is to expand the multitasking further and\nincorporate more of the available human evalua-\ntion data.\nThe offensiveness or typicality score\nare present in the data but are currently unused\nduring training. Utterances rated too low in typ-\nicality or too high in offensiveness in the origi-\nnal spoken dialogue system were not included in\nthe agent’s conversational corpus. We chose to in-\nclude rejected lines in the model training data to\npreserve as much of the problem-speciﬁc data as\npossible. Even if an utterance has been rejected as\noffensive, it may still relate to the context, which\nis information that the model theoretically can uti-\nlize. However, we found all our models to gen-\nerate lines signiﬁcantly more offensive than the\noriginal crowd-authored lines. While this ﬁnding\nis in line with related work on DialoGPT, which\nnotes that models trained on large-scale internet\ntext corpora can have issues with producing offen-\nsive content (Zhang et al., 2020; Li, 2020; Bender\net al., 2021), we would still like to limit offensive\ncontent in a dialogue system deployed to converse\nwith people. A potential improvement to the train-\ning procedure would be to remove rejected lines\nfrom training data. Another approach would en-\ntail the inclusion of typicality or offensiveness in\nthe input which could potentially improve perfor-\nmance. Including the scores might also enable a\nmethod of controlling the typicality or offensive-\nness of the output, like the affect might currently\ndo. It would also be prudent to study to what ex-\ntent the designated affect actually inﬂuences the\nactual output.\nCorrelation between Human Evaluations and\nBLEU\nContrary to ﬁndings in the related work,\nwe found the BLEU score of the individual utter-\nances to be signiﬁcantly correlated with the human\nevaluations on typicality and how much the utter-\nances advance the conversation. Liu et al. (2016)\nnote that for constrained domains, the BLEU score\ncorrelates better with human judgements, which\nthe RDG-Map domain might be considered as.\nHowever, no correlation could be found between\nthe subjective rating of offensiveness and the auto-\nmatic metric. This makes sense considering that\nBLEU is a measure of content similarity, and mi-\nnor changes to the content, like an exclamation\nmark, may cause major changes in the perceived\noffensiveness of an utterance.\nFiltering of Evaluations\nOne major issue we\nexperienced in our crowd-evaluation pipeline con-\ncerns the dishonesty of a few crowd-authors who\ndid not pay attention to the task they accepted.\nWhile most participants performed their tasks well,\na few workers providing nonsensical or random\nratings can tilt the results, especially if these work-\ners participate multiple times.\nTo account for\nthis, ﬁlters ﬂagging random answers are necessary.\nThis is complicated by the fact that the questions\nasked in the form are subjective, e.g., how offen-\nsive or typical an utterance is perceived. It is thus\ndifﬁcult to verify if people responding randomly\nas there are no “correct” answers. A method to ad-\ndress the issue at least partially is to include an ob-\njective control question. However, there are chal-\nlenges around the number of such control ques-\ntions to include (Liu et al., 2013) and efﬁciency of\nsuch trap methods (Lloret et al., 2013; Kittur et al.,\n2013) for complex NLP tasks.\nOur method to detect crowd-raters responding\nrandomly was to manually examine the results and\nexclude workers that gave obviously repetitive an-\nswers, e.g. always answering with the same alter-\nnative throughout multiple tasks. This is a simple\nbut ﬂawed method as raters answering in a random\nor less predicable, but still disingenuous, manner\nare not caught through this method. Additionally,\nour method only works with crowd-workers partic-\nipating in several tasks. A measure that is simple\nto enforce is to prevent workers from participating\nmore than once and hence limit the individual in-\nﬂuence of each worker. However, this may lead to\nworkers avoiding the task since it is only proﬁtable\nfor them to engage in repeated tasks, and also the\nloss of workers that give honest evaluations for\nmultiple sessions. A more reﬁned and automated\nmethod of ﬁltering answers would improve the va-\nlidity of the evaluation scores, and thus by proxy\nimprove the training procedure.\n7.1\nEthical Issues Regarding Language\nModels\nThere are several ethical issue with large-scale\nlanguage models worth discussing.\nWe observe\nsome of the issues brought up by Bender et al.\n(2021), the main one being that the output can\neasily be misinterpreted as coherent or intelligent.\nOne should be careful not to over-attribute the writ-\ning capabilities of language models as being equiv-\nalent to that of a human, despite in this case be-\ning rated similarly to human writers. In this sce-\nnario, we tell the raters that a robot produced the\nutterances, which likely inﬂuenced their judgment\nof typicality. A line assumed to be written by a\nmachine might be considered typical even if it is\nvague or contains unusual wording, since the rater\nmay consider the language capabilities of a ma-\nchine to be limited. For future studies into dia-\nlogue generation models, it might be prudent to\ninclude harsher judgements of quality than used in\nthe present work, e.g., asking the raters to judge\nthe sentence as if it was written by a human, or\nwhether it makes sense logically.\nAnother issue brought up by Bender et al.\n(2021) is the possibility of models producing of-\nfensive language.\nWhile we did notice that the\nlines generated by the language models were eval-\nuated as more offensive than the crowd-authored\nlines, a manual review of the dialogue output of\nthe language models did not disclose any slurs or\nexplicitly derogatory statements.\nThe utterance\nconsidered the most offensive was “I hope this\ngame will be your last because it will be your last\ngame for a very long time” which may be inter-\npreted as a threat to the recipient’s life. Other ut-\nterances considered offensive typically involve ac-\ncusations of laziness or the human not being very\ngood at the game, which are meaningful given the\ndomain and the affect description of the agent.\n8\nConclusion\nThe usage of human-annotated data for training\nmachine learning models is an established prac-\ntice. In this work, we propose and evaluate the uti-\nlization of subjective human evaluations for model\ntraining that would otherwise be used merely for\nevaluation.\nOur results suggest that by using\nnot only crowd-sourced content, but also crowd-\nsourced evaluations, we can increase the perfor-\nmance of our models. We hence argue that future\nwork should explore the inclusion of further sub-\njective ratings and the possibility to make model\ngeneration and evaluation an iterative process and\nhence keep the human in the loop during the devel-\nopment process.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R.\nSo, Jamie Hall, Noah Fiedel, Romal Thoppilan,\nZi Yang, Apoorv Kulshreshtha, Gaurav Nemade,\nYifeng Lu, and Quoc V. Le. 2020. Towards a Hu-\nman-like Open-Domain Chatbot.\narXiv e-prints,\npage arXiv:2001.09977.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics.\nAnja Belz and Ehud Reiter. 2006.\nComparing auto-\nmatic and human evaluation of NLG systems.\nIn\n11th Conference of the European Chapter of the\nAssociation for Computational Linguistics, Trento,\nItaly. Association for Computational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021.\nOn the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, New York, NY, USA. As-\nsociation for Computing Machinery.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell,\nSandhini Agarwal,\nAriel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nYen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu,\nand Jingjing Liu. 2020.\nDistilling knowledge\nlearned in bert for text generation. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7893–7905.\nNina Dethlefs, Helen Hastie, Verena Rieser, and Oliver\nLemon. 2012. Optimising incremental dialogue de-\ncisions using information density for interactive sys-\ntems. In Proceedings of the 2012 Joint Conference\non Empirical Methods in Natural Language Process-\ning and Computational Natural Language Learning,\npages 82–93.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nOndˇrej Duˇsek and Filip Jurˇc´ıˇcek. 2016.\nA context-\naware natural language generator for dialogue sys-\ntems. arXiv preprint arXiv:1608.07076.\nBjarke Felbo, Alan Mislove, Anders Søgaard, Iyad\nRahwan, and Sune Lehmann. 2017. Using millions\nof emoji occurrences to learn any-domain represen-\ntations for detecting sentiment, emotion and sarcasm.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1615–1625, Copenhagen, Denmark. Association for\nComputational Linguistics.\nTatsunori Hashimoto, Hugh Zhang, and Percy Liang.\n2019. Unifying human and statistical evaluation for\nnatural language generation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1689–1701.\nTianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing\nLiu, James Glass, and Fuchun Peng. 2021. Analyz-\ning the forgetting problem in the pretrain-ﬁnetuning\nof dialogue response models. In EACL 2021-16th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339.\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020.\nChallenges in building intelligent open-domain dia-\nlog systems. ACM Transactions on Information Sys-\ntems (TOIS), 38(3):1–32.\nAniket Kittur, Jeffrey V Nickerson, Michael Bernstein,\nElizabeth Gerber, Aaron Shaw, John Zimmerman,\nMatt Lease, and John Horton. 2013.\nThe future\nof crowd work.\nIn Proceedings of the 2013 con-\nference on Computer supported cooperative work,\npages 1301–1318.\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,\nMichel Galley, and Jianfeng Gao. 2016. Deep rein-\nforcement learning for dialogue generation. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1192–\n1202, Austin, Texas. Association for Computational\nLinguistics.\nPiji Li. 2020.\nAn Empirical Investigation of Pre–\nTrained Transformer Language Models for Open–\nDomain Dialogue Generation. arXiv e-prints, page\narXiv:2003.04195.\nZhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu,\nand Pascale Fung. 2019. MoEL: Mixture of empa-\nthetic listeners.\nIn Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 121–132, Hong Kong, China. As-\nsociation for Computational Linguistics.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow NOT to evaluate your dialogue system: An\nempirical study of unsupervised evaluation metrics\nfor dialogue response generation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2122–2132, Austin,\nTexas. Association for Computational Linguistics.\nQian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou,\nZixuan Chen, Bin Zhou, and Dongmei Zhang. 2020.\nYou impress me: Dialogue generation via mutual\npersona perception. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1417–1427, Online. Association\nfor Computational Linguistics.\nQiang Liu, Alexander T Ihler, and Mark Steyvers. 2013.\nScoring workers in crowdsourcing: How many con-\ntrol questions are enough? Advances in neural infor-\nmation processing systems, 26:1914–1922.\nElena Lloret, Laura Plaza, and Ahmet Aker. 2013. Ana-\nlyzing the capabilities of crowdsourcing services for\ntext summarization. Language resources and evalu-\nation, 47(2):337–369.\nRyan Lowe, Michael Noseworthy, Iulian Vlad Ser-\nban, Nicolas Angelard-Gontier, Yoshua Bengio, and\nJoelle Pineau. 2017.\nTowards an automatic Tur-\ning test: Learning to evaluate dialogue responses.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1:\nLong Papers), pages 1116–1126, Vancouver,\nCanada. Association for Computational Linguistics.\nNurul Lubis, Sakriani Sakti, Koichiro Yoshino, and\nSatoshi Nakamura. 2018.\nOptimizing neural re-\nsponse generator with emotional impact information.\nIn 2018 IEEE Spoken Language Technology Work-\nshop (SLT), pages 876–883. IEEE.\nShikib Mehri and Maxine Eskenazi. 2020. Usr: An un-\nsupervised and reference free evaluation metric for\ndialog generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 681–707.\nAlexander Miller, Will Feng, Dhruv Batra, Antoine\nBordes, Adam Fisch, Jiasen Lu, Devi Parikh, and\nJason Weston. 2017. ParlAI: A dialog research soft-\nware platform.\nIn Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 79–84,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nMargaret Mitchell, Dan Bohus, and Ece Kamar. 2014.\nCrowdsourcing language generation templates for\ndialogue systems. In Proceedings of the INLG and\nSIGDIAL 2014 Joint Session, pages 172–180.\nPedro Mota, Maike Paetzel, Andrea Fox, Aida Amini,\nSiddarth Srinivasan, and James Kennedy. 2018. Ex-\npressing coherent personality with incremental ac-\nquisition of multimodal behaviors.\nIn 2018 27th\nIEEE International Symposium on Robot and Hu-\nman Interactive Communication (RO-MAN), pages\n396–403. IEEE.\nKhanh Nguyen, Hal Daum´e III, and Jordan Boyd-\nGraber. 2017.\nReinforcement learning for bandit\nneural machine translation with simulated human\nfeedback. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1464–1474, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nJekaterina Novikova, Ondˇrej Duˇsek, Amanda Cer-\ncas Curry, and Verena Rieser. 2017. Why we need\nnew evaluation metrics for NLG.\nIn Proceedings\nof the 2017 Conference on Empirical Methods in\nNatural Language Processing, pages 2241–2252,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nMaike Paetzel and Ramesh Manuvinakurike. 2019.\n“can you say more about the location?” the devel-\nopment of a pedagogical reference resolution agent.\nIn Dialog for Good - Workshop on Speech and Lan-\nguage Technology Serving Society (DiGo).\nKishore Papineni, S. Roukos, T. Ward, and Wei-Jing\nZhu. 2002. Bleu: a method for automatic evaluation\nof machine translation. In ACL.\nYada Pruksachatkun,\nJason Phang,\nHaokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\nPang, Clara Vania, Katharina Kann, and Samuel R.\nBowman. 2020. Intermediate-task transfer learning\nwith pretrained language models: When and why\ndoes it work?\nIn Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5231–5247, Online. Association for\nComputational Linguistics.\nQiao Qian, Minlie Huang, Haizhou Zhao, Jingfang\nXu, and Xiaoyan Zhu. 2018. Assigning personal-\nity/proﬁle to a chatting machine for coherent conver-\nsation generation. In IJCAI, pages 4279–4285.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018.\nImproving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-do-\nmain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 5370–5381, Florence, Italy. Association\nfor Computational Linguistics.\nEhud Reiter. 2018. A structured review of the validity\nof BLEU.\nComputational Linguistics, 44(3):393–\n401.\nVerena Rieser, Oliver Lemon, and Simon Keizer. 2014.\nNatural language generation as incremental plan-\nning under uncertainty: Adaptive information pre-\nsentation for statistical dialogue systems.\nACM\nTransactions on Speech and Language Processing,\n22(5):979–994.\nJames A Russell. 1980.\nA circumplex model of af-\nfect. Journal of personality and social psychology,\n39(6):1161.\nPararth Shah, Dilek Hakkani-Tur, Bing Liu, and\nGokhan Tur. 2018. Bootstrapping a neural conversa-\ntional agent with dialogue self-play, crowdsourcing\nand on-line reinforcement learning. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3 (Industry\nPapers), pages 41–51.\nLei Shen and Yang Feng. 2020.\nCdl: Curriculum\ndual learning for emotion-controllable response gen-\neration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1808–1822.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCTRL: A Conditional Transformer Language Model\nfor Controllable Generation.\narXiv e-prints, page\narXiv:1909.05858.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nWilliam Yang Wang, Dan Bohus, Ece Kamar, and Eric\nHorvitz. 2012.\nCrowdsourcing the acquisition of\nnatural language corpora: Methods and observations.\nIn 2012 IEEE Spoken Language Technology Work-\nshop (SLT), pages 73–78. IEEE.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019a. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019b. Transfertransfo: A trans-\nfer learning approach for neural network based con-\nversational agents. CoRR, abs/1901.08149.\nRohola Zandie and Mohammad H. Mahoor. 2020.\nEmptransfo: A multi-head transformer architecture\nfor creating empathetic dialog systems. In FLAIRS.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too?\nIn Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2204–\n2213, Melbourne, Australia. Association for Com-\nputational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\nIn International\nConference on Learning Representations.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. In ACL, system demonstration.\nXianda Zhou and William Yang Wang. 2018. Mojitalk:\nGenerating emotional responses at scale.\nIn Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1128–1137.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-Tuning Lan-\nguage Models from Human Preferences. arXiv e-\nprints, page arXiv:1909.08593.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-04-12",
  "updated": "2021-04-12"
}