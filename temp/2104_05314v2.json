{
  "id": "http://arxiv.org/abs/2104.05314v2",
  "title": "Machine learning and deep learning",
  "authors": [
    "Christian Janiesch",
    "Patrick Zschech",
    "Kai Heinrich"
  ],
  "abstract": "Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.",
  "text": "FUNDAMENTALS\nMachine learning and deep learning\nChristian Janiesch1\n& Patrick Zschech2\n& Kai Heinrich3\nReceived: 7 October 2020 /Accepted: 19 March 2021\n# The Author(s) 2021\nAbstract\nToday, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes\nthe capacity of systems to learn from problem-specific training data to automate the process of analytical model building and\nsolve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications,\ndeep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we\nsummarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical\nunderpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss\nthe challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business.\nThese naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence\nservitization.\nKeywords Machine learning . Deep learning . Artificial intelligence . Artificial neural networks . Analytical model building\nJEL classification C6 . C8 . M15 . O3\nIntroduction\nIt is considered easier to explain to a child the nature of what\nconstitutes a sports car as opposed to a normal car by showing\nhim or her examples, rather than trying to formulate explicit\nrules that define a sports car.\nSimilarly, instead of codifying knowledge into computers,\nmachine learning (ML) seeks to automatically learn\nmeaningful relationships and patterns from examples and ob-\nservations (Bishop 2006). Advances in ML have enabled the\nrecent rise of intelligent systems with human-like cognitive\ncapacity that penetrate our business and personal life and\nshape the networked interactions on electronic markets in ev-\nery conceivable way, with companies augmenting decision-\nmaking for productivity, engagement, and employee retention\n(Shrestha et al. 2021), trainable assistant systems adapting to\nindividual user preferences (Fischer et al. 2020), and trading\nagents shaking traditional finance trading markets (Jayanth\nBalaji et al. 2018).\nThe capacity of such systems for advanced problem solv-\ning, generally termed artificial intelligence (AI), is based on\nanalytical models that generate predictions, rules, answers,\nrecommendations, or similar outcomes. First attempts to build\nanalytical models relied on explicitly programming known\nrelationships, procedures, and decision logic into intelligent\nsystems through handcrafted rules (e.g., expert systems for\nmedical diagnoses) (Russell and Norvig 2021). Fueled by\nthe practicability of new programming frameworks, data\navailability, and the broad access to necessary computing\npower, analytical models are nowadays increasingly built\nusing what is generally referred to as ML (Brynjolfsson and\nMcAfee 2017; Goodfellow et al. 2016). ML relieves the hu-\nman of the burden to explicate and formalize his or her\nResponsible Editor: Fabio Lobato\n* Christian Janiesch\nchristian.janiesch@uni-wuerzburg.de\nPatrick Zschech\npatrick.zschech@fau.de\nKai Heinrich\nkai.heinrich@ovgu.de\n1\nFaculty of Business Management & Economics, University of\nWürzburg, Sanderring 2, 97070 Würzburg, Germany\n2\nInstitute of Information Systems, Friedrich-Alexander University\nErlangen-Nürnberg, Lange Gasse 20, 90403 Nürnberg, Germany\n3\nFaculty of Economics and Management,\nOtto-von-Guericke-Universität Magdeburg, Universitätsplatz 2,\n39106 Magdeburg, Germany\nElectronic Markets\nhttps://doi.org/10.1007/s12525-021-00475-2\nknowledge into a machine-accessible form and allows to de-\nvelop intelligent systems more efficiently.\nDuring the last decades, the field of ML has brought forth a\nvariety of remarkable advancements in sophisticated learning\nalgorithms and efficient pre-processing techniques. One of\nthese advancements was the evolution of artificial neural net-\nworks (ANNs) towards increasingly deep neural network ar-\nchitectures with improved learning capabilities summarized as\ndeep learning (DL) (Goodfellow et al. 2016; LeCun et al.\n2015). For specific applications in closed environments, DL\nalready shows superhuman performance by excelling human\ncapabilities (Madani et al. 2018; Silver et al. 2018). However,\nsuch benefits also come at a price as there are several chal-\nlenges to overcome for successfully implementing analytical\nmodels in real business settings. These include the suitable\nchoice from manifold implementation options, bias and drift\nin data, the mitigation of black-box properties, and the reuse of\npreconfigured models (as a service).\nBeyond its hyped appearance, scholars, as well as profes-\nsionals, require a solid understanding of the underlying con-\ncepts, processes as well as challenges for implementing such\ntechnology. Against this background, the goal of this article is\nto convey a fundamental understanding of ML and DL in the\ncontext of electronic markets. In this way, the community can\nbenefit from these technological achievements – be it for the\npurpose of examining large and high-dimensional data assets\ncollected in digital ecosystems or for the sake of designing\nnovel intelligent systems for electronic markets. Following\nrecent advances in the field, this article focuses on analytical\nmodel building and challenges of implementing intelligent\nsystems based on ML and DL. As we examine the field from\na technical perspective, we do not elaborate on the related\nissues of AI technology adoption, policy, and impact on orga-\nnizational culture (for further implications cf. e.g. Stone et al.\n2016).\nIn the next section, we provide a conceptual distinction\nbetween relevant terms and concepts. Subsequently, we shed\nlight on the process of automated analytical model building\nby highlighting the particularities of ML and DL. Then, we\nproceed to discuss several induced challenges when\nimplementing intelligent systems within organizations or\nelectronic markets. In doing so, we highlight environmental\nfactors of implementation and application rather than view-\ning the engineered system itself as the only unit of observa-\ntion. We summarize the article with a brief conclusion.\nConceptual distinction\nTo provide a fundamental understanding of the field, it is\nnecessary to distinguish several relevant terms and concepts\nfrom each other. For this purpose, we first present basic foun-\ndations of AI, before we distinguish i) machine learning\nalgorithms, ii) artificial neural networks, and iii) deep neural\nnetworks. The hierarchical relationship between those terms is\nsummarized in Venn diagram of Fig. 1.\nBroadly defined, AI comprises any technique that enables\ncomputers to mimic human behavior and reproduce or excel\nover human decision-making to solve complex tasks inde-\npendently or with minimal human intervention (Russell and\nNorvig 2021). As such, it is concerned with a variety of\ncentral problems, including knowledge representation, rea-\nsoning, learning, planning, perception, and communication,\nand refers to a variety of tools and methods (e.g., case-based\nreasoning, rule-based systems, genetic algorithms, fuzzy\nmodels, multi-agent systems) (Chen et al. 2008). Early AI\nresearch focused primarily on hard-coded statements in for-\nmal languages, which a computer can then automatically\nreason about based on logical inference rules. This is also\nknown as the knowledge base approach (Goodfellow et al.\n2016). However, the paradigm faces several limitations as\nhumans generally struggle to explicate all their tacit knowl-\nedge that is required to perform complex tasks (Brynjolfsson\nand McAfee 2017).\nMachine learning overcomes such limitations. Generally\nspeaking, ML means that a computer program’s performance\nimproves with experience with respect to some class of tasks\nand performance measures (Jordan and Mitchell 2015). As\nsuch, it aims at automating the task of analytical model build-\ning to perform cognitive tasks like object detection or natural\nlanguage translation. This is achieved by applying algorithms\nthat iteratively learn from problem-specific training data,\nwhich allows computers to find hidden insights and complex\npatterns without explicitly being programmed (Bishop 2006).\nEspecially in tasks related to high-dimensional data such as\nclassification, regression, and clustering, ML shows good ap-\nplicability. By learning from previous computations and\nextracting regularities from massive databases, it can help to\nproduce reliable and repeatable decisions. For this reason, ML\nalgorithms have been successfully applied in many areas, such\nas fraud detection, credit scoring, next-best offer analysis,\nspeech and image recognition, or natural language processing\n(NLP).\nBased on the given problem and the available data, we can\ndistinguish three types of ML: supervised learning, unsuper-\nvised learning, and reinforcement learning. While many ap-\nplications in electronic markets use supervised learning\n(Brynjolfsson and McAfee 2017), for example, to forecast\nstock markets (Jayanth Balaji et al. 2018), to understand cus-\ntomer perceptions (Ramaswamy and DeClerck 2018), to ana-\nlyze customer needs (Kühl et al. 2020), or to search products\n(Bastan et al. 2020), there are implementations of all types, for\nexample, market-making with reinforcement learning\n(Spooner et al. 2018) or unsupervised market segmentation\nusing customer reviews (Ahani et al. 2019). See Table 1 for\nan overview of all three types.\nC. Janiesch et al.\nDepending on the learning task, the field offers various\nclasses of ML algorithms, each of them coming in multiple\nspecifications and variants, including regressions models,\ninstance-based algorithms, decision trees, Bayesian methods,\nand ANNs.\nThe family of artificial neural networks is of particular\ninterest since their flexible structure allows them to be modi-\nfied for a wide variety of contexts across all three types of ML.\nInspired by the principle of information processing in biolog-\nical systems, ANNs consist of mathematical representations of\nconnected processing units called artificial neurons. Like syn-\napses in a brain, each connection between neurons transmits\nsignals whose strength can be amplified or attenuated by a\nweight that is continuously adjusted during the learning pro-\ncess. Signals are only processed by subsequent neurons if a\ncertain threshold is exceeded as determined by an activation\nfunction. Typically, neurons are organized into networks with\ndifferent layers. An input layer usually receives the data input\n(e.g., product images of an online shop), and an output layer\nproduces the ultimate result (e.g., categorization of products).\nIn between, there are zero or more hidden layers that are re-\nsponsible for learning a non-linear mapping between input\nand output (Bishop 2006; Goodfellow et al. 2016). The num-\nber of layers and neurons, among other property choices, such\nas learning rate or activation function, cannot be learned by\nthe learning algorithm. They constitute a model’s\nhyperparameters and must be set manually or determined by\nan optimization routine.\nDeep neural networks typically consist of more than one\nhidden layer, organized in deeply nested network architec-\ntures. Furthermore, they usually contain advanced neurons\nin contrast to simple ANNs. That is, they may use advanced\noperations (e.g., convolutions) or multiple activations in one\nneuron rather than using a simple activation function. These\ncharacteristics allow deep neural networks to be fed with raw\ninput data and automatically discover a representation that is\nArtificial neural networks\nDeep neural networks\ne.g., support vector machine, decision tree, k-nearest neighbors, … \ne.g., shallow autoencoders, …\ne.g., convolutional neural networks, \nrecurrent neural networks, …\nDeep\nlearning\nShallow\nmachine\nlearning\nMachine\nlearning\nMachine learning algorithms\nFig. 1 Venn diagram of machine\nlearning concepts and classes\n(inspired by Goodfellow et al.\n2016, p. 9)\nTable 1\nOverview of types of machine learning\nType\nDescription\nSupervised learning Supervised learning requires a training dataset that covers examples for the input as well as labeled answers or target values for the\noutput. An example could be the prediction of active users subscribed to a market platform in a month’s time as output\n(considered as the target variable or y variable) based on different input characteristics, such as the number of sold products or\npositive user reviews (often referred to as input features or x variables). The pairs of input and output data in the training set are\nthen used to calibrate the open parameters of the ML model. Once the model has been successfully trained, it can be used to\npredict the target variable y given new or unseen data points of the input features x. Regarding the type of supervised learning,\nwe can further distinguish between regression problems, where a numeric value is predicted (e.g., number of users), and\nclassification problems, where the prediction result is a categorical class affiliation such as “lookers” or “buyers”.\nUnsupervised\nlearning\nUnsupervised learning takes place when the learning system is supposed to detect patterns without any pre-existing labels or\nspecifications. Thus, training data only consists of variables x with the goal of finding structural information of interest, such as\ngroups of elements that share common properties (known as clustering) or data representations that are projected from a\nhigh-dimensional space into a lower one (known as dimensionality reduction) (Bishop 2006). A prominent example of\nunsupervised learning in electronic markets is applying clustering techniques to group customers or markets into segments for\nthe purpose of a more target-group specific communication.\nReinforcement\nlearning\nIn a reinforcement learning system, instead of providing input and output pairs, we describe the current state of the system, specify\na goal, provide a list of allowable actions and their environmental constraints for their outcomes, and let the ML model\nexperience the process of achieving the goal by itself using the principle of trial and error to maximize a reward. Reinforcement\nlearning models have been applied with great success in closed world environments such as games (Silver et al. 2018), but they\nare also relevant for multi-agent systems such as electronic markets (Peters et al. 2013).\nMachine learning and deep learning\nneeded for the corresponding learning task. This is the net-\nworks’ core capability, which is commonly known as deep\nlearning. Simple ANNs (e.g., shallow autoencoders) and oth-\ner ML algorithms (e.g., decision trees) can be subsumed under\nthe term shallow machine learning since they do not provide\nsuch functionalities. As there is still no exact demarcation\nbetween the two concepts in literature (see also\nSchmidhuber 2015), we use a dashed line in Fig. 1. While\nsome shallow ML algorithms are considered inherently inter-\npretable by humans and, thus, white boxes, the decision mak-\ning of most advanced ML algorithms is per se untraceable\nunless explained otherwise and, thus, constitutes a black box.\nDL is particularly useful in domains with large and high-\ndimensional data, which is why deep neural networks outper-\nform shallow ML algorithms for most applications in which\ntext, image, video, speech, and audio data needs to be proc-\nessed (LeCun et al. 2015). However, for low-dimensional data\ninput, especially in cases of limited training data availability,\nshallow ML can still produce superior results (Zhang and Ling\n2018), which even tend to be better interpretable than those\ngenerated by deep neural networks (Rudin 2019). Further,\nwhile DL performance can be superhuman, problems that re-\nquire strong AI capabilities such as literal understanding and\nintentionality still cannot be solved as pointedly outlined in\nSearle (1980)'s Chinese room argument.\nProcess of analytical model building\nIn this section, we provide a framework on the process of\nanalytical model building for explicit programming, shallow\nML, and DL as they constitute three distinct concepts to build\nan analytical model. Due to their importance for electronic\nmarkets, we focus the subsequent discussion on the related\naspects of data input, feature extraction, model building, and\nmodel assessment of shallow ML and DL (cf. Figure 2). With\nexplicit programming, feature extraction and model building\nare performed manually by a human when handcrafting rules\nto specify the analytical model.\nData input\nElectronic markets have different stakeholder touchpoints,\nsuch as websites, apps, and social media platforms. Apart\nfrom common numerical data, they generate a vast amount\nof versatile data, in particular unstructured and non-cross-\nsectional data such as time series, image, and text. This data\ncan be exploited for analytical model building towards better\ndecision support or business automation purposes. However,\nextracting patterns and relationships by hand would exceed\nthe cognitive capacity of human operators, which is why\nalgorithmic support is indispensable when dealing with large\nand high-dimensional data.\nTime series data implies a sequential dependency and pat-\nterns over time that need to be detected to form forecasts, often\nresulting in regression problems or trend classification tasks.\nTypical examples involve forecasting financial markets or\npredicting process behavior (Heinrich et al. 2021). Image data\nis often encountered in the context of object recognition or\nobject counting with fields of application ranging from crop\ndetection for yield prediction to autonomous driving\n(Grigorescu et al. 2020). Text data is present when analyzing\nlarge volumes of documents such as corporate e-mails or so-\ncial media posts. Example applications are sentiment analysis\nor machine-based translation and summarization of docu-\nments (Young et al. 2018).\nRecent advancements in DL allow for processing data of\ndifferent types in combination, often referred to as cross-\nmodal learning. This is useful in applications where content\nis subject to multiple forms of representation, such as e-\ncommerce websites where product information is commonly\nrepresented by images, brief descriptions, and other comple-\nmentary text metadata. Once such cross-modal representa-\ntions are learned, they can be used, for example, to improve\nretrieval and recommendation tasks or to detect misinforma-\ntion and fraud (Bastan et al. 2020).\nFeature extraction\nAn important step for the automated identification of patterns\nand relationships from large data assets is the extraction of\nfeatures that can be exploited for model building. In general,\na feature describes a property derived from the raw data input\nwith the purpose of providing a suitable representation. Thus,\nfeature extraction aims to preserve discriminatory information\nand separate factors of variation relevant to the overall learn-\ning task (Goodfellow et al. 2016). For example, when classi-\nfying the helpfulness of customer reviews of an online-shop,\nuseful feature candidates could be the choice of words, the\nlength of the review, and the syntactical properties of the text.\nShallow ML heavily relies on such well-defined features,\nand therefore its performance is dependent on a successful\nextraction process. Multiple feature extraction techniques\nhave emerged over time that are applicable to different types\nof data. For example, when analyzing time-series data, it is\ncommon to apply techniques to extract time-domain features\n(e.g., mean, range, skewness) and frequency-domain features\n(e.g., frequency bands) (Goyal and Pabla 2015); for image\nanalysis, suitable approaches include histograms of oriented\ngradients (HOG) (Dalal and Triggs 2005), scale-invariant fea-\nture transform (SIFT) (Lowe 2004), and the Viola-Jones\nmethod (Viola and Jones 2001); and in NLP, it is common\nto use term frequency-inverse document frequency (TF-IDF)\nC. Janiesch et al.\nvectors (Salton and Buckley 1988), part-of-speech (POS) tag-\nging, and word shape features (Wu et al. 2018). Manual fea-\nture design is a tedious task as it usually requires a lot of\ndomain expertise within an application-specific engineering\nprocess. For this reason, it is considered time-consuming, la-\nbor-intensive, and inflexible.\nDeep neural networks overcome this limitation of\nhandcrafted feature engineering. Their advanced architecture\ngives them the capability of automated feature learning to\nextract discriminative feature representations with minimal\nhuman effort. For this reason, DL better copes with large-\nscale, noisy, and unstructured data. The process of feature\nlearning generally proceeds in a hierarchical manner, with\nhigh-level abstract features being assembled by simpler ones.\nNevertheless, depending on the type of data and the choice of\nDL architecture, there are different mechanisms of feature\nlearning in conjunction with the step of model building.\nModel building\nDuring automated model building, the input is used by a learn-\ning algorithm to identify patterns and relationships that are\nrelevant for the respective learning task. As described above,\nshallow ML requires well-designed features for this task. On\nthis basis, each family of learning algorithms applies different\nmechanisms for analytical model building. For example,\nwhen building a classification model, decision tree algorithms\nexploit the features space by incrementally splitting data re-\ncords into increasingly homogenous partitions following a\nhierarchical, tree-like structure. A support vector machine\n(SVM) seeks to construct a discriminatory hyperplane be-\ntween data points of different classes where the input data is\noften projected into a higher-dimensional feature space for\nbetter separability. These examples demonstrate that there\nare different ways of analytical model building, each of them\nwith individual advantages and disadvantages depending on\nthe input data and the derived features (Kotsiantis et al. 2006).\nBy contrast, DL can directly operate on high-dimensional\nraw input data to perform the task of model building with its\ncapability of automated feature learning. Therefore, DL archi-\ntectures are often organized as end-to-end systems combining\nboth aspects in one pipeline. However, DL can also be applied\nonly for extracting a feature representation, which is subse-\nquently fed into other learning subsystems to exploit the\nstrengths of competing ML algorithms, such as decision trees\nor SVMs.\nVarious DL architectures have emerged over time (Leijnen\nand van Veen 2020; Pouyanfar et al. 2019; Young et al. 2018).\nAlthough basically every architecture can be used for every\ntask, some architectures are more suited for specific data such\nas time series or images. Architectural variants are mostly\ncharacterized by the types of layers, neural units, and connec-\ntions they use. Table 2 summarizes the five groups of\nconvolutional neural networks (CNNs), recurrent neural net-\nworks (RNNs), distributed representations, autoencoders, and\ngenerative adversarial neural networks (GANs). They provide\npromising applications in the field of electronic markets.\nModel assessment\nFor the assessment of a model’s quality, multiple aspects have\nto be taken into account, such as performance, computational\nresources, and interpretability. Performance-based metrics\nevaluate how well a model satisfies the objective specified\nby the learning task. In the area of supervised learning, there\nare well-established guidelines for this purpose. Here, it is\ncommon practice to use k-fold cross-validation to prevent a\nmodel from overfitting and determine its performance on out-\nof-sample data that was not included in the training samples.\nCross-validation provides the opportunity to compare the re-\nliability of ML models by providing multiple out-of-sample\ndata instances that enable comparative statistical testing\n(García and Herrera 2008). Regression models are evaluated\nby measuring estimation errors such as the root mean square\nerror (RMSE) or the mean absolute percentage error (MAPE),\nInput\nInput\nInput\nHandcrafted model building\nOutput\nOutput\nExplicit \nprogramming\nShallow\nmachine\nlearning\nDeep\nlearning\nHandcrafted\nfeature engineering\nAutomated\nmodel building\nFeature learning + automated model building\nOutput\nData input\nFeature extraction\nModel building\nModel assessment\nFig. 2 Process of analytical\nmodel building (inspired by\nGoodfellow et al. 2016, p. 10)\nMachine learning and deep learning\nwhereas classification models are assessed by calculating dif-\nferent ratios of correctly and incorrectly predicted instances,\nsuch as accuracy, recall, precision, and F1 score. Furthermore,\nit is common to apply cost-sensitive measures such as average\ncost per predicted observation, which is helpful in situations\nwhere prediction errors are associated with asymmetric cost\nstructures (Shmueli and Koppius 2011). That is the case, for\nexample, when analyzing transactions in financial markets,\nand the costs of failing to detect a fraudulent transaction are\nremarkably higher than the costs of incorrectly classifying a\nnon-fraudulent transaction.\nTo identify a suitable prediction model for a specific task, it\nis reasonable to compare alternative models of varying com-\nplexities, that is, considering competing model classes as well\nas alternative variants of the same model class. As introduced\nabove, a model’s complexity can be characterized by several\nTable 2\nOverview of deep learning architectures\nArchitecture\nDescription\nConvolutional neural network\n(CNN)\nCNNs are mainly applied for tasks related to computer vision and speech recognition. They are able to address tasks\ninvolving datasets with spatial relationships, where the columns and rows are not interchangeable (e.g., image\ndata). Their network architecture comprises a series of stages that allow hierarchical feature learning as\ndetermined by the respective modeling task. For example, when considering object recognition in images, the\nfirst few layers of the network are responsible for extracting basic features in the form of edges and corners.\nThese are then incrementally aggregated into more complex features in the last few layers resembling the actual\nobjects of interest, such as animals, houses, or cars. Subsequently, the auto-generated features are used for\nprediction purposes to recognize objects of interest in new images (Goodfellow et al. 2016).\nRecurrent neural network (RNN)\nRNNs are designed explicitly for sequential data structures such as time-series data, event sequences, and natural\nlanguage. Their architecture offers internal feedback loops and therefore enables sequential pattern learning to\nmodel time dependencies by forming a memory. Simple RNN architectures are problematic since they suffer\nfrom vanishing gradients, resulting in little or no influence of early memories. More sophisticated architectures,\nsuch as long short-term memory (LSTM) networks with advanced attention mechanisms, attend to this problem.\nRNNs are typically applied for time series forecasting, predicting process behavior (Heinrich et al. 2021), and\nNLP tasks such as sequence transduction and neural machine translation (LeCun et al. 2015).\nDistributed representation\nDistributed representations play an essential role in feature learning and language modeling in NLP tasks, where\nlanguage entities such as words, phrases, and sentences are projected into numerical representations within a\nunified semantic space in the form of embeddings. Word embeddings, for example, encode discrete words into\ndense feature vectors with low dimensionality. Thus, in contrast to classic text representation models, such as\none-hot encodings and bag-of-words (BoW), word embeddings overcome the problem of sparse encodings\nwhile preserving semantic relationships between words. This means that words, which occur in similar contexts\nin a corpus, are also closely positioned to each other in the vector space. On this basis, advanced language models\ncan be developed to perform challenging downstream tasks, such as question-answering, sentiment analysis, and\nnamed entity recognition (Liu et al. 2020). Distributed representations are often applied in combination with\nRNNs to perform tasks with sequential dependencies.\nAutoencoder\nAutoencoders work similarly to word embeddings since they provide a dense feature representation of the input\ndata. However, they are not limited to natural language data but can be applied to any type of input. Such\narchitectures usually consist of an encoding stage where the input is compressed into a low-dimensional rep-\nresentation and a decoding stage in which the network tries to reconstruct the original input from the learned\nfeatures. In this way, the network is forced to keep meaningful information in the latent representation while\ndisregarding irrelevant noise (Goodfellow et al. 2016). Autoencoders are commonly applied for unsupervised\nfeature learning and dimensionality reduction in combination with other subsequent learning systems. However,\ndue to their capability of quantifying reconstruction errors, which are assumed to be significantly higher for\nanomalous samples than for regular instances, they can also be applied for detecting anomalies, such as fraud-\nulent activities in financial markets (Paula et al. 2016).\nGenerative adversarial neural\nnetwork (GAN)\nGenerative adversarial neural networks belong to the family of generative models that aim at learning a probability\ndistribution over a set of training data so that the network can randomly generate new data samples with some\nvariation. For this purpose, GANs consist of two competing sub-networks. The first network is a generator\nnetwork that captures the distribution of the input and generates new examples. The second network is a\ndiscriminator network trying to distinguish real examples from artificially generated ones. Both networks are\ntrained together in a non-cooperative zero-sum game where one network’s gain is another one’s loss until the\ndiscriminator can no longer distinguish between both types of samples. On this basis, GANs are likely to\nrevolutionize domains in which continuously new content or novel product configurations are created (e.g., the\ncomposition of art and music, design of fashion), or where content is converted from one representation to\nanother (e.g., text to image for product descriptions) (Pan et al. 2019). At the same time, however, such ap-\nproaches also pose severe threats with societal implications when abusing them for malicious purposes. In\nparticular, the generation of “deepfake” content in the form of abusive speeches and misleading news to\nmanipulate public opinions or distort financial markets is concerning (Westerlund 2019).\nC. Janiesch et al.\nproperties such as the type of learning mechanisms (e.g., shal-\nlow ML vs. DL), the number and type of manually generated\nor self-extracted features, and the number of trainable param-\neters (e.g., network weights in ANNs). Simpler models usual-\nly do not tend to be flexible enough to capture (non-linear)\nregularities and patterns that are relevant for the learning task.\nOverly complex models, on the other hand, entail a higher risk\nof overfitting. Furthermore, their reasoning is more difficult to\ninterpret (cf. next section), and they are likely to be computa-\ntionally more expensive. Computational costs are expressed\nby memory requirements and the inference time to execute a\nmodel on new data. These criteria are particularly important\nwhen assessing deep neural networks, where several million\nmodel parameters may be processed and stored, which places\nspecial demands on hardware resources. Consequently, it is\ncrucial for business settings with limited resources (such as\nenvironments that heavily rely on mobile devices) to not only\nselect a model at the sweet spot between underfitting and\noverfitting. They should also to evaluate a model’s complexity\nconcerning further trade-off relationships, such as accuracy\nvs. memory usage and speed (Heinrich et al. 2019).\nChallenges for intelligent systems based\non machine learning and deep learning\nElectronic markets are at the dawn of a technology-induced\nshift towards data-driven insights provided by intelligent sys-\ntems (Selz 2020). Already today, shallow ML and DL are\nused to build analytical models for them, and further diffusion\nis foreseeable. For any real-world application, intelligent sys-\ntems do not only face the task of model building, system\nspecification, and implementation. They are prone to several\nissues rooted in how ML and DL operate, which constitute\nchallenges relevant to the Information Systems community.\nThey do require not only technical knowledge but also involve\nhuman and business aspects that go beyond the system’s con-\nfinements to consider the circumstances and the ecosystem of\napplication.\nManaging the triangle of architecture,\nhyperparameters, and training data\nWhen building shallow ML and DL models for intelligent\nsystems, there are nearly endless options for algorithms or\narchitectures, hyperparameters, and training data (Duin\n1994; Heinrich et al. 2021). At the same time, there is a lack\nof established guidelines on how a model should be built for a\nspecific problem to ensure not only performance and cost-\nefficiency but also its robustness and privacy. Moreover, as\noutlined above, there are often several trade-off relations to be\nconsidered in business environments with limited resources,\nsuch as prediction quality vs. computational costs. Therefore,\nthe task of analytical model building is the most crucial since it\nalso determines the business success of an intelligent system.\nFor example, a model that can perform at 99.9% accuracy but\ntakes too long to put out a classification decision is rendered\nuseless and is equal to a 0%-accuracy model in the context of\ntime-critical applications such as proactive monitoring or\nquality assurance in smart factories. Further, different\nimplementations can only be accurately compared when vary-\ning only one of the three edges of the triangle at a time and\nreporting the same metrics. Ultimately, one should consider\nthe necessary skills, available tool support, and the required\nimplementation effort to develop and modify a particular DL\narchitecture (Wanner et al. 2020).\nThus, applications with excellent accuracy achieved in a\nlaboratory setting or on a different dataset may not translate\ninto business success when applied in a real-world environ-\nment in electronic markets as other factors may outweigh the\nML model’s theoretical achievements. This implies that re-\nsearchers should be aware of the situational characteristics of\na models' real-world application to develop an efficacious in-\ntelligent system. It is needless to say that researchers cannot\nknow all factors a priori, but they should familiarize them-\nselves with the fact that there are several architectural options\nwith different baseline variants, which suit different scenarios,\neach with their characteristic properties. Furthermore, multiple\nmetrics such as accuracy and F1 score should be reviewed on\nconsistent benchmarking data across models before making a\nchoice for a model.\nAwareness of bias and drift in data\nIn terms of automated analytical model building, one needs to\nbe aware of (cognitive) biases that are introduced into any\nshallow ML or DL model by using human-generated data.\nThese biases will be heavily adopted by the model (Fuchs\n2018; Howard et al. 2017). That is, the models will exhibit\nthe same (human-)induced tendencies that are present in the\ndata or even amplify them. A cognitive bias is an illogical\ninference or belief that individuals adopt due to flawed\nreporting of facts or due to flawed decision heuristics\n(Haselton et al. 2015). While data-introduced bias is not a\nparticularly new concept, it is amplified in the context of\nML and DL if training data has not been properly selected\nor pre-processed, has class imbalances, or when inferences\nare not reviewed responsibly. Striking examples include\nAmazon’s AI recruiting software that showed discrimination\nagainst women or Google’s Vision AI that produced starkly\ndifferent image labels based on skin color.\nFurther, the validity of recommendations based on data is\nprone to concept drift, which describes a scenario, where “the\nrelation between the input data and the target variable changes\nMachine learning and deep learning\nover time” (Gama et al. 2014). That is, ML models for intel-\nligent systems may not produce satisfactory results, when his-\ntorical data does not describe the present situation adequately\nanymore, for example due to new competitors entering a mar-\nket, new production capabilities becoming available, or un-\nprecedented governmental restrictions. Drift does not have to\nbe sudden but can be incremental, gradual, or reoccurring\n(Gama et al. 2014) and thus hard to detect. While techniques\nfor automated learning exist that involve using trusted data\nwindows and concept descriptions (Widmer and Kubat\n1996), automated strategies for discovering and solving\nbusiness-related problems are a challenge (Pentland et al.\n2020).\nFor applications in electronic markets, considering bias is\nof high importance as most data points will have human points\nof contact. These can be as obvious as social media posts or as\ndisguised as omitted variables. Further, poisoning attacks dur-\ning model retraining can be used to purposefully insert devi-\nating patterns. This entails that training data needs to be care-\nfully reviewed for such human prejudgments. Applications\nbased on this data should be understood as inherently biased\nrather than as impartial AI. This implies that researchers need\nto review their datasets and make public any biases they are\naware of. Again, it is unrealistic to assume that all bias effects\ncan be explicated in large datasets with high-dimensional data.\nNevertheless, to better understand and trust an ML model, it is\nimportant to detect and highlight those effects that have or\nmay have an impact on predictions. Lastly, as constant drift\ncan be assumed in any real-world electronic market, a trained\nmodel is never finished. Companies must put strategies in\nplace to identify, track, and counter concept drift that impacts\nthe quality of their intelligent system’s decisions. Currently,\nmanual checks and periodic model retraining prevail.\nUnpredictability of predictions and the need\nfor explainability\nThe complexity of DL models and some shallow ML models\nsuch as random forest and SVMs, often referred to as of black-\nbox nature, makes it nearly impossible to predict how they\nwill perform in a specific context (Adadi and Berrada 2018).\nThis also entails that users may not be able to review and\nunderstand the recommendations of intelligent systems based\non these models. Moreover, this makes it very difficult to\nprepare for adversarial attacks, which trick and break DL\nmodels (Heinrich et al. 2020). They can be a threat to high-\nstake applications, for example, in terms of perturbations of\nstreet signs for autonomous driving (Eykholt et al. 2018).\nThus, it may become necessary to explain the decision of a\nblack-box model also to ease organizational adoption. Not\nonly do humans prefer simple explanations to trust and adopt\na model, but the requirement of explainability may even be\nenforced by law (Miller 2019).\nThe field of explainable AI (XAI) deals with the augmen-\ntation of existing DL models to produce explanations for out-\nput predictions. For image data, this involves highlighting\nareas of the input image that are responsible for generating a\nspecific output decision (Adadi and Berrada 2018).\nConcerning time series data, methods have been developed\nto highlight the particular important time steps influencing a\nforecast (Assaf and Schumann 2019). A similar approach can\nbe used for highlighting words in a text that lead to specific\nclassification outputs.\nThus, applications in electronic markets with different crit-\nicality and human interaction requirements should be de-\nsigned or augmented distinctively to address the respective\nconcerns. Researchers must review the applications in partic-\nular of DL models for their criticality and accountability.\nPossibly, they must choose an explainable white-box model\nover a more accurate black-box model (Rudin 2019) or con-\nsider XAI augmentations to make the model’s predictions\nmore accessible to its users (Adadi and Berrada 2018).\nResource limitations and transfer learning\nLastly, building and training comprehensive analytical\nmodels with shallow ML or DL is costly and requires large\ndatasets to avoid a cold start. Fortunately, models do not\nalways have to be trained from scratch. The concept of\ntransfer learning allows models that are trained on general\ndatasets (e.g., large-scale image datasets) to be specialized\nfor specific tasks by using a considerably smaller dataset\nthat is problem-specific (Pouyanfar et al. 2019). However,\nusing pre-trained models from foreign sources can pose a\nrisk as the models can be subject to biases and adversarial\nattacks, as introduced above. For example, pre-trained\nmodels may not properly reflect certain environmental\nconstraints or contain backdoors by inserting classification\ntriggers, for example, to misclassify medical images\n(Wang et al. 2020). Governmental interventions to redirect\nor suppress predictions are conceivable as well. Hence, in\nhigh-stake situations, the reuse of publicly available ana-\nlytical models may not be an option. Nevertheless, transfer\nlearning offers a feasible option for small and medium-\nsized enterprises to deploy intelligent systems or enables\nlarge companies to repurpose their own general analytical\nmodels for specific applications.\nIn the context of transfer learning, new markets and\necosystems of AI as a service (AIaaS) are already emerg-\ning. Such marketplaces, for example by Microsoft or\nAmazon Web Services, offer cloud AI applications, AI\nplatforms, and AI infrastructure. In addition to cloud-\nbased benefits for deployments, they also enable transfer\nC. Janiesch et al.\nlearning from already established models to other applica-\ntions. That is, they allow customers with limited AI devel-\nopment resources to purchase pre-trained models and inte-\ngrate them into their own business environments (e.g.,\nNLP models for chatbot applications). New types of ven-\ndors can participate in such markets, for example, by of-\nfering transfer learning results for highly domain-specific\ntasks, such as predictive maintenance for complex ma-\nchines. As outlined above, consumers of servitized DL\nmodels in particular need to be aware of the risks their\nblack-box nature poses and establish similarly strict proto-\ncols as with human operators for similar decisions. As the\nmarket of AIaaS is only emerging, guidelines for respon-\nsible transfer learning have yet to be established (e.g.,\nAmorós et al. 2020).\nConclusion\nWith this fundamentals article, we provide a broad introduc-\ntion to ML and DL. Often subsumed as AI technology, both\nfuel the analytical models underlying contemporary and future\nintelligent systems. We have conceptualized ML, shallow\nML, and DL as well as their algorithms and architectures.\nFurther, we have described the general process of automated\nanalytical model building with its four aspects of data input,\nfeature extraction, model building, and model assessment.\nLastly, we contribute to the ongoing diffusion into electronics\nmarkets by discussing four fundamental challenges for intel-\nligent systems based on ML and DL in real-world ecosystems.\nHere, in particular, AIaaS constitutes a new and unexplored\nelectronic market and will heavily influence other established\nservice platforms. They will, for example, augment the smart-\nness of so-called smart services by providing new ways to learn\nfrom customer data and provide advice or instructions to them\nwithout being explicitly programmed to do so. We estimate that\nmuch of the upcoming research on electronic markets will be\nagainst the backdrop of AIaaS and their ecosystems and devise\nnew applications, roles, and business models for intelligent sys-\ntems based on DL. Related future research will need to address\nand factor in the challenges we presented by providing struc-\ntured methodological guidance to build analytical models, as-\nsess data collections and model performance, and make predic-\ntions safe and accessible to the user.\nFunding This research and development project is funded by the\nBayerische Staatsministerium für Wirtschaft, Landesentwicklung und\nEnergie (StMWi) within the framework concept “Informations- und\nKommunikationstechnik” (grant no. DIK0143/02) and managed by the\nproject management agency VDI+VDE Innovation + Technik GmbH..\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as long as\nyou give appropriate credit to the original author(s) and the source, pro-\nvide a link to the Creative Commons licence, and indicate if changes were\nmade. The images or other third party material in this article are included\nin the article's Creative Commons licence, unless indicated otherwise in a\ncredit line to the material. If material is not included in the article's\nCreative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain\npermission directly from the copyright holder. To view a copy of this\nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAdadi, A., & Berrada, M. (2018). Peeking inside the black-box: A\nsurvey on explainable artificial intelligence (XAI). IEEE Access,\n6, 52138–52160. https://doi.org/10.1109/ACCESS.2018.\n2870052.\nAhani, A., Nilashi, M., Ibrahim, O., Sanzogni, L., & Weaven, S. (2019).\nMarket segmentation and travel choice prediction in Spa hotels\nthrough TripAdvisor’s online reviews. International Journal of\nHospitality Management, 80, 52–77. https://doi.org/10.1016/j.\nijhm.2019.01.003.\nAmorós, L., Hafiz, S. M., Lee, K., & Tol, M. C. (2020). Gimme that\nmodel!: A trusted ML model trading protocol. arXiv:2003.00610\n[cs]. http://arxiv.org/abs/2003.00610\nAssaf, R., & Schumann, A. (2019). Explainable deep neural networks for\nmultivariate time series predictions. Proceedings of the 28th\nInternational Joint Conference on Artificial Intelligence, 6488–\n6490. https://doi.org/10.24963/ijcai.2019/932.\nBastan, M., Ramisa, A., & Tek, M. (2020). Cross-modal fashion product\nsearch with transformer-based Embeddings. CVPR Workshop - 3rd\nworkshop on Computer Vision for Fashion, Art and Design, Seattle:\nWashington.\nBishop, C. M. (2006). Pattern recognition and machine learning\n(Information science and statistics). Springer-Verlag New York,\nInc.\nBrynjolfsson, E., & McAfee, A. (2017). The business of artificial intelli-\ngence. Harvard Business Review, 1–20.\nChen, S. H., Jakeman, A. J., & Norton, J. P. (2008). Artificial intelligence\ntechniques: An introduction to their use for modelling environmen-\ntal systems. Mathematics and Computers in Simulation, 78(2–3),\n379–400. https://doi.org/10.1016/j.matcom.2008.01.028.\nDalal, N., & Triggs, B. (2005). Histograms of oriented gradients for\nhuman detection. 2005 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’05), 1, 886–\n893. https://doi.org/10.1109/CVPR.2005.177.\nDuin, R. P. W. (1994). Superlearning and neural network magic. Pattern\nRecognition Letters, 15(3), 215–217. https://doi.org/10.1016/0167-\n8655(94)90052-3.\nEykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C.,\nPrakash, A., Kohno, T., & Song, D. (2018). Robust physical-world\nattacks on deep learning visual classification. IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2018,\n1625–1634. https://doi.org/10.1109/CVPR.2018.00175.\nFischer, M., Heim, D., Hofmann, A., Janiesch, C., Klima, C., &\nWinkelmann, A. (2020). A taxonomy and archetypes of smart ser-\nvices for smart living. Electronic Markets, 30(1), 131–149. https://\ndoi.org/10.1007/s12525-019-00384-5.\nMachine learning and deep learning\nFuchs, D. J. (2018). The dangers of human-like Bias in machine-learning\nalgorithms. Missouri S&T’s Peer to Peer, 2(1), 15.\nGama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M., & Bouchachia, A.\n(2014). A survey on concept drift adaptation. ACM Computing\nSurveys, 46(4), 1–37. https://doi.org/10.1145/2523813.\nGarcía, S., & Herrera, F. (2008). An extension on “statistical comparisons\nof classifiers over multiple data sets” for all pairwise comparisons.\nJournal of Machine Learning Research, 9(89), 2677–2694.\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. The\nMIT Press.\nGoyal, D., & Pabla, B. S. (2015). Condition based maintenance of\nmachine tools—A review. CIRP Journal of Manufacturing\nScience and Technology, 10, 24–35. https://doi.org/10.1016/j.\ncirpj.2015.05.004.\nGrigorescu, S., Trasnea, B., Cocias, T., & Macesanu, G. (2020). A survey\nof deep learning techniques for autonomous driving. Journal of\nField Robotics, 37(3), 362–386. https://doi.org/10.1002/rob.21918.\nHaselton, M. G., Nettle, D., & Andrews, P. W. (2015). The evolution of\ncognitive Bias. In: D. M. Buss (Ed.), The handbook of evolutionary\npsychology (pp. 724–746). Inc: John Wiley & Sons. https://doi.org/\n10.1002/9780470939376.ch25.\nHeinrich, K., Graf, J., Chen, J., Laurisch, J., & Zschech, P. (2020).\nFool me once, shame on you, fool me twice, shame on me: A\ntaxonomy of attack and defense patterns for AI security.\nProceedings of the 28th European Conference on Information\nSystems (ECIS).\nHeinrich, K., Möller, B., Janiesch, C., & Zschech, P. (2019). Is Bigger\nAlways Better? Lessons Learnt from the Evolution of Deep\nLearning Architectures for Image Classification. Proceedings of\nthe 2019 Pre-ICIS SIGDSA Symposium. https://aisel.aisnet.org/\nsigdsa2019/20\nHeinrich, K., Zschech, P., Janiesch, C., & Bonin, M. (2021). Process data\nproperties matter: Introducing gated convolutional neural networks\n(GCNN) and key-value-predict attention networks (KVP) for next\nevent prediction with deep learning. Decision Support Systems, 143,\n113494. https://doi.org/10.1016/j.dss.2021.113494.\nHoward, A., Zhang, C., & Horvitz, E. (2017). Addressing bias in machine\nlearning algorithms: A pilot study on emotion recognition for intel-\nligent systems. IEEE Workshop on Advanced Robotics and its\nSocial Impacts (ARSO), 1–7. https://doi.org/10.1109/ARSO.2017.\n8025197.\nJayanth Balaji, A., Harish Ram, D. S., & Nair, B. B. (2018). Applicability\nof deep learning models for stock Price forecasting an empirical\nstudy on BANKEX data. Procedia Computer Science, 143, 947–\n953. https://doi.org/10.1016/j.procs.2018.10.340.\nJordan, M. I., & Mitchell, T. M. (2015). Machine learning: Trends, per-\nspectives, and prospects. Science, 349(6245), 255–260. https://doi.\norg/10.1126/science.aaa8415.\nKotsiantis, S. B., Zaharakis, I. D., & Pintelas, P. E. (2006). Machine\nlearning: A review of classification and combining techniques.\nArtificial Intelligence Review, 26(3), 159–190. https://doi.org/10.\n1007/s10462-007-9052-3.\nKühl, N., Mühlthaler, M., & Goutier, M. (2020). Supporting customer-\noriented marketing with artificial intelligence: Automatically quan-\ntifying customer needs from social media. Electronic Markets,\n30(2), 351–367. https://doi.org/10.1007/s12525-019-00351-0.\nLeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature,\n521(7553), 436–444. https://doi.org/10.1038/nature14539.\nLeijnen, S., & van Veen, F. (2020). The Neural Network Zoo.\nP r o c e e d i n g s , 4 7 ( 1 ) , 9 . h t t p s : / / d o i . o r g / 1 0 . 3 3 9 0 /\nproceedings47010009.\nLiu, Z., Lin, Y., & Sun, M. (2020). Representation learning for natural\nlanguage processing. Springer Singapore. https://doi.org/10.1007/\n978-981-15-5573-2.\nLowe, D. G. (2004). Distinctive image features from scale-invariant\nKeypoints. International Journal of Computer Vision, 60(2), 91–\n110. https://doi.org/10.1023/B:VISI.0000029664.99615.94.\nMadani, A., Arnaout, R., Mofrad, M., & Arnaout, R. (2018). Fast and\naccurate view classification of echocardiograms using deep learn-\ning. Npj Digital Medicine, 1(1). https://doi.org/10.1038/s41746-\n017-0013-1.\nMiller, T. (2019). Explanation in artificial intelligence: Insights from the\nsocial sciences. Artificial Intelligence, 267, 1–38. https://doi.org/10.\n1016/j.artint.2018.07.007.\nPan, Z., Yu, W., Yi, X., Khan, A., Yuan, F., & Zheng, Y. (2019). Recent\nProgress on generative adversarial networks (GANs): A survey.\nIEEE Access, 7, 36322–36333. https://doi.org/10.1109/ACCESS.\n2019.2905015.\nPaula, E. L., Ladeira, M., Carvalho, R. N., & Marzagão, T. (2016). Deep\nlearning anomaly detection as support fraud investigation in\nBrazilian exports and anti-money laundering. 15th IEEE\nInternational Conference on Machine Learning and Applications\n(ICMLA), 954–960. https://doi.org/10.1109/ICMLA.2016.0172.\nPentland, B. T., Liu, P., Kremser, W., & Haerem, T. (2020). The dynam-\nics of drift in digitized processes. MIS Quarterly, 44(1), 19–47.\nhttps://doi.org/10.25300/MISQ/2020/14458.\nPeters, M., Ketter, W., Saar-Tsechansky, M., & Collins, J. (2013). A\nreinforcement learning approach to autonomous decision-making\nin smart electricity markets. Machine Learning, 92(1), 5–39.\nhttps://doi.org/10.1007/s10994-013-5340-0.\nPouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y., Reyes, M. P.,\nShyu, M.-L., Chen, S.-C., & Iyengar, S. S. (2019). A survey on\ndeep learning: Algorithms, techniques, and applications. ACM\nComputing Surveys, 51(5), 1–36. https://doi.org/10.1145/\n3234150.\nRamaswamy, S., & DeClerck, N. (2018). Customer perception analysis\nusing deep learning and NLP. Procedia Computer Science, 140,\n170–178. https://doi.org/10.1016/j.procs.2018.10.326.\nRudin, C. (2019). Stop explaining black box machine learning models for\nhigh stakes decisions and use interpretable models instead. Nature\nMachine Intelligence, 1(5), 206–215. https://doi.org/10.1038/\ns42256-019-0048-x.\nRussell, S. J., & Norvig, P. (2021). Artificial intelligence: A modern\napproach (4th ed.). Pearson.\nSalton, G., & Buckley, C. (1988). Term-weighting approaches in\nautomatic text retrieval. Information Processing &\nManagement, 24(5), 513–523. https://doi.org/10.1016/0306-\n4573(88)90021-0.\nSchmidhuber, J. (2015). Deep learning in neural networks: An overview.\nNeural Networks, 61, 85–117. https://doi.org/10.1016/j.neunet.\n2014.09.003.\nSearle, J. R. (1980). Minds, brains, and programs. Behavioral and Brain\nSciences, 3(3), 417–424. https://doi.org/10.1017/\nS0140525X00005756.\nSelz, D. (2020). From electronic markets to data driven insights.\nElectronic Markets, 30(1), 57–59. https://doi.org/10.1007/s12525-\n019-00393-4.\nShmueli, G., & Koppius, O. (2011). Predictive analytics in information\nsystems research. Management Information Systems Quarterly,\n35(3), 553–572. https://doi.org/10.2307/23042796.\nShrestha, Y. R., Krishna, V., & von Krogh, G. (2021). Augmenting\norganizational decision-making with deep learning algorithms:\nPrinciples, promises, and challenges. Journal of Business\nResearch, 123, 588–603. https://doi.org/10.1016/j.jbusres.2020.\n09.068.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,\nLanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T.,\nSimonyan, K., & Hassabis, D. (2018). A general reinforcement\nlearning algorithm that masters chess, shogi, and go through self-\nC. Janiesch et al.\nplay. Science, 362(6419), 1140–1144. https://doi.org/10.1126/\nscience.aar6404.\nSpooner, T., Fearnley, J., Savani, R., & Koukorinis, A. (2018). Market\nmaking via reinforcement learning. Proceedings of the 17th\nInternational Conference on Autonomous Agents and MultiAgent\nsystems, 434–442. arXiv:1804.04216v1\nStone, P., Brooks, R., Brynjolfsson, E., Calo, R., Etzioni, O., Hager, G.,\nHirschberg, J., Kalyanakrishnan, S., Kamar, E., Kraus, S., Leyton-\nBrown, Kevin, Parkes, D., Press, W., Saxenian, A. L., Shah, J.,\nMilind Tambe, & Teller, A. (2016). Artificial Intelligence and Life\nin 2030: the one hundred year study on artificial\nintelligence (Report of the 2015–2016 study panel). Stanford\nUniversity. https://ai100.stanford.edu/2016-report\nViola, P., & Jones, M. (2001). Rapid object detection using a boosted\ncascade of simple features. Proceedings of the 2001 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition.\nCVPR 2001, 1, I-511–I-518. https://doi.org/10.1109/CVPR.2001.\n990517.\nWang, S., Nepal, S., Rudolph, C., Grobler, M., Chen, S., & Chen, T.\n(2020). Backdoor attacks against transfer learning with pre-trained\ndeep learning models. IEEE Transactions on Services Computing,\n1–1. https://doi.org/10.1109/TSC.2020.3000900.\nWanner, J., Heinrich, K., Janiesch, C., & Zschech, P. (2020). How much\nAI do you require? Decision factors for adopting AI technology.\nProceedings of the 41st International Conference on Information\nSystems (ICIS).\nWesterlund, M. (2019). The emergence of Deepfake technology: A re-\nview. Technology Innovation Management Review, 9(11), 39–52.\nhttps://doi.org/10.22215/timreview/1282\nWidmer, G., & Kubat, M. (1996). Learning in the presence of concept\ndrift and hidden contexts. Machine Learning, 23(1), 69–101. https://\ndoi.org/10.1007/BF00116900.\nWu, M., Liu, F., & Cohn, T. (2018). Evaluating the utility of hand-crafted\nfeatures in sequence labelling. Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, 2850–\n2856. https://doi.org/10.18653/v1/D18-1310.\nYoung, T., Hazarika, D., Poria, S., & Cambria, E. (2018). Recent trends\nin deep learning based natural language processing [review article].\nIEEE Computational Intelligence Magazine, 13(3), 55–75. https://\ndoi.org/10.1109/MCI.2018.2840738.\nZhang, Y., & Ling, C. (2018). A strategy to apply machine learning to\nsmall datasets in materials science. npj Computational Materials,\n4(1). https://doi.org/10.1038/s41524-018-0081-z.\nPublisher’s note Springer Nature remains neutral with regard to jurisdic-\ntional claims in published maps and institutional affiliations.\nMachine learning and deep learning\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2021-04-12",
  "updated": "2021-04-14"
}