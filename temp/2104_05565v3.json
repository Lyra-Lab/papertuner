{
  "id": "http://arxiv.org/abs/2104.05565v3",
  "title": "Survey on reinforcement learning for language processing",
  "authors": [
    "Victor Uc-Cetina",
    "Nicolas Navarro-Guerrero",
    "Anabel Martin-Gonzalez",
    "Cornelius Weber",
    "Stefan Wermter"
  ],
  "abstract": "In recent years some researchers have explored the use of reinforcement\nlearning (RL) algorithms as key components in the solution of various natural\nlanguage processing tasks. For instance, some of these algorithms leveraging\ndeep neural learning have found their way into conversational systems. This\npaper reviews the state of the art of RL methods for their possible use for\ndifferent problems of natural language processing, focusing primarily on\nconversational systems, mainly due to their growing relevance. We provide\ndetailed descriptions of the problems as well as discussions of why RL is\nwell-suited to solve them. Also, we analyze the advantages and limitations of\nthese methods. Finally, we elaborate on promising research directions in\nnatural language processing that might benefit from reinforcement learning.",
  "text": "Survey on reinforcement learning for language processing\nV´ıctor Uc-Cetina1, Nicol´as Navarro-Guerrero2, Anabel Martin-Gonzalez1,\nCornelius Weber3, Stefan Wermter3\n1 Universidad Aut´onoma de Yucat´an - {uccetina, amarting}@correo.uady.mx\n2 Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz GmbH - nicolas.navarro@dfki.de\n3 Universit¨at Hamburg - {weber, wermter}@informatik.uni-hamburg.de\nMarch 2022\nAbstract\nIn recent years some researchers have explored the use of reinforcement learning\n(RL) algorithms as key components in the solution of various natural language process-\ning tasks. For instance, some of these algorithms leveraging deep neural learning have\nfound their way into conversational systems. This paper reviews the state of the art of\nRL methods for their possible use for diﬀerent problems of natural language processing,\nfocusing primarily on conversational systems, mainly due to their growing relevance.\nWe provide detailed descriptions of the problems as well as discussions of why RL is\nwell-suited to solve them. Also, we analyze the advantages and limitations of these\nmethods. Finally, we elaborate on promising research directions in natural language\nprocessing that might beneﬁt from reinforcement learning.\nKeywords— reinforcement learning, natural language processing, conversational systems, pars-\ning, translation, text generation\n1\nIntroduction\nMachine learning algorithms have been very successful to solve problems in the natural language\nprocessing (NLP) domain for many years, especially supervised and unsupervised methods. How-\never, this is not the case with reinforcement learning (RL), which is somewhat surprising since\nin other domains, reinforcement learning methods have experienced an increased level of success\nwith some impressive results, for instance in board games such as AlphaGo Zero [115]. Yet, deep\nreinforcement learning for natural language processing is still in its infancy when compared to su-\npervised learning [71]. Thus, the main goal of this article is to provide a review of applications of\nreinforcement learning to NLP. Moreover, we present an analysis of the underlying structure of the\nproblems that make them viable to be treated entirely or partially as RL problems, intended as an\naid to newcomers to the ﬁeld. We also analyze some existing research gaps and provide a list of\npromising research directions in which natural language systems might beneﬁt from reinforcement\nlearning algorithms.\n1\narXiv:2104.05565v3  [cs.CL]  15 Mar 2022\n1.1\nReinforcement learning\nReinforcement learning is a term commonly used to refer to a family of algorithms designed to solve\nproblems in which a sequence of decisions is needed. Reinforcement learning has also been deﬁned\nmore as a kind of learning problem than as a group of algorithms used to solve such problems [124].\nIt is important to mention that reinforcement learning is a very diﬀerent kind of learning than the\nones studied in supervised and unsupervised methods. This kind of learning requires the learning\nsystem, also known as agent, to discover by itself through the interaction with its environment,\nwhich sequence of actions is the best to accomplish its goal.\nThere are three major groups of reinforcement methods, namely, dynamic programming, Monte\nCarlo methods, and temporal diﬀerence methods. Dynamic programming methods estimate state\nor state-action values by making estimates from other estimates. This iteratively intertwines policy\nevaluation and policy improvement updates taking advantage of a model of the environment which\nis used to calculate rewards.\nPolicy evaluation consists of updating the current version of the\nvalue function based on the current policy. Policy improvement consists of greedifying the policy\nfunction based on the current value function. Depending on the algorithm and its implementation\nit might require exhaustive sweeping of the entire state space or not. Monte Carlo methods learn\nfrom complete sample returns, instead of immediate rewards. Unlike dynamic programming, Monte\nCarlo methods only consider one transition path at a time, the path generated with a sample. In\nother words, they do not bootstrap from successor states’ values. Therefore, these kinds of methods\nare more useful when we do not have a model of the environment, the so-called dynamics of the\nenvironment. Temporal diﬀerence methods do not need a model of the environment since they\ncan learn from experience, which can be generated from interactions with the environment. These\nmethods possess the best of dynamic programming and the best of Monte Carlo. From dynamic\nprogramming they inherit the bootstrapping, from Monte Carlo methods they inherit the sampling.\nAs a result of this combination of characteristics, temporal diﬀerence methods have been the most\nwidely used. All these methods pose the decision-making problem as a Markov decision process\n(MDP). An MDP is a mathematical method used to solve decision-making in sequence and considers\nas the minimum existing elements a set of states S, a set of actions A, a transition function T, and\na reward function R. Given an MDP (S, A, T, R), we need to ﬁnd an optimal policy function π,\nwhich represents the solution of our sequence decision problem. The aim of a reinforcement learning\nsystem, or so-called agent, is to maximize some cumulative reward r ∈R through a sequence of\nactions.\nEach pair of state s and action a creates a transition tuple (s, a, r, s′), with s′ being\nthe resulting state. Depending on the algorithm being used and on the particular settings of our\nproblem, the policy π will be estimated diﬀerently.\nA policy π deﬁnes the behavior of the agent at any given moment. In other words, a policy is a\nmapping from the set of states S perceived from the environment to a set of actions A that should\nbe executed in those states. In some cases, the policy can be stored as a lookup table, and in other\ncases it is stored as a function approximator, such as a neural network. The latter is imperative\nwhen we have a large number of states. The policy is the most important mathematical function\nlearned by the reinforcement learning agent, in the sense that it is all the agent needs to control\nits behavior once the learning process has concluded. In general, a policy can be stochastic and we\nformally deﬁne it as π : S →A.\nThe goal in a reinforcement learning problem is speciﬁed by the reward function R. This function\nmaps each state or pair of state-action perceived in the environment to one real number r ∈ℜcalled\nreward. This reward indicates how good or bad a given state is. As we mentioned before, the goal\nof an agent is to maximize the total amount of rewards that it gets in the long run, during its\ninteraction with the environment. The reward function cannot be modiﬁed by the agent, however,\n2\nit can serve as a basis for modifying the policy function. For example, if the action selected by the\ncurrent policy is followed by a low reward, then the policy can be updated in such a way that in\nthe future it indicates a diﬀerent action when the agent encounters the same situation. In general,\nthe reward function can also be a stochastic function and it is formally deﬁned as R : S →ℜ, or\nR : S × A →ℜ.\nA value function indicates which actions are good in the long run.\nThe value of a state is\nbasically an estimation of the total amount of rewards that the agent can expect to accumulate\nin the future, if it starts its path from that state using its current policy. We should not confuse\nthe value function with the reward function. The rewards are given directly by the environment\nwhile the values of the states are estimated by the agent, from its interaction with the environment.\nMany reinforcement learning methods estimate the policy function from the value function. When\nthe value function is a mapping from states to real numbers, it is denoted by the letter V . When\nthe mapping is from pairs of state-action to real numbers, it is denoted by Q. Formally, we can\ndeﬁne the value function as V : S →ℜor Q : S × A →ℜ.\nIn the case of model-based RL, the agent also has access to a model of the transition function\nT of the environment, which may be learnt from experience. For example, given a state and an\naction, the model could predict the next resulting state and reward. Such world models are used for\nplanning, this is, a way to make decisions about the next actions to be performed, without the need\nto experience possible situations. In the case of model-free RL, when a model of the environment\nis missing, we have to solve the reinforcement learning problem without planning and that means\nthat a signiﬁcant amount of experimentation with the environment will be needed.\nOne of the most popular reinforcement learning algorithms is the Q-learning algorithm [132]. As\nits name suggests, it works by estimating a state-action value function Q. The algorithm does not\nrely on a model of the transition function T, and therefore it has to interact with the environment\niteratively. It follows one policy function for exploring the environment and a second greedy policy\nfor updating its estimations of the values of pairs of states and actions that it happens to visit\nduring the learning process. This kind of learning is called oﬀ-policy learning. The algorithm uses\nthe following rule for updating the Q values:\nQ(s, a) ←Q(s, a) + α[r + γ max\na′\nQ(s′, a′) −Q(s, a)].\nIn this learning rule, α is a parameter deﬁned experimentally and it is known as the learning\nrate. It takes values in the interval (0, 1). Moreover, r is the reward signal, γ is known as the\ndiscount parameter and it also takes values in the interval (0, 1), and ﬁnally s′ and a′ denote the\nnext state and the next action to be visited and executed during the next interaction with the\nenvironment.\nSARSA is an on-policy learning algorithm, meaning that instead of using two policies, one for\nbehavior and one for learning, this algorithm uses only one policy. The same policy that is used to\nexplore the environment is the same policy used in the update rule [124]. The update rule of the\nSARSA is the following:\nQ(s, a) ←Q(s, a) + α[r + γQ(s′, a′) −Q(s, a)].\nA very important result in recent years was the development of the deep Q-network [88], in\nwhich a convolutional neural network is trained with a variant of Q-learning.\nThis algorithm,\noriginally designed to learn to play several Atari 2600 games at a superhuman level, is now being\napplied to other learning tasks. Another algorithm, AlphaGo Zero [113], learned to play Go and\nactually defeated the human world champion in 2016. This algorithm uses a deep neural network,\na search algorithm and reinforcement learning rules. The successor model MuZero [108] learns a\n3\nrepresentation of state, a dynamics and a reward prediction function to maximize future rewards\nvia tree search-based planning, achieving more successful game play without prior knowledge of the\ngame rules.\nDeep reinforcement learning is an extension of the classical reinforcement learning methods\nto leverage the representational power of deep models.\nMore speciﬁcally, deep neural networks\nallow reinforcement learning algorithms to approximate and store highly complex value functions,\nstate-action functions, or policy functions. For instance, a Q(s, a) function can be represented as a\nconvolutional neural network or a recurrent one. Similarly to what happened in other domains such\nas computer vision, deep models are also playing a decisive role in the advancement of reinforcement\nlearning research, especially in MDPs with very large state and action spaces. In fact, reinforcement\nlearning and deep neural networks have stayed recently at the center of attention of many researchers\nwho have studied and applied them to solve diﬀerent problems, including problems in natural\nlanguage processing, as we will discuss below.\n1.2\nNatural language processing and RL\nIn natural language processing, one of the main goals is the development of computer programs\ncapable of communicating with humans through the use of natural language. In some applications,\nsuch as machine translation, these programs are used to help humans who speak diﬀerent languages\nto understand each other by translating from one natural language to another. Through the years,\nNLP research has gone from being heavily inﬂuenced by theories of linguistics, such as those pro-\nposed by Noam Chomsky [19, 20], to the corpus linguistics approach of machine learning algorithms\nand more recently the use of deep neural networks as neural language models such as BERT [32]\nand GPT-3 [9].\nAccording to Russell and Norvig [105], to the contrary of formal languages, it is more fruitful\nto deﬁne natural language models as probability distributions over sentences rather than using\ndeﬁnitive sets speciﬁed by grammars. The main challenges when dealing with natural languages are\nthat they are ambiguous, large and constantly changing. That is why initial approaches to model\nnatural languages using grammars were not as successful as modern machine learning approaches.\nIn the former approaches, the grammars needed to be adapted and their size increased to fulﬁl the\ndemands for better performance.\nOne important probabilistic approach to modelling natural languages involves the use of n-\ngrams. A sequence of written symbols of length n is called an n-gram. A model of the probability\ndistribution of strings containing n symbols is therefore called an n-gram model. This model is\ndeﬁned as a Markov chain of order n −1 in which the probability of some symbol si depends\nonly on the immediately preceding n −1 symbols. Formally, we say p(si|si−1, si−2, . . . , s2, s1) =\np(si|si−1, . . . , si−n+1). Probabilistic natural language models based on n-grams can be useful for\ntext classiﬁcation tasks [105].\nImportant advances in the design of algorithms for training deep neural networks, such as the\nrecurrent long short-term memory (LSTM) network [56], have allowed researchers to move from\nprobabilistic language models to language models based on neural networks. The LSTM neural\nmodel has been successfully applied to machine translation. The performance of current translator\nprograms could not be accomplished using the approach based on language grammars alone. These\nnew neural models are highly complex mathematical functions with thousands of parameters which\nare estimated iteratively from a massive number of training examples gathered from the Internet.\nSome problems in natural language processing can be deﬁned as Markov decision processes\nand therefore they can be solved using reinforcement learning algorithms. In Fig. 1, we provide a\nschematic example of how a reinforcement learning agent would be designed to solve a language\n4\nFigure 1: Schematic view of a reinforcement learning agent designed for language processing.\nThe language model agent acts by appending, replacing or deleting strings of words. States\nare strings of words. The language processing environment will provide the agent with the\nstates and rewards after each of the interactions. The reward function is determined by the\nspeciﬁc natural language processing task. One simple possibility for a reward function would\nreinforce every optimal action with a +1.\nprocessing task in which states, actions and rewards operate mainly over strings. A set of basic\noperations may include appending, replacing and deleting words.\nIn this article, we review ﬁve main categories of such problems, namely, syntactic parsing, lan-\nguage understanding, text generation systems, machine translation, and conversational systems. Of\nthese, conversational systems are the most studied ones, which involve ﬁnding an optimal dialog\npolicy that should be followed by an automated system during a conversation with a human user.\nThe other four categories are not widely known applications of reinforcement learning methods and\ntherefore it is interesting to discuss their main beneﬁts and drawbacks. In some of them, it is even\nnot easy to identify the elements of a well-deﬁned Markov decision process. This might explain\nwhy they have not received more attention yet. Identifying these diﬀerent natural language pro-\ncessing problems is important to discover new research lines at the intersection of natural language\nprocessing and reinforcement learning.\nIn the next sections, we describe with more detail these ﬁve categories of natural language\nprocessing problems and their proposed solutions by means of reinforcement learning.\nWe also\ndiscuss the main achievements and core challenges on each of these categories.\n2\nSyntactic parsing\nSyntactic parsing consists of analyzing a string made of symbols belonging to some alphabet, either\nin natural languages or in programming languages. Such analysis is often performed according to a\nset of rules called grammar. There could be many ways to perform parsing, depending on the ﬁnal\ngoal of the system [147, 58, 93, 69]. One of such goals could be the construction of a compiler for\na new programming language when we are working with formal computer languages. Another one\ncould be an application of language understanding for human-computer interaction.\nA grammar can generate many parsing trees and each of these trees speciﬁes the valid structure\nfor sentences of the corresponding language. Since parsing can be represented as a sequential search\nproblem with a parse tree as the ﬁnal goal state, reinforcement learning methods are tools very\n5\nwell suited for the underlying sequential decision problem. In general, a parse is obtained as a path\nwhen an optimal policy is used, in a given MDP.\nConsider for example the simple context-free grammar G1 and the language L(G1) generated\nby it. G1 is a 4-tuple (V, Σ, R, S) where\n• V = {A, B} is a ﬁnite set of variables,\n• Σ = {0, 1, #} is a ﬁnite set, disjoint of V , containing terminal symbols,\n• R is the ﬁnite set of 4 production rules given in Fig. 2, and\n• S ∈V is the initial variable.\nS →0A1\n(1)\nA →0A1 | B\n(2)\nB →#\n(3)\nFigure 2: Grammar G1 with 4 production rules.\nThe language L(G1) generated by grammar G1 is an inﬁnite set of strings.\nEach of these\nstrings is created by starting with the initial variable S and iteratively selecting and applying one\nof the production rules in G1, also called substitution rules. For example, the string 0#1 is a valid\nstring belonging to L(G1) and it can be generated by applying the following sequence of production\nrules S →0A1, A →B and B →#. Looking at this application of rules as a path of string\nsubstitutions, we have S ⇒0A1 ⇒0B1 ⇒0#1. A path of substitutions, known also as derivation,\ncan be represented pictorially as a parse tree. For example, the parse tree for the derivation of the\nstring 00#11 is illustrated in Fig. 3.\nFigure 3: Parse tree of string 00#11 generated from grammar G1.\nFrom the previous grammar example G1 we can notice the similarity between the elements\ndeﬁned in a context-free grammar G = {V, Σ, P, S} and the elements deﬁned in a Markov decision\nprocess M = {S, A, T, R}. Let us now analyze this similarity, element by element, from the point\nof view of an MDP.\n• The starting state s of an MDP M can be deﬁned as the initial variable of a grammar, denoted\nby letter S in grammar G.\n6\n• The set of states S in the MDP M can be deﬁned as the set of strings generated by the\ngrammar, in other words, the language generated by grammar G, this is S = L(G).\n• The set of actions A can be deﬁned as the set of production rules given by grammar G, this\nis A = R; the MDP transition function T would be immediately deﬁned once we have deﬁned\nthe set of production rules itself.\n• The reward function R is the only element that cannot be taken straightforward from the\nelements of the grammar and it should be crafted by the designer of the system.\nIn the speciﬁc application of dependency parsing [65], it has been shown that a parser can\nbe implemented to use a policy learned by reinforcement learning, in order to select the optimal\ntransition in each parsing stage [147]. Given a sentence with n words x = w1w2 . . . wn, we can\nconstruct its dependency tree by selecting a sequence of transitions. A stack data structure is used\nto store partially processed words and also a queue data structure is used to record the remaining\ninput words together with the partially labeled dependency structure constructed by the previous\ntransitions. The construction of the dependency tree is started with an empty stack and the input\nwords being fed into the queue. The algorithm performs 4 diﬀerent types of transitions until the\nqueue is empty. These 4 transitions are: reduce, which takes one word from the stack; shift, which\npushes the next input word into the stack; left-arc, which adds a labeled dependency arc from the\nnext input word to the top of the stack and then takes the word from the top of the stack; and\nﬁnally right-arc, which adds a dependency arc from the top of the stack to the next input word and\npushes that same word into the stack. During the construction of the parsing tree each one of the\ntransitions is selected using a reward signal. In this particular implementation the optimal policy\nfor selecting the transitions is estimated using the SARSA reinforcement learning algorithm.\nAn interesting modiﬁcation found in the implementation of this algorithm is the replacement\nof the Q function by an approximation computed through the calculation of the negative free\nenergies of a restricted Boltzmann machine. The results of this approach for dependency parsing\nusing reinforcement learning are comparable with state-of-the-art methods. More recently, it has\nbeen shown that reinforcement learning can also be used to reduce error propagation in greedy\ndependency parsing [69]. In another approach, Neu et al. [93] used a number of inverse reinforcement\nlearning algorithms to solve the parsing problem with probabilistic context-free grammars. In inverse\nreinforcement learning, given a set of trajectories in the environment, the goal is to ﬁnd a reward\nfunction such that if it is used for estimating the optimal policy, the resulting policy can generate\ntrajectories very similar to the original ones [94].\nAnother dual learning approach for solving the semantic parsing problem is presented by Cao et\nal. [11]. This dual learning algorithm follows the same strategy used by Zhu et al. [149], consisting\nof an adversarial training scheme that can use both labeled and unlabeled data. The primary task\n(semantic parsing) learns the transformation from a query to logical form (Q2LF). The secondary\ntask (natural language generation) learns the transformation from a logical form to a query (LF2Q).\nThe agent from the primary task can teach the agent from the secondary task and vice versa in\na reinforcement learning fashion. A validity reward by checking the output of the primary model\nat the surface and at semantic levels is used. This reward function requires prior knowledge of the\nlogical forms of the domain of interest, and it is used to check for completeness and well-formed\nsemantic representations. The experimental results showed that semantic parsing based on dual\nlearning improves performance across datasets.\nIn a probabilistic context-free grammar, each production rule has a probability assigned to it,\nwhich results in the generation of expert trajectories. Speeding up the learning of parse trees using\nreinforcement learning has also been studied, speciﬁcally the use of apprenticeship reinforcement\n7\nlearning as a variation of inverse RL has been shown to be an eﬀective method for learning a fast\nand accurate parser, requiring only a simple set of features [58]. By abstracting the core problem in\nsyntactic parsing, we can clearly see that it can be posed as an optimization problem in which the\ninput is a language grammar G and one input string w1 to be parsed, and the output is a parse tree\nthat allows the correct parsing of w1. This problem gives rise to the following MDP (S, A, T, R)\n[69]:\n• The set of states S is deﬁned as the set of all possible partial or complete parse trees that\ncan be generated with the given grammar G and the string w1.\n• The set of actions A is formed with all the grammar rules contained in G, this is, the appli-\ncation of each derivation rule of the grammar is considered to be an action.\n• The transition function T can be completely determined and it is deterministic, because given\na selected grammar rule and the current partially parsed string, we can know with certainty\nthe next resulting intermediate parse tree of that string.\n• Finally, the reward function R can be deﬁned as a function of the number of arcs that are\ncorrectly labeled in the resulting parse tree.\nBased on this MDP we can formulate a reinforcement learning system as illustrated in Fig. 4.\nFigure 4: Schematic view of a reinforcement learning agent designed for syntactic parsing.\nThe language processing environment will provide the agent with the states and rewards after\neach of the interactions. The reward function can be deﬁned in various ways, for example, a\npositive reward of 10 may be provided each time an appropriate grammar rule is applied.\n3\nLanguage understanding\nLanguage understanding can also be posed as a Markov decision process and therefore we can\napply sophisticated reinforcement learning algorithms designed in recent years. Furthermore, we\ncan implement them together with deep neural networks to cope with the massive amount of data\nthat text understanding applications typically require.\nConsider a problem of natural language understanding. In such a problem we could have a\ngrammar like the one given in Fig. 5 that allows a program to automatically determine the elements\nof a sentence written in English. Using this grammar, sentences such as “The customer with a\ndiscount wants a refund” and “The customer with a discount cancelled the refund” can be analyzed\n8\n⟨SENTENCE⟩→⟨NOUN PHRASE⟩⟨VERB PHRASE⟩\n⟨NOUN PHRASE⟩→⟨CMPLX NOUN⟩| ⟨CMPLX NOUN⟩⟨PREP PHRASE⟩\n⟨VERB PHRASE⟩→⟨CMPLX VERB⟩| ⟨CMPLX VERB⟩⟨PREP PHRASE⟩\n⟨PREP PHRASE⟩→⟨PREP⟩⟨CMPLX NOUN⟩\n⟨CMPLX NOUN⟩→⟨ARTICLE⟩⟨NOUN⟩\n⟨CMPLX VERB⟩→⟨VERB⟩| ⟨VERB⟩⟨NOUN PHRASE⟩\n⟨ARTICLE⟩→a | the\n⟨NOUN⟩→customer | discount | refund\n⟨VERB⟩→wants | requests | cancelled\n⟨PREP⟩→with\nFigure 5: Grammar deﬁning valid sentences in English, Grammar adapted from [118].\nby an automated system to determine the intention of the customer, which in this case is whether\nshe wants a refund or she wants to cancel a refund she had previously requested. Therefore, a\ngrammar can be used to detect users’ intentions while reinforcement learning can be used to select\nthe optimal sequence of substitutions during the parsing process of the input sentences. Once the\nparser program has been used to determine the grammatical role of each word in the input text\nstring, the result can be stored in a vector-type structure such as [who=user02, intention=“wants”,\ncontent=“discount”]. This vector-type representation of variables who, intention and content, can\nbe used for another program to determine the most appropriate action to be performed next. For\nexample, informing about a discount to a customer. Figure 6 outlines the procedure.\nFigure 6: Schematic view of a reinforcement learning agent designed for text understanding,\nas an example application. The language model agent acts by assigning values to a vector\nor list of variables, as a function of the utterance of a user. The current user user02 is\ncomputed from the user’s utterance. The next state is the string generated from the vector\nof variables as understood by the agent, for example, using a text generator agent (see Fig.\n7). The language processing environment will provide the agent with the states and rewards\nafter each of the interactions. The environment and the reward function are determined by\nthe language understanding task being solved, i.e., an infobot.\nAmbiguities are an important problem in language understanding. For example, the sentence\n9\n“the child observes the cat in the tree” may have two interpretations, whether the child is in\nthe tree or the cat is in the tree. This kind of ambiguity in the language is hard to solve even\nby humans. Sometimes it can be solved by using context or common sense. From the point of\nview of reinforcement learning, there is no obvious way to solve it either. One approach to this\nproblem would be to leverage the powerful text embedding vectors generated by sophisticated\nlanguage models such as GPT together with a function that rewards making corrections as learning\ninteractions go on, taking advantage of the context. GPT-based models are very good at keeping\ncontextual information. A reward function could provide a larger reward when the interpretation\nof the intent is more highly evaluated by a context metric provided by the language model.\nLanguage understanding programs approached by reinforcement learning have to deal with\nsystems that automatically interpret text or voice in the context of a complex control application,\nand use the knowledge extracted to improve control performance. Usually, the text analysis and\nthe learning of the control strategy are carried out both at the same time. For example, Vogel and\nJurafsky [130] implement a system capable to learn to execute navigational instructions expressed in\na natural language. The learning process is carried out using an apprenticeship approach, through\npairs of paths in a map and their corresponding descriptions in English. The challenge here is to\ndiscover which commands match English instructions for navigation. The correspondence is learned\napplying reinforcement learning and using the deviation between the given desired path and the\nroute being followed for the reward signal. This work demonstrates that the semantic meaning\nof spatial terms can be grounded into geometric properties of the paths. In a similar approach\nto language grounding [7] the system learns to interpret text in the context of a complex control\napplication. Using this approach, text analysis and control strategies are learned jointly using a\nneural network and a Monte Carlo search algorithm. The approach is tested on a video game, using\nits oﬃcial manual as a text guide.\nDeep reinforcement learning has also been used to automatically play text games [50], showing\nthat it is possible to extract meaning rather than simply memorizing strings of texts. This is also\nthe case of the work presented by [44], where an LSTM and a Deep Q-Network are employed to\nsolve the sequence-to-sequence problem. This approach is tested with the problem of rephrasing a\nnatural language sentence. The encoding is performed using the LSTM and the decoding is learned\nby the DQN. The LSTM initially suggests a list of words which are taken by the DQN to learn an\nimproved rephrasing of the input sentences.\nZhu et al. [149] presented a semi-supervised approach to tackle the dual task of intent detection\nand slot ﬁlling in natural language understanding (NLU). The suggested architecture consists of a\ndual pseudo-labeling method and a dual learning algorithm. They apply the dual learning method\nby jointly training the NLU and semantic-to-sentence generation (SSG) models, using one agent\nfor each model. As the feedback rewards are non-diﬀerentiable, a reinforcement learning algorithm\nbased on policy gradient is applied for optimization. The two agents collaborate in two closed loops.\nThe NLU2SSG loop starts from a sentence, ﬁrst generating a possible semantic form by the NLU\nagent and then reconstructing the original sentence by SSG. The SSG2NLU loop goes in reverse\norder. Both the NLU and SSG models are pre-trained on labeled data. The corresponding validity\nrewards for the NLU and SSG evaluate whether the semantic forms are valid. The approach was\nevaluated on two public datasets, i.e., ATIS and SNIPS, achieving state-of-the-art performance.\nThe proposed framework is agnostic of the backbone model of the NLU task.\nText understanding is one of the most recent natural language problems approached using rein-\nforcement learning, speciﬁcally by deep reinforcement learning. This approach consists of mapping\ntext descriptions into vector representations. The main goal is to capture the semantics of the texts.\nTherefore, learning good representations is key. In this context, it has been argued that LSTMs\nare better than Bag-Of-Words (BOW) when combined with reinforcement learning algorithms. The\n10\nreason is that LSTMs are more robust to small variations of word usage, and they can learn some\nunderlying semantics of the sentences [91].\nAs we have seen above, the main applications of reinforcement learning in the context of language\nunderstanding have been focused on the learning of navigational directions.\nRL or inverse RL\nrecommend themselves over supervised learning due to the good match between sequential decision\nmaking and parsing. However, it is not diﬃcult to think of other similar applications that could\ntake advantage of this approach. For example, if we can manage to design a system capable to\nunderstand text to some degree of accuracy, such a system could be used to implement intelligent\ntutors, smart enough to understand the questions posed by the user and select the most appropriate\nlearning resource, whether it is some text, audio, video, hyperlink, etc.\nInterestingly, the successful results recently obtained with the combination of deep neural net-\nworks and reinforcement learning algorithms open another dimension of research that appears to be\npromising in the context of parsing and text understanding. As we have mentioned before, creating\nnatural language models is diﬃcult because natural languages are large and constantly changing.\nWe think that deep reinforcement learning (DRL) could become the next best approach to natural\nlanguage parsing and understanding. Our reasoning is based primarily on two facts. First, DRL can\nstore optimally thousands of parameters of the grammars as a neural model, and we have already\nevidence that these neural models can be very eﬀective with other natural language problems such\nas machine translation. Second, reinforcement learning methods would allow the agent to keep\nadapting to changes in a natural language, since the very nature of these algorithms is to learn\nthrough interaction and this feature allows the reinforcement learning agents to constantly adapt\nto changes in their environment.\n4\nText generation systems\nText generation systems are built to automatically generate valid sentences in natural language.\nOne of the components of such systems is a language model. Once the language model is provided\nor learned, the optimization problem consists of generating valid sequences of substrings that will\nsubsequently complete a whole sentence with some meaning in the domain of the application.\nGiven a vector representation of a set of variables in a computational system and their corre-\nsponding values, a reinforcement learning algorithm can be used to generate a sentence in English,\nor any other natural language, that can serve to communicate speciﬁc and meaningful information\nto a human user. However, using the information stored in a set of program variables and construct-\ning sentences in a natural language representing such information is not an easy task. This problem\nhas been studied in the context of generating navigational instructions for humans, where the ﬁrst\nstep is to decide about the content that the system wants to communicate to the human, and the\nsecond step is to build the correct instructions adding word by word. An interesting point in this\napproach is that the reward function is implemented as a hidden Markov model [31] or as a Bayesian\nnetwork [30]. The reinforcement learning process is carried out with a hierarchical algorithm using\nsemi-MDP’s.\nText generation has also been approached using inverse reinforcement learning (IRL) [150] and\ngenerative adversarial networks (GANs) [39]. Shi et al. [111] proposed a new method combining\nGANs and IRL to generate text. The main result of this work is the alleviation of two problems\nrelated to generative adversarial models, namely reward sparsity and mode collapse. The authors\nof this work also introduced new evaluation measures based on BiLingual Evaluation Understudy\n(BLEU) score, designed to evaluate the quality of the generated texts in terms of matching human-\ngenerated expert translations. They showed that the use of IRL can produce more dense reward\n11\nFigure 7: Schematic view of a reinforcement learning agent designed for language generation,\nas an example application. The language model agent acts by selecting words from a relevant\nset of words, which is a function of the current state. The current state is a – possibly\nincomplete – sentence in English. The next state is the sentence resulting from appending\nthe word selected by the agent. The language processing environment will provide the agent\nwith the states and rewards after each of the interactions. Actions might take the form of\nstrings of characters such as n-grams, words, sentences, paragraphs or even full documents.\nThe environment and the reward function are determined by the language processing task\nbeing solved, i.e., text generation.\nsignals and it can also generate more diversiﬁed texts. With this approach, the reward and the\npolicy functions are learned alternately, following an adversarial model strategy. According to the\nauthors, this model can generate texts with higher quality than previous proposed methods based\nalso on GANs, such as SeqGAN [145], RankGAN [80], MaliGAN [13] and LeakGAN [43]. The\nadversarial text generation model uses a discriminator and a generator. The discriminator judges\nwhether a text is real or not, meanwhile the generator learns to generate texts by maximizing a\nreward feedback provided by the discriminator through the use of reinforcement learning.\nThe\ngeneration of entire text sequences that these adversarial models can accomplish helps to avoid the\nexposure bias problem, a known problem experienced by text generation methods based on RNNs.\nThe exposure bias problem [4] lets small discrepancies between the training and inference phases\naccumulate quickly along the generated sequence.\nIn a text generation task the corresponding MDP might be deﬁned as follows:\n• Each state in S is formed with a feature vector describing the current state of the system being\ncontrolled, containing enough information to generate the output string. We can visualize\nthis feature vector as a set of variables that describe the current status of the system.\n• Actions in A will consist of adding or deleting words.\n• With respect to the transition function T, every next state can be determined by the resulting\nstring, after we have added or deleted a word.\n• In this task, the reward function could be learned from a corpus of labeled data or more\nmanually, from human feedback.\nAn advantage of RL methods over supervised learning for text generation becomes apparent\nwhen there is a diversity of valid text output, i.e., multiple diﬀerent generations would be of equal\nquality. In this case, it is problematic for supervised learning to deﬁne a diﬀerentiable error for\n12\nbackpropagation. However, evaluation measures like BLEU or the Recall-Oriented Understudy for\nGisting Evaluation (ROUGE) can be used well to deﬁne a reward function for RL [61]. Future re-\nsearch work can focus on adaptive natural language generation during human-computer interaction,\nassuming a continuously changing learning environment. In natural language generation the main\ngoal is to build a precise model of the language, and the current existing approaches are far from\nbeing generic.\nAnother more complicated possibility is the study of language evolution under a reinforcement\nlearning perspective. In general, language evolution is concerned with how a group of agents can\ncreate their own communication system [10]. The communication system emerges from the inter-\naction of a set of agents inhabiting a common environment. A process like this can be modeled as\na reinforcement learning multi-agent system [89].\nLi et al. [78] used reinforcement learning and inverse reinforcement learning for paraphrase\ngeneration.\nOne of the components of this approach is a generator.\nThe generator is initially\ntrained using deep learning and then it is ﬁne-tuned using RL. The reward of the generator is given\nby a second component of the architecture, the evaluator. The evaluator is a deep model trained\nusing inverse RL to evaluate whether two given phrases are similar to each other.\n5\nMachine translation\nMachine translation (MT) consists in automatically translating sentences from one natural language\nto another one, using a computing device [57]. An MT system is a program that receives text\n(or speech) in some language as input and automatically generates text (or speech), with the\nsame meaning, but in a diﬀerent language (see Fig. 8).\nEarly MT systems translate scientiﬁc\nand technical documents, while current developments involve online translation systems, teaching\nsystems, among others. MT systems have been successfully applied to an increasing number of\npractical problems [133]. Since 1949, when the task of machine translation was proposed to be\nsolved using computers [134], several approaches have been studied over the years.\nFigure 8: Schematic view of a reinforcement learning agent designed for language translation.\nIt gets as input a text in some language A, and responds with another text string in a diﬀerent\nlanguage B. Input and output text strings have the same meaning. The language model agent\nacts by selecting the most relevant string of words. The language processing environment will\nprovide the agent with the states and rewards after each of the interactions. The environment\nand the reward function are determined by the machine translation task being solved, i.e.,\ntranslation from English to Spanish.\nStatistical machine translation (SMT) is by far the most studied approach to machine trans-\nlation. In this paradigm, translations are generated using statistical models whose parameters are\n13\nestimated through the analysis of many samples of existing human translations, known as bilin-\ngual text corpora [8, 63, 136]. SMT algorithms are characterized by their use of machine learning\nmethods, where neural networks have been used with some success [18, 33, 60].\nIn the last decade neural networks have won the battle against statistical methods in the ﬁeld\nof translation. Neural Machine Translation (NMT) [121] uses large neural networks to predict the\nlikelihood of a sequence of words. NMT methods have been broadly applied to advance up-to-date\nphrase-based SMT systems, where a unit of translation may be a sequence of words (instead of a\nsingle word), called a phrase [64]. NMT systems became a major area of development since the\nemergence of deep neural networks in 2012 [3, 138, 48, 47, 67]. Current state-of-the-art machine\nlearning translation systems rely heavily on recurrent neural networks (RNN), such as the Long\nShort-Term Memory (LSTM) network [56]. In the sequence-to-sequence approach [123] depicted in\nFig. 9, which was used for translation [138], two recurrent neural networks are needed, an encoder\nand a decoder. The encoder RNN updates its weights as it receives a sequence of input words in\norder to extract the meaning of the sentence. Then, the decoder RNN updates its corresponding\nweights to generate the correct sequence of output words, in this case, the translated sentence. In the\nRNN approach the encoder makes reference to a program that would internally encode or represent\nthe meaning of the source text, meanwhile the decoder will decode that internal representation\nand output a translated sentence with the correct meaning. There are two problems that arise\nin the training and testing of seq2seq models.\nThese problems are known as 1) exposure bias,\ni.e., the discrepancy between ground-truth dependent prediction during training and model-output\ndependent prediction during testing, and 2) inconsistency between the training and test objectives,\ni.e., measurement. Both problems have been recently studied and various solutions based on RL\nhave been proposed [61].\nFigure 9:\nSequence-to-sequence RNN architecture for machine translation,\nadapted\nfrom [123].\nSimilarly to what can be accomplished in conversational systems, in machine translation, we see\nthat reinforcement learning algorithms can be used to predict the next word or phrase to be uttered\nby a person, specially during a simultaneous translation task, where the content is translated in\nreal-time as it is produced [36]. This prediction is useful to increase the quality and speed up the\ntranslation.\nIn the case of the training, when it is done interactively, there is evidence that reinforcement\nlearning can be used to improve the real-time translation performance after several interactions\nwith humans [40, 120, 119]. Gu et al. [41] propose an NMT model for real-time translation, where\na task-speciﬁc neural network learns to decide which actions to take (i.e., to wait for another source\nword or to emit a target word) using a ﬁxed pre-trained network and policy gradient techniques.\n14\nFurthermore, to tackle the need of massive training data in machine translation, He et al. propose\na dual learning mechanism, which automatically learns from unlabeled data [49]. This method is\nbased on the fact that using a policy gradient algorithm together with a reward function deﬁned\nas the likelihood of the language model, it is possible to create a translation model using examples\nof translation going in both directions, from language one to language two, and from language two\nto language one. With this approach it is possible to obtain an accuracy similar to the accuracy\nobtained with other neural models, but using only 10% of the total number of training examples.\nSpeech translation systems have improved recently due to simultaneous machine translation, in\nwhich translation starts before the full sentence has been observed. In traditional speech translation\nsystems, speech recognition results are ﬁrst segmented into full sentences, then machine translation\nis performed sentence-by-sentence. However, as sentences can be long, i.e., in the case of lectures\nor presentations, this method can cause a signiﬁcant delay between the speaker’s utterance and\nthe translation results, forcing listeners to wait a noticeable time until receiving the translation.\nSimultaneous machine translation avoids this problem by starting to translate before the sentence\nboundaries are detected. As a ﬁrst step in this direction, Grissom II et al. [40] propose an approach\nthat predicts next words and ﬁnal verbs given a partial source language sentence by modeling\nsimultaneous machine translation as a Markov decision process and using reinforcement learning.\nThe policy introduced in this method works by keeping a partial translation, querying an underlying\nmachine translation system and deciding to commit these intermediate translations occasionally.\nThe policy is learned through the iterative imitation learning algorithm SEARN [28]. By letting\nthe policy predict in advance the ﬁnal verb of a source sentence, this method has the potential\nto notably decrease the delay in translation from languages in which, according to their grammar\nrules, the verb is usually placed in the end of the phrases, such as German. However, the successful\nuse of RL is still very challenging, especially in real-world systems using deep neural networks and\nhuge datasets [137].\nReinforcement learning techniques have also had a positive impact in statistical machine trans-\nlation, which uses predictive algorithms to teach a computer how to translate text based on creating\nthe most probable output learned from diﬀerent bilingual text corpora. As the goal in reinforcement\nlearning is to maximize the expected reward for choosing an action at a given state in an MDP\nmodel, algorithms based on bandit feedback for SMT can be visualized as MDP’s with one state,\nwhere selecting an action represents the prediction of an output [68], [75]. Bandit feedback inherits\nthe name from the problem of maximizing the amount of rewards obtained after a sequence of plays\nwith a one-armed bandit machine, without apriori knowledge of the reward distribution function of\nthe bandit machine. Sokolov et al. [120] propose a structured prediction in SMT based on bandit\nfeedback, called bandit expected loss minimization. This approach uses stochastic optimization for\nlearning from partial feedback in the form of an expected 1–BLEU loss criterion [95], [139], as\nopposed to learning from a gold standard reference translation. This is a non-convex optimization\nproblem, which they analyzed in the stochastic gradient method of pseudogradient adaptation [102]\nthat allowed to show convergence of the algorithm. Nevertheless, the algorithm of Sokolov et al.\n[120] presents slow convergence. In other words, such a system needs many rounds of user feedback\nin order to learn in a real-world SMT. Moreover, it requires absolute feedback of translation quality.\nTherefore, Sokolov et al. [119] propose improvements with a strong convexiﬁcation of the learning\nobjective, formalized as bandit cross-entropy minimization to overcome the convergence speed prob-\nlem. They also propose a learning algorithm based on pairwise preference rankings, which simpliﬁes\nthe feedback information.\nThe same approach used for machine translation can be used in a rephrasing system [42].\nThis system receives a sentence as an input, creates an internal representation of the information\ncontained in such a sentence and then generates a second sentence with the same meaning of the\n15\nﬁrst one. The algorithms used to solve such a challenging problem are the long short-term memory\n(LSTM) and a deep Q network (DQN). The former is used to learn the representation of the input\nsentence and the latter is used to generate the output sentence. The experiments presented in this\nwork indicate that the proposed method performs very well at decoding sentences. Furthermore,\nthe algorithm signiﬁcantly outperformed the baseline when it was used to decode sentences never\nseen before, in terms of BLEU scores. The generation of the output string is not explicitly computed\nfrom a vector of variables, instead, this vector representation is implicitly learned and stored in the\nweights of the LSTM and the deep Q network. Similarly, this system does not need an explicit\nmodel of the language to do the rephrasing, because that model is also learned and stored in its\nneural networks. Therefore, the inner workings of this system are the same as a machine learning\ntranslator. It receives a string of words as input and generates another string of words with the\nsame meaning.\nThe rephrasing problem aforementioned consists in generating one string B based on some input\nstring A, in such a way that both strings have the same meaning. Considering this task we can\ndeﬁne an MDP (S, A, P, R) as proposed in [42]:\n• The set of states S is deﬁned as the set of all possible input strings wi.\n• The set of actions A consists of adding and deleting words taken from some vocabulary.\n• The transition function P can be completely determined and it is deterministic. The next\nstate is the string that results from adding or deleting a word.\n• Finally, the reward function R can be deﬁned as a function that measures how similar the\nstrings A and B are, in semantical terms.\nIn general, machine translation can be deﬁned as an optimization problem. In the particular\ncase of simultaneous translation, we can deﬁne an MDP (S, A, P, R) and solve it using reinforcement\nlearning as we explain next. Given an utterance in a language A, we need to ﬁnd the optimal\nutterance B that maximizes a measure of semantic similarity with respect to A. In this kind of\ntranslation problem, when the sentences need to be translated as fast as possible, reinforcement\nlearning can be used for learning when a part of a sentence should be trusted and used to translate\nfuture parts of the same sentence. In this way the person waiting for the translated sentence does not\nneed to wait until the translator gets the last word of the original sentence to start the translation\nprocess. Therefore, the translation process can be accelerated by predicting the next noun or verb.\nThe corresponding MDP is the following [40]:\n• Each state in S contains the string of words already seen by the translator and the next\npredicted word.\n• The actions in A are mainly of three types: to commit to a partial translation, to predict the\nnext word, or to wait for more words.\n• The transition function P, indicating the transitions from one state to another is fully deter-\nmined by the current state and the action performed. We can compute the resulting string\nafter applying an action.\n• The reward function R can be deﬁned based on the BLEU score [99], which basically measures\nhow similar one translation is compared to a reference string, which is assumed to be available\nfor training.\n16\nThere is a number of improvements that could be researched in simultaneous machine translation\nusing reinforcement learning. One is the implementation of these systems in more realistic scenarios\nwhere faster convergence is required. Currently, the experimentation with this approach has involved\nidealized situations in which the phrase to be translated contains only one verb. This constraint\nshould be dropped if we want to employ them in real-world scenarios.\nExperiments with other languages are also needed, especially for those languages that do not\nfall into the set of most spoken languages in the world. This will require the estimation of diﬀerent\noptimal MDP policies, one for each language. However, if the correct recurrent neural model can\nbe deﬁned, using reinforcement learning might help in autonomously learning machine translation.\nIn the same way that AlphaGo managed to play multiple games against itself and improved in the\nprocess, it might be the case that future translator algorithms can learn multiple natural languages\nby talking to themselves.\n6\nConversational systems\nConversational systems are designed to interact with various users using natural language, most\ncommonly in verbal or written form.\nThey are well structured and engineered to serve for in-\nstance as automated web assistance or for natural human-robot interaction. The architecture and\nfunctionality of such systems are heavily dependent on the application.\nThere are two classes of conversational systems. First, open domain systems, usually known as\nchatbots. They are built in a Turing-test fashion. This is, they can hold a conversation basically\nabout any topic, or at least they are trained with that goal in mind. Second, closed domain systems\nwhich are developed more as expert systems, in the sense that they should serve a conversational\npurpose very well deﬁned and bounded. They should be able to provide information or assistance\nabout a speciﬁc topic. In this article we are more interested in this latter system, since serving a\nwell-deﬁned task, can more easily beneﬁt from reinforcement learning, due to reduced state and\naction spaces.\nIn this section, we will see that reinforcement learning algorithms can be used to generate suitable\nresponses during a conversation with a human user. If the system can be programmed to predict\nwith some accuracy how a conversation might occur, then it can optimize the whole process in such\na way that the system can provide more information in less interactions if we are talking about a\nsystem designed to inform humans, or it can make a more interesting conversation if it is designed\nas a chatbot for entertainment. There are a number of factors that aﬀect the eﬀectiveness of a\nconversational system, including context identiﬁcation, dynamic context adaptation, user intention\n[22], and domain knowledge [55].\nConversational systems consist of three basic components whose sophistication will vary from\nsystem to system. These components are:\n1. processing of the input message (perception),\n2. the internal state representation (semantic decoder), and\n3. the actions (dialogue manager).\nThe input is a message from the user, for instance, speech, gestures, text, etc.\nThe user’s\ninput message is converted to its semantic representation by the semantic encoder. The semantic\nrepresentation of the message is further processed to determine an internal state of the system from\nwhich the next action is determined by the dialogue manager. Finally, the actions might include\nthe generation of natural speech, text or other system actions.\n17\nFigure 10: Information ﬂow of a conversational system. This system receives as input a text\nstring containing a question or simply a comment, and it responds with another text string\ncontaining the response. This input and response interaction typically iterates several times.\nGoing from “Input text string x” to “Output response string x” requires the application in\nsequence of a text understanding agent (see Fig. 4) and a text generator agent (see Fig. 7).\nConversational systems are often heuristically-driven and thus the ﬂow of conversation as well\nas the capabilities are speciﬁcally tailored to a single application. Application-speciﬁc rule-based\nsystems can achieve reasonably good performance due to the incorporation of expert domain knowl-\nedge. However, this often requires a huge number of rules, which becomes quickly intractable [55].\nDue to the limitations of rule-based systems there are ongoing eﬀorts to use data-driven or\nstatistical conversational systems based on reinforcement learning since the early 2000s [81, 73, 116,\n117, 131, 144]. In theory, these data-driven conversational systems are capable of adapting based\non interactions with real users. Additionally, they require less development eﬀort but at a cost of\nsigniﬁcant learning time. Although very promising they still need to overcome several limitations\nbefore they are adopted for real-world applications. These limitations stem from both the problem\nitself and from reinforcement learning algorithms.\nReinforcement learning could potentially be applied to all three components of a conversational\nsystem mentioned above, starting with perception of the input message, internal system representa-\ntions as well as the decision of the system’s output. However, we argue that reinforcement learning\nis more readily available for improving the dialogue manager which deals directly with the user in-\nteraction. More diﬃcult but also possible using deep RL would be the learning of suitable internal\nrepresentations based on the success of the interactions.\nIn a recent survey on neural approaches to conversational AI [37], it is recognized that in the last\nfew years, reinforcement learning together with deep learning models have helped to signiﬁcantly\nimprove the quality of conversational agents in multiple tasks and domains. Key aspects of this\ncombination of learning models are that conversational systems are allowed to adapt to diﬀerent\nenvironments, tasks, domains and even user behaviors.\nA large body of research exists for reinforcement learning-based conversational systems. For\ninstance, POMDP-based conversational systems [135, 142, 127, 143, 22] emerged as a strategy to\ncope with uncertainty originating from the perceptual and semantic decoder components. However,\nthey also suﬀer from very large state representations that often become intractable (curse of dimen-\nsionality) which typically necessitates some sort of state space compression [22]. We attribute this\nlimitation to the widespread use of discrete state space representations typical in dialogue manage-\nment and early days of reinforcement learning algorithms. We believe that such limitation could\nbe overcome with continuous state space representations and the use of function approximation\n18\ntechniques such as DQN [88], VIN [125], A3C [87], TRPO [109] and many others. Although there\nhave been attempts to use function approximation techniques within dialogue management systems\n[59, 54], these have not been scaled up. Li et al. [74] simulated a dialogues between two virtual\nagents, and sequences that display three useful conversational properties are rewarded. These prop-\nerties are: informativity, coherence, and ease of answering. This RL model uses policy gradient\nmethods.\nThe main implications of using a continuous representation of the states is that we are required\nto estimate less parameters than when we use a discrete state representation. This is the case when\nwe are dealing with large state spaces. As a result of handling less parameters the learning of policies\ncan be signiﬁcantly accelerated. Moreover, the quality of the learned policies is usually better than\nthe policies learned with discretized state spaces. When we are implementing deep reinforcement\nlearning models the number of weights in our neural network used to store the value functions can\nbe large. However, the number of parameters of a deep model is less than the number of discrete\nstates for which we would need to estimate a value.\nLemon et al. [72] showed that natural language generation problems can be solved using rein-\nforcement learning by jointly optimizing the generation of natural language and the management of\ndialogues. Another approach based on RL to improve the long-turn coherence and consistency of\na conversation is proposed in [146]. With this approach it is possible to obtain smooth transitions\nbetween task and non-task interactions. Papaioannou and Lemon [96] present a chatbot system for\ntask-speciﬁc applications. This system for multimodal human-robot interaction can generate longer\nconversations than a rule-based algorithm. This implies that the learned policy is highly successful\nin creating an engaging experience for chat and task interactions. A conversational agent can be\neﬀectively trained using a simulator [77]. After a preliminary training, the agent is deployed in the\nreal scenario in order to generate interactions with humans. During these interactions with the real\nworld the agent keeps learning. In a similar approach, Li et al. [76] used a movie booking system to\ntest a neural conversational system trained to interact with users by providing information obtained\nfrom a structured database. Interestingly, if the action spaces of the agents are treated as latent\nvariables, it is possible to induce those action spaces from the available data in an unsupervised\nlearning manner. This approach can be used to train dialogue agents using reinforcement learning\n[148].\nSome researchers have tried to develop question answering (QA) systems with multi-step rea-\nsoning capabilities, based on reinforcement learning. Though QA systems cannot be considered full\nconversational systems, both share some common challenges. DeepPath [140], MINERVA [27] and\nM-Walk [112] are recent examples of systems that perform multi-step reasoning on a knowledge\nbase through the use of reinforcement learning.\nMore recently, Yang et al. [141] presented a dialogue system that learns a policy that maxi-\nmizes a joint reward function. The ﬁrst reward term encourages topic coherence by computing the\nsimilarity between the topic representation of the generated response and that of the conversation\nhistory. The second term encourages semantic coherence between the generated response and pre-\nvious utterance by computing mutual information. The last term is based on a language model\nto estimate the grammatical correctness and ﬂuency of the generated response. Lu et al. [83] used\nHindsight Experience Replay (HER) to address the problem of sparse rewards in dialogues. HER\nallows for learning from failures and is thus eﬀective for learning when successful dialogues are rare,\nparticularly early in learning. Liu et al. [82] showed that the goal is to model understanding between\ninterlocutors rather than to simply focus on mimicking human-like responses. To achieve this goal,\na transmitter-receiver-based framework is proposed. The transmitter generates utterances and the\nreceiver measures the similarity between the built impression and the perceived persona. Mutual\npersona perception is then used as a reward to learn to generate personalized dialogues. Chen et al.\n19\n[17] proposed a structured actor-critic model to implement structured deep reinforcement learning.\nIt can learn in parallel from data taken from diﬀerent conversational tasks, achieving stable and\nsample-eﬃcient learning. The method is tested on 18 tasks of PyDial [128]. Plato et al. [98, 97]\npresented a complete attempt at concurrently training conversational agents. Such agents com-\nmunicate only via self-generated language, outperforming supervised and deep learning baselines.\nEach agent has a role and a set of objectives, and they interact using only the language they have\ngenerated.\nOne major problem regarding the building of conversational systems lies in the amount of\ntraining data needed [25] which could originate from simulations (as in most of the research), oﬄine\nlearning (limited number of interaction data sets) and learning from interactions with real users.\nIn fact, training and evaluating such systems require large amounts of data. Similarly, measuring\nthe performance of conversational systems is itself a challenge and diﬀerent ways of measuring it\nhave been proposed. One way is based on the use of some predeﬁned metrics that can be used\nas the reward function of the system, for example, some measurement of the success rate of the\nsystem, which can be calculated when the system solves the user’s problem. Another way of giving\nreward to the system is by counting the number of turns, which gives preference to more succinct\ndialogues. A more sophisticated way would be to automatically assess the sentiment of the evolving\nconversation, generating larger rewards for positive sentiment [6]. Other metrics that are being\nexplored are the coherence, diversity and personal style of a more human-like conversational system\n[37].\nAnother way of measuring the performance is through the use of human simulators. However,\nprogramming human simulators is not a trivial task. Moreover, once we have found a functional\ndialogue policy, there is no way to evaluate it without relying on heuristic methods. Some simulators\nare completely built from available data. The way they work is basically by selecting at the start\nof each training episode a randomly generated goal and a set of constraints. The performance of\nthe system is measured by comparing the sequence of contexts and utterances generated after each\nstep during the training. User simulation is not obvious and is still an ongoing research ﬁeld.\nIn general, conversational systems can be classiﬁed into two diﬀerent types: 1) task-oriented\nsystems, and 2) non-task-oriented systems. Both types of systems can be deﬁned as a general opti-\nmization problem that can be solved using reinforcement learning algorithms. An MDP (S, A, T, R)\nwith the main elements required to solve such an optimization problem is the following:\n• The set of states S is deﬁned as the history of all utterances, such as comments, questions\nand answers happening during the dialogue.\n• The set of actions A consists of all the possible sentences that the system can answer to the\nuser in the next time step.\n• The transition function T. The next state is the updated history of utterances after adding the\nlast sentence generated by the system or the user. The transition function is non-deterministic\nin the case of non-predictable user responses.\n• Finally, the reward function R can be deﬁned as a function that measures the performance\nof the system, or how similar the generated dialogue is with respect to a reference dialogue\nfrom an existing corpus.\nThe training of conversational systems could be also done using human users or using a model\nlearned from corpora of a human-computer dialogue. However, the large number of possible dialogue\nstates and strategies makes it diﬃcult to be explored without employing a simulator. Therefore,\n20\nthe development of reliable user simulators is imperative for building conversational systems, and\nthis comes with its own set of challenges.\nSimulators are in particular useful for getting eﬀective feedback from the environment during\nlearning. For instance, Schatzmann et al. [107] implemented a user simulator using a stack structure\nto represent the states. The dialogue history in this approach consists of sequences of push and pop\noperations. Experiments show the eﬀectiveness of this method to optimize a policy and it was shown\nto outperform a hand-crafted baseline strategy, in a real-world dialogue system. However, using\na simulator always has serious limitations, whether it is manually coded, learned from available\ndata, or a mixture of these approaches. A simulator is by deﬁnition not the real environment and\ntherefore a reinforcement learning policy trained on it will need some or many adjustments to make\nit work properly in the real environment. In general, the development of realistic simulators for\nreinforcement learning and the related methodologies to ﬁne-tune the policies afterwards to make\nthem generalize well in the real world is still an open question. Moreover, the reward function is key\nto providing eﬀective feedback. It is well known that the design of reward functions is a challenging\ntask that requires expert knowledge on the task to be learned and on the speciﬁc algorithm being\nused. Very often, it is only after many iterations in the design process and a signiﬁcant amount of\nexperimentation that reward functions are optimally conﬁgured. Su et al. studied reward estimation\n[122]. This approach is based on the one hand on the use of a recurrent neural network pre-trained\noﬀ-line to serve as a predictor of success and on the other hand, a dialogue policy and a reward\nfunction are trained together. The reward function is modeled with a Gaussian process using active\nlearning.\nChen et al. propose an interactive reinforcement learning framework to address the cold start\nproblem [15]. The framework, referred to as a companion teacher, consists of three parties: 1)\none learning agent, 2) a human user, and 3) a human ‘companion’ teacher. The agent (dialogue\nmanager) consists of a dialogue state tracker and a policy model. The human teacher can guide\nlearning at every turn (time step).\nThe teacher can guide learning by both reward or policy-\nshaping. The authors assume that the dialogue states and policy model are visible to the human\nteacher. In follow-up work [16], a rule-based system is used for reward- and policy-shaping, but the\nsame strategy could be used to incorporate human feedback. The learning agent is implemented\nusing a Deep Q-Network (DQN) and two separate experience memories for the agent and teacher.\nUncertainty estimation is used to control when to ask for feedback and learn from the experience\nmemories. Simulation experiments showed that the proposed approach could signiﬁcantly improve\nlearning speed and accuracy.\n7\nOther language processing tasks\nReinforcement learning has also been used for the improvement of information extraction through\nthe acquisition and incorporation of external information [92]. In this work, a deep Q-network is\ntrained to select actions based on contextual information, leading the information retrieval system to\nimprove its performance by increasing the accuracy of the retrieved documents. This approach can\nhelp to reduce the ambiguity in text interpretation. The selection of actions involves querying and\nextracting new sources of information repetitively. Actions have two components, a reconciliation\ndecision and a query choice. The reward is designed to maximize the extraction accuracy of the\nvalues, and at the same time the number of queries is minimized. The experimental work with two\ndomains shows an improvement over traditional information extractors of 5% on average.\nNews feed recommendation can be seen as a combinatorial optimization problem and therefore it\ncan be modeled as a Markov decision process. He et al. [52] studied the prediction of popular Reddit\n21\nthreads using a bi-directional LSTM architecture and reinforcement learning. Another approach to\nthe same problem involves the incorporation of global context available in the form of discussions\nfrom an external source of knowledge [51]. An interesting idea explored in this approach is the use\nof two Q-functions. The ﬁrst is used to generate a ﬁrst ranking of the actions and the second one\nis utilized to rerank top action candidates. By doing this, good actions can be selected, i.e., These\nactions could otherwise be missed due to the very skewed action space that the algorithm can deal\nwith.\nQuite often we see that dialogue systems provide semantically correct responses which are not\nnecessarily consistent with contextual facts. Mesgar et al. [85] used reinforcement learning to ﬁne-\ntune the responses, optimizing for consistency and semantics.\nGao et al. [38] approached another language processing task using reinforcement learning,\nnamely document summarization. The proposed paradigm uses learning-to-rank as a way to learn\na reward function that is later used to generate near-optimal summaries.\n8\nPromising research directions\nBased on our analysis of the problems and approaches here reported, we now take a step further and\ndescribe 9 research directions that we believe will beneﬁt from a reinforcement learning approach\nin the coming years.\n1. Recognition of the user’s input.\nWe noticed that a common element missing or at\nleast underrepresented in natural language processing research is the recognition of the user’s\ninput. Commonly, this is treated as being inherently uncertain and most research accepts\nthis and tries to cope with it without attempting to solve the source of the problems. This\nalong with all other machine perception problems are very challenging tasks and far from\nbeing solved. We argue that trying to address uncertainty of the user input at the initial\nstages would be more fruitful than simply regarding it as given.\nThus, we argue that a\nfuture research direction would be to develop a reinforcement learning approach for generating\ninternal semantic representations of the user’s message from which other ﬁelds within and\nbeyond natural language processing could beneﬁt.\n2. Internal representation learning. Learning an internal representation of language is a\nmore general research direction. By using deep neural networks and reinforcement learning\nmethods, it is possible to learn to code and decode sequences of text [42]. Although such an\narchitecture was implemented and tested only with a text rephrasing task, we believe that the\nunderlying problem of learning an internal representation of language is inherently related to\nsome of the most important NLP problems, such as text understanding, machine translation,\nlanguage generation, dialogue system management, parsing, etc.\nBy solving the internal\nrepresentation problem of language, we may partially solve the aforementioned problems to\nsome extent. Therefore, research on deep learning and reinforcement learning methods in\na joint approach is currently of great importance to advance the state of the art in NLP\nsystems.\n3. Exploitation of domain knowledge. Another interesting research path is the one aiming\nat discovering ways to enhance RL through the exploitation of domain knowledge available in\nthe form of natural language, as surveyed by Luketina et al. [84]. Some current trends involve\nmethods studying knowledge transfer from descriptive task-dependent language corpora [90].\nPre-trained information retrieval systems can be integrated with RL agents [14] to improve\n22\nthe quality of the queries. Moreover, relevant information can be extracted from sources of\nunstructured data such as game manuals [7].\n4. Exploitation of embodiment. A trend in supervised language learning research considers\nthe importance of embodiment for the emergence of language [1, 53]. Multimodal inputs, such\nas an agent knowing its actuators while performing an action, help in classifying and verbally\ndescribing an action and allows better generalisation to novel action-object combinations [34].\nEmbodied language learning has recently been brought to reinforcement learning scenarios,\nspeciﬁcally question answering where an agent needs to navigate in a scene to answer the\nquestions [126], or where it needs to perform actions on objects to answer questions [29].\nLike dialogue grounded in vision [26], such interactive scenarios extend language learning into\nmultiple modalities. Such applied scenarios also allow to introduce tasks, corresponding re-\nwards, and hence seamless integration of language learning with reinforcement learning. Deep\nreinforcement learning neural architectures are a promising research path for the processing\nof multiple modalities in embodied language learning in a dynamic world.\n5. Language evolution. From a more linguistic point of view, the study of language evolution\nusing a reinforcement learning perspective is also a fertile ﬁeld for research. This process can\nbe modelled by a multi-agent system, where a collection of agents is capable to create their own\ncommunication protocol by means of interaction with a common environment and by applying\nreinforcement learning rules [89]. This kind of research can beneﬁt from the recent advances\nin multi-agent systems and rising computational power.\nMoreover, research on cognitive\nrobotics using neural models together with reinforcement learning methods [24, 23, 103, 35, 45]\nhas reached a point where the addition of language evolution capabilities seems to be more\npromising than ever before.\n6. Word embeddings. More important, from our point of view, are the advances in neural\nlanguage models, especially those for word embedding. The recent trend of continuous lan-\nguage representations might have a huge potential if it is used together with reinforcement\nlearning. Word2vec [86] supplies a continuous vector representation of words. In a continuous\nbag-of-words architecture, Word2vec trains a simple neural network to predict a word from its\nsurrounding words, achieving on its hidden layer a low-dimensional continuous representation\nof words in some semantically meaningful topology. Other word embeddings are GloVe [100],\nwhich yields a similar performance more eﬃciently by using a co-occurrence matrix of words\nin their context, and FastText [5], which includes subword information to enrich word vectors\nand to deal with out-of-vocabulary words.\nA more powerful class of embeddings are contextualized word embeddings, which use the\ncontext, i.e., previous and following words, to embed a word. Two recent models are ELMo\n[101], which uses bidirectional LSTM, and BERT [32], which uses a deep feedforward Trans-\nformer network architecture with self-attention. Both are character-based and hence, like\nFastText, use morphological cues and deal with out-of-vocabulary words.\nBy taking into\naccount the context, they handle diﬀerent meanings of a word (e.g., “He touches a rock” vs\n“He likes rock”). However, simple word embeddings become meaning embeddings, blurring\nthe distinction between word- and sentence embeddings.\nFor the representations of utterances, Word2vec has been extended to Doc2vec [70], and\nother simple schemes are based on a weighted combination of contained word vectors [2, 104].\nHowever, since these simple bag-of-words approaches lose word-order information, the original\nsentence cannot be reconstructed. Sentence generation is also diﬃcult for supervised sentence\nembeddings such as InferSent [21] or Google’s Universal Sentence Encoder [12].\n23\nAn unsupervised approach to sentence vectors are Skip-Thought Vectors [62], which are\ntrained to reconstruct the surrounding sentences of an encoded one. A simpler model would\nbe an encoder-decoder autoencoder architecture, where the decoder reconstructs the same\nutterance that the encoder gets as input, based on a constant-length internal representation.\nHence, this is a constant size continuous vector representation of an utterance, from which\nthe utterance, which itself could consist of continuous word vectors, could also be reproduced.\nTo train utterance vectors on dialogues, large dialogue corpora exist, which can be classiﬁed\ninto human-machine or human-human; spontaneously spoken, scripted spoken, or written\n[110]. Examples are datasets of annotated telephone dialogues, movie dialogues, movie rec-\nommendation dialogues, negotiation dialogues, human-robot interaction, and also question\nanswering contains elements of dialogues.\nSuch continuous language representations could seamlessly play together with continuous\nRL algorithms like CACLA [129], Deterministic Policy Gradient (DPG) [114] or deep DPG\n(DDPG) [79]. These algorithms handle continuous state input and continuous action output.\nActions of a dialogue agent would be the agent’s utterances, which would result in a new\nstate after the response of its communication partner. Continuous utterance representations\nwould allow optimization of an action by gradient ascent to maximize certain rewards which\nexpress desired future state properties. For example, it could be desired to maximize the\npositive sentiment of an upcoming utterance which can be estimated by a diﬀerentiable neural\nnetwork [6].\nOther possible desired state properties could be to maximize a human’s excitement in order\nto motivate him to make a decision; to maximize the duration of the conversation, or lead\nit to an early end with a pleased human; to acquire certain information from, or to pass\non information to the human. However, not all goals can be easily expressed as points in\na continuous utterance space that represents a dialogue.\nTo this end, future research on\nlanguage needs to be extended towards representing more of its semantics, which entails\nunderstanding the entire situation.\n7. Intelligent conversational systems. When conversing with chatbots, it is common to end\nup in the situation where the bot starts responding with “I don’t know what you are talking\nabout” repeatedly, no matter what it is asked.\nThis problem is identiﬁed as the generic\nresponse problem.\nThe cause for this problem might be that such kind of answers occur\nvery often in the training set. Also they are highly compatible with various questions [74].\nAnother issue is when a dataset has similar responses to diﬀerent contexts [106]. One way to\nimprove the eﬃciency in reinforcement learning is through the combination of model-based\nand model-free learning [46]. We propose that this approach might be useful to solve the\ngeneric response problem.\nFurthermore, all the experience gained from working with algorithms designed for text-based\ngames and applications on learning of navigational directions can be extended and adapted\nto be useful in the implementation of intelligent tutors, smart enough to understand the\nquestions posed by the user and select the most appropriate learning resource, whether it is\nsome text, audio, video, hyperlink, etc. Those intelligent tutors can improve over time.\n8. Assessment of conversational systems.\nFinally, in conversational systems, a critical\npoint that needs further investigation is the deﬁnition of robust evaluation schemes that can\nbe automated and used to assess the quality of automatic dialogue systems. Currently, the\nperformance of such systems is measured through ad hoc procedures that depend on the\n24\nspeciﬁc application and most importantly, they require the intervention of a human, which\nmakes these systems very diﬃcult to be scaled.\n9. Document-editing RL Assistants. Kudashkina et al. [66] proposed the domain of voice\ndocument editing as a particularly well-suited one for the development of reinforcement learn-\ning intelligent assistants that can engage in a conversation. They argue that in voice document\nediting, the domain is clearly deﬁned, delimited and the agent has full access to it. These\nconditions are advantageous for an agent that learns the domain of discourse through model-\nbased reinforcement learning. Important future research questions the authors mention are,\nﬁrst, what level of ambition should the agent’s learning have? And second, how should the\ntraining of the assistant be performed, online or oﬄine?\n9\nConclusions\nWe have provided a review of the main categories of natural language processing problems that\nhave been approached using reinforcement learning methods. Some of these problems considered\nreinforcement learning as the main algorithm, such as the dialogue management systems. In others,\nreinforcement learning was used marginally, only to partially help in the solution of the central\nproblem.\nIn both cases, RL algorithms have played an important part in the optimization of\ncontrol policies through the self-exploration of the states and actions.\nWith the current advances in reinforcement learning algorithms, especially with those algorithms\nin which the value functions and policy functions are replaced with deep neural networks, it is\nimpossible not to consider that reinforcement learning will play a major role in solving some of the\nmost important natural language processing problems. Especially, we have witnessed solid evidence\nthat algorithms with self-improvement and self-adaptation capabilities have pushed the performance\nin challenging machine learning problems to the next level.\nCurrently, none of the natural language processing tasks here analyzed have reinforcement learn-\ning methods as state-of-the-art methodologies. Many of the problems are being solved with increas-\ning success using transformer neural network models such as BERT and GPT. However, we argue\nthat reinforcement learning can be jointly applied with deep neural models. Reinforcement learning\ncan provide beneﬁt by its inherent exploratory capacity. This is, reinforcement learning can help\nﬁnd better actions and better states due to its credit assignment approach. The best policies found\nby neural networks, such as transformers, can potentially get ﬁne-tuned by reinforcements.\nAcknowledgements\nThis work received partial support from the German Research Foundation (DFG) under project\nCML (TRR-169). We thank Burhan Hafez for discussions and providing references highly relevant\nto this review.\nReferences\n[1] A. Antunes, A. Laﬂaquiere, T. Ogata, and A. Cangelosi. A Bi-directional Multiple Timescales\nLSTM Model for Grounding of Actions and Verbs. In IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 2614–2621, Macau, China, Nov. 2019.\n25\n[2] S. Arora, Y. Liang, and T. Ma.\nA Simple but Tough-to-Beat Baseline for Sentence Em-\nbeddings. In International Conference on Learning Representations (ICLR), Toulon, France,\nApr. 2017. OpenReview.net.\n[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine Translation by Jointly Learning to\nAlign and Translate. In International Conference on Learning Representations (ICLR), San\nDiego, CA, USA, May 2015. arxiv.\n[4] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled Sampling for Sequence Prediction\nwith Recurrent Neural Networks. In International Conference on Neural Information Pro-\ncessing Systems (NIPS), volume 1, pages 1171–1179, Montreal, QC, Canada, Dec. 2015. MIT\nPress.\n[5] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching Word Vectors with Subword\nInformation. Transactions of the Association for Computational Linguistics, 5:135–146, Dec.\n2017.\n[6] C. Bothe, S. Magg, C. Weber, and S. Wermter. Dialogue-Based Neural Learning to Estimate\nthe Sentiment of a Next Upcoming Utterance. In A. Lintas, S. Rovetta, P. F. Verschure,\nand A. E. Villa, editors, International Conference on Artiﬁcial Neural Networks (ICANN),\nvolume 10614 of Lecture Notes in Computer Science, pages 477–485, Alghero, Italy, Sept.\n2017. Springer International Publishing.\n[7] S. R. K. Branavan, D. Silver, and R. Barzilay. Learning to Win by Reading Manuals in a\nMonte-Carlo Framework. Journal of Artiﬁcial Intelligence Research, 43:661–704, Apr. 2012.\n[8] P. F. Brown, J. Cocke, S. A. D. Pietra, V. J. D. Pietra, F. Jelinek, J. D. Laﬀerty, R. L.\nMercer, and P. S. Roossin. A Statistical Approach to Machine Translation. Computational\nLinguistics, 16(2):79–85, June 1990.\n[9] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei. Language Models Are Few-Shot Learners. In Neural Information Processing\nSystems (NeurIPS), Online Conference, Dec. 2020.\n[10] A. Cangelosi and D. Parisi, editors. Simulating the Evolution of Language. Springer-Verlag,\nLondon, 2002.\n[11] R. Cao, S. Zhu, C. Liu, J. Li, and K. Yu. Semantic Parsing with Dual Learning. In Annual\nMeeting of the Association for Computational Linguistics (ACL), volume 57th, pages 51–64,\nFlorence, Italy, July 2019. Association for Computational Linguistics.\n[12] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant, M. Guajardo-\nCespedes, S. Yuan, C. Tar, Y.-H. Sung, B. Strope, and R. Kurzweil.\nUniversal Sentence\nEncoder. arXiv:1803.11175 [cs], Apr. 2018.\n[13] T. Che, Y. Li, R. Zhang, R. D. Hjelm, W. Li, Y. Song, and Y. Bengio. Maximum-Likelihood\nAugmented Discrete Generative Adversarial Networks. arXiv:1702.07983 [cs], Feb. 2017.\n26\n[14] D. Chen, A. Fisch, J. Weston, and A. Bordes. Reading Wikipedia to Answer Open-Domain\nQuestions. In Annual Meeting of the Association for Computational Linguistics (ACL), vol-\nume 55th, pages 1870–1879, Vancouver, BC, Canada, July 2017. Association for Computa-\ntional Linguistics.\n[15] L. Chen, R. Yang, C. Chang, Z. Ye, X. Zhou, and K. Yu. On-Line Dialogue Policy Learning\nwith Companion Teaching. In Conference of the European Chapter of the Association for\nComputational Linguistics (EACL), volume 15th of Short Papers, pages 198–204, Valencia,\nSpain, Apr. 2017. Association for Computational Linguistics.\n[16] L. Chen, X. Zhou, C. Chang, R. Yang, and K. Yu. Agent-Aware Dropout DQN for Safe\nand Eﬃcient on-Line Dialogue Policy Learning.\nIn Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 2454–2464, Copenhagen, Denmark, Sept. 2017.\nAssociation for Computational Linguistics.\n[17] Z. Chen, L. Chen, X. Liu, and K. Yu. Distributed Structured Actor-Critic Reinforcement\nLearning for Universal Dialogue Management. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 28:2400–2411, 2020.\n[18] K. Cho, B. van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and\nY. Bengio.\nLearning Phrase Representations Using RNN Encoder–Decoder for Statistical\nMachine Translation. In Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1724–1734, Doha, Qatar, Oct. 2014. Association for Computational Linguis-\ntics.\n[19] N. Chomsky. On Certain Formal Properties of Grammars. Information and Control, 2(2):137–\n167, June 1959.\n[20] N. Chomsky. Aspects of the Theory of Syntax. The MIT Press, Cambridge, Mass, May 1965.\n[21] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes.\nSupervised Learning of\nUniversal Sentence Representations from Natural Language Inference Data. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 670–680, Copenhagen,\nDenmark, Sept. 2017. Association for Computational Linguistics.\n[22] P. A. Crook, S. Keizer, Z. Wang, W. Tang, and O. Lemon. Real User Evaluation of a POMDP\nSpoken Dialogue System Using Automatic Belief Compression. Computer Speech & Language,\n28(4):873–887, July 2014.\n[23] F. Cruz, S. Magg, Y. Nagai, and S. Wermter. Improving Interactive Reinforcement Learning:\nWhat Makes a Good Teacher? Connection Science, 30(3):306–325, Mar. 2018.\n[24] F. Cruz, G. I. Parisi, and S. Wermter. Multi-modal Feedback for Aﬀordance-driven Interactive\nReinforcement Learning. In International Joint Conference on Neural Networks (IJCNN),\npages 1–8, Rio de Janeiro, Brazil, July 2018.\n[25] H. Cuay´ahuitl, I. Kruijﬀ-Korbayov´a, and N. Dethlefs. Nonstrict Hierarchical Reinforcement\nLearning for Interactive Systems and Robots. ACM Transactions on Interactive Intelligent\nSystems, 4(3):15:1–15:30, Oct. 2014.\n[26] A. Das, S. Kottur, J. M. F. Moura, S. Lee, and D. Batra. Learning Cooperative Visual Dialog\nAgents with Deep Reinforcement Learning. In IEEE International Conference on Computer\nVision (ICCV), pages 2951–2960, Venice, Italy, Oct. 2017.\n27\n[27] R. Das, S. Dhuliawala, M. Zaheer, L. Vilnis, I. Durugkar, A. Krishnamurthy, A. Smola,\nand A. McCallum.\nGo for a Walk and Arrive at the Answer: Reasoning Over Paths in\nKnowledge Bases Using Reinforcement Learning. In International Conference on Learning\nRepresentations (ICLR), Vancouver, BC, Canada, 2018.\n[28] H. Daum´e III, J. Langford, and D. Marcu. Search-Based Structured Prediction. Machine\nLearning, 75(3):297–325, June 2009.\n[29] Y. Deng, X. Guo, N. Zhang, D. Guo, H. Liu, and F. Sun. MQA: Answering the Question via\nRobotic Manipulation. arXiv:2003.04641 [cs], Dec. 2020.\n[30] N. Dethlefs and H. Cuay´ahuitl. Combining Hierarchical Reinforcement Learning and Bayesian\nNetworks for Natural Language Generation in Situated Dialogue. In European Workshop on\nNatural Language Generation (ENLG), volume 11, pages 110–120, Nancy, France, Sept. 2011.\nAssociation for Computational Linguistics.\n[31] N. Dethlefs and H. Cuay´ahuitl. Hierarchical Reinforcement Learning and Hidden Markov\nModels for Task-Oriented Natural Language Generation. In Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Language Technologies (ACL), volume 49 of\nShort Papers, pages 654–659, Portland, OR, USA, June 2011. Association for Computational\nLinguistics.\n[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-Training of Deep Bidirectional\nTransformers for Language Understanding. In Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies (NAACL\nHLT), pages 4171–4186, Minneapolis, MN, USA, June 2019. Association for Computational\nLinguistics.\n[33] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and Robust\nNeural Network Joint Models for Statistical Machine Translation. In Annual Meeting of the\nAssociation for Computational Linguistics (ACL), volume 52nd, pages 1370–1380, Baltimore,\nMD, USA, June 2014. Association for Computational Linguistics.\n[34] A. Eisermann, J. H. Lee, C. Weber, and S. Wermter. Generalization in Multimodal Language\nLearning from Simulation. In International Joint Conference on Neural Networks (IJCNN),\npages 1–8, Shenzhen, China, 2021.\n[35] M. Eppe, P. D. H. Nguyen, and S. Wermter. From Semantics to Execution: Integrating Action\nPlanning With Reinforcement Learning for Robotic Causal Problem-Solving. Frontiers in\nRobotics and AI, 6(123), Nov. 2019.\n[36] C. F¨ugen, A. Waibel, and M. Kolss. Simultaneous Translation of Lectures and Speeches.\nMachine Translation, 21(4):209–252, Dec. 2007.\n[37] J. Gao, M. Galley, and L. Li. Neural Approaches to Conversational AI. In International ACM\nSIGIR Conference on Research & Development in Information Retrieval, volume 41st, pages\n1371–1374, Ann Arbor, MI, USA, June 2018. Association for Computing Machinery.\n[38] Y. Gao, C. Meyer, M. Mesgar, and I. Gurevych. Reward Learning for Eﬃcient Reinforce-\nment Learning in Extractive Document Summarisation. In International Joint Conference on\nArtiﬁcial Intelligence (IJCAI), 19th, pages 2350–2356, Macao, China, 2019. AAAI Press.\n28\n[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,\nand Y. Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing\nSystems (NIPS), volume 27, pages 2672–2680, Montreal, QC, Canada, Dec. 2014. Curran\nAssociates, Inc.\n[40] A. Grissom II, H. He, J. Boyd-Graber, J. Morgan, and H. Daum´e III. Don’t Until the Final\nVerb Wait: Reinforcement Learning for Simultaneous Machine Translation. In Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 1342–1352, Doha,\nQatar, Oct. 2014. Association for Computational Linguistics.\n[41] J. Gu, G. Neubig, K. Cho, and V. O. Li. Learning to Translate in Real-Time with Neural\nMachine Translation. In Conference of the European Chapter of the Association for Com-\nputational Linguistics (EACL), volume 15th, pages 1053–1062, Valencia, Spain, Apr. 2017.\nAssociation for Computational Linguistics.\n[42] H. Guo. Generating Text with Deep Reinforcement Learning. In NIPS Deep Reinforcement\nLearning Workshop, Montreal, QC, Canada, 2015.\n[43] J. Guo, S. Lu, H. Cai, W. Zhang, Y. Yu, and J. Wang. Long Text Generation Via Adver-\nsarial Training with Leaked Information. Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 32(1):5141–5148, Apr. 2018.\n[44] X. Guo, T. Klinger, C. Rosenbaum, J. P. Bigus, M. Campbell, B. Kawas, K. Talamadupula,\nG. Tesauro, and S. Singh. Learning to Query, Reason, and Answer Questions on Ambiguous\nTexts.\nIn International Conference on Learning Representations (ICLR), Toulon, France,\nApr. 2017.\n[45] M. B. Hafez, C. Weber, M. Kerzel, and S. Wermter. Deep Intrinsically Motivated Continuous\nActor-Critic for Eﬃcient Robotic Visuomotor Skill Learning. Paladyn, Journal of Behavioral\nRobotics, 10(1):14–29, Jan. 2019.\n[46] M. B. Hafez, C. Weber, M. Kerzel, and S. Wermter. Improving Robot Dual-System Motor\nLearning with Intrinsically Motivated Meta-Control and Latent-Space Experience Imagina-\ntion. Robotics and Autonomous Systems, 133:103630, Nov. 2020.\n[47] H. Hassan, A. Aue, C. Chen, V. Chowdhary, J. Clark, C. Federmann, X. Huang, M. Junczys-\nDowmunt, W. Lewis, M. Li, S. Liu, T.-Y. Liu, R. Luo, A. Menezes, T. Qin, F. Seide, X. Tan,\nF. Tian, L. Wu, S. Wu, Y. Xia, D. Zhang, Z. Zhang, and M. Zhou. Achieving Human Parity\non Automatic Chinese to English News Translation. arXiv:1803.05567 [cs], June 2018.\n[48] D. He, H. Lu, Y. Xia, T. Qin, L. Wang, and T.-Y. Liu. Decoding with Value Networks for\nNeural Machine Translation. In International Conference on Neural Information Processing\nSystems (NIPS), volume 30th, pages 177–186, Long Beach, CA, USA, Dec. 2017. Curran\nAssociates Inc.\n[49] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T.-Y. Liu, and W.-Y. Ma. Dual Learning for Machine\nTranslation. In Advances in Neural Information Processing Systems (NIPS), volume 29, pages\n820–828, Barcelona, Spain, Dec. 2016.\n[50] J. He, J. Chen, X. He, J. Gao, L. Li, L. Deng, and M. Ostendorf.\nDeep Reinforcement\nLearning with a Natural Language Action Space. In Annual Meeting of the Association for\nComputational Linguistics (ACL), volume 54, pages 1621–1630, Berlin, Germany, Aug. 2016.\nAssociation for Computational Linguistics.\n29\n[51] J. He, M. Ostendorf, and X. He.\nReinforcement Learning with External Knowledge and\nTwo-Stage Q-Functions for Predicting Popular Reddit Threads. arXiv:1704.06217 [cs], Apr.\n2017.\n[52] J. He, M. Ostendorf, X. He, J. Chen, J. Gao, L. Li, and L. Deng. Deep Reinforcement Learning\nwith a Combinatorial Action Space for Predicting Popular Reddit Threads. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 1838–1848, Austin, TX,\nUSA, Nov. 2016. Association for Computational Linguistics.\n[53] S. Heinrich, Y. Yao, T. Hinz, Z. Liu, T. Hummel, M. Kerzel, C. Weber, and S. Wermter.\nCrossmodal Language Grounding in an Embodied Neurocognitive Model. Frontiers in Neu-\nrorobotics, 14, 2020.\n[54] J. Henderson, O. Lemon, and K. Georgila. Hybrid Reinforcement/Supervised Learning of\nDialogue Policies from Fixed Data Sets. Computational Linguistics, 34(4):487–511, July 2008.\n[55] R. Higashinaka, M. Mizukami, K. Funakoshi, M. Araki, H. Tsukahara, and Y. Kobayashi.\nFatal or Not? Finding Errors That Lead to Dialogue Breakdowns in Chat-Oriented Dialogue\nSystems. In Conference on Empirical Methods in Natural Language Processing (EMNLP),\npages 2243–2248, Lisbon, Portugal, Sept. 2015. Association for Computational Linguistics.\n[56] S. Hochreiter and J. Schmidhuber.\nLong Short-Term Memory.\nNeural Computation,\n9(8):1735–1780, Nov. 1997.\n[57] W. J. Hutchins and H. L. Somers. An Introduction to Machine Translation. Academic Press,\nLondon, Apr. 1992.\n[58] J. Jiang, A. Teichert, J. Eisner, and H. Daum´e III. Learned Prioritization for Trading OﬀAc-\ncuracy and Speed. In Advances in Neural Information Processing Systems (NIPS), volume 25,\nLake Tahoe, NV, USA, Dec. 2012.\n[59] F. Jurcicek, B. Thomson, S. Keizer, F. Mairesse, M. Gasic, K. Yu, and S. J. Young. Natural\nBelief-Critic: A Reinforcement Algorithm for Parameter Estimation in Statistical Spoken Dia-\nlogue Systems. In Annual Conference of the International Speech Communication Association\n(INTERSPEECH), pages 90–93, Makuhari, Japan, Sept. 2010.\n[60] N. Kalchbrenner and P. Blunsom. Recurrent Continuous Translation Models. In Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 1700–1709, Seattle,\nWA, USA, Oct. 2013. Association for Computational Linguistics.\n[61] Y. Keneshloo, T. Shi, N. Ramakrishnan, and C. K. Reddy. Deep Reinforcement Learning for\nSequence-to-Sequence Models. IEEE Transactions on Neural Networks and Learning Systems,\n31(7):2469–2489, July 2020.\n[62] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. Skip-\nThought Vectors. In Advances in Neural Information Processing Systems (NIPS), volume 28,\npages 3294–3302, Montreal, QC, Canada, 2015. Curran Associates, Inc.\n[63] P. Koehn. Statistical Machine Translation. Cambridge University Press, Cambridge ; New\nYork, Dec. 2009.\n30\n[64] P. Koehn, F. J. Och, and D. Marcu. Statistical Phrase-Based Translation. In Conference\nof the North American Chapter of the Association for Computational Linguistics on Human\nLanguage Technology (HLT-NAACL), pages 48–54, Edmonton, AB, Canada, May 2003. As-\nsociation for Computational Linguistics.\n[65] S. K¨ubler, R. McDonald, and J. Nivre. Dependency Parsing. Synthesis Lectures on Human\nLanguage Technologies, 2(1):1–127, Dec. 2008.\n[66] K. Kudashkina, P. M. Pilarski, and R. S. Sutton. Document-Editing Assistants and Model-\nBased Reinforcement Learning as a Path to Conversational AI. arXiv:2008.12095 [cs], Aug.\n2020.\n[67] T. K. Lam, S. Schamoni, and S. Riezler. Interactive-Predictive Neural Machine Translation\nThrough Reinforcement and Imitation. In Proceedings of Machine Translation Summit XVII\nVolume 1: Research Track, pages 96–106, Dublin, Ireland, Aug. 2019. European Association\nfor Machine Translation.\n[68] J. Langford and T. Zhang. The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits.\nIn Advances in Neural Information Processing Systems (NIPS), volume 20th, pages 817–824,\nVancouver, BC, Canada, 2007. Curran Associates Inc.\n[69] M. Lˆe and A. Fokkens. Tackling Error Propagation Through Reinforcement Learning: A Case\nof Greedy Dependency Parsing. In Conference of the European Chapter of the Association\nfor Computational Linguistics (EACL), volume 1, pages 677–687, Valencia, Spain, Apr. 2017.\nAssociation for Computational Linguistics.\n[70] Q. Le and T. Mikolov. Distributed Representations of Sentences and Documents. In Inter-\nnational Conference on Machine Learning (ICML), volume 32nd, pages 1188–1196, Beijing,\nChina, June 2014. PMLR.\n[71] Y. LeCun, Y. Bengio, and G. Hinton. Deep Learning. Nature, 521(7553):436–444, May 2015.\n[72] O. Lemon. Learning What to Say and How to Say It: Joint Optimisation of Spoken Dialogue\nManagement and Natural Language Generation. Computer Speech & Language, 25(2):210–\n221, Apr. 2011.\n[73] E. Levin, R. Pieraccini, and W. Eckert. A Stochastic Model of Human-Machine Interaction for\nLearning Dialog Strategies. IEEE Transactions on Speech and Audio Processing, 8(1):11–23,\nJan. 2000.\n[74] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky. Deep Reinforcement Learning\nfor Dialogue Generation. In Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1192–1202, Austin, TX, USA, Nov. 2016. Association for Computational\nLinguistics.\n[75] L. Li, W. Chu, J. Langford, and R. E. Schapire. A Contextual-Bandit Approach to Per-\nsonalized News Article Recommendation. In International Conference on World Wide Web\n(WWW), volume 19th, pages 661–670, Raleigh, NC, USA, Apr. 2010. Association for Com-\nputing Machinery.\n[76] X. Li, Y.-N. Chen, L. Li, J. Gao, and A. Celikyilmaz. End-to-End Task-Completion Neu-\nral Dialogue Systems.\nIn International Joint Conference on Natural Language Processing\n31\n(IJCNLP), pages 733–743, Taipei, Taiwan, Nov. 2017. Asian Federation of Natural Language\nProcessing.\n[77] X. Li, Z. C. Lipton, B. Dhingra, L. Li, J. Gao, and Y.-N. Chen.\nA User Simulator for\nTask-Completion Dialogues. arXiv:1612.05688 [cs], Nov. 2017.\n[78] Z. Li, X. Jiang, L. Shang, and H. Li.\nParaphrase Generation with Deep Reinforcement\nLearning. In Conference on Empirical Methods in Natural Language Processing (EMNLP),\npages 3865–3878, Brussels, Belgium, 2018. Association for Computational Linguistics.\n[79] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.\nContinuous Control with Deep Reinforcement Learning. arXiv:1509.02971, Sept. 2015.\n[80] K. Lin, D. Li, X. He, Z. Zhang, and M.-t. Sun. Adversarial Ranking for Language Generation.\nIn Advances in Neural Information Processing Systems (NIPS), volume 30, Long Beach, CA,\nUSA, Dec. 2017. Curran Associates, Inc.\n[81] D. J. Litman, M. S. Kearns, S. P. Singh, and M. A. Walker. Automatic Optimization of Di-\nalogue Management. In International Conference on Computational Linguistics (COLING),\nvolume 18th, pages 502–508, Saarbr¨ucken, Germany, Aug. 2000. Association for Computa-\ntional Linguistics.\n[82] Q. Liu, Y. Chen, B. Chen, J.-G. Lou, Z. Chen, B. Zhou, and D. Zhang. You Impress Me:\nDialogue Generation Via Mutual Persona Perception.\nIn Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL), volume 58th, pages 1417–1427, Online, July 2020.\nAssociation for Computational Linguistics.\n[83] K. Lu, S. Zhang, and X. Chen. Goal-Oriented Dialogue Policy Learning from Failures. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence, 33(01):2596–2603, July 2019.\n[84] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas, E. Grefenstette, S. Whiteson,\nand T. Rockt¨aschel. A Survey of Reinforcement Learning Informed by Natural Language.\nIn International Joint Conference on Artiﬁcial Intelligence (IJCAI), 28th, pages 6309–6317,\nMacau, China, Aug. 2019.\n[85] M. Mesgar, E. Simpson, and I. Gurevych. Improving Factual Consistency Between a Response\nand Persona Facts. In Conference of the European Chapter of the Association for Computa-\ntional Linguistics (EACL), volume Main Volume, pages 549–562, Online, 2021. Association\nfor Computational Linguistics.\n[86] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient Estimation of Word Representations\nin Vector Space. arXiv:1301.3781 [cs], Sept. 2013.\n[87] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In International\nConference on Machine Learning (ICML), volume 48 of 33, pages 1928–1937, New York, NY,\nUSA, June 2016. Proceedings of Machine Learning Research (PMLR).\n[88] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou,\nH. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-Level Control Through\nDeep Reinforcement Learning. Nature, 518(7540):529–533, Feb. 2015.\n32\n[89] I. Mordatch and P. Abbeel. Emergence of Grounded Compositional Language in Multi-Agent\nPopulations. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 32(1), Apr. 2018.\n[90] K. Narasimhan, R. Barzilay, and T. Jaakkola. Grounding Language for Transfer in Deep\nReinforcement Learning. Journal of Artiﬁcial Intelligence Research, 63:849–874, Dec. 2018.\n[91] K. Narasimhan, T. D. Kulkarni, and R. Barzilay. Language Understanding for Text-Based\nGames Using Deep Reinforcement Learning. In Conference on Empirical Methods for Natural\nLanguage Processing (EMNLP), pages 1–11, Lisbon, Portugal, Sept. 2015. Association for\nComputational Linguistics.\n[92] K. Narasimhan, A. Yala, and R. Barzilay.\nImproving Information Extraction by Acquir-\ning External Evidence with Reinforcement Learning. In Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 2355–2365, Austin, TX, USA, Nov. 2016.\nAssociation for Computational Linguistics.\n[93] G. Neu and C. Szepesv´ari. Training Parsers by Inverse Reinforcement Learning. Machine\nLearning, 77(2):303, Apr. 2009.\n[94] A. Y. Ng and S. J. Russell. Algorithms for Inverse Reinforcement Learning. In International\nConference on Machine Learning (ICML), volume 17th, pages 663–670, Stanford, CA, USA,\nJune 2000. Morgan Kaufmann Publishers Inc.\n[95] F. J. Och.\nMinimum Error Rate Training in Statistical Machine Translation.\nIn Annual\nMeeting on Association for Computational Linguistics (ACL), volume 1 of 41st, pages 160–\n167, Sapporo, Japan, July 2003. Association for Computational Linguistics.\n[96] I. Papaioannou and O. Lemon. Combining Chat and Task-Based Multimodal Dialogue for\nMore Engaging HRI: A Scalable Method Using Reinforcement Learning. In ACM/IEEE In-\nternational Conference on Human-Robot Interaction (HRI), pages 365–366, Vienna, Austria,\nMar. 2017. ACM.\n[97] A. Papangelis, M. Namazifar, C. Khatri, Y.-C. Wang, P. Molino, and G. Tur. Plato Dialogue\nSystem: A Flexible Conversational AI Research Platform. arXiv:2001.06463 [cs], Jan. 2020.\n[98] A. Papangelis, Y.-C. Wang, P. Molino, and G. Tur.\nCollaborative Multi-Agent Dialogue\nModel Training Via Reinforcement Learning. In Annual SIGdial Meeting on Discourse and\nDialogue (SIGDIAL), volume 20th, pages 92–102, Stockholm, Sweden, Sept. 2019. Association\nfor Computational Linguistics.\n[99] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: A Method for Automatic Evaluation\nof Machine Translation. In Annual Meeting of the Association for Computational Linguistics\n(ACL), volume 40th, pages 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association\nfor Computational Linguistics.\n[100] J. Pennington, R. Socher, and C. Manning. GloVe: Global Vectors for Word Representation.\nIn Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–\n1543, Doha, Qatar, 2014. Association for Computational Linguistics.\n[101] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep\nContextualized Word Representations. In Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (NAACL-HLT),\n33\npages 2227–2237, New Orleans, LA, USA, June 2018. Association for Computational Linguis-\ntics.\n[102] B. T. Poljak. Pseudogradient Adaptation and Training Algorithms. Avtomatika i Teleme-\nhanika, 3:45–68, 1973.\n[103] F. R¨oder, M. Eppe, P. D. H. Nguyen, and S. Wermter. Curious Hierarchical Actor-Critic\nReinforcement Learning. In International Conference on Artiﬁcial Neural Networks (ICANN),\nLecture Notes in Computer Science, pages 408–419, Bratislava, Slovakia, Sept. 2020. Springer\nInternational Publishing.\n[104] A. R¨uckl´e, S. Eger, M. Peyrard, and I. Gurevych. Concatenated Power Mean Word Em-\nbeddings as Universal Cross-Lingual Sentence Representations. arXiv:1803.01400 [cs], Sept.\n2018.\n[105] S. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach. Pearson, Harlow, 3rd\nedition, 2010.\n[106] C. Sankar and S. Ravi. Deep Reinforcement Learning for Modeling Chit-Chat Dialog with\nDiscrete Attributes. In Annual SIGdial Meeting on Discourse and Dialogue, volume 20th,\npages 1–10, Stockholm, Sweden, Sept. 2019. Association for Computational Linguistics.\n[107] J. Schatzmann and S. Young. The Hidden Agenda User Simulation Model. IEEE Transactions\non Audio, Speech, and Language Processing, 17(4):733–747, May 2009.\n[108] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez,\nE. Lockhart, D. Hassabis, T. Graepel, T. Lillicrap, and D. Silver. Mastering Atari, Go, Chess\nand Shogi by Planning with a Learned Model. Nature, 588(7839):604–609, Dec. 2020.\n[109] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust Region Policy Optimiza-\ntion. In International Conference on Machine Learning (ICML), volume 37, pages 1889–1897,\nLille, France, July 2015. Proceedings of Machine Learning Research (PMLR).\n[110] I. V. Serban, R. Lowe, P. Henderson, L. Charlin, and J. Pineau.\nA Survey of Available\nCorpora For Building Data-Driven Dialogue Systems: The Journal Version.\nDialogue &\nDiscourse, 9(1):1–49, May 2018.\n[111] Z. Shi, X. Chen, X. Qiu, and X. Huang.\nToward Diverse Text Generation with Inverse\nReinforcement Learning. In International Joint Conference on Artiﬁcial Intelligence (IJCAI),\nvolume 27th, pages 4361–4367, Stockholm, Sweden, July 2018.\n[112] H.-y. Shum, X.-d. He, and D. Li. From Eliza to XiaoIce: Challenges and Opportunities with\nSocial Chatbots. Frontiers of Information Technology & Electronic Engineering, 19(1):10–26,\nJan. 2018.\n[113] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrit-\ntwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham,\nN. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and\nD. Hassabis. Mastering the Game of Go with Deep Neural Networks and Tree Search. Na-\nture, 529(7587):484–489, Jan. 2016.\n34\n[114] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic Policy\nGradient Algorithms. In International Conference on Machine Learning (ICML), volume 32 of\n31, pages 387–395, Beijing, China, 2014. Proceedings of Machine Learning Research (PMLR).\n[115] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,\nL. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche,\nT. Graepel, and D. Hassabis. Mastering the Game of Go Without Human Knowledge. Nature,\n550(7676):354–359, Oct. 2017.\n[116] S. Singh, M. Kearns, D. J. Litman, and M. A. Walker. Empirical Evaluation of a Reinforce-\nment Learning Spoken Dialogue System. In National Conference on Artiﬁcial Intelligence\n(AAAI), volume 17, pages 645–651, Austin, TX, USA, Aug. 2000. AAAI Press.\n[117] S. P. Singh, D. Litman, M. Kearns, and M. Walker. Optimizing Dialogue Management with\nReinforcement Learning: Experiments with the NJFun System. Journal of Artiﬁcial Intelli-\ngence Research, 16:105–133, Feb. 2002.\n[118] M. Sipser. Introduction to the Theory of Computation. Course Technology Cengage Learning,\nBoston, MA, 3rd edition, 2013.\n[119] A. Sokolov, J. Kreutzer, C. Lo, and S. Riezler. Learning Structured Predictors from Bandit\nFeedback for Interactive NLP. In Annual Meeting of the Association for Computational Lin-\nguistics (ACL), volume 54th, pages 1610–1620, Berlin, Germany, Aug. 2016. Association for\nComputational Linguistics.\n[120] A. Sokolov, S. Riezler, and T. Urvoy. Bandit Structured Prediction for Learning from Partial\nFeedback in Statistical Machine Translation. In Proceedings of MT Summit XV, pages 160–\n171, Miami, FL, USA, Nov. 2015. Association for Machine Translation in the Americas.\n[121] F. Stahlberg.\nNeural Machine Translation: A Review.\nJournal of Artiﬁcial Intelligence\nResearch, 69:343–418, Oct. 2020.\n[122] P.-H. Su, M. Gaˇsi´c, and S. Young. Reward Estimation for Dialogue Policy Optimisation.\nComputer Speech & Language, 51:24–43, Sept. 2018.\n[123] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence Learning with Neural Networks.\nIn Advances in Neural Information Processing Systems (NIPS), volume 27, pages 3104–3112,\nMontreal, QC, Canada, Dec. 2014. Curran Associates, Inc.\n[124] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Adaptive Com-\nputation and Machine Learning Series. The MIT Press, Cambridge, MA, 2nd edition, Nov.\n2018.\n[125] A. Tamar, Y. WU, G. Thomas, S. Levine, and P. Abbeel. Value Iteration Networks. In\nAdvances in Neural Information Processing Systems (NIPS), volume 29, pages 2154–2162,\nBarcelona, Spain, Dec. 2016. Curran Associates, Inc.\n[126] S. Tan and H. Liu. Towards Embodied Scene Description. In Robotics: Science and Systems,\nCorvalis, OR, USA, 2020. RSS Foundation.\n[127] B. Thomson and S. Young. Bayesian Update of Dialogue State: A POMDP Framework for\nSpoken Dialogue Systems. Computer Speech & Language, 24(4):562–588, Oct. 2010.\n35\n[128] S.\nUltes,\nL.\nM.\nRojas-Barahona,\nP.-H.\nSu,\nD.\nVandyke,\nD.\nKim,\nI.\nCasanueva,\nP. Budzianowski, N. Mrkˇsi´c, T.-H. Wen, M. Gaˇsi´c, and S. Young. PyDial: A Multi-Domain\nStatistical Dialogue System Toolkit. In Proceedings of System Demonstrations, volume 55th,\npages 73–78, Vancouver, BC, Canada, July 2017. Association for Computational Linguistics.\n[129] H. van Hasselt and M. A. Wiering. Reinforcement Learning in Continuous Action Spaces.\nIn IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning\n(ADPRL), pages 272–279, Honolulu, HI, USA, Apr. 2007.\n[130] A. Vogel and D. Jurafsky. Learning to Follow Navigational Directions. In Annual Meeting\nof the Association for Computational Linguistics (ACL), volume 48 of ACL, pages 806–814,\nUppsala, Sweden, July 2010. Association for Computational Linguistics.\n[131] M. A. Walker. An Application of Reinforcement Learning to Dialogue Strategy Selection in\na Spoken Dialogue System for Email. Journal of Artiﬁcial Intelligence Research, 12:387–416,\n2000.\n[132] C. J. C. H. Watkins. Learning from Delayed Rewards. Dissertation, Cambridge University,\nMay 1989.\n[133] A. Way. Quality Expectations of Machine Translation. In J. Moorkens, S. Castilho, F. Gas-\npari, and S. Doherty, editors, Translation Quality Assessment: From Principles to Practice,\nvolume 1 of Machine Translation: Technologies and Applications, pages 159–178. Springer\nInternational Publishing, Cham, 2018.\n[134] W. Weaver. Translation. In W. N. Locke and A. D. Booth, editors, Machine Translation of\nLanguages: Fourteen Essays, pages 15–23. The MIT Press, Cambridge, MA, May 1955.\n[135] J. D. Williams and S. Young. Partially Observable Markov Decision Processes for Spoken\nDialog Systems. Computer Speech & Language, 21(2):393–422, Apr. 2007.\n[136] P. Williams, R. Sennrich, M. Post, and P. Koehn. Syntax-Based Statistical Machine Transla-\ntion, volume 9 of Synthesis Lectures on Human Language Technologies. Morgan & Claypool\nPublishers, Aug. 2016.\n[137] L. Wu, F. Tian, T. Qin, J. Lai, and T.-Y. Liu. A Study of Reinforcement Learning for Neural\nMachine Translation. In Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 3612–3621, Brussels, Belgium, Oct. 2018. Association for Computational\nLinguistics.\n[138] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao,\nQ. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu,  L. Kaiser, S. Gouws, Y. Kato,\nT. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa,\nA. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean. Google’s Neural Machine\nTranslation System: Bridging the Gap Between Human and Machine Translation. Computing\nResearch Repository (CoRR) in arXiv, abs/1609.08144:23, 2016.\n[139] J. Wuebker, S. Muehr, P. Lehnen, S. Peitz, and H. Ney. A Comparison of Update Strategies\nfor Large-Scale Maximum Expected BLEU Training. In Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies\n(NAACL HLT), pages 1516–1526, Denver, CO, USA, May 2015. Association for Computa-\ntional Linguistics.\n36\n[140] W. Xiong, T. Hoang, and W. Y. Wang.\nDeepPath: A Reinforcement Learning Method\nfor Knowledge Graph Reasoning. In Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 564–573, Copenhagen, Denmark, Sept. 2017. Association for\nComputational Linguistics.\n[141] M. Yang, W. Huang, W. Tu, Q. Qu, Y. Shen, and K. Lei. Multitask Learning and Reinforce-\nment Learning for Personalized Dialog Generation: An Empirical Study. IEEE Transactions\non Neural Networks and Learning Systems, 32(1):49–62, Jan. 2021.\n[142] S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. The\nHidden Information State Model: A Practical Framework for POMDP-Based Spoken Dialogue\nManagement. Computer Speech & Language, 24(2):150–174, Apr. 2010.\n[143] S. Young, M. Gaˇsi´c, B. Thomson, and J. D. Williams. POMDP-Based Statistical Spoken\nDialog Systems: A Review. Proceedings of the IEEE, 101(5):1160–1179, May 2013.\n[144] S. J. Young. Probabilistic Methods in Spoken-Dialogue Systems. Philosophical Transactions:\nMathematical, Physical and Engineering Sciences, 358(1769):1389–1402, Apr. 2000.\n[145] L. Yu, W. Zhang, J. Wang, and Y. Yu. SeqGAN: Sequence Generative Adversarial Nets with\nPolicy Gradient. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 31(1):2852–\n2858, Feb. 2017.\n[146] Z. Yu, A. Rudnicky, and A. Black. Learning Conversational Systems that Interleave Task\nand Non-Task Content. In International Joint Conference on Artiﬁcial Intelligence (IJCAI),\nvolume 26th, pages 4214–4220, Melbourne, VIC, Australia, 2017.\n[147] L. Zhang and K. P. Chan. Dependency Parsing with Energy-Based Reinforcement Learning.\nIn International Conference on Parsing Technologies (IWPT), volume 11th, pages 234–237,\nParis, France, Oct. 2009. Association for Computational Linguistics.\n[148] T. Zhao, K. Xie, and M. Eskenazi. Rethinking Action Spaces for Reinforcement Learning in\nEnd-to-end Dialog Agents with Latent Variable Models. In Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies\n(NAACL HLT), volume 1, pages 1208–1218, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics.\n[149] S. Zhu, R. Cao, and K. Yu. Dual Learning for Semi-Supervised Natural Language Understand-\ning. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:1936–1947,\n2020.\n[150] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum Entropy Inverse Reinforce-\nment Learning. In National Conference on Artiﬁcial Intelligence (AAAI), volume 3 of 23rd,\npages 1433–1438, Chicago, IL, USA, July 2008. AAAI Press.\n37\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-04-12",
  "updated": "2022-03-15"
}