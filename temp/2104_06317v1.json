{
  "id": "http://arxiv.org/abs/2104.06317v1",
  "title": "Probing Negative Sampling Strategies to Learn GraphRepresentations via Unsupervised Contrastive Learning",
  "authors": [
    "Shiyi Chen",
    "Ziao Wang",
    "Xinni Zhang",
    "Xiaofeng Zhang",
    "Dan Peng"
  ],
  "abstract": "Graph representation learning has long been an important yet challenging task\nfor various real-world applications. However, their downstream tasks are mainly\nperformed in the settings of supervised or semi-supervised learning. Inspired\nby recent advances in unsupervised contrastive learning, this paper is thus\nmotivated to investigate how the node-wise contrastive learning could be\nperformed. Particularly, we respectively resolve the class collision issue and\nthe imbalanced negative data distribution issue. Extensive experiments are\nperformed on three real-world datasets and the proposed approach achieves the\nSOTA model performance.",
  "text": "Probing Negative Sampling Strategies to Learn Graph\nRepresentations via Unsupervised Contrastive Learning\nShiyi Chen1, Ziao Wang2, Xinni Zhang3, Xiaofeng Zhang4 †, and Dan Peng5\nHarbin Institute of Technology (Shenzhen), China\n{19s1510831,20s0510543,pengdan5}@stu.hit.edu.cn,\nwangziao19932@hotmail.com,zhangxiaofeng4@hit.edu.cn\nAbstract. Graph representation learning has long been an important yet chal-\nlenging task for various real-world applications. However, their downstream tasks\nare mainly performed in the settings of supervised or semi-supervised learning.\nInspired by recent advances in unsupervised contrastive learning, this paper is\nthus motivated to investigate how the node-wise contrastive learning could be\nperformed. Particularly, we respectively resolve the class collision issue and the\nimbalanced negative data distribution issue. Extensive experiments are performed\non three real-world datasets and the proposed approach achieves the SOTA model\nperformance.\nKeywords: Graph neural network · contrastive learning · negative sampling.\n1\nIntroduction\nIn the literature, various graph neural network (GNN) models have been proposed for\ngraph analysis tasks, such as node classiﬁcation [14], link prediction [35] and graph\nclassiﬁcation [33]. Generally, most existing GNN-based approaches are proposed to\ntrain, in a supervised manner, graph encoder to embed localized neighboring nodes or\nnode attributes for a graph node into the low-dimensional feature space. By convoluting\nK-hops neighboring nodes, adjacent nodes naturally have similar feature representa-\ntions. Notably, the consequent downstream tasks inevitably rely on the quality of the\nlearnt node embeddings.\nFor many real graph applications, e.g., protein analysis [37], it intuitively requires an\nunavoidable cost or even the specialized domain knowledge to manually annotate suf-\nﬁcient data to well train the graph encoders by minimizing the loss associated with the\nspeciﬁed supervised learning task. Alternatively, a number of milestone unsupervised\nrandom walk based GNNs, including but not limited to node2vec [8] and graph2vec\n[21], are consequently proposed towards training the universal node embeddings and\nthen various supervised downstream tasks are directly applied on these node embed-\ndings. Similarly, another line of unsupervised graph representation learning approaches,\ni.e., graph kernel based methods, also utilizes the graph structural information to em-\nbed graph nodes with similar structures into similar representations. Most recently, the\ncontrasive learning [11,12] is originally proposed to learn feature embeddings for each\n† Contact Author\narXiv:2104.06317v1  [cs.LG]  13 Apr 2021\nimage in a self-supervised manner. To this end, these proposed approaches ﬁrst generate\ntwo random augmentations for the same image and deﬁne these two as a pair of positive\nsamples, and simply treat samples augmented from other images as negative samples.\nThen, the contrastive loss is designed to maximize the mutual information between each\npair of positive samples. The learnt embeddings are believed to well preserve its inher-\nent discriminative features. Research attempts are then made to adapt the successful\ncontrastive learning approaches to unsupervised graph representation learning prob-\nlem [10,30]. In [30], the graph-level representation is built to contrast with each node\nrepresentation to acquire node representations ﬁtting for diverse downstream tasks. [10]\nadopts diffusion graph as another view of the given graph, and the contrast is performed\nbetween graph representation of one view and node representation of another view. As\nall the node embeddings are forced to approximate the same graph representation. In-\ntuitively, a coarser level graph analysis task, e.g., graph classiﬁcation, would beneﬁt a\nlot from such kind of contrastive learning, whereas a ﬁne-grained level task, e.g., node\nclassiﬁcation, might not beneﬁt that large.\nTo address aforementioned research gap, this work is thus motivated to investigate\nwhether the contrastive learning could be effectively carried on in a node-wised manner.\nThat is, for each graph node x to be embedded, our desired contrastive learning is to\nmaximize the mutual information between x and its positive examples x+ instead of a\ngraph representation, and simultaneously to minimize the mutual information between\nx and its negative examples x−. Meanwhile, there exist two research challenges to be\naddressed. First, the sampled negative examples x−might contain the true positive ex-\nample x+ which is known as class collision issue. Second, how the density of negative\nsamples will affect the contrastive learning has not been studied. We assume that the\nunderlying true positive examples could be statistically similar to x, i.e., unseen positive\nexamples should obey the same prior probability distribution as x. Similarly, the multi-\nple typed negative examples x−are assumed to obey different probability distributions.\nWith this assumption, the class collision issue could be intuitively resolved by remov-\ning those examples from the set of x−they are more likely generated by the assumed\npositive data distribution. For the second point, it can be known from the contrasitive\nloss that x will be farther away from feature area with dense x−than area with sparse\nx−. The density distribution of x−is used as factor to determine the distance of x and\nx−is questionable. Therefore, after removing negative examples in doubt, a subset of\nnegative examples should be diversely sampled to form x−for the contrastive learning.\nThus, this paper proposed an adaptive negative sampling strategy for the learning of the\nnode embedding in a node-wised contrastive learning manner. The major contribution\nof this paper are summarized as follows.\n– To the best of our knowledge, this paper is among the ﬁrst attempts to propose a\nnode-wise contrastive learning approach to learn node embedding in an unsuper-\nvised manner. In the proposed approach, positive samples and negative samples are\nassumed to obey different data distributions, and the class collision issue could be\naddressed by eliminating in doubt negative samples if they are more likely gener-\nated by a positive data distribution.\n– We propose a determinantal point process based negative instances sampling strat-\negy which is believed to be able to sample diverse negative examples to be con-\ntrasted to learn node embedding.\n– We perform extensive experiments on several benchmark datasets and the promis-\ning results have demonstrated that the proposed approach is superior to both base-\nline and the state-of-the-art approaches.\nThe rest of this paper is organized as follows. Section 2 reviews related work and\nthen we formulate the problem in Section 3. The proposed approach is detailed in Sec-\ntion 4. Experimental results are reported in Section 5 and we conclude the paper in\nSection 6.\n2\nRelated Work\n2.1\nGraph Representation Learning\nSupervised Methods The earlier graph representation learning attempts have been\nmade in the supervised settings. ChebyNet [4] and GraphWave [31] leverage graph\nFourier transformations to convert graph signals into spectral domain. Kipf and Welling\n[14] propose graph convolutional network (GCN) via a localized ﬁrst-order approxima-\ntion to ChebyNet [4], and extend graph convolution operations to the aggregation of\nneighbor nodes. To further the success of GCN, GAT [29] and GeniePath [16] are pro-\nposed to sample more informative neighbor nodes for the convolutions. There also exist\nsome approaches targeting at resolving efﬁciency issue [9,3].\nUnsupervised Methods The unsupervised graph representation learning methods could\nbe classiﬁed into random walk-based methods [28,23] and graph kernel-based methods\n[2,26]. The random walk-based methods are applied for each graph node and the nodes\nin a sequence of walk are to be encoded. By dosing so, the neighboring nodes generally\nare trained to have similar embeddings regardless of the graph structural information as\nwell as the node attributes. Such kinds of methods are usually transductively performed\nand thus need to be re-trained to represent the unseen nodes which inevitably limits\ntheir wide applicability.\n2.2\nContrastive Learning and Negative Sampling\nContrastive learning is recently proposed to learn feature embeddings in a self-supervised\nmanner. The quality of the learnt embeddings largely replies on the generated positive\ninstance set and the negative instance set. Accordingly, various approaches have been\nproposed with a focus on constructing positive samples. In the domain of NLP, [17]\ntreats the contextual sentences as positive pairs. For the domain of image recognition, a\ngood number of research works are proposed to train encoders to discriminate positive\nsamples from negative samples. For graph data, [8] encodes graph nodes using a gen-\neralized proximity and treats these nodes not appearing in w steps of a random walk or\nwithout common neighbors as negative examples. In [25], two nodes that have a similar\nlocal structure are considered as positive sample pairs despite the node position and\nits attributes. Then, several SOTA approaches have been proposed to adapt contrastive\nlearning on graph data [30,27,10,24]. DGI [30] maximizes the mutual information be-\ntween the node embeddings and graph embeddings.\nInfoGraph [27] treats nodes that are virtually generated by shufﬂing feature matrix\nas negative samples. Mvgrl [10] further deﬁnes two views on graph data and graph\nencoders are trained to maximize mutual information between node representations of\none view and graph representations of another view and vice versa. [24] consider two\nsubgraphs augmented from the same r-ego network as a positive instance pair and these\nsubgraphs from different r-ego as negative sample pair, where r-ego represents the\ninduced subgraph containing the set of neighbor nodes of a given node v within r steps.\n3\nPreliminaries and Problem Formulation\nIn this section, we ﬁrst brieﬂy review the Determinantal Point Process (DPP) [18]\nadopted to diversely sample negative instances, then we describe the notations as well\nas the problem setup.\n3.1\nDeterminantal Point Process\nThe original DPP is to proposed to model negatively correlated random variables, and\nthen it is widely adopted to sample a subset of data where each datum in this set is\nrequired to be correlated with the speciﬁed task, and simultaneously is far away from\neach other. Formally, let P denote a probability distribution deﬁned on a power set 2Y\nof a discrete ﬁnite point set Y = {1, 2, ..., M}. Y ∼P is a subset composed of data\nitems randomly generated from P. Let A be a subset of Y and B ∈RM×M be a real\npositive semi-deﬁnite similarity matrix, then we have\nP(A ⊆Y ) = |BA|,\n(1)\nwhere BA is a sub-matrix of B indexed by the elements of subset A. | · | denotes\nthe determinant operator. If A = {i}, P(A ⊆Y ) = Bi,i; and if A = {i, j}, P(A ∈Y )\ncan be written as\nP(A ⊆Y ) =\n\f\f\f\f\nBi,i Bi,j\nBj,i Bj,j\n\f\f\f\f = P(i ∈Y )P(j ∈Y ) −B2\ni,j,\n(2)\nThus, the non-diagonal matrix entries represent the correlation between a pair of\ndata items. The larger the value of Bi,j, the less likely that i and j appear at the same\ntime. Accordingly, the diversity of entries in the subset A could be calculated. As for our\napproach, DPP is adapted to sample a evenly distributed subset from negative instance\nset.\n3.2\nNotations and Deﬁnitions\nLet G = (V, E) denote a graph, V denote the node set containing N nodes, E ⊆V ×V\ndenote the edge set where e = (vi, vj) ∈E denotes an edge between two graph nodes,\nX = {x1, ..., xd} denote the node feature set where xi ∈Rdin represents the features\nof node vi. The adjacency matrix is denoted as A ∈RN×N, where Aij = 1 represents\nthat there is an edge between vi and vj in the graph and 0 otherwise. For a given node\nvi, its K-hops neighbor set is denoted as NK(vi) containing all neighboring nodes of vi\nwithin K hops, deﬁned as NK = {vj : d(vi, vj) ≤K} where d(vi, vj) is the shortest\npath distance between vi and vj in the graph G. Then, the induced subgraph is deﬁned\nas follows.\nDeﬁnition 1. Induced subgraph s. Given G = (V, E), a subgraph s = (V ′, E′) of G\nis said to be an induced subgraph of G if all the edges between the vertices in V ′ belong\nto E′.\n3.3\nProblem Setup\nGiven a G, our goal is to train a graph encoder G : RN×F ×RN×N →RN×d, such that\nG(X, A) = H = {h1, ..., hN} represents the low-dimensional feature representations,\nwhere hi denotes the embeddings of vi. Then, the learnt G is used to generate node\nembeddings for downstream tasks, e.g. node classiﬁcation.\nThe purpose of our approach is to maximize the mutual information between a pair\nof positive instances, and minimize the mutual information between a pair of negative\nand positive instances simultaneously. Similar to the infoNCE [22], the general form of\nour unsupervised contrastive learning is to minimize the contrastive loss, given as\nL = −\nN\nX\ni=1\nlog\nef(hq\ni ,hk\ni )/τ\nef(hq\ni ,hk\ni )/τ + PN\nj̸=i ef(hq\ni ,hk\nj )/τ ,\n(3)\nwhere f(·, ·) is the score function to score the agreement of two feature embeddings, and\nin our approach the score function is simply the dot product calculated as f(hq\ni , hk\nj ) =\nhq\ni · hk\nj\nT , and τ is the temperature hyper-parameter. hk\ni is a positive instance of hq\ni ,\nand two of them are usually deﬁned as two different random augmentations of the same\ndata. In our model, we deﬁne hq\ni , hk\ni are two random augmented embeddings of vi. Note\nthat the ﬁrst term of the denominator is to maximize the mutual information between a\npair of positive instances, and the second term is to minimize the mutual information\nbetween a pair of negative and positive instances. By optimizing Eq. 3, the employed\nmodel is believed to be able to learn the most discriminative features that are invariant\nto positive instances.\nNevertheless, there are some problems to be address. Intuitively, we hope that two\nnodes that have the same label in the downstream task should have similar embeddings.\nHowever, we treat hk\nj̸=i as negative sample and try to be away from it in feature space\nwhile vj may have the same label as vi, which is called class collision. At the same\ntime, we noticed that the current negative sampling strategy ignores the inﬂuence of\nthe density of embedding distribution of negative samples. A node embedding will be\nupdated to be farther away from feature subspace where its negative nodes are more\ndensely distributed. Therefore, we designed approach to adaptive sampling negative\nsamples to avoid those problems described above.\n4\nThe Proposed Approach\n4.1\nGraph Embeddings\nThe proposed node-wised contrastive learning scheme allows various choices of the\ngraph neural network architectures. We opt for simplicity reason and adopt the com-\nmonly adopted graph convolution network (GCN) [14] as our graph encoder G.\nAugmentation By following [24], we ﬁrst employ a k-steps random walk on G starting\nfrom a speciﬁc node vi, and a sequence of walking nodes seqi = {t1, ..., tk} is used to\nform the set of vertices V ′. The subgraph si induced by V ′ is regarded as a random\naugmentation of node vi. Then, we repeat aforementioned procedure and eventually we\ngenerate two induced subgraphs sq\ni , sk\ni , those embeddings are respectively denoted as\nhq\ni and hk\ni and regarded as a positive pair.\nEncoder The employed GCN layers are deﬁned as σ(eAiXiW) which is used to embed\nnode vi, where eAi = bD\n−1\n2\ni\nbAi bD\n−1\n2\ni\n∈Rni×ni is symmetrically normalized adjacency\nmatrix of a subgraph si. bDi is the degree matrix of bAi = Ai + Ini, where Ai is the\noriginal adjacency matrix of si, Xi ∈Rni×d is the initial features of nodes in si, W ∈\nRdin×d is network parameters, σ is a ReLU [7] non-linearity and ni is the number of\nnodes contained in si. Putting eAi, Xi into graph layer to perform convolution operation\nand then we could acquire node embeddings Hi ∈Rni×d of subgraph si.\nReadout After convolution operation of GNN layers, we feed the embedding set Hi\ninto the readout function R(·) to compute an embedding of vi. The readout function\nadopted in the experiments is given as follows\nR(Hi) = σ( 1\nni\nni\nX\nj=1\nhi,j + max(Hi)),\n(4)\nwhere hi,j represents the j-th node embedding in Hi, max(·) simply takes the largest\nvector along the row-wise and σ is the non-linear sigmoid function. Eventually, the\nnode embedding is acquired as hi = R(Hi). The node encoding process is illustrated\nin Algorithm 1.\n4.2\nResolving Class Collision\nGiven a node vi, its negative and positive sample set are respectively denoted as S−\ni =\n{v1, ..., vi−1, vi+1, vN} and S+\ni = {vi}. To alleviate the class collision issue, it is de-\nsired to discover those “in doubt” negative samples that are more likely to belong to the\nsame class of vi. The overall procedure is depicted as below.\nFirst, we assume that S+\ni and S−\ni respectively obey different prior probability dis-\ntributions. The “in-doubt” negative examples are discovered if they are more likely to\nAlgorithm 1: Generate embeddings of the augmented instances\nInput: graph G, adjacency matrix A, feature matrix X,\ngraph encoder G, score function f, readout function R,\nrandom-walk operator RW, subgraph sampler Γsub, concatenator ∥\nOutput: The node embeddings Hq ∈RN×d, Hk ∈RN×d\n1 Initialize A and X;\n2 for i=1 to N do\n3\nsq\ni = RW(vi), sk\ni = RW(vi)\n4\nAq\ni , Xq\ni = Γsub(sq\ni , A, X)\n5\nAk\ni , Xk\ni = Γsub(sk\ni , A, X)\n6\nHq\ni = G(Aq\ni , Xq\ni ), Hk\ni = G(Ak\ni , Xk\ni )\n7\nhq\ni = R(Hq\ni ), hk\ni = R(Hk\ni )\n8 end\n9 return Hq =\nN\r\r\ni=1\nhq\ni , Hk =\nN\r\r\ni=1\nhk\ni\nbe generated by the data distribution of positive instances. To ﬁt the embedding distri-\nbution p+\ni and p−\ni , we employ two independent neural networks, i.e., F+\ni and F−\ni to ﬁt\nthe distributions. If the probability that vj belongs to S+\ni is higher than the probability\nof being a negative instance, i.e., p+\ni (vj)\np−\ni (vj) > α, we remove vj from S−\ni , where α is the\nsoft-margin to discriminate an instance. Detailed steps are illustrated in the following\nparagraphs.\nForming the sample set S+\ni and S−\ni\nInitially, the positive instance of a given node\nvi is also augmented by vi plotted in orange and the rest nodes plotted in light-blue\nare considered as negative instances, which is shown in Figure 1(a). Apparently, not all\nthe blue data are the true negative instances. To consider the unsupervised settings, it\nis acceptable to assume that the “closest” node to vi should have the same underlying\nclass label. Therefore, a few nearest neighbor nodes vj ∈NK are transited to S+\ni from\nS−\ni . Using the mixup algorithm [34], more positive samples will be generated to further\naugment the positive instance set S+\ni . and the results are illustrated in Figure 1(b).\nFitting the positive and negative instance distribution With the augmented S+\ni and\nS−\ni , we employ two independent two neural networks F+\ni , F−\ni\nto respectively ﬁt the\nembeddings distribution for v ∈S+\ni and v ∈S−\ni as plotted in Figure 1(c). To train F+\ni ,\nwe treat it as a classiﬁer, and data belong to S+\ni is assigned with a virtual label class 1,\nand data belong to S−\ni is virtually assigned with class 0. On the contrary, to train F−\ni ,\nwe will assign label class 1 to the data belonging to S−\ni and assign label class 0 to the\ndata belonging to S+\ni .\nFor a node vj ∈S−\ni , the output of F+\ni (hk\nj ) and F+\ni (hk\nj ) are respectively the proba-\nbility that vj is a positive or negative instance of vi. The ratio of these two probabilities\nwith a soft-margin, calculated as p+\ni (vj)\np−\ni (vj) > α, is adopted to determine whether vj should\n(a) Initial stage: target node v (in red).\n(b) The augmented positive instances.\n(c) Fit the embedding distribution for\npositive and negative samples.\n(d) Resample the negative samples.\nFig. 1. Process of generating positive and negative samples by our approach. In each subﬁgure,\nthe left depicts the graph topological structure and the right plots the feature embedding space\nwhere each colored dot represent the embeddings of a positive or negative data node. In the initial\nstage, plotted in (a), there is only one target node treated as positive node (in red) and the rest are\nnegative nodes (in blue). In (b), the positive nodes are augmented by adding “in-doubt” nodes (in\norange dot) and mixup positive nodes (in orange triangle), and from the right ﬁgure, it is noticed\nthat the embeddings of orange dots and triangles are close to that of the target node (red dot).\nAnd orange and blue nodes could be well separated which indicates two distributions could be\nestimated. In (c), the underlying positive instance distribution and negative instance distribution\ncould be well ﬁt using these data. In (d), the dashed loop is the contour of\np+\ni (vj)\np−\ni (vj). Note that the\nsmaller the orange dashed loop, the more conﬁdent that datum falling a positive instance.\nbe removed from S−\ni or not, and this soft-margin is plotted in the small orange dashed\ncircle as shown in Figure 1(d).\n4.3\nSampling Diverse Negative Examples\nAs illustrated in Fig 2, we can regard the process of contrastive learning as the inter-\naction of forces between positive and negative samples. For the worst case of using all\ninstances in S−\ni for comparison, where the embedding distribution is seriously imbal-\nanced as Fig 2(a), the updated hi will be farther away from the feature subspace where\nnegative samples densely distributed. Intuitively, the comparison result between posi-\ntive and negative samples should not be related to the density of negative samples. To\ncope with this distorted updata result, we adapt the Determinant point process (DPP)\n[18] to our problem. In Fig. 2(b), the DPP algorithm is applied to S−to sample a nega-\ntive instances subset, where sampled negative instances spread across the entire feature\n(a) Unevenly sampled negative instances.\n(b) Diversely sampled negative instances.\nFig. 2. The illustration of the effect of different sampling strategies. The dashed circle denotes that\nthe corresponding node will be sampled. The pink colored dot is the embedding of target node v.\n(a) shows that for a current strategy, using all negatives, the learnt embeddings of v will close to\npurple node whereas we desire that the embeddings of v should, simultaneously, stay away from\nall negative instances as much as possible, as plotted in (b) where a diverse sampling strategy is\napplied, i.e. the proposed DPP strategy, on the embeddings space and reasonably ignores their\ndistribution.\nspace. In this case, the node embedding can avoid the inﬂuence of the density of feature\nspace and be evenly away from each negative sample. We set the correlation between hq\ni\nand each negative instance in S−\ni is equally set to a constant. To calculate the similarity\nbetween negative instances, the Euclidean distance is adopted to measure the pair-wise\ndistance, computed as d(hq\ni , hk\nj ) =\nqPd\nl=1(hq\ni,l −hk\nj,l)2.\n4.4\nNode-wise Contrastive Learning Loss\nAs pointed in [12], different nodes contribute differently to the unsupervised contrastive\nlearning. We are therefore inspired to further differentiate the importance of the di-\nversely sampled negative instances to our node-wise contrastive learning. For those\nnegative instances that are far away from the query instance hq\ni , the contributions of\nthese nodes are rather limited as they could be easily distinguished w.r.t. hq\ni . However\nfor those close negative instances, it is hard for the model to discriminate them and thus\ntheir contributions should be assigned with higher weights.\nAccordingly, the weight of the j-th negative instance’s embedding hk\nj w.r.t. the i-\nth positive instance’s embeddings hq\ni is calculated as wi,j = hq\ni · hk\nj /τw, where τw is\na temperature hyper-parameter. Thus, the overall node-wise contrastive loss could be\nwritten as\nL = −\nN\nX\ni=1\nlog\nef(hq\ni ,hk\ni )\nef(hq\ni ,hk\ni ) + P\nj∈S−\ni wi,jef(hq\ni ,hk\nj ) .\n(5)\n5\nExperimental Results\nIn this section, we ﬁrst brieﬂy introduce experimental datasets, evaluation metrics as\nwell as the experimental settings. Then, to evaluate the model performance, we not\nonly compare ours method with unsupervised models, but also some supervised mod-\nels to fully demonstrate the effectiveness of our approach. Extensive experiments are\nevaluated on several real-world datasets to answer following research questions:\nAlgorithm 2: Generating positive instance set and negative instance set\nInput: Adjacency matrix A, node embeddings Hq, Hk,\nhyper-parameter K, hyper-parameter α,\nK-hops neighbor node set {NK(v1), ..., NK(vN)},\nDPP sampler Γdpp, mixup operator Mix,\nnode embedding set S = {hk\n1, ..., hk\nN},\nneural network set F + = {F+\n1 , ..., F +\nN}, F −= {F−\n1 , ..., F −\nN }.\nOutput: Positive samples sets {S+\n1 , ..., S+\nN},\nnegative samples sets {S−\n1 , ..., S−\nN}.\n1 initialization;\n2 for i = 1 to N do\n3\nS−\ni = S \\ {hk\ni , i ∈NK(vi)}\n4\nS+\ni = {hk\ni , i ∈NK(vi)}\n5\nS+\ni = Mix(S+\ni )\n6 end\n7 for i = 1 to N do\n8\np−\ni = F −\ni (S−\ni )\n9\np+\ni = F +\ni (S+\ni )\n10\nfor j = 1 to N do\n11\nif j /∈NK(vi) and\np+\ni (vj)\np−\ni (vj) > α then\n12\nS−\ni = S−\ni \\ {hk\nj },\n13\nS+\ni = S+\ni ∪{hk\nj }\n14\nend\n15\nend\n16\nS−\ni = Γdpp(S−\ni )\n17 end\n18 return {S+\n1 , ..., S+\nN}, {S−\n1 , ..., S−\nN}\n– RQ1: Whether the proposed approach outperforms the state-of-the-art supervised\nand unsupervised methods or not?\n– RQ2: Whether the proposed components could affect the model performance or\nnot (ablation study)?\n– RQ3: Whether the proposed approach is sensitive to model parameters or not?\n– RQ4: The visualization results of the learnt item embeddings.\n5.1\nExperimental Setup\nDatasets In the experiments, three widely adopted real-world datasets are adopted to\nevaluate the model performance including Cora [19], Citeseer [5] and Pubmed [20].\nWe follow the work [29] to partition each dataset into training set, validation set and\ntest set. The statistics of these datasets are reported in Table 1.\nBaseline models To evaluate the model performance of the proposed approach on node\nclassiﬁcation task, both the unsupervised and supervised methods are compared in the\nexperiments.\nDataset\n# of Nodes\n# of Edges\n# of Features\n# of Classes\nCore\n2708\n5429\n1433\n7\nCiteseer\n3327\n4732\n3703\n6\nPubmed\n19717\n44338\n500\n3\nTable 1. The statistics of experimental datasets.\nAvailable data\nMethod\nCora\nCiteseer\nPubmed\nX,Y\nRaw features\n55.1%\n46.5%\n71.4%\nA,Y\nLP\n68.0%\n45.3%\n63.0%\nX,A,Y\nGCN\n81.5%\n70.3%\n79.0%\nX,A,Y\nChebyshev\n81.2%\n69.8%\n74.4%\nX,A,Y\nGAT\n83.0%\n72.5%\n79.0%\nX,A,Y\nGeniePath\n75.5%\n64.3%\n78.5%\nX,A,Y\nJK-Net\n82.7%\n73.0%\n77.9%\nX,A,Y\nMixHop\n81.9%\n71.4%\n80.8%\nA\nDeepwalk\n67.2%\n43.2%\n65.3%\nX,A\nDeepwalk+features\n70.7%\n51.4%\n74.3%\nX,A\nGAE\n71.5%\n65.8%\n72.1%\nX,A\nGraphSAGE\n68.0%\n68.0%\n68.0%\nX,A\nDGI\n82.3%\n71.8%\n76.8%\nX,S\nDGI\n83.8%\n72.0%\n77.9%\nX,A,S\nMvgrl\n86.8%\n73.3%\n80.1%\nX,A\nours w/o all\n83.5%\n69.3%\n80.6%\nX,A\nours\n84.3%\n73.5%\n81.5%\nTable 2. The average node classiﬁcation results for both supervised and unsupervised models.\nThe available data column highlights the data available to each model during the model training\nprocess (X:features, A:adjacency matrix, S:diffusion matrix, Y:labels).\nThe unsupervised models we used in the experiment are as follows\n– Deepwalk [23] ﬁrst samples related nodes via a truncated random walk, and then\nconstructs negative examples to learn node embeddings. This approach is consid-\nered as the baseline method. We also compare our results to DeepWalk with the\nfeatures concatenated, denoted as DeepWalk+features.\n– GAE [15] is considered as the SOTA approach which applies variational auto-\nencoder to graphs ﬁrstly.\n– GraphSAGE [9] is proposed to learn a function for generating low-dimensional\nembeddings by aggregating the embeddings of more informative neighbor nodes.\nWe use the unsupervised loss function mentioned in [9] to train the model.\n– DGI [30] is considered as the SOTA unsupervised learning approach which max-\nimizes the mutual information between the node-level and the graph-level feature\nembeddings.\n– Mvgrl [10] is the SOTA self-supervised method proposed to learn node level em-\nbeddings by optimizing the contrast between node representations and view-level\ngraph representations.\nThe semi-supervised models we used in the experiment are as follows\n– LP [36] assigns labels to unlabeled samples and is considered as a baseline method.\n– GCN [14] is one of the milestone GNN models originally proposed for node clas-\nsiﬁcation problem. And the model is trained by minimizing the supervised loss.\n– Chebyshev [4] designs the convolution kernels using the Chebyshev inequality to\nspeed up the Fourier transformation for the graph convolution process, and is con-\nsidered as the baseline.\n– GAT [29] is essentially an attention based approach. GAT designs a multi-head self-\nattention layer to assign weights to different feature embeddings of graph nodes,\nand is also treated as the baseline.\n– GeniePath [16] samples neighboring nodes which contribute a lot to the target node\nvia a hybrid of BFS and DFS search strategy.\n– JK-Net [32] adaptively uses different neighborhood ranges for each node to per-\nform aggregation operations.\n– MixHop [1] proposes to perform multi-order convolution to learn general mixing\nof neighborhood information.\nSetting of Model Parameters Our task is to ﬁrst train the node embeddings and then\ndirectly evaluate its node classiﬁcation ability. We set the same experimental settings as\nthe SOTA [30,10] and report the mean classiﬁcation results on the testing set after 50\nruns of training followed by a linear model. We initialize the parameters using Xavier\ninitialization [6] and train the model using Adam optimizer [13] with an initial learning\nrate of 0.001. We follow the same settings as DGI does and set the number of epochs\nto 2000. We vary the the batch size from 50 to 2000, and the early stopping with a\npatience of 20 is adopted. The embedding dimension is set to 512. Unlike DGI, we use\ntwo layers of GCN. We set the step of random walk as 25, soft-margin α as 0.9, dropout\nrate as 0.7.\n5.2\nRQ1: Performance Comparison\nIn this experiment, we ﬁrst learn the node embeddings and then use these embeddings\nto directly evaluate the node classiﬁcation task, and the results are reported in Table 2.\nObviously, the proposed approach achieves the best results both in comparison with un-\nsupervised models or semi-supervised models, except for Cora dataset where the Mvgrl\nachieves the best results, and ours is the second best one. Particularly, the accuracy on\nPubmed model, which has the most nodes, is improved by 81.5%. As the Mvgrl method\ncould make full use of global diffusion information, the model performance is expected\nto be superior to ours. But due to the use of diffusion information, Mvgrl cannot be\nused in inductive way, which limits its applicability. However, it is well noticed that\nour approach is better than the SOTA DGI trained with S. This veriﬁes the effective-\nness of our approach. It is also noteworthy that our model has already performed well\non the Cora and Pubmed datasets without adding any modules, that is, only applying\nnode-level comparison between nodes.\n5.3\nRQ2: Ablation Study\nIn this experiment, we investigate the effectiveness of the proposed component. We\nrespectively remove the component of soft-margin sampling, DPP sampling and node\nweights, and report the results in Table 3.\nVariants\nCora\nCiteseer\nPubmed\nours w/o all\n83.5%\n69.3%\n80.6%\nours with α\n83.8%\n70.9%\n80.8%\nours with DPP\n83.9%\n71.8%\n81.2%\nours with w\n83.8%\n70.1%\n80.9%\nours\n84.3%\n73.5%\n81.5%\nTable 3. The ablation study results. In this table, ours w/o all denotes that we remove all proposed\ncomponents. And ours with α, ours with DPP, ours with w denote the model with soft-margin\nsampling, DPP sampling and node weights, respectively.\nEffect of node weight w From Table 3, it is noticed that if removing all compo-\nnents, the model performance is the worst. If the model is integrated with node weight,\nthe model performance slightly increases. This indicates that the contribution of node\nweight is not signiﬁcant. The reason may be the weight function we designed is too\nsimple.\nEffect of distribution of p+\ni and p−\ni\nIt is noticed that the model performance of “ours\nwith α”, i.e., the positive and negative instance sets are generated by the learnt data dis-\ntribution, improves. This partially veriﬁes the effectiveness of the proposed component.\nEffect of DPP sampling It could be observed that the “ours with DPP” achieves the\nsecond best results w.r.t. all evaluation criteria. This veriﬁes our proposed assumption\nthat the data distribution of negative examples is a key factor in affecting the model\nperformance.\n5.4\nRQ3: Parameter Analysis\nIn the section, we evaluate how the model parameters, e.g., the step of random walk,\nsoft-margin α and batch size, affect the model performance, and the corresponding\nresults are plotted in Figure 3.\nFrom this ﬁgure, we have following observations. First, we highlight that our model\nis insensitive to parameter “batch size” and “random walk length ”, as shown in Fig\n3(a) and 3(b). Second, our model is obviously sensitive to the soft-margin parameter\nα which controls the ratio of of selecting potential positive samples from the negative\nsamples set . It is also well noticed that when α is close to 1, the model performance\ndramatically drops. This veriﬁes that negative samples sampling is crucial to the model\nperformance of the contrastive learning.\n(a) Results on batch size\n(b) Results on random walk\nlength\n(c) Results on soft-margin α\nFig. 3. Parameter analysis results.\n(a) Raw features\n(b) Mvgrl\n(c) Ours\nFig. 4. Visualizing the learnt embeddings of nodes on Pubmed dataset. In this ﬁgure, each color\nrepresents one underlying true class, and each colored point represent the embeddings of a node.\n5.5\nRQ4: Visualization Results\nDue to page limitations, we choose to visualize the node embeddings learnt on Pubmed\ndataset to provide a vivid illustration of the proposed model performance. As there are\nmany nodes in Pubmed, we randomly selected 2000 nodes to plot. We also visualized\nthe results of the Mvgrl method for comparison as this approach achieves the superior\nnode classiﬁcation results. The visualization results are plotted in Figure 4. Obviously,\nour model achieves the best visualization results on Pubmed dataset. It is noticed that\nin the raw features, most of the “red” and “yellow” class are mixed up together, which\nmakes the classiﬁcation task difﬁcult. It is also noticed that the three classes could be\nwell separated and spread over the whole data space by our model, whilst the “red”\nclass and “yellow” class are still mixed up in Figure 4(b). We can infer that our ap-\nproach could veriﬁes that node-level contrastive learning can learn informative low-\ndimensional representations.\n6\nConclusion\nIn this paper, we propose a novel node-wise contrastive learning approach to learn node\nembeddings for a supervised task. Particularly, we propose to resolve class collision\nissue by transiting the detected “in doubt” negative instances from the negative instance\nset to the positive instance set. Furthermore, a DPP-based sampling strategy is proposed\nto evenly sample negative instances for the contrastive learning. Extensive experiments\nare evaluated on three real-world datasets and the promising results demonstrate that\nthe proposed approach is superior to both the baseline and the SOTA approaches.\nReferences\n1. Abu-El-Haija, S., Perozzi, B., Kapoor, A., Alipourfard, N., Lerman, K., Harutyunyan, H.,\nSteeg, G.V., Galstyan, A.: MixHop: Higher-order graph convolutional architectures via spar-\nsiﬁed neighborhood mixing. In: Proceedings of the 36th International Conference on Ma-\nchine Learning. vol. 97, pp. 21–29 (2019)\n2. Borgwardt, K.M., Kriegel, H.P.: Shortest-path kernels on graphs. In: Fifth IEEE International\nConference on Data Mining (ICDM’05). pp. 8–pp (2005)\n3. Chen, J., Ma, T., Xiao, C.: Fastgcn: Fast learning with graph convolutional networks via\nimportance sampling. In: International Conference on Learning Representations (2018)\n4. Defferrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on graphs\nwith fast localized spectral ﬁltering. In: Advances in Neural Information Processing Systems.\nvol. 29 (2016)\n5. Giles, C.L., Bollacker, K.D., Lawrence, S.: Citeseer: An automatic citation indexing system.\nIn: Proceedings of the Third ACM Conference on Digital Libraries, Pittsburgh, PA, USA.\npp. 89–98 (1998)\n6. Glorot, X., Bengio, Y.: Understanding the difﬁculty of training deep feed forward neural net-\nworks. In: Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence\nand Statistics. vol. 9, pp. 249–256. Chia Laguna Resort, Sardinia, Italy (2010)\n7. Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectiﬁer neural networks. In: Proceedings\nof the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics. pp. 315–\n323. Fort Lauderdale, FL, USA (2011)\n8. Grover, A., Leskovec, J.: node2vec: Scalable feature learning for networks. In: Proceedings\nof the 22nd ACM SIGKDD international conference on Knowledge discovery and data min-\ning. pp. 855–864 (2016)\n9. Hamilton, W.L., Ying, R., Leskovec, J.: Inductive representation learning on large graphs. In:\nProceedings of the 31st International Conference on Neural Information Processing Systems.\npp. 1025–1035 (2017)\n10. Hassani, K., Khasahmadi, A.H.: Contrastive multi-view representation learning on graphs.\nIn: International Conference on Machine Learning. pp. 4116–4126 (2020)\n11. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp. 9729–9738 (2020)\n12. Kalantidis, Y., Sariyildiz, M.B., Pion, N., Weinzaepfel, P., Larlus, D.: Hard negative mixing\nfor contrastive learning. In: Advances in Neural Information Processing Systems. vol. 33,\npp. 21798–21809 (2020)\n13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International Con-\nference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings (2015)\n14. Kipf, T., Welling, M.: Semi-supervised classiﬁcation with graph convolutional networks. In:\nInternational Conference of Learning Representations (2017)\n15. Kipf, T.N., Welling, M.: Variational graph auto-encoders. arXiv preprint arXiv:1611.07308\n(2016)\n16. Liu, Z., Chen, C., Li, L., Zhou, J., Li, X., Song, L., Qi, Y.: Geniepath: Graph neural net-\nworks with adaptive receptive paths. In: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence. vol. 33, pp. 4424–4431 (2019)\n17. Logeswaran, L., Lee, H.: An efﬁcient framework for learning sentence representations. In:\nInternational Conference on Learning Representations (2018)\n18. Macchi, O.: The coincidence approach to stochastic point processes. Advances in Applied\nProbability 7, 83–122 (1975)\n19. Mccallum, A.K., Nigam, K., Rennie, J., Seymore, K.: Automating the construction of inter-\nnet portals with machine learning. Information Retrieval 3(2), 127–163 (2000)\n20. Namata, G., London, B., Getoor, L., Huang, B., EDU, U.: Query-driven active surveying\nfor collective classiﬁcation. In: 10th International Workshop on Mining and Learning with\nGraphs (2012)\n21. Narayanan, A., Chandramohan, M., Chen, L., Liu, Y., Saminathan, S.: subgraph2vec: Learn-\ning distributed representations of rooted sub-graphs from large graphs (2016)\n22. van den Oord, A., Li, Y., Vinyals, O.: Representation learning with contrastive predictive\ncoding (2019)\n23. Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: Online learning of social representations. In:\nProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery\nand data mining. pp. 701–710 (2014)\n24. Qiu, J., Chen, Q., Dong, Y., Zhang, J., Yang, H., Ding, M., Wang, K., Tang, J.: Gcc. In:\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining (2020)\n25. Ribeiro, L.F., Saverese, P.H., Figueiredo, D.R.: struc2vec: Learning node representations\nfrom structural identity. In: Proceedings of the 23rd ACM SIGKDD international conference\non knowledge discovery and data mining. pp. 385–394 (2017)\n26. Shervashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K., Borgwardt, K.: Efﬁcient\ngraphlet kernels for large graph comparison. In: Proceedings of the Twelth International\nConference on Artiﬁcial Intelligence and Statistics. pp. 488–495 (2009)\n27. Sun, F.Y., Hoffmann, J., Verma, V., Tang, J.: Infograph: Unsupervised and semi-supervised\ngraph-level representation learning via mutual information maximization. In: International\nConference on Learning Representations (2019)\n28. Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., Mei, Q.: Line: Large-scale information\nnetwork embedding. In: Proceedings of the 24th International Conference on World Wide\nWeb. p. 1067–1077. International World Wide Web Conferences Steering Committee (2015)\n29. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., Bengio, Y.: Graph attention\nnetworks. In: International Conference of Learning Representations (2018)\n30. Veliˇckovi´c, P., Fedus, W., Hamilton, W.L., Li`o, P., Bengio, Y., Hjelm, R.D.: Deep graph\ninfomax. In: International Conference on Learning Representations (2019)\n31. Xu, B., Shen, H., Cao, Q., Cen, K., Cheng, X.: Graph convolutional networks using heat\nkernel for semi-supervised learning. In: Proceedings of the Twenty-Eighth International Joint\nConference on Artiﬁcial Intelligence (2019)\n32. Xu, K., Li, C., Tian, Y., Sonobe, T., ichi Kawarabayashi, K., Jegelka, S.: Representation\nlearning on graphs with jumping knowledge networks. In: International Conference on Ma-\nchine Learning. pp. 5453–5462 (2018)\n33. Yuan, H., Ji, S.: Structpool: Structured graph pooling via conditional random ﬁelds. In: In-\nternational Conference on Learning Representations (2020)\n34. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk mini-\nmization. In: International Conference on Learning Representations (2018)\n35. Zhang, M., Chen, Y.: Link prediction based on graph neural networks. In: Proceedings of the\n32nd International Conference on Neural Information Processing Systems. p. 5171–5181\n(2018)\n36. Zhu, X., Ghahramani, Z., Lafferty, J.: Semi-supervised learning using gaussian ﬁelds and\nharmonic functions. In: Proceedings of the 20th International conference on Machine learn-\ning (ICML-03). p. 912–919 (2003)\n37. Zitnik, M., Leskovec, J.: Predicting multicellular function through multi-layer tissue net-\nworks. Bioinformatics 33, i190–i198 (2017)\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-04-13",
  "updated": "2021-04-13"
}